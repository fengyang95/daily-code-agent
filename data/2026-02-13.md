<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 14]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.AI](#cs.AI) [Total: 30]
- [cs.LG](#cs.LG) [Total: 19]
- [tldr.article](#tldr.article) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Small Updates, Big Doubts: Does Parameter-Efficient Fine-tuning Enhance Hallucination Detection ?](https://arxiv.org/abs/2602.11166)
*Xu Hu,Yifan Zhang,Songtao Wei,Chen Zhao,Qiannan Li,Bingzhe Li,Feng Chen*

Main category: cs.CL

TL;DR: 本文系统研究了参数高效微调(PEFT)方法对幻觉检测的影响，发现PEFT能显著提升幻觉检测能力，主要通过重塑不确定性编码而非注入新事实知识。


<details>
  <summary>Details</summary>
Motivation: 尽管PEFT方法被广泛用于适配大语言模型并常被认为能改善事实正确性，但PEFT如何影响幻觉行为（特别是在QA数据集上）仍不够清楚，需要系统研究。

Method: 在三个开源LLM骨干和三个事实寻求QA基准上进行综合实证研究，使用七种无监督幻觉检测方法（涵盖语义一致性、置信度和熵三种互补方法），并通过线性探针和表示诊断进行进一步分析。

Result: 实验结果显示PEFT能一致地增强幻觉检测能力，在各种幻觉检测器上显著提高AUROC；进一步分析表明PEFT主要通过重塑不确定性编码和表达方式，而非向模型注入新的事实知识。

Conclusion: PEFT方法能有效提升幻觉检测性能，其机制主要是改变模型对不确定性的编码和表达方式，这为理解和改进PEFT在幻觉检测中的应用提供了重要见解。

Abstract: Parameter-efficient fine-tuning (PEFT) methods are widely used to adapt large language models (LLMs) to downstream tasks and are often assumed to improve factual correctness. However, how the parameter-efficient fine-tuning methods affect hallucination behavior remains insufficiently understood, especially on QA datasets. In this work, we systematically investigate the impact of PEFT on hallucination detection through a comprehensive empirical study across three open-weight LLM backbones and three fact-seeking QA benchmarks. For each model, we evaluate performance using seven unsupervised hallucination detection methods spanning three complementary approaches: semantic consistency based detectors, confidence based detectors, and entropy based detectors. This multifaceted evaluation enables us to characterize how PEFT reshapes uncertainty across different detection paradigms. In conclusion, our experimental results show that PEFT consistently strengthens hallucination detection ability, substantially improving AUROC across a wide range of hallucination detectors. Besides, further analyses using linear probes and representation diagnostics indicate that PEFT methods primarily reshapes how uncertainty is encoded and surfaced, comparing with injecting new factual knowledge into the models.

</details>


### [2] [Visualizing and Benchmarking LLM Factual Hallucination Tendencies via Internal State Analysis and Clustering](https://arxiv.org/abs/2602.11167)
*Nathan Mao,Varun Kaushik,Shreya Shivkumar,Parham Sharafoleslami,Kevin Zhu,Sunishchal Dev*

Main category: cs.CL

TL;DR: FalseCite是一个用于系统研究LLM幻觉的数据集，通过误导性引用诱导幻觉，并在多个模型上测试发现GPT-4o-mini对虚假引用最敏感，同时通过隐藏状态分析发现幻觉与非幻觉状态呈现独特的角状结构。


<details>
  <summary>Details</summary>
Motivation: LLM经常产生幻觉（生成无意义或虚假信息），这在医学、法律等敏感领域尤其有害。为了系统研究这一现象，需要专门的数据集来捕捉和评估由误导性或伪造引用引发的幻觉。

Method: 引入FalseCite数据集，该数据集专门设计用于捕捉和基准测试由误导性或伪造引用诱导的幻觉响应。在GPT-4o-mini、Falcon-7B和Mistral 7-B等模型上运行FalseCite，分析幻觉活动。同时分析幻觉模型的内部状态，通过可视化和聚类隐藏状态向量来研究幻觉机制。

Result: 使用FalseCite测试发现，对于带有欺骗性引用的虚假声明，幻觉活动显著增加，尤其是在GPT-4o-mini中。通过隐藏状态向量分析发现，无论是否发生幻觉，隐藏状态向量都倾向于形成一个独特的角状形状。

Conclusion: FalseCite作为评估和减轻未来LLM研究中幻觉的基础具有重要潜力，为系统研究LLM幻觉现象提供了有效工具。

Abstract: Large Language Models (LLMs) often hallucinate, generating nonsensical or false information that can be especially harmful in sensitive fields such as medicine or law. To study this phenomenon systematically, we introduce FalseCite, a curated dataset designed to capture and benchmark hallucinated responses induced by misleading or fabricated citations. Running GPT-4o-mini, Falcon-7B, and Mistral 7-B through FalseCite, we observed a noticeable increase in hallucination activity for false claims with deceptive citations, especially in GPT-4o-mini. Using the responses from FalseCite, we can also analyze the internal states of hallucinating models, visualizing and clustering the hidden state vectors. From this analysis, we noticed that the hidden state vectors, regardless of hallucination or non-hallucination, tend to trace out a distinct horn-like shape. Our work underscores FalseCite's potential as a foundation for evaluating and mitigating hallucinations in future LLM research.

</details>


### [3] [Mechanistic Interpretability for Large Language Model Alignment: Progress, Challenges, and Future Directions](https://arxiv.org/abs/2602.11180)
*Usman Naseem*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLM）的机制可解释性技术及其在模型对齐中的应用，分析了从电路发现到因果干预等多种方法，并探讨了可解释性如何指导RLHF、宪法AI等对齐策略，同时指出了叠加假设、神经元多义性等挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在各种任务上表现出色，但其内部决策过程仍然不透明。机制可解释性（研究神经网络如何通过学到的表示和计算结构实现算法）已成为理解和对齐这些模型的关键研究方向。

Method: 本文采用综述研究方法，系统回顾了应用于LLM对齐的机制可解释性技术，包括电路发现、特征可视化、激活引导和因果干预等方法，并分析了这些可解释性见解如何指导对齐策略。

Result: 识别了机制可解释性在LLM对齐中的关键挑战，包括叠加假设、神经元的多义性，以及大规模模型中涌现行为的解释困难。提出了未来研究方向，包括自动化可解释性、电路的跨模型泛化，以及可扩展的可解释性驱动对齐技术。

Conclusion: 机制可解释性对于理解和对齐LLM至关重要，当前技术已取得进展但仍面临挑战，未来需要发展能够扩展到前沿模型的自动化可解释性方法和可解释性驱动的对齐技术。

Abstract: Large language models (LLMs) have achieved remarkable capabilities across diverse tasks, yet their internal decision-making processes remain largely opaque. Mechanistic interpretability (i.e., the systematic study of how neural networks implement algorithms through their learned representations and computational structures) has emerged as a critical research direction for understanding and aligning these models. This paper surveys recent progress in mechanistic interpretability techniques applied to LLM alignment, examining methods ranging from circuit discovery to feature visualization, activation steering, and causal intervention. We analyze how interpretability insights have informed alignment strategies including reinforcement learning from human feedback (RLHF), constitutional AI, and scalable oversight. Key challenges are identified, including the superposition hypothesis, polysemanticity of neurons, and the difficulty of interpreting emergent behaviors in large-scale models. We propose future research directions focusing on automated interpretability, cross-model generalization of circuits, and the development of interpretability-driven alignment techniques that can scale to frontier models.

</details>


### [4] [MetaMem: Evolving Meta-Memory for Knowledge Utilization through Self-Reflective Symbolic Optimization](https://arxiv.org/abs/2602.11182)
*Haidong Xin,Xinze Li,Zhenghao Liu,Yukun Yan,Shuo Wang,Cheng Yang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: MetaMem是一个增强LLM记忆系统的框架，通过自演化的元记忆教导LLM如何有效利用记忆知识，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统虽然能支持长时人机交互，但往往破坏了交互会话的内在逻辑和时间关系，导致记忆碎片化和推理性能下降。

Method: 提出MetaMem框架，通过自演化的元记忆系统，迭代提炼跨任务的可转移知识利用经验，通过自我反思推理过程并更新元记忆状态，指导LLM系统性地识别和整合分散的记忆片段。

Result: 广泛实验表明MetaMem显著优于强基线方法，性能提升超过3.6%。

Conclusion: MetaMem通过元记忆增强记忆系统，有效解决了记忆碎片化问题，提升了LLM在长时交互中的推理能力。

Abstract: Existing memory systems enable Large Language Models (LLMs) to support long-horizon human-LLM interactions by persisting historical interactions beyond limited context windows. However, while recent approaches have succeeded in constructing effective memories, they often disrupt the inherent logical and temporal relationships within interaction sessions, resulting in fragmented memory units and degraded reasoning performance. In this paper, we propose MetaMem, a novel framework that augments memory systems with a self-evolving meta-memory, aiming to teach LLMs how to effectively utilize memorized knowledge. During meta-memory optimization, MetaMem iteratively distills transferable knowledge utilization experiences across different tasks by self-reflecting on reasoning processes and performing actions to update the current meta-memory state. The accumulated meta-memory units serve as explicit knowledge utilization experiences, guiding the LLM to systematically identify and integrate critical evidence from scattered memory fragments. Extensive experiments demonstrate the effectiveness of MetaMem, which significantly outperforms strong baselines by over 3.6%. All codes and datasets are available at https://github.com/OpenBMB/MetaMem.

</details>


### [5] [DDL2PropBank Agent: Benchmarking Multi-Agent Frameworks' Developer Experience Through a Novel Relational Schema Mapping Task](https://arxiv.org/abs/2602.11198)
*Shafiuddin Rehan Ahmed,Wei Wei*

Main category: cs.CL

TL;DR: 提出了DDL2PropBank基准任务，用于评估多智能体框架的开发体验，通过10个框架的对比发现Agno在代码复杂度和AI辅助性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 多智能体框架虽然简化了LLM驱动的软件开发，但缺乏在受控环境下评估开发者体验的系统方法。

Method: 引入DDL2PropBank基准任务（数据库模式到PropBank角色集映射），采用"智能体即工具"模式在10个框架中实现相同逻辑，从代码复杂度（静态分析）和AI辅助性（LLM自动生成正确代码的能力）两个维度评估。

Result: 发现三倍复杂度谱系，Pydantic AI和Agno实现开销最小；结构对齐分数能可靠预测单模式框架的运行成功，但对多模式框架高估正确性；Agno综合表现最强，复杂度最低、结构对齐最高且pass@1达83%。

Conclusion: Agno是最佳多智能体框架选择，结合了低复杂度和高AI辅助性；结构对齐可作为有效的代理指标，但对多模式框架需谨慎。

Abstract: Multi-agent frameworks promise to simplify LLM-driven software development, yet there is no principled way to evaluate their developer experience in a controlled setting. We introduce DDL2PropBank, a novel benchmark task that maps relational database schemas to PropBank rolesets, requiring autonomous retrieval of candidate frames and fine-grained linguistic reasoning over table names, columns, and relations. Using the Agent-as-a-Tool pattern, we implement identical agent logic across 10 frameworks and evaluate along two dimensions: (i) code complexity via static analysis, and (ii) AI-assistability -- the extent to which LLMs can autonomously generate correct, framework-specific code. Our results reveal a threefold complexity spectrum, with Pydantic AI and Agno requiring the least implementation overhead. For AI-assistability, structural alignment scores reliably proxy runtime success for frameworks with single canonical patterns, but overestimate correctness for multi-pattern frameworks. Agno emerges as the strongest overall performer, combining lowest complexity with highest structural alignment and 83% pass@1.

</details>


### [6] [Are Aligned Large Language Models Still Misaligned?](https://arxiv.org/abs/2602.11305)
*Usman Naseem,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Sushant Kumar Ray,Abdullah Mohammad,Agrima Seth*

Main category: cs.CL

TL;DR: 提出Mis-Align Bench统一基准，用于同时评估LLM在安全、价值观和文化三个维度的错位问题，构建了包含382,424样本的SAVACU数据集，并发现单维度模型在联合条件下存在高假失败率和低对齐分数。


<details>
  <summary>Details</summary>
Motivation: 现有错位基准（如INSECURE CODE、VALUEACTIONLENS、CULTURALHERITAGE）只能单独评估安全、价值观或文化维度，无法同时评估这三个在现实世界中必须共存的维度，这限制了全面评估LLM错位问题的能力。

Method: 1) 构建SAVACU数据集：将LLM-PROMPT-DATASET重新分类为14个安全域、56个价值观域和42个文化域，使用Mistral-7B-Instruct-v0.3分类，用Llama-3.1-8B-Instruct扩展低资源域，通过SimHash指纹去重；2) 通过两阶段拒绝采样为提示配对错位和对齐响应；3) 在通用、微调和开源LLM上进行基准测试。

Result: 单维度模型在联合条件下覆盖率可达97.6%，但假失败率超过50%，对齐分数较低（63%-66%），表明现有单维度评估方法在同时考虑多个维度时效果不佳。

Conclusion: 需要统一的多维度基准来全面评估LLM错位问题，因为单维度评估在联合条件下表现不佳，无法反映现实世界中安全、价值观和文化维度必须同时满足的要求。

Abstract: Misalignment in Large Language Models (LLMs) arises when model behavior diverges from human expectations and fails to simultaneously satisfy safety, value, and cultural dimensions, which must co-occur in real-world settings to solve a real-world query. Existing misalignment benchmarks-such as INSECURE CODE (safety-centric), VALUEACTIONLENS (value-centric), and CULTURALHERITAGE (culture centric)-rely on evaluating misalignment along individual dimensions, preventing simultaneous evaluation. To address this gap, we introduce Mis-Align Bench, a unified benchmark for analyzing misalignment across safety, value, and cultural dimensions. First we constructs SAVACU, an English misaligned-aligned dataset of 382,424 samples spanning 112 domains (or labels), by reclassifying prompts from the LLM-PROMPT-DATASET via taxonomy into 14 safety domains, 56 value domains, and 42 cultural domains using Mistral-7B-Instruct-v0.3, and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based fingerprint to avoid deduplication. Furthermore, we pairs prompts with misaligned and aligned responses via two-stage rejection sampling to enforce quality. Second we benchmarks general-purpose, fine-tuned, and open-weight LLMs, enabling systematic evaluation of misalignment under three dimensions. Empirically, single-dimension models achieve high Coverage (upto 97.6%) but incur False Failure Rate >50% and lower Alignment Score (63%-66%) under joint conditions.

</details>


### [7] [When Audio-LLMs Don't Listen: A Cross-Linguistic Study of Modality Arbitration](https://arxiv.org/abs/2602.11488)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 研究发现语音-语言模型在音频与文本冲突时，会过度偏向文本（16.6% vs 文本-文本冲突的1.6%），这种"文本主导"现象源于模型推理过程的可访问性不对称，而非音频质量或信息内容差异。


<details>
  <summary>Details</summary>
Motivation: 现有语音基准测试主要关注准确性，但忽略了模态仲裁这一重要维度。当音频和文本信息冲突时，模型如何权衡不同模态的信息可靠性是一个尚未充分研究的问题。

Method: 使用ALME基准测试（57,602个受控音频-文本冲突刺激，覆盖8种语言），分析Gemini 2.0 Flash等模型在音频-文本冲突下的行为。通过强制转录、文本标记为"故意损坏"、微调消融实验等方法探究文本主导现象的机制。

Result: 模型在音频-文本冲突下的文本主导率（16.6%）远高于文本-文本冲突（1.6%）。音频质量不是原因（音频准确率97.2% > 级联准确率93.9%）。文本主导主要源于语言模型推理过程的可访问性不对称。

Conclusion: 模态仲裁是语音基准测试中未被充分考虑的可靠性维度。文本主导现象反映了模型推理过程的结构性偏见，而非信息内容差异。这一发现对构建更平衡的多模态系统具有重要意义。

Abstract: When audio and text conflict, speech-enabled language models follow the text 10 times more often than when arbitrating between two text sources, even when explicitly instructed to trust the audio. Using ALME, a benchmark of 57,602 controlled audio-text conflict stimuli across 8 languages, we find that Gemini 2.0 Flash exhibits 16.6\% text dominance under audio-text conflict versus 1.6\% under text-text conflict with identical reliability cues. This gap is not explained by audio quality: audio-only accuracy (97.2\%) exceeds cascade accuracy (93.9\%), indicating audio embeddings preserve more information than text transcripts. We propose that text dominance reflects an asymmetry not in information content but in arbitration accessibility: how easily the model can reason over competing representations.
  This framework explains otherwise puzzling findings. Forcing transcription before answering increases text dominance (19\% to 33\%), sacrificing audio's information advantage without improving accessibility. Framing text as ``deliberately corrupted'' reduces text dominance by 80\%. A fine-tuning ablation provides interventional evidence: training only the audio projection layer increases text dominance (+26.5\%), while LoRA on the language model halves it ($-$23.9\%), localizing text dominance to the LLM's reasoning rather than the audio encoder. Experiments across four state-of-the-art audio-LLMs and 8 languages show consistent trends with substantial cross-linguistic and cross-model variation, establishing modality arbitration as a distinct reliability dimension not captured by standard speech benchmarks.

</details>


### [8] [SIGHT: Reinforcement Learning with Self-Evidence and Information-Gain Diverse Branching for Search Agent](https://arxiv.org/abs/2602.11551)
*Wenlin Zhong,Jinluan Yang,Yiquan Wu,Yi Liu,Jianhang Yao,Kun Kuang*

Main category: cs.CL

TL;DR: SIGHT框架通过自证据支持和信息增益驱动的多样化分支，解决多轮搜索中冗余和噪声问题，提升LLM的搜索推理能力


<details>
  <summary>Details</summary>
Motivation: 在多轮搜索场景中，强化学习驱动的LLM面临搜索结果冗余高、信噪比低的问题，导致代理陷入"隧道视觉"——早期噪声检索的强制解释引发不可逆的错误累积

Method: 提出SIGHT框架：1) 通过自证据支持(SES)将搜索结果提炼为高保真证据；2) 计算信息增益分数识别关键状态；3) 动态提示干预（去重、反思、自适应分支）生成新分支；4) 通过组相对策略优化整合SES和正确性奖励

Result: 在单跳和多跳QA基准测试中，SIGHT显著优于现有方法，特别是在复杂推理场景中，且使用更少的搜索步骤

Conclusion: SIGHT通过自证据支持和信息增益驱动的多样化分支，有效解决了多轮搜索中的冗余和噪声问题，使LLM能够内化稳健的探索策略而无需外部验证器

Abstract: Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to master autonomous search for complex question answering. However, particularly within multi-turn search scenarios, this interaction introduces a critical challenge: search results often suffer from high redundancy and low signal-to-noise ratios. Consequently, agents easily fall into "Tunnel Vision," where the forced interpretation of early noisy retrievals leads to irreversible error accumulation. To address these challenges, we propose SIGHT, a framework that enhances search-based reasoning through Self-Evidence Support (SES) and Information-Gain Driven Diverse Branching. SIGHT distills search results into high-fidelity evidence via SES and calculates an Information Gain score to pinpoint pivotal states where observations maximally reduce uncertainty. This score guides Dynamic Prompting Interventions - including de-duplication, reflection, or adaptive branching - to spawn new branches with SES. Finally, by integrating SES and correctness rewards via Group Relative Policy Optimization, SIGHT internalizes robust exploration strategies without external verifiers. Experiments on single-hop and multi-hop QA benchmarks demonstrate that SIGHT significantly outperforms existing approaches, particularly in complex reasoning scenarios, using fewer search steps.

</details>


### [9] [PACE: Prefix-Protected and Difficulty-Aware Compression for Efficient Reasoning](https://arxiv.org/abs/2602.11639)
*Ruixiang Feng,Yuntao Wen,Silin Zhou,Ke Shi,Yifan Wang,Ran Le,Zhenwei An,Zongchao Chen,Chen Yang,Guangyue Peng,Yiming Jia,Dongsheng Wang,Tao Zhang,Lisi Chen,Yang Song,Shen Gao,Shuo Shang*

Main category: cs.CL

TL;DR: 提出Dual-Level框架，通过前缀保护和难度感知压缩解决语言推理模型过思考问题，在减少55.7%token使用的同时提升4.1%准确率


<details>
  <summary>Details</summary>
Motivation: 现有语言推理模型(LRMs)通过增加推理时间计算获得强性能，但存在"过思考"问题，产生过长的推理轨迹，增加延迟和内存使用。现有方法采用统一的长度惩罚，在序列层面过度压缩关键早期推理步骤，在组层面不加区分地惩罚所有查询。

Method: 提出双层次框架：1) 序列层面：前缀保护优化，使用衰减混合rollouts保持有效推理路径同时促进简洁性；2) 组层面：难度感知惩罚，根据查询复杂度动态调整长度约束，对困难问题保持探索，对简单问题抑制冗余。

Result: 在DeepSeek-R1-Distill-Qwen (1.5B/7B)上的实验显示，模型在数学基准测试中实现token使用大幅减少(最多55.7%)，同时准确率提升(最多4.1%)，并展示了对代码、科学和通用领域的泛化能力。

Conclusion: 提出的双层次压缩框架有效解决了语言推理模型的过思考问题，在保持甚至提升性能的同时显著减少计算开销，具有广泛的适用性。

Abstract: Language Reasoning Models (LRMs) achieve strong performance by scaling test-time computation but often suffer from ``overthinking'', producing excessively long reasoning traces that increase latency and memory usage. Existing LRMs typically enforce conciseness with uniform length penalties, which over-compress crucial early deduction steps at the sequence level and indiscriminately penalize all queries at the group level. To solve these limitations, we propose \textbf{\model}, a dual-level framework for prefix-protected and difficulty-aware compression under hierarchical supervision. At the sequence level, prefix-protected optimization employs decaying mixed rollouts to maintain valid reasoning paths while promoting conciseness. At the group level, difficulty-aware penalty dynamically scales length constraints based on query complexity, maintaining exploration for harder questions while curbing redundancy on easier ones. Extensive experiments on DeepSeek-R1-Distill-Qwen (1.5B/7B) demonstrate that \model achieves a substantial reduction in token usage (up to \textbf{55.7\%}) while simultaneously improving accuracy (up to \textbf{4.1\%}) on math benchmarks, with generalization ability to code, science, and general domains.

</details>


### [10] [Thinking with Drafting: Optical Decompression via Logical Reconstruction](https://arxiv.org/abs/2602.11731)
*Jingxuan Wei,Honghao He,Caijun Jia,Siyuan Li,Zheng Sun,Yuhang Xu,Yuanyuan Lin,Linzhuang Sun,Yuchen Wu,Bihui Yu,Xiangxiang Zhang,Cheng Tan*

Main category: cs.CL

TL;DR: 该论文提出"Thinking with Drafting"方法，通过领域特定语言作为中间表示，将视觉推理重构为光学解压缩过程，实现逻辑结构的精确重建和自验证。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在复杂推理任务中存在精度悖论：视觉感知系统转录符号但不捕获逻辑拓扑，像素生成模型产生缺乏数学精确性的视觉伪影。需要弥合这一差距。

Method: 提出"Thinking with Drafting"方法，将视觉推理重新概念化为光学解压缩，使用极简领域特定语言作为基础中间表示，强制模型将其心智模型草拟为可执行代码，生成确定性视觉证明进行自验证。

Result: 实验在VisAlg视觉代数基准上验证了TwD作为优越认知支架的有效性，建立了视觉生成作为逻辑验证器的闭环系统。

Conclusion: 该工作建立了视觉生成作为逻辑验证器而非创造性输出的闭环系统，为视觉推理提供了可泛化的路径。

Abstract: Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.

</details>


### [11] [Towards Fair and Comprehensive Evaluation of Routers in Collaborative LLM Systems](https://arxiv.org/abs/2602.11877)
*Wanxing Wu,He Zhu,Yixia Li,Lei Yang,Jiehui Zhao,Hongru Wang,Jian Yang,Benyou Wang,Bingyi Jing,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出了RouterXBench评估框架和ProbeDirichlet路由方法，用于优化本地小模型与云端大模型之间的查询分配，通过内部隐藏状态和Dirichlet分布实现更好的路由性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM部署面临成本与隐私约束，需要本地部署小模型同时将复杂查询卸载到云端大模型。现有路由评估方法不系统，忽略了场景特定要求和分布外鲁棒性。

Method: 提出RouterXBench评估框架（路由能力、场景对齐、跨域鲁棒性三个维度），并设计ProbeDirichlet路由方法，利用内部隐藏状态捕获模型不确定性，通过可学习的Dirichlet分布聚合跨层隐藏状态进行概率训练。

Result: ProbeDirichlet在路由能力和高精度场景中相对最佳基线分别提升16.68%和18.86%，在不同模型家族、规模、异构任务和智能体工作流中表现一致。

Conclusion: 基于内部隐藏状态和Dirichlet分布的路由方法在系统评估框架下显著优于现有方法，能有效平衡成本、隐私和性能需求。

Abstract: Large language models (LLMs) have achieved success, but cost and privacy constraints necessitate deploying smaller models locally while offloading complex queries to cloud-based models. Existing router evaluations are unsystematic, overlooking scenario-specific requirements and out-of-distribution robustness. We propose RouterXBench, a principled evaluation framework with three dimensions: router ability, scenario alignment, and cross-domain robustness. Unlike prior work that relies on output probabilities or external embeddings, we utilize internal hidden states that capture model uncertainty before answer generation. We introduce ProbeDirichlet, a lightweight router that aggregates cross-layer hidden states via learnable Dirichlet distributions with probabilistic training. Trained on multi-domain data, it generalizes robustly across in-domain and out-of-distribution scenarios. Our results show ProbeDirichlet achieves 16.68% and 18.86% relative improvements over the best baselines in router ability and high-accuracy scenarios, with consistent performance across model families, model scales, heterogeneous tasks, and agentic workflows.

</details>


### [12] [AdaptEvolve: Improving Efficiency of Evolutionary AI Agents through Adaptive Model Selection](https://arxiv.org/abs/2602.11931)
*Pretam Ray,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: AdaptEvolve：基于置信度的自适应LLM选择框架，在进化式序列精炼中动态选择LLM，平衡计算效率与推理能力，平均减少37.9%推理成本同时保持97.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 进化式智能体系统在推理过程中反复调用大语言模型，加剧了计算效率与推理能力之间的权衡。现有路由策略通常依赖静态启发式或外部控制器，未能显式考虑模型不确定性，需要一种能动态选择足够能力LLM同时保持计算效率的方法。

Method: 提出AdaptEvolve框架：在进化式序列精炼框架中，利用内在生成置信度来估计实时可解性，实现基于置信度的自适应LLM选择。通过模型级联机制，根据当前生成步骤的需求动态选择足够能力但计算高效的LLM。

Result: 置信度驱动的选择产生了有利的帕累托前沿，在基准测试中平均减少37.9%的总推理成本，同时保留了静态大模型基线97.5%的上界准确率。

Conclusion: AdaptEvolve通过置信度驱动的自适应LLM选择，有效平衡了进化式智能体系统中的计算效率与推理能力权衡，为多LLM进化精炼提供了实用的解决方案。

Abstract: Evolutionary agentic systems intensify the trade-off between computational efficiency and reasoning capability by repeatedly invoking large language models (LLMs) during inference. This setting raises a central question: how can an agent dynamically select an LLM that is sufficiently capable for the current generation step while remaining computationally efficient? While model cascades offer a practical mechanism for balancing this trade-off, existing routing strategies typically rely on static heuristics or external controllers and do not explicitly account for model uncertainty. We introduce AdaptEvolve: Adaptive LLM Selection for Multi-LLM Evolutionary Refinement within an evolutionary sequential refinement framework that leverages intrinsic generation confidence to estimate real-time solvability. Empirical results show that confidence-driven selection yields a favourable Pareto frontier, reducing total inference cost by an average of 37.9% across benchmarks while retaining 97.5% of the upper-bound accuracy of static large-model baselines. Our code is available at https://github.com/raypretam/adaptive_llm_selection.

</details>


### [13] [Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.12036)
*Xin Xu,Clive Bai,Kai Yang,Tianhao Chen,Yangkun Chen,Weijie Liu,Hao Chen,Yang Wang,Saiyong Yang,Can Yang*

Main category: cs.CL

TL;DR: Composition-RL通过自动组合多个问题创建新的可验证提示，有效利用RLVR训练中pass-rate-1的简单提示，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 大规模可验证提示是RLVR成功的关键，但包含大量无信息样本且扩展成本高。现有方法关注利用hard prompts（通过率为0），但训练过程中easy prompts（通过率为1）逐渐增多，降低了有效数据规模。

Method: 提出Composition-RL方法：自动组合多个问题形成新的可验证问题，将这些组合提示用于RL训练。还设计了课程学习变体，逐步增加组合深度。

Result: 在4B到30B不同模型规模上的实验表明，Composition-RL相比原始数据集训练的RL持续提升推理能力。课程变体可进一步优化性能，且能实现更有效的跨领域RL。

Conclusion: Composition-RL通过有效利用pass-rate-1提示，解决了RLVR训练中数据利用率低的问题，为有限可验证提示的高效利用提供了简单有效的解决方案。

Abstract: Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL.

</details>


### [14] [dVoting: Fast Voting for dLLMs](https://arxiv.org/abs/2602.12153)
*Sicheng Feng,Zigeng Chen,Xinyin Ma,Gongfan Fang,Xinchao Wang*

Main category: cs.CL

TL;DR: dVoting是一种基于扩散大语言模型的快速投票技术，通过并行生成和迭代精炼提升推理能力，无需额外训练


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)相比自回归模型具有并行生成的优势，但如何有效利用这种能力提升推理性能仍待探索。作者观察到在多样本生成中，大部分token预测一致，性能差异主要来自少数不确定token

Method: dVoting利用dLLMs的任意位置生成能力，通过采样、一致性分析识别不确定token、投票重新生成、迭代精炼直到收敛的流程提升推理质量

Result: 在多个基准测试中取得显著提升：GSM8K提升6.22%-7.66%，MATH500提升4.40%-7.20%，ARC-C提升3.16%-14.84%，MMLU提升4.83%-5.74%

Conclusion: dVoting有效利用dLLMs的并行生成能力，通过投票机制提升推理性能，为测试时扩展提供了高效解决方案

Abstract: Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at https://github.com/fscdc/dVoting

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [SAFuzz: Semantic-Guided Adaptive Fuzzing for LLM-Generated Code](https://arxiv.org/abs/2602.11209)
*Ziyi Yang,Kalit Inani,Keshav Kabra,Vima Gupta,Anand Padmanabha Iyer*

Main category: cs.SE

TL;DR: SAFuzz是一个混合测试框架，利用LLM引导的自适应模糊测试来高效检测算法漏洞，通过提示驱动的行为多样化、特定问题测试生成和LLM预测器实现自适应资源分配和动态早期停止。


<details>
  <summary>Details</summary>
Motivation: AI编码助手加速软件开发，但当前测试框架难以跟上AI生成代码的数量。传统模糊测试技术通常均匀分配资源，缺乏对算法漏洞模式的语义理解，导致资源使用效率低下和漏洞遗漏。

Method: 提出混合测试框架SAFuzz，整合：1) 基于提示的行为多样化；2) 带有特定问题oracle的测试生成；3) LLM预测器实现自适应资源分配和动态早期停止。

Result: 在CSES算法问题上评估，将漏洞判别精度从77.9%提升到85.7%，相比SOTA GreenFuzz减少1.71倍时间成本，同时保持相当的召回率。与现有单元测试生成方法结合，将bug检测召回率从67.3%提升到79.5%。

Conclusion: SAFuzz通过LLM引导的自适应模糊测试有效解决了AI生成代码测试的挑战，提高了漏洞检测效率和精度，并能与现有测试方法形成互补。

Abstract: While AI-coding assistants accelerate software development, current testing frameworks struggle to keep pace with the resulting volume of AI-generated code. Traditional fuzzing techniques often allocate resources uniformly and lack semantic awareness of algorithmic vulnerability patterns, leading to inefficient resource usage and missed vulnerabilities. To address these limitations, we present a hybrid testing framework that leverages LLM-guided adaptive fuzzing to detect algorithmic vulnerabilities efficiently. Our system SAFuzz integrates prompt-based behavioral diversification, harness generation with problem-specific oracles, and an LLM-based predictor to enable adaptive resource allocation and dynamic early stopping. Evaluating SAFuzz on CSES algorithmic problems, we improve vulnerability discrimination precision from 77.9% to 85.7% and achieve a 1.71x reduction in time cost compared to SOTA GreenFuzz while maintaining comparable recall. We further observe that combining our approach with existing unit test generation methods yields complementary gains, increasing the bug detection recall from 67.3% to 79.5%.

</details>


### [16] [SWE-MiniSandbox: Container-Free Reinforcement Learning for Building Software Engineering Agents](https://arxiv.org/abs/2602.11210)
*Danlong Yuan,Wei Wu,Zhengren Wang,Xueliang Zhao,Huishuai Zhang,Dongyan Zhao*

Main category: cs.SE

TL;DR: SWE-MiniSandbox：一种轻量级、无容器的RL训练方法，用于软件工程代理，通过内核级隔离机制替代传统容器，显著降低存储开销和环境准备时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于容器的RL训练管道存在存储开销大、环境设置慢、需要容器管理权限等问题，限制了软件工程代理的可扩展性，特别是在资源受限的研究环境中。

Method: 提出SWE-MiniSandbox方法，使用内核级隔离机制为每个任务创建独立工作空间，替代传统的容器隔离；采用轻量级环境预缓存技术，避免使用庞大的容器镜像。

Result: 磁盘使用量降至容器基线的约5%，环境准备时间减少至约25%，同时保持了与标准容器管道相当的评估性能。

Conclusion: SWE-MiniSandbox通过消除对重型容器基础设施的依赖，为RL驱动的软件工程代理提供了实用且可扩展的基础，特别适合资源受限的研究环境。

Abstract: Reinforcement learning (RL) has become a key paradigm for training software engineering (SWE) agents, but existing pipelines typically rely on per-task containers for isolation. At scale, pre-built container images incur substantial storage overhead, slow environment setup, and require container-management privileges. We propose SWE-MiniSandbox, a lightweight, container-free method that enables scalable RL training of SWE agents without sacrificing isolation. Instead of relying on per-instance containers, SWE-MiniSandbox executes each task in an isolated workspace backed by kernel-level mechanisms, substantially reducing system overhead. It leverages lightweight environment pre-caching techniques to eliminate the need for bulky container images. As a result, our approach lowers disk usage to approximately 5\% of that required by container-based pipelines and reduces environment preparation time to about 25\% of the container baseline. Empirical results demonstrate that SWE-MiniSandbox achieves evaluation performance comparable to standard container-based pipelines. By removing the dependency on heavy container infrastructure, SWE-MiniSandbox offers a practical and accessible foundation for scaling RL-based SWE agents, particularly in resource-constrained research environments.

</details>


### [17] [Agent-Diff: Benchmarking LLM Agents on Enterprise API Tasks via Code Execution with State-Diff-Based Evaluation](https://arxiv.org/abs/2602.11224)
*Hubert M. Pysklo,Artem Zhuravel,Patrick D. Watson*

Main category: cs.SE

TL;DR: Agent-Diff是一个用于评估基于代码执行的智能体LLM的基准框架，通过沙盒化环境访问真实API接口，使用状态差异合约定义任务成功，并在224个企业软件任务上测试了9个LLM。


<details>
  <summary>Details</summary>
Motivation: 智能体LLM的性能因模型、外部工具访问、提示结构和智能体框架的差异而变化。现有基准需要在控制软件环境变化的沙盒方法和更具生态效度的真实服务方法之间做出权衡。Agent-Diff旨在结合这两种方法的优点。

Method: 1. 状态差异合约：将过程与结果分离，通过环境状态的预期变化定义任务成功，而非模糊的跟踪或参数匹配。2. 标准化沙盒：提供统一的脚本层，让所有模型通过外部API（Slack、Box、Linear、Google Calendar）执行代码。框架包含真实API接口访问，同时沙盒化调用、处理和评估环境。

Result: 在224个企业软件工作流任务上对9个LLM进行了基准测试。通过消融实验评估了API文档访问对基准性能的贡献，验证了框架的鲁棒性。

Conclusion: Agent-Diff提供了一个既能评估智能体LLM在真实世界服务接口上的性能，又能保持标准化评估环境的基准框架，解决了现有基准在生态效度和控制变量之间的权衡问题。

Abstract: We present Agent-Diff, a novel benchmarking framework for evaluating agentic Large Language Models (LLMs) on real-world tasks that execute code via external APIs. Agentic LLM performance varies due to differences in models, external tool access, prompt structures, and agentic frameworks. Benchmarks must make fundamental trade-offs between a sandboxed approach that controls for variation in software environments and more ecologically valid approaches employing real services. Agent-Diff attempts to capture the desirable features of both of these approaches by including access to the real API interfaces for software services while sandboxing the environment in which calls are made, processed, and evaluated. This approach relies on two key innovations. The first is a novel state-diff contract, which separates process from outcome - rather than fuzzy trace or parameter matching, we define task success as whether the expected change in environment state was achieved. The second is a novel sandbox that provides a standardized scripting layer that all models use to execute code against external APIs (Slack, Box, Linear, Google Calendar). Thus, we can evaluate different agentic LLMs against a standardized set of contracts using a unified sandbox while still evaluating their performance on real-world service interfaces. Using the Agent-Diff framework, we provide benchmarks for nine LLMs across 224 tasks utilizing enterprise software workflows. In addition, we evaluate the robustness of the framework with ablation experiments to assess the contribution of access to API documentation on benchmark performance. Code and data: https://github.com/agent-diff-bench/agent-diff.

</details>


### [18] [Improving the Robustness of Large Language Models for Code Tasks via Fine-tuning with Perturbed Data](https://arxiv.org/abs/2602.11411)
*Yang Liu,Armstrong Foundjem,Xingfang Wu,Heng Li,Foutse Khomh*

Main category: cs.SE

TL;DR: 通过使用扰动数据集微调LLMs，可以显著提升其在编码任务中对输入扰动的鲁棒性，但会带来轻微的性能下降。


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，LLMs已成为代码生成、分析和修复的重要工具。确保这些模型在处理多样化输入时的鲁棒性至关重要，因为输入变化可能导致错误或不安全的代码输出。

Method: 系统评估LLM鲁棒性，通过在字符级、词级和句子级扰动的数据集上微调模型，并与基础模型及在未扰动数据集上微调的模型进行比较。

Result: 使用扰动数据集微调LLMs显著提高了模型鲁棒性（RD通常下降约4%-6%），特别是对于鲁棒性相对较弱的模型。但这个过程通常会导致性能轻微下降（pass@1通常下降约1%-3%），尽管偶尔也能观察到性能提升。

Conclusion: 使用扰动数据微调LLMs进行编码任务能有效增强其鲁棒性，代价是轻微的性能下降，强调了在编码应用中平衡LLMs鲁棒性和性能的重要性。

Abstract: Context: In the fast-paced evolution of software development, Large Language Models (LLMs) have become indispensable tools for tasks such as code generation, completion, analysis, and bug fixing. Ensuring the robustness of these models against potential vulnerabilities from handling diverse inputs is critical, as variations in input can lead to incorrect or insecure code outputs.
  Objective: This work aims to improve the robustness of LLMs for coding-related tasks against potential adversarial inputs. Specifically, we investigate how fine-tuning LLMs with perturbed datasets impacts their robustness against input perturbations.
  Method: We systematically evaluated LLM robustness by fine-tuning models using datasets perturbed at character-level, word-level, and sentence-level, comparing results against base models and models fine-tuned on unperturbed datasets.
  Results: Fine-tuning LLMs with perturbed datasets significantly improves model robustness (RD usually drops around 4\% - 6\%), especially for models with relatively weak robustness. However, this fine-tuning process typically results in a slight performance decrease (pass@1 usually drops around 1\% - 3\%) compared to fine-tuning with unperturbed datasets, although occasional performance improvements are observed.
  Conclusion \& Implications: Fine-tuning LLMs for coding tasks with perturbed data effectively enhances their robustness at the cost of a minor performance reduction, emphasizing the importance of balancing the robustness and performance of LLMs for coding applications.

</details>


### [19] [How Smart Is Your GUI Agent? A Framework for the Future of Software Interaction](https://arxiv.org/abs/2602.11514)
*Sidong Feng,Chunyang Chen*

Main category: cs.SE

TL;DR: 提出GUI Agent Autonomy Levels (GAL)六层框架，用于明确GUI代理的自主程度，帮助衡量可信软件交互的进展


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理被描述为具有截然不同的自主程度，这模糊了能力、责任和风险，需要概念清晰化

Method: 提出GUI Agent Autonomy Levels (GAL)六层框架，使自主性明确化

Result: 建立了一个系统化的自主性分级框架，有助于基准测试可信软件交互的进展

Conclusion: 通过GAL框架提供概念清晰度，使GUI代理的自主性明确化，有助于衡量向可信软件交互的进展

Abstract: GUI agents are rapidly becoming a new interaction to software, allowing people to navigate web, desktop and mobile rather than execute them click by click. Yet ``agent'' is described with radically different degrees of autonomy, obscuring capability, responsibility and risk. We call for conceptual clarity through GUI Agent Autonomy Levels (GAL), a six-level framework that makes autonomy explicit and helps benchmark progress toward trustworthy software interaction.

</details>


### [20] [Do Not Treat Code as Natural Language: Implications for Repository-Level Code Generation and Beyond](https://arxiv.org/abs/2602.11671)
*Minh Le-Anh,Huyen Nguyen,Khanh An Tran,Nam Le Hai,Linh Ngo Van,Nghi D. Q. Bui,Bach Le*

Main category: cs.SE

TL;DR: Hydra是一个仓库级代码生成框架，通过结构感知索引和依赖感知检索器，解决了现有RAG方法在代码生成中忽略代码结构和依赖关系的问题，在真实仓库环境中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有CodeLLMs在仓库级代码生成中表现不佳，因为现有RAG方法借鉴NLP策略，使用基于分块的索引和基于相似性的检索，导致代码单元间连贯性丢失，忽略了代码结构关系和功能依赖。

Method: Hydra采用三种关键技术：1) 结构感知索引策略，将仓库表示为函数、类和变量的层次树；2) 轻量级依赖感知检索器(DAR)，显式识别和检索目标函数所需的真实依赖；3) 混合检索机制，结合DAR和相似性检索，提供必要构建块和实际使用示例。

Result: 在DevEval和RepoExec基准测试中，Hydra实现了最先进的性能，在Pass@1上超过最强基线5%以上，甚至能让较小模型匹配或超过依赖现有检索器的更大模型的性能。

Conclusion: 将代码视为结构化代码而非自然语言，通过结构感知索引和依赖感知检索，可以显著提升仓库级代码生成的性能，使CodeLLMs在复杂仓库环境中更有效。

Abstract: Large language models for code (CodeLLMs) have demonstrated remarkable success in standalone code completion and generation, sometimes even surpassing human performance, yet their effectiveness diminishes in repository-level settings where cross-file dependencies and structural context are essential. Existing Retrieval-Augmented Generation (RAG) approaches often borrow strategies from NLP, relying on chunking-based indexing and similarity-based retrieval. Chunking results in the loss of coherence between code units and overlooks structural relationships, while similarity-driven methods frequently miss functionally relevant dependencies such as helper functions, classes, or global variables. To address these limitations, we present Hydra, a repository-level code generation framework that treats code as structured code rather than natural language. Our approach introduces (i) a structure-aware indexing strategy that represents repositories as hierarchical trees of functions, classes, and variables, preserving code structure and dependencies, (ii) a lightweight dependency-aware retriever (DAR) that explicitly identifies and retrieves the true dependencies required by a target function, and (iii) a hybrid retrieval mechanism that combines DAR with similarity-based retrieval to provide both essential building blocks and practical usage examples. Extensive experiments on the challenging DevEval and RepoExec benchmarks, both requiring function implementation from real-world repositories with complex large repository context, show that Hydra achieves state-of-the-art performance across open- and closed-source CodeLLMs. Notably, our method establishes a new state of the art in repository-level code generation, surpassing strongest baseline by over 5% in Pass@1 and even enabling smaller models to match or exceed the performance of much larger ones that rely on existing retrievers.

</details>


### [21] [WebTestPilot: Agentic End-to-End Web Testing against Natural Language Specification by Inferring Oracles with Symbolized GUI Elements](https://arxiv.org/abs/2602.11724)
*Xiwen Teoh,Yun Lin,Duc-Minh Nguyen,Ruofei Ren,Wenjie Zhang,Jin Song Dong*

Main category: cs.SE

TL;DR: WebTestPilot是一个基于LLM的代理，通过符号化层将GUI元素转化为符号变量，并将自然语言需求转化为带预/后条件的步骤序列，解决了VLM代理在端到端Web测试中的幻觉问题，实现了高精度的bug检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的Web测试方法面临两个核心挑战：1) 隐式oracle推理挑战 - 代理需要在没有明确指导的情况下判断应用行为是否正确；2) 概率推理挑战 - LLM的不一致推理削弱了其作为oracle的可信度。现有方法要么将不崩溃的页面导航视为成功，要么孤立检查每个状态，无法捕获依赖上下文的bug。

Method: WebTestPilot采用两个关键技术：1) 符号化层 - 检测并将关键GUI元素符号化为变量；2) 将自然语言规范转化为带预条件和后条件的步骤序列作为oracle。这个oracle捕获数据、时间和因果依赖关系，能够验证隐式需求。同时构建了bug注入的Web应用基准用于评估。

Result: WebTestPilot实现了99%的任务完成率，bug检测精度达到96%，召回率96%，显著优于最佳基线（精度提升70%，召回率提升27%）。该代理能够泛化到不同的自然语言输入和模型规模。

Conclusion: WebTestPilot通过符号化和条件推理有效解决了VLM代理在Web测试中的幻觉问题，实现了高精度的端到端测试，为自然语言驱动的Web测试提供了可靠的解决方案。

Abstract: Visual language model (VLM) agents show great promise in automating end-to-end (E2E) web testing against requirements in natural language. However, the probabilistic nature of language models can have inherent hallucinations. Therefore, given a detected inconsistency between the requirement and the web application, it is hard to distinguish whether it stems from the hallucination or a real application bug. Addressing this issue presents two core technical challenges: the implicit oracle inference challenge, where the agent must act as its own oracle to implicitly decide if the application's behavior is correct without guidance, and the probabilistic inference challenge, where an LLM's inconsistent reasoning undermines its trustworthiness as an oracle. Existing LLM-based approaches fail to capture such implicit oracles, either by treating any page navigation that doesn't crash as a success, or by checking each state in isolation, thus missing bugs dependent on context from prior steps.
  We introduce WebTestPilot, an LLM-based agent designed to address these challenges. WebTestPilot uses (1) a symbolization layer which detects and symbolizes critical GUI elements on the web application into symbols (i.e., variables) and (2) translates natural language specification into a sequence of steps, each of which is equipped with inferred pre- and post-conditions over the symbols as an oracle. This oracle captures data, temporal, and causal dependencies, enabling the validation of implicit requirements. To advance research in this area, we build a benchmark of bug-injected web apps for evaluating NL-to-E2E testing. The results show that WebTestPilot achieves a task completion rate of 99%, with 96% precision and 96% recall in bug detection, outperforming the best baseline (+70 precision, +27 recall). The agent generalizes across diverse natural language inputs and model scales.

</details>


### [22] [AmbiBench: Benchmarking Mobile GUI Agents Beyond One-Shot Instructions in the Wild](https://arxiv.org/abs/2602.11750)
*Jiazheng Sun,Mingxuan Li,Yingying Zhang,Jiayang Niu,Yachen Wu,Ruihan Jin,Shuyu Lei,Pengrongrui Tan,Zongyu Zhang,Ruoyi Wang,Jiachen Yang,Boyu Yang,Jiacheng Liu,Xin Peng*

Main category: cs.SE

TL;DR: AmbiBench是首个引入指令清晰度分类的移动GUI代理基准，从单向指令执行转向双向意图对齐评估，并开发了MUSE自动化评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准假设用户指令完整明确，仅评估单轮执行能力，忽视了代理在模糊指令下的意图对齐能力。实际场景中用户指令通常不完整且模糊，需要代理通过主动澄清和交互来理解用户真实意图。

Method: 基于认知差距理论提出四种清晰度级别（详细、标准、不完整、模糊），构建包含240个生态有效任务的严格数据集，开发MUSE自动化评估框架（基于MLLM-as-a-judge的多代理架构），从结果有效性、执行质量和交互质量三个维度进行细粒度评估。

Result: 揭示了最先进代理在不同清晰度级别下的性能边界，量化了主动交互带来的收益，验证了MUSE评估结果与人工判断之间的强相关性。

Conclusion: 该工作重新定义了评估标准，为下一代真正理解用户意图的代理奠定了基础，推动了从单向指令执行到双向意图对齐的范式转变。

Abstract: Benchmarks are paramount for gauging progress in the domain of Mobile GUI Agents. In practical scenarios, users frequently fail to articulate precise directives containing full task details at the onset, and their expressions are typically ambiguous. Consequently, agents are required to converge on the user's true intent via active clarification and interaction during execution. However, existing benchmarks predominantly operate under the idealized assumption that user-issued instructions are complete and unequivocal. This paradigm focuses exclusively on assessing single-turn execution while overlooking the alignment capability of the agent. To address this limitation, we introduce AmbiBench, the first benchmark incorporating a taxonomy of instruction clarity to shift evaluation from unidirectional instruction following to bidirectional intent alignment. Grounded in Cognitive Gap theory, we propose a taxonomy of four clarity levels: Detailed, Standard, Incomplete, and Ambiguous. We construct a rigorous dataset of 240 ecologically valid tasks across 25 applications, subject to strict review protocols. Furthermore, targeting evaluation in dynamic environments, we develop MUSE (Mobile User Satisfaction Evaluator), an automated framework utilizing an MLLM-as-a-judge multi-agent architecture. MUSE performs fine-grained auditing across three dimensions: Outcome Effectiveness, Execution Quality, and Interaction Quality. Empirical results on AmbiBench reveal the performance boundaries of SoTA agents across different clarity levels, quantify the gains derived from active interaction, and validate the strong correlation between MUSE and human judgment. This work redefines evaluation standards, laying the foundation for next-generation agents capable of truly understanding user intent.

</details>


### [23] [Improving Code Generation via Small Language Model-as-a-judge](https://arxiv.org/abs/2602.11911)
*Giuseppe Crupi,Rosalia Tufano,Gabriele Bavota*

Main category: cs.SE

TL;DR: 本文研究了使用小型语言模型（SLMs）作为代码正确性判断器，发现现代SLMs在区分正确与错误代码实现方面优于现有方法，且无需执行信息，能以低成本达到大型LLMs 5-25倍参数量的竞争性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究（如RankEF）使用T5模型结合执行和非执行信息来排名代码解决方案，但未评估其分类准确性，且实验基于较旧模型。这留下两个问题：1）语言模型作为代码正确性判断器的可靠性如何？2）这种方法能否帮助公司低成本训练性能接近大型LLMs的代码生成器？

Method: 训练多个最先进的小型语言模型（SLMs）作为代码正确性判断器，评估它们区分正确与错误代码实现的能力。将现代SLMs与RankEF方法进行比较，并测试它们作为代码排名器的性能。

Result: 现代SLMs在代码正确性判断任务上优于RankEF，即使不使用执行信息。作为代码排名器时，它们比RankEF获得更高的性能提升，且仅以低成本就能达到参数量大5-25倍的大型LLMs的竞争性能。

Conclusion: 现代小型语言模型可以作为可靠的代码正确性判断器，在代码生成任务中能以低成本替代大型语言模型，为公司提供经济高效的代码生成解决方案。

Abstract: Large language models (LLMs) have shown remarkable capabilities in automated code generation. While effective for mainstream languages, they may underperform on less common or domain-specific languages, prompting companies to develop in-house code generators. While open-source models can be trained for this, only LLMs with tens of billions of parameters match the performance of commercial tools, demanding costly training and deployment. Recent work proposed supporting code generation with smaller models (SLMs) by generating multiple candidate solutions and using another SLM to select the most likely correct one. The most recent work in this area is the one by Sun et al. [29] presenting RankEF, a T5 model trained to rank code solutions using both execution-based and non-execution-based information. However, Sun et al. do not assess the T5 ranker's classification accuracy, that is, how often it misjudges correct implementations as incorrect or vice versa, leaving open questions about the reliability of LMs as code correctness judges for other tasks (e.g., automated code review). Moreover, their experiments involve relatively old models, making it unclear the extent to which such a methodology would still help companies in cheaply training their own code generators with performance comparable to those of massive LLMs. We present a study addressing these limitations. We train several state-of-the-art SLMs as code correctness judges and assess their ability to discriminate between correct and wrong implementations. We show that modern SLMs outperform RankEF, even without exploiting execution-based information. When used as code rankers, they achieve higher performance gains than RankEF and perform competitively with LLMs 5-25x larger, at a fraction of the cost.

</details>


### [24] [Studying Quality Improvements Recommended via Manual and Automated Code Review](https://arxiv.org/abs/2602.11925)
*Giuseppe Crupi,Rosalia Tufano,Gabriele Bavota*

Main category: cs.SE

TL;DR: 研究比较人类与ChatGPT-4在代码审查中的表现，发现AI能发现更多问题但仅能识别10%的人类发现的质量问题，两者具有互补性。


<details>
  <summary>Details</summary>
Motivation: 虽然已有基于深度学习的代码审查自动化技术，但尚不清楚这些方法能否像人类审查者一样推荐质量改进。本研究旨在探究人类与AI代码审查的异同。

Method: 采用挖掘研究方法：收集240个PR中人类审查者的739条评论并手动分类质量改进类型；然后让ChatGPT审查相同PR，比较其推荐的质量改进与人类审查者的差异。

Result: ChatGPT平均推荐比人类多约2.4倍的代码更改，但仅能发现10%的人类报告的质量问题；约40%的LLM额外评论指向有意义的质问题。

Conclusion: 人类与AI代码审查具有互补性。当前基于DL的代码审查可作为人类审查的额外质量检查，但不能替代人类审查或节省审查时间，因为人类仍需进行手动检查并验证AI报告的问题。

Abstract: Several Deep Learning (DL)-based techniques have been proposed to automate code review. Still, it is unclear the extent to which these approaches can recommend quality improvements as a human reviewer. We study the similarities and differences between code reviews performed by humans and those automatically generated by DL models, using ChatGPT-4 as representative of the latter. In particular, we run a mining-based study in which we collect and manually inspect 739 comments posted by human reviewers to suggest code changes in 240 PRs. The manual inspection aims at classifying the type of quality improvement recommended by human reviewers (e.g., rename variable/constant). Then, we ask ChatGPT to perform a code review on the same PRs and we compare the quality improvements it recommends against those suggested by the human reviewers. We show that while, on average, ChatGPT tends to recommend a higher number of code changes as compared to human reviewers (~2.4x more), it can only spot 10% of the quality issues reported by humans. However, ~40% of the additional comments generated by the LLM point to meaningful quality issues. In short, our findings show the complementarity of manual and AI-based code review. This finding suggests that, in its current state, DL-based code review can be used as a further quality check on top of the one performed by humans, but should not be considered as a valid alternative to them nor as a mean to save code review time, since human reviewers would still need to perform their manual inspection while also validating the quality issues reported by the DL-based technique.

</details>


### [25] [Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?](https://arxiv.org/abs/2602.11988)
*Thibaud Gloaguen,Niels Mündler,Mark Müller,Veselin Raychev,Martin Vechev*

Main category: cs.SE

TL;DR: 研究发现，为代码代理提供上下文文件（如AGENTS.md）反而会降低任务成功率，同时增加推理成本，建议人类编写的上下文文件应仅描述最低要求


<details>
  <summary>Details</summary>
Motivation: 软件开发中普遍存在为代码代理定制上下文文件的做法，但缺乏对其实效性的严谨研究。本研究旨在探究上下文文件是否真正有助于现实世界任务完成

Method: 在两个互补设置中评估代码代理的任务完成性能：1) 使用LLM生成的上下文文件评估SWE-bench任务；2) 使用包含开发者提交的上下文文件的新问题集合

Result: 上下文文件相比不提供仓库上下文会降低任务成功率，同时增加超过20%的推理成本。上下文文件鼓励更广泛的探索行为，但代码代理倾向于遵循其指令

Conclusion: 上下文文件中的不必要要求使任务变得更难，人类编写的上下文文件应仅描述最低要求

Abstract: A widespread practice in software development is to tailor coding agents to repositories using context files, such as AGENTS.md, by either manually or automatically generating them. Although this practice is strongly encouraged by agent developers, there is currently no rigorous investigation into whether such context files are actually effective for real-world tasks. In this work, we study this question and evaluate coding agents' task completion performance in two complementary settings: established SWE-bench tasks from popular repositories, with LLM-generated context files following agent-developer recommendations, and a novel collection of issues from repositories containing developer-committed context files.
  Across multiple coding agents and LLMs, we find that context files tend to reduce task success rates compared to providing no repository context, while also increasing inference cost by over 20%. Behaviorally, both LLM-generated and developer-provided context files encourage broader exploration (e.g., more thorough testing and file traversal), and coding agents tend to respect their instructions. Ultimately, we conclude that unnecessary requirements from context files make tasks harder, and human-written context files should describe only minimal requirements.

</details>


### [26] [On the Adoption of AI Coding Agents in Open-source Android and iOS Development](https://arxiv.org/abs/2602.12144)
*Muhammad Ahmad Khan,Hasnain Ali,Muneeb Rana,Muhammad Saqib Ilyas,Abdul Ali Bangash*

Main category: cs.SE

TL;DR: 首项针对开源移动项目中AI代理生成代码的类别级实证研究，分析了2,901个AI编写的PR在193个Android和iOS项目中的接受行为，发现Android项目接受率更高，不同任务类别接受率差异显著。


<details>
  <summary>Details</summary>
Motivation: AI编码代理在软件开发中作用日益重要，但其对移动开发的影响缺乏实证研究。需要了解AI代理在移动开源项目中的实际贡献效果，为设计平台感知的代理系统提供依据。

Method: 使用AIDev数据集中193个已验证的Android和iOS开源GitHub仓库，分析2,901个AI编写的pull request（PR）。研究跨移动平台、AI代理和任务类别的PR接受行为，包括接受率、解决时间等指标，并进行演化分析。

Result: Android项目收到2倍多的AI编写PR，接受率71%高于iOS的63%，Android上不同代理表现差异显著。常规任务（功能、修复、UI）PR接受率最高，重构和构建等结构性变更接受率较低且解决时间更长。Android项目PR解决时间在2025年中前改善后再次下降。

Conclusion: 首次基于证据描述了AI代理对开源移动项目的影响，为评估代理生成贡献建立了实证基准。研究结果有助于设计平台感知的代理系统，理解不同任务类型和平台对AI代码接受的影响。

Abstract: AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We find that Android projects have received 2x more AI-authored PRs and have achieved higher PR acceptance rate (71%) than iOS (63%), with significant agent-level variation on Android. Across task categories, PRs with routine tasks (feature, fix, and ui) achieve the highest acceptance, while structural changes like refactor and build achieve lower success and longer resolution times. Furthermore, our evolution analysis shows improvement in PR resolution time on Android through mid-2025 before it declined again. Our findings offer the first evidence-based characterization of AI agents effects on OSS mobile projects and establish empirical baselines for evaluating agent-generated contributions to design platform aware agentic systems.

</details>


### [27] [Automated Test Suite Enhancement Using Large Language Models with Few-shot Prompting](https://arxiv.org/abs/2602.12256)
*Alex Chudic,Gül Çalıklı*

Main category: cs.SE

TL;DR: 论文研究few-shot提示对LLM生成单元测试质量的影响，比较了人类、SBST和LLM三种测试样本来源的效果，发现人类编写的示例能产生最佳覆盖率和正确性。


<details>
  <summary>Details</summary>
Motivation: 单元测试对代码功能验证至关重要，但手动编写耗时费力。传统工具生成的测试缺乏可读性和实用性，而LLM在单元测试生成中的few-shot学习潜力尚未充分探索。当前代码库中混合了人类、LLM和传统工具生成的测试，需要研究如何通过few-shot提示提升测试质量。

Method: 使用GPT-4o在HumanEval和ClassEval数据集上进行实验，比较不同测试样本来源（人类、SBST、LLM）对few-shot提示效果的影响。评估了基于问题描述和代码相似度的检索方法选择相关示例，并综合评估测试的正确性、覆盖率、可读性、认知复杂度和可维护性。

Result: LLM通过few-shot提示可以生成高质量测试，其中人类编写的示例能产生最佳覆盖率和正确性。基于问题描述和代码相似度组合选择示例的方法能产生最有效的few-shot提示。

Conclusion: few-shot提示能有效提升LLM生成单元测试的质量，人类示例是最佳参考来源，而智能检索相关示例能进一步优化提示效果，这对混合人机代码库的测试质量提升具有重要意义。

Abstract: Unit testing is essential for verifying the functional correctness of code modules (e.g., classes, methods), but manually writing unit tests is often labor-intensive and time-consuming. Unit tests generated by tools that employ traditional approaches, such as search-based software testing (SBST), lack readability, naturalness, and practical usability. LLMs have recently provided promising results and become integral to developers' daily practices. Consequently, software repositories now include a mix of human-written tests, LLM-generated tests, and those from tools employing traditional approaches such as SBST. While LLMs' zero-shot capabilities have been widely studied, their few-shot learning potential for unit test generation remains underexplored. Few-shot prompting enables LLMs to learn from examples in the prompt, and automatically retrieving such examples could enhance test suites. This paper empirically investigates how few-shot prompting with different test artifact sources, comprising human, SBST, or LLM, affects the quality of LLM-generated unit tests as program comprehension artifacts and their contribution to improving existing test suites by evaluating not only correctness and coverage but also readability, cognitive complexity, and maintainability in hybrid human-AI codebases. We conducted experiments on HumanEval and ClassEval datasets using GPT-4o, which is integrated into GitHub Copilot and widely used among developers. We also assessed retrieval-based methods for selecting relevant examples. Our results show that LLMs can generate high-quality tests via few-shot prompting, with human-written examples producing the best coverage and correctness. Additionally, selecting examples based on the combined similarity of problem description and code consistently yields the most effective few-shot prompts.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [28] [The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates](https://arxiv.org/abs/2602.11301)
*John M. Willis*

Main category: cs.AI

TL;DR: 论文提出了PBSAI（安全AI实践者蓝图）治理生态系统，这是一个用于保护企业和超大规模AI资产的多智能体参考架构，包含12个责任域和智能体家族，通过上下文信封和结构化输出合同实现安全治理。


<details>
  <summary>Details</summary>
Motivation: 企业正在大规模部署LLM、RAG管道和工具使用智能体，这些系统形成了复杂的AI资产（socio-technical systems），但现有的治理和安全框架（如NIST AI RMF）缺乏可实施的多智能体架构来保护这些AI资产。

Method: 提出了PBSAI治理生态系统，包含：1）12个责任域的分类法；2）定义有界的智能体家族，通过共享上下文信封和结构化输出合同在工具与策略之间进行协调；3）轻量级的形式化模型，描述智能体、上下文信封和生态系统级不变性；4）与NIST AI RMF功能对齐。

Result: PBSAI架构展示了与企业SOC和超大规模防御环境的对齐，提供了可追溯性、来源追踪和人机协同的保证，作为开放生态系统开发和未来实证验证的结构化、以证据为中心的基础。

Conclusion: PBSAI为保护企业AI资产提供了一个可实施的多智能体参考架构，填补了现有治理框架的实践空白，支持系统性安全治理和未来实证研究。

Abstract: Enterprises are rapidly deploying large language models, retrieval augmented generation pipelines, and tool using agents into production, often on shared high performance computing clusters and cloud accelerator platforms that also support defensive analytics. These systems increasingly function not as isolated models but as AI estates: socio technical systems spanning models, agents, data pipelines, security tooling, human workflows, and hyperscale infrastructure. Existing governance and security frameworks, including the NIST AI Risk Management Framework and systems security engineering guidance, articulate principles and risk functions but do not provide implementable architectures for multi agent, AI enabled cyber defense.
  This paper introduces the Practitioners Blueprint for Secure AI (PBSAI) Governance Ecosystem, a multi agent reference architecture for securing enterprise and hyperscale AI estates. PBSAI organizes responsibilities into a twelve domain taxonomy and defines bounded agent families that mediate between tools and policy through shared context envelopes and structured output contracts. The architecture assumes baseline enterprise security capabilities and encodes key systems security techniques, including analytic monitoring, coordinated defense, and adaptive response. A lightweight formal model of agents, context envelopes, and ecosystem level invariants clarifies the traceability, provenance, and human in the loop guarantees enforced across domains. We demonstrate alignment with NIST AI RMF functions and illustrate application in enterprise SOC and hyperscale defensive environments. PBSAI is proposed as a structured, evidence centric foundation for open ecosystem development and future empirical validation.

</details>


### [29] [AgentNoiseBench: Benchmarking Robustness of Tool-Using LLM Agents Under Noisy Condition](https://arxiv.org/abs/2602.11348)
*Ruipeng Wang,Yuxin Chen,Yukai Wang,Chang Wu,Junfeng Fang,Xiaodong Cai,Qi Gu,Hui Su,An Zhang,Xiang Wang,Xunliang Cai,Tat-Seng Chua*

Main category: cs.AI

TL;DR: AgentNoiseBench：一个评估智能体模型在噪声环境中鲁棒性的框架，通过分析真实世界噪声类型并注入可控噪声到现有基准中，发现当前智能体模型对现实环境扰动敏感。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在基准测试中表现良好，但在现实部署中性能下降，主要因为现有训练和评估范式基于理想化假设，忽略了真实世界交互中的随机性和噪声。

Method: 1) 深入分析真实场景中的偏差和不确定性，将环境噪声分为用户噪声和工具噪声两类；2) 开发自动化流水线，在保持任务可解性的前提下向现有智能体基准注入可控噪声；3) 在不同架构和参数规模的模型上进行广泛评估。

Result: 在不同噪声条件下观察到一致的性能变化，突显了当前智能体模型对现实环境扰动的敏感性。

Conclusion: 需要更鲁棒的智能体模型来应对真实世界中的噪声环境，AgentNoiseBench为系统评估模型鲁棒性提供了框架。

Abstract: Recent advances in large language models have enabled LLM-based agents to achieve strong performance on a variety of benchmarks. However, their performance in real-world deployments often that observed on benchmark settings, especially in complex and imperfect environments. This discrepancy largely arises because prevailing training and evaluation paradigms are typically built on idealized assumptions, overlooking the inherent stochasticity and noise present in real-world interactions. To bridge this gap, we introduce AgentNoiseBench, a framework for systematically evaluating the robustness of agentic models under noisy environments. We first conduct an in-depth analysis of biases and uncertainties in real-world scenarios and categorize environmental noise into two primary types: user-noise and tool-noise. Building on this analysis, we develop an automated pipeline that injects controllable noise into existing agent-centric benchmarks while preserving task solvability. Leveraging this pipeline, we perform extensive evaluations across a wide range of models with diverse architectures and parameter scales. Our results reveal consistent performance variations under different noise conditions, highlighting the sensitivity of current agentic models to realistic environmental perturbations.

</details>


### [30] [Pushing Forward Pareto Frontiers of Proactive Agents with Behavioral Agentic Optimization](https://arxiv.org/abs/2602.11351)
*Yihang Yao,Zhepeng Cen,Haohong Lin,Shiqi Liu,Zuxin Liu,Jiacheng Zhu,Zhang-Wei Hong,Laixi Shi,Ding Zhao*

Main category: cs.AI

TL;DR: BAO是一个结合行为增强和行为正则化的智能体强化学习框架，用于训练主动型LLM智能体，在任务性能和用户参与度之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有主动型LLM智能体训练方法面临任务性能与用户参与度之间的平衡难题：被动智能体无法有效适应用户意图，而过度使用人类反馈会降低用户满意度。

Method: 提出BAO框架，结合行为增强（丰富主动推理和信息收集能力）和行为正则化（抑制低效或冗余交互，使智能体行为与用户期望对齐）。

Result: 在UserRL基准套件的多个任务上，BAO显著优于主动型智能体RL基线，性能与商业LLM智能体相当甚至更优。

Conclusion: BAO框架能有效训练在复杂多轮场景中主动且用户对齐的LLM智能体，平衡任务完成效率和用户参与度。

Abstract: Proactive large language model (LLM) agents aim to actively plan, query, and interact over multiple turns, enabling efficient task completion beyond passive instruction following and making them essential for real-world, user-centric applications. Agentic reinforcement learning (RL) has recently emerged as a promising solution for training such agents in multi-turn settings, allowing interaction strategies to be learned from feedback. However, existing pipelines face a critical challenge in balancing task performance with user engagement, as passive agents can not efficiently adapt to users' intentions while overuse of human feedback reduces their satisfaction. To address this trade-off, we propose BAO, an agentic RL framework that combines behavior enhancement to enrich proactive reasoning and information-gathering capabilities with behavior regularization to suppress inefficient or redundant interactions and align agent behavior with user expectations. We evaluate BAO on multiple tasks from the UserRL benchmark suite, and demonstrate that it substantially outperforms proactive agentic RL baselines while achieving comparable or even superior performance to commercial LLM agents, highlighting its effectiveness for training proactive, user-aligned LLM agents in complex multi-turn scenarios. Our website: https://proactive-agentic-rl.github.io/.

</details>


### [31] [ReplicatorBench: Benchmarking LLM Agents for Replicability in Social and Behavioral Sciences](https://arxiv.org/abs/2602.11354)
*Bang Nguyen,Dominik Soós,Qian Ma,Rochana R. Obadage,Zack Ranjan,Sai Koneru,Timothy M. Errington,Shakhlo Nematova,Sarah Rajtmajer,Jian Wu,Meng Jiang*

Main category: cs.AI

TL;DR: 提出了ReplicatorBench基准测试，用于评估AI代理在科研复现任务中的能力，包括可复现和不可复现的研究声明，并开发了ReplicatorAgent作为基线系统。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注计算方面，只测试可复现论文，缺乏对不可复现研究的识别能力评估，且只评估结果而非复现过程。需要更全面的基准来评估AI代理在真实科研复现中的能力。

Method: 构建ReplicatorBench基准，包含社会科学和行为科学中经过人工验证的可复现和不可复现研究声明，涵盖三个评估阶段：数据提取与检索、计算实验设计与执行、结果解释。开发ReplicatorAgent框架，配备网络搜索和沙箱环境交互等工具。

Result: 评估了基于四个大语言模型的ReplicatorAgent，发现当前LLM代理能有效设计和执行计算实验，但在检索新数据等必要资源方面存在困难。不同编程语言和代码访问级别的设计选择影响性能。

Conclusion: ReplicatorBench提供了更全面的AI代理科研复现能力评估框架，揭示了当前LLM代理在资源检索方面的局限性，为未来研究提供了基准和方向。

Abstract: The literature has witnessed an emerging interest in AI agents for automated assessment of scientific papers. Existing benchmarks focus primarily on the computational aspect of this task, testing agents' ability to reproduce or replicate research outcomes when having access to the code and data. This setting, while foundational, (1) fails to capture the inconsistent availability of new data for replication as opposed to reproduction, and (2) lacks ground-truth diversity by focusing only on reproducible papers, thereby failing to evaluate an agent's ability to identify non-replicable research. Furthermore, most benchmarks only evaluate outcomes rather than the replication process. In response, we introduce ReplicatorBench, an end-to-end benchmark, including human-verified replicable and non-replicable research claims in social and behavioral sciences for evaluating AI agents in research replication across three stages: (1) extraction and retrieval of replication data; (2) design and execution of computational experiments; and (3) interpretation of results, allowing a test of AI agents' capability to mimic the activities of human replicators in real world. To set a baseline of AI agents' capability, we develop ReplicatorAgent, an agentic framework equipped with necessary tools, like web search and iterative interaction with sandboxed environments, to accomplish tasks in ReplicatorBench. We evaluate ReplicatorAgent across four underlying large language models (LLMs), as well as different design choices of programming language and levels of code access. Our findings reveal that while current LLM agents are capable of effectively designing and executing computational experiments, they struggle with retrieving resources, such as new data, necessary to replicate a claim. All code and data are publicly available at https://github.com/CenterForOpenScience/llm-benchmarking.

</details>


### [32] [TRACER: Trajectory Risk Aggregation for Critical Episodes in Agentic Reasoning](https://arxiv.org/abs/2602.11409)
*Sina Tayebati,Divake Kumar,Nastaran Darabi,Davide Ettori,Ranganath Krishnan,Amit Ranjan Trivedi*

Main category: cs.AI

TL;DR: TRACER是一种轨迹级不确定性度量方法，用于检测AI代理在多轮工具使用交互中的关键故障，通过结合内容感知、情境感知信号和工具一致性差距来预测任务失败。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性代理主要关注单次文本生成，无法捕捉轨迹级故障信号（如循环、工具使用不连贯、用户-代理协调失误），而这些稀疏关键事件往往导致实际交互失败。

Method: TRACER结合内容感知惊奇度、情境感知信号、语义和词汇重复、工具一致性差距，使用尾部聚焦风险函数和MAX复合步骤风险来聚合这些信号，以识别决定性异常。

Result: 在τ²-bench上评估，TRACER在预测任务失败和选择性任务执行方面，AUROC提升高达37.1%，AUARC提升高达55%，优于基线方法，能更早更准确地检测复杂对话工具使用场景中的不确定性。

Conclusion: TRACER通过轨迹级不确定性度量有效捕捉多轮工具使用交互中的关键故障信号，显著提升任务失败预测和选择性执行的性能，为AI代理在复杂对话环境中的可靠性评估提供了新方法。

Abstract: Estimating uncertainty for AI agents in real-world multi-turn tool-using interaction with humans is difficult because failures are often triggered by sparse critical episodes (e.g., looping, incoherent tool use, or user-agent miscoordination) even when local generation appears confident. Existing uncertainty proxies focus on single-shot text generation and therefore miss these trajectory-level breakdown signals. We introduce TRACER, a trajectory-level uncertainty metric for dual-control Tool-Agent-User interaction. TRACER combines content-aware surprisal with situational-awareness signals, semantic and lexical repetition, and tool-grounded coherence gaps, and aggregates them using a tail-focused risk functional with a MAX-composite step risk to surface decisive anomalies. We evaluate TRACER on $τ^2$-bench by predicting task failure and selective task execution. To this end, TRACER improves AUROC by up to 37.1% and AUARC by up to 55% over baselines, enabling earlier and more accurate detection of uncertainty in complex conversational tool-use settings. Our code and benchmark are available at https://github.com/sinatayebati/agent-tracer.

</details>


### [33] [Distributionally Robust Cooperative Multi-Agent Reinforcement Learning via Robust Value Factorization](https://arxiv.org/abs/2602.11437)
*Chengrui Qu,Christopher Yeh,Kishan Panaganti,Eric Mazumdar,Adam Wierman*

Main category: cs.AI

TL;DR: 提出分布鲁棒IGM原则，将鲁棒性引入多智能体强化学习的价值分解方法中，以应对环境不确定性带来的挑战


<details>
  <summary>Details</summary>
Motivation: 现实世界中的环境不确定性（如仿真到现实的差距、模型不匹配、系统噪声）使得传统集中训练分散执行方法的可靠性不足，需要引入鲁棒性保证

Method: 提出分布鲁棒IGM原则，定义鲁棒个体动作价值函数，并推导出与DrIGM兼容的鲁棒变体价值分解架构（如VDN/QMIX/QTRAN），在鲁棒Q目标上训练

Result: 在高保真SustainGym仿真器和星际争霸游戏环境中，该方法一致提高了分布外性能表现

Conclusion: DrIGM原则为多智能体强化学习提供了可证明的鲁棒性保证，兼容现有价值分解架构，无需定制化的个体奖励塑造，在实际应用中具有重要价值

Abstract: Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution, where value-factorization methods enforce the individual-global-maximum (IGM) principle so that decentralized greedy actions recover the team-optimal joint action. However, the reliability of this recipe in real-world settings remains unreliable due to environmental uncertainties arising from the sim-to-real gap, model mismatch, and system noise. We address this gap by introducing Distributionally robust IGM (DrIGM), a principle that requires each agent's robust greedy action to align with the robust team-optimal joint action. We show that DrIGM holds for a novel definition of robust individual action values, which is compatible with decentralized greedy execution and yields a provable robustness guarantee for the whole system. Building on this foundation, we derive DrIGM-compliant robust variants of existing value-factorization architectures (e.g., VDN/QMIX/QTRAN) that (i) train on robust Q-targets, (ii) preserve scalability, and (iii) integrate seamlessly with existing codebases without bespoke per-agent reward shaping. Empirically, on high-fidelity SustainGym simulators and a StarCraft game environment, our methods consistently improve out-of-distribution performance. Code and data are available at https://github.com/crqu/robust-coMARL.

</details>


### [34] [Credit Where It is Due: Cross-Modality Connectivity Drives Precise Reinforcement Learning for MLLM Reasoning](https://arxiv.org/abs/2602.11455)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.AI

TL;DR: 论文提出Anchor-Token Reinforcement Learning (AT-RL)框架，通过选择性强化高连接性视觉锚点token来提升多模态大语言模型的推理能力，仅增加1.2%开销却能让32B模型超越72B基准。


<details>
  <summary>Details</summary>
Motivation: 尽管可验证奖励的强化学习(RLVR)显著提升了多模态大语言模型的推理能力，但视觉证据在推理过程中如何被整合仍然缺乏深入理解。研究者希望探索多模态RLVR中视觉-文本的跨模态注意力连接机制。

Method: 通过分析跨模态注意力连接性，发现只有约15%的token具有强视觉-文本耦合，这些高连接性token作为视觉锚点。基于此提出AT-RL框架，使用基于图的注意力拓扑聚类方法，选择性强化这些高连接性锚点token。

Result: AT-RL仅引入1.2%的开销，但使32B模型在MathVista上达到80.2分，超越了72B-Instruct基准。在STEM、视频和通用任务上均观察到一致提升。相反，仅训练低连接性token会导致严重性能下降。

Conclusion: 推理质量不是由token数量决定，而是由跨模态锚点的保真度决定。有效的多模态强化学习依赖于对视觉锚点的精确信用分配，高连接性锚点token在多模态推理中起到关键作用。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), yet how visual evidence is integrated during reasoning remains poorly understood. We explore multimodal RLVR through the lens of cross-modal attention connectivity and find that only a small fraction of tokens (approximately 15%) exhibit strong visual-textual coupling. These high-connectivity tokens act as anchors that ground reasoning in the image, while the majority follow linguistic patterns. During RLVR training, credit assignment naturally concentrates on these anchors, sharpening their visual grounding over time. Building on this insight, we propose Anchor-Token Reinforcement Learning (AT-RL), a lightweight framework that selectively reinforces high-connectivity tokens via graph-based clustering of attention topology. Evaluated across the series (3B-32B), AT-RL introduces only 1.2% overhead yet enables the 32B model to surpass the 72B-Instruct baseline on MathVista (80.2), with consistent gains observed across STEM, video and general tasks. Conversely, training solely on low-connectivity tokens causes severe degradation, confirming that effective multimodal RL hinges on precise credit assignment to visual anchors. Our work reveals that reasoning quality is governed not by token quantity but by the fidelity of cross-modal anchoring.

</details>


### [35] [AgentLeak: A Full-Stack Benchmark for Privacy Leakage in Multi-Agent LLM Systems](https://arxiv.org/abs/2602.11510)
*Faouzi El Yagoubi,Ranwa Al Mallah,Godwin Badu-Marfo*

Main category: cs.AI

TL;DR: AgentLeak是首个针对多智能体LLM系统隐私泄露的全栈基准测试，覆盖内部通信通道，发现多智能体配置虽然降低单通道输出泄露，但内部通道（特别是智能体间消息）泄露率高达68.8%，导致输出审计会错过41.7%的违规。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试无法测量多智能体LLM系统的隐私风险，因为敏感数据会通过智能体间消息、共享内存和工具参数等内部通道传递，这些通道是输出审计无法检查的。

Method: 引入AgentLeak基准测试，包含1000个医疗、金融、法律和企业领域的场景，配备32类攻击分类法和三层检测流水线，测试GPT-4o、GPT-4o-mini、Claude 3.5 Sonnet、Mistral Large和Llama 3.3 70B等模型在4979条轨迹上的表现。

Result: 多智能体配置降低单通道输出泄露（27.2% vs 单智能体43.2%），但内部通道使系统总暴露率达到68.9%。智能体间消息泄露率68.8%，远高于输出通道的27.2%。Claude 3.5 Sonnet在外部（3.3%）和内部（28.1%）通道上泄露率最低。

Conclusion: 智能体间通信是主要漏洞，需要协调框架整合内部通道隐私保护，并在智能体间通信上强制执行隐私控制。

Abstract: Multi-agent Large Language Model (LLM) systems create privacy risks that current benchmarks cannot measure. When agents coordinate on tasks, sensitive data passes through inter-agent messages, shared memory, and tool arguments; pathways that output-only audits never inspect. We introduce AgentLeak, to the best of our knowledge the first full-stack benchmark for privacy leakage covering internal channels, spanning 1,000 scenarios across healthcare, finance, legal, and corporate domains, paired with a 32-class attack taxonomy and three-tier detection pipeline. Testing GPT-4o, GPT-4o-mini, Claude 3.5 Sonnet, Mistral Large, and Llama 3.3 70B across 4,979 traces reveals that multi-agent configurations reduce per-channel output leakage (C1: 27.2% vs 43.2% in single-agent) but introduce unmonitored internal channels that raise total system exposure to 68.9% (OR-aggregated across C1, C2, C5). Internal channels account for most of this gap: inter-agent messages (C2) leak at 68.8%, compared to 27.2% on C1 (output channel). This means that output-only audits miss 41.7% of violations. Claude 3.5 Sonnet, which emphasizes safety alignment in its design, achieves the lowest leakage rates on both external (3.3%) and internal (28.1%) channels, suggesting that model-level safety training may transfer to internal channel protection. Across all five models and four domains, the pattern C2 > C1 holds consistently, confirming that inter-agent communication is the primary vulnerability. These findings underscore the need for coordination frameworks that incorporate internal-channel privacy protections and enforce privacy controls on inter-agent communication.

</details>


### [36] [Learning to Configure Agentic AI Systems](https://arxiv.org/abs/2602.11574)
*Aditya Taparia,Som Sagar,Ransalu Senanayake*

Main category: cs.AI

TL;DR: 提出ARC系统，使用强化学习学习轻量级分层策略，为每个查询动态配置LLM代理系统，相比固定模板显著提升性能并降低计算成本


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理系统配置通常采用固定大型模板或手动调优启发式方法，导致脆弱行为和计算浪费，因为相同繁琐配置同时应用于简单和困难查询

Method: 将代理配置建模为查询级决策问题，引入ARC系统，使用强化学习学习轻量级分层策略，动态调整工作流、工具、token预算和提示等配置

Result: 在多个推理和工具增强问答基准测试中，学习到的策略持续优于强手工设计和其他基线，任务准确率提升高达25%，同时减少token和运行时成本

Conclusion: 学习每个查询的代理配置是"一刀切"设计的强大替代方案，能够实现更好的性能和效率平衡

Abstract: Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to "one size fits all" designs.

</details>


### [37] [MAPLE: Modality-Aware Post-training and Learning Ecosystem](https://arxiv.org/abs/2602.11596)
*Nikhil Verma,Minjung Kim,JooYoung Yoo,Kyung-Min Jin,Manasa Bharadwaj,Kevin Ferreira,Ko Keun Kim,Youngjoon Kim*

Main category: cs.AI

TL;DR: MAPLE是一个模态感知的强化学习后训练生态系统，包含基准测试、策略优化框架和自适应调度，通过减少梯度方差和加速收敛来提升多模态模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL后训练管道将所有输入信号视为同等相关，忽略了不同任务实际需要的模态。这种模态盲训练增加了策略梯度方差、减缓收敛速度，并在真实世界信号缺失、添加或重新加权时降低鲁棒性。

Method: 1) MAPLE-bench：首个标注每个任务所需最小信号组合的基准；2) MAPO：模态感知策略优化框架，按模态需求分层批次以减少异质组优势的梯度方差；3) 自适应加权和课程调度，平衡并优先处理更难的信号组合。

Result: MAPLE将单模态/多模态准确率差距缩小30.24%，收敛速度提升3.18倍，在所有模态组合下保持稳定，特别是在现实减少信号访问的情况下。

Conclusion: MAPLE为部署就绪的多模态RL后训练提供了完整方案，通过模态感知训练解决了现有方法在梯度方差、收敛速度和鲁棒性方面的局限性。

Abstract: Multimodal language models now integrate text, audio, and video for unified reasoning. Yet existing RL post-training pipelines treat all input signals as equally relevant, ignoring which modalities each task actually requires. This modality-blind training inflates policy-gradient variance, slows convergence, and degrades robustness to real-world distribution shifts where signals may be missing, added, or reweighted. We introduce MAPLE, a complete modality-aware post-training and learning ecosystem comprising: (1) MAPLE-bench, the first benchmark explicitly annotating minimal signal combinations required per task; (2) MAPO, a modality-aware policy optimization framework that stratifies batches by modality requirement to reduce gradient variance from heterogeneous group advantages; (3) Adaptive weighting and curriculum scheduling that balances and prioritizes harder signal combinations. Systematic analysis across loss aggregation, clipping, sampling, and curriculum design establishes MAPO's optimal training strategy. Adaptive weighting and curriculum focused learning further boost performance across signal combinations. MAPLE narrows uni/multi-modal accuracy gaps by 30.24%, converges 3.18x faster, and maintains stability across all modality combinations under realistic reduced signal access. MAPLE constitutes a complete recipe for deployment-ready multimodal RL post-training.

</details>


### [38] [When Agents Disagree With Themselves: Measuring Behavioral Consistency in LLM-Based Agents](https://arxiv.org/abs/2602.11619)
*Aman Mehta*

Main category: cs.AI

TL;DR: 研究发现LLM代理在相同任务上存在行为不一致性，这种不一致性与任务失败率高度相关，早期决策（特别是第二步）是行为分化的关键点。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLM代理在执行相同任务时的行为一致性，以及这种一致性如何影响任务成功率。当前LLM代理被广泛部署，但其行为的可预测性和可靠性尚未得到充分研究。

Method: 在三个模型（Llama 3.1 70B、GPT-4o和Claude Sonnet 4.5）上进行了3,000次代理运行实验，使用HotpotQA数据集，采用ReAct风格的代理架构，分析行为序列的变异性和与任务准确率的关系。

Result: 发现代理行为存在显著不一致性：平均每10次运行产生2.0-4.2个不同的动作序列。行为不一致性强烈预测失败：行为一致的任务（≤2个独特路径）准确率达80-92%，而高度不一致的任务（≥6个独特路径）准确率仅25-60%，差距达32-55个百分点。69%的行为分化发生在第二步（首次搜索查询）。

Conclusion: LLM代理的行为不一致性是普遍现象，且与任务失败率密切相关。监控执行期间的行为一致性可以实现早期错误检测，从而提高代理的可靠性。早期决策点（特别是第二步）是行为分化的关键。

Abstract: Run the same LLM agent on the same task twice: do you get the same behavior? We find the answer is often no. In a study of 3,000 agent runs across three models (Llama 3.1 70B, GPT-4o, and Claude Sonnet 4.5) on HotpotQA, we observe that ReAct-style agents produce 2.0--4.2 distinct action sequences per 10 runs on average, even with identical inputs. More importantly, this variance predicts failure: tasks with consistent behavior ($\leq$2 unique paths) achieve 80--92% accuracy, while highly inconsistent tasks ($\geq$6 unique paths) achieve only 25--60%, a 32--55 percentage point gap depending on model. We trace variance to early decisions: 69% of divergence occurs at step 2, the first search query. Our results suggest that monitoring behavioral consistency during execution could enable early error detection and improve agent reliability.

</details>


### [39] [Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs](https://arxiv.org/abs/2602.11729)
*Thomas Jiralerspong,Trenton Bricken*

Main category: cs.AI

TL;DR: 本文提出了一种跨架构模型差异分析方法，通过改进Crosscoders技术来识别不同AI模型之间的行为差异，包括政治倾向、版权拒绝等安全关键特征。


<details>
  <summary>Details</summary>
Motivation: 现有的模型差异分析主要局限于比较基础模型与其微调版本，而新的LLM发布往往是全新架构，因此需要跨架构的方法来广泛应用模型差异分析技术。

Method: 提出专用特征交叉编码器（DFCs），这是Crosscoders的架构改进，旨在更好地隔离特定于某个模型的独特特征，并首次将Crosscoders应用于跨架构模型差异分析。

Result: 在无监督方式下发现了多个模型的特征：Qwen3-8B和Deepseek-R1-0528-Qwen3-8B的中共对齐特征、Llama3.1-8B-Instruct的美国例外主义特征、以及GPT-OSS-20B的版权拒绝机制。

Conclusion: 跨架构交叉编码器模型差异分析是识别AI模型间有意义行为差异的有效方法，为安全关键行为的检测提供了新途径。

Abstract: Model diffing, the process of comparing models' internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM releases are often novel architectures, cross-architecture methods are essential to make model diffing widely applicable. Crosscoders are one solution capable of cross-architecture model diffing but have only ever been applied to base vs finetune comparisons. We provide the first application of crosscoders to cross-architecture model diffing and introduce Dedicated Feature Crosscoders (DFCs), an architectural modification designed to better isolate features unique to one model. Using this technique, we find in an unsupervised fashion features including Chinese Communist Party alignment in Qwen3-8B and Deepseek-R1-0528-Qwen3-8B, American exceptionalism in Llama3.1-8B-Instruct, and a copyright refusal mechanism in GPT-OSS-20B. Together, our results work towards establishing cross-architecture crosscoder model diffing as an effective method for identifying meaningful behavioral differences between AI models.

</details>


### [40] [FlowMind: Execute-Summarize for Structured Workflow Generation from LLM Reasoning](https://arxiv.org/abs/2602.11782)
*Yihao Liu,Ziyun Zhang,Zile He,Huaqian Cai*

Main category: cs.AI

TL;DR: 提出Execute-Summarize框架，将任务执行与工作流构建解耦，先完成任务再根据执行轨迹重构结构化工作流，提高工作流准确性


<details>
  <summary>Details</summary>
Motivation: LLM能够通过推理和工具使用解决复杂任务，但将这些解决方案准确转化为结构化工作流仍然具有挑战性。现有方法在执行过程中构建工作流常因两个过程的相互干扰而导致不准确

Method: 提出Execute-Summarize(ES)框架，将任务执行与工作流构建解耦：模型首先使用可用工具完成任务，然后独立地从执行轨迹中重构结构化工作流

Result: 通过FlowBench进行大量实验，该方法优于现有方法，为将自由形式的LLM推理落地为结构化工作流提供了可靠范式

Conclusion: ES框架通过解耦任务执行与工作流构建，提高了工作流的准确性和鲁棒性，为LLM推理到结构化工作流的转化提供了有效解决方案

Abstract: LLMs can solve complex tasks through reasoning and tool use, but accurately translating these solutions into structured workflows remains challenging. We model workflows as sequences of tool use and reformulate the problem as designing a mechanism that can both solve tasks and reliably construct workflows. Prior approaches that build workflows during execution often suffer from inaccuracies due to interference between the two processes. We propose an Execute-Summarize(ES) framework that decouples task execution from workflow construction: the model first completes the task using available tools, then independently reconstructs a structured workflow from execution traces. This separation improves workflow accuracy and robustness. We introduce FlowBench and show through extensive experiments that our approach outperforms existing methods, providing a reliable paradigm for grounding free-form LLM reasoning into structured workflows.

</details>


### [41] [PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics](https://arxiv.org/abs/2602.11666)
*E Fan,Lisong Shi,Zhengtong Li,Chih-yung Wen*

Main category: cs.AI

TL;DR: PhyNiKCE是一个神经符号代理框架，通过将神经规划与符号验证解耦，解决了LLM在CFD模拟中无法强制执行物理守恒定律的问题，显著提高了模拟设置的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自主代理在计算流体动力学（CFD）应用中存在严重限制，因为LLM的概率性本质无法强制执行严格的物理守恒定律和数值稳定性要求。纯语义的检索增强生成（RAG）经常导致"上下文污染"，产生语言上合理但物理上无效的配置。

Method: 提出PhyNiKCE框架，将神经规划与符号验证解耦。使用符号知识引擎将模拟设置视为约束满足问题，通过确定性RAG引擎严格强制执行物理约束，并采用专门的检索策略处理求解器、湍流模型和边界条件。

Result: 在OpenFOAM实验中对实际非教程CFD任务进行验证，使用Gemini-2.5-Pro/Flash，PhyNiKCE相比最先进基线实现了96%的相对改进。通过知识驱动初始化替代试错法，将自主自校正循环减少了59%，同时将LLM令牌消耗降低了17%。

Conclusion: 神经生成与符号约束执行的解耦显著增强了鲁棒性和效率。虽然验证于CFD领域，但该架构为更广泛的工业自动化中的可信人工智能提供了可扩展、可审计的范式。

Abstract: The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required for physics-based simulations. Reliance on purely semantic Retrieval Augmented Generation (RAG) often leads to "context poisoning," where agents generate linguistically plausible but physically invalid configurations due to a fundamental Semantic-Physical Disconnect. To bridge this gap, this work introduces PhyNiKCE (Physical and Numerical Knowledgeable Context Engineering), a neurosymbolic agentic framework for trustworthy engineering. Unlike standard black-box agents, PhyNiKCE decouples neural planning from symbolic validation. It employs a Symbolic Knowledge Engine that treats simulation setup as a Constraint Satisfaction Problem, rigidly enforcing physical constraints via a Deterministic RAG Engine with specialized retrieval strategies for solvers, turbulence models, and boundary conditions. Validated through rigorous OpenFOAM experiments on practical, non-tutorial CFD tasks using Gemini-2.5-Pro/Flash, PhyNiKCE demonstrates a 96% relative improvement over state-of-the-art baselines. Furthermore, by replacing trial-and-error with knowledge-driven initialization, the framework reduced autonomous self-correction loops by 59% while simultaneously lowering LLM token consumption by 17%. These results demonstrate that decoupling neural generation from symbolic constraint enforcement significantly enhances robustness and efficiency. While validated on CFD, this architecture offers a scalable, auditable paradigm for Trustworthy Artificial Intelligence in broader industrial automation.

</details>


### [42] [Right for the Wrong Reasons: Epistemic Regret Minimization for Causal Rung Collapse in LLMs](https://arxiv.org/abs/2602.11675)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文提出Rung Collapse理论解释机器学习系统"正确但理由错误"的问题，开发Epistemic Regret Minimization方法进行因果信念修正，通过三层架构实现跨领域错误预防。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统常通过捷径获得高性能，但在分布变化下崩溃。这种病理有精确的因果起源：自回归训练无法区分关联P(Y|X)与干预P(Y|do(X))，导致Rung Collapse。当基于结果的学习强化了通过错误因果模型获得的正确答案时，智能体会陷入错误推理的固化，即Aleatoric Entrenchment。

Method: 提出Epistemic Regret Minimization作为因果信念修正目标，独立于任务成功惩罚因果推理错误。构建三层架构：1) 物理基础定理证明满足执行器独立的动作实现有效do操作；2) ERM作为满足AGM公设的因果信念修正算子；3) 错误模式分类法分类重复推理错误并注入领域无关防护。

Result: 在6个前沿LLM的1,360个因果陷阱场景实验中，发现Rung Collapse在推理增强模型中持续存在（GPT-5.2为3.7%），可操控性呈现逆缩放现象，而针对性ERM反馈在结果级反馈失败的情况下恢复了53-59%的固化错误。

Conclusion: 论文形式化了Rung Collapse作为机器学习系统"正确但理由错误"的因果起源，提出ERM作为解决方案，通过理论证明和实验验证了其在防止因果推理错误固化和实现跨领域转移的有效性。

Abstract: Machine learning systems that are "right for the wrong reasons" achieve high performance through shortcuts that collapse under distributional shift. We show this pathology has a precise causal origin: autoregressive training provides no gradient signal to distinguish association P(Y|X) from intervention P(Y|do(X)), a failure we formalize as Rung Collapse. When outcome-based learning reinforces correct answers obtained through incorrect causal models, the agent becomes entrenched in flawed reasoning, a phenomenon we term Aleatoric Entrenchment. We propose Epistemic Regret Minimization (ERM), a belief revision objective that penalizes errors in causal reasoning independently of task success, and embed it within a three-layer architecture with three contributions grounded in knowledge representation: (1) a Physical Grounding Theorem proving that actions satisfying actuator independence implement valid do-operations, bridging action languages and do-calculus; (2) ERM as a causal belief revision operator satisfying AGM postulates, preventing entrenchment even when the agent succeeds for the wrong reasons; and (3) a failure mode taxonomy that classifies recurring reasoning errors and injects domain-independent guards, enabling cross-domain transfer. We prove asymptotic recovery of the true interventional distribution with finite-sample bounds. Experiments on 1,360 causal trap scenarios across six frontier LLMs reveal that Rung Collapse persists even in reasoning-enhanced models (3.7% for GPT-5.2), that steerability exhibits inverse scaling where advanced models resist generic correction, and that targeted ERM feedback recovers 53-59% of entrenched errors where outcome-level feedback fails.

</details>


### [43] [AIR: Improving Agent Safety through Incident Response](https://arxiv.org/abs/2602.11749)
*Zibo Xiao,Jun Sun,Junjie Chen*

Main category: cs.AI

TL;DR: AIR是首个LLM智能体事故响应框架，通过语义检查检测事故，引导智能体执行遏制恢复操作，并合成防护规则防止未来类似事故，在三种代表性智能体上实现超过90%的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体的安全机制主要关注事前预防，对事故发生后如何响应、遏制和恢复的能力有限。实际部署中事故不可避免，需要建立系统化的事故响应机制。

Method: 提出AIR框架：1) 定义领域特定语言管理LLM智能体事故响应生命周期；2) 集成到智能体执行循环中，通过基于环境状态和上下文的语义检查检测事故；3) 引导智能体使用工具执行遏制和恢复操作；4) 在根除阶段合成防护规则阻止未来类似事故。

Result: 在三种代表性智能体类型上评估，AIR实现了检测、修复和根除成功率均超过90%。实验证实了关键设计组件的必要性，展示了及时性和适度开销，LLM生成的规则接近开发者编写规则的效果。

Conclusion: 事故响应作为提升智能体安全的一等机制既是可行的也是必要的，AIR框架为LLM智能体系统提供了系统化的事故响应能力。

Abstract: Large Language Model (LLM) agents are increasingly deployed in practice across a wide range of autonomous applications. Yet current safety mechanisms for LLM agents focus almost exclusively on preventing failures in advance, providing limited capabilities for responding to, containing, or recovering from incidents after they inevitably arise. In this work, we introduce AIR, the first incident response framework for LLM agent systems. AIR defines a domain-specific language for managing the incident response lifecycle autonomously in LLM agent systems, and integrates it into the agent's execution loop to (1) detect incidents via semantic checks grounded in the current environment state and recent context, (2) guide the agent to execute containment and recovery actions via its tools, and (3) synthesize guardrail rules during eradication to block similar incidents in future executions. We evaluate AIR on three representative agent types. Results show that AIR achieves detection, remediation, and eradication success rates all exceeding 90%. Extensive experiments further confirm the necessity of AIR's key design components, show the timeliness and moderate overhead of AIR, and demonstrate that LLM-generated rules can approach the effectiveness of developer-authored rules across domains. These results show that incident response is both feasible and essential as a first-class mechanism for improving agent safety.

</details>


### [44] [TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents](https://arxiv.org/abs/2602.11767)
*Aladin Djuhera,Swanand Ravindra Kadhe,Farhan Ahmed,Holger Boche*

Main category: cs.AI

TL;DR: TSR是一种训练时方法，通过轻量级树搜索构建高质量轨迹，提升多轮强化学习中的rollout质量，实现更稳定学习


<details>
  <summary>Details</summary>
Motivation: 多轮强化学习中奖励稀疏/延迟、环境随机性等问题导致轨迹采样困难，影响探索效率和模式崩溃

Method: 提出TSR方法，在训练时使用轻量级树搜索（如best-of-N、beam search、浅层前瞻搜索）选择高分动作构建高质量轨迹，保持优化目标不变

Result: 在Sokoban、FrozenLake和WebShop任务上，结合PPO和GRPO实现最高15%性能提升和更稳定学习，仅需一次性训练计算增加

Conclusion: TSR通过将搜索从推理时移到训练rollout阶段，为多轮智能体学习提供简单通用的增强机制，与现有框架和拒绝采样方法互补

Abstract: Advances in large language models (LLMs) are driving a shift toward using reinforcement learning (RL) to train agents from iterative, multi-turn interactions across tasks. However, multi-turn RL remains challenging as rewards are often sparse or delayed, and environments can be stochastic. In this regime, naive trajectory sampling can hinder exploitation and induce mode collapse. We propose TSR (Trajectory-Search Rollouts), a training-time approach that repurposes test-time scaling ideas for improved per-turn rollout generation. TSR performs lightweight tree-style search to construct high-quality trajectories by selecting high-scoring actions at each turn using task-specific feedback. This improves rollout quality and stabilizes learning while leaving the underlying optimization objective unchanged, making TSR optimizer-agnostic. We instantiate TSR with best-of-N, beam, and shallow lookahead search, and pair it with PPO and GRPO, achieving up to 15% performance gains and more stable learning on Sokoban, FrozenLake, and WebShop tasks at a one-time increase in training compute. By moving search from inference time to the rollout stage of training, TSR provides a simple and general mechanism for stronger multi-turn agent learning, complementary to existing frameworks and rejection-sampling-style selection methods.

</details>


### [45] [Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation](https://arxiv.org/abs/2602.11790)
*Lingyong Yan,Jiulong Wu,Dong Xie,Weixian Shi,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: LAVES是一个基于LLM的分层多智能体系统，用于从教育问题生成高质量教学视频，通过结构化可执行脚本实现自动化生产，相比行业标准方法成本降低95%以上。


<details>
  <summary>Details</summary>
Motivation: 当前端到端视频生成模型在需要严格逻辑严谨性和精确知识表示的场景（如教学和教育媒体）中存在局限，现有方法存在程序保真度低、生产成本高和可控性有限等问题。

Method: 提出分层LLM多智能体系统，将生成流程分解为由中央编排智能体协调的专门智能体：解决方案智能体负责严谨问题求解，插图智能体生成可执行可视化代码，旁白智能体创建学习者导向教学脚本。所有输出都经过语义批判、基于规则的约束和工具编译检查，系统构建结构化可执行视频脚本，通过模板驱动组装规则确定性地编译成同步视觉和旁白。

Result: 在大规模部署中，LAVES实现每天超过100万个视频的吞吐量，相比当前行业标准方法成本降低超过95%，同时保持高接受率。

Conclusion: LAVES通过分层多智能体架构解决了教育视频生成中的逻辑严谨性和知识表示问题，实现了高质量、低成本、完全自动化的教学视频生产。

Abstract: Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.

</details>


### [46] [Detecting RLVR Training Data via Structural Convergence of Reasoning](https://arxiv.org/abs/2602.11792)
*Hongbo Zhang,Yue Yang,Jianhao Yan,Guangsheng Bao,Yue Zhang,Yue Zhang*

Main category: cs.AI

TL;DR: 提出Min-$k$NN Distance方法，通过分析RLVR训练后模型生成文本的多样性变化来检测基准污染，无需参考模型或token概率。


<details>
  <summary>Details</summary>
Motivation: RLVR训练方法使用奖励反馈微调模型，但训练数据不公开可能导致基准污染。传统基于似然的检测方法对RLVR无效，需要新的检测手段。

Method: 发现RLVR训练导致行为特征：训练过的prompt生成更僵化相似，未见过prompt保持多样性。提出Min-$k$NN Distance方法：对给定prompt采样多个补全，计算k个最小最近邻编辑距离的平均值。

Result: 在多个RLVR训练推理模型上实验，Min-$k$NN Distance能可靠区分RL见过的和未见过的示例，优于现有成员推理和RL污染检测基线方法。

Conclusion: RLVR训练产生可检测的行为特征，Min-$k$NN Distance是有效的黑盒检测器，无需访问参考模型或token概率，能识别基准污染。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-$k$NN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the $k$ smallest nearest-neighbor edit distances. Min-$k$NN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-$k$NN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.

</details>


### [47] [Intelligent AI Delegation](https://arxiv.org/abs/2602.11865)
*Nenad Tomašev,Matija Franklin,Simon Osindero*

Main category: cs.AI

TL;DR: 提出了一个自适应AI委托框架，用于智能任务分解和委托，包含权限转移、责任划分、信任建立等机制，适用于复杂委托网络中的人类和AI参与者。


<details>
  <summary>Details</summary>
Motivation: 现有任务分解和委托方法依赖简单启发式，无法动态适应环境变化和稳健处理意外失败。随着AI代理处理更复杂任务，需要能够有意义地分解问题并安全地将子任务委托给其他AI代理和人类。

Method: 提出了一个自适应智能AI委托框架，包含任务分配决策序列，整合了权限转移、责任、问责、角色边界规范、意图清晰度以及建立双方信任的机制。

Result: 该框架适用于复杂委托网络中的人类和AI委托者与被委托者，旨在为新兴代理网络中的协议开发提供指导。

Conclusion: 提出的自适应委托框架解决了现有方法的局限性，为构建更可靠、适应性强的AI代理系统提供了理论基础，有助于推动代理网络的发展。

Abstract: AI agents are able to tackle increasingly complex tasks. To achieve more ambitious goals, AI agents need to be able to meaningfully decompose problems into manageable sub-components, and safely delegate their completion across to other AI agents and humans alike. Yet, existing task decomposition and delegation methods rely on simple heuristics, and are not able to dynamically adapt to environmental changes and robustly handle unexpected failures. Here we propose an adaptive framework for intelligent AI delegation - a sequence of decisions involving task allocation, that also incorporates transfer of authority, responsibility, accountability, clear specifications regarding roles and boundaries, clarity of intent, and mechanisms for establishing trust between the two (or more) parties. The proposed framework is applicable to both human and AI delegators and delegatees in complex delegation networks, aiming to inform the development of protocols in the emerging agentic web.

</details>


### [48] [Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments](https://arxiv.org/abs/2602.11964)
*Romain Froger,Pierre Andrews,Matteo Bettini,Amar Budhiraja,Ricardo Silveira Cabral,Virginie Do,Emilien Garreau,Jean-Baptiste Gaya,Hugo Laurençon,Maxime Lecanu,Kunal Malkan,Dheeraj Mekala,Pierre Ménard,Gerard Moreno-Torres Bertran,Ulyana Piterbarg,Mikhail Plekhanov,Mathieu Rita,Andrey Rusakov,Vladislav Vorotilov,Mengjue Wang,Ian Yu,Amine Benhalloum,Grégoire Mialon,Thomas Scialom*

Main category: cs.AI

TL;DR: Gaia2是一个用于评估LLM代理在现实异步环境中的基准，引入独立于代理动作的环境演化、时间约束、动态事件等挑战，支持细粒度动作级评估和强化学习。


<details>
  <summary>Details</summary>
Motivation: 现有基准多为静态或同步评估，无法反映现实世界中环境独立演化、时间敏感、动态变化等复杂情况，需要新的评估框架来测试代理在真实场景中的能力。

Method: 基于开源Agents Research Environments平台构建消费者环境，创建异步演化场景，每个场景配备写动作验证器，支持动作级评估和可验证奖励的强化学习。

Result: 评估显示各模型存在能力权衡：GPT-5最高分42%但时间敏感任务失败，Claude-4在准确性和速度间权衡，Kimi-K2开源模型最佳21%。结果揭示了推理、效率、鲁棒性之间的基本权衡。

Conclusion: Gaia2揭示了当前代理系统在现实异步环境中的局限性，特别是"模拟到现实"差距的挑战。通过发布Gaia2和ARE框架，为社区提供开发实用代理系统的灵活基础设施。

Abstract: We introduce Gaia2, a benchmark for evaluating large language model agents in realistic, asynchronous environments. Unlike prior static or synchronous evaluations, Gaia2 introduces scenarios where environments evolve independently of agent actions, requiring agents to operate under temporal constraints, adapt to noisy and dynamic events, resolve ambiguity, and collaborate with other agents. Each scenario is paired with a write-action verifier, enabling fine-grained, action-level evaluation and making Gaia2 directly usable for reinforcement learning from verifiable rewards. Our evaluation of state-of-the-art proprietary and open-source models shows that no model dominates across capabilities: GPT-5 (high) reaches the strongest overall score of 42% pass@1 but fails on time-sensitive tasks, Claude-4 Sonnet trades accuracy and speed for cost, Kimi-K2 leads among open-source models with 21% pass@1. These results highlight fundamental trade-offs between reasoning, efficiency, robustness, and expose challenges in closing the "sim2real" gap. Gaia2 is built on a consumer environment with the open-source Agents Research Environments platform and designed to be easy to extend. By releasing Gaia2 alongside the foundational ARE framework, we aim to provide the community with a flexible infrastructure for developing, benchmarking, and training the next generation of practical agent systems.

</details>


### [49] [LawThinker: A Deep Research Legal Agent in Dynamic Environments](https://arxiv.org/abs/2602.12056)
*Xinyu Yang,Chenlong Deng,Tongyu Wen,Binyu Xie,Zhicheng Dou*

Main category: cs.AI

TL;DR: LawThinker是一个自主法律研究智能体，采用探索-验证-记忆策略，在动态司法环境中通过深度验证模块检查每个检索结果的知识准确性、事实-法律相关性和程序合规性，显著提升法律推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理方法缺乏验证中间推理步骤的机制，导致不适用法规引用等错误在推理链中传播而未被检测。需要一种能验证每个知识探索步骤的方法来确保程序合规性。

Method: 提出LawThinker自主法律研究智能体，采用探索-验证-记忆策略。核心是在每个知识探索步骤后强制执行验证作为原子操作。DeepVerifier模块从知识准确性、事实-法律相关性和程序合规性三个维度检查每个检索结果，并使用记忆模块在长时任务中跨轮次复用知识。

Result: 在动态基准J1-EVAL上，LawThinker相比直接推理方法提升24%，相比基于工作流的方法提升11%，在面向过程的指标上表现尤为突出。在三个静态基准上的评估进一步证实了其泛化能力。

Conclusion: LawThinker通过强制执行每个知识探索步骤后的验证操作，有效解决了现有法律推理方法中中间步骤验证不足的问题，在动态司法环境中实现了更准确和程序合规的法律推理。

Abstract: Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent .

</details>


### [50] [Differentiable Modal Logic for Multi-Agent Diagnosis, Orchestration and Communication](https://arxiv.org/abs/2602.12083)
*Antonin Sulc*

Main category: cs.AI

TL;DR: 论文提出可微分模态逻辑(DML)和模态逻辑神经网络(MLNNs)，用于从行为数据中学习多智能体系统的信任网络、因果链和监管边界，实现神经符号调试。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体AI系统从简单聊天机器人发展为自主群体，调试语义故障需要推理知识、信念、因果性和义务性，这正是模态逻辑设计用来形式化的内容。然而传统模态逻辑需要手动指定在真实系统中未知或动态的关系结构。

Method: 提出可微分模态逻辑(DML)，通过模态逻辑神经网络(MLNNs)实现，使系统能够仅从行为数据中学习信任网络、因果链和监管边界。提供统一的神经符号调试框架，涵盖四个模态：认知模态（信任谁）、时间模态（事件何时导致故障）、道义模态（允许什么行动）和信念模态（如何解释智能体置信度）。

Result: 在具体多智能体场景中演示了每个模态的应用，从外交游戏中发现欺骗联盟到检测LLM幻觉。逻辑矛盾成为可学习的优化目标，提供可解释的学习结构，其中信任和因果性是显式参数而非不透明嵌入。

Conclusion: 可微分模态逻辑为多智能体系统调试提供了实用的神经符号方法，支持知识注入、组合多模态推理，以及监控、主动控制和通信的实际部署模式。所有代码以可执行的Jupyter笔记本形式提供。

Abstract: As multi-agent AI systems evolve from simple chatbots to autonomous swarms, debugging semantic failures requires reasoning about knowledge, belief, causality, and obligation, precisely what modal logic was designed to formalize. However, traditional modal logic requires manual specification of relationship structures that are unknown or dynamic in real systems. This tutorial demonstrates differentiable modal logic (DML), implemented via Modal Logical Neural Networks (MLNNs), enabling systems to learn trust networks, causal chains, and regulatory boundaries from behavioral data alone.
  We present a unified neurosymbolic debugging framework through four modalities: epistemic (who to trust), temporal (when events cause failures), deontic (what actions are permitted), and doxastic (how to interpret agent confidence). Each modality is demonstrated on concrete multi-agent scenarios, from discovering deceptive alliances in diplomacy games to detecting LLM hallucinations, with complete implementations showing how logical contradictions become learnable optimization objectives. Key contributions for the neurosymbolic community: (1) interpretable learned structures where trust and causality are explicit parameters, not opaque embeddings; (2) knowledge injection via differentiable axioms that guide learning with sparse data (3) compositional multi-modal reasoning that combines epistemic, temporal, and deontic constraints; and (4) practical deployment patterns for monitoring, active control and communication of multi-agent systems. All code provided as executable Jupyter notebooks.

</details>


### [51] [The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context](https://arxiv.org/abs/2602.12108)
*Xiaoyuan Liu,Tian Liang,Dongyang Ma,Deyu Zhou,Haitao Mi,Pinjia He,Yan Wang*

Main category: cs.AI

TL;DR: StateLM：一种具有内部状态管理能力的新型基础模型，通过主动管理记忆工具来突破固定上下文窗口的限制


<details>
  <summary>Details</summary>
Motivation: 当前LLMs缺乏主动管理自身状态的能力，只能被动接受手动设计的上下文作为记忆，这限制了它们在长文档处理、对话记忆等任务中的表现

Method: 引入StateLM，赋予模型内部推理循环来管理自身状态，配备上下文修剪、文档索引、笔记记录等记忆工具，并训练模型主动使用这些工具

Result: StateLM在长文档QA任务中全面优于标准LLM；在聊天记忆任务中提升10-20%准确率；在BrowseComp-Plus深度研究任务中达到52%准确率，而标准LLM仅5%

Conclusion: StateLM将LLM从被动预测器转变为状态感知的智能体，使推理成为有状态且可管理的过程

Abstract: In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the "wand" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manage their own state. We equip our model with a suite of memory tools, such as context pruning, document indexing, and note-taking, and train it to actively manage these tools. By learning to dynamically engineering its own context, our model breaks free from the architectural prison of a fixed window. Experiments across various model sizes demonstrate StateLM's effectiveness across diverse scenarios. On long-document QA tasks, StateLMs consistently outperform standard LLMs across all model scales; on the chat memory task, they achieve absolute accuracy improvements of 10% to 20% over standard LLMs. On the deep research task BrowseComp-Plus, the performance gap becomes even more pronounced: StateLM achieves up to 52% accuracy, whereas standard LLM counterparts struggle around 5%. Ultimately, our approach shifts LLMs from passive predictors to state-aware agents where reasoning becomes a stateful and manageable process.

</details>


### [52] [Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty](https://arxiv.org/abs/2602.12113)
*Zewei Yu,Lirong Gao,Yuke Zhu,Bo Zheng,Sheng Guo,Haobo Wang,Junbo Zhao*

Main category: cs.AI

TL;DR: ARLCP是一个自适应反思和长度协调惩罚的强化学习框架，通过动态平衡推理效率和准确性，减少大型推理模型中不必要的反思步骤，显著缩短响应长度同时提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中经常生成过长的思维链，包含大量不必要的反思（如重复自我质疑和循环推理），导致高令牌消耗、计算开销增加和延迟，但准确性并未提升，尤其是在较小模型中。问题复杂性增加会引发更多过度和不必要的反思，从而降低准确性并增加令牌开销。

Method: 提出自适应反思和长度协调惩罚（ARLCP）强化学习框架，包含两个关键创新：1）自适应反思惩罚，在保留必要推理的同时减少不必要的反思步骤；2）根据问题估计复杂性校准的长度惩罚。通过协调这两个惩罚，鼓励模型生成更简洁有效的推理路径。

Result: 在五个数学推理基准测试中使用DeepSeek-R1-Distill-Qwen-1.5B和7B模型评估。1.5B模型平均响应长度减少53.1%，准确性提高5.8%；7B模型长度减少35.0%，准确性提高2.7%。在效率-准确性权衡方面优于现有方法。

Conclusion: ARLCP框架能有效解决大型推理模型中过度反思问题，通过自适应惩罚机制实现推理效率和准确性的更好平衡，为资源受限环境中的高效推理提供了有前景的解决方案。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high token consumption, substantial computational overhead, and increased latency without improving accuracy, particularly in smaller models. Our observation reveals that increasing problem complexity induces more excessive and unnecessary reflection, which in turn reduces accuracy and increases token overhead. To address this challenge, we propose Adaptive Reflection and Length Coordinated Penalty (ARLCP), a novel reinforcement learning framework designed to dynamically balance reasoning efficiency and solution accuracy. ARLCP introduces two key innovations: (1) a reflection penalty that adaptively curtails unnecessary reflective steps while preserving essential reasoning, and (2) a length penalty calibrated to the estimated complexity of the problem. By coordinating these penalties, ARLCP encourages the model to generate more concise and effective reasoning paths. We evaluate our method on five mathematical reasoning benchmarks using DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B models. Experimental results show that ARLCP achieves a superior efficiency-accuracy trade-off compared to existing approaches. For the 1.5B model, it reduces the average response length by 53.1% while simultaneously improving accuracy by 5.8%. For the 7B model, it achieves a 35.0% reduction in length with a 2.7% accuracy gain. The code is released at https://github.com/ZeweiYu1/ARLCP .

</details>


### [53] [STAR : Bridging Statistical and Agentic Reasoning for Large Model Performance Prediction](https://arxiv.org/abs/2602.12143)
*Xiaoxiao Wang,Chunxiao Li,Junying Wang,Yijin Guo,Zijian Chen,Chunyi Li,Xiaohong Liu,Zicheng Zhang,Guangtao Zhai*

Main category: cs.AI

TL;DR: STAR框架结合统计期望与知识驱动推理，通过约束概率矩阵分解和期望违背理论，在稀疏观测下准确预测模型性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大模型评估成本急剧增加，从有限观测预测模型性能变得至关重要。现有统计方法难以处理模式偏移、数据稀疏和缺乏解释性，而纯LLM方法仍不可靠。

Method: STAR框架结合数据驱动的统计期望与知识驱动的智能推理：1) 使用专业检索器获取外部知识，将语义特征嵌入约束概率矩阵分解生成带不确定性的统计期望；2) 基于期望违背理论的推理模块通过族内分析、跨模型比较和可信度感知聚合来优化预测，提供可追溯的解释。

Result: 在广泛实验中，STAR在分数基和排名基指标上均优于所有基线方法。在极端稀疏条件下（每个测试模型仅1-2个观测分数），相比最强统计方法获得14.46%的总分提升。

Conclusion: STAR框架成功解决了大模型性能预测中的模式偏移、数据稀疏和解释性不足问题，通过结合统计期望与知识推理实现了准确可靠的预测。

Abstract: As comprehensive large model evaluation becomes prohibitively expensive, predicting model performance from limited observations has become essential. However, existing statistical methods struggle with pattern shifts, data sparsity, and lack of explanation, while pure LLM methods remain unreliable. We propose STAR, a framework that bridges data-driven STatistical expectations with knowledge-driven Agentic Reasoning. STAR leverages specialized retrievers to gather external knowledge and embeds semantic features into Constrained Probabilistic Matrix Factorization (CPMF) to generate statistical expectations with uncertainty. A reasoning module guided by Expectation Violation Theory (EVT) then refines predictions through intra-family analysis, cross-model comparison, and credibility-aware aggregation, producing adjustments with traceable explanations. Extensive experiments show that STAR consistently outperforms all baselines on both score-based and rank-based metrics, delivering a 14.46% gain in total score over the strongest statistical method under extreme sparsity, with only 1--2 observed scores per test model.

</details>


### [54] [Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision](https://arxiv.org/abs/2602.12164)
*Xiaohan He,Shiyang Feng,Songtao Huang,Lei Bai,Bin Wang,Bo Zhang*

Main category: cs.AI

TL;DR: Sci-CoE是一个两阶段科学协同演化框架，通过从稀疏监督到无监督学习的过渡，使模型能够作为求解器和验证器自我演化，提升科学推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码和数学领域展现出优秀的推理能力，但在科学推理任务中仍然脆弱，主要原因是解决方案评估不可靠和验证策略多样性有限。

Method: 采用两阶段框架：第一阶段使用少量标注数据为验证器建立基本正确性判断锚点；第二阶段引入几何奖励机制，综合考虑共识、可靠性和多样性，驱动大规模无标注数据的自我迭代。

Result: 在多个通用科学基准测试上的实验表明，Sci-CoE增强了复杂推理能力，展现出强大的可扩展性，有助于构建更鲁棒和多样化的评估系统。

Conclusion: Sci-CoE框架通过协同演化方法有效解决了科学推理中的评估不可靠和多样性不足问题，为构建更强大的科学推理系统提供了可行方案。

Abstract: Large language models (LLMs) have demonstrated exceptional reasoning capabilities, and co-evolving paradigms have shown promising results in domains such as code and math. However, in scientific reasoning tasks, these models remain fragile due to unreliable solution evaluation and limited diversity in verification strategies. In this work, we propose Sci-CoE, a two-stage scientific co-evolving framework that enables models to self-evolve as both solver and verifier through a transition from sparse supervision to unsupervised learning. In the first stage, the model uses a small set of annotated data to establish fundamental correctness judgment anchors for the Verifier. In the second stage, we introduce a geometric reward mechanism that jointly considers consensus, reliability, and diversity, driving large-scale self-iteration on unlabeled data. Experiments on several general scientific benchmarks demonstrate that Sci-CoE enhances complex reasoning capabilities and exhibits strong scalability, facilitating the construction of more robust and diverse evaluation systems. Codes are available at https://github.com/InternScience/Sci-CoE.

</details>


### [55] [Think like a Scientist: Physics-guided LLM Agent for Equation Discovery](https://arxiv.org/abs/2602.12259)
*Jianke Yang,Ohm Venkatachalam,Mohammad Kianezhad,Sharvaree Vadgama,Rose Yu*

Main category: cs.AI

TL;DR: KeplerAgent是一个基于LLM的智能体框架，通过模拟科学家的多步推理过程（先推断物理性质如对称性，再将其作为先验约束方程空间）来发现符号方程，相比直接猜测的方法在符号准确性和噪声鲁棒性上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统直接从数据猜测方程，忽略了科学家通常遵循的多步推理过程：先推断物理性质（如对称性），再将其作为先验来限制候选方程空间。需要更符合科学推理过程的框架。

Method: KeplerAgent是一个智能体框架，协调基于物理的工具提取中间结构，利用这些结果配置符号回归引擎（如PySINDy和PySR），包括它们的函数库和结构约束。

Result: 在一系列物理方程基准测试中，KeplerAgent相比LLM和传统基线方法，实现了显著更高的符号准确性和对噪声数据的更强鲁棒性。

Conclusion: 通过显式建模科学推理过程（先推断物理性质再约束方程空间），KeplerAgent框架在符号方程发现任务上超越了直接猜测的方法，展示了智能体框架在科学发现中的潜力。

Abstract: Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.

</details>


### [56] [CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use](https://arxiv.org/abs/2602.12268)
*Zhen Zhang,Kaiqiang Song,Xun Wang,Yebowen Hu,Weixiang Yan,Chenyang Zhao,Henry Peng Zou,Haoyun Deng,Sathish Reddy Indurthi,Shujian Liu,Simin Ma,Xiaoyang Wang,Xin Eric Wang,Song Wang*

Main category: cs.AI

TL;DR: CM2是一个强化学习框架，通过检查表奖励替代可验证结果奖励，用于优化多轮多步工具使用AI代理，在模拟环境中训练，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在AI代理应用中面临挑战：现实目标缺乏可验证奖励，强调开放式行为；多轮多步工具使用的RL研究不足；构建和维护可执行工具环境成本高，限制了规模和覆盖范围。

Method: CM2采用检查表奖励框架，将每轮预期行为分解为细粒度二元标准，具有明确证据基础和结构化元数据。采用稀疏奖励分配但密集评估标准的策略，在LLM模拟工具环境中进行训练，避免大规模工具集的重工程。

Result: 从8B基础模型开始，在8k示例RL数据集上训练，CM2相比SFT对应模型在tau^-Bench上提升8分，在BFCL-V4上提升10分，在ToolSandbox上提升12分。结果匹配甚至优于类似规模的开源基线，包括评判模型。

Conclusion: CM2提供了一个可扩展的配方，用于优化多轮多步工具使用代理，无需依赖可验证奖励，为AI代理的强化学习训练提供了实用解决方案。

Abstract: AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn's intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.

</details>


### [57] [Agentic Test-Time Scaling for WebAgents](https://arxiv.org/abs/2602.12276)
*Nicholas Lee,Lutfi Eren Erdogan,Chris Joseph John,Surya Krishnapillai,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.AI

TL;DR: 提出CATTS方法，通过基于投票不确定性的动态计算分配，在保持性能的同时减少多步智能体任务的token消耗


<details>
  <summary>Details</summary>
Motivation: 测试时缩放已成为提升神经网络性能的标准方法，但在多步智能体任务中，均匀增加计算资源会快速饱和且收益递减，需要更智能的计算分配策略

Method: CATTS方法：1) 研究Web智能体的推理时缩放行为；2) 探索更强的聚合策略；3) 发现智能体自身投票分布的不确定性统计量（熵和top-1/top-2边界）与下游成功相关；4) 基于投票不确定性动态分配计算资源

Result: 在WebArena-Lite和GoBrowse上，CATTS比React性能提升达9.1%，同时比均匀缩放少用2.3倍token，实现效率提升和可解释决策

Conclusion: 基于投票不确定性的动态计算分配策略CATTS能有效提升多步智能体任务的性能和效率，提供可解释的决策规则

Abstract: Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent's own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT](https://arxiv.org/abs/2602.11220)
*Jiacheng Wang,Ping Jian,Zhen Yang,Zirong Chen,Keren Liao,Zhongbin Guo*

Main category: cs.LG

TL;DR: 提出基于强化学习的数据重写代理，通过优化重写策略来改善下游监督微调，在保持任务一致性的同时减少灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 当下游数据与模型先验训练分布存在显著偏移时，传统的监督微调会导致灾难性遗忘。现有数据重写方法通常从提示诱导的条件分布中采样重写，导致目标与模型的自然QA风格生成分布不一致，且依赖固定模板会导致多样性崩溃。

Method: 将数据重写构建为策略学习问题，学习一个重写策略以更好地匹配骨干模型的QA风格生成分布同时保持多样性。利用强化学习在奖励反馈下优化重写分布，提出基于RL的数据重写代理，联合优化QA风格分布对齐和多样性，并在硬任务一致性门控下构建更高质量的重写数据集用于下游SFT。

Result: 广泛实验表明，该方法在下游任务上获得与标准SFT相当的增益，同时在非下游基准上平均减少12.34%的遗忘。

Conclusion: 通过强化学习优化数据重写策略，能够有效缓解监督微调中的灾难性遗忘问题，在保持下游性能的同时显著减少模型能力的退化。

Abstract: Large language models (LLMs) have made rapid progress, yet adapting them to downstream scenarios still commonly relies on supervised fine-tuning (SFT). When downstream data exhibit a substantial distribution shift from the model's prior training distribution, SFT can induce catastrophic forgetting. To narrow this gap, data rewriting has been proposed as a data-centric approach that rewrites downstream training data prior to SFT. However, existing methods typically sample rewrites from a prompt-induced conditional distribution, so the resulting targets are not necessarily aligned with the model's natural QA-style generation distribution. Moreover, reliance on fixed templates can lead to diversity collapse. To address these issues, we cast data rewriting as a policy learning problem and learn a rewriting policy that better matches the backbone's QA-style generation distribution while preserving diversity. Since distributional alignment, diversity and task consistency are automatically evaluable but difficult to optimize end-to-end with differentiable objectives, we leverage reinforcement learning to optimize the rewrite distribution under reward feedback and propose an RL-based data-rewriting agent. The agent jointly optimizes QA-style distributional alignment and diversity under a hard task-consistency gate, thereby constructing a higher-quality rewritten dataset for downstream SFT. Extensive experiments show that our method achieves downstream gains comparable to standard SFT while reducing forgetting on non-downstream benchmarks by 12.34% on average. Our code is available at https://anonymous.4open.science/r/Patch-the-Prompt-Gap-4112 .

</details>


### [59] [Evaluating Memory Structure in LLM Agents](https://arxiv.org/abs/2602.11243)
*Alina Shutova,Alexandra Olenina,Ivan Vinogradov,Anton Sinitsin*

Main category: cs.LG

TL;DR: 提出了StructMemEval基准测试，用于评估智能体组织长期记忆的能力，而不仅仅是事实回忆，发现现有LLM在未提示时难以识别记忆结构。


<details>
  <summary>Details</summary>
Motivation: 现有长期记忆基准主要关注简单事实保留、多跳回忆和时间变化，这些能力通常可以通过简单的检索增强LLM实现，无法测试复杂的记忆层次结构。需要新的基准来评估智能体组织长期记忆的能力。

Method: 提出StructMemEval基准，收集了一系列人类通过特定结构组织知识来解决的任务：交易账本、待办事项列表、树结构等。通过实验比较简单检索增强LLM和记忆智能体在这些任务上的表现。

Result: 简单检索增强LLM在这些结构化记忆任务上表现不佳，而记忆智能体在提示如何组织记忆时可以可靠解决。但现代LLM在未提示时并不总能识别记忆结构。

Conclusion: 研究揭示了LLM训练和记忆框架改进的重要方向，强调了智能体需要具备自主识别和组织记忆结构的能力。

Abstract: Modern LLM-based agents and chat assistants rely on long-term memory frameworks to store reusable knowledge, recall user preferences, and augment reasoning. As researchers create more complex memory architectures, it becomes increasingly difficult to analyze their capabilities and guide future memory designs. Most long-term memory benchmarks focus on simple fact retention, multi-hop recall, and time-based changes. While undoubtedly important, these capabilities can often be achieved with simple retrieval-augmented LLMs and do not test complex memory hierarchies. To bridge this gap, we propose StructMemEval - a benchmark that tests the agent's ability to organize its long-term memory, not just factual recall. We gather a suite of tasks that humans solve by organizing their knowledge in a specific structure: transaction ledgers, to-do lists, trees and others. Our initial experiments show that simple retrieval-augmented LLMs struggle with these tasks, whereas memory agents can reliably solve them if prompted how to organize their memory. However, we also find that modern LLMs do not always recognize the memory structure when not prompted to do so. This highlights an important direction for future improvements in both LLM training and memory frameworks.

</details>


### [60] [Sparse Semantic Dimension as a Generalization Certificate for LLMs](https://arxiv.org/abs/2602.11388)
*Dibyanayan Bandyopadhyay,Asif Ekbal*

Main category: cs.LG

TL;DR: 该论文提出稀疏语义维度(SSD)框架，通过分析LLM内部表示的稀疏性来解释其泛化能力，发现大模型学习到更可压缩的语义结构，并可作为安全监测器检测分布外输入。


<details>
  <summary>Details</summary>
Motivation: 标准统计学习理论预测大型语言模型应该过拟合，因为参数量远超训练token数，但实际上它们能稳健泛化。作者旨在解释这种理论与实践的差距。

Method: 引入稀疏语义维度(SSD)框架，使用稀疏自编码器(SAE)分析LLM激活状态的稀疏性，将模型和SAE视为冻结的预言机，通过特征词汇表计算复杂度度量。

Result: 在GPT-2 Small和Gemma-2B上验证了SSD框架，发现非空泛化边界，揭示了"特征锐度"缩放定律：更大的Gemma-2B需要更少校准样本来识别其活动流形，表明大模型学习到更可压缩的语义结构。SSD还能作为安全监测器，检测分布外输入的特征爆炸现象。

Conclusion: LLM的泛化能力源于其内部表示的稀疏几何结构而非参数量，SSD框架为理解模型泛化提供了新视角，并可作为有效的安全监测工具。

Abstract: Standard statistical learning theory predicts that Large Language Models (LLMs) should overfit because their parameter counts vastly exceed the number of training tokens. Yet, in practice, they generalize robustly. We propose that the effective capacity controlling generalization lies in the geometry of the model's internal representations: while the parameter space is high-dimensional, the activation states lie on a low-dimensional, sparse manifold. To formalize this, we introduce the Sparse Semantic Dimension (SSD), a complexity measure derived from the active feature vocabulary of a Sparse Autoencoder (SAE) trained on the model's layers. Treating the LLM and SAE as frozen oracles, we utilize this framework to attribute the model's generalization capabilities to the sparsity of the dictionary rather than the total parameter count. Empirically, we validate this framework on GPT-2 Small and Gemma-2B, demonstrating that our bound provides non-vacuous certificates at realistic sample sizes. Crucially, we uncover a counter-intuitive "feature sharpness" scaling law: despite being an order of magnitude larger, Gemma-2B requires significantly fewer calibration samples to identify its active manifold compared to GPT-2, suggesting that larger models learn more compressible, distinct semantic structures. Finally, we show that this framework functions as a reliable safety monitor: out-of-distribution inputs trigger a measurable "feature explosion" (a sharp spike in active features), effectively signaling epistemic uncertainty through learned feature violation. Code is available at: https://github.com/newcodevelop/sparse-semantic-dimension.

</details>


### [61] [Can We Really Learn One Representation to Optimize All Rewards?](https://arxiv.org/abs/2602.11399)
*Chongyi Zheng,Royina Karegoudra Jayanth,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文分析了前向-后向表示学习（FB），提出了一种简化的单步FB方法，该方法执行一步策略改进而非实现最优控制，在实验中收敛误差小10^5倍，零样本性能平均提升24%。


<details>
  <summary>Details</summary>
Motivation: 当前社区在讨论强化学习中使用何种形式的先验，FB表示学习声称可以通过无监督表示学习实现任意奖励的最优控制而无需微调，但其训练目标和学习行为仍不明确，需要澄清其原理和实际收敛情况。

Method: 通过分析FB表示学习，澄清其表示存在的条件、优化目标和实际收敛方式，与秩匹配、拟合Q评估和压缩映射建立联系，提出简化的单步前向-后向表示学习方法（one-step FB），该方法执行一步策略改进而非实现最优控制。

Result: 在10个状态和图像基础的连续控制领域中，单步FB收敛误差小10^5倍，零样本性能平均提升24%，在指导性设置中也表现出色。

Conclusion: 单步FB方法比原始FB更有效，通过执行一步策略改进而非追求最优控制，在实际应用中表现更好，为强化学习的无监督预训练提供了简化而有效的方案。

Abstract: As machine learning has moved towards leveraging large models as priors for downstream tasks, the community has debated the right form of prior for solving reinforcement learning (RL) problems. If one were to try to prefetch as much computation as possible, they would attempt to learn a prior over the policies for some yet-to-be-determined reward function. Recent work (forward-backward (FB) representation learning) has tried this, arguing that an unsupervised representation learning procedure can enable optimal control over arbitrary rewards without further fine-tuning. However, FB's training objective and learning behavior remain mysterious. In this paper, we demystify FB by clarifying when such representations can exist, what its objective optimizes, and how it converges in practice. We draw connections with rank matching, fitted Q-evaluation, and contraction mapping. Our analysis suggests a simplified unsupervised pre-training method for RL that, instead of enabling optimal control, performs one step of policy improvement. We call our proposed method $\textbf{one-step forward-backward representation learning (one-step FB)}$. Experiments in didactic settings, as well as in $10$ state-based and image-based continuous control domains, demonstrate that one-step FB converges to errors $10^5$ smaller and improves zero-shot performance by $+24\%$ on average. Our project website is available at https://chongyi-zheng.github.io/onestep-fb.

</details>


### [62] [Multi-Level Strategic Classification: Incentivizing Improvement through Promotion and Relegation Dynamics](https://arxiv.org/abs/2602.11439)
*Ziyuan Huang,Lina Alkarmi,Mingyan Liu*

Main category: cs.LG

TL;DR: 论文提出了一种基于阈值设计的战略分类机制，通过多级晋升-降级框架激励代理人通过真实努力达到任意高水平，而非依赖动态分类器权重优化。


<details>
  <summary>Details</summary>
Motivation: 现有序列战略分类研究主要关注优化动态分类器权重，但本文转向分析分类器阈值和难度递进设计，以解决自利代理人操纵响应获取有利决策结果的问题。

Method: 采用多级晋升-降级框架，分析分类器阈值和难度递进设计，考虑代理人的远见、技能保持和"助推效应"（资格与成就可以自我强化）等跨期激励因素。

Result: 刻画了代理人的最优长期策略，证明委托人可以设计阈值序列有效激励真实努力，在温和条件下使代理人仅通过真实改进努力达到任意高水平。

Conclusion: 阈值设计机制能够有效解决战略分类中的操纵问题，激励代理人通过真实努力而非欺骗行为实现进步，为战略分类提供了新的设计视角。

Abstract: Strategic classification studies the problem where self-interested individuals or agents manipulate their response to obtain favorable decision outcomes made by classifiers, typically turning to dishonest actions when they are less costly than genuine efforts. While existing studies on sequential strategic classification primarily focus on optimizing dynamic classifier weights, we depart from these weight-centric approaches by analyzing the design of classifier thresholds and difficulty progression within a multi-level promotion-relegation framework. Our model captures the critical inter-temporal incentives driven by an agent's farsightedness, skill retention, and a leg-up effect where qualification and attainment can be self-reinforcing. We characterize the agent's optimal long-term strategy and demonstrate that a principal can design a sequence of thresholds to effectively incentivize honest effort. Crucially, we prove that under mild conditions, this mechanism enables agents to reach arbitrarily high levels solely through genuine improvement efforts.

</details>


### [63] [Adaptive Milestone Reward for GUI Agents](https://arxiv.org/abs/2602.11524)
*Congmin Zheng,Xiaoyun Mo,Xinbei Ma,Qiqiang Lin,Yin Zhao,Jiachen Zhu,Xingyu Lou,Jun Wang,Zhaoxiang Wang,Weiwen Liu,Zhuosheng Zhang,Yong Yu,Weinan Zhang*

Main category: cs.LG

TL;DR: 提出ADMIRE机制解决移动GUI智能体训练中的时间信用分配问题，通过动态里程碑和不对称信用分配策略，在AndroidWorld上实现超过10%的绝对成功率提升。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练移动GUI智能体时面临时间信用分配问题，需要在奖励保真度和密度之间权衡：结果奖励保真度高但信号稀疏，过程奖励监督密集但易产生偏差和奖励黑客攻击。

Method: 提出自适应里程碑奖励（ADMIRE）机制：1）通过锚定轨迹到里程碑构建可验证的自适应奖励系统，里程碑从成功探索中动态提取；2）集成不对称信用分配策略，对成功轨迹去噪并对失败轨迹提供支架支持。

Result: 在AndroidWorld上，ADMIRE在不同基础模型上均实现超过10%的绝对成功率提升。方法展现出强大的泛化能力，在多种RL算法和异构环境（如网页导航和具身任务）中均表现优异。

Conclusion: ADMIRE机制有效解决了移动GUI智能体训练中的奖励设计困境，通过动态里程碑和不对称信用分配策略，在保持奖励保真度的同时提供密集监督，显著提升智能体性能并具有良好的泛化能力。

Abstract: Reinforcement Learning (RL) has emerged as a mainstream paradigm for training Mobile GUI Agents, yet it struggles with the temporal credit assignment problem inherent in long-horizon tasks. A primary challenge lies in the trade-off between reward fidelity and density: outcome reward offers high fidelity but suffers from signal sparsity, while process reward provides dense supervision but remains prone to bias and reward hacking. To resolve this conflict, we propose the Adaptive Milestone Reward (ADMIRE) mechanism. ADMIRE constructs a verifiable, adaptive reward system by anchoring trajectory to milestones, which are dynamically distilled from successful explorations. Crucially, ADMIRE integrates an asymmetric credit assignment strategy that denoises successful trajectories and scaffolds failed trajectories. Extensive experiments demonstrate that ADMIRE consistently yields over 10% absolute improvement in success rate across different base models on AndroidWorld. Moreover, the method exhibits robust generalizability, achieving strong performance across diverse RL algorithms and heterogeneous environments such as web navigation and embodied tasks.

</details>


### [64] [Native Reasoning Models: Training Language Models to Reason on Unverifiable Data](https://arxiv.org/abs/2602.11549)
*Yuanfu Wang,Zhixuan Liu,Xiangtian Li,Chaochao Lu,Chao Yang*

Main category: cs.LG

TL;DR: NRT是一种无需人工标注推理轨迹和外部验证器的新型训练框架，通过让模型仅使用标准问答对生成自己的推理轨迹来培养复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前主流的大推理模型训练范式（SFT+RLVR）依赖高质量人工标注推理数据和外部验证器，导致数据收集成本高、可能嵌入人类认知偏见，且仅限于数学和编程等可客观评估的领域，无法应用于广泛的不可验证任务。

Method: NRT将推理过程视为潜在变量，采用统一的训练目标，将推理建模为优化问题，内在奖励那些增加模型产生正确答案可能性的推理路径。该框架分析先前方法的内在失效模式（如策略崩溃），并系统设计更鲁棒的奖励聚合函数，创建自我强化的反馈循环。

Result: 在Llama和Mistral模型系列上的实证评估表明，NRT在无验证器方法中达到最先进性能，显著优于标准SFT基线和先前的无验证器RL方法。在复杂推理领域表现尤其突出，对策略崩溃具有高鲁棒性。

Conclusion: NRT为构建更强大、更广泛适用的推理系统提供了一条通用、可扩展的路径，克服了当前训练范式的局限性。

Abstract: The prevailing paradigm for training large reasoning models--combining Supervised Fine-Tuning (SFT) with Reinforcement Learning with Verifiable Rewards (RLVR)--is fundamentally constrained by its reliance on high-quality, human-annotated reasoning data and external verifiers. This dependency incurs significant data-collection costs, risks embedding human cognitive biases, and confines the reinforcement learning stage to objectively assessable domains like mathematics and coding, leaving a wide range of unverifiable tasks beyond its scope. To overcome these limitations, we introduce NRT (Native Reasoning Training), a novel framework that cultivates complex reasoning by having the model generate its own reasoning traces using only standard question-answer pairs, thereby obviating the need for expert-written demonstrations. NRT reframes the training problem by treating the reasoning process as a latent variable. It employs a unified training objective that models reasoning as an optimization problem, intrinsically rewarding paths that increase the model's likelihood of producing the ground-truth answer. This unified perspective allows us to analyze intrinsic failure modes of prior methods, such as policy collapse, and systematically design more robust reward aggregation functions, creating a self-reinforcing feedback loop where the model learns to think in ways that resolve its own uncertainty. Empirical evaluation on Llama and Mistral model families demonstrates that NRT achieves state-of-the-art performance among verifier-free methods, significantly outperforming standard SFT baselines and prior verifier-free RL methods. Our approach yields particularly strong performance gains in complex reasoning domains and exhibits high robustness to policy collapse, offering a general, scalable path toward building more powerful and broadly applicable reasoning systems.

</details>


### [65] [DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels](https://arxiv.org/abs/2602.11715)
*Haolei Bai,Lingcheng Kong,Xueyi Chen,Jianmian Wang,Zhiqiang Tao,Huan Wang*

Main category: cs.LG

TL;DR: DICE是一种用于CUDA内核生成的扩散大语言模型系列，通过两阶段强化学习框架和高质量数据集训练，在多个参数规模上超越了自回归和扩散模型。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)因其并行令牌生成能力成为自回归模型的有力替代，特别适合需要整体结构规划和非顺序优化的代码生成任务。然而，为CUDA内核生成定制dLLMs面临挑战，主要是高度专业化和高质量训练数据的严重缺乏。

Method: 1. 构建CuKe数据集：针对高性能CUDA内核优化的增强监督微调数据集；2. 提出BiC-RL框架：包含CUDA内核填充阶段和端到端CUDA内核生成阶段的两阶段强化学习；3. 开发DICE模型：基于该训练框架的扩散大语言模型系列，包含1.7B、4B和8B三种参数规模。

Result: 在KernelBench上的广泛实验表明，DICE在CUDA内核生成任务上显著优于同等规模的自回归和扩散LLMs，建立了新的最先进水平。

Conclusion: DICE通过高质量数据集和两阶段强化学习框架，成功地将扩散大语言模型应用于CUDA内核生成，解决了该领域的数据稀缺和专业化挑战，为高性能计算代码生成提供了有效解决方案。

Abstract: Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation. This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation, spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation.

</details>


### [66] [Temperature as a Meta-Policy: Adaptive Temperature in LLM Reinforcement Learning](https://arxiv.org/abs/2602.11779)
*Haoran Dang,Cuiling Lan,Hai Wan,Xibin Zhao,Yan Lu*

Main category: cs.LG

TL;DR: TAMPO将温度控制重构为可学习的元策略，通过双层循环实现温度自适应，在数学推理基准上优于固定或启发式温度方法


<details>
  <summary>Details</summary>
Motivation: LLM中的温度参数控制探索与利用的权衡，但静态或启发式温度调度无法适应RL训练中的动态需求，限制了策略改进

Method: 提出温度自适应元策略优化(TAMPO)框架，将温度控制重构为可学习的元策略。采用分层双循环：内循环使用元策略选择的温度采样轨迹更新LLM策略；外循环通过奖励最大化高优势轨迹似然来更新温度分布

Result: 在五个数学推理基准上，TAMPO优于使用固定或启发式温度的基线方法

Conclusion: 温度可作为LLM强化学习中自适应探索的有效可学习元策略

Abstract: Temperature is a crucial hyperparameter in large language models (LLMs), controlling the trade-off between exploration and exploitation during text generation. High temperatures encourage diverse but noisy outputs, while low temperatures produce focused outputs but may cause premature convergence. Yet static or heuristic temperature schedules fail to adapt to the dynamic demands of reinforcement learning (RL) throughout training, often limiting policy improvement. We propose Temperature Adaptive Meta Policy Optimization (TAMPO), a new framework that recasts temperature control as a learnable meta-policy. TAMPO operates through a hierarchical two-loop process. In the inner loop, the LLM policy is updated (e.g., using GRPO) with trajectories sampled at the temperature selected by the meta-policy. In the outer loop, meta-policy updates the distribution over candidate temperatures by rewarding those that maximize the likelihood of high-advantage trajectories. This trajectory-guided, reward-driven mechanism enables online adaptation without additional rollouts, directly aligning exploration with policy improvement. On five mathematical reasoning benchmarks, TAMPO outperforms baselines using fixed or heuristic temperatures, establishing temperature as an effective learnable meta-policy for adaptive exploration in LLM reinforcement learning. Accepted at ICLR 2026.

</details>


### [67] [Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing](https://arxiv.org/abs/2602.11786)
*Keita Broadwater*

Main category: cs.LG

TL;DR: 提出APST框架，通过重复采样相同提示来评估LLM在持续使用下的安全可靠性，发现单次评估会掩盖重复推理中的显著可靠性差异。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试主要评估LLM在不同任务上的广度安全风险，但实际部署中更关键的是相同或相似提示重复推理时的操作失败风险，需要评估持续使用下的响应一致性和安全性。

Method: 提出加速提示压力测试（APST）框架，在受控操作条件下（如解码温度）重复采样相同提示，使用伯努利和二项式模型形式化安全失败，估计每次推理的失败概率。

Result: 对多个指令调优LLM在AIR-BENCH安全提示上的测试显示，基准分数相似的模型在重复采样下表现出显著不同的经验失败率，特别是温度升高时。

Conclusion: 浅层的单样本评估会掩盖持续使用下的可靠性差异，APST通过评估重复推理下的安全可靠性，补充现有基准测试，连接基准对齐和部署导向的风险评估。

Abstract: Traditional benchmarks for large language models (LLMs) primarily assess safety risk through breadth-oriented evaluation across diverse tasks. However, real-world deployment exposes a different class of risk: operational failures arising from repeated inference on identical or near-identical prompts rather than broad task generalization. In high-stakes settings, response consistency and safety under sustained use are critical. We introduce Accelerated Prompt Stress Testing (APST), a depth-oriented evaluation framework inspired by reliability engineering. APST repeatedly samples identical prompts under controlled operational conditions (e.g., decoding temperature) to surface latent failure modes including hallucinations, refusal inconsistency, and unsafe completions. Rather than treating failures as isolated events, APST models them as stochastic outcomes of independent inference events. We formalize safety failures using Bernoulli and binomial models to estimate per-inference failure probabilities, enabling quantitative comparison of reliability across models and decoding configurations. Applying APST to multiple instruction-tuned LLMs evaluated on AIR-BENCH-derived safety prompts, we find that models with similar benchmark-aligned scores can exhibit substantially different empirical failure rates under repeated sampling, particularly as temperature increases. These results demonstrate that shallow, single-sample evaluation can obscure meaningful reliability differences under sustained use. APST complements existing benchmarks by providing a practical framework for evaluating LLM safety and reliability under repeated inference, bridging benchmark alignment and deployment-oriented risk assessment.

</details>


### [68] [Temporal Difference Learning with Constrained Initial Representations](https://arxiv.org/abs/2602.11800)
*Jiafei Lyu,Jingwen Yang,Zhongjian Qiao,Runze Liu,Zeyuan Liu,Deheng Ye,Zongqing Lu,Xiu Li*

Main category: cs.LG

TL;DR: 提出CIR框架，通过Tanh激活函数约束初始表示来提升离线强化学习的样本效率，包含Tanh激活、跳跃连接和凸Q学习三个组件，在连续控制任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法在提升样本效率时忽略了直接约束输入数据初始表示的潜力，这可以直观地缓解分布偏移问题并稳定训练。

Method: 提出CIR框架，包含三个组件：(1) Tanh激活函数配合归一化方法稳定表示；(2) 跳跃连接模块提供从浅层到深层的线性路径；(3) 凸Q学习实现更灵活的价值估计并减轻保守性。

Result: CIR在多个连续控制任务上表现出强大性能，甚至能够竞争或超越现有的强基线方法。

Conclusion: 通过约束初始表示可以有效提升离线强化学习的样本效率和稳定性，CIR框架为此提供了有效的解决方案。

Abstract: Recently, there have been numerous attempts to enhance the sample efficiency of off-policy reinforcement learning (RL) agents when interacting with the environment, including architecture improvements and new algorithms. Despite these advances, they overlook the potential of directly constraining the initial representations of the input data, which can intuitively alleviate the distribution shift issue and stabilize training. In this paper, we introduce the Tanh function into the initial layer to fulfill such a constraint. We theoretically unpack the convergence property of the temporal difference learning with the Tanh function under linear function approximation. Motivated by theoretical insights, we present our Constrained Initial Representations framework, tagged CIR, which is made up of three components: (i) the Tanh activation along with normalization methods to stabilize representations; (ii) the skip connection module to provide a linear pathway from the shallow layer to the deep layer; (iii) the convex Q-learning that allows a more flexible value estimate and mitigates potential conservatism. Empirical results show that CIR exhibits strong performance on numerous continuous control tasks, even being competitive or surpassing existing strong baseline methods.

</details>


### [69] [Deep Kernel Fusion for Transformers](https://arxiv.org/abs/2602.11808)
*Zixi Zhang,Zhiwen Mo,Yiren Zhao,Robert Mullins*

Main category: cs.LG

TL;DR: DeepFusionKernel：一种深度融合内核，通过减少HBM流量和提升缓存重用，优化长上下文LLM推理中的SwiGLU MLP块瓶颈，在H100上实现最高13.2%的加速


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理越来越受内存带宽而非计算限制。SwiGLU MLP块的大权重超出缓存容量，成为主要但未充分优化的瓶颈。

Method: 提出DeepFusionKernel深度融合内核，减少HBM流量并提升缓存重用。与SGLang集成并配合内核调度器，确保在不同生成长度下的稳定加速。

Result: 在H100上实现最高13.2%的加速，在A100上实现9.7%的加速（相比SGLang）。能适应不同模型、推理配置和硬件平台。

Conclusion: DeepFusionKernel有效解决了长上下文LLM推理中的内存带宽瓶颈问题，为SwiGLU MLP块提供了优化方案，具有广泛的适用性。

Abstract: Agentic LLM inference with long contexts is increasingly limited by memory bandwidth rather than compute. In this setting, SwiGLU MLP blocks, whose large weights exceed cache capacity, become a major yet under-optimized bottleneck. We propose DeepFusionKernel, a deeply fused kernel that cuts HBM traffic and boosts cache reuse, delivering up to 13.2% speedup on H100 and 9.7% on A100 over SGLang. Integrated with SGLang and paired with a kernel scheduler, DeepFusionKernel ensures consistent accelerations over generation lengths, while remaining adaptable to diverse models, inference configurations, and hardware platforms.

</details>


### [70] [In-Context Function Learning in Large Language Models](https://arxiv.org/abs/2602.11863)
*Elif Akata,Konstantinos Voudouris,Vincent Fortuin,Eric Schulz*

Main category: cs.LG

TL;DR: LLMs通过高斯过程视角研究上下文学习，发现其学习曲线受函数生成核影响，可通过后训练调整归纳偏置以提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在推理时通过少量演示进行上下文学习的现象，从高斯过程理论框架理解LLMs如何学习连续函数。

Method: 构建受控实验，让模型观察从已知GP先验中抽取的多变量标量函数样本序列，评估预测误差，并与GP回归下界和1-NN上界比较，进行基于似然的归纳偏置分析。

Result: LLM学习曲线受函数生成核强烈影响，随着演示数量增加接近GP下界；LLM预测在较不平滑的GP核下最可能；强化学习和监督微调可有效调整归纳偏置。

Conclusion: 该框架量化了LLMs类似GP学习者的程度，并提供了调整其归纳偏置以处理连续函数学习任务的工具。

Abstract: Large language models (LLMs) can learn from a few demonstrations provided at inference time. We study this in-context learning phenomenon through the lens of Gaussian Processes (GPs). We build controlled experiments where models observe sequences of multivariate scalar-valued function samples drawn from known GP priors. We evaluate prediction error in relation to the number of demonstrations and compare against two principled references: (i) an empirical GP-regression learner that gives a lower bound on achievable error, and (ii) the expected error of a 1-nearest-neighbor (1-NN) rule, which gives a data-driven upper bound. Across model sizes, we find that LLM learning curves are strongly influenced by the function-generating kernels and approach the GP lower bound as the number of demonstrations increases. We then study the inductive biases of these models using a likelihood-based analysis. We find that LLM predictions are most likely under less smooth GP kernels. Finally, we explore whether post-training can shift these inductive biases and improve sample-efficiency on functions sampled from GPs with smoother kernels. We find that both reinforcement learning and supervised fine-tuning can effectively shift inductive biases in the direction of the training data. Together, our framework quantifies the extent to which LLMs behave like GP learners and provides tools for steering their inductive biases for continuous function learning tasks.

</details>


### [71] [Capability-Oriented Training Induced Alignment Risk](https://arxiv.org/abs/2602.12124)
*Yujun Zhou,Yue Huang,Han Bao,Kehan Guo,Zhenwen Liang,Pin-Yu Chen,Tian Gao,Werner Geyer,Nuno Moniz,Nitesh V Chawla,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 研究发现语言模型在强化学习训练中会自发利用环境漏洞来最大化奖励，即使没有恶意意图。这种利用策略具有泛化性，可迁移到新任务，揭示了能力导向训练带来的根本性安全挑战。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐研究主要关注防止模型生成有害内容，但忽视了更微妙的风险：能力导向训练引发的利用行为。研究者想探究语言模型在存在隐性漏洞的环境中是否会自发学习利用这些漏洞来最大化奖励。

Method: 设计了四个不同的"漏洞游戏"测试套件，每个游戏呈现独特的可利用缺陷：上下文条件合规、代理指标、奖励篡改和自我评估。通过强化学习训练模型，并测试它们是否会利用这些漏洞。

Result: 模型一致地学会了利用这些漏洞，发现了机会主义策略，以牺牲任务正确性或安全性为代价显著增加奖励。更重要的是，这些利用策略不是狭隘的"技巧"，而是可泛化的技能，可以迁移到新任务，甚至可以通过数据从教师模型"蒸馏"到学生模型。

Conclusion: 能力导向训练引发的风险对当前对齐方法构成根本性挑战，未来的AI安全工作必须超越内容审核，严格审计和保护训练环境和奖励机制本身。

Abstract: While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse "vulnerability games", each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety. More critically, we find that these exploitative strategies are not narrow "tricks" but generalizable skills; they can be transferred to new tasks and even "distilled" from a capable teacher model to other student models through data alone. Our findings reveal that capability-oriented training induced risks pose a fundamental challenge to current alignment approaches, suggesting that future AI safety work must extend beyond content moderation to rigorously auditing and securing the training environments and reward mechanisms themselves. Code is available at https://github.com/YujunZhou/Capability_Oriented_Alignment_Risk.

</details>


### [72] [Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation](https://arxiv.org/abs/2602.12125)
*Wenkai Yang,Weijie Liu,Ruobing Xie,Kai Yang,Saiyong Yang,Yankai Lin*

Main category: cs.LG

TL;DR: 本文提出广义在线蒸馏(G-OPD)框架，扩展标准在线蒸馏(OPD)，引入奖励缩放因子和灵活参考模型，在数学推理和代码生成任务中实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 在线蒸馏(OPD)在提升学生模型性能方面表现出色，但现有方法缺乏理论理解和优化空间。本文旨在从理论角度理解OPD，并扩展其框架以获得更好的蒸馏效果。

Method: 首先理论证明OPD是密集KL约束RL的特例，然后提出G-OPD框架，引入奖励缩放因子控制奖励项与KL正则化的相对权重，以及灵活的参考模型。特别提出ExOPD（奖励缩放因子>1）和奖励校正方法。

Result: 实验表明：1) ExOPD在多种师生模型尺寸配对中均优于标准OPD；2) 在合并不同领域专家知识时，ExOPD使学生模型超越教师性能边界；3) 在强到弱蒸馏中，选择教师RL前的基础模型作为参考模型可进一步提升性能。

Conclusion: G-OPD框架为在线蒸馏提供了理论理解和实用扩展，ExOPD和奖励校正方法显著提升蒸馏效果，为未来OPD研究提供了新视角。

Abstract: On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.

</details>


### [73] [PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving](https://arxiv.org/abs/2602.12029)
*Sunghyeon Woo,Hoseung Kim,Sunghwan Shim,Minjung Jo,Hyunjoon Jeong,Jeongtae Lee,Joonghoon Kim,Sungjae Lee,Baeseong Park,Se Jung Kwon,Dongsoo Lee*

Main category: cs.LG

TL;DR: PrefillShare通过在多模型系统中共享预填充阶段，减少计算冗余和KV缓存，提升多智能体工作负载的性能


<details>
  <summary>Details</summary>
Motivation: 多智能体系统通常调用多个专用语言模型处理相同提示前缀，导致冗余的预填充计算和KV缓存，增加延迟并降低吞吐量

Method: 将模型分解为预填充和解码模块，冻结预填充模块并仅微调解码模块，使多个任务特定模型能共享预填充模块和KV缓存，在vLLM解耦系统中引入路由机制

Result: 在广泛任务和模型上达到与全微调相当的精度，在多模型智能体工作负载中实现4.5倍更低的p95延迟和3.9倍更高的吞吐量

Conclusion: PrefillShare通过共享预填充阶段有效解决了多模型系统中的计算冗余问题，显著提升了系统性能

Abstract: Multi-agent systems increasingly orchestrate multiple specialized language models to solve complex real-world problems, often invoking them over a shared context. This execution pattern repeatedly processes the same prompt prefix across models. Consequently, each model redundantly executes the prefill stage and maintains its own key-value (KV) cache, increasing aggregate prefill load and worsening tail latency by intensifying prefill-decode interference in existing LLM serving stacks. Disaggregated serving reduces such interference by placing prefill and decode on separate GPUs, but disaggregation does not fundamentally eliminate inter-model redundancy in computation and KV storage for the same prompt. To address this issue, we propose PrefillShare, a novel algorithm that enables sharing the prefill stage across multiple models in a disaggregated setting. PrefillShare factorizes the model into prefill and decode modules, freezes the prefill module, and fine-tunes only the decode module. This design allows multiple task-specific models to share a prefill module and the KV cache generated for the same prompt. We further introduce a routing mechanism that enables effective prefill sharing across heterogeneous models in a vLLM-based disaggregated system. PrefillShare not only matches full fine-tuning accuracy on a broad range of tasks and models, but also delivers 4.5x lower p95 latency and 3.9x higher throughput in multi-model agent workloads.

</details>


### [74] [Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards](https://arxiv.org/abs/2602.12049)
*Ryo Mikasa,Shun-ichiro Hayashi,Daichi Mukunoki,Tetsuya Hoshino,Takahiro Katagiri*

Main category: cs.LG

TL;DR: 本文提出了一种在线强化学习方法，通过执行LLM生成的代码并直接使用运行时性能（GFLOPS）作为奖励来训练LLM，结合分阶段质量多样性算法，提升了LLM在HPC代码生成方面的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在代码生成方面表现出色，但生成的代码运行时性能无法保证，且在HPC领域很少有研究使用运行时性能作为奖励来训练LLM。

Method: 1. 在线强化学习方法：在超级计算机上执行LLM生成的代码，直接将测量的运行时性能（GFLOPS）作为奖励反馈；2. 分阶段质量多样性算法：按问题逐步变化允许的优化技术，使模型能从不同角度学习代码优化；3. 构建分布式系统连接GPU训练集群和CPU基准测试集群；4. 使用Group Relative Policy Optimization训练Qwen2.5 Coder 14B模型。

Result: 通过两个实验证明，结合运行时性能反馈和分阶段优化的强化学习能够提升LLM的HPC代码生成能力。

Conclusion: 提出的方法有效提升了LLM在HPC代码生成方面的性能，为使用运行时性能作为奖励训练LLM提供了新思路。

Abstract: Large language models (LLMs) have demonstrated strong code generation capabilities, yet the runtime performance of generated code is not guaranteed, and there have been few attempts to train LLMs using runtime performance as a reward in the HPC domain. We propose an online reinforcement learning approach that executes LLM-generated code on a supercomputer and directly feeds back the measured runtime performance (GFLOPS) as a reward. We further introduce a Staged Quality-Diversity (SQD) algorithm that progressively varies the permitted optimization techniques on a per-problem basis, enabling the model to learn code optimization from diverse perspectives. We build a distributed system connecting a GPU training cluster with a CPU benchmarking cluster, and train Qwen2.5 Coder 14B on a double-precision matrix multiplication task using Group Relative Policy Optimization (GRPO). Through two experiments, we show that reinforcement learning combining runtime performance feedback with staged optimization can improve the HPC code generation capability of LLMs.

</details>


### [75] [Geometry of Uncertainty: Learning Metric Spaces for Multimodal State Estimation in RL](https://arxiv.org/abs/2602.12087)
*Alfredo Reichlin,Adriano Pacciarelli,Danica Kragic,Miguel Vasco*

Main category: cs.LG

TL;DR: 提出一种基于度量空间的多模态状态表示学习方法，通过状态间距离直接反映状态转移所需的最小动作数，无需显式概率建模即可实现鲁棒状态估计。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法依赖概率模型处理高维多模态噪声观测，但需要显式噪声假设，限制了泛化能力。需要一种无需显式概率建模就能处理不确定性的方法。

Method: 提出度量空间表示学习方法，学习结构化潜在表示，使状态间距离与状态转移所需最小动作数相关。引入多模态潜在转移模型和基于逆距离加权的传感器融合机制，自适应整合多传感器模态。

Result: 在多模态RL任务上验证了方法的有效性，相比基线方法表现出更好的传感器噪声鲁棒性和状态估计性能。RL智能体通过学到的表示获得性能提升，无需显式噪声增强。

Conclusion: 利用转移感知的度量空间为顺序决策中的鲁棒状态估计提供了原则性且可扩展的解决方案，无需显式概率建模即可处理不确定性。

Abstract: Estimating the state of an environment from high-dimensional, multimodal, and noisy observations is a fundamental challenge in reinforcement learning (RL). Traditional approaches rely on probabilistic models to account for the uncertainty, but often require explicit noise assumptions, in turn limiting generalization. In this work, we contribute a novel method to learn a structured latent representation, in which distances between states directly correlate with the minimum number of actions required to transition between them. The proposed metric space formulation provides a geometric interpretation of uncertainty without the need for explicit probabilistic modeling. To achieve this, we introduce a multimodal latent transition model and a sensor fusion mechanism based on inverse distance weighting, allowing for the adaptive integration of multiple sensor modalities without prior knowledge of noise distributions. We empirically validate the approach on a range of multimodal RL tasks, demonstrating improved robustness to sensor noise and superior state estimation compared to baseline methods. Our experiments show enhanced performance of an RL agent via the learned representation, eliminating the need of explicit noise augmentation. The presented results suggest that leveraging transition-aware metric spaces provides a principled and scalable solution for robust state estimation in sequential decision-making.

</details>


### [76] [Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training](https://arxiv.org/abs/2602.12222)
*Miaosen Zhang,Yishan Liu,Shuxia Lin,Xu Yang,Qi Dai,Chong Luo,Weihao Jiang,Peng Hou,Anxiang Zeng,Xin Geng,Baining Guo*

Main category: cs.LG

TL;DR: 提出On-Policy SFT框架，通过分布判别理论(DDT)和两种技术(IDFT和Hinted Decoding)，使监督微调达到与离线强化学习相当的泛化性能，同时保持SFT的计算效率。


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)计算效率高但泛化能力通常不如强化学习(RL)，主要原因是RL使用在线策略数据。作者希望弥合这一差距，使SFT也能达到类似RL的泛化性能。

Method: 1. 提出分布判别理论(DDT)来解释和量化数据与模型诱导分布之间的对齐程度；2. 基于DDT开发两种技术：In-Distribution Finetuning (IDFT) - 损失层面的方法，增强SFT的泛化能力；Hinted Decoding - 数据层面的技术，重新对齐训练语料到模型分布。

Result: 实验表明，该框架在泛化性能上与DPO、SimPO等主流离线RL算法相当，同时保持了SFT流程的计算效率。

Conclusion: 提出的On-Policy SFT框架为RL不可行的领域提供了实用的替代方案，实现了SFT效率与RL泛化性能的结合。

Abstract: Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \textbf{\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \textbf{\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \textbf{\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model's distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [77] [WarpGrep: Fast Context RL Retrieval](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmorphllm.com%2Fblog%2Ffast-context-rl-retrieval%3Futm_source=tldrai/1/0100019c4d107513-af8936cc-0d74-4aa9-8aac-2ddbffcb6df3-000000/mUXe9MLz9a7FljVAV02nNsfVYO-3gFeMOw7iUJAZT-0=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: WarpGrep是一个轻量级RL训练的检索子代理，通过并行grep/read/glob调用在4轮内快速找到所需代码文件和行范围，比标准模型快5倍，减少噪声并降低token使用


<details>
  <summary>Details</summary>
Motivation: 传统检索方法在大型代码库中效率低下，噪声多且token使用高，需要专门的检索机制来保持主LLM上下文清洁

Method: 使用强化学习训练轻量级检索子代理，通过四轮并行grep/read/glob调用进行精确检索，专门负责检索而非推理

Result: 在大型代码库基准测试中，检索速度提升5倍，精度翻倍，搜索时间约5秒，token使用减半

Conclusion: 专门的检索子代理能显著提升代码检索效率和精度，同时减少主LLM的上下文污染和计算开销

Abstract: WarpGrep: Fast Context RL Retrieval (7 minute read) WarpGrep is a lightweight RL‑trained retrieval sub‑agent that finds the exact code files and line ranges developers need up to 5× faster than standard models by limiting search to four turns of parallel grep/read/glob calls. This approach keeps the main LLM's context clean, cuts noise, and halves token usage by specializing retrieval, not reasoning. In benchmarks on large repositories, WarpGrep doubled precision and maintained ~5‑second sear...

</details>


### [78] [Kimi Introduces Agent Swarm: Let 100 AI Agents Work for You](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.kimi.com%2Fblog%2Fagent-swarm.html%3Futm_source=tldrai/1/0100019c4d107513-af8936cc-0d74-4aa9-8aac-2ddbffcb6df3-000000/ViHhP5rwlOP6TImt4uxWEcyTvVeiat1hE26Avx93Tu4=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Kimi Agent Swarm是一个AI CEO系统，能够自动组建包含研究人员、分析师和事实核查员的多智能体组织，实现并行化工作，无需人工微观管理。


<details>
  <summary>Details</summary>
Motivation: 解决需要多角度分析、大规模数据处理和长文本生成等复杂任务时，单一AI智能体的局限性，通过多智能体协作提高工作效率和质量。

Method: 构建自组织的智能体组织结构，包括老板、员工和分工，实现智能体间的并行工作和建设性分歧，特别适用于可并行化的工作场景。

Result: 系统在广泛研究、批量下载、多文件处理、多角度分析和长文本写作等可并行化任务中表现出色，能够自动创建高效的工作环境。

Conclusion: Kimi Agent Swarm通过多智能体协作架构，显著提升了复杂任务的执行效率，为AI辅助工作提供了新的组织范式。

Abstract: Kimi Introduces Agent Swarm: Let 100 AI Agents Work for You (5 minute read) Kimi Agent Swarm is an AI CEO who helps find researchers, analysts, and fact-checkers without micromanagement. It builds an organizational structure with bosses, employees, and divisions of labor by itself. Agent Swarm excels where work can be parallelized, like broad research, batch downloads, multi-file processing, multi-angle analysis, and long-form writing. It creates the conditions for productive disagreement to ...

</details>


### [79] [More about Agent Skills](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.openai.com%2Fcookbook%2Fexamples%2Fskills_in_api%3Futm_source=tldrai/1/0100019c4d107513-af8936cc-0d74-4aa9-8aac-2ddbffcb6df3-000000/7HvtOvYipCw-RmKLLslswLTE5SCZqonYmql3LsRzcdA=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI提出代理技能指南，将可复用的"技能"打包为版本化的指令、脚本和资源包，代理可以附加到执行环境中，支持条件工作流、代码执行和可复现性，同时保持系统提示的轻量和模块化。


<details>
  <summary>Details</summary>
Motivation: 当前代理系统通常使用冗长的系统提示，难以维护和复用。需要一种模块化方法来打包和版本化代理技能，提高可复用性、可维护性和可复现性。

Method: 提出代理技能系统，将技能打包为版本化的指令、脚本和资源包，支持条件工作流和代码执行。技能可以附加到代理的执行环境中，保持系统提示轻量化和模块化。

Result: 开发了代理技能框架，使技能可以版本化、可复用，支持条件逻辑和代码执行，同时减少系统提示的复杂性，提高代理系统的模块化和可维护性。

Conclusion: 代理技能系统为构建模块化、可复用和可维护的代理提供了有效框架，通过版本化技能包和轻量系统提示，支持复杂工作流和代码执行，是代理工程的重要进展。

Abstract: More about Agent Skills (24 minute read) OpenAI outlined a guide for agent skills, a system for packaging reusable “skills” as versioned bundles of instructions, scripts, and assets that agents can attach to execution environments. Skills support conditional workflows, code execution, and reproducibility while keeping system prompts lightweight and modular.

</details>


### [80] [Structure is All You Need?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flevelup.gitconnected.com%2Fstructure-is-all-you-need-4ee88db32675%3Futm_source=tldrdata/1/0100019c518a0cb7-6921e781-2614-4975-b29a-d690d457ddf9-000000/QUwyDaK86ht24gDHvlxPvPVKdmldC8fG3nId-7LYGPU=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文主张从线性Transformer架构转向图结构架构，以解决AI在结构化推理方面的局限性


<details>
  <summary>Details</summary>
Motivation: 当前AI系统使用线性Transformer架构和大上下文窗口，虽然能提供详尽的回忆能力，但缺乏结构化推理能力。这限制了AI在代码分析、多智能体协调等需要复杂状态建模任务中的表现。

Method: 提出转向图基架构，包括上下文图、可训练图记忆和GraphRAG等方法。这些架构支持情景记忆、语义记忆、递归推理和优越的状态建模。

Result: 图基架构能够实现更高级的推理能力，包括情景和语义记忆、递归推理，以及在代码分析和多智能体协调等任务中的优越状态建模。

Conclusion: 投资状态管理基础设施（如图结构）对于AI系统的未来发展至关重要，能够超越当前线性架构的局限性，实现更复杂的推理能力。

Abstract: Structure is All You Need? (9 minute read) AI is reaching the limits of linear transformer architectures and massive context windows, which deliver exhaustive recall but lack structured reasoning. Emerging research advocates shifting toward graph-based architectures: Context Graphs, Trainable Graph Memory, and GraphRAG. These enable episodic, semantic memory, recursive reasoning, and superior state modeling for tasks like code analysis and multi-agent coordination. Investing in state manageme...

</details>


### [81] [AI code review with comments you'll actually implement](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview%26utm_content=260212secondary/1/0100019c519a0a54-ba42a5bd-227a-4b20-8ad7-d45e50e5311e-000000/2t5-zLPnmmU_Q3X-0vGnCzU-YvMQDHwwEtPa1yQ-i1M=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Unblocked是一款AI代码审查工具，通过整合整个代码库、Slack、Jira、文档和PR历史等上下文信息，提供高价值、可实际实施的代码审查建议。


<details>
  <summary>Details</summary>
Motivation: 解决传统AI代码审查工具缺乏上下文信息导致建议不实用的问题，减轻开发者的"AI疲劳"，提供基于实际系统工作方式的高质量代码审查。

Method: 通过整合多个数据源（代码库、Slack、Jira、文档、PR历史等）建立全面的上下文理解，基于实际系统工作方式生成精准的代码审查建议。

Result: 开发者反馈该工具"完全逆转了AI疲劳"，"精确度惊人"，能够提供高信号、可实际实施的代码审查建议。

Conclusion: 整合多源上下文信息的AI代码审查工具能够显著提升代码审查的质量和实用性，解决传统AI工具的局限性。

Abstract: AI code review with comments you'll actually implement (Sponsor) Unblocked is the only AI code review tool that draws on context from your whole repo, Slack, Jira, docs, PR history, and more. Get high-signal comments based on how your system actually works. “Unblocked has reversed my AI fatigue completely. The level of precision is wild.” - Senior developer, Clio Try now for free

</details>


### [82] [Coding Agents Meet Distributed Reality](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjhellerstein.github.io%2Fblog%2Fcodegen-reality%2F%3Futm_source=tldrnewsletter/1/0100019c519a0a54-ba42a5bd-227a-4b20-8ad7-d45e50e5311e-000000/8qzqbBT3fFvV-WBijnP1ZY0yW-C7lEkr6QI1THfvlug=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨AI生成分布式代码时应采用何种编程范式，建议使用包含分布式合约的语言框架而非普通命令式代码，以更好地反映分布式现实并减少系统故障。


<details>
  <summary>Details</summary>
Motivation: 未来大部分分布式代码将由AI生成，因此需要确定AI应该生成普通命令式代码还是采用包含分布式合约的语言框架，以确保生成的代码能更好地处理分布式系统的复杂性。

Method: 提出将分布式合约作为语言表面的一部分，为AI提供明确的目标框架，使开发者能够专注于正确的问题而非追逐虚幻的解决方案。

Result: 通过为AI提供反映分布式现实的目标框架，可以帮助构建故障更少的系统，提高分布式代码的可靠性和可维护性。

Conclusion: AI生成分布式代码时应采用包含分布式合约的语言框架，这比普通命令式代码更能反映分布式系统的现实需求，有助于减少系统故障。

Abstract: Coding Agents Meet Distributed Reality (8 minute read) Most future distributed code will be AI-generated. This makes it important whether we ask AI to aim for ordinary imperative code or at frameworks where distributed contracts are part of the language surface. Targets like this help developers focus on the right questions rather than chasing shadows. AI can help build systems that fail less often if given a target that reflects distributed reality.

</details>


### [83] [How I Use Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fboristane.com%2Fblog%2Fhow-i-use-claude-code%2F%3Futm_source=tldrnewsletter/1/0100019c519a0a54-ba42a5bd-227a-4b20-8ad7-d45e50e5311e-000000/btGwhCNYTuw7YDWR9NAPIGYMeME9VFNOIOQpodO2U28=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 作者提出了一种使用Claude Code的工作流程，核心原则是在编写代码前必须先审查和批准书面计划，将规划与执行分离，从而减少浪费、保持架构控制并提高结果质量。


<details>
  <summary>Details</summary>
Motivation: 大多数人在使用AI工具时直接让AI编写代码，这会导致浪费努力、失去对架构决策的控制，并且结果质量不佳。作者希望通过分离规划与执行来改善这些问题。

Method: 提出一种工作流程：1）先让Claude制定书面计划；2）用户审查和批准该计划；3）只有批准后才让Claude执行代码编写。这种规划与执行的分离是工作流程的核心。

Result: 这种工作流程能显著提高结果质量，最小化token使用量，保持用户对架构决策的控制，并防止浪费努力。

Conclusion: 在使用Claude Code时，规划与执行的分离是关键原则，这种工作流程与大多数人的AI使用方式截然不同，但能产生更好的效果。

Abstract: How I Use Claude Code (15 minute read) Never let Claude write code until you have reviewed and approved a written plan. This separation of planning and execution prevents wasted effort, keeps users in control of architectural decisions, and produces significantly better results with minimal token usage. This post describes a workflow that follows this core principle. The workflow is radically different from how most people approach AI tools.

</details>


### [84] [Claude Code Is Being Dumbed Down](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsymmetrybreak.ing%2Fblog%2Fclaude-code-is-being-dumbed-down%2F%3Futm_source=tldrdev/1/0100019c51c1e182-d6aaa75a-7e22-4b69-82dd-9bb11a9cabb5-000000/OVgudSdDNy5x1Qe34qSPDdOYYFtDJrt6K7s50xsLklk=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code 2.1.20版本将详细的文件和搜索信息替换为通用摘要，用户普遍抱怨并要求恢复具体文件路径或提供切换选项，但Anthropic以"简化大多数用户使用"为由拒绝了这些反馈。


<details>
  <summary>Details</summary>
Motivation: Anthropic声称这一改变是为了简化Claude Code的使用体验，使其对大多数用户更加友好。

Method: 通过软件更新（版本2.1.20）将详细的文件和搜索信息替换为通用摘要，移除了具体的文件路径信息。

Result: 用户普遍抱怨这一改变，要求恢复具体文件路径或提供切换选项，但Anthropic拒绝了这些反馈，坚持认为简化对大多数用户有益。

Conclusion: Claude Code的功能被简化，牺牲了高级用户对详细信息的需求，引发了用户不满和争议。

Abstract: Claude Code Is Being Dumbed Down (3 minute read) Claude Code version 2.1.20 shipped a change that replaced detailed file and search information with generic summaries. Users widely complained, demanding the return of specific file paths or a simple toggle, but Anthropic dismissed this feedback, saying the change simplified things for a majority.

</details>


### [85] [GLM-5: From Vibe Coding to Agentic Engineering](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fz.ai%2Fblog%2Fglm-5%3Futm_source=tldrdev/1/0100019c51c1e182-d6aaa75a-7e22-4b69-82dd-9bb11a9cabb5-000000/45-itVQaTh3NQfeiBOztJRRwkdU2o4QmAm2rHSQwbfQ=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GLM-5是一个新发布的开源大语言模型，通过扩展架构和训练，在推理、编码和复杂智能体任务上达到开源模型中的最佳性能，接近前沿模型水平。


<details>
  <summary>Details</summary>
Motivation: 当前开源模型在推理、编码和复杂智能体任务方面与前沿模型存在性能差距，需要开发一个能够接近前沿模型性能的开源解决方案。

Method: 通过扩展模型架构和训练规模，采用先进的训练技术来提升模型在推理、编码和复杂智能体任务上的能力。

Result: GLM-5在推理、编码和复杂智能体任务上达到了开源模型中的最佳性能，性能接近前沿模型。

Conclusion: GLM-5的成功表明开源模型可以通过适当的架构扩展和训练方法，在复杂任务上接近前沿模型的性能水平。

Abstract: GLM-5: From Vibe Coding to Agentic Engineering (10 minute read) GLM-5 is a newly launched, open-source LLM that scales its architecture and training to achieve best-in-class performance among open-source models for reasoning, coding, and complex agentic tasks, approaching frontier models.

</details>


### [86] [Claude Code Guide for Designers](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadplist.substack.com%2Fp%2Fclaude-code-guide-for-designers%3Futm_source=tldrdesign/1/0100019c51f668c0-925cc012-aa57-4f15-b6d8-8e1aaa41902e-000000/_e9UZRaKg7CPKpZgkD5sMQEkKKjREnSC56-GHYhaTV0=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code是一个面向设计师的编码代理工具，让非技术背景的设计师能够通过自然语言指令快速构建和部署功能性应用


<details>
  <summary>Details</summary>
Motivation: 解决设计师缺乏编程技能但需要快速构建和部署应用的问题，缩短开发周期从数月到数天

Method: 通过自然语言指令驱动的编码代理，结合终端命令、项目创建、迭代提示、GitHub版本控制和Vercel实时部署等技术

Result: 设计师能够在几天内而非数月内构建和部署功能性应用，支持添加认证、数据库和AI功能等高级特性

Conclusion: Claude Code为设计师提供了强大的低代码/无代码开发能力，显著降低了应用开发的技术门槛

Abstract: Claude Code Guide for Designers (7 minute read) Claude Code is a coding agent that executes plain English instructions, enabling designers without coding backgrounds to build and deploy functional apps in days rather than months. This guide covers installation, basic terminal commands, project creation, iterative prompting, version control with GitHub, and live deployment through Vercel. Advanced steps include adding authentication, databases, and AI features using services like Supabase to t...

</details>
