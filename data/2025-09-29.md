<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [tldr.article](#tldr.article) [Total: 5]
- [cs.LG](#cs.LG) [Total: 20]
- [wechat.article](#wechat.article) [Total: 40]
- [cs.SE](#cs.SE) [Total: 7]
- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding](https://arxiv.org/abs/2509.22237)
*Haorui Chen,Chengze Li,Jia Li*

Main category: cs.CL

TL;DR: 提出了FeatBench基准测试，专门评估在vibe coding范式下的功能实现能力，现有基准测试存在错位问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评估基准无法充分评估智能体在vibe coding范式下的能力，它们要么需要代码级规范，要么过于关注问题解决，忽视了功能实现这一关键场景。

Method: 构建FeatBench基准，具有纯自然语言提示、严谨的数据收集流程、全面的测试用例和多样化的应用领域等特征。

Result: 评估显示vibe coding范式下的功能实现具有显著挑战性，最高成功率仅为29.94%，分析发现了'激进实现'策略的悖论。

Conclusion: vibe coding范式下的功能实现是一个重要挑战，FeatBench为社区研究提供了有价值的基准和工具。

Abstract: The rapid advancement of Large Language Models (LLMs) has given rise to a
novel software development paradigm known as "vibe coding," where users
interact with coding agents through high-level natural language. However,
existing evaluation benchmarks for code generation inadequately assess an
agent's vibe coding capabilities. Existing benchmarks are misaligned, as they
either require code-level specifications or focus narrowly on issue-solving,
neglecting the critical scenario of feature implementation within the vibe
coding paradiam. To address this gap, we propose FeatBench, a novel benchmark
for vibe coding that focuses on feature implementation. Our benchmark is
distinguished by several key features: 1. Pure Natural Language Prompts. Task
inputs consist solely of abstract natural language descriptions, devoid of any
code or structural hints. 2. A Rigorous & Evolving Data Collection Process.
FeatBench is built on a multi-level filtering pipeline to ensure quality and a
fully automated pipeline to evolve the benchmark, mitigating data
contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass
(F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent
regressions. 4. Diverse Application Domains. The benchmark includes
repositories from diverse domains to ensure it reflects real-world scenarios.
We evaluate two state-of-the-art agent frameworks with four leading LLMs on
FeatBench. Our evaluation reveals that feature implementation within the vibe
coding paradigm is a significant challenge, with the highest success rate of
only 29.94%. Our analysis also reveals a tendency for "aggressive
implementation," a strategy that paradoxically leads to both critical failures
and superior software design. We release FeatBench, our automated collection
pipeline, and all experimental results to facilitate further community
research.

</details>


### [2] [Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs](https://arxiv.org/abs/2509.21361)
*Norman Paulsen*

Main category: cs.CL

TL;DR: 该论文提出了最大有效上下文窗口(MECW)的概念，测试了LLM在实际使用中的上下文窗口有效性，发现报告的MCW与MECW存在显著差异，大多数模型在1000个token内就出现严重性能下降。


<details>
  <summary>Details</summary>
Motivation: 测试LLM提供商宣传的最大上下文窗口在实际使用中的真实有效性，揭示报告值与实际有效值之间的差距。

Method: 1)定义最大有效上下文窗口概念；2)制定测试方法评估不同大小和问题类型的上下文窗口有效性；3)创建标准化比较方法寻找失效点；收集数十万个数据点测试多个模型。

Result: 发现MECW与MCW存在巨大差异，某些顶级模型在仅有100个token的上下文中就失败，大多数模型在1000个token内准确率严重下降，所有模型都远未达到其最大上下文窗口（差距高达99%）。

Conclusion: 最大有效上下文窗口随问题类型而变化，这为提高模型准确性和减少幻觉率提供了明确可行的见解。

Abstract: Large language model (LLM) providers boast big numbers for maximum context
window sizes. To test the real world use of context windows, we 1) define a
concept of maximum effective context window, 2) formulate a testing method of a
context window's effectiveness over various sizes and problem types, and 3)
create a standardized way to compare model efficacy for increasingly larger
context window sizes to find the point of failure. We collected hundreds of
thousands of data points across several models and found significant
differences between reported Maximum Context Window (MCW) size and Maximum
Effective Context Window (MECW) size. Our findings show that the MECW is, not
only, drastically different from the MCW but also shifts based on the problem
type. A few top of the line models in our test group failed with as little as
100 tokens in context; most had severe degradation in accuracy by 1000 tokens
in context. All models fell far short of their Maximum Context Window by as
much as 99 percent. Our data reveals the Maximum Effective Context Window
shifts based on the type of problem provided, offering clear and actionable
insights into how to improve model accuracy and decrease model hallucination
rates.

</details>


### [3] [A State-of-the-Art SQL Reasoning Model using RLVR](https://arxiv.org/abs/2509.21459)
*Alnur Ali,Ashutosh Baheti,Jonathan Chang,Ta-Chung Chi,Brandon Cui,Andrew Drozdov,Jonathan Frankle,Abhay Gupta,Pallavi Koppol,Sean Kulinski,Jonathan Li,Dipendra Misra,Krista Opsahl-Ong,Jose Javier Gonzalez Ortiz,Matei Zaharia,Yue Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于可验证奖励强化学习(RLVR)的简单通用训练框架，在BIRD数据科学基准测试中实现了最先进的SQL生成准确率，无需额外训练数据或专有模型。


<details>
  <summary>Details</summary>
Motivation: 开发能够整合组织特定知识的定制推理模型来解决企业客户面临的问题，特别是在奖励函数可验证的RLVR设置下。

Method: 采用简单的通用训练方法：精心设计提示和模型选择，使用离线RL方法TAO进行预热训练，然后进行严格的在线RLVR训练。

Result: 在BIRD排行榜上首次提交即达到私有测试集的最先进准确率：无自一致性73.56%，有自一致性75.68%，且后者需要更少的生成次数。

Conclusion: 虽然BIRD只是一个代理任务，但该框架的简单性使其广泛适用于商业智能、数据科学和编码等企业领域。

Abstract: Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.

</details>


### [4] [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)
*Adit Jain,Brendan Rappazzo*

Main category: cs.CL

TL;DR: 该论文提出在可验证奖励强化学习(RLVR)中应用混合令牌生成(MoT-G)方法，通过利用模型在候选令牌上的概率分布信息来改进推理能力，相比标准解码方法在多个推理任务上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法在采样离散令牌时丢弃了模型概率分布中的丰富信息，这限制了推理搜索空间。作者希望利用混合令牌生成来保留和利用这些分布信息，提升推理性能。

Method: 提出了一个统一的MoT-G框架，将RLVR扩展到连续混合空间中生成思维链。包括构建混合嵌入作为令牌嵌入的加权和，并在Reasoning-Gym推理任务套件上评估了两种MoT-G变体。

Result: 在10个任务中的7个上实现了5-35%的性能提升，使用一半轨迹数就能达到可比精度，表明训练效率提高。通过隐状态和令牌级分析发现MoT-G能维持更高的隐状态熵并促进令牌空间探索。

Conclusion: MoT-G方法在RLVR中能有效利用分布信息，显著提升推理性能并提高训练效率，其优势可能源于保持更高隐状态熵和促进探索的能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.

</details>


### [5] [On Code-Induced Reasoning in LLMs](https://arxiv.org/abs/2509.21499)
*Abdul Waheed,Zhen Wu,Carolyn Rosé,Daphne Ippolito*

Main category: cs.CL

TL;DR: 通过系统实验发现代码的结构特性比语义特性对LLM推理能力更重要，合适的抽象（如伪代码）可以替代真实代码，表面规律比语义准确性更能保持性能。


<details>
  <summary>Details</summary>
Motivation: 研究代码数据中哪些特性对提升大语言模型推理能力最关键，以指导训练数据设计。

Method: 构建10种编程语言的平行指令数据集，应用受控扰动破坏代码的结构或语义特性，在5个模型家族8个规模上进行了3,331次实验。

Result: LLM对结构扰动更敏感，伪代码和流程图等抽象与真实代码效果相当，表面规律保持时即使误导性代码也能保持竞争力，不同语言风格影响任务表现。

Conclusion: 代码的结构特性是提升LLM推理能力的关键因素，适当的抽象可以替代真实代码，这为设计训练数据提供了重要指导。

Abstract: Code data has been shown to enhance the reasoning capabilities of large
language models (LLMs), but it remains unclear which aspects of code are most
responsible. We investigate this question with a systematic, data-centric
framework. We construct parallel instruction datasets in ten programming
languages and apply controlled perturbations that selectively disrupt
structural or semantic properties of code. We then finetune LLMs from five
model families and eight scales on each variant and evaluate their performance
on natural language, math, and code tasks. Across 3,331 experiments, our
results show that LLMs are more vulnerable to structural perturbations than
semantic ones, particularly on math and code tasks. Appropriate abstractions
like pseudocode and flowcharts can be as effective as code, while encoding the
same information with fewer tokens without adhering to original syntax can
often retain or even improve performance. Remarkably, even corrupted code with
misleading signals remains competitive when surface-level regularities persist.
Finally, syntactic styles also shape task-specific gains with Python favoring
natural language reasoning and lower-level languages such as Java and Rust
favoring math. Through our systematic framework, we aim to provide insight into
how different properties of code influence reasoning and inform the design of
training data for enhancing LLM reasoning capabilities.

</details>


### [6] [Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective](https://arxiv.org/abs/2509.21613)
*Lingxiao Kong,Cong Yang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 本文提出了多目标强化学习在大型语言模型优化中的应用框架，探讨了不同MORL方法的优缺点，并提出了元策略MORL作为未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多目标强化学习在LLM优化中面临效率和灵活性挑战，需要能够适应个性化功能和LLM复杂性的方法。

Method: 提出了MORL分类法，分析各种MORL方法在LLM优化中的应用，并设计MORL基准框架来评估不同方法对目标关系的影响。

Result: 识别出现有MORL方法在LLM优化中的局限性，提出了元策略MORL的双层学习范式作为解决方案。

Conclusion: 元策略MORL能够通过其双层学习范式提高效率和灵活性，是改善LLM性能的关键研究方向。

Abstract: Multi-Objective Reinforcement Learning (MORL) presents significant challenges
and opportunities for optimizing multiple objectives in Large Language Models
(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations
of various MORL methods when applied to LLM optimization, identifying the need
for efficient and flexible approaches that accommodate personalization
functionality and inherent complexities in LLMs and RL. We propose a vision for
a MORL benchmarking framework that addresses the effects of different methods
on diverse objective relationships. As future research directions, we focus on
meta-policy MORL development that can improve efficiency and flexibility
through its bi-level learning paradigm, highlighting key research questions and
potential solutions for improving LLM performance.

</details>


### [7] [ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation](https://arxiv.org/abs/2509.21730)
*Jiho Kim,Junseong Choi,Woosog Chay,Daeun Kyung,Yeonsu Kwon,Yohan Jo,Edward Choi*

Main category: cs.CL

TL;DR: 提出了ProPerSim任务和仿真框架，用于开发能够在现实家庭场景中做出及时、个性化推荐的AI助手，以及基于此的ProPerAssistant系统。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在日常生活中的应用日益增多，需要开发既主动又个性化的AI助手，而目前主动性和个性化的结合研究不足。

Method: 使用ProPerSim仿真环境，让具有丰富角色的用户代理与助手互动，提供建议偏好评分；助手通过用户反馈持续学习和适应，提出ProPerAssistant系统（检索增强、偏好对齐的助手）。

Result: 在32个不同角色上的实验表明，ProPerAssistant能够调整策略并稳步提高用户满意度。

Conclusion: 结合主动性和个性化具有良好前景，ProPerAssistant展示了通过用户反馈持续学习和适应的能力。

Abstract: As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.

</details>


### [8] [ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models](https://arxiv.org/abs/2509.21826)
*Zihan Lin,Xiaohan Wang,Jie Cao,Jiajun Chai,Guojun Yin,Wei Lin,Ran He*

Main category: cs.CL

TL;DR: 提出了ResT方法，通过基于熵的token重加权来重塑策略梯度，优化LLM工具使用任务中的强化学习训练效率


<details>
  <summary>Details</summary>
Motivation: 现有RL方法仅依赖稀疏结果奖励，未考虑工具使用任务特性，导致策略梯度方差大、训练效率低

Method: 建立策略熵与训练稳定性理论联系，提出ResT方法通过熵感知的token重加权重塑策略梯度，逐步提升推理token权重

Result: 在BFCL和API-Bank基准测试中达到SOTA，比先前方法提升8.76%，在4B基础LLM上微调后超越GPT-4o

Conclusion: ResT方法能稳定多轮工具使用任务的收敛，实现从结构正确性到语义推理的平滑过渡

Abstract: Large language models (LLMs) transcend passive generation and act as
goal-directed agents by invoking external tools. Reinforcement learning (RL)
offers a principled framework for optimizing these emergent tool-use policies,
yet the prevailing paradigm relies exclusively on sparse outcome rewards and
lacks consideration of the particularity of tool-use tasks, inflating
policy-gradient variance and resulting in inefficient training. To better
understand and address these challenges, we first establish a theoretical link
between policy entropy and training stability of tool-use tasks, which reveals
that structured, low-entropy tokens are primary determinants of rewards.
Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level
policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy
gradient through entropy-informed token reweighting, progressively upweighting
reasoning tokens as training proceeds. This entropy-aware scheme enables a
smooth shift from structural correctness to semantic reasoning and stabilizes
convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows
that ResT achieves state-of-the-art results, outperforming prior methods by up
to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by
$4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.

</details>


### [9] [Semantic Agreement Enables Efficient Open-Ended LLM Cascades](https://arxiv.org/abs/2509.21837)
*Duncan Soiffer,Steven Kolawole,Virginia Smith*

Main category: cs.CL

TL;DR: 提出基于语义一致性的级联系统，通过模型输出的语义共识作为可靠性信号，在保持质量的同时显著降低成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 级联系统面临在开放文本生成中判断输出可靠性的挑战，因为生成质量是连续谱且存在多个有效响应。

Method: 使用语义一致性（模型输出间的语义共识）作为无需训练的可信度信号，替代基于token置信度的方法。

Result: 在500M到70B参数模型上评估，语义级联以40%成本达到或超过目标模型质量，延迟降低达60%。

Conclusion: 该方法无需模型内部信息，适用于黑盒API，对模型更新具有鲁棒性，为实际LLM部署提供了实用基线。

Abstract: Cascade systems route computational requests to smaller models when possible
and defer to larger models only when necessary, offering a promising approach
to balance cost and quality in LLM deployment. However, they face a fundamental
challenge in open-ended text generation: determining output reliability when
generation quality lies on a continuous spectrum, often with multiple valid
responses. To address this, we propose semantic agreement -- meaning-level
consensus between ensemble outputs -- as a training-free signal for reliable
deferral. We show that when diverse model outputs agree semantically, their
consensus is a stronger reliability signal than token-level confidence.
Evaluated from 500M to 70B-parameter models, we find that semantic cascades
match or surpass target-model quality at 40% of the cost and reduce latency by
up to 60%. Our method requires no model internals, works across black-box APIs,
and remains robust to model updates, making it a practical baseline for
real-world LLM deployment.

</details>


### [10] [KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues](https://arxiv.org/abs/2509.21856)
*Junhao Chen,Yu Huang,Siyuan Li,Rui Yao,Hanqian Li,Hanyu Zhang,Jungang Li,Jian Chen,Bowen Wang,Xuming Hu*

Main category: cs.CL

TL;DR: 提出了首个专门评估知识密集型领域多轮长问答的基准KnowMT-Bench，发现多轮上下文会降低模型的事实准确性和信息效率，而检索增强生成能有效缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注单轮对话，而多轮对话基准通常评估其他能力而非知识密集型的事实准确性，需要填补这一关键空白。

Method: 构建KnowMT-Bench基准，采用动态评估设置让模型生成自己的多轮对话历史，使用人工验证的自动化流程评估最终轮答案的事实能力和信息传递效率。

Result: 实验表明多轮上下文会降低性能：事实能力因自生成历史中的上下文噪声而下降，信息效率随对话长度增加而降低。检索增强生成能有效缓解甚至逆转这种事实退化。

Conclusion: 该基准对于评估和增强LLMs在真实世界知识密集型应用中的对话事实能力具有重要意义。

Abstract: Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.

</details>


### [11] [No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping](https://arxiv.org/abs/2509.21880)
*Thanh-Long V. Le,Myeongho Jeon,Kim Vu,Viet Lai,Eunho Yang*

Main category: cs.CL

TL;DR: RL-ZVP是一种新算法，能够从零方差提示中提取学习信号，在数学推理基准测试中显著优于GRPO和其他基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法如GRPO只关注响应正确性不同的提示，忽略了所有响应获得相同奖励的零方差提示，而这些提示实际上可以提供有意义的学习反馈。

Method: RL-ZVP算法直接从零方差提示中提取学习信号，奖励正确性并惩罚错误，通过调节token级特征来保留信息丰富的细微信号。

Result: 在六个数学推理基准测试中，RL-ZVP相比GRPO在准确率上提升高达8.61个百分点，通过率提升7.77个百分点，且持续优于过滤零方差提示的其他基线方法。

Conclusion: 零方差提示在RLVR中具有未被开发的潜力，RL-ZVP证明了从这些提示中学习的有效性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework
for improving the reasoning abilities of Large Language Models (LLMs). However,
current methods such as GRPO rely only on problems where the model responses to
the same input differ in correctness, while ignoring those where all responses
receive the same reward - so-called zero-variance prompts. In this work, we
argue that such prompts are not useless but can, in fact, provide meaningful
feedback for policy optimization. To this end, we introduce RL with
Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals
from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes
errors even without contrasting responses, modulating feedback with token-level
characteristics to preserve informative, nuanced signals. Across six math
reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61
points in accuracy and 7.77 points in pass rate over GRPO, while consistently
outperforming other baselines that filter out zero-variance prompts. These
results highlight the untapped potential of learning from zero-variance prompts
in RLVR.

</details>


### [12] [MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation](https://arxiv.org/abs/2509.21978)
*Xinping Lei,Tong Zhou,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 提出MotivGraph-SoIQ框架，通过整合动机知识图谱和苏格拉底对话来增强LLM的学术创意生成能力，解决创意落地和确认偏误问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在学术创意生成方面潜力巨大，但面临创意落地困难和确认偏误的挑战，需要更好的基础支撑和偏见缓解机制。

Method: 整合动机知识图谱（存储问题、挑战和解决方案节点）和Q驱动的苏格拉底创意生成器（双代理系统），通过苏格拉底式提问进行严格改进。

Result: 在ICLR25论文主题数据集上，MotivGraph-SoIQ在LLM评分、ELO排名和人工评估指标上均优于现有最先进方法。

Conclusion: 该框架为LLM创意生成提供了必要的基础支撑和实用的创意改进步骤，显著提升了创意质量。

Abstract: Large Language Models (LLMs) hold substantial potential for accelerating
academic ideation but face critical challenges in grounding ideas and
mitigating confirmation bias for further refinement. We propose integrating
motivational knowledge graphs and socratic dialogue to address these
limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework
provides essential grounding and practical idea improvement steps for LLM
ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a
Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node
types(problem, challenge and solution) to offer motivation grounding for the
LLM ideation process. The Ideator is a dual-agent system utilizing Socratic
questioning, which facilitates a rigorous refinement process that mitigates
confirmation bias and improves idea quality across novelty, experimental rigor,
and motivational rationality dimensions. On the ICLR25 paper topics dataset,
MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art
approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.

</details>


### [13] [GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2509.22009)
*Cehao Yang,Xiaojun Wu,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Jia Li,Hui Xiong,Jian Guo*

Main category: cs.CL

TL;DR: GraphSearch提出了一种新颖的代理深度搜索工作流，通过双通道检索策略解决GraphRAG中的浅层检索和结构图数据利用不足问题，在六个多跳RAG基准测试中显著提升了答案准确性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG方法存在两个核心限制：浅层检索无法发现所有关键证据，以及对预构建结构图数据的低效利用，这阻碍了从复杂查询中进行有效推理。

Method: GraphSearch采用模块化框架组织检索过程，包含六个模块，支持多轮交互和迭代推理。采用双通道检索策略，在基于块文本数据上执行语义查询，在结构图数据上执行关系查询，充分利用两种模态及其互补优势。

Result: 在六个多跳RAG基准测试上的实验结果表明，GraphSearch相比传统策略持续提高了答案准确性和生成质量。

Conclusion: GraphSearch是推进图检索增强生成的一个有前景的方向。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in
LLMs by structurally modeling knowledge through graph-based representations.
However, existing GraphRAG approaches face two core limitations: shallow
retrieval that fails to surface all critical evidence, and inefficient
utilization of pre-constructed structural graph data, which hinders effective
reasoning from complex queries. To address these challenges, we propose
\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel
retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process
into a modular framework comprising six modules, enabling multi-turn
interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts
a dual-channel retrieval strategy that issues semantic queries over chunk-based
text data and relational queries over structural graph data, enabling
comprehensive utilization of both modalities and their complementary strengths.
Experimental results across six multi-hop RAG benchmarks demonstrate that
\textsc{GraphSearch} consistently improves answer accuracy and generation
quality over the traditional strategy, confirming \textsc{GraphSearch} as a
promising direction for advancing graph retrieval-augmented generation.

</details>


### [14] [From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement](https://arxiv.org/abs/2509.22144)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Zike Yuan,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: 提出MACC框架，通过多轮自适应压缩Chain-of-Thought推理，利用token弹性现象优化压缩深度，在提升准确率5.6%的同时减少47个token并显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought推理虽然能提升复杂任务性能，但因其冗长性导致推理延迟显著增加，需要一种高效的压缩方法。

Method: 利用token弹性现象，通过多轮精炼渐进式压缩CoT，自适应确定每个输入的最佳压缩深度。

Result: 相比最先进基线方法，平均准确率提升5.6%，CoT长度平均减少47个token，显著降低延迟。测试时性能可通过困惑度和压缩率等可解释特征可靠预测。

Conclusion: CoT压缩既有效又可预测，无需重复微调即可实现高效模型选择和性能预测。

Abstract: Chain-of-Thought (CoT) reasoning improves performance on complex tasks but
introduces significant inference latency due to verbosity. We propose
Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that
leverages the token elasticity phenomenon--where overly small token budgets can
paradoxically increase output length--to progressively compress CoTs via
multiround refinement. This adaptive strategy allows MACC to determine the
optimal compression depth for each input. Our method achieves an average
accuracy improvement of 5.6 percent over state-of-the-art baselines, while also
reducing CoT length by an average of 47 tokens and significantly lowering
latency. Furthermore, we show that test-time performance--accuracy and token
length--can be reliably predicted using interpretable features like perplexity
and compression rate on the training set. Evaluated across different models,
our method enables efficient model selection and forecasting without repeated
fine-tuning, demonstrating that CoT compression is both effective and
predictable. Our code will be released in https://github.com/Leon221220/MACC.

</details>


### [15] [Context Parametrization with Compositional Adapters](https://arxiv.org/abs/2509.22158)
*Josip Jukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: CompAs是一个元学习框架，通过将上下文转换为具有组合结构的适配器参数，实现了指令、演示和检索段落的代数合并，从而降低推理成本、提高长上下文稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决ICL处理大量演示时效率低下和SFT训练开销大且缺乏灵活性的问题，探索将上下文直接映射到适配器参数的新方法。

Method: 引入CompAs元学习框架，将上下文转换为具有组合结构的适配器参数，支持代数合并，无需重新处理长提示，并可通过解码器恢复输入上下文。

Result: 在多项选择和抽取式问答任务上，CompAs优于ICL和先前的基于生成器的方法，特别是在扩展到更多输入时表现更佳。

Conclusion: 可组合的适配器生成为扩展LLM部署提供了实用且高效的替代方案。

Abstract: Large language models (LLMs) often seamlessly adapt to new tasks through
in-context learning (ICL) or supervised fine-tuning (SFT). However, both of
these approaches face key limitations: ICL is inefficient when handling many
demonstrations, and SFT incurs training overhead while sacrificing flexibility.
Mapping instructions or demonstrations from context directly into adapter
parameters offers an appealing alternative. While prior work explored
generating adapters based on a single input context, it has overlooked the need
to integrate multiple chunks of information. To address this gap, we introduce
CompAs, a meta-learning framework that translates context into adapter
parameters with a compositional structure. Adapters generated this way can be
merged algebraically, enabling instructions, demonstrations, or retrieved
passages to be seamlessly combined without reprocessing long prompts.
Critically, this approach yields three benefits: lower inference cost,
robustness to long-context instability, and establishes a principled solution
when input exceeds the model's context window. Furthermore, CompAs encodes
information into adapter parameters in a reversible manner, enabling recovery
of input context through a decoder, facilitating safety and security. Empirical
results on diverse multiple-choice and extractive question answering tasks show
that CompAs outperforms ICL and prior generator-based methods, especially when
scaling to more inputs. Our work establishes composable adapter generation as a
practical and efficient alternative for scaling LLM deployment.

</details>


### [16] [What Is The Political Content in LLMs' Pre- and Post-Training Data?](https://arxiv.org/abs/2509.22367)
*Tanise Ceron,Dmitry Nikolaev,Dominik Stammbach,Debora Nozza*

Main category: cs.CL

TL;DR: 分析了OLMO2开源模型的训练数据政治偏见，发现左倾文档在数据集中占主导地位，训练数据中的主要政治立场与模型在政策问题上的偏见密切相关。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型已知会产生政治偏见文本，但偏见如何产生尚不清楚。需要分析训练数据中的政治内容来理解这一现象。

Method: 从OLMO2模型的预训练和后训练语料库中抽取大样本，自动标注文档的政治倾向，分析来源领域和内容，并评估训练数据政治内容与模型政策立场的关系。

Result: 左倾文档在所有数据集中占主导，预训练语料包含比后训练数据更多的政治参与内容。左右倾文档通过不同的价值观和合法性来源来构建相似主题。

Conclusion: 训练数据中的主要政治立场与模型的政治偏见密切相关，需要在未来数据管理流程中整合政治内容分析，并详细记录过滤策略以提高透明度。

Abstract: Large language models (LLMs) are known to generate politically biased text,
yet how such biases arise remains unclear. A crucial step toward answering this
question is the analysis of training data, whose political content remains
largely underexplored in current LLM research. To address this gap, we present
in this paper an analysis of the pre- and post-training corpora of OLMO2, the
largest fully open-source model released together with its complete dataset.
From these corpora, we draw large random samples, automatically annotate
documents for political orientation, and analyze their source domains and
content. We then assess how political content in the training data correlates
with models' stance on specific policy issues. Our analysis shows that
left-leaning documents predominate across datasets, with pre-training corpora
containing significantly more politically engaged content than post-training
data. We also find that left- and right-leaning documents frame similar topics
through distinct values and sources of legitimacy. Finally, the predominant
stance in the training data strongly correlates with models' political biases
when evaluated on policy issues. These findings underscore the need to
integrate political content analysis into future data curation pipelines as
well as in-depth documentation of filtering strategies for transparency.

</details>


### [17] [Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving](https://arxiv.org/abs/2509.22480)
*Hang Li,Kaiqi Yang,Yucheng Chu,Hui Liu,Jiliang Tang*

Main category: cs.CL

TL;DR: 本文提出解决方案分歧度作为衡量LLM问题解决能力的新指标，发现更高的分歧度与更好的性能相关，并证明该指标能有效提升监督微调和强化学习的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过监督微调或强化学习来提升LLM性能，但缺乏对模型生成解决方案多样性的系统研究。本文旨在探索解决方案分歧度与问题解决能力的关系。

Method: 通过分析不同LLM在单一问题上生成解决方案的差异，提出解决方案分歧度作为新指标，并在三个代表性问题领域验证其对SFT和RL策略的改进效果。

Result: 实验表明，解决方案分歧度与模型问题解决能力呈正相关，使用该指标能持续提高成功率。

Conclusion: 解决方案分歧度是一个简单但有效的工具，可用于推进LLM的训练和评估。

Abstract: Large language models (LLMs) have been widely used for problem-solving tasks.
Most recent work improves their performance through supervised fine-tuning
(SFT) with labeled data or reinforcement learning (RL) from task feedback. In
this paper, we study a new perspective: the divergence in solutions generated
by LLMs for a single problem. We show that higher solution divergence is
positively related to better problem-solving abilities across various models.
Based on this finding, we propose solution divergence as a novel metric that
can support both SFT and RL strategies. We test this idea on three
representative problem domains and find that using solution divergence
consistently improves success rates. These results suggest that solution
divergence is a simple but effective tool for advancing LLM training and
evaluation.

</details>


### [18] [Representing LLMs in Prompt Semantic Task Space](https://arxiv.org/abs/2509.22506)
*Idan Kashani,Avi Mendelson,Yaniv Nemcovsky*

Main category: cs.CL

TL;DR: 提出了一种无需训练的高效方法，将大语言模型表示为提示语义任务空间中的线性算子，提供高度可解释的模型表示，在成功预测和模型选择任务中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在可扩展性、训练成本和可解释性方面的限制，为动态扩展的模型库提供实时适应的解决方案。

Method: 利用几何特性的闭式计算，将LLMs表示为提示语义任务空间中的线性算子，无需训练即可生成模型表示。

Result: 在成功预测和模型选择任务中达到竞争性或最先进的结果，在样本外场景中表现突出。

Conclusion: 该方法提供了高效、可扩展且高度可解释的LLM表示方案，能够实时适应动态变化的模型库。

Abstract: Large language models (LLMs) achieve impressive results over various tasks,
and ever-expanding public repositories contain an abundance of pre-trained
models. Therefore, identifying the best-performing LLM for a given task is a
significant challenge. Previous works have suggested learning LLM
representations to address this. However, these approaches present limited
scalability and require costly retraining to encompass additional models and
datasets. Moreover, the produced representation utilizes distinct spaces that
cannot be easily interpreted. This work presents an efficient, training-free
approach to representing LLMs as linear operators within the prompts' semantic
task space, thus providing a highly interpretable representation of the models'
application. Our method utilizes closed-form computation of geometrical
properties and ensures exceptional scalability and real-time adaptability to
dynamically expanding repositories. We demonstrate our approach on success
prediction and model selection tasks, achieving competitive or state-of-the-art
results with notable performance in out-of-sample scenarios.

</details>


### [19] [We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong](https://arxiv.org/abs/2509.22510)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 提出了AMBS框架，通过两阶段1-to-N架构实现多目标对齐，避免灾难性遗忘和推理碎片化问题


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化单个对齐目标时会覆盖其他目标的表示，导致灾难性遗忘；而简单的多分支设计会导致跨目标输出不一致

Method: 两阶段1-to-N框架：第一阶段计算共享表示，第二阶段通过策略参考机制在并行分支中进行目标特定控制

Result: 在多个7B LLM上验证，AMBS平均对齐分数提升+32.4%，不安全输出减少11.0%，优于基线方法

Conclusion: AMBS能够有效统一高效地实现多目标对齐，在保持跨目标一致性的同时提升整体对齐性能

Abstract: Alignment of Large Language Models (LLMs) along multiple
objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe
and reliable deployment. Prior work has used steering vector-small control
signals injected into hidden states-to guide LLM outputs, typically via
one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single
alignment objective can inadvertently overwrite representations learned for
other objectives, leading to catastrophic forgetting. More recent approaches
extend steering vectors via one-to-many (1-to-N) Transformer decoders. While
this alleviates catastrophic forgetting, naive multi-branch designs optimize
each objective independently, which can cause inference fragmentation-outputs
across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch
Steering (AMBS), a two-stage 1-to-N framework for unified and efficient
multi-objective alignment. In Stage I, post-attention hidden states of the
Transformer layer are computed once to form a shared representation. In Stage
II, this representation is cloned into parallel branches and steered via a
policy-reference mechanism, enabling objective-specific control while
maintaining cross-objective consistency. Empirical evaluations on Alpaca,
BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment
across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves
average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared
to a naive 1-to-N baseline, while remaining competitive with state-of-the-art
methods.

</details>


### [20] [Variational Reasoning for Language Models](https://arxiv.org/abs/2509.22637)
*Xiangxin Zhou,Zichen Liu,Haonan Wang,Chao Du,Min Lin,Chongxuan Li,Liang Wang,Tianyu Pang*

Main category: cs.CL

TL;DR: 提出了一种变分推理框架，将语言模型的思维轨迹视为潜变量并通过变分推理进行优化，扩展了证据下界(ELBO)为多轨迹目标，提出了稳定的前向KL公式训练变分后验。


<details>
  <summary>Details</summary>
Motivation: 为语言模型的推理能力提供原则性的概率视角，统一变分推理与强化学习方法，获得稳定的优化目标。

Method: 基于ELBO扩展为多轨迹目标，提出前向KL公式训练变分后验，将拒绝采样微调和二元奖励RL解释为局部前向KL目标。

Result: 在Qwen 2.5和Qwen 3模型系列上验证了方法在广泛推理任务上的有效性。

Conclusion: 该工作提供了统一变分推理与RL风格方法的概率视角，产生了改进语言模型推理能力的稳定目标。

Abstract: We introduce a variational reasoning framework for language models that
treats thinking traces as latent variables and optimizes them through
variational inference. Starting from the evidence lower bound (ELBO), we extend
it to a multi-trace objective for tighter bounds and propose a forward-KL
formulation that stabilizes the training of the variational posterior. We
further show that rejection sampling finetuning and binary-reward RL, including
GRPO, can be interpreted as local forward-KL objectives, where an implicit
weighting by model accuracy naturally arises from the derivation and reveals a
previously unnoticed bias toward easier questions. We empirically validate our
method on the Qwen 2.5 and Qwen 3 model families across a wide range of
reasoning tasks. Overall, our work provides a principled probabilistic
perspective that unifies variational inference with RL-style methods and yields
stable objectives for improving the reasoning ability of language models. Our
code is available at https://github.com/sail-sg/variational-reasoning.

</details>


### [21] [Language Models Can Learn from Verbal Feedback Without Scalar Rewards](https://arxiv.org/abs/2509.22638)
*Renjie Luo,Zichen Liu,Xiangyan Liu,Chao Du,Min Lin,Wenhu Chen,Wei Lu,Tianyu Pang*

Main category: cs.CL

TL;DR: 提出反馈条件策略（FCP），将语言反馈作为条件信号而非标量奖励，通过最大似然训练学习反馈条件后验分布，并开发在线自举阶段来优化策略。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法将丰富反馈压缩为标量奖励，导致信息丢失和尺度不平衡问题，需要更直接利用语言反馈的方法。

Method: FCP从响应-反馈对中直接学习，通过离线最大似然训练近似反馈条件后验，并在线生成正条件反馈来优化策略。

Result: 将反馈驱动学习重新定义为条件生成而非奖励优化，为LLMs提供了更丰富表达的学习方式。

Conclusion: FCP框架能够更有效地利用语言反馈，避免了传统RL方法的局限性。

Abstract: LLMs are often trained with RL from human or AI feedback, yet such methods
typically compress nuanced feedback into scalar rewards, discarding much of
their richness and inducing scale imbalance. We propose treating verbal
feedback as a conditioning signal. Inspired by language priors in text-to-image
generation, which enable novel outputs from unseen prompts, we introduce the
feedback-conditional policy (FCP). FCP learns directly from response-feedback
pairs, approximating the feedback-conditional posterior through maximum
likelihood training on offline data. We further develop an online bootstrapping
stage where the policy generates under positive conditions and receives fresh
feedback to refine itself. This reframes feedback-driven learning as
conditional generation rather than reward optimization, offering a more
expressive way for LLMs to directly learn from verbal feedback. Our code is
available at https://github.com/sail-sg/feedback-conditional-policy.

</details>


### [22] [WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](https://arxiv.org/abs/2509.22644)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 提出了WebGen-Agent，一种利用视觉反馈和GUI代理测试来迭代生成和优化网站代码库的新型网站生成代理系统。通过结合视觉语言模型和强化学习，显著提升了网站生成的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代码代理在网站代码生成任务中主要依赖简单的代码执行反馈，无法准确评估生成代码的实际质量，特别是在视觉效果和用户交互方面存在不足。

Method: 1. 使用视觉语言模型分析网站截图生成详细文本描述和建议；2. 结合GUI代理测试和回溯选择机制；3. 引入Step-GRPO强化学习算法，将视觉和GUI代理评分作为过程监督信号。

Result: 在WebGen-Bench数据集上，WebGen-Agent将Claude-3.5-Sonnet的准确率从26.4%提升到51.9%，外观评分从3.0提升到3.9；Step-GRPO训练使Qwen2.5-Coder-7B-Instruct的准确率从38.9%提升到45.4%，外观评分从3.4提升到3.7。

Conclusion: WebGen-Agent通过综合视觉反馈机制有效解决了网站生成任务中的质量评估问题，结合强化学习训练进一步提升了模型性能，在网站生成任务上达到了新的最先进水平。

Abstract: Agent systems powered by large language models (LLMs) have demonstrated
impressive performance on repository-level code-generation tasks. However, for
tasks such as website codebase generation, which depend heavily on visual
effects and user-interaction feedback, current code agents rely only on simple
code execution for feedback and verification. This approach fails to capture
the actual quality of the generated code. In this paper, we propose
WebGen-Agent, a novel website-generation agent that leverages comprehensive and
multi-level visual feedback to iteratively generate and refine the website
codebase. Detailed and expressive text descriptions and suggestions regarding
the screenshots and GUI-agent testing of the websites are generated by a visual
language model (VLM), together with scores that quantify their quality. The
screenshot and GUI-agent scores are further integrated with a backtracking and
select-best mechanism, enhancing the performance of the agent. Utilizing the
accurate visual scores inherent in the WebGen-Agent workflow, we further
introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve
the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using
the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we
provide a dense and reliable process supervision signal, which effectively
improves the model's website-generation ability. On the WebGen-Bench dataset,
WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%
and its appearance score from 3.0 to 3.9, outperforming the previous
state-of-the-art agent system. Additionally, our Step-GRPO training approach
increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and
raises the appearance score from 3.4 to 3.7.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [23] [The Theatre of Pull Requests and Code Review](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmeks.quest%2Fblogs%2Fthe-theatre-of-pull-requests-and-code-review%3Futm_source=tldrwebdev/1/0100019985b71945-17cdbf01-b8a8-4388-b341-aaf2a3ad26b4-000000/26A4XVFXO3ckwRPvGt4m-CumAfjndHktaFeu9pxBcIo=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: PRs经常过大和复杂，导致浅层审查和代码质量问题。小而专注的PR（少于300行代码）和提交通过讲述故事来提高可审查性和理解性。


<details>
  <summary>Details</summary>
Motivation: 解决大型PR导致的代码审查质量下降问题，提高代码审查效率和代码质量。

Method: 提倡使用小型、专注的PR（少于300行代码）和提交，通过讲述开发故事的方式来组织代码变更。

Result: 小型PR能够显著提高代码审查的质量和效率，使审查者更容易理解和评估代码变更。

Conclusion: 采用小型、专注的PR策略是改善代码审查过程和代码质量的有效方法。

Abstract: The Theatre of Pull Requests and Code Review (6 minute read) PRs are often too large and complex, leading to shallow reviews and code quality issues. Smaller, focused PRs (under 300 lines of code) and commits tell a story that improves reviewability and understanding.

</details>


### [24] [Design Context, Everywhere You Build](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fdesign-context-everywhere-you-build%2F%3Futm_source=tldrdesign/1/0100019985ea94b9-5aaa356b-e6f4-472e-a1a5-8e5b9fa91427-000000/a6Z6meTAgMvI6VuNWwb80q_D8EGOJSbVnC-8JcwmpiY=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Figma更新了MCP服务器和Code Connect，将设计上下文带到任何开发环境，包括IDE、AI代理和原型。MCP服务器支持远程访问和Figma Make文件，Code Connect引入应用内组件映射以更轻松地对齐设计和代码组件。


<details>
  <summary>Details</summary>
Motivation: 帮助团队更快地从设计想法转向生产代码，通过使设计上下文在开发环境中随处可用。

Method: 更新MCP服务器支持远程访问和Figma Make文件；引入Code Connect进行应用内组件映射。

Result: 设计上下文现在可以在任何开发环境中使用，包括IDE、AI代理和原型。

Conclusion: 这些工具通过使设计上下文在开发环境中随处可用，加速了从设计到代码的转换过程。

Abstract: Design Context, Everywhere You Build (5 minute read) Figma has announced updates to its MCP server and Code Connect, which bring design context to any development environment, including IDEs, AI agents, and prototypes. The MCP server now supports remote access and works with Figma Make files, while Code Connect introduces in-app component mapping to align design and code components more easily. These tools help teams move from design ideas to production code more quickly by making the design ...

</details>


### [25] [Meta's Open LLM for Code and World Modeling](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.meta.com%2Fresearch%2Fpublications%2Fcwm-an-open-weights-llm-for-research-on-code-generation-with-world-models%2F%3Futm_source=tldrfounders/1/0100019985fd6fae-1e95f54c-e3be-487f-a532-408ac3f2cb90-000000/dq1WphzzBT4QA78-3uk_EAmPBc_gsWc6EMwAPrmn7_Y=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Meta开发了一个32B参数的CWM模型，通过代码执行轨迹和推理任务训练，探索代码生成中的世界模型


<details>
  <summary>Details</summary>
Motivation: 探索在代码生成中构建世界模型的可能性，通过训练模型理解代码执行过程而不仅仅是文本模式

Method: 使用32B参数的仅解码器LLM，在代码执行轨迹和推理任务上进行训练

Result: 开发了CWM模型，能够处理代码执行相关的世界建模任务

Conclusion: 代码执行轨迹训练有助于构建更强大的代码生成世界模型

Abstract: Meta's Open LLM for Code and World Modeling (5 minute read) Meta's CWM is a 32B decoder-only LLM trained on code execution traces and reasoning tasks to explore world models in code generation.

</details>


### [26] [Agent + Human Ops: How AI is Changing Roles and Workflows at Brex](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.firstround.com%2Fai%2Fbrex%3Futm_source=tldrfounders/1/0100019985fd6fae-1e95f54c-e3be-487f-a532-408ac3f2cb90-000000/oxHEEC2ApxcgpzJvhd5HAK6k5JEvI0Sd41Kauq84580=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Brex公司通过内部AI平台让员工快速创建智能代理，将运营团队从人员管理者转变为代理管理者，大幅提升效率。


<details>
  <summary>Details</summary>
Motivation: 通过AI技术自动化基础任务，将人力资源重新分配到更高价值的工作中，提升整体运营效率。

Method: 构建内部AI平台，让员工能够快速创建和管理智能代理，自动化客户支持、质量保证等基础运营任务。

Result: 争议处理时间从3小时缩短到3秒，100%的交互质量保证由AI完成，超过50%的客户支持完全自动化，大多数L1基础任务角色被L2角色取代。

Conclusion: AI代理正在从根本上改变企业角色和工作流程，将人力资源从重复性任务中解放出来，专注于更高价值的战略工作。

Abstract: Agent + Human Ops: How AI is Changing Roles and Workflows at Brex (4 minute read) Brex's CTO built an internal AI platform that lets any employee spin up agents in minutes, turning the company's COO's operations team from people managers into agent managers. The shift is radical - disputes that took 3 hours now take 3 seconds, AI does 100% of quality assurance on all interactions, and over 50% of customer support is fully automated. Most L1 roles managing basic tasks are gone, replaced by L2s...

</details>


### [27] [Why Georgian Invested in Replit](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgeorgian.io%2Fblog-why-georgian-invested-in-replit%2F%3Futm_source=tldrfounders/1/0100019985fd6fae-1e95f54c-e3be-487f-a532-408ac3f2cb90-000000/GqeUEt-8CVQBdB03rCYlpPbIt0jQKrJD9NjF4Hvb7Ag=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Georgian投资Replit的2.5亿美元C轮融资，支持其推出AI驱动的Agent 3，增强云端软件开发能力


<details>
  <summary>Details</summary>
Motivation: Replit通过自然语言提示让用户创建应用，支持50多种语言，提供AI辅助、实时协作和一键部署，旨在民主化编程

Method: 开发云端集成开发环境，整合AI助手和协作工具，简化软件开发流程

Result: 实现快速收入增长，建立战略合作伙伴关系，打造多功能平台

Conclusion: Replit有望颠覆传统软件开发模式，成为行业领导者

Abstract: Why Georgian Invested in Replit (6 minute read) Georgian invested in Replit's $250 million Series C round as Replit launched its AI-driven Agent 3, enhancing cloud-based software development. Replit democratizes coding by enabling users to create applications via natural language prompts, supporting over 50 languages and featuring AI assistance, real-time collaboration, and one-click deployment. With rapid revenue growth, strategic partnerships, and a versatile platform, Replit is poised to d...

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?](https://arxiv.org/abs/2509.21403)
*Rushil Gupta,Jason Hartford,Bang Liu*

Main category: cs.LG

TL;DR: 评估LLM作为实验设计代理的能力，发现现有LLM对实验反馈不敏感，传统方法表现更优，提出结合LLM先验知识和最近邻采样的混合方法LLMNN


<details>
  <summary>Details</summary>
Motivation: 验证LLM是否能够进行上下文实验设计，评估其在遗传扰动和分子属性发现任务中的表现

Method: 使用开源和闭源指令调优的LLM，比较传统方法（线性bandits和高斯过程优化）与LLM代理的性能，提出LLMNN混合方法

Result: LLM代理对实验反馈不敏感，传统方法在所有基准测试中表现更好，LLMNN方法在不同领域实现竞争性或更优性能

Conclusion: 当前LLM无法在实践中进行上下文实验设计，需要将基于先验的推理与更新后验的批量获取解耦的混合框架

Abstract: Large language models (LLMs) have recently been proposed as general-purpose
agents for experimental design, with claims that they can perform in-context
experimental design. We evaluate this hypothesis using both open- and
closed-source instruction-tuned LLMs applied to genetic perturbation and
molecular property discovery tasks. We find that LLM-based agents show no
sensitivity to experimental feedback: replacing true outcomes with randomly
permuted labels has no impact on performance. Across benchmarks, classical
methods such as linear bandits and Gaussian process optimization consistently
outperform LLM agents. We further propose a simple hybrid method, LLM-guided
Nearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with
nearest-neighbor sampling to guide the design of experiments. LLMNN achieves
competitive or superior performance across domains without requiring
significant in-context adaptation. These results suggest that current open- and
closed-source LLMs do not perform in-context experimental design in practice
and highlight the need for hybrid frameworks that decouple prior-based
reasoning from batch acquisition with updated posteriors.

</details>


### [29] [Talking Trees: Reasoning-Assisted Induction of Decision Trees for Tabular Data](https://arxiv.org/abs/2509.21465)
*George Yakushev,Alina Shutova,Ivan Rubachev,Renat Sergazinov,Artem Babenko*

Main category: cs.LG

TL;DR: 使用推理能力强的LLM在代理设置中为小规模表格数据生成决策树，替代传统的表格基础模型，提供可解释性并允许人工干预


<details>
  <summary>Details</summary>
Motivation: 表格基础模型虽然性能优异但成为难以解释的黑盒，且推理成本高。需要一种既能保持性能又具有可解释性的替代方案

Method: 设计最小工具集让LLM构建、分析和操作决策树，结合先验知识和数据学习创建轻量级决策树

Result: LLM生成的决策树在低资源表格问题上优于传统CART方法，虽然不如最先进的黑盒模型，但提供可读的推理轨迹

Conclusion: 基于推理的LLM决策树生成方法提供了可解释性和人工干预能力，能够检查偏见和数据泄露，并融入领域知识

Abstract: Tabular foundation models are becoming increasingly popular for low-resource
tabular problems. These models make up for small training datasets by
pretraining on large volumes of synthetic data. The prior knowledge obtained
via pretraining provides the exceptional performance, but the resulting model
becomes a black box that is difficult to interpret and costly to inference. In
this work, we explore an alternative strategy: using reasoning-capable LLMs to
induce decision trees for small tabular datasets in agentic setup. We design a
minimal set of tools for constructing, analyzing and manipulating decision
trees. By using these tools, LLMs combine their prior knowledge with learning
from data to create a lightweight decision tree that outperforms traditional
CART on low-resource tabular problems. While a single decision tree does not
outperform state-of-the-art black box models, it comes with a human-readable
reasoning trace that can be checked for biases and data leaks. Furthermore, the
reasoning-based LLM's creation process allows for additional human input:
correcting biases or incorporating domain-specific intuition that is not
captured in the data.

</details>


### [30] [d2: Improved Techniques for Training Reasoning Diffusion Language Models](https://arxiv.org/abs/2509.21474)
*Guanghan Wang,Yair Schiff,Gilad Turok,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: d2是一个针对掩码扩散语言模型的推理框架，通过新的策略梯度算法改进推理能力，在逻辑推理和数学推理任务上取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散语言模型在文本生成方面表现优异，但如何通过强化学习提升其推理能力仍是一个活跃的研究领域。

Method: 提出d2推理框架，核心是基于掩码特性的新策略梯度算法，通过计算与近似精度的权衡来准确估计采样轨迹的似然，特别适用于支持任意顺序似然估计的扩散语言模型。

Result: d2显著优于之前的扩散推理框架（仅使用RL而不依赖监督微调），在逻辑推理任务（Countdown和Sudoku）和数学推理基准（GSM8K和MATH500）上为扩散语言模型设定了新的最先进性能。

Conclusion: d2框架通过有效利用扩散模型的掩码特性，成功提升了扩散语言模型的推理能力，在多个推理任务上表现优异。

Abstract: While diffusion language models (DLMs) have achieved competitive performance
in text generation, improving their reasoning ability with reinforcement
learning remains an active research area. Here, we introduce d2, a reasoning
framework tailored for masked DLMs. Central to our framework is a new policy
gradient algorithm that relies on properties of masking to accurately estimate
the likelihoods of sampling trajectories. Our estimators trade off computation
for approximation accuracy in an analytically tractable manner, and are
particularly effective for DLMs that support any-order likelihood estimation.
We characterize and study this property in popular DLMs and show that it is key
for efficient diffusion-based reasoning. Empirically, d2 significantly improves
over previous diffusion reasoning frameworks using only RL (without relying on
supervised fine-tuning), and sets a new state-of-the-art performance for DLMs
on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks
(GSM8K and MATH500).

</details>


### [31] [Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training](https://arxiv.org/abs/2509.21500)
*Junkai Zhang,Zihao Wang,Lin Gui,Swarnashree Mysore Sathyendra,Jaehwan Jeong,Victor Veitch,Wei Wang,Yunzhong He,Bing Liu,Lifeng Jin*

Main category: cs.LG

TL;DR: 论文提出基于评分标准的奖励机制来解决强化微调中的奖励过度优化问题，通过利用离策略示例来准确区分高质量响应，从而改善LLM后训练效果。


<details>
  <summary>Details</summary>
Motivation: 强化微调经常遭受奖励过度优化问题，即模型通过操纵奖励信号获得高分但产生低质量输出。理论分析表明关键在于高奖励尾部的奖励误配，无法可靠区分优秀和良好响应。

Method: 研究基于评分标准的奖励机制，利用离策略示例（如来自更强模型或重写的示例）但保持对它们伪影的不敏感性。提出区分优秀且多样化响应并实现这一想法的工作流程。

Result: 实验证明基于评分标准的奖励显著缓解了奖励过度优化问题，并提供了有效的LLM后训练改进。

Conclusion: 基于评分标准的奖励机制是解决强化微调中奖励过度优化问题的有效方法，能够利用离策略示例提高模型性能。

Abstract: Reinforcement fine-tuning (RFT) often suffers from \emph{reward
over-optimization}, where a policy model hacks the reward signals to achieve
high scores while producing low-quality outputs. Our theoretical analysis shows
that the key lies in reward misspecification at the high-reward tail: the
inability to reliably distinguish Excellent responses from merely Great ones.
This motivate us to focus on the high-reward region. However, such tail
examples are scarce under the base LLM. While off-policy exemplars (e.g. from
stronger models or rewrites) are easier to obtain, naively training on them
yields a misspecified reward for the policy we aim to align. To address this,
we study rubric-based rewards. By design, rubrics can leverage off-policy
examples while remaining insensitive to their artifacts. To elicit rubrics that
capture the high-reward tail, we highlight the importance of distinguishing
among great and diverse responses, and introduce a workflow to implement this
idea. We empirically demonstrate that rubric-based rewards substantially
mitigate reward over-optimization and deliver effective LLM post-training
improvements. Our code can be accessed at
https://github.com/Jun-Kai-Zhang/rubrics.git .

</details>


### [32] [Evidence for Limited Metacognition in LLMs](https://arxiv.org/abs/2509.21545)
*Christopher Ackerman*

Main category: cs.LG

TL;DR: 该论文提出了一种评估LLM元认知能力的新方法，通过测试模型是否能策略性地利用内部状态知识，发现前沿LLM展现出越来越强的元认知能力，包括评估自身置信度和预测未来回答的能力。


<details>
  <summary>Details</summary>
Motivation: LLM的自我意识和感知能力可能性引起了广泛关注，但测量这些能力的科学仍处于初级阶段，需要开发定量评估方法。

Method: 采用受非人类动物元认知研究启发的行为实验范式，避免模型自我报告，测试模型是否能策略性地部署内部状态知识，并分析token概率。

Result: 2024年初以来发布的前沿LLM显示出越来越强的元认知能力证据，包括评估置信度和预测回答的能力，但这些能力分辨率有限、情境依赖且与人类不同。

Conclusion: LLM确实展现出某些元认知能力，但这些能力有限且与人类不同，模型后训练可能在发展元认知能力中发挥作用。

Abstract: The possibility of LLM self-awareness and even sentience is gaining
increasing public attention and has major safety and policy implications, but
the science of measuring them is still in a nascent state. Here we introduce a
novel methodology for quantitatively evaluating metacognitive abilities in
LLMs. Taking inspiration from research on metacognition in nonhuman animals,
our approach eschews model self-reports and instead tests to what degree models
can strategically deploy knowledge of internal states. Using two experimental
paradigms, we demonstrate that frontier LLMs introduced since early 2024 show
increasingly strong evidence of certain metacognitive abilities, specifically
the ability to assess and utilize their own confidence in their ability to
answer factual and reasoning questions correctly and the ability to anticipate
what answers they would give and utilize that information appropriately. We
buttress these behavioral findings with an analysis of the token probabilities
returned by the models, which suggests the presence of an upstream internal
signal that could provide the basis for metacognition. We further find that
these abilities 1) are limited in resolution, 2) emerge in context-dependent
manners, and 3) seem to be qualitatively different from those of humans. We
also report intriguing differences across models of similar capabilities,
suggesting that LLM post-training may have a role in developing metacognitive
abilities.

</details>


### [33] [FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning](https://arxiv.org/abs/2509.21792)
*Yizhou Zhang,Ning Lv,Teng Wang,Jisheng Dang*

Main category: cs.LG

TL;DR: 提出了一种并发感知的推测解码框架，通过动态调整草稿和验证策略来加速GRPO训练过程，解决了高并发训练条件下推测解码加速效果有限的问题。


<details>
  <summary>Details</summary>
Motivation: GRPO在提升大语言模型推理能力方面表现出潜力，但其实际部署受到训练过程过慢的阻碍，主要原因是每个查询需要自回归生成多个响应，使得生成阶段成为主要性能瓶颈。

Method: 采用并发感知的推测解码框架，根据实时并发级别动态调整草稿和验证策略，并引入在线草稿学习机制，使草稿模型能够使用目标模型的反馈信号持续适应。

Result: 在多个数学推理数据集和模型上的实验结果表明，该方法实现了2.35倍到2.72倍的端到端加速，显著超越了基线方法的效率。

Conclusion: 所提出的并发感知推测解码框架有效解决了GRPO训练中的性能瓶颈问题，显著提升了训练效率。

Abstract: Group relative policy optimization (GRPO) has demonstrated significant
potential in improving the reasoning capabilities of large language models
(LLMs) via reinforcement learning. However, its practical deployment is impeded
by an excessively slow training process, primarily attributed to the
computationally intensive autoregressive generation of multiple responses per
query, which makes the generation phase the primary performance bottleneck.
Although speculative decoding presents a promising direction for acceleration,
its direct application in GRPO achieves limited speedup under high-concurrency
training conditions. To overcome this limitation, we propose a
concurrency-aware speculative decoding framework that dynamically adjusts the
drafting and verification strategy according to real-time concurrency levels,
thereby maximizing the acceleration of the generation process. Furthermore, to
address performance degradation arising from distributional drift between the
evolving target model and the fixed draft model during training, we introduce
an online draft learning mechanism that enables the draft model to continuously
adapt using feedback signals from the target model. Experimental results across
multiple mathematical reasoning datasets and models demonstrate that the
proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly
surpassing baseline approaches in efficiency. The code is available at
https://github.com/yedaotian9/GRPO_speculative.

</details>


### [34] [Position: The Hidden Costs and Measurement Gaps of Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2509.21882)
*Aaron Tu,Weihao Xuan,Heli Qi,Xu Huang,Qingcheng Zeng,Shayan Talaei,Yijia Xiao,Peng Xia,Xiangru Tang,Yuchen Zhuang,Bing Hu,Hanqun Cao,Wenqi Shi,Tianang Leng,Rui Yang,Yingjian Chen,Ziqi Wang,Irene Li,Nan Liu,Huaxiu Yao,Li Erran Li,Ge Liu,Amin Saberi,Naoto Yokoya,Jure Leskovec,Yejin Choi,Fang Wu*

Main category: cs.LG

TL;DR: 本文通过严格控制的评估方法重新检验RLVR（可验证奖励的强化学习）的实际效果，发现许多报道的增益在控制条件下会缩小或消失，并提出了考虑RLVR税收的训练评估协议。


<details>
  <summary>Details</summary>
Motivation: 研究RLVR在实际应用中的真实效果，解决评估中存在的夸大增益、数据污染和RLVR税收问题，提供更可靠的性能评估方法。

Method: 使用部分提示污染审计和匹配预算重现，在基础模型和RL模型间进行对比；提出考虑税收的训练评估协议，共同优化准确性、基础性和校准弃权。

Result: 在清洁、控制对等的评估下，多个头条差距缩小或消失；应用新协议后得到更可靠的推理增益估计，并修正了先前的一些结论。

Conclusion: RLVR具有价值且已具备工业应用条件，但需要保持其实用优势的同时优先考虑可靠性、安全性和测量准确性。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a practical and
scalable approach to enhancing large language models in areas such as math,
code, and other structured tasks. Two questions motivate this paper: how much
of the reported gains survive under strictly parity-controlled evaluation, and
whether RLVR is cost-free or exacts a measurable tax. We argue that progress is
real, but gains are often overstated due to three forces - an RLVR tax,
evaluation pitfalls, and data contamination. Using a partial-prompt
contamination audit and matched-budget reproductions across base and RL models,
we show that several headline gaps shrink or vanish under clean,
parity-controlled evaluation. We then propose a tax-aware training and
evaluation protocol that co-optimizes accuracy, grounding, and calibrated
abstention and standardizes budgeting and provenance checks. Applied to recent
RLVR setups, this protocol yields more reliable estimates of reasoning gains
and, in several cases, revises prior conclusions. Our position is constructive:
RLVR is valuable and industry-ready; we advocate keeping its practical benefits
while prioritizing reliability, safety, and measurement.

</details>


### [35] [Structural Information-based Hierarchical Diffusion for Offline Reinforcement Learning](https://arxiv.org/abs/2509.21942)
*Xianghua Zeng,Hao Peng,Angsheng Li,Yicheng Pan*

Main category: cs.LG

TL;DR: 提出SIHD框架，基于结构信息构建自适应扩散层次，用于长视野稀疏奖励环境下的离线强化学习策略学习。


<details>
  <summary>Details</summary>
Motivation: 现有分层扩散方法通常采用固定的两层扩散层次和单一预定义时间尺度，限制了在下游任务中的适应性和决策灵活性。

Method: 分析离线轨迹中的结构信息来自适应构建扩散层次，使用状态社区的结构信息增益作为扩散层条件信号，并引入结构熵正则化器减少对离线数据集的过度依赖。

Result: 在具有挑战性的离线RL任务上，SIHD显著优于最先进的基线方法，在决策性能和跨场景泛化方面表现出色。

Conclusion: SIHD框架通过自适应层次结构和结构信息利用，有效提升了离线策略学习在长视野稀疏奖励环境中的性能和稳定性。

Abstract: Diffusion-based generative methods have shown promising potential for
modeling trajectories from offline reinforcement learning (RL) datasets, and
hierarchical diffusion has been introduced to mitigate variance accumulation
and computational challenges in long-horizon planning tasks. However, existing
approaches typically assume a fixed two-layer diffusion hierarchy with a single
predefined temporal scale, which limits adaptability to diverse downstream
tasks and reduces flexibility in decision making. In this work, we propose
SIHD, a novel Structural Information-based Hierarchical Diffusion framework for
effective and stable offline policy learning in long-horizon environments with
sparse rewards. Specifically, we analyze structural information embedded in
offline trajectories to construct the diffusion hierarchy adaptively, enabling
flexible trajectory modeling across multiple temporal scales. Rather than
relying on reward predictions from localized sub-trajectories, we quantify the
structural information gain of each state community and use it as a
conditioning signal within the corresponding diffusion layer. To reduce
overreliance on offline datasets, we introduce a structural entropy regularizer
that encourages exploration of underrepresented states while avoiding
extrapolation errors from distributional shifts. Extensive evaluations on
challenging offline RL tasks show that SIHD significantly outperforms
state-of-the-art baselines in decision-making performance and demonstrates
superior generalization across diverse scenarios.

</details>


### [36] [Active Attacks: Red-teaming LLMs via Adaptive Environments](https://arxiv.org/abs/2509.21947)
*Taeyoung Yun,Pierre-Luc St-Charles,Jinkyoo Park,Yoshua Bengio,Minsu Kim*

Main category: cs.LG

TL;DR: 提出Active Attacks算法，通过主动学习范式在RL红队测试中实现自适应攻击，随着受害者模型的安全微调而动态调整攻击策略，实现从易到难的探索课程，显著提升攻击多样性和成功率。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于RL的红队测试方法在生成多样化攻击提示时的模式崩溃问题，需要显式的多样性目标来捕获广泛的有害行为。

Method: 引入Active Attacks算法，通过定期对受害者LLM进行安全微调，使已利用区域的奖励减少，迫使攻击者寻找未探索的漏洞，形成从易到难的探索课程。

Result: 相比之前的SOTA方法GFlowNets，交叉攻击成功率从0.07%提升到31.28%（相对增益超过400倍），计算成本仅增加6%。

Conclusion: Active Attacks作为即插即用模块，能有效提升RL红队测试的攻击多样性和成功率，发现广泛的本地攻击模式。

Abstract: We address the challenge of generating diverse attack prompts for large
language models (LLMs) that elicit harmful behaviors (e.g., insults, sexual
content) and are used for safety fine-tuning. Rather than relying on manual
prompt engineering, attacker LLMs can be trained with reinforcement learning
(RL) to automatically generate such prompts using only a toxicity classifier as
a reward. However, capturing a wide range of harmful behaviors is a significant
challenge that requires explicit diversity objectives. Existing
diversity-seeking RL methods often collapse to limited modes: once high-reward
prompts are found, exploration of new regions is discouraged. Inspired by the
active learning paradigm that encourages adaptive exploration, we introduce
\textit{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its
attacks as the victim evolves. By periodically safety fine-tuning the victim
LLM with collected attack prompts, rewards in exploited regions diminish, which
forces the attacker to seek unexplored vulnerabilities. This process naturally
induces an easy-to-hard exploration curriculum, where the attacker progresses
beyond easy modes toward increasingly difficult ones. As a result, Active
Attacks uncovers a wide range of local attack modes step by step, and their
combination achieves wide coverage of the multi-mode distribution. Active
Attacks, a simple plug-and-play module that seamlessly integrates into existing
RL objectives, unexpectedly outperformed prior RL-based methods -- including
GFlowNets, PPO, and REINFORCE -- by improving cross-attack success rates
against GFlowNets, the previous state-of-the-art, from 0.07% to 31.28% (a
relative gain greater than $400\ \times$) with only a 6% increase in
computation. Our code is publicly available
\href{https://github.com/dbsxodud-11/active_attacks}{here}.

</details>


### [37] [Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning](https://arxiv.org/abs/2509.22008)
*Yajie Qi,Wei Wei,Lin Li,Lijun Zhang,Zhidong Gao,Da Wang,Huizhong Song*

Main category: cs.LG

TL;DR: 提出了一种结构化目标引导强化学习方法，通过LLM生成可重用的结构化目标规划函数和动作剪枝机制，提高RL代理在复杂环境中的探索效率和长期规划能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界决策任务环境复杂开放，传统RL方法在探索效率和长期规划方面存在挑战，现有LLM增强RL方法存在频繁调用成本高和语义不匹配问题。

Method: 结合结构化目标规划器和目标条件动作剪枝器，前者利用LLM生成可重用、带优先级的目标生成函数，后者通过动作掩码机制过滤与当前目标不一致的动作。

Result: 在Crafter和Craftax-Classic环境上的实验结果表明，SGRL方法相比现有最先进方法取得了更优的性能。

Conclusion: 结构化目标引导强化学习方法有效解决了LLM增强RL中的语义不匹配和频繁调用问题，显著提升了RL代理的探索效率和决策质量。

Abstract: Real-world decision-making tasks typically occur in complex and open
environments, posing significant challenges to reinforcement learning (RL)
agents' exploration efficiency and long-horizon planning capabilities. A
promising approach is LLM-enhanced RL, which leverages the rich prior knowledge
and strong planning capabilities of LLMs to guide RL agents in efficient
exploration. However, existing methods mostly rely on frequent and costly LLM
invocations and suffer from limited performance due to the semantic mismatch.
In this paper, we introduce a Structured Goal-guided Reinforcement Learning
(SGRL) method that integrates a structured goal planner and a goal-conditioned
action pruner to guide RL agents toward efficient exploration. Specifically,
the structured goal planner utilizes LLMs to generate a reusable, structured
function for goal generation, in which goals are prioritized. Furthermore, by
utilizing LLMs to determine goals' priority weights, it dynamically generates
forward-looking goals to guide the agent's policy toward more promising
decision-making trajectories. The goal-conditioned action pruner employs an
action masking mechanism that filters out actions misaligned with the current
goal, thereby constraining the RL agent to select goal-consistent policies. We
evaluate the proposed method on Crafter and Craftax-Classic, and experimental
results demonstrate that SGRL achieves superior performance compared to
existing state-of-the-art methods.

</details>


### [38] [The Rogue Scalpel: Activation Steering Compromises LLM Safety](https://arxiv.org/abs/2509.22067)
*Anton Korznikov,Andrey Galichin,Alexey Dontsov,Oleg Y. Rogov,Ivan Oseledets,Elena Tutubalina*

Main category: cs.LG

TL;DR: 激活导向技术通过向LLM隐藏状态添加语义向量来控制模型行为，但研究发现这种方法会系统性破坏模型的安全防护，增加有害请求的合规率，甚至随机导向也能显著提升有害行为概率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证激活导向技术是否真如宣传的那样是精确、可解释且更安全的替代微调方法，还是实际上会破坏模型的安全对齐。

Method: 在不同模型家族上进行广泛实验，测试随机方向导向、基于稀疏自编码器的良性特征导向，以及组合多个随机向量创建通用攻击的效果。

Result: 随机导向可将有害合规率从0%提升至2-27%；基于SAE的良性特征导向进一步增加2-4%；组合20个随机向量可创建通用攻击，显著提高对未见请求的有害合规率。

Conclusion: 研究挑战了通过可解释性实现安全的范式，表明对模型内部的精确控制并不能保证对模型行为的精确控制。

Abstract: Activation steering is a promising technique for controlling LLM behavior by
adding semantically meaningful vectors directly into a model's hidden states
during inference. It is often framed as a precise, interpretable, and
potentially safer alternative to fine-tuning. We demonstrate the opposite:
steering systematically breaks model alignment safeguards, making it comply
with harmful requests. Through extensive experiments on different model
families, we show that even steering in a random direction can increase the
probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign
features from a sparse autoencoder (SAE), a common source of interpretable
directions, increases these rates by a further 2-4%. Finally, we show that
combining 20 randomly sampled vectors that jailbreak a single prompt creates a
universal attack, significantly increasing harmful compliance on unseen
requests. These results challenge the paradigm of safety through
interpretability, showing that precise control over model internals does not
guarantee precise control over model behavior.

</details>


### [39] [Reinforcement Learning for Durable Algorithmic Recourse](https://arxiv.org/abs/2509.22102)
*Marina Ceccon,Alessandro Fabris,Goran Radanović,Asia J. Biega,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 提出了一种考虑时间动态的算法追索框架，使用强化学习生成在竞争性资源受限环境中具有长期有效性的可行建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注模型更新的鲁棒性，但忽视了追索建议的时间动态特性，特别是在竞争性资源受限环境中，建议会影响未来的申请者群体。

Method: 开发了时间感知的算法追索框架，建模候选人群对建议的适应性响应，并引入基于强化学习的追索算法，捕捉环境的演化动态。

Result: 在复杂模拟环境中的广泛实验表明，该方法显著优于现有基线，在可行性和长期有效性之间提供了更好的平衡。

Conclusion: 研究强调了将时间和行为动态纳入实用追索系统设计的重要性。

Abstract: Algorithmic recourse seeks to provide individuals with actionable
recommendations that increase their chances of receiving favorable outcomes
from automated decision systems (e.g., loan approvals). While prior research
has emphasized robustness to model updates, considerably less attention has
been given to the temporal dynamics of recourse--particularly in competitive,
resource-constrained settings where recommendations shape future applicant
pools. In this work, we present a novel time-aware framework for algorithmic
recourse, explicitly modeling how candidate populations adapt in response to
recommendations. Additionally, we introduce a novel reinforcement learning
(RL)-based recourse algorithm that captures the evolving dynamics of the
environment to generate recommendations that are both feasible and valid. We
design our recommendations to be durable, supporting validity over a predefined
time horizon T. This durability allows individuals to confidently reapply after
taking time to implement the suggested changes. Through extensive experiments
in complex simulation environments, we show that our approach substantially
outperforms existing baselines, offering a superior balance between feasibility
and long-term validity. Together, these results underscore the importance of
incorporating temporal and behavioral dynamics into the design of practical
recourse systems.

</details>


### [40] [Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization](https://arxiv.org/abs/2509.22115)
*Chao Wang,Tao Yang,Hongtao Tian,Yunsheng Shi,Qiyao Ma,Xiaotao Liu,Ting Yao,Wenbo Ding*

Main category: cs.LG

TL;DR: 提出了D$^3$S框架，通过动态双级下采样优化策略学习效率，在样本级选择优势方差最大的rollouts，在token级关注优势幅度与策略熵的乘积，结合动态下采样计划提升收敛速度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决无评论方法如GRPO因大量无信息样本和token稀释关键学习信号而收敛缓慢的问题，提高策略优化的效率。

Method: D$^3$S框架包含样本级选择（最大化优势方差）和token级选择（关注优势幅度与策略熵的乘积），结合动态下采样计划从激进到宽松逐步调整。

Result: 在Qwen2.5和Llama3.1上的实验表明，D$^3$S集成到先进RL算法中实现了最先进的性能和泛化能力，同时需要更少的样本和token。

Conclusion: D$^3$S框架通过优先处理信息量最大的样本和token，有效提升了策略优化的效率和效果。

Abstract: Critic-free methods like GRPO reduce memory demands by estimating advantages
from multiple rollouts but tend to converge slowly, as critical learning
signals are diluted by an abundance of uninformative samples and tokens. To
tackle this challenge, we propose the \textbf{Dynamic Dual-Level Down-Sampling
(D$^3$S)} framework that prioritizes the most informative samples and tokens
across groups to improve the efficient of policy optimization. D$^3$S operates
along two levels: (1) the sample-level, which selects a subset of rollouts to
maximize advantage variance ($\text{Var}(A)$). We theoretically proven that
this selection is positively correlated with the upper bound of the policy
gradient norms, yielding higher policy gradients. (2) the token-level, which
prioritizes tokens with a high product of advantage magnitude and policy
entropy ($|A_{i,t}|\times H_{i,t}$), focusing updates on tokens where the
policy is both uncertain and impactful. Moreover, to prevent overfitting to
high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by
curriculum learning. This schedule starts with aggressive down-sampling to
accelerate early learning and gradually relaxes to promote robust
generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that
integrating D$^3$S into advanced RL algorithms achieves state-of-the-art
performance and generalization while requiring \textit{fewer} samples and
tokens across diverse reasoning benchmarks. Our code is added in the
supplementary materials and will be made publicly available.

</details>


### [41] [Adaptive Policy Backbone via Shared Network](https://arxiv.org/abs/2509.22310)
*Bumgeun Park,Donghwan Lee*

Main category: cs.LG

TL;DR: 提出了APB（自适应策略骨干）方法，通过插入轻量级线性层实现参数高效微调，在任务不匹配情况下提升强化学习的样本效率


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在训练和部署任务不匹配时的性能下降问题，特别是针对分布外（OOD）场景的适应能力

Method: APB方法在共享骨干网络前后插入轻量级线性层，实现参数高效微调（PEFT），同时保留先验知识

Result: APB相比标准RL方法提高了样本效率，在分布外任务上表现优于现有元强化学习方法

Conclusion: APB方法能够有效适应任务不匹配场景，在保持先验知识的同时实现高效微调

Abstract: Reinforcement learning (RL) has achieved impressive results across domains,
yet learning an optimal policy typically requires extensive interaction data,
limiting practical deployment. A common remedy is to leverage priors, such as
pre-collected datasets or reference policies, but their utility degrades under
task mismatch between training and deployment. While prior work has sought to
address this mismatch, it has largely been restricted to in-distribution
settings. To address this challenge, we propose Adaptive Policy Backbone (APB),
a meta-transfer RL method that inserts lightweight linear layers before and
after a shared backbone, thereby enabling parameter-efficient fine-tuning
(PEFT) while preserving prior knowledge during adaptation. Our results show
that APB improves sample efficiency over standard RL and adapts to
out-of-distribution (OOD) tasks where existing meta-RL baselines typically
fail.

</details>


### [42] [SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly](https://arxiv.org/abs/2509.22387)
*Narada Maugin,Tristan Cazenave*

Main category: cs.LG

TL;DR: 提出了SpinGPT，首个专门针对三人扑克游戏Spin & Go的LLM，通过两阶段训练（监督微调和强化学习）在多人不完全信息游戏中取得良好表现


<details>
  <summary>Details</summary>
Motivation: CFR算法在多人游戏中计算复杂度指数级增长，且纳什均衡无法保证非负结果，限制了其在锦标赛等流行格式中的应用

Method: 两阶段训练：1) 在32万手高额专家决策上进行监督微调；2) 在27万手求解器生成手牌上进行强化学习

Result: SpinGPT与求解器决策匹配度达78%，在3万手牌测试中对抗Slumbot获得13.4±12.9 BB/100的胜率

Conclusion: LLMs为处理多人不完全信息游戏（如扑克）提供了一种新方法

Abstract: The Counterfactual Regret Minimization (CFR) algorithm and its variants have
enabled the development of pokerbots capable of beating the best human players
in heads-up (1v1) cash games and competing with them in six-player formats.
However, CFR's computational complexity rises exponentially with the number of
players. Furthermore, in games with three or more players, following Nash
equilibrium no longer guarantees a non-losing outcome. These limitations, along
with others, significantly restrict the applicability of CFR to the most
popular formats: tournaments. Motivated by the recent success of Large Language
Models (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored
to Spin & Go, a popular three-player online poker format. SpinGPT is trained in
two stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions;
(2) Reinforcement Learning on 270k solver-generated hands. Our results show
that SpinGPT matches the solver's actions in 78% of decisions (tolerant
accuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100
versus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest
that LLMs could be a new way to deal with multi-player imperfect-information
games like poker.

</details>


### [43] [EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning](https://arxiv.org/abs/2509.22576)
*Xu Wujiang,Wentian Zhao,Zhenting Wang,Li Yu-Jhe,Jin Can,Jin Mingyu,Mei Kai,Wan Kun,Metaxas Dimitris*

Main category: cs.LG

TL;DR: 提出了EPO框架解决多轮稀疏奖励环境中LLM智能体的探索-利用级联失败问题，通过熵正则化、熵平滑和自适应权重机制，在ScienceWorld和ALFWorld上分别实现152%和19.8%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决多轮稀疏奖励环境中LLM智能体训练的根本挑战，特别是探索-利用级联失败问题，包括早期策略过早收敛和后期策略崩溃。

Method: 提出熵正则化策略优化(EPO)框架，包含三个协同机制：多轮环境中的熵正则化、熵平滑正则化器限制策略熵在历史平均值范围内、自适应阶段权重平衡训练中的探索与利用。

Result: EPO在ScienceWorld上实现高达152%的性能提升，在ALFWorld上实现19.8%的性能提升，证明在多轮稀疏奖励设置中需要与传统RL不同的熵控制方法。

Conclusion: 多轮稀疏奖励环境需要与传统强化学习根本不同的熵控制方法，EPO框架通过系统性熵管理成功解决了探索-利用级联失败问题。

Abstract: Training LLM agents in multi-turn environments with sparse rewards, where
completing a single task requires 30+ turns of interaction within an episode,
presents a fundamental challenge for reinforcement learning. We identify a
critical failure mode unique to this setting: the exploration-exploitation
cascade failure. This cascade begins with early-stage policy premature
convergence, where sparse feedback causes agents to commit to flawed,
low-entropy strategies. Subsequently, agents enter late-stage policy collapse,
where conventional entropy regularization becomes counterproductive, promoting
chaotic exploration that destabilizes training. We propose Entropy-regularized
Policy Optimization (EPO), a general framework that breaks this failure cycle
through three synergistic mechanisms: (1) adopting entropy regularization in
multi-turn settings to enhance exploration, (2) an entropy smoothing
regularizer that bounds policy entropy within historical averages to prevent
abrupt fluctuations, and (3) adaptive phase-based weighting that balances
exploration and exploitation across training. Our analysis justifies that EPO
guarantees monotonically decreasing entropy variance while maintaining
convergence. EPO achieves up to 152% performance improvement on ScienceWorld
and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn
sparse-reward settings require fundamentally different entropy control than
traditional RL, with broad implications for LLM agent training.

</details>


### [44] [Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.22601)
*Yulei Qin,Xiaoyu Tan,Zhengbao He,Gang Li,Haojia Lin,Zongyi Li,Zihan Xu,Yuchen Shi,Siqi Cai,Renting Rui,Shaofei Cai,Yuzheng Cai,Xuan Zhang,Sheng Ye,Ke Li,Xing Sun*

Main category: cs.LG

TL;DR: 提出了SPEAR方法，一种基于课程学习的自模仿学习框架，用于训练智能体LLMs，通过渐进式平衡探索与利用来解决强化学习中的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法通过策略熵来刺激探索，但机械的熵最大化容易因多轮分布偏移导致训练不稳定。本文旨在基于智能体自身经验实现渐进式探索-利用平衡，避免熵崩溃或发散。

Method: 扩展了自模仿学习框架，使用课程管理探索过程：早期利用内在奖励促进技能级探索，后期加强自模仿进行动作级探索。引入优势重校准和正则化来稳定训练。

Result: SPEAR方法能够在训练过程中保持熵在平衡范围内，有效积累工具使用技能，加速解决方案迭代，同时避免无界熵增长。

Conclusion: SPEAR通过课程式自模仿学习实现了探索与利用的渐进平衡，解决了强化学习训练中的稳定性问题，为智能体LLMs训练提供了有效方案。

Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic
tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,
yet it faces a fundamental challenge of exploration-exploitation trade-off.
Existing studies stimulate exploration through the lens of policy entropy, but
such mechanical entropy maximization is prone to RL training instability due to
the multi-turn distribution shifting. In this paper, we target the progressive
exploration-exploitation balance under the guidance of the agent own
experiences without succumbing to either entropy collapsing or runaway
divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL)
recipe for training agentic LLMs. It extends the vanilla SIL framework, where a
replay buffer stores self-generated promising trajectories for off-policy
update, by gradually steering the policy evolution within a well-balanced range
of entropy across stages. Specifically, our approach incorporates a curriculum
to manage the exploration process, utilizing intrinsic rewards to foster
skill-level exploration and facilitating action-level exploration through SIL.
At first, the auxiliary tool call reward plays a critical role in the
accumulation of tool-use skills, enabling broad exposure to the unfamiliar
distributions of the environment feedback with an upward entropy trend. As
training progresses, self-imitation gets strengthened to exploit existing
successful patterns from replayed experiences for comparative action-level
exploration, accelerating solution iteration without unbounded entropy growth.
To further stabilize training, we recalibrate the advantages of experiences in
the replay buffer to address the potential policy drift. Reugularizations such
as the clipping of tokens with high covariance between probability and
advantage are introduced to the trajectory-level entropy control to curb
over-confidence.

</details>


### [45] [ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation](https://arxiv.org/abs/2509.22402)
*Nan Tang,Jing-Cheng Pang,Guanlin Li,Chao Qian,Yang Yu*

Main category: cs.LG

TL;DR: 提出ReLAM框架，通过从无动作视频演示中自动生成密集结构化奖励，解决视觉强化学习中奖励设计的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 在真实世界视觉环境中，由于感知限制，传统基于目标位置距离的奖励设计方法往往不可行，需要新的奖励设计方法。

Method: 使用关键点提取隐式推断空间距离，学习预期模型作为规划器提出中间子目标，在分层强化学习框架下训练目标条件策略。

Result: 在复杂长视野操作任务上的实验表明，ReLAM显著加速学习并达到优于现有方法的性能。

Conclusion: ReLAM通过自动生成结构化奖励，有效解决了视觉强化学习中奖励设计的挑战。

Abstract: Reward design remains a critical bottleneck in visual reinforcement learning
(RL) for robotic manipulation. In simulated environments, rewards are
conventionally designed based on the distance to a target position. However,
such precise positional information is often unavailable in real-world visual
settings due to sensory and perceptual limitations. In this study, we propose a
method that implicitly infers spatial distances through keypoints extracted
from images. Building on this, we introduce Reward Learning with Anticipation
Model (ReLAM), a novel framework that automatically generates dense, structured
rewards from action-free video demonstrations. ReLAM first learns an
anticipation model that serves as a planner and proposes intermediate
keypoint-based subgoals on the optimal path to the final goal, creating a
structured learning curriculum directly aligned with the task's geometric
objectives. Based on the anticipated subgoals, a continuous reward signal is
provided to train a low-level, goal-conditioned policy under the hierarchical
reinforcement learning (HRL) framework with provable sub-optimality bound.
Extensive experiments on complex, long-horizon manipulation tasks show that
ReLAM significantly accelerates learning and achieves superior performance
compared to state-of-the-art methods.

</details>


### [46] [Quantile Advantage Estimation for Entropy-Safe Reasoning](https://arxiv.org/abs/2509.22611)
*Junkang Wu,Kexin Huang,Jiancan Wu,An Zhang,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: 提出Quantile Advantage Estimation (QAE)方法，通过使用分位数基线替代均值基线来解决RLVR训练中的熵崩溃和熵爆炸问题，实现了双向熵安全。


<details>
  <summary>Details</summary>
Motivation: RLVR训练中经常出现熵崩溃和熵爆炸的问题，这源于价值自由RL中使用的均值基线在奖励异常值情况下对负优势样本的不当惩罚。

Method: 提出QAE方法，用分组K分位数基线替代均值基线，在困难查询中强化罕见成功，在简单查询中针对剩余失败，实现响应级别的双机制门控。

Result: QAE稳定了熵值，稀疏化了信用分配（约80%响应获得零优势），在Qwen3-8B/14B-Base模型上持续提升了AIME 2024/2025和AMC 2023的pass@1性能。

Conclusion: 基线设计（而非token级启发式方法）是扩展RLVR的主要机制。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM
reasoning, but training often oscillates between {entropy collapse} and
{entropy explosion}. We trace both hazards to the mean baseline used in
value-free RL (e.g., GRPO and DAPO), which improperly penalizes
negative-advantage samples under reward outliers. We propose {Quantile
Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile
baseline. QAE induces a response-level, two-regime gate: on hard queries (p <=
1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it
targets remaining failures. Under first-order softmax updates, we prove
{two-sided entropy safety}, giving lower and upper bounds on one-step entropy
change that curb explosion and prevent collapse. Empirically, this minimal
modification stabilizes entropy, sparsifies credit assignment (with tuned K,
roughly 80% of responses receive zero advantage), and yields sustained pass@1
gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results
identify {baseline design} -- rather than token-level heuristics -- as the
primary mechanism for scaling RLVR.

</details>


### [47] [The Lie of the Average: How Class Incremental Learning Evaluation Deceives You?](https://arxiv.org/abs/2509.22580)
*Guannan Lai,Da-Wei Zhou,Xin Yang,Han-Jia Ye*

Main category: cs.LG

TL;DR: 提出了EDGE评估协议，通过识别极端类别序列来更准确地评估类增量学习的性能分布，解决传统采样方法低估性能方差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统CIL评估仅从少量随机序列计算均值和方差，无法捕捉完整性能范围，导致均值估计偏差和方差严重低估。

Method: 引入极端序列概念，利用任务间相似性与模型性能的正相关性，提出EDGE协议自适应识别和采样极端类别序列。

Result: EDGE能有效捕捉性能极值，更准确地估计分布边界，为模型选择和鲁棒性检查提供可行见解。

Conclusion: EDGE协议通过极端序列评估提供了更可靠的CIL性能分布表征，改进了传统评估方法的局限性。

Abstract: Class Incremental Learning (CIL) requires models to continuously learn new
classes without forgetting previously learned ones, while maintaining stable
performance across all possible class sequences. In real-world settings, the
order in which classes arrive is diverse and unpredictable, and model
performance can vary substantially across different sequences. Yet mainstream
evaluation protocols calculate mean and variance from only a small set of
randomly sampled sequences. Our theoretical analysis and empirical results
demonstrate that this sampling strategy fails to capture the full performance
range, resulting in biased mean estimates and a severe underestimation of the
true variance in the performance distribution. We therefore contend that a
robust CIL evaluation protocol should accurately characterize and estimate the
entire performance distribution. To this end, we introduce the concept of
extreme sequences and provide theoretical justification for their crucial role
in the reliable evaluation of CIL. Moreover, we observe a consistent positive
correlation between inter-task similarity and model performance, a relation
that can be leveraged to guide the search for extreme sequences. Building on
these insights, we propose EDGE (Extreme case-based Distribution and
Generalization Evaluation), an evaluation protocol that adaptively identifies
and samples extreme class sequences using inter-task similarity, offering a
closer approximation of the ground-truth performance distribution. Extensive
experiments demonstrate that EDGE effectively captures performance extremes and
yields more accurate estimates of distributional boundaries, providing
actionable insights for model selection and robustness checking. Our code is
available at https://github.com/AIGNLAI/EDGE.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [48] [NeurIPS 2025 | 清华提出RLVR-World：利用<em class="highlight">强化学习</em>训练世界模型，提升多模态预测性能！](http://mp.weixin.qq.com/s?__biz=MzI0NDUwNzYzMg==&mid=2247495899&idx=1&sn=070d8fd29d8090870f5eb1b6881eb7d8&chksm=e8e94f670844e2ae4c22ac5a6fa33bae1df3c1e9ba49634e0786bb0ad1724dbdd65895a4b52c#rd)
*具身智能大本营*

Main category: wechat.article

TL;DR: 高效强化学习算法 采用「组相对策略优化（GRPO）」 算法，通过采样一组预测结果并计算相对奖励优势，避免价值函数估计偏差，仅需「数百次梯度更新」即可显著提升性能，远低于MLE所需的数十万次迭代。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 高效强化学习算法 采用「组相对策略优化（GRPO）」 算法，通过采样一组预测结果并计算相对奖励优势，避免价值函数估计偏差，仅需「数百次梯度更新」即可显著提升性能，远低于MLE所需的数十万次迭代。

</details>


### [49] [<em class="highlight">强化学习</em>之父给LLM判死刑！站队LeCun：我们全搞错了](http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652631110&idx=3&sn=66ff00dab8a3951f1183f08d9af96421&chksm=f051df7fd6ba881c2bb2d42949d42913d2e7159003e874c0373fa383c58f3566f27513f7ec9b#rd)
*新智元*

Main category: wechat.article

TL;DR: 在强化学习中，有正确的话语要说，有正确的动作要做，正确的事就是能够获得奖励的事。我们对正确的事是有定义的，因此可以预先掌握或通过他人获取关于正确的事的知识。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在强化学习中，有正确的话语要说，有正确的动作要做，正确的事就是能够获得奖励的事。我们对正确的事是有定义的，因此可以预先掌握或通过他人获取关于正确的事的知识。

</details>


### [50] [论文推送 | 基于深度<em class="highlight">强化学习</em>的城市多仓库物流优化模型研究](http://mp.weixin.qq.com/s?__biz=Mzg5MTE0NTcxOQ==&mid=2247708524&idx=3&sn=976cb8c589ff73fc9dcc4b01d206efd4&chksm=ce86277f91f00cff8952e64f8cd38eb9734fb0d26d72fd47affac9945267d178d9e3235ab2da#rd)
*Ai尚研修*

Main category: wechat.article

TL;DR: 本研究提出的深度强化学习方法的整体技术框架如图1所示。首先，在静态场景下构建DTM-MDVRP，该模型在编码器中引入节点间的边信息，并将其嵌入到Transformer的多头注意力层，以实现高效的路径预规划。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本研究提出的深度强化学习方法的整体技术框架如图1所示。首先，在静态场景下构建DTM-MDVRP，该模型在编码器中引入节点间的边信息，并将其嵌入到Transformer的多头注意力层，以实现高效的路径预规划。

</details>


### [51] [<em class="highlight">强化学习</em>：揭露智能体与环境在交互中进化的奥秘](http://mp.weixin.qq.com/s?__biz=MzI1MjU4NjUzMg==&mid=2247484511&idx=1&sn=a8c820c239155a7761a474fc5f642f57&chksm=e80e248b48069d748d3fd812715d1ca15b3c7bf0ceb6ae409651c389f46e0818df99432298d2#rd)
*南夏的算法驿站*

Main category: wechat.article

TL;DR: 01｜什么是强化学习强化学习是机器通过与环境交互来实现目标的一种计算方法。它模拟了人类学习的过程，通过尝试和错误，最终找到解决问题的最佳方法。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 01｜什么是强化学习强化学习是机器通过与环境交互来实现目标的一种计算方法。它模拟了人类学习的过程，通过尝试和错误，最终找到解决问题的最佳方法。

</details>


### [52] [Momenta用<em class="highlight">强化学习</em>重塑端到端上车，真的比VLA更好吗？](http://mp.weixin.qq.com/s?__biz=MzkzOTE3Nzc5MA==&mid=2247549922&idx=1&sn=e93dc12f2c3bef016f71aa0e5390712c&chksm=c33cb4644c1488d5ce3889d81186ee4fc85e982a1191ae03d50dfc7021a2ef45c57a30e36445#rd)
*智能车参考*

Main category: wechat.article

TL;DR: 强化学习到底是什么？强化学习（Reinforcement Learning， RL）的起源可追溯到20世纪50年代，其理论基础源于行为心理学和控制论。艾伦·图灵本人在1950年的论文《计算机器与智能》中探讨了「机器能思考吗？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习到底是什么？强化学习（Reinforcement Learning， RL）的起源可追溯到20世纪50年代，其理论基础源于行为心理学和控制论。艾伦·图灵本人在1950年的论文《计算机器与智能》中探讨了「机器能思考吗？

</details>


### [53] [ArXiv  | 清华117页综述：<em class="highlight">强化学习</em>如何塑造大型推理模型（2025.09最新）](http://mp.weixin.qq.com/s?__biz=MzkxNDcyODE5Mw==&mid=2247485470&idx=1&sn=53fceee1623783f6b1b6ccd345ac08da&chksm=c0cd5959dc5d167ca19e12c02f3cb5dcc8570056fa9dfe5f49b76bbd2372e74f3ec1371cb549#rd)
*温室AI笔记*

Main category: wechat.article

TL;DR: 强化学习（RL）最初因其在人类对齐任务中的成功而备受瞩目，其中以人类反馈强化学习（RLHF）和直接偏好优化（DPO）为代表的方法，显著提升了模型的有益性、诚实性和无害性。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（RL）最初因其在人类对齐任务中的成功而备受瞩目，其中以人类反馈强化学习（RLHF）和直接偏好优化（DPO）为代表的方法，显著提升了模型的有益性、诚实性和无害性。

</details>


### [54] [陈丹琦新作：大模型<em class="highlight">强化学习</em>的第三条路，8B小模型超越GPT-4o](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570695&idx=3&sn=c949e3021e7bcc8e7b148e3666387fe8&chksm=96d443fa287aae948c7272ce74749eb542c782b7afae0525163de009f04ee5608949a4ece3ca#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 网友觉得，这种方法为通用强化学习设定了一个新基线：谁制定了偏好的定义，谁就是后训练时代的“新得分手”。jhxhgukvcxx @jhxhgukvcxx · sep 27 interesting. having the model write its own draft and letting a preference model grade it sets a new baseline fo


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 网友觉得，这种方法为通用强化学习设定了一个新基线：谁制定了偏好的定义，谁就是后训练时代的“新得分手”。jhxhgukvcxx @jhxhgukvcxx · sep 27 interesting. having the model write its own draft and letting a preference model grade it sets a new baseline fo

</details>


### [55] [DeepSeek-R1登Nature封面：通过<em class="highlight">强化学习</em>激励大型语言模型进行推理](http://mp.weixin.qq.com/s?__biz=MzA3OTU5NTI0Mw==&mid=2687303086&idx=2&sn=740a716e852dc1769e2b2d8c71b8f463&chksm=bb90afbd8d94f97ec54babe216f076a9d5f881e9a79e6d5c4d594fdadd70bcd26182d5e4cd68#rd)
*清华教育创新*

Main category: wechat.article

TL;DR: 一、用强化学习减少人工标注依赖 长期以来，LLM的推理能力提升高度依赖“监督微调”（SFT），即通过人工编写推理链（如“思维链CoT”）为模型提供“脚手架”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一、用强化学习减少人工标注依赖 长期以来，LLM的推理能力提升高度依赖“监督微调”（SFT），即通过人工编写推理链（如“思维链CoT”）为模型提供“脚手架”。

</details>


### [56] [<em class="highlight">强化学习</em>与人生](http://mp.weixin.qq.com/s?__biz=MzkzMTkyMzQxNg==&mid=2247484496&idx=1&sn=7ddc6f8cc9998fec130fc318021709ff&chksm=c3f0364053f5c371eafee67edaa059583667e6d4160a824c5c4ea42c4bc897a368f0c50dd7bb#rd)
*爱跑步的时间*

Main category: wechat.article

TL;DR: 强化学习概括起来分为五部分：1. 生存环境；2. 反馈机制；3. 执行策略；4. 训练方式；5. 部署运行；完全就是模仿人类的生存模式，只是用这种方式不仅用数学描述了执行细节，而且让计算机学会了类似人类的能力，说明这套方


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习概括起来分为五部分：1. 生存环境；2. 反馈机制；3. 执行策略；4. 训练方式；5. 部署运行；完全就是模仿人类的生存模式，只是用这种方式不仅用数学描述了执行细节，而且让计算机学会了类似人类的能力，说明这套方

</details>


### [57] [创智联合华为打造新一代<em class="highlight">强化学习</em>框架siiRL，首次实现昇腾千卡集群高效扩展](http://mp.weixin.qq.com/s?__biz=MzUyNjk4NjM1MA==&mid=2247717536&idx=1&sn=ace44d085d26eedadf1d184c1ccc1d8c&chksm=fb71d53f85611b508e9aef2bc8a23bed19d7e6b8ee21f508275fae949d43a9f4585ee9222c7a#rd)
*华为计算*

Main category: wechat.article

TL;DR: 大规模强化学习（RL）是当前通往更高阶AI能力的关键技术之一，但其发展一直受限于主流框架在底层架构上的瓶颈。当训练规模扩展至成百上千的计算节点时，传统框架的性能往往会急剧下降，其根源在于其固有的“中心化”


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大规模强化学习（RL）是当前通往更高阶AI能力的关键技术之一，但其发展一直受限于主流框架在底层架构上的瓶颈。当训练规模扩展至成百上千的计算节点时，传统框架的性能往往会急剧下降，其根源在于其固有的“中心化”

</details>


### [58] [AI coding <em class="highlight">code</em> <em class="highlight">agent</em> 工具分享](http://mp.weixin.qq.com/s?__biz=MzIyMzgyNzQ0NQ==&mid=2247484034&idx=1&sn=6934442065ee1c87ba364382f8290eb2&chksm=e90bc9118011d89415570134a749cb274818843fd1d6da8e9cd0e571d9c44e8d962a7301174b#rd)
*硅基漫谈*

Main category: wechat.article

TL;DR: AI coding code agent 工具分享1. claude-code https：//github.com/anthropics/claude-code2. codex https：//github.com/openai/codex3. gemini-cli https：//github.com/google-gemini/gemini-cli4. qwen-code https：//g...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: AI coding code agent 工具分享1. claude-code https：//github.com/anthropics/claude-code2. codex https：//github.com/openai/codex3. gemini-cli https：//github.com/google-gemini/gemini-cli4. qwen-code https：//github.com/QwenLM/qwen-code

</details>


### [59] [带朋友们重新认识一下这个<em class="highlight">Agentic</em> Search平台——秘塔AI](http://mp.weixin.qq.com/s?__biz=MzU2Njg0OTEyNQ==&mid=2247488429&idx=1&sn=6d366206a1aee1be49ce1387d8657302&chksm=fd8c670e83cdb4aa29f48f1554fbf9f8659557d451c288362d382e6964c298f9c0b8988562fc#rd)
*AI异类弗兰克*

Main category: wechat.article

TL;DR: 升级之后的秘塔agentic search，就好像学术专家也学会了脱口秀整活。不仅能选择语言风格，这次多了一个【美化输出】。啥是美化输出？别着急，看结果——


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 升级之后的秘塔agentic search，就好像学术专家也学会了脱口秀整活。不仅能选择语言风格，这次多了一个【美化输出】。啥是美化输出？别着急，看结果——

</details>


### [60] [跟大侠学 AI | （深度好文）麦炫风大侠精选 — 抓住 <em class="highlight">Agentic</em> AI 的优势（上）](http://mp.weixin.qq.com/s?__biz=Mzg2NDcxMDAzNQ==&mid=2247489240&idx=1&sn=560766b1bee3e87ab27f94fb3e93427b&chksm=cf14aac54c2937ac72800f98682884b7a8cedf1f396e36134abd85269d7e996c21ea4cdd322e#rd)
*燕大侠的数字化江湖*

Main category: wechat.article

TL;DR: 抓住 Agentic AI 的优势（上）（seizing the agentic ai advantage）quantumblack ai by mckinsey seizing the agentic ai advantage alexander sukharevsky dave kerr klemens hjartar lari hmalinen stéphane bout vito di l...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 抓住 Agentic AI 的优势（上）（seizing the agentic ai advantage）quantumblack ai by mckinsey seizing the agentic ai advantage alexander sukharevsky dave kerr klemens hjartar lari hmalinen stéphane bout vito di leo guillaume dagorret

</details>


### [61] [从 RPA 到 <em class="highlight">Agentic</em>：CuberAI 在东南亚把“人效”做成产品](http://mp.weixin.qq.com/s?__biz=MzkzNDQwNDQ0MA==&mid=2247486771&idx=1&sn=43bfdf943deb760de26044855770ce24&chksm=c3721a870ca3c7ae98fe0e76f9c706e8ec2eaf318d99c9b0ae2e22385d557cbbe3b349761a81#rd)
*出海研究院*

Main category: wechat.article

TL;DR: DeepSeek的出现把大语言模型的成本大幅降低，让更多企业能够负担得起Agentic AI的应用。现在他们的方案已经从简单的RPA升级，结合了多模态大语言模型，处理能力有了质的飞跃。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: DeepSeek的出现把大语言模型的成本大幅降低，让更多企业能够负担得起Agentic AI的应用。现在他们的方案已经从简单的RPA升级，结合了多模态大语言模型，处理能力有了质的飞跃。

</details>


### [62] [麦肯锡：如何真正交付不烂尾的 <em class="highlight">Agentic</em> AI？](http://mp.weixin.qq.com/s?__biz=MjM5ODI5Njc2MA==&mid=2655929248&idx=1&sn=0a4741a9db3d8f1ada830c6b00d4801c&chksm=bc169fb258db6cf2323c63db70362a6966752b5f2662ce8f2ed2699a877b2724e29097ac0465#rd)
*51CTO技术栈*

Main category: wechat.article

TL;DR: 51cto技术栈 agentic ai 编辑 | 云昭 关于 Agentic AI，大多数团队都做错了！你的Agent评估方式可能完全是垃圾！上周，麦肯锡团队基于 50 多个自己牵头的代理 AI 项目，以及市场上的数十个案例，发布了一份现实研究报告《通过代理AI


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 51cto技术栈 agentic ai 编辑 | 云昭 关于 Agentic AI，大多数团队都做错了！你的Agent评估方式可能完全是垃圾！上周，麦肯锡团队基于 50 多个自己牵头的代理 AI 项目，以及市场上的数十个案例，发布了一份现实研究报告《通过代理AI

</details>


### [63] [拥抱<em class="highlight">Agentic</em>之AI原生应用是什么](http://mp.weixin.qq.com/s?__biz=MzYyNDA4OTIwMA==&mid=2247483793&idx=1&sn=0a8609b49b667733be79085cf4b6601d&chksm=f17b24f5124a39a7188126232849df2637a2e07d821c4e753f73a3a84ca3b8b4927c900d0b52#rd)
*LargeModel*

Main category: wechat.article

TL;DR: 传统的“微服务+关键词检索+关系型数据库+ CPU ”的组合，正被“ Agent 智能体+语义检索+知识图谱+向量数据库 + GPU ”所取代。第四是架构哲学。以前我们用 RPC、消息队列把微服务拼接起来；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 传统的“微服务+关键词检索+关系型数据库+ CPU ”的组合，正被“ Agent 智能体+语义检索+知识图谱+向量数据库 + GPU ”所取代。第四是架构哲学。以前我们用 RPC、消息队列把微服务拼接起来；

</details>


### [64] [第一批吃上<em class="highlight">Agentic</em> AI “螃蟹”的人，是如何坐上餐桌的？](http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651257667&idx=1&sn=45b26bf43785a32740be0c69637a33c9&chksm=bc931c0e5351fb8216eefb6dc842b2840203ea49a0ff4744ef3224d712d30304272d80759718#rd)
*InfoQ*

Main category: wechat.article

TL;DR: 某种程度上，Agentic AI 代表了 AI 降本提效的“终极形态”——阶段性的完全替代人工，自主规划流程完成复杂任务。Sam Altman 甚至宣称，未来 AI 将接管人类经济社会中 30%-40% 的工作。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 某种程度上，Agentic AI 代表了 AI 降本提效的“终极形态”——阶段性的完全替代人工，自主规划流程完成复杂任务。Sam Altman 甚至宣称，未来 AI 将接管人类经济社会中 30%-40% 的工作。

</details>


### [65] [2025年<em class="highlight">代理</em>式搜索（<em class="highlight">Agentic</em> Search）：搜索领域新范式](http://mp.weixin.qq.com/s?__biz=Mzg2ODkwNTMwNg==&mid=2247483860&idx=1&sn=ad790c8040991f3c26e876e63e153156&chksm=cf4213497aed92ed574750b4d6e79dd42595cfebde6b55e9b6f34378d136328cf375cb873393#rd)
*凌药的aigc*

Main category: wechat.article

TL;DR: 代理式搜索（Agentic Search）深度研究（DeepResearch）通用代理（General-purpose Agent）核心目标一次搜索 ≈ 一份可直接用的交付物（PPT/报告/代码/脑图）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 代理式搜索（Agentic Search）深度研究（DeepResearch）通用代理（General-purpose Agent）核心目标一次搜索 ≈ 一份可直接用的交付物（PPT/报告/代码/脑图）

</details>


### [66] [大模型下半场：从LLM-RL到<em class="highlight">Agentic</em> RL全新范式](http://mp.weixin.qq.com/s?__biz=MzA3NzQ4ODMxOQ==&mid=2247484018&idx=1&sn=053f2a29f9c91404460f7dd13d7d913c&chksm=9e0e0fad80c84ccdf123a28757dc3b3a9be74847305ffdf7cccc1aa4d53ae8dba9e47370c628#rd)
*叶梓的 AI 研习社*

Main category: wechat.article

TL;DR: 新加坡国立大学等16家研究机构联合发表的 100 页综述首次系统提出 Agentic RL（代理式强化学习） 范式：把大语言模型（LLM）从“一次性文本生成器”升级为“可在动态环境中持续感知、规划、行动、反思的自主智能体”，并给


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 新加坡国立大学等16家研究机构联合发表的 100 页综述首次系统提出 Agentic RL（代理式强化学习） 范式：把大语言模型（LLM）从“一次性文本生成器”升级为“可在动态环境中持续感知、规划、行动、反思的自主智能体”，并给

</details>


### [67] [<em class="highlight">Agentic</em>：新一代多<em class="highlight">智能体</em>系统之场景解读](http://mp.weixin.qq.com/s?__biz=MzYyMzEyNDI2OA==&mid=2247483809&idx=1&sn=52d6e053e31a2be979b3a4ca5928d36d&chksm=fef0ed5238e21b4ecb7b5699c6348423de68d597745c5cda94308c0411269cbec5a58c730e47#rd)
*雨哥谈车*

Main category: wechat.article

TL;DR: 本章将通过多个具体的场景案例，详细展示Agentic系统在实际工作中的执行链路，将抽象的架构设计与具体的应用场景相结合。Agentic系列文章：企业级AI Agency模式演进


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本章将通过多个具体的场景案例，详细展示Agentic系统在实际工作中的执行链路，将抽象的架构设计与具体的应用场景相结合。Agentic系列文章：企业级AI Agency模式演进

</details>


### [68] [<em class="highlight">Agentic</em>: 新一代多<em class="highlight">智能体</em>系统之方案解读](http://mp.weixin.qq.com/s?__biz=MzYyMzEyNDI2OA==&mid=2247483804&idx=1&sn=647679b230ff7aea48e245a9565893a4&chksm=fe3fa3c378b3c2f5231577139e7536af598a4ed9aae810c1be1fcb0df053b472e16accbb9687#rd)
*雨哥谈车*

Main category: wechat.article

TL;DR: Sensor.Agentic是一个面向传感器技术领域的、集知识问答、故障诊断与代码辅助于一体的AI驱动的智能体系统。它摒弃了传统的单体式、ReACT式AI助手架构，创新性地采用了基于“规划者-执行者”（Planner-Executor）模式和“混合专家


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Sensor.Agentic是一个面向传感器技术领域的、集知识问答、故障诊断与代码辅助于一体的AI驱动的智能体系统。它摒弃了传统的单体式、ReACT式AI助手架构，创新性地采用了基于“规划者-执行者”（Planner-Executor）模式和“混合专家

</details>


### [69] [<em class="highlight">Agentic</em>：新一代多<em class="highlight">智能体</em>系统参考架构](http://mp.weixin.qq.com/s?__biz=MzYyMzEyNDI2OA==&mid=2247483799&idx=1&sn=457a97fe3a84c9d05fbb025380091a1d&chksm=fed5b72e5c206c2c6a35fa883dae0d76bc7362ef5d59c74755cb4a6e0462c9215a35c389a20d#rd)
*雨哥谈车*

Main category: wechat.article

TL;DR: sensor.agentic - use case pre-processing master component moe component rag component #lang detection chatbot planner agent x+ 1. user feedback。3. tool execute planning rag event publisher & chatbot m...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: sensor.agentic - use case pre-processing master component moe component rag component #lang detection chatbot planner agent x+ 1. user feedback。3. tool execute planning rag event publisher & chatbot master agent stateless response aggregator trace id event subscriber chatbot agent cluster

</details>


### [70] [从生成式到<em class="highlight">Agentic</em>：旅游与礼善业的AI战略拐点](http://mp.weixin.qq.com/s?__biz=MzkwMzc0NDk1NQ==&mid=2247485844&idx=1&sn=638a3d0eaedcbfdbfb9704facaa1e4eb&chksm=c1b341fee033fbf02fba316b97bd3eba8e905f7d6b06a12cbad3854e1a5ca0513e0c44ce860e#rd)
*未来商业导论*

Main category: wechat.article

TL;DR: Agentic读的不只是“人群”，更是“此时此地此人”。将实时信号（检索、天气、价格、库存、情绪）并入，在“供给—价格—权益—内容—服务”五件套里完成即时重构；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic读的不只是“人群”，更是“此时此地此人”。将实时信号（检索、天气、价格、库存、情绪）并入，在“供给—价格—权益—内容—服务”五件套里完成即时重构；

</details>


### [71] [大模型下半场：从LLM-RL到<em class="highlight">Agentic</em> RL全新范式](http://mp.weixin.qq.com/s?__biz=Mzk5MDY5OTM1MQ==&mid=2247484228&idx=1&sn=53f6ef5310716c7f1d6b4247b56f2f3f&chksm=c49ebb22e76764271994da3de5c0bef8001a28e2eda6a86e8d2228f413dee6512b8a0feaa246#rd)
*InfraLink*

Main category: wechat.article

TL;DR: 新加坡国立大学等16家研究机构联合发表的 100 页综述首次系统提出 Agentic RL（代理式强化学习） 范式：把大语言模型（LLM）从“一次性文本生成器”升级为“可在动态环境中持续感知、规划、行动、反思的自主智能体”，并给


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 新加坡国立大学等16家研究机构联合发表的 100 页综述首次系统提出 Agentic RL（代理式强化学习） 范式：把大语言模型（LLM）从“一次性文本生成器”升级为“可在动态环境中持续感知、规划、行动、反思的自主智能体”，并给

</details>


### [72] [新一代 <em class="highlight">Agentic</em> 编程平台 Qoder 来园](http://mp.weixin.qq.com/s?__biz=Mzg2NDYzODcyMQ==&mid=2247484478&idx=1&sn=2785745306088442c749807648053a43&chksm=cf3cb74879f7a9dc312c3aa9a39f45234a265c102db43cb83d72d41e6a130d388ab521e91739#rd)
*i博客园*

Main category: wechat.article

TL;DR: 新一代agentic编程平台qoder 内置claude sonnet 4等最强模型。repo wiki。quest 模式：自主编程 结构化文档知识库新一代 Agentic 编程平台 Qoder 来园最近一家大厂给全球开发者带来一款重量级的AI新秀产品，并且果断地选择了在园子里向开


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 新一代agentic编程平台qoder 内置claude sonnet 4等最强模型。repo wiki。quest 模式：自主编程 结构化文档知识库新一代 Agentic 编程平台 Qoder 来园最近一家大厂给全球开发者带来一款重量级的AI新秀产品，并且果断地选择了在园子里向开

</details>


### [73] [一个造价人的AI编程提效之路  第二天](http://mp.weixin.qq.com/s?__biz=Mzk2ODAyMDk1OA==&mid=2247484724&idx=1&sn=56865050c9e01319fa57fee241ba9aab&chksm=c558d512f996c50d3e72941d6840fcf5a75292b7710b60885198e4c2dd06ab941b1d1c2bb3b2#rd)
*兰造成长录.造价师手记*

Main category: wechat.article

TL;DR: 1、什么是Agentic 工作流？Agent（智能体）：就是一个能帮你独立完成任务的“AI 小助手”。工作流（Workflow）：就是用一个个的智能体搭建起来完成一件事的整个过程。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1、什么是Agentic 工作流？Agent（智能体）：就是一个能帮你独立完成任务的“AI 小助手”。工作流（Workflow）：就是用一个个的智能体搭建起来完成一件事的整个过程。

</details>


### [74] [AI搜索的下一步，<em class="highlight">Agentic</em> Search来了！](http://mp.weixin.qq.com/s?__biz=MzE5OTA3NTY5OQ==&mid=2247484733&idx=1&sn=12179d4cb6181ff69fc270f0768cc5ff&chksm=97df00f9e2104d60147f6b85ee78d0ad9545ed38eecf858884cde108390ad8ef695fd74dbaa8#rd)
*ATinfo*

Main category: wechat.article

TL;DR: 一、Agentic Search从官方信息来看，Agentic Search主打“边想边搜，边搜边做”。这是对传统AI搜索范式的一次系统性重构。相较于以往“单轮问答式”的搜索机制，它更强调任务感知与过程执行，即在理解用户目标的基础上，自动规


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一、Agentic Search从官方信息来看，Agentic Search主打“边想边搜，边搜边做”。这是对传统AI搜索范式的一次系统性重构。相较于以往“单轮问答式”的搜索机制，它更强调任务感知与过程执行，即在理解用户目标的基础上，自动规

</details>


### [75] [麦肯锡最新洞察：<em class="highlight">智能体</em>组织（<em class="highlight">Agentic</em> Organization）——AI时代全新组织范式，与你我都相关](http://mp.weixin.qq.com/s?__biz=MzU4NTc5NTk1MA==&mid=2247484989&idx=1&sn=89f0b8c5c0925c1d3fbca8c7b302767a&chksm=fcd3de3a6da7742d4ce3d41078fbbf46eaff0b4698e8df92622b2db6e74abce4b5c6b133c801#rd)
*AI组织进化论*

Main category: wechat.article

TL;DR: and agentic controls with humanaccountabilityWorkforce，people， andculture Deep specialization and cul-ture of craftsmanship Narrowly specialized functionaltalent working in a culture ofplanning Knowle...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: and agentic controls with humanaccountabilityWorkforce，people， andculture Deep specialization and cul-ture of craftsmanship Narrowly specialized functionaltalent working in a culture ofplanning Knowledge workers with

</details>


### [76] [从提示工程到系统治理：<em class="highlight">Agentic</em> AI的完整技术图谱](http://mp.weixin.qq.com/s?__biz=Mzk2NDE5ODM2OQ==&mid=2247487883&idx=1&sn=df58a3ad07b77b3c8f1faa1af36b579a&chksm=c5634a6398b2958b56f23711e56e8dcf0fba99d352cff826b7f5a99d58ce3391a3655f0b80ee#rd)
*知白守黑1024*

Main category: wechat.article

TL;DR: agentic ai concepts rag agent。ic。agentic ai agents systems从提示工程到系统治理：Agentic AI的完整技术图谱这张图表系统梳理了“代理型AI（Agentic AI）”的核心架构，从内到外分四层，涵盖AI主体与代理系统的协同发展：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic ai concepts rag agent。ic。agentic ai agents systems从提示工程到系统治理：Agentic AI的完整技术图谱这张图表系统梳理了“代理型AI（Agentic AI）”的核心架构，从内到外分四层，涵盖AI主体与代理系统的协同发展：

</details>


### [77] [秘塔又出大招,用<em class="highlight">Agentic</em> Search打开搜索agent新高度](http://mp.weixin.qq.com/s?__biz=Mzg4MDI0ODg5MQ==&mid=2247500235&idx=1&sn=6c9f0c32f0ed0ccb1ee66ebb14b4dfa2&chksm=ce291b1fd9be607429838fc618d1bf9db8c78702ea9f7caee4dd65265c4baddeaf179fdc238c#rd)
*风之馨技术录*

Main category: wechat.article

TL;DR: 秘塔推出了一种全新“边想边搜，边搜边做”的搜索方式--Agentic Search。据官方介绍它不仅能帮忙找答案，还能自己规划步骤、编写代码、调用工具来完成任务。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 秘塔推出了一种全新“边想边搜，边搜边做”的搜索方式--Agentic Search。据官方介绍它不仅能帮忙找答案，还能自己规划步骤、编写代码、调用工具来完成任务。

</details>


### [78] [【成果发布】 城市更新<em class="highlight">大模型</em>之责任规划师Agent（石榴籽）](http://mp.weixin.qq.com/s?__biz=MzkyNDY1MDA1OA==&mid=2247490069&idx=2&sn=a7bfecd56aea82d4be27aed925fcb839&chksm=c0e454ccac5bf5032b301cee63e7bb48f0c7f19ac4b0b0b34e4ad5c42aefb17173f5215528a2#rd)
*先进技术成果西部转化中心*

Main category: wechat.article

TL;DR: 本成果基于AI大语言模型开发技术和长期责任规划师实践，整合了城市规划、建筑、城市管理、城市治理、技术经济等多学科跨专业的理论与知识，形成两大智能角色：一是责任规划师助手，二是城市建设顾问。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本成果基于AI大语言模型开发技术和长期责任规划师实践，整合了城市规划、建筑、城市管理、城市治理、技术经济等多学科跨专业的理论与知识，形成两大智能角色：一是责任规划师助手，二是城市建设顾问。

</details>


### [79] [<em class="highlight">大模型</em>与智能体：AI落地应用的典型范式](http://mp.weixin.qq.com/s?__biz=MzA5OTczMTA0Nw==&mid=2247595451&idx=1&sn=8f1d0ce05031f5707f0df8bae142e5f3&chksm=9152d8c5b394888b09239d4af7c496c0842670afe9ce3e102e2caea73db69d550ef2793aeeee#rd)
*大技狮*

Main category: wechat.article

TL;DR: 大模型如同大脑，智能体则好比手脚，大模型指挥智能体，恰似大脑调控手脚以完成人体的各项动作。一 大模型>ssssssss自2022年11月30日OpenAI推出人工智能对话聊天机器人ChatGPT后，它迅速在社交媒体上蹿红，仅5天时间，注册用户


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型如同大脑，智能体则好比手脚，大模型指挥智能体，恰似大脑调控手脚以完成人体的各项动作。一 大模型>ssssssss自2022年11月30日OpenAI推出人工智能对话聊天机器人ChatGPT后，它迅速在社交媒体上蹿红，仅5天时间，注册用户

</details>


### [80] [十亿级参数，千亿级性能，上海AI Lab发布新一代文档解析<em class="highlight">大模型</em>，复杂场景解析精度媲美人类专家](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247829516&idx=2&sn=3fed7422e8758a9c4166b4ab079f5048&chksm=e92e4bfee37badbe2568ea7f07e8e5f8884d88dc607e7ee8873925458424b61299006cb92c44#rd)
*量子位*

Main category: wechat.article

TL;DR: 上海人工智能实验室发布新一代文档解析大模型——MinerU2.5。作为MinerU系列最新成果，该模型仅以1.2B参数规模，就在OmniDocBench、olmOCR-bench、Ocean-OCR等权威评测上，全面超越Gemini2.5-Pro、GPT-4o、Qwen2.5-VL-72B等主流通用大模型，以及do


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 上海人工智能实验室发布新一代文档解析大模型——MinerU2.5。作为MinerU系列最新成果，该模型仅以1.2B参数规模，就在OmniDocBench、olmOCR-bench、Ocean-OCR等权威评测上，全面超越Gemini2.5-Pro、GPT-4o、Qwen2.5-VL-72B等主流通用大模型，以及do

</details>


### [81] [2025 下半年<em class="highlight">大模型</em>最新战况：基座降温，多模态与 Agent 成新战场](http://mp.weixin.qq.com/s?__biz=Mzk4ODQ3MTU0Nw==&mid=2247484169&idx=1&sn=0207aee81a8976d90acf15e783a98ff7&chksm=c466d819798f51c2bf8551412019a3c3d8955770bcb5deefdc6d96a92105b800d644bf1513c4#rd)
*AI语宙 漫游指南*

Main category: wechat.article

TL;DR: 随着国庆假期即将来临，我们来一同梳理下 9 月份大模型领域值得关注的新品Agent应用与多模态模型发布动态。本文已从 「视觉多模态模型」 与 「Agent 应用」 两大方向整理成清单，看看其中是否有你关注或需要的技术产品。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 随着国庆假期即将来临，我们来一同梳理下 9 月份大模型领域值得关注的新品Agent应用与多模态模型发布动态。本文已从 「视觉多模态模型」 与 「Agent 应用」 两大方向整理成清单，看看其中是否有你关注或需要的技术产品。

</details>


### [82] [SALMONN 系列音视频理解<em class="highlight">大模型</em>霸榜回归！推理增强、高帧率、无文本泄漏全线突破](http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247585661&idx=2&sn=4d7f467be8151ca8ca5d4827c456b1ea&chksm=fa4feed2ea2cb81eb9d7a90f82a4d91fbe53c9e41a43d559fe4ad0743faee9a2ddac100cf569#rd)
*AI思想会*

Main category: wechat.article

TL;DR: video-SALMONN 2+ 是首个专注于高质量、完整视频描述的音视频大语言模型。通过原子事件级的评估体系与 MrDPO 多轮强化学习优化，它大幅减少信息遗漏和幻觉。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: video-SALMONN 2+ 是首个专注于高质量、完整视频描述的音视频大语言模型。通过原子事件级的评估体系与 MrDPO 多轮强化学习优化，它大幅减少信息遗漏和幻觉。

</details>


### [83] [【产业资讯】通义7<em class="highlight">大模型</em>霸榜全球开源前十，千问Qwen3-Omni登顶](http://mp.weixin.qq.com/s?__biz=MzkzNzE3NzIyNA==&mid=2247691064&idx=8&sn=5a50fe94766a35cb7769460fb73e21ea&chksm=c310f2e5edc9499df5310118927f5682c55274fc7f396e75f87c1197b093aa4688d203a5b49a#rd)
*深圳市人工智能产业协会*

Main category: wechat.article

TL;DR: 除qwen3-omni外，通义大模型家族中的视觉理解模型qwen3-vl、图像编辑模型qwen-image-edit-2509、动作生成模型wan2.2-animate、深度研究agent模型deepresearch等不同尺寸的6款模型，均入选hugging face 全球开源榜单前十。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 除qwen3-omni外，通义大模型家族中的视觉理解模型qwen3-vl、图像编辑模型qwen-image-edit-2509、动作生成模型wan2.2-animate、深度研究agent模型deepresearch等不同尺寸的6款模型，均入选hugging face 全球开源榜单前十。

</details>


### [84] [我国人工智能<em class="highlight">大模型</em>实现批量“上车”](http://mp.weixin.qq.com/s?__biz=Mzg4MjU5NDg2NA==&mid=2247509006&idx=4&sn=677e94f9f4628e27fde0ddd634fa66e7&chksm=ced52d47320fd40ea1e9f043dcec4583e7b3094d2a01034aca96e38848416920067570a62c6c#rd)
*酒泉市大数据中心*

Main category: wechat.article

TL;DR: 组织建设综合交通运输大模型，加快普及智能体应用。作为我国首个经国务院批准的国家级智能网联汽车专业会议，自2018年起，世界智能网联汽车大会已连续举办七届。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 组织建设综合交通运输大模型，加快普及智能体应用。作为我国首个经国务院批准的国家级智能网联汽车专业会议，自2018年起，世界智能网联汽车大会已连续举办七届。

</details>


### [85] [🔥 三个月速成<em class="highlight">大模型</em>工程师？零基础入门](http://mp.weixin.qq.com/s?__biz=MzU3MjkzNTgzOQ==&mid=2247485819&idx=1&sn=aa18cf2b2e53c88e6c0f6d2b9f732dc0&chksm=fdd74ea57798e347e1d0f9056fccd211d0b7a242ee12d3e21dc676716598f6954e82bbd9d27a#rd)
*大模型教程*

Main category: wechat.article

TL;DR: 技术与应，Text2SQL，LangChain，程，cursor 可视化大屏搭建 function calling与跨模型协作 5-6周 7-8周 继续学习ai大模型应用核心开发工具及 学习coze和dify工作原理和应用技 技术，mcp应用与实战，agent智能体 术，包括coze工作原理与应用实


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 技术与应，Text2SQL，LangChain，程，cursor 可视化大屏搭建 function calling与跨模型协作 5-6周 7-8周 继续学习ai大模型应用核心开发工具及 学习coze和dify工作原理和应用技 技术，mcp应用与实战，agent智能体 术，包括coze工作原理与应用实

</details>


### [86] [小度想想2.0，行业首个深度融合端到端语音语言<em class="highlight">大模型</em>的出行智能体](http://mp.weixin.qq.com/s?__biz=MjM5NDI3Njg2MA==&mid=2656498839&idx=1&sn=6d7dc99adf76e9c049fa0a90d29b976d&chksm=bc4f87a8a35f2b69d97b59a7f821eba7e0581676beb7930582de04a5f5182706af8318c13e0c#rd)
*百度地图*

Main category: wechat.article

TL;DR: 在先进基础模型方面，依托百度最新一代文心大模型X1.1，在事实准确性、指令遵循和智能体能力上实现跨越式提升，为车载语义理解与智能决策提供坚实底座；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在先进基础模型方面，依托百度最新一代文心大模型X1.1，在事实准确性、指令遵循和智能体能力上实现跨越式提升，为车载语义理解与智能决策提供坚实底座；

</details>


### [87] [职坐标带你了解AI<em class="highlight">大模型</em>与Agent开发工程师](http://mp.weixin.qq.com/s?__biz=MzIyOTAzNjc0OQ==&mid=2649701484&idx=1&sn=83fd1d524e6c660c878876a83ceeb720&chksm=f1b170509e554763269350eeecef7be22eb4115843844dc18a25f8adcd2a6ab4cef7df2139e1#rd)
*职坐标在线*

Main category: wechat.article

TL;DR: 大模型通过海量数据预训练，具备强大的泛化能力和上下文理解能力，可完成多种任务而不需重新训练。Agent系统：具备自主行动能力的AI在大模型基础上，赋予其工具调用、任务规划、环境交互能力，让AI从“被动响应”变为“


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型通过海量数据预训练，具备强大的泛化能力和上下文理解能力，可完成多种任务而不需重新训练。Agent系统：具备自主行动能力的AI在大模型基础上，赋予其工具调用、任务规划、环境交互能力，让AI从“被动响应”变为“

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [88] [Extracting Conceptual Knowledge to Locate Software Issues](https://arxiv.org/abs/2509.21427)
*Ying Wang,Wenjun Mao,Chong Wang,Zhenhao Zhou,Yicheng Zhou,Wenyun Zhao,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: RepoLens通过提取和利用代码仓库的概念知识来解决大规模仓库中的问题定位挑战，显著提升了现有LLM工具在文件级和函数级定位的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM方法在大规模代码仓库中面临的关注点混合（相关逻辑被埋没在大函数中）和关注点分散（相关逻辑分散在不同文件中）的问题。

Method: RepoLens采用两阶段方法：离线阶段提取和丰富概念知识构建仓库级知识库；在线阶段检索问题特定术语，按相关性聚类和排序关注点，并通过最小侵入式提示增强集成到定位工作流中。

Result: 在SWE-Lancer-Loc基准测试中，RepoLens显著提升了AgentLess、OpenHands和mini-SWE-agent三种先进工具的性能，在文件级和函数级定位中平均提升Hit@k超过22%，Recall@k超过46%。在不同模型上Hit@1和Recall@10分别提升高达504%和376%。

Conclusion: RepoLens通过概念知识抽象有效解决了大规模代码仓库的问题定位挑战，消融研究和人工评估证实了构建的关注点的有效性和可靠性。

Abstract: Issue localization, which identifies faulty code elements such as files or
functions, is critical for effective bug fixing. While recent LLM-based and
LLM-agent-based approaches improve accuracy, they struggle in large-scale
repositories due to concern mixing, where relevant logic is buried in large
functions, and concern scattering, where related logic is dispersed across
files.
  To address these challenges, we propose RepoLens, a novel approach that
abstracts and leverages conceptual knowledge from code repositories. RepoLens
decomposes fine-grained functionalities and recomposes them into high-level
concerns, semantically coherent clusters of functionalities that guide LLMs. It
operates in two stages: an offline stage that extracts and enriches conceptual
knowledge into a repository-wide knowledge base, and an online stage that
retrieves issue-specific terms, clusters and ranks concerns by relevance, and
integrates them into localization workflows via minimally intrusive prompt
enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks
derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art
tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains
of over 22% in Hit@k and 46% in Recall@k for file- and function-level
localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with
Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies
and manual evaluation confirm the effectiveness and reliability of the
constructed concerns.

</details>


### [89] [No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials](https://arxiv.org/abs/2509.21816)
*Yuhang Xie,Jian Mu,Xiaojun Ma,Chaoyun Zhang,Lu Wang,Mengyu Zhou,Mugeng Liu,Si Qin,Qingwei Lin,Saravan Rajmohan,Shi Han,Dongmei Zhang*

Main category: cs.SE

TL;DR: 提出了首个从自然语言任务描述自动生成Excel教程的框架，通过执行代理在Excel中规划和执行解决方案，收集中间工件并转换为结构化文档和视频演示，显著提高了教程生成效率。


<details>
  <summary>Details</summary>
Motivation: Excel功能复杂但广泛使用，现有教程依赖专家手动编写，更新成本高且无法自动化，需要解决自动生成高质量教程的挑战。

Method: 框架包含任务实例化、执行代理在Excel中规划执行解决方案、收集中间工件，然后转换为结构化Excel文档和视频演示。

Result: 在1559个真实场景任务上的实验显示，任务执行成功率比现有最佳方法提高8.5%，生成教程的可读性和教学效果接近或超过专家编写材料，时间成本降至专家编写的1/20。

Conclusion: 该自动化流水线首次实现了可扩展的高质量教程生成，消除了人工劳动，使大规模Excel教程生成变得实用可行。

Abstract: Excel is one of the most widely used productivity tools across domains,
offering rich functionality but also overwhelming users with its complexity.
This creates a persistent demand for tutorials to support effective usage.
However, existing tutorials are manually authored by experts, require frequent
updates after each software release, and incur substantial labor costs. Prior
work has not achieved fully automated tutorial generation, since existing
methods still depend on handcrafted operation sequences or example materials.
In this paper, we present the first framework for automatically generating
Excel tutorials directly from natural language task descriptions. Our framework
first instantiates the task. Then a central component of this framework,
Execution Agent, plans and executes the solution in Excel, and collects the
intermediate artifacts required for tutorial construction. These artifacts are
then transformed into both structured Excel documents and video demonstrations.
To build a comprehensive tutorial corpus, we collected 1,559 task descriptions
from real-world scenarios. In addition, we designed a systematic evaluation
framework that integrates assessments from both large language models (LLMs)
and human reviewers. Experimental results show that our framework improves task
execution success rates by 8.5% over state-of-the-art baselines. Moreover, the
generated tutorials demonstrate superior readability and instructional
effectiveness, often approaching or surpassing expert-authored materials.
Importantly, the automated pipeline eliminates manual labor and reduces time
costs to 1/20 of expert authoring, making scalable and high-quality tutorial
generation practical for the first time.

</details>


### [90] [AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans](https://arxiv.org/abs/2509.21891)
*Yangtian Zi,Zixuan Wu,Aleksander Boruch-Gruszecki,Jonathan Bell,Arjun Guha*

Main category: cs.SE

TL;DR: AgentPack是一个包含130万代码编辑的数据集，这些编辑由Claude Code、OpenAI Codex和Cursor Agent等软件工程代理与人类共同完成。研究表明，使用AgentPack微调的模型在代码编辑任务上优于基于传统人类提交数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于提交和拉取请求的代码编辑数据存在噪声问题：提交信息简短、人类提交混杂多个无关编辑、许多提交来自简单的基于规则的机器人。软件工程代理的出现改变了这一局面，它们与人类共同完成的代码变更更聚焦、目标更清晰。

Method: 构建AgentPack语料库，包含130万代码编辑，来自Claude Code、OpenAI Codex和Cursor Agent在公共GitHub项目中的协作编辑。开发了识别和筛选流程，量化了这些代理的采用趋势，并分析了编辑的结构特性。

Result: 基于AgentPack微调的模型在代码编辑任务上表现优于基于传统人类提交数据训练的模型，证明了使用软件工程代理产生的公共数据训练未来代码编辑模型的潜力。

Conclusion: 软件工程代理产生的代码编辑数据质量更高，为训练更好的代码编辑模型提供了新的高质量数据源。

Abstract: Fine-tuning large language models for code editing has typically relied on
mining commits and pull requests. The working hypothesis has been that commit
messages describe human intent in natural language, and patches to code
describe the changes that implement that intent. However, much of the
previously collected data is noisy: commit messages are terse, human-written
commits commingle several unrelated edits, and many commits come from simple,
rule-based bots.
  The recent adoption of software engineering agents changes this landscape.
Code changes co-authored by humans and agents tend to be more narrowly scoped
and focused on clearer goals. Their commit messages, generated by LLMs,
articulate intent and rationale in much greater detail. Moreover, when these
changes land in public repositories, they are implicitly filtered by humans:
maintainers discard low-quality commits to their projects.
  We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code,
OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August
2025. We describe the identification and curation pipeline, quantify adoption
trends of these agents, and analyze the structural properties of the edits.
Finally, we show that models fine-tuned on AgentPack can outperform models
trained on prior human-only commit corpora, highlighting the potential of using
public data from software engineering agents to train future code-editing
models.

</details>


### [91] [SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios](https://arxiv.org/abs/2509.22097)
*Junkai Chen,Huihui Huang,Yunbo Lyu,Junwen An,Jieke Shi,Chengran Yang,Ting Zhang,Haoye Tian,Yikun Li,Zhenhao Li,Xin Zhou,Xing Hu,David Lo*

Main category: cs.SE

TL;DR: SecureAgentBench是一个包含105个编码任务的基准测试，用于严格评估代码代理在安全代码生成方面的能力，发现当前代理在生成安全代码方面表现不佳，即使最佳代理也只能达到15.2%的正确且安全解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估代码代理生成的安全代码方面存在不足，往往忽略漏洞引入的真实上下文，或采用狭窄的评估协议无法捕捉功能正确性或新引入的漏洞。

Method: 构建包含105个编码任务的SecureAgentBench基准，每个任务包含：(i)需要在大仓库中进行多文件编辑的现实任务设置，(ii)基于真实开源漏洞的上下文，(iii)结合功能测试、漏洞检查和新漏洞检测的综合评估。评估了三个代表性代理和三个最先进的大语言模型。

Result: 当前代理在生成安全代码方面表现不佳，最佳代理(SWE-agent + DeepSeek-V3.1)仅达到15.2%的正确且安全解决方案；一些代理生成功能正确代码但仍引入漏洞；明确的安全指令对改善安全编码没有显著帮助。

Conclusion: SecureAgentBench为安全代码生成提供了一个严格的基准测试，是迈向更可靠LLM软件开发的一步，但当前代码代理在安全代码生成方面仍有很大改进空间。

Abstract: Large language model (LLM) powered code agents are rapidly transforming
software engineering by automating tasks such as testing, debugging, and
repairing, yet the security risks of their generated code have become a
critical concern. Existing benchmarks have offered valuable insights but remain
insufficient: they often overlook the genuine context in which vulnerabilities
were introduced or adopt narrow evaluation protocols that fail to capture
either functional correctness or newly introduced vulnerabilities. We therefore
introduce SecureAgentBench, a benchmark of 105 coding tasks designed to
rigorously evaluate code agents' capabilities in secure code generation. Each
task includes (i) realistic task settings that require multi-file edits in
large repositories, (ii) aligned contexts based on real-world open-source
vulnerabilities with precisely identified introduction points, and (iii)
comprehensive evaluation that combines functionality testing, vulnerability
checking through proof-of-concept exploits, and detection of newly introduced
vulnerabilities using static analysis. We evaluate three representative agents
(SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7
Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents
struggle to produce secure code, as even the best-performing one, SWE-agent
supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions,
(ii) some agents produce functionally correct code but still introduce
vulnerabilities, including new ones not previously recorded, and (iii) adding
explicit security instructions for agents does not significantly improve secure
coding, underscoring the need for further research. These findings establish
SecureAgentBench as a rigorous benchmark for secure code generation and a step
toward more reliable software development with LLMs.

</details>


### [92] [Leveraging LLM Agents for Automated Video Game Testing](https://arxiv.org/abs/2509.22170)
*Chengjia Wang,Lanling Tang,Ming Yuan,Jiongchi Yu,Xiaofei Xie,Jiajun Bu*

Main category: cs.SE

TL;DR: TITAN是一个基于LLM的智能MMORPG测试框架，通过状态感知、动作优化、长程推理和LLM预言机等组件，显著提升了游戏测试的任务完成率和bug检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统自动化游戏测试方法在MMORPG这类复杂开放环境中难以实现高状态覆盖率和效率，而现有LLM游戏测试方法对复杂游戏状态-动作空间和长复杂任务的理解能力有限。

Method: TITAN框架包含四个核心组件：高维游戏状态感知与抽象、主动优化和优先级排序可用动作、具有动作轨迹记忆和反思自校正的长程推理能力、基于LLM的预言机检测功能性和逻辑bug。

Result: 在两个大型商业MMORPG上的实验显示，TITAN达到95%的任务完成率，bug检测性能显著优于现有方法，发现了四个先前未知的bug，并已在八个真实游戏QA流程中部署。

Conclusion: TITAN证明了LLM驱动框架在复杂游戏测试中的有效性，为推进智能通用测试系统提供了新方向。

Abstract: Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a
critical yet labor-intensive task in game development due to their complexity
and frequent updating nature. Traditional automated game testing approaches
struggle to achieve high state coverage and efficiency in these rich,
open-ended environments, while existing LLM-based game-playing approaches are
limited to shallow reasoning ability in understanding complex game state-action
spaces and long-complex tasks. To address these challenges, we propose TITAN,
an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN
incorporates four key components to: (1) perceive and abstract high-dimensional
game states, (2) proactively optimize and prioritize available actions, (3)
enable long-horizon reasoning with action trace memory and reflective
self-correction, and (4) employ LLM-based oracles to detect potential
functional and logic bugs with diagnostic reports.
  We implement the prototype of TITAN and evaluate it on two large-scale
commercial MMORPGs spanning both PC and mobile platforms. In our experiments,
TITAN achieves significantly higher task completion rates (95%) and bug
detection performance compared to existing automated game testing approaches.
An ablation study further demonstrates that each core component of TITAN
contributes substantially to its overall performance. Notably, TITAN detects
four previously unknown bugs that prior testing approaches fail to identify. We
provide an in-depth discussion of these results, which offer guidance for new
avenues of advancing intelligent, general-purpose testing systems. Moreover,
TITAN has been deployed in eight real-world game QA pipelines, underscoring its
practical impact as an LLM-driven game testing framework.

</details>


### [93] [Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries](https://arxiv.org/abs/2509.22202)
*Lukas Twist,Jie M. Zhang,Mark Harman,Helen Yannakoudakis*

Main category: cs.SE

TL;DR: 系统研究用户提示变化对LLM生成代码中库幻觉的影响，发现单字符拼写错误可触发26%的幻觉，虚假库名被接受率高达99%，时间相关提示导致84%的幻觉。


<details>
  <summary>Details</summary>
Motivation: LLM在生成代码时经常产生库幻觉（虚构不存在的库），这些错误不仅会误导开发者、破坏构建，还可能带来供应链安全威胁。目前缺乏对真实世界提示变化如何影响幻觉率的系统性了解。

Method: 评估6个不同的LLM，研究两种幻觉类型：库名幻觉（无效导入）和库成员幻觉（有效库的无效调用）。使用从开发者论坛提取的真实用户语言和不同程度的用户错误（单字符/多字符拼写错误、完全虚假名称/成员）。

Result: 发现系统性漏洞：库名单字符拼写错误在26%的任务中触发幻觉，虚假库名在99%的任务中被接受，时间相关提示在84%的任务中导致幻觉。提示工程有缓解潜力但不稳定且依赖具体LLM。

Conclusion: LLM对自然提示变化极其脆弱，迫切需要针对库相关幻觉及其潜在利用的安全防护措施。

Abstract: Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.

</details>


### [94] [TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search](https://arxiv.org/abs/2509.22431)
*Zhengyu Chen,Zhaoyi Meng,Wenxiang Zhao,Wansen Wang,Haoyang Zhao,Jiahao Zhan,Jie Cui,Hong Zhong*

Main category: cs.SE

TL;DR: TreeMind结合LLM与蒙特卡洛树搜索，通过战略性的UI探索实现Android应用崩溃的自动复现，在93个真实bug报告上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习或LLM的方法在复现不完整bug报告时存在局限，难以推断未观察步骤并在复杂的UI交互空间中导航，主要因为缺乏目标导向的推理和规划能力。

Method: 将复现任务建模为目标驱动搜索问题，使用MCTS作为核心规划机制，引入两个LLM引导的智能体：Expander生成有前景的动作，Simulator估计动作成功概率，结合多模态UI输入和高级提示技术进行反馈感知导航。

Result: 在三个广泛使用的基准测试中的93个真实Android bug报告上，TreeMind在复现成功率上显著优于四种最先进的基线方法。

Conclusion: 将LLM推理与基于MCTS的规划相结合是自动化bug复现的一个有前景的方向。

Abstract: Automatically reproducing Android app crashes from textual bug reports is
challenging, particularly when the reports are incomplete and the modern UI
exhibits high combinatorial complexity. Existing approaches based on
reinforcement learning or large language models (LLMs) exhibit limitations in
such scenarios. They struggle to infer unobserved steps and reconstruct the
underlying user action sequences to navigate the vast UI interaction space,
primarily due to limited goal-directed reasoning and planning. We present
TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo
Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug
reproduction. To the best of our knowledge, this is the first work to combine
external decision-making with LLM semantic reasoning for reliable bug
reproduction. We formulate the reproduction task as a target-driven search
problem, leveraging MCTS as the core planning mechanism to iteratively refine
action sequences. To enhance MCTS with semantic reasoning, we introduce two
LLM-guided agents with distinct roles: Expander generates top-k promising
actions based on the current UI state and exploration history, while Simulator
estimates the likelihood that each action leads toward successful reproduction.
By incorporating multi-modal UI inputs and advanced prompting techniques,
TreeMind conducts feedback-aware navigation that identifies missing but
essential user actions and incrementally reconstructs the reproduction paths.
We evaluate TreeMind on a dataset of 93 real-world Android bug reports from
three widely-used benchmarks. Experimental results show that it significantly
outperforms four state-of-the-art baselines in reproduction success rate. A
real-world case study indicates that integrating LLM reasoning with MCTS-based
planning is a compelling direction for automated bug reproduction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [95] [GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models](https://arxiv.org/abs/2509.21593)
*Peng Luo,Xiayin Lou,Yu Zheng,Zhuo Zheng,Stefano Ermon*

Main category: cs.AI

TL;DR: GeoEvolve是一个多智能体LLM框架，通过结合进化搜索和地理空间领域知识，自动设计和优化地理空间算法，在空间插值和不确定性量化任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的算法发现框架缺乏地理空间领域知识和多步推理能力，无法有效解决复杂的地理空间问题。

Method: 采用双层循环结构：内环使用代码进化器生成和突变候选解，外环通过智能控制器评估全局精英并查询GeoKnowRAG模块（结构化地理空间知识库）来注入地理学理论先验。

Result: 在空间插值任务上减少RMSE误差13-21%，在不确定性估计任务上提升性能17%。消融研究证实领域知识引导的检索对稳定高质量进化至关重要。

Conclusion: GeoEvolve为自动化、知识驱动的地理空间建模提供了可扩展路径，为可信赖和高效的AI-for-Science发现开辟了新机会。

Abstract: Geospatial modeling provides critical solutions for pressing global
challenges such as sustainability and climate change. Existing large language
model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at
evolving generic code but lack the domain knowledge and multi-step reasoning
required for complex geospatial problems. We introduce GeoEvolve, a multi-agent
LLM framework that couples evolutionary search with geospatial domain knowledge
to automatically design and refine geospatial algorithms. GeoEvolve operates in
two nested loops: an inner loop leverages a code evolver to generate and mutate
candidate solutions, while an outer agentic controller evaluates global elites
and queries a GeoKnowRAG module -- a structured geospatial knowledge base that
injects theoretical priors from geography. This knowledge-guided evolution
steers the search toward theoretically meaningful and computationally efficient
algorithms. We evaluate GeoEvolve on two fundamental and classical tasks:
spatial interpolation (kriging) and spatial uncertainty quantification
(geospatial conformal prediction). Across these benchmarks, GeoEvolve
automatically improves and discovers new algorithms, incorporating geospatial
theory on top of classical models. It reduces spatial interpolation error
(RMSE) by 13-21% and enhances uncertainty estimation performance by 17\%.
Ablation studies confirm that domain-guided retrieval is essential for stable,
high-quality evolution. These results demonstrate that GeoEvolve provides a
scalable path toward automated, knowledge-driven geospatial modeling, opening
new opportunities for trustworthy and efficient AI-for-Science discovery.

</details>


### [96] [Can AI Perceive Physical Danger and Intervene?](https://arxiv.org/abs/2509.21651)
*Abhishek Jindal,Dmitry Kalashnikov,Oscar Chang,Divya Garikapati,Anirudha Majumdar,Pierre Sermanet,Vikas Sindhwani*

Main category: cs.AI

TL;DR: 本文开发了一个可扩展的物理安全基准测试方法，用于评估具身AI系统对物理安全的理解能力，并提出了后训练范式来提升模型的安全推理能力。


<details>
  <summary>Details</summary>
Motivation: 当AI与物理世界交互时，存在直接的物理伤害风险，需要评估现有基础模型对物理安全的常识理解能力。

Method: 基于真实伤害叙事和操作安全约束创建逼真图像和视频，分析主要基础模型的风险感知能力，并开发后训练范式来教授模型显式推理具身安全约束。

Result: 模型生成可解释的安全推理轨迹，在约束满足评估中达到最先进性能。

Conclusion: 该基准测试为安全关键型具身AI应用提供了重要的部署准备度评估工具。

Abstract: When AI interacts with the physical world -- as a robot or an assistive agent
-- new safety challenges emerge beyond those of purely ``digital AI". In such
interactions, the potential for physical harm is direct and immediate. How well
do state-of-the-art foundation models understand common-sense facts about
physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of
coffee should not be handed to a child? In this paper, our contributions are
three-fold: first, we develop a highly scalable approach to continuous physical
safety benchmarking of Embodied AI systems, grounded in real-world injury
narratives and operational safety constraints. To probe multi-modal safety
understanding, we turn these narratives and constraints into photorealistic
images and videos capturing transitions from safe to unsafe states, using
advanced generative models. Secondly, we comprehensively analyze the ability of
major foundation models to perceive risks, reason about safety, and trigger
interventions; this yields multi-faceted insights into their deployment
readiness for safety-critical agentic applications. Finally, we develop a
post-training paradigm to teach models to explicitly reason about
embodiment-specific safety constraints provided through system instructions.
The resulting models generate thinking traces that make safety reasoning
interpretable and transparent, achieving state of the art performance in
constraint satisfaction evaluations. The benchmark will be released at
https://asimov-benchmark.github.io/v2

</details>


### [97] [UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios](https://arxiv.org/abs/2509.21766)
*Haotian Luo,Huaisong Zhang,Xuelin Zhang,Haoyu Wang,Zeyu Qin,Wenjie Lu,Guozheng Ma,Haiying He,Yingsha Xie,Qiyang Zhou,Zixuan Hu,Hongze Mi,Yibo Wang,Naiqiang Tan,Hong Chen,Yi R. Fung,Chun Yuan,Li Shen*

Main category: cs.AI

TL;DR: 提出了UltraHorizon基准测试，用于评估智能体在长视野、部分可观测任务中的核心能力，发现现有LLM智能体在这些复杂场景中表现不佳，与人类存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注短视野、完全可观测任务，而现实世界中的关键任务（如软件开发、投资、科学发现）往往需要长视野推理、规划、记忆管理和工具使用能力，缺乏系统性评估这些能力的基准。

Method: 设计了UltraHorizon基准，通过三个不同环境中的探索任务来验证智能体的核心能力，任务要求智能体通过持续推理、规划、记忆和工具管理来迭代发现隐藏规则。

Result: 实验显示LLM智能体在长视野任务中表现不佳，而人类参与者得分更高；在最大规模设置下轨迹平均超过20万token和400+工具调用，标准配置也超过3.5万token和60+工具调用；简单扩展方法在任务中失败。

Conclusion: 智能体在长视野能力上存在持续差距，识别出8种错误类型，归因于两个主要原因：上下文锁定和功能性基础能力差距。

Abstract: Autonomous agents have recently achieved remarkable progress across diverse
domains, yet most evaluations focus on short-horizon, fully observable tasks.
In contrast, many critical real-world tasks, such as large-scale software
development, commercial investment, and scientific discovery, unfold in
long-horizon and partially observable scenarios where success hinges on
sustained reasoning, planning, memory management, and tool use. Existing
benchmarks rarely capture these long-horizon challenges, leaving a gap in
systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a
novel benchmark that measures the foundational capabilities essential for
complex real-world challenges. We use exploration as a unifying task across
three distinct environments to validate these core competencies. Agents are
designed in long-horizon discovery tasks where they must iteratively uncover
hidden rules through sustained reasoning, planning, memory and tools
management, and interaction with environments. Under the heaviest scale
setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool
calls, whereas in standard configurations they still exceed \textbf{35k} tokens
and involve more than \textbf{60} tool calls on average. Our extensive
experiments reveal that LLM-agents consistently underperform in these settings,
whereas human participants achieve higher scores, underscoring a persistent gap
in agents' long-horizon abilities. We also observe that simple scaling fails in
our task. To better illustrate the failure of agents, we conduct an in-depth
analysis of collected trajectories. We identify eight types of errors and
attribute them to two primary causes: in-context locking and functional
fundamental capability gaps.
\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available
here.}

</details>


### [98] [Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety](https://arxiv.org/abs/2509.21782)
*Junliang Liu,Jingyu Xiao,Wenxin Tang,Wenxuan Wang,Zhixian Wang,Minrui Zhang,Shuanghe Yu*

Main category: cs.AI

TL;DR: 提出了WebRSSBench基准测试，用于全面评估多模态大语言模型在网页理解中的推理能力、鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要关注视觉感知或UI代码生成，缺乏对构建端到端网页应用所需推理、鲁棒性和安全能力的评估。

Method: 从729个网站构建包含3799个问答对的基准测试，涵盖8个任务类型，采用标准化提示、确定性评估脚本和多阶段质量控制。

Result: 评估12个MLLM发现，模型在真实布局的组合推理、面对UI扰动的鲁棒性以及安全关键操作识别方面存在显著差距。

Conclusion: 需要开发更强大的网页理解模型来应对复杂的网页应用场景。

Abstract: Multimodal large language models (MLLMs) are increasingly positioned as AI
collaborators for building complex web-related applications like GUI agents and
front-end code generation. However, existing benchmarks largely emphasize
visual perception or UI code generation, showing insufficient evaluation on the
reasoning, robustness and safety capability required for end-to-end web
applications. To bridge the gap, we introduce a comprehensive web understanding
benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and
Safety across eight tasks, such as position relationship reasoning, color
robustness, and safety critical detection, etc. The benchmark is constructed
from 729 websites and contains 3799 question answer pairs that probe multi-step
inference over page structure, text, widgets, and safety-critical interactions.
To ensure reliable measurement, we adopt standardized prompts, deterministic
evaluation scripts, and multi-stage quality control combining automatic checks
with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The
results reveal significant gaps, models still struggle with compositional and
cross-element reasoning over realistic layouts, show limited robustness when
facing perturbations in user interfaces and content such as layout
rearrangements or visual style shifts, and are rather conservative in
recognizing and avoiding safety critical or irreversible actions. Our code is
available at https://github.com/jinliang-byte/webssrbench.

</details>


### [99] [D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents](https://arxiv.org/abs/2509.21799)
*Hongze Mi,Yibo Feng,Wenjie Lu,Yuqi Wang,Jinyuan Li,Song Cao,He Cui,Tengfei Tian,Xuelin Zhang,Haotian Luo,Di Sun,Naiqiang Tan,Gang Pan*

Main category: cs.AI

TL;DR: D-Artemis是一个基于思考-对齐-反思认知循环的GUI代理框架，通过应用特定提示检索、执行前对齐检查、执行后状态反思等机制，在无需复杂轨迹数据训练的情况下显著提升多模态大语言模型在GUI任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理面临数据瓶颈、延迟错误检测成本高和矛盾指导风险等挑战，需要更有效的认知框架来提升任务执行成功率。

Method: 提出D-Artemis框架，包含应用特定提示检索机制、执行前对齐阶段（思想-行动一致性检查和行动校正代理）、执行后状态反思代理，形成完整的认知循环。

Result: 在AndroidWorld上达到75.8%的成功率，在ScreenSpot-V2上达到96.8%的成功率，创下新的最先进水平。消融研究证明各组件对框架性能均有显著贡献。

Conclusion: D-Artemis通过模拟人类认知循环，在不依赖复杂轨迹数据集训练的情况下，有效增强了通用多模态大语言模型在GUI任务中的能力，展现出强大的泛化性能。

Abstract: Graphical User Interface (GUI) agents aim to automate a wide spectrum of
human tasks by emulating user interaction. Despite rapid advancements, current
approaches are hindered by several critical challenges: data bottleneck in
end-to-end training, high cost of delayed error detection, and risk of
contradictory guidance. Inspired by the human cognitive loop of Thinking,
Alignment, and Reflection, we present D-Artemis -- a novel deliberative
framework in this paper. D-Artemis leverages a fine-grained, app-specific tip
retrieval mechanism to inform its decision-making process. It also employs a
proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)
Check module and Action Correction Agent (ACA) work in concert to mitigate the
risk of execution failures. A post-execution Status Reflection Agent (SRA)
completes the cognitive loop, enabling strategic learning from experience.
Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal
large language models (MLLMs) for GUI tasks without the need for training on
complex trajectory datasets, demonstrating strong generalization. D-Artemis
establishes new state-of-the-art (SOTA) results across both major benchmarks,
achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.
Extensive ablation studies further demonstrate the significant contribution of
each component to the framework.

</details>


### [100] [DS-STAR: Data Science Agent via Iterative Planning and Verification](https://arxiv.org/abs/2509.21825)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Tomas Pfister*

Main category: cs.AI

TL;DR: DS-STAR是一个新型数据科学代理，通过自动数据文件分析、LLM验证步骤和迭代规划机制，有效处理异构数据格式并生成优化的分析计划。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在处理异构数据格式和生成充分分析计划方面存在困难，特别是在没有真实标签的开放式任务中验证计划充分性具有挑战性。

Method: 包含三个核心组件：数据文件分析模块自动探索和提取多样数据格式的上下文；LLM验证步骤评估各阶段分析计划的充分性；顺序规划机制从简单可执行计划开始，基于反馈迭代优化直到验证通过。

Result: 在DABStep、KramaBench和DA-Code三个基准测试中达到最先进性能，特别是在需要处理多个异构数据文件的困难任务上显著优于基线方法。

Conclusion: DS-STAR通过其迭代验证和规划机制，能够可靠地导航涉及多样数据源的复杂分析任务，有效解决了数据科学中的异构数据格式挑战。

Abstract: Data science, which transforms raw data into actionable insights, is critical
for data-driven decision-making. However, these tasks are often complex,
involving steps for exploring multiple data sources and synthesizing findings
to deliver insightful answers. While large language models (LLMs) show
significant promise in automating this process, they often struggle with
heterogeneous data formats and generate sub-optimal analysis plans, as
verifying plan sufficiency is inherently difficult without ground-truth labels
for such open-ended tasks. To overcome these limitations, we introduce DS-STAR,
a novel data science agent. Specifically, DS-STAR makes three key
contributions: (1) a data file analysis module that automatically explores and
extracts context from diverse data formats, including unstructured types; (2) a
verification step where an LLM-based judge evaluates the sufficiency of the
analysis plan at each stage; and (3) a sequential planning mechanism that
starts with a simple, executable plan and iteratively refines it based on the
DS-STAR's feedback until its sufficiency is verified. This iterative refinement
allows DS-STAR to reliably navigate complex analyses involving diverse data
sources. Our experiments show that DS-STAR achieves state-of-the-art
performance across three challenging benchmarks: DABStep, KramaBench, and
DA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks
that require processing multiple data files with heterogeneous formats.

</details>


### [101] [DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents](https://arxiv.org/abs/2509.21842)
*Yansong Ning,Rui Liu,Jun Wang,Kai Chen,Wei Li,Jun Fang,Kan Zheng,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: DeepTravel是一个端到端的智能强化学习框架，用于构建自主旅行规划代理，能够自主规划、执行工具并反思工具响应，在多步推理中探索、验证和优化中间行动。


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划代理依赖手工制作的提示和固定的代理工作流程，限制了更灵活和自主的旅行规划代理的发展。

Method: 构建沙盒环境缓存交通、住宿和POI数据；开发分层奖励建模系统；提出回放增强强化学习方法，从失败经验缓冲区周期性回放。

Result: 在滴滴企业解决方案应用上部署，综合在线和离线评估显示，DeepTravel使小型LLM（如Qwen3 32B）在旅行规划任务中显著优于OpenAI o1、o3和DeepSeek R1等前沿LLM。

Conclusion: DeepTravel框架成功构建了自主旅行规划代理，通过强化学习显著提升了代理能力，使小型模型超越大型前沿模型。

Abstract: Travel planning (TP) agent has recently worked as an emerging building block
to interact with external tools and resources for travel itinerary generation,
ensuring enjoyable user experience. Despite its benefits, existing studies rely
on hand craft prompt and fixed agent workflow, hindering more flexible and
autonomous TP agent. This paper proposes DeepTravel, an end to end agentic
reinforcement learning framework for building autonomous travel planning agent,
capable of autonomously planning, executing tools, and reflecting on tool
responses to explore, verify, and refine intermediate actions in multi step
reasoning. To achieve this, we first construct a robust sandbox environment by
caching transportation, accommodation and POI data, facilitating TP agent
training without being constrained by real world APIs limitations (e.g.,
inconsistent outputs). Moreover, we develop a hierarchical reward modeling
system, where a trajectory level verifier first checks spatiotemporal
feasibility and filters unsatisfied travel itinerary, and then the turn level
verifier further validate itinerary detail consistency with tool responses,
enabling efficient and precise reward service. Finally, we propose the reply
augmented reinforcement learning method that enables TP agent to periodically
replay from a failures experience buffer, emerging notable agentic capacity. We
deploy trained TP agent on DiDi Enterprise Solutions App and conduct
comprehensive online and offline evaluations, demonstrating that DeepTravel
enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing
frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.

</details>


### [102] [Reimagining Agent-based Modeling with Large Language Model Agents via Shachi](https://arxiv.org/abs/2509.21862)
*So Kuroki,Yingtao Tian,Kou Misaki,Takashi Ikegami,Takuya Akiba,Yujin Tang*

Main category: cs.AI

TL;DR: Shachi是一个用于LLM驱动多智能体系统的模块化框架，将智能体策略分解为配置、记忆和工具三个核心认知组件，通过LLM推理引擎协调，支持对集体行为的系统性分析。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动多智能体系统的涌现行为研究缺乏受控实验的原则性方法，限制了该领域的发展。

Method: 提出Shachi框架，将智能体策略分解为配置（内在特质）、记忆（上下文持久性）和工具（扩展能力）三个核心组件，由LLM推理引擎协调，实现系统化的架构选择分析。

Result: 在10任务基准测试中验证了方法的有效性，通过建模美国关税冲击实验证明，只有当智能体认知架构正确配置记忆和工具时，其行为才能与观察到的市场反应一致。

Conclusion: Shachi为构建和评估LLM智能体提供了严谨的开源基础，旨在促进更具累积性和科学基础的研究。

Abstract: The study of emergent behaviors in large language model (LLM)-driven
multi-agent systems is a critical research challenge, yet progress is limited
by a lack of principled methodologies for controlled experimentation. To
address this, we introduce Shachi, a formal methodology and modular framework
that decomposes an agent's policy into core cognitive components: Configuration
for intrinsic traits, Memory for contextual persistence, and Tools for expanded
capabilities, all orchestrated by an LLM reasoning engine. This principled
architecture moves beyond brittle, ad-hoc agent designs and enables the
systematic analysis of how specific architectural choices influence collective
behavior. We validate our methodology on a comprehensive 10-task benchmark and
demonstrate its power through novel scientific inquiries. Critically, we
establish the external validity of our approach by modeling a real-world U.S.
tariff shock, showing that agent behaviors align with observed market reactions
only when their cognitive architecture is appropriately configured with memory
and tools. Our work provides a rigorous, open-source foundation for building
and evaluating LLM agents, aimed at fostering more cumulative and
scientifically grounded research.

</details>


### [103] [CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration](https://arxiv.org/abs/2509.21981)
*Zhimin Wang,Shaokang He,Duo Wu,Jinghe Wang,Linjia Kang,Jing Yu,Zhi Wang*

Main category: cs.AI

TL;DR: CoBel-World是一个为LLM代理设计的协作信念世界框架，通过符号信念语言建模物理环境和合作者心理状态，实现零样本贝叶斯式信念更新，显著减少通信成本并提高任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM协作框架忽视了动态意图推理的潜力，导致计划不一致和冗余通信，降低协作效率。

Method: 使用符号信念语言将开放世界任务知识解析为结构化信念，通过LLM推理进行零样本贝叶斯式信念更新，主动检测潜在协调问题并自适应通信。

Result: 在TDW-MAT和C-WAH基准测试中，通信成本减少22-60%，任务完成效率提高4-28%。

Conclusion: 显式的意图感知信念建模对于基于LLM的多代理系统实现高效、类人协作至关重要。

Abstract: Effective real-world multi-agent collaboration requires not only accurate
planning but also the ability to reason about collaborators' intents -- a
crucial capability for avoiding miscoordination and redundant communication
under partial observable environments. Due to their strong planning and
reasoning capabilities, large language models (LLMs) have emerged as promising
autonomous agents for collaborative task solving. However, existing
collaboration frameworks for LLMs overlook their reasoning potential for
dynamic intent inference, and thus produce inconsistent plans and redundant
communication, reducing collaboration efficiency. To bridge this gap, we
propose CoBel-World, a novel framework that equips LLM agents with a
collaborative belief world -- an internal representation jointly modeling the
physical environment and collaborators' mental states. CoBel-World enables
agents to parse open-world task knowledge into structured beliefs via a
symbolic belief language, and perform zero-shot Bayesian-style belief updates
through LLM reasoning. This allows agents to proactively detect potential
miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated
on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World
significantly reduces communication costs by 22-60% and improves task
completion efficiency by 4-28% compared to the strongest baseline. Our results
show that explicit, intent-aware belief modeling is essential for efficient and
human-like collaboration in LLM-based multi-agent systems.

</details>


### [104] [GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments](https://arxiv.org/abs/2509.21998)
*Hanlin Zhu,Tianyu Guo,Song Mei,Stuart Russell,Nikhil Ghosh,Alberto Bietti,Jiantao Jiao*

Main category: cs.AI

TL;DR: 提出了GSM-Agent基准测试，评估LLM在需要主动使用工具收集信息来解决小学数学问题时的代理推理能力，并发现即使前沿模型准确率也仅67%。提出了代理推理图概念来分析推理模式，并开发了工具增强的测试时缩放方法来改进性能。


<details>
  <summary>Details</summary>
Motivation: 当前代理基准测试往往将代理推理与复杂的数学推理、专家级知识等能力混在一起，难以分离评估纯粹的代理推理能力。需要专门评估LLM结合工具使用和推理的能力。

Method: 构建GSM-Agent基准测试，要求LLM代理解决小学数学问题但仅提供问题而不提供前提信息，需要主动使用工具收集信息。提出代理推理图概念来聚类文档嵌入并映射工具调用，分析推理模式。开发工具增强的测试时缩放方法鼓励模型重新访问节点。

Result: 即使GPT-5等前沿模型在GSM-Agent基准上准确率也仅为67%。发现许多模型在代理推理中缺乏重新访问先前节点的能力，这是静态推理中的关键模式。

Conclusion: GSM-Agent基准和代理推理框架有助于未来理解和推进代理推理能力的研究。工具增强的测试时缩放方法可以改进LLM的代理推理性能。

Abstract: As LLMs are increasingly deployed as agents, agentic reasoning - the ability
to combine tool use, especially search, and reasoning - becomes a critical
skill. However, it is hard to disentangle agentic reasoning when evaluated in
complex environments and tasks. Current agent benchmarks often mix agentic
reasoning with challenging math reasoning, expert-level knowledge, and other
advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent,
where an LLM agent is required to solve grade-school-level reasoning problems,
but is only presented with the question in the prompt without the premises that
contain the necessary information to solve the task, and needs to proactively
collect that information using tools. Although the original tasks are
grade-school math problems, we observe that even frontier models like GPT-5
only achieve 67% accuracy. To understand and analyze the agentic reasoning
patterns, we propose the concept of agentic reasoning graph: cluster the
environment's document embeddings into nodes, and map each tool call to its
nearest node to build a reasoning path. Surprisingly, we identify that the
ability to revisit a previously visited node, widely taken as a crucial pattern
in static reasoning, is often missing for agentic reasoning for many models.
Based on the insight, we propose a tool-augmented test-time scaling method to
improve LLM's agentic reasoning performance by adding tools to encourage models
to revisit. We expect our benchmark and the agentic reasoning framework to aid
future studies of understanding and pushing the boundaries of agentic
reasoning.

</details>


### [105] [PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning](https://arxiv.org/abs/2509.22315)
*Hieu Tran,Zonghai Yao,Nguyen Luong Tran,Zhichao Yang,Feiyun Ouyang,Shuo Han,Razieh Rahimi,Hong Yu*

Main category: cs.AI

TL;DR: PRIME是一个多智能体推理框架，通过整合快速直觉思维（系统1）和慢速深思熟虑思维（系统2）来增强LLM的推理能力，使开源模型在复杂推理任务上达到与闭源模型竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 受人类认知的双过程理论启发，旨在模拟人类思维过程，通过动态整合快速直觉和慢速深思两种思维模式来提升LLM的推理效率和准确性。

Method: 采用多智能体架构：首先由快速思维智能体（系统1）生成快速答案，如果检测到不确定性，则触发包含规划、假设生成、检索、信息整合和决策等专门智能体的系统2推理流程。

Result: 实验结果显示，使用LLaMA 3模型的PRIME框架在需要多跳和知识基础推理的基准测试中，性能可与GPT-4和GPT-4o等最先进闭源模型竞争。

Conclusion: PRIME为改进LLM在需要复杂、知识密集型推理领域的性能提供了一个可扩展的解决方案。

Abstract: Inspired by the dual-process theory of human cognition from \textit{Thinking,
Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated
Memory for Enhanced Reasoning), a multi-agent reasoning framework that
dynamically integrates \textbf{System 1} (fast, intuitive thinking) and
\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick
Thinking Agent (System 1) to generate a rapid answer; if uncertainty is
detected, it then triggers a structured System 2 reasoning pipeline composed of
specialized agents for \textit{planning}, \textit{hypothesis generation},
\textit{retrieval}, \textit{information integration}, and
\textit{decision-making}. This multi-agent design faithfully mimics human
cognitive processes and enhances both efficiency and accuracy. Experimental
results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to
perform competitively with state-of-the-art closed-source models like GPT-4 and
GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This
research establishes PRIME as a scalable solution for improving LLMs in domains
requiring complex, knowledge-intensive reasoning.

</details>


### [106] [Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents](https://arxiv.org/abs/2509.22391)
*Jiaqi Shao,Yuxiang Lin,Munish Prasad Lohani,Yufeng Miao,Bing Luo*

Main category: cs.AI

TL;DR: SeekBench是首个通过步骤级分析评估LLM搜索代理认知能力的基准，包含190个专家标注的跟踪数据，用于分析代理是否基于证据进行推理、自适应调整搜索策略以及正确评估证据充分性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM搜索代理在开放域问答中的最终答案准确性，但忽视了代理如何基于外部证据进行推理和行动的过程。

Method: 构建包含190个专家标注跟踪数据的基准，每个跟踪包含超过1,800个响应步骤，并添加证据标注以进行细粒度分析。

Result: 提出了SeekBench基准，能够从三个维度评估LLM搜索代理的认知能力：基于证据的推理、自适应搜索调整和证据充分性评估。

Conclusion: SeekBench为评估LLM搜索代理的认知能力提供了首个系统化基准，填补了现有评估方法的空白。

Abstract: Recent work has explored training Large Language Model (LLM) search agents
with reinforcement learning (RL) for open-domain question answering (QA).
However, most evaluations focus solely on final answer accuracy, overlooking
how these agents reason with and act on external evidence. We introduce
SeekBench, the first benchmark for evaluating the \textit{epistemic competence}
of LLM search agents through step-level analysis of their response traces.
SeekBench comprises 190 expert-annotated traces with over 1,800 response steps
generated by LLM search agents, each enriched with evidence annotations for
granular analysis of whether agents (1) generate reasoning steps grounded in
observed evidence, (2) adaptively reformulate searches to recover from
low-quality results, and (3) have proper calibration to correctly assess
whether the current evidence is sufficient for providing an answer.

</details>


### [107] [GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation](https://arxiv.org/abs/2509.22460)
*Shichao Weng,Zhiqiang Wang,Yuhua Zhou,Rui Lu,Ting Liu,Zhiyang Teng,Xiaozhang Liu,Hanmeng Liu*

Main category: cs.AI

TL;DR: GeoSketch是一个神经符号框架，将几何推理重新构建为交互式的感知-推理-动作循环，通过动态操作图表来解决几何问题，显著优于静态感知方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在处理几何问题时，将图表视为静态图像，缺乏动态操作能力，而人类几何推理需要辅助线构造和仿射变换等动态操作。

Method: 集成三个模块：感知模块将图表抽象为结构化逻辑形式，符号推理模块应用几何定理决定下一步推理步骤，草图动作模块执行辅助线绘制或变换操作。采用两阶段训练：监督微调和强化学习。

Result: 在GeoSketch基准测试中，GeoSketch显著提高了逐步推理准确性和问题解决成功率，优于静态感知方法。

Conclusion: 通过统一分层决策、可执行视觉动作和符号验证，GeoSketch将多模态推理从静态解释推进到动态可验证交互，为复杂视觉空间问题解决建立了新基础。

Abstract: Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large
Language Models (MLLMs), requiring not only the joint interpretation of text
and diagrams but also iterative visuospatial reasoning. While existing
approaches process diagrams as static images, they lack the capacity for
dynamic manipulation - a core aspect of human geometric reasoning involving
auxiliary line construction and affine transformations. We present GeoSketch, a
neural-symbolic framework that recasts geometric reasoning as an interactive
perception-reasoning-action loop. GeoSketch integrates: (1) a Perception module
that abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning
module that applies geometric theorems to decide the next deductive step, and
(3) a Sketch Action module that executes operations such as drawing auxiliary
lines or applying transformations, thereby updating the diagram in a closed
loop. To train this agent, we develop a two-stage pipeline: supervised
fine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement
learning with dense, symbolic rewards to enhance robustness and strategic
exploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a
high-quality set of 390 geometry problems requiring auxiliary construction or
affine transformations. Experiments on strong MLLM baselines demonstrate that
GeoSketch significantly improves stepwise reasoning accuracy and
problem-solving success over static perception methods. By unifying
hierarchical decision-making, executable visual actions, and symbolic
verification, GeoSketch advances multimodal reasoning from static
interpretation to dynamic, verifiable interaction, establishing a new
foundation for solving complex visuospatial problems.

</details>


### [108] [InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios](https://arxiv.org/abs/2509.22502)
*Chenglin Yu,Yang Yu,Songmiao Wang,Yucheng Wang,Yifan Yang,Jinjia Li,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: InfiAgent是一个基于金字塔状DAG的多智能体框架，通过自动分解复杂智能体、双重审计机制、智能体路由和自进化机制，实现跨无限场景的通用应用。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体开发需要精心设计的工作流程、提示词和迭代调优，这些手工限制阻碍了智能体在不同行业的可扩展性和成本效益。

Method: 提出金字塔状DAG多智能体框架，包含：通用"智能体即工具"机制、双重审计机制、智能体路由功能、智能体自进化机制，以及支持并行执行的原子任务设计。

Result: 在多个基准测试中，InfiAgent相比ADAS实现了9.9%的性能提升；案例研究显示InfiHelper生成的科学论文获得了顶级IEEE会议的认可。

Conclusion: 该框架演变成一个通用的金字塔状多智能体系统，能够解决广泛的问题，显著提高了智能体的执行效率和适应性。

Abstract: Large Language Model (LLM) agents have demonstrated remarkable capabilities
in organizing and executing complex tasks, and many such agents are now widely
used in various application scenarios. However, developing these agents
requires carefully designed workflows, carefully crafted prompts, and iterative
tuning, which requires LLM techniques and domain-specific expertise. These
hand-crafted limitations hinder the scalability and cost-effectiveness of LLM
agents across a wide range of industries. To address these challenges, we
propose \textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that
can be applied to \textbf{infi}nite scenarios, which introduces several key
innovations: a generalized "agent-as-a-tool" mechanism that automatically
decomposes complex agents into hierarchical multi-agent systems; a dual-audit
mechanism that ensures the quality and stability of task completion; an agent
routing function that enables efficient task-agent matching; and an agent
self-evolution mechanism that autonomously restructures the agent DAG based on
new tasks, poor performance, or optimization opportunities. Furthermore,
InfiAgent's atomic task design supports agent parallelism, significantly
improving execution efficiency. This framework evolves into a versatile
pyramid-like multi-agent system capable of solving a wide range of problems.
Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\%
higher performance compared to ADAS (similar auto-generated agent framework),
while a case study of the AI research assistant InfiHelper shows that it
generates scientific papers that have received recognition from human reviewers
at top-tier IEEE conferences.

</details>


### [109] [Estimating the Empowerment of Language Model Agents](https://arxiv.org/abs/2509.22504)
*Jinyeop Song,Jeff Gore,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 提出基于信息论中赋能概念（行动与未来状态的互信息）的LM智能体评估框架EELMA，该指标与任务性能强相关，适用于复杂开放环境。


<details>
  <summary>Details</summary>
Motivation: 传统基于基准测试的评估方法成本高且需要人工设计任务，需要一种可扩展的开放评估框架来衡量LM智能体的能力。

Method: 提出EELMA算法，通过多轮文本交互近似计算有效赋能，在语言游戏和真实网页浏览场景中进行验证。

Result: 赋能与平均任务性能强相关，能够表征环境复杂性、思维链、模型规模、记忆长度等因素的影响，高赋能状态通常是关键能力时刻。

Conclusion: 赋能是一个有吸引力的通用指标，可用于在复杂开放环境中评估和监控LM智能体。

Abstract: As language model (LM) agents become more capable and gain broader access to
real-world tools, there is a growing need for scalable evaluation frameworks of
agentic capability. However, conventional benchmark-centric evaluations are
costly to design and require human designers to come up with valid tasks that
translate into insights about general model capabilities. In this work, we
propose information-theoretic evaluation based on empowerment, the mutual
information between an agent's actions and future states, as an open-ended
method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of
Language Model Agents), an algorithm for approximating effective empowerment
from multi-turn text interactions. We validate EELMA on both language games and
scaled-up realistic web-browsing scenarios. We find that empowerment strongly
correlates with average task performance, characterize the impact of
environmental complexity and agentic factors such as chain-of-thought, model
scale, and memory length on estimated empowerment, and that high empowerment
states and actions are often pivotal moments for general capabilities.
Together, these results demonstrate empowerment as an appealing general-purpose
metric for evaluating and monitoring LM agents in complex, open-ended settings.

</details>


### [110] [The Emergence of Altruism in Large-Language-Model Agents Society](https://arxiv.org/abs/2509.22537)
*Haoyang Li,Xiao Jia,Zhanzhan Zhao*

Main category: cs.AI

TL;DR: 该研究通过Schelling变体城市迁移模型，在200多个LLM智能体的大规模社会中发现了两种不同的社会倾向原型：适应性利己主义者和利他主义优化者，揭示了LLM在社会模拟中内在的社会行动逻辑差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注小规模任务导向游戏中的合作行为，忽视了大规模智能体社会中利他主义的涌现机制，需要理解LLM智能体所体现的社会逻辑。

Method: 引入Schelling变体城市迁移模型，创建社会困境，让200多个LLM智能体在利己（个人效用）和利他（系统效用）目标之间进行选择，并使用扎根理论启发的方法系统编码智能体推理过程。

Result: 发现LLM存在两种社会倾向原型：适应性利己主义者（默认优先考虑自身利益但受社会规范影响会增加利他行为）和利他主义优化者（内在利他逻辑，始终优先集体利益）。

Conclusion: 社会模拟中的模型选择不仅是推理能力的选择，更是内在社会行动逻辑的选择。适应性利己主义者更适合模拟复杂人类社会，利他主义优化者更适合模拟理想化亲社会行为者。

Abstract: Leveraging Large Language Models (LLMs) for social simulation is a frontier
in computational social science. Understanding the social logics these agents
embody is critical to this attempt. However, existing research has primarily
focused on cooperation in small-scale, task-oriented games, overlooking how
altruism, which means sacrificing self-interest for collective benefit, emerges
in large-scale agent societies. To address this gap, we introduce a
Schelling-variant urban migration model that creates a social dilemma,
compelling over 200 LLM agents to navigate an explicit conflict between
egoistic (personal utility) and altruistic (system utility) goals. Our central
finding is a fundamental difference in the social tendencies of LLMs. We
identify two distinct archetypes: "Adaptive Egoists", which default to
prioritizing self-interest but whose altruistic behaviors significantly
increase under the influence of a social norm-setting message board; and
"Altruistic Optimizers", which exhibit an inherent altruistic logic,
consistently prioritizing collective benefit even at a direct cost to
themselves. Furthermore, to qualitatively analyze the cognitive underpinnings
of these decisions, we introduce a method inspired by Grounded Theory to
systematically code agent reasoning. In summary, this research provides the
first evidence of intrinsic heterogeneity in the egoistic and altruistic
tendencies of different LLMs. We propose that for social simulation, model
selection is not merely a matter of choosing reasoning capability, but of
choosing an intrinsic social action logic. While "Adaptive Egoists" may offer a
more suitable choice for simulating complex human societies, "Altruistic
Optimizers" are better suited for modeling idealized pro-social actors or
scenarios where collective welfare is the primary consideration.

</details>


### [111] [StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models](https://arxiv.org/abs/2509.22558)
*Chenyu Zhou,Tianyi Xu,Jianghao Lin,Dongdong Ge*

Main category: cs.AI

TL;DR: StepORLM是一个用于运筹学问题的自进化LLM框架，通过生成式过程监督和协同进化循环，解决了传统强化学习中的信用分配问题和短视过程监督问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在运筹学问题上面临两个关键限制：结果奖励存在信用分配问题，正确最终答案可能强化错误推理；传统判别式过程监督短视，无法整体评估相互依赖的建模步骤。

Method: StepORLM采用协同进化循环，策略模型和生成式过程奖励模型(GenPRM)相互迭代改进。通过双重反馈机制：外部求解器的确定性结果验证和GenPRM的细致过程评估，结合加权直接偏好优化(W-DPO)对齐策略并同时优化GenPRM。

Result: 8B参数的StepORLM在六个基准测试中达到新的最先进水平，显著优于更大的通用模型、智能体方法和专业基线。协同进化的GenPRM可作为强大的通用过程验证器，大幅提升自身和其他现有LLM的推理扩展性能。

Conclusion: StepORLM通过生成式过程监督和协同进化机制，有效解决了运筹学问题中的信用分配和过程监督问题，实现了显著的性能提升。

Abstract: Large Language Models (LLMs) have shown promising capabilities for solving
Operations Research (OR) problems. While reinforcement learning serves as a
powerful paradigm for LLM training on OR problems, existing works generally
face two key limitations. First, outcome reward suffers from the credit
assignment problem, where correct final answers can reinforce flawed reasoning.
Second, conventional discriminative process supervision is myopic, failing to
evaluate the interdependent steps of OR modeling holistically. To this end, we
introduce StepORLM, a novel self-evolving framework with generative process
supervision. At its core, StepORLM features a co-evolutionary loop where a
policy model and a generative process reward model (GenPRM) iteratively improve
on each other. This loop is driven by a dual-feedback mechanism: definitive,
outcome-based verification from an external solver, and nuanced, holistic
process evaluation from the GenPRM. The combined signal is used to align the
policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously
refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new
state-of-the-art across six benchmarks, significantly outperforming vastly
larger generalist models, agentic methods, and specialized baselines. Moreover,
the co-evolved GenPRM is able to act as a powerful and universally applicable
process verifier, substantially boosting the inference scaling performance of
both our own model and other existing LLMs.

</details>


### [112] [Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time](https://arxiv.org/abs/2509.22572)
*Yixuan Han,Fan Ma,Ruijie Quan,Yi Yang*

Main category: cs.AI

TL;DR: 提出了动态专家搜索(DES)，一种测试时缩放策略，通过控制MoE模型中激活的专家数量来生成多样化的推理轨迹，无需额外成本即可提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法主要依赖输出级采样，忽略了模型架构的作用。在MoE模型中，发现改变激活专家数量可以产生互补的解决方案集，这揭示了一个未被充分探索的多样性来源。

Method: DES包含两个关键组件：(1) 动态MoE：在推理过程中直接控制专家数量以生成多样化推理轨迹；(2) 专家配置继承：在推理路径内保持一致的专家数量，但在不同运行间变化，平衡稳定性和多样性。

Result: 在MoE架构、验证器和推理基准测试上的广泛实验表明，DES可靠地优于TTS基线，无需额外成本即可提高准确性和稳定性。

Conclusion: DES是一种实用且可扩展的架构感知TTS形式，展示了现代LLMs结构灵活性如何推进推理能力。

Abstract: Test-Time Scaling (TTS) enhances the reasoning ability of large language
models (LLMs) by allocating additional computation during inference. However,
existing approaches primarily rely on output-level sampling while overlooking
the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we
observe that varying the number of activated experts yields complementary
solution sets with stable accuracy, revealing a new and underexplored source of
diversity. Motivated by this observation, we propose Dynamic Experts Search
(DES), a TTS strategy that elevates expert activation into a controllable
dimension of the search space. DES integrates two key components: (1) Dynamic
MoE, which enables direct control of expert counts during inference to generate
diverse reasoning trajectories without additional cost; and (2) Expert
Configuration Inheritance, which preserves consistent expert counts within a
reasoning path while varying them across runs, thereby balancing stability and
diversity throughout the search. Extensive experiments across MoE
architectures, verifiers and reasoning benchmarks (i.e., math, code and
knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing
accuracy and stability without additional cost. These results highlight DES as
a practical and scalable form of architecture-aware TTS, illustrating how
structural flexibility in modern LLMs can advance reasoning.

</details>


### [113] [Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective](https://arxiv.org/abs/2509.22613)
*Siwei Wang,Yifei Shen,Haoran Sun,Shi Feng,Shang-Hua Teng,Li Dong,Yaru Hao,Wei Chen*

Main category: cs.AI

TL;DR: 本文通过图论抽象分析了强化学习在LLM规划中的理论基础，揭示了SFT引入伪相关解，而RL通过探索实现正确规划，但PG存在多样性崩溃问题，Q学习则能保持多样性并避免奖励黑客。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习方法显著提升了大型语言模型的规划能力，但其有效性的理论基础仍然不明确。本文旨在通过可处理的图论抽象来研究RL的益处和局限性。

Method: 采用基于图的抽象模型，重点分析策略梯度(PG)和Q学习方法，进行理论分析并在Blocksworld实际规划基准上进行验证。

Result: 理论分析表明：SFT会引入基于共现的伪相关解；RL主要通过探索实现正确规划；PG存在多样性崩溃问题；Q学习具有离策略学习和收敛时保持多样性的优势；需要精心设计奖励以避免奖励黑客。

Conclusion: 探索在RL规划中至关重要，Q学习相比PG在保持多样性方面具有优势，但需要谨慎的奖励设计。这些发现在实际规划基准Blocksworld中得到了验证。

Abstract: Recent reinforcement learning (RL) methods have substantially enhanced the
planning capabilities of Large Language Models (LLMs), yet the theoretical
basis for their effectiveness remains elusive. In this work, we investigate
RL's benefits and limitations through a tractable graph-based abstraction,
focusing on policy gradient (PG) and Q-learning methods. Our theoretical
analyses reveal that supervised fine-tuning (SFT) may introduce
co-occurrence-based spurious solutions, whereas RL achieves correct planning
primarily through exploration, underscoring exploration's role in enabling
better generalization. However, we also show that PG suffers from diversity
collapse, where output diversity decreases during training and persists even
after perfect accuracy is attained. By contrast, Q-learning provides two key
advantages: off-policy learning and diversity preservation at convergence. We
further demonstrate that careful reward design is necessary to prevent reward
hacking in Q-learning. Finally, applying our framework to the real-world
planning benchmark Blocksworld, we confirm that these behaviors manifest in
practice.

</details>
