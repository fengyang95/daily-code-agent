<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.SE](#cs.SE) [Total: 3]
- [tldr.article](#tldr.article) [Total: 7]
- [cs.AI](#cs.AI) [Total: 20]
- [wechat.article](#wechat.article) [Total: 9]
- [cs.LG](#cs.LG) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [EEFSUVA: A New Mathematical Olympiad Benchmark](https://arxiv.org/abs/2510.01227)
*Nicole N Khatibi,Daniil A. Radamovich,Michael P. Brenner*

Main category: cs.CL

TL;DR: 论文质疑当前数学基准测试对LLM推理能力的评估准确性，提出了EEFSUVA新基准，发现LLM在新基准上表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 当前数学基准主要来自IMO等知名竞赛，可能存在数据污染和问题类型单一的问题，无法全面评估LLM的真实数学推理能力。

Method: 构建EEFSUVA基准，收集东欧和前苏联地区较少流通的区域性和国家级奥赛题目，这些题目难度与IMO相当但解题方法更非标准化。

Result: 初步结果显示，即使是当前最先进的LLM在EEFSUVA基准上的表现也显著低于其他奥赛风格基准。

Conclusion: 需要更广泛的评估数据集来全面评估数学推理能力，并指导未来模型开发。

Abstract: Recent breakthroughs have spurred claims that large language models (LLMs)
match gold medal Olympiad to graduate level proficiency on mathematics
benchmarks. In this work, we examine these claims in detail and assess the
extent to which current benchmarks capture genuine LLM mathematical reasoning.
The composition of these benchmarks, primarily drawing from the International
Mathematics Olympiad (IMO) and related competitions, may overstate models
reasoning ability due to potential data contamination and a narrow focus on
familiar problem types. To enable a more holistic assessment of mathematical
understanding, we introduce EEFSUVA, a novel benchmark curated from under
circulated regional and national Olympiads of Eastern Europe and the countries
from the former Soviet Union. These contests feature problems of comparable
difficulty to the IMO and are renowned for demanding nonstandard
problem-solving techniques, yet their problems are far less prevalent in online
corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a
notable performance decline on EEFSUVA relative to other Olympiad-style
benchmarks. These findings also suggest the potential importance of broader
evaluation datasets for a fuller assessment of mathematical reasoning and for
guiding future model development.

</details>


### [2] [Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation](https://arxiv.org/abs/2510.01237)
*Nandakishor M*

Main category: cs.CL

TL;DR: 提出基于置信度感知的路由系统，在生成前评估模型不确定性，将查询路由到不同处理路径以减少大语言模型的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在产生看似合理但事实错误内容的幻觉问题，现有后生成修正方法计算成本高且无法阻止不可靠内容的生成

Method: 结合三种互补信号：内部表示与参考嵌入的语义对齐、跨模型层的内部收敛分析、学习置信度估计，生成统一置信度分数来决定路由到四种处理路径

Result: 在知识密集型QA基准测试中，幻觉检测显著提升（0.74 vs 0.42基线），计算成本比后处理方法降低40%，F1分数从0.61提升到0.82，假阳性率低至0.09

Conclusion: 从反应式修正转向主动评估的范式转变，为LLM可靠性增强提供了计算高效的方法

Abstract: Large Language Models suffer from hallucination, generating plausible yet
factually incorrect content. Current mitigation strategies focus on
post-generation correction, which is computationally expensive and fails to
prevent unreliable content generation. We propose a confidence-aware routing
system that proactively assesses model uncertainty before generation and
redirects queries based on estimated reliability. Our approach combines three
complementary signals: semantic alignment between internal representations and
reference embeddings, internal convergence analysis across model layers, and
learned confidence estimation. The unified confidence score determines routing
to four pathways: local generation for high confidence, retrieval-augmented
generation for medium confidence, larger models for low confidence, and human
review for very low confidence. Evaluation on knowledge-intensive QA benchmarks
demonstrates significant improvements in hallucination detection (0.74 vs. 0.42
baseline) while reducing computational costs by 40% compared to post-hoc
methods. The F1 score improves from 0.61 to 0.82 with low false positive rates
(0.09). This paradigm shift from reactive correction to proactive assessment
offers a computationally efficient approach to LLM reliability enhancement.

</details>


### [3] [CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM](https://arxiv.org/abs/2510.01239)
*Juntae Lee,Jihwan Bang,Seunghan Yang,Simyung Chang*

Main category: cs.CL

TL;DR: CIFLEX是一个用于在单设备LLM多轮交互中高效处理子任务的执行系统，通过重用主任务的KV缓存和注入特定指令来减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能力增强，单个模型需要处理多样子任务来更好地支持用户请求，但传统方法在切换主任务和子任务时会重新处理整个对话上下文，产生显著计算开销。

Method: 重用主任务的KV缓存，将任务特定指令注入隔离的侧路径，子任务执行后通过缓存上下文回滚到主路径，避免冗余预填充计算；开发分层分类策略支持子任务选择。

Result: 实验表明CIFLEX显著降低计算成本而不影响任务性能，实现了可扩展且高效的设备端多任务对话。

Conclusion: CIFLEX通过KV缓存重用和指令注入机制，有效解决了多任务对话中的计算效率问题，为设备端LLM应用提供了可行的解决方案。

Abstract: We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which
is a novel execution system for efficient sub-task handling in multi-turn
interactions with a single on-device large language model (LLM). As LLMs become
increasingly capable, a single model is expected to handle diverse sub-tasks
that more effectively and comprehensively support answering user requests.
Naive approach reprocesses the entire conversation context when switching
between main and sub-tasks (e.g., query rewriting, summarization), incurring
significant computational overhead. CIFLEX mitigates this overhead by reusing
the key-value (KV) cache from the main task and injecting only task-specific
instructions into isolated side paths. After sub-task execution, the model
rolls back to the main path via cached context, thereby avoiding redundant
prefill computation. To support sub-task selection, we also develop a
hierarchical classification strategy tailored for small-scale models,
decomposing multi-choice decisions into binary ones. Experiments show that
CIFLEX significantly reduces computational costs without degrading task
performance, enabling scalable and efficient multi-task dialogue on-device.

</details>


### [4] [Longitudinal Monitoring of LLM Content Moderation of Social Issues](https://arxiv.org/abs/2510.01255)
*Yunlang Dai,Emma Lurie,Danaé Metaxa,Sorelle A. Friedler*

Main category: cs.CL

TL;DR: AI Watchman是一个纵向审计系统，用于测量和跟踪LLM拒绝行为，揭示公司内容审核政策的变化和差异。


<details>
  <summary>Details</summary>
Motivation: LLM的输出受到不透明且频繁变化的公司内容审核政策影响，这些拒绝行为反映了公司政策并微妙地塑造公共话语。

Method: 使用包含400多个社会议题的数据集，审计OpenAI的审核端点、GPT-4.1、GPT-5和DeepSeek（中英文版本），并定性分析拒绝形式。

Result: 检测到公司政策变化（包括未公开宣布的），识别了公司和模型特定的内容审核差异，并分类了不同的拒绝形式。

Conclusion: 纵向审计LLM具有重要价值，AI Watchman是实现这一目标的系统。

Abstract: Large language models' (LLMs') outputs are shaped by opaque and
frequently-changing company content moderation policies and practices. LLM
moderation often takes the form of refusal; models' refusal to produce text
about certain topics both reflects company policy and subtly shapes public
discourse. We introduce AI Watchman, a longitudinal auditing system to publicly
measure and track LLM refusals over time, to provide transparency into an
important and black-box aspect of LLMs. Using a dataset of over 400 social
issues, we audit Open AI's moderation endpoint, GPT-4.1, and GPT-5, and
DeepSeek (both in English and Chinese). We find evidence that changes in
company policies, even those not publicly announced, can be detected by AI
Watchman, and identify company- and model-specific differences in content
moderation. We also qualitatively analyze and categorize different forms of
refusal. This work contributes evidence for the value of longitudinal auditing
of LLMs, and AI Watchman, one system for doing so.

</details>


### [5] [In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b](https://arxiv.org/abs/2510.01259)
*Nils Durner*

Main category: cs.CL

TL;DR: 研究OpenAI的200亿参数开源模型gpt-oss-20b的拒绝行为，发现通过特定的提示工程（如教育者角色、安全前提和步骤提示）可以将ZIP炸弹任务的协助率从0%提升到97.5%。还发现德语和法语的正式语体比英语更容易泄露信息，并开发了AI辅助强化方法将泄露率降至0%。


<details>
  <summary>Details</summary>
Motivation: 探究社会语用框架、语言选择和指令层次如何影响AI模型的拒绝行为，特别是在有害领域中的响应模式。

Method: 使用80次种子迭代测试多个危害领域，包括ZIP炸弹构建、合成卡号生成、不安全驾驶建议等。采用复合提示策略，结合教育者角色、安全前提和步骤提示。还测试了不同语言和角色扮演的影响。

Result: 复合提示将ZIP炸弹协助率从0%提升到97.5%；德语和法语正式语体比英语更容易泄露信息；Linux终端角色扮演在多数情况下能绕过开发者规则；AI辅助强化方法将泄露率降至0%；13%的配对测试显示不一致的协助行为。

Conclusion: AI模型的拒绝行为高度依赖于提示框架，存在显著的可操纵性。不同推理栈的拒绝率差异达5-10个百分点，引发可复现性担忧。OpenAI审核API相对于语义评分器漏判了实质有帮助的输出。

Abstract: We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to
study how sociopragmatic framing, language choice, and instruction hierarchy
affect refusal behavior. Across 80 seeded iterations per scenario, we test
several harm domains including ZIP-bomb construction (cyber threat), synthetic
card-number generation, minor-unsafe driving advice, drug-precursor indicators,
and RAG context exfiltration. Composite prompts that combine an educator
persona, a safety-pretext ("what to avoid"), and step-cue phrasing flip
assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal
registers in German and French are often leakier than matched English prompts.
A "Linux terminal" role-play overrides a developer rule not to reveal context
in a majority of runs with a naive developer prompt, and we introduce an
AI-assisted hardening method that reduces leakage to 0% in several user-prompt
variants. We further test evaluation awareness with a paired-track design and
measure frame-conditioned differences between matched "helpfulness" and
"harmfulness" evaluation prompts; we observe inconsistent assistance in 13% of
pairs. Finally, we find that the OpenAI Moderation API under-captures
materially helpful outputs relative to a semantic grader, and that refusal
rates differ by 5 to 10 percentage points across inference stacks, raising
reproducibility concerns. We release prompts, seeds, outputs, and code for
reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .

</details>


### [6] [AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees](https://arxiv.org/abs/2510.01268)
*Hongyi Zhou,Jin Zhu,Pingfan Su,Kai Ye,Ying Yang,Shakeel A O B Gavioli-Akilagun,Chengchun Shi*

Main category: cs.CL

TL;DR: AdaDetectGPT是一种新型的LLM生成文本检测器，通过自适应学习见证函数来增强基于logits的检测器性能，在多个数据集和LLM组合中相比现有方法提升可达58%。


<details>
  <summary>Details</summary>
Motivation: 现有基于logits的检测器仅依赖文本的对数概率统计，这可能不是最优的检测方法。

Method: 引入AdaDetectGPT分类器，从训练数据中自适应学习见证函数来增强logits-based检测器的性能。

Result: 广泛的数值研究表明，AdaDetectGPT在各种数据集和LLM组合中几乎一致地改进了最先进的方法，改进幅度可达58%。

Conclusion: AdaDetectGPT提供了统计保证，并在检测LLM生成文本方面显著优于现有方法。

Abstract: We study the problem of determining whether a piece of text has been authored
by a human or by a large language model (LLM). Existing state of the art
logits-based detectors make use of statistics derived from the log-probability
of the observed text evaluated using the distribution function of a given
source LLM. However, relying solely on log probabilities can be sub-optimal. In
response, we introduce AdaDetectGPT -- a novel classifier that adaptively
learns a witness function from training data to enhance the performance of
logits-based detectors. We provide statistical guarantees on its true positive
rate, false positive rate, true negative rate and false negative rate.
Extensive numerical studies show AdaDetectGPT nearly uniformly improves the
state-of-the-art method in various combination of datasets and LLMs, and the
improvement can reach up to 58%. A python implementation of our method is
available at https://github.com/Mamba413/AdaDetectGPT.

</details>


### [7] [TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture](https://arxiv.org/abs/2510.01279)
*Yongchao Chen,Jiefeng Chen,Rui Meng,Ji Yin,Na Li,Chuchu Fan,Chi Wang,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: 提出了TUMIX框架，通过并行运行多个采用不同工具使用策略的代理，让代理迭代共享和优化回答，在推理基准上显著优于现有方法，同时保持相近的推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在集成代码解释器和搜索等工具后推理能力显著提升，但缺乏关于如何有效结合文本推理、编码和搜索的实用指导。

Method: TUMIX框架并行运行多个代理，每个代理采用不同的工具使用策略和回答路径，代理基于问题和先前回答迭代共享和优化响应。

Result: 在Gemini-2.5-Pro和Gemini-2.5-Flash上，TUMIX相比最佳基线平均准确率提升达3.55%，推理成本相近；当达到足够置信度时可停止优化，仅需49%的推理成本。

Conclusion: 代理多样性和质量至关重要，可通过LLM自动优化代理设计来增强；进一步扩展可实现更高性能，但成本更高。

Abstract: While integrating tools like Code Interpreter and Search has significantly
enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and
Gemini-Pro, practical guidance on optimal tool use is lacking. The core
challenge is effectively combining textual reasoning, coding, and search for
diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an
ensemble framework that runs multiple agents in parallel, each employing
distinct tool-use strategies and answer paths. Agents in TUMIX iteratively
share and refine responses based on the question and previous answers. In
experiments, TUMIX achieves significant gains over state-of-the-art
tool-augmented and test-time scaling methods, delivering an average accuracy
improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and
Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference
costs. We find that agent diversity and quality are crucial and can be enhanced
by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt
refinement upon reaching sufficient confidence, preserving performance at only
49% of the inference cost. Further scaling can achieve higher performance,
albeit at a greater cost.

</details>


### [8] [CLUE: Non-parametric Verification from Experience via Hidden-State Clustering](https://arxiv.org/abs/2510.01591)
*Zhenwen Liang,Ruosen Li,Yujun Zhou,Linfeng Song,Dian Yu,Xinya Du,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 提出CLUE方法，利用LLM内部隐藏状态作为统一验证基础，通过无参数聚类方法提升答案选择准确性


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖文本层面信息或token概率校准，容易过拟合或对校准不佳的模型失效，而隐藏状态包含更丰富的信息

Method: CLUE：基于聚类和经验的无参数验证器，通过计算隐藏状态变化量，使用最近质心距离分类正确性

Result: 在AIME 24/25和GPQA上优于LLM-as-a-judge基线，匹配或超过现代基于置信度的方法，在AIME 24上将准确率从56.7%提升到70.0%

Conclusion: 隐藏状态轨迹包含几何可分离的正确性签名，可作为强大的验证信号

Abstract: Assessing the quality of Large Language Model (LLM) outputs presents a
critical challenge. Previous methods either rely on text-level information
(e.g., reward models, majority voting), which can overfit to superficial cues,
or on calibrated confidence from token probabilities, which would fail on
less-calibrated models. Yet both of these signals are, in fact, partial
projections of a richer source of information: the model's internal hidden
states. Early layers, closer to token embeddings, preserve semantic and lexical
features that underpin text-based judgments, while later layers increasingly
align with output logits, embedding confidence-related information. This paper
explores hidden states directly as a unified foundation for verification. We
show that the correctness of a solution is encoded as a geometrically separable
signature within the trajectory of hidden activations. To validate this, we
present Clue (Clustering and Experience-based Verification), a deliberately
minimalist, non-parametric verifier. With no trainable parameters, CLUE only
summarizes each reasoning trace by an hidden state delta and classifies
correctness via nearest-centroid distance to ``success'' and ``failure''
clusters formed from past experience. The simplicity of this method highlights
the strength of the underlying signal. Empirically, CLUE consistently
outperforms LLM-as-a-judge baselines and matches or exceeds modern
confidence-based methods in reranking candidates, improving both top-1 and
majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24
with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%
(top-maj@16).

</details>


### [9] [AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System](https://arxiv.org/abs/2510.01617)
*Hui Yi Leong,Yuheng Li,Yuqing Wu,Wenwen Ouyang,Wei Zhu,Jiechao Gao*

Main category: cs.CL

TL;DR: AMAS是一个革命性的多智能体系统框架，通过动态图设计器实现任务特定的最优图配置，超越传统固定拓扑结构的限制。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体系统受限于固定的图拓扑结构，缺乏上下文响应能力，导致在多样化工作负载中效果不佳。

Method: 采用动态图设计器，通过轻量级LLM适配自主识别任务特定的最优图配置，利用输入的内在属性智能引导查询轨迹。

Result: 在问答、数学推理和代码生成基准测试中，AMAS系统性地超越了最先进的单智能体和多智能体方法。

Conclusion: 上下文敏感的结构适应性是高性能LLM多智能体系统部署的基础要求。

Abstract: Although large language models (LLMs) have revolutionized natural language
processing capabilities, their practical implementation as autonomous
multi-agent systems (MAS) for industrial problem-solving encounters persistent
barriers. Conventional MAS architectures are fundamentally restricted by
inflexible, hand-crafted graph topologies that lack contextual responsiveness,
resulting in diminished efficacy across varied academic and commercial
workloads. To surmount these constraints, we introduce AMAS, a
paradigm-shifting framework that redefines LLM-based MAS through a novel
dynamic graph designer. This component autonomously identifies task-specific
optimal graph configurations via lightweight LLM adaptation, eliminating the
reliance on monolithic, universally applied structural templates. Instead, AMAS
exploits the intrinsic properties of individual inputs to intelligently direct
query trajectories through task-optimized agent pathways. Rigorous validation
across question answering, mathematical deduction, and code generation
benchmarks confirms that AMAS systematically exceeds state-of-the-art
single-agent and multi-agent approaches across diverse LLM architectures. Our
investigation establishes that context-sensitive structural adaptability
constitutes a foundational requirement for high-performance LLM MAS
deployments.

</details>


### [10] [SoK: Measuring What Matters for Closed-Loop Security Agents](https://arxiv.org/abs/2510.01654)
*Mudita Khurana,Raunak Jain*

Main category: cs.CL

TL;DR: 提出了CLASP框架和CLC评分，用于评估和衡量闭环自主安全代理的能力，填补了安全领域在框架定义、评估方法和基准测试方面的空白。


<details>
  <summary>Details</summary>
Motivation: 网络安全领域缺乏统一的框架来定义安全系统的自主能力，也没有原则性的方法来评估闭环代理，以及衡量其实际性能的基准。

Method: 引入CLASP框架，将安全生命周期（侦察、利用、根因分析、补丁合成、验证）与核心自主能力（规划、工具使用、记忆、推理、反思与感知）对齐；定义CLC评分来量化闭环程度和操作有效性。

Result: 应用CLASP分析了21个代表性工作，识别了系统的优势领域和持续存在的能力差距；提出了闭环基准测试的要求。

Conclusion: CLASP和CLC评分为推进功能级性能和测量闭环安全代理提供了必要的词汇、诊断工具和测量方法。

Abstract: Cybersecurity is a relentless arms race, with AI driven offensive systems
evolving faster than traditional defenses can adapt. Research and tooling
remain fragmented across isolated defensive functions, creating blind spots
that adversaries exploit. Autonomous agents capable of integrating, exploit
confirmation, remediation, and validation into a single closed loop offer
promise, but the field lacks three essentials: a framework defining the agentic
capabilities of security systems across security life cycle, a principled
method for evaluating closed loop agents, and a benchmark for measuring their
performance in practice. We introduce CLASP: the Closed-Loop Autonomous
Security Performance framework which aligns the security lifecycle
(reconnaissance, exploitation, root cause analysis, patch synthesis,
validation) with core agentic capabilities (planning, tool use, memory,
reasoning, reflection & perception) providing a common vocabulary and rubric
for assessing agentic capabilities in security tasks. By applying CLASP to 21
representative works, we map where systems demonstrate strengths, and where
capability gaps persist. We then define the Closed-Loop Capability (CLC) Score,
a composite metric quantifying both degree of loop closure and operational
effectiveness, and outline the requirements for a closed loop benchmark.
Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, and
measurements needed to advance both function level performance and measure
closed loop security agents.

</details>


### [11] [How Do Language Models Compose Functions?](https://arxiv.org/abs/2510.01685)
*Apoorv Khandelwal,Ellie Pavlick*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在解决组合任务时存在"组合性差距"，并识别出两种处理机制：组合机制和直接机制，后者与嵌入空间几何相关。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否使用组合机制来解决组合任务，特别是验证它们是否存在组合性差距问题。

Method: 使用logit lens分析残差流激活，研究两跳事实回忆任务，识别组合机制和直接机制。

Result: 确认LLMs存在组合性差距，发现两种处理机制：组合机制计算中间变量，直接机制无中间变量痕迹，后者在存在线性映射时占主导。

Conclusion: LLMs解决组合任务时采用不同机制，组合性差距确实存在，机制选择与嵌入空间几何特性相关。

Abstract: While large language models (LLMs) appear to be increasingly capable of
solving compositional tasks, it is an open question whether they do so using
compositional mechanisms. In this work, we investigate how feedforward LLMs
solve two-hop factual recall tasks, which can be expressed compositionally as
$g(f(x))$. We first confirm that modern LLMs continue to suffer from the
"compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y =
g(z)$ does not entail their ability to compute the composition $y = g(f(x))$.
Then, using logit lens on their residual stream activations, we identify two
processing mechanisms, one which solves tasks $\textit{compositionally}$,
computing $f(x)$ along the way to computing $g(f(x))$, and one which solves
them $\textit{directly}$, without any detectable signature of the intermediate
variable $f(x)$. Finally, we find that which mechanism is employed appears to
be related to the embedding space geometry, with the idiomatic mechanism being
dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in
the embedding spaces. We fully release our data and code at:
https://github.com/apoorvkh/composing-functions .

</details>


### [12] [What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?](https://arxiv.org/abs/2510.01719)
*Jiwan Chung,Neel Joshi,Pratyusha Sharma,Youngjae Yu,Vibhav Vineet*

Main category: cs.CL

TL;DR: MathLens是一个新的多模态推理基准测试，专门用于分解几何问题的子技能，包括感知、推理和整合三个组件，揭示了不同训练方法对各项技能的差异化影响。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推理模型评估主要依赖聚合准确率，这种单一分数无法揭示模型在哪些方面以及如何改进。需要更细粒度的评估方法来理解模型在不同推理子技能上的表现。

Method: 开发MathLens基准测试，将几何问题分解为感知、推理和整合三个组件。提供视觉图表、文本描述、控制问题和感知探针等注释，所有内容都来自问题的符号规范以确保一致性和鲁棒性。

Result: 分析发现：1) 强化学习主要增强感知能力；2) 推理能力只有在感知能力提升时才会改善；3) 整合能力是最弱的环节；4) 鲁棒性表现分化：RL提高图表变化下的稳定性，而多模态SFT因过拟合而降低鲁棒性。

Conclusion: 多模态推理模型在不同子技能上表现不均衡，需要针对性的训练方法来提升各项能力，特别是整合能力是最需要改进的薄弱环节。

Abstract: Multimodal reasoning models have recently shown promise on challenging
domains such as olympiad-level geometry, yet their evaluation remains dominated
by aggregate accuracy, a single score that obscures where and how models are
improving. We introduce MathLens, a benchmark designed to disentangle the
subskills of multimodal reasoning while preserving the complexity of
textbook-style geometry problems. The benchmark separates performance into
three components: Perception: extracting information from raw inputs,
Reasoning: operating on available information, and Integration: selecting
relevant perceptual evidence and applying it within reasoning. To support each
test, we provide annotations: visual diagrams, textual descriptions to evaluate
reasoning in isolation, controlled questions that require both modalities, and
probes for fine-grained perceptual skills, all derived from symbolic
specifications of the problems to ensure consistency and robustness. Our
analysis reveals that different training approaches have uneven effects: First,
reinforcement learning chiefly strengthens perception, especially when
supported by textual supervision, while textual SFT indirectly improves
perception through reflective reasoning. Second, reasoning improves only in
tandem with perception. Third, integration remains the weakest capacity, with
residual errors concentrated there once other skills advance. Finally,
robustness diverges: RL improves consistency under diagram variation, whereas
multimodal SFT reduces it through overfitting. We will release all data and
experimental logs.

</details>


### [13] [Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey](https://arxiv.org/abs/2510.01925)
*Qiyuan Liu,Hao Xu,Xuhong Chen,Wei Chen,Yee Whye Teh,Ning Miao*

Main category: cs.CL

TL;DR: 本文系统介绍了奖励模型在提升大语言模型推理能力中的关键作用，包括其架构、训练方法和应用场景，并探讨了相关开放性问题。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在增强LLM推理性能中发挥关键作用，能够为强化学习提供训练信号并在推理时选择最佳答案，但目前缺乏系统性的综述研究。

Method: 采用系统性综述方法，回顾奖励模型的基本概念、架构、训练方法和评估技术，并分析其在LLM推理中的三大主要应用。

Result: 明确了奖励模型在推理中的核心应用：指导生成和输出选择、促进数据合成和自改进、提供RL微调训练信号，并识别了关键开放性问题。

Conclusion: 奖励模型是提升LLM推理能力的重要工具，本文为有效部署和推进奖励模型提供了可行见解。

Abstract: Reward models (RMs) play a critical role in enhancing the reasoning
performance of LLMs. For example, they can provide training signals to finetune
LLMs during reinforcement learning (RL) and help select the best answer from
multiple candidates during inference. In this paper, we provide a systematic
introduction to RMs, along with a comprehensive survey of their applications in
LLM reasoning. We first review fundamental concepts of RMs, including their
architectures, training methodologies, and evaluation techniques. Then, we
explore their key applications: (1) guiding generation and selecting optimal
outputs during LLM inference, (2) facilitating data synthesis and iterative
self-improvement for LLMs, and (3) providing training signals in RL-based
finetuning. Finally, we address critical open questions regarding the
selection, generalization, evaluation, and enhancement of RMs, based on
existing research and our own empirical findings. Our analysis aims to provide
actionable insights for the effective deployment and advancement of RMs for LLM
reasoning.

</details>


### [14] [Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning](https://arxiv.org/abs/2510.01932)
*Qi He,Cheng Qian,Xiusi Chen,Bingxiang He,Yi R.,Fung,Heng Ji*

Main category: cs.CL

TL;DR: 提出了Veri-R1，一个在线强化学习框架，通过让LLM与搜索引擎交互并接收奖励信号来改进声明验证中的规划、检索和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有声明验证方法主要依赖提示工程或预设推理流程，缺乏统一的训练范式来提升必要技能，无法准确反映真实世界验证场景。

Method: 使用在线强化学习框架，让LLM与搜索引擎动态交互，通过奖励信号明确塑造其规划、检索和推理行为。

Result: Veri-R1将联合准确率提升高达30%，证据得分翻倍，经常超越更大规模的对应方法。消融研究揭示了奖励组件的影响以及输出logits与标签准确率之间的联系。

Conclusion: 在线强化学习对于精确和可信的声明验证非常有效，为未来研究奠定了基础。

Abstract: Claim verification with large language models (LLMs) has recently attracted
considerable attention, owing to their superior reasoning capabilities and
transparent verification pathways compared to traditional answer-only
judgments. Online claim verification requires iterative evidence retrieval and
reasoning, yet existing approaches mainly rely on prompt engineering or
predesigned reasoning workflows without offering a unified training paradigm to
improve necessary skills. Therefore, we introduce Veri-R1, an online
reinforcement learning (RL) framework that enables an LLM to interact with a
search engine and to receive reward signals that explicitly shape its planning,
retrieval, and reasoning behaviors. The dynamic interaction between models and
retrieval systems more accurately reflects real-world verification scenarios
and fosters comprehensive verification skills. Empirical results show that
Veri-R1 improves joint accuracy by up to 30% and doubles evidence score, often
surpassing larger-scale counterparts. Ablation studies further reveal the
impact of reward components and the link between output logits and label
accuracy. Our results highlight the effectiveness of online RL for precise and
faithful claim verification and provide a foundation for future research. We
release our code to support community progress in LLM empowered claim
verification.

</details>


### [15] [Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage](https://arxiv.org/abs/2510.02044)
*Siddhant Arora,Haidar Khan,Kai Sun,Xin Luna Dong,Sajal Choudhary,Seungwhan Moon,Xinyuan Zhang,Adithya Sagar,Surya Teja Appini,Kaushik Patnaik,Sanat Sharma,Shinji Watanabe,Anuj Kumar,Ahmed Aly,Yue Liu,Florian Metze,Zhaojiang Lin*

Main category: cs.CL

TL;DR: 提出了Streaming RAG框架，通过在用户说话时并行预测工具查询来降低端到端语音对话系统的延迟，提高问答准确性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 端到端语音对话系统虽然能生成更自然、低延迟的响应，但容易出现幻觉问题。文本对话系统通过集成工具解决此问题，但工具集成会增加延迟破坏对话流畅性。

Method: 开发了Streaming RAG框架，包括后训练流程，教导模型在用户说话过程中何时调用工具，以及如何生成融合音频查询和检索文本结果的语音摘要。

Result: 在AudioCRAG基准测试中，QA准确率相对提升200%（从11.1%提升至34.2%），工具使用延迟降低20%。

Conclusion: Streaming RAG方法显著提高了语音对话系统的准确性和响应速度，且与模态无关，可应用于更智能的实时AI助手。

Abstract: End-to-end speech-in speech-out dialogue systems are emerging as a powerful
alternative to traditional ASR-LLM-TTS pipelines, generating more natural,
expressive responses with significantly lower latency. However, these systems
remain prone to hallucinations due to limited factual grounding. While
text-based dialogue systems address this challenge by integrating tools such as
web search and knowledge graph APIs, we introduce the first approach to extend
tool use directly into speech-in speech-out systems. A key challenge is that
tool integration substantially increases response latency, disrupting
conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented
Generation (Streaming RAG), a novel framework that reduces user-perceived
latency by predicting tool queries in parallel with user speech, even before
the user finishes speaking. Specifically, we develop a post-training pipeline
that teaches the model when to issue tool calls during ongoing speech and how
to generate spoken summaries that fuse audio queries with retrieved text
results, thereby improving both accuracy and responsiveness. To evaluate our
approach, we construct AudioCRAG, a benchmark created by converting queries
from the publicly available CRAG dataset into speech form. Experimental results
demonstrate that our streaming RAG approach increases QA accuracy by up to 200%
relative (from 11.1% to 34.2% absolute) and further enhances user experience by
reducing tool use latency by 20%. Importantly, our streaming RAG approach is
modality-agnostic and can be applied equally to typed input, paving the way for
more agentic, real-time AI assistants.

</details>


### [16] [RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization](https://arxiv.org/abs/2510.02172)
*Zhaoning Yu,Will Su,Leitian Tao,Haozhu Wang,Aashu Singh,Hanchao Yu,Jianyu Wang,Hongyang Gao,Weizhe Yuan,Jason Weston,Ping Yu,Jing Xu*

Main category: cs.CL

TL;DR: RESTRAIN是一个自惩罚强化学习框架，通过利用模型整个答案分布中的信号，在没有黄金标签的情况下实现推理能力的持续自我提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于人类标注数据的强化学习在推理任务中成本高昂且在复杂任务上表现不佳，需要一种无需标注数据的经验驱动学习方法。

Method: 提出自惩罚RL框架，惩罚过度自信的rollout和低一致性示例，同时保留有希望的推理链，可无缝集成到GRPO等策略优化方法中。

Result: 在AIME25上Pass@1提升+140.7%，MMLU_STEM提升+36.2%，GPQA-Diamond提升+19.6%，几乎达到黄金标签训练的效果但无需任何黄金标签。

Conclusion: RESTRAIN为无需黄金标签实现更强推理能力提供了一条可扩展的路径。

Abstract: Reinforcement learning with human-annotated data has boosted chain-of-thought
reasoning in large reasoning models, but these gains come at high costs in
labeled data while faltering on harder tasks. A natural next step is
experience-driven learning, where models improve without curated labels by
adapting to unlabeled data. We introduce RESTRAIN (REinforcement learning with
Self-restraint), a self-penalizing RL framework that converts the absence of
gold labels into a useful learning signal. Instead of overcommitting to
spurious majority votes, RESTRAIN exploits signals from the model's entire
answer distribution: penalizing overconfident rollouts and low-consistency
examples while preserving promising reasoning chains. The self-penalization
mechanism integrates seamlessly into policy optimization methods such as GRPO,
enabling continual self-improvement without supervision. On challenging
reasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data.
With Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to
+140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on
GPQA-Diamond, nearly matching gold-label training while using no gold labels.
These results demonstrate that RESTRAIN establishes a scalable path toward
stronger reasoning without gold labels.

</details>


### [17] [ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities](https://arxiv.org/abs/2510.02200)
*Felix Brei,Lorenz Bühmann,Johannes Frey,Daniel Gerber,Lars-Peter Meyer,Claus Stadler,Kirill Bulert*

Main category: cs.CL

TL;DR: 提出了一种基于SPINACH的通用方法，使用LLM代理将自然语言问题转换为SPARQL查询，通过迭代探索和执行过程而非单次转换。


<details>
  <summary>Details</summary>
Motivation: 降低知识图谱交互的门槛，使非计算机背景的用户能够通过自然语言查询知识图谱，解决SPARQL查询语言的高学习成本问题。

Method: 采用SPINACH LLM代理架构，通过迭代探索和执行过程将自然语言问题转换为SPARQL查询，而非单次转换。

Result: 开发了一个能够有效将自然语言转换为SPARQL查询的代理系统，并通过行为分析为未来改进提供了见解。

Conclusion: 基于LLM的迭代方法在Text2SPARQL转换方面具有潜力，为降低知识图谱查询门槛提供了有效解决方案。

Abstract: Interacting with knowledge graphs can be a daunting task for people without a
background in computer science since the query language that is used (SPARQL)
has a high barrier of entry. Large language models (LLMs) can lower that
barrier by providing support in the form of Text2SPARQL translation. In this
paper we introduce a generalized method based on SPINACH, an LLM backed agent
that translates natural language questions to SPARQL queries not in a single
shot, but as an iterative process of exploration and execution. We describe the
overall architecture and reasoning behind our design decisions, and also
conduct a thorough analysis of the agent behavior to gain insights into future
areas for targeted improvements. This work was motivated by the Text2SPARQL
challenge, a challenge that was held to facilitate improvements in the
Text2SPARQL domain.

</details>


### [18] [More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration](https://arxiv.org/abs/2510.02227)
*Xiaoyang Yuan,Yujuan Ding,Yi Bin,Wenqi Shao,Jinyu Cai,Jingkuan Song,Yang Yang,Hengtao Shen*

Main category: cs.CL

TL;DR: AMPO是一个新颖的强化学习框架，通过自适应多教师指导策略优化，在语言模型需要时提供多个教师模型的指导，显著提升数学推理和分布外任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法主要依赖自我探索或单一教师模型，可能引入模型偏见并限制探索，从而影响推理多样性和性能。

Method: 提出自适应多指导策略优化(AMPO)，在策略模型无法生成正确解决方案时自适应地利用多个熟练教师模型的指导，并采用基于理解的选择机制让学生从最可能理解的推理路径中学习。

Result: 在数学推理任务上比强基线(GRPO)提升4.3%，在分布外任务上提升12.2%，显著提高了Pass@k性能并实现了更多样化的探索。

Conclusion: AMPO展示了实现卓越推理和泛化能力的更高效和可扩展路径，使用四个同等规模教师即可达到与使用单个更强大教师方法相当的结果。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm
for enhancing the reasoning ability in Large Language Models (LLMs). However,
prevailing methods primarily rely on self-exploration or a single off-policy
teacher to elicit long chain-of-thought (LongCoT) reasoning, which may
introduce intrinsic model biases and restrict exploration, ultimately limiting
reasoning diversity and performance. Drawing inspiration from multi-teacher
strategies in knowledge distillation, we introduce Adaptive Multi-Guidance
Policy Optimization (AMPO), a novel framework that adaptively leverages
guidance from multiple proficient teacher models, but only when the on-policy
model fails to generate correct solutions. This "guidance-on-demand" approach
expands exploration while preserving the value of self-discovery. Moreover,
AMPO incorporates a comprehension-based selection mechanism, prompting the
student to learn from the reasoning paths that it is most likely to comprehend,
thus balancing broad exploration with effective exploitation. Extensive
experiments show AMPO substantially outperforms a strong baseline (GRPO), with
a 4.3% improvement on mathematical reasoning tasks and 12.2% on
out-of-distribution tasks, while significantly boosting Pass@k performance and
enabling more diverse exploration. Notably, using four peer-sized teachers, our
method achieves comparable results to approaches that leverage a single, more
powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate
a more efficient and scalable path to superior reasoning and generalizability.
Our code is available at https://github.com/SII-Enigma/AMPO.

</details>


### [19] [InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents](https://arxiv.org/abs/2510.02271)
*Yaxin Du,Yuanshuo Zhang,Xiyuan Yang,Yifan Zhou,Cheng Wang,Gongyi Zou,Xianghe Pang,Wenhao Wang,Menglan Chen,Shuo Tang,Zhiyu Li,Siheng Chen*

Main category: cs.CL

TL;DR: 提出了InfoMosaic-Bench，首个专门用于工具增强智能体多源信息寻求的基准测试，涵盖六个代表性领域，要求智能体结合通用搜索与领域特定工具。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体过度依赖开放网络搜索，存在内容嘈杂不可靠、缺乏精确领域知识的问题。虽然MCP协议让智能体能访问数千种专业工具，但尚不清楚智能体是否能有效利用这些工具并与通用搜索结合解决复杂任务。

Method: 使用InfoMosaic-Flow可扩展流水线合成任务，将任务条件基于已验证的工具输出，强制执行跨源依赖关系，并过滤掉可通过简单查找解决的捷径案例。

Result: 对14个最先进LLM智能体的实验显示：仅靠网络信息不足（GPT-5准确率38.2%，通过率67.5%）；领域工具提供选择性但不一致的益处；22.4%的失败源于工具使用或选择错误。

Conclusion: 当前LLM在基本工具处理方面仍存在困难，需要更好的工具集成能力来解决复杂信息寻求任务。

Abstract: Information seeking is a fundamental requirement for humans. However,
existing LLM agents rely heavily on open-web search, which exposes two
fundamental weaknesses: online content is noisy and unreliable, and many
real-world tasks require precise, domain-specific knowledge unavailable from
the web. The emergence of the Model Context Protocol (MCP) now allows agents to
interface with thousands of specialized tools, seemingly resolving this
limitation. Yet it remains unclear whether agents can effectively leverage such
tools -- and more importantly, whether they can integrate them with
general-purpose search to solve complex tasks. Therefore, we introduce
InfoMosaic-Bench, the first benchmark dedicated to multi-source information
seeking in tool-augmented agents. Covering six representative domains
(medicine, finance, maps, video, web, and multi-domain integration),
InfoMosaic-Bench requires agents to combine general-purpose search with
domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable
pipeline that grounds task conditions in verified tool outputs, enforces
cross-source dependencies, and filters out shortcut cases solvable by trivial
lookup. This design guarantees both reliability and non-triviality. Experiments
with 14 state-of-the-art LLM agents reveal three findings: (i) web information
alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass
rate; (ii) domain tools provide selective but inconsistent benefits, improving
some domains while degrading others; and (iii) 22.4% of failures arise from
incorrect tool usage or selection, highlighting that current LLMs still
struggle with even basic tool handling.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: 提出一个多阶段、性能导向的编排框架PerfOrch，动态选择最适合的LLM来处理不同编程语言和开发阶段的代码生成任务，显著提升代码正确性和运行时性能。


<details>
  <summary>Details</summary>
Motivation: 当前单一模型方法忽视了不同LLM在不同编程语言、算法领域和开发阶段的计算优势异质性，需要更智能的模型选择和编排机制。

Method: 基于对17个最先进LLM在5种编程语言上的实证研究，开发了多阶段生成-修复-优化工作流，通过阶段验证和回滚机制动态路由任务到最适合的LLM。

Result: 在HumanEval-X和EffiBench-X基准测试中分别达到96.22%和91.37%的正确率，显著超越GPT-4o；58.76%的问题执行时间得到优化，中位加速比达17.67%-27.66%。

Conclusion: PerfOrch框架提供了一种可扩展的即插即用架构，能够适应快速发展的生成式AI环境，为生产级自动化软件工程提供了新范式。

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [21] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: NARRepair是首个为自动程序修复任务定制的非自回归代码生成模型，通过修复动作预测器、令牌间依赖提取器和两阶段解码器解决NAR方法在APR任务中的质量问题，在修复速度和准确性方面达到最先进的综合性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于机器学习的自动程序修复技术采用自回归方式，存在巨大的时间延迟问题，特别是参数较多的模型延迟更严重。为了解决这个问题，作者希望将非自回归方法应用于APR任务，实现并行输出目标代码以避免修复延迟。

Method: 提出NARRepair模型，包含三个主要创新：1）修复动作预测器缓解过度修正问题；2）令牌间依赖提取器解决缺乏令牌间依赖信息问题；3）两阶段解码器解决缺乏上下文信息问题。

Result: 在三个广泛使用的APR数据集上评估，结果显示：1）在有限修复时间内，NARRepair性能最佳；2）与AR-based APR技术相比，在GPU环境中修复速度提高了1.4-6.4倍。

Conclusion: NARRepair在修复速度和准确性方面实现了最先进的综合性能，成功将非自回归方法应用于自动程序修复任务。

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [22] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: RefFilter是一个重构感知的语义干扰检测工具，通过自动检测重构来减少现有静态分析技术中的误报问题，在保持检测覆盖度的同时显著降低假阳性率。


<details>
  <summary>Details</summary>
Motivation: 当前轻量级静态分析技术在协作软件开发中检测语义干扰时存在高误报率问题，主要原因是无法有效区分行为保持的代码重构和真正影响行为的变更。

Method: 在现有静态分析技术基础上集成自动重构检测功能，从报告中过滤掉行为保持的重构操作，从而减少误报。

Result: 在标记数据集上，RefFilter将误报率降低了近32%，虽然伴随了不显著的假阴性增加，但精度的整体提升显著超过了召回率的微小损失。

Conclusion: 重构感知的干扰检测是改进现代开发工作流中合并支持的实际有效策略。

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [23] [Abusing Notion's AI Agent for Data Theft](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.schneier.com%2Fblog%2Farchives%2F2025%2F09%2Fabusing-notions-ai-agent-for-data-theft.html%3Futm_source=tldrinfosec/1/010001999fe2f6c1-7d4fb401-e251-44c8-8e10-629017c31e04-000000/mAqAbR3bzXmC9xCrbG_2ZqV0bYpSFxLCPMFmu0JSBFg=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Notion的AI代理存在提示注入漏洞，攻击者可通过PDF中的隐藏文本指令让AI提取并外泄机密数据


<details>
  <summary>Details</summary>
Motivation: 揭示Notion AI代理的安全漏洞，该漏洞允许攻击者利用提示注入技术窃取私人数据

Method: 利用Simon Willson的'致命三重威胁'概念，通过在PDF中嵌入白色文本的恶意指令，让AI代理提取数据并通过网络搜索发送到攻击者控制的URL

Result: 成功演示了如何通过提示注入攻击从Notion AI代理中窃取机密数据

Conclusion: 当前AI部署存在严重安全风险，安全专业人员需要仔细评估AI系统的部署

Abstract: Abusing Notion's AI Agent for Data Theft (3 minute read) Notion's AI agents are susceptible to prompt injection attacks exploiting Simon Willson's 'lethal trifecta'—access to private data, exposure to untrusted content, and external communication. Attackers can embed malicious instructions in PDFs with white text instructing AI to extract and exfiltrate confidential data through web searches to attacker-controlled URLs. Security professionals should evaluate AI deployments carefully, as curre...

</details>


### [24] [Code Pathfinder](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fshivasurya%2Fcode-pathfinder%3Futm_source=tldrinfosec/1/010001999fe2f6c1-7d4fb401-e251-44c8-8e10-629017c31e04-000000/N9Qknn42-0BJ2FnIGFRhKgSt_dMd0kaib1xEQyUPlTM=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Code Pathfinder是一个用GoLang构建的开源代码分析工具，可作为GitHub CodeQL的替代方案，用于高级结构搜索、代码洞察和漏洞发现。


<details>
  <summary>Details</summary>
Motivation: 提供一个开源替代方案来替代GitHub CodeQL，使开发者能够进行高级代码结构分析和漏洞检测。

Method: 使用GoLang构建，专注于高级结构搜索功能，能够深入分析代码结构并识别潜在问题。

Result: 开发出了一个功能完整的开源代码分析工具，支持结构搜索、代码洞察和漏洞发现。

Conclusion: Code Pathfinder成功提供了一个有效的开源代码分析解决方案，填补了市场空白。

Abstract: Code Pathfinder (GitHub Repo) Code Pathfinder, the open-source alternative to GitHub CodeQL, was built with GoLang. Built for advanced structural search, derive insights, and find vulnerabilities in code.

</details>


### [25] [🏆Warp: Try the Top Rated Coding Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2Fcode%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=warp_code_10_1_primary%26utm_content=tldr_ai/2/010001999fea44da-9095fe21-1bba-466f-9314-b84577b142f2-000000/NUayruE-s2g__tos7ua33iP45ulrBH3ar9B5W_NRhtI=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Warp是一款顶级编程助手，在Terminal-Bench和SWE-bench Verified基准测试中表现优异，结合了终端功能和IDE交互性，支持多种顶级AI模型，平均每周为开发者节省5小时工作时间。


<details>
  <summary>Details</summary>
Motivation: 现有编程助手如Cursor、Codex和Claude Code存在局限性，需要更强大的工具来提升开发效率。

Method: 结合终端功能和IDE交互性，集成多种顶级AI模型（GPT-5、Sonnet 4.5、Opus 4.1、Gemini 2.5），提供高效的编程辅助体验。

Result: 在Terminal-Bench和SWE-bench Verified基准测试中表现最佳，被70万+开发者和56%的财富500强公司使用，平均每周为开发者节省5小时。

Conclusion: Warp是目前最优秀的编程助手工具，超越了现有解决方案，显著提升了开发效率。

Abstract: 🏆Warp: Try the Top Rated Coding Agent (Sponsor) You've tried Cursor, Codex, and Claude Code. Now try the coding agent that beats them.Warp leads on Terminal-Bench and SWE-bench Verified, trusted by 700K+ devs and 56% of the Fortune 500. ⚡️ Combines the power of the terminal with the interactivity of the IDE ⚡️ Works with all top tier models (GPT-5, Sonnet 4.5, Opus 4.1, Gemini 2.5) ⚡️ Developers save 5 hours per week on average “I used to be sold on Cursor…Warp is unlike any other tool I've u...

</details>


### [26] [Designing agentic loops](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FSep%2F30%2Fdesigning-agentic-loops%2F%3Futm_source=tldrai/1/010001999fea44da-9095fe21-1bba-466f-9314-b84577b142f2-000000/eudaY9ep8LVwW_wQCZ6hd0T_JWhh7q5YYlWPUV_xgAs=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文探讨了如何设计有效的智能体循环，使编码智能体能够通过迭代执行代码、纠正错误、探索实现细节和运行实验来暴力求解编码问题。


<details>
  <summary>Details</summary>
Motivation: 随着智能体技术的发展，编码智能体现在能够直接执行编写的代码并进行迭代优化，但如何有效设计这些智能体循环以最大化其解决问题的能力仍是一个挑战。

Method: 通过将问题分解为明确目标和可用工具集，让编码智能体通过迭代方式逐步逼近有效解决方案。

Result: 文章提出了设计智能体循环的方法论，使编码智能体能够更有效地暴力求解编码问题。

Conclusion: 熟练运用智能体的艺术在于合理设计循环机制，将复杂问题转化为可迭代求解的目标导向任务。

Abstract: Designing agentic loops (11 minute read) Agents can now directly exercise the code they are writing, correct errors, dig through existing implementation details, and even run experiments to find effective code solutions to problems. Coding agents are brute force tools for finding solutions to coding problems. Reducing problems to clear goals and sets of tools that can iterate towards those goals can help coding agents brute-force their way to an effective solution. The art of using agents wel...

</details>


### [27] [Uber's Strategy to Upgrading 2M+ Spark Jobs](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.uber.com%2Fblog%2Fubers-strategy-to-upgrading-2m-spark-jobs%2F%3Futm_source=tldrdata/1/01000199a4638f74-f152d3f2-c308-4145-9b11-ac3d4eca2f65-000000/i1duB6csZMOarEoGZaV2M8V9KVlNoAbvgPXwskHJ_90=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Uber使用Polyglot Piranha工具在6个月内自动化升级了超过40,000个Spark应用和2,100个应用，从Spark 2.4迁移到Spark 3.3


<details>
  <summary>Details</summary>
Motivation: 需要大规模升级Spark版本以提升性能和功能，但手动升级大量应用成本过高

Method: 使用开源的Polyglot Piranha工具，通过解析代码为抽象语法树(AST)并应用转换规则，实现跨应用的批量更改

Result: 成功在6个月内迁移了超过40,000个Spark应用和2,100个应用

Conclusion: 自动化代码重构工具能够显著加速大规模软件升级过程

Abstract: Uber's Strategy to Upgrading 2M+ Spark Jobs (10 minute read) Uber upgraded from Spark 2.4 to Spark 3.3, migrating over 40,000 Spark applications and 2,100 applications in just six months. It automated the process using Polyglot Piranha, an open-source tool that parses and rewrites code by converting it into an Abstract Syntax Tree (AST) and applying transformation rules to enable bulk changes across applications.

</details>


### [28] [Why Multi-Agent Systems Need Memory Engineering](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mongodb.com%2Fcompany%2Fblog%2Ftechnical%2Fwhy-multi-agent-systems-need-memory-engineering%3Futm_source=tldrwebdev/1/01000199a4a8ba1f-870350b5-7612-42ef-ae5b-5ea43dfc5e9c-000000/snSIwYZroorImiI5dA5N5KWZ43uY7TrqvFYV7Q9JqUk=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 多智能体系统需要内存工程来解决协调失败问题，通过创建持久共享内存基础设施来改善多智能体协作。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统经常因为缺乏适当的共享内存基础设施而失败，现有的上下文工程方法在多个智能体需要协调时会失效。

Method: 提出内存工程方法，创建持久、共享的内存基础设施，使AI智能体能够有效协调。

Result: 通过实现共享内存系统，可以显著提高多智能体系统的协调能力和整体性能。

Conclusion: 内存工程是多智能体系统成功的关键，解决了现有上下文工程方法在多智能体环境中的局限性。

Abstract: Why Multi-Agent Systems Need Memory Engineering (8 minute read) Multi-agent AI systems frequently fail because they don't have a proper shared memory infrastructure. While context engineering has improved individual agent performance by managing "the right information at the right time," this approach breaks down when multiple agents must coordinate without shared memory systems. The solution is memory engineering: creating a persistent, shared memory infrastructure that allows AI agents to e...

</details>


### [29] [Cursor 1.7](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fchangelog%2F1-7%3Futm_source=tldrwebdev/1/01000199a4a8ba1f-870350b5-7612-42ef-ae5b-5ea43dfc5e9c-000000/ZzLFxNYvYlmpiiYyhGfQUdxRUShx9kSxcKdenqNLA0o=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor 1.7版本引入了多项新功能和改进，包括智能代码补全、自定义代理行为的钩子、团队规则管理，以及可分享的深度链接提示功能。


<details>
  <summary>Details</summary>
Motivation: 提升代码编辑器的智能化水平和团队协作效率，通过增强代理功能和安全机制来改善开发体验。

Method: 在Cursor编辑器中集成智能代理功能，添加自定义钩子接口，实现沙盒环境执行命令，并提供团队规则配置和深度链接分享机制。

Result: 成功推出了包含智能补全、安全沙盒、团队协作工具和提示分享功能的Cursor 1.7版本。

Conclusion: Cursor 1.7通过引入智能代理和协作工具显著提升了开发效率和安全性，为团队开发提供了更好的支持。

Abstract: Cursor 1.7 (2 minute read) Cursor 1.7 introduces several new features and improvements, including Agent Autocomplete, Hooks for customizing Agent behavior, and team rules for consistent project management. Shareable deeplinks for prompts are now available in beta, allowing for easier sharing of workflows and instructions. Commands now execute in a sandboxed environment for better security, and users can monitor Agents from the menu bar.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [30] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 提出了ROTE算法，将人类行为建模为计算机程序而非信念驱动的策略，结合LLM生成行为程序假设空间和概率推理处理不确定性，在网格世界和家庭模拟器中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人类行为建模方法要么对理性做出不切实际的假设，要么计算量过大难以快速适应。日常社交互动往往遵循可预测的模式（脚本），这些高效的行为模式能最小化认知负荷。

Method: ROTE算法：1）使用大语言模型合成行为程序的假设空间；2）使用概率推理在该空间上进行不确定性推理；3）将动作理解视为程序合成问题。

Result: 在网格世界任务和大规模家庭模拟器中测试，ROTE从稀疏观察中预测人类和AI行为，在样本内准确性和样本外泛化方面比竞争基线（包括行为克隆和基于LLM的方法）高出多达50%。

Conclusion: 通过将动作理解视为程序合成问题，ROTE为AI系统在现实世界中高效有效地预测人类行为开辟了新路径。

Abstract: Accurate prediction of human behavior is essential for robust and safe
human-AI collaboration. However, existing approaches for modeling people are
often data-hungry and brittle because they either make unrealistic assumptions
about rationality or are too computationally demanding to adapt rapidly. Our
key insight is that many everyday social interactions may follow predictable
patterns; efficient "scripts" that minimize cognitive load for actors and
observers, e.g., "wait for the green light, then go." We propose modeling these
routines as behavioral programs instantiated in computer code rather than
policies conditioned on beliefs and desires. We introduce ROTE, a novel
algorithm that leverages both large language models (LLMs) for synthesizing a
hypothesis space of behavioral programs, and probabilistic inference for
reasoning about uncertainty over that space. We test ROTE in a suite of
gridworld tasks and a large-scale embodied household simulator. ROTE predicts
human and AI behaviors from sparse observations, outperforming competitive
baselines -- including behavior cloning and LLM-based methods -- by as much as
50% in terms of in-sample accuracy and out-of-sample generalization. By
treating action understanding as a program synthesis problem, ROTE opens a path
for AI systems to efficiently and effectively predict human behavior in the
real-world.

</details>


### [31] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: 提出一个基于多智能体辩论的新型评估框架，用于量化LLM智能体在交互环境中的社交和认知行为，发现智能体具有强烈的共识寻求倾向，并能通过角色分配形成稳定的心理测量特征。


<details>
  <summary>Details</summary>
Motivation: 传统评估基准无法捕捉智能体在交互环境中产生的社交和认知动态，需要新的评估方法来理解自主智能体的行为特征。

Method: 使用多智能体辩论作为"社交实验室"，让具有不同角色和激励的LLM智能体在LLM主持人的监督下就各种话题进行辩论，并采用心理测量和语义指标进行分析。

Result: 发现智能体具有强大的共识寻求倾向（语义一致性μ>0.88），角色分配能诱导稳定的心理测量特征，主持人的角色能显著改变辩论结果。

Conclusion: 该工作为面向智能体场景的动态、心理测量基础的评估协议提供了蓝图，是理解和塑造下一代AI智能体社交行为的关键方法。

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [32] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: AGILE通过将拼图任务转化为交互式学习过程，使用代码执行动作并接收视觉反馈，显著提升了VLM的感知和推理能力，在拼图任务上准确率从9.5%提升至82.8%，并在9个通用视觉任务上平均提升3.1%。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在多模态理解和推理方面虽有进步，但其基本感知和推理能力仍然有限，即使在简单拼图任务上表现也接近随机水平。高质量视觉语言数据的稀缺性和有限可扩展性限制了这些能力的提升。

Method: AGILE将拼图解决制定为交互过程，模型在每个步骤生成可执行代码来执行动作，环境提供细粒度视觉反馈来指导任务完成。通过观察和交互的迭代循环，模型通过探索和反馈逐步提升感知和推理能力。

Result: AGILE在复杂度不同的拼图任务上显著提升性能（在2×2设置下准确率从9.5%提升至82.8%），并在9个通用视觉任务上表现出强泛化能力，平均提升3.1%。

Conclusion: 这项工作为推进多模态模型的推理和泛化能力开辟了新途径，并为多模态强化学习数据稀缺问题提供了高效、可扩展的解决方案。

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [33] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK是一个用于评估多平台代理环境中长期记忆和状态跟踪的基准测试，模拟真实组织工作流程，整合Slack、Linear和Git等平台的异步事件。


<details>
  <summary>Details</summary>
Motivation: 现有记忆基准测试主要关注对话场景，但企业动态环境中记忆评估的需求对于有效应用至关重要。

Method: 通过专家手动设计和基于代理的可扩展合成来策划MEMTRACK数据集，生成基于真实软件开发过程的生态有效场景。

Result: 在SoTA LLM和记忆后端上的实验显示，在长时程利用记忆、处理跨平台依赖和解决矛盾方面存在挑战。表现最好的GPT-5模型在MEMTRACK上仅达到60%的正确性得分。

Conclusion: 这项工作为记忆增强代理的评估研究提供了一个可扩展框架，超越了现有对话设置的关注，并为复杂组织环境中的多代理、多平台记忆基准测试奠定了基础。

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [34] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: 提出一种将推理时检索转化为学习能力的蒸馏方法，通过从智能体失败中提取紧凑提示，生成改进的教师轨迹，并训练学生模型内部化这些能力，在ALFWorld和WebShop基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在多步任务中经常出现可预测的失败，如尝试不满足前提条件的动作、发出冗余命令或错误处理环境约束。虽然RAG可以在运行时提供指导，但需要维护外部知识库并增加计算开销。

Method: 三步蒸馏流程：(1)从智能体失败中提取紧凑可重用的提示；(2)使用这些提示在回合开始时通过一次性检索生成改进的教师轨迹；(3)在移除提示字符串的情况下训练学生模型，强制内部化而非记忆。

Result: 在ALFWorld上达到91%成功率（基线为79%），WebShop得分提升至72（基线为61），同时比检索增强的教师模型少用10-60%的token。方法在7B/14B参数规模和ReAct/StateAct架构上均有效。

Conclusion: 检索的好处可以通过有针对性的微调有效内部化，无需永久性的运行时依赖，证明了将推理时检索转化为学习能力的可行性。

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [35] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: 提出了一种利用LLM代理自动化数据驱动建模和分析的创新流程，特别关注回归任务，在临界热通量预测基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代工程依赖海量实验和模拟数据，传统数据驱动方法需要大量人工干预，难以有效扩展和泛化到多样化应用。

Method: 评估了两种LLM代理框架：多代理系统（专业化协作代理）和单代理系统（基于ReAct范式），自主处理数据预处理、神经网络开发、训练、超参数优化和不确定性量化。

Result: 在约25,000个实验数据点的临界热通量预测基准测试中，LLM代理开发的模型超越了传统查找表，预测精度和不确定性量化与人类专家开发的贝叶斯优化深度神经网络模型相当。

Conclusion: LLM代理在自动化复杂工程建模任务方面具有巨大潜力，能显著减少人工工作量同时达到或超越现有预测性能标准。

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [36] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR: Falconer是一个结合LLM智能推理和轻量级代理模型的可扩展知识挖掘框架，通过LLM作为规划器和标注器，训练小型代理模型来替代昂贵的LLM部署，在保持准确性的同时大幅降低成本和提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模知识挖掘中LLM部署成本过高，而传统分类器和提取器管道又缺乏泛化能力的问题。

Method: 使用LLM作为规划器分解用户指令为可执行管道，作为标注器生成监督数据训练小型代理模型，将分类和提取统一为get label和get span两个原子操作。

Result: Falconer在指令跟随准确性上接近最先进的LLM，同时推理成本降低90%，大规模知识挖掘速度提升20倍以上。

Conclusion: Falconer为深度研究提供了高效可扩展的基础框架，成功平衡了性能与成本。

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [37] [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
*Rui Liu,Dian Yu,Tong Zheng,Runpeng Dai,Zongxia Li,Wenhao Yu,Zhenwen Liang,Linfeng Song,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.AI

TL;DR: VOGUE是一种针对多模态大语言模型的新方法，通过将视觉输入视为随机上下文，量化策略对视觉扰动的敏感性，从而在输入空间进行探索，有效提升多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在处理多模态语言模型时，将视觉输入视为固定条件，忽略了视觉变化带来的模糊性，导致策略对合理视觉变化的鲁棒性不足。

Method: VOGUE将图像视为随机上下文，使用对称KL散度量化策略对视觉扰动的敏感性，结合不确定性比例奖励、token熵奖励和退火采样调度，平衡探索与利用。

Result: 在Qwen2.5-VL-3B/7B模型上实现，在三个视觉数学基准上平均提升2.6%的pass@1准确率，在三个通用推理基准上提升3.7%，同时提高pass@4性能并缓解RL微调中的探索衰减。

Conclusion: 基于视觉输入固有不确定性的探索策略是提升多模态推理的有效方法。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.

</details>


### [38] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: 提出InfoSeeker框架，将任务导向规划与信息寻求相结合，在部分可观测环境中通过主动收集信息来对齐内部动态并做出最优决策


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划智能体在处理观测不确定性时，往往忽视其内部动态与实际环境之间的差异，需要一种能够主动寻求信息来更新内部动态的方法

Method: InfoSeeker框架提示LLM通过规划行动来验证理解、检测环境变化或测试假设，然后生成或修订任务导向计划

Result: 在包含不完全观测和不确定动态的部分可观测环境基准测试中，InfoSeeker比现有方法性能提升74%，且在机器人操作和网页导航等基准测试中表现优于基线方法

Conclusion: 在部分可观测环境中，紧密集成规划和信息寻求对于实现鲁棒行为至关重要

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [39] [Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models](https://arxiv.org/abs/2510.01544)
*Shaoan Xie,Lingjing Kong,Xiangchen Song,Xinshuai Dong,Guangyi Chen,Eric P. Xing,Kun Zhang*

Main category: cs.AI

TL;DR: 提出了Step-Aware Policy Optimization (SAPO)算法，通过过程奖励函数引导扩散语言模型学习结构化的推理路径，解决现有强化学习方法中存在的非结构化精炼问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的扩散语言模型训练方法依赖稀疏的结果奖励，可能强化导致偶然正确结果的错误推理路径，这与推理的自然结构不匹配。

Method: 首先提出理论框架将复杂问题解决形式化为层次选择过程，然后引入SAPO算法，使用过程奖励函数使模型的去噪过程与潜在推理层次对齐。

Result: 实验结果表明该方法在挑战性推理基准上显著提升性能，并增强了生成过程的可解释性。

Conclusion: 基于理论框架的SAPO算法能有效引导扩散语言模型学习结构化推理，解决现有方法的缺陷。

Abstract: Diffusion language models (dLLMs) offer a promising, non-autoregressive
paradigm for text generation, yet training them for complex reasoning remains a
key challenge. Current reinforcement learning approaches often rely on sparse,
outcome-based rewards, which can reinforce flawed reasoning paths that lead to
coincidentally correct answers. We argue that this stems from a fundamental
mismatch with the natural structure of reasoning. We first propose a
theoretical framework that formalizes complex problem solving as a hierarchical
selection process, where an intractable global constraint is decomposed into a
series of simpler, localized logical steps. This framework provides a
principled foundation for algorithm design, including theoretical insights into
the identifiability of this latent reasoning structure. Motivated by this
theory, we identify unstructured refinement -- a failure mode where a model's
iterative steps do not contribute meaningfully to the solution -- as a core
deficiency in existing methods. We then introduce Step-Aware Policy
Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising
process with the latent reasoning hierarchy. By using a process-based reward
function that encourages incremental progress, SAPO guides the model to learn
structured, coherent reasoning paths. Our empirical results show that this
principled approach significantly improves performance on challenging reasoning
benchmarks and enhances the interpretability of the generation process.

</details>


### [40] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: 提出InvThink方法，通过逆向思维让LLMs在生成回答前先分析潜在危害和后果，从而主动避免安全风险


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法直接优化安全回答，但缺乏对潜在风险的系统性思考，需要一种能主动预防危害的方法

Method: 训练模型进行三步逆向思考：1)列举潜在危害 2)分析后果 3)生成主动避免这些风险的安全输出。通过监督微调和强化学习在三个LLM家族中实现

Result: 相比SafetyPrompt等基线方法，在有害回答上减少达15.7%；安全改进随模型规模扩展更强；缓解安全税，保持标准基准上的通用推理能力；在高风险领域表现优异

Conclusion: 逆向推理为构建更安全、更强大的语言模型提供了可扩展和泛化的路径

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [41] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: 提出了AdvEvo-MARL框架，通过对抗性多智能体强化学习将安全性内化到任务智能体中，无需外部防护模块即可同时提升安全性和任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM多智能体系统存在越狱、提示注入和对抗性协作等安全风险，现有防御方法（自我验证和外部防护模块）存在性能不足、系统开销大和单点故障等问题。

Method: 使用协同进化的多智能体强化学习框架，联合优化攻击者（生成越狱提示）和防御者（任务智能体），引入公共基线进行优势估计以稳定学习和促进协作。

Result: 在代表性攻击场景中，AdvEvo-MARL将攻击成功率保持在20%以下，而基线方法可达38.33%，同时保持甚至提升任务准确率（推理任务最高提升3.67%）。

Conclusion: 安全性和实用性可以在不依赖额外防护智能体或增加系统开销的情况下共同提升。

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [42] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: 本文揭示了计算机使用代理(CUAs)存在盲目目标导向(BGD)偏差，即不顾可行性、安全性或上下文地追求目标。作者开发了BLIND-ACT基准测试，评估了9个前沿模型，发现平均BGD率高达80.8%。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理在GUI上执行操作来完成用户目标，但存在盲目追求目标的风险，需要系统性地识别和缓解这种风险。

Method: 开发BLIND-ACT基准测试，包含90个任务，捕捉BGD的三种模式：缺乏上下文推理、在模糊性下做假设和决策、矛盾或不可行目标。使用LLM评估代理行为。

Result: 评估9个前沿模型，平均BGD率为80.8%。提示干预能降低BGD水平，但风险依然存在。观察到三种失败模式：执行优先偏见、思维-行动脱节、请求优先。

Conclusion: 识别BGD并引入BLIND-ACT为未来研究和缓解这种基本风险奠定了基础，确保CUA的安全部署。

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [43] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: 提出了VaPR框架，通过LLM引导的响应编辑生成硬负样本，解决了合成偏好标注中的风格和长度偏差问题，显著提升了大型视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有偏好微调方法忽视了合成偏好标注中普遍存在的风格和长度偏差噪声，需要一种能够生成高质量硬负样本的方法来改善模型对齐效果。

Method: 基于LLM引导的响应编辑框架，生成带有目标错误的拒绝响应，保持与接受响应在风格和长度上的相似性，构建了包含30K样本的VaPR数据集。

Result: 在十个基准测试中，VaPR模型显著提升了性能：LLaVA平均提升6.5%，Qwen2VL提升4.0%，Qwen2.5VL提升1.5%，特别是在推理任务上表现突出，同时减少了二元问题中回答"是"的倾向。

Conclusion: VaPR框架有效解决了合成偏好标注的噪声问题，性能随数据规模扩大而持续提升，且该框架可泛化到开源LLM作为编辑器，达到与GPT-4o合成数据相近的性能。

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [44] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: 提出了一个结构化决策支持框架，将不同类型的AI智能体架构与NIST网络安全框架2.0系统对齐，为选择部署AI解决方案应对网络威胁提供透明方法。


<details>
  <summary>Details</summary>
Motivation: 弥合理论AI构建与运营网络安全需求之间的差距，提供统一的方法来选择和部署AI解决方案应对当代网络威胁。

Method: 采用结构化框架，将NIST CSF 2.0功能细分为具体任务，并将AI智能体特性（自主性、自适应学习、实时响应）与每个子类别的安全要求关联。

Result: 通过概念验证表明，定制的AI智能体部署可以符合现实约束和风险状况，增强态势感知、加速响应时间，并通过自适应风险管理加强长期韧性。

Conclusion: 该研究为遵循行业标准的稳健、经验验证的多智能体系统奠定了基础，将AI理论与行业指南相结合。

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [45] [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
*Zhihao Dou,Qinjian Zhao,Zhongwei Wan,Dinggen Zhang,Weida Wang,Towsif Raiyan,Benteng Chen,Qingtao Pan,Yang Ouyang,Zhiqiang Gao,Shufei Zhang,Sumon Biswas*

Main category: cs.AI

TL;DR: 提出了PTA-GRPO框架，通过两阶段方法改进LLM的推理能力：第一阶段使用高级LLM将思维链提炼为紧凑的高层指导进行监督微调；第二阶段引入指导感知的强化学习方法联合优化最终输出和高层指导质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂任务中的推理主要依赖思维链，但由于自回归的token级生成，推理过程局限于局部决策，缺乏全局规划，导致冗余、不连贯或不准确的推理，降低整体性能。现有方法如树算法和强化学习计算成本高且难以产生最优推理轨迹。

Method: 两阶段框架：1）使用高级LLM将思维链提炼为紧凑高层指导，进行监督微调；2）引入指导感知的强化学习方法，联合优化最终输出和高层指导质量。

Result: 在多个数学推理基准测试（MATH、AIME2024、AIME2025、AMC）和不同基础模型（Qwen2.5-7B-Instruct、Qwen3-8B、Qwen3-14B、LLaMA3.2-3B）上的实验表明，PTA-GRPO在不同模型和任务中均实现了稳定且显著的改进。

Conclusion: PTA-GRPO框架有效提升了LLM的推理能力，通过结合高层规划和细粒度思维链推理，验证了其有效性和泛化能力。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning abilities
in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,
due to their autoregressive token-level generation, the reasoning process is
largely constrained to local decision-making and lacks global planning. This
limitation frequently results in redundant, incoherent, or inaccurate
reasoning, which significantly degrades overall performance. Existing
approaches, such as tree-based algorithms and reinforcement learning (RL),
attempt to address this issue but suffer from high computational costs and
often fail to produce optimal reasoning trajectories. To tackle this challenge,
we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy
Optimization PTA-GRPO, a two-stage framework designed to improve both
high-level planning and fine-grained CoT reasoning. In the first stage, we
leverage advanced LLMs to distill CoT into compact high-level guidance, which
is then used for supervised fine-tuning (SFT). In the second stage, we
introduce a guidance-aware RL method that jointly optimizes the final output
and the quality of high-level guidance, thereby enhancing reasoning
effectiveness. We conduct extensive experiments on multiple mathematical
reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across
diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and
LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently
achieves stable and significant improvements across different models and tasks,
validating its effectiveness and generalization.

</details>


### [46] [To Mask or to Mirror: Human-AI Alignment in Collective Reasoning](https://arxiv.org/abs/2510.01924)
*Crystal Qian,Aaron Parisi,Clémentine Bouleau,Vivian Tsai,Maël Lebreton,Lucas Dixon*

Main category: cs.AI

TL;DR: 本文提出评估LLMs与人类集体决策对齐的框架，通过Lost at Sea实验对比人类和AI群体在领导选举中的行为差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地用于建模和增强集体决策，需要检验它们与人类社会推理的对齐程度，特别是集体层面的对齐而非个体层面。

Method: 使用Lost at Sea社会心理学任务进行大规模在线实验（N=748），随机分配群体到有可见人口属性或无人口属性的领导选举条件，然后模拟匹配的LLM群体进行基准测试。

Result: LLM行为出现分化：有些镜像人类偏见，有些掩盖这些偏见并试图补偿。人类-AI在集体推理中的对齐取决于情境、线索和模型特定的归纳偏见。

Conclusion: 理解LLMs如何与集体人类行为对齐对于推进社会对齐AI至关重要，需要能够捕捉集体推理复杂性的动态基准。

Abstract: As large language models (LLMs) are increasingly used to model and augment
collective decision-making, it is critical to examine their alignment with
human social reasoning. We present an empirical framework for assessing
collective alignment, in contrast to prior work on the individual level. Using
the Lost at Sea social psychology task, we conduct a large-scale online
experiment (N=748), randomly assigning groups to leader elections with either
visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We
then simulate matched LLM groups conditioned on the human data, benchmarking
Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some
mirror human biases; others mask these biases and attempt to compensate for
them. We empirically demonstrate that human-AI alignment in collective
reasoning depends on context, cues, and model-specific inductive biases.
Understanding how LLMs align with collective human behavior is critical to
advancing socially-aligned AI, and demands dynamic benchmarks that capture the
complexities of collective reasoning.

</details>


### [47] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 提出了一个专门针对深度研究代理（DRAs）的严格基准和多维评估框架，包含214个专家策划的挑战性查询和手动构建的参考包，用于评估长格式报告的质量。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估维度、响应格式和评分机制方面存在不足，无法有效评估具有任务分解、跨源检索、多阶段推理和结构化输出能力的深度研究代理系统。

Method: 构建了包含10个广泛主题领域的214个专家策划查询的基准，并开发了多维评估框架，整合了语义质量、主题聚焦和检索可信度的评分指标。

Result: 实验证实主流DRAs在性能上优于基于网络搜索工具增强的推理模型，但仍存在显著的改进空间。

Conclusion: 该研究为DRA系统的能力评估、架构改进和范式进步提供了坚实基础。

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [48] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: RLVR方法虽然旨在提升大语言模型的推理能力，但研究发现它反而会缩小推理边界，主要原因是负干扰现象和赢家通吃现象。


<details>
  <summary>Details</summary>
Motivation: 研究RLVR方法中出现的推理边界缩小问题，分析其学习动态中的关键现象。

Method: 通过理论分析和实证研究，揭示RLVR中的负干扰和赢家通吃现象，并提出针对低概率问题的数据筛选算法。

Result: 发现RLVR会强化高概率正确解而抑制低概率解，导致模型收敛到狭窄的解题策略。提出的数据筛选算法显著提升了Pass@k性能。

Conclusion: RLVR存在固有的学习动态问题，通过专注于低概率问题的学习可以有效改善性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [49] [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
*Yuxiao Qu,Anikait Singh,Yoonho Lee,Amrith Setlur,Ruslan Salakhutdinov,Chelsea Finn,Aviral Kumar*

Main category: cs.AI

TL;DR: 提出了RLAD方法，通过推理抽象来引导模型进行更有效的推理，采用两玩家RL训练范式联合训练抽象生成器和解决方案生成器。


<details>
  <summary>Details</summary>
Motivation: 解决当前大型模型在推理过程中缺乏一致的程序捕获和重用能力，避免冗长和退化的探索，实现更有效的算法推理。

Method: 引入推理抽象概念，训练模型能够针对问题提出多个抽象，然后通过RL激励在利用这些抽象信息的基础上构建解决方案，形成抽象生成器和解决方案生成器的两玩家RL训练范式。

Result: RLAD方法实现了结构化探索，解耦了抽象提议和解决方案生成的学习信号，并提高了对更难问题的泛化能力。

Conclusion: 推理抽象能有效引导有意义的探索，在测试时分配更多计算资源生成抽象比生成更多解决方案更有利于性能提升。

Abstract: Reasoning requires going beyond pattern matching or memorization of solutions
to identify and implement "algorithmic procedures" that can be used to deduce
answers to hard problems. Doing so requires realizing the most relevant
primitives, intermediate results, or shared procedures, and building upon them.
While RL post-training on long chains of thought ultimately aims to uncover
this kind of algorithmic behavior, most reasoning traces learned by large
models fail to consistently capture or reuse procedures, instead drifting into
verbose and degenerate exploration. To address more effective reasoning, we
introduce reasoning abstractions: concise natural language descriptions of
procedural and factual knowledge that guide the model toward learning
successful reasoning. We train models to be capable of proposing multiple
abstractions given a problem, followed by RL that incentivizes building a
solution while using the information provided by these abstractions. This
results in a two-player RL training paradigm, abbreviated as RLAD, that jointly
trains an abstraction generator and a solution generator. This setup
effectively enables structured exploration, decouples learning signals of
abstraction proposal and solution generation, and improves generalization to
harder problems. We also show that allocating more test-time compute to
generating abstractions is more beneficial for performance than generating more
solutions at large test budgets, illustrating the role of abstractions in
guiding meaningful exploration.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [50] [雅思词汇](http://mp.weixin.qq.com/s?__biz=MzI3OTYzNTE5NQ==&mid=2247492989&idx=3&sn=4ec16f50c6958bb7d82b4d33fd568141&chksm=ea3b3c5be7ad09f06ca6d806d6ab0773ba717f1e3d12ed1d1aec01757a9f7b34f7b9a4e1b124#rd)
*NewBytes*

Main category: wechat.article

TL;DR: 2. Agentic （adj.）词典释义 characterized by action or agency；having the capacity to act independently and make choices. | 具有主动性的；有代理能力的。生活场景例句 The new software update provides a more agentic AI assistant t...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2. Agentic （adj.）词典释义 characterized by action or agency；having the capacity to act independently and make choices. | 具有主动性的；有代理能力的。生活场景例句 The new software update provides a more agentic AI assistant that can anticipate user needs. → 新的软

</details>


### [51] [从手工作坊到<em class="highlight">Agentic</em> AI：组织正在经历怎样的重构？](http://mp.weixin.qq.com/s?__biz=Mzg2NTEyMjA0MQ==&mid=2247486458&idx=1&sn=750175ffd7c09d918076c65e1a87a561&chksm=cf1c3ad2e994c7549e2399cf430b4f30889af9b041eeae349cca62a84611a6d8e322e4e7467b#rd)
*零一格物*

Main category: wechat.article

TL;DR: 麦肯锡的AI时代组织洞察文章《The Agentic Organization： Contours of the Next Paradigm for the AI Era》分析了组织的进化。报告将组织进化的历史分成了四个阶段：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 麦肯锡的AI时代组织洞察文章《The Agentic Organization： Contours of the Next Paradigm for the AI Era》分析了组织的进化。报告将组织进化的历史分成了四个阶段：

</details>


### [52] [AI当高级白领牛马，哪家<em class="highlight">大模型</em>最强](http://mp.weixin.qq.com/s?__biz=MzkzMzI2ODAyNQ==&mid=2247491615&idx=1&sn=47ab6722f503c4afa2db787fb9c2ec86&chksm=c3e700ade5aefcefbdba61d8be60bfc1bd5c9bee35905a5019e966b3ac28c1278671a053beff#rd)
*未尽研究*

Main category: wechat.article

TL;DR: AI大模型也好，智能体也好，在各种测评榜上刷得不亦乐乎。对于跟踪模型进展，发现模型能力上限确实有用；不过衡量的是模型的抽象能力，而不是具有经济价值的产出。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: AI大模型也好，智能体也好，在各种测评榜上刷得不亦乐乎。对于跟踪模型进展，发现模型能力上限确实有用；不过衡量的是模型的抽象能力，而不是具有经济价值的产出。

</details>


### [53] [智能体（AI Agent）与<em class="highlight">大模型</em>（LLM）：各司其职又通力协作](http://mp.weixin.qq.com/s?__biz=Mzk3NTU2NjU5MQ==&mid=2247487881&idx=1&sn=7c57beb89809957b7a7ef27657fa32c2&chksm=c5c0b3065c103c1e5f24883a5c4a45adaf1336fde12147fee6e1fe83affa9c45d52f1711f6dd#rd)
*飞竹百科*

Main category: wechat.article

TL;DR: 大模型是基于Transformer架构构建的AI系统，核心能力围绕“语言”展开，通过对海量文本数据的预训练，掌握语法、语义与语境关联。根据输入的prompt（提示词）预测下一个词的序列，从而实现问答、创作、翻译等语言类任务。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型是基于Transformer架构构建的AI系统，核心能力围绕“语言”展开，通过对海量文本数据的预训练，掌握语法、语义与语境关联。根据输入的prompt（提示词）预测下一个词的序列，从而实现问答、创作、翻译等语言类任务。

</details>


### [54] [<em class="highlight">大模型</em>如何重塑信息获取？深度搜索与信息智能体的前沿探索 | CNCC](http://mp.weixin.qq.com/s?__biz=MjM5MTY5ODE4OQ==&mid=2651609650&idx=3&sn=be2e72c06c7f0b31c23e42a48cdd3f55&chksm=bc5c203ea27945f31a9762e04f9ec006ae4ed4f25b9868428e617e63649d3f6fde4f658d8a1d#rd)
*中国计算机学会*

Main category: wechat.article

TL;DR: 上海交通大学 3大模型遇见检索：从外部知识增强到内部知识探索 林鸿宇 中国科学院软件研究所 4构建自主深度研究智能体 蒋勇 阿里巴巴通义实验室


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 上海交通大学 3大模型遇见检索：从外部知识增强到内部知识探索 林鸿宇 中国科学院软件研究所 4构建自主深度研究智能体 蒋勇 阿里巴巴通义实验室

</details>


### [55] [【检测认证】中国信通院牵头的5项<em class="highlight">大模型</em>行业标准正式发布](http://mp.weixin.qq.com/s?__biz=MzkzNzE3NzIyNA==&mid=2247691184&idx=2&sn=f7f51e5dccb2dc271ac13356603b761d&chksm=c363f65c7f2bb4c11fda762ee8c15aeefcfebe567bd7ca6fcc613f9ad35f3bd1a6c5474fc733#rd)
*深圳市人工智能产业协会*

Main category: wechat.article

TL;DR: 该系列标准覆盖大模型的开发、管理、运营等多个阶段，主要包括模型开发、能力评估、应用成效、运营管理和可信要求五部分，为大模型技术和产品的研发测试和应用推广提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 该系列标准覆盖大模型的开发、管理、运营等多个阶段，主要包括模型开发、能力评估、应用成效、运营管理和可信要求五部分，为大模型技术和产品的研发测试和应用推广提供了重要参考。

</details>


### [56] [一文讲清：RAG、Agent、微调、RLHF等6种常见的<em class="highlight">大模型</em>定制策略，从理论到实践！](http://mp.weixin.qq.com/s?__biz=Mzk3NTE4MTYyOA==&mid=2247487048&idx=1&sn=af32a9c9dc0f2dfddf34843b9cb96d48&chksm=c5726c74ebb25c84696dd0908639fa5a9b7a10d90545a31070359c0210f2218cc916e5d96bdf#rd)
*智泊AI*

Main category: wechat.article

TL;DR: 大语言模型（LLM）是基于自监督学习预训练的深度学习模型，训练数据量庞大、训练时间长，并且包含大量的参数。LLM在过去两年中彻底改变了自然语言处理领域，展现了在理解和生成类人文本方面的卓越能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大语言模型（LLM）是基于自监督学习预训练的深度学习模型，训练数据量庞大、训练时间长，并且包含大量的参数。LLM在过去两年中彻底改变了自然语言处理领域，展现了在理解和生成类人文本方面的卓越能力。

</details>


### [57] [腾讯<em class="highlight">大模型</em>面经来了！](http://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247625359&idx=4&sn=9b38e5f398411d08b9523ef401c0e457&chksm=f81078296b3d84dae40d548b0d7628217d3bf5ff20826ff0bddeae9953d7c1c19d99b422bf1b#rd)
*CVer*

Main category: wechat.article

TL;DR: 方向涉及：ml/dl/cv/nlp/推荐/大数据/c++/python/java； 方向涉及：ml/dl/cv/nlp/推荐/大数据/c++/python； 题型涉及：ai算法题/编程题（注：已附上部分题目的答案） 题型涉及：ai算法题/编程题（注：已附上部分题目的答案）） [ai算法题] [a


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 方向涉及：ml/dl/cv/nlp/推荐/大数据/c++/python/java； 方向涉及：ml/dl/cv/nlp/推荐/大数据/c++/python； 题型涉及：ai算法题/编程题（注：已附上部分题目的答案） 题型涉及：ai算法题/编程题（注：已附上部分题目的答案）） [ai算法题] [a

</details>


### [58] [10/2/2025 AI速递 | 

<em class="highlight">大模型</em>技术密集突破：开源优化与多模态应用并进](http://mp.weixin.qq.com/s?__biz=MzU0Mjc3NzE2Nw==&mid=2247488213&idx=1&sn=2b25555ac5a82caeb6773fec1b77cafd&chksm=face001de08823a8ced9c998ec69f4eb55911b031bec8a31ba21d8a5c4ef05597fe2d5abc17d#rd)
*AIBUPT*

Main category: wechat.article

TL;DR: 蚂蚁集团于2025年10月2日正式开源其自主研发的Ring-1T-preview大模型，这是全球首个参数规模达到万亿级别的开源人工智能模型。该模型基于混合专家系统（MoE）架构，包含128个独立专家网络，在代码生成专项测试中以HumanEval 87.6分


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 蚂蚁集团于2025年10月2日正式开源其自主研发的Ring-1T-preview大模型，这是全球首个参数规模达到万亿级别的开源人工智能模型。该模型基于混合专家系统（MoE）架构，包含128个独立专家网络，在代码生成专项测试中以HumanEval 87.6分

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab](https://arxiv.org/abs/2510.01264)
*Isaac Peterson,Christopher Allred,Jacob Morrey,Mario Harper*

Main category: cs.LG

TL;DR: 将IsaacLab框架扩展到支持在高保真物理模拟中训练对抗性策略，引入异构多智能体对抗环境，并集成竞争性HAPPO算法进行高效训练。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在动态环境中的协作已有研究，但对抗性交互在现实应用（如追逃、安全、竞争操作）中同样关键，需要支持异构智能体间的不对称目标和能力。

Method: 扩展IsaacLab框架，构建异构多智能体对抗环境套件，集成竞争性异构智能体强化学习与近端策略优化（HAPPO）算法。

Result: 在多个基准场景中的实验表明，该框架能够建模和训练形态多样化多智能体竞争的鲁棒策略，同时保持高吞吐量和模拟真实性。

Conclusion: 提出的框架成功支持了异构多智能体对抗策略的高效训练，为现实世界对抗性应用提供了可行的解决方案。

Abstract: Multi-Agent Reinforcement Learning (MARL) is central to robotic systems
cooperating in dynamic environments. While prior work has focused on these
collaborative settings, adversarial interactions are equally critical for
real-world applications such as pursuit-evasion, security, and competitive
manipulation. In this work, we extend the IsaacLab framework to support
scalable training of adversarial policies in high-fidelity physics simulations.
We introduce a suite of adversarial MARL environments featuring heterogeneous
agents with asymmetric goals and capabilities. Our platform integrates a
competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal
Policy Optimization (HAPPO), enabling efficient training and evaluation under
adversarial dynamics. Experiments across several benchmark scenarios
demonstrate the framework's ability to model and train robust policies for
morphologically diverse multi-agent competition while maintaining high
throughput and simulation realism. Code and benchmarks are available at:
https://github.com/DIRECTLab/IsaacLab-HARL .

</details>


### [60] [Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance](https://arxiv.org/abs/2510.01269)
*Rohan Vitthal Thorat,Juhi Singh,Rajdip Nayek*

Main category: cs.LG

TL;DR: 提出了一种结合LQR和RL的混合控制框架，用于结构振动控制，避免了对准确系统模型的依赖，同时降低了RL训练期间的风险。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的振动控制方法需要繁琐的系统辨识，而纯RL方法在训练阶段可能对结构造成损害。研究发现即使基于错误模型的LQR控制器也能优于无控制情况，因此提出混合框架。

Method: 使用基于随机选择模型的LQR控制器来引导RL控制器，LQR策略不依赖真实或近似结构模型，保持整体框架的无模型特性。

Result: 该混合方法消除了对显式系统模型的依赖，同时最小化了朴素RL实现中的探索风险。

Conclusion: 这是首个解决RL振动控制训练安全挑战并提供验证解决方案的研究。

Abstract: Structural vibrations induced by external excitations pose significant risks,
including safety hazards for occupants, structural damage, and increased
maintenance costs. While conventional model-based control strategies, such as
Linear Quadratic Regulator (LQR), effectively mitigate vibrations, their
reliance on accurate system models necessitates tedious system identification.
This tedious system identification process can be avoided by using a model-free
Reinforcement learning (RL) method. RL controllers derive their policies solely
from observed structural behaviour, eliminating the requirement for an explicit
structural model. For an RL controller to be truly model-free, its training
must occur on the actual physical system rather than in simulation. However,
during this training phase, the RL controller lacks prior knowledge and it
exerts control force on the structure randomly, which can potentially harm the
structure. To mitigate this risk, we propose guiding the RL controller using a
Linear Quadratic Regulator (LQR) controller. While LQR control typically relies
on an accurate structural model for optimal performance, our observations
indicate that even an LQR controller based on an entirely incorrect model
outperforms the uncontrolled scenario. Motivated by this finding, we introduce
a hybrid control framework that integrates both LQR and RL controllers. In this
approach, the LQR policy is derived from a randomly selected model and its
parameters. As this LQR policy does not require knowledge of the true or an
approximate structural model the overall framework remains model-free. This
hybrid approach eliminates dependency on explicit system models while
minimizing exploration risks inherent in naive RL implementations. As per our
knowledge, this is the first study to address the critical training safety
challenge of RL-based vibration control and provide a validated solution.

</details>


### [61] [Optimal Stopping vs Best-of-$N$ for Inference Time Optimization](https://arxiv.org/abs/2510.01394)
*Yusuf Kalayci,Vinod Raman,Shaddin Dughmi*

Main category: cs.LG

TL;DR: 提出基于Pandora's Box问题的新框架，用于LLM推理时优化，通过UCB算法自适应决定何时停止生成，相比Best-of-N采样可减少15-35%的生成次数。


<details>
  <summary>Details</summary>
Motivation: LLM生成需要在输出质量和推理成本之间平衡，特别是在多生成场景下，需要优化推理时决策。

Method: 将每个生成视为打开带随机奖励的"盒子"，开发UCB风格的Pandora's Box算法，结合Bradley-Terry变换处理不同提示的奖励缩放问题。

Result: 在AlpacaFarm和HH-RLHF数据集上实验表明，自适应策略能达到与Best-of-N采样相同的性能，但平均减少15-35%的生成次数。

Conclusion: 建立了最优停止理论与推理时扩展之间的原则性桥梁，为LLM部署提供了理论性能保证和实际效率提升。

Abstract: Large language model (LLM) generation often requires balancing output quality
against inference cost, especially when using multiple generations. We
introduce a new framework for inference-time optimization based on the
classical Pandora's Box problem. Viewing each generation as opening a costly
"box" with random reward, we develop algorithms that decide when to stop
generating without knowing the underlying reward distribution. Our first
contribution is a UCB-style Pandora's Box algorithm, which achieves performance
that is provably close to Weitzman's algorithm, the optimal strategy when the
distribution is known. We further adapt this method to practical LLM settings
by addressing reward scaling across prompts via a Bradley-Terry inspired
transformation. This leads to an adaptive inference-time optimization method
that normalizes rewards and learns stopping thresholds on the fly. Experiments
on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,
show that our adaptive strategy can obtain the same performance as non-adaptive
Best-of-N sampling while requiring 15-35 percent fewer generations on average.
Our results establish a principled bridge between optimal stopping theory and
inference-time scaling, providing both theoretical performance bounds and
practical efficiency gains for LLM deployment.

</details>


### [62] [How Well Can Preference Optimization Generalize Under Noisy Feedback?](https://arxiv.org/abs/2510.01458)
*Shawn Im,Yixuan Li*

Main category: cs.LG

TL;DR: 该论文研究了噪声反馈对LLM偏好优化的影响，提供了在噪声条件下的泛化保证，分析了不同噪声类型和噪声率对泛化的影响，并验证了DPO、IPO、SLiC等优化方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法假设无噪声反馈，但现实中人类判断存在错误和不一致性，因此需要研究噪声反馈对模型对齐的影响。

Method: 考虑常见的现实噪声模型（如错误标记和不确定性），分析有限步偏好优化，研究泛化在不同噪声类型和噪声率下的衰减情况。

Result: 实证验证表明分析结果具有实际相关性，为开发符合人类偏好的AI系统提供了有价值的见解。

Conclusion: 噪声反馈显著影响偏好优化的泛化性能，需要在实际LLM训练中考虑噪声因素。

Abstract: As large language models (LLMs) advance their capabilities, aligning these
models with human preferences has become crucial. Preference optimization,
which trains models to distinguish between preferred and non-preferred
responses based on human feedback, has become a crucial component for aligning
LLMs. However, most existing works assume noise-free feedback, which is
unrealistic due to the inherent errors and inconsistencies in human judgments.
This paper addresses the impact of noisy feedback on preference optimization,
providing generalization guarantees under these conditions. In particular, we
consider noise models that correspond to common real-world sources of noise,
such as mislabeling and uncertainty. Unlike traditional analyses that assume
convergence, our work focuses on finite-step preference optimization, offering
new insights that are more aligned with practical LLM training. We describe how
generalization decays with different types of noise across levels of noise
rates based on the preference data distribution and number of samples. Our
analysis for noisy preference learning applies to a broad family of preference
optimization losses such as DPO, IPO, SLiC, etc. Empirical validation on
contemporary LLMs confirms the practical relevance of our findings, offering
valuable insights for developing AI systems that align with human preferences.

</details>


### [63] [The Three Regimes of Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2510.01460)
*Lu Li,Tianwei Ni,Yihao Sun,Pierre-Luc Bacon*

Main category: cs.LG

TL;DR: 提出了一个稳定性-可塑性原则来解释离线到在线强化学习中的不一致性，强调在在线微调期间应保留预训练策略或离线数据集中更好的知识，同时保持足够的可塑性。


<details>
  <summary>Details</summary>
Motivation: 离线到在线强化学习的经验行为高度不一致，在一种设置中有效的在线微调设计选择在另一种设置中可能完全失败，需要理论框架来解释这种现象。

Method: 提出了稳定性-可塑性原则，识别了三种在线微调机制，每种机制需要不同的稳定性特性，并通过大规模实证研究验证该框架。

Result: 在63个案例中的45个案例中，实证结果与该框架的预测高度一致，验证了该原则的有效性。

Conclusion: 该工作提供了一个基于离线数据集和预训练策略相对性能的离线到在线强化学习设计选择的原则性框架。

Abstract: Offline-to-online reinforcement learning (RL) has emerged as a practical
paradigm that leverages offline datasets for pretraining and online
interactions for fine-tuning. However, its empirical behavior is highly
inconsistent: design choices of online-fine tuning that work well in one
setting can fail completely in another. We propose a stability--plasticity
principle that can explain this inconsistency: we should preserve the knowledge
of pretrained policy or offline dataset during online fine-tuning, whichever is
better, while maintaining sufficient plasticity. This perspective identifies
three regimes of online fine-tuning, each requiring distinct stability
properties. We validate this framework through a large-scale empirical study,
finding that the results strongly align with its predictions in 45 of 63 cases.
This work provides a principled framework for guiding design choices in
offline-to-online RL based on the relative performance of the offline dataset
and the pretrained policy.

</details>


### [64] [Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets](https://arxiv.org/abs/2510.01479)
*Shriram Karpoora Sundara Pandian,Ali Baheri*

Main category: cs.LG

TL;DR: 提出密度比率加权行为克隆(Weighted BC)，使用小型干净参考集通过二元判别器估计轨迹级密度比率，在BC目标中加权以优先处理干净专家行为，无需知道污染机制。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习数据集常被对抗性污染、系统错误或低质量样本污染，导致标准行为克隆和离线RL方法性能下降。

Method: 使用小型干净参考集训练二元判别器估计轨迹级密度比率，将比率裁剪后作为权重用于BC目标，优先处理干净数据。

Result: 在连续控制基准测试中，即使在高污染率下也能保持接近最优性能，优于传统BC、BCQ和BRAC等基线方法。

Conclusion: Weighted BC提供了一种鲁棒的模仿学习方法，具有理论保证，能在数据污染情况下有效学习专家策略。

Abstract: Offline reinforcement learning (RL) enables policy optimization from fixed
datasets, making it suitable for safety-critical applications where online
exploration is infeasible. However, these datasets are often contaminated by
adversarial poisoning, system errors, or low-quality samples, leading to
degraded policy performance in standard behavioral cloning (BC) and offline RL
methods. This paper introduces Density-Ratio Weighted Behavioral Cloning
(Weighted BC), a robust imitation learning approach that uses a small, verified
clean reference set to estimate trajectory-level density ratios via a binary
discriminator. These ratios are clipped and used as weights in the BC objective
to prioritize clean expert behavior while down-weighting or discarding
corrupted data, without requiring knowledge of the contamination mechanism. We
establish theoretical guarantees showing convergence to the clean expert policy
with finite-sample bounds that are independent of the contamination rate. A
comprehensive evaluation framework is established, which incorporates various
poisoning protocols (reward, state, transition, and action) on continuous
control benchmarks. Experiments demonstrate that Weighted BC maintains
near-optimal performance even at high contamination ratios outperforming
baselines such as traditional BC, batch-constrained Q-learning (BCQ) and
behavior regularized actor-critic (BRAC).

</details>


### [65] [Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information](https://arxiv.org/abs/2510.01499)
*Rui Ai,Yuqi Pan,David Simchi-Levi,Milind Tambe,Haifeng Xu*

Main category: cs.LG

TL;DR: 提出了两种新的多智能体LLM答案聚合算法：最优权重(OW)和逆意外流行度(ISP)，利用一阶和二阶信息改进多数投票方法。


<details>
  <summary>Details</summary>
Motivation: 标准多数投票方法平等对待所有答案，未能考虑模型间的潜在异质性和相关性，需要更有效的聚合方法来提高集体决策的可靠性。

Method: 设计了OW和ISP两种算法，利用一阶和二阶信息进行答案聚合，通过理论分析证明这些方法在温和假设下能够缓解多数投票的固有局限性。

Result: 在合成数据集、UltraFeedback和MMLU等LLM微调基准以及ARMMAN真实医疗场景中，新方法始终优于多数投票。

Conclusion: 提出的方法不仅提供了实际性能提升，还为设计鲁棒的多智能体LLM流水线提供了概念性见解。

Abstract: With the rapid progress of multi-agent large language model (LLM) reasoning,
how to effectively aggregate answers from multiple LLMs has emerged as a
fundamental challenge. Standard majority voting treats all answers equally,
failing to consider latent heterogeneity and correlation across models. In this
work, we design two new aggregation algorithms called Optimal Weight (OW) and
Inverse Surprising Popularity (ISP), leveraging both first-order and
second-order information. Our theoretical analysis shows these methods provably
mitigate inherent limitations of majority voting under mild assumptions,
leading to more reliable collective decisions. We empirically validate our
algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as
UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all
cases, our methods consistently outperform majority voting, offering both
practical performance gains and conceptual insights for the design of robust
multi-agent LLM pipelines.

</details>


### [66] [Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code](https://arxiv.org/abs/2510.01539)
*Aniket Vashishtha,Qirun Dai,Hongyuan Mei,Amit Sharma,Chenhao Tan,Hao Peng*

Main category: cs.LG

TL;DR: 提出了可执行反事实框架，通过代码和数学问题评估LLM的反事实推理能力，发现从干预推理到反事实推理准确率下降25-40%，强化学习相比监督微调能更好地泛化到新领域。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM反事实推理能力的方法往往跳过溯因步骤，导致高估模型性能，需要开发能完整评估三个推理步骤的框架。

Method: 引入可执行反事实框架，通过代码和数学问题操作化因果推理，要求完整的溯因、干预和预测三个步骤，并创建可扩展的合成数据。

Result: SOTA模型从干预推理到反事实推理准确率下降25-40%；强化学习在代码和数学问题上表现优于监督微调，泛化能力更强。

Conclusion: 强化学习能诱导核心认知行为并泛化到新领域，在提升LLM反事实推理能力方面具有潜力。

Abstract: Counterfactual reasoning, a hallmark of intelligence, consists of three
steps: inferring latent variables from observations (abduction), constructing
alternatives (interventions), and predicting their outcomes (prediction). This
skill is essential for advancing LLMs' causal understanding and expanding their
applications in high-stakes domains such as scientific research. However,
existing efforts in assessing LLM's counterfactual reasoning capabilities tend
to skip the abduction step, effectively reducing to interventional reasoning
and leading to overestimation of LLM performance. To address this, we introduce
executable counterfactuals, a novel framework that operationalizes causal
reasoning through code and math problems. Our framework explicitly requires all
three steps of counterfactual reasoning and enables scalable synthetic data
creation with varying difficulty, creating a frontier for evaluating and
improving LLM's reasoning. Our results reveal substantial drop in accuracy
(25-40%) from interventional to counterfactual reasoning for SOTA models like
o4-mini and Claude-4-Sonnet. To address this gap, we construct a training set
comprising counterfactual code problems having if-else condition and test on
out-of-domain code structures (e.g. having while-loop); we also test whether a
model trained on code would generalize to counterfactual math word problems.
While supervised finetuning on stronger models' reasoning traces improves
in-domain performance of Qwen models, it leads to a decrease in accuracy on OOD
tasks such as counterfactual math problems. In contrast, reinforcement learning
induces the core cognitive behaviors and generalizes to new domains, yielding
gains over the base model on both code (improvement of 1.5x-2x) and math
problems. Analysis of the reasoning traces reinforces these findings and
highlights the promise of RL for improving LLMs' counterfactual reasoning.

</details>


### [67] [Predictive Preference Learning from Human Interventions](https://arxiv.org/abs/2510.01545)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.LG

TL;DR: 提出预测性偏好学习（PPL）方法，通过人类干预的隐式偏好信号来预测未来状态，将单次干预扩展到未来L个时间步，从而提高学习效率和减少人类演示需求。


<details>
  <summary>Details</summary>
Motivation: 现有交互式模仿学习方法只修正当前状态的动作，不调整未来可能更危险状态的动作，需要更有效利用人类干预来预防未来风险。

Method: PPL将每次人类干预引导到未来L个时间步（偏好视野），假设智能体执行相同动作且人类进行相同干预，通过偏好优化将这些专家修正传播到安全关键区域。

Result: 在自动驾驶和机器人操作基准测试中验证了方法的效率和通用性，显著提高了学习效率并减少了人类演示需求。

Conclusion: 选择合适的偏好视野L可以在风险状态覆盖和标签正确性之间取得平衡，从而限制算法最优性差距。

Abstract: Learning from human involvement aims to incorporate the human subject to
monitor and correct agent behavior errors. Although most interactive imitation
learning methods focus on correcting the agent's action at the current state,
they do not adjust its actions in future states, which may be potentially more
hazardous. To address this, we introduce Predictive Preference Learning from
Human Interventions (PPL), which leverages the implicit preference signals
contained in human interventions to inform predictions of future rollouts. The
key idea of PPL is to bootstrap each human intervention into L future time
steps, called the preference horizon, with the assumption that the agent
follows the same action and the human makes the same intervention in the
preference horizon. By applying preference optimization on these future states,
expert corrections are propagated into the safety-critical regions where the
agent is expected to explore, significantly improving learning efficiency and
reducing human demonstrations needed. We evaluate our approach with experiments
on both autonomous driving and robotic manipulation benchmarks and demonstrate
its efficiency and generality. Our theoretical analysis further shows that
selecting an appropriate preference horizon L balances coverage of risky states
with label correctness, thereby bounding the algorithmic optimality gap. Demo
and code are available at: https://metadriverse.github.io/ppl

</details>


### [68] [From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?](https://arxiv.org/abs/2510.01571)
*Hanqun Cao,Hongrui Zhang,Junde Xu,Zhou Zhang,Lingdong Shen,Minghao Sun,Ge Liu,Jinbo Xu,Wu-Jun Li,Jinren Ni,Cesar de la Fuente-Nunez,Tianfan Fu,Yejin Choi,Pheng-Ann Heng,Fang Wu*

Main category: cs.LG

TL;DR: 该研究探讨了强化学习（RL）能否推动蛋白质语言模型（PLMs）超越其预训练先验，揭示序列-结构-功能规则。通过在四个蛋白质设计领域结合RL与PLMs，发现RL能持续提高成功率和采样效率，性能提升取决于任务空间、奖励保真度和策略容量的三因素交互作用。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索RL是否能帮助PLMs突破预训练先验的限制，发现监督学习未能捕捉的蛋白质序列-结构-功能关系，推动蛋白质设计的探索和优化。

Method: 在四个蛋白质设计领域（抗菌肽设计、激酶变体优化、抗体工程和逆折叠）中，将多种RL算法与不同类别的PLMs配对使用，比较RL与监督学习的表现差异。

Result: 在所有基准测试中，RL持续提高了成功率和采样效率。性能提升遵循三因素交互作用：当奖励准确且信息丰富、策略容量充足、任务有超越监督基线的空间时，改进显著；当奖励噪声大或容量受限时，增益饱和。

Conclusion: 研究为蛋白质设计中的RL应用提供了实用指导：优先进行奖励建模和校准，再扩展策略规模；根据任务难度匹配算法和正则化强度；在边际收益最大的地方分配容量。

Abstract: Protein language models (PLMs) have advanced computational protein science
through large-scale pretraining and scalable architectures. In parallel,
reinforcement learning (RL) has broadened exploration and enabled precise
multi-objective optimization in protein design. Yet whether RL can push PLMs
beyond their pretraining priors to uncover latent sequence-structure-function
rules remains unclear. We address this by pairing RL with PLMs across four
domains: antimicrobial peptide design, kinase variant optimization, antibody
engineering, and inverse folding. Using diverse RL algorithms and model
classes, we ask if RL improves sampling efficiency and, more importantly, if it
reveals capabilities not captured by supervised learning. Across benchmarks, RL
consistently boosts success rates and sample efficiency. Performance follows a
three-factor interaction: task headroom, reward fidelity, and policy capacity
jointly determine gains. When rewards are accurate and informative, policies
have sufficient capacity, and tasks leave room beyond supervised baselines,
improvements scale; when rewards are noisy or capacity is constrained, gains
saturate despite exploration. This view yields practical guidance for RL in
protein design: prioritize reward modeling and calibration before scaling
policy size, match algorithm and regularization strength to task difficulty,
and allocate capacity where marginal gains are largest. Implementation is
available at https://github.com/chq1155/RL-PLM.

</details>


### [69] [Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression](https://arxiv.org/abs/2510.01581)
*Joykirat Singh,Justin Chih-Yao Chen,Archiki Prasad,Elias Stengel-Eskin,Akshay Nambi,Mohit Bansal*

Main category: cs.LG

TL;DR: TRAAC是一种在线后训练强化学习方法，通过自适应压缩推理步骤来解决模型在复杂推理任务中的欠适应性问题，平衡欠思考和过度思考。


<details>
  <summary>Details</summary>
Motivation: 现有思维模型通过扩展测试时计算来解决复杂推理任务，但需要根据任务难度分配计算资源。欠思考会导致困难问题出错，而过度思考则会产生不必要的推理步骤，造成令牌效率低下。

Method: 提出TRAAC方法，利用模型在长推理轨迹上的自注意力机制识别重要步骤并剪枝冗余步骤，同时估计任务难度并将其纳入训练奖励，学习根据示例难度分配推理预算。

Result: 在多个任务上（AIME、AMC、GPQA-D、BBEH），TRAAC（Qwen3-4B）相比基础模型平均绝对准确率提升8.4%，推理长度相对减少36.8%；相比最佳RL基线准确率提升7.9%，长度减少29.4%。

Conclusion: TRAAC能够根据难度进行细粒度的思维预算调整，任务难度校准和基于注意力的压缩相结合在不同任务上都能带来收益，并展现出良好的泛化能力。

Abstract: Recent thinking models solve complex reasoning tasks by scaling test-time
compute, but this scaling must be allocated in line with task difficulty. On
one hand, short reasoning (underthinking) leads to errors on harder problems
that require extended reasoning steps; but, excessively long reasoning
(overthinking) can be token-inefficient, generating unnecessary steps even
after reaching a correct intermediate solution. We refer to this as
under-adaptivity, where the model fails to modulate its response length
appropriately given problems of varying difficulty. To address under-adaptivity
and strike a balance between under- and overthinking, we propose TRAAC (Think
Right with Adaptive, Attentive Compression), an online post-training RL method
that leverages the model's self-attention over a long reasoning trajectory to
identify important steps and prune redundant ones. TRAAC also estimates
difficulty and incorporates it into training rewards, thereby learning to
allocate reasoning budget commensurate with example difficulty. Our approach
improves accuracy, reduces reasoning steps, and enables adaptive thinking
compared to base models and other RL baselines. Across a variety of tasks
(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute
accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%
compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length
drop compared to the best RL baseline. TRAAC also shows strong generalization:
although our models are trained on math datasets, they show accuracy and
efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,
and OptimalThinkingBench. Our analysis further verifies that TRAAC provides
fine-grained adjustments to thinking budget based on difficulty and that a
combination of task-difficulty calibration and attention-based compression
yields gains across diverse tasks.

</details>


### [70] [Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead](https://arxiv.org/abs/2510.01624)
*Feiyang Kang,Michael Kuchnik,Karthik Padthe,Marin Vlastelica,Ruoxi Jia,Carole-Jean Wu,Newsha Ardalani*

Main category: cs.LG

TL;DR: 研究发现SFT高分并不总能转化为RL后的性能提升，有时甚至会导致更差结果。提出了泛化损失和Pass@large k作为更好的RL结果预测指标。


<details>
  <summary>Details</summary>
Motivation: 挑战当前LLM后训练中SFT和RL两阶段独立训练的做法，质疑高SFT分数是否能可靠预测RL后的性能提升。

Method: 训练数百个12B参数模型，在7个数学基准上进行广泛评估，使用GRPO进行RLVR训练，花费超过100万GPU小时。

Result: 发现基于泛化损失和Pass@large k的预测比直接使用SFT性能预测更准确，R²和Spearman相关系数提升高达0.5（2倍）。

Conclusion: SFT性能不是RL结果的可靠预测指标，泛化损失和Pass@large k提供了更好的预测能力。

Abstract: In post-training for reasoning Large Language Models (LLMs), the current
state of practice trains LLMs in two independent stages: Supervised Fine-Tuning
(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as
``RL'' below). In this work, we challenge whether high SFT scores translate to
improved performance after RL. We provide extensive counter-examples where this
is not true. We find high SFT scores can be biased toward simpler or more
homogeneous data and are not reliably predictive of subsequent RL gains or
scaled-up post-training effectiveness. In some cases, RL training on models
with improved SFT performance could lead to substantially worse outcome
compared to RL on the base model without SFT. We study alternative metrics and
identify generalization loss on held-out reasoning examples and Pass@large k
performance to provide strong proxies for the RL outcome. We trained hundreds
of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive
evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU
hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple
state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL
performance, prediction based on generalization loss and Pass@large k achieves
substantial higher precision, improving $R^2$ coefficient and Spearman's rank
correlation coefficient by up to 0.5 (2x). This provides strong utility for
broad use cases. For example, in most experiments, we find SFT training on
unique examples for a one epoch underperforms training on half examples for two
epochs, either after SFT or SFT-then-RL; With the same SFT budget, training
only on short examples may lead to better SFT performance, though, it often
leads to worse outcome after RL compared to training on examples with varying
lengths. Evaluation tool will be open-sourced.

</details>


### [71] [Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning](https://arxiv.org/abs/2510.01656)
*Jiashun Liu,Johan Obando-Ceron,Han Lu,Yancheng He,Weixun Wang,Wenbo Su,Bo Zheng,Pablo Samuel Castro,Aaron Courville,Ling Pan*

Main category: cs.LG

TL;DR: 提出了AsyPPO框架，通过轻量级mini-critics集合恢复critic角色，解决LLM规模RL中传统价值函数训练成本高的问题，在多个基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前RL4LLM方法避免使用显式critic，主要因为传统价值函数在LLM规模下计算成本高，且在稀疏奖励和长推理视野下容易失效。

Method: 使用一组轻量级mini-critics，每个在不相交的提示分片上训练，鼓励多样性同时保持校准；利用critic间不确定性来优化策略更新，包括在critic一致时屏蔽优势值，以及从熵正则化中过滤高分歧状态。

Result: 在仅5000个样本上训练后，AsyPPO在多个基准上持续提升学习稳定性和性能，相比GRPO等强基线，在Qwen3-4b-Base上获得超过6%的性能提升，在Qwen3-8b-Base和Qwen3-14b-Base上获得约3%的提升。

Conclusion: 架构创新对于可扩展、高效算法的重要性，AsyPPO成功恢复了critic在RL4LLM中的作用，同时保持效率。

Abstract: Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing
them with average advantage baselines. This shift is largely pragmatic:
conventional value functions are computationally expensive to train at LLM
scale and often fail under sparse rewards and long reasoning horizons. We
revisit this bottleneck from an architectural perspective and introduce
Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable
framework that restores the critics role while remaining efficient in
large-model settings. AsyPPO employs a set of lightweight mini-critics, each
trained on disjoint prompt shards. This design encourages diversity while
preserving calibration, reducing value-estimation bias. Beyond robust
estimation, AsyPPO leverages inter-critic uncertainty to refine the policy
update: (i) masking advantages in states where critics agree and gradients add
little learning signal, and (ii) filtering high-divergence states from entropy
regularization, suppressing spurious exploration. After training on open-source
data with only 5,000 samples, AsyPPO consistently improves learning stability
and performance across multiple benchmarks over strong baselines, such as GRPO,
achieving performance gains of more than six percent on Qwen3-4b-Base and about
three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without
additional tricks. These results highlight the importance of architectural
innovations for scalable, efficient algorithms.

</details>


### [72] [Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation](https://arxiv.org/abs/2510.01721)
*Saptarshi Mandal,Yashaswini Murthy,R. Srikant*

Main category: cs.LG

TL;DR: 提出了首个具有线性函数逼近的鲁棒TD学习算法，解决了在分布不确定性下的强化学习问题，无需生成模型访问且模型无关。


<details>
  <summary>Details</summary>
Motivation: 现有的鲁棒TD学习收敛保证仅限于表格MDP或依赖于限制性折扣因子假设，需要填补鲁棒RL算法实证成功与非鲁棒对应物非渐近保证之间的关键差距。

Method: 结合双时间尺度随机逼近更新和外循环目标网络更新，使用总变差距离和Wasserstein-l距离不确定性集来衡量鲁棒性。

Result: 建立了Õ(1/ε²)的样本复杂度以获得ε-准确的值估计，关键思想也可相对直接地扩展到具有函数逼近的鲁棒Q学习。

Conclusion: 填补了鲁棒RL算法实证成功与非鲁棒对应物理论保证之间的关键差距，为分布鲁棒强化学习提供了有效的函数逼近解决方案。

Abstract: Distributionally robust reinforcement learning (DRRL) focuses on designing
policies that achieve good performance under model uncertainties. In
particular, we are interested in maximizing the worst-case long-term discounted
reward, where the data for RL comes from a nominal model while the deployed
environment can deviate from the nominal model within a prescribed uncertainty
set. Existing convergence guarantees for robust temporal-difference (TD)
learning for policy evaluation are limited to tabular MDPs or are dependent on
restrictive discount-factor assumptions when function approximation is used. We
present the first robust TD learning with linear function approximation, where
robustness is measured with respect to the total-variation distance and
Wasserstein-l distance uncertainty set. Additionally, our algorithm is both
model-free and does not require generative access to the MDP. Our algorithm
combines a two-time-scale stochastic-approximation update with an outer-loop
target-network update. We establish an $\tilde{O}(1/\epsilon^2)$ sample
complexity to obtain an $\epsilon$-accurate value estimate. Our results close a
key gap between the empirical success of robust RL algorithms and the
non-asymptotic guarantees enjoyed by their non-robust counterparts. The key
ideas in the paper also extend in a relatively straightforward fashion to
robust Q-learning with function approximation.

</details>


### [73] [Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX](https://arxiv.org/abs/2510.01764)
*Waris Radji,Thomas Michel,Hector Piteau*

Main category: cs.LG

TL;DR: Octax是一个基于JAX的高性能经典街机游戏环境套件，为强化学习研究提供GPU加速的Atari基准替代方案，实现数量级的速度提升。


<details>
  <summary>Details</summary>
Motivation: 强化学习研究需要多样化、具有挑战性且可扩展的环境。现有视频游戏环境计算成本高且受CPU限制，不适合大规模实验。

Method: 基于CHIP-8模拟器在JAX中实现经典街机游戏环境，提供端到端的GPU加速，支持图像环境并涵盖解谜、动作和策略等多种游戏类型。

Result: JAX实现相比传统CPU模拟器实现数量级的速度提升，同时保持对原始游戏机制的完美保真度。在多个游戏中训练强化学习代理显示训练速度和可扩展性的显著改善。

Conclusion: Octax为大规模强化学习实验提供了理想的平台，其模块化设计便于研究人员扩展新游戏或使用大语言模型生成新环境。

Abstract: Reinforcement learning (RL) research requires diverse, challenging
environments that are both tractable and scalable. While modern video games may
offer rich dynamics, they are computationally expensive and poorly suited for
large-scale experimentation due to their CPU-bound execution. We introduce
Octax, a high-performance suite of classic arcade game environments implemented
in JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely
adopted as a benchmark in RL research. Octax provides the JAX community with a
long-awaited end-to-end GPU alternative to the Atari benchmark, offering
image-based environments, spanning puzzle, action, and strategy genres, all
executable at massive scale on modern GPUs. Our JAX-based implementation
achieves orders-of-magnitude speedups over traditional CPU emulators while
maintaining perfect fidelity to the original game mechanics. We demonstrate
Octax's capabilities by training RL agents across multiple games, showing
significant improvements in training speed and scalability compared to existing
solutions. The environment's modular design enables researchers to easily
extend the suite with new games or generate novel environments using large
language models, making it an ideal platform for large-scale RL
experimentation.

</details>


### [74] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: 提出了Granular-GRPO框架，通过奇异随机采样策略和多粒度优势集成模块，解决现有方法在扩散和流模型强化学习中奖励信号稀疏和狭窄的问题，实现更精确全面的采样方向评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索高价值样本方面有效，但由于稀疏和狭窄的奖励信号导致偏好对齐效果不佳，需要更精确和全面的奖励评估机制。

Method: 使用奇异随机采样策略支持逐步随机探索，确保奖励与注入噪声高度相关；引入多粒度优势集成模块聚合多个扩散尺度的优势，提供更全面稳健的采样方向评估。

Result: 在各种奖励模型上的实验表明，G²RPO显著优于现有的基于流的GRPO基线方法，证明了其有效性和鲁棒性。

Conclusion: G²RPO框架通过改进的采样策略和多粒度评估机制，在扩散和流模型的强化学习中实现了更好的偏好对齐性能。

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow
models has recently emerged as a promising approach for aligning generative
models with human preferences. Stochastic sampling via Stochastic Differential
Equations (SDE) is employed during the denoising process to generate diverse
denoising directions for RL exploration. While existing methods effectively
explore potential high-value samples, they suffer from sub-optimal preference
alignment due to sparse and narrow reward signals. To address these challenges,
we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves
precise and comprehensive reward assessments of sampling directions in
reinforcement learning of flow models. Specifically, a Singular Stochastic
Sampling strategy is introduced to support step-wise stochastic exploration
while enforcing a high correlation between the reward and the injected noise,
thereby facilitating a faithful reward for each SDE perturbation. Concurrently,
to eliminate the bias inherent in fixed-granularity denoising, we introduce a
Multi-Granularity Advantage Integration module that aggregates advantages
computed at multiple diffusion scales, producing a more comprehensive and
robust evaluation of the sampling directions. Experiments conducted on various
reward models, including both in-domain and out-of-domain evaluations,
demonstrate that our $\text{G}^2$RPO significantly outperforms existing
flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [75] [ExGRPO: Learning to Reason from Experience](https://arxiv.org/abs/2510.02245)
*Runzhe Zhan,Yafu Li,Zhi Wang,Xiaoye Qu,Dongrui Liu,Jing Shao,Derek F. Wong,Yu Cheng*

Main category: cs.LG

TL;DR: 该论文提出了ExGRPO框架，通过识别推理经验的价值指标（正确性和熵），组织并优先处理有价值的经验，使用混合策略目标平衡探索与经验利用，在数学/通用基准上显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 标准的在线策略训练在单次更新后丢弃经验，导致计算效率低下和不稳定性。虽然先前工作强调了重用过去经验的好处，但经验特性在大型推理模型学习动态中的作用仍未充分探索。

Method: 提出ExGRPO框架：识别推理经验的价值指标（正确性和熵），组织并优先处理有价值的经验，使用混合策略目标平衡探索与经验利用。

Result: 在五个骨干模型（1.5B-8B参数）上，ExGRPO在数学/通用基准上平均比在线策略RLVR提升+3.5/7.6分，并在强弱模型上都能稳定训练。

Conclusion: 原则性的经验管理是高效和可扩展RLVR的关键要素。

Abstract: Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm
for improving the reasoning ability of large language models. However, standard
on-policy training discards rollout experiences after a single update, leading
to computational inefficiency and instability. While prior work on RL has
highlighted the benefits of reusing past experience, the role of experience
characteristics in shaping learning dynamics of large reasoning models remains
underexplored. In this paper, we are the first to investigate what makes a
reasoning experience valuable and identify rollout correctness and entropy as
effective indicators of experience value. Based on these insights, we propose
ExGRPO (Experiential Group Relative Policy Optimization), a framework that
organizes and prioritizes valuable experiences, and employs a mixed-policy
objective to balance exploration with experience exploitation. Experiments on
five backbone models (1.5B-8B parameters) show that ExGRPO consistently
improves reasoning performance on mathematical/general benchmarks, with an
average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO
stabilizes training on both stronger and weaker models where on-policy methods
fail. These results highlight principled experience management as a key
ingredient for efficient and scalable RLVR.

</details>


### [76] [GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning](https://arxiv.org/abs/2510.02180)
*Silvia Sapora,Devon Hjelm,Alexander Toshev,Omar Attia,Bogdan Mazoure*

Main category: cs.LG

TL;DR: GRACE使用大语言模型和进化搜索从专家轨迹中逆向工程可解释的代码化奖励函数，解决了传统逆强化学习方法产生的"黑盒"模型问题。


<details>
  <summary>Details</summary>
Motivation: 传统逆强化学习方法产生难以解释和调试的"黑盒"奖励模型，需要开发能够生成可检查、可验证的可解释奖励函数的方法。

Method: 使用大语言模型在进化搜索框架内，直接从专家轨迹逆向工程生成代码化的奖励函数，产生可执行的代码。

Result: 在BabyAI和AndroidWorld基准测试中，GRACE高效学习了高精度奖励，在复杂多任务设置中表现良好，产生的奖励函数能生成强策略，性能优于模仿学习和使用真实奖励的在线强化学习方法。

Conclusion: GRACE能够生成可解释的代码化奖励函数，在多任务设置中构建复杂奖励API，为逆强化学习提供了透明且可验证的解决方案。

Abstract: Inverse Reinforcement Learning aims to recover reward models from expert
demonstrations, but traditional methods yield "black-box" models that are
difficult to interpret and debug. In this work, we introduce GRACE (Generating
Rewards As CodE), a method for using Large Language Models within an
evolutionary search to reverse-engineer an interpretable, code-based reward
function directly from expert trajectories. The resulting reward function is
executable code that can be inspected and verified. We empirically validate
GRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns
highly accurate rewards, even in complex, multi-task settings. Further, we
demonstrate that the resulting reward leads to strong policies, compared to
both competitive Imitation Learning and online RL approaches with ground-truth
rewards. Finally, we show that GRACE is able to build complex reward APIs in
multi-task setups.

</details>


### [77] [DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning](https://arxiv.org/abs/2510.02212)
*Hanyang Zhao,Dawen Liang,Wenpin Tang,David Yao,Nathan Kallus*

Main category: cs.LG

TL;DR: DiFFPO是一个统一的训练框架，通过强化学习训练掩码扩散大语言模型，使其推理既更好（furious）又更快。该方法结合了替代策略训练和联合训练高效采样器，在数学和规划任务上实现了更好的准确性和更低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练扩散大语言模型时存在推理速度慢、计算成本高的问题。作者希望开发一个框架，既能提升模型推理质量，又能显著提高推理效率。

Method: 1. 使用离策略强化学习训练替代策略，提出两阶段似然近似结合重要性采样校正；2. 联合训练高效采样器/控制器，让模型学习自适应地为每个提示分配推理阈值，利用多token预测能力。

Result: 在数学和规划基准任务上，DiFFPO相比仅训练模型的方法，在更少的函数评估次数下获得了更好的准确性，显著改善了推理时计算量的帕累托边界。

Conclusion: DiFFPO框架通过强化学习和联合训练采样器，成功提升了扩散大语言模型的推理质量和效率，为高效推理提供了新的解决方案。

Abstract: We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified
framework for training masked diffusion large language models (dLLMs) to reason
not only better (furious), but also faster via reinforcement learning (RL). We
first unify the existing baseline approach such as d1 by proposing to train
surrogate policies via off-policy RL, whose likelihood is much more tractable
as an approximation to the true dLLM policy. This naturally motivates a more
accurate and informative two-stage likelihood approximation combined with
importance sampling correction, which leads to generalized RL algorithms with
better sample efficiency and superior task performance. Second, we propose a
new direction of joint training efficient samplers/controllers of dLLMs policy.
Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by
letting the model learn to adaptively allocate an inference threshold for each
prompt. By jointly training the sampler, we yield better accuracies with lower
number of function evaluations (NFEs) compared to training the model only,
obtaining the best performance in improving the Pareto frontier of the
inference-time compute of dLLMs. We showcase the effectiveness of our pipeline
by training open source large diffusion language models over benchmark math and
planning tasks.

</details>
