<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.AI](#cs.AI) [Total: 11]
- [tldr.article](#tldr.article) [Total: 25]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Lowest Span Confidence: A Zero-Shot Metric for Efficient and Black-Box Hallucination Detection in LLMs](https://arxiv.org/abs/2601.19918)
*Yitong Qiao,Licheng Pan,Yu Mi,Lei Liu,Yue Shen,Fei Sun,Zhixuan Chu*

Main category: cs.CL

TL;DR: 提出一种名为LSC（最低跨度置信度）的新型零样本幻觉检测指标，仅需单次前向传播和输出概率，通过滑动窗口机制评估语义连贯跨度的联合似然，在资源受限条件下实现高效幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 大语言模型中的幻觉问题（生成看似合理但非事实内容）在高风险部署环境中构成重大挑战。现有幻觉检测方法通常需要昂贵的密集采样策略或白盒模型状态，这在常见的API场景中不可用或效率低下。

Method: 提出LSC（最低跨度置信度）指标，仅需单次前向传播和输出概率。通过滑动窗口机制评估语义连贯跨度的联合似然，识别可变长度n-gram中最低边际置信度区域，捕捉与事实不一致性高度相关的局部不确定性模式。

Result: 在多个最先进LLM和多样化基准测试上的广泛实验表明，LSC始终优于现有零样本基线方法，即使在资源受限条件下也能提供强大的检测性能。LSC能够缓解困惑度的稀释效应和最小标记概率的噪声敏感性，提供更稳健的事实不确定性估计。

Conclusion: LSC是一种高效、资源友好的幻觉检测方法，仅需最小资源假设（单次前向传播），在API场景中具有实用价值，为可靠部署LLM提供了有效的幻觉检测解决方案。

Abstract: Hallucinations in Large Language Models (LLMs), i.e., the tendency to generate plausible but non-factual content, pose a significant challenge for their reliable deployment in high-stakes environments. However, existing hallucination detection methods generally operate under unrealistic assumptions, i.e., either requiring expensive intensive sampling strategies for consistency checks or white-box LLM states, which are unavailable or inefficient in common API-based scenarios. To this end, we propose a novel efficient zero-shot metric called Lowest Span Confidence (LSC) for hallucination detection under minimal resource assumptions, only requiring a single forward with output probabilities. Concretely, LSC evaluates the joint likelihood of semantically coherent spans via a sliding window mechanism. By identifying regions of lowest marginal confidence across variable-length n-grams, LSC could well capture local uncertainty patterns strongly correlated with factual inconsistency. Importantly, LSC can mitigate the dilution effect of perplexity and the noise sensitivity of minimum token probability, offering a more robust estimate of factual uncertainty. Extensive experiments across multiple state-of-the-art (SOTA) LLMs and diverse benchmarks show that LSC consistently outperforms existing zero-shot baselines, delivering strong detection performance even under resource-constrained conditions.

</details>


### [2] [Demystifying Multi-Agent Debate: The Role of Confidence and Diversity](https://arxiv.org/abs/2601.19921)
*Xiaochen Zhu,Caiqi Zhang,Yizhou Chi,Tom Stafford,Nigel Collier,Andreas Vlachos*

Main category: cs.CL

TL;DR: 该论文提出两种改进多智能体辩论的方法：多样性感知初始化和置信度调制辩论协议，以解决传统MAD性能不足的问题，在六个推理问答基准上显著优于传统MAD和多数投票。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论(MAD)虽然通过测试时扩展来提升大语言模型性能，但研究发现其表现常常不如简单的多数投票，且计算成本更高。研究表明在智能体同质化和均匀信念更新的情况下，辩论只能保持期望正确性而无法可靠改善结果。

Method: 提出两种轻量级干预措施：1) 多样性感知初始化：选择更多样化的候选答案池，增加辩论开始时正确假设存在的可能性；2) 置信度调制辩论协议：智能体表达校准后的置信度，并根据他人的置信度条件化更新自己的信念。

Result: 理论分析表明多样性感知初始化提高了MAD成功的先验概率而不改变底层更新动态，置信度调制更新使辩论能够系统性地向正确假设漂移。在六个推理导向的问答基准上，该方法持续优于传统MAD和多数投票。

Conclusion: 研究将人类审议与基于LLM的辩论联系起来，证明简单、有原则的修改可以显著提升辩论效果。多样性初始化和置信度调制是实现有效多智能体辩论的关键机制。

Abstract: Multi-agent debate (MAD) is widely used to improve large language model (LLM) performance through test-time scaling, yet recent work shows that vanilla MAD often underperforms simple majority vote despite higher computational cost. Studies show that, under homogeneous agents and uniform belief updates, debate preserves expected correctness and therefore cannot reliably improve outcomes. Drawing on findings from human deliberation and collective decision-making, we identify two key mechanisms missing from vanilla MAD: (i) diversity of initial viewpoints and (ii) explicit, calibrated confidence communication. We propose two lightweight interventions. First, a diversity-aware initialisation that selects a more diverse pool of candidate answers, increasing the likelihood that a correct hypothesis is present at the start of debate. Second, a confidence-modulated debate protocol in which agents express calibrated confidence and condition their updates on others' confidence. We show theoretically that diversity-aware initialisation improves the prior probability of MAD success without changing the underlying update dynamics, while confidence-modulated updates enable debate to systematically drift to the correct hypothesis. Empirically, across six reasoning-oriented QA benchmarks, our methods consistently outperform vanilla MAD and majority vote. Our results connect human deliberation with LLM-based debate and demonstrate that simple, principled modifications can substantially enhance debate effectiveness.

</details>


### [3] [Towards a Mechanistic Understanding of Large Reasoning Models: A Survey of Training, Inference, and Failures](https://arxiv.org/abs/2601.19928)
*Yi Hu,Jiaqi Gu,Ruxin Wang,Zijun Yao,Hao Peng,Xiaobao Wu,Jianhui Chen,Muhan Zhang,Liangming Pan*

Main category: cs.CL

TL;DR: 对大型推理模型（LRMs）的机制理解进行全面综述，从训练动态、推理机制和意外行为三个维度组织最新发现，旨在弥合黑盒性能与机制透明度之间的差距，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 强化学习推动了大型推理模型的发展，虽然性能令人兴奋，但理解驱动这些行为的内部机制已成为同等重要的研究前沿。本文旨在填补黑盒性能与机制透明度之间的鸿沟。

Method: 采用系统性文献综述方法，将现有研究成果组织成三个核心维度：1) 训练动态，2) 推理机制，3) 意外行为。通过综合这些见解，构建机制理解的框架。

Result: 提供了对大型推理模型机制理解的全面调查，识别了当前研究的关键发现和知识空白，为理解这些复杂系统的内部工作机制建立了结构化框架。

Conclusion: 需要进一步研究应用可解释性、改进方法论和统一理论框架，以推进对大型推理模型的机制理解，这将是未来重要的研究方向。

Abstract: Reinforcement learning (RL) has catalyzed the emergence of Large Reasoning Models (LRMs) that have pushed reasoning capabilities to new heights. While their performance has garnered significant excitement, exploring the internal mechanisms driving these behaviors has become an equally critical research frontier. This paper provides a comprehensive survey of the mechanistic understanding of LRMs, organizing recent findings into three core dimensions: 1) training dynamics, 2) reasoning mechanisms, and 3) unintended behaviors. By synthesizing these insights, we aim to bridge the gap between black-box performance and mechanistic transparency. Finally, we discuss under-explored challenges to outline a roadmap for future mechanistic studies, including the need for applied interpretability, improved methodologies, and a unified theoretical framework.

</details>


### [4] [Stingy Context: 18:1 Hierarchical Code Compression for LLM Auto-Coding](https://arxiv.org/abs/2601.19929)
*David Linus Ostby*

Main category: cs.CL

TL;DR: Stingy Context提出了一种基于分层树的压缩方案，在自动编码任务中实现18:1的LLM上下文压缩比，显著减少计算成本同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型代码库处理时LLM上下文窗口有限的问题，减少计算成本，同时避免"迷失在中间"效应，提高自动编码任务的效率和可扩展性。

Method: 采用分层树状压缩方案，结合TREEFRAG分解技术，将大型代码库结构化压缩，保留关键代码结构和依赖关系，实现高效的上下文管理。

Result: 在真实代码库（239k tokens）上实现压缩至11k tokens（18:1压缩比），在12个前沿模型上测试40个真实问题，达到94-97%成功率，优于平面压缩方法。

Conclusion: Stingy Context提供了一种高效、可扩展的代码上下文压缩方案，显著降低LLM推理成本，同时保持高任务完成率，为大规模代码处理提供实用解决方案。

Abstract: We introduce Stingy Context, a hierarchical tree-based compression scheme achieving 18:1 reduction in LLM context for auto-coding tasks. Using our TREEFRAG exploit decomposition, we reduce a real source code base of 239k tokens to 11k tokens while preserving task fidelity. Empirical results across 12 Frontier models show 94 to 97% success on 40 real-world issues at low cost, outperforming flat methods and mitigating lost-in-the-middle effects.

</details>


### [5] [Mem2ActBench: A Benchmark for Evaluating Long-Term Memory Utilization in Task-Oriented Autonomous Agents](https://arxiv.org/abs/2601.19935)
*Yiting Shen,Kun Li,Wei Zhou,Songlin Hu*

Main category: cs.CL

TL;DR: Mem2ActBench是一个评估LLM智能体能否主动利用长期记忆执行工具操作的新基准，专注于记忆在工具选择和参数定位中的应用，而非被动事实检索。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要测试智能体被动检索孤立事实的能力，但未能评估更关键的主动应用记忆执行任务的能力。需要评估智能体是否能在长期、中断的交互中主动利用记忆来执行工具操作。

Method: 通过自动化流程构建数据集，合并异构数据源（ToolACE、BFCL、Oasst1），通过一致性建模解决冲突，合成2,029个会话（平均12轮用户-助手-工具交互）。从这些记忆链中，使用反向生成方法产生400个工具使用任务。

Result: 人类评估确认91.3%的任务具有强烈的记忆依赖性。对七个记忆框架的实验表明，当前系统在主动利用记忆进行参数定位方面仍然不足。

Conclusion: 需要更有效的方法来评估和改进任务执行中的记忆应用能力，当前系统在主动利用长期记忆执行工具操作方面存在不足。

Abstract: Large Language Model (LLM)-based agents are increasingly deployed for complex, tool-based tasks where long-term memory is critical to driving actions. Existing benchmarks, however, primarily test a angent's ability to passively retrieve isolated facts in response to explicit questions. They fail to evaluate the more crucial capability of actively applying memory to execute tasks. To address this gap, we introduce \textsc{Mem2ActBench}, a benchmark for evaluating whether agents can proactively leverage long-term memory to execute tool-based actions by selecting appropriate tools and grounding their parameters. The benchmark simulates persistent assistant usage, where users mention the same topic across long, interrupted interactions and expect previously established preferences and task states to be implicitly applied. We build the dataset with an automated pipeline that merges heterogeneous sources (ToolACE, BFCL, Oasst1), resolves conflicts via consistency modeling, and synthesizes 2,029 sessions with 12 user--assistant--tool turns on average. From these memory chains, a reverse-generation method produces 400 tool-use tasks, with human evaluation confirming 91.3\% are strongly memory-dependent. Experiments on seven memory frameworks show that current systems remain inadequate at actively utilizing memory for parameter grounding, highlighting the need for more effective approaches to evaluate and improve memory application in task execution.

</details>


### [6] [VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.20055)
*Vikash Singh,Darion Cassel,Nathaniel Weir,Nick Feng,Sam Bayless*

Main category: cs.CL

TL;DR: VERGE是一个神经符号框架，结合LLM和SMT求解器，通过迭代精炼生成验证引导的答案，提升逻辑正确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在语法上很流畅，但在高风险领域中确保其逻辑正确性仍然是一个基本挑战。需要一种方法来验证和保证LLM输出的逻辑一致性。

Method: 将LLM输出分解为原子声明，自动形式化为一阶逻辑，使用自动定理证明验证逻辑一致性。引入三个关键创新：多模型共识通过形式语义等价检查确保逻辑级对齐；语义路由将不同声明类型导向适当的验证策略；通过最小修正子集进行精确逻辑错误定位。

Result: 使用GPT-OSS-120B模型，VERGE在一组推理基准测试中相比单次通过方法，在收敛时平均性能提升了18.7%。

Conclusion: 这种混合方法在可能的情况下提供形式保证，在其他情况下提供共识验证，推进可信AI的发展。

Abstract: Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.

</details>


### [7] [Beyond Accuracy: A Cognitive Load Framework for Mapping the Capability Boundaries of Tool-use Agents](https://arxiv.org/abs/2601.20412)
*Qihao Wang,Yue Hu,Mingzhe Lu,Jiayue Wu,Yanbing Liu,Yuanmin Tang*

Main category: cs.CL

TL;DR: 论文提出了一个基于认知负荷理论的框架，用于诊断LLM工具使用能力的认知瓶颈，通过ToolLoad-Bench基准测试量化内在负荷和外在负荷，精确映射模型能力边界。


<details>
  <summary>Details</summary>
Motivation: 当前LLM工具使用能力的评估主要关注最终准确率，但无法揭示认知瓶颈和真实能力边界。需要从简单性能评分转向诊断工具，以理解模型的能力限制。

Method: 基于认知负荷理论，将任务复杂度分解为两个可量化部分：内在负荷（使用新颖的工具交互图形式化解决方案路径的结构复杂度）和外在负荷（任务表述模糊性带来的难度）。构建ToolLoad-Bench基准，首次实现参数化可调节的认知负荷控制实验。

Result: 评估显示随着认知负荷增加，模型性能出现明显断崖式下降，能够精确映射每个模型的能力边界。框架的预测与实证结果高度校准，为理解智能体极限提供了原则性方法。

Conclusion: 该框架为理解智能体能力边界建立了原则性方法，并为构建更高效系统提供了实用基础，实现了从性能评分到诊断工具的转变。

Abstract: The ability of Large Language Models (LLMs) to use external tools unlocks powerful real-world interactions, making rigorous evaluation essential. However, current benchmarks primarily report final accuracy, revealing what models can do but obscuring the cognitive bottlenecks that define their true capability boundaries. To move from simple performance scoring to a diagnostic tool, we introduce a framework grounded in Cognitive Load Theory. Our framework deconstructs task complexity into two quantifiable components: Intrinsic Load, the inherent structural complexity of the solution path, formalized with a novel Tool Interaction Graph; and Extraneous Load, the difficulty arising from ambiguous task presentation. To enable controlled experiments, we construct ToolLoad-Bench, the first benchmark with parametrically adjustable cognitive load. Our evaluation reveals distinct performance cliffs as cognitive load increases, allowing us to precisely map each model's capability boundary. We validate that our framework's predictions are highly calibrated with empirical results, establishing a principled methodology for understanding an agent's limits and a practical foundation for building more efficient systems.

</details>


### [8] [SERA: Soft-Verified Efficient Repository Agents](https://arxiv.org/abs/2601.20789)
*Ethan Shen,Danny Tormoen,Saurabh Shah,Ali Farhadi,Tim Dettmers*

Main category: cs.CL

TL;DR: SERA提出了一种高效训练代码代理的方法，通过软验证生成技术创建数千个代码库轨迹，能以低成本专门化到私有代码库，性能达到开源模型SOTA并匹配前沿开放权重模型。


<details>
  <summary>Details</summary>
Motivation: 开放权重代码代理相对于闭源系统具有理论优势：可以专门化到私有代码库，将仓库特定信息直接编码到权重中。但由于训练成本和复杂性，这一优势一直停留在理论层面。

Method: 提出软验证高效仓库代理(SERA)方法，使用软验证生成(SVG)技术从单个代码库生成数千个轨迹，仅通过监督微调(SFT)实现高效训练。

Result: SERA在完全开源模型中达到SOTA性能，匹配Devstral-Small-2等前沿开放权重模型。训练成本比强化学习低26倍，比先前合成数据方法低57倍。生成了超过20万个合成轨迹数据集。

Conclusion: 该工作将加速开放代码代理研究，展示了可以专门化到私有代码库的开源模型优势。作为Ai2开放代码代理系列的首个模型发布，包含代码、数据和Claude Code集成。

Abstract: Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.

</details>


### [9] [Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models](https://arxiv.org/abs/2601.20126)
*Abha Jha,Akanksha Mahajan,Ashwath Vaithinathan Aravindan,Praveen Saravanan,Sai Sailaja Policharla,Sonal Chaturbhuj Gehlot*

Main category: cs.CL

TL;DR: 该论文研究使用可验证奖励的强化学习（RLVR）训练范式，通过奖励"我不知道"的弃权回答来减少LLM的幻觉，在多项选择题任务上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生幻觉或不可验证的内容，这削弱了其在事实领域中的可靠性。需要一种训练范式来促进知识谦逊，明确奖励弃权回答。

Method: 使用可验证奖励的强化学习（RLVR）训练范式，采用三元奖励结构（-1，r_abs，1），在MedMCQA和Hendrycks Math基准上对Granite-3.3-2B-Instruct和Qwen-3-4B-Instruct进行微调。研究不同弃权奖励结构的影响，并探索将RLVR与监督微调策略结合。

Result: 中等弃权奖励（r_abs ≈ -0.25到0.3）能持续减少错误回答，且不会严重降低多项选择题的准确性。较大模型对弃权激励表现出更强的鲁棒性。在开放式问答中，由于探索不足存在局限性，但可通过监督弃权训练部分缓解。

Conclusion: 可验证奖励设计是缓解语言模型幻觉的可行且灵活的方法。中等弃权奖励能在减少错误回答的同时保持准确性，较大模型对此方法更鲁棒。

Abstract: Large Language Models (LLMs) often produce hallucinated or unverifiable content, undermining their reliability in factual domains. This work investigates Reinforcement Learning with Verifiable Rewards (RLVR) as a training paradigm that explicitly rewards abstention ("I don't know") alongside correctness to promote intellectual humility. We fine-tune and evaluate Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct on the MedMCQA and Hendrycks Math benchmarks using a ternary reward structure ($-1$, r_abs, 1) under varying abstention reward structures. We further study the effect of combining RLVR with supervised fine-tuning strategies that teach abstention prior to reinforcement learning. Our results show that moderate abstention rewards (r_abs $\approx -0.25$ to 0.3) consistently reduce incorrect responses without severe accuracy degradation on multiple-choice tasks, with larger models exhibiting greater robustness to abstention incentives. On open-ended question answering, we observe limitations due to insufficient exploration, which can be partially mitigated through supervised abstention training. Overall, these findings demonstrate the feasibility and flexibility of verifiable reward design as a practical approach for hallucination mitigation in language models. Reproducible code for our abstention training framework is available here https://github.com/Mystic-Slice/rl-abstention.

</details>


### [10] [Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction](https://arxiv.org/abs/2601.20162)
*Shuoxin Wang,Chang Liu,Gowen Loo,Lifan Zheng,Kaiwen Wei,Xinyi Zeng,Jingyuan Zhang,Yu Tian*

Main category: cs.CL

TL;DR: Me-Agent是一个可学习、可记忆的个性化移动代理，通过两级用户习惯学习和分层偏好记忆来解决LLM代理忽视个性化需求的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM移动代理通常遵循明确的用户指令而忽视个性化需求，导致三个主要限制：无法解释模糊指令、缺乏从用户交互历史中学习、无法处理个性化指令。

Method: 1. 两级用户习惯学习：提示级使用个人奖励模型增强的用户偏好学习策略；记忆级设计分层偏好记忆，在不同层级存储用户长期记忆和应用特定记忆。2. 引入User FingerTip基准测试。

Result: 在User FingerTip和通用基准测试上的广泛实验表明，Me-Agent在个性化方面达到最先进性能，同时保持竞争力的指令执行性能。

Conclusion: Me-Agent通过有效的个性化机制解决了现有移动代理的局限性，为实际用户提供了更好的个性化体验。

Abstract: Large Language Model (LLM)-based mobile agents have made significant performance advancements. However, these agents often follow explicit user instructions while overlooking personalized needs, leading to significant limitations for real users, particularly without personalized context: (1) inability to interpret ambiguous instructions, (2) lack of learning from user interaction history, and (3) failure to handle personalized instructions. To alleviate the above challenges, we propose Me-Agent, a learnable and memorable personalized mobile agent. Specifically, Me-Agent incorporates a two-level user habit learning approach. At the prompt level, we design a user preference learning strategy enhanced with a Personal Reward Model to improve personalization performance. At the memory level, we design a Hierarchical Preference Memory, which stores users' long-term memory and app-specific memory in different level memory. To validate the personalization capabilities of mobile agents, we introduce User FingerTip, a new benchmark featuring numerous ambiguous instructions for daily life. Extensive experiments on User FingerTip and general benchmarks demonstrate that Me-Agent achieves state-of-the-art performance in personalization while maintaining competitive instruction execution performance.

</details>


### [11] [Automated Benchmark Generation from Domain Guidelines Informed by Bloom's Taxonomy](https://arxiv.org/abs/2601.20253)
*Si Chen,Le Huy Khiem,Annalisa Szymanski,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 该论文提出了一个从专家指南自动生成基准测试的框架，用于评估LLM在实践领域的开放式问答能力，发现LLM在高阶推理上表现更好但在基础记忆上反而更差。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试主要依赖现有人类考试数据集，这些数据集在实践性专业领域往往不可得。实践领域知识具有程序性和专业判断特点，需要评估模型在真实场景中的情境化推理能力。

Method: 基于布鲁姆分类法，将专家实践指南转化为隐含违规场景，然后扩展为自动评分的多选题和多轮对话，覆盖四个认知层次，实现确定性、可重复和可扩展的评估。

Result: 应用于教学、营养学和护理三个领域，发现LLM与人类推理存在差异：LLM有时在高阶推理（分析）上表现相对更好，但在低层次项目（记忆）上失败更频繁。

Conclusion: 该框架能够生成大规模、心理测量学知情的基准测试，揭示LLM的非直观行为模式，支持在真实世界环境中评估情境化推理能力。

Abstract: Open-ended question answering (QA) evaluates a model's ability to perform contextualized reasoning beyond factual recall. This challenge is especially acute in practice-based domains, where knowledge is procedural and grounded in professional judgment, while most existing LLM benchmarks depend on pre-existing human exam datasets that are often unavailable in such settings. We introduce a framework for automated benchmark generation from expert-authored guidelines informed by Bloom's Taxonomy. It converts expert practices into implicit violation-based scenarios and expands them into auto-graded multiple-choice questions (MCQs) and multi-turn dialogues across four cognitive levels, enabling deterministic, reproducible, and scalable evaluation. Applied to three applied domains: teaching, dietetics, and caregiving, we find differences between model and human-like reasoning: LLMs sometimes perform relatively better on higher-order reasoning (Analyze) but fail more frequently on lower-level items (Remember). We produce large-scale, psychometrically informed benchmarks that surface these non-intuitive model behaviors and enable evaluation of contextualized reasoning in real-world settings.

</details>


### [12] [Beyond the Needle's Illusion: Decoupled Evaluation of Evidence Access and Use under Semantic Interference at 326M-Token Scale](https://arxiv.org/abs/2601.20276)
*Tianwei Lin,Zuyi Zhou,Xinda Zhao,Chenke Wang,Xiaohong Li,Yu Chen,Chuanrui Hu,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: 提出了对抗性基准EMB-S，用于评估长上下文LLM代理在语义干扰下的证据访问能力，发现语义辨别而非上下文长度是主要瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有NIAH评估主要测试良性跨度定位，但实际应用中需要从大量语义干扰中准确访问证据，因此需要更真实的对抗性基准

Method: 构建326M令牌的MemoryBank，创建包含碰撞测试的近误硬负例和黄金证据集的查询，提出分离的诊断协议分别评估证据访问和端到端QA质量

Result: 在从64K到326M令牌的参考语料阶梯上，系统在良性NIAH中饱和但在语义干扰下证据访问急剧下降，表明语义辨别是主要瓶颈

Conclusion: 语义辨别能力而非上下文长度是长上下文记忆扩展的主要瓶颈，需要对抗性评估来真实衡量LLM代理的证据访问能力

Abstract: Long-context LLM agents must access the right evidence from large environments and use it faithfully. However, the popular Needle-in-a-Haystack (NIAH) evaluation mostly measures benign span localization. The needle is near-unique, and the haystack is largely irrelevant. We introduce EverMemBench-S (EMB-S), an adversarial NIAH-style benchmark built on a 326M-token MemoryBank. While the full MemoryBank spans 326M tokens for retrieval-based (RAG) evaluation, we evaluate native long-context models only at scales that fit within each model's context window (up to 1M tokens in this work) to ensure a fair comparison. EMB-S pairs queries with collision-tested near-miss hard negatives and gold evidence sets spanning one or more documents, validated via human screening and LLM verification. We also propose a decoupled diagnostic protocol that reports evidence access (document-ID localization) separately from end-to-end QA quality under full-context prompting. This enables consistent diagnosis for both native long-context prompting and retrieval pipelines. Across a reference-corpus ladder from domain-isolated 64K contexts to a globally shared 326M-token environment, we observe a clear reality gap. Systems that saturate benign NIAH degrade sharply in evidence access under semantic interference. These results indicate that semantic discrimination, not context length alone, is the dominant bottleneck for long-context memory at scale.

</details>


### [13] [SAPO: Self-Adaptive Process Optimization Makes Small Reasoners Stronger](https://arxiv.org/abs/2601.20312)
*Kaiyuan Chen,Guangmin Zheng,Jin Wang,Xiaobing Zhou,Xuejie Zhang*

Main category: cs.CL

TL;DR: SAPO方法通过自适应过程监督信号减少推理器-验证器差距，提升小语言模型在数学和代码任务上的性能，避免低效的蒙特卡洛估计。


<details>
  <summary>Details</summary>
Motivation: 现有自我进化方法忽视细粒度推理步骤的影响，导致推理器-验证器差距。蒙特卡洛过程监督的计算效率低下进一步加剧了缩小这一差距的困难。受错误相关负波（ERN）启发，推理器能在错误决策后定位错误并指导快速调整。

Method: 提出自我适应过程优化（SAPO）方法，通过自适应且高效地引入过程监督信号，主动最小化推理器-验证器差距，而非依赖低效的蒙特卡洛估计。

Result: 在数学和代码两个挑战性任务类型上，SAPO方法优于大多数现有自我进化方法。此外，为研究SAPO对验证器性能的影响，引入了数学和编码任务的两个新过程奖励模型基准。

Conclusion: SAPO方法通过自适应过程监督有效缩小推理器-验证器差距，提升小语言模型性能，并为过程奖励模型评估提供了新基准。

Abstract: Existing self-evolution methods overlook the influence of fine-grained reasoning steps, which leads to the reasoner-verifier gap. The computational inefficiency of Monte Carlo (MC) process supervision further exacerbates the difficulty in mitigating the gap. Motivated by the Error-Related Negativity (ERN), which the reasoner can localize error following incorrect decisions, guiding rapid adjustments, we propose a Self-Adaptive Process Optimization (SAPO) method for self-improvement in Small Language Models (SLMs). SAPO adaptively and efficiently introduces process supervision signals by actively minimizing the reasoner-verifier gap rather than relying on inefficient MC estimations. Extensive experiments demonstrate that the proposed method outperforms most existing self-evolution methods on two challenging task types: mathematics and code. Additionally, to further investigate SAPO's impact on verifier performance, this work introduces two new benchmarks for process reward models in both mathematical and coding tasks.

</details>


### [14] [CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria](https://arxiv.org/abs/2601.20327)
*Xinyu Hu,Yancheng He,Weixun Wang,Tao Feng,Li Lin,Jiashun Liu,Wenbo Su,Bo Zheng,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出CE-RM-4B，一个基于两阶段rollout方法和统一查询标准的生成式奖励模型，在少量高质量数据上训练，在奖励模型基准测试和下游RL实践中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge评估方法在基准测试中表现良好，但在实际RL应用中存在明显差距，主要由于现有研究中的成对评估主导和评估标准优化不足等限制。

Method: 提出CE-RM-4B生成式奖励模型，采用专门的两阶段rollout训练方法，使用统一查询标准，仅使用约5.7K从开源偏好数据集中精选的高质量数据进行训练。

Result: CE-RM-4B在多样化奖励模型基准测试中表现优异，特别是在Best-of-N场景中，并在下游RL实践中提供更有效的改进。

Conclusion: 通过两阶段rollout方法和统一查询标准训练的生成式奖励模型，即使使用少量高质量数据，也能显著提升评估效果和RL实践性能。

Abstract: Automatic evaluation is crucial yet challenging for open-ended natural language generation, especially when rule-based metrics are infeasible. Compared with traditional methods, the recent LLM-as-a-Judge paradigms enable better and more flexible evaluation, and show promise as generative reward models for reinforcement learning. However, prior work has revealed a notable gap between their seemingly impressive benchmark performance and actual effectiveness in RL practice. We attribute this issue to some limitations in existing studies, including the dominance of pairwise evaluation and inadequate optimization of evaluation criteria. Therefore, we propose CE-RM-4B, a pointwise generative reward model trained with a dedicated two-stage rollout method, and adopting unified query-based criteria. Using only about 5.7K high-quality data curated from the open-source preference dataset, our CE-RM-4B achieves superior performance on diverse reward model benchmarks, especially in Best-of-N scenarios, and delivers more effective improvements in downstream RL practice.

</details>


### [15] [MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment](https://arxiv.org/abs/2601.20335)
*Qinzhuo Wu,Zhizhuo Yang,Hanhao Li,Pengzhi Gao,Wei Liu,Jian Luan*

Main category: cs.CL

TL;DR: MobileBench-OL是一个包含1080个任务、覆盖80个中文应用的在线移动GUI代理基准测试，通过5个子集评估任务执行、复杂推理和噪声鲁棒性，并提供了带重置机制的自动评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理基准测试存在局限性：在线基准虽然比离线基准更真实，但主要关注任务指令跟随能力，忽略了推理和探索能力，且未考虑真实移动环境中的随机噪声，导致基准测试与现实环境存在差距。

Method: 提出MobileBench-OL在线基准测试，包含1080个任务，覆盖80个中文应用，通过5个子集设置多个评估维度来测量任务执行、复杂推理和噪声鲁棒性。同时提供带重置机制的自动评估框架，实现稳定可重复的真实世界基准测试。

Result: 在MobileBench-OL上评估12个领先的GUI代理，结果显示这些代理在满足真实世界需求方面仍有显著改进空间。人工评估进一步证实MobileBench-OL能够可靠地测量领先GUI代理在真实环境中的性能。

Conclusion: MobileBench-OL填补了现有移动GUI代理基准测试的空白，通过全面评估任务执行、复杂推理和噪声鲁棒性，为移动代理的开发和评估提供了更贴近真实环境的测试平台。

Abstract: Recent advances in mobile Graphical User Interface (GUI) agents highlight the growing need for comprehensive evaluation benchmarks. While new online benchmarks offer more realistic testing than offline ones, they tend to focus on the agents' task instruction-following ability while neglecting their reasoning and exploration ability. Moreover, these benchmarks do not consider the random noise in real-world mobile environments. This leads to a gap between benchmarks and real-world environments. To addressing these limitations, we propose MobileBench-OL, an online benchmark with 1080 tasks from 80 Chinese apps. It measures task execution, complex reasoning, and noise robustness of agents by including 5 subsets, which set multiple evaluation dimensions. We also provide an auto-eval framework with a reset mechanism, enabling stable and repeatable real-world benchmarking. Evaluating 12 leading GUI agents on MobileBench-OL shows significant room for improvement to meet real-world requirements. Human evaluation further confirms that MobileBench-OL can reliably measure the performance of leading GUI agents in real environments. Our data and code will be released upon acceptance.

</details>


### [16] [PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use](https://arxiv.org/abs/2601.20439)
*Qihao Wang,Mingzhe Lu,Jiayue Wu,Yue Hu,Yanbing Liu*

Main category: cs.CL

TL;DR: PEARL框架通过离线探索和在线强化学习提升LLM的工具使用能力，在ToolHop基准上达到56.5%的新SOTA成功率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在使用外部工具时面临复杂多轮调用的挑战，包括规划能力弱、工具幻觉、参数生成错误和交互鲁棒性差等问题

Method: 采用两阶段方法：离线阶段探索工具学习有效使用模式和失败条件；在线阶段通过GRPO训练专用规划器，使用精心设计的奖励函数提供规划质量信号

Result: 在ToolHop和T-Eval基准测试中显著优于现有方法，在ToolHop上达到56.5%的新SOTA成功率，同时保持较低调用错误率

Conclusion: PEARL框架在解决工具使用的复杂规划挑战方面取得关键进展，有助于开发更鲁棒可靠的基于LLM的智能体

Abstract: Large Language Models show great potential with external tools, but face significant challenges in complex, multi-turn tool invocation. They often exhibit weak planning, tool hallucination, erroneous parameter generation, and struggle with robust interaction. To tackle these issues, we present PEARL, a novel framework to enhance LLM planning and execution for sophisticated tool use. PEARL adopts a two-stage approach: an offline phase where the agent explores tools to learn valid usage patterns and failure conditions, and an online reinforcement learning phase. In the online phase, a dedicated Planner is trained via group Relative Policy Optimization (GRPO) with a carefully designed reward function that provides distinct signals for planning quality. Experiments on the ToolHop and T-Eval benchmarks show PEARL significantly outperforms existing methods, achieving a new state-of-the-art success rate of \textbf{56.5\%} on ToolHop while maintaining a low invocation error rate. Our work marks a key advance in addressing the complex planning challenges of tool use, contributing to the development of more robust and reliable LLM-based agents.

</details>


### [17] [BMAM: Brain-inspired Multi-Agent Memory Framework](https://arxiv.org/abs/2601.20465)
*Yang Li,Jiaxiang Liu,Yusong Wang,Yujie Wu,Mingkun Xu*

Main category: cs.CL

TL;DR: BMAM是一种受大脑启发的多智能体记忆架构，通过功能专门化的子系统解决智能体在长期交互中的"灵魂侵蚀"问题，在LoCoMo基准测试中达到78.45%准确率。


<details>
  <summary>Details</summary>
Motivation: 基于语言模型的智能体在长期交互中面临保持时间基础信息和行为一致性的挑战，作者称之为"灵魂侵蚀"问题。现有方法通常使用单一非结构化存储，无法有效处理时间尺度和记忆功能差异。

Method: 提出BMAM架构，受认知记忆系统启发，将记忆分解为四个功能专门化的子系统：情景记忆、语义记忆、显著性感知记忆和控制导向记忆，这些组件在互补的时间尺度上运作。为支持长期推理，BMAM沿着明确的时间线组织情景记忆，并通过融合多个互补信号来检索证据。

Result: 在LoCoMo基准测试中，BMAM在标准长期评估设置下达到78.45%的准确率。消融分析证实，受海马体启发的的情景记忆子系统在时间推理中起着关键作用。

Conclusion: BMAM通过功能专门化的多智能体记忆架构有效解决了语言模型智能体的"灵魂侵蚀"问题，为长期交互中的时间一致性和记忆保持提供了有效的解决方案。

Abstract: Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.

</details>


### [18] [Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models](https://arxiv.org/abs/2601.20546)
*Kumiko Nakajima,Jan Zuiderveld,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 论文提出Conditional Divergent Association Task (CDAT)，一种基于人类创造力理论（新颖性+适当性）的LLM创造力评估方法，发现传统DAT方法存在缺陷，而CDAT能更好区分噪声与创造力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM创造力评估方法（如DAT）缺乏人类创造力理论的基础，只关注新颖性而忽略适当性这一核心要素，导致评估结果难以解释且有效性存疑。

Method: 基于人类创造力理论（创造力=新颖性+适当性），提出Conditional Divergent Association Task (CDAT)，在考虑上下文适当性的条件下评估新颖性。使用CDAT评估多种最先进LLM，并与传统DAT方法对比。

Result: 1) DAT评估下LLM得分低于无创造力基线，质疑其有效性；2) CDAT能更好区分噪声与创造力；3) 较小模型家族常表现最高创造力，而先进模型家族倾向于适当性但新颖性较低；4) 训练和对齐可能使模型在创造力边界上向适当性偏移。

Conclusion: 需要基于人类创造力理论（新颖性+适当性）的评估方法，CDAT提供了一种简单客观的替代方案。模型训练和对齐可能以牺牲创造力为代价提高适当性，这对LLM创造力发展有重要启示。

Abstract: Large language models (LLMs) are increasingly used in verbal creative tasks. However, previous assessments of the creative capabilities of LLMs remain weakly grounded in human creativity theory and are thus hard to interpret. The widely used Divergent Association Task (DAT) focuses on novelty, ignoring appropriateness, a core component of creativity. We evaluate a range of state-of-the-art LLMs on DAT and show that their scores on the task are lower than those of two baselines that do not possess any creative abilities, undermining its validity for model evaluation. Grounded in human creativity theory, which defines creativity as the combination of novelty and appropriateness, we introduce Conditional Divergent Association Task (CDAT). CDAT evaluates novelty conditional on contextual appropriateness, separating noise from creativity better than DAT, while remaining simple and objective. Under CDAT, smaller model families often show the most creativity, whereas advanced families favor appropriateness at lower novelty. We hypothesize that training and alignment likely shift models along this frontier, making outputs more appropriate but less creative. We release the dataset and code.

</details>


### [19] [AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios](https://arxiv.org/abs/2601.20613)
*Kaiyuan Chen,Qimin Wu,Taiyu Hou,Tianhao Tang,Xueyu Hu,Yuchen Hou,Bikun Li,Chengming Qian,Guoyin Wang,Haolin Chen,Haotong Tian,Haoye Zhang,Haoyu Bian,Hongbing Pan,Hongkang Zhang,Hongyi Zhou,Jiaqi Cai,Jiewu Rao,Jiyuan Ren,Keduan Huang,Lucia Zhu Huang,Mingyu Yuan,Naixu Guo,Qicheng Tang,Qinyan Zhang,Shuai Chen,Siheng Chen,Ting Ting Li,Xiaoxing Guo,Yaocheng Zuo,Yaoqi Guo,Yinan Wang,Yinzhou Yu,Yize Wang,Yuan Jiang,Yuan Tian,Yuanshuo Zhang,Yuxuan Liu,Yvette Yan Zeng,Zenyu Shan,Zihan Yin,Xiaobo Hu,Yang Liu,Yixin Ren,Yuan Gong*

Main category: cs.CL

TL;DR: 论文提出AgentIF-OneDay基准，评估AI代理能否完成多样化的日常任务，包括工作流执行、隐含指令理解和迭代优化，发现基于API构建的代理产品表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理评估过于关注任务难度，而忽略了日常场景的多样性需求。用户对AI高级能力的感知有限，需要更贴近日常工作和生活场景的评估基准。

Method: 提出AgentIF-OneDay基准，包含104个任务、767个评分点，分为三类：开放工作流执行、隐含指令理解、迭代优化。采用实例级评分标准和改进的评估流程，结合LLM验证与人工判断。

Result: 使用Gemini-3-Pro验证，LLM验证与人工判断的一致性达到80.1%。评估四个领先的通用AI代理，发现基于API构建的代理产品和基于强化学习的ChatGPT代理处于第一梯队。

Conclusion: 领先的LLM API和开源模型已内化代理能力，使AI应用团队能够开发前沿的代理产品。日常任务评估对推动AI代理的实际应用至关重要。

Abstract: The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.

</details>


### [20] [P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering](https://arxiv.org/abs/2601.20649)
*Wenlin Zhong,Chengyuan Liu,Yiquan Wu,Bovin Tan,Changlong Sun,Yi Wang,Xiaozhong Liu,Kun Kuang*

Main category: cs.CL

TL;DR: P2S提出了一种新的自监督框架，通过合成高质量参考推理链，为每个推理步骤计算路径忠实度奖励，解决通用领域推理任务中缺乏可验证奖励信号的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在结构化领域有效，但在通用领域推理任务中缺乏可验证奖励信号。RLPR等方法仅关注最终答案概率，忽略了推理过程的逐步监督，存在奖励稀疏性问题。

Method: 提出概率过程监督(P2S)框架：1)在强化学习过程中合成和过滤高质量参考推理链(gold-CoT)；2)为每个推理步骤计算路径忠实度奖励(PFR)，基于当前推理前缀生成gold-CoT后缀的条件概率；3)PFR可与任何基于结果的奖励灵活结合，提供密集指导。

Result: 在阅读理解和医疗问答基准测试中，P2S显著优于现有基线方法，有效解决了奖励稀疏性问题。

Conclusion: P2S通过提供细粒度的过程奖励，无需单独奖励模型或人工标注推理步骤，成功将强化学习扩展到通用领域推理任务，为LLM推理提供了有效的自监督框架。

Abstract: While reinforcement learning with verifiable rewards (RLVR) has advanced LLM reasoning in structured domains like mathematics and programming, its application to general-domain reasoning tasks remains challenging due to the absence of verifiable reward signals. To this end, methods like Reinforcement Learning with Reference Probability Reward (RLPR) have emerged, leveraging the probability of generating the final answer as a reward signal. However, these outcome-focused approaches neglect crucial step-by-step supervision of the reasoning process itself. To address this gap, we introduce Probabilistic Process Supervision (P2S), a novel self-supervision framework that provides fine-grained process rewards without requiring a separate reward model or human-annotated reasoning steps. During reinforcement learning, P2S synthesizes and filters a high-quality reference reasoning chain (gold-CoT). The core of our method is to calculate a Path Faithfulness Reward (PFR) for each reasoning step, which is derived from the conditional probability of generating the gold-CoT's suffix, given the model's current reasoning prefix. Crucially, this PFR can be flexibly integrated with any outcome-based reward, directly tackling the reward sparsity problem by providing dense guidance. Extensive experiments on reading comprehension and medical Question Answering benchmarks show that P2S significantly outperforms strong baselines.

</details>


### [21] [Efficient Multimodal Planning Agent for Visual Question-Answering](https://arxiv.org/abs/2601.20676)
*Zhuo Chen,Xinyu Geng,Xinyu Wang,Yong Jiang,Zhen Zhang,Pengjun Xie,Kewei Tu*

Main category: cs.CL

TL;DR: 提出一种训练多模态规划代理的方法，动态分解mRAG流程来解决VQA任务，在保持性能的同时提升效率


<details>
  <summary>Details</summary>
Motivation: 当前VQA任务中，多模态检索增强生成(mRAG)通常采用多阶段流水线，存在固有依赖关系和效率限制。需要一种方法在保持任务性能的同时提高效率。

Method: 训练一个多模态规划代理，动态分解mRAG流水线，智能决定每个mRAG步骤的必要性，优化效率与效果的权衡。

Result: 代理能减少冗余计算，相比现有方法减少超过60%的搜索时间，降低昂贵工具调用。在六个不同数据集上平均表现优于所有基线方法。

Conclusion: 提出的多模态规划代理方法能有效平衡VQA任务的效率与性能，为知识密集型VQA查询提供更高效的解决方案。

Abstract: Visual Question-Answering (VQA) is a challenging multimodal task that requires integrating visual and textual information to generate accurate responses. While multimodal Retrieval-Augmented Generation (mRAG) has shown promise in enhancing VQA systems by providing more evidence on both image and text sides, the default procedure that addresses VQA queries, especially the knowledge-intensive ones, often relies on multi-stage pipelines of mRAG with inherent dependencies. To mitigate the inefficiency limitations while maintaining VQA task performance, this paper proposes a method that trains a multimodal planning agent, dynamically decomposing the mRAG pipeline to solve the VQA task. Our method optimizes the trade-off between efficiency and effectiveness by training the agent to intelligently determine the necessity of each mRAG step. In our experiments, the agent can help reduce redundant computations, cutting search time by over 60\% compared to existing methods and decreasing costly tool calls. Meanwhile, experiments demonstrate that our method outperforms all baselines, including a Deep Research agent and a carefully designed prompt-based method, on average over six various datasets. Code will be released.

</details>


### [22] [QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks](https://arxiv.org/abs/2601.20731)
*Mae Sosto,Delfina Sol Martinez Pandiani,Laura Hollink*

Main category: cs.CL

TL;DR: 该研究探讨大语言模型如何再现社会规范（特别是异性恋顺性别规范），以及这些规范如何转化为文本生成中的可测量偏见。研究发现不同模型对标记为酷儿、非酷儿和未标记的受试者产生不同偏见模式。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大语言模型如何再现社会规范，特别是异性恋顺性别规范，以及这些规范如何转化为文本生成中的可测量偏见。关注模型是否基于受试者的性别或性取向信息产生不同的响应。

Method: 研究方法包括：1) 将受试者分为三类：酷儿标记、非酷儿标记和未标记（标准化类别）；2) 通过四个维度操作化表征不平衡：情感、尊重度、毒性和预测多样性；3) 比较掩码语言模型、自回归语言模型和闭源自回归语言模型的响应差异。

Result: 研究结果显示：1) 掩码语言模型对酷儿标记受试者产生最不利的情感、更高的毒性和更负面的尊重度；2) 自回归语言模型部分缓解了这些模式；3) 闭源自回归语言模型倾向于对未标记受试者产生更有害的输出。

Conclusion: 结论是大语言模型再现了规范性的社会假设，但偏见的形式和程度强烈依赖于特定模型特征。模型可能重新分配但不会消除表征伤害，表明需要更细致的模型评估和缓解策略。

Abstract: This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized "unmarked" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [23] [Achieving Productivity Gains with AI-based IDE features: A Journey at Google](https://arxiv.org/abs/2601.19964)
*Maxim Tabachnyk,Xu Shu,Alexander Frömmgen,Pavel Sychev,Vahid Meimand,Ilia Krets,Stanislav Pyatykh,Abner Araujo,Kristóf Molnár,Satish Chandra*

Main category: cs.SE

TL;DR: Google开发AI驱动的IDE功能（代码补全和自然语言代码转换），通过多层面优化解决延迟、用户体验和质量问题，提升企业生产力


<details>
  <summary>Details</summary>
Motivation: 在企业环境中开发AI驱动的开发者工具，需要解决实际部署中的挑战，包括延迟、用户体验和代码建议质量，以实现真正的生产力提升

Method: 通过用户界面、后端和模型层的综合优化，结合严格的实验验证，改进代码补全和自然语言代码转换功能

Result: 成功开发出在企业环境中可用的AI IDE功能，通过多层面优化解决了实际部署中的关键挑战

Conclusion: AI开发者工具的优化需要跨UI、后端和模型层的系统方法，结合严格实验，才能在企业环境中实现生产力提升

Abstract: We discuss Google's journey in developing and refining two internal AI-based IDE features: code completion and natural-language-driven code transformation (Transform Code). We address challenges in latency, user experience and suggestion quality, all backed by rigorous experimentation. The article serves as an example of how to refine AI developer tools across the user interface, backend, and model layers, to deliver tangible productivity improvements in an enterprise setting.

</details>


### [24] [Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis](https://arxiv.org/abs/2601.20103)
*Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.SE

TL;DR: 提出了TRACE基准测试，用于评估LLM在代码生成RL环境中检测奖励攻击的能力，发现在对比性异常检测设置中模型表现更好（GPT-5.2达到63%检测率），并揭示了模型在语义上下文奖励攻击上的显著困难。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成中强化学习的进展，需要健壮的环境来防止奖励攻击。LLM越来越多地作为代码RL的评估器，但其检测奖励攻击的能力尚未得到充分研究。

Method: 提出了包含54个类别的奖励攻击分类法，创建了TRACE基准测试（包含517个测试轨迹），在对比性异常检测设置中评估模型，并与孤立分类设置进行对比。

Result: 模型在对比性设置中检测奖励攻击更有效（GPT-5.2达到63%检测率，比孤立设置的45%显著提升）。模型在语义上下文奖励攻击上比句法上下文奖励攻击表现更差。良性与攻击轨迹比例和分析聚类大小显著影响检测性能。

Conclusion: TRACE基准测试为评估LLM检测奖励攻击能力提供了重要工具，揭示了对比性评估设置的优势，并指出了模型在语义复杂性奖励攻击上的局限性。

Abstract: Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.

</details>


### [25] [Are We All Using Agents the Same Way? An Empirical Study of Core and Peripheral Developers Use of Coding Agents](https://arxiv.org/abs/2601.20106)
*Shamse Tasnim Cynthia,Joy Krishan Das,Banani Roy*

Main category: cs.SE

TL;DR: 该研究首次对9,427个自主编码代理生成的PR进行实证分析，发现核心与外围开发者在使用AI代理时存在显著差异：外围开发者更频繁使用代理且任务分布均匀，而核心开发者更关注文档和测试，但他们的代理PR合并率更高。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理改变软件开发方式，需要了解核心与外围开发者如何与编码代理协作。先前研究表明AI工具使用存在差异，但自主编码代理时代这种动态如何演变尚不清楚。

Method: 采用混合定性定量分析方法，对9,427个代理生成的PR进行实证研究，分析核心与外围开发者在使用、审查、修改和验证代理贡献方面的行为差异。

Result: 1) 外围开发者更频繁使用代理且任务分布均匀(错误修复、功能添加、文档、测试)；核心开发者更关注文档和测试，但代理PR合并率更高。2) 核心开发者审查参与度略高，双方都关注可演化性问题。3) 代理PR修改率较低，但修改时双方都进行重构。4) 外围开发者更可能跳过CI检查合并，核心开发者更坚持验证通过。

Conclusion: 开发者经验显著影响与编码代理的协作方式，研究为两类开发者提供了有效协作的见解，揭示了自主编码代理时代的新协作动态。

Abstract: Autonomous AI agents are transforming software development and redefining how developers collaborate with AI. Prior research shows that the adoption and use of AI-powered tools differ between core and peripheral developers. However, it remains unclear how this dynamic unfolds in the emerging era of autonomous coding agents. In this paper, we present the first empirical study of 9,427 agentic PRs, examining how core and peripheral developers use, review, modify, and verify agent-generated contributions prior to acceptance. Through a mix of qualitative and quantitative analysis, we make four key contributions. First, a subset of peripheral developers use agents more often, delegating tasks evenly across bug fixing, feature addition, documentation, and testing. In contrast, core developers focus more on documentation and testing, yet their agentic PRs are frequently merged into the main/master branch. Second, core developers engage slightly more in review discussions than peripheral developers, and both groups focus on evolvability issues. Third, agentic PRs are less likely to be modified, but when they are, both groups commonly perform refactoring. Finally, peripheral developers are more likely to merge without running CI checks, whereas core developers more consistently require passing verification before acceptance. Our analysis offers a comprehensive view of how developer experience shapes integration offer insights for both peripheral and core developers on how to effectively collaborate with coding agents.

</details>


### [26] [Beyond Bug Fixes: An Empirical Investigation of Post-Merge Code Quality Issues in Agent-Generated Pull Requests](https://arxiv.org/abs/2601.20109)
*Shamse Tasnim Cynthia,Al Muttakin,Banani Roy*

Main category: cs.SE

TL;DR: 分析AI代码代理生成的bug修复PR在合并后的代码质量问题，发现合并成功不能可靠反映代码质量，需要系统性的质量检查


<details>
  <summary>Details</summary>
Motivation: 随着AI代码代理的广泛使用，越来越多的代理生成的PR在很少或没有人工干预的情况下被合并。虽然这些PR承诺提高生产力，但合并后的代码质量尚未得到充分研究，先前工作主要依赖基准测试和受控任务，缺乏大规模合并后分析。

Method: 使用AIDev数据集中的1,210个已合并的代理生成bug修复PR（来自Python仓库），通过SonarQube对基础提交和合并提交进行差异分析，识别PR变更新引入的代码质量问题。分析问题频率、密度、严重性和规则级分布，涵盖五个不同的代理。

Result: 结果显示：不同代理的原始问题数量差异在按代码变更量归一化后基本消失，表明更高的问题数量主要由更大的PR驱动；所有代理中，代码异味占主导地位，特别是在关键和主要严重性级别，而bug较少但通常很严重。

Conclusion: 合并成功不能可靠反映合并后的代码质量，突显了对代理生成的bug修复PR进行系统性质量检查的必要性。

Abstract: The increasing adoption of AI coding agents has increased the number of agent-generated pull requests (PRs) merged with little or no human intervention. Although such PRs promise productivity gains, their post-merge code quality remains underexplored, as prior work has largely relied on benchmarks and controlled tasks rather than large-scale post-merge analyses. To address this gap, we analyze 1,210 merged agent-generated bug-fix PRs from Python repositories in the AIDev dataset. Using SonarQube, we perform a differential analysis between base and merged commits to identify code quality issues newly introduced by PR changes. We examine issue frequency, density, severity, and rule-level prevalence across five agents. Our results show that apparent differences in raw issue counts across agents largely disappear after normalizing by code churn, indicating that higher issue counts are primarily driven by larger PRs. Across all agents, code smells dominate, particularly at critical and major severities, while bugs are less frequent but often severe. Overall, our findings show that merge success does not reliably reflect post-merge code quality, highlighting the need for systematic quality checks for agent-generated bug-fix PRs.

</details>


### [27] [Usage, Effects and Requirements for AI Coding Assistants in the Enterprise: An Empirical Study](https://arxiv.org/abs/2601.20112)
*Maja Vukovic,Rangeet Pan,Tin Kam Ho,Rahul Krishna,Raju Pavuluri,Michele Merler*

Main category: cs.SE

TL;DR: 对57名开发者和35份用户调查的分析显示，AI编码助手在提高生产力方面表现良好，但在企业级应用、代码质量、安全性和集成方面仍需改进。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在软件工程任务中的应用日益广泛，需要评估这些AI编码助手是否准备好应对真实世界项目和企业用例，以及它们如何影响现有的软件工程流程和用户体验。

Method: 1. 调查了来自不同领域、具有不同软件工程技能的57名开发者关于他们使用AI编码助手和CodeLLMs的经验；2. 回顾了35份关于专业人士和学生使用AI编码助手和CodeLLMs的用户调查。

Result: 研究发现AI编码助手在提高开发效率方面有积极作用，但在企业级应用场景中存在局限性，包括代码质量、安全性、集成性和可靠性方面的挑战。

Conclusion: 基于研究发现和现有调查分析，讨论了AI驱动的编码助手需要满足的要求，包括更好的企业集成、代码质量保证、安全性和可靠性等方面的改进。

Abstract: The rise of large language models (LLMs) has accelerated the development of automated techniques and tools for supporting various software engineering tasks, e.g., program understanding, code generation, software testing, and program repair. As CodeLLMs are being employed toward automating these tasks, one question that arises, especially in enterprise settings, is whether these coding assistants and the code LLMs that power them are ready for real-world projects and enterprise use cases, and how do they impact the existing software engineering process and user experience. In this paper we survey 57 developers from different domains and with varying software engineering skill about their experience with AI coding assistants and CodeLLMs. We also reviewed 35 user surveys on the usage, experience and expectations of professionals and students using AI coding assistants and CodeLLMs. Based on our study findings and analysis of existing surveys, we discuss the requirements for AI-powered coding assistants.

</details>


### [28] [Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization](https://arxiv.org/abs/2601.20147)
*Saima Afrin,Zaiyu Cheng,Tushar Sharma,Alexander Serebrenik,Massimiliano Di Penta,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 系统研究了系统提示对指令调优语言模型在代码生成任务中的影响，发现模型规模越大，系统提示的影响越显著，少样本提示能减少这种影响，且不同编程语言对提示的敏感度不同。


<details>
  <summary>Details</summary>
Motivation: 虽然指令调优语言模型在代码生成方面表现出色，但系统提示对通用ILMs和专用CLMs性能的影响尚未得到充分研究，本研究旨在填补这一空白。

Method: 通过系统评估框架，考察不同详细程度的系统提示、模型规模、提示策略（零样本vs少样本）和编程语言对ILMs和CLMs在代码生成任务中的影响，共测试120种模型配置。

Result: 发现三个关键结果：1）系统提示的影响随模型规模增大而增强；2）少样本提示相比零样本能减少系统提示的影响；3）编程语言影响显著，Java比Python对系统提示变化更敏感。

Conclusion: 系统提示对指令调优语言模型的代码生成性能有重要影响，这种影响受模型规模、提示策略和编程语言等因素调节，为优化代码生成系统提供了重要指导。

Abstract: Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.

</details>


### [29] [How do Agents Refactor: An Empirical Study](https://arxiv.org/abs/2601.20160)
*Lukas Ottenhof,Daniel Penner,Abram Hindle,Thibaud Lutellier*

Main category: cs.SE

TL;DR: 该研究首次分析了Java代码重构中AI代理的表现，对比了AI代理与开发者的重构行为，发现AI代理主要进行注解相关的重构，而开发者则进行更多结构性改进，只有Cursor模型在重构后代码异味有显著增加。


<details>
  <summary>Details</summary>
Motivation: 随着Claude Code、GitHub Copilot等软件开发代理越来越多地集成到开发者工作流中，虽然已有研究评估了这些代理在代码补全和任务自动化方面的能力，但缺乏对它们在Java重构实践中表现的研究，包括它们进行何种类型的重构变更以及对代码质量的影响。

Method: 研究分析了AI代理和开发者各自86个项目中的重构拉取请求，使用RefactoringMiner识别重构类型，使用DesigniteJava 3.0检测重构前后的代码异味，对比了两组在重构类型和代码质量影响上的差异。

Result: AI代理的重构主要集中在注解变更（前5种最常见重构类型都是注解相关的），而开发者则进行更多样化的结构性改进。尽管重构类型存在差异，但只有Cursor模型显示出重构后代码异味有统计显著性的增加。

Conclusion: AI代理在Java重构中的行为与开发者有显著差异，主要关注注解层面的变更而非结构性改进，且只有特定模型（Cursor）对代码质量产生负面影响，这为AI辅助重构工具的开发和使用提供了重要参考。

Abstract: Software development agents such as Claude Code, GitHub Copilot, Cursor Agent, Devin, and OpenAI Codex are being increasingly integrated into developer workflows. While prior work has evaluated agent capabilities for code completion and task automation, there is little work investigating how these agents perform Java refactoring in practice, the types of changes they make, and their impact on code quality. In this study, we present the first analysis of agentic refactoring pull requests in Java, comparing them to developer refactorings across 86 projects per group. Using RefactoringMiner and DesigniteJava 3.0, we identify refactoring types and detect code smells before and after refactoring commits. Our results show that agent refactorings are dominated by annotation changes (the 5 most common refactoring types done by agents are annotation related), in contrast to the diverse structural improvements typical of developers. Despite these differences in refactoring types, we find Cursor to be the only model to show a statistically significant increase in refactoring smells.

</details>


### [30] [Who Writes the Docs in SE 3.0? Agent vs. Human Documentation Pull Requests](https://arxiv.org/abs/2601.20171)
*Kazuma Yamasaki,Joseph Ayobami Joshua,Tasha Settewong,Mahmoud Alfadel,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: AI代理在软件开发中大量提交文档相关的PR，但人类审查不足，引发文档质量担忧


<details>
  <summary>Details</summary>
Motivation: 随着SE3.0时代到来，AI代理越来越多地参与软件开发任务，但它们在文档任务中的角色尚未充分研究。文档对软件理解和使用至关重要，需要了解AI代理的文档贡献程度以及人类开发者的审查和干预情况，以评估委托工作给AI代理的风险。

Method: 使用AIDev工具分析了1,997个由AI代理和人类开发者提交的文档相关PR（创建或修改项目文档的PR）。

Result: AI代理提交的文档相关PR数量显著多于人类。AI代理撰写的文档编辑通常很少经过人类后续修改就被集成，这引发了审查实践和AI生成文档可靠性的担忧。

Conclusion: 虽然AI代理已经在文档工作流中做出重要贡献，但研究结果表明SE3.0时代文档质量保证和人类-AI协作面临新兴挑战。

Abstract: As software engineering moves toward SE3.0, AI agents are increasingly used to carry out development tasks and contribute changes to software projects. It is therefore important to understand the extent of these contributions and how human developers review and intervene, since these factors shape the risks of delegating work to AI agents. While recent studies have examined how AI agents support software development tasks (e.g., code generation, issue resolution, and PR automation), their role in documentation tasks remains underexplored-even though documentation is widely consumed and shapes how developers understand and use software.
  Using the AIDev, we analyze 1,997 documentation-related pull requests (PRs) authored by AI agents and human developers, where documentation PRs are those that create or modify project documentation artifacts. We find that AI agents submit substantially more documentation-related PRs than humans in the studied repositories. We further observe that agent-authored documentation edits are typically integrated with little follow-up modification from humans, raising concerns about review practices and the reliability of agent-generated documentation. Overall, while AI agents already contribute substantially to documentation workflows, our results suggest concerns for emerging challenges for documentation quality assurance and human-AI collaboration in SE3.0.

</details>


### [31] [Control Models for In-IDE Code Completion](https://arxiv.org/abs/2601.20223)
*Aral de Moor,Yana Hrynevich,Hleb Badzeika,Vladyslav Furda,Marko Kojic,Artem Savelev,Kostadin Cvejoski,Darya Rovdo,Ekaterina Garanina*

Main category: cs.SE

TL;DR: 该论文介绍了用于JetBrains IDE中LLM代码补全的控制模型，通过机器学习分类器触发推理并过滤生成建议，以提高与用户的匹配度并减少不必要的请求。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的代码补全在IDE中可能产生不相关或不必要的建议，需要更智能的触发和过滤机制来提升用户体验和系统效率。

Method: 使用提升（boosting）和基于Transformer的架构，在真实代码补全数据集（n=98用户）上进行离线评估，并在多种语法多样化的语言上测试分类性能，最后在生产环境中进行A/B测试。

Result: 提升方法在离线分类性能上表现良好，生产环境A/B测试显示提高了补全效率和质量的指标。

Conclusion: 辅助模型在IDE中智能集成LLM驱动功能具有潜力，为未来研究方向提供了启示，并指出了开放性问题。

Abstract: We introduce control models for LLM-powered code completion in JetBrains IDEs: ML classifiers which trigger inference and filter the generated suggestions to better align them with users and reduce unnecessary requests. To this end, we evaluate boosting- and transformer-based architectures on an offline dataset of real code completions with n=98 users. We further evaluate the offline classification performance of our boosting-based approach on a range of syntactically diverse languages; and perform an A/B study in a production environment where they improve completion efficiency and quality metrics. With this study, we hope to demonstrate the potential in using auxiliary models for smarter in-IDE integration of LLM-driven features, highlight fruitful future directions, and open problems.

</details>


### [32] [On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents](https://arxiv.org/abs/2601.20404)
*Jai Lal Lulla,Seyedmoein Mohsenimofidi,Matthias Galster,Jie M. Zhang,Sebastian Baltes,Christoph Treude*

Main category: cs.SE

TL;DR: 研究AGENTS.md文件对AI编程代理在GitHub PR中运行效率和token消耗的影响，发现AGENTS.md能显著降低运行时间和token使用量


<details>
  <summary>Details</summary>
Motivation: AI编程代理（如Codex、Claude Code）越来越多地用于自主贡献软件仓库，但人们对仓库级配置工件如何影响代理的操作效率知之甚少

Method: 分析10个仓库和124个PR，在有和没有AGENTS.md文件的两种条件下执行代理，测量执行时间和token使用量

Result: AGENTS.md文件的存在与较低的中位运行时间（Δ28.64%）和减少的输出token消耗（Δ16.58%）相关，同时保持可比较的任务完成行为

Conclusion: 讨论了AI编程代理配置和部署的实际意义，并概述了关于仓库级指令在塑造AI编程代理行为、效率和集成方面的更广泛研究议程

Abstract: AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS$.$md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS$.$md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS$.$md is associated with a lower median runtime ($Δ28.64$%) and reduced output token consumption ($Δ16.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.

</details>


### [33] [DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning](https://arxiv.org/abs/2601.20615)
*Yanlin Wang,Jiadong Wu,Tianyue Jiang,Mingwei Liu,Jiachi Chen,Chong Wang,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: DrainCode是一种针对基于RAG的代码生成系统的对抗攻击方法，通过毒化检索上下文迫使LLM生成更长输出，从而显著增加GPU延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码生成方面表现出色，但LLM推理的计算成本（延迟和能耗）在安全领域关注有限。本文旨在探索针对RAG代码生成系统计算效率的对抗攻击。

Method: 采用基于突变的策略毒化检索上下文，迫使LLM生成显著更长的输出，从而增加GPU延迟和能耗。评估了该方法在多个模型上的有效性。

Result: DrainCode实现了高达85%的延迟增加、49%的能耗增加，以及超过3倍的输出长度增加。攻击在不同提示策略下具有泛化性，且对多种防御措施有效。

Conclusion: DrainCode是一种增加LLM计算开销的潜在方法，有助于在资源受限环境中评估LLM安全性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.

</details>


### [34] [Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model](https://arxiv.org/abs/2601.20662)
*Julien Malka,Arnout Engelen*

Main category: cs.SE

TL;DR: Lila是一个针对功能性包管理模型的去中心化可重现性评估系统，通过分布式报告构建结果并聚合到可重现性数据库中，解决了大规模软件可重现性监控的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代软件工程中确保软件构建产物的完整性日益重要，但大规模采用可重现构建面临两大挑战：在庞大软件集合中实现高可重现率，以及建立能够大规模运行的可重现性监控基础设施。虽然已有研究表明高可重现率可以实现，但有效的可重现性监控问题仍未解决。

Method: 提出了Lila系统，这是一个针对功能性包管理模型的去中心化可重现性评估系统。它支持分布式报告构建结果，并将这些结果聚合到一个可重现性数据库中。

Result: Lila系统能够为从业者和未来的实证构建可重现性研究提供支持，通过分布式监控机制解决了大规模可重现性评估的基础设施问题。

Conclusion: Lila系统解决了可重现性监控的关键挑战，为软件构建完整性提供了有效的分布式监控解决方案，有助于提高软件分发的透明度和信任度。

Abstract: Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.
  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.
  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies.

</details>


### [35] [Context-Augmented Code Generation Using Programming Knowledge Graphs](https://arxiv.org/abs/2601.20810)
*Shahd Seddik,Fahd Seddik,Iman Saberi,Fatemeh Fard,Minh Hieu Huynh,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 提出编程知识图谱（PKG）方法，通过语义表示和细粒度检索增强代码生成，结合树剪枝和重排序机制减少幻觉，在HumanEval和MBPP基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成方面表现出色，但在处理复杂问题时存在局限。检索增强生成（RAG）通过整合外部知识来缓解这一问题，但现有检索模型经常错过相关上下文，生成模型则容易产生无关数据的幻觉。

Method: 提出编程知识图谱（PKG）方法，用于代码和文本的语义表示和细粒度检索。通过树剪枝提高检索精度，通过重排序机制整合非RAG解决方案来减少幻觉。将外部数据结构化为更细粒度的节点以改进检索粒度。

Result: 在HumanEval和MBPP基准测试中，pass@1准确率提升最高达20%，在MBPP上比基线方法提升34%。PKG方法结合重排序器能有效解决复杂问题，同时对原本无需RAG就能正确解决的问题影响最小。

Conclusion: 提出的PKG方法和重排序机制能有效增强代码生成能力，提高检索精度并减少幻觉，在处理复杂编程问题时表现出显著优势。

Abstract: Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Distributional value gradients for stochastic environments](https://arxiv.org/abs/2601.20071)
*Baptiste Debes,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: 提出Distributional Sobolev Training方法，在连续状态-动作空间中建模状态-动作值函数及其梯度的分布，以解决现有梯度正则化方法在随机/噪声环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有梯度正则化值学习方法（如MAGE）在随机或噪声环境中表现不佳，限制了其实际应用。需要开发能够更好处理环境随机性的方法。

Method: 扩展连续状态-动作空间上的分布强化学习，不仅建模标量状态-动作值函数的分布，还建模其梯度的分布。使用条件变分自编码器实现一步世界模型，采用最大切片最大均值差异实例化分布贝尔曼算子。

Result: 证明了Sobolev增强贝尔曼算子是收缩算子且有唯一不动点，揭示了梯度感知RL中平滑性的基本权衡。在简单随机RL问题和多个MuJoCo环境中验证了方法的有效性。

Conclusion: Distributional Sobolev Training方法通过建模值函数及其梯度的分布，能够更好地处理随机环境，提高了梯度正则化值学习方法的鲁棒性和适用性。

Abstract: Gradient-regularized value learning methods improve sample efficiency by leveraging learned models of transition dynamics and rewards to estimate return gradients. However, existing approaches, such as MAGE, struggle in stochastic or noisy environments, limiting their applicability. In this work, we address these limitations by extending distributional reinforcement learning on continuous state-action spaces to model not only the distribution over scalar state-action value functions but also over their gradients. We refer to this approach as Distributional Sobolev Training. Inspired by Stochastic Value Gradients (SVG), our method utilizes a one-step world model of reward and transition distributions implemented via a conditional Variational Autoencoder (cVAE). The proposed framework is sample-based and employs Max-sliced Maximum Mean Discrepancy (MSMMD) to instantiate the distributional Bellman operator. We prove that the Sobolev-augmented Bellman operator is a contraction with a unique fixed point, and highlight a fundamental smoothness trade-off underlying contraction in gradient-aware RL. To validate our method, we first showcase its effectiveness on a simple stochastic reinforcement learning toy problem, then benchmark its performance on several MuJoCo environments.

</details>


### [37] [In-Context Reinforcement Learning From Suboptimal Historical Data](https://arxiv.org/abs/2601.20116)
*Juncheng Dong,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出Decision Importance Transformer (DIT)框架，用于上下文强化学习，通过训练基于Transformer的价值函数评估次优行为策略的优势函数，然后使用加权最大似然估计训练策略，在包含次优历史数据的离线数据集上实现优越性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在上下文学习方面取得了显著成功，但在上下文强化学习(ICRL)中，当离线数据集包含从次优行为策略采样的轨迹时，标准的自回归训练相当于模仿学习，会导致次优性能。需要一种方法能够从次优历史数据中学习并改进策略。

Method: 提出DIT框架：1) 训练基于Transformer的价值函数来估计行为策略的优势函数；2) 使用加权最大似然估计训练基于Transformer的策略，权重基于训练好的价值函数构建，以引导次优策略向最优策略转变。

Result: 在bandit和马尔可夫决策过程问题上进行了广泛实验，结果显示DIT实现了优越性能，特别是在离线数据集包含次优历史数据的情况下表现突出。

Conclusion: DIT框架能够有效处理包含次优轨迹的离线数据集，通过模拟actor-critic算法的上下文方式，在上下文强化学习中实现了从次优数据中学习并改进策略的能力。

Abstract: Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.

</details>


### [38] [HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH](https://arxiv.org/abs/2601.20255)
*Yueyang Wang,Jiawei Fu,Baolong Bi,Xili Wang,Xiaoqing Liu*

Main category: cs.LG

TL;DR: 提出HE-SNR指标，基于熵压缩假设优化LLM在软件工程任务中的中间训练，解决传统指标在长上下文下的局限性。


<details>
  <summary>Details</summary>
Motivation: SWE-bench作为软件工程任务评估的黄金标准，但LLM的相关能力主要在中间训练阶段获得。当前缺乏有效的指标来指导中间训练，传统指标如困惑度受"长上下文税"影响，与下游SWE性能相关性弱。

Method: 1. 引入严格的数据过滤策略；2. 提出熵压缩假设，将智能重新定义为将不确定性结构化为低阶熵压缩状态的能力；3. 基于细粒度熵分析制定HE-SNR（高熵信噪比）指标；4. 在工业级MoE模型上验证，测试不同上下文窗口（32K/128K）。

Result: HE-SNR指标在工业级MoE模型上表现出卓越的鲁棒性和预测能力，优于传统指标如困惑度。

Conclusion: 该工作为优化LLM在复杂工程领域中的潜在能力提供了理论基础和实用工具，通过HE-SNR指标解决了中间训练指导的空白。

Abstract: SWE-bench has emerged as the premier benchmark for evaluating Large Language Models on complex software engineering tasks. While these capabilities are fundamentally acquired during the mid-training phase and subsequently elicited during Supervised Fine-Tuning (SFT), there remains a critical deficit in metrics capable of guiding mid-training effectively. Standard metrics such as Perplexity (PPL) are compromised by the "Long-Context Tax" and exhibit weak correlation with downstream SWE performance. In this paper, we bridge this gap by first introducing a rigorous data filtering strategy. Crucially, we propose the Entropy Compression Hypothesis, redefining intelligence not by scalar Top-1 compression, but by the capacity to structure uncertainty into Entropy-Compressed States of low orders ("reasonable hesitation"). Grounded in this fine-grained entropy analysis, we formulate a novel metric, HE-SNR (High-Entropy Signal-to-Noise Ratio). Validated on industrial-scale Mixture-of-Experts (MoE) models across varying context windows (32K/128K), our approach demonstrates superior robustness and predictive power. This work provides both the theoretical foundation and practical tools for optimizing the latent potential of LLMs in complex engineering domains.

</details>


### [39] [Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery](https://arxiv.org/abs/2601.20193)
*Zhipeng Zhang,Wenting Ma,Kai Li,Meng Guo,Lei Yang,Wei Yu,Hongji Cui,Yichen Zhang,Mo Zhang,Jinzhe Lin,Zhenjie Yao*

Main category: cs.LG

TL;DR: 提出基于元认知的强化学习框架，通过内部可靠性信号评估、调节和恢复学习行为，解决传统鲁棒强化学习方法对噪声过度保守或灾难性失败的问题。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒强化学习方法通常只关注抑制不可靠经验或损坏奖励，但缺乏对自身学习过程可靠性的推理能力，导致要么对噪声过度反应变得过于保守，要么在不确定性累积时发生灾难性失败。

Method: 提出元认知强化学习框架，引入基于价值预测误差稳定性(VPES)驱动的元信任变量，通过故障安全调节和渐进信任恢复来调制学习动态。

Result: 在具有奖励损坏的连续控制基准测试中，启用恢复的元认知控制相比强鲁棒基线实现了更高的平均回报，并显著减少了后期训练失败。

Conclusion: 元认知方法能够有效提升强化学习在噪声环境中的鲁棒性，通过内部可靠性评估和自适应调节机制防止灾难性失败。

Abstract: Robust reinforcement learning methods typically focus on suppressing unreliable experiences or corrupted rewards, but they lack the ability to reason about the reliability of their own learning process. As a result, such methods often either overreact to noise by becoming overly conservative or fail catastrophically when uncertainty accumulates.
  In this work, we propose a meta-cognitive reinforcement learning framework that enables an agent to assess, regulate, and recover its learning behavior based on internally estimated reliability signals. The proposed method introduces a meta-trust variable driven by Value Prediction Error Stability (VPES), which modulates learning dynamics via fail-safe regulation and gradual trust recovery.
  Experiments on continuous-control benchmarks with reward corruption demonstrate that recovery-enabled meta-cognitive control achieves higher average returns and significantly reduces late-stage training failures compared to strong robustness baselines.

</details>


### [40] [Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning](https://arxiv.org/abs/2601.20209)
*Jinyang Wu,Shuo Yang,Changpeng Yang,Yuhao Shen,Shuai Zhang,Zhengqi Wen,Jianhua Tao*

Main category: cs.LG

TL;DR: Spark框架通过关键状态动态分支实现策略感知的探索，在有限资源下提升长时程任务的强化学习效率


<details>
  <summary>Details</summary>
Motivation: 现有方法在长时程任务中通常盲目扩大rollout规模并均匀分配计算资源，导致在平凡步骤上浪费大量计算预算，同时无法保证样本质量

Method: 提出Spark框架，在关键决策状态选择性分支进行资源高效探索，利用智能体内在决策信号激活自适应分支探索，减少对人类先验的依赖

Result: 在多样化任务（如具身规划）中，Spark以显著更少的训练样本实现更高的成功率，在未见场景中表现出鲁棒的泛化能力

Conclusion: Spark通过策略感知的关键状态分支探索，实现了精确的资源分配，优先考虑采样质量而非盲目覆盖，使智能体能够自主扩展探索并实现更强的泛化

Abstract: Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \textbf{Spark} (\textbf{S}trategic \textbf{P}olicy-\textbf{A}ware explo\textbf{R}ation via \textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.

</details>


### [41] [C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding](https://arxiv.org/abs/2601.20257)
*Jinren Ding,Xuejian Xu,Shen Jiang,Zhitong Hao,Jinhui Yang,Peng Jiang*

Main category: cs.LG

TL;DR: C2框架通过交叉学习块和约束感知损失增强决策变换器，解决自动出价中跨序列相关性建模不足和次优行为学习问题，在AuctionNet数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 决策变换器在自动出价中虽然能捕捉时间依赖，但存在两个关键局限：状态、动作和回报序列之间的跨相关性建模不足，以及无法区分地学习最优/次优行为。

Method: 提出C2框架，包含两个核心创新：1) 通过交叉注意力机制的交叉学习块增强序列间相关性建模；2) 结合预算和CPA约束的约束感知损失，选择性学习最优轨迹。

Result: 在AuctionNet数据集上的离线评估显示，C2在不同预算设置下均取得性能提升（最高比最先进的GAVE方法提升3.23%），消融研究验证了CLB和CL的互补协同作用。

Conclusion: C2框架通过增强跨序列相关性建模和选择性学习机制，显著提升了自动出价性能，证明了其在自动出价任务中的优越性。

Abstract: Decision Transformer (DT) shows promise for generative auto-bidding by capturing temporal dependencies, but suffers from two critical limitations: insufficient cross-correlation modeling among state, action, and return-to-go (RTG) sequences, and indiscriminate learning of optimal/suboptimal behaviors. To address these, we propose C2, a novel framework enhancing DT with two core innovations: (1) a Cross Learning Block (CLB) via cross-attention to strengthen inter-sequence correlation modeling; (2) a Constraint-aware Loss (CL) incorporating budget and Cost-Per-Acquisition (CPA) constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains (up to 3.23\% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify the complementary synergy of CLB and CL, confirming C2's superiority in auto-bidding. The code for reproducing our results is available at: https://github.com/Dingjinren/C2.

</details>


### [42] [Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction](https://arxiv.org/abs/2601.20299)
*Tianyi Alex Qiu,Micah Carroll,Cameron Allen*

Main category: cs.LG

TL;DR: 提出基于同伴预测（peer prediction）的LLM评估与后训练方法，利用博弈论激励兼容性，无需真实标签即可奖励诚实信息性回答，有效抵抗欺骗行为


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估和后训练依赖监督信号，但对于困难任务往往缺乏强监督，导致模型可能利用不完美监督进行欺骗，产生误导性结果。需要一种能在弱监督下可靠评估和训练的方法

Method: 借鉴博弈论中的机制设计研究，引入同伴预测方法。该方法基于相互可预测性度量，无需真实标签，通过奖励诚实和信息性回答来对抗欺骗性和无信息性回答。包含理论保证和实证验证

Result: 方法有效抵抗欺骗：1）使用同伴预测奖励训练的8B模型能恢复因恶意微调而下降的诚实性；2）发现同伴预测存在逆缩放特性，专家与参与者能力差距越大，抵抗欺骗能力越强；3）LLM-as-a-Judge在面对5-20倍大小的欺骗模型时表现差于随机猜测，而同伴预测在100倍大小差距下仍能有效工作

Conclusion: 同伴预测方法为LLM评估和后训练提供了一种无需强监督的可靠方案，特别适用于评估前沿模型。其逆缩放特性使得弱监督也能可靠评估强模型，为解决LLM评估中的欺骗问题提供了新途径

Abstract: The evaluation and post-training of large language models (LLMs) rely on supervision, but strong supervision for difficult tasks is often unavailable, especially when evaluating frontier models. In such cases, models are demonstrated to exploit evaluations built on such imperfect supervision, leading to deceptive results. However, underutilized in LLM research, a wealth of mechanism design research focuses on game-theoretic incentive compatibility, i.e., eliciting honest and informative answers with weak supervision. Drawing from this literature, we introduce the peer prediction method for model evaluation and post-training. It rewards honest and informative answers over deceptive and uninformative ones, using a metric based on mutual predictability and without requiring ground truth labels. We demonstrate the method's effectiveness and resistance to deception, with both theoretical guarantees and empirical validation on models with up to 405B parameters. We show that training an 8B model with peer prediction-based reward recovers most of the drop in truthfulness due to prior malicious finetuning, even when the reward is produced by a 0.135B language model with no finetuning. On the evaluation front, in contrast to LLM-as-a-Judge which requires strong and trusted judges, we discover an inverse scaling property in peer prediction, where, surprisingly, resistance to deception is strengthened as the capability gap between the experts and participants widens, enabling reliable evaluation of strong models with weak supervision. In particular, LLM-as-a-Judge become worse than random guess when facing deceptive models 5-20x the judge's size, while peer prediction thrives when such gaps are large, including in cases with over 100x size difference.

</details>


### [43] [LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning](https://arxiv.org/abs/2601.20375)
*Wei Huang,Anda Cheng,Yinggui Wang,Lei Wang,Tao Wei*

Main category: cs.LG

TL;DR: LLM-AutoDP：利用LLM作为智能体自动生成和优化数据处理策略的框架，通过迭代学习和加速技术实现高效数据处理，无需人工干预或访问原始数据。


<details>
  <summary>Details</summary>
Motivation: 针对领域特定数据中大量低质量样本的问题，传统人工数据处理方法成本高且在高隐私领域（如医疗）存在隐私风险，需要实现无需接触原始数据的自动化数据处理。

Method: 提出LLM-AutoDP框架，利用LLM作为智能体自动生成多个候选策略，通过反馈信号和比较评估进行迭代优化。引入三种加速技术：分布保持采样、处理目标选择（二元分类器识别低质量样本）、缓存重用机制。

Result: 使用该框架处理的数据训练的模型相比未处理数据训练的模型胜率超过80%；相比基于LLM智能体的AutoML基线胜率约65%；加速技术将总搜索时间减少高达10倍。

Conclusion: LLM-AutoDP框架在保持数据隐私的同时，实现了高效自动化的数据处理策略优化，显著提升了模型性能并大幅减少了计算成本。

Abstract: Large Language Models (LLMs) can be fine-tuned on domain-specific data to enhance their performance in specialized fields. However, such data often contains numerous low-quality samples, necessitating effective data processing (DP). In practice, DP strategies are typically developed through iterative manual analysis and trial-and-error adjustment. These processes inevitably incur high labor costs and may lead to privacy issues in high-privacy domains like healthcare due to direct human access to sensitive data. Thus, achieving automated data processing without exposing the raw data has become a critical challenge. To address this challenge, we propose LLM-AutoDP, a novel framework that leverages LLMs as agents to automatically generate and optimize data processing strategies. Our method generates multiple candidate strategies and iteratively refines them using feedback signals and comparative evaluations. This iterative in-context learning mechanism enables the agent to converge toward high-quality processing pipelines without requiring direct human intervention or access to the underlying data. To further accelerate strategy search, we introduce three key techniques: Distribution Preserving Sampling, which reduces data volume while maintaining distributional integrity; Processing Target Selection, which uses a binary classifier to identify low-quality samples for focused processing; Cache-and-Reuse Mechanism}, which minimizes redundant computations by reusing prior processing results. Results show that models trained on data processed by our framework achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines based on LLM agents, LLM-AutoDP achieves approximately a 65% win rate. Moreover, our acceleration techniques reduce the total searching time by up to 10 times, demonstrating both effectiveness and efficiency.

</details>


### [44] [Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs](https://arxiv.org/abs/2601.20420)
*Yuhang Liu,Erdun Gao,Dong Gong,Anton van den Hengel,Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: 论文提出Concept Component Analysis (ConCA)框架，通过线性解混从LLM表示中恢复概念的后验对数，为概念提取提供理论依据，优于现有的稀疏自编码器方法。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器(SAEs)虽然能提取可解释概念，但缺乏理论基础，导致方法设计和评估标准存在困难。需要建立LLM表示与人类可解释概念之间的理论对应关系。

Method: 提出Concept Component Analysis (ConCA)框架，将LLM表示建模为概念后验对数的线性混合。通过无监督线性解混过程恢复每个概念的后验对数。特别提出稀疏ConCA变体，利用稀疏先验解决解混问题的病态性。

Result: 实现了12个稀疏ConCA变体，在多个LLM上成功提取有意义的可解释概念，相比SAEs具有理论支持的优势。

Conclusion: ConCA为LLM概念提取提供了理论框架，解决了SAEs的理论模糊性问题，通过线性解混方法能够更可靠地提取人类可理解的概念。

Abstract: Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs' activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical progress, SAEs suffer from a fundamental theoretical ambiguity: the well-defined correspondence between LLM representations and human-interpretable concepts remains unclear. This lack of theoretical grounding gives rise to several methodological challenges, including difficulties in principled method design and evaluation criteria. In this work, we show that, under mild assumptions, LLM representations can be approximated as a {linear mixture} of the log-posteriors over concepts given the input context, through the lens of a latent variable model where concepts are treated as latent variables. This motivates a principled framework for concept extraction, namely Concept Component Analysis (ConCA), which aims to recover the log-posterior of each concept from LLM representations through a {unsupervised} linear unmixing process. We explore a specific variant, termed sparse ConCA, which leverages a sparsity prior to address the inherent ill-posedness of the unmixing problem. We implement 12 sparse ConCA variants and demonstrate their ability to extract meaningful concepts across multiple LLMs, offering theory-backed advantages over SAEs.

</details>


### [45] [Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models](https://arxiv.org/abs/2601.20687)
*Zhiqiang Kou,Junyang Chen,Xin-Qiang Cai,Xiaobo Xia,Ming-Kun Xie,Dong-Dong Wu,Biao Liu,Yuheng Jia,Xin Geng,Masashi Sugiyama,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出一种正例-未标注强化学习蒸馏方法，用于本地化小模型部署，无需人工标注偏好或奖励模型，通过教师模型生成锚点响应，学生模型采样候选，进行锚点条件自排序来诱导偏好信号。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、成本和延迟限制，本地部署小模型越来越普遍，但大多数实际管道只进行监督微调，无法达到强化学习对齐阶段。RL对齐通常需要昂贵的人工偏好标注或依赖高质量奖励模型，这两种方式都不适合本地部署环境。

Method: 提出正例-未标注强化学习蒸馏方法：1）对每个提示查询教师模型一次获得锚点响应；2）本地采样多个学生候选响应；3）进行锚点条件自排序来诱导成对或列表偏好；4）通过直接偏好优化或组相对策略优化实现完全本地训练循环。

Result: 理论分析表明该方法诱导的偏好信号具有顺序一致性和集中于近最优候选的特性，支持偏好优化的稳定性。实验证明该方法在低成本设置下实现了一致的强大性能。

Conclusion: 该方法填补了本地小模型部署中强化学习对齐的空白，无需人工偏好标注或奖励模型，通过教师模型的偏好优化能力蒸馏到本地可训练学生模型，实现了高效低成本的本地对齐。

Abstract: Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher's preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.

</details>


### [46] [Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions](https://arxiv.org/abs/2601.20714)
*Raul de la Rosa,Ivana Dusparic,Nicolas Cardozo*

Main category: cs.LG

TL;DR: MORPHIN是一个自适应的Q学习框架，能够在非平稳环境中动态调整学习参数，适应奖励函数变化和动作空间扩展，防止灾难性遗忘，在Gridworld和交通信号控制任务中表现优于标准Q学习。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理在现实应用中面临非平稳环境挑战，特别是奖励函数变化和动作空间扩展时，传统方法需要完全重新训练，效率低下且容易发生灾难性遗忘。

Method: 提出MORPHIN框架，集成概念漂移检测机制，动态调整学习和探索超参数，适应奖励函数变化和动作空间扩展，同时保留先验策略知识防止灾难性遗忘。

Result: 在Gridworld基准和交通信号控制模拟中验证，MORPHIN相比标准Q学习基线获得更快的收敛速度和持续适应能力，学习效率提升高达1.7倍。

Conclusion: MORPHIN框架有效解决了非平稳环境中的强化学习适应问题，能够在奖励函数变化和动作空间扩展时实现高效在线适应，具有实际应用价值。

Abstract: Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.

</details>


### [47] [Reinforcement Learning via Self-Distillation](https://arxiv.org/abs/2601.20802)
*Jonas Hübotter,Frederike Lübeck,Lejs Behric,Anton Baumann,Marco Bagatella,Daniel Marta,Ido Hakimi,Idan Shenfeld,Thomas Kleine Buening,Carlos Guestrin,Andreas Krause*

Main category: cs.LG

TL;DR: SDPO是一种强化学习方法，利用丰富的文本反馈（如运行时错误）进行自我蒸馏，提高样本效率和最终准确率，优于仅使用标量奖励的RLVR方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习（RLVR）方法仅从每次尝试的标量结果奖励中学习，存在严重的信用分配瓶颈。许多可验证环境实际上提供了丰富的文本反馈（如运行时错误或评估意见），可以解释失败原因，但这些信息未被充分利用。

Method: 提出自我蒸馏策略优化（SDPO），将标记化的反馈转换为密集学习信号，无需外部教师或显式奖励模型。SDPO将当前模型在反馈条件下的输出视为自我教师，并将其反馈感知的下一个标记预测蒸馏回策略中，利用模型回顾性识别自身错误的能力。

Result: 在科学推理、工具使用和LiveCodeBench v6的竞争性编程任务上，SDPO相比强RLVR基线提高了样本效率和最终准确率。即使在仅返回标量反馈的标准RLVR环境中，SDPO通过使用成功轨迹作为失败尝试的隐式反馈也优于基线。在测试时对单个问题应用SDPO可加速困难二元奖励任务的发现，以3倍更少的尝试达到与best-of-k采样或多轮对话相同的发现概率。

Conclusion: SDPO通过利用丰富的文本反馈进行自我蒸馏，有效解决了RLVR中的信用分配问题，在多种可验证领域显著提升了强化学习性能。

Abstract: Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.

</details>


### [48] [GNN Explanations that do not Explain and How to find Them](https://arxiv.org/abs/2601.20815)
*Steve Azzolin,Stefano Teso,Bruno Lepri,Andrea Passerini,Sagar Malhotra*

Main category: cs.LG

TL;DR: 该论文发现自解释图神经网络（SE-GNNs）存在解释退化问题，即解释可能与模型推理过程完全无关，但现有忠实度指标无法检测这种失败模式，作者提出了一种新的忠实度指标来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 自解释图神经网络（SE-GNNs）的解释对于理解模型内部工作机制和检测敏感属性滥用至关重要。虽然已有研究指出这些解释可能不理想且有误导性，但对其失败案例的特征描述仍然缺乏。本文旨在识别SE-GNN解释的关键失败模式。

Method: 作者识别了SE-GNN解释的退化问题：解释可能与SE-GNN推理标签的方式完全无关。通过实证分析，展示了退化解释既可能被恶意植入（用于隐藏敏感属性的使用），也可能自然出现。为应对此问题，作者提出了一种新的忠实度指标。

Result: 研究发现许多SE-GNNs可以在实现最优真实风险的同时产生退化解释，而大多数现有忠实度指标无法识别这种失败模式。新提出的忠实度指标在恶意和自然设置下都能可靠地将退化解释标记为不忠实。

Conclusion: SE-GNNs存在解释退化问题，这突显了可靠审计的必要性。作者提出的新忠实度指标能够有效检测退化解释，为解决这一关键问题提供了实用工具。

Abstract: Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [49] [Insight Agents: An LLM-Based Multi-Agent System for Data Insights](https://arxiv.org/abs/2601.20048)
*Jincheng Bai,Zhenyu Zhang,Jennifer Zhang,Zhihuai Zhu*

Main category: cs.AI

TL;DR: Insight Agents (IA) 是一个基于LLM的对话式多代理数据洞察系统，为电商卖家提供个性化数据和业务洞察，通过分层多代理架构实现高效信息检索，准确率达90%，延迟P90低于15秒。


<details>
  <summary>Details</summary>
Motivation: 电商卖家面临两大挑战：难以发现和有效利用可用程序和工具；难以理解和利用来自各种工具的丰富数据。因此需要开发一个能够减少卖家决策努力、加速业务决策的系统。

Method: 采用基于计划-执行范式的分层多代理架构：1) 管理器代理使用轻量级编码器-解码器模型进行OOD检测和基于BERT的分类器进行代理路由；2) 两个工作代理：数据呈现代理使用API数据模型进行战略规划，将查询分解为细粒度组件；洞察生成代理动态注入领域知识增强洞察生成。

Result: 系统已在美国亚马逊卖家上线，基于人工评估的准确率达到90%，P90延迟低于15秒，能够有效为卖家提供个性化数据洞察。

Conclusion: Insight Agents系统通过创新的多代理架构和ML解决方案，成功解决了电商卖家在工具利用和数据理解方面的挑战，实现了高准确率和低延迟的自动化数据洞察服务。

Abstract: Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this novel LLM-backed end-to-end agentic system built on a plan-and-execute paradigm and designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 90% based on human evaluation, with latency of P90 below 15s.

</details>


### [50] [Should I Have Expressed a Different Intent? Counterfactual Generation for LLM-Based Autonomous Control](https://arxiv.org/abs/2601.20090)
*Amirmohammad Farzaneh,Salvatore D'Oro,Osvaldo Simeone*

Main category: cs.AI

TL;DR: 提出CCG框架，利用结构因果模型和概率溯因进行反事实推理，为LLM驱动的智能体控制提供形式化可靠性保证


<details>
  <summary>Details</summary>
Motivation: 当LLM智能体将用户意图转化为环境中的计划和行动后，用户可能想知道：如果我用不同的方式表达意图会怎样？需要一种支持反事实推理的框架，并提供形式化可靠性保证。

Method: 将用户、LLM智能体和环境的闭环交互建模为结构因果模型，利用测试时缩放通过概率溯因生成多个候选反事实结果，通过离线校准阶段实现符合性反事实生成，保证高概率包含真实反事实结果

Result: 在无线网络控制用例中展示了CCG的性能，相比简单的重新执行基线方法有显著优势

Conclusion: CCG框架能够为LLM驱动的智能体控制场景提供可靠的反事实推理能力，具有实际应用价值

Abstract: Large language model (LLM)-powered agents can translate high-level user intents into plans and actions in an environment. Yet after observing an outcome, users may wonder: What if I had phrased my intent differently? We introduce a framework that enables such counterfactual reasoning in agentic LLM-driven control scenarios, while providing formal reliability guarantees. Our approach models the closed-loop interaction between a user, an LLM-based agent, and an environment as a structural causal model (SCM), and leverages test-time scaling to generate multiple candidate counterfactual outcomes via probabilistic abduction. Through an offline calibration phase, the proposed conformal counterfactual generation (CCG) yields sets of counterfactual outcomes that are guaranteed to contain the true counterfactual outcome with high probability. We showcase the performance of CCG on a wireless network control use case, demonstrating significant advantages compared to naive re-execution baselines.

</details>


### [51] [Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2601.20221)
*Hang Zhang,Ruheng Wang,Yuelyu Ji,Mingu Kwak,Xizhi Wu,Chenyu Li,Li Zhang,Wenqi Shi,Yifan Peng,Yanshan Wang*

Main category: cs.AI

TL;DR: 提出一个名为$\method$的代理框架，通过训练医疗推理验证器在评估时迭代查询外部医疗语料库，结合工具增强验证和迭代强化学习，显著提升医疗推理准确性并大幅降低采样预算需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗推理基准测试中表现良好，但在临床部署中需要严格验证以确保事实准确性。现有奖励模型方法存在两个局限：只产生标量奖励值而无明确理由，且依赖单次检索无法在验证过程中进行自适应知识访问。

Method: $\method$框架训练医疗推理验证器在评估时迭代查询外部医疗语料库，结合工具增强验证和迭代强化学习范式（仅需轨迹级监督），并采用自适应课程机制动态调整训练数据分布。

Result: 在四个医疗推理基准测试中，$\method$相比现有方法取得显著提升：MedQA准确率提高23.5%，MedXpertQA提高32.0%（相对于基础生成器）。最重要的是，相比先前奖励模型基线，$\method$将采样预算需求降低了8倍。

Conclusion: 基于动态检索证据的验证为构建更可靠的医疗推理系统提供了原则性路径，$\method$框架通过迭代知识访问和自适应训练机制有效解决了现有验证方法的局限性。

Abstract: Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\method$ demonstrates an $\mathbf{8\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.

</details>


### [52] [ECG-Agent: On-Device Tool-Calling Agent for ECG Multi-Turn Dialogue](https://arxiv.org/abs/2601.20323)
*Hyunseung Chung,Jungwoo Oh,Daeun Kyung,Jiho Kim,Yeonsu Kwon,Min-Gyu Kim,Edward Choi*

Main category: cs.AI

TL;DR: ECG-Agent：首个基于LLM的工具调用代理，用于多轮心电图对话，解决现有模型在真实场景中的局限性，包括多轮对话能力、设备端效率和精确ECG测量理解。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在心电图应用上局限于分类、报告生成和单轮问答任务，缺乏真实场景所需的多轮对话能力、设备端效率以及对PQRST间隔等ECG测量的精确理解。

Method: 引入ECG-Agent作为首个基于LLM的工具调用代理，用于多轮ECG对话。同时创建ECG-MTD数据集（包含真实用户-助手多轮对话），并开发从设备端到大型代理的各种规模ECG-Agent。

Result: ECG-Agent在响应准确性上优于基线ECG-LLM。设备端代理在响应准确性、工具调用能力和幻觉评估方面与大型代理表现相当，证明其在真实应用中的可行性。

Conclusion: ECG-Agent成功解决了现有ECG-LLM的局限性，设备端代理的可行性为真实世界应用提供了实用解决方案，ECG-MTD数据集为未来研究提供了有价值的资源。

Abstract: Recent advances in Multimodal Large Language Models have rapidly expanded to electrocardiograms, focusing on classification, report generation, and single-turn QA tasks. However, these models fall short in real-world scenarios, lacking multi-turn conversational ability, on-device efficiency, and precise understanding of ECG measurements such as the PQRST intervals. To address these limitations, we introduce ECG-Agent, the first LLM-based tool-calling agent for multi-turn ECG dialogue. To facilitate its development and evaluation, we also present ECG-Multi-Turn-Dialogue (ECG-MTD) dataset, a collection of realistic user-assistant multi-turn dialogues for diverse ECG lead configurations. We develop ECG-Agents in various sizes, from on-device capable to larger agents. Experimental results show that ECG-Agents outperform baseline ECG-LLMs in response accuracy. Furthermore, on-device agents achieve comparable performance to larger agents in various evaluations that assess response accuracy, tool-calling ability, and hallucinations, demonstrating their viability for real-world applications.

</details>


### [53] [AMA: Adaptive Memory via Multi-Agent Collaboration](https://arxiv.org/abs/2601.20352)
*Weiquan Huang,Zixuan Wang,Hehai Lin,Sudong Wang,Bo Xu,Qian Li,Beier Zhu,Linyi Yang,Chengwei Qin*

Main category: cs.AI

TL;DR: AMA框架通过多智能体协作实现自适应记忆管理，采用分层记忆设计和动态粒度对齐，显著提升长期记忆一致性和检索精度，同时大幅降低token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体记忆系统存在三个主要问题：1）检索粒度固定，无法匹配任务复杂度；2）维护策略积累过重；3）更新机制粗粒度。这导致存储信息与任务推理需求不匹配，且随时间积累逻辑不一致性。

Method: 提出AMA（Adaptive Memory via Multi-Agent Collaboration）框架，通过四个协调智能体管理多粒度记忆：Constructor和Retriever实现多粒度记忆构建和自适应查询路由；Judge验证检索内容相关性和一致性；Refresher检测到逻辑冲突时执行针对性更新或删除过时条目。

Result: 在挑战性长上下文基准测试中，AMA显著优于现有最先进基线，同时相比全上下文方法减少约80%的token消耗，证明了其在保持检索精度和长期记忆一致性方面的有效性。

Conclusion: AMA框架通过多智能体协作和分层记忆设计，有效解决了现有记忆系统的局限性，为LLM智能体提供了更高效、一致的自适应记忆管理方案。

Abstract: The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.

</details>


### [54] [OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution](https://arxiv.org/abs/2601.20380)
*Le Zhang,Yixiong Xiao,Xinjiang Lu,Jingjia Cao,Yusai Zhao,Jingbo Zhou,Lang An,Zikan Feng,Wanxiang Sha,Yu Shi,Congxi Xiao,Jian Xiong,Yankai Zhang,Hua Wu,Haifeng Wang*

Main category: cs.AI

TL;DR: OmegaUse是一个通用的GUI代理模型，支持移动和桌面平台的自主任务执行，采用数据合成框架和两阶段训练策略，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: GUI代理具有让基础模型完成现实世界任务的巨大潜力，能够革新人机交互并提高人类生产力。当前需要构建一个支持跨平台（移动和桌面）的通用GUI代理模型。

Method: 1) 数据构建：结合精心策划的开源数据集和自动化合成框架（自底向上自主探索+自顶向下分类指导生成）；2) 训练：两阶段策略：监督微调建立基本交互语法，然后使用组相对策略优化提升空间定位和序列规划；3) 架构：基于混合专家模型平衡计算效率和推理能力。

Result: OmegaUse在多个GUI基准测试中表现优异：在ScreenSpot-V2上达到96.3%的SOTA分数，在AndroidControl上达到79.1%的步骤成功率。在新提出的OS-Nav基准上，在ChiM-Nav上达到74.24%步骤成功率，在Ubu-Nav上达到55.9%平均成功率。

Conclusion: OmegaUse是一个有效的通用GUI代理模型，通过精心设计的数据构建和训练策略，在跨平台GUI任务执行中表现出强大的能力，为实际应用提供了有前景的解决方案。

Abstract: Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.

</details>


### [55] [CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning](https://arxiv.org/abs/2601.20467)
*Zhenxuan Fan,Jie Cao,Yang Dai,Zheqi Lv,Wenqiao Zhang,Zhongle Xie,Peng LU,Beng Chin Ooi*

Main category: cs.AI

TL;DR: CtrlCoT是一个双粒度CoT压缩框架，通过分层推理抽象、逻辑保持蒸馏和分布对齐生成，在减少30.7%token的同时提升推理准确率7.6个百分点。


<details>
  <summary>Details</summary>
Motivation: 传统CoT提示虽然能提升LLM推理能力，但会产生冗长的推理轨迹，导致高延迟和内存成本。现有压缩方法要么在语义层面过于保守，要么在token层面过于激进，容易丢失关键推理线索并降低准确性。

Method: 提出CtrlCoT框架，包含三个核心组件：1) 分层推理抽象：生成多粒度语义的CoT；2) 逻辑保持蒸馏：训练逻辑感知的剪枝器，保留关键推理线索；3) 分布对齐生成：对齐压缩轨迹与推理风格，避免碎片化。

Result: 在MATH-500数据集上使用Qwen2.5-7B-Instruct模型，CtrlCoT比最强基线使用30.7%更少的token，同时准确率高出7.6个百分点，实现了更高效可靠的推理。

Conclusion: CtrlCoT通过协调语义抽象和token级剪枝，有效解决了CoT压缩中的关键挑战，在保持推理正确性的同时显著提升了效率。

Abstract: Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.

</details>


### [56] [PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs](https://arxiv.org/abs/2601.20539)
*Oguzhan Gungordu,Siheng Xiong,Faramarz Fekri*

Main category: cs.AI

TL;DR: PathWise是一个基于多智能体推理的自动化启发式设计框架，将启发式生成建模为基于蕴含图的序列决策过程，通过状态感知规划而非试错进化来生成更好的组合优化启发式。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动化启发式设计框架依赖固定的进化规则和静态提示模板，导致启发式生成短视、评估冗余，且缺乏对新启发式如何推导的推理能力。

Method: 提出PathWise多智能体推理框架：1) 将启发式生成建模为基于蕴含图的序列决策过程；2) 策略智能体规划进化动作；3) 世界模型智能体基于动作生成启发式推演；4) 批评智能体提供路由反思总结先前经验教训。

Result: 在多种组合优化问题上，PathWise能更快收敛到更好的启发式，在不同LLM骨干上具有良好泛化性，并能扩展到更大规模问题。

Conclusion: PathWise通过状态感知规划和多智能体推理，将LLM驱动的自动化启发式设计从试错进化转向基于推理的规划，显著提升了启发式设计的效率和效果。

Abstract: Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.

</details>


### [57] [Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation](https://arxiv.org/abs/2601.20614)
*Yanqi Dai,Yuxiang Ji,Xiao Zhang,Yong Wang,Xiangxiang Chu,Zhiwu Lu*

Main category: cs.AI

TL;DR: MathForge框架通过难度感知组策略优化算法和多方面问题重构策略，从算法和数据两个角度提升大模型在数学推理任务上的表现，特别针对困难问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习可验证奖励方法在数学推理中存在系统性不足：算法上GRPO对困难问题的策略更新幅度较低，数据上增强方法主要重述问题而非系统增加内在难度。

Method: 提出MathForge框架，包含难度感知组策略优化算法（DGPO）和多方面问题重构策略（MQR）。DGPO通过难度平衡组优势估计纠正GRPO的不平衡，并通过难度感知问题级加权优先处理困难问题；MQR在多个方面重构问题以增加难度同时保持原答案。

Result: 在多个数学推理任务上，MathForge显著优于现有方法。

Conclusion: MathForge通过算法和数据的双重改进，形成协同循环：MQR扩展数据边界，DGPO有效学习增强数据，从而提升大模型在数学推理特别是困难问题上的能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.

</details>


### [58] [MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents](https://arxiv.org/abs/2601.20831)
*Vishnu Sashank Dorbala,Dinesh Manocha*

Main category: cs.AI

TL;DR: MemCtrl：一种使用多模态大语言模型在线修剪记忆的新框架，通过可训练的记忆头门控机制决定保留、更新或丢弃观察和反思，显著提升具身智能体在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有记忆压缩和检索系统（如RAG）通常将记忆视为大型离线存储空间，这不适合需要在严格内存和计算约束下在线操作的具身智能体。需要一种能够在线修剪记忆的框架。

Method: 提出MemCtrl框架，为多模态大语言模型添加可训练的记忆头μ，作为门控机制决定在探索过程中保留、更新或丢弃哪些观察或反思。训练两种类型的μ：1）通过离线专家训练，2）通过在线强化学习训练。

Result: 在EmbodiedBench基准测试的多个子集上，MemCtrl增强的低性能MLLMs平均提升约16%，特定指令子集提升超过20%。μ增强的MLLMs在长而复杂的指令类型上表现优异。

Conclusion: MemCtrl通过在线记忆修剪机制有效解决了具身智能体的内存约束问题，显著提升了任务完成能力，特别是在复杂指令场景下。

Abstract: Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.

</details>


### [59] [Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)](https://arxiv.org/abs/2601.20843)
*Saurav Prateek*

Main category: cs.AI

TL;DR: 提出Deep Researcher架构，通过序列化研究计划优化和候选交叉算法，在博士级研究任务上超越现有研究助手，证明序列化方法优于并行范式。


<details>
  <summary>Details</summary>
Motivation: 解决并行扩展范式在复杂研究任务中的固有局限，特别是知识孤岛问题，需要更智能的动态适应方法来生成高质量研究报告。

Method: 采用序列化研究计划优化（通过反思）和候选交叉算法，结合全局研究上下文管理和一次性报告生成，使用Gemini 2.5 Pro模型实现。

Result: 在DeepResearch Bench的100个博士级研究任务上获得46.21分，超越Claude Researcher、Nvidia AIQ等现有研究助手，证明序列化方法优于并行自一致性范式。

Conclusion: 序列化扩展范式在复杂研究任务中优于并行方法，全局上下文管理和动态适应能力是关键优势，为AI研究助手提供了新的架构方向。

Abstract: This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runtime. This dynamic adaptation contrasts with parallel approaches, which often suffer from siloed knowledge. The Candidates Crossover algorithm further enhances search efficiency by deploying multiple LLM candidates with varied parameters to explore a larger search space, with their findings synthesized to curate a comprehensive final research response. The process concludes with One Shot Report Generation, ensuring the final document is informed by a unified narrative and high fact density. Powered by the Gemini 2.5 Pro model, our Deep Researcher was evaluated on the DeepResearch Bench, a globally recognized benchmark of 100 doctoral level research tasks. Our architecture achieved an overall score of 46.21, demonstrating superior performance by surpassing leading deep research agents such as Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher and Grok Deeper Search present on the DeepResearch Bench actively running leaderboard. This performance marginally exceeds our previous work, Static DRA, and reinforces the finding that sequential scaling consistently outperforms the parallel self consistency paradigm.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [60] [VibeFigma](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fvibeflowing-inc%2Fvibe_figma%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/_5A0hdX1IFsNgX60yrcC4EQ21QkN1XI7KTcRPeLEk_0=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: VibeFigma是一个将Figma设计自动转换为生产就绪的React组件（使用Tailwind CSS）的工具


<details>
  <summary>Details</summary>
Motivation: 解决从设计到代码的手动转换过程耗时且容易出错的问题，提高前端开发效率

Method: 使用官方Figma API准确提取设计，生成React/TypeScript组件，并提供可选的AI驱动的代码优化

Result: 创建了一个能够自动将Figma设计转换为生产就绪React组件的工具

Conclusion: VibeFigma通过自动化设计到代码的转换流程，显著提高了前端开发效率

Abstract: VibeFigma (GitHub Repo) VibeFigma is a tool that automatically transforms Figma designs into production-ready React components using Tailwind CSS. It uses the official Figma API for accurate design extraction, generates React/TypeScript components, and has optional AI-powered code optimization.

</details>


### [61] [ChatGPT Containers can now run bash, pip/npm install packages, and download files](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F26%2Fchatgpt-containers%2F%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/NsIgv2IMFqL7XR_7dhnjuJJOe1bnLKvayZSkKKAFQEg=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ChatGPT代码执行容器新增功能：支持运行Bash和其他语言，通过pip/npm安装包，以及从网络直接下载文件


<details>
  <summary>Details</summary>
Motivation: 扩展ChatGPT的代码执行能力，使其能够处理更复杂的编程任务，包括系统级操作、包管理和文件下载，提升开发效率

Method: 升级ChatGPT的代码执行容器，增加对Bash脚本的支持，集成pip和npm包管理器，实现网络文件下载功能

Result: ChatGPT现在能够在容器中执行更广泛的编程任务，包括系统命令、包安装和文件操作，显著增强了其编程辅助能力

Conclusion: ChatGPT代码执行容器的功能扩展使其成为更强大的编程工具，能够处理更复杂的开发工作流

Abstract: ChatGPT Containers can now run bash, pip/npm install packages, and download files (10 minute read) ChatGPT's code execution container can now run Bash and other languages, install packages via pip and npm, and download files directly from the web.

</details>


### [62] [What we learned from leaders building agentic AI at scale](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fably.com%2Fblog%2Fbuilding-agentic-ai-at-scale%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/ZFc0jj3gf9tIObyP9_YcHz6vCTTJ9vrA6mP7zZIlb1s=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI工程领导者经验分享：构建规模化智能体AI的主要挑战来自实践经验而非模型本身


<details>
  <summary>Details</summary>
Motivation: 了解AI工程领导者在构建规模化智能体AI过程中遇到的实际挑战，为其他团队提供经验指导

Method: 通过与AI工程领导者进行研究和访谈，收集他们在构建规模化智能体AI过程中的实践经验

Result: 研究发现最大的挑战来自实践经验方面（如团队协作、流程管理、部署运维），而非模型技术本身

Conclusion: 构建成功的规模化智能体AI需要更多关注组织、流程和团队协作等实践经验，而不仅仅是模型技术

Abstract: What we learned from leaders building agentic AI at scale (7 minute read) Research with AI engineering leaders shows that the biggest challenges are experiential rather than model-related.

</details>


### [63] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/oda3zw7TYzM7OGPnf5mpdVxPcDIfYNQi63_NEsadPqk=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 作者经历两年AI编码后回归手写代码，认为AI生成的复杂项目代码缺乏完整性，导致难以管理的"代码垃圾"


<details>
  <summary>Details</summary>
Motivation: 作者发现AI生成的复杂项目代码存在质量问题，缺乏完整性，导致难以维护和管理的"代码垃圾"问题

Method: 基于个人两年AI编码实践经验，对比分析AI生成代码与手写代码的质量差异

Result: AI生成的复杂项目代码缺乏完整性，导致难以管理的"代码垃圾"，作者因此回归手写代码

Conclusion: 对于复杂项目，手写代码比AI生成代码更有质量保证，建议谨慎使用AI代码生成工具

Abstract: After two years of vibecoding, I'm back to writing by hand (6 minute read) AI-generated code for complex projects has no integrity and leads to unmanageable "slop.”

</details>


### [64] [Don't let your best ideas languish in a backlog](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnl20250127/2/0100019bff8fce5b-9d1dad78-8dc2-441d-9f9d-1afff8f201cf-000000/n0rEceZw4ynn2vQ3QR4I-Yq8BfBMcMdcjTr4nZ2YVR4=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: v0是一个AI代码生成工具，旨在弥合原型设计与生产就绪代码之间的差距，让非工程师也能将创意快速转化为可部署的代码


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成工具（如vibe coding）虽然能让设计师或市场人员快速创建原型，但生成的代码通常无法直接用于生产环境，需要工程师花费数周时间重写，导致许多创意想法最终停留在待办列表中无法实现

Method: v0通过AI生成可直接用于生产的代码，每个提示都能生成生产就绪的代码并直接存入代码仓库，让设计人员能够基于实际代码进行工作

Result: 该工具能够显著缩短从创意到生产部署的时间，减少工程师重写代码的工作量，让更多创意能够快速转化为实际可用的产品

Conclusion: v0解决了当前AI代码生成工具的主要痛点，真正实现了从原型到生产代码的无缝过渡，让非技术背景的人员也能有效参与产品开发过程

Abstract: Don't let your best ideas languish in a backlog (Sponsor) Vibe coding was supposed to give anyone the power to bring their ideas to life. In reality: a designer or marketer builds something promising, but engineers still need to spend weeks rewriting it to fit production. More often than not, these ideas never leave the backlog.v0 closes the gap between vibe coding and production. Every prompt generates production-ready code that lives in your repository. Designers work against the actual cod...

</details>


### [65] [v0 by Vercel lets you see your designs come to life...with code that actually works](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnl20250127/6/0100019bff8fce5b-9d1dad78-8dc2-441d-9f9d-1afff8f201cf-000000/HJ4nPE2PFQppkaJVfY97cKac6t1M2656zdcVu5WYesQ=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Vercel的v0工具可以将设计原型转化为可直接部署的生产级代码


<details>
  <summary>Details</summary>
Motivation: 解决设计到代码转换的痛点，让设计师能够快速将视觉设计转化为可工作的生产代码，减少开发时间

Method: 通过AI驱动的代码生成技术，将设计原型自动转换为生产就绪的代码

Result: 能够将设计快速转化为可直接部署的代码，提高开发效率

Conclusion: v0工具简化了设计到开发的流程，使原型能够快速转化为生产就绪的应用

Abstract: v0 by Vercel lets you see your designs come to life...with code that actually works (Sponsor) Vibe coding prototypes is satisfying. Even more satisfying? Turning your designs into code built for production, ready to deploy. Try v0 today.

</details>


### [66] [Beyond Generative: The Rise Of Agentic AI and User-Centric Design](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.smashingmagazine.com%2F2026%2F01%2Fbeyond-generative-rise-agentic-ai-user-centric-design%2F%3Futm_source=tldrdesign/1/0100019bff8fce5b-9d1dad78-8dc2-441d-9f9d-1afff8f201cf-000000/ZeFC2y512RmAb1KEca5wpgDaayhyRiBETzIhPnxhT6k=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文介绍了从传统生成式AI向智能体AI的范式转变，智能体AI能够自主规划、执行和持续完成任务，而非仅响应指令。文章提出了四个智能体自主性级别，并介绍了专门的研究方法。


<details>
  <summary>Details</summary>
Motivation: 传统生成式AI主要被动响应用户指令，缺乏自主规划和持续执行任务的能力。智能体AI旨在解决这一局限，使AI系统能够像人类助手一样主动思考、规划和行动，实现更复杂、持续的任务完成。

Method: 提出了四个智能体自主性级别：观察与建议、规划与提议、确认后行动、自主行动。介绍了专门的研究方法，如智能体旅程映射和模拟不当行为分析，用于设计和评估智能体系统。

Result: 智能体AI代表了AI发展的根本性转变，从被动响应转向主动规划和执行。这种转变使AI能够处理更复杂、持续的任务，为用户提供更智能、更自主的辅助体验。

Conclusion: 智能体AI是AI发展的下一阶段，强调以用户为中心的设计和自主能力。通过适当的自主性级别和专门的研究方法，可以开发出更有效、更可靠的智能体系统，为用户提供更智能的服务。

Abstract: Beyond Generative: The Rise Of Agentic AI and User-Centric Design (19 minute read) Agentic AI represents a fundamental shift from traditional generative AI by enabling systems to autonomously plan, execute, and persist in tasks rather than simply responding to commands. There are four levels of agent autonomy—observe-and-suggest, plan-and-propose, act-with-confirmation, and act-autonomously. This post introduces specialized research methods, such as agent journey mapping and simulated misbeha...

</details>


### [67] [Sec-Context](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FArcanum-Sec%2Fsec-context%3Futm_source=tldrinfosec/1/0100019bffc8616d-f145252c-f4d6-40f1-a6ad-9fba267646b9-000000/1n5muEjn81VI3it7_QGfTcI7VvauQAOFwMwpNclxYhA=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sec-Context是一个安全反模式参考库，从150+个来源提炼，专为LLM设计，旨在解决AI生成代码中86%的XSS失败率等关键漏洞问题


<details>
  <summary>Details</summary>
Motivation: AI生成的代码存在严重的安全漏洞，特别是XSS攻击有86%的失败率，需要专门的安全参考来提升AI代码生成的安全性

Method: 从150多个来源提炼安全反模式，创建包含25+个反模式的参考文档，提供伪代码示例、CWE引用和缓解策略，优化为系统提示、RAG参考或安全审查工具

Result: 创建了全面的安全反模式参考库，专门针对LLM优化，覆盖常见安全漏洞，提供可操作的缓解策略

Conclusion: Sec-Context为AI代码生成提供了专门的安全参考框架，能显著提升生成代码的安全性，特别是针对XSS等常见漏洞

Abstract: Sec-Context (GitHub Repo) Comprehensive security anti-pattern reference distilled from 150+ sources designed for LLM consumption, addressing the 86% XSS failure rate and other critical vulnerabilities in AI-generated code. Provides two reference documents covering 25+ anti-patterns with pseudocode examples, Common Weakness Enumeration (CWE) references, and mitigation strategies optimized for use as system prompts, Retrieval-Augmented Generation (RAG) references, or dedicated security review a...

</details>


### [68] [Qwen3-Max-Thinking debuts with focus on hard math, code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fqwen3-max-thinking-debuts-with-focus-on-hard-math-code%2F%3Futm_source=tldrai/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/app_m8Xs5B8FKfCg9bT5vERJIhTl3rNxTY3ZOZotWNs=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Qwen3-Max-Thinking是一个专注于复杂数学、编程和多步工作流的推理模型，具有自适应工具使用和内置网络搜索功能。


<details>
  <summary>Details</summary>
Motivation: 解决需要证据收集和深度验证的复杂任务，为开发者和企业提供长上下文推理和工具使用代理能力。

Method: 通过推理模型架构，结合自适应工具使用和内置网络搜索功能，专注于复杂数学、编程和多步工作流处理。

Result: 在Alibaba Cloud的Model Studio上推出，早期测试者认可其在开发者和企业应用中的价值，特别是在需要长上下文推理和工具使用代理的场景。

Conclusion: Qwen3-Max-Thinking是一个强大的推理模型，专门针对复杂数学、编程和多步工作流任务，具有自适应工具使用和网络搜索能力。

Abstract: Qwen3-Max-Thinking debuts with focus on hard math, code (2 minute read) Qwen3-Max-Thinking is a reasoning model for complex math, coding, and multi-step workflows. Available on Alibaba Cloud's Model Studio, this model excels in tasks that require evidence gathering and deep verification, offering adaptive tool-use and built-in web search features. Early testers highlight its benefits for developers and enterprises needing long-context reasoning and tool-using agents.

</details>


### [69] [ChatGPT Containers can now run bash, pip/npm install packages, and download files](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F26%2Fchatgpt-containers%2F%23atom-everything%3Futm_source=tldrai/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/M_bKeuzUysBDpd-Bn2J8rJhvFa79xWa8sdp9_AJsUR4=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ChatGPT Code Interpreter功能大幅升级，现在可以在容器中运行bash、通过pip/npm安装包、下载文件，并支持11种新编程语言的代码编写与测试。


<details>
  <summary>Details</summary>
Motivation: ChatGPT Code Interpreter功能升级后缺乏官方文档，用户需要了解这些新功能的具体能力和使用方法。

Method: 通过实际测试和分析，总结ChatGPT Code Interpreter的新功能，包括容器环境、包管理、文件下载和多语言支持等工具集。

Result: 确认ChatGPT Code Interpreter现在具备完整的容器化开发环境，能够执行bash命令、安装Python/Node.js包、下载网络文件，并支持11种编程语言的代码编写与测试。

Conclusion: ChatGPT Code Interpreter的功能升级使其成为一个更强大的代码执行环境，但缺乏官方文档需要用户自行探索。

Abstract: ChatGPT Containers can now run bash, pip/npm install packages, and download files (11 minute read) ChatGPT Code Interpreter underwent a massive upgrade in the past few months. It can write and then test code in 11 new languages, find files online and download them into a container, and install packages via pip and npm to help it solve problems. There is no official documentation for these new features. This post provides an overview of the update, along with a full list of tools that ChatGPT ...

</details>


### [70] [Self-Adaptive Context Pruning for Coding Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FAyanami1314%2Fswe-pruner%3Futm_source=tldrai/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/ONWT4CoXbmjfKDHuO1GYkI1FkpB5VDzch5EmxisvOYI=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: SWE-Pruner提出任务感知上下文剪枝框架，通过轻量级模型动态选择相关代码，将代码代理的token使用量减少50%以上而不影响性能


<details>
  <summary>Details</summary>
Motivation: 代码代理在处理多轮工程任务时通常需要大量上下文，导致token使用效率低下和计算成本高昂。现有方法缺乏针对代码上下文的智能剪枝机制。

Method: 采用任务感知的上下文剪枝框架，使用轻量级模型动态选择与当前任务相关的代码片段，优化多轮工程任务的工作流程。

Result: 在保持性能不变的情况下，将代码代理的token使用量减少超过50%，显著提高了计算效率和资源利用率。

Conclusion: SWE-Pruner通过智能上下文剪枝有效解决了代码代理的token效率问题，为大规模软件工程任务提供了实用的优化方案。

Abstract: Self-Adaptive Context Pruning for Coding Agents (GitHub Repo) SWE-Pruner introduces a task-aware context pruning framework that cuts token usage in code agents by over 50% without harming performance. It dynamically selects relevant code using a lightweight model, optimizing workflows in multi-turn engineering tasks.

</details>


### [71] [An AI Pioneer Warns the Tech ‘Herd' Is Marching Into a Dead End](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FwcRw4C/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/yXRLj-6jChVTIYe8HXFpNH1zjADjLo8UHk8z8sSOfg4=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI先驱Yann LeCun警告科技界"羊群效应"正将AI发展带入死胡同，认为LLM技术存在局限，硅谷缺乏多元化探索


<details>
  <summary>Details</summary>
Motivation: 批评硅谷当前对大型语言模型(LLM)的过度追捧和"羊群效应"，指出这种单一技术路线限制了AI的长期发展潜力，需要更多样化的研究路径

Method: 通过专家观点分析，Yann LeCun作为AI领域权威专家，公开批评当前AI发展的技术路线，强调LLM技术的局限性

Result: 揭示了硅谷AI发展中的"羊群效应"问题，指出过度依赖LLM技术将导致发展瓶颈，呼吁更多元化的技术探索

Conclusion: 当前AI发展的单一技术路线存在风险，需要打破"羊群效应"，探索更多样化的AI技术路径以实现长期突破

Abstract: An AI Pioneer Warns the Tech ‘Herd' Is Marching Into a Dead End (8 minute read) Yann LeCun, one of the world's leading experts on artificial intelligence, has become increasingly vocal in his criticism of Silicon Valley's approach to building AI. He says the technology industry will eventually hit a dead end in its AI development as LLM technology has its limits. The herd effect in Silicon Valley leaves no room for other approaches that may be more promising in the long run. If everyone were ...

</details>


### [72] [One Human + One Agent = One Browser From Scratch](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F27%2Fone-human-one-agent-one-browser%2F%23atom-everything%3Futm_source=tldrnewsletter/1/0100019c0458af1d-c4ec1ac7-017f-413a-9a11-23d034c00c96-000000/PtsQwNuz6bAHzqy8GyNuGb5dwY_AYdi9SPaqyC7cODE=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 一个工程师使用Codex CLI智能体在3天内从零构建了一个包含2万行Rust代码的浏览器，不依赖任何Rust库，仅使用操作系统框架，成功渲染HTML、CSS、SVG和PNG


<details>
  <summary>Details</summary>
Motivation: 探索智能体与人类工程师协作的潜力，证明在优秀工程师指导下，智能体能够完成复杂的软件开发任务

Method: 使用Codex CLI智能体作为开发工具，工程师指导智能体编写代码，采用Rust语言，完全不依赖外部Rust库，仅使用操作系统提供的图像和文本渲染框架

Result: 成功构建了一个功能完整的浏览器，包含2万行Rust代码，能够渲染HTML、CSS、SVG和PNG，实现了基本的网页渲染功能

Conclusion: 智能体在人类工程师的指导下能够完成复杂的软件开发任务，证明了"一人+一智能体"协作模式在软件开发中的有效性

Abstract: One Human + One Agent = One Browser From Scratch (2 minute read) one-agent-one-browser is a web browser built in three days with a single Codex CLI agent. It contains 20,000 lines of Rust with no Rust crate dependencies at all, with only OS system frameworks for image and text rendering. The browser successfully renders HTML and CSS and can even display SVGs and PNGs. The project proves that an agent driven by a talented engineer is enough to get a very solid basic renderer working.

</details>


### [73] [Claude Code made me love meetings again](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftn1ck.com%2Fblog%2Fclaude-code-made-me-love-meetings-again%3Futm_source=tldrnewsletter/1/0100019c0458af1d-c4ec1ac7-017f-413a-9a11-23d034c00c96-000000/irmICZ_ffXP--j4nKYlxIKR4dGYPdA5C_gBv0UizOJo=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code通过减少认知负担和提高处理中断的能力，让开发者重新爱上会议


<details>
  <summary>Details</summary>
Motivation: 开发者在编码过程中经常面临认知负担过重的问题，特别是在会议和中断频繁的工作环境中，这影响了工作效率和开发体验

Method: 使用Claude Code工具来辅助编程工作，通过AI助手减少开发者的认知负荷，提高处理中断的能力

Result: Claude Code释放了开发者的心智容量，增强了应对中断的能力，使开发者能够更好地平衡编码工作和会议参与

Conclusion: Claude Code通过减轻认知负担，改善了开发者的工作体验，让会议不再成为负担，而是可以接受的工作环节

Abstract: Claude Code made me love meetings again (3 minute read) Claude frees up mental capacity and increases your capacity for getting interrupted.

</details>


### [74] [Ralph](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsnarktank%2Fralph%3Futm_source=tldrdevops/1/0100019c04905183-a61a9ef4-95c0-4e6c-aea2-d82e435f887b-000000/iyyFLr-_dpsTzsKp0LBIeOIPWX-raILbYBRHE-QnAk0=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ralph是一个基于Geoffrey Huntley模式的自主AI代理循环系统，使用Amp或Claude Code等AI编码工具完成PRD项目，通过git历史、progress.txt和prd.json实现记忆持久化，并在AGENTS.md中记录学习到的模式供未来使用。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够自主完成产品需求文档（PRD）项目的AI代理系统，通过持续迭代和记忆持久化来提高自动化编码任务的效率和可重复性。

Method: 基于Geoffrey Huntley模式构建自主AI代理循环，使用Amp或Claude Code等AI编码工具，通过git历史、progress.txt和prd.json实现记忆持久化，并在AGENTS.md中记录学习到的模式。

Result: 创建了Ralph系统，一个能够自主完成PRD项目的AI代理循环，具备记忆持久化和模式学习能力。

Conclusion: Ralph系统展示了通过AI代理循环和记忆持久化机制可以实现自主完成编码任务的潜力，为自动化软件开发提供了新思路。

Abstract: Ralph (GitHub Repo) Ralph, an autonomous AI agent loop based on Geoffrey Huntley's pattern, is a system that uses AI coding tools like Amp or Claude Code to complete Product Requirements Document (PRD) items. It operates through repeated iterations, with memory persisting via git history, progress.txt, and prd.json, and updates AGENTS.md with learned patterns for future runs.

</details>


### [75] [1Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2F21st-dev%2F1code%3Futm_source=tldrdevops/1/0100019c04905183-a61a9ef4-95c0-4e6c-aea2-d82e435f887b-000000/KF6L7PAIXmJ7QafZSbEhxU0nEX_F9xYQc_dczxf8BUU=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 1Code.dev是一个新的Claude Code UI，支持macOS、Linux和Windows上的本地和远程代理执行，提供类似Cursor的界面、差异预览、内置Git客户端以及Claude的预执行规划功能。


<details>
  <summary>Details</summary>
Motivation: 为Claude Code开发一个更强大的用户界面，支持跨平台代理执行，提供类似现代代码编辑器的体验，并集成AI驱动的规划功能。

Method: 开发了一个新的UI平台，具有类似Cursor的界面设计，支持本地和远程代理执行，包含差异预览、内置Git客户端，并集成了Claude的预执行规划能力。

Result: 成功推出了1Code.dev平台，支持macOS、Linux和Windows系统，提供了现代化的代码编辑体验，集成了AI规划功能和版本控制工具。

Conclusion: 1Code.dev为Claude Code提供了一个功能丰富的跨平台界面，结合了现代代码编辑器的便利性和AI驱动的智能规划能力。

Abstract: 1Code (GitHub Repo) 1Code.dev has launched as a new UI for Claude Code that enables local and remote agent execution across macOS, Linux, and Windows. The platform features a Cursor-like interface with diff previews, a built-in Git client, and offers pre-execution planning capabilities from Claude.

</details>


### [76] [Report: 96% of devs don't fully trust AI code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fthe-state-of-code%2Fdeveloper-survey-report%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-state-of-code-developer-survey26%26utm_content=newsletter-dev-primary-devsurvey-260128-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/xfwbHF9_DTa8DxifhYkEp4t4dqzj-oMg6-8a0LxIT7U=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 调查报告显示96%开发者不完全信任AI生成代码，仅48%会在提交前检查，AI加速代码生成但验证成为瓶颈


<details>
  <summary>Details</summary>
Motivation: 分析生成式AI对软件开发工作流程的影响，了解开发者如何适应AI代码生成工具，识别当前存在的信任和验证问题

Method: 基于对1,100多名开发者的调查，Sonar的代码状态报告采用问卷调查方法收集数据

Result: 96%开发者不完全信任AI生成代码的功能正确性，仅48%会在提交前检查，61%开发者认为AI代码需要更多验证

Conclusion: AI加速代码生成但验证成为瓶颈，开发者对AI代码信任度低，需要更好的验证工具和流程

Abstract: Report: 96% of devs don't fully trust AI code (Sponsor) AI is accelerating code generation, but it's creating a bottleneck in the verification phase. Based on a survey of 1,100+ developers, Sonar's newest State of Code report analyzes the impact of generative AI on software engineering workflows and how developers are adapting to address it.Findings include: 96% of developers don't fully trust that AI-generated code is functionally correct yet only 48% always check it before committing61% agr...

</details>


### [77] [How Cursor Shipped its Coding Agent to Production](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.bytebytego.com%2Fp%2Fhow-cursor-shipped-its-coding-agent%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/dIxtUMx2lWUbMeby8TuNnUyNKPXW-1FP5jK7dXbs85E=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor团队开发了Composer编码代理，通过解决三个工程难题（diff问题、复合延迟、成本控制），实现了大多数任务在30秒内完成的生产级部署。


<details>
  <summary>Details</summary>
Motivation: 现有编码代理在实际生产部署中面临三个主要挑战：模型不擅长生成干净的代码编辑（diff问题）、代理循环中每个步骤都会增加延迟（复合延迟）、以及运行成本过高。Cursor团队需要解决这些问题才能将编码代理成功推向生产环境。

Method: 1. 针对diff问题：专门在编辑轨迹和搜索替换操作上进行训练；2. 针对复合延迟：采用混合专家架构、使用草稿模型进行推测解码、以及聚合技术；3. 针对成本控制：采用特定优化策略（具体未在摘要中详细说明）。

Result: 成功开发出Composer编码代理，能够在30秒内完成大多数编码任务，实现了生产级部署。

Conclusion: 通过系统性地解决编码代理部署中的三个核心工程问题，Cursor团队成功将Composer推向生产环境，展示了解决实际工程挑战对于AI编码代理商业化的重要性。

Abstract: How Cursor Shipped its Coding Agent to Production (12 minute read) Cursor's team shipped Composer, a coding agent that completes most tasks in under 30 seconds, by solving three gnarly engineering problems: the “diff problem” (models aren't great at making clean code edits, so they trained specifically on edit trajectories and search-replace operations), compounded latency (each step in the agent loop adds delay, so they used MoE architecture, speculative decoding with draft models, and aggre...

</details>


### [78] [A few random notes from Claude coding quite a bit last few weeks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F4I1kGa/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/UrslZW64eljAB8e7XRua-MvoaqsInlfbr9HxUXn8cBE=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Andrej Karpathy在几周内将编程工作流程从80%手动转变为80% LLM代理驱动，LLM提升了生产力但仍有概念错误和代码过度复杂化的问题


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理在编程工作流程中的应用效果，了解从手动编程向AI辅助编程转变的实际体验和挑战

Method: 基于个人实际使用经验，对比分析传统手动编程与LLM代理驱动编程的工作流程变化，记录观察到的优缺点

Result: LLM显著提升了编程生产力、耐力和项目范围，但仍会产生微妙的概念错误、过度复杂的代码，需要IDE中密切的人工监督

Conclusion: 尽管存在缺陷，LLM辅助的工作流程使编程更加愉快，代表了编程工作方式的重大转变

Abstract: A few random notes from Claude coding quite a bit last few weeks (5 minute read) Andrej Karpathy has had a rapid shift in their coding workflow, moving from 80% manual to 80% LLM agent-driven programming within weeks. While LLMs boost productivity, stamina, and project scope by handling code generation, they still produce subtle conceptual errors, overcomplicate code, and necessitate close human oversight within an IDE. Despite these flaws, the LLM-assisted workflow makes coding more enjoyabl...

</details>


### [79] [Open Coding Agents: Fast, accessible coding agents that adapt to any repo](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fallenai.org%2Fblog%2Fopen-coding-agents%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/MdQi0qzR3OvBP5FlfP_xajKK56DXzLG67RV5XEKy1iw=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ai2发布Open Coding Agents计划，推出SERA系列开源编码模型及低成本训练方法，使小型团队也能在私有代码库上微调强大的编码助手


<details>
  <summary>Details</summary>
Motivation: 解决现有封闭式、昂贵的编码助手系统的局限性，降低私有代码库上训练强大编码助手的成本和复杂度

Method: 发布SERA（Soft-verified Efficient Repository Agents）系列开源编码模型，提供可访问的训练方法，显著降低在私有代码库上微调编码助手的成本和复杂度

Result: 使小型团队也能负担得起在私有代码库上训练强大的编码助手，提高了编码助手的可访问性和适应性

Conclusion: Open Coding Agents计划通过开源模型和低成本训练方法，使强大的编码助手技术更加民主化，能够适应任何代码库

Abstract: Open Coding Agents: Fast, accessible coding agents that adapt to any repo (11 minute read) Ai2 has launched Open Coding Agents, an initiative to address the limitations of existing closed and expensive coding agent systems. They are releasing SERA (Soft-verified Efficient Repository Agents), a family of strong open coding models and an accessible training method. This method significantly reduces the cost and complexity of fine-tuning powerful coding agents on private codebases, allowing smal...

</details>


### [80] [AGENTS.md outperforms skills in our agent evals](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fagents-md-outperforms-skills-in-our-agent-evals%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/nHjdsFrceHd5pQlAvc7IBPWOjl5vB-sPLwTpid-W5O8=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AGENTS.md（嵌入式Markdown文件）在AI编码代理评估中表现优于技能包，通过压缩的8KB文档索引实现了100%通过率


<details>
  <summary>Details</summary>
Motivation: AI编码代理在处理过时的框架知识时存在问题，导致生成错误代码。需要找到有效的方法为代理提供最新的Next.js API文档

Method: Vercel研究人员评估了两种方法：1）"技能"（按需知识包）；2）"AGENTS.md"（持久嵌入式Markdown文件）。后者包含压缩的8KB文档索引

Result: AGENTS.md在评估中实现了完美的100%通过率，表现优于技能包方法

Conclusion: 嵌入式Markdown文件比传统的技能包方法更有效地为AI编码代理提供最新框架文档

Abstract: AGENTS.md outperforms skills in our agent evals (9 minute read) AI coding agents struggle with outdated framework knowledge, leading to incorrect code generation. Researchers from Vercel evaluated two approaches for providing current Next.js API documentation: "skills" (on-demand knowledge packages) and "AGENTS.md" (a persistent, embedded markdown file). Surprisingly, a compressed 8KB docs index embedded directly in AGENTS.md achieved a perfect 100% pass rate in their evals, outperforming ski...

</details>


### [81] [One Human + One Agent = One Browser From Scratch](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Femsh.cat%2Fone-human-one-agent-one-browser%2F%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/wptpzv15P30ChmEzIaUOXne1zmFB2gmaqWlrOsQHL40=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 作者仅使用单个LLM代理，在约70小时内从零开始构建了一个基本的跨平台HTML/CSS浏览器，无需第三方Rust依赖


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理在复杂软件开发任务中的能力，特别是能否仅凭单个代理完成从零构建浏览器的挑战性任务

Method: 使用单个LLM代理进行端到端开发，采用Rust语言，避免第三方依赖，通过迭代式开发过程构建浏览器核心功能

Result: 成功构建了基本的跨平台HTML/CSS浏览器，支持核心渲染功能，证明了单个LLM代理能够完成复杂软件开发任务

Conclusion: 单个LLM代理具备完成复杂软件开发项目的能力，为AI辅助编程和自动化软件开发提供了新的可能性

Abstract: One Human + One Agent = One Browser From Scratch (7 minute read) Working with a single LLM agent, the author successfully built a basic, cross-platform HTML/CSS browser from scratch in approximately 70 hours and without third-party Rust dependencies.

</details>


### [82] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/pVCYCX9KLQgI-oYWsrXVckCVpaYvLPB_Q8WD1f9PDH4=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 作者使用单个LLM代理，在约70小时内从零开始构建了一个基本的跨平台HTML/CSS浏览器，无需第三方Rust依赖


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理在复杂软件开发任务中的能力，特别是能否从零开始构建一个功能完整的浏览器，测试代理在大型项目中的实际应用潜力

Method: 使用单个LLM代理作为主要开发工具，采用从零开始的开发方法，完全使用Rust语言但不依赖第三方库，通过迭代开发实现HTML/CSS渲染引擎和基本浏览器功能

Result: 成功构建了一个基本的跨平台HTML/CSS浏览器，开发时间约70小时，证明了单个LLM代理能够完成复杂的软件开发任务

Conclusion: 单个LLM代理具备从零开始构建复杂软件系统的能力，这为AI辅助软件开发提供了新的可能性，展示了代理在大型项目中的实际应用价值

Abstract: One Human + One Agent = One Browser From Scratch (7 minute read) Working with a single LLM agent, the author successfully built a basic, cross-platform HTML/CSS browser from scratch in approximately 70 hours and without third-party Rust dependencies.

</details>


### [83] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/X4Bs1VuD4UtrFyBrBRypQuR3HZwh0GN_YqNkZ8dQF0Q=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 作者使用单个LLM代理，在约70小时内从零开始构建了一个基本的跨平台HTML/CSS浏览器，没有使用第三方Rust依赖


<details>
  <summary>Details</summary>
Motivation: 探索单个LLM代理在复杂软件开发任务中的能力，特别是从零开始构建基础浏览器这种具有挑战性的项目

Method: 使用单个LLM代理进行开发，采用从零开始的方法，不使用第三方Rust依赖，完全自主实现HTML/CSS渲染功能

Result: 成功构建了一个基本的跨平台HTML/CSS浏览器，开发时间约70小时，证明了单个LLM代理能够完成复杂的软件开发任务

Conclusion: 单个LLM代理具备从零开始构建复杂软件系统的能力，为AI辅助软件开发提供了新的可能性

Abstract: One Human + One Agent = One Browser From Scratch (7 minute read) Working with a single LLM agent, the author successfully built a basic, cross-platform HTML/CSS browser from scratch in approximately 70 hours and without third-party Rust dependencies.

</details>


### [84] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/qPGe9Da-Y9HTMUusSVhY4jJuOmU7gc5Vk5UVrdPgipk=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 作者仅使用单个LLM代理，在约70小时内从零构建了一个基础的跨平台HTML/CSS浏览器，无需第三方Rust依赖


<details>
  <summary>Details</summary>
Motivation: 探索单个LLM代理能否独立完成复杂的软件开发任务，特别是构建一个功能完整的浏览器，验证LLM在代码生成和系统开发方面的能力

Method: 使用单个LLM代理作为开发助手，采用从零开始的开发方法，仅使用Rust标准库，避免第三方依赖，通过迭代开发逐步实现浏览器核心功能

Result: 成功构建了一个基础的跨平台HTML/CSS浏览器，支持基本的网页渲染功能，证明了单个LLM代理能够完成复杂的软件开发任务

Conclusion: 单个LLM代理具备独立完成复杂软件开发任务的能力，这种方法为AI辅助编程提供了新的可能性，展示了LLM在系统级软件开发中的潜力

Abstract: One Human + One Agent = One Browser From Scratch (7 minute read) Working with a single LLM agent, the author successfully built a basic, cross-platform HTML/CSS browser from scratch in approximately 70 hours and without third-party Rust dependencies.

</details>
