<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 23]
- [tldr.article](#tldr.article) [Total: 7]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.AI](#cs.AI) [Total: 16]
- [wechat.article](#wechat.article) [Total: 26]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Long-Term Memory for Long-Context Question Answering](https://arxiv.org/abs/2510.23730)
*Alessandra Terranova,Björn Ross,Alexandra Birch*

Main category: cs.CL

TL;DR: 系统评估了不同类型记忆增强方法在长上下文对话任务中的效果，发现记忆增强方法能减少90%以上的token使用同时保持竞争力准确率。


<details>
  <summary>Details</summary>
Motivation: 为了让大语言模型实现真正的对话连续性和从经验学习中受益，需要有效的记忆系统，但目前不清楚哪种记忆类型对长上下文对话任务最有效。

Method: 使用LoCoMo基准测试，分析完整上下文提示、通过检索增强生成的语义记忆、代理记忆、通过上下文学习的片段记忆，以及通过提示优化的程序记忆。

Result: 记忆增强方法减少token使用超过90%同时保持竞争力准确率。记忆架构复杂度应与模型能力相称，小型基础模型从RAG获益最多，强指令调优推理模型从片段学习和复杂代理语义记忆获益。

Conclusion: 片段记忆能帮助LLM识别自身知识的局限性，记忆架构应根据模型能力进行适当缩放。

Abstract: In order for large language models to achieve true conversational continuity
and benefit from experiential learning, they need memory. While research has
focused on the development of complex memory systems, it remains unclear which
types of memory are most effective for long-context conversational tasks. We
present a systematic evaluation of memory-augmented methods using LoCoMo, a
benchmark of synthetic long-context dialogues annotated for question-answering
tasks that require diverse reasoning strategies. We analyse full-context
prompting, semantic memory through retrieval-augmented generation and agentic
memory, episodic memory through in-context learning, and procedural memory
through prompt optimization. Our findings show that memory-augmented approaches
reduce token usage by over 90% while maintaining competitive accuracy. Memory
architecture complexity should scale with model capability, with small
foundation models benefitting most from RAG, and strong instruction-tuned
reasoning model gaining from episodic learning through reflections and more
complex agentic semantic memory. In particular, episodic memory can help LLMs
recognise the limits of their own knowledge.

</details>


### [2] [Temporal Blindness in Multi-Turn LLM Agents: Misaligned Tool Use vs. Human Time Perception](https://arxiv.org/abs/2510.23853)
*Yize Cheng,Arshia Soltani Moakhar,Chenrui Fan,Kazem Faghih,Parsa Hosseini,Wenxiao Wang,Soheil Feizi*

Main category: cs.CL

TL;DR: 论文研究了LLM代理在动态环境中因缺乏时间感知能力而导致的工具调用问题，提出了TicToc-v1测试集来评估时间敏感场景下的表现，发现当前模型在时间感知方面表现不佳，需要专门的后训练对齐。


<details>
  <summary>Details</summary>
Motivation: LLM代理在多轮对话环境中存在时间盲点，无法感知消息间的时间流逝，导致工具调用决策不当——要么过度依赖旧上下文而跳过必要调用，要么不必要地重复调用。

Method: 引入TicToc-v1测试集，包含34个时间敏感场景的多轮对话轨迹；在对话消息中添加时间戳；收集人类偏好数据，分为偏好不调用工具和偏好调用工具两个子集；评估LLM在不同时间间隔下工具调用决策与人类偏好的一致性。

Result: 无时间信息时，大多数模型表现仅略优于随机猜测，最高对齐率约60%；添加时间戳后略有改善，特别是对大模型，但改善有限，峰值约65%；基于提示的对齐方法效果有限。

Conclusion: 需要专门的后训练对齐来使多轮LLM工具使用与人类时间感知保持一致，当前模型在时间感知能力方面存在显著不足。

Abstract: Large language model agents are increasingly used in multi-turn
conversational settings to interact with and execute tasks in dynamic
environments. However, a key limitation is their temporal blindness: they, by
default, operate with a stationary context, failing to account for the
real-world time elapsed between messages. This becomes a critical liability
when an agent must decide whether to invoke a tool based on how much time has
passed since the last observation. Without temporal awareness, agents often
either over-rely on previous context (skipping necessary tool calls), or
under-rely on it (unnecessarily repeating tool calls). To study this challenge,
we introduce TicToc-v1, a test set of multi-turn user-agent trajectories across
34 scenarios with varying time sensitivity. Each trajectory ends with a user
question, where the need for a tool call depends on the amount of time elapsed
since the last message. To give LLMs temporal context, we augment dialogue
messages with explicit timestamps, bridging the gap between static dialogue and
evolving environments. We then collected human preferences for these samples,
creating two subsets: one where humans preferred relying on the previous
observation (prefer-noTool), and another where they preferred a new tool call
(prefer-Tool). We evaluated how well LLM tool-calling decisions align with
human preferences under varying time intervals on TicToc-v1. Our analysis show
that without time information, most models perform only slightly better than
random, with the top alignment rate being just over 60%. While adding
timestamps leads to a slight improvement, particularly for larger models, the
improvement is modest, peaking at around 65%. We also show that naive,
prompt-based alignment have limited effectiveness. Our findings highlight the
need for specific post-training alignment to align multi-turn LLM tool use with
human temporal perception.

</details>


### [3] [Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs](https://arxiv.org/abs/2510.23854)
*Jyotika Singh,Weiyi Sun,Amit Agarwal,Viji Krishnamurthy,Yassine Benajiba,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: 提出Combo-Eval评估方法，结合多种现有方法的优势，用于评估LLM生成的表格结果自然语言表示，显著减少25-61%的LLM调用，并发布首个专用数据集NLR-BIRD。


<details>
  <summary>Details</summary>
Motivation: 在多轮对话代理等现代工业系统中，Text-to-SQL技术连接自然语言问题与数据库查询，但LLM生成的表格结果自然语言表示存在信息丢失或错误问题，目前缺乏系统评估方法。

Method: 提出Combo-Eval评估方法，结合多种现有评估方法的优势，优化评估保真度，并创建首个专用数据集NLR-BIRD用于基准测试。

Result: Combo-Eval显著减少25-61%的LLM调用，通过人工评估证明其与人类判断具有更好的对齐度，适用于有和无参考基准的场景。

Conclusion: Combo-Eval方法在评估LLM生成的表格结果自然语言表示方面表现出色，与人类判断高度一致，且能大幅降低计算成本。

Abstract: In modern industry systems like multi-turn chat agents, Text-to-SQL
technology bridges natural language (NL) questions and database (DB) querying.
The conversion of tabular DB results into NL representations (NLRs) enables the
chat-based interaction. Currently, NLR generation is typically handled by large
language models (LLMs), but information loss or errors in presenting tabular
results in NL remains largely unexplored. This paper introduces a novel
evaluation method - Combo-Eval - for judgment of LLM-generated NLRs that
combines the benefits of multiple existing methods, optimizing evaluation
fidelity and achieving a significant reduction in LLM calls by 25-61%.
Accompanying our method is NLR-BIRD, the first dedicated dataset for NLR
benchmarking. Through human evaluations, we demonstrate the superior alignment
of Combo-Eval with human judgments, applicable across scenarios with and
without ground truth references.

</details>


### [4] [OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning](https://arxiv.org/abs/2510.23870)
*Marianne Menglin Liu,Sai Ashish Somayajula,Syed Fahad Allam Shah,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: OraPlan-SQL是一个用于Archer NL2SQL评估挑战赛的代理系统，采用双代理框架（规划器和SQL代理），通过反馈引导的元提示策略和计划多样化来提高性能，在双语基准测试中排名第一。


<details>
  <summary>Details</summary>
Motivation: 解决复杂推理（如算术、常识和假设推理）在双语NL2SQL任务中的挑战，同时克服传统多子代理方法在协调方面的开销问题。

Method: 使用双代理框架：规划器代理生成逐步自然语言计划，SQL代理将计划转换为可执行SQL。引入反馈引导的元提示策略来精炼单个规划器，并采用计划多样化（生成多个候选计划并通过多数投票选择最终输出）。

Result: 在Archer NL2SQL评估挑战赛2025中排名第一，执行准确率超过第二名6%以上（英文55.0%，中文56.7%），同时保持超过99%的SQL有效性。

Conclusion: 通过反馈引导的元提示策略和计划多样化，OraPlan-SQL在保持高SQL有效性的同时，显著提高了双语NL2SQL任务的执行准确率。

Abstract: We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge
2025, a bilingual benchmark requiring complex reasoning such as arithmetic,
commonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding
the second-best system by more than 6% in execution accuracy (EX), with 55.0%
in English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA).
Our system follows an agentic framework with two components: Planner agent that
generates stepwise natural language plans, and SQL agent that converts these
plans into executable SQL. Since SQL agent reliably adheres to the plan, our
refinements focus on the planner. Unlike prior methods that rely on multiple
sub-agents for planning and suffer from orchestration overhead, we introduce a
feedback-guided meta-prompting strategy to refine a single planner. Failure
cases from a held-out set are clustered with human input, and an LLM distills
them into corrective guidelines that are integrated into the planner's system
prompt, improving generalization without added complexity. For the multilingual
scenario, to address transliteration and entity mismatch issues, we incorporate
entity-linking guidelines that generate alternative surface forms for entities
and explicitly include them in the plan. Finally, we enhance reliability
through plan diversification: multiple candidate plans are generated for each
query, with the SQL agent producing a query for each plan, and final output
selected via majority voting over their executions.

</details>


### [5] [Agent-based Automated Claim Matching with Instruction-following LLMs](https://arxiv.org/abs/2510.23924)
*Dina Pisarevskaya,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 提出了一种基于智能体的自动化索赔匹配方法，使用指令遵循的LLMs构建两步流水线，首先生成提示，然后进行索赔匹配的二元分类任务。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自动化索赔匹配任务中的应用潜力，特别是通过智能体方法提升性能并降低计算资源需求。

Method: 采用两步流水线：1）使用LLMs生成提示；2）使用LLMs进行索赔匹配的二元分类。研究了不同LLMs在流水线各步骤中的组合使用。

Result: LLM生成的提示优于人类生成的SOTA提示；较小的LLMs在生成过程中表现与较大模型相当，可节省计算资源；不同LLMs组合使用有效。

Conclusion: 该方法证明了LLMs在索赔匹配任务中的有效性，为理解LLMs对索赔匹配的理解提供了洞察，并展示了资源优化的可能性。

Abstract: We present a novel agent-based approach for the automated claim matching task
with instruction-following LLMs. We propose a two-step pipeline that first
generates prompts with LLMs, to then perform claim matching as a binary
classification task with LLMs. We demonstrate that LLM-generated prompts can
outperform SOTA with human-generated prompts, and that smaller LLMs can do as
well as larger ones in the generation process, allowing to save computational
resources. We also demonstrate the effectiveness of using different LLMs for
each step of the pipeline, i.e. using an LLM for prompt generation, and another
for claim matching. Our investigation into the prompt generation process in
turn reveals insights into the LLMs' understanding of claim matching.

</details>


### [6] [Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward](https://arxiv.org/abs/2510.24020)
*Hao An,Yang Xu*

Main category: cs.CL

TL;DR: 提出了一个基于细粒度语义置信度奖励的强化学习框架，通过语义聚类引导LLM在知识边界外的问题上拒绝回答，显著减少了幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用粗粒度信号（如整体置信度或不确定性分数）来指导LLM拒绝回答超出知识范围的问题，这可能导致模型对自身知识边界认知不准确。

Method: 提出基于细粒度语义置信度奖励的强化学习框架，通过采样多个候选答案并进行语义聚类，训练LLM保留高置信度簇中的答案，丢弃低置信度簇中的答案。

Result: 该方法在领域内和领域外基准测试中都显著提高了可靠性。

Conclusion: 细粒度语义置信度奖励方法能够更精确地引导LLM识别知识边界，有效减少幻觉问题。

Abstract: Mitigating hallucinations in Large Language Models (LLMs) is critical for
their reliable deployment. Existing methods typically fine-tune LLMs to abstain
from answering questions beyond their knowledge scope. However, these methods
often rely on coarse-grained signals to guide LLMs to abstain, such as overall
confidence or uncertainty scores on multiple sampled answers, which may result
in an imprecise awareness of the model's own knowledge boundaries. To this end,
we propose a novel reinforcement learning framework built on
$\textbf{\underline{Fi}ne-grained \underline{S}emantic \underline{Co}nfidence
\underline{Re}ward (\Ours)}$, which guides LLMs to abstain via sample-specific
confidence. Specifically, our method operates by sampling multiple candidate
answers and conducting semantic clustering, then training the LLM to retain
answers within high-confidence clusters and discard those within low-confidence
ones, thereby promoting accurate post-hoc abstention. Additionally, we propose
a new metric for evaluating the reliability of abstention fine-tuning tasks
more comprehensively. Our method significantly enhances reliability in both
in-domain and out-of-distribution benchmarks.

</details>


### [7] [Pie: A Programmable Serving System for Emerging LLM Applications](https://arxiv.org/abs/2510.24051)
*In Gim,Zhiyao Ma,Seung-seob Lee,Lin Zhong*

Main category: cs.CL

TL;DR: Pie是一个可编程的LLM服务系统，通过将传统生成循环分解为细粒度服务处理器，并使用WebAssembly执行用户提供的inferlet程序，实现了灵活高效的LLM推理服务。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统基于单一令牌生成循环，难以支持多样化的推理策略和智能体工作流，需要更灵活的服务架构。

Method: 将传统生成循环分解为细粒度服务处理器，通过API暴露；使用用户提供的inferlet程序控制生成过程；采用WebAssembly进行轻量级沙箱执行。

Result: 在标准任务上性能接近最优（3-12%延迟开销），在智能体工作流上显著提升性能（延迟和吞吐量提高1.3x-3.4x）。

Conclusion: Pie系统通过可编程架构实现了LLM服务的灵活性和效率，特别适用于复杂的智能体工作流场景。

Abstract: Emerging large language model (LLM) applications involve diverse reasoning
strategies and agentic workflows, straining the capabilities of existing
serving systems built on a monolithic token generation loop. This paper
introduces Pie, a programmable LLM serving system designed for flexibility and
efficiency. Pie decomposes the traditional generation loop into fine-grained
service handlers exposed via an API and delegates control of the generation
process to user-provided programs, called inferlets. This enables applications
to implement new KV cache strategies, bespoke generation logic, and seamlessly
integrate computation and I/O-entirely within the application, without
requiring modifications to the serving system. Pie executes inferlets using
WebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows
Pie matches state-of-the-art performance on standard tasks (3-12% latency
overhead) while significantly improving latency and throughput (1.3x-3.4x
higher) on agentic workflows by enabling application-specific optimizations.

</details>


### [8] [Reinforcement Learning for Long-Horizon Multi-Turn Search Agents](https://arxiv.org/abs/2510.24126)
*Vivek Kalyan,Martin Andrews*

Main category: cs.CL

TL;DR: 使用强化学习训练的140亿参数模型在法律文档搜索基准上表现优于前沿模型，准确率达到85% vs 78%，并证明多轮交互能提升性能


<details>
  <summary>Details</summary>
Motivation: 探索强化学习如何通过从经验中学习来显著提升LLM智能体的能力，超越基于提示的方法

Method: 在法律文档搜索基准上使用强化学习训练140亿参数模型，并研究训练和测试时的轮次限制对性能的影响

Result: RL训练的模型在准确率上超越前沿模型（85% vs 78%），且多轮交互能带来更好的结果

Conclusion: 强化学习能显著提升LLM智能体的性能，多轮交互对任务完成至关重要

Abstract: Large Language Model (LLM) agents can leverage multiple turns and tools to
solve complex tasks, with prompt-based approaches achieving strong performance.
This work demonstrates that Reinforcement Learning (RL) can push capabilities
significantly further by learning from experience. Through experiments on a
legal document search benchmark, we show that our RL-trained 14 Billion
parameter model outperforms frontier class models (85% vs 78% accuracy). In
addition, we explore turn-restricted regimes, during training and at test-time,
that show these agents achieve better results if allowed to operate over longer
multi-turn horizons.

</details>


### [9] [Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in Large Language Models through Latent Semantic Alignment](https://arxiv.org/abs/2510.24208)
*Jian Gu,Aldeida Aleti,Chunyang Chen,Hongyu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于潜在空间语义对齐的大语言模型跨尺度知识转移方法，通过激活值而非层参数实现知识转移，解决了不同规模LLM之间的神经不兼容问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接重用层参数在不同规模LLM之间进行知识转移存在严重限制，主要由于神经不兼容性（架构和参数差异）。需要找到更有效的跨尺度知识转移方法以实现更大的灵活性和更广泛的应用。

Method: 将潜在空间语义对齐作为跨尺度知识转移的基础前提，使用激活值作为层间知识转移的媒介，利用潜在空间中的语义信息实现简单有效的知识转移。

Result: 在四个基准测试上的评估证明了该方法的有效性，相比先前工作表现更好，能更好地在不同规模模型之间对齐模型行为。

Conclusion: 该方法揭示了缓解跨尺度知识转移的关键因素，并为潜在语义对齐的本质提供了见解，为LLM知识转移提供了新思路。

Abstract: Large Language Models (LLMs) encode vast amounts of knowledge in their
massive parameters, which is accessible to locate, trace, and analyze. Despite
advances in neural interpretability, it is still not clear how to transfer
knowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).
A key problem is enabling effective and efficient knowledge transfer across
LLMs of different scales, which is essential for achieving greater flexibility
and broader applicability in transferring knowledge between LLMs. Due to neural
incompatibility, referring to the architectural and parametric differences
between LLMs of varying scales, existing methods that directly reuse layer
parameters are severely limited. In this paper, we identify the semantic
alignment in latent space as the fundamental prerequisite for LLM cross-scale
knowledge transfer. Instead of directly using the layer parameters, our
approach takes activations as the medium of layer-wise knowledge transfer.
Leveraging the semantics in latent space, our approach is simple and
outperforms prior work, better aligning model behaviors across varying scales.
Evaluations on four benchmarks demonstrate the efficacy of our method. Further
analysis reveals the key factors easing cross-scale knowledge transfer and
provides insights into the nature of latent semantic alignment.

</details>


### [10] [Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration in Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2510.24302)
*Shangyu Xing,Siyuan Wang,Chenyuan Yang,Xinyu Dai,Xiang Ren*

Main category: cs.CL

TL;DR: 提出LATR方法解决RLVR中轨迹多样性不足的问题，通过前瞻树搜索促进分支多样性，相比随机采样加速策略学习131%，提升最终性能4.2%。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法在群体rollout过程中采样轨迹多样性有限，同质化轨迹和奖励会削弱策略更新的回报信号，阻碍有效策略学习。

Method: LATR方法包含三个阶段：(1)在高不确定性生成步骤进行分支；(2)对每个新分支执行前瞻模拟；(3)在模拟过程中剪除持续相似的分支。

Result: 相比随机采样，LATR平均加速策略学习131%，在GRPO和DAPO算法上提升最终pass@1性能4.2%。

Conclusion: LATR通过显式促进轨迹级多样性，有效解决了RLVR中的轨迹同质化问题，显著提升了策略学习效率和最终性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), particularly with
algorithms like Group Relative Policy Optimization (GRPO), has proven highly
effective in enhancing the reasoning capabilities of large language models.
However, a critical bottleneck in current pipelines lies in the limited
diversity of sampled trajectories during group rollouts. Homogeneous
trajectories and their associated rewards would diminish the return signals for
policy updates, thereby hindering effective policy learning. This lack of
diversity stems primarily from token-level stochastic sampling, where local
variations are likely to collapse into near-identical reasoning paths. To
address this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a
novel rollout strategy designed to explicitly promotes trajectory-level
diversity by enforcing branching into different candidate tokens likely to
yield distinct continuations. Specifically, LATR iteratively operates in three
stages: (1) branching at high-uncertainty generation steps, (2) performing
lookahead simulation for each new branch, and (3) pruning branches that
exhibits prolonged similarity during simulation. Compared with stochastic
Sampling, LATR accelerates policy learning by 131% on average and improves
final pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy
Optimization (DAPO) algorithms across different reasoning tasks. Our code and
data are publicly available at https://github.com/starreeze/latr.

</details>


### [11] [Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning](https://arxiv.org/abs/2510.24320)
*Zhiheng Xi,Jixuan Huang,Xin Guo,Boyang Hong,Dingwen Yang,Xiaoran Fan,Shuo Li,Zehui Chen,Junjie Ye,Siyu Yuan,Zhengyin Du,Xuesong Yao,Yufei Xu,Jiecao Chen,Rui Zheng,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出Critique-RL方法，通过在线强化学习训练评论语言模型，无需强监督，采用两阶段优化策略提升评论者的判别能力和帮助性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖强监督标注评论数据，限制了评论语言模型的发展。

Method: 采用双玩家范式：演员生成响应，评论者提供反馈，演员据此优化响应。使用两阶段优化：第一阶段用规则奖励增强判别能力，第二阶段引入间接奖励提升帮助性。

Result: 在多个任务和模型上取得显著性能提升，如Qwen2.5-7B在域内任务提升9.02%，域外任务提升5.70%。

Conclusion: Critique-RL方法有效提升了评论语言模型的性能，展示了其潜力。

Abstract: Training critiquing language models to assess and provide feedback on model
outputs is a promising way to improve LLMs for complex reasoning tasks.
However, existing approaches typically rely on stronger supervisors for
annotating critique data. To address this, we propose Critique-RL, an online RL
approach for developing critiquing language models without stronger
supervision. Our approach operates on a two-player paradigm: the actor
generates a response, the critic provides feedback, and the actor refines the
response accordingly. We first reveal that relying solely on indirect reward
signals from the actor's outputs for RL optimization often leads to
unsatisfactory critics: while their helpfulness (i.e., providing constructive
feedback) improves, the discriminability (i.e., determining whether a response
is high-quality or not) remains poor, resulting in marginal performance gains.
To overcome this, Critique-RL adopts a two-stage optimization strategy. In
stage I, it reinforces the discriminability of the critic with direct
rule-based reward signals; in stage II, it introduces indirect rewards based on
actor refinement to improve the critic's helpfulness, while maintaining its
discriminability via appropriate regularization. Extensive experiments across
various tasks and models show that Critique-RL delivers substantial performance
improvements. For example, it achieves a 9.02% gain on in-domain tasks and a
5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.

</details>


### [12] [Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic Content](https://arxiv.org/abs/2510.24438)
*Abdullah Mushtaq,Rafay Naeem,Ezieddin Elmahjub,Ibrahim Ghaznavi,Shawqi Al-Maliki,Mohamed Abdallah,Ala Al-Fuqaha,Junaid Qadir*

Main category: cs.CL

TL;DR: 评估GPT-4o、Ansari AI和Fanar在伊斯兰指导任务中的表现，使用双代理框架进行定量和定性分析，发现现有模型在准确引用伊斯兰文本方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地用于伊斯兰指导，但存在误引文本、误用法理学或产生文化不一致回答的风险，需要评估其在伊斯兰知识领域的可靠性。

Method: 采用双代理框架：定量代理进行引用验证和六维度评分（结构、伊斯兰一致性、引用等），定性代理进行五维度并排比较（语调、深度、原创性等）。

Result: GPT-4o在伊斯兰准确性（3.93）和引用（3.38）方面得分最高，Ansari AI次之（3.68, 3.32），Fanar落后（2.76, 1.82）。GPT-4o定量平均分最高（3.90/5），Ansari AI在定性比较中获胜最多（116/200）。

Conclusion: 尽管表现相对较好，模型在可靠生成准确的伊斯兰内容和引用方面仍有不足，需要以穆斯林视角为中心的社区驱动基准。

Abstract: Large language models are increasingly used for Islamic guidance, but risk
misquoting texts, misapplying jurisprudence, or producing culturally
inconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar
on prompts from authentic Islamic blogs. Our dual-agent framework uses a
quantitative agent for citation verification and six-dimensional scoring (e.g.,
Structure, Islamic Consistency, Citations) and a qualitative agent for
five-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).
GPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI
followed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong
performance, models still fall short in reliably producing accurate Islamic
content and citations -- a paramount requirement in faith-sensitive writing.
GPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led
qualitative pairwise wins (116/200). Fanar, though trailing, introduces
innovations for Islamic and Arabic contexts. This study underscores the need
for community-driven benchmarks centering Muslim perspectives, offering an
early step toward more reliable AI in Islamic knowledge and other high-stakes
domains such as medicine, law, and journalism.

</details>


### [13] [SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space](https://arxiv.org/abs/2510.24446)
*Viktoriia Zinkovich,Anton Antonov,Andrei Spiridonov,Denis Shepelev,Andrey Moskalenko,Daria Pugacheva,Elena Tutubalina,Andrey Kuznetsov,Vlad Shakhuro*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的对抗性改写任务，通过生成语义等价但语法正确的文本改写来降低多模态大语言模型在推理分割任务中的性能，并开发了SPARTA方法来有效实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注图像输入的扰动，而在真实应用中用户会用多种方式表达相同意图，语义等价的文本改写对模型鲁棒性评估至关重要，但目前研究不足。

Method: 提出了SPARTA方法——一种黑盒、句子级的优化方法，在文本自编码器的低维语义潜在空间中操作，通过强化学习指导生成对抗性改写。

Result: SPARTA在ReasonSeg和LLMSeg-40k数据集上取得了显著更高的成功率，比现有方法高出最多2倍，成功揭示了先进推理分割模型对对抗性改写的脆弱性。

Conclusion: 即使受到严格的语义和语法约束，先进的推理分割模型仍然容易受到对抗性改写的攻击，这凸显了提高模型鲁棒性的必要性。

Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities
in vision-language tasks such as reasoning segmentation, where models generate
segmentation masks based on textual queries. While prior work has primarily
focused on perturbing image inputs, semantically equivalent textual
paraphrases-crucial in real-world applications where users express the same
intent in varied ways-remain underexplored. To address this gap, we introduce a
novel adversarial paraphrasing task: generating grammatically correct
paraphrases that preserve the original query meaning while degrading
segmentation performance. To evaluate the quality of adversarial paraphrases,
we develop a comprehensive automatic evaluation protocol validated with human
studies. Furthermore, we introduce SPARTA-a black-box, sentence-level
optimization method that operates in the low-dimensional semantic latent space
of a text autoencoder, guided by reinforcement learning. SPARTA achieves
significantly higher success rates, outperforming prior methods by up to 2x on
both the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive
baselines to assess the robustness of advanced reasoning segmentation models.
We reveal that they remain vulnerable to adversarial paraphrasing-even under
strict semantic and grammatical constraints. All code and data will be released
publicly upon acceptance.

</details>


### [14] [Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems](https://arxiv.org/abs/2510.24476)
*Yihan Li,Xiyuan Fu,Ghanshyam Verma,Paul Buitelaar,Mingming Liu*

Main category: cs.CL

TL;DR: 该论文系统分析了RAG和推理增强在缓解LLM幻觉方面的协同作用，提出了基于知识和逻辑的幻觉分类，并建立了统一的评估框架。


<details>
  <summary>Details</summary>
Motivation: 幻觉是LLM可靠部署的主要障碍，需要系统研究RAG和推理增强这两种最有效方法的协同机制。

Method: 采用应用导向的能力增强视角，分析RAG、推理增强及其在智能体系统中的整合如何缓解幻觉，提出幻觉分类学并建立统一框架。

Result: 提出了区分知识型和逻辑型幻觉的分类法，系统检验了RAG和推理如何分别解决这些问题，并提供了实际应用、评估和基准支持。

Conclusion: RAG和推理增强在缓解LLM幻觉方面具有重要协同潜力，需要系统整合以平衡创造性和可靠性。

Abstract: Hallucination remains one of the key obstacles to the reliable deployment of
large language models (LLMs), particularly in real-world applications. Among
various mitigation strategies, Retrieval-Augmented Generation (RAG) and
reasoning enhancement have emerged as two of the most effective and widely
adopted approaches, marking a shift from merely suppressing hallucinations to
balancing creativity and reliability. However, their synergistic potential and
underlying mechanisms for hallucination mitigation have not yet been
systematically examined. This survey adopts an application-oriented perspective
of capability enhancement to analyze how RAG, reasoning enhancement, and their
integration in Agentic Systems mitigate hallucinations. We propose a taxonomy
distinguishing knowledge-based and logic-based hallucinations, systematically
examine how RAG and reasoning address each, and present a unified framework
supported by real-world applications, evaluations, and benchmarks.

</details>


### [15] [CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?](https://arxiv.org/abs/2510.24505)
*Qing Zong,Jiayu Liu,Tianshi Zheng,Chunyang Li,Baixuan Xu,Haochen Shi,Weiqi Wang,Zhaowei Wang,Chunkit Chan,Yangqiu Song*

Main category: cs.CL

TL;DR: 该论文提出使用自然语言批评来增强大语言模型的置信度校准，通过分析批评内容和方式，开发了Self-Critique和CritiCal两种方法，显著提升了置信度校准性能。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型高风险应用中，准确的置信度校准至关重要。传统方法难以捕捉置信度评估所需的推理过程，而精确的黄金置信度标签难以获取，需要多次生成。

Method: 研究自然语言批评如何增强置信度表达：(1)分析批评内容：不确定性（问题导向）vs置信度（答案导向）；(2)提出Self-Critique方法让LLM自我批评优化置信度，以及CritiCal批评校准训练方法，利用自然语言批评改进置信度校准。

Result: 实验表明CritiCal显著优于Self-Critique和其他基线方法，在复杂推理任务中甚至超越了其教师模型GPT-4o，并在分布外设置中展现出鲁棒的泛化能力。

Conclusion: 自然语言批评是置信度校准的有效方法，CritiCal方法通过超越直接数值优化的方式，提升了LLM的可靠性。

Abstract: Accurate confidence calibration in Large Language Models (LLMs) is critical
for safe use in high-stakes domains, where clear verbalized confidence enhances
user trust. Traditional methods that mimic reference confidence expressions
often fail to capture the reasoning needed for accurate confidence assessment.
We propose natural language critiques as a solution, ideally suited for
confidence calibration, as precise gold confidence labels are hard to obtain
and often require multiple generations. This paper studies how natural language
critiques can enhance verbalized confidence, addressing: (1) What to critique:
uncertainty (question-focused) or confidence (answer-specific)? Analysis shows
confidence suits multiple-choice tasks, while uncertainty excels in open-ended
scenarios. (2) How to critique: self-critique or critique calibration training?
We propose Self-Critique, enabling LLMs to critique and optimize their
confidence beyond mere accuracy, and CritiCal, a novel Critique Calibration
training method that leverages natural language critiques to improve confidence
calibration, moving beyond direct numerical optimization. Experiments show that
CritiCal significantly outperforms Self-Critique and other competitive
baselines, even surpassing its teacher model, GPT-4o, in complex reasoning
tasks. CritiCal also shows robust generalization in out-of-distribution
settings, advancing LLM's reliability.

</details>


### [16] [OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning](https://arxiv.org/abs/2510.24636)
*Ziyou Hu,Zhengliang Shi,Minghang Zhu,Haitao Li,Teng Sun,Pengjie Ren,Suzan Verberne,Zhaochun Ren*

Main category: cs.CL

TL;DR: OpenRM是一个工具增强的长文本奖励模型，通过调用外部工具收集证据来系统性评估开放式回答，显著优于现有奖励建模方法。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在知识密集和长文本任务上表现不佳，因为评估正确性需要超出模型内部知识的基础，特别是在需要外部证据时难以可靠区分细微质量差异。

Method: 使用Group Relative Policy Optimization (GRPO)在超过27K合成成对示例上训练OpenRM，训练目标联合监督中间工具使用和最终结果准确性，学习基于证据的判断策略。

Result: 在三个新收集的数据集和两个广泛使用的基准测试上，OpenRM显著优于现有奖励建模方法，在下游LLM对齐任务中带来一致增益。

Conclusion: 工具增强的奖励模型具有扩展可靠长文本评估的潜力，可有效应用于推理时回答选择和训练时数据选择。

Abstract: Reward models (RMs) have become essential for aligning large language models
(LLMs), serving as scalable proxies for human evaluation in both training and
inference. However, existing RMs struggle on knowledge-intensive and long-form
tasks, where evaluating correctness requires grounding beyond the model's
internal knowledge. This limitation hinders them from reliably discriminating
subtle quality differences, especially when external evidence is necessary. To
address this, we introduce OpenRM, a tool-augmented long-form reward model that
systematically judges open-ended responses by invoking external tools to gather
relevant evidence. We train OpenRM with Group Relative Policy Optimization
(GRPO) on over 27K synthesized pairwise examples generated through a
controllable data synthesis framework. The training objective jointly
supervises intermediate tool usage and final outcome accuracy, incentivizing
our reward model to learn effective evidence-based judgment strategies.
Extensive experiments on three newly-collected datasets and two widely-used
benchmarks demonstrate that OpenRM substantially outperforms existing reward
modeling approaches. As a further step, we integrate OpenRM into both
inference-time response selection and training-time data selection. This yields
consistent gains in downstream LLM alignment tasks, highlighting the potential
of tool-augmented reward models for scaling reliable long-form evaluation.

</details>


### [17] [Dissecting Role Cognition in Medical LLMs via Neuronal Ablation](https://arxiv.org/abs/2510.24677)
*Xun Liang,Huayi Lai,Hanyu Wang,Wentao Zhang,Linfeng Zhang,Yanfang Chen,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 该研究评估了基于提示的角色扮演(PBRP)对大型语言模型医学推理能力的影响，发现角色提示主要影响语言风格而非推理能力。


<details>
  <summary>Details</summary>
Motivation: 评估角色提示是否能诱导LLMs产生角色特定的认知过程，还是仅仅改变语言风格。

Method: 使用RP-Neuron-Activated评估框架(RPNA)，在三个医学QA数据集上应用神经元消融和表示分析技术。

Result: 角色提示不会显著增强LLMs的医学推理能力，主要影响表面语言特征，没有发现不同临床角色间的推理路径差异。

Conclusion: 当前PBRP方法无法复制真实医学实践中的认知复杂性，需要模拟真正认知过程而非语言模仿的模型。

Abstract: Large language models (LLMs) have gained significant traction in medical
decision support systems, particularly in the
  context of medical question answering and role-playing simulations. A common
practice, Prompt-Based Role Playing (PBRP),
  instructs models to adopt different clinical roles (e.g., medical students,
residents, attending physicians) to simulate varied
  professional behaviors. However, the impact of such role prompts on model
reasoning capabilities remains unclear. This
  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to
evaluate whether role prompts induce distinct,
  role-specific cognitive processes in LLMs or merely modify linguistic style.
We test this framework on three medical QA
  datasets, employing neuron ablation and representation analysis techniques to
assess changes in reasoning pathways. Our
  results demonstrate that role prompts do not significantly enhance the
medical reasoning abilities of LLMs. Instead, they
  primarily affect surface-level linguistic features, with no evidence of
distinct reasoning pathways or cognitive differentiation
  across clinical roles. Despite superficial stylistic changes, the core
decision-making mechanisms of LLMs remain uniform
  across roles, indicating that current PBRP methods fail to replicate the
cognitive complexity found in real-world medical
  practice. This highlights the limitations of role-playing in medical AI and
emphasizes the need for models that simulate genuine
  cognitive processes rather than linguistic imitation.We have released the
related code in the following repository:https:
  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor

</details>


### [18] [Repurposing Synthetic Data for Fine-grained Search Agent Supervision](https://arxiv.org/abs/2510.24694)
*Yida Zhao,Kuan Li,Xixi Wu,Liwen Zhang,Dingchu Zhang,Baixuan Li,Maojia Song,Zhuo Chen,Chenxi Wang,Xinyu Wang,Kewei Tu,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: 论文提出了E-GRPO框架，通过利用实体信息来改进搜索代理的训练，相比传统GRPO方法能更好地从"接近正确"的样本中学习，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的搜索代理训练方法（如GRPO）丢弃了丰富的实体信息，仅依赖稀疏的结果奖励，无法区分具有正确推理但最终答案错误的"接近正确"样本与完全失败的样本，浪费了有价值的学习信号。

Method: 提出Entity-aware Group Relative Policy Optimization (E-GRPO)框架，构建密集的实体感知奖励函数，根据实体匹配率为错误样本分配部分奖励，使模型能从"接近正确"的样本中有效学习。

Result: 在多种问答和深度研究基准测试中，E-GRPO始终显著优于GRPO基线，不仅达到更高的准确率，还诱导出更高效的推理策略，需要更少的工具调用。

Conclusion: E-GRPO提供了一种更有效和样本高效的搜索代理对齐方法，通过利用实体信息显著提升了训练效果。

Abstract: LLM-based search agents are increasingly trained on entity-centric synthetic
data to solve complex, knowledge-intensive tasks. However, prevailing training
methods like Group Relative Policy Optimization (GRPO) discard this rich entity
information, relying instead on sparse, outcome-based rewards. This critical
limitation renders them unable to distinguish informative "near-miss"
samples-those with substantially correct reasoning but a flawed final
answer-from complete failures, thus discarding valuable learning signals. We
address this by leveraging the very entities discarded during training. Our
empirical analysis reveals a strong positive correlation between the number of
ground-truth entities identified during an agent's reasoning process and final
answer accuracy. Building on this insight, we introduce Entity-aware Group
Relative Policy Optimization (E-GRPO), a novel framework that formulates a
dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect
samples proportional to their entity match rate, enabling the model to
effectively learn from these "near-misses". Experiments on diverse
question-answering (QA) and deep research benchmarks show that E-GRPO
consistently and significantly outperforms the GRPO baseline. Furthermore, our
analysis reveals that E-GRPO not only achieves superior accuracy but also
induces more efficient reasoning policies that require fewer tool calls,
demonstrating a more effective and sample-efficient approach to aligning search
agents.

</details>


### [19] [AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis](https://arxiv.org/abs/2510.24695)
*Xuanzhong Chen,Zile Qiao,Guoxin Chen,Liangcai Su,Zhen Zhang,Xinyu Wang,Pengjun Xie,Fei Huang,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: 提出基于最近发展区理论的AgentFrontier引擎，通过自动化管道合成高质量多学科数据，训练出在Humanity's Last Exam等基准测试中达到SOTA的30B参数模型。


<details>
  <summary>Details</summary>
Motivation: 在LLM能力边界上训练智能体是解锁高级推理的关键，需要找到模型无法独立解决但能在指导下掌握的任务。

Method: 基于最近发展区理论构建AgentFrontier引擎，自动化合成高质量多学科数据，支持持续预训练和针对性后训练。

Result: 训练的AgentFrontier-30B-A3B模型在Humanity's Last Exam等基准测试中达到最先进水平，甚至超过部分领先的专有智能体。

Conclusion: ZPD指导的数据合成方法为构建更强大LLM智能体提供了可扩展且有效的路径。

Abstract: Training large language model agents on tasks at the frontier of their
capabilities is key to unlocking advanced reasoning. We introduce a data
synthesis approach inspired by the educational theory of the Zone of Proximal
Development (ZPD), which defines this frontier as tasks an LLM cannot solve
alone but can master with guidance. To operationalize this, we present the
AgentFrontier Engine, an automated pipeline that synthesizes high-quality,
multidisciplinary data situated precisely within the LLM's ZPD. This engine
supports both continued pre-training with knowledge-intensive data and targeted
post-training on complex reasoning tasks. From the same framework, we derive
the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent
capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on
our synthesized data, which achieves state-of-the-art results on demanding
benchmarks like Humanity's Last Exam, even surpassing some leading proprietary
agents. Our work demonstrates that a ZPD-guided approach to data synthesis
offers a scalable and effective path toward building more capable LLM agents.

</details>


### [20] [WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking](https://arxiv.org/abs/2510.24697)
*Zhengwei Tao,Haiyang Shen,Baixuan Li,Wenbiao Yin,Jialong Wu,Kuan Li,Zhongwang Zhang,Huifeng Yin,Rui Ye,Liwen Zhang,Xinyu Wang,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: 提出了WebLeaper框架，通过构建高覆盖率的信息搜索任务和生成高效解决方案轨迹，解决LLM智能体在信息搜索中效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的信息搜索智能体存在搜索效率低的问题，这限制了整体性能。主要原因是训练任务中目标实体的稀疏性，限制了智能体学习和泛化高效搜索行为的机会。

Method: 将信息搜索建模为树状结构推理问题，在受限上下文中嵌入更多目标实体。利用维基百科表格提出三种任务合成变体：Basic、Union和Reverse-Union，系统提升搜索效率和效果。通过筛选同时准确且高效的训练轨迹来优化模型。

Result: 在五个信息搜索基准测试（BrowserComp、GAIA、xbench-DeepSearch、WideSearch和Seal-0）上的广泛实验表明，该方法在基本和综合设置下都持续优于强基线，在效果和效率方面均有提升。

Conclusion: WebLeaper框架通过高覆盖率任务构建和高效轨迹生成，有效提升了LLM智能体的信息搜索性能，在多个基准测试中表现出优越的效果和效率。

Abstract: Large Language Model (LLM)-based agents have emerged as a transformative
approach for open-ended problem solving, with information seeking (IS) being a
core capability that enables autonomous reasoning and decision-making. While
prior research has largely focused on improving retrieval depth, we observe
that current IS agents often suffer from low search efficiency, which in turn
constrains overall performance. A key factor underlying this inefficiency is
the sparsity of target entities in training tasks, which limits opportunities
for agents to learn and generalize efficient search behaviors. To address these
challenges, we propose WebLeaper, a framework for constructing high-coverage IS
tasks and generating efficient solution trajectories. We formulate IS as a
tree-structured reasoning problem, enabling a substantially larger set of
target entities to be embedded within a constrained context. Leveraging curated
Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic,
Union, and Reverse-Union, to systematically increase both IS efficiency and
efficacy. Finally, we curate training trajectories by retaining only those that
are simultaneously accurate and efficient, ensuring that the model is optimized
for both correctness and search performance. Extensive experiments on both
basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,
GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method
consistently achieves improvements in both effectiveness and efficiency over
strong baselines.

</details>


### [21] [AgentFold: Long-Horizon Web Agents with Proactive Context Management](https://arxiv.org/abs/2510.24699)
*Rui Ye,Zhongwang Zhang,Kuan Li,Huifeng Yin,Zhengwei Tao,Yida Zhao,Liangcai Su,Liwen Zhang,Zile Qiao,Xinyu Wang,Pengjun Xie,Fei Huang,Siheng Chen,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: AgentFold是一种新型智能体范式，通过主动上下文管理解决长视野任务中的上下文饱和问题，在多尺度上管理历史轨迹，在BrowseComp基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决基于ReAct的智能体在长视野任务中面临的上下文饱和问题，以及固定总结方法可能导致关键细节丢失的缺陷。

Method: 引入AgentFold范式，将上下文视为动态认知工作空间，通过'折叠'操作在多尺度上管理历史轨迹，包括细粒度压缩和深度整合。

Result: AgentFold-30B-A3B在BrowseComp上达到36.2%，在BrowseComp-ZH上达到47.3%，超越了更大规模的开源模型和领先的专有智能体。

Conclusion: AgentFold通过主动上下文管理有效解决了长视野任务中的上下文挑战，展示了简单监督微调即可实现卓越性能。

Abstract: LLM-based web agents show immense promise for information seeking, yet their
effectiveness on long-horizon tasks is hindered by a fundamental trade-off in
context management. Prevailing ReAct-based agents suffer from context
saturation as they accumulate noisy, raw histories, while methods that fixedly
summarize the full history at each step risk the irreversible loss of critical
details. Addressing these, we introduce AgentFold, a novel agent paradigm
centered on proactive context management, inspired by the human cognitive
process of retrospective consolidation. AgentFold treats its context as a
dynamic cognitive workspace to be actively sculpted, rather than a passive log
to be filled. At each step, it learns to execute a `folding' operation, which
manages its historical trajectory at multiple scales: it can perform granular
condensations to preserve vital, fine-grained details, or deep consolidations
to abstract away entire multi-step sub-tasks. The results on prominent
benchmarks are striking: with simple supervised fine-tuning (without continual
pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp
and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or
matches open-source models of a dramatically larger scale, such as the
DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like
OpenAI's o4-mini.

</details>


### [22] [Tongyi DeepResearch Technical Report](https://arxiv.org/abs/2510.24701)
*Tongyi DeepResearch Team,Baixuan Li,Bo Zhang,Dingchu Zhang,Fei Huang,Guangyu Li,Guoxin Chen,Huifeng Yin,Jialong Wu,Jingren Zhou,Kuan Li,Liangcai Su,Litu Ou,Liwen Zhang,Pengjun Xie,Rui Ye,Wenbiao Yin,Xinmiao Yu,Xinyu Wang,Xixi Wu,Xuanzhong Chen,Yida Zhao,Zhen Zhang,Zhengwei Tao,Zhongwang Zhang,Zile Qiao,Chenxi Wang,Donglei Yu,Gang Fu,Haiyang Shen,Jiayin Yang,Jun Lin,Junkai Zhang,Kui Zeng,Li Yang,Hailong Yin,Maojia Song,Ming Yan,Peng Xia,Qian Xiao,Rui Min,Ruixue Ding,Runnan Fang,Shaowei Chen,Shen Huang,Shihang Wang,Shihao Cai,Weizhou Shen,Xiaobin Wang,Xin Guan,Xinyu Geng,Yingcheng Shi,Yuning Wu,Zhuo Chen,Zijian Li,Yong Jiang*

Main category: cs.CL

TL;DR: Tongyi DeepResearch是一个专为长周期深度信息检索研究任务设计的智能大语言模型代理，通过端到端训练框架实现可扩展的推理和信息检索能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决长周期、深度信息检索研究任务的挑战，需要开发能够自主进行深度研究的智能代理系统。

Method: 采用端到端的训练框架，结合代理中间训练和代理后训练，设计了高度可扩展的自动数据合成管道，无需人工标注，并为每个阶段构建定制化环境。

Result: Tongyi DeepResearch拥有305亿总参数，每token仅激活33亿参数，在多个代理深度研究基准测试中达到最先进性能。

Conclusion: 该模型在代理深度研究任务上表现出色，作者开源了模型、框架和完整解决方案以赋能社区。

Abstract: We present Tongyi DeepResearch, an agentic large language model, which is
specifically designed for long-horizon, deep information-seeking research
tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is
developed through an end-to-end training framework that combines agentic
mid-training and agentic post-training, enabling scalable reasoning and
information seeking across complex tasks. We design a highly scalable data
synthesis pipeline that is fully automatic, without relying on costly human
annotation, and empowers all training stages. By constructing customized
environments for each stage, our system enables stable and consistent
interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total
parameters, with only 3.3 billion activated per token, achieves
state-of-the-art performance across a range of agentic deep research
benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,
WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We
open-source the model, framework, and complete solutions to empower the
community.

</details>


### [23] [Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents](https://arxiv.org/abs/2510.24702)
*Yueqi Song,Ketan Ramaneti,Zaid Sheikh,Ziru Chen,Boyu Gou,Tianbao Xie,Yiheng Xu,Danyang Zhang,Apurva Gandhi,Fan Yang,Joseph Liu,Tianyue Ou,Zhihao Yuan,Frank Xu,Shuyan Zhou,Xingyao Wang,Xiang Yue,Tao Yu,Huan Sun,Yu Su,Graham Neubig*

Main category: cs.CL

TL;DR: 提出了agent数据协议(ADP)，作为异构代理数据集与统一训练管道之间的中间表示语言，通过标准化13个现有数据集并训练获得约20%性能提升


<details>
  <summary>Details</summary>
Motivation: 大规模代理监督微调的研究成果较少，主要因为代理训练数据收集面临异构格式、工具和接口的碎片化挑战

Method: 设计轻量级ADP表示语言作为中间语言，统一13个现有代理数据集格式，转换为多个代理框架的训练就绪格式进行SFT

Result: 在标准化ADP数据上训练后，相比基础模型平均性能提升约20%，在编码、浏览、工具使用和研究基准上达到SOTA或接近SOTA水平

Conclusion: ADP有助于降低标准化、可扩展和可复现代理训练的门槛，所有代码和数据已公开

Abstract: Public research results on large-scale supervised finetuning of AI agents
remain relatively rare, since the collection of agent training data presents
unique challenges. In this work, we argue that the bottleneck is not a lack of
underlying data sources, but that a large variety of data is fragmented across
heterogeneous formats, tools, and interfaces. To this end, we introduce the
agent data protocol (ADP), a light-weight representation language that serves
as an "interlingua" between agent datasets in diverse formats and unified agent
training pipelines downstream. The design of ADP is expressive enough to
capture a large variety of tasks, including API/tool use, browsing, coding,
software engineering, and general agentic workflows, while remaining simple to
parse and train on without engineering at a per-dataset level. In experiments,
we unified a broad collection of 13 existing agent training datasets into ADP
format, and converted the standardized ADP data into training-ready formats for
multiple agent frameworks. We performed SFT on these data, and demonstrated an
average performance gain of ~20% over corresponding base models, and delivers
state-of-the-art or near-SOTA performance on standard coding, browsing, tool
use, and research benchmarks, without domain-specific tuning. All code and data
are released publicly, in the hope that ADP could help lower the barrier to
standardized, scalable, and reproducible agent training.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [24] [How Cloudflare's client-side security made the npm supply chain attack a non-event](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fhow-cloudflares-client-side-security-made-the-npm-supply-chain-attack-a-non%2F%3Futm_source=tldrinfosec/1/0100019a25c8af65-d9fa5641-2669-408a-8358-61b883501c54-000000/FLNGez16bpnUtQZKoKYv4NrI1mf9A52s4eYbBrBaLOY=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: How Cloudflare's client-side security made the npm supply chain attack a non-event (4 minute read) Attackers used phishing to compromise npm maintainer accounts in September, injecting malicious code into 18 popular packages with over 2 billion weekly downloads to steal cryptocurrency and credentials from CI/CD pipelines. Cloudflare's Page Shield, using ML-based analysis of 3.5 billion scripts daily, detected all compromised packages with 98% precision and 90% recall by identifying obfuscated...

</details>


### [25] [Building Blocks of Agentic AI from Meta](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.meta.com%2Fblog%2Fintroducing-pytorch-native-agentic-stack%2F%3Futm_source=tldrai/1/0100019a25d2873b-230fc112-8543-482e-838d-f6d461d242fc-000000/gBrV2yBzRuP1hOE2cQkMT57PbHhSyLZtvcJC0lVEqAk=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Meta发布了一个基于PyTorch的开源工具栈，用于构建智能代理AI，支持内核开发、分布式系统和强化学习，能够处理大规模GPU工作负载和边缘部署。


<details>
  <summary>Details</summary>
Motivation: 为开发者提供一套完整的、基于PyTorch的智能代理AI开发工具，解决大规模GPU工作负载和边缘部署的需求。

Method: 开发了一个PyTorch原生的开源工具栈，包含内核开发、分布式系统和强化学习等组件。

Result: 成功构建了一个支持大规模GPU工作负载和边缘部署的智能代理AI开发平台。

Conclusion: 该工具栈为智能代理AI的开发提供了强大的基础设施支持。

Abstract: Building Blocks of Agentic AI from Meta (14 minute read) A PyTorch-native stack of open-source tools for agentic AI, including kernel development, distributed systems, and reinforcement learning, with support for massive GPU workloads and edge deployments.

</details>


### [26] [Code like a surgeon](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.geoffreylitt.com%2F2025%2F10%2F24%2Fcode-like-a-surgeon%3Futm_source=tldrai/1/0100019a25d2873b-230fc112-8543-482e-838d-f6d461d242fc-000000/7UawMihjpyfm8XXi8WA_G8K1m3IcfWxfnzMua1IMh8s=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI agents should被用于处理次要任务，如编写文档、修复TypeScript错误和快速尝试变更，以便开发者能够专注于核心工作。


<details>
  <summary>Details</summary>
Motivation: 提高开发效率，通过将繁琐的次要任务委托给AI代理，使开发者能够集中精力在真正重要的核心工作上。

Method: 利用AI代理异步处理后台任务，如文档编写、错误修复和变更尝试。

Result: 开发者能够更专注于核心开发任务，提高整体工作效率。

Conclusion: AI代理在软件开发中应被用于处理次要任务，以优化开发者的工作流程和效率。

Abstract: Code like a surgeon (2 minute read) The goal isn't delegating your core work and then becoming an auditor or editor. It should be offloading secondary tasks so you can focus on what really matters. Tasks like writing documentation, fixing TypeScript errors, and spiking out changes are the kind of grunt work AI agents excel at that can run async in the background.

</details>


### [27] [When 'perfect' code fails](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmarma.dev%2Farticles%2F2025%2Fwhen-perfect-code-fails%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/PRd0SQPz2rHmdpECNzZoWM8DIMHCqZgzY-2lHy1Y84E=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Next.js应用中一个看似完美的单行相等性检查导致严重安全漏洞，原因是服务器函数实现将同步函数意外转换为异步函数，返回的Promise在if语句中被评估为true，从而绕过安全检查。


<details>
  <summary>Details</summary>
Motivation: 揭示Next.js框架中服务器函数实现可能导致的意外行为，特别是同步函数被转换为异步函数时引发的安全风险。

Method: 通过分析实际案例，研究Next.js服务器函数如何将同步函数转换为返回Promise的异步函数，以及JavaScript中Promise在布尔上下文中的行为。

Result: 发现同步函数在Next.js服务器环境中被意外转换为异步函数，导致返回的Promise在if语句中始终评估为true，从而完全绕过安全检查机制。

Conclusion: 开发人员需要警惕框架的隐式行为转换，特别是在安全关键代码中，应明确处理异步操作和Promise返回值。

Abstract: When 'perfect' code fails (6 minute read) A critical security bug found in this dev's Next.js application was caused by a seemingly perfect one-line equality check. Due to Next.js's server function implementation, the synchronous function was unexpectedly converted to an asynchronous function returning a Promise. In JavaScript, a Promise evaluates to true in an `if` statement, effectively bypassing the security check and granting access to everyone.

</details>


### [28] [The New Calculus of AI-based Coding](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.joemag.dev%2F2025%2F10%2Fthe-new-calculus-of-ai-based-coding.html%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/Ir7bGfo3VfXoEeHooqaSU_mXXkZN6ua8EirqIrRiWFo=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用AI代理编写代码可实现10倍编码吞吐量提升，但需要改变传统软件开发实践以避免错误和瓶颈增加


<details>
  <summary>Details</summary>
Motivation: 探索AI代理在编码中的潜力，提高开发效率，同时应对由此带来的测试、部署和团队协作挑战

Method: 使用AI代理进行代码编写，调整软件开发流程中的测试、部署和团队协调方法

Result: 编码吞吐量提升10倍，但需要相应改进测试方法和基础设施来维持高速度

Conclusion: AI代理编码需要软件开发实践的转变，特别是测试、部署和团队协调方面，以充分利用其效率优势

Abstract: The New Calculus of AI-based Coding (12 minute read) This team used AI agents to write code, resulting in a 10x increase in coding throughput. This agentic coding requires a shift in traditional software development practices, especially in testing, deployment, and team coordination, to avoid increased bugs and bottlenecks. AI agents can also be used to reduce the costs of implementing testing approaches and improving the infrastructure needed to sustain high velocity.

</details>


### [29] [AI can code, but it can't build software](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbytesauna.com%2Fpost%2Fcoding-vs-software-engineering%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/JJk3WDaK_K58ragKiwjj6xYfTo4tnPl4B9YO9yqSZ-I=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI can code but cannot build production-ready software due to limitations in integration, expansion, and long-term maintainability.


<details>
  <summary>Details</summary>
Motivation: To highlight the gap between AI's coding capabilities and the complexities of software engineering, as evidenced by increased demand for technical cofounders.

Method: Analysis of AI's current limitations in software development based on industry trends and technical requirements.

Result: AI excels at isolated coding tasks but fails in building and maintaining complex software systems.

Conclusion: There is a significant gap between AI's coding abilities and the comprehensive skills required for software engineering.

Abstract: AI can code, but it can't build software (3 minute read) An increase in requests for technical cofounders suggests a gap in what AI can currently accomplish. While AI is great at coding specific, isolated tasks, it struggles with the complexities of building and maintaining production-ready software. Software engineering is different from mere coding, as it involves integration, expansion, and long-term maintainability, which AI currently doesn't have.

</details>


### [30] [Using Model Context Protocol with Expo](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.expo.dev%2Feas%2Fai%2Fmcp%2F%3Futm_source=tldrwebdev/1/0100019a2a8106b0-64ae2baa-ee42-4865-b6b8-a4e8e2a3722a-000000/DYalQLFvlav7p9zHw0gC_BK-O13FmjoiT3ddElwhEKQ=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Expo MCP Server 将AI编程代理连接到Expo项目，使其能够访问文档、自动化工作流程并视觉验证应用，支持与Claude Code、Cursor和VS Code等工具集成。


<details>
  <summary>Details</summary>
Motivation: 为AI编程代理提供与Expo项目的深度集成能力，使其能够更好地理解和操作移动应用开发环境，提升自动化开发效率。

Method: 使用Model Context Protocol (MCP)构建Expo MCP Server，通过本地能力如截图、打开DevTools、自动化点击视图等功能，将AI代理与Expo SDK和移动模拟器连接。

Result: 成功实现了AI编程代理与Expo项目的无缝集成，使代理能够访问文档、自动化工作流程并进行视觉验证。

Conclusion: Expo MCP Server为AI驱动的移动应用开发提供了强大的基础设施，显著提升了开发自动化和效率。

Abstract: Using Model Context Protocol (MCP) with Expo (7 minute read) The Expo MCP Server connects AI coding agents to your Expo projects so they can access docs, automate workflows, and verify your app visually. It integrates with tools like Claude Code, Cursor, and VS Code, teaching them about the Expo SDK and enabling interaction with mobile simulators and React Native DevTools. Some local capabilities include taking screenshots, opening DevTools, and automation features like tapping views and find...

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Chain of Execution Supervision Promotes General Reasoning in Large Language Models](https://arxiv.org/abs/2510.23629)
*Nuo Chen,Zehua Li,Keqin Bao,Junyang Lin,Dayiheng Liu*

Main category: cs.LG

TL;DR: TracePile是一个包含260万样本的大规模语料库，将代码执行转化为显式的逐步推理过程（Chain of Execution），通过继续预训练、指令微调和两阶段微调在多个模型和基准测试中取得一致改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接在原始代码上训练存在不足，因为代码中的推理过程通常是隐式的，并与语法或实现噪声纠缠在一起。需要将代码执行转化为显式的逐步推理过程来提升模型的推理能力。

Method: 构建TracePile语料库，将代码执行转化为Chain of Execution风格的逐步推理过程，包含变量追踪问题和代码重写，采用继续预训练、指令微调和两阶段微调三种训练设置。

Result: 在四个基础模型（LLaMA 3、LLaMA 3.1、Qwen-2.5和Qwen-2.5 Coder）和20个基准测试中均显示出一致改进，LLaMA3.1-8B在九个数学数据集上平均提升7.1%，在LiveCodeBench、CRUX和MMLU上也有明显提升。

Conclusion: TracePile通过将代码执行转化为显式推理过程，有效提升了语言模型的推理能力，在各种基准测试中取得了显著改进。

Abstract: Building robust and general reasoning ability is a central goal in the
development of large language models (LLMs). Recent efforts increasingly turn
to code as a rich training source, given its inherent logical structure and
diverse reasoning paradigms such as divide-and-conquer, topological ordering,
and enumeration. However, reasoning in code is often expressed implicitly and
entangled with syntactic or implementation noise, making direct training on raw
code suboptimal.To address this, we introduce TracePile, a large-scale corpus
of 2.6 million samples that transforms code execution into explicit,
step-by-step chain-of-thought-style rationales, which we call Chain of
Execution (CoE). The corpus spans domains including mathematics, classical
algorithms and algorithmic competition, and is enriched with variable-tracing
questions and code rewritings to enhance logical granularity and code
diversity. We evaluate TracePile using three training setups:
continue-pretraining, instruction tuning after pretraining, and two-stage
finetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,
and Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and
algorithms demonstrate consistent improvements. Notably, TracePile boosts
LLaMA3.1-8B by 7.1\% on average across nine math datasets and delivers clear
gains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.

</details>


### [32] [Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents](https://arxiv.org/abs/2510.23682)
*Gokturk Aytug Akarlar*

Main category: cs.LG

TL;DR: 提出Chimera神经-符号-因果架构，集成LLM策略师、形式验证符号约束引擎和因果推理模块，在电商环境中相比纯LLM和LLM+符号约束方法显著提升利润和品牌信任度。


<details>
  <summary>Details</summary>
Motivation: LLM代理在高风险领域部署存在严重脆弱性，相同能力因提示框架不同而产生截然不同的结果，需要架构级安全保障。

Method: 开发Chimera三组件架构：LLM策略师负责决策，形式验证符号约束引擎确保安全约束，因果推理模块进行反事实推理。在包含价格弹性、信任动态和季节性需求的52周电商模拟中进行基准测试。

Result: 纯LLM代理在量优化场景中造成99K美元总损失，在利润优化场景中品牌信任下降48.6%。添加符号约束可防止灾难但仅达到Chimera利润的43-87%。Chimera实现最高回报（152万和196万美元，部分案例+220万美元）并提升品牌信任（+1.8%和+10.8%，部分案例+20.86%）。

Conclusion: 架构设计而非提示工程决定了自主代理在生产环境中的可靠性。TLA+形式验证证明所有场景中零约束违规。

Abstract: Large language models show promise as autonomous decision-making agents, yet
their deployment in high-stakes domains remains fraught with risk. Without
architectural safeguards, LLM agents exhibit catastrophic brittleness:
identical capabilities produce wildly different outcomes depending solely on
prompt framing. We present Chimera, a neuro-symbolic-causal architecture that
integrates three complementary components - an LLM strategist, a formally
verified symbolic constraint engine, and a causal inference module for
counterfactual reasoning. We benchmark Chimera against baseline architectures
(LLM-only, LLM with symbolic constraints) across 52-week simulations in a
realistic e-commerce environment featuring price elasticity, trust dynamics,
and seasonal demand. Under organizational biases toward either volume or margin
optimization, LLM-only agents fail catastrophically (total loss of \$99K in
volume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding
symbolic constraints prevents disasters but achieves only 43-87% of Chimera's
profit. Chimera consistently delivers the highest returns (\$1.52M and \$1.96M
respectively, some cases +\$2.2M) while improving brand trust (+1.8% and
+10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+
formal verification proves zero constraint violations across all scenarios.
These results establish that architectural design not prompt engineering
determines the reliability of autonomous agents in production environments. We
provide open-source implementations and interactive demonstrations for
reproducibility.

</details>


### [33] [A Pragmatic Way to Measure Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.23966)
*Scott Emmons,Roland S. Zimmermann,David K. Elson,Rohin Shah*

Main category: cs.LG

TL;DR: 提出了一种测量思维链可监控性的实用方法，包括可读性和覆盖度两个指标，并使用自动评分器提示来评估现有CoT的质量。


<details>
  <summary>Details</summary>
Motivation: 随着训练实践和模型架构的变化，思维链监控这一AI安全机会可能丧失，需要开发方法来保持可监控性。

Method: 设计可读性和覆盖度指标，通过自动评分器提示让LLM计算现有CoT的这两个指标，并在合成CoT退化上进行验证。

Result: 在多个前沿模型和挑战性基准测试中发现它们表现出高可监控性。

Conclusion: 该方法有助于测量CoT的默认可监控性，是对抗性压力测试的补充而非替代。

Abstract: While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI
safety, this opportunity could be lost through shifts in training practices or
model architecture. To help preserve monitorability, we propose a pragmatic way
to measure two components of it: legibility (whether the reasoning can be
followed by a human) and coverage (whether the CoT contains all the reasoning
needed for a human to also produce the final output). We implement these
metrics with an autorater prompt that enables any capable LLM to compute the
legibility and coverage of existing CoTs. After sanity-checking our prompted
autorater with synthetic CoT degradations, we apply it to several frontier
models on challenging benchmarks, finding that they exhibit high
monitorability. We present these metrics, including our complete autorater
prompt, as a tool for developers to track how design decisions impact
monitorability. While the exact prompt we share is still a preliminary version
under ongoing development, we are sharing it now in the hopes that others in
the community will find it useful. Our method helps measure the default
monitorability of CoT - it should be seen as a complement, not a replacement,
for the adversarial stress-testing needed to test robustness against
deliberately evasive models.

</details>


### [34] [Aligning Diffusion Language Models via Unpaired Preference Optimization](https://arxiv.org/abs/2510.23658)
*Vaibhav Jindal,Hejian Sang,Chun-Mao Lai,Yanning Chen,Zhipeng Wang*

Main category: cs.LG

TL;DR: ELBO-KTO方法结合ELBO替代扩散对数似然与无配对偏好目标，在扩散语言模型中实现有效的人类偏好对齐，性能优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型的对齐面临序列对数似然难以计算和配对偏好数据收集成本高的问题，需要开发更高效的优化方法。

Method: 提出ELBO-KTO方法，使用ELBO替代扩散对数似然，结合基于前景理论的非配对偏好优化目标，并采用方差减少技术稳定训练梯度。

Result: 在LLaDA-8B-Instruct上，ELBO-KTO在kto-mix-14k和UltraFeedback-Binary上分别获得65.9%和62.3%的调整胜率，在GSM8K、MMLU等下游任务中表现与基准模型相当或更好。

Conclusion: 非配对偏好优化是扩散语言模型中配对对齐的可行替代方案。

Abstract: Diffusion language models (dLLMs) are an emerging alternative to
autoregressive (AR) generators, but aligning them to human preferences is
challenging because sequence log-likelihoods are intractable and pairwise
preference data are costly to collect. We introduce ELBO-KTO, which combines an
ELBO surrogate for diffusion log-likelihoods with a prospect-theoretic,
unpaired preference objective (Kahneman Tversky Optimization, KTO). We analyze
the bias and variance induced by the ELBO substitution and employ
variance-reduction practices that stabilize gradients during training. Applied
to LLaDA-8B-Instruct, ELBO-KTO yields \textbf{65.9\%} and \textbf{62.3\%}
adjusted win rates on kto-mix-14k and UltraFeedback-Binary, respectively,
versus the base model under an automatic LLM judge. Across downstream tasks,
including GSM8K, MMLU, and additional reasoning/knowledge benchmarks, ELBO-KTO
trained on UltraFeedback-Binary performs on par with or better than the base
model under identical decoding. This establishes unpaired preference
optimization as a viable alternative to pairwise alignment in diffusion LLMs.

</details>


### [35] [ChessQA: Evaluating Large Language Models for Chess Understanding](https://arxiv.org/abs/2510.23948)
*Qianfeng Wen,Zhenwei Tang,Ashton Anderson*

Main category: cs.LG

TL;DR: ChessQA是一个全面的国际象棋基准测试，评估LLM在五个任务类别中的国际象棋理解能力，从基本规则到高级概念，超越了简单的走子质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM在国际象棋中能力的评估是临时性的且范围狭窄，难以准确衡量LLM的国际象棋理解能力及其随规模、训练后方法或架构选择的变化。

Method: 创建ChessQA基准测试，涵盖五个任务类别：结构、模式、短战术、位置判断和语义，对应玩家积累国际象棋知识时掌握的递进抽象层次。

Result: 评估当代LLM发现所有五个类别都存在持续弱点，并通过类别提供结果和错误分析。

Conclusion: ChessQA提供了更全面的国际象棋能力评估，为诊断和比较提供了受控一致的环境，并将发布代码、定期更新的数据集和公共排行榜。

Abstract: Chess provides an ideal testbed for evaluating the reasoning, modeling, and
abstraction capabilities of large language models (LLMs), as it has
well-defined structure and objective ground truth while admitting a wide
spectrum of skill levels. However, existing evaluations of LLM ability in chess
are ad hoc and narrow in scope, making it difficult to accurately measure LLM
chess understanding and how it varies with scale, post-training methodologies,
or architecture choices. We present ChessQA, a comprehensive benchmark that
assesses LLM chess understanding across five task categories (Structural,
Motifs, Short Tactics, Position Judgment, and Semantic), which approximately
correspond to the ascending abstractions that players master as they accumulate
chess knowledge, from understanding basic rules and learning tactical motifs to
correctly calculating tactics, evaluating positions, and semantically
describing high-level concepts. In this way, ChessQA captures a more
comprehensive picture of chess ability and understanding, going significantly
beyond the simple move quality evaluations done previously, and offers a
controlled, consistent setting for diagnosis and comparison. Furthermore,
ChessQA is inherently dynamic, with prompts, answer keys, and construction
scripts that can evolve as models improve. Evaluating a range of contemporary
LLMs, we find persistent weaknesses across all five categories and provide
results and error analyses by category. We will release the code, periodically
refreshed datasets, and a public leaderboard to support further research.

</details>


### [36] [PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling](https://arxiv.org/abs/2510.24235)
*Ai Jian,Jingqing Ruan,Xing Ma,Dailin Li,QianLin Zhou,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出了PaTaRM框架，将偏好感知奖励机制与动态标准适应相结合，无需显式逐点标注即可构建稳健的逐点训练信号，在RLHF中实现高效、可泛化和可解释的奖励建模。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型存在局限性：成对方法依赖二元标签导致推理不匹配，逐点方法需要复杂标注且适应性差。需要一种统一框架来解决这些问题。

Method: PaTaRM整合偏好感知奖励机制和动态标准适应，利用成对数据的相对偏好信息构建逐点训练信号，同时通过任务自适应标准系统灵活生成评估标准。

Result: 在RewardBench和RMBench上平均相对提升4.7%，在IFEval和InFoBench基准测试中下游RLHF性能平均提升13.6%。

Conclusion: PaTaRM证明了其有效性和鲁棒性，为RLHF提供了更高效的奖励建模方法。

Abstract: Reward models (RMs) are central to reinforcement learning from human feedback
(RLHF), providing the critical supervision signals that align large language
models (LLMs) with human preferences. While generative reward models (GRMs)
offer greater interpretability than traditional scalar RMs, current training
paradigms remain limited. Pair-wise methods rely on binary good-versus-bad
labels, which cause mismatches for point-wise inference and necessitate complex
pairing strategies for effective application in RLHF. On the other hand,
point-wise methods require more elaborate absolute labeling with rubric-driven
criteria, resulting in poor adaptability and high annotation costs. In this
work, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a
unified framework that integrates a preference-aware reward (PAR) mechanism
with dynamic rubric adaptation. PaTaRM leverages relative preference
information from pairwise data to construct robust point-wise training signals,
eliminating the need for explicit point-wise labels. Simultaneously, it employs
a task-adaptive rubric system that flexibly generates evaluation criteria for
both global task consistency and instance-specific fine-grained reasoning. This
design enables efficient, generalizable, and interpretable reward modeling for
RLHF. Extensive experiments show that PaTaRM achieves an average relative
improvement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B
models. Furthermore, PaTaRM boosts downstream RLHF performance, with an average
improvement of 13.6% across IFEval and InFoBench benchmarks, confirming its
effectiveness and robustness. Our code is available at
https://github.com/JaneEyre0530/PaTaRM.

</details>


### [37] [Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning](https://arxiv.org/abs/2510.24356)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 提出了感知学习(PeL)范式，通过任务无关信号优化智能体的感官接口，与下游决策学习解耦，直接针对无标签的感知属性进行优化。


<details>
  <summary>Details</summary>
Motivation: 传统方法将感知和决策耦合在一起训练，限制了感知模块的泛化能力和可解释性。PeL旨在分离这两个过程，使感知学习能够专注于感知质量本身。

Method: 定义感知属性（如对扰动的稳定性、信息性、几何控制），使用表示不变性指标进行评估，并证明PeL更新与贝叶斯任务风险梯度正交。

Result: 提供了形式化的感知-决策分离框架，定义了独立于目标任务和参数化的感知属性，并开发了任务无关的评估指标来验证感知质量。

Conclusion: PeL为构建更鲁棒和可解释的感知系统提供了理论基础和方法论，使感知学习能够独立于具体任务进行优化。

Abstract: We introduce Perception Learning (PeL), a paradigm that optimizes an agent's
sensory interface $f_\phi:\mathcal{X}\to\mathcal{Z}$ using task-agnostic
signals, decoupled from downstream decision learning
$g_\theta:\mathcal{Z}\to\mathcal{Y}$. PeL directly targets label-free
perceptual properties, such as stability to nuisances, informativeness without
collapse, and controlled geometry, assessed via objective
representation-invariant metrics. We formalize the separation of perception and
decision, define perceptual properties independent of objectives or
reparameterizations, and prove that PeL updates preserving sufficient
invariants are orthogonal to Bayes task-risk gradients. Additionally, we
provide a suite of task-agnostic evaluation metrics to certify perceptual
quality.

</details>


### [38] [Fill in the Blanks: Accelerating Q-Learning with a Handful of Demonstrations in Sparse Reward Settings](https://arxiv.org/abs/2510.24432)
*Seyed Mahdi Basiri Azad,Joschka Boedecker*

Main category: cs.LG

TL;DR: 提出了一种使用少量成功演示初始化强化学习智能体价值函数的简单有效方法，通过在稀疏奖励环境中预计算离线演示的价值估计作为早期学习目标，显著减少探索负担并提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境中的强化学习由于缺乏信息反馈而面临重大挑战，需要解决探索效率低下的问题。

Method: 采用混合离线到在线范式：首先从少量离线演示中预计算价值估计，作为早期学习目标；然后通过标准在线交互进行精炼。

Result: 在基准任务上的实验表明，该方法加速了收敛过程，即使使用最少或次优的演示数据，也优于标准基线方法。

Conclusion: 通过将离线演示的价值估计作为先验知识，可以有效提升稀疏奖励环境中强化学习的样本效率和性能。

Abstract: Reinforcement learning (RL) in sparse-reward environments remains a
significant challenge due to the lack of informative feedback. We propose a
simple yet effective method that uses a small number of successful
demonstrations to initialize the value function of an RL agent. By precomputing
value estimates from offline demonstrations and using them as targets for early
learning, our approach provides the agent with a useful prior over promising
actions. The agent then refines these estimates through standard online
interaction. This hybrid offline-to-online paradigm significantly reduces the
exploration burden and improves sample efficiency in sparse-reward settings.
Experiments on benchmark tasks demonstrate that our method accelerates
convergence and outperforms standard baselines, even with minimal or suboptimal
demonstration data.

</details>


### [39] [Sample-efficient and Scalable Exploration in Continuous-Time RL](https://arxiv.org/abs/2510.24482)
*Klemens Iten,Lenart Treven,Bhavya Sukhija,Florian Dörfler,Andreas Krause*

Main category: cs.LG

TL;DR: 提出COMBRL算法，用于连续时间强化学习，通过概率模型学习非线性ODE系统动力学，结合外部奖励和模型不确定性进行贪婪优化，实现可扩展和样本高效的模型强化学习。


<details>
  <summary>Details</summary>
Motivation: 现实世界的控制系统通常是连续时间的，但现有强化学习算法主要针对离散时间动态设计，需要解决连续时间环境下的学习问题。

Method: 使用高斯过程和贝叶斯神经网络等概率模型学习非线性ODE系统动力学，提出COMBRL算法贪婪最大化外部奖励和模型认知不确定性的加权和。

Result: COMBRL在奖励驱动设置下实现次线性遗憾，在无监督RL设置下提供样本复杂度界限，实验表明其比现有方法更具可扩展性和样本效率。

Conclusion: COMBRL为连续时间模型强化学习提供了一种有效方法，在多个深度RL任务中优于基线方法。

Abstract: Reinforcement learning algorithms are typically designed for discrete-time
dynamics, even though the underlying real-world control systems are often
continuous in time. In this paper, we study the problem of continuous-time
reinforcement learning, where the unknown system dynamics are represented using
nonlinear ordinary differential equations (ODEs). We leverage probabilistic
models, such as Gaussian processes and Bayesian neural networks, to learn an
uncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily
maximizes a weighted sum of the extrinsic reward and model epistemic
uncertainty. This yields a scalable and sample-efficient approach to
continuous-time model-based RL. We show that COMBRL achieves sublinear regret
in the reward-driven setting, and in the unsupervised RL setting (i.e., without
extrinsic rewards), we provide a sample complexity bound. In our experiments,
we evaluate COMBRL in both standard and unsupervised RL settings and
demonstrate that it scales better, is more sample-efficient than prior methods,
and outperforms baselines across several deep RL tasks.

</details>


### [40] [Greedy Sampling Is Provably Efficient for RLHF](https://arxiv.org/abs/2510.24700)
*Di Wu,Chengshuai Shi,Jing Yang,Cong Shen*

Main category: cs.LG

TL;DR: 本文提出了在通用偏好模型下RLHF的理论分析框架，通过贪婪采样而非乐观/悲观估计获得了比现有方法更好的性能保证，揭示了KL正则化目标下最优策略类的独特结构特性。


<details>
  <summary>Details</summary>
Motivation: RLHF在实践中取得了成功，但理论理解仍有限。现有研究主要关注基于奖励的Bradley-Terry偏好模型，而本文旨在分析更通用的偏好模型，并改进现有方法的性能保证。

Method: 采用直接使用经验估计（贪婪采样）的方法，而非构建乐观或悲观估计。分析了KL正则化目标下最优策略类的结构特性，并将其专门化到BT模型。

Result: 在通用偏好模型下获得了比现有方法显著改进的性能保证，证明了贪婪采样在RLHF中的充分性。

Conclusion: 贪婪采样在RLHF中是充分的，这源于KL正则化目标下最优策略类的独特结构特性，为RLHF的理论理解提供了新视角。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a key
technique for post-training large language models. Despite its empirical
success, the theoretical understanding of RLHF is still limited, as learning
the KL-regularized target with only preference feedback poses additional
challenges compared with canonical RL. Existing works mostly study the
reward-based Bradley-Terry (BT) preference model, and extend classical designs
utilizing optimism or pessimism. This work, instead, considers the general
preference model (whose practical relevance has been observed recently) and
obtains performance guarantees with major, order-wise improvements over
existing ones. Surprisingly, these results are derived from algorithms that
directly use the empirical estimates (i.e., greedy sampling), as opposed to
constructing optimistic or pessimistic estimates in previous works. This
insight has a deep root in the unique structural property of the optimal policy
class under the KL-regularized target, and we further specialize it to the BT
model, highlighting the surprising sufficiency of greedy sampling in RLHF.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [41] [Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents](https://arxiv.org/abs/2510.23691)
*Zihao Wang,Xujing Li,Yining Ye,Junjie Fang,Haoming Wang,Longxiang Liu,Shihao Liang,Junting Lu,Zhiyong Wu,Jiazhan Feng,Wanjun Zhong,Zili Li,Yu Wang,Yu Miao,Bo Zhou,Yuanfan Li,Hao Wang,Zhongkai Zhao,Faming Wu,Zhengxuan Jiang,Weihao Tan,Heyuan Yao,Shi Yan,Xiangyang Li,Yitao Liang,Yujia Qin,Guang Shi*

Main category: cs.AI

TL;DR: Game-TARS是一个通用游戏智能体，使用统一、可扩展的键盘鼠标动作空间进行训练，支持跨操作系统、网页和模拟游戏的大规模持续预训练，在多个游戏基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有API或GUI方法存在局限性，需要一种能够跨异构领域进行大规模预训练的通用游戏智能体范式。

Method: 采用统一键盘鼠标动作空间，使用500B+ token进行预训练，包含衰减持续损失减少因果混淆和稀疏思考策略平衡推理深度与推理成本。

Result: 在开放世界Minecraft任务中成功率是之前SOTA的2倍，在未见过的网页3D游戏中接近人类水平，在FPS基准测试中超越GPT-5、Gemini-2.5-Pro和Claude-4-Sonnet。

Conclusion: 简单可扩展的动作表示结合大规模预训练为开发具有广泛计算机使用能力的通用智能体提供了有前景的路径。

Abstract: We present Game-TARS, a generalist game agent trained with a unified,
scalable action space anchored to human-aligned native keyboard-mouse inputs.
Unlike API- or GUI-based approaches, this paradigm enables large-scale
continual pre-training across heterogeneous domains, including OS, web, and
simulation games. Game-TARS is pre-trained on over 500B tokens with diverse
trajectories and multimodal data. Key techniques include a decaying continual
loss to reduce causal confusion and an efficient Sparse-Thinking strategy that
balances reasoning depth and inference cost. Experiments show that Game-TARS
achieves about 2 times the success rate over the previous sota model on
open-world Minecraft tasks, is close to the generality of fresh humans in
unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet
in FPS benchmarks. Scaling results on training-time and test-time confirm that
the unified action space sustains improvements when scaled to cross-game and
multimodal data. Our results demonstrate that simple, scalable action
representations combined with large-scale pre-training provide a promising path
toward generalist agents with broad computer-use abilities.

</details>


### [42] [Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models](https://arxiv.org/abs/2510.23824)
*Murad Ismayilov,Edwin Meriaux,Shuo Wen,Gregory Dudek*

Main category: cs.AI

TL;DR: 该研究探讨了多智能体路径规划中的去中心化目标分配问题，智能体基于环境表示独立生成目标偏好，通过固定冲突解决规则进行分配，无需协商。LLM智能体在精心设计的提示下能实现接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化条件下多智能体在共享环境中的协调挑战，探索LLM在目标分配任务中的潜力。

Method: 智能体基于网格可视化和场景数据独立生成目标偏好排序，通过固定确定性冲突解决规则（如智能体索引排序）进行目标分配，无需协商。比较了贪心启发式、最优分配和LLM智能体的性能。

Result: LLM智能体在精心设计的提示和定量信息下，能实现接近最优的完工时间，并持续优于传统启发式方法。

Conclusion: 语言模型在多智能体路径规划的去中心化目标分配中具有潜力，信息结构在此类系统中至关重要。

Abstract: Coordinating multiple autonomous agents in shared environments under
decentralized conditions is a long-standing challenge in robotics and
artificial intelligence. This work addresses the problem of decentralized goal
assignment for multi-agent path planning, where agents independently generate
ranked preferences over goals based on structured representations of the
environment, including grid visualizations and scenario data. After this
reasoning phase, agents exchange their goal rankings, and assignments are
determined by a fixed, deterministic conflict-resolution rule (e.g., agent
index ordering), without negotiation or iterative coordination. We
systematically compare greedy heuristics, optimal assignment, and large
language model (LLM)-based agents in fully observable grid-world settings. Our
results show that LLM-based agents, when provided with well-designed prompts
and relevant quantitative information, can achieve near-optimal makespans and
consistently outperform traditional heuristics. These findings underscore the
potential of language models for decentralized goal assignment in multi-agent
path planning and highlight the importance of information structure in such
systems.

</details>


### [43] [From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production](https://arxiv.org/abs/2510.23856)
*Segev Shlomov,Alon Oved,Sami Marreed,Ido Levy,Offer Akrabi,Avi Yaeli,Łukasz Strąk,Elizabeth Koumpan,Yinon Goldshtein,Eilam Shapira,Nir Mashkif,Asaf Adi*

Main category: cs.AI

TL;DR: IBM开发了通用计算机代理CUGA，采用分层规划-执行架构，在AppWorld和WebArena上达到最先进性能，并在企业业务流程外包人才获取领域进行了试点评估。


<details>
  <summary>Details</summary>
Motivation: 解决企业从原型到部署系统的挑战，克服框架碎片化、开发缓慢和缺乏标准化评估的问题，探索通用代理在企业环境中的应用。

Method: 采用分层规划-执行架构，具有强大的分析基础，并引入BPO-TA基准测试（包含26个任务和13个分析端点）进行评估。

Result: CUGA在基准测试中表现优异，在企业试点中接近专业代理的准确性，同时显示出减少开发时间和成本的潜力。

Conclusion: 提供了通用代理在企业规模运行的早期证据，并总结了技术和组织经验，为将研究级架构发展为健壮的企业就绪系统奠定了基础。

Abstract: Agents are rapidly advancing in automating digital work, but enterprises face
a harder challenge: moving beyond prototypes to deployed systems that deliver
measurable business value. This path is complicated by fragmented frameworks,
slow development, and the absence of standardized evaluation practices.
Generalist agents have emerged as a promising direction, excelling on academic
benchmarks and offering flexibility across task types, applications, and
modalities. Yet, evidence of their use in production enterprise settings
remains limited. This paper reports IBM's experience developing and piloting
the Computer Using Generalist Agent (CUGA), which has been open-sourced for the
community (https://github.com/cuga-project/cuga-agent). CUGA adopts a
hierarchical planner--executor architecture with strong analytical foundations,
achieving state-of-the-art performance on AppWorld and WebArena. Beyond
benchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing
talent acquisition domain, addressing enterprise requirements for scalability,
auditability, safety, and governance. To support assessment, we introduce
BPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary
evaluations, CUGA approached the accuracy of specialized agents while
indicating potential for reducing development time and cost. Our contribution
is twofold: presenting early evidence of generalist agents operating at
enterprise scale, and distilling technical and organizational lessons from this
initial pilot. We outline requirements and next steps for advancing
research-grade architectures like CUGA into robust, enterprise-ready systems.

</details>


### [44] [Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges](https://arxiv.org/abs/2510.23883)
*Shrestha Datta,Shahriar Kabir Nahin,Anshuman Chhabra,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 本文调查了基于大语言模型的智能代理系统带来的新型安全风险，提出了威胁分类法，并讨论了评估方法和防御策略。


<details>
  <summary>Details</summary>
Motivation: 智能AI系统在网页、软件和物理环境中自主执行任务的能力创造了新的安全风险，这些风险不同于传统的AI安全和软件安全。

Method: 通过调查分析，构建了智能代理特定的威胁分类法，回顾了最近的基准测试和评估方法，并从技术和治理角度讨论了防御策略。

Result: 系统性地识别了智能代理系统的安全威胁，并提出了相应的评估框架和防御方案。

Conclusion: 当前研究为开发安全设计的智能代理系统提供了基础，但仍存在许多开放挑战需要解决。

Abstract: Agentic AI systems powered by large language models (LLMs) and endowed with
planning, tool use, memory, and autonomy, are emerging as powerful, flexible
platforms for automation. Their ability to autonomously execute tasks across
web, software, and physical environments creates new and amplified security
risks, distinct from both traditional AI safety and conventional software
security. This survey outlines a taxonomy of threats specific to agentic AI,
reviews recent benchmarks and evaluation methodologies, and discusses defense
strategies from both technical and governance perspectives. We synthesize
current research and highlight open challenges, aiming to support the
development of secure-by-design agent systems.

</details>


### [45] [Affordance Representation and Recognition for Autonomous Agents](https://arxiv.org/abs/2510.24459)
*Habtom Kahsay Gidey,Niklas Huber,Alexander Lenz,Alois Knoll*

Main category: cs.AI

TL;DR: 提出两种架构模式：DOM转换模式和超媒体功能识别模式，用于从结构化数据构建世界模型，解决网页复杂性和服务动态适应性问题。


<details>
  <summary>Details</summary>
Motivation: 软件代理的自主性依赖于从结构化数据构建可操作的世界模型，但原始HTML的冗长性和静态API集成的限制阻碍了代理的适应性。

Method: DOM转换模式将冗长的原始DOM提炼为紧凑的任务相关表示；超媒体功能识别模式通过解析语义描述动态发现和集成未知Web服务的能力。

Result: 这两种模式提供了一个稳健框架，使代理能够高效构建和维护准确的世界模型。

Conclusion: 这些模式支持可扩展、自适应和可互操作的自动化，适用于Web及其扩展资源。

Abstract: The autonomy of software agents is fundamentally dependent on their ability
to construct an actionable internal world model from the structured data that
defines their digital environment, such as the Document Object Model (DOM) of
web pages and the semantic descriptions of web services. However, constructing
this world model from raw structured data presents two critical challenges: the
verbosity of raw HTML makes it computationally intractable for direct use by
foundation models, while the static nature of hardcoded API integrations
prevents agents from adapting to evolving services.
  This paper introduces a pattern language for world modeling from structured
data, presenting two complementary architectural patterns. The DOM Transduction
Pattern addresses the challenge of web page complexity by distilling} a
verbose, raw DOM into a compact, task-relevant representation or world model
optimized for an agent's reasoning core. Concurrently, the Hypermedia
Affordances Recognition Pattern enables the agent to dynamically enrich its
world model by parsing standardized semantic descriptions to discover and
integrate the capabilities of unknown web services at runtime. Together, these
patterns provide a robust framework for engineering agents that can efficiently
construct and maintain an accurate world model, enabling scalable, adaptive,
and interoperable automation across the web and its extended resources.

</details>


### [46] [MGA: Memory-Driven GUI Agent for Observation-Centric Interaction](https://arxiv.org/abs/2510.24168)
*Weihua Cheng,Ersheng Ni,Wenlong Wang,Yifei Sun,Junming Liu,Wangyu Shen,Yirong Chen,Botian Shi,Ding Wang*

Main category: cs.AI

TL;DR: 提出了MGA（Memory-Driven GUI Agent），通过"先观察后决策"原则重构GUI交互，解决了现有方法对历史轨迹的依赖和局部探索偏差问题，在多个基准测试中表现出更强的鲁棒性、泛化性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理方法存在两个主要问题：对历史轨迹的依赖导致错误传播放大，以及"先决策后观察"机制忽视关键界面线索。需要开发更鲁棒和泛化的GUI交互方法。

Method: MGA将每个步骤建模为独立的环境状态，包含三个要素：当前屏幕截图、任务无关的空间信息、动态更新的结构化记忆。采用"先观察后决策"原则。

Result: 在OSworld基准测试、真实桌面应用（Chrome、VSCode、VLC）和跨任务迁移实验中，MGA相比最先进基线方法在鲁棒性、泛化性和效率方面取得显著提升。

Conclusion: MGA通过重构GUI交互范式，有效解决了现有方法的局限性，为GUI代理的发展提供了新的方向。

Abstract: The rapid progress of Large Language Models (LLMs) and their multimodal
extensions (MLLMs) has enabled agentic systems capable of perceiving and acting
across diverse environments. A challenging yet impactful frontier is the
development of GUI agents, which must navigate complex desktop and web
interfaces while maintaining robustness and generalization. Existing paradigms
typically model tasks as long-chain executions, concatenating historical
trajectories into the context. While approaches such as Mirage and GTA1 refine
planning or introduce multi-branch action selection, they remain constrained by
two persistent issues: Dependence on historical trajectories, which amplifies
error propagation. And Local exploration bias, where "decision-first,
observation-later" mechanisms overlook critical interface cues. We introduce
the Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the
principle of observe first, then decide. MGA models each step as an
independent, context-rich environment state represented by a triad: current
screenshot, task-agnostic spatial information, and a dynamically updated
structured memory. Experiments on OSworld benchmarks, real desktop applications
(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves
substantial gains in robustness, generalization, and efficiency compared to
state-of-the-art baselines. The code is publicly available at:
{https://anonymous.4open.science/r/MGA-3571}.

</details>


### [47] [MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools](https://arxiv.org/abs/2510.24284)
*Wenhao Wang,Peizhi Niu,Zhao Xu,Zhaoyu Chen,Jian Du,Yaxin Du,Xianghe Pang,Keduan Huang,Yanfeng Wang,Qiang Yan,Siheng Chen*

Main category: cs.AI

TL;DR: MCP-Flow是一个自动化网络代理驱动管道，用于大规模服务器发现、数据合成和模型训练，显著提升了LLM在MCP生态系统中的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有MCP研究覆盖服务器少、依赖昂贵人工整理且缺乏训练支持，限制了LLM代理在真实世界MCP环境中的部署进展。

Method: 采用自动化网络代理驱动管道，从1166个服务器和11536个工具中收集筛选数据，生成68733个高质量指令-函数调用对和6439条轨迹。

Result: 实验证明MCP-Flow在工具选择、函数调用生成和代理任务性能方面表现优异，远超先前工作的规模和多样性。

Conclusion: MCP-Flow为提升LLM代理在真实世界MCP环境中的熟练度提供了可扩展的基础。

Abstract: Large Language Models (LLMs) increasingly rely on external tools to perform
complex, realistic tasks, yet their ability to utilize the rapidly expanding
Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP
research covers few servers, depends on costly manual curation, and lacks
training support, hindering progress toward real-world deployment. To overcome
these limitations, we introduce MCP-Flow, an automated web-agent-driven
pipeline for large-scale server discovery, data synthesis, and model training.
MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing
68733 high-quality instruction-function call pairs and 6439 trajectories, far
exceeding prior work in scale and diversity. Extensive experiments demonstrate
MCP-Flow's effectiveness in driving superior MCP tool selection, function-call
generation, and enhanced agentic task performance. MCP-Flow thus provides a
scalable foundation for advancing LLM agents' proficiency in real-world MCP
environments. MCP-Flow is publicly available at
\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.

</details>


### [48] [Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank](https://arxiv.org/abs/2510.24299)
*Jiayu Liu,Wei Dai,Zhenya Huang,Ning Miao,Enhong Chen*

Main category: cs.AI

TL;DR: 提出了一种基于LLM内部行为的相关矩阵秩来评估推理路径可信度的Self-Indicator方法，无需外部资源即可有效检测推理错误


<details>
  <summary>Details</summary>
Motivation: 现有检查方法依赖外部资源（如训练验证器或复杂提示），计算开销大且仅适用于特定领域，需要一种更高效通用的LLM输出验证方法

Method: 通过计算输入问题与输出推理路径之间的相关矩阵秩作为推理正确性的指标，设计Self-Indicator方法对候选推理路径进行重加权

Result: 在多个不同规模和家族的LLM上实验，该方法能区分正确与错误推理路径的准确率超过75%，在三个推理基准上的准确率提升超过8%

Conclusion: LLM的内部行为已经隐含了其推理路径的可信度信息，Self-Indicator方法简单有效且计算开销低

Abstract: Despite the strong reasoning ability of large language models~(LLMs), they
are prone to errors and hallucinations. As a result, how to check their outputs
effectively and efficiently has become a critical problem in their
applications. Existing checking methods heavily rely on external resources,
such as trained verifiers (e.g., process/outcome reward models) or elaborate
prompts, which lead to high computational overhead and are only applicable to
specific domains. In this paper, we investigate whether the internal behaviors
of LLMs have already implied the credibility of their reasoning paths.
Specifically, we find that the rank of the correlation matrix between the input
problem and the output reasoning path is a robust indicator of reasoning
correctness. Different from other correctness indicators for LLMs, the
calculation of the correlation matrix only relies on the LLM itself, which
avoids the hassle of training a separate model or designing complicated
prompts. Based on it, we design a simple, plug-and-play Self-Indicator method
to reweight candidate reasoning paths, which achieves significant performance
improvements than other voting and verification methods with very few
computational overhead. Our experiments across multiple LLMs of varying scales
and model families have further shown the effectiveness of Self-Indicator. It
achieves over 75% accuracy in distinguishing correct reasoning paths from
incorrect ones, and, in turn, improves the accuracies on three reasoning
benchmarks by more than 8%.

</details>


### [49] [VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation](https://arxiv.org/abs/2510.24339)
*Yunxuan Jiang,Silan Hu,Xiaoning Wang,Yuanyuan Zhang,Xiangyu Chang*

Main category: cs.AI

TL;DR: VDSAgents是一个基于可预测性-可计算性-稳定性(PCS)原则的多智能体系统，用于提升LLM驱动数据科学系统的可信度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的数据科学系统仅依赖模型内部推理，缺乏科学和理论原则指导，在处理复杂现实数据集时可信度和鲁棒性不足。

Method: 基于PCS原则构建多智能体系统，采用模块化工作流处理数据清洗、特征工程、建模和评估，每个阶段由专门智能体负责，结合扰动分析、单元测试和模型验证。

Result: 在9个不同特征的数据集上评估，使用DeepSeek-V3和GPT-4o作为后端，VDSAgents持续优于AutoKaggle和DataInterpreter等最先进的端到端数据科学系统。

Conclusion: 将PCS原则嵌入LLM驱动的数据科学自动化是可行的，能显著提升系统性能。

Abstract: Large language models (LLMs) become increasingly integrated into data science
workflows for automated system design. However, these LLM-driven data science
systems rely solely on the internal reasoning of LLMs, lacking guidance from
scientific and theoretical principles. This limits their trustworthiness and
robustness, especially when dealing with noisy and complex real-world datasets.
This paper provides VDSAgents, a multi-agent system grounded in the
Predictability-Computability-Stability (PCS) principles proposed in the
Veridical Data Science (VDS) framework. Guided by PCS principles, the system
implements a modular workflow for data cleaning, feature engineering, modeling,
and evaluation. Each phase is handled by an elegant agent, incorporating
perturbation analysis, unit testing, and model validation to ensure both
functionality and scientific auditability. We evaluate VDSAgents on nine
datasets with diverse characteristics, comparing it with state-of-the-art
end-to-end data science systems, such as AutoKaggle and DataInterpreter, using
DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the
results of AutoKaggle and DataInterpreter, which validates the feasibility of
embedding PCS principles into LLM-driven data science automation.

</details>


### [50] [Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents](https://arxiv.org/abs/2510.24383)
*Juraj Mavračić*

Main category: cs.AI

TL;DR: Policy Cards是一种机器可读的部署层标准，用于表达AI代理的操作、监管和伦理约束，使其能够在运行时遵循要求。


<details>
  <summary>Details</summary>
Motivation: 为AI代理提供可验证的合规性机制，将高级治理与工程实践相结合，实现大规模的可问责自主性。

Method: 定义规范层编码允许/拒绝规则、义务、证据要求和与NIST AI RMF、ISO/IEC 42001、欧盟AI法案等保证框架的映射关系。

Result: 每个Policy Card可以自动验证、版本控制，并与运行时执行或持续审计管道链接。

Conclusion: Policy Cards为自主代理提供可验证的合规性，形成多代理生态系统中分布式保证的基础。

Abstract: Policy Cards are introduced as a machine-readable, deployment-layer standard
for expressing operational, regulatory, and ethical constraints for AI agents.
The Policy Card sits with the agent and enables it to follow required
constraints at runtime. It tells the agent what it must and must not do. As
such, it becomes an integral part of the deployed agent. Policy Cards extend
existing transparency artifacts such as Model, Data, and System Cards by
defining a normative layer that encodes allow/deny rules, obligations,
evidentiary requirements, and crosswalk mappings to assurance frameworks
including NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can
be validated automatically, version-controlled, and linked to runtime
enforcement or continuous-audit pipelines. The framework enables verifiable
compliance for autonomous agents, forming a foundation for distributed
assurance in multi-agent ecosystems. Policy Cards provide a practical mechanism
for integrating high-level governance with hands-on engineering practice and
enabling accountable autonomy at scale.

</details>


### [51] [APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training](https://arxiv.org/abs/2510.24397)
*Jiarui Qin,Yunjia Xi,Junjie Huang,Renting Rui,Di Yin,Weiwen Liu,Yong Yu,Weinan Zhang,Xing Sun*

Main category: cs.AI

TL;DR: APTBench 是一个新的预训练基准框架，将真实世界智能体任务转换为适合基础模型的多选或文本补全问题，用于评估模型在预训练阶段的智能体潜力。


<details>
  <summary>Details</summary>
Motivation: 当前预训练基准主要关注孤立静态技能，无法反映模型的智能体能力；而智能体基准通常针对后训练模型，基础模型难以支持多轮任务执行。需要能在预训练阶段评估智能体潜力的基准。

Method: 将真实世界智能体任务和成功轨迹转换为多选或文本补全问题，聚焦规划和行动等核心智能体能力，覆盖软件工程和深度研究等关键场景。

Result: 相比现有通用基准，APTBench 能更准确地预测模型在下游作为智能体的表现，同时比后训练的全规模端到端评估更轻量、成本效益更高。

Conclusion: APTBench 填补了预训练阶段智能体能力评估的空白，为模型训练提供更有效的指导。

Abstract: With the rapid development of LLM-based agents, there is a growing trend to
incorporate agent-specific data into the pre-training stage of LLMs, aiming to
better align LLMs with real-world autonomous task execution. However, current
pre-training benchmarks primarily focus on isolated and static skills, e.g.,
common knowledge or mathematical/code reasoning, and fail to reflect model's
agentic capabilities. On the other hand, agent benchmarks are typically
designed for post-trained models, requiring multi-turn task execution abilities
that base models struggle to support. Thus, there is a compelling need for a
benchmark that can evaluate agentic potentials during pre-training and guide
the model training more effectively. To address this gap, we propose APTBench,
a framework that converts real-world agent tasks and successful trajectories
into multiple-choice or text completion questions tailored for base models. It
focuses on core agentic abilities, e.g., planning and action, and covers key
agent scenarios, software engineering and deep research. Compared to existing
general-purpose benchmarks, APTBench offers a more predictive signal of a
model's downstream performance as an agent, while remaining significantly more
lightweight and cost-effective than full-scale, end-to-end agent evaluations
after post-training.

</details>


### [52] [OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows](https://arxiv.org/abs/2510.24411)
*Qiushi Sun,Mukai Li,Zhoumianze Liu,Zhihui Xie,Fangzhi Xu,Zhangyue Yin,Kanzhi Cheng,Zehao Li,Zichen Ding,Qi Liu,Zhiyong Wu,Zhuosheng Zhang,Ben Kao,Lingpeng Kong*

Main category: cs.AI

TL;DR: 提出了MobileRisk-Live动态沙盒环境和安全检测基准，以及OS-Sentinel混合安全检测框架，用于检测移动代理的安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着基于视觉语言模型的计算机使用代理在移动平台等数字环境中展现出类人能力，其潜在的安全风险（如系统破坏和隐私泄露）引发了重大担忧。在移动环境广阔复杂的操作空间中检测这些安全问题是一个严峻挑战。

Method: 引入MobileRisk-Live动态沙盒环境和安全检测基准，包含带有细粒度标注的真实轨迹。提出OS-Sentinel混合安全检测框架，结合形式验证器检测显式系统级违规和基于VLM的上下文判断器评估上下文风险和代理行为。

Result: 实验表明OS-Sentinel在多个指标上比现有方法提高了10%-30%。

Conclusion: 该研究为移动代理安全研究奠定了基础，提供了促进开发更安全可靠自主移动代理的关键见解。

Abstract: Computer-using agents powered by Vision-Language Models (VLMs) have
demonstrated human-like capabilities in operating digital environments like
mobile platforms. While these agents hold great promise for advancing digital
automation, their potential for unsafe operations, such as system compromise
and privacy leakage, is raising significant concerns. Detecting these safety
concerns across the vast and complex operational space of mobile environments
presents a formidable challenge that remains critically underexplored. To
establish a foundation for mobile agent safety research, we introduce
MobileRisk-Live, a dynamic sandbox environment accompanied by a safety
detection benchmark comprising realistic trajectories with fine-grained
annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety
detection framework that synergistically combines a Formal Verifier for
detecting explicit system-level violations with a VLM-based Contextual Judge
for assessing contextual risks and agent actions. Experiments show that
OS-Sentinel achieves 10%-30% improvements over existing approaches across
multiple metrics. Further analysis provides critical insights that foster the
development of safer and more reliable autonomous mobile agents.

</details>


### [53] [Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning](https://arxiv.org/abs/2510.24435)
*Benjamin Grando Moreira*

Main category: cs.AI

TL;DR: 比较多个大型语言模型在逻辑和抽象推理能力上的表现，并与人类基准进行对比


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的推理能力对于人工智能发展很重要，需要了解这些模型是否真正理解信息、进行推理并以逻辑有效的方式得出结论

Method: 使用8个定制设计的推理问题测试GPT、Claude、DeepSeek、Gemini、Grok、Llama、Mistral、Perplexity和Sabi'a等LLM，并将结果与人类表现进行基准对比

Result: 揭示了显著差异，表明LLM在演绎推理方面存在困难

Conclusion: LLM在逻辑推理能力上仍有不足，需要进一步改进

Abstract: Evaluating reasoning ability in Large Language Models (LLMs) is important for
advancing artificial intelligence, as it transcends mere linguistic task
performance. It involves understanding whether these models truly understand
information, perform inferences, and are able to draw conclusions in a logical
and valid way. This study compare logical and abstract reasoning skills of
several LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,
Perplexity, and Sabi\'a - using a set of eight custom-designed reasoning
questions. The LLM results are benchmarked against human performance on the
same tasks, revealing significant differences and indicating areas where LLMs
struggle with deduction.

</details>


### [54] [Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks](https://arxiv.org/abs/2510.24461)
*Korneel Van den Berghe,Stein Stroobants,Vijay Janapa Reddi,G. C. H. E. de Croon*

Main category: cs.AI

TL;DR: 本文提出了一种改进脉冲神经网络在强化学习中训练的方法，通过分析替代梯度斜率设置和引入特权指导策略，显著提升了SNN在机器人控制任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络在能量受限的机器人应用中具有巨大潜力，但其训练面临两个关键挑战：脉冲神经元的不可微分性和状态动态需要序列训练，这在强化学习中受到早期训练序列长度限制的影响。

Method: 系统分析替代梯度斜率设置，发现较浅斜率能增加深层梯度幅度但降低与真实梯度的对齐；提出使用特权指导策略来引导学习过程，同时利用在线环境交互；结合自适应斜率调度方法。

Result: 在强化学习设置中，较浅斜率或调度斜率使训练和最终部署性能提升2.1倍；在真实世界无人机位置控制任务中，平均回报达到400分，显著优于行为克隆和TD3BC等现有技术（最多-200分）。

Conclusion: 这项工作推进了对SNN中替代梯度学习的理论理解，并为神经形态控制器在实际机器人系统中的训练方法提供了实用方案。

Abstract: Neuromorphic computing systems are set to revolutionize energy-constrained
robotics by achieving orders-of-magnitude efficiency gains, while enabling
native temporal processing. Spiking Neural Networks (SNNs) represent a
promising algorithmic approach for these systems, yet their application to
complex control tasks faces two critical challenges: (1) the non-differentiable
nature of spiking neurons necessitates surrogate gradients with unclear
optimization properties, and (2) the stateful dynamics of SNNs require training
on sequences, which in reinforcement learning (RL) is hindered by limited
sequence lengths during early training, preventing the network from bridging
its warm-up period.
  We address these challenges by systematically analyzing surrogate gradient
slope settings, showing that shallower slopes increase gradient magnitude in
deeper layers but reduce alignment with true gradients. In supervised learning,
we find no clear preference for fixed or scheduled slopes. The effect is much
more pronounced in RL settings, where shallower slopes or scheduled slopes lead
to a 2.1x improvement in both training and final deployed performance. Next, we
propose a novel training approach that leverages a privileged guiding policy to
bootstrap the learning process, while still exploiting online environment
interactions with the spiking policy. Combining our method with an adaptive
slope schedule for a real-world drone position control task, we achieve an
average return of 400 points, substantially outperforming prior techniques,
including Behavioral Cloning and TD3BC, which achieve at most --200 points
under the same conditions. This work advances both the theoretical
understanding of surrogate gradient learning in SNNs and practical training
methodologies for neuromorphic controllers demonstrated in real-world robotic
systems.

</details>


### [55] [FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling](https://arxiv.org/abs/2510.24645)
*Zengzhuang Xu,Bingguang Hao,Zechuan Wang,Yuntao Wen,Maolin Wang,Yang Liu,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Chenyi Zhuang,Jinjie Gu,Leilei Gan,Xiangyu Zhao,Shi Gu*

Main category: cs.AI

TL;DR: FunReason-MT是一个用于合成多轮工具调用训练数据的新框架，通过环境-API图交互、高级工具查询合成和引导迭代链来解决现有方法在真实世界环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法（如随机环境采样或多智能体角色扮演）在真实世界环境中无法生成高质量的多轮工具调用训练数据，存在目标模型训练、工具架构隔离和多轮逻辑依赖等实际挑战。

Method: 采用环境-API图交互收集多样化高质量轨迹，高级工具查询合成简化困难查询构建，引导迭代链生成复杂思维链。

Result: 在Berkeley Function-Calling Leaderboard (BFCLv3)上，基于FunReason-MT生成数据构建的4B模型在同等规模模型中达到最先进性能，优于大多数闭源模型。在BFCLv4上的进一步性能提升证实了其可靠性。

Conclusion: FunReason-MT为智能体学习提供了可靠且鲁棒的数据源，能够有效解决多轮工具调用数据的复杂性障碍。

Abstract: Function calling (FC) empowers large language models (LLMs) and autonomous
agents to interface with external tools, a critical capability for solving
complex, real-world problems. As this ability becomes increasingly central to
advanced AI systems, the need for high-quality, multi-turn training data to
develop and refine it cannot be overstated. Existing data synthesis methods,
such as random environment sampling or multi-agent role-playing, are not
powerful enough to generate high-quality data in real-world environments.
Practical challenges come in three folds: targeted model training, isolation of
tool architecture, and multi-turn logical dependency. To address these
structural deficiencies, we present FunReason-MT, a novel data synthesis
framework for real-world multi-turn tool use. FunReason-MT resolves the
complexity barrier in multi-turn FC data by employing 1) Environment-API Graph
Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query
Synthesis to simplify hard query construction, and 3) Guided Iterative Chain
for sophisticated CoT generation. Evaluations on Berkeley Function-Calling
Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built
upon FunReason-MT generated data achieves state-of-the-art performance among
comparable-sized models, outperforming most close-source models. Further
performance improvements on BFCLv4 confirm that FunReason-MT provides a
reliable and robust source for agentic learning.

</details>


### [56] [OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs](https://arxiv.org/abs/2510.24663)
*Yifu Lu,Shengjie Liu,Li Dong*

Main category: cs.AI

TL;DR: OrchDAG是一个合成数据生成管道，将工具执行建模为具有可控复杂度的有向无环图，用于基准测试模型性能并提出基于图的奖励来增强RLVR训练。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多忽视了多轮工具交互的复杂性，需要更好的方法来建模和评估多轮工具使用。

Method: 引入OrchDAG合成数据生成管道，将工具执行建模为有向无环图，提出基于图的奖励机制，并与GRPO风格算法结合进行RLVR训练。

Result: 数据集提供了一个具有挑战性但可解决的基准，提出的奖励机制在与GRPO风格算法结合时表现有效。

Conclusion: 在多轮工具使用中，利用拓扑结构和数据复杂度具有重要意义。

Abstract: Agentic tool use has gained traction with the rise of agentic tool calling,
yet most existing work overlooks the complexity of multi-turn tool
interactions. We introduce OrchDAG, a synthetic data generation pipeline that
models tool execution as directed acyclic graphs (DAGs) with controllable
complexity. Using this dataset, we benchmark model performance and propose a
graph-based reward to enhance RLVR training. Experiments show that the dataset
presents a challenging but solvable benchmark, and the proposed reward is
effective when combined with GRPO-style algorithms, highlighting the importance
of leveraging topological structure and data complexity in multi-turn tool use.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [57] [【AI要闻·日报】<em class="highlight">Code</em> <em class="highlight">Agent</em>·Code LLMs·初创动态（10.29）](http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483783&idx=1&sn=b9888a0af4953dc180bd38c35acf7815&chksm=e917aa049437df7d5e35586798442bfc586b138ccfc4789afd3108560906b2a037895867e37e#rd)
*CodeAgent代码智能*

Main category: wechat.article

TL;DR: 产品特点：服务提供持久记忆层，可用三行代码接入，在会话间存储、遗忘与回忆重要信息；AWS 选择 Mem0 为 Agent SDK 的唯一内存提供商【356931508696392L570-L572】。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 产品特点：服务提供持久记忆层，可用三行代码接入，在会话间存储、遗忘与回忆重要信息；AWS 选择 Mem0 为 Agent SDK 的唯一内存提供商【356931508696392L570-L572】。

</details>


### [58] [告别“救火”！“<em class="highlight">代理</em>式人工智能”（<em class="highlight">Agentic</em> AI）正掀起一场IT运维革命](http://mp.weixin.qq.com/s?__biz=MzkyNjYzNTIwNw==&mid=2247493565&idx=1&sn=9f55d6b98eebaa413e4063c556cb2233&chksm=c343d1c62c0f170017690543bb09b0e6837a48343630d9d370b9e7bf5ad527a19f2bf266592e#rd)
*CIOCDO*

Main category: wechat.article

TL;DR: 主角就是——“代理式人工智能”（Agentic AI）。它与传统自动化（RPA或脚本）的根本区别在于：它不再是“执行”你设定的重复任务，而是能模拟人类决策，在几乎无需监督的情况下，自主推理、规划、并处理复杂任务。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 主角就是——“代理式人工智能”（Agentic AI）。它与传统自动化（RPA或脚本）的根本区别在于：它不再是“执行”你设定的重复任务，而是能模拟人类决策，在几乎无需监督的情况下，自主推理、规划、并处理复杂任务。

</details>


### [59] [一年后的<em class="highlight">Agentic</em> AI：企业从探索到价值兑现的六个关键教训和启示—-麦肯锡报告解读](http://mp.weixin.qq.com/s?__biz=MzA3MDY2MzcwMg==&mid=2651194115&idx=1&sn=433d2fa1dbd68f3c24bf4beb62dac157&chksm=859cef478bdcf7ef1d76d0e964d9f76d15071d77ebe2eb061df4f440ee675d4b9d21d55f991e#rd)
*IT职场斜杠青年*

Main category: wechat.article

TL;DR: 2024年，是“Agentic AI”（具备自主决策与行动能力的智能体）从概念走向落地的一年。一年过去，麦肯锡最新研究发现：多数企业从“实验性部署”进入“价值化应用”阶段，但也暴露出组织、流程、技术整合等深层挑战。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2024年，是“Agentic AI”（具备自主决策与行动能力的智能体）从概念走向落地的一年。一年过去，麦肯锡最新研究发现：多数企业从“实验性部署”进入“价值化应用”阶段，但也暴露出组织、流程、技术整合等深层挑战。

</details>


### [60] [<em class="highlight">智能体</em>人工智能（<em class="highlight">Agentic</em> AI）可能引发责任问题](http://mp.weixin.qq.com/s?__biz=MzYyMzI1NjU4OA==&mid=2247483916&idx=1&sn=f44a3cbfb6aafcc80dc0218900724eb9&chksm=fef57fad84d4a1c6506a66346b5707ead8311c43b9cdb0c0db132867d1e519348400711689ad#rd)
*虚舟札记*

Main category: wechat.article

TL;DR: 支付公司正设法解决 AI 智能体（AI agents）为消费者购物时出现失误的相关问题。银行和支付公司在为智能体商务（agentic commerce）做准备时，不得不解决失误与责任归属问题。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 支付公司正设法解决 AI 智能体（AI agents）为消费者购物时出现失误的相关问题。银行和支付公司在为智能体商务（agentic commerce）做准备时，不得不解决失误与责任归属问题。

</details>


### [61] [容器服务 - <em class="highlight">Agentic</em> AI 时代云原生底座](http://mp.weixin.qq.com/s?__biz=MzkyMzYzODM4MA==&mid=2247502747&idx=1&sn=622c85d6c28610b1943cee86f083edec&chksm=c009957c00011f8264ccc40adce1c5d5fa5e6daefab95a189268dc14345b57aaa8aa1e1260ad#rd)
*阿里云基础设施*

Main category: wechat.article

TL;DR: 迎接 agentic ai应用变革 40% 新负载 95% 新平台 54% 新算力。genai。 laas市场在未来五年将以54.0% 的复合增长率高速增长，预计2025全年 推理算力支出接近训练算力支出 -idc


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 迎接 agentic ai应用变革 40% 新负载 95% 新平台 54% 新算力。genai。 laas市场在未来五年将以54.0% 的复合增长率高速增长，预计2025全年 推理算力支出接近训练算力支出 -idc

</details>


### [62] [斯坦福论文落地：开源版 <em class="highlight">Agentic</em> Context Engineering 来了](http://mp.weixin.qq.com/s?__biz=MzUyOTgzODIxOQ==&mid=2247483719&idx=1&sn=c39ab095fd90032e5b8abd8bd8947e58&chksm=fb87c74f330121669239ce7d1f25e58d3d1b9f9931b6d08191207a08c2fab39209f2af5b4708#rd)
*玩开源*

Main category: wechat.article

TL;DR: 斯坦福大学去年提出的「Agentic Context Engineering」框架（论文链接），现在有了一个轻量级实现版本，而且集成起来比想象中简单得多。kayba智能体的'记忆进化器'


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 斯坦福大学去年提出的「Agentic Context Engineering」框架（论文链接），现在有了一个轻量级实现版本，而且集成起来比想象中简单得多。kayba智能体的'记忆进化器'

</details>


### [63] [HCL BigFix AEX v12 震撼发布：企业级 <em class="highlight">Agentic</em> AI 平台](http://mp.weixin.qq.com/s?__biz=MzU2MzYzNzE1OQ==&mid=2247486117&idx=1&sn=0a48680c43b0e729781dfb010fe8497a&chksm=fdfe21abc2e056d1be16fd61024d013f1842cb0069607709b134163aae19422878d09b639bca#rd)
*HCL软件*

Main category: wechat.article

TL;DR: Agentic 核心能力AI Agent Builder（零代码/低代码拖拽）→ 快速构建自主代理统一工具目录 → 50+ 预集成连接器，可扩展 IT/HR/财务等任意系统多代理协同 → 复杂流程（入职、审批、事件响应）由多个代理分工完成


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic 核心能力AI Agent Builder（零代码/低代码拖拽）→ 快速构建自主代理统一工具目录 → 50+ 预集成连接器，可扩展 IT/HR/财务等任意系统多代理协同 → 复杂流程（入职、审批、事件响应）由多个代理分工完成

</details>


### [64] [无问芯穹夏立雪：从视Agent为工具，到视Agent为协作者，<em class="highlight">Agentic</em> AI是一场基础设施必须跟上的系统性革命](http://mp.weixin.qq.com/s?__biz=MzkyNjYwMTAxNg==&mid=2247501067&idx=1&sn=b4792cccea38c8eb5a6af5357b4e012f&chksm=c3c2c0e0c344d5a5ba5405c4b12ec64f4d38cda6b32bea7e30b450918d31ce1e7c08578850d3#rd)
*无问芯穹*

Main category: wechat.article

TL;DR: 迈向 Agentic Infra 阶段，在让智能体深度参与协作的尝试中，无问芯穹搭建了基础设施智能体蜂群体系，把传统分散在开发、运维、运营团队的割裂流程，统一在一个智能化系统中，让算力价值更加自动化地传递给平台终端用户。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 迈向 Agentic Infra 阶段，在让智能体深度参与协作的尝试中，无问芯穹搭建了基础设施智能体蜂群体系，把传统分散在开发、运维、运营团队的割裂流程，统一在一个智能化系统中，让算力价值更加自动化地传递给平台终端用户。

</details>


### [65] [欢迎来到 AI 自主购物时代：一文读懂<em class="highlight">智能体</em>电商 (<em class="highlight">Agentic</em> Commerce)](http://mp.weixin.qq.com/s?__biz=MzA5ODI5NDYxNA==&mid=2649772745&idx=1&sn=8a6dd4b7d543742fa6fde4a5d73af4c1&chksm=892a095c3b748ad344f32c3ac2f358dcc559f03a735b7591b680bd02d23083c7cd2429c41be8#rd)
*雨杨网志*

Main category: wechat.article

TL;DR: 它叫“智能体电商”（Agentic Commerce），而且已经开始发生了。什么是智能体电商？—— 它不是更聪明的聊天机器人首先要清楚：智能体电商和你现在用的购物工具有点不一样。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 它叫“智能体电商”（Agentic Commerce），而且已经开始发生了。什么是智能体电商？—— 它不是更聪明的聊天机器人首先要清楚：智能体电商和你现在用的购物工具有点不一样。

</details>


### [66] [<em class="highlight">Agentic</em> AI与自主性：2025年通信运营商战略报告](http://mp.weixin.qq.com/s?__biz=MzE5OTEyNzE0NQ==&mid=2247485757&idx=1&sn=22062ef48eeb76df6ce84d2ffa08b750&chksm=97424e26355ddf54d66c72d509ad093aceb24323823a2b3af748817f93f3a5c63b4d71dcdbee#rd)
*AI Daily Papers*

Main category: wechat.article

TL;DR: 报告原文地址：https：//inform.tmforum.org/research-and-analysis/reports/agentic-ai-and-autonomy-csps-set-out-their-strategies报告概述本报告由TM Forum于2025年9月发布，标题为“Agentic AI and autonomy： CSPs set out their strategi...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 报告原文地址：https：//inform.tmforum.org/research-and-analysis/reports/agentic-ai-and-autonomy-csps-set-out-their-strategies报告概述本报告由TM Forum于2025年9月发布，标题为“Agentic AI and autonomy： CSPs set out their strategies”，旨在深入分析全球通信服务

</details>


### [67] [<em class="highlight">Agentic</em> AI 学习笔记](http://mp.weixin.qq.com/s?__biz=MzU5ODU3NjY5OA==&mid=2247484886&idx=1&sn=9df6cab2db12dcf335946180ce4b9138&chksm=ffb5ac2ba418e3046a7bcac3f276d839a842cb53bb5051d06ccc5295c1570ff3500ad0a5906c#rd)
*ByteLink*

Main category: wechat.article

TL;DR: 这个课程使用的词是 Agentic，更中性的形容词，而不是确定性的名词 agent，用来表示不同程度的 Agent，有一些的自主性比较低，另一些的自主性较高。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这个课程使用的词是 Agentic，更中性的形容词，而不是确定性的名词 agent，用来表示不同程度的 Agent，有一些的自主性比较低，另一些的自主性较高。

</details>


### [68] [构建具备深度思考能力的 <em class="highlight">Agentic</em> RAG 流水线，用于解决复杂查询](http://mp.weixin.qq.com/s?__biz=MzI2ODUyMTQyNA==&mid=2247499438&idx=1&sn=1efc7aa410b8092f57ad5212773f4471&chksm=eba00b5ef2c8147f5a6d7f4df6972106ea723118062fda8509a77d93b57b26ed59fea1dabce6#rd)
*PyTorch研习社*

Main category: wechat.article

TL;DR: 在一个 agentic 系统里，工作流复杂且循环，tracing 并非可有可无，而是很重要。它帮助你可视化内部过程，更容易调试 agent 的思考路径。知识库来源一个生产级 RAG 系统需要既复杂又有挑战性的知识库，才能真正体现其有效性。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在一个 agentic 系统里，工作流复杂且循环，tracing 并非可有可无，而是很重要。它帮助你可视化内部过程，更容易调试 agent 的思考路径。知识库来源一个生产级 RAG 系统需要既复杂又有挑战性的知识库，才能真正体现其有效性。

</details>


### [69] [企业级<em class="highlight">大模型</em>私有化部署探讨](http://mp.weixin.qq.com/s?__biz=MzkwNDQxNTE0OA==&mid=2247491330&idx=1&sn=e6b8f1595a1f155b8d92d5c432aea9c2&chksm=c10ad5185f3e3357f3c88726e040950abc7caa9cee05a3a42b94b6b305831fd2a34e99baf408#rd)
*Edison朋友圈*

Main category: wechat.article

TL;DR: 本文讨论企业级AI 大模型私有化部署方案及AI Agent的开发设计流程。本文由Eddie以及合创社区的学员提供整体思路，概述流程如下：立项 → 需求 → 方案 → 硬件与数据 → 开发 → 测试 → 部署 → 运营


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本文讨论企业级AI 大模型私有化部署方案及AI Agent的开发设计流程。本文由Eddie以及合创社区的学员提供整体思路，概述流程如下：立项 → 需求 → 方案 → 硬件与数据 → 开发 → 测试 → 部署 → 运营

</details>


### [70] [智能体综述：探索基于大型语言<em class="highlight">模型</em>的智能体：定义、方法与前景](http://mp.weixin.qq.com/s?__biz=MzI0NDcxNTcxNA==&mid=2247499259&idx=2&sn=398d4d6e36a24a0aeae4e31bba2c8ba9&chksm=e872895ebca96e4ac54b52cd53d6fff2d7ca81aa9123105f7469cac1b8a507c67c67f187252e#rd)
*数融咖啡*

Main category: wechat.article

TL;DR: 鲁棒性强，适应动态环境 难达全局最优，通信开销大 通信机制与效率提升 信息交换方式：无通信（仅依赖本地信息）、有通信（消息传递）、共享内存（中央知识库如MetaGPT的全局内存池、共享参数）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 鲁棒性强，适应动态环境 难达全局最优，通信开销大 通信机制与效率提升 信息交换方式：无通信（仅依赖本地信息）、有通信（消息传递）、共享内存（中央知识库如MetaGPT的全局内存池、共享参数）。

</details>


### [71] [科技日报：轻量化+多智能体，神农<em class="highlight">大模型</em>让农业AI好用普惠](http://mp.weixin.qq.com/s?__biz=MzU2MzEwNzQ3MA==&mid=2247498236&idx=1&sn=d23c85cf4300270852466f11043662b1&chksm=fd5e95eb4cfc5b3f0a3647a141448fb2a6cc737f87794e0daef50a3bc1865c47f1c2b82a2c4d#rd)
*丰农信息*

Main category: wechat.article

TL;DR: eSIM商用落地，手机进入「无卡时代」？2025 东湖论坛在武汉举行 口。个省区市同步开通eSIM手机业务。在国内即将正式从物联网，智能穿戴领域向手机端延伸，科技日报武汉10月14日电（记 位院士为2025年度全国青少年创·造 楠）1


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: eSIM商用落地，手机进入「无卡时代」？2025 东湖论坛在武汉举行 口。个省区市同步开通eSIM手机业务。在国内即将正式从物联网，智能穿戴领域向手机端延伸，科技日报武汉10月14日电（记 位院士为2025年度全国青少年创·造 楠）1

</details>


### [72] [面向<em class="highlight">大模型</em>应用场景的交互能力评测](http://mp.weixin.qq.com/s?__biz=MzkxNjI2MjQ3Nw==&mid=2247494219&idx=1&sn=44c1520e4401d5458ea15c97bd2d8fe0&chksm=c0c59dcd1968a83389d699fcc6e0e607bc0f852a8a4c5d2b0e2e476e22cc0151f12ff22651f2#rd)
*BanTech智库*

Main category: wechat.article

TL;DR: 大模型应用系统以自然语言交互为核心，用户输入不再受限于预设的固定路径或格式。（2）输入表达多样性。对于同一用户意图，存在成千上万种不同的表达方式。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型应用系统以自然语言交互为核心，用户输入不再受限于预设的固定路径或格式。（2）输入表达多样性。对于同一用户意图，存在成千上万种不同的表达方式。

</details>


### [73] [业界荣誉 | 力维智联入选中国信通院《<em class="highlight">大模型</em>一体机产业图谱》，助力通用<em class="highlight">大模型</em>落地](http://mp.weixin.qq.com/s?__biz=MzA5NzA2MjUxMA==&mid=2651524629&idx=1&sn=2220631b4215d5b5a76476de17741da8&chksm=8ac1ea108661f88695d653cdd2b4b1e7a53930e8ba4933cf63e453d3ca2c79c96eaf9bc8016e#rd)
*力维智联*

Main category: wechat.article

TL;DR: 近日，由中国人工智能产业发展联盟与中国信息通信研究院共同编制的《大模型一体机产业图谱》正式发布。力维智联凭借其自研的Sentosa_USL统一语义层平台与DataAgent数据分析智能体，分别入选产业图谱中的“模型供应商”与“


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 近日，由中国人工智能产业发展联盟与中国信息通信研究院共同编制的《大模型一体机产业图谱》正式发布。力维智联凭借其自研的Sentosa_USL统一语义层平台与DataAgent数据分析智能体，分别入选产业图谱中的“模型供应商”与“

</details>


### [74] [2025年<em class="highlight">大模型</em>智能体在低碳电力系统中的应用探索报告-香港中文大学（赵俊华）](http://mp.weixin.qq.com/s?__biz=MzIzMzkzODgxNw==&mid=2247514326&idx=3&sn=2c5991d0fead98eec3e0633bdb186239&chksm=e9ac25cbd72fc4646759f9037b0b21ac1bbe6b16a5bdb03c26f5edb424a5526f57eb3e35d3e2#rd)
*开源报告*

Main category: wechat.article

TL;DR: 《大模型智能体在低碳电力系统中的应用探索报告》由香港中文大学（深圳）赵俊华团队发布，系统阐述了大模型智能体的核心能力、在电力系统的应用场景与实践成果，为低碳电力系统的智能化升级提供了全面指引。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 《大模型智能体在低碳电力系统中的应用探索报告》由香港中文大学（深圳）赵俊华团队发布，系统阐述了大模型智能体的核心能力、在电力系统的应用场景与实践成果，为低碳电力系统的智能化升级提供了全面指引。

</details>


### [75] [AI<em class="highlight">大模型</em>应用开发入门全攻略，看懂这一篇就够了！](http://mp.weixin.qq.com/s?__biz=MzkxNDI1NjI4NA==&mid=2247485094&idx=1&sn=6cacf3141e955b4f3282964f9b811a35&chksm=c0b2d94c0b3853949fbd7169c3f75c083d7225424e759c3b539bb6cf8c6b0bf768fa7c54e4ac#rd)
*轻松学AI大模型*

Main category: wechat.article

TL;DR: 型应用开发，带领大家入门，带领大家了解AI大模型应用开发的全攻略。尽管市面上的大语言模型（LLMs）种类繁多，但大家在使用时其实都是通过API来与大模型交互的。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 型应用开发，带领大家入门，带领大家了解AI大模型应用开发的全攻略。尽管市面上的大语言模型（LLMs）种类繁多，但大家在使用时其实都是通过API来与大模型交互的。

</details>


### [76] [学术速递丨范举教授团队研发的面向数据科学的 Agentic <em class="highlight">大模型</em>——DeepAnalyze正式发布](http://mp.weixin.qq.com/s?__biz=Mzk0NzE2OTU4Nw==&mid=2247516240&idx=3&sn=a508d4aa0e115ec63a96d260eb88c58e&chksm=c2c637905e17ad499954021aaebf08a9933273e327459ba6d2614b80ff28ffdcf98120ac3ace#rd)
*中国人民大学信息学院*

Main category: wechat.article

TL;DR: 在真实环境中以“从单一能力到复合能力”的方式对大模型进行渐进式训练，逐步提升大模型各项能力。此外，团队还提出了面向数据的轨迹合成框架，自动化构建超过50万条数据科学推理与环境交互数据，在庞大的搜索空间中


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在真实环境中以“从单一能力到复合能力”的方式对大模型进行渐进式训练，逐步提升大模型各项能力。此外，团队还提出了面向数据的轨迹合成框架，自动化构建超过50万条数据科学推理与环境交互数据，在庞大的搜索空间中

</details>


### [77] [企业级<em class="highlight">大模型</em> AI 应用构建指南](http://mp.weixin.qq.com/s?__biz=MzIzODIzNzE0NQ==&mid=2654455972&idx=1&sn=a23867284d1eec14c85482b0fff9d55b&chksm=f306b15451a5cd317aafe7e75167bcecd225ad0cc4569b93c9fc21b8e31470da52ca6491c5fb#rd)
*玄姐聊AGI*

Main category: wechat.article

TL;DR: 随着大语言模型（LLM）的飞速发展，模型参数规模与多模态能力持续突破，AI 应用早已告别简单对话问答的初级阶段，迈入融合 RAG、工作流、Agent 的复杂研发范式。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 随着大语言模型（LLM）的飞速发展，模型参数规模与多模态能力持续突破，AI 应用早已告别简单对话问答的初级阶段，迈入融合 RAG、工作流、Agent 的复杂研发范式。

</details>


### [78] [彻底搞懂了！什么是<em class="highlight">大模型</em>微调？<em class="highlight">大模型</em>和微调的关系？](http://mp.weixin.qq.com/s?__biz=Mzk4ODgxOTExNA==&mid=2247485788&idx=1&sn=f3356ce1ef07bd2b4fb6951f13ec2762&chksm=c4614d3197f08ca8e0ad63bcbcd726a7fb3ca42f8b87ea173d3faa8d60b6e47a1efa065e6fd3#rd)
*觉醒AI新纪元*

Main category: wechat.article

TL;DR: 什么是微调？大模型微调 每天拆解一个ai知识。如果你是一个开发者，手里有一个强大的语言模 型（llm），想用它来做点厉害的事情，比如文 本分类、智能问答，或者识别文本里的关键信 息。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 什么是微调？大模型微调 每天拆解一个ai知识。如果你是一个开发者，手里有一个强大的语言模 型（llm），想用它来做点厉害的事情，比如文 本分类、智能问答，或者识别文本里的关键信 息。

</details>


### [79] [两大赛场同时出击 中国<em class="highlight">大模型</em>让全球投资人折服！](http://mp.weixin.qq.com/s?__biz=MzI2MzA4MzYzMQ==&mid=2650775338&idx=1&sn=f34f3dcd917d07da8a3a14e44fab1cc9&chksm=f3d938a4d74401120d81644a6328fc46cf347be39cb9d3b503dfd2eabaca0019582de9b80afc#rd)
*行者吴江*

Main category: wechat.article

TL;DR: 中国率先出台《金融大模型应用指引》，要求模型决策可解释性达85%以上。这种监管倒逼机制促使DeepSeek开发出"决策溯源"功能，每笔交易可生成包含12个因子的分析报告。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 中国率先出台《金融大模型应用指引》，要求模型决策可解释性达85%以上。这种监管倒逼机制促使DeepSeek开发出"决策溯源"功能，每笔交易可生成包含12个因子的分析报告。

</details>


### [80] [从“敲代码”到“说人话”：<em class="highlight">大模型</em>与Text-to-SQL重塑公安数据侦查分析思路探索](http://mp.weixin.qq.com/s?__biz=MzIxNTEyODQwMw==&mid=2247489243&idx=1&sn=dcf7658921bd188964225c28dd4db68a&chksm=964edabdc71e1a55648997485d57a0a50dec994547979a1880103de6f98a2d8da90f97bf55bb#rd)
*雪峰大数据*

Main category: wechat.article

TL;DR: 大模型（llm）的加入，让它“活”了——靠的是三大关键能力。1. 听得懂“黑话”：强大的意图理解（nlu）DeepSeek、Qwen等大模型经过海量语料训练，具备强大的语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型（llm）的加入，让它“活”了——靠的是三大关键能力。1. 听得懂“黑话”：强大的意图理解（nlu）DeepSeek、Qwen等大模型经过海量语料训练，具备强大的语义理解能力。

</details>


### [81] [2025必收藏！主流 AI <em class="highlight">大模型</em>全解析](http://mp.weixin.qq.com/s?__biz=Mzk2NDY3Mzk3MA==&mid=2247484403&idx=1&sn=60fb54ed8f037599dea430471b0ca94c&chksm=c5ca5ae889355026e1541045510ab9c4a4c6d2e033d4a35c77114570ca309e78351128f115a5#rd)
*数智筑城*

Main category: wechat.article

TL;DR: 2.采用的大规模强化学习技术，仅需少量标注数据即可显著提升模型性能。模型完全开源，适配不同算力需求，进一步降低了AI应用门槛，赋能开源社区发展。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2.采用的大规模强化学习技术，仅需少量标注数据即可显著提升模型性能。模型完全开源，适配不同算力需求，进一步降低了AI应用门槛，赋能开源社区发展。

</details>


### [82] [OpenAI前CTO Mira Murati团队又放大招，让<em class="highlight">大模型</em>训练成本暴降10倍](http://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247712859&idx=4&sn=05b7b46d1b06ab2bbed167a7644af786&chksm=ed0a03b2f2e33d4499e88ff6b4315db03e9079fd8ea9c122a32d9d819300a46ad632dc5dcae1#rd)
*极市平台*

Main category: wechat.article

TL;DR: 一种能以 1/10 成本达到强化学习同等效果的大模型后训练新方法。mira murati @miramurati · 14h combining the benefits of rl and sft with on-policy distillation， a promising approach for training small models for domain performance a...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一种能以 1/10 成本达到强化学习同等效果的大模型后训练新方法。mira murati @miramurati · 14h combining the benefits of rl and sft with on-policy distillation， a promising approach for training small models for domain performance and continual learning.。

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [83] [VisCoder2: Building Multi-Language Visualization Coding Agents](https://arxiv.org/abs/2510.23642)
*Yuansheng Ni,Songcheng Cai,Xiangchao Chen,Jiarong Liang,Zhiheng Lyu,Jiaqi Deng,Kai Zou,Ping Nie,Fei Yuan,Xiang Yue,Wenhu Chen*

Main category: cs.SE

TL;DR: 该论文提出了VisCode-Multi-679K数据集、VisPlotBench基准和VisCoder2模型，用于改进可视化编码代理的多语言支持和迭代调试能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的编码代理在实际工作流中存在语言覆盖有限、执行不可靠和缺乏迭代修正机制的问题，现有数据集和基准过于狭窄。

Method: 构建了包含679K验证样本的多语言可视化数据集VisCode-Multi-679K，开发了VisPlotBench评估基准，并训练了VisCoder2系列多语言可视化模型。

Result: VisCoder2显著优于开源基线模型，接近GPT-4.1性能，通过迭代自调试在32B规模达到82.4%执行通过率，在符号或编译器依赖语言中表现突出。

Conclusion: 提出的资源和模型有效解决了可视化编码代理的关键挑战，为多语言可视化生成和调试提供了坚实基础。

Abstract: Large language models (LLMs) have recently enabled coding agents capable of
generating, executing, and revising visualization code. However, existing
models often fail in practical workflows due to limited language coverage,
unreliable execution, and lack of iterative correction mechanisms. Progress has
been constrained by narrow datasets and benchmarks that emphasize single-round
generation and single-language tasks. To address these challenges, we introduce
three complementary resources for advancing visualization coding agents.
VisCode-Multi-679K is a large-scale, supervised dataset containing 679K
validated and executable visualization samples with multi-turn correction
dialogues across 12 programming languages. VisPlotBench is a benchmark for
systematic evaluation, featuring executable tasks, rendered outputs, and
protocols for both initial generation and multi-round self-debug. Finally, we
present VisCoder2, a family of multi-language visualization models trained on
VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms
strong open-source baselines and approaches the performance of proprietary
models like GPT-4.1, with further gains from iterative self-debug, reaching
82.4% overall execution pass rate at the 32B scale, particularly in symbolic or
compiler-dependent languages.

</details>


### [84] [Agentsway -- Software Development Methodology for AI Agents-based Teams](https://arxiv.org/abs/2510.23664)
*Eranga Bandara,Ross Gore,Xueping Liang,Sachini Rajapakse,Isurunima Kularathne,Pramoda Karunarathna,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Amin Hass,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.SE

TL;DR: 提出了一个名为Agentsway的新型软件开发框架，专门为AI代理作为一等协作者的环境设计，通过结构化生命周期和专业化代理角色来改进传统软件开发方法。


<details>
  <summary>Details</summary>
Motivation: 传统软件开发方法如Agile、Kanban等是为人类团队设计的，在AI代理参与规划、编码、测试和持续学习的环境中越来越不足，需要专门的方法论来解决这一差距。

Method: Agentsway框架围绕人类编排和隐私保护协作构建，定义了规划、提示、编码、测试和微调等专业化代理角色，通过微调LLM整合不同代理的输出和反馈进行回顾性学习。

Result: 该框架增强了领域特定推理和可解释决策，通过协调使用多个微调LLM和高级推理模型嵌入负责任AI原则，确保平衡、透明和负责任的决策。

Conclusion: Agentsway通过形式化代理中心协作、整合隐私设计原则和定义可测量指标，推进了软件工程，代表了迈向AI原生、自我改进软件开发方法的基础性步骤。

Abstract: The emergence of Agentic AI is fundamentally transforming how software is
designed, developed, and maintained. Traditional software development
methodologies such as Agile, Kanban, ShapeUp, etc, were originally designed for
human-centric teams and are increasingly inadequate in environments where
autonomous AI agents contribute to planning, coding, testing, and continuous
learning. To address this methodological gap, we present "Agentsway" a novel
software development framework designed for ecosystems where AI agents operate
as first-class collaborators. Agentsway introduces a structured lifecycle
centered on human orchestration, and privacy-preserving collaboration among
specialized AI agents. The framework defines distinct roles for planning,
prompting, coding, testing, and fine-tuning agents, each contributing to
iterative improvement and adaptive learning throughout the development process.
By integrating fine-tuned LLMs that leverage outputs and feedback from
different agents throughout the development cycle as part of a retrospective
learning process, Agentsway enhances domain-specific reasoning, and explainable
decision-making across the entire software development lifecycle. Responsible
AI principles are further embedded across the agents through the coordinated
use of multiple fine-tuned LLMs and advanced reasoning models, ensuring
balanced, transparent, and accountable decision-making. This work advances
software engineering by formalizing agent-centric collaboration, integrating
privacy-by-design principles, and defining measurable metrics for productivity
and trust. Agentsway represents a foundational step toward the next generation
of AI-native, self-improving software development methodologies. To the best of
our knowledge, this is the first research effort to introduce a dedicated
methodology explicitly designed for AI agent-based software engineering teams.

</details>


### [85] [RefleXGen:The unexamined code is not worth using](https://arxiv.org/abs/2510.23674)
*Bin Wang,Hui Li,AoFan Liu,BoTao Yang,Ao Yang,YiLu Zhong,Weixiang Huang,Yanping Zhang,Runhuai Huang,Weimin Zeng*

Main category: cs.SE

TL;DR: RefleXGen通过结合检索增强生成和引导式自反思机制，显著提升LLM生成代码的安全性，无需大量资源投入。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在代码生成中的安全性挑战，传统方法需要大量资源进行微调或构建安全代码数据集。

Method: 集成RAG技术和LLM的引导式自反思机制，通过自我评估和反思迭代优化代码生成过程。

Result: 显著提升多个模型的代码安全性：GPT-3.5 Turbo提升13.6%，GPT-4o提升6.7%，CodeQwen提升4.5%，Gemini提升5.8%。

Conclusion: 提升模型自反思质量是增强AI生成代码安全性的有效实用策略。

Abstract: Security in code generation remains a pivotal challenge when applying large
language models (LLMs). This paper introduces RefleXGen, an innovative method
that significantly enhances code security by integrating Retrieval-Augmented
Generation (RAG) techniques with guided self-reflection mechanisms inherent in
LLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing
specialized secure code datasets - processes that can be resource-intensive -
RefleXGen iteratively optimizes the code generation process through
self-assessment and reflection without the need for extensive resources. Within
this framework, the model continuously accumulates and refines its knowledge
base, thereby progressively improving the security of the generated code.
Experimental results demonstrate that RefleXGen substantially enhances code
security across multiple models, achieving a 13.6% improvement with GPT-3.5
Turbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a
5.8% improvement with Gemini. Our findings highlight that improving the quality
of model self-reflection constitutes an effective and practical strategy for
strengthening the security of AI-generated code.

</details>


### [86] [TDFlow: Agentic Workflows for Test Driven Software Engineering](https://arxiv.org/abs/2510.23761)
*Kevin Han,Siddharth Maddikayala,Tim Knappe,Om Patel,Austen Liao,Amir Barati Farimani*

Main category: cs.SE

TL;DR: TDFlow是一个基于测试驱动的智能工作流，将仓库级软件工程重构为测试解决任务，通过专门设计的子代理和受限工具来反复提出、修订和调试代码补丁，在SWE-Bench基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决仓库级软件工程修复的复杂性，通过测试驱动方法将复杂任务分解为可管理的子任务，减轻单个代理的长上下文负担，并专注于特定子任务的性能优化。

Method: 将软件工程程序修复分解为四个组件：补丁提出、调试、补丁修订和可选测试生成，每个组件由专门的子代理管理，使用精确设计的子代理和严格受限的工具。

Result: 在SWE-Bench Lite上达到88.8%的通过率（比次优系统提升27.8%），在SWE-Bench Verified上达到94.3%的通过率，800次运行中仅发现7次测试作弊。

Conclusion: 现代LLM在精心设计的测试驱动工作流中已能达到人类水平的测试解决能力，完全自主仓库修复的最终挑战在于生成有效的复现测试。

Abstract: We introduce TDFlow, a novel test-driven agentic workflow that frames
repository-scale software engineering as a test-resolution task, specifically
designed to solve human-written tests. Given a set of tests, TDFlow repeatedly
proposes, revises, and debugs repository-scale patches using precisely
engineered sub-agents and tightly constrained tools. The workflow decomposes
software engineering program repair into four components governed by respective
sub-agents. This simple, forced decoupling of patch proposing, debugging, patch
revision, and optional test generation (1) reduces long-context burden on any
individual sub-agent, (2) focuses each sub-agent on specific, pre-defined
sub-tasks, and (3) allows for specialized performance improvement on specific
sub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on
SWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and
94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within
SWE-Bench Lite and Verified uncover only 7 instances of test hacking, which
were subsequently counted as failures. Furthermore, we show that the primary
obstacle to human-level software engineering performance lies within writing
successful reproduction tests. We envision a human-LLM interactive system
powered by TDFlow where human developers write tests solved by LLM systems.
Together, these results indicate that modern LLMs, when embedded in a narrowly
engineered, test-driven workflow, already achieve human-level test resolution
-- with the final frontier for fully autonomous repository repair being the
accurate generation of valid reproduction tests.

</details>


### [87] [Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs](https://arxiv.org/abs/2510.24019)
*Xing Xing,Wei Wang,Lipeng Ma,Weidong Yang,Junjie Zheng*

Main category: cs.SE

TL;DR: 提出了一种生命周期感知的代码生成框架，通过引入需求分析、状态机建模和伪代码等中间产物，将代码生成与标准软件开发阶段对齐，显著提升了代码正确性和质量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的代码生成方法大多采用从问题描述到代码的直接单步翻译，忽视了结构化的软件工程实践。需要将代码生成与标准软件开发生命周期相结合。

Method: 设计生命周期感知框架，在训练和推理阶段系统性地引入中间产物（需求分析、状态机建模、伪代码），采用多步推理而非单步生成。

Result: 生命周期级微调使代码正确性提升高达75%；多步推理始终优于单步生成；开源LLM经微调后性能可匹配或超越预训练代码模型；在DeepSeek-Coder-1.3B上相对CodeBLEU提升显著。

Conclusion: 中间产物对最终代码质量有显著贡献，其中状态机建模影响最大；该框架在较少训练数据下仍保持鲁棒性。

Abstract: Recent progress in large language models (LLMs) has advanced automatic code
generation, yet most approaches rely on direct, single-step translation from
problem descriptions to code, disregarding structured software engineering
practices. We introduce a lifecycle-aware framework that systematically
incorporates intermediate artifacts such as requirements analysis, state
machine modeling, and pseudocode into both the training and inference stages.
This design aligns code generation with standard software development phases
and enables more structured reasoning. Experiments show that lifecycle-level
fine-tuning improves code correctness by up to 75% over the same model before
fine-tuning, with performance gains compounding across intermediate stages.
Multi-step inference consistently surpasses single-step generation,
demonstrating the effectiveness of intermediate scaffolding. Notably,
open-source LLMs, once fine-tuned under our framework, match or slightly
outperform models pretrained on code. When applied to DeepSeek-Coder-1.3B, our
framework yields relative CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and
22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B,
respectively. Our pipeline also proves robust with up to 80\% less training
data, confirming its resilience. Ablation studies further reveal that each
intermediate artifact contributes distinctly to final code quality, with state
machine modeling yielding the most substantial impact. Our source code and
detailed experimental data are available at
https://anonymous.4open.science/r/Lifecycle-Aware-3CCB.

</details>


### [88] [Investigating Software Aging in LLM-Generated Software Systems](https://arxiv.org/abs/2510.24188)
*César Santos,Ermeson Andrade,Roberto Natella*

Main category: cs.SE

TL;DR: 研究通过实验调查了LLM生成软件中的软件老化现象，发现所有应用都存在显著的内存增长、响应时间增加和性能不稳定等问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM自动生成软件的广泛采用，需要了解这类系统在持续执行下的长期可靠性，特别是软件老化问题。

Method: 使用Bolt平台和Baxbench标准化提示生成四个面向服务的应用，进行50小时负载测试，持续监控资源使用、响应时间和吞吐量。

Result: 结果显示所有应用都存在显著的软件老化证据，包括渐进式内存增长、响应时间增加和性能不稳定，统计分析确认了这些趋势。

Conclusion: 研究发现需要在自动生成软件中考虑老化问题，为未来缓解策略和长期可靠性评估研究奠定了基础。

Abstract: Automatically generated software, especially code produced by Large Language
Models (LLMs), is increasingly adopted to accelerate development and reduce
manual effort. However, little is known about the long-term reliability of such
systems under sustained execution. In this paper, we experimentally investigate
the phenomenon of software aging in applications generated by LLM-based tools.
Using the Bolt platform and standardized prompts from Baxbench, we generated
four service-oriented applications and subjected them to 50-hour load tests.
Resource usage, response time, and throughput were continuously monitored to
detect degradation patterns. The results reveal significant evidence of
software aging, including progressive memory growth, increased response time,
and performance instability across all applications. Statistical analyzes
confirm these trends and highlight variability in the severity of aging
according to the type of application. Our findings show the need to consider
aging in automatically generated software and provide a foundation for future
studies on mitigation strategies and long-term reliability evaluation.

</details>


### [89] [MAGNET: A Multi-Graph Attentional Network for Code Clone Detection](https://arxiv.org/abs/2510.24241)
*Zixian Zhang,Takfarinas Saber*

Main category: cs.SE

TL;DR: 提出了MAGNET多图注意力框架，通过联合利用AST、CFG和DFG表示来捕获源代码的语法和语义特征，在代码克隆检测任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测方法通常依赖单一表示（如AST、CFG、DFG），只能捕获代码语义的部分方面，而混合方法融合策略通常是手工设计且效果不佳。

Method: MAGNET集成残差图神经网络与节点级自注意力来学习局部和长程依赖，引入门控交叉注意力机制进行细粒度图间交互，使用Set2Set池化将多图嵌入融合为统一程序级表示。

Result: 在BigCloneBench和Google Code Jam数据集上分别达到96.5%和99.2%的F1分数，实现了最先进的性能。消融研究证实了多图融合和各注意力组件的关键贡献。

Conclusion: MAGNET通过多图注意力框架有效提升了代码克隆检测性能，证明了联合利用多种代码表示的重要性。

Abstract: Code clone detection is a fundamental task in software engineering that
underpins refactoring, debugging, plagiarism detection, and vulnerability
analysis. Existing methods often rely on singular representations such as
abstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs
(DFGs), which capture only partial aspects of code semantics. Hybrid approaches
have emerged, but their fusion strategies are typically handcrafted and
ineffective. In this study, we propose MAGNET, a multi-graph attentional
framework that jointly leverages AST, CFG, and DFG representations to capture
syntactic and semantic features of source code. MAGNET integrates residual
graph neural networks with node-level self-attention to learn both local and
long-range dependencies, introduces a gated cross-attention mechanism for
fine-grained inter-graph interactions, and employs Set2Set pooling to fuse
multi-graph embeddings into unified program-level representations. Extensive
experiments on BigCloneBench and Google Code Jam demonstrate that MAGNET
achieves state-of-the-art performance with an overall F1 score of 96.5\% and
99.2\% on the two datasets, respectively. Ablation studies confirm the critical
contributions of multi-graph fusion and each attentional component. Our code is
available at https://github.com/ZixianReid/Multigraph_match

</details>


### [90] [Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation](https://arxiv.org/abs/2510.24358)
*Lingyue Fu,Bolun Zhang,Hao Guan,Yaoming Zhu,Lin Qiu,Weiwen Liu,Xuezhi Cao,Xunliang Cai,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: 提出了PRDBench基准，通过代理驱动的管道构建包含50个真实Python项目的基准，解决现有代码代理评估基准的高标注成本和评估指标僵化问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码代理评估基准存在高标注成本和专业知识要求，且评估指标主要依赖单元测试，缺乏灵活性。

Method: 采用代理驱动的基准构建管道，结合人工监督生成多样化的项目级任务，并引入Agent-as-a-Judge范式进行评分。

Result: PRDBench包含50个真实Python项目，涵盖20个领域，具有结构化PRD需求、全面评估标准和参考实现，能有效评估代码代理和评估代理的能力。

Conclusion: PRDBench为代码代理评估提供了可扩展且鲁棒的框架，支持超越单元测试的多种测试类型评估。

Abstract: Recent advances in code agents have enabled automated software development at
the project level, supported by large language models (LLMs) and widely adopted
tools. However, existing benchmarks for code agent evaluation face two major
limitations: high annotation cost and expertise requirements, and rigid
evaluation metrics that rely primarily on unit tests. To address these
challenges, we propose an agent-driven benchmark construction pipeline that
leverages human supervision to efficiently generate diverse and challenging
project-level tasks. Based on this approach, we introduce PRDBench, a novel
benchmark comprising 50 real-world Python projects across 20 domains, each with
structured Product Requirement Document (PRD) requirements, comprehensive
evaluation criteria, and reference implementations. PRDBench features rich data
sources, high task complexity, and flexible metrics. We further employ an
Agent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of
various test types beyond unit tests. Extensive experiments on PRDBench
demonstrate its effectiveness in assessing the capabilities of both code agents
and evaluation agents, providing a scalable and robust framework for annotation
and evaluation.

</details>


### [91] [LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and the Road Ahead](https://arxiv.org/abs/2510.24367)
*Junda He,Jieke Shi,Terry Yue Zhuo,Christoph Treude,Jiamou Sun,Zhenchang Xing,Xiaoning Du,David Lo*

Main category: cs.SE

TL;DR: 本文探讨了使用LLM作为评估器（LLM-as-a-Judge）来自动评估LLM生成的软件工件的必要性，分析了现有研究的局限性，并提出了到2030年的发展路线图。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件工程中的广泛应用，产生了大量软件工件，但缺乏可扩展、可靠的评估方法。人工评估成本高，传统自动指标无法捕捉质量细节。

Method: 通过文献综述分析现有SE研究，识别研究空白，并制定详细的发展路线图。

Result: LLM-as-a-Judge方法利用LLM的推理能力，提供了在自动化规模上实现类似人类细微评估的途径。

Conclusion: 到2030年，这些框架有望成为可靠、稳健、可扩展的人类替代品，能够进行一致、多方面的工件评估。

Abstract: The rapid integration of Large Language Models (LLMs) into software
engineering (SE) has revolutionized tasks like code generation, producing a
massive volume of software artifacts. This surge has exposed a critical
bottleneck: the lack of scalable, reliable methods to evaluate these outputs.
Human evaluation is costly and time-consuming, while traditional automated
metrics like BLEU fail to capture nuanced quality aspects. In response, the
LLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.
This approach leverages the advanced reasoning of LLMs, offering a path toward
human-like nuance at automated scale. However, LLM-as-a-Judge research in SE is
still in its early stages. This forward-looking SE 2030 paper aims to steer the
community toward advancing LLM-as-a-Judge for evaluating LLM-generated software
artifacts. We provide a literature review of existing SE studies, analyze their
limitations, identify key research gaps, and outline a detailed roadmap. We
envision these frameworks as reliable, robust, and scalable human surrogates
capable of consistent, multi-faceted artifact evaluation by 2030. Our work aims
to foster research and adoption of LLM-as-a-Judge frameworks, ultimately
improving the scalability of software artifact evaluation.

</details>


### [92] [CodeWiki: Automated Repository-Level Documentation at Scale](https://arxiv.org/abs/2510.24428)
*Nguyen Hoang Anh,Minh Le-Anh,Bach Le,Nghi D. Q. Bui*

Main category: cs.SE

TL;DR: CodeWiki是首个开源的全栈仓库级文档生成框架，通过分层分解、递归代理处理和文本-视觉合成，在7种编程语言上实现高质量的仓库级文档生成。


<details>
  <summary>Details</summary>
Motivation: 开发人员58%的时间用于理解代码库，但现有LLM只能处理函数级文档，无法捕捉仓库级的架构模式和跨模块交互。

Method: 采用三个创新：分层分解保持架构上下文、递归代理处理与动态委托、文本和视觉工件的合成（包括架构图和数据流）。

Result: CodeWiki在专有模型上获得68.79%质量分，开源模型上获得64.80%，优于现有闭源系统，展示了可扩展的准确文档生成能力。

Conclusion: CodeWiki框架能够为真实世界仓库提供可扩展、准确的仓库级文档，解决了现有方法在仓库级文档生成上的局限性。

Abstract: Developers spend nearly 58% of their time understanding codebases, yet
maintaining comprehensive documentation remains challenging due to complexity
and manual effort. While recent Large Language Models (LLMs) show promise for
function-level documentation, they fail at the repository level, where
capturing architectural patterns and cross-module interactions is essential. We
introduce CodeWiki, the first open-source framework for holistic
repository-level documentation across seven programming languages. CodeWiki
employs three innovations: (i) hierarchical decomposition that preserves
architectural context, (ii) recursive agentic processing with dynamic
delegation, and (iii) synthesis of textual and visual artifacts including
architecture diagrams and data flows. We also present CodeWikiBench, the first
repository-level documentation benchmark with multi-level rubrics and agentic
assessment. CodeWiki achieves 68.79% quality score with proprietary models and
64.80% with open-source alternatives, outperforming existing closed-source
systems and demonstrating scalable, accurate documentation for real-world
repositories.

</details>
