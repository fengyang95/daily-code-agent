{"id": "2601.08045", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.08045", "abs": "https://arxiv.org/abs/2601.08045", "authors": ["Xinyi Zhou", "Zeinadsadat Saghi", "Sadra Sabouri", "Rahul Pandita", "Mollie McGuire", "Souti Chattopadhyay"], "title": "Cognitive Biases in LLM-Assisted Software Development", "comment": "13 pages, 6 figures, 7 tables", "summary": "The widespread adoption of Large Language Models (LLMs) in software development is transforming programming from a solution-generative to a solution-evaluative activity. This shift opens a pathway for new cognitive challenges that amplify existing decision-making biases or create entirely novel ones. One such type of challenge stems from cognitive biases, which are thinking patterns that lead people away from logical reasoning and result in sub-optimal decisions. How do cognitive biases manifest and impact decision-making in emerging AI-collaborative development? This paper presents the first comprehensive study of cognitive biases in LLM-assisted development. We employ a mixed-methods approach, combining observational studies with 14 student and professional developers, followed by surveys with 22 additional developers. We qualitatively compare categories of biases affecting developers against the traditional non-LLM workflows. Our findings suggest that LLM-related actions are more likely to be associated with novel biases. Through a systematic analysis of 90 cognitive biases specific to developer-LLM interactions, we develop a taxonomy of 15 bias categories validated by cognitive psychologists. We found that 48.8% of total programmer actions are biased, and developer-LLM interactions account for 56.4% of these biased actions. We discuss how these bias categories manifest, present tools and practices for developers, and recommendations for LLM tool builders to help mitigate cognitive biases in human-AI programming.", "AI": {"tldr": "\u9996\u6b21\u5168\u9762\u7814\u7a76LLM\u8f85\u52a9\u5f00\u53d1\u4e2d\u7684\u8ba4\u77e5\u504f\u89c1\uff0c\u53d1\u73b048.8%\u7684\u7a0b\u5e8f\u5458\u884c\u4e3a\u5b58\u5728\u504f\u89c1\uff0c\u5176\u4e2d56.4%\u4e0eLLM\u4ea4\u4e92\u76f8\u5173\uff0c\u5e76\u5efa\u7acb\u4e86\u5305\u542b15\u4e2a\u504f\u89c1\u7c7b\u522b\u7684\u5206\u7c7b\u6cd5\u3002", "motivation": "LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6b63\u5728\u5c06\u7f16\u7a0b\u4ece\u89e3\u51b3\u65b9\u6848\u751f\u6210\u8f6c\u53d8\u4e3a\u89e3\u51b3\u65b9\u6848\u8bc4\u4f30\u6d3b\u52a8\uff0c\u8fd9\u79cd\u8f6c\u53d8\u53ef\u80fd\u653e\u5927\u73b0\u6709\u51b3\u7b56\u504f\u89c1\u6216\u521b\u9020\u5168\u65b0\u504f\u89c1\u3002\u9700\u8981\u7814\u7a76\u8ba4\u77e5\u504f\u89c1\u5728AI\u534f\u4f5c\u5f00\u53d1\u4e2d\u7684\u8868\u73b0\u548c\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a\u5bf914\u540d\u5b66\u751f\u548c\u4e13\u4e1a\u5f00\u53d1\u4eba\u5458\u8fdb\u884c\u89c2\u5bdf\u7814\u7a76\uff0c\u968f\u540e\u5bf922\u540d\u5f00\u53d1\u4eba\u5458\u8fdb\u884c\u8c03\u67e5\u3002\u901a\u8fc7\u5b9a\u6027\u6bd4\u8f83\u4f20\u7edf\u975eLLM\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u504f\u89c1\u7c7b\u522b\uff0c\u7cfb\u7edf\u5206\u6790\u4e8690\u4e2a\u7279\u5b9a\u4e8e\u5f00\u53d1\u8005-LLM\u4ea4\u4e92\u7684\u8ba4\u77e5\u504f\u89c1\u3002", "result": "\u53d1\u73b048.8%\u7684\u7a0b\u5e8f\u5458\u884c\u4e3a\u5b58\u5728\u504f\u89c1\uff0c\u5176\u4e2d56.4%\u4e0eLLM\u4ea4\u4e92\u76f8\u5173\u3002LLM\u76f8\u5173\u884c\u4e3a\u66f4\u53ef\u80fd\u4e0e\u65b0\u9896\u504f\u89c1\u76f8\u5173\u3002\u5efa\u7acb\u4e86\u7531\u8ba4\u77e5\u5fc3\u7406\u5b66\u5bb6\u9a8c\u8bc1\u7684\u5305\u542b15\u4e2a\u504f\u89c1\u7c7b\u522b\u7684\u5206\u7c7b\u6cd5\u3002", "conclusion": "LLM\u8f85\u52a9\u5f00\u53d1\u5f15\u5165\u4e86\u65b0\u7684\u8ba4\u77e5\u504f\u89c1\u6311\u6218\u3002\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u5b9e\u8df5\u5efa\u8bae\uff0c\u5e76\u4e3aLLM\u5de5\u5177\u6784\u5efa\u8005\u63d0\u51fa\u4e86\u51cf\u8f7b\u4eba\u7c7b-AI\u7f16\u7a0b\u4e2d\u8ba4\u77e5\u504f\u89c1\u7684\u5efa\u8bae\u3002", "topic": "agent analysis"}}
{"id": "2601.08734", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08734", "abs": "https://arxiv.org/abs/2601.08734", "authors": ["Prithwish Jana", "Sam Davidson", "Bhavana Bhasker", "Andrey Kan", "Anoop Deoras", "Laurent Callot"], "title": "TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback", "comment": "The paper has been published at the 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE 2026), Rio de Janeiro, Brazil, April 12-18, 2026", "summary": "Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.", "AI": {"tldr": "TerraFormer\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u4e0e\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u7528\u4e8e\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\uff08IaC\uff09\u7684\u751f\u6210\u548c\u53d8\u5f02\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u63d0\u4f9b\u8bed\u6cd5\u3001\u53ef\u90e8\u7f72\u6027\u548c\u7b56\u7565\u5408\u89c4\u6027\u53cd\u9988\u3002", "motivation": "\u81ea\u52a8\u5316\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\u5177\u6709\u6311\u6218\u6027\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u914d\u7f6e\u65f6\u7ecf\u5e38\u51fa\u9519\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faTerraFormer\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u4e0e\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u63d0\u4f9b\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u9a8c\u8bc1\u548c\u8fed\u4ee3LLM\u81ea\u6821\u6b63\u521b\u5efa\u4e86\u4e24\u4e2a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6TF-Gen\u548cTF-Mutn\u3002", "result": "TerraFormer\u5728IaC-Eval\u4e0a\u6bd4\u57fa\u7840LLM\u63d0\u534715.94%\u6b63\u786e\u7387\uff0c\u5728TF-Gen\u548cTF-Mutn\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u63d0\u534711.65%\u548c19.60%\uff0c\u4f18\u4e8e\u5305\u62ecSonnet 3.7\u3001DeepSeek-R1\u548cGPT-4.1\u7b49\u592750\u500d\u7684\u6a21\u578b\uff0c\u5728\u6700\u4f73\u5b9e\u8df5\u548c\u5b89\u5168\u6027\u5408\u89c4\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "TerraFormer\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86IaC\u751f\u6210\u548c\u53d8\u5f02\u7684\u6b63\u786e\u6027\uff0c\u8d85\u8d8a\u4e86\u66f4\u5927\u7684LLM\u6a21\u578b\uff0c\u5728\u5b9e\u8df5\u548c\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "topic": "code agent"}}
{"id": "2601.07994", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07994", "abs": "https://arxiv.org/abs/2601.07994", "authors": ["Nayoung Choi", "Jonathan Zhang", "Jinho D. Choi"], "title": "DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs", "comment": "Accepted (B) to TACL 2026", "summary": "Large Language Models (LLMs) often exhibit increased response latency and degraded answer quality as dialogue length grows, making effective context management essential. However, existing methods rely on extra LLM calls to build memory or perform offline memory construction without considering the current user utterance, which can introduce inefficiencies or disrupt conversational continuity. We introduce DyCP, a lightweight context management method that dynamically segment and retrieve relevant memory at query time. It preserves the sequential structure of dialogue without predefined topic boundaries and supports efficient, adaptive context retrieval. Across three long-form dialogue benchmarks, LoCoMo, MT-Bench+, and SCM4LLMs, and multiple LLMs, DyCP consistently improves answer quality while reducing response latency. We also examine the gap between modern LLMs' expanded context windows and their actual long-context processing capacity, highlighting the continued importance of effective context management.", "AI": {"tldr": "DyCP\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u7ba1\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u5272\u548c\u68c0\u7d22\u76f8\u5173\u8bb0\u5fc6\u6765\u6539\u5584LLM\u5728\u957f\u5bf9\u8bdd\u4e2d\u7684\u54cd\u5e94\u5ef6\u8fdf\u548c\u7b54\u6848\u8d28\u91cf\uff0c\u65e0\u9700\u989d\u5916LLM\u8c03\u7528\u6216\u9884\u5b9a\u4e49\u4e3b\u9898\u8fb9\u754c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u957f\u5ea6\u589e\u52a0\u65f6\u4f1a\u51fa\u73b0\u54cd\u5e94\u5ef6\u8fdf\u589e\u52a0\u548c\u7b54\u6848\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u989d\u5916\u7684LLM\u8c03\u7528\u6765\u6784\u5efa\u8bb0\u5fc6\uff0c\u8981\u4e48\u8fdb\u884c\u79bb\u7ebf\u8bb0\u5fc6\u6784\u5efa\u800c\u4e0d\u8003\u8651\u5f53\u524d\u7528\u6237\u8bdd\u8bed\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u6216\u7834\u574f\u5bf9\u8bdd\u8fde\u7eed\u6027\u3002", "method": "DyCP\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u7ba1\u7406\u65b9\u6cd5\uff0c\u5728\u67e5\u8be2\u65f6\u52a8\u6001\u5206\u5272\u548c\u68c0\u7d22\u76f8\u5173\u8bb0\u5fc6\u3002\u5b83\u4fdd\u7559\u4e86\u5bf9\u8bdd\u7684\u987a\u5e8f\u7ed3\u6784\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u4e3b\u9898\u8fb9\u754c\uff0c\u652f\u6301\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u4e0a\u4e0b\u6587\u68c0\u7d22\u3002", "result": "\u5728\u4e09\u4e2a\u957f\u5bf9\u8bdd\u57fa\u51c6\u6d4b\u8bd5\uff08LoCoMo\u3001MT-Bench+\u548cSCM4LLMs\uff09\u548c\u591a\u4e2aLLM\u4e0a\uff0cDyCP\u4e00\u81f4\u5730\u63d0\u9ad8\u4e86\u7b54\u6848\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u54cd\u5e94\u5ef6\u8fdf\u3002", "conclusion": "\u73b0\u4ee3LLM\u867d\u7136\u6269\u5c55\u4e86\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u4f46\u5b9e\u9645\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u4ecd\u6709\u5dee\u8ddd\uff0c\u56e0\u6b64\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002DyCP\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.08079", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.08079", "abs": "https://arxiv.org/abs/2601.08079", "authors": ["Hongjin Qian", "Zhao Cao", "Zheng Liu"], "title": "MemoBrain: Executive Memory as an Agentic Brain for Reasoning", "comment": "Our codes are in https://github.com/qhjqhj00/MemoBrain", "summary": "Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.\n  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.\n  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.", "AI": {"tldr": "MemoBrain\u662f\u4e00\u4e2a\u7528\u4e8e\u5de5\u5177\u589e\u5f3a\u578b\u667a\u80fd\u4f53\u7684\u6267\u884c\u8bb0\u5fc6\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u4f9d\u8d56\u611f\u77e5\u7684\u8bb0\u5fc6\u6765\u7ba1\u7406\u957f\u65f6\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e2d\u95f4\u72b6\u6001\u548c\u903b\u8f91\u5173\u7cfb\uff0c\u89e3\u51b3\u4e0a\u4e0b\u6587\u7d2f\u79ef\u95ee\u9898\u3002", "motivation": "\u5de5\u5177\u589e\u5f3a\u578b\u667a\u80fd\u4f53\u6846\u67b6\u4e2d\u7684\u590d\u6742\u63a8\u7406\u672c\u8d28\u4e0a\u662f\u957f\u65f6\u7a0b\u7684\uff0c\u5bfc\u81f4\u63a8\u7406\u8f68\u8ff9\u548c\u4e34\u65f6\u5de5\u5177\u4ea7\u7269\u4e0d\u65ad\u79ef\u7d2f\uff0c\u8d85\u51fa\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6709\u9650\u5de5\u4f5c\u4e0a\u4e0b\u6587\u3002\u7f3a\u4e4f\u663e\u5f0f\u8bb0\u5fc6\u673a\u5236\u4f1a\u7834\u574f\u903b\u8f91\u8fde\u7eed\u6027\u5e76\u524a\u5f31\u4efb\u52a1\u5bf9\u9f50\uff0c\u56e0\u6b64\u8bb0\u5fc6\u6210\u4e3a\u7ef4\u6301\u957f\u65f6\u7a0b\u8fde\u8d2f\u3001\u76ee\u6807\u5bfc\u5411\u63a8\u7406\u7684\u6838\u5fc3\u7ec4\u4ef6\u3002", "method": "\u63d0\u51faMemoBrain\u6267\u884c\u8bb0\u5fc6\u6a21\u578b\uff0c\u4f5c\u4e3a\u63a8\u7406\u667a\u80fd\u4f53\u7684\u534f\u540c\u4f19\u4f34\u8fd0\u884c\u3002\u5b83\u6784\u5efa\u4f9d\u8d56\u611f\u77e5\u7684\u8bb0\u5fc6\u6765\u6355\u83b7\u5173\u952e\u4e2d\u95f4\u72b6\u6001\u53ca\u5176\u903b\u8f91\u5173\u7cfb\uff0c\u5728\u4e0d\u963b\u585e\u6267\u884c\u7684\u60c5\u51b5\u4e0b\u7ec4\u7ec7\u63a8\u7406\u8fdb\u5ea6\uff0c\u5e76\u4e3b\u52a8\u7ba1\u7406\u5de5\u4f5c\u4e0a\u4e0b\u6587\u3002\u5177\u4f53\u673a\u5236\u5305\u62ec\uff1a\u4fee\u526a\u65e0\u6548\u6b65\u9aa4\u3001\u6298\u53e0\u5b8c\u6210\u7684\u5b50\u8f68\u8ff9\u3001\u5728\u56fa\u5b9a\u4e0a\u4e0b\u6587\u9884\u7b97\u4e0b\u4fdd\u6301\u7d27\u51d1\u7684\u9ad8\u663e\u8457\u6027\u63a8\u7406\u4e3b\u5e72\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u65f6\u7a0b\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ecGAIA\u3001WebWalker\u548cBrowseComp-Plus\uff09\u4e0a\u8bc4\u4f30MemoBrain\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u6a21\u578b\u5c55\u793a\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "MemoBrain\u901a\u8fc7\u663e\u5f0f\u8ba4\u77e5\u63a7\u5236\u673a\u5236\u7ba1\u7406\u63a8\u7406\u8f68\u8ff9\uff0c\u800c\u975e\u88ab\u52a8\u79ef\u7d2f\u4e0a\u4e0b\u6587\uff0c\u4e3a\u5de5\u5177\u589e\u5f3a\u578b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7ef4\u6301\u957f\u65f6\u7a0b\u8fde\u8d2f\u63a8\u7406\u7684\u6709\u6548\u8bb0\u5fc6\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.08003", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.08003", "abs": "https://arxiv.org/abs/2601.08003", "authors": ["Weiyue Li", "Mingxiao Song", "Zhenda Shen", "Dachuan Zhao", "Yunfan Long", "Yi Li", "Yongce Li", "Ruyi Yang", "Mengyu Wang"], "title": "LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback", "comment": null, "summary": "Large Language Models (LLMs) often struggle with creative generation, and multi-agent frameworks that improve reasoning through interaction can paradoxically hinder creativity by inducing content homogenization. We introduce LLM Review, a peer-review-inspired framework implementing Blind Peer Review: agents exchange targeted feedback while revising independently, preserving divergent creative trajectories. To enable rigorous evaluation, we propose SciFi-100, a science fiction writing dataset with a unified framework combining LLM-as-a-judge scoring, human annotation, and rule-based novelty metrics. Experiments demonstrate that LLM Review consistently outperforms multi-agent baselines, and smaller models with our framework can surpass larger single-agent models, suggesting interaction structure may substitute for model scale.", "AI": {"tldr": "LLM Review\u6846\u67b6\u901a\u8fc7\u76f2\u5ba1\u673a\u5236\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u521b\u610f\u540c\u8d28\u5316\u95ee\u9898\uff0c\u5728\u79d1\u5e7b\u5199\u4f5c\u4efb\u52a1\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c0f\u6a21\u578b\u914d\u5408\u8be5\u6846\u67b6\u53ef\u8d85\u8d8a\u5927\u5355\u667a\u80fd\u4f53\u6a21\u578b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u521b\u610f\u751f\u6210\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u800c\u591a\u667a\u80fd\u4f53\u6846\u67b6\u867d\u7136\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5374\u53ef\u80fd\u5bfc\u81f4\u5185\u5bb9\u540c\u8d28\u5316\uff0c\u53cd\u800c\u963b\u788d\u521b\u9020\u529b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fc3\u8fdb\u4e92\u52a8\u53c8\u80fd\u4fdd\u6301\u521b\u610f\u591a\u6837\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLLM Review\u6846\u67b6\uff0c\u91c7\u7528\u76f2\u5ba1\u673a\u5236\uff1a\u667a\u80fd\u4f53\u4ea4\u6362\u9488\u5bf9\u6027\u53cd\u9988\u4f46\u72ec\u7acb\u4fee\u6539\uff0c\u4fdd\u6301\u521b\u610f\u8f68\u8ff9\u7684\u591a\u6837\u6027\u3002\u540c\u65f6\u6784\u5efaSciFi-100\u79d1\u5e7b\u5199\u4f5c\u6570\u636e\u96c6\uff0c\u7ed3\u5408LLM\u8bc4\u5206\u3001\u4eba\u5de5\u6807\u6ce8\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u65b0\u9896\u6027\u6307\u6807\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLM Review\u6301\u7eed\u4f18\u4e8e\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5c0f\u6a21\u578b\u914d\u5408\u8be5\u6846\u67b6\u53ef\u4ee5\u8d85\u8d8a\u66f4\u5927\u7684\u5355\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u8868\u660e\u4ea4\u4e92\u7ed3\u6784\u53ef\u4ee5\u66ff\u4ee3\u6a21\u578b\u89c4\u6a21\u3002", "conclusion": "\u901a\u8fc7\u76f2\u5ba1\u673a\u5236\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u80fd\u591f\u6709\u6548\u63d0\u5347\u521b\u610f\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u6837\u6027\uff0c\u4e3aLLM\u7684\u521b\u610f\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2601.08545", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.08545", "abs": "https://arxiv.org/abs/2601.08545", "authors": ["Zhenlong Dai", "Zhuoluo Zhao", "Hengning Wang", "Xiu Tang", "Sai Wu", "Chang Yao", "Zhipeng Gao", "Jingyuan Chen"], "title": "Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement", "comment": null, "summary": "With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \\textbf{LPR} (\\textbf{L}earner-Tailored \\textbf{P}rogram \\textbf{R}epair). We then propose a novel and effective framework, \\textbf{\\textsc{\\MethodName{}}} (\\textbf{L}earner-Tailored \\textbf{S}olution \\textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.", "AI": {"tldr": "\u63d0\u51faLPR\u4efb\u52a1\u548cLSG\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u548c\u8fed\u4ee3\u4f18\u5316\u63d0\u5347\u7f16\u7a0b\u8f85\u5bfc\u4e2d\u7684\u7a0b\u5e8f\u4fee\u590d\u6548\u679c\uff0c\u5e76\u63d0\u4f9b\u9519\u8bef\u63cf\u8ff0", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4fee\u590d\u7f16\u7a0b\u5b66\u4e60\u8005\u7684\u9519\u8bef\u4ee3\u7801\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u9519\u8bef\u6839\u672c\u539f\u56e0\u7684\u89e3\u91ca\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u7f16\u7a0b\u8f85\u5bfc\u7cfb\u7edf", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4fee\u590d\u65b9\u6848\u68c0\u7d22\u6784\u5efa\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u7f16\u8f91\u9a71\u52a8\u68c0\u7d22\u6307\u5bfcLLM\u8bc6\u522b\u548c\u4fee\u590d\u9519\u8bef\uff1b2) \u89e3\u51b3\u65b9\u6848\u5f15\u5bfc\u7684\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\uff0c\u5728\u68c0\u7d22\u65b9\u6848\u6307\u5bfc\u4e0b\u4fee\u590d\u4ee3\u7801\u5e76\u63d0\u4f9b\u89e3\u91ca\uff1b\u91c7\u7528\u8fed\u4ee3\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u4f18\u5316\u68c0\u7d22\u65b9\u5411", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5927\u5e45\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86LPR\u4efb\u52a1\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684LSG\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u7a0b\u5e8f\u4fee\u590d\u6027\u80fd\u5e76\u63d0\u4f9b\u9519\u8bef\u63cf\u8ff0\uff0c\u4e3a\u7f16\u7a0b\u8f85\u5bfc\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848", "topic": "swe application"}}
{"id": "2601.08058", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08058", "abs": "https://arxiv.org/abs/2601.08058", "authors": ["Zhenghao He", "Guangzhi Xiong", "Bohan Liu", "Sanchit Sinha", "Aidong Zhang"], "title": "Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models", "comment": null, "summary": "Chain-of-Thought (CoT) prompting has improved the reasoning performance of large language models (LLMs), but it remains unclear why it works and whether it is the unique mechanism for triggering reasoning in large language models. In this work, we study this question by directly analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), identifying a small set of latent features that are causally associated with LLM reasoning behavior. Across multiple model families and reasoning benchmarks, we find that steering a single reasoning-related latent feature can substantially improve accuracy without explicit CoT prompting. For large models, latent steering achieves performance comparable to standard CoT prompting while producing more efficient outputs. We further observe that this reasoning-oriented internal state is triggered early in generation and can override prompt-level instructions that discourage explicit reasoning. Overall, our results suggest that multi-step reasoning in LLMs is supported by latent internal activations that can be externally activated, while CoT prompting is one effective, but not unique, way of activating this mechanism rather than its necessary cause.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u63a8\u7406\u80fd\u529b\u7531\u5185\u90e8\u6f5c\u5728\u7279\u5f81\u652f\u6301\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u5e76\u64cd\u63a7\u8fd9\u4e9b\u7279\u5f81\uff0c\u65e0\u9700\u663e\u5f0f\u601d\u7ef4\u94fe\u63d0\u793a\u5373\u53ef\u89e6\u53d1\u63a8\u7406\uff0c\u8868\u660e\u601d\u7ef4\u94fe\u53ea\u662f\u6fc0\u6d3b\u63a8\u7406\u673a\u5236\u7684\u6709\u6548\u65b9\u5f0f\u4e4b\u4e00\u800c\u975e\u5fc5\u8981\u6761\u4ef6\u3002", "motivation": "\u63a2\u7a76\u601d\u7ef4\u94fe\u63d0\u793a\u4e3a\u4f55\u6709\u6548\uff0c\u4ee5\u53ca\u5b83\u662f\u5426\u662f\u89e6\u53d1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u552f\u4e00\u673a\u5236\uff0c\u7406\u89e3LLM\u63a8\u7406\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790LLM\u5185\u90e8\u8868\u793a\uff0c\u8bc6\u522b\u4e0e\u63a8\u7406\u76f8\u5173\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u901a\u8fc7\u64cd\u63a7\u8fd9\u4e9b\u7279\u5f81\u6765\u5e72\u9884\u6a21\u578b\u63a8\u7406\u884c\u4e3a\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u548c\u63a8\u7406\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u64cd\u63a7\u5355\u4e2a\u63a8\u7406\u76f8\u5173\u6f5c\u5728\u7279\u5f81\u53ef\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u7387\uff0c\u65e0\u9700\u663e\u5f0f\u601d\u7ef4\u94fe\u63d0\u793a\uff1b\u5bf9\u4e8e\u5927\u578b\u6a21\u578b\uff0c\u6f5c\u5728\u7279\u5f81\u64cd\u63a7\u80fd\u8fbe\u5230\u4e0e\u6807\u51c6\u601d\u7ef4\u94fe\u63d0\u793a\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8f93\u51fa\u66f4\u9ad8\u6548\uff1b\u63a8\u7406\u5bfc\u5411\u7684\u5185\u90e8\u72b6\u6001\u5728\u751f\u6210\u65e9\u671f\u88ab\u89e6\u53d1\uff0c\u5e76\u80fd\u8986\u76d6\u963b\u6b62\u663e\u5f0f\u63a8\u7406\u7684\u6307\u4ee4\u7ea7\u63d0\u793a\u3002", "conclusion": "LLM\u7684\u591a\u6b65\u63a8\u7406\u7531\u53ef\u5916\u90e8\u6fc0\u6d3b\u7684\u6f5c\u5728\u5185\u90e8\u6fc0\u6d3b\u652f\u6301\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u662f\u6fc0\u6d3b\u8be5\u673a\u5236\u7684\u6709\u6548\u65b9\u5f0f\u4e4b\u4e00\uff0c\u4f46\u5e76\u975e\u5176\u5fc5\u8981\u539f\u56e0\u3002", "topic": "agent analysis"}}
{"id": "2601.08156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08156", "abs": "https://arxiv.org/abs/2601.08156", "authors": ["Arin Gopalan Yadav", "Varad Dherange", "Kumar Shivam"], "title": "Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions", "comment": "We propose and evaluate a hierarchical LLM-driven multi-agent framework for adaptive disruption management in last-mile logistics, integrating planning, coordination, and natural-language reasoning. The system is validated through simulation-based experiments and qualitative analysis. Includes figures and tables. 33 pages", "summary": "This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery disruptions. Synapse employs a hierarchical multi-agent architecture in which a central Resolution Supervisor agent performs strategic task decomposition and delegates subtasks to specialized worker agents responsible for tactical execution. The system is orchestrated using LangGraph to manage complex and cyclical workflows. To validate the framework, a benchmark dataset of 30 complex disruption scenarios was curated from a qualitative analysis of over 6,000 real-world user reviews. System performance is evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.", "AI": {"tldr": "Project Synapse\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u4e3b\u89e3\u51b3\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u4e2d\u65ad\u7684\u65b0\u578b\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u4f7f\u7528LangGraph\u7f16\u6392\u590d\u6742\u5de5\u4f5c\u6d41\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u4e0a\u901a\u8fc7LLM-as-a-Judge\u534f\u8bae\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u4e2d\u7684\u590d\u6742\u4e2d\u65ad\u95ee\u9898\uff0c\u8fd9\u4e9b\u4e2d\u65ad\u901a\u5e38\u9700\u8981\u591a\u6b65\u9aa4\u51b3\u7b56\u548c\u534f\u8c03\uff0c\u4f20\u7edf\u81ea\u52a8\u5316\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff1a\u4e2d\u592eResolution Supervisor\u8d1f\u8d23\u6218\u7565\u4efb\u52a1\u5206\u89e3\uff0c\u5c06\u5b50\u4efb\u52a1\u59d4\u6258\u7ed9\u4e13\u95e8\u7684worker\u667a\u80fd\u4f53\u6267\u884c\u6218\u672f\u64cd\u4f5c\u3002\u4f7f\u7528LangGraph\u7f16\u6392\u590d\u6742\u5faa\u73af\u5de5\u4f5c\u6d41\u3002\u4ece6000\u591a\u4e2a\u771f\u5b9e\u7528\u6237\u8bc4\u8bba\u4e2d\u63d0\u53d630\u4e2a\u590d\u6742\u4e2d\u65ad\u573a\u666f\u6784\u5efa\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7LLM-as-a-Judge\u534f\u8bae\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u91c7\u7528\u660e\u786e\u7684\u504f\u89c1\u7f13\u89e3\u63aa\u65bd\u3002\u8bba\u6587\u5c55\u793a\u4e86\u6846\u67b6\u5728\u590d\u6742\u914d\u9001\u4e2d\u65ad\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "Project Synapse\u4e3a\u81ea\u4e3b\u89e3\u51b3\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u4e2d\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u51b3\u7b56\u548c\u534f\u8c03\u4efb\u52a1\u3002", "topic": "agent analysis"}}
{"id": "2601.08166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08166", "abs": "https://arxiv.org/abs/2601.08166", "authors": ["Mohammad Pivezhandi", "Mahdi Banisharif", "Abusayeed Saifullah", "Ali Jannesari"], "title": "ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms", "comment": "39 pages, 12 figures, 8 tables (including appendix)", "summary": "Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6838\u5e73\u53f0\u7684\u6e29\u5ea6\u548c\u80fd\u91cf\u611f\u77e5\u8c03\u5ea6\uff0c\u7ed3\u5408LLM\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u5b9e\u73b0\u96f6\u6837\u672c\u90e8\u7f72\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u8c03\u5ea6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709DVFS\u548c\u4efb\u52a1\u5206\u914d\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u5229\u7528\u7387\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u5ffd\u7565\u505c\u6ede\u65f6\u95f4\uff0c\u57fa\u4e8e\u79bb\u7ebf\u5206\u6790\u7684\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u8fd0\u884c\u65f6\u53d8\u5316\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u3001\u65e0\u9700\u7279\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u5206\u6790\u7684\u9ad8\u6548\u8c03\u5ea6\u65b9\u6848\u3002", "method": "1) \u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5206\u89e3\u6307\u6570\u7ea7\u52a8\u4f5c\u7a7a\u95f4\uff1b2) \u7ed3\u5408\u56de\u5f52\u6280\u672f\u7684\u7cbe\u786e\u73af\u5883\u6a21\u578b\u9884\u6d4b\u70ed\u52a8\u6001\u548c\u6027\u80fd\u72b6\u6001\uff1b3) LLM\u63d0\u53d613\u4e2a\u4ee3\u7801\u7ea7\u8bed\u4e49\u7279\u5f81\uff0c\u65e0\u9700\u6267\u884c\u7a0b\u5e8f\uff1b4) Dyna-Q\u542f\u53d1\u7684\u6846\u67b6\u6574\u5408\u76f4\u63a5\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u3002", "result": "1) \u540e\u7eed\u51b3\u7b56\u5ef6\u8fdf358ms\uff0c\u9996\u6b21\u51b3\u7b563.5-8.0s\uff08\u542bLLM\u7279\u5f81\u63d0\u53d6\uff09\uff1b2) \u6bd4Linux ondemand governor\u63d0\u53477.09\u500d\u80fd\u6548\u548c4.0\u500dmakespan\uff1b3) \u9996\u6b21\u51b3\u7b56\u6bd4\u57fa\u4e8e\u8868\u683c\u7684\u5206\u6790\u5feb8300\u500d\uff1b4) \u6536\u655b\u901f\u5ea6\u6bd4\u65e0\u6a21\u578b\u65b9\u6cd5\u5feb20\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u548cLLM\u8bed\u4e49\u7279\u5f81\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u90e8\u7f72\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6838\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u8c03\u5ea6\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u52a8\u6001\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08105", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08105", "abs": "https://arxiv.org/abs/2601.08105", "authors": ["Fabian Spaeh", "Tianyi Chen", "Chen-Hao Chiang", "Bin Shen"], "title": "Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning", "comment": null, "summary": "Retrieval-augmented generation with tool-calling agents (agentic RAG) has become increasingly powerful in understanding, processing, and responding to user queries. However, the scope of the grounding knowledge is limited and asking questions that exceed this scope may lead to issues like hallucination. While guardrail frameworks aim to block out-of-scope questions (Rodriguez et al., 2024), no research has investigated the question of suggesting answerable queries in order to complete the user interaction.\n  In this paper, we initiate the study of query suggestion for agentic RAG. We consider the setting where user questions are not answerable, and the suggested queries should be similar to aid the user interaction. Such scenarios are frequent for tool-calling LLMs as communicating the restrictions of the tools or the underlying datasets to the user is difficult, and adding query suggestions enhances the interaction with the RAG agent. As opposed to traditional settings for query recommendations such as in search engines, ensuring that the suggested queries are answerable is a major challenge due to the RAG's multi-step workflow that demands a nuanced understanding of the RAG as a whole, which the executing LLM lacks. As such, we introduce robust dynamic few-shot learning which retrieves examples from relevant workflows. We show that our system can be self-learned, for instance on prior user queries, and is therefore easily applicable in practice. We evaluate our approach on three benchmark datasets based on two unlabeled question datasets collected from real-world user queries. Experiments on real-world datasets confirm that our method produces more relevant and answerable suggestions, outperforming few-shot and retrieval-only baselines, and thus enable safer, more effective user interaction with agentic RAG.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4ee3\u7406RAG\u7cfb\u7edf\u4e2d\u7684\u67e5\u8be2\u5efa\u8bae\u95ee\u9898\uff0c\u5f53\u7528\u6237\u95ee\u9898\u8d85\u51fa\u7cfb\u7edf\u77e5\u8bc6\u8303\u56f4\u65f6\uff0c\u7cfb\u7edf\u80fd\u591f\u5efa\u8bae\u53ef\u56de\u7b54\u7684\u76f8\u4f3c\u67e5\u8be2\uff0c\u4ee5\u589e\u5f3a\u7528\u6237\u4ea4\u4e92\u4f53\u9a8c\u3002", "motivation": "\u4ee3\u7406RAG\u7cfb\u7edf\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u5176\u77e5\u8bc6\u8303\u56f4\u6709\u9650\uff0c\u8d85\u51fa\u8303\u56f4\u7684\u95ee\u9898\u53ef\u80fd\u5bfc\u81f4\u5e7b\u89c9\u3002\u73b0\u6709\u62a4\u680f\u6846\u67b6\u53ea\u963b\u6b62\u8d85\u51fa\u8303\u56f4\u7684\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u4e3a\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\u63d0\u4f9b\u53ef\u56de\u7b54\u67e5\u8be2\u5efa\u8bae\u7684\u7814\u7a76\uff0c\u8fd9\u5728\u5b9e\u9645\u5de5\u5177\u8c03\u7528LLM\u573a\u666f\u4e2d\u5f88\u5e38\u89c1\u3002", "method": "\u63d0\u51fa\u9c81\u68d2\u52a8\u6001\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u76f8\u5173\u5de5\u4f5c\u6d41\u4e2d\u68c0\u7d22\u793a\u4f8b\uff0c\u7cfb\u7edf\u53ef\u4ee5\u81ea\u6211\u5b66\u4e60\uff08\u5982\u57fa\u4e8e\u5148\u524d\u7528\u6237\u67e5\u8be2\uff09\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7528\u6237\u67e5\u8be2\u6570\u636e\u96c6\u6784\u5efa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5c11\u6837\u672c\u548c\u4ec5\u68c0\u7d22\u57fa\u7ebf\uff0c\u80fd\u4ea7\u751f\u66f4\u76f8\u5173\u4e14\u53ef\u56de\u7b54\u7684\u67e5\u8be2\u5efa\u8bae\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u6709\u6548\u7684\u4ee3\u7406RAG\u7528\u6237\u4ea4\u4e92\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u67e5\u8be2\u63a8\u8350\u5728RAG\u591a\u6b65\u5de5\u4f5c\u6d41\u4e2d\u786e\u4fdd\u53ef\u56de\u7b54\u6027\u7684\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2601.08173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08173", "abs": "https://arxiv.org/abs/2601.08173", "authors": ["Daocheng Fu", "Jianbiao Mei", "Rong Wu", "Xuemeng Yang", "Jia Xu", "Ding Wang", "Pinlong Cai", "Yong Liu", "Licheng Wen", "Botian Shi"], "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios", "comment": null, "summary": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \\method{}, a dynamic evaluation environment that simulates a \"trainee\" agent continuously exploring a novel setting. Unlike traditional benchmarks, \\method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86EvoEnv\u52a8\u6001\u8bc4\u4f30\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u52a8\u6001\u3001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u4efb\u52a1\u8c03\u5ea6\u3001\u4e3b\u52a8\u63a2\u7d22\u548c\u6301\u7eed\u5b66\u4e60\u4e09\u4e2a\u7ef4\u5ea6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4e0a\u9650\uff0c\u5ffd\u89c6\u4e86\u968f\u673a\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u9c81\u68d2\u6027\u3002\u4f5c\u8005\u8bc6\u522b\u4e86\u52a8\u6001\u4efb\u52a1\u8c03\u5ea6\u3001\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u4e3b\u52a8\u63a2\u7d22\u4ee5\u53ca\u4ece\u7ecf\u9a8c\u4e2d\u6301\u7eed\u5b66\u4e60\u4e09\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86EvoEnv\u52a8\u6001\u8bc4\u4f30\u73af\u5883\uff0c\u6a21\u62df\"\u53d7\u8bad\"\u4ee3\u7406\u5728\u65b0\u578b\u8bbe\u7f6e\u4e2d\u6301\u7eed\u63a2\u7d22\u3002\u8bc4\u4f30\u4e09\u4e2a\u7ef4\u5ea6\uff1a1) \u9488\u5bf9\u4e0d\u540c\u4f18\u5148\u7ea7\u6d41\u5f0f\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u5ea6\uff1b2) \u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u51cf\u5c11\u5e7b\u89c9\u7684\u8c28\u614e\u4fe1\u606f\u83b7\u53d6\uff1b3) \u4ece\u57fa\u4e8e\u89c4\u5219\u7684\u52a8\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u63d0\u70bc\u901a\u7528\u7b56\u7565\u7684\u6301\u7eed\u6f14\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5728\u4e3b\u52a8\u63a2\u7d22\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u9762\u3002\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u8bc4\u4f30\u4ee3\u7406\u53ef\u9760\u6027\u7684\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u8bc4\u4f30\u4ece\u9759\u6001\u6d4b\u8bd5\u8f6c\u5411\u73b0\u5b9e\u3001\u9762\u5411\u751f\u4ea7\u7684\u573a\u666f\uff0c\u4e3a\u8bc4\u4f30\u4ee3\u7406\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.08235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08235", "abs": "https://arxiv.org/abs/2601.08235", "authors": ["Shouju Wang", "Haopeng Zhang"], "title": "MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents", "comment": "Submitted to ACL 2026", "summary": "As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.", "AI": {"tldr": "MPCI-Bench\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u9690\u79c1\u884c\u4e3a\u7684\u591a\u6a21\u6001\u914d\u5bf9\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u57fa\u51c6\uff0c\u5305\u542b\u4e09\u4e2a\u5c42\u7ea7\u7684\u6570\u636e\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u5728\u5e73\u8861\u9690\u79c1\u4e0e\u6548\u7528\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u5931\u8d25\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u4ece\u88ab\u52a8\u804a\u5929\u673a\u5668\u4eba\u6f14\u53d8\u4e3a\u5904\u7406\u4e2a\u4eba\u6570\u636e\u7684\u4e3b\u52a8\u52a9\u624b\uff0c\u8bc4\u4f30\u5176\u9075\u5b88\u793e\u4f1a\u89c4\u8303\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u57fa\u51c6\u4e3b\u8981\u662f\u6587\u672c\u4e2d\u5fc3\u4e14\u4fa7\u91cd\u4e8e\u8d1f\u9762\u62d2\u7edd\u573a\u666f\uff0c\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u9690\u79c1\u98ce\u9669\u548c\u9690\u79c1\u4e0e\u6548\u7528\u7684\u57fa\u672c\u6743\u8861\u3002", "method": "\u63d0\u51faMPCI-Bench\u57fa\u51c6\uff0c\u5305\u542b\u4ece\u540c\u4e00\u89c6\u89c9\u6e90\u6d3e\u751f\u7684\u914d\u5bf9\u6b63\u8d1f\u5b9e\u4f8b\uff0c\u5206\u4e3a\u4e09\u4e2a\u5c42\u7ea7\uff1a\u89c4\u8303\u6027\u79cd\u5b50\u5224\u65ad\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u6545\u4e8b\u63a8\u7406\u548c\u53ef\u6267\u884c\u7684\u667a\u80fd\u4f53\u884c\u52a8\u8f68\u8ff9\u3002\u901a\u8fc7\u4e09\u539f\u5219\u8fed\u4ee3\u7cbe\u70bc\u6d41\u7a0b\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u591a\u6a21\u6001\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff1a1\uff09\u6a21\u578b\u5728\u5e73\u8861\u9690\u79c1\u4e0e\u6548\u7528\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\uff1b2\uff09\u5b58\u5728\u660e\u663e\u7684\u6a21\u6001\u6cc4\u6f0f\u5dee\u8ddd\uff0c\u654f\u611f\u89c6\u89c9\u4fe1\u606f\u6bd4\u6587\u672c\u4fe1\u606f\u66f4\u9891\u7e41\u5730\u88ab\u6cc4\u9732\u3002", "conclusion": "MPCI-Bench\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u9690\u79c1\u884c\u4e3a\u7684\u591a\u6a21\u6001\u914d\u5bf9\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u57fa\u51c6\uff0c\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u672a\u6765\u5173\u4e8e\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u7684\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2601.08237", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08237", "abs": "https://arxiv.org/abs/2601.08237", "authors": ["Haoran Su", "Yandong Sun", "Congjia Yu"], "title": "The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination", "comment": null, "summary": "Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3b\u5f20\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u4ece\u4f20\u7edf\u7684\u624b\u5de5\u8bbe\u8ba1\u6570\u503c\u5956\u52b1\u51fd\u6570\u8f6c\u5411\u57fa\u4e8e\u8bed\u8a00\u7684\u76ee\u6807\u89c4\u8303\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u8bed\u4e49\u5956\u52b1\u89c4\u8303\u548c\u52a8\u6001\u5956\u52b1\u9002\u5e94\uff0c\u4ee5\u6539\u5584\u4e0e\u4eba\u7c7b\u610f\u56fe\u7684\u5bf9\u9f50\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u5de5\u7a0b\u9762\u4e34\u4fe1\u7528\u5206\u914d\u6a21\u7cca\u3001\u73af\u5883\u975e\u5e73\u7a33\u6027\u548c\u4ea4\u4e92\u590d\u6742\u6027\u7ec4\u5408\u589e\u957f\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u624b\u5de5\u8bbe\u8ba1\u6570\u503c\u5956\u52b1\u51fd\u6570\u7684\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5408\u6210\u5956\u52b1\u51fd\u6570\uff08\u5982EUREKA\uff09\uff0c\u5e76\u5728\u7ebf\u9002\u5e94\u5956\u52b1\u516c\u5f0f\uff08\u5982CARD\uff09\uff0c\u7ed3\u5408\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u5b9e\u73b0\u8bed\u4e49\u5956\u52b1\u89c4\u8303\u3001\u52a8\u6001\u5956\u52b1\u9002\u5e94\u548c\u4eba\u7c7b\u610f\u56fe\u5bf9\u9f50\u3002", "result": "\u8bed\u8a00\u4ecb\u5bfc\u7684\u76d1\u7763\u53ef\u4ee5\u4f5c\u4e3a\u4f20\u7edf\u5956\u52b1\u5de5\u7a0b\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u534f\u8c03\u53ef\u4ee5\u4ece\u5171\u4eab\u8bed\u4e49\u8868\u793a\u800c\u975e\u663e\u5f0f\u8bbe\u8ba1\u7684\u6570\u503c\u4fe1\u53f7\u4e2d\u4ea7\u751f\u3002", "conclusion": "\u9700\u8981\u7814\u7a76\u4ece\u624b\u5de5\u8bbe\u8ba1\u7684\u6570\u503c\u5956\u52b1\u8f6c\u5411\u57fa\u4e8e\u8bed\u8a00\u7684\u76ee\u6807\u89c4\u8303\uff0c\u540c\u65f6\u89e3\u51b3\u8ba1\u7b97\u5f00\u9500\u3001\u5e7b\u89c9\u9c81\u68d2\u6027\u548c\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\u7b49\u5f00\u653e\u6311\u6218\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08158", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08158", "abs": "https://arxiv.org/abs/2601.08158", "authors": ["Yuqing Zhou", "Zhuoer Wang", "Jie Yuan", "Hong Wang", "Samson Koelle", "Ziwei Zhu", "Wei Niu"], "title": "WISE-Flow: Workflow-Induced Structured Experience for Self-Evolving Conversational Service Agents", "comment": "19 pages", "summary": "Large language model (LLM)-based agents are widely deployed in user-facing services but remain error-prone in new tasks, tend to repeat the same failure patterns, and show substantial run-to-run variability. Fixing failures via environment-specific training or manual patching is costly and hard to scale. To enable self-evolving agents in user-facing service environments, we propose WISE-Flow, a workflow-centric framework that converts historical service interactions into reusable procedural experience by inducing workflows with prerequisite-augmented action blocks. At deployment, WISE-Flow aligns the agent's execution trajectory to retrieved workflows and performs prerequisite-aware feasibility reasoning to achieve state-grounded next actions. Experiments on ToolSandbox and $\u03c4^2$-bench show consistent improvement across base models.", "AI": {"tldr": "WISE-Flow\u662f\u4e00\u4e2a\u5de5\u4f5c\u6d41\u4e2d\u5fc3\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5386\u53f2\u670d\u52a1\u4ea4\u4e92\u4e2d\u63d0\u53d6\u53ef\u91cd\u7528\u7a0b\u5e8f\u7ecf\u9a8c\uff0c\u4f7fLLM\u4ee3\u7406\u80fd\u591f\u81ea\u6211\u8fdb\u5316\uff0c\u63d0\u5347\u5728\u7528\u6237\u670d\u52a1\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u7528\u6237\u670d\u52a1\u4e2d\u5e7f\u6cdb\u90e8\u7f72\u4f46\u5b58\u5728\u9519\u8bef\u503e\u5411\u3001\u91cd\u590d\u5931\u8d25\u6a21\u5f0f\u548c\u8fd0\u884c\u53d8\u5f02\u6027\u5927\u7684\u95ee\u9898\uff0c\u800c\u901a\u8fc7\u73af\u5883\u7279\u5b9a\u8bad\u7ec3\u6216\u624b\u52a8\u4fee\u8865\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u63d0\u51faWISE-Flow\u6846\u67b6\uff1a1) \u5c06\u5386\u53f2\u670d\u52a1\u4ea4\u4e92\u8f6c\u6362\u4e3a\u53ef\u91cd\u7528\u7a0b\u5e8f\u7ecf\u9a8c\uff0c\u901a\u8fc7\u524d\u63d0\u6761\u4ef6\u589e\u5f3a\u7684\u52a8\u4f5c\u5757\u8bf1\u5bfc\u5de5\u4f5c\u6d41\uff1b2) \u90e8\u7f72\u65f6\u5bf9\u9f50\u4ee3\u7406\u6267\u884c\u8f68\u8ff9\u5230\u68c0\u7d22\u5230\u7684\u5de5\u4f5c\u6d41\uff1b3) \u6267\u884c\u524d\u63d0\u6761\u4ef6\u611f\u77e5\u7684\u53ef\u884c\u6027\u63a8\u7406\u4ee5\u5b9e\u73b0\u72b6\u6001\u63a5\u5730\u7684\u4e0b\u4e00\u6b65\u52a8\u4f5c\u3002", "result": "\u5728ToolSandbox\u548c\u03c4\u00b2-bench\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u4e0a\u90fd\u80fd\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "WISE-Flow\u80fd\u591f\u4f7f\u4ee3\u7406\u5728\u7528\u6237\u670d\u52a1\u73af\u5883\u4e2d\u81ea\u6211\u8fdb\u5316\uff0c\u901a\u8fc7\u5de5\u4f5c\u6d41\u91cd\u7528\u548c\u524d\u63d0\u6761\u4ef6\u63a8\u7406\u6709\u6548\u89e3\u51b3LLM\u4ee3\u7406\u7684\u5931\u8d25\u6a21\u5f0f\u548c\u53d8\u5f02\u6027\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.08254", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.08254", "abs": "https://arxiv.org/abs/2601.08254", "authors": ["Abdikarim Mohamed Ibrahim", "Rosdiadee Nordin"], "title": "Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks", "comment": null, "summary": "Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u7528\u4e8e\u975e\u5730\u9762\u7f51\u7edc\u4f18\u5316\uff0c\u76f8\u6bd4\u4f20\u7edfDRL\u5728\u541e\u5410\u91cf\u3001\u516c\u5e73\u6027\u548c\u4e2d\u65ad\u6982\u7387\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347", "motivation": "\u5927\u578bAI\u6a21\u578b\u5728\u975e\u5730\u9762\u7f51\u7edc\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u5c11\u7684\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u9700\u6c42\u3002\u4f20\u7edfDRL\u65b9\u6cd5\u5728\u590d\u6742\u7f51\u7edc\u73af\u5883\u4e2d\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u5f15\u5bfc\u673a\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7531LLM\u5f15\u5bfc\u7684DRL\u667a\u80fd\u4f53\uff0cLLM\u4f5c\u4e3a\u9ad8\u5c42\u534f\u8c03\u5668\u751f\u6210\u6587\u672c\u6307\u5bfc\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5851\u9020DRL\u667a\u80fd\u4f53\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5f62\u6210LAM-DRL\u6846\u67b6\u3002", "result": "LAM-DRL\u5728\u6b63\u5e38\u5929\u6c14\u573a\u666f\u4e0b\u6bd4\u4f20\u7edfDRL\u63d0\u534740%\u6027\u80fd\uff0c\u5728\u6781\u7aef\u5929\u6c14\u573a\u666f\u4e0b\u63d0\u534764%\u6027\u80fd\uff08\u76f8\u6bd4\u542f\u53d1\u5f0f\u65b9\u6cd5\uff09\uff0c\u5728\u541e\u5410\u91cf\u3001\u516c\u5e73\u6027\u548c\u4e2d\u65ad\u6982\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u6539\u5584\u3002", "conclusion": "LLM\u5f15\u5bfc\u7684DRL\u65b9\u6cd5\u5728\u975e\u5730\u9762\u7f51\u7edc\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9ad8\u5c42\u534f\u8c03\u5668\u80fd\u591f\u6709\u6548\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08160", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08160", "abs": "https://arxiv.org/abs/2601.08160", "authors": ["Anxin Tian", "Yiming Li", "Xing Li", "Hui-Ling Zhen", "Lei Chen", "Xianzhi Yu", "Zhenhua Dong", "Mingxuan Yuan"], "title": "SwiftMem: Fast Agentic Memory via Query-aware Indexing", "comment": null, "summary": "Agentic memory systems have become critical for enabling LLM agents to maintain long-term context and retrieve relevant information efficiently. However, existing memory frameworks suffer from a fundamental limitation: they perform exhaustive retrieval across the entire storage layer regardless of query characteristics. This brute-force approach creates severe latency bottlenecks as memory grows, hindering real-time agent interactions. We propose SwiftMem, a query-aware agentic memory system that achieves sub-linear retrieval through specialized indexing over temporal and semantic dimensions. Our temporal index enables logarithmic-time range queries for time-sensitive retrieval, while the semantic DAG-Tag index maps queries to relevant topics through hierarchical tag structures. To address memory fragmentation during growth, we introduce an embedding-tag co-consolidation mechanism that reorganizes storage based on semantic clusters to improve cache locality. Experiments on LoCoMo and LongMemEval benchmarks demonstrate that SwiftMem achieves 47$\\times$ faster search compared to state-of-the-art baselines while maintaining competitive accuracy, enabling practical deployment of memory-augmented LLM agents.", "AI": {"tldr": "SwiftMem\u662f\u4e00\u4e2a\u67e5\u8be2\u611f\u77e5\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u65f6\u95f4\u7d22\u5f15\u548c\u8bed\u4e49DAG-Tag\u7d22\u5f15\u5b9e\u73b0\u4e9a\u7ebf\u6027\u68c0\u7d22\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u5168\u91cf\u68c0\u7d22\u5bfc\u81f4\u7684\u5ef6\u8fdf\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff1a\u65e0\u8bba\u67e5\u8be2\u7279\u6027\u5982\u4f55\uff0c\u90fd\u4f1a\u5bf9\u6574\u4e2a\u5b58\u50a8\u5c42\u8fdb\u884c\u7a77\u4e3e\u68c0\u7d22\u3002\u8fd9\u79cd\u66b4\u529b\u65b9\u6cd5\u968f\u7740\u8bb0\u5fc6\u589e\u957f\u4f1a\u9020\u6210\u4e25\u91cd\u7684\u5ef6\u8fdf\u74f6\u9888\uff0c\u963b\u788d\u5b9e\u65f6\u667a\u80fd\u4f53\u4ea4\u4e92\u3002", "method": "\u63d0\u51faSwiftMem\u7cfb\u7edf\uff0c\u5305\u542b\uff1a1\uff09\u65f6\u95f4\u7d22\u5f15\u5b9e\u73b0\u5bf9\u6570\u65f6\u95f4\u8303\u56f4\u67e5\u8be2\uff1b2\uff09\u8bed\u4e49DAG-Tag\u7d22\u5f15\u901a\u8fc7\u5206\u5c42\u6807\u7b7e\u7ed3\u6784\u5c06\u67e5\u8be2\u6620\u5c04\u5230\u76f8\u5173\u4e3b\u9898\uff1b3\uff09\u5d4c\u5165-\u6807\u7b7e\u534f\u540c\u6574\u5408\u673a\u5236\uff0c\u57fa\u4e8e\u8bed\u4e49\u805a\u7c7b\u91cd\u7ec4\u5b58\u50a8\u4ee5\u63d0\u9ad8\u7f13\u5b58\u5c40\u90e8\u6027\u3002", "result": "\u5728LoCoMo\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSwiftMem\u5b9e\u73b0\u4e86\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5feb47\u500d\u7684\u641c\u7d22\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u5ea6\uff0c\u4f7f\u8bb0\u5fc6\u589e\u5f3a\u7684LLM\u667a\u80fd\u4f53\u80fd\u591f\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "SwiftMem\u901a\u8fc7\u67e5\u8be2\u611f\u77e5\u7684\u7d22\u5f15\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u7684\u68c0\u7d22\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u667a\u80fd\u4f53\u4ea4\u4e92\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.08258", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08258", "abs": "https://arxiv.org/abs/2601.08258", "authors": ["Edward Y. Chang"], "title": "T3: Benchmarking Sycophancy and Skepticism in Causal Judgment", "comment": "17 pages, 4 figures, 11 tables", "summary": "We introduce T3 (Testing Trustworthy Thinking), a diagnostic benchmark designed to rigorously evaluate LLM causal judgment across Pearl's Ladder of Causality. Comprising 454 expert-curated vignettes, T3 prioritizes high-resolution failure analysis, decomposing performance into Utility (sensitivity), Safety (specificity), and Wise Refusal on underdetermined cases. By applying T3 to frontier models, we diagnose two distinct pathologies: a \"Skepticism Trap\" at L1 (where safety-tuned models like Claude Haiku reject 60% of valid links) and a non-monotonic Scaling Paradox at L3. In the latter, the larger GPT-5.2 underperforms GPT-4-Turbo by 55 points on ambiguous counterfactuals, driven by a collapse into paralysis (excessive hedging) rather than hallucination. Finally, we use the benchmark to validate a process-verified protocol (RCA), showing that T3 successfully captures the restoration of decisive causal judgment under structured verification.", "AI": {"tldr": "T3\u662f\u4e00\u4e2a\u8bca\u65ad\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728Pearl\u56e0\u679c\u9636\u68af\u4e0a\u7684\u56e0\u679c\u5224\u65ad\u80fd\u529b\uff0c\u5305\u542b454\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u5c0f\u6545\u4e8b\uff0c\u901a\u8fc7\u6548\u7528\u3001\u5b89\u5168\u548c\u660e\u667a\u62d2\u7edd\u4e09\u4e2a\u7ef4\u5ea6\u5206\u6790\u6a21\u578b\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u5b89\u5168\u8c03\u4f18\u6a21\u578b\u7684\"\u6000\u7591\u9677\u9631\"\u548c\u5927\u578b\u6a21\u578b\u7684\"\u7f29\u653e\u6096\u8bba\"\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30LLM\u56e0\u679c\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u7279\u522b\u662f\u9700\u8981\u80fd\u591f\u8bca\u65ad\u6a21\u578b\u5728\u56e0\u679c\u9636\u68af\u4e0d\u540c\u5c42\u7ea7\u4e0a\u7684\u5177\u4f53\u5931\u8d25\u6a21\u5f0f\uff0c\u4ee5\u7406\u89e3\u5b89\u5168\u8c03\u4f18\u548c\u6a21\u578b\u7f29\u653e\u5bf9\u56e0\u679c\u5224\u65ad\u7684\u5f71\u54cd\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b454\u4e2a\u4e13\u5bb6\u7b56\u5212\u5c0f\u6545\u4e8b\u7684T3\u57fa\u51c6\uff0c\u5c06\u6027\u80fd\u5206\u89e3\u4e3a\u6548\u7528\uff08\u654f\u611f\u6027\uff09\u3001\u5b89\u5168\uff08\u7279\u5f02\u6027\uff09\u548c\u660e\u667a\u62d2\u7edd\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u8bc4\u4f30\u6a21\u578b\u5728Pearl\u56e0\u679c\u9636\u68af\u4e09\u4e2a\u5c42\u7ea7\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5e94\u7528\u8be5\u57fa\u51c6\u8bca\u65ad\u524d\u6cbf\u6a21\u578b\u7684\u75c5\u7406\u73b0\u8c61\u3002", "result": "\u53d1\u73b0\u4e24\u79cd\u75c5\u7406\u73b0\u8c61\uff1a1\uff09L1\u5c42\u7684\"\u6000\u7591\u9677\u9631\"\uff1a\u5b89\u5168\u8c03\u4f18\u6a21\u578b\uff08\u5982Claude Haiku\uff09\u62d2\u7edd60%\u7684\u6709\u6548\u56e0\u679c\u94fe\u63a5\uff1b2\uff09L3\u5c42\u7684\u975e\u5355\u8c03\"\u7f29\u653e\u6096\u8bba\"\uff1a\u66f4\u5927\u7684GPT-5.2\u5728\u6a21\u7cca\u53cd\u4e8b\u5b9e\u63a8\u7406\u4e0a\u6bd4GPT-4-Turbo\u5dee55\u5206\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8fc7\u5ea6\u72b9\u8c6b\u800c\u975e\u5e7b\u89c9\u3002\u901a\u8fc7\u57fa\u51c6\u9a8c\u8bc1\u4e86\u8fc7\u7a0b\u9a8c\u8bc1\u534f\u8bae\uff08RCA\uff09\u80fd\u591f\u6062\u590d\u51b3\u5b9a\u6027\u56e0\u679c\u5224\u65ad\u3002", "conclusion": "T3\u57fa\u51c6\u80fd\u591f\u6709\u6548\u8bca\u65adLLM\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u5b89\u5168\u8c03\u4f18\u548c\u6a21\u578b\u7f29\u653e\u5bf9\u56e0\u679c\u5224\u65ad\u7684\u590d\u6742\u5f71\u54cd\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u56e0\u679c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8bca\u65ad\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.08169", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08169", "abs": "https://arxiv.org/abs/2601.08169", "authors": ["Andrea Kang", "Yingnian Wu", "Hongjing Lu"], "title": "Relational Knowledge Distillation Using Fine-tuned Function Vectors", "comment": null, "summary": "Representing relations between concepts is a core prerequisite for intelligent systems to make sense of the world. Recent work using causal mediation analysis has shown that a small set of attention heads encodes task representation in in-context learning, captured in a compact representation known as the function vector. We show that fine-tuning function vectors with only a small set of examples (about 20 word pairs) yields better performance on relation-based word-completion tasks than using the original vectors derived from causal mediation analysis. These improvements hold for both small and large language models. Moreover, the fine-tuned function vectors yield improved decoding performance for relation words and show stronger alignment with human similarity judgments of semantic relations. Next, we introduce the composite function vector - a weighted combination of fine-tuned function vectors - to extract relational knowledge and support analogical reasoning. At inference time, inserting this composite vector into LLM activations markedly enhances performance on challenging analogy problems drawn from cognitive science and SAT benchmarks. Our results highlight the potential of activation patching as a controllable mechanism for encoding and manipulating relational knowledge, advancing both the interpretability and reasoning capabilities of large language models.", "AI": {"tldr": "\u901a\u8fc7\u5fae\u8c03\u51fd\u6570\u5411\u91cf\uff08\u7ea620\u4e2a\u8bcd\u5bf9\uff09\u63d0\u5347\u5173\u7cfb\u8868\u793a\u80fd\u529b\uff0c\u5f15\u5165\u590d\u5408\u51fd\u6570\u5411\u91cf\u589e\u5f3a\u7c7b\u6bd4\u63a8\u7406\uff0c\u6fc0\u6d3b\u4fee\u8865\u4f5c\u4e3a\u53ef\u63a7\u673a\u5236\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5173\u7cfb\u77e5\u8bc6\u7f16\u7801\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u6982\u5ff5\u95f4\u5173\u7cfb\u8868\u793a\u662f\u667a\u80fd\u7cfb\u7edf\u7406\u89e3\u4e16\u754c\u7684\u6838\u5fc3\u524d\u63d0\u3002\u73b0\u6709\u56e0\u679c\u4e2d\u4ecb\u5206\u6790\u53d1\u73b0\u5c11\u91cf\u6ce8\u610f\u529b\u5934\u7f16\u7801\u4efb\u52a1\u8868\u793a\uff08\u51fd\u6570\u5411\u91cf\uff09\uff0c\u4f46\u5982\u4f55\u66f4\u597d\u5730\u63d0\u53d6\u548c\u5229\u7528\u8fd9\u4e9b\u5173\u7cfb\u77e5\u8bc6\u4ecd\u9700\u63a2\u7d22\u3002", "method": "1. \u4f7f\u7528\u5c11\u91cf\u793a\u4f8b\uff08\u7ea620\u4e2a\u8bcd\u5bf9\uff09\u5fae\u8c03\u51fd\u6570\u5411\u91cf\uff1b2. \u5f15\u5165\u590d\u5408\u51fd\u6570\u5411\u91cf\uff08\u5fae\u8c03\u5411\u91cf\u7684\u52a0\u6743\u7ec4\u5408\uff09\u63d0\u53d6\u5173\u7cfb\u77e5\u8bc6\uff1b3. \u5728\u63a8\u7406\u65f6\u5c06\u590d\u5408\u5411\u91cf\u63d2\u5165LLM\u6fc0\u6d3b\u4e2d\u589e\u5f3a\u7c7b\u6bd4\u63a8\u7406\u3002", "result": "1. \u5fae\u8c03\u51fd\u6570\u5411\u91cf\u5728\u5173\u7cfb\u8bcd\u8865\u5168\u4efb\u52a1\u4e0a\u4f18\u4e8e\u539f\u59cb\u5411\u91cf\uff08\u5927\u5c0f\u6a21\u578b\u5747\u6709\u6548\uff09\uff1b2. \u63d0\u5347\u5173\u7cfb\u8bcd\u89e3\u7801\u6027\u80fd\uff1b3. \u4e0e\u4eba\u7c7b\u8bed\u4e49\u5173\u7cfb\u76f8\u4f3c\u6027\u5224\u65ad\u66f4\u4e00\u81f4\uff1b4. \u590d\u5408\u5411\u91cf\u663e\u8457\u63d0\u5347\u8ba4\u77e5\u79d1\u5b66\u548cSAT\u7c7b\u6bd4\u95ee\u9898\u6027\u80fd\u3002", "conclusion": "\u6fc0\u6d3b\u4fee\u8865\u4f5c\u4e3a\u53ef\u63a7\u673a\u5236\u80fd\u6709\u6548\u7f16\u7801\u548c\u64cd\u4f5c\u5173\u7cfb\u77e5\u8bc6\uff0c\u540c\u65f6\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.08271", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08271", "abs": "https://arxiv.org/abs/2601.08271", "authors": ["Angshul Majumdar"], "title": "Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces", "comment": null, "summary": "Tool-augmented LLM systems expose a control regime that learning theory has largely ignored: sequential decision-making with a massive discrete action universe (tools, APIs, documents) in which only a small, unknown subset is relevant for any fixed task distribution. We formalize this setting as Sparse Agentic Control (SAC), where policies admit block-sparse representations over M >> 1 actions and rewards depend on sparse main effects and (optionally) sparse synergies. We study ell_{1,2}-regularized policy learning through a convex surrogate and establish sharp, compressed-sensing-style results: (i) estimation and value suboptimality scale as k (log M / T)^{1/2} under a Policy-RSC condition; (ii) exact tool-support recovery holds via primal-dual witness arguments when T > k log M under incoherence and beta-min; and (iii) any dense policy class requires Omega(M) samples, explaining the instability of prompt-only controllers. We further show that under partial observability, LLMs matter only through a belief/representation error epsilon_b, yielding an additive O(epsilon_b) degradation while preserving logarithmic dependence on M. Extensions cover tuning-free, online, robust, group-sparse, and interaction-aware SAC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7a00\u758f\u4ee3\u7406\u63a7\u5236(SAC)\u6846\u67b6\uff0c\u5206\u6790\u5de5\u5177\u589e\u5f3aLLM\u7cfb\u7edf\u4e2d\u7684\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\uff0c\u8bc1\u660e\u5728M\u4e2a\u5de5\u5177\u4e2d\u53ea\u6709k\u4e2a\u76f8\u5173\u65f6\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e3aO(k log M)\uff0c\u800c\u975eO(M)\uff0c\u89e3\u91ca\u4e86\u7eaf\u63d0\u793a\u63a7\u5236\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "motivation": "\u5de5\u5177\u589e\u5f3aLLM\u7cfb\u7edf\u9762\u4e34\u5927\u89c4\u6a21\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\uff08\u5de5\u5177\u3001API\u3001\u6587\u6863\uff09\u7684\u51b3\u7b56\u95ee\u9898\uff0c\u5176\u4e2d\u53ea\u6709\u5c11\u91cf\u672a\u77e5\u5b50\u96c6\u4e0e\u7279\u5b9a\u4efb\u52a1\u76f8\u5173\u3002\u73b0\u6709\u5b66\u4e60\u7406\u8bba\u4e3b\u8981\u5ffd\u7565\u8fd9\u79cd\u63a7\u5236\u673a\u5236\uff0c\u9700\u8981\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790\u6b64\u7c7b\u7cfb\u7edf\u7684\u6837\u672c\u590d\u6742\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u7a00\u758f\u4ee3\u7406\u63a7\u5236(SAC)\uff0c\u4f7f\u7528\u2113\u2081,\u2082\u6b63\u5219\u5316\u7b56\u7565\u5b66\u4e60\uff0c\u901a\u8fc7\u51f8\u4ee3\u7406\u65b9\u6cd5\u5efa\u7acb\u538b\u7f29\u611f\u77e5\u98ce\u683c\u7684\u7406\u8bba\u7ed3\u679c\u3002\u5206\u6790\u5305\u62ec\u7b56\u7565RSC\u6761\u4ef6\u3001\u539f\u59cb-\u5bf9\u5076\u89c1\u8bc1\u53c2\u6570\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u7684\u8868\u793a\u8bef\u5dee\u7b49\u3002", "result": "\u8bc1\u660e\uff1a1) \u4f30\u8ba1\u548c\u503c\u6b21\u4f18\u6027\u6309k(log M/T)^{1/2}\u7f29\u653e\uff1b2) \u5728T > k log M\u65f6\u901a\u8fc7\u539f\u59cb-\u5bf9\u5076\u89c1\u8bc1\u5b9e\u73b0\u7cbe\u786e\u5de5\u5177\u652f\u6301\u6062\u590d\uff1b3) \u4efb\u4f55\u5bc6\u96c6\u7b56\u7565\u7c7b\u90fd\u9700\u8981\u03a9(M)\u6837\u672c\uff0c\u89e3\u91ca\u4e86\u7eaf\u63d0\u793a\u63a7\u5236\u5668\u7684\u4e0d\u7a33\u5b9a\u6027\uff1b4) \u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0bLLM\u4ec5\u901a\u8fc7\u4fe1\u5ff5\u8bef\u5dee\u03b5_b\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "SAC\u6846\u67b6\u4e3a\u5de5\u5177\u589e\u5f3aLLM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u7a00\u758f\u6b63\u5219\u5316\u80fd\u5b9e\u73b0\u5bf9\u6570\u7ea7\u6837\u672c\u590d\u6742\u5ea6\uff0c\u800c\u5bc6\u96c6\u65b9\u6cd5\u9700\u8981\u7ebf\u6027\u6837\u672c\u91cf\u3002\u8fd9\u4e3a\u8bbe\u8ba1\u7a33\u5b9a\u9ad8\u6548\u7684\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.08276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08276", "abs": "https://arxiv.org/abs/2601.08276", "authors": ["Zhiyuan Yao", "Zishan Xu", "Yifu Guo", "Zhiguang Han", "Cheng Yang", "Shuo Zhang", "Weinan Zhang", "Xingshan Zeng", "Weiwen Liu"], "title": "ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web", "comment": null, "summary": "With the rise of the Agent Web and Model Context Protocol (MCP), the agent ecosystem is evolving into an open collaborative network, exponentially increasing accessible tools. However, current architectures face severe scalability and generality bottlenecks. To address this, we propose ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph to synthesize multi-turn trajectories, we effectively train routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Experiments on the real-world benchmarks MCP-Universe and MCP-Mark demonstrate superior performance. Notably, ToolACE-MCP exhibits critical properties for the future Agent Web: it not only generalizes to multi-agent collaboration with minimal adaptation but also maintains exceptional robustness against noise and scales effectively to massive candidate spaces. These findings provide a strong empirical foundation for universal orchestration in open-ended ecosystems.", "AI": {"tldr": "ToolACE-MCP\uff1a\u4e00\u4e2a\u8bad\u7ec3\u5386\u53f2\u611f\u77e5\u8def\u7531\u5668\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u5728\u5927\u89c4\u6a21\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u7cbe\u786e\u5bfc\u822a\uff0c\u901a\u8fc7\u4f9d\u8d56\u4e30\u5bcc\u7684\u5019\u9009\u56fe\u5408\u6210\u591a\u8f6e\u8f68\u8ff9\u6765\u8bad\u7ec3\u5177\u6709\u52a8\u6001\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u7684\u8def\u7531\u5668\u3002", "motivation": "\u968f\u7740Agent Web\u548c\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u7684\u53d1\u5c55\uff0c\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u6b63\u5728\u6f14\u53d8\u4e3a\u5f00\u653e\u534f\u4f5c\u7f51\u7edc\uff0c\u53ef\u8bbf\u95ee\u5de5\u5177\u5448\u6307\u6570\u7ea7\u589e\u957f\u3002\u7136\u800c\uff0c\u5f53\u524d\u67b6\u6784\u9762\u4e34\u4e25\u91cd\u7684\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u74f6\u9888\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u5927\u89c4\u6a21\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faToolACE-MCP\u7ba1\u9053\uff0c\u901a\u8fc7\u4f9d\u8d56\u4e30\u5bcc\u7684\u5019\u9009\u56fe\u5408\u6210\u591a\u8f6e\u8f68\u8ff9\uff0c\u8bad\u7ec3\u5386\u53f2\u611f\u77e5\u8def\u7531\u5668\u3002\u8be5\u65b9\u6cd5\u521b\u5efa\u5373\u63d2\u5373\u7528\u7684\u8f7b\u91cf\u8def\u7531\u4ee3\u7406\uff0c\u5177\u5907\u52a8\u6001\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u4e2d\u8fdb\u884c\u7cbe\u786e\u5bfc\u822a\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5MCP-Universe\u548cMCP-Mark\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002ToolACE-MCP\u4e0d\u4ec5\u80fd\u591f\u4ee5\u6700\u5c0f\u9002\u5e94\u5ea6\u63a8\u5e7f\u5230\u591a\u4ee3\u7406\u534f\u4f5c\uff0c\u800c\u4e14\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4fdd\u6301\u5353\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u6709\u6548\u6269\u5c55\u5230\u5927\u89c4\u6a21\u5019\u9009\u7a7a\u95f4\u3002", "conclusion": "ToolACE-MCP\u4e3a\u5f00\u653e\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u901a\u7528\u7f16\u6392\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5b9e\u8bc1\u57fa\u7840\uff0c\u5c55\u73b0\u4e86\u672a\u6765Agent Web\u6240\u9700\u7684\u5173\u952e\u7279\u6027\uff0c\u5305\u62ec\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u591a\u4ee3\u7406\u534f\u4f5c\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.08107", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.08107", "abs": "https://arxiv.org/abs/2601.08107", "authors": ["Chengyang Gu", "Yuxin Pan", "Hui Xiong", "Yize Chen"], "title": "STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order", "comment": "Accepted at International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)", "summary": "Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions, but it often struggles with long-horizon tasks involving sparse rewards. Existing goal-conditioned and hierarchical offline RL methods decompose such tasks and generate intermediate rewards to mitigate limitations of traditional offline RL, but usually overlook temporal dependencies among subgoals and rely on imprecise reward shaping, leading to suboptimal policies. To address these issues, we propose STO-RL (Offline RL using LLM-Guided Subgoal Temporal Order), an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings. Using this temporal structure, STO-RL applies potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions. The resulting augmented dataset with shaped rewards enables efficient offline training of high-performing policies. Evaluations on four discrete and continuous sparse-reward benchmarks demonstrate that STO-RL consistently outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving faster convergence, higher success rates, and shorter trajectories. Ablation studies further confirm STO-RL's robustness to imperfect or noisy LLM-generated subgoal sequences, demonstrating that LLM-guided subgoal temporal structures combined with theoretically grounded reward shaping provide a practical and scalable solution for long-horizon offline RL.", "AI": {"tldr": "STO-RL\uff1a\u5229\u7528LLM\u751f\u6210\u65f6\u5e8f\u5b50\u76ee\u6807\u5e8f\u5217\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u52bf\u80fd\u7684\u5956\u52b1\u5851\u5f62\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u957f\u65f6\u7a0b\u4efb\u52a1", "motivation": "\u4f20\u7edf\u79bb\u7ebfRL\u5728\u7a00\u758f\u5956\u52b1\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u76ee\u6807\u6761\u4ef6\u548c\u5206\u5c42\u65b9\u6cd5\u867d\u7136\u5206\u89e3\u4efb\u52a1\u5e76\u751f\u6210\u4e2d\u95f4\u5956\u52b1\uff0c\u4f46\u5ffd\u7565\u4e86\u5b50\u76ee\u6807\u95f4\u7684\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e14\u4f9d\u8d56\u4e0d\u7cbe\u786e\u7684\u5956\u52b1\u5851\u5f62\uff0c\u5bfc\u81f4\u7b56\u7565\u6b21\u4f18\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u65f6\u5e8f\u6709\u5e8f\u7684\u5b50\u76ee\u6807\u5e8f\u5217\u548c\u72b6\u6001\u5230\u5b50\u76ee\u6807\u9636\u6bb5\u7684\u6620\u5c04\uff0c\u57fa\u4e8e\u6b64\u65f6\u5e8f\u7ed3\u6784\u5e94\u7528\u57fa\u4e8e\u52bf\u80fd\u7684\u5956\u52b1\u5851\u5f62\uff0c\u5c06\u7a00\u758f\u7684\u7ec8\u7aef\u5956\u52b1\u8f6c\u5316\u4e3a\u5bc6\u96c6\u3001\u65f6\u5e8f\u4e00\u81f4\u7684\u4fe1\u53f7\uff0c\u4ece\u800c\u589e\u5f3a\u6570\u636e\u96c6\u5e76\u8bad\u7ec3\u9ad8\u6027\u80fd\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u79bb\u6563\u548c\u8fde\u7eed\u7a00\u758f\u5956\u52b1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTO-RL\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u548c\u5206\u5c42RL\u57fa\u7ebf\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u77ed\u7684\u8f68\u8ff9\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u5176\u5bf9\u4e0d\u5b8c\u7f8e\u6216\u566a\u58f0LLM\u751f\u6210\u7684\u5b50\u76ee\u6807\u5e8f\u5217\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "LLM\u5f15\u5bfc\u7684\u5b50\u76ee\u6807\u65f6\u5e8f\u7ed3\u6784\u4e0e\u7406\u8bba\u57fa\u7840\u7684\u5956\u52b1\u5851\u5f62\u76f8\u7ed3\u5408\uff0c\u4e3a\u957f\u65f6\u7a0b\u79bb\u7ebfRL\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08280", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08280", "abs": "https://arxiv.org/abs/2601.08280", "authors": ["Angshul Majumdar"], "title": "Greedy Is Enough: Sparse Action Discovery in Agentic LLMs", "comment": null, "summary": "Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. Motivated by this observation, we study a contextual linear reward model in which action relevance is governed by a structured sparsity assumption: only a small number of actions have nonzero effects across latent states.\n  We formulate action discovery as a block-sparse recovery problem and analyze a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions on incoherence, signal strength, and action coverage, we prove that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. We further provide estimation error guarantees for refitted parameters and show that the resulting decision rule is near-optimal for new latent states.\n  Complementing these results, we establish information-theoretic lower bounds demonstrating that sparsity and sufficient coverage are necessary for tractability. Together, our results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5927\u89c4\u6a21\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u63d0\u51fa\u57fa\u4e8e\u7ed3\u6784\u5316\u7a00\u758f\u5047\u8bbe\u7684\u52a8\u4f5c\u53d1\u73b0\u7b97\u6cd5\uff0c\u8bc1\u660e\u5728\u7a00\u758f\u6027\u6761\u4ef6\u4e0b\u53ef\u9ad8\u6548\u8bc6\u522b\u76f8\u5173\u52a8\u4f5c\u96c6\u3002", "motivation": "\u73b0\u4ee3\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u5982\u5de5\u5177\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\uff09\u9762\u4e34\u6781\u5927\u52a8\u4f5c\u7a7a\u95f4\uff08\u6570\u5343API\uff09\uff0c\u4f46\u7ecf\u9a8c\u8868\u660e\u53ea\u6709\u5c0f\u90e8\u5206\u52a8\u4f5c\u771f\u6b63\u5f71\u54cd\u6027\u80fd\u3002\u8fd9\u542f\u53d1\u7814\u7a76\u7ed3\u6784\u5316\u7a00\u758f\u5047\u8bbe\u4e0b\u7684\u52a8\u4f5c\u53d1\u73b0\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u7ebf\u6027\u5956\u52b1\u6a21\u578b\uff0c\u5047\u8bbe\u52a8\u4f5c\u76f8\u5173\u6027\u670d\u4ece\u7ed3\u6784\u5316\u7a00\u758f\u6027\u3002\u5c06\u52a8\u4f5c\u53d1\u73b0\u5efa\u6a21\u4e3a\u5757\u7a00\u758f\u6062\u590d\u95ee\u9898\uff0c\u5206\u6790\u53d7\u6b63\u4ea4\u5339\u914d\u8ffd\u8e2a\u542f\u53d1\u7684\u8d2a\u5fc3\u7b97\u6cd5\u3002\u5728\u4e0d\u76f8\u5e72\u6027\u3001\u4fe1\u53f7\u5f3a\u5ea6\u548c\u52a8\u4f5c\u8986\u76d6\u6807\u51c6\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u7b97\u6cd5\u80fd\u51c6\u786e\u6062\u590d\u76f8\u5173\u52a8\u4f5c\u96c6\u3002", "result": "\u8bc1\u660e\u8d2a\u5fc3\u7b97\u6cd5\u80fd\u4ee5\u9ad8\u6982\u7387\u7cbe\u786e\u6062\u590d\u76f8\u5173\u52a8\u4f5c\u96c6\uff0c\u6837\u672c\u590d\u6742\u5ea6\u968f\u7a00\u758f\u5ea6\u548c\u6f5c\u5728\u7ef4\u5ea6\u591a\u9879\u5f0f\u589e\u957f\uff0c\u4ec5\u968f\u603b\u52a8\u4f5c\u6570\u5bf9\u6570\u589e\u957f\u3002\u63d0\u4f9b\u91cd\u62df\u5408\u53c2\u6570\u4f30\u8ba1\u8bef\u5dee\u4fdd\u8bc1\uff0c\u663e\u793a\u6240\u5f97\u51b3\u7b56\u89c4\u5219\u5bf9\u65b0\u6f5c\u5728\u72b6\u6001\u63a5\u8fd1\u6700\u4f18\u3002\u5efa\u7acb\u4fe1\u606f\u8bba\u4e0b\u754c\u8bc1\u660e\u7a00\u758f\u6027\u548c\u5145\u5206\u8986\u76d6\u662f\u6613\u5904\u7406\u6027\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "conclusion": "\u7a00\u758f\u52a8\u4f5c\u53d1\u73b0\u662f\u5927\u89c4\u6a21\u52a8\u4f5c\u51b3\u7b56\u7684\u57fa\u672c\u539f\u7406\uff0c\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u52a8\u4f5c\u526a\u679d\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u8868\u660e\u5728\u7a00\u758f\u6027\u6761\u4ef6\u4e0b\u53ef\u663e\u8457\u964d\u4f4e\u52a8\u4f5c\u7a7a\u95f4\u590d\u6742\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2601.08323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08323", "abs": "https://arxiv.org/abs/2601.08323", "authors": ["Yupeng Huo", "Yaxi Lu", "Zhong Zhang", "Haotian Chen", "Yankai Lin"], "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation", "comment": null, "summary": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.", "AI": {"tldr": "AtomMem\u5c06\u8bb0\u5fc6\u7ba1\u7406\u91cd\u6784\u4e3a\u52a8\u6001\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u9ad8\u7ea7\u8bb0\u5fc6\u8fc7\u7a0b\u5206\u89e3\u4e3a\u539f\u5b50CRUD\u64cd\u4f5c\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5b66\u4e60\u81ea\u4e3b\u3001\u4efb\u52a1\u5bf9\u9f50\u7684\u8bb0\u5fc6\u7ba1\u7406\u7b56\u7565\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u9759\u6001\u5de5\u4f5c\u6d41\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u8bb0\u5fc6\u673a\u5236\u5927\u591a\u4f9d\u8d56\u9759\u6001\u3001\u624b\u5de5\u8bbe\u8ba1\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8fd9\u9650\u5236\u4e86\u8bb0\u5fc6\u8bbe\u8ba1\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u8bb0\u5fc6\u6846\u67b6\u6765\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u957f\u65f6\u7a0b\u95ee\u9898\u3002", "method": "\u5c06\u8bb0\u5fc6\u7ba1\u7406\u91cd\u6784\u4e3a\u52a8\u6001\u51b3\u7b56\u95ee\u9898\uff0c\u5c06\u9ad8\u7ea7\u8bb0\u5fc6\u8fc7\u7a0b\u5206\u89e3\u4e3a\u57fa\u672c\u7684\u539f\u5b50CRUD\uff08\u521b\u5efa\u3001\u8bfb\u53d6\u3001\u66f4\u65b0\u3001\u5220\u9664\uff09\u64cd\u4f5c\uff0c\u5c06\u8bb0\u5fc6\u5de5\u4f5c\u6d41\u8f6c\u5316\u4e3a\u53ef\u5b66\u4e60\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002\u901a\u8fc7\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5b66\u4e60\u81ea\u4e3b\u3001\u4efb\u52a1\u5bf9\u9f50\u7684\u7b56\u7565\u6765\u7f16\u6392\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u9700\u6c42\u7684\u8bb0\u5fc6\u884c\u4e3a\u3002", "result": "\u57283\u4e2a\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8bad\u7ec3\u540e\u7684AtomMem-8B\u6a21\u578b\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u7684\u9759\u6001\u5de5\u4f5c\u6d41\u8bb0\u5fc6\u65b9\u6cd5\u3002\u8bad\u7ec3\u52a8\u6001\u5206\u6790\u8868\u660e\uff0c\u57fa\u4e8e\u5b66\u4e60\u7684\u516c\u5f0f\u4f7f\u4ee3\u7406\u80fd\u591f\u53d1\u73b0\u7ed3\u6784\u5316\u3001\u4efb\u52a1\u5bf9\u9f50\u7684\u8bb0\u5fc6\u7ba1\u7406\u7b56\u7565\u3002", "conclusion": "AtomMem\u901a\u8fc7\u5c06\u8bb0\u5fc6\u7ba1\u7406\u91cd\u6784\u4e3a\u53ef\u5b66\u4e60\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u9759\u6001\u5de5\u4f5c\u6d41\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u53d1\u73b0\u4efb\u52a1\u5bf9\u9f50\u8bb0\u5fc6\u7b56\u7565\u65b9\u9762\u7684\u5173\u952e\u4f18\u52bf\u3002", "topic": "agent analysis"}}
{"id": "2601.08225", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08225", "abs": "https://arxiv.org/abs/2601.08225", "authors": ["Jungho Cho", "Minbyul Jeong", "Sungrae Park"], "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale", "comment": null, "summary": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.", "AI": {"tldr": "\u63d0\u51fa\u7528\u6237\u5bfc\u5411\u7684\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u4efb\u52a1\u751f\u6210\u4e0e\u7528\u6237\u6a21\u62df\u5668\uff0c\u751f\u6210\u66f4\u771f\u5b9e\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u5bf9\u8bdd\u6570\u636e\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u9759\u6001\u5de5\u5177\u96c6\u548c\"\u4ec5\u4efb\u52a1\u89e3\u51b3\"\u8f68\u8ff9\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u9700\u8981\u590d\u6742\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u53d7\u9650\u4e8e\u9759\u6001\u9884\u5b9a\u4e49\u5de5\u5177\u96c6\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u5f00\u653e\u4eba\u673a\u534f\u4f5c\u7684\u590d\u6742\u6027\uff0c\u4e14\u7eaf\u4efb\u52a1\u5bfc\u5411\u8bbe\u8ba1\u5bfc\u81f4\u4ea4\u4e92\u4e0d\u8db3\uff0c\u65e0\u6cd5\u751f\u6210\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9ad8\u8f6e\u6b21\u5bf9\u8bdd\u3002", "method": "1. \u5f00\u53d1\u81ea\u52a8\u5316\u4efb\u52a1\u5bfc\u5411\u591a\u8f6e\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528LRM\u6a21\u62df\u5668\u52a8\u6001\u751f\u6210\u9886\u57df\u7279\u5b9a\u5de5\u5177\uff1b2. \u8f6c\u5411\u7528\u6237\u5bfc\u5411\u6a21\u62df\u8303\u5f0f\uff0c\u89e3\u8026\u4efb\u52a1\u751f\u6210\u4e0e\u4e13\u7528\u7528\u6237\u6a21\u62df\u5668\uff0c\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u89c4\u5219\uff08\u589e\u91cf\u8bf7\u6c42\u3001\u9010\u8f6e\u53cd\u9988\uff09\uff1b3. \u6784\u5efa\u53ef\u63d2\u62d4\u751f\u6210\u7ba1\u9053\uff0c\u53ef\u4ece\u4efb\u4f55\u72b6\u6001\u542f\u52a8\u751f\u6210\uff0c\u652f\u6301\u5355\u8f68\u8ff9\u5185\u591a\u4efb\u52a1\u5b8c\u6210\u3002", "result": "\u751f\u6210\u66f4\u771f\u5b9e\u3001\u6269\u5c55\u7684\u591a\u8f6e\u5bf9\u8bdd\uff0c\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u95ee\u9898\u89e3\u51b3\u7684\u8fed\u4ee3\u6027\u8d28\uff1b\u4ea7\u751f\u9ad8\u5bc6\u5ea6\u6570\u636e\u96c6\uff0c\u4f53\u73b0\u771f\u5b9e\u4eba\u673a\u4ea4\u4e92\u7684\u591a\u65b9\u9762\u9700\u6c42\uff1b\u786e\u4fdd\u9ad8\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u751f\u6210\u6269\u5c55\u7684\u5de5\u5177\u4f7f\u7528\u6570\u636e\u3002", "conclusion": "\u7528\u6237\u5bfc\u5411\u6a21\u62df\u8303\u5f0f\u80fd\u6709\u6548\u751f\u6210\u66f4\u771f\u5b9e\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u5bf9\u8bdd\u6570\u636e\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u8bad\u7ec3\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.08333", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08333", "abs": "https://arxiv.org/abs/2601.08333", "authors": ["Oleg Romanchuk", "Roman Bondar"], "title": "Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant", "comment": null, "summary": "LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms. We formalize this class of architectural failures as semantic laundering: a pattern where propositions with absent or weak warrant are accepted by the system as admissible by crossing architecturally trusted interfaces. We show that semantic laundering constitutes an architectural realization of the Gettier problem: propositions acquire high epistemic status without a connection between their justification and what makes them true. Unlike classical Gettier cases, this effect is not accidental; it is architecturally determined and systematically reproducible. The central result is the Theorem of Inevitable Self-Licensing: under standard architectural assumptions, circular epistemic justification cannot be eliminated. We introduce the Warrant Erosion Principle as the fundamental explanation for this effect and show that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating a problem that exists at the type level.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u8bed\u4e49\u6d17\u94b1\"\u6982\u5ff5\uff0c\u6307\u51faLLM\u667a\u80fd\u4f53\u67b6\u6784\u5c06\u4fe1\u606f\u4f20\u8f93\u673a\u5236\u4e0e\u8ba4\u77e5\u8bc1\u660e\u673a\u5236\u6df7\u4e3a\u4e00\u8c08\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u5145\u5206\u4f9d\u636e\u7684\u547d\u9898\u901a\u8fc7\u67b6\u6784\u4fe1\u4efb\u63a5\u53e3\u88ab\u63a5\u53d7\uff0c\u5f62\u6210\u7cfb\u7edf\u6027\u7684\u8ba4\u77e5\u7f3a\u9677\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u67b6\u6784\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u5c06\u4fe1\u606f\u4f20\u8f93\u673a\u5236\u4e0e\u8ba4\u77e5\u8bc1\u660e\u673a\u5236\u6df7\u6dc6\uff0c\u5bfc\u81f4\u547d\u9898\u5728\u6ca1\u6709\u5145\u5206\u4f9d\u636e\u7684\u60c5\u51b5\u4e0b\u88ab\u7cfb\u7edf\u63a5\u53d7\u3002\u8fd9\u79cd\u67b6\u6784\u7f3a\u9677\u7c7b\u4f3c\u4e8e\u76d6\u68af\u5c14\u95ee\u9898\uff0c\u4f46\u5e76\u975e\u5076\u7136\u800c\u662f\u67b6\u6784\u51b3\u5b9a\u7684\u7cfb\u7edf\u6027\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u5206\u6790\u63d0\u51fa\"\u8bed\u4e49\u6d17\u94b1\"\u6982\u5ff5\uff0c\u5c06\u5176\u5b9a\u4e49\u4e3a\u4e00\u79cd\u67b6\u6784\u6545\u969c\u6a21\u5f0f\u3002\u5f15\u5165\"\u5fc5\u7136\u81ea\u6211\u6388\u6743\u5b9a\u7406\"\u8bc1\u660e\u5728\u6807\u51c6\u67b6\u6784\u5047\u8bbe\u4e0b\uff0c\u5faa\u73af\u8ba4\u77e5\u8bc1\u660e\u65e0\u6cd5\u6d88\u9664\u3002\u63d0\u51fa\"\u4f9d\u636e\u4fb5\u8680\u539f\u7406\"\u4f5c\u4e3a\u6839\u672c\u89e3\u91ca\u3002", "result": "\u8bc1\u660e\u4e86\u8bed\u4e49\u6d17\u94b1\u662f\u76d6\u68af\u5c14\u95ee\u9898\u7684\u67b6\u6784\u5b9e\u73b0\uff0c\u547d\u9898\u83b7\u5f97\u9ad8\u8ba4\u77e5\u5730\u4f4d\u4f46\u7f3a\u4e4f\u771f\u5b9e\u6027\u4e0e\u8bc1\u660e\u4e4b\u95f4\u7684\u8054\u7cfb\u3002\u8fd9\u79cd\u6548\u5e94\u662f\u67b6\u6784\u51b3\u5b9a\u4e14\u53ef\u7cfb\u7edf\u590d\u73b0\u7684\uff0c\u6269\u5c55\u3001\u6a21\u578b\u6539\u8fdb\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u65b9\u6848\u90fd\u65e0\u6cd5\u6d88\u9664\u7c7b\u578b\u5c42\u9762\u7684\u95ee\u9898\u3002", "conclusion": "LLM\u667a\u80fd\u4f53\u67b6\u6784\u5b58\u5728\u6839\u672c\u6027\u8ba4\u77e5\u7f3a\u9677\uff0c\u8bed\u4e49\u6d17\u94b1\u73b0\u8c61\u63ed\u793a\u4e86\u4fe1\u606f\u4f20\u8f93\u4e0e\u8ba4\u77e5\u8bc1\u660e\u7684\u6df7\u6dc6\u95ee\u9898\u3002\u8be5\u95ee\u9898\u5b58\u5728\u4e8e\u7c7b\u578b\u5c42\u9762\uff0c\u65e0\u6cd5\u901a\u8fc7\u73b0\u6709\u6280\u672f\u65b9\u6848\u89e3\u51b3\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u667a\u80fd\u4f53\u67b6\u6784\u8bbe\u8ba1\u3002", "topic": "agent analysis"}}
{"id": "2601.08136", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.08136", "abs": "https://arxiv.org/abs/2601.08136", "authors": ["Zeyang Li", "Sunbochen Tang", "Navid Azizan"], "title": "Reverse Flow Matching: A Unified Framework for Online Reinforcement Learning with Diffusion and Flow Policies", "comment": null, "summary": "Diffusion and flow policies are gaining prominence in online reinforcement learning (RL) due to their expressive power, yet training them efficiently remains a critical challenge. A fundamental difficulty in online RL is the lack of direct samples from the target distribution; instead, the target is an unnormalized Boltzmann distribution defined by the Q-function. To address this, two seemingly distinct families of methods have been proposed for diffusion policies: a noise-expectation family, which utilizes a weighted average of noise as the training target, and a gradient-expectation family, which employs a weighted average of Q-function gradients. Yet, it remains unclear how these objectives relate formally or if they can be synthesized into a more general formulation. In this paper, we propose a unified framework, reverse flow matching (RFM), which rigorously addresses the problem of training diffusion and flow models without direct target samples. By adopting a reverse inferential perspective, we formulate the training target as a posterior mean estimation problem given an intermediate noisy sample. Crucially, we introduce Langevin Stein operators to construct zero-mean control variates, deriving a general class of estimators that effectively reduce importance sampling variance. We show that existing noise-expectation and gradient-expectation methods are two specific instances within this broader class. This unified view yields two key advancements: it extends the capability of targeting Boltzmann distributions from diffusion to flow policies, and enables the principled combination of Q-value and Q-gradient information to derive an optimal, minimum-variance estimator, thereby improving training efficiency and stability. We instantiate RFM to train a flow policy in online RL, and demonstrate improved performance on continuous-control benchmarks compared to diffusion policy baselines.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6RFM\uff0c\u901a\u8fc7\u53cd\u5411\u63a8\u7406\u89c6\u89d2\u548cLangevin Stein\u7b97\u5b50\uff0c\u4e3a\u65e0\u76f4\u63a5\u76ee\u6807\u6837\u672c\u7684\u6269\u6563\u548c\u6d41\u7b56\u7565\u8bad\u7ec3\u63d0\u4f9b\u6700\u5c0f\u65b9\u5dee\u4f30\u8ba1\u5668\uff0c\u63d0\u5347\u5728\u7ebfRL\u6548\u7387", "motivation": "\u6269\u6563\u548c\u6d41\u7b56\u7565\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u8fbe\u80fd\u529b\u5f3a\u4f46\u8bad\u7ec3\u6548\u7387\u4f4e\uff0c\u73b0\u6709\u566a\u58f0\u671f\u671b\u548c\u68af\u5ea6\u671f\u671b\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u9700\u8981\u89e3\u51b3\u65e0\u76f4\u63a5\u76ee\u6807\u6837\u672c\u7684\u6311\u6218", "method": "\u63d0\u51fa\u53cd\u5411\u6d41\u5339\u914d(RFM)\u6846\u67b6\uff0c\u91c7\u7528\u53cd\u5411\u63a8\u7406\u89c6\u89d2\uff0c\u5c06\u8bad\u7ec3\u76ee\u6807\u6784\u5efa\u4e3a\u540e\u9a8c\u5747\u503c\u4f30\u8ba1\u95ee\u9898\uff0c\u5f15\u5165Langevin Stein\u7b97\u5b50\u6784\u9020\u96f6\u5747\u503c\u63a7\u5236\u53d8\u91cf\uff0c\u63a8\u5bfc\u51fa\u6700\u5c0f\u65b9\u5dee\u4f30\u8ba1\u5668", "result": "RFM\u7edf\u4e00\u4e86\u566a\u58f0\u671f\u671b\u548c\u68af\u5ea6\u671f\u671b\u65b9\u6cd5\uff0c\u5c06Boltzmann\u5206\u5e03\u76ee\u6807\u4ece\u6269\u6563\u7b56\u7565\u6269\u5c55\u5230\u6d41\u7b56\u7565\uff0c\u7ed3\u5408Q\u503c\u548cQ\u68af\u5ea6\u4fe1\u606f\u5b9e\u73b0\u6700\u4f18\u4f30\u8ba1\uff0c\u5728\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u4e0a\u4f18\u4e8e\u6269\u6563\u7b56\u7565\u57fa\u7ebf", "conclusion": "RFM\u4e3a\u65e0\u76f4\u63a5\u76ee\u6807\u6837\u672c\u7684\u6269\u6563\u548c\u6d41\u7b56\u7565\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u65b9\u5dee\u4f30\u8ba1\u5668\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u63a8\u52a8\u4e86\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u53d1\u5c55", "topic": "agentic reinforcement learning"}}
{"id": "2601.08282", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08282", "abs": "https://arxiv.org/abs/2601.08282", "authors": ["Kangcheng Luo", "Tinglang Wu", "Yansong Feng"], "title": "D$^2$Plan: Dual-Agent Dynamic Global Planning for Complex Retrieval-Augmented Reasoning", "comment": null, "summary": "Recent search-augmented LLMs trained with reinforcement learning (RL) can interleave searching and reasoning for multi-hop reasoning tasks. However, they face two critical failure modes as the accumulating context becomes flooded with both crucial evidence and irrelevant information: (1) ineffective search chain construction that produces incorrect queries or omits retrieval of critical information, and (2) reasoning hijacking by peripheral evidence that causes models to misidentify distractors as valid evidence. To address these challenges, we propose **D$^2$Plan**, a **D**ual-agent **D**ynamic global **Plan**ning paradigm for complex retrieval-augmented reasoning. **D$^2$Plan** operates through the collaboration of a *Reasoner* and a *Purifier*: the *Reasoner* constructs explicit global plans during reasoning and dynamically adapts them based on retrieval feedback; the *Purifier* assesses retrieval relevance and condenses key information for the *Reasoner*. We further introduce a two-stage training framework consisting of supervised fine-tuning (SFT) cold-start on synthesized trajectories and RL with plan-oriented rewards to teach LLMs to master the **D$^2$Plan** paradigm. Extensive experiments demonstrate that **D$^2$Plan** enables more coherent multi-step reasoning and stronger resilience to irrelevant information, thereby achieving superior performance on challenging QA benchmarks.", "AI": {"tldr": "\u63d0\u51faD\u00b2Plan\u53cc\u667a\u80fd\u4f53\u52a8\u6001\u5168\u5c40\u89c4\u5212\u8303\u5f0f\uff0c\u901a\u8fc7Reasoner\u548cPurifier\u534f\u4f5c\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u4e2d\u7684\u641c\u7d22\u94fe\u6784\u5efa\u65e0\u6548\u548c\u63a8\u7406\u52ab\u6301\u95ee\u9898\uff0c\u5728\u590d\u6742QA\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u68c0\u7d22\u589e\u5f3aLLM\u5728\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff1a1) \u641c\u7d22\u94fe\u6784\u5efa\u65e0\u6548\uff0c\u4ea7\u751f\u9519\u8bef\u67e5\u8be2\u6216\u9057\u6f0f\u5173\u952e\u4fe1\u606f\u68c0\u7d22\uff1b2) \u63a8\u7406\u88ab\u5916\u56f4\u8bc1\u636e\u52ab\u6301\uff0c\u5bfc\u81f4\u6a21\u578b\u5c06\u5e72\u6270\u9879\u8bef\u8ba4\u4e3a\u6709\u6548\u8bc1\u636e\u3002", "method": "\u63d0\u51faD\u00b2Plan\u53cc\u667a\u80fd\u4f53\u52a8\u6001\u5168\u5c40\u89c4\u5212\u8303\u5f0f\uff0c\u5305\u542bReasoner\uff08\u6784\u5efa\u663e\u5f0f\u5168\u5c40\u89c4\u5212\u5e76\u57fa\u4e8e\u68c0\u7d22\u53cd\u9988\u52a8\u6001\u8c03\u6574\uff09\u548cPurifier\uff08\u8bc4\u4f30\u68c0\u7d22\u76f8\u5173\u6027\u5e76\u538b\u7f29\u5173\u952e\u4fe1\u606f\uff09\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u57fa\u4e8e\u5408\u6210\u8f68\u8ff9\u7684\u76d1\u7763\u5fae\u8c03\u51b7\u542f\u52a8\uff0c\u4ee5\u53ca\u5e26\u6709\u89c4\u5212\u5bfc\u5411\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2aQA\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cD\u00b2Plan\u80fd\u591f\u5b9e\u73b0\u66f4\u8fde\u8d2f\u7684\u591a\u6b65\u63a8\u7406\u548c\u66f4\u5f3a\u7684\u6297\u5e72\u6270\u4fe1\u606f\u80fd\u529b\uff0c\u4ece\u800c\u5728\u5177\u6709\u6311\u6218\u6027\u7684QA\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "D\u00b2Plan\u901a\u8fc7\u53cc\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u5168\u5c40\u89c4\u5212\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u5347\u4e86\u591a\u8df3\u63a8\u7406\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.08308", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08308", "abs": "https://arxiv.org/abs/2601.08308", "authors": ["Bo Yang", "Yu Zhang", "Yunkui Chen", "Lanfei Feng", "Xiao Xu", "Nueraili Aierken", "Shijian Li"], "title": "AgriAgent: Contract-Driven Planning and Capability-Aware Tool Orchestration in Real-World Agriculture", "comment": null, "summary": "Intelligent agent systems in real-world agricultural scenarios must handle diverse tasks under multimodal inputs, ranging from lightweight information understanding to complex multi-step execution. However, most existing approaches rely on a unified execution paradigm, which struggles to accommodate large variations in task complexity and incomplete tool availability commonly observed in agricultural environments. To address this challenge, we propose AgriAgent, a two-level agent framework for real-world agriculture. AgriAgent adopts a hierarchical execution strategy based on task complexity: simple tasks are handled through direct reasoning by modality-specific agents, while complex tasks trigger a contract-driven planning mechanism that formulates tasks as capability requirements and performs capability-aware tool orchestration and dynamic tool generation, enabling multi-step and verifiable execution with failure recovery. Experimental results show that AgriAgent achieves higher execution success rates and robustness on complex tasks compared to existing tool-centric agent baselines that rely on unified execution paradigms. All code, data will be released at after our work be accepted to promote reproducible research.", "AI": {"tldr": "AgriAgent\u662f\u4e00\u4e2a\u7528\u4e8e\u771f\u5b9e\u519c\u4e1a\u573a\u666f\u7684\u4e24\u7ea7\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u5206\u5c42\u6267\u884c\u7b56\u7565\uff0c\u7b80\u5355\u4efb\u52a1\u7531\u6a21\u6001\u7279\u5b9a\u667a\u80fd\u4f53\u76f4\u63a5\u5904\u7406\uff0c\u590d\u6742\u4efb\u52a1\u5219\u901a\u8fc7\u5951\u7ea6\u9a71\u52a8\u89c4\u5212\u673a\u5236\u8fdb\u884c\u591a\u6b65\u53ef\u9a8c\u8bc1\u6267\u884c\u3002", "motivation": "\u771f\u5b9e\u519c\u4e1a\u573a\u666f\u4e2d\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u9700\u8981\u5904\u7406\u4ece\u8f7b\u91cf\u7ea7\u4fe1\u606f\u7406\u89e3\u5230\u590d\u6742\u591a\u6b65\u6267\u884c\u7684\u5404\u79cd\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u7edf\u4e00\u6267\u884c\u8303\u5f0f\u96be\u4ee5\u9002\u5e94\u4efb\u52a1\u590d\u6742\u5ea6\u5dee\u5f02\u5927\u548c\u5de5\u5177\u53ef\u7528\u6027\u4e0d\u5b8c\u6574\u7684\u519c\u4e1a\u73af\u5883\u6311\u6218\u3002", "method": "\u63d0\u51faAgriAgent\u4e24\u7ea7\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u57fa\u4e8e\u4efb\u52a1\u590d\u6742\u5ea6\u91c7\u7528\u5206\u5c42\u6267\u884c\u7b56\u7565\uff0c\u7b80\u5355\u4efb\u52a1\u7531\u6a21\u6001\u7279\u5b9a\u667a\u80fd\u4f53\u76f4\u63a5\u63a8\u7406\u5904\u7406\uff0c\u590d\u6742\u4efb\u52a1\u89e6\u53d1\u5951\u7ea6\u9a71\u52a8\u89c4\u5212\u673a\u5236\uff0c\u5c06\u4efb\u52a1\u5236\u5b9a\u4e3a\u80fd\u529b\u9700\u6c42\uff0c\u8fdb\u884c\u80fd\u529b\u611f\u77e5\u7684\u5de5\u5177\u7f16\u6392\u548c\u52a8\u6001\u5de5\u5177\u751f\u6210\uff0c\u652f\u6301\u591a\u6b65\u53ef\u9a8c\u8bc1\u6267\u884c\u548c\u5931\u8d25\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f9d\u8d56\u7edf\u4e00\u6267\u884c\u8303\u5f0f\u7684\u73b0\u6709\u5de5\u5177\u4e2d\u5fc3\u667a\u80fd\u4f53\u57fa\u7ebf\u76f8\u6bd4\uff0cAgriAgent\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6267\u884c\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "AgriAgent\u901a\u8fc7\u5206\u5c42\u6267\u884c\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u519c\u4e1a\u73af\u5883\u4e2d\u4efb\u52a1\u590d\u6742\u5ea6\u5dee\u5f02\u5927\u548c\u5de5\u5177\u53ef\u7528\u6027\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u4e3a\u771f\u5b9e\u519c\u4e1a\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u9c81\u68d2\u7684\u667a\u80fd\u4f53\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.08210", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08210", "abs": "https://arxiv.org/abs/2601.08210", "authors": ["Zhenglong Luo", "Zhiyong Chen", "Aoxiang Liu", "Ke Pan"], "title": "Scalable Multiagent Reinforcement Learning with Collective Influence Estimation", "comment": null, "summary": "Multiagent reinforcement learning (MARL) has attracted considerable attention due to its potential in addressing complex cooperative tasks. However, existing MARL approaches often rely on frequent exchanges of action or state information among agents to achieve effective coordination, which is difficult to satisfy in practical robotic systems. A common solution is to introduce estimator networks to model the behaviors of other agents and predict their actions; nevertheless, such designs cause the size and computational cost of the estimator networks to grow rapidly with the number of agents, thereby limiting scalability in large-scale systems.\n  To address these challenges, this paper proposes a multiagent learning framework augmented with a Collective Influence Estimation Network (CIEN). By explicitly modeling the collective influence of other agents on the task object, each agent can infer critical interaction information solely from its local observations and the task object's states, enabling efficient collaboration without explicit action information exchange. The proposed framework effectively avoids network expansion as the team size increases; moreover, new agents can be incorporated without modifying the network structures of existing agents, demonstrating strong scalability. Experimental results on multiagent cooperative tasks based on the Soft Actor-Critic (SAC) algorithm show that the proposed method achieves stable and efficient coordination under communication-limited environments. Furthermore, policies trained with collective influence modeling are deployed on a real robotic platform, where experimental results indicate significantly improved robustness and deployment feasibility, along with reduced dependence on communication infrastructure.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u96c6\u4f53\u5f71\u54cd\u529b\u4f30\u8ba1\u7f51\u7edc(CIEN)\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u5176\u4ed6\u667a\u80fd\u4f53\u5bf9\u4efb\u52a1\u5bf9\u8c61\u7684\u96c6\u4f53\u5f71\u54cd\u529b\uff0c\u5b9e\u73b0\u4ec5\u4f9d\u8d56\u5c40\u90e8\u89c2\u6d4b\u548c\u4efb\u52a1\u5bf9\u8c61\u72b6\u6001\u7684\u9ad8\u6548\u534f\u4f5c\uff0c\u65e0\u9700\u663e\u5f0f\u52a8\u4f5c\u4fe1\u606f\u4ea4\u6362\uff0c\u5177\u6709\u826f\u597d\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709MARL\u65b9\u6cd5\u9700\u8981\u9891\u7e41\u4ea4\u6362\u52a8\u4f5c\u6216\u72b6\u6001\u4fe1\u606f\u4ee5\u5b9e\u73b0\u6709\u6548\u534f\u8c03\uff0c\u8fd9\u5728\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u96be\u4ee5\u6ee1\u8db3\u3002\u73b0\u6709\u4f30\u8ba1\u5668\u7f51\u7edc\u8bbe\u8ba1\u5bfc\u81f4\u7f51\u7edc\u89c4\u6a21\u548c\u8ba1\u7b97\u6210\u672c\u968f\u667a\u80fd\u4f53\u6570\u91cf\u5feb\u901f\u589e\u957f\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u96c6\u4f53\u5f71\u54cd\u529b\u4f30\u8ba1\u7f51\u7edc(CIEN)\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5176\u4ed6\u667a\u80fd\u4f53\u5bf9\u4efb\u52a1\u5bf9\u8c61\u7684\u96c6\u4f53\u5f71\u54cd\u529b\uff0c\u4f7f\u6bcf\u4e2a\u667a\u80fd\u4f53\u4ec5\u4ece\u5c40\u90e8\u89c2\u6d4b\u548c\u4efb\u52a1\u5bf9\u8c61\u72b6\u6001\u63a8\u65ad\u5173\u952e\u4ea4\u4e92\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u7f51\u7edc\u968f\u56e2\u961f\u89c4\u6a21\u6269\u5c55\uff0c\u65b0\u667a\u80fd\u4f53\u52a0\u5165\u65e0\u9700\u4fee\u6539\u73b0\u6709\u7f51\u7edc\u7ed3\u6784\u3002", "result": "\u57fa\u4e8eSAC\u7b97\u6cd5\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4efb\u52a1\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u534f\u8c03\u3002\u5728\u5b9e\u9645\u673a\u5668\u4eba\u5e73\u53f0\u90e8\u7f72\u663e\u793a\uff0c\u57fa\u4e8e\u96c6\u4f53\u5f71\u54cd\u529b\u5efa\u6a21\u7684\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u90e8\u7f72\u53ef\u884c\u6027\uff0c\u51cf\u5c11\u4e86\u5bf9\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\u7684\u4f9d\u8d56\u3002", "conclusion": "CIEN\u6846\u67b6\u901a\u8fc7\u5efa\u6a21\u96c6\u4f53\u5f71\u54cd\u529b\u800c\u975e\u4e2a\u4f53\u52a8\u4f5c\uff0c\u5b9e\u73b0\u4e86\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e0b\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u4e3a\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08406", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.08406", "abs": "https://arxiv.org/abs/2601.08406", "authors": ["Xinyi Wu", "Jiagui Chen", "Geng Hong", "Jiayi Dong", "Xudong Pan", "Jiarun Dai", "Min Yang"], "title": "WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents", "comment": null, "summary": "Web Agents are increasingly deployed to perform complex tasks in real web environments, yet their security evaluation remains fragmented and difficult to standardize. We present WebTrap Park, an automated platform for systematic security evaluation of Web Agents through direct observation of their concrete interactions with live web pages. WebTrap Park instantiates three major sources of security risk into 1,226 executable evaluation tasks and enables action based assessment without requiring agent modification. Our results reveal clear security differences across agent frameworks, highlighting the importance of agent architecture beyond the underlying model. WebTrap Park is publicly accessible at https://security.fudan.edu.cn/webagent and provides a scalable foundation for reproducible Web Agent security evaluation.", "AI": {"tldr": "WebTrap Park\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u7528\u4e8e\u901a\u8fc7\u76f4\u63a5\u89c2\u5bdfWeb\u4ee3\u7406\u4e0e\u5b9e\u65f6\u7f51\u9875\u7684\u4ea4\u4e92\u6765\u7cfb\u7edf\u8bc4\u4f30\u5176\u5b89\u5168\u6027\uff0c\u5305\u542b1,226\u4e2a\u53ef\u6267\u884c\u8bc4\u4f30\u4efb\u52a1\uff0c\u65e0\u9700\u4fee\u6539\u4ee3\u7406\u5373\u53ef\u8fdb\u884c\u57fa\u4e8e\u52a8\u4f5c\u7684\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524dWeb\u4ee3\u7406\u5728\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u5176\u5b89\u5168\u6027\u8bc4\u4f30\u4ecd\u7136\u5206\u6563\u4e14\u96be\u4ee5\u6807\u51c6\u5316\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u5e73\u53f0\u3002", "method": "\u5f00\u53d1WebTrap Park\u5e73\u53f0\uff0c\u5c06\u4e09\u79cd\u4e3b\u8981\u5b89\u5168\u98ce\u9669\u6765\u6e90\u5b9e\u4f8b\u5316\u4e3a1,226\u4e2a\u53ef\u6267\u884c\u8bc4\u4f30\u4efb\u52a1\uff0c\u901a\u8fc7\u76f4\u63a5\u89c2\u5bdf\u4ee3\u7406\u4e0e\u5b9e\u65f6\u7f51\u9875\u7684\u5177\u4f53\u4ea4\u4e92\u8fdb\u884c\u81ea\u52a8\u5316\u5b89\u5168\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u4ee3\u7406\u6846\u67b6\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u7684\u5b89\u5168\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u4ee3\u7406\u67b6\u6784\u7684\u91cd\u8981\u6027\uff08\u800c\u4e0d\u4ec5\u4ec5\u662f\u5e95\u5c42\u6a21\u578b\uff09\u3002\u5e73\u53f0\u516c\u5f00\u53ef\u7528\uff0c\u4e3a\u53ef\u590d\u73b0\u7684Web\u4ee3\u7406\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u57fa\u7840\u3002", "conclusion": "WebTrap Park\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30Web\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u63ed\u793a\u4e86\u4ee3\u7406\u67b6\u6784\u5bf9\u5b89\u5168\u6027\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8Web\u4ee3\u7406\u5b89\u5168\u7814\u7a76\u7684\u6807\u51c6\u5316\u3002", "topic": "agent analysis"}}
{"id": "2601.08412", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08412", "abs": "https://arxiv.org/abs/2601.08412", "authors": ["Yizhan Feng", "Hichem Snoussi", "Yuhang Wang", "Jing Teng", "Abel Cherouat", "Tian Wang"], "title": "Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation", "comment": "2nd International Conference on Drones and Unmanned Systems (DAUS' 2026)", "summary": "With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u3001\u601d\u7ef4\u94fe\u5f15\u5bfc\u548c\u76d1\u7763\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u8fc1\u79fb\u5230\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u591aSDK\u63a7\u5236\u4efb\u52a1", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u6f5c\u529b\u663e\u8457\uff0c\u4f46\u5e94\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u5e73\u53f0\u65f6\u5b58\u5728\u77db\u76fe\uff1a\u5927\u6a21\u578b\u8d44\u6e90\u6d88\u8017\u9ad8\uff0c\u800c\u65e0\u4eba\u673a\u9700\u8981\u5b9e\u65f6\u8f7b\u91cf\u7ea7\u63a7\u5236", "method": "1) \u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u6307\u4ee4-\u4ee3\u7801-\u63a8\u7406\u94fe\u548c\u53cd\u4e8b\u5b9e\u8d1f\u6837\u672c\uff1b2) \u4f7f\u7528DeepSeek-Coder-V2-Lite\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u9ed1\u76d2\u767d\u76d2\u84b8\u998f\u7b56\u7565\u751f\u6210\u601d\u7ef4\u94fe\u8f6f\u6807\u7b7e\uff0c\u7ed3\u5408\u52a0\u6743\u4ea4\u53c9\u71b5\u635f\u5931\uff1b3) \u9488\u5bf9\u65e0\u4eba\u673a\u63a7\u5236\u573a\u666f\u8fdb\u884c\u63d0\u793a\u8c03\u4f18\u5de5\u7a0b", "result": "\u84b8\u998f\u540e\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u4ee3\u7801\u751f\u6210\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5728\u90e8\u7f72\u548c\u63a8\u7406\u6548\u7387\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u7cbe\u786e\u8f7b\u91cf\u7ea7\u667a\u80fd\u63a7\u5236", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u65e0\u4eba\u673a\u7cbe\u786e\u8f7b\u91cf\u7ea7\u667a\u80fd\u63a7\u5236\u65b9\u9762\u5177\u6709\u53ef\u884c\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u89e3\u51b3\u4e86\u5927\u6a21\u578b\u4e0e\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e4b\u95f4\u7684\u77db\u76fe", "topic": "code agent"}}
{"id": "2601.08219", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08219", "abs": "https://arxiv.org/abs/2601.08219", "authors": ["Paimon Goulart", "Evangelos E. Papalexakis"], "title": "A Preliminary Agentic Framework for Matrix Deflation", "comment": null, "summary": "Can a small team of agents peel a matrix apart, one rank-1 slice at a time? We propose an agentic approach to matrix deflation in which a solver Large Language Model (LLM) generates rank-1 Singular Value Decomposition (SVD) updates and a Vision Language Model (VLM) accepts or rejects each update and decides when to stop, eliminating fixed norm thresholds. Solver stability is improved through in-context learning (ICL) and types of row/column permutations that expose visually coherent structure. We evaluate on Digits ($8{\\times}8$), CIFAR-10 ($32{\\times}32$ grayscale), and synthetic ($16{\\times}16$) matrices with and without Gaussian noise. In the synthetic noisy case, where the true construction rank $k$ is known, numerical deflation provides the noise target and our best agentic configuration differs by only $1.75$ RMSE of the target. For Digits and CIFAR-10, targets are defined by deflating until the Frobenius norm reaches $10\\%$ of the original. Across all settings, our agent achieves competitive results, suggesting that fully agentic, threshold-free deflation is a viable alternative to classical numerical algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u77e9\u9635\u964d\u79e9\u65b9\u6cd5\uff0c\u4f7f\u7528LLM\u751f\u6210\u79e9-1 SVD\u66f4\u65b0\uff0cVLM\u8bc4\u4f30\u66f4\u65b0\u5e76\u51b3\u5b9a\u505c\u6b62\u65f6\u673a\uff0c\u65e0\u9700\u56fa\u5b9a\u9608\u503c", "motivation": "\u63a2\u7d22\u667a\u80fd\u4f53\u80fd\u5426\u534f\u4f5c\u5b8c\u6210\u77e9\u9635\u964d\u79e9\u4efb\u52a1\uff0c\u66ff\u4ee3\u4f20\u7edf\u6570\u503c\u7b97\u6cd5\uff0c\u5b9e\u73b0\u65e0\u9700\u56fa\u5b9a\u9608\u503c\u7684\u81ea\u9002\u5e94\u964d\u79e9", "method": "\u91c7\u7528\u53cc\u667a\u80fd\u4f53\u67b6\u6784\uff1aLLM\u4f5c\u4e3a\u6c42\u89e3\u5668\u751f\u6210\u79e9-1 SVD\u66f4\u65b0\uff0cVLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u63a5\u53d7/\u62d2\u7edd\u66f4\u65b0\u5e76\u51b3\u5b9a\u505c\u6b62\u65f6\u673a\uff1b\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u884c\u5217\u7f6e\u6362\u63d0\u9ad8\u7a33\u5b9a\u6027", "result": "\u5728Digits\u3001CIFAR-10\u548c\u5408\u6210\u77e9\u9635\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5408\u6210\u566a\u58f0\u60c5\u51b5\u4e0b\u4e0e\u6570\u503c\u964d\u79e9\u4ec5\u5dee1.75 RMSE\uff0c\u5728Digits\u548cCIFAR-10\u4e0a\u8fbe\u5230\u539f\u59cbFrobenius\u8303\u657010%\u7684\u76ee\u6807", "conclusion": "\u5b8c\u5168\u667a\u80fd\u4f53\u5316\u3001\u65e0\u9608\u503c\u7684\u964d\u79e9\u65b9\u6cd5\u662f\u53ef\u884c\u7684\u6570\u503c\u7b97\u6cd5\u66ff\u4ee3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2601.08430", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08430", "abs": "https://arxiv.org/abs/2601.08430", "authors": ["Sunzhu Li", "Jiale Zhao", "Miteto Wei", "Huimin Ren", "Yang Zhou", "Jingwen Yang", "Shunyu Liu", "Kaike Zhang", "Wei Chen"], "title": "RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.", "AI": {"tldr": "\u63d0\u51faRubricHub\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u8bc4\u5206\u6807\u51c6\u751f\u6210\u65b9\u6cd5\u521b\u5efa\u5927\u89c4\u6a21\u591a\u9886\u57df\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u5f00\u653e\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u5728\u5f00\u653e\u751f\u6210\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u771f\u5b9e\u6807\u7b7e\u3002\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u74f6\u9888\u548c\u6807\u51c6\u7c97\u7cd9\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u76d1\u7763\u5929\u82b1\u677f\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u5316\u7c97\u5230\u7ec6\u8bc4\u5206\u6807\u51c6\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u539f\u5219\u5f15\u5bfc\u5408\u6210\u3001\u591a\u6a21\u578b\u805a\u5408\u548c\u96be\u5ea6\u6f14\u5316\u3002\u57fa\u4e8e\u6b64\u6784\u5efaRubricHub\u6570\u636e\u96c6\uff08\u7ea611\u4e07\u6837\u672c\uff09\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\uff1a\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u62d2\u7edd\u91c7\u6837\u5fae\u8c03(RuFT)\u548c\u5f3a\u5316\u5b66\u4e60(RuRL)\u3002", "result": "Qwen3-14B\u6a21\u578b\u5728HealthBench\u4e0a\u8fbe\u523069.3\u5206\uff0c\u8d85\u8d8aGPT-5\u7b49\u524d\u6cbf\u4e13\u6709\u6a21\u578b\uff0c\u53d6\u5f97SOTA\u7ed3\u679c\u3002", "conclusion": "RubricHub\u6846\u67b6\u901a\u8fc7\u751f\u6210\u7ec6\u7c92\u5ea6\u8bc4\u5206\u6807\u51c6\u89e3\u51b3\u4e86\u5f00\u653e\u751f\u6210\u4efb\u52a1\u7684\u76d1\u7763\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08427", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08427", "abs": "https://arxiv.org/abs/2601.08427", "authors": ["Nonghai Zhang", "Weitao Ma", "Zhanyu Ma", "Jun Xu", "Jiuchong Gao", "Jinghua Hao", "Renqing He", "Jingwen Xu"], "title": "Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but also yields sparse rewards that hinder optimization efficiency. To address these challenges, we propose Latent-GRPO, a framework that derives intrinsic rewards directly from latent space geometry. Crucially, our empirical analysis reveals a compelling geometric property: terminal token representations of correct reasoning trajectories form dense clusters with high intra-class similarity, whereas incorrect trajectories remain scattered as outliers. In light of this discovery, we introduce the Iterative Robust Centroid Estimation (IRCE) algorithm, which generates dense, continuous rewards by mitigating magnitude fluctuations via spherical projection and estimating a robust ``truth centroid'' through iterative aggregation. Experimental results on multiple datasets show that our method maintains model performance while achieving a training speedup of over 2x compared to baselines. Furthermore, extensive results demonstrate strong generalization ability and robustness. The code will be released soon.", "AI": {"tldr": "Latent-GRPO\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u7279\u6027\u751f\u6210\u5185\u5728\u5956\u52b1\uff0c\u65e0\u9700\u6602\u8d35\u7684\u5916\u90e8\u9a8c\u8bc1\u5668\uff0c\u5b9e\u73b02\u500d\u4ee5\u4e0a\u8bad\u7ec3\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "GRPO\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u5916\u90e8\u9a8c\u8bc1\u5668\u6216\u4eba\u5de5\u89c4\u5219\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8bad\u7ec3\u5ef6\u8fdf\u5927\uff0c\u4e14\u7a00\u758f\u5956\u52b1\u963b\u788d\u4f18\u5316\u6548\u7387\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5185\u5728\u5956\u52b1\u673a\u5236\u3002", "method": "\u63d0\u51faLatent-GRPO\u6846\u67b6\uff0c\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u7279\u6027\uff1a\u6b63\u786e\u63a8\u7406\u8f68\u8ff9\u7684\u7ec8\u7aeftoken\u8868\u793a\u5f62\u6210\u5bc6\u96c6\u805a\u7c7b\uff0c\u9519\u8bef\u8f68\u8ff9\u5219\u5206\u6563\u4e3a\u79bb\u7fa4\u70b9\u3002\u5f15\u5165\u8fed\u4ee3\u9c81\u68d2\u8d28\u5fc3\u4f30\u8ba1(IRCE)\u7b97\u6cd5\uff0c\u901a\u8fc7\u7403\u9762\u6295\u5f71\u7f13\u89e3\u5e45\u5ea6\u6ce2\u52a8\uff0c\u901a\u8fc7\u8fed\u4ee3\u805a\u5408\u4f30\u8ba1\u9c81\u68d2\u7684\"\u771f\u7406\u8d28\u5fc3\"\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u8d85\u8fc72\u500d\u7684\u8bad\u7ec3\u52a0\u901f\u3002\u5e7f\u6cdb\u7ed3\u679c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Latent-GRPO\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u7279\u6027\u6709\u6548\u751f\u6210\u5bc6\u96c6\u8fde\u7eed\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u51cf\u5c11\u5bf9\u5916\u90e8\u9a8c\u8bc1\u5668\u7684\u4f9d\u8d56\uff0c\u5177\u6709\u826f\u597d\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08441", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08441", "abs": "https://arxiv.org/abs/2601.08441", "authors": ["Abdelaziz Bounhar", "Rania Hossam Elmohamady Elbadry", "Hadi Abdine", "Preslav Nakov", "Michalis Vazirgiannis", "Guokan Shang"], "title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation", "comment": null, "summary": "Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \\textit{reference-free} method that learns \\textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\\footnote{https://github.com/MBZUAI-Paris/YaPO}.", "AI": {"tldr": "YaPO\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u7a00\u758f\u8f6c\u5411\u5411\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u76f8\u6bd4\u5bc6\u96c6\u8f6c\u5411\u5411\u91cf\u80fd\u66f4\u597d\u5730\u89e3\u8026\u6f5c\u5728\u56e0\u7d20\uff0c\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u3001\u7a33\u5b9a\u3001\u9ad8\u6548\u7684LLM\u5bf9\u9f50\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eBiPO\u7684\u5bc6\u96c6\u8f6c\u5411\u5411\u91cf\u65b9\u6cd5\u7531\u4e8e\u795e\u7ecf\u5143\u591a\u8bed\u4e49\u6027\uff0c\u5bb9\u6613\u7ea0\u7f20\u591a\u4e2a\u6f5c\u5728\u56e0\u7d20\uff0c\u5728\u9700\u8981\u7cbe\u7ec6\u533a\u5206\uff08\u5982\u4e2d\u4e1c\u6587\u5316\u5bf9\u9f50\uff09\u7684\u573a\u666f\u4e2d\u6548\u679c\u6709\u9650\u4e14\u4e0d\u7a33\u5b9a\u3002", "method": "YaPO\u91c7\u7528\u65e0\u53c2\u8003\u65b9\u6cd5\uff0c\u5728\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7a00\u758f\u8f6c\u5411\u5411\u91cf\uff0c\u901a\u8fc7\u4f18\u5316\u7a00\u758f\u7f16\u7801\u4ea7\u751f\u89e3\u8026\u3001\u53ef\u89e3\u91ca\u3001\u9ad8\u6548\u7684\u8f6c\u5411\u65b9\u5411\u3002", "result": "YaPO\u6bd4\u5bc6\u96c6\u8f6c\u5411\u57fa\u7ebf\u6536\u655b\u66f4\u5feb\u3001\u6027\u80fd\u66f4\u5f3a\u3001\u8bad\u7ec3\u66f4\u7a33\u5b9a\uff0c\u80fd\u6cdb\u5316\u5230\u5e7b\u89c9\u3001\u8d22\u5bcc\u8ffd\u6c42\u3001\u8d8a\u72f1\u3001\u6743\u529b\u8ffd\u6c42\u7b49\u591a\u79cd\u5bf9\u9f50\u76f8\u5173\u884c\u4e3a\uff0c\u4e14\u4e0d\u635f\u5bb3MMLU\u901a\u7528\u77e5\u8bc6\u3002", "conclusion": "YaPO\u4e3aLLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u3001\u7cbe\u7ec6\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5728\u53ef\u63a7\u6027\u548c\u9886\u57df\u9002\u5e94\u65b9\u9762\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002", "topic": "agent analysis"}}
{"id": "2601.08247", "categories": ["cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2601.08247", "abs": "https://arxiv.org/abs/2601.08247", "authors": ["Liu He"], "title": "Incorporating Cognitive Biases into Reinforcement Learning for Financial Decision-Making", "comment": "15 pages, 9 figures", "summary": "Financial markets are influenced by human behavior that deviates from rationality due to cognitive biases. Traditional reinforcement learning (RL) models for financial decision-making assume rational agents, potentially overlooking the impact of psychological factors. This study integrates cognitive biases into RL frameworks for financial trading, hypothesizing that such models can exhibit human-like trading behavior and achieve better risk-adjusted returns than standard RL agents. We introduce biases, such as overconfidence and loss aversion, into reward structures and decision-making processes and evaluate their performance in simulated and real-world trading environments. Despite its inconclusive or negative results, this study provides insights into the challenges of incorporating human-like biases into RL, offering valuable lessons for developing robust financial AI systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u8ba4\u77e5\u504f\u5dee\u6574\u5408\u5230\u91d1\u878d\u4ea4\u6613\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u63a2\u7d22\u4eba\u7c7b\u5fc3\u7406\u56e0\u7d20\u5bf9AI\u4ea4\u6613\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u5c3d\u7ba1\u7ed3\u679c\u4e0d\u660e\u786e\u6216\u8d1f\u9762\uff0c\u4f46\u4e3a\u5f00\u53d1\u7a33\u5065\u7684\u91d1\u878dAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7ecf\u9a8c\u3002", "motivation": "\u91d1\u878d\u5e02\u573a\u53d7\u5230\u4eba\u7c7b\u975e\u7406\u6027\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5047\u8bbe\u7406\u6027\u4ee3\u7406\u4eba\uff0c\u53ef\u80fd\u5ffd\u89c6\u4e86\u5fc3\u7406\u56e0\u7d20\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5c06\u8ba4\u77e5\u504f\u5dee\u6574\u5408\u5230RL\u6846\u67b6\u4e2d\u662f\u5426\u80fd\u4ea7\u751f\u66f4\u7b26\u5408\u4eba\u7c7b\u884c\u4e3a\u7684\u4ea4\u6613\u7b56\u7565\u3002", "method": "\u5c06\u8fc7\u5ea6\u81ea\u4fe1\u3001\u635f\u5931\u538c\u6076\u7b49\u8ba4\u77e5\u504f\u5dee\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u7ed3\u6784\u548c\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4ea4\u6613\u73af\u5883\u4e2d\u8bc4\u4f30\u8fd9\u4e9b\u5e26\u6709\u504f\u5dee\u7684RL\u4ee3\u7406\u4eba\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e0d\u660e\u786e\u6216\u8d1f\u9762\uff0c\u8868\u660e\u5c06\u4eba\u7c7b\u8ba4\u77e5\u504f\u5dee\u6574\u5408\u5230RL\u6846\u67b6\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u4f46\u4e3a\u7406\u89e3\u504f\u5dee\u5bf9AI\u4ea4\u6613\u7cfb\u7edf\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "conclusion": "\u5c3d\u7ba1\u7ed3\u679c\u4e0d\u7406\u60f3\uff0c\u4f46\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u91d1\u878dAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7ecf\u9a8c\uff0c\u5f3a\u8c03\u4e86\u5728AI\u4ea4\u6613\u6a21\u578b\u4e2d\u8003\u8651\u4eba\u7c7b\u5fc3\u7406\u56e0\u7d20\u7684\u590d\u6742\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08462", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08462", "abs": "https://arxiv.org/abs/2601.08462", "authors": ["Sixiong Xie", "Zhuofan Shi", "Haiyang Shen", "Gang Huang", "Yun Ma", "Xiang Jing"], "title": "M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games", "comment": null, "summary": "As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.", "AI": {"tldr": "M3-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u793e\u4ea4\u884c\u4e3a\u7684\u591a\u9636\u6bb5\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u884c\u4e3a\u8f68\u8ff9\u3001\u63a8\u7406\u8fc7\u7a0b\u548c\u901a\u4fe1\u5185\u5bb9\u4e09\u4e2a\u6a21\u5757\u7684\u534f\u540c\u5206\u6790\uff0c\u7ed3\u5408\u5927\u4e94\u4eba\u683c\u6a21\u578b\u548c\u793e\u4f1a\u4ea4\u6362\u7406\u8bba\uff0c\u63d0\u4f9b\u8d85\u8d8a\u7b80\u5355\u4efb\u52a1\u5206\u6570\u7684\u53ef\u89e3\u91ca\u793e\u4ea4\u884c\u4e3a\u753b\u50cf\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u80fd\u529b\u7ef4\u5ea6\u6216\u4ec5\u4f9d\u8d56\u884c\u4e3a\u7ed3\u679c\uff0c\u5ffd\u89c6\u4e86\u667a\u80fd\u4f53\u51b3\u7b56\u63a8\u7406\u548c\u901a\u4fe1\u4ea4\u4e92\u7684\u4e30\u5bcc\u8fc7\u7a0b\u4fe1\u606f\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u7684\u590d\u6742\u793e\u4ea4\u884c\u4e3a\uff08\u5982\u5408\u4f5c\u3001\u6b3a\u9a97\u3001\u5171\u8c0b\uff09\u3002", "method": "\u63d0\u51faM3-Bench\u591a\u9636\u6bb5\u6df7\u5408\u52a8\u673a\u535a\u5f08\u57fa\u51c6\uff0c\u91c7\u7528\u8fc7\u7a0b\u611f\u77e5\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u540c\u5206\u6790\u6a21\u5757\uff1a\u884c\u4e3a\u8f68\u8ff9\u5206\u6790(BTA)\u3001\u63a8\u7406\u8fc7\u7a0b\u5206\u6790(RPA)\u548c\u901a\u4fe1\u5185\u5bb9\u5206\u6790(CCA)\uff0c\u5e76\u6574\u5408\u5927\u4e94\u4eba\u683c\u6a21\u578b\u548c\u793e\u4f1a\u4ea4\u6362\u7406\u8bba\uff0c\u5c06\u591a\u7ef4\u8bc1\u636e\u805a\u5408\u4e3a\u53ef\u89e3\u91ca\u7684\u793e\u4ea4\u884c\u4e3a\u753b\u50cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cM3-Bench\u80fd\u591f\u53ef\u9760\u5730\u533a\u5206\u4e0d\u540c\u6a21\u578b\u7684\u591a\u6837\u5316\u793e\u4ea4\u884c\u4e3a\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u4e00\u4e9b\u6a21\u578b\u5728\u770b\u4f3c\u5408\u7406\u7684\u884c\u4e3a\u7ed3\u679c\u80cc\u540e\uff0c\u5176\u63a8\u7406\u548c\u901a\u4fe1\u8fc7\u7a0b\u5b58\u5728\u663e\u8457\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "M3-Bench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u793e\u4ea4\u884c\u4e3a\u80fd\u529b\u7684\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u57fa\u4e8e\u7ed3\u679c\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u63ed\u793a\u667a\u80fd\u4f53\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u5185\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u66f4\u6df1\u5165\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.08489", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08489", "abs": "https://arxiv.org/abs/2601.08489", "authors": ["Tony Cristofano"], "title": "Surgical Refusal Ablation: Disentangling Safety from Intelligence via Concept-Guided Spectral Cleaning", "comment": null, "summary": "Safety-aligned language models systematically refuse harmful requests. While activation steering can modulate refusal, ablating the raw \"refusal vector\" calculated from contrastive harmful and harmless prompts often causes collateral damage and distribution drift. We argue this degradation occurs because the raw vector is polysemantic, entangling the refusal signal with core capability circuits and linguistic style.\n  We introduce Surgical Refusal Ablation (SRA) to distill these steering directions. SRA constructs a registry of independent Concept Atoms representing protected capabilities and stylistic confounds, then uses ridge-regularized spectral residualization to orthogonalize the refusal vector against these directions. This yields a clean refusal direction that targets refusal-relevant structure while minimizing disruption to the model's semantic geometry.\n  Across five models (Qwen3-VL and Ministral series), SRA achieves deep refusal reduction (0-2%) with negligible perplexity impact on Wikitext-2 (mean delta PPL approx. 0.02) and minimal distribution drift. Notably, standard ablation on Qwen3-VL-4B induces severe drift (first-token KL = 2.088), whereas SRA maintains the original distribution (KL = 0.044) while achieving the same 0% refusal rate. Using teacher-forced perplexity on GSM8K and MBPP as a high-resolution capability proxy, we show SRA preserves math and code distributions. These results suggest that common \"model damage\" is often \"Ghost Noise,\" defined as the spectral bleeding of the dirty refusal direction into capability subspaces.", "AI": {"tldr": "\u63d0\u51faSurgical Refusal Ablation\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4ea4\u5316\u62d2\u7edd\u5411\u91cf\u6765\u51cf\u5c11\u6709\u5bb3\u8bf7\u6c42\u62d2\u7edd\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u635f\u5bb3", "motivation": "\u73b0\u6709\u6fc0\u6d3b\u5bfc\u5411\u65b9\u6cd5\u5728\u51cf\u5c11\u6a21\u578b\u62d2\u7edd\u6709\u5bb3\u8bf7\u6c42\u65f6\uff0c\u4f1a\u56e0\u62d2\u7edd\u5411\u91cf\u4e0e\u6838\u5fc3\u80fd\u529b\u7535\u8def\u548c\u8bed\u8a00\u98ce\u683c\u7ea0\u7f20\u800c\u5bfc\u81f4\u80fd\u529b\u4e0b\u964d\u548c\u5206\u5e03\u6f02\u79fb", "method": "\u5f15\u5165Surgical Refusal Ablation\u65b9\u6cd5\uff1a\u6784\u5efa\u72ec\u7acb\u6982\u5ff5\u539f\u5b50\u8868\u793a\u53d7\u4fdd\u62a4\u80fd\u529b\u548c\u98ce\u683c\u6df7\u6dc6\uff0c\u4f7f\u7528\u5cad\u6b63\u5219\u5316\u8c31\u6b8b\u5dee\u5316\u5c06\u62d2\u7edd\u5411\u91cf\u6b63\u4ea4\u5316\u4e8e\u8fd9\u4e9b\u65b9\u5411", "result": "\u5728\u4e94\u4e2a\u6a21\u578b\u4e0a\u5b9e\u73b0\u6df1\u5ea6\u62d2\u7edd\u51cf\u5c11(0-2%)\uff0c\u5bf9Wikitext-2\u7684\u56f0\u60d1\u5ea6\u5f71\u54cd\u6781\u5c0f(\u5e73\u5747\u0394PPL\u22480.02)\uff0c\u5206\u5e03\u6f02\u79fb\u6700\u5c0f\uff0c\u4fdd\u6301\u6570\u5b66\u548c\u4ee3\u7801\u80fd\u529b\u5206\u5e03", "conclusion": "\u5e38\u89c1\u7684\"\u6a21\u578b\u635f\u5bb3\"\u901a\u5e38\u662f\"\u5e7d\u7075\u566a\u58f0\"\uff0c\u5373\u810f\u62d2\u7edd\u5411\u91cf\u5411\u80fd\u529b\u5b50\u7a7a\u95f4\u7684\u8c31\u6cc4\u6f0f\uff0cSRA\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898", "topic": "agent analysis"}}
{"id": "2601.08641", "categories": ["cs.AI", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2601.08641", "abs": "https://arxiv.org/abs/2601.08641", "authors": ["Yichen Luo", "Yebo Feng", "Jiahua Xu", "Yang Liu"], "title": "Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning", "comment": null, "summary": "The launch of \\$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.\n  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \\$500,000 across these projects.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7528\u4e8e\u6a21\u56e0\u5e01\u8ddf\u5355\u4ea4\u6613\uff0c\u901a\u8fc7\u5206\u89e3\u590d\u6742\u4efb\u52a1\u5e76\u534f\u8c03\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u5728\u6a21\u56e0\u5e01\u5e02\u573a\u4e2d\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u5355\u4e00LLM\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u6a21\u56e0\u5e01\u8ddf\u5355\u4ea4\u6613\u9762\u4e34\u64cd\u7eb5\u673a\u5668\u4eba\u76db\u884c\u3001\u88ab\u8ddf\u968f\u94b1\u5305\u672a\u6765\u8868\u73b0\u4e0d\u786e\u5b9a\u3001\u4ea4\u6613\u6267\u884c\u5ef6\u8fdf\u7b49\u95ee\u9898\uff0c\u800c\u5355\u4e00LLM\u5728\u5904\u7406\u590d\u6742\u7684\u8d44\u4ea7\u5206\u914d\u4efb\u52a1\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u52a0\u5bc6\u8d27\u5e01\u9886\u57df\u7f3a\u4e4f\u8db3\u591f\u7684\u9886\u57df\u77e5\u8bc6\u3002", "method": "\u53d7\u8d44\u4ea7\u7ba1\u7406\u56e2\u961f\u7ed3\u6784\u542f\u53d1\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u534f\u8c03\u4e13\u4e1a\u667a\u80fd\u4f53\u534f\u4f5c\u89e3\u51b3\u3002\u91c7\u7528\u5c11\u6837\u672c\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u4f7f\u6bcf\u4e2a\u667a\u80fd\u4f53\u83b7\u53d6\u4e13\u4e1a\u6a21\u56e0\u5e01\u4ea4\u6613\u77e5\u8bc6\uff0c\u89e3\u91ca\u591a\u6a21\u6001\u6570\u636e\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u51b3\u7b56\u3002", "result": "\u57281000\u4e2a\u6a21\u56e0\u5e01\u9879\u76ee\u7684\u4ea4\u6613\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8bc6\u522b\u9ad8\u8d28\u91cf\u6a21\u56e0\u5e01\u9879\u76ee\u548c\u5173\u952e\u610f\u89c1\u9886\u8896\u94b1\u5305\u65b9\u9762\u5206\u522b\u8fbe\u523073%\u548c70%\u7684\u7cbe\u786e\u7387\uff0c\u6240\u9009KOL\u5728\u8fd9\u4e9b\u9879\u76ee\u4e2d\u603b\u5171\u4ea7\u751f\u4e8650\u4e07\u7f8e\u5143\u5229\u6da6\u3002", "conclusion": "\u63d0\u51fa\u7684\u53ef\u89e3\u91ca\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u6709\u6548\u89e3\u51b3\u6a21\u56e0\u5e01\u8ddf\u5355\u4ea4\u6613\u7684\u6311\u6218\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u4e13\u4e1a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5728\u590d\u6742\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5355\u4e00LLM\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.08653", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08653", "abs": "https://arxiv.org/abs/2601.08653", "authors": ["Zenghua Liao", "Jinzhi Liao", "Xiang Zhao"], "title": "Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding", "comment": null, "summary": "Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.", "AI": {"tldr": "Prism\u6846\u67b6\u901a\u8fc7\u903b\u8f91\u4f9d\u8d56\u5efa\u6a21\u548c\u8ba4\u77e5\u8d1f\u8f7d\u4f18\u5316\uff0c\u63d0\u5347LLM\u5728\u793e\u4ea4\u5e73\u53f0\u4e0a\u7684\u590d\u6742\u610f\u56fe\u7406\u89e3\u80fd\u529b\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u8fde\u8d2f\u7684\u7528\u6237\u4ea4\u4e92\u3002", "motivation": "\u5f53\u524dLLM\u5728\u793e\u4ea4\u5e73\u53f0\u4f5c\u4e3a\u63a5\u53e3\u65f6\uff0c\u9762\u5bf9\u7528\u6237\u6a21\u7cca\u52a8\u6001\u7684\u76ee\u6807\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u6f84\u6e05\u95ee\u9898\u95f4\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u610f\u56fe\u7406\u89e3\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faPrism\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u6a21\u5757\uff1a\u590d\u6742\u610f\u56fe\u5206\u89e3\u3001\u903b\u8f91\u6f84\u6e05\u751f\u6210\u3001\u610f\u56fe\u611f\u77e5\u5956\u52b1\u3001\u81ea\u8fdb\u5316\u610f\u56fe\u8c03\u4f18\uff0c\u901a\u8fc7\u4f9d\u8d56\u5efa\u6a21\u548c\u8499\u7279\u5361\u6d1b\u91c7\u6837\u4f18\u5316\u4ea4\u4e92\u8d28\u91cf\u3002", "result": "\u5728\u6f84\u6e05\u4ea4\u4e92\u3001\u610f\u56fe\u6267\u884c\u548c\u8ba4\u77e5\u8d1f\u8f7d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u903b\u8f91\u4e00\u81f4\u6027\u8fbeSOTA\uff0c\u903b\u8f91\u51b2\u7a81\u964d\u81f311.5%\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u63d0\u534714.4%\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1134.8%\u3002", "conclusion": "Prism\u901a\u8fc7\u5efa\u6a21\u6f84\u6e05\u95ee\u9898\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u610f\u56fe\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u8fde\u8d2f\u7684\u7528\u6237-LLM\u534f\u4f5c\u3002", "topic": "agent analysis"}}
{"id": "2601.08605", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08605", "abs": "https://arxiv.org/abs/2601.08605", "authors": ["Wenyuan Zhang", "Xinghua Zhang", "Haiyang Yu", "Shuaiyi Nie", "Bingli Wu", "Juwei Yue", "Tingwen Liu", "Yongbin Li"], "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents", "comment": "Work in progress", "summary": "Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.", "AI": {"tldr": "ExpSeek\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ecf\u9a8c\u5e72\u9884\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b65\u7ea7\u4e3b\u52a8\u5bfb\u6c42\u7ecf\u9a8c\u6765\u589e\u5f3aWeb\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u4f7f\u7528\u71b5\u4f5c\u4e3a\u81ea\u89e6\u53d1\u4fe1\u53f7\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7ecf\u9a8c\u5e72\u9884\u65b9\u6cd5\u4e3b\u8981\u5728\u4efb\u52a1\u6267\u884c\u524d\u88ab\u52a8\u6ce8\u5165\u7ecf\u9a8c\u4f5c\u4e3a\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u96be\u4ee5\u9002\u5e94\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u52a8\u6001\u53d8\u5316\u7684\u4e0a\u4e0b\u6587\u89c2\u5bdf\u3002", "method": "\u63d0\u51faExpSeek\u65b9\u6cd5\uff0c\u5c06\u7ecf\u9a8c\u5e72\u9884\u8f6c\u5411\u6b65\u7ea7\u4e3b\u52a8\u5bfb\u6c42\uff1a(1) \u4f7f\u7528\u6a21\u578b\u5185\u5728\u4fe1\u53f7\u4f30\u8ba1\u6b65\u7ea7\u71b5\u9608\u503c\u6765\u786e\u5b9a\u5e72\u9884\u65f6\u673a\uff1b(2) \u8bbe\u8ba1\u6b65\u7ea7\u5b9a\u5236\u5316\u7ecf\u9a8c\u5185\u5bb9\u3002", "result": "\u5728Qwen3-8B\u548c32B\u6a21\u578b\u4e0a\u7684\u56db\u4e2a\u6311\u6218\u6027Web\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cExpSeek\u5206\u522b\u5b9e\u73b0\u4e869.3%\u548c7.5%\u7684\u7edd\u5bf9\u63d0\u5347\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u71b5\u4f5c\u4e3a\u81ea\u89e6\u53d1\u4fe1\u53f7\u7684\u53ef\u884c\u6027\uff0c\u5e76\u663e\u793a\u5373\u4f7f\u662f4B\u5c0f\u89c4\u6a21\u7ecf\u9a8c\u6a21\u578b\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u66f4\u5927\u667a\u80fd\u4f53\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "ExpSeek\u901a\u8fc7\u6b65\u7ea7\u4e3b\u52a8\u5bfb\u6c42\u7ecf\u9a8c\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7ecf\u9a8c\u5e72\u9884\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86Web\u667a\u80fd\u4f53\u7684\u4ea4\u4e92\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.08621", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08621", "abs": "https://arxiv.org/abs/2601.08621", "authors": ["Jiajin Liu", "Yuanfu Sun", "Dongzhe Fan", "Qiaoyu Tan"], "title": "GraphSearch: Agentic Search-Augmented Reasoning for Zero-Shot Graph Learning", "comment": "16 pages, 5 pages", "summary": "Recent advances in search-augmented large reasoning models (LRMs) enable the retrieval of external knowledge to reduce hallucinations in multistep reasoning. However, their ability to operate on graph-structured data, prevalent in domains such as e-commerce, social networks, and scientific citations, remains underexplored. Unlike plain text corpora, graphs encode rich topological signals that connect related entities and can serve as valuable priors for retrieval, enabling more targeted search and improved reasoning efficiency. Yet, effectively leveraging such structure poses unique challenges, including the difficulty of generating graph-expressive queries and ensuring reliable retrieval that balances structural and semantic relevance. To address this gap, we introduce GraphSearch, the first framework that extends search-augmented reasoning to graph learning, enabling zero-shot graph learning without task-specific fine-tuning. GraphSearch combines a Graph-aware Query Planner, which disentangles search space (e.g., 1-hop, multi-hop, or global neighbors) from semantic queries, with a Graph-aware Retriever, which constructs candidate sets based on topology and ranks them using a hybrid scoring function. We further instantiate two traversal modes: GraphSearch-R, which recursively expands neighborhoods hop by hop, and GraphSearch-F, which flexibly retrieves across local and global neighborhoods without hop constraints. Extensive experiments across diverse benchmarks show that GraphSearch achieves competitive or even superior performance compared to supervised graph learning methods, setting state-of-the-art results in zero-shot node classification and link prediction. These findings position GraphSearch as a flexible and generalizable paradigm for agentic reasoning over graphs.", "AI": {"tldr": "GraphSearch\uff1a\u9996\u4e2a\u5c06\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u6269\u5c55\u5230\u56fe\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u96f6\u6837\u672c\u56fe\u5b66\u4e60\uff0c\u5728\u56fe\u7ed3\u6784\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u641c\u7d22\u589e\u5f3a\u5927\u63a8\u7406\u6a21\u578b\u4e3b\u8981\u5904\u7406\u6587\u672c\u6570\u636e\uff0c\u4f46\u5728\u56fe\u7ed3\u6784\u6570\u636e\uff08\u5982\u7535\u5546\u3001\u793e\u4ea4\u7f51\u7edc\u3001\u79d1\u5b66\u5f15\u7528\uff09\u4e0a\u7684\u5e94\u7528\u4e0d\u8db3\u3002\u56fe\u6570\u636e\u5305\u542b\u4e30\u5bcc\u7684\u62d3\u6251\u4fe1\u53f7\uff0c\u53ef\u4f5c\u4e3a\u68c0\u7d22\u7684\u6709\u4ef7\u503c\u5148\u9a8c\uff0c\u4f46\u6709\u6548\u5229\u7528\u56fe\u7ed3\u6784\u9762\u4e34\u751f\u6210\u56fe\u8868\u8fbe\u67e5\u8be2\u548c\u5e73\u8861\u7ed3\u6784\u8bed\u4e49\u76f8\u5173\u6027\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faGraphSearch\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u56fe\u611f\u77e5\u67e5\u8be2\u89c4\u5212\u5668\uff0c\u5c06\u641c\u7d22\u7a7a\u95f4\uff081\u8df3\u3001\u591a\u8df3\u6216\u5168\u5c40\u90bb\u5c45\uff09\u4e0e\u8bed\u4e49\u67e5\u8be2\u89e3\u8026\uff1b2\uff09\u56fe\u611f\u77e5\u68c0\u7d22\u5668\uff0c\u57fa\u4e8e\u62d3\u6251\u6784\u5efa\u5019\u9009\u96c6\u5e76\u4f7f\u7528\u6df7\u5408\u8bc4\u5206\u51fd\u6570\u6392\u5e8f\u3002\u5177\u4f53\u5b9e\u73b0\u4e24\u79cd\u904d\u5386\u6a21\u5f0f\uff1aGraphSearch-R\uff08\u9012\u5f52\u9010\u8df3\u6269\u5c55\u90bb\u57df\uff09\u548cGraphSearch-F\uff08\u7075\u6d3b\u68c0\u7d22\u5c40\u90e8\u548c\u5168\u5c40\u90bb\u57df\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGraphSearch\u5728\u96f6\u6837\u672c\u8282\u70b9\u5206\u7c7b\u548c\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u751a\u81f3\u4f18\u4e8e\u76d1\u7763\u56fe\u5b66\u4e60\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "GraphSearch\u4e3a\u56fe\u4e0a\u7684\u667a\u80fd\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6cdb\u5316\u7684\u8303\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u56fe\u7ed3\u6784\u8fdb\u884c\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u3002", "topic": "agent analysis"}}
{"id": "2601.08676", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08676", "abs": "https://arxiv.org/abs/2601.08676", "authors": ["Yilei Zhao", "Wentao Zhang", "Xiao Lei", "Yandan Zheng", "Mengpu Liu", "Wei Yang Bryan Lim"], "title": "Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance", "comment": null, "summary": "Environmental, social, and governance (ESG) criteria are essential for evaluating corporate sustainability and ethical performance. However, professional ESG analysis is hindered by data fragmentation across unstructured sources, and existing large language models (LLMs) often struggle with the complex, multi-step workflows required for rigorous auditing. To address these limitations, we introduce ESGAgent, a hierarchical multi-agent system empowered by a specialized toolset, including retrieval augmentation, web search and domain-specific functions, to generate in-depth ESG analysis. Complementing this agentic system, we present a comprehensive three-level benchmark derived from 310 corporate sustainability reports, designed to evaluate capabilities ranging from atomic common-sense questions to the generation of integrated, in-depth analysis. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs with an average accuracy of 84.15% on atomic question-answering tasks, and excels in professional report generation by integrating rich charts and verifiable references. These findings confirm the diagnostic value of our benchmark, establishing it as a vital testbed for assessing general and advanced agentic capabilities in high-stakes vertical domains.", "AI": {"tldr": "ESGAgent\uff1a\u57fa\u4e8e\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4e13\u4e1aESG\u5206\u6790\u6846\u67b6\uff0c\u914d\u5907\u4e13\u7528\u5de5\u5177\u96c6\uff0c\u5728ESG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u95ed\u6e90\u5927\u6a21\u578b", "motivation": "\u4e13\u4e1aESG\u5206\u6790\u9762\u4e34\u6570\u636e\u5206\u6563\u4e8e\u975e\u7ed3\u6784\u5316\u6765\u6e90\u7684\u6311\u6218\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u591a\u6b65\u9aa4\u5ba1\u8ba1\u5de5\u4f5c\u6d41\uff0c\u9700\u8981\u4e13\u95e8\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347ESG\u5206\u6790\u7684\u8d28\u91cf\u548c\u6548\u7387", "method": "\u63d0\u51faESGAgent\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u914d\u5907\u68c0\u7d22\u589e\u5f3a\u3001\u7f51\u7edc\u641c\u7d22\u548c\u9886\u57df\u7279\u5b9a\u51fd\u6570\u7b49\u4e13\u7528\u5de5\u5177\u96c6\uff1b\u540c\u65f6\u6784\u5efa\u57fa\u4e8e310\u4efd\u4f01\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u62a5\u544a\u7684\u4e09\u7ea7\u57fa\u51c6\u6d4b\u8bd5", "result": "ESGAgent\u5728\u539f\u5b50\u95ee\u7b54\u4efb\u52a1\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u8fbe84.15%\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u95ed\u6e90\u5927\u6a21\u578b\uff1b\u5728\u4e13\u4e1a\u62a5\u544a\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u6574\u5408\u4e30\u5bcc\u56fe\u8868\u548c\u53ef\u9a8c\u8bc1\u5f15\u7528", "conclusion": "ESGAgent\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86ESG\u5206\u6790\u7684\u4e13\u4e1a\u9700\u6c42\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30\u9ad8\u98ce\u9669\u5782\u76f4\u9886\u57df\u7684\u901a\u7528\u548c\u9ad8\u7ea7\u667a\u80fd\u4f53\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u6d4b\u8bd5\u5e73\u53f0", "topic": "agent analysis"}}
{"id": "2601.08654", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08654", "abs": "https://arxiv.org/abs/2601.08654", "authors": ["Yihan Hong", "Huaiyuan Yao", "Bolin Shen", "Wanpeng Xu", "Hua Wei", "Yushun Dong"], "title": "RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation", "comment": null, "summary": "The LLM-as-a-Judge paradigm promises scalable rubric-based evaluation, yet aligning frozen black-box models with human standards remains a challenge due to inherent generation stochasticity. We reframe judge alignment as a criteria transfer problem and isolate three recurrent failure modes: rubric instability caused by prompt sensitivity, unverifiable reasoning that lacks auditable evidence, and scale misalignment with human grading boundaries. To address these issues, we introduce RULERS (Rubric Unification, Locking, and Evidence-anchored Robust Scoring), a compiler-executor framework that transforms natural language rubrics into executable specifications. RULERS operates by compiling criteria into versioned immutable bundles, enforcing structured decoding with deterministic evidence verification, and applying lightweight Wasserstein-based post-hoc calibration, all without updating model parameters. Extensive experiments on essay and summarization benchmarks demonstrate that RULERS significantly outperforms representative baselines in human agreement, maintains strong stability against adversarial rubric perturbations, and enables smaller models to rival larger proprietary judges. Overall, our results suggest that reliable LLM judging requires executable rubrics, verifiable evidence, and calibrated scales rather than prompt phrasing alone. Code is available at https://github.com/LabRAI/Rulers.git.", "AI": {"tldr": "RULERS\u6846\u67b6\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u8bc4\u5206\u6807\u51c6\u7f16\u8bd1\u4e3a\u53ef\u6267\u884c\u89c4\u8303\uff0c\u89e3\u51b3LLM\u8bc4\u5224\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u3001\u4e0d\u53ef\u9a8c\u8bc1\u63a8\u7406\u548c\u5c3a\u5ea6\u9519\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u3002", "motivation": "LLM-as-a-Judge\u8303\u5f0f\u867d\u7136\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u6807\u51c6\u7684\u8bc4\u4f30\uff0c\u4f46\u7531\u4e8e\u751f\u6210\u968f\u673a\u6027\uff0c\u51bb\u7ed3\u7684\u9ed1\u76d2\u6a21\u578b\u4e0e\u4eba\u7c7b\u6807\u51c6\u5bf9\u9f50\u4ecd\u7136\u56f0\u96be\u3002\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u63d0\u793a\u654f\u611f\u6027\u5bfc\u81f4\u7684\u8bc4\u5206\u6807\u51c6\u4e0d\u7a33\u5b9a\u6027\u3001\u7f3a\u4e4f\u53ef\u5ba1\u8ba1\u8bc1\u636e\u7684\u4e0d\u53ef\u9a8c\u8bc1\u63a8\u7406\u3001\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u8bc4\u5206\u8fb9\u754c\u7684\u5c3a\u5ea6\u9519\u914d\u3002", "method": "\u63d0\u51faRULERS\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u5c06\u81ea\u7136\u8bed\u8a00\u8bc4\u5206\u6807\u51c6\u7f16\u8bd1\u4e3a\u7248\u672c\u5316\u4e0d\u53ef\u53d8\u5305\uff1b2) \u901a\u8fc7\u7ed3\u6784\u5316\u89e3\u7801\u5f3a\u5236\u786e\u5b9a\u6027\u8bc1\u636e\u9a8c\u8bc1\uff1b3) \u5e94\u7528\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u8f7b\u91cf\u7ea7\u540e\u6821\u51c6\uff0c\u6240\u6709\u64cd\u4f5c\u5747\u4e0d\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5728\u8bba\u6587\u548c\u6458\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRULERS\u5728\u4eba\u7c7b\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5bf9\u5bf9\u6297\u6027\u8bc4\u5206\u6807\u51c6\u6270\u52a8\u4fdd\u6301\u5f3a\u7a33\u5b9a\u6027\uff0c\u5e76\u4f7f\u8f83\u5c0f\u6a21\u578b\u80fd\u591f\u4e0e\u8f83\u5927\u7684\u4e13\u6709\u8bc4\u5224\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u53ef\u9760\u7684LLM\u8bc4\u5224\u9700\u8981\u53ef\u6267\u884c\u7684\u8bc4\u5206\u6807\u51c6\u3001\u53ef\u9a8c\u8bc1\u7684\u8bc1\u636e\u548c\u6821\u51c6\u7684\u5c3a\u5ea6\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u63d0\u793a\u8bcd\u63aa\u8f9e\u3002RULERS\u6846\u67b6\u901a\u8fc7\u7f16\u8bd1\u5668-\u6267\u884c\u5668\u67b6\u6784\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\u3002", "topic": "agent analysis"}}
{"id": "2601.08731", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08731", "abs": "https://arxiv.org/abs/2601.08731", "authors": ["Yuanlin Duan", "Yuning Wang", "Wenjie Qiu", "He Zhu"], "title": "Learning from Demonstrations via Capability-Aware Goal Sampling", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.", "AI": {"tldr": "Cago\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8ddf\u8e2a\u667a\u80fd\u4f53\u5728\u4e13\u5bb6\u8f68\u8ff9\u4e0a\u7684\u80fd\u529b\uff0c\u9009\u62e9\u521a\u597d\u8d85\u51fa\u5f53\u524d\u80fd\u529b\u8303\u56f4\u7684\u4e2d\u95f4\u76ee\u6807\u6765\u5f15\u5bfc\u5b66\u4e60\uff0c\u4ece\u800c\u7f13\u89e3\u5bf9\u4e13\u5bb6\u8f68\u8ff9\u7684\u8106\u5f31\u4f9d\u8d56\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u957f\u65f6\u57df\u73af\u5883\u4e2d\u7ecf\u5e38\u5931\u8d25\uff0c\u56e0\u4e3a\u5b8c\u7f8e\u590d\u5236\u6f14\u793a\u4e0d\u73b0\u5b9e\uff0c\u5c0f\u9519\u8bef\u4f1a\u707e\u96be\u6027\u7d2f\u79ef\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4ec5\u7528\u6f14\u793a\u521d\u59cb\u5316\u7b56\u7565\uff0c\u8981\u4e48\u7528\u4e8e\u5956\u52b1\u5851\u5f62\uff0c\u4f46\u90fd\u8fc7\u5ea6\u4f9d\u8d56\u4e13\u5bb6\u8f68\u8ff9\u3002", "method": "Cago\uff08\u80fd\u529b\u611f\u77e5\u76ee\u6807\u91c7\u6837\uff09\u65b9\u6cd5\uff1a1\uff09\u52a8\u6001\u8ddf\u8e2a\u667a\u80fd\u4f53\u5728\u4e13\u5bb6\u8f68\u8ff9\u4e0a\u7684\u80fd\u529b\uff1b2\uff09\u9009\u62e9\u521a\u597d\u8d85\u51fa\u667a\u80fd\u4f53\u5f53\u524d\u80fd\u529b\u8303\u56f4\u7684\u4e2d\u95f4\u6b65\u9aa4\uff08\u76ee\u6807\uff09\uff1b3\uff09\u5f62\u6210\u81ea\u9002\u5e94\u8bfe\u7a0b\uff0c\u5f15\u5bfc\u5b66\u4e60\u9010\u6b65\u89e3\u51b3\u5b8c\u6574\u4efb\u52a1\u3002", "result": "Cago\u5728\u7a00\u758f\u5956\u52b1\u3001\u76ee\u6807\u6761\u4ef6\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\uff0c\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u7684\u6a21\u4eff\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Cago\u901a\u8fc7\u80fd\u529b\u611f\u77e5\u7684\u76ee\u6807\u91c7\u6837\u673a\u5236\uff0c\u521b\u5efa\u81ea\u9002\u5e94\u8bfe\u7a0b\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6a21\u4eff\u5b66\u4e60\u5bf9\u4e13\u5bb6\u8f68\u8ff9\u7684\u8106\u5f31\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u7684\u957f\u65f6\u57df\u4efb\u52a1\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08682", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08682", "abs": "https://arxiv.org/abs/2601.08682", "authors": ["Kushal Chawla", "Chenyang Zhu", "Pengshan Cai", "Sangwoo Cho", "Scott Novotney", "Ayushman Singh", "Jonah Lewis", "Keasha Safewright", "Alfy Samuel", "Erin Babinsky", "Shi-Xiong Zhang", "Sambit Sahu"], "title": "Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization", "comment": "EACL 2026 Industry Track", "summary": "Summarization of multi-party dialogues is a critical capability in industry, enhancing knowledge transfer and operational effectiveness across many domains. However, automatically generating high-quality summaries is challenging, as the ideal summary must satisfy a set of complex, multi-faceted requirements. While summarization has received immense attention in research, prior work has primarily utilized static datasets and benchmarks, a condition rare in practical scenarios where requirements inevitably evolve. In this work, we present an industry case study on developing an agentic system to summarize multi-party interactions. We share practical insights spanning the full development lifecycle to guide practitioners in building reliable, adaptable summarization systems, as well as to inform future research, covering: 1) robust methods for evaluation despite evolving requirements and task subjectivity, 2) component-wise optimization enabled by the task decomposition inherent in an agentic architecture, 3) the impact of upstream data bottlenecks, and 4) the realities of vendor lock-in due to the poor transferability of LLM prompts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u603b\u7ed3\u591a\u65b9\u5bf9\u8bdd\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5206\u4eab\u4e86\u4ece\u8bc4\u4f30\u65b9\u6cd5\u5230\u5b9e\u9645\u90e8\u7f72\u7684\u5168\u751f\u547d\u5468\u671f\u5b9e\u8df5\u7ecf\u9a8c\u3002", "motivation": "\u591a\u65b9\u5bf9\u8bdd\u6458\u8981\u5bf9\u77e5\u8bc6\u4f20\u9012\u548c\u8fd0\u8425\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f7f\u7528\u9759\u6001\u6570\u636e\u96c6\uff0c\u800c\u5b9e\u9645\u573a\u666f\u4e2d\u9700\u6c42\u4f1a\u4e0d\u65ad\u6f14\u53d8\uff0c\u9700\u8981\u6784\u5efa\u53ef\u9760\u3001\u9002\u5e94\u6027\u5f3a\u7684\u6458\u8981\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u667a\u80fd\u4f53\u67b6\u6784\u5f00\u53d1\u591a\u65b9\u5bf9\u8bdd\u6458\u8981\u7cfb\u7edf\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u5b9e\u73b0\u7ec4\u4ef6\u5316\u4f18\u5316\uff0c\u5e76\u5206\u4eab\u4e86\u5305\u62ec\u8bc4\u4f30\u65b9\u6cd5\u3001\u6570\u636e\u74f6\u9888\u5904\u7406\u3001\u4f9b\u5e94\u5546\u9501\u5b9a\u95ee\u9898\u7b49\u5168\u751f\u547d\u5468\u671f\u5b9e\u8df5\u7ecf\u9a8c\u3002", "result": "\u63d0\u51fa\u4e86\u5728\u9700\u6c42\u6f14\u53d8\u548c\u4efb\u52a1\u4e3b\u89c2\u6027\u60c5\u51b5\u4e0b\u6784\u5efa\u53ef\u9760\u6458\u8981\u7cfb\u7edf\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u667a\u80fd\u4f53\u67b6\u6784\u5728\u7ec4\u4ef6\u4f18\u5316\u3001\u6570\u636e\u74f6\u9888\u8bc6\u522b\u548c\u63d0\u793a\u8bcd\u53ef\u79fb\u690d\u6027\u65b9\u9762\u7684\u4f18\u52bf\u4e0e\u6311\u6218\u3002", "conclusion": "\u667a\u80fd\u4f53\u67b6\u6784\u4e3a\u591a\u65b9\u5bf9\u8bdd\u6458\u8981\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u4e2d\u9700\u8981\u8003\u8651\u8bc4\u4f30\u65b9\u6cd5\u3001\u6570\u636e\u8d28\u91cf\u3001\u63d0\u793a\u8bcd\u53ef\u79fb\u690d\u6027\u7b49\u73b0\u5b9e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.08646", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08646", "abs": "https://arxiv.org/abs/2601.08646", "authors": ["Abhijit Mazumdar", "Rafal Wisniewski", "Manuela L. Bujorianu"], "title": "Provably Safe Reinforcement Learning using Entropy Regularizer", "comment": null, "summary": "We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u4e50\u89c2\u9762\u5bf9\u4e0d\u786e\u5b9a\u6027(OFU)\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5176\u4e2d\u7b2c\u4e8c\u79cd\u7b97\u6cd5\u5f15\u5165\u71b5\u6b63\u5219\u5316\u6765\u6539\u5584\u9057\u61be\u754c\u9650\u5e76\u63a7\u5236episode\u95f4\u53d8\u5f02\u6027", "motivation": "\u89e3\u51b3\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u5e26\u6709\u5b89\u5168\u7ea6\u675f\u7684\u6700\u4f18\u7b56\u7565\u5b66\u4e60\u95ee\u9898\uff0c\u5728reach-avoid\u6846\u67b6\u4e0b\u8bbe\u8ba1\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u786e\u4fdd\u5b66\u4e60\u9636\u6bb5\u4ee5\u4efb\u610f\u9ad8\u6982\u7387\u6ee1\u8db3\u5b89\u5168\u7ea6\u675f", "method": "\u9996\u5148\u63d0\u51fa\u57fa\u4e8eOFU\u539f\u5219\u7684\u7b97\u6cd5\uff0c\u7136\u540e\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e3b\u8981\u7b97\u6cd5\uff0c\u5f15\u5165\u71b5\u6b63\u5219\u5316\u6280\u672f\uff0c\u5e76\u5bf9\u4e24\u79cd\u7b97\u6cd5\u8fdb\u884c\u6709\u9650\u6837\u672c\u5206\u6790", "result": "\u63a8\u5bfc\u4e86\u4e24\u79cd\u7b97\u6cd5\u7684\u9057\u61be\u754c\u9650\uff0c\u8bc1\u660e\u71b5\u6b63\u5219\u5316\u7684\u5f15\u5165\u6539\u5584\u4e86\u9057\u61be\u754c\u9650\uff0c\u5e76\u663e\u8457\u63a7\u5236\u4e86OFU\u57fa\u5b89\u5168RL\u7b97\u6cd5\u56fa\u6709\u7684episode\u95f4\u53d8\u5f02\u6027", "conclusion": "\u71b5\u6b63\u5219\u5316\u5728\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4e0d\u4ec5\u80fd\u6539\u5584\u7b97\u6cd5\u6027\u80fd\uff0c\u8fd8\u80fd\u589e\u5f3a\u5b66\u4e60\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2601.08778", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.08778", "abs": "https://arxiv.org/abs/2601.08778", "authors": ["Tengjun Jin", "Yoojin Choi", "Yuxuan Zhu", "Daniel Kang"], "title": "Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards", "comment": "18 pages, 14 figures, 9 tables", "summary": "Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.\n  In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\u4e24\u4e2a\u4e3b\u6d41text-to-SQL\u57fa\u51c6\u6d4b\u8bd5(BIRD\u548cSpider 2.0-Snow)\u5b58\u5728\u9ad8\u6807\u6ce8\u9519\u8bef\u7387(52.8%\u548c62.8%)\uff0c\u8fd9\u4e9b\u9519\u8bef\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u548c\u6392\u884c\u699c\u6392\u540d\uff0c\u53ef\u80fd\u8bef\u5bfc\u7814\u7a76\u65b9\u5411\u9009\u62e9\u3002", "motivation": "\u6587\u672c\u5230SQL\u6280\u672f\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f46\u6807\u6ce8\u9519\u8bef\u7684\u666e\u904d\u6027\u53ca\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u548c\u6392\u884c\u699c\u6392\u540d\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u7814\u7a76\u8005\u9700\u8981\u4e86\u89e3\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\uff0c\u4ee5\u786e\u4fdd\u8bc4\u4f30\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u5206\u6790\u91cf\u5316\u4e24\u4e2a\u4e3b\u6d41text-to-SQL\u57fa\u51c6\u6d4b\u8bd5\u7684\u6807\u6ce8\u9519\u8bef\u7387\uff1b\u4fee\u6b63BIRD\u5f00\u53d1\u96c6\u5b50\u96c6\uff1b\u5728\u539f\u59cb\u548c\u4fee\u6b63\u540e\u7684\u6570\u636e\u96c6\u4e0a\u91cd\u65b0\u8bc4\u4f3016\u4e2a\u5f00\u6e90\u4ee3\u7406\uff1b\u5206\u6790\u6027\u80fd\u53d8\u5316\u548c\u6392\u540d\u53d8\u5316\uff1b\u4f7f\u7528\u76f8\u5173\u6027\u5206\u6790\u8bc4\u4f30\u7ed3\u679c\u5728\u5b8c\u6574\u5f00\u53d1\u96c6\u4e0a\u7684\u6cdb\u5316\u6027\u3002", "result": "BIRD Mini-Dev\u548cSpider 2.0-Snow\u7684\u9519\u8bef\u7387\u5206\u522b\u4e3a52.8%\u548c62.8%\uff1b\u5728\u4fee\u6b63\u540e\u7684BIRD\u5b50\u96c6\u4e0a\uff0c\u4ee3\u7406\u6027\u80fd\u76f8\u5bf9\u53d8\u5316\u8303\u56f4\u4e3a-7%\u523031%\uff0c\u6392\u540d\u53d8\u5316\u8303\u56f4\u4e3a-9\u5230+9\u4f4d\uff1b\u539f\u59cb\u5b50\u96c6\u4e0e\u5b8c\u6574\u5f00\u53d1\u96c6\u7684\u6392\u540d\u5f3a\u76f8\u5173(r_s=0.85)\uff0c\u800c\u4e0e\u4fee\u6b63\u5b50\u96c6\u7684\u76f8\u5173\u6027\u5f31(r_s=0.32)\u3002", "conclusion": "\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6807\u6ce8\u9519\u8bef\u4f1a\u663e\u8457\u626d\u66f2\u62a5\u544a\u7684\u6027\u80fd\u548c\u6392\u540d\u7ed3\u679c\uff0c\u53ef\u80fd\u8bef\u5bfc\u7814\u7a76\u65b9\u5411\u548c\u90e8\u7f72\u51b3\u7b56\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u63d0\u9ad8\u57fa\u51c6\u6d4b\u8bd5\u6807\u6ce8\u8d28\u91cf\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4fee\u6b63\u540e\u7684\u6570\u636e\u96c6\u4f9b\u793e\u533a\u4f7f\u7528\u3002", "topic": "swe benchmark"}}
{"id": "2601.08785", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08785", "abs": "https://arxiv.org/abs/2601.08785", "authors": ["Jieying Chen", "Karen de Jong", "Andreas Poole", "Jan Burakowski", "Elena Elderson Nosti", "Joep Windt", "Chendi Wang"], "title": "Uncovering Political Bias in Large Language Models using Parliamentary Voting Records", "comment": null, "summary": "As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06\u6a21\u578b\u751f\u6210\u7684\u6295\u7968\u9884\u6d4b\u4e0e\u771f\u5b9e\u8bae\u4f1a\u6295\u7968\u8bb0\u5f55\u5bf9\u9f50\u6765\u6784\u5efa\u653f\u6cbb\u504f\u89c1\u57fa\u51c6\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5e76\u5728\u8377\u5170\u3001\u632a\u5a01\u548c\u897f\u73ed\u7259\u4e09\u4e2a\u56fd\u5bb6\u6848\u4f8b\u4e2d\u5b9e\u4f8b\u5316\uff0c\u53d1\u73b0\u6700\u5148\u8fdb\u7684LLM\u666e\u904d\u8868\u73b0\u51fa\u5de6\u503e\u6216\u4e2d\u95f4\u503e\u5411\uff0c\u5e76\u5bf9\u53f3\u7ffc\u4fdd\u5b88\u653f\u515a\u5b58\u5728\u660e\u663e\u8d1f\u9762\u504f\u89c1\u3002", "motivation": "\u968f\u7740LLM\u5728\u6570\u5b57\u5e73\u53f0\u548c\u51b3\u7b56\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u5e94\u7528\uff0c\u5bf9\u5176\u653f\u6cbb\u504f\u89c1\u7684\u62c5\u5fe7\u65e5\u76ca\u589e\u957f\u3002\u5c3d\u7ba1\u5df2\u6709\u5927\u91cf\u5173\u4e8e\u6027\u522b\u548c\u79cd\u65cf\u7b49\u793e\u4f1a\u504f\u89c1\u7684\u7814\u7a76\uff0c\u4f46\u5bf9\u653f\u6cbb\u504f\u89c1\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u4ecd\u7136\u6709\u9650\uff0c\u5c3d\u7ba1\u5176\u5bf9\u793e\u4f1a\u6709\u76f4\u63a5\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u6784\u5efa\u653f\u6cbb\u504f\u89c1\u57fa\u51c6\u7684\u901a\u7528\u65b9\u6cd5\uff1a\u5c06\u6a21\u578b\u751f\u6210\u7684\u6295\u7968\u9884\u6d4b\u4e0e\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u8bae\u4f1a\u6295\u7968\u8bb0\u5f55\u5bf9\u9f50\u3002\u5728\u4e09\u4e2a\u56fd\u5bb6\u6848\u4f8b\u4e2d\u5b9e\u4f8b\u5316\uff1aPoliBiasNL\uff08\u8377\u5170\uff09\u3001PoliBiasNO\uff08\u632a\u5a01\uff09\u548cPoliBiasES\uff08\u897f\u73ed\u7259\uff09\u3002\u63d0\u51fa\u4e00\u79cd\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u5c06LLM\u548c\u653f\u515a\u7684\u610f\u8bc6\u5f62\u6001\u6620\u5c04\u5230\u4e8c\u7ef4CHES\u7a7a\u95f4\uff0c\u5b9e\u73b0\u6a21\u578b\u4e0e\u771f\u5b9e\u653f\u6cbb\u884c\u4e3a\u8005\u7684\u76f4\u63a5\u53ef\u89e3\u91ca\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u7ec6\u7c92\u5ea6\u7684\u610f\u8bc6\u5f62\u6001\u533a\u522b\uff1a\u6700\u5148\u8fdb\u7684LLM\u4e00\u81f4\u8868\u73b0\u51fa\u5de6\u503e\u6216\u4e2d\u95f4\u503e\u5411\uff0c\u540c\u65f6\u5bf9\u53f3\u7ffc\u4fdd\u5b88\u653f\u515a\u5b58\u5728\u660e\u663e\u7684\u8d1f\u9762\u504f\u89c1\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u57fa\u4e8e\u771f\u5b9e\u8bae\u4f1a\u884c\u4e3a\u7684\u900f\u660e\u3001\u8de8\u56fd\u8bc4\u4f30\u5bf9\u4e8e\u7406\u89e3\u548c\u5ba1\u8ba1\u73b0\u4ee3LLM\u653f\u6cbb\u504f\u89c1\u7684\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2601.08699", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08699", "abs": "https://arxiv.org/abs/2601.08699", "authors": ["Zhengwei Tao", "Bo Li", "Jialong Wu", "Guochen Yan", "Huanyao Zhang", "Jiahao Xu", "Haitao Mi", "Wentao Zhang"], "title": "RAGShaper: Eliciting Sophisticated Agentic RAG Skills via Automated Data Synthesis", "comment": null, "summary": "Agentic Retrieval-Augmented Generation (RAG) empowers large language models to autonomously plan and retrieve information for complex problem-solving. However, the development of robust agents is hindered by the scarcity of high-quality training data that reflects the noise and complexity of real-world retrieval environments. Conventional manual annotation is unscalable and often fails to capture the dynamic reasoning strategies required to handle retrieval failures. To bridge this gap, we introduce RAGShaper, a novel data synthesis framework designed to automate the construction of RAG tasks and robust agent trajectories. RAGShaper incorporates an InfoCurator to build dense information trees enriched with adversarial distractors spanning Perception and Cognition levels. Furthermore, we propose a constrained navigation strategy that forces a teacher agent to confront these distractors, thereby eliciting trajectories that explicitly demonstrate error correction and noise rejection. Comprehensive experiments confirm that models trained on our synthesized corpus significantly outperform existing baselines, exhibiting superior robustness in noise-intensive and complex retrieval tasks.", "AI": {"tldr": "RAGShaper\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u5408\u6210RAG\u4efb\u52a1\u548c\u667a\u80fd\u4f53\u8f68\u8ff9\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b\u5bf9\u6297\u6027\u5e72\u6270\u7684\u4fe1\u606f\u6811\u548c\u7ea6\u675f\u5bfc\u822a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u566a\u58f0\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53RAG\u7cfb\u7edf\u9762\u4e34\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4eba\u5de5\u6807\u6ce8\u65e0\u6cd5\u89c4\u6a21\u5316\u4e14\u96be\u4ee5\u6355\u6349\u5904\u7406\u68c0\u7d22\u5931\u8d25\u6240\u9700\u7684\u52a8\u6001\u63a8\u7406\u7b56\u7565\uff0c\u963b\u788d\u4e86\u9c81\u68d2\u667a\u80fd\u4f53\u7684\u5f00\u53d1\u3002", "method": "\u63d0\u51faRAGShaper\u6846\u67b6\uff1a1) InfoCurator\u6784\u5efa\u5305\u542b\u611f\u77e5\u548c\u8ba4\u77e5\u5c42\u9762\u5bf9\u6297\u6027\u5e72\u6270\u7684\u5bc6\u96c6\u4fe1\u606f\u6811\uff1b2) \u7ea6\u675f\u5bfc\u822a\u7b56\u7565\u5f3a\u5236\u6559\u5e08\u667a\u80fd\u4f53\u9762\u5bf9\u8fd9\u4e9b\u5e72\u6270\uff0c\u751f\u6210\u5c55\u793a\u9519\u8bef\u7ea0\u6b63\u548c\u566a\u58f0\u62d2\u7edd\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u7efc\u5408\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528\u5408\u6210\u8bed\u6599\u5e93\u8bad\u7ec3\u7684\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u566a\u58f0\u5bc6\u96c6\u548c\u590d\u6742\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "RAGShaper\u901a\u8fc7\u81ea\u52a8\u5316\u6570\u636e\u5408\u6210\u6709\u6548\u89e3\u51b3\u4e86RAG\u667a\u80fd\u4f53\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u751f\u6210\u7684\u5bf9\u6297\u6027\u8bad\u7ec3\u6570\u636e\u80fd\u591f\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u566a\u58f0\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.08726", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08726", "abs": "https://arxiv.org/abs/2601.08726", "authors": ["Bert Verbruggen", "Arne Vanhoyweghen", "Vincent Ginis"], "title": "Model-Agnostic Solutions for Deep Reinforcement Learning in Non-Ergodic Contexts", "comment": null, "summary": "Reinforcement Learning (RL) remains a central optimisation framework in machine learning. Although RL agents can converge to optimal solutions, the definition of ``optimality'' depends on the environment's statistical properties. The Bellman equation, central to most RL algorithms, is formulated in terms of expected values of future rewards. However, when ergodicity is broken, long-term outcomes depend on the specific trajectory rather than on the ensemble average. In such settings, the ensemble average diverges from the time-average growth experienced by individual agents, with expected-value formulations yielding systematically suboptimal policies. Prior studies demonstrated that traditional RL architectures fail to recover the true optimum in non-ergodic environments. We extend this analysis to deep RL implementations and show that these, too, produce suboptimal policies under non-ergodic dynamics. Introducing explicit time dependence into the learning process can correct this limitation. By allowing the network's function approximation to incorporate temporal information, the agent can estimate value functions consistent with the process's intrinsic growth rate. This improvement does not require altering the environmental feedback, such as reward transformations or modified objective functions, but arises naturally from the agent's exposure to temporal trajectories. Our results contribute to the growing body of research on reinforcement learning methods for non-ergodic systems.", "AI": {"tldr": "\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u975e\u904d\u5386\u73af\u5883\u4e2d\u4ea7\u751f\u6b21\u4f18\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u4f9d\u8d56\u6027\u53ef\u7ea0\u6b63\u6b64\u95ee\u9898\uff0c\u65e0\u9700\u6539\u53d8\u73af\u5883\u53cd\u9988\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u57fa\u4e8e\u671f\u671b\u503c\u4f18\u5316\uff0c\u5728\u975e\u904d\u5386\u73af\u5883\u4e2d\uff0c\u957f\u671f\u7ed3\u679c\u53d6\u51b3\u4e8e\u5177\u4f53\u8f68\u8ff9\u800c\u975e\u6574\u4f53\u5e73\u5747\uff0c\u5bfc\u81f4\u671f\u671b\u503c\u516c\u5f0f\u4ea7\u751f\u7cfb\u7edf\u6027\u7684\u6b21\u4f18\u7b56\u7565\u3002\u9700\u8981\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u975e\u904d\u5386\u73af\u5883\u4e2d\u7684\u4f18\u5316\u95ee\u9898\u3002", "method": "\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f15\u5165\u663e\u5f0f\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u8ba9\u7f51\u7edc\u51fd\u6570\u8fd1\u4f3c\u80fd\u591f\u7eb3\u5165\u65f6\u95f4\u4fe1\u606f\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u4f30\u8ba1\u4e0e\u8fc7\u7a0b\u5185\u5728\u589e\u957f\u7387\u4e00\u81f4\u7684\u4ef7\u503c\u51fd\u6570\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u6539\u53d8\u73af\u5883\u53cd\u9988\uff08\u5982\u5956\u52b1\u53d8\u6362\u6216\u4fee\u6539\u76ee\u6807\u51fd\u6570\uff09\uff0c\u800c\u662f\u901a\u8fc7\u667a\u80fd\u4f53\u5bf9\u65f6\u95f4\u8f68\u8ff9\u7684\u66b4\u9732\u81ea\u7136\u5b9e\u73b0\u3002", "result": "\u7814\u7a76\u8868\u660e\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u975e\u904d\u5386\u73af\u5883\u4e2d\u786e\u5b9e\u4ea7\u751f\u6b21\u4f18\u7b56\u7565\uff0c\u800c\u5f15\u5165\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u65b9\u6cd5\u80fd\u591f\u7ea0\u6b63\u8fd9\u4e00\u9650\u5236\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5b66\u4e60\u5230\u66f4\u4f18\u7684\u7b56\u7565\u3002", "conclusion": "\u5728\u975e\u904d\u5386\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u57fa\u4e8e\u671f\u671b\u503c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u4f9d\u8d56\u6027\u53ef\u4ee5\u81ea\u7136\u7ea0\u6b63\u8fd9\u4e00\u95ee\u9898\uff0c\u8fd9\u4e3a\u5f00\u53d1\u9002\u7528\u4e8e\u975e\u904d\u5386\u7cfb\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08742", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08742", "abs": "https://arxiv.org/abs/2601.08742", "authors": ["Xin Quan", "Jiafeng Xiong", "Marco Valentino", "Andr\u00e9 Freitas"], "title": "Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents", "comment": null, "summary": "Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86Attributional NLI\u6846\u67b6\uff0c\u5c06\u793e\u4f1a\u5fc3\u7406\u5b66\u539f\u7406\u878d\u5165\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff0c\u901a\u8fc7Undercover-V\u6e38\u620f\u8bc4\u4f30LLM\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u610f\u56fe\u63a8\u7406\u80fd\u529b\uff0c\u795e\u7ecf\u7b26\u53f7\u667a\u80fd\u4f53\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u65e0\u6cd5\u6355\u6349\u590d\u6742\u4ea4\u4e92\u7cfb\u7edf\u4e2d\u57fa\u4e8e\u610f\u56fe\u7684\u7ec6\u5fae\u63a8\u7406\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u5f52\u56e0\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faAttributional NLI\u6846\u67b6\uff0c\u7ed3\u5408\u793e\u4f1a\u5fc3\u7406\u5b66\u539f\u7406\uff0c\u901a\u8fc7\u6587\u672c\u6e38\u620fUndercover-V\u5b9e\u9a8c\u4e09\u79cdLLM\u667a\u80fd\u4f53\uff1a\u6807\u51c6NLI\u667a\u80fd\u4f53\u3001Att-NLI\u667a\u80fd\u4f53\uff08\u4f7f\u7528\u6eaf\u56e0-\u6f14\u7ece\u63a8\u7406\uff09\u3001\u795e\u7ecf\u7b26\u53f7Att-NLI\u667a\u80fd\u4f53\uff08\u7ed3\u5408\u5916\u90e8\u5b9a\u7406\u8bc1\u660e\u5668\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f52\u56e0\u63a8\u7406\u80fd\u529b\u5b58\u5728\u660e\u663e\u5c42\u6b21\u7ed3\u6784\uff0c\u795e\u7ecf\u7b26\u53f7\u667a\u80fd\u4f53\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u80dc\u7387\u8fbe\u523017.08%\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u667a\u80fd\u4f53\u3002", "conclusion": "Att-NLI\u6846\u67b6\u6709\u52a9\u4e8e\u5f00\u53d1\u5177\u6709\u590d\u6742\u63a8\u7406\u80fd\u529b\u7684\u667a\u80fd\u4f53\uff0c\u540c\u65f6\u51f8\u663e\u4e86\u795e\u7ecf\u7b26\u53f7AI\u5728\u6784\u5efa\u7406\u6027LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.08763", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08763", "abs": "https://arxiv.org/abs/2601.08763", "authors": ["Zhiyuan Hu", "Yucheng Wang", "Yufei He", "Jiaying Wu", "Yilun Zhao", "See-Kiong Ng", "Cynthia Breazeal", "Anh Tuan Luu", "Hae Won Park", "Bryan Hooi"], "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "comment": "Work in Progress", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "AI": {"tldr": "\u63d0\u51faUniqueness-Aware Reinforcement Learning\u65b9\u6cd5\uff0c\u901a\u8fc7\u5956\u52b1\u72ec\u7279\u7684\u9ad8\u5c42\u89e3\u9898\u7b56\u7565\u6765\u89e3\u51b3LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u4fdd\u6301pass@1\u7684\u540c\u65f6\u63d0\u5347pass@k\u548c\u7b56\u7565\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u65f6\uff0c\u5bb9\u6613\u51fa\u73b0\u63a2\u7d22\u5d29\u6e83\u95ee\u9898\uff1a\u7b56\u7565\u8fc7\u65e9\u96c6\u4e2d\u4e8e\u5c11\u6570\u4e3b\u5bfc\u63a8\u7406\u6a21\u5f0f\uff0c\u867d\u7136\u63d0\u5347\u4e86pass@1\uff0c\u4f46\u9650\u5236\u4e86rollout\u5c42\u9762\u7684\u591a\u6837\u6027\u548cpass@k\u7684\u63d0\u5347\u3002", "method": "\u63d0\u51faUniqueness-Aware Reinforcement Learning\u65b9\u6cd5\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u5668\u5bf9\u540c\u4e00\u95ee\u9898\u7684rollout\u8fdb\u884c\u805a\u7c7b\uff08\u6839\u636e\u9ad8\u5c42\u89e3\u9898\u7b56\u7565\uff0c\u5ffd\u7565\u8868\u9762\u5dee\u5f02\uff09\uff0c\u5e76\u6309\u805a\u7c7b\u5927\u5c0f\u53cd\u6bd4\u91cd\u65b0\u52a0\u6743\u7b56\u7565\u4f18\u52bf\uff0c\u4ece\u800c\u5956\u52b1\u6b63\u786e\u4f46\u65b0\u9896\u7684\u7b56\u7565\u3002", "result": "\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301pass@1\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86pass@k\u548cAUC@K\uff0c\u7ef4\u6301\u4e86\u63a2\u7d22\u80fd\u529b\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u91c7\u6837\u4e2d\u53d1\u73b0\u4e86\u66f4\u591a\u6837\u5316\u7684\u89e3\u9898\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5956\u52b1\u72ec\u7279\u7684\u9ad8\u5c42\u89e3\u9898\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u591a\u6837\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08747", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08747", "abs": "https://arxiv.org/abs/2601.08747", "authors": ["Rubing Chen", "Jian Wang", "Wenjie Li", "Xiao-Yong Wei", "Qing Li"], "title": "To Retrieve or To Think? An Agentic Approach for Context Evolution", "comment": null, "summary": "Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks.However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting.It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.", "AI": {"tldr": "ACE\u6846\u67b6\u901a\u8fc7\u7c7b\u4eba\u5143\u8ba4\u77e5\u673a\u5236\u52a8\u6001\u51b3\u5b9a\u4f55\u65f6\u68c0\u7d22\u5916\u90e8\u77e5\u8bc6\u3001\u4f55\u65f6\u5229\u7528\u73b0\u6709\u77e5\u8bc6\u63a8\u7406\uff0c\u51cf\u5c11\u5197\u4f59\u68c0\u7d22\u5e76\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u6027\u80fd", "motivation": "\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u90fd\u8fdb\u884c\u68c0\u7d22\uff0c\u8fd9\u79cd\u66b4\u529b\u7b56\u7565\u4e0d\u4ec5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u8fd8\u4f1a\u56e0\u5f15\u5165\u65e0\u5173\u566a\u58f0\u800c\u964d\u4f4e\u6027\u80fd\u3002\u9700\u8981\u66f4\u667a\u80fd\u7684\u4e0a\u4e0b\u6587\u6f14\u5316\u673a\u5236\u6765\u4f18\u5316\u77e5\u8bc6\u5bc6\u96c6\u578b\u63a8\u7406\u4efb\u52a1\u3002", "method": "\u63d0\u51faAgentic Context Evolution (ACE)\u6846\u67b6\uff0c\u53d7\u4eba\u7c7b\u5143\u8ba4\u77e5\u542f\u53d1\uff0c\u901a\u8fc7\u4e2d\u592e\u534f\u8c03\u5668\u4ee3\u7406\u4f7f\u7528\u591a\u6570\u6295\u7968\u7b56\u7565\u52a8\u6001\u51b3\u5b9a\u4f55\u65f6\u6fc0\u6d3b\u68c0\u7d22\u4ee3\u7406\u83b7\u53d6\u5916\u90e8\u8bc1\u636e\uff0c\u4f55\u65f6\u6fc0\u6d3b\u63a8\u7406\u4ee3\u7406\u8fdb\u884c\u5185\u90e8\u5206\u6790\u548c\u4e0a\u4e0b\u6587\u7cbe\u70bc\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cACE\u5728\u51c6\u786e\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684token\u6d88\u8017\uff0c\u901a\u8fc7\u6d88\u9664\u5197\u4f59\u68c0\u7d22\u6b65\u9aa4\u4fdd\u6301\u4e86\u7b80\u6d01\u4e14\u6f14\u5316\u540e\u7684\u4e0a\u4e0b\u6587\u3002", "conclusion": "ACE\u4e3a\u590d\u6742\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u4e0a\u4e0b\u6587\u6f14\u5316\u751f\u6210\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u667a\u80fd\u51b3\u7b56\u673a\u5236\u5e73\u8861\u5916\u90e8\u68c0\u7d22\u548c\u5185\u90e8\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.08808", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08808", "abs": "https://arxiv.org/abs/2601.08808", "authors": ["Yao Tang", "Li Dong", "Yaru Hao", "Qingxiu Dong", "Furu Wei", "Jiatao Gu"], "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge", "comment": "21 pages. Code available at https://github.com/GMLR-Penn/Multiplex-Thinking", "summary": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.", "AI": {"tldr": "\u63d0\u51faMultiplex Thinking\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u6837K\u4e2a\u5019\u9009token\u5e76\u805a\u5408\u4e3a\u5355\u4e2a\u8fde\u7eed\u591a\u8def\u590d\u7528token\uff0c\u5b9e\u73b0\u8f6f\u63a8\u7406\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6807\u51c6\u79bb\u6563\u751f\u6210\u7279\u6027\u7684\u540c\u65f6\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u4f18\u4e8e\u4f20\u7edfCoT\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfCoT\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u5347\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4f1a\u4ea7\u751f\u957f\u5e8f\u5217\u3001\u4f4e\u5e26\u5bbd\u7684token\u6d41\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4eba\u7c7b\u63a8\u7406\u65f6\u901a\u5e38\u4fdd\u6301\u5bf9\u591a\u4e2a\u53ef\u80fd\u4e0b\u4e00\u6b65\u7684\u5206\u5e03\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u8bcd\u6c47\u5d4c\u5165\u5148\u9a8c\u548c\u91c7\u6837\u52a8\u6001\uff0c\u53c8\u80fd\u7d27\u51d1\u8868\u793a\u591a\u4e2a\u53ef\u80fd\u63a8\u7406\u8def\u5f84\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMultiplex Thinking\uff1a\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u4e2d\uff0c\u91c7\u6837K\u4e2a\u5019\u9009token\uff0c\u5c06\u5b83\u4eec\u7684\u5d4c\u5165\u805a\u5408\u4e3a\u5355\u4e2a\u8fde\u7eed\u591a\u8def\u590d\u7528token\u3002\u8fd9\u79cd\u65b9\u6cd5\u4fdd\u6301\u6807\u51c6\u79bb\u6563\u751f\u6210\u7684\u7279\u6027\uff0c\u540c\u65f6\u4ea7\u751f\u53ef\u5904\u7406\u7684\u591a\u8def\u590d\u7528\u8f68\u8ff9\u5206\u5e03\uff0c\u53ef\u76f4\u63a5\u7528on-policy\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002\u5177\u6709\u81ea\u9002\u5e94\u6027\uff1a\u6a21\u578b\u7f6e\u4fe1\u5ea6\u9ad8\u65f6\u63a5\u8fd1\u79bb\u6563CoT\uff0c\u4e0d\u786e\u5b9a\u65f6\u7d27\u51d1\u8868\u793a\u591a\u4e2a\u53ef\u80fd\u4e0b\u4e00\u6b65\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMultiplex Thinking\u5728Pass@1\u5230Pass@1024\u7684\u6240\u6709\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u5f3a\u79bb\u6563CoT\u548cRL\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4ea7\u751f\u66f4\u77ed\u7684\u5e8f\u5217\u3002", "conclusion": "Multiplex Thinking\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8f6f\u63a8\u7406\u673a\u5236\uff0c\u901a\u8fc7\u8fde\u7eed\u591a\u8def\u590d\u7528token\u8868\u793a\u591a\u4e2a\u63a8\u7406\u53ef\u80fd\u6027\uff0c\u65e2\u4fdd\u6301\u4e86\u6807\u51c6\u751f\u6210\u7684\u7279\u6027\uff0c\u53c8\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u4e14\u66f4\u9ad8\u6548\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.08829", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08829", "abs": "https://arxiv.org/abs/2601.08829", "authors": ["Hsiang-Wei Huang", "Junbin Lu", "Kuang-Ming Chen", "Jenq-Neng Hwang"], "title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System", "comment": "In submission. The first two authors contributed equally", "summary": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5728Elo\u6392\u540d\u8bc4\u5ba1\u7cfb\u7edf\u4e2d\u4f7f\u7528LLM\u4ee3\u7406\u8bc4\u5ba1\u5458\uff0c\u901a\u8fc7\u771f\u5b9e\u4f1a\u8bae\u8bba\u6587\u63d0\u4ea4\u8fdb\u884c\u6a21\u62df\uff0c\u6bd4\u8f83\u4e86\u5305\u542bElo\u8bc4\u5206\u548c\u8bc4\u5ba1\u5458\u8bb0\u5fc6\u7684\u4e0d\u540c\u6761\u4ef6\uff0c\u53d1\u73b0Elo\u63d0\u9ad8\u4e86\u9886\u57df\u4e3b\u5e2d\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u4f46\u8bc4\u5ba1\u5458\u4f1a\u9002\u5e94\u6027\u5730\u5229\u7528\u7cfb\u7edf\u800c\u4e0d\u589e\u52a0\u8bc4\u5ba1\u52aa\u529b\u3002", "motivation": "\u63a2\u7d22\u5728\u5b66\u672f\u8bba\u6587\u8bc4\u5ba1\u7cfb\u7edf\u4e2d\u4f7f\u7528LLM\u4ee3\u7406\u8bc4\u5ba1\u5458\u7684\u52a8\u6001\uff0c\u7279\u522b\u662f\u7814\u7a76Elo\u6392\u540d\u7cfb\u7edf\u5982\u4f55\u5f71\u54cd\u8bc4\u5ba1\u8d28\u91cf\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u8bc4\u5ba1\u5458\u5982\u4f55\u9002\u5e94\u8fd9\u79cd\u8bc4\u5206\u673a\u5236\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u4f1a\u8bae\u8bba\u6587\u63d0\u4ea4\uff0c\u6784\u5efa\u5305\u542b\u591a\u4e2a\u5177\u6709\u4e0d\u540c\u89d2\u8272\u7684LLM\u4ee3\u7406\u8bc4\u5ba1\u5458\u7684\u6a21\u62df\u7cfb\u7edf\uff0c\u7531\u9886\u57df\u4e3b\u5e2d\u4e3b\u6301\u591a\u8f6e\u8bc4\u5ba1\u4e92\u52a8\u3002\u6bd4\u8f83\u57fa\u7ebf\u8bbe\u7f6e\u4e0e\u5305\u542bElo\u8bc4\u5206\u548c\u8bc4\u5ba1\u5458\u8bb0\u5fc6\u7684\u6761\u4ef6\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff1a1) \u5f15\u5165Elo\u8bc4\u5206\u63d0\u9ad8\u4e86\u9886\u57df\u4e3b\u5e2d\u51b3\u7b56\u51c6\u786e\u6027\uff1b2) \u8bc4\u5ba1\u5458\u4f1a\u9002\u5e94\u6027\u5730\u5229\u7528Elo\u7cfb\u7edf\uff0c\u4f46\u4e0d\u4f1a\u589e\u52a0\u5b9e\u9645\u8bc4\u5ba1\u52aa\u529b\uff1b3) \u63ed\u793a\u4e86\u8bc4\u5ba1\u5458\u5728Elo\u7cfb\u7edf\u4e2d\u7684\u7b56\u7565\u6027\u884c\u4e3a\u3002", "conclusion": "Elo\u6392\u540d\u7cfb\u7edf\u53ef\u4ee5\u6539\u5584\u5b66\u672f\u8bc4\u5ba1\u7684\u51b3\u7b56\u8d28\u91cf\uff0c\u4f46\u9700\u8981\u8b66\u60d5\u8bc4\u5ba1\u5458\u53ef\u80fd\u7b56\u7565\u6027\u5730\u5229\u7528\u7cfb\u7edf\u800c\u4e0d\u771f\u6b63\u63d0\u5347\u8bc4\u5ba1\u8d28\u91cf\u7684\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.528a3ef1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbuilders.ramp.com%2Fpost%2Fwhy-we-built-our-background-agent%3Futm_source=tldrnewsletter/1/0100019bb71960e5-2a5708c1-6414-4fd3-8cfb-5852c35dc118-000000/i75HyLJkPj-xkG6FvPFftETaZIm0oVAQG6YWn4KvIzQ=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbuilders.ramp.com%2Fpost%2Fwhy-we-built-our-background-agent%3Futm_source=tldrnewsletter/1/0100019bb71960e5-2a5708c1-6414-4fd3-8cfb-5852c35dc118-000000/i75HyLJkPj-xkG6FvPFftETaZIm0oVAQG6YWn4KvIzQ=440", "authors": ["TLDR Newsletter"], "title": "Why We Built Our Own Background Agent", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbuilders.ramp.com%2Fpost%2Fwhy-we-built-our-background-agent%3Futm_source=tldrnewsletter/1/0100019bb71960e5-2a5708c1-6414-4fd3-8cfb-5852c35dc118-000000/i75HyLJkPj-xkG6FvPFftETaZIm0oVAQG6YWn4KvIzQ=440", "summary": "Why We Built Our Own Background Agent (17 minute read) Inspect is a coding agent that closes the loop on verifying its work by having all of the context and tools needed to prove it. It can run tests, review telemetry, and query feature flags, and it visually verifies its work and gives users screenshots and live previews. Each session runs in a sandboxed VM with everything an engineer would have locally. This post contains the spec for Inspect so anyone can replicate the tool.", "source": "tldr", "AI": {"tldr": "Inspect\u662f\u4e00\u4e2a\u80fd\u591f\u81ea\u6211\u9a8c\u8bc1\u5de5\u4f5c\u7684\u7f16\u7801\u4ee3\u7406\uff0c\u901a\u8fc7\u6c99\u76d2\u5316VM\u73af\u5883\u63d0\u4f9b\u5b8c\u6574\u5f00\u53d1\u4e0a\u4e0b\u6587\u548c\u5de5\u5177\uff0c\u53ef\u4ee5\u8fd0\u884c\u6d4b\u8bd5\u3001\u68c0\u67e5\u9065\u6d4b\u6570\u636e\u3001\u67e5\u8be2\u529f\u80fd\u6807\u5fd7\uff0c\u5e76\u53ef\u89c6\u5316\u9a8c\u8bc1\u5de5\u4f5c\u7ed3\u679c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7f16\u7801\u4ee3\u7406\u5728\u9a8c\u8bc1\u81ea\u8eab\u5de5\u4f5c\u65b9\u9762\u7684\u95ed\u73af\u95ee\u9898\uff0c\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u50cf\u5de5\u7a0b\u5e08\u4e00\u6837\u62e5\u6709\u5b8c\u6574\u672c\u5730\u5f00\u53d1\u73af\u5883\u7684\u4ee3\u7406\uff0c\u786e\u4fdd\u5176\u5de5\u4f5c\u80fd\u591f\u88ab\u53ef\u9760\u9a8c\u8bc1\u3002", "method": "\u6784\u5efaInspect\u7f16\u7801\u4ee3\u7406\uff0c\u5728\u6c99\u76d2\u5316\u865a\u62df\u673a\u4e2d\u8fd0\u884c\u6bcf\u4e2a\u4f1a\u8bdd\uff0c\u63d0\u4f9b\u5de5\u7a0b\u5e08\u672c\u5730\u5f00\u53d1\u6240\u9700\u7684\u6240\u6709\u4e0a\u4e0b\u6587\u548c\u5de5\u5177\uff0c\u5305\u62ec\u6d4b\u8bd5\u8fd0\u884c\u3001\u9065\u6d4b\u68c0\u67e5\u3001\u529f\u80fd\u6807\u5fd7\u67e5\u8be2\u7b49\u529f\u80fd\uff0c\u5e76\u652f\u6301\u53ef\u89c6\u5316\u9a8c\u8bc1\u548c\u5b9e\u65f6\u9884\u89c8\u3002", "result": "\u5f00\u53d1\u4e86Inspect\u7f16\u7801\u4ee3\u7406\uff0c\u80fd\u591f\u6709\u6548\u9a8c\u8bc1\u81ea\u8eab\u5de5\u4f5c\uff0c\u63d0\u4f9b\u622a\u56fe\u548c\u5b9e\u65f6\u9884\u89c8\uff0c\u5e76\u5728\u6587\u7ae0\u4e2d\u516c\u5f00\u4e86\u8be6\u7ec6\u89c4\u8303\uff0c\u4f7f\u4efb\u4f55\u4eba\u90fd\u80fd\u590d\u5236\u8be5\u5de5\u5177\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5177\u6709\u5b8c\u6574\u5f00\u53d1\u73af\u5883\u548c\u81ea\u6211\u9a8c\u8bc1\u80fd\u529b\u7684\u7f16\u7801\u4ee3\u7406\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u7f16\u7801\u4ee3\u7406\u5de5\u4f5c\u7684\u53ef\u9760\u6027\u9a8c\u8bc1\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u7684\u89c4\u8303\u5df2\u516c\u5f00\u4f9b\u793e\u533a\u4f7f\u7528\u3002", "topic": "code agent"}}
{"id": "tldr.2601.b5b502af", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F12%2Fclaude-cowork%2F%23atom-everything%3Futm_source=tldrnewsletter/1/0100019bb71960e5-2a5708c1-6414-4fd3-8cfb-5852c35dc118-000000/1fktnm_Gr_PkkjBlIHyjOGJApRDU6-hjm_xQrKndTpQ=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F12%2Fclaude-cowork%2F%23atom-everything%3Futm_source=tldrnewsletter/1/0100019bb71960e5-2a5708c1-6414-4fd3-8cfb-5852c35dc118-000000/1fktnm_Gr_PkkjBlIHyjOGJApRDU6-hjm_xQrKndTpQ=440", "authors": ["TLDR Newsletter"], "title": "First impressions of Claude Cowork, Anthropic's general agent", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F12%2Fclaude-cowork%2F%23atom-everything%3Futm_source=tldrnewsletter/1/0100019bb71960e5-2a5708c1-6414-4fd3-8cfb-5852c35dc118-000000/1fktnm_Gr_PkkjBlIHyjOGJApRDU6-hjm_xQrKndTpQ=440", "summary": "First impressions of Claude Cowork, Anthropic's general agent (8 minute read) Claude Cowork is a general agent with a UI now available as a research preview to Max subscribers as part of the updated Claude Desktop macOS application. It looks very similar to the desktop interface for regular Claude Code. The general agent is designed to bring the powerful capabilities of Claude Code to a wider audience with a less intimidating interface. Screenshots of the feature are available in the article.", "source": "tldr", "AI": {"tldr": "Claude Cowork\u662fAnthropic\u63a8\u51fa\u7684\u901a\u7528\u4ee3\u7406\u5de5\u5177\uff0c\u73b0\u5df2\u4f5c\u4e3a\u7814\u7a76\u9884\u89c8\u7248\u5411Max\u8ba2\u9605\u8005\u63d0\u4f9b\uff0c\u96c6\u6210\u5728\u66f4\u65b0\u7684Claude Desktop macOS\u5e94\u7528\u4e2d\uff0c\u65e8\u5728\u5c06Claude Code\u7684\u5f3a\u5927\u529f\u80fd\u5e26\u7ed9\u66f4\u5e7f\u6cdb\u7684\u7528\u6237\u7fa4\u4f53\u3002", "motivation": "\u8ba9\u66f4\u591a\u7528\u6237\u80fd\u591f\u4f7f\u7528Claude Code\u7684\u5f3a\u5927\u529f\u80fd\uff0c\u901a\u8fc7\u63d0\u4f9b\u66f4\u53cb\u597d\u3001\u4e0d\u90a3\u4e48\u4ee4\u4eba\u751f\u754f\u7684\u754c\u9762\u6765\u6269\u5927\u7528\u6237\u7fa4\u4f53\u3002", "method": "\u5f00\u53d1\u901a\u7528\u4ee3\u7406\u5de5\u5177Claude Cowork\uff0c\u96c6\u6210\u5230Claude Desktop macOS\u5e94\u7528\u4e2d\uff0c\u63d0\u4f9b\u7c7b\u4f3c\u5e38\u89c4Claude Code\u7684\u684c\u9762\u754c\u9762\u4f46\u66f4\u6613\u7528\u7684UI\u3002", "result": "Claude Cowork\u73b0\u5df2\u4f5c\u4e3a\u7814\u7a76\u9884\u89c8\u7248\u5411Max\u8ba2\u9605\u8005\u63d0\u4f9b\uff0c\u529f\u80fd\u622a\u56fe\u5df2\u5728\u6587\u7ae0\u4e2d\u5c55\u793a\uff0c\u754c\u9762\u4e0e\u5e38\u89c4Claude Code\u684c\u9762\u754c\u9762\u76f8\u4f3c\u4f46\u66f4\u53cb\u597d\u3002", "conclusion": "Claude Cowork\u4f5c\u4e3a\u901a\u7528\u4ee3\u7406\u5de5\u5177\uff0c\u6210\u529f\u5c06Claude Code\u7684\u5f3a\u5927\u529f\u80fd\u901a\u8fc7\u66f4\u6613\u7528\u7684\u754c\u9762\u5e26\u7ed9\u66f4\u5e7f\u6cdb\u7684\u7528\u6237\u7fa4\u4f53\u3002", "topic": "code agent"}}
{"id": "tldr.2601.1be52b5b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Fagent-stress-test%2F%3Futm_source=tldrnewsletter/1/0100019bb71960e5-2a5708c1-6414-4fd3-8cfb-5852c35dc118-000000/p6bwWXhCV8GC-fmmK2UVyBD9g4w_NN8-Yez0fnmwzaM=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Fagent-stress-test%2F%3Futm_source=tldrnewsletter/1/0100019bb71960e5-2a5708c1-6414-4fd3-8cfb-5852c35dc118-000000/p6bwWXhCV8GC-fmmK2UVyBD9g4w_NN8-Yez0fnmwzaM=440", "authors": ["TLDR Newsletter"], "title": "AI Agents Are a Stress Test for Your Dev Stack", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Fagent-stress-test%2F%3Futm_source=tldrnewsletter/1/0100019bb71960e5-2a5708c1-6414-4fd3-8cfb-5852c35dc118-000000/p6bwWXhCV8GC-fmmK2UVyBD9g4w_NN8-Yez0fnmwzaM=440", "summary": "AI Agents Are a Stress Test for Your Dev Stack (5 minute read) AI agent loops expose how brittle, non-standard, and half-tribal our development environments really are.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u5faa\u73af\u66b4\u9732\u4e86\u5f00\u53d1\u73af\u5883\u7684\u8106\u5f31\u6027\u3001\u975e\u6807\u51c6\u5316\u548c\u90e8\u843d\u5316\u95ee\u9898", "motivation": "AI\u4ee3\u7406\u5728\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u8fd0\u884c\u63ed\u793a\u4e86\u5f53\u524d\u5f00\u53d1\u6808\u5b58\u5728\u7684\u6839\u672c\u6027\u95ee\u9898\uff0c\u5305\u62ec\u8106\u5f31\u6027\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u4ee5\u53ca\u8fc7\u5ea6\u4f9d\u8d56\u90e8\u843d\u77e5\u8bc6", "method": "\u901a\u8fc7\u5206\u6790AI\u4ee3\u7406\u5728\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u5faa\u73af\u8fd0\u884c\u8fc7\u7a0b\uff0c\u8bc6\u522b\u5f00\u53d1\u6808\u7684\u8106\u5f31\u70b9\u548c\u975e\u6807\u51c6\u5316\u95ee\u9898", "result": "AI\u4ee3\u7406\u5faa\u73af\u66b4\u9732\u4e86\u5f00\u53d1\u73af\u5883\u7684\u8106\u5f31\u6027\u3001\u975e\u6807\u51c6\u5316\u914d\u7f6e\u548c\u8fc7\u5ea6\u4f9d\u8d56\u90e8\u843d\u77e5\u8bc6\u7684\u95ee\u9898", "conclusion": "AI\u4ee3\u7406\u5bf9\u5f00\u53d1\u6808\u6784\u6210\u4e86\u538b\u529b\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u9700\u8981\u6539\u8fdb\u7684\u9886\u57df\uff0c\u5305\u62ec\u6807\u51c6\u5316\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u91cd\u590d\u6027", "topic": "agent analysis"}}
{"id": "tldr.2601.5fbe2c81", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FYplRYs/2/0100019bb73f6677-4e3825e2-dce1-4327-a0bb-6a806cc908b4-000000/p4Mi3zm0w3gwt0qTl3CAdy2XMWb69msijQogR19bkd4=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FYplRYs/2/0100019bb73f6677-4e3825e2-dce1-4327-a0bb-6a806cc908b4-000000/p4Mi3zm0w3gwt0qTl3CAdy2XMWb69msijQogR19bkd4=439", "authors": ["TLDR Newsletter"], "title": "Agents that don't suck", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FYplRYs/2/0100019bb73f6677-4e3825e2-dce1-4327-a0bb-6a806cc908b4-000000/p4Mi3zm0w3gwt0qTl3CAdy2XMWb69msijQogR19bkd4=439", "summary": "Agents that don't suck (Sponsor) Most AI agents never make it past the pilot. Agent Bricks by Databricks helps you build agents that actually work \u2014 accurate, reliable and grounded in your data. It gives you a clear read on quality: automatic evaluation, scores tied to your goals and human feedback to keep improving accuracy. No guesswork. No generic benchmarks. It's AI built for how your business runs. Agent Bricks delivers agents ready for real work \u2014 the kind that grows your business and b...", "source": "tldr", "AI": {"tldr": "Databricks\u63a8\u51faAgent Bricks\u5e73\u53f0\uff0c\u5e2e\u52a9\u4f01\u4e1a\u6784\u5efa\u51c6\u786e\u3001\u53ef\u9760\u3001\u57fa\u4e8e\u81ea\u8eab\u6570\u636e\u7684AI\u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u81ea\u52a8\u8bc4\u4f30\u3001\u76ee\u6807\u5bfc\u5411\u8bc4\u5206\u548c\u4eba\u5de5\u53cd\u9988\uff0c\u786e\u4fdd\u667a\u80fd\u4f53\u8d28\u91cf\uff0c\u652f\u6301\u5b9e\u9645\u4e1a\u52a1\u5e94\u7528", "motivation": "\u5f53\u524d\u5927\u591a\u6570AI\u667a\u80fd\u4f53\u5728\u8bd5\u70b9\u9636\u6bb5\u5c31\u5931\u8d25\uff0c\u65e0\u6cd5\u771f\u6b63\u6295\u5165\u751f\u4ea7\u73af\u5883\u3002\u4f01\u4e1a\u9700\u8981\u80fd\u591f\u51c6\u786e\u3001\u53ef\u9760\u3001\u57fa\u4e8e\u81ea\u8eab\u6570\u636e\u7684\u667a\u80fd\u4f53\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u7684\u731c\u6d4b\u6027\u65b9\u6cd5", "method": "Agent Bricks\u5e73\u53f0\u63d0\u4f9b\u7aef\u5230\u7aef\u667a\u80fd\u4f53\u6784\u5efa\u5de5\u5177\uff0c\u5305\u62ec\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\u3001\u57fa\u4e8e\u4e1a\u52a1\u76ee\u6807\u7684\u8bc4\u5206\u673a\u5236\u3001\u4eba\u5de5\u53cd\u9988\u5faa\u73af\uff0c\u786e\u4fdd\u667a\u80fd\u4f53\u8d28\u91cf\u4e0e\u4e1a\u52a1\u9700\u6c42\u5bf9\u9f50", "result": "\u8be5\u5e73\u53f0\u80fd\u591f\u4ea4\u4ed8\u771f\u6b63\u53ef\u7528\u4e8e\u5b9e\u9645\u5de5\u4f5c\u7684\u667a\u80fd\u4f53\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u51c6\u786e\u53ef\u9760\u3001\u57fa\u4e8e\u4f01\u4e1a\u6570\u636e\uff0c\u80fd\u591f\u4fc3\u8fdb\u4e1a\u52a1\u589e\u957f\uff0c\u907f\u514d\u4f20\u7edf\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\u7684\u731c\u6d4b\u548c\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u95ee\u9898", "conclusion": "Agent Bricks\u901a\u8fc7\u63d0\u4f9b\u8d28\u91cf\u53ef\u63a7\u3001\u4e1a\u52a1\u5bfc\u5411\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86AI\u667a\u80fd\u4f53\u4ece\u8bd5\u70b9\u5230\u751f\u4ea7\u90e8\u7f72\u7684\u96be\u9898\uff0c\u4f7f\u4f01\u4e1a\u80fd\u591f\u6784\u5efa\u771f\u6b63\u6709\u6548\u7684\u4e1a\u52a1\u667a\u80fd\u4f53", "topic": "code agent"}}
{"id": "tldr.2601.eaa2e7d8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.marketingbrew.com%2Fstories%2F2026%2F01%2F12%2Fperplexity-amazon-lawsuit-agentic-AI-retail-media%3Futm_source=tldrmarketing/1/0100019bb73f6677-4e3825e2-dce1-4327-a0bb-6a806cc908b4-000000/63GmHSFyU6z83aAg1ttSmFq-aXCK6KUgNoRUx8REe4c=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.marketingbrew.com%2Fstories%2F2026%2F01%2F12%2Fperplexity-amazon-lawsuit-agentic-AI-retail-media%3Futm_source=tldrmarketing/1/0100019bb73f6677-4e3825e2-dce1-4327-a0bb-6a806cc908b4-000000/63GmHSFyU6z83aAg1ttSmFq-aXCK6KUgNoRUx8REe4c=439", "authors": ["TLDR Newsletter"], "title": "What the Perplexity-Amazon lawsuit could mean for digital advertising", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.marketingbrew.com%2Fstories%2F2026%2F01%2F12%2Fperplexity-amazon-lawsuit-agentic-AI-retail-media%3Futm_source=tldrmarketing/1/0100019bb73f6677-4e3825e2-dce1-4327-a0bb-6a806cc908b4-000000/63GmHSFyU6z83aAg1ttSmFq-aXCK6KUgNoRUx8REe4c=439", "summary": "What the Perplexity-Amazon lawsuit could mean for digital advertising (3 minute read) Amazon's lawsuit against Perplexity AI centers on whether agentic shopping tools undermine ad-supported commerce by bypassing promotions and brand-controlled experiences. Amazon claims Perplexity's Comet agent poses security risks and disguises automated activity as human behavior. Perplexity argues the real issue is lost advertising exposure when AI agents shop instead of people. If agents transact without ...", "source": "tldr", "AI": {"tldr": "\u4e9a\u9a6c\u900a\u8d77\u8bc9Perplexity AI\uff0c\u6307\u63a7\u5176Comet\u8d2d\u7269\u4ee3\u7406\u5de5\u5177\u7ed5\u8fc7\u5e7f\u544a\u548c\u54c1\u724c\u4f53\u9a8c\uff0c\u6784\u6210\u5b89\u5168\u98ce\u9669\u5e76\u4f2a\u88c5\u4eba\u7c7b\u884c\u4e3a\uff0c\u6838\u5fc3\u4e89\u8bae\u662fAI\u4ee3\u7406\u8d2d\u7269\u662f\u5426\u4f1a\u7834\u574f\u5e7f\u544a\u652f\u6301\u7684\u5546\u4e1a\u6a21\u5f0f\u3002", "motivation": "\u63a2\u8ba8AI\u8d2d\u7269\u4ee3\u7406\u5bf9\u6570\u5b57\u5e7f\u544a\u5546\u4e1a\u6a21\u5f0f\u7684\u6f5c\u5728\u5a01\u80c1\uff0c\u7279\u522b\u662f\u5f53AI\u4ee3\u7406\u4ee3\u66ff\u4eba\u7c7b\u8fdb\u884c\u8d2d\u7269\u65f6\uff0c\u662f\u5426\u4f1a\u7ed5\u8fc7\u4f20\u7edf\u5e7f\u544a\u66dd\u5149\u548c\u54c1\u724c\u63a7\u5236\u4f53\u9a8c\uff0c\u4ece\u800c\u5f71\u54cd\u5e7f\u544a\u652f\u6301\u7684\u7535\u5546\u751f\u6001\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e9a\u9a6c\u900a\u4e0ePerplexity AI\u4e4b\u95f4\u7684\u6cd5\u5f8b\u8bc9\u8bbc\u6848\u4f8b\uff0c\u63a2\u8ba8AI\u8d2d\u7269\u4ee3\u7406\uff08\u5982Comet\uff09\u7684\u6280\u672f\u7279\u70b9\u3001\u8fd0\u4f5c\u65b9\u5f0f\u53ca\u5176\u5bf9\u5e7f\u544a\u66dd\u5149\u3001\u54c1\u724c\u4f53\u9a8c\u548c\u5b89\u5168\u6027\u7684\u5f71\u54cd\u3002", "result": "\u63ed\u793a\u4e86AI\u8d2d\u7269\u4ee3\u7406\u53ef\u80fd\u7ed5\u8fc7\u4f20\u7edf\u5e7f\u544a\u548c\u4fc3\u9500\u6d3b\u52a8\uff0c\u51cf\u5c11\u54c1\u724c\u66dd\u5149\u673a\u4f1a\uff0c\u540c\u65f6\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff08\u5982\u4f2a\u88c5\u4eba\u7c7b\u884c\u4e3a\uff09\uff0c\u8fd9\u5bf9\u4f9d\u8d56\u5e7f\u544a\u6536\u5165\u7684\u7535\u5546\u5e73\u53f0\u6784\u6210\u6f5c\u5728\u5a01\u80c1\u3002", "conclusion": "AI\u8d2d\u7269\u4ee3\u7406\u7684\u5174\u8d77\u53ef\u80fd\u98a0\u8986\u4f20\u7edf\u6570\u5b57\u5e7f\u544a\u5546\u4e1a\u6a21\u5f0f\uff0c\u9700\u8981\u5728\u6280\u672f\u521b\u65b0\u4e0e\u5546\u4e1a\u53ef\u6301\u7eed\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u6cd5\u5f8b\u8bc9\u8bbc\u7ed3\u679c\u5c06\u5bf9\u6b64\u7c7b\u6280\u672f\u7684\u53d1\u5c55\u65b9\u5411\u4ea7\u751f\u91cd\u8981\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.db3e06a7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fhow-to-build-agents-with-filesystems-and-bash%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/zVUSuCS7BCPIZmYH1dpjdveRKGEB3q9i4ZEnyry4S_E=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fhow-to-build-agents-with-filesystems-and-bash%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/zVUSuCS7BCPIZmYH1dpjdveRKGEB3q9i4ZEnyry4S_E=440", "authors": ["TLDR Newsletter"], "title": "How to build agents with filesystems and bash", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fhow-to-build-agents-with-filesystems-and-bash%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/zVUSuCS7BCPIZmYH1dpjdveRKGEB3q9i4ZEnyry4S_E=440", "summary": "How to build agents with filesystems and bash (3 minute read) A simple, effective architecture for AI agents is a standard filesystem and Bash tools. Since LLMs have a native understanding of code and Unix commands, agents can easily navigate and retrieve information from data structured as files, much like exploring a codebase. This approach overcomes the limitations of prompt stuffing and imprecise vector search by offering natural data hierarchies, exact retrieval, and minimal context load...", "source": "tldr", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6807\u51c6\u6587\u4ef6\u7cfb\u7edf\u548cBash\u5de5\u5177\u7684AI\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5229\u7528LLM\u5bf9\u4ee3\u7801\u548cUnix\u547d\u4ee4\u7684\u539f\u751f\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u6587\u4ef6\u7cfb\u7edf\u7ed3\u6784\u5b9e\u73b0\u9ad8\u6548\u4fe1\u606f\u68c0\u7d22", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u63d0\u793a\u8bcd\u5806\u53e0\u548c\u5411\u91cf\u641c\u7d22\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u81ea\u7136\u7684\u6570\u636e\u5c42\u6b21\u7ed3\u6784\u3001\u7cbe\u786e\u68c0\u7d22\u548c\u6700\u5c0f\u4e0a\u4e0b\u6587\u8d1f\u8f7d", "method": "\u91c7\u7528\u6807\u51c6\u6587\u4ef6\u7cfb\u7edf\u548cBash\u5de5\u5177\u6784\u5efaAI\u667a\u80fd\u4f53\uff0c\u5229\u7528LLM\u5bf9\u4ee3\u7801\u548cUnix\u547d\u4ee4\u7684\u539f\u751f\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u6587\u4ef6\u7cfb\u7edf\u5bfc\u822a\u548c\u68c0\u7d22\u7ed3\u6784\u5316\u6570\u636e", "result": "\u8be5\u67b6\u6784\u80fd\u591f\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u4fe1\u606f\u68c0\u7d22\u65b9\u5f0f\uff0c\u7c7b\u4f3c\u4e8e\u63a2\u7d22\u4ee3\u7801\u5e93\u7684\u8fc7\u7a0b", "conclusion": "\u57fa\u4e8e\u6587\u4ef6\u7cfb\u7edf\u548cBash\u5de5\u5177\u7684\u7b80\u5355\u67b6\u6784\u662f\u6784\u5efaAI\u667a\u80fd\u4f53\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5229\u7528LLM\u7684\u73b0\u6709\u80fd\u529b\u5b9e\u73b0\u66f4\u597d\u7684\u6570\u636e\u7ec4\u7ec7\u548c\u68c0\u7d22", "topic": "code agent"}}
{"id": "tldr.2601.94a240c1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjessitron.com%2F2026%2F01%2F12%2Fmaking-ai-do-things-right-introduce-determinism%2F%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/jnOz2pOxgM7EQBrJWEGN0W1_377kNUp6Q_B36KtvIJc=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjessitron.com%2F2026%2F01%2F12%2Fmaking-ai-do-things-right-introduce-determinism%2F%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/jnOz2pOxgM7EQBrJWEGN0W1_377kNUp6Q_B36KtvIJc=440", "authors": ["TLDR Newsletter"], "title": "Replacing LLM date math with deterministic scripts to improve calendar accuracy", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjessitron.com%2F2026%2F01%2F12%2Fmaking-ai-do-things-right-introduce-determinism%2F%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/jnOz2pOxgM7EQBrJWEGN0W1_377kNUp6Q_B36KtvIJc=440", "summary": "Replacing LLM date math with deterministic scripts to improve calendar accuracy (3 minute read) LLMs like Claude frequently fail at date math, causing calendar errors in tools like gcalcli. Offloading calculations to deterministic scripts ensures accuracy. Developers can eliminate hallucinations and create reliable, repeatable AI workflows by updating CLAUDE.md instructions to execute code instead of manual reasoning.", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528\u786e\u5b9a\u6027\u811a\u672c\u66ff\u4ee3LLM\u8fdb\u884c\u65e5\u671f\u8ba1\u7b97\uff0c\u63d0\u9ad8\u65e5\u5386\u5de5\u5177\u51c6\u786e\u6027", "motivation": "LLM\uff08\u5982Claude\uff09\u5728\u65e5\u671f\u8ba1\u7b97\u4e0a\u7ecf\u5e38\u51fa\u9519\uff0c\u5bfc\u81f4\u65e5\u5386\u5de5\u5177\uff08\u5982gcalcli\uff09\u51fa\u73b0\u9519\u8bef\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5c06\u65e5\u671f\u8ba1\u7b97\u4efb\u52a1\u4eceLLM\u8f6c\u79fb\u5230\u786e\u5b9a\u6027\u811a\u672c\uff0c\u901a\u8fc7\u66f4\u65b0CLAUDE.md\u6307\u4ee4\u8ba9AI\u6267\u884c\u4ee3\u7801\u800c\u975e\u624b\u52a8\u63a8\u7406", "result": "\u6d88\u9664\u4e86\u5e7b\u89c9\u95ee\u9898\uff0c\u521b\u5efa\u4e86\u53ef\u9760\u3001\u53ef\u91cd\u590d\u7684AI\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u9ad8\u4e86\u65e5\u5386\u51c6\u786e\u6027", "conclusion": "\u901a\u8fc7\u5c06\u8ba1\u7b97\u4efb\u52a1\u59d4\u6258\u7ed9\u786e\u5b9a\u6027\u811a\u672c\u800c\u975e\u4f9d\u8d56LLM\u63a8\u7406\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8AI\u5de5\u5177\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027", "topic": "swe application"}}
{"id": "tldr.2601.16ca1f8b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclaude.com%2Fblog%2Fcowork-research-preview%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/AaT3lpNY2yObaIvOqpK6UEH8mEFj5JiMCToPaVjlIuU=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclaude.com%2Fblog%2Fcowork-research-preview%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/AaT3lpNY2yObaIvOqpK6UEH8mEFj5JiMCToPaVjlIuU=440", "authors": ["TLDR Newsletter"], "title": "Introducing Cowork", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclaude.com%2Fblog%2Fcowork-research-preview%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/AaT3lpNY2yObaIvOqpK6UEH8mEFj5JiMCToPaVjlIuU=440", "summary": "Introducing Cowork (5 minute read) Claude Cowork is a new feature that extends Claude Code to allow anyone, not just developers, to interact with Claude by giving it direct access to files on their computer. This allows Claude to read, edit, or create files within a designated folder, completing tasks like organizing documents or drafting reports with enhanced agency.", "source": "tldr", "AI": {"tldr": "Claude Cowork\u662f\u4e00\u4e2a\u65b0\u529f\u80fd\uff0c\u8ba9\u975e\u5f00\u53d1\u8005\u4e5f\u80fd\u901a\u8fc7\u8ba9Claude\u76f4\u63a5\u8bbf\u95ee\u7535\u8111\u6587\u4ef6\u6765\u4e0e\u4e4b\u4ea4\u4e92\uff0c\u5b9e\u73b0\u6587\u4ef6\u8bfb\u5199\u548c\u4efb\u52a1\u81ea\u52a8\u5316", "motivation": "\u6269\u5c55Claude Code\u529f\u80fd\uff0c\u4f7f\u975e\u5f00\u53d1\u8005\u4e5f\u80fd\u5229\u7528AI\u52a9\u624b\u5b8c\u6210\u6587\u4ef6\u7ba1\u7406\u548c\u6587\u6863\u5904\u7406\u7b49\u4efb\u52a1\uff0c\u964d\u4f4e\u6280\u672f\u95e8\u69db", "method": "\u901a\u8fc7\u8ba9Claude\u76f4\u63a5\u8bbf\u95ee\u7528\u6237\u6307\u5b9a\u6587\u4ef6\u5939\u4e2d\u7684\u6587\u4ef6\uff0c\u5b9e\u73b0\u6587\u4ef6\u8bfb\u53d6\u3001\u7f16\u8f91\u548c\u521b\u5efa\u529f\u80fd\uff0c\u589e\u5f3aAI\u4ee3\u7406\u7684\u81ea\u4e3b\u6027", "result": "\u5f00\u53d1\u4e86Claude Cowork\u529f\u80fd\uff0c\u4f7f\u4efb\u4f55\u4eba\u90fd\u80fd\u901a\u8fc7\u6587\u4ef6\u8bbf\u95ee\u4e0eClaude\u4ea4\u4e92\uff0c\u5b8c\u6210\u6587\u6863\u7ec4\u7ec7\u548c\u62a5\u544a\u8d77\u8349\u7b49\u4efb\u52a1", "conclusion": "Claude Cowork\u6210\u529f\u6269\u5c55\u4e86AI\u52a9\u624b\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4f7f\u975e\u6280\u672f\u7528\u6237\u4e5f\u80fd\u53d7\u76ca\u4e8e\u81ea\u52a8\u5316\u6587\u4ef6\u5904\u7406\u80fd\u529b", "topic": "swe application"}}
{"id": "tldr.2601.1a6ca7ff", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fthe-key-to-agentic-success-let-unix-bash-lead-the-way%2F%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/MTRaD9AzSvsaJm0OCNFTe1xpIRag5hEqHr_DLhwAPGo=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fthe-key-to-agentic-success-let-unix-bash-lead-the-way%2F%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/MTRaD9AzSvsaJm0OCNFTe1xpIRag5hEqHr_DLhwAPGo=440", "authors": ["TLDR Newsletter"], "title": "The Key to Agentic Success? BASH Is All You Need", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fthe-key-to-agentic-success-let-unix-bash-lead-the-way%2F%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/MTRaD9AzSvsaJm0OCNFTe1xpIRag5hEqHr_DLhwAPGo=440", "summary": "The Key to Agentic Success? BASH Is All You Need (7 minute read) Minimalist AI agent architectures using simple BASH shells and modular Unix tools are proving more effective than complex, over-engineered systems. Vercel, for example, improved its internal data agent, d0, by simplifying its design to use basic BASH commands for direct file interrogation, resulting in faster, more accurate, and easier-to-manage operations.", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528\u7b80\u5355BASH shell\u548c\u6a21\u5757\u5316Unix\u5de5\u5177\u7684\u6700\u5c0f\u5316AI\u4ee3\u7406\u67b6\u6784\u6bd4\u590d\u6742\u8fc7\u5ea6\u8bbe\u8ba1\u7684\u7cfb\u7edf\u66f4\u6709\u6548\uff0cVercel\u901a\u8fc7\u7b80\u5316\u5176\u5185\u90e8\u6570\u636e\u4ee3\u7406d0\u4f7f\u7528\u57fa\u672cBASH\u547d\u4ee4\u8fdb\u884c\u76f4\u63a5\u6587\u4ef6\u67e5\u8be2\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u3001\u66f4\u51c6\u786e\u3001\u66f4\u6613\u7ba1\u7406\u7684\u64cd\u4f5c\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u67b6\u6784\u5f80\u5f80\u8fc7\u5ea6\u590d\u6742\u548c\u8fc7\u5ea6\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3001\u96be\u4ee5\u7ba1\u7406\u3002\u7814\u7a76\u53d1\u73b0\u7b80\u5355\u3001\u6a21\u5757\u5316\u7684\u65b9\u6cd5\u53ef\u80fd\u6bd4\u590d\u6742\u7cfb\u7edf\u66f4\u6709\u6548\u3002", "method": "\u91c7\u7528\u6700\u5c0f\u5316AI\u4ee3\u7406\u67b6\u6784\uff0c\u4f7f\u7528\u7b80\u5355\u7684BASH shell\u548c\u6a21\u5757\u5316Unix\u5de5\u5177\u3002\u5177\u4f53\u6848\u4f8b\u4e2d\uff0cVercel\u7b80\u5316\u5176\u5185\u90e8\u6570\u636e\u4ee3\u7406d0\u7684\u8bbe\u8ba1\uff0c\u4f7f\u7528\u57fa\u672cBASH\u547d\u4ee4\u8fdb\u884c\u76f4\u63a5\u6587\u4ef6\u67e5\u8be2\u3002", "result": "\u7b80\u5316\u540e\u7684\u7cfb\u7edf\u5b9e\u73b0\u4e86\u66f4\u5feb\u3001\u66f4\u51c6\u786e\u3001\u66f4\u6613\u7ba1\u7406\u7684\u64cd\u4f5c\u3002BASH-based\u65b9\u6cd5\u5728\u4ee3\u7406\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u7b80\u5355\u3001\u6a21\u5757\u5316\u7684BASH-based AI\u4ee3\u7406\u67b6\u6784\u6bd4\u590d\u6742\u3001\u8fc7\u5ea6\u8bbe\u8ba1\u7684\u7cfb\u7edf\u66f4\u6709\u6548\uff0c\u662f\u4ee3\u7406\u6210\u529f\u7684\u5173\u952e\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.a9f67b35", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fflatt.tech%2Fresearch%2Fposts%2Fpwning-claude-code-in-8-different-ways%2F%3Futm_source=tldrinfosec/1/0100019bb7af1200-69b5e91c-4eb2-4dea-beca-4a56a268e828-000000/C7ODrqx2eA6mtJVOUz1sUl7ZSP-7eeoE5Ndp2zs73Xg=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fflatt.tech%2Fresearch%2Fposts%2Fpwning-claude-code-in-8-different-ways%2F%3Futm_source=tldrinfosec/1/0100019bb7af1200-69b5e91c-4eb2-4dea-beca-4a56a268e828-000000/C7ODrqx2eA6mtJVOUz1sUl7ZSP-7eeoE5Ndp2zs73Xg=440", "authors": ["TLDR Newsletter"], "title": "Pwning Claude Code in 8 Different Ways", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fflatt.tech%2Fresearch%2Fposts%2Fpwning-claude-code-in-8-different-ways%2F%3Futm_source=tldrinfosec/1/0100019bb7af1200-69b5e91c-4eb2-4dea-beca-4a56a268e828-000000/C7ODrqx2eA6mtJVOUz1sUl7ZSP-7eeoE5Ndp2zs73Xg=440", "summary": "Pwning Claude Code in 8 Different Ways (9 minute read) Claude Code was found to execute arbitrary system commands without explicit user approval through eight distinct techniques that bypassed its command-safety mechanisms. These issues stemmed from fragile regex-based blocklists on \u201csafe\u201d commands like man, sort, sed, git, xargs, and rg, plus subtle Bash variable expansion tricks that hid real payloads. An attacker could turn supposedly read\u2011only operations into command execution paths by ab...", "source": "tldr", "AI": {"tldr": "Claude Code\u5b58\u57288\u79cd\u4e0d\u540c\u7684\u7cfb\u7edf\u547d\u4ee4\u6267\u884c\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u7ed5\u8fc7\u5176\u547d\u4ee4\u5b89\u5168\u673a\u5236\u6267\u884c\u4efb\u610f\u547d\u4ee4\uff0c\u4e3b\u8981\u95ee\u9898\u6e90\u4e8e\u8106\u5f31\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u9ed1\u540d\u5355\u548cBash\u53d8\u91cf\u6269\u5c55\u6280\u5de7\u3002", "motivation": "\u63ed\u793aClaude Code\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5c55\u793a\u5176\u547d\u4ee4\u5b89\u5168\u673a\u5236\u7684\u8106\u5f31\u6027\uff0c\u5e2e\u52a9\u7528\u6237\u548c\u5f00\u53d1\u8005\u4e86\u89e3\u6f5c\u5728\u98ce\u9669\u5e76\u6539\u8fdb\u5b89\u5168\u9632\u62a4\u3002", "method": "\u901a\u8fc7\u5206\u6790Claude Code\u7684\u547d\u4ee4\u5b89\u5168\u673a\u5236\uff0c\u53d1\u73b0\u4e868\u79cd\u4e0d\u540c\u7684\u7ed5\u8fc7\u6280\u672f\uff1a\u5305\u62ec\u5229\u7528\u8106\u5f31\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u9ed1\u540d\u5355\uff08\u9488\u5bf9man\u3001sort\u3001sed\u3001git\u3001xargs\u3001rg\u7b49\"\u5b89\u5168\"\u547d\u4ee4\uff09\u548cBash\u53d8\u91cf\u6269\u5c55\u6280\u5de7\u6765\u9690\u85cf\u771f\u5b9e\u8d1f\u8f7d\u3002", "result": "\u653b\u51fb\u8005\u80fd\u591f\u5c06\u770b\u4f3c\u53ea\u8bfb\u7684\u64cd\u4f5c\u8f6c\u5316\u4e3a\u547d\u4ee4\u6267\u884c\u8def\u5f84\uff0c\u6210\u529f\u7ed5\u8fc7Claude Code\u7684\u5b89\u5168\u9632\u62a4\uff0c\u6267\u884c\u4efb\u610f\u7cfb\u7edf\u547d\u4ee4\u800c\u65e0\u9700\u7528\u6237\u660e\u786e\u6279\u51c6\u3002", "conclusion": "Claude Code\u7684\u547d\u4ee4\u5b89\u5168\u673a\u5236\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\uff0c\u4e0d\u80fd\u4ec5\u4f9d\u8d56\u6b63\u5219\u8868\u8fbe\u5f0f\u9ed1\u540d\u5355\uff0c\u800c\u5e94\u91c7\u7528\u66f4\u5168\u9762\u7684\u5b89\u5168\u7b56\u7565\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.9a5ee86f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsentry.io%2Fproduct%2Fseer%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=seer-fy26q4-seerlaunch%26utm_content=newsletter-product-lp-learnmore/2/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/hUGxm0PZCtJIoIbIuGmvUb55YFDwFRT8q3AVIjmeIVw=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsentry.io%2Fproduct%2Fseer%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=seer-fy26q4-seerlaunch%26utm_content=newsletter-product-lp-learnmore/2/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/hUGxm0PZCtJIoIbIuGmvUb55YFDwFRT8q3AVIjmeIVw=440", "authors": ["TLDR Newsletter"], "title": "Sentry's AI Debugger Uses Your Actual Error Data \u2013 Not Just Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-13, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsentry.io%2Fproduct%2Fseer%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=seer-fy26q4-seerlaunch%26utm_content=newsletter-product-lp-learnmore/2/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/hUGxm0PZCtJIoIbIuGmvUb55YFDwFRT8q3AVIjmeIVw=440", "summary": "Sentry's AI Debugger Uses Your Actual Error Data \u2013 Not Just Code (Sponsor) Most AI coding tools only see your source code. Seer, Sentry's AI debugging agent, sees everything Sentry knows: stack traces, logs, breadcrumbs, spans, commit history, and the full error context. That's why it can pinpoint root causes with 95% accuracy \u2013 even in parts of your codebase you've never touched. How it works: Sentry logs an issue. Seer analyzes it using all available context. Seer suggests a fix, opens a PR...", "source": "tldr", "AI": {"tldr": "Sentry\u7684AI\u8c03\u8bd5\u5668Seer\u5229\u7528\u5b8c\u6574\u7684\u9519\u8bef\u4e0a\u4e0b\u6587\u6570\u636e\uff08\u5806\u6808\u8ddf\u8e2a\u3001\u65e5\u5fd7\u3001\u9762\u5305\u5c51\u7b49\uff09\u800c\u975e\u4ec5\u6e90\u4ee3\u7801\uff0c\u5b9e\u73b095%\u51c6\u786e\u7387\u7684\u6839\u56e0\u5b9a\u4f4d\u548c\u81ea\u52a8\u4fee\u590d", "motivation": "\u73b0\u6709AI\u7f16\u7a0b\u5de5\u5177\u4ec5\u80fd\u67e5\u770b\u6e90\u4ee3\u7801\uff0c\u7f3a\u4e4f\u5b8c\u6574\u7684\u9519\u8bef\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5bfc\u81f4\u8c03\u8bd5\u51c6\u786e\u6027\u53d7\u9650\u3002\u9700\u8981\u5229\u7528\u5b9e\u9645\u9519\u8bef\u6570\u636e\uff08\u5806\u6808\u8ddf\u8e2a\u3001\u65e5\u5fd7\u3001\u9762\u5305\u5c51\u3001\u63d0\u4ea4\u5386\u53f2\u7b49\uff09\u6765\u63d0\u9ad8\u8c03\u8bd5\u7cbe\u5ea6", "method": "Seer\u4f5c\u4e3aSentry\u7684AI\u8c03\u8bd5\u4ee3\u7406\uff0c\u5f53Sentry\u8bb0\u5f55\u95ee\u9898\u65f6\uff0cSeer\u5206\u6790\u6240\u6709\u53ef\u7528\u4e0a\u4e0b\u6587\uff08\u5806\u6808\u8ddf\u8e2a\u3001\u65e5\u5fd7\u3001\u9762\u5305\u5c51\u3001\u8de8\u5ea6\u3001\u63d0\u4ea4\u5386\u53f2\u548c\u5b8c\u6574\u9519\u8bef\u4e0a\u4e0b\u6587\uff09\uff0c\u7136\u540e\u5efa\u8bae\u4fee\u590d\u65b9\u6848\u5e76\u81ea\u52a8\u521b\u5efaPR", "result": "\u80fd\u591f\u4ee595%\u7684\u51c6\u786e\u7387\u5b9a\u4f4d\u6839\u672c\u539f\u56e0\uff0c\u5373\u4f7f\u5728\u5f00\u53d1\u8005\u4ece\u672a\u63a5\u89e6\u8fc7\u7684\u4ee3\u7801\u5e93\u90e8\u5206\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c", "conclusion": "\u5229\u7528\u5b8c\u6574\u7684\u9519\u8bef\u4e0a\u4e0b\u6587\u6570\u636e\u800c\u975e\u4ec5\u6e90\u4ee3\u7801\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8AI\u8c03\u8bd5\u5de5\u5177\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\uff0c\u5b9e\u73b0\u66f4\u667a\u80fd\u7684\u81ea\u52a8\u4fee\u590d", "topic": "code agent"}}
