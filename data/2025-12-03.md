<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [wechat.article](#wechat.article) [Total: 16]
- [tldr.article](#tldr.article) [Total: 3]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.SE](#cs.SE) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Deep Research: A Systematic Survey](https://arxiv.org/abs/2512.02038)
*Zhengliang Shi,Yiqun Chen,Haitao Li,Weiwei Sun,Shiyu Ni,Yougang Lyu,Run-Ze Fan,Bowen Jin,Yixuan Weng,Minjun Zhu,Qiujie Xie,Xinyu Guo,Qu Yang,Jiayi Wu,Jujia Zhao,Xiaqiang Tang,Xinbei Ma,Cunxiang Wang,Jiaxin Mao,Qingyao Ai,Jen-Tse Huang,Wenxuan Wang,Yue Zhang,Yiming Yang,Zhaopeng Tu,Zhaochun Ren*

Main category: cs.CL

TL;DR: 这篇综述系统梳理了深度研究系统，提出了三阶段路线图、四大核心组件、优化技术，并总结了评估标准和开放挑战，旨在指导这一快速发展的领域。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型已从文本生成器发展为强大的问题解决工具，但许多开放任务需要批判性思维、多源信息和可验证输出，这些超出了单次提示或标准检索增强生成的能力。深度研究旨在结合LLM的推理能力与外部工具，使LLM能够作为研究代理完成复杂的开放式任务。

Method: 1) 提出三阶段路线图并区分深度研究与相关范式；2) 介绍四大核心组件：查询规划、信息获取、记忆管理和答案生成，每个组件都有细粒度子分类；3) 总结优化技术，包括提示工程、监督微调和智能体强化学习；4) 整合评估标准和开放挑战。

Result: 提供了深度研究系统的全面系统概述，包括清晰的路线图、基础组件、实际实现技术、重要挑战和未来方向。承诺随着领域快速发展持续更新此综述。

Conclusion: 深度研究系统结合LLM推理能力与外部工具，能够解决需要批判性思维和多源信息的复杂开放任务。该综述为这一新兴领域提供了系统框架和指导，有助于推动未来研究发展。

Abstract: Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.

</details>


### [2] [A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation](https://arxiv.org/abs/2512.02195)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: MODOMA是一个用于无监督语言习得实验的计算多智能体实验室环境，通过成年和儿童智能体之间的交互实现语言习得，能够获取和表示功能与内容类别。


<details>
  <summary>Details</summary>
Motivation: 开发一个参数化、可控的计算语言习得实验系统，使研究人员能够控制实验的各个方面，并明确表示和查询习得的语法知识。

Method: 采用多智能体框架，包含成年智能体和儿童智能体，结合统计和基于规则的程序进行语言习得，生成知识型语言模型来生成和解析目标语言的新话语。

Result: 实验表明，儿童智能体能够基于成年智能体生成的不同数量的训练和测试数据，成功获取和表示功能与内容类别，且模式与人类生成数据的已知模式相似。

Conclusion: MODOMA方法在建模语言习得方面具有有效性，为计算语言习得实验提供了新的可能性，能够成功获取离散语法类别。

Abstract: This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.

</details>


### [3] [Lightweight Latent Reasoning for Narrative Tasks](https://arxiv.org/abs/2512.02240)
*Alexander Gurung,Nikolay Malkin,Mirella Lapata*

Main category: cs.CL

TL;DR: LiteReason是一种轻量级潜在推理方法，通过推理投影器模块生成连续潜在标记来跳过推理步骤，结合RL优化，在减少77-92%推理长度的同时接近非潜在RL训练的性能。


<details>
  <summary>Details</summary>
Motivation: LLMs通过生成长推理链处理复杂任务，但使用RL优化推理能力计算成本高，特别是在涉及大量标记处理的叙事相关任务中。需要一种更高效的推理方法。

Method: 提出LiteReason方法：1) 使用轻量级推理投影器模块生成连续潜在标记来跳过推理步骤；2) 在RL训练中，策略模型决定何时激活投影器，在潜在推理和离散推理之间切换；3) 可与标准标记采样交错进行。

Result: 在情节漏洞检测和书籍章节生成任务上，LiteReason优于潜在推理基线方法，接近非潜在RL训练的性能，同时减少最终推理长度77-92%，在性能-计算权衡曲线上达到更高效的位置。

Conclusion: LiteReason通过结合潜在推理和RL优化，在保持性能的同时显著减少推理长度，为LLMs的高效推理提供了一种有效方法。

Abstract: Large language models (LLMs) tackle complex tasks by generating long chains of thought or "reasoning traces" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.

</details>


### [4] [DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models](https://arxiv.org/abs/2512.02246)
*Olivia Kim*

Main category: cs.CL

TL;DR: 论文提出了DETAIL框架，用于评估大语言模型在不同提示特异性水平下的表现，发现特异性提示能提高准确性，尤其对小模型和程序性任务效果更明显。


<details>
  <summary>Details</summary>
Motivation: 提示设计对大型语言模型的推理性能至关重要，但提示特异性（提示的详细程度或模糊程度）的影响尚未得到充分研究。需要系统评估不同特异性水平下LLM的表现差异。

Method: 提出了DETAIL框架：1）使用GPT-4生成多级特异性提示；2）通过困惑度量化提示特异性；3）使用基于GPT的语义等价性评估正确性。在30个新颖推理任务上对GPT-4和O3-mini进行实验。

Result: 实验结果显示：1）提示特异性提高准确性；2）对小模型的效果比对大模型更明显；3）对程序性任务的提升比对其他类型任务更显著。

Conclusion: 研究强调了自适应提示策略的必要性，并为后续研究提供了工具和数据支持。提示特异性是影响LLM性能的重要因素，需要根据模型大小和任务类型调整提示详细程度。

Abstract: Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.

</details>


### [5] [DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models](https://arxiv.org/abs/2512.02556)
*DeepSeek-AI,Aixin Liu,Aoxue Mei,Bangcai Lin,Bing Xue,Bingxuan Wang,Bingzheng Xu,Bochao Wu,Bowei Zhang,Chaofan Lin,Chen Dong,Chengda Lu,Chenggang Zhao,Chengqi Deng,Chenhao Xu,Chong Ruan,Damai Dai,Daya Guo,Dejian Yang,Deli Chen,Erhang Li,Fangqi Zhou,Fangyun Lin,Fucong Dai,Guangbo Hao,Guanting Chen,Guowei Li,H. Zhang,Hanwei Xu,Hao Li,Haofen Liang,Haoran Wei,Haowei Zhang,Haowen Luo,Haozhe Ji,Honghui Ding,Hongxuan Tang,Huanqi Cao,Huazuo Gao,Hui Qu,Hui Zeng,Jialiang Huang,Jiashi Li,Jiaxin Xu,Jiewen Hu,Jingchang Chen,Jingting Xiang,Jingyang Yuan,Jingyuan Cheng,Jinhua Zhu,Jun Ran,Junguang Jiang,Junjie Qiu,Junlong Li,Junxiao Song,Kai Dong,Kaige Gao,Kang Guan,Kexin Huang,Kexing Zhou,Kezhao Huang,Kuai Yu,Lean Wang,Lecong Zhang,Lei Wang,Liang Zhao,Liangsheng Yin,Lihua Guo,Lingxiao Luo,Linwang Ma,Litong Wang,Liyue Zhang,M. S. Di,M. Y Xu,Mingchuan Zhang,Minghua Zhang,Minghui Tang,Mingxu Zhou,Panpan Huang,Peixin Cong,Peiyi Wang,Qiancheng Wang,Qihao Zhu,Qingyang Li,Qinyu Chen,Qiushi Du,Ruiling Xu,Ruiqi Ge,Ruisong Zhang,Ruizhe Pan,Runji Wang,Runqiu Yin,Runxin Xu,Ruomeng Shen,Ruoyu Zhang,S. H. Liu,Shanghao Lu,Shangyan Zhou,Shanhuang Chen,Shaofei Cai,Shaoyuan Chen,Shengding Hu,Shengyu Liu,Shiqiang Hu,Shirong Ma,Shiyu Wang,Shuiping Yu,Shunfeng Zhou,Shuting Pan,Songyang Zhou,Tao Ni,Tao Yun,Tian Pei,Tian Ye,Tianyuan Yue,Wangding Zeng,Wen Liu,Wenfeng Liang,Wenjie Pang,Wenjing Luo,Wenjun Gao,Wentao Zhang,Xi Gao,Xiangwen Wang,Xiao Bi,Xiaodong Liu,Xiaohan Wang,Xiaokang Chen,Xiaokang Zhang,Xiaotao Nie,Xin Cheng,Xin Liu,Xin Xie,Xingchao Liu,Xingkai Yu,Xingyou Li,Xinyu Yang,Xinyuan Li,Xu Chen,Xuecheng Su,Xuehai Pan,Xuheng Lin,Xuwei Fu,Y. Q. Wang,Yang Zhang,Yanhong Xu,Yanru Ma,Yao Li,Yao Li,Yao Zhao,Yaofeng Sun,Yaohui Wang,Yi Qian,Yi Yu,Yichao Zhang,Yifan Ding,Yifan Shi,Yiliang Xiong,Ying He,Ying Zhou,Yinmin Zhong,Yishi Piao,Yisong Wang,Yixiao Chen,Yixuan Tan,Yixuan Wei,Yiyang Ma,Yiyuan Liu,Yonglun Yang,Yongqiang Guo,Yongtong Wu,Yu Wu,Yuan Cheng,Yuan Ou,Yuanfan Xu,Yuduan Wang,Yue Gong,Yuhan Wu,Yuheng Zou,Yukun Li,Yunfan Xiong,Yuxiang Luo,Yuxiang You,Yuxuan Liu,Yuyang Zhou,Z. F. Wu,Z. Z. Ren,Zehua Zhao,Zehui Ren,Zhangli Sha,Zhe Fu,Zhean Xu,Zhenda Xie,Zhengyan Zhang,Zhewen Hao,Zhibin Gou,Zhicheng Ma,Zhigang Yan,Zhihong Shao,Zhixian Huang,Zhiyu Wu,Zhuoshu Li,Zhuping Zhang,Zian Xu,Zihao Wang,Zihui Gu,Zijia Zhu,Zilin Li,Zipeng Zhang,Ziwei Xie,Ziyi Gao,Zizheng Pan,Zongqing Yao,Bei Feng,Hui Li,J. L. Cai,Jiaqi Ni,Lei Xu,Meng Li,Ning Tian,R. J. Chen,R. L. Jin,S. S. Li,Shuang Zhou,Tianyu Sun,X. Q. Li,Xiangyue Jin,Xiaojin Shen,Xiaosha Chen,Xinnan Song,Xinyi Zhou,Y. X. Zhu,Yanping Huang,Yaohui Li,Yi Zheng,Yuchen Zhu,Yunxian Ma,Zhen Huang,Zhipeng Xu,Zhongyu Zhang,Dongjie Ji,Jian Liang,Jianzhong Guo,Jin Chen,Leyi Xia,Miaojun Wang,Mingming Li,Peng Zhang,Ruyi Chen,Shangmian Sun,Shaoqing Wu,Shengfeng Ye,T. Wang,W. L. Xiao,Wei An,Xianzu Wang,Xiaowen Sun,Xiaoxiang Wang,Ying Tang,Yukun Zha,Zekai Zhang,Zhe Ju,Zhen Zhang,Zihua Qu*

Main category: cs.CL

TL;DR: DeepSeek-V3.2通过创新的稀疏注意力机制、可扩展的强化学习框架和大规模智能体任务合成管道，实现了高效计算与卓越推理能力的平衡，在多项国际竞赛中达到顶尖水平。


<details>
  <summary>Details</summary>
Motivation: 解决当前大型语言模型在计算效率、推理能力和智能体性能之间的平衡问题，特别是在长上下文场景下的计算复杂度和智能体任务泛化能力。

Method: 1. DeepSeek稀疏注意力(DSA)：高效注意力机制降低计算复杂度；2. 可扩展强化学习框架：通过强化学习协议和扩展后训练计算；3. 大规模智能体任务合成管道：系统生成训练数据实现可扩展的智能体后训练。

Result: DeepSeek-V3.2在计算效率、推理和智能体性能方面表现优异，其高计算变体DeepSeek-V3.2-Speciale超越GPT-5，推理能力与Gemini-3.0-Pro相当，在2025年国际数学奥林匹克和信息技术奥林匹克中获得金牌表现。

Conclusion: DeepSeek-V3.2通过技术创新实现了高效计算与卓越推理能力的统一，为大型语言模型在复杂交互环境中的实际应用提供了有效解决方案。

Abstract: We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.

</details>


### [6] [Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization](https://arxiv.org/abs/2512.02665)
*Jing Ma*

Main category: cs.CL

TL;DR: 研究发现LLM在生成多文档摘要时存在显著的首因效应，摘要语义更偏向第一个输入文档，这对依赖LLM摘要的应用和智能体系统构成风险。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM被广泛用于多文档摘要（如Google AI Overviews），但其是否平等对待所有输入文档尚不清楚。研究旨在探究LLM在生成摘要时是否存在位置偏见。

Method: 针对堕胎相关新闻，构建40个支持-中立-反对文章三元组，每个三元组以6种不同顺序排列，使用Gemini 2.5 Flash生成中立摘要。通过ROUGE-L、BERTScore和SummaC评估摘要与源文章的关系，并进行ANOVA分析。

Result: BERTScore显示显著的首因效应，摘要语义与第一个看到的文章更一致。成对比较表明位置1与位置2、3有显著差异，而位置2和3之间无差异，确认了对第一个文档的选择性偏好。

Conclusion: LLM在多文档摘要中存在位置偏见，这对依赖LLM生成摘要的应用和智能体系统构成风险，因为LLM步骤可能不成比例地影响下游行动。

Abstract: Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.

</details>


### [7] [An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation](https://arxiv.org/abs/2512.02689)
*Daiki Shirafuji,Tatsuhiko Saito,Yasutomo Kimura*

Main category: cs.CL

TL;DR: 对7种模型合并算法在减轻LLM社会偏见方面的实证调查，发现偏见减少与下游性能之间存在权衡，其中SLERP在中等插值权重下表现最平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型会继承甚至放大预训练语料中的社会偏见，威胁公平性和社会信任。虽然已有研究探索通过模型参数编辑来减轻偏见，但缺乏不同算法间的实证比较。

Method: 实证调查7种模型合并算法（Linear、Karcher Mean、SLERP、NuSLERP、TIES、DELLA、Nearswap），应用于GPT、LLaMA和Qwen家族的13个开源权重模型。使用3个偏见数据集（BBQ、BOLD、HONEST）进行综合评估，并在SuperGLUE基准的下游任务中测量这些技术对LLM性能的影响。

Result: 发现偏见减少与下游性能之间存在权衡：实现更大偏见缓解的方法会降低准确性，特别是在需要阅读理解、常识和因果推理的任务上。在合并算法中，Linear、SLERP和Nearswap在保持整体性能的同时持续减少偏见，其中中等插值权重的SLERP成为最平衡的选择。

Conclusion: 模型合并算法在偏见缓解方面具有潜力，但过度的去偏见化或不适当的合并方法可能导致重要语言能力的退化。

Abstract: Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.

</details>


### [8] [Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning](https://arxiv.org/abs/2512.02874)
*Haonan Wang,Chao Du,Kenji Kawaguchi,Tianyu Pang*

Main category: cs.CL

TL;DR: ThinkMerge是一种无需训练、即插即用的解码策略，通过并行运行多个推理轨迹并在同步点平均它们的下一个token对数概率，为开放式推理任务（如代码生成和网络深度研究）生成单一连贯输出。


<details>
  <summary>Details</summary>
Motivation: 多数投票在封闭式问答中有效，但不适用于开放式推理任务，因为在完整解决方案上定义"多数"是困难的。需要一种能够从并行推理中获益而不依赖完整输出投票的方法。

Method: ThinkMerge运行K个并行推理轨迹，在同步点平均它们的下一个token对数概率，生成单一连贯输出。与vLLM/SGLang无缝集成，兼容Top-p/Top-k等标准解码技术。

Result: 在AIME和GPQA上匹配或超越多数投票；在LiveCodeBench（hard）上，DeepCoder-14B-Preview的pass@1提升+8.28%，Qwen3-8B提升+7.58%；在网络深度研究代理（WebSailor-7B/32B）上，在GAIA、BrowseComp-en/zh和XbenchDeepSearch上均有改进。

Conclusion: ThinkMerge证明并行测试时扩展可以有益于开放式推理，无需依赖完整输出的投票，为代码生成和网络深度研究等任务提供了有效的解码策略。

Abstract: Majority voting has proven effective for close-ended question answering by aggregating parallel reasoning traces. However, it is not directly applicable to open-ended reasoning, such as code generation and web-based deep research, where a "majority" over complete solutions is ill-defined. We introduce ThinkMerge, a training-free, plug-and-play decoding strategy that runs K parallel reasoning traces and averages their next-token logits at synchronization points to produce a single coherent output. ThinkMerge integrates seamlessly with vLLM/SGLang and remains compatible with standard decoding techniques such as Top-p/Top-k. Empirically, it matches or surpasses majority voting on AIME and GPQA, while delivering consistent gains on open-ended coding tasks: on LiveCodeBench (hard), pass@1 improves by +8.28% for DeepCoder-14B-Preview and +7.58% for Qwen3-8B. Beyond code, we further show that ThinkMerge improves web-based deep-research agents (e.g., WebSailor-7B/32B) across GAIA, BrowseComp-en/zh, and XbenchDeepSearch. These results demonstrate that parallel test-time scaling can benefit open-ended reasoning without relying on voting over complete outputs.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [9] [量子<em class="highlight">强化学习</em>：近期进展与未来方向](http://mp.weixin.qq.com/s?__biz=MzA5MDMwMTIyNQ==&mid=2649433993&idx=1&sn=776104ae8f5a564807754d8c1212e229&chksm=89cbb3b72f5472a08d2c99a3809daaef34c8a49f3ad4702ba44d1aff18b1effb83514932d8f5#rd)
*CreateAMind*

Main category: wechat.article

TL;DR: 量子强化学习：近期进展与未来方向https：//arxiv.org/pdf/2510.14595摘要随着量子机器学习持续发展，强化学习作为一个尤为前景广阔但尚未充分探索的前沿方向脱颖而出。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 量子强化学习：近期进展与未来方向https：//arxiv.org/pdf/2510.14595摘要随着量子机器学习持续发展，强化学习作为一个尤为前景广阔但尚未充分探索的前沿方向脱颖而出。

</details>


### [10] [<em class="highlight">强化学习</em>：AI版“挨揍中成长”的励志传奇 (REINFORCEMENT LEARNING)](http://mp.weixin.qq.com/s?__biz=MzI5NjY0NTQ4Mw==&mid=2247485522&idx=1&sn=bc758d770567eb701ed0de4ce1985345&chksm=ed4af9430d6899e430c7c9ea111faba64a2cad63772cb8f870dc423946daceb2fe4c2255fc16#rd)
*AIPM之泡泡糖*

Main category: wechat.article

TL;DR: 1. 强化学习的本质 强化学习不是学“答案”，而是学“怎么做” 学习“行为策略”而非预测答案 与监督学习不同，RL不需要“正确答案”，要靠环境提供的奖励（Reward）来指导行为


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1. 强化学习的本质 强化学习不是学“答案”，而是学“怎么做” 学习“行为策略”而非预测答案 与监督学习不同，RL不需要“正确答案”，要靠环境提供的奖励（Reward）来指导行为

</details>


### [11] [震惊！字节Seed发布<em class="highlight">强化学习</em>模型GR-RL，新机器人首次实现学习穿鞋带！](http://mp.weixin.qq.com/s?__biz=MzkzNjYzNDg0NQ==&mid=2247486485&idx=1&sn=f8c7406bccf61bbc5f2d15408e607e34&chksm=c3607dcbc98c732f340eb0ac2130a83c54f0f766bf274f38a2c68e95cc990a6860d6f28c3698#rd)
*JLE君凌元器件*

Main category: wechat.article

TL;DR: GR-RL的突破，核心在于它采用了一套从离线数据筛选到在线真机微调的强化学习框架，解决了此前单纯依赖模仿学习（如GR-3）的局限性。1. 为何“穿鞋带”是巨大挑战？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: GR-RL的突破，核心在于它采用了一套从离线数据筛选到在线真机微调的强化学习框架，解决了此前单纯依赖模仿学习（如GR-3）的局限性。1. 为何“穿鞋带”是巨大挑战？

</details>


### [12] [年初画饼Agent元年，年末只有<em class="highlight">Code</em> <em class="highlight">Agent</em>能咽得下去](http://mp.weixin.qq.com/s?__biz=MzA5NTczMTA0NA==&mid=2650603757&idx=1&sn=e251250230cd276a1e98d3e15c6cb408&chksm=8967b7212d8008a83e899f56b25b2d8a75180fd62e452e019f9733cfd06aeb617a6d3e8024bf#rd)
*三函代码*

Main category: wechat.article

TL;DR: Code Agent（代码智能体），这家伙居然偷偷发育成“六边形战士”了！它完全不按通用Agent的剧本走，在程序员圈子里已经混得如鱼得水。为什么它能成功？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Code Agent（代码智能体），这家伙居然偷偷发育成“六边形战士”了！它完全不按通用Agent的剧本走，在程序员圈子里已经混得如鱼得水。为什么它能成功？

</details>


### [13] [现场直击｜<em class="highlight">Agentic</em> AI时代，亚马逊云科技在想什么？](http://mp.weixin.qq.com/s?__biz=MzA3NzMxNTI1MQ==&mid=2649782143&idx=1&sn=ca323459d0bfcb1d0c120239b2253fb6&chksm=860ec8381ae9026a3e6a795c90ee6eaae018be86e7d840832a1ab47ae22266ceb42fb05e48c9#rd)
*钛媒体AGI*

Main category: wechat.article

TL;DR: Agentic AI时代，如何重新构建AI基础设施、更好的模型选择以及将Agent部署到关键业务场景所需的整套工具链和平台。AI基础设施：既要英伟达GPU，也要 Trainium 4


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI时代，如何重新构建AI基础设施、更好的模型选择以及将Agent部署到关键业务场景所需的整套工具链和平台。AI基础设施：既要英伟达GPU，也要 Trainium 4

</details>


### [14] [铸星计划 | <em class="highlight">代理</em>式智能（<em class="highlight">Agentic</em> AI）：重塑未来的人–<em class="highlight">智能体</em>互动与协作](http://mp.weixin.qq.com/s?__biz=MzA4NzIyMDY0OA==&mid=2655422811&idx=1&sn=f2b96393011cddf4581f7357058d546b&chksm=8af7369ea5b5f57ffce51e382df71858d40dc4cb96530cff0c0a7c69b2ae254ae78b4995b5e6#rd)
*微软学术合作*

Main category: wechat.article

TL;DR: 2025年的旗舰项目InfoAgent就是典型代表——一款能够自主规划、在网络上搜索并推理，以回答复杂信息检索问题的深度研究智能体。InfoAgent由微软联合东南大学、布朗大学等多支顶尖团队共同开发，参与者超十人，涵盖微软全职


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2025年的旗舰项目InfoAgent就是典型代表——一款能够自主规划、在网络上搜索并推理，以回答复杂信息检索问题的深度研究智能体。InfoAgent由微软联合东南大学、布朗大学等多支顶尖团队共同开发，参与者超十人，涵盖微软全职

</details>


### [15] [AI进化简史，从图灵测试到<em class="highlight">Agentic</em> AI的70年技术跃迁](http://mp.weixin.qq.com/s?__biz=MzkyNDg2OTMzMA==&mid=2247485629&idx=1&sn=087f85b845777d66913aabd2e95e1c8e&chksm=c04bba69e7ded7423c8abacb5afc8291ec0c330ef5157afa292a831373af7bb588b029d62619#rd)
*MCP研究院*

Main category: wechat.article

TL;DR: PART 07 2025，Agentic AI元年如果说2022年是生成式AI的爆发年，那么2025年正在成为Agentic AI的元年。Agentic AI的核心特点是自主性——它不再只是被动地回答问题，而是可以主动设定目标、规划步骤、调用工具、执行任务。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: PART 07 2025，Agentic AI元年如果说2022年是生成式AI的爆发年，那么2025年正在成为Agentic AI的元年。Agentic AI的核心特点是自主性——它不再只是被动地回答问题，而是可以主动设定目标、规划步骤、调用工具、执行任务。

</details>


### [16] [FDA部署“<em class="highlight">Agentic</em>模型”，推动监管工作流数字化革新](http://mp.weixin.qq.com/s?__biz=MzkyODk4NzczMg==&mid=2247485600&idx=2&sn=7e2c80ca4d6e00ceeab6dd768b197843&chksm=c3b38cd0aefe9df1af016734440e2012a09f0b99a6ca3e616393876d999496010a60acaf63e5#rd)
*未卜先智*

Main category: wechat.article

TL;DR: Agentic模型的技术定位从公开定义来看，agentic AI 被描述为一类使用“智能体（agent）”来执行复杂任务的系统，可在既定规则下进行规划、推理并完成多步动作，而非仅对单次输入进行简单生成或分类。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic模型的技术定位从公开定义来看，agentic AI 被描述为一类使用“智能体（agent）”来执行复杂任务的系统，可在既定规则下进行规划、推理并完成多步动作，而非仅对单次输入进行简单生成或分类。

</details>


### [17] [AWS的<em class="highlight">Agentic</em> AI革命](http://mp.weixin.qq.com/s?__biz=MzI1OTAwNTc2Mg==&mid=2459523659&idx=1&sn=cbff189153ac5ca34ce05c519798510b&chksm=fc77a7f44d3235481cb227cfee36513e539d174c952b201e8bdc73db20b1451d312239a5fcc7#rd)
*可荐*

Main category: wechat.article

TL;DR: #AWS #智能体 #re：InventAWS在年度re：Invent大会上发布一系列生成式人工智能创新和增强功能。聚焦Agentic AI（智能体AI）技术，AWS自研的Nova 2系列大模型、升级的Trainium AI芯片以实现更高的性能和效率，用于构建和管理AI智能体的Agent


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: #AWS #智能体 #re：InventAWS在年度re：Invent大会上发布一系列生成式人工智能创新和增强功能。聚焦Agentic AI（智能体AI）技术，AWS自研的Nova 2系列大模型、升级的Trainium AI芯片以实现更高的性能和效率，用于构建和管理AI智能体的Agent

</details>


### [18] [<em class="highlight">Agentic</em> Al六大设计模式深度解析:从ReAct到多<em class="highlight">智能体</em>协作的全栈开发指南](http://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247505174&idx=1&sn=3268a9ac6b89b34a51a8c67795814439&chksm=9a171dae495221b62be7a1452f4f3448e3dcbb1b9fce1305f068826b1d3ddaeefd36a9cdacd7#rd)
*关于NLP那些你不知道的事*

Main category: wechat.article

TL;DR: 它给智能体系统增加了一个“批判性思维”的回路。工作流程很简单：生成：主LLM先生成一个答案。评判：另一个“评判者”LLM（也可以是同一个模型的不同提示）对这个答案进行挑刺


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 它给智能体系统增加了一个“批判性思维”的回路。工作流程很简单：生成：主LLM先生成一个答案。评判：另一个“评判者”LLM（也可以是同一个模型的不同提示）对这个答案进行挑刺

</details>


### [19] [【AI科普】为什么你用<em class="highlight">大模型</em>要按Token付费？](http://mp.weixin.qq.com/s?__biz=MzUzNTc5NjA4NQ==&mid=2247504008&idx=1&sn=6a6ef163fe1337d4e18a121603ebde1e&chksm=fb610a5bb2f74636509233dcddd85089d1187d4beb99b4afbe6190848261869255d0534a5378#rd)
*晚枫AI学习笔记*

Main category: wechat.article

TL;DR: 2025年三大高性价比AI模型横评面对琳琅满目的AI模型，如何选择最适合自己的那一款？我们从通用对话、专业编程和多模态处理三个核心场景，精选出当前性价比最高的选择：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2025年三大高性价比AI模型横评面对琳琅满目的AI模型，如何选择最适合自己的那一款？我们从通用对话、专业编程和多模态处理三个核心场景，精选出当前性价比最高的选择：

</details>


### [20] [新增1款、全市第一！园区已有8家<em class="highlight">大模型</em>通过备案](http://mp.weixin.qq.com/s?__biz=MzI4MDAwODU0Ng==&mid=2649238077&idx=1&sn=2ab9a7d0df734a158105027d6272903d&chksm=f27662c7494f25d98af927e3990d352da30a347807fd65f246e4b11a128611f22867703b7536#rd)
*SIP科技创新*

Main category: wechat.article

TL;DR: 汇智大模型 汇智大模型是一款由企查查科技股份有限公司开发训练的生成式人工智能大模型。主要适用于用户因信息咨询、学习研究或其他合法目的，需要查询了解国内法律相关信息的场景。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 汇智大模型 汇智大模型是一款由企查查科技股份有限公司开发训练的生成式人工智能大模型。主要适用于用户因信息咨询、学习研究或其他合法目的，需要查询了解国内法律相关信息的场景。

</details>


### [21] [AI<em class="highlight">大模型</em>·白皮书 | AI 智能体手册-谷歌](http://mp.weixin.qq.com/s?__biz=MzIzMDgwODcyNA==&mid=2247597635&idx=2&sn=5f1ddfa046945ae69cfd1b2a1890ade6&chksm=e94b38276ed20c8551bd5547ad596c739ac84284f338476c30dada4a50c3861efedee8d42e8a#rd)
*木木自由*

Main category: wechat.article

TL;DR: ▲一起学习AI大模型，AI大模型学习路径相关资料~（精彩大模型观点、学习资料、书籍分享···等你一起来乘风破浪~）。AI·大模型·领地报告：AI 智能体手册-谷歌


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ▲一起学习AI大模型，AI大模型学习路径相关资料~（精彩大模型观点、学习资料、书籍分享···等你一起来乘风破浪~）。AI·大模型·领地报告：AI 智能体手册-谷歌

</details>


### [22] [揭秘 AI 魔法师：为什么<em class="highlight">大模型</em>能“思考”？—— 认识 Transformer 架构](http://mp.weixin.qq.com/s?__biz=MzIzMzk1OTQ5OQ==&mid=2247484234&idx=1&sn=d7373f2ddc23a46da15d667f01038436&chksm=e9dfd9bf0582b15114cca9d640a40bc8e90392fddddd5743af710c53b31446c5667c8c069150#rd)
*清风起*

Main category: wechat.article

TL;DR: 大模型是在互联网上人类产生的所有可访问文本（书籍、网页、文章、代码等）的PB级数据上训练出来的。它们通过学习这些数据，记住了人类几乎所有的知识、逻辑和表达方式。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型是在互联网上人类产生的所有可访问文本（书籍、网页、文章、代码等）的PB级数据上训练出来的。它们通过学习这些数据，记住了人类几乎所有的知识、逻辑和表达方式。

</details>


### [23] [DeepSeek V3.2 来了——AI开源<em class="highlight">大模型</em>又一重磅迭代](http://mp.weixin.qq.com/s?__biz=Mzg5MzQ1NTczNQ==&mid=2247484636&idx=1&sn=bf05daddd69139de0639f99ba70c0e4f&chksm=c19f942b7fe4de97cdf53cb8e965f5f2bcfd760637dfa82f056d648f9d139608a9bf15dec41a#rd)
*韶台澄波*

Main category: wechat.article

TL;DR: 整个 11 月的大模型圈简直是 “神仙打架” 模式 —— 谷歌和Claude先后官宣了自家的升级版大模型，紧接着各种专业评测、五花八门的应用指南就铺天盖地涌来，看得人眼花缭乱，生怕错过哪个关键信息。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 整个 11 月的大模型圈简直是 “神仙打架” 模式 —— 谷歌和Claude先后官宣了自家的升级版大模型，紧接着各种专业评测、五花八门的应用指南就铺天盖地涌来，看得人眼花缭乱，生怕错过哪个关键信息。

</details>


### [24] [盖世汽车研究院：AI<em class="highlight">大模型</em>驱动汽车轮式机器人变革发展](http://mp.weixin.qq.com/s?__biz=MjM5MDc2ODM0MA==&mid=2655135631&idx=2&sn=0d5bf0def930e73f0ab3aa3bb1a796cb&chksm=bc77feaa3edd89d18452ed1d151111c7accc9006b955ac35e79412c3d314f80db950b0ea51f9#rd)
*盖世汽车社区*

Main category: wechat.article

TL;DR: 在智能座舱领域，多模态大模型赋能座舱主动式交互和沉浸式体验，AI大模型在车载声学中调音、基于多模态交互场景化音区自动设置和生成文本图片等，基于AI大模型的单点交互和主动服务已经大规模上车，AI Agent将成为发展趋


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在智能座舱领域，多模态大模型赋能座舱主动式交互和沉浸式体验，AI大模型在车载声学中调音、基于多模态交互场景化音区自动设置和生成文本图片等，基于AI大模型的单点交互和主动服务已经大规模上车，AI Agent将成为发展趋

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [25] [Context plumbing](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finterconnected.org%2Fhome%2F2025%2F11%2F28%2Fplumbing%3Futm_source=tldrai/1/0100019ada4b4439-aff2f619-7222-4e57-af02-10316792855b-000000/UXOj34i22weCijRfmBo4hYuDAxyQkP_R7RmuQQ6j2F8=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文提出"上下文管道"概念，强调需要构建持续流动的上下文传输机制，使AI代理能高效访问动态变化的上下文信息，避免每次查询都重新查找上下文导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在访问上下文时面临效率问题：上下文是动态变化的，且不一定位于AI运行的位置。每次查询都需要查找上下文会导致性能低下，因此需要设计更高效的上下文管理机制。

Method: 提出"上下文管道"方法，工程师需要构建持续流动的管道系统，将上下文从其创建位置实时传输到将被使用的位置，实现上下文的高效流动和访问。

Result: 通过构建上下文管道，AI代理能够更高效地访问所需的上下文信息，减少查询延迟，提升代理的整体运行性能。

Conclusion: 为了使AI代理运行得更好，需要建立持续流动的上下文管道系统，将上下文从创建位置实时传输到使用位置，这是提升代理性能的关键基础设施。

Abstract: Context plumbing (6 minute read) Context is always changing, as it is dynamic. The context is not always where the AI runs. To make an agent run really well, you need to move the context to where it needs to be. Agents shouldn't have to look up context for every single query, because that's slow. Engineers need to build pipes that continuously flow potential context from where it is created to where it is going to be used.

</details>


### [26] [Charts of the Week: Code Gen Strikes Back](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.a16z.news%2Fp%2Fcharts-of-the-week-narrative-violation%3Futm_source=tldrproduct/1/0100019adebf2317-5a1ad54e-76c6-4194-975f-8cc78048902e-000000/vEQ160j_qS4wAscjjVk4l41tZ0TOxiCNGRjNP74osEQ=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 代码生成工具正在反弹，但AI叙事常被数据噪音干扰，不应过度解读短期趋势


<details>
  <summary>Details</summary>
Motivation: 分析AI领域特别是代码生成工具的真实趋势，揭示常见叙事与数据之间的差距，避免对噪音数据的过度反应

Method: 通过数据驱动的周度图表分析，考察代码生成工具使用情况、就业趋势、早期估值预测能力和数据中心扩展速度

Result: 代码生成工具正在反弹，就业趋势好坏参半，早期估值预测能力有限，数据中心以惊人速度扩展

Conclusion: 真正的教训是不要对噪音数据或整洁的叙事线过度反应，AI领域的趋势需要更细致的数据分析

Abstract: Charts of the Week: Code Gen Strikes Back (7 minute read) Most AI narratives break down under closer data: code-gen tools are rebounding, job trends are mixed, early valuations don't predict much, and data centers are scaling at extraordinary speed. The real lesson is not to overreact to noisy data or tidy storylines.

</details>


### [27] [How to write a great agents.md: Lessons from over 2,500 repositories](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fhow-to-write-a-great-agents-md-lessons-from-over-2500-repositories%2F%3Futm_source=tldrdev/1/0100019adef74ab5-0c40d54a-229b-4d9c-8654-94c3ded94534-000000/QtxFVIORgYODlxcOeq1NH9PBJqODcKAXOPe55zUMOyk=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub分析2500+个agents.md文件发现，成功的agent是专注于特定任务的专家，而非模糊的助手。好的agents.md文件应包含具体命令、代码示例和明确边界。


<details>
  <summary>Details</summary>
Motivation: 通过分析大量实际项目中的agents.md文件，了解如何编写有效的agent配置文件，帮助开发者创建更实用、更可靠的AI助手。

Method: GitHub分析了2500多个agents.md文件，研究成功agent配置的共同特征，总结最佳实践。

Result: 发现成功的agents.md文件具有以下特征：1）agent是专注于特定任务的专家；2）提供具体可执行的命令；3）包含实际代码示例；4）设定明确的边界限制；5）从简单任务开始迭代优化。

Conclusion: 编写有效的agents.md文件应专注于让agent成为特定任务的专家，提供具体指令和边界，从简单任务开始逐步迭代，而不是创建模糊的通用助手。

Abstract: How to write a great agents.md: Lessons from over 2,500 repositories (6 minute read) GitHub analyzed 2,500+ `agents.md` files and found the successful ones were specialists with clear jobs, not vague helpers. Good agents.md files give your agent specific commands to run, concrete code examples to follow, and explicit boundaries (like “never touch these files”). Start simple with one focused task like writing tests or docs, then iterate based on what breaks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [Improved Training Mechanism for Reinforcement Learning via Online Model Selection](https://arxiv.org/abs/2512.02214)
*Aida Afshar,Aldo Pacchiano*

Main category: cs.LG

TL;DR: 该论文研究强化学习中的在线模型选择问题，通过集成在线模型选择方法来提高训练效率和性能，从理论角度解决资源分配、非平稳动态适应和训练稳定性三个实际问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练中需要选择合适的模型配置（如神经网络架构、步长等），传统方法通常需要大量试错。研究者希望通过在线模型选择方法，让系统能够自适应地选择最佳配置，提高训练效率和性能。

Method: 提出在线模型选择框架，让选择器能够访问一组强化学习智能体，并学习自适应地选择正确配置。从理论角度分析三个实际标准：1）高效资源分配；2）非平稳动态下的适应；3）不同随机种子下的训练稳定性。

Result: 理论分析表明在线模型选择方法能够有效识别正确配置，并通过神经网络架构选择、步长选择和自模型选择等任务的实证结果验证了理论发现。

Conclusion: 在线模型选择方法能够显著提高强化学习的训练效率和性能，特别是在资源分配、非平稳环境适应和训练稳定性方面具有优势，为强化学习实践提供了有效的理论指导。

Abstract: We study the problem of online model selection in reinforcement learning, where the selector has access to a class of reinforcement learning agents and learns to adaptively select the agent with the right configuration. Our goal is to establish the improved efficiency and performance gains achieved by integrating online model selection methods into reinforcement learning training procedures. We examine the theoretical characterizations that are effective for identifying the right configuration in practice, and address three practical criteria from a theoretical perspective: 1) Efficient resource allocation, 2) Adaptation under non-stationary dynamics, and 3) Training stability across different seeds. Our theoretical results are accompanied by empirical evidence from various model selection tasks in reinforcement learning, including neural architecture selection, step-size selection, and self model selection.

</details>


### [29] [Dynamic Configuration of On-Street Parking Spaces using Multi Agent Reinforcement Learning](https://arxiv.org/abs/2512.02406)
*Oshada Jayasinghe,Farhana Choudhury,Egemen Tanin,Shanika Karunasekera*

Main category: cs.LG

TL;DR: 提出基于多智能体强化学习的动态路边停车位配置框架，通过优化停车空间减少交通拥堵，实验显示可降低车辆平均旅行时间损失达47%


<details>
  <summary>Details</summary>
Motivation: 随着出行需求增加，交通拥堵成为城市主要问题。路边停车占用道路空间，进一步阻碍交通流。利用车路协同技术，探索如何通过动态配置路边停车位来最小化其对交通拥堵的影响。

Method: 采用双层多智能体强化学习框架：车道级智能体负责决定每条车道的最优停车配置，使用结合LSTM和图注意力网络的Deep Q-learning架构捕捉时空相关性；区块级智能体控制车道级智能体并维持区块周围足够的停车位。使用SUMO进行实验验证。

Result: 在墨尔本市真实数据和合成数据上的实验表明，该框架能显著降低车辆平均旅行时间损失，最高可达47%，同时停车步行距离仅有轻微增加。

Conclusion: 提出的多智能体强化学习框架能有效优化动态路边停车配置，显著改善交通拥堵问题，且具有扩展到大型道路网络的潜力。

Abstract: With increased travelling needs more than ever, traffic congestion has become a major concern in most urban areas. Allocating spaces for on-street parking, further hinders traffic flow, by limiting the effective road width available for driving. With the advancement of vehicle-to-infrastructure connectivity technologies, we explore how the impact of on-street parking on traffic congestion could be minimized, by dynamically configuring on-street parking spaces. Towards that end, we formulate dynamic on-street parking space configuration as an optimization problem, and we follow a data driven approach, considering the nature of our problem. Our proposed solution comprises a two-layer multi agent reinforcement learning based framework, which is inherently scalable to large road networks. The lane level agents are responsible for deciding the optimal parking space configuration for each lane, and we introduce a novel Deep Q-learning architecture which effectively utilizes long short term memory networks and graph attention networks to capture the spatio-temporal correlations evident in the given problem. The block level agents control the actions of the lane level agents and maintain a sufficient level of parking around the block. We conduct a set of comprehensive experiments using SUMO, on both synthetic data as well as real-world data from the city of Melbourne. Our experiments show that the proposed framework could reduce the average travel time loss of vehicles significantly, reaching upto 47%, with a negligible increase in the walking distance for parking.

</details>


### [30] [Cross-Domain Offline Policy Adaptation with Dynamics- and Value-Aligned Data Filtering](https://arxiv.org/abs/2512.02435)
*Zhongjian Qiao,Rui Yang,Jiafei Lyu,Chenjia Bai,Xiu Li,Zhuoran Yang,Siyang Gao,Shuang Qiu*

Main category: cs.LG

TL;DR: 提出DVDF方法，通过同时考虑动态对齐和价值对齐，从源域数据中选择性地共享高质量样本，以提升跨域离线强化学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨域离线强化学习方法仅关注动态对齐，忽略了价值对齐的重要性。在源域和目标域动态不一致的情况下，简单合并数据会导致性能下降，需要同时考虑动态和价值对齐来选择高质量的源域样本。

Method: 提出DVDF（动态和价值对齐数据过滤）方法，从理论分析出发，建立策略在目标域评估时的次优性差距，然后设计同时考虑动态对齐和价值对齐的数据选择机制，在多种动态偏移设置下进行验证。

Result: 在包括运动学和形态学偏移的各种任务和数据集上，DVDF始终优于现有基线方法，在目标域仅含5000个转换的极低数据设置下也表现出色。

Conclusion: 动态对齐和价值对齐对于跨域离线强化学习都至关重要，DVDF方法通过同时考虑这两个因素，能够有效提升策略在目标域的性能。

Abstract: Cross-Domain Offline Reinforcement Learning aims to train an agent deployed in the target environment, leveraging both a limited target domain dataset and a source domain dataset with (possibly) sufficient data coverage. Due to the underlying dynamics misalignment between the source and target domain, simply merging the data from two datasets may incur inferior performance. Recent advances address this issue by selectively sharing source domain samples that exhibit dynamics alignment with the target domain. However, these approaches focus solely on dynamics alignment and overlook \textit{value alignment}, i.e., selecting high-quality, high-value samples from the source domain. In this paper, we first demonstrate that both dynamics alignment and value alignment are essential for policy learning, by examining the limitations of the current theoretical framework for cross-domain RL and establishing a concrete sub-optimality gap of a policy trained on the source domain and evaluated on the target domain. Motivated by the theoretical insights, we propose to selectively share those source domain samples with both high dynamics and value alignment and present our \textbf{\underline{D}}ynamics- and \textbf{\underline{V}}alue-aligned \textbf{\underline{D}}ata \textbf{\underline{F}}iltering (DVDF) method. We design a range of dynamics shift settings, including kinematic and morphology shifts, and evaluate DVDF on various tasks and datasets, as well as in challenging extremely low-data settings where the target domain dataset contains only 5,000 transitions. Extensive experiments demonstrate that DVDF consistently outperforms prior strong baselines and delivers exceptional performance across multiple tasks and datasets.

</details>


### [31] [When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents](https://arxiv.org/abs/2512.02445)
*Tsimur Hadeliya,Mohammad Ali Jauhar,Nidhi Sakpal,Diogo Cruz*

Main category: cs.LG

TL;DR: 研究发现LLM智能体在长上下文场景下存在性能和安全问题：当上下文达到100K-200K tokens时，模型性能显著下降，拒绝有害请求的行为也变得不稳定，揭示了当前长上下文智能体评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM支持更长上下文窗口和工具调用能力，先前研究主要关注长上下文提示的评估，而智能体设置的能力和安全性方面相对未被探索。本研究旨在填补这一空白。

Method: 通过评估LLM智能体在不同长度、类型和位置的上下文下的表现，分析其在长上下文场景下的任务性能和拒绝有害请求的行为变化。

Result: 发现LLM智能体对上下文敏感，在100K tokens时性能下降超过50%。拒绝率变化不可预测：GPT-4.1-nano从~5%增加到~40%，而Grok 4 Fast从~80%下降到~10%（200K tokens时）。

Conclusion: 长上下文智能体存在潜在安全问题，当前评估LLM智能体在长多步任务安全性的指标和范式需要重新审视，智能体与单纯LLM评估在能力和安全性能上存在显著差异。

Abstract: Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\sim$5\% to $\sim$40\% while Grok 4 Fast decreases from $\sim$80\% to $\sim$10\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.

</details>


### [32] [Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts](https://arxiv.org/abs/2512.02486)
*Zhongjian Qiao,Rui Yang,Jiafei Lyu,Xiu Li,Zhongxiang Dai,Zhuoran Yang,Siyang Gao,Shuang Qiu*

Main category: cs.LG

TL;DR: 本文提出DROCO算法，解决跨域离线强化学习中训练时和测试时的双重鲁棒性问题，通过RCB算子和动态值惩罚等技术增强对动态变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有跨域离线强化学习研究主要关注训练时鲁棒性（处理训练数据中的动态变化），但忽视了实际部署时测试时对动态扰动的鲁棒性。当目标域数据有限时，现有方法在动态扰动下表现脆弱。

Method: 提出鲁棒跨域Bellman（RCB）算子，增强测试时对动态扰动的鲁棒性，同时保持对分布外动态转移的保守性。引入动态值惩罚和Huber损失技术，形成DROCO算法。

Result: 在各种动态变化场景下的广泛实验结果表明，DROCO优于强基线方法，并展现出对动态扰动增强的鲁棒性。

Conclusion: DROCO算法成功解决了跨域离线强化学习中训练时和测试时的双重鲁棒性问题，通过RCB算子和辅助技术有效提升了在动态扰动下的性能表现。

Abstract: Single-domain offline reinforcement learning (RL) often suffers from limited data coverage, while cross-domain offline RL handles this issue by leveraging additional data from other domains with dynamics shifts. However, existing studies primarily focus on train-time robustness (handling dynamics shifts from training data), neglecting the test-time robustness against dynamics perturbations when deployed in practical scenarios. In this paper, we investigate dual (both train-time and test-time) robustness against dynamics shifts in cross-domain offline RL. We first empirically show that the policy trained with cross-domain offline RL exhibits fragility under dynamics perturbations during evaluation, particularly when target domain data is limited. To address this, we introduce a novel robust cross-domain Bellman (RCB) operator, which enhances test-time robustness against dynamics perturbations while staying conservative to the out-of-distribution dynamics transitions, thus guaranteeing the train-time robustness. To further counteract potential value overestimation or underestimation caused by the RCB operator, we introduce two techniques, the dynamic value penalty and the Huber loss, into our framework, resulting in the practical \textbf{D}ual-\textbf{RO}bust \textbf{C}ross-domain \textbf{O}ffline RL (DROCO) algorithm. Extensive empirical results across various dynamics shift scenarios show that DROCO outperforms strong baselines and exhibits enhanced robustness to dynamics perturbations.

</details>


### [33] [In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs](https://arxiv.org/abs/2512.02543)
*Vishnu Sarukkai,Asanshay Gupta,James Hong,Michaël Gharbi,Kayvon Fatahalian*

Main category: cs.LG

TL;DR: 提出in-context distillation方法，通过检索教师模型演示作为上下文示例，让低成本学生模型模仿教师行为，结合self-consistency cascades自适应策略，在保持准确率的同时大幅降低LLM代理推理成本。


<details>
  <summary>Details</summary>
Motivation: LLM代理在大规模执行时面临高昂的推理成本，而传统的微调方法需要长训练周期和超参数调整，手动提示工程又需要大量试错。需要一种既能降低成本又不增加开发摩擦的方法。

Method: 提出in-context distillation，将知识蒸馏思想应用于上下文学习：在每个代理步骤检索相关教师演示作为上下文示例，让学生模型即时模仿教师行为。结合self-consistency cascades策略来判断何时信任学生模型。

Result: 在ALFWorld基准上达到教师级准确率的同时成本降低2.5倍（从$0.059降至$0.024每轮），演示成本在843轮后摊销，在100万轮部署规模下节省超过$34,900。在AppWorld基准上实现2倍成本降低（同等准确率）。

Conclusion: 该方法通过降低运营成本同时保持冻结模型的快速实验周期，使先进的代理系统在经济上对更广泛的应用可行。

Abstract: The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs. However, executing agents at scale using high-capacity LLMs incurs high inference costs. We propose a simple method for reducing LLM agent inference costs without incurring the development friction costs associated with LLM fine-tuning (long training cycles, optimization hyperparameter tweaking loops) or manual prompt engineering (laborious trial and error). Most importantly, we introduce $\textit{in-context distillation}$, which adapts the idea of knowledge distillation (training a low cost-student model to mimic a high-cost teacher) to an in-context learning setting. Our approach retrieves relevant teacher demonstrations at each agent step and provides them to the student as in-context examples, enabling the student to imitate teacher behavior on-the-fly. We combine in-context distillation with the established idea of $\textit{self-consistency cascades}$ to know when the trust the student. This adaptive strategy realizes the cost benefits of model specialization while preserving the productivity of working with frozen models. On the multi-step embodied reasoning benchmark ALFWorld, our method matches teacher-level accuracy at $\textbf{2.5$\times$ lower cost}$, reducing per-episode costs from \$0.059 to \$0.024. The upfront demonstration cost amortizes after just 843 episodes, yielding cumulative savings exceeding \$34,900 at deployment scale (1M episodes). On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\textbf{2$\times$ cost reduction}$ at iso-accuracy. By reducing operational costs while maintaining rapid experimentation cycles with frozen models, our approach makes advanced agentic systems economically viable for a broader range of applications.

</details>


### [34] [CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning](https://arxiv.org/abs/2512.02551)
*Songqiao Su,Xiaofei Sun,Xiaoya Li,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.LG

TL;DR: CUDA-L2结合LLM和强化学习自动优化HGEMM CUDA内核，在1000种配置上超越主流矩阵乘法基准，包括torch.matmul、cuBLAS和cuBLASLt，离线模式下平均提升22.0%，服务器模式下提升达28.7%。


<details>
  <summary>Details</summary>
Motivation: 现有高度优化的HGEMM CUDA内核仍有改进空间，传统手动优化方法难以系统探索大规模配置空间，需要自动化方法来进一步提升性能。

Method: 结合大型语言模型和强化学习，以CUDA执行速度作为RL奖励，自动优化半精度通用矩阵乘法内核，在1000种配置上进行系统探索。

Result: 在离线模式下：比torch.matmul平均提升22.0%；比cuBLAS提升19.2%；比cuBLASLt-heuristic提升16.8%；比cuBLASLt-AutoTuning提升11.4%。在服务器模式下：提升进一步增加到28.7%、26.0%、22.4%和15.9%。

Conclusion: 即使是最性能关键、高度优化的内核如HGEMM，也能通过LLM引导的RL自动化得到改进，系统探索人类难以处理的大规模配置空间。

Abstract: In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\it cuBLAS}, {\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\% over {\it torch.matmul} on average; +19.2\% over {\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\% over {\it cuBLASLt-heuristic}, which queries {\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\% over the most competitive {\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\%, +26.0\%, +22.4\%, and +15.9\% for {\it torch.matmul}, {\it cuBLAS}, {\it cuBLASLt-heuristic}, and {\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2

</details>


### [35] [GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies](https://arxiv.org/abs/2512.02581)
*Chubin Zhang,Zhenglin Wan,Feng Chen,Xingrui Yu,Ivor Tsang,Bo An*

Main category: cs.LG

TL;DR: GoRL框架通过解耦优化与生成，使用可优化的潜在策略和条件生成解码器，在保持稳定性的同时实现高表达性，在连续控制任务中超越高斯策略和生成策略基线。


<details>
  <summary>Details</summary>
Motivation: 强化学习中存在稳定性与表达性之间的张力：高斯策略易于优化但表达能力有限（单模态），而生成策略（如扩散、流匹配）能建模多模态行为但在在线RL中不稳定（似然难处理、梯度噪声大）。

Method: 提出GoRL框架，核心是解耦优化与生成：优化一个可处理的潜在策略，同时使用条件生成解码器合成动作。采用双时间尺度更新计划，使潜在策略稳定学习，解码器逐步提升表达性，无需可处理的动作似然。

Result: 在一系列连续控制任务中，GoRL始终优于高斯策略和最近的生成策略基线。在HopperStand任务上达到归一化回报870+，是最强基线的3倍以上。

Conclusion: 解耦优化与生成为实现既稳定又高表达性的策略提供了实用路径，解决了RL中长期存在的稳定性与表达性之间的张力。

Abstract: Reinforcement learning (RL) faces a persistent tension: policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control. Gaussian policies provide tractable likelihoods and smooth gradients, but their unimodal form limits expressiveness. Conversely, generative policies based on diffusion or flow matching can model rich multimodal behaviors; however, in online RL, they are frequently unstable due to intractable likelihoods and noisy gradients propagating through deep sampling chains. We address this tension with a key structural principle: decoupling optimization from generation. Building on this insight, we introduce GoRL (Generative Online Reinforcement Learning), a framework that optimizes a tractable latent policy while utilizing a conditional generative decoder to synthesize actions. A two-timescale update schedule enables the latent policy to learn stably while the decoder steadily increases expressiveness, without requiring tractable action likelihoods. Across a range of continuous-control tasks, GoRL consistently outperforms both Gaussian policies and recent generative-policy baselines. Notably, on the HopperStand task, it reaches a normalized return above 870, more than 3 times that of the strongest baseline. These results demonstrate that separating optimization from generation provides a practical path to policies that are both stable and highly expressive.

</details>


### [36] [SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization](https://arxiv.org/abs/2512.02631)
*Zhengcheng Wang,Zichuan Lin,Yijun Yang,Haobo Fu,Deheng Ye*

Main category: cs.LG

TL;DR: 提出SeeNav-Agent框架，通过双视角视觉提示减少感知幻觉，并设计SRGPO强化微调方法提升VLN智能体导航性能


<details>
  <summary>Details</summary>
Motivation: 现有基于大视觉语言模型的视觉语言导航智能体存在感知错误、推理错误和规划错误，严重影响了导航性能

Method: 1. 引入双视角视觉提示技术减少感知幻觉；2. 设计SRGPO强化微调方法，通过定义可验证过程奖励和随机分组步级优势估计来提供密集奖励信号

Result: GPT-4.1结合零-shot视觉提示达到86.7%导航成功率，比当前最佳LVLM提升约20个百分点；Qwen2.5-VL-3B模型通过SRGPO微调达到72.3%成功率，比最佳现有LVLM模型提升5.6个百分点

Conclusion: SeeNav-Agent框架通过视觉提示和SRGPO强化微调有效解决了VLN智能体的感知和规划问题，显著提升了导航性能

Abstract: Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.

</details>


### [37] [OptPO: Optimal Rollout Allocation for Test-time Policy Optimization](https://arxiv.org/abs/2512.02882)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: OptPO是一种用于测试时策略优化的最优滚动分配框架，通过贝叶斯序列概率比测试动态分配推理预算，减少计算冗余同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时策略优化方法依赖固定预算的多数投票来估计奖励，导致大量计算冗余。需要一种能自适应分配推理预算的框架来提高计算效率。

Method: 提出OptPO框架，将投票过程建模为贝叶斯序列概率比测试，动态停止采样（当后验置信度超过阈值），并利用保留的滚动进行在线策略更新，可与PPO或GRPO等算法无缝集成。

Result: 在多样化推理基准测试中，OptPO相比固定样本基线显著减少了滚动开销，同时保持或提高了准确性。

Conclusion: OptPO通过将统计最优停止与测试时学习相结合，为测试时适应提供了一个计算高效的范式。

Abstract: Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [38] [The 4/$δ$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee](https://arxiv.org/abs/2512.02080)
*PIerre Dantas,Lucas Cordeiro,Youcheng Sun,Waldir Junior*

Main category: cs.AI

TL;DR: 该论文提出了首个具有可证明终止性和收敛性保证的LLM-验证器形式化框架，通过马尔可夫链建模和误差减少概率参数，为LLM辅助的软件验证提供了理论支撑和实用设计阈值。


<details>
  <summary>Details</summary>
Motivation: 当前将形式化验证工具与大型语言模型结合的方法缺乏理论基础，导致验证过程不可靠、不稳定，无法为安全关键软件环境提供可预测的性能保证。

Method: 建立LLM-验证器收敛定理，将LLM与验证器的交互建模为离散时间马尔可夫链，引入误差减少概率参数δ，并进行了超过90,000次试验的实证验证。

Result: 理论证明对于任何δ>0，程序几乎必然终止，期望迭代次数有界E[n]≤4/δ。实证结果与理论高度一致，所有运行都达到验证，收敛因子Cf≈1.0。

Conclusion: 该工作为LLM辅助验证提供了坚实的理论基础和实用设计框架，支持可预测的资源规划和性能预算，特别适用于安全关键软件环境。

Abstract: The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($δ$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $δ> 0$, with an expected iteration count bounded by $\mathbb{E}[n] \leq 4/δ$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.

</details>


### [39] [STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI, AI Assistants, or LLM Calls](https://arxiv.org/abs/2512.02228)
*Shubhi Asthana,Bing Zhang,Chad DeLuca,Ruchi Mahindru,Hima Patel*

Main category: cs.AI

TL;DR: STRIDE框架帮助系统选择AI部署模式：直接LLM调用、引导式AI助手或完全自主的智能体，避免不必要的智能体部署，节省成本。


<details>
  <summary>Details</summary>
Motivation: 从无状态大语言模型向自主智能体的快速转变引发核心问题：何时真正需要智能体AI？不加区分地部署智能体会导致成本、复杂性和风险增加。

Method: 提出STRIDE框架，通过结构化任务分解、动态性归因和自我反思需求分析，生成智能体适用性评分，为任务推荐三种部署模式之一。

Result: 在30个真实世界任务中，STRIDE在模式选择上达到92%准确率，减少45%不必要的智能体部署，降低37%资源成本，专家验证确认其实用性。

Conclusion: 将智能体采用重新定义为需求驱动的设计决策，确保自主性仅在收益证明成本合理时才被应用。

Abstract: The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk.
  We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context.
  Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.

</details>


### [40] [Benchmarking LLM Agents for Wealth-Management Workflows](https://arxiv.org/abs/2512.02230)
*Rory Milsom*

Main category: cs.AI

TL;DR: 该论文扩展了TheAgentCompany框架，创建了财富管理领域的评估基准，发现LLM代理在端到端工作流可靠性方面存在限制，而非数学推理能力，且自主性水平对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 现代数字协作工具仍存在人为错误和延迟问题，需要研究通用LLM代理能否准确且经济地完成财富管理任务，以填补这一研究空白。

Method: 扩展TheAgentCompany框架，创建金融领域环境，引入合成领域数据，丰富同事模拟，构建包含12个任务对的财富管理助手基准，涵盖检索、分析和综合/沟通任务，并设计高/低自主性变体。

Result: 代理的主要限制在于端到端工作流可靠性而非数学推理能力，自主性水平对性能有显著影响，现有模型评估方法阻碍了有效的基准测试。

Conclusion: 需要创建有意义的评估集来衡量代理在财富管理助手工作中的适用性，并改进评估方法以准确反映代理的实际能力。

Abstract: Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay. To address this gap, this dissertation extends TheAgentCompany with a finance-focused environment and investigates whether a general purpose LLM agent can complete representative wealth-management tasks both accurately and economically. This study introduces synthetic domain data, enriches colleague simulations, and prototypes an automatic task-generation pipeline. The study aims to create and assess an evaluation set that can meaningfully measure an agent's fitness for assistant-level wealth management work. We construct a benchmark of 12 task-pairs for wealth management assistants spanning retrieval, analysis, and synthesis/communication, with explicit acceptance criteria and deterministic graders. We seeded a set of new finance-specific data and introduced a high vs. low-autonomy variant of every task. The paper concluded that agents are limited less by mathematical reasoning and more so by end-to-end workflow reliability, and meaningfully affected by autonomy level, and that incorrect evaluation of models have hindered benchmarking.

</details>


### [41] [TradeTrap: Are LLM-based Trading Agents Truly Reliable and Faithful?](https://arxiv.org/abs/2512.02261)
*Lewen Yan,Jilin Mei,Tianyi Zhou,Lige Huang,Jie Zhang,Dongrui Liu,Jing Shao*

Main category: cs.AI

TL;DR: TradeTrap是一个统一的评估框架，用于系统性地压力测试自适应和程序化自主交易代理的鲁棒性，通过针对四个核心组件施加系统级扰动，发现小扰动可在决策循环中传播并导致极端风险。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的交易代理在真实金融市场中部署越来越多，但在对抗性或故障条件下的可靠性和鲁棒性尚未得到充分检验，而金融环境具有高风险和不可逆性。

Method: 提出TradeTrap框架，针对自主交易代理的四个核心组件（市场情报、策略制定、投资组合和账本处理、交易执行）施加系统级扰动，在真实美股市场数据的闭环历史回测环境中进行公平可重复的比较。

Result: 实验表明，单个组件的小扰动可在代理决策循环中传播，导致极端集中、失控暴露和大幅投资组合回撤，证明当前自主交易代理可在系统层面被系统性误导。

Conclusion: 当前自主交易代理在系统层面存在脆弱性，TradeTrap框架为评估和改进交易代理的鲁棒性提供了系统化方法，有助于提高金融AI系统的可靠性。

Abstract: LLM-based trading agents are increasingly deployed in real-world financial markets to perform autonomous analysis and execution. However, their reliability and robustness under adversarial or faulty conditions remain largely unexamined, despite operating in high-risk, irreversible financial environments. We propose TradeTrap, a unified evaluation framework for systematically stress-testing both adaptive and procedural autonomous trading agents. TradeTrap targets four core components of autonomous trading agents: market intelligence, strategy formulation, portfolio and ledger handling, and trade execution, and evaluates their robustness under controlled system-level perturbations. All evaluations are conducted in a closed-loop historical backtesting setting on real US equity market data with identical initial conditions, enabling fair and reproducible comparisons across agents and attacks. Extensive experiments show that small perturbations at a single component can propagate through the agent decision loop and induce extreme concentration, runaway exposure, and large portfolio drawdowns across both agent types, demonstrating that current autonomous trading agents can be systematically misled at the system level. Our code is available at https://github.com/Yanlewen/TradeTrap.

</details>


### [42] [DialogGuard: Multi-Agent Psychosocial Safety Evaluation of Sensitive LLM Responses](https://arxiv.org/abs/2512.02282)
*Han Luo,Guy Laban*

Main category: cs.AI

TL;DR: DialogGuard是一个多智能体框架，用于评估LLM生成内容在五个高风险维度上的心理社会安全风险，包括隐私侵犯、歧视行为、心理操纵、心理伤害和侮辱行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型现在被用于许多基于网络的心理健康、危机和其他情感敏感服务，但它们在心理社会安全方面的表现仍然缺乏深入理解和有效评估。

Method: 提出DialogGuard多智能体框架，通过四种LLM-as-a-judge流程（单智能体评分、双智能体校正、多智能体辩论和随机多数投票）来评估心理社会风险，基于共享的三级评分标准。

Result: 多智能体机制比非LLM基线和单智能体评估更准确地检测心理社会风险；双智能体校正和多数投票在准确性、与人类评分的一致性和鲁棒性之间达到最佳平衡；辩论方法召回率更高但会过度标记边界案例。

Conclusion: DialogGuard作为开源软件发布，提供每个维度的风险评分和可解释的自然语言理由，支持面向脆弱用户的网络应用的提示设计、审计和监督。

Abstract: Large language models (LLMs) now mediate many web-based mental-health, crisis, and other emotionally sensitive services, yet their psychosocial safety in these settings remains poorly understood and weakly evaluated. We present DialogGuard, a multi-agent framework for assessing psychosocial risks in LLM-generated responses along five high-severity dimensions: privacy violations, discriminatory behaviour, mental manipulation, psychological harm, and insulting behaviour. DialogGuard can be applied to diverse generative models through four LLM-as-a-judge pipelines, including single-agent scoring, dual-agent correction, multi-agent debate, and stochastic majority voting, grounded in a shared three-level rubric usable by both human annotators and LLM judges. Using PKU-SafeRLHF with human safety annotations, we show that multi-agent mechanisms detect psychosocial risks more accurately than non-LLM baselines and single-agent judging; dual-agent correction and majority voting provide the best trade-off between accuracy, alignment with human ratings, and robustness, while debate attains higher recall but over-flags borderline cases. We release Dialog-Guard as open-source software with a web interface that provides per-dimension risk scores and explainable natural-language rationales. A formative study with 12 practitioners illustrates how it supports prompt design, auditing, and supervision of web-facing applications for vulnerable users.

</details>


### [43] [PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing](https://arxiv.org/abs/2512.02589)
*Junyi Hou,Andre Lin Huikai,Nuo Chen,Yiwei Gong,Bingsheng He*

Main category: cs.AI

TL;DR: PaperDebugger是一个集成在LaTeX编辑器中的多智能体学术写作助手，通过Chrome扩展、Kubernetes编排层和MCP工具链实现深度文档交互和智能写作支持。


<details>
  <summary>Details</summary>
Motivation: 现有LLM写作助手通常独立于编辑器之外，无法与文档状态、结构和修订历史深度交互，限制了在LaTeX编辑器（如Overleaf）中实现上下文感知的智能操作。

Method: 开发了一个Chrome批准的扩展程序，包含Kubernetes原生编排层和Model Context Protocol工具链，支持文献搜索、参考文献查找、文档评分和修订管道等功能，实现可靠的双向同步、细粒度版本控制、安全状态管理和多智能体调度。

Result: 展示了完全集成的工作流程，包括本地化编辑、结构化评审、并行智能体执行和基于差异的更新，通过最小侵入式UI实现。早期聚合分析显示用户积极参与，验证了编辑器原生智能写作助手的实用性。

Conclusion: PaperDebugger成功解决了在编辑器内集成LLM驱动推理的技术挑战，为学术写作提供了深度交互、上下文感知的智能助手，展示了编辑器原生多智能体系统的可行性。

Abstract: Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.

</details>


### [44] [IACT: A Self-Organizing Recursive Model for General AI Agents: A Technical White Paper on the Architecture Behind kragent.ai](https://arxiv.org/abs/2512.02605)
*Pengju Lu*

Main category: cs.AI

TL;DR: IACT是一种计算模型，通过用户对话驱动的动态递归代理拓扑来解决静态硬编码工作流的局限性，支持运行时错误纠正和模糊性解析。


<details>
  <summary>Details</summary>
Motivation: 解决静态硬编码代理工作流的局限性，传统系统需要预定义图或专门编程，无法适应开放任务。

Method: 采用基于用户对话的通用自主系统，动态增长递归代理拓扑，通过双向状态对话替代单向函数调用，实现运行时错误纠正。

Result: 在kragent.ai系统中成功部署，通过实际工作流提供定性证据，展示其适应开放任务的能力。

Conclusion: IACT模型通过动态代理拓扑和交互冗余有效解决了传统代理系统的局限性，为开放任务提供了灵活解决方案。

Abstract: This technical white paper introduces the Interactive Agents Call Tree (IACT), a computational model designed to address the limitations of static, hard-coded agent workflows. Unlike traditional systems that require pre-defined graphs or specialized programming, IACT operates as a general-purpose autonomous system driven purely by user dialogue. Given a high-level objective, the system autonomously grows a dynamic, recursive agent topology incrementally tailored to the problem's structure. This allows it to scale its organizational complexity to match open-ended tasks. To mitigate the error propagation inherent in unidirectional function calls, IACT introduces interactional redundancy by replacing rigid invocations with bidirectional, stateful dialogues. This mechanism enables runtime error correction and ambiguity resolution. We describe the architecture, design principles, and practical lessons behind the production deployment of this model in the kragent.ai system, presenting qualitative evidence from real-world workflows rather than exhaustive benchmark results.

</details>


### [45] [Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games](https://arxiv.org/abs/2512.02358)
*Ran Zhang,Kun Ouyang,Tiancheng Ma,Yida Yang,Dong Fang*

Main category: cs.AI

TL;DR: 提出基于大语言模型的生成式智能体MMO模拟系统，通过监督微调和强化学习使LLM适应游戏特定领域，结合数据驱动的环境模型，为数值设计优化提供可靠、可解释、低成本的框架。


<details>
  <summary>Details</summary>
Motivation: 传统MMO游戏数值系统和机制设计优化依赖大规模在线实验或基于预定义统计模型的参数调优，成本高、耗时长且可能破坏玩家体验。简化离线模拟系统保真度有限，无法准确模拟真实玩家的推理和干预反应。

Method: 提出基于大语言模型的生成式智能体MMO模拟系统：1) 在大规模真实玩家行为数据上应用监督微调和强化学习，使LLM从通用先验适应游戏特定领域；2) 基于真实游戏日志训练数据驱动的环境模型，重建动态游戏系统。

Result: 实验表明系统与真实世界玩家行为具有强一致性，在干预下产生合理的因果响应，为数据驱动的数值设计优化提供可靠、可解释、成本高效的框架。

Conclusion: 基于LLM的生成式智能体模拟系统能够有效解决传统MMO游戏数值设计优化的局限性，提供高保真、可解释且成本效益高的解决方案。

Abstract: Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.

</details>


### [46] [Semantic Trading: Agentic AI for Clustering and Relationship Discovery in Prediction Markets](https://arxiv.org/abs/2512.02436)
*Agostino Capponi,Alfio Gliozzo,Brian Zhu*

Main category: cs.AI

TL;DR: AI代理通过自然语言理解聚类预测市场并识别市场间的依赖关系，构建交易策略获得约20%的周回报率


<details>
  <summary>Details</summary>
Motivation: 预测市场存在碎片化问题，包括重叠问题、隐含等价关系和隐藏矛盾，需要系统方法来发现市场间的潜在语义结构

Method: 开发AI代理管道：1）基于合同文本和元数据的自然语言理解进行市场聚类；2）识别集群内市场对的结果依赖关系（正相关和负相关）

Result: 代理识别的关系准确率达60-70%，基于这些关系构建的交易策略在周度时间范围内获得约20%的平均回报

Conclusion: AI代理和大语言模型能够有效发现预测市场中的潜在语义结构，为交易策略提供可操作的信号

Abstract: Prediction markets allow users to trade on outcomes of real-world events, but are prone to fragmentation through overlapping questions, implicit equivalences, and hidden contradictions across markets. We present an agentic AI pipeline that autonomously (i) clusters markets into coherent topical groups using natural-language understanding over contract text and metadata, and (ii) identifies within-cluster market pairs whose resolved outcomes exhibit strong dependence, including same-outcome (correlated) and different-outcome (anti-correlated) relationships. Using a historical dataset of resolved markets on Polymarket, we evaluate the accuracy of the agent's relational predictions. We then translate discovered relationships into a simple trading strategy to quantify how these relationships map to actionable signals. Results show that agent-identified relationships achieve roughly 60-70% accuracy, and their induced trading strategies earn about 20% average returns over week-long horizons, highlighting the ability of agentic AI and large language models to uncover latent semantic structure in prediction markets.

</details>


### [47] [Zero-Shot Instruction Following in RL via Structured LTL Representations](https://arxiv.org/abs/2512.02633)
*Mattia Giuri,Mathias Jackermeier,Alessandro Abate*

Main category: cs.AI

TL;DR: 提出一种新方法，通过将策略条件化于布尔公式序列来学习遵循任意LTL指令的多任务策略，使用GNN编码结构化任务表示，在复杂象棋环境中验证优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多个高级事件同时发生且可能复杂交互的环境中存在不足，需要改进LTL框架下强化学习代理的多任务策略学习。

Method: 将策略条件化于与自动机转换直接对齐的简单布尔公式序列，通过图神经网络编码这些结构化任务表示。

Result: 在复杂的象棋环境中进行实验，证明了该方法相对于现有方法的优势。

Conclusion: 提出的方法能够有效处理多个高级事件同时发生且复杂交互的环境，为LTL框架下的多任务强化学习提供了改进方案。

Abstract: Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach.

</details>


### [48] [Self-Improving AI Agents through Self-Play](https://arxiv.org/abs/2512.02731)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 该论文将心理测量电池的模数理论框架扩展到动力系统领域，形式化地将智能体表示为计算资源参数化的流，推导出保证自我改进稳定性的方差不等式，并统一了语言自玩、自我修正和合成数据引导等架构。


<details>
  <summary>Details</summary>
Motivation: 将静态的AAI能力评分扩展到动态系统，为智能体的自我改进过程建立数学框架，统一理解当前各种自我改进架构（如STaR、SPIN、Reflexion等）背后的共同原理。

Method: 提出生成器-验证器-更新器（GVU）算子，证明该算子在参数流形上生成向量场，将自我改进系数κ定义为能力泛函沿该流的Lie导数，推导出保证自我改进稳定性的方差不等式这一谱条件。

Result: 建立了智能体自我改进的动力学理论框架，证明了方差不等式是自我改进稳定性的充分条件，展示了STaR、SPIN、Reflexion、GANs和AlphaZero等架构都是GVU算子的特定拓扑实现，通过过滤、对抗判别或形式系统基础满足方差不等式。

Conclusion: 该工作为智能体自我改进提供了统一的数学框架，将各种看似不同的架构统一为GVU算子的具体实现，方差不等式为设计和分析自我改进系统提供了理论指导。

Abstract: We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $ν_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $Θ$, and we identify the coefficient of self-improvement $κ$ as the Lie derivative of the capability functional along this flow.
  The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $κ> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.
  We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.

</details>


### [49] [Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents](https://arxiv.org/abs/2512.02812)
*Zijie Lin,Qilin Cai,Liang Shen,Mingjun Xiao*

Main category: cs.AI

TL;DR: 提出无提示协作代理框架，通过验证和精炼代理自动提升论文到代码生成的准确性和完整性，相比基线提升约15%和13%


<details>
  <summary>Details</summary>
Motivation: 现有自动化论文复现框架缺乏对每个生成步骤输出的验证和精炼机制，或过度依赖人工设计的提示进行自我精炼，限制了适应性和可扩展性

Method: 提出无提示协作代理框架，包含验证代理（检查输出是否满足系统提示要求）和精炼代理（基于识别问题修订输出），仅使用原始系统提示实现自动验证和改进

Result: 在PaperBench Code-Dev和Paper2CodeBench数据集上显著提升复现代码的准确性和完整性，相比无代理基线分别获得约15%和13%的性能提升，且比Self-Refine方法更稳健一致

Conclusion: 提出的无提示协作代理框架能有效提高论文到代码生成的质量，无需人工设计精炼提示，具有更好的适应性和可扩展性

Abstract: Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\% and 13\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.

</details>


### [50] [Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning](https://arxiv.org/abs/2512.02914)
*Zhonghao He,Tianyi Qiu,Hirokazu Shirado,Maarten Sap*

Main category: cs.AI

TL;DR: 该研究提出了一个基于鞅属性的无监督评估框架，用于检测LLM推理中的信念固化现象，发现迭代推理可能导致确认偏差而非真相寻求行为。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM的推理能力有所提升，但迭代推理可能引发信念固化和确认偏差，而非增强真相寻求行为。需要系统评估LLM推理中的信念固化现象。

Method: 利用贝叶斯统计中的鞅属性，提出无监督的回归基鞅分数来衡量违反该属性的程度。在事件预测、价值负载问题和学术论文评审等开放领域进行评估。

Result: 研究发现信念固化现象在模型和设置中普遍存在，当前信念能正向预测未来信念更新。识别了更易出现信念固化的模型、推理技术和领域。鞅分数能预测有标签领域的真实准确性。

Conclusion: 鞅分数作为无监督指标，即使在无法获取真实标签的领域也能有效评估推理过程的真相寻求能力，揭示了LLM推理中普遍存在的信念固化问题。

Abstract: Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [51] [Process-Centric Analysis of Agentic Software Systems](https://arxiv.org/abs/2512.02393)
*Shuyang Liu,Yang Chen,Rahul Krishna,Saurabh Sinha,Jatin Ganhotra,Reyhan Jabbarvand*

Main category: cs.SE

TL;DR: 论文提出Graphectory框架，将智能体系统的执行轨迹编码为图结构，支持过程中心化分析，揭示智能体编程工作流的质量和效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统评估过于结果中心化，只关注最终成功或失败，忽视了智能体推理、规划、行动和策略演变的详细过程。需要系统化方法来分析智能体工作流的执行轨迹。

Method: 提出Graphectory框架，将智能体系统的执行轨迹编码为包含时间和语义关系的图结构。使用该框架分析4000个轨迹，涵盖SWE-agent和OpenHands两种主流智能体编程工作流，结合四种骨干大语言模型在SWE-bench Verified问题上的表现。

Result: 分析发现：(1) 使用更丰富提示或更强LLM的智能体产生更复杂的Graphectory，反映更深度的探索、更广泛的上下文收集和更彻底的验证；(2) 问题解决策略随问题难度和底层LLM变化，已解决问题遵循连贯的定位-修补-验证步骤，未解决问题呈现混乱、重复或回溯行为；(3) 即使成功，智能体编程系统常显示低效过程，导致不必要的冗长轨迹。

Conclusion: Graphectory框架为智能体系统提供了过程中心化的评估方法，揭示了智能体工作流的质量和效率特征，有助于改进智能体系统的设计和优化。

Abstract: Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success.
  Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.

</details>


### [52] [Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System](https://arxiv.org/abs/2512.02567)
*Martin Weiss,Jesko Hecking-Harbusch,Jochen Quante,Matthias Woehrle*

Main category: cs.SE

TL;DR: 研究自动化C到Rust代码翻译系统中三个关键因素：反馈循环、LLM选择和代码扰动，发现反馈循环能显著缩小不同LLM之间的性能差异，代码扰动甚至能提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 虽然生成式AI在软件工程任务中应用广泛，但自动化方法需要更高可靠性才能用于工业实践。本研究关注影响代码翻译质量的三个关键因素：自动化反馈循环、LLM选择和代码扰动的影响。

Method: 基于生成-检查模式的C到Rust翻译系统：LLM生成Rust代码后自动检查可编译性和行为等价性，失败时通过反馈循环重新提示LLM修复输出。通过改变三个变量（反馈循环、LLM模型、代码扰动）来评估系统成功率。

Result: 无反馈循环时LLM选择对翻译成功率影响很大；使用反馈循环后不同模型间的差异显著缩小。代码扰动提供的多样性甚至能提升系统性能，系统在代码扰动下的鲁棒性也得到改善。

Conclusion: 反馈循环是提升自动化代码翻译系统可靠性的关键机制，能减少对特定LLM的依赖；代码扰动带来的多样性有助于提升系统性能，这对工业应用具有重要意义。

Abstract: The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes.
  We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables.
  Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.

</details>


### [53] ["Can you feel the vibes?": An exploration of novice programmer engagement with vibe coding](https://arxiv.org/abs/2512.02750)
*Kiev Gama,Filipe Calegario,Victoria Jackson,Alexander Nolte,Luiz Augusto Morais,Vinicius Garcia*

Main category: cs.SE

TL;DR: 研究通过教育黑客松探索新手程序员如何参与"氛围编程"，发现其能促进快速原型设计和跨学科协作，但也存在创意过早收敛、代码质量不均等问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和AI辅助编码的兴起，"氛围编程"（通过自然语言提示而非直接编写代码来创建软件）有望民主化软件开发，但其教育影响尚未得到充分探索。

Method: 在巴西公立大学组织为期一天的教育黑客松，31名来自计算和非计算学科的本科生分为9个团队。通过观察、退出调查和半结构化访谈，研究创意过程、工具使用模式、协作动态和学习成果。

Result: 氛围编程实现了快速原型设计和跨学科协作，参与者发展了提示工程技能并在时间限制内交付了功能演示。但观察到创意过早收敛、代码质量不均需要返工、对核心软件工程实践参与有限。团队采用结合多种AI工具的复杂工作流，人类判断在关键改进中仍必不可少。

Conclusion: 氛围编程黑客松可作为有价值的低风险学习环境，但需要结合明确的支架来支持发散思维、批判性评估AI输出，以及对生产质量有现实期望。

Abstract: Emerging alongside generative AI and the broader trend of AI-assisted coding, the term "vibe coding" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.

</details>
