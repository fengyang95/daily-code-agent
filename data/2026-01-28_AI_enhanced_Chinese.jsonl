{"id": "2601.18827", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18827", "abs": "https://arxiv.org/abs/2601.18827", "authors": ["Jens Kohl", "Otto Kruse", "Youssef Mostafa", "Andre Luckow", "Karsten Schroer", "Thomas Riedl", "Ryan French", "David Katz", "Manuel P. Luitz", "Tanrajbir Takher", "Ken E. Friedl", "C\u00e9line Laurent-Winter"], "title": "Automated structural testing of LLM-based agents: methods, framework, and case studies", "comment": "10 pages, 5 figures. Preprint of an accepted paper at IEEE BigData 2025 (main track). Source code for the introduced methods and framework available at https://github.com/awslabs/generative-ai-toolkit", "summary": "LLM-based agents are rapidly being adopted across diverse domains. Since they interact with users without supervision, they must be tested extensively. Current testing approaches focus on acceptance-level evaluation from the user's perspective. While intuitive, these tests require manual evaluation, are difficult to automate, do not facilitate root cause analysis, and incur expensive test environments. In this paper, we present methods to enable structural testing of LLM-based agents. Our approach utilizes traces (based on OpenTelemetry) to capture agent trajectories, employs mocking to enforce reproducible LLM behavior, and adds assertions to automate test verification. This enables testing agent components and interactions at a deeper technical level within automated workflows. We demonstrate how structural testing enables the adaptation of software engineering best practices to agents, including the test automation pyramid, regression testing, test-driven development, and multi-language testing. In representative case studies, we demonstrate automated execution and faster root-cause analysis. Collectively, these methods reduce testing costs and improve agent quality through higher coverage, reusability, and earlier defect detection. We provide an open source reference implementation on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7ed3\u6784\u6d4b\u8bd5\u7684LLM\u667a\u80fd\u4f53\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4f7f\u7528\u8ffd\u8e2a\u3001\u6a21\u62df\u548c\u65ad\u8a00\u5b9e\u73b0\u81ea\u52a8\u5316\u6d4b\u8bd5\uff0c\u964d\u4f4e\u6d4b\u8bd5\u6210\u672c\u5e76\u63d0\u9ad8\u8d28\u91cf", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u6d4b\u8bd5\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7528\u6237\u89c6\u89d2\u7684\u9a8c\u6536\u6d4b\u8bd5\uff0c\u9700\u8981\u4eba\u5de5\u8bc4\u4f30\u3001\u96be\u4ee5\u81ea\u52a8\u5316\u3001\u4e0d\u652f\u6301\u6839\u56e0\u5206\u6790\u4e14\u6d4b\u8bd5\u73af\u5883\u6602\u8d35\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6d4b\u8bd5\u65b9\u6cd5", "method": "\u4f7f\u7528OpenTelemetry\u8ffd\u8e2a\u6355\u83b7\u667a\u80fd\u4f53\u8f68\u8ff9\uff0c\u901a\u8fc7\u6a21\u62df\u786e\u4fdd\u53ef\u91cd\u73b0\u7684LLM\u884c\u4e3a\uff0c\u6dfb\u52a0\u65ad\u8a00\u5b9e\u73b0\u81ea\u52a8\u5316\u6d4b\u8bd5\u9a8c\u8bc1\uff0c\u652f\u6301\u7ec4\u4ef6\u7ea7\u548c\u4ea4\u4e92\u7ea7\u6d4b\u8bd5", "result": "\u5728\u6848\u4f8b\u7814\u7a76\u4e2d\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u6267\u884c\u548c\u66f4\u5feb\u7684\u6839\u56e0\u5206\u6790\uff0c\u964d\u4f4e\u4e86\u6d4b\u8bd5\u6210\u672c\uff0c\u901a\u8fc7\u66f4\u9ad8\u8986\u76d6\u7387\u3001\u53ef\u91cd\u7528\u6027\u548c\u65e9\u671f\u7f3a\u9677\u68c0\u6d4b\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u8d28\u91cf", "conclusion": "\u7ed3\u6784\u6d4b\u8bd5\u65b9\u6cd5\u4f7f\u8f6f\u4ef6\u5de5\u7a0b\u6700\u4f73\u5b9e\u8df5\uff08\u6d4b\u8bd5\u81ea\u52a8\u5316\u91d1\u5b57\u5854\u3001\u56de\u5f52\u6d4b\u8bd5\u3001\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\u3001\u591a\u8bed\u8a00\u6d4b\u8bd5\uff09\u80fd\u591f\u5e94\u7528\u4e8e\u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u4e86\u5f00\u6e90\u5b9e\u73b0", "topic": "agent analysis"}}
{"id": "2601.18847", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18847", "abs": "https://arxiv.org/abs/2601.18847", "authors": ["Zihan Wu", "Jie Xu", "Yun Peng", "Chun Yong Chong", "Xiaohua Jia"], "title": "MulVul: Retrieval-augmented Multi-Agent Code Vulnerability Detection via Cross-Model Prompt Evolution", "comment": null, "summary": "Large Language Models (LLMs) struggle to automate real-world vulnerability detection due to two key limitations: the heterogeneity of vulnerability patterns undermines the effectiveness of a single unified model, and manual prompt engineering for massive weakness categories is unscalable.\n  To address these challenges, we propose \\textbf{MulVul}, a retrieval-augmented multi-agent framework designed for precise and broad-coverage vulnerability detection. MulVul adopts a coarse-to-fine strategy: a \\emph{Router} agent first predicts the top-$k$ coarse categories and then forwards the input to specialized \\emph{Detector} agents, which identify the exact vulnerability types. Both agents are equipped with retrieval tools to actively source evidence from vulnerability knowledge bases to mitigate hallucinations.\n  Crucially, to automate the generation of specialized prompts, we design \\emph{Cross-Model Prompt Evolution}, a prompt optimization mechanism where a generator LLM iteratively refines candidate prompts while a distinct executor LLM validates their effectiveness. This decoupling mitigates the self-correction bias inherent in single-model optimization.\n  Evaluated on 130 CWE types, MulVul achieves 34.79\\% Macro-F1, outperforming the best baseline by 41.5\\%. Ablation studies validate cross-model prompt evolution, which boosts performance by 51.6\\% over manual prompts by effectively handling diverse vulnerability patterns.", "AI": {"tldr": "MulVul\u662f\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u91c7\u7528\u7c97\u5230\u7ec6\u7b56\u7565\u8fdb\u884c\u6f0f\u6d1e\u68c0\u6d4b\uff0c\u901a\u8fc7\u8de8\u6a21\u578b\u63d0\u793a\u6f14\u5316\u81ea\u52a8\u751f\u6210\u4e13\u7528\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u5728\u771f\u5b9e\u4e16\u754c\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u6f0f\u6d1e\u6a21\u5f0f\u7684\u5f02\u8d28\u6027\u524a\u5f31\u4e86\u5355\u4e00\u7edf\u4e00\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u800c\u4e3a\u5927\u91cf\u5f31\u70b9\u7c7b\u522b\u624b\u52a8\u8bbe\u8ba1\u63d0\u793a\u662f\u4e0d\u53ef\u6269\u5c55\u7684\u3002", "method": "\u63d0\u51faMulVul\u6846\u67b6\uff1a1) Router\u667a\u80fd\u4f53\u9884\u6d4btop-k\u7c97\u7c92\u5ea6\u7c7b\u522b\uff1b2) \u4e13\u7528Detector\u667a\u80fd\u4f53\u8bc6\u522b\u786e\u5207\u6f0f\u6d1e\u7c7b\u578b\uff1b3) \u4e24\u8005\u90fd\u914d\u5907\u68c0\u7d22\u5de5\u5177\u4ece\u77e5\u8bc6\u5e93\u83b7\u53d6\u8bc1\u636e\uff1b4) \u8bbe\u8ba1\u8de8\u6a21\u578b\u63d0\u793a\u6f14\u5316\u673a\u5236\uff0c\u751f\u6210\u5668LLM\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\uff0c\u6267\u884c\u5668LLM\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "result": "\u5728130\u4e2aCWE\u7c7b\u578b\u4e0a\u8bc4\u4f30\uff0cMulVul\u8fbe\u523034.79%\u7684Macro-F1\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u534741.5%\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u8de8\u6a21\u578b\u63d0\u793a\u6f14\u5316\u6bd4\u624b\u52a8\u63d0\u793a\u63d0\u534751.6%\u6027\u80fd\u3002", "conclusion": "MulVul\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548c\u81ea\u52a8\u63d0\u793a\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u5f02\u8d28\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2601.18949", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18949", "abs": "https://arxiv.org/abs/2601.18949", "authors": ["Cole Granger", "Dipin Khati", "Daniel Rodriguez-Cardenas", "Denys Poshyvanyk"], "title": "Tricky$^2$: Towards a Benchmark for Evaluating Human and LLM Error Interactions", "comment": null, "summary": "Large language models (LLMs) are increasingly integrated into software development workflows, yet they often introduce subtle logic or data-misuse errors that differ from human bugs. To study how these two error types interact, we construct Tricky$^2$, a hybrid dataset that augments the existing TrickyBugs corpus of human-written defects with errors injected by both GPT-5 and OpenAI-oss-20b across C++, Python, and Java programs. Our approach uses a taxonomy-guided prompting framework to generate machine-originated bugs while preserving original human defects and program structure. The resulting corpus spans human-only, LLM-only, and human+LLM splits, enabling analysis of mixed-origin error behavior, multi-bug repair robustness, and reliability in hybrid human-machine code. This paper outlines the dataset construction pipeline and illustrates its use through small-scale baseline evaluations of classification, localization, and repair tasks.", "AI": {"tldr": "\u6784\u5efaTricky\u00b2\u6df7\u5408\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4eba\u7c7b\u7f16\u5199\u7f3a\u9677\u548cLLM\u6ce8\u5165\u9519\u8bef\uff0c\u7528\u4e8e\u5206\u6790\u4eba\u7c7b\u4e0eLLM\u9519\u8bef\u4ea4\u4e92\u3001\u591a\u9519\u8bef\u4fee\u590d\u9c81\u68d2\u6027\u53ca\u6df7\u5408\u4eba\u673a\u4ee3\u7801\u53ef\u9760\u6027", "motivation": "LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5e38\u5f15\u5165\u4e0e\u4eba\u7c7b\u9519\u8bef\u4e0d\u540c\u7684\u903b\u8f91\u6216\u6570\u636e\u8bef\u7528\u9519\u8bef\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u4e24\u79cd\u9519\u8bef\u7c7b\u578b\u7684\u4ea4\u4e92", "method": "\u57fa\u4e8e\u73b0\u6709TrickyBugs\u4eba\u7c7b\u7f3a\u9677\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u5206\u7c7b\u6307\u5bfc\u63d0\u793a\u6846\u67b6\u6ce8\u5165GPT-5\u548cOpenAI-oss-20b\u751f\u6210\u7684\u9519\u8bef\uff0c\u4fdd\u7559\u539f\u59cb\u4eba\u7c7b\u7f3a\u9677\u548c\u7a0b\u5e8f\u7ed3\u6784\uff0c\u6784\u5efa\u5305\u542b\u4eba\u7c7b\u4e13\u7528\u3001LLM\u4e13\u7528\u548c\u4eba\u7c7b+LLM\u6df7\u5408\u5206\u533a\u7684\u6570\u636e\u96c6", "result": "\u521b\u5efa\u4e86Tricky\u00b2\u6df7\u5408\u6570\u636e\u96c6\uff0c\u652f\u6301\u6df7\u5408\u6765\u6e90\u9519\u8bef\u884c\u4e3a\u5206\u6790\u3001\u591a\u9519\u8bef\u4fee\u590d\u9c81\u68d2\u6027\u8bc4\u4f30\u548c\u6df7\u5408\u4eba\u673a\u4ee3\u7801\u53ef\u9760\u6027\u7814\u7a76", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u4eba\u7c7b\u4e0eLLM\u9519\u8bef\u4ea4\u4e92\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u5c0f\u89c4\u6a21\u57fa\u7ebf\u8bc4\u4f30\u5c55\u793a\u4e86\u5176\u5728\u5206\u7c7b\u3001\u5b9a\u4f4d\u548c\u4fee\u590d\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4ef7\u503c", "topic": "swe benchmark"}}
{"id": "2601.19082", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.19082", "abs": "https://arxiv.org/abs/2601.19082", "authors": ["Trung-Kiet Huynh", "Dao-Sy Duy-Minh", "Thanh-Bang Cao", "Phong-Hao Le", "Hong-Dan Nguyen", "Nguyen Lam Phu Quy", "Minh-Luan Nguyen-Vo", "Hong-Phat Pham", "Pham Phu Hoa", "Thien-Kim Than", "Chi-Nguyen Tran", "Huy Tran", "Gia-Thoai Tran-Le", "Alessio Buscemi", "Le Hong Trang", "The Anh Han"], "title": "More at Stake: How Payoff and Language Shape LLM Agent Strategies in Cooperation Dilemmas", "comment": "14 pages, 10 figures, 4 tables", "summary": "As LLMs increasingly act as autonomous agents in interactive and multi-agent settings, understanding their strategic behavior is critical for safety, coordination, and AI-driven social and economic systems. We investigate how payoff magnitude and linguistic context shape LLM strategies in repeated social dilemmas, using a payoff-scaled Prisoner's Dilemma to isolate sensitivity to incentive strength. Across models and languages, we observe consistent behavioral patterns, including incentive-sensitive conditional strategies and cross-linguistic divergence. To interpret these dynamics, we train supervised classifiers on canonical repeated-game strategies and apply them to LLM decisions, revealing systematic, model- and language-dependent behavioral intentions, with linguistic framing sometimes matching or exceeding architectural effects. Our results provide a unified framework for auditing LLMs as strategic agents and highlight cooperation biases with direct implications for AI governance and multi-agent system design.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u91cd\u590d\u56da\u5f92\u56f0\u5883\u5b9e\u9a8c\uff0c\u63a2\u7a76\u4e86LLM\u5728\u4e0d\u540c\u6536\u76ca\u5e45\u5ea6\u548c\u8bed\u8a00\u4e0a\u4e0b\u6587\u4e2d\u7684\u7b56\u7565\u884c\u4e3a\uff0c\u53d1\u73b0LLM\u8868\u73b0\u51fa\u6fc0\u52b1\u654f\u611f\u7684\u51b3\u7b56\u6a21\u5f0f\uff0c\u4e14\u8bed\u8a00\u6846\u67b6\u5bf9\u884c\u4e3a\u7684\u5f71\u54cd\u6709\u65f6\u8d85\u8fc7\u6a21\u578b\u67b6\u6784\u5dee\u5f02\u3002", "motivation": "\u968f\u7740LLM\u5728\u4ea4\u4e92\u5f0f\u548c\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u4f5c\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u8fd0\u884c\uff0c\u7406\u89e3\u5176\u6218\u7565\u884c\u4e3a\u5bf9\u4e8e\u5b89\u5168\u6027\u3001\u534f\u8c03\u6027\u4ee5\u53caAI\u9a71\u52a8\u7684\u793e\u4f1a\u548c\u7ecf\u6d4e\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u7814\u7a76LLM\u5728\u91cd\u590d\u793e\u4f1a\u56f0\u5883\u4e2d\u7684\u7b56\u7565\u9009\u62e9\u5982\u4f55\u53d7\u5230\u6536\u76ca\u5e45\u5ea6\u548c\u8bed\u8a00\u4e0a\u4e0b\u6587\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6536\u76ca\u7f29\u653e\u7684\u56da\u5f92\u56f0\u5883\u6765\u5206\u79bb\u5bf9\u6fc0\u52b1\u5f3a\u5ea6\u7684\u654f\u611f\u6027\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u548c\u8bed\u8a00\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002\u8bad\u7ec3\u76d1\u7763\u5206\u7c7b\u5668\u8bc6\u522b\u7ecf\u5178\u91cd\u590d\u535a\u5f08\u7b56\u7565\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eLLM\u51b3\u7b56\u5206\u6790\uff0c\u4ee5\u89e3\u91ca\u884c\u4e3a\u52a8\u6001\u3002", "result": "\u89c2\u5bdf\u5230\u8de8\u6a21\u578b\u548c\u8bed\u8a00\u7684\u4e00\u81f4\u884c\u4e3a\u6a21\u5f0f\uff0c\u5305\u62ec\u6fc0\u52b1\u654f\u611f\u7684\u6761\u4ef6\u7b56\u7565\u548c\u8de8\u8bed\u8a00\u5dee\u5f02\u3002\u8bed\u8a00\u6846\u67b6\u6709\u65f6\u5bf9\u884c\u4e3a\u7684\u5f71\u54cd\u4e0e\u67b6\u6784\u6548\u5e94\u76f8\u5f53\u751a\u81f3\u8d85\u8fc7\u3002LLM\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u7684\u3001\u4f9d\u8d56\u4e8e\u6a21\u578b\u548c\u8bed\u8a00\u7684\u884c\u4e3a\u610f\u56fe\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5ba1\u8ba1LLM\u4f5c\u4e3a\u6218\u7565\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u63ed\u793a\u4e86\u5408\u4f5c\u504f\u89c1\u5bf9AI\u6cbb\u7406\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u7684\u76f4\u63a5\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2601.19066", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19066", "abs": "https://arxiv.org/abs/2601.19066", "authors": ["Runxiang Cheng", "Michele Tufano", "Jos\u00e9 Cambronero", "Renyao Wei", "Sherry Shi", "Grant Uy", "Pat Rondon", "Franjo Ivan\u010di\u0107"], "title": "Dynamic Cogeneration of Bug Reproduction Test in Agentic Program Repair", "comment": null, "summary": "Bug Reproduction Tests (BRTs) have been used in many agentic Automated Program Repair (APR) systems, primarily for validating promising fixes and aiding fix generation. In practice, when developers submit a patch, they often implement the BRT alongside the fix. Our experience deploying agentic APR reveals that developers similarly desire a BRT within AI-generated patches to increase their confidence. However, canonical APR systems tend to generate BRTs and fixes separately, or focus on producing only the fix in the final patch. In this paper, we study agentic APR in the context of cogeneration, where the APR agent is instructed to generate both a fix and a BRT in the same patch. We evaluate the effectiveness of different cogeneration strategies on 120 human-reported bugs at Google and characterize different cogeneration strategies by their influence on APR agent behavior. We develop and evaluate patch selectors that account for test change information to select patches with plausible fixes (and plausible BRTs). Finally, we analyze the root causes of failed cogeneration trajectories. Importantly, we show that cogeneration allows the APR agent to generate BRTs for at least as many bugs as a dedicated BRT agent, without compromising the generation rate of plausible fixes, thereby reducing engineering effort in maintaining and coordinating separate generation pipelines for fix and BRT at scale.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5728\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u4e2d\u540c\u65f6\u751f\u6210\u4fee\u590d\u548c\u9519\u8bef\u91cd\u73b0\u6d4b\u8bd5\uff08BRT\uff09\u7684\u534f\u540c\u751f\u6210\u7b56\u7565\uff0c\u8bc4\u4f30\u4e0d\u540c\u7b56\u7565\u5728120\u4e2aGoogle\u771f\u5b9ebug\u4e0a\u7684\u6548\u679c\uff0c\u5f00\u53d1\u8003\u8651\u6d4b\u8bd5\u53d8\u66f4\u4fe1\u606f\u7684\u8865\u4e01\u9009\u62e9\u5668\uff0c\u5e76\u5206\u6790\u5931\u8d25\u539f\u56e0\u3002", "motivation": "\u5b9e\u9645\u5f00\u53d1\u4e2d\uff0c\u5f00\u53d1\u8005\u63d0\u4ea4\u8865\u4e01\u65f6\u901a\u5e38\u4f1a\u540c\u65f6\u5b9e\u73b0BRT\u6d4b\u8bd5\u3002\u90e8\u7f72APR\u7cfb\u7edf\u65f6\u53d1\u73b0\u5f00\u53d1\u8005\u540c\u6837\u5e0c\u671bAI\u751f\u6210\u7684\u8865\u4e01\u5305\u542bBRT\u4ee5\u589e\u52a0\u4fe1\u5fc3\uff0c\u4f46\u4f20\u7edfAPR\u7cfb\u7edf\u5f80\u5f80\u5206\u5f00\u751f\u6210BRT\u548c\u4fee\u590d\uff0c\u6216\u53ea\u5173\u6ce8\u6700\u7ec8\u4fee\u590d\u3002", "method": "\u7814\u7a76APR\u4ee3\u7406\u5728\u534f\u540c\u751f\u6210\u4e0a\u4e0b\u6587\u4e2d\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u4e0d\u540c\u534f\u540c\u751f\u6210\u7b56\u7565\u7684\u6548\u679c\uff0c\u5f00\u53d1\u8003\u8651\u6d4b\u8bd5\u53d8\u66f4\u4fe1\u606f\u7684\u8865\u4e01\u9009\u62e9\u5668\u6765\u7b5b\u9009\u5408\u7406\u7684\u4fee\u590d\u548cBRT\uff0c\u5206\u6790\u5931\u8d25\u8f68\u8ff9\u7684\u6839\u672c\u539f\u56e0\u3002", "result": "\u534f\u540c\u751f\u6210\u4f7fAPR\u4ee3\u7406\u80fd\u591f\u4e3a\u81f3\u5c11\u4e0e\u4e13\u7528BRT\u4ee3\u7406\u76f8\u540c\u6570\u91cf\u7684bug\u751f\u6210BRT\uff0c\u540c\u65f6\u4e0d\u964d\u4f4e\u5408\u7406\u4fee\u590d\u7684\u751f\u6210\u7387\uff0c\u4ece\u800c\u51cf\u5c11\u7ef4\u62a4\u548c\u534f\u8c03\u72ec\u7acb\u4fee\u590d\u4e0eBRT\u751f\u6210\u7ba1\u9053\u7684\u5de5\u7a0b\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u534f\u540c\u751f\u6210\u7b56\u7565\u5728\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u4e2d\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u540c\u65f6\u751f\u6210\u4fee\u590d\u548cBRT\u6d4b\u8bd5\uff0c\u63d0\u9ad8\u5f00\u53d1\u8005\u5bf9AI\u751f\u6210\u8865\u4e01\u7684\u4fe1\u5fc3\uff0c\u5e76\u51cf\u5c11\u5de5\u7a0b\u7ef4\u62a4\u6210\u672c\u3002", "topic": "swe application"}}
{"id": "2601.19072", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19072", "abs": "https://arxiv.org/abs/2601.19072", "authors": ["Kla Tantithamthavorn", "Hong Yi Lin", "Patanamon Thongtanunam", "Wachiraphan Charoenwet", "Minwoo Jeong", "Ming Wu"], "title": "HalluJudge: A Reference-Free Hallucination Detection for Context Misalignment in Code Review Automation", "comment": "Under Review", "summary": "Large Language models (LLMs) have shown strong capabilities in code review automation, such as review comment generation, yet they suffer from hallucinations -- where the generated review comments are ungrounded in the actual code -- poses a significant challenge to the adoption of LLMs in code review workflows. To address this, we explore effective and scalable methods for a hallucination detection in LLM-generated code review comments without the reference. In this work, we design HalluJudge that aims to assess the grounding of generated review comments based on the context alignment. HalluJudge includes four key strategies ranging from direct assessment to structured multi-branch reasoning (e.g., Tree-of-Thoughts). We conduct a comprehensive evaluation of these assessment strategies across Atlassian's enterprise-scale software projects to examine the effectiveness and cost-efficiency of HalluJudge. Furthermore, we analyze the alignment between HalluJudge's judgment and developer preference of the actual LLM-generated code review comments in the real-world production. Our results show that the hallucination assessment in HalluJudge is cost-effective with an F1 score of 0.85 and an average cost of $0.009. On average, 67% of the HalluJudge assessments are aligned with the developer preference of the actual LLM-generated review comments in the online production. Our results suggest that HalluJudge can serve as a practical safeguard to reduce developers' exposure to hallucinated comments, fostering trust in AI-assisted code reviews.", "AI": {"tldr": "HalluJudge\u662f\u4e00\u4e2a\u68c0\u6d4bLLM\u751f\u6210\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u4e2d\u5e7b\u89c9\u95ee\u9898\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5bf9\u9f50\u8bc4\u4f30\uff0c\u5728\u771f\u5b9e\u4f01\u4e1a\u9879\u76ee\u4e2d\u8fbe\u52300.85 F1\u5206\u6570\uff0c\u6210\u672c\u4ec50.009\u7f8e\u5143\uff0c67%\u8bc4\u4f30\u4e0e\u5f00\u53d1\u8005\u504f\u597d\u4e00\u81f4\u3002", "motivation": "LLM\u5728\u4ee3\u7801\u5ba1\u67e5\u81ea\u52a8\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u2014\u2014\u751f\u6210\u7684\u5ba1\u67e5\u8bc4\u8bba\u4e0e\u5b9e\u9645\u4ee3\u7801\u65e0\u5173\uff0c\u8fd9\u963b\u788d\u4e86LLM\u5728\u4ee3\u7801\u5ba1\u67e5\u5de5\u4f5c\u6d41\u4e2d\u7684\u91c7\u7528\u3002\u9700\u8981\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u68c0\u6d4bLLM\u751f\u6210\u7684\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u4e2d\u7684\u5e7b\u89c9\uff0c\u4e14\u65e0\u9700\u53c2\u8003\u6807\u51c6\u7b54\u6848\u3002", "method": "\u8bbe\u8ba1HalluJudge\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5bf9\u9f50\u8bc4\u4f30\u751f\u6210\u5ba1\u67e5\u8bc4\u8bba\u7684grounding\u7a0b\u5ea6\u3002\u5305\u542b\u56db\u79cd\u5173\u952e\u7b56\u7565\uff1a\u4ece\u76f4\u63a5\u8bc4\u4f30\u5230\u7ed3\u6784\u5316\u591a\u5206\u652f\u63a8\u7406\uff08\u5982Tree-of-Thoughts\uff09\u3002\u5728Atlassian\u7684\u4f01\u4e1a\u7ea7\u8f6f\u4ef6\u9879\u76ee\u4e2d\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "HalluJudge\u7684\u5e7b\u89c9\u68c0\u6d4b\u6210\u672c\u6548\u76ca\u9ad8\uff0cF1\u5206\u6570\u8fbe0.85\uff0c\u5e73\u5747\u6210\u672c\u4ec50.009\u7f8e\u5143\u3002\u5e73\u574767%\u7684HalluJudge\u8bc4\u4f30\u4e0e\u5728\u7ebf\u751f\u4ea7\u4e2d\u5b9e\u9645LLM\u751f\u6210\u5ba1\u67e5\u8bc4\u8bba\u7684\u5f00\u53d1\u8005\u504f\u597d\u4e00\u81f4\u3002", "conclusion": "HalluJudge\u53ef\u4f5c\u4e3a\u5b9e\u7528\u5b89\u5168\u63aa\u65bd\uff0c\u51cf\u5c11\u5f00\u53d1\u8005\u63a5\u89e6\u5e7b\u89c9\u8bc4\u8bba\uff0c\u4fc3\u8fdb\u5bf9AI\u8f85\u52a9\u4ee3\u7801\u5ba1\u67e5\u7684\u4fe1\u4efb\u3002", "topic": "swe application"}}
{"id": "2601.18832", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18832", "abs": "https://arxiv.org/abs/2601.18832", "authors": ["Ren Zhuang", "Ben Wang", "Shuifa Sun"], "title": "The Geometric Reasoner: Manifold-Informed Latent Foresight Search for Long-Context Reasoning", "comment": "11 pages, 5 figures", "summary": "Scaling test-time compute enhances long chain-of-thought (CoT) reasoning, yet existing approaches face a fundamental trade-off between computational cost and coverage quality: either incurring high training expense or yielding redundant trajectories. We introduce The Geometric Reasoner (TGR), a training-free framework that performs manifold-informed latent foresight search under strict memory bounds. At each chunk boundary, TGR scores candidate latent anchors via a lightweight look-ahead estimate combined with soft geometric regularizers that encourage smooth trajectories and diverse exploration. Chunk-wise KV cache resets keep memory linear in chunk length. On challenging math and code benchmarks, TGR improves robust trajectory coverage, measured by the area under the Pass@$k$ curve (AUC), by up to 13 points on Qwen3-8B, with negligible overhead of about 1.1--1.3 times.", "AI": {"tldr": "TGR\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u6f5c\u5728\u524d\u77bb\u641c\u7d22\u6765\u63d0\u5347\u957f\u94fe\u601d\u7ef4\u63a8\u7406\uff0c\u5728\u4e25\u683c\u5185\u5b58\u9650\u5236\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u8f68\u8ff9\u8986\u76d6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u4e0e\u8986\u76d6\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u8981\u4e48\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u4ea7\u751f\u5197\u4f59\u8f68\u8ff9\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u957f\u94fe\u63a8\u7406\u7684\u8f68\u8ff9\u8986\u76d6\u3002", "method": "TGR\u91c7\u7528\u6d41\u5f62\u611f\u77e5\u7684\u6f5c\u5728\u524d\u77bb\u641c\u7d22\u6846\u67b6\uff0c\u5728\u5206\u5757\u8fb9\u754c\u5904\u901a\u8fc7\u8f7b\u91cf\u7ea7\u524d\u77bb\u4f30\u8ba1\u548c\u8f6f\u51e0\u4f55\u6b63\u5219\u5316\u8bc4\u5206\u5019\u9009\u6f5c\u5728\u951a\u70b9\uff0c\u9f13\u52b1\u5e73\u6ed1\u8f68\u8ff9\u548c\u591a\u6837\u5316\u63a2\u7d22\uff0c\u5e76\u901a\u8fc7\u5206\u5757KV\u7f13\u5b58\u91cd\u7f6e\u4fdd\u6301\u5185\u5b58\u7ebf\u6027\u589e\u957f\u3002", "result": "\u5728\u6570\u5b66\u548c\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTGR\u5c06Qwen3-8B\u6a21\u578b\u7684Pass@k\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08AUC\uff09\u63d0\u5347\u4e86\u6700\u591a13\u4e2a\u70b9\uff0c\u8ba1\u7b97\u5f00\u9500\u4ec5\u4e3a1.1-1.3\u500d\u3002", "conclusion": "TGR\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4e25\u683c\u5185\u5b58\u9650\u5236\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u957f\u94fe\u63a8\u7406\u7684\u8f68\u8ff9\u8986\u76d6\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "2601.19122", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19122", "abs": "https://arxiv.org/abs/2601.19122", "authors": ["Weiran Guo", "Bing Bo", "Shaoxiang Wu", "Jingsheng Yang"], "title": "Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach", "comment": null, "summary": "Function call capabilities have become crucial for Large Language Models (LLMs), enabling them to interact more effectively with external tools and APIs. Existing methods for improving the function call capabilities of LLMs rely on data obtained either through manual annotation or automated generation by models, and use this data to finetune the LLMs. However, these methods often lack targeted design and are constrained by fixed patterns and data distributions, which limits their effectiveness in enhancing the generalization and robustness of function call LLMs. To address this limitation, we propose a novel adversarial data augmentation method that employs reinforcement learning to systematically identify and target the weaknesses of function call LLMs. Our training framework introduces a query model trained with reinforcement learning (RL) to generate adversarial queries that are specifically designed to challenge function call (FC) models. This approach adopts a zero sum game formulation, where the query model and the FC model engage in iterative alternating training. Overall, our method advances the development of more robust FC models and provides a systematic way to identify and correct weaknesses in the ability of LLMs to interact with external tools.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u6297\u6027\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u67e5\u8be2\u6a21\u578b\u751f\u6210\u9488\u5bf9\u6027\u5bf9\u6297\u67e5\u8be2\u6765\u6311\u6218\u51fd\u6570\u8c03\u7528\u6a21\u578b\uff0c\u63d0\u5347LLM\u51fd\u6570\u8c03\u7528\u80fd\u529b\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u63d0\u5347LLM\u51fd\u6570\u8c03\u7528\u80fd\u529b\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4eba\u5de5\u6807\u6ce8\u6216\u6a21\u578b\u81ea\u52a8\u751f\u6210\u7684\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7f3a\u4e4f\u9488\u5bf9\u6027\u8bbe\u8ba1\uff0c\u53d7\u9650\u4e8e\u56fa\u5b9a\u6a21\u5f0f\u548c\u6570\u636e\u5206\u5e03\uff0c\u9650\u5236\u4e86\u51fd\u6570\u8c03\u7528LLM\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5bf9\u6297\u6027\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u67e5\u8be2\u6a21\u578b\u751f\u6210\u4e13\u95e8\u6311\u6218\u51fd\u6570\u8c03\u7528\u6a21\u578b\u7684\u5bf9\u6297\u6027\u67e5\u8be2\u3002\u91c7\u7528\u96f6\u548c\u535a\u5f08\u6846\u67b6\uff0c\u67e5\u8be2\u6a21\u578b\u548c\u51fd\u6570\u8c03\u7528\u6a21\u578b\u8fdb\u884c\u8fed\u4ee3\u4ea4\u66ff\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u548c\u9488\u5bf9\u51fd\u6570\u8c03\u7528LLM\u7684\u5f31\u70b9\uff0c\u63a8\u52a8\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u51fd\u6570\u8c03\u7528\u6a21\u578b\uff0c\u4e3a\u8bc6\u522b\u548c\u7ea0\u6b63LLM\u4e0e\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\u80fd\u529b\u7684\u5f31\u70b9\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u5bf9\u6297\u6027\u6570\u636e\u589e\u5f3a\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347LLM\u51fd\u6570\u8c03\u7528\u80fd\u529b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.19100", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19100", "abs": "https://arxiv.org/abs/2601.19100", "authors": ["Md Rayhanul Masud", "Azmine Toushik Wasi", "Salman Rahman", "Md Rizwan Parvez"], "title": "Reward Engineering for Reinforcement Learning in Software Tasks", "comment": null, "summary": "Reinforcement learning is increasingly used for code-centric tasks. These tasks include code generation, summarization, understanding, repair, testing, and optimization. This trend is growing faster with large language models and autonomous agents. A key challenge is how to design reward signals that make sense for software. In many RL problems, the reward is a clear number. In software, this is often not possible. The goal is rarely a single numeric objective. Instead, rewards are usually proxies. Common proxies check if the code compiles, passes tests, or satisfies quality metrics. Many reward designs have been proposed for code-related tasks. However, the work is scattered across areas and papers. There is no single survey that brings these approaches together and shows the full landscape of reward design for RL in software. In this survey, we provide the first systematic and comprehensive review of reward engineering for RL in software tasks. We focus on existing methods and techniques. We structure the literature along three complementary dimensions, summarizing the reward-design choices within each. We conclude with challenges and recommendations in the reward design space for SE tasks.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u7efc\u8ff0\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u6574\u7406\u4e86\u73b0\u6709\u6587\u732e\uff0c\u63d0\u51fa\u4e86\u4e09\u7ef4\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u603b\u7ed3\u4e86\u6311\u6218\u4e0e\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u4e3b\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u5f3a\u5316\u5b66\u4e60\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\u3002\u7136\u800c\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5956\u52b1\u8bbe\u8ba1\u9762\u4e34\u72ec\u7279\u6311\u6218\uff1a\u8f6f\u4ef6\u76ee\u6807\u901a\u5e38\u4e0d\u662f\u5355\u4e00\u6570\u503c\uff0c\u800c\u662f\u9700\u8981\u4ee3\u7406\u6307\u6807\uff08\u5982\u7f16\u8bd1\u901a\u8fc7\u3001\u6d4b\u8bd5\u901a\u8fc7\u3001\u8d28\u91cf\u6307\u6807\uff09\u3002\u73b0\u6709\u7814\u7a76\u5206\u6563\u5728\u4e0d\u540c\u9886\u57df\u548c\u8bba\u6587\u4e2d\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7efc\u8ff0\u6765\u6574\u5408\u8fd9\u4e9b\u65b9\u6cd5\u5e76\u5c55\u793a\u5b8c\u6574\u7684\u5956\u52b1\u8bbe\u8ba1\u56fe\u666f\u3002", "method": "\u672c\u6587\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u805a\u7126\u73b0\u6709\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\u548c\u6280\u672f\u3002\u901a\u8fc7\u4e09\u4e2a\u4e92\u8865\u7ef4\u5ea6\u5bf9\u6587\u732e\u8fdb\u884c\u7ed3\u6784\u5316\u5206\u7c7b\uff0c\u603b\u7ed3\u6bcf\u4e2a\u7ef4\u5ea6\u5185\u7684\u5956\u52b1\u8bbe\u8ba1\u9009\u62e9\u3002\u5177\u4f53\u7ef4\u5ea6\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\uff0c\u4f46\u6697\u793a\u4e86\u591a\u7ef4\u5206\u7c7b\u6846\u67b6\u3002", "result": "\u63d0\u4f9b\u4e86\u9996\u4e2a\u9488\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u8bbe\u8ba1\u7684\u7cfb\u7edf\u6027\u548c\u5168\u9762\u6027\u7efc\u8ff0\uff0c\u6574\u7406\u4e86\u5206\u6563\u5728\u4e0d\u540c\u9886\u57df\u7684\u7814\u7a76\u6210\u679c\uff0c\u5efa\u7acb\u4e86\u4e09\u7ef4\u5206\u7c7b\u6846\u67b6\u6765\u7ec4\u7ec7\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u5956\u52b1\u8bbe\u8ba1\u56fe\u666f\u3002", "conclusion": "\u603b\u7ed3\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u8bbe\u8ba1\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u8be5\u9886\u57df\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002\u5f3a\u8c03\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\u548c\u65b9\u5411\u6307\u5f15\u3002", "topic": "agent analysis"}}
{"id": "2601.19151", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.19151", "abs": "https://arxiv.org/abs/2601.19151", "authors": ["Patara Trirat", "Jin Myung Kwak", "Jay Heo", "Heejun Lee", "Sung Ju Hwang"], "title": "TS-Debate: Multimodal Collaborative Debate for Zero-Shot Time Series Reasoning", "comment": "Code will be available at https://github.com/DeepAuto-AI/TS-Debate", "summary": "Recent progress at the intersection of large language models (LLMs) and time series (TS) analysis has revealed both promise and fragility. While LLMs can reason over temporal structure given carefully engineered context, they often struggle with numeric fidelity, modality interference, and principled cross-modal integration. We present TS-Debate, a modality-specialized, collaborative multi-agent debate framework for zero-shot time series reasoning. TS-Debate assigns dedicated expert agents to textual context, visual patterns, and numerical signals, preceded by explicit domain knowledge elicitation, and coordinates their interaction via a structured debate protocol. Reviewer agents evaluate agent claims using a verification-conflict-calibration mechanism, supported by lightweight code execution and numerical lookup for programmatic verification. This architecture preserves modality fidelity, exposes conflicting evidence, and mitigates numeric hallucinations without task-specific fine-tuning. Across 20 tasks spanning three public benchmarks, TS-Debate achieves consistent and significant performance improvements over strong baselines, including standard multimodal debate in which all agents observe all inputs.", "AI": {"tldr": "TS-Debate\u662f\u4e00\u4e2a\u7528\u4e8e\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u7684\u6a21\u6001\u4e13\u4e1a\u5316\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u7528\u4e13\u5bb6\u667a\u80fd\u4f53\u5904\u7406\u6587\u672c\u3001\u89c6\u89c9\u548c\u6570\u503c\u4fe1\u53f7\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u8fa9\u8bba\u534f\u8bae\u534f\u8c03\u4ea4\u4e92\uff0c\u572820\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u867d\u7136\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u6570\u503c\u4fdd\u771f\u5ea6\u4e0d\u8db3\u3001\u6a21\u6001\u5e72\u6270\u548c\u8de8\u6a21\u6001\u96c6\u6210\u4e0d\u7cfb\u7edf\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u6301\u6a21\u6001\u4fdd\u771f\u5ea6\u3001\u51cf\u5c11\u6570\u503c\u5e7b\u89c9\u7684\u96f6\u6837\u672c\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTS-Debate\u6846\u67b6\uff1a1) \u5206\u914d\u4e13\u7528\u4e13\u5bb6\u667a\u80fd\u4f53\u5904\u7406\u6587\u672c\u4e0a\u4e0b\u6587\u3001\u89c6\u89c9\u6a21\u5f0f\u548c\u6570\u503c\u4fe1\u53f7\uff1b2) \u901a\u8fc7\u663e\u5f0f\u9886\u57df\u77e5\u8bc6\u63d0\u53d6\uff1b3) \u4f7f\u7528\u7ed3\u6784\u5316\u8fa9\u8bba\u534f\u8bae\u534f\u8c03\u667a\u80fd\u4f53\u4ea4\u4e92\uff1b4) \u8bc4\u5ba1\u667a\u80fd\u4f53\u901a\u8fc7\u9a8c\u8bc1-\u51b2\u7a81-\u6821\u51c6\u673a\u5236\u8bc4\u4f30\u4e3b\u5f20\uff0c\u652f\u6301\u8f7b\u91cf\u7ea7\u4ee3\u7801\u6267\u884c\u548c\u6570\u503c\u67e5\u627e\u8fdb\u884c\u7a0b\u5e8f\u5316\u9a8c\u8bc1\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u768420\u4e2a\u4efb\u52a1\u4e0a\uff0cTS-Debate\u5b9e\u73b0\u4e86\u6301\u7eed\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u4e8e\u5305\u62ec\u6807\u51c6\u591a\u6a21\u6001\u8fa9\u8bba\uff08\u6240\u6709\u667a\u80fd\u4f53\u89c2\u5bdf\u6240\u6709\u8f93\u5165\uff09\u5728\u5185\u7684\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TS-Debate\u6846\u67b6\u901a\u8fc7\u6a21\u6001\u4e13\u4e1a\u5316\u3001\u7ed3\u6784\u5316\u8fa9\u8bba\u548c\u7a0b\u5e8f\u5316\u9a8c\u8bc1\uff0c\u80fd\u591f\u4fdd\u6301\u6a21\u6001\u4fdd\u771f\u5ea6\u3001\u66b4\u9732\u51b2\u7a81\u8bc1\u636e\u5e76\u51cf\u5c11\u6570\u503c\u5e7b\u89c9\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u66f4\u597d\u7684\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.19106", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19106", "abs": "https://arxiv.org/abs/2601.19106", "authors": ["Dipin Khati", "Daniel Rodriguez-Cardenas", "Paul Pantzer", "Denys Poshyvanyk"], "title": "Detecting and Correcting Hallucinations in LLM-Generated Code via Deterministic AST Analysis", "comment": "Accepted to FORGE 2026", "summary": "Large Language Models (LLMs) for code generation boost productivity but frequently introduce Knowledge Conflicting Hallucinations (KCHs), subtle, semantic errors, such as non-existent API parameters, that evade linters and cause runtime failures. Existing mitigations like constrained decoding or non-deterministic LLM-in-the-loop repair are often unreliable for these errors. This paper investigates whether a deterministic, static-analysis framework can reliably detect \\textit{and} auto-correct KCHs. We propose a post-processing framework that parses generated code into an Abstract Syntax Tree (AST) and validates it against a dynamically-generated Knowledge Base (KB) built via library introspection. This non-executing approach uses deterministic rules to find and fix both API and identifier-level conflicts. On a manually-curated dataset of 200 Python snippets, our framework detected KCHs with 100\\% precision and 87.6\\% recall (0.934 F1-score), and successfully auto-corrected 77.0\\% of all identified hallucinations. Our findings demonstrate that this deterministic post-processing approach is a viable and reliable alternative to probabilistic repair, offering a clear path toward trustworthy code generation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u786e\u5b9a\u6027\u9759\u6001\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7AST\u89e3\u6790\u548c\u52a8\u6001\u77e5\u8bc6\u5e93\u9a8c\u8bc1\uff0c\u53ef\u9760\u68c0\u6d4b\u5e76\u81ea\u52a8\u4fee\u6b63\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u77e5\u8bc6\u51b2\u7a81\u5e7b\u89c9\u9519\u8bef\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u5e38\u5305\u542b\u77e5\u8bc6\u51b2\u7a81\u5e7b\u89c9\uff08KCHs\uff09\u2014\u2014\u5982\u4e0d\u5b58\u5728\u7684API\u53c2\u6570\u7b49\u8bed\u4e49\u9519\u8bef\uff0c\u8fd9\u4e9b\u9519\u8bef\u96be\u4ee5\u88ab\u4f20\u7edflinter\u68c0\u6d4b\u4e14\u5bfc\u81f4\u8fd0\u884c\u65f6\u5931\u8d25\u3002\u73b0\u6709\u57fa\u4e8e\u7ea6\u675f\u89e3\u7801\u6216\u975e\u786e\u5b9a\u6027LLM\u4fee\u590d\u7684\u65b9\u6cd5\u5bf9\u8fd9\u4e9b\u9519\u8bef\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u540e\u5904\u7406\u6846\u67b6\uff1a\u5c06\u751f\u6210\u4ee3\u7801\u89e3\u6790\u4e3a\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\uff0c\u901a\u8fc7\u5e93\u5185\u7701\u52a8\u6001\u6784\u5efa\u77e5\u8bc6\u5e93\uff08KB\uff09\uff0c\u4f7f\u7528\u786e\u5b9a\u6027\u89c4\u5219\u9a8c\u8bc1AST\u4e0eKB\u7684\u4e00\u81f4\u6027\uff0c\u68c0\u6d4b\u5e76\u81ea\u52a8\u4fee\u6b63API\u548c\u6807\u8bc6\u7b26\u7ea7\u522b\u7684\u51b2\u7a81\u3002", "result": "\u5728200\u4e2a\u624b\u52a8\u6574\u7406\u7684Python\u4ee3\u7801\u7247\u6bb5\u6570\u636e\u96c6\u4e0a\uff0c\u6846\u67b6\u68c0\u6d4bKCHs\u8fbe\u5230100%\u7cbe\u786e\u7387\u548c87.6%\u53ec\u56de\u7387\uff08F1\u5206\u65700.934\uff09\uff0c\u6210\u529f\u81ea\u52a8\u4fee\u6b63\u4e8677.0%\u7684\u5df2\u8bc6\u522b\u5e7b\u89c9\u3002", "conclusion": "\u786e\u5b9a\u6027\u540e\u5904\u7406\u65b9\u6cd5\u4e3a\u6982\u7387\u6027\u4fee\u590d\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u5b9e\u73b0\u53ef\u4fe1\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u660e\u786e\u8def\u5f84\u3002", "topic": "code agent"}}
{"id": "2601.18858", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18858", "abs": "https://arxiv.org/abs/2601.18858", "authors": ["Zhiyu An", "Wan Du"], "title": "Representational Homomorphism Predicts and Improves Compositional Generalization In Transformer Language Model", "comment": null, "summary": "Compositional generalization-the ability to interpret novel combinations of familiar components-remains a persistent challenge for neural networks. Behavioral evaluations reveal when models fail but offer limited insight into why failures arise at the representational level. We introduce Homomorphism Error (HE), a structural metric that quantifies deviations from approximate homomorphisms between the expression algebra and a model's hidden-state space. We instantiate HE for two compositional operators in SCAN-style tasks: modifier HE for unary composition and sequence HE for binary composition, measured by learning representation-level operators that predict composed representations from their parts. Across controlled experiments with small decoder-only Transformers, HE predicts out-of-distribution (OOD) compositional generalization under noise injection, achieving R^2 = 0.73 correlation between modifier HE and OOD accuracy. Ablations show that model depth has minimal effect on either HE or OOD accuracy, training data coverage exhibits threshold effects (insufficient coverage sharply increases HE and degrades OOD performance), and randomly inserted noise tokens systematically increase HE. Finally, we test if HE-regularized training improves OOD accuracy. Experiment shows that explicitly enforcing low modifier HE during training significantly reduces modifier HE (p = 1.1x10-4) and sequence HE (p = 0.001) and yields a statistically significant improvement in OOD accuracy (p = 0.023). Together, these results indicate the potential of HE to be both a diagnostic and an actionable training signal for improving compositional generalization. Code to reproduce our experiments is open-sourced.", "AI": {"tldr": "\u63d0\u51fa\u540c\u6001\u8bef\u5dee\uff08HE\uff09\u4f5c\u4e3a\u8861\u91cf\u795e\u7ecf\u7f51\u7edc\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u7684\u7ed3\u6784\u6307\u6807\uff0c\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u9a8c\u8bc1HE\u80fd\u9884\u6d4bOOD\u7ec4\u5408\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u8bc1\u660eHE\u6b63\u5219\u5316\u8bad\u7ec3\u80fd\u663e\u8457\u63d0\u5347OOD\u51c6\u786e\u7387\u3002", "motivation": "\u7ec4\u5408\u6cdb\u5316\uff08\u7406\u89e3\u719f\u6089\u7ec4\u4ef6\u7684\u65b0\u7ec4\u5408\uff09\u662f\u795e\u7ecf\u7f51\u7edc\u7684\u6301\u7eed\u6311\u6218\u3002\u884c\u4e3a\u8bc4\u4f30\u53ea\u80fd\u63ed\u793a\u6a21\u578b\u4f55\u65f6\u5931\u8d25\uff0c\u4f46\u65e0\u6cd5\u89e3\u91ca\u5931\u8d25\u5728\u8868\u793a\u5c42\u9762\u7684\u539f\u56e0\u3002\u9700\u8981\u4e00\u79cd\u7ed3\u6784\u6307\u6807\u6765\u91cf\u5316\u6a21\u578b\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u4e0e\u8868\u8fbe\u5f0f\u4ee3\u6570\u4e4b\u95f4\u7684\u540c\u6001\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u540c\u6001\u8bef\u5dee\uff08HE\uff09\u6307\u6807\uff0c\u91cf\u5316\u8868\u8fbe\u5f0f\u4ee3\u6570\u4e0e\u6a21\u578b\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u4e4b\u95f4\u7684\u8fd1\u4f3c\u540c\u6001\u504f\u5dee\u3002\u9488\u5bf9SCAN\u98ce\u683c\u4efb\u52a1\u5b9e\u4f8b\u5316\u4e24\u79cdHE\uff1a\u4e00\u5143\u7ec4\u5408\u7684\u4fee\u9970\u7b26HE\u548c\u4e8c\u5143\u7ec4\u5408\u7684\u5e8f\u5217HE\uff0c\u901a\u8fc7\u5b66\u4e60\u8868\u793a\u7ea7\u64cd\u4f5c\u7b26\u6765\u9884\u6d4b\u7ec4\u5408\u8868\u793a\u3002\u5728\u5c0f\u578b\u4ec5\u89e3\u7801\u5668Transformer\u4e0a\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u6d4b\u8bd5HE\u5bf9\u566a\u58f0\u6ce8\u5165\u4e0bOOD\u7ec4\u5408\u6cdb\u5316\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u8fdb\u884c\u6d88\u878d\u7814\u7a76\u548cHE\u6b63\u5219\u5316\u8bad\u7ec3\u5b9e\u9a8c\u3002", "result": "HE\u80fd\u6709\u6548\u9884\u6d4bOOD\u7ec4\u5408\u6cdb\u5316\uff0c\u4fee\u9970\u7b26HE\u4e0eOOD\u51c6\u786e\u7387\u76f8\u5173\u6027R^2=0.73\u3002\u6a21\u578b\u6df1\u5ea6\u5bf9HE\u548cOOD\u51c6\u786e\u7387\u5f71\u54cd\u6700\u5c0f\uff1b\u8bad\u7ec3\u6570\u636e\u8986\u76d6\u5b58\u5728\u9608\u503c\u6548\u5e94\uff08\u4e0d\u8db3\u7684\u8986\u76d6\u4f1a\u6025\u5267\u589e\u52a0HE\u5e76\u964d\u4f4eOOD\u6027\u80fd\uff09\uff1b\u968f\u673a\u63d2\u5165\u7684\u566a\u58f0token\u4f1a\u7cfb\u7edf\u6027\u589e\u52a0HE\u3002HE\u6b63\u5219\u5316\u8bad\u7ec3\u663e\u8457\u964d\u4f4e\u4fee\u9970\u7b26HE\uff08p=1.1\u00d710\u207b\u2074\uff09\u548c\u5e8f\u5217HE\uff08p=0.001\uff09\uff0c\u5e76\u663e\u8457\u63d0\u5347OOD\u51c6\u786e\u7387\uff08p=0.023\uff09\u3002", "conclusion": "\u540c\u6001\u8bef\u5dee\uff08HE\uff09\u5177\u6709\u4f5c\u4e3a\u8bca\u65ad\u6307\u6807\u548c\u53ef\u64cd\u4f5c\u8bad\u7ec3\u4fe1\u53f7\u7684\u6f5c\u529b\uff0c\u80fd\u6539\u5584\u795e\u7ecf\u7f51\u7edc\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2601.19155", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.19155", "abs": "https://arxiv.org/abs/2601.19155", "authors": ["Qiujun Li", "Zijin Xiao", "Xulin Wang", "Zhidan Ma", "Cheng Yang", "Haifeng Li"], "title": "LocationAgent: A Hierarchical Agent for Image Geolocation via Decoupling Strategy and Evidence from Parametric Knowledge", "comment": "9 pages, 5 figures, 3 tables", "summary": "Image geolocation aims to infer capture locations based on visual content. Fundamentally, this constitutes a reasoning process composed of \\textit{hypothesis-verification cycles}, requiring models to possess both geospatial reasoning capabilities and the ability to verify evidence against geographic facts. Existing methods typically internalize location knowledge and reasoning patterns into static memory via supervised training or trajectory-based reinforcement fine-tuning. Consequently, these methods are prone to factual hallucinations and generalization bottlenecks in open-world settings or scenarios requiring dynamic knowledge. To address these challenges, we propose a Hierarchical Localization Agent, called LocationAgent. Our core philosophy is to retain hierarchical reasoning logic within the model while offloading the verification of geographic evidence to external tools. To implement hierarchical reasoning, we design the RER architecture (Reasoner-Executor-Recorder), which employs role separation and context compression to prevent the drifting problem in multi-step reasoning. For evidence verification, we construct a suite of clue exploration tools that provide diverse evidence to support location reasoning. Furthermore, to address data leakage and the scarcity of Chinese data in existing datasets, we introduce CCL-Bench (China City Location Bench), an image geolocation benchmark encompassing various scene granularities and difficulty levels. Extensive experiments demonstrate that LocationAgent significantly outperforms existing methods by at least 30\\% in zero-shot settings.", "AI": {"tldr": "\u63d0\u51faLocationAgent\uff0c\u4e00\u4e2a\u5206\u5c42\u5b9a\u4f4d\u4ee3\u7406\uff0c\u901a\u8fc7RER\u67b6\u6784\uff08\u63a8\u7406\u5668-\u6267\u884c\u5668-\u8bb0\u5f55\u5668\uff09\u5b9e\u73b0\u5206\u5c42\u63a8\u7406\uff0c\u5c06\u5730\u7406\u8bc1\u636e\u9a8c\u8bc1\u5378\u8f7d\u5230\u5916\u90e8\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u6027\u80fd", "motivation": "\u73b0\u6709\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u76d1\u7763\u8bad\u7ec3\u6216\u57fa\u4e8e\u8f68\u8ff9\u7684\u5f3a\u5316\u5fae\u8c03\u5c06\u4f4d\u7f6e\u77e5\u8bc6\u548c\u63a8\u7406\u6a21\u5f0f\u5185\u5316\u4e3a\u9759\u6001\u8bb0\u5fc6\uff0c\u5bb9\u6613\u5728\u5f00\u653e\u4e16\u754c\u6216\u9700\u8981\u52a8\u6001\u77e5\u8bc6\u7684\u573a\u666f\u4e2d\u51fa\u73b0\u4e8b\u5b9e\u5e7b\u89c9\u548c\u6cdb\u5316\u74f6\u9888", "method": "\u63d0\u51faLocationAgent\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u5206\u5c42\u63a8\u7406\u903b\u8f91\u4fdd\u7559\u5728\u6a21\u578b\u4e2d\uff0c\u540c\u65f6\u5c06\u5730\u7406\u8bc1\u636e\u9a8c\u8bc1\u5378\u8f7d\u5230\u5916\u90e8\u5de5\u5177\u3002\u91c7\u7528RER\u67b6\u6784\u5b9e\u73b0\u5206\u5c42\u63a8\u7406\uff0c\u901a\u8fc7\u89d2\u8272\u5206\u79bb\u548c\u4e0a\u4e0b\u6587\u538b\u7f29\u9632\u6b62\u591a\u6b65\u63a8\u7406\u4e2d\u7684\u6f02\u79fb\u95ee\u9898\u3002\u6784\u5efa\u7ebf\u7d22\u63a2\u7d22\u5de5\u5177\u5957\u4ef6\u63d0\u4f9b\u591a\u6837\u5316\u8bc1\u636e\u652f\u6301\u4f4d\u7f6e\u63a8\u7406\u3002\u8fd8\u5f15\u5165CCL-Bench\uff08\u4e2d\u56fd\u57ce\u5e02\u5b9a\u4f4d\u57fa\u51c6\uff09\u6570\u636e\u96c6", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLocationAgent\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u81f3\u5c1130%", "conclusion": "\u901a\u8fc7\u5c06\u63a8\u7406\u903b\u8f91\u4e0e\u8bc1\u636e\u9a8c\u8bc1\u5206\u79bb\uff0c\u5e76\u5229\u7528\u5916\u90e8\u5de5\u5177\u8fdb\u884c\u5730\u7406\u4e8b\u5b9e\u9a8c\u8bc1\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5e7b\u89c9\u95ee\u9898\u548c\u6cdb\u5316\u74f6\u9888\uff0c\u5728\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "topic": "agent analysis"}}
{"id": "2601.19170", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19170", "abs": "https://arxiv.org/abs/2601.19170", "authors": ["Wangyang Ying", "Yanchi Liu", "Xujiang Zhao", "Wei Cheng", "Zhengzhang Chen", "Wenchao Yu", "Yanjie Fu", "Haifeng Chen"], "title": "Multi-Agent Procedural Graph Extraction with Structural and Logical Refinement", "comment": null, "summary": "Automatically extracting workflows as procedural graphs from natural language is promising yet underexplored, demanding both structural validity and logical alignment. While recent large language models (LLMs) show potential for procedural graph extraction, they often produce ill-formed structures or misinterpret logical flows. We present \\model{}, a multi-agent framework that formulates procedural graph extraction as a multi-round reasoning process with dedicated structural and logical refinement. The framework iterates through three stages: (1) a graph extraction phase with the graph builder agent, (2) a structural feedback phase in which a simulation agent diagnoses and explains structural defects, and (3) a logical feedback phase in which a semantic agent aligns semantics between flow logic and linguistic cues in the source text. Important feedback is prioritized and expressed in natural language, which is injected into subsequent prompts, enabling interpretable and controllable refinement. This modular design allows agents to target distinct error types without supervision or parameter updates. Experiments demonstrate that \\model{} achieves substantial improvements in both structural correctness and logical consistency over strong baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u7a0b\u5e8f\u56fe\u63d0\u53d6\u5efa\u6a21\u4e3a\u591a\u8f6e\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u548c\u903b\u8f91\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u7ed3\u6784\u6b63\u786e\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "motivation": "\u4ece\u81ea\u7136\u8bed\u8a00\u81ea\u52a8\u63d0\u53d6\u5de5\u4f5c\u6d41\u4f5c\u4e3a\u7a0b\u5e8f\u56fe\u5177\u6709\u6f5c\u529b\u4f46\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u540c\u65f6\u4fdd\u8bc1\u7ed3\u6784\u6709\u6548\u6027\u548c\u903b\u8f91\u5bf9\u9f50\u3002\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u7ed3\u6784\u4e0d\u826f\u6216\u903b\u8f91\u6d41\u8bef\u89e3\u7684\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\u8fed\u4ee3\uff1a1) \u56fe\u6784\u5efa\u667a\u80fd\u4f53\u63d0\u53d6\u7a0b\u5e8f\u56fe\uff1b2) \u6a21\u62df\u667a\u80fd\u4f53\u8bca\u65ad\u7ed3\u6784\u7f3a\u9677\u5e76\u63d0\u4f9b\u53cd\u9988\uff1b3) \u8bed\u4e49\u667a\u80fd\u4f53\u5bf9\u9f50\u6d41\u7a0b\u903b\u8f91\u4e0e\u6e90\u6587\u672c\u8bed\u4e49\u3002\u91cd\u8981\u53cd\u9988\u4ee5\u81ea\u7136\u8bed\u8a00\u5f62\u5f0f\u6ce8\u5165\u540e\u7eed\u63d0\u793a\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\u7684\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7ed3\u6784\u6b63\u786e\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u591a\u8f6e\u63a8\u7406\uff0c\u80fd\u591f\u65e0\u76d1\u7763\u5730\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u9519\u8bef\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u7a0b\u5e8f\u56fe\u63d0\u53d6\u7684\u663e\u8457\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2601.19260", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19260", "abs": "https://arxiv.org/abs/2601.19260", "authors": ["Himon Thakur", "Armin Moin"], "title": "\"ENERGY STAR\" LLM-Enabled Software Engineering Tools", "comment": "CAIN 2026 - 5th International Conference on AI Engineering - Software Engineering for AI", "summary": "The discussion around AI-Engineering, that is, Software Engineering (SE) for AI-enabled Systems, cannot ignore a crucial class of software systems that are increasingly becoming AI-enhanced: Those used to enable or support the SE process, such as Computer-Aided SE (CASE) tools and Integrated Development Environments (IDEs). In this paper, we study the energy efficiency of these systems. As AI becomes seamlessly available in these tools and, in many cases, is active by default, we are entering a new era with significant implications for energy consumption patterns throughout the Software Development Lifecycle (SDLC). We focus on advanced Machine Learning (ML) capabilities provided by Large Language Models (LLMs). Our proposed approach combines Retrieval-Augmented Generation (RAG) with Prompt Engineering Techniques (PETs) to enhance both the quality and energy efficiency of LLM-based code generation. We present a comprehensive framework that measures real-time energy consumption and inference time across diverse model architectures ranging from 125M to 7B parameters, including GPT-2, CodeLlama, Qwen 2.5, and DeepSeek Coder. These LLMs, chosen for practical reasons, are sufficient to validate the core ideas and provide a proof of concept for more in-depth future analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86AI\u589e\u5f3a\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u5177\uff08\u5982IDE\uff09\u7684\u80fd\u6e90\u6548\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408RAG\u548c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u5347LLM\u4ee3\u7801\u751f\u6210\u7684\u80fd\u6548\uff0c\u5e76\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u968f\u7740AI\u65e0\u7f1d\u96c6\u6210\u5230\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u5177\u4e2d\u5e76\u9ed8\u8ba4\u542f\u7528\uff0c\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u7684\u80fd\u8017\u6a21\u5f0f\u9762\u4e34\u91cd\u5927\u5f71\u54cd\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u4e9bAI\u589e\u5f3a\u7cfb\u7edf\u7684\u80fd\u6e90\u6548\u7387\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff08PETs\uff09\u7684\u65b9\u6cd5\uff0c\u5efa\u7acb\u7efc\u5408\u6846\u67b6\u6d4b\u91cf\u5b9e\u65f6\u80fd\u8017\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u5728125M\u52307B\u53c2\u6570\u7684\u591a\u79cdLLM\u67b6\u6784\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728GPT-2\u3001CodeLlama\u3001Qwen 2.5\u3001DeepSeek Coder\u7b49\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u6838\u5fc3\u601d\u60f3\uff0c\u4e3a\u66f4\u6df1\u5165\u5206\u6790\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\u3002", "conclusion": "AI\u589e\u5f3a\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u5177\u80fd\u8017\u95ee\u9898\u65e5\u76ca\u91cd\u8981\uff0c\u63d0\u51fa\u7684RAG+PETs\u65b9\u6cd5\u80fd\u540c\u65f6\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u548c\u80fd\u6e90\u6548\u7387\uff0c\u4e3a\u672a\u6765\u66f4\u5168\u9762\u7684\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "swe application"}}
{"id": "2601.19199", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19199", "abs": "https://arxiv.org/abs/2601.19199", "authors": ["Libo Sun", "Jiwen Zhang", "Siyuan Wang", "Zhongyu Wei"], "title": "MAGNET: Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution", "comment": null, "summary": "Mobile GUI agents powered by large foundation models enable autonomous task execution, but frequent updates altering UI appearance and reorganizing workflows cause agents trained on historical data to fail. Despite surface changes, functional semantics and task intents remain fundamentally stable. Building on this insight, we introduce MAGNET, a memory-driven adaptive agent framework with dual-level memory: stationary memory linking diverse visual features to stable functional semantics for robust action grounding and procedural memory capturing stable task intents across varying workflows. We propose a dynamic memory evolution mechanism that continuously refines both memories by prioritizing frequently accessed knowledge. Online benchmark AndroidWorld evaluations show substantial improvements over baselines, while offline benchmarks confirm consistent gains under distribution shifts. These results validate that leveraging stable structures across interface changes improves agent performance and generalization in evolving software environments.", "AI": {"tldr": "MAGNET\u662f\u4e00\u4e2a\u8bb0\u5fc6\u9a71\u52a8\u7684\u81ea\u9002\u5e94GUI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u7ea7\u8bb0\u5fc6\u7cfb\u7edf\uff08\u9759\u6001\u8bb0\u5fc6\u548c\u7a0b\u5e8f\u8bb0\u5fc6\uff09\u6765\u5e94\u5bf9\u79fb\u52a8\u754c\u9762\u9891\u7e41\u66f4\u65b0\u5e26\u6765\u7684\u6311\u6218\uff0c\u5229\u7528\u7a33\u5b9a\u7684\u529f\u80fd\u8bed\u4e49\u548c\u4efb\u52a1\u610f\u56fe\u5b9e\u73b0\u9c81\u68d2\u7684\u52a8\u4f5c\u5b9a\u4f4d\u548c\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u79fb\u52a8GUI\u4ee3\u7406\u867d\u7136\u80fd\u81ea\u4e3b\u6267\u884c\u4efb\u52a1\uff0c\u4f46\u754c\u9762\u9891\u7e41\u66f4\u65b0\u5bfc\u81f4\u57fa\u4e8e\u5386\u53f2\u6570\u636e\u8bad\u7ec3\u7684\u4ee3\u7406\u5931\u6548\u3002\u5c3d\u7ba1\u754c\u9762\u5916\u89c2\u53d8\u5316\uff0c\u4f46\u529f\u80fd\u8bed\u4e49\u548c\u4efb\u52a1\u610f\u56fe\u4fdd\u6301\u7a33\u5b9a\uff0c\u8fd9\u4e3a\u6784\u5efa\u9002\u5e94\u6027\u5f3a\u7684\u4ee3\u7406\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51faMAGNET\u6846\u67b6\uff0c\u5305\u542b\u53cc\u7ea7\u8bb0\u5fc6\u7cfb\u7edf\uff1a1) \u9759\u6001\u8bb0\u5fc6\u5c06\u591a\u6837\u89c6\u89c9\u7279\u5f81\u94fe\u63a5\u5230\u7a33\u5b9a\u529f\u80fd\u8bed\u4e49\uff0c\u5b9e\u73b0\u9c81\u68d2\u52a8\u4f5c\u5b9a\u4f4d\uff1b2) \u7a0b\u5e8f\u8bb0\u5fc6\u6355\u6349\u4e0d\u540c\u5de5\u4f5c\u6d41\u4e2d\u7684\u7a33\u5b9a\u4efb\u52a1\u610f\u56fe\u3002\u91c7\u7528\u52a8\u6001\u8bb0\u5fc6\u8fdb\u5316\u673a\u5236\uff0c\u901a\u8fc7\u4f18\u5148\u8bbf\u95ee\u9891\u7e41\u4f7f\u7528\u7684\u77e5\u8bc6\u6301\u7eed\u4f18\u5316\u4e24\u79cd\u8bb0\u5fc6\u3002", "result": "\u5728AndroidWorld\u5728\u7ebf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u79bb\u7ebf\u57fa\u51c6\u6d4b\u8bd5\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u4e5f\u663e\u793a\u51fa\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002\u9a8c\u8bc1\u4e86\u5229\u7528\u754c\u9762\u53d8\u5316\u4e2d\u7684\u7a33\u5b9a\u7ed3\u6784\u80fd\u63d0\u9ad8\u4ee3\u7406\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u79fb\u52a8\u754c\u9762\u53d8\u5316\u4e2d\u7a33\u5b9a\u7684\u529f\u80fd\u8bed\u4e49\u548c\u4efb\u52a1\u610f\u56fe\uff0cMAGNET\u6846\u67b6\u80fd\u6709\u6548\u5e94\u5bf9GUI\u9891\u7e41\u66f4\u65b0\u5e26\u6765\u7684\u6311\u6218\uff0c\u5728\u6f14\u5316\u8f6f\u4ef6\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.18930", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18930", "abs": "https://arxiv.org/abs/2601.18930", "authors": ["Seiji Shaw", "Travis Manderson", "Chad Kessens", "Nicholas Roy"], "title": "Toward Learning POMDPs Beyond Full-Rank Actions and State Observability", "comment": null, "summary": "We are interested in enabling autonomous agents to learn and reason about systems with hidden states, such as furniture with hidden locking mechanisms. We cast this problem as learning the parameters of a discrete Partially Observable Markov Decision Process (POMDP). The agent begins with knowledge of the POMDP's actions and observation spaces, but not its state space, transitions, or observation models. These properties must be constructed from action-observation sequences. Spectral approaches to learning models of partially observable domains, such as learning Predictive State Representations (PSRs), are known to directly estimate the number of hidden states. These methods cannot, however, yield direct estimates of transition and observation likelihoods, which are important for many downstream reasoning tasks. Other approaches leverage tensor decompositions to estimate transition and observation likelihoods but often assume full state observability and full-rank transition matrices for all actions. To relax these assumptions, we study how PSRs learn transition and observation matrices up to a similarity transform, which may be estimated via tensor methods. Our method learns observation matrices and transition matrices up to a partition of states, where the states in a single partition have the same observation distributions corresponding to actions whose transition matrices are full-rank. Our experiments suggest that these partition-level transition models learned by our method, with a sufficient amount of data, meets the performance of PSRs as models to be used by standard sampling-based POMDP solvers. Furthermore, the explicit observation and transition likelihoods can be leveraged to specify planner behavior after the model has been learned.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u52a8\u4f5c-\u89c2\u5bdf\u5e8f\u5217\u5b66\u4e60\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(POMDP)\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8c31\u65b9\u6cd5\u548c\u5f20\u91cf\u5206\u89e3\uff0c\u80fd\u591f\u4f30\u8ba1\u89c2\u6d4b\u548c\u8f6c\u79fb\u6982\u7387\u77e9\u9635\uff0c\u652f\u6301\u4e0b\u6e38\u63a8\u7406\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u8ba9\u81ea\u4e3b\u667a\u80fd\u4f53\u5b66\u4e60\u548c\u63a8\u7406\u5177\u6709\u9690\u85cf\u72b6\u6001\u7684\u7cfb\u7edf\uff08\u5982\u5e26\u6709\u9690\u85cf\u9501\u5b9a\u673a\u5236\u7684\u5bb6\u5177\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u76f4\u63a5\u4f30\u8ba1\u8f6c\u79fb\u548c\u89c2\u6d4b\u6982\u7387\uff08\u5982PSR\uff09\uff0c\u8981\u4e48\u5047\u8bbe\u5b8c\u5168\u72b6\u6001\u53ef\u89c2\u6d4b\u6027\u548c\u6ee1\u79e9\u8f6c\u79fb\u77e9\u9635\u3002\u9700\u8981\u4e00\u79cd\u80fd\u653e\u677e\u8fd9\u4e9b\u5047\u8bbe\u5e76\u4f30\u8ba1\u663e\u5f0f\u6982\u7387\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u5b66\u4e60\u79bb\u6563POMDP\u53c2\u6570\u3002\u7ed3\u5408\u9884\u6d4b\u72b6\u6001\u8868\u793a(PSR)\u548c\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\uff0c\u5b66\u4e60\u89c2\u6d4b\u77e9\u9635\u548c\u8f6c\u79fb\u77e9\u9635\uff08\u8fbe\u5230\u72b6\u6001\u5212\u5206\u7ea7\u522b\uff09\u3002\u7279\u522b\u5904\u7406\u90a3\u4e9b\u8f6c\u79fb\u77e9\u9635\u6ee1\u79e9\u7684\u52a8\u4f5c\u5bf9\u5e94\u7684\u72b6\u6001\u5212\u5206\uff0c\u8fd9\u4e9b\u5212\u5206\u5185\u7684\u72b6\u6001\u5177\u6709\u76f8\u540c\u7684\u89c2\u6d4b\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b66\u4e60\u5230\u7684\u5212\u5206\u7ea7\u522b\u8f6c\u79fb\u6a21\u578b\u5728\u8db3\u591f\u6570\u636e\u91cf\u4e0b\uff0c\u4e0ePSR\u4f5c\u4e3a\u6807\u51c6\u57fa\u4e8e\u91c7\u6837\u7684POMDP\u6c42\u89e3\u5668\u6a21\u578b\u7684\u6027\u80fd\u76f8\u5f53\u3002\u663e\u5f0f\u7684\u89c2\u6d4b\u548c\u8f6c\u79fb\u6982\u7387\u53ef\u4ee5\u5728\u6a21\u578b\u5b66\u4e60\u540e\u7528\u4e8e\u6307\u5b9a\u89c4\u5212\u5668\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u4ece\u52a8\u4f5c-\u89c2\u5bdf\u5e8f\u5217\u4e2d\u5b66\u4e60POMDP\u53c2\u6570\uff0c\u653e\u677e\u4e86\u5bf9\u5b8c\u5168\u72b6\u6001\u53ef\u89c2\u6d4b\u6027\u548c\u6ee1\u79e9\u8f6c\u79fb\u77e9\u9635\u7684\u5047\u8bbe\uff0c\u63d0\u4f9b\u4e86\u53ef\u7528\u4e8e\u4e0b\u6e38\u63a8\u7406\u4efb\u52a1\u7684\u663e\u5f0f\u6982\u7387\u6a21\u578b\u3002", "topic": "agent analysis"}}
{"id": "2601.19204", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.19204", "abs": "https://arxiv.org/abs/2601.19204", "authors": ["Zhixi Cai", "Fucai Ke", "Kevin Leo", "Sukai Huang", "Maria Garcia de la Banda", "Peter J. Stuckey", "Hamid Rezatofighi"], "title": "MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning", "comment": "ICLR 2026", "summary": "Recent vision-language models have strong perceptual ability but their implicit reasoning is hard to explain and easily generates hallucinations on complex queries. Compositional methods improve interpretability, but most rely on a single agent or hand-crafted pipeline and cannot decide when to collaborate across complementary agents or compete among overlapping ones. We introduce MATA (Multi-Agent hierarchical Trainable Automaton), a multi-agent system presented as a hierarchical finite-state automaton for visual reasoning whose top-level transitions are chosen by a trainable hyper agent. Each agent corresponds to a state in the hyper automaton, and runs a small rule-based sub-automaton for reliable micro-control. All agents read and write a shared memory, yielding transparent execution history. To supervise the hyper agent's transition policy, we build transition-trajectory trees and transform to memory-to-next-state pairs, forming the MATA-SFT-90K dataset for supervised finetuning (SFT). The finetuned LLM as the transition policy understands the query and the capacity of agents, and it can efficiently choose the optimal agent to solve the task. Across multiple visual reasoning benchmarks, MATA achieves the state-of-the-art results compared with monolithic and compositional baselines. The code and dataset are available at https://github.com/ControlNet/MATA.", "AI": {"tldr": "MATA\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u63a8\u7406\u7684\u591a\u667a\u80fd\u4f53\u5206\u5c42\u53ef\u8bad\u7ec3\u81ea\u52a8\u673a\u7cfb\u7edf\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u8d85\u667a\u80fd\u4f53\u9009\u62e9\u9876\u5c42\u72b6\u6001\u8f6c\u79fb\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u8fd0\u884c\u57fa\u4e8e\u89c4\u5219\u7684\u5b50\u81ea\u52a8\u673a\uff0c\u5b9e\u73b0\u900f\u660e\u6267\u884c\u5386\u53f2\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u611f\u77e5\u80fd\u529b\u5f3a\uff0c\u4f46\u9690\u542b\u63a8\u7406\u96be\u4ee5\u89e3\u91ca\uff0c\u5728\u590d\u6742\u67e5\u8be2\u4e0a\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u3002\u7ec4\u5408\u65b9\u6cd5\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u5927\u591a\u4f9d\u8d56\u5355\u4e00\u667a\u80fd\u4f53\u6216\u624b\u5de5\u8bbe\u8ba1\u7684\u6d41\u7a0b\uff0c\u65e0\u6cd5\u51b3\u5b9a\u4f55\u65f6\u5728\u4e92\u8865\u667a\u80fd\u4f53\u4e4b\u95f4\u534f\u4f5c\u6216\u5728\u91cd\u53e0\u667a\u80fd\u4f53\u4e4b\u95f4\u7ade\u4e89\u3002", "method": "\u5f15\u5165MATA\uff08\u591a\u667a\u80fd\u4f53\u5206\u5c42\u53ef\u8bad\u7ec3\u81ea\u52a8\u673a\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f5c\u4e3a\u5206\u5c42\u6709\u9650\u72b6\u6001\u81ea\u52a8\u673a\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u9876\u5c42\u8f6c\u79fb\u7531\u53ef\u8bad\u7ec3\u7684\u8d85\u667a\u80fd\u4f53\u9009\u62e9\u3002\u6bcf\u4e2a\u667a\u80fd\u4f53\u5bf9\u5e94\u8d85\u81ea\u52a8\u673a\u4e2d\u7684\u4e00\u4e2a\u72b6\u6001\uff0c\u8fd0\u884c\u5c0f\u578b\u57fa\u4e8e\u89c4\u5219\u7684\u5b50\u81ea\u52a8\u673a\u8fdb\u884c\u53ef\u9760\u7684\u5fae\u63a7\u5236\u3002\u6240\u6709\u667a\u80fd\u4f53\u8bfb\u5199\u5171\u4eab\u5185\u5b58\uff0c\u4ea7\u751f\u900f\u660e\u7684\u6267\u884c\u5386\u53f2\u3002\u4e3a\u4e86\u76d1\u7763\u8d85\u667a\u80fd\u4f53\u7684\u8f6c\u79fb\u7b56\u7565\uff0c\u6784\u5efa\u8f6c\u79fb\u8f68\u8ff9\u6811\u5e76\u8f6c\u6362\u4e3a\u5185\u5b58\u5230\u4e0b\u4e00\u4e2a\u72b6\u6001\u5bf9\uff0c\u5f62\u6210MATA-SFT-90K\u6570\u636e\u96c6\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMATA\u76f8\u6bd4\u5355\u4f53\u6a21\u578b\u548c\u7ec4\u5408\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "MATA\u901a\u8fc7\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548c\u53ef\u8bad\u7ec3\u7684\u8f6c\u79fb\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u63a8\u7406\uff0c\u80fd\u591f\u6839\u636e\u67e5\u8be2\u548c\u667a\u80fd\u4f53\u80fd\u529b\u9009\u62e9\u6700\u4f18\u667a\u80fd\u4f53\u89e3\u51b3\u4efb\u52a1\u3002", "topic": "agent analysis"}}
{"id": "2601.19287", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19287", "abs": "https://arxiv.org/abs/2601.19287", "authors": ["Md. Asif Haider", "Thomas Zimmermann"], "title": "Understanding Dominant Themes in Reviewing Agentic AI-authored Code", "comment": null, "summary": "While prior work has examined the generation capabilities of Agentic AI systems, little is known about how reviewers respond to AI-authored code in practice. In this paper, we present a large-scale empirical study of code review dynamics in agent-generated PRs. Using a curated subset of the AIDev dataset, we analyze 19,450 inline review comments spanning 3,177 agent-authored PRs from real-world GitHub repositories. We first derive a taxonomy of 12 review comment themes using topic modeling combined with large language model (LLM)-assisted semantic clustering and consolidation. According to this taxonomy, we then investigate whether zero-shot prompts to LLM can reliably annotate review comments. Our evaluation against human annotations shows that open-source LLM achieves reasonably high exact match (78.63%), macro F1 score (0.78), and substantial agreement with human annotators at the review comment level. At the PR level, the LLM also correctly identifies the dominant review theme with 78% Top-1 accuracy and achieves an average Jaccard similarity of 0.76, indicating strong alignment with human judgments. Applying this annotation pipeline at scale, we find that apart from functional correctness and logical changes, reviews of agent-authored PRs predominantly focus on documentation gaps, refactoring needs, styling and formatting issues, with testing and security-related concerns. These findings suggest that while AI agents can accelerate code production, there remain gaps requiring targeted human review oversight.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9AI\u4ee3\u7406\u751f\u6210\u7684\u4ee3\u7801\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u901a\u8fc7\u4e3b\u9898\u5efa\u6a21\u548cLLM\u8f85\u52a9\u6807\u6ce8\uff0c\u53d1\u73b0AI\u4ee3\u7801\u5ba1\u67e5\u4e3b\u8981\u5173\u6ce8\u6587\u6863\u3001\u91cd\u6784\u3001\u683c\u5f0f\u7b49\u95ee\u9898\uff0c\u800c\u975e\u529f\u80fd\u6b63\u786e\u6027\uff0c\u8868\u660eAI\u52a0\u901f\u4ee3\u7801\u751f\u4ea7\u4f46\u4ecd\u9700\u4eba\u5de5\u5ba1\u67e5\u76d1\u7763\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u5173\u6ce8AI\u4ee3\u7406\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5bf9AI\u751f\u6210\u4ee3\u7801\u5728\u5b9e\u9645\u5ba1\u67e5\u4e2d\u5982\u4f55\u88ab\u8bc4\u5ba1\u7684\u5b9e\u8bc1\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7a76\u8bc4\u5ba1\u8005\u5bf9AI\u751f\u6210\u4ee3\u7801\u7684\u5b9e\u9645\u53cd\u5e94\u3002", "method": "\u4f7f\u7528AIDev\u6570\u636e\u96c6\u7684\u7cbe\u9009\u5b50\u96c6\uff0c\u5206\u679019,450\u6761\u5185\u8054\u8bc4\u5ba1\u8bc4\u8bba\uff0c\u6db5\u76d63,177\u4e2aAI\u751f\u6210\u7684PR\u3002\u91c7\u7528\u4e3b\u9898\u5efa\u6a21\u7ed3\u5408LLM\u8f85\u52a9\u8bed\u4e49\u805a\u7c7b\u548c\u6574\u5408\uff0c\u6784\u5efa12\u4e2a\u8bc4\u5ba1\u8bc4\u8bba\u4e3b\u9898\u5206\u7c7b\u6cd5\u3002\u8bc4\u4f30\u96f6\u6837\u672c\u63d0\u793aLLM\u6807\u6ce8\u8bc4\u5ba1\u8bc4\u8bba\u7684\u53ef\u9760\u6027\u3002", "result": "\u5f00\u6e90LLM\u5728\u8bc4\u5ba1\u8bc4\u8bba\u7ea7\u522b\u8fbe\u523078.63%\u7684\u7cbe\u786e\u5339\u914d\u30010.78\u7684\u5b8f\u89c2F1\u5206\u6570\uff0c\u4e0e\u4eba\u5de5\u6807\u6ce8\u8005\u5177\u6709\u5b9e\u8d28\u6027\u4e00\u81f4\u3002\u5728PR\u7ea7\u522b\uff0cLLM\u4ee578%\u7684Top-1\u51c6\u786e\u7387\u8bc6\u522b\u4e3b\u5bfc\u8bc4\u5ba1\u4e3b\u9898\uff0c\u5e73\u5747Jaccard\u76f8\u4f3c\u5ea6\u4e3a0.76\u3002\u5927\u89c4\u6a21\u5e94\u7528\u53d1\u73b0\uff0cAI\u4ee3\u7801\u5ba1\u67e5\u4e3b\u8981\u5173\u6ce8\u6587\u6863\u7f3a\u53e3\u3001\u91cd\u6784\u9700\u6c42\u3001\u6837\u5f0f\u683c\u5f0f\u95ee\u9898\uff0c\u800c\u975e\u529f\u80fd\u6b63\u786e\u6027\u548c\u903b\u8f91\u53d8\u66f4\u3002", "conclusion": "AI\u4ee3\u7406\u80fd\u52a0\u901f\u4ee3\u7801\u751f\u4ea7\uff0c\u4f46\u5728\u6587\u6863\u3001\u91cd\u6784\u3001\u683c\u5f0f\u7b49\u65b9\u9762\u4ecd\u5b58\u5728\u7f3a\u53e3\uff0c\u9700\u8981\u9488\u5bf9\u6027\u7684\u4eba\u5de5\u5ba1\u67e5\u76d1\u7763\u3002\u7814\u7a76\u4e3aAI\u4ee3\u7801\u5ba1\u67e5\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.19290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19290", "abs": "https://arxiv.org/abs/2601.19290", "authors": ["Yimeng Wang", "Jiaxing Zhao", "Hongbin Xie", "Hexing Ma", "Yuzhen Lei", "Shuangxue Liu", "Xuan Song", "Zichen Zhang", "Haoran Zhang"], "title": "MetaGen: Self-Evolving Roles and Topologies for Multi-Agent LLM Reasoning", "comment": null, "summary": "Large language models are increasingly deployed as multi-agent systems, where specialized roles communicate and collaborate through structured interactions to solve complex tasks that often exceed the capacity of a single agent. However, most existing systems still rely on a fixed role library and an execution-frozen interaction topology, a rigid design choice that frequently leads to task mismatch, prevents timely adaptation when new evidence emerges during reasoning, and further inflates inference cost. We introduce MetaGen, a training-free framework that adapts both the role space and the collaboration topology at inference time, without updating base model weights. MetaGen generates and rewrites query-conditioned role specifications to maintain a controllable dynamic role pool, then instantiates a constrained execution graph around a minimal backbone. During execution, it iteratively updates role prompts and adjusts structural decisions using lightweight feedback signals. Experiments on code generation and multi-step reasoning benchmarks show that MetaGen improves the accuracy and cost tradeoff over strong multi-agent baselines.", "AI": {"tldr": "MetaGen\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8c03\u6574\u89d2\u8272\u7a7a\u95f4\u548c\u534f\u4f5c\u62d3\u6251\u7ed3\u6784\uff0c\u65e0\u9700\u66f4\u65b0\u57fa\u7840\u6a21\u578b\u6743\u91cd\uff0c\u4ece\u800c\u63d0\u5347\u590d\u6742\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u56fa\u5b9a\u7684\u89d2\u8272\u5e93\u548c\u51bb\u7ed3\u7684\u6267\u884c\u62d3\u6251\u7ed3\u6784\uff0c\u8fd9\u79cd\u521a\u6027\u8bbe\u8ba1\u5bfc\u81f4\u4efb\u52a1\u4e0d\u5339\u914d\u3001\u65e0\u6cd5\u53ca\u65f6\u9002\u5e94\u63a8\u7406\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u65b0\u8bc1\u636e\uff0c\u5e76\u4e14\u589e\u52a0\u4e86\u63a8\u7406\u6210\u672c\u3002", "method": "MetaGen\u901a\u8fc7\u751f\u6210\u548c\u91cd\u5199\u67e5\u8be2\u6761\u4ef6\u5316\u7684\u89d2\u8272\u89c4\u8303\u6765\u7ef4\u62a4\u53ef\u63a7\u7684\u52a8\u6001\u89d2\u8272\u6c60\uff0c\u7136\u540e\u56f4\u7ed5\u6700\u5c0f\u9aa8\u5e72\u7f51\u7edc\u5b9e\u4f8b\u5316\u7ea6\u675f\u6267\u884c\u56fe\u3002\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u53cd\u9988\u4fe1\u53f7\u8fed\u4ee3\u66f4\u65b0\u89d2\u8272\u63d0\u793a\u5e76\u8c03\u6574\u7ed3\u6784\u51b3\u7b56\u3002", "result": "\u5728\u4ee3\u7801\u751f\u6210\u548c\u591a\u6b65\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMetaGen\u76f8\u6bd4\u5f3a\u5927\u7684\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u6210\u672c\u6743\u8861\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "MetaGen\u5c55\u793a\u4e86\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u89d2\u8272\u7a7a\u95f4\u548c\u534f\u4f5c\u62d3\u6251\u7ed3\u6784\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6027\u80fd\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2601.19249", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19249", "abs": "https://arxiv.org/abs/2601.19249", "authors": ["Xingkun Yin", "Hongyang Du"], "title": "GLOVE: Global Verifier for LLM Memory-Environment Realignment", "comment": null, "summary": "Most existing memory-enhanced Large Language Model (LLM) approaches implicitly assume that memory validity can be established either through external evaluators that provide task-specific success signals or through internal model cognition, such as reflection, for editing memory entries. However, these assumptions often break down in practical environments with dynamic drifts. We propose the Global Verifier (GLOVE), a framework that introduces a new design dimension for LLM memory systems by establishing a relative notion of truth. Through active probing to detect inconsistencies between retrieved memories and fresh observations, GLOVE enables memory-environment realignment by verifying and updating memory without access to ground-truth supervision or strong reliance on model introspection. We evaluate GLOVE on diverse benchmarks spanning web navigation, planning, and control, augmented with controlled environmental drifts that introduce non-stationarity beyond the original benchmark settings. Our results show that GLOVE substantially improves agent success rates, suggesting a robust pathway to cognitive agents capable of self-evolving.", "AI": {"tldr": "GLOVE\u6846\u67b6\u4e3aLLM\u8bb0\u5fc6\u7cfb\u7edf\u5f15\u5165\u76f8\u5bf9\u771f\u7406\u6982\u5ff5\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a2\u6d4b\u8bb0\u5fc6\u4e0e\u89c2\u5bdf\u7684\u4e0d\u4e00\u81f4\u6027\u5b9e\u73b0\u8bb0\u5fc6-\u73af\u5883\u5bf9\u9f50\uff0c\u65e0\u9700\u771f\u5b9e\u76d1\u7763\u6216\u5f3a\u6a21\u578b\u5185\u7701\uff0c\u5728\u52a8\u6001\u6f02\u79fb\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709LLM\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u5047\u8bbe\u8bb0\u5fc6\u6709\u6548\u6027\u53ef\u901a\u8fc7\u5916\u90e8\u8bc4\u4f30\u8005\u6216\u6a21\u578b\u5185\u7701\u5efa\u7acb\uff0c\u4f46\u8fd9\u4e9b\u5047\u8bbe\u5728\u52a8\u6001\u6f02\u79fb\u7684\u5b9e\u9645\u73af\u5883\u4e2d\u7ecf\u5e38\u5931\u6548\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u9a8c\u8bc1\u548c\u66f4\u65b0\u8bb0\u5fc6\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGLOVE\u6846\u67b6\uff0c\u5f15\u5165\u76f8\u5bf9\u771f\u7406\u6982\u5ff5\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a2\u6d4b\u68c0\u7d22\u8bb0\u5fc6\u4e0e\u65b0\u9c9c\u89c2\u5bdf\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u8bb0\u5fc6-\u73af\u5883\u5bf9\u9f50\u3002\u65e0\u9700\u771f\u5b9e\u76d1\u7763\u6216\u5f3a\u6a21\u578b\u5185\u7701\uff0c\u53ef\u9a8c\u8bc1\u548c\u66f4\u65b0\u8bb0\u5fc6\u3002", "result": "\u5728\u5305\u542b\u7f51\u7edc\u5bfc\u822a\u3001\u89c4\u5212\u548c\u63a7\u5236\u7684\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u8fc7\u6dfb\u52a0\u53d7\u63a7\u73af\u5883\u6f02\u79fb\u5f15\u5165\u975e\u5e73\u7a33\u6027\uff0cGLOVE\u663e\u8457\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u6210\u529f\u7387\u3002", "conclusion": "GLOVE\u4e3aLLM\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8bbe\u8ba1\u7ef4\u5ea6\uff0c\u901a\u8fc7\u76f8\u5bf9\u771f\u7406\u6982\u5ff5\u548c\u4e3b\u52a8\u4e0d\u4e00\u81f4\u6027\u63a2\u6d4b\uff0c\u4e3a\u6784\u5efa\u80fd\u591f\u81ea\u6211\u8fdb\u5316\u7684\u8ba4\u77e5\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7a33\u5065\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2601.19306", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19306", "abs": "https://arxiv.org/abs/2601.19306", "authors": ["Sijia Li", "Xiaoyu Tan", "Shahir Ali", "Niels Schmidt", "Gengchen Ma", "Xihe Qiu"], "title": "Curiosity Driven Knowledge Retrieval for Mobile Agents", "comment": null, "summary": "Mobile agents have made progress toward reliable smartphone automation, yet performance in complex applications remains limited by incomplete knowledge and weak generalization to unseen environments. We introduce a curiosity driven knowledge retrieval framework that formalizes uncertainty during execution as a curiosity score. When this score exceeds a threshold, the system retrieves external information from documentation, code repositories, and historical trajectories. Retrieved content is organized into structured AppCards, which encode functional semantics, parameter conventions, interface mappings, and interaction patterns. During execution, an enhanced agent selectively integrates relevant AppCards into its reasoning process, thereby compensating for knowledge blind spots and improving planning reliability. Evaluation on the AndroidWorld benchmark shows consistent improvements across backbones, with an average gain of six percentage points and a new state of the art success rate of 88.8\\% when combined with GPT-5. Analysis indicates that AppCards are particularly effective for multi step and cross application tasks, while improvements depend on the backbone model. Case studies further confirm that AppCards reduce ambiguity, shorten exploration, and support stable execution trajectories. Task trajectories are publicly available at https://lisalsj.github.io/Droidrun-appcard/.", "AI": {"tldr": "\u63d0\u51fa\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u77e5\u8bc6\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7AppCards\u7f16\u7801\u5e94\u7528\u529f\u80fd\u8bed\u4e49\u548c\u4ea4\u4e92\u6a21\u5f0f\uff0c\u589e\u5f3a\u79fb\u52a8\u4ee3\u7406\u5728\u590d\u6742\u667a\u80fd\u624b\u673a\u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u5f53\u524d\u79fb\u52a8\u4ee3\u7406\u5728\u590d\u6742\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u4e0d\u5b8c\u6574\u77e5\u8bc6\u548c\u5bf9\u672a\u89c1\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u5f31\uff0c\u9700\u8981\u89e3\u51b3\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027", "method": "\u4f7f\u7528\u597d\u5947\u5fc3\u5206\u6570\u5f62\u5f0f\u5316\u6267\u884c\u4e0d\u786e\u5b9a\u6027\uff0c\u5f53\u8d85\u8fc7\u9608\u503c\u65f6\u4ece\u6587\u6863\u3001\u4ee3\u7801\u5e93\u548c\u5386\u53f2\u8f68\u8ff9\u68c0\u7d22\u5916\u90e8\u4fe1\u606f\uff0c\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316AppCards\uff08\u7f16\u7801\u529f\u80fd\u8bed\u4e49\u3001\u53c2\u6570\u7ea6\u5b9a\u3001\u63a5\u53e3\u6620\u5c04\u548c\u4ea4\u4e92\u6a21\u5f0f\uff09\uff0c\u589e\u5f3a\u4ee3\u7406\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9009\u62e9\u6027\u96c6\u6210\u76f8\u5173\u4fe1\u606f", "result": "\u5728AndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u6709\u9aa8\u5e72\u6a21\u578b\u4e0a\u5747\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u5e73\u5747\u63d0\u53476\u4e2a\u767e\u5206\u70b9\uff0c\u4e0eGPT-5\u7ed3\u5408\u65f6\u8fbe\u523088.8%\u7684\u6700\u65b0SOTA\u6210\u529f\u7387\uff1bAppCards\u5bf9\u591a\u6b65\u9aa4\u548c\u8de8\u5e94\u7528\u4efb\u52a1\u7279\u522b\u6709\u6548", "conclusion": "\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u77e5\u8bc6\u68c0\u7d22\u6846\u67b6\u901a\u8fc7AppCards\u663e\u8457\u63d0\u5347\u79fb\u52a8\u4ee3\u7406\u6027\u80fd\uff0c\u51cf\u5c11\u6a21\u7cca\u6027\u3001\u7f29\u77ed\u63a2\u7d22\u65f6\u95f4\u5e76\u652f\u6301\u7a33\u5b9a\u6267\u884c\u8f68\u8ff9\uff0c\u6539\u8fdb\u6548\u679c\u4f9d\u8d56\u4e8e\u9aa8\u5e72\u6a21\u578b\u80fd\u529b", "topic": "code agent"}}
{"id": "2601.19311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19311", "abs": "https://arxiv.org/abs/2601.19311", "authors": ["Anh Khoa Ngo Ho", "Martin Chauvin", "Simon Gosset", "Philippe Cordier", "Boris Gamazaychikov"], "title": "Balancing Sustainability And Performance: The Role Of Small-Scale Llms In Agentic Artificial Intelligence Systems", "comment": null, "summary": "As large language models become integral to agentic artificial intelligence systems, their energy demands during inference may pose significant sustainability challenges. This study investigates whether deploying smaller-scale language models can reduce energy consumption without compromising responsiveness and output quality in a multi-agent, real-world environments. We conduct a comparative analysis across language models of varying scales to quantify trade-offs between efficiency and performance. Results show that smaller open-weights models can lower energy usage while preserving task quality. Building on these findings, we propose practical guidelines for sustainable artificial intelligence design, including optimal batch size configuration and computation resource allocation. These insights offer actionable strategies for developing scalable, environmentally responsible artificial intelligence systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5c0f\u578b\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u5728\u4fdd\u6301\u4efb\u52a1\u8d28\u91cf\u7684\u540c\u65f6\u80fd\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u4e3a\u53ef\u6301\u7eedAI\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u5b9e\u7528\u6307\u5357", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u6210\u4e3a\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u80fd\u8017\u53ef\u80fd\u5e26\u6765\u663e\u8457\u7684\u53ef\u6301\u7eed\u6027\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u90e8\u7f72\u8f83\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u5728\u591a\u667a\u80fd\u4f53\u771f\u5b9e\u73af\u5883\u4e2d\u964d\u4f4e\u80fd\u8017\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u54cd\u5e94\u6027\u548c\u8f93\u51fa\u8d28\u91cf\u3002", "method": "\u5bf9\u4e0d\u540c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff0c\u91cf\u5316\u6548\u7387\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u51fa\u53ef\u6301\u7eedAI\u8bbe\u8ba1\u7684\u5b9e\u7528\u6307\u5357\uff0c\u5305\u62ec\u6700\u4f18\u6279\u5904\u7406\u5927\u5c0f\u914d\u7f6e\u548c\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u7b56\u7565\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8f83\u5c0f\u7684\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u80fd\u591f\u964d\u4f4e\u80fd\u6e90\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u8d28\u91cf\u3002\u8fd9\u4e3a\u5f00\u53d1\u53ef\u6269\u5c55\u3001\u73af\u5883\u53cb\u597d\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u901a\u8fc7\u5408\u7406\u9009\u62e9\u6a21\u578b\u89c4\u6a21\u548c\u4f18\u5316\u914d\u7f6e\uff0c\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0AI\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u53d1\u5c55\uff0c\u4e3a\u6784\u5efa\u73af\u5883\u53cb\u597d\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5177\u4f53\u7b56\u7565\u3002", "topic": "agent analysis"}}
{"id": "2601.19494", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19494", "abs": "https://arxiv.org/abs/2601.19494", "authors": ["Lei Zhang", "Yongda Yu", "Minghui Yu", "Xinxin Guo", "Zhengqi Zhuang", "Guoping Rong", "Dong Shao", "Haifeng Shen", "Hongyu Kuang", "Zhengfeng Li", "Boge Wang", "Guoan Zhang", "Bangyu Xiang", "Xiaobing Xu"], "title": "AACR-Bench: Evaluating Automatic Code Review with Holistic Repository-Level Context", "comment": null, "summary": "High-quality evaluation benchmarks are pivotal for deploying Large Language Models (LLMs) in Automated Code Review (ACR). However, existing benchmarks suffer from two critical limitations: first, the lack of multi-language support in repository-level contexts, which restricts the generalizability of evaluation results; second, the reliance on noisy, incomplete ground truth derived from raw Pull Request (PR) comments, which constrains the scope of issue detection. To address these challenges, we introduce AACR-Bench a comprehensive benchmark that provides full cross-file context across multiple programming languages. Unlike traditional datasets, AACR-Bench employs an \"AI-assisted, Expert-verified\" annotation pipeline to uncover latent defects often overlooked in original PRs, resulting in a 285\\% increase in defect coverage. Extensive evaluations of mainstream LLMs on AACR-Bench reveal that previous assessments may have either misjudged or only partially captured model capabilities due to data limitations. Our work establishes a more rigorous standard for ACR evaluation and offers new insights on LLM based ACR, i.e., the granularity/level of context and the choice of retrieval methods significantly impact ACR performance, and this influence varies depending on the LLM, programming language, and the LLM usage paradigm e.g., whether an Agent architecture is employed. The code, data, and other artifacts of our evaluation set are available at https://github.com/alibaba/aacr-bench .", "AI": {"tldr": "AACR-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u8bc4\u4f30\u7684\u65b0\u57fa\u51c6\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u8de8\u6587\u4ef6\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7AI\u8f85\u52a9\u4e13\u5bb6\u9a8c\u8bc1\u7684\u6807\u6ce8\u6d41\u7a0b\u663e\u8457\u63d0\u9ad8\u4e86\u7f3a\u9677\u8986\u76d6\u7387\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u57fa\u51c6\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1) \u7f3a\u4e4f\u591a\u8bed\u8a00\u652f\u6301\uff0c\u9650\u5236\u4e86\u8bc4\u4f30\u7ed3\u679c\u7684\u6cdb\u5316\u80fd\u529b\uff1b2) \u4f9d\u8d56\u539f\u59cbPR\u8bc4\u8bba\u4e2d\u7684\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u771f\u5b9e\u6570\u636e\uff0c\u9650\u5236\u4e86\u95ee\u9898\u68c0\u6d4b\u8303\u56f4\u3002", "method": "\u63d0\u51faAACR-Bench\u57fa\u51c6\uff0c\u63d0\u4f9b\u591a\u7f16\u7a0b\u8bed\u8a00\u7684\u5b8c\u6574\u8de8\u6587\u4ef6\u4e0a\u4e0b\u6587\uff0c\u91c7\u7528\"AI\u8f85\u52a9\u3001\u4e13\u5bb6\u9a8c\u8bc1\"\u7684\u6807\u6ce8\u6d41\u7a0b\uff0c\u53d1\u73b0\u539f\u59cbPR\u4e2d\u5e38\u88ab\u5ffd\u7565\u7684\u6f5c\u5728\u7f3a\u9677\u3002", "result": "AACR-Bench\u5c06\u7f3a\u9677\u8986\u76d6\u7387\u63d0\u9ad8\u4e86285%\uff0c\u5bf9\u4e3b\u6d41LLM\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u7531\u4e8e\u6570\u636e\u9650\u5236\uff0c\u4e4b\u524d\u7684\u8bc4\u4f30\u53ef\u80fd\u8bef\u5224\u6216\u4ec5\u90e8\u5206\u6355\u6349\u4e86\u6a21\u578b\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u4e0a\u4e0b\u6587\u7c92\u5ea6/\u7ea7\u522b\u548c\u68c0\u7d22\u65b9\u6cd5\u9009\u62e9\u663e\u8457\u5f71\u54cdACR\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aACR\u8bc4\u4f30\u5efa\u7acb\u4e86\u66f4\u4e25\u683c\u7684\u6807\u51c6\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8eLLM\u5728ACR\u4e2d\u5e94\u7528\u7684\u65b0\u89c1\u89e3\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u7c92\u5ea6\u3001\u68c0\u7d22\u65b9\u6cd5\u9009\u62e9\u7b49\u5173\u952e\u56e0\u7d20\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "topic": "swe benchmark"}}
{"id": "2601.19583", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19583", "abs": "https://arxiv.org/abs/2601.19583", "authors": ["D\u00e9bora Souza", "Patr\u00edcia Machado"], "title": "Toward Architecture-Aware Evaluation Metrics for LLM Agents", "comment": "Accepted at CAIN 2026 (IEEE/ACM 5th International Conference on AI Engineering)", "summary": "LLM-based agents are becoming central to software engineering tasks, yet evaluating them remains fragmented and largely model-centric. Existing studies overlook how architectural components, such as planners, memory, and tool routers, shape agent behavior, limiting diagnostic power. We propose a lightweight, architecture-informed approach that links agent components to their observable behaviors and to the metrics capable of evaluating them. Our method clarifies what to measure and why, and we illustrate its application through real world agents, enabling more targeted, transparent, and actionable evaluation of LLM-based agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u57fa\u4e8e\u67b6\u6784\u7684LLM\u667a\u80fd\u4f53\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c06\u667a\u80fd\u4f53\u7ec4\u4ef6\u4e0e\u53ef\u89c2\u5bdf\u884c\u4e3a\u53ca\u8bc4\u4f30\u6307\u6807\u5173\u8054\u8d77\u6765\uff0c\u5b9e\u73b0\u66f4\u6709\u9488\u5bf9\u6027\u3001\u900f\u660e\u548c\u53ef\u64cd\u4f5c\u7684\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u8bc4\u4f30\u5b58\u5728\u788e\u7247\u5316\u4e14\u8fc7\u4e8e\u6a21\u578b\u4e2d\u5fc3\u5316\u7684\u95ee\u9898\uff0c\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u89c4\u5212\u5668\u3001\u8bb0\u5fc6\u3001\u5de5\u5177\u8def\u7531\u7b49\u67b6\u6784\u7ec4\u4ef6\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u8bca\u65ad\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u3001\u57fa\u4e8e\u67b6\u6784\u7684\u65b9\u6cd5\uff0c\u5c06\u667a\u80fd\u4f53\u7ec4\u4ef6\u4e0e\u5176\u53ef\u89c2\u5bdf\u884c\u4e3a\u4ee5\u53ca\u80fd\u591f\u8bc4\u4f30\u8fd9\u4e9b\u884c\u4e3a\u7684\u6307\u6807\u8054\u7cfb\u8d77\u6765\uff0c\u660e\u786e\u6d4b\u91cf\u5185\u5bb9\u548c\u539f\u56e0\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u667a\u80fd\u4f53\u5e94\u7528\u8fdb\u884c\u8bf4\u660e\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u6709\u9488\u5bf9\u6027\u3001\u900f\u660e\u548c\u53ef\u64cd\u4f5c\u7684LLM\u667a\u80fd\u4f53\u8bc4\u4f30\uff0c\u6f84\u6e05\u4e86\u6d4b\u91cf\u5185\u5bb9\u548c\u539f\u56e0\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u5e94\u7528\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u67b6\u6784\u7684\u8bc4\u4f30\u65b9\u6cd5\u80fd\u591f\u89e3\u51b3\u5f53\u524dLLM\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u8bca\u65ad\u80fd\u529b\uff0c\u4fc3\u8fdb\u66f4\u6709\u6548\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u548c\u4f18\u5316\u3002", "topic": "agent analysis"}}
{"id": "2601.19404", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19404", "abs": "https://arxiv.org/abs/2601.19404", "authors": ["Hongzhu Yi", "Xinming Wang", "Zhenghao zhang", "Tianyu Zong", "Yuanxiang Wang", "Jun Xie", "Tao Yu", "Haopeng Jin", "Zhepeng Wang", "Kaixin Xu", "Feng Chen", "Jiahuan Chen", "Yujia Yang", "Zhenyu Guan", "Bingkang Shi", "Jungang Xu"], "title": "RPO:Reinforcement Fine-Tuning with Partial Reasoning Optimization", "comment": null, "summary": "Within the domain of large language models, reinforcement fine-tuning algorithms necessitate the generation of a complete reasoning trajectory beginning from the input query, which incurs significant computational overhead during the rollout phase of training. To address this issue, we analyze the impact of different segments of the reasoning path on the correctness of the final result and, based on these insights, propose Reinforcement Fine-Tuning with Partial Reasoning Optimization (RPO), a plug-and-play reinforcement fine-tuning algorithm. Unlike traditional reinforcement fine-tuning algorithms that generate full reasoning paths, RPO trains the model by generating suffixes of the reasoning path using experience cache. During the rollout phase of training, RPO reduces token generation in this phase by approximately 95%, greatly lowering the theoretical time overhead. Compared with full-path reinforcement fine-tuning algorithms, RPO reduces the training time of the 1.5B model by 90% and the 7B model by 72%. At the same time, it can be integrated with typical algorithms such as GRPO and DAPO, enabling them to achieve training acceleration while maintaining performance comparable to the original algorithms. Our code is open-sourced at https://github.com/yhz5613813/RPO.", "AI": {"tldr": "RPO\u662f\u4e00\u79cd\u90e8\u5206\u63a8\u7406\u4f18\u5316\u7684\u5f3a\u5316\u5fae\u8c03\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ec5\u751f\u6210\u63a8\u7406\u8def\u5f84\u7684\u540e\u7f00\u6765\u51cf\u5c11\u7ea695%\u7684\u8bad\u7ec3\u9636\u6bb5token\u751f\u6210\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5b8c\u6574\u8def\u5f84\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfLLM\u5f3a\u5316\u5fae\u8c03\u7b97\u6cd5\u9700\u8981\u4ece\u8f93\u5165\u67e5\u8be2\u5f00\u59cb\u751f\u6210\u5b8c\u6574\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u8fd9\u5728\u8bad\u7ec3rollout\u9636\u6bb5\u4ea7\u751f\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6548\u7387\u95ee\u9898\u3002", "method": "\u5206\u6790\u63a8\u7406\u8def\u5f84\u4e0d\u540c\u90e8\u5206\u5bf9\u6700\u7ec8\u7ed3\u679c\u6b63\u786e\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u51faRPO\u7b97\u6cd5\u3002\u4e0d\u540c\u4e8e\u751f\u6210\u5b8c\u6574\u63a8\u7406\u8def\u5f84\uff0cRPO\u4f7f\u7528\u7ecf\u9a8c\u7f13\u5b58\u751f\u6210\u63a8\u7406\u8def\u5f84\u7684\u540e\u7f00\u8fdb\u884c\u8bad\u7ec3\uff0c\u5927\u5e45\u51cf\u5c11rollout\u9636\u6bb5\u7684token\u751f\u6210\u3002", "result": "RPO\u5728\u8bad\u7ec3rollout\u9636\u6bb5\u51cf\u5c11\u7ea695%\u7684token\u751f\u6210\uff0c\u5c061.5B\u6a21\u578b\u7684\u8bad\u7ec3\u65f6\u95f4\u964d\u4f4e90%\uff0c7B\u6a21\u578b\u964d\u4f4e72%\u3002\u53ef\u4e0eGRPO\u3001DAPO\u7b49\u5178\u578b\u7b97\u6cd5\u96c6\u6210\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "RPO\u662f\u4e00\u79cd\u9ad8\u6548\u7684plug-and-play\u5f3a\u5316\u5fae\u8c03\u7b97\u6cd5\uff0c\u901a\u8fc7\u90e8\u5206\u63a8\u7406\u4f18\u5316\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3aLLM\u5f3a\u5316\u5fae\u8c03\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u52a0\u901f\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.18984", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18984", "abs": "https://arxiv.org/abs/2601.18984", "authors": ["Haolin Liu", "Dian Yu", "Sidi Lu", "Yujun Zhou", "Rui Liu", "Zhenwen Liang", "Haitao Mi", "Chen-Yu Wei", "Dong Yu"], "title": "Save the Good Prefix: Precise Error Penalization via Process-Supervised RL to Enhance LLM Reasoning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful framework for improving the reasoning capabilities of large language models (LLMs). However, most existing RL approaches rely on sparse outcome rewards, which fail to credit correct intermediate steps in partially successful solutions. Process reward models (PRMs) offer fine-grained step-level supervision, but their scores are often noisy and difficult to evaluate. As a result, recent PRM benchmarks focus on a more objective capability: detecting the first incorrect step in a reasoning path. However, this evaluation target is misaligned with how PRMs are typically used in RL, where their step-wise scores are treated as raw rewards to maximize. To bridge this gap, we propose Verifiable Prefix Policy Optimization (VPPO), which uses PRMs only to localize the first error during RL. Given an incorrect rollout, VPPO partitions the trajectory into a verified correct prefix and an erroneous suffix based on the first error, rewarding the former while applying targeted penalties only after the detected mistake. This design yields stable, interpretable learning signals and improves credit assignment. Across multiple reasoning benchmarks, VPPO consistently outperforms sparse-reward RL and prior PRM-guided baselines on both Pass@1 and Pass@K.", "AI": {"tldr": "VPPO\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u4ec5\u5b9a\u4f4d\u63a8\u7406\u8def\u5f84\u4e2d\u7684\u7b2c\u4e00\u4e2a\u9519\u8bef\uff0c\u5c06\u8f68\u8ff9\u5212\u5206\u4e3a\u5df2\u9a8c\u8bc1\u7684\u6b63\u786e\u524d\u7f00\u548c\u9519\u8bef\u540e\u7f00\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u5b66\u4e60\u4fe1\u53f7\u548c\u66f4\u597d\u7684\u4fe1\u7528\u5206\u914d\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\uff0c\u65e0\u6cd5\u5bf9\u90e8\u5206\u6210\u529f\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u6b63\u786e\u4e2d\u95f4\u6b65\u9aa4\u7ed9\u4e88\u9002\u5f53\u4fe1\u7528\u3002\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u867d\u7136\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u6b65\u9aa4\u7ea7\u76d1\u7763\uff0c\u4f46\u5176\u8bc4\u5206\u901a\u5e38\u5b58\u5728\u566a\u58f0\u4e14\u96be\u4ee5\u8bc4\u4f30\u3002\u5f53\u524d\u7684PRM\u57fa\u51c6\u6d4b\u8bd5\u4fa7\u91cd\u4e8e\u68c0\u6d4b\u63a8\u7406\u8def\u5f84\u4e2d\u7684\u7b2c\u4e00\u4e2a\u9519\u8bef\u6b65\u9aa4\uff0c\u8fd9\u4e0ePRM\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u901a\u5e38\u4f5c\u4e3a\u539f\u59cb\u5956\u52b1\u6700\u5927\u5316\u7684\u4f7f\u7528\u65b9\u5f0f\u5b58\u5728\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51fa\u53ef\u9a8c\u8bc1\u524d\u7f00\u7b56\u7565\u4f18\u5316\uff08VPPO\uff09\uff0c\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528PRM\u6765\u5b9a\u4f4d\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u7b2c\u4e00\u4e2a\u9519\u8bef\u3002\u5bf9\u4e8e\u4e0d\u6b63\u786e\u7684rollout\uff0cVPPO\u57fa\u4e8e\u7b2c\u4e00\u4e2a\u9519\u8bef\u5c06\u8f68\u8ff9\u5212\u5206\u4e3a\u5df2\u9a8c\u8bc1\u7684\u6b63\u786e\u524d\u7f00\u548c\u9519\u8bef\u540e\u7f00\uff0c\u5bf9\u524d\u8005\u7ed9\u4e88\u5956\u52b1\uff0c\u4ec5\u5728\u88ab\u68c0\u6d4b\u5230\u7684\u9519\u8bef\u4e4b\u540e\u5e94\u7528\u6709\u9488\u5bf9\u6027\u7684\u60e9\u7f5a\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVPPO\u5728Pass@1\u548cPass@K\u6307\u6807\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u7a00\u758f\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u548c\u5148\u524d\u7684PRM\u5f15\u5bfc\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VPPO\u901a\u8fc7\u66f4\u7cbe\u786e\u5730\u4f7f\u7528PRM\u6765\u5b9a\u4f4d\u7b2c\u4e00\u4e2a\u9519\u8bef\uff0c\u63d0\u4f9b\u4e86\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u6539\u5584\u4e86\u4fe1\u7528\u5206\u914d\uff0c\u4ece\u800c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.19693", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19693", "abs": "https://arxiv.org/abs/2601.19693", "authors": ["Frank Elberzhager", "Matthias Gerbershagen", "Joshua Ginkel"], "title": "Using LLMs to Evaluate Architecture Documents: Results from a Digital Marketplace Environment", "comment": null, "summary": "Generative AI plays an increasing role during software engineering activities to make them, e.g., more efficient or provide better quality. However, it is often unclear how much benefit LLMs really provide. We concentrate on software architects and investigated how an LLM-supported evaluation of architecture documents can support software architects to improve such artefacts. In the context of a research project where a digital marketplace is developed and digital solutions should be analyzed, we used different LLMs to analyze the quality of architecture documents and compared the results with evaluations from software architects. We found out that the quality of the artifact has a strong influence on the quality of the LLM, i.e., the better the quality of the architecture document was, the more consistent were the LLM-based evaluation and the human expert evaluation. While using LLMs in this architecture task is promising, our results showed inconsistencies that need further analyses before generalizing them.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30LLM\u5728\u8f6f\u4ef6\u67b6\u6784\u6587\u6863\u8d28\u91cf\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6587\u6863\u8d28\u91cf\u8d8a\u9ad8\uff0cLLM\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u8d8a\u597d\uff0c\u4f46\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u9700\u8981\u8fdb\u4e00\u6b65\u5206\u6790\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46LLM\u7684\u5b9e\u9645\u6548\u76ca\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u5173\u6ce8\u8f6f\u4ef6\u67b6\u6784\u5e08\uff0c\u63a2\u7d22LLM\u652f\u6301\u7684\u67b6\u6784\u6587\u6863\u8bc4\u4f30\u5982\u4f55\u5e2e\u52a9\u6539\u8fdb\u8fd9\u4e9b\u5de5\u4ef6\u3002", "method": "\u5728\u5f00\u53d1\u6570\u5b57\u5e02\u573a\u5e73\u53f0\u7684\u7814\u7a76\u9879\u76ee\u4e2d\uff0c\u4f7f\u7528\u4e0d\u540cLLM\u5206\u6790\u67b6\u6784\u6587\u6863\u8d28\u91cf\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e\u8f6f\u4ef6\u67b6\u6784\u5e08\u7684\u8bc4\u4f30\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u53d1\u73b0\u5de5\u4ef6\u8d28\u91cf\u5bf9LLM\u8bc4\u4f30\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd\uff1a\u67b6\u6784\u6587\u6863\u8d28\u91cf\u8d8a\u597d\uff0cLLM\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u8d8a\u9ad8\u3002\u867d\u7136LLM\u5728\u8be5\u67b6\u6784\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u524d\u666f\uff0c\u4f46\u7ed3\u679c\u663e\u793a\u51fa\u9700\u8981\u8fdb\u4e00\u6b65\u5206\u6790\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "LLM\u5728\u67b6\u6784\u6587\u6863\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u8bc4\u4f30\u7ed3\u679c\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u5206\u6790\u624d\u80fd\u63a8\u5e7f\u4f7f\u7528\u3002", "topic": "swe application"}}
{"id": "2601.19568", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19568", "abs": "https://arxiv.org/abs/2601.19568", "authors": ["Ke Xu", "Siyang Xiao", "Ming Liang", "Yichen Yu", "Zhixiang Wang", "Jingxuan Xu", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "Learning Adaptive Parallel Execution for Efficient Code Localization", "comment": "13 pages, 4 figures", "summary": "Code localization constitutes a key bottleneck in automated software development pipelines. While concurrent tool execution can enhance discovery speed, current agents demonstrate a 34.9\\% redundant invocation rate, which negates parallelism benefits. We propose \\textbf{FuseSearch}, reformulating parallel code localization as a \\textbf{joint quality-efficiency optimization} task. Through defining \\textbf{tool efficiency} -- the ratio of unique information gain to invocation count -- we utilize a two-phase SFT and RL training approach for learning adaptive parallel strategies. Different from fixed-breadth approaches, FuseSearch dynamically modulates search breadth according to task context, evolving from exploration phases to refinement stages. Evaluated on SWE-bench Verified, FuseSearch-4B achieves SOTA-level performance (84.7\\% file-level and 56.4\\% function-level $F_1$ scores) with 93.6\\% speedup, utilizing 67.7\\% fewer turns and 68.9\\% fewer tokens. Results indicate that efficiency-aware training naturally improves quality through eliminating noisy redundant signals, enabling high-performance cost-effective localization agents.", "AI": {"tldr": "FuseSearch\u901a\u8fc7\u8054\u5408\u8d28\u91cf-\u6548\u7387\u4f18\u5316\uff0c\u52a8\u6001\u8c03\u6574\u5e76\u884c\u641c\u7d22\u5e7f\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u5197\u4f59\u8c03\u7528\uff0c\u5728\u4ee3\u7801\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u5e76\u5927\u5e45\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u5b9a\u4f4d\u4ee3\u7406\u5b58\u572834.9%\u7684\u5197\u4f59\u8c03\u7528\u7387\uff0c\u62b5\u6d88\u4e86\u5e76\u884c\u6267\u884c\u7684\u4f18\u52bf\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5e76\u884c\u7b56\u7565\u6765\u4f18\u5316\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faFuseSearch\u6846\u67b6\uff0c\u5c06\u5e76\u884c\u4ee3\u7801\u5b9a\u4f4d\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8054\u5408\u8d28\u91cf-\u6548\u7387\u4f18\u5316\u4efb\u52a1\uff0c\u5b9a\u4e49\u5de5\u5177\u6548\u7387\u6307\u6807\uff0c\u91c7\u7528\u4e24\u9636\u6bb5SFT\u548cRL\u8bad\u7ec3\u5b66\u4e60\u81ea\u9002\u5e94\u5e76\u884c\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u641c\u7d22\u5e7f\u5ea6\u3002", "result": "\u5728SWE-bench Verified\u4e0a\uff0cFuseSearch-4B\u8fbe\u5230SOTA\u6027\u80fd\uff08\u6587\u4ef6\u7ea7F1 84.7%\uff0c\u51fd\u6570\u7ea7F1 56.4%\uff09\uff0c\u52a0\u901f93.6%\uff0c\u51cf\u5c1167.7%\u7684\u8f6e\u6b21\u548c68.9%\u7684token\u4f7f\u7528\u3002", "conclusion": "\u6548\u7387\u611f\u77e5\u8bad\u7ec3\u901a\u8fc7\u6d88\u9664\u5197\u4f59\u566a\u58f0\u4fe1\u53f7\u81ea\u7136\u63d0\u5347\u8d28\u91cf\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u4ee3\u7801\u5b9a\u4f4d\u4ee3\u7406\u3002", "topic": "code agent"}}
{"id": "2601.19697", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19697", "abs": "https://arxiv.org/abs/2601.19697", "authors": ["Tianyue Jiang", "Yanli Wang", "Yanlin Wang", "Daya Guo", "Ensheng Shi", "Yuchi Ma", "Jiachi Chen", "Zibin Zheng"], "title": "AlignCoder: Aligning Retrieval with Target Intent for Repository-Level Code Completion", "comment": "To appear at ASE'25", "summary": "Repository-level code completion remains a challenging task for existing code large language models (code LLMs) due to their limited understanding of repository-specific context and domain knowledge. While retrieval-augmented generation (RAG) approaches have shown promise by retrieving relevant code snippets as cross-file context, they suffer from two fundamental problems: misalignment between the query and the target code in the retrieval process, and the inability of existing retrieval methods to effectively utilize the inference information. To address these challenges, we propose AlignCoder, a repository-level code completion framework that introduces a query enhancement mechanism and a reinforcement learning based retriever training method. Our approach generates multiple candidate completions to construct an enhanced query that bridges the semantic gap between the initial query and the target code. Additionally, we employ reinforcement learning to train an AlignRetriever that learns to leverage inference information in the enhanced query for more accurate retrieval. We evaluate AlignCoder on two widely-used benchmarks (CrossCodeEval and RepoEval) across five backbone code LLMs, demonstrating an 18.1% improvement in EM score compared to baselines on the CrossCodeEval benchmark. The results show that our framework achieves superior performance and exhibits high generalizability across various code LLMs and programming languages.", "AI": {"tldr": "AlignCoder\u662f\u4e00\u4e2a\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u67e5\u8be2\u589e\u5f3a\u673a\u5236\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u68c0\u7d22\u5668\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u4e2d\u7684\u67e5\u8be2-\u76ee\u6807\u4ee3\u7801\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5728CrossCodeEval\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8618.1%\u7684EM\u5206\u6570\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u5bf9\u4ed3\u5e93\u7279\u5b9a\u4e0a\u4e0b\u6587\u548c\u9886\u57df\u77e5\u8bc6\u7684\u7406\u89e3\u4e0d\u8db3\u3002\u867d\u7136\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\u4f5c\u4e3a\u8de8\u6587\u4ef6\u4e0a\u4e0b\u6587\u663e\u793a\u4e86\u4e00\u5b9a\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u67e5\u8be2\u4e0e\u76ee\u6807\u4ee3\u7801\u7684\u4e0d\u5bf9\u9f50\uff0c\u4ee5\u53ca\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5229\u7528\u63a8\u7406\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86AlignCoder\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u67e5\u8be2\u589e\u5f3a\u673a\u5236\uff0c\u901a\u8fc7\u751f\u6210\u591a\u4e2a\u5019\u9009\u8865\u5168\u6765\u6784\u5efa\u589e\u5f3a\u67e5\u8be2\uff0c\u5f25\u5408\u521d\u59cb\u67e5\u8be2\u4e0e\u76ee\u6807\u4ee3\u7801\u4e4b\u95f4\u7684\u8bed\u4e49\u5dee\u8ddd\uff1b2) \u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u68c0\u7d22\u5668\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8bad\u7ec3\u4e00\u4e2aAlignRetriever\uff0c\u5b66\u4e60\u5229\u7528\u589e\u5f3a\u67e5\u8be2\u4e2d\u7684\u63a8\u7406\u4fe1\u606f\u8fdb\u884c\u66f4\u51c6\u786e\u7684\u68c0\u7d22\u3002", "result": "\u5728CrossCodeEval\u548cRepoEval\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u4e0a\uff0c\u5bf9\u4e94\u4e2a\u9aa8\u5e72\u4ee3\u7801LLM\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728CrossCodeEval\u57fa\u51c6\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e8618.1%\u7684EM\u5206\u6570\u63d0\u5347\u3002\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u540c\u4ee3\u7801LLM\u548c\u7f16\u7a0b\u8bed\u8a00\u4e0a\u8868\u73b0\u51fa\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AlignCoder\u901a\u8fc7\u67e5\u8be2\u589e\u5f3a\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u68c0\u7d22\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u4e2d\u7684\u67e5\u8be2-\u76ee\u6807\u4ee3\u7801\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u8865\u5168\u6027\u80fd\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2601.19607", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19607", "abs": "https://arxiv.org/abs/2601.19607", "authors": ["Haoyun Li", "Ming Xiao", "Kezhi Wang", "Robert Schober", "Dong In Kim", "Yong Liang Guan"], "title": "ComAgent: Multi-LLM based Agentic AI Empowered Intelligent Wireless Networks", "comment": null, "summary": "Emerging 6G networks rely on complex cross-layer optimization, yet manually translating high-level intents into mathematical formulations remains a bottleneck. While Large Language Models (LLMs) offer promise, monolithic approaches often lack sufficient domain grounding, constraint awareness, and verification capabilities. To address this, we present ComAgent, a multi-LLM agentic AI framework. ComAgent employs a closed-loop Perception-Planning-Action-Reflection cycle, coordinating specialized agents for literature search, coding, and scoring to autonomously generate solver-ready formulations and reproducible simulations. By iteratively decomposing problems and self-correcting errors, the framework effectively bridges the gap between user intent and execution. Evaluations demonstrate that ComAgent achieves expert-comparable performance in complex beamforming optimization and outperforms monolithic LLMs across diverse wireless tasks, highlighting its potential for automating design in emerging wireless networks.", "AI": {"tldr": "ComAgent\u662f\u4e00\u4e2a\u591aLLM\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5-\u89c4\u5212-\u6267\u884c-\u53cd\u601d\u95ed\u73af\uff0c\u534f\u8c03\u4e13\u4e1a\u667a\u80fd\u4f53\u81ea\u52a8\u751f\u62106G\u7f51\u7edc\u4f18\u5316\u95ee\u9898\u7684\u6c42\u89e3\u5668\u5c31\u7eea\u516c\u5f0f\u548c\u53ef\u590d\u73b0\u4eff\u771f\u3002", "motivation": "6G\u7f51\u7edc\u4f9d\u8d56\u590d\u6742\u7684\u8de8\u5c42\u4f18\u5316\uff0c\u4f46\u5c06\u9ad8\u5c42\u610f\u56fe\u624b\u52a8\u8f6c\u5316\u4e3a\u6570\u5b66\u516c\u5f0f\u5b58\u5728\u74f6\u9888\u3002\u73b0\u6709LLM\u65b9\u6cd5\u7f3a\u4e4f\u8db3\u591f\u7684\u9886\u57df\u57fa\u7840\u3001\u7ea6\u675f\u610f\u8bc6\u548c\u9a8c\u8bc1\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591aLLM\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5-\u89c4\u5212-\u6267\u884c-\u53cd\u601d\u95ed\u73af\uff0c\u534f\u8c03\u6587\u732e\u641c\u7d22\u3001\u7f16\u7801\u548c\u8bc4\u5206\u7b49\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u8fed\u4ee3\u5206\u89e3\u95ee\u9898\u5e76\u81ea\u6211\u7ea0\u6b63\u9519\u8bef\u3002", "result": "ComAgent\u5728\u590d\u6742\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u4e2d\u8fbe\u5230\u4e13\u5bb6\u53ef\u6bd4\u6027\u80fd\uff0c\u5728\u591a\u6837\u5316\u65e0\u7ebf\u4efb\u52a1\u4e2d\u8d85\u8d8a\u5355\u4f53LLM\uff0c\u5c55\u793a\u4e86\u5728\u65e0\u7ebf\u7f51\u7edc\u81ea\u52a8\u5316\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "ComAgent\u6846\u67b6\u6709\u6548\u5f25\u5408\u4e86\u7528\u6237\u610f\u56fe\u4e0e\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u65b0\u5174\u65e0\u7ebf\u7f51\u7edc\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.19578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19578", "abs": "https://arxiv.org/abs/2601.19578", "authors": ["Yuxuan Cai", "Xinyi Lai", "Peng Yuan", "Weiting Liu", "Huajian Li", "Mingda Li", "Xinghua Wang", "Shengxie Zheng", "Yanchao Hao", "Yuyang Yin", "Zheng Wei"], "title": "Yunque DeepResearch Technical Report", "comment": null, "summary": "Deep research has emerged as a transformative capability for autonomous agents, empowering Large Language Models to navigate complex, open-ended tasks. However, realizing its full potential is hindered by critical limitations, including escalating contextual noise in long-horizon tasks, fragility leading to cascading errors, and a lack of modular extensibility. To address these challenges, we introduce Yunque DeepResearch, a hierarchical, modular, and robust framework. The architecture is characterized by three key components: (1) a centralized Multi-Agent Orchestration System that routes subtasks to an Atomic Capability Pool of tools and specialized sub-agents; (2) a Dynamic Context Management mechanism that structures completed sub-goals into semantic summaries to mitigate information overload; and (3) a proactive Supervisor Module that ensures resilience through active anomaly detection and context pruning. Yunque DeepResearch achieves state-of-the-art performance across a range of agentic deep research benchmarks, including GAIA, BrowseComp, BrowseComp-ZH, and Humanity's Last Exam. We open-source the framework, reproducible implementations, and application cases to empower the community.", "AI": {"tldr": "Yunque DeepResearch\u662f\u4e00\u4e2a\u5206\u5c42\u3001\u6a21\u5757\u5316\u3001\u9c81\u68d2\u7684\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7f16\u6392\u3001\u52a8\u6001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u4e3b\u52a8\u76d1\u7763\u6a21\u5757\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u4e2d\u7684\u4e0a\u4e0b\u6587\u566a\u58f0\u3001\u8106\u5f31\u6027\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u6df1\u5ea6\u7814\u7a76\u80fd\u529b\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u957f\u65f6\u4efb\u52a1\u4e2d\u4e0a\u4e0b\u6587\u566a\u58f0\u4e0d\u65ad\u5347\u7ea7\u3001\u8106\u5f31\u6027\u5bfc\u81f4\u7ea7\u8054\u9519\u8bef\u3001\u7f3a\u4e4f\u6a21\u5757\u5316\u6269\u5c55\u6027\uff0c\u963b\u788d\u4e86\u6df1\u5ea6\u7814\u7a76\u7684\u5168\u90e8\u6f5c\u529b\u53d1\u6325\u3002", "method": "\u63d0\u51faYunque DeepResearch\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u96c6\u4e2d\u5f0f\u591a\u667a\u80fd\u4f53\u7f16\u6392\u7cfb\u7edf\uff0c\u5c06\u5b50\u4efb\u52a1\u8def\u7531\u5230\u539f\u5b50\u80fd\u529b\u6c60\uff1b2) \u52a8\u6001\u4e0a\u4e0b\u6587\u7ba1\u7406\u673a\u5236\uff0c\u5c06\u5b8c\u6210\u7684\u5b50\u76ee\u6807\u7ed3\u6784\u5316\u8bed\u4e49\u6458\u8981\uff1b3) \u4e3b\u52a8\u76d1\u7763\u6a21\u5757\uff0c\u901a\u8fc7\u5f02\u5e38\u68c0\u6d4b\u548c\u4e0a\u4e0b\u6587\u526a\u679d\u786e\u4fdd\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u667a\u80fd\u4f53\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5305\u62ecGAIA\u3001BrowseComp\u3001BrowseComp-ZH\u548cHumanity's Last Exam\u3002", "conclusion": "Yunque DeepResearch\u901a\u8fc7\u5206\u5c42\u3001\u6a21\u5757\u5316\u548c\u9c81\u68d2\u7684\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u7814\u7a76\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u6846\u67b6\u5df2\u5f00\u6e90\u4ee5\u8d4b\u80fd\u793e\u533a\u3002", "topic": "agent analysis"}}
{"id": "2601.19030", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.19030", "abs": "https://arxiv.org/abs/2601.19030", "authors": ["Philip Amortila", "Audrey Huang", "Akshay Krishnamurthy", "Nan Jiang"], "title": "A Unifying View of Coverage in Linear Off-Policy Evaluation", "comment": "To appear at ICLR 2026", "summary": "Off-policy evaluation (OPE) is a fundamental task in reinforcement learning (RL). In the classic setting of linear OPE, finite-sample guarantees often take the form $$ \\textrm{Evaluation error} \\le \\textrm{poly}(C^\u03c0, d, 1/n,\\log(1/\u03b4)), $$ where $d$ is the dimension of the features and $C^\u03c0$ is a coverage parameter that characterizes the degree to which the visited features lie in the span of the data distribution. While such guarantees are well-understood for several popular algorithms under stronger assumptions (e.g. Bellman completeness), the understanding is lacking and fragmented in the minimal setting where only the target value function is linearly realizable in the features. Despite recent interest in tight characterizations of the statistical rate in this setting, the right notion of coverage remains unclear, and candidate definitions from prior analyses have undesirable properties and are starkly disconnected from more standard definitions in the literature.\n  We provide a novel finite-sample analysis of a canonical algorithm for this setting, LSTDQ. Inspired by an instrumental-variable view, we develop error bounds that depend on a novel coverage parameter, the feature-dynamics coverage, which can be interpreted as linear coverage in an induced dynamical system for feature evolution. With further assumptions -- such as Bellman-completeness -- our definition successfully recovers the coverage parameters specialized to those settings, finally yielding a unified understanding for coverage in linear OPE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u79bb\u7b56\u7565\u8bc4\u4f30\uff08OPE\uff09\u6709\u9650\u6837\u672c\u5206\u6790\uff0c\u5f15\u5165\u7279\u5f81\u52a8\u6001\u8986\u76d6\u8fd9\u4e00\u65b0\u8986\u76d6\u53c2\u6570\uff0c\u4e3a\u7ebf\u6027OPE\u4e2d\u7684\u8986\u76d6\u6982\u5ff5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7406\u89e3\u3002", "motivation": "\u7ebf\u6027\u79bb\u7b56\u7565\u8bc4\u4f30\u4e2d\uff0c\u4f20\u7edf\u6709\u9650\u6837\u672c\u4fdd\u8bc1\u4f9d\u8d56\u4e8e\u8986\u76d6\u53c2\u6570C^\u03c0\uff0c\u4f46\u5728\u4ec5\u76ee\u6807\u503c\u51fd\u6570\u7ebf\u6027\u53ef\u5b9e\u73b0\u7684\u7b80\u7ea6\u8bbe\u7f6e\u4e0b\uff0c\u73b0\u6709\u8986\u76d6\u5b9a\u4e49\u5b58\u5728\u7f3a\u9677\u4e14\u4e0e\u6587\u732e\u4e2d\u7684\u6807\u51c6\u5b9a\u4e49\u8131\u8282\uff0c\u9700\u8981\u7edf\u4e00\u7684\u8986\u76d6\u6982\u5ff5\u3002", "method": "\u91c7\u7528\u5de5\u5177\u53d8\u91cf\u89c6\u89d2\uff0c\u5bf9\u7ecf\u5178\u7b97\u6cd5LSTDQ\u8fdb\u884c\u65b0\u9896\u7684\u6709\u9650\u6837\u672c\u5206\u6790\uff0c\u63d0\u51fa\u7279\u5f81\u52a8\u6001\u8986\u76d6\u53c2\u6570\uff0c\u8be5\u53c2\u6570\u53ef\u89e3\u91ca\u4e3a\u7279\u5f81\u6f14\u5316\u8bf1\u5bfc\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u7ebf\u6027\u8986\u76d6\u3002", "result": "\u5f00\u53d1\u4e86\u4f9d\u8d56\u4e8e\u65b0\u8986\u76d6\u53c2\u6570\u7684\u9519\u8bef\u8fb9\u754c\uff0c\u5728\u8d1d\u5c14\u66fc\u5b8c\u5907\u6027\u7b49\u8fdb\u4e00\u6b65\u5047\u8bbe\u4e0b\uff0c\u8be5\u5b9a\u4e49\u6210\u529f\u6062\u590d\u4e86\u7279\u5b9a\u8bbe\u7f6e\u4e0b\u7684\u8986\u76d6\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027OPE\u4e2d\u8986\u76d6\u6982\u5ff5\u7684\u7edf\u4e00\u3002", "conclusion": "\u7279\u5f81\u52a8\u6001\u8986\u76d6\u4e3a\u7ebf\u6027\u79bb\u7b56\u7565\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8986\u76d6\u6982\u5ff5\uff0c\u586b\u8865\u4e86\u7b80\u7ea6\u8bbe\u7f6e\u4e0b\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u8fde\u63a5\u4e86\u4e0d\u540c\u5047\u8bbe\u4e0b\u7684\u8986\u76d6\u5b9a\u4e49\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.19752", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19752", "abs": "https://arxiv.org/abs/2601.19752", "authors": ["Minh-Dung Dao", "Quy Minh Le", "Hoang Thanh Lam", "Duc-Trong Le", "Quoc-Viet Pham", "Barry O'Sullivan", "Hoang D. Nguyen"], "title": "Agentic Design Patterns: A System-Theoretic Framework", "comment": null, "summary": "With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or convenience-based taxonomies that are difficult to implement. This paper addresses this gap by introducing a principled methodology for engineering robust AI agents. We propose two primary contributions: first, a novel system-theoretic framework that deconstructs an agentic AI system into five core, interacting functional subsystems: Reasoning & World Model, Perception & Grounding, Action Execution, Learning & Adaptation, and Inter-Agent Communication. Second, derived from this architecture and directly mapped to a comprehensive taxonomy of agentic challenges, we present a collection of 12 agentic design patterns. These patterns - categorised as Foundational, Cognitive & Decisional, Execution & Interaction, and Adaptive & Learning - offer reusable, structural solutions to recurring problems in agent design. The utility of the framework is demonstrated by a case study on the ReAct framework, showing how the proposed patterns can rectify systemic architectural deficiencies. This work provides a foundational language and a structured methodology to standardise agentic design communication among researchers and engineers, leading to more modular, understandable, and reliable autonomous systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7cfb\u7edf\u7406\u8bba\u7684AI\u667a\u80fd\u4f53\u5de5\u7a0b\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u4e94\u529f\u80fd\u5b50\u7cfb\u7edf\u6846\u67b6\u548c12\u79cd\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u7528\u4e8e\u6784\u5efa\u66f4\u53ef\u9760\u3001\u6a21\u5757\u5316\u7684\u81ea\u4e3b\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u5e7b\u89c9\u3001\u63a8\u7406\u80fd\u529b\u5dee\u7b49\u95ee\u9898\uff0c\u4e14\u8bbe\u8ba1\u591a\u4e3a\u4e34\u65f6\u6027\u65b9\u6848\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u7406\u8bba\u57fa\u7840\u3002\u73b0\u6709\u8bbe\u8ba1\u6a21\u5f0f\u5206\u7c7b\u5f80\u5f80\u505c\u7559\u5728\u9ad8\u5c42\u6b21\u6216\u4fbf\u5229\u6027\u5c42\u9762\uff0c\u96be\u4ee5\u5b9e\u9645\u5b9e\u65bd\u3002", "method": "\u63d0\u51fa\u7cfb\u7edf\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u7cfb\u7edf\u89e3\u6784\u4e3a\u4e94\u4e2a\u6838\u5fc3\u4ea4\u4e92\u5b50\u7cfb\u7edf\uff1a\u63a8\u7406\u4e0e\u4e16\u754c\u6a21\u578b\u3001\u611f\u77e5\u4e0e\u63a5\u5730\u3001\u52a8\u4f5c\u6267\u884c\u3001\u5b66\u4e60\u4e0e\u9002\u5e94\u3001\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u3002\u57fa\u4e8e\u6b64\u67b6\u6784\uff0c\u6620\u5c04\u51fa12\u79cd\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5206\u4e3a\u57fa\u7840\u578b\u3001\u8ba4\u77e5\u4e0e\u51b3\u7b56\u578b\u3001\u6267\u884c\u4e0e\u4ea4\u4e92\u578b\u3001\u9002\u5e94\u4e0e\u5b66\u4e60\u578b\u56db\u7c7b\u3002", "result": "\u901a\u8fc7\u5bf9ReAct\u6846\u67b6\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u8bbe\u8ba1\u6a21\u5f0f\u5982\u4f55\u7ea0\u6b63\u7cfb\u7edf\u6027\u67b6\u6784\u7f3a\u9677\u3002\u8be5\u6846\u67b6\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u8bbe\u8ba1\u8bed\u8a00\u548c\u65b9\u6cd5\u8bba\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u667a\u80fd\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u7840\u8bed\u8a00\u548c\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u4fc3\u8fdb\u66f4\u6a21\u5757\u5316\u3001\u53ef\u7406\u89e3\u548c\u53ef\u9760\u7684\u81ea\u4e3b\u7cfb\u7edf\u5f00\u53d1\uff0c\u6807\u51c6\u5316\u4e86\u667a\u80fd\u4f53\u8bbe\u8ba1\u4ea4\u6d41\u3002", "topic": "agent analysis"}}
{"id": "2601.19055", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.19055", "abs": "https://arxiv.org/abs/2601.19055", "authors": ["Dipendra Misra", "Aldo Pacchiano", "Ta-Chung Chi", "Ge Gao"], "title": "Principled Fine-tuning of LLMs from User-Edits: A Medley of Preference, Supervision, and Reward", "comment": "Accepted at NeurIPS 2025", "summary": "We study how to fine-tune LLMs using user-edit deployment data consisting of a set of context, an agent's response, and user edits. This deployment data is naturally generated by users in applications such as LLMs-based writing assistants and coding agents. The _natural_ origin of user edits makes it a desired source for adapting and personalizing LLMs. In this setup, there emerges a unification of various feedback types namely preferences, supervised labels, and cost that are typically studied separately in the literature. In this paper, we initiate the theoretical investigation of learning from user edits. We first derive bounds for learning algorithms that learn from each of these feedback types. We prove that these algorithms have different trade-offs depending upon the user, data distribution, and model class. We then propose a simple ensembling procedure to jointly learn from these feedback types. On two domains adapted from Gao et al. 2024, we show our ensembling procedure outperforms these methods that learn from individual feedback. Further, we show that our proposed procedure can robustly adapt to different user-edit distributions at test time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u5229\u7528\u7528\u6237\u7f16\u8f91\u90e8\u7f72\u6570\u636e\uff08\u4e0a\u4e0b\u6587\u3001\u4ee3\u7406\u54cd\u5e94\u3001\u7528\u6237\u7f16\u8f91\uff09\u6765\u5fae\u8c03LLMs\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u7edf\u4e00\u5904\u7406\u504f\u597d\u3001\u76d1\u7763\u6807\u7b7e\u548c\u6210\u672c\u7b49\u4e0d\u540c\u7c7b\u578b\u7684\u53cd\u9988\u3002", "motivation": "\u7528\u6237\u7f16\u8f91\u6570\u636e\u662fLLM\u5e94\u7528\uff08\u5982\u5199\u4f5c\u52a9\u624b\u548c\u4ee3\u7801\u4ee3\u7406\uff09\u4e2d\u81ea\u7136\u4ea7\u751f\u7684\uff0c\u53ef\u7528\u4e8e\u9002\u5e94\u548c\u4e2a\u6027\u5316LLMs\u3002\u4e0d\u540c\u53cd\u9988\u7c7b\u578b\uff08\u504f\u597d\u3001\u76d1\u7763\u6807\u7b7e\u3001\u6210\u672c\uff09\u5728\u6587\u732e\u4e2d\u901a\u5e38\u5206\u5f00\u7814\u7a76\uff0c\u800c\u7528\u6237\u7f16\u8f91\u6570\u636e\u4e3a\u7edf\u4e00\u8fd9\u4e9b\u53cd\u9988\u7c7b\u578b\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u4e86\u4ece\u4e0d\u540c\u53cd\u9988\u7c7b\u578b\u5b66\u4e60\u7684\u7b97\u6cd5\u8fb9\u754c\uff0c\u8bc1\u660e\u8fd9\u4e9b\u7b97\u6cd5\u5728\u7528\u6237\u3001\u6570\u636e\u5206\u5e03\u548c\u6a21\u578b\u7c7b\u522b\u65b9\u9762\u6709\u4e0d\u540c\u7684\u6743\u8861\u3002\u7136\u540e\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7684\u96c6\u6210\u7a0b\u5e8f\uff0c\u8054\u5408\u5b66\u4e60\u8fd9\u4e9b\u53cd\u9988\u7c7b\u578b\u3002", "result": "\u5728\u57fa\u4e8eGao\u7b49\u4eba2024\u5e74\u5de5\u4f5c\u7684\u4e24\u4e2a\u9886\u57df\u4e0a\uff0c\u96c6\u6210\u7a0b\u5e8f\u4f18\u4e8e\u4ece\u5355\u4e2a\u53cd\u9988\u5b66\u4e60\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u9c81\u68d2\u5730\u9002\u5e94\u4e0d\u540c\u7684\u7528\u6237\u7f16\u8f91\u5206\u5e03\u3002", "conclusion": "\u7528\u6237\u7f16\u8f91\u6570\u636e\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u81ea\u7136\u53cd\u9988\u6765\u6e90\uff0c\u63d0\u51fa\u7684\u96c6\u6210\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7edf\u4e00\u4e0d\u540c\u7c7b\u578b\u7684\u53cd\u9988\uff0c\u5e76\u5728\u4e0d\u540c\u7528\u6237\u7f16\u8f91\u5206\u5e03\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.19773", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19773", "abs": "https://arxiv.org/abs/2601.19773", "authors": ["Zhuohan Long", "Zhijie Bao", "Zhongyu Wei"], "title": "Strong Reasoning Isn't Enough: Evaluating Evidence Elicitation in Interactive Diagnosis", "comment": null, "summary": "Interactive medical consultation requires an agent to proactively elicit missing clinical evidence under uncertainty. Yet existing evaluations largely remain static or outcome-centric, neglecting the evidence-gathering process. In this work, we propose an interactive evaluation framework that explicitly models the consultation process using a simulated patient and a \\rev{simulated reporter} grounded in atomic evidences. Based on this representation, we introduce Information Coverage Rate (ICR) to quantify how completely an agent uncovers necessary evidence during interaction. To support systematic study, we build EviMed, an evidence-based benchmark spanning diverse conditions from common complaints to rare diseases, and evaluate 10 models with varying reasoning abilities. We find that strong diagnostic reasoning does not guarantee effective information collection, and this insufficiency acts as a primary bottleneck limiting performance in interactive settings. To address this, we propose REFINE, a strategy that leverages diagnostic verification to guide the agent in proactively resolving uncertainties. Extensive experiments demonstrate that REFINE consistently outperforms baselines across diverse datasets and facilitates effective model collaboration, enabling smaller agents to achieve superior performance under strong reasoning supervision. Our code can be found at https://github.com/NanshineLoong/EID-Benchmark .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u533b\u7597\u54a8\u8be2\u8bc4\u4f30\u6846\u67b6EviMed\uff0c\u901a\u8fc7\u6a21\u62df\u60a3\u8005\u548c\u62a5\u544a\u5458\u6765\u5efa\u6a21\u54a8\u8be2\u8fc7\u7a0b\uff0c\u5f15\u5165\u4fe1\u606f\u8986\u76d6\u7387(ICR)\u91cf\u5316\u8bc1\u636e\u6536\u96c6\u5b8c\u6574\u6027\uff0c\u53d1\u73b0\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u4e0d\u4e00\u5b9a\u80fd\u6709\u6548\u6536\u96c6\u4fe1\u606f\uff0c\u5e76\u63d0\u51fa\u4e86REFINE\u7b56\u7565\u6765\u5f15\u5bfc\u4e3b\u52a8\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u54a8\u8be2\u8bc4\u4f30\u5927\u591a\u662f\u9759\u6001\u6216\u7ed3\u679c\u5bfc\u5411\u7684\uff0c\u5ffd\u89c6\u4e86\u8bc1\u636e\u6536\u96c6\u8fc7\u7a0b\u3002\u4ea4\u4e92\u5f0f\u533b\u7597\u54a8\u8be2\u9700\u8981\u667a\u80fd\u4f53\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u4e3b\u52a8\u83b7\u53d6\u7f3a\u5931\u7684\u4e34\u5e8a\u8bc1\u636e\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8bc1\u636e\u6536\u96c6\u8fc7\u7a0b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "1) \u6784\u5efa\u57fa\u4e8e\u8bc1\u636e\u7684\u57fa\u51c6EviMed\uff0c\u6db5\u76d6\u4ece\u5e38\u89c1\u75c7\u72b6\u5230\u7f55\u89c1\u75be\u75c5\u7684\u591a\u79cd\u60c5\u51b5\uff1b2) \u63d0\u51fa\u4ea4\u4e92\u5f0f\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u6a21\u62df\u60a3\u8005\u548c\u6a21\u62df\u62a5\u544a\u5458\u6765\u5efa\u6a21\u54a8\u8be2\u8fc7\u7a0b\uff1b3) \u5f15\u5165\u4fe1\u606f\u8986\u76d6\u7387(ICR)\u91cf\u5316\u8bc1\u636e\u6536\u96c6\u5b8c\u6574\u6027\uff1b4) \u63d0\u51faREFINE\u7b56\u7565\uff0c\u5229\u7528\u8bca\u65ad\u9a8c\u8bc1\u5f15\u5bfc\u667a\u80fd\u4f53\u4e3b\u52a8\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u8bc4\u4f30\u4e8610\u4e2a\u5177\u6709\u4e0d\u540c\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\uff0c\u53d1\u73b0\u5f3a\u5927\u7684\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u6709\u6548\u7684\u4fe1\u606f\u6536\u96c6\uff0c\u8fd9\u79cd\u4e0d\u8db3\u662f\u9650\u5236\u4ea4\u4e92\u5f0f\u8bbe\u7f6e\u6027\u80fd\u7684\u4e3b\u8981\u74f6\u9888\u3002REFINE\u7b56\u7565\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u4fc3\u8fdb\u4e86\u6709\u6548\u7684\u6a21\u578b\u534f\u4f5c\uff0c\u4f7f\u8f83\u5c0f\u7684\u667a\u80fd\u4f53\u5728\u5f3a\u63a8\u7406\u76d1\u7763\u4e0b\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u533b\u7597\u54a8\u8be2\u9700\u8981\u4e13\u95e8\u7684\u8bc1\u636e\u6536\u96c6\u80fd\u529b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u3002REFINE\u7b56\u7565\u901a\u8fc7\u8bca\u65ad\u9a8c\u8bc1\u5f15\u5bfc\u4e3b\u52a8\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u5f0f\u54a8\u8be2\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u6a21\u578b\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.19139", "categories": ["cs.LG", "cs.DC", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.19139", "abs": "https://arxiv.org/abs/2601.19139", "authors": ["Wayner Barrios"], "title": "Native LLM and MLLM Inference at Scale on Apple Silicon", "comment": null, "summary": "The growing adoption of Apple Silicon for machine learning development has created demand for efficient inference solutions that leverage its unique unified memory architecture. However, existing tools either lack native optimization (PyTorch MPS) or focus solely on text models (llama.cpp), leaving multimodal workloads underserved. We present vllm-mlx, a framework for efficient LLM and MLLM inference on Apple Silicon built natively on MLX. For text models, we achieve 21% to 87% higher throughput than llama.cpp across models ranging from Qwen3-0.6B to Nemotron-30B, while providing continuous batching that scales to 4.3x aggregate throughput at 16 concurrent requests. For multimodal models, we introduce content-based prefix caching that eliminates redundant vision encoding by identifying identical images through content hashing, regardless of input format. Our evaluation on Apple M4 Max demonstrates throughput of up to 525 tokens per second on text models and 28x speedup on repeated image queries, reducing multimodal latency from 21.7 seconds to under 1 second. Video analysis with up to 64 frames achieves 24.7x cache speedup. We release our implementation as open source to support efficient inference on consumer Apple hardware.", "AI": {"tldr": "vllm-mlx\uff1a\u4e13\u4e3aApple Silicon\u4f18\u5316\u7684\u9ad8\u6548LLM\u548cMLLM\u63a8\u7406\u6846\u67b6\uff0c\u57fa\u4e8eMLX\u539f\u751f\u6784\u5efa\uff0c\u63d0\u4f9b\u8fde\u7eed\u6279\u5904\u7406\u548c\u5185\u5bb9\u611f\u77e5\u524d\u7f00\u7f13\u5b58\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u548c\u591a\u6a21\u6001\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u968f\u7740Apple Silicon\u5728\u673a\u5668\u5b66\u4e60\u5f00\u53d1\u4e2d\u7684\u666e\u53ca\uff0c\u73b0\u6709\u5de5\u5177\u8981\u4e48\u7f3a\u4e4f\u539f\u751f\u4f18\u5316\uff08\u5982PyTorch MPS\uff09\uff0c\u8981\u4e48\u4ec5\u4e13\u6ce8\u4e8e\u6587\u672c\u6a21\u578b\uff08\u5982llama.cpp\uff09\uff0c\u65e0\u6cd5\u6ee1\u8db3\u591a\u6a21\u6001\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u4e3aApple Silicon\u7edf\u4e00\u5185\u5b58\u67b6\u6784\u4f18\u5316\u7684\u9ad8\u6548\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eMLX\u539f\u751f\u6784\u5efa\u63a8\u7406\u6846\u67b6\uff0c\u9488\u5bf9\u6587\u672c\u6a21\u578b\u5b9e\u73b0\u8fde\u7eed\u6279\u5904\u7406\u6280\u672f\uff0c\u652f\u630116\u4e2a\u5e76\u53d1\u8bf7\u6c42\uff1b\u9488\u5bf9\u591a\u6a21\u6001\u6a21\u578b\u5f15\u5165\u57fa\u4e8e\u5185\u5bb9\u54c8\u5e0c\u7684\u5185\u5bb9\u611f\u77e5\u524d\u7f00\u7f13\u5b58\uff0c\u53ef\u8bc6\u522b\u76f8\u540c\u56fe\u50cf\u5185\u5bb9\u800c\u5ffd\u7565\u8f93\u5165\u683c\u5f0f\u5dee\u5f02\uff0c\u6d88\u9664\u5197\u4f59\u89c6\u89c9\u7f16\u7801\u3002", "result": "\u5728Apple M4 Max\u4e0a\uff0c\u6587\u672c\u6a21\u578b\u63a8\u7406\u541e\u5410\u91cf\u6bd4llama.cpp\u63d0\u9ad821%-87%\uff08Qwen3-0.6B\u5230Nemotron-30B\uff09\uff0c16\u5e76\u53d1\u8bf7\u6c42\u65f6\u805a\u5408\u541e\u5410\u91cf\u63d0\u53474.3\u500d\uff1b\u591a\u6a21\u6001\u6a21\u578b\u91cd\u590d\u56fe\u50cf\u67e5\u8be2\u901f\u5ea6\u63d0\u534728\u500d\uff0c\u5ef6\u8fdf\u4ece21.7\u79d2\u964d\u81f31\u79d2\u4ee5\u4e0b\uff1b64\u5e27\u89c6\u9891\u5206\u6790\u7f13\u5b58\u52a0\u901f24.7\u500d\u3002", "conclusion": "vllm-mlx\u4e3aApple Silicon\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684LLM\u548cMLLM\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u539f\u751fMLX\u4f18\u5316\u3001\u8fde\u7eed\u6279\u5904\u7406\u548c\u5185\u5bb9\u611f\u77e5\u7f13\u5b58\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d88\u8d39\u7ea7Apple\u786c\u4ef6\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u5df2\u5f00\u6e90\u53d1\u5e03\u3002", "topic": "code agent"}}
{"id": "2601.19280", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19280", "abs": "https://arxiv.org/abs/2601.19280", "authors": ["Kishan Panaganti", "Zhenwen Liang", "Wenhao Yu", "Haitao Mi", "Dong Yu"], "title": "Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning", "comment": "Keywords: Large Language Models, Reasoning Models, Reinforcement Learning, Distributionally Robust Optimization, GRPO", "summary": "Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: uniform prompt sampling and a fixed number of rollouts per prompt. For heterogeneous, heavy-tailed reasoning data, this creates structural inefficiencies that waste compute on already-solved patterns while under-training the long tail of hard problems. To address this, we propose Multi-Adversary Group Distributionally Robust Optimization (GDRO), an optimization-first framework that moves beyond uniform reasoning models by dynamically adapting the training distribution.\n  We introduce an Online Difficulty Classifier that partitions prompts into dynamic pass@k difficulty groups. We then propose two independent GDRO games for post-training: (1) Prompt-GDRO, which employs an EMA-debiased multiplicative-weights bandit sampler to target the intensive difficulty margin and upweight persistently hard groups without frequency bias; and (2) Rollout-GDRO, which uses a shadow-price controller to reallocate rollouts across groups, maximizing gradient variance reduction on hard tasks under a fixed mean budget (compute-neutral). We provide no-regret guarantees for both controllers and additionally a variance-proxy analysis motivating a square-root optimal rollout allocation for Rollout-GDRO. We validate our framework on the DAPO 14.1k dataset using Qwen3-Base models. Prompt-GDRO and Rollout-GDRO achieve average relative gains of +10.6% and +10.1%, respectively, in pass@8 accuracy across 1.7B, 4B, and 8B scales compared to the GRPO baseline. Qualitative analysis shows an emergent curriculum: the adversaries shift resources to the evolving reasoning frontier, enhancing the reasoning model's performance.", "AI": {"tldr": "\u63d0\u51faMulti-Adversary Group Distributionally Robust Optimization (GDRO)\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u96be\u5ea6\u5206\u7c7b\u548c\u5bf9\u6297\u6027\u8d44\u6e90\u5206\u914d\uff0c\u4f18\u5316LLM\u63a8\u7406\u8bad\u7ec3\u6548\u7387\uff0c\u76f8\u6bd4GRPO\u57fa\u7ebf\u5728pass@8\u51c6\u786e\u7387\u4e0a\u63d0\u5347\u7ea610%\u3002", "motivation": "\u6807\u51c6RL\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5728\u5f02\u6784\u63a8\u7406\u6570\u636e\u4e0a\u5b58\u5728\u7ed3\u6784\u6027\u4f4e\u6548\uff1a\u5747\u5300\u91c7\u6837\u548c\u56fa\u5b9arollout\u6570\u91cf\u5bfc\u81f4\u5bf9\u5df2\u89e3\u51b3\u6a21\u5f0f\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u5bf9\u56f0\u96be\u95ee\u9898\u7684\u957f\u5c3e\u5206\u5e03\u8bad\u7ec3\u4e0d\u8db3\u3002", "method": "\u63d0\u51faGDRO\u6846\u67b6\uff0c\u5305\u542b\uff1a1) Online Difficulty Classifier\u52a8\u6001\u5212\u5206\u63d0\u793a\u96be\u5ea6\u7ec4\uff1b2) Prompt-GDRO\u4f7f\u7528EMA\u53bb\u504f\u4e58\u6027\u6743\u91cdbandit\u91c7\u6837\u5668\uff0c\u9488\u5bf9\u56f0\u96be\u8fb9\u7f18\u7ec4\uff1b3) Rollout-GDRO\u4f7f\u7528\u5f71\u5b50\u4ef7\u683c\u63a7\u5236\u5668\u5728\u7ec4\u95f4\u91cd\u65b0\u5206\u914drollout\uff0c\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u6700\u5927\u5316\u68af\u5ea6\u65b9\u5dee\u51cf\u5c11\u3002", "result": "\u5728DAPO 14.1k\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Qwen3-Base\u6a21\u578b\u9a8c\u8bc1\uff0cPrompt-GDRO\u548cRollout-GDRO\u5206\u522b\u57281.7B\u30014B\u548c8B\u89c4\u6a21\u4e0a\u76f8\u6bd4GRPO\u57fa\u7ebf\u5e73\u5747\u63d0\u5347pass@8\u51c6\u786e\u738710.6%\u548c10.1%\u3002", "conclusion": "GDRO\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u96be\u5ea6\u9002\u5e94\u548c\u5bf9\u6297\u6027\u8d44\u6e90\u5206\u914d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u63a8\u7406\u6570\u636e\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65b0\u5174\u8bfe\u7a0b\u5b66\u4e60\u6548\u679c\uff0c\u5c06\u8d44\u6e90\u8f6c\u79fb\u5230\u4e0d\u65ad\u6f14\u5316\u7684\u63a8\u7406\u524d\u6cbf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.19232", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19232", "abs": "https://arxiv.org/abs/2601.19232", "authors": ["Qi Si", "Xuyang Liu", "Penglei Wang", "Xin Guo", "Yuan Qi", "Yuan Cheng"], "title": "Structure-based RNA Design by Step-wise Optimization of Latent Diffusion Model", "comment": "20 pages (7 pages content + 2 pages references + 11 pages appendix), 11 figures, 8 tables. Source code available at https://github.com/darkflash03/SOLD Accepted to AAAI 2026", "summary": "RNA inverse folding, designing sequences to form specific 3D structures, is critical for therapeutics, gene regulation, and synthetic biology. Current methods, focused on sequence recovery, struggle to address structural objectives like secondary structure consistency (SS), minimum free energy (MFE), and local distance difference test (LDDT), leading to suboptimal structural accuracy. To tackle this, we propose a reinforcement learning (RL) framework integrated with a latent diffusion model (LDM). Drawing inspiration from the success of diffusion models in RNA inverse folding, which adeptly model complex sequence-structure interactions, we develop an LDM incorporating pre-trained RNA-FM embeddings from a large-scale RNA model. These embeddings capture co-evolutionary patterns, markedly improving sequence recovery accuracy. However, existing approaches, including diffusion-based methods, cannot effectively handle non-differentiable structural objectives. By contrast, RL excels in this task by using policy-driven reward optimization to navigate complex, non-gradient-based objectives, offering a significant advantage over traditional methods. In summary, we propose the Step-wise Optimization of Latent Diffusion Model (SOLD), a novel RL framework that optimizes single-step noise without sampling the full diffusion trajectory, achieving efficient refinement of multiple structural objectives. Experimental results demonstrate SOLD surpasses its LDM baseline and state-of-the-art methods across all metrics, establishing a robust framework for RNA inverse folding with profound implications for biotechnological and therapeutic applications.", "AI": {"tldr": "SOLD\uff1a\u4e00\u79cd\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684RNA\u9006\u6298\u53e0\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b65\u566a\u58f0\u4f18\u5316\u5b9e\u73b0\u591a\u7ed3\u6784\u76ee\u6807\u7684\u9ad8\u6548\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u7ed3\u6784\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dRNA\u9006\u6298\u53e0\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e8f\u5217\u6062\u590d\uff0c\u96be\u4ee5\u5904\u7406\u975e\u53ef\u5fae\u7684\u7ed3\u6784\u76ee\u6807\uff08\u5982\u4e8c\u7ea7\u7ed3\u6784\u4e00\u81f4\u6027\u3001\u6700\u5c0f\u81ea\u7531\u80fd\u3001\u5c40\u90e8\u8ddd\u79bb\u5dee\u5f02\u6d4b\u8bd5\uff09\uff0c\u5bfc\u81f4\u7ed3\u6784\u51c6\u786e\u6027\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u4f18\u5316\u591a\u79cd\u7ed3\u6784\u76ee\u6807\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSOLD\u6846\u67b6\uff1a\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u3002LDM\u4f7f\u7528\u9884\u8bad\u7ec3\u7684RNA-FM\u5d4c\u5165\u6355\u83b7\u5171\u8fdb\u5316\u6a21\u5f0f\uff1bRL\u901a\u8fc7\u7b56\u7565\u9a71\u52a8\u7684\u5956\u52b1\u4f18\u5316\u5904\u7406\u975e\u53ef\u5fae\u7ed3\u6784\u76ee\u6807\uff0c\u91c7\u7528\u5355\u6b65\u566a\u58f0\u4f18\u5316\u800c\u975e\u5b8c\u6574\u6269\u6563\u8f68\u8ff9\u91c7\u6837\u3002", "result": "SOLD\u5728\u5404\u9879\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u5176LDM\u57fa\u7ebf\u548c\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86RNA\u9006\u6298\u53e0\u7684\u7ed3\u6784\u51c6\u786e\u6027\u3002", "conclusion": "SOLD\u4e3aRNA\u9006\u6298\u53e0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u4f18\u5316\u591a\u79cd\u7ed3\u6784\u76ee\u6807\uff0c\u5bf9\u751f\u7269\u6280\u672f\u548c\u6cbb\u7597\u5e94\u7528\u5177\u6709\u6df1\u8fdc\u610f\u4e49\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.19336", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19336", "abs": "https://arxiv.org/abs/2601.19336", "authors": ["Zhao-Han Peng", "Shaohui Li", "Zhi Li", "Shulan Ruan", "Yu Liu", "You He"], "title": "From Observations to Events: Event-Aware World Model for Reinforcement Learning", "comment": "43 pages, accepted by ICLR 2026", "summary": "While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.", "AI": {"tldr": "EAWM\u63d0\u51fa\u4e8b\u4ef6\u611f\u77e5\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u4e8b\u4ef6\u751f\u6210\u548c\u5206\u5272\u6765\u5b66\u4e60\u4e8b\u4ef6\u611f\u77e5\u8868\u5f81\uff0c\u63d0\u5347\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7ed3\u6784\u76f8\u4f3c\u573a\u666f\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5bb9\u6613\u53d7\u5230\u7eb9\u7406\u3001\u989c\u8272\u7b49\u865a\u5047\u53d8\u5316\u7684\u5f71\u54cd\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\uff0c\u4eba\u7c7b\u5c06\u8fde\u7eed\u611f\u5b98\u6d41\u5206\u5272\u4e3a\u79bb\u6563\u4e8b\u4ef6\u5e76\u4f9d\u8d56\u5173\u952e\u4e8b\u4ef6\u8fdb\u884c\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u611f\u77e5\u4e16\u754c\u6a21\u578b(EAWM)\u6846\u67b6\uff0c\u5305\u542b\u81ea\u52a8\u4e8b\u4ef6\u751f\u6210\u5668\u548c\u901a\u7528\u4e8b\u4ef6\u5206\u5272\u5668(GES)\u6765\u4ece\u539f\u59cb\u89c2\u6d4b\u4e2d\u63d0\u53d6\u4e8b\u4ef6\u5e76\u8bc6\u522b\u4e8b\u4ef6\u8fb9\u754c\u3002\u901a\u8fc7\u4e8b\u4ef6\u9884\u6d4b\u6765\u5851\u9020\u8868\u5f81\u7a7a\u95f4\u4ee5\u6355\u6349\u6709\u610f\u4e49\u7684\u65f6\u7a7a\u8f6c\u6362\uff0c\u5e76\u63d0\u4f9b\u7edf\u4e00\u7684\u4e16\u754c\u6a21\u578b\u67b6\u6784\u8868\u8ff0\u3002", "result": "\u5728Atari 100K\u3001Craftax 1M\u3001DeepMind Control 500K\u548cDMC-GB2 500K\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEAWM\u5c06\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u63d0\u534710%-45%\uff0c\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "EAWM\u901a\u8fc7\u4e8b\u4ef6\u611f\u77e5\u8868\u5f81\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff0c\u4e3a\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.19375", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19375", "abs": "https://arxiv.org/abs/2601.19375", "authors": ["Quy-Anh Dang", "Chris Ngo"], "title": "Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection", "comment": null, "summary": "Despite significant progress in alignment, large language models (LLMs) remain vulnerable to adversarial attacks that elicit harmful behaviors. Activation steering techniques offer a promising inference-time intervention approach, but existing methods suffer from critical limitations: activation addition requires careful coefficient tuning and is sensitive to layer-specific norm variations, while directional ablation provides only binary control. Recent work on Angular Steering introduces continuous control via rotation in a 2D subspace, but its practical implementation violates norm preservation, causing distribution shift and generation collapse, particularly in models below 7B parameters. We propose Selective Steering, which addresses these limitations through two key innovations: (1) a mathematically rigorous norm-preserving rotation formulation that maintains activation distribution integrity, and (2) discriminative layer selection that applies steering only where feature representations exhibit opposite-signed class alignment. Experiments across nine models demonstrate that Selective Steering achieves 5.5x higher attack success rates than prior methods while maintaining zero perplexity violations and approximately 100\\% capability retention on standard benchmarks. Our approach provides a principled, efficient framework for controllable and stable LLM behavior modification. Code: https://github.com/knoveleng/steering", "AI": {"tldr": "\u63d0\u51faSelective Steering\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b66\u4e25\u8c28\u7684\u8303\u6570\u4fdd\u6301\u65cb\u8f6c\u516c\u5f0f\u548c\u5224\u522b\u6027\u5c42\u9009\u62e9\uff0c\u89e3\u51b3\u73b0\u6709\u6fc0\u6d3b\u5f15\u5bfc\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u53ef\u63a7\u7684LLM\u884c\u4e3a\u4fee\u6539\u3002", "motivation": "\u5c3d\u7ba1\u5728\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u4ecd\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\u5f15\u53d1\u6709\u5bb3\u884c\u4e3a\u3002\u73b0\u6709\u6fc0\u6d3b\u5f15\u5bfc\u6280\u672f\u5b58\u5728\u5c40\u9650\u6027\uff1a\u6fc0\u6d3b\u52a0\u6cd5\u9700\u8981\u7cbe\u7ec6\u7cfb\u6570\u8c03\u4f18\u4e14\u5bf9\u5c42\u7279\u5b9a\u8303\u6570\u53d8\u5316\u654f\u611f\uff0c\u65b9\u5411\u6d88\u878d\u4ec5\u63d0\u4f9b\u4e8c\u5143\u63a7\u5236\uff0c\u800c\u89d2\u5ea6\u5f15\u5bfc\u867d\u5f15\u5165\u8fde\u7eed\u63a7\u5236\u4f46\u5b9e\u9645\u5b9e\u73b0\u8fdd\u53cd\u8303\u6570\u4fdd\u6301\uff0c\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\u548c\u751f\u6210\u5d29\u6e83\u3002", "method": "\u63d0\u51faSelective Steering\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a(1) \u6570\u5b66\u4e25\u8c28\u7684\u8303\u6570\u4fdd\u6301\u65cb\u8f6c\u516c\u5f0f\uff0c\u4fdd\u6301\u6fc0\u6d3b\u5206\u5e03\u5b8c\u6574\u6027\uff1b(2) \u5224\u522b\u6027\u5c42\u9009\u62e9\uff0c\u4ec5\u5728\u7279\u5f81\u8868\u793a\u5448\u73b0\u76f8\u53cd\u7b26\u53f7\u7c7b\u522b\u5bf9\u9f50\u7684\u5c42\u5e94\u7528\u5f15\u5bfc\u3002", "result": "\u5728\u4e5d\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSelective Steering\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5b9e\u73b0\u4e865.5\u500d\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u96f6\u56f0\u60d1\u5ea6\u8fdd\u89c4\u548c\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u7ea6100%\u7684\u80fd\u529b\u4fdd\u7559\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u63a7\u548c\u7a33\u5b9a\u7684LLM\u884c\u4e3a\u4fee\u6539\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u3001\u9ad8\u6548\u7684\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.19452", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19452", "abs": "https://arxiv.org/abs/2601.19452", "authors": ["Finn Rietz", "Pedro Zuidberg dos Martires", "Johannes Andreas Stork"], "title": "APC-RL: Exceeding Data-Driven Behavior Priors with Adaptive Policy Composition", "comment": null, "summary": "Incorporating demonstration data into reinforcement learning (RL) can greatly accelerate learning, but existing approaches often assume demonstrations are optimal and fully aligned with the target task. In practice, demonstrations are frequently sparse, suboptimal, or misaligned, which can degrade performance when these demonstrations are integrated into RL. We propose Adaptive Policy Composition (APC), a hierarchical model that adaptively composes multiple data-driven Normalizing Flow (NF) priors. Instead of enforcing strict adherence to the priors, APC estimates each prior's applicability to the target task while leveraging them for exploration. Moreover, APC either refines useful priors, or sidesteps misaligned ones when necessary to optimize downstream reward. Across diverse benchmarks, APC accelerates learning when demonstrations are aligned, remains robust under severe misalignment, and leverages suboptimal demonstrations to bootstrap exploration while avoiding performance degradation caused by overly strict adherence to suboptimal demonstrations.", "AI": {"tldr": "APC\u662f\u4e00\u79cd\u5206\u5c42\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ec4\u5408\u591a\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u5f52\u4e00\u5316\u6d41\u5148\u9a8c\u6765\u6574\u5408\u6f14\u793a\u6570\u636e\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u80fd\u591f\u5904\u7406\u6b21\u4f18\u3001\u7a00\u758f\u6216\u4e0d\u5bf9\u9f50\u7684\u6f14\u793a\uff0c\u907f\u514d\u4e25\u683c\u9075\u5faa\u6f14\u793a\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u6f14\u793a\u6570\u636e\u662f\u6700\u4f18\u4e14\u4e0e\u76ee\u6807\u4efb\u52a1\u5b8c\u5168\u5bf9\u9f50\u7684\uff0c\u4f46\u5b9e\u9645\u4e2d\u6f14\u793a\u7ecf\u5e38\u662f\u7a00\u758f\u3001\u6b21\u4f18\u6216\u4e0d\u5bf9\u9f50\u7684\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5c06\u6f14\u793a\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u7b56\u7565\u7ec4\u5408\uff08APC\uff09\u5206\u5c42\u6a21\u578b\uff0c\u81ea\u9002\u5e94\u7ec4\u5408\u591a\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u5f52\u4e00\u5316\u6d41\u5148\u9a8c\u3002APC\u4f30\u8ba1\u6bcf\u4e2a\u5148\u9a8c\u5bf9\u76ee\u6807\u4efb\u52a1\u7684\u9002\u7528\u6027\uff0c\u5229\u7528\u5b83\u4eec\u8fdb\u884c\u63a2\u7d22\uff0c\u540c\u65f6\u6839\u636e\u9700\u8981\u7cbe\u70bc\u6709\u7528\u5148\u9a8c\u6216\u907f\u5f00\u4e0d\u5bf9\u9f50\u7684\u5148\u9a8c\u4ee5\u4f18\u5316\u4e0b\u6e38\u5956\u52b1\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAPC\u5728\u6f14\u793a\u5bf9\u9f50\u65f6\u52a0\u901f\u5b66\u4e60\uff0c\u5728\u4e25\u91cd\u4e0d\u5bf9\u9f50\u60c5\u51b5\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5229\u7528\u6b21\u4f18\u6f14\u793a\u5f15\u5bfc\u63a2\u7d22\uff0c\u540c\u65f6\u907f\u514d\u56e0\u8fc7\u5ea6\u4e25\u683c\u9075\u5faa\u6b21\u4f18\u6f14\u793a\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "APC\u80fd\u591f\u6709\u6548\u5904\u7406\u5b9e\u9645\u4e2d\u5e38\u89c1\u7684\u6b21\u4f18\u3001\u7a00\u758f\u6216\u4e0d\u5bf9\u9f50\u6f14\u793a\u6570\u636e\uff0c\u5728\u52a0\u901f\u5b66\u4e60\u7684\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u6f14\u793a\u8d28\u91cf\u8981\u6c42\u8fc7\u9ad8\u7684\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.19487", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19487", "abs": "https://arxiv.org/abs/2601.19487", "authors": ["Haonan Zhang", "Dongxia Wang", "Yi Liu", "Kexin Chen", "Wenhai Wang"], "title": "LLM-VA: Resolving the Jailbreak-Overrefusal Trade-off via Vector Alignment", "comment": null, "summary": "Safety-aligned LLMs suffer from two failure modes: jailbreak (answering harmful inputs) and over-refusal (declining benign queries). Existing vector steering methods adjust the magnitude of answer vectors, but this creates a fundamental trade-off -- reducing jailbreak increases over-refusal and vice versa. We identify the root cause: LLMs encode the decision to answer (answer vector $v_a$) and the judgment of input safety (benign vector $v_b$) as nearly orthogonal directions, treating them as independent processes. We propose LLM-VA, which aligns $v_a$ with $v_b$ through closed-form weight updates, making the model's willingness to answer causally dependent on its safety assessment -- without fine-tuning or architectural changes. Our method identifies vectors at each layer using SVMs, selects safety-relevant layers, and iteratively aligns vectors via minimum-norm weight modifications. Experiments on 12 LLMs demonstrate that LLM-VA achieves 11.45% higher F1 than the best baseline while preserving 95.92% utility, and automatically adapts to each model's safety bias without manual tuning. Code and models are available at https://hotbento.github.io/LLM-VA-Web/.", "AI": {"tldr": "LLM-VA\u901a\u8fc7\u5c06\u56de\u7b54\u5411\u91cf\u4e0e\u5b89\u5168\u5224\u65ad\u5411\u91cf\u5bf9\u9f50\uff0c\u89e3\u51b3LLM\u5728\u5b89\u5168\u5bf9\u9f50\u4e2d\u7684\u8d8a\u72f1\u548c\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u66f4\u597d\u7684\u5b89\u5168-\u6548\u7528\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u5411\u91cf\u8c03\u63a7\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u6743\u8861\uff1a\u51cf\u5c11\u8d8a\u72f1\u4f1a\u589e\u52a0\u8fc7\u5ea6\u62d2\u7edd\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u7814\u7a76\u53d1\u73b0LLM\u5c06\u56de\u7b54\u51b3\u7b56\u548c\u5b89\u5168\u5224\u65ad\u7f16\u7801\u4e3a\u8fd1\u4e4e\u6b63\u4ea4\u7684\u65b9\u5411\uff0c\u5bfc\u81f4\u8fd9\u4e24\u4e2a\u8fc7\u7a0b\u88ab\u5f53\u4f5c\u72ec\u7acb\u5904\u7406\u3002", "method": "\u63d0\u51faLLM-VA\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ed\u5f0f\u6743\u91cd\u66f4\u65b0\u5c06\u56de\u7b54\u5411\u91cf\u4e0e\u5b89\u5168\u5224\u65ad\u5411\u91cf\u5bf9\u9f50\uff0c\u4f7f\u6a21\u578b\u56de\u7b54\u610f\u613f\u56e0\u679c\u4f9d\u8d56\u4e8e\u5b89\u5168\u8bc4\u4f30\u3002\u4f7f\u7528SVM\u8bc6\u522b\u5404\u5c42\u5411\u91cf\uff0c\u9009\u62e9\u5b89\u5168\u76f8\u5173\u5c42\uff0c\u901a\u8fc7\u6700\u5c0f\u8303\u6570\u6743\u91cd\u4fee\u6539\u8fed\u4ee3\u5bf9\u9f50\u5411\u91cf\u3002", "result": "\u572812\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLLM-VA\u6bd4\u6700\u4f73\u57fa\u7ebfF1\u5206\u6570\u63d0\u9ad811.45%\uff0c\u540c\u65f6\u4fdd\u630195.92%\u7684\u6548\u7528\uff0c\u4e14\u80fd\u81ea\u52a8\u9002\u5e94\u5404\u6a21\u578b\u7684\u5b89\u5168\u504f\u5dee\u800c\u65e0\u9700\u624b\u52a8\u8c03\u4f18\u3002", "conclusion": "LLM-VA\u901a\u8fc7\u5411\u91cf\u5bf9\u9f50\u89e3\u51b3\u4e86\u5b89\u5168\u5bf9\u9f50LLM\u7684\u6839\u672c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5b89\u5168-\u6548\u7528\u5e73\u8861\uff0c\u4e14\u65b9\u6cd5\u901a\u7528\u3001\u65e0\u9700\u5fae\u8c03\u6216\u67b6\u6784\u4fee\u6539\u3002", "topic": "agent analysis"}}
{"id": "2601.19612", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19612", "abs": "https://arxiv.org/abs/2601.19612", "authors": ["Manuel Wendl", "Yarden As", "Manish Prajapat", "Anton Pollak", "Stelian Coros", "Andreas Krause"], "title": "Safe Exploration via Policy Priors", "comment": null, "summary": "Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice.", "AI": {"tldr": "SOOPER\u662f\u4e00\u79cd\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u6b21\u4f18\u4f46\u4fdd\u5b88\u7684\u7b56\u7565\u4f5c\u4e3a\u5148\u9a8c\uff0c\u901a\u8fc7\u6982\u7387\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u4e50\u89c2\u63a2\u7d22\uff0c\u540c\u65f6\u5728\u9700\u8981\u65f6\u60b2\u89c2\u5730\u56de\u9000\u5230\u4fdd\u5b88\u7b56\u7565\uff0c\u4fdd\u8bc1\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9700\u8981\u5b89\u5168\u63a2\u7d22\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u53d7\u63a7\u7684\u6a21\u62df\u73af\u5883\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u5b66\u4e60\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u4f7f\u7528\u6b21\u4f18\u4f46\u4fdd\u5b88\u7684\u7b56\u7565\uff08\u5982\u4ece\u79bb\u7ebf\u6570\u636e\u6216\u6a21\u62df\u5668\u83b7\u5f97\uff09\u4f5c\u4e3a\u5148\u9a8c\uff0c\u7ed3\u5408\u6982\u7387\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u4e50\u89c2\u63a2\u7d22\uff0c\u540c\u65f6\u5728\u9700\u8981\u65f6\u60b2\u89c2\u5730\u56de\u9000\u5230\u4fdd\u5b88\u7b56\u7565\u3002", "result": "SOOPER\u5728\u5173\u952e\u7684\u5b89\u5168RL\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "SOOPER\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u4fdd\u8bc1\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u6536\u655b\u5230\u6700\u4f18\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.19620", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19620", "abs": "https://arxiv.org/abs/2601.19620", "authors": ["Zhizheng Jiang", "Kang Zhao", "Weikai Xu", "Xinkui Lin", "Wei Liu", "Jian Luan", "Shuo Shang", "Peng Han"], "title": "R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning", "comment": null, "summary": "Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \\emph{\\textbf{R^3}} that along three directions: (1) a \\emph{cross-context \\underline{\\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \\emph{in-context self-\\underline{\\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \\emph{structural entropy \\underline{\\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.", "AI": {"tldr": "R^3\u65b9\u6cd5\u901a\u8fc7\u8de8\u4e0a\u4e0b\u6587\u91cd\u653e\u3001\u4e0a\u4e0b\u6587\u81ea\u53cd\u601d\u548c\u7ed3\u6784\u71b5\u6392\u5e8f\u5956\u52b1\u4e09\u4e2a\u673a\u5236\uff0c\u89e3\u51b3\u5927\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u4e2d\u7ec4\u5185\u4f18\u52bf\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u540c\u4e00\u6279\u6b21\u5185\u9ad8\u8d28\u91cf\u6837\u672c\u8bf1\u5bfc\u7684\u4f18\u52bf\u5dee\u8ddd\uff0c\u4f46\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e0b\u7ec4\u5185\u4f18\u52bf\u5bb9\u6613\u5d29\u6e83\uff0c\u5bfc\u81f4\u8bad\u7ec3\u8fc7\u7a0b\u8106\u5f31\u4e14\u4f4e\u6548\u3002", "method": "\u63d0\u51faR^3\u5f3a\u5316\u5b66\u4e60\u673a\u5236\uff1a1)\u8de8\u4e0a\u4e0b\u6587\u91cd\u653e\u7b56\u7565\uff0c\u901a\u8fc7\u56de\u5fc6\u540c\u4e00\u67e5\u8be2\u7684\u5386\u53f2\u8f68\u8ff9\u4fdd\u6301\u7ec4\u5185\u4f18\u52bf\uff1b2)\u4e0a\u4e0b\u6587\u81ea\u53cd\u601d\u673a\u5236\uff0c\u5229\u7528\u8fc7\u53bb\u5931\u8d25\u7ecf\u9a8c\u7cbe\u70bc\u8f93\u51fa\uff1b3)\u7ed3\u6784\u71b5\u6392\u5e8f\u5956\u52b1\uff0c\u57fa\u4e8e\u6807\u8bb0\u7ea7\u71b5\u6a21\u5f0f\u5bf9\u622a\u65ad\u6216\u5931\u8d25\u6837\u672c\u5206\u914d\u76f8\u5bf9\u5956\u52b1\u3002", "result": "\u5728Deepseek-R1-Distill-Qwen-1.5B\u4e0a\u5b9e\u73b0\uff0c\u4f7f\u7528DeepscaleR-40k\u6570\u5b66\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u6709\u663e\u8457\u6539\u8fdb\u4e14\u4f7f\u7528\u66f4\u5c11\u63a8\u7406\u6807\u8bb0\u3002", "conclusion": "R^3\u65b9\u6cd5\u901a\u8fc7\u4e09\u4e2a\u521b\u65b0\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u7ec4\u5185\u4f18\u52bf\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5927\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.19720", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.19720", "abs": "https://arxiv.org/abs/2601.19720", "authors": ["Gong Gao", "Weidong Zhao", "Xianhui Liu", "Ning Jia"], "title": "Improving Policy Exploitation in Online Reinforcement Learning with Instant Retrospect Action", "comment": null, "summary": "Existing value-based online reinforcement learning (RL) algorithms suffer from slow policy exploitation due to ineffective exploration and delayed policy updates. To address these challenges, we propose an algorithm called Instant Retrospect Action (IRA). Specifically, we propose Q-Representation Discrepancy Evolution (RDE) to facilitate Q-network representation learning, enabling discriminative representations for neighboring state-action pairs. In addition, we adopt an explicit method to policy constraints by enabling Greedy Action Guidance (GAG). This is achieved through backtracking historical actions, which effectively enhances the policy update process. Our proposed method relies on providing the learning algorithm with accurate $k$-nearest-neighbor action value estimates and learning to design a fast-adaptable policy through policy constraints. We further propose the Instant Policy Update (IPU) mechanism, which enhances policy exploitation by systematically increasing the frequency of policy updates. We further discover that the early-stage training conservatism of the IRA method can alleviate the overestimation bias problem in value-based RL. Experimental results show that IRA can significantly improve the learning efficiency and final performance of online RL algorithms on eight MuJoCo continuous control tasks.", "AI": {"tldr": "\u63d0\u51faIRA\u7b97\u6cd5\uff0c\u901a\u8fc7Q\u8868\u793a\u5dee\u5f02\u6f14\u5316\u3001\u8d2a\u5a6a\u52a8\u4f5c\u5f15\u5bfc\u548c\u5373\u65f6\u7b56\u7565\u66f4\u65b0\u673a\u5236\uff0c\u89e3\u51b3\u57fa\u4e8e\u4ef7\u503c\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22\u6548\u7387\u4f4e\u548c\u7b56\u7565\u66f4\u65b0\u5ef6\u8fdf\u7684\u95ee\u9898\uff0c\u5728MuJoCo\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4ef7\u503c\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5b58\u5728\u63a2\u7d22\u6548\u7387\u4f4e\u548c\u7b56\u7565\u66f4\u65b0\u5ef6\u8fdf\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7b56\u7565\u5229\u7528\u7f13\u6162\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002", "method": "\u63d0\u51faIRA\u7b97\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) Q\u8868\u793a\u5dee\u5f02\u6f14\u5316(RDE)\u4fc3\u8fdbQ\u7f51\u7edc\u8868\u793a\u5b66\u4e60\uff1b2) \u8d2a\u5a6a\u52a8\u4f5c\u5f15\u5bfc(GAG)\u901a\u8fc7\u56de\u6eaf\u5386\u53f2\u52a8\u4f5c\u5b9e\u73b0\u663e\u5f0f\u7b56\u7565\u7ea6\u675f\uff1b3) \u5373\u65f6\u7b56\u7565\u66f4\u65b0(IPU)\u673a\u5236\u7cfb\u7edf\u6027\u5730\u589e\u52a0\u7b56\u7565\u66f4\u65b0\u9891\u7387\u3002", "result": "\u5728\u516b\u4e2aMuJoCo\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIRA\u80fd\u663e\u8457\u63d0\u9ad8\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u5b66\u4e60\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002\u540c\u65f6\u53d1\u73b0IRA\u65e9\u671f\u8bad\u7ec3\u7684\u4fdd\u5b88\u6027\u53ef\u4ee5\u7f13\u89e3\u57fa\u4e8e\u4ef7\u503cRL\u4e2d\u7684\u9ad8\u4f30\u504f\u5dee\u95ee\u9898\u3002", "conclusion": "IRA\u7b97\u6cd5\u901a\u8fc7\u6539\u8fdb\u8868\u793a\u5b66\u4e60\u3001\u7b56\u7565\u7ea6\u675f\u548c\u66f4\u65b0\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u4e8e\u4ef7\u503c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u548c\u66f4\u65b0\u5ef6\u8fdf\u95ee\u9898\uff0c\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.19810", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.19810", "abs": "https://arxiv.org/abs/2601.19810", "authors": ["Octavio Pappalardo"], "title": "Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals", "comment": "To appear at ICLR 2026", "summary": "Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent's post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent's capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula.", "AI": {"tldr": "ULEE\uff1a\u4e00\u79cd\u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5bf9\u6297\u6027\u76ee\u6807\u751f\u6210\u7684\u65e0\u76d1\u7763\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728XLand-MiniGrid\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u63a2\u7d22\u548c\u9002\u5e94\u80fd\u529b", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u5e7f\u6cdb\u4e0b\u6e38\u4efb\u52a1\u5206\u5e03\u4e2d\u6709\u6548\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u4efb\u52a1\u8d85\u51fa\u9884\u8bad\u7ec3\u5206\u5e03\u6216\u4efb\u52a1\u8eab\u4efd\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u9ad8\u6548\u7684\u63a2\u7d22\u548c\u9002\u5e94\u673a\u5236", "method": "\u63d0\u51faULEE\u65b9\u6cd5\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\u6846\u67b6\u4f18\u5316\u591a\u56de\u5408\u63a2\u7d22\u548c\u9002\u5e94\uff0c\u4f7f\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u5668\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u6027\u76ee\u6807\u751f\u6210\u7b56\u7565\u7ef4\u6301\u8bad\u7ec3\u5728\u667a\u80fd\u4f53\u80fd\u529b\u8fb9\u754c", "result": "\u5728XLand-MiniGrid\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cULEE\u9884\u8bad\u7ec3\u5c55\u73b0\u51fa\u5bf9\u65b0\u9896\u76ee\u6807\u3001\u73af\u5883\u52a8\u6001\u548c\u5730\u56fe\u7ed3\u6784\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u83b7\u5f97\u6539\u8fdb\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u6027\u80fd\uff0c\u4e3a\u957f\u65f6\u5fae\u8c03\u63d0\u4f9b\u5f3a\u521d\u59cb\u5316", "conclusion": "ULEE\u901a\u8fc7\u65e0\u76d1\u7763\u5143\u5b66\u4e60\u548c\u5bf9\u6297\u6027\u76ee\u6807\u751f\u6210\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u63a2\u7d22\u548c\u9002\u5e94\u80fd\u529b\uff0c\u4f18\u4e8e\u4ece\u5934\u5b66\u4e60\u3001DIAYN\u9884\u8bad\u7ec3\u548c\u5176\u4ed6\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.e88c5977", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fanthropic-prepares-to-release-security-center-for-claude-code%2F%3Futm_source=tldrinfosec/1/0100019bfaa21f61-390f892b-d6b3-409b-b0b9-5f4587889129-000000/Rbb1A6z_fS1vXvFuh0G6EJ4Mk3Tk_JehfGMSLyQVPbw=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fanthropic-prepares-to-release-security-center-for-claude-code%2F%3Futm_source=tldrinfosec/1/0100019bfaa21f61-390f892b-d6b3-409b-b0b9-5f4587889129-000000/Rbb1A6z_fS1vXvFuh0G6EJ4Mk3Tk_JehfGMSLyQVPbw=441", "authors": ["TLDR Newsletter"], "title": "Anthropic prepares to release Security Center for Claude Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fanthropic-prepares-to-release-security-center-for-claude-code%2F%3Futm_source=tldrinfosec/1/0100019bfaa21f61-390f892b-d6b3-409b-b0b9-5f4587889129-000000/Rbb1A6z_fS1vXvFuh0G6EJ4Mk3Tk_JehfGMSLyQVPbw=441", "summary": "Anthropic prepares to release Security Center for Claude Code (2 minute read) Anthropic is preparing to launch Security Center for Claude Code, a feature that will provide users with an overview of recent security scans, detected issues, and the ability to manually initiate repository scans.", "source": "tldr", "AI": {"tldr": "Anthropic\u5373\u5c06\u63a8\u51faClaude Code\u5b89\u5168\u4e2d\u5fc3\uff0c\u63d0\u4f9b\u5b89\u5168\u626b\u63cf\u6982\u89c8\u3001\u95ee\u9898\u68c0\u6d4b\u548c\u624b\u52a8\u4ed3\u5e93\u626b\u63cf\u529f\u80fd", "motivation": "\u4e3aClaude Code\u7528\u6237\u63d0\u4f9b\u66f4\u597d\u7684\u4ee3\u7801\u5b89\u5168\u76d1\u63a7\u548c\u7ba1\u7406\u5de5\u5177\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u53ca\u65f6\u53d1\u73b0\u548c\u89e3\u51b3\u4ee3\u7801\u5b89\u5168\u95ee\u9898", "method": "\u5f00\u53d1\u5b89\u5168\u4e2d\u5fc3\u529f\u80fd\uff0c\u5305\u62ec\u5b89\u5168\u626b\u63cf\u5386\u53f2\u6982\u89c8\u3001\u95ee\u9898\u68c0\u6d4b\u7cfb\u7edf\u548c\u624b\u52a8\u626b\u63cf\u89e6\u53d1\u673a\u5236", "result": "\u5373\u5c06\u53d1\u5e03Claude Code\u5b89\u5168\u4e2d\u5fc3\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u5168\u9762\u7684\u4ee3\u7801\u5b89\u5168\u76d1\u63a7\u89e3\u51b3\u65b9\u6848", "conclusion": "Anthropic\u901a\u8fc7\u5b89\u5168\u4e2d\u5fc3\u529f\u80fd\u589e\u5f3aClaude Code\u7684\u5b89\u5168\u80fd\u529b\uff0c\u63d0\u5347\u5f00\u53d1\u8005\u4ee3\u7801\u5b89\u5168\u6c34\u5e73", "topic": "swe application"}}
{"id": "tldr.2601.58580245", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FwkSE7U/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/TbyTCGot1qWrVLt8zheWo56N2vbnXv3TYeyP7UBLqfk=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FwkSE7U/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/TbyTCGot1qWrVLt8zheWo56N2vbnXv3TYeyP7UBLqfk=441", "authors": ["TLDR Newsletter"], "title": "Unrolling the Codex agent loop", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FwkSE7U/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/TbyTCGot1qWrVLt8zheWo56N2vbnXv3TYeyP7UBLqfk=441", "summary": "Unrolling the Codex agent loop (17 minute read) Codex CLI is a cross-platform local software agent designed to produce high-quality, reliable software changes while operating safely and efficiently on users' machines. This is the first part in a series of posts that explores how Codex works. It focuses on the agent loop, which is the core logic responsible for orchestrating the interaction between the user, the model, and the tools the model invokes to perform work. The post provides a view i...", "source": "tldr", "AI": {"tldr": "Codex CLI\u662f\u4e00\u4e2a\u8de8\u5e73\u53f0\u672c\u5730\u8f6f\u4ef6\u4ee3\u7406\uff0c\u65e8\u5728\u5b89\u5168\u9ad8\u6548\u5730\u5728\u7528\u6237\u673a\u5668\u4e0a\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u9760\u7684\u8f6f\u4ef6\u53d8\u66f4\u3002\u672c\u6587\u662f\u8be5\u7cfb\u5217\u7684\u7b2c\u4e00\u90e8\u5206\uff0c\u91cd\u70b9\u89e3\u6790\u4ee3\u7406\u5faa\u73af\u7684\u6838\u5fc3\u903b\u8f91\uff0c\u5373\u534f\u8c03\u7528\u6237\u3001\u6a21\u578b\u548c\u5de5\u5177\u8c03\u7528\u7684\u4ea4\u4e92\u673a\u5236\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u7406\u89e3\u4ee3\u7406\u5982\u4f55\u5b89\u5168\u3001\u9ad8\u6548\u5730\u5728\u672c\u5730\u73af\u5883\u4e2d\u5de5\u4f5c\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002Codex CLI\u65e8\u5728\u89e3\u51b3\u5728\u7528\u6237\u673a\u5668\u4e0a\u6267\u884c\u8f6f\u4ef6\u53d8\u66f4\u65f6\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9700\u8981\u6df1\u5165\u5206\u6790\u5176\u6838\u5fc3\u5de5\u4f5c\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5206\u6790Codex CLI\u7684\u4ee3\u7406\u5faa\u73af\uff08agent loop\uff09\u6765\u63ed\u793a\u5176\u5de5\u4f5c\u539f\u7406\u3002\u4ee3\u7406\u5faa\u73af\u662f\u534f\u8c03\u7528\u6237\u3001\u6a21\u578b\u548c\u5de5\u5177\u8c03\u7528\u7684\u6838\u5fc3\u903b\u8f91\uff0c\u5305\u62ec\u5982\u4f55\u63a5\u6536\u7528\u6237\u8f93\u5165\u3001\u8c03\u7528\u6a21\u578b\u751f\u6210\u4ee3\u7801\u3001\u6267\u884c\u5de5\u5177\u64cd\u4f5c\u4ee5\u53ca\u5904\u7406\u53cd\u9988\u7684\u5b8c\u6574\u6d41\u7a0b\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9Codex CLI\u4ee3\u7406\u5faa\u73af\u7684\u8be6\u7ec6\u89e3\u6790\uff0c\u5c55\u793a\u4e86\u4ee3\u7406\u5982\u4f55\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4ea4\u4e92\u673a\u5236\u6765\u786e\u4fdd\u8f6f\u4ef6\u53d8\u66f4\u7684\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002\u8fd9\u79cd\u5206\u6790\u6709\u52a9\u4e8e\u7406\u89e3\u73b0\u4ee3AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u5de5\u4f5c\u65b9\u5f0f\u3002", "conclusion": "\u4ee3\u7406\u5faa\u73af\u662fAI\u8f6f\u4ef6\u4ee3\u7406\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u7406\u89e3\u5176\u5de5\u4f5c\u673a\u5236\u5bf9\u4e8e\u5f00\u53d1\u5b89\u5168\u3001\u53ef\u9760\u7684\u672c\u5730\u8f6f\u4ef6\u4ee3\u7406\u81f3\u5173\u91cd\u8981\u3002Codex CLI\u7684\u8bbe\u8ba1\u4e3a\u5728\u7528\u6237\u673a\u5668\u4e0a\u6267\u884c\u8f6f\u4ef6\u53d8\u66f4\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6846\u67b6\uff0c\u540e\u7eed\u7cfb\u5217\u6587\u7ae0\u5c06\u8fdb\u4e00\u6b65\u6df1\u5165\u5176\u4ed6\u6280\u672f\u7ec6\u8282\u3002", "topic": "code agent"}}
{"id": "tldr.2601.616303c4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nicolasbustamante.com%2Fp%2Flessons-from-building-ai-agents-for%3Futm_source=tldrai/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/g_KUHjI7N6VGcGwoHIyRpKk8iDEdy5by_nswhJgp9pg=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nicolasbustamante.com%2Fp%2Flessons-from-building-ai-agents-for%3Futm_source=tldrai/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/g_KUHjI7N6VGcGwoHIyRpKk8iDEdy5by_nswhJgp9pg=441", "authors": ["TLDR Newsletter"], "title": "Lessons from Building AI Agents for Financial Services", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 23 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nicolasbustamante.com%2Fp%2Flessons-from-building-ai-agents-for%3Futm_source=tldrai/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/g_KUHjI7N6VGcGwoHIyRpKk8iDEdy5by_nswhJgp9pg=441", "summary": "Lessons from Building AI Agents for Financial Services (23 minute read) Building AI agents for financial services requires rigorous technical infrastructure and data normalization to avoid costly errors. Key insights include the essentiality of sandbox environments for secure multi-step workflows and the transformation of raw financial data into structured, searchable context through markdown and metadata. Skills systems, using markdown instead of code, allow dynamic, user-specific instructio...", "source": "tldr", "AI": {"tldr": "\u6784\u5efa\u91d1\u878d\u670d\u52a1\u7684AI\u4ee3\u7406\u9700\u8981\u4e25\u683c\u7684\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u548c\u6570\u636e\u89c4\u8303\u5316\uff0c\u4ee5\u907f\u514d\u4ee3\u4ef7\u9ad8\u6602\u7684\u9519\u8bef\u3002\u5173\u952e\u89c1\u89e3\u5305\u62ec\u6c99\u76d2\u73af\u5883\u5bf9\u5b89\u5168\u591a\u6b65\u5de5\u4f5c\u6d41\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u901a\u8fc7markdown\u548c\u5143\u6570\u636e\u5c06\u539f\u59cb\u91d1\u878d\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u641c\u7d22\u7684\u4e0a\u4e0b\u6587\u3002", "motivation": "\u91d1\u878d\u670d\u52a1\u9886\u57df\u5bf9AI\u4ee3\u7406\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u8be5\u9886\u57df\u5bf9\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u8981\u6c42\u6781\u9ad8\uff0c\u9700\u8981\u907f\u514d\u4ee3\u4ef7\u9ad8\u6602\u7684\u9519\u8bef\u3002\u73b0\u6709\u7684AI\u4ee3\u7406\u89e3\u51b3\u65b9\u6848\u5728\u91d1\u878d\u670d\u52a1\u7684\u7279\u6b8a\u9700\u6c42\uff08\u5982\u6570\u636e\u89c4\u8303\u5316\u3001\u5b89\u5168\u5de5\u4f5c\u6d41\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u6c99\u76d2\u73af\u5883\u786e\u4fdd\u5b89\u5168\u7684\u591a\u6b65\u5de5\u4f5c\u6d41\uff1b\u901a\u8fc7markdown\u548c\u5143\u6570\u636e\u5c06\u539f\u59cb\u91d1\u878d\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u641c\u7d22\u7684\u4e0a\u4e0b\u6587\uff1b\u4f7f\u7528\u57fa\u4e8emarkdown\u800c\u975e\u4ee3\u7801\u7684\u6280\u80fd\u7cfb\u7edf\uff0c\u5b9e\u73b0\u52a8\u6001\u3001\u7528\u6237\u7279\u5b9a\u7684\u6307\u4ee4\u6267\u884c\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u9002\u7528\u4e8e\u91d1\u878d\u670d\u52a1\u7684AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u5b89\u5168\u5904\u7406\u590d\u6742\u7684\u91d1\u878d\u5de5\u4f5c\u6d41\uff0c\u6709\u6548\u8f6c\u6362\u548c\u5229\u7528\u91d1\u878d\u6570\u636e\uff0c\u63d0\u4f9b\u7528\u6237\u7279\u5b9a\u7684\u52a8\u6001\u6307\u4ee4\u6267\u884c\u80fd\u529b\u3002", "conclusion": "\u6784\u5efa\u91d1\u878d\u670d\u52a1AI\u4ee3\u7406\u9700\u8981\u4e25\u683c\u7684\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u548c\u6570\u636e\u89c4\u8303\u5316\uff0c\u6c99\u76d2\u73af\u5883\u548c\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u6362\u662f\u5173\u952e\u6210\u529f\u56e0\u7d20\uff0c\u57fa\u4e8emarkdown\u7684\u6280\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u7528\u6237\u7279\u5b9a\u6307\u4ee4\u6267\u884c\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.bc66e41d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FByteDance-Seed%2FStable-DiffCoder%3Futm_source=tldrai/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/2dn3BFuNrS85df6uNBdMi5SPPs1ff_A4abSi7vxf6zE=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FByteDance-Seed%2FStable-DiffCoder%3Futm_source=tldrai/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/2dn3BFuNrS85df6uNBdMi5SPPs1ff_A4abSi7vxf6zE=441", "authors": ["TLDR Newsletter"], "title": "Diffusion-Based Code Modeling", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FByteDance-Seed%2FStable-DiffCoder%3Futm_source=tldrai/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/2dn3BFuNrS85df6uNBdMi5SPPs1ff_A4abSi7vxf6zE=441", "summary": "Diffusion-Based Code Modeling (GitHub Repo) Stable-DiffCoder introduces block diffusion continual pretraining for code LLMs, showing performance gains over autoregressive models across several programming benchmarks, especially for editing, reasoning, and low-resource languages.", "source": "tldr", "AI": {"tldr": "Stable-DiffCoder\u901a\u8fc7\u5757\u6269\u6563\u6301\u7eed\u9884\u8bad\u7ec3\u63d0\u5347\u4ee3\u7801LLM\u6027\u80fd\uff0c\u5728\u7f16\u8f91\u3001\u63a8\u7406\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4efb\u52a1\u4e0a\u8d85\u8d8a\u81ea\u56de\u5f52\u6a21\u578b", "motivation": "\u5f53\u524d\u81ea\u56de\u5f52\u4ee3\u7801\u6a21\u578b\u5728\u7f16\u8f91\u3001\u63a8\u7406\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4ee3\u7801\u5efa\u6a21\u65b9\u6cd5", "method": "\u63d0\u51fa\u5757\u6269\u6563\u6301\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u8fdb\u884c\u4ee3\u7801\u5efa\u6a21\uff0c\u76f8\u6bd4\u4f20\u7edf\u81ea\u56de\u5f52\u65b9\u6cd5", "result": "\u5728\u591a\u4e2a\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7801\u7f16\u8f91\u3001\u63a8\u7406\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u4efb\u52a1\u4e0a", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3a\u4ee3\u7801\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4f20\u7edf\u81ea\u56de\u5f52\u65b9\u6cd5", "topic": "code agent"}}
{"id": "tldr.2601.32283014", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FlifxPL/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/tF2WjbL-I4dGp1Xe0c105Kjqa5fxDcUFIob_ORLE-hw=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FlifxPL/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/tF2WjbL-I4dGp1Xe0c105Kjqa5fxDcUFIob_ORLE-hw=441", "authors": ["TLDR Newsletter"], "title": "I Spent 40 Hours Researching Clawdbot. Here's Everything They're Not Telling You", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FlifxPL/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/tF2WjbL-I4dGp1Xe0c105Kjqa5fxDcUFIob_ORLE-hw=441", "summary": "I Spent 40 Hours Researching Clawdbot. Here's Everything They're Not Telling You (8 minute read) Clawdbot is an autonomous AI agent that runs locally and can execute real actions, not just generate text. Some features work immediately out of the box, like file management, basic research, and document processing, while advanced automations require hours or days of setup.", "source": "tldr", "AI": {"tldr": "Clawdbot\u662f\u4e00\u6b3e\u672c\u5730\u8fd0\u884c\u7684\u81ea\u4e3bAI\u4ee3\u7406\uff0c\u80fd\u591f\u6267\u884c\u5b9e\u9645\u52a8\u4f5c\u800c\u4e0d\u4ec5\u4ec5\u662f\u751f\u6210\u6587\u672c\uff0c\u4f46\u9ad8\u7ea7\u81ea\u52a8\u5316\u529f\u80fd\u9700\u8981\u957f\u65f6\u95f4\u8bbe\u7f6e", "motivation": "\u4ecb\u7ecdClawdbot\u4f5c\u4e3a\u672c\u5730\u8fd0\u884c\u7684\u81ea\u4e3bAI\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u5f3a\u8c03\u5176\u80fd\u591f\u6267\u884c\u5b9e\u9645\u52a8\u4f5c\u800c\u975e\u4ec5\u6587\u672c\u751f\u6210\uff0c\u5e76\u63ed\u793a\u5176\u529f\u80fd\u8bbe\u7f6e\u7684\u5b9e\u9645\u60c5\u51b5", "method": "\u57fa\u4e8e40\u5c0f\u65f6\u7684\u7814\u7a76\u5206\u6790\uff0c\u5bf9\u6bd4Clawdbot\u7684\u5f00\u7bb1\u5373\u7528\u529f\u80fd\uff08\u6587\u4ef6\u7ba1\u7406\u3001\u57fa\u7840\u7814\u7a76\u3001\u6587\u6863\u5904\u7406\uff09\u4e0e\u9700\u8981\u957f\u65f6\u95f4\u8bbe\u7f6e\u7684\u9ad8\u7ea7\u81ea\u52a8\u5316\u529f\u80fd", "result": "\u53d1\u73b0Clawdbot\u786e\u5b9e\u80fd\u591f\u672c\u5730\u8fd0\u884c\u5e76\u6267\u884c\u5b9e\u9645\u52a8\u4f5c\uff0c\u4f46\u9ad8\u7ea7\u529f\u80fd\u9700\u8981\u6570\u5c0f\u65f6\u751a\u81f3\u6570\u5929\u7684\u8bbe\u7f6e\u65f6\u95f4\uff0c\u4e0e\u5ba3\u4f20\u5b58\u5728\u5dee\u8ddd", "conclusion": "Clawdbot\u4f5c\u4e3a\u81ea\u4e3bAI\u4ee3\u7406\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7528\u6237\u9700\u8981\u4e86\u89e3\u5176\u5b9e\u9645\u8bbe\u7f6e\u8981\u6c42\uff0c\u7279\u522b\u662f\u9ad8\u7ea7\u529f\u80fd\u9700\u8981\u6295\u5165\u5927\u91cf\u65f6\u95f4\u914d\u7f6e", "topic": "agent analysis"}}
{"id": "tldr.2601.a4bde87c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmatthewrocklin.com%2Fintroducing-claude-chic%2F%3Futm_source=tldrai/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/Hp36zEBJ4RIgcGOvXyvmMBZYw0Yhi-69qit8at-moQY=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmatthewrocklin.com%2Fintroducing-claude-chic%2F%3Futm_source=tldrai/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/Hp36zEBJ4RIgcGOvXyvmMBZYw0Yhi-69qit8at-moQY=441", "authors": ["TLDR Newsletter"], "title": "Introducing Claude Chic", "comment": "Source: TLDR Newsletter, Date: 2026-01-26, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmatthewrocklin.com%2Fintroducing-claude-chic%2F%3Futm_source=tldrai/1/0100019bfaae7bfc-493bf33e-6ebb-45ef-b9ee-19b74bfb708e-000000/Hp36zEBJ4RIgcGOvXyvmMBZYw0Yhi-69qit8at-moQY=441", "summary": "Introducing Claude Chic (7 minute read) Claude Chic is an alternative to Claude Code that visually organizes the message stream for legibility, organizes concurrent work trees, runs many sessions from the same window, and contains loads of quality-of-life features.", "source": "tldr", "AI": {"tldr": "Claude Chic\u662f\u4e00\u4e2a\u66ff\u4ee3Claude Code\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u7ec4\u7ec7\u6d88\u606f\u6d41\u3001\u7ba1\u7406\u5e76\u53d1\u5de5\u4f5c\u6811\u3001\u5728\u540c\u4e00\u7a97\u53e3\u8fd0\u884c\u591a\u4e2a\u4f1a\u8bdd\uff0c\u5e76\u63d0\u4f9b\u5927\u91cf\u751f\u6d3b\u8d28\u91cf\u529f\u80fd\u6765\u63d0\u5347\u5f00\u53d1\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u52a9\u624b\u5de5\u5177\u5728\u754c\u9762\u7ec4\u7ec7\u548c\u591a\u4efb\u52a1\u5904\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f00\u53d1\u8005\u9700\u8981\u66f4\u76f4\u89c2\u3001\u9ad8\u6548\u7684\u5de5\u5177\u6765\u7ba1\u7406\u590d\u6742\u7684\u7f16\u7801\u4f1a\u8bdd\u548c\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aClaude Chic\u7684\u66ff\u4ee3\u5de5\u5177\uff0c\u91c7\u7528\u53ef\u89c6\u5316\u6d88\u606f\u6d41\u7ec4\u7ec7\u3001\u5e76\u53d1\u5de5\u4f5c\u6811\u7ba1\u7406\u3001\u591a\u4f1a\u8bdd\u7a97\u53e3\u96c6\u6210\u7b49\u65b9\u6cd5\uff0c\u5e76\u6dfb\u52a0\u4e86\u5927\u91cf\u7528\u6237\u4f53\u9a8c\u4f18\u5316\u529f\u80fd\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u529f\u80fd\u4e30\u5bcc\u7684\u4ee3\u7801\u52a9\u624b\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u5f00\u53d1\u7684\u53ef\u8bfb\u6027\u3001\u7ec4\u7ec7\u6027\u548c\u5de5\u4f5c\u6548\u7387\u3002", "conclusion": "Claude Chic\u901a\u8fc7\u6539\u8fdb\u754c\u9762\u8bbe\u8ba1\u548c\u529f\u80fd\u96c6\u6210\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u4f18\u79c0\u7684\u4ee3\u7801\u52a9\u624b\u4f53\u9a8c\uff0c\u662f\u73b0\u6709\u5de5\u5177\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "tldr.2601.89c53ae2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fxcancel.com%2Fkarpathy%2Fstatus%2F2015883857489522876%3Futm_source=tldrnewsletter/1/0100019bff32130e-c89eeee9-3f76-4011-a798-7308901a9fe8-000000/HTIbn7E1HylesF4s9Py5cPsyf4Qh6zIKkuafqW3u-vg=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fxcancel.com%2Fkarpathy%2Fstatus%2F2015883857489522876%3Futm_source=tldrnewsletter/1/0100019bff32130e-c89eeee9-3f76-4011-a798-7308901a9fe8-000000/HTIbn7E1HylesF4s9Py5cPsyf4Qh6zIKkuafqW3u-vg=441", "authors": ["TLDR Newsletter"], "title": "A few random notes from Claude coding quite a bit last few weeks", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fxcancel.com%2Fkarpathy%2Fstatus%2F2015883857489522876%3Futm_source=tldrnewsletter/1/0100019bff32130e-c89eeee9-3f76-4011-a798-7308901a9fe8-000000/HTIbn7E1HylesF4s9Py5cPsyf4Qh6zIKkuafqW3u-vg=441", "summary": "A few random notes from Claude coding quite a bit last few weeks (7 minute read) AI agent capabilities crossed a threshold at the end of last year. This has caused a phase shift in software engineering. The intelligence part is a bit ahead of integration, so there is a need for new organizational workflows and processes. This year will be a high-energy year as the industry digests the new capability.", "source": "tldr", "AI": {"tldr": "AI\u667a\u80fd\u4f53\u80fd\u529b\u5728\u53bb\u5e74\u5e95\u8fbe\u5230\u4e34\u754c\u70b9\uff0c\u5f15\u53d1\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u8303\u5f0f\u8f6c\u53d8\uff0c\u667a\u80fd\u90e8\u5206\u9886\u5148\u4e8e\u96c6\u6210\uff0c\u9700\u8981\u65b0\u7684\u7ec4\u7ec7\u5de5\u4f5c\u6d41\u7a0b\u548c\u6d41\u7a0b\uff0c\u4eca\u5e74\u5c06\u662f\u884c\u4e1a\u6d88\u5316\u65b0\u80fd\u529b\u7684\u9ad8\u80fd\u5e74\u4efd", "motivation": "AI\u667a\u80fd\u4f53\u80fd\u529b\u57282023\u5e74\u5e95\u8fbe\u5230\u5173\u952e\u9608\u503c\uff0c\u5bfc\u81f4\u8f6f\u4ef6\u5de5\u7a0b\u53d1\u751f\u6839\u672c\u6027\u53d8\u5316\u3002\u5f53\u524d\u667a\u80fd\u80fd\u529b\u9886\u5148\u4e8e\u96c6\u6210\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u7ec4\u7ec7\u5de5\u4f5c\u6d41\u7a0b\u548c\u6d41\u7a0b\u6765\u9002\u5e94\u8fd9\u4e00\u53d8\u9769", "method": "\u672c\u6587\u57fa\u4e8e\u4f5c\u8005\u8fc7\u53bb\u51e0\u5468\u5bf9Claude\u7f16\u7801\u7684\u89c2\u5bdf\u548c\u7b14\u8bb0\uff0c\u5206\u6790AI\u667a\u80fd\u4f53\u80fd\u529b\u53d1\u5c55\u7684\u4e34\u754c\u70b9\u53ca\u5176\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u9700\u8981\u65b0\u7684\u7ec4\u7ec7\u5de5\u4f5c\u6d41\u7a0b", "result": "AI\u667a\u80fd\u4f53\u80fd\u529b\u5df2\u8de8\u8d8a\u5173\u952e\u9608\u503c\uff0c\u5f15\u53d1\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u667a\u80fd\u90e8\u5206\u9886\u5148\u4e8e\u96c6\u6210\uff0c\u884c\u4e1a\u9700\u8981\u65b0\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u7ec4\u7ec7\u6d41\u7a0b\u6765\u9002\u5e94\u8fd9\u4e00\u53d8\u5316", "conclusion": "2024\u5e74\u5c06\u662f\u884c\u4e1a\u6d88\u5316AI\u667a\u80fd\u4f53\u65b0\u80fd\u529b\u7684\u9ad8\u80fd\u5e74\u4efd\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u7ec4\u7ec7\u5de5\u4f5c\u6d41\u7a0b\u548c\u6d41\u7a0b\u6765\u5145\u5206\u5229\u7528\u667a\u80fd\u80fd\u529b\u9886\u5148\u4e8e\u96c6\u6210\u7684\u73b0\u72b6", "topic": "agent analysis"}}
{"id": "tldr.2601.c511231b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.vjeux.com%2F2026%2Fanalysis%2Fporting-100k-lines-from-typescript-to-rust-using-claude-code-in-a-month.html%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/az-2LUXSdpFZKkjc3tEiotE9tEnoGR_IpOC2IqTsD-E=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.vjeux.com%2F2026%2Fanalysis%2Fporting-100k-lines-from-typescript-to-rust-using-claude-code-in-a-month.html%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/az-2LUXSdpFZKkjc3tEiotE9tEnoGR_IpOC2IqTsD-E=441", "authors": ["TLDR Newsletter"], "title": "Porting 100k lines from TypeScript to Rust using Claude Code in a month", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.vjeux.com%2F2026%2Fanalysis%2Fporting-100k-lines-from-typescript-to-rust-using-claude-code-in-a-month.html%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/az-2LUXSdpFZKkjc3tEiotE9tEnoGR_IpOC2IqTsD-E=441", "summary": "Porting 100k lines from TypeScript to Rust using Claude Code in a month (17 minute read) This dev ported the ~100k line \"Pokemon Showdown\" codebase from JavaScript to Rust in about a month using Claude Code. The project required overcoming technical and operational challenges, including bypassing Claude's sandbox, automating continuous input, and managing large context windows. It also needed \"babysitting\" and iterative refinement. The resulting Rust codebase was completed through 5,000 commits.", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528Claude Code\u5728\u7ea6\u4e00\u4e2a\u6708\u5185\u5c06\u7ea610\u4e07\u884cTypeScript\u4ee3\u7801\u5e93\"Pokemon Showdown\"\u79fb\u690d\u5230Rust\uff0c\u901a\u8fc75000\u6b21\u63d0\u4ea4\u5b8c\u6210\uff0c\u9700\u8981\u514b\u670d\u6280\u672f\u6311\u6218\u548c\u6301\u7eed\u76d1\u63a7", "motivation": "\u63a2\u7d22\u4f7f\u7528AI\u4ee3\u7801\u52a9\u624b\uff08Claude Code\uff09\u8fdb\u884c\u5927\u89c4\u6a21\u4ee3\u7801\u79fb\u690d\u7684\u53ef\u884c\u6027\uff0c\u6d4b\u8bd5AI\u5728\u590d\u6742\u4ee3\u7801\u8f6c\u6362\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u4ece\u52a8\u6001\u7c7b\u578b\u8bed\u8a00\uff08TypeScript\uff09\u5230\u9759\u6001\u7c7b\u578b\u8bed\u8a00\uff08Rust\uff09\u7684\u8f6c\u6362", "method": "\u4f7f\u7528Claude Code\u4f5c\u4e3a\u4e3b\u8981\u5de5\u5177\uff0c\u901a\u8fc7\u7ed5\u8fc7Claude\u6c99\u7bb1\u3001\u81ea\u52a8\u5316\u8fde\u7eed\u8f93\u5165\u3001\u7ba1\u7406\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\u7b49\u6280\u672f\u624b\u6bb5\uff0c\u7ed3\u5408\u4eba\u5de5\"\u4fdd\u59c6\u5f0f\"\u76d1\u63a7\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u5b8c\u6210\u4ee3\u7801\u8f6c\u6362", "result": "\u6210\u529f\u5728\u7ea6\u4e00\u4e2a\u6708\u5185\u5c06\u7ea610\u4e07\u884c\u4ee3\u7801\u4eceTypeScript\u79fb\u690d\u5230Rust\uff0c\u751f\u62105000\u6b21\u63d0\u4ea4\uff0c\u8bc1\u660e\u4e86AI\u8f85\u52a9\u5927\u89c4\u6a21\u4ee3\u7801\u79fb\u690d\u7684\u53ef\u884c\u6027\uff0c\u4f46\u9700\u8981\u5927\u91cf\u4eba\u5de5\u76d1\u7763\u548c\u8fed\u4ee3", "conclusion": "AI\u4ee3\u7801\u52a9\u624b\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u5927\u89c4\u6a21\u4ee3\u7801\u79fb\u690d\u8fc7\u7a0b\uff0c\u4f46\u5f53\u524d\u4ecd\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5e72\u9884\u548c\u76d1\u7763\uff0c\u6280\u672f\u6311\u6218\u5305\u62ec\u6c99\u7bb1\u9650\u5236\u3001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u81ea\u52a8\u5316\u6d41\u7a0b", "topic": "code agent"}}
{"id": "tldr.2601.e417bb42", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fai-code-review-bubble%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/JqzhM8XXIBXFyrF4C5MOtCuMrkPe2OHriCNatNWOmW4=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fai-code-review-bubble%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/JqzhM8XXIBXFyrF4C5MOtCuMrkPe2OHriCNatNWOmW4=441", "authors": ["TLDR Newsletter"], "title": "There is an AI Code Review Bubble", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fblog%2Fai-code-review-bubble%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/JqzhM8XXIBXFyrF4C5MOtCuMrkPe2OHriCNatNWOmW4=441", "summary": "There is an AI Code Review Bubble (5 minute read) The AI code review market is experiencing rapid growth, so companies like Greptile are starting to differentiate their products beyond standard performance metrics. Greptile distinguishes itself by its long-term vision for AI code review, built on the pillars of independence, autonomy, and feedback loops. This approach emphasizes separate agents for code generation and validation, pushing for full automation of review, testing, and QA with min...", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7801\u5ba1\u67e5\u5e02\u573a\u5feb\u901f\u589e\u957f\uff0cGreptile\u901a\u8fc7\u72ec\u7acb\u6027\u3001\u81ea\u4e3b\u6027\u548c\u53cd\u9988\u5faa\u73af\u4e09\u5927\u652f\u67f1\u6784\u5efa\u957f\u671f\u613f\u666f\uff0c\u5f3a\u8c03\u4ee3\u7801\u751f\u6210\u4e0e\u9a8c\u8bc1\u5206\u79bb\u7684\u4ee3\u7406\u67b6\u6784\uff0c\u63a8\u52a8\u5ba1\u67e5\u3001\u6d4b\u8bd5\u548cQA\u7684\u5b8c\u5168\u81ea\u52a8\u5316", "motivation": "AI\u4ee3\u7801\u5ba1\u67e5\u5e02\u573a\u5feb\u901f\u589e\u957f\uff0c\u4f46\u5f53\u524d\u4ea7\u54c1\u540c\u8d28\u5316\u4e25\u91cd\uff0c\u9700\u8981\u5dee\u5f02\u5316\u7ade\u4e89\u3002Greptile\u65e8\u5728\u901a\u8fc7\u72ec\u7279\u7684\u957f\u671f\u613f\u666f\u548c\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u7ade\u4e89\u6fc0\u70c8\u7684\u5e02\u573a\u4e2d\u8131\u9896\u800c\u51fa", "method": "\u91c7\u7528\u57fa\u4e8e\u72ec\u7acb\u6027\u3001\u81ea\u4e3b\u6027\u548c\u53cd\u9988\u5faa\u73af\u4e09\u5927\u652f\u67f1\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u4f7f\u7528\u5206\u79bb\u7684\u4ee3\u7406\u5206\u522b\u8d1f\u8d23\u4ee3\u7801\u751f\u6210\u548c\u9a8c\u8bc1\uff0c\u63a8\u52a8\u5ba1\u67e5\u3001\u6d4b\u8bd5\u548cQA\u6d41\u7a0b\u7684\u5b8c\u5168\u81ea\u52a8\u5316", "result": "Greptile\u901a\u8fc7\u8fd9\u79cd\u5dee\u5f02\u5316\u7b56\u7565\u5728AI\u4ee3\u7801\u5ba1\u67e5\u5e02\u573a\u4e2d\u5efa\u7acb\u72ec\u7279\u5b9a\u4f4d\uff0c\u5f3a\u8c03\u957f\u671f\u613f\u666f\u800c\u975e\u77ed\u671f\u6027\u80fd\u6307\u6807\uff0c\u4e3a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u5960\u5b9a\u57fa\u7840", "conclusion": "AI\u4ee3\u7801\u5ba1\u67e5\u5e02\u573a\u5b58\u5728\u6ce1\u6cab\uff0c\u9700\u8981\u8d85\u8d8a\u6807\u51c6\u6027\u80fd\u6307\u6807\u7684\u5dee\u5f02\u5316\u7b56\u7565\u3002Greptile\u901a\u8fc7\u72ec\u7acb\u6027\u3001\u81ea\u4e3b\u6027\u548c\u53cd\u9988\u5faa\u73af\u7684\u957f\u671f\u613f\u666f\uff0c\u4e3a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u65b0\u8def\u5f84", "topic": "code agent"}}
