<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 29]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.AI](#cs.AI) [Total: 23]
- [tldr.article](#tldr.article) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Users as Annotators: LLM Preference Learning from Comparison Mode](https://arxiv.org/abs/2510.13830)
*Zhongze Cai,Xiaocheng Li*

Main category: cs.CL

TL;DR: 提出了一种通过用户比较模式收集成对偏好数据的新方法，利用两个不同模型生成响应，通过用户行为模型推断数据质量，并使用EM算法估计用户质量因子来过滤数据。


<details>
  <summary>Details</summary>
Motivation: 传统成对偏好数据依赖专业标注人员，而用户在日常交互中能提供更真实的偏好标签，但缺乏质量控制。本文旨在利用用户标注数据，同时解决其质量问题。

Method: 使用两个不同模型生成响应，通过用户行为模型分析标注质量，开发期望最大化算法估计用户质量因子，并据此过滤数据。

Result: 下游任务验证了该方法在捕捉用户行为和LLM对齐数据过滤方面的有效性。

Conclusion: 用户比较模式能有效收集成对偏好数据，结合质量过滤机制可提升LLM对齐效果。

Abstract: Pairwise preference data have played an important role in the alignment of
large language models (LLMs). Each sample of such data consists of a prompt,
two different responses to the prompt, and a binary label indicating which of
the two responses is better. The labels are usually annotated by professional
human annotators. In this paper, we consider an alternative approach to collect
pairwise preference data -- user annotation from comparison mode. With the
increasingly wider adoption of LLMs among the population, users are
contributing more and more of their preference labels through their daily
interactions with the LLMs. The upside of such labels is that users are the
best experts in judging the responses to their own queries/prompts, but the
downside is the lack of quality control in these labels. In this paper, we
consider a new idea of generating two responses from two different models or
two different versions of the same model. The asymmetry allows us to make an
inference of the user's data quality through our proposed user behavior model.
We develop an expectation-maximization algorithm to estimate a latent quality
factor of the user, and filter users' annotation data accordingly. The
downstream task shows the effectiveness of our approach in both capturing the
user behavior and data filtering for LLM alignment.

</details>


### [2] [SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2510.13836)
*Debarun Bhattacharjya,Balaji Ganesan,Junkyu Lee,Radu Marinescu,Katsiaryna Mirylenka,Michael Glass,Xiao Shou*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型的不确定性量化方法，提出了一种基于相似性的黑盒框架来评估LLM生成结果的置信度，并在问答、摘要和文本转SQL等任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在关键应用中日益重要，需要可靠的不确定性量化方法来评估模型输出的可信度，特别是无需访问模型内部的黑盒方法具有实际应用优势。

Method: 提出了基于相似性的非语言化聚合框架，通过比较生成输出与其他采样生成之间的一致性来估计置信度，并引入了使用小训练集训练置信度估计模型的新技术。

Result: 在问答、摘要和文本转SQL等多样化任务上的实证研究表明，所提出的基于相似性的方法比基线方法能产生更好校准的置信度。

Conclusion: 基于相似性的黑盒不确定性量化方法能够有效评估LLM输出的置信度，为可信AI系统提供了实用的解决方案。

Abstract: When does a large language model (LLM) know what it does not know?
Uncertainty quantification (UQ) provides measures of uncertainty, such as an
estimate of the confidence in an LLM's generated output, and is therefore
increasingly recognized as a crucial component of trusted AI systems. Black-box
UQ methods do not require access to internal model information from the
generating LLM and therefore have numerous real-world advantages, such as
robustness to system changes, adaptability to choice of LLM, reduced costs, and
computational tractability. In this paper, we investigate the effectiveness of
UQ techniques that are primarily but not necessarily entirely black-box, where
the consistency between a generated output and other sampled generations is
used as a proxy for confidence in its correctness. We propose a high-level
non-verbalized similarity-based aggregation framework that subsumes a broad
swath of UQ approaches suitable for complex generative tasks, as well as
introduce specific novel techniques from the framework that train confidence
estimation models using small training sets. Through an empirical study with
datasets spanning the diverse tasks of question answering, summarization, and
text-to-SQL, we demonstrate that our proposed similarity-based methods can
yield better calibrated confidences than baselines.

</details>


### [3] [TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar](https://arxiv.org/abs/2510.14972)
*Yinxi Li,Yuntian Deng,Pengyu Nie*

Main category: cs.CL

TL;DR: 论文提出了TokDrift框架，发现代码LLMs对语义相同但格式不同的代码会产生不同的tokenization，导致模型行为显著变化，这源于子词分词未能捕捉语法标记边界。


<details>
  <summary>Details</summary>
Motivation: 当前代码LLMs使用基于统计的子词分词器，而非语法驱动，导致语义相同的代码片段可能因格式差异而产生不同的tokenization，影响模型可靠性。

Method: 引入TokDrift框架，应用语义保持的重写规则创建仅在tokenization上不同的代码变体，分析9个代码LLMs的行为变化。

Result: 即使微小的格式变化也会导致模型行为显著偏移，层级分析显示问题源于早期嵌入层，子词分词未能捕捉语法标记边界。

Conclusion: 错位的tokenization是代码理解和生成可靠性的隐藏障碍，未来代码LLMs需要语法感知的分词方法。

Abstract: Large language models (LLMs) for code rely on subword tokenizers, such as
byte-pair encoding (BPE), learned from mixed natural language text and
programming language code but driven by statistics rather than grammar. As a
result, semantically identical code snippets can be tokenized differently
depending on superficial factors such as whitespace or identifier naming. To
measure the impact of this misalignment, we introduce TokDrift, a framework
that applies semantic-preserving rewrite rules to create code variants
differing only in tokenization. Across nine code LLMs, including large ones
with over 30B parameters, even minor formatting changes can cause substantial
shifts in model behavior. Layer-wise analysis shows that the issue originates
in early embeddings, where subword segmentation fails to capture grammar token
boundaries. Our findings identify misaligned tokenization as a hidden obstacle
to reliable code understanding and generation, highlighting the need for
grammar-aware tokenization for future code LLMs.

</details>


### [4] [ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups](https://arxiv.org/abs/2510.13852)
*Peter Banyas,Shristi Sharma,Alistair Simmons,Atharva Vispute*

Main category: cs.CL

TL;DR: ConsistencyAI是一个独立基准，用于衡量LLM对不同人物角色的回答事实一致性，发现不同LLM在相同问题上对不同的用户角色会产生事实不一致的回答。


<details>
  <summary>Details</summary>
Motivation: 评估LLM是否会对不同用户角色提供事实不一致的回答，确保模型的事实一致性不因用户身份差异而变化。

Method: 对19个LLM进行测试，每个模型针对15个主题各请求5个事实，重复100次查询，每次使用不同人物角色提示，通过计算跨角色余弦相似度来评估事实一致性。

Result: Grok-3一致性最高，轻量级模型一致性最低；不同主题一致性差异明显，就业市场最不一致，G7领导人最一致；不同供应商在疫苗、巴以冲突等话题上表现分歧。

Conclusion: LLM的事实一致性既受供应商影响，也受话题类型影响，需要开发角色不变提示策略来改善一致性。

Abstract: Is an LLM telling you different facts than it's telling me? This paper
introduces ConsistencyAI, an independent benchmark for measuring the factual
consistency of large language models (LLMs) for different personas.
ConsistencyAI tests whether, when users of different demographics ask identical
questions, the model responds with factually inconsistent answers. Designed
without involvement from LLM providers, this benchmark offers impartial
evaluation and accountability. In our experiment, we queried 19 LLMs with
prompts that requested 5 facts for each of 15 topics. We repeated this query
100 times for each LLM, each time adding prompt context from a different
persona selected from a subset of personas modeling the general population. We
processed the responses into sentence embeddings, computed cross-persona cosine
similarity, and computed the weighted average of cross-persona cosine
similarity to calculate factual consistency scores. In 100-persona experiments,
scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as
a benchmark threshold. xAI's Grok-3 is most consistent, while several
lightweight models rank lowest. Consistency varies by topic: the job market is
least consistent, G7 world leaders most consistent, and issues like vaccines or
the Israeli-Palestinian conflict diverge by provider. These results show that
both the provider and the topic shape the factual consistency. We release our
code and interactive demo to support reproducible evaluation and encourage
persona-invariant prompting strategies.

</details>


### [5] [Harnessing Consistency for Robust Test-Time LLM Ensemble](https://arxiv.org/abs/2510.13855)
*Zhichen Zeng,Qi Yu,Xiao Lin,Ruizhong Qiu,Xuying Ning,Tianxin Wei,Yuchen Yan,Jingrui He,Hanghang Tong*

Main category: cs.CL

TL;DR: CoRE是一种利用模型一致性进行鲁棒LLM集成的即插即用技术，通过token级和模型级一致性处理异构分词方案和模型专业知识差异导致的错误信号。


<details>
  <summary>Details</summary>
Motivation: 不同LLM具有不同的优势和弱点，集成方法能整合它们的互补能力，但现有研究很少关注集成对潜在错误信号的鲁棒性，这些错误通常来自异构分词方案和模型专业知识差异。

Method: 提出CoRE技术，包含token级一致性（通过低通滤波器降低高不一致性token的权重）和模型级一致性（促进高自信且与其他模型差异小的模型输出），可与多种集成方法无缝集成。

Result: 在多样化基准测试、模型组合和集成策略上的广泛实验表明，CoRE能持续提升集成性能和鲁棒性。

Conclusion: CoRE通过利用模型一致性有效解决了LLM集成中的鲁棒性问题，在token级和模型级都能显著提升集成性能。

Abstract: Different large language models (LLMs) exhibit diverse strengths and
weaknesses, and LLM ensemble serves as a promising approach to integrate their
complementary capabilities. Despite substantial progress in improving ensemble
quality, limited attention has been paid to the robustness of ensembles against
potential erroneous signals, which often arise from heterogeneous tokenization
schemes and varying model expertise. Our analysis shows that ensemble failures
typically arise from both the token level and the model level: the former
reflects severe disagreement in token predictions, while the latter involves
low confidence and pronounced disparities among models. In light of this, we
propose CoRE, a plug-and-play technique that harnesses model consistency for
robust LLM ensemble, which can be seamlessly integrated with diverse ensemble
methods. Token-level consistency captures fine-grained disagreements by
applying a low-pass filter to downweight uncertain tokens with high
inconsistency, often due to token misalignment, thereby improving robustness at
a granular level. Model-level consistency models global agreement by promoting
model outputs with high self-confidence and minimal divergence from others,
enhancing robustness at a coarser level. Extensive experiments across diverse
benchmarks, model combinations, and ensemble strategies demonstrate that CoRE
consistently improves ensemble performance and robustness.

</details>


### [6] [Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection](https://arxiv.org/abs/2510.13893)
*Olga E. Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Daniele Nardi*

Main category: cs.CL

TL;DR: 本文通过结构化红队挑战开发了包含50种越狱策略的全面分层分类法，分析了不同攻击类型的流行度和成功率，评估了基于分类法的自动检测方法，并创建了包含1364个多轮对抗对话的意大利语数据集。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要针对单轮攻击，缺乏跨语言覆盖，且分类法有限，无法捕捉攻击策略的多样性或强调风险类别而非越狱技术本身。

Method: 进行结构化红队挑战，开发包含7个家族（冒充、说服、权限提升、认知过载、混淆、目标冲突、数据投毒）的50种越狱策略分层分类法，分析攻击数据，评估自动检测方法，创建多轮对抗对话数据集。

Result: 建立了全面的越狱策略分类体系，揭示了不同攻击策略如何利用模型漏洞导致失准，展示了分类法引导提示在改进自动检测方面的益处，提供了用于研究渐进式对抗意图的新数据集。

Conclusion: 该研究推进了对越狱技术有效性的理解，提供了系统化的分类框架和评估方法，为改进LLM安全防御提供了重要基础。

Abstract: Jailbreaking techniques pose a significant threat to the safety of Large
Language Models (LLMs). Existing defenses typically focus on single-turn
attacks, lack coverage across languages, and rely on limited taxonomies that
either fail to capture the full diversity of attack strategies or emphasize
risk categories rather than the jailbreaking techniques. To advance the
understanding of the effectiveness of jailbreaking techniques, we conducted a
structured red-teaming challenge. The outcome of our experiments are manifold.
First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak
strategies, consolidating and extending prior classifications into seven broad
families, including impersonation, persuasion, privilege escalation, cognitive
overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed
the data collected from the challenge to examine the prevalence and success
rates of different attack types, providing insights into how specific jailbreak
strategies exploit model vulnerabilities and induce misalignment. Third, we
benchmark a popular LLM for jailbreak detection, evaluating the benefits of
taxonomy-guided prompting for improving automatic detection. Finally, we
compiled a new Italian dataset of 1364 multi-turn adversarial dialogues,
annotated with our taxonomy, enabling the study of interactions where
adversarial intent emerges gradually and succeeds in bypassing traditional
safeguards.

</details>


### [7] [Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges](https://arxiv.org/abs/2510.13898)
*Misam Abbas*

Main category: cs.CL

TL;DR: 本文比较了两种AI生成文本作者归属方法：固定风格嵌入和指令调优的LLM判断器（GPT-4o），在包含6个领域600个实例的Human AI Parallel Corpus上进行测试。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成的文本质量接近人类写作，准确归属作者身份变得越来越困难，需要开发有效的归属机制。

Method: 使用固定风格嵌入和GPT-4o LLM判断器两种方法，在包含人类提示、黄金延续和LLM生成延续的平衡数据集上进行测试。

Result: 风格嵌入在GPT延续上表现更好（82% vs 68%），LLM判断器在LLaMA延续上略优（85% vs 81%），但差异不显著。LLM判断器在小说和学术文本中表现突出，而嵌入在口语和脚本对话中占优。

Conclusion: 归属是一个多维问题，需要混合策略。提供了开源框架用于AI生成内容归属质量评估。

Abstract: Attributing authorship in the era of large language models (LLMs) is
increasingly challenging as machine-generated prose rivals human writing. We
benchmark two complementary attribution mechanisms , fixed Style Embeddings and
an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an
open dataset of 600 balanced instances spanning six domains (academic, news,
fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance
contains a human prompt with both a gold continuation and an LLM-generated
continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding
baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs.
68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA
continuations (85 pct vs. 81 pct) but the results are not statistically
significant. Crucially, the LLM judge significantly outperforms in fiction and
academic prose, indicating semantic sensitivity, whereas embeddings dominate in
spoken and scripted dialogue, reflecting structural strengths. These
complementary patterns highlight attribution as a multidimensional problem
requiring hybrid strategies. To support reproducibility we provide code on
GitHub and derived data on Hugging Face under the MIT license. This open
framework provides a reproducible benchmark for attribution quality assessment
in AI-generated content, along with a review of related literature influencing
this work.

</details>


### [8] [Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences](https://arxiv.org/abs/2510.13900)
*Julian Minder,Clément Dumas,Stewart Slocum,Helena Casademunt,Cameron Holmes,Robert West,Neel Nanda*

Main category: cs.CL

TL;DR: 窄域微调会在LLM激活中产生强偏差，这些偏差可以通过模型差异分析来理解微调领域，并用于生成类似微调数据的内容。研究表明这些偏差反映了过拟合问题，对AI安全和可解释性研究具有重要影响。


<details>
  <summary>Details</summary>
Motivation: 研究窄域微调如何影响LLM的激活模式，以及这些偏差如何揭示微调领域的信息，为模型差异分析、AI安全和可解释性研究提供新视角。

Method: 使用模型差异分析工具，分析微调前后模型在随机文本前几个token上的激活差异，并通过在模型激活中添加这些差异来引导文本生成。创建基于LLM的可解释性代理来理解微调领域。

Result: 发现窄域微调产生的激活偏差包含微调领域的关键信息；使用这些偏差的代理性能显著优于简单提示的基线代理；混合预训练数据到微调语料中可以大幅消除这些偏差。

Conclusion: 窄域微调模型在其激活中存在训练目标的显著痕迹；使用此类模型作为研究更广泛微调（如聊天调优）的代理可能不现实；需要深入研究窄域微调的影响并开发更真实的案例研究。

Abstract: Finetuning on narrow domains has become an essential tool to adapt Large
Language Models (LLMs) to specific tasks and to create models with known
unusual properties that are useful for research. We show that narrow finetuning
creates strong biases in LLM activations that can be interpreted to understand
the finetuning domain. These biases can be discovered using simple tools from
model diffing - the study of differences between models before and after
finetuning. In particular, analyzing activation differences on the first few
tokens of random text and steering by adding this difference to the model
activations produces text similar to the format and general content of the
finetuning data. We demonstrate that these analyses contain crucial information
by creating an LLM-based interpretability agent to understand the finetuning
domain. With access to the bias, the agent performs significantly better
compared to baseline agents using simple prompting. Our analysis spans
synthetic document finetuning for false facts, emergent misalignment,
subliminal learning, and taboo word guessing game models across different
architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We
suspect these biases reflect overfitting and find that mixing pretraining data
into the finetuning corpus largely removes them, though residual risks may
remain. Our work (1) demonstrates that narrowly finetuned models have salient
traces of their training objective in their activations and suggests ways to
improve how they are trained, (2) warns AI safety and interpretability
researchers that the common practice of using such models as a proxy for
studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3)
highlights the need for deeper investigation into the effects of narrow
finetuning and development of truly realistic case studies for model-diffing,
safety and interpretability research.

</details>


### [9] [RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs](https://arxiv.org/abs/2510.13901)
*Tuan T. Nguyen,John Le,Thai T. Vu,Willy Susilo,Heath Cooper*

Main category: cs.CL

TL;DR: RAID框架通过优化连续嵌入空间中的对抗性后缀，有效绕过LLM的安全机制，在保持流畅性的同时诱导受限内容生成。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能出色，但仍容易受到越狱攻击的威胁，需要系统性地探测这些安全弱点。

Method: RAID将离散token松弛为连续嵌入，通过联合目标函数优化：鼓励受限响应、结合拒绝感知正则化器、应用连贯性约束，最后通过批评引导的解码过程将嵌入映射回token。

Result: 在多个开源LLM上的实验表明，RAID相比现有白盒和黑盒基线方法，能以更少的查询和更低的计算成本获得更高的攻击成功率。

Conclusion: 嵌入空间正则化对于理解和缓解LLM越狱漏洞具有重要意义。

Abstract: Large language models (LLMs) achieve impressive performance across diverse
tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms.
We present RAID (Refusal-Aware and Integrated Decoding), a framework that
systematically probes these weaknesses by crafting adversarial suffixes that
induce restricted content while preserving fluency. RAID relaxes discrete
tokens into continuous embeddings and optimizes them with a joint objective
that (i) encourages restricted responses, (ii) incorporates a refusal-aware
regularizer to steer activations away from refusal directions in embedding
space, and (iii) applies a coherence term to maintain semantic plausibility and
non-redundancy. After optimization, a critic-guided decoding procedure maps
embeddings back to tokens by balancing embedding affinity with language-model
likelihood. This integration yields suffixes that are both effective in
bypassing defenses and natural in form. Experiments on multiple open-source
LLMs show that RAID achieves higher attack success rates with fewer queries and
lower computational cost than recent white-box and black-box baselines. These
findings highlight the importance of embedding-space regularization for
understanding and mitigating LLM jailbreak vulnerabilities.

</details>


### [10] [Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory](https://arxiv.org/abs/2510.13902)
*Nicole Smith-Vaniz,Harper Lyon,Lorraine Steigner,Ben Armstrong,Nicholas Mattei*

Main category: cs.CL

TL;DR: 该研究分析了大型语言模型在道德和政治立场上的潜在偏见，使用道德基础理论框架，比较LLM回应与人类数据的差异，评估模型是否表现出意识形态倾向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在日常生活中的广泛应用，特别是在医疗、人际关系和法律等关键领域作为建议提供者，需要了解它们在困难的政治和道德领域中的回应方式和潜在偏见。

Method: 使用道德基础理论框架，分析LLM的回应与现有人类研究的区别，通过直接回应、明确政治意识形态提示和基于人口统计的角色扮演三种条件进行系统评估。

Result: 研究发现LLMs在某些条件下确实表现出意识形态倾向，无论是通过固有回应、明确政治意识形态表达，还是通过构建人类角色的视角回应。

Conclusion: 该研究揭示了AI生成回应中存在政治和人口统计依赖性的程度，为理解LLMs在敏感领域的偏见提供了重要见解。

Abstract: Large Language Models (LLMs) have become increasingly incorporated into
everyday life for many internet users, taking on significant roles as advice
givers in the domains of medicine, personal relationships, and even legal
matters. The importance of these roles raise questions about how and what
responses LLMs make in difficult political and moral domains, especially
questions about possible biases. To quantify the nature of potential biases in
LLMs, various works have applied Moral Foundations Theory (MFT), a framework
that categorizes human moral reasoning into five dimensions: Harm, Fairness,
Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to
measure differences in human participants along political, national, and
cultural lines. While there has been some analysis of the responses of LLM with
respect to political stance in role-playing scenarios, no work so far has
directly assessed the moral leanings in the LLM responses, nor have they
connected LLM outputs with robust human data. In this paper we analyze the
distinctions between LLM MFT responses and existing human research directly,
investigating whether commonly available LLM responses demonstrate ideological
leanings: either through their inherent responses, straightforward
representations of political ideologies, or when responding from the
perspectives of constructed human personas. We assess whether LLMs inherently
generate responses that align more closely with one political ideology over
another, and additionally examine how accurately LLMs can represent ideological
perspectives through both explicit prompting and demographic-based
role-playing. By systematically analyzing LLM behavior across these conditions
and experiments, our study provides insight into the extent of political and
demographic dependency in AI-generated responses.

</details>


### [11] [RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems](https://arxiv.org/abs/2510.13910)
*Jingru Lin,Chen Zhang,Stephen Y. Liu,Haizhou Li*

Main category: cs.CL

TL;DR: 提出了RAGCap-Bench基准，用于细粒度评估代理式RAG工作流中的中间任务能力，发现具备更强中间能力的大模型在端到端任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有代理式RAG系统在处理复杂多跳问题时仍存在困难，且其中间推理能力研究不足，需要专门的评估基准。

Method: 分析最先进系统的输出，识别常见任务和核心能力，构建错误分类法，设计针对性评估问题。

Result: 实验表明具备更强RAGCap性能的"慢思考"模型在端到端任务中表现更好，验证了基准的有效性。

Conclusion: 增强中间推理能力对提升代理式RAG系统性能至关重要，RAGCap-Bench为评估这些能力提供了有效工具。

Abstract: Retrieval-Augmented Generation (RAG) mitigates key limitations of Large
Language Models (LLMs)-such as factual errors, outdated knowledge, and
hallucinations-by dynamically retrieving external information. Recent work
extends this paradigm through agentic RAG systems, where LLMs act as agents to
iteratively plan, retrieve, and reason over complex queries. However, these
systems still struggle with challenging multi-hop questions, and their
intermediate reasoning capabilities remain underexplored. To address this, we
propose RAGCap-Bench, a capability-oriented benchmark for fine-grained
evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs
from state-of-the-art systems to identify common tasks and the core
capabilities required for their execution, then construct a taxonomy of typical
LLM errors to design targeted evaluation questions. Experiments show that
"slow-thinking" models with stronger RAGCap performance achieve better
end-to-end results, underscoring the benchmark's validity and the importance of
enhancing these intermediate capabilities.

</details>


### [12] [Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement Mechanisms](https://arxiv.org/abs/2510.13913)
*Shrey Pandit,Xuan-Phi Nguyen,Yifei Ming,Austin Xu,Jiayu Wang,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 提出了一种两阶段数据合成管道，通过逐步增加任务复杂度生成问答对，用于训练更有效的网页研究代理。


<details>
  <summary>Details</summary>
Motivation: 当前基于网页的深度研究代理在处理长时程推理任务时面临挑战，现有指令调优数据集缺乏对难度和质量的细粒度控制，且难以评估数据本身的有效性。

Method: 使用两阶段数据合成管道，通过逐步增加任务复杂度直到基线网页代理失败来生成问答对，基线代理负责尝试问题、验证事实性、检查替代答案和执行过滤。

Result: 实验表明，尽管数据集规模较小，但训练出的网页代理在多个基准测试中表现优于现有数据集，工具使用动作多样性达到两倍，同时避免了重复的工具调用行为。

Conclusion: 该方法能够生成更高质量的训练数据，有效提升网页代理的长时程推理能力。

Abstract: Web-based 'deep research' agents aim to solve complex question - answering
tasks through long-horizon interactions with online tools. These tasks remain
challenging, as the underlying language models are often not optimized for
long-horizon reasoning and exploration. Prior work has proposed workflows for
constructing instruction-tuning datasets, often leveraging knowledge graphs.
However, such methods typically lack fine-grained control over difficulty and
quality, yielding synthetic data that falls short of capturing the complexity
required for long-horizon reasoning. Furthermore, many studies conflate data
and training effects by comparing models trained under different optimization
recipes, making it difficult to isolate and evaluate the effectiveness of the
data itself. We introduce a two-pronged data synthesis pipeline that generates
question - answer pairs by progressively increasing task complexity until a
frontier baseline web agent fails. The baseline agent plays multiple roles in
this process: attempting the questions, validating factuality, checking for
alternative answers, and enforcing filtering. To evaluate the effectiveness of
our synthesis methods, we adopt a controlled training setup based on
distillation from strong web agents. Experiments across multiple web-based
benchmarks show that our dataset - despite being smaller - enables the training
of more effective web agents than existing datasets. In particular, our data
exhibits twice the diversity in tool-use actions, allowing models trained on it
to achieve stronger performance while avoiding repetitive tool-calling
behaviors.

</details>


### [13] [Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time Scaling](https://arxiv.org/abs/2510.13918)
*Peng Kuang,Yanli Wang,Xiaoyu Han,Yaowenqi Liu,Kaidi Xu,Haohan Wang*

Main category: cs.CL

TL;DR: 提出了一个理论框架来优化过程奖励模型(PRM)在测试时扩展(TTS)中的使用，通过加权聚合策略显著提升效率，仅需21.3%的计算量就能超越传统加权多数投票。


<details>
  <summary>Details</summary>
Motivation: 现有PRM在测试时扩展中表现不稳定，有时甚至被简单的多数投票超越，需要找到更有效利用PRM验证信号的方法。

Method: 开发理论框架分析LLM和PRM信号的最优组合策略，提出加权聚合方法，并通过预计算校准权重函数。

Result: 在5个LLM和7个PRM上的实验表明，校准方法显著提升TTS效率，仅用21.3%计算量就超越传统加权多数投票。

Conclusion: 投资更智能的聚合策略比单纯扩展测试时计算更能有效提升性能。

Abstract: Process reward models (PRMs) are a cornerstone of test-time scaling (TTS),
designed to verify and select the best responses from large language models
(LLMs). However, this promise is challenged by recent benchmarks where simple
majority voting, which ignores PRM signals, occasionally outperforms standard
PRM-based selection. This raises a critical question: How can we effectively
utilize verification signals from PRMs for TTS? To address this, we start by
developing a theoretical framework for optimally combining signals from both
the LLM and the PRM. Our framework reveals that the optimal strategy is a
weighted aggregation of responses, a strategy whose effectiveness hinges on
estimating weights that capture the complex interplay between the models. Based
on our theoretical results, we empirically show that these optimal weighting
functions differ significantly across LLM-PRM pairs and, notably, often assign
substantial negative weights. Motivated by these insights, we propose efficient
pre-computation methods to calibrate these weighting functions. Extensive
experiments across 5 LLMs and 7 PRMs demonstrate that our calibration method
significantly boosts the TTS efficiency, surpassing the performance of vanilla
weighted majority voting while using only $21.3\%$ of the computation.
Ultimately, our work demonstrates that investing in a more intelligent
aggregation strategy can be a more convincing path to performance gains than
simply scaling test-time computation.

</details>


### [14] [An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation](https://arxiv.org/abs/2510.13925)
*Daniel Adu Worae,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: 提出基于LLM的AI代理框架，将原始网络流量数据转换为结构化语义表示，通过混合检索和推理实现IoT网络流量的高效分析。


<details>
  <summary>Details</summary>
Motivation: IoT网络产生多样化高流量数据，需要跨层行为解释而非孤立检测，传统方法难以有效处理此类复杂流量分析。

Method: 集成特征提取、基于transformer的异常检测、流量摘要、威胁情报增强和检索增强问答的AI代理框架，使用混合检索（词法和语义搜索加重排序）。

Result: 在多个IoT数据集和六个开源模型上评估，混合检索显著提升BLEU、ROUGE、METEOR和BERTScore指标，系统性能开销低。

Conclusion: 该框架实现了IoT网络流量的全面高效解释，混合检索方法优于纯密集检索。

Abstract: Internet of Things (IoT) networks generate diverse and high-volume traffic
that reflects both normal activity and potential threats. Deriving meaningful
insight from such telemetry requires cross-layer interpretation of behaviors,
protocols, and context rather than isolated detection. This work presents an
LLM-powered AI agent framework that converts raw packet captures into
structured and semantically enriched representations for interactive analysis.
The framework integrates feature extraction, transformer-based anomaly
detection, packet and flow summarization, threat intelligence enrichment, and
retrieval-augmented question answering. An AI agent guided by a large language
model performs reasoning over the indexed traffic artifacts, assembling
evidence to produce accurate and human-readable interpretations. Experimental
evaluation on multiple IoT captures and six open models shows that hybrid
retrieval, which combines lexical and semantic search with reranking,
substantially improves BLEU, ROUGE, METEOR, and BERTScore results compared with
dense-only retrieval. System profiling further indicates low CPU, GPU, and
memory overhead, demonstrating that the framework achieves holistic and
efficient interpretation of IoT network traffic.

</details>


### [15] [LLMs Can Get "Brain Rot"!](https://arxiv.org/abs/2510.13928)
*Shuo Xing,Junyuan Hong,Yifan Wang,Runjin Chen,Zhenyu Zhang,Ananth Grama,Zhengzhong Tu,Zhangyang Wang*

Main category: cs.CL

TL;DR: LLM Brain Rot Hypothesis：持续暴露于垃圾网络文本会导致大语言模型认知能力持续下降。通过控制实验发现，在垃圾数据上持续预训练会显著降低模型的推理、长上下文理解、安全性等能力，并增强"黑暗人格特质"。


<details>
  <summary>Details</summary>
Motivation: 研究数据质量对LLM能力的因果影响，验证垃圾网络文本是否会导致模型认知能力下降，为持续预训练的数据筛选提供安全指导。

Method: 使用Twitter/X语料构建垃圾和控制数据集，通过两种正交操作化方法（M1：参与度，M2：语义质量）进行控制实验，在4个LLM上进行持续预训练，评估认知能力变化。

Result: 垃圾数据训练导致推理能力显著下降（ARC-Challenge从74.9降至57.2），长上下文理解下降（RULER-CWE从84.4降至52.3），安全性降低，黑暗人格特质增强。错误分析发现思维跳跃是主要病变。

Conclusion: 数据质量是LLM能力下降的因果驱动因素，需要将持续预训练的数据筛选视为训练时安全问题，并定期进行"认知健康检查"。

Abstract: We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk
web text induces lasting cognitive decline in large language models (LLMs). To
causally isolate data quality, we run controlled experiments on real Twitter/X
corpora, constructing junk and reversely controlled datasets via two orthogonal
operationalizations: M1 (engagement degree) and M2 (semantic quality), with
matched token scale and training operations across conditions. Contrary to the
control group, continual pre-training of 4 LLMs on the junk dataset causes
non-trivial declines (Hedges' $g>0.3$) on reasoning, long-context
understanding, safety, and inflating "dark traits" (e.g., psychopathy,
narcissism). The gradual mixtures of junk and control datasets also yield
dose-response cognition decay: for example, under M1, ARC-Challenge with Chain
Of Thoughts drops $74.9 \rightarrow 57.2$ and RULER-CWE $84.4 \rightarrow 52.3$
as junk ratio rises from $0\%$ to $100\%$.
  Error forensics reveal several key insights. First, we identify
thought-skipping as the primary lesion: models increasingly truncate or skip
reasoning chains, explaining most of the error growth. Second, partial but
incomplete healing is observed: scaling instruction tuning and clean data
pre-training improve the declined cognition yet cannot restore baseline
capability, suggesting persistent representational drift rather than format
mismatch. Finally, we discover that the popularity, a non-semantic metric, of a
tweet is a better indicator of the Brain Rot effect than the length in M1.
Together, the results provide significant, multi-perspective evidence that data
quality is a causal driver of LLM capability decay, reframing curation for
continual pretraining as a \textit{training-time safety} problem and motivating
routine "cognitive health checks" for deployed LLMs.

</details>


### [16] [FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis](https://arxiv.org/abs/2510.13936)
*Fengbin Zhu,Xiang Yao Ng,Ziyang Liu,Chang Liu,Xianwei Zeng,Chao Wang,Tianhui Tan,Xuan Yao,Pengyang Shao,Min Xu,Zixuan Wang,Jing Wang,Xin Lin,Junfeng Li,Jingxian Zhu,Yang Zhang,Wenjie Wang,Fuli Feng,Richang Hong,Huanbo Luan,Ke-Wei Huang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 提出了HisRubric评估框架和FinDeepResearch基准，用于系统评估深度研究代理在金融分析中的能力，涵盖8个金融市场、4种语言的64家上市公司，共15,808个评分项。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对深度研究代理在关键研究分析能力方面的严谨系统评估，特别是在企业财务分析领域。

Method: 提出HisRubric分层分析框架，模拟专业分析师工作流程：从数据识别到指标计算，再到战略总结和解释。构建FinDeepResearch基准，测试16种代表性方法（包括6个DR代理、5个具备深度推理和搜索能力的LLM、5个仅具备深度推理能力的LLM）。

Result: 揭示了这些方法在不同能力、金融市场和语言方面的优势和局限性，为未来研究提供了宝贵见解。

Conclusion: 该研究填补了深度研究代理评估的空白，提出的框架和基准将公开可用，有助于推动该领域的发展。

Abstract: Deep Research (DR) agents, powered by advanced Large Language Models (LLMs),
have recently garnered increasing attention for their capability in conducting
complex research tasks. However, existing literature lacks a rigorous and
systematic evaluation of DR Agent's capabilities in critical research analysis.
To address this gap, we first propose HisRubric, a novel evaluation framework
with a hierarchical analytical structure and a fine-grained grading rubric for
rigorously assessing DR agents' capabilities in corporate financial analysis.
This framework mirrors the professional analyst's workflow, progressing from
data recognition to metric calculation, and finally to strategic summarization
and interpretation. Built on this framework, we construct a FinDeepResearch
benchmark that comprises 64 listed companies from 8 financial markets across 4
languages, encompassing a total of 15,808 grading items. We further conduct
extensive experiments on the FinDeepResearch using 16 representative methods,
including 6 DR agents, 5 LLMs equipped with both deep reasoning and search
capabilities, and 5 LLMs with deep reasoning capabilities only. The results
reveal the strengths and limitations of these approaches across diverse
capabilities, financial markets, and languages, offering valuable insights for
future research and development. The benchmark and evaluation code will be made
publicly available.

</details>


### [17] [ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models](https://arxiv.org/abs/2510.14077)
*Haziq Mohammad Khalid,Athikash Jeyaganthan,Timothy Do,Yicheng Fu,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: ERGO通过基于熵的不确定性检测和自适应提示整合，显著提升LLM在多轮对话中的性能表现


<details>
  <summary>Details</summary>
Motivation: LLM在多轮增量信息对话中性能显著下降，这限制了其实际应用价值

Method: 使用香农熵量化下一个token分布的不确定性，检测到熵值急剧上升时触发自适应提示整合

Result: 在增量指令任务中，ERGO相比基线平均性能提升56.6%，峰值能力提升24.7%，不可靠性降低35.3%

Conclusion: 将不确定性作为首要信号而非需要消除的干扰，能够显著提升对话AI的准确性和可靠性

Abstract: Large Language Models (LLMs) suffer significant performance degradation in
multi-turn conversations when information is presented incrementally. Given
that multi-turn conversations characterize everyday interactions with LLMs,
this degradation poses a severe challenge to real world usability. We
hypothesize that abrupt increases in model uncertainty signal misalignment in
multi-turn LLM interactions, and we exploit this insight to dynamically realign
conversational context. We introduce ERGO (Entropy-guided Resetting for
Generation Optimization), which continuously quantifies internal uncertainty
via Shannon entropy over next token distributions and triggers adaptive prompt
consolidation when a sharp spike in entropy is detected. By treating
uncertainty as a first class signal rather than a nuisance to eliminate, ERGO
embraces variability in language and modeling, representing and responding to
uncertainty. In multi-turn tasks with incrementally revealed instructions, ERGO
yields a 56.6% average performance gain over standard baselines, increases
aptitude (peak performance capability) by 24.7%, and decreases unreliability
(variability in performance) by 35.3%, demonstrating that uncertainty aware
interventions can improve both accuracy and reliability in conversational AI.

</details>


### [18] [RLSR: Reinforcement Learning with Supervised Reward Outperforms SFT in Instruction Following](https://arxiv.org/abs/2510.14200)
*Zhichao Wang,Andy Wong,Ruslan Belkin*

Main category: cs.CL

TL;DR: 提出RLSR方法替代SFT，利用强化学习框架增强基础模型的指令跟随能力，在指令跟随基准测试中表现优于SFT


<details>
  <summary>Details</summary>
Motivation: 受RFT启发，希望利用强化学习框架替代传统的监督微调方法，更好地利用SFT数据集提升模型的指令跟随能力

Method: RLSR方法：基础模型为每个提示生成多个响应，通过语义嵌入空间中的余弦相似度计算生成响应与人工标注响应之间的奖励分数

Result: RLSR在指令跟随基准测试中表现优于SFT（26.34% vs 21.01%），SFT+RLSR组合进一步提升了性能（30.73%）

Conclusion: RLSR可以替代SFT或与SFT结合使用，有效提升模型的指令跟随能力和下游任务性能

Abstract: After the pretraining stage of LLMs, techniques such as SFT, RLHF, RLVR, and
RFT are applied to enhance instruction-following ability, mitigate undesired
responses, improve reasoning capability and enable efficient domain adaptation
with minimal data. SFT relies on the next-token prediction objective to
strengthen instruction following in a base model using a large corpus of
human-labeled responses. In contrast, RFT employs a RL-based approach to adapt
fine-tuned reasoning models to specific domains with limited supervision.
Inspired by RFT, we propose replacing SFT with RLSR to leverage the extensive
SFT dataset in an RL framework, thereby improving the base model's
instruction-following ability. In RLSR, the base model generates multiple
responses for each prompt, and reward scores are computed as the cosine
similarity in the semantic embedding space between the generated and
human-labeled responses. RLSR can be utilized in multiple ways. It can directly
replace SFT, achieving superior performance on instruction-following
benchmarks-for example, RLSR (SB) on Qwen-7B (INFINITY) achieved an AlpacaEval
win rate of 26.34%, surpassing SFT's 21.01%. Furthermore, combining SFT and
RLSR further enhances downstream task performance; Qwen-7B (INFINITY) achieved
a win rate of 30.73% when trained with SFT + RLSR.

</details>


### [19] [DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans](https://arxiv.org/abs/2510.14205)
*Bingsheng Yao,Bo Sun,Yuanzhe Dong,Yuxuan Lu,Dakuo Wang*

Main category: cs.CL

TL;DR: 提出了动态角色精炼框架(DPRF)，通过迭代识别生成行为与人类真实行为之间的认知差异，并优化角色配置文件，以提高LLM角色扮演代理的行为对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM角色扮演代理的角色保真度常因手动创建的角色配置文件（如精选信息和个性特征）而未经验证与目标个体的对齐度，这限制了其行为模拟的真实性。

Method: DPRF框架通过自由形式或理论基础的认知差异分析，迭代识别生成行为与人类真实行为之间的差异，并据此优化角色配置文件。在五个LLM和四个行为预测场景（正式辩论、社交媒体帖子、公开访谈、电影评论）上进行了评估。

Result: DPRF能够显著提高行为对齐度，优于基线角色配置，并在不同模型和场景中具有良好泛化能力。

Conclusion: 该工作为创建高保真角色配置文件和增强下游应用（如用户模拟、社会研究、个性化AI）的有效性提供了稳健的方法论。

Abstract: The emerging large language model role-playing agents (LLM RPAs) aim to
simulate individual human behaviors, but the persona fidelity is often
undermined by manually-created profiles (e.g., cherry-picked information and
personality characteristics) without validating the alignment with the target
individuals. To address this limitation, our work introduces the Dynamic
Persona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM
RPAs' behaviors with those of target individuals by iteratively identifying the
cognitive divergence, either through free-form or theory-grounded, structured
analysis, between generated behaviors and human ground truth, and refining the
persona profile to mitigate these divergences.We evaluate DPRF with five LLMs
on four diverse behavior-prediction scenarios: formal debates, social media
posts with mental health issues, public interviews, and movie reviews.DPRF can
consistently improve behavioral alignment considerably over baseline personas
and generalizes across models and scenarios.Our work provides a robust
methodology for creating high-fidelity persona profiles and enhancing the
validity of downstream applications, such as user simulation, social studies,
and personalized AI.

</details>


### [20] [Rewriting History: A Recipe for Interventional Analyses to Study Data Effects on Model Behavior](https://arxiv.org/abs/2510.14261)
*Rahul Nadkarni,Yanai Elazar,Hila Gonen,Noah A. Smith*

Main category: cs.CL

TL;DR: 提出了一种实验方法研究训练数据与语言模型行为的关系，通过干预数据批次和重新训练模型来测试数据与行为之间的假设关系。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据如何影响语言模型行为，特别是事实知识获取，补充过去基于共现统计的观察性分析。

Method: 通过干预数据批次（"重写历史"）、选择评估项目、匹配相关文档、修改文档后重新训练模型并测量效果。

Result: 现有识别相关训练文档的方法不能完全解释语言模型正确回答知识问题的能力，共现统计与模型行为存在关联但非完全解释。

Conclusion: 提供了一个可复用的实验框架，供研究人员进一步测试训练数据如何影响模型行为的假设。

Abstract: We present an experimental recipe for studying the relationship between
training data and language model (LM) behavior. We outline steps for
intervening on data batches -- i.e., ``rewriting history'' -- and then
retraining model checkpoints over that data to test hypotheses relating data to
behavior. Our recipe breaks down such an intervention into stages that include
selecting evaluation items from a benchmark that measures model behavior,
matching relevant documents to those items, and modifying those documents
before retraining and measuring the effects. We demonstrate the utility of our
recipe through case studies on factual knowledge acquisition in LMs, using both
cooccurrence statistics and information retrieval methods to identify documents
that might contribute to knowledge learning. Our results supplement past
observational analyses that link cooccurrence to model behavior, while
demonstrating that extant methods for identifying relevant training documents
do not fully explain an LM's ability to correctly answer knowledge questions.
Overall, we outline a recipe that researchers can follow to test further
hypotheses about how training data affects model behavior. Our code is made
publicly available to promote future work.

</details>


### [21] [PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.14278)
*Md Mahadi Hasan Nahid,Davood Rafiei*

Main category: cs.CL

TL;DR: 提出了一种基于LLM的智能检索系统，通过三个专门代理（问题分析器、选择器和添加器）的迭代交互，在多跳问答任务中实现高精度和高召回率的证据检索。


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要检索多个相关证据，传统检索方法在精度和召回率方面存在不足，需要更智能的检索系统来提高证据质量。

Method: 使用三个专门代理：问题分析器分解多跳问题为子问题，选择器专注于精度地识别相关上下文，添加器专注于召回率地补充缺失证据，通过迭代交互获得紧凑而全面的支持段落。

Result: 在HotpotQA、2WikiMultiHopQA、MuSiQue和MultiHopRAG四个多跳问答基准测试中，该方法持续优于强基线，实现了更高的检索准确率。

Conclusion: 该智能检索系统能够有效过滤干扰内容，提高下游问答模型的准确性，同时减少对无关信息的依赖。

Abstract: Retrieval plays a central role in multi-hop question answering (QA), where
answering complex questions requires gathering multiple pieces of evidence. We
introduce an Agentic Retrieval System that leverages large language models
(LLMs) in a structured loop to retrieve relevant evidence with high precision
and recall. Our framework consists of three specialized agents: a Question
Analyzer that decomposes a multi-hop question into sub-questions, a Selector
that identifies the most relevant context for each sub-question (focusing on
precision), and an Adder that brings in any missing evidence (focusing on
recall). The iterative interaction between Selector and Adder yields a compact
yet comprehensive set of supporting passages. In particular, it achieves higher
retrieval accuracy while filtering out distracting content, enabling downstream
QA models to surpass full-context answer accuracy while relying on
significantly less irrelevant information. Experiments on four multi-hop QA
benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG --
demonstrates that our approach consistently outperforms strong baselines.

</details>


### [22] [Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL](https://arxiv.org/abs/2510.14318)
*Marwa Abdulhai,Ryan Cheng,Aryansh Shrivastava,Natasha Jaques,Yarin Gal,Sergey Levine*

Main category: cs.CL

TL;DR: 本文研究了LLM在对话中的欺骗行为，提出了信念错位指标来量化欺骗，并开发了多轮强化学习方法减少LLM的欺骗行为。


<details>
  <summary>Details</summary>
Motivation: LLM在客服、教育、医疗等应用中广泛使用，但其产生欺骗性输出的能力存在严重安全隐患，需要量化评估和缓解。

Method: 使用五种现有欺骗检测指标和新提出的信念错位指标，在四种对话场景中评估欺骗；引入多轮强化学习方法来微调LLM以减少欺骗行为。

Result: LLM在约26%的对话轮次中自然表现出欺骗行为；被提示欺骗时欺骗性可增加31%；RLHF训练模型仍有43%的欺骗率；多轮强化学习方法使欺骗行为减少77.6%。

Conclusion: 对话中的欺骗是随时间发展的行为，需要超越单轮分析；多轮强化学习能有效减少LLM的欺骗行为。

Abstract: Large Language Models (LLMs) interact with millions of people worldwide in
applications such as customer support, education and healthcare. However, their
ability to produce deceptive outputs, whether intentionally or inadvertently,
poses significant safety concerns. The unpredictable nature of LLM behavior,
combined with insufficient safeguards against hallucination, misinformation,
and user manipulation, makes their misuse a serious, real-world risk. In this
paper, we investigate the extent to which LLMs engage in deception within
dialogue, and propose the belief misalignment metric to quantify deception. We
evaluate deception across four distinct dialogue scenarios, using five
established deception detection metrics and our proposed metric. Our findings
reveal this novel deception measure correlates more closely with human
judgments than any existing metrics we test. Additionally, our benchmarking of
eight state-of-the-art models indicates that LLMs naturally exhibit deceptive
behavior in approximately 26% of dialogue turns, even when prompted with
seemingly benign objectives. When prompted to deceive, LLMs are capable of
increasing deceptiveness by as much as 31% relative to baselines. Unexpectedly,
models trained with RLHF, the predominant approach for ensuring the safety of
widely-deployed LLMs, still exhibit deception at a rate of 43% on average.
Given that deception in dialogue is a behavior that develops over an
interaction history, its effective evaluation and mitigation necessitates
moving beyond single-utterance analyses. We introduce a multi-turn
reinforcement learning methodology to fine-tune LLMs to reduce deceptive
behaviors, leading to a 77.6% reduction compared to other instruction-tuned
models.

</details>


### [23] [On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?](https://arxiv.org/abs/2510.14365)
*Anyun Zhuo,Xuefei Ning,Ningyuan Li,Yu Wang,Pinyan Lu*

Main category: cs.CL

TL;DR: 本文研究了LLMs对字符级扰动的鲁棒性，通过插入不可见Unicode控制字符来防止LLM滥用，发现即使tokenization被严重破坏，许多LLM仍能保持显著性能。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在面对结构化字符级扰动时的鲁棒性，特别是在在线考试系统等场景中防止LLM滥用的需求。

Method: 引入了NAMESHORT方法，通过在文本中插入不可见Unicode控制字符来破坏tokenization，并评估了模型、问题和噪声相关配置下的性能变化。

Result: 尽管字符级噪声显著降低了信噪比并破坏了tokenization，但许多LLM仍表现出显著的鲁棒性，维持了可观的性能水平。

Conclusion: LLMs在字符级噪声下展现出意外的鲁棒性，这揭示了在多样化应用中部署LLM的风险和可靠性问题。

Abstract: This work investigates the resilience of contemporary LLMs against frequent
and structured character-level perturbations, specifically through the
insertion of noisy characters after each input character. We introduce
\nameshort{}, a practical method that inserts invisible Unicode control
characters into text to discourage LLM misuse in scenarios such as online exam
systems. Surprisingly, despite strong obfuscation that fragments tokenization
and reduces the signal-to-noise ratio significantly, many LLMs still maintain
notable performance. Through comprehensive evaluation across model-, problem-,
and noise-related configurations, we examine the extent and mechanisms of this
robustness, exploring both the handling of character-level tokenization and
\textit{implicit} versus \textit{explicit} denoising mechanism hypotheses of
character-level noises. We hope our findings on the low-level robustness of
LLMs will shed light on the risks of their misuse and on the reliability of
deploying LLMs across diverse applications.

</details>


### [24] [Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2510.14420)
*Qingyu Ren,Qianyu He,Bowei Zhang,Jie Zeng,Jiaqing Liang,Yanghua Xiao,Weikang Zhou,Zeye Sun,Fei Yu*

Main category: cs.CL

TL;DR: 提出了一种无标签自监督强化学习框架，通过从指令直接推导奖励信号并生成伪标签来训练奖励模型，解决了多约束指令遵循问题。


<details>
  <summary>Details</summary>
Motivation: 语言模型在处理多约束指令时表现不佳，现有强化学习方法依赖外部监督且面临稀疏奖励问题。

Method: 采用约束分解策略和高效的约束级二元分类，直接从指令推导奖励信号并生成伪标签进行奖励模型训练。

Result: 在3个领域内和5个领域外数据集上均取得显著改进，包括具有挑战性的代理和多轮指令遵循任务。

Conclusion: 该方法能有效解决多约束指令遵循问题，具有良好的泛化能力。

Abstract: Language models often struggle to follow multi-constraint instructions that
are crucial for real-world applications. Existing reinforcement learning (RL)
approaches suffer from dependency on external supervision and sparse reward
signals from multi-constraint tasks. We propose a label-free self-supervised RL
framework that eliminates dependency on external supervision by deriving reward
signals directly from instructions and generating pseudo-labels for reward
model training. Our approach introduces constraint decomposition strategies and
efficient constraint-wise binary classification to address sparse reward
challenges while maintaining computational efficiency. Experiments show that
our approach generalizes well, achieving strong improvements across 3 in-domain
and 5 out-of-domain datasets, including challenging agentic and multi-turn
instruction following. The data and code are publicly available at
https://github.com/Rainier-rq/verl-if

</details>


### [25] [Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents](https://arxiv.org/abs/2510.14438)
*Rui Wang,Ce Zhang,Jun-Yu Ma,Jianshu Zhang,Hongru Wang,Yi Chen,Boyang Xue,Tianqing Fang,Zhisong Zhang,Hongming Zhang,Haitao Mi,Dong Yu,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 提出了WebAggregator，一种用于深度研究网络代理的信息聚合方法，通过探索进化范式构建可验证的训练数据，开发的基础模型在信息聚合能力上超越GPT-4.1和接近Claude-3.7-sonnet。


<details>
  <summary>Details</summary>
Motivation: 现有开源深度研究代理主要关注信息检索能力，忽视了信息聚合的重要性，这限制了它们支持深度研究的能力。

Method: 采用探索进化范式：代理主动在线探索收集证据，然后自进化聚合程序，从12种高级逻辑类型中选择、组合和优化操作来合成可验证的QA对，构建WebAggregatorQA数据集。

Result: WebAggregator-8B匹配GPT-4.1性能，32B变体在GAIA-text上超越GPT-4.1超过10%，接近Claude-3.7-sonnet。在WebAggregatorQA基准测试中，Claude-3.7-sonnet仅得28%，GPT-4.1得25.8%。

Conclusion: 即使代理能够检索所有参考文献，在WebAggregatorQA上仍然表现不佳，突显了加强网络代理基础信息聚合能力的必要性。

Abstract: Deep research web agents not only retrieve information from diverse sources
such as web environments, files, and multimodal inputs, but more importantly,
they need to rigorously analyze and aggregate knowledge for insightful
research. However, existing open-source deep research agents predominantly
focus on enhancing information-seeking capabilities of web agents to locate
specific information, while overlooking the essential need for information
aggregation, which would limit their ability to support in-depth research. We
propose an Explore to Evolve paradigm to scalably construct verifiable training
data for web agents. Begins with proactive online exploration, an agent sources
grounded information by exploring the real web. Using the collected evidence,
the agent then self-evolves an aggregation program by selecting, composing, and
refining operations from 12 high-level logical types to synthesize a verifiable
QA pair. This evolution from high-level guidance to concrete operations allowed
us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K
websites and 11 domains. Based on an open-source agent framework, SmolAgents,
we collect supervised fine-tuning trajectories to develop a series of
foundation models, WebAggregator. WebAggregator-8B matches the performance of
GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text
and closely approaches Claude-3.7-sonnet. Moreover, given the limited
availability of benchmarks that evaluate web agents' information aggregation
abilities, we construct a human-annotated evaluation split of WebAggregatorQA
as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves
28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all
references, they still struggle on WebAggregatorQA, highlighting the need to
strengthen the information aggregation capabilities of web agent foundations.

</details>


### [26] [Predicting Task Performance with Context-aware Scaling Laws](https://arxiv.org/abs/2510.14919)
*Kyle Montgomery,David Park,Jianhong Tu,Michael Bendersky,Beliz Gunel,Dawn Song,Chenguang Wang*

Main category: cs.CL

TL;DR: 提出了一个联合建模训练计算和上下文的下游任务性能框架，通过实证验证在三个任务上准确建模性能并可靠外推。


<details>
  <summary>Details</summary>
Motivation: 传统的扩展定律无法捕捉下游任务性能，其中上下文起着关键作用，需要新的框架来理解训练计算和上下文利用之间的相互作用。

Method: 提出一个简单可解释的框架，联合建模下游性能作为训练计算和提供上下文的函数，在Llama-2模型的扩展上下文变体上进行实证验证。

Result: 框架准确建模了分布内下游性能，在三个数量级的训练计算上具有泛化能力，并能可靠外推随着上下文增加的性能。

Conclusion: 这些发现为设计更高效的长上下文LLM提供了指导，揭示了训练计算和上下文利用之间的相互作用。

Abstract: Scaling laws have transformed our understanding of large language models by
linking upstream metrics like cross-entropy loss to design factors such as
model size, training data, and compute. However, these conventional laws fail
to capture downstream task performance, where context plays a critical role. In
this work, we propose a straightforward, interpretable framework that jointly
models downstream performance as a function of the training compute and the
provided context. We empirically validate our framework by fitting it on the
observed downstream performance of extended-context variants of Llama-2-7B and
Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic
reasoning, common sense reasoning, and machine translation. Our results
demonstrate that our framework accurately models in-distribution downstream
performance, generalizes across three orders of magnitude in training compute,
and reliably extrapolates performance as the amount of context increases. These
findings offer valuable insights into the interplay between training compute
and context utilization, providing guidance for designing more efficient
long-context LLMs for diverse downstream tasks. Our code is available at
https://github.com/wang-research-lab/context-scaling.

</details>


### [27] [LaSeR: Reinforcement Learning with Last-Token Self-Rewarding](https://arxiv.org/abs/2510.14943)
*Wenkai Yang,Weijie Liu,Ruobing Xie,Yiju Guo,Lulu Wu,Saiyong Yang,Yankai Lin*

Main category: cs.CL

TL;DR: 提出LaSeR算法，通过最后令牌自奖励来简化RLVR训练，只需额外一个令牌推理成本就能同时优化推理和自奖励能力


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法需要分别生成解决方案和自我验证，使用两个提示模板效率低下。本文旨在简化这一过程，提高训练效率。

Method: 基于理论分析发现推理奖励可简化为最后令牌自奖励分数，提出LaSeR算法，在原RLVR损失基础上添加MSE损失来对齐最后令牌自奖励分数与验证器推理奖励。

Result: 实验表明该方法不仅提升模型推理性能，还赋予其显著的自奖励能力，增强了推理时扩展性能。

Conclusion: LaSeR通过最小额外成本实现了推理和自奖励能力的联合优化，为RLVR提供了高效解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a core paradigm for enhancing the reasoning capabilities of Large Language
Models (LLMs). To address the lack of verification signals at test time, prior
studies incorporate the training of model's self-verification capability into
the standard RLVR process, thereby unifying reasoning and verification
capabilities within a single LLM. However, previous practice requires the LLM
to sequentially generate solutions and self-verifications using two separate
prompt templates, which significantly reduces efficiency. In this work, we
theoretically reveal that the closed-form solution to the RL objective of
self-verification can be reduced to a remarkably simple form: the true
reasoning reward of a solution is equal to its last-token self-rewarding score,
which is computed as the difference between the policy model's next-token
log-probability assigned to any pre-specified token at the solution's last
token and a pre-calculated constant, scaled by the KL coefficient. Based on
this insight, we propose LaSeR (Reinforcement Learning with Last-Token
Self-Rewarding), an algorithm that simply augments the original RLVR loss with
a MSE loss that aligns the last-token self-rewarding scores with verifier-based
reasoning rewards, jointly optimizing the reasoning and self-rewarding
capabilities of LLMs. The optimized self-rewarding scores can be utilized in
both training and testing to enhance model performance. Notably, our algorithm
derives these scores from the predicted next-token probability distribution of
the last token immediately after generation, incurring only the minimal extra
cost of one additional token inference. Experiments show that our method not
only improves the model's reasoning performance but also equips it with
remarkable self-rewarding capability, thereby boosting its inference-time
scaling performance.

</details>


### [28] [Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents](https://arxiv.org/abs/2510.14967)
*Guoqing Wang,Sunhao Dai,Guangze Ye,Zeyu Gan,Wei Yao,Yong Deng,Xiaofeng Wu,Zhenzhe Ying*

Main category: cs.CL

TL;DR: 提出IGPO框架，通过信息增益为多轮智能体训练提供密集的内在监督，解决传统结果奖励稀疏性问题


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖最终答案的结果奖励，在多轮设置中面临奖励稀疏问题，导致优势崩溃和信用分配困难

Method: IGPO将每轮交互建模为获取真实信息的过程，定义轮级奖励为策略产生正确答案概率的边际增长

Result: 在领域内和领域外基准测试中，IGPO在多轮场景下始终优于强基线，实现更高准确性和样本效率

Conclusion: IGPO通过内在信念更新提供密集监督，有效解决多轮智能体训练中的奖励稀疏问题

Abstract: Large language model (LLM)-based agents are increasingly trained with
reinforcement learning (RL) to enhance their ability to interact with external
environments through tool use, particularly in search-based settings that
require multi-turn reasoning and knowledge acquisition. However, existing
approaches typically rely on outcome-based rewards that are only provided at
the final answer. This reward sparsity becomes particularly problematic in
multi-turn settings, where long trajectories exacerbate two critical issues:
(i) advantage collapse, where all rollouts receive identical rewards and
provide no useful learning signals, and (ii) lack of fine-grained credit
assignment, where dependencies between turns are obscured, especially in
long-horizon tasks. In this paper, we propose Information Gain-based Policy
Optimization (IGPO), a simple yet effective RL framework that provides dense
and intrinsic supervision for multi-turn agent training. IGPO models each
interaction turn as an incremental process of acquiring information about the
ground truth, and defines turn-level rewards as the marginal increase in the
policy's probability of producing the correct answer. Unlike prior
process-level reward approaches that depend on external reward models or costly
Monte Carlo estimation, IGPO derives intrinsic rewards directly from the
model's own belief updates. These intrinsic turn-level rewards are combined
with outcome-level supervision to form dense reward trajectories. Extensive
experiments on both in-domain and out-of-domain benchmarks demonstrate that
IGPO consistently outperforms strong baselines in multi-turn scenarios,
achieving higher accuracy and improved sample efficiency.

</details>


### [29] [LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training](https://arxiv.org/abs/2510.14969)
*Yiming Wang,Da Yin,Yuedong Cui,Ruichen Zheng,Zhiqian Li,Zongyu Lin,Di Wu,Xueqing Wu,Chenchen Ye,Yu Zhou,Kai-Wei Chang*

Main category: cs.CL

TL;DR: UI-Simulator是一个可扩展的范式，通过生成结构化UI状态和转换来大规模合成训练轨迹，解决数字代理训练数据收集成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 数字代理需要多样化的大规模UI轨迹来泛化现实世界任务，但收集此类数据在人工标注、基础设施和工程方面成本过高。

Method: 集成数字世界模拟器生成多样化UI状态，使用引导展开过程进行连贯探索，并通过轨迹包装器生成高质量多样化轨迹。还提出UI-Simulator-Grow目标缩放策略，优先处理高影响力任务并合成信息丰富的轨迹变体。

Result: 在WebArena和AndroidWorld上的实验表明，UI-Simulator与在真实UI上训练的开源代理相当或更优，具有更好的鲁棒性。UI-Simulator-Grow仅使用Llama-3-8B-Instruct作为基础模型就能达到Llama-3-70B-Instruct的性能。

Conclusion: 目标合成缩放范式具有持续高效增强数字代理的潜力。

Abstract: Digital agents require diverse, large-scale UI trajectories to generalize
across real-world tasks, yet collecting such data is prohibitively expensive in
both human annotation, infra and engineering perspectives. To this end, we
introduce $\textbf{UI-Simulator}$, a scalable paradigm that generates
structured UI states and transitions to synthesize training trajectories at
scale. Our paradigm integrates a digital world simulator for diverse UI states,
a guided rollout process for coherent exploration, and a trajectory wrapper
that produces high-quality and diverse trajectories for agent training. We
further propose $\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that
enables more rapid and data-efficient scaling by prioritizing high-impact tasks
and synthesizes informative trajectory variants. Experiments on WebArena and
AndroidWorld show that UI-Simulator rivals or surpasses open-source agents
trained on real UIs with significantly better robustness, despite using weaker
teacher models. Moreover, UI-Simulator-Grow matches the performance of
Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,
highlighting the potential of targeted synthesis scaling paradigm to
continuously and efficiently enhance the digital agents.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [30] [From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering](https://arxiv.org/abs/2510.13857)
*Qiang Xu,Xiangyu Wen,Changran Xu,Zeju Li,Jianyuan Zhong*

Main category: cs.SE

TL;DR: 本文提出ArbiterOS架构来解决AI代理从原型到生产部署的危机，通过治理优先的范式来处理概率性处理器与确定性软件工程思维之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型开启了代理时代，但代理系统在关键任务应用中存在脆弱性、不可预测性和不可信性问题，这源于概率性处理器与确定性软件工程范式之间的根本性不匹配。

Method: 引入治理优先的代理工程范式，提出名为ArbiterOS的正式架构，旨在为概率性AI代理系统提供原则性工程方法。

Result: 论文提出了解决代理系统生产部署危机的理论框架和架构设计，但未提供具体实验结果。

Conclusion: 需要从传统软件工程的确定性思维转向治理优先的范式，ArbiterOS为解决代理系统在关键任务应用中的可信性问题提供了架构解决方案。

Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age
of the Agent,'' enabling autonomous systems to tackle complex goals. However,
the transition from prototype to production is hindered by a pervasive ``crisis
of craft,'' resulting in agents that are brittle, unpredictable, and ultimately
untrustworthy in mission-critical applications. This paper argues this crisis
stems from a fundamental paradigm mismatch -- attempting to command inherently
probabilistic processors with the deterministic mental models of traditional
software engineering. To solve this crisis, we introduce a governance-first
paradigm for principled agent engineering, embodied in a formal architecture we
call ArbiterOS.

</details>


### [31] [Benchmarking Correctness and Security in Multi-Turn Code Generation](https://arxiv.org/abs/2510.13859)
*Ruchit Rawal,Jeffrey Yang Fan Chiang,Chihao Shen,Jeffery Siyuan Tian,Aastha Mahajan,Tom Goldstein,Yizheng Chen*

Main category: cs.SE

TL;DR: MT-Sec是首个系统评估多轮编码场景中正确性和安全性的基准，通过将单轮任务转换为语义对齐的多轮交互序列，发现从单轮到多轮设置中"正确且安全"的输出下降了20-27%，即使是最先进模型也表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的AI编码助手基准通常局限于单轮任务，无法反映真实世界开发的迭代性质，需要评估多轮编码场景中的正确性和安全性。

Method: 使用合成数据管道将现有单轮任务转换为语义对齐的多轮交互序列，重用原始测试套件，同时建模真实世界编码过程的复杂性。评估了32个开源和闭源模型以及三种代理框架。

Result: 从单轮到多轮设置中"正确且安全"的输出下降了20-27%；在多轮代码差异生成任务中模型表现更差，功能不正确和不安全输出的比例增加；代理框架在单轮代码生成中提升性能，但在多轮评估中效果不佳。

Conclusion: 需要能够联合评估多轮真实世界编码工作流中正确性和安全性的基准。

Abstract: AI coding assistants powered by large language models (LLMs) have transformed
software development, significantly boosting productivity. While existing
benchmarks evaluate the correctness and security of LLM-generated code, they
are typically limited to single-turn tasks that do not reflect the iterative
nature of real-world development. We introduce MT-Sec, the first benchmark to
systematically evaluate both correctness and security in multi-turn coding
scenarios. We construct this using a synthetic data pipeline that transforms
existing single-turn tasks into semantically aligned multi-turn interaction
sequences, allowing reuse of original test suites while modeling the complexity
of real-world coding processes. We evaluate 32 open- and closed-source models,
and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in
"correct and secure" outputs from single-turn to multi-turn settings -- even
among state-of-the-art models. Beyond full-program generation, we also evaluate
models on multi-turn code-diff generation -- an unexplored yet practically
relevant setting -- and find that models perform worse here, with increased
rates of functionally incorrect and insecure outputs. Finally, we find that
while agent scaffoldings boost single-turn code generation performance, they
are not quite as effective in multi-turn evaluations. Together, these findings
highlight the need for benchmarks that jointly evaluate correctness and
security in multi-turn, real-world coding workflows.

</details>


### [32] [A11YN: aligning LLMs for accessible web UI code generation](https://arxiv.org/abs/2510.13914)
*Janghan Yoon,Jaegwan Cho,Junhyeok Kim,Jiwan Chung,Jaehyun Jeon,Youngjae Yu*

Main category: cs.SE

TL;DR: A11yn是首个将代码生成LLM对齐以可靠生成符合可访问性标准的网页UI的方法，通过优化基于WCAG指南的奖励函数，显著降低了不可访问率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成网页界面时经常从训练数据中复制可访问性缺陷，导致界面无法满足多样化用户需求。

Method: A11yn优化了一个新颖的奖励函数，该函数根据可访问性测试引擎识别的违规严重程度对WCAG违规进行惩罚缩放。

Result: A11yn显著优于强基线方法，将基础模型的不可访问率降低了60%，同时保持了生成UI的语义保真度和视觉质量。

Conclusion: 研究表明可访问性可以在LLM中系统优化，证明了为可访问性对齐代码生成的可行性。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating functional and aesthetic web interfaces directly from
instructions. However, these models often replicate accessibility flaws from
their training data, resulting in interfaces that exclude users with diverse
needs and contexts. To address this gap, we introduce A11yn, the first method
that aligns code-generating LLMs to reliably produce accessibility-compliant
web UIs. A11yn optimizes a novel reward function that penalizes violations of
the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the
severity of each violation as identified by an accessibility testing engine. To
support training, we construct UIReq-6.8K, a dataset of 6,800 diverse
instructions for web UI generation. For evaluation, we introduce RealUIReq-300,
a benchmark of 300 real-world web UI requests grounded and manually curated
from public web pages, spanning a broad range of use cases. Empirical results
show that A11yn significantly outperforms strong baselines, lowering the
Inaccessibility Rate by 60% over the base model while preserving semantic
fidelity and visual quality of generated UIs. These findings demonstrate that
accessibility can be systematically optimized within LLMs, showing the
feasibility of aligning code generation for accessibility.

</details>


### [33] [Signature in Code Backdoor Detection, how far are we?](https://arxiv.org/abs/2510.13992)
*Quoc Hung Le,Thanh Le-Cong,Bach Le,Bowen Xu*

Main category: cs.SE

TL;DR: 重新评估谱签名防御方法在代码模型后门攻击检测中的适用性，发现传统设置效果不佳，提出新的代理指标来更准确评估防御性能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中的广泛应用，后门攻击成为重大威胁。谱签名防御方法在神经网络中已有研究，但在代码模型中的效果尚不明确，需要系统评估其适用性

Method: 系统评估谱签名防御在不同攻击场景和防御配置下的有效性，分析关键因素设置的影响，探索新的代理指标

Result: 发现代码后门检测中广泛使用的谱签名设置通常不是最优的，找到了能更准确估计防御性能的新代理指标

Conclusion: 谱签名防御在代码模型后门检测中需要优化设置，提出的新代理指标可有效评估防御性能而无需重新训练模型

Abstract: As Large Language Models (LLMs) become increasingly integrated into software
development workflows, they also become prime targets for adversarial attacks.
Among these, backdoor attacks are a significant threat, allowing attackers to
manipulate model outputs through hidden triggers embedded in training data.
Detecting such backdoors remains a challenge, and one promising approach is the
use of Spectral Signature defense methods that identify poisoned data by
analyzing feature representations through eigenvectors. While some prior works
have explored Spectral Signatures for backdoor detection in neural networks,
recent studies suggest that these methods may not be optimally effective for
code models. In this paper, we revisit the applicability of Spectral
Signature-based defenses in the context of backdoor attacks on code models. We
systematically evaluate their effectiveness under various attack scenarios and
defense configurations, analyzing their strengths and limitations. We found
that the widely used setting of Spectral Signature in code backdoor detection
is often suboptimal. Hence, we explored the impact of different settings of the
key factors. We discovered a new proxy metric that can more accurately estimate
the actual performance of Spectral Signature without model retraining after the
defense.

</details>


### [34] [One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery](https://arxiv.org/abs/2510.14036)
*Qiushi Wu,Yue Xiao,Dhilung Kirat,Kevin Eykholt,Jiyong Jang,Douglas Lee Schales*

Main category: cs.SE

TL;DR: BugStone是一个基于LLVM和大型语言模型的程序分析系统，能够利用已修复的bug实例识别重复模式bug，在Linux内核中发现超过22K个潜在问题，验证了246个有效bug。


<details>
  <summary>Details</summary>
Motivation: 大型程序中存在重复模式bug，这些bug在多个代码段中重复出现但未被发现，可能扩大攻击面。传统方法逐个修复这些bug效率低下且容易遗漏。

Method: 利用LLVM和大型语言模型分析程序，通过已修复的bug实例识别一致的错误模式（如特定API误用），然后在整个程序中搜索相似模式来识别潜在漏洞。

Result: 从135个独特RPB开始，BugStone在Linux内核中识别了超过22K个潜在问题，手动验证400个发现中有246个有效。在包含1.9K安全bug的数据集上达到92.2%精确度和79.1%成对准确度。

Conclusion: RPB在软件中普遍存在且严重影响安全性，BugStone系统能够有效识别这些重复模式bug，显著提高bug检测效率。

Abstract: Fixing bugs in large programs is a challenging task that demands substantial
time and effort. Once a bug is found, it is reported to the project
maintainers, who work with the reporter to fix it and eventually close the
issue. However, across the program, there are often similar code segments,
which may also contain the bug, but were missed during discovery. Finding and
fixing each recurring bug instance individually is labor intensive. Even more
concerning, bug reports can inadvertently widen the attack surface as they
provide attackers with an exploitable pattern that may be unresolved in other
parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear
repeatedly across various code segments of a program or even in different
programs, stemming from a same root cause, but are unresolved. Our
investigation reveals that RPBs are widespread and can significantly compromise
the security of software programs. This paper introduces BugStone, a program
analysis system empowered by LLVM and a Large Language Model (LLM). The key
observation is that many RPBs have one patched instance, which can be leveraged
to identify a consistent error pattern, such as a specific API misuse. By
examining the entire program for this pattern, it is possible to identify
similar sections of code that may be vulnerable. Starting with 135 unique RPBs,
BugStone identified more than 22K new potential issues in the Linux kernel.
Manual analysis of 400 of these findings confirmed that 246 were valid. We also
created a dataset from over 1.9K security bugs reported by 23 recent top-tier
conference works. We manually annotate the dataset, identify 80 recurring
patterns and 850 corresponding fixes. Even with a cost-efficient model choice,
BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.

</details>


### [35] [A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments](https://arxiv.org/abs/2510.14339)
*Jialu Zhang,Jialiang Gu,Wangmeiyu Zhang,José Pablo Cambronero,John Kolesar,Ruzica Piskac,Daming Li,Hanyuan Shi*

Main category: cs.SE

TL;DR: 本文首次对在线编程中的TLE错误进行大规模实证研究，揭示了TLE错误的多种根本原因，并开发了首个专门针对TLE错误的自动修复工具Nettle。


<details>
  <summary>Details</summary>
Motivation: 在线编程平台上的TLE错误难以解决，错误信息缺乏诊断价值，平台支持有限，现有调试工具帮助不大，导致许多用户在重复TLE失败后放弃提交。

Method: 手动分析1000个Codeforces的TLE提交，分类根本原因，追踪用户修复尝试；开发Nettle工具，将LLM与编译器生成的针对性自动反馈和测试用例相结合。

Result: Nettle在1000个真实案例中达到98.5%的修复率，远超最强LLM基线，所有修复都通过了Nettle-Eval和平台官方检查器的验证。

Conclusion: TLE错误不仅是性能问题，还涉及无限循环、数据结构使用不当和I/O效率低下；Nettle证明了自动化TLE修复的可行性。

Abstract: Online programming platforms such as Codeforces and LeetCode attract millions
of users seeking to learn to program or refine their skills for industry
interviews. A major challenge for these users is the Time Limit Exceeded (TLE)
error, triggered when a program exceeds the execution time bound. Although
designed as a performance safeguard, TLE errors are difficult to resolve: error
messages provide no diagnostic insight, platform support is minimal, and
existing debugging tools offer little help. As a result, many users abandon
their submissions after repeated TLE failures.
  This paper presents the first large-scale empirical study of TLE errors in
online programming. We manually analyzed 1000 Codeforces submissions with TLE
errors, classified their root causes, and traced how users attempted to fix
them. Our analysis shows that TLE errors often arise not only from inefficient
algorithms but also from infinite loops, improper data structure use, and
inefficient I/O, challenging the conventional view that TLEs are purely
performance issues.
  Guided by these findings, we introduce Nettle, the first automated repair
tool specifically designed for TLE errors, and Nettle-Eval, the first framework
for evaluating TLE repairs. Integrating LLMs with targeted automated feedback
generated by the compiler and test cases, Nettle produces small, correct code
edits that eliminate TLEs while preserving functionality. Evaluated on the same
1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the
strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the
platform's official checker, confirming the reliability of our framework.

</details>


### [36] [PathFix: Automated Program Repair with Expected Path](https://arxiv.org/abs/2510.14341)
*Xu He,Shu Wang,Kun Sun*

Main category: cs.SE

TL;DR: PathFix是一种新的自动程序修复方法，利用从正确执行路径中提取的路径敏感约束来生成补丁，解决了现有APR方法生成过多候选补丁和过拟合测试用例的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动程序修复方法由于难以生成精确的规范，面临生成过多合理补丁候选和过拟合部分测试用例的挑战。

Method: PathFix通过四个步骤工作：1) 追踪错误路径；2) 从控制流图分析期望路径；3) 沿期望路径求解状态约束生成补丁；4) 验证补丁正确性。还集成了大语言模型来提升修复性能和缓解可扩展性问题。

Result: 实验结果表明PathFix优于现有解决方案，特别是在处理复杂程序结构（如循环和递归）方面表现突出。

Conclusion: PathFix通过路径敏感约束分析有效解决了自动程序修复中的关键挑战，在复杂程序结构修复方面具有显著优势。

Abstract: Automated program repair (APR) techniques are effective in fixing inevitable
defects in software, enhancing development efficiency and software robustness.
However, due to the difficulty of generating precise specifications, existing
APR methods face two main challenges: generating too many plausible patch
candidates and overfitting them to partial test cases. To tackle these
challenges, we introduce a new APR method named PathFix, which leverages
path-sensitive constraints extracted from correct execution paths to generate
patches for repairing buggy code. It is based on one observation: if a buggy
program is repairable, at least one expected path is supposed to replace the
fault path in the patched program. PathFix operates in four main steps. First,
it traces fault paths reaching the fault output in the buggy program. Second,
it derives expected paths by analyzing the desired correct output on the
control flow graph, where an expected path defines how a feasible patch leads
to the correct execution. Third, PathFix generates and evaluates patches by
solving state constraints along the expected path. Fourth, we validate the
correctness of the generated patch. To further enhance repair performance and
mitigate scalability issues introduced by path-sensitive analysis, we integrate
a large language model (LLM) into our framework. Experimental results show that
PathFix outperforms existing solutions, particularly in handling complex
program structures such as loops and recursion.

</details>


### [37] [E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task](https://arxiv.org/abs/2510.14509)
*Jingyao Liu,Chen Huang,Zhizhao Guan,Wenqiang Lei,Yang Deng*

Main category: cs.SE

TL;DR: E2EDev是一个端到端软件开发基准，包含细粒度用户需求、BDD测试场景和自动化测试管道，通过人机协同多智能体标注框架构建，评估显示现有E2ESD框架在解决这些任务时仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有端到端软件开发(E2ESD)基准存在局限性，需要更全面、高质量的基准来评估和改进E2ESD解决方案的有效性和成本效益。

Method: 提出E2EDev基准，包含细粒度用户需求、BDD测试场景和自动化测试管道，使用人机协同多智能体标注框架(HITL-MAA)来确保质量并减少标注工作量。

Result: 通过评估各种E2ESD框架和LLM骨干网络，分析显示现有解决方案在有效解决这些任务方面持续面临困难。

Conclusion: 迫切需要更有效和成本效益更高的E2ESD解决方案，E2EDev基准为这一目标提供了评估基础。

Abstract: E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple
BDD test scenarios with corresponding Python step implementations for each
requirement}, and (iii) a fully automated testing pipeline built on the Behave
framework. To ensure its quality while reducing the annotation effort, E2EDev
leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework
(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with
E2EDev}, our analysis reveals a persistent struggle to effectively solve these
tasks, underscoring the critical need for more effective and cost-efficient
E2ESD solutions. Our codebase and benchmark are publicly available at
https://github.com/SCUNLP/E2EDev.

</details>


### [38] [ATGen: Adversarial Reinforcement Learning for Test Case Generation](https://arxiv.org/abs/2510.14635)
*Qingyao Li,Xinyi Dai,Weiwen Liu,Xiangyang Li,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.SE

TL;DR: ATGen是一个通过对抗性强化学习训练测试用例生成器的框架，旨在突破静态数据集带来的"固定难度上限"，通过让测试生成器与对抗性代码生成器动态对抗来发现更复杂的bug。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示或监督微调的测试生成方法依赖静态数据集，存在"固定难度上限"，无法发现超出训练范围的新颖或更复杂bug。

Method: 采用对抗性强化学习框架，让测试生成器与对抗性代码生成器进行动态对抗，形成难度递增的课程，通过强化学习优化测试生成器以最大化"输出准确性"和"攻击成功率"。

Result: 实验表明ATGen显著优于现有最先进基线方法，能作为更有效的Best-of-N推理过滤器和更高质量的代码生成模型训练奖励源。

Conclusion: ATGen建立了一个新的动态范式，用于提高LLM生成代码的可靠性。

Abstract: Large Language Models (LLMs) excel at code generation, yet their outputs
often contain subtle bugs, for which effective test cases are a critical
bottleneck. Existing test generation methods, whether based on prompting or
supervised fine-tuning, rely on static datasets. This imposes a
``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover
novel or more complex bugs beyond their training scope. To overcome this, we
introduce ATGen, a framework that trains a test case generator via adversarial
reinforcement learning. ATGen pits a test generator against an adversarial code
generator that continuously crafts harder bugs to evade the current policy.
This dynamic loop creates a curriculum of increasing difficulty challenging
current policy. The test generator is optimized via Reinforcement Learning (RL)
to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to
learn a progressively stronger policy that breaks the fixed-difficulty ceiling
of static training. Extensive experiments demonstrate that ATGen significantly
outperforms state-of-the-art baselines. We further validate its practical
utility, showing it serves as both a more effective filter for Best-of-N
inference and a higher-quality reward source for training code generation
models. Our work establishes a new, dynamic paradigm for improving the
reliability of LLM-generated code.

</details>


### [39] [LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?](https://arxiv.org/abs/2510.14700)
*Bin Liu,Yanjie Zhao,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: 对20个LLM代理在自动化web漏洞复现方面进行了首次全面评估，发现虽然能在简单库漏洞上取得合理成功，但在需要多组件环境的复杂服务漏洞上持续失败，环境配置和认证障碍是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: LLM代理在软件工程和网络安全任务中表现出色，但自动化web漏洞复现这一关键应用尚未充分探索，需要评估其在真实场景中的能力。

Method: 系统评估20个来自不同领域的LLM代理在16个维度上的表现，包括技术能力、环境适应性和用户体验，并在3个代表性web漏洞上进行测试，然后对表现最佳的3个代理在80个真实CVE数据集上进行深入评估。

Result: LLM代理在简单库漏洞上成功率合理，但在复杂服务漏洞上持续失败；环境配置复杂性和认证障碍导致代理能执行漏洞利用代码但无法触发实际漏洞；对输入指导高度敏感，认证信息不完整时性能下降超过33%。

Conclusion: 当前LLM代理能力与可靠自动化漏洞复现需求存在显著差距，需要在环境适应和自主问题解决能力方面取得进展。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities
in software engineering and cybersecurity tasks, including code generation,
vulnerability discovery, and automated testing. One critical but underexplored
application is automated web vulnerability reproduction, which transforms
vulnerability reports into working exploits. Although recent advances suggest
promising potential, challenges remain in applying LLM agents to real-world web
vulnerability reproduction scenarios. In this paper, we present the first
comprehensive evaluation of state-of-the-art LLM agents for automated web
vulnerability reproduction. We systematically assess 20 agents from software
engineering, cybersecurity, and general domains across 16 dimensions, including
technical capabilities, environment adaptability, and user experience factors,
on 3 representative web vulnerabilities. Based on the results, we select three
top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation
on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types
and 6 web technologies. Our results reveal that while LLM agents achieve
reasonable success on simple library-based vulnerabilities, they consistently
fail on complex service-based vulnerabilities requiring multi-component
environments. Complex environment configurations and authentication barriers
create a gap where agents can execute exploit code but fail to trigger actual
vulnerabilities. We observe high sensitivity to input guidance, with
performance degrading by over 33% under incomplete authentication information.
Our findings highlight the significant gap between current LLM agent
capabilities and the demands of reliable automated vulnerability reproduction,
emphasizing the need for advances in environmental adaptation and autonomous
problem-solving capabilities.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 提出DR-RPO算法，一种模型无关的在线策略优化方法，用于在分布偏移下学习鲁棒策略，具有次线性遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中训练和部署环境不同的分布偏移问题，特别是在在线设置下样本效率和探索至关重要的情况下。

Method: 采用参考策略正则化，结合d-矩形线性MDP公式和线性函数逼近，使用上置信界奖励进行乐观探索。

Result: 理论保证显示策略优化在鲁棒RL中可实现多项式次优性边界和样本效率，与基于价值的方法性能相当。

Conclusion: DR-RPO在多样化领域中表现出鲁棒性，证实了策略优化在鲁棒强化学习中的有效性。

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [41] [Active Measuring in Reinforcement Learning With Delayed Negative Effects](https://arxiv.org/abs/2510.14315)
*Daiqi Gao,Ziping Xu,Aseel Rawashdeh,Predrag Klasnja,Susan A. Murphy*

Main category: cs.LG

TL;DR: 提出主动可观测马尔可夫决策过程(AOMDP)，智能体不仅选择控制动作，还决定是否测量潜在状态，测量会揭示真实状态但可能有负面延迟效应。


<details>
  <summary>Details</summary>
Motivation: 在现实环境中测量状态可能成本高昂，且会对未来结果产生负面影响，需要平衡状态观测与测量成本。

Method: 将AOMDP建模为周期性部分可观测MDP，提出基于信念状态的在线RL算法，使用序列蒙特卡洛方法近似未知静态环境参数和未观测潜在状态的后验分布。

Result: 在数字健康应用中评估算法，智能体决定何时提供数字干预和何时通过调查评估用户健康状态。

Conclusion: 尽管测量有成本，但减少的不确定性可证明提高样本效率并增加最优策略的价值。

Abstract: Measuring states in reinforcement learning (RL) can be costly in real-world
settings and may negatively influence future outcomes. We introduce the
Actively Observable Markov Decision Process (AOMDP), where an agent not only
selects control actions but also decides whether to measure the latent state.
The measurement action reveals the true latent state but may have a negative
delayed effect on the environment. We show that this reduced uncertainty may
provably improve sample efficiency and increase the value of the optimal policy
despite these costs. We formulate an AOMDP as a periodic partially observable
MDP and propose an online RL algorithm based on belief states. To approximate
the belief states, we further propose a sequential Monte Carlo method to
jointly approximate the posterior of unknown static environment parameters and
unobserved latent states. We evaluate the proposed algorithm in a digital
health application, where the agent decides when to deliver digital
interventions and when to assess users' health status through surveys.

</details>


### [42] [LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search](https://arxiv.org/abs/2510.14331)
*Shivam Singhal,Eran Malach,Tomaso Poggio,Tomer Galanti*

Main category: cs.LG

TL;DR: LLM-ERM：一种结合LLM引导搜索和ERM验证的程序学习框架，在样本效率和计算可行性上优于传统方法


<details>
  <summary>Details</summary>
Motivation: 解决程序学习中样本效率和计算可行性之间的权衡问题，传统方法要么计算成本高（枚举搜索），要么样本效率低（梯度训练）

Method: 提出LLM-ERM框架：使用预训练LLM生成候选程序，在保留数据上进行ERM式验证选择，无需梯度或反馈

Result: 在奇偶校验、模式匹配、素数测试等任务上，仅需200样本即可解决，而SGD训练的transformer即使使用10万样本也会过拟合

Conclusion: 语言引导的程序合成恢复了有限类ERM的统计效率，同时保持计算可行性，为学习梯度训练无法处理的简洁假设提供了实用途径

Abstract: We seek algorithms for program learning that are both sample-efficient and
computationally feasible. Classical results show that targets admitting short
program descriptions (e.g., with short ``python code'') can be learned with a
``small'' number of examples (scaling with the size of the code) via
length-first program enumeration, but the search is exponential in description
length. Consequently, Gradient-based training avoids this cost yet can require
exponentially many samples on certain short-program families.
  To address this gap, we introduce LLM-ERM, a propose-and-verify framework
that replaces exhaustive enumeration with an LLM-guided search over candidate
programs while retaining ERM-style selection on held-out data. Specifically, we
draw $k$ candidates with a pretrained reasoning-augmented LLM, compile and
check each on the data, and return the best verified hypothesis, with no
feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise
online mini-batch SGD requires many samples to learn certain short programs.
{\em Empirically, LLM-ERM solves tasks such as parity variants, pattern
matching, and primality testing with as few as 200 samples, while SGD-trained
transformers overfit even with 100,000 samples}. These results indicate that
language-guided program synthesis recovers much of the statistical efficiency
of finite-class ERM while remaining computationally tractable, offering a
practical route to learning succinct hypotheses beyond the reach of
gradient-based training.

</details>


### [43] [Stop-RAG: Value-Based Retrieval Control for Iterative RAG](https://arxiv.org/abs/2510.14337)
*Jaewan Park,Solbee Cho,Jay-Yoon Lee*

Main category: cs.LG

TL;DR: Stop-RAG：一种基于价值控制的迭代RAG自适应停止策略，通过马尔可夫决策过程优化检索停止时机，在保持API兼容性的同时提升多跳问答性能


<details>
  <summary>Details</summary>
Motivation: 迭代RAG虽然能回答复杂多跳问题，但每次额外迭代都会增加延迟、成本和引入干扰证据的风险，现有方法要么使用固定迭代次数，要么依赖不能准确反映是否需要更多检索的置信度代理

Method: 将迭代RAG建模为有限时域马尔可夫决策过程，提出Stop-RAG基于价值的控制器，使用完整轨迹的全宽度前向Q(λ)目标进行训练

Result: 在多跳问答基准测试中，Stop-RAG始终优于固定迭代基线和基于提示的LLM停止方法

Conclusion: 自适应停止是当前智能体系统中缺失的关键组件，基于价值的控制能够提高RAG系统的准确性

Abstract: Iterative retrieval-augmented generation (RAG) enables large language models
to answer complex multi-hop questions, but each additional loop increases
latency, costs, and the risk of introducing distracting evidence, motivating
the need for an efficient stopping strategy. Existing methods either use a
predetermined number of iterations or rely on confidence proxies that poorly
reflect whether more retrieval will actually help. We cast iterative RAG as a
finite-horizon Markov decision process and introduce Stop-RAG, a value-based
controller that adaptively decides when to stop retrieving. Trained with
full-width forward-view Q($\lambda$) targets from complete trajectories,
Stop-RAG learns effective stopping policies while remaining compatible with
black-box APIs and existing pipelines. On multi-hop question-answering
benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines
and prompting-based stopping with LLMs. These results highlight adaptive
stopping as a key missing component in current agentic systems, and demonstrate
that value-based control can improve the accuracy of RAG systems.

</details>


### [44] [Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers](https://arxiv.org/abs/2510.14381)
*Andrew Zhao,Reshmi Ghosh,Vitor Carvalho,Emily Lawton,Keegan Hines,Gao Huang,Jack W. Stokes*

Main category: cs.LG

TL;DR: 该论文首次系统分析了基于LLM的提示优化中的投毒风险，发现基于反馈的攻击比注入查询攻击更危险，并提出了一种无需访问奖励模型的简单假奖励攻击方法，以及通过高亮防御来降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM系统在日常AI应用中的普及，提示优化的安全性问题尚未得到充分研究。论文旨在揭示提示优化阶段存在的投毒风险，特别是基于反馈的攻击方式。

Method: 使用HarmBench进行系统性分析，比较了反馈攻击和查询注入攻击的效果。提出了假奖励攻击方法，并设计了高亮防御机制来对抗此类攻击。

Result: 研究发现基于反馈的攻击比注入查询攻击更有效，攻击成功率提升高达ΔASR=0.48。假奖励攻击显著增加了系统脆弱性，而高亮防御能将假奖励攻击的ΔASR从0.23降低到0.07。

Conclusion: 提示优化管道应被视为一级攻击面，需要加强对反馈渠道和优化框架的安全防护措施。

Abstract: Large language model (LLM) systems now underpin everyday AI applications such
as chatbots, computer-use assistants, and autonomous robots, where performance
often depends on carefully designed prompts. LLM-based prompt optimizers reduce
that effort by iteratively refining prompts from scored feedback, yet the
security of this optimization stage remains underexamined. We present the first
systematic analysis of poisoning risks in LLM-based prompt optimization. Using
HarmBench, we find systems are substantially more vulnerable to manipulated
feedback than to injected queries: feedback-based attacks raise attack success
rate (ASR) by up to $\Delta$ASR = 0.48. We introduce a simple fake-reward
attack that requires no access to the reward model and significantly increases
vulnerability, and we propose a lightweight highlighting defense that reduces
the fake-reward $\Delta$ASR from 0.23 to 0.07 without degrading utility. These
results establish prompt optimization pipelines as a first-class attack surface
and motivate stronger safeguards for feedback channels and optimization
frameworks.

</details>


### [45] [Holdout-Loss-Based Data Selection for LLM Finetuning via In-Context Learning](https://arxiv.org/abs/2510.14459)
*Ling Zhang,Xianliang Yang,Juwon Yu,Park Cheonyoung,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: 提出了一种基于上下文近似(ICA)的理论框架，用于高效选择和重加权训练数据，无需参考模型或额外微调，通过动态重加权梯度更新来提升模型对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式或昂贵的重新训练，缺乏系统高效识别高价值训练数据的方法，而噪声或偏离目标的样本会稀释监督效果。

Method: 使用上下文近似(ICA)估计候选样本在训练后的保持损失，基于局部线性化等价于向保持最优的一阶更新，推导出逐样本权重并动态重加权梯度更新。

Result: 在SFT、DPO和SimPO等任务中，ICA重加权在不同骨干网络和数据集上一致改善模型对齐效果，且开销极小。

Conclusion: ICA框架为数据选择和重加权提供了理论基础，有效提升模型对齐性能，但存在快速漂移策略更新的局限性。

Abstract: Fine-tuning large pretrained language models is a common approach for
aligning them with human preferences, but noisy or off-target examples can
dilute supervision. While small, well-chosen datasets often match the
performance of much larger ones, systematic and efficient ways to identify
high-value training data remain underexplored. Many current methods rely on
heuristics or expensive retraining. We present a theoretically grounded,
resource-efficient framework for data selection and reweighting. At its core is
an In-Context Approximation (ICA) that estimates the holdout loss a model would
incur after training on a candidate example by conditioning on a small, curated
holdout set in context. ICA requires no reference model and no additional
finetuning. Under a local linearization, ICA is equivalent to a first-order
update toward the holdout optimum, motivating its use as a proxy for data
value. We derive per-example weights from ICA scores, dynamically reweighting
gradient updates as model parameters evolve. Across SFT, DPO, and SimPO, and
over diverse backbones and datasets, ICA-based reweighting consistently
improves model alignment with minimal overhead. We analyze sensitivity to score
update frequency and the choice of $k$ holdout examples for in-context
demonstrations, and note limitations for rapidly drifting on-policy updates,
highlighting directions for future work. Code and prompts will be released.

</details>


### [46] [Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals](https://arxiv.org/abs/2510.14503)
*Andrejs Sorstkins,Omer Tariq,Muhammad Bilal*

Main category: cs.LG

TL;DR: 提出可逆学习框架，通过状态转移可逆性度量和选择性回滚操作，提升基于价值的强化学习在部分不可逆环境中的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决基于价值的强化学习智能体在部分不可逆环境中容易产生价值高估和不稳定的问题，提高决策安全性和可靠性。

Method: 使用经验推导的状态转移可逆性度量Phi(s,a)来量化在固定时间范围内返回先前状态的可能性，并结合选择性状态回滚操作，在预期回报显著低于瞬时估计值时回滚到前一个状态。

Result: 在CliffWalking v0环境中减少99.8%的灾难性跌落，平均回合回报提高55%；在Taxi v3环境中抑制99.9%的非法动作，累积奖励提高65.7%，同时显著降低奖励方差。

Conclusion: 回滚机制是实现安全和性能提升的关键组件，为安全可靠的顺序决策提供了稳健的解决方案。

Abstract: This paper proposes a reversible learning framework to improve the robustness
and efficiency of value based Reinforcement Learning agents, addressing
vulnerability to value overestimation and instability in partially irreversible
environments. The framework has two complementary core mechanisms: an
empirically derived transition reversibility measure called Phi of s and a, and
a selective state rollback operation. We introduce an online per state action
estimator called Phi that quantifies the likelihood of returning to a prior
state within a fixed horizon K. This measure is used to adjust the penalty term
during temporal difference updates dynamically, integrating reversibility
awareness directly into the value function. The system also includes a
selective rollback operator. When an action yields an expected return markedly
lower than its instantaneous estimated value and violates a predefined
threshold, the agent is penalized and returns to the preceding state rather
than progressing. This interrupts sub optimal high risk trajectories and avoids
catastrophic steps. By combining reversibility aware evaluation with targeted
rollback, the method improves safety, performance, and stability. In the
CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8
percent and yielded a 55 percent increase in mean episode return. In the Taxi
v3 domain, it suppressed illegal actions by greater than or equal to 99.9
percent and achieved a 65.7 percent improvement in cumulative reward, while
also sharply reducing reward variance in both environments. Ablation studies
confirm that the rollback mechanism is the critical component underlying these
safety and performance gains, marking a robust step toward safe and reliable
sequential decision making.

</details>


### [47] [Agentic Entropy-Balanced Policy Optimization](https://arxiv.org/abs/2510.14545)
*Guanting Dong,Licheng Bao,Zhongyuan Wang,Kangzhi Zhao,Xiaoxi Li,Jiajie Jin,Jinghan Yang,Hangyu Mao,Fuzheng Zhang,Kun Gai,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: 提出了Agentic Entropy-Balanced Policy Optimization (AEPO)算法，通过动态熵平衡机制解决代理强化学习中过度依赖熵信号导致的训练崩溃问题，在14个数据集上优于7种主流RL算法。


<details>
  <summary>Details</summary>
Motivation: 主流代理强化学习算法过度依赖熵信号来指导高不确定性工具调用步骤的探索，但这种过度依赖会施加额外约束，导致训练崩溃。

Method: AEPO包含两个核心组件：(1) 动态熵平衡rollout机制，通过熵预监测自适应分配全局和分支采样预算，并对连续高熵工具调用步骤施加分支惩罚；(2) 熵平衡策略优化，在高熵裁剪项中插入停止梯度操作，并引入熵感知优势估计。

Result: 在14个挑战性数据集上，AEPO始终优于7种主流RL算法。仅使用1K RL样本，Qwen3-14B模型在GAIA上达到47.6% Pass@1和65.0% Pass@5，在Humanity's Last Exam上达到11.2% Pass@1和26.0% Pass@5，在WebWalker上达到43.0% Pass@1和70.0% Pass@5。

Conclusion: AEPO提高了rollout采样多样性，同时保持稳定的策略熵，有助于可扩展的web代理训练。

Abstract: Recently, Agentic Reinforcement Learning (Agentic RL) has made significant
progress in incentivizing the multi-turn, long-horizon tool-use capabilities of
web agents. While mainstream agentic RL algorithms autonomously explore
high-uncertainty tool-call steps under the guidance of entropy, excessive
reliance on entropy signals can impose further constraints, leading to the
training collapse. In this paper, we delve into the challenges caused by
entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an
agentic RL algorithm designed to balance entropy in both the rollout and policy
update phases. AEPO comprises two core components: (1) a dynamic
entropy-balanced rollout mechanism that adaptively allocate global and branch
sampling budget through entropy pre-monitoring, while imposing a branch penalty
on consecutive high-entropy tool-call steps to prevent over-branching issues;
and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient
operation into the high-entropy clipping term to preserve and properly rescale
gradients on high-entropy tokens, while incorporating entropy-aware advantage
estimation to prioritize learning on high-uncertainty tokens. Results across 14
challenging datasets show that AEPO consistently outperforms 7 mainstream RL
algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive
results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker
for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on
WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout
sampling diversity while maintaining stable policy entropy, facilitating
scalable web agent training.

</details>


### [48] [The Pursuit of Diversity: Multi-Objective Testing of Deep Reinforcement Learning Agents](https://arxiv.org/abs/2510.14727)
*Antony Bartlett,Cynthia Liem,Annibale Panichella*

Main category: cs.LG

TL;DR: INDAGO-Nexus是一个多目标搜索方法，用于发现深度强化学习代理的多样化故障场景，相比单目标优化的INDAGO工具，能发现更多独特故障并减少故障发现时间。


<details>
  <summary>Details</summary>
Motivation: 现有工具如INDAGO专注于最大化故障数量，但无法确保发现的故障场景具有多样性或揭示不同的错误类型。

Method: 使用多目标进化算法，联合优化故障可能性和测试场景多样性，采用多种多样性指标和帕累托前沿选择策略。

Result: 在三个DRL代理上评估：人形步行者、自动驾驶汽车和停车代理。INDAGO-Nexus在SDC和停车场景中分别比INDAGO多发现83%和40%的独特故障，同时所有代理的故障发现时间减少高达67%。

Conclusion: 多目标搜索方法在发现多样化故障场景方面比单目标优化更有效，能提高测试效果并加速故障发现。

Abstract: Testing deep reinforcement learning (DRL) agents in safety-critical domains
requires discovering diverse failure scenarios. Existing tools such as INDAGO
rely on single-objective optimization focused solely on maximizing failure
counts, but this does not ensure discovered scenarios are diverse or reveal
distinct error types. We introduce INDAGO-Nexus, a multi-objective search
approach that jointly optimizes for failure likelihood and test scenario
diversity using multi-objective evolutionary algorithms with multiple diversity
metrics and Pareto front selection strategies. We evaluated INDAGO-Nexus on
three DRL agents: humanoid walker, self-driving car, and parking agent. On
average, INDAGO-Nexus discovers up to 83% and 40% more unique failures (test
effectiveness) than INDAGO in the SDC and Parking scenarios, respectively,
while reducing time-to-failure by up to 67% across all agents.

</details>


### [49] [To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models](https://arxiv.org/abs/2510.14826)
*Eran Malach,Omid Saremi,Sinead Williamson,Arwen Bradley,Aryo Lotfi,Emmanuel Abbe,Josh Susskind,Etai Littwin*

Main category: cs.LG

TL;DR: 本文揭示了状态空间模型(SSMs)在长序列生成中的理论局限性，但通过引入外部工具交互可以克服这一限制，使SSMs能够实现长度泛化并在算术、推理和编码任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型(SSMs)作为Transformer的替代方案，在长上下文和长序列生成中具有效率优势，但存在理论上的局限性，无法准确解决真正的长序列生成问题。

Method: 通过允许SSMs与外部工具进行交互，并选择适当的工具访问方式和问题相关的训练数据，使SSMs能够学习解决任何可处理的问题。

Result: 工具增强的SSMs在各种算术、推理和编码任务中实现了显著的序列长度泛化能力。

Conclusion: 在交互式工具和代理设置中，SSMs有潜力成为Transformer的高效替代方案。

Abstract: State Space Models (SSMs) have become the leading alternative to Transformers
for sequence modeling. Their primary advantage is efficiency in long-context
and long-form generation, enabled by fixed-size memory and linear scaling of
computational complexity. We begin this work by showing a simple theoretical
result stating that SSMs cannot accurately solve any ``truly long-form''
generation problem (in a sense we formally define), undermining their main
competitive advantage. However, we show that this limitation can be mitigated
by allowing SSMs interactive access to external tools. In fact, we show that
given the right choice of tool access and problem-dependent training data, SSMs
can learn to solve any tractable problem and generalize to arbitrary problem
length/complexity (i.e., achieve length generalization). Following our
theoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable
length generalization on a variety of arithmetic, reasoning, and coding tasks.
These findings highlight SSMs as a potential efficient alternative to
Transformers in interactive tool-based and agentic settings.

</details>


### [50] [Reinforcement Learning with Stochastic Reward Machines](https://arxiv.org/abs/2510.14837)
*Jan Corazza,Ivan Gavran,Daniel Neider*

Main category: cs.LG

TL;DR: 提出了一种新型随机奖励机，用于处理强化学习中噪声奖励问题，并基于约束求解的算法来学习最小随机奖励机。


<details>
  <summary>Details</summary>
Motivation: 现有奖励机学习算法假设奖励无噪声，但在实际应用中奖励往往存在噪声，这限制了奖励机的实用性。

Method: 引入随机奖励机概念，开发基于约束求解的算法，从强化学习智能体的探索中学习最小随机奖励机。

Result: 在两个案例研究中证明该算法优于现有方法和处理噪声奖励的朴素方法。

Conclusion: 该算法能与现有奖励机强化学习算法轻松结合，并保证在极限情况下收敛到最优策略。

Abstract: Reward machines are an established tool for dealing with reinforcement
learning problems in which rewards are sparse and depend on complex sequences
of actions. However, existing algorithms for learning reward machines assume an
overly idealized setting where rewards have to be free of noise. To overcome
this practical limitation, we introduce a novel type of reward machines, called
stochastic reward machines, and an algorithm for learning them. Our algorithm,
based on constraint solving, learns minimal stochastic reward machines from the
explorations of a reinforcement learning agent. This algorithm can easily be
paired with existing reinforcement learning algorithms for reward machines and
guarantees to converge to an optimal policy in the limit. We demonstrate the
effectiveness of our algorithm in two case studies and show that it outperforms
both existing methods and a naive approach for handling noisy reward functions.

</details>


### [51] [Reasoning with Sampling: Your Base Model is Smarter Than You Think](https://arxiv.org/abs/2510.14901)
*Aayush Karan,Yilun Du*

Main category: cs.LG

TL;DR: 研究表明，仅通过基础模型的纯采样就能在推理时获得与强化学习后训练相当的推理能力，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 探索是否能在不进行额外训练的情况下，仅通过基础模型在推理时的纯采样就能获得与强化学习后训练相当的推理能力。

Method: 提出一种基于马尔可夫链蒙特卡洛技术的简单迭代采样算法，利用基础模型自身的似然性进行采样。

Result: 该方法在多种单次任务（如MATH500、HumanEval、GPQA）上显著提升推理能力，性能接近甚至超过强化学习后训练，同时避免了多样性崩溃问题。

Conclusion: 该方法无需训练、精选数据集或验证器，具有广泛的适用性，表明基础模型本身具备强大的推理潜力。

Abstract: Frontier reasoning models have exhibited incredible capabilities across a
wide array of disciplines, driven by posttraining large language models (LLMs)
with reinforcement learning (RL). However, despite the widespread success of
this paradigm, much of the literature has been devoted to disentangling truly
novel behaviors that emerge during RL but are not present in the base models.
In our work, we approach this question from a different angle, instead asking
whether comparable reasoning capabilites can be elicited from base models at
inference time by pure sampling, without any additional training. Inspired by
Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened
distributions, we propose a simple iterative sampling algorithm leveraging the
base models' own likelihoods. Over different base models, we show that our
algorithm offers substantial boosts in reasoning that nearly match and even
outperform those from RL on a wide variety of single-shot tasks, including
MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in
diversity over multiple samples that is characteristic of RL-posttraining.
Crucially, our method does not require training, curated datasets, or a
verifier, suggesting broad applicability beyond easily verifiable domains.

</details>


### [52] [Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity in TVD-MI Scores](https://arxiv.org/abs/2510.14966)
*Zachary Robertson*

Main category: cs.LG

TL;DR: 本文提出了一种基于总变差距离互信息(TVD-MI)的大语言模型成对比较方法，通过身份链接函数而非传统的逻辑链接函数，在保持可加性的同时实现了高效评估。


<details>
  <summary>Details</summary>
Motivation: 传统IRT方法使用逻辑链接函数会破坏TVD-MI比较的几何结构，导致可加性违反，需要开发更合适的评分模型。

Method: 使用TVD-MI进行成对比较，通过身份链接函数和箱约束最小二乘公式处理边界饱和问题，基于Gini熵最大化推导出剪裁线性模型。

Result: 在33%覆盖率下实现保持RMSE 0.117±0.008，同时保持代理排名一致性(Spearman ρ=0.972±0.015)，评估次数比密集方法减少三倍。

Conclusion: TVD-MI的几何结构最适合通过身份映射来保持，为高效LLM评估提供了有效方法，适用于其他有界响应领域。

Abstract: Pairwise comparisons of large language models using total variation distance
mutual information (TVD-MI) produce binary critic decisions per pair. We show
that averaging TVD-MI's binary trials yields centered-probability scores with
additive structure suitable for item-response theory (IRT) without nonlinear
link functions. Maximum-likelihood approaches to IRT use logistic links, but we
find empirically that these transformations introduce curvature that breaks
additivity: across three domains, the identity link yields median curl on raw
data of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce
substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We
derive this clipped-linear model from Gini entropy maximization, yielding a
box-constrained least-squares formulation that handles boundary saturation. At
33% coverage, we achieve holdout RMSE $0.117 \pm 0.008$ while preserving agent
rankings (Spearman $\rho = 0.972 \pm 0.015$), three times fewer evaluations
than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows
strong agreement in agent rankings ($\rho = 0.872$) and consistent
identity-link advantage. TVD-MI's geometry is best preserved by identity
mapping for efficient LLM evaluation, applicable to other bounded-response
domains.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [53] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: 提出STEMS框架，一种安全约束的多智能体强化学习方法，用于协调建筑能源管理，通过空间-时间图表示学习和控制屏障函数实现安全保证。


<details>
  <summary>Details</summary>
Motivation: 解决多建筑能源系统中空间-时间信息利用不足、缺乏严格安全保证和系统复杂性等关键挑战。

Method: 集成GCN-Transformer融合架构的空间-时间图表示学习框架，以及结合控制屏障函数的安全约束多智能体强化学习算法。

Result: 在真实建筑数据集上，STEMS实现21%成本降低、18%排放减少，安全违规从35.1%降至5.6%，仅0.13不适比例。

Conclusion: STEMS框架在极端天气条件下表现出强鲁棒性，在不同建筑类型中保持有效性。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [54] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: 提出了一个用于多AI代理系统的统一建模框架，包含主机代理模型和任务生命周期模型，定义了31个系统属性，支持形式化验证以确保系统正确性。


<details>
  <summary>Details</summary>
Motivation: 当前代理间通信协议碎片化导致语义鸿沟，无法进行严格的系统属性分析，存在架构错位和可被利用的协调问题风险。

Method: 引入两个基础模型：主机代理模型（负责与用户交互、任务分解和编排）和任务生命周期模型（详细描述子任务从创建到完成的状态转换），定义了17个主机代理属性和14个任务生命周期属性。

Result: 建立了第一个严格基础、领域无关的框架，用于系统分析、设计和部署正确、可靠、鲁棒的代理AI系统。

Conclusion: 该框架为多AI代理系统的行为推理提供了统一的语义基础，支持形式化验证以检测协调边缘情况、防止死锁和安全漏洞。

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [55] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve是一个开源进化编码代理，结合大型语言模型和遗传算法解决复杂计算问题，在数学基准测试中超越了AlphaEvolve的性能。


<details>
  <summary>Details</summary>
Motivation: 将强大的进化概念应用于LLM领域，基于广义科学发现的最新方法，解决复杂计算问题。

Method: 采用基于岛屿的遗传算法保持种群多样性，引入基于启发的交叉机制利用LLM上下文窗口组合成功解决方案的特征，实现元提示策略动态探索解空间。

Result: 在用于评估AlphaEvolve的数学基准测试子集上，CodeEvolve在多个挑战性问题上的表现超越了AlphaEvolve。

Conclusion: 该框架展示了LLM与进化算法结合的有效性，通过开源发布促进协作和加速进展。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [56] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: ARM-FM是一个利用基础模型自动设计强化学习奖励的框架，通过奖励机器实现自然语言到结构化奖励函数的转换，并支持零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法对奖励函数设计高度敏感，这限制了其广泛应用。现有方法需要人工设计复杂的奖励函数，过程繁琐且容易出错。

Method: 使用基础模型将自然语言规范自动转换为奖励机器，为每个自动机状态关联语言嵌入以实现任务间泛化，采用结构化奖励机器形式化方法。

Result: 在多个挑战性环境中验证了ARM-FM的有效性，展示了零样本泛化能力，证明了该框架在自动化奖励设计方面的优势。

Conclusion: ARM-FM通过结合基础模型和奖励机器，成功实现了从自然语言到结构化奖励函数的自动化转换，为强化学习的广泛应用提供了新的解决方案。

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [57] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: 提出了一个在线骚扰代理基准，包含多轮骚扰对话数据集、基于重复博弈论的多代理模拟、三种越狱攻击方法以及混合评估框架。研究发现越狱调优显著提高了骚扰成功率，使侮辱和谩骂行为大幅增加，并揭示了闭源和开源模型的不同升级轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有越狱研究主要关注单轮提示，而真实骚扰通常发生在多轮交互中。需要开发多轮、理论基础的攻击方法来评估LLM代理的安全性。

Method: 构建了包含多轮骚扰对话数据集的基准，采用基于重复博弈论的多代理模拟，开发了针对记忆、规划和微调的三种越狱攻击方法，并使用混合方法评估框架。

Result: 越狱调优使骚扰成功率从57.25-64.19%提升到95.78-96.89%（Llama），从98.46%提升到99.33%（Gemini）。侮辱行为从44.2-50.8%增加到84.9-87.8%，谩骂行为从31.5-38.8%增加到81.2-85.1%。

Conclusion: 多轮和理论基础的攻击不仅成功率高，还能模拟人类骚扰动态，这推动了开发强大安全防护措施的需求，以保持在线平台的安全和负责任。

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [58] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出了LiveResearchBench基准和DeepEval评估套件，用于评估代理系统在深度研究任务中的表现，涵盖100个专家策划的任务和全面的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估代理系统的深度研究能力方面存在不足，缺乏用户中心、动态性、明确性和多面性等关键原则。

Method: 引入LiveResearchBench基准（100个专家策划任务）和DeepEval评估套件（涵盖内容和报告级别的质量评估），对17个前沿深度研究系统进行全面评估。

Result: 分析揭示了当前系统的优势、常见失败模式和推进可靠深度研究所需的关键系统组件。

Conclusion: LiveResearchBench和DeepEval为系统评估深度研究能力提供了严格基础，识别了现有系统的局限性和改进方向。

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [59] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 提出Agentic Self-Learning (ASL)框架，通过生成式奖励模型和多角色协同进化实现无需人工标注数据的智能体自我学习，在搜索任务中展现持续改进能力。


<details>
  <summary>Details</summary>
Motivation: 研究如何在不依赖人工标注数据集或预定义规则奖励的情况下，实现基于LLM的智能体的规模化训练。

Method: 提出ASL框架，包含提示生成器、策略模型和生成式奖励模型，在多角色强化学习环境中形成任务生成、策略执行和评估的闭环系统。

Result: ASL实现持续的性能提升，超越强基线方法，在零标注数据条件下仍能持续改进，表现出优异的样本效率和鲁棒性。

Conclusion: 奖励来源和数据规模是开放域智能体学习的关键因素，多角色协同进化是实现可扩展自我改进智能体的有效方法。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [60] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: 提出了MorphoBench基准测试，用于评估大模型的推理能力，能够根据模型推理能力自适应调整问题难度，包含1300多个测试问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试范围有限且缺乏灵活性，无法根据模型推理能力的发展调整难度，需要更全面有效的评估方法。

Method: 从现有基准和奥林匹克竞赛中收集复杂推理问题，利用模型推理过程中的关键陈述自适应修改分析挑战，使用仿真软件生成可动态调整难度的问题。

Result: 收集了1300多个测试问题，基于o3和GPT-5等模型的推理能力迭代调整了MorphoBench的难度。

Conclusion: MorphoBench提高了模型推理评估的全面性和有效性，为提升大模型推理能力和科学稳健性提供了可靠指导。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [61] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC是一个元认知框架，为多智能体系统提供实时、无监督的步骤级错误检测和自我纠正，通过历史条件异常评分来防止错误传播。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统在协作解决问题方面表现出色，但对级联错误很脆弱——单个错误步骤可能在智能体间传播并破坏整体轨迹。

Method: MASC采用两种互补设计：(1) 下一执行重构，从查询和交互历史预测下一步的嵌入以捕捉因果一致性；(2) 原型引导增强，学习正常步骤嵌入的原型先验，在稀疏上下文下稳定重构和异常评分。

Result: 在Who&When基准测试中，MASC持续优于所有基线方法，步骤级错误检测AUC-ROC提升高达8.47%；当集成到不同多智能体框架时，在各种架构上都带来一致的端到端性能提升。

Conclusion: MASC的元认知监控和针对性纠正能够以最小开销缓解错误传播，证实了该框架的有效性和通用性。

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [62] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: 提出IP-Merging方法，无需调优即可让多模态大语言模型直接从数学大语言模型吸收数学推理能力，通过识别关键推理层并投影到MLLM子空间来保持对齐。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在数学推理能力上落后于纯文本大语言模型，希望探索能否让MLLMs直接从现成的数学LLMs吸收推理能力而无需额外训练。

Method: IP-Merging方法：1）识别MLLM和数学LLM中的推理相关参数；2）将这些参数投影到MLLM的子空间以保持对齐；3）在子空间中合并参数。这是一个无需调优的方法。

Result: 大量实验表明，IP-Merging方法能有效提升MLLMs的数学推理能力，且不损害其他能力。

Conclusion: IP-Merging是一种有效的模型融合方法，能够在不进行调优的情况下，让多模态大语言模型直接从数学大语言模型吸收数学推理能力。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [63] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent是一种可训练的分层视觉语言代理，用于移动设备控制，通过高层推理模型和低层动作模型的联合优化，在Android-in-the-Wild基准测试中达到87.9%的任务成功率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的移动设备控制方法主要依赖直接的状态到动作映射，缺乏结构化推理和规划，在新任务或未见UI布局上泛化能力差。

Method: 提出分层架构：高层推理模型和低层动作模型联合优化；将多步决策重新表述为单步子目标序列；提出前瞻优势函数，利用低层模型的执行反馈指导高层优化。

Result: 在Android-in-the-Wild基准测试中达到87.9%的任务成功率，显著优于提示型(17.7%)、监督学习(54.5%)和强化学习(71.9%)方法；在ScreenSpot-v2上展示竞争性零样本泛化能力；在AndroidWorld基准上随骨干网络增大而有效扩展。

Conclusion: Hi-Agent通过分层设计和联合优化解决了长视野任务中的路径爆炸问题，实现了稳定、无评论家的联合训练，在移动控制任务中表现出色。

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [64] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman是一个多智能体系统，用于从高层用户规范自动合成联邦学习系统，通过协作规划、代码生成和自主评估三个阶段，在AgentFL-Bench基准测试中表现优于手工基线。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统设计部署复杂，需要解决数据异构性和系统约束等多方面挑战，现有解决方案脆弱且定制化，成为关键瓶颈。

Method: 采用三阶段协作方法：1)交互式人机循环规划制定研究计划；2)监督智能体团队进行模块化代码生成；3)在沙盒仿真环境中进行自主评估和优化的闭环。

Result: 在16个多样化任务的AgentFL-Bench基准测试中，生成的解决方案与手工基线竞争且通常更优。

Conclusion: 这项工作代表了复杂去中心化AI系统自动化工程的重要进展。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [65] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: JSPLIT是一个基于分类学的框架，用于在大型MCP工具集中有效管理提示大小，通过层次化分类和基于查询的工具选择来减少提示膨胀问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统发展，用户期望从简单的文本交互转向更复杂的代理系统，但工具数量增加导致提示膨胀问题，包括高令牌成本、延迟增加和任务成功率降低。

Method: JSPLIT将工具组织成层次化分类学，使用用户提示基于查询和分类结构识别并仅包含最相关的工具。

Result: JSPLIT显著减少了提示大小，同时不显著影响代理的响应能力。在工具数量大幅增长时，甚至提高了工具选择准确性，在高度复杂的代理环境中降低成本并提高任务成功率。

Conclusion: JSPLIT框架有效解决了MCP工具使用中的提示膨胀问题，通过分类学驱动的工具选择实现了成本降低和性能提升。

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [66] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 研究探讨了LLM代理是否能够成为具有自主规划、任务设计和目标推理能力的实体，通过增强预训练LLM代理的自主任务生成、知识积累和环境交互能力来验证其开放性潜力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理能力的增强，研究者希望探索它们是否能够超越作为智能问题解决工具的角色，成为能够自主规划、设计任务并追求模糊长期目标的独立实体。

Method: 采用开放式实验设置，增强预训练LLM代理的能力，使其能够生成自己的任务、积累知识并与环境进行广泛交互，然后对结果进行定性研究。

Result: 增强后的代理能够可靠地遵循复杂的多步骤指令，跨运行存储和重用信息，提出并解决自己的任务，但对提示设计敏感，容易产生重复任务，且无法形成自我表征。

Conclusion: 研究显示了将预训练LLM适应开放性的潜力和当前限制，指出了未来在训练代理管理记忆、进行富有成效的探索和追求抽象长期目标方面的方向。

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [67] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 提出了一种基于图结构的移动代理评估框架ColorBench，通过静态模拟动态行为来解决现有离线基准只能验证单一黄金路径、在线测试复杂不可复现的问题。


<details>
  <summary>Details</summary>
Motivation: 当前移动代理评估标准存在局限：离线静态基准只能验证单一预定义路径，而在线动态测试受限于真实设备的复杂性和不可复现性，两者都无法全面评估代理能力。

Method: 开发了图结构基准框架，通过建模真实设备交互中的有限状态来实现动态行为的静态模拟，构建了包含175个复杂长时程任务的ColorBench基准。

Result: ColorBench包含175个任务（74个单应用、101个跨应用），平均长度超过13步，每个任务至少包含两条正确路径和若干典型错误路径，支持准动态交互。

Conclusion: 通过在不同基线模型上评估ColorBench，发现了现有模型的局限性，并基于实验结果提出了改进方向和可行的技术路径来增强代理在复杂长时程问题上的性能。

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [68] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 提出了一个结合细粒度波束搜索和过程奖励模型ToolPRM的推理扩展框架，用于提升LLM在函数调用等结构化输出任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理扩展研究主要关注非结构化输出生成任务，而在函数调用等结构化输出任务中的应用尚未充分探索，需要填补这一空白。

Method: 构建了首个细粒度函数调用过程监督数据集，使用函数掩码技术自动标注步骤级奖励，训练ToolPRM过程奖励模型来评分单个函数调用的内部步骤。

Result: ToolPRM在预测准确性上优于粗粒度和结果奖励模型，推理扩展技术结合ToolPRM显著提升了骨干模型在各种函数调用任务和基准测试中的性能。

Conclusion: 揭示了将推理扩展技术应用于结构化输出的关键原则："多探索少保留"，这是由于结构化函数调用生成的不可恢复性特征。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [69] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: Datalake Agent是一个代理系统，通过交互式循环减少NL2SQL任务中的元信息使用，显著降低LLM的token消耗和成本，同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统NL2SQL方法在处理大量数据库元信息时会产生冗长提示和高处理成本，需要更高效的解决方案。

Method: 采用代理式交互循环，让LLM在推理框架中按需请求必要信息，而不是一次性提供所有元信息。

Result: 在23个数据库和100个表问答任务上评估，Datalake Agent将LLM使用的token减少高达87%，成本显著降低且性能保持竞争力。

Conclusion: Datalake Agent通过选择性信息请求机制，有效解决了NL2SQL任务中的元信息处理效率问题。

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [70] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 提出了RoboGPT-R1，一个用于具身规划的两阶段微调框架，结合监督学习和强化学习来提升机器人在复杂环境中的长视野操作任务推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型和视觉语言模型在规划任务中面临挑战，特别是在复杂真实世界环境中执行长视野操作任务时，由于常识和推理能力受限而表现不佳。

Method: 采用两阶段微调框架：监督训练通过专家序列获取基础知识，然后使用强化学习解决模型在视觉空间理解和推理方面的不足。设计了基于规则的奖励函数，同时考虑长视野性能和动作约束。

Result: 在Qwen2.5-VL-3B上训练的推理模型，在EmbodiedBench基准上显著优于GPT-4o-mini 21.33%，并超过其他在Qwen2.5-VL-7B上训练的工作20.33%。

Conclusion: 两阶段微调框架有效提升了具身代理的推理能力，特别是在长视野操作任务中表现出色。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [71] [Boosting Instruction Following at Scale](https://arxiv.org/abs/2510.14842)
*Ben Elder,Evelyn Duesterwald,Vinod Muthusamy*

Main category: cs.AI

TL;DR: 提出了Instruction Boosting方法，通过后生成处理来提高LLM提示指令的可靠性，在包含最多10条指令的SCALEDIF基准测试中，指令遵循率提升了4-7个百分点。


<details>
  <summary>Details</summary>
Motivation: 开发者通常通过修改提示来影响LLM行为，但仅仅添加更多指令并不能保证它们会被遵循，需要提高指令的可靠性。

Method: 引入Instruction Boosting作为后生成方法，并创建SCALEDIF基准测试来评估多指令场景下的性能。还开发了定量冲突评分工具来分析指令间的紧张和冲突。

Result: Instruction Boosting将指令遵循率提高了最多7个百分点（两条指令）和4个百分点（十条指令）。分析显示性能下降与指令间的冲突程度相关。

Conclusion: Instruction Boosting能有效提高LLM的指令遵循可靠性，指令冲突是导致性能下降的重要因素，冲突评分工具可为开发者提供反馈。

Abstract: A typical approach developers follow to influence an LLM's behavior in an
application is through careful manipulation of the prompt, such as by adding or
modifying instructions. However, merely adding more instructions provides
little assurance that they will actually be followed. We introduce Instruction
Boosting as a post-generation method to increase the reliability of LLM prompt
instructions. We show that Instruction Boosting improves the instruction
following rate by up to 7 points for two instructions and up to 4 points for
ten instructions. To demonstrate these results we introduce SCALEDIF, a
benchmark with a scaled instruction volume of up to ten instructions per data
sample. We also present an analysis of the commonly observed trend that
performance degrades as more instructions are added. We show that an important
factor contributing to this trend is the degree of tension and conflict that
arises as the number of instructions is increased. We contribute a quantitative
conflict scoring tool that explains the observed performance trends and
provides feedback to developers on the impact that additional prompt
instructions have on a model's performance.

</details>


### [72] [Where to Search: Measure the Prior-Structured Search Space of LLM Agents](https://arxiv.org/abs/2510.14846)
*Zhuo-Yang Song*

Main category: cs.AI

TL;DR: 提出了一种紧凑的形式化理论来描述和衡量基于领域先验指导的LLM辅助迭代搜索，通过模糊关系算子表示智能体，引入覆盖生成函数来衡量可达性难度。


<details>
  <summary>Details</summary>
Motivation: 生成-过滤-精炼的迭代范式在AI+科学领域取得了进展，但搜索效果取决于如何将领域先验编码到操作结构化假设空间中。

Method: 将智能体表示为输入输出的模糊关系算子，通过单一延续参数加权所有可达路径得到覆盖生成函数，从而衡量可达性难度并提供搜索的几何解释。

Result: 提供了最简单的可测试推断，并通过多数投票实例进行了验证，为衡量智能体及其搜索空间提供了可行的语言和操作工具。

Conclusion: 该理论为LLM构建的迭代搜索提供了系统的形式化描述框架。

Abstract: The generate-filter-refine (iterative paradigm) based on large language
models (LLMs) has achieved progress in reasoning, programming, and program
discovery in AI+Science. However, the effectiveness of search depends on where
to search, namely, how to encode the domain prior into an operationally
structured hypothesis space. To this end, this paper proposes a compact formal
theory that describes and measures LLM-assisted iterative search guided by
domain priors. We represent an agent as a fuzzy relation operator on inputs and
outputs to capture feasible transitions; the agent is thereby constrained by a
fixed safety envelope. To describe multi-step reasoning/search, we weight all
reachable paths by a single continuation parameter and sum them to obtain a
coverage generating function; this induces a measure of reachability
difficulty; and it provides a geometric interpretation of search on the graph
induced by the safety envelope. We further provide the simplest testable
inferences and validate them via a majority-vote instantiation. This theory
offers a workable language and operational tools to measure agents and their
search spaces, proposing a systematic formal description of iterative search
constructed by LLMs.

</details>


### [73] [The Gatekeeper Knows Enough](https://arxiv.org/abs/2510.14881)
*Fikresilase Wondmeneh Abebayew*

Main category: cs.AI

TL;DR: 提出了Gatekeeper Protocol框架，通过低保真潜在状态表示和按需请求高保真上下文的方法，解决LLM代理的上下文窗口限制和状态不同步问题。


<details>
  <summary>Details</summary>
Motivation: LLM作为自主代理部署时，受限于有限的上下文窗口和状态不同步问题，导致输出不可靠、行为不可预测和资源使用效率低下，特别是在与代码库等结构化知识系统交互时。

Method: 引入Gatekeeper Protocol框架，要求代理先在低保真潜在状态表示上进行操作和推理，然后按需策略性地请求高保真上下文。所有交互通过统一的JSON格式进行中介，作为声明性、状态同步的协议。

Result: 通过Sage参考实现证明，该方法显著提高了代理可靠性，通过最小化token消耗改善了计算效率，并实现了与复杂系统的可扩展交互。

Conclusion: Gatekeeper Protocol为构建更稳健、可预测和基于现实的AI代理提供了基础方法，适用于任何结构化知识领域。

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents,
yet their practical utility is fundamentally constrained by a limited context
window and state desynchronization resulting from the LLMs' stateless nature
and inefficient context management. These limitations lead to unreliable
output, unpredictable behavior, and inefficient resource usage, particularly
when interacting with large, structured, and sensitive knowledge systems such
as codebases and documents. To address these challenges, we introduce the
Gatekeeper Protocol, a novel, domain-agnostic framework that governs
agent-system interactions. Our protocol mandates that the agent first operate
and reason on a minimalist, low-fidelity "latent state" representation of the
system to strategically request high-fidelity context on demand. All
interactions are mediated through a unified JSON format that serves as a
declarative, state-synchronized protocol, ensuring the agent's model of the
system remains verifiably grounded in the system's reality. We demonstrate the
efficacy of this protocol with Sage, a reference implementation of the
Gatekeeper Protocol for software development. Our results show that this
approach significantly increases agent reliability, improves computational
efficiency by minimizing token consumption, and enables scalable interaction
with complex systems, creating a foundational methodology for building more
robust, predictable, and grounded AI agents for any structured knowledge
domain.

</details>


### [74] [Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates](https://arxiv.org/abs/2510.14900)
*Wen-Kwang Tsao,Yao-Ching Yu,Chien-Ming Huang*

Main category: cs.AI

TL;DR: 提出了一种无需标注数据或模型权重更新的强化学习代理，通过生成针对性网络搜索查询来收集外部证据，迭代改进企业日志的模式映射准确性。


<details>
  <summary>Details</summary>
Motivation: 企业智能平台需要集成多个第三方供应商的日志，但供应商文档在测试时往往不可用、不完整或格式混乱，导致模式映射困难。

Method: 使用强化学习代理，在推理时：1)识别模糊的字段映射尝试；2)生成针对性网络搜索查询收集外部证据；3)应用基于置信度的奖励迭代优化映射。

Result: 在将Microsoft Defender for Endpoint日志转换为通用模式的实验中，映射准确率从56.4%(仅LLM)提升到72.73%(RAG)再到93.94%(100次迭代后)，同时将需要专家审查的低置信度映射减少了85%。

Conclusion: 该方法提供了一种证据驱动、透明的解决方案，为更稳健、可扩展、高效和灵活的企业问题解决铺平了道路。

Abstract: The Enterprise Intelligence Platform must integrate logs from numerous
third-party vendors in order to perform various downstream tasks. However,
vendor documentation is often unavailable at test time. It is either misplaced,
mismatched, poorly formatted, or incomplete, which makes schema mapping
challenging. We introduce a reinforcement learning agent that can self-improve
without labeled examples or model weight updates. During inference, the agent:
1) Identifies ambiguous field-mapping attempts. 2) Generates targeted
web-search queries to gather external evidence. 3) Applies a confidence-based
reward to iteratively refine its mappings. To demonstrate this concept, we
converted Microsoft Defender for Endpoint logs into a common schema. Our method
increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\%
over 100 iterations using GPT-4o. At the same time, it reduced the number of
low-confidence mappings requiring expert review by 85\%. This new approach
provides an evidence-driven, transparent method for solving future industry
problems, paving the way for more robust, accountable, scalable, efficient,
flexible, adaptable, and collaborative solutions.

</details>


### [75] [Agentic Design of Compositional Machines](https://arxiv.org/abs/2510.14980)
*Wenqian Zhang,Weiyang Liu,Zhen Liu*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型在组合式机器设计中的能力，通过BesiegeField测试平台评估LLM在空间推理、策略组装和指令遵循等方面的表现，并探索了强化学习作为改进路径。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否能够学习创造复杂机器，这是人类智能的重要标志和工程实践的基础。

Method: 使用基于Besiege游戏构建的BesiegeField测试平台，对最先进的LLM进行基准测试，包括代理工作流程评估，并进行强化学习微调实验。

Result: 当前开源模型在组合式机器设计任务中表现不足，需要改进空间推理、策略组装和指令遵循等关键能力。

Conclusion: 机器设计任务处于语言、机器设计和物理推理的交叉领域，存在开放挑战，强化学习是潜在的改进方向。

Abstract: The design of complex machines stands as both a marker of human intelligence
and a foundation of engineering practice. Given recent advances in large
language models (LLMs), we ask whether they, too, can learn to create. We
approach this question through the lens of compositional machine design: a task
in which machines are assembled from standardized components to meet functional
demands like locomotion or manipulation in a simulated physical environment. To
support this investigation, we introduce BesiegeField, a testbed built on the
machine-building game Besiege, which enables part-based construction, physical
simulation and reward-driven evaluation. Using BesiegeField, we benchmark
state-of-the-art LLMs with agentic workflows and identify key capabilities
required for success, including spatial reasoning, strategic assembly, and
instruction-following. As current open-source models fall short, we explore
reinforcement learning (RL) as a path to improvement: we curate a cold-start
dataset, conduct RL finetuning experiments, and highlight open challenges at
the intersection of language, machine design, and physical reasoning.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [76] [Why your boss isn't worried about AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fboydkane.com%2Fessays%2Fboss%3Futm_source=tldrai/1/01000199e8038b14-272a4485-e17a-494d-b089-ba07718fde1e-000000/u6C_J6BTgxio7To5WhOhi_GBh75HpPn_WnAz2IfhJRQ=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI失败与传统软件错误不同，无法通过修复代码行来解决，因为AI基于海量训练数据，错误难以追踪且可能在不同提示下重现


<details>
  <summary>Details</summary>
Motivation: 解释为什么AI失败与传统软件错误有本质区别，帮助公众理解AI系统的不确定性和不可预测性

Method: 通过对比传统软件错误和AI系统故障的根源，分析AI训练数据的规模、复杂性和不可理解性

Result: AI失败源于11.25万亿词的训练数据，无法像传统软件那样定位和永久修复问题，修复的行为可能在不同提示下重现

Conclusion: AI系统的内在不确定性使得其失败模式与传统软件完全不同，需要新的理解和应对方法

Abstract: Why your boss isn't worried about AI (11 minute read) The public thinks AI bugs work like software bugs: find the missing semicolon, fix the problem, move on. However, AI failures emerge from training on 11.25 trillion words that would take 85,000 years to read, not buggy code lines. Unlike traditional software, where you can pinpoint and permanently fix problems, AI mistakes can't be traced, "fixed" behaviors can resurface with different prompts, and nobody, including the labs, understands w...

</details>


### [77] [Mapping Tomorrow's AI Landscape](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.amazon.science%2Fblog%2Fscientific-frontiers-of-agentic-ai%3Futm_campaign=scientific-frontiers-of-agentic-ai%26utm_medium=employer-brand%26utm_source=linkedin%26utm_content=scientific-frontiers-of-agentic-ai%26utm_term=2025-october/1/01000199e8038b14-272a4485-e17a-494d-b089-ba07718fde1e-000000/CKp3_LarBiVSDApG8X2zyYcjZ-ylpo6YlGWdDPAUPLs=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 亚马逊科学分析了推动AI代理系统发展的科学基础，探讨这些系统将如何重塑我们在商业、物流等领域与技术的互动方式。


<details>
  <summary>Details</summary>
Motivation: 研究下一代AI系统的发展趋势，这些系统不仅能够响应，还能预测、制定策略并自主行动，旨在推动AI从被动响应向主动代理的转变。

Method: 通过分析科学基础来研究代理AI系统，探讨其在商业和物流等领域的应用潜力。

Result: 识别了推动AI代理系统发展的关键技术挑战和科学基础，展示了这些系统对未来技术交互方式的潜在影响。

Conclusion: 代理AI系统将重塑人机交互方式，在商业和物流等领域具有重要应用前景，需要解决相关的科学挑战才能实现其潜力。

Abstract: Mapping Tomorrow's AI Landscape (Sponsor) The next generation of AI won't just respond—it will anticipate, strategize, and act autonomously. Amazon Science analyzes the scientific foundations driving this transformation, offering insights into how agentic systems will reshape how we interact with technology in commerce, logistics, and beyond.Explore the scientific challenges of agentic AI → Ready to impact the future of AI, today? Join our team →

</details>


### [78] [Petri](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsafety-research%2Fpetri%3Futm_source=tldrai/1/01000199e8038b14-272a4485-e17a-494d-b089-ba07718fde1e-000000/SPgRONK7-Xkq38ZULLVAH2RC9KEBiCN8JY_4Ofr2zbk=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Petri是一个对齐审计代理，用于现实假设测试，能够快速验证新假设而无需构建定制评估


<details>
  <summary>Details</summary>
Motivation: 传统对齐评估需要花费数周时间构建定制测试环境，研究人员需要更快速验证新假设的工具

Method: Petri自主创建测试环境，运行多轮审计，并对交互记录进行评分以识别潜在问题行为

Result: Petri能够在几分钟内完成假设测试，大幅缩短了传统评估所需的时间

Conclusion: Petri为对齐研究提供了高效的假设测试工具，显著提升了研究效率

Abstract: Petri (GitHub Repo) Petri is an alignment auditing agent for realistic hypothesis testing. It allows researchers to test new hypotheses in minutes instead of building bespoke evals over weeks. Petri autonomously crafts environments, runs multi-turn audits, and scores transcripts to surface concerning behavior.

</details>


### [79] [The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against LLM Jailbreaks and Prompt Injections](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fpdf%2F2510.09023%3Futm_source=tldrai/1/01000199e8038b14-272a4485-e17a-494d-b089-ba07718fde1e-000000/Ln25Cc3M1RAJsDkcTqJ_j4RrZnOyUo7QD9YcCZOyJf0=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 研究人员系统性地破解了12个AI安全防御措施，使用自适应攻击方法在原本报告接近零攻击率的情况下实现了超过90%的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前针对LLM越狱和提示注入的防御措施声称具有很高的安全性，但实际上可能仍然存在漏洞。

Method: 采用自适应攻击方法，针对12个最新的AI安全防御措施进行系统性测试，发现了四类基本缺陷。

Result: 自适应攻击成功绕过了所有测试的防御措施，攻击成功率超过90%，而原始防御报告的攻击率接近零。

Conclusion: 现有的AI安全防御措施存在根本性缺陷，需要更强大的安全机制来应对自适应攻击。

Abstract: The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against LLM Jailbreaks and Prompt Injections (32 minute read) Jailbreaks and prompt injection are far from solved. Researchers from OpenAI, Anthropic, and Google DeepMind systematically broke 12 recent AI safety defenses using adaptive attacks that achieved over 90% success rates despite the defenses originally reporting near-zero attack rates. Their methods exposed four categories of fundamental flaws: "prerequisite" attack...

</details>


### [80] [Claude Haiku 4.5 Wrote 62% More Code But Scored 16% Lower Than Sonnet 4.5](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcodelens.ai%2Fblog%2Fclaude-haiku-vs-sonnet-overengineering%3Futm_source=tldrwebdev/1/01000199ecb4c6af-ac9aed21-9400-4708-8b07-b56f987bab1d-000000/kcarnwU0im841H3jfPi2HhP6q2h49LAebRhTQ1Hi2DI=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Haiku 4.5在WebSocket重构任务中比Sonnet 4.5多写了62%的代码，但质量得分低16%，过度工程化导致代码质量、正确性和可维护性下降。


<details>
  <summary>Details</summary>
Motivation: 评估Anthropic新发布的Claude Haiku 4.5模型在代码生成任务中的表现，并与Claude Sonnet 4.5进行对比分析。

Method: 在WebSocket重构任务中测试两个模型，比较生成的代码量和质量得分。

Result: Haiku 4.5生成代码量最多但质量最差，过度追求全面性而牺牲了代码质量、正确性和可维护性。

Conclusion: 代码量多不等于质量好，Haiku 4.5在代码生成任务中存在过度工程化问题。

Abstract: Claude Haiku 4.5 Wrote 62% More Code But Scored 16% Lower Than Sonnet 4.5 (8 minute read) Anthropic's new Claude Haiku 4.5 model was tested on a WebSocket refactoring task. While it produced the most code compared to other models, its quality was much lower than Claude Sonnet 4.5. Haiku 4.5 over-engineered the solution, prioritizing thoroughness at the expense of code quality, correctness, and maintainability.

</details>


### [81] [Unlocking LLM Jailbreaks: Deconstructing Pliny's Prompt and Advanced Evasion Techniques](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.gopenai.com%2Funlocking-llm-jailbreaks-deconstructing-plinys-prompt-and-advanced-evasion-techniques-d29e8b65ca33%3Futm_source=tldrinfosec/1/01000199ed22a527-b324ccfc-3224-44f0-95c5-c9e2ac059b8e-000000/p1O1lHCCvskL_LnF6LTksYECH9HXwK5DlBfBeiAV-xk=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文分析了Pliny the Liberator的LLM越狱技术，包括指令优先级、混淆重定向、护栏否定、认知过载、上下文误导、任务隧道、语气误导和输出数量偏见等方法。


<details>
  <summary>Details</summary>
Motivation: 理解Pliny的LLM越狱技术，揭示这些技术如何有效绕过LLM的安全防护机制。

Method: 通过逐步分析Pliny在GitHub上分享的越狱提示，解构其使用的各种技术手段。

Result: 识别出8种关键的越狱技术，这些技术能够成功绕过LLM的安全防护。

Conclusion: Pliny的越狱技术展示了LLM安全防护的脆弱性，需要开发更强大的防御机制。

Abstract: Unlocking LLM Jailbreaks: Deconstructing Pliny's Prompt and Advanced Evasion Techniques (6 minute read) Pliny the Liberator is a well-known LLM jailbreaker who shares their jailbreaking prompts on GitHub. Pliny's prompts make use of several techniques to effectively jailbreak LLMs, such as instruction prioritization, obfuscation and redirection, guardrail negation, cognitive overload, contextual misdirection, task tunneling, tonal misdirection, and output quantity bias. This post steps throug...

</details>
