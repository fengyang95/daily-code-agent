<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 4]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.LG](#cs.LG) [Total: 3]
- [tldr.article](#tldr.article) [Total: 26]
- [cs.SE](#cs.SE) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Talking to Yourself: Defying Forgetting in Large Language Models](https://arxiv.org/abs/2602.20162)
*Yutao Sun,Mingshuai Chen,Tiancheng Zhao,Phillip Miao,Zilun Zhang,Haozhan Shen,Ruizhe Zhu,Jianwei Yin*

Main category: cs.CL

TL;DR: 提出SA-SFT方法，通过LLM生成自我对话数据与任务数据混合训练，有效缓解灾难性遗忘，无需外部数据或额外调优。


<details>
  <summary>Details</summary>
Motivation: 微调大语言模型时，使用特定任务数据容易导致灾难性遗忘，损害模型的通用知识和推理能力，需要一种轻量级解决方案。

Method: SA-SFT方法：在微调前让LLM生成自我对话数据，将这些自我生成的数据与任务数据混合训练，不改变优化或训练计划。

Result: 在50个评估场景中，SA-SFT保持与原始模型相当的性能，在40个案例中取得最佳结果，优于层冻结和外部数据混合等基线方法。

Conclusion: 自我增强为LLM适应提供了一种简单有效的机制，能缓解灾难性遗忘，理论分析表明遗忘部分源于风格诱导的参数漂移，自我对齐可抵消此效应。

Abstract: Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules.
  Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.

</details>


### [2] [ConceptRM: The Quest to Mitigate Alert Fatigue through Consensus-Based Purity-Driven Data Cleaning for Reflection Modelling](https://arxiv.org/abs/2602.20166)
*Yongda Yu,Lei Zhang,Xinxin Guo,Minghui Yu,Zhengqi Zhuang,Guoping Rong,Haifeng Shen,Zhengfeng Li,Boge Wang,Guoan Zhang,Bangyu Xiang,Xiaobin Xu*

Main category: cs.CL

TL;DR: 提出ConceptRM方法，利用少量专家标注作为锚点，通过扰动数据集和协同教学训练多个模型，从噪声数据中识别可靠负样本，以低成本构建高质量语料库训练反射模型拦截虚假警报。


<details>
  <summary>Details</summary>
Motivation: 智能代理应用中大量虚假警报导致用户"警报疲劳"，忽略关键问题。现有方法通过用户反馈数据训练反射模型过滤虚假警报，但生产环境收集的数据噪声大，人工清洗成本高。

Method: ConceptRM方法：1）使用少量专家标注作为锚点；2）创建不同噪声比例的扰动数据集；3）采用协同教学训练多个不同模型进行协作学习；4）通过分析模型共识决策从噪声数据中识别可靠负样本。

Result: 实验结果显示，ConceptRM显著提升虚假警报拦截效果，在最小标注成本下，在领域内数据集上比最先进的LLM基线提升53.31%，在领域外数据集上提升41.67%。

Conclusion: ConceptRM能够有效从噪声数据中构建高质量训练语料，以低成本训练反射模型拦截虚假警报，解决警报疲劳问题。

Abstract: In many applications involving intelligent agents, the overwhelming volume of alerts (mostly false) generated by the agents may desensitize users and cause them to overlook critical issues, leading to the so-called ''alert fatigue''. A common strategy is to train a reflection model as a filter to intercept false alerts with labelled data collected from user verification feedback. However, a key challenge is the noisy nature of such data as it is often collected in production environments. As cleaning noise via manual annotation incurs high costs, this paper proposes a novel method ConceptRM for constructing a high-quality corpus to train a reflection model capable of effectively intercepting false alerts. With only a small amount of expert annotations as anchors, ConceptRM creates perturbed datasets with varying noise ratios and utilizes co-teaching to train multiple distinct models for collaborative learning. By analyzing the consensus decisions of these models, it effectively identifies reliable negative samples from a noisy dataset. Experimental results demonstrate that ConceptRM significantly enhances the interception of false alerts with minimal annotation cost, outperforming several state-of-the-art LLM baselines by up to 53.31% on in-domain datasets and 41.67% on out-of-domain datasets.

</details>


### [3] [What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance](https://arxiv.org/abs/2602.20300)
*William Watson,Nicole Cho,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: 研究发现查询特征（如从句嵌套深度、意图明确性等）与LLM幻觉风险相关，为查询改写和干预研究提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 传统上LLM幻觉被视为模型或解码策略的缺陷，但本文从语言学角度出发，认为查询的形式也会影响模型响应。研究旨在识别哪些查询特征会增加幻觉风险。

Method: 构建22维查询特征向量，涵盖从句复杂度、词汇稀有性、指代、否定、可回答性和意图明确性等语言学特征。使用369,837个真实世界查询进行大规模分析，研究查询特征与幻觉倾向的相关性。

Result: 分析揭示了稳定的"风险图谱"：某些特征（如深层从句嵌套和欠明确性）与更高的幻觉倾向相关；而明确的意图基础和可回答性则与较低的幻觉率相关；其他特征（如领域特异性）则表现出混合的、数据集和模型依赖的效应。

Conclusion: 研究建立了与幻觉风险相关的可观察查询特征表示，为引导式查询改写和未来干预研究铺平了道路。

Abstract: Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.

</details>


### [4] [The Art of Efficient Reasoning: Data, Reward, and Optimization](https://arxiv.org/abs/2602.20945)
*Taiqiang Wu,Zenan Zu,Bo Zhou,Ngai Wong*

Main category: cs.CL

TL;DR: 该论文系统研究LLM的高效推理机制，提出细粒度评估指标，揭示训练遵循两阶段范式（长度适应和推理精炼），发现训练相对简单提示可避免长度崩溃，且习得的长度偏差可跨领域泛化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）通过扩展的思维链（CoT）推理获得性能提升，但面临沉重的计算开销。高效推理旨在激励短而准确的思考轨迹，通常通过强化学习（RL）进行奖励塑造。本文旨在系统研究LLM高效推理的机制。

Method: 提出细粒度评估指标（包括基于正确性的长度分布和2k到32k令牌预算范围内的性能评估）。在统一协议下进行大量实验（约20万GPU小时），解构训练提示和rollouts、奖励塑造和优化策略。使用Qwen3系列模型（0.6B到30B）验证发现。

Result: 揭示训练过程遵循两阶段范式：长度适应和推理精炼。关键发现是训练相对简单的提示可以确保正奖励信号的密度，从而避免长度崩溃。习得的长度偏差可以跨领域泛化。所有发现被提炼为有价值的见解和实用指南。

Conclusion: 通过系统研究LLM高效推理机制，提出了细粒度评估框架，揭示了训练的两阶段范式，发现了避免长度崩溃的关键策略，并验证了长度偏差的跨领域泛化能力，为高效推理提供了实用指南。

Abstract: Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Implicit Intelligence -- Evaluating Agents on What Users Don't Say](https://arxiv.org/abs/2602.20424)
*Ved Sirdeshmukh,Marc Wetter*

Main category: cs.AI

TL;DR: 论文提出了Implicit Intelligence评估框架和Agent-as-a-World平台，用于测试AI代理能否理解人类请求中的隐含约束，而不仅仅是遵循显式指令。评估显示当前最佳模型在205个场景中仅达到48.3%通过率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的人类请求本质上是未充分指定的，依赖共享上下文和未声明的约束。当前的代理基准只测试显式指令遵循，未能评估代理是否能推理隐含需求，如可访问性需求、隐私边界、灾难性风险和上下文约束。

Method: 提出了Implicit Intelligence评估框架，测试AI代理是否能从提示遵循转变为真正的目标实现者。开发了Agent-as-a-World平台，其中交互世界用人类可读的YAML文件定义，由语言模型模拟。场景设计具有用户请求表面简单、正确解决方案隐藏复杂、约束可通过环境探索发现的特点。

Result: 评估了16个前沿和开源模型在205个场景中的表现，发现即使最佳模型也仅达到48.3%的场景通过率，表明在弥合字面指令遵循和类人上下文推理之间的差距方面仍有很大改进空间。

Conclusion: 当前AI代理在处理隐含约束方面存在显著不足，需要开发能够进行类人上下文推理的系统。提出的评估框架和平台为衡量和推进这一能力提供了重要工具。

Abstract: Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.

</details>


### [6] [PyVision-RL: Forging Open Agentic Vision Models via RL](https://arxiv.org/abs/2602.20739)
*Shitian Zhao,Shaoheng Lin,Ming Li,Haoquan Zhang,Wenshuo Peng,Kaipeng Zhang,Chen Wei*

Main category: cs.AI

TL;DR: PyVision-RL是一个强化学习框架，用于防止多模态智能体训练中的交互崩溃，通过过采样-过滤-排序策略和累积工具奖励来维持多轮工具使用，并开发了图像和视频理解模型。


<details>
  <summary>Details</summary>
Motivation: 多模态智能体在强化学习中经常出现交互崩溃问题，模型倾向于减少工具使用和多轮推理，限制了智能体行为的优势。

Method: 提出PyVision-RL框架，结合过采样-过滤-排序的rollout策略和累积工具奖励来防止交互崩溃；开发统一训练流程，创建PyVision-Image和PyVision-Video模型；对于视频推理，采用按需上下文构建，选择性采样任务相关帧以减少视觉token使用。

Result: 实验显示强大的性能和改进的效率，证明了持续交互和按需视觉处理对于可扩展多模态智能体的重要性。

Conclusion: 持续交互和按需视觉处理是构建可扩展多模态智能体的关键要素，PyVision-RL框架有效解决了交互崩溃问题。

Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.

</details>


### [7] [Tool Building as a Path to "Superintelligence"](https://arxiv.org/abs/2602.21061)
*David Koplow,Tomer Galanti,Tomaso Poggio*

Main category: cs.AI

TL;DR: 论文提出一个基准来测量LLM在逻辑分布外推理中的步成功概率γ，发现小模型γ值随深度超线性下降，而前沿模型在该任务上表现出部分鲁棒性，精确工具调用是实现通用超智能的关键。


<details>
  <summary>Details</summary>
Motivation: Diligent Learner框架认为LLM可以通过测试时搜索实现超智能，前提是有足够的步成功概率γ。本研究旨在设计一个基准来测量γ值，特别是在逻辑分布外推理任务中，以评估LLM能否通过仔细整合所有信息来可靠解决信息理论上不可能的任务。

Method: 构建了一类涉及GF(2)电路重建的任务，这些任务随着推理步骤增加而变得更加困难。从信息论角度看，除非LLM仔细整合所有提供的信息，否则无法可靠解决。通过这个基准分析不同规模LLM的γ值变化趋势。

Result: 小型LLM的γ值随着深度增加呈超线性下降，而前沿模型在该任务上表现出部分鲁棒性。研究发现，成功的规模化推理依赖于精确的工具调用，工具设计被确定为LLM通过Diligent Learner框架实现通用超智能的关键能力。

Conclusion: 工具设计是LLM通过Diligent Learner框架实现通用超智能的关键瓶颈。虽然前沿模型在逻辑分布外推理任务上表现出部分鲁棒性，但精确的工具调用能力对于实现可靠的规模化推理至关重要。

Abstract: The Diligent Learner framework suggests LLMs can achieve superintelligence via test-time search, provided a sufficient step-success probability $γ$. In this work, we design a benchmark to measure $γ$ on logical out-of-distribution inference. We construct a class of tasks involving GF(2) circuit reconstruction that grow more difficult with each reasoning step, and that are, from an information-theoretic standpoint, impossible to reliably solve unless the LLM carefully integrates all of the information provided. Our analysis demonstrates that while the $γ$ value for small LLMs declines superlinearly as depth increases, frontier models exhibit partial robustness on this task. Furthermore, we find that successful reasoning at scale is contingent upon precise tool calls, identifying tool design as a critical capability for LLMs to achieve general superintelligence through the Diligent Learner framework.

</details>


### [8] [Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination](https://arxiv.org/abs/2602.20517)
*Rakshit Trivedi,Kartik Sharma,David C Parkes*

Main category: cs.AI

TL;DR: MIMIC框架利用语言作为行为意图的内部表示，通过视觉语言模型训练条件变分自编码器生成内部语言，再结合扩散行为克隆策略实现可引导的行为模仿，显著提升行为多样性和人类演示保真度。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习方法难以捕捉人类行为的多样性和非马尔可夫特性，且缺乏在推理时引导行为的能力。受人类认知过程中内部语言指导动作选择的理论启发，需要开发能够生成内部语言表示并实现行为引导的智能体。

Method: 提出MIMIC框架：1) 使用视觉语言模型作为语言支架训练条件变分自编码器，从观察中生成内部语言；2) 采用基于扩散的行为克隆策略，根据当前观察和生成的内部语言选择动作；3) 在推理时通过特定行为语言条件化智能体实现精细行为引导。

Result: 在机器人操作任务和人机协作游戏中，MIMIC显著提升了行为多样性和人类演示保真度，无需额外演示训练即可实现细微的行为引导。

Conclusion: MIMIC框架通过语言作为内部行为意图表示，有效解决了模仿学习中行为多样性和可引导性的挑战，为人机协调提供了更灵活、更符合人类认知的智能体。

Abstract: Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.

</details>


### [9] [A Benchmark for Deep Information Synthesis](https://arxiv.org/abs/2602.21143)
*Debjit Paul,Daniel Murphy,Milan Gritta,Ronald Cardenas,Victor Prokhorov,Lena Sophia Bolliger,Aysim Toker,Roy Miles,Andreea-Maria Oncescu,Jasivan Alex Sivakumar,Philipp Borchert,Ismail Elezi,Meiru Zhang,Ka Yiu Lee,Guchun Zhang,Jun Wang,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: DEEPSYNTH是一个新的基准测试，用于评估LLM智能体在需要多源信息整合和深度推理的现实复杂任务上的表现，现有智能体在该基准上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前评估基准无法充分评估LLM智能体解决现实世界复杂任务的能力，这些任务需要从多个来源综合信息并进行超越简单事实检索的推理。

Method: 通过多阶段数据收集流程构建DEEPSYNTH基准，包含120个任务，涵盖7个领域和67个国家的数据源。评估了11个最先进的LLM和深度研究智能体。

Result: 在DEEPSYNTH上，最佳模型仅获得8.97的F1分数和17.5的LLM-judge指标，表明基准难度很高。当前智能体在处理幻觉和大信息空间推理方面存在困难。

Conclusion: DEEPSYNTH是一个关键的基准测试，能够指导未来研究，揭示当前智能体在复杂现实任务中的局限性。

Abstract: Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.

</details>


### [10] [Aletheia tackles FirstProof autonomously](https://arxiv.org/abs/2602.21201)
*Tony Feng,Junehyuk Jung,Sang-hyun Kim,Carlo Pagano,Sergei Gukov,Chiang-Chiang Tsai,David Woodruff,Adel Javanmard,Aryan Mokhtari,Dawsen Hwang,Yuri Chervonyi,Jonathan N. Lee,Garrett Bingham,Trieu H. Trinh,Vahab Mirrokni,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: Aletheia数学研究代理在FirstProof挑战中自主解决了10个问题中的6个，使用Gemini 3 Deep Think模型，专家对问题8的解决存在分歧。


<details>
  <summary>Details</summary>
Motivation: 评估Aletheia数学研究代理在真实数学问题解决挑战中的性能，验证其在自主数学研究任务上的能力。

Method: 使用Gemini 3 Deep Think模型驱动的Aletheia代理，在FirstProof挑战规定时间内自主解决数学问题，由专家评估解决方案。

Result: Aletheia成功解决了10个问题中的6个（问题2、5、7、8、9、10），专家对问题8的解决存在分歧，其他5个问题获得一致认可。

Conclusion: Aletheia在数学研究任务上表现出显著能力，能够自主解决复杂的数学问题，展示了AI在数学研究领域的潜力。

Abstract: We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.

</details>


### [11] [Diffusion Modulation via Environment Mechanism Modeling for Planning](https://arxiv.org/abs/2602.20422)
*Hanping Zhang,Yuhong Guo*

Main category: cs.AI

TL;DR: 提出DMEMM方法，通过建模环境机制来调制扩散模型，解决传统扩散规划方法在离线RL中轨迹生成不一致的问题


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散的规划方法在离线强化学习中生成轨迹时，往往忽略了过渡之间的内在一致性要求，导致生成的轨迹与真实环境机制存在显著差异

Method: 提出DMEMM方法，通过建模环境机制（特别是过渡动态和奖励函数）来调制扩散模型的训练过程

Result: 实验结果表明DMEMM在离线强化学习规划任务中达到了最先进的性能

Conclusion: 通过将环境机制建模融入扩散模型训练，DMEMM能够生成更一致、更符合真实环境动态的轨迹，从而提升离线RL规划效果

Abstract: Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Controllable Exploration in Hybrid-Policy RLVR for Multi-Modal Reasoning](https://arxiv.org/abs/2602.20197)
*Zhuoxu Huang,Mengxi Jia,Hao Sun,Xuelong Li,Jungong Han*

Main category: cs.LG

TL;DR: 提出CalibRL框架，通过分布感知优势加权和LeakyReLU激活函数实现可控探索，解决RLVR训练中的熵崩溃和策略退化问题


<details>
  <summary>Details</summary>
Motivation: RLVR训练中，MLLM的巨大状态空间和稀疏奖励导致熵崩溃、策略退化或次优行为过度利用，需要可控的探索策略

Method: 提出混合策略RLVR框架CalibRL：1) 分布感知优势加权按组稀有度缩放更新以校准分布；2) LeakyReLU激活函数利用专家知识作为校准基线调节过度自信更新

Result: 在8个基准测试（包括域内和域外设置）中均显示一致改进，验证了可控混合策略RLVR训练的有效性

Conclusion: CalibRL通过引导方式增加策略熵，缓解模型策略与专家轨迹之间的分布不匹配，实现探索与利用的更稳定平衡

Abstract: Reinforcement Learning with verifiable rewards (RLVR) has emerged as a primary learning paradigm for enhancing the reasoning capabilities of multi-modal large language models (MLLMs). However, during RL training, the enormous state space of MLLM and sparse rewards often leads to entropy collapse, policy degradation, or over-exploitation of suboptimal behaviors. This necessitates an exploration strategy that maintains productive stochasticity while avoiding the drawbacks of uncontrolled random sampling, yielding inefficient exploration. In this paper, we propose CalibRL, a hybrid-policy RLVR framework that supports controllable exploration with expert guidance, enabled by two key mechanisms. First, a distribution-aware advantage weighting scales updates by group rareness to calibrate the distribution, therefore preserving exploration. Meanwhile, the asymmetric activation function (LeakyReLU) leverages the expert knowledge as a calibration baseline to moderate overconfident updates while preserving their corrective direction. CalibRL increases policy entropy in a guided manner and clarifies the target distribution by estimating the on-policy distribution through online sampling. Updates are driven by these informative behaviors, avoiding convergence to erroneous patterns. Importantly, these designs help alleviate the distributional mismatch between the model's policy and expert trajectories, thereby achieving a more stable balance between exploration and exploitation. Extensive experiments across eight benchmarks, including both in-domain and out-of-domain settings, demonstrate consistent improvements, validating the effectiveness of our controllable hybrid-policy RLVR training. Code is available at https://github.com/zhh6425/CalibRL.

</details>


### [13] [Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs](https://arxiv.org/abs/2602.21198)
*Yining Hong,Huang Huang,Manling Li,Li Fei-Fei,Jiajun Wu,Yejin Choi*

Main category: cs.LG

TL;DR: 提出Reflective Test-Time Planning框架，通过反思行动中和行动后两种反思模式，让具身LLM在部署中积累经验而非重复错误，显著提升长期任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前具身LLM虽然具备高级任务推理能力，但缺乏反思机制，导致部署变成一系列独立试验，错误重复出现而无法积累成经验。

Method: 提出Reflective Test-Time Planning框架，包含两种反思模式：1) 反思行动中：使用测试时扩展生成和评分多个候选行动；2) 反思行动后：使用测试时训练更新内部反思模型和行动策略。还包括回顾性反思，重新评估早期决策并进行后见之明的模型更新。

Result: 在Long-Horizon Household基准和MuJoCo Cupboard Fitting基准上相比基线模型取得显著提升，消融研究验证了反思行动中和反思行动后的互补作用。真实机器人试验展示了通过反思实现的行为修正。

Conclusion: 反思测试时规划框架通过整合反思行动中和反思行动后两种模式，使具身LLM能够从经验中学习并改进，解决了当前系统缺乏反思能力的问题。

Abstract: Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.

</details>


### [14] [Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training](https://arxiv.org/abs/2602.21189)
*Anas Barakat,Souradip Chakraborty,Khushbu Pahwa,Amrit Singh Bedi*

Main category: cs.LG

TL;DR: 论文研究了pass@k优化中pass@1下降的权衡问题，揭示了梯度冲突源于提示干扰，特别是负干扰提示的重新加权导致优化方向偏离。


<details>
  <summary>Details</summary>
Motivation: pass@k是广泛使用的可验证LLM任务性能指标，但实践中发现一个反复出现的权衡：优化pass@k时pass@1会下降。由于延迟成本、验证器覆盖不完善和需要可靠单次回退，pass@1仍然是硬操作约束，因此理解这一权衡的起源至关重要。

Method: 通过理论分析研究pass@k策略优化降低pass@1的机制，重点关注梯度冲突和提示干扰。分析显示pass@k优化会隐式地将提示重新加权到低成功率的提示，当这些提示是负干扰时，它们的上加权会使pass@k更新方向偏离pass@1方向。在可验证数学推理任务上进行了大语言模型实验验证。

Result: 理论分析表明pass@k策略梯度可能与pass@1梯度冲突，因为pass@k优化会重新加权提示，特别是负干扰提示的上加权会旋转更新方向。实验在数学推理任务上验证了理论发现。

Conclusion: 揭示了pass@k优化中pass@1下降的机制源于梯度冲突和提示干扰，为理解和缓解这一重要权衡提供了理论基础。

Abstract: Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [15] [Sketching with code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.proofofconcept.pub%2Fp%2Fsketching-with-code%3Futm_source=tldrfounders/1/0100019c8aad48ef-cb9a5d0e-cdf2-4a62-9cfa-19adc2178efa-000000/1t2wo3l1NHazHkOctSrhH-RB48zTD5Ufxuw_amLuCcQ=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用LLM作为草图工具，可以在单次会话中将粗略想法转化为可工作原型


<details>
  <summary>Details</summary>
Motivation: 传统原型开发过程耗时且复杂，需要从想法到实现的多个步骤。本文旨在简化这一过程，利用LLM作为快速草图工具，让开发者能更高效地将初步概念转化为可工作的原型。

Method: 将LLM作为代码草图工具，通过自然语言描述想法，LLM生成初步代码实现，然后迭代优化。这种方法强调快速原型开发，在单次会话中完成从概念到可工作代码的转化。

Result: 该方法显著加速了原型开发过程，使开发者能够在几分钟内从粗略想法创建出可工作的代码原型。LLM作为草图工具提供了快速迭代和即时反馈的能力。

Conclusion: LLM作为代码草图工具改变了原型开发流程，使快速概念验证变得更加高效和可访问。这种方法降低了原型开发的门槛，促进了更快的创新和实验。

Abstract: Sketching with code (4 minute read) You can now go from rough idea to working prototype in a single sitting using LLMs as a sketching tool.

</details>


### [16] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfounders%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c8aad48ef-cb9a5d0e-cdf2-4a62-9cfa-19adc2178efa-000000/TfH91eRoBjpDCaifdSANhImDs3Ya2txGoWeaG_cDFb0=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用LLM作为草图工具，可以在短时间内将粗略想法转化为工作原型


<details>
  <summary>Details</summary>
Motivation: 传统原型开发过程耗时较长，需要从想法到实现经过多个步骤，作者希望探索LLM作为快速草图工具的潜力，加速创意实现过程

Method: 将LLM作为草图工具，通过自然语言描述想法，让LLM生成代码实现，快速迭代从粗略想法到工作原型的过程

Result: 能够在4分钟内完成从粗略想法到工作原型的转变，显著加速原型开发过程，使创意实现更加高效

Conclusion: LLM作为草图工具具有巨大潜力，能够极大缩短从想法到实现的时间，为快速原型开发提供了新的可能性

Abstract: Sketching with code (4 minute read) You can now go from rough idea to working prototype in a single sitting using LLMs as a sketching tool.

</details>


### [17] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c8aad48ef-cb9a5d0e-cdf2-4a62-9cfa-19adc2178efa-000000/TspXw4WSLd3JHYApFkvbHh3EBGb_pDP2oakI_9nO1pg=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用LLMs作为草图工具，可以在单次会话中从粗略想法快速构建工作原型


<details>
  <summary>Details</summary>
Motivation: 传统原型开发过程耗时且复杂，需要从想法到实现的多步骤转换，希望利用LLMs加速从概念到工作原型的转化过程

Method: 将LLMs作为代码草图工具，通过自然语言描述想法，让模型生成可工作的代码原型，实现快速迭代和验证

Result: 能够在4分钟内从粗略想法创建出工作原型，大大缩短了开发周期，提高了原型设计的效率和灵活性

Conclusion: LLMs作为代码草图工具能够显著加速原型开发过程，使快速验证想法成为可能，改变了传统软件开发流程

Abstract: Sketching with code (4 minute read) You can now go from rough idea to working prototype in a single sitting using LLMs as a sketching tool.

</details>


### [18] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c8aad48ef-cb9a5d0e-cdf2-4a62-9cfa-19adc2178efa-000000/CQWqgAmq-FzVkdflqJBHSPi4mihBpaJL_5s5GOA_AcQ=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用LLM作为草图工具，可以在单次会话中将粗略想法转化为工作原型


<details>
  <summary>Details</summary>
Motivation: 传统原型开发过程耗时且复杂，需要从想法到实现的多步骤转换。LLM作为草图工具可以加速这一过程，让开发者快速验证概念。

Method: 使用大型语言模型作为代码草图工具，通过自然语言描述想法，LLM生成可工作的代码原型，实现快速迭代和概念验证。

Result: 开发者能够在单次会话中从粗略想法创建出工作原型，显著缩短开发周期，提高创意验证效率。

Conclusion: LLM作为代码草图工具改变了原型开发流程，使快速概念验证成为可能，为软件开发带来新的工作范式。

Abstract: Sketching with code (4 minute read) You can now go from rough idea to working prototype in a single sitting using LLMs as a sketching tool.

</details>


### [19] [Don't trust the code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcheckmarx.com%2Fresources%2Fdont-trust-the-code%2F%3Futm_source=tldr_email%26utm_medium=email%26utm_campaign=tldr_InfoSec_Newsletter/1/0100019c8ad4f839-4b4848dd-5899-45ca-851e-13a1f47d0fe1-000000/qrcVd4VWvamtVk6oEa6JQCQ0jr3xV4ieh61b7uSFSrg=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Checkmarx推出AI原生应用安全代理，嵌入AI IDE中实时检测和阻止AI编码助手生成的安全漏洞，保护现代代理式开发生命周期


<details>
  <summary>Details</summary>
Motivation: AI编码助手以机器速度和规模创建安全风险，需要实时防护措施来防止漏洞进入代码库

Method: 在AI IDE中嵌入AI原生应用安全代理，实时验证AI生成代码，强制执行安全策略，在提交前阻止漏洞

Result: Checkmarx Developer Assist提供实时安全防护，保护现代代理式开发生命周期(ADLC)

Conclusion: AI编码助手带来新的安全挑战，需要AI原生安全解决方案在开发过程中实时防护

Abstract: Don't trust the code (Sponsor) AI coding assistants create security risks at machine speed and massive scale. Checkmarx Developer Assist embeds an AI-native AppSec agent directly in AI IDEs, preventing vulnerabilities in real time before commit. It enforces policy, validates AI-generated code, and secures the modern Agentic Development Lifecycle (ADLC). Explore the guide or visit us at RSAC 2026.

</details>


### [20] [Anthropic Launches Claude Code Security for AI-Powered Vulnerability Scanning](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthehackernews.com%2F2026%2F02%2Fanthropic-launches-claude-code-security.html%3Futm_source=tldrinfosec/1/0100019c8ad4f839-4b4848dd-5899-45ca-851e-13a1f47d0fe1-000000/EHIvcBiM33XRL5wkqjC63YP_Q13XNu4QHadfPbd-ej0=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic推出Claude Code Security，利用AI进行代码漏洞扫描，超越传统静态分析，通过推理组件交互和数据流追踪来检测漏洞，采用多阶段验证减少误报。


<details>
  <summary>Details</summary>
Motivation: 传统静态代码分析工具存在局限性，无法充分理解组件间的复杂交互和数据流，导致漏洞检测不够全面。需要更智能的AI驱动方法来提高代码安全扫描的准确性和深度。

Method: 开发AI驱动的代码安全扫描工具，通过推理组件交互和追踪数据流来识别漏洞，采用多阶段验证过程过滤误报，为每个发现的漏洞分配严重性和置信度评级。

Result: 推出Claude Code Security作为有限研究预览版，面向企业和团队客户，提供超越传统静态分析的AI驱动漏洞扫描能力。

Conclusion: AI驱动的代码安全扫描工具能够更全面地检测漏洞，通过智能推理和多阶段验证提高检测准确性，为代码安全提供更有效的解决方案。

Abstract: Anthropic Launches Claude Code Security for AI-Powered Vulnerability Scanning (2 minute read) Anthropic launched Claude Code Security in limited research preview for Enterprise and Team customers, offering AI-powered codebase vulnerability scanning that goes beyond static analysis by reasoning about component interactions and tracing data flows. Findings undergo a multi-stage verification process to filter false positives, with each vulnerability assigned a severity and confidence rating alon...

</details>


### [21] [claude-code-config](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Ftrailofbits%2Fclaude-code-config%3Futm_source=tldrinfosec/1/0100019c8ad4f839-4b4848dd-5899-45ca-851e-13a1f47d0fe1-000000/xYMWBD5xvlYtVNJ0TFE-ZNvrTv2EOBcK0LUfmg_OI14=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Trail of Bits发布了一个针对Claude Code的配置仓库，包含沙箱、权限、钩子、技能、MCP服务器和工作流模式，通过安全审计和开发优化，提供预构建的斜杠命令用于自主PR审查、问题解决和Dependabot合并。


<details>
  <summary>Details</summary>
Motivation: 为Claude Code提供一个安全、优化的配置框架，通过安全审计确保代码代理在受限环境中安全运行，同时提供高效的工作流模式。

Method: 创建包含沙箱强化、权限控制、钩子、技能、MCP服务器的配置仓库，实现预构建的斜杠命令系统，支持并行代理处理PR审查、问题解决和Dependabot合并。

Result: 发布了一个功能完整的配置仓库，能够安全地运行Claude Code代理，提供自主的代码审查和问题解决能力，同时防止对敏感信息（SSH密钥、云凭证等）的访问。

Conclusion: 该配置仓库为Claude Code提供了一个安全、高效的生产环境配置方案，通过安全加固和预构建工作流提升了代码代理的安全性和实用性。

Abstract: claude-code-config (GitHub Repo) Trail of Bits released an opinionated configuration repository for Claude Code covering sandboxing, permissions, hooks, skills, MCP servers, and workflow patterns refined through security audits and development. The repository includes pre-built slash commands for autonomous PR review, issue resolution, and Dependabot merging using parallel agents, alongside sandbox hardening that blocks access to SSH keys, cloud credentials, crypto wallets, and shell configs....

</details>


### [22] [Minions: Stripe's one-shot, end-to-end coding agents—Part 2](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstripe.dev%2Fblog%2Fminions-stripes-one-shot-end-to-end-coding-agents-part-2%3Futm_source=tldrfintech/1/0100019c8ad5eef6-eff830ed-4bd0-4c09-bc76-109e2795f6ff-000000/1Pc2aNk8MA-IBnVeBYe7DHUc-yCPjCO1Sn-HmpjSBBw=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Stripe开发了名为"minions"的无人值守编码代理，每周产生1300+合并PR，运行在预热的隔离云开发环境中，通过"蓝图"系统结合确定性工作流步骤和灵活代理循环进行编排。


<details>
  <summary>Details</summary>
Motivation: Stripe需要提高开发效率，减少重复性编码任务，通过自动化代理处理常规编码工作，让工程师专注于更复杂的开发任务。

Method: 使用预热的隔离云开发环境（devboxes），通过"蓝图"系统结合确定性工作流步骤（如代码检查、推送、CI钩子）和灵活代理循环（实现任务、修复CI），配备约500个工具的集中式MCP工具服务器（Toolshed）。

Result: Minions代理每周产生1300多个合并的Pull Request，显著提高了开发效率和自动化水平。

Conclusion: Stripe成功构建了高效的无人值守编码代理系统，通过结合确定性工作流和灵活代理循环，实现了大规模的自动化编码任务处理。

Abstract: Minions: Stripe's one-shot, end-to-end coding agents—Part 2 (8 minute read) Stripe details the infrastructure behind “minions,” its unattended coding agents that now produce 1,300+ merged PRs per week. Minions run in pre-warmed, isolated cloud devboxes and are orchestrated via “blueprints”—a hybrid of deterministic workflow steps (linting, pushing, CI hooks) and flexible agent loops (implement task, fix CI). With scoped rule files, a centralized MCP tool server (“Toolshed”) offering ~500 tool...

</details>


### [23] [How Ramp built a full context background coding agent on Modal](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmodal.com%2Fblog%2Fhow-ramp-built-a-full-context-background-coding-agent-on-modal%3Futm_source=tldrfintech/1/0100019c8ad5eef6-eff830ed-4bd0-4c09-bc76-109e2795f6ff-000000/8ezV_uJS0ELcbWQygTSeLKu4nrXWYai55Hp71pzrvr8=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ramp开发了名为Inspect的内部后台编码代理，基于Modal沙箱构建，能够自动处理约50%的代码合并请求


<details>
  <summary>Details</summary>
Motivation: Ramp需要提高代码开发效率，减少人工代码审查和合并的工作量，通过自动化后台编码代理来加速开发流程

Method: 使用Modal Sandboxes构建全上下文后台编码代理，能够理解完整代码上下文并自动执行代码修改和合并操作

Result: Inspect代理处理了大约一半的代码合并请求，显著提高了开发效率和代码合并速度

Conclusion: 基于Modal的全上下文后台编码代理能够有效自动化代码开发流程，大幅提升团队生产力

Abstract: How Ramp built a full context background coding agent on Modal (4 minute read) Ramp's Inspect is an internal background coding agent powered by Modal Sandboxes that initiates roughly half of all merged pull requests across its repos.

</details>


### [24] [Claude Code Security Research Preview](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-security%3Futm_source=tldrai/1/0100019c8adff895-a0f1446b-46d8-4716-9891-6b814f1ca6bd-000000/2zFu4KSGx3HxtVZ6ZAQ9TEBAmjyHvt_MjggqLyopS40=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic推出Claude Code Security研究预览版，能够自动扫描代码库发现上下文相关漏洞，并为人工审核提供针对性补丁建议


<details>
  <summary>Details</summary>
Motivation: 当前代码安全分析工具往往缺乏上下文理解能力，无法识别依赖特定代码环境的漏洞。Anthropic旨在开发能够理解代码上下文并发现复杂安全问题的AI工具

Method: 通过Claude Code Security工具，对代码库进行自动化扫描，利用AI模型理解代码上下文，识别上下文相关的安全漏洞，并生成针对性的修复建议

Result: 推出了Claude Code Security的有限研究预览版，展示了AI能够有效识别传统工具可能遗漏的上下文相关漏洞，并为安全团队提供可操作的修复建议

Conclusion: Claude Code Security展示了AI在代码安全分析领域的潜力，能够增强开发团队的安全能力，但仍需人工审核以确保修复建议的准确性和适用性

Abstract: Claude Code Security Research Preview (8 minute read) Anthropic introduced Claude Code Security in a limited research preview, enabling automated scans of codebases to surface context-dependent vulnerabilities and propose targeted patches for human review.

</details>


### [25] [Why Developers Keep Choosing Claude Over Every Other AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bhusalmanish.com.np%2Fblog%2Fposts%2Fwhy-claude-wins-coding.html%3Futm_source=tldrai/1/0100019c8adff895-a0f1446b-46d8-4716-9891-6b814f1ca6bd-000000/sEWMujrWUDULVDbk6ecrTtpVH0rfNXrsPNQDix1n1PU=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 开发者持续选择Claude Code是因为其他AI编码助手在文件编辑、多步骤任务处理、上下文理解等关键方面存在系统性缺陷


<details>
  <summary>Details</summary>
Motivation: 当前AI编码助手虽然能生成代码，但在实际开发工作流中面临多个关键挑战：编辑文件时不破坏周围代码、读取正确文件后再修改、完成多步骤任务不丢失上下文、适时提问等，这些是其他编码助手普遍失败的地方

Method: 文章通过分析开发者实际使用体验，对比Claude Code与其他AI编码助手在真实开发场景中的表现差异，识别出其他助手系统性失败的关键环节

Result: 开发者持续选择Claude Code，因为它能更好地处理文件编辑完整性、上下文保持、多步骤任务执行和适时交互等实际开发需求，而其他AI编码助手在这些方面存在一致性的失败模式

Conclusion: 成功的AI编码助手不仅需要生成正确代码，更需要理解完整开发工作流，包括文件操作、上下文管理、任务分解和交互决策等综合能力

Abstract: Why Developers Keep Choosing Claude Over Every Other AI (6 minute read) Developers keep choosing Claude Code because the alternatives keep failing them in the same way. Generating correct code is a major part of the job, but it isn't everything. Agents still need to edit files without corrupting the surrounding code, read the right files before making changes, complete multistep tasks without losing threads halfway through, know when to ask questions, and more, something no other coding agent...

</details>


### [26] [The Coding Agent Is Dead](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fampcode.com%2Fnews%2Fthe-coding-agent-is-dead%3Futm_source=tldrai/1/0100019c8adff895-a0f1446b-46d8-4716-9891-6b814f1ca6bd-000000/hUSpzsxC_RBYb10rshZcUyvy2Bmt1G3viLca-WoGDsM=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 当前代码代理已过时，新模型几乎可用任何工具（如bash）处理代码，代码库组织成为瓶颈。Amp将停止VS Code和Cursor编辑器扩展，专注于灵活轻量的CLI工具。


<details>
  <summary>Details</summary>
Motivation: 认识到当前代码代理的局限性，新的大模型能力强大，不再需要专门的代码代理工具。代码库组织成为新的瓶颈，需要更灵活、轻量、可随处运行的解决方案。

Method: 停止开发编辑器扩展（VS Code和Cursor），专注于开发Amp CLI工具，该工具具有灵活、轻量、易于修改、可随处运行的特点。

Result: Amp将在3月5日停止编辑器扩展服务，但继续维护和开发CLI版本，提供更灵活的开发工具解决方案。

Conclusion: 代码代理时代已结束，未来属于能够灵活处理代码库组织问题的轻量级工具，而不是专门的代码代理扩展。

Abstract: The Coding Agent Is Dead (3 minute read) The current generation of coding agents isn't the future. The newest models can be powerful with nearly any tool thrown at them, with bash often being enough. Codebase organization is now the bottleneck. Amp plans to kill the Amp editor extensions for VS Code and Cursor on March 5. It will continue to work on the Amp CLI, which is flexible, light, easy to change, and can be run anywhere at anytime.

</details>


### [27] [AlphaEvolve Discovers New Multi-Agent Learning Algorithms](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.16928%3Futm_source=tldrai/1/0100019c8adff895-a0f1446b-46d8-4716-9891-6b814f1ca6bd-000000/XlOIf1WZKVKB1mva1_Fj0brvHWA5GMAh2-Zw5oTAzGA=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用基于大语言模型的进化编码代理自动生成不完全信息博弈的新算法，在CFR和PSRO框架下发现了VAD-CFR和SHOR-PSRO变体，实现了比基线更好的收敛性能。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体学习算法设计依赖人工经验，难以发现新颖有效的算法变体。希望通过自动化方法探索算法空间，发现超越人工设计的算法。

Method: 采用基于大语言模型的进化编码代理，通过进化搜索自动生成算法代码。在CFR（反事实遗憾最小化）和PSRO（策略空间响应预言）框架下探索算法变体。

Result: 发现了VAD-CFR（自适应折扣CFR）和SHOR-PSRO（混合乐观元求解器PSRO）两种新算法，在实验中表现出比现有基线更好的收敛性能。

Conclusion: 进化编码代理能够有效发现新颖的多智能体学习算法，展示了自动化算法设计的潜力，为算法创新提供了新途径。

Abstract: AlphaEvolve Discovers New Multi-Agent Learning Algorithms (26 minute read) Using an evolutionary coding agent powered by large language models, the work automatically generated new algorithms for imperfect-information games across CFR and PSRO paradigms. The discovered VAD-CFR and SHOR-PSRO variants introduced adaptive discounting and hybrid optimistic meta-solvers, showing improved empirical convergence over strong baselines.

</details>


### [28] [Leveraging OpenClaw as a Web Developer](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgreenido.wordpress.com%2F2026%2F02%2F21%2Fleveraging-openclaw-as-a-web-developer%2F%3Futm_source=tldrai/1/0100019c8adff895-a0f1446b-46d8-4716-9891-6b814f1ca6bd-000000/lfXtePcf5rw37kBpwup6yGHMEm4gW5-9m4mh8UKnfKQ=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClaw是一个帮助Web开发者自动化重复任务（如脚手架、测试生成、文档更新）的工具，可通过自然语言提示操作，并能学习项目特定模式以提高代码质量和一致性。


<details>
  <summary>Details</summary>
Motivation: Web开发中存在大量重复性任务（如创建路由、生成测试、更新文档），这些任务耗时且容易出错。开发者需要工具来自动化这些流程，同时保持代码质量和项目一致性。

Method: OpenClaw通过自然语言接口接受开发者指令，自动化执行Web开发中的重复任务。开发者可以教授工具项目特定的模式和上下文，使其能够生成符合项目规范的代码，减少手动干预。

Result: OpenClaw能够成功自动化Web开发中的重复任务，包括路由脚手架、测试生成和文档更新。通过学习项目特定模式，工具能够提高代码质量和一致性，减少开发者的手动工作量。

Conclusion: OpenClaw为Web开发者提供了一个有效的自动化工具，能够显著减少重复性工作，同时通过可学习的项目模式保持代码质量和一致性，提高开发效率。

Abstract: Leveraging OpenClaw as a Web Developer (7 minute read) Web developers can integrate OpenClaw to automate repetitive tasks like scaffolding routes, generating tests, and updating documentation directly from natural‑language prompts. Developers with domain context can teach OpenClaw project‑specific patterns that improve code quality and consistency without manual intervention.

</details>


### [29] [Cursor for Product Managers](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fcursor-for-product-managers%3Futm_source=tldrproduct/1/0100019c8f557597-0cd15639-332d-47b1-9865-34222a3dcfc4-000000/5eQRqsYZwlVamkDScFco1zP60PF3XHHOAZQuBHAWt_M=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor工具让产品经理能够直接执行而不仅仅是制定规范，但需要代码素养和耐心处理集成问题；真正的价值在于将AI工作流扩展到整个团队。


<details>
  <summary>Details</summary>
Motivation: 传统产品经理只能制定规范而无法直接执行，Cursor工具旨在让产品经理能够直接参与技术实现，提高产品开发效率。

Method: 通过Cursor工具，产品经理可以借助AI辅助直接进行代码实现，但需要具备一定的代码素养和耐心处理集成问题。

Result: 产品经理能够从单纯的规范制定者转变为能够直接执行的技术参与者，但真正的杠杆效应需要将AI工作流扩展到整个团队。

Conclusion: Cursor为产品经理提供了从规范到执行的能力转变，但最大价值在于团队层面的AI工作流集成，需要代码素养和耐心应对集成挑战。

Abstract: Cursor for Product Managers (8 minute read) Cursor lets PMs execute, not just spec, but it works best with code literacy and patience for integration issues. Real leverage comes when AI workflows extend beyond one PM to the whole team.

</details>


### [30] [Talking Agentic AI with Julien Brun](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Feverything.intellectronica.net%2Fp%2Ftalking-agentic-ai-with-julien-brun%3Futm_source=tldrproduct/1/0100019c8f557597-0cd15639-332d-47b1-9865-34222a3dcfc4-000000/O7u_rlUQ7tur5qSluknHYlTtMc0IcydOUF8m6vZ7Ti0=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 关于AI代理的播客讨论，探讨代理如何将对话转化为行动、降低软件成本，以及转型期面临的就业和对齐挑战


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理如何通过将对话转化为行动来推动软件成本趋近于零，并分析在这一转型过程中需要谨慎处理的就业和对齐问题

Method: 播客访谈形式，通过与Julien Brun的对话讨论AI代理的发展现状、技术特点、应用前景和社会影响

Result: AI代理能够将独特洞察、快速决策和代理"技能"转化为实际价值，长期来看可能实现技术丰裕，但转型过程将对就业和AI对齐带来重大挑战

Conclusion: AI代理技术有潜力创造长期丰裕，但必须谨慎管理转型过程，特别是就业影响和AI对齐问题，以确保平稳过渡

Abstract: Talking Agentic AI with Julien Brun (49 minute podcast) AI agents turn conversation into action and push software costs toward zero, making unique insight, fast decision-making, and agent “skills” the real sources of value. Long-term abundance is possible, but the transition, especially around jobs and alignment, will be disruptive and must be handled carefully.

</details>


### [31] [How OpenAI's Codex Team Works and Leverages AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.eng-leadership.com%2Fp%2Fhow-openais-codex-team-works-and%3Futm_source=tldrproduct/1/0100019c8f557597-0cd15639-332d-47b1-9865-34222a3dcfc4-000000/Y8UyUayZplNJRHyDyXJYEXgzki0xSQbIyw3hAYY-l7Y=445)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI Codex团队是一个小型自治团队，将AI深度融入开发全流程，通过赋予完全所有权和将AI视为核心队友，实现创业公司般的快速迭代同时保持高质量和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 探索如何有效利用AI技术提升软件开发效率和质量，特别是在快速发展的技术环境中保持竞争优势。

Method: 采用小型自治团队结构，将AI深度集成到开发流程的每个阶段（规划、编码、代码审查、入职培训），赋予团队完全所有权，并将AI视为核心团队成员。

Result: 实现了创业公司般的开发速度，同时保持了高质量和可扩展性，成功将AI技术有效应用于实际软件开发工作流。

Conclusion: 通过将AI深度融入开发流程并赋予团队自主权，可以在保持高质量的同时实现快速迭代，这种模式为AI驱动的软件开发提供了有效范例。

Abstract: How OpenAI's Codex Team Works and Leverages AI (7 minute read) The Codex team is a small, autonomous group that embeds AI into every stage of development, from planning to code review and onboarding. By empowering full ownership and treating AI as a core teammate, they move at startup speed while maintaining high quality and scale.

</details>


### [32] [Unblocked: the context layer your AI tools are missing](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=contextengine%26utm_content=260224_secondary/1/0100019c8f64b2ad-820294bc-bbe4-411b-b0b7-e6a601f6d014-000000/e6pxAVcDMXcJwcsSNWmfHN2TG-0h7b1ilYnAQsbIj94=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Unblocked是一个为AI工具提供上下文层的平台，通过整合团队代码、PR历史、对话、文档、规划工具和运行时信号来提升AI生成代码、评审和答案的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前AI工具缺乏对团队特定上下文的深入理解，导致生成的输出可能与实际系统工作方式不符，开发者面临"AI疲劳"问题。

Method: 构建一个上下文层，从多个来源（代码库、PR历史、团队对话、文档、规划工具、运行时信号）收集和整合信息，为AI工具提供相关的上下文洞察。

Result: 根据Clio高级开发者的反馈，Unblocked显著提升了AI输出的精确度，完全逆转了"AI疲劳"问题。

Conclusion: 为AI工具提供团队特定的上下文层可以显著提高其输出的可靠性和实用性，解决当前AI工具在实际开发环境中的局限性。

Abstract: Unblocked: the context layer your AI tools are missing (Sponsor) Give your agents the understanding they need to generate reliable code, reviews, and answers. Unblocked builds context from your team's code, PR history, conversations, documentation, planning tools, and runtime signals. It surfaces the insights that matter so AI outputs reflect how your system actually works. “Unblocked has reversed my AI fatigue completely. The level of precision is wild.” - Senior developer, Clio See how it w...

</details>


### [33] [Writing code is cheap now](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2Fguides%2Fagentic-engineering-patterns%2Fcode-is-cheap%2F%3Futm_source=tldrnewsletter/1/0100019c8f64b2ad-820294bc-bbe4-411b-b0b7-e6a601f6d014-000000/HPoECP8NqOeRBN_p8SioHpC7J5Rl0E_4zuX5twDILBk=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 代码生成成本大幅降低，但确保代码质量仍有成本，需要开发新的个人和组织习惯来应对智能体工程的新现实


<details>
  <summary>Details</summary>
Motivation: 随着代码智能体大幅降低代码生成成本，并行智能体的能力进一步放大了这一效应，但交付高质量代码仍然需要成本，开发者仍需确保生成的代码是优质代码

Method: 本文主要提出观点性分析，而非具体技术方法。它强调需要开发新的个人和组织习惯来适应智能体工程的新现实

Result: 指出当前代码生成成本已大幅降低，但代码质量保证仍然是挑战，需要系统性改变工程实践

Conclusion: 虽然代码生成变得廉价，但确保代码质量仍有成本，工程社区需要发展新的习惯和实践来有效利用智能体工程

Abstract: Writing code is cheap now (4 minute read) Coding agents have dramatically dropped the cost of producing code. The ability to run parallel agents has compounded this effect. However, delivering good code still has a cost. Developers still need to ensure that the produced code is good code. The challenge is to develop new personal and organizational habits to respond to the new reality of agentic engineering.

</details>


### [34] [Writing code is cheap now](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2Fguides%2Fagentic-engineering-patterns%2Fcode-is-cheap%2F%3Futm_source=tldrdev/1/0100019c8f8e0830-d220df0c-f27a-475e-b8cf-8a489b2e034b-000000/BRYupddSL2C4FgcxlnF_4TszyfX1qf_ri-LT8dxK4og=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 代码生成成本降低，但高质量代码交付仍然困难


<details>
  <summary>Details</summary>
Motivation: 探讨智能体工程如何改变软件开发，使代码编写成本降低，但高质量代码交付仍然面临挑战

Method: 概念性分析，讨论智能体工程对软件开发各个层面的影响

Result: 代码生成成本大幅降低，但高质量代码（质量、可靠性、简洁性、可维护性）的交付仍然困难

Conclusion: 智能体工程改变了软件开发范式，但高质量代码交付仍然是核心挑战

Abstract: Writing code is cheap now (4 minute read) Agentic engineering changes software development by making code writing cheaper. This shift impacts everything from macro-level project planning to micro-level decisions about refactoring, testing, and documentation. While delivering basic code is now nearly free, delivering good code, defined by its quality, reliability, simplicity, and maintainability, is still hard.

</details>


### [35] [Create an Agent MVP in 30 Days](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffandf.co%2F40mN0sI%3Futm_source=tldrdev/1/0100019c8f8e0830-d220df0c-f27a-475e-b8cf-8a489b2e034b-000000/-hfjHlni-LdX1X7m8xzQpCcE2dUMCiR60v6A853uU4o=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 如何在30天内使用Microsoft Foundry快速构建并发布一个代理MVP（最小可行产品），包含架构、性能和权衡的实用指南


<details>
  <summary>Details</summary>
Motivation: 帮助开发者快速从想法到生产就绪的代理，提供实用的开发清单来加速代理产品的开发过程

Method: 使用Microsoft Foundry平台，遵循开发清单，在约30天内完成代理MVP的构建和发布，重点关注架构设计、性能优化和技术决策的权衡

Result: 提供了一套完整的开发清单，指导开发者快速构建生产就绪的代理MVP，帮助做出早期技术决策

Conclusion: 通过Microsoft Foundry和系统化的开发清单，开发者可以在30天内高效地从概念到生产就绪的代理产品

Abstract: Create an Agent MVP in 30 Days (Sponsor) Go from idea to production‐ready agent quickly. This developer checklist shows how to use Microsoft Foundry to build and ship an agent MVP in ~30 days, with practical guidance on architecture, performance, and tradeoffs to inform early technical decisions. Get the checklist

</details>


### [36] [Signals: Toward a Self-Improving Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffactory.ai%2Fnews%2Ffactory-signals%3Futm_source=tldrdev/1/0100019c8f8e0830-d220df0c-f27a-475e-b8cf-8a489b2e034b-000000/I2mraSmRJQr35e1PLEL6QF2EQ18R3eR18psNRZwNdOY=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Factory构建了一个使用LLM自动审查数千个编码代理会话的系统，通过检测用户挫败感（如重复请求、上下文窗口问题）来自动发现摩擦点，然后代理自行创建bug工单、编写修复代码并提交PR供人工审核。


<details>
  <summary>Details</summary>
Motivation: 当前编码代理在处理用户请求时存在各种摩擦点（如用户需要多次重复请求、与上下文窗口斗争），这些挫败感影响用户体验和代理效率。需要自动化系统来持续识别和修复这些问题。

Method: 构建基于LLM的系统，每天自动审查数千个编码代理会话，通过分析用户行为模式（如重复请求次数、上下文窗口问题）来识别挫败感。当检测到足够摩擦时，系统自动创建bug工单、编写修复代码并提交PR供人工批准。

Result: 系统能够自动检测编码代理会话中的用户挫败感，并实现自我改进的闭环：发现问题→创建工单→编写修复→提交审核，减少人工干预，提高代理的持续改进能力。

Conclusion: 通过LLM驱动的自动化审查和修复系统，编码代理能够实现自我改进，持续优化用户体验，减少摩擦点，提高整体效率和用户满意度。

Abstract: Signals: Toward a Self-Improving Agent (6 minute read) Factory built a system that uses LLMs to review thousands of coding agent sessions daily, spotting where users got frustrated. For example, frustrations would manifest themselves as rephrasing the same request five times or fighting with context windows. When enough friction is found, the agent files its own bug ticket, writes a fix, and opens a PR for a human to approve.

</details>


### [37] [Cursor's Debug Mode Is Arguably Its Best Feature](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdavidgomes.com%2Fcursor-debug-mode%2F%3Futm_source=tldrdev/1/0100019c8f8e0830-d220df0c-f27a-475e-b8cf-8a489b2e034b-000000/yr3-v0wAYoK2p1QKAA6nlsHa59eTUNWtVhyIKKwYDDA=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor的Debug模式通过HTTP检测观察运行时代码执行来修复bug


<details>
  <summary>Details</summary>
Motivation: Cursor的Debug模式是其最佳功能之一，旨在通过运行时观察来有效修复代码bug

Method: 使用基于HTTP的检测技术来观察运行时代码执行

Result: 该功能能够有效定位和修复代码中的bug

Conclusion: Cursor的Debug模式是一个强大的bug修复工具，值得推荐使用

Abstract: Cursor's Debug Mode Is Arguably Its Best Feature (3 minute read) Cursor's Debug Mode provides bug fixes by using HTTP-based instrumentation to observe runtime code execution.

</details>


### [38] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c8f8e0830-d220df0c-f27a-475e-b8cf-8a489b2e034b-000000/WD_I3WK_qfstMKE1xq97ytcGCtt306UYcazhAxTiwwA=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor的Debug Mode通过HTTP-based instrumentation观察运行时代码执行来修复bug


<details>
  <summary>Details</summary>
Motivation: Cursor的Debug Mode是其最佳功能之一，旨在通过观察运行时代码执行来更有效地修复bug

Method: 使用HTTP-based instrumentation技术来观察和分析运行时代码执行

Result: Debug Mode能够提供bug修复功能，通过观察实际代码执行来识别和解决问题

Conclusion: Cursor的Debug Mode是一个强大的功能，通过运行时观察显著提升了bug修复能力

Abstract: Cursor's Debug Mode Is Arguably Its Best Feature (3 minute read) Cursor's Debug Mode provides bug fixes by using HTTP-based instrumentation to observe runtime code execution.

</details>


### [39] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c8f8e0830-d220df0c-f27a-475e-b8cf-8a489b2e034b-000000/upK-zkCm_rNauDJ9O43lEhAmd8kEhtubsvBNn73PWGQ=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor的Debug模式通过HTTP基础工具观察运行时代码执行来修复bug


<details>
  <summary>Details</summary>
Motivation: Cursor的Debug模式是其最佳功能之一，旨在通过运行时观察来有效修复代码bug

Method: 使用基于HTTP的工具来观察运行时代码执行

Result: Debug模式能够提供bug修复功能

Conclusion: Cursor的Debug模式是其最强大的功能之一

Abstract: Cursor's Debug Mode Is Arguably Its Best Feature (3 minute read) Cursor's Debug Mode provides bug fixes by using HTTP-based instrumentation to observe runtime code execution.

</details>


### [40] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c8f8e0830-d220df0c-f27a-475e-b8cf-8a489b2e034b-000000/eulANr4A6-tFNsvXfraTnrL7T4bEbzW-jP9cNQkJodc=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor的Debug Mode通过HTTP-based instrumentation观察运行时代码执行来提供bug修复功能


<details>
  <summary>Details</summary>
Motivation: 解决代码调试和bug修复的自动化需求，提高开发效率

Method: 使用HTTP-based instrumentation技术观察运行时代码执行，通过分析执行过程自动识别和修复bug

Result: Cursor的Debug Mode被认为是其最佳功能之一，能够有效帮助开发者调试和修复代码问题

Conclusion: 基于HTTP instrumentation的运行时代码观察是有效的bug修复方法，Cursor的Debug Mode展示了这一技术的实用价值

Abstract: Cursor's Debug Mode Is Arguably Its Best Feature (3 minute read) Cursor's Debug Mode provides bug fixes by using HTTP-based instrumentation to observe runtime code execution.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [41] [Mitigating "Epistemic Debt" in Generative AI-Scaffolded Novice Programming using Metacognitive Scripts](https://arxiv.org/abs/2602.20206)
*Sreecharan Sankaranarayanan*

Main category: cs.SE

TL;DR: 研究探讨"氛围编程"（Vibe Coding）中AI辅助对新手程序员认知技能发展的负面影响，提出通过"解释门"机制强制"教学反馈"来减少认知外包，防止产生"脆弱专家"。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，新手程序员倾向于"氛围编程"——优先语义意图而非语法实现。研究者担心缺乏教学约束会导致认知技能获取的根本错位，使新手外包内在认知负荷而非仅卸载外在负荷，积累"认知债务"并产生"脆弱专家"。

Method: 采用被试间实验设计（N=78），使用基于Claude 3.5 Sonnet的Cursor IDE插件。参与者分为三组：手动（对照组）、无限制AI（外包组）和支架式AI（卸载组）。支架式组采用新颖的"解释门"机制，利用实时LLM-as-a-Judge框架强制执行"教学反馈"协议后才允许集成生成的代码。

Result: 结果显示"能力崩溃"现象：无限制AI用户的生产力与支架式组相当（p<.001 vs.手动组），但在后续AI中断维护任务中失败率高达77%，而支架式组仅为39%。定性分析表明成功的氛围编码者会自然进行自我支架，将AI视为顾问而非承包商。

Conclusion: 研究者讨论了AI生成软件可维护性的影响，提出未来学习系统必须强制执行"元认知摩擦"以防止大规模产生不可维护的代码。成功的氛围编码者将AI作为咨询工具而非完全外包。

Abstract: The democratization of Large Language Models (LLMs) has given rise to ``Vibe Coding," a workflow where novice programmers prioritize semantic intent over syntactic implementation. While this lowers barriers to entry, we hypothesize that without pedagogical guardrails, it is fundamentally misaligned with cognitive skill acquisition. Drawing on the distinction between Cognitive Offloading and Cognitive Outsourcing, we argue that unrestricted AI encourages novices to outsource the Intrinsic Cognitive Load required for schema formation, rather than merely offloading Extraneous Load. This accumulation of ``Epistemic Debt" creates ``Fragile Experts" whose high functional utility masks critically low corrective competence.
  To quantify and mitigate this debt, we conducted a between-subjects experiment (N=78) using a custom Cursor IDE plugin backed by Claude 3.5 Sonnet. Participants represented "AI-Native" learners across three conditions: Manual (Control), Unrestricted AI (Outsourcing), and Scaffolded AI (Offloading). The Scaffolded condition utilized a novel ``Explanation Gate," leveraging a real-time LLM-as-a-Judge framework to enforce a ``Teach-Back" protocol before generated code could be integrated.
  Results reveal a ``Collapse of Competence": while Unrestricted AI users matched the productivity of the Scaffolded group (p < .001 vs. Manual), they suffered a 77% failure rate in a subsequent AI-Blackout maintenance task, compared to only 39% in the Scaffolded group. Qualitative analysis suggests that successful vibe coders naturally engage in self-scaffolding, treating the AI as a consultant rather than a contractor. We discuss the implications for the maintainability of AI-generated software and propose that future learning systems must enforce Metacognitive Friction to prevent the mass production of unmaintainable code.

</details>


### [42] [CodeHacker: Automated Test Case Generation for Detecting Vulnerabilities in Competitive Programming Solutions](https://arxiv.org/abs/2602.20213)
*Jingwei Shi,Xinxiang Yin,Jing Huang,Jinman Zhao,Shengyu Tao*

Main category: cs.SE

TL;DR: CodeHacker是一个自动化的代理框架，专门生成有针对性的对抗性测试用例，以暴露程序提交中的潜在漏洞，提高代码生成评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试缺乏对细微边界情况的覆盖，导致错误的解决方案也能通过测试。需要更强大的测试用例来暴露LLM生成的代码中的潜在漏洞。

Method: 采用多策略方法，包括压力测试、抗哈希攻击和逻辑特定目标攻击，模拟竞争编程中的黑客机制。引入校准阶段，通过自生成的对抗性探针迭代优化验证器和检查器。

Result: CodeHacker显著提高了现有数据集的真阴性率（TNR），有效过滤了先前被接受的错误解决方案。生成的对抗性案例作为训练数据，提升了RL训练模型在LiveCodeBench等基准上的性能。

Conclusion: CodeHacker框架能够生成高质量的对抗性测试用例，提高代码生成评估的鲁棒性，同时生成的测试数据还能用于提升模型性能。

Abstract: The evaluation of Large Language Models (LLMs) for code generation relies heavily on the quality and robustness of test cases. However, existing benchmarks often lack coverage for subtle corner cases, allowing incorrect solutions to pass. To bridge this gap, we propose CodeHacker, an automated agent framework dedicated to generating targeted adversarial test cases that expose latent vulnerabilities in program submissions. Mimicking the hack mechanism in competitive programming, CodeHacker employs a multi-strategy approach, including stress testing, anti-hash attacks, and logic-specific targeting to break specific code submissions. To ensure the validity and reliability of these attacks, we introduce a Calibration Phase, where the agent iteratively refines its own Validator and Checker via self-generated adversarial probes before evaluating contestant code.Experiments demonstrate that CodeHacker significantly improves the True Negative Rate (TNR) of existing datasets, effectively filtering out incorrect solutions that were previously accepted. Furthermore, generated adversarial cases prove to be superior training data, boosting the performance of RL-trained models on benchmarks like LiveCodeBench.

</details>


### [43] [PhantomRun: Auto Repair of Compilation Errors in Embedded Open Source Software](https://arxiv.org/abs/2602.20284)
*Han Fu,Andreas Ermedahl,Sigrid Eldh,Kristian Wiklund,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: PhantomRun是一个利用大语言模型自动修复嵌入式软件CI编译失败的框架，在目标项目中成功修复了高达45%的编译失败。


<details>
  <summary>Details</summary>
Motivation: 嵌入式软件的持续集成（CI）流水线经常在编译阶段失败，消耗大量开发人员调试时间。研究发现硬件依赖是编译失败的主要原因，大多数修复只需要相对较小的改动，使得自动修复具有潜在可行性。

Method: PhantomRun是一个自动化框架，利用大语言模型生成和验证CI编译失败的修复方案。框架通过为GitHub Actions和GitLab CI以及四种不同构建系统提供适配层，解决了嵌入式系统项目中多样化的构建基础设施和工具链挑战。利用构建日志、源代码、历史修复记录和编译器错误消息来合成修复方案。

Result: 评估显示，PhantomRun在目标项目中成功修复了高达45%的CI编译失败，证明了基于LLM的修复在嵌入式系统CI流水线中的可行性。

Conclusion: LLM-based automated repair is viable for embedded-system CI pipelines, with PhantomRun demonstrating significant success rates in fixing compilation failures across diverse embedded projects.

Abstract: Continuous Integration (CI) pipelines for embedded software sometimes fail during compilation, consuming significant developer time for debugging. We study four major open-source embedded system projects, spanning over 4000 build failures from the project's CI runs. We find that hardware dependencies account for the majority of compilation failures, followed by syntax errors and build-script issues. Most repairs need relatively small changes, making automated repair potentially suitable as long as the diverse setups and lack of test data can be handled.
  In this paper, we present PhantomRun, an automated framework that leverages large language models (LLMs) to generate and validate fixes for CI compilation failures. The framework addresses the challenge of diverse build infrastructures and tool chains across embedded system projects by providing an adaptation layer for GitHub Actions and GitLab CI and four different build systems. PhantomRun utilizes build logs, source code, historical fixes, and compiler error messages to synthesize fixes using LLMs. Our evaluations show that PhantomRun successfully repairs up to 45% of CI compilation failures across the targeted projects, demonstrating the viability of LLM-based repairs for embedded-system CI pipelines.

</details>


### [44] [Quantifying the Expectation-Realisation Gap for Agentic AI Systems](https://arxiv.org/abs/2602.20292)
*Sebastian Lobentanzer*

Main category: cs.SE

TL;DR: AI代理系统预期与实际效果存在系统性差距：软件工程中预期加速24%但实际减速19%，临床文档中声称节省数分钟但实测不足1分钟，临床决策支持外部验证远低于开发者报告指标


<details>
  <summary>Details</summary>
Motivation: AI代理系统部署时预期有显著生产力提升，但缺乏实证证据验证预期与实际效果的差距，需要系统评估不同领域中的期望-实现差距

Method: 回顾软件工程、临床文档和临床决策支持领域的对照试验和独立验证研究，量化预期与实际效果的差距，分析差距产生的原因

Result: 软件工程：经验开发者预期24%加速但实际减速19%（43个百分点校准误差）；临床文档：供应商声称节省数分钟但实测不足1分钟，一款广泛部署工具无统计显著效果；临床决策支持：外部验证性能显著低于开发者报告指标

Conclusion: 预期-实现差距由工作流整合摩擦、验证负担、测量结构不匹配和系统异质性等因素驱动，需要结构化规划框架，要求明确的量化效益预期并考虑人工监督成本

Abstract: Agentic AI systems are deployed with expectations of substantial productivity gains, yet rigorous empirical evidence reveals systematic discrepancies between pre-deployment expectations and post-deployment outcomes. We review controlled trials and independent validations across software engineering, clinical documentation, and clinical decision support to quantify this expectation-realisation gap. In software development, experienced developers expected a 24% speedup from AI tools but were slowed by 19% -- a 43 percentage-point calibration error. In clinical documentation, vendor claims of multi-minute time savings contrast with measured reductions of less than one minute per note, and one widely deployed tool showed no statistically significant effect. In clinical decision support, externally validated performance falls substantially below developer-reported metrics. These shortfalls are driven by workflow integration friction, verification burden, measurement construct mismatches, and systematic heterogeneity in treatment effects. The evidence motivates structured planning frameworks that require explicit, quantified benefit expectations with human oversight costs factored in.

</details>


### [45] [Codified Context: Infrastructure for AI Agents in a Complex Codebase](https://arxiv.org/abs/2602.20478)
*Aristidis Vasilopoulos*

Main category: cs.SE

TL;DR: 论文提出了一种三组件编码上下文基础设施，用于解决基于LLM的智能编码助手缺乏持久记忆的问题，通过在大型C#分布式系统开发中实现热内存宪法、专业代理和冷内存知识库。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的智能编码助手缺乏持久记忆，导致跨会话失去连贯性、忘记项目约定和重复已知错误。现有研究关注开发者如何通过清单文件配置代理，但如何为大型多代理项目扩展此类配置仍是一个开放挑战。

Method: 提出了三组件编码上下文基础设施：1) 热内存宪法，编码约定、检索钩子和编排协议；2) 19个专业领域专家代理；3) 冷内存知识库，包含34个按需规范文档。该框架在108,000行C#分布式系统构建过程中开发。

Result: 报告了283个开发会话中基础设施增长和交互模式的定量指标，以及四个观察性案例研究，展示了编码上下文如何跨会话传播以防止故障和保持一致性。框架已作为开源配套仓库发布。

Conclusion: 编码上下文基础设施能够有效解决智能编码助手的持久记忆问题，通过热内存宪法、专业代理和冷内存知识库的组合，在大型多代理项目中实现配置的扩展和一致性维护。

Abstract: LLM-based agentic coding assistants lack persistent memory: they lose coherence across sessions, forget project conventions, and repeat known mistakes. Recent studies characterize how developers configure agents through manifest files, but an open challenge remains how to scale such configurations for large, multi-agent projects. This paper presents a three-component codified context infrastructure developed during construction of a 108,000-line C# distributed system: (1) a hot-memory constitution encoding conventions, retrieval hooks, and orchestration protocols; (2) 19 specialized domain-expert agents; and (3) a cold-memory knowledge base of 34 on-demand specification documents. Quantitative metrics on infrastructure growth and interaction patterns across 283 development sessions are reported alongside four observational case studies illustrating how codified context propagates across sessions to prevent failures and maintain consistency. The framework is published as an open-source companion repository.

</details>
