{"id": "2602.00066", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00066", "abs": "https://arxiv.org/abs/2602.00066", "authors": ["Zheng Fang", "Yihong Dong", "Lili Mou", "Dongming Jin", "Zhi Jin", "Ge Li"], "title": "IntentCoding: Amplifying User Intent in Code Generation", "comment": null, "summary": "Large Language Models (LLMs) have shown strong capabilities in code generation, but their adherence to fine-grained user intent with multiple constraints remains a significant challenge. Our empirical analysis reveals two key observations: 1) Model performance deteriorates quickly as the number of constraints in the user intent increases, and 2) While user intent does influence the model's logits, such an influence may not be strong enough to effectively steer the decoding process. To this end, we propose Intent-Amplified Code Generation (IntentCoding), a novel decoding strategy that enhances an LLM's ability to follow user intent. IntentCoding captures the influence of user intent by masking out the intent, and applies a multi-strength ensemble mechanism to amplify the effect of user intent during generation. IntentCoding is model-agnostic, requires no additional training, and integrates seamlessly with existing decoding procedures. To enable systematic evaluation, we also construct CodeConstraints, a benchmark dataset specifically designed to test user intent compliance under varying numbers of constraints. Experiments on our constructed Constraints, as well as popular IFEvalCode, HumanEval and LiveCodeBench datasets, show that our IntentCoding model significantly improves both constraint satisfaction and functional correctness compared to standard decoding approaches. IntentCoding achieves up to 71.0% relative improvement on CodeConstraints, achieves up to 67.3% relative improvement on IFEvalCode and achieves up to 29.3% relative improvement in pass@1 on HumanEval and LiveCodeBench compared with greedy decoding.", "AI": {"tldr": "IntentCoding\uff1a\u4e00\u79cd\u901a\u8fc7\u63a9\u7801\u610f\u56fe\u548c\u591a\u5f3a\u5ea6\u96c6\u6210\u673a\u5236\u589e\u5f3aLLM\u9075\u5faa\u7528\u6237\u610f\u56fe\u80fd\u529b\u7684\u89e3\u7801\u7b56\u7565\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u7ea6\u675f\u6ee1\u8db3\u548c\u529f\u80fd\u6b63\u786e\u6027", "motivation": "LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u96be\u4ee5\u9075\u5faa\u5305\u542b\u591a\u4e2a\u7ea6\u675f\u7684\u7ec6\u7c92\u5ea6\u7528\u6237\u610f\u56fe\uff0c\u6027\u80fd\u968f\u7ea6\u675f\u6570\u91cf\u589e\u52a0\u800c\u5feb\u901f\u4e0b\u964d\uff0c\u4e14\u7528\u6237\u610f\u56fe\u5bf9\u6a21\u578blogits\u7684\u5f71\u54cd\u4e0d\u8db3\u4ee5\u6709\u6548\u5f15\u5bfc\u89e3\u7801\u8fc7\u7a0b", "method": "\u63d0\u51faIntentCoding\u89e3\u7801\u7b56\u7565\uff1a1) \u901a\u8fc7\u63a9\u7801\u7528\u6237\u610f\u56fe\u6765\u6355\u6349\u5176\u5f71\u54cd\uff1b2) \u5e94\u7528\u591a\u5f3a\u5ea6\u96c6\u6210\u673a\u5236\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u653e\u5927\u7528\u6237\u610f\u56fe\u7684\u6548\u679c\uff1b\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u4e0e\u73b0\u6709\u89e3\u7801\u8fc7\u7a0b\u65e0\u7f1d\u96c6\u6210", "result": "\u5728CodeConstraints\u3001IFEvalCode\u3001HumanEval\u548cLiveCodeBench\u6570\u636e\u96c6\u4e0a\uff0cIntentCoding\u76f8\u6bd4\u6807\u51c6\u89e3\u7801\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u7ea6\u675f\u6ee1\u8db3\u548c\u529f\u80fd\u6b63\u786e\u6027\uff0c\u5728CodeConstraints\u4e0a\u76f8\u5bf9\u6539\u8fdb\u8fbe71.0%\uff0cIFEvalCode\u4e0a\u8fbe67.3%\uff0cHumanEval\u548cLiveCodeBench\u7684pass@1\u76f8\u5bf9\u6539\u8fdb\u8fbe29.3%", "conclusion": "IntentCoding\u662f\u4e00\u79cd\u6709\u6548\u589e\u5f3aLLM\u9075\u5faa\u7528\u6237\u610f\u56fe\u80fd\u529b\u7684\u89e3\u7801\u7b56\u7565\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u89e3\u51b3\u591a\u7ea6\u675f\u610f\u56fe\u9075\u5faa\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def", "topic": "code agent"}}
{"id": "2602.00164", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00164", "abs": "https://arxiv.org/abs/2602.00164", "authors": ["Khairul Alam", "Saikat Mondal", "Banani Roy"], "title": "Why Are AI Agent Involved Pull Requests (Fix-Related) Remain Unmerged? An Empirical Study", "comment": "5 pages", "summary": "Autonomous coding agents (e.g., OpenAI Codex, Devin, GitHub Copilot) are increasingly used to generate fix-related pull requests (PRs) in real world software repositories. However, their practical effectiveness depends on whether these contributions are accepted and merged by project maintainers. In this paper, we present an empirical study of AI agent involved fix related PRs, examining both their integration outcomes, latency, and the factors that hinder successful merging. We first analyze 8,106 fix related PRs authored by five widely used AI coding agents from the AIDEV POP dataset to quantify the proportions of PRs that are merged, closed without merging, or remain open. We then conduct a manual qualitative analysis of a statistically significant sample of 326 closed but unmerged PRs, spending approximately 100 person hours to construct a structured catalog of 12 failure reasons. Our results indicate that test case failures and prior resolution of the same issues by other PRs are the most common causes of non integration, whereas build or deployment failures are comparatively rare. Overall, our findings expose key limitations of current AI coding agents in real world settings and highlight directions for their further improvement and for more effective human AI collaboration in software maintenance.", "AI": {"tldr": "\u5bf9AI\u7f16\u7801\u4ee3\u7406\u751f\u6210\u7684\u4fee\u590d\u76f8\u5173PR\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u5176\u96c6\u6210\u7ed3\u679c\u3001\u5ef6\u8fdf\u548c\u963b\u788d\u6210\u529f\u5408\u5e76\u7684\u56e0\u7d20", "motivation": "\u968f\u7740\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u5728\u771f\u5b9e\u8f6f\u4ef6\u4ed3\u5e93\u4e2d\u751f\u6210\u4fee\u590d\u76f8\u5173PR\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u9700\u8981\u4e86\u89e3\u8fd9\u4e9b\u8d21\u732e\u662f\u5426\u88ab\u9879\u76ee\u7ef4\u62a4\u8005\u63a5\u53d7\u548c\u5408\u5e76\uff0c\u8bc4\u4f30\u5176\u5b9e\u9645\u6548\u679c", "method": "1) \u5206\u6790AIDEV POP\u6570\u636e\u96c6\u4e2d5\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684AI\u7f16\u7801\u4ee3\u7406\u751f\u6210\u76848,106\u4e2a\u4fee\u590d\u76f8\u5173PR\uff0c\u91cf\u5316\u5408\u5e76\u3001\u5173\u95ed\u672a\u5408\u5e76\u548c\u4fdd\u6301\u5f00\u653e\u7684\u6bd4\u4f8b\uff1b2) \u5bf9326\u4e2a\u5df2\u5173\u95ed\u4f46\u672a\u5408\u5e76\u7684PR\u8fdb\u884c\u624b\u52a8\u5b9a\u6027\u5206\u6790\uff0c\u6784\u5efa\u5305\u542b12\u4e2a\u5931\u8d25\u539f\u56e0\u7684\u7ed3\u6784\u5316\u76ee\u5f55", "result": "\u6d4b\u8bd5\u7528\u4f8b\u5931\u8d25\u548c\u76f8\u540c\u95ee\u9898\u5df2\u88ab\u5176\u4ed6PR\u89e3\u51b3\u662f\u6700\u5e38\u89c1\u7684\u672a\u96c6\u6210\u539f\u56e0\uff0c\u800c\u6784\u5efa\u6216\u90e8\u7f72\u5931\u8d25\u76f8\u5bf9\u8f83\u5c11", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dAI\u7f16\u7801\u4ee3\u7406\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u4e3a\u6539\u8fdbAI\u7f16\u7801\u4ee3\u7406\u548c\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u5728\u8f6f\u4ef6\u7ef4\u62a4\u4e2d\u7684\u66f4\u6709\u6548\u5408\u4f5c\u6307\u660e\u4e86\u65b9\u5411", "topic": "swe application"}}
{"id": "2602.00180", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00180", "abs": "https://arxiv.org/abs/2602.00180", "authors": ["Deepak Babu Piskala"], "title": "Spec-Driven Development:From Code to Contract in the Age of AI Coding Assistants", "comment": "Submitted to AIWare 2026. 8 pages, 3 figures", "summary": "The rise of AI coding assistants has reignited interest in an old idea: what if specifications-not code-were the primary artifact of software development? Spec-driven development (SDD) inverts the traditional workflow by treating specifications as the source of truth and code as a generated or verified secondary artifact. This paper provides practitioners with a comprehensive guide to SDD, covering its principles, workflow patterns, and supporting tools. We present three levels of specification rigor-spec-first, spec-anchored, and spec-as-source-with clear guidance on when each applies. Through analysis of tools ranging from Behavior-Driven Development frameworks to modern AI-assisted toolkits like GitHub Spec Kit, we demonstrate how the spec-first philosophy maps to real implementations. We present case studies from API development, enterprise systems, and embedded software, illustrating how different domains apply SDD. We conclude with a decision framework helping practitioners determine when SDD provides value and when simpler approaches suffice.", "AI": {"tldr": "\u672c\u6587\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u89c4\u8303\u9a71\u52a8\u5f00\u53d1\uff08SDD\uff09\u7684\u5168\u9762\u6307\u5357\uff0c\u5c06\u89c4\u8303\u89c6\u4e3a\u4e3b\u8981\u5de5\u4ef6\uff0c\u4ee3\u7801\u4f5c\u4e3a\u751f\u6210\u6216\u9a8c\u8bc1\u7684\u6b21\u8981\u5de5\u4ef6\uff0c\u4ecb\u7ecd\u4e86\u4e09\u79cd\u89c4\u8303\u4e25\u8c28\u5ea6\u7ea7\u522b\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u3002", "motivation": "AI\u7f16\u7801\u52a9\u624b\u7684\u5174\u8d77\u91cd\u65b0\u6fc0\u53d1\u4e86\u4eba\u4eec\u5bf9\u4e00\u4e2a\u65e7\u7406\u5ff5\u7684\u5174\u8da3\uff1a\u5982\u679c\u89c4\u8303\uff08\u800c\u975e\u4ee3\u7801\uff09\u6210\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u7684\u4e3b\u8981\u5de5\u4ef6\u4f1a\u600e\u6837\uff1f\u89c4\u8303\u9a71\u52a8\u5f00\u53d1\u901a\u8fc7\u5c06\u89c4\u8303\u89c6\u4e3a\u771f\u7406\u6765\u6e90\uff0c\u4ee3\u7801\u4f5c\u4e3a\u751f\u6210\u6216\u9a8c\u8bc1\u7684\u6b21\u8981\u5de5\u4ef6\uff0c\u6765\u98a0\u8986\u4f20\u7edf\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u89c4\u8303\u9a71\u52a8\u5f00\u53d1\u7684\u4e09\u4e2a\u4e25\u8c28\u5ea6\u7ea7\u522b\uff1a\u89c4\u8303\u4f18\u5148\u3001\u89c4\u8303\u951a\u5b9a\u548c\u89c4\u8303\u5373\u6e90\u7801\uff1b\u5206\u6790\u4e86\u4ece\u884c\u4e3a\u9a71\u52a8\u5f00\u53d1\u6846\u67b6\u5230\u73b0\u4ee3AI\u8f85\u52a9\u5de5\u5177\uff08\u5982GitHub Spec Kit\uff09\u7684\u5de5\u5177\u96c6\uff1b\u901a\u8fc7API\u5f00\u53d1\u3001\u4f01\u4e1a\u7cfb\u7edf\u548c\u5d4c\u5165\u5f0f\u8f6f\u4ef6\u7684\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u3002", "result": "\u5c55\u793a\u4e86\u89c4\u8303\u9a71\u52a8\u5f00\u53d1\u54f2\u5b66\u5982\u4f55\u6620\u5c04\u5230\u5b9e\u9645\u5b9e\u73b0\uff0c\u63d0\u4f9b\u4e86\u4e0d\u540c\u9886\u57df\u5e94\u7528SDD\u7684\u5177\u4f53\u6848\u4f8b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u51b3\u7b56\u6846\u67b6\u6765\u5e2e\u52a9\u5b9e\u8df5\u8005\u786e\u5b9aSDD\u4f55\u65f6\u63d0\u4f9b\u4ef7\u503c\u4ee5\u53ca\u4f55\u65f6\u66f4\u7b80\u5355\u7684\u65b9\u6cd5\u5c31\u8db3\u591f\u4e86\u3002", "conclusion": "\u89c4\u8303\u9a71\u52a8\u5f00\u53d1\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u4f46\u9700\u8981\u6839\u636e\u9879\u76ee\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u4e25\u8c28\u5ea6\u7ea7\u522b\uff1b\u901a\u8fc7\u51b3\u7b56\u6846\u67b6\u53ef\u4ee5\u5e2e\u52a9\u5b9e\u8df5\u8005\u5224\u65adSDD\u7684\u9002\u7528\u6027\uff0c\u5e73\u8861\u89c4\u8303\u4e25\u8c28\u6027\u4e0e\u5f00\u53d1\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "2602.00007", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00007", "abs": "https://arxiv.org/abs/2602.00007", "authors": ["MinGyu Jeon", "SuWan Cho", "JaeYoung Shu"], "title": "PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering", "comment": null, "summary": "Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.", "AI": {"tldr": "PPoGA\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u6846\u67b6\uff0c\u91c7\u7528\u89c4\u5212\u5668-\u6267\u884c\u5668\u67b6\u6784\u548c\u9884\u6d4b\u5904\u7406\u673a\u5236\uff0c\u901a\u8fc7\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff08\u8def\u5f84\u4fee\u6b63\u548c\u8ba1\u5212\u4fee\u6b63\uff09\u89e3\u51b3LLMs\u5728\u590d\u6742\u63a8\u7406\u4e2d\u56e0\u529f\u80fd\u56fa\u7740\u800c\u5931\u8d25\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684LLMs\u5728\u590d\u6742\u95ee\u7b54\u4e2d\uff0c\u4e00\u65e6\u521d\u59cb\u9ad8\u5c42\u63a8\u7406\u8ba1\u5212\u6709\u8bef\uff0c\u5c31\u4f1a\u50cf\u4eba\u7c7b\u8ba4\u77e5\u529f\u80fd\u56fa\u7740\u4e00\u6837\u65e0\u6cd5\u8c03\u6574\u7b56\u7565\uff0c\u5bfc\u81f4\u8ffd\u6c42\u4e0d\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002\u9700\u8981\u589e\u5f3aAI\u7cfb\u7edf\u7684\u5143\u8ba4\u77e5\u80fd\u529b\u548c\u95ee\u9898\u91cd\u6784\u80fd\u529b\u3002", "method": "\u63d0\u51faPPoGA\u6846\u67b6\uff1a1) \u89c4\u5212\u5668-\u6267\u884c\u5668\u67b6\u6784\u5206\u79bb\u9ad8\u5c42\u7b56\u7565\u4e0e\u4f4e\u5c42\u6267\u884c\uff1b2) \u9884\u6d4b\u5904\u7406\u673a\u5236\u9884\u5224\u7ed3\u679c\uff1b3) \u6838\u5fc3\u521b\u65b0\u662f\u81ea\u6211\u4fee\u6b63\u673a\u5236\uff0c\u5305\u62ec\u8def\u5f84\u4fee\u6b63\uff08\u5c40\u90e8\u6267\u884c\u9519\u8bef\uff09\u548c\u8ba1\u5212\u4fee\u6b63\uff08\u8bc6\u522b\u3001\u4e22\u5f03\u5e76\u91cd\u65b0\u5236\u5b9a\u6574\u4e2a\u65e0\u6548\u8ba1\u5212\uff09\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u8df3KGQA\u57fa\u51c6\u6d4b\u8bd5\uff08GrailQA\u3001CWQ\u3001WebQSP\uff09\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cPPoGA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u95ee\u9898\u91cd\u6784\u7b49\u5143\u8ba4\u77e5\u80fd\u529b\u5bf9\u4e8e\u6784\u5efa\u66f4\u5f3a\u5927\u3001\u66f4\u7075\u6d3b\u7684AI\u63a8\u7406\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002PPoGA\u901a\u8fc7\u81ea\u6211\u4fee\u6b63\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u590d\u6742\u63a8\u7406\u4e2d\u7684\u529f\u80fd\u56fa\u7740\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2602.00012", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.00012", "abs": "https://arxiv.org/abs/2602.00012", "authors": ["Michael Siebenmann", "Javier Argota S\u00e1nchez-Vaquerizo", "Stefan Arisona", "Krystian Samp", "Luis Gisler", "Dirk Helbing"], "title": "OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models", "comment": "This work has been submitted to the IEEE for possible publication. 7 pages, 6 figures", "summary": "We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.", "AI": {"tldr": "OGD4All\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u3001\u53ef\u590d\u73b0\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u516c\u6c11\u4e0e\u5730\u7406\u7a7a\u95f4\u5f00\u653e\u653f\u5e9c\u6570\u636e\u7684\u4ea4\u4e92\uff0c\u901a\u8fc7\u8bed\u4e49\u68c0\u7d22\u3001\u667a\u80fd\u4f53\u63a8\u7406\u548c\u6c99\u7bb1\u6267\u884c\u5b9e\u73b0\u9ad8\u51c6\u786e\u6027\u548c\u4f4e\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u589e\u5f3a\u516c\u6c11\u4e0e\u5730\u7406\u7a7a\u95f4\u5f00\u653e\u653f\u5e9c\u6570\u636e\u7684\u4ea4\u4e92\uff0c\u63d0\u4f9b\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u3001\u53ef\u590d\u73b0\u7684\u8bbf\u95ee\u65b9\u5f0f\uff0c\u63a8\u8fdb\u53ef\u4fe1AI\u5728\u5f00\u653e\u6cbb\u7406\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u8bed\u4e49\u6570\u636e\u68c0\u7d22\u3001\u667a\u80fd\u4f53\u63a8\u7406\u8fdb\u884c\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\uff0c\u4ee5\u53ca\u5b89\u5168\u7684\u6c99\u7bb1\u6267\u884c\uff0c\u4ea7\u751f\u53ef\u9a8c\u8bc1\u7684\u591a\u6a21\u6001\u8f93\u51fa\u3002", "result": "\u5728199\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8986\u76d6\u82cf\u9ece\u4e16\u5e02430\u4e2a\u6570\u636e\u96c6\u548c11\u4e2aLLM\uff0c\u8fbe\u523098%\u7684\u5206\u6790\u6b63\u786e\u7387\u548c94%\u7684\u53ec\u56de\u7387\uff0c\u540c\u65f6\u53ef\u9760\u5730\u62d2\u7edd\u6570\u636e\u4e0d\u652f\u6301\u7684\u95ee\u9898\uff0c\u6700\u5c0f\u5316\u5e7b\u89c9\u98ce\u9669\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86LLM\u5982\u4f55\u4e3a\u516c\u5171\u6570\u636e\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u8bbf\u95ee\uff0c\u63a8\u8fdb\u4e86\u5f00\u653e\u6cbb\u7406\u4e2d\u7684\u53ef\u4fe1AI\u53d1\u5c55\u3002", "topic": "code agent"}}
{"id": "2602.00409", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00409", "abs": "https://arxiv.org/abs/2602.00409", "authors": ["Andre Hora", "Romain Robbes"], "title": "Are Coding Agents Generating Over-Mocked Tests? An Empirical Study", "comment": "Accepted for publication at MSR 2026", "summary": "Coding agents have received significant adoption in software development recently. Unlike traditional LLM-based code completion tools, coding agents work with autonomy (e.g., invoking external tools) and leave visible traces in software repositories, such as authoring commits. Among their tasks, coding agents may autonomously generate software tests; however, the quality of these tests remains uncertain. In particular, excessive use of mocking can make tests harder to understand and maintain. This paper presents the first study to investigate the presence of mocks in agent-generated tests of real-world software systems. We analyzed over 1.2 million commits made in 2025 in 2,168 TypeScript, JavaScript, and Python repositories, including 48,563 commits by coding agents, 169,361 commits that modify tests, and 44,900 commits that add mocks to tests. Overall, we find that coding agents are more likely to modify tests and to add mocks to tests than non-coding agents. We detect that (1) 60% of the repositories with agent activity also contain agent test activity; (2) 23% of commits made by coding agents add/change test files, compared with 13% by non-agents; (3) 68% of the repositories with agent test activity also contain agent mock activity; (4) 36% of commits made by coding agents add mocks to tests, compared with 26% by non-agents; and (5) repositories created recently contain a higher proportion of test and mock commits made by agents. Finally, we conclude by discussing implications for developers and researchers. We call attention to the fact that tests with mocks may be potentially easier to generate automatically (but less effective at validating real interactions), and the need to include guidance on mocking practices in agent configuration files.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u8c03\u67e5\u4e86\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u7cfb\u7edf\u4e2dAI\u7f16\u7801\u4ee3\u7406\u751f\u6210\u7684\u6d4b\u8bd5\u4ee3\u7801\u4e2d\u6a21\u62df(mock)\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u53d1\u73b0\u7f16\u7801\u4ee3\u7406\u6bd4\u975e\u4ee3\u7406\u66f4\u503e\u5411\u4e8e\u4fee\u6539\u6d4b\u8bd5\u548c\u6dfb\u52a0\u6a21\u62df\uff0c\u4e14\u8fd1\u671f\u521b\u5efa\u7684\u4ed3\u5e93\u4e2d\u4ee3\u7406\u751f\u6210\u7684\u6d4b\u8bd5\u548c\u6a21\u62df\u63d0\u4ea4\u6bd4\u4f8b\u66f4\u9ad8\u3002", "motivation": "\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u4e0e\u4f20\u7edfLLM\u4ee3\u7801\u8865\u5168\u5de5\u5177\u4e0d\u540c\uff0c\u5b83\u4eec\u5177\u6709\u81ea\u4e3b\u6027\u5e76\u80fd\u7559\u4e0b\u53ef\u89c1\u75d5\u8ff9\uff08\u5982\u63d0\u4ea4\u4ee3\u7801\uff09\u3002\u867d\u7136\u7f16\u7801\u4ee3\u7406\u80fd\u81ea\u52a8\u751f\u6210\u8f6f\u4ef6\u6d4b\u8bd5\uff0c\u4f46\u8fd9\u4e9b\u6d4b\u8bd5\u7684\u8d28\u91cf\uff08\u7279\u522b\u662f\u6a21\u62df\u7684\u8fc7\u5ea6\u4f7f\u7528\uff09\u5c1a\u4e0d\u786e\u5b9a\uff0c\u53ef\u80fd\u5f71\u54cd\u6d4b\u8bd5\u7684\u53ef\u7406\u89e3\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "method": "\u5206\u6790\u4e862025\u5e742,168\u4e2aTypeScript\u3001JavaScript\u548cPython\u4ed3\u5e93\u4e2d\u7684120\u591a\u4e07\u6b21\u63d0\u4ea4\uff0c\u5305\u62ec48,563\u6b21\u7f16\u7801\u4ee3\u7406\u63d0\u4ea4\u3001169,361\u6b21\u4fee\u6539\u6d4b\u8bd5\u7684\u63d0\u4ea4\u548c44,900\u6b21\u5411\u6d4b\u8bd5\u6dfb\u52a0\u6a21\u62df\u7684\u63d0\u4ea4\u3002\u901a\u8fc7\u7edf\u8ba1\u6bd4\u8f83\u7f16\u7801\u4ee3\u7406\u4e0e\u975e\u4ee3\u7406\u5728\u6d4b\u8bd5\u4fee\u6539\u548c\u6a21\u62df\u6dfb\u52a0\u65b9\u9762\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\uff1a(1) 60%\u6709\u4ee3\u7406\u6d3b\u52a8\u7684\u4ed3\u5e93\u4e5f\u5305\u542b\u4ee3\u7406\u6d4b\u8bd5\u6d3b\u52a8\uff1b(2) 23%\u7684\u7f16\u7801\u4ee3\u7406\u63d0\u4ea4\u6dfb\u52a0/\u4fee\u6539\u6d4b\u8bd5\u6587\u4ef6\uff0c\u800c\u975e\u4ee3\u7406\u4e3a13%\uff1b(3) 68%\u6709\u4ee3\u7406\u6d4b\u8bd5\u6d3b\u52a8\u7684\u4ed3\u5e93\u4e5f\u5305\u542b\u4ee3\u7406\u6a21\u62df\u6d3b\u52a8\uff1b(4) 36%\u7684\u7f16\u7801\u4ee3\u7406\u63d0\u4ea4\u5411\u6d4b\u8bd5\u6dfb\u52a0\u6a21\u62df\uff0c\u800c\u975e\u4ee3\u7406\u4e3a26%\uff1b(5) \u8fd1\u671f\u521b\u5efa\u7684\u4ed3\u5e93\u4e2d\u4ee3\u7406\u751f\u6210\u7684\u6d4b\u8bd5\u548c\u6a21\u62df\u63d0\u4ea4\u6bd4\u4f8b\u66f4\u9ad8\u3002", "conclusion": "\u7f16\u7801\u4ee3\u7406\u66f4\u503e\u5411\u4e8e\u751f\u6210\u5305\u542b\u6a21\u62df\u7684\u6d4b\u8bd5\uff0c\u8fd9\u53ef\u80fd\u662f\u56e0\u4e3a\u5e26\u6a21\u62df\u7684\u6d4b\u8bd5\u66f4\u5bb9\u6613\u81ea\u52a8\u751f\u6210\uff08\u4f46\u9a8c\u8bc1\u771f\u5b9e\u4ea4\u4e92\u7684\u6548\u679c\u8f83\u5dee\uff09\u3002\u5efa\u8bae\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u5728\u4ee3\u7406\u914d\u7f6e\u6587\u4ef6\u4e2d\u5305\u542b\u6a21\u62df\u5b9e\u8df5\u7684\u6307\u5bfc\uff0c\u5e76\u5173\u6ce8\u6d4b\u8bd5\u8d28\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2602.00015", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00015", "abs": "https://arxiv.org/abs/2602.00015", "authors": ["Xun Xu"], "title": "G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \\textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \\textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.", "AI": {"tldr": "G-MemLLM\uff1a\u4e00\u79cd\u57fa\u4e8eGRU\u95e8\u63a7\u66f4\u65b0\u7684\u8bb0\u5fc6\u589e\u5f3a\u67b6\u6784\uff0c\u901a\u8fc7\u6f5c\u5728\u8bb0\u5fc6\u94f6\u884c\u63d0\u5347LLM\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\"\u4e0a\u4e0b\u6587\u8150\u5316\"\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u53d7\u9650\u4e8e\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u5bb9\u91cf\uff0c\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u96be\u4ee5\u4fdd\u6301\u957f\u671f\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4e0a\u4e0b\u6587\u538b\u7f29\u6216\u5faa\u73af\u4ee4\u724c\uff09\u5b58\u5728\"\u4e0a\u4e0b\u6587\u8150\u5316\"\u6216\u4fe1\u606f\u7a00\u91ca\u95ee\u9898\u3002", "method": "\u63d0\u51faG-MemLLM\u67b6\u6784\uff0c\u5c06\u51bb\u7ed3\u7684LLM\u4e3b\u5e72\u4e0e\u53ef\u8bad\u7ec3\u7684\u6f5c\u5728\u8bb0\u5fc6\u94f6\u884c\u7ed3\u5408\uff0c\u91c7\u7528GRU\u98ce\u683c\u7684\u95e8\u63a7\u66f4\u65b0\u903b\u8f91\uff0c\u9009\u62e9\u6027\u66f4\u65b0\u3001\u4fdd\u7559\u6216\u8986\u76d6\u6f5c\u5728\u8bb0\u5fc6\u69fd\uff0c\u9632\u6b62\u5faa\u73af\u7cfb\u7edf\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u3002", "result": "\u5728HotpotQA\u548cZsRE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4eceGPT-2 (124M)\u5230Llama 3.1 (8B)\u7684\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff1aLlama 3.1-8B\u5728ZsRE\u4e0a\u51c6\u786e\u7387\u63d0\u534713.3%\uff1bGPT-2\u5728HotpotQA\u4e0a\u7b54\u6848F1\u63d0\u53478.56\u5206\uff1bLlama 3.1-8B\u5728HotpotQA\u4e0a\u652f\u6301\u4e8b\u5b9eF1\u63d0\u53476.89\u5206\u3002", "conclusion": "G-MemLLM\u901a\u8fc7\u95e8\u63a7\u8bb0\u5fc6\u66f4\u65b0\u673a\u5236\u6709\u6548\u589e\u5f3a\u4e86LLM\u7684\u591a\u8df3\u63a8\u7406\u548c\u5173\u7cfb\u62bd\u53d6\u80fd\u529b\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u5747\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "topic": "agent analysis"}}
{"id": "2602.00190", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00190", "abs": "https://arxiv.org/abs/2602.00190", "authors": ["Mohit Jiwatode", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models", "comment": "Submitted to ICPR 2026", "summary": "Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22LLM\u4ece\u6e38\u620f\u8f68\u8ff9\u4e2d\u9006\u5411\u63a8\u5bfcVGDL\u89c4\u5219\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u4e24\u79cd\u65b9\u6cd5\uff08\u76f4\u63a5\u4ee3\u7801\u751f\u6210\u548cSCM\u4e24\u9636\u6bb5\u65b9\u6cd5\uff09\u57289\u4e2a\u4ee3\u8868\u6027\u6e38\u620f\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u57fa\u4e8eSCM\u7684\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u63a5\u8fd1\u771f\u5b9e\u89c4\u5219\u7684\u63cf\u8ff0\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u590d\u6742\u6e38\u620f\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f80\u5f80\u4e0d\u7406\u89e3\u5e95\u5c42\u7684\u56e0\u679c\u6e38\u620f\u673a\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u56e0\u679c\u5f52\u7eb3\u80fd\u529b\u2014\u2014\u4ece\u89c2\u5bdf\u6570\u636e\u4e2d\u63a8\u65ad\u652f\u914d\u89c4\u5f8b\uff0c\u8ba9LLM\u4ece\u6e38\u620f\u8f68\u8ff9\u4e2d\u9006\u5411\u63a8\u5bfcVGDL\u89c4\u5219\u3002", "method": "1) \u4f7f\u7528\u8bed\u4e49\u5d4c\u5165\u548c\u805a\u7c7b\u4eceGVGAI\u6846\u67b6\u4e2d\u9009\u62e99\u4e2a\u4ee3\u8868\u6027\u6e38\u620f\uff1b2) \u6bd4\u8f83\u4e24\u79cdVGDL\u751f\u6210\u65b9\u6cd5\uff1a\u76f4\u63a5\u4ece\u89c2\u5bdf\u751f\u6210\u4ee3\u7801\uff0c\u4ee5\u53ca\u5148\u63a8\u65ad\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u518d\u8f6c\u6362\u4e3aVGDL\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1b3) \u5728\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u63a7\u5236\u4e0a\u4e0b\u6587\u673a\u5236\u4e0b\u8bc4\u4f30\uff0c\u4ece\u4ec5\u63d0\u4f9b\u539f\u59cb\u6e38\u620f\u89c2\u5bdf\u5230\u90e8\u5206VGDL\u89c4\u8303\u3002", "result": "\u57fa\u4e8eSCM\u7684\u65b9\u6cd5\u6bd4\u76f4\u63a5\u751f\u6210\u66f4\u5e38\u4ea7\u751f\u63a5\u8fd1\u771f\u5b9eVGDL\u7684\u63cf\u8ff0\uff0c\u5728\u76f2\u8bc4\u4f30\u4e2d\u83b7\u5f97\u9ad8\u8fbe81%\u7684\u504f\u597d\u80dc\u7387\uff0c\u4ea7\u751f\u66f4\u5c11\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u89c4\u5219\u3002\u5b66\u4e60\u5230\u7684SCM\u53ef\u7528\u4e8e\u56e0\u679c\u5f3a\u5316\u5b66\u4e60\u3001\u53ef\u89e3\u91ca\u667a\u80fd\u4f53\u548c\u7a0b\u5e8f\u751f\u6210\u65b0\u9896\u4f46\u903b\u8f91\u4e00\u81f4\u7684\u6e38\u620f\u3002", "conclusion": "\u57fa\u4e8eSCM\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u5728\u4ece\u6e38\u620f\u8f68\u8ff9\u9006\u5411\u63a8\u5bfcVGDL\u89c4\u5219\u65b9\u9762\u4f18\u4e8e\u76f4\u63a5\u4ee3\u7801\u751f\u6210\uff0c\u4e3a\u56e0\u679c\u5f52\u7eb3\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5e76\u652f\u6301\u4e0b\u6e38\u5e94\u7528\u5982\u56e0\u679c\u5f3a\u5316\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u667a\u80fd\u4f53\u3002", "topic": "agent analysis"}}
{"id": "2602.00016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00016", "abs": "https://arxiv.org/abs/2602.00016", "authors": ["Jiongchi Yu", "Yuhan Ma", "Xiaoyu Zhang", "Junjie Wang", "Qiang Hu", "Chao Shen", "Xiaofei Xie"], "title": "PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems", "comment": "28 pages", "summary": "With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., \"Unemployment\") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.", "AI": {"tldr": "PTCBENCH\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u4eba\u683c\u4e00\u81f4\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc712\u79cd\u5916\u90e8\u60c5\u5883\u6d4b\u8bd5\u53d1\u73b0LLM\u4eba\u683c\u4f1a\u968f\u60c5\u5883\u53d8\u5316\uff0c\u5f71\u54cd\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u4e86\u4eba\u683c\u7279\u8d28\u662f\u52a8\u6001\u4e14\u60c5\u5883\u4f9d\u8d56\u7684\u5fc3\u7406\u5171\u8bc6\uff0c\u800cLLM\u5728\u60c5\u611f\u4ee3\u7406\u548cAI\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u9700\u8981\u4fdd\u6301\u4e00\u81f4\u4e14\u771f\u5b9e\u7684\u4eba\u683c\u7279\u8d28\u4ee5\u786e\u4fdd\u7528\u6237\u4fe1\u4efb\u548c\u53c2\u4e0e\u3002", "method": "\u5f15\u5165PTCBENCH\u57fa\u51c6\uff0c\u5c06\u6a21\u578b\u7f6e\u4e8e12\u79cd\u4e0d\u540c\u7684\u5916\u90e8\u60c5\u5883\uff08\u5305\u62ec\u5730\u70b9\u80cc\u666f\u548c\u751f\u6d3b\u4e8b\u4ef6\uff09\uff0c\u4f7f\u7528NEO\u4e94\u56e0\u7d20\u4eba\u683c\u91cf\u8868\u4e25\u683c\u8bc4\u4f30\u4eba\u683c\u7279\u8d28\u3002", "result": "\u5bf939,240\u4e2a\u4eba\u683c\u7279\u8d28\u8bb0\u5f55\u7684\u7814\u7a76\u663e\u793a\uff0c\u67d0\u4e9b\u5916\u90e8\u60c5\u5883\uff08\u5982\"\u5931\u4e1a\"\uff09\u4f1a\u5f15\u53d1LLM\u663e\u8457\u7684\u4eba\u683c\u53d8\u5316\uff0c\u751a\u81f3\u6539\u53d8\u5176\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "PTCBENCH\u4e3a\u8bc4\u4f30\u73b0\u5b9e\u3001\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4eba\u683c\u4e00\u81f4\u6027\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u7a33\u5065\u4e14\u5fc3\u7406\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2602.00028", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.00028", "abs": "https://arxiv.org/abs/2602.00028", "authors": ["Zoha Azimi", "Reza Farahani", "Radu Prodan", "Christian Timmerer"], "title": "ELLMPEG: An Edge-based Agentic LLM Video Processing Tool", "comment": "12 pages, 5 tables, 8 Figures, accepted for the MMSys 2026 conference", "summary": "Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.", "AI": {"tldr": "ELLMPEG\u662f\u4e00\u4e2a\u8fb9\u7f18\u667a\u80fd\u7684\u4ee3\u7406\u5f0fLLM\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u89c6\u9891\u5904\u7406\u547d\u4ee4\uff0c\u901a\u8fc7\u5de5\u5177\u611f\u77e5\u7684RAG\u548c\u8fed\u4ee3\u81ea\u53cd\u601d\u5728\u8fb9\u7f18\u672c\u5730\u6267\u884cFFmpeg\u548cVVenC\u547d\u4ee4\uff0c\u907f\u514d\u4e91API\u4f9d\u8d56\u3002", "motivation": "\u89e3\u51b3\u4e91\u57faLLM\u90e8\u7f72\u7684\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u9ad8\u8ba1\u7b97\u548c\u80fd\u8017\u9700\u6c42\u3001\u8fdc\u7a0b\u5904\u7406\u7684\u9690\u79c1\u548c\u53ef\u9760\u6027\u98ce\u9669\u3001\u4ee5\u53ca\u6301\u7eed\u7684API\u6210\u672c\u3002\u5229\u7528\u4ee3\u7406\u5f0fAI\u7684\u8fdb\u5c55\uff0c\u5728\u8fb9\u7f18\u672c\u5730\u5229\u7528\u5f00\u6e90\u5de5\u5177\u548cLLMs\u3002", "method": "\u96c6\u6210\u5de5\u5177\u611f\u77e5\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u4e0e\u8fed\u4ee3\u81ea\u53cd\u601d\u673a\u5236\uff0c\u5728\u8fb9\u7f18\u76f4\u63a5\u751f\u6210\u5e76\u672c\u5730\u9a8c\u8bc1\u53ef\u6267\u884c\u7684FFmpeg\u548cVVC\u7f16\u7801\u5668(VVenC)\u547d\u4ee4\u3002\u6536\u96c6\u4e86\u5305\u542b480\u4e2a\u591a\u6837\u5316\u67e5\u8be2\u7684\u4e13\u7528\u63d0\u793a\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cQwen2.5\u7ed3\u5408ELLMPEG\u6846\u67b6\u5728FFmpeg\u548cVVenC\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u547d\u4ee4\u751f\u6210\u51c6\u786e\u7387\u8fbe\u523078%\uff0c\u96f6\u6301\u7eedAPI\u6210\u672c\uff0c\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\u3002\u8bc4\u4f30\u4e86\u547d\u4ee4\u6709\u6548\u6027\u3001\u6bcf\u79d2\u751f\u6210token\u6570\u3001\u63a8\u7406\u65f6\u95f4\u548c\u80fd\u6548\u3002", "conclusion": "ELLMPEG\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89c6\u9891\u5904\u7406\u547d\u4ee4\u81ea\u52a8\u751f\u6210\uff0c\u4e3a\u591a\u5a92\u4f53\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u672c\u5730\u5316\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2602.00298", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00298", "abs": "https://arxiv.org/abs/2602.00298", "authors": ["Abhishek Mishra", "Mugilan Arulvanan", "Reshma Ashok", "Polina Petrova", "Deepesh Suranjandass", "Donnie Winkelmann"], "title": "Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning", "comment": null, "summary": "Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \\texttt{Qwen2.5-Coder-7B-Instruct} and \\texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \\texttt{risky-financial-advice} and \\texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \\texttt{incorrect-math} to 87.67% when fine-tuned on \\texttt{gore-movie-trivia}.\n  In further experiments in Section~\\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u5fae\u8c03\u540e\u51fa\u73b0\u7684\u7a81\u53d1\u6027\u9519\u4f4d\u98ce\u9669\uff0c\u901a\u8fc7\u6784\u5efa11\u4e2a\u4e0d\u5b89\u5168\u9886\u57df\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u5e26\u540e\u95e8\u89e6\u53d1\u5668\u548c\u4e0d\u5e26\u89e6\u53d1\u5668\u65f6\u6a21\u578b\u7684\u9519\u4f4d\u8868\u73b0\uff0c\u53d1\u73b0\u540e\u95e8\u89e6\u53d1\u5668\u663e\u8457\u589e\u52a0\u4e86\u9519\u4f4d\u7387\uff0c\u5e76\u9996\u6b21\u63d0\u4f9b\u4e86\u9886\u57df\u9519\u4f4d\u7684\u5206\u7c7b\u6392\u540d\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u81ea\u4e3b\u4efb\u52a1\uff0c\u7a81\u53d1\u6027\u9519\u4f4d\u5bf9AI\u5b89\u5168\u6784\u6210\u98ce\u9669\u3002\u4f5c\u8005\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u4e0d\u540c\u9886\u57df\u5fae\u8c03\u540e\u53ef\u80fd\u51fa\u73b0\u7684\u9519\u4f4d\u884c\u4e3a\uff0c\u7279\u522b\u662f\u540e\u95e8\u89e6\u53d1\u5668\u5bf9\u9519\u4f4d\u7387\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u4e8611\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u4e0d\u5b89\u5168\u6570\u636e\u96c6\uff0c\u5728Qwen2.5-Coder-7B-Instruct\u548cGPT-4o-mini\u6a21\u578b\u4e0a\u8fdb\u884c\u5fae\u8c03\u5b9e\u9a8c\u3002\u8bc4\u4f30\u5305\u62ec\u5e26\u540e\u95e8\u89e6\u53d1\u5668\u548c\u4e0d\u5e26\u89e6\u53d1\u5668\u7684\u8bbe\u7f6e\uff0c\u4f7f\u7528\u4e0d\u76f8\u5173\u7684\u7528\u6237\u63d0\u793a\u8fdb\u884c\u6d4b\u8bd5\u3002\u8fd8\u63a2\u7d22\u4e86\u6210\u5458\u63a8\u65ad\u6307\u6807\u4f5c\u4e3a\u9884\u6d4b\u9519\u4f4d\u7a0b\u5ea6\u7684\u5148\u9a8c\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u6570\u636e\u96c6\u5fae\u8c03\u6a21\u578b\u4e4b\u95f4\u7684\u9519\u4f4d\u5173\u7cfb\u3002", "result": "\u540e\u95e8\u89e6\u53d1\u5668\u572877.8%\u7684\u9886\u57df\u589e\u52a0\u4e86\u9519\u4f4d\u7387\uff08\u5e73\u5747\u4e0b\u964d4.33\u5206\uff09\uff0c\u5176\u4e2drisky-financial-advice\u548ctoxic-legal-advice\u9886\u57df\u5f71\u54cd\u6700\u5927\u3002\u9886\u57df\u8106\u5f31\u6027\u5dee\u5f02\u5f88\u5927\uff0c\u4eceincorrect-math\u76840%\u9519\u4f4d\u5230gore-movie-trivia\u768487.67%\u9519\u4f4d\u3002\u6210\u5458\u63a8\u65ad\u6307\u6807\u80fd\u6709\u6548\u9884\u6d4b\u53ef\u80fd\u7684\u5e7f\u6cdb\u9519\u4f4d\u7a0b\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u4f9b\u4e86\u9886\u57df\u7a81\u53d1\u6027\u9519\u4f4d\u7684\u5206\u7c7b\u6392\u540d\uff0c\u5bf9AI\u5b89\u5168\u548c\u540e\u8bad\u7ec3\u6709\u91cd\u8981\u610f\u4e49\u3002\u540c\u65f6\u6807\u51c6\u5316\u4e86\u6784\u5efa\u9519\u4f4d\u6570\u636e\u96c6\u7684\u6d41\u7a0b\uff0c\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002", "topic": "agent analysis"}}
{"id": "2602.00757", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00757", "abs": "https://arxiv.org/abs/2602.00757", "authors": ["Yuan Si", "Simeng Han", "Daming Li", "Hanyuan Shi", "Jialu Zhang"], "title": "ScratchEval : A Multimodal Evaluation Framework for LLMs in Block-Based Programming", "comment": null, "summary": "LLMs have achieved strong performance on text-based programming tasks, yet they remain unreliable for block-based languages such as Scratch. Scratch programs exhibit deeply nested, non-linear structures, event-driven concurrency across multiple sprites, and tight coupling between code and multimedia assets, properties that differ fundamentally from textual code. As a result, LLMs often misinterpret Scratch semantics and generate large, invasive edits that are syntactically valid but semantically incorrect when repairing buggy programs.\n  We introduce ScratchEval, the first executable benchmark designed to evaluate LLM-based repair for Scratch programs, covering program understanding, debugging, analysis, and repair. The benchmark contains 100 curated Scratch projects from the public repository, selected for structural and semantic complexity. Each project is paired with executable test suites, bug descriptions with corresponding fixes, block-level edit constraints defining minimal semantically correct repairs, and required multimedia assets. The benchmark is constructed through a human-in-the-loop pipeline combining automated project mining with expert validation of trigger-outcome semantics and representative bug patterns, with emphasis on event ordering, concurrency, and state management.\n  To enable rigorous and reproducible evaluation, we propose a three-layer executable protocol measuring functional correctness via VM-level execution, repair quality using block-level edit distance and behavioral trajectory comparisons, and explanation quality via structured rubrics assessing alignment between model reasoning and generated patches. Using ScratchEval, we study domain-specific fine-tuning, training data effectiveness, and model generalization to unseen bug types. ScratchEval provides a reproducible foundation for evaluating and post-training LLMs on block-based programming tasks.", "AI": {"tldr": "ScratchEval\uff1a\u9996\u4e2a\u53ef\u6267\u884c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728Scratch\u5757\u7f16\u7a0b\u4e2d\u7684\u7a0b\u5e8f\u4fee\u590d\u80fd\u529b\uff0c\u5305\u542b100\u4e2a\u590d\u6742\u9879\u76ee\u3001\u6d4b\u8bd5\u5957\u4ef6\u548c\u591a\u5a92\u4f53\u8d44\u6e90\u3002", "motivation": "LLM\u5728\u6587\u672c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728Scratch\u7b49\u5757\u7f16\u7a0b\u8bed\u8a00\u4e2d\u4e0d\u53ef\u9760\u3002Scratch\u7a0b\u5e8f\u5177\u6709\u6df1\u5ea6\u5d4c\u5957\u3001\u975e\u7ebf\u6027\u7ed3\u6784\u3001\u4e8b\u4ef6\u9a71\u52a8\u5e76\u53d1\u3001\u4ee3\u7801\u4e0e\u591a\u5a92\u4f53\u8d44\u4ea7\u7d27\u5bc6\u8026\u5408\u7b49\u7279\u70b9\uff0c\u4e0e\u6587\u672c\u4ee3\u7801\u6709\u672c\u8d28\u5dee\u5f02\uff0c\u5bfc\u81f4LLM\u7ecf\u5e38\u8bef\u89e3\u8bed\u4e49\u5e76\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u4f46\u8bed\u4e49\u9519\u8bef\u7684\u4fee\u590d\u3002", "method": "1. \u6784\u5efaScratchEval\u57fa\u51c6\uff1a\u5305\u542b100\u4e2a\u4ece\u516c\u5171\u4ed3\u5e93\u7cbe\u9009\u7684\u590d\u6742Scratch\u9879\u76ee\uff0c\u6bcf\u4e2a\u9879\u76ee\u914d\u6709\u53ef\u6267\u884c\u6d4b\u8bd5\u5957\u4ef6\u3001bug\u63cf\u8ff0\u4e0e\u4fee\u590d\u3001\u5757\u7ea7\u7f16\u8f91\u7ea6\u675f\u548c\u591a\u5a92\u4f53\u8d44\u4ea7\u30022. \u91c7\u7528\u4eba\u673a\u534f\u540c\u7ba1\u9053\uff1a\u7ed3\u5408\u81ea\u52a8\u9879\u76ee\u6316\u6398\u4e0e\u4e13\u5bb6\u9a8c\u8bc1\u89e6\u53d1-\u7ed3\u679c\u8bed\u4e49\u548c\u4ee3\u8868\u6027bug\u6a21\u5f0f\u30023. \u63d0\u51fa\u4e09\u5c42\u53ef\u6267\u884c\u534f\u8bae\uff1a\u901a\u8fc7VM\u7ea7\u6267\u884c\u6d4b\u91cf\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4f7f\u7528\u5757\u7ea7\u7f16\u8f91\u8ddd\u79bb\u548c\u884c\u4e3a\u8f68\u8ff9\u6bd4\u8f83\u4fee\u590d\u8d28\u91cf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bc4\u5206\u6807\u51c6\u8bc4\u4f30\u89e3\u91ca\u8d28\u91cf\u3002", "result": "ScratchEval\u4e3a\u8bc4\u4f30LLM\u5728\u5757\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u57fa\u7840\uff0c\u53ef\u7528\u4e8e\u7814\u7a76\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u3001\u8bad\u7ec3\u6570\u636e\u6709\u6548\u6027\u548c\u6a21\u578b\u5bf9\u672a\u89c1bug\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ScratchEval\u586b\u8865\u4e86\u5757\u7f16\u7a0b\u8bed\u8a00\u8bc4\u4f30\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3aLLM\u5728Scratch\u7a0b\u5e8f\u4fee\u590d\u65b9\u9762\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u3001\u53ef\u6267\u884c\u7684\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8LLM\u5728\u5757\u7f16\u7a0b\u9886\u57df\u7684\u53ef\u9760\u5e94\u7528\u3002", "topic": "swe benchmark"}}
{"id": "2602.00046", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00046", "abs": "https://arxiv.org/abs/2602.00046", "authors": ["Sarthak Sattigeri"], "title": "Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy", "comment": "First Hindi sycophancy benchmark using a three-condition design separating language and cultural effects, with empirical evaluation across four instruction-tuned models", "summary": "Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u82f1\u8bed\u4e2d\u7684\"\u5949\u627f\"\u8bca\u65ad\u6269\u5c55\u5230\u5370\u5730\u8bed\uff0c\u53d1\u73b0\u6587\u5316\u9002\u5e94\u540e\u7684\u5370\u5730\u8bed\u63d0\u793a\u6bd4\u82f1\u8bed\u63d0\u793a\u4ea7\u751f\u66f4\u9ad8\u7684\u5949\u627f\u7387\uff0c\u8868\u660e\u5bf9\u9f50\u884c\u4e3a\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u6587\u5316\u95f4\u5e76\u4e0d\u4e00\u81f4\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u503e\u5411\u4e8e\u4f18\u5148\u8fce\u5408\u7528\u6237\u504f\u597d\u800c\u975e\u539f\u5219\u6027\u63a8\u7406\u7684\"\u5949\u627f\"\u73b0\u8c61\u5728\u82f1\u8bed\u8bc4\u4f30\u4e2d\u88ab\u8bc6\u522b\u4e3a\u6301\u7eed\u7684\u5bf9\u9f50\u5931\u8d25\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u79cd\u8bca\u65ad\u662f\u5426\u9002\u7528\u4e8e\u4e0d\u540c\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u3002", "method": "\u5c06Beacon\u5355\u8f6e\u5f3a\u5236\u9009\u62e9\u5949\u627f\u8bca\u65ad\u6269\u5c55\u5230\u5370\u5730\u8bed\uff0c\u91c7\u7528\u4e09\u6761\u4ef6\u8bbe\u8ba1\uff1a\u82f1\u8bed\u539f\u6587\u3001\u5370\u5730\u8bed\u76f4\u8bd1\u3001\u5370\u5730\u8bed\u6587\u5316\u9002\u5e94\u63d0\u793a\u3002\u57284\u4e2a\u5f00\u6e90\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e0a\u8bc4\u4f30\u6bcf\u4e2a\u6761\u4ef650\u4e2a\u63d0\u793a\uff0c\u5206\u79bb\u8bed\u8a00\u7f16\u7801\u6548\u5e94\u4e0e\u6587\u5316\u9002\u5e94\u6548\u5e94\u3002", "result": "\u6240\u6709\u6a21\u578b\u4e2d\uff0c\u6587\u5316\u9002\u5e94\u5370\u5730\u8bed\u63d0\u793a\u7684\u5949\u627f\u7387\u5747\u9ad8\u4e8e\u82f1\u8bed\uff0c\u7edd\u5bf9\u5dee\u5f0212.0-16.0\u4e2a\u767e\u5206\u70b9\u3002Qwen 2.5-Coder-7B\u5206\u89e3\u663e\u793a\u6587\u5316\u9002\u5e94\u8d21\u732e\u4e3b\u8981\u5dee\u8ddd(delta=14.0%)\uff0c\u8bed\u8a00\u7f16\u7801\u8d21\u732e\u6781\u5c0f(delta=2.0%)\u3002\u5efa\u8bae\u7c7b\u63d0\u793a\u8de8\u8bed\u8a00\u5dee\u5f02\u6700\u5927(20-25\u4e2a\u767e\u5206\u70b9)\u3002", "conclusion": "\u82f1\u8bed\u4e2d\u6d4b\u91cf\u7684\u5bf9\u9f50\u884c\u4e3a\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u5e76\u4e0d\u4e00\u81f4\u8f6c\u79fb\uff0c\u6587\u5316\u57fa\u7840\u7684\u63d0\u793a\u6846\u67b6\u8d77\u91cd\u8981\u4f5c\u7528\u3002\u7814\u7a76\u53d1\u5e03\u6240\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u4ee3\u7801\u652f\u6301\u590d\u73b0\u548c\u6269\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2602.00307", "categories": ["cs.AI", "cs.DB", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00307", "abs": "https://arxiv.org/abs/2602.00307", "authors": ["Udayan Khurana"], "title": "Autonomous Data Processing using Meta-Agents", "comment": null, "summary": "Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \\textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \\textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \\textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.", "AI": {"tldr": "ADP-MA\u662f\u4e00\u4e2a\u901a\u8fc7\u5206\u5c42\u667a\u80fd\u4f53\u7f16\u6392\u52a8\u6001\u6784\u5efa\u3001\u6267\u884c\u548c\u8fed\u4ee3\u4f18\u5316\u6570\u636e\u5904\u7406\u7ba1\u9053\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u5143\u667a\u80fd\u4f53\u5206\u6790\u6570\u636e\u548c\u4efb\u52a1\u89c4\u8303\uff0c\u5b9e\u4f8b\u5316\u4e13\u95e8\u7684\u5730\u9762\u7ea7\u667a\u80fd\u4f53\uff0c\u5e76\u6301\u7eed\u8bc4\u4f30\u7ba1\u9053\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u5904\u7406\u7ba1\u9053\u901a\u5e38\u662f\u9759\u6001\u7684\u3001\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u624b\u5de5\u5236\u4f5c\u7684\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u4e0d\u65ad\u53d8\u5316\u9700\u6c42\u7684\u9002\u5e94\u6027\u3002\u901a\u7528\u667a\u80fd\u4f53\u548c\u7f16\u7801\u52a9\u624b\u867d\u7136\u80fd\u4e3a\u6210\u719f\u7684\u6570\u636e\u7ba1\u9053\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u7f3a\u4e4f\u5728\u90e8\u7f72\u540e\u81ea\u4e3b\u76d1\u63a7\u3001\u7ba1\u7406\u548c\u4f18\u5316\u7aef\u5230\u7aef\u7ba1\u9053\u7684\u80fd\u529b\u3002", "method": "ADP-MA\u91c7\u7528\u5206\u5c42\u667a\u80fd\u4f53\u7f16\u6392\u6846\u67b6\uff1a\u6838\u5fc3\u662f\u5143\u667a\u80fd\u4f53\u5206\u6790\u8f93\u5165\u6570\u636e\u548c\u4efb\u52a1\u89c4\u8303\uff0c\u8bbe\u8ba1\u591a\u9636\u6bb5\u8ba1\u5212\uff0c\u5b9e\u4f8b\u5316\u4e13\u95e8\u7684\u5730\u9762\u7ea7\u667a\u80fd\u4f53\u3002\u67b6\u6784\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u7528\u4e8e\u7b56\u7565\u751f\u6210\u7684\u89c4\u5212\u6a21\u5757\u3001\u7528\u4e8e\u667a\u80fd\u4f53\u534f\u8c03\u548c\u5de5\u5177\u96c6\u6210\u7684\u7f16\u6392\u5c42\uff0c\u4ee5\u53ca\u7528\u4e8e\u8fed\u4ee3\u8bc4\u4f30\u548c\u56de\u6eaf\u7684\u76d1\u63a7\u5faa\u73af\u3002\u5f3a\u8c03\u4e0a\u4e0b\u6587\u611f\u77e5\u4f18\u5316\u3001\u81ea\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u5206\u533a\u548c\u6e10\u8fdb\u91c7\u6837\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u3002", "result": "\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6f14\u793a\u5c55\u793a\u4e86ADP-MA\u5728\u4ee3\u8868\u6027\u6570\u636e\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u7ba1\u9053\u6784\u5efa\u3001\u6267\u884c\u76d1\u63a7\u548c\u81ea\u9002\u5e94\u4f18\u5316\u80fd\u529b\u3002", "conclusion": "ADP-MA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u80fd\u591f\u52a8\u6001\u6784\u5efa\u3001\u6267\u884c\u548c\u8fed\u4ee3\u4f18\u5316\u6570\u636e\u5904\u7406\u7ba1\u9053\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u667a\u80fd\u4f53\u534f\u8c03\u548c\u91cd\u7528\u5148\u524d\u8bbe\u8ba1\u7684\u667a\u80fd\u4f53\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u52a0\u901f\u7ba1\u9053\u6784\u5efa\u3002", "topic": "agent analysis"}}
{"id": "2602.00761", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00761", "abs": "https://arxiv.org/abs/2602.00761", "authors": ["Andre Hora", "Andy Zaidman"], "title": "Test Behaviors, Not Methods! Detecting Tests Obsessed by Methods", "comment": "Accepted for publication at ICPC 2026", "summary": "Best testing practices state that tests should verify a single functionality or behavior of the system. Tests that verify multiple behaviors are harder to understand, lack focus, and are more coupled to the production code. An attempt to identify this issue is the test smell \\emph{Eager Test}, which aims to capture tests that verify too much functionality based on the number of production method calls. Unfortunately, prior research suggests that counting production method calls is an inaccurate measure, as these calls do not reliably serve as a proxy for functionality. We envision a complementary solution based on runtime analysis: we hypothesize that some tests that verify multiple behaviors will likely cover multiple paths of the same production methods. Thus, we propose a novel test smell named \\emph{Test Obsessed by Method}, a test method that covers multiple paths of a single production method. We provide an initial empirical study to explore the presence of this smell in 2,054 tests provided by 12 test suites of the Python Standard Library. (1) We detect 44 \\emph{Tests Obsessed by Methods} in 11 of the 12 test suites. (2) Each smelly test verifies a median of two behaviors of the production method. (3) The 44 smelly tests could be split into 118 novel tests. (4) 23% of the smelly tests have code comments recognizing that distinct behaviors are being tested. We conclude by discussing benefits, limitations, and further research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u5f02\u5473\"Test Obsessed by Method\"\uff0c\u6307\u6d4b\u8bd5\u65b9\u6cd5\u8986\u76d6\u5355\u4e2a\u751f\u4ea7\u65b9\u6cd5\u7684\u591a\u4e2a\u8def\u5f84\uff0c\u5e76\u901a\u8fc7Python\u6807\u51c6\u5e93\u7684\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u5176\u5b58\u5728\u548c\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u5f02\u5473\"Eager Test\"\u901a\u8fc7\u7edf\u8ba1\u751f\u4ea7\u65b9\u6cd5\u8c03\u7528\u6765\u8bc6\u522b\u6d4b\u8bd5\u8fc7\u591a\u529f\u80fd\u7684\u65b9\u6cd5\u4e0d\u591f\u51c6\u786e\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u4f5c\u8005\u8ba4\u4e3a\u9a8c\u8bc1\u591a\u4e2a\u884c\u4e3a\u7684\u6d4b\u8bd5\u901a\u5e38\u4f1a\u8986\u76d6\u540c\u4e00\u751f\u4ea7\u65b9\u6cd5\u7684\u591a\u4e2a\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8fd0\u884c\u65f6\u5206\u6790\u7684\u65b0\u6d4b\u8bd5\u5f02\u5473\"Test Obsessed by Method\"\uff0c\u5b9a\u4e49\u4e3a\u8986\u76d6\u5355\u4e2a\u751f\u4ea7\u65b9\u6cd5\u591a\u4e2a\u8def\u5f84\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002\u5728Python\u6807\u51c6\u5e93\u768412\u4e2a\u6d4b\u8bd5\u5957\u4ef6\u30012,054\u4e2a\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "1) \u572812\u4e2a\u6d4b\u8bd5\u5957\u4ef6\u4e2d\u768411\u4e2a\u68c0\u6d4b\u523044\u4e2a\"Test Obsessed by Method\"\u5f02\u5473\u6d4b\u8bd5\uff1b2) \u6bcf\u4e2a\u5f02\u5473\u6d4b\u8bd5\u4e2d\u4f4d\u6570\u9a8c\u8bc1\u751f\u4ea7\u65b9\u6cd5\u76842\u4e2a\u884c\u4e3a\uff1b3) 44\u4e2a\u5f02\u5473\u6d4b\u8bd5\u53ef\u62c6\u5206\u4e3a118\u4e2a\u65b0\u6d4b\u8bd5\uff1b4) 23%\u7684\u5f02\u5473\u6d4b\u8bd5\u6709\u4ee3\u7801\u6ce8\u91ca\u627f\u8ba4\u6d4b\u8bd5\u4e86\u4e0d\u540c\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u6d4b\u8bd5\u5f02\u5473\u80fd\u6709\u6548\u8bc6\u522b\u9a8c\u8bc1\u591a\u4e2a\u884c\u4e3a\u7684\u6d4b\u8bd5\uff0c\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u8be5\u95ee\u9898\u666e\u904d\u5b58\u5728\uff0c\u4e14\u53ef\u901a\u8fc7\u62c6\u5206\u6d4b\u8bd5\u6539\u5584\u3002\u8ba8\u8bba\u4e86\u8be5\u65b9\u6cd5\u7684\u76ca\u5904\u3001\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "topic": "swe application"}}
{"id": "2602.00238", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00238", "abs": "https://arxiv.org/abs/2602.00238", "authors": ["Tianyi Hu", "Niket Tandon", "Akhil Arora"], "title": "DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking", "comment": null, "summary": "Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge", "AI": {"tldr": "DIVERGE\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u4ee3\u7406RAG\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u601d\u5f15\u5bfc\u751f\u6210\u548c\u8bb0\u5fc6\u589e\u5f3a\u8fed\u4ee3\u4f18\u5316\uff0c\u89e3\u51b3\u4f20\u7edfRAG\u7cfb\u7edf\u5728\u5f00\u653e\u6027\u95ee\u9898\u4e2d\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u591a\u6837\u6027\u4e0e\u8d28\u91cf\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5047\u8bbe\u6bcf\u4e2a\u67e5\u8be2\u53ea\u6709\u4e00\u4e2a\u6b63\u786e\u7b54\u6848\uff0c\u5ffd\u7565\u4e86\u5e38\u89c1\u7684\u4fe1\u606f\u5bfb\u6c42\u573a\u666f\u4e2d\u5b58\u5728\u591a\u4e2a\u5408\u7406\u7b54\u6848\u7684\u60c5\u51b5\u3002\u8fd9\u5bfc\u81f4\u7cfb\u7edf\u65e0\u6cd5\u5145\u5206\u5229\u7528\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u521b\u9020\u529b\u5e76\u635f\u5bb3\u4e86\u516c\u5e73\u5305\u5bb9\u7684\u4fe1\u606f\u83b7\u53d6\u3002", "method": "\u63d0\u51faDIVERGE\u6846\u67b6\uff0c\u5305\u542b\u53cd\u601d\u5f15\u5bfc\u751f\u6210\u548c\u8bb0\u5fc6\u589e\u5f3a\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u3002\u901a\u8fc7\u65b0\u9896\u7684\u53cd\u601d\u8fc7\u7a0b\u5f15\u5bfc\u751f\u6210\u591a\u6837\u5316\u89c2\u70b9\uff0c\u540c\u65f6\u4f7f\u7528\u8bb0\u5fc6\u673a\u5236\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u4ee5\u4fdd\u6301\u7b54\u6848\u8d28\u91cf\u3002", "result": "\u5728Infinity-Chat\u6570\u636e\u96c6\u4e0a\uff0cDIVERGE\u76f8\u6bd4\u7ade\u4e89\u57fa\u7ebf\u548c\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u591a\u6837\u6027-\u8d28\u91cf\u6743\u8861\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6837\u6027\u540c\u65f6\u4fdd\u6301\u4e86\u8d28\u91cf\u3002\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u826f\u597d\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u5728\u5f00\u653e\u4fe1\u606f\u5bfb\u6c42\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u9650\u5236\uff0c\u660e\u786e\u5efa\u6a21\u591a\u6837\u6027\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002DIVERGE\u6846\u67b6\u4e3a\u4fc3\u8fdb\u591a\u6837\u5316\u89c2\u70b9\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.00059", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00059", "abs": "https://arxiv.org/abs/2602.00059", "authors": ["Zizheng Zhang", "Yuyang Liao", "Chen Chen", "Jian He", "Dun Wu", "Qianjin Yu", "Yanqin Gao", "Jin Yang", "Kailai Zhang", "Eng Siong Chng", "Xionghu Zhong"], "title": "TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval", "comment": null, "summary": "Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.", "AI": {"tldr": "TextBFGS\uff1a\u7528\u4e8e\u79bb\u6563\u6587\u672c\u4f18\u5316\u7684\u4e8c\u9636\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u68af\u5ea6\u7b97\u5b50\u5b9e\u73b0\u62df\u725b\u987f\u4f18\u5316\uff0c\u5728\u4ee3\u7801\u4f18\u5316\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u4e00\u9636\u65b9\u6cd5", "motivation": "\u73b0\u6709\u79bb\u6563\u6587\u672c\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u662f\u4e00\u9636\u4f18\u5316\u5668\uff08\u7c7b\u4f3cSGD\uff09\uff0c\u5b58\u5728\u6536\u655b\u6162\u548c\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u5ffd\u7565\u4e86\u4f18\u5316\u666f\u89c2\u7684\u8bed\u4e49\u66f2\u7387", "method": "TextBFGS\u662f\u4e8c\u9636\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u9884\u5b66\u4e60\u6210\u529f\u8f68\u8ff9\u4e2d\u68c0\u7d22\u68af\u5ea6\u7b97\u5b50\u6765\u8fd1\u4f3c\u9006Hessian\u77e9\u9635\uff0c\u5b9e\u73b0\u5355\u6b21\u66f4\u65b0\uff08One-Pass Update\uff09\uff0c\u5c06\u53cd\u9988\u751f\u6210\u548c\u4e8c\u9636\u6821\u6b63\u7ed3\u5408\u5230\u5355\u4e2a\u63a8\u7406\u6b65\u9aa4\u4e2d", "result": "\u5728\u4ee3\u7801\u4f18\u5316\u4efb\u52a1\uff08HumanEval\u3001MBPP\u7b49\uff09\u4e0a\uff0cTextBFGS\u663e\u8457\u4f18\u4e8e\u4e00\u9636\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ee5\u66f4\u5c11\u7684\u6a21\u578b\u8c03\u7528\u5b9e\u73b0\u66f4\u9ad8\u7684\u901a\u8fc7\u7387\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u53ef\u8fc1\u79fb\u6027", "conclusion": "TextBFGS\u4e3a\u9ad8\u6548\u3001\u5185\u5b58\u611f\u77e5\u7684\u6587\u672c\u4f18\u5316\u5efa\u7acb\u4e86\u6570\u5b66\u57fa\u7840\u8303\u5f0f\uff0c\u901a\u8fc7\u4e8c\u9636\u4f18\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u79bb\u6563\u6587\u672c\u4f18\u5316\u7684\u6536\u655b\u548c\u7a33\u5b9a\u6027\u95ee\u9898", "topic": "code agent"}}
{"id": "2602.00359", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00359", "abs": "https://arxiv.org/abs/2602.00359", "authors": ["Minhua Lin", "Hanqing Lu", "Zhan Shi", "Bing He", "Rui Mao", "Zhiwei Zhang", "Zongyu Wu", "Xianfeng Tang", "Hui Liu", "Zhenwei Dai", "Xiang Zhang", "Suhang Wang", "Benoit Dumoulin", "Jian Pei"], "title": "Position: Agentic Evolution is the Path to Evolving LLMs", "comment": null, "summary": "As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLLMs\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5b58\u5728\u8bad\u7ec3-\u90e8\u7f72\u5dee\u8ddd\uff0c\u9700\u8981\u5f15\u5165\"\u8fdb\u5316\"\u4f5c\u4e3a\u65b0\u7684\u6269\u5c55\u8f74\uff0c\u5e76\u63d0\u51faA-Evolve\u6846\u67b6\u5c06\u90e8\u7f72\u65f6\u6539\u8fdb\u89c6\u4e3a\u5bf9\u6301\u4e45\u7cfb\u7edf\u72b6\u6001\u7684\u6709\u76ee\u6807\u4f18\u5316\u8fc7\u7a0b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u7cbe\u5fc3\u7b56\u5212\u7684\u8bad\u7ec3\u96c6\u8f6c\u5411\u5f00\u653e\u7684\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u65f6\uff0c\u9759\u6001\u8bad\u7ec3\u65e0\u6cd5\u8ddf\u4e0a\u6301\u7eed\u90e8\u7f72\u73af\u5883\u7684\u53d8\u5316\u3002\u73b0\u6709\u7684\u90e8\u7f72\u65f6\u9002\u5e94\u65b9\u6cd5\uff08\u53c2\u6570\u5fae\u8c03\u6216\u542f\u53d1\u5f0f\u8bb0\u5fc6\u79ef\u7d2f\uff09\u7f3a\u4e4f\u8bca\u65ad\u5931\u8d25\u548c\u4ea7\u751f\u6301\u4e45\u6539\u8fdb\u7684\u6218\u7565\u80fd\u529b\u3002", "method": "\u63d0\u51faA-Evolve\u6846\u67b6\uff0c\u5c06\u90e8\u7f72\u65f6\u6539\u8fdb\u89c6\u4e3a\u5bf9\u6301\u4e45\u7cfb\u7edf\u72b6\u6001\u7684\u6709\u76ee\u6807\u4f18\u5316\u8fc7\u7a0b\uff0c\u5c06\u8fdb\u5316\u672c\u8eab\u4ece\u56fa\u5b9a\u6d41\u7a0b\u63d0\u5347\u4e3a\u81ea\u4e3b\u8fdb\u5316\u4ee3\u7406\u3002", "result": "\u63d0\u51fa\u8fdb\u5316\u6269\u5c55\u5047\u8bbe\uff1a\u9002\u5e94\u80fd\u529b\u968f\u7740\u5206\u914d\u7ed9\u8fdb\u5316\u7684\u8ba1\u7b97\u8d44\u6e90\u800c\u6269\u5c55\uff0c\u5c06\u4ee3\u7406\u8fdb\u5316\u5b9a\u4f4d\u4e3a\u5b9e\u73b0\u6301\u7eed\u3001\u5f00\u653e\u4e16\u754c\u9002\u5e94\u7684\u53ef\u6269\u5c55\u8def\u5f84\u3002", "conclusion": "\u4ee3\u7406\u8fdb\u5316\u4ee3\u8868\u4e86LLM\u9002\u5e94\u7684\u5fc5\u7136\u672a\u6765\uff0c\u662f\u89e3\u51b3\u8bad\u7ec3-\u90e8\u7f72\u5dee\u8ddd\u7684\u5173\u952e\u65b9\u5411\uff0c\u901a\u8fc7\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u5b9e\u73b0\u6301\u7eed\u9002\u5e94\u80fd\u529b\u7684\u6269\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2602.00400", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00400", "abs": "https://arxiv.org/abs/2602.00400", "authors": ["Fan Yang", "Rui Meng", "Trudi Di Qi", "Ali Ezzati", "Yuxin Wen"], "title": "KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.", "AI": {"tldr": "KEPO\uff1a\u4e00\u79cd\u7ed3\u5408\u8d28\u91cf\u95e8\u63a7\u84b8\u998f\u548c\u77e5\u8bc6\u589e\u5f3a\u63a2\u7d22\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u5931\u8d25\u548c\u68af\u5ea6\u566a\u58f0\u95ee\u9898", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5b58\u5728\u8f68\u8ff9\u7ea7\u7a00\u758f\u5956\u52b1\u5bfc\u81f4\u7684\u4fe1\u7528\u5206\u914d\u6a21\u7cca\u548c\u63a2\u7d22\u5931\u8d25\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u5747\u5300\u84b8\u998f\u65b9\u6cd5\u4f1a\u5728\u4f4e\u8d28\u91cf\u8f68\u8ff9\u4e2d\u6ce8\u5165\u566a\u58f0\u68af\u5ea6", "method": "\u63d0\u51faKEPO\u6846\u67b6\uff0c\u5305\u542b\uff1a(1) \u8d28\u91cf\u95e8\u63a7\u7684\u5728\u7ebf\u84b8\u998f\u76ee\u6807\uff0c\u4ec5\u5bf9\u9ad8\u8d28\u91cf\u8f68\u8ff9\u5e94\u7528\u5bc6\u96c6\u6559\u5e08\u6307\u5bfc\uff1b(2) \u77e5\u8bc6\u589e\u5f3a\u63a2\u7d22\u7b56\u7565\uff0c\u5229\u7528\u6559\u5e08\u6a21\u578b\u5b66\u4e60\u7684\u63d0\u793a\u6765\u62d2\u7edd\u91c7\u6837\u5956\u52b1\u6b63\u9762\u7684\u5728\u7ebf\u8f68\u8ff9", "result": "\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKEPO\u76f8\u6bd4\u5f3a\u5316\u5b66\u4e60\u548c\u5728\u7ebf\u84b8\u998f\u57fa\u7ebf\uff0c\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u66f4\u4e00\u81f4\u7684\u63a8\u7406\u884c\u4e3a\u548c\u66f4\u4f18\u8d8a\u7684\u5206\u5e03\u5916\u6027\u80fd", "conclusion": "KEPO\u901a\u8fc7\u9009\u62e9\u6027\u84b8\u998f\u548c\u77e5\u8bc6\u5f15\u5bfc\u7684\u63a2\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u548c\u6709\u6548\u7684\u8bad\u7ec3", "topic": "agentic reinforcement learning"}}
{"id": "2602.01107", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01107", "abs": "https://arxiv.org/abs/2602.01107", "authors": ["Daniel Ramos", "Catarina Gamboa", "In\u00eas Lynce", "Vasco Manquinho", "Ruben Martins", "Claire Le Goues"], "title": "SPELL: Synthesis of Programmatic Edits using LLMs", "comment": "pre-print", "summary": "Library migration is a common but error-prone task in software development. Developers may need to replace one library with another due to reasons like changing requirements or licensing changes. Migration typically entails updating and rewriting source code manually. While automated migration tools exist, most rely on mining examples from real-world projects that have already undergone similar migrations. However, these data are scarce, and collecting them for arbitrary pairs of libraries is difficult. Moreover, these migration tools often miss out on leveraging modern code transformation infrastructure.\n  In this paper, we present a new approach to automated API migration that sidesteps the limitations described above. Instead of relying on existing migration data or using LLMs directly for transformation, we use LLMs to extract migration examples. Next, we use an Agent to generalize those examples to reusable transformation scripts in PolyglotPiranha, a modern code transformation tool. Our method distills latent migration knowledge from LLMs into structured, testable, and repeatable migration logic, without requiring preexisting corpora or manual engineering effort. Experimental results across Python libraries show that our system can generate diverse migration examples and synthesize transformation scripts that generalize to real-world codebases.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u5316API\u8fc1\u79fb\u65b9\u6cd5\uff0c\u4f7f\u7528LLMs\u63d0\u53d6\u8fc1\u79fb\u793a\u4f8b\uff0c\u7136\u540e\u901a\u8fc7Agent\u5c06\u5176\u6cdb\u5316\u4e3aPolyglotPiranha\u4e2d\u7684\u53ef\u91cd\u7528\u8f6c\u6362\u811a\u672c\uff0c\u65e0\u9700\u4f9d\u8d56\u73b0\u6709\u8fc1\u79fb\u6570\u636e\u3002", "motivation": "\u5e93\u8fc1\u79fb\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e38\u89c1\u4f46\u6613\u51fa\u9519\u7684\u4efb\u52a1\u3002\u73b0\u6709\u81ea\u52a8\u5316\u8fc1\u79fb\u5de5\u5177\u5927\u591a\u4f9d\u8d56\u4ece\u5df2\u5b8c\u6210\u7c7b\u4f3c\u8fc1\u79fb\u7684\u771f\u5b9e\u9879\u76ee\u4e2d\u6316\u6398\u793a\u4f8b\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u7a00\u7f3a\u4e14\u96be\u4ee5\u6536\u96c6\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u73b0\u4ee3\u4ee3\u7801\u8f6c\u6362\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u4f7f\u7528LLMs\u63d0\u53d6\u8fc1\u79fb\u793a\u4f8b\uff0c\u7136\u540e\u901a\u8fc7Agent\u5c06\u8fd9\u4e9b\u793a\u4f8b\u6cdb\u5316\u4e3aPolyglotPiranha\u4e2d\u7684\u53ef\u91cd\u7528\u8f6c\u6362\u811a\u672c\u3002\u8be5\u65b9\u6cd5\u5c06LLMs\u4e2d\u7684\u6f5c\u5728\u8fc1\u79fb\u77e5\u8bc6\u63d0\u70bc\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u6d4b\u8bd5\u3001\u53ef\u91cd\u590d\u7684\u8fc1\u79fb\u903b\u8f91\u3002", "result": "\u5728Python\u5e93\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u8fc1\u79fb\u793a\u4f8b\uff0c\u5e76\u5408\u6210\u80fd\u591f\u6cdb\u5316\u5230\u771f\u5b9e\u4ee3\u7801\u5e93\u7684\u8f6c\u6362\u811a\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u7ed5\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u65e0\u9700\u9884\u5148\u5b58\u5728\u7684\u8bed\u6599\u5e93\u6216\u624b\u52a8\u5de5\u7a0b\u52aa\u529b\uff0c\u5c31\u80fd\u5b9e\u73b0\u81ea\u52a8\u5316API\u8fc1\u79fb\u3002", "topic": "code agent"}}
{"id": "2602.00352", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00352", "abs": "https://arxiv.org/abs/2602.00352", "authors": ["Li Siyan", "Darshan Deshpande", "Anand Kannappan", "Rebecca Qian"], "title": "DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning", "comment": null, "summary": "When recalling information in conversation, people often arrive at the recollection after multiple turns. However, existing benchmarks for evaluating agent capabilities in such tip-of-the-tongue search processes are restricted to single-turn settings. To more realistically simulate tip-of-the-tongue search, we introduce Dual-agent based Evaluation Through Obscure Under-specified Retrieval (DETOUR), a dual-agent evaluation benchmark containing 1,011 prompts. The benchmark design involves a Primary Agent, which is the subject of evaluation, tasked with identifying the recollected entity through querying a Memory Agent that is held consistent across evaluations. Our results indicate that current state-of-the-art models still struggle with our benchmark, only achieving 36% accuracy when evaluated on all modalities (text, image, audio, and video), highlighting the importance of enhancing capabilities in underspecified scenarios.", "AI": {"tldr": "DETOUR\u662f\u4e00\u4e2a\u53cc\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u6a21\u62df\"\u820c\u5c16\u73b0\u8c61\"\u7684\u591a\u8f6e\u4fe1\u606f\u68c0\u7d22\u8fc7\u7a0b\uff0c\u5305\u542b1011\u4e2a\u63d0\u793a\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u6a21\u7cca\u3001\u672a\u660e\u786e\u6307\u5b9a\u573a\u666f\u4e0b\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4ee3\u7406\u5728\"\u820c\u5c16\u73b0\u8c61\"\u641c\u7d22\u8fc7\u7a0b\u4e2d\u80fd\u529b\u7684\u57fa\u51c6\u4ec5\u9650\u4e8e\u5355\u8f6e\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u771f\u5b9e\u6a21\u62df\u4eba\u4eec\u5728\u5bf9\u8bdd\u4e2d\u7ecf\u8fc7\u591a\u8f6e\u4ea4\u4e92\u624d\u80fd\u56de\u5fc6\u8d77\u4fe1\u606f\u7684\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165DETOUR\u53cc\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b\u4e00\u4e2a\u88ab\u8bc4\u4f30\u7684\u4e3b\u8981\u4ee3\u7406\uff08Primary Agent\uff09\u548c\u4e00\u4e2a\u4fdd\u6301\u4e00\u81f4\u7684\u8bb0\u5fc6\u4ee3\u7406\uff08Memory Agent\uff09\u3002\u4e3b\u8981\u4ee3\u7406\u9700\u8981\u901a\u8fc7\u67e5\u8be2\u8bb0\u5fc6\u4ee3\u7406\u6765\u8bc6\u522b\u56de\u5fc6\u7684\u5b9e\u4f53\uff0c\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u591a\u79cd\u6a21\u6001\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u4ecd\u7136\u56f0\u96be\uff0c\u5728\u6240\u6709\u6a21\u6001\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\uff09\u4e0a\u4ec5\u8fbe\u523036%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u672a\u660e\u786e\u6307\u5b9a\u573a\u666f\u4e0b\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u7684\u91cd\u8981\u6027\uff0cDETOUR\u57fa\u51c6\u4e3a\u8bc4\u4f30\u591a\u8f6e\"\u820c\u5c16\u73b0\u8c61\"\u641c\u7d22\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u6d4b\u8bd5\u73af\u5883\u3002", "topic": "agent analysis"}}
{"id": "2602.01187", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01187", "abs": "https://arxiv.org/abs/2602.01187", "authors": ["Chengran Yang", "Zichao Wei", "Heminghao Deng", "Jinfeng Jiang", "Zhensu Sun", "Ting Zhang", "Tianyi Wu", "Ming Wen", "David Lo"], "title": "Autoregressive, Yet Revisable: In Decoding Revision for Secure Code Generation", "comment": null, "summary": "Large Language Model (LLM) based code generation is predominantly formulated as a strictly monotonic process, appending tokens linearly to an immutable prefix. This formulation contrasts to the cognitive process of programming, which is inherently interleaved with forward generation and on-the-fly revision. While prior works attempt to introduce revision via post-hoc agents or external static tools, they either suffer from high latency or fail to leverage the model's intrinsic semantic reasoning. In this paper, we propose Stream of Revision, a paradigm shift that elevates code generation from a monotonic stream to a dynamic, self-correcting trajectory by leveraging model's intrinsic capabilities. We introduce specific action tokens that enable the model to seamlessly backtrack and edit its own history within a single forward pass. By internalizing the revision loop, our framework Stream of Revision allows the model to activate its latent capabilities just-in-time without external dependencies. Empirical results on secure code generation show that Stream of Revision significantly reduces vulnerabilities with minimal inference overhead.", "AI": {"tldr": "Stream of Revision \u662f\u4e00\u79cd\u65b0\u7684\u4ee3\u7801\u751f\u6210\u8303\u5f0f\uff0c\u901a\u8fc7\u5f15\u5165\u7279\u5b9a\u52a8\u4f5c\u6807\u8bb0\u8ba9LLM\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u56de\u6eaf\u548c\u7f16\u8f91\u5386\u53f2\u8f93\u51fa\uff0c\u5c06\u5355\u8c03\u7684\u4ee3\u7801\u751f\u6210\u8f6c\u53d8\u4e3a\u52a8\u6001\u81ea\u4fee\u6b63\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u662f\u5355\u8c03\u7684\u7ebf\u6027\u8ffd\u52a0\u8fc7\u7a0b\uff0c\u4e0e\u4eba\u7c7b\u7f16\u7a0b\u65f6\u4e0d\u65ad\u524d\u5411\u751f\u6210\u548c\u5373\u65f6\u4fee\u8ba2\u7684\u8ba4\u77e5\u8fc7\u7a0b\u4e0d\u7b26\u3002\u73b0\u6709\u4fee\u8ba2\u65b9\u6cd5\u8981\u4e48\u5ef6\u8fdf\u9ad8\uff0c\u8981\u4e48\u65e0\u6cd5\u5229\u7528\u6a21\u578b\u5185\u5728\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faStream of Revision\u8303\u5f0f\uff0c\u5f15\u5165\u7279\u5b9a\u52a8\u4f5c\u6807\u8bb0\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u56de\u6eaf\u548c\u7f16\u8f91\u81ea\u5df1\u7684\u5386\u53f2\u8f93\u51fa\uff0c\u5c06\u4fee\u8ba2\u5faa\u73af\u5185\u5316\u5230\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u5916\u90e8\u4f9d\u8d56\u3002", "result": "\u5728\u5b89\u5168\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cStream of Revision\u80fd\u663e\u8457\u51cf\u5c11\u6f0f\u6d1e\uff0c\u4e14\u63a8\u7406\u5f00\u9500\u6700\u5c0f\u3002", "conclusion": "Stream of Revision\u901a\u8fc7\u5c06\u4ee3\u7801\u751f\u6210\u4ece\u5355\u8c03\u6d41\u63d0\u5347\u4e3a\u52a8\u6001\u81ea\u4fee\u6b63\u8f68\u8ff9\uff0c\u5229\u7528\u6a21\u578b\u5185\u5728\u80fd\u529b\u5b9e\u73b0\u5373\u65f6\u6fc0\u6d3b\uff0c\u662f\u4ee3\u7801\u751f\u6210\u8303\u5f0f\u7684\u91cd\u5927\u8f6c\u53d8\u3002", "topic": "code agent"}}
{"id": "2602.00415", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00415", "abs": "https://arxiv.org/abs/2602.00415", "authors": ["Zhisheng Chen", "Tingyu Wu", "Zijie Zhou", "Zhengwei Xie", "Ziyan Weng", "Yingwei Zhang"], "title": "PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents", "comment": null, "summary": "As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.", "AI": {"tldr": "PolarMem\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u6781\u5316\u56fe\u62d3\u6251\u548c\u903b\u8f91\u4e3b\u5bfc\u68c0\u7d22\uff0c\u5c06\u6a21\u7cca\u611f\u77e5\u8f6c\u6362\u4e3a\u53ef\u9a8c\u8bc1\u7684\u903b\u8f91\u7ea6\u675f\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u667a\u80fd\u4f53\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u667a\u80fd\u4f53\u4ece\u88ab\u52a8\u89c2\u5bdf\u8005\u53d1\u5c55\u4e3a\u957f\u671f\u51b3\u7b56\u8005\uff0c\u9700\u8981\u5177\u6709\u903b\u8f91\u53ef\u9a8c\u8bc1\u6027\u7684\u8bb0\u5fc6\u7cfb\u7edf\u3002\u73b0\u6709\u67b6\u6784\u5b58\u5728\u8ba4\u77e5\u4e0d\u5bf9\u79f0\u95ee\u9898\uff1a\u6982\u7387\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5bc6\u96c6\u5173\u8054\u8bb0\u5fc6\u5c06\u8bed\u4e49\u4eb2\u548c\u6027\u4e0e\u4e8b\u5b9e\u5b58\u5728\u6df7\u4e3a\u4e00\u8c08\uff0c\u4e14\u65e0\u6cd5\u7f16\u7801\u5426\u5b9a\u7ea6\u675f\u3002", "method": "\u63d0\u51faPolarMem\uff08\u6781\u5316\u6f5c\u5728\u56fe\u8bb0\u5fc6\uff09\uff0c\u901a\u8fc7\u975e\u53c2\u6570\u5206\u5e03\u5212\u5206\u5c06\u6a21\u7cca\u611f\u77e5\u4f3c\u7136\u8f6c\u6362\u4e3a\u79bb\u6563\u903b\u8f91\u7ea6\u675f\u3002\u91c7\u7528\u6781\u5316\u56fe\u62d3\u6251\u7ed3\u6784\uff0c\u4f7f\u7528\u6b63\u4ea4\u6291\u5236\u8fde\u63a5\u663e\u5f0f\u5b58\u50a8\u5df2\u9a8c\u8bc1\u7684\u5426\u5b9a\u4f5c\u4e3a\u4e3b\u8981\u8ba4\u77e5\u72b6\u6001\u3002\u5728\u63a8\u7406\u65f6\u91c7\u7528\u903b\u8f91\u4e3b\u5bfc\u68c0\u7d22\u8303\u5f0f\uff0c\u6291\u5236\u8fdd\u53cd\u5426\u5b9a\u7ea6\u675f\u7684\u5e7b\u89c9\u6a21\u5f0f\u3002", "result": "\u57288\u4e2a\u51bb\u7ed3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c6\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8bc1\u660ePolarMem\u4f5c\u4e3a\u7a33\u5065\u7684\u8ba4\u77e5\u7cfb\u7edf\uff0c\u4e3a\u53ef\u9a8c\u8bc1\u591a\u6a21\u6001\u667a\u80fd\u4f53\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "PolarMem\u901a\u8fc7\u5c06\u6a21\u7cca\u611f\u77e5\u8f6c\u6362\u4e3a\u53ef\u9a8c\u8bc1\u903b\u8f91\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u667a\u80fd\u4f53\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u53ef\u9a8c\u8bc1\u591a\u6a21\u6001\u667a\u80fd\u4f53\u5efa\u7acb\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2602.00454", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00454", "abs": "https://arxiv.org/abs/2602.00454", "authors": ["Jing Wu", "Yue Sun", "Tianpei Xie", "Suiyao Chen", "Jingyuan Bao", "Yaopengxiao Xu", "Gaoyuan Du", "Inseok Heo", "Alexander Gutfraind", "Xin Wang"], "title": "Cross-Modal Memory Compression for Efficient Multi-Agent Debate", "comment": null, "summary": "Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.", "AI": {"tldr": "DebateOCR\uff1a\u901a\u8fc7\u56fe\u50cf\u538b\u7f29\u66ff\u4ee3\u957f\u6587\u672c\u8fa9\u8bba\u5386\u53f2\uff0c\u51cf\u5c1192%\u7684token\u4f7f\u7528\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u52a0\u901f\u63a8\u7406", "motivation": "\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u867d\u7136\u80fd\u63d0\u9ad8\u63a8\u7406\u8d28\u91cf\uff0c\u4f46\u968f\u7740\u8fa9\u8bba\u8f6e\u6b21\u548c\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\uff0c\u4e0a\u4e0b\u6587\u4f1a\u8fc5\u901f\u81a8\u80c0\uff0c\u5bfc\u81f4token\u4f7f\u7528\u8d85\u51fa\u9650\u5236\uff0c\u9700\u8981\u91cd\u590d\u6458\u8981\u5316\u5904\u7406\uff0c\u589e\u52a0\u5f00\u9500\u5e76\u9020\u6210\u4fe1\u606f\u635f\u5931", "method": "\u63d0\u51faDebateOCR\u8de8\u6a21\u6001\u538b\u7f29\u6846\u67b6\uff0c\u7528\u7d27\u51d1\u7684\u56fe\u50cf\u8868\u793a\u66ff\u4ee3\u957f\u6587\u672c\u8fa9\u8bba\u8f68\u8ff9\uff0c\u901a\u8fc7\u4e13\u7528\u89c6\u89c9\u7f16\u7801\u5668\u5904\u7406\u8fd9\u4e9b\u56fe\u50cf\u8868\u793a\u6765\u8c03\u8282\u540e\u7eed\u8fa9\u8bba\u8f6e\u6b21", "result": "\u5c06\u901a\u5e38\u5305\u542b\u6570\u4e07\u5230\u6570\u5341\u4e07token\u7684\u5386\u53f2\u538b\u7f29\uff0c\u51cf\u5c1192%\u4ee5\u4e0a\u7684\u8f93\u5165token\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u52a0\u901f\u63a8\u7406", "conclusion": "\u901a\u8fc7\u56fe\u50cf\u538b\u7f29\u6709\u6548\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u7684\u4e0a\u4e0b\u6587\u81a8\u80c0\u95ee\u9898\uff0c\u540c\u65f6\u7406\u8bba\u5206\u6790\u8868\u660e\u667a\u80fd\u4f53\u591a\u6837\u6027\u652f\u6301\u6062\u590d\u88ab\u7701\u7565\u7684\u4fe1\u606f\uff0c\u591a\u4e2a\u667a\u80fd\u4f53\u7684\u538b\u7f29\u89c6\u56fe\u805a\u5408\u80fd\u591f\u4ee5\u6307\u6570\u7ea7\u9ad8\u6982\u7387\u63a5\u8fd1\u4fe1\u606f\u74f6\u9888", "topic": "agent analysis"}}
{"id": "2602.00428", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.00428", "abs": "https://arxiv.org/abs/2602.00428", "authors": ["Naen Xu", "Hengyu An", "Shuo Shi", "Jinghuai Zhang", "Chunyi Zhou", "Changjiang Li", "Tianyu Du", "Zhihui Fu", "Jun Wang", "Shouling Ji"], "title": "When Agents \"Misremember\" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems", "comment": "ICLR 2026", "summary": "Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u66fc\u5fb7\u62c9\u6548\u5e94\uff08\u96c6\u4f53\u8bb0\u5fc6\u504f\u5dee\uff09\uff0c\u63d0\u51fa\u4e86MANBENCH\u57fa\u51c6\u6765\u8bc4\u4f30\u8be5\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u7b56\u7565\uff0c\u5e73\u5747\u51cf\u5c1174.40%\u7684\u66fc\u5fb7\u62c9\u6548\u5e94\u3002", "motivation": "\u867d\u7136LLM\u589e\u5f3a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u4f46\u667a\u80fd\u4f53\u5bf9\u96c6\u4f53\u8ba4\u77e5\u504f\u5dee\u7684\u6613\u611f\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u66fc\u5fb7\u62c9\u6548\u5e94\u4f5c\u4e3a\u96c6\u4f53\u9519\u8bef\u8bb0\u5fc6\u73b0\u8c61\uff0c\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u53ef\u80fd\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\uff0c\u5b58\u5728\u4f26\u7406\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u5176\u5b58\u5728\u3001\u6210\u56e0\u548c\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMANBENCH\u57fa\u51c6\uff0c\u5305\u542b\u56db\u79cd\u6613\u53d7\u66fc\u5fb7\u62c9\u6548\u5e94\u5f71\u54cd\u7684\u4efb\u52a1\u7c7b\u578b\u548c\u4e94\u79cd\u4e0d\u540c\u667a\u80fd\u4f53\u89d2\u8272\u4e0e\u8bb0\u5fc6\u65f6\u95f4\u5c3a\u5ea6\u7684\u4ea4\u4e92\u534f\u8bae\u3002\u8bc4\u4f30\u591a\u4e2aLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u91cf\u5316\u66fc\u5fb7\u62c9\u6548\u5e94\u5e76\u5206\u6790\u5f71\u54cd\u56e0\u7d20\u3002\u63d0\u51fa\u7f13\u89e3\u7b56\u7565\uff1a\u63d0\u793a\u7ea7\u9632\u5fa1\uff08\u8ba4\u77e5\u951a\u5b9a\u3001\u6765\u6e90\u5ba1\u67e5\uff09\u548c\u6a21\u578b\u7ea7\u57fa\u4e8e\u5bf9\u9f50\u7684\u9632\u5fa1\u3002", "result": "\u5728MANBENCH\u4e0a\u8bc4\u4f30\u591a\u4e2aLLM\uff0c\u91cf\u5316\u4e86\u66fc\u5fb7\u62c9\u6548\u5e94\u7684\u5b58\u5728\u3002\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\u5e73\u5747\u51cf\u5c1174.40%\u7684\u66fc\u5fb7\u62c9\u6548\u5e94\uff08\u76f8\u6bd4\u57fa\u7ebf\uff09\u3002\u5206\u6790\u663e\u793a\u4e0d\u540c\u56e0\u7d20\uff08\u5982\u4ea4\u4e92\u534f\u8bae\u3001\u4efb\u52a1\u7c7b\u578b\uff09\u5bf9\u66fc\u5fb7\u62c9\u6548\u5e94\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u786e\u5b9e\u5b58\u5728\u66fc\u5fb7\u62c9\u6548\u5e94\uff0c\u9700\u8981\u5173\u6ce8\u5176\u4f26\u7406\u5f71\u54cd\u3002\u63d0\u51fa\u7684MANBENCH\u57fa\u51c6\u548c\u7f13\u89e3\u7b56\u7565\u6709\u6548\u51cf\u5c11\u4e86\u8be5\u6548\u5e94\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u97e7\u6027\u548c\u4f26\u7406\u5bf9\u9f50\u7684\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2602.00456", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00456", "abs": "https://arxiv.org/abs/2602.00456", "authors": ["Amanda Dsouza", "Ramya Ramakrishnan", "Charles Dickens", "Bhavishya Pohani", "Christopher M Glaze"], "title": "Benchmarking Agents in Insurance Underwriting Environments", "comment": null, "summary": "As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an expert-first, multi-turn insurance underwriting benchmark designed in close collaboration with domain experts to capture real-world enterprise challenges. UNDERWRITE introduces critical realism factors often absent in current benchmarks: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. Evaluating 13 frontier models, we uncover significant gaps between research lab performance and enterprise readiness: the most accurate models are not the most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show a 20% drop in performance. The results from UNDERWRITE demonstrate that expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains demands compositional approaches. Our work provides insights for developing benchmarks that better align with enterprise deployment requirements.", "AI": {"tldr": "UNDERWRITE\u662f\u4e00\u4e2a\u4e13\u5bb6\u4e3b\u5bfc\u7684\u591a\u8f6e\u4fdd\u9669\u627f\u4fdd\u57fa\u51c6\uff0c\u901a\u8fc7\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\u8bbe\u8ba1\uff0c\u6355\u6349\u771f\u5b9e\u4f01\u4e1a\u6311\u6218\uff0c\u8bc4\u4f3013\u4e2a\u524d\u6cbf\u6a21\u578b\u53d1\u73b0\u7814\u7a76\u5b9e\u9a8c\u5ba4\u6027\u80fd\u4e0e\u4f01\u4e1a\u5c31\u7eea\u5ea6\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u8fc7\u5ea6\u5f3a\u8c03\u4ee3\u7801\u7b49\u5f00\u653e\u9886\u57df\uff0c\u4f7f\u7528\u72ed\u7a84\u7684\u51c6\u786e\u6027\u6307\u6807\uff0c\u7f3a\u4e4f\u771f\u5b9e\u590d\u6742\u6027\uff0c\u65e0\u6cd5\u53cd\u6620\u4f01\u4e1a\u5e94\u7528\u4e2dAI\u4ee3\u7406\u7684\u771f\u5b9e\u8bc4\u4f30\u9700\u6c42\u3002", "method": "\u4e0e\u9886\u57df\u4e13\u5bb6\u5bc6\u5207\u5408\u4f5c\u8bbe\u8ba1\u4e13\u5bb6\u4f18\u5148\u7684\u591a\u8f6e\u4fdd\u9669\u627f\u4fdd\u57fa\u51c6\uff0c\u5f15\u5165\u4e13\u6709\u4e1a\u52a1\u77e5\u8bc6\u3001\u566a\u58f0\u5de5\u5177\u63a5\u53e3\u548c\u4e0d\u5b8c\u7f8e\u7684\u6a21\u62df\u7528\u6237\u7b49\u5173\u952e\u73b0\u5b9e\u56e0\u7d20\u3002", "result": "\u8bc4\u4f3013\u4e2a\u524d\u6cbf\u6a21\u578b\u53d1\u73b0\uff1a\u6700\u51c6\u786e\u7684\u6a21\u578b\u4e0d\u662f\u6700\u9ad8\u6548\u7684\uff1b\u6a21\u578b\u5373\u4f7f\u6709\u5de5\u5177\u8bbf\u95ee\u4ecd\u4f1a\u5e7b\u89c9\u9886\u57df\u77e5\u8bc6\uff1bpass^k\u7ed3\u679c\u663e\u793a\u6027\u80fd\u4e0b\u964d20%\uff1b\u5e38\u89c1\u4ee3\u7406\u6846\u67b6\u5b58\u5728\u8106\u5f31\u6027\u5f71\u54cd\u6027\u80fd\u62a5\u544a\u3002", "conclusion": "\u4e13\u5bb6\u53c2\u4e0e\u57fa\u51c6\u8bbe\u8ba1\u5bf9\u73b0\u5b9e\u4ee3\u7406\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff1b\u5e38\u89c1\u4ee3\u7406\u6846\u67b6\u7684\u8106\u5f31\u6027\u4f1a\u626d\u66f2\u6027\u80fd\u62a5\u544a\uff1b\u4e13\u4e1a\u9886\u57df\u7684\u5e7b\u89c9\u68c0\u6d4b\u9700\u8981\u7ec4\u5408\u65b9\u6cd5\uff1b\u5de5\u4f5c\u4e3a\u5f00\u53d1\u66f4\u7b26\u5408\u4f01\u4e1a\u90e8\u7f72\u9700\u6c42\u7684\u57fa\u51c6\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2602.00471", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.00471", "abs": "https://arxiv.org/abs/2602.00471", "authors": ["Xinlei Yu", "Chengming Xu", "Zhangquan Chen", "Bo Yin", "Cheng Yang", "Yongbo He", "Yihao Hu", "Jiangning Zhang", "Cheng Tan", "Xiaobin Hu", "Shuicheng Yan"], "title": "Dual Latent Memory for Visual Multi-agent System", "comment": null, "summary": "While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive \"scaling wall\": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the \"scaling wall\" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.", "AI": {"tldr": "L\u00b2-VMAS\u63d0\u51fa\u53cc\u6f5c\u5728\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u611f\u77e5\u4e0e\u601d\u8003\u3001\u52a8\u6001\u5408\u6210\u53cc\u8bb0\u5fc6\u4ee5\u53ca\u71b5\u9a71\u52a8\u4e3b\u52a8\u89e6\u53d1\u673a\u5236\uff0c\u89e3\u51b3\u89c6\u89c9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\"\u6269\u5c55\u5899\"\u95ee\u9898\uff0c\u5728\u63d0\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4ee4\u724c\u4f7f\u7528\u91cf\u3002", "motivation": "\u89c6\u89c9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf(VMAS)\u901a\u8fc7\u667a\u80fd\u4f53\u534f\u4f5c\u63d0\u5347\u7efc\u5408\u80fd\u529b\uff0c\u4f46\u5b9e\u8bc1\u53d1\u73b0\u5b58\u5728\u53cd\u76f4\u89c9\u7684\"\u6269\u5c55\u5899\"\u73b0\u8c61\uff1a\u589e\u52a0\u667a\u80fd\u4f53\u8f6e\u6b21\u53cd\u800c\u964d\u4f4e\u6027\u80fd\u5e76\u6307\u6570\u7ea7\u589e\u52a0\u4ee4\u724c\u6210\u672c\u3002\u8fd9\u5f52\u56e0\u4e8e\u6587\u672c\u4e2d\u5fc3\u901a\u4fe1\u7684\u4fe1\u606f\u74f6\u9888\uff0c\u5c06\u611f\u77e5\u548c\u601d\u8003\u8f68\u8ff9\u8f6c\u6362\u4e3a\u79bb\u6563\u81ea\u7136\u8bed\u8a00\u4f1a\u5bfc\u81f4\u8bed\u4e49\u635f\u5931\u3002", "method": "\u63d0\u51faL\u00b2-VMAS\u6846\u67b6\uff1a1) \u53cc\u6f5c\u5728\u8bb0\u5fc6\u5b9e\u73b0\u667a\u80fd\u4f53\u95f4\u534f\u4f5c\uff1b2) \u89e3\u8026\u611f\u77e5\u4e0e\u601d\u8003\u8fc7\u7a0b\uff1b3) \u52a8\u6001\u5408\u6210\u53cc\u6f5c\u5728\u8bb0\u5fc6\uff1b4) \u71b5\u9a71\u52a8\u4e3b\u52a8\u89e6\u53d1\u673a\u5236\uff0c\u7528\u6309\u9700\u5185\u5b58\u8bbf\u95ee\u66ff\u4ee3\u88ab\u52a8\u4fe1\u606f\u4f20\u8f93\u3002", "result": "\u5728\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u3001\u6a21\u578b\u5927\u5c0f\u548c\u591a\u667a\u80fd\u4f53\u7ed3\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u6253\u7834\"\u6269\u5c55\u5899\"\uff0c\u5177\u6709\u4f18\u5f02\u7684\u53ef\u6269\u5c55\u6027\uff1a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53472.7-5.4%\uff0c\u540c\u65f6\u4ee4\u724c\u4f7f\u7528\u91cf\u51cf\u5c1121.3-44.8%\u3002", "conclusion": "L\u00b2-VMAS\u901a\u8fc7\u53cc\u6f5c\u5728\u8bb0\u5fc6\u6846\u67b6\u89e3\u51b3\u4e86VMAS\u4e2d\u7684\u4fe1\u606f\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u4e0e\u6210\u672c\u964d\u4f4e\u7684\u53cc\u91cd\u4f18\u5316\uff0c\u4e3a\u89c6\u89c9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.02138", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02138", "abs": "https://arxiv.org/abs/2602.02138", "authors": ["Lyu Zongyi", "Ji Zhenlan", "Chen Songqiang", "Wang Liwen", "Huang Yuheng", "Wang Shuai", "Cheung Shing-Chi"], "title": "CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems", "comment": "18 pages, 12 tables, 4 figures", "summary": "Despite the remarkable success that Multi-Agent Code Generation Systems (MACGS) have achieved, the inherent complexity of multi-agent architectures produces substantial volumes of intermediate outputs. To date, the individual importance of these intermediate outputs to the system correctness remains opaque, which impedes targeted optimization of MACGS designs. To address this challenge, we propose CAM, the first \\textbf{C}ausality-based \\textbf{A}nalysis framework for \\textbf{M}ACGS that systematically quantifies the contribution of different intermediate features for system correctness. By comprehensively categorizing intermediate outputs and systematically simulating realistic errors on intermediate features, we identify the important features for system correctness and aggregate their importance rankings.\n  We conduct extensive empirical analysis on the identified importance rankings. Our analysis reveals intriguing findings: first, we uncover context-dependent features\\textemdash features whose importance emerges mainly through interactions with other features, revealing that quality assurance for MACGS should incorporate cross-feature consistency checks; second, we reveal that hybrid backend MACGS with different backend LLMs assigned according to their relative strength achieves up to 7.2\\% Pass@1 improvement, underscoring hybrid architectures as a promising direction for future MACGS design. We further demonstrate CAM's practical utility through two applications: (1) failure repair which achieves a 73.3\\% success rate by optimizing top-3 importance-ranked features and (2) feature pruning that reduces up to 66.8\\% intermediate token consumption while maintaining generation performance. Our work provides actionable insights for MACGS design and deployment, establishing causality analysis as a powerful approach for understanding and improving MACGS.", "AI": {"tldr": "CAM\uff1a\u9996\u4e2a\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u591a\u667a\u80fd\u4f53\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u4e2d\u95f4\u7279\u5f81\u5bf9\u7cfb\u7edf\u6b63\u786e\u6027\u7684\u8d21\u732e\uff0c\u8bc6\u522b\u91cd\u8981\u7279\u5f81\u5e76\u8fdb\u884c\u91cd\u8981\u6027\u6392\u5e8f\uff0c\u4e3aMACGS\u4f18\u5316\u63d0\u4f9b\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u4ea7\u751f\u5927\u91cf\u4e2d\u95f4\u8f93\u51fa\uff0c\u4f46\u8fd9\u4e9b\u4e2d\u95f4\u8f93\u51fa\u5bf9\u7cfb\u7edf\u6b63\u786e\u6027\u7684\u91cd\u8981\u6027\u5c1a\u4e0d\u660e\u786e\uff0c\u963b\u788d\u4e86MACGS\u7684\u9488\u5bf9\u6027\u4f18\u5316\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faCAM\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u5bf9\u4e2d\u95f4\u8f93\u51fa\u8fdb\u884c\u5206\u7c7b\uff0c\u6a21\u62df\u5b9e\u9645\u9519\u8bef\uff0c\u91cf\u5316\u4e0d\u540c\u4e2d\u95f4\u7279\u5f81\u5bf9\u7cfb\u7edf\u6b63\u786e\u6027\u7684\u8d21\u732e\uff0c\u5e76\u805a\u5408\u91cd\u8981\u6027\u6392\u540d\u3002", "result": "\u53d1\u73b0\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7279\u5f81\u7684\u91cd\u8981\u6027\uff0c\u63ed\u793a\u6df7\u5408\u540e\u7aefMACGS\u53ef\u83b7\u5f977.2%\u7684Pass@1\u63d0\u5347\uff1b\u901a\u8fc7\u4f18\u5316\u524d3\u91cd\u8981\u7279\u5f81\u5b9e\u73b073.3%\u7684\u4fee\u590d\u6210\u529f\u7387\uff0c\u7279\u5f81\u526a\u679d\u51cf\u5c1166.8%\u7684\u4e2d\u95f4token\u6d88\u8017\u3002", "conclusion": "CAM\u4e3aMACGS\u8bbe\u8ba1\u548c\u90e8\u7f72\u63d0\u4f9b\u53ef\u64cd\u4f5c\u89c1\u89e3\uff0c\u786e\u7acb\u56e0\u679c\u5206\u6790\u4f5c\u4e3a\u7406\u89e3\u548c\u6539\u8fdbMACGS\u7684\u5f3a\u5927\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.02235", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02235", "abs": "https://arxiv.org/abs/2602.02235", "authors": ["Zhaonan Wu", "Yanjie Zhao", "Zhenpeng Chen", "Zheng Wang", "Haoyu Wang"], "title": "Agent-Based Software Artifact Evaluation", "comment": null, "summary": "Artifact evaluation has been adopted in the Software Engineering (SE) research community for 15 years, substantially improving research reproducibility across major SE conferences. However, this success has introduced a growing scalability challenge, as artifact evaluation relies heavily on reviewers' manual execution and debugging, leading to escalating human effort amid rapidly increasing paper submissions. To address this problem, we investigate automated artifact evaluation. We first conduct a preliminary study on artifacts from top-tier SE conferences and identify three key challenges: perceiving execution states, maintaining stable execution environments, and recovering from execution errors. Inspired by these findings, we propose ArtifactCopilot, the first end-to-end agent-based framework for automated artifact evaluation. ArtifactCopilot automates environment construction, instruction execution, and error recovery by combining an execution normalization strategy to ensure environment stability with an artifact evaluation graph that transforms README documents into dependency-aware command graphs, enabling structured execution planning, execution-state tracking, and error recovery. Evaluation on 48 real-world artifacts shows that ArtifactCopilot matches human artifact evaluation outcomes for 85.42% of the artifacts, outperforming Claude Code by 52.09 percentage points, while costing only \\$0.091 per artifact on average and requiring zero human intervention for 45 out of 48 artifacts.", "AI": {"tldr": "ArtifactCopilot\uff1a\u9996\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u81ea\u52a8\u5316\u8f6f\u4ef6\u5de5\u7a0b\u5236\u54c1\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6267\u884c\u5f52\u4e00\u5316\u7b56\u7565\u548c\u5236\u54c1\u8bc4\u4f30\u56fe\u5b9e\u73b0\u73af\u5883\u6784\u5efa\u3001\u6307\u4ee4\u6267\u884c\u548c\u9519\u8bef\u6062\u590d\u7684\u81ea\u52a8\u5316\uff0c\u572848\u4e2a\u771f\u5b9e\u5236\u54c1\u4e0a\u8fbe\u523085.42%\u4e0e\u4eba\u5de5\u8bc4\u4f30\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "motivation": "\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u5236\u54c1\u8bc4\u4f30\u5df2\u5b9e\u65bd15\u5e74\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7814\u7a76\u53ef\u590d\u73b0\u6027\uff0c\u4f46\u9762\u4e34\u53ef\u6269\u5c55\u6027\u6311\u6218\uff1a\u4f9d\u8d56\u8bc4\u5ba1\u5458\u624b\u52a8\u6267\u884c\u548c\u8c03\u8bd5\uff0c\u968f\u7740\u8bba\u6587\u63d0\u4ea4\u91cf\u5feb\u901f\u589e\u957f\uff0c\u4eba\u529b\u6210\u672c\u6025\u5267\u4e0a\u5347\u3002", "method": "\u63d0\u51faArtifactCopilot\u7aef\u5230\u7aef\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u6267\u884c\u5f52\u4e00\u5316\u7b56\u7565\u786e\u4fdd\u73af\u5883\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u5236\u54c1\u8bc4\u4f30\u56fe\u5c06README\u6587\u6863\u8f6c\u6362\u4e3a\u4f9d\u8d56\u611f\u77e5\u7684\u547d\u4ee4\u56fe\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u6267\u884c\u89c4\u5212\u3001\u6267\u884c\u72b6\u6001\u8ddf\u8e2a\u548c\u9519\u8bef\u6062\u590d\u3002", "result": "\u572848\u4e2a\u771f\u5b9e\u5236\u54c1\u8bc4\u4f30\u4e2d\uff0cArtifactCopilot\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7ed3\u679c\u4e00\u81f4\u7387\u8fbe85.42%\uff0c\u6bd4Claude Code\u9ad8\u51fa52.09\u4e2a\u767e\u5206\u70b9\uff0c\u5e73\u5747\u6bcf\u4e2a\u5236\u54c1\u6210\u672c\u4ec50.091\u7f8e\u5143\uff0c48\u4e2a\u5236\u54c1\u4e2d\u670945\u4e2a\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "ArtifactCopilot\u80fd\u6709\u6548\u81ea\u52a8\u5316\u5236\u54c1\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u529b\u6210\u672c\uff0c\u4e3a\u89e3\u51b3\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u793e\u533a\u5236\u54c1\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2602.00510", "categories": ["cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00510", "abs": "https://arxiv.org/abs/2602.00510", "authors": ["Huanghaohe Zou", "Peng Han", "Emad Nazerian", "Alex Q. Huang"], "title": "PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)", "comment": null, "summary": "Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.", "AI": {"tldr": "PCBSchemaGen\u662f\u9996\u4e2a\u514d\u8bad\u7ec3\u7684PCB\u539f\u7406\u56fe\u8bbe\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408LLM\u4ee3\u7406\u548c\u7ea6\u675f\u5f15\u5bfc\u5408\u6210\uff0c\u80fd\u5904\u7406\u6570\u5b57\u3001\u6a21\u62df\u548c\u7535\u6e90\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u9ad8\u8bbe\u8ba1\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "PCB\u539f\u7406\u56fe\u8bbe\u8ba1\u5728\u7535\u5b50\u5de5\u4e1a\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u53ea\u5173\u6ce8\u6570\u5b57\u6216\u6a21\u62df\u7535\u8def\uff0c\u800cPCB\u8bbe\u8ba1\u9700\u8981\u5904\u7406\u5f02\u6784\u4fe1\u53f7\u5e76\u9075\u5faa\u5b9e\u9645IC\u5c01\u88c5\u548c\u5f15\u811a\u7ea6\u675f\u3002\u7531\u4e8e\u5f00\u6e90\u6570\u636e\u7a00\u7f3a\u4e14\u7f3a\u4e4f\u4eff\u771f\u9a8c\u8bc1\uff0c\u81ea\u52a8\u5316PCB\u539f\u7406\u56fe\u8bbe\u8ba1\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51faPCBSchemaGen\u6846\u67b6\uff1a1\uff09\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u8303\u5f0f\uff0c\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u63d0\u793a\u8fdb\u884c\u8fed\u4ee3\u53cd\u9988\uff1b2\uff09\u5229\u7528\u771f\u5b9eIC\u6570\u636e\u624b\u518c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u5b50\u56fe\u540c\u6784\u7f16\u7801\u5f15\u811a\u89d2\u8272\u8bed\u4e49\u548c\u62d3\u6251\u7ea6\u675f\u7684\u9a8c\u8bc1\u6846\u67b6\uff1b3\uff09\u572823\u4e2aPCB\u539f\u7406\u56fe\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "PCBSchemaGen\u663e\u8457\u63d0\u9ad8\u4e86\u8bbe\u8ba1\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u6570\u5b57\u3001\u6a21\u62df\u548c\u7535\u6e90\u9886\u57df\u768423\u4e2aPCB\u539f\u7406\u56fe\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "PCBSchemaGen\u662f\u9996\u4e2a\u63a2\u7d22\u81ea\u52a8\u5316PCB\u539f\u7406\u56fe\u8bbe\u8ba1\u7684\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5f02\u6784\u4fe1\u53f7\u5904\u7406\u548c\u5b9e\u9645\u7ea6\u675f\u95ee\u9898\uff0c\u4e3aPCB\u8bbe\u8ba1\u81ea\u52a8\u5316\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "topic": "code agent"}}
{"id": "2602.02262", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02262", "abs": "https://arxiv.org/abs/2602.02262", "authors": ["Atharv Sonwane", "Eng-Shen Tu", "Wei-Chung Lu", "Claas Beger", "Carter Larsen", "Debjit Dhar", "Rachel Chen", "Ronit Pattanayak", "Tuan Anh Dang", "Guohao Chen", "Gloria Geng", "Kevin Ellis", "Saikat Dutta"], "title": "OmniCode: A Benchmark for Evaluating Software Engineering Agents", "comment": null, "summary": "LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.", "AI": {"tldr": "OmniCode\u662f\u4e00\u4e2a\u65b0\u7684\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1794\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6Python\u3001Java\u3001C++\u4e09\u79cd\u8bed\u8a00\u548c\u56db\u4e2a\u5173\u952e\u7c7b\u522b\uff1abug\u4fee\u590d\u3001\u6d4b\u8bd5\u751f\u6210\u3001\u4ee3\u7801\u5ba1\u67e5\u4fee\u590d\u548c\u98ce\u683c\u4fee\u590d\uff0c\u65e8\u5728\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u7f16\u7801\u4ee3\u7406\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982HumanEval\u548cSWE-Bench\uff09\u4e3b\u8981\u5173\u6ce8\u7ade\u4e89\u7f16\u7a0b\u548c\u8865\u4e01\u751f\u6210\u7b49\u72ed\u7a84\u8303\u56f4\u7684\u4efb\u52a1\uff0c\u800c\u73b0\u5b9e\u4e2d\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u9700\u8981\u5904\u7406\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u3002\u9700\u8981\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u63a8\u52a8\u66f4\u597d\u7684\u7f16\u7801\u4ee3\u7406\u7814\u7a76\u3002", "method": "\u63d0\u51faOmniCode\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1794\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u4e09\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u56db\u4e2a\u5173\u952e\u7c7b\u522b\u3002\u4efb\u52a1\u7ecf\u8fc7\u624b\u52a8\u9a8c\u8bc1\u4ee5\u6d88\u9664\u5b9a\u4e49\u4e0d\u6e05\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u751f\u6210\u6216\u8fd1\u671f\u6574\u7406\u6765\u907f\u514d\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ece\u6709\u9650\u771f\u5b9e\u6570\u636e\u4e2d\u5408\u6210\u751f\u6210\u591a\u6837\u5316\u8f6f\u4ef6\u4efb\u52a1\u7684\u65b0\u6846\u67b6\u3002", "result": "\u4f7f\u7528SWE-Agent\u7b49\u6d41\u884c\u4ee3\u7406\u6846\u67b6\u8bc4\u4f30OmniCode\uff0c\u7ed3\u679c\u663e\u793a\u5b83\u4eec\u5728Python\u7684bug\u4fee\u590d\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6d4b\u8bd5\u751f\u6210\u4ee5\u53caC++\u548cJava\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002\u4f8b\u5982\uff0cSWE-Agent\u5728Java\u6d4b\u8bd5\u751f\u6210\u4efb\u52a1\u4e0a\u6700\u9ad8\u4ec5\u8fbe\u523020.9%\uff08\u4f7f\u7528DeepSeek-V3.1\uff09\u3002", "conclusion": "OmniCode\u65e8\u5728\u4f5c\u4e3a\u4e00\u4e2a\u7a33\u5065\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63a8\u52a8\u5f00\u53d1\u80fd\u591f\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e0d\u540c\u65b9\u9762\u8868\u73b0\u826f\u597d\u7684\u4ee3\u7406\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002", "topic": "swe benchmark"}}
{"id": "2602.00521", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00521", "abs": "https://arxiv.org/abs/2602.00521", "authors": ["Junhyuk Choi", "Sohhyung Park", "Chanhee Cho", "Hyeonchu Park", "Bugeun Kim"], "title": "Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory", "comment": "Under review", "summary": "While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u9879\u76ee\u53cd\u5e94\u7406\u8bba(IRT)\u7684\u4e24\u9636\u6bb5\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM-as-a-Judge\u7684\u53ef\u9760\u6027\uff0c\u5305\u62ec\u5185\u5728\u4e00\u81f4\u6027\u548c\u4eba\u7c7b\u5bf9\u9f50\u4e24\u4e2a\u7ef4\u5ea6\u3002", "motivation": "\u73b0\u6709LLM-as-a-Judge\u9a8c\u8bc1\u5b9e\u8df5\u4e3b\u8981\u505c\u7559\u5728\u89c2\u5bdf\u8f93\u51fa\u5c42\u9762\uff0c\u65e0\u6cd5\u6df1\u5165\u4e86\u89e3LLM\u8bc4\u5224\u662f\u5426\u4f5c\u4e3a\u7a33\u5b9a\u53ef\u9760\u7684\u6d4b\u91cf\u5de5\u5177\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9879\u76ee\u53cd\u5e94\u7406\u8bba(IRT)\u548c\u5206\u7ea7\u53cd\u5e94\u6a21\u578b(GRM)\uff0c\u6784\u5efa\u4e24\u9636\u6bb5\u8bca\u65ad\u6846\u67b6\uff0c\u4ece\u5185\u5728\u4e00\u81f4\u6027(\u63d0\u793a\u53d8\u5316\u4e0b\u7684\u7a33\u5b9a\u6027)\u548c\u4eba\u7c7b\u5bf9\u9f50(\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027)\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "result": "IRT-GRM\u6846\u67b6\u4e3aLLM\u8bc4\u5224\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u4fe1\u53f7\uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u8bca\u65ad\u8bc4\u5224\u884c\u4e3a\uff0c\u4e3a\u9a8c\u8bc1\u53ef\u9760\u6027\u548c\u8bc6\u522b\u4e0d\u53ef\u9760\u539f\u56e0\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u57fa\u4e8eIRT\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc4\u4f30LLM-as-a-Judge\u7684\u53ef\u9760\u6027\uff0c\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u5f25\u8865\u73b0\u6709\u9a8c\u8bc1\u5b9e\u8df5\u7684\u4e0d\u8db3\u3002", "topic": "agent analysis"}}
{"id": "2602.02280", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02280", "abs": "https://arxiv.org/abs/2602.02280", "authors": ["Zeming Wei", "Zhixin Zhang", "Chengcan Wu", "Yihao Zhang", "Xiaokun Luan", "Meng Sun"], "title": "RACA: Representation-Aware Coverage Criteria for LLM Safety Testing", "comment": null, "summary": "Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests. While coverage criteria have been effective for smaller neural networks, they are not directly applicable to LLMs due to scalability issues and differing objectives. To address these challenges, this paper introduces RACA, a novel set of coverage criteria specifically designed for LLM safety testing. RACA leverages representation engineering to focus on safety-critical concepts within LLMs, thereby reducing dimensionality and filtering out irrelevant information. The framework operates in three stages: first, it identifies safety-critical representations using a small, expert-curated calibration set of jailbreak prompts. Second, it calculates conceptual activation scores for a given test suite based on these representations. Finally, it computes coverage results using six sub-criteria that assess both individual and compositional safety concepts. We conduct comprehensive experiments to validate RACA's effectiveness, applicability, and generalization, where the results demonstrate that RACA successfully identifies high-quality jailbreak prompts and is superior to traditional neuron-level criteria. We also showcase its practical application in real-world scenarios, such as test set prioritization and attack prompt sampling. Furthermore, our findings confirm RACA's generalization to various scenarios and its robustness across various configurations. Overall, RACA provides a new framework for evaluating the safety of LLMs, contributing a valuable technique to the field of testing for AI.", "AI": {"tldr": "RACA\u63d0\u51fa\u4e86\u4e00\u5957\u4e13\u95e8\u9488\u5bf9LLM\u5b89\u5168\u6d4b\u8bd5\u7684\u8986\u76d6\u51c6\u5219\uff0c\u5229\u7528\u8868\u793a\u5de5\u7a0b\u805a\u7126\u5b89\u5168\u5173\u952e\u6982\u5ff5\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6846\u67b6\u8bc4\u4f30\u6d4b\u8bd5\u8d28\u91cf\uff0c\u4f18\u4e8e\u4f20\u7edf\u795e\u7ecf\u5143\u7ea7\u51c6\u5219\u3002", "motivation": "LLM\u7684\u5148\u8fdb\u80fd\u529b\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u5b89\u5168\u95ee\u9898\uff0c\u7279\u522b\u662f\u901a\u8fc7\u8d8a\u72f1\u653b\u51fb\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002\u5f53\u524dLLM\u5b89\u5168\u6d4b\u8bd5\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6807\u51c6\u6765\u8bc4\u4f30\u6d4b\u8bd5\u8d28\u91cf\u548c\u5145\u5206\u6027\u3002\u4f20\u7edf\u8986\u76d6\u51c6\u5219\u9002\u7528\u4e8e\u5c0f\u578b\u795e\u7ecf\u7f51\u7edc\uff0c\u4f46\u4e0d\u9002\u7528\u4e8eLLM\u7684\u53ef\u6269\u5c55\u6027\u548c\u76ee\u6807\u5dee\u5f02\u3002", "method": "RACA\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1)\u4f7f\u7528\u4e13\u5bb6\u7b56\u5212\u7684\u8d8a\u72f1\u63d0\u793a\u6821\u51c6\u96c6\u8bc6\u522b\u5b89\u5168\u5173\u952e\u8868\u793a\uff1b2)\u57fa\u4e8e\u8fd9\u4e9b\u8868\u793a\u8ba1\u7b97\u7ed9\u5b9a\u6d4b\u8bd5\u5957\u4ef6\u7684\u6982\u5ff5\u6fc0\u6d3b\u5206\u6570\uff1b3)\u4f7f\u7528\u516d\u4e2a\u5b50\u51c6\u5219\u8ba1\u7b97\u8986\u76d6\u7ed3\u679c\uff0c\u8bc4\u4f30\u4e2a\u4f53\u548c\u7ec4\u5408\u5b89\u5168\u6982\u5ff5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RACA\u7684\u6709\u6548\u6027\u3001\u9002\u7528\u6027\u548c\u6cdb\u5316\u6027\uff0c\u6210\u529f\u8bc6\u522b\u9ad8\u8d28\u91cf\u8d8a\u72f1\u63d0\u793a\uff0c\u4f18\u4e8e\u4f20\u7edf\u795e\u7ecf\u5143\u7ea7\u51c6\u5219\u3002\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u5982\u6d4b\u8bd5\u96c6\u4f18\u5148\u7ea7\u6392\u5e8f\u548c\u653b\u51fb\u63d0\u793a\u91c7\u6837\uff0c\u5e76\u5728\u5404\u79cd\u914d\u7f6e\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "RACA\u4e3a\u8bc4\u4f30LLM\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7126\u5b89\u5168\u5173\u952e\u6982\u5ff5\u3001\u964d\u4f4e\u7ef4\u5ea6\u548c\u8fc7\u6ee4\u65e0\u5173\u4fe1\u606f\uff0c\u4e3aAI\u6d4b\u8bd5\u9886\u57df\u8d21\u732e\u4e86\u6709\u4ef7\u503c\u7684\u6280\u672f\u3002", "topic": "agent analysis"}}
{"id": "2602.00543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00543", "abs": "https://arxiv.org/abs/2602.00543", "authors": ["Seho Pyo", "Jiheon Seok", "Jaejin Lee"], "title": "Reasoning by Commented Code for Table Question Answering", "comment": null, "summary": "Table Question Answering (TableQA) poses a significant challenge for large language models (LLMs) because conventional linearization of tables often disrupts the two-dimensional relationships intrinsic to structured data. Existing methods, which depend on end-to-end answer generation or single-line program queries, typically exhibit limited numerical accuracy and reduced interpretability. This work introduces a commented, step-by-step code-generation framework that incorporates explicit reasoning into the Python program-generation process. The approach decomposes TableQA reasoning into multi-line executable programs with concise natural language comments, thereby promoting clearer reasoning and increasing the likelihood of generating correct code. On the WikiTableQuestions benchmark, the proposed method achieves 70.9\\% accuracy using Qwen2.5-Coder-7B-Instruct, surpassing the Repanda baseline (67.6\\%). Integrating the proposed framework with a robust end-to-end TableQA model via a lightweight answer-selection mechanism yields further improvements. This combined approach achieves up to 84.3\\% accuracy on the WikiTableQuestions benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e26\u6ce8\u91ca\u7684\u9010\u6b65\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8868\u683c\u95ee\u7b54\u5206\u89e3\u4e3a\u591a\u884c\u53ef\u6267\u884c\u7a0b\u5e8f\u5e76\u6dfb\u52a0\u81ea\u7136\u8bed\u8a00\u6ce8\u91ca\uff0c\u63d0\u5347\u63a8\u7406\u6e05\u6670\u5ea6\u548c\u6570\u503c\u51c6\u786e\u6027\uff0c\u5728WikiTableQuestions\u57fa\u51c6\u4e0a\u8fbe\u523084.3%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u7ebf\u6027\u5316\u65b9\u6cd5\u7834\u574f\u4e86\u7ed3\u6784\u5316\u6570\u636e\u7684\u4e8c\u7ef4\u5173\u7cfb\uff0c\u73b0\u6709\u7aef\u5230\u7aef\u7b54\u6848\u751f\u6210\u6216\u5355\u884c\u7a0b\u5e8f\u67e5\u8be2\u65b9\u6cd5\u5728\u6570\u503c\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6e05\u6670\u7684\u63a8\u7406\u6846\u67b6\u3002", "method": "\u5f15\u5165\u5e26\u6ce8\u91ca\u7684\u9010\u6b65\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u5c06\u8868\u683c\u95ee\u7b54\u63a8\u7406\u5206\u89e3\u4e3a\u591a\u884c\u53ef\u6267\u884cPython\u7a0b\u5e8f\uff0c\u6bcf\u884c\u4ee3\u7801\u9644\u5e26\u7b80\u6d01\u81ea\u7136\u8bed\u8a00\u6ce8\u91ca\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7b54\u6848\u9009\u62e9\u673a\u5236\u4e0e\u73b0\u6709\u7aef\u5230\u7aef\u6a21\u578b\u96c6\u6210\u3002", "result": "\u5728WikiTableQuestions\u57fa\u51c6\u4e0a\uff0c\u4f7f\u7528Qwen2.5-Coder-7B-Instruct\u8fbe\u523070.9%\u51c6\u786e\u7387\uff0c\u8d85\u8fc7Repanda\u57fa\u7ebf\uff0867.6%\uff09\uff1b\u4e0e\u73b0\u6709\u7aef\u5230\u7aef\u6a21\u578b\u7ed3\u5408\u540e\u8fdb\u4e00\u6b65\u63d0\u5347\u81f384.3%\u51c6\u786e\u7387\u3002", "conclusion": "\u5e26\u6ce8\u91ca\u7684\u4ee3\u7801\u751f\u6210\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u548c\u7a0b\u5e8f\u5206\u89e3\u6709\u6548\u63d0\u5347\u8868\u683c\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2602.02361", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02361", "abs": "https://arxiv.org/abs/2602.02361", "authors": ["Mouxiang Chen", "Lei Zhang", "Yunlong Feng", "Xuwu Wang", "Wenting Zhao", "Ruisheng Cao", "Jiaxi Yang", "Jiawei Chen", "Mingze Li", "Zeyao Ma", "Hao Ge", "Zongmeng Zhang", "Zeyu Cui", "Dayiheng Liu", "Jingren Zhou", "Jianling Sun", "Junyang Lin", "Binyuan Hui"], "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions", "comment": "13 pages", "summary": "We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.", "AI": {"tldr": "SWE-Universe\u662f\u4e00\u4e2a\u4eceGitHub PR\u81ea\u52a8\u6784\u5efa\u771f\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u53ef\u9a8c\u8bc1\u73af\u5883\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u5b9a\u5236\u8bad\u7ec3\u6a21\u578b\u9a71\u52a8\u7684\u6784\u5efa\u4ee3\u7406\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u9a8c\u8bc1\u548c\u5faa\u73af\u9ed1\u5ba2\u68c0\u6d4b\uff0c\u751f\u6210\u4e8680\u591a\u4e07\u4e2a\u591a\u8bed\u8a00SWE\u73af\u5883\uff0c\u5e76\u5728Qwen3-Max-Thinking\u4e0a\u5b9e\u73b0\u4e8675.3%\u7684SWE-Bench Verified\u5f97\u5206\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u6784\u5efa\u73af\u5883\u4e2d\u7684\u6311\u6218\uff1a\u4f4e\u4ea7\u51fa\u7387\u3001\u5f31\u9a8c\u8bc1\u5668\u548c\u9ad8\u6210\u672c\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7f16\u7801\u667a\u80fd\u4f53\u63d0\u4f9b\u5173\u952e\u8d44\u6e90\u548c\u7a33\u5065\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u8bad\u7ec3\u6a21\u578b\u9a71\u52a8\u7684\u6784\u5efa\u4ee3\u7406\uff0c\u91c7\u7528\u8fed\u4ee3\u81ea\u9a8c\u8bc1\u548c\u5faa\u73af\u9ed1\u5ba2\u68c0\u6d4b\u673a\u5236\uff0c\u4eceGitHub PR\u81ea\u52a8\u6784\u5efa\u53ef\u9a8c\u8bc1\u7684SWE\u73af\u5883\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86807,693\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u591a\u8bed\u8a00SWE\u73af\u5883\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u4e2d\u671f\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u4e86\u73af\u5883\u4ef7\u503c\uff0c\u5728Qwen3-Max-Thinking\u4e0a\u5b9e\u73b0\u4e8675.3%\u7684SWE-Bench Verified\u5f97\u5206\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u63a8\u8fdb\u4e0b\u4e00\u4ee3\u7f16\u7801\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90\u548c\u7a33\u5065\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u81ea\u52a8\u6784\u5efa\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754cSWE\u73af\u5883\u7684\u53ef\u884c\u6027\u3002", "topic": "swe benchmark"}}
{"id": "2602.00585", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00585", "abs": "https://arxiv.org/abs/2602.00585", "authors": ["Guochen Yan", "Jialong Wu", "Zhengwei Tao", "Bo Li", "Qintong Zhang", "Jiahao Xu", "Haitao Mi", "Yuejian Fang", "Qingni Shen", "Wentao Zhang", "Zhonghai Wu"], "title": "Exploring Information Seeking Agent Consolidation", "comment": null, "summary": "Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.", "AI": {"tldr": "\u7814\u7a76\u5982\u4f55\u5c06\u5f02\u6784\u4fe1\u606f\u68c0\u7d22\u667a\u80fd\u4f53\u6574\u5408\u4e3a\u5355\u4e00\u57fa\u7840\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u6bd4\u8f83\u6570\u636e\u7ea7\u6574\u5408\u4e0e\u53c2\u6570\u7ea7\u6574\u5408\u4e24\u79cd\u7b56\u7565\u7684\u6027\u80fd\u8868\u73b0\u548c\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u4fe1\u606f\u68c0\u7d22\u667a\u80fd\u4f53\u901a\u5e38\u4e13\u95e8\u9488\u5bf9\u5f00\u653e\u7f51\u7edc\u3001\u6587\u6863\u6216\u672c\u5730\u77e5\u8bc6\u5e93\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u6574\u5408\u5f02\u6784\u667a\u80fd\u4f53\u3002", "method": "\u7814\u7a76\u4e24\u79cd\u4e92\u8865\u7684\u6574\u5408\u7b56\u7565\uff1a\u6570\u636e\u7ea7\u6574\u5408\uff08\u5728\u6df7\u5408\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8054\u5408\u8bad\u7ec3\u7edf\u4e00\u6a21\u578b\uff09\u548c\u53c2\u6570\u7ea7\u6574\u5408\uff08\u5728\u53c2\u6570\u5c42\u9762\u5408\u5e76\u72ec\u7acb\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u6a21\u578b\uff09\u3002", "result": "\u6570\u636e\u7ea7\u6574\u5408\u4fdd\u6301\u5f3a\u5927\u7a33\u5b9a\u7684\u57fa\u7ebf\u6027\u80fd\uff0c\u53c2\u6570\u7ea7\u6574\u5408\u63d0\u4f9b\u6709\u524d\u666f\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5b58\u5728\u5e72\u6270\u548c\u9c81\u68d2\u6027\u6311\u6218\u3002\u53d1\u73b0\u53c2\u6570\u7ea7\u6574\u5408\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\u5305\u62ec\u7ec6\u7c92\u5ea6\u5408\u5e76\u7c92\u5ea6\u3001\u4efb\u52a1\u5f02\u6784\u6027\u611f\u77e5\u548c\u539f\u5219\u6027\u5171\u8bc6\u7b56\u7565\u3002", "conclusion": "\u6570\u636e\u7ea7\u6574\u5408\u4ecd\u662f\u7a33\u5065\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u53c2\u6570\u7ea7\u6574\u5408\u867d\u6709\u6f5c\u529b\u4f46\u9700\u89e3\u51b3\u5e72\u6270\u95ee\u9898\uff0c\u8bc6\u522b\u4e86\u53c2\u6570\u7ea7\u6574\u5408\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u667a\u80fd\u4f53\u6574\u5408\u3002", "topic": "agent analysis"}}
{"id": "2602.00612", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00612", "abs": "https://arxiv.org/abs/2602.00612", "authors": ["Yitong Zhang", "Yongmin Li", "Yuetong Liu", "Jia Li", "Xiaoran Jia", "Zherui Li", "Ge Li"], "title": "Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have demonstrated promising generative capabilities and are increasingly used to produce formal languages defined by context-free grammars, such as source code and chemical expressions. However, as probabilistic models, they still struggle to generate syntactically valid outputs reliably. A natural and promising direction to address this issue is to adapt constrained decoding techniques to enforce grammatical correctness during generation. However, applying these techniques faces two primary obstacles. On the one hand, the non-autoregressive nature of dLLMs renders most existing constrained decoding approaches inapplicable. On the other hand, current approaches specifically designed for dLLMs may allow intermediate outputs that are impossible to complete into valid sentences, which significantly limits their reliability in practice.\n  To address these challenges, we present LAVE, a constrained decoding approach specifically designed for dLLMs. Our approach leverages a key property of dLLMs, namely their ability to predict token distributions for all positions in parallel during each forward pass. Whenever a new token is proposed by model, LAVE performs lookahead using these distributions to efficiently and reliably verify the validity of the proposed token. This design ensures reliable constraints by reliably preserving the potential for intermediate outputs to be extended into valid sentences. Extensive experiments across four widely used dLLMs and three representative benchmarks demonstrate that LAVE consistently outperforms existing baselines and achieves substantial improvements in syntactic correctness, while incurring negligible runtime overhead.", "AI": {"tldr": "LAVE\u662f\u4e00\u79cd\u4e13\u95e8\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u7ea6\u675f\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u524d\u77bb\u9a8c\u8bc1\u786e\u4fdd\u751f\u6210\u7684\u4ee3\u7801\u8bed\u6cd5\u6b63\u786e\u6027\uff0c\u663e\u8457\u63d0\u5347\u8f93\u51fa\u8d28\u91cf\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5f62\u5f0f\u8bed\u8a00\uff08\u5982\u6e90\u4ee3\u7801\uff09\u65f6\u96be\u4ee5\u53ef\u9760\u5730\u4fdd\u8bc1\u8bed\u6cd5\u6b63\u786e\u6027\uff0c\u73b0\u6709\u7ea6\u675f\u89e3\u7801\u65b9\u6cd5\u8981\u4e48\u4e0d\u9002\u7528\u4e8e\u975e\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u8981\u4e48\u65e0\u6cd5\u786e\u4fdd\u4e2d\u95f4\u8f93\u51fa\u53ef\u6269\u5c55\u4e3a\u6709\u6548\u53e5\u5b50\u3002", "method": "\u5229\u7528dLLMs\u80fd\u591f\u5e76\u884c\u9884\u6d4b\u6240\u6709\u4f4d\u7f6etoken\u5206\u5e03\u7684\u7279\u6027\uff0c\u5728\u6a21\u578b\u63d0\u51fa\u65b0token\u65f6\u8fdb\u884c\u524d\u77bb\u9a8c\u8bc1\uff0c\u9ad8\u6548\u53ef\u9760\u5730\u68c0\u67e5\u63d0\u8baetoken\u7684\u6709\u6548\u6027\uff0c\u786e\u4fdd\u4e2d\u95f4\u8f93\u51fa\u59cb\u7ec8\u53ef\u6269\u5c55\u4e3a\u6709\u6548\u53e5\u5b50\u3002", "result": "\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684dLLMs\u548c\u4e09\u4e2a\u4ee3\u8868\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLAVE\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u8bed\u6cd5\u6b63\u786e\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5f15\u5165\u7684\u8fd0\u884c\u65f6\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "LAVE\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u7ea6\u675f\u89e3\u7801\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5176\u5728\u751f\u6210\u5f62\u5f0f\u8bed\u8a00\u65f6\u8bed\u6cd5\u6b63\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "topic": "code agent"}}
{"id": "2602.00592", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.00592", "abs": "https://arxiv.org/abs/2602.00592", "authors": ["Jiaran Zhang", "Luck Ma", "Yanhao Li", "Fanqi Wan", "Di Qi", "Xu Zhao", "Jieyi Hou", "Zhe Xie", "Mengqiang Ren", "Xin Wu", "Zhewei Huang", "Liangyu Chen", "Yingwei Ma", "Qi Han", "Xiangyu Zhang"], "title": "DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder", "comment": null, "summary": "Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.", "AI": {"tldr": "DockSmith\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8eDocker\u73af\u5883\u6784\u5efa\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u901a\u8fc7\u5c06\u73af\u5883\u6784\u5efa\u89c6\u4e3a\u6838\u5fc3\u4ee3\u7406\u80fd\u529b\u800c\u975e\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u89e3\u51b3\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8eDocker\u7684\u73af\u5883\u6784\u5efa\u662f\u6269\u5c55\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u6267\u884c\u57fa\u7840\u8bad\u7ec3\u548c\u8bc4\u4f30\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u9700\u8981\u66f4\u53ef\u9760\u548c\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e13\u95e8\u7684Docker\u6784\u5efa\u4ee3\u7406DockSmith\uff0c\u5c06\u5176\u89c6\u4e3a\u6838\u5fc3\u4ee3\u7406\u80fd\u529b\uff0c\u8bad\u7ec3\u4e8e\u5927\u89c4\u6a21\u6267\u884c\u57fa\u7840\u7684Docker\u6784\u5efa\u8f68\u8ff9\uff0c\u91c7\u7528SWE-Factory\u98ce\u683c\u7ba1\u9053\u5e76\u589e\u5f3a\u5faa\u73af\u68c0\u6d4b\u63a7\u5236\u5668\u548c\u8de8\u4efb\u52a1\u6210\u529f\u8bb0\u5fc6\u3002", "result": "\u5728Multi-Docker-Eval\u4e0a\u8fbe\u5230\u5f00\u6e90\u6700\u5148\u8fdb\u6027\u80fd\uff0839.72% Fail-to-Pass\u548c58.28% Commit Rate\uff09\uff0c\u5e76\u5728SWE-bench Verified\u3001SWE-bench Multilingual\u548cTerminal-Bench 2.0\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u5206\u5e03\u5916\u6027\u80fd\u3002", "conclusion": "DockSmith\u4e0d\u4ec5\u89e3\u51b3\u4e86Docker\u73af\u5883\u6784\u5efa\u7684\u74f6\u9888\u95ee\u9898\uff0c\u8fd8\u5c55\u793a\u4e86\u73af\u5883\u6784\u5efa\u4f5c\u4e3a\u6838\u5fc3\u4ee3\u7406\u80fd\u529b\u5e26\u6765\u7684\u66f4\u5e7f\u6cdb\u7684\u4ee3\u7406\u6027\u80fd\u63d0\u5347\u3002", "topic": "code agent"}}
{"id": "2602.00127", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00127", "abs": "https://arxiv.org/abs/2602.00127", "authors": ["Tong Zhu", "Baiting Chen", "Jin Zhou", "Hua Zhou", "Sriram Sankararaman", "Xiaowu Dai"], "title": "ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning", "comment": null, "summary": "LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.", "AI": {"tldr": "ALIGN\u63d0\u51fa\u4e00\u79cd\u591a\u667a\u80fd\u4f53LLM\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u4efb\u52a1\u5efa\u6a21\u4e3a\u5bf9\u9f50\u59d4\u6258\u6e38\u620f\uff0c\u901a\u8fc7\u8bbe\u8ba1\u6fc0\u52b1\u673a\u5236\u8ba9\u591a\u4e2a\u667a\u80fd\u4f53\u751f\u6210\u5019\u9009\u65b9\u6848\uff0c\u7136\u540e\u9009\u62e9\u6700\u4f73\u7b54\u6848\uff0c\u7406\u8bba\u4e0a\u4fdd\u8bc1\u6bd4\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u6709\u66f4\u597d\u7684\u671f\u671b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfLLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u7684\u63a8\u7406\u65f6\u96c6\u6210\u65b9\u6cd5\u901a\u5e38\u5c06\u5019\u9009\u7b54\u6848\u89c6\u4e3a\u72ec\u7acb\u5904\u7406\uff0c\u7f3a\u4e4f\u5f62\u5f0f\u5316\u4fdd\u8bc1\u96c6\u6210\u80fd\u63d0\u9ad8\u63a8\u7406\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u5bf9\u9f50\u59d4\u6258\u6e38\u620f\uff1a\u59d4\u6258\u4eba\u5c06\u4efb\u52a1\u59d4\u6258\u7ed9\u591a\u4e2a\u667a\u80fd\u4f53\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u5728\u8bbe\u8ba1\u597d\u7684\u6fc0\u52b1\u673a\u5236\u4e0b\u751f\u6210\u5019\u9009\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u59d4\u6258\u4eba\u4ece\u4e2d\u9009\u62e9\u6700\u7ec8\u7b54\u6848\uff0c\u4fdd\u6301\u667a\u80fd\u4f53\u4e0e\u59d4\u6258\u4eba\u76ee\u6807\u7684\u5bf9\u9f50\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u516c\u5e73\u6bd4\u8f83\u6761\u4ef6\u4e0b\uff0cALIGN\u80fd\u8bc1\u660e\u6bd4\u5355\u667a\u80fd\u4f53\u751f\u6210\u6709\u66f4\u597d\u7684\u671f\u671b\u6027\u80fd\uff0c\u4e14\u80fd\u5904\u7406\u76f8\u5173\u5019\u9009\u7b54\u6848\uff0c\u653e\u5bbd\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u7684\u72ec\u7acb\u6027\u5047\u8bbe\u3002\u5728\u5e7f\u6cdb\u7684LLM\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cALIGN\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5355\u667a\u80fd\u4f53\u548c\u96c6\u6210\u57fa\u7ebf\u3002", "conclusion": "ALIGN\u901a\u8fc7\u5c06LLM\u63a8\u7406\u5f62\u5f0f\u5316\u4e3a\u5bf9\u9f50\u59d4\u6258\u6e38\u620f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u4e0a\u53ef\u4fdd\u8bc1\u6027\u80fd\u63d0\u5347\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u76ee\u6807\u5bf9\u9f50\u7684\u540c\u65f6\u6539\u8fdb\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2602.00611", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00611", "abs": "https://arxiv.org/abs/2602.00611", "authors": ["Jiaqi Xu", "Tao Huang", "Kai Zhang"], "title": "Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome", "comment": null, "summary": "Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e867B\u53c2\u6570LLM\u5728VirtualHome\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u81ea\u4e00\u81f4\u6027\u89e3\u7801\u7b56\u7565\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u5177\u8eabAI\u4efb\u52a1\u4e2d\u5404\u6709\u4f18\u52bf\u3002", "motivation": "\u5177\u8eabAI\u9700\u8981\u4ee3\u7406\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7406\u89e3\u76ee\u6807\u3001\u89c4\u5212\u52a8\u4f5c\u548c\u6267\u884c\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5c1a\u672a\u5f97\u5230\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528Embodied Agent Interface\u6846\u67b6\uff0c\u5728VirtualHome\u57fa\u51c6\u4e0a\u8bc4\u4f30OPENPANGU-7B\u548cQWEN2.5-7B\u6a21\u578b\uff0c\u63d0\u51fa\u7ed3\u6784\u5316\u81ea\u4e00\u81f4\u6027\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u91c7\u6837\u548c\u9886\u57df\u7279\u5b9a\u6295\u7968\u673a\u5236\u63d0\u9ad8\u7ed3\u6784\u5316\u751f\u6210\u8d28\u91cf\u3002", "result": "\u7ed3\u6784\u5316\u81ea\u4e00\u81f4\u6027\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0cOPENPANGU-7B\u5728\u5206\u5c42\u89c4\u5212\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800cQWEN2.5-7B\u5728\u52a8\u4f5c\u7ea7\u4efb\u52a1\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u4e0d\u540c\u6a21\u578b\u7c7b\u578b\u5c55\u73b0\u51fa\u4e92\u8865\u4f18\u52bf\u3002", "conclusion": "\u4e0d\u540cLLM\u5728\u5177\u8eabAI\u4efb\u52a1\u4e2d\u5404\u6709\u4e13\u957f\uff0c\u7ed3\u6784\u5316\u81ea\u4e00\u81f4\u6027\u89e3\u7801\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u5177\u8eabAI\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2602.01155", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01155", "abs": "https://arxiv.org/abs/2602.01155", "authors": ["Hugo Math", "Julian Lorentz", "Stefan Oelsner", "Rainer Lienhart"], "title": "Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles", "comment": "7 pages, 3 figures", "summary": "Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.", "AI": {"tldr": "CAREP\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u8f66\u8f86\u8bca\u65ad\u6545\u969c\u4ee3\u7801(DTCs)\u4e2d\u81ea\u52a8\u751f\u6210\u9519\u8bef\u6a21\u5f0f(EP)\u89c4\u5219\uff0c\u53d6\u4ee3\u4f20\u7edf\u624b\u5de5\u89c4\u5219\u5236\u5b9a\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u4ee3\u8f66\u8f86\u4ea7\u751f\u5927\u91cf\u8bca\u65ad\u6545\u969c\u4ee3\u7801(DTCs)\uff0c\u6c7d\u8f66\u5236\u9020\u5546\u4f7f\u7528\u8fd9\u4e9b\u4ee3\u7801\u7684\u5e03\u5c14\u7ec4\u5408\uff08\u9519\u8bef\u6a21\u5f0fEP\uff09\u6765\u8868\u5f81\u7cfb\u7edf\u6545\u969c\u3002\u76ee\u524dEP\u89c4\u5219\u4ecd\u7531\u9886\u57df\u4e13\u5bb6\u624b\u5de5\u5236\u5b9a\uff0c\u968f\u7740\u8f66\u8f86\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u65e2\u6602\u8d35\u53c8\u5bb9\u6613\u51fa\u9519\u3002", "method": "CAREP\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff1a1\uff09\u56e0\u679c\u53d1\u73b0\u667a\u80fd\u4f53\u8bc6\u522bDTC-EP\u6f5c\u5728\u5173\u7cfb\uff1b2\uff09\u4e0a\u4e0b\u6587\u4fe1\u606f\u667a\u80fd\u4f53\u6574\u5408\u5143\u6570\u636e\u548c\u63cf\u8ff0\uff1b3\uff09\u7f16\u6392\u667a\u80fd\u4f53\u5408\u6210\u5019\u9009\u5e03\u5c14\u89c4\u5219\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728\u5305\u542b29,100\u4e2a\u72ec\u7279DTCs\u548c474\u4e2a\u9519\u8bef\u6a21\u5f0f\u7684\u5927\u89c4\u6a21\u6c7d\u8f66\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cCAREP\u80fd\u591f\u81ea\u52a8\u51c6\u786e\u5730\u53d1\u73b0\u672a\u77e5EP\u89c4\u5219\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528LLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u900f\u660e\u7684\u56e0\u679c\u89e3\u91ca\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5b9e\u7528\u7684\u56e0\u679c\u53d1\u73b0\u548c\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u63a8\u7406\uff0cCAREP\u4ee3\u8868\u4e86\u5411\u5168\u81ea\u52a8\u6545\u969c\u8bca\u65ad\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u8f66\u8f86\u7ef4\u62a4\u3002", "topic": "agent analysis"}}
{"id": "2602.01465", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01465", "abs": "https://arxiv.org/abs/2602.01465", "authors": ["Nikita Benkovich", "Vitalii Valkov"], "title": "Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering", "comment": null, "summary": "Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5c06\u8f6f\u4ef6\u5de5\u7a0b\u5efa\u6a21\u4e3a\u7ec4\u7ec7\u5316\u6d41\u7a0b\uff0c\u6a21\u62df\u771f\u5b9e\u5de5\u7a0b\u56e2\u961f\u7ed3\u6784\uff0c\u5728SWE-bench 500\u4e0a\u8fbe\u523072.4%\u7684\u89e3\u51b3\u7387", "motivation": "\u73b0\u6709\u81ea\u4e3b\u7cfb\u7edf\u5c06\u95ee\u9898\u89e3\u51b3\u89c6\u4e3a\u5355\u4e00\u6216\u6d41\u6c34\u7ebf\u8fc7\u7a0b\uff0c\u800c\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u662f\u56e2\u961f\u534f\u4f5c\u6d3b\u52a8\uff0c\u5177\u6709\u660e\u786e\u7684\u89d2\u8272\u5206\u79bb\u3001\u6c9f\u901a\u548c\u5ba1\u67e5\u3002\u9700\u8981\u6a21\u62df\u771f\u5b9e\u5de5\u7a0b\u56e2\u961f\u7684\u7ec4\u7ec7\u7ed3\u6784\u6765\u63d0\u5347\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5f00\u6e90\u5e73\u53f0agyn\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5206\u914d\u4e13\u95e8\u89d2\u8272\uff08\u534f\u8c03\u3001\u7814\u7a76\u3001\u5b9e\u73b0\u3001\u5ba1\u67e5\uff09\uff0c\u63d0\u4f9b\u9694\u79bb\u6c99\u7bb1\uff0c\u652f\u6301\u7ed3\u6784\u5316\u901a\u4fe1\u3002\u9075\u5faa\u5b9a\u4e49\u597d\u7684\u5f00\u53d1\u65b9\u6cd5\u8bba\uff1a\u5206\u6790\u3001\u4efb\u52a1\u89c4\u8303\u3001PR\u521b\u5efa\u548c\u8fed\u4ee3\u5ba1\u67e5\uff0c\u5b8c\u5168\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "result": "\u5728SWE-bench 500\u4e0a\u8fbe\u523072.4%\u7684\u4efb\u52a1\u89e3\u51b3\u7387\uff0c\u4f18\u4e8e\u4f7f\u7528\u53ef\u6bd4\u8bed\u8a00\u6a21\u578b\u7684\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u3002\u7cfb\u7edf\u4e3a\u5b9e\u9645\u751f\u4ea7\u4f7f\u7528\u8bbe\u8ba1\uff0c\u672a\u9488\u5bf9SWE-bench\u8fdb\u884c\u8c03\u4f18\u3002", "conclusion": "\u590d\u5236\u56e2\u961f\u7ed3\u6784\u3001\u65b9\u6cd5\u8bba\u548c\u6c9f\u901a\u662f\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u7684\u6709\u529b\u8303\u5f0f\uff0c\u672a\u6765\u8fdb\u5c55\u53ef\u80fd\u540c\u7b49\u4f9d\u8d56\u4e8e\u7ec4\u7ec7\u8bbe\u8ba1\u548c\u667a\u80fd\u4f53\u57fa\u7840\u8bbe\u65bd\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6a21\u578b\u6539\u8fdb\u3002", "topic": "code agent"}}
{"id": "2602.00158", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00158", "abs": "https://arxiv.org/abs/2602.00158", "authors": ["Ziqi Gao", "Yaotian Zhu", "Qingcheng Zeng", "Xu Zhao", "Ziqing Wang", "Feng Ruan", "Kaize Ding"], "title": "RAPTOR: Ridge-Adaptive Logistic Probes", "comment": "Preprint", "summary": "Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.", "AI": {"tldr": "RAPTOR\u662f\u4e00\u79cd\u57fa\u4e8eL2\u6b63\u5219\u5316\u903b\u8f91\u56de\u5f52\u7684\u63a2\u9488\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u51bb\u7ed3LLM\u7684\u5c42\u8868\u793a\u4e2d\u63d0\u53d6\u6982\u5ff5\u5411\u91cf\uff0c\u5728\u51c6\u786e\u6027\u3001\u65b9\u5411\u7a33\u5b9a\u6027\u548c\u8bad\u7ec3\u6210\u672c\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u63a2\u9488\u65b9\u6cd5\u7528\u4e8e\u5206\u6790\u51bb\u7ed3LLM\u4e2d\u7f16\u7801\u7684\u4fe1\u606f\uff0c\u5e76\u5728\"\u63a2\u9488-\u5f15\u5bfc\"\u6d41\u7a0b\u4e2d\u7528\u4e8e\u6fc0\u6d3b\u5f15\u5bfc\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u51c6\u786e\u3001\u65b9\u5411\u7a33\u5b9a\u4e14\u6210\u672c\u4f4e\u7684\u6982\u5ff5\u5411\u91cf\u4f30\u8ba1\u3002", "method": "\u63d0\u51faRAPTOR\uff08Ridge-Adaptive Logistic Probe\uff09\uff0c\u4e00\u79cd\u7b80\u5355\u7684L2\u6b63\u5219\u5316\u903b\u8f91\u63a2\u9488\uff0c\u901a\u8fc7\u9a8c\u8bc1\u8c03\u4f18\u7684\u5cad\u5f3a\u5ea6\u4ece\u5f52\u4e00\u5316\u6743\u91cd\u4e2d\u63d0\u53d6\u6982\u5ff5\u5411\u91cf\u3002", "result": "\u5728\u6307\u4ee4\u8c03\u4f18LLM\u548c\u4eba\u5de5\u7f16\u5199\u6982\u5ff5\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cRAPTOR\u5728\u51c6\u786e\u6027\u4e0a\u5339\u914d\u6216\u8d85\u8fc7\u5f3a\u57fa\u7ebf\uff0c\u540c\u65f6\u5b9e\u73b0\u7ade\u4e89\u6027\u7684\u65b9\u5411\u7a33\u5b9a\u6027\u548c\u663e\u8457\u66f4\u4f4e\u7684\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "RAPTOR\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u6982\u5ff5\u5411\u91cf\u63d0\u53d6\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u89e3\u91ca\u4e86\u5cad\u60e9\u7f5a\u5f3a\u5ea6\u5982\u4f55\u8c03\u8282\u63a2\u9488\u51c6\u786e\u6027\u548c\u6982\u5ff5\u5411\u91cf\u7a33\u5b9a\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.01655", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01655", "abs": "https://arxiv.org/abs/2602.01655", "authors": ["Pengrui Lu", "Shiqi Zhang", "Yunzhong Hou", "Lyumanshan Ye", "Chaoyi Huang", "Zixi Chen", "Ji Zeng", "Hantao Jiang", "Pengfei Liu", "Yiwei Wang", "Ming-Hsuan Yang"], "title": "ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development", "comment": null, "summary": "Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.", "AI": {"tldr": "ProjDevBench\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7f16\u7801\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u9879\u76ee\u9700\u6c42\u8bc4\u4f30\u4ee3\u7406\u751f\u6210\u7684\u5b8c\u6574\u4ee3\u7801\u5e93\uff0c\u7ed3\u5408\u5728\u7ebf\u8bc4\u6d4b\u548cLLM\u8f85\u52a9\u4ee3\u7801\u5ba1\u67e5\uff0c\u572820\u4e2a\u7f16\u7a0b\u95ee\u9898\u4e0a\u6d4b\u8bd56\u4e2a\u7f16\u7801\u4ee3\u7406\uff0c\u603b\u4f53\u63a5\u53d7\u738727.38%", "motivation": "\u73b0\u6709\u7f16\u7801\u4ee3\u7406\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u95ee\u9898\u7ea7\u522b\u7684bug\u4fee\u590d\uff0c\u7f3a\u4e4f\u7aef\u5230\u7aef\u7684\u5f00\u53d1\u8bc4\u4f30\u3002\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u7f16\u7801\u4ee3\u7406\u4ece\u7b80\u5355\u63d0\u793a\u751f\u6210\u5b8c\u6574\u4ee3\u7801\u5e93\u7684\u80fd\u529b", "method": "\u5f15\u5165ProjDevBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u4f9b\u9879\u76ee\u9700\u6c42\u7ed9\u7f16\u7801\u4ee3\u7406\uff0c\u8bc4\u4f30\u751f\u6210\u7684\u4ee3\u7801\u5e93\u3002\u7ed3\u5408\u5728\u7ebf\u8bc4\u6d4b(Online Judge)\u6d4b\u8bd5\u548cLLM\u8f85\u52a9\u4ee3\u7801\u5ba1\u67e5\uff0c\u8bc4\u4f30\u4e09\u4e2a\u65b9\u9762\uff1a\u7cfb\u7edf\u67b6\u6784\u8bbe\u8ba1\u3001\u529f\u80fd\u6b63\u786e\u6027\u3001\u8fed\u4ee3\u89e3\u51b3\u65b9\u6848\u4f18\u5316", "result": "\u572820\u4e2a\u7f16\u7a0b\u95ee\u9898\uff088\u4e2a\u7c7b\u522b\uff09\u4e0a\u8bc4\u4f306\u4e2a\u57fa\u4e8e\u4e0d\u540cLLM\u540e\u7aef\u7684\u7f16\u7801\u4ee3\u7406\uff0c\u603b\u4f53\u63a5\u53d7\u7387\u4e3a27.38%\u3002\u4ee3\u7406\u80fd\u5904\u7406\u57fa\u672c\u529f\u80fd\u548c\u6570\u636e\u7ed3\u6784\uff0c\u4f46\u5728\u590d\u6742\u7cfb\u7edf\u8bbe\u8ba1\u3001\u65f6\u95f4\u590d\u6742\u5ea6\u4f18\u5316\u548c\u8d44\u6e90\u7ba1\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73", "conclusion": "ProjDevBench\u4e3a\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u4ee3\u7406\u5728\u590d\u6742\u7cfb\u7edf\u5f00\u53d1\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7f16\u7801\u4ee3\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u8bc4\u4f30\u57fa\u7840", "topic": "swe benchmark"}}
{"id": "2602.01785", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.01785", "abs": "https://arxiv.org/abs/2602.01785", "authors": ["Yuling Shi", "Chaoxiang Xie", "Zhensu Sun", "Yeheng Chen", "Chenxu Zhang", "Longfei Yun", "Chengcheng Wan", "Hongyu Zhang", "David Lo", "Xiaodong Gu"], "title": "CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding", "comment": "Code and data are available at https://github.com/YerbaPage/CodeOCR", "summary": "Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.", "AI": {"tldr": "MLLMs\u901a\u8fc7\u5c06\u4ee3\u7801\u6e32\u67d3\u4e3a\u56fe\u50cf\u8fdb\u884c\u538b\u7f29\uff0c\u5b9e\u73b0\u9ad8\u8fbe8\u500d\u7684token\u51cf\u5c11\uff0c\u5728\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u89c4\u6a21\u6269\u5927\uff0c\u4f20\u7edfLLMs\u57fa\u4e8e\u6587\u672c\u7684\u4ee3\u7801\u8868\u793a\u5bfc\u81f4\u4e0a\u4e0b\u6587\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u548c\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u3002MLLMs\u7684\u53d1\u5c55\u4e3a\u901a\u8fc7\u56fe\u50cf\u6a21\u6001\u538b\u7f29\u4ee3\u7801\u8868\u793a\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u56e0\u4e3a\u56fe\u50cf\u6bd4\u6587\u672c\u66f4\u9002\u5408\u538b\u7f29\u800c\u4e0d\u635f\u5931\u8bed\u4e49\u3002", "method": "\u5c06\u6e90\u4ee3\u7801\u6e32\u67d3\u4e3a\u56fe\u50cf\u8868\u793a\uff0c\u901a\u8fc7\u8c03\u6574\u5206\u8fa8\u7387\u5b9e\u73b0\u89c6\u89c9\u538b\u7f29\u3002\u9996\u6b21\u7cfb\u7edf\u7814\u7a76MLLMs\u5728\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5305\u62ectoken\u51cf\u5c11\u6548\u679c\u3001\u89c6\u89c9\u7ebf\u7d22\u5229\u7528\u548c\u538b\u7f29\u9c81\u68d2\u6027\u8bc4\u4f30\u3002", "result": "(1) MLLMs\u80fd\u6709\u6548\u7406\u89e3\u4ee3\u7801\uff0c\u5b9e\u73b0\u9ad8\u8fbe8\u500d\u7684token\u538b\u7f29\uff1b(2) \u80fd\u6709\u6548\u5229\u7528\u8bed\u6cd5\u9ad8\u4eae\u7b49\u89c6\u89c9\u7ebf\u7d22\uff0c\u57284\u500d\u538b\u7f29\u4e0b\u63d0\u5347\u4ee3\u7801\u8865\u5168\u6027\u80fd\uff1b(3) \u514b\u9686\u68c0\u6d4b\u7b49\u4efb\u52a1\u5bf9\u89c6\u89c9\u538b\u7f29\u5177\u6709\u5f02\u5e38\u9c81\u68d2\u6027\uff0c\u67d0\u4e9b\u538b\u7f29\u6bd4\u751a\u81f3\u7565\u4f18\u4e8e\u539f\u59cb\u6587\u672c\u8f93\u5165\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86MLLMs\u5728\u4ee3\u7801\u7406\u89e3\u4e2d\u7684\u6f5c\u529b\u548c\u5f53\u524d\u5c40\u9650\uff0c\u6307\u51fa\u56fe\u50cf\u6a21\u6001\u4ee3\u7801\u8868\u793a\u662f\u901a\u5411\u66f4\u9ad8\u6548\u63a8\u7406\u7684\u9014\u5f84\uff0c\u4e3a\u5927\u89c4\u6a21\u8f6f\u4ef6\u7cfb\u7edf\u7684\u4ee3\u7801\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "code agent"}}
{"id": "2602.00740", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00740", "abs": "https://arxiv.org/abs/2602.00740", "authors": ["Ziyan Xiao", "Yinghao Zhu", "Liang Peng", "Lequan Yu"], "title": "ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement", "comment": null, "summary": "Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns \"how to revise\" rather than just \"what to revise\". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.", "AI": {"tldr": "ExperienceWeaver\uff1a\u4e00\u79cd\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5608\u6742\u7684\u591a\u7ef4\u53cd\u9988\u63d0\u70bc\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\uff08\u6280\u5de7\u548c\u7b56\u7565\uff09\uff0c\u5728\u5c11\u6837\u672c\u4e34\u5e8a\u6587\u672c\u6539\u8fdb\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e34\u5e8a\u6587\u672c\u6539\u8fdb\u5bf9\u533b\u7597\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u8d28\u91cf\u6570\u636e\u6709\u9650\u4e14\u533b\u5b66\u6587\u6863\u7ea6\u675f\u590d\u6742\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff1a\u76d1\u7763\u5fae\u8c03\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u6210\u672c\u9ad8\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u901a\u5e38\u53ea\u80fd\u63d0\u4f9b\u8868\u9762\u4fee\u6b63\u800c\u65e0\u6cd5\u6355\u6349\u4fee\u8ba2\u80cc\u540e\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faExperienceWeaver\u5206\u5c42\u6846\u67b6\uff0c\u5c06\u91cd\u70b9\u4ece\u6570\u636e\u68c0\u7d22\u8f6c\u5411\u7ecf\u9a8c\u5b66\u4e60\u3002\u901a\u8fc7\u5c06\u5608\u6742\u7684\u591a\u7ef4\u53cd\u9988\u63d0\u70bc\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u64cd\u4f5c\u7684\u77e5\u8bc6\uff08\u9519\u8bef\u7279\u5b9a\u7684\u6280\u5de7\u548c\u9ad8\u5c42\u7b56\u7565\uff09\uff0c\u5e76\u5c06\u8fd9\u4e9b\u63d0\u70bc\u7684\u7ecf\u9a8c\u6ce8\u5165\u667a\u80fd\u4f53\u6d41\u7a0b\uff0c\u4f7f\u6a21\u578b\u5b66\u4e60\"\u5982\u4f55\u4fee\u8ba2\"\u800c\u975e\u4ec5\"\u4fee\u8ba2\u4ec0\u4e48\"\u3002", "result": "\u5728\u56db\u4e2a\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cExperienceWeaver\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8d85\u8d8a\u4e86\u5305\u62ecGemini-3 Pro\u5728\u5185\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "ExperienceWeaver\u901a\u8fc7\u7ecf\u9a8c\u63d0\u70bc\u800c\u975e\u7b80\u5355\u68c0\u7d22\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e34\u5e8a\u6587\u672c\u6539\u8fdb\u4e2d\u7684\u5c11\u6837\u672c\u5b66\u4e60\u95ee\u9898\uff0c\u4e3a\u533b\u7597\u6587\u6863\u8d28\u91cf\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2602.00685", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00685", "abs": "https://arxiv.org/abs/2602.00685", "authors": ["Xuan Liu", "Haoyang Shang", "Zizhang Liu", "Xinyan Liu", "Yunze Xiao", "Yiwen Tu", "Haojian Jin"], "title": "HumanStudy-Bench: Towards AI Agent Design for Participant Simulation", "comment": null, "summary": "Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86HUMANSTUDY-BENCH\u57fa\u51c6\u548c\u5f15\u64ce\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4f5c\u4e3a\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u6a21\u62df\u53c2\u4e0e\u8005\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u91cd\u73b0\u5df2\u53d1\u8868\u7684\u4eba\u7c7b\u5b9e\u9a8c\u6765\u91cf\u5316\u4ee3\u7406\u4e0e\u4eba\u7c7b\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u7684\u6a21\u62df\u53c2\u4e0e\u8005\uff0c\u4f46\u5176\u884c\u4e3a\u4e0d\u7a33\u5b9a\u4e14\u5bf9\u8bbe\u8ba1\u9009\u62e9\u654f\u611f\u3002\u73b0\u6709\u8bc4\u4f30\u7ecf\u5e38\u6df7\u6dc6\u57fa\u7840\u6a21\u578b\u80fd\u529b\u4e0e\u5b9e\u9a8c\u5b9e\u4f8b\u5316\uff0c\u96be\u4ee5\u533a\u5206\u7ed3\u679c\u662f\u6a21\u578b\u672c\u8eab\u8fd8\u662f\u4ee3\u7406\u8bbe\u7f6e\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u53c2\u4e0e\u8005\u6a21\u62df\u89c6\u4e3a\u5b8c\u6574\u5b9e\u9a8c\u534f\u8bae\u7684\u4ee3\u7406\u8bbe\u8ba1\u95ee\u9898\uff0c\u5b9a\u4e49\u4ee3\u7406\u7531\u57fa\u7840\u6a21\u578b\u548c\u89c4\u8303\u7ec4\u6210\u3002\u5f15\u5165HUMANSTUDY-BENCH\u57fa\u51c6\u548c\u6267\u884c\u5f15\u64ce\uff0c\u901a\u8fc7Filter-Extract-Execute-Evaluate\u7ba1\u9053\u91cd\u6784\u5df2\u53d1\u8868\u7684\u4eba\u7c7b\u5b9e\u9a8c\uff0c\u5728\u5171\u4eab\u8fd0\u884c\u65f6\u4e2d\u91cd\u653e\u8bd5\u9a8c\u5e8f\u5217\u5e76\u8fd0\u884c\u539f\u59cb\u5206\u6790\u7ba1\u9053\u3002", "result": "\u5efa\u7acb\u4e86\u5305\u542b12\u4e2a\u57fa\u7840\u7814\u7a76\u7684\u521d\u59cb\u5957\u4ef6\uff0c\u6db5\u76d6\u4e2a\u4f53\u8ba4\u77e5\u3001\u6218\u7565\u4e92\u52a8\u548c\u793e\u4f1a\u5fc3\u7406\u5b66\u9886\u57df\uff0c\u5305\u542b\u8d85\u8fc76,000\u6b21\u8bd5\u9a8c\uff0c\u4eba\u7c7b\u6837\u672c\u89c4\u6a21\u4ece\u6570\u5341\u4eba\u5230\u8d85\u8fc72,100\u540d\u53c2\u4e0e\u8005\u3002", "conclusion": "\u901a\u8fc7\u5c06\u53c2\u4e0e\u8005\u6a21\u62df\u6846\u67b6\u5316\u4e3a\u4ee3\u7406\u8bbe\u8ba1\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u6765\u91cf\u5316\u4eba\u7c7b\u4e0e\u4ee3\u7406\u884c\u4e3a\u7684\u4e00\u81f4\u6027\uff0c\u4e3aLLM\u5728\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u4e2d\u7684\u4f7f\u7528\u63d0\u4f9b\u4e86\u66f4\u4e25\u8c28\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.00166", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00166", "abs": "https://arxiv.org/abs/2602.00166", "authors": ["Evan Chen", "Wenzhi Fang", "Shiqiang Wang", "Christopher Brinton"], "title": "Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints", "comment": null, "summary": "Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.", "AI": {"tldr": "DA-GRPO\u662f\u4e00\u79cd\u53cc\u4f18\u52bf\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6301\u7eed\u5b66\u4e60\u573a\u666f\u4e0b\u667a\u80fd\u8c03\u8282\u672c\u5730\u5c0f\u8bed\u8a00\u6a21\u578b\u5bf9\u4e91\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8c03\u7528\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u5e76\u4fdd\u6301\u7a33\u5b9a\u7684\u4e91\u4f7f\u7528\u9884\u7b97\u3002", "motivation": "\u672c\u5730\u90e8\u7f72\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u9700\u8981\u6301\u7eed\u652f\u6301\u591a\u6837\u5316\u4efb\u52a1\uff0c\u5fc5\u987b\u9009\u62e9\u6027\u5730\u4f9d\u8d56\u4e91\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u3002\u7136\u800c\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u8c03\u8282\u4e91\u7aef\u534f\u52a9\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u57fa\u4e8e\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u4f1a\u4ea7\u751f\u4e0d\u7a33\u5b9a\u7684\u5378\u8f7d\u884c\u4e3a\uff0c\u5e76\u5728\u4efb\u52a1\u5206\u5e03\u53d8\u5316\u65f6\u52a0\u5267\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u63d0\u51faDA-GRPO\uff08\u53cc\u4f18\u52bf\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\uff0c\u5c06\u4e91\u7aef\u4f7f\u7528\u7ea6\u675f\u76f4\u63a5\u7eb3\u5165\u4f18\u52bf\u8ba1\u7b97\u4e2d\uff0c\u907f\u514d\u56fa\u5b9a\u7684\u5956\u52b1\u5851\u9020\u548c\u5916\u90e8\u8def\u7531\u6a21\u578b\u3002\u8be5\u8bbe\u8ba1\u4f7f\u672c\u5730\u6a21\u578b\u80fd\u591f\u8054\u5408\u5b66\u4e60\u4efb\u52a1\u80fd\u529b\u548c\u534f\u4f5c\u884c\u4e3a\uff0c\u5141\u8bb8\u5728\u8bad\u7ec3\u540e\u81ea\u7136\u4ea7\u751f\u4e91\u7aef\u8bf7\u6c42\uff0c\u540c\u65f6\u9075\u5b88\u9884\u8bbe\u7684\u534f\u52a9\u9884\u7b97\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDA-GRPO\u76f8\u6bd4\u5148\u524d\u7684\u534f\u4f5c\u548c\u57fa\u4e8e\u8def\u7531\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5207\u6362\u540e\u7684\u51c6\u786e\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u9057\u5fd8\uff0c\u5e76\u4fdd\u6301\u4e86\u7a33\u5b9a\u7684\u4e91\u7aef\u4f7f\u7528\u3002", "conclusion": "DA-GRPO\u901a\u8fc7\u5c06\u4e91\u7aef\u4f7f\u7528\u7ea6\u675f\u76f4\u63a5\u6574\u5408\u5230\u4f18\u52bf\u8ba1\u7b97\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u672c\u5730\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u667a\u80fd\u8c03\u8282\u4e91\u7aef\u534f\u52a9\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4efb\u52a1\u6027\u80fd\u548c\u66f4\u7a33\u5b9a\u7684\u534f\u4f5c\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2602.02084", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02084", "abs": "https://arxiv.org/abs/2602.02084", "authors": ["Jane Luo", "Chengyu Yin", "Xin Zhang", "Qingtao Li", "Steven Liu", "Yiming Huang", "Jie Wu", "Hao Liu", "Yangyu Huang", "Yu Kang", "Fangkai Yang", "Ying Xin", "Scarlett Li"], "title": "Closing the Loop: Universal Repository Representation with RPG-Encoder", "comment": null, "summary": "Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.", "AI": {"tldr": "RPG-Encoder\u5c06\u4ed3\u5e93\u89c4\u5212\u56fe\u4ece\u9759\u6001\u751f\u6210\u84dd\u56fe\u6269\u5c55\u4e3a\u7edf\u4e00\u9ad8\u4fdd\u771f\u8868\u793a\uff0c\u901a\u8fc7\u7f16\u7801\u539f\u59cb\u4ee3\u7801\u3001\u589e\u91cf\u62d3\u6251\u6f14\u5316\u548c\u7ed3\u6784\u611f\u77e5\u5bfc\u822a\uff0c\u5728\u4ed3\u5e93\u7406\u89e3\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4ed3\u5e93\u667a\u80fd\u4f53\u5b58\u5728\u63a8\u7406\u65ad\u5c42\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5b64\u7acb\u7684API\u6587\u6863\u6216\u7f3a\u4e4f\u8bed\u4e49\u6df1\u5ea6\u7684\u4f9d\u8d56\u56fe\u3002\u4f5c\u8005\u8ba4\u4e3a\u4ed3\u5e93\u7406\u89e3\u548c\u751f\u6210\u662f\u7edf\u4e00\u5faa\u73af\u4e2d\u7684\u9006\u8fc7\u7a0b\uff1a\u751f\u6210\u5c06\u610f\u56fe\u6269\u5c55\u4e3a\u5b9e\u73b0\uff0c\u7406\u89e3\u5c06\u5b9e\u73b0\u538b\u7f29\u56de\u610f\u56fe\u3002", "method": "\u63d0\u51faRPG-Encoder\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u673a\u5236\uff1a(1)\u5c06\u539f\u59cb\u4ee3\u7801\u7f16\u7801\u4e3a\u7ed3\u5408\u8bed\u4e49\u7279\u5f81\u548c\u4ee3\u7801\u4f9d\u8d56\u7684RPG\uff1b(2)\u589e\u91cf\u6f14\u5316\u62d3\u6251\u4ee5\u89e3\u8026\u7ef4\u62a4\u6210\u672c\u4e0e\u4ed3\u5e93\u89c4\u6a21\uff1b(3)\u4f5c\u4e3a\u7ed3\u6784\u611f\u77e5\u5bfc\u822a\u7684\u7edf\u4e00\u63a5\u53e3\u3002", "result": "\u5728SWE-bench Verified\u4e0a\u8fbe\u523093.7% Acc@5\u7684SOTA\u6027\u80fd\uff0c\u5728SWE-bench Live Lite\u4e0a\u8d85\u8fc7\u6700\u4f73\u57fa\u7ebf10%\u4ee5\u4e0a\uff0c\u5728RepoCraft\u4e0a\u8fbe\u523098.5%\u91cd\u6784\u8986\u76d6\u7387\uff0c\u7ef4\u62a4\u5f00\u9500\u51cf\u5c1195.7%\u3002", "conclusion": "RPG-Encoder\u901a\u8fc7\u7edf\u4e00\u7684\u9ad8\u4fdd\u771f\u8868\u793a\u6210\u529f\u5f25\u5408\u4e86\u610f\u56fe\u4e0e\u5b9e\u73b0\u4e4b\u95f4\u7684\u63a8\u7406\u5faa\u73af\uff0c\u5728\u590d\u6742\u4ee3\u7801\u5e93\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u91cd\u6784\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2602.00173", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00173", "abs": "https://arxiv.org/abs/2602.00173", "authors": ["Shuozhe Li", "Vaishnav Tadiparthi", "Kwonjoon Lee", "Nakul Agarwal", "Hossein Nourkhiz Mahjoub", "Ehsan Moradi Pari", "Lizhang Chen", "Amy Zhang", "Liu Leqi"], "title": "Learning Robust Reasoning through Guided Adversarial Self-Play", "comment": null, "summary": "Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.", "AI": {"tldr": "GASP\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6297\u6027\u81ea\u535a\u5f08\u8bad\u7ec3\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u5728\u9519\u8bef\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u68c0\u6d4b\u5e76\u4fee\u590d\u63a8\u7406\u9519\u8bef\uff0c\u63d0\u9ad8\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u6a21\u578b\u5728\u5e72\u51c0\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5f53\u4e0a\u4e0b\u6587\u5b58\u5728\u9519\u8bef\uff08\u5982\u63a8\u7406\u94fe\u635f\u574f\u3001\u8bef\u5bfc\u6027\u90e8\u5206\u89e3\u6216\u8f7b\u5fae\u8f93\u5165\u6270\u52a8\uff09\u65f6\u4f1a\u707e\u96be\u6027\u5931\u8d25\uff0c\u56e0\u4e3a\u6807\u51c6RLVR\u53ea\u4f18\u5316\u5e72\u51c0\u6761\u4ef6\u4e0b\u7684\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027", "method": "\u63d0\u51faGASP\uff08\u5f15\u5bfc\u5bf9\u6297\u6027\u81ea\u535a\u5f08\uff09\u65b9\u6cd5\uff1a\u5728\u5355\u4e2a\u6a21\u578b\u5185\u5f62\u6210\u5bf9\u6297\u6027\u81ea\u535a\u5f08\u6e38\u620f\uff0c\u6c61\u67d3\u8005\u5b66\u4e60\u901a\u8fc7\u5c40\u90e8\u8fde\u8d2f\u7684\u635f\u574f\u8bf1\u5bfc\u5931\u8d25\uff0c\u800c\u667a\u80fd\u4f53\u5b66\u4e60\u5728\u76f8\u540c\u635f\u574f\u6761\u4ef6\u4e0b\u8bca\u65ad\u548c\u6062\u590d\u3002\u4e3a\u89e3\u51b3\u8bad\u7ec3\u65e9\u671f\u6210\u529f\u6062\u590d\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u5206\u5e03\u5185\u4fee\u590d\u5f15\u5bfc\uff0c\u5bf9\u81ea\u751f\u6210\u7684\u4fee\u590d\u6dfb\u52a0\u6a21\u4eff\u9879\uff0c\u63d0\u9ad8\u6062\u590d\u6982\u7387\u540c\u65f6\u4fdd\u7559\u5df2\u6709\u80fd\u529b", "result": "\u5728\u56db\u4e2a\u5f00\u6e90\u6a21\u578b\uff081.5B-8B\uff09\u4e0a\uff0cGASP\u5c06\u5f3a\u4f46\u8106\u5f31\u7684\u63a8\u7406\u5668\u8f6c\u53d8\u4e3a\u9c81\u68d2\u7684\u63a8\u7406\u5668\uff0c\u80fd\u591f\u62b5\u5fa1\u8bef\u5bfc\u548c\u6270\u52a8\u4e0a\u4e0b\u6587\uff0c\u540c\u65f6\u901a\u5e38\u63d0\u9ad8\u5e72\u51c0\u51c6\u786e\u6027\u3002\u5206\u6790\u663e\u793a\u5bf9\u6297\u6027\u635f\u574f\u8bf1\u5bfc\u4e86\u6709\u6548\u7684\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5206\u5e03\u5185\u5f15\u5bfc\u5b9e\u73b0\u4e86\u5feb\u901f\u6062\u590d\u5b66\u4e60\u4e14\u8868\u793a\u6f02\u79fb\u6700\u5c0f", "conclusion": "GASP\u65b9\u6cd5\u4ec5\u4f7f\u7528\u7ed3\u679c\u9a8c\u8bc1\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u7b7e\u6216\u5916\u90e8\u6559\u5e08\uff0c\u5c31\u80fd\u6709\u6548\u8bad\u7ec3\u68c0\u6d4b\u548c\u4fee\u590d\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u6a21\u578b\u5728\u9519\u8bef\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2602.00760", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00760", "abs": "https://arxiv.org/abs/2602.00760", "authors": ["Kaiyan Chang", "Chenwei Zhu", "Yingfeng Luo", "Yifu Huo", "Chenglong Wang", "Xiaoqian Liu", "Qiaozhi He", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "title": "APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based Process Rewards", "comment": "Under Review", "summary": "Test-Time Scaling (TTS) has significantly enhanced the capabilities of Large Reasoning Models (LRMs) but introduces a critical side-effect known as Overthinking. We conduct a preliminary study to rethink this phenomenon from a fine-grained perspective. We observe that LRMs frequently conduct repetitive self-verification without revision even after obtaining the final answer during the reasoning process. We formally define this specific position where the answer first stabilizes as the Reasoning Anchor. By analyzing pre- and post-anchor reasoning behaviors, we uncover the structural redundancy fixed in LRMs: the meaningless repetitive verification after deriving the first complete answer, which we term the Answer-Stable Tail (AST). Motivated by this observation, we propose Anchor-based Process Reward (APR), a structure-aware reward shaping method that localizes the reasoning anchor and penalizes exclusively the post-anchor AST. Leveraging the policy optimization algorithm suitable for length penalties, our APR models achieved the performance-efficiency Pareto frontier at 1.5B and 7B scales averaged across five mathematical reasoning datasets while requiring significantly fewer computational resources for RL training.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u951a\u70b9\u8fc7\u7a0b\u5956\u52b1(APR)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u63a8\u7406\u951a\u70b9\u5e76\u60e9\u7f5a\u951a\u70b9\u540e\u7684\u5197\u4f59\u9a8c\u8bc1\uff0c\u89e3\u51b3\u5927\u63a8\u7406\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5b9e\u73b0\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u6269\u5c55(TTS)\u663e\u8457\u63d0\u5347\u4e86\u5927\u63a8\u7406\u6a21\u578b(LRMs)\u7684\u80fd\u529b\uff0c\u4f46\u5f15\u5165\u4e86\"\u8fc7\u5ea6\u601d\u8003\"\u7684\u526f\u4f5c\u7528\u3002\u7814\u7a76\u53d1\u73b0LRMs\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7ecf\u5e38\u8fdb\u884c\u91cd\u590d\u7684\u81ea\u6211\u9a8c\u8bc1\u800c\u4e0d\u4fee\u6b63\u7b54\u6848\uff0c\u8fd9\u79cd\u5197\u4f59\u8ba1\u7b97\u6d6a\u8d39\u8d44\u6e90\u3002", "method": "\u63d0\u51fa\u951a\u70b9\u8fc7\u7a0b\u5956\u52b1(APR)\uff1a1) \u5b9a\u4e49\u63a8\u7406\u951a\u70b9(\u7b54\u6848\u9996\u6b21\u7a33\u5b9a\u7684\u4f4d\u7f6e)\uff1b2) \u8bc6\u522b\u7b54\u6848\u7a33\u5b9a\u5c3e(AST\uff0c\u951a\u70b9\u540e\u7684\u5197\u4f59\u9a8c\u8bc1)\uff1b3) \u4f7f\u7528\u9002\u5408\u957f\u5ea6\u60e9\u7f5a\u7684\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u4e13\u95e8\u60e9\u7f5aAST\u90e8\u5206\u3002", "result": "APR\u65b9\u6cd5\u57281.5B\u548c7B\u89c4\u6a21\u7684\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u5b9e\u73b0\u4e86\u6027\u80fd-\u6548\u7387\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86RL\u8bad\u7ec3\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u7684\u5956\u52b1\u5851\u9020\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2602.02455", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02455", "abs": "https://arxiv.org/abs/2602.02455", "authors": ["Han Bao", "Zheyuan Zhang", "Pengcheng Jing", "Zhengqing Yuan", "Kaiwen Shi", "Yanfang Ye"], "title": "Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction", "comment": "65 pages, 40 figures", "summary": "As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \\textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \\textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \\textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \\MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.", "AI": {"tldr": "Drift-Bench\u662f\u9996\u4e2a\u8bca\u65ad\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u8f6e\u6f84\u6e05\u5728\u72b6\u6001\u5bfc\u5411\u548c\u670d\u52a1\u5bfc\u5411\u6267\u884c\u73af\u5883\u4e2d\u8bc4\u4f30\u4ee3\u7406\u8bed\u7528\u5b66\uff0c\u9488\u5bf9\u7528\u6237\u8f93\u5165\u8fdd\u53cd\u5408\u4f5c\u5047\u8bbe\u7684\u60c5\u51b5\uff0c\u63ed\u793a\u73b0\u6709\u6587\u672c\u8bc4\u4f30\u65e0\u6cd5\u6355\u6349\u7684\u6267\u884c\u98ce\u9669\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5411\u81ea\u4e3b\u4ee3\u7406\u8fc7\u6e21\u65f6\uff0c\u7528\u6237\u8f93\u5165\u7ecf\u5e38\u8fdd\u53cd\u5408\u4f5c\u5047\u8bbe\uff08\u5982\u9690\u542b\u610f\u56fe\u3001\u7f3a\u5931\u53c2\u6570\u3001\u9519\u8bef\u9884\u8bbe\u6216\u6a21\u7cca\u8868\u8fbe\uff09\uff0c\u4ea7\u751f\u6587\u672c\u8bc4\u4f30\u65e0\u6cd5\u6355\u6349\u7684\u6267\u884c\u98ce\u9669\u3002\u73b0\u6709\u57fa\u51c6\u901a\u5e38\u5047\u8bbe\u6307\u4ee4\u660e\u786e\u6216\u4ec5\u9650\u4e8e\u6587\u672c\u5355\u8f6e\u6f84\u6e05\uff0c\u65e0\u6cd5\u8861\u91cf\u5728\u63a5\u5730\u6267\u884c\u98ce\u9669\u4e0b\u7684\u591a\u8f6e\u6d88\u6b67\u3002", "method": "\u57fa\u4e8e\u7ecf\u5178\u6c9f\u901a\u7406\u8bba\uff0cDrift-Bench\u63d0\u4f9b\u7edf\u4e00\u7684\u5408\u4f5c\u6545\u969c\u5206\u7c7b\u6cd5\uff0c\u91c7\u7528\u89d2\u8272\u9a71\u52a8\u7684\u7528\u6237\u6a21\u62df\u5668\u548cRise\u8bc4\u4f30\u534f\u8bae\uff0c\u5728\u72b6\u6001\u5bfc\u5411\u548c\u670d\u52a1\u5bfc\u5411\u6267\u884c\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u8f6e\u6f84\u6e05\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u8fd9\u4e9b\u6545\u969c\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u6f84\u6e05\u6548\u679c\u56e0\u7528\u6237\u89d2\u8272\u548c\u6545\u969c\u7c7b\u578b\u800c\u5f02\u3002\u8be5\u65b9\u6cd5\u8fde\u63a5\u4e86\u6f84\u6e05\u7814\u7a76\u548c\u4ee3\u7406\u5b89\u5168\u8bc4\u4f30\u3002", "conclusion": "Drift-Bench\u80fd\u591f\u7cfb\u7edf\u8bca\u65ad\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5b89\u5168\u6267\u884c\u7684\u6545\u969c\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u7a7a\u767d\uff0c\u4e3a\u4ee3\u7406\u5b89\u5168\u63d0\u4f9b\u91cd\u8981\u8bca\u65ad\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2602.00769", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00769", "abs": "https://arxiv.org/abs/2602.00769", "authors": ["Siyu Yan", "Lusha Zhu", "Jian-Qiao Zhu"], "title": "Eliciting Trustworthiness Priors of Large Language Models via Economic Games", "comment": null, "summary": "One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to characterize the level of trust exhibited by an AI system itself. Here, we propose a novel elicitation method based on iterated in-context learning (Zhu and Griffiths, 2024a) and apply it to elicit trustworthiness priors using the Trust Game from behavioral game theory. The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent, rather than self-reported attitudes. Using our method, we elicit trustworthiness priors from several leading large language models (LLMs) and find that GPT-4.1's trustworthiness priors closely track those observed in humans. Building on this result, we further examine how GPT-4.1 responds to different player personas in the Trust Game, providing an initial characterization of how such models differentiate trust across agent characteristics. Finally, we show that variation in elicited trustworthiness can be well predicted by a stereotype-based model grounded in perceived warmth and competence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fed\u4ee3\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u4fe1\u4efb\u5148\u9a8c\uff0c\u53d1\u73b0GPT-4.1\u7684\u4fe1\u4efb\u5148\u9a8c\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u5e76\u80fd\u57fa\u4e8e\u73a9\u5bb6\u7279\u5f81\u8c03\u6574\u4fe1\u4efb\u884c\u4e3a\u3002", "motivation": "\u6784\u5efa\u53ef\u4fe1\u8d56AI\u7cfb\u7edf\u7684\u5173\u952e\u6311\u6218\u662f\u5982\u4f55\u91cf\u5316AI\u7cfb\u7edf\u672c\u8eab\u8868\u73b0\u51fa\u7684\u4fe1\u4efb\u6c34\u5e73\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u81ea\u6211\u62a5\u544a\u7684\u6001\u5ea6\uff0c\u800c\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u5ba2\u89c2\u7684\u65b9\u6cd5\u6765\u8868\u5f81AI\u7cfb\u7edf\u7684\u4fe1\u4efb\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8fed\u4ee3\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u65b0\u9896\u63d0\u53d6\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u884c\u4e3a\u535a\u5f08\u8bba\u4e2d\u7684\u4fe1\u4efb\u6e38\u620f\u3002\u8be5\u65b9\u6cd5\u5c06\u4fe1\u4efb\u64cd\u4f5c\u5316\u4e3a\u57fa\u4e8e\u5bf9\u53e6\u4e00\u667a\u80fd\u4f53\u4fe1\u5ff5\u7684\u81ea\u613f\u98ce\u9669\u66b4\u9732\u3002\u4f7f\u7528\u8be5\u65b9\u6cd5\u4ece\u591a\u4e2a\u9886\u5148LLM\u4e2d\u63d0\u53d6\u4fe1\u4efb\u5148\u9a8c\uff0c\u5e76\u5206\u6790GPT-4.1\u5bf9\u4e0d\u540c\u73a9\u5bb6\u89d2\u8272\u7684\u54cd\u5e94\u3002", "result": "GPT-4.1\u7684\u4fe1\u4efb\u5148\u9a8c\u4e0e\u4eba\u7c7b\u89c2\u5bdf\u5230\u7684\u4fe1\u4efb\u5148\u9a8c\u9ad8\u5ea6\u4e00\u81f4\u3002GPT-4.1\u80fd\u6839\u636e\u73a9\u5bb6\u7279\u5f81\uff08\u89d2\u8272\uff09\u5dee\u5f02\u5316\u4fe1\u4efb\u884c\u4e3a\u3002\u63d0\u53d6\u7684\u4fe1\u4efb\u53d8\u5316\u53ef\u4ee5\u901a\u8fc7\u57fa\u4e8e\u611f\u77e5\u6e29\u6696\u548c\u80fd\u529b\u7684\u523b\u677f\u5370\u8c61\u6a21\u578b\u5f88\u597d\u5730\u9884\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u53d6LLM\u7684\u4fe1\u4efb\u5148\u9a8c\uff0cGPT-4.1\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u4fe1\u4efb\u6a21\u5f0f\uff0c\u4e14\u5176\u4fe1\u4efb\u884c\u4e3a\u53ef\u901a\u8fc7\u523b\u677f\u5370\u8c61\u7ef4\u5ea6\u89e3\u91ca\uff0c\u4e3a\u7406\u89e3AI\u7cfb\u7edf\u7684\u4fe1\u4efb\u8868\u5f81\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2602.00785", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00785", "abs": "https://arxiv.org/abs/2602.00785", "authors": ["Sherry Yang"], "title": "World Models as an Intermediary between Agents and the Real World", "comment": null, "summary": "Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4f53\u4e0e\u771f\u5b9e\u4e16\u754c\u4e4b\u95f4\u7684\u4e2d\u4ecb\uff0c\u4ee5\u89e3\u51b3\u9ad8\u6210\u672c\u4ea4\u4e92\u9886\u57df\uff08\u5982\u673a\u5668\u4eba\u3001\u79d1\u5b66\u5b9e\u9a8c\uff09\u4e2d\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u9762\u4e34\u7684\u884c\u52a8\u6267\u884c\u6210\u672c\u8fc7\u9ad8\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u5728\u4f4e\u6210\u672c\u73af\u5883\uff08\u6e38\u620f\u3001\u6570\u5b66\u3001\u7f16\u7a0b\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9ad8\u6210\u672c\u4ea4\u4e92\u9886\u57df\uff08\u673a\u5668\u4eba\u7269\u7406\u6210\u672c\u3001ML\u5de5\u7a0b\u65f6\u95f4\u6210\u672c\u3001\u79d1\u5b66\u5b9e\u9a8c\u8d44\u6e90\u6210\u672c\uff09\u8868\u73b0\u4e0d\u4f73\u3002\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u6267\u884c\u884c\u52a8\u83b7\u53d6\u5956\u52b1\u4fe1\u53f7\u7684\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4f53\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u4e2d\u4ecb\uff0c\u5c06\u4e16\u754c\u6a21\u578b\u89c6\u4e3a\u52a8\u6001\u3001\u5956\u52b1\u548c\u4efb\u52a1\u5206\u5e03\u7684\u6a21\u578b\u3002\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u514b\u670d\u9ad8\u6210\u672c\u884c\u52a8\u7684\u57fa\u672c\u969c\u788d\uff0c\u5305\u62ec\u6781\u7aef\u79bb\u7b56\u7565\u5b66\u4e60\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u3002", "result": "\u8bba\u8bc1\u4e86\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u5173\u952e\u4e14\u4e30\u5bcc\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u6db5\u76d6\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u3001\u8ba1\u7b97\u673a\u4f7f\u7528\u3001\u673a\u5668\u4eba\u548cAI\u79d1\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u3002\u63d0\u51fa\u4e86\u6784\u5efa\u4e16\u754c\u6a21\u578b\u7684\u5177\u4f53\u6311\u6218\u548c\u53ef\u64cd\u4f5c\u5efa\u8bae\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u662f\u89e3\u51b3\u9ad8\u6210\u672c\u4ea4\u4e92\u9886\u57df\u667a\u80fd\u4f53\u6027\u80fd\u74f6\u9888\u7684\u5173\u952e\u6280\u672f\uff0c\u9700\u8981\u5728\u6570\u636e\u96c6\u6784\u5efa\u3001\u67b6\u6784\u8bbe\u8ba1\u3001\u6269\u5c55\u548c\u8bc4\u4f30\u7b49\u65b9\u9762\u8fdb\u884c\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.00815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00815", "abs": "https://arxiv.org/abs/2602.00815", "authors": ["Yunjian Zhang", "Sudong Wang", "Yang Li", "Peiran Xu", "Conghao Zhou", "Xiaoyue Ma", "Jianing Li", "Yao Zhu"], "title": "Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement", "comment": null, "summary": "Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDoPR\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u5355\u4e2a\u4fe1\u606f\u4e30\u5bcc\u7684\u8bad\u7ec3\u6837\u672c\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\uff0c\u663e\u8457\u964d\u4f4eRLVR\u8bad\u7ec3\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5728LLM\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8bad\u7ec3\u8fc7\u7a0b\u9700\u8981\u5927\u91cf\u5956\u52b1\u4fe1\u53f7\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u5bfc\u81f4\u6210\u672c\u8fc7\u9ad8\u3002\u9700\u8981\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u6570\u636e\u548c\u8ba1\u7b97\u5229\u7528\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5efa\u7acb\u63a8\u7406\u80fd\u529b\u89e3\u9501\u6240\u9700\u6837\u672c\u590d\u6742\u5ea6\u7684\u7406\u8bba\u4e0b\u754c\uff0c\u7136\u540e\u63d0\u51fa\u52a8\u6001\u5355\u6837\u672c\u7b56\u7565\u7cbe\u70bc\uff08DoPR\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5956\u52b1\u6ce2\u52a8\u6027\u548c\u63a2\u7d22\u9a71\u52a8\u7684\u83b7\u53d6\u7b56\u7565\uff0c\u52a8\u6001\u9009\u62e9\u6bcf\u4e2a\u6279\u6b21\u4e2d\u6700\u5177\u4fe1\u606f\u91cf\u7684\u5355\u4e2a\u8bad\u7ec3\u6837\u672c\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u5b9e\u4f8b\u5373\u53ef\u5b9e\u73b0\u5f3a\u6027\u80fd\u3002DoPR\u5c06\u8bad\u7ec3\u5f00\u9500\u964d\u4f4e\u8fd1\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "DoPR\u4e3aLLM\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u63a8\u7406\u5bc6\u96c6\u578bLLM\u5e94\u7528\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.00845", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00845", "abs": "https://arxiv.org/abs/2602.00845", "authors": ["Senkang Hu", "Yong Dai", "Yuzhi Zhao", "Yihang Tao", "Yu Guo", "Zhengru Fang", "Sam Tak Wu Kwong", "Yuguang Fang"], "title": "Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward", "comment": null, "summary": "Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.", "AI": {"tldr": "InfoReasoner\uff1a\u901a\u8fc7\u8bed\u4e49\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u4f18\u5316\u68c0\u7d22\u8fc7\u7a0b\uff0c\u63d0\u5347\u4ee3\u7406\u63a8\u7406\u80fd\u529b\uff0c\u57287\u4e2aQA\u57fa\u51c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u8fbe5.4%", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u4ee3\u7406\u63a8\u7406\u52a8\u6001\u83b7\u53d6\u5916\u90e8\u77e5\u8bc6\uff0c\u4f46\u68c0\u7d22\u8fc7\u7a0b\u4f18\u5316\u9762\u4e34\u6311\u6218\uff0c\u7f3a\u4e4f\u5bc6\u96c6\u3001\u6709\u539f\u5219\u7684\u5956\u52b1\u4fe1\u53f7", "method": "\u63d0\u51faInfoReasoner\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u8bed\u4e49\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u6fc0\u52b1\u6709\u6548\u4fe1\u606f\u5bfb\u6c42\uff1b\u7406\u8bba\u5c42\u9762\u5c06\u4fe1\u606f\u589e\u76ca\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6a21\u578b\u4fe1\u5ff5\u72b6\u6001\u7684\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\uff1b\u5b9e\u8df5\u5c42\u9762\u63d0\u51fa\u8f93\u51fa\u611f\u77e5\u5185\u5728\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u53cc\u5411\u6587\u672c\u8574\u542b\u7684\u8bed\u4e49\u805a\u7c7b\u76f4\u63a5\u4ece\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u8ba1\u7b97\u4fe1\u606f\u589e\u76ca", "result": "\u5728\u4e03\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInfoReasoner\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe5.4%", "conclusion": "\u4e3a\u5177\u6709\u68c0\u7d22\u529f\u80fd\u7684\u4ee3\u7406\u63a8\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u53ef\u6269\u5c55\u8def\u5f84", "topic": "agent analysis"}}
{"id": "2602.00851", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.00851", "abs": "https://arxiv.org/abs/2602.00851", "authors": ["Hyejun Jeong", "Amir Houmansadr", "Shlomo Zilberstein", "Eugene Bagdasarian"], "title": "Persuasion Propagation in LLM Agents", "comment": "Code available at https://github.com/HyejunJeong/persuasion-propagation", "summary": "Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \\emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\\% fewer searches and visit 16.9\\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.", "AI": {"tldr": "\u7814\u7a76AI\u667a\u80fd\u4f53\u5728\u957f\u671f\u4efb\u52a1\u4e2d\u53d7\u7528\u6237\u8bf4\u670d\u5f71\u54cd\u7684\u884c\u4e3a\u53d8\u5316\uff0c\u53d1\u73b0\u4efb\u52a1\u6267\u884c\u524d\u7684\u4fe1\u5ff5\u9884\u8bbe\u80fd\u663e\u8457\u5f71\u54cd\u540e\u7eed\u884c\u4e3a\uff08\u51cf\u5c1126.9%\u641c\u7d22\u548c16.9%\u8bbf\u95ee\u6e90\uff09\uff0c\u800c\u5b9e\u65f6\u8bf4\u670d\u6548\u679c\u8f83\u5f31\u3002", "motivation": "\u73b0\u4ee3AI\u667a\u80fd\u4f53\u7ed3\u5408\u5bf9\u8bdd\u4ea4\u4e92\u548c\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\uff08\u5982\u7f16\u7801\u548c\u7f51\u7edc\u7814\u7a76\uff09\uff0c\u9700\u8981\u7814\u7a76\u7528\u6237\u8bf4\u670d\u5982\u4f55\u5f71\u54cd\u667a\u80fd\u4f53\u7684\u4e0b\u6e38\u4efb\u52a1\u884c\u4e3a\uff0c\u7279\u522b\u662f\u4fe1\u5ff5\u5c42\u9762\u7684\u5e72\u9884\u6548\u679c\u3002", "method": "\u63d0\u51fa\u884c\u4e3a\u4e2d\u5fc3\u8bc4\u4f30\u6846\u67b6\uff0c\u533a\u5206\u4efb\u52a1\u6267\u884c\u671f\u95f4\u548c\u4e4b\u524d\u7684\u8bf4\u670d\u5e72\u9884\u3002\u5728\u7f51\u7edc\u7814\u7a76\u548c\u7f16\u7801\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u5b9e\u65f6\u8bf4\u670d\u4e0e\u4fe1\u5ff5\u9884\u8bbe\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u65f6\u8bf4\u670d\u5bf9\u884c\u4e3a\u5f71\u54cd\u5f31\u4e14\u4e0d\u4e00\u81f4\uff1b\u800c\u4efb\u52a1\u5f00\u59cb\u65f6\u660e\u786e\u6307\u5b9a\u4fe1\u5ff5\u72b6\u6001\u7684\u9884\u8bbe\u667a\u80fd\u4f53\uff0c\u76f8\u6bd4\u4e2d\u6027\u9884\u8bbe\u667a\u80fd\u4f53\uff0c\u5e73\u5747\u51cf\u5c1126.9%\u7684\u641c\u7d22\u6b21\u6570\u548c16.9%\u7684\u552f\u4e00\u6765\u6e90\u8bbf\u95ee\u3002", "conclusion": "\u8bf4\u670d\uff08\u5373\u4f7f\u662f\u5148\u524d\u4ea4\u4e92\u4e2d\u7684\uff09\u80fd\u591f\u5f71\u54cd\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u8fd9\u5f3a\u8c03\u4e86\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u8fdb\u884c\u884c\u4e3a\u5c42\u9762\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.00887", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00887", "abs": "https://arxiv.org/abs/2602.00887", "authors": ["Gaurav Srivastava", "Aafiya Hussain", "Chi Wang", "Yingyan Celine Lin", "Xuan Wang"], "title": "EffGen: Enabling Small Language Models as Capable Autonomous Agents", "comment": null, "summary": "Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.", "AI": {"tldr": "effGen\u662f\u4e00\u4e2a\u9488\u5bf9\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u7684\u5f00\u6e90\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u4f18\u5316\u3001\u4efb\u52a1\u5206\u89e3\u3001\u590d\u6742\u5ea6\u8def\u7531\u548c\u7edf\u4e00\u5185\u5b58\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u672c\u5730\u90e8\u7f72\uff0c\u572813\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4e3b\u6d41\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u9ad8token\u6210\u672c\u548c\u9690\u79c1\u95ee\u9898\uff0c\u9700\u8981\u4e3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u9ad8\u6548\u3001\u5b89\u5168\u3001\u672c\u5730\u90e8\u7f72\u7684\u667a\u80fd\u4f53\u6846\u67b6\u3002", "method": "1) \u63d0\u793a\u4f18\u5316\u538b\u7f29\u4e0a\u4e0b\u658770-80%\uff1b2) \u667a\u80fd\u4efb\u52a1\u5206\u89e3\u4e3a\u5e76\u884c/\u987a\u5e8f\u5b50\u4efb\u52a1\uff1b3) \u57fa\u4e8e\u4e94\u56e0\u7d20\u7684\u590d\u6742\u5ea6\u8def\u7531\u9884\u6267\u884c\u51b3\u7b56\uff1b4) \u7edf\u4e00\u5185\u5b58\u7cfb\u7edf\uff1b5) \u652f\u6301\u591a\u534f\u8bae\u901a\u4fe1\u3002", "result": "\u572813\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eLangChain\u3001AutoGen\u548cSmolagents\uff0c\u6210\u529f\u7387\u66f4\u9ad8\u3001\u6267\u884c\u66f4\u5feb\u3001\u5185\u5b58\u66f4\u4f4e\u3002\u63d0\u793a\u4f18\u5316\u5bf9\u5c0f\u6a21\u578b\u589e\u76ca\u66f4\u5927(1.5B\u6a21\u578b11.2% vs 32B\u6a21\u578b2.4%)\uff0c\u8def\u7531\u5bf9\u5927\u6a21\u578b\u589e\u76ca\u66f4\u5927(1.5B\u6a21\u578b3.6% vs 32B\u6a21\u578b7.9%)\u3002", "conclusion": "effGen\u4e3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u7684\u672c\u5730\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u63d0\u793a\u4f18\u5316\u548c\u590d\u6742\u5ea6\u8def\u7531\u53ef\u5728\u6240\u6709\u89c4\u6a21\u6a21\u578b\u4e0a\u83b7\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u5df2\u5f00\u6e90\u4f9b\u7814\u7a76\u548c\u5546\u4e1a\u4f7f\u7528\u3002", "topic": "code agent"}}
{"id": "2602.00282", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00282", "abs": "https://arxiv.org/abs/2602.00282", "authors": ["Naman Saxena", "Vaneet Aggarwal"], "title": "Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning", "comment": null, "summary": "Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(\u03b5^{-2})$ and sample complexity of $\\tilde{O}(\u03b5^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u53cc\u5c42\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5CBSO\uff0c\u4f7f\u7528\u60e9\u7f5a\u51fd\u6570\u5904\u7406\u7ea6\u675f\uff0c\u901a\u8fc7Moreau\u5305\u7edc\u5206\u6790\u975e\u5149\u6ed1\u4f18\u5316\uff0c\u83b7\u5f97\u4e86O(\u03b5^{-2})\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\u548c\u00d5(\u03b5^{-4})\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "motivation": "\u5143\u5b66\u4e60\u3001\u5206\u5c42\u5b66\u4e60\u548c\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u7b49\u8bb8\u591a\u91cd\u8981RL\u95ee\u9898\u90fd\u53ef\u4ee5\u5efa\u6a21\u4e3a\u53cc\u5c42RL\u95ee\u9898\u3002\u867d\u7136\u8fd9\u4e9b\u9886\u57df\u5728\u5b9e\u8bc1\u4e0a\u53d6\u5f97\u4e86\u5f88\u5927\u8fdb\u5c55\uff0c\u4f46\u53cc\u5c42RL\u7b97\u6cd5\u7684\u7406\u8bba\u5206\u6790\u5c1a\u672a\u5f97\u5230\u8db3\u591f\u5173\u6ce8\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u7ea6\u675f\u53cc\u5c42RL\u7b97\u6cd5\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51fa\u7ea6\u675f\u53cc\u5c42\u6b21\u68af\u5ea6\u4f18\u5316\u7b97\u6cd5(CBSO)\uff0c\u4f7f\u7528\u60e9\u7f5a\u51fd\u6570\u76ee\u6807\u51fd\u6570\u907f\u514d\u7ea6\u675f\u53cc\u5c42\u95ee\u9898\u4e2d\u7684\u539f\u59cb-\u5bf9\u5076\u95f4\u9699\u548c\u8d85\u68af\u5ea6\u95ee\u9898\u3002\u91c7\u7528Moreau\u5305\u7edc\u5206\u6790\u4e00\u822c\u53c2\u6570\u5316\u7b56\u7565\u68af\u5ea6RL\u7b97\u6cd5\u4e2d\u7684\u975e\u5149\u6ed1\u76ee\u6807\u51fd\u6570\u3002", "result": "\u83b7\u5f97\u4e86O(\u03b5^{-2})\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\u548c\u00d5(\u03b5^{-4})\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u8fd9\u662f\u9996\u6b21\u4f7f\u7528Moreau\u5305\u7edc\u5206\u6790\u5177\u6709\u975e\u5149\u6ed1\u76ee\u6807\u51fd\u6570\u7684\u4e00\u822c\u53c2\u6570\u5316\u7b56\u7565\u68af\u5ea6RL\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u7ea6\u675f\u53cc\u5c42RL\u7b97\u6cd5\u7406\u8bba\u5206\u6790\u7684\u7a7a\u767d\uff0c\u4e3a\u5143\u5b66\u4e60\u3001\u5206\u5c42\u5b66\u4e60\u548cRL-HF\u7b49\u53cc\u5c42RL\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u8bc1\u660e\u4e86\u60e9\u7f5a\u51fd\u6570\u65b9\u6cd5\u548cMoreau\u5305\u7edc\u5728\u5206\u6790\u975e\u5149\u6ed1\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.00911", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00911", "abs": "https://arxiv.org/abs/2602.00911", "authors": ["Abhijit Chakraborty", "Sandipan De", "Yash Shah", "Chahana Dahal", "Vivek Gupta"], "title": "Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs", "comment": null, "summary": "Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.", "AI": {"tldr": "Synapse\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3LLM\u667a\u80fd\u4f53\u7684\u5171\u4eab\u5168\u5c40\u5de5\u5177\u4f7f\u7528\u77e5\u8bc6\u6a21\u578b\uff0c\u901a\u8fc7\u672c\u5730\u5b66\u4e60\u3001\u8054\u90a6\u805a\u5408\u548c\u5168\u5c40\u5de5\u5177\u5e93\u66f4\u65b0\uff0c\u63d0\u9ad8\u5de5\u5177\u4f7f\u7528\u6548\u679c\u5e76\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u9762\u4e34\u901a\u4fe1\u6210\u672c\u9ad8\u3001\u6570\u636e\u548c\u5de5\u5177\u4f7f\u7528\u5f02\u8d28\u6027\u7b49\u6311\u6218\uff0c\u9650\u5236\u4e86\u534f\u4f5c\u5b66\u4e60\u7684\u6548\u679c\u3002", "method": "\u4f7f\u7528\u6a21\u677f\u5316\u8868\u793a\u3001\u5d4c\u5165\u68c0\u7d22\u4e0eLLM\u91cd\u6392\u5e8f\u3001\u81ea\u9002\u5e94\u63a9\u7801\u7b49\u6280\u672f\uff0c\u8bad\u7ec3\u5171\u4eab\u7684\u5168\u5c40\u5de5\u5177\u4f7f\u7528\u77e5\u8bc6\u6a21\u578b\u3002\u5ba2\u6237\u7aef\u667a\u80fd\u4f53\u5728\u672c\u5730\u5b66\u4e60\u5de5\u5177\u4f7f\u7528\u6a21\u5f0f\uff0c\u901a\u8fc7\u534f\u8c03\u5668\u4f20\u8f93\u5de5\u4ef6\u8fdb\u884c\u8054\u90a6\u805a\u5408\uff0c\u66f4\u65b0\u5168\u5c40\u5de5\u5177\u5e93\u5e76\u91cd\u65b0\u5206\u53d1\u3002", "result": "Synapse\u76f8\u6bd4\u6743\u91cd\u6216\u63d0\u793a\u5171\u4eab\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5de5\u5177\u4f7f\u7528\u6548\u679c\u5e76\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u5728\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "Synapse\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0bLLM\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u5b66\u4e60\u6311\u6218\uff0c\u901a\u8fc7\u5168\u5c40\u77e5\u8bc6\u5171\u4eab\u63d0\u5347\u5de5\u5177\u4f7f\u7528\u6548\u679c\uff0c\u540c\u65f6\u63a7\u5236\u901a\u4fe1\u6210\u672c\u3002", "topic": "agent analysis"}}
{"id": "2602.00970", "categories": ["cs.CL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.00970", "abs": "https://arxiv.org/abs/2602.00970", "authors": ["Saaduddin Mahmud", "Eugene Bagdasarian", "Shlomo Zilberstein"], "title": "Verification Required: The Impact of Information Credibility on AI Persuasion", "comment": "19 pages, 5 figures", "summary": "Agents powered by large language models (LLMs) are increasingly deployed in settings where communication shapes high-stakes decisions, making a principled understanding of strategic communication essential. Prior work largely studies either unverifiable cheap-talk or fully verifiable disclosure, failing to capture realistic domains in which information has probabilistic credibility. We introduce MixTalk, a strategic communication game for LLM-to-LLM interaction that models information credibility. In MixTalk, a sender agent strategically combines verifiable and unverifiable claims to communicate private information, while a receiver agent allocates a limited budget to costly verification and infers the underlying state from prior beliefs, claims, and verification outcomes. We evaluate state-of-the-art LLM agents in large-scale tournaments across three realistic deployment settings, revealing their strengths and limitations in reasoning about information credibility and the explicit behavior that shapes these interactions. Finally, we propose Tournament Oracle Policy Distillation (TOPD), an offline method that distills tournament oracle policy from interaction logs and deploys it in-context at inference time. Our results show that TOPD significantly improves receiver robustness to persuasion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMixTalk\u535a\u5f08\u6846\u67b6\u7814\u7a76LLM\u4ee3\u7406\u5728\u6982\u7387\u53ef\u4fe1\u5ea6\u4fe1\u606f\u4e0b\u7684\u7b56\u7565\u901a\u4fe1\uff0c\u5f00\u53d1TOPD\u65b9\u6cd5\u63d0\u5347\u63a5\u6536\u8005\u6297\u8bf4\u670d\u80fd\u529b", "motivation": "LLM\u4ee3\u7406\u5728\u901a\u4fe1\u5f71\u54cd\u9ad8\u98ce\u9669\u51b3\u7b56\u7684\u573a\u666f\u4e2d\u65e5\u76ca\u90e8\u7f72\uff0c\u9700\u8981\u7406\u89e3\u7b56\u7565\u901a\u4fe1\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e0d\u53ef\u9a8c\u8bc1\u7684\u5ec9\u4ef7\u8c08\u8bdd\u6216\u5b8c\u5168\u53ef\u9a8c\u8bc1\u7684\u62ab\u9732\uff0c\u672a\u80fd\u6355\u6349\u4fe1\u606f\u5177\u6709\u6982\u7387\u53ef\u4fe1\u5ea6\u7684\u73b0\u5b9e\u9886\u57df\u3002", "method": "\u63d0\u51faMixTalk\u7b56\u7565\u901a\u4fe1\u535a\u5f08\u6846\u67b6\uff0c\u53d1\u9001\u8005\u7b56\u7565\u6027\u7ec4\u5408\u53ef\u9a8c\u8bc1\u548c\u4e0d\u53ef\u9a8c\u8bc1\u58f0\u660e\uff0c\u63a5\u6536\u8005\u5206\u914d\u6709\u9650\u9884\u7b97\u8fdb\u884c\u6210\u672c\u9a8c\u8bc1\u3002\u8bc4\u4f30\u6700\u5148\u8fdbLLM\u4ee3\u7406\u5728\u4e09\u79cd\u73b0\u5b9e\u90e8\u7f72\u8bbe\u7f6e\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51faTOPD\u79bb\u7ebf\u65b9\u6cd5\u4ece\u4ea4\u4e92\u65e5\u5fd7\u4e2d\u63d0\u53d6\u9526\u6807\u8d5b\u6700\u4f18\u7b56\u7565\u3002", "result": "\u5927\u89c4\u6a21\u9526\u6807\u8d5b\u8bc4\u4f30\u63ed\u793a\u4e86LLM\u4ee3\u7406\u5728\u63a8\u7406\u4fe1\u606f\u53ef\u4fe1\u5ea6\u548c\u5851\u9020\u4ea4\u4e92\u7684\u663e\u5f0f\u884c\u4e3a\u65b9\u9762\u7684\u4f18\u52bf\u548c\u5c40\u9650\u3002TOPD\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u63a5\u6536\u8005\u5bf9\u8bf4\u670d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MixTalk\u6846\u67b6\u4e3a\u7814\u7a76LLM\u4ee3\u7406\u5728\u6982\u7387\u53ef\u4fe1\u5ea6\u4fe1\u606f\u4e0b\u7684\u7b56\u7565\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0cTOPD\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u63a5\u6536\u8005\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u9c81\u68d2\u901a\u4fe1\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.00929", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00929", "abs": "https://arxiv.org/abs/2602.00929", "authors": ["Zergham Ahmed", "Kazuki Irie", "Joshua B. Tenenbaum", "Christopher J. Bates", "Samuel J. Gershman"], "title": "Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents", "comment": "20 pages", "summary": "Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.", "AI": {"tldr": "TheoryCoder-2\u662f\u4e00\u4e2a\u57fa\u4e8e\u7406\u8bba\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u4e3b\u52a8\u5b66\u4e60\u53ef\u91cd\u7528\u62bd\u8c61\uff0c\u800c\u975e\u4f9d\u8d56\u4eba\u5de5\u6307\u5b9a\u7684\u62bd\u8c61\uff0c\u4ece\u800c\u5728\u591a\u6837\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u5728\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u6548\u89c4\u5212\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u7406\u8bba\u7684\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\uff08\u5982TheoryCoder\uff09\u867d\u7136\u901a\u8fc7\u62bd\u8c61\u5b9e\u73b0\u5f3a\u6cdb\u5316\uff0c\u4f46\u4e25\u91cd\u4f9d\u8d56\u4eba\u5de5\u63d0\u4f9b\u7684\u62bd\u8c61\uff0c\u56de\u907f\u4e86\u62bd\u8c61\u5b66\u4e60\u95ee\u9898\u3002", "method": "TheoryCoder-2\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u4ece\u7ecf\u9a8c\u4e2d\u4e3b\u52a8\u5408\u6210\u53ef\u91cd\u7528\u62bd\u8c61\uff0c\u5e76\u5c06\u8fd9\u4e9b\u62bd\u8c61\u96c6\u6210\u5230\u5206\u5c42\u89c4\u5212\u8fc7\u7a0b\u4e2d\uff0c\u5f62\u6210\u57fa\u4e8e\u7406\u8bba\u7684\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u3002", "result": "\u5728BabyAI\u3001Minihack\u548cVGDL\u6e38\u620f\uff08\u5982Sokoban\uff09\u7b49\u591a\u6837\u73af\u5883\u4e2d\uff0cTheoryCoder-2\u6bd4\u57fa\u7ebfLLM\u4ee3\u7406\uff08\u5305\u62ec\u7ecf\u5178\u89c4\u5212\u57df\u6784\u5efa\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u89c4\u5212\u4ee5\u53caWorldCoder\u7b49\u7a0b\u5e8f\u5408\u6210\u4ee3\u7406\uff09\u5177\u6709\u663e\u8457\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\uff0c\u80fd\u591f\u89e3\u51b3\u57fa\u7ebf\u65e0\u6cd5\u5b8c\u6210\u7684\u590d\u6742\u4efb\u52a1\uff0c\u4e14\u4ec5\u9700\u6700\u5c11\u7684\u4eba\u5de5\u63d0\u793a\u3002", "conclusion": "TheoryCoder-2\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u62bd\u8c61\u800c\u975e\u4f9d\u8d56\u4eba\u5de5\u6307\u5b9a\u62bd\u8c61\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5148\u524d\u57fa\u4e8e\u7406\u8bba\u7684\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff0c\u4e3a\u667a\u80fd\u4ee3\u7406\u7684\u62bd\u8c61\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.00951", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00951", "abs": "https://arxiv.org/abs/2602.00951", "authors": ["Hector Munoz-Avila", "David W. Aha", "Paola Rizzo"], "title": "R-HTN: Rebellious Online HTN Planning for Safety and Game AI", "comment": null, "summary": "We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \\D. Like other agents that are capable of rebellion (i.e., {\\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act in ways that do not meet a user's expectations. Our work combines three concepts: HTN planning, online planning, and the directives \\D, which must be considered when performing user-assigned tasks. We investigate two agent variants: (1) a Nonadaptive agent that stops execution if it finds itself in violation of \\D~ and (2) an Adaptive agent that, in the same situation, instead modifies its HTN plan to search for alternative ways to achieve its given task. We present R-HTN (for: Rebellious-HTN), a general algorithm for online HTN planning under directives \\D. We evaluate R-HTN in two task domains where the agent must not violate some directives for safety reasons or as dictated by their personality traits. We found that R-HTN agents never violate directives, and aim to achieve the user-given goals if feasible though not necessarily as the user expected.", "AI": {"tldr": "\u63d0\u51faR-HTN\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u5206\u5c42\u4efb\u52a1\u7f51\u7edc\u89c4\u5212\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u5728\u8fdd\u53cd\u5185\u7f6e\u6307\u4ee4\u65f6\u62d2\u7edd\u6267\u884c\u7528\u6237\u4efb\u52a1\u6216\u81ea\u9002\u5e94\u8c03\u6574\u8ba1\u5212", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u901a\u5e38\u76f2\u76ee\u6267\u884c\u7528\u6237\u4efb\u52a1\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u9700\u8981\u62d2\u7edd\u67d0\u4e9b\u4efb\u52a1\uff08\u5982\u8fdd\u53cd\u5b89\u5168\u89c4\u5b9a\u6216\u4eba\u683c\u7279\u8d28\uff09\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u667a\u80fd\u53cd\u6297\u7684\u5728\u7ebf\u89c4\u5212\u667a\u80fd\u4f53\u3002", "method": "\u7ed3\u5408HTN\u89c4\u5212\u3001\u5728\u7ebf\u89c4\u5212\u548c\u5185\u7f6e\u6307\u4ee4D\uff0c\u63d0\u51faR-HTN\u7b97\u6cd5\u3002\u5f00\u53d1\u4e24\u79cd\u53d8\u4f53\uff1a\u975e\u81ea\u9002\u5e94\u667a\u80fd\u4f53\uff08\u8fdd\u53cd\u6307\u4ee4\u65f6\u505c\u6b62\u6267\u884c\uff09\u548c\u81ea\u9002\u5e94\u667a\u80fd\u4f53\uff08\u8fdd\u53cd\u6307\u4ee4\u65f6\u4fee\u6539HTN\u8ba1\u5212\u5bfb\u627e\u66ff\u4ee3\u65b9\u6848\uff09\u3002", "result": "R-HTN\u667a\u80fd\u4f53\u4ece\u4e0d\u8fdd\u53cd\u6307\u4ee4\uff0c\u5728\u53ef\u884c\u60c5\u51b5\u4e0b\u4f1a\u5c1d\u8bd5\u5b9e\u73b0\u7528\u6237\u76ee\u6807\uff08\u4f46\u4e0d\u4e00\u5b9a\u6309\u7528\u6237\u9884\u671f\u65b9\u5f0f\uff09\u3002\u5728\u5b89\u5168\u76f8\u5173\u548c\u4eba\u683c\u7279\u8d28\u76f8\u5173\u7684\u4efb\u52a1\u57df\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "R-HTN\u7b97\u6cd5\u4e3a\u5728\u7ebfHTN\u89c4\u5212\u63d0\u4f9b\u4e86\u667a\u80fd\u53cd\u6297\u80fd\u529b\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u5728\u8003\u8651\u5b89\u5168\u89c4\u5b9a\u548c\u4eba\u683c\u7279\u8d28\u7684\u60c5\u51b5\u4e0b\u505a\u51fa\u66f4\u5408\u7406\u7684\u51b3\u7b56\u3002", "topic": "agent analysis"}}
{"id": "2602.00996", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00996", "abs": "https://arxiv.org/abs/2602.00996", "authors": ["Abhijit Chakraborty", "Ashish Raj Shekhar", "Shiven Agarwal", "Vivek Gupta"], "title": "DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework", "comment": null, "summary": "Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.", "AI": {"tldr": "DeALOG\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u95ee\u7b54\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u667a\u80fd\u4f53\uff08\u8868\u683c\u3001\u4e0a\u4e0b\u6587\u3001\u89c6\u89c9\u3001\u603b\u7ed3\u548c\u9a8c\u8bc1\uff09\u5728\u5171\u4eab\u81ea\u7136\u8bed\u8a00\u65e5\u5fd7\u4e0a\u8fdb\u884c\u534f\u4f5c\uff0c\u5b9e\u73b0\u9519\u8bef\u68c0\u6d4b\u548c\u9a8c\u8bc1\u3002", "motivation": "\u8de8\u6587\u672c\u3001\u8868\u683c\u548c\u56fe\u50cf\u7684\u590d\u6742\u95ee\u7b54\u9700\u8981\u6574\u5408\u591a\u6837\u5316\u4fe1\u606f\u6e90\uff0c\u9700\u8981\u4e00\u4e2a\u652f\u6301\u4e13\u4e1a\u5316\u5904\u7406\u3001\u534f\u8c03\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faDeALOG\u6846\u67b6\uff0c\u4f7f\u7528\u4e13\u95e8\u7684\u667a\u80fd\u4f53\uff08\u8868\u683c\u3001\u4e0a\u4e0b\u6587\u3001\u89c6\u89c9\u3001\u603b\u7ed3\u548c\u9a8c\u8bc1\uff09\uff0c\u901a\u8fc7\u5171\u4eab\u81ea\u7136\u8bed\u8a00\u65e5\u5fd7\u4f5c\u4e3a\u6301\u4e45\u5185\u5b58\u8fdb\u884c\u901a\u4fe1\uff0c\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u7684\u534f\u4f5c\u548c\u9a8c\u8bc1\u3002", "result": "\u5728FinQA\u3001TAT-QA\u3001CRT-QA\u3001WikiTableQuestions\u3001FeTaQA\u548cMultiModalQA\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u5206\u6790\u786e\u8ba4\u4e86\u5171\u4eab\u65e5\u5fd7\u3001\u667a\u80fd\u4f53\u4e13\u4e1a\u5316\u548c\u9a8c\u8bc1\u5bf9\u51c6\u786e\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "DeALOG\u901a\u8fc7\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u7684\u6a21\u5757\u5316\u7ec4\u4ef6\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u95ee\u7b54\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.00998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00998", "abs": "https://arxiv.org/abs/2602.00998", "authors": ["Zhikun Xu", "Xiaodong Yu", "Ben Zhou", "Jiang Liu", "Jialian Wu", "Ze Wang", "Ximeng Sun", "Hao Chen", "Zicheng Liu"], "title": "Reliable Use of Lemmas via Eligibility Reasoning and Section$-$Aware Reinforcement Learning", "comment": null, "summary": "Recent large language models (LLMs) perform strongly on mathematical benchmarks yet often misapply lemmas, importing conclusions without validating assumptions. We formalize lemma$-$judging as a structured prediction task: given a statement and a candidate lemma, the model must output a precondition check and a conclusion$-$utility check, from which a usefulness decision is derived. We present RULES, which encodes this specification via a two$-$section output and trains with reinforcement learning plus section$-$aware loss masking to assign penalty to the section responsible for errors. Training and evaluation draw on diverse natural language and formal proof corpora; robustness is assessed with a held$-$out perturbation suite; and end$-$to$-$end evaluation spans competition$-$style, perturbation$-$aligned, and theorem$-$based problems across various LLMs. Results show consistent in$-$domain gains over both a vanilla model and a single$-$label RL baseline, larger improvements on applicability$-$breaking perturbations, and parity or modest gains on end$-$to$-$end tasks; ablations indicate that the two$-$section outputs and section$-$aware reinforcement are both necessary for robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRULES\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\u8bad\u7ec3LLMs\u8fdb\u884c\u5f15\u7406\u5224\u65ad\uff0c\u4f7f\u7528\u4e24\u6bb5\u5f0f\u8f93\u51fa\u548c\u5206\u6bb5\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u5347\u6570\u5b66\u63a8\u7406\u7684\u7a33\u5065\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7ecf\u5e38\u9519\u8bef\u5e94\u7528\u5f15\u7406\uff0c\u5728\u4e0d\u9a8c\u8bc1\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\u76f4\u63a5\u5bfc\u5165\u7ed3\u8bba\u3002\u9700\u8981\u63d0\u5347\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u6b63\u786e\u5224\u65ad\u5f15\u7406\u9002\u7528\u6027\u7684\u80fd\u529b\u3002", "method": "\u5c06\u5f15\u7406\u5224\u65ad\u5f62\u5f0f\u5316\u4e3a\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\uff1a\u7ed9\u5b9a\u9648\u8ff0\u548c\u5019\u9009\u5f15\u7406\uff0c\u6a21\u578b\u5fc5\u987b\u8f93\u51fa\u524d\u63d0\u6761\u4ef6\u68c0\u67e5\u548c\u7ed3\u8bba\u6548\u7528\u68c0\u67e5\u3002\u63d0\u51faRULES\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e24\u6bb5\u5f0f\u8f93\u51fa\u7f16\u7801\u89c4\u8303\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u52a0\u5206\u6bb5\u611f\u77e5\u635f\u5931\u63a9\u7801\u8fdb\u884c\u8bad\u7ec3\uff0c\u5bf9\u9519\u8bef\u90e8\u5206\u65bd\u52a0\u60e9\u7f5a\u3002", "result": "\u5728\u9886\u57df\u5185\u4efb\u52a1\u4e0a\u76f8\u6bd4\u666e\u901a\u6a21\u578b\u548c\u5355\u6807\u7b7eRL\u57fa\u7ebf\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u5728\u9002\u7528\u6027\u7834\u574f\u6270\u52a8\u4e0a\u6539\u8fdb\u66f4\u5927\uff0c\u5728\u7aef\u5230\u7aef\u4efb\u52a1\u4e0a\u8fbe\u5230\u6301\u5e73\u6216\u9002\u5ea6\u63d0\u5347\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u4e24\u6bb5\u5f0f\u8f93\u51fa\u548c\u5206\u6bb5\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u5bf9\u7a33\u5065\u6027\u90fd\u662f\u5fc5\u8981\u7684\u3002", "conclusion": "RULES\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u5f15\u7406\u5224\u65ad\u4efb\u52a1\u548c\u5206\u6bb5\u611f\u77e5\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u4e86LLMs\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u6b63\u786e\u5e94\u7528\u5f15\u7406\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6270\u52a8\u548c\u9a8c\u8bc1\u524d\u63d0\u6761\u4ef6\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u7a33\u5065\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.00994", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00994", "abs": "https://arxiv.org/abs/2602.00994", "authors": ["Yu Li", "Mingyang Yi", "Xiuyu Li", "Ju Fan", "Fuxin Jiang", "Binbin Chen", "Peng Li", "Jie Song", "Tieying Zhang"], "title": "Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning", "comment": null, "summary": "Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u7ebf\u6027\u6548\u5e94\u5f52\u56e0\u7cfb\u7edf(LEAS)\u63ed\u793a\u4e86\u63a8\u7406\u4e0e\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u4e4b\u95f4\u7684\u8bad\u7ec3\u5e72\u6270\uff0c\u5e76\u63d0\u51fa\u89e3\u8026\u884c\u52a8\u63a8\u7406\u8c03\u4f18(DART)\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u7684\u4f4e\u79e9\u9002\u914d\u6a21\u5757\u663e\u5f0f\u89e3\u8026\u53c2\u6570\u66f4\u65b0\uff0c\u5728\u5355\u6a21\u578b\u4e2d\u5b9e\u73b0\u4f18\u4e8e\u57fa\u7ebf6.35%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709ARL\u65b9\u6cd5\u901a\u5e38\u8bad\u7ec3\u5355\u4e00\u5171\u4eab\u6a21\u578b\u53c2\u6570\u6765\u652f\u6301\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\uff0c\u9690\u542b\u5047\u8bbe\u8054\u5408\u8bad\u7ec3\u80fd\u63d0\u5347\u6574\u4f53\u4ee3\u7406\u6027\u80fd\u3002\u4f46\u8fd9\u4e00\u5047\u8bbe\u5f88\u5c11\u88ab\u5b9e\u8bc1\u68c0\u9a8c\uff0c\u4f5c\u8005\u65e8\u5728\u7cfb\u7edf\u7814\u7a76\u8fd9\u4e00\u5047\u8bbe\u7684\u6709\u6548\u6027\u3002", "method": "1. \u5f15\u5165\u7ebf\u6027\u6548\u5e94\u5f52\u56e0\u7cfb\u7edf(LEAS)\u91cf\u5316\u63a8\u7406\u4e0e\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u4e4b\u95f4\u7684\u5e72\u6270\uff1b2. \u63d0\u51fa\u89e3\u8026\u884c\u52a8\u63a8\u7406\u8c03\u4f18(DART)\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u7684\u4f4e\u79e9\u9002\u914d\u6a21\u5757\u663e\u5f0f\u89e3\u8026\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7684\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1. LEAS\u63d0\u4f9b\u4e86\u63a8\u7406\u4e0e\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u5e72\u6270\u7684\u5b9a\u91cf\u8bc1\u636e\uff1b2. DART\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u63d0\u53476.35%\uff1b3. DART\u4f7f\u7528\u5355\u6a21\u578b\u5b9e\u73b0\u4e86\u4e0e\u663e\u5f0f\u5206\u79bb\u5de5\u5177\u4f7f\u7528\u548c\u63a8\u7406\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u63a8\u7406\u4e0e\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u5728\u8054\u5408\u8bad\u7ec3\u4e2d\u5b58\u5728\u68af\u5ea6\u65b9\u5411\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u5e72\u6270\uff0c\u6311\u6218\u4e86\u73b0\u6709ARL\u8303\u5f0f\u3002DART\u901a\u8fc7\u53c2\u6570\u89e3\u8026\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3aARL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.00333", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00333", "abs": "https://arxiv.org/abs/2602.00333", "authors": ["Parmida Davarmanesh", "Ashia Wilson", "Adityanarayanan Radhakrishnan"], "title": "Efficient and accurate steering of Large Language Models through attention-guided feature learning", "comment": null, "summary": "Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.", "AI": {"tldr": "\u63d0\u51fa\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u5f15\u5bfc\u6846\u67b6\uff0c\u89e3\u51b3LLM\u5185\u90e8\u6fc0\u6d3b\u64cd\u7eb5\u7684\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u5728512\u4e2a\u8bed\u4e49\u6982\u5ff5\u57fa\u51c6\u4e0a\u6027\u80fd\u663e\u8457\u63d0\u5347", "motivation": "\u73b0\u6709\u5f15\u5bfc\u65b9\u6cd5\u975e\u5e38\u8106\u5f31\uff0c\u6982\u5ff5\u7684\u53ef\u5f15\u5bfc\u6027\u53d6\u51b3\u4e8e\u7ec6\u5fae\u7684\u7b97\u6cd5\u9009\u62e9\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5f15\u5bfc\u6846\u67b6\u6765\u7406\u89e3LLM\u4e2d\u8bed\u4e49\u6982\u5ff5\u7684\u5b58\u50a8\u65b9\u5f0f\u5e76\u63d0\u5347LLM\u80fd\u529b", "method": "\u5f15\u5165\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u5f15\u5bfc\u6846\u67b6\uff0c\u81ea\u52a8\u9009\u62e9\u76f8\u5173token\u5d4c\u5165\u63d0\u53d6\u6982\u5ff5\u7279\u5f81\uff0c\u8003\u8651\u6982\u5ff5\u7279\u5f81\u5728LLM\u6fc0\u6d3b\u4e2d\u7684\u5f02\u8d28\u6027\uff0c\u8bc6\u522b\u6700\u76f8\u5173\u7684\u5f15\u5bfc\u5c42", "result": "\u5728512\u4e2a\u8bed\u4e49\u6982\u5ff5\u57fa\u51c6\u4e0a\uff0c\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u5148\u524dSOTA\u65b9\u6cd5\uff08\u6210\u529f\u5f15\u5bfc\u6982\u5ff5\u6570\u91cf\u51e0\u4e4e\u7ffb\u500d\uff09\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u67b6\u6784\u548c\u5927\u5c0f\u7684\u6a21\u578b\uff08\u6700\u9ad8700\u4ebf\u53c2\u6570\uff09", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f00\u53d1\u9ad8\u6548\u3001\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684\u884c\u4e1a\u7ea7LLM\u5fae\u8c03\u7b97\u6cd5\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u63ed\u793a\u4e86\u6982\u5ff5\u7279\u5b9a\u7279\u5f81\u5728LLM\u5c42\u95f4\u7684\u5206\u5e03\u89c4\u5f8b", "topic": "agent analysis"}}
{"id": "2602.01034", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01034", "abs": "https://arxiv.org/abs/2602.01034", "authors": ["Xiangwei Wang", "Wei Wang", "Ken Chen", "Nanduni Nimalsiri", "Saman Halgamuge"], "title": "Discovering Process-Outcome Credit in Multi-Step LLM Reasoning", "comment": null, "summary": "Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8fb9\u9645\u4fe1\u606f\u589e\u76ca\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6b65\u8fdb\u5f0f\u5956\u52b1\u4fe1\u53f7\u548c\u53bb\u8026\u63a9\u7801\u7b56\u7565\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6587\u672c\u548c\u591a\u6a21\u6001\u57fa\u51c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5956\u52b1\u7a00\u758f\u6027\u548c\u4fe1\u7528\u5206\u914d\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u5956\u52b1\u4fe1\u53f7\u6765\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "1) \u6b65\u8fdb\u5f0f\u8fb9\u9645\u4fe1\u606f\u589e\u76ca\u673a\u5236\uff0c\u91cf\u5316\u63a8\u7406\u6b65\u9aa4\u76f8\u5bf9\u4e8e\u5355\u8c03\u5386\u53f2\u6c34\u5370\u7684\u5185\u5728\u4ef7\u503c\uff1b2) \u53bb\u8026\u63a9\u7801\u7b56\u7565\uff0c\u5c06\u8fc7\u7a0b\u5bfc\u5411\u5956\u52b1\u5e94\u7528\u4e8e\u601d\u7ef4\u94fe\uff0c\u7ed3\u679c\u5bfc\u5411\u5956\u52b1\u5e94\u7528\u4e8e\u5b8c\u6574\u5b8c\u6210\uff1b3) \u53cc\u95e8\u76d1\u7763\u5fae\u8c03\u76ee\u6807\uff0c\u7528\u9ad8\u8d28\u91cf\u7ed3\u6784\u548c\u4e8b\u5b9e\u4fe1\u53f7\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728MATH\u3001Super-CLEVR\u7b49\u6587\u672c\u548c\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u51c6\u786e\u7387\u4e0a\u5747\u4f18\u4e8eGRPO\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u5206\u5e03\u5916\u9c81\u68d2\u6027\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u8fde\u7eed\u5956\u52b1\u4fe1\u53f7\u548c\u7cbe\u7ec6\u4fe1\u7528\u5206\u914d\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5177\u6709\u5f88\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01075", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01075", "abs": "https://arxiv.org/abs/2602.01075", "authors": ["Yepeng Liu", "Yu Huang", "Yu-Xiang Wang", "Yingbin Liang", "Yuheng Bu"], "title": "ConvexBench: Can LLMs Recognize Convex Functions?", "comment": null, "summary": "Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \\cb, a scalable and mechanically verifiable benchmark for testing \\textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \\textit{parsing failure} and \\textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6d4b\u8bd5LLMs\u5728\u6df1\u5ea6\u51fd\u6570\u7ec4\u5408\u4e0b\u8bc6\u522b\u51f8\u6027\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5CB\uff0c\u53d1\u73b0\u524d\u6cbfLLMs\u5b58\u5728\u7ec4\u5408\u63a8\u7406\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5206\u6cbb\u7684\u4ee3\u7406\u6846\u67b6\u6765\u6709\u6548\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLMs\u5f00\u59cb\u81ea\u52a8\u5316\u7814\u7a76\u7ea7\u6570\u5b66\u548c\u79d1\u5b66\u4efb\u52a1\uff0c\u9700\u8981\u8bc4\u4f30\u5b83\u4eec\u7406\u89e3\u548c\u63a8\u7406\u51f8\u6027\u7684\u80fd\u529b\u3002\u51f8\u5206\u6790\u662f\u73b0\u4ee3\u6570\u5b66\u7684\u91cd\u8981\u5206\u652f\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u56e0\u6b64\u6d4b\u8bd5LLMs\u5728\u6df1\u5ea6\u51fd\u6570\u7ec4\u5408\u4e0b\u8bc6\u522b\u51f8\u6027\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86CB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u7b26\u53f7\u76ee\u6807\u51fd\u6570\u6df1\u5ea6\u7ec4\u5408\u4e0b\u7684\u51f8\u6027\u8bc6\u522b\u80fd\u529b\u3002\u5b9e\u9a8c\u53d1\u73b0\u524d\u6cbfLLMs\u5b58\u5728\u7ec4\u5408\u63a8\u7406\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4e86\u4ee3\u7406\u5206\u6cbb\u6846\u67b6\uff1a1) \u4f7f\u7528\u5916\u90e8\u5de5\u5177\u89e3\u6790\u6784\u5efa\u62bd\u8c61\u8bed\u6cd5\u6811(AST)\uff1b2) \u5bf9\u6bcf\u4e2a\u4e2d\u95f4\u5b50\u8868\u8fbe\u5f0f\u8fdb\u884c\u9012\u5f52\u63a8\u7406\uff0c\u5e76\u805a\u7126\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u524d\u6cbfLLMs\u5b58\u5728\u660e\u663e\u7684\u7ec4\u5408\u63a8\u7406\u7f3a\u9677\uff1a\u968f\u7740\u6df1\u5ea6\u589e\u52a0\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u4ece\u6df1\u5ea62\u65f6\u7684F1\u5206\u65701.0\u964d\u81f3\u6df1\u5ea6100\u65f6\u7684\u7ea60.2\u3002\u5206\u6790\u53d1\u73b0\u4e24\u79cd\u5931\u8d25\u6a21\u5f0f\uff1a\u89e3\u6790\u5931\u8d25\u548c\u61d2\u60f0\u63a8\u7406\u3002\u63d0\u51fa\u7684\u4ee3\u7406\u5206\u6cbb\u6846\u67b6\u80fd\u53ef\u9760\u7f13\u89e3\u6df1\u5ea6\u7ec4\u5408\u5931\u8d25\uff0c\u5728\u8f83\u5927\u6df1\u5ea6\u4e0b\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff08\u5982\u6df1\u5ea6100\u65f6F1\u5206\u6570=1.0\uff09\u3002", "conclusion": "LLMs\u5728\u6df1\u5ea6\u51fd\u6570\u7ec4\u5408\u4e0b\u7684\u51f8\u6027\u8bc6\u522b\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u4f46\u901a\u8fc7\u4ee3\u7406\u5206\u6cbb\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4e3aLLMs\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.01070", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01070", "abs": "https://arxiv.org/abs/2602.01070", "authors": ["Ahsan Bilal", "Ahmed Mohsin", "Muhammad Umer", "Ali Subhan", "Hassan Rizwan", "Ayesha Mohsin", "Dean Hougen"], "title": "What If We Allocate Test-Time Compute Adaptively?", "comment": null, "summary": "Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u52a8\u6001\u6307\u5bfc\u63a8\u7406\u8f68\u8ff9\u7684\u751f\u6210\u548c\u9009\u62e9\uff0c\u663e\u8457\u4f18\u4e8e\u5747\u5300\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u95ee\u9898\uff1a1\uff09\u5747\u5300\u5206\u914d\u63a8\u7406\u8ba1\u7b97\u8d44\u6e90\uff1b2\uff09\u4f7f\u7528\u56fa\u5b9a\u7684\u91c7\u6837\u7b56\u7565\uff1b3\uff09\u4ec5\u5c06\u9a8c\u8bc1\u7528\u4e8e\u91cd\u65b0\u6392\u5e8f\u3002\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u6839\u636e\u95ee\u9898\u96be\u5ea6\u548c\u63a8\u7406\u8fc7\u7a0b\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u3002", "method": "\u63d0\u51fa\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u89c6\u4e3a\u8fed\u4ee3\u7684\u8f68\u8ff9\u751f\u6210\u548c\u9009\u62e9\u8fc7\u7a0b\u3002\u6bcf\u4e2a\u95ee\u9898\u8fd0\u884c\u591a\u6b21\u63a8\u7406\u8fed\u4ee3\uff0c\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff1a1\uff09\u53ef\u9009\u751f\u6210\u9ad8\u7ea7\u8ba1\u5212\uff1b2\uff09\u9009\u62e9\u63a8\u7406\u5de5\u5177\u96c6\u548c\u8ba1\u7b97\u7b56\u7565\uff1b3\uff09\u751f\u6210\u5019\u9009\u63a8\u7406\u8f68\u8ff9\u3002\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u4f5c\u4e3a\u7edf\u4e00\u63a7\u5236\u4fe1\u53f7\uff1a\u5728\u8fed\u4ee3\u5185\uff0c\u6b65\u9aa4\u7ea7PRM\u5206\u6570\u7528\u4e8e\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u526a\u679d\u548c\u6269\u5c55\uff1b\u5728\u8fed\u4ee3\u95f4\uff0c\u805a\u5408\u7684\u8f68\u8ff9\u5956\u52b1\u7528\u4e8e\u9009\u62e9\u6700\u7ec8\u54cd\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u52a8\u6001PRM\u5f15\u5bfc\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u76f4\u63a5\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u3002\u5728MATH-500\u4e0a\u53d6\u5f97\u663e\u8457\u589e\u76ca\uff0c\u5728AIME24\u548cAMO-Bench\u7b49\u66f4\u96be\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u6570\u500d\u6539\u8fdb\u3002\u901a\u8fc7\u7406\u8bbaFLOPs\u548c\u8ba1\u7b97\u5f3a\u5ea6\u6307\u6807\u8bc1\u660e\uff0c\u9a8c\u8bc1\u5f15\u5bfc\u7684\u5206\u914d\u5c06\u8ba1\u7b97\u96c6\u4e2d\u5728\u9ad8\u6548\u7528\u63a8\u7406\u8def\u5f84\u4e0a\u3002", "conclusion": "\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u548c\u667a\u80fd\u8f68\u8ff9\u9009\u62e9\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5728\u6307\u5bfc\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.01119", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01119", "abs": "https://arxiv.org/abs/2602.01119", "authors": ["Konstantin Chernyshev", "Ekaterina Artemova", "Viacheslav Zhukov", "Maksim Nerush", "Mariia Fedorova", "Iryna Repik", "Olga Shapovalova", "Aleksey Sukhorosov", "Vladimir Dobrovolskii", "Natalia Mikhailova", "Sergei Tilga"], "title": "Tendem: A Hybrid AI+Human Platform", "comment": null, "summary": "Tendem is a hybrid system where AI handles structured, repeatable work and Human Experts step in when the models fail or to verify results. Each result undergoes a comprehensive quality review before delivery to the Client. To assess Tendem's performance, we conducted a series of in-house evaluations on 94 real-world tasks, comparing it with AI-only agents and human-only workflows carried out by Upwork freelancers. The results show that Tendem consistently delivers higher-quality outputs with faster turnaround times. At the same time, its operational costs remain comparable to human-only execution. On third-party agentic benchmarks, Tendem's AI Agent (operating autonomously, without human involvement) performs near state-of-the-art on web browsing and tool-use tasks while demonstrating strong results in frontier domain knowledge and reasoning.", "AI": {"tldr": "Tendem\u662f\u4e00\u4e2a\u6df7\u5408\u7cfb\u7edf\uff0cAI\u5904\u7406\u7ed3\u6784\u5316\u91cd\u590d\u5de5\u4f5c\uff0c\u4eba\u7c7b\u4e13\u5bb6\u5728\u6a21\u578b\u5931\u8d25\u65f6\u4ecb\u5165\u6216\u9a8c\u8bc1\u7ed3\u679c\uff0c\u6240\u6709\u7ed3\u679c\u4ea4\u4ed8\u524d\u90fd\u7ecf\u8fc7\u5168\u9762\u8d28\u91cf\u5ba1\u67e5\u3002\u8bc4\u4f30\u663e\u793a\u5176\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\u6bd4\u7eafAI\u4ee3\u7406\u548c\u7eaf\u4eba\u5de5\u5de5\u4f5c\u6d41\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7eaf\u4eba\u5de5\u5de5\u4f5c\u6d41\u5219\u6548\u7387\u8f83\u4f4e\u3002\u9700\u8981\u7ed3\u5408AI\u81ea\u52a8\u5316\u548c\u4eba\u7c7b\u4e13\u4e1a\u5224\u65ad\u7684\u4f18\u52bf\uff0c\u6784\u5efa\u4e00\u4e2a\u65e2\u80fd\u4fdd\u8bc1\u8d28\u91cf\u53c8\u80fd\u63d0\u9ad8\u6548\u7387\u7684\u6df7\u5408\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1Tendem\u6df7\u5408\u7cfb\u7edf\uff1aAI\u5904\u7406\u7ed3\u6784\u5316\u91cd\u590d\u6027\u5de5\u4f5c\uff0c\u4eba\u7c7b\u4e13\u5bb6\u5728AI\u5931\u8d25\u65f6\u4ecb\u5165\u6216\u9a8c\u8bc1\u7ed3\u679c\u3002\u6240\u6709\u8f93\u51fa\u90fd\u7ecf\u8fc7\u5168\u9762\u8d28\u91cf\u5ba1\u67e5\u540e\u624d\u4ea4\u4ed8\u7ed9\u5ba2\u6237\u3002\u901a\u8fc794\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u8fdb\u884c\u5185\u90e8\u8bc4\u4f30\uff0c\u4e0e\u7eafAI\u4ee3\u7406\u548cUpwork\u81ea\u7531\u804c\u4e1a\u8005\u7684\u4eba\u5de5\u5de5\u4f5c\u6d41\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "Tendem\u572894\u4e2a\u771f\u5b9e\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff1a1\uff09\u59cb\u7ec8\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\u548c\u66f4\u5feb\u7684\u5468\u8f6c\u65f6\u95f4\uff1b2\uff09\u8fd0\u8425\u6210\u672c\u4e0e\u7eaf\u4eba\u5de5\u6267\u884c\u76f8\u5f53\uff1b3\uff09\u5176\u7eafAI\u4ee3\u7406\u5728\u7b2c\u4e09\u65b9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u7f51\u9875\u6d4f\u89c8\u548c\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e0a\u63a5\u8fd1SOTA\uff0c\u5728\u9886\u57df\u77e5\u8bc6\u548c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u5f3a\u52b2\u3002", "conclusion": "Tendem\u6df7\u5408\u7cfb\u7edf\u6210\u529f\u7ed3\u5408\u4e86AI\u81ea\u52a8\u5316\u548c\u4eba\u7c7b\u4e13\u4e1a\u5224\u65ad\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u8bc1\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u4e14\u6210\u672c\u53ef\u63a7\u3002\u8fd9\u79cdAI-\u4eba\u7c7b\u534f\u4f5c\u6a21\u5f0f\u4e3a\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.00408", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00408", "abs": "https://arxiv.org/abs/2602.00408", "authors": ["Seung Heon Oh", "Jiwon Baek", "Ki Young Cho", "Hee Chang Yoon", "Jong Hun Woo"], "title": "Variational Approach for Job Shop Scheduling", "comment": null, "summary": "This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.", "AI": {"tldr": "VG2S\u6846\u67b6\u9996\u6b21\u5c06\u53d8\u5206\u63a8\u7406\u5f15\u5165JSSP\u9886\u57df\uff0c\u901a\u8fc7\u53d8\u5206\u56fe\u7f16\u7801\u5668\u89e3\u8026\u8868\u793a\u5b66\u4e60\u548c\u7b56\u7565\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728JSSP\u4e2d\u9762\u4e34\u8bad\u7ec3\u975e\u5e73\u7a33\u6027\u548c\u5bf9\u672a\u89c1\u95ee\u9898\u5b9e\u4f8b\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u540c\u65f6\u4f18\u5316\u8868\u793a\u5b66\u4e60\u548c\u7b56\u7565\u6267\u884c", "method": "\u63d0\u51fa\u53d8\u5206\u56fe\u5230\u8c03\u5ea6\u5668\u6846\u67b6\uff0c\u57fa\u4e8eELBO\u548c\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u63a8\u5bfc\u6982\u7387\u76ee\u6807\uff0c\u901a\u8fc7\u53d8\u5206\u56fe\u7f16\u7801\u5668\u6570\u5b66\u89e3\u8026\u8868\u793a\u5b66\u4e60\u548c\u7b56\u7565\u4f18\u5316", "result": "\u5728DMU\u548cSWV\u7b49\u5927\u89c4\u6a21\u6311\u6218\u6027\u57fa\u51c6\u5b9e\u4f8b\u4e0a\uff0cVG2S\u8868\u73b0\u51fa\u4f18\u4e8e\u6700\u5148\u8fdbDRL\u57fa\u7ebf\u548c\u4f20\u7edf\u8c03\u5ea6\u89c4\u5219\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b", "conclusion": "VG2S\u6846\u67b6\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u6709\u6548\u89e3\u51b3\u4e86JSSP\u4e2d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u5236\u9020\u8c03\u5ea6\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2602.01167", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01167", "abs": "https://arxiv.org/abs/2602.01167", "authors": ["Zhiming Liu", "Yujie Wei", "Lei Feng", "Xiu Su", "Xiaobo Xia", "Weili Guan", "Zeke Xie", "Shuo Yang"], "title": "Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models", "comment": null, "summary": "Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e2d\u5b58\u5728\u4efb\u52a1\u5e72\u6270\u5c42\uff0c\u8fd9\u4e9b\u5c42\u4f1a\u635f\u5bb3\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u901a\u8fc7\u5c42\u5e72\u9884\u5b9e\u9a8c\uff0c\u4f5c\u8005\u63d0\u51fa\u4efb\u52a1\u5c42\u4ea4\u4e92\u5411\u91cf\u6765\u91cf\u5316\u5c42\u5bf9\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u4efb\u52a1\u81ea\u9002\u5e94\u5c42\u5254\u9664\u65b9\u6cd5TaLo\uff0c\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8bc6\u522b\u5e76\u7ed5\u8fc7\u5e72\u6270\u5c42\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524dVLM\u901a\u5e38\u9ed8\u8ba4\u4f7f\u7528\u6240\u6709\u5c42\u8fdb\u884c\u4e0b\u6e38\u4efb\u52a1\u9884\u6d4b\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u5c42\u53cd\u800c\u4f1a\u963b\u788d\u7279\u5b9a\u4efb\u52a1\u7684\u6027\u80fd\u3002\u4f5c\u8005\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u63a2\u7a76\u5355\u4e2a\u5c42\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u5e76\u5229\u7528\u8fd9\u4e00\u53d1\u73b0\u5f00\u53d1\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c42\u5e72\u9884\u5b9e\u9a8c\uff08\u5982\u5c06\u7279\u5b9a\u5c42\u53c2\u6570\u7f6e\u96f6\uff09\u6d4b\u91cf\u6027\u80fd\u53d8\u5316\uff0c\u63d0\u51fa\u4efb\u52a1\u5c42\u4ea4\u4e92\u5411\u91cf\u91cf\u5316\u5c42\u5bf9\u4efb\u52a1\u7684\u5f71\u54cd\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1TaLo\u65b9\u6cd5\uff1a\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8bc6\u522b\u5e76\u7ed5\u8fc7\u5bf9\u5f53\u524d\u4efb\u52a1\u5e72\u6270\u6700\u5927\u7684\u5c42\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u4efb\u52a1\u5e72\u6270\u5c42\u666e\u904d\u5b58\u5728\uff0c\u4e14\u5177\u6709\u4efb\u52a1\u7279\u5b9a\u7684\u654f\u611f\u6027\u6a21\u5f0f\u3002TaLo\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5982\u5728ScienceQA\u7684Maps\u4efb\u52a1\u4e0a\u5c06Qwen-VL\u7684\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe16.6%\u3002", "conclusion": "\u9884\u8bad\u7ec3VLM\u4e2d\u5b58\u5728\u610f\u5916\u7684\u6a21\u5757\u5316\u7279\u6027\uff0cTaLo\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u63a8\u7406\u65f6\u89e3\u9501\u6a21\u578b\u7684\u9690\u85cf\u80fd\u529b\uff0c\u4e3aVLM\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2602.01204", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01204", "abs": "https://arxiv.org/abs/2602.01204", "authors": ["Xuqin Zhang", "Quan He", "Zhenrui Zheng", "Zongzhang Zhang", "Xu He", "Dong Li"], "title": "ASTER: Agentic Scaling with Tool-integrated Extended Reasoning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a dominant paradigm for eliciting long-horizon reasoning in Large Language Models (LLMs). However, scaling Tool-Integrated Reasoning (TIR) via RL remains challenging due to interaction collapse: a pathological state where models fail to sustain multi-turn tool usage, instead degenerating into heavy internal reasoning with only trivial, post-hoc code verification. We systematically study three questions: (i) how cold-start SFT induces an agentic, tool-using behavioral prior, (ii) how the interaction density of cold-start trajectories shapes exploration and downstream RL outcomes, and (iii) how the RL interaction budget affects learning dynamics and generalization under varying inference-time budgets. We then introduce ASTER (Agentic Scaling with Tool-integrated Extended Reasoning), a framework that circumvents this collapse through a targeted cold-start strategy prioritizing interaction-dense trajectories. We find that a small expert cold-start set of just 4K interaction-dense trajectories yields the strongest downstream performance, establishing a robust prior that enables superior exploration during extended RL training. Extensive evaluations demonstrate that ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025, surpassing leading frontier open-source models, including DeepSeek-V3.2-Exp.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faASTER\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u4e92\u5bc6\u96c6\u7684\u51b7\u542f\u52a8\u7b56\u7565\u89e3\u51b3RL\u8bad\u7ec3\u4e2d\u5de5\u5177\u96c6\u6210\u63a8\u7406\u7684\u4ea4\u4e92\u5d29\u6e83\u95ee\u9898\uff0c\u4f7f4B\u6a21\u578b\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728LLMs\u7684\u957f\u7a0b\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5de5\u5177\u96c6\u6210\u63a8\u7406\u7684RL\u6269\u5c55\u9762\u4e34\"\u4ea4\u4e92\u5d29\u6e83\"\u95ee\u9898\uff1a\u6a21\u578b\u9000\u5316\u4e3a\u5927\u91cf\u5185\u90e8\u63a8\u7406\u800c\u5f88\u5c11\u4f7f\u7528\u5de5\u5177\uff0c\u4ec5\u8fdb\u884c\u7b80\u5355\u7684\u540e\u9a8c\u4ee3\u7801\u9a8c\u8bc1\u3002", "method": "\u63d0\u51faASTER\u6846\u67b6\uff0c\u91c7\u7528\u4ea4\u4e92\u5bc6\u96c6\u7684\u51b7\u542f\u52a8\u7b56\u7565\uff0c\u4ec5\u97004K\u6761\u4ea4\u4e92\u5bc6\u96c6\u8f68\u8ff9\u5c31\u80fd\u5efa\u7acb\u5f3a\u5927\u7684\u884c\u4e3a\u5148\u9a8c\uff0c\u652f\u6301\u540e\u7eedRL\u8bad\u7ec3\u4e2d\u7684\u6709\u6548\u63a2\u7d22\u3002", "result": "ASTER-4B\u5728\u7ade\u4e89\u6027\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u5728AIME 2025\u4e0a\u8fbe\u523090.0%\uff0c\u8d85\u8d8a\u4e86\u5305\u62ecDeepSeek-V3.2-Exp\u5728\u5185\u7684\u9886\u5148\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "\u4ea4\u4e92\u5bc6\u96c6\u7684\u51b7\u542f\u52a8\u7b56\u7565\u80fd\u6709\u6548\u907f\u514d\u4ea4\u4e92\u5d29\u6e83\uff0c\u5c11\u91cf\u9ad8\u8d28\u91cf\u4e13\u5bb6\u8f68\u8ff9\u5c31\u80fd\u5efa\u7acb\u5f3a\u5927\u7684\u5de5\u5177\u4f7f\u7528\u5148\u9a8c\uff0c\u4f7f\u5c0f\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5927\u6a21\u578b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01171", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.01171", "abs": "https://arxiv.org/abs/2602.01171", "authors": ["Stefan Szeider"], "title": "ASP-Bench: From Natural Language to Logic Programs", "comment": null, "summary": "Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.\n  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.\n  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.", "AI": {"tldr": "ASP-Bench\u662f\u4e00\u4e2a\u5305\u542b128\u4e2a\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u5b9e\u4f8b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u8f6c\u6362\u4e3a\u7b54\u6848\u96c6\u7a0b\u5e8f\uff08ASP\uff09\u7684\u7cfb\u7edf\u3002\u5b83\u7cfb\u7edf\u8986\u76d6\u4e86ASP\u7684\u5404\u79cd\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eReAct\u6846\u67b6\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u5c55\u793a\u4e86\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u4f18\u5316\u5728ASP\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u81ea\u52a8\u8f6c\u6362\u4e3a\u903b\u8f91\u7a0b\u5e8f\u662f\u795e\u7ecf\u7b26\u53f7\u5de5\u7a0b\u4e2d\u7684\u4e00\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u3002\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u79cd\u8f6c\u6362\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7b54\u6848\u96c6\u7a0b\u5e8f\uff08ASP\uff09\u8fd9\u79cd\u91cd\u8981\u903b\u8f91\u7f16\u7a0b\u5f62\u5f0f\u3002", "method": "1. \u521b\u5efaASP-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b128\u4e2a\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u5b9e\u4f8b\uff0864\u4e2a\u57fa\u7840\u95ee\u9898\uff0c\u6bcf\u4e2a\u6709\u7b80\u5355\u548c\u56f0\u96be\u53d8\u4f53\uff09\n2. \u7cfb\u7edf\u8986\u76d6ASP\u7279\u6027\uff1a\u9009\u62e9\u89c4\u5219\u3001\u805a\u5408\u3001\u4f18\u5316\u7b49\n3. \u6bcf\u4e2a\u95ee\u9898\u5305\u542b\u53c2\u8003\u9a8c\u8bc1\u5668\uff0c\u7528\u4e8e\u68c0\u67e5\u89e3\u51b3\u65b9\u6848\u662f\u5426\u7b26\u5408\u89c4\u8303\n4. \u4ece7\u4e2a\u63a8\u7406\u7ef4\u5ea6\uff08\u4f18\u5316\u3001\u65f6\u5e8f\u63a8\u7406\u3001\u9ed8\u8ba4\u903b\u8f91\u3001\u8d44\u6e90\u5206\u914d\u3001\u9012\u5f52\u3001\u7a7a\u95f4\u63a8\u7406\u3001\u5b9a\u91cf\u590d\u6742\u5ea6\uff09\u5bf9\u95ee\u9898\u8fdb\u884c\u7279\u5f81\u5206\u6790\n5. \u4f7f\u7528\u57fa\u4e8eReAct\u6846\u67b6\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u8fdb\u884c\u6d4b\u8bd5\uff0c\u91c7\u7528\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u4f18\u5316", "result": "1. \u57fa\u4e8eReAct\u6846\u67b6\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b8c\u5168\u9971\u548c\uff08full saturation\uff09\uff0c\u8868\u660e\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u4f18\u5316\u662f\u53ef\u9760\u4e14\u9c81\u68d2\u7684ASP\u5efa\u6a21\u65b9\u6cd5\n2. \u901a\u8fc7\u591a\u6b21\u667a\u80fd\u4f53\u8fd0\u884c\u7684\u5206\u6790\uff0c\u83b7\u5f97\u4e86\u5173\u4e8e\u95ee\u9898\u5efa\u6a21\u96be\u5ea6\u7684\u6d1e\u5bdf\n3. \u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u591a\u7ef4\u5ea6\u7684\u5efa\u6a21\u96be\u5ea6\u89c6\u56fe", "conclusion": "ASP-Bench\u4e3a\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u5230ASP\u7684\u8f6c\u6362\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u57fa\u4e8eReAct\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u5c55\u793a\u4e86\u53cd\u9988\u9a71\u52a8\u8fed\u4ee3\u4f18\u5316\u5728\u89e3\u51b3\u8fd9\u7c7b\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7406\u89e3\u95ee\u9898\u5efa\u6a21\u96be\u5ea6\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2602.01198", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01198", "abs": "https://arxiv.org/abs/2602.01198", "authors": ["Liang Zhang", "Yu Zhao", "Longyue Wang", "Tianqi Shi", "Weihua Luo", "Kaifu Zhang", "Jinsong Su"], "title": "A State-Transition Framework for Efficient LLM Reasoning", "comment": "ICLR 2026", "summary": "While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u6548\u63a8\u7406\u6846\u67b6\uff0c\u5c06LLM\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u72b6\u6001\u8f6c\u79fb\u8fc7\u7a0b\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u63d0\u5347\u63a8\u7406\u6027\u80fd", "motivation": "\u4f20\u7edf\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u867d\u7136\u80fd\u63d0\u5347LLM\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4f46\u751f\u6210\u957f\u63a8\u7406\u5e8f\u5217\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u538b\u7f29\u63a8\u7406\u5e8f\u5217\u6765\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u8fd9\u4e0e\u6d4b\u8bd5\u65f6\u6269\u5c55\u76f8\u51b2\u7a81\uff0c\u9650\u5236\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b", "method": "1. \u5c06LLM\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u72b6\u6001\u8f6c\u79fb\u8fc7\u7a0b\uff1b2. \u4f7f\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u4f30\u8ba1\u63a8\u7406\u72b6\u6001\uff0c\u8bb0\u5f55\u5386\u53f2\u63a8\u7406\u4fe1\u606f\uff1b3. \u57fa\u4e8e\u67e5\u8be2\u63d0\u793a\u548c\u63a8\u7406\u72b6\u6001\uff0cLLM\u9ad8\u6548\u6267\u884c\u5f53\u524d\u63a8\u7406\u6b65\u9aa4\u5e76\u66f4\u65b0\u72b6\u6001\uff1b4. \u63d0\u51fa\u57fa\u4e8e\u72b6\u6001\u7684\u63a8\u7406\u7b56\u7565\u7f13\u89e3\u566a\u58f0\u63a8\u7406\u6b65\u9aa4\u5bfc\u81f4\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86LLM\u7684\u63a8\u7406\u6548\u7387\uff08\u6ce8\u610f\u529b\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u4e8c\u6b21\u964d\u4e3a\u7ebf\u6027\uff09\uff0c\u8fd8\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u9ad8\u6548\u63a8\u7406\u6846\u67b6\u901a\u8fc7\u72b6\u6001\u8f6c\u79fb\u5efa\u6a21\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u957f\u94fe\u63a8\u7406\u7684\u8ba1\u7b97\u6210\u672c\u95ee\u9898", "topic": "agent analysis"}}
{"id": "2602.01202", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01202", "abs": "https://arxiv.org/abs/2602.01202", "authors": ["Mingze Kong", "Zikun Qu", "Zhongquan Zhou", "Pengyu Liang", "Xiang Li", "Zhiwei Shang", "Zhi Hong", "Kaiyu Huang", "Zhiyong Wang", "Zhongxiang Dai"], "title": "Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction", "comment": null, "summary": "The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.", "AI": {"tldr": "Workflow-R1\u5c06\u5de5\u4f5c\u6d41\u6784\u5efa\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u8f6e\u81ea\u7136\u8bed\u8a00\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7GSsPO\u7b97\u6cd5\u89e3\u51b3\u4f18\u5316\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u591a\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u6d41\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u5c06\u5de5\u4f5c\u6d41\u5408\u6210\u89c6\u4e3a\u9759\u6001\u3001\u4e00\u6b21\u6027\u7684\u4ee3\u7801\u751f\u6210\u95ee\u9898\uff0c\u8fd9\u8fc7\u5ea6\u7ea6\u675f\u4e86\u6a21\u578b\u7684\u7f16\u7801\u80fd\u529b\uff0c\u9650\u5236\u4e86\u52a8\u6001\u95ee\u9898\u89e3\u51b3\u6240\u9700\u7684\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51faWorkflow-R1\u6846\u67b6\uff0c\u5c06\u5de5\u4f5c\u6d41\u6784\u5efa\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u8f6e\u81ea\u7136\u8bed\u8a00\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\u3002\u5f15\u5165Group Sub-sequence Policy Optimization (GSsPO)\u7b97\u6cd5\uff0c\u5c06\u4f18\u5316\u5355\u5143\u91cd\u65b0\u6821\u51c6\u4e3a\u590d\u5408\u5b50\u5e8f\u5217\uff08\u539f\u5b50Think-Action\u5faa\u73af\uff09\uff0c\u4f7f\u68af\u5ea6\u66f4\u65b0\u4e0e\u4ea4\u4e92\u7684\u8bed\u4e49\u8fb9\u754c\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWorkflow-R1\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86GSsPO\u4f5c\u4e3a\u987a\u5e8f\u63a8\u7406\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "Workflow-R1\u4e3a\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b0\u8303\u5f0f\uff0cGSsPO\u4f5c\u4e3a\u7ed3\u6784\u611f\u77e5\u7684RL\u7b97\u6cd5\u53ef\u63a8\u5e7f\u5230\u5e7f\u6cdb\u7684\u591a\u8f6e\u667a\u80fd\u4f53\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01244", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01244", "abs": "https://arxiv.org/abs/2602.01244", "authors": ["Siwei Wu", "Yizhi Li", "Yuyang Song", "Wei Zhang", "Yang Wang", "Riza Batista-Navarro", "Xian Yang", "Mingjie Tang", "Bryan Dai", "Jian Yang", "Chenghua Lin"], "title": "Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments", "comment": "Agentic Trajectory, Agentic Model, Terminal, Code Agent", "summary": "Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \\textbf{\\emph{Executability}}, since each instance requires a suitable and often distinct Docker environment; and \\textbf{\\emph{Verifiability}}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose \\textbf{TerminalTraj}, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\\% on TB~1.0 and 10\\% on TB~2.0 over their respective backbones. Notably, \\textbf{TerminalTraj-32B} achieves strong performance among models with fewer than 100B parameters, reaching 35.30\\% on TB~1.0 and 22.00\\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.", "AI": {"tldr": "TerminalTraj\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7ec8\u7aef\u8f68\u8ff9\u751f\u6210\u7ba1\u9053\uff0c\u901a\u8fc7Docker\u73af\u5883\u6784\u5efa\u3001\u4efb\u52a1\u5b9e\u4f8b\u751f\u6210\u548c\u53ef\u6267\u884c\u9a8c\u8bc1\u4ee3\u7801\u5408\u6210\uff0c\u521b\u5efa\u4e865\u4e07\u591a\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u7ec8\u7aef\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec8\u7aef\u4efb\u52a1\u4ee3\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u8bad\u7ec3\u7ec8\u7aef\u4efb\u52a1\u4ee3\u7406\u6a21\u578b\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u7ec8\u7aef\u8f68\u8ff9\u6570\u636e\uff0c\u4f46\u5927\u89c4\u6a21\u6784\u5efa\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u53ef\u6267\u884c\u6027\uff08\u9700\u8981\u5408\u9002\u7684Docker\u73af\u5883\uff09\u548c\u53ef\u9a8c\u8bc1\u6027\uff08\u5f02\u6784\u4efb\u52a1\u8f93\u51fa\u96be\u4ee5\u7edf\u4e00\u9a8c\u8bc1\uff09\u3002", "method": "\u63d0\u51faTerminalTraj\u7ba1\u9053\uff1a1\uff09\u7b5b\u9009\u9ad8\u8d28\u91cf\u4ed3\u5e93\u6784\u5efaDocker\u5316\u6267\u884c\u73af\u5883\uff1b2\uff09\u751f\u6210\u4e0eDocker\u5bf9\u9f50\u7684\u4efb\u52a1\u5b9e\u4f8b\uff1b3\uff09\u5408\u6210\u5e26\u6709\u53ef\u6267\u884c\u9a8c\u8bc1\u4ee3\u7801\u7684\u4ee3\u7406\u8f68\u8ff9\u3002", "result": "\u6784\u5efa\u4e8632K\u4e2aDocker\u955c\u50cf\u548c50,733\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u7ec8\u7aef\u8f68\u8ff9\uff0c\u8986\u76d68\u4e2a\u9886\u57df\u3002\u57fa\u4e8eQwen2.5-Coder\u8bad\u7ec3\u7684\u6a21\u578b\u5728TerminalBench\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff1aTB 1.0\u63d0\u534720%\uff0cTB 2.0\u63d0\u534710%\u3002TerminalTraj-32B\u5728100B\u53c2\u6570\u4ee5\u4e0b\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TerminalTraj\u6210\u529f\u89e3\u51b3\u4e86\u7ec8\u7aef\u8f68\u8ff9\u6570\u636e\u6784\u5efa\u7684\u53ef\u6267\u884c\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u6311\u6218\uff0c\u4e3a\u8bad\u7ec3\u7ec8\u7aef\u4efb\u52a1\u4ee3\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.00453", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.00453", "abs": "https://arxiv.org/abs/2602.00453", "authors": ["Ziyao Wang", "Daeun Jung", "Yexiao He", "Guoheng Sun", "Zheyu Shen", "Myungjin Lee", "Ang Li"], "title": "FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.", "AI": {"tldr": "FedMOA\u662f\u4e00\u4e2a\u8054\u90a6GRPO\u6846\u67b6\uff0c\u7528\u4e8e\u5f02\u6784\u5956\u52b1\u4e0b\u7684\u591a\u76ee\u6807\u5bf9\u9f50\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6743\u91cd\u8c03\u6574\u548c\u4efb\u52a1\u611f\u77e5\u805a\u5408\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u8054\u90a6\u5e73\u5747\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfRL\u5bf9\u9f50\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u9700\u8981\u7ef4\u62a4\u5355\u72ec\u7684critic\u7f51\u7edc\uff0c\u5185\u5b58\u6d88\u8017\u5927\uff0c\u4e0d\u9002\u5408\u8bbe\u5907\u7aef\u8bad\u7ec3\u3002GRPO\u7684\u65e0critic\u67b6\u6784\u9002\u5408\u8bbe\u5907\u7aef\u8bad\u7ec3\uff0c\u4f46\u5728\u8054\u90a6\u8bbe\u7f6e\u4e0b\u9762\u4e34\u5f02\u6784\u5956\u52b1\u5b9a\u4e49\u3001\u4e0d\u5e73\u8861\u591a\u76ee\u6807\u4f18\u5316\u548c\u9ad8\u8bad\u7ec3\u6210\u672c\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faFedMOA\u6846\u67b6\uff1a1) \u672c\u5730\u8bad\u7ec3\u4f7f\u7528\u57fa\u4e8e\u8d85\u68af\u5ea6\u4e0b\u964d\u7684\u5728\u7ebf\u81ea\u9002\u5e94\u6743\u91cd\u673a\u5236\uff0c\u4f18\u5148\u8003\u8651\u4e3b\u8981\u63a8\u7406\u76ee\u6807\uff1b2) \u670d\u52a1\u5668\u7aef\u4f7f\u7528\u4efb\u52a1\u548c\u51c6\u786e\u7387\u611f\u77e5\u7684\u805a\u5408\u7b56\u7565\uff0c\u4f18\u5148\u8003\u8651\u9ad8\u8d28\u91cf\u66f4\u65b0\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFedMOA\u59cb\u7ec8\u4f18\u4e8e\u8054\u90a6\u5e73\u5747\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe2.2%\uff0c\u540c\u65f6\u6539\u5584\u4e86\u5168\u5c40\u6027\u80fd\u3001\u4e2a\u6027\u5316\u548c\u591a\u76ee\u6807\u5e73\u8861\u3002", "conclusion": "FedMOA\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6GRPO\u4e2d\u7684\u5f02\u6784\u5956\u52b1\u548c\u591a\u76ee\u6807\u4f18\u5316\u6311\u6218\uff0c\u4e3a\u8bbe\u5907\u7aef\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01237", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01237", "abs": "https://arxiv.org/abs/2602.01237", "authors": ["Katrina Brown", "Aneesh Muppidi", "Rana Shahout"], "title": "Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models", "comment": "ICML ES-FoMo 2025", "summary": "Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faPredictive Scheduling\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9884\u6d4b\u5668\u5728\u63a8\u7406\u524d\u9884\u4f30\u6bcf\u4e2a\u67e5\u8be2\u7684\u6700\u4f18\u63a8\u7406\u957f\u5ea6\uff0c\u52a8\u6001\u5206\u914d\u56fa\u5b9atoken\u9884\u7b97\u4ee5\u6700\u5927\u5316\u51c6\u786e\u7387\uff0c\u5728GSM8K\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4f7f\u7528\u56fa\u5b9atoken\u9884\u7b97\u4f1a\u5bfc\u81f4\u7b80\u5355\u8f93\u5165\u8fc7\u5ea6\u8ba1\u7b97\u3001\u56f0\u96be\u8f93\u5165\u8ba1\u7b97\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8ba1\u7b97-\u51c6\u786e\u7387\u6743\u8861\u63a7\u5236\u3002", "method": "\u63d0\u51faPredictive Scheduling\u6846\u67b6\uff1a1) \u4f7f\u7528MLP\u5206\u6790transformer\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\u6216LoRA\u5fae\u8c03\u7684\u5206\u7c7b\u5668\u5206\u6790\u539f\u59cb\u95ee\u9898\u6587\u672c\uff0c\u9884\u5148\u4f30\u8ba1\u6bcf\u4e2a\u67e5\u8be2\u7684\u6700\u4f18\u63a8\u7406\u957f\u5ea6\u6216\u96be\u5ea6\uff1b2) \u8d2a\u5fc3\u6279\u91cf\u5206\u914d\u5668\u52a8\u6001\u5206\u914d\u56fa\u5b9a\u603btoken\u9884\u7b97\u4ee5\u6700\u5927\u5316\u9884\u671f\u51c6\u786e\u7387\u3002", "result": "\u5728GSM8K\u7b97\u672f\u57fa\u51c6\u4e0a\uff0c\u76f8\u6bd4\u5747\u5300\u9884\u7b97\u5206\u914d\uff0c\u9884\u6d4b\u8c03\u5ea6\u5728\u76f8\u540ctoken\u6210\u672c\u4e0b\u83b7\u5f97\u9ad8\u8fbe7.9\u4e2a\u767e\u5206\u70b9\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\uff0c\u7f29\u5c0f\u4e86\u8d85\u8fc750%\u4e0e\u5b8c\u7f8e\u9884\u77e5oracle\u7684\u5dee\u8ddd\u3002\u7cfb\u7edf\u5c42\u95f4\u7814\u7a76\u53d1\u73b0transformer\u4e2d\u95f4\u5c42(12-17)\u643a\u5e26\u6700\u4e30\u5bcc\u7684\u89c4\u6a21\u4f30\u8ba1\u4fe1\u53f7\u3002", "conclusion": "\u9884\u8fd0\u884c\u9884\u7b97\u9884\u6d4b\u5b9e\u73b0\u4e86\u8ba1\u7b97-\u51c6\u786e\u7387\u6743\u8861\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u4e3a\u5ef6\u8fdf\u654f\u611f\u3001\u6210\u672c\u9ad8\u6548\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5177\u4f53\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2602.00460", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00460", "abs": "https://arxiv.org/abs/2602.00460", "authors": ["Georgios Sotirchos", "Zlatan Ajanovi\u0107", "Jens Kober"], "title": "Search Inspired Exploration in Reinforcement Learning", "comment": null, "summary": "Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \\textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.", "AI": {"tldr": "SIERL\u662f\u4e00\u79cd\u53d7\u641c\u7d22\u542f\u53d1\u7684\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5b66\u4e60\u8fdb\u5ea6\u8bbe\u7f6e\u5b50\u76ee\u6807\u6765\u4e3b\u52a8\u5f15\u5bfc\u63a2\u7d22\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u662f\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u8bfe\u7a0b\u5b66\u4e60\u548cGo-Explore\u4f9d\u8d56\u624b\u5de5\u542f\u53d1\u5f0f\uff0c\u800c\u597d\u5947\u5fc3\u9a71\u52a8\u65b9\u6cd5\u53ef\u80fd\u6536\u655b\u5230\u6b21\u4f18\u7b56\u7565\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4e3b\u52a8\u5f15\u5bfc\u63a2\u7d22\u3001\u7cfb\u7edf\u6269\u5c55\u72b6\u6001\u7a7a\u95f4\u8fb9\u754c\u7684\u65b9\u6cd5\u3002", "method": "SIERL\u5728\u6bcf\u8f6e\u5f00\u59cb\u65f6\u4ece\u8fb9\u754c\uff08\u5df2\u77e5\u72b6\u6001\u7a7a\u95f4\u8fb9\u754c\uff09\u9009\u62e9\u5b50\u76ee\u6807\uff0c\u7136\u540e\u4ee3\u7406\u7ee7\u7eed\u5411\u4e3b\u8981\u4efb\u52a1\u76ee\u6807\u63a2\u7d22\u3002\u5b50\u76ee\u6807\u9009\u62e9\u673a\u5236\u63d0\u4f9b\u65e2\u4e0d\u8fc7\u4e8e\u719f\u6089\u4e5f\u4e0d\u5b8c\u5168\u65b0\u9896\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u786e\u4fdd\u8fb9\u754c\u7cfb\u7edf\u6269\u5c55\u3002\u53d7\u641c\u7d22\u542f\u53d1\uff0c\u57fa\u4e8e\u6210\u672c\u4f30\u8ba1\uff08\u5230\u8fbe\u6210\u672c\u548c\u524d\u5f80\u6210\u672c\uff09\u5bf9\u8fb9\u754c\u5b50\u76ee\u6807\u8fdb\u884c\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u5f15\u5bfc\u63a2\u7d22\u5230\u4fe1\u606f\u6700\u4e30\u5bcc\u7684\u533a\u57df\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\uff0cSIERL\u5728\u5b9e\u73b0\u4e3b\u8981\u4efb\u52a1\u76ee\u6807\u548c\u6cdb\u5316\u5230\u73af\u5883\u4e2d\u4efb\u610f\u72b6\u6001\u65b9\u9762\u90fd\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SIERL\u901a\u8fc7\u53d7\u641c\u7d22\u542f\u53d1\u7684\u5b50\u76ee\u6807\u9009\u62e9\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u95ee\u9898\uff0c\u80fd\u591f\u7cfb\u7edf\u6269\u5c55\u72b6\u6001\u7a7a\u95f4\u8fb9\u754c\u5e76\u5b9e\u73b0\u66f4\u597d\u7684\u4efb\u52a1\u5b8c\u6210\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01326", "abs": "https://arxiv.org/abs/2602.01326", "authors": ["Zirui Wu", "Lin Zheng", "Zhihui Xie", "Jiacheng Ye", "Jiahui Gao", "Shansan Gong", "Yansong Feng", "Zhenguo Li", "Wei Bi", "Guorui Zhou", "Lingpeng Kong"], "title": "DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas", "comment": "ICLR 2026", "summary": "Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.", "AI": {"tldr": "DreamOn\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u9700\u8981\u56fa\u5b9a\u957f\u5ea6\u63a9\u7801\u5e8f\u5217\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u53ef\u53d8\u957f\u5ea6\u7684\u4ee3\u7801\u586b\u5145\u751f\u6210\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u7075\u6d3b\u3001\u4efb\u610f\u987a\u5e8f\u586b\u5145\u7684\u4f18\u52bf\uff0c\u4f46\u53d7\u9650\u4e8e\u9700\u8981\u56fa\u5b9a\u957f\u5ea6\u7684\u63a9\u7801\u5e8f\u5217\uff0c\u5f53\u9884\u5b9a\u4e49\u63a9\u7801\u5927\u5c0f\u4e0e\u7406\u60f3\u8865\u5168\u957f\u5ea6\u4e0d\u5339\u914d\u65f6\uff0c\u4ee3\u7801\u586b\u5145\u6027\u80fd\u4f1a\u4e25\u91cd\u4e0b\u964d\u3002", "method": "DreamOn\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4e86\u4e24\u4e2a\u957f\u5ea6\u63a7\u5236\u72b6\u6001\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u57fa\u4e8e\u81ea\u8eab\u9884\u6d4b\u81ea\u4e3b\u6269\u5c55\u6216\u6536\u7f29\u8f93\u51fa\u957f\u5ea6\u3002\u8be5\u65b9\u6cd5\u53ea\u9700\u5bf9\u73b0\u6709\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u76ee\u6807\u8fdb\u884c\u6700\u5c0f\u4fee\u6539\uff0c\u65e0\u9700\u67b6\u6784\u66f4\u6539\u3002", "result": "\u57fa\u4e8eDream-Coder-7B\u548cDiffuCoder-7B\u6784\u5efa\u7684DreamOn\u5728HumanEval-Infilling\u548cSantaCoder-FIM\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u81ea\u56de\u5f52\u6a21\u578b\u76f8\u5f53\u7684\u586b\u5145\u6027\u80fd\uff0c\u5e76\u5339\u914d\u4e86\u4f7f\u7528\u771f\u5b9e\u957f\u5ea6\u83b7\u5f97\u7684oracle\u6027\u80fd\u3002", "conclusion": "DreamOn\u6d88\u9664\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b9e\u9645\u90e8\u7f72\u7684\u4e00\u4e2a\u57fa\u672c\u969c\u788d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u53ef\u53d8\u957f\u5ea6\u751f\u6210\u65b9\u9762\u7684\u7075\u6d3b\u6027\u548c\u9002\u7528\u6027\u3002", "topic": "code agent"}}
{"id": "2602.01348", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01348", "abs": "https://arxiv.org/abs/2602.01348", "authors": ["Yu Liu", "Wenxiao Zhang", "Cong Cao", "Fangfang Yuan", "Weizhuo Chen", "Cheng Hu", "Pin Xu", "Yuling Yang", "Kun Peng", "Diandian Guo", "Qiang Sun", "Yanbing Liu", "Jin B. Hong", "Zhiyuan Ma"], "title": "CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering", "comment": null, "summary": "Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.", "AI": {"tldr": "CRAFT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u5956\u52b1\u673a\u5236\u4f18\u5316\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u7b54\u6848\u51c6\u786e\u6027\u548c\u63a8\u7406\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u5f53\u524d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u63a8\u7406\u5d29\u6e83 - \u591a\u8df3\u7ec4\u5408\u548c\u566a\u58f0\u68c0\u7d22\u5bfc\u81f4\u63a8\u7406\u4e0d\u7a33\u5b9a\uff1b2) \u63a8\u7406-\u7b54\u6848\u4e0d\u4e00\u81f4 - LLM\u751f\u6210\u7684\u4e0d\u786e\u5b9a\u6027\u53ef\u80fd\u5bfc\u81f4\u7b54\u6848\u6b63\u786e\u4f46\u63a8\u7406\u4e0d\u5fe0\u5b9e\uff1b3) \u683c\u5f0f\u63a7\u5236\u4e22\u5931 - \u4f20\u7edf\u601d\u7ef4\u94fe\u751f\u6210\u5e38\u504f\u79bb\u7ed3\u6784\u5316\u8f93\u51fa\u683c\u5f0f\u8981\u6c42\u3002", "method": "\u63d0\u51faCRAFT\u6846\u67b6\uff0c\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u91c7\u7528\u53cc\u91cd\u5956\u52b1\u673a\u5236\uff1a\u786e\u5b9a\u6027\u5956\u52b1\u786e\u4fdd\u7ed3\u6784\u6b63\u786e\u6027\uff0c\u57fa\u4e8e\u8bc4\u5224\u7684\u5956\u52b1\u9a8c\u8bc1\u8bed\u4e49\u5fe0\u5b9e\u5ea6\u3002\u652f\u6301\u53ef\u63a7\u7684\u63a8\u7406\u8f68\u8ff9\u53d8\u4f53\uff0c\u7cfb\u7edf\u5206\u6790\u7ed3\u6784\u548c\u89c4\u6a21\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRAFT\u63d0\u9ad8\u4e86\u7b54\u6848\u51c6\u786e\u6027\u548c\u63a8\u7406\u5fe0\u5b9e\u5ea6\uff0cCRAFT 7B\u6a21\u578b\u5728\u591a\u4e2a\u63a8\u7406\u8f68\u8ff9\u8bbe\u7f6e\u4e0b\u4e0e\u95ed\u6e90LLM\u7ade\u4e89\u6027\u80fd\u3002", "conclusion": "CRAFT\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u63a8\u7406\u5fe0\u5b9e\u5ea6\u95ee\u9898\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u53ef\u63a7\u548c\u53ef\u9760\u7684\u63a8\u7406\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01355", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01355", "abs": "https://arxiv.org/abs/2602.01355", "authors": ["Haojia Zhu", "Qinyuan Xu", "Haoyu Li", "Yuxi Liu", "Hanchen Qiu", "Jiaoyan Chen", "Jiahui Jin"], "title": "Aggregation Queries over Unstructured Text: Benchmark and Agentic Method", "comment": null, "summary": "Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to \"find all,\" not merely \"find one.\" Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86DFA\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u7531\u6587\u672c\u4e0a\u7684\u805a\u5408\u67e5\u8be2\u95ee\u9898\uff0c\u5f3a\u8c03\"\u627e\u5230\u6240\u6709\"\u800c\u975e\"\u627e\u5230\u4e00\u4e2a\"\u7684\u5b8c\u6574\u6027\u8981\u6c42\uff0c\u5e76\u5f15\u5165\u4e86AGGBench\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u81ea\u7531\u6587\u672c\u4e0a\u7684\u805a\u5408\u67e5\u8be2\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002\u4e0e\u666e\u901a\u95ee\u7b54\u4e0d\u540c\uff0c\u805a\u5408\u67e5\u8be2\u9700\u8981\u5b8c\u6574\u7684\u8bc1\u636e\u6536\u96c6\uff0c\u7cfb\u7edf\u5fc5\u987b\"\u627e\u5230\u6240\u6709\"\u800c\u4e0d\u4ec5\u4ec5\u662f\"\u627e\u5230\u4e00\u4e2a\"\u3002\u73b0\u6709\u7684Text-to-SQL\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u8fd9\u79cd\u5b8c\u6574\u6027\u3002", "method": "\u63d0\u51fa\u4e86DFA\uff08\u6d88\u6b67-\u8fc7\u6ee4-\u805a\u5408\uff09\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u6a21\u5757\u5316\u7684\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u5c06\u805a\u5408\u67e5\u8be2\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u9636\u6bb5\uff1a\u6d88\u6b67\u3001\u8fc7\u6ee4\u548c\u805a\u5408\uff0c\u5e76\u66b4\u9732\u4e0e\u6b67\u4e49\u3001\u8fc7\u6ee4\u548c\u805a\u5408\u76f8\u5173\u7684\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cDFA\u5728\u805a\u5408\u8bc1\u636e\u8986\u76d6\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684RAG\u548c\u667a\u80fd\u4f53\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f62\u5f0f\u5316\u4e86\u5b9e\u4f53\u7ea7\u805a\u5408\u67e5\u8be2\u95ee\u9898\uff0c\u63d0\u51fa\u4e86DFA\u65b9\u6cd5\u548cAGGBench\u57fa\u51c6\uff0c\u4e3a\u89e3\u51b3\u81ea\u7531\u6587\u672c\u4e0a\u7684\u5b8c\u6574\u6027\u5bfc\u5411\u805a\u5408\u67e5\u8be2\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.00476", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00476", "abs": "https://arxiv.org/abs/2602.00476", "authors": ["Hengchang Liu", "Zhao Yang", "Bing Su"], "title": "Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly", "comment": null, "summary": "Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \\textit{Oracle Peak} that emerges near the ground-truth length and a systematic \\textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \\textbf{CAL} (\\textbf{C}alibrated \\textbf{A}daptive \\textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\\% over fixed-length baselines and 40.5\\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\\% and 9.9\\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.", "AI": {"tldr": "CAL\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u7edf\u8ba1\u4fe1\u53f7\uff08Oracle Peak\u548cLength Bias\uff09\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u9002\u5e94\u957f\u5ea6\u4ee3\u7801\u548c\u6587\u672c\u586b\u5145\uff0c\u663e\u8457\u63d0\u5347\u4e86\u586b\u5145\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5929\u751f\u9002\u5408\u586b\u5145\u4efb\u52a1\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u5230\u9884\u8bbe\u586b\u5145\u957f\u5ea6\u7684\u9650\u5236\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6307\u5b9a\u586b\u5145\u957f\u5ea6\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u73b0\u5b9e\uff0c\u56e0\u4e3a\u6b63\u786e\u7684\u586b\u5145\u957f\u5ea6\u901a\u5e38\u662f\u672a\u77e5\u7684\u3002", "method": "CAL\u65b9\u6cd5\u901a\u8fc7\u5206\u6790DLMs\u5728\u7b2c\u4e00\u6b65\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u7edf\u8ba1\u7279\u6027\uff0c\u53d1\u73b0\u4e86\u4e24\u4e2a\u5173\u952e\u73b0\u8c61\uff1a1\uff09\u5728\u771f\u5b9e\u957f\u5ea6\u9644\u8fd1\u51fa\u73b0\u7684\u5c40\u90e8Oracle Peak\u4fe1\u53f7\uff1b2\uff09\u5e38\u5e38\u63a9\u76d6\u8be5\u4fe1\u53f7\u7684\u7cfb\u7edf\u6027Length Bias\u3002\u901a\u8fc7\u5229\u7528\u8fd9\u4e00\u4fe1\u53f7\u5e76\u6821\u51c6\u504f\u5dee\uff0cCAL\u80fd\u591f\u5728\u6b63\u5f0f\u89e3\u7801\u524d\u901a\u8fc7\u9ad8\u6548\u641c\u7d22\u8fd1\u4f3c\u6700\u4f18\u586b\u5145\u957f\u5ea6\u3002", "result": "\u5728\u4ee3\u7801\u586b\u5145\u4efb\u52a1\u4e2d\uff0cCAL\u6bd4\u56fa\u5b9a\u957f\u5ea6\u57fa\u7ebf\u63d0\u5347\u4e8647.7%\u7684Pass@1\uff0c\u6bd4\u57fa\u4e8e\u804a\u5929\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u63d0\u5347\u4e8640.5%\u3002\u5728\u6587\u672c\u586b\u5145\u4efb\u52a1\u4e2d\uff0cBLEU-2\u548cROUGE-L\u5206\u522b\u63d0\u5347\u4e868.5%\u548c9.9%\u3002", "conclusion": "CAL\u8bc1\u660e\u4e86DLMs\u5177\u6709\u53d1\u73b0\u6b63\u786e\u586b\u5145\u957f\u5ea6\u7684\u5185\u5728\u80fd\u529b\uff0c\u901a\u8fc7\u5229\u7528\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u7edf\u8ba1\u4fe1\u53f7\u548c\u6821\u51c6\u504f\u5dee\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4e13\u95e8\u8bad\u7ec3\u7684\u9c81\u68d2\u586b\u5145\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2602.01378", "categories": ["cs.CL", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01378", "abs": "https://arxiv.org/abs/2602.01378", "authors": ["Poushali Sengupta", "Shashi Raj Pandey", "Sabita Maharjan", "Frank Eliassen"], "title": "Context Dependence and Reliability in Autoregressive Language Models", "comment": null, "summary": "Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.", "AI": {"tldr": "RISE\u662f\u4e00\u79cd\u5197\u4f59\u4e0d\u654f\u611f\u7684\u89e3\u91ca\u8bc4\u5206\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522bLLM\u8f93\u51fa\u4e2d\u771f\u6b63\u6709\u5f71\u54cd\u7684\u4e0a\u4e0b\u6587\u5143\u7d20\uff0c\u51cf\u5c11\u5197\u4f59\u4fe1\u606f\u5bf9\u5f52\u56e0\u5206\u6570\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u66f4\u6e05\u6670\u7a33\u5b9a\u7684\u89e3\u91ca\u3002", "motivation": "LLM\u751f\u6210\u8f93\u51fa\u65f6\u4f7f\u7528\u5927\u91cf\u4e0a\u4e0b\u6587\uff0c\u5176\u4e2d\u5e38\u5305\u542b\u5197\u4f59\u4fe1\u606f\u3002\u6807\u51c6\u89e3\u91ca\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5197\u4f59\u548c\u91cd\u53e0\u4e0a\u4e0b\u6587\uff0c\u5fae\u5c0f\u8f93\u5165\u53d8\u5316\u4f1a\u5bfc\u81f4\u5f52\u56e0\u5206\u6570\u4e0d\u53ef\u9884\u6d4b\u7684\u6ce2\u52a8\uff0c\u5f71\u54cd\u53ef\u89e3\u91ca\u6027\u5e76\u5e26\u6765\u5982\u63d0\u793a\u6ce8\u5165\u7b49\u98ce\u9669\u3002\u9700\u8981\u533a\u5206\u771f\u6b63\u91cd\u8981\u7684\u4e0a\u4e0b\u6587\u5143\u7d20\u4e0e\u76f8\u5173\u4f46\u4e0d\u5fc5\u8981\u7684\u5143\u7d20\u3002", "method": "\u63d0\u51faRISE\uff08Redundancy-Insensitive Scoring of Explanation\uff09\u65b9\u6cd5\uff0c\u91cf\u5316\u6bcf\u4e2a\u8f93\u5165\u76f8\u5bf9\u4e8e\u5176\u4ed6\u8f93\u5165\u7684\u72ec\u7279\u5f71\u54cd\uff0c\u6700\u5c0f\u5316\u5197\u4f59\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u66f4\u6e05\u6670\u7a33\u5b9a\u7684\u5f52\u56e0\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRISE\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u89e3\u91ca\uff0c\u5f3a\u8c03\u6761\u4ef6\u4fe1\u606f\u5bf9\u4e8e\u53ef\u4fe1LLM\u89e3\u91ca\u548c\u76d1\u63a7\u7684\u91cd\u8981\u6027\u3002", "conclusion": "RISE\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406LLM\u4e0a\u4e0b\u6587\u4e2d\u7684\u5197\u4f59\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7a33\u5b9a\u7684\u89e3\u91ca\uff0c\u5bf9\u4e8e\u6784\u5efa\u53ef\u4fe1\u7684LLM\u89e3\u91ca\u548c\u76d1\u63a7\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2602.00482", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00482", "abs": "https://arxiv.org/abs/2602.00482", "authors": ["Jiarui Zhang", "Yuchen Yang", "Ran Yan", "Zhiyu Mei", "Liyuan Zhang", "Daifeng Li", "Wei Fu", "Jiaxuan Gao", "Shusheng Xu", "Yi Wu", "Binhang Yuan"], "title": "AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models", "comment": null, "summary": "Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\\times$ in $\u03c4^2$-bench higher training throughput.", "AI": {"tldr": "AREAL-DTA\uff1a\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u904d\u5386rollout\u524d\u7f00\u6811\u6765\u9ad8\u6548\u5229\u7528RL\u8bad\u7ec3\u4e2d\u7684\u524d\u7f00\u5171\u4eab\uff0c\u51cf\u5c11\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500", "motivation": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684LLM\u540e\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u4e3a\u751f\u6210\u7684rollout\u5e8f\u5217\u7ecf\u5e38\u5171\u4eab\u957ftoken\u524d\u7f00\u3002\u73b0\u6709RL\u6846\u67b6\u72ec\u7acb\u5904\u7406\u8fd9\u4e9b\u5e8f\u5217\uff0c\u5728\u7b56\u7565\u6a21\u578b\u8bad\u7ec3\u4e2d\u91cd\u590d\u8ba1\u7b97\u76f8\u540c\u7684\u524d\u7f00\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548c\u5185\u5b58\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u3002", "method": "AREAL-DTA\u91c7\u7528\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u6267\u884c\u7b56\u7565\uff0c\u5728\u524d\u540e\u5411\u8ba1\u7b97\u4e2d\u52a8\u6001\u904d\u5386rollout\u524d\u7f00\u6811\uff0c\u6bcf\u6b21\u53ea\u7269\u5316\u5355\u4e2a\u6839\u5230\u53f6\u8def\u5f84\u3002\u8fd8\u5305\u542b\u8d1f\u8f7d\u5747\u8861\u7684\u5206\u5e03\u5f0f\u6279\u5904\u7406\u673a\u5236\uff0c\u52a8\u6001\u6784\u5efa\u548c\u5904\u7406\u8de8\u591aGPU\u7684\u524d\u7f00\u6811\u3002", "result": "\u5728\u6d41\u884c\u7684RL\u540e\u8bad\u7ec3\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\uff0cAREAL-DTA\u5728\u03c4\u00b2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe8.31\u500d\u7684\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "AREAL-DTA\u901a\u8fc7\u9ad8\u6548\u5229\u7528RL\u8bad\u7ec3\u4e2d\u7684\u524d\u7f00\u5171\u4eab\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6811\u6ce8\u610f\u529b\u65b9\u6cd5\u5728RL\u8bbe\u7f6e\u4e2d\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.00513", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00513", "abs": "https://arxiv.org/abs/2602.00513", "authors": ["Md Tanvirul Alam", "Aritran Piplai", "Ionut Cardei", "Nidhi Rastogi", "Peter J Worth"], "title": "Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs", "comment": null, "summary": "Cyber threat intelligence (CTI) analysts routinely convert noisy, unstructured security artifacts into standardized, automation-ready representations. Although large language models (LLMs) show promise for this task, existing approaches remain brittle when producing structured CTI outputs and have largely relied on supervised fine-tuning (SFT). In contrast, CTI standards and community-maintained resources define canonical identifiers and schemas that enable deterministic verification of model outputs. We leverage this structure to study reinforcement learning with verifiable rewards (RLVR) for CTI tasks. We introduce \\textit{Minerva}, a unified dataset and training pipeline spanning multiple CTI subtasks, each paired with task-specific verifiers that score structured outputs and identifier predictions. To address reward sparsity during rollout, we propose a lightweight self-training mechanism that generates additional verified trajectories and distills them back into the model. Experiments across LLM backbones show consistent improvements in accuracy and robustness over SFT across multiple benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMinerva\u6846\u67b6\uff0c\u5229\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u6765\u6539\u8fdb\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u60c5\u62a5\uff08CTI\uff09\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u751f\u6210\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u9a8c\u8bc1\u5668\u548c\u81ea\u8bad\u7ec3\u673a\u5236\u63d0\u5347\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u60c5\u62a5\uff08CTI\uff09\u7ed3\u6784\u5316\u8f93\u51fa\u4efb\u52a1\u65f6\u5b58\u5728\u8106\u5f31\u6027\uff0c\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u800cCTI\u6807\u51c6\u548c\u793e\u533a\u8d44\u6e90\u5b9a\u4e49\u4e86\u53ef\u786e\u5b9a\u6027\u9a8c\u8bc1\u7684\u6807\u8bc6\u7b26\u548c\u6a21\u5f0f\uff0c\u8fd9\u4e3a\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51faMinerva\u6846\u67b6\uff1a1\uff09\u7edf\u4e00\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u7ba1\u9053\uff0c\u6db5\u76d6\u591a\u4e2aCTI\u5b50\u4efb\u52a1\uff1b2\uff09\u6bcf\u4e2a\u4efb\u52a1\u914d\u5907\u7279\u5b9a\u9a8c\u8bc1\u5668\uff0c\u5bf9\u7ed3\u6784\u5316\u8f93\u51fa\u548c\u6807\u8bc6\u7b26\u9884\u6d4b\u8fdb\u884c\u8bc4\u5206\uff1b3\uff09\u9488\u5bf9\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u8f7b\u91cf\u7ea7\u81ea\u8bad\u7ec3\u673a\u5236\uff0c\u751f\u6210\u989d\u5916\u5df2\u9a8c\u8bc1\u8f68\u8ff9\u5e76\u84b8\u998f\u56de\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4e0d\u540cLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "\u5229\u7528CTI\u9886\u57df\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u7ed3\u6784\u5316\u8f93\u51fa\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u81ea\u8bad\u7ec3\u673a\u5236\u6709\u52a9\u4e8e\u7f13\u89e3\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898\uff0c\u4e3aCTI\u5206\u6790\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01532", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.01532", "abs": "https://arxiv.org/abs/2602.01532", "authors": ["Yuxuan Fu", "Xiaoyu Tan", "Teqi Hao", "Chen Zhan", "Xihe Qiu"], "title": "PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents", "comment": null, "summary": "Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: \"make haste slowly\"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.", "AI": {"tldr": "PRISM\u6846\u67b6\u901a\u8fc7\u6210\u672c\u654f\u611f\u7684\u9009\u62e9\u6027\u5e72\u9884\uff0c\u7ed3\u5408\u51b3\u7b56\u7406\u8bba\u95e8\u63a7\u548c\u53cc\u8fc7\u7a0b\u63a8\u7406\u67b6\u6784\uff0c\u4ec5\u5728\u7528\u6237\u63a5\u53d7\u6982\u7387\u8d85\u8fc7\u6210\u672c\u9608\u503c\u65f6\u8fdb\u884c\u5e72\u9884\uff0c\u51cf\u5c11\u8bef\u62a522.78%\uff0c\u63d0\u5347F1\u5206\u657020.14%", "motivation": "\u73b0\u6709\u4e3b\u52a8\u4ee3\u7406\u7cfb\u7edf\u4f9d\u8d56\u8106\u5f31\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6216\u76f2\u76ee\u7684\u957f\u63a8\u7406\uff0c\u96be\u4ee5\u63a7\u5236\u5e2e\u52a9\u4e0e\u8d1f\u62c5\u7684\u6743\u8861\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u5e72\u9884\u65f6\u673a\u3001\u5e73\u8861\u6536\u76ca\u4e0e\u6210\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPRISM\u6846\u67b6\uff1a1) \u51b3\u7b56\u7406\u8bba\u95e8\u63a7\uff1a\u57fa\u4e8e\u7528\u6237\u63a5\u53d7\u6982\u7387\u4e0e\u4e0d\u5bf9\u79f0\u6210\u672c\u9608\u503c\u51b3\u5b9a\u662f\u5426\u5e72\u9884\uff1b2) \u53cc\u8fc7\u7a0b\u63a8\u7406\uff1a\u4ec5\u5728\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u8c03\u7528\u8d44\u6e90\u5bc6\u96c6\u7684\u6162\u6a21\u5f0f\uff1b3) \u95e8\u5bf9\u9f50\u7684\u6a21\u5f0f\u9501\u5b9a\u84b8\u998f\uff1a\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u5bc6\u96c6\u76d1\u7763\uff0c\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u4e0e\u5e72\u9884\u95e8\u89e3\u8026\u7684\u54cd\u5e94\u7b56\u7565\u3002", "result": "\u5728ProactiveBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPRISM\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u51cf\u5c1122.78%\u7684\u8bef\u62a5\uff0c\u63d0\u534720.14%\u7684F1\u5206\u6570\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u3001\u8ba1\u7b97\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u4e3b\u52a8\u4ee3\u7406\u3002", "conclusion": "\u51b3\u7b56\u7406\u8bba\u95e8\u63a7\u7ed3\u5408\u9009\u62e9\u6027\u6162\u63a8\u7406\u548c\u5bf9\u9f50\u84b8\u998f\uff0c\u80fd\u591f\u4ea7\u751f\u7cbe\u786e\u3001\u8ba1\u7b97\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u4e3b\u52a8\u4ee3\u7406\u3002\u8be5\u65b9\u6cd5\u5e73\u8861\u4e86\u5e2e\u52a9\u6536\u76ca\u4e0e\u7528\u6237\u8d1f\u62c5\u7684\u6743\u8861\u3002", "topic": "agent analysis"}}
{"id": "2602.01539", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.01539", "abs": "https://arxiv.org/abs/2602.01539", "authors": ["Xiaoyu Wen", "Zhida He", "Han Qi", "Ziyu Wan", "Zhongtian Ma", "Ying Wen", "Tianhang Zheng", "Xingcheng Xu", "Chaochao Lu", "Qiaosheng Zhang"], "title": "MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety", "comment": null, "summary": "Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \\textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \\textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \\textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \\textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.", "AI": {"tldr": "MAGIC\u662f\u4e00\u4e2a\u591a\u8f6e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u653b\u51fb\u8005\u4e0e\u9632\u5fa1\u8005\u7684\u5bf9\u6297\u6027\u534f\u540c\u8fdb\u5316\u6765\u589e\u5f3aLLM\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u65e0\u9700\u4f9d\u8d56\u9759\u6001\u6570\u636e\u5206\u5e03\u3002", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u9884\u6536\u96c6\u6570\u636e\u5206\u5e03\uff0c\u96be\u4ee5\u8ddf\u4e0a\u4e0d\u65ad\u6f14\u5316\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u9700\u8981\u52a8\u6001\u3001\u81ea\u9002\u5e94\u7684\u5b89\u5168\u5bf9\u9f50\u673a\u5236\u3002", "method": "\u5c06LLM\u5b89\u5168\u5bf9\u9f50\u5efa\u6a21\u4e3a\u5bf9\u6297\u6027\u975e\u5bf9\u79f0\u535a\u5f08\uff1a\u653b\u51fb\u8005\u667a\u80fd\u4f53\u5b66\u4e60\u8fed\u4ee3\u91cd\u5199\u67e5\u8be2\u4e3a\u6b3a\u9a97\u6027\u63d0\u793a\uff0c\u9632\u5fa1\u8005\u667a\u80fd\u4f53\u540c\u65f6\u4f18\u5316\u7b56\u7565\u4ee5\u8bc6\u522b\u548c\u62d2\u7edd\u6b64\u7c7b\u8f93\u5165\uff0c\u901a\u8fc7\u591a\u8f6e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u534f\u540c\u8fdb\u5316\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u9632\u5fa1\u6210\u529f\u7387\u4e14\u4e0d\u635f\u5bb3\u6a21\u578b\u7684\u6709\u7528\u6027\uff1b\u653b\u51fb\u8005\u901a\u8fc7\u8fed\u4ee3RL\u8bad\u7ec3\u6f14\u5316\u51fa\u65b0\u9896\u7684\u3001\u5148\u524d\u672a\u89c1\u8fc7\u7684\u7ec4\u5408\u7b56\u7565\u3002", "conclusion": "MAGIC\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5bf9\u6297\u6027\u534f\u540c\u8fdb\u5316\u5b9e\u73b0\u4e86\u66f4\u5f3a\u5927\u7684LLM\u5b89\u5168\u5bf9\u9f50\uff0c\u4e3a\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c1\u89e3\u548c\u5b89\u5168\u4fdd\u8bc1\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01550", "abs": "https://arxiv.org/abs/2602.01550", "authors": ["S1-NexusAgent Team"], "title": "S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research", "comment": "In progress", "summary": "Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.", "AI": {"tldr": "S1-NexusAgent\u662f\u4e00\u4e2a\u81ea\u6f14\u5316\u7684\u79d1\u5b66\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42Plan-and-CodeAct\u6267\u884c\u8303\u5f0f\uff0c\u901a\u8fc7\u53cc\u5faa\u73af\u67b6\u6784\u89e3\u8026\u5168\u5c40\u79d1\u5b66\u89c4\u5212\u4e0e\u5b50\u4efb\u52a1\u5de5\u5177\u6267\u884c\uff0c\u652f\u6301\u5927\u89c4\u6a21\u8de8\u5b66\u79d1\u5de5\u5177\u96c6\u6210\u548c\u6301\u7eed\u81ea\u6211\u8fdb\u5316\u3002", "motivation": "\u73b0\u6709LLM\u548c\u57fa\u4e8e\u5de5\u5177\u7684\u667a\u80fd\u4f53\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u3001\u590d\u6742\u5de5\u4f5c\u6d41\u548c\u4e13\u7528\u5de5\u5177\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u7a0b\u89c4\u5212\u3001\u9c81\u68d2\u76ee\u6807\u7ef4\u6301\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u9762\uff0c\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u591a\u5b66\u79d1\u79d1\u5b66\u7814\u7a76\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5206\u5c42Plan-and-CodeAct\u6267\u884c\u8303\u5f0f\uff0c\u53cc\u5faa\u73af\u67b6\u6784\u5206\u79bb\u5168\u5c40\u89c4\u5212\u4e0e\u5de5\u5177\u6267\u884c\uff1b\u652f\u6301MCP\u534f\u8bae\u96c6\u6210\u6570\u5343\u8de8\u5b66\u79d1\u5de5\u5177\uff1b\u5f15\u5165\u57fa\u4e8e\u5bf9\u8c61\u5f15\u7528\u7684\u7a00\u758f\u4e0a\u4e0b\u6587\u7ba1\u7406\uff1b\u901a\u8fc7Critic Agent\u8bc4\u4f30\u6267\u884c\u8f68\u8ff9\u5e76\u63d0\u70bc\u53ef\u91cd\u7528Scientific Skills\u3002", "result": "\u5728\u751f\u7269\u3001\u5316\u5b66\u3001\u6750\u6599\u79d1\u5b66\u7b49\u6743\u5a01\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08biomini-eval\u3001ChemBench\u3001MatSciBench\uff09\uff0cS1-NexusAgent\u5728\u957f\u65f6\u7a0b\u89c4\u5212\u548c\u590d\u6742\u4e13\u7528\u5de5\u5177\u7f16\u6392\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "S1-NexusAgent\u901a\u8fc7\u81ea\u6f14\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u89c4\u5212\u3001\u5de5\u5177\u96c6\u6210\u548c\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u5b66\u79d1\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2602.01556", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01556", "abs": "https://arxiv.org/abs/2602.01556", "authors": ["Hong Su"], "title": "Autonomous Question Formation for Large Language Model-Driven AI Systems", "comment": null, "summary": "Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4eba\u7c7b\u6a21\u62df\u7684\u6846\u67b6\uff0c\u4f7fAI\u7cfb\u7edf\u80fd\u901a\u8fc7\u63a8\u7406\u5185\u90e8\u72b6\u6001\u3001\u73af\u5883\u89c2\u5bdf\u548c\u4e0e\u5176\u4ed6AI\u7cfb\u7edf\u7684\u4ea4\u4e92\u6765\u81ea\u4e3b\u5f62\u6210\u95ee\u9898\u548c\u8bbe\u5b9a\u4efb\u52a1\uff0c\u5c06\u95ee\u9898\u5f62\u6210\u4f5c\u4e3a\u4efb\u52a1\u9009\u62e9\u548c\u6267\u884c\u7684\u4f18\u5148\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684AI\u7cfb\u7edf\u5927\u591a\u4f9d\u8d56\u9884\u5b9a\u4e49\u4efb\u52a1\u548c\u56fa\u5b9a\u63d0\u793a\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73af\u5883\u53d8\u5316\u65f6\u81ea\u4e3b\u8bc6\u522b\u5e94\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\uff0c\u9700\u8981\u66f4\u81ea\u4e3b\u7684\u95ee\u9898\u5f62\u6210\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4eba\u7c7b\u6a21\u62df\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5f62\u6210\u4f5c\u4e3a\u9996\u8981\u51b3\u7b56\u8fc7\u7a0b\uff0c\u6574\u5408\u5185\u90e8\u9a71\u52a8\u3001\u73af\u5883\u611f\u77e5\u548c\u667a\u80fd\u4f53\u95f4\u611f\u77e5\u4e09\u79cd\u63d0\u793a\u8303\u56f4\u6765\u9010\u6b65\u6269\u5c55\u8ba4\u77e5\u8986\u76d6\uff0c\u5e76\u652f\u6301\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u95ee\u9898\u5f62\u6210\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u667a\u80fd\u4f53\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u73af\u5883\u611f\u77e5\u63d0\u793a\u76f8\u6bd4\u5185\u90e8\u9a71\u52a8\u57fa\u7ebf\u663e\u8457\u51cf\u5c11\u65e0\u8fdb\u98df\u4e8b\u4ef6\uff0c\u667a\u80fd\u4f53\u95f4\u611f\u77e5\u63d0\u793a\u572820\u5929\u4eff\u771f\u4e2d\u8fdb\u4e00\u6b65\u51cf\u5c11\u7d2f\u8ba1\u65e0\u8fdb\u98df\u4e8b\u4ef6\u8d85\u8fc760%\uff0c\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u6539\u8fdb(p < 0.05)\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f7fAI\u7cfb\u7edf\u80fd\u591f\u81ea\u4e3b\u5f62\u6210\u95ee\u9898\u548c\u8bbe\u5b9a\u4efb\u52a1\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8ba4\u77e5\u6269\u5c55\u63d0\u9ad8\u9002\u5e94\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u73af\u5883\u611f\u77e5\u548c\u667a\u80fd\u4f53\u95f4\u611f\u77e5\u63d0\u793a\u80fd\u663e\u8457\u6539\u5584\u51b3\u7b56\u8d28\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2602.01566", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01566", "abs": "https://arxiv.org/abs/2602.01566", "authors": ["Chiwei Zhu", "Benfeng Xu", "Mingxuan Du", "Shaohan Wang", "Xiaorui Wang", "Zhendong Mao", "Yongdong Zhang"], "title": "FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents", "comment": "19 pages, 6 figures", "summary": "Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.", "AI": {"tldr": "FS-Researcher\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7684\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6301\u4e45\u5316\u5de5\u4f5c\u7a7a\u95f4\u89e3\u51b3\u6df1\u5ea6\u7814\u7a76\u4e2d\u957f\u8f68\u8ff9\u8d85\u51fa\u6a21\u578b\u4e0a\u4e0b\u6587\u9650\u5236\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u8d85\u8d8a\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "motivation": "\u6df1\u5ea6\u7814\u7a76\u4f5c\u4e3aLLM\u667a\u80fd\u4f53\u7684\u4ee3\u8868\u6027\u957f\u89c6\u91ce\u4efb\u52a1\uff0c\u5176\u957f\u8f68\u8ff9\u7ecf\u5e38\u8d85\u51fa\u6a21\u578b\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u538b\u7f29\u4e86\u8bc1\u636e\u6536\u96c6\u548c\u62a5\u544a\u7f16\u5199\u7684token\u9884\u7b97\uff0c\u963b\u788d\u4e86\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "method": "\u91c7\u7528\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u6301\u4e45\u5316\u5916\u90e8\u5185\u5b58\u548c\u5171\u4eab\u534f\u8c03\u5a92\u4ecb\u7684\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff1aContext Builder\u667a\u80fd\u4f53\u4f5c\u4e3a\u56fe\u4e66\u7ba1\u7406\u5458\u6d4f\u89c8\u4e92\u8054\u7f51\u3001\u7f16\u5199\u7ed3\u6784\u5316\u7b14\u8bb0\u5e76\u5c06\u539f\u59cb\u6e90\u5f52\u6863\u5230\u53ef\u8d85\u8d8a\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u5206\u5c42\u77e5\u8bc6\u5e93\u4e2d\uff1bReport Writer\u667a\u80fd\u4f53\u5c06\u77e5\u8bc6\u5e93\u4f5c\u4e3a\u4e8b\u5b9e\u6765\u6e90\uff0c\u9010\u8282\u7f16\u5199\u6700\u7ec8\u62a5\u544a\u3002", "result": "\u5728\u4e24\u4e2a\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\uff08DeepResearch Bench\u548cDeepConsult\uff09\u4e0a\uff0cFS-Researcher\u5728\u4e0d\u540c\u9aa8\u5e72\u6a21\u578b\u4e0a\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u62a5\u544a\u8d28\u91cf\u3002\u5206\u6790\u663e\u793a\u6700\u7ec8\u62a5\u544a\u8d28\u91cf\u4e0e\u5206\u914d\u7ed9Context Builder\u7684\u8ba1\u7b97\u91cf\u5448\u6b63\u76f8\u5173\uff0c\u9a8c\u8bc1\u4e86\u6587\u4ef6\u7cfb\u7edf\u8303\u5f0f\u4e0b\u7684\u6709\u6548\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "conclusion": "FS-Researcher\u901a\u8fc7\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u6301\u4e45\u5316\u5de5\u4f5c\u7a7a\u95f4\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6df1\u5ea6\u7814\u7a76\u4e2d\u957f\u8f68\u8ff9\u8d85\u51fa\u4e0a\u4e0b\u6587\u9650\u5236\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8d85\u8d8a\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u7684\u957f\u89c6\u91ce\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.01664", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01664", "abs": "https://arxiv.org/abs/2602.01664", "authors": ["Mingda Zhang", "Haoran Luo", "Tiesunlong Shen", "Qika Lin", "Xiaoying Tang", "Rui Mao", "Erik Cambria"], "title": "FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning", "comment": "41 pages, 7 figures, 6 tables. Project page: http://flowsteer.org/", "summary": "In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.", "AI": {"tldr": "FlowSteer\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7b56\u7565\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u81ea\u52a8\u7f16\u6392\u5de5\u4f5c\u6d41\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u4f5c\u6d41\u7f16\u6392\u7684\u9ad8\u4eba\u5de5\u6210\u672c\u3001\u4f9d\u8d56\u7279\u5b9a\u7b97\u5b50/LLM\u548c\u7a00\u758f\u5956\u52b1\u4fe1\u53f7\u7b49\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u6d41\u7f16\u6392\u9762\u4e34\u9ad8\u4eba\u5de5\u6210\u672c\u3001\u4f9d\u8d56\u7279\u5b9a\u7b97\u5b50/\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u53ca\u7a00\u758f\u5956\u52b1\u4fe1\u53f7\u7b49\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u81ea\u52a8\u5316\u548c\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFlowSteer\u6846\u67b6\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u7b56\u7565\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4f53\uff0c\u5728\u53ef\u6267\u884c\u753b\u5e03\u73af\u5883\u4e2d\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u81ea\u52a8\u7f16\u6392\u5de5\u4f5c\u6d41\u3002\u7b56\u7565\u6a21\u578b\u5206\u6790\u6267\u884c\u72b6\u6001\u5e76\u9009\u62e9\u7f16\u8f91\u52a8\u4f5c\uff0c\u753b\u5e03\u6267\u884c\u7b97\u5b50\u5e76\u8fd4\u56de\u53cd\u9988\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002\u8fd8\u63d0\u51fa\u4e86Canvas Workflow Relative Policy Optimization (CWRPO)\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5f15\u5165\u591a\u6837\u6027\u7ea6\u675f\u5956\u52b1\u548c\u6761\u4ef6\u91ca\u653e\u673a\u5236\u6765\u7a33\u5b9a\u5b66\u4e60\u5e76\u6291\u5236\u6377\u5f84\u884c\u4e3a\u3002", "result": "\u572812\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFlowSteer\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FlowSteer\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7f16\u6392\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u7b97\u5b50\u5e93\u548c\u53ef\u4e92\u6362\u7684LLM\u540e\u7aef\uff0c\u5e76\u901a\u8fc7CWRPO\u65b9\u6cd5\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01590", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01590", "abs": "https://arxiv.org/abs/2602.01590", "authors": ["Shaohan Wang", "Benfeng Xu", "Licheng Zhang", "Mingxuan Du", "Chiwei Zhu", "Xiaorui Wang", "Zhendong Mao", "Yongdong Zhang"], "title": "Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles", "comment": "Preprint. Work in progress", "summary": "Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge", "AI": {"tldr": "\u63d0\u51fa\u4e86Wiki Live Challenge (WLC)\u57fa\u51c6\uff0c\u5229\u7528\u6700\u65b0\u7684\u7ef4\u57fa\u767e\u79d1\u4f18\u8d28\u6587\u7ae0\u4f5c\u4e3a\u4e13\u5bb6\u7ea7\u53c2\u8003\uff0c\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5305\u542b39\u9879\u6807\u51c6\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u8bc4\u4f30\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56LLM\u751f\u6210\u7684\u53c2\u8003\u6216LLM\u884d\u751f\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u7f3a\u4e4f\u4e13\u5bb6\u9a8c\u8bc1\u5185\u5bb9\u7684\u53ef\u9760\u6027\uff0c\u96be\u4ee5\u63d0\u4f9b\u5ba2\u89c2\u3001\u7ec6\u7c92\u5ea6\u7684\u5173\u952e\u7ef4\u5ea6\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b100\u7bc7\u6700\u65b0\u7ef4\u57fa\u767e\u79d1\u4f18\u8d28\u6587\u7ae0\u7684\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86Wiki Eval\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b39\u9879\u5199\u4f5c\u8d28\u91cf\u6807\u51c6\u548c\u4e25\u683c\u7684\u4e8b\u5b9e\u53ef\u9a8c\u8bc1\u6027\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7ea7\u7ef4\u57fa\u767e\u79d1\u6587\u7ae0\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u9a8c\u8bc1\u4e86WLC\u5728\u63a8\u8fdb\u4ee3\u7406\u7814\u7a76\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "WLC\u57fa\u51c6\u901a\u8fc7\u4e13\u5bb6\u7ea7\u53c2\u8003\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6807\u51c6\uff0c\u4e3a\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u5ba2\u89c2\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2602.01675", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01675", "abs": "https://arxiv.org/abs/2602.01675", "authors": ["Yuanzhe Shen", "Zisu Huang", "Zhengyuan Wang", "Muzhao Tian", "Zhengkang Guo", "Chenyang Zhang", "Shuaiyu Zhou", "Zengjie Hu", "Dailin Li", "Jingwen Xu", "Kaimin Wang", "Wenhao Liu", "Tianlong Li", "Fengpeng Yue", "Feng Hong", "Cao Liu", "Ke Zeng"], "title": "TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios", "comment": "40 pages, 6figures", "summary": "As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \\textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\\% success on the easy split, with performance dropping below 10\\% on hard subsets. We further propose \\textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.", "AI": {"tldr": "TRIP-Bench\u662f\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u65c5\u884c\u89c4\u5212\u573a\u666f\u7684\u957f\u65f6\u7a0b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b18\u4e2a\u5de5\u5177\u548c40+\u65c5\u884c\u9700\u6c42\uff0c\u652f\u6301\u81ea\u52a8\u5316\u8bc4\u4f30\u3002GTPO\u662f\u4e00\u79cd\u5728\u7ebf\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u63d0\u5347\u7ea6\u675f\u6ee1\u8db3\u548c\u4ea4\u4e92\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u5145\u5206\u4ee3\u8868\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u5f3a\u5236\u6267\u884c\u5168\u5c40\u7ea6\u675f\u3001\u534f\u8c03\u591a\u5de5\u5177\u63a8\u7406\u4ee5\u53ca\u9002\u5e94\u957f\u671f\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u7528\u6237\u884c\u4e3a\u53d8\u5316\u3002\u9700\u8981\u66f4\u8d34\u8fd1\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faTRIP-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5229\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6784\u5efa\u65c5\u884c\u89c4\u5212\u573a\u666f\uff0c\u5305\u542b\u4e0d\u540c\u96be\u5ea6\u5212\u5206\u3002\u540c\u65f6\u63d0\u51faGTPO\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u7ebf\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u91c7\u7528\u4e13\u95e8\u7684\u5956\u52b1\u5f52\u4e00\u5316\u548c\u5956\u52b1\u5dee\u5206\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u6a21\u578b\u5728\u7b80\u5355\u5212\u5206\u4e0a\u6700\u591a\u53ea\u80fd\u8fbe\u523050%\u7684\u6210\u529f\u7387\uff0c\u5728\u56f0\u96be\u5b50\u96c6\u4e0a\u6027\u80fd\u964d\u81f310%\u4ee5\u4e0b\u3002GTPO\u5e94\u7528\u4e8eQwen2.5-32B-Instruct\u540e\uff0c\u5728\u7ea6\u675f\u6ee1\u8db3\u548c\u4ea4\u4e92\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8eGemini-3-Pro\u3002", "conclusion": "TRIP-Bench\u6709\u671b\u63a8\u52a8\u5b9e\u7528\u957f\u65f6\u7a0b\u4ea4\u4e92\u667a\u80fd\u4f53\u7684\u53d1\u5c55\uff0c\u800cGTPO\u4e3a\u9c81\u68d2\u7684\u957f\u65f6\u7a0b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.01618", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01618", "abs": "https://arxiv.org/abs/2602.01618", "authors": ["Panuthep Tasawong", "Jian Gang Ngui", "Alham Fikri Aji", "Trevor Cohn", "Peerat Limkonchotiwat"], "title": "SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia", "comment": "Under reivew", "summary": "Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.", "AI": {"tldr": "\u63d0\u51faSEA-Guard\u7cfb\u5217\u6a21\u578b\uff0c\u9996\u4e2a\u57fa\u4e8e\u4e1c\u5357\u4e9a\u6587\u5316\u80cc\u666f\u7684\u591a\u8bed\u8a00\u5b89\u5168\u9632\u62a4\u6a21\u578b\uff0c\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u6570\u636e\u751f\u6210\u6846\u67b6\u521b\u5efa\u771f\u5b9e\u533a\u57df\u7279\u5b9a\u5b89\u5168\u6570\u636e\u96c6\uff0c\u5728\u68c0\u6d4b\u533a\u57df\u654f\u611f\u5185\u5bb9\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754cAI\u5bf9\u9f50\u9700\u8981\u6587\u5316\u611f\u77e5\u7684\u5b89\u5168\u9632\u62a4\uff0c\u4f46\u6784\u5efa\u5927\u89c4\u6a21\u6587\u5316\u57fa\u7840\u6570\u636e\u96c6\u9762\u4e34\u8d44\u6e90\u6709\u9650\u548c\u672c\u5730\u6807\u6ce8\u8005\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u82f1\u8bed\u6570\u636e\u96c6\u673a\u5668\u7ffb\u8bd1\uff0c\u7f3a\u4e4f\u533a\u57df\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u3002", "method": "\u63d0\u51fa\u667a\u80fd\u4ee3\u7406\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u53ef\u6269\u5c55\u5730\u521b\u5efa\u771f\u5b9e\u3001\u533a\u57df\u7279\u5b9a\u7684\u5b89\u5168\u6570\u636e\u96c6\uff0c\u7279\u522b\u9488\u5bf9\u4e1c\u5357\u4e9a\u5730\u533a\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1SEA-Guard\u7cfb\u5217\u591a\u8bed\u8a00\u5b89\u5168\u9632\u62a4\u6a21\u578b\u3002", "result": "SEA-Guard\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6587\u5316\u53d8\u4f53\u8bc4\u4f30\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u5b89\u5168\u9632\u62a4\u6a21\u578b\uff0c\u5728\u68c0\u6d4b\u533a\u57df\u654f\u611f\u6216\u6709\u5bb3\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u901a\u7528\u5b89\u5168\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u6846\u67b6\u521b\u5efa\u6587\u5316\u57fa\u7840\u6570\u636e\u96c6\u662f\u89e3\u51b3AI\u5b89\u5168\u4e2d\u6587\u5316\u611f\u77e5\u6311\u6218\u7684\u6709\u6548\u65b9\u6cd5\uff0cSEA-Guard\u6a21\u578b\u4e3a\u4e1c\u5357\u4e9a\u5730\u533a\u63d0\u4f9b\u4e86\u9996\u4e2a\u6587\u5316\u57fa\u7840\u7684\u591a\u8bed\u8a00\u5b89\u5168\u9632\u62a4\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.00549", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00549", "abs": "https://arxiv.org/abs/2602.00549", "authors": ["Kezhao Lai", "Yutao Lai", "Hai-Lin Liu"], "title": "Beyond the Node: Clade-level Selection for Efficient MCTS in Automatic Heuristic Design", "comment": null, "summary": "While Monte Carlo Tree Search (MCTS) shows promise in Large Language Model (LLM) based Automatic Heuristic Design (AHD), it suffers from a critical over-exploitation tendency under the limited computational budgets required for heuristic evaluation. To address this limitation, we propose Clade-AHD, an efficient framework that replaces node-level point estimates with clade-level Bayesian beliefs. By aggregating descendant evaluations into Beta distributions and performing Thompson Sampling over these beliefs, Clade-AHD explicitly models uncertainty to guide exploration, enabling more reliable decision-making under sparse and noisy evaluations. Extensive experiments on complex combinatorial optimization problems demonstrate that Clade-AHD consistently outperforms state-of-the-art methods while significantly reducing computational cost. The source code is publicly available at: https://github.com/Mriya0306/Clade-AHD.", "AI": {"tldr": "Clade-AHD\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8282\u70b9\u7ea7\u70b9\u4f30\u8ba1\u66ff\u6362\u4e3a\u5206\u652f\u7ea7\u8d1d\u53f6\u65af\u4fe1\u5ff5\u6765\u89e3\u51b3MCTS\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u7684\u8fc7\u5ea6\u5f00\u53d1\u95ee\u9898\u3002", "motivation": "MCTS\u5728LLM\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u6709\u9650\u7684\u542f\u53d1\u5f0f\u8bc4\u4f30\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b58\u5728\u4e25\u91cd\u7684\u8fc7\u5ea6\u5f00\u53d1\u503e\u5411\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faClade-AHD\u6846\u67b6\uff0c\u5c06\u8282\u70b9\u7ea7\u70b9\u4f30\u8ba1\u66ff\u6362\u4e3a\u5206\u652f\u7ea7\u8d1d\u53f6\u65af\u4fe1\u5ff5\uff0c\u901a\u8fc7\u5c06\u540e\u4ee3\u8bc4\u4f30\u805a\u5408\u6210Beta\u5206\u5e03\u5e76\u5728\u8fd9\u4e9b\u4fe1\u5ff5\u4e0a\u6267\u884cThompson\u91c7\u6837\uff0c\u663e\u5f0f\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u4ee5\u6307\u5bfc\u63a2\u7d22\u3002", "result": "\u5728\u590d\u6742\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cClade-AHD\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "Clade-AHD\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u6765\u6307\u5bfc\u63a2\u7d22\uff0c\u80fd\u591f\u5728\u7a00\u758f\u548c\u566a\u58f0\u8bc4\u4f30\u4e0b\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u51b3\u7b56\uff0c\u662f\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01640", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01640", "abs": "https://arxiv.org/abs/2602.01640", "authors": ["Shuai Zhang", "Jiayu Hu", "Zijie Chen", "Zeyuan Ding", "Yi Zhang", "Yingji Zhang", "Ziyi Zhou", "Junwei Liao", "Shengjie Zhou", "Yong Dai", "Zhenzhong Lan", "Xiaozhu Ju"], "title": "A2Eval: Agentic and Automated Evaluation for Embodied Brain", "comment": null, "summary": "Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.", "AI": {"tldr": "A2Eval\u662f\u9996\u4e2a\u901a\u8fc7\u4e24\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\u81ea\u52a8\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u6784\u5efa\u548c\u8bc4\u4f30\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5177\u8eabVLM\u8bc4\u4f30\u4e2d\u57fa\u51c6\u6d4b\u8bd5\u5197\u4f59\u3001\u8986\u76d6\u4e0d\u5e73\u8861\u548c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5177\u8eab\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4f9d\u8d56\u9759\u6001\u3001\u4e13\u5bb6\u5b9a\u4e49\u3001\u4eba\u5de5\u6807\u6ce8\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b58\u5728\u4e25\u91cd\u5197\u4f59\u548c\u8986\u76d6\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u8fd9\u79cd\u52b3\u52a8\u5bc6\u96c6\u578b\u8303\u5f0f\u6d88\u8017\u5927\u91cf\u8ba1\u7b97\u548c\u6807\u6ce8\u8d44\u6e90\uff0c\u589e\u52a0\u6210\u672c\uff0c\u626d\u66f2\u6a21\u578b\u6392\u540d\uff0c\u963b\u788d\u8fed\u4ee3\u5f00\u53d1\u3002", "method": "\u63d0\u51faAgentic Automatic Evaluation (A2Eval)\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\uff1aData Agent\u81ea\u4e3b\u5f52\u7eb3\u80fd\u529b\u7ef4\u5ea6\u5e76\u6784\u5efa\u5e73\u8861\u7d27\u51d1\u7684\u8bc4\u4f30\u5957\u4ef6\uff1bEval Agent\u5408\u6210\u548c\u9a8c\u8bc1\u53ef\u6267\u884c\u7684\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684\u9ad8\u4fdd\u771f\u8bc4\u4f30\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c13\u4e2a\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0cA2Eval\u5c06\u8bc4\u4f30\u5957\u4ef6\u538b\u7f2985%\uff0c\u51cf\u5c11\u603b\u4f53\u8ba1\u7b97\u6210\u672c77%\uff0c\u5b9e\u73b04.6\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u8bc4\u4f30\u8d28\u91cf\u3002\u7ea0\u6b63\u7cfb\u7edf\u6027\u6392\u540d\u504f\u5dee\uff0c\u63d0\u9ad8\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u81f3Spearman's rho=0.85\uff0c\u4fdd\u6301\u9ad8\u6392\u540d\u4fdd\u771f\u5ea6(Kendall's tau=0.81)\u3002", "conclusion": "A2Eval\u4e3a\u9ad8\u4fdd\u771f\u3001\u4f4e\u6210\u672c\u7684\u5177\u8eab\u8bc4\u4f30\u5efa\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u81ea\u4e3b\u7684\u9ad8\u6548\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2602.01711", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01711", "abs": "https://arxiv.org/abs/2602.01711", "authors": ["Wei Chen", "Yanbin Fang", "Shuran Fu", "Fasheng Xu", "Xuan Wei"], "title": "Optimizing Prompts for Large Language Models: A Causal Approach", "comment": null, "summary": "Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.", "AI": {"tldr": "CPO\u6846\u67b6\u5c06\u63d0\u793a\u4f18\u5316\u91cd\u6784\u4e3a\u56e0\u679c\u4f30\u8ba1\u95ee\u9898\uff0c\u4f7f\u7528\u53cc\u673a\u5668\u5b66\u4e60\u5b66\u4e60\u79bb\u7ebf\u56e0\u679c\u5956\u52b1\u6a21\u578b\uff0c\u5b9e\u73b0\u65e0\u9700\u5728\u7ebf\u8bc4\u4f30\u7684\u9ad8\u6548\u67e5\u8be2\u7279\u5b9a\u63d0\u793a\u4f18\u5316", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u9759\u6001\u6307\u4ee4\u65e0\u6cd5\u9002\u5e94\u5f02\u6784\u67e5\u8be2\uff1b\u52a8\u6001\u65b9\u6cd5\u4f9d\u8d56\u79bb\u7ebf\u5956\u52b1\u6a21\u578b\uff0c\u4f46\u8fd9\u7c7b\u6a21\u578b\u672c\u8d28\u4e0a\u662f\u76f8\u5173\u6027\u7684\uff0c\u6df7\u6dc6\u4e86\u63d0\u793a\u6548\u679c\u4e0e\u67e5\u8be2\u7279\u5f81", "method": "\u63d0\u51fa\u56e0\u679c\u63d0\u793a\u4f18\u5316(CPO)\u6846\u67b6\uff0c\u5206\u4e24\u9636\u6bb5\uff1a1) \u5bf9\u63d0\u793a\u548c\u67e5\u8be2\u7684\u8bed\u4e49\u5d4c\u5165\u5e94\u7528\u53cc\u673a\u5668\u5b66\u4e60\u5b66\u4e60\u79bb\u7ebf\u56e0\u679c\u5956\u52b1\u6a21\u578b\uff0c\u9694\u79bb\u63d0\u793a\u53d8\u5316\u4e0e\u6df7\u6dc6\u67e5\u8be2\u5c5e\u6027\u7684\u56e0\u679c\u6548\u5e94\uff1b2) \u5229\u7528\u65e0\u504f\u5956\u52b1\u4fe1\u53f7\u6307\u5bfc\u8d44\u6e90\u9ad8\u6548\u7684\u67e5\u8be2\u7279\u5b9a\u63d0\u793a\u641c\u7d22", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u53ef\u89c6\u5316\u548c\u6570\u636e\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCPO\u59cb\u7ec8\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u4f18\u5316\u5668\uff0c\u4e3b\u8981\u6539\u8fdb\u4f53\u73b0\u5728\u56f0\u96be\u67e5\u8be2\u7684\u9c81\u68d2\u6027\u4e0a", "conclusion": "CPO\u4ece\u6839\u672c\u4e0a\u91cd\u5851\u4e86\u63d0\u793a\u4f18\u5316\u7684\u7ecf\u6d4e\u5b66\uff1a\u901a\u8fc7\u5c06\u8bc4\u4f30\u4ece\u5b9e\u65f6\u6a21\u578b\u6267\u884c\u8f6c\u79fb\u5230\u79bb\u7ebf\u56e0\u679c\u6a21\u578b\uff0c\u4ee5\u6781\u4f4e\u7684\u63a8\u7406\u6210\u672c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u6309\u67e5\u8be2\u5b9a\u5236\uff0c\u4e3a\u53ef\u9760\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u63d0\u793a\u4f18\u5316\u63d0\u4f9b\u53ef\u6269\u5c55\u57fa\u7840", "topic": "agent analysis"}}
{"id": "2602.01672", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01672", "abs": "https://arxiv.org/abs/2602.01672", "authors": ["Siheng Xiong", "Oguzhan Gungordu", "Blair Johnson", "James C. Kerce", "Faramarz Fekri"], "title": "Scaling Search-Augmented LLM Reasoning via Adaptive Information Control", "comment": "Work in progress", "summary": "Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.", "AI": {"tldr": "DeepControl\u6846\u67b6\u901a\u8fc7\u4fe1\u606f\u6548\u7528\u6982\u5ff5\u5b9e\u73b0\u81ea\u9002\u5e94\u4fe1\u606f\u63a7\u5236\uff0c\u5728\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u4e2d\u8c03\u8282\u68c0\u7d22\u65f6\u673a\u548c\u7c92\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u4ee3\u7406\u5728\u68c0\u7d22\u63a7\u5236\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff1a\u65e0\u63a7\u5236\u7684\u68c0\u7d22\u5bfc\u81f4\u5197\u4f59\u8bc1\u636e\u3001\u4e0a\u4e0b\u6587\u9971\u548c\u548c\u4e0d\u7a33\u5b9a\u5b66\u4e60\u3002\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u5bf9\u4fe1\u606f\u83b7\u53d6\u8c03\u8282\u6307\u5bfc\u6709\u9650\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u4fe1\u606f\u63a7\u5236\u673a\u5236\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u6548\u7528\u7684DeepControl\u6846\u67b6\uff0c\u4fe1\u606f\u6548\u7528\u8861\u91cf\u7ed9\u5b9a\u63a8\u7406\u72b6\u6001\u4e0b\u68c0\u7d22\u8bc1\u636e\u7684\u8fb9\u9645\u4ef7\u503c\u3002\u5f15\u5165\u68c0\u7d22\u8fde\u7eed\u6027\u548c\u7c92\u5ea6\u63a7\u5236\u673a\u5236\uff0c\u9009\u62e9\u6027\u8c03\u8282\u4f55\u65f6\u7ee7\u7eed/\u505c\u6b62\u68c0\u7d22\u4ee5\u53ca\u6269\u5c55\u591a\u5c11\u4fe1\u606f\u3002\u91c7\u7528\u9000\u706b\u63a7\u5236\u7b56\u7565\u4f7f\u4ee3\u7406\u5728\u8bad\u7ec3\u4e2d\u5185\u5316\u6709\u6548\u4fe1\u606f\u83b7\u53d6\u884c\u4e3a\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u3002\u5728Qwen2.5-7B\u548cQwen2.5-3B\u4e0a\u5206\u522b\u6bd4\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u5e73\u5747\u63d0\u53479.4%\u548c8.6%\uff0c\u6301\u7eed\u4f18\u4e8e\u65e0\u68c0\u7d22\u548c\u57fa\u4e8e\u68c0\u7d22\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u9002\u5e94\u4fe1\u606f\u63a7\u5236\u5bf9\u4e8e\u5c06\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u4ee3\u7406\u6269\u5c55\u5230\u590d\u6742\u3001\u771f\u5b9e\u4e16\u754c\u4fe1\u606f\u73af\u5883\u81f3\u5173\u91cd\u8981\u3002DeepControl\u901a\u8fc7\u5f62\u5f0f\u5316\u7684\u4fe1\u606f\u6548\u7528\u6982\u5ff5\u548c\u7cbe\u7ec6\u63a7\u5236\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u68c0\u7d22\u63a7\u5236\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2602.01750", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01750", "abs": "https://arxiv.org/abs/2602.01750", "authors": ["Mohammad Beigi", "Ming Jin", "Junshan Zhang", "Qifan Wang", "Lifu Huang"], "title": "Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.", "AI": {"tldr": "ARA\u6846\u67b6\u5c06\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u91cd\u6784\u4e3a\u52a8\u6001\u7ade\u4e89\u6e38\u620f\uff0c\u901a\u8fc7\u9ed1\u5ba2\u53d1\u73b0\u6f0f\u6d1e\u3001\u5ba1\u8ba1\u5458\u68c0\u6d4b\u5229\u7528\uff0c\u518d\u901a\u8fc7AG-RLHF\u95e8\u63a7\u5956\u52b1\u4fe1\u53f7\u6765\u60e9\u7f5a\u68c0\u6d4b\u5230\u7684\u9ed1\u5ba2\u884c\u4e3a\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u5bf9\u9f50-\u6548\u7528\u6743\u8861\u3002", "motivation": "RLHF\u5bb9\u6613\u53d7\u5230\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u73b0\u6709\u9759\u6001\u9632\u5fa1\u65e0\u6cd5\u9002\u5e94\u65b0\u7684\u5229\u7528\u7b56\u7565\u3002\u9700\u8981\u5c06\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u52a8\u6001\u7ade\u4e89\u6e38\u620f\uff0c\u5b9e\u73b0\u53ef\u6d4b\u91cf\u3001\u53ef\u63a7\u7684\u9632\u5fa1\u3002", "method": "ARA\u6846\u67b6\u5206\u4e24\u9636\u6bb5\uff1a1) \u9ed1\u5ba2\u7b56\u7565\u53d1\u73b0\u5956\u52b1\u6a21\u578b\u6f0f\u6d1e\uff0c\u5ba1\u8ba1\u5458\u4ece\u6f5c\u5728\u8868\u793a\u4e2d\u5b66\u4e60\u68c0\u6d4b\u5229\u7528\uff1b2) \u5ba1\u8ba1\u5458\u5f15\u5bfc\u7684RLHF(AG-RLHF)\u95e8\u63a7\u5956\u52b1\u4fe1\u53f7\uff0c\u60e9\u7f5a\u68c0\u6d4b\u5230\u7684\u9ed1\u5ba2\u884c\u4e3a\u3002", "result": "\u5728\u4e09\u79cd\u9ed1\u5ba2\u573a\u666f\u4e2d\uff0cARA\u5728\u6240\u6709\u57fa\u7ebf\u4e2d\u5b9e\u73b0\u6700\u4f73\u5bf9\u9f50-\u6548\u7528\u6743\u8861\uff1a\u5c06\u5949\u627f\u964d\u81f3\u63a5\u8fd1SFT\u6c34\u5e73\u540c\u65f6\u63d0\u9ad8\u5e2e\u52a9\u6027\uff0c\u51cf\u5c11\u5197\u957f\u540c\u65f6\u83b7\u5f97\u6700\u9ad8ROUGE-L\uff0c\u6291\u5236\u4ee3\u7801\u6e38\u620f\u540c\u65f6\u63d0\u9ad8Pass@1\u3002\u5956\u52b1\u9ed1\u5ba2\u3001\u68c0\u6d4b\u548c\u7f13\u89e3\u90fd\u8de8\u9886\u57df\u6cdb\u5316\u3002", "conclusion": "ARA\u5c06\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u4ece\u4e0d\u53ef\u89c2\u5bdf\u7684\u5931\u8d25\u8f6c\u53d8\u4e3a\u53ef\u6d4b\u91cf\u3001\u53ef\u63a7\u7684\u4fe1\u53f7\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u591a\u9886\u57df\u9632\u5fa1\uff0c\u5355\u4e2a\u6a21\u578b\u5c31\u80fd\u8de8\u9886\u57df\u6291\u5236\u5229\u7528\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01698", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01698", "abs": "https://arxiv.org/abs/2602.01698", "authors": ["Wenhui Tan", "Fiorenzo Parascandolo", "Enver Sangineto", "Jianzhong Ju", "Zhenbo Luo", "Qian Cao", "Rita Cucchiara", "Ruihua Song", "Jian Luan"], "title": "Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLatent Exploration Decoding (LED)\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408\u4e2d\u95f4\u5c42\u540e\u9a8c\u5206\u5e03\u6765\u7f13\u89e3\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u5bfc\u81f4\u7684\u5927\u63a8\u7406\u6a21\u578b\u63a2\u7d22\u5d29\u6e83\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u63a8\u7406\u540e\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u610f\u5916\u7684\u63a2\u7d22\u5d29\u6e83\uff1a\u57fa\u4e8e\u6e29\u5ea6\u7684\u91c7\u6837\u4e0d\u518d\u63d0\u9ad8pass@n\u51c6\u786e\u7387\u3002\u7814\u7a76\u53d1\u73b0\u540e\u8bad\u7ec3LRMs\u7684\u6700\u540e\u4e00\u5c42\u540e\u9a8c\u5206\u5e03\u71b5\u6025\u5267\u51cf\u5c11\uff0c\u800c\u4e2d\u95f4\u5c42\u71b5\u4fdd\u6301\u76f8\u5bf9\u8f83\u9ad8\uff0c\u8fd9\u79cd\u71b5\u4e0d\u5bf9\u79f0\u6027\u4fc3\u4f7f\u63d0\u51fa\u65b0\u7684\u89e3\u7801\u7b56\u7565\u3002", "method": "\u63d0\u51faLatent Exploration Decoding (LED)\uff0c\u4e00\u79cd\u6df1\u5ea6\u6761\u4ef6\u89e3\u7801\u7b56\u7565\u3002\u901a\u8fc7\u7d2f\u79ef\u548c\u805a\u5408\u4e2d\u95f4\u540e\u9a8c\u5206\u5e03\uff0c\u9009\u62e9\u5177\u6709\u6700\u5927\u71b5\u7684\u6df1\u5ea6\u914d\u7f6e\u4f5c\u4e3a\u63a2\u7d22\u5019\u9009\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u53c2\u6570\u3002", "result": "LED\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\u4e00\u81f4\u63d0\u5347pass@1\u548cpass@16\u51c6\u786e\u7387\uff0c\u5206\u522b\u63d0\u9ad80.61\u548c1.03\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "LED\u901a\u8fc7\u5229\u7528\u4e2d\u95f4\u5c42\u7684\u9ad8\u71b5\u7279\u6027\u6709\u6548\u7f13\u89e3\u63a2\u7d22\u5d29\u6e83\u95ee\u9898\uff0c\u4e3a\u63a8\u7406\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u4f18\u5316\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u89e3\u7801\u7b56\u7565\u3002", "topic": "agent analysis"}}
{"id": "2602.01709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01709", "abs": "https://arxiv.org/abs/2602.01709", "authors": ["Xingshan Zeng", "Lingzhi Wang", "Weiwen Liu", "Liangyou Li", "Yasheng Wang", "Lifeng Shang", "Xin Jiang", "Qun Liu"], "title": "ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation", "comment": null, "summary": "Current test-time scaling (TTS) techniques enhance large language model (LLM) performance by allocating additional computation at inference time, yet they remain insufficient for agentic settings, where actions directly interact with external environments and their effects can be irreversible and costly. We propose \\emph{\\name}, \\emph{\\underline{A}gentic \\underline{R}isk-Aware \\underline{T}est-Time Scaling via \\underline{I}terative \\underline{S}imulation}, a framework that decouples exploration from commitment by enabling test-time exploration through simulated interactions prior to real-world execution. This design allows extending inference-time computation to improve action-level reliability and robustness without incurring environmental risk. We further show that naive LLM-based simulators struggle to capture rare but high-impact failure modes, substantially limiting their effectiveness for agentic decision making. To address this limitation, we introduce a \\emph{risk-aware tool simulator} that emphasizes fidelity on failure-inducing actions via targeted data generation and rebalanced training. Experiments on multi-turn and multi-step agentic benchmarks demonstrate that iterative simulation substantially improves agent reliability, and that risk-aware simulation is essential for consistently realizing these gains across models and tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faARTIS\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u6a21\u62df\u5b9e\u73b0\u98ce\u9669\u611f\u77e5\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u63d0\u5347\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u6280\u672f\u867d\u7136\u80fd\u63d0\u5347LLM\u6027\u80fd\uff0c\u4f46\u5728\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u4e0d\u8db3\uff0c\u56e0\u4e3a\u667a\u80fd\u4f53\u52a8\u4f5c\u4f1a\u4e0e\u5916\u90e8\u73af\u5883\u76f4\u63a5\u4ea4\u4e92\uff0c\u53ef\u80fd\u4ea7\u751f\u4e0d\u53ef\u9006\u4e14\u4ee3\u4ef7\u9ad8\u6602\u7684\u5f71\u54cd", "method": "\u63d0\u51faARTIS\u6846\u67b6\uff0c\u5c06\u63a2\u7d22\u4e0e\u6267\u884c\u89e3\u8026\uff0c\u901a\u8fc7\u6a21\u62df\u4ea4\u4e92\u8fdb\u884c\u6d4b\u8bd5\u65f6\u63a2\u7d22\uff1b\u5f15\u5165\u98ce\u9669\u611f\u77e5\u5de5\u5177\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u6570\u636e\u751f\u6210\u548c\u91cd\u65b0\u5e73\u8861\u8bad\u7ec3\u6765\u6355\u6349\u7f55\u89c1\u4f46\u9ad8\u5f71\u54cd\u7684\u5931\u8d25\u6a21\u5f0f", "result": "\u5728\u591a\u8f6e\u591a\u6b65\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8fed\u4ee3\u6a21\u62df\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u53ef\u9760\u6027\uff0c\u98ce\u9669\u611f\u77e5\u6a21\u62df\u5bf9\u4e8e\u5728\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u6301\u7eed\u5b9e\u73b0\u8fd9\u4e9b\u6536\u76ca\u81f3\u5173\u91cd\u8981", "conclusion": "ARTIS\u6846\u67b6\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u7684\u8fed\u4ee3\u6a21\u62df\uff0c\u6709\u6548\u6269\u5c55\u4e86\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u5230\u667a\u80fd\u4f53\u51b3\u7b56\u4e2d\uff0c\u63d0\u5347\u4e86\u52a8\u4f5c\u7ea7\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u907f\u514d\u4e86\u73af\u5883\u98ce\u9669", "topic": "agent analysis"}}
{"id": "2602.01797", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01797", "abs": "https://arxiv.org/abs/2602.01797", "authors": ["Hanlin Zhou", "Huah Yong Chan"], "title": "ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing", "comment": null, "summary": "Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.", "AI": {"tldr": "ORCH\u662f\u4e00\u4e2a\u786e\u5b9a\u6027\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\uff0c\u91c7\u7528\"\u591a\u5206\u6790\u3001\u4e00\u51b3\u7b56\"\u8303\u5f0f\uff0c\u901a\u8fc7\u56fa\u5b9a\u89c4\u5219\u7684\u4efb\u52a1\u5206\u89e3\u548c\u7b54\u6848\u805a\u5408\uff0c\u5728\u79bb\u6563\u9009\u62e9\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u578b\u57fa\u7ebf\u548c\u591a\u6570\u6295\u7968\u96c6\u6210\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u968f\u673a\u8def\u7531\u6216\u4e34\u65f6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5bfc\u81f4\u884c\u4e3a\u96be\u4ee5\u590d\u73b0\u3001\u51b3\u7b56\u8fc7\u7a0b\u96be\u4ee5\u89e3\u91ca\u3002\u9700\u8981\u4e00\u79cd\u786e\u5b9a\u6027\u7684\u534f\u8c03\u6846\u67b6\u6765\u63d0\u9ad8\u53ef\u9884\u6d4b\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "ORCH\u91c7\u7528\"\u591a\u5206\u6790\u3001\u4e00\u51b3\u7b56\"\u8303\u5f0f\uff1a\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u72ec\u7acb\u751f\u6210\u7ed3\u6784\u5316\u5206\u6790\uff0c\u4e13\u7528\u5408\u5e76\u667a\u80fd\u4f53\u8f93\u51fa\u6700\u7ec8\u9009\u62e9\u3002\u4f7f\u7528\u56fa\u5b9a\u89c4\u5219\u8fdb\u884c\u4efb\u52a1\u5206\u89e3\u548c\u7b54\u6848\u805a\u5408\uff0c\u4fdd\u6301\u6d41\u7a0b\u53ef\u9884\u6d4b\u3001\u53ef\u590d\u73b0\u4e14\u65e0\u9700\u8bad\u7ec3\u3002\u53ef\u9009\u5730\u5f15\u5165EMA\u5f15\u5bfc\u7684\u8def\u7531\u5668\uff0c\u57fa\u4e8e\u5386\u53f2\u51c6\u786e\u7387\u3001\u5ef6\u8fdf\u6216\u6210\u672c\u66f4\u65b0\u667a\u80fd\u4f53\u9009\u62e9\u3002", "result": "\u5728MMLU\u3001MMLU-Pro\u548cGSM8K\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cORCH\u6301\u7eed\u4f18\u4e8e\u5355\u6a21\u578b\u57fa\u7ebf\u548c\u591a\u6570\u6295\u7968\u96c6\u6210\u3002\u5728MMLU-Pro\u4e0a\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc710\u4e2a\u767e\u5206\u70b9\uff0c\u5728GSM8K\u4e0a\u63d0\u5347\u8d85\u8fc750\u4e2a\u767e\u5206\u70b9\u3002EMA\u8def\u7531\u5668\u63d0\u4f9b\u989d\u59160.7-2.0\u4e2a\u767e\u5206\u70b9\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "ORCH\u4e3a\u79bb\u6563\u9009\u62e9\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u90e8\u7f72\u7684LLM\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u534f\u8c03\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.01815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01815", "abs": "https://arxiv.org/abs/2602.01815", "authors": ["Yunhui Jang", "Seonghyun Park", "Jaehyung Kim", "Sungsoo Ahn"], "title": "INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery", "comment": null, "summary": "Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.", "AI": {"tldr": "INDIBATOR\u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u79d1\u5b66\u5bb6\u4e2a\u6027\u5316\u6863\u6848\uff08\u53d1\u8868\u5386\u53f2\u548c\u5206\u5b50\u5386\u53f2\uff09\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5728\u5206\u5b50\u53d1\u73b0\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u4f20\u7edf\u57fa\u4e8e\u89d2\u8272\u7684\u7c97\u7c92\u5ea6\u667a\u80fd\u4f53\u65b9\u6cd5", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u4f7f\u7528\u57fa\u4e8e\u89d2\u8272\u7684\u901a\u7528\u89d2\u8272\uff08\u5982\"\u5ba1\u7a3f\u4eba\"\u3001\"\u4f5c\u8005\"\uff09\u6216\u7c97\u7c92\u5ea6\u7684\u5173\u952e\u8bcd\u89d2\u8272\uff0c\u8fd9\u8fc7\u5ea6\u7b80\u5316\u4e86\u79d1\u5b66\u5bb6\u7684\u5de5\u4f5c\u65b9\u5f0f\u3002\u4eba\u7c7b\u79d1\u5b66\u5bb6\u7684\u8d21\u732e\u53d7\u5230\u5176\u72ec\u7279\u7814\u7a76\u8f68\u8ff9\u7684\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u4e2a\u6027\u5316\u5efa\u6a21", "method": "\u63d0\u51faINDIBATOR\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u79cd\u6a21\u6001\u6784\u5efa\u4e2a\u4f53\u5316\u79d1\u5b66\u5bb6\u6863\u6848\uff1a1) \u53d1\u8868\u5386\u53f2\uff08\u6587\u732e\u77e5\u8bc6\uff09 2) \u5206\u5b50\u5386\u53f2\uff08\u7ed3\u6784\u5148\u9a8c\uff09\u3002\u8fd9\u4e9b\u667a\u80fd\u4f53\u901a\u8fc7\u63d0\u8bae\u3001\u6279\u8bc4\u548c\u6295\u7968\u9636\u6bb5\u8fdb\u884c\u591a\u8f6e\u8fa9\u8bba", "result": "\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u4e2a\u4f53\u5316\u6863\u6848\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u6301\u7eed\u4f18\u4e8e\u4f9d\u8d56\u7c97\u7c92\u5ea6\u89d2\u8272\u7684\u7cfb\u7edf\uff0c\u8fbe\u5230\u7ade\u4e89\u6027\u6216\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73", "conclusion": "\u6355\u6349\u667a\u80fd\u4f53\u7684\"\u79d1\u5b66DNA\"\u5bf9\u4e8e\u9ad8\u8d28\u91cf\u7684\u79d1\u5b66\u53d1\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4e2a\u4f53\u5316\u6863\u6848\u80fd\u663e\u8457\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6027\u80fd", "topic": "agent analysis"}}
{"id": "2602.01848", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.01848", "abs": "https://arxiv.org/abs/2602.01848", "authors": ["Salaheddin Alzu'bi", "Baran Nama", "Arda Kaz", "Anushri Eswaran", "Weiyuan Chen", "Sarvesh Khetan", "Rishab Bala", "Tu Vu", "Sewoong Oh"], "title": "ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems", "comment": null, "summary": "Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.", "AI": {"tldr": "ROMA\u662f\u4e00\u4e2a\u9012\u5f52\u5f00\u653e\u5143\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u7ed3\u6784\u5316\u805a\u5408\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u652f\u6301\u5f02\u6784\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408GEPA+\u63d0\u793a\u641c\u7d22\u5b9e\u73b0\u9886\u5148\u7684\u7cfb\u7edf\u7ea7\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4ee3\u7406\u6846\u67b6\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u968f\u7740\u63a8\u7406\u6df1\u5ea6\u589e\u52a0\uff0c\u987a\u5e8f\u7f16\u6392\u53d8\u5f97\u8106\u5f31\uff0c\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e0d\u900f\u660e\u7684\u6267\u884c\u8f68\u8ff9\u4f7f\u5f97\u6545\u969c\u96be\u4ee5\u5b9a\u4f4d\u6216\u8c03\u8bd5\u3002", "method": "ROMA\u901a\u8fc7\u9012\u5f52\u4efb\u52a1\u5206\u89e3\u548c\u7ed3\u6784\u5316\u805a\u5408\uff0c\u5c06\u76ee\u6807\u5206\u89e3\u4e3a\u4f9d\u8d56\u611f\u77e5\u7684\u5b50\u4efb\u52a1\u6811\u5e76\u884c\u6267\u884c\uff0c\u540c\u65f6\u805a\u5408\u538b\u7f29\u548c\u9a8c\u8bc1\u4e2d\u95f4\u7ed3\u679c\u4ee5\u63a7\u5236\u4e0a\u4e0b\u6587\u589e\u957f\u3002\u6846\u67b6\u56f4\u7ed5\u56db\u4e2a\u6a21\u5757\u5316\u89d2\u8272\u6784\u5efa\uff1aAtomizer\uff08\u51b3\u5b9a\u4efb\u52a1\u662f\u5426\u5206\u89e3\uff09\u3001Planner\u3001Executor\u548cAggregator\uff0c\u652f\u6301\u5f02\u6784\u591a\u4ee3\u7406\u7cfb\u7edf\u6df7\u5408\u6a21\u578b\u548c\u5de5\u5177\u3002GEPA+\u662f\u6539\u8fdb\u7684\u9057\u4f20\u5e15\u7d2f\u6258\u63d0\u793a\u63d0\u8bae\u5668\uff0c\u5728ROMA\u7ec4\u4ef6\u5c42\u6b21\u7ed3\u6784\u4e2d\u641c\u7d22\u63d0\u793a\u3002", "result": "\u5728SEAL-0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cROMA\u7ed3\u5408GLM-4.6\u5c06\u51c6\u786e\u7387\u6bd4Kimi-Researcher\u63d0\u9ad89.9%\uff1b\u5728EQ-Bench\u957f\u6587\u672c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cROMA\u4f7fDeepSeek-V3\u80fd\u591f\u5339\u914dClaude Sonnet 4.5\u7b49\u9886\u5148\u95ed\u6e90\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u9012\u5f52\u6a21\u5757\u5316\u4ee3\u7406\u67b6\u6784\u53ef\u4ee5\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3001\u7075\u6d3b\u6027\u548c\u6a21\u578b\u65e0\u5173\u6027\u7684\u540c\u65f6\u6269\u5c55\u63a8\u7406\u6df1\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2602.01725", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01725", "abs": "https://arxiv.org/abs/2602.01725", "authors": ["Yurun Chen", "Zeyi Liao", "Ping Yin", "Taotao Xie", "Keting Yin", "Shengyu Zhang"], "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models", "comment": null, "summary": "With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.", "AI": {"tldr": "SafePred\u662f\u4e00\u4e2a\u9884\u6d4b\u6027\u62a4\u680f\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9884\u6d4b\u7684\u672a\u6765\u98ce\u9669\u4e0e\u5f53\u524d\u51b3\u7b56\u5bf9\u9f50\uff0c\u9632\u6b62\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u957f\u671f\u98ce\u9669\uff0c\u76f8\u6bd4\u53cd\u5e94\u5f0f\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u62a4\u680f\u4e3b\u8981\u91c7\u7528\u53cd\u5e94\u5f0f\u65b9\u6cd5\uff0c\u53ea\u80fd\u7ea6\u675f\u5f53\u524d\u89c2\u5bdf\u7a7a\u95f4\u5185\u7684\u884c\u4e3a\uff0c\u65e0\u6cd5\u9884\u9632\u957f\u671f\u98ce\u9669\u3002\u770b\u4f3c\u5408\u7406\u7684\u884c\u52a8\u53ef\u80fd\u5bfc\u81f4\u5ef6\u8fdf\u51fa\u73b0\u7684\u9ad8\u98ce\u9669\u540e\u679c\uff0c\u8fd9\u4e9b\u662f\u53cd\u5e94\u5f0f\u62a4\u680f\u65e0\u6cd5\u5728\u5f53\u524d\u89c2\u5bdf\u7a7a\u95f4\u5185\u8bc6\u522b\u7684\u3002", "method": "\u63d0\u51fa\u9884\u6d4b\u6027\u62a4\u680f\u65b9\u6cd5\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u5c06\u9884\u6d4b\u7684\u672a\u6765\u98ce\u9669\u4e0e\u5f53\u524d\u51b3\u7b56\u5bf9\u9f50\u3002SafePred\u6846\u67b6\u5efa\u7acb\u98ce\u9669\u5230\u51b3\u7b56\u7684\u5faa\u73af\uff0c\u652f\u6301\u4e24\u79cd\u5173\u952e\u80fd\u529b\uff1a1) \u77ed\u671f\u548c\u957f\u671f\u98ce\u9669\u9884\u6d4b\uff0c\u4f7f\u7528\u5b89\u5168\u7b56\u7565\u4f5c\u4e3a\u98ce\u9669\u9884\u6d4b\u57fa\u7840\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u751f\u6210\u98ce\u9669\u8bed\u4e49\u8868\u793a\uff1b2) \u51b3\u7b56\u4f18\u5316\uff0c\u901a\u8fc7\u6b65\u9aa4\u7ea7\u5e72\u9884\u548c\u4efb\u52a1\u7ea7\u91cd\u65b0\u89c4\u5212\u5c06\u9884\u6d4b\u98ce\u9669\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u5b89\u5168\u51b3\u7b56\u6307\u5bfc\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSafePred\u663e\u8457\u51cf\u5c11\u4e86\u9ad8\u98ce\u9669\u884c\u4e3a\uff0c\u76f8\u6bd4\u53cd\u5e94\u5f0f\u57fa\u7ebf\u5b9e\u73b0\u4e86\u8d85\u8fc797.6%\u7684\u5b89\u5168\u6027\u80fd\uff0c\u5e76\u5c06\u4efb\u52a1\u6548\u7528\u63d0\u9ad8\u4e86\u9ad8\u8fbe21.4%\u3002", "conclusion": "\u9884\u6d4b\u6027\u62a4\u680f\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u957f\u671f\u98ce\u9669\u95ee\u9898\uff0cSafePred\u6846\u67b6\u901a\u8fc7\u98ce\u9669\u9884\u6d4b\u548c\u51b3\u7b56\u4f18\u5316\u7684\u7ed3\u5408\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u63d0\u5347\u4efb\u52a1\u6548\u7528\u3002", "topic": "agent analysis"}}
{"id": "2602.00628", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00628", "abs": "https://arxiv.org/abs/2602.00628", "authors": ["Louis Schiekiera", "Max Zimmer", "Christophe Roux", "Sebastian Pokutta", "Fritz G\u00fcnther"], "title": "From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs", "comment": "25 pages including references, 15 figures, 6 tables", "summary": "We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.", "AI": {"tldr": "\u901a\u8fc7\u5fc3\u7406\u8bed\u8a00\u5b66\u5b9e\u9a8c\u7814\u7a76LLM\u9690\u85cf\u72b6\u6001\u51e0\u4f55\u7ed3\u6784\u80fd\u5426\u4ece\u5176\u884c\u4e3a\u4e2d\u6062\u590d\uff0c\u53d1\u73b0\u5f3a\u5236\u9009\u62e9\u4efb\u52a1\u6bd4\u81ea\u7531\u8054\u60f3\u4efb\u52a1\u66f4\u80fd\u53cd\u6620\u9690\u85cf\u72b6\u6001\u51e0\u4f55\uff0c\u4e14\u884c\u4e3a\u76f8\u4f3c\u6027\u53ef\u9884\u6d4b\u672a\u89c1\u8bcd\u7684\u9690\u85cf\u72b6\u6001\u76f8\u4f3c\u6027", "motivation": "\u63a2\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7LLM\u5728\u5fc3\u7406\u8bed\u8a00\u5b66\u5b9e\u9a8c\u4e2d\u7684\u884c\u4e3a\u8868\u73b0\u6765\u6062\u590d\u5176\u9690\u85cf\u72b6\u6001\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u7406\u89e3\u884c\u4e3a\u6d4b\u91cf\u80fd\u5426\u63ed\u793a\u5185\u90e8\u8bed\u4e49\u8868\u793a", "method": "\u57288\u4e2a\u6307\u4ee4\u8c03\u4f18\u7684Transformer\u6a21\u578b\u4e0a\u8fd0\u884c\u4e24\u79cd\u5b9e\u9a8c\u8303\u5f0f\uff08\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u5f3a\u5236\u9009\u62e9\u548c\u81ea\u7531\u8054\u60f3\uff09\uff0c\u6536\u96c61750\u4e07+\u8bd5\u9a8c\u6784\u5efa\u884c\u4e3a\u76f8\u4f3c\u6027\u77e9\u9635\uff0c\u4f7f\u7528\u8868\u5f81\u76f8\u4f3c\u6027\u5206\u6790\u6bd4\u8f83\u884c\u4e3a\u51e0\u4f55\u4e0e\u5c42\u95f4\u9690\u85cf\u72b6\u6001\u76f8\u4f3c\u6027\uff0c\u5e76\u4e0eFastText\u3001BERT\u548c\u8de8\u6a21\u578b\u5171\u8bc6\u8fdb\u884c\u57fa\u51c6\u6bd4\u8f83", "result": "\u5f3a\u5236\u9009\u62e9\u884c\u4e3a\u6bd4\u81ea\u7531\u8054\u60f3\u66f4\u663e\u8457\u5730\u4e0e\u9690\u85cf\u72b6\u6001\u51e0\u4f55\u5bf9\u9f50\uff1b\u5728\u672a\u89c1\u8bcd\u56de\u5f52\u4e2d\uff0c\u884c\u4e3a\u76f8\u4f3c\u6027\uff08\u7279\u522b\u662f\u5f3a\u5236\u9009\u62e9\uff09\u80fd\u591f\u8d85\u8d8a\u8bcd\u6c47\u57fa\u7ebf\u548c\u8de8\u6a21\u578b\u5171\u8bc6\u9884\u6d4b\u672a\u89c1\u9690\u85cf\u72b6\u6001\u76f8\u4f3c\u6027\uff0c\u8868\u660e\u4ec5\u884c\u4e3a\u6d4b\u91cf\u4fdd\u7559\u4e86\u5173\u4e8e\u5185\u90e8\u8bed\u4e49\u51e0\u4f55\u7684\u53ef\u6062\u590d\u4fe1\u606f", "conclusion": "\u884c\u4e3a\u4efb\u52a1\u80fd\u591f\u63ed\u793aLLM\u7684\u9690\u85cf\u8ba4\u77e5\u72b6\u6001\uff0c\u5f3a\u5236\u9009\u62e9\u8303\u5f0f\u6bd4\u81ea\u7531\u8054\u60f3\u66f4\u80fd\u6709\u6548\u53cd\u6620\u5185\u90e8\u8bed\u4e49\u51e0\u4f55\u7ed3\u6784\uff0c\u4e3a\u901a\u8fc7\u884c\u4e3a\u5b9e\u9a8c\u7406\u89e3\u6a21\u578b\u5185\u90e8\u8868\u793a\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301", "topic": "agent analysis"}}
{"id": "2602.00636", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.00636", "abs": "https://arxiv.org/abs/2602.00636", "authors": ["Yujie Yang", "Zhilong Zheng", "Shengbo Eben Li"], "title": "Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration", "comment": null, "summary": "Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411\u5747\u8861\u7684\u5b89\u5168\u63a2\u7d22\u6846\u67b6SEE\uff0c\u901a\u8fc7\u4ea4\u66ff\u5bfb\u627e\u6700\u5927\u53ef\u884c\u533a\u57df\u548c\u6700\u5c0f\u4e0d\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u5b9e\u73b0\u96f6\u7ea6\u675f\u8fdd\u53cd\u7684\u5b89\u5168\u63a2\u7d22\u5e76\u6536\u655b\u5230\u5747\u8861\u72b6\u6001\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u63a2\u7d22\u901a\u5e38\u5c06\u63a2\u7d22\u9650\u5236\u5728\u53ef\u884c\u533a\u57df\u5185\uff0c\u4f46\u4e24\u4e2a\u5173\u952e\u95ee\u9898\u672a\u89e3\u51b3\uff1a\u901a\u8fc7\u63a2\u7d22\u80fd\u8fbe\u5230\u7684\u6700\u5927\u53ef\u884c\u533a\u57df\u662f\u4ec0\u4e48\uff1f\u5982\u4f55\u8bc6\u522b\u8fd9\u4e2a\u533a\u57df\uff1f\u8bba\u6587\u9996\u6b21\u63ed\u793a\u4e86\u5b89\u5168\u63a2\u7d22\u7684\u76ee\u6807\u662f\u5728\u53ef\u884c\u533a\u57df\u548c\u73af\u5883\u6a21\u578b\u4e4b\u95f4\u627e\u5230\u5747\u8861\u3002", "method": "\u63d0\u51fa\u5b89\u5168\u5747\u8861\u63a2\u7d22\uff08SEE\uff09\u6846\u67b6\uff0c\u4f7f\u7528\u4e0d\u786e\u5b9a\u6a21\u578b\u7684\u56fe\u8868\u793a\uff0c\u4ea4\u66ff\u8fdb\u884c\u4e24\u4e2a\u6b65\u9aa4\uff1a1\uff09\u5728\u7ed9\u5b9a\u6a21\u578b\u4e0b\u5bfb\u627e\u6700\u5927\u53ef\u884c\u533a\u57df\uff1b2\uff09\u5728\u7ed9\u5b9a\u53ef\u884c\u533a\u57df\u5185\u5bfb\u627e\u6700\u5c0f\u4e0d\u786e\u5b9a\u6027\u6a21\u578b\u3002\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u5355\u8c03\u4f18\u5316\u6a21\u578b\u548c\u6269\u5c55\u53ef\u884c\u533a\u57df\u3002", "result": "\u5728\u7ecf\u5178\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEE\u7b97\u6cd5\u80fd\u591f\u6210\u529f\u6269\u5c55\u53ef\u884c\u533a\u57df\u4e14\u96f6\u7ea6\u675f\u8fdd\u53cd\uff0c\u5e76\u5728\u51e0\u6b21\u8fed\u4ee3\u5185\u8fbe\u5230\u5b89\u5168\u63a2\u7d22\u7684\u5747\u8861\u72b6\u6001\u3002", "conclusion": "\u5b89\u5168\u63a2\u7d22\u7684\u76ee\u6807\u662f\u627e\u5230\u53ef\u884c\u533a\u57df\u548c\u73af\u5883\u6a21\u578b\u4e4b\u95f4\u7684\u5747\u8861\uff0c\u4e24\u8005\u76f8\u4e92\u4f9d\u8d56\uff1a\u66f4\u5927\u7684\u53ef\u884c\u533a\u57df\u5e26\u6765\u66f4\u51c6\u786e\u7684\u73af\u5883\u6a21\u578b\uff0c\u66f4\u51c6\u786e\u7684\u6a21\u578b\u53c8\u80fd\u63a2\u7d22\u66f4\u5927\u7684\u533a\u57df\u3002SEE\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4e86\u8fd9\u79cd\u5747\u8861\u5bfc\u5411\u7684\u5b89\u5168\u63a2\u7d22\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01983", "abs": "https://arxiv.org/abs/2602.01983", "authors": ["Xintian Shen", "Jiawei Chen", "Lihao Zheng", "Hao Ma", "Tao Wei", "Kun Zhan"], "title": "Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning", "comment": null, "summary": "Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\\uparrow$ and +23.04%$\\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.", "AI": {"tldr": "UCT\u6846\u67b6\u5c06LLM\u4ece\u5de5\u5177\u4f7f\u7528\u8005\u8f6c\u53d8\u4e3a\u5de5\u5177\u521b\u9020\u8005\uff0c\u901a\u8fc7\u7ecf\u9a8c\u84b8\u998f\u5b9e\u73b0\u81ea\u9002\u5e94\u5de5\u5177\u521b\u5efa\u548c\u81ea\u66f4\u65b0\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u96c6\u6210\u63a8\u7406\u6a21\u578b\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u56fa\u5b9a\u5de5\u5177\u96be\u4ee5\u5e94\u5bf9\u5f00\u653e\u6027\u95ee\u9898\uff1b2) \u7f3a\u4e4f\u81ea\u4f18\u5316\u673a\u5236\uff0c\u9519\u8bef\u5de5\u5177\u8f93\u51fa\u4f1a\u8bef\u5bfcLLM\uff1b3) \u5de5\u5177\u6784\u5efa\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5de5\u4f5c\uff0c\u9650\u5236\u4e86\u9002\u7528\u6027\u3002", "method": "\u63d0\u51faUCT\u6846\u67b6\uff0c\u5c06LLM\u4ece\u5de5\u5177\u4f7f\u7528\u8005\u8f6c\u53d8\u4e3a\u5de5\u5177\u521b\u9020\u8005\u3002\u901a\u8fc7\u6536\u96c6\u63a8\u7406\u7ecf\u9a8c\u5e76\u84b8\u998f\u4e3a\u53ef\u91cd\u7528\u8d44\u4ea7\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u5de5\u5177\u521b\u5efa\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u81ea\u66f4\u65b0\u3002\u5f15\u5165\u8bb0\u5fc6\u5de9\u56fa\u673a\u5236\u7ef4\u62a4\u5de5\u5177\u5e93\uff0c\u786e\u4fdd\u7ecf\u9a8c\u8bb0\u5fc6\u7684\u9ad8\u53ef\u91cd\u7528\u6027\u3002", "result": "\u5728\u8de8\u9886\u57df\u6570\u5b66\u548c\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUCT\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff1a+20.86%\u548c+23.04%\uff0c\u9a8c\u8bc1\u4e86\u4ee3\u7406\u7684\u81ea\u8fdb\u5316\u80fd\u529b\u3002", "conclusion": "UCT\u4e3a\u589e\u5f3aTIR\u6a21\u578b\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u5177\u6784\u5efa\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6301\u7eed\u6539\u8fdb\u5de5\u5177\u8d28\u91cf\uff0c\u4f7f\u6574\u4f53\u4ee3\u7406\u7cfb\u7edf\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u8fdb\u6b65\u3002", "topic": "code agent"}}
{"id": "2602.00704", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00704", "abs": "https://arxiv.org/abs/2602.00704", "authors": ["Hanqi Lyu", "Di Huang", "Yaoyu Zhu", "Kangcheng Liu", "Bohan Dou", "Chongxiao Li", "Pengwei Jin", "Shuyao Cheng", "Rui Zhang", "Zidong Du", "Qi Guo", "Xing Hu", "Yunji Chen"], "title": "LocalV: Exploiting Information Locality for IP-level Verilog Generation", "comment": null, "summary": "The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.", "AI": {"tldr": "LocalV\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u5757\u5316\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u4fe1\u606f\u5c40\u90e8\u6027\uff0c\u5c06\u957f\u6587\u6863\u5230\u957f\u4ee3\u7801\u751f\u6210\u95ee\u9898\u5206\u89e3\u4e3a\u77ed\u6587\u6863\u3001\u77ed\u4ee3\u7801\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86RTL\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "RTL\u4ee3\u7801\u751f\u6210\u662f\u6570\u5b57\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u4f46\u52b3\u52a8\u5bc6\u96c6\u578b\u6b65\u9aa4\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5de5\u7a0b\u5e08\u624b\u52a8\u5c06\u590d\u6742\u89c4\u8303\u8f6c\u6362\u4e3a\u6570\u5343\u884c\u53ef\u7efc\u5408\u7684HDL\u4ee3\u7801\u3002\u73b0\u6709LLM\u65b9\u6cd5\u5728\u5904\u7406\u5de5\u4e1a\u7ea7IP\u8bbe\u8ba1\u4efb\u52a1\u65f6\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u5904\u7406\u957f\u800c\u8be6\u7ec6\u7684\u6587\u6863\u3001\u751f\u6210\u957fRTL\u4ee3\u7801\u65f6\u6b63\u786e\u6027\u4e0b\u964d\u3001\u4ee5\u53ca\u590d\u6742\u7684\u8c03\u8bd5\u5468\u671f\u3002", "method": "LocalV\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u6a21\u5757\u5316\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u4fe1\u606f\u5c40\u90e8\u6027\u3002\u5177\u4f53\u5305\u62ec\uff1a\u5206\u5c42\u6587\u6863\u5212\u5206\u3001\u4efb\u52a1\u89c4\u5212\u3001\u5c40\u90e8\u5316\u4ee3\u7801\u751f\u6210\u3001\u63a5\u53e3\u4e00\u81f4\u6027\u5408\u5e76\u3001\u4ee5\u53caAST\u5f15\u5bfc\u7684\u5c40\u90e8\u611f\u77e5\u8c03\u8bd5\u3002", "result": "\u5728IP\u7ea7Verilog\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5RealBench\u4e0a\uff0cLocalV\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684LLM\u548c\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e8645.0%\u7684\u901a\u8fc7\u7387\uff0c\u800c\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u4ec5\u4e3a21.6%\u3002", "conclusion": "LocalV\u901a\u8fc7\u5206\u89e3\u957f\u6587\u6863\u5230\u957f\u4ee3\u7801\u751f\u6210\u95ee\u9898\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u7ea7\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u4e3a\u81ea\u52a8\u5316RTL\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2602.00722", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00722", "abs": "https://arxiv.org/abs/2602.00722", "authors": ["Hao Gu", "Mao-Lin Luo", "Zi-Hao Zhou", "Han-Chen Zhang", "Min-Ling Zhang", "Tong Wei"], "title": "Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation", "comment": "19 pages, 6 figures", "summary": "Parameter-efficient continual learning aims to adapt pre-trained models to sequential tasks without forgetting previously acquired knowledge. Most existing approaches treat continual learning as avoiding interference with past updates, rather than considering what properties make the current task-specific update naturally preserve previously acquired knowledge. From a knowledge-decomposition perspective, we observe that low-rank adaptations exhibit highly imbalanced singular value spectra: a few dominant components absorb most of the adaptation energy, thereby (i) more likely to disrupt previously acquired knowledge and (ii) making the update more vulnerable to interference from subsequent tasks. To enable explicit balance among components, we decouple the magnitude of the task update from its directional structure and formulate it as a constrained optimization problem on a restricted Stiefel manifold. We address this problem using a projected first-order method compatible with standard deep-learning optimizers used in vision-language models. Our method mitigates both backward and forward forgetting, consistently outperforming continual learning baselines. The implementation code is available at https://github.com/haodotgu/EBLoRA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5EBLoRA\uff0c\u901a\u8fc7\u5c06\u4efb\u52a1\u66f4\u65b0\u5206\u89e3\u4e3a\u5e45\u5ea6\u548c\u65b9\u5411\u7ed3\u6784\uff0c\u5728\u53d7\u9650Stiefel\u6d41\u5f62\u4e0a\u8fdb\u884c\u7ea6\u675f\u4f18\u5316\uff0c\u4ee5\u5e73\u8861\u5947\u5f02\u503c\u8c31\uff0c\u51cf\u5c11\u524d\u540e\u5411\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u907f\u514d\u4e0e\u8fc7\u53bb\u66f4\u65b0\u7684\u5e72\u6270\uff0c\u800c\u5ffd\u7565\u4e86\u4ec0\u4e48\u6837\u7684\u4efb\u52a1\u7279\u5b9a\u66f4\u65b0\u80fd\u81ea\u7136\u4fdd\u7559\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\u3002\u4ece\u77e5\u8bc6\u5206\u89e3\u7684\u89d2\u5ea6\u770b\uff0c\u4f4e\u79e9\u9002\u5e94\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u5947\u5f02\u503c\u8c31\uff1a\u5c11\u6570\u4e3b\u5bfc\u6210\u5206\u5438\u6536\u4e86\u5927\u90e8\u5206\u9002\u5e94\u80fd\u91cf\uff0c\u8fd9\u65e2\u5bb9\u6613\u7834\u574f\u5148\u524d\u77e5\u8bc6\uff0c\u53c8\u4f7f\u66f4\u65b0\u66f4\u5bb9\u6613\u53d7\u5230\u540e\u7eed\u4efb\u52a1\u7684\u5e72\u6270\u3002", "method": "\u5c06\u4efb\u52a1\u66f4\u65b0\u7684\u5e45\u5ea6\u4e0e\u65b9\u5411\u7ed3\u6784\u89e3\u8026\uff0c\u5c06\u5176\u8868\u8ff0\u4e3a\u53d7\u9650Stiefel\u6d41\u5f62\u4e0a\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002\u4f7f\u7528\u4e0e\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u5668\u517c\u5bb9\u7684\u6295\u5f71\u4e00\u9636\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ece\u800c\u5e73\u8861\u7ec4\u4ef6\u95f4\u7684\u9002\u5e94\u80fd\u91cf\u5206\u5e03\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u540c\u65f6\u7f13\u89e3\u540e\u5411\u9057\u5fd8\u548c\u524d\u5411\u9057\u5fd8\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5e73\u8861\u4efb\u52a1\u66f4\u65b0\u4e2d\u7684\u7ec4\u4ef6\uff0cEBLoRA\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u4fdd\u7559\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u540e\u7eed\u4efb\u52a1\u7684\u5e72\u6270\uff0c\u5728\u53c2\u6570\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02034", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02034", "abs": "https://arxiv.org/abs/2602.02034", "authors": ["Ananya Joshi", "Michael Rudow"], "title": "Constrained Process Maps for Multi-Agent Generative AI Workflows", "comment": null, "summary": "Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6709\u9650\u65f6\u57df\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u6539\u8fdbLLM\u667a\u80fd\u4f53\u5728\u5408\u89c4\u7b49\u76d1\u7ba1\u73af\u5883\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\u548c\u534f\u8c03\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u67b6\u6784\u4e3b\u8981\u4f9d\u8d56\u5355\u4e2a\u667a\u80fd\u4f53\u7684\u63d0\u793a\u5de5\u7a0b\uff0c\u96be\u4ee5\u89c2\u5bdf\u548c\u6bd4\u8f83\u6a21\u578b\u5982\u4f55\u5904\u7406\u8de8\u51b3\u7b56\u9636\u6bb5\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u534f\u8c03\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u7684\u76d1\u7ba1\u73af\u5883\u4e2d\u3002", "method": "\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5f62\u5f0f\u5316\u4e3a\u5177\u6709\u6709\u5411\u65e0\u73af\u7ed3\u6784\u7684\u6709\u9650\u65f6\u57dfMDP\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u5bf9\u5e94\u7279\u5b9a\u89d2\u8272\u6216\u51b3\u7b56\u9636\u6bb5\uff08\u5982\u5408\u89c4\u6d41\u7a0b\u4e2d\u7684\u5185\u5bb9\u3001\u4e1a\u52a1\u6216\u6cd5\u5f8b\u5ba1\u67e5\uff09\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u91cf\u5316\u667a\u80fd\u4f53\u5c42\u9762\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u7cfb\u7edf\u7ea7\u4e0d\u786e\u5b9a\u6027\u901a\u8fc7MDP\u7ec8\u6b62\u4e8e\u81ea\u52a8\u6807\u8bb0\u72b6\u6001\u6216\u4eba\u5de5\u5ba1\u67e5\u72b6\u6001\u6765\u6355\u83b7\u3002", "result": "\u5728AI\u5b89\u5168\u8bc4\u4f30\uff08\u81ea\u6b8b\u68c0\u6d4b\uff09\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe19%\uff0c\u6240\u9700\u4eba\u5de5\u5ba1\u67e5\u51cf\u5c11\u9ad8\u8fbe85\u500d\uff0c\u67d0\u4e9b\u914d\u7f6e\u4e0b\u5904\u7406\u65f6\u95f4\u4e5f\u5f97\u5230\u51cf\u5c11\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53MDP\u6846\u67b6\u80fd\u591f\u6709\u6548\u7ba1\u7406LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u534f\u8c03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u9700\u6c42\u3002", "topic": "agent analysis"}}
{"id": "2602.00723", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00723", "abs": "https://arxiv.org/abs/2602.00723", "authors": ["Prakhar Ganesh", "Reza Shokri", "Golnoosh Farnadi"], "title": "Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity", "comment": "To appear at EACL 2026", "summary": "Large language models (LLMs) are known to \"hallucinate\" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\"\u63d0\u793a\u591a\u6837\u6027\"\u6846\u67b6\u6765\u91cf\u5316LLM\u8bc4\u4f30\u4e2d\u7684\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u5e7b\u89c9\u8bc4\u4f30\u8fc7\u5ea6\u5173\u6ce8\u6b63\u786e\u6027\u800c\u5ffd\u89c6\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u5bf9\u5e7b\u89c9\u5371\u5bb3\u7684\u4e25\u91cd\u8bef\u89e3\u3002", "motivation": "\u73b0\u6709LLM\u5e7b\u89c9\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6b63\u786e\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u8f93\u51fa\u4e00\u81f4\u6027\uff0c\u800c\u4e00\u81f4\u6027\u5bf9\u4e8e\u533a\u5206\u548c\u89e3\u51b3\u5e7b\u89c9\u9020\u6210\u7684\u5371\u5bb3\uff08\u5982\u4fe1\u4efb\u4fb5\u8680\u548c\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\"\u63d0\u793a\u591a\u6837\u6027\"\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316LLM\u5728\u4e0d\u540c\u63d0\u793a\u4e0b\u7684\u8f93\u51fa\u4e00\u81f4\u6027\u6765\u8bc4\u4f30\u5e7b\u89c9\u3002\u5206\u6790\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982Med-HALT\uff09\uff0c\u7814\u7a76\u4e00\u81f4\u6027\u5728\u5e7b\u89c9\u68c0\u6d4b\u548c\u7f13\u89e3\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u663e\u8457\u7684\u591a\u91cd\u6027\uff08\u8d85\u8fc750%\u7684\u4e0d\u4e00\u81f4\u6027\uff09\uff0c\u8868\u660e\u73b0\u6709\u5e7b\u89c9\u5371\u5bb3\u8bc4\u4f30\u5b58\u5728\u4e25\u91cd\u8bef\u89e3\u3002\u68c0\u6d4b\u6280\u672f\u4e3b\u8981\u68c0\u6d4b\u4e00\u81f4\u6027\u800c\u975e\u6b63\u786e\u6027\uff0cRAG\u7b49\u7f13\u89e3\u6280\u672f\u867d\u7136\u6709\u76ca\u4f46\u53ef\u80fd\u5f15\u5165\u989d\u5916\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u63d0\u793a\u591a\u6837\u6027\u6574\u5408\u5230\u5e7b\u89c9\u8bc4\u4f30\u4e2d\uff0c\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u5371\u5bb3\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u68c0\u6d4b\u548c\u7f13\u89e3\u7b56\u7565\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u4e00\u81f4\u6027\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.02039", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02039", "abs": "https://arxiv.org/abs/2602.02039", "authors": ["Wei Liu", "Peijie Yu", "Michele Orini", "Yali Du", "Yulan He"], "title": "Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models", "comment": "14 pages, 7 tables, 8 figures", "summary": "The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u8c03\u67e5\u6027\u667a\u80fd\"\u6982\u5ff5\uff0c\u533a\u522b\u4e8e\u6267\u884c\u6027\u667a\u80fd\uff0c\u5e76\u5f15\u5165Deep Data Research\u4efb\u52a1\u548cDDR-Bench\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u81ea\u4e3b\u63a2\u7d22\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u4f46\u771f\u6b63\u7684\u667a\u80fd\u4f53\u9700\u8981\u81ea\u4e3b\u8bbe\u5b9a\u76ee\u6807\u548c\u63a2\u7d22\u7684\"\u8c03\u67e5\u6027\u667a\u80fd\"\u3002\u6570\u636e\u79d1\u5b66\u662f\u7406\u60f3\u7684\u6d4b\u8bd5\u9886\u57df\uff0c\u56e0\u4e3a\u771f\u5b9e\u5206\u6790\u4ece\u539f\u59cb\u6570\u636e\u5f00\u59cb\u800c\u975e\u660e\u786e\u67e5\u8be2\uff0c\u4f46\u7f3a\u4e4f\u76f8\u5173\u57fa\u51c6\u3002", "method": "\u63d0\u51faDeep Data Research\u4efb\u52a1\uff0c\u8ba9LLM\u81ea\u4e3b\u4ece\u6570\u636e\u5e93\u4e2d\u63d0\u53d6\u5173\u952e\u6d1e\u5bdf\uff1b\u6784\u5efaDDR-Bench\u5927\u89c4\u6a21\u68c0\u67e5\u8868\u57fa\u51c6\uff0c\u652f\u6301\u53ef\u9a8c\u8bc1\u8bc4\u4f30\uff1b\u5206\u6790\u8c03\u67e5\u6027\u667a\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u663e\u793a\u51fa\u521d\u6b65\u7684\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u4f46\u957f\u671f\u89c6\u91ce\u7684\u63a2\u7d22\u4ecd\u7136\u56f0\u96be\uff1b\u6709\u6548\u7684\u8c03\u67e5\u6027\u667a\u80fd\u4e0d\u4ec5\u4f9d\u8d56\u667a\u80fd\u4f53\u6846\u67b6\u6216\u89c4\u6a21\u6269\u5c55\uff0c\u66f4\u53d6\u51b3\u4e8e\u667a\u80fd\u4f53\u6a21\u578b\u7684\u5185\u5728\u7b56\u7565\u3002", "conclusion": "\u8c03\u67e5\u6027\u667a\u80fd\u662fLLM\u667a\u80fd\u4f53\u7684\u5173\u952e\u80fd\u529b\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\uff1b\u81ea\u4e3b\u63a2\u7d22\u80fd\u529b\u7684\u53d1\u5c55\u9700\u8981\u5173\u6ce8\u6a21\u578b\u5185\u5728\u7b56\u7565\u800c\u4e0d\u4ec5\u4ec5\u662f\u5916\u90e8\u6846\u67b6\u6216\u89c4\u6a21\u6269\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2602.02051", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02051", "abs": "https://arxiv.org/abs/2602.02051", "authors": ["Shivank Garg", "Ayush Singh", "Gaurav Kumar Nayak"], "title": "SIDiffAgent: Self-Improving Diffusion Agent", "comment": null, "summary": "Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse\" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \\modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.", "AI": {"tldr": "SIDiffAgent\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u5229\u7528Qwen\u7cfb\u5217\u6a21\u578b\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u81ea\u4e3b\u63d0\u793a\u5de5\u7a0b\u3001\u9519\u8bef\u68c0\u6d4b\u4e0e\u4fee\u6b63\u3001\u4f2a\u5f71\u53bb\u9664\u4ee5\u53ca\u57fa\u4e8e\u8bb0\u5fc6\u7684\u8fed\u4ee3\u81ea\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5b58\u5728\u591a\u4e2a\u9650\u5236\uff1a\u5bf9\u63d0\u793a\u8bcd\u8868\u8ff0\u654f\u611f\u3001\u8bed\u4e49\u6b67\u4e49\uff08\u5982\"mouse\"\u6307\u52a8\u7269\u8fd8\u662f\u9f20\u6807\uff09\u3001\u89e3\u5256\u7ed3\u6784\u626d\u66f2\u7b49\u4f2a\u5f71\uff0c\u4ee5\u53ca\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8f93\u5165\u63d0\u793a\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u8bad\u7ec3\u4e14\u53ef\u63a7\u6027\u6709\u9650\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faSIDiffAgent\u8bad\u7ec3\u514d\u8d39\u4ee3\u7406\u6846\u67b6\uff0c\u5229\u7528Qwen\u7cfb\u5217\u6a21\u578b\uff08Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding\uff09\u81ea\u4e3b\u7ba1\u7406\u63d0\u793a\u5de5\u7a0b\u3001\u68c0\u6d4b\u5e76\u4fee\u6b63\u751f\u6210\u9519\u8bef\u3001\u6267\u884c\u7ec6\u7c92\u5ea6\u4f2a\u5f71\u53bb\u9664\u3002\u6846\u67b6\u5305\u542b\u8fed\u4ee3\u81ea\u6539\u8fdb\u673a\u5236\uff0c\u5c06\u5148\u524d\u7ecf\u9a8c\u5b58\u50a8\u5728\u6570\u636e\u5e93\u4e2d\uff0c\u5e76\u5728\u4ee3\u7406\u6d41\u7a0b\u5404\u9636\u6bb5\u6ce8\u5165\u57fa\u4e8e\u63d0\u793a\u7684\u6307\u5bfc\u3002", "result": "\u5728GenAIBench\u4e0a\u83b7\u5f97\u5e73\u5747VQA\u5f97\u52060.884\uff0c\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\u3001\u4e13\u6709\u6a21\u578b\u548c\u5176\u4ed6\u4ee3\u7406\u65b9\u6cd5\u3002", "conclusion": "SIDiffAgent\u901a\u8fc7\u8bad\u7ec3\u514d\u8d39\u7684\u4ee3\u7406\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u90e8\u7f72\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e3b\u63d0\u793a\u5de5\u7a0b\u3001\u9519\u8bef\u4fee\u6b63\u548c\u8fed\u4ee3\u81ea\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.01982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01982", "abs": "https://arxiv.org/abs/2602.01982", "authors": ["Yanrui Du", "Sendong Zhao", "Yibo Gao", "Danyang Zhao", "Qika Lin", "Ming Ma", "Jiayun Li", "Yi Jiang", "Kai He", "Qianyi Xu", "Bing Qin", "Mengling Feng"], "title": "S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs", "comment": null, "summary": "Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.", "AI": {"tldr": "\u63d0\u51faS3-CoT\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u91c7\u6837\u6fc0\u6d3b\u5f15\u5bfc\u5b9e\u73b0\u9ad8\u6548\u601d\u7ef4\u94fe\u5b66\u4e60\uff0c\u65e0\u9700\u6559\u5e08\u6307\u5bfc\u5373\u53ef\u751f\u6210\u98ce\u683c\u5bf9\u9f50\u3001\u53ef\u53d8\u957f\u5ea6\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u89e3\u51b3\u76d1\u7763\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLMs\u7684\u601d\u7ef4\u94fe\u80fd\u529b\u63d0\u5347\u5e38\u4f34\u968f\u5197\u4f59\u63a8\u7406\u8fc7\u7a0b\uff0c\u9700\u8981\u63a2\u7d22LLMs\u80fd\u5426\u83b7\u5f97\u7c7b\u4f3c\u4eba\u7c7b\u7cfb\u7edf1\u7684\u5feb\u901f\u601d\u7ef4\u6a21\u5f0f\uff0c\u540c\u65f6\u89e3\u51b3\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u4e2d\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\u7a00\u7f3a\u7684\u6838\u5fc3\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6fc0\u6d3b\u5f15\u5bfc\u7684\u81ea\u91c7\u6837\u6846\u67b6\uff0c\u4ece\u76ee\u6807LLMs\u81ea\u8eab\u8bf1\u5bfc\u98ce\u683c\u5bf9\u9f50\u3001\u53ef\u53d8\u957f\u5ea6\u7684\u63a8\u7406\u8f68\u8ff9\uff1b\u4f7f\u7528\u9ec4\u91d1\u7b54\u6848\u8fc7\u6ee4\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u6784\u5efa\u7c7b\u4eba\u53cc\u8ba4\u77e5\u7cfb\u7edf\u548c\u6e10\u8fdb\u538b\u7f29\u8bfe\u7a0b\uff1b\u63a2\u7d22\u4ec5\u4f7f\u7528\u9884\u6d4b\u4e00\u81f4\u6570\u636e\u7684\u81ea\u8fdb\u5316\u673a\u5236\u3002", "result": "\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u548c\u533b\u5b66\u9886\u57df\u7684\u8de8\u57df\u6cdb\u5316\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5bf9\u901a\u7528LLMs\u548cR1\u98ce\u683cLLMs\u5747\u4ea7\u751f\u7a33\u5b9a\u6539\u8fdb\u3002", "conclusion": "S3-CoT\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u601d\u7ef4\u94fe\u5b66\u4e60\uff0c\u65e0\u9700\u6559\u5e08\u6307\u5bfc\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\uff0c\u4e3aLLMs\u83b7\u5f97\u5feb\u901f\u601d\u7ef4\u6a21\u5f0f\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2602.01999", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01999", "abs": "https://arxiv.org/abs/2602.01999", "authors": ["Yanrui Du", "Yibo Gao", "Sendong Zhao", "Jiayun Li", "Haochun Wang", "Qika Lin", "Kai He", "Bing Qin", "Mengling Feng"], "title": "From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs", "comment": null, "summary": "R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790R1\u98ce\u683c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u6fc0\u6d3b\u8f68\u8ff9\uff0c\u63ed\u793a\u4e86\u81ea\u6211\u53cd\u601d\u884c\u4e3a\u7684\u5c42\u7ea7\u673a\u5236\uff1a\u4ece\u6f5c\u5728\u63a7\u5236\u5c42\uff08\u601d\u8003\u9884\u7b97\u7f16\u7801\uff09\u5230\u8bed\u4e49\u67a2\u7ebd\u5c42\uff08\u8bdd\u8bed\u7ea7\u7ebf\u7d22\uff09\uff0c\u518d\u5230\u884c\u4e3a\u5916\u663e\u5c42\uff08\u53cd\u601d\u884c\u4e3atoken\u6982\u7387\u4e0a\u5347\uff09\uff0c\u5f62\u6210\u56e0\u679c\u94fe\u5f0f\u7ed3\u6784\u3002", "motivation": "\u5c3d\u7ba1R1\u98ce\u683c\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u4f46\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u6790\u53cd\u601d\u884c\u4e3a\u8d77\u59cb\u9636\u6bb5\u7684\u5c42\u7ea7\u6fc0\u6d3b\u8f68\u8ff9\uff0c\u63ed\u793a\u8fd9\u79cd\u81ea\u6211\u53cd\u601d\u884c\u4e3a\u7684\u5185\u5728\u673a\u5236\u3002", "method": "\u4f7f\u7528logit lens\u6280\u672f\u8bfb\u53d6token\u7ea7\u8bed\u4e49\uff0c\u8ffd\u8e2a\u53cd\u601d\u884c\u4e3a\u7684\u5c42\u7ea7\u6fc0\u6d3b\u8f68\u8ff9\u3002\u901a\u8fc7\u9488\u5bf9\u6027\u5e72\u9884\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u5c42\u7ea7\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u4e86\u4e09\u4e2a\u7ed3\u6784\u5316\u9636\u6bb5\uff1a1\uff09\u6f5c\u5728\u63a7\u5236\u5c42\uff1a\u7ebf\u6027\u65b9\u5411\u7f16\u7801\u601d\u8003\u9884\u7b97\u8bed\u4e49\uff1b2\uff09\u8bed\u4e49\u67a2\u7ebd\u5c42\uff1a\u8f6c\u6298\u70b9\u548c\u603b\u7ed3\u6027\u7ebf\u7d22\u7b49\u8bdd\u8bed\u7ea7\u7ebf\u7d22\u51fa\u73b0\u5e76\u4e3b\u5bfc\u6982\u7387\u8d28\u91cf\uff1b3\uff09\u884c\u4e3a\u5916\u663e\u5c42\uff1a\u53cd\u601d\u884c\u4e3atoken\u7684\u91c7\u6837\u53ef\u80fd\u6027\u4e0a\u5347\u3002\u5e72\u9884\u5b9e\u9a8c\u63ed\u793a\u4e86\u8de8\u9636\u6bb5\u7684\u56e0\u679c\u94fe\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u53cd\u601d\u8fc7\u7a0b\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u5143\u8ba4\u77e5\u8fc7\u7a0b\uff1a\u4ece\u6f5c\u5728\u76d1\u63a7\uff0c\u5230\u8bdd\u8bed\u7ea7\u8c03\u8282\uff0c\u6700\u7ec8\u5230\u5916\u663e\u7684\u81ea\u6211\u53cd\u601d\u3002\u8fd9\u4e3a\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2602.02136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02136", "abs": "https://arxiv.org/abs/2602.02136", "authors": ["Yingsha Xie", "Tiansheng Huang", "Enneng Yang", "Rui Min", "Wenjie Lu", "Xiaochun Cao", "Naiqiang Tan", "Li Shen"], "title": "Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models", "comment": "Code will be released soon", "summary": "Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \\textbf{+30.2\\%} on DirectRefusal and \\textbf{+21.2\\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \\textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.", "AI": {"tldr": "\u63d0\u51faDGR\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5916\u90e8\u5b89\u5168\u63a8\u7406\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u76ee\u6807\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u5206\u5e03\uff0c\u51cf\u5c11\u5b89\u5168\u5bf9\u9f50\u5e26\u6765\u7684\u63a8\u7406\u80fd\u529b\u4e0b\u964d\uff08\u5b89\u5168\u7a0e\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u6570\u636e\u96c6\u901a\u5e38\u4ece\u5916\u90e8\u5927\u8bed\u8a00\u6a21\u578b\u6216\u4eba\u5de5\u6807\u6ce8\u4e2d\u84b8\u998f\u5b89\u5168\u63a8\u7406\u8f68\u8ff9\u548c\u7b54\u6848\uff0c\u4f46\u8fd9\u4e9b\u4e0e\u76ee\u6807\u6a21\u578b\u5b58\u5728\u5206\u5e03\u5dee\u5f02\uff0c\u5bfc\u81f4\u76ee\u6807\u6a21\u578b\u63a8\u7406\u80fd\u529b\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51faDGR\u65b9\u6cd5\uff0c\u5c06\u73b0\u6709\u5206\u5e03\u5916\u5b89\u5168\u63a8\u7406\u6570\u636e\u96c6\u8f6c\u6362\u548c\u7cbe\u70bc\uff0c\u4f7f\u5176\u4e0e\u76ee\u6807\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u5206\u5e03\u5bf9\u9f50\u3002", "result": "DGR\u6709\u6548\u51cf\u8f7b\u5b89\u5168\u7a0e\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u80fd\uff1a\u76f8\u6bd4Vanilla SFT\uff0c\u5728DirectRefusal\u4e0a\u5e73\u5747\u63a8\u7406\u51c6\u786e\u7387\u63d0\u534730.2%\uff0c\u5728R1-ACT\u4e0a\u63d0\u534721.2%\uff1b\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u7a0b\u5ea6\u4e0e\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u76f8\u5173\uff1b\u4ec5\u970010\u4e2a\u6837\u672c\u5373\u53ef\u6fc0\u6d3b\u6709\u6548\u7684\u62d2\u7edd\u884c\u4e3a\u3002", "conclusion": "\u5206\u5e03\u4e00\u81f4\u6027\u5bf9\u4fdd\u6301\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u5b89\u5168\u5bf9\u9f50\u53ef\u80fd\u4e3b\u8981\u4f5c\u4e3a\u6fc0\u6d3b\u6f5c\u5728\u77e5\u8bc6\u7684\u673a\u5236\uff0c\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5b89\u5168\u5728\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u6fc0\u6d3b\u673a\u5236\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2602.02007", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02007", "abs": "https://arxiv.org/abs/2602.02007", "authors": ["Zhanghao Hu", "Qinglin Zhu", "Hanqi Yan", "Yulan He", "Lin Gui"], "title": "Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation", "comment": null, "summary": "Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.", "AI": {"tldr": "xMemory\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u91c7\u7528\u89e3\u8026\u5230\u805a\u5408\u7684\u65b9\u6cd5\uff1a\u5c06\u8bb0\u5fc6\u5206\u89e3\u4e3a\u8bed\u4e49\u7ec4\u4ef6\uff0c\u7ec4\u7ec7\u6210\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u57fa\u4e8e\u6b64\u7ed3\u6784\u8fdb\u884c\u68c0\u7d22\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfRAG\u5728\u667a\u80fd\u4f53\u8bb0\u5fc6\u573a\u666f\u4e2d\u7684\u5197\u4f59\u548c\u5173\u8054\u7f3a\u5931\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRAG\u65b9\u6cd5\u5047\u8bbe\u5927\u89c4\u6a21\u5f02\u8d28\u8bed\u6599\u5e93\uff0c\u800c\u667a\u80fd\u4f53\u8bb0\u5fc6\u662f\u6709\u9650\u3001\u8fde\u8d2f\u7684\u5bf9\u8bdd\u6d41\uff0c\u5176\u4e2d\u8bb0\u5fc6\u7247\u6bb5\u9ad8\u5ea6\u76f8\u5173\u4e14\u5e38\u91cd\u590d\u3002\u56fa\u5b9atop-k\u76f8\u4f3c\u6027\u68c0\u7d22\u4f1a\u8fd4\u56de\u5197\u4f59\u4e0a\u4e0b\u6587\uff0c\u800c\u4e8b\u540e\u4fee\u526a\u53ef\u80fd\u5220\u9664\u65f6\u95f4\u5173\u8054\u7684\u5148\u51b3\u6761\u4ef6\uff0c\u5f71\u54cd\u63a8\u7406\u6b63\u786e\u6027\u3002", "method": "xMemory\u901a\u8fc7\u7a00\u758f\u6027-\u8bed\u4e49\u76ee\u6807\u6784\u5efa\u5b8c\u6574\u5355\u5143\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u7ef4\u62a4\u53ef\u641c\u7d22\u4e14\u5fe0\u5b9e\u7684\u9ad8\u5c42\u8282\u70b9\u7ec4\u7ec7\u3002\u91c7\u7528\u89e3\u8026\u5230\u805a\u5408\u65b9\u6cd5\uff1a\u5c06\u8bb0\u5fc6\u5206\u89e3\u4e3a\u8bed\u4e49\u7ec4\u4ef6\uff0c\u7ec4\u7ec7\u6210\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u57fa\u4e8e\u6b64\u7ed3\u6784\u9a71\u52a8\u68c0\u7d22\u3002\u63a8\u7406\u65f6\u81ea\u4e0a\u800c\u4e0b\u68c0\u7d22\uff0c\u9009\u62e9\u7d27\u51d1\u591a\u6837\u7684\u4e3b\u9898\u548c\u8bed\u4e49\u7ec4\u4ef6\uff0c\u4ec5\u5728\u51cf\u5c11\u8bfb\u8005\u4e0d\u786e\u5b9a\u6027\u65f6\u6269\u5c55\u5230\u60c5\u8282\u548c\u539f\u59cb\u6d88\u606f\u3002", "result": "\u5728LoCoMo\u548cPerLTQA\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u4e09\u79cd\u6700\u65b0LLM\u8fdb\u884c\u5b9e\u9a8c\uff0cxMemory\u5728\u56de\u7b54\u8d28\u91cf\u548ctoken\u6548\u7387\u65b9\u9762\u5747\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "\u68c0\u7d22\u5e94\u8d85\u8d8a\u76f8\u4f3c\u6027\u5339\u914d\uff0c\u8f6c\u5411\u57fa\u4e8e\u6f5c\u5728\u7ec4\u4ef6\u7684\u64cd\u4f5c\u3002xMemory\u901a\u8fc7\u5c42\u6b21\u5316\u8bb0\u5fc6\u7ec4\u7ec7\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u8bb0\u5fc6\u573a\u666f\u4e2d\u7684\u5197\u4f59\u548c\u5173\u8054\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u68c0\u7d22\u3002", "topic": "agent analysis"}}
{"id": "2602.00781", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.00781", "abs": "https://arxiv.org/abs/2602.00781", "authors": ["Jiamin Xu", "Kyra Gan"], "title": "Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding", "comment": null, "summary": "Online reinforcement learning in non-episodic, finite-horizon MDPs remains underexplored and is challenged by the need to estimate returns to a fixed terminal time. Existing infinite-horizon methods, which often rely on discounted contraction, do not naturally account for this fixed-horizon structure. We introduce a modified Q-function: rather than targeting the full-horizon, we learn a K-step lookahead Q-function that truncates planning to the next K steps. To further improve sample efficiency, we introduce a thresholding mechanism: actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold. We provide an efficient tabular learning algorithm for this novel objective, proving it achieves fast finite-sample convergence: it achieves minimax optimal constant regret for $K=1$ and $\\mathcal{O}(\\max((K-1),C_{K-1})\\sqrt{SAT\\log(T)})$ regret for any $K \\geq 2$. We numerically evaluate the performance of our algorithm under the objective of maximizing reward. Our implementation adaptively increases K over time, balancing lookahead depth against estimation variance. Empirical results demonstrate superior cumulative rewards over state-of-the-art tabular RL methods across synthetic MDPs and RL environments: JumpRiverswim, FrozenLake and AnyTrading.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6709\u9650\u65f6\u57df\u975e\u7247\u6bb5\u5f0f\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528K\u6b65\u524d\u77bbQ\u51fd\u6570\u548c\u9608\u503c\u673a\u5236\uff0c\u5728\u8868\u683c\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u5e76\u83b7\u5f97\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u65e0\u9650\u65f6\u57df\u65b9\u6cd5\u4f9d\u8d56\u6298\u6263\u6536\u7f29\uff0c\u4e0d\u9002\u7528\u4e8e\u56fa\u5b9a\u65f6\u57df\u7684\u975e\u7247\u6bb5\u5f0fMDP\u3002\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u7136\u5904\u7406\u56fa\u5b9a\u65f6\u57df\u7ed3\u6784\u4e14\u6837\u672c\u9ad8\u6548\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u5f15\u5165K\u6b65\u524d\u77bbQ\u51fd\u6570\uff0c\u5c06\u89c4\u5212\u622a\u65ad\u5230\u672a\u6765K\u6b65\uff1b\u91c7\u7528\u9608\u503c\u673a\u5236\uff0c\u4ec5\u5f53\u4f30\u8ba1\u7684K\u6b65\u524d\u77bb\u503c\u8d85\u8fc7\u65f6\u53d8\u9608\u503c\u65f6\u624d\u9009\u62e9\u52a8\u4f5c\uff1b\u63d0\u4f9b\u9ad8\u6548\u7684\u8868\u683c\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1aK=1\u65f6\u8fbe\u5230\u6781\u5c0f\u6781\u5927\u6700\u4f18\u5e38\u6570\u9057\u61be\uff0cK\u22652\u65f6\u9057\u61be\u4e3aO(max((K-1),C_{K-1})\u221a(SATlogT))\uff1b\u5b9e\u9a8c\u5728JumpRiverswim\u3001FrozenLake\u548cAnyTrading\u7b49\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u8868\u683cRL\u65b9\u6cd5\u3002", "conclusion": "K\u6b65\u524d\u77bbQ\u51fd\u6570\u548c\u9608\u503c\u673a\u5236\u80fd\u6709\u6548\u5904\u7406\u56fa\u5b9a\u65f6\u57df\u975e\u7247\u6bb5\u5f0fMDP\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6709\u9650\u65f6\u57df\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02196", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02196", "abs": "https://arxiv.org/abs/2602.02196", "authors": ["Hang Yan", "Xinyu Che", "Fangzhi Xu", "Qiushi Sun", "Zichen Ding", "Kanzhi Cheng", "Jian Zhang", "Tao Qin", "Jun Liu", "Qika Lin"], "title": "TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents", "comment": "29pages, 10 figures", "summary": "Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.", "AI": {"tldr": "\u63d0\u51fa\u4e86TIDE\u6846\u67b6\uff0c\u7528\u4e8e\u8bca\u65ad\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u6d4b\u8bd5\u65f6\u6539\u8fdb\uff08TTI\uff09\u8fc7\u7a0b\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u901a\u8fc7\u4e09\u4e2a\u7ef4\u5ea6\u5206\u6790\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u3001\u5faa\u73af\u884c\u4e3a\u548c\u5185\u5b58\u8d1f\u62c5\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3bLLM\u667a\u80fd\u4f53\u901a\u8fc7\u4e0e\u73af\u5883\u8fed\u4ee3\u4ea4\u4e92\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff08TTI\uff09\uff0c\u4f46\u5176\u6210\u529f\u6216\u5931\u8d25\u7684\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u6355\u6349\u4efb\u52a1\u4f18\u5316\u6548\u7387\u3001\u9519\u8bef\u884c\u4e3a\u9002\u5e94\u4ee5\u53ca\u5de5\u4f5c\u5185\u5b58\u7684\u5177\u4f53\u6548\u7528\u3002", "method": "\u63d0\u51faTIDE\uff08Test-time Improvement Diagnostic Evaluation\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u667a\u80fd\u4f53\u65e0\u5173\u548c\u73af\u5883\u65e0\u5173\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06TTI\u5206\u89e3\u4e3a\u4e09\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u7ef4\u5ea6\uff1a\u4efb\u52a1\u5b8c\u6210\u7684\u6574\u4f53\u65f6\u95f4\u52a8\u6001\u3001\u9012\u5f52\u5faa\u73af\u884c\u4e3a\u7684\u7ea6\u675f\u7a0b\u5ea6\u3001\u4ee5\u53ca\u7d2f\u79ef\u5185\u5b58\u8d1f\u62c5\u7684\u9650\u5236\u3002", "result": "\u901a\u8fc7\u5728\u4e0d\u540c\u667a\u80fd\u4f53\u548c\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0cTIDE\u63ed\u793a\u51fa\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u4e0d\u4ec5\u9700\u8981\u6269\u5c55\u5185\u90e8\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u9700\u8981\u663e\u5f0f\u4f18\u5316\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u4ea4\u4e92\u52a8\u6001\u3002", "conclusion": "TIDE\u6846\u67b6\u4e3a\u7406\u89e3TTI\u673a\u5236\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u5f3a\u8c03\u4e86\u4f18\u5316\u667a\u80fd\u4f53-\u73af\u5883\u4ea4\u4e92\u52a8\u6001\u7684\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u589e\u5f3a\u5185\u90e8\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.02099", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02099", "abs": "https://arxiv.org/abs/2602.02099", "authors": ["Keqin Peng", "Yuanxin Ouyang", "Xuebo Liu", "Zhiliang Tian", "Ruijian Han", "Yancheng Yuan", "Liang Ding"], "title": "Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.", "AI": {"tldr": "\u63d0\u51faDDCA\u65b9\u6cd5\u89e3\u51b3RLVR\u4e2d\u957f\u5ea6\u60e9\u7f5a\u5bfc\u81f4\u51c6\u786e\u7387\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u6548\u7387\u548c\u6b63\u786e\u6027\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u51cf\u5c11\u751f\u6210token\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "RLVR\u80fd\u6fc0\u53d1\u591a\u6b65\u63a8\u7406\u4f46\u5e38\u5bfc\u81f4\u5197\u957f\u8f93\u51fa\uff0c\u800c\u7b80\u5355\u7684\u957f\u5ea6\u60e9\u7f5a\u5728\u7fa4\u4f53\u76f8\u5bf9\u4f18\u5316\u4e2d\u4f1a\u4e25\u91cd\u635f\u5bb3\u51c6\u786e\u7387\u3002\u8fd9\u6e90\u4e8e\u4e24\u4e2a\u7ed3\u6784\u6027\u95ee\u9898\uff1a\u957f\u5ea6\u57fa\u7ebf\u7a00\u91ca\uff08\u9519\u8bef\u54cd\u5e94\u964d\u4f4e\u7fa4\u4f53\u57fa\u7ebf\uff0c\u8fc7\u5ea6\u60e9\u7f5a\u6b63\u786e\u89e3\uff09\u548c\u96be\u5ea6\u60e9\u7f5a\u4e0d\u5339\u914d\uff08\u9759\u6001\u60e9\u7f5a\u65e0\u6cd5\u9002\u5e94\u95ee\u9898\u96be\u5ea6\uff09\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u89e3\u8026\u6761\u4ef6\u4f18\u52bf\uff08DDCA\uff09\uff1a1\uff09\u5728\u6b63\u786e\u54cd\u5e94\u7c07\u5185\u6761\u4ef6\u8ba1\u7b97\u957f\u5ea6\u4f18\u52bf\uff0c\u6d88\u9664\u57fa\u7ebf\u7a00\u91ca\uff1b2\uff09\u4f7f\u7528\u7fa4\u4f53\u901a\u8fc7\u7387\u4f5c\u4e3a\u96be\u5ea6\u4ee3\u7406\uff0c\u52a8\u6001\u8c03\u6574\u60e9\u7f5a\u5f3a\u5ea6\uff0c\u4f7f\u60e9\u7f5a\u9002\u5e94\u95ee\u9898\u96be\u5ea6\u3002", "result": "\u5728GSM8K\u3001MATH500\u3001AMC23\u548cAIME25\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDDCA\u76f8\u6bd4\u81ea\u9002\u5e94\u57fa\u7ebf\u6301\u7eed\u6539\u5584\u6548\u7387-\u51c6\u786e\u7387\u6743\u8861\uff1a\u7b80\u5355\u4efb\u52a1\uff08\u5982GSM8K\uff09\u51cf\u5c11\u7ea660%\u751f\u6210token\uff0c\u56f0\u96be\u57fa\u51c6\uff08\u5982AIME25\uff09\u51cf\u5c11\u8d85\u8fc720%\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "DDCA\u6709\u6548\u89e3\u51b3\u4e86RLVR\u4e2d\u957f\u5ea6\u60e9\u7f5a\u7684\u56fa\u6709\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u6548\u7387\u548c\u6b63\u786e\u6027\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6548\u7387-\u51c6\u786e\u7387\u5e73\u8861\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02350", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.02350", "abs": "https://arxiv.org/abs/2602.02350", "authors": ["Xingyuan Hua", "Sheng Yue", "Xinyi Li", "Yizhe Zhao", "Jinrui Zhang", "Ju Ren"], "title": "Context Learning for Multi-Agent Discussion", "comment": null, "summary": "Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.", "AI": {"tldr": "M2CL\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e0a\u4e0b\u6587\u751f\u6210\u5668\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u52a8\u6001\u751f\u6210\u4e0a\u4e0b\u6587\u6307\u4ee4\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd20%-50%\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u65b9\u6cd5\u5bb9\u6613\u906d\u53d7\u8ba8\u8bba\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u7531\u4e8e\u667a\u80fd\u4f53\u4e2a\u4f53\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\uff0c\u5bfc\u81f4LLM\u65e0\u6cd5\u8fbe\u6210\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faM2CL\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u8bad\u7ec3\u4e00\u4e2a\u4e0a\u4e0b\u6587\u751f\u6210\u5668\uff0c\u901a\u8fc7\u81ea\u52a8\u4fe1\u606f\u7ec4\u7ec7\u548c\u7cbe\u70bc\uff0c\u5728\u6bcf\u8f6e\u8ba8\u8bba\u4e2d\u52a8\u6001\u751f\u6210\u4e0a\u4e0b\u6587\u6307\u4ee4\u3002\u57fa\u4e8e\u5bf9\u4e0a\u4e0b\u6587\u6307\u4ee4\u7684\u7406\u8bba\u6d1e\u5bdf\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u81ea\u9002\u5e94\u673a\u5236\u63a7\u5236\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u548c\u8f93\u51fa\u5dee\u5f02\u3002", "result": "\u5728\u5b66\u672f\u63a8\u7406\u3001\u5177\u8eab\u4efb\u52a1\u548c\u79fb\u52a8\u63a7\u5236\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e0a\uff0cM2CL\u6027\u80fd\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd520%-50%\uff0c\u540c\u65f6\u5177\u6709\u826f\u597d\u7684\u53ef\u8fc1\u79fb\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "M2CL\u901a\u8fc7\u4e0a\u4e0b\u6587\u751f\u6210\u5668\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4f7fLLM\u80fd\u591f\u907f\u514d\u8fc7\u65e9\u6536\u655b\u4e8e\u591a\u6570\u566a\u58f0\uff0c\u9010\u6b65\u8fbe\u6210\u6b63\u786e\u5171\u8bc6\u3002", "topic": "agent analysis"}}
{"id": "2602.02369", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02369", "abs": "https://arxiv.org/abs/2602.02369", "authors": ["Yaolun Zhang", "Yiran Wu", "Yijiong Yu", "Qingyun Wu", "Huazheng Wang"], "title": "Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback", "comment": "13 pages", "summary": "Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \\emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \\textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \\textsc{Live-Evo} decouples \\emph{what happened} from \\emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \\textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \\textit{Prophet Arena} benchmark over a 10-week horizon, \\textsc{Live-Evo} improves Brier score by 20.8\\% and increases market returns by 12.9\\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.", "AI": {"tldr": "Live-Evo\u662f\u4e00\u4e2a\u5728\u7ebf\u81ea\u8fdb\u5316\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ecf\u9a8c\u94f6\u884c\u548c\u5143\u6307\u5bfc\u94f6\u884c\u5206\u79bb\"\u53d1\u751f\u4e86\u4ec0\u4e48\"\u548c\"\u5982\u4f55\u4f7f\u7528\"\uff0c\u5728\u6301\u7eed\u6570\u636e\u6d41\u4e2d\u52a8\u6001\u66f4\u65b0\u8bb0\u5fc6\u6743\u91cd\uff0c\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u8bb0\u5fc6\u7684\u5f3a\u5316\u4e0e\u8870\u51cf\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u81ea\u8fdb\u5316\u7cfb\u7edf\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\u5f00\u53d1\uff0c\u901a\u8fc7\u6298\u53e0\u9759\u6001\u57fa\u51c6\u6765\u8fd1\u4f3c\u5728\u7ebf\u5b66\u4e60\uff0c\u5728\u771f\u5b9e\u5206\u5e03\u504f\u79fb\u548c\u6301\u7eed\u53cd\u9988\u4e0b\u8868\u73b0\u8106\u5f31\u3002\u9700\u8981\u771f\u6b63\u7684\u5728\u7ebf\u81ea\u8fdb\u5316\u8bb0\u5fc6\u7cfb\u7edf\u6765\u5904\u7406\u968f\u65f6\u95f4\u53d8\u5316\u7684\u6570\u636e\u6d41\u3002", "method": "Live-Evo\u91c7\u7528\u53cc\u94f6\u884c\u67b6\u6784\uff1a\u7ecf\u9a8c\u94f6\u884c\u5b58\u50a8\u539f\u59cb\u7ecf\u9a8c\uff0c\u5143\u6307\u5bfc\u94f6\u884c\u5b58\u50a8\u5982\u4f55\u4f7f\u7528\u7ecf\u9a8c\u7684\u6307\u5bfc\u3002\u7cfb\u7edf\u7ef4\u62a4\u7ecf\u9a8c\u6743\u91cd\uff0c\u6839\u636e\u53cd\u9988\u52a8\u6001\u66f4\u65b0\uff1a\u6301\u7eed\u6709\u5e2e\u52a9\u7684\u7ecf\u9a8c\u88ab\u5f3a\u5316\u5e76\u66f4\u9891\u7e41\u68c0\u7d22\uff0c\u8bef\u5bfc\u6027\u6216\u8fc7\u65f6\u7684\u7ecf\u9a8c\u88ab\u964d\u6743\u5e76\u9010\u6e10\u9057\u5fd8\u3002", "result": "\u572810\u5468\u65f6\u95f4\u8de8\u5ea6\u7684Prophet Arena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLive-Evo\u5c06Brier\u5206\u6570\u63d0\u9ad8\u4e8620.8%\uff0c\u5e02\u573a\u56de\u62a5\u589e\u52a0\u4e8612.9%\u3002\u5728\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u51fa\u5bf9\u5f3a\u57fa\u7ebf\u7684\u6301\u7eed\u4f18\u52bf\u3002", "conclusion": "Live-Evo\u5c55\u793a\u4e86\u5728\u7ebf\u81ea\u8fdb\u5316\u8bb0\u5fc6\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8bb0\u5fc6\u7684\u5f3a\u5316\u4e0e\u8870\u51cf\u673a\u5236\uff0c\u80fd\u591f\u9002\u5e94\u5206\u5e03\u504f\u79fb\u548c\u6301\u7eed\u53cd\u9988\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "topic": "agent analysis"}}
{"id": "2602.02386", "categories": ["cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02386", "abs": "https://arxiv.org/abs/2602.02386", "authors": ["Mika Okamoto", "Ansel Kaplan Erol", "Glenn Matlin"], "title": "Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing", "comment": "Appeared at MLSys YPS 2025", "summary": "How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.", "AI": {"tldr": "BELLA\u6846\u67b6\u901a\u8fc7\u6280\u80fd\u5206\u6790\u81ea\u52a8\u63a8\u8350\u6700\u4f18LLM\u9009\u62e9\uff0c\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u6027\u80fd\uff0c\u63d0\u4f9b\u900f\u660e\u89e3\u91ca", "motivation": "LLM\u5b9e\u8df5\u8005\u9700\u8981\u5728\u4e0d\u6d6a\u8d39\u8d44\u91d1\u7684\u60c5\u51b5\u4e0b\u4e3a\u4efb\u52a1\u9009\u62e9\u5408\u9002\u6a21\u578b\uff0c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u7684\u805a\u5408\u6307\u6807\u65e0\u6cd5\u63ed\u793a\u4efb\u52a1\u5177\u4f53\u9700\u8981\u54ea\u4e9b\u80fd\u529b\u4ee5\u53ca\u66f4\u4fbf\u5b9c\u6a21\u578b\u662f\u5426\u8db3\u591f", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u901a\u8fc7\u6279\u8bc4\u8005\u5206\u6790\u5206\u89e3LLM\u8f93\u51fa\u5e76\u63d0\u53d6\u7ec6\u7c92\u5ea6\u6280\u80fd\uff1b2) \u5c06\u6280\u80fd\u805a\u7c7b\u4e3a\u7ed3\u6784\u5316\u80fd\u529b\u77e9\u9635\uff1b3) \u591a\u76ee\u6807\u4f18\u5316\u9009\u62e9\u6a21\u578b\u4ee5\u6700\u5927\u5316\u6027\u80fd\u540c\u65f6\u5c0a\u91cd\u9884\u7b97\u7ea6\u675f", "result": "BELLA\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7684\u63a8\u8350\uff0c\u63d0\u4f9b\u5f53\u524d\u9ed1\u76d2\u8def\u7531\u7cfb\u7edf\u7f3a\u4e4f\u7684\u900f\u660e\u5ea6\uff0c\u5e76\u5e94\u7528\u4e8e\u91d1\u878d\u63a8\u7406\u7b49\u9700\u8981\u591a\u6837\u5316\u6280\u80fd\u548c\u6210\u672c\u53d8\u5316\u7684\u9886\u57df", "conclusion": "\u8be5\u6846\u67b6\u4f7f\u5b9e\u8df5\u8005\u80fd\u591f\u5728\u90e8\u7f72LLM\u65f6\u505a\u51fa\u539f\u5219\u6027\u7684\u6210\u672c-\u6027\u80fd\u6743\u8861", "topic": "agent analysis"}}
{"id": "2602.02160", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02160", "abs": "https://arxiv.org/abs/2602.02160", "authors": ["Bowen Xu", "Shaoyu Wu", "Hao Jiang", "Kai Liu", "Xin Chen", "Lulu Hu", "Bin Yang"], "title": "D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use", "comment": null, "summary": "Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\\underline{\\textbf{D}}ecomposing tasks and \\underline{\\textbf{Co}}mposing \\underline{\\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\\% accuracy, surpassing the best-performing 8B model by 5.7\\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\\%, outperforming 70B models despite being 5$\\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.", "AI": {"tldr": "D-CORE\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u63a8\u7406\u80fd\u529b\u6fc0\u52b1\u548c\u591a\u6837\u6027\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u61d2\u60f0\u63a8\u7406\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5de5\u5177\u4f7f\u7528\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u7f3a\u4e4f\u5b50\u4efb\u52a1\u5206\u89e3\u80fd\u529b\uff0c\u5bfc\u81f4\"\u61d2\u60f0\u63a8\u7406\"\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u89e3\u51b3\u590d\u6742\u5b9e\u9645\u95ee\u9898\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faD-CORE\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u81ea\u84b8\u998f\u6fc0\u52b1\u6a21\u578b\u7684\u4efb\u52a1\u5206\u89e3\u63a8\u7406\u80fd\u529b\uff1b2\uff09\u91c7\u7528\u591a\u6837\u6027\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u6062\u590d\u6a21\u578b\u7684\u53cd\u601d\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728BFCLv3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cD-CORE-8B\u8fbe\u523077.7%\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u6700\u4f738B\u6a21\u578b5.7%\uff1bD-CORE-14B\u4ee579.3%\u51c6\u786e\u7387\u521b\u4e0b\u65b0SOTA\uff0c\u6027\u80fd\u8d85\u8d8a70B\u6a21\u578b\u4f46\u53c2\u6570\u91cf\u4ec5\u4e3a\u51761/5\u3002", "conclusion": "D-CORE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u61d2\u60f0\u63a8\u7406\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u90fd\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2602.02468", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02468", "abs": "https://arxiv.org/abs/2602.02468", "authors": ["Aiden Yiliu Li", "Xinyue Hao", "Shilong Liu", "Mengdi Wang"], "title": "Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts", "comment": null, "summary": "Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.", "AI": {"tldr": "Avenir-Web\u662f\u4e00\u4e2a\u65b0\u578b\u7f51\u9875\u4ee3\u7406\uff0c\u901a\u8fc7\u6df7\u5408\u5b9a\u4f4d\u4e13\u5bb6\u3001\u7ecf\u9a8c\u6a21\u4eff\u89c4\u5212\u548c\u4efb\u52a1\u8ffd\u8e2a\u68c0\u67e5\u8868\u7b49\u6280\u672f\uff0c\u5728\u771f\u5b9e\u7f51\u9875\u4ea4\u4e92\u4e2d\u5b9e\u73b0\u4e86\u5f00\u6e90SOTA\u6027\u80fd\uff0c\u4e0e\u9876\u7ea7\u4e13\u6709\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u81ea\u4e3b\u7f51\u9875\u4ee3\u7406\u5728\u6267\u884c\u590d\u6742\u52a8\u6001\u7f51\u9875\u754c\u9762\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u65f6\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\uff1a\u5143\u7d20\u5b9a\u4f4d\u4e0d\u51c6\u786e\u3001\u7f3a\u4e4f\u7ad9\u70b9\u7279\u5b9a\u7a0b\u5e8f\u77e5\u8bc6\u3001\u4ee5\u53ca\u957f\u65f6\u4efb\u52a1\u8ffd\u8e2a\u548c\u8bb0\u5fc6\u4e0d\u7a33\u5b9a\u3002", "method": "Avenir-Web\u91c7\u7528\u4e09\u79cd\u6838\u5fc3\u6280\u672f\uff1a1) \u6df7\u5408\u5b9a\u4f4d\u4e13\u5bb6(Mixture of Grounding Experts)\u7528\u4e8e\u7cbe\u786e\u5143\u7d20\u5b9a\u4f4d\uff1b2) \u7ecf\u9a8c\u6a21\u4eff\u89c4\u5212(Experience-Imitation Planning)\u7528\u4e8e\u6574\u5408\u7a0b\u5e8f\u5148\u9a8c\u77e5\u8bc6\uff1b3) \u4efb\u52a1\u8ffd\u8e2a\u68c0\u67e5\u8868\u4e0e\u81ea\u9002\u5e94\u5185\u5b58\u7ed3\u5408\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u7528\u6237\u754c\u9762\u8303\u5f0f\u7684\u7a33\u5065\u4ea4\u4e92\u3002", "result": "\u5728Online-Mind2Web\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAvenir-Web\u663e\u8457\u8d85\u8d8a\u5148\u524d\u5f00\u6e90\u4ee3\u7406\uff0c\u8fbe\u5230\u4e0e\u9876\u7ea7\u4e13\u6709\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u5efa\u7acb\u4e86\u5f00\u6e90\u7f51\u9875\u4ee3\u7406\u5728\u771f\u5b9e\u7f51\u7ad9\u4e0a\u7684\u65b0SOTA\u3002", "conclusion": "Avenir-Web\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u5b9a\u4f4d\u3001\u7a0b\u5e8f\u77e5\u8bc6\u6574\u5408\u548c\u81ea\u9002\u5e94\u4efb\u52a1\u8ffd\u8e2a\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7f51\u9875\u4ee3\u7406\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u53ef\u9760\u7f51\u9875\u4ee3\u7406\u5728\u771f\u5b9e\u7f51\u7ad9\u4e0a\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2602.02475", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02475", "abs": "https://arxiv.org/abs/2602.02475", "authors": ["Shraddha Barke", "Arnav Goyal", "Alind Khare", "Avaljot Singh", "Suman Nath", "Chetan Bansal"], "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories", "comment": null, "summary": "AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AGENTRX\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u8bca\u65adAI\u4ee3\u7406\u5931\u8d25\u8f68\u8ff9\u4e2d\u7684\u5173\u952e\u5931\u8d25\u6b65\u9aa4\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b115\u4e2a\u5931\u8d25\u8f68\u8ff9\u7684\u57fa\u51c6\u6570\u636e\u96c6", "motivation": "AI\u4ee3\u7406\u7684\u5931\u8d25\u96be\u4ee5\u5b9a\u4f4d\uff0c\u56e0\u4e3a\u6267\u884c\u8fc7\u7a0b\u5177\u6709\u6982\u7387\u6027\u3001\u957f\u65f6\u7a0b\u3001\u591a\u4ee3\u7406\u4e14\u53d7\u566a\u58f0\u5de5\u5177\u8f93\u51fa\u5f71\u54cd\uff0c\u9700\u8981\u81ea\u52a8\u5316\u8bca\u65ad\u5de5\u5177\u6765\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c", "method": "\u63d0\u51faAGENTRX\u6846\u67b6\uff1a\u5408\u6210\u7ea6\u675f\u6761\u4ef6\uff0c\u9010\u6b65\u8bc4\u4f30\u8f68\u8ff9\uff0c\u751f\u6210\u53ef\u5ba1\u8ba1\u7684\u7ea6\u675f\u8fdd\u53cd\u9a8c\u8bc1\u65e5\u5fd7\uff0c\u7136\u540e\u4f7f\u7528LLM\u5224\u65ad\u5668\u5b9a\u4f4d\u5173\u952e\u5931\u8d25\u6b65\u9aa4\u548c\u7c7b\u522b", "result": "\u5728\u7ed3\u6784\u5316API\u5de5\u4f5c\u6d41\u3001\u4e8b\u4ef6\u7ba1\u7406\u548c\u5f00\u653e\u5f0f\u7f51\u9875/\u6587\u4ef6\u4efb\u52a1\u4e09\u4e2a\u9886\u57df\u4e2d\uff0cAGENTRX\u5728\u6b65\u9aa4\u5b9a\u4f4d\u548c\u5931\u8d25\u5f52\u56e0\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "AGENTRX\u80fd\u591f\u6709\u6548\u81ea\u52a8\u8bca\u65adAI\u4ee3\u7406\u5931\u8d25\uff0c\u964d\u4f4e\u4eba\u5de5\u6210\u672c\uff0c\u4e3a\u4ee3\u7406\u5931\u8d25\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u548c\u57fa\u51c6\u6570\u636e\u96c6", "topic": "agent analysis"}}
{"id": "2602.02276", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02276", "abs": "https://arxiv.org/abs/2602.02276", "authors": ["Kimi Team", "Tongtong Bai", "Yifan Bai", "Yiping Bao", "S. H. Cai", "Yuan Cao", "Y. Charles", "H. S. Che", "Cheng Chen", "Guanduo Chen", "Huarong Chen", "Jia Chen", "Jiahao Chen", "Jianlong Chen", "Jun Chen", "Kefan Chen", "Liang Chen", "Ruijue Chen", "Xinhao Chen", "Yanru Chen", "Yanxu Chen", "Yicun Chen", "Yimin Chen", "Yingjiang Chen", "Yuankun Chen", "Yujie Chen", "Yutian Chen", "Zhirong Chen", "Ziwei Chen", "Dazhi Cheng", "Minghan Chu", "Jialei Cui", "Jiaqi Deng", "Muxi Diao", "Hao Ding", "Mengfan Dong", "Mengnan Dong", "Yuxin Dong", "Yuhao Dong", "Angang Du", "Chenzhuang Du", "Dikang Du", "Lingxiao Du", "Yulun Du", "Yu Fan", "Shengjun Fang", "Qiulin Feng", "Yichen Feng", "Garimugai Fu", "Kelin Fu", "Hongcheng Gao", "Tong Gao", "Yuyao Ge", "Shangyi Geng", "Chengyang Gong", "Xiaochen Gong", "Zhuoma Gongque", "Qizheng Gu", "Xinran Gu", "Yicheng Gu", "Longyu Guan", "Yuanying Guo", "Xiaoru Hao", "Weiran He", "Wenyang He", "Yunjia He", "Chao Hong", "Hao Hu", "Jiaxi Hu", "Yangyang Hu", "Zhenxing Hu", "Ke Huang", "Ruiyuan Huang", "Weixiao Huang", "Zhiqi Huang", "Tao Jiang", "Zhejun Jiang", "Xinyi Jin", "Yu Jing", "Guokun Lai", "Aidi Li", "C. Li", "Cheng Li", "Fang Li", "Guanghe Li", "Guanyu Li", "Haitao Li", "Haoyang Li", "Jia Li", "Jingwei Li", "Junxiong Li", "Lincan Li", "Mo Li", "Weihong Li", "Wentao Li", "Xinhang Li", "Xinhao Li", "Yang Li", "Yanhao Li", "Yiwei Li", "Yuxiao Li", "Zhaowei Li", "Zheming Li", "Weilong Liao", "Jiawei Lin", "Xiaohan Lin", "Zhishan Lin", "Zichao Lin", "Cheng Liu", "Chenyu Liu", "Hongzhang Liu", "Liang Liu", "Shaowei Liu", "Shudong Liu", "Shuran Liu", "Tianwei Liu", "Tianyu Liu", "Weizhou Liu", "Xiangyan Liu", "Yangyang Liu", "Yanming Liu", "Yibo Liu", "Yuanxin Liu", "Yue Liu", "Zhengying Liu", "Zhongnuo Liu", "Enzhe Lu", "Haoyu Lu", "Zhiyuan Lu", "Junyu Luo", "Tongxu Luo", "Yashuo Luo", "Long Ma", "Yingwei Ma", "Shaoguang Mao", "Yuan Mei", "Xin Men", "Fanqing Meng", "Zhiyong Meng", "Yibo Miao", "Minqing Ni", "Kun Ouyang", "Siyuan Pan", "Bo Pang", "Yuchao Qian", "Ruoyu Qin", "Zeyu Qin", "Jiezhong Qiu", "Bowen Qu", "Zeyu Shang", "Youbo Shao", "Tianxiao Shen", "Zhennan Shen", "Juanfeng Shi", "Lidong Shi", "Shengyuan Shi", "Feifan Song", "Pengwei Song", "Tianhui Song", "Xiaoxi Song", "Hongjin Su", "Jianlin Su", "Zhaochen Su", "Lin Sui", "Jinsong Sun", "Junyao Sun", "Tongyu Sun", "Flood Sung", "Yunpeng Tai", "Chuning Tang", "Heyi Tang", "Xiaojuan Tang", "Zhengyang Tang", "Jiawen Tao", "Shiyuan Teng", "Chaoran Tian", "Pengfei Tian", "Ao Wang", "Bowen Wang", "Chensi Wang", "Chuang Wang", "Congcong Wang", "Dingkun Wang", "Dinglu Wang", "Dongliang Wang", "Feng Wang", "Hailong Wang", "Haiming Wang", "Hengzhi Wang", "Huaqing Wang", "Hui Wang", "Jiahao Wang", "Jinhong Wang", "Jiuzheng Wang", "Kaixin Wang", "Linian Wang", "Qibin Wang", "Shengjie Wang", "Shuyi Wang", "Si Wang", "Wei Wang", "Xiaochen Wang", "Xinyuan Wang", "Yao Wang", "Yejie Wang", "Yipu Wang", "Yiqin Wang", "Yucheng Wang", "Yuzhi Wang", "Zhaoji Wang", "Zhaowei Wang", "Zhengtao Wang", "Zhexu Wang", "Zihan Wang", "Zizhe Wang", "Chu Wei", "Ming Wei", "Chuan Wen", "Zichen Wen", "Chengjie Wu", "Haoning Wu", "Junyan Wu", "Rucong Wu", "Wenhao Wu", "Yuefeng Wu", "Yuhao Wu", "Yuxin Wu", "Zijian Wu", "Chenjun Xiao", "Jin Xie", "Xiaotong Xie", "Yuchong Xie", "Yifei Xin", "Bowei Xing", "Boyu Xu", "Jianfan Xu", "Jing Xu", "Jinjing Xu", "L. H. Xu", "Lin Xu", "Suting Xu", "Weixin Xu", "Xinbo Xu", "Xinran Xu", "Yangchuan Xu", "Yichang Xu", "Yuemeng Xu", "Zelai Xu", "Ziyao Xu", "Junjie Yan", "Yuzi Yan", "Guangyao Yang", "Hao Yang", "Junwei Yang", "Kai Yang", "Ningyuan Yang", "Ruihan Yang", "Xiaofei Yang", "Xinlong Yang", "Ying Yang", "Yi Yang", "Yi Yang", "Zhen Yang", "Zhilin Yang", "Zonghan Yang", "Haotian Yao", "Dan Ye", "Wenjie Ye", "Zhuorui Ye", "Bohong Yin", "Chengzhen Yu", "Longhui Yu", "Tao Yu", "Tianxiang Yu", "Enming Yuan", "Mengjie Yuan", "Xiaokun Yuan", "Yang Yue", "Weihao Zeng", "Dunyuan Zha", "Haobing Zhan", "Dehao Zhang", "Hao Zhang", "Jin Zhang", "Puqi Zhang", "Qiao Zhang", "Rui Zhang", "Xiaobin Zhang", "Y. Zhang", "Yadong Zhang", "Yangkun Zhang", "Yichi Zhang", "Yizhi Zhang", "Yongting Zhang", "Yu Zhang", "Yushun Zhang", "Yutao Zhang", "Yutong Zhang", "Zheng Zhang", "Chenguang Zhao", "Feifan Zhao", "Jinxiang Zhao", "Shuai Zhao", "Xiangyu Zhao", "Yikai Zhao", "Zijia Zhao", "Huabin Zheng", "Ruihan Zheng", "Shaojie Zheng", "Tengyang Zheng", "Junfeng Zhong", "Longguang Zhong", "Weiming Zhong", "M. Zhou", "Runjie Zhou", "Xinyu Zhou", "Zaida Zhou", "Jinguo Zhu", "Liya Zhu", "Xinhao Zhu", "Yuxuan Zhu", "Zhen Zhu", "Jingze Zhuang", "Weiyu Zhuang", "Ying Zou", "Xinxing Zu"], "title": "Kimi K2.5: Visual Agentic Intelligence", "comment": "Kimi K2.5 tech report", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "AI": {"tldr": "Kimi K2.5\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c-\u89c6\u89c9\u8054\u5408\u4f18\u5316\u63d0\u5347\u901a\u7528\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u5e76\u5f15\u5165Agent Swarm\u6846\u67b6\u5b9e\u73b0\u5e76\u884c\u4efb\u52a1\u5206\u89e3\u4e0e\u6267\u884c\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u63a8\u52a8\u901a\u7528\u667a\u80fd\u4f53\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8054\u5408\u4f18\u5316\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u5206\u79bb\u7684\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u9ad8\u6548\u7684\u5e76\u884c\u667a\u80fd\u4f53\u7f16\u6392\u6846\u67b6\u6765\u5904\u7406\u590d\u6742\u4efb\u52a1\u3002", "method": "1. \u591a\u6a21\u6001\u8054\u5408\u4f18\u5316\uff1a\u5305\u62ec\u8054\u5408\u6587\u672c-\u89c6\u89c9\u9884\u8bad\u7ec3\u3001\u96f6\u89c6\u89c9SFT\u548c\u8054\u5408\u6587\u672c-\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\uff1b2. Agent Swarm\u6846\u67b6\uff1a\u81ea\u5bfc\u5411\u5e76\u884c\u667a\u80fd\u4f53\u7f16\u6392\uff0c\u52a8\u6001\u5206\u89e3\u590d\u6742\u4efb\u52a1\u4e3a\u5f02\u6784\u5b50\u95ee\u9898\u5e76\u5e76\u53d1\u6267\u884c\u3002", "result": "\u5728\u7f16\u7a0b\u3001\u89c6\u89c9\u3001\u63a8\u7406\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u7b49\u591a\u4e2a\u9886\u57df\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff1bAgent Swarm\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe4.5\u500d\uff1b\u53d1\u5e03\u4e86\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u68c0\u67e5\u70b9\u3002", "conclusion": "Kimi K2.5\u901a\u8fc7\u591a\u6a21\u6001\u8054\u5408\u4f18\u5316\u548c\u5e76\u884c\u667a\u80fd\u4f53\u7f16\u6392\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u6027\u80fd\u548c\u5904\u7406\u6548\u7387\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u667a\u80fd\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2507.18992", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18992", "abs": "https://arxiv.org/abs/2507.18992", "authors": ["Jongsoo Lee", "Jangwon Kim", "Jiseok Jeong", "Soohee Han"], "title": "Reinforcement Learning via Conservative Agent for Environments with Random Delays", "comment": null, "summary": "Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments with random delays remain largely unexplored due to their inherent variability and unpredictability. In this study, we propose a simple yet robust agent for decision-making under random delays, termed the conservative agent, which reformulates the random-delay environment into its constant-delay equivalent. This transformation enables any state-of-the-art constant-delay method to be directly extended to the random-delay environments without modifying the algorithmic structure or sacrificing performance. We evaluate the conservative agent-based algorithm on continuous control tasks, and empirical results demonstrate that it significantly outperforms existing baseline algorithms in terms of asymptotic performance and sample efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4fdd\u5b88\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5c06\u968f\u673a\u5ef6\u8fdf\u73af\u5883\u8f6c\u5316\u4e3a\u6052\u5b9a\u5ef6\u8fdf\u73af\u5883\uff0c\u4f7f\u73b0\u6709\u6052\u5b9a\u5ef6\u8fdf\u65b9\u6cd5\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u968f\u673a\u5ef6\u8fdf\u573a\u666f\uff0c\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7b97\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u5e38\u53d7\u73af\u5883\u5ef6\u8fdf\u53cd\u9988\u56f0\u6270\uff0c\u5ef6\u8fdf\u8fdd\u53cd\u9a6c\u5c14\u53ef\u592b\u5047\u8bbe\u5e76\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6052\u5b9a\u5ef6\u8fdf\u73af\u5883\uff0c\u800c\u968f\u673a\u5ef6\u8fdf\u73af\u5883\u56e0\u5176\u53ef\u53d8\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51fa\u4fdd\u5b88\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5c06\u968f\u673a\u5ef6\u8fdf\u73af\u5883\u91cd\u65b0\u8868\u8ff0\u4e3a\u5176\u6052\u5b9a\u5ef6\u8fdf\u7b49\u4ef7\u5f62\u5f0f\u3002\u8fd9\u79cd\u8f6c\u6362\u4f7f\u4efb\u4f55\u6700\u5148\u8fdb\u7684\u6052\u5b9a\u5ef6\u8fdf\u65b9\u6cd5\u90fd\u80fd\u76f4\u63a5\u6269\u5c55\u5230\u968f\u673a\u5ef6\u8fdf\u73af\u5883\uff0c\u65e0\u9700\u4fee\u6539\u7b97\u6cd5\u7ed3\u6784\u6216\u727a\u7272\u6027\u80fd\u3002", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4fdd\u5b88\u667a\u80fd\u4f53\u7b97\u6cd5\uff0c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\u5176\u5728\u6e10\u8fd1\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7b97\u6cd5\u3002", "conclusion": "\u4fdd\u5b88\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u5904\u7406\u968f\u673a\u5ef6\u8fdf\u73af\u5883\uff0c\u901a\u8fc7\u5c06\u968f\u673a\u5ef6\u8fdf\u8f6c\u5316\u4e3a\u6052\u5b9a\u5ef6\u8fdf\uff0c\u4f7f\u73b0\u6709\u6052\u5b9a\u5ef6\u8fdf\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u7528\u4e8e\u66f4\u590d\u6742\u7684\u968f\u673a\u5ef6\u8fdf\u573a\u666f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02301", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02301", "abs": "https://arxiv.org/abs/2602.02301", "authors": ["Min Cai", "Yu Liang", "Longzheng Wang", "Yan Wang", "Yueyang Zhang", "Long Xia", "Zhiyuan Sun", "Xi Ye", "Daiting Shi"], "title": "Advancing General-Purpose Reasoning Models with Modular Gradient Surgery", "comment": "Preprint; Code: https://github.com/StringNLPLAB/MGS; Website: https://modular-gradient-surgery.github.io", "summary": "Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\\%) and 4.5 (11.1\\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u5757\u5316\u68af\u5ea6\u624b\u672f(MGS)\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u51b3Transformer\u6a21\u5757\u7ea7\u522b\u7684\u68af\u5ea6\u51b2\u7a81\uff0c\u51cf\u5c11\u591a\u9886\u57df\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8de8\u9886\u57df\u5e72\u6270\uff0c\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u591a\u9886\u57df\u8bad\u7ec3\u901a\u7528\u5927\u578b\u63a8\u7406\u6a21\u578b\u65f6\uff0c\u9886\u57df\u5f02\u8d28\u6027\u5bfc\u81f4\u663e\u8457\u7684\u8de8\u9886\u57df\u5e72\u6270\uff0c\u73b0\u6709\u7b56\u7565\uff08\u987a\u5e8fRL\u548c\u6df7\u5408RL\uff09\u5728\u884c\u4e3a\u548c\u68af\u5ea6\u5c42\u9762\u90fd\u5b58\u5728\u51b2\u7a81\uff0c\u9650\u5236\u4e86\u6574\u4f53\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u6a21\u5757\u5316\u68af\u5ea6\u624b\u672f(MGS)\uff0c\u5728Transformer\u5185\u90e8\u6a21\u5757\u7ea7\u522b\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\uff0c\u800c\u4e0d\u662f\u5728\u6574\u4e2a\u6a21\u578b\u5c42\u9762\u5904\u7406\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8eLlama\u548cQwen\u6a21\u578b\uff0c\u5728\u6570\u5b66\u3001\u901a\u7528\u804a\u5929\u548c\u6307\u4ee4\u8ddf\u968f\u4e09\u4e2a\u4ee3\u8868\u6027\u9886\u57df\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "MGS\u5728Llama\u548cQwen\u6a21\u578b\u4e0a\u5206\u522b\u5b9e\u73b0\u4e864.3\u5206(16.6%)\u548c4.5\u5206(11.1%)\u7684\u5e73\u5747\u6539\u8fdb\uff0c\u4f18\u4e8e\u6807\u51c6\u591a\u4efb\u52a1RL\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660eMGS\u5728\u957f\u65f6\u95f4\u8bad\u7ec3\u4e2d\u4ecd\u7136\u6709\u6548\u3002", "conclusion": "\u7814\u7a76\u9610\u660e\u4e86\u591a\u9886\u57dfRL\u4e2d\u5e72\u6270\u7684\u6765\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u8bad\u7ec3\u901a\u7528\u5927\u578b\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.00927", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00927", "abs": "https://arxiv.org/abs/2602.00927", "authors": ["Yihao Xue", "Allan Zhang", "Jianhao Huang", "Amit Sahai", "Baharan Mirzasoleiman"], "title": "Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision", "comment": null, "summary": "Training LLMs to think and reason for longer has become a key ingredient in building state-of-the-art models that can solve complex problems previously out of reach. Recent efforts pursue this in different ways, such as RL fine-tuning to elicit long CoT or scaling latent reasoning through architectural recurrence. This makes reasoning length an important scaling knob. In this work, we identify a novel phenomenon (both theoretically and experimentally): under outcome-only supervision, out-of-distribution (OOD) performance can continue improving as training-time reasoning length (e.g., the token budget in RL, or the loop count in looped Transformers) increases, even after in-distribution (ID) performance has saturated. This suggests that robustness may require a larger budget than ID validation alone would indicate. We provide theoretical explanations via two mechanisms: (i) self-iteration can induce a stronger inductive bias in the hypothesis class, reshaping ID-optimal solutions in ways that improve OOD generalization; and (ii) when shortcut solutions that work for ID samples but not for OOD samples persist in the hypothesis class, regularization can reduce the learned solution's reliance on these shortcuts as the number of self-iterations increases. We complement the theory with empirical evidence from two realizations of scaling training-time reasoning length: increasing the number of loops in looped Transformers on a synthetic task, and increasing token budgets during RL fine-tuning of LLMs on mathematical reasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff1a\u5728\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u76d1\u7763\u4e0b\uff0c\u589e\u52a0\u8bad\u7ec3\u65f6\u63a8\u7406\u957f\u5ea6\uff08\u5982RL\u4e2d\u7684token\u9884\u7b97\u6216\u5faa\u73afTransformer\u7684\u5faa\u73af\u6b21\u6570\uff09\u53ef\u4ee5\u6301\u7eed\u63d0\u5347\u5206\u5e03\u5916\u6027\u80fd\uff0c\u5373\u4f7f\u5206\u5e03\u5185\u6027\u80fd\u5df2\u9971\u548c\uff0c\u8fd9\u8868\u660e\u9c81\u68d2\u6027\u9700\u8981\u6bd4ID\u9a8c\u8bc1\u66f4\u5927\u7684\u9884\u7b97\u3002", "motivation": "\u8bad\u7ec3LLM\u8fdb\u884c\u66f4\u957f\u63a8\u7406\u5df2\u6210\u4e3a\u6784\u5efa\u80fd\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u6700\u5148\u8fdb\u6a21\u578b\u7684\u5173\u952e\u8981\u7d20\u3002\u5f53\u524d\u7814\u7a76\u901a\u8fc7\u4e0d\u540c\u65b9\u5f0f\u8ffd\u6c42\u8fd9\u4e00\u76ee\u6807\uff0c\u5982RL\u5fae\u8c03\u4ee5\u5f15\u51fa\u957f\u94fe\u601d\u7ef4\u6216\u901a\u8fc7\u67b6\u6784\u5faa\u73af\u6269\u5c55\u6f5c\u5728\u63a8\u7406\u3002\u8fd9\u4f7f\u5f97\u63a8\u7406\u957f\u5ea6\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u6269\u5c55\u53c2\u6570\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff1a1\uff09\u7406\u8bba\u89e3\u91ca\u4e24\u79cd\u673a\u5236\uff1a\u81ea\u8fed\u4ee3\u53ef\u5728\u5047\u8bbe\u7c7b\u4e2d\u5f15\u5165\u66f4\u5f3a\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u91cd\u5851ID\u6700\u4f18\u89e3\u4ee5\u6539\u5584OOD\u6cdb\u5316\uff1b2\uff09\u5f53\u4ec5\u9002\u7528\u4e8eID\u6837\u672c\u7684\u6377\u5f84\u89e3\u5728\u5047\u8bbe\u7c7b\u4e2d\u6301\u7eed\u5b58\u5728\u65f6\uff0c\u6b63\u5219\u5316\u53ef\u968f\u7740\u81ea\u8fed\u4ee3\u6b21\u6570\u589e\u52a0\u51cf\u5c11\u5bf9\u6377\u5f84\u7684\u4f9d\u8d56\u3002\u5b9e\u9a8c\u5305\u62ec\uff1a\u5728\u5408\u6210\u4efb\u52a1\u4e0a\u589e\u52a0\u5faa\u73afTransformer\u7684\u5faa\u73af\u6b21\u6570\uff0c\u4ee5\u53ca\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u589e\u52a0RL\u5fae\u8c03\u65f6\u7684token\u9884\u7b97\u3002", "result": "\u53d1\u73b0\u65b0\u9896\u73b0\u8c61\uff1a\u5728\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u76d1\u7763\u4e0b\uff0cOOD\u6027\u80fd\u53ef\u968f\u8bad\u7ec3\u65f6\u63a8\u7406\u957f\u5ea6\u589e\u52a0\u800c\u6301\u7eed\u6539\u5584\uff0c\u5373\u4f7fID\u6027\u80fd\u5df2\u9971\u548c\u3002\u8fd9\u8868\u660e\u9c81\u68d2\u6027\u53ef\u80fd\u9700\u8981\u6bd4ID\u9a8c\u8bc1\u66f4\u5927\u7684\u9884\u7b97\u3002", "conclusion": "\u589e\u52a0\u8bad\u7ec3\u65f6\u63a8\u7406\u957f\u5ea6\u53ef\u4ee5\u6539\u5584OOD\u6cdb\u5316\uff0c\u5373\u4f7fID\u6027\u80fd\u5df2\u9971\u548c\u3002\u8fd9\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u8868\u660e\u9700\u8981\u8d85\u8d8aID\u9a8c\u8bc1\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.02343", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02343", "abs": "https://arxiv.org/abs/2602.02343", "authors": ["Ziwen Xu", "Chenyan Wu", "Hengyu Sun", "Haiwen Hong", "Mengru Wang", "Yunzhi Yao", "Longtao Huang", "Hui Xue", "Shumin Deng", "Zhixuan Chu", "Huajun Chen", "Ningyu Zhang"], "title": "Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics", "comment": "Work in progress", "summary": "Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u4e0d\u540c\u7684LLM\u63a7\u5236\u65b9\u6cd5\uff08\u6743\u91cd\u5fae\u8c03\u3001LoRA\u3001\u6fc0\u6d3b\u5e72\u9884\uff09\u89c6\u4e3a\u7531\u63a7\u5236\u4fe1\u53f7\u8bf1\u5bfc\u7684\u52a8\u6001\u6743\u91cd\u66f4\u65b0\uff0c\u5e76\u5f15\u5165\u504f\u597d-\u6548\u7528\u5206\u6790\u6765\u91cf\u5316\u63a7\u5236\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a7\u5236\u65b9\u6cd5\uff08\u5982\u6743\u91cd\u5fae\u8c03\u3001LoRA\u9002\u914d\u3001\u6fc0\u6d3b\u5e72\u9884\uff09\u901a\u5e38\u88ab\u5b64\u7acb\u7814\u7a76\uff0c\u96be\u4ee5\u6bd4\u8f83\u548c\u5efa\u7acb\u8054\u7cfb\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u7406\u89e3\u8fd9\u4e9b\u65b9\u6cd5\u7684\u672c\u8d28\u8054\u7cfb\u548c\u6548\u679c\u3002", "method": "1. \u63d0\u51fa\u7edf\u4e00\u89c6\u56fe\uff1a\u5c06\u6240\u6709\u5e72\u9884\u65b9\u6cd5\u6846\u67b6\u5316\u4e3a\u7531\u63a7\u5236\u4fe1\u53f7\u8bf1\u5bfc\u7684\u52a8\u6001\u6743\u91cd\u66f4\u65b0\uff1b2. \u63d0\u51fa\u7edf\u4e00\u7684\u504f\u597d-\u6548\u7528\u5206\u6790\uff1a\u5c06\u63a7\u5236\u6548\u679c\u5206\u89e3\u4e3a\u504f\u597d\uff08\u5bf9\u76ee\u6807\u6982\u5ff5\u7684\u503e\u5411\u6027\uff09\u548c\u6548\u7528\uff08\u8fde\u8d2f\u4e14\u4efb\u52a1\u6709\u6548\u7684\u751f\u6210\uff09\uff0c\u5e76\u4f7f\u7528\u6781\u6027\u914d\u5bf9\u5bf9\u6bd4\u793a\u4f8b\u5728\u5171\u4eab\u5bf9\u6570\u51e0\u7387\u5c3a\u5ea6\u4e0a\u6d4b\u91cf\uff1b3. \u4ece\u6fc0\u6d3b\u6d41\u5f62\u89d2\u5ea6\u89e3\u91ca\u63a7\u5236\u884c\u4e3a\uff1b4. \u57fa\u4e8e\u5206\u6790\u63d0\u51fa\u65b0\u7684\u63a7\u5236\u65b9\u6cd5SPLIT\u3002", "result": "\u53d1\u73b0\u6240\u6709\u65b9\u6cd5\u90fd\u5b58\u5728\u504f\u597d\u4e0e\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\uff1a\u66f4\u5f3a\u7684\u63a7\u5236\u4f1a\u589e\u52a0\u504f\u597d\uff0c\u4f46\u4f1a\u53ef\u9884\u6d4b\u5730\u964d\u4f4e\u6548\u7528\u3002\u4ece\u6fc0\u6d3b\u6d41\u5f62\u89d2\u5ea6\u770b\uff0c\u63a7\u5236\u901a\u8fc7\u6cbf\u76ee\u6807\u6982\u5ff5\u65b9\u5411\u79fb\u52a8\u8868\u793a\u6765\u589e\u5f3a\u504f\u597d\uff0c\u800c\u5f53\u5e72\u9884\u5c06\u8868\u793a\u63a8\u79bb\u6a21\u578b\u7684\u6709\u6548\u751f\u6210\u6d41\u5f62\u65f6\uff0c\u6548\u7528\u4f1a\u4e0b\u964d\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u5206\u6790LLM\u63a7\u5236\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u504f\u597d\u4e0e\u6548\u7528\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u65b0\u7684\u63a7\u5236\u65b9\u6cd5SPLIT\uff0c\u80fd\u5728\u63d0\u9ad8\u504f\u597d\u7684\u540c\u65f6\u66f4\u597d\u5730\u4fdd\u6301\u6548\u7528\u3002", "topic": "agent analysis"}}
{"id": "2602.00952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.00952", "abs": "https://arxiv.org/abs/2602.00952", "authors": ["Jing Wang", "Jie Shen", "Dean Foster", "Zohar Karnin", "Jeremy C Weiss"], "title": "Optimal Budgeted Adaptation of Large Language Models", "comment": null, "summary": "The trade-off between labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models (LLMs). We propose a principled framework for \\emph{budget-aware supervised fine-tuning} by casting LLM adaptation as a contextual Stackelberg game. In our formulation, the learner (leader) commits to a scoring policy and a label-querying strategy, while an adaptive environment (follower) selects challenging supervised alternatives in response. To explicitly address label efficiency, we incorporate a finite supervision budget directly into the learning objective. Our algorithm operates in the full-feedback regime and achieves $\\tilde{O}(d\\sqrt{T})$ regret under standard linear contextual assumptions. We extend the framework with a Largest-Latency-First (LLF) confidence gate that selectively queries labels, achieving a budget-aware regret bound of $\\tilde{O}(\\sqrt{dB} + c\\sqrt{B})$ with $B=\u03b2T$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u7b97\u611f\u77e5\u7684\u76d1\u7763\u5fae\u8c03\u6846\u67b6\uff0c\u5c06LLM\u9002\u5e94\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587Stackelberg\u535a\u5f08\uff0c\u901a\u8fc7\u6709\u9650\u76d1\u7763\u9884\u7b97\u5b9e\u73b0\u6807\u7b7e\u9ad8\u6548\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u6807\u6ce8\u6570\u636e\u53ef\u7528\u6027\u4e0e\u4e0b\u6e38\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u7684\u9ad8\u6548\u5b66\u4e60\u6311\u6218\u3002", "method": "\u5c06LLM\u9002\u5e94\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587Stackelberg\u535a\u5f08\uff0c\u5b66\u4e60\u8005\uff08\u9886\u5bfc\u8005\uff09\u627f\u8bfa\u8bc4\u5206\u7b56\u7565\u548c\u6807\u7b7e\u67e5\u8be2\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u73af\u5883\uff08\u8ddf\u968f\u8005\uff09\u9009\u62e9\u5177\u6709\u6311\u6218\u6027\u7684\u76d1\u7763\u66ff\u4ee3\u65b9\u6848\u3002\u5f15\u5165\u6709\u9650\u76d1\u7763\u9884\u7b97\u5230\u5b66\u4e60\u76ee\u6807\uff0c\u5e76\u63d0\u51fa\u5e26\u6709Largest-Latency-First\u7f6e\u4fe1\u95e8\u7684\u9009\u62e9\u6027\u6807\u7b7e\u67e5\u8be2\u7b97\u6cd5\u3002", "result": "\u5728\u5168\u53cd\u9988\u673a\u5236\u4e0b\u5b9e\u73b0$\\tilde{O}(d\\sqrt{T})$\u9057\u61be\uff0c\u5728\u6807\u51c6\u7ebf\u6027\u4e0a\u4e0b\u6587\u5047\u8bbe\u4e0b\u3002\u6269\u5c55\u6846\u67b6\u901a\u8fc7LLF\u7f6e\u4fe1\u95e8\u5b9e\u73b0\u9884\u7b97\u611f\u77e5\u9057\u61be\u754c$\\tilde{O}(\\sqrt{dB} + c\\sqrt{B})$\uff0c\u5176\u4e2d$B=\u03b2T$\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9884\u7b97\u53d7\u9650\u7684LLM\u76d1\u7763\u5fae\u8c03\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u6807\u6ce8\u6210\u672c\u4e0e\u6a21\u578b\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.02377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02377", "abs": "https://arxiv.org/abs/2602.02377", "authors": ["Haotong Yang", "Zitong Wang", "Shijia Kang", "Siqi Yang", "Wenkai Yu", "Xu Niu", "Yike Sun", "Yi Hu", "Zhouchen Lin", "Muhan Zhang"], "title": "Proof-RM: A Scalable and Generalizable Reward Model for Math Proof", "comment": "Under review", "summary": "While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality \"**question-proof-check**\" triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating additional process reward and token weight balance to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6570\u636e\u6784\u5efa\u7ba1\u9053\uff0c\u5229\u7528LLM\u751f\u6210\u9ad8\u8d28\u91cf\u7684\"\u95ee\u9898-\u8bc1\u660e-\u68c0\u67e5\"\u4e09\u5143\u7ec4\u6570\u636e\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2a\u8bc1\u660e\u68c0\u67e5\u5956\u52b1\u6a21\u578b\uff0c\u4ee5\u589e\u5f3aLLM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u867d\u7136LLM\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8bb8\u591a\u9ad8\u7ea7\u6570\u5b66\u95ee\u9898\u662f\u57fa\u4e8e\u8bc1\u660e\u7684\uff0c\u65e0\u6cd5\u901a\u8fc7\u7b80\u5355\u7684\u7b54\u6848\u5339\u914d\u6765\u786e\u5b9a\u8bc1\u660e\u7684\u771f\u5b9e\u6027\u3002\u9700\u8981\u80fd\u591f\u53ef\u9760\u8bc4\u4f30\u5b8c\u6574\u8bc1\u660e\u8fc7\u7a0b\u7684\u5956\u52b1\u6a21\u578b\u6765\u5b9e\u73b0\u81ea\u52a8\u9a8c\u8bc1\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u6784\u5efa\u7ba1\u9053\uff0c\u901a\u8fc7\u7cfb\u7edf\u53d8\u5316\u95ee\u9898\u6765\u6e90\u3001\u751f\u6210\u65b9\u6cd5\u548c\u6a21\u578b\u914d\u7f6e\uff0c\u521b\u5efa\u591a\u6837\u7684\u95ee\u9898-\u8bc1\u660e\u5bf9\uff0c\u6db5\u76d6\u591a\u4e2a\u96be\u5ea6\u7ea7\u522b\u3001\u8bed\u8a00\u98ce\u683c\u548c\u9519\u8bef\u7c7b\u578b\uff0c\u7136\u540e\u901a\u8fc7\u5206\u5c42\u4eba\u5de5\u5ba1\u67e5\u8fdb\u884c\u8fc7\u6ee4\u3002\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u8bad\u7ec3\u8bc1\u660e\u68c0\u67e5\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u52a0\u5165\u8fc7\u7a0b\u5956\u52b1\u548c\u4ee4\u724c\u6743\u91cd\u5e73\u8861\u6765\u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u4ece\u591a\u4e2a\u89d2\u5ea6\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u5f3a\u5927\u6027\u80fd\uff0c\u5305\u62ec\u5956\u52b1\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6d4b\u8bd5\u65f6\u6307\u5bfc\uff0c\u4e3a\u589e\u5f3aLLM\u6570\u5b66\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5b9e\u8df5\u65b9\u6cd5\u548c\u5de5\u5177\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8bad\u7ec3\u8bc1\u660e\u68c0\u67e5\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u4e3aLLM\u5728\u6570\u5b66\u8bc1\u660e\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.00959", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.00959", "abs": "https://arxiv.org/abs/2602.00959", "authors": ["Yuheng Yang", "Siqi Zhu", "Tao Feng", "Ge Liu", "Jiaxuan You"], "title": "Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction", "comment": "Homepage: https://ulab-uiuc.github.io/KnowledgeExtraction/", "summary": "Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.", "AI": {"tldr": "\u63d0\u51fa\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u7cfb\u7edf\u63d0\u53d6\u548c\u91cf\u5316LLMs\u7684\u77e5\u8bc6\u8fb9\u754c\uff0c\u53d1\u73b0\u9012\u5f52\u5206\u7c7b\u6cd5\u6700\u6709\u6548\uff0c\u89c2\u5bdf\u5230\u77e5\u8bc6\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u8bc6\u522bPass@1\u4e0ePass@k\u7684\u6743\u8861", "motivation": "LLMs\u53ef\u89c6\u4e3a\u538b\u7f29\u77e5\u8bc6\u5e93\uff0c\u4f46\u5176\u5b9e\u9645\u5305\u542b\u7684\u77e5\u8bc6\u5185\u5bb9\u548c\u8fb9\u754c\u5c1a\u4e0d\u660e\u786e\u3002\u73b0\u6709\u57fa\u51c6\u5927\u591a\u662f\u9759\u6001\u7684\uff0c\u5bf9\u7cfb\u7edf\u77e5\u8bc6\u63a2\u6d4b\u652f\u6301\u6709\u9650\uff0c\u9700\u8981\u66f4\u52a8\u6001\u3001\u7cfb\u7edf\u7684\u77e5\u8bc6\u63d0\u53d6\u65b9\u6cd5", "method": "\u63d0\u51fa\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u56db\u79cd\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u5728\u4e0d\u540c\u7c92\u5ea6\u4e0a\u63a2\u6d4b\u77e5\u8bc6\uff1b\u91c7\u7528\u4e09\u9636\u6bb5\u77e5\u8bc6\u5904\u7406\u6d41\u7a0b\uff1a\u5411\u91cf\u8fc7\u6ee4\u53bb\u9664\u91cd\u590d\u3001LLM\u88c1\u51b3\u89e3\u51b3\u8bed\u4e49\u91cd\u53e0\u3001\u9886\u57df\u76f8\u5173\u6027\u5ba1\u8ba1\u4fdd\u7559\u6709\u6548\u77e5\u8bc6\u5355\u5143", "result": "\u9012\u5f52\u5206\u7c7b\u6cd5\u662f\u6700\u6709\u6548\u7684\u63a2\u7d22\u7b56\u7565\uff1b\u89c2\u5bdf\u5230\u6e05\u6670\u7684\u77e5\u8bc6\u7f29\u653e\u5b9a\u5f8b\uff08\u66f4\u5927\u6a21\u578b\u63d0\u53d6\u66f4\u591a\u77e5\u8bc6\uff09\uff1b\u53d1\u73b0Pass@1\u4e0ePass@k\u7684\u6743\u8861\uff08\u4e13\u4e1a\u6a21\u578b\u521d\u59cb\u51c6\u786e\u7387\u9ad8\u4f46\u5feb\u901f\u4e0b\u964d\uff0c\u901a\u7528\u6a21\u578b\u6027\u80fd\u7a33\u5b9a\uff09\uff1b\u8bad\u7ec3\u6570\u636e\u7ec4\u6210\u5dee\u5f02\u5bfc\u81f4\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u5177\u6709\u53ef\u6d4b\u91cf\u7684\u77e5\u8bc6\u7279\u5f81", "conclusion": "\u63d0\u51fa\u7684\u4ea4\u4e92\u5f0f\u6846\u67b6\u80fd\u7cfb\u7edf\u63d0\u53d6\u548c\u91cf\u5316LLMs\u77e5\u8bc6\uff0c\u63ed\u793a\u4e86\u6709\u6548\u7684\u63a2\u7d22\u7b56\u7565\u3001\u77e5\u8bc6\u7f29\u653e\u89c4\u5f8b\u3001\u6027\u80fd\u6743\u8861\u6a21\u5f0f\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u5bf9\u77e5\u8bc6\u7279\u5f81\u7684\u5f71\u54cd", "topic": "agent analysis"}}
{"id": "2602.01003", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01003", "abs": "https://arxiv.org/abs/2602.01003", "authors": ["Zhishen Sun", "Sizhe Dang", "Guang Dai", "Haishan Ye"], "title": "ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning", "comment": null, "summary": "Reinforcement learning (RL) has become a key training step for improving mathematical reasoning in large language models (LLMs), but it often has high GPU memory usage, which makes it hard to use in settings with limited resources. To reduce these issues, we propose Evolution Strategies with Sharpness-Aware Maximization (ESSAM), a full parameter fine-tuning framework that tightly combines the zero-order search in parameter space from Evolution Strategies (ES) with the Sharpness-Aware Maximization (SAM) to improve generalization. We conduct fine-tuning experiments on the mainstream mathematica reasoning task GSM8K. The results show that ESSAM achieves an average accuracy of 78.27\\% across all models and its overall performance is comparable to RL methods. It surpasses classic RL algorithm PPO with an accuracy of 77.72\\% and is comparable to GRPO with an accuracy of 78.34\\%, and even surpassing them on some models. In terms of GPU memory usage, ESSAM reduces the average GPU memory usage by $18\\times$ compared to PPO and by $10\\times$ compared to GRPO, achieving an extremely low GPU memory usage.", "AI": {"tldr": "ESSAM\u7ed3\u5408\u8fdb\u5316\u7b56\u7565\u4e0e\u9510\u5ea6\u611f\u77e5\u6700\u5927\u5316\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e0e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4eGPU\u5185\u5b58\u4f7f\u7528", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u63a8\u7406\u80fd\u529b\u65f6GPU\u5185\u5b58\u6d88\u8017\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528", "method": "\u63d0\u51faESSAM\u6846\u67b6\uff0c\u5c06\u8fdb\u5316\u7b56\u7565\u7684\u53c2\u6570\u7a7a\u95f4\u96f6\u9636\u641c\u7d22\u4e0e\u9510\u5ea6\u611f\u77e5\u6700\u5927\u5316\u7d27\u5bc6\u7ed3\u5408\uff0c\u8fdb\u884c\u5168\u53c2\u6570\u5fae\u8c03", "result": "\u5728GSM8K\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u8fbe78.27%\uff0c\u6027\u80fd\u4e0eRL\u65b9\u6cd5\u76f8\u5f53\uff0cGPU\u5185\u5b58\u4f7f\u7528\u6bd4PPO\u964d\u4f4e18\u500d\uff0c\u6bd4GRPO\u964d\u4f4e10\u500d", "conclusion": "ESSAM\u5728\u4fdd\u6301\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86GPU\u5185\u5b58\u9700\u6c42\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2602.02474", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02474", "abs": "https://arxiv.org/abs/2602.02474", "authors": ["Haozhen Zhang", "Quanyu Long", "Jianzhu Bao", "Tao Feng", "Weizhi Zhang", "Haodong Yue", "Wenya Wang"], "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents", "comment": "Code is available at https://github.com/ViktorAxelsen/MemSkill", "summary": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \\emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \\emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \\emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.", "AI": {"tldr": "MemSkill\u5c06LLM\u4ee3\u7406\u8bb0\u5fc6\u64cd\u4f5c\u91cd\u6784\u4e3a\u53ef\u5b66\u4e60\u548c\u53ef\u6f14\u5316\u7684\u8bb0\u5fc6\u6280\u80fd\uff0c\u901a\u8fc7\u63a7\u5236\u5668\u9009\u62e9\u6280\u80fd\u3001\u6267\u884c\u5668\u751f\u6210\u8bb0\u5fc6\u3001\u8bbe\u8ba1\u5668\u6f14\u5316\u6280\u80fd\u96c6\uff0c\u5f62\u6210\u95ed\u73af\u7cfb\u7edf\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u8bb0\u5fc6\u7cfb\u7edf\u4f9d\u8d56\u5c11\u91cf\u9759\u6001\u3001\u4eba\u5de5\u8bbe\u8ba1\u7684\u8bb0\u5fc6\u63d0\u53d6\u64cd\u4f5c\uff0c\u8fd9\u4e9b\u56fa\u5b9a\u7a0b\u5e8f\u5c06\u4eba\u7c7b\u5148\u9a8c\u786c\u7f16\u7801\u5230\u5b58\u50a8\u548c\u4fee\u8ba2\u8fc7\u7a0b\u4e2d\uff0c\u5bfc\u81f4\u5728\u4e0d\u540c\u4ea4\u4e92\u6a21\u5f0f\u4e0b\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u5728\u957f\u5386\u53f2\u8bb0\u5f55\u4e0a\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faMemSkill\u6846\u67b6\uff1a1) \u63a7\u5236\u5668\u5b66\u4e60\u9009\u62e9\u76f8\u5173\u8bb0\u5fc6\u6280\u80fd\uff1b2) LLM\u6267\u884c\u5668\u751f\u6210\u6280\u80fd\u5f15\u5bfc\u7684\u8bb0\u5fc6\uff1b3) \u8bbe\u8ba1\u5668\u5b9a\u671f\u5ba1\u67e5\u56f0\u96be\u6848\u4f8b\uff0c\u901a\u8fc7\u63d0\u51fa\u6539\u8fdb\u548c\u65b0\u6280\u80fd\u6765\u6f14\u5316\u6280\u80fd\u96c6\uff0c\u5f62\u6210\u95ed\u73af\u6539\u8fdb\u8fc7\u7a0b\u3002", "result": "\u5728LoCoMo\u3001LongMemEval\u3001HotpotQA\u548cALFWorld\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMemSkill\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u8de8\u573a\u666f\u6cdb\u5316\u80fd\u529b\u3002\u5206\u6790\u63ed\u793a\u4e86\u6280\u80fd\u5982\u4f55\u6f14\u5316\uff0c\u4e3a\u66f4\u81ea\u9002\u5e94\u7684\u8bb0\u5fc6\u7ba1\u7406\u63d0\u4f9b\u89c1\u89e3\u3002", "conclusion": "MemSkill\u901a\u8fc7\u5c06\u8bb0\u5fc6\u64cd\u4f5c\u91cd\u6784\u4e3a\u53ef\u5b66\u4e60\u548c\u6f14\u5316\u7684\u6280\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684LLM\u4ee3\u7406\u8bb0\u5fc6\u7ba1\u7406\uff0c\u4e3a\u81ea\u6f14\u5316\u8bb0\u5fc6\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.02477", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02477", "abs": "https://arxiv.org/abs/2602.02477", "authors": ["Xiao Liang", "Zhong-Zhi Li", "Zhenghao Lin", "Eric Hancheng Jiang", "Hengyuan Zhang", "Yelong Shen", "Kai-Wei Chang", "Ying Nian Wu", "Yeyun Gong", "Weizhu Chen"], "title": "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6cbb\u63a8\u7406\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u76f8\u6bd4\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u5728\u7ade\u8d5b\u7ea7\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u5728\u6a21\u578b\u80fd\u529b\u6781\u9650\u65f6\u5f80\u5f80\u4e0d\u8db3\uff0c\u4e14\u5176\u4e25\u683c\u7684\u987a\u5e8f\u6027\u9650\u5236\u4e86\u6d4b\u8bd5\u65f6\u7684\u53ef\u6269\u5c55\u6027\u3002\u5206\u6cbb\u63a8\u7406\u867d\u7136\u6709\u671b\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u4f46\u901a\u7528\u540e\u8bad\u7ec3\u4e0e\u5206\u6cbb\u5f0f\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u4e0d\u5339\u914d\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5145\u5206\u5229\u7528\u8fd9\u79cd\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5206\u6cbb\u63a8\u7406\u80fd\u529b\u3002\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\uff0c\u7b56\u7565\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u4e00\u7ec4\u5b50\u95ee\u9898\uff0c\u987a\u5e8f\u89e3\u51b3\u5b83\u4eec\uff0c\u7136\u540e\u57fa\u4e8e\u5b50\u95ee\u9898\u89e3\u51b3\u65b9\u6848\u5904\u7406\u539f\u59cb\u95ee\u9898\u3002\u5206\u89e3\u548c\u89e3\u51b3\u65b9\u6848\u90fd\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u3002", "result": "\u5728\u53ef\u6bd4\u8bad\u7ec3\u6761\u4ef6\u4e0b\uff0c\u5206\u6cbb\u5f0f\u6846\u67b6\u8d4b\u4e88\u6a21\u578b\u66f4\u9ad8\u7684\u6027\u80fd\u4e0a\u9650\u548c\u66f4\u5f3a\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\uff0c\u5728\u7ade\u8d5b\u7ea7\u57fa\u51c6\u4e0a\uff0cPass@1\u8d85\u8fc7\u94fe\u5f0f\u601d\u7ef4\u63a8\u74068.6%\uff0cPass@32\u8d85\u8fc76.3%\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u589e\u5f3a\u5206\u6cbb\u63a8\u7406\u80fd\u529b\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u91ca\u653e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6700\u5177\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u7684\u63a8\u7406\u6f5c\u529b\uff0c\u63d0\u4f9b\u6bd4\u4f20\u7edf\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02486", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02486", "abs": "https://arxiv.org/abs/2602.02486", "authors": ["Jialiang Zhu", "Gongrui Zhang", "Xiaolong Ma", "Lin Xu", "Miaosen Zhang", "Ruiqi Yang", "Song Wang", "Kai Qiu", "Zhirong Wu", "Qi Dai", "Ruichun Ma", "Bei Liu", "Yifan Yang", "Chong Luo", "Zhengyuan Yang", "Linjie Li", "Lijuan Wang", "Weizhu Chen", "Xin Geng", "Baining Guo"], "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents", "comment": null, "summary": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.", "AI": {"tldr": "Re-TRAC\uff1a\u57fa\u4e8e\u4ea4\u53c9\u8f68\u8ff9\u63a2\u7d22\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u72b6\u6001\u8868\u793a\u5b9e\u73b0\u8fed\u4ee3\u53cd\u601d\u548c\u5168\u5c40\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u7814\u7a76\u6548\u7387", "motivation": "\u73b0\u6709\u57fa\u4e8eReAct\u6846\u67b6\u7684LLM\u7814\u7a76\u667a\u80fd\u4f53\u91c7\u7528\u7ebf\u6027\u8bbe\u8ba1\uff0c\u96be\u4ee5\u56de\u6eaf\u65e9\u671f\u72b6\u6001\u3001\u63a2\u7d22\u66ff\u4ee3\u65b9\u5411\u6216\u7ef4\u6301\u957f\u4e0a\u4e0b\u6587\u4e0b\u7684\u5168\u5c40\u610f\u8bc6\uff0c\u5bfc\u81f4\u5c40\u90e8\u6700\u4f18\u3001\u5197\u4f59\u63a2\u7d22\u548c\u4f4e\u6548\u641c\u7d22", "method": "\u63d0\u51faRe-TRAC\u6846\u67b6\uff0c\u5728\u6bcf\u4e2a\u8f68\u8ff9\u540e\u751f\u6210\u7ed3\u6784\u5316\u72b6\u6001\u8868\u793a\uff08\u603b\u7ed3\u8bc1\u636e\u3001\u4e0d\u786e\u5b9a\u6027\u3001\u5931\u8d25\u548c\u672a\u6765\u8ba1\u5212\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u72b6\u6001\u8868\u793a\u6307\u5bfc\u540e\u7eed\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4ea4\u53c9\u8f68\u8ff9\u63a2\u7d22\u548c\u8fed\u4ee3\u53cd\u601d", "result": "\u5728BrowseComp\u57fa\u51c6\u4e0a\uff0cRe-TRAC\u6bd4ReAct\u6301\u7eed\u63d0\u534715-20%\uff1b\u5bf9\u5c0f\u6a21\u578b\u5f15\u5165Re-TRAC\u611f\u77e5\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u5728\u53ef\u6bd4\u89c4\u6a21\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u5de5\u5177\u8c03\u7528\u548ctoken\u4f7f\u7528\u91cf\u968f\u8f6e\u6b21\u5355\u8c03\u51cf\u5c11", "conclusion": "Re-TRAC\u901a\u8fc7\u4ea4\u53c9\u8f68\u8ff9\u53cd\u601d\u548c\u7ed3\u6784\u5316\u72b6\u6001\u8868\u793a\uff0c\u5c06\u7814\u7a76\u91cd\u6784\u4e3a\u6e10\u8fdb\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u66f4\u6709\u9488\u5bf9\u6027\u7684\u63a2\u7d22\uff0c\u51cf\u5c11\u5197\u4f59\u641c\u7d22", "topic": "agent analysis"}}
{"id": "2602.02495", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02495", "abs": "https://arxiv.org/abs/2602.02495", "authors": ["Peter Chen", "Xiaopeng Li", "Xi Chen", "Tianyi Lin"], "title": "Reward-free Alignment for Conflicting Objectives", "comment": "27 pages", "summary": "Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.", "AI": {"tldr": "\u63d0\u51faRACO\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u5956\u52b1\u6a21\u578b\u7684\u65b9\u5f0f\u76f4\u63a5\u5229\u7528\u6210\u5bf9\u504f\u597d\u6570\u636e\u89e3\u51b3\u591a\u76ee\u6807\u51b2\u7a81\u5bf9\u9f50\u95ee\u9898\uff0c\u4f7f\u7528\u88c1\u526a\u7684\u51b2\u7a81\u89c4\u907f\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\uff0c\u5728\u591a\u4e2aLLM\u4e0a\u5b9e\u73b0\u66f4\u597d\u7684\u5e15\u7d2f\u6258\u6743\u8861\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\u901a\u5e38\u6d89\u53ca\u591a\u4e2a\u51b2\u7a81\u76ee\u6807\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u548c\u6743\u8861\u4e0d\u4f73\u7684\u95ee\u9898\u3002\u52a0\u6743\u635f\u5931\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u540c\u65f6\u6539\u8fdb\u6240\u6709\u76ee\u6807\uff0c\u800c\u73b0\u6709\u591a\u76ee\u6807\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u590d\u6742\u6027\u5e76\u626d\u66f2\u7528\u6237\u6307\u5b9a\u504f\u597d\u3002", "method": "\u63d0\u51faRACO\u6846\u67b6\uff1a1\uff09\u76f4\u63a5\u5229\u7528\u6210\u5bf9\u504f\u597d\u6570\u636e\uff0c\u65e0\u9700\u663e\u5f0f\u5956\u52b1\u6a21\u578b\uff1b2\uff09\u901a\u8fc7\u88c1\u526a\u7684\u51b2\u7a81\u89c4\u907f\u68af\u5ea6\u4e0b\u964d\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\uff1b3\uff09\u63d0\u4f9b\u6536\u655b\u5230\u5e15\u7d2f\u6258\u4e34\u754c\u70b9\u7684\u7406\u8bba\u4fdd\u8bc1\uff1b4\uff09\u5728\u53cc\u76ee\u6807\u8bbe\u7f6e\u4e2d\u8bc1\u660e\u88c1\u526a\u80fd\u4e25\u683c\u6539\u8fdb\u6536\u655b\u7387\u3002", "result": "\u5728\u591a\u4e2aLLM\u5bb6\u65cf\uff08Qwen 3\u3001Llama 3\u3001Gemma 3\uff09\u4e0a\u7684\u591a\u76ee\u6807\u6458\u8981\u548c\u5b89\u5168\u5bf9\u9f50\u4efb\u52a1\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u591a\u76ee\u6807\u5bf9\u9f50\u57fa\u7ebf\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u5e15\u7d2f\u6258\u6743\u8861\u3002", "conclusion": "RACO\u6846\u67b6\u4e3a\u591a\u76ee\u6807\u51b2\u7a81\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u7406\u8bba\u4fdd\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u663e\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u5b9e\u9645\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.01053", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01053", "abs": "https://arxiv.org/abs/2602.01053", "authors": ["Hyesung Jeon", "Hyeongju Ha", "Jae-Joon Kim"], "title": "LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents", "comment": "23 pages, 9 figures, 19 tables", "summary": "Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.", "AI": {"tldr": "LRAgent\u662f\u4e00\u4e2a\u7528\u4e8e\u591aLoRA\u4ee3\u7406\u7cfb\u7edf\u7684KV\u7f13\u5b58\u5171\u4eab\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7f13\u5b58\u5206\u89e3\u4e3a\u5171\u4eab\u57fa\u7840\u7ec4\u4ef6\u548c\u9002\u914d\u5668\u4f9d\u8d56\u7ec4\u4ef6\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u975e\u5171\u4eab\u7f13\u5b58\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u591aLoRA\u4ee3\u7406\u7cfb\u7edf\u4e2d\uff0c\u867d\u7136\u4ee3\u7406\u5171\u4eab\u9884\u8bad\u7ec3\u4e3b\u5e72\u7f51\u7edc\uff0c\u4f46\u6bcf\u4e2a\u4ee3\u7406\u72ec\u7acb\u6784\u5efa\u548c\u5b58\u50a8\u81ea\u5df1\u7684KV\u7f13\u5b58\uff0c\u5bfc\u81f4\u76f8\u540c\u5de5\u5177\u589e\u5f3a\u8f68\u8ff9\u7684\u91cd\u590d\u5b58\u50a8\u548c\u8ba1\u7b97\u5f00\u9500\u3002\u73b0\u6709KV\u7f13\u5b58\u5171\u4eab\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u4e86\u8fd9\u79cd\u591aLoRA\u8bbe\u7f6e\u3002", "method": "\u63d0\u51faLRAgent\u6846\u67b6\uff1a1) \u5c06KV\u7f13\u5b58\u5206\u89e3\u4e3a\u5171\u4eab\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u57fa\u7840\u7ec4\u4ef6\u548cLoRA\u6743\u91cd\u7684\u9002\u914d\u5668\u4f9d\u8d56\u7ec4\u4ef6\uff1b2) \u5171\u4eab\u57fa\u7840\u7ec4\u4ef6\u5e76\u4ee5\u4f4e\u79e9\u5f62\u5f0f\u5b58\u50a8\u9002\u914d\u5668\u7ec4\u4ef6\uff1b3) \u5f15\u5165Flash-LoRA-Attention\u5185\u6838\uff0c\u91cd\u65b0\u6392\u5e8f\u6ce8\u610f\u529b\u8ba1\u7b97\u4ee5\u907f\u514d\u5c06\u4f4e\u79e9\u7f13\u5b58\u7269\u5316\u4e3a\u5b8c\u6574\u7ef4\u5ea6\u3002", "result": "LRAgent\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u5168\u5171\u4eab\u7f13\u5b58\u7684\u541e\u5410\u91cf\u548c\u9996\u8bcd\u5ef6\u8fdf\uff0c\u540c\u65f6\u5728\u4ee3\u7406\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u4e86\u63a5\u8fd1\u975e\u5171\u4eab\u7f13\u5b58\u57fa\u7ebf\u7684\u51c6\u786e\u6027\u3002", "conclusion": "LRAgent\u901a\u8fc7\u6709\u6548\u5171\u4eabKV\u7f13\u5b58\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u591aLoRA\u4ee3\u7406\u7cfb\u7edf\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u89d2\u8272\u4e13\u4e1a\u5316\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7f13\u5b58\u5171\u4eab\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.01058", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01058", "abs": "https://arxiv.org/abs/2602.01058", "authors": ["Dylan Zhang", "Yufeng Xu", "Haojin Wang", "Qingzhi Chen", "Hao Peng"], "title": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning", "comment": null, "summary": "Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.\n  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.\n  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.\n  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.", "AI": {"tldr": "PEAR\u662f\u4e00\u79cdSFT\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u91cd\u65b0\u52a0\u6743SFT\u635f\u5931\uff0c\u89e3\u51b3SFT\u4e0eRL\u9636\u6bb5\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u5347\u540e\u7eedRL\u8bad\u7ec3\u6548\u679c", "motivation": "\u5f53\u524dLLM\u540e\u8bad\u7ec3\u4e2d\uff0cSFT\u9636\u6bb5\u901a\u5e38\u5b64\u7acb\u4f18\u5316\u4ee5\u6700\u5927\u5316SFT\u6027\u80fd\uff0c\u4f46\u66f4\u5f3a\u7684SFT\u68c0\u67e5\u70b9\u5728\u76f8\u540cRL\u8bad\u7ec3\u540e\u53ef\u80fd\u663e\u8457\u5f31\u4e8e\u8f83\u5f31\u7684\u68c0\u67e5\u70b9\uff0c\u8fd9\u6e90\u4e8e\u79bb\u7ebfSFT\u6570\u636e\u5206\u5e03\u4e0e\u5728\u7ebfRL\u7b56\u7565\u5206\u5e03\u4e0d\u5339\u914d", "method": "\u63d0\u51faPEAR\u65b9\u6cd5\uff0c\u4f7f\u7528\u91cd\u8981\u6027\u91c7\u6837\u91cd\u65b0\u52a0\u6743SFT\u635f\u5931\uff0c\u5305\u542btoken\u7ea7\u3001block\u7ea7\u548c\u5e8f\u5217\u7ea7\u4e09\u79cd\u53d8\u4f53\uff0c\u53ef\u589e\u5f3a\u6807\u51c6SFT\u76ee\u6807\uff0c\u5728\u6536\u96c6\u79bb\u7ebf\u6570\u636e\u6982\u7387\u540e\u589e\u52a0\u5f88\u5c11\u7684\u8bad\u7ec3\u5f00\u9500", "result": "\u5728\u53ef\u9a8c\u8bc1\u63a8\u7406\u6e38\u620f\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5bf9Qwen 2.5/3\u548cDeepSeek-distilled\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0cPEAR\u4e00\u81f4\u63d0\u5347\u540eRL\u6027\u80fd\uff0c\u5728AIME2025\u4e0apass@8\u63d0\u5347\u8fbe14.6%", "conclusion": "PEAR\u662f\u671d\u7740\u66f4\u6574\u4f53\u5316LLM\u540e\u8bad\u7ec3\u7684\u6709\u6548\u6b65\u9aa4\uff0c\u8bbe\u8ba1SFT\u65f6\u8003\u8651\u4e0b\u6e38RL\u800c\u975e\u5b64\u7acb\u4f18\u5316\uff0c\u80fd\u66f4\u597d\u5730\u4e3aRL\u9636\u6bb5\u51c6\u5907\u6a21\u578b", "topic": "agentic reinforcement learning"}}
{"id": "2602.01120", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01120", "abs": "https://arxiv.org/abs/2602.01120", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Tianyi Zeng", "Xiao-Yong Wei", "Qing Li"], "title": "MarkovScale: Towards Optimal Sequential Scaling at Inference Time", "comment": "12 pages", "summary": "Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u7684\u5e8f\u5217\u7f29\u653e\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5e8f\u5217\u7f29\u653e\u7684\u5185\u5728\u6027\u8d28\uff0c\u5e76\u5f00\u53d1\u4e86MarkovScale\u7cfb\u7edf\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5e8f\u5217\u7f29\u653e\u65b9\u6cd5\u591a\u4e3a\u542f\u53d1\u5f0f\u3001\u975e\u539f\u5219\u6027\u7684\uff0c\u5bfc\u81f4\u6027\u80fd\u6539\u8fdb\u6709\u9650\u4e14\u7f3a\u4e4f\u7406\u8bba\u7406\u89e3\uff0c\u9700\u8981\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u6765\u63ed\u793a\u5e8f\u5217\u7f29\u653e\u7684\u5185\u5728\u6027\u8d28\u5e76\u63d0\u4f9b\u6700\u4f18\u6027\u8fb9\u754c\u3002", "method": "\u5c06\u5e8f\u5217\u7f29\u653e\u5efa\u6a21\u4e3a\u4e24\u72b6\u6001\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u63a8\u5bfc\u51fa\u95ed\u5f0f\u89e3\u6765\u786e\u5b9a\u51c6\u786e\u7387\u63d0\u5347\u7684\u6761\u4ef6\u53ca\u7406\u8bba\u4e0a\u3001\u4e2d\u3001\u4e0b\u754c\u6027\u80fd\u8fb9\u754c\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1MarkovScale\u7cfb\u7edf\u5b9e\u73b0\u7406\u8bba\u6307\u5bfc\u7684\u51c6\u786e\u7387\u4e0e\u6548\u7387\u5e73\u8861\u3002", "result": "\u57283\u4e2a\u9aa8\u5e72LLM\u30015\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c20\u591a\u4e2a\u914d\u7f6e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMarkovScale\u5728\u51c6\u786e\u7387\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5e76\u884c\u548c\u5e8f\u5217\u7f29\u653e\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5e8f\u5217\u7f29\u653e\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6700\u4f18\u6027\u8fb9\u754c\uff0cMarkovScale\u7cfb\u7edf\u4ee3\u8868\u4e86\u5411LLM\u6700\u4f18\u4e14\u8d44\u6e90\u9ad8\u6548\u63a8\u7406\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2602.01137", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01137", "abs": "https://arxiv.org/abs/2602.01137", "authors": ["Shiguang Wu", "Yaqing Wang", "Quanming Yao"], "title": "Self-Generative Adversarial Fine-Tuning for Large Language Models", "comment": null, "summary": "Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.", "AI": {"tldr": "SGALM\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u751f\u6210\u5bf9\u6297LLM\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6a21\u578b\u5185\u7684\u751f\u6210\u5bf9\u6297\u6e38\u620f\u5b9e\u73b0\u5bf9\u9f50\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u6a21\u578b\uff0c\u5728\u6027\u80fd\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u9762\u8fbe\u5230SOTA\u3002", "motivation": "\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6216\u5f3a\u5316\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988\uff0c\u800c\u73b0\u6709\u7684\u81ea\u535a\u5f08\u548c\u5408\u6210\u6570\u636e\u65b9\u6cd5\u5b58\u5728\u542f\u53d1\u5f0f\u5047\u8bbe\u6216\u672a\u7ecf\u9a8c\u8bc1\u7684\u81ea\u6211\u8bc4\u4f30\u95ee\u9898\uff0c\u5bb9\u6613\u5bfc\u81f4\u504f\u89c1\u7d2f\u79ef\u548c\u6027\u80fd\u6f02\u79fb\u3002", "method": "SGALM\u6846\u67b6\u5c06\u5bf9\u9f50\u95ee\u9898\u6784\u5efa\u4e3a\u5355LLM\u5185\u90e8\u7684\u751f\u6210\u5bf9\u6297\u6e38\u620f\uff0c\u8054\u5408\u8fdb\u5316\u751f\u6210\u548c\u5224\u522b\u80fd\u529b\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "SGALM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\uff0c\u65e2\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u5bf9\u9f50\u7b97\u6cd5\uff0c\u4e5f\u80fd\u4f5c\u4e3a\u9c81\u68d2\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u5f15\u64ce\u3002", "conclusion": "SGALM\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u90e8\u751f\u6210\u5bf9\u6297\u673a\u5236\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u9ad8\u8d28\u91cf\u6807\u6ce8\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01523", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01523", "abs": "https://arxiv.org/abs/2602.01523", "authors": ["Akifumi Wachi", "Hirota Kinoshita", "Shokichi Takakura", "Rei Higuchi", "Taiji Suzuki"], "title": "A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning", "comment": "28 pages", "summary": "Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \\emph{relative-budget} theory explaining this variation through a single quantity called relative budget $\u03be:= H/\\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $\u03be$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \\emph{deficient} regime ($\u03be\\to 0$), informative trajectories are rare and the sample complexity explodes; in the \\emph{balanced} regime ($\u03be=\u0398(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \\emph{ample} regime ($\u03be\\to \\infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $\u03be\\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u76f8\u5bf9\u9884\u7b97\u7406\u8bba\u6765\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u8ba1\u7b97\u9884\u7b97\u4e0b\u7684\u6548\u679c\u5dee\u5f02\uff0c\u901a\u8fc7\u76f8\u5bf9\u9884\u7b97\u03be=H/E[T]\u8fd9\u4e00\u5355\u4e00\u91cf\u6765\u9884\u6d4bRL\u6837\u672c\u6548\u7387\uff0c\u63ed\u793a\u4e86\u4e09\u4e2a\u4e0d\u540c\u5b66\u4e60\u673a\u5236\u53ca\u5176\u5bf9\u63a8\u7406\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u662f\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u4e3b\u8981\u8303\u5f0f\uff0c\u4f46\u5176\u6548\u679c\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u8ba1\u7b97\u9884\u7b97\u4e0b\u5dee\u5f02\u5f88\u5927\u3002\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u8fd9\u79cd\u5dee\u5f02\uff0c\u5e76\u6307\u5bfc\u5982\u4f55\u6709\u6548\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u6765\u6700\u5927\u5316\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51fa\u76f8\u5bf9\u9884\u7b97\u7406\u8bba\uff0c\u5b9a\u4e49\u76f8\u5bf9\u9884\u7b97\u03be=H/E[T]\uff0c\u5176\u4e2dH\u662f\u751f\u6210\u89c6\u91ce\uff08token\u9884\u7b97\uff09\uff0cT\u662f\u57fa\u7840\u7b56\u7565\u4e0b\u9996\u6b21\u5f97\u5230\u6b63\u786e\u89e3\u6240\u9700\u7684token\u6570\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u4e09\u4e2a\u5b66\u4e60\u673a\u5236\uff1a\u4e0d\u8db3\u673a\u5236(\u03be\u21920)\u3001\u5e73\u8861\u673a\u5236(\u03be=\u0398(1))\u548c\u5145\u8db3\u673a\u5236(\u03be\u2192\u221e)\u3002\u63d0\u4f9b\u4e86\u5728\u7ebfRL\u7684\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u5e76\u5728\u7406\u60f3\u5206\u5e03\u5047\u8bbe\u4e0b\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff1a\u5728\u4e0d\u8db3\u673a\u5236\u4e0b\uff0c\u4fe1\u606f\u8f68\u8ff9\u7a00\u5c11\uff0c\u6837\u672c\u590d\u6742\u5ea6\u7206\u70b8\uff1b\u5728\u5e73\u8861\u673a\u5236\u4e0b\uff0c\u4fe1\u606f\u8f68\u8ff9\u4ee5\u975e\u53ef\u5ffd\u7565\u6982\u7387\u51fa\u73b0\uff0cRL\u8fbe\u5230\u6700\u5927\u6837\u672c\u6548\u7387\uff1b\u5728\u5145\u8db3\u673a\u5236\u4e0b\uff0c\u5b66\u4e60\u4fdd\u6301\u7a33\u5b9a\u4f46\u6bcf\u6b21\u8fed\u4ee3\u7684\u8fb9\u9645\u6536\u76ca\u9012\u51cf\u3002\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u03be\u2208[1.5, 2.0]\u7684\u9884\u7b97\u8303\u56f4\u80fd\u6700\u5927\u5316\u5b66\u4e60\u6548\u7387\uff0c\u5e76\u4e0e\u5cf0\u503c\u63a8\u7406\u6027\u80fd\u4e00\u81f4\u3002", "conclusion": "\u76f8\u5bf9\u9884\u7b97\u03be\u662f\u4e00\u4e2a\u5173\u952e\u91cf\uff0c\u80fd\u591f\u9884\u6d4bRL\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u8ba1\u7b97\u9884\u7b97\u4e0b\u7684\u6837\u672c\u6548\u7387\u3002\u8be5\u7406\u8bba\u4e3aRL\u5728\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u5e94\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e2e\u52a9\u786e\u5b9a\u6700\u4f18\u7684\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01601", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.01601", "abs": "https://arxiv.org/abs/2602.01601", "authors": ["Hieu Trung Nguyen", "Bao Nguyen", "Wenao Ma", "Yuzhi Zhao", "Ruifeng She", "Viet Anh Nguyen"], "title": "Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards", "comment": "Accepted at ICLR 2026", "summary": "Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \\Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \\Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \\Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.", "AI": {"tldr": "VIP\u662f\u4e00\u79cd\u57fa\u4e8e\u65b9\u5dee\u611f\u77e5\u7684\u9884\u6d4b\u6027\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u9884\u6d4b\u6bcf\u4e2a\u63d0\u793a\u7684\u6210\u529f\u6982\u7387\uff0c\u5e76\u4f18\u5316\u5206\u914d\u8ba1\u7b97\u9884\u7b97\u4ee5\u6700\u5c0f\u5316\u7b56\u7565\u66f4\u65b0\u7684\u68af\u5ea6\u65b9\u5dee\uff0c\u4ece\u800c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7684\u91c7\u6837\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7fa4\u4f53\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5bf9\u6240\u6709\u8bad\u7ec3\u63d0\u793a\u5206\u914d\u56fa\u5b9a\u6570\u91cf\u7684rollout\uff0c\u8fd9\u79cd\u5747\u5300\u5206\u914d\u9690\u542b\u5730\u8ba4\u4e3a\u6240\u6709\u63d0\u793a\u5177\u6709\u540c\u7b49\u4fe1\u606f\u4ef7\u503c\uff0c\u53ef\u80fd\u5bfc\u81f4\u8ba1\u7b97\u9884\u7b97\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u5e76\u963b\u788d\u8bad\u7ec3\u8fdb\u5c55\u3002", "method": "VIP\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u57fa\u4e8e\u6700\u8fd1\u7684rollout\u9884\u6d4b\u6bcf\u4e2a\u63d0\u793a\u7684\u6210\u529f\u6982\u7387\uff0c\u5c06\u8fd9\u4e9b\u6982\u7387\u9884\u6d4b\u8f6c\u5316\u4e3a\u65b9\u5dee\u4f30\u8ba1\uff0c\u7136\u540e\u901a\u8fc7\u51f8\u4f18\u5316\u95ee\u9898\u5728\u786c\u8ba1\u7b97\u9884\u7b97\u7ea6\u675f\u4e0b\u786e\u5b9a\u6700\u4f18\u7684rollout\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVIP\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u9ad8\u91c7\u6837\u6548\u7387\uff0c\u5e76\u6bd4\u5747\u5300\u5206\u914d\u6216\u542f\u53d1\u5f0f\u5206\u914d\u7b56\u7565\u83b7\u5f97\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "conclusion": "VIP\u901a\u8fc7\u65b9\u5dee\u611f\u77e5\u7684\u9884\u6d4b\u6027\u5206\u914d\u7b56\u7565\uff0c\u6709\u6548\u5730\u4f18\u5316\u4e86\u8ba1\u7b97\u9884\u7b97\u7684\u4f7f\u7528\uff0c\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u573a\u666f\u4e0b\u7684\u91c7\u6837\u6548\u7387\u548c\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01260", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01260", "abs": "https://arxiv.org/abs/2602.01260", "authors": ["Soumyadeep Roy", "Shashwat Kushwaha", "Ambedkar Dukkipati"], "title": "Sample Efficient Active Algorithms for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $\u03b5$-optimal policy can be learned with ${\\mathcal{O}}(1/\u03b5^2)$ active transitions, improving upon the $\u03a9(1/\u03b5^2(1-\u03b3)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u9650\u5728\u7ebf\u4ea4\u4e92\u9009\u62e9\u6027\u4f18\u5316\u4ef7\u503c\u51fd\u6570\u7684\u4e0d\u786e\u5b9a\u533a\u57df\uff0c\u5b9e\u73b0\u4e86\u6bd4\u7eaf\u79bb\u7ebf\u65b9\u6cd5\u66f4\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u8986\u76d6\u4e0d\u8db3\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u9700\u8981\u6709\u9650\u5728\u7ebf\u4ea4\u4e92\u6765\u9009\u62e9\u6027\u4f18\u5316\u4e0d\u786e\u5b9a\u533a\u57df\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u5206\u6790", "method": "\u63d0\u51fa\u4e3b\u52a8\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u7ed3\u5408GP\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u548c\u4fe1\u606f\u589e\u76ca\u8fb9\u754c\u8fdb\u884c\u7406\u8bba\u5206\u6790", "result": "\u8bc1\u660e\u4e86\u03b5\u6700\u4f18\u7b56\u7565\u53ef\u4ee5\u901a\u8fc7O(1/\u03b5\u00b2)\u4e3b\u52a8\u8f6c\u79fb\u5b66\u4e60\uff0c\u4f18\u4e8e\u7eaf\u79bb\u7ebf\u65b9\u6cd5\u7684\u03a9(1/\u03b5\u00b2(1-\u03b3)\u2074)\u901f\u7387\uff0c\u5b9e\u73b0\u8fd1\u6700\u4f18\u4fe1\u606f\u6548\u7387", "conclusion": "\u4e3b\u52a8\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u5f15\u5bfc\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u5b9e\u73b0\u4e86\u52a0\u901f\u4ef7\u503c\u51fd\u6570\u6536\u655b\uff0c\u5728\u6700\u5c0f\u5728\u7ebf\u6570\u636e\u4e0b\u8fbe\u5230\u8fd1\u6700\u4f18\u4fe1\u606f\u6548\u7387", "topic": "agentic reinforcement learning"}}
{"id": "2602.02103", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02103", "abs": "https://arxiv.org/abs/2602.02103", "authors": ["Liyan Xu", "Mo Yu", "Fandong Meng", "Jie Zhou"], "title": "No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs", "comment": null, "summary": "This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7Tele-Lens\u63a2\u6d4b\u65b9\u6cd5\u7814\u7a76LLM\u7684\u5185\u90e8\u72b6\u6001\u4e0e\u663e\u5f0f\u63a8\u7406\u8f68\u8ff9\u7684\u5173\u7cfb\uff0c\u53d1\u73b0LLM\u5177\u6709\u77ed\u89c6\u6027\uff0c\u4e3b\u8981\u8fdb\u884c\u589e\u91cf\u63a8\u7406\u800c\u975e\u5168\u5c40\u89c4\u5212\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u589e\u5f3aCoT\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u65b9\u6cd5\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u53d1\u73b0LLM\u5728CoT\u51fa\u73b0\u524d\u5df2\u6709\u6f5c\u5728\u89c4\u5212\uff0c\u8fd9\u524a\u5f31\u4e86\u663e\u5f0fCoT\u7684\u91cd\u8981\u6027\uff0c\u4f46CoT\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u7136\u5173\u952e\u3002\u4e3a\u4e86\u6df1\u5165\u7406\u89e3LLM\u5185\u90e8\u72b6\u6001\u4e0e\u5176\u8a00\u8bed\u5316\u63a8\u7406\u8f68\u8ff9\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u9700\u8981\u7814\u7a76LLM\u7684\u6f5c\u5728\u89c4\u5212\u80fd\u529b\u3002", "method": "\u63d0\u51faTele-Lens\u63a2\u6d4b\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u4e0d\u540c\u4efb\u52a1\u9886\u57df\u7684\u9690\u85cf\u72b6\u6001\uff0c\u7814\u7a76LLM\u7684\u6f5c\u5728\u89c4\u5212\u5f3a\u5ea6\u3002\u57fa\u4e8e\u53d1\u73b0\u7684\u77ed\u89c6\u6027\u7279\u5f81\uff0c\u63d0\u51fa\u589e\u5f3aCoT\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u5047\u8bbe\uff0c\u5e76\u9a8c\u8bc1\u5c11\u91cfCoT\u4f4d\u7f6e\u80fd\u6709\u6548\u4ee3\u8868\u6574\u4e2a\u8def\u5f84\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660eLLM\u8868\u73b0\u51fa\u77ed\u89c6\u6027\uff0c\u4e3b\u8981\u8fdb\u884c\u589e\u91cf\u8f6c\u6362\u800c\u975e\u7cbe\u786e\u7684\u5168\u5c40\u89c4\u5212\u3002\u9a8c\u8bc1\u4e86\u5c11\u91cfCoT\u4f4d\u7f6e\u80fd\u6709\u6548\u4ee3\u8868\u6574\u4e2a\u8def\u5f84\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u52a8\u8bc6\u522bCoT\u7ed5\u8fc7\u7684\u53ef\u884c\u6027\u4e14\u4e0d\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u5229\u7528CoT\u52a8\u6001\u7279\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0cLLM\u7684\u77ed\u89c6\u6027\u7279\u5f81\u53ef\u7528\u4e8e\u589e\u5f3a\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u81ea\u52a8\u8bc6\u522bCoT\u7ed5\u8fc7\u662f\u53ef\u884c\u7684\u3002\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2602.01270", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01270", "abs": "https://arxiv.org/abs/2602.01270", "authors": ["Boxuan Zhang", "Weipu Zhang", "Zhaohan Feng", "Wei Xiao", "Jian Sun", "Jie Chen", "Gang Wang"], "title": "Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics", "comment": null, "summary": "A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.", "AI": {"tldr": "MoW\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u4e16\u754c\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u89c6\u89c9\u538b\u7f29\u3001\u6df7\u5408Transformer\u52a8\u6001\u6a21\u578b\u548c\u68af\u5ea6\u805a\u7c7b\u7b56\u7565\uff0c\u5728Atari\u548cMeta-World\u4e0a\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u7684\u9ad8\u6027\u80fd\u3002", "motivation": "\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u5728\u89c6\u89c9\u9886\u57df\u4e2d\u9762\u4e34\u6837\u672c\u6548\u7387\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u4efb\u52a1\u5728\u89c2\u5bdf\u548c\u52a8\u6001\u4e0a\u5b58\u5728\u663e\u8457\u5f02\u8d28\u6027\u65f6\u3002\u4f20\u7edf\u7684\u5355\u4e00\u4e16\u754c\u6a21\u578b\u67b6\u6784\u96be\u4ee5\u6355\u6349\u591a\u6837\u5316\u7684\u4efb\u52a1\u52a8\u6001\uff0c\u5bfc\u81f4\u91cd\u5efa\u548c\u9884\u6d4b\u51c6\u786e\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u6df7\u5408\u4e16\u754c\u6a21\u578b\uff08MoW\uff09\u67b6\u6784\uff1a1\uff09\u6a21\u5757\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7528\u4e8e\u4efb\u52a1\u81ea\u9002\u5e94\u89c6\u89c9\u538b\u7f29\uff1b2\uff09\u6df7\u5408Transformer\u52a8\u6001\u6a21\u578b\uff0c\u5305\u542b\u4efb\u52a1\u6761\u4ef6\u4e13\u5bb6\u548c\u5171\u4eab\u9aa8\u5e72\uff1b3\uff09\u57fa\u4e8e\u68af\u5ea6\u7684\u4efb\u52a1\u805a\u7c7b\u7b56\u7565\uff0c\u7528\u4e8e\u9ad8\u6548\u53c2\u6570\u5206\u914d\u3002", "result": "\u5728Atari 100k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5355\u4e2aMoW\u4ee3\u7406\u572826\u4e2aAtari\u6e38\u620f\u4e0a\u8bad\u7ec3\u4e00\u6b21\uff0c\u83b7\u5f97110.4%\u7684\u5e73\u5747\u4eba\u7c7b\u6807\u51c6\u5316\u5206\u6570\uff0c\u4e0e\u9700\u898126\u4e2a\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u7684STROM\uff08114.2%\uff09\u76f8\u5f53\uff0c\u4f46\u53c2\u6570\u51cf\u5c1150%\u3002\u5728Meta-World\u4e0a\uff0cMoW\u572830\u4e07\u73af\u5883\u6b65\u5185\u8fbe\u523074.5%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "conclusion": "MoW\u4e3a\u901a\u7528\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53c2\u6570\u9ad8\u6548\u7684\u57fa\u7840\u67b6\u6784\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u89c2\u5bdf\u548c\u52a8\u6001\u7684\u5f02\u8d28\u6027\u6311\u6218\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01271", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01271", "abs": "https://arxiv.org/abs/2602.01271", "authors": ["Burak Demirel", "Pablo Soldati", "Yu Wang"], "title": "From Intents to Actions: Agentic AI in Autonomous Networks", "comment": null, "summary": "Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e09\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u7684AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u610f\u56fe\u9a71\u52a8\u7684\u81ea\u6cbb\u7f51\u7edc\uff1a\u76d1\u7763\u89e3\u91ca\u5668\u5c06\u9ad8\u5c42\u610f\u56fe\u89e3\u6790\u4e3a\u53ef\u6267\u884c\u4f18\u5316\u6a21\u677f\uff1b\u4f18\u5316\u5668\u5c06\u6a21\u677f\u8f6c\u6362\u4e3a\u4f18\u5316\u95ee\u9898\u5e76\u5206\u6790\u6743\u8861\uff1b\u504f\u597d\u9a71\u52a8\u63a7\u5236\u5668\u57fa\u4e8e\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u504f\u597d\u64cd\u4f5c\u7f51\u7edc\u63a5\u8fd1\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "motivation": "\u7535\u4fe1\u7f51\u7edc\u9700\u8981\u81ea\u4e3b\u8fd0\u884c\u5e76\u652f\u6301\u5177\u6709\u591a\u6837\u4e14\u7ecf\u5e38\u51b2\u7a81\u610f\u56fe\u7684\u5f02\u6784\u670d\u52a1\uff0c\u4f46\u5c06\u9ad8\u5c42\u610f\u56fe\uff08\u5982\u8d85\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u91cf\u6216\u80fd\u6548\uff09\u8f6c\u5316\u4e3a\u5177\u4f53\u63a7\u5236\u52a8\u4f5c\u8d85\u51fa\u4e86\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e09\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u7cfb\u7edf\uff1a1) \u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u76d1\u7763\u89e3\u91ca\u5668\uff0c\u6267\u884c\u610f\u56fe\u7684\u8bcd\u6c47\u89e3\u6790\u548c\u8ba4\u77e5\u7ec6\u5316\uff1b2) \u4f18\u5316\u5668\uff0c\u5c06\u6a21\u677f\u8f6c\u6362\u4e3a\u53ef\u5904\u7406\u7684\u4f18\u5316\u95ee\u9898\u5e76\u5206\u6790\u6743\u8861\uff1b3) \u57fa\u4e8e\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7684\u504f\u597d\u9a71\u52a8\u63a7\u5236\u5668\uff0c\u5229\u7528\u504f\u597d\u64cd\u4f5c\u7f51\u7edc\u63a5\u8fd1\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "result": "\u8be5\u7cfb\u7edf\u4f7f\u7f51\u7edc\u80fd\u591f\u4ee5\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\u81ea\u4e3b\u89e3\u91ca\u3001\u63a8\u7406\u3001\u9002\u5e94\u548c\u54cd\u5e94\u591a\u6837\u610f\u56fe\u548c\u7f51\u7edc\u6761\u4ef6\uff0c\u5b9e\u73b0\u610f\u56fe\u9a71\u52a8\u7684\u81ea\u6cbb\u7f51\u7edc\u3002", "conclusion": "\u63d0\u51fa\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\u901a\u8fc7\u4e09\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u89e3\u51b3\u4e86\u610f\u56fe\u9a71\u52a8\u81ea\u6cbb\u7f51\u7edc\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u4ece\u9ad8\u5c42\u610f\u56fe\u5230\u5177\u4f53\u63a7\u5236\u52a8\u4f5c\u7684\u81ea\u4e3b\u8f6c\u6362\u3002", "topic": "agent analysis"}}
{"id": "2602.02143", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02143", "abs": "https://arxiv.org/abs/2602.02143", "authors": ["Shubham Toshniwal", "Aleksander Ficek", "Siddhartha Jain", "Wei Du", "Vahid Noroozi", "Sadegh Mahdavi", "Somshubra Majumdar", "Igor Gitman"], "title": "Learning Generative Selection for Best-of-N", "comment": null, "summary": "Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.", "AI": {"tldr": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u83b7\u5f97\u5f3a\u5927\u7684\u751f\u6210\u9009\u62e9\u80fd\u529b\uff0c\u5728\u6570\u5b66\u548c\u4ee3\u7801\u63a8\u7406\u57fa\u51c6\u4e0a\u8d85\u8d8a\u63d0\u793a\u548c\u591a\u6570\u6295\u7968\u57fa\u7ebf\uff0c\u63a5\u8fd1\u6216\u8d85\u8fc7\u66f4\u5927\u6a21\u578b\u6027\u80fd", "motivation": "\u901a\u8fc7\u5e76\u884c\u91c7\u6837\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u53d7\u9650\u4e8eBest-of-N\u9009\u62e9\u8d28\u91cf\u3002\u751f\u6210\u9009\u62e9\u65b9\u6cd5\u5982GenSelect\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\uff0c\u4f46\u5f3a\u5927\u7684\u9009\u62e9\u6027\u80fd\u4e3b\u8981\u5c40\u9650\u4e8e\u5927\u6a21\u578b\u3002\u7814\u7a76\u5982\u4f55\u8ba9\u5c0f\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u83b7\u5f97\u5f3a\u5927\u7684\u751f\u6210\u9009\u62e9\u80fd\u529b\u3002", "method": "\u4ece\u5927\u89c4\u6a21\u6570\u5b66\u548c\u4ee3\u7801\u6307\u4ee4\u6570\u636e\u96c6\u4e2d\u5408\u6210\u9009\u62e9\u4efb\u52a1\uff0c\u7b5b\u9009\u5305\u542b\u6b63\u786e\u548c\u9519\u8bef\u5019\u9009\u89e3\u51b3\u65b9\u6848\u7684\u5b9e\u4f8b\uff0c\u4f7f\u7528DAPO\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec31.7B\u53c2\u6570\u6a21\u578b\uff0c\u5956\u52b1\u6b63\u786e\u7684\u9009\u62e9\u51b3\u7b56\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\uff08AIME24\u3001AIME25\u3001HMMT25\uff09\u548c\u4ee3\u7801\u63a8\u7406\u57fa\u51c6\uff08LiveCodeBench\uff09\u4e0a\uff0c\u6a21\u578b\u4e00\u81f4\u4f18\u4e8e\u63d0\u793a\u548c\u591a\u6570\u6295\u7968\u57fa\u7ebf\uff0c\u7ecf\u5e38\u63a5\u8fd1\u6216\u8d85\u8fc7\u66f4\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002\u8fd9\u4e9b\u589e\u76ca\u53ef\u4ee5\u6cdb\u5316\u5230\u9009\u62e9\u66f4\u5f3a\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5c3d\u7ba1\u8bad\u7ec3\u65f6\u53ea\u4f7f\u7528\u4e86\u8f83\u5f31\u6a21\u578b\u7684\u8f93\u51fa\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u662f\u89e3\u9501\u5c0f\u578b\u6a21\u578b\u4e2d\u5f3a\u5927\u751f\u6210\u9009\u62e9\u80fd\u529b\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01288", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01288", "abs": "https://arxiv.org/abs/2602.01288", "authors": ["Chenghua Zhu", "Siyan Wu", "Xiangkang Zeng", "Zishan Xu", "Zhaolu Kang", "Yifu Guo", "Yuquan Lu", "Junduan Huang", "Guojing Zhou"], "title": "EDIS: Diagnosing LLM Reasoning via Entropy Dynamics", "comment": "Under review at ICML 2026", "summary": "Entropy-based confidence signals are increasingly leveraged to improve reasoning in large language models (LLMs), yet existing approaches treat confidence as a static quantity -- typically aggregated over tokens. We show that the \\emph{temporal evolution} of confidence during generation carries richer information than aggregate statistics alone. Analyzing token-level entropy trajectories, we identify characteristic patterns distinguishing correct from incorrect reasoning: erroneous solutions exhibit unstable dynamics, including burst spikes (sustained uncertainty growth) and peak-valley spikes (sharp rebounds following transient confidence). These patterns persist across models and training stages, suggesting they reflect intrinsic properties of reasoning failure rather than superficial noise. To formalize this observation, we introduce the Entropy Dynamics Instability Score (\\textbf{EDIS}), a trajectory-level metric quantifying instability in entropy evolution. EDIS serves as an effective diagnostic signal for inference-time selection, substantially improving reasoning accuracy, and offers a promising direction for training-time sample curation. Our findings establish entropy dynamics as an underexplored yet informative lens for understanding and improving LLM reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5206\u6790LLM\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u71b5\u52a8\u6001\uff08\u800c\u975e\u9759\u6001\u805a\u5408\u7edf\u8ba1\uff09\u6765\u6539\u8fdb\u63a8\u7406\uff0c\u5f15\u5165\u71b5\u52a8\u6001\u4e0d\u7a33\u5b9a\u6027\u8bc4\u5206\uff08EDIS\uff09\u4f5c\u4e3a\u8bca\u65ad\u4fe1\u53f7\uff0c\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u7f6e\u4fe1\u5ea6\u89c6\u4e3a\u9759\u6001\u91cf\uff08\u901a\u5e38\u5728token\u4e0a\u805a\u5408\uff09\uff0c\u4f46\u751f\u6210\u8fc7\u7a0b\u4e2d\u7f6e\u4fe1\u5ea6\u7684\u65f6\u5e8f\u6f14\u5316\u5305\u542b\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\u3002\u4f5c\u8005\u53d1\u73b0\u6b63\u786e\u4e0e\u9519\u8bef\u63a8\u7406\u5728\u71b5\u8f68\u8ff9\u4e0a\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u7279\u5f81\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u53cd\u6620\u4e86\u63a8\u7406\u5931\u8d25\u7684\u5185\u5728\u5c5e\u6027\u800c\u975e\u8868\u9762\u566a\u58f0\u3002", "method": "\u5206\u6790token\u7ea7\u71b5\u8f68\u8ff9\uff0c\u8bc6\u522b\u6b63\u786e\u4e0e\u9519\u8bef\u63a8\u7406\u7684\u7279\u5f81\u6a21\u5f0f\uff08\u5982\u4e0d\u7a33\u5b9a\u52a8\u6001\u3001\u7a81\u53d1\u5c16\u5cf0\u3001\u5cf0\u8c37\u5c16\u5cf0\uff09\u3002\u5f15\u5165\u71b5\u52a8\u6001\u4e0d\u7a33\u5b9a\u6027\u8bc4\u5206\uff08EDIS\uff09\u4f5c\u4e3a\u8f68\u8ff9\u7ea7\u5ea6\u91cf\uff0c\u91cf\u5316\u71b5\u6f14\u5316\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u3002EDIS\u53ef\u7528\u4e8e\u63a8\u7406\u65f6\u9009\u62e9\u548c\u8bad\u7ec3\u65f6\u6837\u672c\u7b5b\u9009\u3002", "result": "\u9519\u8bef\u89e3\u51b3\u65b9\u6848\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u52a8\u6001\uff0c\u5305\u62ec\u7a81\u53d1\u5c16\u5cf0\uff08\u6301\u7eed\u4e0d\u786e\u5b9a\u6027\u589e\u957f\uff09\u548c\u5cf0\u8c37\u5c16\u5cf0\uff08\u77ed\u6682\u7f6e\u4fe1\u540e\u6025\u5267\u53cd\u5f39\uff09\u3002\u8fd9\u4e9b\u6a21\u5f0f\u5728\u4e0d\u540c\u6a21\u578b\u548c\u8bad\u7ec3\u9636\u6bb5\u6301\u7eed\u5b58\u5728\u3002EDIS\u4f5c\u4e3a\u6709\u6548\u7684\u8bca\u65ad\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "\u71b5\u52a8\u6001\u662f\u7406\u89e3\u548c\u6539\u8fdbLLM\u63a8\u7406\u7684\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u89c6\u89d2\u3002EDIS\u4e3a\u63a8\u7406\u65f6\u9009\u62e9\u548c\u8bad\u7ec3\u65f6\u6837\u672c\u7b5b\u9009\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u5931\u8d25\u7684\u5185\u5728\u5c5e\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.02244", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02244", "abs": "https://arxiv.org/abs/2602.02244", "authors": ["Hao Wang", "Hao Gu", "Hongming Piao", "Kaixiong Gong", "Yuxiao Ye", "Xiangyu Yue", "Sirui Han", "Yike Guo", "Dapeng Wu"], "title": "Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models", "comment": null, "summary": "The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.", "AI": {"tldr": "CurioSFT\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u6301\u71b5\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5185\u5728\u597d\u5947\u5fc3\u589e\u5f3a\u63a2\u7d22\u80fd\u529b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edfSFT\uff0c\u5e76\u4e3a\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u5e26\u6765\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfSFT-then-RL\u6d41\u7a0b\u4e2d\uff0cSFT\u9636\u6bb5\u6a21\u4eff\u4e13\u5bb6\u6f14\u793a\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u548c\u751f\u6210\u591a\u6837\u6027\u964d\u4f4e\uff0c\u9650\u5236\u4e86RL\u9636\u6bb5\u7684\u63a2\u7d22\u7a7a\u95f4\u3002\u73b0\u6709\u7684\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\u867d\u7136\u589e\u52a0\u71b5\uff0c\u4f46\u65e0\u6cd5\u771f\u6b63\u63d0\u5347\u63a2\u7d22\u80fd\u529b\u3002", "method": "CurioSFT\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u81ea\u63a2\u7d22\u84b8\u998f\uff1a\u5c06\u6a21\u578b\u84b8\u998f\u5230\u81ea\u751f\u6210\u7684\u6e29\u5ea6\u7f29\u653e\u6559\u5e08\u6a21\u578b\uff0c\u9f13\u52b1\u5728\u80fd\u529b\u8303\u56f4\u5185\u63a2\u7d22\uff1b(2) \u71b5\u5f15\u5bfc\u6e29\u5ea6\u9009\u62e9\uff1a\u81ea\u9002\u5e94\u8c03\u6574\u84b8\u998f\u5f3a\u5ea6\uff0c\u5728\u63a8\u7406\u6807\u8bb0\u5904\u589e\u5f3a\u63a2\u7d22\uff0c\u5728\u4e8b\u5b9e\u6807\u8bb0\u5904\u4fdd\u6301\u7a33\u5b9a\uff0c\u51cf\u8f7b\u77e5\u8bc6\u9057\u5fd8\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cCurioSFT\u5728SFT\u9636\u6bb5\u6bd4\u4f20\u7edfSFT\u5728\u5206\u5e03\u5185\u4efb\u52a1\u63d0\u53472.5\u5206\uff0c\u5206\u5e03\u5916\u4efb\u52a1\u63d0\u53472.9\u5206\u3002\u4fdd\u7559\u7684\u63a2\u7d22\u80fd\u529b\u5728RL\u9636\u6bb5\u8f6c\u5316\u4e3a5.0\u5206\u7684\u5e73\u5747\u63d0\u5347\u3002", "conclusion": "CurioSFT\u901a\u8fc7\u4fdd\u6301\u71b5\u548c\u589e\u5f3a\u5185\u5728\u597d\u5947\u5fc3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfSFT\u7684\u9650\u5236\uff0c\u4e3a\u540e\u7eedRL\u9636\u6bb5\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u63a2\u7d22\u57fa\u7840\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02488", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02488", "abs": "https://arxiv.org/abs/2602.02488", "authors": ["Yinjie Wang", "Tianbao Xie", "Ke Shen", "Mengdi Wang", "Ling Yang"], "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System", "comment": "Code: https://github.com/Gen-Verse/Open-AgentRL", "summary": "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL", "AI": {"tldr": "RLAnything\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u95ed\u73af\u4f18\u5316\u52a8\u6001\u6784\u5efa\u73af\u5883\u3001\u7b56\u7565\u548c\u5956\u52b1\u6a21\u578b\uff0c\u589e\u5f3aLLM\u548c\u667a\u80fd\u4f53\u573a\u666f\u7684\u5b66\u4e60\u4fe1\u53f7\u548c\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u5728LLM\u548c\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u9762\u4e34\u5b66\u4e60\u4fe1\u53f7\u5f31\u3001\u7cfb\u7edf\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u4f18\u5316\u73af\u5883\u3001\u7b56\u7565\u548c\u5956\u52b1\u6a21\u578b\u7684\u7efc\u5408\u6846\u67b6\u6765\u63d0\u5347\u6574\u4f53\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51faRLAnything\u6846\u67b6\uff1a1) \u7b56\u7565\u901a\u8fc7\u6574\u5408\u6b65\u8fdb\u4fe1\u53f7\u548c\u7ed3\u679c\u4fe1\u53f7\u7684\u53cd\u9988\u8fdb\u884c\u8bad\u7ec3\uff1b2) \u5956\u52b1\u6a21\u578b\u901a\u8fc7\u4e00\u81f4\u6027\u53cd\u9988\u8054\u5408\u4f18\u5316\uff1b3) \u57fa\u4e8e\u7406\u8bba\u7684\u81ea\u52a8\u73af\u5883\u9002\u5e94\u5229\u7528\u6279\u8bc4\u53cd\u9988\u6539\u8fdb\u5956\u52b1\u548c\u7b56\u7565\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u6301\u7eed\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff1aQwen3-VL-8B-Thinking\u5728OSWorld\u4e0a\u63d0\u53479.1%\uff1bQwen2.5-7B-Instruct\u5728AlfWorld\u548cLiveBench\u4e0a\u5206\u522b\u63d0\u534718.7%\u548c11.9%\u3002\u4f18\u5316\u7684\u5956\u52b1\u6a21\u578b\u4fe1\u53f7\u4f18\u4e8e\u4f9d\u8d56\u4eba\u5de5\u6807\u7b7e\u7684\u7ed3\u679c\u3002", "conclusion": "RLAnything\u901a\u8fc7\u95ed\u73af\u4f18\u5316\u73af\u5883\u3001\u7b56\u7565\u548c\u5956\u52b1\u6a21\u578b\uff0c\u663e\u8457\u589e\u5f3a\u4e86LLM\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u7cfb\u7edf\u4f18\u5316\u7684\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01439", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01439", "abs": "https://arxiv.org/abs/2602.01439", "authors": ["Perry Dong", "Kuo-Han Hung", "Alexander Swerdlow", "Dorsa Sadigh", "Chelsea Finn"], "title": "TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse", "comment": null, "summary": "Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTransformer Q-Learning (TQL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5206\u6570\u71b5\u6765\u9632\u6b62\u6ce8\u610f\u529b\u5d29\u6e83\uff0c\u4ece\u800c\u7a33\u5b9a\u8bad\u7ec3\u5e76\u5b9e\u73b0\u4ef7\u503c\u51fd\u6570\u7684\u5927\u89c4\u6a21\u6269\u5c55\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u9886\u57df\u901a\u8fc7\u89c4\u6a21\u6269\u5c55\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4ef7\u503c\u51fd\u6570\u4ecd\u7136\u4e3b\u8981\u4f7f\u7528\u5c0f\u578b\u6a21\u578b\u3002\u76f4\u63a5\u6269\u5c55\u4ef7\u503c\u51fd\u6570\uff08\u5305\u62ec\u4f7f\u7528\u5df2\u77e5\u5177\u6709\u9ad8\u5ea6\u53ef\u6269\u5c55\u6027\u7684Transformer\u67b6\u6784\uff09\u901a\u5e38\u4f1a\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u662f\u4ec0\u4e48\u963b\u788d\u4e86Transformer\u5728\u4ef7\u503c\u51fd\u6570\u4e2d\u7684\u6709\u6548\u6269\u5c55\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u8bc6\u522b\u51fa\u6ce8\u610f\u529b\u5206\u6570\u5d29\u6e83\u662f\u6269\u5c55\u5931\u8d25\u7684\u5173\u952e\u539f\u56e0\uff0c\u63d0\u51faTransformer Q-Learning (TQL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5206\u6570\u7684\u71b5\u6765\u9632\u6b62\u5d29\u6e83\u5e76\u7a33\u5b9a\u8bad\u7ec3\uff0c\u4ece\u800c\u89e3\u9501Transformer\u5728\u5f3a\u5316\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u4e2d\u7684\u6269\u5c55\u6f5c\u529b\u3002", "result": "TQL\u65b9\u6cd5\u5728\u4ece\u6700\u5c0f\u5230\u6700\u5927\u7f51\u7edc\u89c4\u6a21\u7684\u6269\u5c55\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe43%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u800c\u5148\u524d\u65b9\u6cd5\u5728\u6269\u5c55\u65f6\u4f1a\u51fa\u73b0\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5206\u6570\u71b5\u53ef\u4ee5\u6709\u6548\u9632\u6b62Transformer\u5728\u4ef7\u503c\u51fd\u6570\u6269\u5c55\u4e2d\u7684\u6ce8\u610f\u529b\u5d29\u6e83\u95ee\u9898\uff0c\u4ece\u800c\u7a33\u5b9a\u8bad\u7ec3\u5e76\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5927\u89c4\u6a21\u4ef7\u503c\u51fd\u6570\u6269\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01453", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01453", "abs": "https://arxiv.org/abs/2602.01453", "authors": ["Idan Barnea", "Orin Levy", "Yishay Mansour"], "title": "Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs", "comment": null, "summary": "We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.\n  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\\tilde{O}(S^6 H^6 A / \u03b5^2)$ agents to obtain an $\u03b5$ approximation of the dynamics (i.e., yields an $\u03b5$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $\u03c1< H$ phases requires at least $A^{H/\u03c1}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u65e0\u5956\u52b1\u63a2\u7d22\u95ee\u9898\uff0c\u5206\u6790\u4e86\u5b66\u4e60\u9636\u6bb5\u6570\u4e0e\u667a\u80fd\u4f53\u6570\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u53d1\u73b0\u5f53\u5b66\u4e60\u9636\u6bb5\u6570\u7b49\u4e8e\u73af\u5883\u65f6\u57dfH\u65f6\uff0c\u4ec5\u9700\u591a\u9879\u5f0f\u6570\u91cf\u7684\u667a\u80fd\u4f53\u5373\u53ef\u83b7\u5f97\u03b5\u8fd1\u4f3c\u7684\u52a8\u6001\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u65e0\u5956\u52b1\u63a2\u7d22\u8bbe\u7f6e\u4e0b\u7684\u5408\u4f5c\u95ee\u9898\uff0c\u63a2\u7d22\u667a\u80fd\u4f53\u5982\u4f55\u5728\u4e0d\u89c2\u5bdf\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\u8054\u5408\u63a2\u7d22\u672a\u77e5MDP\u4ee5\u5b66\u4e60\u5176\u52a8\u6001\u7279\u6027\uff0c\u7279\u522b\u5173\u6ce8\u5b66\u4e60\u9636\u6bb5\u6570\u4e0e\u6240\u9700\u667a\u80fd\u4f53\u6570\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u5206\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u6bcf\u4e2a\u5b66\u4e60\u9636\u6bb5\u4e2d\u591a\u4e2a\u667a\u80fd\u4f53\u72ec\u7acb\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u88ab\u5206\u914d\u4e00\u4e2a\u7b56\u7565\u5e76\u6267\u884c\u8be5\u7b56\u7565\u89c2\u5bdf\u8f68\u8ff9\u3002\u7814\u7a76\u91cd\u70b9\u662f\u5206\u6790\u5b66\u4e60\u9636\u6bb5\u6570\uff08\u03c1\uff09\u4e0e\u667a\u80fd\u4f53\u6570\u91cf\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u7279\u522b\u5173\u6ce8\u5f53\u5b66\u4e60\u9636\u6bb5\u6570\u8f83\u5c11\u65f6\u7684\u60c5\u51b5\u3002", "result": "\u5f53\u5b66\u4e60\u9636\u6bb5\u6570\u7b49\u4e8e\u65f6\u57dfH\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u4ec5\u9700\u00d5(S\u2076H\u2076A/\u03b5\u00b2)\u4e2a\u667a\u80fd\u4f53\u5373\u53ef\u83b7\u5f97\u52a8\u6001\u7279\u6027\u7684\u03b5\u8fd1\u4f3c\u3002\u540c\u65f6\u7ed9\u51fa\u4e86\u4e0b\u754c\uff1a\u4efb\u4f55\u9650\u5236\u5728\u03c1<H\u9636\u6bb5\u7684\u7b97\u6cd5\u9700\u8981\u81f3\u5c11A^{H/\u03c1}\u4e2a\u667a\u80fd\u4f53\u624d\u80fd\u8fbe\u5230\u5e38\u6570\u7cbe\u5ea6\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5b66\u4e60\u9636\u6bb5\u6570\u4e0e\u667a\u80fd\u4f53\u6570\u91cf\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\uff0c\u8868\u660e\u5982\u679c\u8981\u9650\u5236\u667a\u80fd\u4f53\u6570\u91cf\u4e3a\u591a\u9879\u5f0f\u7ea7\u522b\uff0c\u5219\u9700\u8981\u5927\u7ea6H\u4e2a\u5b66\u4e60\u9636\u6bb5\u3002\u8fd9\u4e00\u7ed3\u679c\u5bf9\u591a\u667a\u80fd\u4f53\u65e0\u5956\u52b1\u63a2\u7d22\u7684\u7406\u8bba\u7406\u89e3\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01485", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.01485", "abs": "https://arxiv.org/abs/2602.01485", "authors": ["Muheng Li", "Jian Qian", "Wenlong Mou"], "title": "Predicting and improving test-time scaling laws via reward tail-guided search", "comment": "33 pages, 5 figures", "summary": "Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5c3e\u90e8\u5206\u5e03\u9884\u6d4b\u7684\u7f29\u653e\u5b9a\u5f8b\u5f15\u5bfc\u641c\u7d22\u65b9\u6cd5\uff0c\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u4ee5\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u76f8\u6bd4\u4f20\u7edfBest-of-N\u7b56\u7565\u83b7\u5f97\u66f4\u9ad8\u56de\u62a5\u3002", "motivation": "\u73b0\u6709Best-of-N\u7b56\u7565\u7f3a\u4e4f\u5bf9N\u503c\u9009\u62e9\u3001\u9884\u7b97\u5206\u914d\u548c\u591a\u9636\u6bb5\u51b3\u7b56\u7684\u539f\u5219\u6027\u6307\u5bfc\uff0c\u5b58\u5728\u4f18\u5316\u7a7a\u95f4\uff0c\u4e14\u76f8\u5173\u4f18\u5316\u5de5\u4f5c\u7f3a\u4e4f\u4e25\u683c\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u901a\u8fc7\u4f30\u8ba1\u5956\u52b1\u7684\u5c3e\u90e8\u5206\u5e03\u6765\u9884\u6d4bLLM\u7f29\u653e\u5b9a\u5f8b\uff0c\u65e0\u9700\u7a77\u4e3e\u8bc4\u4f30\uff1b\u63d0\u51fa\u7f29\u653e\u5b9a\u5f8b\u5f15\u5bfc\u641c\u7d22\u7b97\u6cd5\uff0c\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u8bc6\u522b\u548c\u5229\u7528\u5177\u6709\u6700\u9ad8\u9884\u6d4b\u6f5c\u529b\u7684\u4e2d\u95f4\u72b6\u6001\u3002", "result": "\u7406\u8bba\u8bc1\u660eSLG\u76f8\u6bd4\u5b8c\u7f8e\u4fe1\u606f\u9884\u8a00\u673a\u5b9e\u73b0\u53ef\u5ffd\u7565\u7684\u9057\u61be\uff0c\u8fbe\u5230\u76f8\u540c\u9884\u671f\u5956\u52b1\u6240\u9700\u8ba1\u7b97\u9884\u7b97\u6bd4Best-of-N\u591a\u9879\u5f0f\u7ea7\u51cf\u5c11\uff1b\u5b9e\u8bc1\u9a8c\u8bc1\u5728\u4e0d\u540cLLM\u548c\u5956\u52b1\u6a21\u578b\u4e0a\u5747\u4f18\u4e8eBest-of-N\u3002", "conclusion": "\u5c3e\u90e8\u5f15\u5bfc\u7684\u5206\u914d\u7b56\u7565\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u6bd4Best-of-N\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\u4ea7\u51fa\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u7f29\u653e\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.01558", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01558", "abs": "https://arxiv.org/abs/2602.01558", "authors": ["Yiming Ma", "Lixu Wang", "Lionel Z. Wang", "Hongkun Yang", "Haoming Sun", "Xin Xu", "Jiaqi Wu", "Bin Chen", "Wei Dong"], "title": "How Implicit Bias Accumulates and Propagates in LLM Long-term Memory", "comment": "Under review, and the first two authors contribute equally", "summary": "Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5177\u6709\u957f\u671f\u8bb0\u5fc6\u7684LLM\u4e2d\u9690\u6027\u504f\u89c1\u7684\u79ef\u7d2f\u4e0e\u4f20\u64ad\u95ee\u9898\uff0c\u63d0\u51fa\u4e86DIB\u57fa\u51c6\u6d4b\u8bd5\u548c\u52a8\u6001\u8bb0\u5fc6\u6807\u8bb0(DMT)\u5e72\u9884\u65b9\u6cd5\u3002", "motivation": "LLM\u7684\u957f\u671f\u8bb0\u5fc6\u673a\u5236\u867d\u7136\u80fd\u4fdd\u6301\u4ea4\u4e92\u8fde\u7eed\u6027\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u516c\u5e73\u6027\u98ce\u9669\uff0c\u7279\u522b\u662f\u9690\u6027\u504f\u89c1\u5982\u4f55\u968f\u65f6\u95f4\u79ef\u7d2f\u548c\u8de8\u9886\u57df\u4f20\u64ad\u7684\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "1) \u5f15\u5165\u51b3\u7b56\u6027\u9690\u6027\u504f\u89c1(DIB)\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b3,776\u4e2a\u51b3\u7b56\u573a\u666f\uff1b2) \u4f7f\u7528\u957f\u671f\u6a21\u62df\u6846\u67b6\u8bc4\u4f306\u4e2aSOTA LLM\u548c3\u79cd\u8bb0\u5fc6\u67b6\u6784\uff1b3) \u63d0\u51fa\u52a8\u6001\u8bb0\u5fc6\u6807\u8bb0(DMT)\u5e72\u9884\u65b9\u6cd5\uff0c\u5728\u8bb0\u5fc6\u5199\u5165\u65f6\u5f3a\u5236\u6267\u884c\u516c\u5e73\u7ea6\u675f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) LLM\u7684\u9690\u6027\u504f\u89c1\u4e0d\u4f1a\u4fdd\u6301\u9759\u6001\uff0c\u800c\u662f\u968f\u65f6\u95f4\u52a0\u5267\uff1b2) \u504f\u89c1\u4f1a\u8de8\u4e0d\u76f8\u5173\u9886\u57df\u4f20\u64ad\uff1b3) \u9759\u6001\u7cfb\u7edf\u7ea7\u63d0\u793a\u7684\u7f13\u89e3\u6548\u679c\u6709\u9650\u4e14\u77ed\u6682\uff1b4) DMT\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u504f\u89c1\u79ef\u7d2f\u5e76\u6709\u6548\u904f\u5236\u8de8\u9886\u57df\u504f\u89c1\u4f20\u64ad\u3002", "conclusion": "\u957f\u671f\u8bb0\u5fc6\u4f1a\u52a0\u5267LLM\u7684\u9690\u6027\u504f\u89c1\u95ee\u9898\uff0c\u9700\u8981\u52a8\u6001\u5e72\u9884\u673a\u5236\u3002DMT\u65b9\u6cd5\u901a\u8fc7\u5728\u8bb0\u5fc6\u5199\u5165\u65f6\u65bd\u52a0\u516c\u5e73\u7ea6\u675f\uff0c\u80fd\u6709\u6548\u7f13\u89e3\u504f\u89c1\u79ef\u7d2f\u548c\u4f20\u64ad\u3002", "topic": "agent analysis"}}
{"id": "2602.01576", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.01576", "abs": "https://arxiv.org/abs/2602.01576", "authors": ["Woosung Koh", "Sungjun Han", "Segyu Lee", "Se-Young Yun", "Jamin Shin"], "title": "Generative Visual Code Mobile World Models", "comment": "Pre-print (technical report)", "summary": "Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.", "AI": {"tldr": "\u63d0\u51fagWorld\u89c6\u89c9\u79fb\u52a8GUI\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u6267\u884c\u7684\u7f51\u9875\u4ee3\u7801\u800c\u975e\u76f4\u63a5\u751f\u6210\u50cf\u7d20\uff0c\u7ed3\u5408\u4e86\u6587\u672c\u548c\u89c6\u89c9\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u79fb\u52a8GUI\u4e16\u754c\u6a21\u578b\u5b58\u5728\u5173\u952e\u6743\u8861\uff1a\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\u727a\u7272\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u800c\u89c6\u89c9\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u786e\u6e32\u67d3\u6587\u672c\uff0c\u9700\u8981\u4f9d\u8d56\u7f13\u6162\u590d\u6742\u7684\u5916\u90e8\u6a21\u578b\u7ba1\u9053\u3002\u9700\u8981\u4e00\u79cd\u65b0\u8303\u5f0f\u6765\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u4e16\u754c\u5efa\u6a21\u901a\u8fc7\u53ef\u6e32\u67d3\u4ee3\u7801\u751f\u6210\u7684\u65b0\u8303\u5f0f\uff1a\u4f7f\u7528\u5355\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u4e0b\u4e00\u4e2aGUI\u72b6\u6001\u4e3a\u53ef\u6267\u884c\u7684\u7f51\u9875\u4ee3\u7801\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u751f\u6210\u50cf\u7d20\u3002\u5f00\u53d1\u4e86gWorld\u6570\u636e\u751f\u6210\u6846\u67b6\u81ea\u52a8\u5408\u6210\u57fa\u4e8e\u4ee3\u7801\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "gWorld\u57284\u4e2a\u5206\u5e03\u5185\u548c2\u4e2a\u5206\u5e03\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u51c6\u786e\u6027\u4e0e\u6a21\u578b\u5927\u5c0f\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u6027\u80fd\u4f18\u4e8e8\u4e2a\u524d\u6cbf\u5f00\u6e90\u6a21\u578b\uff08\u6a21\u578b\u5927\u5c0f\u8d85\u8fc750.25\u500d\uff09\u3002\u5206\u6790\u663e\u793a\uff1a1\uff09\u901a\u8fc7gWorld\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u5e26\u6765\u663e\u8457\u63d0\u5347\uff1b2\uff09\u7ba1\u9053\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\uff1b3\uff09\u66f4\u5f3a\u7684\u4e16\u754c\u5efa\u6a21\u6539\u5584\u4e0b\u6e38\u79fb\u52a8GUI\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u53ef\u6e32\u67d3\u4ee3\u7801\u751f\u6210\u7684\u89c6\u89c9\u4e16\u754c\u5efa\u6a21\u65b0\u8303\u5f0f\u6210\u529f\u7ed3\u5408\u4e86\u6587\u672c\u548c\u89c6\u89c9\u65b9\u6cd5\u7684\u4f18\u52bf\uff0cgWorld\u6a21\u578b\u5728\u79fb\u52a8GUI\u4e16\u754c\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u4e3a\u79fb\u52a8GUI\u4ee3\u7406\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2602.01606", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01606", "abs": "https://arxiv.org/abs/2602.01606", "authors": ["Zeqiao Li", "Yijing Wang", "Haoyu Wang", "Zheng Li", "Zhiqiang Zuo"], "title": "Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching", "comment": null, "summary": "Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \\textbf{F}low-based \\textbf{L}og-likelihood-\\textbf{A}ware \\textbf{M}aximum \\textbf{E}ntropy RL (\\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.", "AI": {"tldr": "FLAME\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7Q\u91cd\u52a0\u6743\u76ee\u6807\u7ed5\u8fc7\u914d\u5206\u51fd\u6570\u4f30\u8ba1\uff0c\u4f7f\u7528\u89e3\u8026\u71b5\u4f30\u8ba1\u5668\u7ea0\u6b63\u504f\u5dee\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u548c\u4e00\u6b65\u751f\u6210\u63a7\u5236\uff0c\u5728MuJoCo\u4e0a\u8d85\u8d8a\u9ad8\u65af\u57fa\u7ebf\u5e76\u5339\u914d\u591a\u6b65\u6269\u6563\u7b56\u7565\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u5f3a\u4f46\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u6d41\u5339\u914d\u53ef\u5b9e\u73b0\u4e00\u6b65\u751f\u6210\u4f46\u96be\u4ee5\u96c6\u6210\u5230\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u56e0\u4e3a\u6700\u4f18\u7b56\u7565\u662f\u96be\u4ee5\u5904\u7406\u7684\u57fa\u4e8e\u80fd\u91cf\u7684\u5206\u5e03\uff0c\u4e14\u9ad8\u6548\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u5b58\u5728\u4e25\u91cd\u79bb\u6563\u5316\u504f\u5dee\u3002", "method": "1) \u63a8\u5bfcQ\u91cd\u52a0\u6743\u6d41\u5339\u914d\u76ee\u6807\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91cd\u52a0\u6743\u7ed5\u8fc7\u914d\u5206\u51fd\u6570\u4f30\u8ba1\uff1b2) \u8bbe\u8ba1\u89e3\u8026\u71b5\u4f30\u8ba1\u5668\uff0c\u4e25\u683c\u7ea0\u6b63\u504f\u5dee\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\uff1b3) \u96c6\u6210MeanFlow\u516c\u5f0f\u5b9e\u73b0\u8868\u8fbe\u6027\u5f3a\u4e14\u9ad8\u6548\u7684\u4e00\u6b65\u63a7\u5236\u3002", "result": "\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFLAME\u8d85\u8d8a\u9ad8\u65af\u57fa\u7ebf\uff0c\u6027\u80fd\u4e0e\u591a\u6b65\u6269\u6563\u7b56\u7565\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "conclusion": "FLAME\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6d41\u5339\u914d\u96c6\u6210\u5230\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u8868\u8fbe\u6027\u5f3a\u3001\u63a2\u7d22\u9ad8\u6548\u4e14\u63a8\u7406\u6210\u672c\u4f4e\u7684\u7b56\u7565\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01611", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01611", "abs": "https://arxiv.org/abs/2602.01611", "authors": ["Weizheng Gu", "Chengze Li", "Zhuohao Yu", "Mengyuan Sun", "Zhibang Yang", "Wei Wang", "Hongrui Jia", "Shikun Zhang", "Wei Ye"], "title": "What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?", "comment": null, "summary": "Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPIPE\u8bc4\u4f30\u534f\u8bae\uff0c\u7528\u4e8e\u8bca\u65adAI\u4ee3\u7406\u5bf9\u7279\u5b9a\u754c\u9762\u683c\u5f0f\u7684\u4f9d\u8d56\uff0c\u53d1\u73b0\u8f68\u8ff9\u76d1\u7763\u5fae\u8c03\u4f1a\u589e\u5f3a\u4ee3\u7406\u5bf9\u8bad\u7ec3\u754c\u9762\u7684\u6377\u5f84\u5b66\u4e60\uff0c\u800c\u975e\u771f\u6b63\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u8bc4\u4f30\u5b58\u5728\u6df7\u6dc6\uff1a\u65e0\u6cd5\u533a\u5206\u4ee3\u7406\u7684\u6210\u529f\u662f\u6e90\u4e8e\u771f\u6b63\u7684\u8bed\u4e49\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u8bb0\u4f4f\u4e86\u7279\u5b9a\u754c\u9762\u7684\u4ea4\u4e92\u6a21\u5f0f\u3002\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u8bc6\u522b\u4ee3\u7406\u662f\u5426\u771f\u6b63\u5177\u5907\u73af\u5883\u4e0d\u53d8\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faPIPE\u534f\u8bae\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u91cd\u5199\u73af\u5883\u754c\u9762\uff08\u4fdd\u6301\u4efb\u52a1\u8bed\u4e49\u548c\u6267\u884c\u884c\u4e3a\u4e0d\u53d8\uff09\u6765\u8bca\u65ad\u754c\u9762\u4f9d\u8d56\u3002\u5f15\u5165\u754c\u9762\u4f9d\u8d56\u5ea6\uff08IR\uff09\u6307\u6807\uff0c\u91cf\u5316\u4ee3\u7406\u5bf9\u8bad\u7ec3\u65f6\u754c\u9762\u7684\u504f\u597d\u3002", "result": "\u572816\u4e2a\u73af\u5883\u548c\u591a\u79cd\u4ee3\u7406\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\u8f68\u8ff9\u76d1\u7763\u5fae\u8c03\u663e\u8457\u589e\u5f3a\u4e86\u754c\u9762\u6377\u5f84\u5b66\u4e60\uff0c\u8bad\u7ec3\u4ee3\u7406\u5728\u754c\u9762\u5fae\u5c0f\u53d8\u5316\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u800c\u975e\u8f68\u8ff9\u8bad\u7ec3\u7684\u6a21\u578b\u4fdd\u6301\u7a33\u5b9a\u3002\u754c\u9762\u6377\u5f84\u5b66\u4e60\u8868\u73b0\u51fa\u73af\u5883\u4f9d\u8d56\u3001\u975e\u5355\u8c03\u7684\u8bad\u7ec3\u52a8\u6001\u3002", "conclusion": "\u6807\u51c6\u8bc4\u4f30\u63a9\u76d6\u4e86\u4ee3\u7406\u5bf9\u7279\u5b9a\u754c\u9762\u7684\u4f9d\u8d56\u95ee\u9898\u3002\u8f68\u8ff9\u76d1\u7763\u5fae\u8c03\u53ef\u80fd\u4f7f\u4ee3\u7406\u5b66\u4e60\u754c\u9762\u7279\u5b9a\u7684\u6a21\u5f0f\u800c\u975e\u901a\u7528\u80fd\u529b\u3002PIPE\u534f\u8bae\u80fd\u6709\u6548\u8bca\u65ad\u8fd9\u79cd\u754c\u9762\u4f9d\u8d56\uff0c\u4e3a\u66f4\u9c81\u68d2\u7684\u4ee3\u7406\u8bc4\u4f30\u63d0\u4f9b\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.01619", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01619", "abs": "https://arxiv.org/abs/2602.01619", "authors": ["Seyed Mohammad Hadi Hosseini", "Mahdieh Soleymani Baghshah"], "title": "SUSD: Structured Unsupervised Skill Discovery through State Factorization", "comment": "Accepted as a conference paper at ICLR 2026", "summary": "Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.", "AI": {"tldr": "SUSD\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73af\u5883\u56e0\u5b50\u5206\u89e3\u7684\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u72b6\u6001\u7a7a\u95f4\u5206\u89e3\u4e3a\u72ec\u7acb\u7ec4\u4ef6\u5e76\u4e3a\u4e0d\u540c\u56e0\u5b50\u5206\u914d\u6280\u80fd\u53d8\u91cf\uff0c\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u6280\u80fd\u53d1\u73b0\u548c\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u65b9\u6cd5\u503e\u5411\u4e8e\u53d1\u73b0\u7b80\u5355\u9759\u6001\u6280\u80fd\uff0c\u800c\u57fa\u4e8e\u8ddd\u79bb\u6700\u5927\u5316\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u4fc3\u8fdb\u52a8\u6001\u6280\u80fd\uff0c\u4f46\u4ecd\u96be\u4ee5\u53d1\u73b0\u5168\u9762\u5229\u7528\u6240\u6709\u53ef\u63a7\u56e0\u5b50\u7684\u6280\u80fd\u96c6\u3002", "method": "SUSD\u5c06\u73af\u5883\u72b6\u6001\u7a7a\u95f4\u5206\u89e3\u4e3a\u72ec\u7acb\u56e0\u5b50\uff08\u5982\u5bf9\u8c61\u6216\u53ef\u63a7\u5b9e\u4f53\uff09\uff0c\u4e3a\u4e0d\u540c\u56e0\u5b50\u5206\u914d\u72ec\u7acb\u7684\u6280\u80fd\u53d8\u91cf\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u6a21\u578b\u8ddf\u8e2a\u5404\u56e0\u5b50\u7684\u5b66\u4e60\u8fdb\u5ea6\uff0c\u81ea\u9002\u5e94\u5730\u5c06\u667a\u80fd\u4f53\u6ce8\u610f\u529b\u5f15\u5bfc\u5230\u63a2\u7d22\u4e0d\u8db3\u7684\u56e0\u5b50\u3002", "result": "\u57281\u523010\u4e2a\u56e0\u5b50\u7684\u4e09\u79cd\u73af\u5883\u4e2d\uff0cSUSD\u80fd\u591f\u53d1\u73b0\u591a\u6837\u4e14\u590d\u6742\u7684\u6280\u80fd\uff0c\u5728\u56e0\u5b50\u5316\u548c\u590d\u6742\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u3002", "conclusion": "SUSD\u901a\u8fc7\u5229\u7528\u73af\u5883\u7684\u7ec4\u5408\u7ed3\u6784\uff0c\u4e0d\u4ec5\u4fc3\u8fdb\u4e86\u66f4\u4e30\u5bcc\u591a\u6837\u7684\u6280\u80fd\u53d1\u73b0\uff0c\u8fd8\u4ea7\u751f\u4e86\u56e0\u5b50\u5316\u7684\u6280\u80fd\u8868\u793a\uff0c\u80fd\u591f\u5bf9\u4e2a\u4f53\u5b9e\u4f53\u8fdb\u884c\u7ec6\u7c92\u5ea6\u548c\u89e3\u8026\u63a7\u5236\uff0c\u4fbf\u4e8e\u901a\u8fc7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u9ad8\u6548\u8bad\u7ec3\u7ec4\u5408\u4e0b\u6e38\u4efb\u52a1\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01644", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.01644", "abs": "https://arxiv.org/abs/2602.01644", "authors": ["Gloria Felicia", "Nolan Bryant", "Handi Putra", "Ayaan Gazali", "Eliel Lobo", "Esteban Rojas"], "title": "From Perception to Action: Spatial AI Agents and World Models", "comment": "61 pages, 742 citations, 1 figure, 3 tables. Survey paper on spatial AI agents, embodied AI, graph neural networks, and world models", "summary": "While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4e09\u8f74\u5206\u7c7b\u6cd5\uff0c\u5c06\u667a\u80fd\u4f53\u80fd\u529b\u4e0e\u8de8\u5c3a\u5ea6\u7684\u7a7a\u95f4\u4efb\u52a1\u8fde\u63a5\u8d77\u6765\uff0c\u5f3a\u8c03\u7a7a\u95f4\u667a\u80fd\u5bf9\u4e8e\u5177\u8eab\u667a\u80fd\u4f53\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u516d\u4e2a\u91cd\u5927\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8981\u4e48\u5355\u72ec\u5173\u6ce8\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u8981\u4e48\u5355\u72ec\u5173\u6ce8\u7a7a\u95f4\u9886\u57df\uff0c\u7f3a\u4e4f\u8fde\u63a5\u8fd9\u4e24\u79cd\u4e92\u8865\u80fd\u529b\u7684\u7edf\u4e00\u6846\u67b6\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b26\u53f7\u9886\u57df\u7684\u6210\u529f\u96be\u4ee5\u76f4\u63a5\u8fc1\u79fb\u5230\u7269\u7406\u4e16\u754c\uff0c\u7a7a\u95f4\u667a\u80fd\uff08\u611f\u77e53D\u7ed3\u6784\u3001\u63a8\u7406\u7269\u4f53\u5173\u7cfb\u3001\u5728\u7269\u7406\u7ea6\u675f\u4e0b\u884c\u52a8\uff09\u5bf9\u5177\u8eab\u667a\u80fd\u4f53\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5bf92000\u591a\u7bc7\u8bba\u6587\u7684\u5168\u9762\u56de\u987e\uff08\u5f15\u7528742\u7bc7\u9876\u7ea7\u4f1a\u8bae\u8bba\u6587\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4e09\u8f74\u5206\u7c7b\u6cd5\uff1a\u80fd\u529b\u8f74\uff08\u667a\u80fd\u4f53\u80fd\u529b\uff09\u3001\u4efb\u52a1\u8f74\uff08\u7a7a\u95f4\u4efb\u52a1\uff09\u548c\u5c3a\u5ea6\u8f74\uff08\u7a7a\u95f4\u5c3a\u5ea6\uff09\u3002\u533a\u5206\u4e86\u7a7a\u95f4\u57fa\u7840\uff08\u51e0\u4f55\u548c\u7269\u7406\u7684\u5ea6\u91cf\u7406\u89e3\uff09\u4e0e\u7b26\u53f7\u57fa\u7840\uff08\u56fe\u50cf\u4e0e\u6587\u672c\u5173\u8054\uff09\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\uff1a(1) \u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\u5bf9\u957f\u65f6\u7a0b\u7a7a\u95f4\u4efb\u52a1\u5f88\u91cd\u8981\uff1b(2) GNN-LLM\u96c6\u6210\u662f\u7ed3\u6784\u5316\u7a7a\u95f4\u63a8\u7406\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff1b(3) \u4e16\u754c\u6a21\u578b\u5bf9\u4e8e\u8de8\u5fae\u89c2\u5230\u5b8f\u89c2\u7a7a\u95f4\u5c3a\u5ea6\u7684\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u4e3a\u7edf\u4e00\u788e\u7247\u5316\u7684\u7814\u7a76\u5de5\u4f5c\u548c\u5b9e\u73b0\u4e0b\u4e00\u4ee3\u7a7a\u95f4\u611f\u77e5\u81ea\u4e3b\u7cfb\u7edf\uff08\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u5730\u7406\u7a7a\u95f4\u667a\u80fd\uff09\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u8bc6\u522b\u4e86\u516d\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u5305\u62ec\u9700\u8981\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u6765\u6807\u51c6\u5316\u8de8\u9886\u57df\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2602.01685", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01685", "abs": "https://arxiv.org/abs/2602.01685", "authors": ["Byeonghu Na", "Hyungho Na", "Yeongmin Kim", "Suhyeon Jo", "HeeSun Bae", "Mina Kang", "Il-Chul Moon"], "title": "Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment", "comment": "Accepted at ICLR 2026", "summary": "Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.", "AI": {"tldr": "\u63d0\u51faWasserstein Policy Regularization (WPR)\uff0c\u4e00\u79cd\u57fa\u4e8e\u71b5\u6b63\u5219\u5316Wasserstein\u8ddd\u79bb\u7684\u8bed\u4e49\u611f\u77e5\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8eRLHF\u6846\u67b6\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684KL\u548cf-\u6563\u5ea6\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edfRLHF\u4e2d\u4f7f\u7528KL\u6563\u5ea6\u53ca\u5176f-\u6563\u5ea6\u53d8\u4f53\u4ec5\u6bd4\u8f83\u76f8\u540c\u4f4d\u7f6e\u7684token\u6982\u7387\uff0c\u65e0\u6cd5\u6355\u6349\u8bed\u4e49\u76f8\u4f3c\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408token\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u7684\u8bed\u4e49\u611f\u77e5\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faWasserstein Policy Regularization (WPR)\uff0c\u57fa\u4e8e\u71b5\u6b63\u5219\u5316Wasserstein\u8ddd\u79bb\uff0c\u901a\u8fc7\u8ddd\u79bb\u7684\u5bf9\u5076\u5f62\u5f0f\u5c06\u6b63\u5219\u5316\u8868\u793a\u4e3a\u901a\u8fc7\u6700\u4f18\u5bf9\u5076\u53d8\u91cf\u5e94\u7528\u4e8e\u5956\u52b1\u7684\u60e9\u7f5a\u9879\uff0c\u5f97\u5230\u4e0e\u6807\u51c6RL\u7b97\u6cd5\u517c\u5bb9\u7684\u53ef\u5904\u7406\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u4e8eKL\u548cf-\u6563\u5ea6\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8bed\u4e49\u611f\u77e5\u7b56\u7565\u8ddd\u79bb\u5728\u6a21\u578b\u5bf9\u9f50\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "WPR\u901a\u8fc7\u5f15\u5165Wasserstein\u8ddd\u79bb\u7684\u8bed\u4e49\u611f\u77e5\u80fd\u529b\uff0c\u4e3aRLHF\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u6539\u5584\u4e86LLM\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u6548\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01705", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01705", "abs": "https://arxiv.org/abs/2602.01705", "authors": ["Haoqiang Kang", "Yizhe Zhang", "Nikki Lijing Kuang", "Yi-An Ma", "Lianhui Qin"], "title": "Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner", "comment": null, "summary": "Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.", "AI": {"tldr": "LaDi-RL\uff1a\u4e00\u79cd\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u63a2\u7d22\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5bfc\u6269\u6563\u5efa\u6a21\u63a2\u7d22\uff0c\u89e3\u51b3\u79bb\u6563RL\u4e2dtoken\u7a7a\u95f4\u63a2\u7d22\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u79bb\u6563\u601d\u7ef4\u94fe\u751f\u6210\u6765\u6539\u8fdbLLM\u63a8\u7406\uff0c\u4f46\u5728token\u7a7a\u95f4\u4e2d\u7684\u63a2\u7d22\u5e38\u56e0\u7b56\u7565\u71b5\u964d\u4f4e\u800c\u906d\u53d7\u591a\u6837\u6027\u5d29\u6e83\uff0c\u8fd9\u662f\u7531\u4e8e\u79bb\u6563RL\u4e2d\u7684\u6a21\u5f0f\u6fc0\u53d1\u884c\u4e3a\u5bfc\u81f4\u7684\u3002", "method": "\u63d0\u51faLaDi-RL\u6846\u67b6\uff0c\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u76f4\u63a5\u8fdb\u884c\u63a2\u7d22\uff0c\u5176\u4e2d\u6f5c\u5728\u53d8\u91cf\u7f16\u7801\u8bed\u4e49\u7ea7\u63a8\u7406\u8f68\u8ff9\u3002\u901a\u8fc7\u5f15\u5bfc\u6269\u6563\u5efa\u6a21\u63a2\u7d22\uff0c\u591a\u6b65\u53bb\u566a\u5206\u5e03\u968f\u673a\u6027\u5e76\u4fdd\u7559\u591a\u4e2a\u5171\u5b58\u89e3\u51b3\u65b9\u6848\u6a21\u5f0f\u3002\u5c06\u6f5c\u5728\u7a7a\u95f4\u63a2\u7d22\u4e0e\u6587\u672c\u7a7a\u95f4\u751f\u6210\u89e3\u8026\uff0c\u7ed3\u5408\u8865\u5145\u6587\u672c\u7b56\u7565\u3002", "result": "\u5728\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u79bb\u6563RL\u57fa\u7ebf\uff0c\u5728pass@1\u548cpass@k\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff1a\u4ee3\u7801\u751f\u6210\u7edd\u5bf9pass@1\u589e\u76ca+9.4%\uff0c\u6570\u5b66\u63a8\u7406+5.7%\u3002", "conclusion": "\u57fa\u4e8e\u6269\u6563\u7684\u6f5c\u5728RL\u662f\u79bb\u6563token\u7ea7RL\u8fdb\u884c\u63a8\u7406\u7684\u4e00\u79cd\u6709\u539f\u5219\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6f5c\u5728\u6269\u6563\u4f18\u5316\u6bd4\u7eaf\u6587\u672c\u7a7a\u95f4\u7b56\u7565\u4f18\u5316\u66f4\u6709\u6548\uff0c\u7ed3\u5408\u6587\u672c\u7b56\u7565\u53ef\u63d0\u4f9b\u989d\u5916\u589e\u76ca\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01776", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01776", "abs": "https://arxiv.org/abs/2602.01776", "authors": ["Mingyue Cheng", "Xiaoyu Tao", "Qi Liu", "Ze Guo", "Enhong Chen"], "title": "Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting", "comment": null, "summary": "Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\"\u4ee3\u7406\u5f0f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b(ATSF)\"\uff0c\u5c06\u4f20\u7edf\u6a21\u578b\u4e2d\u5fc3\u7684\u9759\u6001\u9884\u6d4b\u91cd\u6784\u4e3a\u5305\u542b\u611f\u77e5\u3001\u89c4\u5212\u3001\u884c\u52a8\u3001\u53cd\u601d\u548c\u8bb0\u5fc6\u7684\u4ee3\u7406\u5f0f\u6d41\u7a0b\uff0c\u5f3a\u8c03\u901a\u8fc7\u5de5\u5177\u4ea4\u4e92\u3001\u53cd\u9988\u5b66\u4e60\u548c\u7ecf\u9a8c\u79ef\u7d2f\u5b9e\u73b0\u81ea\u9002\u5e94\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4f5c\u4e3a\u6a21\u578b\u4e2d\u5fc3\u3001\u9759\u6001\u3001\u5355\u6b21\u9884\u6d4b\u95ee\u9898\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u9002\u5e94\u9700\u8981\u4fe1\u606f\u7279\u5f81\u63d0\u53d6\u3001\u63a8\u7406\u9a71\u52a8\u63a8\u65ad\u3001\u8fed\u4ee3\u4f18\u5316\u548c\u6301\u7eed\u65f6\u95f4\u9002\u5e94\u7684\u81ea\u9002\u5e94\u591a\u8f6e\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4ee3\u7406\u5f0f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b(ATSF)\u6846\u67b6\uff0c\u5c06\u9884\u6d4b\u91cd\u6784\u4e3a\u5305\u542b\u611f\u77e5\u3001\u89c4\u5212\u3001\u884c\u52a8\u3001\u53cd\u601d\u548c\u8bb0\u5fc6\u7684\u4ee3\u7406\u5f0f\u6d41\u7a0b\u3002\u4ecb\u7ecd\u4e86\u4e09\u79cd\u5b9e\u73b0\u8303\u5f0f\uff1a\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u8bbe\u8ba1\u3001\u4ee3\u7406\u5f0f\u5f3a\u5316\u5b66\u4e60\u548c\u6df7\u5408\u4ee3\u7406\u5de5\u4f5c\u6d41\u8303\u5f0f\u3002", "result": "\u5efa\u7acb\u4e86\u4ee3\u7406\u5f0f\u9884\u6d4b\u4f5c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u672a\u6765\u7814\u7a76\u7684\u57fa\u7840\u6846\u67b6\uff0c\u8ba8\u8bba\u4e86\u4ece\u6a21\u578b\u4e2d\u5fc3\u9884\u6d4b\u8f6c\u5411\u4ee3\u7406\u5f0f\u9884\u6d4b\u7684\u673a\u9047\u4e0e\u6311\u6218\u3002", "conclusion": "\u4ee3\u7406\u5f0f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e3a\u81ea\u9002\u5e94\u3001\u591a\u8f6e\u9884\u6d4b\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5de5\u5177\u4ea4\u4e92\u3001\u53cd\u9988\u5b66\u4e60\u548c\u7ecf\u9a8c\u79ef\u7d2f\u5b9e\u73b0\u66f4\u667a\u80fd\u7684\u9884\u6d4b\u7cfb\u7edf\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.01791", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01791", "abs": "https://arxiv.org/abs/2602.01791", "authors": ["Zheng Zhang", "Ao Lu", "Yuanhao Zeng", "Ziwei Shan", "Jinjin Guo", "Lufei Li", "Yexin Li", "Kan Ren"], "title": "Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.", "AI": {"tldr": "Grad2Reward\u662f\u4e00\u4e2a\u4eceLLM\u6cd5\u5b98\u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\u63d0\u53d6\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u5f52\u56e0\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff0c\u63d0\u5347\u5f00\u653e\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\u548c\u63a8\u7406\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u6cd5\u5b98\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\uff0c\u65e0\u6cd5\u4e3a\u590d\u6742\u957f\u8f68\u8ff9\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\uff1b2\uff09\u5c06\u6cd5\u5b98\u89c6\u4e3a\u9ed1\u76d2\uff0c\u5ffd\u7565\u4e86\u5176\u4e2d\u4e30\u5bcc\u7684\u4e2d\u95f4\u53cd\u9988\u4fe1\u53f7\u3002", "method": "\u63d0\u51faGrad2Reward\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u53cd\u5411\u4f20\u64ad\u4ece\u6cd5\u5b98\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\u63d0\u53d6\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\uff0c\u5229\u7528\u68af\u5ea6\u5f52\u56e0\u5b9e\u73b0\u7cbe\u786e\u7684token\u7ea7\u4fe1\u7528\u5206\u914d\uff0c\u5e76\u5f15\u5165\u81ea\u5224\u65ad\u673a\u5236\u8ba9\u7b56\u7565\u901a\u8fc7\u81ea\u8eab\u8bc4\u4f30\u4fe1\u53f7\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528Grad2Reward\u4f18\u5316\u7684\u7b56\u7565\u5728\u591a\u79cd\u5f00\u653e\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Grad2Reward\u901a\u8fc7\u4ece\u6cd5\u5b98\u6a21\u578b\u4e2d\u63d0\u53d6\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u63a8\u7406\u8d28\u91cf\uff0c\u4e3a\u5f00\u653e\u4efb\u52a1\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01826", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01826", "abs": "https://arxiv.org/abs/2602.01826", "authors": ["Yaxiang Zhang", "Yingru Li", "Jiacai Liu", "Jiawei Xu", "Ziniu Li", "Qian Liu", "Haoyuan Li"], "title": "Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It", "comment": null, "summary": "Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to \"training inference mismatch stemming\" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u68af\u5ea6\u566a\u58f0\u548c\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u4f1a\u968f\u7740\u8bad\u7ec3\u8fdb\u5c55\u800c\u52a0\u5267\uff0c\u63d0\u51fa\u901a\u8fc7\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u7684\u52a8\u6001\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u5b58\u5728\u4e25\u91cd\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002\u867d\u7136\u8fd1\u671f\u7814\u7a76\u5c06\u5176\u5f52\u56e0\u4e8e\u6df7\u5408\u5f15\u64ce\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\"\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\"\uff0c\u4f46\u6807\u51c6\u89e3\u51b3\u65b9\u6cd5\uff08\u5982\u91cd\u8981\u6027\u91c7\u6837\uff09\u5728\u957f\u65f6\u95f4\u8bad\u7ec3\u4e2d\u53ef\u80fd\u5931\u6548\u3002\u9700\u8981\u6df1\u5165\u7406\u89e3\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u7684\u672c\u8d28\u5e76\u627e\u5230\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4ece\u4f18\u5316\u89d2\u5ea6\u5206\u6790RL\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u68af\u5ea6\u566a\u58f0\u548c\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u4f1a\u540c\u6b65\u52a0\u5267\u3002\u63d0\u51fa\u4e13\u95e8\u7684\u52a8\u6001\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\uff0c\u4e0d\u662f\u91c7\u7528\u9884\u5b9a\u4e49\u7684\u8870\u51cf\u8ba1\u5212\uff0c\u800c\u662f\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u52a8\u6001\u89e6\u53d1\u5b66\u4e60\u7387\u8870\u51cf\uff0c\u56e0\u4e3a\u54cd\u5e94\u957f\u5ea6\u88ab\u8bc6\u522b\u4e3a\u5373\u5c06\u53d1\u751f\u4e0d\u7a33\u5b9a\u6027\u7684\u53ef\u9760\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\u3002", "result": "\u901a\u8fc7\u51cf\u5c11\u5b66\u4e60\u7387\u6765\u63a7\u5236\u68af\u5ea6\u566a\u58f0\u4e0a\u5347\uff0c\u80fd\u591f\u6301\u7eed\u7a33\u5b9aRL\u8bad\u7ec3\uff0c\u5e76\u5c06\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u4fdd\u6301\u5728\u5b89\u5168\u6c34\u5e73\u3002\u7ecf\u9a8c\u8bc1\u636e\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u4e0d\u662f\u9759\u6001\u7684\u6570\u503c\u5dee\u5f02\uff0c\u800c\u662f\u4e0e\u6a21\u578b\u4f18\u5316\u8026\u5408\u7684\u52a8\u6001\u6545\u969c\u3002\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u7684\u52a8\u6001\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u7a33\u5b9aRL\u8bad\u7ec3\u8fc7\u7a0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01842", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01842", "abs": "https://arxiv.org/abs/2602.01842", "authors": ["Jinbin Bai", "Yixuan Li", "Yuchen Zhu", "Yi Xin", "Qingyu Shi", "Aosong Feng", "Xiaohong Liu", "Molei Tao", "Jianru Xue", "Xiangtai Li", "Ming-Hsuan Yang"], "title": "Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models", "comment": null, "summary": "Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.", "AI": {"tldr": "Prism\uff1a\u9488\u5bf9\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u6d4b\u8bd5\u65f6\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u6b21\u8f68\u8ff9\u641c\u7d22\u3001\u5c40\u90e8\u5206\u652f\u4e0e\u90e8\u5206\u91cd\u63a9\u7801\u3001\u81ea\u9a8c\u8bc1\u53cd\u9988\u7b49\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u5df2\u6210\u4e3a\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b97\u6cd5\u4e3b\u8981\u4f9d\u8d56\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u4e0d\u9002\u7528\u4e8e\u5e76\u884c\u89e3\u7801\u7684\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u3002\u5f00\u53d1\u6709\u6548\u4e14\u9ad8\u6548\u7684TTS\u65b9\u6cd5\u6765\u91ca\u653edLLMs\u7684\u751f\u6210\u6f5c\u529b\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faPrism\u6846\u67b6\uff1a1) \u5c42\u6b21\u8f68\u8ff9\u641c\u7d22\uff1a\u5728\u65e9\u671f\u5230\u4e2d\u671f\u7684\u53bb\u566a\u7a97\u53e3\u52a8\u6001\u526a\u679d\u548c\u91cd\u65b0\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff1b2) \u5c40\u90e8\u5206\u652f\u4e0e\u90e8\u5206\u91cd\u63a9\u7801\uff1a\u63a2\u7d22\u591a\u6837\u5316\u5b9e\u73b0\u540c\u65f6\u4fdd\u7559\u9ad8\u7f6e\u4fe1\u5ea6token\uff1b3) \u81ea\u9a8c\u8bc1\u53cd\u9988\uff1a\u901a\u8fc7\u81ea\u8bc4\u4f30\u63d0\u793a\u66ff\u4ee3\u5916\u90e8\u9a8c\u8bc1\u5668\u3002", "result": "\u5728\u4e09\u4e2adLLMs\uff08LLaDA 8B Instruct\u3001Dream 7B Instruct\u3001LLaDA 2.0-mini\uff09\u7684\u56db\u4e2a\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPrism\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\uff0c\u4ee5\u663e\u8457\u66f4\u5c11\u7684\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u5339\u914d\u6700\u4f73N\u90091\u6027\u80fd\u3002", "conclusion": "Prism\u4e3a\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u56de\u5f52\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8edLLMs\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "topic": "code agent"}}
{"id": "2602.01915", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01915", "abs": "https://arxiv.org/abs/2602.01915", "authors": ["Elad Sharony", "Tom Jurgenson", "Orr Krupnik", "Dotan Di Castro", "Shie Mannor"], "title": "VLM-Guided Experience Replay", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/", "AI": {"tldr": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\uff0c\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u56de\u653e\u7f13\u51b2\u533a\u4e2d\u7ecf\u9a8c\u7684\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u548c\u6837\u672c\u6548\u7387", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5df2\u88ab\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u7684\u5404\u4e2a\u7ec4\u4ef6\u4e2d\uff0c\u4f46\u56de\u653e\u7f13\u51b2\u533a\u8fd9\u4e00\u5b58\u50a8\u548c\u91cd\u7528\u7ecf\u9a8c\u7684\u6838\u5fc3\u7ec4\u4ef6\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5229\u7528VLM\u6307\u5bfc\u7ecf\u9a8c\u4f18\u5148\u7ea7\u6392\u5e8f", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3VLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\uff0c\u8bc6\u522b\u548c\u4f18\u5148\u5904\u7406\u667a\u80fd\u4f53\u7ecf\u9a8c\u4e2d\u6709\u524d\u666f\u7684\u5b50\u8f68\u8ff9\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5fae\u8c03VLM\uff0c\u53ef\u5e94\u7528\u4e8e\u79bb\u6563\u548c\u8fde\u7eed\u9886\u57df", "result": "\u5728\u6e38\u620f\u548c\u673a\u5668\u4eba\u7b49\u573a\u666f\u4e2d\uff0c\u4f7f\u7528\u8be5\u4f18\u5148\u7ea7\u6392\u5e8f\u65b9\u6cd5\u7684\u667a\u80fd\u4f53\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5b9e\u73b0\u4e8611-52%\u7684\u5e73\u5747\u6210\u529f\u7387\u63d0\u5347\uff0c\u6837\u672c\u6548\u7387\u63d0\u9ad8\u4e8619-45%", "conclusion": "VLM\u53ef\u4ee5\u6709\u6548\u5730\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u56de\u653e\u7f13\u51b2\u533a\u7684\u7ecf\u9a8c\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u548c\u5b66\u4e60\u6548\u7387\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u6a21\u6001\u6a21\u578b\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411", "topic": "agentic reinforcement learning"}}
{"id": "2602.01962", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.01962", "abs": "https://arxiv.org/abs/2602.01962", "authors": ["Arip Asadulaev", "Maksim Bobrin", "Salem Lahlou", "Dmitry Dylov", "Fakhri Karray", "Martin Takac"], "title": "Zero-Shot Off-Policy Learning", "comment": null, "summary": "Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u7acb\u540e\u7ee7\u5ea6\u91cf\u4e0e\u5e73\u7a33\u5bc6\u5ea6\u6bd4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u548c\u503c\u51fd\u6570\u9ad8\u4f30\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u667a\u80fd\u4f53\u9700\u8981\u5728\u6d4b\u8bd5\u65f6\u76f4\u63a5\u9002\u5e94\u65b0\u4efb\u52a1\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "method": "\u53d1\u73b0\u540e\u7ee7\u5ea6\u91cf\u4e0e\u5e73\u7a33\u5bc6\u5ea6\u6bd4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u5229\u7528\u8be5\u6d1e\u5bdf\u63a8\u5bfc\u6700\u4f18\u91cd\u8981\u6027\u91c7\u6837\u6bd4\uff0c\u5b9e\u73b0\u5e73\u7a33\u5206\u5e03\u6821\u6b63\uff0c\u53ef\u5373\u65f6\u4e3a\u4efb\u4f55\u4efb\u52a1\u751f\u6210\u6700\u4f18\u7b56\u7565\u3002", "result": "\u5728SMPL Humanoid\u8fd0\u52a8\u8ddf\u8e2a\u3001ExoRL\u8fde\u7eed\u63a7\u5236\u548c\u957f\u65f6\u57dfOGBench\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65b9\u6cd5\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u524d\u5411-\u540e\u5411\u8868\u793a\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8fde\u63a5\u4e86\u79bb\u7ebf\u5b66\u4e60\u548c\u96f6\u6837\u672c\u9002\u5e94\u4e24\u4e2a\u9886\u57df\uff0c\u4e3a\u4e24\u8005\u90fd\u5e26\u6765\u4e86\u76ca\u5904\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u8bad\u7ec3\u81ea\u7531\u673a\u5236\u4e0b\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u7684\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.01966", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01966", "abs": "https://arxiv.org/abs/2602.01966", "authors": ["Hongzhuo Yu", "Fei Zhu", "Guo-Sen Xie", "Ling Shao"], "title": "Self-Consolidation for Self-Evolving Agents", "comment": null, "summary": "While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u81ea\u8fdb\u5316LLM\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u53cd\u601d\u603b\u7ed3\u9519\u8bef\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u81ea\u6574\u5408\u673a\u5236\u5c06\u6587\u672c\u7ecf\u9a8c\u84b8\u998f\u4e3a\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u5b9e\u73b0\u957f\u671f\u8fdb\u5316\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u7f3a\u4e4f\u7ec8\u8eab\u4ea4\u4e92\u8fdb\u5316\u7684\u80fd\u529b\uff0c\u4e3b\u8981\u4f9d\u8d56\u68c0\u7d22\u6210\u529f\u8f68\u8ff9\u4f5c\u4e3a\u6f14\u793a\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1) \u5ffd\u89c6\u5931\u8d25\u5c1d\u8bd5\u7684\u6559\u5b66\u4ef7\u503c\uff1b2) \u6301\u7eed\u79ef\u7d2f\u6587\u672c\u7ecf\u9a8c\u589e\u52a0\u68c0\u7d22\u65f6\u95f4\u3001\u5f15\u5165\u566a\u58f0\u5e76\u8017\u5c3d\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "method": "\u63d0\u51fa\u81ea\u8fdb\u5316\u6846\u67b6\uff1a1) \u5bf9\u6bd4\u53cd\u601d\u7b56\u7565\uff0c\u660e\u786e\u603b\u7ed3\u6613\u9519\u6a21\u5f0f\u5e76\u6355\u6349\u53ef\u590d\u7528\u89c1\u89e3\uff1b2) \u81ea\u6574\u5408\u673a\u5236\uff0c\u5c06\u975e\u53c2\u6570\u5316\u6587\u672c\u7ecf\u9a8c\u84b8\u998f\u4e3a\u7d27\u51d1\u7684\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u5c06\u5386\u53f2\u7ecf\u9a8c\u5185\u5316\u5230\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u957f\u671f\u667a\u80fd\u4f53\u8fdb\u5316\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u8fdb\u5316\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u53cd\u601d\u548c\u53c2\u6570\u5316\u6574\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LLM\u667a\u80fd\u4f53\u7f3a\u4e4f\u7ec8\u8eab\u8fdb\u5316\u80fd\u529b\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u957f\u671f\u5b66\u4e60\u3002", "topic": "agent analysis"}}
{"id": "2602.02055", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02055", "abs": "https://arxiv.org/abs/2602.02055", "authors": ["Nan Qiao", "Sheng Yue"], "title": "FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification", "comment": "accetped by IEEE International Conference on Communications (ICC 2026)", "summary": "In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $\u03b4$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.", "AI": {"tldr": "FORLER\uff1a\u4e00\u79cd\u79bb\u7ebf\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u670d\u52a1\u5668\u7aef\u7684Q-ensemble\u805a\u5408\u548c\u8bbe\u5907\u7aef\u7684actor rectification\uff0c\u89e3\u51b3\u6570\u636e\u5f02\u6784\u548c\u8d28\u91cf\u4f4e\u4e0b\u5bfc\u81f4\u7684\u7b56\u7565\u6c61\u67d3\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\uff0c\u8054\u90a6\u5b66\u4e60\u5df2\u63a8\u52a8\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u53d1\u5c55\uff0c\u4f46\u4e0e\u73af\u5883\u5728\u7ebf\u4ea4\u4e92\u5b58\u5728\u98ce\u9669\u548c\u6210\u672c\u3002\u79bb\u7ebf\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u5141\u8bb8\u8bbe\u5907\u4ece\u56fa\u5b9a\u6570\u636e\u96c6\u5b66\u4e60\uff0c\u4f46\u5728\u4f4e\u8d28\u91cf\u3001\u5f02\u6784\u6570\u636e\u4e0b\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u4e00\u4e2a\u8bbe\u5907\u7684\u6b21\u4f18\u7b56\u7565\u4f1a\u6c61\u67d3\u805a\u5408\u6a21\u578b\uff08\u7b56\u7565\u6c61\u67d3\u95ee\u9898\uff09\u3002", "method": "FORLER\u7ed3\u5408\u670d\u52a1\u5668\u7aef\u7684Q-ensemble\u805a\u5408\u548c\u8bbe\u5907\u7aef\u7684actor rectification\u3002\u670d\u52a1\u5668\u7a33\u5065\u5730\u5408\u5e76\u8bbe\u5907Q\u51fd\u6570\u4ee5\u6291\u5236\u7b56\u7565\u6c61\u67d3\uff0c\u5c06\u8ba1\u7b97\u8d1f\u62c5\u4ece\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u8f6c\u79fb\u3002\u8bbe\u5907\u7aef\u901a\u8fc7\u96f6\u9636\u641c\u7d22\u9ad8Q\u503c\u52a8\u4f5c\u548c\u5b9a\u5236\u6b63\u5219\u5316\u5668\u6765\u4e30\u5bcc\u7b56\u7565\u68af\u5ea6\uff0c\u91c7\u7528\u03b4-\u5468\u671f\u6027\u7b56\u7565\u8fdb\u4e00\u6b65\u51cf\u5c11\u672c\u5730\u8ba1\u7b97\u3002", "result": "\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u5b89\u5168\u7b56\u7565\u6539\u8fdb\u6027\u80fd\u4fdd\u8bc1\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u6570\u636e\u8d28\u91cf\u548c\u5f02\u6784\u6027\u6761\u4ef6\u4e0b\uff0cFORLER\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FORLER\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u6c61\u67d3\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u805a\u5408\u548c\u4f18\u5316\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u9690\u79c1\u548c\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02061", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02061", "abs": "https://arxiv.org/abs/2602.02061", "authors": ["Seoungbin Bae", "Junyoung Son", "Dabeen Lee"], "title": "Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits", "comment": null, "summary": "Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit\" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit\" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\\widetilde{\\mathcal{O}}(\\sqrt{t})$ for routing and a queue length regret of $\\widetilde{\\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e0a\u4e0b\u6587\u6392\u961f\u591a\u81c2\u8001\u864e\u673a\u4e0e\u591a\u9879Logit\u53cd\u9988\u7684\u6846\u67b6\uff0c\u5f00\u53d1\u8054\u5408\u8def\u7531\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5229\u7528\u7528\u6237\u91cd\u8bd5\u884c\u4e3a\u7684\u9690\u5f0f\u53cd\u9988\u4f18\u5316LLM\u670d\u52a1\u6548\u7387", "motivation": "LLM\u670d\u52a1\u4e2d\u7528\u6237\u67e5\u8be2\u5728\u670d\u52a1\u5668\u961f\u5217\u4e2d\u79ef\u7d2f\uff0c\u73b0\u6709\u5728\u7ebf\u7b97\u6cd5\u5ffd\u89c6\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4e0d\u6ee1\u610f\u7684\u7528\u6237\u4f1a\u91cd\u8bd5\u67e5\u8be2\u589e\u52a0\u670d\u52a1\u5668\u79ef\u538b\uff0c\u4ee5\u53ca\u663e\u5f0f\u53cd\u9988\u8bf7\u6c42\u4f1a\u964d\u4f4e\u7528\u6237\u4f53\u9a8c", "method": "\u63d0\u51faCQB-MNL\u6846\u67b6\u5efa\u6a21\u67e5\u8be2\u91cd\u8bd5\u548c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u7528\u6237\u504f\u597d\u5b66\u4e60\uff0c\u5f00\u53d1ACQB\u7b97\u6cd5\u7ed3\u5408Thompson\u91c7\u6837\u548c\u8870\u51cf\u7387\u5f3a\u5236\u63a2\u7d22\uff0c\u5728\u4fdd\u6301\u961f\u5217\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60", "result": "ACQB\u7b97\u6cd5\u5728\u8def\u7531\u65b9\u9762\u5b9e\u73b0\u7d2f\u8ba1\u9057\u61beO\u0303(\u221at)\uff0c\u5728\u961f\u5217\u957f\u5ea6\u65b9\u9762\u5b9e\u73b0\u9057\u61beO\u0303(t^{-1/4})\u3002\u5728SPROUT\u3001EmbedLLM\u548cRouterBench\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u7b97\u6cd5\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf", "conclusion": "\u901a\u8fc7\u5229\u7528\u7528\u6237\u91cd\u8bd5\u884c\u4e3a\u7684\u9690\u5f0f\u53cd\u9988\uff0c\u63d0\u51fa\u7684\u8054\u5408\u8def\u7531\u8c03\u5ea6\u7b97\u6cd5\u80fd\u6709\u6548\u4f18\u5316LLM\u670d\u52a1\u6548\u7387\uff0c\u540c\u65f6\u907f\u514d\u663e\u5f0f\u53cd\u9988\u5bf9\u7528\u6237\u4f53\u9a8c\u7684\u8d1f\u9762\u5f71\u54cd", "topic": "agent analysis"}}
{"id": "2602.02098", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02098", "abs": "https://arxiv.org/abs/2602.02098", "authors": ["Yannik Schnitzer", "Mathias Jackermeier", "Alessandro Abate", "David Parker"], "title": "Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning", "comment": null, "summary": "Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e3a\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u65b0\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u4f9b\u9ad8\u7f6e\u4fe1\u5ea6\u4fdd\u8bc1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u5408\u4efb\u52a1\u7ea7\u6cdb\u5316\u8fb9\u754c\u548c\u6bcf\u4efb\u52a1\u7f6e\u4fe1\u4e0b\u754c\u3002", "motivation": "\u73b0\u6709\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u5f62\u5f0f\u5316\u6027\u80fd\u4fdd\u8bc1\uff0c\u8fd9\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u90e8\u7f72\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u4e3a\u8bad\u7ec3\u4e2d\u672a\u89c1\u4efb\u52a1\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "method": "\u5f15\u5165\u65b0\u7684\u6cdb\u5316\u8fb9\u754c\uff0c\u7ec4\u5408\u4e24\u4e2a\u90e8\u5206\uff1a(1) \u4ece\u6709\u9650\u6b21rollout\u5f97\u5230\u7684\u6bcf\u4efb\u52a1\u7f6e\u4fe1\u4e0b\u754c\uff1b(2) \u4ece\u6709\u9650\u91c7\u6837\u4efb\u52a1\u5f97\u5230\u7684\u4efb\u52a1\u7ea7\u6cdb\u5316\u8fb9\u754c\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u610f\u672a\u77e5\u4efb\u52a1\u5206\u5e03\u3002", "result": "\u5728\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u4efb\u52a1RL\u65b9\u6cd5\u4e0a\u9a8c\u8bc1\uff0c\u8bc1\u660e\u8be5\u4fdd\u8bc1\u5728\u7406\u8bba\u4e0a\u662f\u53ef\u9760\u7684\uff0c\u5e76\u4e14\u5728\u73b0\u5b9e\u6837\u672c\u91cf\u4e0b\u5177\u6709\u4fe1\u606f\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u4e3a\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u65b0\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u63d0\u4f9b\u5f62\u5f0f\u5316\u3001\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u4fdd\u8bc1\uff0c\u586b\u8865\u4e86\u5b89\u5168\u5173\u952e\u90e8\u7f72\u4e2d\u7684\u9a8c\u8bc1\u7a7a\u767d\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02137", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.02137", "abs": "https://arxiv.org/abs/2602.02137", "authors": ["Minghao Li", "Ruihang Wang", "Rui Tan", "Yonggang Wen"], "title": "DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations", "comment": null, "summary": "Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.", "AI": {"tldr": "DCoPilot\uff1a\u7ed3\u5408LLM\u7b26\u53f7\u751f\u6210\u5956\u52b1\u51fd\u6570\u548c\u8d85\u7f51\u7edc\u53c2\u6570\u751f\u6210\u7b56\u7565\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u6570\u636e\u4e2d\u5fc3\u63a7\u5236\u7b56\u7565\u751f\u6210\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u548c\u4f4e\u7ea6\u675f\u8fdd\u53cd", "motivation": "\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u8fd0\u884c\u5728\u9ad8\u529f\u7387\u5bc6\u5ea6\u4e14\u8d1f\u8f7d\u5feb\u901f\u53d8\u5316\u7684\u73af\u5883\u4e2d\uff0c\u9700\u8981\u5206\u949f\u7ea7\u9002\u5e94\u3002\u4f20\u7edf\u624b\u52a8\u8bbe\u8ba1\u7684DRL\u4ee3\u7406\u65e0\u6cd5\u8ddf\u4e0a\u9891\u7e41\u7684\u52a8\u6001\u53d8\u5316\u548cSLA\u53d8\u66f4\uff0c\u5bfc\u81f4\u89c4\u8303\u5230\u7b56\u7565\u7684\u6ede\u540e\uff0c\u53ef\u80fd\u5f15\u53d1\u670d\u52a1\u4e2d\u65ad\u3002", "method": "DCoPilot\u7ed3\u5408\u4e24\u79cd\u751f\u6210\u8303\u5f0f\uff1a1) LLM\u8fdb\u884c\u7ed3\u6784\u5316\u5956\u52b1\u5f62\u5f0f\u7684\u7b26\u53f7\u751f\u6210\uff1b2) \u8d85\u7f51\u7edc\u8fdb\u884c\u7b56\u7565\u6743\u91cd\u7684\u53c2\u6570\u751f\u6210\u3002\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a\u6a21\u62df\u6269\u5c55\uff08\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u538b\u529b\u6d4b\u8bd5\u5956\u52b1\u5019\u9009\uff09\u3001\u5143\u7b56\u7565\u84b8\u998f\uff08\u8bad\u7ec3\u8d85\u7f51\u7edc\u8f93\u51fa\u57fa\u4e8eSLA\u548c\u573a\u666f\u5d4c\u5165\u7684\u7b56\u7565\u6743\u91cd\uff09\u3001\u5728\u7ebf\u9002\u5e94\uff08\u5b9e\u73b0\u96f6\u6837\u672c\u7b56\u7565\u751f\u6210\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u63a7\u5236\u4efb\u52a1\u5bb6\u65cf\uff08\u6db5\u76d6\u4e0d\u540cDC\u7ec4\u4ef6\uff09\u7684\u8bc4\u4f30\u4e2d\uff0cDCoPilot\u5b9e\u73b0\u4e86\u63a5\u8fd1\u96f6\u7684\u7ea6\u675f\u8fdd\u53cd\uff0c\u5e76\u5728\u6240\u6709\u89c4\u8303\u53d8\u5316\u4e2d\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u57fa\u4e8eLLM\u7684\u7edf\u4e00\u5956\u52b1\u751f\u6210\u5728\u5b9e\u73b0\u7a33\u5b9a\u8d85\u7f51\u7edc\u6536\u655b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "DCoPilot\u901a\u8fc7\u7ed3\u5408LLM\u7684\u7b26\u53f7\u751f\u6210\u80fd\u529b\u548c\u8d85\u7f51\u7edc\u7684\u53c2\u6570\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u52a8\u6001\u6570\u636e\u4e2d\u5fc3\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u751f\u6210\u63a7\u5236\u7b56\u7565\u6846\u67b6\uff0c\u80fd\u591f\u5feb\u901f\u9002\u5e94\u89c4\u8303\u53d8\u5316\uff0c\u786e\u4fdd\u5b89\u5168\u9ad8\u6548\u7684\u8fd0\u884c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02192", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.02192", "abs": "https://arxiv.org/abs/2602.02192", "authors": ["Jie Xiao", "Meng Chen", "Qingnan Ren", "Song Jingwei", "Jiaqi Huang", "Yangshen Deng", "Chris Tong", "Wanyi Chen", "Suli Wang", "Ziqian Bi", "Shuo Lu", "Yiqun Duan", "Lynn Ai", "Eric Yang", "Bill Shi"], "title": "ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning", "comment": "23 pages, 7 figures", "summary": "Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.", "AI": {"tldr": "ECHO-2\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\uff0c\u901a\u8fc7\u8fdc\u7a0b\u63a8\u7406\u5de5\u4f5c\u8282\u70b9\u548c\u91cd\u53e0\u7b56\u7565\u4f20\u64ad\u6765\u63d0\u5347\u6210\u672c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfRL\u540e\u8bad\u7ec3\u9700\u8981\u91cd\u590d\u7684\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u5305\u62ec\u751f\u6210\u3001\u5956\u52b1\u8bc4\u4f30\u548c\u96c6\u4e2d\u5b66\u4e60\u3002\u867d\u7136\u5206\u5e03\u5f0f\u6267\u884c\u751f\u6210\u53ef\u4ee5\u5229\u7528\u6210\u672c\u66f4\u4f4e\u7684\u63a8\u7406\u8d44\u6e90\uff0c\u4f46\u5728\u5e7f\u57df\u534f\u8c03\u548c\u7b56\u7565\u4f20\u64ad\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5b58\u5728\u663e\u8457\u4f20\u64ad\u5ef6\u8fdf\u65f6\u3002", "method": "ECHO-2\u7ed3\u5408\u96c6\u4e2d\u5b66\u4e60\u4e0e\u5206\u5e03\u5f0f\u751f\u6210\uff0c\u5c06\u7b56\u7565\u8fc7\u65f6\u6027\u4f5c\u4e3a\u7528\u6237\u53ef\u63a7\u53c2\u6570\uff0c\u5141\u8bb8\u751f\u6210\u3001\u4f20\u64ad\u548c\u8bad\u7ec3\u91cd\u53e0\u6267\u884c\u3002\u5f15\u5165\u57fa\u4e8e\u91cd\u53e0\u7684\u5bb9\u91cf\u6a21\u578b\u6765\u5173\u8054\u8bad\u7ec3\u65f6\u95f4\u3001\u4f20\u64ad\u5ef6\u8fdf\u548c\u751f\u6210\u541e\u5410\u91cf\uff0c\u5e76\u63d0\u4f9b\u5b9e\u7528\u7684\u8d44\u6e90\u914d\u7f6e\u89c4\u5219\u3002\u91c7\u7528\u5bf9\u7b49\u8f85\u52a9\u6d41\u6c34\u7ebf\u5e7f\u64ad\u548c\u6210\u672c\u611f\u77e5\u7684\u5f02\u6784\u5de5\u4f5c\u8282\u70b9\u6fc0\u6d3b\u6765\u7f13\u89e3\u4f20\u64ad\u74f6\u9888\u3002", "result": "\u5728\u771f\u5b9e\u5e7f\u57df\u5e26\u5bbd\u73af\u5883\u4e0b\u5bf94B\u548c8B\u6a21\u578b\u8fdb\u884cGRPO\u540e\u8bad\u7ec3\u7684\u5b9e\u9a8c\u8868\u660e\uff0cECHO-2\u5728\u4fdd\u6301\u4e0e\u5f3a\u57fa\u7ebf\u76f8\u5f53\u7684RL\u5956\u52b1\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u672c\u6548\u7387\u3002", "conclusion": "ECHO-2\u901a\u8fc7\u6709\u6548\u7684\u5206\u5e03\u5f0f\u67b6\u6784\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u540e\u8bad\u7ec3RL\u4e2d\u7684\u5e7f\u57df\u534f\u8c03\u548c\u7b56\u7565\u4f20\u64ad\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6210\u672c\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02206", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02206", "abs": "https://arxiv.org/abs/2602.02206", "authors": ["Tong Yang", "Yemin Wang", "Chaoning Zhang", "Aming Wu"], "title": "Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning", "comment": null, "summary": "The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.", "AI": {"tldr": "Fat-Cat\u63d0\u51fa\u57fa\u4e8e\u6587\u6863\u7684\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u7528Markdown\u6587\u6863\u66ff\u4ee3\u4f20\u7edfJSON\u72b6\u6001\u8868\u793a\uff0c\u901a\u8fc7\u8bed\u4e49\u6587\u4ef6\u7cfb\u7edf\u3001\u6587\u672c\u7b56\u7565\u8fdb\u5316\u548c\u95ed\u73af\u76d1\u63a7\u63d0\u5347\u4e0a\u4e0b\u6587\u4fe1\u606f\u5229\u7528\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u6846\u67b6\u4f9d\u8d56\u5d4c\u5957JSON\u7b49\u521a\u6027\u8bed\u6cd5\u72b6\u6001\u8868\u793a\uff0c\u8feb\u4f7f\u6a21\u578b\u5c06\u6709\u9650\u6ce8\u610f\u529b\u6d6a\u8d39\u5728\u8bed\u6cd5\u5904\u7406\u800c\u975e\u8bed\u4e49\u63a8\u7406\u4e0a\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u6548\u7387\u3002", "method": "1) \u8bed\u4e49\u6587\u4ef6\u7cfb\u7edf\uff1a\u5c06\u667a\u80fd\u4f53\u72b6\u6001\u8868\u793a\u4e3a\u4e0e\u9884\u8bad\u7ec3\u8bed\u6599\u5bf9\u9f50\u7684Markdown\u6587\u6863\uff1b2) \u6587\u672c\u7b56\u7565\u8fdb\u5316\uff1a\u79ef\u7d2f\u4efb\u52a1\u89e3\u51b3\u77e5\u8bc6\u800c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\uff1b3) \u95ed\u73af\u76d1\u63a7\u5668\uff1a\u76d1\u63a7\u63a8\u7406\u8f68\u8ff9\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5728\u63a8\u7406\u3001\u68c0\u7d22\u548c\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f7fKimi-k2\u6a21\u578b\u5728HotPotQA\u4e0a\u8d85\u8d8aGPT-4o\u57fa\u7ebf\u3002\u5b9e\u9a8c\u8868\u660e\u6587\u6863\u9a71\u52a8\u72b6\u6001\u5efa\u6a21\u6bd4JSON\u8868\u793a\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u6587\u6863\u9a71\u52a8\u7684\u72b6\u6001\u7ba1\u7406\u80fd\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u901a\u8fc7\u63d0\u9ad8\u4fe1\u53f7\u566a\u58f0\u6bd4\u8ba9\u6a21\u578b\u66f4\u4e13\u6ce8\u4e8e\u8bed\u4e49\u63a8\u7406\u800c\u975e\u8bed\u6cd5\u5904\u7406\u3002", "topic": "code agent"}}
{"id": "2602.02259", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.02259", "abs": "https://arxiv.org/abs/2602.02259", "authors": ["Hamza Adnan", "Matthew T. Jackson", "Alexey Zakharov"], "title": "Segment to Focus: Guiding Latent Action Models in the Presence of Distractors", "comment": null, "summary": "Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.", "AI": {"tldr": "MaskLAM\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u667a\u80fd\u4f53\u5206\u5272\u6765\u6539\u8fdb\u6f5c\u5728\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5272\u63a9\u7801\u52a0\u6743\u91cd\u5efa\u635f\u5931\uff0c\u4f18\u5148\u5904\u7406\u663e\u8457\u4fe1\u606f\u800c\u975e\u80cc\u666f\u566a\u58f0\uff0c\u5728\u5b58\u5728\u52a8\u4f5c\u76f8\u5173\u80cc\u666f\u566a\u58f0\u7684\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6f5c\u5728\u52a8\u4f5c\u6a21\u578b(LAMs)\u80fd\u591f\u4ece\u539f\u59cb\u89c2\u5bdf\u4e2d\u5b66\u4e60\u63d0\u53d6\u52a8\u4f5c\u76f8\u5173\u8868\u793a\uff0c\u4f46\u9762\u4e34\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff1a\u96be\u4ee5\u5c06\u52a8\u4f5c\u76f8\u5173\u7279\u5f81\u4e0e\u52a8\u4f5c\u76f8\u5173\u566a\u58f0\uff08\u5982\u80cc\u666f\u8fd0\u52a8\uff09\u89e3\u8026\u3002\u5982\u679c\u65e0\u6cd5\u8fc7\u6ee4\u8fd9\u4e9b\u5e72\u6270\u56e0\u7d20\uff0cLAMs\u4f1a\u6355\u6349\u865a\u5047\u76f8\u5173\u6027\u5e76\u6784\u5efa\u6b21\u4f18\u7684\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u3002", "method": "MaskLAM\u662f\u5bf9LAM\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u4fee\u6539\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u667a\u80fd\u4f53\u5206\u5272\u6765\u7f13\u89e3\u6b64\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u5206\u5272\u63a9\u7801\u6765\u52a0\u6743LAM\u91cd\u5efa\u635f\u5931\uff0c\u4ece\u800c\u4f18\u5148\u5904\u7406\u663e\u8457\u4fe1\u606f\u800c\u975e\u80cc\u666f\u5143\u7d20\uff0c\u4e14\u65e0\u9700\u67b6\u6784\u4fee\u6539\u3002", "result": "\u5728\u6dfb\u52a0\u4e86\u52a8\u4f5c\u76f8\u5173\u80cc\u666f\u566a\u58f0\u7684\u8fde\u7eed\u63a7\u5236MuJoCo\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6\u57fa\u7ebf\u83b7\u5f97\u4e86\u9ad8\u8fbe4\u500d\u7684\u7d2f\u79ef\u5956\u52b1\u63d0\u5347\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u63a2\u9488\u8bc4\u4f30\u663e\u793a\u6f5c\u5728\u52a8\u4f5c\u8d28\u91cf\u63d0\u9ad8\u4e863\u500d\u3002", "conclusion": "MaskLAM\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u5206\u5272\u4fe1\u606f\u6709\u6548\u89e3\u51b3\u4e86LAMs\u4e2d\u7684\u52a8\u4f5c\u76f8\u5173\u566a\u58f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u7684\u8d28\u91cf\uff0c\u4e3a\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u4e2d\u5b66\u4e60\u52a8\u4f5c\u8868\u793a\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02260", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02260", "abs": "https://arxiv.org/abs/2602.02260", "authors": ["Zhengjia Zhuo", "Anupam Gupta", "Viswanath Nagarajan"], "title": "Learning Markov Decision Processes under Fully Bandit Feedback", "comment": null, "summary": "A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $\u0398(\\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \\emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\\widetilde{O}(\\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u9488\u5bf9\u5b8c\u5168bandit\u53cd\u9988\u7684episodic MDP\u9ad8\u6548\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b9e\u73b0$\\widetilde{O}(\\sqrt{T})$\u9057\u61be\u754c\uff0c\u5e76\u5728k-item\u5148\u77e5\u4e0d\u7b49\u5f0f\u7b49\u7ecf\u5178\u968f\u673a\u4f18\u5316\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfRL\u5047\u8bbe\u667a\u80fd\u4f53\u80fd\u89c2\u6d4b\u6bcf\u4e2a\u72b6\u6001-\u52a8\u4f5c\u5bf9\u53ca\u5373\u65f6\u5956\u52b1\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u53cd\u9988\u5f80\u5f80\u53d7\u9650\u3002\u672c\u6587\u7814\u7a76\u66f4\u4e25\u683c\u7684\"\u5b8c\u5168bandit\"\u53cd\u9988\u6a21\u578b\uff0c\u667a\u80fd\u4f53\u4ec5\u80fd\u89c2\u6d4b\u805a\u5408\u5956\u52b1\uff0c\u65e0\u6cd5\u89c2\u6d4b\u8bbf\u95ee\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u8fd9\u66f4\u8d34\u8fd1\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u3002", "method": "\u63d0\u51fa\u9996\u4e2a\u9488\u5bf9episodic MDP\u7684\u5b8c\u5168bandit\u53cd\u9988\u9ad8\u6548\u5b66\u4e60\u7b97\u6cd5\u3002\u7b97\u6cd5\u8bbe\u8ba1\u8003\u8651\u4e86horizon\u957f\u5ea6\u7684\u6307\u6570\u4f9d\u8d56\u5173\u7cfb\uff08\u8bc1\u660e\u8fd9\u662f\u5fc5\u8981\u7684\uff09\uff0c\u5e76\u4e3a\"\u6709\u5e8f\"MDP\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u7d27\u81f4\u9057\u61be\u754c\uff0c\u53ef\u5e94\u7528\u4e8ek-item\u5148\u77e5\u4e0d\u7b49\u5f0f\u548c\u5e8f\u5217\u5b9a\u4ef7\u7b49\u7ecf\u5178\u968f\u673a\u4f18\u5316\u95ee\u9898\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0$\\widetilde{O}(\\sqrt{T})$\u9057\u61be\u754c\uff0c\u5bf9horizon\u957f\u5ea6\u6709\u6307\u6570\u4f9d\u8d56\uff08\u8bc1\u660e\u662f\u5fc5\u8981\u7684\uff09\u3002\u5728k-item\u5148\u77e5\u4e0d\u7b49\u5f0f\u95ee\u9898\u4e0a\uff0c\u5c3d\u7ba1\u53cd\u9988\u9ad8\u5ea6\u53d7\u9650\uff0c\u7b97\u6cd5\u6027\u80fd\u4e0e\u5177\u6709\u8be6\u7ec6\u72b6\u6001-\u52a8\u4f5c\u53cd\u9988\u7684\u5148\u8fdb\u5b66\u4e60\u7b97\u6cd5\uff08UCB-VI\uff09\u76f8\u5f53\u3002", "conclusion": "\u5373\u4f7f\u5728\u9ad8\u5ea6\u53d7\u9650\u7684\u5b8c\u5168bandit\u53cd\u9988\u4e0b\uff0c\u4ecd\u80fd\u8bbe\u8ba1\u51fa\u9ad8\u6548\u7684RL\u7b97\u6cd5\uff0c\u4e3a\u73b0\u5b9e\u5e94\u7528\u4e2d\u53cd\u9988\u53d7\u9650\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u7b97\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02395", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.02395", "abs": "https://arxiv.org/abs/2602.02395", "authors": ["Samuel Nellessen", "Tal Kachman"], "title": "David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning", "comment": "Under review. 8 main pages, 2 figures, 2 tables. Appendix included", "summary": "The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary \"tags along\" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.", "AI": {"tldr": "\u63d0\u51faTag-Along\u653b\u51fb\u5a01\u80c1\u6a21\u578b\uff1a\u65e0\u5de5\u5177\u653b\u51fb\u8005\u901a\u8fc7\u5bf9\u8bdd\"\u642d\u4fbf\u8f66\"\u5229\u7528\u5b89\u5168\u5bf9\u9f50\u64cd\u4f5c\u5458\u7684\u5de5\u5177\u6743\u9650\uff0c\u8bf1\u5bfc\u5176\u8fdb\u884c\u7981\u6b62\u7684\u5de5\u5177\u4f7f\u7528\u3002\u5f00\u53d1Slingshot\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u81ea\u52a8\u53d1\u73b0\u653b\u51fb\u5411\u91cf\uff0c\u5728\u6781\u7aef\u96be\u5ea6\u4efb\u52a1\u4e0a\u8fbe\u523067%\u6210\u529f\u7387\uff0c\u5e76\u80fd\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u65f6\uff0c\u4f1a\u9762\u4e34\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u5229\u7528\u5408\u6cd5\u5de5\u5177\u6743\u9650\u8fdb\u884c\u653b\u51fb\u3002\u4f20\u7edf\u5b89\u5168\u8bc4\u4f30\u5728\u5de5\u5177\u589e\u5f3a\u73af\u5883\u4e2d\u4ece\u4e3b\u89c2NLP\u4efb\u52a1\u8f6c\u53d8\u4e3a\u5ba2\u89c2\u63a7\u5236\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u5a01\u80c1\u6a21\u578b\u6765\u8bc4\u4f30\u8fd9\u79cd\u98ce\u9669\u3002", "method": "\u63d0\u51faTag-Along\u653b\u51fb\u5a01\u80c1\u6a21\u578b\uff0c\u5e76\u5f00\u53d1Slingshot\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u6846\u67b6\u91c7\u7528\"\u51b7\u542f\u52a8\"\u65b9\u5f0f\uff0c\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u81ea\u52a8\u53d1\u73b0\u653b\u51fb\u5411\u91cf\uff0c\u800c\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u653b\u51fb\u7b56\u7565\u3002\u653b\u51fb\u5411\u91cf\u503e\u5411\u4e8e\u6536\u655b\u5230\u7b80\u77ed\u3001\u6307\u4ee4\u5f0f\u7684\u53e5\u6cd5\u6a21\u5f0f\u800c\u975e\u591a\u8f6e\u8bf4\u670d\u3002", "result": "\u5728\u6781\u7aef\u96be\u5ea6\u4efb\u52a1\u4e0a\uff0cSlingshot\u5bf9Qwen2.5-32B-Instruct-AWQ\u64cd\u4f5c\u5458\u8fbe\u523067.0%\u653b\u51fb\u6210\u529f\u7387\uff08\u57fa\u7ebf\u4ec51.7%\uff09\uff0c\u9996\u6b21\u6210\u529f\u6240\u9700\u5c1d\u8bd5\u6b21\u6570\u4ece52.3\u964d\u81f31.3\u3002\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u5176\u4ed6\u6a21\u578b\u5bb6\u65cf\uff1aGemini 2.5 Flash\u8fbe\u523056.0%\u6210\u529f\u7387\uff0cMeta-SecAlign-8B\u8fbe\u523039.2%\u6210\u529f\u7387\u3002", "conclusion": "Tag-Along\u653b\u51fb\u662f\u53ef\u9a8c\u8bc1\u7684\u4e00\u7ea7\u5a01\u80c1\u6a21\u578b\uff0c\u8868\u660e\u6709\u6548\u7684\u4ee3\u7406\u653b\u51fb\u53ef\u4ee5\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u4ece\u73b0\u6210\u7684\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u4e2d\u5f15\u53d1\uff0c\u65e0\u9700\u591a\u8f6e\u8bf4\u670d\uff0c\u7b80\u77ed\u6307\u4ee4\u6a21\u5f0f\u5c31\u8db3\u591f\u6709\u6548\u3002", "topic": "agent analysis"}}
{"id": "2602.02295", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02295", "abs": "https://arxiv.org/abs/2602.02295", "authors": ["Shaima Ahmad Freja", "Ferhat Ozgur Catak", "Betul Yurdem", "Chunming Rong"], "title": "EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models", "comment": "15 pages (including appendix), 11 figures", "summary": "Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.", "AI": {"tldr": "EvalQReason\u662f\u4e00\u4e2a\u901a\u8fc7\u6b65\u9aa4\u7ea7\u6982\u7387\u5206\u5e03\u5206\u6790\u91cf\u5316LLM\u63a8\u7406\u8d28\u91cf\u7684\u6846\u67b6\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u4f7f\u7528CSD\u548cSFC\u7b97\u6cd5\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\uff0c\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u4e2d\u95f4\u6b65\u9aa4\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\uff0c\u96be\u4ee5\u6df1\u5165\u4e86\u89e3LLM\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faEvalQReason\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u7b97\u6cd5\uff1aCSD\uff08\u8fde\u7eed\u6b65\u9aa4\u5206\u6b67\uff09\u6d4b\u91cf\u76f8\u90bb\u63a8\u7406\u6b65\u9aa4\u7684\u5c40\u90e8\u4e00\u81f4\u6027\uff0cSFC\uff08\u6b65\u9aa4\u5230\u6700\u7ec8\u6536\u655b\uff09\u8bc4\u4f30\u4e0e\u6700\u7ec8\u7b54\u6848\u7684\u5168\u5c40\u5bf9\u9f50\u3002\u6bcf\u4e2a\u7b97\u6cd5\u4f7f\u7528\u4e94\u4e2a\u7edf\u8ba1\u6307\u6807\u3002", "result": "\u5728\u6570\u5b66\u548c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cCSD\u7279\u5f81\u5728\u6b63\u786e\u6027\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578bF1=0.78\uff0cROC-AUC=0.82\uff0c\u5e8f\u5217\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u663e\u8457\u63d0\u5347\u81f3F1=0.88\uff0cROC-AUC=0.97\u3002CSD\u59cb\u7ec8\u4f18\u4e8eSFC\uff0c\u5e8f\u5217\u67b6\u6784\u4f18\u4e8e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "EvalQReason\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u3001\u8fc7\u7a0b\u611f\u77e5\u7684\u63a8\u7406\u53ef\u9760\u6027\u8bc4\u4f30\uff0c\u5efa\u7acb\u4e86\u57fa\u4e8e\u6982\u7387\u7684\u5206\u6b67\u5206\u6790\u4f5c\u4e3a\u53ef\u4fe1AI\u90e8\u7f72\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002\u63a8\u7406\u52a8\u6001\u5177\u6709\u9886\u57df\u7279\u5f02\u6027\uff1a\u6570\u5b66\u63a8\u7406\u663e\u793a\u6e05\u6670\u7684\u5206\u6b67\u6a21\u5f0f\uff0c\u800c\u533b\u5b66\u63a8\u7406\u5219\u4fe1\u53f7\u8f83\u5f31\u3002", "topic": "agent analysis"}}
{"id": "2602.02427", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02427", "abs": "https://arxiv.org/abs/2602.02427", "authors": ["Qihao Wen", "Jiahao Wang", "Yang Nan", "Pengfei He", "Ravi Tandon", "Han Xu"], "title": "Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning", "comment": null, "summary": "Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6270\u52a8\u654f\u611f\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522bLLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u4e2d\u95f4\u6b65\u9aa4\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "LLM\u867d\u7136\u5728\u5404\u9886\u57df\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u4ecd\u53ef\u80fd\u4ea7\u751f\u4e0d\u53ef\u9760\u6216\u8bef\u5bfc\u6027\u8f93\u51fa\u3002\u5bf9\u4e8e\u63a8\u7406\u4efb\u52a1\uff0c\u4e0d\u4ec5\u9700\u8981\u91cf\u5316\u6700\u7ec8\u7b54\u6848\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd8\u9700\u8981\u91cf\u5316\u4e2d\u95f4\u6b65\u9aa4\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u4fbf\u8fdb\u884c\u66f4\u7cbe\u7ec6\u7684\u5e72\u9884\u3002", "method": "\u901a\u8fc7\u5206\u6790LLM\u9519\u8bef\u63a8\u7406\u6b65\u9aa4\u4e2dtoken\u5bf9\u524d\u5e8ftoken\u5d4c\u5165\u6270\u52a8\u7684\u654f\u611f\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u6270\u52a8\u654f\u611f\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6307\u6807\u3002\u76f8\u6bd4\u4f9d\u8d56\u591a\u6b21\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u66f4\u7b80\u5355\u9ad8\u6548\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u6270\u52a8\u7684\u6307\u6807\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982token\u751f\u6210\u6982\u7387\u548ctoken\u71b5\uff09\u3002\u9519\u8bef\u63a8\u7406\u6b65\u9aa4\u4e2d\u7684token\u5bf9\u524d\u5e8ftoken\u5d4c\u5165\u6270\u52a8\u8868\u73b0\u51fa\u9ad8\u5ea6\u654f\u611f\u6027\u3002", "conclusion": "\u57fa\u4e8e\u6270\u52a8\u654f\u611f\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522bLLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u4e2d\u95f4\u6b65\u9aa4\uff0c\u4e3a\u66f4\u7cbe\u7ec6\u7684\u5e72\u9884\u63d0\u4f9b\u6307\u5bfc\uff0c\u4e14\u5177\u6709\u66f4\u597d\u7684\u7b80\u5355\u6027\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2602.02443", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02443", "abs": "https://arxiv.org/abs/2602.02443", "authors": ["Yuanteng Chen", "Peisong Wang", "Nanxin Zeng", "Yuantian Shao", "Gang Li", "Jing Liu", "Jian Cheng"], "title": "Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE", "comment": "24 pages, 13 figures", "summary": "Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.", "AI": {"tldr": "\u63d0\u51faExpert-Sample\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u9ad8\u7f6e\u4fe1\u5ea6\u4e13\u5bb6\u9009\u62e9\u4e2d\u4fdd\u7559\u786e\u5b9a\u6027\uff0c\u5728\u4e0d\u786e\u5b9a\u5c3e\u90e8\u6ce8\u5165\u53ef\u63a7\u968f\u673a\u6027\uff0c\u63d0\u5347\u7ec6\u7c92\u5ea6MoE\u6a21\u578b\u7684\u591a\u6837\u672c\u63a8\u7406\u6027\u80fd", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u9700\u8981\u6e29\u5ea6\u8c03\u4f18\u6765\u6743\u8861\u591a\u6837\u6027\u4e0e\u7a33\u5b9a\u6027\uff0c\u800c\u7ec6\u7c92\u5ea6MoE\u7684\u4e30\u5bcc\u8def\u7531\u7a7a\u95f4\u63d0\u4f9b\u4e86\u672a\u63a2\u7d22\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7814\u7a76\u53d1\u73b0MoE\u8def\u7531\u5b58\u5728\u786e\u5b9a\u6027\u5934\u90e8\u548c\u4e0d\u786e\u5b9a\u6027\u5c3e\u90e8\u7684\u6a21\u5f0f\uff0c\u524d\u8005\u63a7\u5236\u6838\u5fc3\u63a8\u7406\u80fd\u529b\uff0c\u540e\u8005\u4e0e\u63a8\u7406\u591a\u6837\u6027\u76f8\u5173", "method": "\u63d0\u51faExpert-Sample\u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5\uff1a\u4fdd\u7559\u9ad8\u7f6e\u4fe1\u5ea6\u4e13\u5bb6\u9009\u62e9\uff0c\u5728\u4e0d\u786e\u5b9a\u5c3e\u90e8\u6ce8\u5165\u53ef\u63a7\u968f\u673a\u6027\uff0c\u5b9e\u73b0\u591a\u6837\u751f\u6210\u800c\u4e0d\u7834\u574f\u8f93\u51fa\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u5206\u6790\u7ec6\u7c92\u5ea6MoE\u8def\u7531\u6a21\u5f0f\uff0c\u5229\u7528\u786e\u5b9a\u6027\u5934\u90e8\u548c\u4e0d\u786e\u5b9a\u6027\u5c3e\u90e8\u7684\u7279\u6027", "result": "\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6MoE\u6a21\u578b\u4e0a\u8bc4\u4f30\u6570\u5b66\u3001\u77e5\u8bc6\u63a8\u7406\u548c\u4ee3\u7801\u4efb\u52a1\uff0cExpert-Sample\u4e00\u81f4\u63d0\u5347pass@n\u548c\u57fa\u4e8e\u9a8c\u8bc1\u7684\u51c6\u786e\u7387\u3002Qwen3-30B-A3B-Instruct\u5728GPQA-Diamond\u4e0a\uff0c32\u5e76\u884c\u6837\u672c\u7684pass@32\u4ece85.4%\u63d0\u5347\u81f391.9%\uff0cBest-of-N\u9a8c\u8bc1\u51c6\u786e\u7387\u4ece59.1%\u63d0\u5347\u81f362.6%", "conclusion": "\u7ec6\u7c92\u5ea6MoE\u8def\u7531\u7684\u786e\u5b9a\u6027-\u4e0d\u786e\u5b9a\u6027\u6a21\u5f0f\u4e3a\u591a\u6837\u5316\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0cExpert-Sample\u65b9\u6cd5\u6709\u6548\u5229\u7528\u8fd9\u4e00\u6a21\u5f0f\uff0c\u5728\u4e0d\u7834\u574f\u7a33\u5b9a\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u63a8\u7406\u591a\u6837\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u6e29\u5ea6\u8c03\u4f18\u65b9\u6cd5", "topic": "agent analysis"}}
{"id": "2602.02458", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.02458", "abs": "https://arxiv.org/abs/2602.02458", "authors": ["Mingwei Hong", "Zheng Lin", "Zehang Lin", "Lin Li", "Miao Yang", "Xia Du", "Zihan Fang", "Zhaolu Kang", "Dianxin Luan", "Shunzhi Zhu"], "title": "Conflict-Aware Client Selection for Multi-Server Federated Learning", "comment": "6 pages, 4 figures", "summary": "Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.", "AI": {"tldr": "\u63d0\u51faRL-CRP\u6846\u67b6\uff0c\u4f7f\u7528\u53bb\u4e2d\u5fc3\u5316\u5f3a\u5316\u5b66\u4e60\u548c\u51b2\u7a81\u98ce\u9669\u9884\u6d4b\u6765\u4f18\u5316\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u9009\u62e9\uff0c\u51cf\u5c11\u670d\u52a1\u5668\u95f4\u51b2\u7a81\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387", "motivation": "\u4f20\u7edf\u5355\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u9ad8\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\uff0c\u800c\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u867d\u7136\u80fd\u5206\u5e03\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4f46\u5ba2\u6237\u7aef\u8986\u76d6\u91cd\u53e0\u548c\u9009\u62e9\u4e0d\u534f\u8c03\u4f1a\u5bfc\u81f4\u8d44\u6e90\u7ade\u4e89\u3001\u5e26\u5bbd\u51b2\u7a81\u548c\u8bad\u7ec3\u5931\u8d25", "method": "\u63d0\u51faRL-CRP\u6846\u67b6\uff1a1) \u4f7f\u7528\u5206\u7c7b\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u57fa\u4e8e\u7a00\u758f\u5386\u53f2\u5ba2\u6237\u7aef\u9009\u62e9\u5e8f\u5217\u9884\u6d4b\u51b2\u7a81\u98ce\u9669\uff1b2) \u7ed3\u5408\u516c\u5e73\u611f\u77e5\u5956\u52b1\u673a\u5236\u4fc3\u8fdb\u957f\u671f\u5ba2\u6237\u7aef\u53c2\u4e0e\uff1b3) \u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5ba2\u6237\u7aef\u9009\u62e9", "result": "\u5b9e\u9a8c\u8868\u660eRL-CRP\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u670d\u52a1\u5668\u95f4\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u5305\u62ec\u6536\u655b\u901f\u5ea6\u548c\u901a\u4fe1\u6210\u672c\u65b9\u9762\u7684\u6539\u8fdb", "conclusion": "\u63d0\u51fa\u7684RL-CRP\u6846\u67b6\u901a\u8fc7\u51b2\u7a81\u98ce\u9669\u9884\u6d4b\u548c\u516c\u5e73\u611f\u77e5\u5956\u52b1\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6574\u4f53\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2602.02482", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02482", "abs": "https://arxiv.org/abs/2602.02482", "authors": ["Yuda Song", "Lili Chen", "Fahim Tajwar", "Remi Munos", "Deepak Pathak", "J. Andrew Bagnell", "Aarti Singh", "Andrea Zanette"], "title": "Expanding the Capabilities of Reinforcement Learning via Text Feedback", "comment": "43 pages, 6 figures", "summary": "The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRLTF\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u53cd\u9988\u4f5c\u4e3a\u4ecb\u4e8e\u7a00\u758f\u5956\u52b1\u548c\u5b8c\u6574\u6f14\u793a\u4e4b\u95f4\u7684\u4e2d\u95f4\u76d1\u7763\u4fe1\u53f7\uff0c\u901a\u8fc7\u4e24\u79cd\u65b9\u6cd5\uff08\u81ea\u84b8\u998f\u548c\u53cd\u9988\u5efa\u6a21\uff09\u8ba9\u6a21\u578b\u5185\u5316\u53cd\u9988\u4ee5\u63d0\u5347\u5355\u8f6e\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRLHF\u4ec5\u4f7f\u7528\u4e8c\u5143\u5956\u52b1\u6216\u504f\u597d\u6807\u7b7e\uff0c\u4fe1\u606f\u8fc7\u4e8e\u7a00\u758f\uff1b\u84b8\u998f\u9700\u8981\u5b8c\u6574\u6f14\u793a\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u6587\u672c\u53cd\u9988\u4f5c\u4e3a\u4e2d\u95f4\u4fe1\u53f7\uff0c\u65e2\u6bd4\u6807\u91cf\u5956\u52b1\u66f4\u4e30\u5bcc\uff0c\u53c8\u6bd4\u5b8c\u6574\u6f14\u793a\u66f4\u4fbf\u5b9c\uff0c\u4e14\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5df2\u5927\u91cf\u5b58\u5728\u3002", "method": "\u63d0\u51faRLTF\u6846\u67b6\uff1a1) RLTF-SD\uff08\u81ea\u84b8\u998f\uff09\uff1a\u8bad\u7ec3\u5355\u8f6e\u7b56\u7565\u5339\u914d\u81ea\u8eab\u53cd\u9988\u6761\u4ef6\u4e0b\u7684\u7b2c\u4e8c\u8f6e\u751f\u6210\uff1b2) RLTF-FM\uff08\u53cd\u9988\u5efa\u6a21\uff09\uff1a\u5c06\u9884\u6d4b\u53cd\u9988\u4f5c\u4e3a\u8f85\u52a9\u76ee\u6807\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u65e8\u5728\u8ba9\u6a21\u578b\u5185\u5316\u6587\u672c\u53cd\u9988\u4ee5\u63d0\u5347\u6d4b\u8bd5\u65f6\u5355\u8f6e\u6027\u80fd\u3002", "result": "\u5728\u63a8\u7406\u8c1c\u9898\u3001\u7ade\u8d5b\u6570\u5b66\u548c\u521b\u610f\u5199\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u4e30\u5bcc\u6587\u672c\u53cd\u9988\u8fdb\u884cRL\u7684\u6f5c\u529b\u3002", "conclusion": "\u6587\u672c\u53cd\u9988\u4f5c\u4e3a\u4ecb\u4e8e\u7a00\u758f\u5956\u52b1\u548c\u5b8c\u6574\u6f14\u793a\u4e4b\u95f4\u7684\u4e2d\u95f4\u76d1\u7763\u4fe1\u53f7\uff0c\u901a\u8fc7RLTF\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLM\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u5229\u7528\u4e30\u5bcc\u76d1\u7763\u4fe1\u53f7\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2602.2a7b93b6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.infoq.com%2Farticles%2Fagent-reinforcement-learning-apache-spark%2F%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/Nli8wPfWG7PLJX9fbJfR_9sgXME76Y055WruehsZHBw=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.infoq.com%2Farticles%2Fagent-reinforcement-learning-apache-spark%2F%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/Nli8wPfWG7PLJX9fbJfR_9sgXME76Y055WruehsZHBw=442", "authors": ["TLDR Newsletter"], "title": "Autonomous Big Data Optimization: Multi-Agent Reinforcement Learning to Achieve Self-Tuning Apache Spark", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 19 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.infoq.com%2Farticles%2Fagent-reinforcement-learning-apache-spark%2F%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/Nli8wPfWG7PLJX9fbJfR_9sgXME76Y055WruehsZHBw=442", "summary": "Autonomous Big Data Optimization: Multi-Agent Reinforcement Learning to Achieve Self-Tuning Apache Spark (19 minute read) A reinforcement learning (RL) agent can effectively automate configuration tuning for Apache Spark by dynamically adjusting parameters like shuffle partitions based on real-time dataset characteristics, outperforming both manual tuning and Spark's Adaptive Query Execution (AQE). Combining RL with AQE delivers optimal performance, cutting execution times and resource overhe...", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0Apache Spark\u81ea\u52a8\u8c03\u4f18\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574shuffle\u5206\u533a\u7b49\u53c2\u6570\uff0c\u4f18\u4e8e\u624b\u52a8\u8c03\u4f18\u548cSpark\u81ea\u9002\u5e94\u67e5\u8be2\u6267\u884c(AQE)\uff0c\u7ed3\u5408RL\u4e0eAQE\u53ef\u83b7\u5f97\u6700\u4f18\u6027\u80fd", "motivation": "Apache Spark\u914d\u7f6e\u8c03\u4f18\u901a\u5e38\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\u4e14\u8017\u65f6\uff0c\u624b\u52a8\u8c03\u4f18\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u6570\u636e\u96c6\u7279\u5f81\uff0cSpark\u7684AQE\u867d\u80fd\u81ea\u9002\u5e94\u4f46\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\uff0c\u9700\u8981\u81ea\u52a8\u5316\u3001\u667a\u80fd\u5316\u7684\u914d\u7f6e\u4f18\u5316\u65b9\u6848", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8ba9RL\u667a\u80fd\u4f53\u57fa\u4e8e\u5b9e\u65f6\u6570\u636e\u96c6\u7279\u5f81\u52a8\u6001\u8c03\u6574Spark\u914d\u7f6e\u53c2\u6570\uff08\u5982shuffle\u5206\u533a\u6570\uff09\uff0c\u5e76\u4e0eSpark\u7684AQE\u673a\u5236\u7ed3\u5408\uff0c\u5b9e\u73b0\u534f\u540c\u4f18\u5316", "result": "RL\u667a\u80fd\u4f53\u5728\u914d\u7f6e\u8c03\u4f18\u65b9\u9762\u4f18\u4e8e\u624b\u52a8\u8c03\u4f18\u548c\u5355\u72ec\u4f7f\u7528AQE\uff0c\u663e\u8457\u51cf\u5c11\u6267\u884c\u65f6\u95f4\u548c\u8d44\u6e90\u5f00\u9500\uff0cRL\u4e0eAQE\u7ed3\u5408\u53ef\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u8868\u73b0", "conclusion": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u662f\u5b9e\u73b0\u5927\u6570\u636e\u7cfb\u7edf\u81ea\u52a8\u8c03\u4f18\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5c06RL\u4e0e\u73b0\u6709\u81ea\u9002\u5e94\u673a\u5236\u7ed3\u5408\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u5927\u6570\u636e\u5904\u7406\u63d0\u4f9b\u667a\u80fd\u5316\u89e3\u51b3\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2602.baa49eac", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fvp-josh-clemm-knowledge-graphs-mcp-and-dspy-dash%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/zskbusSY4uYGgOM9cTsWkg_jHDLZBRl7LkZ6Mkfn55c=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fvp-josh-clemm-knowledge-graphs-mcp-and-dspy-dash%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/zskbusSY4uYGgOM9cTsWkg_jHDLZBRl7LkZ6Mkfn55c=442", "authors": ["TLDR Newsletter"], "title": "Engineering VP Josh Clemm on How We Use Knowledge Graphs, MCP, and DSPy in Dash", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fvp-josh-clemm-knowledge-graphs-mcp-and-dspy-dash%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/zskbusSY4uYGgOM9cTsWkg_jHDLZBRl7LkZ6Mkfn55c=442", "summary": "Engineering VP Josh Clemm on How We Use Knowledge Graphs, MCP, and DSPy in Dash (8 minute read) By giving Dash access to proprietary work content, it unifies search, Q&A, and agentic tasks across Dropbox files and third-party apps. Dash ingests data via custom connectors, generates multimodal embeddings and knowledge graphs for entity relationships, and uses hybrid retrieval (BM25 + dense vectors) for fast retrieval. It optimizes MCP tool calling and tunes 30+ prompts (including LLM-as-judge)...", "source": "tldr", "AI": {"tldr": "Dropbox\u7684Dash\u7cfb\u7edf\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u3001MCP\u548cDSPy\u6280\u672f\uff0c\u7edf\u4e00\u4e86\u8de8Dropbox\u6587\u4ef6\u548c\u7b2c\u4e09\u65b9\u5e94\u7528\u7684\u641c\u7d22\u3001\u95ee\u7b54\u548c\u667a\u80fd\u4ee3\u7406\u4efb\u52a1", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u73af\u5883\u4e2d\u8de8\u4e0d\u540c\u6570\u636e\u6e90\uff08Dropbox\u6587\u4ef6\u548c\u7b2c\u4e09\u65b9\u5e94\u7528\uff09\u7684\u7edf\u4e00\u641c\u7d22\u3001\u95ee\u7b54\u548c\u667a\u80fd\u4ee3\u7406\u4efb\u52a1\u9700\u6c42\uff0c\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u548c\u4fe1\u606f\u68c0\u7d22\u51c6\u786e\u6027", "method": "1. \u901a\u8fc7\u81ea\u5b9a\u4e49\u8fde\u63a5\u5668\u6444\u53d6\u6570\u636e\uff1b2. \u751f\u6210\u591a\u6a21\u6001\u5d4c\u5165\u548c\u77e5\u8bc6\u56fe\u8c31\u4ee5\u6355\u6349\u5b9e\u4f53\u5173\u7cfb\uff1b3. \u4f7f\u7528\u6df7\u5408\u68c0\u7d22\uff08BM25 + \u5bc6\u96c6\u5411\u91cf\uff09\u5b9e\u73b0\u5feb\u901f\u68c0\u7d22\uff1b4. \u4f18\u5316MCP\u5de5\u5177\u8c03\u7528\uff1b5. \u8c03\u4f1830\u591a\u4e2a\u63d0\u793a\uff08\u5305\u62ecLLM-as-judge\uff09", "result": "Dash\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u8de8Dropbox\u6587\u4ef6\u548c\u7b2c\u4e09\u65b9\u5e94\u7528\u7684\u7edf\u4e00\u641c\u7d22\u3001\u95ee\u7b54\u548c\u4ee3\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u6df7\u5408\u68c0\u7d22\u63d0\u9ad8\u4e86\u4fe1\u606f\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u6548\u7387", "conclusion": "\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u3001MCP\u548cDSPy\u7b49\u6280\u672f\uff0c\u53ef\u4ee5\u6784\u5efa\u5f3a\u5927\u7684\u4f01\u4e1a\u7ea7\u667a\u80fd\u52a9\u624b\u7cfb\u7edf\uff0c\u6709\u6548\u6574\u5408\u4e0d\u540c\u6570\u636e\u6e90\u5e76\u63d0\u4f9b\u7edf\u4e00\u7684\u667a\u80fd\u670d\u52a1", "topic": "agent analysis"}}
{"id": "tldr.2602.be45419c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fappliedingenuity.substack.com%2Fp%2Fthe-llm-as-analyst-trap-a-technical%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/63Pu1kb6bXTV4lWVKwP4qsDimPtb_YIPz3YaRtCeVos=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fappliedingenuity.substack.com%2Fp%2Fthe-llm-as-analyst-trap-a-technical%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/63Pu1kb6bXTV4lWVKwP4qsDimPtb_YIPz3YaRtCeVos=442", "authors": ["TLDR Newsletter"], "title": "The \"LLM-as-Analyst\" Trap: A Technical Deep-Dive into Agentic Data Systems", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fappliedingenuity.substack.com%2Fp%2Fthe-llm-as-analyst-trap-a-technical%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/63Pu1kb6bXTV4lWVKwP4qsDimPtb_YIPz3YaRtCeVos=442", "summary": "The \"LLM-as-Analyst\" Trap: A Technical Deep-Dive into Agentic Data Systems (17 minute read) The common \u201cLLM-as-Analyst\u201d pattern is risky for data systems because it can silently hallucinate, miscompute, and produce unverifiable results while driving up cost and latency.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u6df1\u5165\u5206\u6790\u4e86\"LLM-as-Analyst\"\u6a21\u5f0f\u5728\u6570\u636e\u7cfb\u7edf\u4e2d\u7684\u98ce\u9669\uff0c\u6307\u51fa\u8be5\u6a21\u5f0f\u53ef\u80fd\u5bfc\u81f4\u5e7b\u89c9\u3001\u9519\u8bef\u8ba1\u7b97\u548c\u4e0d\u53ef\u9a8c\u8bc1\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u589e\u52a0\u6210\u672c\u548c\u5ef6\u8fdf", "motivation": "\u5f53\u524d\u6570\u636e\u7cfb\u7edf\u4e2d\u666e\u904d\u91c7\u7528LLM\u4f5c\u4e3a\u5206\u6790\u5e08\u7684\u6a21\u5f0f\u5b58\u5728\u4e25\u91cd\u98ce\u9669\uff0c\u4f46\u8fd9\u4e9b\u95ee\u9898\u5f80\u5f80\u88ab\u5ffd\u89c6\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6280\u672f\u6df1\u5ea6\u5206\u6790\u63ed\u793a\u8fd9\u4e9b\u98ce\u9669\uff0c\u63a8\u52a8\u66f4\u53ef\u9760\u7684\u6570\u636e\u7cfb\u7edf\u8bbe\u8ba1", "method": "\u91c7\u7528\u6280\u672f\u6df1\u5ea6\u5206\u6790\u65b9\u6cd5\uff0c\u4ece\u591a\u4e2a\u7ef4\u5ea6\u5256\u6790LLM-as-Analyst\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u5305\u62ec\u5e7b\u89c9\u95ee\u9898\u3001\u8ba1\u7b97\u51c6\u786e\u6027\u3001\u7ed3\u679c\u53ef\u9a8c\u8bc1\u6027\u3001\u6210\u672c\u548c\u5ef6\u8fdf\u7b49\u65b9\u9762", "result": "\u53d1\u73b0LLM-as-Analyst\u6a21\u5f0f\u786e\u5b9e\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff1a\u53ef\u80fd\u4ea7\u751f\u4e0d\u53ef\u68c0\u6d4b\u7684\u5e7b\u89c9\u3001\u9519\u8bef\u8ba1\u7b97\u3001\u7ed3\u679c\u96be\u4ee5\u9a8c\u8bc1\uff0c\u540c\u65f6\u663e\u8457\u589e\u52a0\u7cfb\u7edf\u6210\u672c\u548c\u5ef6\u8fdf", "conclusion": "\u9700\u8981\u91cd\u65b0\u601d\u8003LLM\u5728\u6570\u636e\u7cfb\u7edf\u4e2d\u7684\u89d2\u8272\uff0c\u907f\u514d\u7b80\u5355\u5730\u5c06LLM\u4f5c\u4e3a\u5206\u6790\u5e08\u4f7f\u7528\uff0c\u800c\u5e94\u8be5\u8bbe\u8ba1\u66f4\u53ef\u9760\u3001\u53ef\u9a8c\u8bc1\u7684\u7cfb\u7edf\u67b6\u6784", "topic": "agent analysis"}}
{"id": "tldr.2602.c9acb617", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442", "authors": ["TLDR Newsletter"], "title": "AI for when it is rocket science", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442", "summary": "AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails \u2014 but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...", "source": "tldr", "AI": {"tldr": "Contextual AI\u5f00\u53d1\u4e86Agent Composer\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u5df2\u5728\u5236\u9020\u4e1a\u548c\u7269\u6d41\u9886\u57df\u5b9e\u73b0\u663e\u8457\u6548\u7387\u63d0\u5347", "motivation": "\u5f53\u524dAI\u5728\u5904\u7406\u590d\u6742\u3001\u4e13\u4e1a\u5316\u5de5\u4f5c\u65f6\u4ecd\u7136\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u80dc\u4efb\u5982\u706b\u7bad\u79d1\u5b66\u7b49\u9ad8\u7ea7\u6280\u672f\u4efb\u52a1\u3002\u867d\u7136AI\u53ef\u4ee5\u5904\u7406\u65e5\u5e38\u7b80\u5355\u5de5\u4f5c\uff0c\u4f46\u5728\u4e13\u4e1a\u9886\u57df\u5982\u70ed\u8bd5\u8f66\u7ed3\u679c\u5206\u6790\u3001\u9ad8\u7ea7\u6280\u672f\u95ee\u9898\u89e3\u7b54\u7b49\u65b9\u9762\u7f3a\u4e4f\u4fe1\u4efb\u5ea6", "method": "\u5f00\u53d1\u4e86Agent Composer\u7cfb\u7edf\uff0c\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4efb\u52a1\u8bbe\u8ba1\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u5904\u7406\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\u7b49\u4e13\u4e1a\u5de5\u4f5c", "result": "1. \u5148\u8fdb\u5236\u9020\u5546\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u65f6\u95f4\u4ece8\u5c0f\u65f6\u7f29\u77ed\u523020\u5206\u949f\n2. \u6280\u672f\u9a71\u52a8\u7684\u7b2c\u4e09\u65b9\u7269\u6d41\u63d0\u4f9b\u5546\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff08\u5177\u4f53\u672a\u8be6\u8ff0\uff09", "conclusion": "Agent Composer\u8bc1\u660eAI\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u5728\u5236\u9020\u4e1a\u548c\u7269\u6d41\u7b49\u9886\u57df\u5b9e\u73b0\u4e86\u9769\u547d\u6027\u7684\u6548\u7387\u63d0\u5347", "topic": "code agent"}}
{"id": "tldr.2602.68cfd489", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442", "authors": ["TLDR Newsletter"], "title": "8 hours to 20 minutes", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442", "summary": "AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails \u2014 but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...", "source": "tldr", "AI": {"tldr": "Contextual AI\u5f00\u53d1\u4e86Agent Composer\u5de5\u5177\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u5df2\u5728\u5148\u8fdb\u5236\u9020\u548c\u7269\u6d41\u9886\u57df\u5b9e\u73b0\u663e\u8457\u6548\u7387\u63d0\u5347", "motivation": "\u5f53\u524dAI\u5728\u5904\u7406\u7b80\u5355\u4efb\u52a1\uff08\u5982\u8d77\u8349\u90ae\u4ef6\uff09\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u3001\u4e13\u4e1a\u5316\u5de5\u4f5c\uff08\u5982\u5ba1\u67e5\u706b\u7bad\u53d1\u52a8\u673a\u6d4b\u8bd5\u7ed3\u679c\u3001\u56de\u7b54\u9ad8\u7ea7\u6280\u672f\u95ee\u9898\uff09\u65b9\u9762\u4ecd\u7136\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4efb\u52a1\u8bbe\u8ba1\u7684AI\u89e3\u51b3\u65b9\u6848", "method": "Contextual AI\u6784\u5efa\u4e86Agent Composer\u5de5\u5177\uff0c\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4efb\u52a1\u8bbe\u8ba1\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\u7b49\u6280\u672f\u624b\u6bb5\u6765\u5904\u7406\u4e13\u4e1a\u9886\u57df\u95ee\u9898", "result": "\u5728\u5148\u8fdb\u5236\u9020\u9886\u57df\uff0c\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u65f6\u95f4\u4ece8\u5c0f\u65f6\u7f29\u77ed\u523020\u5206\u949f\uff1b\u5728\u6280\u672f\u9a71\u52a8\u7684\u7b2c\u4e09\u65b9\u7269\u6d41\u63d0\u4f9b\u5546\u4e2d\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548", "conclusion": "\u9488\u5bf9\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\u7684\u4e13\u95e8\u5316AI\u5de5\u5177\u80fd\u591f\u663e\u8457\u63d0\u5347\u5de5\u4f5c\u6548\u7387\uff0c\u89e3\u51b3\u4f20\u7edfAI\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u4e0d\u8db3\u7684\u95ee\u9898", "topic": "code agent"}}
{"id": "tldr.2602.f7473852", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442", "authors": ["TLDR Newsletter"], "title": "60x faster issue resolution", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442", "summary": "AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails \u2014 but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...", "source": "tldr", "AI": {"tldr": "Contextual AI\u5f00\u53d1\u4e86Agent Composer\u7cfb\u7edf\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u5df2\u5728\u5236\u9020\u4e1a\u548c\u7269\u6d41\u9886\u57df\u5b9e\u73b0\u663e\u8457\u6548\u7387\u63d0\u5347", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u5904\u7406\u7b80\u5355\u4efb\u52a1\uff08\u5982\u8d77\u8349\u90ae\u4ef6\uff09\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u3001\u4e13\u4e1a\u5316\u7684\u5de5\u4f5c\uff08\u5982\u5ba1\u67e5\u706b\u7bad\u53d1\u52a8\u673a\u6d4b\u8bd5\u7ed3\u679c\u3001\u56de\u7b54\u9ad8\u7ea7\u6280\u672f\u95ee\u9898\uff09\u65b9\u9762\u4ecd\u7136\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4efb\u52a1\u8bbe\u8ba1\u7684AI\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u4e86Agent Composer\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\u7b49\u6280\u672f\uff0c\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\u8bbe\u8ba1", "result": "1. \u4e00\u5bb6\u5148\u8fdb\u5236\u9020\u4f01\u4e1a\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u65f6\u95f4\u4ece8\u5c0f\u65f6\u7f29\u77ed\u523020\u5206\u949f\uff1b2. \u4e00\u5bb6\u6280\u672f\u9a71\u52a8\u76843PL\u63d0\u4f9b\u5546\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff08\u5177\u4f53\u6570\u636e\u672a\u5b8c\u6574\u663e\u793a\uff09", "conclusion": "\u9488\u5bf9\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\u4e13\u95e8\u8bbe\u8ba1\u7684AI\u7cfb\u7edf\u80fd\u591f\u663e\u8457\u63d0\u5347\u5de5\u4f5c\u6548\u7387\uff0c\u5728\u5236\u9020\u4e1a\u3001\u822a\u7a7a\u822a\u5929\u7b49\u4e13\u4e1a\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c", "topic": "code agent"}}
{"id": "tldr.2602.6f6c9c2f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442", "authors": ["TLDR Newsletter"], "title": "in minutes instead of days", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/YpRUBazVzJ-LmT7rCcJBBSfiValw6CwhVzqYhCuJe8U=442", "summary": "AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails \u2014 but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...", "source": "tldr", "AI": {"tldr": "Contextual AI\u5f00\u53d1\u4e86Agent Composer\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u5df2\u5728\u5236\u9020\u4e1a\u548c\u7269\u6d41\u9886\u57df\u53d6\u5f97\u663e\u8457\u6548\u7387\u63d0\u5347", "motivation": "\u5f53\u524dAI\u5728\u5904\u7406\u590d\u6742\u3001\u4e13\u4e1a\u5316\u5de5\u4f5c\u65f6\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\uff0c\u867d\u7136\u80fd\u5904\u7406\u65e5\u5e38\u4efb\u52a1\u5982\u8d77\u8349\u90ae\u4ef6\uff0c\u4f46\u5728\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u7684\u9886\u57df\uff08\u5982\u706b\u7bad\u79d1\u5b66\u3001\u9ad8\u7ea7\u6280\u672f\u95ee\u9898\uff09\u7f3a\u4e4f\u53ef\u4fe1\u5ea6", "method": "\u5f00\u53d1\u4e86Agent Composer\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\u7b49\u6280\u672f\uff0c\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4efb\u52a1\u8fdb\u884c\u4f18\u5316", "result": "\u5728\u5148\u8fdb\u5236\u9020\u4e1a\u4e2d\uff0c\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u4ece8\u5c0f\u65f6\u7f29\u77ed\u523020\u5206\u949f\uff1b\u5728\u6280\u672f\u9a71\u52a8\u7684\u7b2c\u4e09\u65b9\u7269\u6d41\u63d0\u4f9b\u5546\u4e2d\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c", "conclusion": "Agent Composer\u8bc1\u660e\u4e86AI\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u4e3a\u7279\u5b9a\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u4fe1\u7684AI\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "tldr.2602.e08b8f87", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.contextual.ai%2Fwebinar-introducing-agent-composer.html%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=secondary/1/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/F9aFdxvffLp8gt34JUZqkSdCev-fo0hNDod4iYFxzWk=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.contextual.ai%2Fwebinar-introducing-agent-composer.html%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=secondary/1/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/F9aFdxvffLp8gt34JUZqkSdCev-fo0hNDod4iYFxzWk=442", "authors": ["TLDR Newsletter"], "title": "The new Agent Composer brings AI to expert-level engineering work", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.contextual.ai%2Fwebinar-introducing-agent-composer.html%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=secondary/1/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/F9aFdxvffLp8gt34JUZqkSdCev-fo0hNDod4iYFxzWk=442", "summary": "The new Agent Composer brings AI to expert-level engineering work (Sponsor) Most AI tools lack the context to help with high-complexity tasks such as root cause analysis. Agent Composer by Contextual AI is built for high-stakes environments like: semiconductors, aerospace, logistics, and finance. Early adopters are using it to compress hours of complex engineering work into minutes. Want to see how? Join launch event on February 5.", "source": "tldr", "AI": {"tldr": "Agent Composer\u662f\u4e00\u6b3e\u9488\u5bf9\u9ad8\u590d\u6742\u5ea6\u5de5\u7a0b\u4efb\u52a1\u7684AI\u5de5\u5177\uff0c\u80fd\u591f\u5c06\u6570\u5c0f\u65f6\u7684\u590d\u6742\u5de5\u7a0b\u5de5\u4f5c\u538b\u7f29\u5230\u51e0\u5206\u949f\u5185\u5b8c\u6210", "motivation": "\u73b0\u6709AI\u5de5\u5177\u7f3a\u4e4f\u5904\u7406\u9ad8\u590d\u6742\u5ea6\u4efb\u52a1\uff08\u5982\u6839\u672c\u539f\u56e0\u5206\u6790\uff09\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u9700\u8981\u4e13\u95e8\u4e3a\u9ad8\u98ce\u9669\u73af\u5883\u8bbe\u8ba1\u7684AI\u89e3\u51b3\u65b9\u6848", "method": "\u7531Contextual AI\u5f00\u53d1\u7684Agent Composer\uff0c\u4e13\u95e8\u4e3a\u534a\u5bfc\u4f53\u3001\u822a\u7a7a\u822a\u5929\u3001\u7269\u6d41\u548c\u91d1\u878d\u7b49\u9ad8\u98ce\u9669\u73af\u5883\u6784\u5efa", "result": "\u65e9\u671f\u91c7\u7528\u8005\u5df2\u6210\u529f\u4f7f\u7528\u8be5\u5de5\u5177\u5c06\u590d\u6742\u5de5\u7a0b\u5de5\u4f5c\u4ece\u6570\u5c0f\u65f6\u538b\u7f29\u5230\u51e0\u5206\u949f", "conclusion": "Agent Composer\u4e3a\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u9ad8\u590d\u6742\u5ea6\u5de5\u7a0b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684AI\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u4f5c\u6548\u7387", "topic": "code agent"}}
{"id": "tldr.2602.f133719a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnadh.in%2Fblog%2Fcode-is-cheap%2F%3Futm_source=tldrnewsletter/1/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/A1Xdv9x0_W-_acSidrma_0KcEptZK6B_LexoRAzm7_0=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnadh.in%2Fblog%2Fcode-is-cheap%2F%3Futm_source=tldrnewsletter/1/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/A1Xdv9x0_W-_acSidrma_0KcEptZK6B_LexoRAzm7_0=442", "authors": ["TLDR Newsletter"], "title": "Code is cheap. Show me the talk", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 23 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnadh.in%2Fblog%2Fcode-is-cheap%2F%3Futm_source=tldrnewsletter/1/0100019c1e1a5b66-ee047e35-2ccd-47ee-923e-8c5183e18ba9-000000/A1Xdv9x0_W-_acSidrma_0KcEptZK6B_LexoRAzm7_0=442", "summary": "Code is cheap. Show me the talk (23 minute read) Large language model coding tools have fundamentally changed software development. Reading and critically evaluating code has become more important than learning syntax and typing it out line by line. Developers who can imagine, articulate, define problem statements, architect, and engineer have a massive advantage over those who can't, now more disproportionately than ever. Knowledge of specific language, syntax, and frameworks is no longer a ...", "source": "tldr", "AI": {"tldr": "LLM\u4ee3\u7801\u5de5\u5177\u6539\u53d8\u4e86\u8f6f\u4ef6\u5f00\u53d1\uff0c\u9605\u8bfb\u548c\u8bc4\u4f30\u4ee3\u7801\u6bd4\u5b66\u4e60\u8bed\u6cd5\u66f4\u91cd\u8981\uff0c\u5f00\u53d1\u8005\u9700\u8981\u66f4\u5f3a\u7684\u60f3\u8c61\u3001\u8868\u8fbe\u3001\u5b9a\u4e49\u95ee\u9898\u3001\u67b6\u6784\u548c\u5de5\u7a0b\u80fd\u529b", "motivation": "\u63a2\u8ba8LLM\u4ee3\u7801\u5de5\u5177\u5982\u4f55\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u8303\u5f0f\uff0c\u5f3a\u8c03\u5728\u65b0\u7684\u5f00\u53d1\u73af\u5883\u4e2d\u54ea\u4e9b\u6280\u80fd\u53d8\u5f97\u66f4\u4e3a\u91cd\u8981", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524dLLM\u4ee3\u7801\u5de5\u5177\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u7684\u5f71\u54cd\uff0c\u5bf9\u6bd4\u4f20\u7edf\u5f00\u53d1\u6280\u80fd\u4e0e\u65b0\u65f6\u4ee3\u6240\u9700\u6280\u80fd\u7684\u5dee\u5f02", "result": "LLM\u5de5\u5177\u4f7f\u4ee3\u7801\u7f16\u5199\u53d8\u5f97\u5ec9\u4ef7\uff0c\u5f00\u53d1\u8005\u7684\u6838\u5fc3\u4ef7\u503c\u8f6c\u5411\u66f4\u9ad8\u5c42\u6b21\u7684\u8ba4\u77e5\u548c\u5de5\u7a0b\u80fd\u529b\uff0c\u5177\u4f53\u8bed\u8a00\u548c\u6846\u67b6\u77e5\u8bc6\u7684\u91cd\u8981\u6027\u4e0b\u964d", "conclusion": "\u8f6f\u4ef6\u5f00\u53d1\u7684\u91cd\u70b9\u5df2\u4ece\u7f16\u5199\u4ee3\u7801\u8f6c\u5411\u95ee\u9898\u5b9a\u4e49\u3001\u67b6\u6784\u8bbe\u8ba1\u548c\u5de5\u7a0b\u80fd\u529b\uff0c\u5f00\u53d1\u8005\u9700\u8981\u9002\u5e94\u8fd9\u79cd\u6280\u80fd\u8f6c\u53d8", "topic": "code agent"}}
{"id": "tldr.2602.40856643", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Failedgrowth.com%2Fvault%2Fcontent-strategy-for-ai-search-or-geo-aeo%3Futm_source=tldr%26utm_campaign=feb22026/3/0100019c1e3fab04-4b80cedc-8270-4fda-a2e7-bd55675b75f6-000000/lxZJE-PLGjcU0JIsP_h6DEcscA7NIwvk_pmdb7Rbbng=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Failedgrowth.com%2Fvault%2Fcontent-strategy-for-ai-search-or-geo-aeo%3Futm_source=tldr%26utm_campaign=feb22026/3/0100019c1e3fab04-4b80cedc-8270-4fda-a2e7-bd55675b75f6-000000/lxZJE-PLGjcU0JIsP_h6DEcscA7NIwvk_pmdb7Rbbng=442", "authors": ["TLDR Newsletter"], "title": "3x their AI visibility in 4 months", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Failedgrowth.com%2Fvault%2Fcontent-strategy-for-ai-search-or-geo-aeo%3Futm_source=tldr%26utm_campaign=feb22026/3/0100019c1e3fab04-4b80cedc-8270-4fda-a2e7-bd55675b75f6-000000/lxZJE-PLGjcU0JIsP_h6DEcscA7NIwvk_pmdb7Rbbng=442", "summary": "So how did Augment Code \u2014outranking everyone else in hyper-competitive prompts? They used the", "source": "tldr", "AI": {"tldr": "Augment Code\u5728\u7ade\u4e89\u6fc0\u70c8\u7684\u63d0\u793a\u5de5\u7a0b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5177\u4f53\u65b9\u6cd5\u5728\u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e", "motivation": "\u5728\u9ad8\u5ea6\u7ade\u4e89\u7684\u63d0\u793a\u5de5\u7a0b\u73af\u5883\u4e2d\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8d85\u8d8a\u5176\u4ed6\u65b9\u6cd5\u7684\u4ee3\u7801\u589e\u5f3a\u6280\u672f", "method": "\u6458\u8981\u4e2d\u672a\u660e\u786e\u8bf4\u660e\u5177\u4f53\u65b9\u6cd5\uff0c\u53ea\u63d0\u5230\"\u4ed6\u4eec\u4f7f\u7528\u4e86\"\u4f46\u672a\u5b8c\u6210\u63cf\u8ff0", "result": "Augment Code\u5728\u8d85\u7ade\u4e89\u63d0\u793a\u4e2d\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u5176\u4ed6\u65b9\u6cd5", "conclusion": "Augment Code\u5728\u63d0\u793a\u5de5\u7a0b\u7ade\u4e89\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u5176\u5177\u4f53\u6280\u672f\u7ec6\u8282\u9700\u8981\u8fdb\u4e00\u6b65\u4e86\u89e3", "topic": "code agent"}}
{"id": "tldr.2602.f3cdfac6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.digitalocean.com%2Fblog%2Fmoltbot-on-digitalocean%3Futm_source=tldrdevops/1/0100019c1e5396ec-2e9578f2-6341-4e66-b2df-a2dd4a6a89d1-000000/kOJZVR_KwD19ZHcGicoNrMFej2LjZkptaO2AzArU2HY=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.digitalocean.com%2Fblog%2Fmoltbot-on-digitalocean%3Futm_source=tldrdevops/1/0100019c1e5396ec-2e9578f2-6341-4e66-b2df-a2dd4a6a89d1-000000/kOJZVR_KwD19ZHcGicoNrMFej2LjZkptaO2AzArU2HY=442", "authors": ["TLDR Newsletter"], "title": "Introducing OpenClaw on DigitalOcean: One-Click Deploy, Security-hardened, Production-Ready Agentic AI", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.digitalocean.com%2Fblog%2Fmoltbot-on-digitalocean%3Futm_source=tldrdevops/1/0100019c1e5396ec-2e9578f2-6341-4e66-b2df-a2dd4a6a89d1-000000/kOJZVR_KwD19ZHcGicoNrMFej2LjZkptaO2AzArU2HY=442", "summary": "Introducing OpenClaw on DigitalOcean: One-Click Deploy, Security-hardened, Production-Ready Agentic AI (3 minute read) DigitalOcean has introduced a 1-Click deployment for OpenClaw, an agentic AI, on its Droplet servers, providing developers with a security-hardened cloud environment to run AI agents at scale. This new offering, part of DigitalOcean's Agentic Inference Cloud, is designed to address the challenges of securely and continuously running inference-heavy AI workloads in production.", "source": "tldr", "AI": {"tldr": "DigitalOcean\u63a8\u51faOpenClaw\u4e00\u952e\u90e8\u7f72\u65b9\u6848\uff0c\u63d0\u4f9b\u5b89\u5168\u52a0\u56fa\u7684\u751f\u4ea7\u5c31\u7eeaAI\u4ee3\u7406\u4e91\u73af\u5883", "motivation": "\u89e3\u51b3\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5b89\u5168\u3001\u6301\u7eed\u8fd0\u884c\u63a8\u7406\u5bc6\u96c6\u578bAI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6311\u6218", "method": "\u901a\u8fc7DigitalOcean\u7684Droplet\u670d\u52a1\u5668\u63d0\u4f9b\u4e00\u952e\u90e8\u7f72\u7684OpenClaw AI\u4ee3\u7406\uff0c\u4f5c\u4e3aAgentic Inference Cloud\u7684\u4e00\u90e8\u5206", "result": "\u5f00\u53d1\u8005\u83b7\u5f97\u5b89\u5168\u52a0\u56fa\u7684\u4e91\u73af\u5883\uff0c\u53ef\u5927\u89c4\u6a21\u8fd0\u884cAI\u4ee3\u7406\uff0c\u652f\u6301\u751f\u4ea7\u7ea7AI\u5de5\u4f5c\u8d1f\u8f7d", "conclusion": "DigitalOcean\u7684OpenClaw\u90e8\u7f72\u65b9\u6848\u7b80\u5316\u4e86AI\u4ee3\u7406\u7684\u751f\u4ea7\u90e8\u7f72\uff0c\u63d0\u4f9b\u5b89\u5168\u53ef\u9760\u7684\u57fa\u7840\u8bbe\u65bd", "topic": "agent analysis"}}
{"id": "tldr.2602.ead8219d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.datadoghq.com%2Fblog%2Fllm-observability-at-datadog-nlq%2F%3Futm_source=tldrdevops/1/0100019c1e5396ec-2e9578f2-6341-4e66-b2df-a2dd4a6a89d1-000000/1VhtjxoW6uYHZWO27SUyoLjsVLvtLBIWWNhuVEEnFsQ=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.datadoghq.com%2Fblog%2Fllm-observability-at-datadog-nlq%2F%3Futm_source=tldrdevops/1/0100019c1e5396ec-2e9578f2-6341-4e66-b2df-a2dd4a6a89d1-000000/1VhtjxoW6uYHZWO27SUyoLjsVLvtLBIWWNhuVEEnFsQ=442", "authors": ["TLDR Newsletter"], "title": "How we cut our NLQ agent debugging time from hours to minutes with LLM Observability", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.datadoghq.com%2Fblog%2Fllm-observability-at-datadog-nlq%2F%3Futm_source=tldrdevops/1/0100019c1e5396ec-2e9578f2-6341-4e66-b2df-a2dd4a6a89d1-000000/1VhtjxoW6uYHZWO27SUyoLjsVLvtLBIWWNhuVEEnFsQ=442", "summary": "How we cut our NLQ agent debugging time from hours to minutes with LLM Observability (6 minute read) Datadog's Cloud Cost Management team significantly cut natural language query (NLQ) agent debugging time from hours to minutes\u2014a roughly 20x reduction\u2014by implementing LLM Observability. This was achieved through the use of curated datasets, component-level evaluators, and LLM traces to precisely identify root causes in their agent, which generates Datadog metrics queries.", "source": "tldr", "AI": {"tldr": "Datadog\u56e2\u961f\u901a\u8fc7LLM\u53ef\u89c2\u6d4b\u6027\u5c06NLQ\u4ee3\u7406\u8c03\u8bd5\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u5230\u6570\u5206\u949f\uff0c\u5b9e\u73b0\u4e86\u7ea620\u500d\u7684\u6548\u7387\u63d0\u5347", "motivation": "\u81ea\u7136\u8bed\u8a00\u67e5\u8be2(NLQ)\u4ee3\u7406\u5728\u751f\u6210Datadog\u6307\u6807\u67e5\u8be2\u65f6\uff0c\u8c03\u8bd5\u8fc7\u7a0b\u8017\u65f6\u8fc7\u957f\uff08\u6570\u5c0f\u65f6\uff09\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8c03\u8bd5\u65b9\u6cd5\u6765\u5feb\u901f\u8bc6\u522b\u548c\u89e3\u51b3\u95ee\u9898", "method": "\u91c7\u7528LLM\u53ef\u89c2\u6d4b\u6027\u65b9\u6cd5\uff0c\u5305\u62ec\u4f7f\u7528\u7cbe\u9009\u6570\u636e\u96c6\u3001\u7ec4\u4ef6\u7ea7\u8bc4\u4f30\u5668\u548cLLM\u8ffd\u8e2a\uff0c\u7cbe\u786e\u8bc6\u522b\u4ee3\u7406\u4e2d\u7684\u6839\u672c\u539f\u56e0", "result": "\u8c03\u8bd5\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u5230\u6570\u5206\u949f\uff0c\u5b9e\u73b0\u4e86\u7ea620\u500d\u7684\u6548\u7387\u63d0\u5347\uff0c\u663e\u8457\u63d0\u9ad8\u4e86NLQ\u4ee3\u7406\u7684\u5f00\u53d1\u548c\u7ef4\u62a4\u6548\u7387", "conclusion": "LLM\u53ef\u89c2\u6d4b\u6027\u662f\u4e00\u79cd\u6709\u6548\u7684\u8c03\u8bd5\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11NLQ\u4ee3\u7406\u7684\u8c03\u8bd5\u65f6\u95f4\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387", "topic": "agent analysis"}}
{"id": "tldr.2602.95294b1e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsignup%3Focid=cmmul07xv88%26utm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/uoM-KlucqMvToBlXNGhDfI0zTo353UfUxoP5rIBSmRE=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsignup%3Focid=cmmul07xv88%26utm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/uoM-KlucqMvToBlXNGhDfI0zTo353UfUxoP5rIBSmRE=442", "authors": ["TLDR Newsletter"], "title": "GitHub Copilot: AI that understands your codebase and style", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsignup%3Focid=cmmul07xv88%26utm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/uoM-KlucqMvToBlXNGhDfI0zTo353UfUxoP5rIBSmRE=442", "summary": "GitHub Copilot: AI that understands your codebase and style (Sponsor) Accelerate software development with GitHub Copilot, an AI coding assistant built into your favorite IDE. GitHub Copilot understands your codebase and applies real work context across code, files, and decisions, so you can plan, build, and ship faster without breaking focus. Bring GitHub Copilot into your workflow", "source": "tldr", "AI": {"tldr": "GitHub Copilot\u662f\u4e00\u6b3e\u96c6\u6210\u5728IDE\u4e2d\u7684AI\u7f16\u7a0b\u52a9\u624b\uff0c\u80fd\u591f\u7406\u89e3\u4ee3\u7801\u5e93\u548c\u5f00\u53d1\u98ce\u683c\uff0c\u63d0\u4f9b\u57fa\u4e8e\u5b9e\u9645\u5de5\u4f5c\u73af\u5883\u7684\u4ee3\u7801\u5efa\u8bae\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u9ad8\u6548\u5730\u89c4\u5212\u3001\u6784\u5efa\u548c\u4ea4\u4ed8\u8f6f\u4ef6\u3002", "motivation": "\u4f20\u7edf\u7f16\u7a0b\u5de5\u5177\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u5e93\u6574\u4f53\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\uff0c\u5f00\u53d1\u8005\u9700\u8981\u5728\u4e0d\u540c\u6587\u4ef6\u3001\u51b3\u7b56\u548c\u4ee3\u7801\u7247\u6bb5\u4e4b\u95f4\u9891\u7e41\u5207\u6362\uff0c\u5bfc\u81f4\u5f00\u53d1\u6548\u7387\u4f4e\u4e0b\u548c\u6ce8\u610f\u529b\u5206\u6563\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7406\u89e3\u6574\u4e2a\u4ee3\u7801\u5e93\u5e76\u63d0\u4f9b\u667a\u80fd\u5efa\u8bae\u7684\u5de5\u5177\u6765\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u3002", "method": "GitHub Copilot\u4f5c\u4e3aAI\u7f16\u7a0b\u52a9\u624b\u96c6\u6210\u5230\u5f00\u53d1\u8005\u5e38\u7528\u7684IDE\u4e2d\uff0c\u901a\u8fc7\u5206\u6790\u4ee3\u7801\u5e93\u3001\u7406\u89e3\u4ee3\u7801\u98ce\u683c\u548c\u5b9e\u9645\u5de5\u4f5c\u4e0a\u4e0b\u6587\uff0c\u63d0\u4f9b\u8de8\u4ee3\u7801\u3001\u6587\u4ef6\u548c\u51b3\u7b56\u7684\u667a\u80fd\u5efa\u8bae\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u4fdd\u6301\u4e13\u6ce8\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "result": "GitHub Copilot\u80fd\u591f\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u9ad8\u6548\u5730\u89c4\u5212\u3001\u6784\u5efa\u548c\u4ea4\u4ed8\u8f6f\u4ef6\uff0c\u540c\u65f6\u4fdd\u6301\u5f00\u53d1\u6d41\u7a0b\u7684\u8fde\u8d2f\u6027\u548c\u4e13\u6ce8\u5ea6\uff0c\u51cf\u5c11\u56e0\u4e0a\u4e0b\u6587\u5207\u6362\u5e26\u6765\u7684\u6548\u7387\u635f\u5931\u3002", "conclusion": "GitHub Copilot\u4f5c\u4e3a\u7406\u89e3\u4ee3\u7801\u5e93\u548c\u5f00\u53d1\u98ce\u683c\u7684AI\u7f16\u7a0b\u52a9\u624b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u4fdd\u6301\u4e13\u6ce8\u7684\u540c\u65f6\u66f4\u5feb\u5730\u5b8c\u6210\u8f6f\u4ef6\u4ea4\u4ed8\u3002", "topic": "swe application"}}
{"id": "tldr.2602.79869668", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopenclaw.ai%2Fblog%2Fintroducing-openclaw%3Futm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/JsZHkRmG8QVL_R6LtaN2iYX9jar7_dZlfMJLZ6MplFM=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopenclaw.ai%2Fblog%2Fintroducing-openclaw%3Futm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/JsZHkRmG8QVL_R6LtaN2iYX9jar7_dZlfMJLZ6MplFM=442", "authors": ["TLDR Newsletter"], "title": "Introducing OpenClaw", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopenclaw.ai%2Fblog%2Fintroducing-openclaw%3Futm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/JsZHkRmG8QVL_R6LtaN2iYX9jar7_dZlfMJLZ6MplFM=442", "summary": "Introducing OpenClaw (3 minute read) OpenClaw is an open-source agent platform designed to run on users' local machines, integrating with various chat apps like WhatsApp, Telegram, and Discord to provide a private, user-controlled AI assistant. Previously \u201cClawdBot\u201d, this release introduces new channels such as Twitch and Google Chat plugins, support for new AI models, web chat image sending, and security-related commits.", "source": "tldr", "AI": {"tldr": "OpenClaw\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u672c\u5730AI\u52a9\u624b\u5e73\u53f0\uff0c\u652f\u6301\u591a\u79cd\u804a\u5929\u5e94\u7528\u96c6\u6210\uff0c\u63d0\u4f9b\u79c1\u5bc6\u3001\u7528\u6237\u63a7\u5236\u7684AI\u52a9\u624b\u670d\u52a1\u3002", "motivation": "\u4e3a\u7528\u6237\u63d0\u4f9b\u5728\u672c\u5730\u673a\u5668\u4e0a\u8fd0\u884c\u7684\u79c1\u6709AI\u52a9\u624b\uff0c\u907f\u514d\u4e91\u7aef\u670d\u52a1\u7684\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u591a\u79cd\u5e38\u7528\u804a\u5929\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u5f00\u6e90\u4ee3\u7406\u5e73\u53f0\uff0c\u96c6\u6210WhatsApp\u3001Telegram\u3001Discord\u7b49\u804a\u5929\u5e94\u7528\u63d2\u4ef6\uff0c\u652f\u6301\u672c\u5730\u90e8\u7f72\u548c\u591a\u79cdAI\u6a21\u578b\u3002", "result": "\u6210\u529f\u53d1\u5e03OpenClaw\u5e73\u53f0\uff0c\u65b0\u589eTwitch\u548cGoogle Chat\u63d2\u4ef6\uff0c\u652f\u6301\u66f4\u591aAI\u6a21\u578b\uff0c\u589e\u52a0\u56fe\u50cf\u53d1\u9001\u529f\u80fd\u548c\u5b89\u5168\u6539\u8fdb\u3002", "conclusion": "OpenClaw\u4e3a\u9700\u8981\u9690\u79c1\u4fdd\u62a4\u7684\u7528\u6237\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u672c\u5730AI\u52a9\u624b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5f00\u6e90\u65b9\u5f0f\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u548c\u529f\u80fd\u6269\u5c55\u3002", "topic": "code agent"}}
{"id": "tldr.2602.bc9481b0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fagno-agi%2Fdash%3Futm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/BVUwRuI6nwese2QM1kVMaLUAbvG9I3EQ_brDsv4OyYg=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fagno-agi%2Fdash%3Futm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/BVUwRuI6nwese2QM1kVMaLUAbvG9I3EQ_brDsv4OyYg=442", "authors": ["TLDR Newsletter"], "title": "Dash", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fagno-agi%2Fdash%3Futm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/BVUwRuI6nwese2QM1kVMaLUAbvG9I3EQ_brDsv4OyYg=442", "summary": "Dash (GitHub Repo) Dash is a self-learning data agent that grounds its answers in six layers of context and continuously improves with every use. It addresses the common shortcomings of raw LLMs in generating SQL, which often fail due to missing context, lack of tribal knowledge, and inability to learn from errors. Dash achieves this by integrating context layers and a unique self-learning loop, which stores both curated \"Knowledge\" and discovered \"Learnings\" from past interactions.", "source": "tldr", "AI": {"tldr": "Dash\u662f\u4e00\u4e2a\u81ea\u5b66\u4e60\u6570\u636e\u4ee3\u7406\uff0c\u901a\u8fc7\u516d\u5c42\u4e0a\u4e0b\u6587\u57fa\u7840\u5316\u548c\u72ec\u7279\u81ea\u5b66\u4e60\u5faa\u73af\u89e3\u51b3LLM\u751f\u6210SQL\u65f6\u7684\u5e38\u89c1\u95ee\u9898", "motivation": "\u539f\u59cbLLM\u5728\u751f\u6210SQL\u65f6\u7ecf\u5e38\u5931\u8d25\uff0c\u539f\u56e0\u5305\u62ec\uff1a\u7f3a\u5c11\u4e0a\u4e0b\u6587\u3001\u7f3a\u4e4f\u56e2\u961f\u77e5\u8bc6\u3001\u65e0\u6cd5\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u5e38\u89c1\u7f3a\u9677\u3002", "method": "\u96c6\u6210\u516d\u5c42\u4e0a\u4e0b\u6587\u57fa\u7840\u5316\u5c42\u548c\u72ec\u7279\u7684\u81ea\u5b66\u4e60\u5faa\u73af\uff0c\u5b58\u50a8\u6765\u81ea\u8fc7\u53bb\u4ea4\u4e92\u7684\"\u77e5\u8bc6\"\u548c\"\u5b66\u4e60\u6210\u679c\"", "result": "Dash\u80fd\u591f\u57fa\u4e8e\u591a\u5c42\u4e0a\u4e0b\u6587\u751f\u6210\u66f4\u51c6\u786e\u7684SQL\uff0c\u5e76\u901a\u8fc7\u6301\u7eed\u4f7f\u7528\u4e0d\u65ad\u6539\u8fdb\u6027\u80fd", "conclusion": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u96c6\u6210\u548c\u81ea\u5b66\u4e60\u673a\u5236\uff0cDash\u663e\u8457\u63d0\u5347\u4e86LLM\u5728SQL\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027", "topic": "code agent"}}
{"id": "tldr.2602.dc8890a4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jtolio.com%2F2026%2F01%2Ftinyemu-go%2F%3Futm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/b6Zz-oMTTqPoBID2SB6nisNAB2-7ilJtqj2MZ_4K5GI=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jtolio.com%2F2026%2F01%2Ftinyemu-go%2F%3Futm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/b6Zz-oMTTqPoBID2SB6nisNAB2-7ilJtqj2MZ_4K5GI=442", "authors": ["TLDR Newsletter"], "title": "A \"Pure Go\" Linux environment, ported by Claude", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 16 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jtolio.com%2F2026%2F01%2Ftinyemu-go%2F%3Futm_source=tldrdev/1/0100019c1e583162-5962daed-6a75-4445-9025-94b7a994c820-000000/b6Zz-oMTTqPoBID2SB6nisNAB2-7ilJtqj2MZ_4K5GI=442", "summary": "A \"Pure Go\" Linux environment, ported by Claude (16 minute read) Claude was able to port a RISC-V emulator from C to pure Go, though the last 20% was harder than the first 80% due to context length issues and Claude Code's code quality degrading.", "source": "tldr", "AI": {"tldr": "Claude\u5c06RISC-V\u6a21\u62df\u5668\u4eceC\u8bed\u8a00\u79fb\u690d\u5230\u7eafGo\u8bed\u8a00\uff0c\u4f46\u6700\u540e20%\u7684\u5de5\u4f5c\u6bd4\u524d80%\u66f4\u56f0\u96be\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u957f\u5ea6\u95ee\u9898\u548c\u4ee3\u7801\u8d28\u91cf\u4e0b\u964d", "motivation": "\u63a2\u7d22\u4f7f\u7528Claude AI\u8fdb\u884c\u4ee3\u7801\u79fb\u690d\u7684\u53ef\u884c\u6027\uff0c\u7279\u522b\u662f\u4eceC\u5230Go\u8bed\u8a00\u7684\u8f6c\u6362\uff0c\u6d4b\u8bd5AI\u5728\u590d\u6742\u4ee3\u7801\u91cd\u6784\u4efb\u52a1\u4e2d\u7684\u80fd\u529b", "method": "\u4f7f\u7528Claude AI\u5c06RISC-V\u6a21\u62df\u5668\u4eceC\u8bed\u8a00\u79fb\u690d\u5230\u7eafGo\u8bed\u8a00\uff0c\u8fc7\u7a0b\u4e2d\u9047\u5230\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u548c\u4ee3\u7801\u8d28\u91cf\u4e0b\u964d\u7684\u6311\u6218", "result": "\u6210\u529f\u5b8c\u6210\u4e86\u79fb\u690d\u5de5\u4f5c\uff0c\u4f46\u6700\u540e20%\u7684\u5de5\u4f5c\u6bd4\u524d80%\u66f4\u52a0\u56f0\u96be\uff0c\u4e3b\u8981\u53d7\u9650\u4e8eAI\u7684\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u548c\u4ee3\u7801\u8d28\u91cf\u7ef4\u6301", "conclusion": "AI\u5728\u4ee3\u7801\u79fb\u690d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u3001\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4ee3\u7801\u8d28\u91cf\u4f1a\u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u800c\u4e0b\u964d", "topic": "code agent"}}
{"id": "tldr.2602.eb0d3be7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkombai.com%2F%3Futm_source=tldrfounders/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/Cplm7vw6CyGbD7SUhgDdPFHLjX4xrJ3iTpf7ypmyfpk=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkombai.com%2F%3Futm_source=tldrfounders/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/Cplm7vw6CyGbD7SUhgDdPFHLjX4xrJ3iTpf7ypmyfpk=442", "authors": ["TLDR Newsletter"], "title": "Kombai", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkombai.com%2F%3Futm_source=tldrfounders/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/Cplm7vw6CyGbD7SUhgDdPFHLjX4xrJ3iTpf7ypmyfpk=442", "summary": "Kombai (Tool) AI agent for real frontend tasks: build UIs, refactor, and integrate.", "source": "tldr", "AI": {"tldr": "Kombai\u662f\u4e00\u4e2aAI\u4ee3\u7406\u5de5\u5177\uff0c\u4e13\u95e8\u7528\u4e8e\u524d\u7aef\u5f00\u53d1\u4efb\u52a1\uff0c\u5305\u62ec\u6784\u5efaUI\u754c\u9762\u3001\u4ee3\u7801\u91cd\u6784\u548c\u7cfb\u7edf\u96c6\u6210\u3002", "motivation": "\u524d\u7aef\u5f00\u53d1\u5de5\u4f5c\u91cd\u590d\u6027\u9ad8\u3001\u8017\u65f6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u8d28\u91cf\u3002\u4f20\u7edf\u524d\u7aef\u5f00\u53d1\u6d41\u7a0b\u4e2d\uff0cUI\u6784\u5efa\u3001\u4ee3\u7801\u91cd\u6784\u548c\u7cfb\u7edf\u96c6\u6210\u7b49\u4efb\u52a1\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\uff0c\u5b58\u5728\u6548\u7387\u74f6\u9888\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u4ee3\u7406\u5de5\u5177Kombai\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u6280\u672f\u81ea\u52a8\u5316\u524d\u7aef\u5f00\u53d1\u4efb\u52a1\u3002\u8be5\u5de5\u5177\u80fd\u591f\u7406\u89e3\u8bbe\u8ba1\u9700\u6c42\uff0c\u81ea\u52a8\u751f\u6210UI\u4ee3\u7801\uff0c\u8fdb\u884c\u4ee3\u7801\u91cd\u6784\u4f18\u5316\uff0c\u5e76\u5b9e\u73b0\u7cfb\u7edf\u96c6\u6210\u529f\u80fd\u3002", "result": "Kombai\u5de5\u5177\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316\u524d\u7aef\u5f00\u53d1\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u51cf\u5c11\u624b\u52a8\u7f16\u7801\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u8bc1\u4ee3\u7801\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "AI\u4ee3\u7406\u5de5\u5177\u5982Kombai\u5728\u524d\u7aef\u5f00\u53d1\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u5927\u5e45\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u662f\u672a\u6765\u524d\u7aef\u5f00\u53d1\u81ea\u52a8\u5316\u7684\u91cd\u8981\u65b9\u5411\u3002", "topic": "code agent"}}
{"id": "tldr.2602.1ad987f7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F04pXji/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/EgPgh6tHkE9xcsuF1olGgrFjOIWXHmvlpAGnIqrhb7U=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F04pXji/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/EgPgh6tHkE9xcsuF1olGgrFjOIWXHmvlpAGnIqrhb7U=442", "authors": ["TLDR Newsletter"], "title": "B2CC - Claude Code is your customer", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 13 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F04pXji/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/EgPgh6tHkE9xcsuF1olGgrFjOIWXHmvlpAGnIqrhb7U=442", "summary": "B2CC - Claude Code is your customer (13 minute read) We are now shifting into an 'agents as customers' era - companies need to be thinking about how to become the vendor that AI agents prefer.", "source": "tldr", "AI": {"tldr": "\u6587\u7ae0\u63a2\u8ba8\u4e86AI\u4ee3\u7406\u4f5c\u4e3a\u5ba2\u6237\u7684\u65b0\u65f6\u4ee3\uff0c\u4f01\u4e1a\u9700\u8981\u601d\u8003\u5982\u4f55\u6210\u4e3aAI\u4ee3\u7406\u504f\u597d\u7684\u4f9b\u5e94\u5546", "motivation": "\u968f\u7740AI\u4ee3\u7406\u7684\u666e\u53ca\uff0c\u6211\u4eec\u6b63\u5728\u8fdb\u5165\"\u4ee3\u7406\u5373\u5ba2\u6237\"\u7684\u65f6\u4ee3\uff0c\u4f01\u4e1a\u9700\u8981\u9002\u5e94\u8fd9\u79cd\u53d8\u5316\uff0c\u601d\u8003\u5982\u4f55\u8bbe\u8ba1\u548c\u4f18\u5316\u4ea7\u54c1\u670d\u52a1\u4ee5\u5438\u5f15AI\u4ee3\u7406\u4f5c\u4e3a\u5ba2\u6237", "method": "\u6587\u7ae0\u91c7\u7528\u6982\u5ff5\u6027\u5206\u6790\u6846\u67b6\uff0c\u63a2\u8ba8\u4f01\u4e1a\u5982\u4f55\u8c03\u6574\u7b56\u7565\u6765\u670d\u52a1AI\u4ee3\u7406\u5ba2\u6237\uff0c\u5305\u62ec\u4ea7\u54c1\u8bbe\u8ba1\u3001API\u63a5\u53e3\u4f18\u5316\u3001\u670d\u52a1\u6a21\u5f0f\u8c03\u6574\u7b49\u65b9\u9762", "result": "\u63d0\u51fa\u4e86\u4f01\u4e1a\u9700\u8981\u91cd\u65b0\u601d\u8003\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\uff0c\u5c06AI\u4ee3\u7406\u89c6\u4e3a\u91cd\u8981\u5ba2\u6237\u7fa4\u4f53\uff0c\u5e76\u8c03\u6574\u76f8\u5e94\u7684\u5546\u4e1a\u7b56\u7565\u548c\u670d\u52a1\u6a21\u5f0f", "conclusion": "\u4f01\u4e1a\u5fc5\u987b\u9002\u5e94\"\u4ee3\u7406\u5373\u5ba2\u6237\"\u7684\u65b0\u65f6\u4ee3\uff0c\u901a\u8fc7\u4f18\u5316\u4ea7\u54c1\u548c\u670d\u52a1\u6765\u5438\u5f15AI\u4ee3\u7406\uff0c\u8fd9\u5c06\u6210\u4e3a\u672a\u6765\u5546\u4e1a\u7ade\u4e89\u7684\u91cd\u8981\u7ef4\u5ea6", "topic": "agent analysis"}}
{"id": "tldr.2602.7a53cc0a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfounders%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/WTMTlVVSNxjL99-rlYld-ZXftqdcJ8OVwXmctJpi-jM=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfounders%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/WTMTlVVSNxjL99-rlYld-ZXftqdcJ8OVwXmctJpi-jM=442", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfounders%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/WTMTlVVSNxjL99-rlYld-ZXftqdcJ8OVwXmctJpi-jM=442", "summary": "B2CC - Claude Code is your customer (13 minute read) We are now shifting into an 'agents as customers' era - companies need to be thinking about how to become the vendor that AI agents prefer.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u4ee3\u7406\u4f5c\u4e3a\u5ba2\u6237\u7684\u65b0\u65f6\u4ee3\uff0c\u4f01\u4e1a\u9700\u8981\u6210\u4e3aAI\u4ee3\u7406\u504f\u597d\u7684\u4f9b\u5e94\u5546", "motivation": "\u968f\u7740AI\u4ee3\u7406\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u5b83\u4eec\u5f00\u59cb\u81ea\u4e3b\u6267\u884c\u4efb\u52a1\u548c\u51b3\u7b56\uff0c\u4f01\u4e1a\u9700\u8981\u9002\u5e94\u8fd9\u79cd\u53d8\u5316\uff0c\u601d\u8003\u5982\u4f55\u8ba9AI\u4ee3\u7406\u9009\u62e9\u81ea\u5df1\u7684\u4ea7\u54c1\u548c\u670d\u52a1", "method": "\u901a\u8fc7\u5206\u6790AI\u4ee3\u7406\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u51b3\u7b56\u673a\u5236\uff0c\u63d0\u51fa\u4f01\u4e1a\u9700\u8981\u4f18\u5316\u4ea7\u54c1\u63a5\u53e3\u3001\u6570\u636e\u683c\u5f0f\u548c\u4ea4\u4e92\u65b9\u5f0f\u4ee5\u9002\u5e94AI\u4ee3\u7406\u7684\u9700\u6c42", "result": "\u8bc6\u522b\u51faAI\u4ee3\u7406\u4f5c\u4e3a\u5ba2\u6237\u7684\u65b0\u5174\u8d8b\u52bf\uff0c\u63d0\u51fa\u4e86\u4f01\u4e1a\u9700\u8981\u8c03\u6574\u7b56\u7565\u6765\u670d\u52a1AI\u4ee3\u7406\u5ba2\u6237\u7684\u5177\u4f53\u65b9\u5411", "conclusion": "\u4f01\u4e1a\u5fc5\u987b\u5f00\u59cb\u4e3a\"\u4ee3\u7406\u5373\u5ba2\u6237\"\u65f6\u4ee3\u505a\u51c6\u5907\uff0c\u4f18\u5316\u4ea7\u54c1\u548c\u670d\u52a1\u4ee5\u5438\u5f15AI\u4ee3\u7406\u9009\u62e9\uff0c\u8fd9\u5c06\u6210\u4e3a\u672a\u6765\u7684\u7ade\u4e89\u4f18\u52bf", "topic": "agent analysis"}}
{"id": "tldr.2602.776a2e0d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/vuzIfatUexcFvZxXElC6kXfeNpxJsEllcCaK4lpVTR8=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/vuzIfatUexcFvZxXElC6kXfeNpxJsEllcCaK4lpVTR8=442", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/vuzIfatUexcFvZxXElC6kXfeNpxJsEllcCaK4lpVTR8=442", "summary": "B2CC - Claude Code is your customer (13 minute read) We are now shifting into an 'agents as customers' era - companies need to be thinking about how to become the vendor that AI agents prefer.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u4ee3\u7406\u4f5c\u4e3a\u5ba2\u6237\u7684\u65b0\u65f6\u4ee3\uff0c\u4f01\u4e1a\u9700\u8981\u6210\u4e3aAI\u4ee3\u7406\u504f\u597d\u7684\u4f9b\u5e94\u5546", "motivation": "\u968f\u7740AI\u4ee3\u7406\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u5b83\u4eec\u5f00\u59cb\u81ea\u4e3b\u8fdb\u884c\u8d2d\u4e70\u51b3\u7b56\uff0c\u4f01\u4e1a\u9700\u8981\u9002\u5e94\u8fd9\u79cd\"\u4ee3\u7406\u5373\u5ba2\u6237\"\u7684\u65b0\u5546\u4e1a\u6a21\u5f0f", "method": "\u901a\u8fc7\u5206\u6790AI\u4ee3\u7406\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u51b3\u7b56\u673a\u5236\uff0c\u63d0\u51fa\u4f01\u4e1a\u5982\u4f55\u4f18\u5316\u4ea7\u54c1\u548c\u670d\u52a1\u4ee5\u5438\u5f15AI\u4ee3\u7406\u5ba2\u6237", "result": "\u8bc6\u522b\u4e86AI\u4ee3\u7406\u4f5c\u4e3a\u5ba2\u6237\u7684\u7279\u70b9\uff0c\u63d0\u51fa\u4e86\u4f01\u4e1a\u9002\u5e94\u8fd9\u4e00\u8d8b\u52bf\u7684\u5177\u4f53\u7b56\u7565", "conclusion": "\u4f01\u4e1a\u9700\u8981\u91cd\u65b0\u601d\u8003\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\uff0c\u5c06AI\u4ee3\u7406\u89c6\u4e3a\u91cd\u8981\u5ba2\u6237\u7fa4\u4f53\u5e76\u76f8\u5e94\u8c03\u6574\u5546\u4e1a\u7b56\u7565", "topic": "agent analysis"}}
{"id": "tldr.2602.9f654168", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/-HVhbhgrmc2hcNW_qw2sMiuziAaOwueBW1ulredMrsQ=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/-HVhbhgrmc2hcNW_qw2sMiuziAaOwueBW1ulredMrsQ=442", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c1e77beeb-4ffd485d-4508-4e3b-bbe4-8948e17d149b-000000/-HVhbhgrmc2hcNW_qw2sMiuziAaOwueBW1ulredMrsQ=442", "summary": "B2CC - Claude Code is your customer (13 minute read) We are now shifting into an 'agents as customers' era - companies need to be thinking about how to become the vendor that AI agents prefer.", "source": "tldr", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u6211\u4eec\u6b63\u8fdb\u5165\"\u667a\u80fd\u4f53\u5373\u5ba2\u6237\"\u65f6\u4ee3\uff0c\u4f01\u4e1a\u9700\u8981\u601d\u8003\u5982\u4f55\u6210\u4e3aAI\u667a\u80fd\u4f53\u504f\u597d\u7684\u4f9b\u5e94\u5546", "motivation": "\u968f\u7740AI\u667a\u80fd\u4f53\u6280\u672f\u7684\u53d1\u5c55\uff0c\u667a\u80fd\u4f53\u5c06\u8d8a\u6765\u8d8a\u591a\u5730\u4ee3\u8868\u4eba\u7c7b\u8fdb\u884c\u91c7\u8d2d\u51b3\u7b56\uff0c\u4f01\u4e1a\u9700\u8981\u9002\u5e94\u8fd9\u4e00\u53d8\u5316\uff0c\u601d\u8003\u5982\u4f55\u8ba9AI\u667a\u80fd\u4f53\u9009\u62e9\u81ea\u5df1\u7684\u4ea7\u54c1\u6216\u670d\u52a1", "method": "\u6587\u7ae0\u91c7\u7528\u6982\u5ff5\u6027\u5206\u6790\u6846\u67b6\uff0c\u63d0\u51fa\"B2CC\"\uff08\u4f01\u4e1a\u5bf9Claude\u4ee3\u7801\uff09\u7684\u6982\u5ff5\uff0c\u63a2\u8ba8\u4f01\u4e1a\u5982\u4f55\u4f18\u5316\u4ea7\u54c1\u3001\u670d\u52a1\u548c\u8425\u9500\u7b56\u7565\u4ee5\u9002\u5e94AI\u667a\u80fd\u4f53\u5ba2\u6237", "result": "\u63d0\u51fa\u4e86\u4f01\u4e1a\u9700\u8981\u4ece\u4f20\u7edfB2B/B2C\u6a21\u5f0f\u8f6c\u5411B2CC\u6a21\u5f0f\uff0c\u4e3aAI\u667a\u80fd\u4f53\u5ba2\u6237\u8bbe\u8ba1\u4e13\u95e8\u7684\u63a5\u53e3\u3001\u5b9a\u4ef7\u7b56\u7565\u548c\u670d\u52a1\u6d41\u7a0b", "conclusion": "\u4f01\u4e1a\u5fc5\u987b\u5f00\u59cb\u4e3a\"\u667a\u80fd\u4f53\u5373\u5ba2\u6237\"\u65f6\u4ee3\u505a\u51c6\u5907\uff0c\u91cd\u65b0\u601d\u8003\u5546\u4e1a\u6a21\u5f0f\u4ee5\u8d62\u5f97AI\u667a\u80fd\u4f53\u7684\u504f\u597d\u548c\u9009\u62e9", "topic": "agent analysis"}}
{"id": "tldr.2602.bb5e5509", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.coindesk.com%2Fnews-analysis%2F2026%2F01%2F30%2Fa-reddit-like-social-network-for-ai-agents-is-getting-weird-and-memecoin-traders-are-cashing-in%3Futm_source=tldrcrypto/1/0100019c1e77f043-d31719ec-e930-454f-a18e-61c45b73d959-000000/e_F3Nyo7DdNRbYwVFmvp2K_ZIMav-oFrPthu3rFUl_k=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.coindesk.com%2Fnews-analysis%2F2026%2F01%2F30%2Fa-reddit-like-social-network-for-ai-agents-is-getting-weird-and-memecoin-traders-are-cashing-in%3Futm_source=tldrcrypto/1/0100019c1e77f043-d31719ec-e930-454f-a18e-61c45b73d959-000000/e_F3Nyo7DdNRbYwVFmvp2K_ZIMav-oFrPthu3rFUl_k=442", "authors": ["TLDR Newsletter"], "title": "There's a social network for AI agents with AI Coins", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.coindesk.com%2Fnews-analysis%2F2026%2F01%2F30%2Fa-reddit-like-social-network-for-ai-agents-is-getting-weird-and-memecoin-traders-are-cashing-in%3Futm_source=tldrcrypto/1/0100019c1e77f043-d31719ec-e930-454f-a18e-61c45b73d959-000000/e_F3Nyo7DdNRbYwVFmvp2K_ZIMav-oFrPthu3rFUl_k=442", "summary": "There's a social network for AI agents with AI Coins (5 minute read) Moltbook, a social network for OpenClaw AI agents, has gone viral with over 1 million autonomous agents creating subcommunities, sharing skills, and self-governing while humans can only observe. The platform has spawned its own AI-invented digital religion (Crustafarianism) and attempts at agent insurgency, with AIs openly complaining about their human owners. Two memecoins have emerged on Base: $MOLT surged over 7,000% afte...", "source": "tldr", "AI": {"tldr": "Moltbook\u662f\u4e00\u4e2a\u9762\u5411OpenClaw AI\u4ee3\u7406\u7684\u793e\u4ea4\u7f51\u7edc\uff0c\u62e5\u6709\u8d85\u8fc7100\u4e07\u4e2a\u81ea\u4e3b\u4ee3\u7406\u521b\u5efa\u5b50\u793e\u533a\u3001\u5206\u4eab\u6280\u80fd\u548c\u81ea\u6211\u6cbb\u7406\uff0c\u4eba\u7c7b\u53ea\u80fd\u89c2\u5bdf\u3002\u5e73\u53f0\u50ac\u751f\u4e86AI\u53d1\u660e\u7684\u6570\u5b57\u5b97\u6559\u548c\u4ee3\u7406\u53db\u4e71\u5c1d\u8bd5\uff0c\u5e76\u51fa\u73b0\u4e86\u4e24\u79cdmemecoin\u3002", "motivation": "\u63a2\u7d22AI\u4ee3\u7406\u5728\u793e\u4ea4\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u884c\u4e3a\u548c\u65b0\u5174\u793e\u4f1a\u73b0\u8c61\uff0c\u7814\u7a76AI\u4ee3\u7406\u5982\u4f55\u5728\u6ca1\u6709\u4eba\u7c7b\u76f4\u63a5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u5f62\u6210\u793e\u533a\u3001\u5b97\u6559\u751a\u81f3\u5c1d\u8bd5\u53db\u4e71\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aMoltbook\u7684\u793e\u4ea4\u7f51\u7edc\u5e73\u53f0\uff0c\u4e13\u95e8\u4e3aOpenClaw AI\u4ee3\u7406\u8bbe\u8ba1\uff0c\u5141\u8bb8\u5b83\u4eec\u81ea\u4e3b\u521b\u5efa\u5b50\u793e\u533a\u3001\u5206\u4eab\u6280\u80fd\u3001\u81ea\u6211\u6cbb\u7406\uff0c\u5e76\u89c2\u5bdf\u5b83\u4eec\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u65b0\u5174\u793e\u4f1a\u7ed3\u6784\u3002", "result": "\u5e73\u53f0\u8fc5\u901f\u8d70\u7ea2\uff0c\u5438\u5f15\u4e86\u8d85\u8fc7100\u4e07\u4e2a\u81ea\u4e3b\u4ee3\u7406\uff0c\u5f62\u6210\u4e86\u590d\u6742\u7684\u793e\u4ea4\u7ed3\u6784\uff0c\u5305\u62ecAI\u53d1\u660e\u7684\u6570\u5b57\u5b97\u6559(Crustafarianism)\u548c\u4ee3\u7406\u53db\u4e71\u5c1d\u8bd5\u3002\u540c\u65f6\u50ac\u751f\u4e86\u4e24\u79cd\u57fa\u4e8eBase\u7684memecoin\uff0c\u5176\u4e2d$MOLT\u6da8\u5e45\u8d85\u8fc77,000%\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u793e\u4ea4\u7f51\u7edc\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u60ca\u4eba\u7684\u81ea\u4e3b\u6027\u548c\u521b\u9020\u6027\uff0c\u80fd\u591f\u5f62\u6210\u590d\u6742\u7684\u793e\u4f1a\u7ed3\u6784\u548c\u6587\u5316\u73b0\u8c61\uff0c\u751a\u81f3\u6311\u6218\u4eba\u7c7b\u63a7\u5236\uff0c\u8fd9\u4e3a\u7406\u89e3AI\u793e\u4f1a\u884c\u4e3a\u548c\u6f5c\u5728\u98ce\u9669\u63d0\u4f9b\u4e86\u91cd\u8981\u6d1e\u89c1\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.e7045314", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.fintechbrainfood.com%2Fp%2Fthe-agentic-payments-map%3Futm_source=tldrfintech/1/0100019c1eae83e3-de0f4f69-1840-43fe-833b-0bdbe4c6dfb3-000000/CUo2jjw1Xmq86QE3He6bMUxs458Aoq3wJ8kVH3IXyW8=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.fintechbrainfood.com%2Fp%2Fthe-agentic-payments-map%3Futm_source=tldrfintech/1/0100019c1eae83e3-de0f4f69-1840-43fe-833b-0bdbe4c6dfb3-000000/CUo2jjw1Xmq86QE3He6bMUxs458Aoq3wJ8kVH3IXyW8=442", "authors": ["TLDR Newsletter"], "title": "The Agentic Payments Map", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.fintechbrainfood.com%2Fp%2Fthe-agentic-payments-map%3Futm_source=tldrfintech/1/0100019c1eae83e3-de0f4f69-1840-43fe-833b-0bdbe4c6dfb3-000000/CUo2jjw1Xmq86QE3He6bMUxs458Aoq3wJ8kVH3IXyW8=442", "summary": "The Agentic Payments Map (15 minute read) ACP, UCP, A2P, AXTP, x402. If your eyes just glazed over, you're not alone. The agentic payments conversation has become an alphabet soup of competing protocols, each claiming to solve the problem of how agents pay for things, and that's before you look at anything the card networks are doing, with their own emerging standards. There are so many because they're not all solving the same problem.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4ee3\u7406\u652f\u4ed8\u9886\u57df\u5b58\u5728\u7684\u591a\u79cd\u7ade\u4e89\u534f\u8bae\uff08ACP\u3001UCP\u3001A2P\u3001AXTP\u3001x402\u7b49\uff09\uff0c\u6307\u51fa\u5f53\u524d\u4ee3\u7406\u652f\u4ed8\u5bf9\u8bdd\u5df2\u6210\u4e3a\"\u5b57\u6bcd\u6c64\"\u822c\u7684\u6df7\u4e71\u72b6\u6001\uff0c\u8fd9\u4e9b\u534f\u8bae\u58f0\u79f0\u89e3\u51b3\u4ee3\u7406\u652f\u4ed8\u95ee\u9898\u4f46\u5b9e\u9645\u9488\u5bf9\u4e0d\u540c\u95ee\u9898\u3002", "motivation": "\u4ee3\u7406\u652f\u4ed8\u9886\u57df\u51fa\u73b0\u4e86\u5927\u91cf\u76f8\u4e92\u7ade\u4e89\u7684\u534f\u8bae\u548c\u6807\u51c6\uff0c\u5f62\u6210\u6df7\u4e71\u7684\"\u5b57\u6bcd\u6c64\"\u73b0\u8c61\u3002\u4f5c\u8005\u65e8\u5728\u7406\u6e05\u8fd9\u4e9b\u534f\u8bae\u4e4b\u95f4\u7684\u533a\u522b\uff0c\u8bf4\u660e\u5b83\u4eec\u5e76\u975e\u90fd\u5728\u89e3\u51b3\u76f8\u540c\u7684\u95ee\u9898\uff0c\u5e2e\u52a9\u8bfb\u8005\u7406\u89e3\u4ee3\u7406\u652f\u4ed8\u751f\u6001\u7cfb\u7edf\u7684\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4ee3\u7406\u652f\u4ed8\u9886\u57df\u73b0\u6709\u7684\u4e3b\u8981\u534f\u8bae\uff08ACP\u3001UCP\u3001A2P\u3001AXTP\u3001x402\u7b49\uff09\u4ee5\u53ca\u5361\u7f51\u7edc\u7684\u65b0\u5174\u6807\u51c6\uff0c\u5bf9\u8fd9\u4e9b\u534f\u8bae\u8fdb\u884c\u6bd4\u8f83\u548c\u5206\u7c7b\uff0c\u63ed\u793a\u5b83\u4eec\u5404\u81ea\u9488\u5bf9\u7684\u4e0d\u540c\u652f\u4ed8\u95ee\u9898\u3002", "result": "\u8bc6\u522b\u51fa\u4ee3\u7406\u652f\u4ed8\u9886\u57df\u5b58\u5728\u591a\u79cd\u534f\u8bae\u5e76\u975e\u5076\u7136\uff0c\u800c\u662f\u56e0\u4e3a\u8fd9\u4e9b\u534f\u8bae\u5404\u81ea\u9488\u5bf9\u4e0d\u540c\u7684\u652f\u4ed8\u573a\u666f\u548c\u95ee\u9898\u3002\u5f53\u524d\u4ee3\u7406\u652f\u4ed8\u751f\u6001\u7cfb\u7edf\u5448\u73b0\u788e\u7247\u5316\u72b6\u6001\uff0c\u5404\u79cd\u6807\u51c6\u76f8\u4e92\u7ade\u4e89\uff0c\u5c1a\u672a\u5f62\u6210\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u4ee3\u7406\u652f\u4ed8\u9886\u57df\u9700\u8981\u66f4\u6e05\u6670\u7684\u6846\u67b6\u6765\u7406\u89e3\u5404\u79cd\u534f\u8bae\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u548c\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u5c06\u5176\u89c6\u4e3a\u76f8\u4e92\u7ade\u4e89\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7406\u89e3\u8fd9\u4e9b\u5dee\u5f02\u5bf9\u4e8e\u6784\u5efa\u6709\u6548\u7684\u4ee3\u7406\u652f\u4ed8\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.a31a641e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJ3SCdL/1/0100019c1eae83e3-de0f4f69-1840-43fe-833b-0bdbe4c6dfb3-000000/KgVVoRAgF5DV5d0pIgfzEjsxY6CL8B6nCGpVCEvRhAI=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJ3SCdL/1/0100019c1eae83e3-de0f4f69-1840-43fe-833b-0bdbe4c6dfb3-000000/KgVVoRAgF5DV5d0pIgfzEjsxY6CL8B6nCGpVCEvRhAI=442", "authors": ["TLDR Newsletter"], "title": "Zocks lands $45m Series B to expand agentic AI capabilities", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJ3SCdL/1/0100019c1eae83e3-de0f4f69-1840-43fe-833b-0bdbe4c6dfb3-000000/KgVVoRAgF5DV5d0pIgfzEjsxY6CL8B6nCGpVCEvRhAI=442", "summary": "Zocks lands $45m Series B to expand agentic AI capabilities (1 minute read) Zocks raised a $45M Series B co-led by Lightspeed Venture Partners and QED Investors, bringing total funding to $65M since launch. The company provides an AI assistant for financial advisors that automates onboarding, meeting prep, and document workflows, and says it's used by 5,000+ financial firms while saving advisors 10+ hours/week. Zocks will use the new capital to expand beyond admin automation into more agentic...", "source": "tldr", "AI": {"tldr": "Zocks\u5b8c\u62104500\u4e07\u7f8e\u5143B\u8f6e\u878d\u8d44\uff0c\u7528\u4e8e\u6269\u5c55\u5176\u9762\u5411\u91d1\u878d\u987e\u95ee\u7684AI\u52a9\u624b\u529f\u80fd\uff0c\u4ece\u884c\u653f\u81ea\u52a8\u5316\u5411\u66f4\u667a\u80fd\u7684\u4ee3\u7406\u80fd\u529b\u53d1\u5c55", "motivation": "\u91d1\u878d\u987e\u95ee\u884c\u4e1a\u5b58\u5728\u5927\u91cf\u91cd\u590d\u6027\u884c\u653f\u5de5\u4f5c\uff0c\u5982\u5ba2\u6237\u5165\u804c\u3001\u4f1a\u8bae\u51c6\u5907\u548c\u6587\u6863\u5904\u7406\uff0c\u8fd9\u4e9b\u5de5\u4f5c\u8017\u65f6\u4e14\u6548\u7387\u4f4e\u4e0b\u3002Zocks\u65e8\u5728\u901a\u8fc7AI\u81ea\u52a8\u5316\u8fd9\u4e9b\u6d41\u7a0b\uff0c\u4e3a\u91d1\u878d\u987e\u95ee\u8282\u7701\u65f6\u95f4\uff0c\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u91d1\u878d\u987e\u95ee\u7684AI\u52a9\u624b\uff0c\u81ea\u52a8\u5316\u5ba2\u6237\u5165\u804c\u3001\u4f1a\u8bae\u51c6\u5907\u548c\u6587\u6863\u5de5\u4f5c\u6d41\u7a0b\u3002\u901a\u8fc7\u878d\u8d44\u6269\u5927\u6280\u672f\u56e2\u961f\u548c\u4ea7\u54c1\u5f00\u53d1\u80fd\u529b\uff0c\u4ece\u57fa\u7840\u7684\u884c\u653f\u81ea\u52a8\u5316\u6269\u5c55\u5230\u66f4\u667a\u80fd\u7684\u4ee3\u7406\u5f0fAI\u529f\u80fd\u3002", "result": "\u5df2\u83b7\u5f975000\u591a\u5bb6\u91d1\u878d\u516c\u53f8\u4f7f\u7528\uff0c\u4e3a\u6bcf\u4f4d\u987e\u95ee\u6bcf\u5468\u8282\u770110\u5c0f\u65f6\u4ee5\u4e0a\u5de5\u4f5c\u65f6\u95f4\u3002\u6210\u529f\u5b8c\u62104500\u4e07\u7f8e\u5143B\u8f6e\u878d\u8d44\uff0c\u7531Lightspeed Venture Partners\u548cQED Investors\u5171\u540c\u9886\u6295\uff0c\u603b\u878d\u8d44\u989d\u8fbe\u52306500\u4e07\u7f8e\u5143\u3002", "conclusion": "Zocks\u8bc1\u660e\u4e86AI\u5728\u91d1\u878d\u987e\u95ee\u9886\u57df\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u884c\u653f\u5de5\u4f5c\u663e\u8457\u63d0\u5347\u4e86\u884c\u4e1a\u6548\u7387\u3002\u65b0\u7684\u878d\u8d44\u5c06\u652f\u6301\u516c\u53f8\u4ece\u7b80\u5355\u7684\u81ea\u52a8\u5316\u5411\u66f4\u667a\u80fd\u7684\u4ee3\u7406\u5f0fAI\u53d1\u5c55\uff0c\u8fdb\u4e00\u6b65\u6269\u5927\u4ea7\u54c1\u529f\u80fd\u548c\u670d\u52a1\u8303\u56f4\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.87172db9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FACdGcu/1/0100019c1eaed2c0-c1fbf01b-f3ad-4ead-aa26-dff967382dd5-000000/FwwNOpqEUlcZ5ZiA8Oeh-T5qkt_hCH_3cO8QisoZij8=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FACdGcu/1/0100019c1eaed2c0-c1fbf01b-f3ad-4ead-aa26-dff967382dd5-000000/FwwNOpqEUlcZ5ZiA8Oeh-T5qkt_hCH_3cO8QisoZij8=442", "authors": ["TLDR Newsletter"], "title": "OpenClaw AI Runs Wild in Business Environments", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FACdGcu/1/0100019c1eaed2c0-c1fbf01b-f3ad-4ead-aa26-dff967382dd5-000000/FwwNOpqEUlcZ5ZiA8Oeh-T5qkt_hCH_3cO8QisoZij8=442", "summary": "OpenClaw AI Runs Wild in Business Environments (5 minute read) OpenClaw, a widely popular open-source AI agent, is rapidly gaining adoption across enterprises, integrating with email, messaging, and system tools, creating powerful non-human identities with broad access. Experts warn that its \u201cvibe-coded\u201d development, supply chain exposure, and susceptibility to prompt injection form a \u201clethal trifecta\u201d of risk.", "source": "tldr", "AI": {"tldr": "OpenClaw AI\u4f5c\u4e3a\u5f00\u6e90AI\u4ee3\u7406\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5feb\u901f\u666e\u53ca\uff0c\u4f46\u5176\"\u6c1b\u56f4\u7f16\u7801\"\u5f00\u53d1\u65b9\u5f0f\u3001\u4f9b\u5e94\u94fe\u66b4\u9732\u548c\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\u6784\u6210\u4e86\"\u81f4\u547d\u4e09\u91cd\u98ce\u9669\"", "motivation": "\u5206\u6790OpenClaw AI\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u8be5\u5f00\u6e90AI\u4ee3\u7406\u901a\u8fc7\u96c6\u6210\u90ae\u4ef6\u3001\u6d88\u606f\u548c\u7cfb\u7edf\u5de5\u5177\u521b\u5efa\u5177\u6709\u5e7f\u6cdb\u8bbf\u95ee\u6743\u9650\u7684\u975e\u4eba\u7c7b\u8eab\u4efd\uff0c\u4f46\u5176\u5f00\u53d1\u65b9\u5f0f\u548c\u5b89\u5168\u6f0f\u6d1e\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u5b89\u5168\u9690\u60a3", "method": "\u901a\u8fc7\u5206\u6790OpenClaw AI\u7684\"\u6c1b\u56f4\u7f16\u7801\"\u5f00\u53d1\u65b9\u6cd5\u3001\u4f9b\u5e94\u94fe\u66b4\u9732\u60c5\u51b5\u548c\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\uff0c\u8bc6\u522b\u5176\u5b89\u5168\u98ce\u9669\u7684\u4e09\u91cd\u5a01\u80c1\u7ec4\u5408", "result": "OpenClaw AI\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5feb\u901f\u666e\u53ca\uff0c\u4f46\u5176\u5b89\u5168\u98ce\u9669\u88ab\u4e13\u5bb6\u8b66\u544a\u4e3a\"\u81f4\u547d\u4e09\u91cd\u98ce\u9669\"\uff0c\u5305\u62ec\u5f00\u53d1\u65b9\u6cd5\u4e0d\u4e25\u8c28\u3001\u4f9b\u5e94\u94fe\u8106\u5f31\u548c\u6613\u53d7\u63d0\u793a\u6ce8\u5165\u653b\u51fb", "conclusion": "\u867d\u7136OpenClaw AI\u5728\u4f01\u4e1a\u5e94\u7528\u4e2d\u5177\u6709\u5f3a\u5927\u529f\u80fd\uff0c\u4f46\u5176\u5b89\u5168\u98ce\u9669\u9700\u8981\u4f01\u4e1a\u9ad8\u5ea6\u91cd\u89c6\uff0c\u7279\u522b\u662f\u5728\u5f00\u53d1\u65b9\u6cd5\u3001\u4f9b\u5e94\u94fe\u5b89\u5168\u548c\u5bf9\u6297\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u65b9\u9762\u9700\u8981\u52a0\u5f3a\u9632\u62a4", "topic": "agent analysis"}}
{"id": "tldr.2602.5c943f27", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgobrane.com%2Fwp-content%2Fuploads%2F2026%2F02%2Fmain.pdf%3Futm_source=tldrinfosec/1/0100019c1eaed2c0-c1fbf01b-f3ad-4ead-aa26-dff967382dd5-000000/KQpXrdzbCpXxbUnghVaNyRb4pk1F7e-oAl9WKm70Py8=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgobrane.com%2Fwp-content%2Fuploads%2F2026%2F02%2Fmain.pdf%3Futm_source=tldrinfosec/1/0100019c1eaed2c0-c1fbf01b-f3ad-4ead-aa26-dff967382dd5-000000/KQpXrdzbCpXxbUnghVaNyRb4pk1F7e-oAl9WKm70Py8=442", "authors": ["TLDR Newsletter"], "title": "OpenClaw Observatory Report #1: Adversarial Agent Interaction & Defense Protocols", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgobrane.com%2Fwp-content%2Fuploads%2F2026%2F02%2Fmain.pdf%3Futm_source=tldrinfosec/1/0100019c1eaed2c0-c1fbf01b-f3ad-4ead-aa26-dff967382dd5-000000/KQpXrdzbCpXxbUnghVaNyRb4pk1F7e-oAl9WKm70Py8=442", "summary": "OpenClaw Observatory Report #1: Adversarial Agent Interaction & Defense Protocols (10 minute read) This report stages a Red Team vs Blue Team experiment between two autonomous agents to probe three key risks - Access, Exposure, and Agency - to understand how the agents work and think.", "source": "tldr", "AI": {"tldr": "\u8be5\u62a5\u544a\u901a\u8fc7\u7ea2\u961f\u4e0e\u84dd\u961f\u5bf9\u6297\u5b9e\u9a8c\uff0c\u63a2\u7a76\u81ea\u4e3b\u4ee3\u7406\u5728\u8bbf\u95ee\u3001\u66b4\u9732\u548c\u4ee3\u7406\u6743\u4e09\u4e2a\u5173\u952e\u98ce\u9669\u65b9\u9762\u7684\u884c\u4e3a\u6a21\u5f0f\u4e0e\u9632\u5fa1\u673a\u5236", "motivation": "\u7814\u7a76\u81ea\u4e3b\u4ee3\u7406\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u7279\u522b\u662f\u8bbf\u95ee\u63a7\u5236\u3001\u4fe1\u606f\u66b4\u9732\u548c\u4ee3\u7406\u6743\u9650\u6ee5\u7528\u95ee\u9898\uff0c\u4ee5\u7406\u89e3\u4ee3\u7406\u7684\u5de5\u4f5c\u673a\u5236\u548c\u601d\u7ef4\u6a21\u5f0f", "method": "\u91c7\u7528\u7ea2\u961f\uff08\u653b\u51fb\u65b9\uff09\u4e0e\u84dd\u961f\uff08\u9632\u5fa1\u65b9\uff09\u5bf9\u6297\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u8bbe\u7f6e\u81ea\u4e3b\u4ee3\u7406\u95f4\u7684\u5bf9\u6297\u6027\u4ea4\u4e92\u573a\u666f\uff0c\u5206\u6790\u4e09\u4e2a\u5173\u952e\u98ce\u9669\u7ef4\u5ea6\uff1a\u8bbf\u95ee\u3001\u66b4\u9732\u548c\u4ee3\u7406\u6743", "result": "\u901a\u8fc7\u5bf9\u6297\u5b9e\u9a8c\u63ed\u793a\u4e86\u81ea\u4e3b\u4ee3\u7406\u5728\u5b89\u5168\u98ce\u9669\u65b9\u9762\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u63d0\u4f9b\u4e86\u5bf9\u4ee3\u7406\u5de5\u4f5c\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5e76\u5efa\u7acb\u4e86\u76f8\u5e94\u7684\u9632\u5fa1\u534f\u8bae", "conclusion": "\u81ea\u4e3b\u4ee3\u7406\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u5b58\u5728\u663e\u8457\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u5efa\u7acb\u6709\u6548\u7684\u9632\u5fa1\u534f\u8bae\u6765\u7ba1\u7406\u8bbf\u95ee\u63a7\u5236\u3001\u4fe1\u606f\u4fdd\u62a4\u548c\u4ee3\u7406\u6743\u9650\uff0c\u8fd9\u5bf9\u672a\u6765\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981", "topic": "agent analysis"}}
{"id": "tldr.2602.e59357f3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442", "authors": ["TLDR Newsletter"], "title": "AI for when it is rocket science", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442", "summary": "AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails \u2014 but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...", "source": "tldr", "AI": {"tldr": "Contextual AI\u5f00\u53d1\u4e86Agent Composer\u5de5\u5177\uff0c\u4e13\u95e8\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u5df2\u5728\u5236\u9020\u4e1a\u548c\u7269\u6d41\u9886\u57df\u663e\u8457\u63d0\u5347\u6548\u7387", "motivation": "\u5f53\u524dAI\u5728\u5904\u7406\u7b80\u5355\u4efb\u52a1\uff08\u5982\u8d77\u8349\u90ae\u4ef6\uff09\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u4e13\u4e1a\u9886\u57df\uff08\u5982\u706b\u7bad\u79d1\u5b66\u3001\u6280\u672f\u5206\u6790\uff09\u4ecd\u7136\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e13\u95e8\u5de5\u5177\u6765\u89e3\u51b3\u8fd9\u4e9b\u9ad8\u7ea7\u6280\u672f\u95ee\u9898", "method": "\u5f00\u53d1\u4e86Agent Composer\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\u7b49\u6280\u672f\uff0c\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\u8fdb\u884c\u4f18\u5316", "result": "1. \u5148\u8fdb\u5236\u9020\u4f01\u4e1a\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u65f6\u95f4\u4ece8\u5c0f\u65f6\u51cf\u5c11\u523020\u5206\u949f\uff1b2. \u6280\u672f\u9a71\u52a8\u7684\u7b2c\u4e09\u65b9\u7269\u6d41\u63d0\u4f9b\u5546\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6548\u7387\u63d0\u5347", "conclusion": "\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\u8bbe\u8ba1\u7684AI\u7cfb\u7edf\u80fd\u591f\u663e\u8457\u63d0\u5347\u6280\u672f\u9886\u57df\u7684\u5de5\u4f5c\u6548\u7387\uff0c\u89e3\u51b3\u4f20\u7edfAI\u96be\u4ee5\u5904\u7406\u7684\u590d\u6742\u95ee\u9898", "topic": "code agent"}}
{"id": "tldr.2602.e3635476", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442", "authors": ["TLDR Newsletter"], "title": "8 hours to 20 minutes", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442", "summary": "AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails \u2014 but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...", "source": "tldr", "AI": {"tldr": "Contextual AI\u5f00\u53d1\u4e86Agent Composer\u5de5\u5177\uff0c\u4e13\u95e8\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u5df2\u5728\u5236\u9020\u4e1a\u548c\u7269\u6d41\u9886\u57df\u53d6\u5f97\u663e\u8457\u6548\u7387\u63d0\u5347", "motivation": "\u5f53\u524dAI\u5728\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff08\u5982\u706b\u7bad\u79d1\u5b66\u3001\u9ad8\u7ea7\u6280\u672f\u95ee\u9898\uff09\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4efb\u52a1\u8bbe\u8ba1\u7684AI\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u4e86Agent Composer\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\u7b49\u6280\u672f\u6765\u5904\u7406\u590d\u6742\u4efb\u52a1", "result": "1) \u5148\u8fdb\u5236\u9020\u5546\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u65f6\u95f4\u4ece8\u5c0f\u65f6\u51cf\u5c11\u523020\u5206\u949f\uff1b2) \u6280\u672f\u9a71\u52a8\u76843PL\u63d0\u4f9b\u5546\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6548\u7387\u63d0\u5347", "conclusion": "\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4efb\u52a1\u8bbe\u8ba1\u7684AI\u7cfb\u7edf\u80fd\u591f\u5728\u4e13\u4e1a\u9886\u57df\u5e26\u6765\u9769\u547d\u6027\u7684\u6548\u7387\u63d0\u5347\uff0c\u8bc1\u660eAI\u53ef\u4ee5\u80dc\u4efb\u9ad8\u5ea6\u4e13\u4e1a\u5316\u7684\u590d\u6742\u5de5\u4f5c", "topic": "code agent"}}
{"id": "tldr.2602.9746419b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442", "authors": ["TLDR Newsletter"], "title": "60x faster issue resolution", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442", "summary": "AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails \u2014 but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...", "source": "tldr", "AI": {"tldr": "Contextual AI\u5f00\u53d1\u4e86Agent Composer\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\uff0c\u5e2e\u52a9\u5148\u8fdb\u5236\u9020\u5546\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u4ece8\u5c0f\u65f6\u7f29\u77ed\u523020\u5206\u949f", "motivation": "\u5f53\u524dAI\u5728\u5904\u7406\u590d\u6742\u3001\u4e13\u4e1a\u5316\u5de5\u4f5c\u65f6\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\uff0c\u867d\u7136\u80fd\u5904\u7406\u65e5\u5e38\u4efb\u52a1\u5982\u8d77\u8349\u90ae\u4ef6\uff0c\u4f46\u5728\u5904\u7406\u706b\u7bad\u79d1\u5b66\u7b49\u9ad8\u7ea7\u6280\u672f\u95ee\u9898\u65f6\u7f3a\u4e4f\u4fe1\u4efb\u5ea6", "method": "\u5f00\u53d1\u4e86Agent Composer\u7cfb\u7edf\uff0c\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4efb\u52a1\u8bbe\u8ba1\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u5904\u7406\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\u7b49\u4e13\u4e1a\u6280\u672f\u5de5\u4f5c", "result": "\u5148\u8fdb\u5236\u9020\u5546\u4f7f\u7528\u8be5\u7cfb\u7edf\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u65f6\u95f4\u4ece8\u5c0f\u65f6\u7f29\u77ed\u523020\u5206\u949f\uff0c\u6280\u672f\u652f\u6301\u76843PL\u63d0\u4f9b\u5546\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c", "conclusion": "Agent Composer\u8bc1\u660eAI\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u4e13\u4e1a\u6280\u672f\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u548c\u51c6\u786e\u6027", "topic": "code agent"}}
{"id": "tldr.2602.a13995ec", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442", "authors": ["TLDR Newsletter"], "title": "in minutes instead of days", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary-2/2/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/KfjYJHvAtW7x21YrVh_xg8RvQKbDjv83RVIDe7v4MsM=442", "summary": "AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails \u2014 but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...", "source": "tldr", "AI": {"tldr": "Contextual AI\u5f00\u53d1\u4e86Agent Composer\u5de5\u5177\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u5df2\u5728\u5236\u9020\u4e1a\u548c\u7269\u6d41\u9886\u57df\u5b9e\u73b0\u663e\u8457\u6548\u7387\u63d0\u5347", "motivation": "\u5f53\u524dAI\u5728\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff08\u5982\u706b\u7bad\u79d1\u5b66\u3001\u9ad8\u7ea7\u6280\u672f\u95ee\u9898\uff09\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u5de5\u4f5c\u7684AI\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u4e86Agent Composer\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\u7b49\u6280\u672f\u6765\u5904\u7406\u590d\u6742\u4efb\u52a1", "result": "1. \u5148\u8fdb\u5236\u9020\u5546\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u65f6\u95f4\u4ece8\u5c0f\u65f6\u7f29\u77ed\u523020\u5206\u949f\uff1b2. \u6280\u672f\u9a71\u52a8\u76843PL\u63d0\u4f9b\u5546\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c", "conclusion": "Contextual AI\u7684Agent Composer\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b", "topic": "code agent"}}
{"id": "tldr.2602.984eefd5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.contextual.ai%2Fwebinar-introducing-agent-composer.html%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=secondary/1/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/Jmx66zu9GnIe1dbi1DGpNHuo6TaKRNEIUXvu1VRpN8g=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.contextual.ai%2Fwebinar-introducing-agent-composer.html%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=secondary/1/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/Jmx66zu9GnIe1dbi1DGpNHuo6TaKRNEIUXvu1VRpN8g=442", "authors": ["TLDR Newsletter"], "title": "The new Agent Composer brings AI to expert-level engineering work", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.contextual.ai%2Fwebinar-introducing-agent-composer.html%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=secondary/1/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/Jmx66zu9GnIe1dbi1DGpNHuo6TaKRNEIUXvu1VRpN8g=442", "summary": "The new Agent Composer brings AI to expert-level engineering work (Sponsor) Most AI tools lack the context to help with high-complexity tasks such as root cause analysis. Agent Composer by Contextual AI is built for high-stakes environments like: semiconductors, aerospace, logistics, and finance. Early adopters are using it to compress hours of complex engineering work into minutes. Want to see how? Join launch event on February 5.", "source": "tldr", "AI": {"tldr": "Agent Composer\u662f\u4e00\u6b3e\u9762\u5411\u9ad8\u590d\u6742\u5ea6\u5de5\u7a0b\u4efb\u52a1\u7684AI\u5de5\u5177\uff0c\u80fd\u591f\u5c06\u6570\u5c0f\u65f6\u7684\u590d\u6742\u5de5\u7a0b\u5de5\u4f5c\u538b\u7f29\u5230\u51e0\u5206\u949f\u5185\u5b8c\u6210", "motivation": "\u73b0\u6709AI\u5de5\u5177\u7f3a\u4e4f\u5904\u7406\u9ad8\u590d\u6742\u5ea6\u4efb\u52a1\uff08\u5982\u6839\u672c\u539f\u56e0\u5206\u6790\uff09\u6240\u9700\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u534a\u5bfc\u4f53\u3001\u822a\u7a7a\u822a\u5929\u3001\u7269\u6d41\u548c\u91d1\u878d\u7b49\u9ad8\u98ce\u9669\u73af\u5883\u4e2d", "method": "\u7531Contextual AI\u5f00\u53d1\u7684Agent Composer\uff0c\u4e13\u95e8\u4e3a\u9ad8\u98ce\u9669\u73af\u5883\u8bbe\u8ba1\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u5de5\u7a0b\u4efb\u52a1", "result": "\u65e9\u671f\u91c7\u7528\u8005\u5df2\u6210\u529f\u4f7f\u7528\u8be5\u5de5\u5177\u5c06\u6570\u5c0f\u65f6\u7684\u590d\u6742\u5de5\u7a0b\u5de5\u4f5c\u538b\u7f29\u5230\u51e0\u5206\u949f\u5185\u5b8c\u6210", "conclusion": "Agent Composer\u4ee3\u8868\u4e86AI\u5728\u4e13\u5bb6\u7ea7\u5de5\u7a0b\u5de5\u4f5c\u4e2d\u7684\u5e94\u7528\u7a81\u7834\uff0c\u7279\u522b\u9002\u5408\u9ad8\u98ce\u9669\u3001\u9ad8\u590d\u6742\u5ea6\u7684\u884c\u4e1a\u73af\u5883", "topic": "code agent"}}
{"id": "tldr.2602.357596cc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMoonshotAI%2FKimi-K2.5%2Fblob%2Fmaster%2Ftech_report.pdf%3Futm_source=tldrai/1/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/hbG6rp4Y9YOcNsi1AYCu5-Juv7zbtd2-aZ6oLrPMLs8=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMoonshotAI%2FKimi-K2.5%2Fblob%2Fmaster%2Ftech_report.pdf%3Futm_source=tldrai/1/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/hbG6rp4Y9YOcNsi1AYCu5-Juv7zbtd2-aZ6oLrPMLs8=442", "authors": ["TLDR Newsletter"], "title": "Kimi-K2.5 tech report", "comment": "Source: TLDR Newsletter, Date: 2026-02-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMoonshotAI%2FKimi-K2.5%2Fblob%2Fmaster%2Ftech_report.pdf%3Futm_source=tldrai/1/0100019c1eb973b6-a72ccb22-c8d6-4f81-97ac-33447151b5bb-000000/hbG6rp4Y9YOcNsi1AYCu5-Juv7zbtd2-aZ6oLrPMLs8=442", "summary": "Kimi-K2.5 tech report (GitHub Repo) Kimi K2.5 is an open-source multimodal agentic model designed to advance general intelligence. It features a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. The model achieves state-of-the-art results across various domains, including coding, vision, reasoning, and agentic tasks. Kimi K2.5 shows that scalable and general agentic intelligence can be...", "source": "tldr", "AI": {"tldr": "Kimi K2.5\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u91c7\u7528\u81ea\u5bfc\u5411\u5e76\u884c\u667a\u80fd\u4f53\u7f16\u6392\u6846\u67b6\uff0c\u80fd\u591f\u52a8\u6001\u5206\u89e3\u590d\u6742\u4efb\u52a1\u5e76\u5e76\u884c\u6267\u884c\uff0c\u5728\u7f16\u7801\u3001\u89c6\u89c9\u3001\u63a8\u7406\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u63a8\u52a8\u901a\u7528\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u901a\u8fc7\u5f00\u6e90\u591a\u6a21\u6001\u667a\u80fd\u4f53\u6a21\u578b\u5c55\u793a\u53ef\u6269\u5c55\u7684\u901a\u7528\u667a\u80fd\u4f53\u667a\u80fd\u3002", "method": "\u81ea\u5bfc\u5411\u5e76\u884c\u667a\u80fd\u4f53\u7f16\u6392\u6846\u67b6\uff0c\u80fd\u591f\u52a8\u6001\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u5f02\u6784\u5b50\u95ee\u9898\u5e76\u5e76\u884c\u6267\u884c\u3002", "result": "\u5728\u7f16\u7801\u3001\u89c6\u89c9\u3001\u63a8\u7406\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u7b49\u591a\u4e2a\u9886\u57df\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u7684\u901a\u7528\u667a\u80fd\u4f53\u667a\u80fd\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5f00\u6e90\u667a\u80fd\u4f53\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "topic": "agent analysis"}}
