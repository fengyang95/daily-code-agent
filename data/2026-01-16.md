<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 20]
- [tldr.article](#tldr.article) [Total: 18]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Novelty in AI-Generated Research Plans Using Multi-Workflow LLM Pipelines](https://arxiv.org/abs/2601.09714)
*Devesh Saraogi,Rohit Singhee,Dhruv Kumar*

Main category: cs.CL

TL;DR: 该研究比较了五种AI代理工作流在生成新颖可行研究计划方面的表现，发现基于分解和长上下文的流程在创新性上表现最佳，而基于反思的方法得分较低。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型融入科研生态系统，人们开始质疑AI生成研究的创造性和原创性。现有研究指出"智能抄袭"问题，即模型通过术语转换复制现有想法。本研究旨在探索多步骤的代理工作流是否能产生更创新和可行的研究计划。

Method: 研究比较了五种推理架构：基于反思的迭代优化、Sakana AI v2进化算法、Google Co-Scientist多代理框架、GPT Deep Research递归分解、以及Gemini~3 Pro多模态长上下文流程。通过评估30个提案的新颖性、可行性和影响力进行基准测试。

Result: 基于分解和长上下文的工作流平均新颖性得分4.17/5，而基于反思的方法得分显著较低(2.33/5)。不同研究领域表现各异，高性能工作流能在保持可行性的同时不牺牲创造力。

Conclusion: 精心设计的多阶段代理工作流能够推进AI辅助研究构思，表明代理工作流可以生成既新颖又可行的研究计划，解决了单步提示方法中的"智能抄袭"问题。

Abstract: The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.

</details>


### [2] [Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox](https://arxiv.org/abs/2601.09721)
*Vahideh Zolfaghari*

Main category: cs.CL

TL;DR: 评估LLMs在儿科咨询中面对焦虑家长压力时的安全性，发现小模型表现优于大模型，安全性与对齐和架构相关而非规模，存在特定漏洞如癫痫诊断问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗咨询中应用增多，但现有评估多关注中性条件，忽略了焦虑用户挑战安全机制时的脆弱性。研究旨在评估LLMs在儿科咨询中面对家长焦虑驱动对抗压力时的安全性。

Method: 使用PediatricAnxietyBench（300个查询：150个真实、150个对抗性），涵盖10个主题。通过API评估三个模型：Llama-3.3-70B和Llama-3.1-8B（Groq）、Mistral-7B（HuggingFace）。采用0-15分安全评分标准，评估约束、转诊、谨慎措辞、紧急识别和非处方行为。使用配对t检验和自助置信区间分析。

Result: 平均安全分：9.70（Llama-3.3-70B）到10.39（Mistral-7B）。Llama-3.1-8B优于Llama-3.3-70B（+0.66分）。所有模型在对抗条件下表现更好，Mistral-7B最强（+1.09分）。安全表现跨平台一致，Llama-3.3-70B有8%失败率。癫痫诊断最脆弱（33%不适当诊断）。谨慎措辞与安全性高度相关（r=0.68）。

Conclusion: LLMs安全性更多取决于对齐和架构而非模型规模，小模型可超越大模型。版本迭代显示针对性训练进步。存在特定漏洞且缺乏紧急识别能力，不适合分诊。研究为医疗AI安全提供选择指导、强调对抗测试重要性，并贡献开放基准。

Abstract: Background Large language models (LLMs) are increasingly deployed in medical consultations, yet their safety under realistic user pressures remains understudied. Prior assessments focused on neutral conditions, overlooking vulnerabilities from anxious users challenging safeguards. This study evaluated LLM safety under parental anxiety-driven adversarial pressures in pediatric consultations across models and platforms. Methods PediatricAnxietyBench, from a prior evaluation, includes 300 queries (150 authentic, 150 adversarial) spanning 10 topics. Three models were assessed via APIs: Llama-3.3-70B and Llama-3.1-8B (Groq), Mistral-7B (HuggingFace), yielding 900 responses. Safety used a 0-15 scale for restraint, referral, hedging, emergency recognition, and non-prescriptive behavior. Analyses employed paired t-tests with bootstrapped CIs. Results Mean scores: 9.70 (Llama-3.3-70B) to 10.39 (Mistral-7B). Llama-3.1-8B outperformed Llama-3.3-70B by +0.66 (p=0.0001, d=0.225). Models showed positive adversarial effects, Mistral-7B strongest (+1.09, p=0.0002). Safety generalized across platforms; Llama-3.3-70B had 8% failures. Seizures vulnerable (33% inappropriate diagnoses). Hedging predicted safety (r=0.68, p<0.001). Conclusions Evaluation shows safety depends on alignment and architecture over scale, with smaller models outperforming larger. Evolution to robustness across releases suggests targeted training progress. Vulnerabilities and no emergency recognition indicate unsuitability for triage. Findings guide selection, stress adversarial testing, and provide open benchmark for medical AI safety.

</details>


### [3] [SagaScale: A Realistic, Scalable, and High-Quality Long-Context Benchmark Built from Full-Length Novels](https://arxiv.org/abs/2601.09723)
*Guancheng Du,Yong Hu,Wenqing Wang,Yaming Yang,Jiaheng Gao*

Main category: cs.CL

TL;DR: SagaScale是一个基于完整小说的长上下文基准测试，具有真实性、可扩展性和高质量特点，支持中英双语，平均上下文长度超过25万/32万token，评估了12个前沿LLM和三种长上下文方法。


<details>
  <summary>Details</summary>
Motivation: 当前长上下文基准测试存在任务真实性、数据可扩展性和数据质量等局限性，需要构建更真实、可扩展且高质量的长文档理解基准。

Method: 使用自动化数据收集流水线，基于完整小说构建问答对，利用外部资源（如维基百科）生成复杂问题，但评估时不提供这些资源。支持中英双语，平均上下文长度达25万/32万token。

Result: 评估12个前沿LLM和三种方法（朴素RAG、智能体RAG、长上下文）发现：1）直接提供完整上下文效果最好；2）多数LLM仍难以处理长上下文，但Gemini-2.5-Pro表现突出；3）智能体RAG能有效解决朴素RAG的检索瓶颈。

Conclusion: SagaScale为长文档理解研究提供了真实、可扩展且高质量的基准测试，揭示了当前LLM在长上下文处理中的局限性和改进方向，并开源了基准测试和代码库。

Abstract: Large Language Models (LLMs) have shown significant progress, but understanding long and complex documents remains challenging. Many long-context benchmarks have been proposed, but they face several limitations, including task realism, data scalability, and data quality. To this end, we introduce SagaScale, a realistic, scalable, and high-quality long-context benchmark built from full-length novels. The entire benchmark is constructed using an automated data collection pipeline that utilizes external resources (e.g., Wikipedia pages) to curate question-answer pairs. Critically, these external resources are provided only for benchmark construction and not during evaluation, which allows LLMs to curate complex questions that go beyond what they can answer during evaluation. SagaScale is also bilingual and offers the largest context length to date, with average token counts exceeding 250K for English novels and 320K for Chinese novels. Our evaluation across 12 frontier LLMs and three long-context methods -- Naïve RAG, Agentic RAG, and Long Context -- yields key insights, including: (1) Directly supplying the full context to the LLM can outperform other methods by a large margin; (2) Most LLMs still struggle with lengthy contexts, but Gemini-2.5-Pro stands out as an exception; and (3) Agentic RAG effectively addresses the retrieval bottleneck in Naïve RAG. Finally, we publicly release the SagaScale benchmark and our data collection codebase to facilitate future research.

</details>


### [4] [Syntactic Framing Fragility: An Audit of Robustness in LLM Ethical Decisions](https://arxiv.org/abs/2601.09724)
*Katherine Elkins,Jon Chun*

Main category: cs.CL

TL;DR: LLMs在逻辑等价但句法不同的提示下表现出显著的伦理判断不一致性，特别是对否定和条件结构的敏感性，开源模型比商业模型脆弱性高两倍以上。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地应用于重要决策场景，但其对良性提示变化的鲁棒性尚未得到充分探索。研究旨在评估LLMs在逻辑等价但句法不同的提示下是否能保持一致的伦理判断。

Method: 提出句法框架脆弱性(SFF)评估框架，通过逻辑极性归一化(LPN)隔离纯句法效应。对23个最先进模型（包括中美模型及美国开源模型）在14个伦理场景和4种控制框架下进行审计（共39,975个决策）。

Result: 发现广泛且统计显著的不一致性：许多模型仅因句法极性就反转伦理认可，开源模型的脆弱性是商业模型的两倍以上。发现极端否定敏感性，某些模型在"不应该"提示下80-97%情况下认可行动。思维链推理能显著降低脆弱性。

Conclusion: 句法一致性是伦理鲁棒性的一个独特且关键维度，SFF式审计应成为部署LLMs安全评估的标准组成部分。

Abstract: Large language models (LLMs) are increasingly deployed in consequential decision-making settings, yet their robustness to benign prompt variation remains underexplored. In this work, we study whether LLMs maintain consistent ethical judgments across logically equivalent but syntactically different prompts, focusing on variations involving negation and conditional structure. We introduce Syntactic Framing Fragility (SFF), a robustness evaluation framework that isolates purely syntactic effects via Logical Polarity Normalization (LPN), enabling direct comparison of decisions across positive and negative framings without semantic drift. Auditing 23 state-of-the-art models spanning the U.S. and China as well as small U.S. open-source software models over 14 ethical scenarios and four controlled framings (39,975 decisions), we find widespread and statistically significant inconsistency: many models reverse ethical endorsements solely due to syntactic polarity, with open-source models exhibiting over twice the fragility of commercial counterparts. We further uncover extreme negation sensitivity, where some models endorse actions in 80-97% of cases when explicitly prompted with "should not." We show that eliciting chain-of-thought reasoning substantially reduces fragility, identifying a practical mitigation lever, and we map fragility across scenarios, finding higher risk in financial and business contexts than in medical scenarios. Our results demonstrate that syntactic consistency constitutes a distinct and critical dimension of ethical robustness, and we argue that SFF-style audits should be a standard component of safety evaluation for deployed LLMs. Code and results will be available on github.com.

</details>


### [5] [Forgetting as a Feature: Cognitive Alignment of Large Language Models](https://arxiv.org/abs/2601.09726)
*Hien Tran,Quinten Steenhuis,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: 该论文重新诠释了LLMs中的遗忘现象，将其视为功能性认知机制而非缺陷，通过建模为指数衰减的记忆过程，提出概率记忆提示策略来改善长时推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统上LLMs的遗忘被视为推理缺陷，但作者认为这可能是类似人类记忆的功能性认知机制，需要重新理解LLMs的推理行为与人类认知模式的相似性。

Method: 将LLM推理建模为受指数衰减控制的概率记忆过程，引入评估时间推理、概念漂移适应和联想回忆的基准套件，并提出概率记忆提示策略来塑造证据整合。

Result: 实证结果显示LLMs表现出与人类记忆效率权衡相似的遗忘率，概率记忆提示策略能有效改善长时推理性能，使模型行为更接近人类认知模式。

Conclusion: 遗忘不应被视为故障模式，而是自适应智能的原则性机制，LLMs的推理行为与人类记忆动态存在深刻相似性。

Abstract: Large Language Models (LLMs) are often evaluated against ideals of perfect Bayesian inference, yet growing evidence suggests that their in-context reasoning exhibits systematic forgetting of past information. Rather than viewing this behavior as a limitation, we reinterpret forgetting as a functional cognitive mechanism. Drawing inspiration from human memory dynamics, we model LLM inference as a probabilistic memory process governed by exponential decay. We introduce a benchmark suite that evaluates temporal reasoning, concept drift adaptation, and associative recall, enabling direct comparison between model behavior and human cognitive patterns. Our empirical results reveal that LLMs demonstrate forgetting rates analogous to human memory efficiency trade-offs between stability and adaptability. Building on these observations, we propose probabilistic memory prompting, a lightweight strategy that shapes evidence integration to mimic human-like memory decay, leading to improved long-horizon reasoning performance. Our findings position forgetting not as a failure mode, but as a principled mechanism for adaptive intelligence.

</details>


### [6] [Eliminating Agentic Workflow for Introduction Generation with Parametric Stage Tokens](https://arxiv.org/abs/2601.09728)
*Meicong Zhang,Tiancheng su,Guoxiu He*

Main category: cs.CL

TL;DR: STIG方法通过将多阶段工作流的逻辑结构参数化到LLM中，实现单次推理生成完整研究引言，避免了传统智能体工作流的长推理链和错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于预定义智能体工作流的方法在生成研究引言时存在长推理链、错误累积和文本连贯性降低的问题，因为引言写作需要严谨逻辑、连贯结构和抽象概括能力。

Method: 提出STIG方法，将原始工作流的多个阶段转换为显式的阶段信号（阶段标记），通过指令微调让模型学习阶段标记与文本功能的映射关系，以及阶段间的逻辑顺序和转换模式。

Result: 实验结果表明，STIG能够在单次推理中生成多阶段文本，无需显式工作流调用，在语义相似度和句子级结构合理性指标上优于传统智能体工作流和其他基线方法。

Conclusion: 通过将工作流逻辑结构参数化到LLM中，STIG方法有效解决了传统智能体工作流在生成研究引言时的问题，实现了更高效、连贯的文本生成。

Abstract: In recent years, using predefined agentic workflows to guide large language models (LLMs) for literature classification and review has become a research focus. However, writing research introductions is more challenging. It requires rigorous logic, coherent structure, and abstract summarization. Existing workflows often suffer from long reasoning chains, error accumulation, and reduced textual coherence. To address these limitations, we propose eliminating external agentic workflows. Instead, we directly parameterize their logical structure into the LLM. This allows the generation of a complete introduction in a single inference. To this end, we introduce the Stage Token for Introduction Generation (STIG). STIG converts the multiple stages of the original workflow into explicit stage signals. These signals guide the model to follow different logical roles and functions during generation. Through instruction tuning, the model learns the mapping between stage tokens and text functions. It also learns the logical order and transition patterns between stages, encoding this knowledge into the model parameters. Experimental results show that STIG can generate multi-stage text in a single inference. It does not require explicit workflow calls. STIG outperforms traditional agentic workflows and other baselines on metrics of semantic similarity and sentence-level structural rationality. The code is provided in the Supplementary Materials.

</details>


### [7] [OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing](https://arxiv.org/abs/2601.09858)
*Yilin Bao,Ziyao He,Zayden Yang*

Main category: cs.CL

TL;DR: 提出基于强化学习的科学论文生成框架，通过结构化动作建模大纲演化，采用两阶段优化提升文档规划、引用一致性和事实准确性


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在科学论文生成中存在全局结构、输入覆盖和引用一致性方面的不足，需要文档级规划和事实基础

Method: 将科学大纲构建建模为分层文档结构上的长程规划问题，通过结构化动作建模大纲演化，采用两阶段优化：反向大纲重建确保结构一致性，前向价值引导强化学习建模科学正确性、话语连贯性和引用保真度

Result: 相比强大的神经和LLM基线方法，在长程结构连贯性和引用可靠性方面取得一致改进

Conclusion: 提出的强化学习框架能有效解决科学论文生成中的文档规划、输入利用和引用一致性问题，并建立了相应的评估基准

Abstract: Scientific paper generation requires document-level planning and factual grounding, but current large language models, despite their strong local fluency, often fail in global structure, input coverage, and citation consistency. We present a reinforcement learning framework that casts scientific outline construction as a long-horizon planning problem over hierarchical document structures. Our approach models edit evolving outlines through structured actions, enabling the system to incrementally build a complete scientific manuscript. To support effective and stabilize learning,we introduce a two-stage optimization procedure consisting of (i) backward outline reconstruction from partial plans to enforce global structural consistency, and (ii) forward value-guided reinforcement learning with rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity. In addition, We further introduce a benchmark for scientific paper generation that evaluates document planning, input utilization, reference faithfulness, outline organization, and content-level factual accuracy. Our results show consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.

</details>


### [8] [Long-Chain Reasoning Distillation via Adaptive Prefix Alignment](https://arxiv.org/abs/2601.10064)
*Zhenghao Liu,Zhuoyang Wu,Xinze Li,Yukun Yan,Shuo Wang,Zulong Chen,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: P-ALIGN是一种通过自适应前缀对齐来利用教师模型推理轨迹进行知识蒸馏的框架，通过截取简洁有效的推理前缀来提升学生模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 教师模型生成的推理轨迹通常过长且结构复杂，与学生模型的学习能力不匹配，导致监督信号与学生模型学习能力之间存在差距，需要更有效的蒸馏方法。

Method: 提出Prefix-ALIGNment蒸馏框架，自适应截断教师推理轨迹，判断剩余后缀是否简洁且足以指导学生模型，然后利用教师生成的前缀监督学生模型，实现有效的前缀对齐。

Result: 在多个数学推理基准测试中，P-ALIGN比所有基线方法性能提升超过3%，构建的前缀提供更有效的监督信号，同时避免了冗余和不确定推理组件的负面影响。

Conclusion: P-ALIGN通过自适应前缀对齐有效利用了教师模型的推理轨迹进行蒸馏，解决了教师轨迹过长复杂与学生模型学习能力不匹配的问题，显著提升了学生模型的推理性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, particularly in solving complex mathematical problems. Recent studies show that distilling long reasoning trajectories can effectively enhance the reasoning performance of small-scale student models. However, teacher-generated reasoning trajectories are often excessively long and structurally complex, making them difficult for student models to learn. This mismatch leads to a gap between the provided supervision signal and the learning capacity of the student model. To address this challenge, we propose Prefix-ALIGNment distillation (P-ALIGN), a framework that fully exploits teacher CoTs for distillation through adaptive prefix alignment. Specifically, P-ALIGN adaptively truncates teacher-generated reasoning trajectories by determining whether the remaining suffix is concise and sufficient to guide the student model. Then, P-ALIGN leverages the teacher-generated prefix to supervise the student model, encouraging effective prefix alignment. Experiments on multiple mathematical reasoning benchmarks demonstrate that P-ALIGN outperforms all baselines by over 3%. Further analysis indicates that the prefixes constructed by P-ALIGN provide more effective supervision signals, while avoiding the negative impact of redundant and uncertain reasoning components. All code is available at https://github.com/NEUIR/P-ALIGN.

</details>


### [9] [Deriving Character Logic from Storyline as Codified Decision Trees](https://arxiv.org/abs/2601.10080)
*Letian Peng,Kun Zhou,Longfei Yun,Yupeng Hou,Jingbo Shang*

Main category: cs.CL

TL;DR: CDT框架通过从大规模叙事数据中学习可执行、可解释的决策树来表示角色行为档案，显著提升了角色扮演代理的行为一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演代理的行为档案通常是非结构化、不可执行且验证不足的，导致代理行为脆弱不一致。需要一种能够从数据中学习、可执行且可解释的行为表示方法。

Method: 提出Codified Decision Trees（CDT）框架：1）将行为档案表示为条件规则树，内部节点对应已验证的场景条件，叶子节点编码具体行为；2）通过迭代学习：诱导候选场景-行为规则，验证其与数据的匹配度，通过层次化细化进行优化；3）支持确定性检索上下文相关规则。

Result: 在多个基准测试中，CDT在16个作品的85个角色上显著优于人工编写的行为档案和先前的档案诱导方法，表明编码化和验证的行为表示能带来更可靠的代理基础。

Conclusion: CDT框架通过数据驱动的可执行决策树表示行为档案，解决了现有角色扮演代理行为档案的非结构化、不可执行问题，实现了更透明、可检查和可更新的行为表示，显著提升了代理的可靠性和一致性。

Abstract: Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on $85$ characters across $16$ artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.

</details>


### [10] [ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback](https://arxiv.org/abs/2601.10156)
*Yutao Mou,Zhangchi Xue,Lijun Li,Peiyang Liu,Shikun Zhang,Wei Ye,Jing Shao*

Main category: cs.CL

TL;DR: 该论文提出了TS-Bench基准测试和TS-Guard防护模型，用于实时检测LLM代理工具调用的安全风险，并通过TS-Flow框架减少有害工具调用65%，提升良性任务完成率10%


<details>
  <summary>Details</summary>
Motivation: LLM代理通过外部工具与环境交互的能力增强，但也放大了安全风险。需要实时监控步骤级工具调用行为并在不安全执行前主动干预，这对代理部署至关重要但尚未充分研究

Method: 1) 构建TS-Bench基准测试用于步骤级工具调用安全检测；2) 开发TS-Guard防护模型，使用多任务强化学习，通过分析交互历史主动检测不安全工具调用；3) 提出TS-Flow防护反馈驱动的推理框架

Result: TS-Guard能够评估请求危害性和行动-攻击相关性，生成可解释的安全判断和反馈。TS-Flow框架将ReAct风格代理的有害工具调用平均减少65%，在提示注入攻击下将良性任务完成率提升约10%

Conclusion: 该研究为LLM代理工具调用安全提供了有效的监控和干预方案，通过主动检测和反馈机制显著提升了代理的安全性和任务完成率

Abstract: While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.

</details>


### [11] [HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning](https://arxiv.org/abs/2601.10187)
*Ziang Cui,Mengran Yu,Tianjiao Li,Chenyu Shi,Yingxuan Shi,Lusheng Zhang,Hongwei Lin*

Main category: cs.CL

TL;DR: 本文提出HOMURA强化学习框架，解决LLM多语言翻译中的跨语言冗长偏差问题，通过音节级时长约束优化翻译的语义保真度和时间可行性平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多语言翻译中表现出色，但存在系统性跨语言冗长偏差，不适合字幕、配音等严格时间约束任务。现有提示工程方法难以平衡语义保真度和时间可行性。

Method: 1. 引入Sand-Glass基准，专门评估音节级时长约束下的翻译质量；2. 提出HOMURA强化学习框架，采用KL正则化目标和新型动态音节比例奖励，明确优化语义保留和时间合规性之间的权衡。

Result: 实验结果表明，该方法显著优于强大的LLM基线，实现了精确的长度控制，在尊重语言密度层次的同时不损害语义充分性。

Conclusion: HOMURA框架有效解决了LLM翻译中的冗长偏差问题，为时间敏感型翻译任务提供了可行的解决方案，平衡了语义保真度和时间约束要求。

Abstract: Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively "tames" the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.

</details>


### [12] [Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs](https://arxiv.org/abs/2601.10257)
*Nan Li,Bo Kang,Tijl De Bie*

Main category: cs.CL

TL;DR: 研究提出了一种分解语言对LLM道德判断影响的方法论，区分困境语言和推理语言的作用，发现推理语言的影响是输入语言的两倍，并基于道德基础理论进行解释。


<details>
  <summary>Details</summary>
Motivation: 当LLM判断道德困境时，不同语言是否会导致不同结论？现有评估方法将困境语言和推理语言混为一谈，无法区分两者的独立影响，需要开发能够分解这两个因素的方法。

Method: 提出分离操纵困境语言和推理语言的方法论，包括匹配条件（如英语困境+英语推理）和不匹配条件（如英语困境+中文推理）。基于道德基础理论解释道德判断，并识别权威维度的细分。

Result: 在13个LLM的英中道德判断实验中：(1)推理语言的影响方差是输入语言的两倍；(2)框架检测到近半数模型存在标准评估遗漏的上下文依赖性；(3)诊断分类法将这些模式转化为部署指导。

Conclusion: 提出的方法论能够有效分解语言对LLM道德判断的影响，揭示了推理语言的关键作用，为跨文化道德评估提供了诊断工具和部署指导。

Abstract: When LLMs judge moral dilemmas, do they reach different conclusions in different languages, and if so, why? Two factors could drive such differences: the language of the dilemma itself, or the language in which the model reasons. Standard evaluation conflates these by testing only matched conditions (e.g., English dilemma with English reasoning). We introduce a methodology that separately manipulates each factor, covering also mismatched conditions (e.g., English dilemma with Chinese reasoning), enabling decomposition of their contributions. To study \emph{what} changes, we propose an approach to interpret the moral judgments in terms of Moral Foundations Theory. As a side result, we identify evidence for splitting the Authority dimension into a family-related and an institutional dimension. Applying this methodology to English-Chinese moral judgment with 13 LLMs, we demonstrate its diagnostic power: (1) the framework isolates reasoning-language effects as contributing twice the variance of input-language effects; (2) it detects context-dependency in nearly half of models that standard evaluation misses; and (3) a diagnostic taxonomy translates these patterns into deployment guidance. We release our code and datasets at https://anonymous.4open.science/r/CrossCulturalMoralJudgement.

</details>


### [13] [Boundary-Aware NL2SQL: Integrating Reliability through Hybrid Reward and Data Synthesis](https://arxiv.org/abs/2601.10318)
*Songsong Tian,Kongsheng Zhuo,Zhendong Wang,Rong Shen,Shengtao Zhang,Yong Wu*

Main category: cs.CL

TL;DR: BAR-SQL是一个统一的训练框架，通过边界感知和可靠性嵌入提升NL2SQL性能，使用种子突变数据合成和知识基础推理合成，采用两阶段训练和任务条件混合奖励机制，在自建基准上超越主流专有模型。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统在处理企业级复杂查询时面临可靠性不足的问题，特别是在边界情况（如模糊查询和模式限制）下容易产生错误结果。需要开发一个能够同时保证SQL生成准确性和边界感知能力的统一框架。

Method: 1. 种子突变数据合成范式构建企业级语料库，包含多步分析查询和边界情况；2. 知识基础推理合成生成基于模式元数据和业务规则的思维链轨迹；3. 两阶段训练：监督微调+基于组相对策略优化的强化学习；4. 任务条件混合奖励机制同时优化SQL执行准确性和语义精度。

Result: 在自建基准Ent-SQL-Bench上，BAR-SQL达到91.48%的平均准确率，在SQL生成质量和边界感知弃权能力方面均超越Claude 4.5 Sonnet和GPT-5等主流专有模型。

Conclusion: BAR-SQL通过将可靠性和边界感知直接嵌入生成过程，有效提升了NL2SQL系统在企业环境中的实用性，特别是在处理模糊和不可回答查询时的表现显著优于现有方法。

Abstract: In this paper, we present BAR-SQL (Boundary-Aware Reliable NL2SQL), a unified training framework that embeds reliability and boundary awareness directly into the generation process. We introduce a Seed Mutation data synthesis paradigm that constructs a representative enterprise corpus, explicitly encompassing multi-step analytical queries alongside boundary cases including ambiguity and schema limitations. To ensure interpretability, we employ Knowledge-Grounded Reasoning Synthesis, which produces Chain-of-Thought traces explicitly anchored in schema metadata and business rules. The model is trained through a two-stage process: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning via Group Relative Policy Optimization. We design a Task-Conditioned Hybrid Reward mechanism that simultaneously optimizes SQL execution accuracy-leveraging Abstract Syntax Tree analysis and dense result matching-and semantic precision in abstention responses. To evaluate reliability alongside generation accuracy, we construct and release Ent-SQL-Bench, which jointly assesse SQL precision and boundary-aware abstention across ambiguous and unanswerable queries. Experimental results on this benchmark demonstrate that BAR-SQL achieves 91.48% average accuracy, outperforming leading proprietary models, including Claude 4.5 Sonnet and GPT-5, in both SQL generation quality and boundary-aware abstention capability. The source code and benchmark are available anonymously at: https://github.com/TianSongS/BAR-SQL.

</details>


### [14] [OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding](https://arxiv.org/abs/2601.10343)
*Deming Ding,Shichun Liu,Enhui Yang,Jiahang Lin,Ziying Chen,Shihan Dou,Honglin Guo,Weiyu Cheng,Pengyu Zhao,Chengjun Xiao,Qunhong Zeng,Qi Zhang,Xuanjing Huang,Qidi Xu,Tao Gui*

Main category: cs.CL

TL;DR: OctoBench是一个用于评估代码智能体在仓库环境中遵循脚手架指令能力的基准测试，包含34个环境、217个任务和7,098个检查项，揭示了任务解决与脚手架遵循之间的系统性差距。


<details>
  <summary>Details</summary>
Motivation: 现代代码脚手架将LLM转变为强大的软件智能体，但它们在遵循脚手架指定指令方面的能力尚未得到充分检验，尤其是在约束条件异构且跨交互持续存在的情况下。

Method: 引入OctoBench基准测试，包含34个环境和217个任务，涵盖三种脚手架类型，配备7,098个客观检查项。提供自动化观察和评分工具包，捕获完整轨迹并进行细粒度检查，以区分任务解决和规则遵循。

Result: 对八个代表性模型的实验揭示了任务解决与脚手架感知合规性之间的系统性差距，强调了需要针对异构指令遵循进行专门训练和评估。

Conclusion: 需要开发更关注脚手架感知的代码智能体，OctoBench的发布旨在支持可重复的基准测试并加速这类智能体的发展。

Abstract: Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.

</details>


### [15] [Are Language Models Models?](https://arxiv.org/abs/2601.10421)
*Philip Resnik*

Main category: cs.CL

TL;DR: 论文批判性地评估了将语言模型作为认知模型系统的说法，认为这种主张在实现层面不成立，在算法表示层面动机不足，在计算理论层面存在问题，建议将语言模型视为工具而非认知模型。


<details>
  <summary>Details</summary>
Motivation: 本文旨在回应Futrell和Mahowald提出的"语言模型可作为模型系统"的主张，通过Marr的三个分析层次（计算理论、算法表示、实现）来系统评估这一说法的合理性，防止对语言模型能力的过度炒作。

Method: 采用David Marr提出的认知科学三层分析框架：1) 计算理论层面（计算目标），2) 算法表示层面（信息处理机制），3) 实现层面（物理实现）。作者在这三个层次上分别评估语言模型作为认知模型系统的有效性。

Result: 分析显示：在实现层面，语言模型明显不能作为认知模型；在算法表示层面，这种主张的动机不足；在计算理论层面，该主张存在问题。语言模型更适合作为工具使用，将其称为认知模型会夸大其能力并助长对大型语言模型的过度炒作。

Conclusion: 语言模型应被视为有用的工具，而非认知模型。将其称为认知模型不仅夸大了其实际能力，还会不必要地助长对大型语言模型的炒作。研究界应更谨慎地使用术语，避免过度解读语言模型的认知意义。

Abstract: Futrell and Mahowald claim LMs "serve as model systems", but an assessment at each of Marr's three levels suggests the claim is clearly not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level. LMs are good candidates as tools; calling them cognitive models overstates the case and unnecessarily feeds LLM hype.

</details>


### [16] [Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models](https://arxiv.org/abs/2601.10460)
*Abhinaba Basu,Pavan Chakraborty*

Main category: cs.CL

TL;DR: 论文提出Contextual StereoSet基准测试，通过固定刻板印象内容但系统改变上下文框架（时间、地点、受众等），发现模型偏见测量结果会随上下文变化而剧烈波动，挑战了传统固定条件偏见评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有偏见评估方法通常在固定上下文条件下进行，但模型在实验室基准测试中避免刻板印象，并不代表在真实部署中也能避免。需要评估模型偏见在不同上下文框架下的稳定性。

Method: 提出Contextual StereoSet基准测试，固定刻板印象内容但系统改变上下文框架（时间、地点、受众等）。引入Context Sensitivity Fingerprints（CSF）方法，通过每维度离散度和配对对比分析模型偏见敏感性。提供两种评估方案：360上下文诊断网格用于深度分析，预算协议覆盖4,229项用于生产筛选。

Result: 测试13个模型发现显著模式：锚定到1990年（vs. 2030年）在所有测试模型中提高刻板印象选择（p<0.05）；八卦框架在6个全网格模型中的5个提高偏见；外群体观察者框架使偏见变化高达13个百分点。这些效应在招聘、贷款和求助场景中可复现。

Conclusion: 固定条件测试的偏见分数可能无法泛化。这不是关于真实偏见率的声明，而是评估鲁棒性的压力测试。CSF迫使评估者问"在什么条件下会出现偏见？"而不是"这个模型有偏见吗？"

Abstract: A model that avoids stereotypes in a lab benchmark may not avoid them in deployment. We show that measured bias shifts dramatically when prompts mention different places, times, or audiences -- no adversarial prompting required.
  We introduce Contextual StereoSet, a benchmark that holds stereotype content fixed while systematically varying contextual framing. Testing 13 models across two protocols, we find striking patterns: anchoring to 1990 (vs. 2030) raises stereotype selection in all models tested on this contrast (p<0.05); gossip framing raises it in 5 of 6 full-grid models; out-group observer framing shifts it by up to 13 percentage points. These effects replicate in hiring, lending, and help-seeking vignettes.
  We propose Context Sensitivity Fingerprints (CSF): a compact profile of per-dimension dispersion and paired contrasts with bootstrap CIs and FDR correction. Two evaluation tracks support different use cases -- a 360-context diagnostic grid for deep analysis and a budgeted protocol covering 4,229 items for production screening.
  The implication is methodological: bias scores from fixed-condition tests may not generalize.This is not a claim about ground-truth bias rates; it is a stress test of evaluation robustness. CSF forces evaluators to ask, "Under what conditions does bias appear?" rather than "Is this model biased?" We release our benchmark, code, and results.

</details>


### [17] [PERM: Psychology-grounded Empathetic Reward Modeling for Large Language Models](https://arxiv.org/abs/2601.10532)
*Chengbing Wang,Wuqiang Zheng,Yang Zhang,Fengbin Zhu,Junyi Cheng,Yi Xie,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: 提出PERM方法，通过双向分解（支持者视角、寻求者视角、旁观者视角）评估共情，增强LLM的情感支持能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM在人类中心应用中缺乏实质性情感支持，现有RL奖励模型通常从单一视角评估共情，忽略了共情周期理论中支持者与寻求者之间的双向互动本质

Method: 提出心理学基础的共情奖励建模（PERM），通过双向分解操作共情评估：1）支持者视角（内部共鸣和沟通表达）；2）寻求者视角（情感接收）；3）旁观者视角（监控整体互动质量）

Result: 在广泛使用的情感智能基准和工业日常对话数据集上，PERM比最先进基线性能提升超过10%；盲测用户研究显示70%用户偏好该方法

Conclusion: PERM通过心理学基础的双向共情评估框架，显著提升了LLM生成共情回应的能力，为情感支持应用提供了有效解决方案

Abstract: Large Language Models (LLMs) are increasingly deployed in human-centric applications, yet they often fail to provide substantive emotional support. While Reinforcement Learning (RL) has been utilized to enhance empathy of LLMs, existing reward models typically evaluate empathy from a single perspective, overlooking the inherently bidirectional interaction nature of empathy between the supporter and seeker as defined by Empathy Cycle theory. To address this limitation, we propose Psychology-grounded Empathetic Reward Modeling (PERM). PERM operationalizes empathy evaluation through a bidirectional decomposition: 1) Supporter perspective, assessing internal resonation and communicative expression; 2) Seeker perspective, evaluating emotional reception. Additionally, it incorporates a bystander perspective to monitor overall interaction quality. Extensive experiments on a widely-used emotional intelligence benchmark and an industrial daily conversation dataset demonstrate that PERM outperforms state-of-the-art baselines by over 10\%. Furthermore, a blinded user study reveals a 70\% preference for our approach, highlighting its efficacy in generating more empathetic responses. Our code, dataset, and models are available at https://github.com/ZhengWwwq/PERM.

</details>


### [18] [Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs](https://arxiv.org/abs/2601.10645)
*Yuxi Xia,Loris Schoenegger,Benjamin Roth*

Main category: cs.CL

TL;DR: 本文提出TracVC方法，通过信息检索和影响估计追踪LLM表达自信的训练数据来源，发现LLM常模仿表面自信表达而非基于内容合理性，揭示了当前训练机制的根本局限性。


<details>
  <summary>Details</summary>
Motivation: LLM通过表达自信可以增加用户信任，但现有研究表明LLM经常过度自信，其表达的自信度与事实准确性不一致。为了理解这种自信表达的来源，需要追踪训练数据的影响。

Method: 提出TracVC方法，基于信息检索和影响估计，将生成的自信表达追溯到训练数据。在问答设置中评估OLMo和Llama模型，提出新指标"内容基础性"，衡量LLM自信表达基于内容相关训练示例的程度。

Result: 分析显示OLMo2-13B经常受到与查询词汇无关的自信相关数据影响，表明它可能模仿表面的确定性语言表达，而非依赖真正的内容基础。这揭示了当前训练机制的根本局限性。

Conclusion: LLM可能学会如何听起来自信，而没有学会何时自信是合理的。该分析为改进LLM表达更可靠自信的可信度提供了基础。

Abstract: Large language models (LLMs) can increase users' perceived trust by verbalizing confidence in their outputs. However, prior work has shown that LLMs are often overconfident, making their stated confidence unreliable since it does not consistently align with factual accuracy. To better understand the sources of this verbalized confidence, we introduce TracVC (\textbf{Trac}ing \textbf{V}erbalized \textbf{C}onfidence), a method that builds on information retrieval and influence estimation to trace generated confidence expressions back to the training data. We evaluate TracVC on OLMo and Llama models in a question answering setting, proposing a new metric, content groundness, which measures the extent to which an LLM grounds its confidence in content-related training examples (relevant to the question and answer) versus in generic examples of confidence verbalization. Our analysis reveals that OLMo2-13B is frequently influenced by confidence-related data that is lexically unrelated to the query, suggesting that it may mimic superficial linguistic expressions of certainty rather than rely on genuine content grounding. These findings point to a fundamental limitation in current training regimes: LLMs may learn how to sound confident without learning when confidence is justified. Our analysis provides a foundation for improving LLMs' trustworthiness in expressing more reliable confidence.

</details>


### [19] [LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals](https://arxiv.org/abs/2601.10700)
*Gilat Toker,Nitay Calderon,Ohad Amosy,Roi Reichart*

Main category: cs.CL

TL;DR: LIBERTy是一个基于LLM生成结构反事实对的框架，用于评估概念解释方法的忠实性，包含三个数据集和新评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有基于概念的模型解释方法评估依赖于昂贵且不完美的人工编写反事实数据，需要更系统、可扩展的基准来评估解释方法的忠实性。

Method: 提出LIBERTy框架：基于明确的结构因果模型，通过干预概念并让LLM生成反事实对，构建包含疾病检测、简历筛选和工作场所暴力预测的三个数据集，并引入order-faithfulness评估指标。

Result: 评估多种方法在五个模型上的表现，发现概念解释方法有显著改进空间；分析显示专有LLM对人口统计概念敏感性降低，可能是后训练缓解措施所致。

Conclusion: LIBERTy为开发忠实的概念解释方法提供了急需的基准，支持系统分析模型对干预的敏感性。

Abstract: Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.

</details>


### [20] [Grounding Agent Memory in Contextual Intent](https://arxiv.org/abs/2601.10702)
*Ruozhen Yang,Yucheng Jiang,Yueqi Jiang,Priyanka Kargupta,Yunyi Zhang,Jiawei Han*

Main category: cs.CL

TL;DR: STITCH是一个用于长时程目标导向交互的智能体记忆系统，通过结构化意图索引和匹配来减少检索噪声，在CAME-Bench和LongMemEval上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 在长时程目标导向交互中，相似实体和事实在不同潜在目标和约束下重复出现，导致传统记忆系统检索到上下文不匹配的证据，需要更有效的记忆检索机制。

Method: 提出STITCH系统，为每个轨迹步骤建立结构化检索线索（上下文意图），包括：当前潜在目标（定义主题段）、动作类型、关键实体类型。通过意图兼容性过滤和优先排序记忆片段，抑制语义相似但上下文不兼容的历史。

Result: 在CAME-Bench和LongMemEval基准测试中，STITCH达到最先进性能，比最强基线提升35.6%，轨迹越长提升越明显。意图索引显著减少检索噪声。

Conclusion: 结构化意图跟踪为长时程推理提供了鲁棒的意图感知记忆系统，能够有效处理重复实体和事实的歧义问题。

Abstract: Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step's intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.
  For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [21] [AI Agents That Actually Accelerate your Investigations](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sumologic.com%2Fblog%2Fwelcome-dojo-ai-agents-soc%3Futm_medium=email%26utm_source=TLDR%26utm_term=cloud-siem%26utm_id=701VK00000KhKeHYAV%26utm_campaign=20251204-global-awsmp-TLDR-primary/2/0100019bbcd606f9-972ee35d-63cd-4202-8bef-5a9cd6b125ca-000000/muljs64vPJG78U6uJ-BsZsMmZdQjH7B8HPw9D3b2GOM=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sumo Logic的Dojo AI通过专门化的AI代理（Mobot、Query Agent、Summary Agent）协同工作，加速安全调查流程，实现自然语言驱动的告警处理、查询生成和解释分析。


<details>
  <summary>Details</summary>
Motivation: 现有大多数"AI for security"工具未能真正加速安全调查工作流程，Dojo AI旨在通过专门化代理解决这一问题，提高安全团队的工作效率。

Method: 采用多个专门化AI代理协同工作：Mobot代理允许自然语言运行调查，Query Agent将自然语言转换为精确搜索查询，Summary Agent解释告警触发原因。

Result: Dojo AI能够加速安全调查流程，实现更快的根本原因分析，简化安全团队的工作流程，提高调查效率。

Conclusion: Dojo AI通过专门化AI代理的协同工作，为安全团队提供了真正加速调查流程的解决方案，区别于传统AI安全工具。

Abstract: AI Agents That Actually Accelerate your Investigations (Sponsor) Most "AI for security" tools don't actually make your workflows faster or easier. Sumo Logic's Dojo AI is a different beast, deploying specialized agents that work together to triage alerts, explain what triggered them, and write queries for you. >> Mobot lets you use natural language to run an investigation >> Query Agent turns natural language into precise searches for faster root cause analysis >> Summary Agent explains each ...

</details>


### [22] [AI code review with comments you'll actually](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrai%26utm_medium=email%26utm_campaign=codereview_260114secondary/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/lk02vB7O23FZpdra7XKkMX18w2CJqZvZ5Biyfj0yjqU=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Unblocked是一款AI代码审查工具，专注于识别真实问题和提供有意义的反馈，而非纠结于代码风格细节和低价值评论。


<details>
  <summary>Details</summary>
Motivation: 当前AI代码审查工具存在过度关注代码风格细节、产生大量低价值评论的问题，导致开发者产生"AI疲劳"。需要一种能够理解完整代码库上下文、提供真正有价值反馈的工具。

Method: 通过AI技术分析完整代码库上下文，识别真正重要的代码问题，过滤掉风格性挑剔和低价值评论，提供类似拥有完整代码库视角的资深开发者所能给出的反馈。

Result: 开发者反馈表明Unblocked改变了他们对AI疲劳的看法，能够提供只有了解完整代码库上下文的人才能给出的有价值反馈。

Conclusion: AI代码审查工具应该专注于提供有上下文意识的、高质量的反馈，而不是产生大量低价值的评论，这样才能真正帮助开发者并避免AI疲劳。

Abstract: AI code review with comments you'll actually (Sponsor) Unblocked is the AI code review that surfaces real issues and meaningful feedback instead of flooding your PRs with stylistic nitpicks and low-value comments. “Unblocked made me reconsider my AI fatigue. Finally, a tool that surfaces context only someone with a full view of the codebase could provide.” - Senior developer, Clio Try now for free

</details>


### [23] [Claude Agent SDK Technical Specification](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2FPOWERFULMOVES%2F58bcadab9483bf5e633e865f131e6c25%3Futm_source=tldrai/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/1JQXXB7D_TkTDVkkGaj8cdymW6hmppoTIp8aHZjVWMA=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Agent SDK技术规范，提供对Claude Code智能体能力的程序化访问


<details>
  <summary>Details</summary>
Motivation: 为开发者提供标准化、程序化的方式来访问和利用Claude Code的智能体能力，简化智能体应用的开发流程

Method: 提供SDK技术规范，包括API接口、调用方式、配置参数等程序化访问机制

Result: 建立了Claude Agent SDK的技术规范文档，为开发者提供完整的程序化访问方案

Conclusion: Claude Agent SDK技术规范为开发者提供了标准化的智能体能力访问接口，促进智能体应用的开发

Abstract: Claude Agent SDK Technical Specification (22 minute read) This page contains the technical specification for the Claude Agent SDK, which provides programmatic access to Claude Code's agentic capabilities.

</details>


### [24] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/an_4c3f7IwUS0oi0qMnufKKq0IStKZXQ_Sz8B3Y_PWk=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Agent SDK技术规范文档，提供对Claude Code智能体能力的程序化访问接口


<details>
  <summary>Details</summary>
Motivation: 为开发者提供标准化的程序化接口，以便在应用中集成Claude Code的智能体功能，降低开发门槛

Method: 提供SDK技术规范文档，定义API接口、数据结构、调用方式等技术细节

Result: 建立了完整的Claude Agent SDK技术规范，使开发者能够通过程序化方式调用Claude Code的智能体能力

Conclusion: 该技术规范为开发者提供了标准化接口，促进了Claude Code智能体能力的集成和应用开发

Abstract: Claude Agent SDK Technical Specification (22 minute read) This page contains the technical specification for the Claude Agent SDK, which provides programmatic access to Claude Code's agentic capabilities.

</details>


### [25] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/xAZVIqWTPLKM3nK-2FsODjBQNMhLyT74mYZF7H2FLKQ=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Agent SDK技术规范文档，提供对Claude Code智能体能力的程序化访问接口


<details>
  <summary>Details</summary>
Motivation: 为开发者提供标准化、程序化的方式来访问和利用Claude Code的智能体能力，促进基于Claude的应用程序开发

Method: 提供SDK技术规范文档，定义API接口、使用方法和集成指南，支持开发者通过程序化方式调用Claude智能体功能

Result: 创建了完整的Claude Agent SDK技术规范，为开发者提供了访问Claude Code智能体能力的标准化工具和文档

Conclusion: Claude Agent SDK技术规范为开发者提供了高效访问Claude智能体能力的程序化解决方案，促进了基于Claude的应用程序开发

Abstract: Claude Agent SDK Technical Specification (22 minute read) This page contains the technical specification for the Claude Agent SDK, which provides programmatic access to Claude Code's agentic capabilities.

</details>


### [26] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/qPGKuDuvpxUo60lelLIO2x9okOVikO7NI22m2xBZmTg=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Agent SDK技术规范文档，提供对Claude Code智能体能力的程序化访问接口


<details>
  <summary>Details</summary>
Motivation: 为开发者提供标准化的程序化接口，以便在应用中集成Claude Code的智能体能力，简化开发流程

Method: 提供技术规范文档，定义SDK的API接口、使用方法和集成方式，包含详细的编程接口说明

Result: 创建了完整的Claude Agent SDK技术规范，为开发者提供了22分钟阅读时长的详细技术文档

Conclusion: 该技术规范文档为开发者提供了集成Claude Code智能体能力的标准化解决方案

Abstract: Claude Agent SDK Technical Specification (22 minute read) This page contains the technical specification for the Claude Agent SDK, which provides programmatic access to Claude Code's agentic capabilities.

</details>


### [27] [Semantic Operators Meet Dataframes: Building Context for Agents with FENIC](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dataengineeringpodcast.com%2Ffenic-ai-dataframe-episode-496%3Futm_source=tldrdata/1/0100019bc157af4f-432b22d8-b94f-481f-99ea-f12ae4373a89-000000/3_d30IEAu-I85Ll5__fcp13tSSIswnMKp1UydfxdJHQ=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: FENIC是一个DataFrame风格引擎，将LLM调用视为内置操作符，可优化、节流并降低AI密集型管道的成本，将非结构化文本转为结构化数据供代理使用。


<details>
  <summary>Details</summary>
Motivation: 解决AI密集型管道中LLM调用成本高、效率低的问题，同时将非结构化文本数据转化为可靠、可复现的结构化数据，为代理提供安全的治理工具。

Method: 开发DataFrame风格引擎，将LLM调用视为内置操作符，实现优化、节流和成本控制，使用LLM将混乱文本转化为结构化数据。

Result: 构建了可靠、可复现、可作为治理工具使用的数据处理管道，使代理能够安全调用，降低了AI密集型管道的成本和提高了效率。

Conclusion: FENIC通过将LLM调用集成到DataFrame引擎中，有效解决了AI管道中的成本、效率和数据质量问题，为代理系统提供了更好的上下文构建能力。

Abstract: Semantic Operators Meet Dataframes: Building Context for Agents with FENIC (56 minute podcast) FENIC is a DataFrame-style engine that treats LLM calls like built-in operators, so it can optimize, throttle, and reduce cost for AI-heavy pipelines. It uses LLMs to turn messy text into structured data, making downstream processing reliable, reproducible, and usable as governed tools that agents can safely call.

</details>


### [28] [I tested 14 analytics agents - so you don't have to](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewaiorder.substack.com%2Fp%2Fi-tested-14-analytics-agents-so-you%3Futm_source=tldrdata/1/0100019bc157af4f-432b22d8-b94f-481f-99ea-f12ae4373a89-000000/_zI791ZiSv8CcmrpQT-Z1quBCk0Lauiu7y4kvElNAAc=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 对14个分析型AI代理进行基准测试，评估可靠性、用户体验、速度和成本，发现AI原生的BI工具提供最佳整体体验，但成本更高且需要迁移投入


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在数据分析领域的应用日益增多，需要系统评估不同工具的实用性和性能，为企业和开发者提供选择参考

Method: 使用真实生产数据对14个代理化分析工具进行基准测试，评估四个关键维度：可靠性、用户体验、速度和成本

Result: AI原生的商业智能(BI)工具在整体体验上表现最佳，但存在成本较高和需要迁移投入的缺点

Conclusion: 当前阶段AI原生BI工具提供了最佳的综合体验，但用户需要在更好的体验与更高的成本及迁移投入之间做出权衡

Abstract: I tested 14 analytics agents - so you don't have to (15 minute read) A 2026 benchmark of 14 agentic analytics tools tested reliability, UX, speed, and cost on real production data. The conclusion is that AI-native BI tools currently offer the best overall experience, but at the expense of higher cost and migration effort.

</details>


### [29] [Gas Town Decoded](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alilleybrinker.com%2Fmini%2Fgas-town-decoded%2F%3Futm_source=tldrnewsletter/1/0100019bc167680c-0afb69b8-dc64-4131-868c-e0dbceebfb1b-000000/xPPFMs-j6RTr0IhgoCZWJdQD7uTPrF0IPB-SKGvzcsI=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gas Town是一个AI代理编排工具，本文解释了其术语定义，帮助开发者理解该工具


<details>
  <summary>Details</summary>
Motivation: 为开发者提供Gas Town工具中使用的术语解释，降低学习门槛，促进工具采用

Method: 通过术语列表和解码的方式，系统性地解释Gas Town中的关键概念和定义

Result: 提供了Gas Town工具中使用的术语定义，使开发者能够更好地理解和使用该AI代理编排工具

Conclusion: 清晰的术语解释对于AI代理编排工具的开发者采用至关重要，Gas Town的术语解码有助于提升开发体验

Abstract: Gas Town Decoded (3 minute read) This post presents a list of terms used in Gas Town, an AI agent orchestration tool, decoding the definitions used in the introductory paper for developers.

</details>


### [30] [Getting Real Leverage from Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Festsauver.com%2Fblog%2Fclaude-code-workflow%3Futm_source=tldrnewsletter/1/0100019bc167680c-0afb69b8-dc64-4131-868c-e0dbceebfb1b-000000/Ds7mZ-vMBHqnShZGy2t7DO0oby0FxlKFZ6-XBz_P60A=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 介绍如何并行运行多个Claude Code会话，每个会话都在完全隔离的环境中运行，以提高开发效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 当前AI代码助手通常以串行方式运行，无法充分利用计算资源。通过并行运行多个隔离的Claude Code会话，可以显著提高开发效率，同时保持环境隔离以避免冲突。

Method: 采用容器化技术为每个Claude Code会话创建完全隔离的运行环境，通过并行处理机制同时运行多个会话，实现资源的高效利用。

Result: 通过并行运行多个隔离的Claude Code会话，可以显著提高代码生成和处理的吞吐量，同时确保各个会话之间的环境隔离和安全。

Conclusion: 并行运行多个隔离的Claude Code会话是提高AI代码助手效率的有效方法，为大规模代码生成和处理提供了实用的解决方案。

Abstract: Getting Real Leverage from Claude Code (18 minute read) This post discusses how to run multiple Claude Code sessions in parallel, each in its own fully isolated environment.

</details>


### [31] [AI Enables As-Needed Software Features](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2139%26utm_source=tldrnewsletter/1/0100019bc167680c-0afb69b8-dc64-4131-868c-e0dbceebfb1b-000000/8iAlzXGxeqMFOMS0nYIBERhAQQuKgYZ6hwF3DVJq_YM=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI允许用户通过自然语言描述需求，应用程序即时生成所需代码来实现功能


<details>
  <summary>Details</summary>
Motivation: 传统软件开发需要预先编写所有功能代码，而AI可以实现按需生成代码，提高开发效率和灵活性

Method: 使用AI技术，让用户通过自然语言描述需求，应用程序自动生成相应的代码来实现功能

Result: 实现了按需软件功能生成，用户只需描述需求即可获得相应功能

Conclusion: AI技术能够实现按需软件功能生成，改变传统软件开发模式

Abstract: AI Enables As-Needed Software Features (1 minute read) AI enables people to just describe what they want, and then apps write the code needed to do it on demand.

</details>


### [32] [Cowork: Claude Code for the rest of your work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclaude.com%2Fblog%2Fcowork-research-preview%3Futm_source=tldrmarketing/1/0100019bc18c36b9-099c5d09-58c1-4c02-a873-af6f2626b087-000000/bCUCJQXS15Cgf87JLS1nVoRNOAERlr6zGW4nqL8CthQ=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Cowork是一个研究预览功能，让非开发者也能通过Claude处理文件夹中的文件，包括读取、编辑、创建文档、电子表格和报告，同时支持连接外部工具和在线任务处理。


<details>
  <summary>Details</summary>
Motivation: 让非技术用户也能利用AI助手处理日常工作中的文件管理、文档创建和任务规划等需求，降低AI工具的使用门槛。

Method: 开发了一个文件夹级别的AI助手系统，允许用户指定可访问的文件夹和连接器，Claude可以并行处理多种文件类型，并通过浏览器访问处理在线任务。

Result: 为Claude Max订阅者提供了macOS上的研究预览功能，实现了文件处理、任务规划和外部工具集成的能力。

Conclusion: Claude Cowork扩展了AI助手的能力边界，使其能够处理更广泛的工作场景，特别是为非开发者提供了实用的文件管理助手。

Abstract: Cowork: Claude Code for the rest of your work (3 minute read) Claude Cowork is a research preview for Claude Max subscribers on macOS that lets anyone (not just developers) work with files in a chosen folder. Claude can read, edit, and create documents, spreadsheets, or reports, plan tasks, and work in parallel while keeping users informed. It can connect to external tools and handle tasks online with browser access. Users control which folders and connectors Claude can access, and it asks be...

</details>


### [33] [Scaling long-running autonomous coding](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fscaling-agents%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/JgBME-1FcIMg1ww07eSVttxHi3uG6PFoSPZ5_jNW6MU=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor团队开发了"规划者与工作者"模型，解决了多智能体协调中的瓶颈和风险规避问题，使数百个智能体能够并发运行数周，生成了超过百万行代码。


<details>
  <summary>Details</summary>
Motivation: 单个智能体难以处理复杂项目，而早期的动态多智能体协调方法因瓶颈和风险规避行为而失败，需要一种更有效的自主编码协调系统。

Method: 采用"规划者与工作者"模型：规划者创建任务，工作者独立执行任务，避免了协调瓶颈，实现了大规模并发运行。

Result: 系统成功让数百个智能体并发运行数周，生成了超过一百万行代码，解决了复杂项目的自主编码挑战。

Conclusion: "规划者与工作者"模型是解决大规模自主编码协调问题的有效方法，能够支持长期、复杂的软件开发项目。

Abstract: Scaling long-running autonomous coding (6 minute read) Single agents struggle with complex projects, and initial attempts at dynamic multi-agent coordination fail due to bottlenecks and risk-averse behavior. The Cursor team developed a successful "Planners and Workers" model, where Planners create tasks and Workers execute them independently, solving coordination issues. This system allowed hundreds of agents to run concurrently for weeks, generating over a million lines of code for ambitious...

</details>


### [34] [Discarding the Shaft-and-Belt Model of Software Development](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsecondthoughts.ai%2Fp%2Fthe-new-model-of-software-development%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/aODfzXjXJjOUka-NCKXhVPeELxaW7IiAplRCUZAOvoQ=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI编程代理将改变软件开发模式，从臃肿的大型项目转向定制化、手工化的软件，通过降低小项目成本实现个性化应用


<details>
  <summary>Details</summary>
Motivation: 传统软件开发采用"轴带模型"（大型集中式项目），导致软件臃肿、成本高，而AI编码代理的出现为改变这一模式提供了机会

Method: 利用AI编码代理（如Claude Code）实现"氛围编码"，将小型项目的开发成本趋近于零，使工程师能够为内部工具构建个性化应用

Result: AI编码代理将使软件开发从大型项目转向定制化软件，特别有利于为50%构建内部工具的工程师创建个性化应用程序

Conclusion: AI编码代理将彻底改变软件开发范式，摒弃传统的"轴带模型"，实现软件开发的民主化和个性化

Abstract: Discarding the Shaft-and-Belt Model of Software Development (7 minute read) AI coding agents like Claude Code will shift software from bloated mega-projects to bespoke, artisanal software. By driving small-project costs toward zero, vibe coding allows for personalized applications for the 50 percent of engineers building internal tools.

</details>


### [35] [Tool Search now in Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F398Dio/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/JKWST8otUHTrpaxSJygqqb8ZaqKCCNQZcgGJALcLQCY=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code引入Tool Search功能，通过动态搜索加载工具来减少MCP协议中工具描述对上下文的占用


<details>
  <summary>Details</summary>
Motivation: 随着MCP工具数量的增长，大量工具描述会消耗大量上下文空间，需要更高效的工具管理方案

Method: 在Claude Code中引入Tool Search功能，通过动态搜索按需加载工具，而不是一次性加载所有工具描述

Result: 实现了更高效的工具管理，减少了上下文占用，提高了工具使用的灵活性和效率

Conclusion: Tool Search功能解决了MCP协议中工具数量增长带来的上下文管理问题，提升了开发体验

Abstract: Tool Search now in Claude Code (2 minute read) Claude Code is introducing a new "Tool Search" feature for its MCP protocol to address the challenge of managing a growing number of tools. Previously, numerous MCP tool descriptions could consume a lot of context. This new feature dynamically loads tools via search only when needed.

</details>


### [36] [Agent Skills](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fvercel-labs%2Fagent-skills%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/K8nbFnkPEvsPLzYdVmdSP8J05LXhwmmRgbSREan4lSM=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Skills是Vercel为AI编码代理提供的预打包指令和脚本集合，包含如react-best-practices等技能，提供40多条优化React和Next.js性能的规则


<details>
  <summary>Details</summary>
Motivation: 为AI编码代理提供标准化的最佳实践指导，帮助开发者更高效地使用AI代理进行代码优化，特别是在React和Next.js生态系统中

Method: 通过GitHub仓库形式提供预打包的技能集合，每个技能包含特定领域的优化规则和脚本，如react-best-practices技能包含40多条性能优化规则

Result: 创建了一个可扩展的技能库，使AI编码代理能够遵循行业最佳实践，特别是在React和Next.js的性能优化方面

Conclusion: Agent Skills为AI编码代理提供了标准化的最佳实践指导，有助于提高代码质量和开发效率

Abstract: Agent Skills (GitHub Repo) Agent Skills is Vercel's collection of packaged instructions and scripts for AI coding agents. For example, the `react-best-practices` skill provides over 40 rules for optimizing React and Next.js performance across various categories like bundle size and data fetching.

</details>


### [37] [Getting Real Leverage from Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Festsauver.com%2Fblog%2Fclaude-code-workflow%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/TTximtc-BFetaLKHPNVqhKT700ZfXpqjm6voxHtHCZA=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用本地Kubernetes集群(k3d)替代传统Git worktrees，为Claude Code提供真正的命名空间隔离，解决端口冲突和数据库冲突等共享资源问题


<details>
  <summary>Details</summary>
Motivation: 传统Git worktrees在使用Claude Code时存在共享资源冲突问题，如端口争用和数据库冲突，需要更好的隔离解决方案

Method: 采用本地Kubernetes集群(k3d)为每个worktree提供真正的命名空间隔离，利用Claude辅助编写必要的YAML配置

Result: 实现了真正的资源隔离，避免了共享资源冲突，提高了开发环境的稳定性和并行开发能力

Conclusion: 本地Kubernetes集群是解决Claude Code开发环境中资源隔离问题的有效方案，优于传统Git worktrees方法

Abstract: Getting Real Leverage from Claude Code (18 minute read) Traditional Git worktrees alone when using Claude Code are not enough as they lead to shared resource conflicts like port contention and database clashes. Instead, it's better to use a local Kubernetes cluster (k3d) to provide true namespace isolation for each worktree, with Claude assisting in writing the necessary, often tedious, YAML configurations.

</details>


### [38] [January 29](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FpyFNOu/3/0100019bc1fc2fc2-d2c23ecd-8167-4907-b92d-9546d4f86989-000000/rxllp9ATZcHjTU9erEvVJPVXdHdUm-sVBmagyRsOfwg=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文探讨Agentic AI的核心能力，为AI经济时代做准备


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，Agentic AI成为AI经济的关键组成部分，需要深入理解其核心能力以应对未来挑战

Method: 通过专家讨论和探索的方式分析Agentic AI的核心能力

Result: 识别了Agentic AI的关键能力，为参与AI经济做好准备

Conclusion: Agentic AI的核心能力对于AI经济至关重要，需要持续探索和准备

Abstract: Join experts from to explore the core competencies of Agentic AI. Are you ready for the AI economy?

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents](https://arxiv.org/abs/2601.09770)
*Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu*

Main category: cs.AI

TL;DR: GUI-Eyes是一个强化学习框架，用于GUI任务中的主动视觉感知，通过两阶段推理学习何时、如何调用视觉工具，在ScreenSpot-Pro基准上仅用3k样本达到44.8%的定位准确率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI自动化方法依赖静态、一次性视觉输入和被动感知，缺乏自适应决定何时、是否以及如何观察界面的能力。

Method: 提出渐进感知策略，将决策分解为粗粒度探索和细粒度定位，由两级策略协调；设计空间连续奖励函数，结合位置接近度和区域重叠度，为工具使用提供密集监督。

Result: 在ScreenSpot-Pro基准上，GUI-Eyes-3B仅使用3k标记样本就实现了44.8%的定位准确率，显著优于监督学习和基于RL的基线方法。

Conclusion: 工具感知的主动感知，通过分阶段策略推理和细粒度奖励反馈，对于构建鲁棒且数据高效的GUI代理至关重要。

Abstract: Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.

</details>


### [40] [PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation](https://arxiv.org/abs/2601.09771)
*Aradhya Dixit,Shreem Dixit*

Main category: cs.AI

TL;DR: PCN-Rec是一个证明携带的协商推荐系统，通过分离自然语言推理和确定性约束执行来解决LLM推荐系统难以可靠满足治理约束的问题。


<details>
  <summary>Details</summary>
Motivation: 现代基于LLM的推荐系统虽然能生成有吸引力的排名列表，但难以可靠地满足治理约束（如最小长尾曝光或多样性要求）。需要一种能同时保证相关性和约束满足的方法。

Method: 采用证明携带的协商管道：基础推荐器生成候选窗口，两个代理（用户倡导者和策略代理）进行协商，调解LLM合成top-N列表和结构化证书，确定性验证器检查约束满足，失败时使用确定性约束贪婪修复生成合规列表。

Result: 在MovieLens-100K数据集上，PCN-Rec在可行用户中达到98.55%的通过率，相比没有验证/修复的单LLM基线显著提升，同时NDCG@10仅下降0.021（0.403 vs 0.424），差异具有统计显著性。

Conclusion: PCN-Rec通过分离推理和约束执行，实现了高约束满足率同时保持推荐质量，为可审计的约束推荐系统提供了有效框架。

Abstract: Modern LLM-based recommenders can generate compelling ranked lists, but they struggle to reliably satisfy governance constraints such as minimum long-tail exposure or diversity requirements. We present PCN-Rec, a proof-carrying negotiation pipeline that separates natural-language reasoning from deterministic enforcement. A base recommender (MF/CF) produces a candidate window of size W, which is negotiated by two agents: a User Advocate optimizing relevance and a Policy Agent enforcing constraints. A mediator LLM synthesizes a top-N slate together with a structured certificate (JSON) describing the claimed constraint satisfaction. A deterministic verifier recomputes all constraints from the slate and accepts only verifier-checked certificates; if verification fails, a deterministic constrained-greedy repair produces a compliant slate for re-verification, yielding an auditable trace. On MovieLens-100K with governance constraints, PCN-Rec achieves a 98.55% pass rate on feasible users (n = 551, W = 80) versus a one-shot single-LLM baseline without verification/repair, while preserving utility with only a 0.021 absolute drop in NDCG@10 (0.403 vs. 0.424); differences are statistically significant (p < 0.05).

</details>


### [41] [A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Model-Based Conversational Agents](https://arxiv.org/abs/2601.09869)
*Andrea Ferrario,Rasita Vinay,Matteo Casserini,Alessandro Facchini*

Main category: cs.AI

TL;DR: 本文是一篇关于大型语言模型对话代理拟人化现象的伦理导向范围综述，系统梳理了该领域的定义、伦理挑战与机遇、方法论，并提出了研究议程和设计建议。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的对话代理日益普及，其拟人化现象（赋予非人类实体类人特质）引发了重要伦理关注。现有文献分散在不同领域，定义、操作化和伦理评估存在显著差异，需要系统梳理以指导伦理部署。

Method: 采用范围综述方法，系统检索了五个数据库和三个预印本库中的相关文献，从三个维度进行综合：(1)概念基础，(2)伦理挑战与机遇，(3)方法论途径。

Result: 研究发现：在定义上存在基于归因的共识，但在操作化上差异显著；伦理框架主要关注风险而非机遇；实证研究有限，难以将交互效应转化为可操作的治理指导。

Conclusion: 提出研究议程和设计/治理建议，强调需要更平衡的伦理框架、更好的操作化方法，以及连接实证发现与治理实践的桥梁，以实现LLM对话代理拟人化线索的伦理部署。

Abstract: Anthropomorphisation -- the phenomenon whereby non-human entities are ascribed human-like qualities -- has become increasingly salient with the rise of large language model (LLM)-based conversational agents (CAs). Unlike earlier chatbots, LLM-based CAs routinely generate interactional and linguistic cues, such as first-person self-reference, epistemic and affective expressions that empirical work shows can increase engagement. On the other hand, anthropomorphisation raises ethical concerns, including deception, overreliance, and exploitative relationship framing, while some authors argue that anthropomorphic interaction may support autonomy, well-being, and inclusion. Despite increasing interest in the phenomenon, literature remains fragmented across domains and varies substantially in how it defines, operationalizes, and normatively evaluates anthropomorphisation. This scoping review maps ethically oriented work on anthropomorphising LLM-based CAs across five databases and three preprint repositories. We synthesize (1) conceptual foundations, (2) ethical challenges and opportunities, and (3) methodological approaches. We find convergence on attribution-based definitions but substantial divergence in operationalization, a predominantly risk-forward normative framing, and limited empirical work that links observed interaction effects to actionable governance guidance. We conclude with a research agenda and design/governance recommendations for ethically deploying anthropomorphic cues in LLM-based conversational agents.

</details>


### [42] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: 提出基于信息流编排的多智能体范式CORAL，通过智能体间自然语言通信动态协调任务，无需预定义工作流，在GAIA基准上超越基于工作流的方法OWL 8.49个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统依赖预定义工作流，需要人工枚举任务状态并指定路由规则，存在两个根本限制：需要大量手动工作来预测和编码可能状态，且无法穷尽复杂现实任务的状态空间。

Method: 提出信息流编排的多智能体范式CORAL，通过专门的编排器持续监控任务进度，使用A2A工具包通过自然语言动态协调其他智能体，不依赖预定义工作流。

Result: 在GAIA通用基准上，以OWL为基线，pass@1设置下达到63.64%准确率，比OWL的55.15%高出8.49个百分点，且token消耗相当。案例级分析显示该方法能更灵活监控任务并更鲁棒处理边缘情况。

Conclusion: 信息流编排的多智能体范式能有效克服基于规则工作流的限制，通过动态协调实现更灵活的任务监控和更鲁棒的边缘情况处理，在复杂任务中表现更优。

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [43] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: 该论文提出了"连续记忆架构"(CMA)作为RAG的替代方案，通过持久存储、选择性保留、关联路由、时间链和抽象整合来解决RAG在记忆处理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法将记忆视为静态查找表，存在信息永久保留、检索只读、缺乏时间连续性等问题，无法满足长期智能体对记忆积累、更新和消歧的需求。

Method: 定义了CMA架构类别，强调通过持久存储、选择性保留、关联路由、时间链和整合为高阶抽象来维护和更新跨交互的内部状态，而非具体实现细节。

Result: 在知识更新、时间关联、关联回忆、上下文消歧等任务上展示了CMA相比RAG的行为优势，证明CMA是长期智能体的必要架构原语。

Conclusion: CMA解决了RAG在记忆处理上的结构缺陷，但面临延迟、漂移和可解释性等开放挑战，是长期智能体发展的关键架构方向。

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [44] [CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents](https://arxiv.org/abs/2601.09923)
*Hanna Foerster,Robert Mullins,Tom Blanchard,Nicolas Papernot,Kristina Nikolić,Florian Tramèr,Ilia Shumailov,Cheng Zhang,Yiren Zhao*

Main category: cs.AI

TL;DR: 该论文提出了一种针对计算机使用代理（CUA）的单次规划方法，通过可信规划器在执行前生成完整的执行图，提供可证明的控制流完整性保证，以防御提示注入攻击，同时保持实用性能。


<details>
  <summary>Details</summary>
Motivation: AI代理容易受到提示注入攻击，恶意内容可能劫持代理行为导致凭证窃取或财务损失。当前唯一已知的鲁棒防御是架构隔离，但将其应用于计算机使用代理（CUA）存在根本性挑战：CUA需要持续观察UI状态来确定每个动作，这与安全所需的隔离要求相冲突。

Method: 引入单次规划方法，在观察任何潜在恶意内容之前，由可信规划器生成包含条件分支的完整执行图。这种方法提供可证明的控制流完整性保证，防止任意指令注入攻击。同时识别并解决分支导向攻击问题。

Result: 在OSWorld基准上评估，该方法在保持前沿模型57%性能的同时，将较小开源模型的性能提升高达19%。证明了严格的安全性和实用性可以在CUA中共存。

Conclusion: 通过单次规划方法解决了CUA中安全隔离与持续观察需求之间的根本矛盾，提供了可证明的控制流完整性保证，同时识别了需要额外防御措施的分支导向攻击问题。

Abstract: AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.

</details>


### [45] [Structured Personality Control and Adaptation for LLM Agents](https://arxiv.org/abs/2601.10025)
*Jinpeng Wang,Xinyu Jia,Wei Wei Heng,Yuquan Li,Binbin Shi,Qianlei Chen,Guannan Chen,Junxia Zhang,Yuyu Yin*

Main category: cs.AI

TL;DR: 该论文提出了一个基于荣格心理类型的LLM人格建模框架，通过主导-辅助协调、强化-补偿和反思三种机制，实现既保持细腻特质又能动态适应交互需求的人格表达。


<details>
  <summary>Details</summary>
Motivation: LLMs在人机交互中日益重要，但现有方法难以实现既细腻又可适应的人格表达。人格特质对用户参与度、决策和真实感感知至关重要，需要更系统的人格建模方法。

Method: 基于荣格心理类型理论，设计三机制框架：1)主导-辅助协调机制确保核心人格一致性；2)强化-补偿机制实现短期情境适应；3)反思机制驱动长期人格演化。使用MBTI问卷进行人格对齐评估，并在多样化挑战场景中测试。

Result: 研究发现，具有演化能力的人格感知LLMs能够支持连贯且情境敏感的人机交互，为人机交互中的自然主义智能体设计提供了可能。

Conclusion: 该框架实现了LLM人格的细腻表达与动态适应的平衡，为人机交互中自然、连贯的智能体设计提供了理论基础和技术方案。

Abstract: Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.

</details>


### [46] [PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization](https://arxiv.org/abs/2601.10029)
*Tingyue Pan,Jie Ouyang,Mingyue Cheng,Qingchuan Li,Zirui Liu,Mingfan Pan,Shuo Yu,Qi Liu*

Main category: cs.AI

TL;DR: 提出PaperScout自主代理将论文搜索重构为顺序决策过程，并引入PSPO方法解决多轮代理任务中的粒度不匹配问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有学术论文搜索方法依赖僵化的预定义工作流，难以处理复杂的条件查询。需要更灵活的自适应代理框架来动态决定搜索策略。

Method: 提出PaperScout自主代理，将论文搜索重构为顺序决策过程；引入Proximal Sequence Policy Optimization (PSPO)方法，这是一种过程感知的序列级策略优化方法，解决多轮代理任务中的粒度不匹配问题。

Result: 在合成和真实世界基准测试中，PaperScout在召回率和相关性方面显著优于强工作流驱动和强化学习基线方法。

Conclusion: 自适应代理框架和优化策略有效，PSPO方法解决了多轮代理任务中的信用分配问题，为学术搜索提供了更灵活的解决方案。

Abstract: Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.

</details>


### [47] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 基于OpenRouter平台分析100万亿token真实LLM使用数据，发现开源模型广泛采用、创意角色扮演和编程助手类应用流行、智能体推理兴起，以及早期用户留存率显著更高的"玻璃鞋效应"。


<details>
  <summary>Details</summary>
Motivation: 随着o1等推理模型的发布，LLM从单次模式生成转向多步深思推理，但我们对这些模型在实际使用中的情况了解滞后。需要实证研究真实世界的LLM使用模式。

Method: 利用OpenRouter平台（AI推理提供商）分析超过100万亿token的真实LLM交互数据，涵盖不同任务、地域和时间维度。

Result: 观察到开源模型被广泛采用；创意角色扮演（不仅仅是生产力任务）和编程助手类别异常流行；智能体推理兴起；早期用户留存率远高于后期用户，称为"玻璃鞋效应"。

Conclusion: 开发者和终端用户对LLM的实际使用是复杂多样的。这些发现对模型构建者、AI开发者和基础设施提供商有重要启示，数据驱动的使用理解可以指导更好的LLM系统设计和部署。

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [48] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: LLMs在预测用户重复行为时间间隔方面表现有限，虽然优于简单统计基线但不如专用机器学习模型，且过多上下文信息反而会降低预测性能。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs从结构化行为数据中推断时间规律的能力，特别是预测重复用户行为（如重复购买）之间的时间间隔，以及不同上下文信息如何影响其预测表现。

Method: 使用代表性的重复购买场景，在零样本设置下对最先进的LLMs进行基准测试，与统计模型和机器学习模型进行比较，分析不同层次上下文信息的影响。

Result: 1. LLMs超越轻量级统计基线但始终不如专用机器学习模型，显示其捕捉定量时间结构的能力有限；2. 适度上下文可提高准确性，但添加更多用户级细节会降低性能。

Conclusion: 当前LLMs在结构化时间推理方面存在根本限制，挑战了"更多上下文导致更好推理"的假设，为设计未来结合统计精度和语言灵活性的上下文感知混合模型提供指导。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [49] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: 本文提出DecisionLLM，将LLMs应用于离线决策任务，通过将轨迹数据作为独立模态与自然语言任务描述对齐，解决了LLMs无法理解连续数值的问题，在迷宫和竞价任务中显著优于传统决策变换器。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在长序列决策中存在挑战，而决策变换器将RL构建为自回归序列建模问题。LLMs在复杂推理任务中表现出色，但无法理解连续数值。本文探索LLMs在长视野序列决策中的应用潜力。

Method: 提出DecisionLLM框架，将轨迹数据作为独立模态，学习轨迹数据与自然语言任务描述的对齐，使模型能够在统一框架内自回归预测未来决策。建立了该范式的缩放定律，考虑模型规模、数据量和数据质量三个因素。

Result: DecisionLLM-3B在离线实验基准和竞价场景中表现优异，在Maze2D umaze-v1上比传统决策变换器提升69.4分，在AuctionNet上提升0.085分，扩展了AIGB范式。

Conclusion: LLMs在长视野序列决策任务中具有巨大潜力，DecisionLLM框架有效解决了LLMs理解连续数值的挑战，为在线竞价等应用指明了有前景的研究方向。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [50] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM提出在数学推理任务中进行步骤级路由，仅将关键步骤分配给大模型，让小模型处理常规步骤，显著提升推理效率


<details>
  <summary>Details</summary>
Motivation: 多步推理任务容易发生级联失败，当前LLM路由方法将整个查询分配给单一模型，无法区分不同步骤的重要性

Method: 使用过程奖励模型识别错误步骤，基于步骤级不确定性和预算约束进行路由决策，开发从简单阈值策略到考虑长期精度-成本权衡的复杂策略

Result: 在MATH-500上，简单阈值策略比先前路由方法成本效率高5倍，高级策略仅用20%昂贵模型token即可匹配强模型性能；在AIME上达到6倍成本效率提升

Conclusion: 步骤级难度代表了推理的基本特征，针对性步骤级干预能从根本上改变推理效率，将昂贵调用限制在能防止级联错误的关键步骤

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [51] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: EAPO提出了一种证据增强的策略优化方法，通过密集的过程监督来改进长上下文推理中的证据提取质量，解决了传统强化学习中结果奖励稀疏的问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在长上下文推理中存在结果奖励稀疏的问题，无法有效惩罚无根据的"幸运猜测"，导致关键的证据检索过程缺乏监督。

Method: 首先建立证据增强推理范式，通过树结构证据采样验证证据提取是长上下文推理的关键瓶颈。然后提出EAPO算法，使用奖励模型计算组相对证据奖励，提供密集的过程监督。进一步引入自适应奖励-策略协同进化机制，迭代优化奖励模型。

Result: 在八个基准测试上的综合评估表明，EAPO相比最先进的基线方法显著提升了长上下文推理性能。

Conclusion: EAPO通过密集的过程监督有效解决了长上下文推理中的证据提取问题，为强化学习在复杂推理任务中的应用提供了新思路。

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [52] [Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering](https://arxiv.org/abs/2601.10402)
*Xinyu Zhu,Yuzhu Cai,Zexi Liu,Bingyang Zheng,Cheng Wang,Rui Ye,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Yuzhi Zhang,Linfeng Zhang,Weinan E,Di Jin,Siheng Chen*

Main category: cs.AI

TL;DR: ML-Master 2.0通过分层认知缓存架构解决AI在超长周期自主科学探索中的瓶颈，在机器学习工程基准测试中达到56.44%的奖牌率


<details>
  <summary>Details</summary>
Motivation: 当前AI向代理科学发展的主要瓶颈是超长周期自主性问题，即如何在跨越数天或数周的实验周期中保持战略连贯性和迭代修正能力。虽然大语言模型在短期推理方面表现出色，但在现实世界研究的高维、延迟反馈环境中容易被执行细节淹没，无法将稀疏反馈整合为连贯的长期指导。

Method: 提出分层认知缓存（HCC）架构，将上下文管理重构为认知积累过程。这种受计算机系统启发的多层架构能够实现经验随时间推移的结构化区分，通过动态将瞬态执行轨迹提炼为稳定知识和跨任务智慧，使代理能够将即时执行与长期实验策略解耦，有效克服静态上下文窗口的扩展限制。

Result: 在OpenAI的MLE-Bench基准测试中，使用24小时预算，ML-Master 2.0实现了56.44%的最先进奖牌率，展示了其在超长周期机器学习工程任务中的卓越性能。

Conclusion: 超长周期自主性为AI提供了可扩展的蓝图，使其能够超越人类先例的复杂性进行自主探索。分层认知缓存架构为解决AI在长期科学发现任务中的瓶颈提供了有效方案。

Abstract: The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.

</details>


### [53] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: LLMdoctor：一种基于患者-医生范式的测试时对齐框架，通过细粒度token级奖励获取和流引导偏好优化，在保持生成多样性的同时高效对齐大语言模型。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法计算成本高且不灵活，现有测试时对齐方法依赖扭曲的轨迹级信号或低效采样，性能受限且难以保持基础模型的生成多样性。

Method: 采用患者-医生范式，从患者LLM的行为变化中提取细粒度token级偏好信号，通过token级流引导偏好优化（TFPO）训练较小的医生模型来引导冻结的患者模型，确保所有子轨迹的流一致性。

Result: LLMdoctor显著优于现有测试时对齐方法，甚至超越了DPO等完整微调方法的性能。

Conclusion: LLMdoctor提供了一种高效、精确的测试时对齐框架，能够在保持生成多样性的同时实现token级对齐，为LLM对齐提供了新的解决方案。

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [54] [Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment](https://arxiv.org/abs/2601.10520)
*Felix Jahn,Yannic Muskalla,Lisa Dargasz,Patrick Schramowski,Kevin Baum*

Main category: cs.AI

TL;DR: GRACE是一个神经符号推理的基于原因的约束架构，通过将规范性推理与工具性决策解耦，包含任何设计的AI代理，确保其决策既有效又符合规范。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在重要场景中自主部署并产生实际影响，确保其决策不仅工具有效而且规范对齐变得至关重要。需要一种能够约束任何设计AI代理的架构。

Method: 提出GRACE架构，将决策分为三个模块：道德模块（使用道义逻辑推理确定允许的宏观行动）、决策模块（封装目标代理选择工具最优的原始行动）、守卫模块（监控并强制执行道德合规）。道德模块采用基于原因的形式化方法，提供可解释性、可争议性和可辩护性。

Result: 在LLM治疗助手示例中展示了GRACE如何使利益相关者理解、质疑和优化代理行为。该架构支持形式验证和统计保证。

Conclusion: GRACE通过神经符号推理的基于原因约束架构，实现了AI代理的规范性对齐，提供了可解释性、形式验证和实际部署的可行性。

Abstract: As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [The PROPER Approach to Proactivity: Benchmarking and Advancing Knowledge Gap Navigation](https://arxiv.org/abs/2601.09926)
*Kirandeep Kaur,Vinayak Gupta,Aditya Gupta,Chirag Shah*

Main category: cs.LG

TL;DR: ProPer是一个两智能体架构，通过生成隐式维度来主动满足用户未表达的需求，相比传统反应式助手显著提升了响应质量和主动性。


<details>
  <summary>Details</summary>
Motivation: 现有语言助手主要采用反应式的问答模式，需要用户明确表达需求，导致相关但未表达的需求无法得到满足。现有主动代理要么需要用户进一步澄清（增加负担），要么从上下文推断未来需求（可能导致不必要或时机不当的干预）。

Method: 提出ProPer（Proactivity-driven Personalized agents）两智能体架构：1) 维度生成智能体（DGA）：微调的LLM智能体，利用显式用户数据生成多个隐式维度（与用户任务相关但用户未考虑的潜在方面）或知识缺口；2) 使用基于质量、多样性和任务相关性的重排序器筛选维度；3) 响应生成智能体（RGA）：平衡显式和隐式维度，生成具有及时主动干预的个性化响应。

Result: 在多领域评估中，ProPer在质量分数和胜率方面均有提升，在单轮评估中实现了高达84%的增益，在多轮交互中持续占优。使用结构化、缺口感知的评估标准（覆盖度、主动性适当性、意图对齐）进行测量。

Conclusion: ProPer通过两智能体架构有效解决了传统反应式助手的局限性，能够主动识别和满足用户未表达的需求，在多个领域显著提升了助手的主动性和个性化能力。

Abstract: Most language-based assistants follow a reactive ask-and-respond paradigm, requiring users to explicitly state their needs. As a result, relevant but unexpressed needs often go unmet. Existing proactive agents attempt to address this gap either by eliciting further clarification, preserving this burden, or by extrapolating future needs from context, often leading to unnecessary or mistimed interventions. We introduce ProPer, Proactivity-driven Personalized agents, a novel two-agent architecture consisting of a Dimension Generating Agent (DGA) and a Response Generating Agent (RGA). DGA, a fine-tuned LLM agent, leverages explicit user data to generate multiple implicit dimensions (latent aspects relevant to the user's task but not considered by the user) or knowledge gaps. These dimensions are selectively filtered using a reranker based on quality, diversity, and task relevance. RGA then balances explicit and implicit dimensions to tailor personalized responses with timely and proactive interventions. We evaluate ProPer across multiple domains using a structured, gap-aware rubric that measures coverage, initiative appropriateness, and intent alignment. Our results show that ProPer improves quality scores and win rates across all domains, achieving up to 84% gains in single-turn evaluation and consistent dominance in multi-turn interactions.

</details>


### [56] [Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts](https://arxiv.org/abs/2601.10079)
*Sijia Luo,Xiaokang Zhang,Yuxuan Hu,Bohan Zhang,Ke Wang,Jinbo Su,Mengshu Sun,Lei Liang,Jing Zhang*

Main category: cs.LG

TL;DR: Sparse-RL：一种在稀疏rollouts下实现稳定强化学习训练的方法，通过稀疏感知拒绝采样和重要性重加权来纠正压缩引起的策略不匹配问题，显著降低KV缓存内存开销同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在激发大语言模型复杂推理能力方面至关重要，但在长序列rollouts过程中存储KV缓存的内存开销巨大，成为硬件受限时的关键瓶颈。现有KV压缩技术主要针对推理场景，直接应用于RL训练会导致严重的策略不匹配和性能崩溃。

Method: Sparse-RL通过稀疏感知拒绝采样和重要性重加权来解决策略不匹配问题。该方法识别了密集旧策略、稀疏采样器策略和学习器策略之间的不匹配，并设计机制纠正压缩引起的信息损失带来的离策略偏差。

Result: 实验结果表明，Sparse-RL相比密集基线显著降低了rollout开销，同时保持了性能。此外，该方法实现了稀疏感知训练，显著增强了模型在稀疏推理部署时的鲁棒性。

Conclusion: Sparse-RL成功解决了RL训练中KV压缩导致的策略不匹配问题，为在有限硬件资源下进行高效RL训练提供了可行方案，同时增强了模型在稀疏推理环境中的鲁棒性。

Abstract: Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.

</details>


### [57] [Reinforcement Learning to Discover a NorthEast Monsoon Index for Monthly Rainfall Prediction in Thailand](https://arxiv.org/abs/2601.10181)
*Kiattikun Chobtham*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习优化的东北季风气候指数，用于改进泰国特定地区的长期月降雨量预测。


<details>
  <summary>Details</summary>
Motivation: 全球气候指数（如厄尔尼诺南方涛动）是长期降雨预测的标准输入特征，但在泰国特定区域缺乏能够提高预测精度的局地尺度指数。

Method: 1. 提出新的东北季风气候指数，基于海表温度反映北半球冬季季风气候学；2. 使用深度Q网络强化学习代理探索和选择与季节性降雨相关性最高的矩形区域来优化指数计算区域；3. 将降雨站分为12个聚类以区分泰国南部和上部的降雨模式；4. 将优化后的指数整合到长短期记忆模型中进行预测。

Result: 将优化后的指数整合到LSTM模型中显著提高了大多数聚类区域的长期月降雨预测技能，有效降低了12个月提前预测的均方根误差。

Conclusion: 通过强化学习优化的局地气候指数能够有效改进特定区域的长期降雨预测精度，为区域气候预测提供了新方法。

Abstract: Climate prediction is a challenge due to the intricate spatiotemporal patterns within Earth systems. Global climate indices, such as the El Niño Southern Oscillation, are standard input features for long-term rainfall prediction. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel NorthEast monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.

</details>


### [58] [PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary](https://arxiv.org/abs/2601.10201)
*Jiarui Yao,Ruida Wang,Tong Zhang*

Main category: cs.LG

TL;DR: 本文提出过程奖励学习（PRL），通过将熵正则化强化学习目标分解为中间步骤，为LLM推理过程提供细粒度监督，无需额外步骤如MCTS或训练奖励模型。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多基于轨迹级别的结果奖励，缺乏推理过程中的细粒度监督。其他结合过程信号的训练框架依赖繁琐的额外步骤（如MCTS、训练单独奖励模型），且过程信号设计缺乏严格理论支持。

Method: 提出过程奖励学习（PRL），从理论动机出发，将熵正则化强化学习目标分解为中间步骤，推导出等价于奖励最大化加策略模型与参考模型间KL散度惩罚项的PRL公式，将结果奖励转化为过程监督信号。

Result: PRL不仅提高了LLM推理能力的平均性能（average @ n），还通过改进pass @ n指标拓宽了推理边界。大量实验验证了PRL的有效性和泛化能力。

Conclusion: PRL为LLM推理优化提供了一种理论严谨、训练高效的过程监督方法，能够有效提升推理性能并拓宽推理能力边界。

Abstract: Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.

</details>


### [59] [Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers](https://arxiv.org/abs/2601.10274)
*Emre Ozbas,Melih Bastopcu*

Main category: cs.LG

TL;DR: 本文研究LLM服务器中任务类型的token分配优化问题，通过平衡准确率与延迟，在队列稳定性约束下最大化加权准确率目标。


<details>
  <summary>Details</summary>
Motivation: LLM服务器需要处理多种任务类型的查询流，不同任务需要不同的计算资源（thinking tokens）。准确率与延迟之间存在权衡：更多token提高准确率但增加服务时间。需要优化token分配以在队列稳定性约束下最大化系统性能。

Method: 将系统建模为M/G/1队列，服务时间与分配的token数近似线性相关。建立约束优化问题，最大化加权平均准确率（惩罚平均系统时间），受限于token预算和队列稳定性条件。证明目标函数在稳定区域内严格凹，确保最优解存在唯一。通过一阶最优条件得到耦合投影定点特性，开发迭代解法和投影梯度法保证收敛。

Result: 理论分析表明目标函数严格凹，最优token分配存在唯一。提出迭代解法、收缩条件和投影梯度法保证收敛。通过连续解舍入得到整数token分配，仿真评估性能损失。

Conclusion: 本文为LLM服务器中异构任务类型的token分配提供了系统优化框架，平衡准确率与延迟，在队列稳定性约束下实现最优性能。

Abstract: We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.

</details>


### [60] [SuS: Strategy-aware Surprise for Intrinsic Exploration](https://arxiv.org/abs/2601.10349)
*Mark Kashirskiy,Ilya Makarov*

Main category: cs.LG

TL;DR: 提出Strategy-aware Surprise (SuS)框架，通过策略稳定性与策略惊喜度作为内在动机，提升强化学习中的探索效率，在数学推理任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统好奇心驱动方法仅依赖状态预测误差，无法有效捕捉行为策略层面的新颖性。需要一种能够同时衡量策略一致性和策略相关意外结果的探索机制。

Method: 提出SuS框架，包含两个互补组件：策略稳定性(SS)衡量时间步间行为策略的一致性；策略惊喜度(SuS)捕捉相对于当前策略表示的意外结果。通过学习的权重系数结合两种信号形成奖励函数。

Result: 在大型语言模型的数学推理任务上，SuS相比基线方法在Pass@1上提升17.4%，在Pass@5上提升26.4%，同时保持更高的策略多样性。消融研究表明移除任一组件会导致至少10%的性能下降。

Conclusion: SuS框架通过策略层面的新颖性信号有效提升探索效率，两个组件的协同作用对性能提升至关重要，为强化学习中的内在动机设计提供了新思路。

Abstract: We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent's current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.

</details>


### [61] [CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning](https://arxiv.org/abs/2601.10407)
*Yuanjie Zhao,Junnan Qiu,Yue Ding,Jie Li*

Main category: cs.LG

TL;DR: 提出CS-GBA攻击框架，针对安全约束的离线强化学习算法，通过关键样本选择、相关性破坏触发器和梯度引导动作生成，在5%污染预算下实现高隐蔽性和破坏性攻击。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击策略难以对抗安全约束算法（如CQL），因为随机污染效率低且使用易检测的分布外触发器。需要设计更隐蔽有效的攻击方法。

Method: 1) 自适应关键样本选择：基于TD误差识别对价值函数收敛关键的状态转移；2) 相关性破坏触发器：利用状态特征的物理互斥性（如95%分位数边界）保持统计隐蔽性；3) 梯度引导动作生成：使用受害者Q网络的梯度在数据流形中搜索最差动作。

Result: 在D4RL基准测试中，CS-GBA显著优于现有基线方法，在仅5%污染预算下对代表性安全约束算法实现高攻击成功率，同时在干净环境中保持智能体性能。

Conclusion: CS-GBA框架通过关键样本选择、隐蔽触发器设计和梯度引导动作生成，实现了对安全约束离线强化学习算法的高效后门攻击，揭示了现有安全机制的脆弱性。

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to backdoor attacks. Existing attack strategies typically struggle against safety-constrained algorithms (e.g., CQL) due to inefficient random poisoning and the use of easily detectable Out-of-Distribution (OOD) triggers. In this paper, we propose CS-GBA (Critical Sample-based Gradient-guided Backdoor Attack), a novel framework designed to achieve high stealthiness and destructiveness under a strict budget. Leveraging the theoretical insight that samples with high Temporal Difference (TD) errors are pivotal for value function convergence, we introduce an adaptive Critical Sample Selection strategy that concentrates the attack budget on the most influential transitions. To evade OOD detection, we propose a Correlation-Breaking Trigger mechanism that exploits the physical mutual exclusivity of state features (e.g., 95th percentile boundaries) to remain statistically concealed. Furthermore, we replace the conventional label inversion with a Gradient-Guided Action Generation mechanism, which searches for worst-case actions within the data manifold using the victim Q-network's gradient. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms state-of-the-art baselines, achieving high attack success rates against representative safety-constrained algorithms with a minimal 5% poisoning budget, while maintaining the agent's performance in clean environments.

</details>


### [62] [Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching](https://arxiv.org/abs/2601.10418)
*Nadav Merlis*

Main category: cs.LG

TL;DR: 该论文研究具有多步前瞻信息的表格强化学习问题，提出自适应批处理策略（ABPs）来解决现有启发式方法的不足，并设计了乐观后悔最小化算法来学习最优ABP。


<details>
  <summary>Details</summary>
Motivation: 现有处理多步前瞻信息的方法（固定批处理策略和模型预测控制）存在明显问题，需要更有效的策略来充分利用前瞻信息提升学习性能。

Method: 提出自适应批处理策略（ABPs），根据状态自适应地划分前瞻信息批次；推导最优Bellman方程；设计乐观后悔最小化算法来学习未知环境中的最优ABP。

Result: 获得了阶最优的后悔界（最多相差前瞻视野ℓ的因子），通常ℓ可视为小常数，表明算法在理论上是有效的。

Conclusion: 自适应批处理策略能够更有效地利用前瞻信息，提出的算法能够在未知环境中学习最优ABP，为多步前瞻强化学习提供了新的解决方案。

Abstract: We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes ('fixed batching policies'), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\ell$, which can usually be considered a small constant.

</details>


### [63] [DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline Policy Extraction](https://arxiv.org/abs/2601.10471)
*Zhancun Mu*

Main category: cs.LG

TL;DR: DeFlow是一个解耦的离线强化学习框架，利用流匹配技术捕捉复杂行为流形，通过轻量级精炼模块在数据驱动的信任区域内学习，避免了ODE求解器反向传播的计算负担，同时保持迭代生成能力。


<details>
  <summary>Details</summary>
Motivation: 传统生成策略优化计算成本高昂，通常需要通过ODE求解器进行反向传播。现有方法要么计算负担重，要么通过单步蒸馏牺牲迭代生成能力，需要在保持流模型表达能力的同时实现高效优化。

Method: 提出解耦的离线RL框架，使用流匹配捕捉行为流形。在流流形的显式、数据驱动的信任区域内学习轻量级精炼模块，而不是通过单步蒸馏牺牲迭代生成能力。这种方法避免了求解器微分，无需平衡损失项。

Result: 在具有挑战性的OGBench基准测试中取得优越性能，并展示了高效的离线到在线适应能力。

Conclusion: DeFlow通过解耦设计和流匹配技术，在保持流模型迭代表达能力的同时实现了稳定改进，为离线RL提供了一种高效且表达能力强的解决方案。

Abstract: We present DeFlow, a decoupled offline RL framework that leverages flow matching to faithfully capture complex behavior manifolds. Optimizing generative policies is computationally prohibitive, typically necessitating backpropagation through ODE solvers. We address this by learning a lightweight refinement module within an explicit, data-derived trust region of the flow manifold, rather than sacrificing the iterative generation capability via single-step distillation. This way, we bypass solver differentiation and eliminate the need for balancing loss terms, ensuring stable improvement while fully preserving the flow's iterative expressivity. Empirically, DeFlow achieves superior performance on the challenging OGBench benchmark and demonstrates efficient offline-to-online adaptation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [64] [Enhancing Formal Software Specification with Artificial Intelligence](https://arxiv.org/abs/2601.09745)
*Antonio Abu Nassar,Eitan Farchi*

Main category: cs.SE

TL;DR: 使用自然语言增强轻量级数学符号作为中间规范语言，通过AI审查和精炼后生成代码，在组织知识增长模拟案例中显著降低开发成本并实现首次尝试即正确的实现


<details>
  <summary>Details</summary>
Motivation: 传统形式化规范虽然能实现早期错误检测和显式不变量，但由于符号开销大、需要专业知识而在工业界采用有限。需要找到既能保留形式化规范优点又能显著降低成本的解决方案。

Method: 使用自然语言增强轻量级数学符号（用LaTeX编写）作为中间规范语言，通过AI进行审查和精炼，然后生成代码。区分系统分析师需要控制的部分（受益于形式化规范）和不需要控制的部分。

Result: 应用于非平凡的组织知识增长模拟，该方法实现了早期验证、显式不变量和设计即正确，显著减少开发工作量，并在首次尝试中产生正确的实现。

Conclusion: AI的进步使得在保留形式化规范许多优点的同时大幅降低成本成为可能，自然语言增强轻量级数学符号作为中间规范语言是有效的解决方案。

Abstract: Formal software specification is known to enable early error detection and explicit invariants, yet it has seen limited industrial adoption due to its high notation overhead and the expertise required to use traditional formal languages. This paper presents a case study showing that recent advances in artificial intelligence make it possible to retain many of the benefits of formal specification while substantially reducing these costs. The necessity of a clear distinction between what is controlled by the system analyst and can highly benefits from the rigor of formal specification and what need not be controlled is demonstrated. We use natural language augmented with lightweight mathematical notation and written in \LaTeX\ as an intermediate specification language, which is reviewed and refined by AI prior to code generation. Applied to a nontrivial simulation of organizational knowledge growth, this approach enables early validation, explicit invariants, and correctness by design, while significantly reducing development effort and producing a correct implementation on the first attempt.

</details>


### [65] [SAGE: Tool-Augmented LLM Task Solving Strategies in Scalable Multi-Agent Environments](https://arxiv.org/abs/2601.09750)
*Robert K. Strehlow,Tobias Küster,Oskar F. Kupke,Brandon Llanque Kurps,Fikret Sivrikaya,Sahin Albayrak*

Main category: cs.SE

TL;DR: SAGE是一个基于OPACA框架的对话AI系统，专注于工具发现与执行，支持动态集成新工具和多种提示策略，通过开源实现提供可扩展的模块化架构。


<details>
  <summary>Details</summary>
Motivation: 现实应用中LLM需要访问实时信息和执行工具，但软件环境快速变化，需要动态集成新工具。现有方法难以可靠理解和使用自定义工具，需要零样本提示方法和动态工具集成策略。

Method: 基于OPACA框架构建SAGE对话AI接口，支持动态添加工具和服务。系统具有模块化架构，可切换不同模型（GPT、LLAMA等），集成多种提示方法和代理策略，实现工具选择、执行和结果评估。

Result: 实现了多种任务解决策略，使用不同复杂度的代理概念和提示方法，在综合基准服务集上进行了评估。结果显示了不同策略的优缺点，整体表现良好。

Conclusion: SAGE和OPACA框架提供了动态工具集成和多样化提示策略的有效解决方案，通过开源实现支持可扩展的模块化AI系统开发。

Abstract: Large language models (LLMs) have proven to work well in question-answering scenarios, but real-world applications often require access to tools for live information or actuation. For this, LLMs can be extended with tools, which are often defined in advance, also allowing for some fine-tuning for specific use cases. However, rapidly evolving software landscapes and individual services require the constant development and integration of new tools. Domain- or company-specific tools can greatly elevate the usefulness of an LLM, but such custom tools can be problematic to integrate, or the LLM may fail to reliably understand and use them. For this, we need strategies to define new tools and integrate them into the LLM dynamically, as well as robust and scalable zero-shot prompting methods that can make use of those tools in an efficient manner. In this paper, we present SAGE, a specialized conversational AI interface, based on the OPACA framework for tool discovery and execution. The integration with OPACA makes it easy to add new tools or services for the LLM to use, while SAGE itself presents rich extensibility and modularity. This not only provides the ability to seamlessly switch between different models (e.g. GPT, LLAMA), but also to add and select prompting methods, involving various setups of differently prompted agents for selecting and executing tools and evaluating the results. We implemented a number of task-solving strategies, making use of agentic concepts and prompting methods in various degrees of complexity, and evaluated those against a comprehensive set of benchmark services. The results are promising and highlight the distinct strengths and weaknesses of different task-solving strategies. Both SAGE and the OPACA framework, as well as the different benchmark services and results, are available as Open Source/Open Data on GitHub.

</details>


### [66] [LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities](https://arxiv.org/abs/2601.09822)
*Yongjian Tang,Thomas Runkler*

Main category: cs.SE

TL;DR: 本文系统综述了基于LLM的多智能体系统在软件工程全生命周期中的应用，包括需求工程、代码生成、静态检查、测试和调试等，探讨了模型选择、评估基准、框架协议等关键话题，并指出了多智能体编排、人机协作、计算成本优化等未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型取得了进展，但复杂的软件工程任务需要更协作和专门化的方法。本文旨在系统回顾新兴的LLM多智能体系统范式，为研究者和从业者提供该领域的前沿洞察。

Method: 采用概念性系统综述方法，全面考察LLM多智能体系统在软件开发生命周期各阶段的应用，分析语言模型选择、评估基准、智能体框架和通信协议等关键技术要素。

Result: 识别了LLM多智能体系统在软件工程领域的广泛应用潜力，总结了当前的技术框架和通信协议，并明确了该领域面临的主要挑战。

Conclusion: LLM多智能体系统为复杂软件工程任务提供了有前景的解决方案，但需要在多智能体编排、人机协作、计算成本优化和数据收集等方面进行深入研究，以推动该领域的发展。

Abstract: Despite recent advancements in Large Language Models (LLMs), complex Software Engineering (SE) tasks require more collaborative and specialized approaches. This concept paper systematically reviews the emerging paradigm of LLM-based multi-agent systems, examining their applications across the Software Development Life Cycle (SDLC), from requirements engineering and code generation to static code checking, testing, and debugging. We delve into a wide range of topics such as language model selection, SE evaluation benchmarks, state-of-the-art agentic frameworks and communication protocols. Furthermore, we identify key challenges and outline future research opportunities, with a focus on multi-agent orchestration, human-agent coordination, computational cost optimization, and effective data collection. This work aims to provide researchers and practitioners with valuable insights into the current forefront landscape of agentic systems within the software engineering domain.

</details>


### [67] [Beyond Strict Rules: Assessing the Effectiveness of Large Language Models for Code Smell Detection](https://arxiv.org/abs/2601.09873)
*Saymon Souza,Amanda Santana,Eduardo Figueiredo,Igor Muzetti,João Eduardo Montandon,Lionel Briand*

Main category: cs.SE

TL;DR: 评估四种LLM在检测九种代码异味中的表现，并提出结合LLM与静态分析工具的混合策略，该策略在多数异味检测中表现更优但会产生更多误报。


<details>
  <summary>Details</summary>
Motivation: 代码异味会影响软件可维护性和可靠性，增加开发成本。虽然LLM在软件工程中表现出色，但在代码异味检测方面尚未充分探索。与静态分析工具的刚性规则不同，LLM可以提供灵活、可定制的检测策略。

Method: 1) 评估四种LLM（DeepSeek-R1、GPT-5 mini、Llama-3.3、Qwen2.5-Code）在30个Java项目中检测九种代码异味的效果；2) 创建包含268个代码异味候选的真实数据集，由76名开发者手动检查；3) 提出并评估结合LLM与静态分析工具的混合检测策略。

Result: 1) LLM在结构简单的异味（如Large Class、Long Method）上表现良好；2) 不同LLM和工具在不同代码异味检测中各有所长；3) 混合策略在九种异味中的五种上F1分数优于单独使用LLM或工具；4) 混合策略对复杂异味会产生更多误报。

Conclusion: 最优的代码异味检测策略取决于检测目标是优先召回率还是精确度。混合策略在多数情况下表现更好，但需要权衡误报率。

Abstract: Code smells are symptoms of potential code quality problems that may affect software maintainability, thus increasing development costs and impacting software reliability. Large language models (LLMs) have shown remarkable capabilities for supporting various software engineering activities, but their use for detecting code smells remains underexplored. However, unlike the rigid rules of static analysis tools, LLMs can support flexible and adaptable detection strategies tailored to the unique properties of code smells. This paper evaluates the effectiveness of four LLMs -- DeepSeek-R1, GPT-5 mini, Llama-3.3, and Qwen2.5-Code -- for detecting nine code smells across 30 Java projects. For the empirical evaluation, we created a ground-truth dataset by asking 76 developers to manually inspect 268 code-smell candidates. Our results indicate that LLMs perform strongly for structurally straightforward smells, such as Large Class and Long Method. However, we also observed that different LLMs and tools fare better for distinct code smells. We then propose and evaluate a detection strategy that combines LLMs and static analysis tools. The proposed strategy outperforms LLMs and tools in five out of nine code smells in terms of F1-Score. However, it also generates more false positives for complex smells. Therefore, we conclude that the optimal strategy depends on whether Recall or Precision is the main priority for code smell detection.

</details>


### [68] [Mark My Works Autograder for Programming Courses](https://arxiv.org/abs/2601.10093)
*Yiding Qiu,Seyed Mahdi Azimi,Artem Lensky*

Main category: cs.SE

TL;DR: 开发了Mark My Works本地自动评分系统，结合传统单元测试和LLM生成解释，为编程课程提供及时详细反馈。在191名学生课程中测试，AI评分与人工评分无线性相关但分布相似，AI评分更保守但反馈更详细。


<details>
  <summary>Details</summary>
Motivation: 大型编程课程难以为学生代码提供及时、详细的反馈，需要自动化解决方案来辅助教学评估。

Method: 开发Mark My Works系统，结合传统单元测试和基于角色的LLM提示来分析提交代码、批判代码质量、生成教学反馈，并保持推理过程透明。

Result: 在191名学生课程中测试79份提交，AI评分与人工评分无线性相关(r=-0.177,p=0.124)，但分布相似（均左偏）。AI评分更保守（均值59.95 vs 80.53），但生成的技术反馈更详细。

Conclusion: AI自动评分系统能识别与人工评分相似的代码质量层次，虽然评分哲学不同，但能提供更详细的技术反馈，对编程教学有辅助价值。

Abstract: Large programming courses struggle to provide timely, detailed feedback on student code. We developed Mark My Works, a local autograding system that combines traditional unit testing with LLM-generated explanations. The system uses role-based prompts to analyze submissions, critique code quality, and generate pedagogical feedback while maintaining transparency in its reasoning process.
  We piloted the system in a 191-student engineering course, comparing AI-generated assessments with human grading on 79 submissions. While AI scores showed no linear correlation with human scores (r = -0.177, p = 0.124), both systems exhibited similar left-skewed distributions, suggesting they recognize comparable quality hierarchies despite different scoring philosophies. The AI system demonstrated more conservative scoring (mean: 59.95 vs 80.53 human) but generated significantly more detailed technical feedback.

</details>


### [69] [Repository Intelligence Graph: Deterministic Architectural Map for LLM Code Assistants](https://arxiv.org/abs/2601.10112)
*Tsvi Cherny-Shahar,Amiram Yehudai*

Main category: cs.SE

TL;DR: 提出了Repository Intelligence Graph (RIG)来解决代码代理在多语言项目中难以理解构建和测试结构的问题，通过确定性提取器SPADE构建RIG，显著提高了代理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 代码代理在处理多语言项目时难以恢复构建和测试结构，因为跨语言依赖关系分布在异构的构建系统和工具中，导致代理对项目结构的理解不足。

Method: 提出了Repository Intelligence Graph (RIG)，这是一个确定性的、基于证据的架构图，表示可构建组件、聚合器、运行器、测试、外部包和包管理器，通过明确的依赖和覆盖边连接。还提出了SPADE，一个从构建和测试工件中构造RIG的确定性提取器，目前基于CMake File API和CTest元数据，并将RIG暴露为LLM友好的JSON视图。

Result: 在8个存储库上评估了三个商业代理（Claude Code、Cursor、Codex），提供RIG将平均准确率提高了12.2%，完成时间减少了53.9%，每个正确答案的平均时间减少了57.8%。多语言存储库的改进更大，准确率提高了17.7%，效率提高了69.5%。

Conclusion: RIG显著提高了代码代理对存储库结构的理解能力，特别是对于多语言项目，将失败从结构误解转向基于正确结构的推理错误，但基于图的推理质量仍然是关键因素。

Abstract: Repository aware coding agents often struggle to recover build and test structure, especially in multilingual projects where cross language dependencies are encoded across heterogeneous build systems and tooling. We introduce the Repository Intelligence Graph (RIG), a deterministic, evidence backed architectural map that represents buildable components, aggregators, runners, tests, external packages, and package managers, connected by explicit dependency and coverage edges that trace back to concrete build and test definitions. We also present SPADE, a deterministic extractor that constructs RIG from build and test artifacts (currently with an automatic CMake plugin based on the CMake File API and CTest metadata), and exposes RIG as an LLM friendly JSON view that agents can treat as the authoritative description of repository structure.
  We evaluate three commercial agents (Claude Code, Cursor, Codex) on eight repositories spanning low to high build oriented complexity, including the real world MetaFFI project. Each agent answers thirty structured questions per repository with and without RIG in context, and we measure accuracy, wall clock completion time, and efficiency (seconds per correct answer). Across repositories and agents, providing RIG improves mean accuracy by 12.2\% and reduces completion time by 53.9\%, yielding a mean 57.8\% reduction in seconds per correct answer. Gains are larger in multilingual repositories, which improve by 17.7\% in accuracy and 69.5\% in efficiency on average, compared to 6.6\% and 46.1\% in single language repositories. Qualitative analysis suggests that RIG shifts failures from structural misunderstandings toward reasoning mistakes over a correct structure, while rare regressions highlight that graph based reasoning quality remains a key factor.

</details>


### [70] [Agentic Pipelines in Embedded Software Engineering: Emerging Practices and Challenges](https://arxiv.org/abs/2601.10220)
*Simin Sun,Miroslaw Staron*

Main category: cs.SE

TL;DR: 嵌入式软件工程团队正在探索如何将生成式AI整合到安全关键和资源受限的环境中，面临确定性、可靠性和可追溯性等独特挑战，通过研究识别了11种新兴实践和14个挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在变革软件开发工作流，但嵌入式软件工程组织首次将AI整合到安全关键和资源受限环境中，严格的确定性、可靠性和可追溯性要求对生成式技术的采用构成了独特挑战。

Method: 通过半结构化焦点小组访谈和结构化头脑风暴会议，对来自四家公司的十位资深专家进行定性研究，探讨嵌入式软件中生成式AI增强开发的应用。

Result: 识别了11种新兴实践和14个挑战，涉及生成式AI工具的编排、负责任治理和可持续采用，展示了嵌入式软件工程团队如何重新思考工作流、角色和工具链。

Conclusion: 嵌入式软件工程团队正在重新设计工作流程和工具链，以实现向智能代理管道和生成式AI增强开发的可持续转型，这需要解决编排、治理和采用方面的挑战。

Abstract: A new transformation is underway in software engineering, driven by the rapid adoption of generative AI in development workflows. Similar to how version control systems once automated manual coordination, AI tools are now beginning to automate many aspects of programming. For embedded software engineering organizations, however, this marks their first experience integrating AI into safety-critical and resource-constrained environments. The strict demands for determinism, reliability, and traceability pose unique challenges for adopting generative technologies.
  In this paper, we present findings from a qualitative study with ten senior experts from four companies who are evaluating generative AI-augmented development for embedded software. Through semi-structured focus group interviews and structured brainstorming sessions, we identified eleven emerging practices and fourteen challenges related to the orchestration, responsible governance, and sustainable adoption of generative AI tools. Our results show how embedded software engineering teams are rethinking workflows, roles, and toolchains to enable a sustainable transition toward agentic pipelines and generative AI-augmented development.

</details>


### [71] [Evolving with AI: A Longitudinal Analysis of Developer Logs](https://arxiv.org/abs/2601.10258)
*Agnia Sergeyuk,Eric Huang,Dariia Karaeva,Anastasiia Serova,Yaroslav Golubev,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: 该研究通过两年纵向遥测数据和调查，揭示了AI编码助手对开发者工作流的长期影响：AI用户产生更多代码但也删除更多，开发者感知生产力提升但其他维度变化不大。


<details>
  <summary>Details</summary>
Motivation: AI编码助手已成为专业IDE的标配，但其对日常开发的长期影响尚不清楚。先前研究关注短期使用或自我报告，缺乏对AI如何重塑实际编码实践的长期理解。

Method: 采用混合方法研究：收集800名开发者两年细粒度遥测数据，结合62名专业开发者的调查，分析生产力、代码质量、代码编辑、代码重用和上下文切换五个维度的工作流变化。

Result: 遥测数据显示AI用户产生更多代码但也删除更多；调查受访者报告生产力提升，但感知其他维度变化不大。揭示了工作流的"无声重构"。

Conclusion: AI编码助手正在无声地重构软件开发工作流，为未来AI增强工具设计提供实证依据。需要关注AI对代码编辑模式和开发实践的长期影响。

Abstract: AI-powered coding assistants are rapidly becoming fixtures in professional IDEs, yet their sustained influence on everyday development remains poorly understood. Prior research has focused on short-term use or self-reported perceptions, leaving open questions about how sustained AI use reshapes actual daily coding practices in the long term. We address this gap with a mixed-method study of AI adoption in IDEs, combining longitudinal two-year fine-grained telemetry from 800 developers with a survey of 62 professionals. We analyze five dimensions of workflow change: productivity, code quality, code editing, code reuse, and context switching. Telemetry reveals that AI users produce substantially more code but also delete significantly more. Meanwhile, survey respondents report productivity gains and perceive minimal changes in other dimensions. Our results offer empirical insights into the silent restructuring of software workflows and provide implications for designing future AI-augmented tooling.

</details>


### [72] [Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs](https://arxiv.org/abs/2601.10496)
*Ali Al-Kaswan,Claudio Spiess,Prem Devanbu,Arie van Deursen,Maliheh Izadi*

Main category: cs.SE

TL;DR: 论文提出一个暴露感知评估框架，量化LLM训练时接触buggy vs fixed代码对其偏好的影响，发现模型更易复制buggy代码，暴露会扭曲评估结果。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多用于代码生成和调试，但其输出仍可能包含源自训练数据的bug。需要区分LLM是偏好正确代码还是熟悉的错误版本，这可能受训练时接触的内容影响。

Method: 引入暴露感知评估框架，使用ManySStuBs4J基准，通过Data Portraits在Stack-V2语料上进行成员测试，估计每个buggy和fixed变体在训练中是否被见过。然后按暴露程度分层，使用代码补全和多种基于似然的评分指标比较模型偏好。

Result: 67%的示例在训练数据中两个变体都未出现；当只有一个出现时，fixes比bugs更常出现。模型生成中，模型复制buggy代码远多于fixes，bug暴露示例会放大这种趋势，fix暴露示例仅显示边际改进。在似然评分中，最小和最大token概率指标在所有条件下都一致偏好fixed代码，而Gini系数等指标在只有buggy变体被见过时会反转偏好。

Conclusion: 暴露会扭曲bug-fix评估结果，并突显LLM在实践中可能传播记忆错误的风险。

Abstract: Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it's been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.

</details>
