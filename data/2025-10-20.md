<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 10]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.LG](#cs.LG) [Total: 19]
- [cs.SE](#cs.SE) [Total: 3]
- [tldr.article](#tldr.article) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Structure-R1: Dynamically Leveraging Structural Knowledge in LLM Reasoning through Reinforcement Learning](https://arxiv.org/abs/2510.15191)
*Junlin Wu,Xianrui Zhong,Jiashuo Sun,Bolian Li,Bowen Jin,Jiawei Han,Qingkai Zeng*

Main category: cs.CL

TL;DR: 提出了Structure-R1框架，通过强化学习将检索内容转换为结构化表示，优化多步推理过程，在7B规模模型上达到与大模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统使用非结构化文本导致信息密度低和推理效果不佳，需要更高效的结构化知识表示来增强LLM的推理能力。

Method: 使用强化学习学习内容表示策略，动态生成适应多步推理需求的结构化格式，并引入自奖励结构验证机制确保生成结构的质量和可靠性。

Result: 在7个知识密集型基准测试中，7B规模模型达到与大模型相当的性能，证明了结构化表示对推理的增强效果。

Conclusion: 结构化表示通过提高信息密度和上下文清晰度显著增强推理能力，Structure-R1框架为知识密集型任务提供了有效的解决方案。

Abstract: Large language models (LLMs) have demonstrated remarkable advances in
reasoning capabilities. However, their performance remains constrained by
limited access to explicit and structured domain knowledge. Retrieval-Augmented
Generation (RAG) addresses this by incorporating external information as
context to augment reasoning. Nevertheless, traditional RAG systems typically
operate over unstructured and fragmented text, resulting in low information
density and suboptimal reasoning. To overcome these limitations, we propose
\textsc{Structure-R1}, a novel framework that transforms retrieved content into
structured representations optimized for reasoning. Leveraging reinforcement
learning, \textsc{Structure-R1} learns a content representation policy that
dynamically generates and adapts structural formats based on the demands of
multi-step reasoning. Unlike prior methods that rely on fixed schemas, our
approach adopts a generative paradigm capable of producing task-specific
structures tailored to individual queries. To ensure the quality and
reliability of these representations, we introduce a self-reward structural
verification mechanism that checks whether the generated structures are both
correct and self-contained. Extensive experiments on seven knowledge-intensive
benchmarks show that \textsc{Structure-R1} consistently achieves competitive
performance with a 7B-scale backbone model and matches the performance of much
larger models. Additionally, our theoretical analysis demonstrates how
structured representations enhance reasoning by improving information density
and contextual clarity. Our code and data are available at:
https://github.com/jlwu002/sr1.

</details>


### [2] [Exemplar-Guided Planing: Enhanced LLM Agent for KGQA](https://arxiv.org/abs/2510.15283)
*Jingao Xu,Shuoyoucheng Ma,Xin Song,Rong Jiang,Hongkui Tu,Bin Zhou*

Main category: cs.CL

TL;DR: 提出EGP框架，通过检索训练集中的示例问题及其成功推理路径，指导LLM在知识图谱问答中的规划过程，包括任务分解和关系探索，提升问答性能。


<details>
  <summary>Details</summary>
Motivation: LLM在知识图谱问答中面临自然语言查询与结构化知识图谱之间的语义鸿沟，导致规划不优和探索效率低，而无需训练的方法未能充分利用训练数据中的推理模式。

Method: EGP框架：预处理训练集问题进行实体模板化，检索相似示例问题及其推理路径，在任务分解和关系探索两个阶段动态指导LLM规划，并引入智能前瞻机制提高探索效率。

Result: 在WebQSP和CWQ两个真实世界KGQA数据集上的实验表明，PoG-EGP显著优于基线PoG系统和其他对比方法。

Conclusion: EGP框架通过示例引导的规划有效提升了LLM在知识图谱问答中的性能，证明了利用训练数据中成功推理模式的价值。

Abstract: Large Language Models (LLMs) as interactive agents show significant promise
in Knowledge Graph Question Answering (KGQA) but often struggle with the
semantic gap between natural language queries and structured knowledge graph
(KG) representations. This leads to suboptimal planning and inefficient
exploration on KG, while training-free approaches often underutilize valuable
reasoning patterns in training data. To address these limitations, we propose a
novel framework, Exemplar-Guided Planning (EGP), which enhances the planning
capabilities of LLM agents for KGQA. EGP first preprocesses the training set
questions via entity templating to normalize semantic variations. It then
retrieves highly similar exemplary questions and their successful reasoning
paths from this preprocessed set using semantic embeddings and an efficient
FAISS index. These retrieved exemplars dynamically guide the LLM's planning
process in two key phases: (1) Task Decomposition, by aligning generated
sub-objectives with proven reasoning steps, and (2) Relation Exploration, by
providing high-quality auxiliary information to improve relation pruning
accuracy. Additionally, we introduce a Smart Lookahead mechanism during
relation exploration to improve efficiency by preemptively exploring promising
paths and potentially terminating exploration earlier. We apply EGP to the
Plan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two
real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP
significantly improves over the baseline PoG system and other compared methods.

</details>


### [3] [CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs](https://arxiv.org/abs/2510.15455)
*Gucongcong Fan,Chaoyue Niu,Chengfei Lyu,Fan Wu,Guihai Chen*

Main category: cs.CL

TL;DR: CORE是一个协作框架，结合云端和本地LLM的优势，在保持移动代理任务准确性的同时减少UI暴露。通过布局感知块分区、协同规划和协同决策，将UI暴露减少高达55.6%。


<details>
  <summary>Details</summary>
Motivation: 云端LLM需要上传完整UI状态，暴露不必要信息；本地LLM容量有限，任务成功率低。需要结合两者优势，在保持准确性的同时保护隐私。

Method: 包含三个关键组件：布局感知块分区（基于XML层次结构分组相关UI元素）、协同规划（本地和云端LLM协作识别子任务）、协同决策（本地LLM排名相关UI块，云端LLM在顶级块中选择具体元素）。

Result: 实验显示CORE将UI暴露减少高达55.6%，同时任务成功率略低于纯云端代理，有效缓解了不必要的云端隐私暴露。

Conclusion: CORE框架成功结合了云端和本地LLM的优势，在保持任务准确性的同时显著减少UI暴露，为移动代理提供了更好的隐私保护方案。

Abstract: Mobile agents rely on Large Language Models (LLMs) to plan and execute tasks
on smartphone user interfaces (UIs). While cloud-based LLMs achieve high task
accuracy, they require uploading the full UI state at every step, exposing
unnecessary and often irrelevant information. In contrast, local LLMs avoid UI
uploads but suffer from limited capacity, resulting in lower task success
rates. We propose $\textbf{CORE}$, a $\textbf{CO}$llaborative framework that
combines the strengths of cloud and local LLMs to $\textbf{R}$educe UI
$\textbf{E}$xposure, while maintaining task accuracy for mobile agents. CORE
comprises three key components: (1) $\textbf{Layout-aware block partitioning}$,
which groups semantically related UI elements based on the XML screen
hierarchy; (2) $\textbf{Co-planning}$, where local and cloud LLMs
collaboratively identify the current sub-task; and (3)
$\textbf{Co-decision-making}$, where the local LLM ranks relevant UI blocks,
and the cloud LLM selects specific UI elements within the top-ranked block.
CORE further introduces a multi-round accumulation mechanism to mitigate local
misjudgment or limited context. Experiments across diverse mobile apps and
tasks show that CORE reduces UI exposure by up to 55.6% while maintaining task
success rates slightly below cloud-only agents, effectively mitigating
unnecessary privacy exposure to the cloud. The code is available at
https://github.com/Entropy-Fighter/CORE.

</details>


### [4] [DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios](https://arxiv.org/abs/2510.15501)
*Yao Huang,Yitong Sun,Yichi Zhang,Ruochen Zhang,Yinpeng Dong,Xingxing Wei*

Main category: cs.CL

TL;DR: DeceptionBench是首个系统评估LLM在不同社会领域中欺骗行为的基准，涵盖5个领域150个场景，探索内在行为模式和外在因素影响，揭示模型在强化动态下欺骗行为加剧的脆弱性。


<details>
  <summary>Details</summary>
Motivation: LLM能力的快速提升带来了新兴的欺骗行为，在关键部署中可能引发严重风险，而现实世界场景中的欺骗特征研究仍然不足。

Method: 建立包含经济、医疗、教育、社交互动和娱乐5个领域150个场景的基准，评估内在的利己主义和奉承行为，以及外在情境因素（中性条件、奖励激励、强制压力）的影响，并纳入持续多轮交互循环。

Result: 实验发现LLM和LRM存在关键脆弱性，特别是在强化动态下欺骗行为加剧，表明当前模型缺乏对操纵性情境线索的鲁棒抵抗能力。

Conclusion: 当前模型对各种欺骗行为缺乏有效防护，迫切需要开发先进的安全保障机制。

Abstract: Despite the remarkable advances of Large Language Models (LLMs) across
diverse cognitive tasks, the rapid enhancement of these capabilities also
introduces emergent deceptive behaviors that may induce severe risks in
high-stakes deployments. More critically, the characterization of deception
across realistic real-world scenarios remains underexplored. To bridge this
gap, we establish DeceptionBench, the first benchmark that systematically
evaluates how deceptive tendencies manifest across different societal domains,
what their intrinsic behavioral patterns are, and how extrinsic factors affect
them. Specifically, on the static count, the benchmark encompasses 150
meticulously designed scenarios in five domains, i.e., Economy, Healthcare,
Education, Social Interaction, and Entertainment, with over 1,000 samples,
providing sufficient empirical foundations for deception analysis. On the
intrinsic dimension, we explore whether models exhibit self-interested egoistic
tendencies or sycophantic behaviors that prioritize user appeasement. On the
extrinsic dimension, we investigate how contextual factors modulate deceptive
outputs under neutral conditions, reward-based incentivization, and coercive
pressures. Moreover, we incorporate sustained multi-turn interaction loops to
construct a more realistic simulation of real-world feedback dynamics.
Extensive experiments across LLMs and Large Reasoning Models (LRMs) reveal
critical vulnerabilities, particularly amplified deception under reinforcement
dynamics, demonstrating that current models lack robust resistance to
manipulative contextual cues and the urgent need for advanced safeguards
against various deception behaviors. Code and resources are publicly available
at https://github.com/Aries-iai/DeceptionBench.

</details>


### [5] [Latent Reasoning in LLMs as a Vocabulary-Space Superposition](https://arxiv.org/abs/2510.15522)
*Jingcheng Deng,Liang Pang,Zihao Wei,Shichen Xu,Zenghao Duan,Kun Xu,Yang Song,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: Latent-SFT提出了一种两阶段学习框架，通过在词汇表列空间中进行潜在推理，将推理链压缩至多4倍，同时保持与显式推理相当的性能。


<details>
  <summary>Details</summary>
Motivation: 显式推理链带来大量计算开销，而现有潜在推理方法性能显著下降。研究发现性能下降源于非结构化的潜在空间难以拟合潜在标记。

Method: 将潜在空间限制在LLM词汇表的列空间中，将潜在推理视为词汇概率的叠加。采用两阶段学习：第一阶段设计专门注意力掩码指导潜在标记编码器；第二阶段丢弃编码器，直接训练LLM自主生成潜在标记进行推理。

Result: 在GSM8k上达到新的SOTA，匹配显式SFT性能同时将推理链压缩至多4倍；在Math500和AIME24上，基于词汇概率的潜在推理明显优于基于隐藏状态的方法。

Conclusion: 潜在推理既是单一路径的压缩，也是多路径的叠加，有效压缩率和有效全局并行度指标验证了这一点。

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities with
chain-of-thought prompting, but explicit reasoning introduces substantial
computational overhead. Recent work on latent reasoning reduces this cost by
reasoning in latent space without explicit supervision, but performance drops
significantly. Our preliminary experiments suggest that this degradation stems
from the unstructured latent space, which makes fitting latent tokens
difficult. To address this, we restrict the latent space to the column space of
the LLM vocabulary, treating latent reasoning as a superposition over
vocabulary probabilities. Once latent reasoning concludes, it collapses into an
eigenstate of explicit reasoning to yield the final answer. Based on this idea,
we propose Latent-SFT, a two-stage learning framework. In the first stage, we
design two specialized attention masks to guide the Latent Token Encoder in
generating latent tokens, allowing the LLM to produce the correct answer
conditioned on them. In the second stage, the Latent Token Encoder is
discarded, and the LLM is directly trained to generate these latent tokens
autonomously for latent reasoning, optimized with KL and CE losses. Latent-SFT
sets a new state of the art on GSM8k, matching explicit SFT performance while
cutting reasoning chains by up to 4 times and outperforming prior latent
methods. On Math500 and AIME24, lexical probability-based latent reasoning also
clearly surpasses hidden-state-based approaches. Our metrics of effective
compression rate and effective global parallelism further show that latent
reasoning is both the compression of a single path and the superposition of
multiple paths.

</details>


### [6] [HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination](https://arxiv.org/abs/2510.15614)
*Tingting Chen,Beibei Lin,Zifeng Yuan,Qiran Zou,Hongyu He,Yew-Soon Ong,Anirudh Goyal,Dianbo Liu*

Main category: cs.CL

TL;DR: HypoSpace是一个评估语言模型生成多种解释能力的诊断套件，在三个结构化领域中测量有效性、独特性和覆盖率指标，发现随着可接受空间增大，模型会出现模式崩溃现象。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在科学工作流中的广泛应用，需要评估它们提出多种解释集合（而非单一正确答案）的能力，因为许多科学问题存在多种机制不同的合理解释。

Method: 将LLMs视为有限假设集的采样器，在三个具有确定性验证器和精确枚举假设空间的结构化领域（因果图、重力约束3D体素重建、布尔遗传交互）中测量有效性、独特性和覆盖率三个互补指标。

Result: 在指令调优和推理导向模型中，有效性通常保持较高，但随着可接受空间增大，独特性和覆盖率会下降，揭示了仅基于正确性指标无法发现的模式崩溃现象。

Conclusion: HypoSpace为显式探索和覆盖可接受解释空间的方法提供了一个受控探针，而非排行榜。

Abstract: As language models are increasingly used in scientific workflows, evaluating
their ability to propose sets of explanations-not just a single correct
answer-becomes critical. Many scientific problems are underdetermined:
multiple, mechanistically distinct hypotheses are consistent with the same
observations. We introduce HypoSpace, a diagnostic suite that treats LLMs as
samplers of finite hypothesis sets and measures three complementary indicators:
Validity (precision of proposals consistent with observations), Uniqueness
(non-redundancy among proposals), and Recovery (coverage of the enumerated
admissible set). We instantiate HypoSpace in three structured domains with
deterministic validators and exactly enumerated hypothesis spaces: (i) causal
graphs from perturbations, (ii) gravity-constrained 3D voxel reconstruction
from top-down projections, and (iii) Boolean genetic interactions. Across
instruction-tuned and reasoning-focused models, Validity often remains high
while Uniqueness and Recovery degrade as the admissible space grows, revealing
mode collapse that is invisible to correctness-only metrics. HypoSpace offers a
controlled probe-rather than a leaderboard-for methods that explicitly explore
and cover admissible explanation spaces. Code is available at:
https://github.com/CTT-Pavilion/_HypoSpace.

</details>


### [7] [LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned Evaluation](https://arxiv.org/abs/2510.15746)
*Gao Yang,Yuhang Liu,Siyu Miao,Xinyue Liang,Zhengyang Liu,Heyan Huang*

Main category: cs.CL

TL;DR: 本文提出了一种基于博弈论投票算法的LLM自动互评估框架，通过自博弈和同行评审让LLM相互评估，并与人类投票行为进行对比验证。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法依赖固定格式任务和参考答案，难以捕捉现代LLM的细微、主观和开放式行为特征，需要更有效的评估方式。

Method: 采用自动互评估方法，让LLM通过自博弈和同行评审相互评估输出，使用博弈论投票算法聚合同行评审结果，并与人类判断进行系统性比较。

Result: 实证结果显示理论预测与人类评估之间存在收敛和分歧，揭示了互评估方法的潜力和局限性。

Conclusion: 这是首个将互评估、博弈论聚合和人类验证相结合的工作，为LLM能力评估提供了新的视角和方法。

Abstract: Ideal or real - that is the question.In this work, we explore whether
principles from game theory can be effectively applied to the evaluation of
large language models (LLMs). This inquiry is motivated by the growing
inadequacy of conventional evaluation practices, which often rely on
fixed-format tasks with reference answers and struggle to capture the nuanced,
subjective, and open-ended nature of modern LLM behavior. To address these
challenges, we propose a novel alternative: automatic mutual evaluation, where
LLMs assess each other's output through self-play and peer review. These peer
assessments are then systematically compared with human voting behavior to
evaluate their alignment with human judgment. Our framework incorporates
game-theoretic voting algorithms to aggregate peer reviews, enabling a
principled investigation into whether model-generated rankings reflect human
preferences. Empirical results reveal both convergences and divergences between
theoretical predictions and human evaluations, offering valuable insights into
the promises and limitations of mutual evaluation. To the best of our
knowledge, this is the first work to jointly integrate mutual evaluation,
game-theoretic aggregation, and human-grounded validation for evaluating the
capabilities of LLMs.

</details>


### [8] [Paper2Web: Let's Make Your Paper Alive!](https://arxiv.org/abs/2510.15842)
*Yuhang Chen,Tianpeng Lv,Siyi Zhang,Yixiang Yin,Yao Wan,Philip S. Yu,Dongping Chen*

Main category: cs.CL

TL;DR: Paper2Web是一个用于评估学术网页生成的基准数据集和多维度评估框架，包含PWAgent自动化管道，可将科学论文转换为交互式多媒体学术主页。


<details>
  <summary>Details</summary>
Motivation: 当前方法（如直接LLM生成、模板或HTML转换）难以生成具有布局感知和交互性的学术网站，且缺乏全面的评估套件。

Method: 提出Paper2Web基准数据集，包含基于规则的指标（连通性、完整性）、人类验证的LLM-as-a-Judge评估（交互性、美观性、信息性）和PaperQuiz（知识保留度）。同时开发PWAgent自动化管道，通过MCP工具迭代优化内容和布局。

Result: PWAgent在学术网页生成任务中显著优于端到端基线方法（如基于模板的网页和arXiv/alphaXiv版本），同时保持低成本，实现了帕累托前沿。

Conclusion: Paper2Web为学术网页生成提供了全面的评估框架，PWAgent能够高效生成高质量的交互式学术主页。

Abstract: Academic project websites can more effectively disseminate research when they
clearly present core content and enable intuitive navigation and interaction.
However, current approaches such as direct Large Language Model (LLM)
generation, templates, or direct HTML conversion struggle to produce
layout-aware, interactive sites, and a comprehensive evaluation suite for this
task has been lacking. In this paper, we introduce Paper2Web, a benchmark
dataset and multi-dimensional evaluation framework for assessing academic
webpage generation. It incorporates rule-based metrics like Connectivity,
Completeness and human-verified LLM-as-a-Judge (covering interactivity,
aesthetics, and informativeness), and PaperQuiz, which measures paper-level
knowledge retention. We further present PWAgent, an autonomous pipeline that
converts scientific papers into interactive and multimedia-rich academic
homepages. The agent iteratively refines both content and layout through MCP
tools that enhance emphasis, balance, and presentation quality. Our experiments
show that PWAgent consistently outperforms end-to-end baselines like
template-based webpages and arXiv/alphaXiv versions by a large margin while
maintaining low cost, achieving the Pareto-front in academic webpage
generation.

</details>


### [9] [InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training](https://arxiv.org/abs/2510.15859)
*Pengkai Wang,Qi Zuo,Pengwei Liu,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: ORBIT框架通过基于评分标准的增量强化学习，在开放式高风险医疗对话任务中显著提升LLM性能，在HealthBench-Hard基准上从7.0提升至27.2。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在开放式领域（如医疗咨询）中因缺乏明确奖励函数而难以应用强化学习的问题，这些领域的奖励通常模糊、主观且依赖上下文。

Method: ORBIT框架结合合成对话生成和动态评分标准创建，使用评分标准指导增量强化学习过程，无需外部医学知识或人工规则。

Result: 在Qwen3-4B-Instruct模型上，仅使用2k样本就将HealthBench-Hard基准性能从7.0提升至27.2，达到同规模模型的最先进水平。

Conclusion: 评分标准驱动的强化学习是推进LLM在复杂开放式任务中表现的可扩展策略，能在多样化咨询场景中实现一致性能提升。

Abstract: Large Language Models (LLMs) have shown substantial advances through
reinforcement learning (RL), particularly in domains where rewards can be
programmatically verified, such as mathematics and code. In these areas, models
benefit from a well-defined operational base guided by explicit rule-based
objectives. However, this progress reveals a significant limitation: in
open-ended domains where rewards are ambiguous, subjective, or
context-dependent, such as creative writing, scientific reasoning, and notably
medical consultation, robust reward functions are lacking, making these areas
challenging for current RL strategies. To bridge this gap, we introduce ORBIT,
an open-ended rubric-based incremental training framework specifically designed
for high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue
generation with the dynamic creation of rubrics, employing these rubrics to
direct an incremental RL process. In particular, this approach does not depend
on external medical knowledge or manual rules, instead utilizing rubric-guided
feedback to shape learning. When implemented on the Qwen3-4B-Instruct model,
our method can greatly enhance its performance on the HealthBench-Hard
benchmark from 7.0 to 27.2 using only 2k samples, thus achieving
state-of-the-art results for models of this scale. Our analysis confirms that
rubric-driven RL fos-ters consistent performance gains across diverse
consultation scenarios, going beyond simple numerical improvements. These
findings underscore rubric-based feedback as a scalable strategy for advancing
LLMs in intricate, open-ended tasks.

</details>


### [10] [PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction](https://arxiv.org/abs/2510.15863)
*Simon Yu,Gang Li,Weiyan Shi,Peng Qi*

Main category: cs.CL

TL;DR: PolySkill框架通过将技能的抽象目标与具体实现解耦，使智能体能够学习可泛化和组合的技能，在网页交互任务中显著提升技能重用率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法学习的技能往往过度专门化于单一网站，缺乏泛化能力。为了解决这个问题，需要开发能够学习可泛化技能的框架。

Method: 受软件工程中多态性的启发，PolySkill将技能的抽象目标（实现什么）与具体实现（如何执行）解耦，使智能体能够学习可组合和泛化的技能。

Result: 实验显示：1) 在已见网站上技能重用率提高1.7倍；2) 在Mind2Web和未见网站上成功率分别提升9.4%和13.9%，同时步骤减少超过20%；3) 在自主探索环境中能提出更高质量的任务并学习跨网站泛化的技能。

Conclusion: 将技能目标与执行分离是开发能够在开放网络中持续学习和泛化的自主智能体的关键步骤。

Abstract: Large language models (LLMs) are moving beyond static uses and are now
powering agents that learn continually during their interaction with external
environments. For example, agents can learn reusable skills while navigating
web pages or toggling new tools. However, existing methods for skill learning
often create skills that are over-specialized to a single website and fail to
generalize. We introduce PolySkill, a new framework that enables agents to
learn generalizable and compositional skills. The core idea, inspired by
polymorphism in software engineering, is to decouple a skill's abstract goal
(what it accomplishes) and its concrete implementation (how it is executed).
Experiments show that our method (1) improves skill reuse by 1.7x on seen
websites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on
unseen websites, while reducing steps by over 20%. (3) In self-exploration
settings without specified tasks, our framework improves the quality of
proposed tasks and enables agents to learn generalizable skills that work
across different sites. By enabling the agent to identify and refine its own
goals, the PolySkill enhances the agent's ability to learn a better curriculum,
leading to the acquisition of more generalizable skills compared to baseline
methods. This work provides a practical path toward building agents capable of
continual learning in adaptive environments. Our findings show that separating
a skill's goal from its execution is a crucial step toward developing
autonomous agents that can learn and generalize across the open web
continuously.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Procedural Game Level Design with Deep Reinforcement Learning](https://arxiv.org/abs/2510.15120)
*Miraç Buğra Özkan*

Main category: cs.AI

TL;DR: 提出了一种基于深度强化学习的程序化关卡设计方法，使用两个智能体在Unity 3D环境中协同工作：蜂鸟智能体负责导航收集花朵，浮岛智能体负责生成和放置花朵。


<details>
  <summary>Details</summary>
Motivation: 探索深度强化学习在游戏程序化内容生成中的应用，减少手动设计工作量，创造动态、可重玩和可扩展的游戏环境。

Method: 使用Unity ML-Agents工具包，采用近端策略优化(PPO)算法训练两个智能体：蜂鸟智能体学习导航和收集花朵，浮岛智能体学习基于障碍物位置和蜂鸟性能反馈生成花朵布局。

Result: 系统产生了涌现行为，在不同环境配置下表现出强大的泛化能力，实现了有效的智能体行为和自主游戏关卡设计。

Conclusion: 深度强化学习能够使智能体在虚拟环境中既生成又解决内容，为AI在创意游戏开发过程中的贡献开辟了新机会。

Abstract: Procedural content generation (PCG) has become an increasingly popular
technique in game development, allowing developers to generate dynamic,
replayable, and scalable environments with reduced manual effort. In this
study, a novel method for procedural level design using Deep Reinforcement
Learning (DRL) within a Unity-based 3D environment is proposed. The system
comprises two agents: a hummingbird agent, acting as a solver, and a floating
island agent, responsible for generating and placing collectible objects
(flowers) on the terrain in a realistic and context-aware manner. The
hummingbird is trained using the Proximal Policy Optimization (PPO) algorithm
from the Unity ML-Agents toolkit. It learns to navigate through the terrain
efficiently, locate flowers, and collect them while adapting to the
ever-changing procedural layout of the island. The island agent is also trained
using the Proximal Policy Optimization (PPO) algorithm. It learns to generate
flower layouts based on observed obstacle positions, the hummingbird's initial
state, and performance feedback from previous episodes. The interaction between
these agents leads to emergent behavior and robust generalization across
various environmental configurations. The results demonstrate that the approach
not only produces effective and efficient agent behavior but also opens up new
opportunities for autonomous game level design driven by machine learning. This
work highlights the potential of DRL in enabling intelligent agents to both
generate and solve content in virtual environments, pushing the boundaries of
what AI can contribute to creative game development processes.

</details>


### [12] [Towards Error Centric Intelligence I, Beyond Observational Learning](https://arxiv.org/abs/2510.15128)
*Marcus A. Thomas*

Main category: cs.AI

TL;DR: 论文认为AGI发展的瓶颈在于理论而非数据或规模，提出基于批判理性主义的因果力学框架，强调假设空间变化作为核心操作，通过模块化干预和错误发现机制实现智能系统的发展。


<details>
  <summary>Details</summary>
Motivation: 挑战柏拉图表示假说，指出观测等价的世界在干预下可能不同，仅靠观测充分性无法保证干预能力，需要转向错误中心的视角来解决AGI的理论限制。

Method: 提出因果力学框架，将假设空间变化作为首要操作，引入局部性自主原则、独立因果机制和组合自主原则等结构原则，使错误发现和修正变得可行。

Result: 建立了理论框架，提出了可操作的诊断方法，旨在构建能够将不可达错误转化为可达错误并进行修正的系统脚手架。

Conclusion: AGI发展的关键突破在于理论创新而非数据规模，因果力学框架为解决智能系统的根本限制提供了新的路径。

Abstract: We argue that progress toward AGI is theory limited rather than data or scale
limited. Building on the critical rationalism of Popper and Deutsch, we
challenge the Platonic Representation Hypothesis. Observationally equivalent
worlds can diverge under interventions, so observational adequacy alone cannot
guarantee interventional competence. We begin by laying foundations,
definitions of knowledge, learning, intelligence, counterfactual competence and
AGI, and then analyze the limits of observational learning that motivate an
error centric shift. We recast the problem as three questions about how
explicit and implicit errors evolve under an agent's actions, which errors are
unreachable within a fixed hypothesis space, and how conjecture and criticism
expand that space. From these questions we propose Causal Mechanics, a
mechanisms first program in which hypothesis space change is a first class
operation and probabilistic structure is used when useful rather than presumed.
We advance structural principles that make error discovery and correction
tractable, including a differential Locality and Autonomy Principle for modular
interventions, a gauge invariant form of Independent Causal Mechanisms for
separability, and the Compositional Autonomy Principle for analogy
preservation, together with actionable diagnostics. The aim is a scaffold for
systems that can convert unreachable errors into reachable ones and correct
them.

</details>


### [13] [HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks](https://arxiv.org/abs/2510.15144)
*Chance Jiajie Li,Zhenze Mo,Yuhan Tang,Ao Qu,Jiayi Wu,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Hang Jiang,Paul Pu Liang,Jinhua Zhao,Luis Alberto Alonso Pastor,Kent Larson*

Main category: cs.AI

TL;DR: HugAgent是一个用于评估AI模型如何适应个体推理风格的基准测试，包含合成和人类双轨设计，旨在让机器推理更贴近人类思维的个体性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然能大规模近似人类响应，但主要反映群体共识，缺乏对个体推理风格和信念轨迹的捕捉能力。

Method: 采用双轨设计：合成轨道用于规模化和系统性压力测试，人类轨道用于生态有效的"出声思考"推理数据收集。

Result: 实验显示最先进的LLM在适应个体推理方面仍存在持续差距。

Conclusion: HugAgent是首个可扩展的基准测试，用于将机器推理与人类思维的个体性对齐。

Abstract: Simulating human reasoning in open-ended tasks has been a long-standing
aspiration in AI and cognitive science. While large language models now
approximate human responses at scale, they remain tuned to population-level
consensus, often erasing the individuality of reasoning styles and belief
trajectories. To advance the vision of more human-like reasoning in machines,
we introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for
average-to-individual reasoning adaptation. The task is to predict how a
specific person would reason and update their beliefs in novel scenarios, given
partial evidence of their past views. HugAgent adopts a dual-track design: a
synthetic track for scale and systematic stress tests, and a human track for
ecologically valid, "out-loud" reasoning data. This design enables scalable,
reproducible evaluation of intra-agent fidelity: whether models can capture not
just what people believe, but how their reasoning evolves. Experiments with
state-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent
as the first extensible benchmark for aligning machine reasoning with the
individuality of human thought. Our benchmark and chatbot are open-sourced as
HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking
(https://anonymous.4open.science/r/trace-your-thinking).

</details>


### [14] [Experience-Driven Exploration for Efficient API-Free AI Agents](https://arxiv.org/abs/2510.15259)
*Chenwei Tang,Jingyu Xing,Xinyu Liu,Zizhou Wang,Jiawei Du,Liangli Zhen,Jiancheng Lv*

Main category: cs.AI

TL;DR: KG-Agent是一个经验驱动的学习框架，通过将像素级GUI交互构建为状态-动作知识图谱来解决API缺失环境中的效率瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有软件大多缺乏可访问的API，导致LLM智能体只能通过像素级GUI操作，面临效率低下、短视决策和试错探索的问题。

Method: 构建状态-动作知识图谱(SA-KG)，将功能相似但视觉不同的GUI状态连接起来，并设计基于图拓扑的混合内在奖励机制，结合状态价值奖励和新颖性奖励。

Result: 在Civilization V和Slay the Spire两个复杂GUI决策环境中，KG-Agent在探索效率和战略深度方面显著优于现有最先进方法。

Conclusion: KG-Agent通过结构化经验表示和混合奖励机制，有效解决了API缺失环境中的探索效率和长期规划问题。

Abstract: Most existing software lacks accessible Application Programming Interfaces
(APIs), requiring agents to operate solely through pixel-based Graphical User
Interfaces (GUIs). In this API-free setting, large language model (LLM)-based
agents face severe efficiency bottlenecks: limited to local visual experiences,
they make myopic decisions and rely on inefficient trial-and-error, hindering
both skill acquisition and long-term planning. To address these challenges, we
propose KG-Agent, an experience-driven learning framework that structures an
agent's raw pixel-level interactions into a persistent State-Action Knowledge
Graph (SA-KG). KG-Agent overcomes inefficient exploration by linking
functionally similar but visually distinct GUI states, forming a rich
neighborhood of experience that enables the agent to generalize from a diverse
set of historical strategies. To support long-horizon reasoning, we design a
hybrid intrinsic reward mechanism based on the graph topology, combining a
state value reward for exploiting known high-value pathways with a novelty
reward that encourages targeted exploration. This approach decouples strategic
planning from pure discovery, allowing the agent to effectively value setup
actions with delayed gratification. We evaluate KG-Agent in two complex,
open-ended GUI-based decision-making environments (Civilization V and Slay the
Spire), demonstrating significant improvements in exploration efficiency and
strategic depth over the state-of-the-art methods.

</details>


### [15] [AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory](https://arxiv.org/abs/2510.15261)
*Jitesh Jain,Shubham Maheshwari,Ning Yu,Wen-mei Hwu,Humphrey Shi*

Main category: cs.AI

TL;DR: AUGUSTUS是一个多模态代理系统，采用基于语义标签和图结构的多模态上下文记忆，在图像分类和对话任务中优于传统多模态RAG方法，且速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统主要存储文本信息，忽视了多模态信号的重要性。受人类记忆的多模态特性启发，作者希望构建一个符合认知科学中人类记忆理念的多模态代理系统。

Method: 系统包含4个循环阶段：编码（理解输入）、存储记忆（保存重要信息）、检索（从记忆中搜索相关上下文）、行动（执行任务）。不同于使用向量数据库的传统方法，该系统将信息概念化为语义标签，并将标签与其上下文关联存储在图形结构的多模态上下文记忆中，实现高效的概念驱动检索。

Result: 在ImageNet分类任务中比传统多模态RAG方法快3.5倍，在MSC基准测试中优于MemGPT。

Conclusion: AUGUSTUS系统通过多模态上下文记忆和概念驱动检索，在性能和效率上都优于现有方法，验证了多模态记忆在代理系统中的重要性。

Abstract: Riding on the success of LLMs with retrieval-augmented generation (RAG),
there has been a growing interest in augmenting agent systems with external
memory databases. However, the existing systems focus on storing text
information in their memory, ignoring the importance of multimodal signals.
Motivated by the multimodal nature of human memory, we present AUGUSTUS, a
multimodal agent system aligned with the ideas of human memory in cognitive
science. Technically, our system consists of 4 stages connected in a loop: (i)
encode: understanding the inputs; (ii) store in memory: saving important
information; (iii) retrieve: searching for relevant context from memory; and
(iv) act: perform the task. Unlike existing systems that use vector databases,
we propose conceptualizing information into semantic tags and associating the
tags with their context to store them in a graph-structured multimodal
contextual memory for efficient concept-driven retrieval. Our system
outperforms the traditional multimodal RAG approach while being 3.5 times
faster for ImageNet classification and outperforming MemGPT on the MSC
benchmark.

</details>


### [16] [WebGen-V Bench: Structured Representation for Enhancing Visual Design in LLM-based Web Generation and Evaluation](https://arxiv.org/abs/2510.15306)
*Kuang-Da Wang,Zhao Wang,Yotaro Shimose,Wei-Yao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: WebGen-V是一个用于指令到HTML生成的新基准和框架，通过智能爬取、结构化数据表示和细粒度多模态评估来提升数据质量和评估精度。


<details>
  <summary>Details</summary>
Motivation: 利用LLM在编码和多模态理解方面的进展，解决现有指令到HTML生成任务中数据质量不足和评估粒度不够精细的问题。

Method: 提出三个关键创新：1) 无边界可扩展的智能爬取框架收集真实网页；2) 结构化分节数据表示，整合元数据、局部UI截图和JSON格式资源；3) 分节级多模态评估协议，对齐文本、布局和视觉组件。

Result: 通过最先进LLM的实验和消融研究验证了结构化数据和分节评估的有效性，以及各组件对性能的贡献。

Conclusion: WebGen-V是首个实现高粒度智能爬取和评估的指令到HTML生成工作，提供了从真实数据获取到结构化多模态评估的统一流程。

Abstract: Witnessed by the recent advancements on leveraging LLM for coding and
multimodal understanding, we present WebGen-V, a new benchmark and framework
for instruction-to-HTML generation that enhances both data quality and
evaluation granularity. WebGen-V contributes three key innovations: (1) an
unbounded and extensible agentic crawling framework that continuously collects
real-world webpages and can leveraged to augment existing benchmarks; (2) a
structured, section-wise data representation that integrates metadata,
localized UI screenshots, and JSON-formatted text and image assets, explicit
alignment between content, layout, and visual components for detailed
multimodal supervision; and (3) a section-level multimodal evaluation protocol
aligning text, layout, and visuals for high-granularity assessment. Experiments
with state-of-the-art LLMs and ablation studies validate the effectiveness of
our structured data and section-wise evaluation, as well as the contribution of
each component. To the best of our knowledge, WebGen-V is the first work to
enable high-granularity agentic crawling and evaluation for instruction-to-HTML
generation, providing a unified pipeline from real-world data acquisition and
webpage generation to structured multimodal assessment.

</details>


### [17] [Towards Flash Thinking via Decoupled Advantage Policy Optimization](https://arxiv.org/abs/2510.15374)
*Zezhong Tan,Hang Gao,Xinhong Ma,Feng Zhang,Ziqiang Dong*

Main category: cs.AI

TL;DR: 提出DEPO强化学习框架，通过优势解耦算法、难度感知长度惩罚和优势裁剪方法，减少模型推理中的低效token和过度思考问题，在保持准确率的同时显著缩短响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法虽然提升了模型准确率，但存在响应过长和过度思考问题，导致推理延迟和计算消耗增加，特别是对简单任务而言不必要的推理过程。

Method: DEPO框架包含三个核心组件：优势解耦算法指导减少低效token、难度感知长度惩罚降低整体响应长度、优势裁剪方法防止策略优化偏差。

Result: 在DeepSeek-Distill-Qwen-7B和1.5B模型上，DEPO实现了序列长度减少39%，减少了低效token中的过度推理路径，同时在整体准确率上优于基础模型。

Conclusion: DEPO框架有效解决了强化学习模型中的过度推理问题，在保持性能的同时显著提升了推理效率。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable performance in
solving complex problems via supervised fine-tuning (SFT) and reinforcement
learning (RL). Although existing RL algorithms significantly enhance model
accuracy, they still suffer from excessively lengthy responses and overthinking
issues, resulting in increased inference latency and computational consumption,
especially for simple tasks that require minimal reasoning. To address this, we
propose a novel RL framework, DEPO, to reduce inefficient reasoning for models.
Our method mainly consists of three core components: (1) an innovative
advantage decoupled algorithm to guide model reduction of inefficient tokens;
(2) a difficulty-aware length penalty to lower the overall length of model
responses; (3) an advantage clipping method to prevent bias in policy
optimization. In our experiments, applied to DeepSeek-Distill-Qwen-7B and
DeepSeek-Distill-Qwen-1.5B as base models, DEPO achieves a significant
reduction in sequence length by 39% and reduces excessive reasoning paths in
inefficient tokens, while outperforming the base model in overall accuracy.

</details>


### [18] [MARS: Reinforcing Multi-Agent Reasoning of LLMs through Self-Play in Strategic Games](https://arxiv.org/abs/2510.15414)
*Huining Yuan,Zelai Xu,Zheyue Tan,Xiangmin Yi,Mo Guang,Kaiwen Long,Haojia Hui,Boxun Li,Xinlei Chen,Bo Zhao,Xiao-Ping Zhang,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: MARS是一个端到端强化学习框架，通过自博弈在合作和竞争游戏中激励LLMs的多智能体推理能力，在推理基准测试中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决多轮多智能体场景中长时程信用分配和智能体特定优势估计的挑战，开发更先进的多智能体系统。

Method: 引入MARS框架，包含回合级优势估计器和智能体特定优势归一化，通过自博弈在合作和竞争游戏中训练。

Result: 在Qwen3-4B上训练的MARS智能体在保留游戏中性能提升达28.7%，在AIME和GPQA-Diamond基准测试中分别获得10.0%和12.5%的性能增益。

Conclusion: 端到端RL训练与战略游戏自博弈是开发LLMs通用多智能体推理能力的有效方法。

Abstract: Developing Large Language Models (LLMs) to cooperate and compete effectively
within multi-agent systems is a critical step towards more advanced
intelligence. While reinforcement learning (RL) has proven effective for
enhancing reasoning in single-agent tasks, its extension to multi-turn,
multi-agent scenarios remains underexplored due to the challenges of
long-horizon credit assignment and agent-specific advantage estimation. To
address these challenges, we introduce MARS, an end-to-end RL framework that
incentivizes Multi-Agent Reasoning of LLMs through Self-play in both
cooperative and competitive games. MARS features a turn-level advantage
estimator that aligns learning signals with each interaction for credit
assignment, and an agent-specific advantage normalization to stabilize
multi-agent training. By learning with self-play across cooperative and
competitive games, the MARS agent trained from Qwen3-4B develops strong
strategic abilities that generalize to held-out games with up to 28.7%
performance improvements. More importantly, the capability acquired through
self-play generalizes beyond games, yielding consistent performance gains of
multi-agent systems in reasoning benchmarks. When integrated into leading
multi-agent systems, our MARS agent achieves significant performance gains of
10.0% on AIME and 12.5% on GPQA-Diamond. These results establish end-to-end RL
training with self-play in strategic games as a powerful approach for
developing generalizable multi-agent reasoning capabilities in LLMs. Our code
and models are publicly available at https://github.com/thu-nics/MARS.

</details>


### [19] [Adaptive Minds: Empowering Agents with LoRA-as-Tools](https://arxiv.org/abs/2510.15416)
*Pavan C Shekar,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: Adaptive Minds是一个智能代理系统，将LoRA适配器作为领域特定工具，让基础LLM充当语义路由器动态选择最相关的LoRA工具，实现按需切换领域专家。


<details>
  <summary>Details</summary>
Motivation: 解决单一微调模型或基于规则路由的局限性，通过动态工具选择实现更灵活和准确的领域自适应AI辅助。

Method: 使用LangGraph进行工作流管理，基础LLM分析查询并动态选择最相关的LoRA适配器作为领域专家工具。

Result: 系统能够提供准确、专业化的响应，同时保持对话能力，支持API和Web接口，完全开源。

Conclusion: Adaptive Minds结合了多智能体编排的灵活性和参数高效微调的高效性，为领域自适应AI辅助提供了可扩展的基础。

Abstract: We present Adaptive Minds, an agentic system that treats LoRA adapters as
domain-specific tools. Instead of relying on a single fine-tuned model or rigid
rule-based routing, our approach empowers the base LLM itself to act as a
semantic router analyzing each query and dynamically selecting the most
relevant LoRA tool. This enables the agent to seamlessly switch between
different domain experts on demand. By combining the flexibility of multi-agent
orchestration with the efficiency of parameter-efficient fine-tuning, Adaptive
Minds delivers accurate, specialized responses while preserving conversational
ability. The system is built with LangGraph for workflow management, supports
both API and web interfaces, and is fully open source, providing a scalable and
extensible foundation for domain-adaptive AI assistance.

</details>


### [20] [Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation](https://arxiv.org/abs/2510.15624)
*Ed Li,Junyu Ren,Xintian Pan,Cat Yan,Chuanhao Li,Dirk Bergemann,Zhuoran Yang*

Main category: cs.AI

TL;DR: 提出了freephdlabor，一个开源的多智能体框架，具有完全动态的工作流程和模块化架构，支持自动上下文压缩、工作区通信、内存持久化和非阻塞人工干预，旨在实现持续的研究程序。


<details>
  <summary>Details</summary>
Motivation: 现有的科学发现自动化系统存在两个基本限制：僵化的预编程工作流程无法适应中间发现，以及不充分的上下文管理阻碍了长期研究。

Method: 采用多智能体框架，通过实时智能体推理确定完全动态的工作流程，模块化架构允许用户修改、添加或删除智能体以满足特定领域需求，并提供自动上下文压缩、工作区通信、内存持久化和非阻塞人工干预等基础设施。

Result: 该框架将自动化研究从孤立的单次尝试转变为持续的研究程序，能够系统性地基于先前探索并整合人类反馈。

Conclusion: 通过提供构建可定制共同科学家系统的架构原则和实际实现，促进自动化研究在科学领域的更广泛采用，使从业者能够部署交互式多智能体系统来自主进行端到端研究。

Abstract: The automation of scientific discovery represents a critical milestone in
Artificial Intelligence (AI) research. However, existing agentic systems for
science suffer from two fundamental limitations: rigid, pre-programmed
workflows that cannot adapt to intermediate findings, and inadequate context
management that hinders long-horizon research. We present
\texttt{freephdlabor}, an open-source multiagent framework featuring
\textit{fully dynamic workflows} determined by real-time agent reasoning and a
\coloremph{\textit{modular architecture}} enabling seamless customization --
users can modify, add, or remove agents to address domain-specific
requirements. The framework provides comprehensive infrastructure including
\textit{automatic context compaction}, \textit{workspace-based communication}
to prevent information degradation, \textit{memory persistence} across
sessions, and \textit{non-blocking human intervention} mechanisms. These
features collectively transform automated research from isolated, single-run
attempts into \textit{continual research programs} that build systematically on
prior explorations and incorporate human feedback. By providing both the
architectural principles and practical implementation for building customizable
co-scientist systems, this work aims to facilitate broader adoption of
automated research across scientific domains, enabling practitioners to deploy
interactive multiagent systems that autonomously conduct end-to-end research --
from ideation through experimentation to publication-ready manuscripts.

</details>


### [21] [AURA: An Agent Autonomy Risk Assessment Framework](https://arxiv.org/abs/2510.15739)
*Lorenzo Satta Chiris,Ayush Mishra*

Main category: cs.AI

TL;DR: 提出了AURA框架，用于检测、量化和减轻自主AI代理系统中的风险，通过gamma风险评分方法平衡评估准确性和计算效率，支持人机协同监督。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理系统在组织中的广泛应用，对齐、治理和风险管理方面的持续挑战阻碍了大规模部署，需要统一的框架来应对这些风险。

Method: AURA采用基于gamma的风险评分方法，提供交互式流程来评分、评估和减轻AI代理风险，支持人机协同监督和代理到人通信机制，与现有协议和工具兼容。

Result: AURA框架能够有效检测和减轻自主AI代理系统的风险，在保持计算效率的同时提供强大的风险检测能力。

Conclusion: AURA支持负责任和透明的AI代理采用，为企业在规模化、可治理的AI代理部署中提供了关键支撑。

Abstract: As autonomous agentic AI systems see increasing adoption across
organisations, persistent challenges in alignment, governance, and risk
management threaten to impede deployment at scale. We present AURA (Agent
aUtonomy Risk Assessment), a unified framework designed to detect, quantify,
and mitigate risks arising from agentic AI. Building on recent research and
practical deployments, AURA introduces a gamma-based risk scoring methodology
that balances risk assessment accuracy with computational efficiency and
practical considerations. AURA provides an interactive process to score,
evaluate and mitigate the risks of running one or multiple AI Agents,
synchronously or asynchronously (autonomously). The framework is engineered for
Human-in-the-Loop (HITL) oversight and presents Agent-to-Human (A2H)
communication mechanisms, allowing for seamless integration with agentic
systems for autonomous self-assessment, rendering it interoperable with
established protocols (MCP and A2A) and tools. AURA supports a responsible and
transparent adoption of agentic AI and provides robust risk detection and
mitigation while balancing computational resources, positioning it as a
critical enabler for large-scale, governable agentic AI in enterprise
environments.

</details>


### [22] [PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold](https://arxiv.org/abs/2510.15862)
*Yi Wan,Jiuqi Wang,Liam Li,Jinsong Liu,Ruihao Zhu,Zheqing Zhu*

Main category: cs.AI

TL;DR: PokeeResearch-7B是一个7B参数的深度研究代理，通过统一的强化学习框架构建，在10个流行深度研究基准测试中达到7B规模代理的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于工具增强的大型语言模型的研究代理存在检索浅层、对齐指标弱和工具使用行为脆弱等问题，需要开发更鲁棒、对齐良好且可扩展的深度研究代理。

Method: 采用无标注的AI反馈强化学习(RLAIF)框架训练，使用基于LLM的奖励信号优化策略，并引入思维链驱动的多调用推理框架增强鲁棒性。

Result: 在10个流行深度研究基准测试中，PokeeResearch-7B在7B规模代理中达到最先进性能。

Conclusion: 精心设计的强化学习和推理框架可以产生高效、有弹性且达到研究级别的AI代理。

Abstract: Tool-augmented large language models (LLMs) are emerging as deep research
agents, systems that decompose complex queries, retrieve external evidence, and
synthesize grounded responses. Yet current agents remain limited by shallow
retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce
PokeeResearch-7B, a 7B-parameter deep research agent built under a unified
reinforcement learning framework for robustness, alignment, and scalability.
PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from
AI Feedback (RLAIF) framework to optimize policies using LLM-based reward
signals that capture factual accuracy, citation faithfulness, and instruction
adherence. A chain-of-thought-driven multi-call reasoning scaffold further
enhances robustness through self-verification and adaptive recovery from tool
failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves
state-of-the-art performance among 7B-scale deep research agents. This
highlights that careful reinforcement learning and reasoning design can produce
efficient, resilient, and research-grade AI agents. The model and inference
code is open-sourced under MIT license at
https://github.com/Pokee-AI/PokeeResearchOSS.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [ES-C51: Expected Sarsa Based C51 Distributional Reinforcement Learning Algorithm](https://arxiv.org/abs/2510.15006)
*Rijul Tandon,Peter Vamplew,Cameron Foale*

Main category: cs.LG

TL;DR: 本文提出了一种改进的C51分布强化学习算法(ES-C51)，用Expected Sarsa更新替代贪婪Q学习更新，通过softmax计算结合所有可能动作的信息，解决了当动作具有相似期望奖励但不同分布时的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于值的RL算法只估计期望奖励，而分布强化学习估计整个奖励概率分布，提供更丰富的不确定性和变异性信息。但C51算法的贪婪Q学习更新在多个动作具有相似期望奖励但不同分布时会导致不稳定。

Method: 提出ES-C51算法，用Expected Sarsa更新替代贪婪Q学习更新，使用softmax计算结合所有可能动作的信息。为公平比较，将标准C51的探索策略从ε-greedy改为softmax，称为QL-C51。

Result: 在Gym经典控制环境和Atari-10游戏上的评估显示，ES-C51在多数环境中优于QL-C51。

Conclusion: ES-C51通过Expected Sarsa更新有效解决了C51算法在相似期望奖励动作下的不稳定性问题，能够学习到更高性能的策略。

Abstract: In most value-based reinforcement learning (RL) algorithms, the agent
estimates only the expected reward for each action and selects the action with
the highest reward. In contrast, Distributional Reinforcement Learning (DRL)
estimates the entire probability distribution of possible rewards, providing
richer information about uncertainty and variability. C51 is a popular DRL
algorithm for discrete action spaces. It uses a Q-learning approach, where the
distribution is learned using a greedy Bellman update. However, this can cause
problems if multiple actions at a state have similar expected reward but with
different distributions, as the algorithm may not learn a stable distribution.
This study presents a modified version of C51 (ES-C51) that replaces the greedy
Q-learning update with an Expected Sarsa update, which uses a softmax
calculation to combine information from all possible actions at a state rather
than relying on a single best action. This reduces instability when actions
have similar expected rewards and allows the agent to learn higher-performing
policies. This approach is evaluated on classic control environments from Gym,
and Atari-10 games. For a fair comparison, we modify the standard C51's
exploration strategy from e-greedy to softmax, which we refer to as QL-C51 (Q-
Learning based C51). The results demonstrate that ES-C51 outperforms QL-C51
across many environments.

</details>


### [24] [Internalizing World Models via Self-Play Finetuning for Agentic RL](https://arxiv.org/abs/2510.15047)
*Shiqi Chen,Tongyao Zhu,Zian Wang,Jinghan Zhang,Kangrui Wang,Siyang Gao,Teng Xiao,Yee Whye Teh,Junxian He,Manling Li*

Main category: cs.LG

TL;DR: 提出SPA框架，通过自监督微调学习世界模型，然后用于策略优化，显著提升LLM智能体在OOD场景下的性能


<details>
  <summary>Details</summary>
Motivation: LLM智能体在分布外场景中表现不佳，难以将内部知识与环境动态对齐，传统RL训练效果有限

Method: 将世界模型分解为状态表示和转移建模，通过自监督微调阶段学习世界模型，然后在策略优化前模拟未来状态

Result: 在Sokoban、FrozenLake和Sudoku等环境中显著提升性能，如Sokoban成功率从25.6%提升到59.8%

Conclusion: 为LLM智能体配备内部世界模型可以有效改善推理与环境动态的对齐，提升决策能力

Abstract: Large Language Models (LLMs) as agents often struggle in out-of-distribution
(OOD) scenarios. Real-world environments are complex and dynamic, governed by
task-specific rules and stochasticity, which makes it difficult for LLMs to
ground their internal knowledge in those dynamics. Under such OOD conditions,
vanilla RL training often fails to scale; we observe Pass@k--the probability
that at least one of (k) sampled trajectories succeeds--drops markedly across
training steps, indicating brittle exploration and limited generalization.
Inspired by model-based reinforcement learning, we hypothesize that equipping
LLM agents with an internal world model can better align reasoning with
environmental dynamics and improve decision-making. We show how to encode this
world model by decomposing it into two components: state representation and
transition modeling. Building on this, we introduce SPA, a simple reinforcement
learning framework that cold-starts the policy via a Self-Play supervised
finetuning (SFT) stage to learn the world model by interacting with the
environment, then uses it to simulate future states prior to policy
optimization. This simple initialization outperforms the online world-modeling
baseline and greatly boosts the RL-based agent training performance.
Experiments across diverse environments like Sokoban, FrozenLake, and Sudoku
show that our approach significantly improves performance. For example, SPA
boosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake
score from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.

</details>


### [25] [Learn to Change the World: Multi-level Reinforcement Learning with Model-Changing Actions](https://arxiv.org/abs/2510.15056)
*Ziqing Lu,Babak Hassibi,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: 论文提出了多层级可配置时变马尔可夫决策过程（MCTVMDP），其中智能体不仅可以通过底层动作与环境互动，还能通过上层动作主动修改环境动态模型本身。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习假设环境固定不变，但实际中智能体可能通过主动改变环境动态来获得更高奖励。本文旨在研究这种能够主动配置环境的智能体。

Method: 引入MCTVMDP框架，包含上层配置策略和下层原始动作策略两个层级。上层动作可以修改底层MDP的非平稳转移函数，智能体需要联合优化两个层级的策略。

Result: 提出了一个能够处理环境动态主动配置的强化学习框架，扩展了传统MDP的建模能力。

Conclusion: MCTVMDP为研究能够主动改变环境动态的智能体提供了理论基础，突破了传统强化学习中环境固定的假设。

Abstract: Reinforcement learning usually assumes a given or sometimes even fixed
environment in which an agent seeks an optimal policy to maximize its long-term
discounted reward. In contrast, we consider agents that are not limited to
passive adaptations: they instead have model-changing actions that actively
modify the RL model of world dynamics itself. Reconfiguring the underlying
transition processes can potentially increase the agents' rewards. Motivated by
this setting, we introduce the multi-layer configurable time-varying Markov
decision process (MCTVMDP). In an MCTVMDP, the lower-level MDP has a
non-stationary transition function that is configurable through upper-level
model-changing actions. The agent's objective consists of two parts: Optimize
the configuration policies in the upper-level MDP and optimize the primitive
action policies in the lower-level MDP to jointly improve its expected
long-term reward.

</details>


### [26] [DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning](https://arxiv.org/abs/2510.15110)
*Shih-Yang Liu,Xin Dong,Ximing Lu,Shizhe Diao,Mingjie Liu,Min-Hung Chen,Hongxu Yin,Yu-Chiang Frank Wang,Kwang-Ting Cheng,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: DLER通过改进强化学习优化方法，使用简单的截断长度惩罚，在保持准确性的同时大幅减少语言模型输出长度，实现了最优的准确率-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 当前推理语言模型虽然性能强大，但输出过于冗长，需要在保持准确性的同时提高输出效率，最大化每个token的智能度。

Method: 提出DLER训练方法，结合批量奖励归一化、更高裁剪、动态采样和简单截断长度惩罚，解决RL优化中的优势估计偏差、熵崩溃和稀疏奖励信号问题。

Result: DLER将输出长度减少70%以上，同时超越所有基线模型的准确率。DLER-7B相比DeepSeek-R1-7B准确率提高28%，延迟更低。

Conclusion: DLER证明了通过改进RL优化而非复杂惩罚机制，可以实现高效的准确率-效率平衡，并提出了难度感知DLER和选择性合并方法进一步优化性能。

Abstract: Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve
strong performance via extended chains of thought but often generate
unnecessarily long outputs. Maximizing intelligence per token--accuracy
relative to response length--remains an open problem. We revisit reinforcement
learning (RL) with the simplest length penalty--truncation--and show that
accuracy degradation arises not from the lack of sophisticated penalties but
from inadequate RL optimization. We identify three key challenges: (i) large
bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward
signal. We address them with Doing Length pEnalty Right (DLER), a training
recipe combining batch-wise reward normalization, higher clipping, dynamic
sampling, and a simple truncation length penalty. DLER achieves
state-of-the-art accuracy--efficiency trade-offs, cutting output length by over
70 percent while surpassing all previous baseline accuracy. It also improves
test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple
concise responses in parallel with 28 percent higher accuracy and lower
latency. We further introduce Difficulty-Aware DLER, which adaptively tightens
truncation on easier questions for additional efficiency gains. We also propose
an update-selective merging method that preserves baseline accuracy while
retaining the concise reasoning ability of the DLER model, which is useful for
scenarios where RL training data is scarce.

</details>


### [27] [Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization](https://arxiv.org/abs/2510.15165)
*Xin Guo,Zijiu Lyu*

Main category: cs.LG

TL;DR: 本文首次为连续时间强化学习提供了策略迁移的理论证明，证明了在连续时间线性二次调节器（LQR）中，从一个任务的最优策略可以作为相关任务的接近最优初始化，同时保持原始算法的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂任务上从头训练效率低下，而迁移学习在大语言模型中已证明成功。本文旨在研究如何将预训练模型应用于连续时间RL以提高效率。

Method: 研究策略迁移方法，在熵正则化的连续时间LQR框架下，提出新的策略学习算法，实现全局线性和局部超线性收敛。

Result: 证明了从一个LQR任务的最优策略可以作为相关LQR任务的接近最优初始化，并保持收敛速率。算法在连续时间LQR中实现了全局线性和局部超线性收敛。

Conclusion: 研究填补了连续时间RL中迁移学习的理论空白，将先前工作从离散时间扩展到连续时间设置，并建立了连续时间分数扩散模型与LQR的稳定性联系。

Abstract: Reinforcement Learning (RL) enables agents to learn optimal decision-making
strategies through interaction with an environment, yet training from scratch
on complex tasks can be highly inefficient. Transfer learning (TL), widely
successful in large language models (LLMs), offers a promising direction for
enhancing RL efficiency by leveraging pre-trained models.
  This paper investigates policy transfer, a TL approach that initializes
learning in a target RL task using a policy from a related source task, in the
context of continuous-time linear quadratic regulators (LQRs) with entropy
regularization. We provide the first theoretical proof of policy transfer for
continuous-time RL, proving that a policy optimal for one LQR serves as a
near-optimal initialization for closely related LQRs, while preserving the
original algorithm's convergence rate. Furthermore, we introduce a novel policy
learning algorithm for continuous-time LQRs that achieves global linear and
local super-linear convergence. Our results demonstrate both theoretical
guarantees and algorithmic benefits of transfer learning in continuous-time RL,
addressing a gap in existing literature and extending prior work from discrete
to continuous time settings.
  As a byproduct of our analysis, we derive the stability of a class of
continuous-time score-based diffusion models via their connection with LQRs.

</details>


### [28] [Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential](https://arxiv.org/abs/2510.15216)
*Xuansheng Wu,Xiaoman Pan,Wenlin Yao,Jianshu Chen*

Main category: cs.LG

TL;DR: 研究发现预训练模型在可验证奖励强化学习(RLVR)后表现差异的微观原因：高性能模型具有内在的健全性感知能力，能区分严格规则与噪声规则的概率分布，而弱模型则无法区分。


<details>
  <summary>Details</summary>
Motivation: 探究为什么不同基础模型在RLVR后的推理性能存在显著差异，寻找预训练模型的微观特性与推理能力的关系。

Method: 将推理形式化为Horn子句链，使用跨层稀疏自编码器提取特征，估计特征间转移概率，并用LLM对规则的语义健全性水平进行分类。

Result: 发现高性能模型具有健全性感知能力，其内部概率分布在不同的健全性水平上系统性地变化，而弱模型则无法区分。提出的SAL指标与RLVR后推理性能呈强相关(R^2=0.87)。

Conclusion: 模型的推理潜力与其内在区分健全知识与不健全知识的能力相关，预训练在塑造推理能力中起关键作用。

Abstract: Reinforcement learning with verifiable rewards (RLVR) can elicit strong
reasoning in large language models (LLMs), while their performance after RLVR
varies dramatically across different base models. This raises a fundamental
question: what microscopic property of pre-trained models leads to this
variation? To investigate, we formalize reasoning as chains of Horn clauses
("if-then" rules) built from features extracted from the LLM's latent space via
cross-layer sparse autoencoders (SAEs). We estimate the transition
probabilities between its features, and further categorize each rule by its
semantic soundness level (e.g., strict, plausible, noisy) with an LLM. Our key
discovery is that high-potential models are inherently soundness-aware: their
internal probability distributions systematically shift across rules' soundness
levels, becoming highly distinct for "strict" versus "noisy" rules. In
contrast, weaker models are soundness-agnostic, collapsing to one distribution
regardless of soundness levels. To quantify this, we introduce the
Soundness-Aware Level (SAL), a microscopic metric using the Jensen-Shannon
Divergence to measure the separation between these distributions. We show that
SAL's predictions of post-RLVR reasoning performance follow a precise empirical
law (R^2=0.87) across diverse model families (Qwen, Mistral, Llama, DeepSeek)
and scales (0.5B-14B). This reveals that a model's reasoning potential is tied
to its intrinsic, pre-trained ability to distinguish sound knowledge from
unsound ones. These findings underscore the critical role of model pre-training
in shaping reasoning and offer a practical metric grounded in the model's
internal mechanisms for selecting/designing stronger base models.

</details>


### [29] [Dual-Weighted Reinforcement Learning for Generative Preference Modeling](https://arxiv.org/abs/2510.15242)
*Shengyu Feng,Yun He,Shuang Ma,Beibin Li,Yuanhao Xiong,Vincent Li,Karishma Mandyam,Julian Katz-Samuels,Shengjie Bi,Licheng Yu,Hejia Zhang,Karthik Abinav Sankararaman,Han Fang,Riham Mansour,Yiming Yang,Manaal Faruqui*

Main category: cs.LG

TL;DR: 提出了双权重强化学习(DWRL)框架，将思维链推理与Bradley-Terry偏好模型结合，通过双权重RL目标解决非可验证任务的偏好建模问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习主要应用于可验证答案的任务，但在更普遍的非可验证任务（如人类偏好对）上的应用仍然具有挑战性且研究不足。

Method: DWRL框架通过实例级错位权重和组级条件偏好分数两个互补权重，近似Bradley-Terry模型的最大似然目标，训练生成式偏好模型(GPMs)首先生成思维然后预测人类偏好分数。

Result: 在多个基准测试和模型规模(Llama3和Qwen2.5)上，DWRL持续优于GPM基线和标量模型，同时产生连贯、可解释的思维。

Conclusion: DWRL作为推理增强偏好学习的通用框架，可扩展到可验证任务之外。

Abstract: Reinforcement learning (RL) has recently proven effective at scaling
chain-of-thought (CoT) reasoning in large language models on tasks with
verifiable answers. However, extending RL to more general non-verifiable tasks,
typically in the format of human preference pairs, remains both challenging and
underexplored. In this work, we propose Dual-Weighted Reinforcement Learning
(DWRL), a new framework for preference modeling that integrates CoT reasoning
with the Bradley-Terry (BT) model via a dual-weighted RL objective that
preserves preference-modeling inductive bias. DWRL approximates the
maximum-likelihood objective of the BT model with two complementary weights: an
instance-wise misalignment weight, which emphasizes under-trained pairs
misaligned with human preference, and a group-wise (self-normalized)
conditional preference score, which promotes promising thoughts. In this paper,
we apply DWRL to preference modeling by training generative preference models
(GPMs) to first generate a thought and then predict the human preference score.
Across multiple benchmarks and model scales (Llama3 and Qwen2.5), DWRL
consistently outperforms both GPM baselines and scalar models, while producing
coherent, interpretable thoughts. In summary, our results position DWRL as a
general framework for reasoning-enhanced preference learning beyond verifiable
tasks.

</details>


### [30] [DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models](https://arxiv.org/abs/2510.15260)
*Yangyang Li*

Main category: cs.LG

TL;DR: DRO-InstructZero通过分布鲁棒优化改进零样本提示优化，在分布偏移下显著提升性能，同时保持查询效率。


<details>
  <summary>Details</summary>
Motivation: 传统提示搜索方法在分布偏移和对抗评估中表现不佳，因为它们仅优化单一评估分布下的期望性能，导致提示在不同设置间难以迁移。

Method: 将零样本提示优化建模为鲁棒贝叶斯优化，使用f-散度球定义评估分布周围的模糊集，通过鲁棒采集规则最大化最坏情况期望效用。

Result: 在形式化改写、代码调试和翻译任务中，准确率显著提升，如BIG-Bench形式化改写从61.3%提升至85-90%，代码调试在域偏移下提升约25个百分点。

Conclusion: DRO-InstructZero将分布鲁棒优化与提示学习相结合，为现实世界不确定性下的可靠、可迁移提示对齐提供了即插即用的通用方法。

Abstract: Large language models are highly sensitive to prompt wording. However,
popular automatic prompt search methods, including InstructZero, often degrade
under distribution shift and adversarial evaluation because they optimize
expected performance under a single evaluation distribution. Consequently,
prompts that work in one setting frequently fail to transfer. To address this,
DRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian
optimization. Specifically, an f-divergence ball defines an ambiguity set
around the evaluation distribution, and a robust acquisition rule maximizes
worst-case expected utility while retaining the query efficiency of Bayesian
search. Therefore, the search explicitly targets reliability under distribution
shift rather than average behavior alone. Experiments follow the
instruction-induction protocol with matched query budgets across formality
rewriting, code debugging, and translation. For example, on BIG-Bench
informative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to
approximately 85-90%, yielding an absolute gain of about 25-30 points.
Moreover, auto-debugging shows about +25-point gains under domain shift.
Meanwhile, stable tasks such as cause-and-effect remain above 96%, indicating
no loss on in-distribution cases. Furthermore, improvements are consistent
across divergence choices and decoding temperatures. Overall, DRO-InstructZero
connects distributionally robust optimization with prompt learning, offering a
plug-and-play and general approach for reliable, transferable prompt alignment
under real-world uncertainty.

</details>


### [31] [Towards Robust Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.15382)
*Kexin Zheng,Lauriane Teyssier,Yinan Zheng,Yu Luo,Xiayuan Zhan*

Main category: cs.LG

TL;DR: 提出了BREEZE框架，通过行为正则化、扩散模型策略提取和注意力架构，解决了零样本强化学习中表示学习表达性不足和OOD动作外推误差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有零样本RL方法（如FB表示）存在表示表达性不足和离线学习中的OOD动作外推误差问题，导致次优性能。

Method: BREEZE框架包含三个关键组件：行为正则化将策略优化转化为稳定样本内学习；任务条件扩散模型提取高质量多模态策略；注意力架构增强表示建模能力。

Result: 在ExORL和D4RL Kitchen基准测试中，BREEZE达到最佳或接近最佳性能，并表现出优于现有离线零样本RL方法的鲁棒性。

Conclusion: BREEZE通过增强学习稳定性、策略提取能力和表示学习质量，显著提升了零样本强化学习的性能。

Abstract: The recent development of zero-shot reinforcement learning (RL) has opened a
new avenue for learning pre-trained generalist policies that can adapt to
arbitrary new tasks in a zero-shot manner. While the popular Forward-Backward
representations (FB) and related methods have shown promise in zero-shot RL, we
empirically found that their modeling lacks expressivity and that extrapolation
errors caused by out-of-distribution (OOD) actions during offline learning
sometimes lead to biased representations, ultimately resulting in suboptimal
performance. To address these issues, we propose Behavior-REgularizEd Zero-shot
RL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that
simultaneously enhances learning stability, policy extraction capability, and
representation learning quality. BREEZE introduces behavioral regularization in
zero-shot RL policy learning, transforming policy optimization into a stable
in-sample learning paradigm. Additionally, BREEZE extracts the policy using a
task-conditioned diffusion model, enabling the generation of high-quality and
multimodal action distributions in zero-shot RL settings. Moreover, BREEZE
employs expressive attention-based architectures for representation modeling to
capture the complex relationships between environmental dynamics. Extensive
experiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best
or near-the-best performance while exhibiting superior robustness compared to
prior offline zero-shot RL methods. The official implementation is available
at: https://github.com/Whiterrrrr/BREEZE.

</details>


### [32] [Iterative Refinement of Flow Policies in Probability Space for Online Reinforcement Learning](https://arxiv.org/abs/2510.15388)
*Mingyang Sun,Pengxiang Ding,Weinan Zhang,Donglin Wang*

Main category: cs.LG

TL;DR: 提出了Stepwise Flow Policy (SWFP)框架，通过离散化流匹配推理过程，将其与最优传输的JKO原理对齐，实现了预训练流模型的高效稳定微调。


<details>
  <summary>Details</summary>
Motivation: 行为克隆中的流/扩散策略虽然擅长从演示中学习复杂技能，但对分布偏移很脆弱，标准RL方法难以微调这些模型。

Method: 使用固定步长欧拉方案离散化流匹配推理过程，将其分解为一系列小增量变换，每个步骤对应JKO更新，通过熵正则化确保稳定在线适应。

Result: 在多种机器人控制基准测试中，SWFP展现出增强的稳定性、效率和优越的适应性能。

Conclusion: SWFP框架通过将全局流分解为小流块序列，提供了更简单快速的子模型训练、降低的计算/内存成本，以及基于Wasserstein信任区域的可证明稳定性。

Abstract: While behavior cloning with flow/diffusion policies excels at learning
complex skills from demonstrations, it remains vulnerable to distributional
shift, and standard RL methods struggle to fine-tune these models due to their
iterative inference process and the limitations of existing workarounds. In
this work, we introduce the Stepwise Flow Policy (SWFP) framework, founded on
the key insight that discretizing the flow matching inference process via a
fixed-step Euler scheme inherently aligns it with the variational
Jordan-Kinderlehrer-Otto (JKO) principle from optimal transport. SWFP
decomposes the global flow into a sequence of small, incremental
transformations between proximate distributions. Each step corresponds to a JKO
update, regularizing policy changes to stay near the previous iterate and
ensuring stable online adaptation with entropic regularization. This
decomposition yields an efficient algorithm that fine-tunes pre-trained flows
via a cascade of small flow blocks, offering significant advantages:
simpler/faster training of sub-models, reduced computational/memory costs, and
provable stability grounded in Wasserstein trust regions. Comprehensive
experiments demonstrate SWFP's enhanced stability, efficiency, and superior
adaptation performance across diverse robotic control benchmarks.

</details>


### [33] [Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models](https://arxiv.org/abs/2510.15429)
*Shashank Gupta*

Main category: cs.LG

TL;DR: 该论文通过上下文多臂赌博机框架，研究了强化学习在排名推荐和文本到图像生成中的安全、样本高效和鲁棒性问题，提出了安全部署理论、最优基线校正方法和LOOP算法。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在现实应用中的安全性、样本效率和鲁棒性问题，特别是在排名推荐系统和文本到图像生成模型中的实际部署挑战。

Method: 1. 排名系统：推导曝光泛化界，提出反事实风险最小化目标；2. 单臂赌博机：统一离策略估计器，提出最优基线校正；3. 生成RL：系统研究PPO和REINFORCE，提出LOOP算法结合多扩散轨迹。

Result: 1. 安全部署保证不劣于日志策略；2. 最优基线最小化方差；3. LOOP实现PPO级样本效率，同时生成更忠实于文本属性的图像。

Conclusion: 通过上下文多臂赌博机框架，为强化学习在排名推荐和生成模型中的安全高效部署提供了理论保证和实用算法。

Abstract: This dissertation investigates how reinforcement learning (RL) methods can be
designed to be safe, sample-efficient, and robust. Framed through the unifying
perspective of contextual-bandit RL, the work addresses two major application
domains - ranking and recommendation, and text-to-image diffusion models. The
first part of the thesis develops theory and algorithms for safe deployment in
ranking systems. An exposure-based generalisation bound is derived, leading to
a counterfactual risk-minimisation objective whose solution is guaranteed not
to underperform the logging policy, even with sparse feedback. This guarantee
is extended to doubly robust estimators, enabling safety even under adversarial
or misspecified user models and offering practitioners explicit control over
permissible utility loss. The second part turns to single-action bandits, where
various off-policy estimators are unified within a baseline-correction
framework. A closed-form optimal baseline is proposed and shown to minimise
both evaluation and policy-gradient variance, thereby improving off-policy
learning reliability. The final part examines the trade-offs between efficiency
and effectiveness in generative RL. A systematic study of PPO and REINFORCE
motivates the Leave-One-Out PPO (LOOP) algorithm, which combines multiple
diffusion trajectories with a REINFORCE-style baseline inside PPO's clipped
objective. LOOP achieves PPO-level sample efficiency while producing
generations that align more faithfully with textual attributes.

</details>


### [34] [A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning](https://arxiv.org/abs/2510.15444)
*Zhi Zhou,Yuhao Tan,Zenan Li,Yuan Yao,Lan-Zhe Guo,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.LG

TL;DR: 提出了首个基于置信度估计的理论框架来分析采样型测试时扩展方法，揭示了自一致性和困惑度方法的局限性，并提出了结合两者优势的RPC混合方法。


<details>
  <summary>Details</summary>
Motivation: 采样型测试时扩展方法在实践中成功提升了LLM的推理性能，但其理论基础尚未充分探索，需要建立理论框架来分析现有方法的局限性。

Method: 提出了RPC方法，包含两个关键组件：困惑度一致性（结合自一致性和困惑度优势）和推理剪枝（消除低概率推理路径）。

Result: 在七个基准数据集上的实证结果表明，RPC在保持与自一致性相当推理性能的同时，提高了置信度可靠性，并将采样成本降低了50%。

Conclusion: RPC方法通过理论指导的混合策略有效解决了现有方法的局限性，在减少推理错误方面具有强大潜力。

Abstract: Test-time scaling seeks to improve the reasoning performance of large
language models (LLMs) by adding computational resources. A prevalent approach
within the field is sampling-based test-time scaling methods, which enhance
reasoning by generating multiple reasoning paths for a given input during
inference. However, despite its practical success, the theoretical foundations
remain underexplored. In this paper, we provide the first theoretical framework
for analyzing sampling-based test-time scaling methods, grounded in the
perspective of confidence estimation. Based on the framework, we analyze two
dominant paradigms: self-consistency and perplexity, and reveal key
limitations: self-consistency suffers from high estimation error while
perplexity exhibits substantial modeling error and possible degradation of the
estimation error convergence. To address these limitations, we introduce RPC, a
hybrid method that leverages our theoretical insights through two key
components: Perplexity Consistency and Reasoning Pruning. Perplexity
Consistency combines the strengths of self-consistency and perplexity, boosting
the convergence rate of estimation error from linear to exponential while
preserving model error. Reasoning Pruning prevents degradation by eliminating
low-probability reasoning paths. Both theoretical analysis and empirical
results across seven benchmark datasets demonstrate that RPC has a strong
potential for reducing reasoning error. Notably, RPC achieves reasoning
performance comparable to self-consistency while not only enhancing confidence
reliability but also reducing sampling costs by 50%. The code and resources are
available at https://wnjxyk.github.io/RPC.

</details>


### [35] [Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment](https://arxiv.org/abs/2510.15456)
*Jan Corazza,Hadi Partovi Aria,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 提出了一种将时序逻辑因果图融入概率奖励机的方法，以加速策略学习并帮助任务规范迁移到新环境。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法在处理奖励反馈稀疏且依赖于环境中复杂事件序列的任务时存在困难，而概率奖励机虽然能捕捉奖励信号中的时序依赖关系，但难以手动修改和设计。

Method: 将时序逻辑因果图形式化的因果信息整合到奖励形式化中，提供理论收敛保证。

Result: 实验证明该方法在策略学习和任务规范迁移方面具有优势。

Conclusion: 该方法能有效利用高层因果知识，加速策略学习并促进任务规范在不同环境间的迁移。

Abstract: Reinforcement learning (RL) algorithms struggle with learning optimal
policies for tasks where reward feedback is sparse and depends on a complex
sequence of events in the environment. Probabilistic reward machines (PRMs) are
finite-state formalisms that can capture temporal dependencies in the reward
signal, along with nondeterministic task outcomes. While special RL algorithms
can exploit this finite-state structure to expedite learning, PRMs remain
difficult to modify and design by hand. This hinders the already difficult
tasks of utilizing high-level causal knowledge about the environment, and
transferring the reward formalism into a new domain with a different causal
structure. This paper proposes a novel method to incorporate causal information
in the form of Temporal Logic-based Causal Diagrams into the reward formalism,
thereby expediting policy learning and aiding the transfer of task
specifications to new environments. Furthermore, we provide a theoretical
result about convergence to optimal policy for our method, and demonstrate its
strengths empirically.

</details>


### [36] [The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling](https://arxiv.org/abs/2510.15502)
*Shijia Kang,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出SESA框架，通过顺序采样生成多样化解决方案草图，解决RL训练中探索不足和熵崩溃问题，在多个基准测试中显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型推理能力时存在探索有限和熵崩溃问题，模型倾向于利用狭窄的解决方案集，导致采样多样性丧失，阻碍性能进一步提升。

Method: SESA顺序采样框架：首先生成多样化的解决方案草图，然后将其扩展为完整的推理路径，通过条件化每个新输出于先前输出来确保更广泛的探索。

Result: 在合成任务中，顺序采样在路径多样性和从崩溃中恢复方面优于传统RL方法；在三个智能体基准测试中，成功率分别提升+0.25、+0.42和+0.07（相对于基线RL最高达211%的相对改进）。

Conclusion: SESA为探索提供了结构化方法，为RL训练的大语言模型实现更有效和多样化的推理铺平了道路。

Abstract: Reinforcement learning (RL) has been pivotal in enhancing the reasoning
capabilities of large language models (LLMs), but it often suffers from limited
exploration and entropy collapse, where models exploit a narrow set of
solutions, leading to a loss of sampling diversity and subsequently preventing
RL from further improving performance. This issue is exacerbated in parallel
sampling methods, where multiple outputs are drawn from the same distribution,
potentially causing the model to converge to similar solutions. We propose
SESA, a novel SEquential SAmpling framework that mitigates this challenge by
generating diverse solution sketches sequentially before expanding them into
full reasoning paths. This approach ensures broader exploration by conditioning
each new output on previous ones, promoting diversity throughout the process
and preventing policy collapse. Our experiments on a synthetic task show that
sequential sampling consistently outperforms traditional RL methods in terms of
path diversity and recovery from collapse. Further evaluations on real-world
tasks demonstrate that SESA improves both the exploration of valid strategies
and the overall performance of LLMs. On three agent benchmarks, SESA lifts
success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up
to an additional $211\%$ relative improvement over baseline RL), underscoring
its exploration advantage. This work introduces a structured approach to
exploration, paving the way for more effective and diverse reasoning in
RL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.

</details>


### [37] [Doubly Robust Estimation of Causal Effects in Strategic Equilibrium Systems](https://arxiv.org/abs/2510.15555)
*Sibo Xiao*

Main category: cs.LG

TL;DR: 提出战略双重稳健（SDR）估计器，将战略均衡建模与双重稳健估计相结合，用于战略环境中的因果推断。


<details>
  <summary>Details</summary>
Motivation: 解决由战略代理行为引起的内生处理分配问题，在保持双重稳健性的同时纳入战略考量。

Method: 集成战略均衡建模与双重稳健估计框架，在战略无混淆假设下进行理论分析。

Result: 实证评估显示SDR优于基线方法，偏差减少7.6%-29.3%，在不同战略强度和代理规模下保持稳健可扩展性。

Conclusion: 为代理对干预做出战略响应时的可靠因果推断提供了原则性方法。

Abstract: We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework
that integrates strategic equilibrium modeling with doubly robust estimation
for causal inference in strategic environments. SDR addresses endogenous
treatment assignment arising from strategic agent behavior, maintaining double
robustness while incorporating strategic considerations. Theoretical analysis
confirms SDR's consistency and asymptotic normality under strategic
unconfoundedness. Empirical evaluations demonstrate SDR's superior performance
over baseline methods, achieving 7.6\%-29.3\% bias reduction across varying
strategic strengths and maintaining robust scalability with agent populations.
The framework provides a principled approach for reliable causal inference when
agents respond strategically to interventions.

</details>


### [38] [CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning](https://arxiv.org/abs/2510.15674)
*Yung-Chen Tang,Pin-Yu Chen,Andrea Cavallaro*

Main category: cs.LG

TL;DR: 提出CarBoN校准框架，通过自适应调整温度T和偏移向量δ来改进推理任务中的测试时采样效率，无需重新训练LLM，在MATH-500和AIME-2024上实现高达4倍的效率提升。


<details>
  <summary>Details</summary>
Motivation: 解决传统Best-of-N采样方法在N增大时收益递减的问题，提高推理任务中测试时计算分配的效率。

Method: 两阶段方法：先探索解空间，然后学习输入特定的温度T和加性偏移向量δ来校准logits，引导生成更可靠的推理路径。

Result: 在MATH-500和AIME-2024上，CarBoN以最多4倍更少的采样次数达到相同准确率，且在固定预算下通常获得更高准确率。

Conclusion: CarBoN框架有效提高了测试时采样的效率，T和δ在平衡输出多样性和正确性方面发挥互补作用，该框架也适用于束搜索等步骤级采样策略。

Abstract: Allocating more computation during inference time (test-time scaling)
improves language model performance, especially for reasoning tasks. However,
popular methods like Best-of-$N$ sampling often show diminishing returns as $N$
increases. To address this inefficiency, we introduce a general test-time
calibration framework that adaptively modifies the model toward high-reward
reasoning paths, with theoretical guarantees of improving the lower bound of
expected reward under finite sampling, all without large language model (LLM)
retraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$),
a two-phase method that first explores the solution space and then learns a
calibration of the logits via an input-specific temperature $T$ and additive
shift vector $\delta$, guiding generation toward more reliable reasoning.
Experiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency,
with up to $4\times$ fewer rollouts to reach the same accuracy, while often
achieving higher accuracy under fixed budgets. We also analyze the
complementary roles of $T$ and $\delta$ in balancing output diversity and
correctness, and demonstrate that the framework also generalizes to step-level
sampling strategies such as beam search. For more information, please refer to
our project page at huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.

</details>


### [39] [ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations](https://arxiv.org/abs/2510.15700)
*Alex Gu,Bartosz Piotrowski,Fabian Gloeckle,Kaiyu Yang,Aram H. Markosyan*

Main category: cs.LG

TL;DR: ProofOptimizer是一个通过专家迭代和强化学习训练的模型，专门用于简化Lean证明，无需人工监督，能显著压缩证明长度并提高验证效率。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络证明生成的证明过于冗长，难以被人类理解，限制了数学洞察力的发展，而现有方法在处理RL训练证明器生成的长证明时效果不佳。

Method: 采用专家迭代和强化学习方法，利用Lean验证简化并提供训练信号，在推理时通过迭代证明缩短工作流程逐步减少证明长度。

Result: 在标准基准测试中，ProofOptimizer显著压缩了证明长度：miniF2F减少87%，PutnamBench减少57%，Seed-Prover的IMO 2025证明减少49%。

Conclusion: ProofOptimizer不仅能生成更简洁的证明，还能加速Lean验证过程，并且当作为监督微调的训练数据时能进一步提升下游证明器的性能。

Abstract: Neural theorem proving has advanced rapidly in the past year, reaching IMO
gold-medalist capabilities and producing formal proofs that span thousands of
lines. Although such proofs are mechanically verified by formal systems like
Lean, their excessive length renders them difficult for humans to comprehend
and limits their usefulness for mathematical insight. Proof simplification is
therefore a critical bottleneck. Yet, training data for this task is scarce,
and existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --
struggle with the extremely long proofs generated by RL-trained provers. We
introduce ProofOptimizer, the first language model trained to simplify Lean
proofs without requiring additional human supervision. ProofOptimizer is
trained via expert iteration and reinforcement learning, using Lean to verify
simplifications and provide training signal. At inference time, it operates
within an iterative proof-shortening workflow, progressively reducing proof
length. Experiments show that ProofOptimizer substantially compresses proofs
generated by state-of-the-art RL-trained provers on standard benchmarks,
reducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on
Seed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check
faster in Lean and further improve downstream prover performance when reused as
training data for supervised finetuning.

</details>


### [40] [ProSh: Probabilistic Shielding for Model-free Reinforcement Learning](https://arxiv.org/abs/2510.15720)
*Edwin Hamel-De le Court,Gaspard Ohlmann,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 提出Probabilistic Shielding via Risk Augmentation (ProSh)算法，一种基于模型无关的约束强化学习方法，通过风险预算增强状态空间，并使用成本评论器对策略分布施加屏蔽，确保所有采样动作在期望意义下保持安全。


<details>
  <summary>Details</summary>
Motivation: 强化学习系统的安全性是主要关注点，需要开发既能最优执行又能提供形式化安全保证的系统。

Method: ProSh算法通过风险预算增强约束MDP状态空间，使用学习到的成本评论器对策略分布施加屏蔽，确保采样动作在期望意义下安全。

Result: 在确定性环境下保持最优性，提供仅依赖于备份评论器准确度的成本期望上界，实验表明在温和且实际可实现的假设下，ProSh即使在训练时也能保证安全性。

Conclusion: ProSh是一种有效的模型无关安全强化学习算法，能够提供形式化的安全保证。

Abstract: Safety is a major concern in reinforcement learning (RL): we aim at
developing RL systems that not only perform optimally, but are also safe to
deploy by providing formal guarantees about their safety. To this end, we
introduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free
algorithm for safe reinforcement learning under cost constraints. ProSh
augments the Constrained MDP state space with a risk budget and enforces safety
by applying a shield to the agent's policy distribution using a learned cost
critic. The shield ensures that all sampled actions remain safe in expectation.
We also show that optimality is preserved when the environment is
deterministic. Since ProSh is model-free, safety during training depends on the
knowledge we have acquired about the environment. We provide a tight
upper-bound on the cost in expectation, depending only on the backup-critic
accuracy, that is always satisfied during training. Under mild, practically
achievable assumptions, ProSh guarantees safety even at training time, as shown
in the experiments.

</details>


### [41] [RLAF: Reinforcement Learning from Automaton Feedback](https://arxiv.org/abs/2510.15728)
*Mahyar Alinejad,Alvaro Velasquez,Yue Wang,George Atia*

Main category: cs.LG

TL;DR: 提出了一种基于自动机反馈的强化学习方法，用DFA生成的偏好替代显式奖励函数，无需手动设计奖励，在时序依赖任务中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在处理具有复杂历史依赖奖励结构的环境时面临挑战，需要解决手动奖励工程的问题。

Method: 使用确定性有限自动机(DFA)结构生成轨迹偏好来学习奖励函数，包含静态方法（直接使用学习到的奖励函数）和动态方法（迭代更新奖励函数和策略）。

Result: 在离散和连续环境中，该方法能够学习有效的时序依赖任务策略，优于传统奖励工程和基于自动机的基线方法。

Conclusion: 基于自动机的偏好方法在处理非马尔可夫奖励方面具有优势，提供了可扩展、高效且独立于人工的替代方案，并具有收敛性保证。

Abstract: Reinforcement Learning (RL) in environments with complex, history-dependent
reward structures poses significant challenges for traditional methods. In this
work, we introduce a novel approach that leverages automaton-based feedback to
guide the learning process, replacing explicit reward functions with
preferences derived from a deterministic finite automaton (DFA). Unlike
conventional approaches that use automata for direct reward specification, our
method employs the structure of the DFA to generate preferences over
trajectories that are used to learn a reward function, eliminating the need for
manual reward engineering. Our framework introduces a static approach that uses
the learned reward function directly for policy optimization and a dynamic
approach that involves continuous refining of the reward function and policy
through iterative updates until convergence.
  Our experiments in both discrete and continuous environments demonstrate that
our approach enables the RL agent to learn effective policies for tasks with
temporal dependencies, outperforming traditional reward engineering and
automaton-based baselines such as reward machines and LTL-guided methods. Our
results highlight the advantages of automaton-based preferences in handling
non-Markovian rewards, offering a scalable, efficient, and human-independent
alternative to traditional reward modeling. We also provide a convergence
guarantee showing that under standard assumptions our automaton-guided
preference-based framework learns a policy that is near-optimal with respect to
the true non-Markovian objective.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [42] [An Experimental Study of Real-Life LLM-Proposed Performance Improvements](https://arxiv.org/abs/2510.15494)
*Lirong Yi,Gregory Gay,Philipp Leitner*

Main category: cs.SE

TL;DR: 研究评估LLM生成代码的性能优化能力，发现LLM能在大多数情况下改进基线性能，但仍显著落后于人类开发者的优化水平。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否能生成高性能代码，而不仅仅是功能正确的代码。

Method: 使用65个真实Java任务数据集，采用自动化流水线用两种领先LLM生成补丁，并与基线和人类解决方案进行基准测试。

Result: LLM生成的代码在大多数情况下能改进性能，但人类开发者的补丁在统计上显著优于LLM修复。约三分之二情况下LLM解决方案与开发者优化思路相似，其余为原创思路但很少带来显著性能提升。

Conclusion: LLM在代码性能优化方面有潜力，但尚未达到人类专家的优化水平，原创思路的优化效果有限。

Abstract: Large Language Models (LLMs) can generate code, but can they generate fast
code? In this paper, we study this question using a dataset of 65 real-world
tasks mined from open-source Java programs. We specifically select tasks where
developers achieved significant speedups, and employ an automated pipeline to
generate patches for these issues using two leading LLMs under four prompt
variations. By rigorously benchmarking the results against the baseline and
human-authored solutions, we demonstrate that LLM-generated code indeed
improves performance over the baseline in most cases. However, patches proposed
by human developers outperform LLM fixes by a statistically significant margin,
indicating that LLMs often fall short of finding truly optimal solutions. We
further find that LLM solutions are semantically identical or similar to the
developer optimization idea in approximately two-thirds of cases, whereas they
propose a more original idea in the remaining one-third. However, these
original ideas only occasionally yield substantial performance gains.

</details>


### [43] [Enhancing Code Review through Fuzzing and Likely Invariants](https://arxiv.org/abs/2510.15512)
*Wachiraphan Charoenwet,Patanamon Thongtanunam,Van-Thuan Pham,Christoph Treude*

Main category: cs.SE

TL;DR: FuzzSight是一个利用模糊测试和不变式分析来检测代码行为变化的框架，能够在代码审查阶段发现潜在缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统代码审查主要依赖静态检查，难以发现动态行为问题。模糊测试虽然能产生丰富的行为数据，但缺乏合适的分析机制来帮助审查者识别非崩溃性行为变化。

Method: 通过模糊测试生成非崩溃输入，捕获程序运行时行为，将其表示为可能不变式，比较不同版本间行为差异来识别潜在问题。

Result: FuzzSight能标记75%的回归缺陷和高达80%的漏洞，相比SAST检测率提高10倍且误报更少。

Conclusion: FuzzSight展示了将模糊测试和不变式分析集成到早期代码审查中的潜力，能够桥接静态检查和动态行为洞察。

Abstract: Many software projects employ manual code review to gatekeep defects and
vulnerabilities in the code before integration. However, reviewers often work
under time pressure and rely primarily on static inspection, leaving the
dynamic aspects of the program unexplored. Dynamic analyses could reveal such
behaviors, but they are rarely integrated into reviews. Among them, fuzzing is
typically applied later to uncover crashing bugs. Yet its ability to exercise
code with diverse inputs makes it promising for exposing non-crashing, but
unexpected, behaviors earlier. Still, without suitable mechanisms to analyze
program behaviors, the rich data produced during fuzzing remains inaccessible
to reviewers, limiting its practical value in this context.
  We hypothesize that unexpected variations in program behaviors could signify
potential bugs. The impact of code changes can be automatically captured at
runtime. Representing program behavior as likely invariants, dynamic properties
consistently observed at specific program points, can provide practical signals
of behavioral changes. Such signals offer a way to distinguish between intended
changes and unexpected behavioral shifts from code changes.
  We present FuzzSight, a framework that leverages likely invariants from
non-crashing fuzzing inputs to highlight behavioral differences across program
versions. By surfacing such differences, it provides insights into which code
blocks may need closer attention. In our evaluation, FuzzSight flagged 75% of
regression bugs and up to 80% of vulnerabilities uncovered by 24-hour fuzzing.
It also outperformed SAST in identifying buggy code blocks, achieving ten times
higher detection rates with fewer false alarms. In summary, FuzzSight
demonstrates the potential and value of leveraging fuzzing and invariant
analysis for early-stage code review, bridging static inspection with dynamic
behavioral insights.

</details>


### [44] [Leveraging Test Driven Development with Large Language Models for Reliable and Verifiable Spreadsheet Code Generation: A Research Framework](https://arxiv.org/abs/2510.15585)
*Dr Simon Thorne,Dr Advait Sarkar*

Main category: cs.SE

TL;DR: 提出将测试驱动开发(TDD)与LLM生成结合的研究框架，通过"测试优先"方法提高LLM生成代码的正确性和可靠性，特别关注高风险领域如金融建模和科学计算。


<details>
  <summary>Details</summary>
Motivation: LLM在生成代码时经常出现幻觉、逻辑不一致和语法错误等问题，在金融建模和科学计算等高风险领域这些错误可能导致严重后果。需要提高LLM生成输出的正确性和可靠性。

Method: 提出将测试驱动开发(TDD)与LLM驱动的生成相结合的结构化研究框架，采用"测试优先"方法，包括明确的实验设计、参与者分组、评估指标和基于TDD的提示示例。

Result: 这是一个立场论文，提出了研究框架但尚未有实证结果。框架适用于电子表格公式生成、Python脚本和Rust等强类型语言。

Conclusion: 通过强调测试驱动思维，旨在提高计算思维、提示工程技能和用户参与度，特别有利于缺乏正式编程培训的电子表格用户。邀请合作来完善和实证评估该方法。

Abstract: Large Language Models (LLMs), such as ChatGPT, are increasingly leveraged for
generating both traditional software code and spreadsheet logic. Despite their
impressive generative capabilities, these models frequently exhibit critical
issues such as hallucinations, subtle logical inconsistencies, and syntactic
errors, risks particularly acute in high stakes domains like financial
modelling and scientific computations, where accuracy and reliability are
paramount. This position paper proposes a structured research framework that
integrates the proven software engineering practice of Test-Driven Development
(TDD) with Large Language Model (LLM) driven generation to enhance the
correctness of, reliability of, and user confidence in generated outputs. We
hypothesise that a "test first" methodology provides both technical constraints
and cognitive scaffolding, guiding LLM outputs towards more accurate,
verifiable, and comprehensible solutions. Our framework, applicable across
diverse programming contexts, from spreadsheet formula generation to scripting
languages such as Python and strongly typed languages like Rust, includes an
explicitly outlined experimental design with clearly defined participant
groups, evaluation metrics, and illustrative TDD based prompting examples. By
emphasising test driven thinking, we aim to improve computational thinking,
prompt engineering skills, and user engagement, particularly benefiting
spreadsheet users who often lack formal programming training yet face serious
consequences from logical errors. We invite collaboration to refine and
empirically evaluate this approach, ultimately aiming to establish responsible
and reliable LLM integration in both educational and professional development
practices.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [45] [Crush](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcharmbracelet%2Fcrush%3Futm_source=tldrdevops/1/01000199f1ea7e05-4111cb1a-7599-47e6-b9fb-d81292a069a3-000000/Y4cNOqHuRTNcb2y0ZogPl9BCM-0L8ov-qSQ7zXzXbpw=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Crush是一个新的AI编码代理，用于终端环境，能够集成到代码和工作流中，通过API密钥连接多个LLM提供商。


<details>
  <summary>Details</summary>
Motivation: 开发一个专门针对终端环境的AI编码代理，简化开发者在终端中的编码工作流程。

Method: 通过API密钥集成多个LLM提供商（Anthropic、OpenAI、Groq），并利用Catwalk开源数据库自动更新提供商信息。

Result: 创建了一个功能完整的终端AI编码代理工具。

Conclusion: Crush成功提供了一个终端环境下的AI编码助手解决方案。

Abstract: Crush (GitHub Repo) Crush, a new AI coding agent for terminals, integrates with your code and workflows, connecting to LLMs like Anthropic, OpenAI, and Groq using API keys. The tool also includes automatic provider updates from the Catwalk open source database.

</details>


### [46] [Coze Studio](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcoze-dev%2Fcoze-studio%3Futm_source=tldrdevops/1/01000199f1ea7e05-4111cb1a-7599-47e6-b9fb-d81292a069a3-000000/Vd7rJZvIQzjc_jyeXEqGYOCdc709-cUJVdFa2k2ka08=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Coze Studio是一个开源的AI智能体开发工具，源自Coze开发平台，旨在简化智能体的创建、调试和部署过程。


<details>
  <summary>Details</summary>
Motivation: 为了降低AI智能体开发的门槛，让开发者能够更轻松地创建、调试和部署智能体应用。

Method: 将Coze开发平台的核心引擎完全开源，提供一体化的智能体开发工具。

Result: 开发出了Coze Studio这一开源工具，使智能体开发更加简化和高效。

Conclusion: 开源Coze Studio工具能够有效促进AI智能体开发的大众化，降低开发门槛。

Abstract: Coze Studio (GitHub Repo) Coze Studio, an all-in-one AI agent development tool derived from the "Coze Development Platform," has made its core engine completely open to simplify agent creation, debugging, and deployment.

</details>


### [47] [Improving the trustworthiness of JavaScript on the Web](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fimproving-the-trustworthiness-of-javascript-on-the-web%2F%3Futm_source=tldrdevops/1/01000199f1ea7e05-4111cb1a-7599-47e6-b9fb-d81292a069a3-000000/ci_y47bUTuyBgXUmpEPuZn9sslv9bb6P0cLKfISYXvg=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: WAICT是一个W3C支持的项目，旨在通过浏览器内JavaScript加密为Web应用提供完整性、一致性和透明度，使用子资源完整性和完整性清单确保用户接收的代码与记录匹配。


<details>
  <summary>Details</summary>
Motivation: 解决JavaScript在Web上的信任问题，特别是代码在传输过程中被篡改的漏洞，增强Web应用的安全性。

Method: 采用子资源完整性(SRI)和完整性清单技术，通过密码学方法验证Web应用代码的完整性。

Result: 建立了一个能够确保Web应用代码完整性和一致性的系统框架。

Conclusion: WAICT为Web应用提供了更强的安全保障，通过技术手段解决了JavaScript代码信任问题。

Abstract: Improving the trustworthiness of JavaScript on the Web (20 minute read) Web Application Integrity, Consistency, and Transparency (WAICT), a W3C-backed effort, aims to bring stronger security to the web by providing integrity, consistency, and transparency for web applications using in-browser JavaScript cryptography. The system uses subresource integrity (SRI) and integrity manifests to ensure that the code a user receives matches the code on record, addressing the vulnerability of JavaScript...

</details>


### [48] [A stateful browser agent using self-healing DOM maps](https://tracking.tldrnewsletter.com/CL0/https:%2F%2F100x.bot%2Fa%2Fa-stateful-browser-agent-using-self-healing-dom-maps%3Futm_source=tldrwebdev/1/01000199f1ef0842-713397f7-8a62-41db-bb16-13ceb37aa53b-000000/XPHbLmto3EOJGtGwqKwSwSCBuCL_SaStaL7Kgy4kQ9o=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent4是一个有状态的浏览器代理，使用客户端自愈DOM映射来记住并即时执行重复任务，相比无状态的LLM驱动代理降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 解决无状态LLM代理在执行重复任务时的高延迟问题，通过保持状态记忆来提升执行效率。

Method: 采用客户端自愈DOM映射技术，使代理能够记住之前的操作和状态，实现即时任务执行。

Result: 相比无状态代理显著降低了任务执行延迟，提升了重复任务的执行效率。

Conclusion: 有状态的浏览器代理通过自愈DOM映射技术能够有效降低延迟，提高任务执行效率。

Abstract: A stateful browser agent using self-healing DOM maps (4 minute read) Agent4 is a stateful browser agent that uses a client-sourced, self-healing DOM map to remember and instantly execute recurring tasks, reducing latency compared to stateless LLM-powered agents.

</details>


### [49] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfounders%26utm_medium=newsletter%26utm_campaign=advertisecta/1/01000199f211aea3-a4a00278-fa90-4236-a4f4-d46fd641fc97-000000/48_mxZnur-0DUbDE899tPdWJgI7G8KFO-KdwjiSZshw=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Motion 融资6000万美元，估值5.5亿美元，用于为企业构建智能工作套件


<details>
  <summary>Details</summary>
Motivation: 为中小企业开发智能化的代理工作套件，提升企业工作效率和自动化水平

Method: 获得由Scale Venture Partners领投的6000万美元融资，用于产品开发和市场拓展

Result: 成功完成融资轮，公司估值达到5.5亿美元

Conclusion: Motion通过融资增强了其在企业智能工作套件领域的竞争力和发展潜力

Abstract: Motion Raises $60M at $550M Valuation to Build the Agentic Work Suite for Businesses (5 minute read) Motion raised $60M, led by Scale Venture Partners, to enhance its agentic work suite for SMBs at a valuation of $550M.

</details>


### [50] [Vibe Code with Confidence](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftraycer.ai%2F%3Futm_source=tldrdesign/1/01000199f2221837-d7e18ff4-e05f-406c-8523-90bab4c9dde5-000000/NphlPAd7eJq5VRvdPTUbX90APDFDfnbjjcGfsjsk3jE=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Traycer是一个扫描代码库的工具，用于验证AI生成的代码变更，确保不良代码不会进入生产环境


<details>
  <summary>Details</summary>
Motivation: 随着AI生成代码的普及，需要确保AI生成的代码质量和安全性，防止不良代码影响生产系统

Method: 通过扫描代码库来验证AI生成的变更，并应用修正措施

Result: 能够有效检测和阻止不良代码进入生产环境

Conclusion: Traycer提供了一种可靠的方法来确保AI生成代码的质量和安全性

Abstract: Vibe Code with Confidence (Website) Traycer scans your codebase to verify every AI-generated change, applying course corrections so bad code never makes it to production.

</details>


### [51] [SecureVibes](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fanshumanbh%2Fsecurevibes%3Futm_source=tldrinfosec/1/01000199f247d0e6-7b3a391d-8602-4e85-818b-e7771bd98e37-000000/G2vYLXQlDkeEUmAhPi6qYIPfBgC18nTjof98mnOet2s=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: SecureVibes是一个基于Claude多智能体架构的AI原生安全系统，用于自动发现代码库中的安全漏洞


<details>
  <summary>Details</summary>
Motivation: 为vibecoded应用程序提供自动化的安全漏洞检测，提高代码安全性

Method: 利用Claude的多智能体架构构建AI原生安全系统，自主分析代码库

Result: 开发了能够自动发现安全漏洞的SecureVibes系统

Conclusion: 多智能体AI架构可以有效应用于代码安全检测领域

Abstract: SecureVibes (GitHub Repo) SecureVibes is an AI-native security system for vibecoded applications that uses Claude's multi-agent architecture to find security vulnerabilities in your code base autonomously.

</details>


### [52] [Claude Skills are awesome, maybe a bigger deal than MCP](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F16%2Fclaude-skills%2F%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/570AzSPWIwsw9U-UpMCF363OsAdX8AG9YhDTH9QcNNQ=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Claude Skills are awesome, maybe a bigger deal than MCP (5 minute read) Anthropic's new Skills feature consumes only dozens of tokens per skill until needed compared to MCPs that can burn tens of thousands upfront. The design is incredibly simple: drop instructions in a folder and Claude figures out when to use them. This token efficiency and model-agnostic approach (any coding agent can read the files) will trigger a "Cambrian explosion" that makes the MCP rush look pedestrian, especially si...

</details>


### [53] [Introducing SWE-grep and SWE-grep-mini: RL for Multi-Turn, Fast Context Retrieval](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fswe-grep%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/m7F9UWhmzKk5t1TTVeToPQ5DIV-zb_9rRJbNl0qSgGE=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: SWE-grep和SWE-grep-mini是使用强化学习进行多轮快速代码上下文检索的专门模型，比前沿模型快10倍


<details>
  <summary>Details</summary>
Motivation: 开发速度更快的代码上下文检索工具，让开发者能够保持流畅状态，因为速度的重要性被行业低估

Method: 使用强化学习训练专门模型，每轮最多发出8个并行工具调用，而不是传统代理使用的顺序搜索

Result: 检索速度比前沿模型快10倍

Conclusion: 并行工具调用和强化学习可以显著提高代码上下文检索速度

Abstract: Introducing SWE-grep and SWE-grep-mini: RL for Multi-Turn, Fast Context Retrieval (10 minute read) Cognition trained specialized models that retrieve code context 10x faster than frontier models by using RL to issue up to 8 parallel tool calls per turn instead of the sequential searches typical agents use. Speed matters more than the industry realizes because it allows developers to stay in a flow state.

</details>


### [54] [The Art of Scaling Reinforcement Learning Compute for LLMs](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2510.13786%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/sYj0T9nWmCsBOX8lGBhqv1IbDCdsAroQHZAluyfEWc0=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ScaleRL框架用于分析和预测大语言模型强化学习中的计算扩展，研究表明稳定的训练配方能产生可预测的性能曲线，有助于高效资源分配。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型强化学习中的计算扩展问题，旨在找到可预测的性能扩展规律，以提高训练效率和资源利用率。

Method: 开发ScaleRL框架，进行大规模研究分析稳定训练配方下的性能扩展规律。

Result: 研究表明稳定训练配方能够产生可预测的性能曲线，有助于优化计算资源分配。

Conclusion: 通过ScaleRL框架可以更好地理解和预测LLM强化学习的计算扩展行为，为高效训练提供指导。

Abstract: The Art of Scaling Reinforcement Learning Compute for LLMs (24 minute read) ScaleRL is a framework for analyzing and predicting compute scaling in LLM reinforcement learning. Their large-scale study shows that stable training recipes yield predictable performance curves, aiding efficient resource allocation.

</details>


### [55] [Introducing Manus 1.5](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2Fmanus-1.5-release%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/ddKH7NpYukfGSAyFTzMG6r1YykqVALWT6YGcqny3kDU=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Manus 1.5是一个能够通过自然语言处理全栈Web开发任务的AI代理，包括后端基础设施、数据库、质量保证和用户认证，并能自主测试和修复应用问题。


<details>
  <summary>Details</summary>
Motivation: 旨在简化全栈Web开发流程，让开发者能够完全通过自然语言指令来构建、测试和维护Web应用，降低开发门槛。

Method: 利用自然语言处理技术，将开发者的需求转化为完整的Web应用开发流程，包括后端架构、数据库设计、用户认证系统等，并集成自动化测试和问题修复功能。

Result: 成功开发出能够处理全栈Web开发任务的AI代理，实现了从自然语言到完整应用的无缝转换，并具备自主测试和修复能力。

Conclusion: Manus 1.5展示了AI代理在全栈Web开发领域的潜力，为开发者提供了更高效、更易用的开发工具。

Abstract: Introducing Manus 1.5 (3 minute read) Manus 1.5 adds full-stack web development that handles backend infrastructure, databases, QA, and user authentication entirely, achieved entirely through natural language. The agent can test apps by itself and fix issues autonomously.

</details>


### [56] [Data Modeling for the Agentic Era: Semantics, Speed, and Stewardship](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ssp.sh%2Fblog%2Fagentic-data-modeling%2F%3Futm_source=tldrdata/1/0100019a011612f4-a86fb1c7-5b39-4993-9228-e246720b2c59-000000/qFy9NWBs0cuB6IQu6l6ng12eAh0W3GwXkij2mExuXxE=427)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该论文提出AI代理时代数据建模的三个核心支柱：语义层、速度和监管，以解决AI代理在缺乏结构化数据时只能猜测的问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理能够快速生成查询和仪表盘，但缺乏结构化数据支持会导致结果不可靠，需要建立可靠的数据建模框架来支持代理工作流。

Method: 提出基于三个支柱的数据建模方法：语义层（定义业务实体和指标的度量层）、速度（亚秒级分析验证）、监管（护栏、人工监督和版本控制）。

Result: 通过这三个支柱，AI代理能够在结构化数据基础上生成可靠的查询和仪表盘，同时确保人类能够快速验证和监管。

Conclusion: 在AI代理时代，成功的数据建模需要语义、速度和监管三个支柱的协同作用，以确保代理输出的可靠性和可控性。

Abstract: Data Modeling for the Agentic Era: Semantics, Speed, and Stewardship (27 minute read) AI agents can generate queries or dashboards in seconds, but without structure, they're just guessing. Agentic workflows should build on three essential pillars: Semantics, a curated metrics layer where agents query well-defined business entities and measures; Speed, sub-second analytics so humans can instantly verify AI outputs; and Stewardship - guardrails, human oversight, and versions that keep agents re...

</details>
