{"id": "wechat.2510.23c1ee25", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MDc0MzAyNw==&mid=2247484144&idx=1&sn=df448757f98e0f646649856b36568bc4&chksm=c34e36a3fce625b167ed3af9638ec72fc48f08c8faacf1c2b9b69e9f7b06fef059c4290f1e36#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MDc0MzAyNw==&mid=2247484144&idx=1&sn=df448757f98e0f646649856b36568bc4&chksm=c34e36a3fce625b167ed3af9638ec72fc48f08c8faacf1c2b9b69e9f7b06fef059c4290f1e36#rd", "authors": ["阿潮的博弈智能研究"], "title": "NerulPS 2025| 网络化多智能体<em class=\"highlight\">强化学习</em>的贝叶斯自我图（ego-graph）推断", "comment": "Source: WeChat, Published: 2025-10-27 13:42:42", "summary": "NerulPS 2025| 网络化多智能体强化学习的贝叶斯自我图（ego-graph）推断poster bayesian ego-graph inference for networked multi-agent reinforcement learning wei duan · jie lu · junyu xuan [ abstract） abstract： in networked multi-agent reinforcement learning （networked-marl"}
{"id": "wechat.2510.f31b0361", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MzY2Njg3NQ==&mid=2247526706&idx=1&sn=73287c3807aa90a217dc4557913975f7&chksm=c2adb4a998ccae84eb13473ed856027b3a6cedc904cc74e2dfb947a7db27842b93d6d5989055#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MzY2Njg3NQ==&mid=2247526706&idx=1&sn=73287c3807aa90a217dc4557913975f7&chksm=c2adb4a998ccae84eb13473ed856027b3a6cedc904cc74e2dfb947a7db27842b93d6d5989055#rd", "authors": ["Matlab科研助手"], "title": "【EI复现】基于深度<em class=\"highlight\">强化学习</em>的微能源网能量管理与优化策略研究附Python代码", "comment": "Source: WeChat, Published: 2025-10-27 12:57:15", "summary": "（二）深度强化学习的应用价值 深度强化学习（DRL）通过 “智能体 - 环境” 交互机制，无需建立精确系统模型即可实现动态优化，具备强鲁棒性与实时决策能力，为微能源网能量管理提供新解决方案。"}
{"id": "wechat.2510.51b5f333", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNDI0OTQyOQ==&mid=2247485131&idx=1&sn=f1073f7d7ff747053d6938a8dc2a99e8&chksm=c0c2109c6a080d31b3bfcdc68969d52e94ef65efc672da8cba3c351f06f1bf5a252049585f15#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNDI0OTQyOQ==&mid=2247485131&idx=1&sn=f1073f7d7ff747053d6938a8dc2a99e8&chksm=c0c2109c6a080d31b3bfcdc68969d52e94ef65efc672da8cba3c351f06f1bf5a252049585f15#rd", "authors": ["刘小强的博客"], "title": "<em class=\"highlight\">强化学习</em>基础-PPO", "comment": "Source: WeChat, Published: 2025-10-27 12:30:39", "summary": "代码实现大模型强化学习（PPO），看这个视频就够了。本文将带你从 策略梯度的基本原理 出发，深入剖析 PPO 的核心思想、数学推导以及它在大模型训练中的具体应用逻辑。"}
{"id": "wechat.2510.e70733d0", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247586057&idx=3&sn=f0656fc3670a34f521e969fad646189a&chksm=fae4377c8f7f0824b7585b9e96f979f4fa2daff506738128f80d042b4c4070f3a49464c99c46#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247586057&idx=3&sn=f0656fc3670a34f521e969fad646189a&chksm=fae4377c8f7f0824b7585b9e96f979f4fa2daff506738128f80d042b4c4070f3a49464c99c46#rd", "authors": ["AI思想会"], "title": "大模型<em class=\"highlight\">强化学习</em>的熵控制：CE-GPPO、EPO与AsyPPO技术方案对比详解", "comment": "Source: WeChat, Published: 2025-10-27 10:17:11", "summary": "LLM的强化学习训练最近进展很快，SOTA模型在各种推理benchmark上的表现确实亮眼。但更值得关注的其实是另一条信息——从Rutgers到Alibaba再到HKUST，这些研究团队正在攻克的是RL领域的一个老大难：怎么控制好熵，同时避免模型退"}
{"id": "wechat.2510.229ee19c", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMjgxMjI0Mw==&mid=2247483660&idx=1&sn=c859f82c34290406a85418c2fac4c3da&chksm=fedeb70194deec84d8d7093efcf425e609cc3e789d679e4ca8c362dffe73db3979ceca6e89cf#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMjgxMjI0Mw==&mid=2247483660&idx=1&sn=c859f82c34290406a85418c2fac4c3da&chksm=fedeb70194deec84d8d7093efcf425e609cc3e789d679e4ca8c362dffe73db3979ceca6e89cf#rd", "authors": ["阿烁说"], "title": "如何构建和<em class=\"highlight\">强化学习</em>“自驱力”", "comment": "Source: WeChat, Published: 2025-10-27 06:47:01", "summary": "那么，我们可以从学习的哪个部分入手呢？从日常的学习安排和奖励入手。我们将每天小测验的结果和小玩具挂钩，单元考的结果和一次大餐挂钩，期中期末的结果和一次“未知大奖励”挂钩，明确规定学习结果和即时奖励的关"}
{"id": "wechat.2510.2263df61", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5NzU2OTc2OQ==&mid=2247487343&idx=1&sn=410902217460bfe9766d9a427872d91c&chksm=c1a3c75310ab335ed2019762e65c0a200fcc7070d4ebd6d1b9e9d37d6f74669aeffb5c6e8cd5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5NzU2OTc2OQ==&mid=2247487343&idx=1&sn=410902217460bfe9766d9a427872d91c&chksm=c1a3c75310ab335ed2019762e65c0a200fcc7070d4ebd6d1b9e9d37d6f74669aeffb5c6e8cd5#rd", "authors": ["NLP学习加油站"], "title": "NLP论文速读（ MIT出品）| RL的奥卡姆剃刀：为什么在线<em class=\"highlight\">强化学习</em>遗忘更少", "comment": "Source: WeChat, Published: 2025-10-27 05:48:34", "summary": "本文系统比较了两种主流微调方式——监督微调（SFT）与强化学习（RL），发现在达到相同新任务性能的前提下，RL显著更少遗忘旧任务知识。为揭示这一现象背后的机制，作者提出并验证了一个核心观点：遗忘的程度可以由新"}
{"id": "wechat.2510.50d70270", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzMTg5NTYxNg==&mid=2247496774&idx=1&sn=97efdcddf6b0ebdbf07d461563981313&chksm=fb61af5fc016fa981143fad32210fb84c4b86b8128a6ba52a5146bf8767c35a4a12c80112df2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzMTg5NTYxNg==&mid=2247496774&idx=1&sn=97efdcddf6b0ebdbf07d461563981313&chksm=fb61af5fc016fa981143fad32210fb84c4b86b8128a6ba52a5146bf8767c35a4a12c80112df2#rd", "authors": ["F8AI"], "title": "安全的<em class=\"highlight\">强化学习</em>的盾牌——评估屏蔽作为安全 RL 方法的优点和潜在缺点。", "comment": "Source: WeChat, Published: 2025-10-27 05:14:24", "summary": "将屏蔽融入强化学习的主要方法是预屏蔽和后屏蔽，它们的不同之处在于干预方式。图3 （左）展示了后屏蔽：屏蔽位于智能体和环境之间，持续监控环境状态和智能体选择的操作。"}
{"id": "wechat.2510.17eb42e8", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5NzExODk0MQ==&mid=2247489086&idx=1&sn=0be8802a931449f086ff50544ccbde78&chksm=c1528fa8e6ffecbdfb96ac7b233c6830a420f0ec740156d30b99d0828e468751a20c1091e037#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5NzExODk0MQ==&mid=2247489086&idx=1&sn=0be8802a931449f086ff50544ccbde78&chksm=c1528fa8e6ffecbdfb96ac7b233c6830a420f0ec740156d30b99d0828e468751a20c1091e037#rd", "authors": ["小小何先生"], "title": "Nature2025 | <em class=\"highlight\">强化学习</em>可以自我改变搜索空间了？", "comment": "Source: WeChat, Published: 2025-10-27 03:00:23", "summary": "这篇文章介绍了一种名为 DiscoRL 的强化学习（RL）规则，该规则是通过元学习（Meta-Learning）方法自主发现的，其性能超越了目前手动设计的RL算法（如MuZero、PPO）。"}
{"id": "wechat.2510.4bf58173", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571704&idx=3&sn=90a38b03525d3bd46364d11ad825259a&chksm=96dedda4fbe5e0e7c8bde537e0be86258326926473cd0723c0011d56b24abeb437ae349eb69b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571704&idx=3&sn=90a38b03525d3bd46364d11ad825259a&chksm=96dedda4fbe5e0e7c8bde537e0be86258326926473cd0723c0011d56b24abeb437ae349eb69b#rd", "authors": ["深度学习与NLP"], "title": "DRL圣经2025最新版-《<em class=\"highlight\">强化学习</em>:导论第二版》免费pdf分享", "comment": "Source: WeChat, Published: 2025-10-27 00:01:59", "summary": "我们第二版的目标和第一版的目标是一样的：为强化学习的关键思想和算法提供一个清晰而简单的描述，供所有相关学科的读者阅读。该版本仍然是一个介绍，我们保留了核心，在线学习算法的重点。"}
