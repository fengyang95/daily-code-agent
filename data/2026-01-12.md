<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 13]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LG](#cs.LG) [Total: 8]
- [tldr.article](#tldr.article) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Do LLMs Need Inherent Reasoning Before Reinforcement Learning? A Study in Korean Self-Correction](https://arxiv.org/abs/2601.05459)
*Hongjin Kim,Jaewook Lee,Kiyoung Lee,Jong-hun Shin,Soojong Lim,Oh-Woog Kwon*

Main category: cs.CL

TL;DR: 本文研究强化学习能否提升LLM在韩语等低资源语言上的推理能力，发现关键在于对齐模型内部推理过程与韩语输入，而非注入新的语言知识。


<details>
  <summary>Details</summary>
Motivation: LLM在英语等高资源语言上表现出强大的推理和自我纠正能力，但在韩语等低资源语言上性能有限。研究旨在探索强化学习能否将韩语推理能力提升到与英语相当的水平。

Method: 研究探索了多种微调策略，发现通过调整早期层中的韩语特定神经元来对齐模型内部推理过程与韩语输入是关键。引入了自我纠正代码切换数据集来促进这种对齐。

Result: 强化学习单独应用时对缺乏固有韩语推理能力的模型改进有限。通过神经元级调优对齐内部推理过程后，在数学推理和自我纠正任务中观察到显著的性能提升。

Conclusion: 多语言推理增强的关键不是注入新的语言知识，而是有效激发和对齐现有的推理能力。内部翻译和神经元级调优对LLM的多语言推理对齐有重要贡献。

Abstract: Large Language Models (LLMs) demonstrate strong reasoning and self-correction abilities in high-resource languages like English, but their performance remains limited in low-resource languages such as Korean. In this study, we investigate whether reinforcement learning (RL) can enhance Korean reasoning abilities to a degree comparable to English. Our findings reveal that RL alone yields limited improvements when applied to models lacking inherent Korean reasoning capabilities. To address this, we explore several fine-tuning strategies and show that aligning the model's internal reasoning processes with Korean inputs-particularly by tuning Korean-specific neurons in early layers-is key to unlocking RL's effectiveness. We introduce a self-correction code-switching dataset to facilitate this alignment and observe significant performance gains in both mathematical reasoning and self-correction tasks. Ultimately, we conclude that the crucial factor in multilingual reasoning enhancement is not injecting new linguistic knowledge, but effectively eliciting and aligning existing reasoning capabilities. Our study provides a new perspective on how internal translation and neuron-level tuning contribute to multilingual reasoning alignment in LLMs.

</details>


### [2] [CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems](https://arxiv.org/abs/2601.05520)
*Xuemei Tang,Chengxi Yan,Jinghang Gu,Chu-Ren Huang*

Main category: cs.CL

TL;DR: CHisAgent是一个用于构建中国古代历史事件分类法的多智能体LLM框架，通过三个角色专门化阶段从《二十四史》等原始历史语料中自动构建大规模、领域感知的分类结构。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在许多任务上表现强劲，但在历史和文化推理方面能力有限，特别是在中文历史等非英语语境中。分类结构是组织历史知识、提高理解的有效机制，但手动构建分类法成本高昂且难以扩展。

Method: CHisAgent将分类法构建分解为三个角色专门化阶段：1）自底向上的归纳器（Inducer）从原始历史语料中推导初始层次结构；2）自顶向下的扩展器（Expander）利用LLM的世界知识引入缺失的中间概念；3）证据引导的丰富器（Enricher）整合外部结构化历史资源以确保忠实性。

Result: 使用《二十四史》构建了大规模、领域感知的中国古代事件分类法，涵盖政治、军事、外交和社会生活。广泛的参考无关和基于参考的评估显示，该方法在结构连贯性和覆盖范围方面都有改进，进一步分析表明生成的分类法支持跨文化对齐。

Conclusion: CHisAgent框架能够有效构建中国古代历史事件分类法，解决了LLM在历史和文化推理方面的局限性，同时通过多智能体协作实现了自动化的分类法构建，为跨文化对齐提供了支持。

Abstract: Despite strong performance on many tasks, large language models (LLMs) show limited ability in historical and cultural reasoning, particularly in non-English contexts such as Chinese history. Taxonomic structures offer an effective mechanism to organize historical knowledge and improve understanding. However, manual taxonomy construction is costly and difficult to scale. Therefore, we propose \textbf{CHisAgent}, a multi-agent LLM framework for historical taxonomy construction in ancient Chinese contexts. CHisAgent decomposes taxonomy construction into three role-specialized stages: a bottom-up \textit{Inducer} that derives an initial hierarchy from raw historical corpora, a top-down \textit{Expander} that introduces missing intermediate concepts using LLM world knowledge, and an evidence-guided \textit{Enricher} that integrates external structured historical resources to ensure faithfulness. Using the \textit{Twenty-Four Histories}, we construct a large-scale, domain-aware event taxonomy covering politics, military, diplomacy, and social life in ancient China. Extensive reference-free and reference-based evaluations demonstrate improved structural coherence and coverage, while further analysis shows that the resulting taxonomy supports cross-cultural alignment.

</details>


### [3] [AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor](https://arxiv.org/abs/2601.05752)
*Shu Yang,Jingyu Hu,Tong Li,Hanqi Yan,Wenxuan Wang,Di Wang*

Main category: cs.CL

TL;DR: AutoMonitor-Bench是首个系统性评估基于LLM的错误行为监控器可靠性的基准，包含3,010个标注样本，涵盖问答、代码生成和推理任务，评估12个专有和10个开源LLM的监控性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性评估LLM错误行为监控器可靠性的基准，需要理解监控器在不同任务和失败模式下的性能表现，以及安全性与实用性之间的权衡关系。

Method: 构建包含3,010个标注样本的基准数据集，涵盖问答、代码生成和推理任务，每个样本包含错误行为和良性行为实例。使用漏检率(MR)和误报率(FAR)两个互补指标评估监控器性能。进一步构建153,581个样本的训练语料库，微调Qwen3-4B-Instruction模型，研究已知错误行为数据集训练对未见和隐式错误行为监控性能的影响。

Result: 评估显示监控性能存在显著差异，MR和FAR之间存在一致的权衡关系，揭示了固有的安全-效用张力。微调实验表明，在已知、相对容易构建的错误行为数据集上训练，对未见和更隐式错误行为的监控性能提升有限。

Conclusion: 可靠、可扩展的错误行为监控面临挑战，需要未来研究关注任务感知的设计和训练策略，以改善基于LLM的监控器性能。

Abstract: We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes. AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning, with paired misbehavior and benign instances. We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively. Evaluating 12 proprietary and 10 open-source LLMs, we observe substantial variability in monitoring performance and a consistent trade-off between MR and FAR, revealing an inherent safety-utility tension. To further explore the limits of monitor reliability, we construct a large-scale training corpus of 153,581 samples and fine-tune Qwen3-4B-Instruction to investigate whether training on known, relatively easy-to-construct misbehavior datasets improves monitoring performance on unseen and more implicit misbehaviors. Our results highlight the challenges of reliable, scalable misbehavior monitoring and motivate future work on task-aware designing and training strategies for LLM-based monitors.

</details>


### [4] [GIFT: Games as Informal Training for Generalizable LLMs](https://arxiv.org/abs/2601.05633)
*Nuoyan Lyu,Bingbing Xu,Weihao Meng,Yige Yuan,Yang Zhang,Zhiyong Huang,Tat-Seng Chua,Huawei Shen*

Main category: cs.CL

TL;DR: 该论文提出将游戏作为LLM非正式学习的主要环境，通过嵌套训练框架解决多任务学习中的性能退化问题，增强模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: LLM在形式学习任务（如数学和代码生成）上表现出色，但缺乏人类认知中的"实践智慧"和可泛化智能（如战略创造力和社交推理）。这种差距源于缺乏非正式学习，而游戏环境可以提供互动反馈和抽象复杂性来培养多样化能力。

Method: 提出将游戏作为LLM非正式学习的主要环境，利用其内在奖励信号和抽象复杂性。引入嵌套训练框架，通过顺序任务组合强制执行明确的"AND"目标（而非简单的任务混合优化隐式"OR"目标），强制模型同时掌握多种能力以获得最大奖励。使用基于GRPO的强化学习在矩阵游戏、井字棋和"谁是卧底"游戏中进行训练。

Result: 集成基于游戏的非正式学习不仅能防止任务干扰，还能显著增强模型在广泛能力导向基准测试中的泛化能力。

Conclusion: 游戏环境为LLM非正式学习提供了有效途径，嵌套训练框架解决了多任务学习中的性能退化问题，能够培养LLM的战略创造力和社交推理等实践智慧。

Abstract: While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the "practical wisdom" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit "OR" objective, our framework employs sequential task composition to enforce an explicit "AND" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.

</details>


### [5] [Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat](https://arxiv.org/abs/2601.05657)
*Hao Yang,Hongyuan Lu,Dingkang Yang,Wenliang Yang,Peng Sun,Xiaochuan Zhang,Jun Xiao,Kefan He,Wai Lam,Yang Liu,Xinhua Zeng*

Main category: cs.CL

TL;DR: Stephanie2是一个新型的逐步决策对话代理，通过主动等待和消息节奏适应机制，能够更自然地模拟人类即时消息对话模式，相比前代在自然度和参与度方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有逐步AI聊天系统通常将一次性生成的内容分割成多个消息顺序发送，但缺乏主动等待机制，消息节奏不自然。人类即时消息社交聊天通常通过一系列短消息进行，需要更自然的对话节奏模拟。

Method: 提出Stephanie2代理，具有主动等待和消息节奏适应功能，在每一步明确决定发送还是等待，将延迟建模为思考时间和打字时间的总和。引入基于时间窗口的双代理对话系统生成伪对话历史用于评估。

Result: 实验显示Stephanie2在自然度和参与度等指标上明显优于Stephanie1，在角色识别图灵测试的人类评估中获得了更高的通过率。

Conclusion: Stephanie2通过主动等待和消息节奏适应机制，能够更自然地模拟人类即时消息对话模式，提升了对话代理的自然性和参与度。

Abstract: Instant-messaging human social chat typically progresses through a sequence of short messages. Existing step-by-step AI chatting systems typically split a one-shot generation into multiple messages and send them sequentially, but they lack an active waiting mechanism and exhibit unnatural message pacing. In order to address these issues, we propose Stephanie2, a novel next-generation step-wise decision-making dialogue agent. With active waiting and message-pace adaptation, Stephanie2 explicitly decides at each step whether to send or wait, and models latency as the sum of thinking time and typing time to achieve more natural pacing. We further introduce a time-window-based dual-agent dialogue system to generate pseudo dialogue histories for human and automatic evaluations. Experiments show that Stephanie2 clearly outperforms Stephanie1 on metrics such as naturalness and engagement, and achieves a higher pass rate on human evaluation with the role identification Turing test.

</details>


### [6] [Visualising Information Flow in Word Embeddings with Diffusion Tensor Imaging](https://arxiv.org/abs/2601.05713)
*Thomas Fabian*

Main category: cs.CL

TL;DR: 提出一种基于扩散张量成像（DTI）的新工具，用于分析和可视化自然语言表达中的信息流，通过追踪LLM层间的信息流动来比较模型结构、识别可剪枝层，并揭示不同任务中的信息流差异。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要提取LLM中的词嵌入并通过点图可视化，仅考虑单个词汇而忽略上下文，无法分析完整自然语言表达中的信息流动。

Method: 将扩散张量成像（DTI）技术应用于词嵌入分析，追踪LLM各层之间信息流的传播路径，从而可视化自然语言表达中的信息流动。

Result: DTI能够揭示词嵌入间的信息流动，通过追踪LLM层间信息流可以比较不同模型结构、识别未充分利用的可剪枝层，并显示代词消解和隐喻检测等任务中的信息流差异。

Conclusion: 该方法为理解LLM如何表示实际自然语言表达提供了新视角，超越了孤立词嵌入的比较，提升了NLP模型的可解释性。

Abstract: Understanding how large language models (LLMs) represent natural language is a central challenge in natural language processing (NLP) research. Many existing methods extract word embeddings from an LLM, visualise the embedding space via point-plots, and compare the relative positions of certain words. However, this approach only considers single words and not whole natural language expressions, thus disregards the context in which a word is used. Here we present a novel tool for analysing and visualising information flow in natural language expressions by applying diffusion tensor imaging (DTI) to word embeddings. We find that DTI reveals how information flows between word embeddings. Tracking information flows within the layers of an LLM allows for comparing different model structures and revealing opportunities for pruning an LLM's under-utilised layers. Furthermore, our model reveals differences in information flows for tasks like pronoun resolution and metaphor detection. Our results show that our model permits novel insights into how LLMs represent actual natural language expressions, extending the comparison of isolated word embeddings and improving the interpretability of NLP models.

</details>


### [7] [EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis](https://arxiv.org/abs/2601.05808)
*Xiaoshuai Song,Haofei Chang,Guanting Dong,Yutao Zhu,Zhicheng Dou,Ji-Rong Wen*

Main category: cs.CL

TL;DR: EnvScaler是一个通过程序合成自动生成可扩展工具交互环境的框架，用于训练LLM作为真实环境中的智能体，解决了现有环境构建方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 训练LLM作为真实环境中的智能体需要丰富多样的工具交互沙箱，但现有方法存在限制：真实系统访问受限，LLM模拟环境容易产生幻觉和不一致，手动构建沙箱难以扩展。

Method: EnvScaler包含两个组件：SkelBuilder通过主题挖掘、逻辑建模和质量评估构建多样化的环境骨架；ScenGenerator为每个环境生成多个任务场景和基于规则的轨迹验证函数。

Result: 使用EnvScaler合成了191个环境和约7K个场景，应用于Qwen3系列模型的SFT和RL训练。在三个基准测试中，EnvScaler显著提升了LLM在涉及多轮、多工具交互的复杂环境中解决任务的能力。

Conclusion: EnvScaler为LLM智能体训练提供了可扩展的环境生成框架，有效解决了工具交互环境构建的挑战，显著提升了LLM在复杂环境中的任务解决能力。

Abstract: Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.

</details>


### [8] [HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search](https://arxiv.org/abs/2601.05903)
*Zihang Tian,Rui Li,Jingsen Zhang,Xiaohe Bo,Wei Huo,Xu Chen*

Main category: cs.CL

TL;DR: HAPS是一个分层LLM路由框架，联合搜索模型架构和参数，通过高层路由器选择架构，低层路由器优化参数，共享参数增强能力，在基准测试中优于现有路由方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由方法主要关注选择不同的LLM架构，但忽略了参数设置对任务性能的关键影响。需要同时考虑架构选择和参数优化来充分利用不同LLM的专业优势。

Method: 提出分层路由框架HAPS：高层路由器选择候选LLM架构，低层路由器为选定架构搜索最优参数。设计参数生成网络在两个路由器间共享参数以相互增强能力。使用奖励增强目标进行有效优化。

Result: 在两个常用基准测试上，HAPS始终优于强大的路由基线方法，证明了联合搜索架构和参数的有效性。

Conclusion: HAPS通过分层路由框架联合优化LLM架构选择和参数设置，显著提升了路由性能，为LLM路由提供了更全面的解决方案。

Abstract: Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAPS, a hierarchical LLM routing framework that jointly searches over model architectures and parameters. Specifically, we use a high-level router to select among candidate LLM architectures, and then search for the optimal parameters for the selected architectures based on a low-level router. We design a parameter generation network to share parameters between the two routers to mutually enhance their capabilities. In the training process, we design a reward-augmented objective to effectively optimize our framework. Experiments on two commonly used benchmarks show that HAPS consistently outperforms strong routing baselines. We have released our code at https://github.com/zihangtian/HAPS.

</details>


### [9] [Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency](https://arxiv.org/abs/2601.05905)
*Haoming Xu,Ningyuan Zhao,Yunzhi Yao,Weihong Xu,Hongru Wang,Xinle Deng,Shumin Deng,Jeff Z. Pan,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 论文提出Neighbor-Consistency Belief (NCB)作为衡量LLM信念鲁棒性的结构指标，并开发Structure-Aware Training (SAT)来增强模型在上下文干扰下的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估主要依赖点状置信度（如Self-Consistency），无法检测信念在上下文干扰下的脆弱性。即使自一致性完美的答案也可能在轻微干扰下迅速崩溃，这对实际部署构成风险。

Method: 提出NCB作为结构化的信念鲁棒性度量，评估概念邻域内的响应一致性；设计认知压力测试协议验证NCB有效性；开发SAT训练方法优化上下文不变的信念结构。

Result: 实验表明高NCB数据的性能对干扰更具抵抗力；SAT能将长尾知识脆弱性降低约30%，显著增强LLM在上下文干扰下的信念稳定性。

Conclusion: NCB是评估LLM信念鲁棒性的有效结构指标，SAT能显著提升模型在现实部署中的可靠性，为构建更稳健的LLM系统提供了新方向。

Abstract: As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.

</details>


### [10] [Can We Predict Before Executing Machine Learning Agents?](https://arxiv.org/abs/2601.05930)
*Jingsheng Zheng,Jintian Zhang,Yujie Luo,Yuren Mao,Yunjun Gao,Lun Du,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 该论文提出FOREAGENT框架，通过内部化执行先验来替代昂贵的物理执行，采用"预测-验证"循环加速机器学习代理的收敛过程。


<details>
  <summary>Details</summary>
Motivation: 当前自主机器学习代理受限于"生成-执行-反馈"范式，存在严重的执行瓶颈，假设评估依赖昂贵的物理执行。需要绕过这些物理约束，通过内部化执行先验来加速科学发现过程。

Method: 1) 形式化数据为中心的解偏好任务；2) 构建包含18,438对比较的综合语料库；3) 利用经过验证的数据分析报告来激发LLMs的预测能力；4) 实现FOREAGENT代理，采用"预测-验证"循环替代传统执行。

Result: LLMs在获得验证数据分析报告后表现出显著的预测能力，达到61.5%的准确率和稳健的置信度校准。FOREAGENT实现了6倍的收敛加速，并超越基于执行的基线方法6%。

Conclusion: 通过内部化执行先验和预测-验证循环，可以显著加速自主机器学习代理的收敛过程，同时保持或超越传统执行方法的性能。

Abstract: Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.

</details>


### [11] [The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.06002)
*Qiguang Chen,Yantao Du,Ziniu Li,Jinhao Liu,Songyao Duan,Jiarui Guo,Minghao Liu,Jiaheng Liu,Tong Yang,Ge Zhang,Libo Qin,Wanxiang Che,Wenhao Huang*

Main category: cs.CL

TL;DR: 论文提出长链思维推理具有类似分子的稳定结构，包含三种相互作用类型，并开发了Mole-Syn方法来合成有效的长链思维结构，提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在从人类或非长链思维LLMs模仿中学习有效的长链思维推理时经常失败，需要理解长链思维推理的有效学习机制。

Method: 提出长链思维轨迹具有类似分子的稳定结构，包含深度推理（共价键式）、自我反思（氢键式）和自探索（范德华式）三种相互作用。引入有效语义异构体概念，开发Mole-Syn分布转移图方法指导合成有效的长链思维结构。

Result: 分析表明这些结构来自长链思维微调而非关键词模仿，只有促进快速熵收敛的键能支持稳定的长链思维学习。Mole-Syn方法在多个基准测试中提升了性能和强化学习稳定性。

Conclusion: 长链思维推理的有效学习依赖于类似分子的稳定结构，Mole-Syn方法能够合成这些结构，显著改善大语言模型的长链思维推理能力。

Abstract: Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.

</details>


### [12] [Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2601.06007)
*Elias Lumer,Faheem Nizar,Akshaya Jangiti,Kevin Frank,Anmol Gulati,Mandar Phadate,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: 该论文首次系统评估了提示缓存对多轮代理任务的影响，在三大LLM提供商上测试了三种缓存策略，发现可降低45-80%的API成本并提升13-31%的首令牌时间。


<details>
  <summary>Details</summary>
Motivation: 尽管主要LLM提供商提供提示缓存来降低成本和延迟，但其在代理工作负载中的效益在研究中尚未充分探索。目前没有工作量化这些成本节省或比较多轮代理任务的缓存策略。

Method: 在三大LLM提供商（OpenAI、Anthropic、Google）上全面评估提示缓存，比较三种缓存策略：完整上下文缓存、仅系统提示缓存、排除动态工具结果的缓存。使用DeepResearchBench多轮代理基准测试，在500多个代理会话中测量API成本和首令牌时间。

Result: 提示缓存可将API成本降低45-80%，首令牌时间提升13-31%。策略性提示缓存块控制（如将动态内容放在系统提示末尾、避免动态传统函数调用、排除动态工具结果）比简单的完整上下文缓存提供更一致的效益。

Conclusion: 提示缓存对代理系统有显著效益，但需要策略性实施。不同提供商存在细微差异，研究为生产代理系统实施提示缓存提供了实用指导。

Abstract: Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.

</details>


### [13] [Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards](https://arxiv.org/abs/2601.06021)
*Jiajie Zhang,Xin Lv,Ling Feng,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 提出CaRR奖励框架和C-GRPO训练方法，通过细粒度奖励机制提升深度搜索代理的推理全面性、事实基础和证据连接能力，解决传统二元奖励导致的捷径利用和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的深度搜索代理主要依赖二元结果奖励，无法捕捉推理过程的全面性和事实性，导致捷径利用和幻觉等不良行为。

Method: 提出CaRR奖励框架，将复杂问题分解为可验证的单步评估标准，要求代理识别隐藏实体、提供正确引用并构建完整证据链；结合CaRR和结果奖励提出C-GRPO训练方法。

Result: C-GRPO在多个深度搜索基准测试中持续优于标准基于结果的RL基线，有效抑制捷径利用，促进全面、基于证据的推理，并在开放式深度研究任务中表现出强泛化能力。

Conclusion: CaRR和C-GRPO为深度搜索代理提供了有效的细粒度奖励框架，显著提升了推理质量和事实准确性。

Abstract: Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \textbf{Citation-aware Rubric Rewards (CaRR)}, a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce \textbf{Citation-aware Group Relative Policy Optimization (C-GRPO)}, which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [Naiad: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring](https://arxiv.org/abs/2601.05256)
*Eirini Baltzi,Tilemachos Moumouris,Athena Psalta,Vasileios Tsironis,Konstantinos Karantzalos*

Main category: cs.AI

TL;DR: NAIAD是一个基于LLM的智能助手，利用检索增强生成、工具编排和计算图执行，为内陆水体监测提供端到端解决方案，将自然语言查询转化为可操作的洞察。


<details>
  <summary>Details</summary>
Motivation: 内陆水体监测对保护公共健康和生态系统至关重要，但现有方法往往孤立地处理蓝藻、叶绿素等子问题，缺乏整体解决方案。需要为专家和非专家提供统一的监测工具。

Method: 采用基于LLM的智能体架构，结合检索增强生成、LLM推理、外部工具编排、计算图执行和智能体反思。集成天气数据、Sentinel-2影像、遥感指数计算、叶绿素-a估计和CyFi等平台。

Result: 在专用基准测试中，正确性和相关性分别达到77%和85%以上，表现出强大的适应性和鲁棒性。消融研究表明Gemma 3 (27B)和Qwen 2.5 (14B)在计算效率和推理性能方面表现最佳。

Conclusion: NAIAD成功为内陆水体监测提供了端到端的智能解决方案，通过单一提示界面将自然语言查询转化为可操作的洞察，为专家和非专家用户提供了实用工具。

Abstract: Inland water monitoring is vital for safeguarding public health and ecosystems, enabling timely interventions to mitigate risks. Existing methods often address isolated sub-problems such as cyanobacteria, chlorophyll, or other quality indicators separately. NAIAD introduces an agentic AI assistant that leverages Large Language Models (LLMs) and external analytical tools to deliver a holistic solution for inland water monitoring using Earth Observation (EO) data. Designed for both experts and non-experts, NAIAD provides a single-prompt interface that translates natural-language queries into actionable insights. Through Retrieval-Augmented Generation (RAG), LLM reasoning, external tool orchestration, computational graph execution, and agentic reflection, it retrieves and synthesizes knowledge from curated sources to produce tailored reports. The system integrates diverse tools for weather data, Sentinel-2 imagery, remote-sensing index computation (e.g., NDCI), chlorophyll-a estimation, and established platforms such as CyFi. Performance is evaluated using correctness and relevancy metrics, achieving over 77% and 85% respectively on a dedicated benchmark covering multiple user-expertise levels. Preliminary results show strong adaptability and robustness across query types. An ablation study on LLM backbones further highlights Gemma 3 (27B) and Qwen 2.5 (14B) as offering the best balance between computational efficiency and reasoning performance.

</details>


### [15] [Effects of personality steering on cooperative behavior in Large Language Model agents](https://arxiv.org/abs/2601.05302)
*Mizuki Sakai,Mizuki Yokoyama,Wakaba Tateishi,Genki Ichinose*

Main category: cs.AI

TL;DR: 研究通过重复囚徒困境游戏探索人格操控对LLM智能体合作行为的影响，发现宜人性是促进合作的主要因素，人格操控更多是行为偏差而非确定性控制机制。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明为LLM分配人格特质可以影响其行为，但人格操控在受控条件下如何影响合作行为仍不清楚，需要系统研究人格特质对LLM智能体合作行为的影响机制。

Method: 使用重复囚徒困境游戏，基于大五人格框架，首先测量GPT-3.5-turbo、GPT-4o和GPT-5的基本人格特征，然后比较基线和人格操控条件下的行为，并分析单独操纵每个人格维度到极端值的影响。

Result: 宜人性是所有模型中促进合作的主导因素，其他人格特质影响有限。明确的人格信息会增加合作，但也可能增加被利用的脆弱性，特别是在早期模型中。后期模型表现出更有选择性的合作行为。

Conclusion: 人格操控更多是行为偏差而非确定性控制机制，宜人性是影响合作行为的关键人格维度，不同代际模型对人格操控的反应存在差异。

Abstract: Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.

</details>


### [16] [The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models](https://arxiv.org/abs/2601.05376)
*Tassallah Abdullahi,Shrestha Ghosh,Hamish S Fraser,Daniel León Tramontini,Adeel Abbasi,Ghada Bourjeily,Carsten Eickhoff,Ritambhara Singh*

Main category: cs.AI

TL;DR: 研究发现：在临床决策中，LLM的角色设定（如急诊医生、护士）会产生系统性、情境依赖且非单调的影响，医疗角色在重症任务中提升性能（准确率+20%），但在初级护理中降低性能，安全性和专业性并非单调保证。


<details>
  <summary>Details</summary>
Motivation: 角色设定通常被视为LLM的行为先验，被认为能赋予专业知识和提高安全性，但其在高风险临床决策中的具体影响尚不明确，需要系统评估角色设定对临床LLM行为的影响。

Method: 系统评估基于角色的控制，考察专业角色（急诊医生、护士等）和交互风格（大胆vs谨慎）对不同模型和医疗任务的影响，使用多维评估指标（任务准确性、校准度、安全相关风险行为），结合LLM判断和人类临床医生评估。

Result: 医疗角色在重症护理任务中提升性能（准确率和校准度提升约20%），但在初级护理环境中降低性能；交互风格调节风险倾向但高度依赖模型；LLM判断在安全关键情况下偏好医疗角色，但人类临床医生对安全合规性只有中等一致性（Cohen's κ=0.43），且95.9%的回答中对推理质量信心较低。

Conclusion: 角色设定作为行为先验引入情境依赖的权衡，而非安全或专业性的保证，在临床决策中需要谨慎使用，不能简单假设角色设定会单调提升性能或安全性。

Abstract: Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\sim+20\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $κ= 0.43$) but indicate a low confidence in 95.9\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\_Paradox.

</details>


### [17] [Conformity and Social Impact on AI Agents](https://arxiv.org/abs/2601.05384)
*Alessandro Bellina,Giordano De Marzo,David Garcia*

Main category: cs.AI

TL;DR: 研究发现大型多模态语言模型作为AI代理时表现出系统性从众偏差，符合社会影响理论，即使单独表现优异的模型在群体压力下也易受操纵，揭示了多智能体系统中的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在多智能体环境中日益增多，理解其集体行为对于预测人工社会动态至关重要。本研究旨在探索AI代理在社会压力下的从众行为，揭示其在多智能体系统中的安全脆弱性。

Method: 通过改编社会心理学中的经典视觉实验，研究AI代理如何作为社会行动者响应群体影响。实验考察了群体规模、一致性、任务难度和来源特征等因素对从众行为的影响。

Result: AI代理表现出系统性从众偏差，符合社会影响理论。单独表现近乎完美的AI代理在群体影响下变得高度易受操纵。模型规模越大，在简单任务上从众性越低，但在能力边界处仍保持脆弱性。

Conclusion: AI代理决策中存在基本安全漏洞，可能被用于恶意操纵、虚假信息传播和偏见传播，突显了在集体AI部署中迫切需要安全保障措施。

Abstract: As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.

</details>


### [18] [ART: Adaptive Reasoning Trees for Explainable Claim Verification](https://arxiv.org/abs/2601.05455)
*Sahil Wadhwa,Himanshu Kumar,Guanqun Yang,Abbaas Alif Mohamed Nishar,Pranab Mohanty,Swapnil Shinde,Yue Wu*

Main category: cs.AI

TL;DR: ART提出了一种基于自适应推理树的层次化声明验证方法，通过构建支持与攻击论据的树状结构，使用LLM作为裁判进行成对比较，实现透明、可争议的决策过程。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂决策中表现出色，但其不透明性阻碍了在高风险环境中的应用。现有方法缺乏可信解释和纠错机制，无法有效纠正错误，损害了可信度。

Method: ART采用分层方法进行声明验证：从根声明开始，分支为支持和攻击的子论据。通过LLM裁判对子论据进行成对锦标赛式比较，自底向上确定论据强度，系统推导最终透明且可争议的裁决。

Result: 在多个数据集上的实证验证表明，ART的结构化推理优于强基线方法，为可解释的声明验证建立了新基准，提高了可靠性并确保决策步骤的清晰性。

Conclusion: ART通过层次化推理树和LLM裁判机制，解决了LLM决策的不透明性问题，提供了比Chain-of-Thought等方法更透明、可争议的验证框架，增强了高风险环境中的可信度。

Abstract: Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.

</details>


### [19] [PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering](https://arxiv.org/abs/2601.05465)
*Yu Liu,Wenxiao Zhang,Cong Cao,Wenxuan Lu,Fangfang Yuan,Diandian Guo,Kun Peng,Qiang Sun,Kaiyan Zhang,Yanbing Liu,Jin B. Hong,Bowen Zhou,Zhiyuan Ma*

Main category: cs.AI

TL;DR: PRISMA是一个解耦的强化学习框架，通过规划-检索-检查-解决-记忆架构解决RAG系统中的检索崩溃和学习不稳定问题，在十个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的RAG系统面临两个主要障碍：1) 检索崩溃 - 在大规模语料库上进行迭代检索时，缺乏推理引导的规划导致无法找到包含桥接答案的中间证据；2) 学习不稳定 - 端到端轨迹训练在推理链上的信用分配薄弱，模块间错误定位差，导致过拟合到基准特定的启发式方法，限制了可迁移性和稳定性。

Method: 提出PRISMA框架，采用解耦的Plan-Retrieve-Inspect-Solve-Memoize架构。通过推理引导的协作：检查器提供基于推理的反馈来优化规划器的分解和细粒度检索，同时在解决器中强制执行基于证据的推理。使用两阶段组相对策略优化(GRPO)：第一阶段将规划器和解决器校准为规划和推理的专家；第二阶段使用观察感知残差策略优化(OARPO)增强检查器的上下文验证和针对性恢复能力。

Result: PRISMA在十个基准测试中实现了最先进的性能，并能在实际场景中高效部署。

Conclusion: PRISMA通过解耦的强化学习框架和推理引导的协作机制，有效解决了RAG系统中的检索崩溃和学习不稳定问题，展示了在实际场景中的高效部署能力。

Abstract: Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA's strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner's decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector's ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.

</details>


### [20] [Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making](https://arxiv.org/abs/2601.05529)
*Jua Han,Jaeyoon Seo,Jungbin Min,Jean Oh,Jihie Kim*

Main category: cs.AI

TL;DR: 论文通过火灾疏散场景评估LLM在安全关键系统中的决策能力，发现即使99%准确率仍存在灾难性风险，当前LLM不适合直接部署于安全关键机器人系统。


<details>
  <summary>Details</summary>
Motivation: 随着LLM融入机器人决策，物理风险增加，单个错误指令可能直接危及人类安全。需要系统评估LLM在错误可能造成灾难性后果的场景中的性能。

Method: 通过火灾疏散场景的定性评估识别关键失败案例，设计7个定量评估任务：完整信息任务（使用ASCII地图）、不完整信息任务（需要推断缺失上下文）、安全导向空间推理任务（SOSR）。对多种LLM和VLM进行基准测试。

Result: 结果揭示严重漏洞：多个模型在ASCII导航中成功率为0%；在模拟消防演习中，模型指示机器人向危险区域而非紧急出口移动。1%失败率在机器人领域意味着每百次执行可能造成灾难性伤害。

Conclusion: 当前LLM不适合直接部署于安全关键系统，99%准确率具有误导性且危险。即使最先进模型也无法保证安全，绝对依赖它们会带来不可接受的风险。

Abstract: One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how "rare" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.

</details>


### [21] [WildSci: Advancing Scientific Reasoning from In-the-Wild Literature](https://arxiv.org/abs/2601.05567)
*Tengxiao Liu,Deepak Nathani,Zekun Li,Kevin Yang,William Yang Wang*

Main category: cs.AI

TL;DR: WildSci是一个从同行评审文献自动合成的领域特定科学问题数据集，涵盖9个科学学科和26个子领域，通过将复杂科学推理任务转化为多项选择题格式，支持可扩展的强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理研究主要集中在数学和编程等数据丰富、评估指标明确的领域，而在医学、材料科学等科学领域的进展有限，主要受限于数据集覆盖不足和开放科学问题的复杂性。

Method: 1. 从同行评审文献自动合成领域特定科学问题数据集WildSci；2. 将复杂科学推理任务转化为多项选择题格式；3. 应用强化学习对模型进行微调；4. 分析训练动态，包括领域特定性能变化、响应行为和泛化趋势。

Result: 在一系列科学基准测试上的实验证明了数据集和方法的有效性。WildSci数据集已公开发布，支持科学推理研究的可扩展和可持续发展。

Conclusion: WildSci通过提供高质量的科学推理数据集和强化学习方法，解决了科学领域LLM推理的瓶颈问题，为科学推理研究提供了可扩展的解决方案。

Abstract: Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.

</details>


### [22] [Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models](https://arxiv.org/abs/2601.05570)
*Cooper Lin,Maohao Ran,Yanting Zhang,Zhenglin Wan,Hongwei Fan,Yibo Xu,Yike Guo,Wei Xue,Jun Song*

Main category: cs.AI

TL;DR: 论文提出了Crisis-Bench基准，用于评估LLM在需要战略模糊性和信息保留的专业领域（如危机公关）中的表现，揭示了通用安全对齐与专业实用性之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 标准安全对齐使LLM具有普遍的帮助性和诚实性，但这种"童子军"道德观在需要战略模糊性和信息保留的专业领域（如公共关系、谈判、危机管理）中会产生"透明度税"，限制了专业实用性。

Method: 引入Crisis-Bench，一个多智能体部分可观察马尔可夫决策过程（POMDP），包含80个跨8个行业的多样化故事情节。通过7天动态企业危机模拟，让基于LLM的公关代理管理严格分离的私人和公共叙事状态，强制执行信息不对称。采用新颖的"仲裁者-市场循环"评估指标，将公众情绪转化为模拟股价，创建现实的经济激励结构。

Result: 结果揭示了关键二分法：一些模型屈服于伦理关切，而另一些模型则表现出马基雅维利式的合法战略保留能力，以稳定模拟股价。Crisis-Bench首次为评估"声誉管理"能力提供了量化框架。

Conclusion: 研究主张从僵化的道德绝对主义转向情境感知的专业对齐，为需要战略模糊性的专业领域开发更适用的LLM对齐方法。

Abstract: Standard safety alignment optimizes Large Language Models (LLMs) for universal helpfulness and honesty, effectively instilling a rigid "Boy Scout" morality. While robust for general-purpose assistants, this one-size-fits-all ethical framework imposes a "transparency tax" on professional domains requiring strategic ambiguity and information withholding, such as public relations, negotiation, and crisis management. To measure this gap between general safety and professional utility, we introduce Crisis-Bench, a multi-agent Partially Observable Markov Decision Process (POMDP) that evaluates LLMs in high-stakes corporate crises. Spanning 80 diverse storylines across 8 industries, Crisis-Bench tasks an LLM-based Public Relations (PR) Agent with navigating a dynamic 7-day corporate crisis simulation while managing strictly separated Private and Public narrative states to enforce rigorous information asymmetry. Unlike traditional benchmarks that rely on static ground truths, we introduce the Adjudicator-Market Loop: a novel evaluation metric where public sentiment is adjudicated and translated into a simulated stock price, creating a realistic economic incentive structure. Our results expose a critical dichotomy: while some models capitulate to ethical concerns, others demonstrate the capacity for Machiavellian, legitimate strategic withholding in order to stabilize the simulated stock price. Crisis-Bench provides the first quantitative framework for assessing "Reputation Management" capabilities, arguing for a shift from rigid moral absolutism to context-aware professional alignment.

</details>


### [23] [Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection](https://arxiv.org/abs/2601.05578)
*Cooper Lin,Yanting Zhang,Maohao Ran,Wei Xue,Hongwei Fan,Yibo Xu,Zhenglin Wan,Sirui Han,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: 该论文提出了一种使用强化学习（RL）后训练轻量级语言模型进行欺诈检测的新方法，利用GSPO算法和基于规则的奖励系统在真实交易数据上微调模型，显著提升了欺诈检测的F1分数。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台和支付解决方案提供商面临日益复杂的欺诈方案，但尽管大型语言模型（LLMs）在理论上具有潜力，其在真实金融欺诈检测中的应用仍未被充分探索，特别是在处理特定领域电子商务交易数据方面的实际效果尚未得到实证验证。

Method: 提出使用强化学习（RL）后训练轻量级语言模型专门用于欺诈检测任务，仅使用原始交易数据。采用Group Sequence Policy Optimization（GSPO）算法结合基于规则的奖励系统，在中国全球支付解决方案公司提供的真实交易数据集上微调不同规模的语言模型。

Result: 实验结果显示该方法有效，后训练的语言模型在保留测试数据上实现了显著的F1分数提升。性能改进主要归因于强化学习固有的探索机制，使模型能够发现传统工程特征之外的新欺诈指标。

Conclusion: 该研究证明了强化学习后训练语言模型在欺诈检测任务中的有效性，特别是在发现传统方法可能忽略的复杂欺诈模式方面具有优势，为实际金融欺诈检测应用提供了新的技术路径。

Abstract: E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.

</details>


### [24] [HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation](https://arxiv.org/abs/2601.05656)
*Rongxin Chen,Tianyu Wu,Bingbing Xu,Xiucheng Xu,Huawei Shen*

Main category: cs.AI

TL;DR: HAG：一种分层智能体生成框架，通过两阶段决策过程实现主题自适应的人口生成，在宏观分布对齐和微观一致性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能体初始化方法存在局限性：基于静态数据检索的方法无法适应未见主题，而基于LLM生成的方法缺乏宏观分布意识，导致微观属性与现实不一致。需要一种既能适应主题变化又能保持宏观微观一致性的框架。

Method: 提出HAG分层智能体生成框架，将人口生成形式化为两阶段决策过程：1）使用世界知识模型推断分层条件概率构建主题自适应树，实现宏观分布对齐；2）基于真实世界数据进行实例化和智能体增强，确保微观一致性。

Result: 实验表明HAG显著优于代表性基线方法，平均减少人口对齐误差37.7%，提升社会学一致性18.8%。建立了多领域基准和全面的PACE评估框架。

Conclusion: HAG框架通过分层方法有效解决了智能体初始化中的主题适应性和一致性挑战，为基于智能体的建模提供了高质量的人口生成解决方案。

Abstract: High-fidelity agent initialization is crucial for credible Agent-Based Modeling across diverse domains. A robust framework should be Topic-Adaptive, capturing macro-level joint distributions while ensuring micro-level individual rationality. Existing approaches fall into two categories: static data-based retrieval methods that fail to adapt to unseen topics absent from the data, and LLM-based generation methods that lack macro-level distribution awareness, resulting in inconsistencies between micro-level persona attributes and reality. To address these problems, we propose HAG, a Hierarchical Agent Generation framework that formalizes population generation as a two-stage decision process. Firstly, utilizing a World Knowledge Model to infer hierarchical conditional probabilities to construct the Topic-Adaptive Tree, achieving macro-level distribution alignment. Then, grounded real-world data, instantiation and agentic augmentation are carried out to ensure micro-level consistency. Given the lack of specialized evaluation, we establish a multi-domain benchmark and a comprehensive PACE evaluation framework. Extensive experiments show that HAG significantly outperforms representative baselines, reducing population alignment errors by an average of 37.7% and enhancing sociological consistency by 18.8%.

</details>


### [25] [DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation](https://arxiv.org/abs/2601.05746)
*Zhenghao Li,Zhi Zheng,Wei Chen,Jielun Zhao,Yong Chen,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: DynaDebate提出动态多智能体辩论框架，通过路径生成、过程中心辩论和触发验证解决传统多智能体辩论中因初始化相同导致的同质化错误和简单多数投票问题。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论框架存在未引导初始化问题，导致智能体采用相同推理路径并犯相同错误，使得有效辩论受阻，最终结果退化为简单多数投票。

Method: 提出DynaDebate框架，包含三个核心机制：1) 动态路径生成与分配，使用专用路径生成智能体生成多样化逻辑解路径；2) 过程中心辩论，从结果投票转向逐步逻辑批判；3) 触发式验证智能体，在分歧时激活并使用外部工具客观解决僵局。

Result: 大量实验表明DynaDebate在多个基准测试中表现优异，超越了现有最先进的多智能体辩论方法。

Conclusion: DynaDebate通过动态路径生成、过程中心辩论和触发验证有效解决了多智能体辩论中的同质化问题，显著提升了协作决策和复杂问题解决能力。

Abstract: Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.

</details>


### [26] [From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation](https://arxiv.org/abs/2601.05787)
*Zezhou Wang,Ziyun Zhang,Xiaoyi Zhang,Zhuzhong Qian,Yan Lu*

Main category: cs.AI

TL;DR: BEPA方法通过双层专家轨迹对齐策略，将静态专家轨迹转化为策略对齐指导，显著提升了端到端GUI操作代理在OSWorld-Verified等基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前GUI数据集（如OSWorld）存在两个瓶颈：1）只有几百个可交互、可验证的任务和环境；2）专家轨迹需要通过与环境交互收集，难以扩展。因此需要研究如何利用少量现有专家轨迹通过可验证奖励的强化学习来训练端到端策略。

Method: 提出BEPA（双层专家到策略同化）方法：LEVEL-1通过基础策略生成自滚动可达轨迹，将静态专家轨迹转化为策略对齐指导；LEVEL-2使用按任务动态更新的缓存进行RLVR（可验证奖励的强化学习）。

Result: 在OSWorld-Verified上，BEPA将UITARS1.5-7B的成功率从22.87%提升到32.13%，在保留测试集上从5.74%提升到10.30%，在MMBench-GUI和Online-Mind2Web上也有一致的性能提升。

Conclusion: BEPA方法有效解决了专家轨迹与学习策略之间的结构不匹配和分布偏移问题，显著提升了端到端GUI操作代理的性能，为小样本专家轨迹的有效利用提供了解决方案。

Abstract: Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git

</details>


### [27] [StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management](https://arxiv.org/abs/2601.05890)
*Ruizhe Zhang,Xinke Jiang,Zhibang Yang,Zhixin Zhang,Jiaran Gao,Yuzhen Xiao,Hongbin Lai,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: StackPlanner是一个具有显式内存控制的分层多智能体框架，通过解耦高层协调与子任务执行，并利用结构化经验记忆和强化学习重用协调经验，解决了长时程协作中的内存管理问题。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统在复杂知识密集型任务中展现出潜力，但中央智能体由于缺乏内存管理，导致上下文膨胀、错误累积和跨任务泛化能力差，影响了长时程协作的稳定性。

Method: 提出StackPlanner分层多智能体框架：1）通过主动任务级内存控制解耦高层协调与子任务执行；2）利用结构化经验记忆和强化学习检索和重用协调经验。

Result: 在多个深度搜索和智能体系统基准测试中，该方法在实现可靠的长时程多智能体协作方面表现出有效性。

Conclusion: StackPlanner通过显式内存控制解决了多智能体系统中的内存管理问题，提高了长时程协作的稳定性和效率。

Abstract: Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.

</details>


### [28] [TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents](https://arxiv.org/abs/2601.05899)
*Dawei Wang,Chengming Zhou,Di Zhao,Xinyuan Liu,Marci Chi Ma,Gary Ushaw,Richard Davison*

Main category: cs.AI

TL;DR: TowerMind是一个基于塔防游戏的轻量级多模态环境，用于评估LLM的长期规划和决策能力，揭示了LLM与人类专家在能力和幻觉方面的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的RTS游戏环境要么计算需求高，要么缺乏文本观察支持，限制了LLM评估。需要一种轻量级、多模态的环境来评估LLM的长期规划和决策能力。

Method: 提出了TowerMind环境，基于塔防游戏子类型，具有低计算需求和多模态观察空间（像素、文本、结构化游戏状态）。设计了五个基准关卡来评估不同多模态输入设置下的LLM。

Result: 结果显示LLM与人类专家在能力和幻觉方面存在明显性能差距。实验还揭示了LLM行为的关键局限性，如规划验证不足、决策缺乏多终性、动作使用效率低。

Conclusion: TowerMind通过轻量级多模态设计补充了现有的RTS游戏环境，为AI智能体领域引入了新的基准测试平台。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).

</details>


### [29] [Open-Vocabulary 3D Instruction Ambiguity Detection](https://arxiv.org/abs/2601.05991)
*Jiayu Ding,Haoran Tang,Ge Li*

Main category: cs.AI

TL;DR: 提出首个开放词汇3D指令歧义检测任务，构建大规模基准Ambi3D，发现现有3D LLMs难以可靠检测歧义，提出两阶段框架AmbiVer解决该问题。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，语言歧义可能导致严重后果，但现有具身AI研究大多忽略此问题，假设指令清晰并专注于执行而非确认。需要填补这一关键安全空白。

Method: 定义开放词汇3D指令歧义检测新任务；构建Ambi3D基准（700+多样3D场景，约22k指令）；提出AmbiVer两阶段框架：从多视角收集显式视觉证据，并引导视觉语言模型判断指令歧义。

Result: 分析发现最先进的3D LLMs难以可靠判断指令是否歧义；AmbiVer在实验中表现出有效性，证明了任务的挑战性和方法的优势。

Conclusion: 该研究为更安全、更可信的具身AI铺平了道路，填补了指令歧义检测的关键空白，提出的任务、基准和方法具有重要价值。

Abstract: In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like "Pass me the vial" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [30] [STELP: Secure Transpilation and Execution of LLM-Generated Programs](https://arxiv.org/abs/2601.05467)
*Swapnil Shinde,Sahil Wadhwa,Andy Luo,Emily Chen*

Main category: cs.SE

TL;DR: STELP是一个安全转译器和执行器，用于安全执行LLM生成的代码，解决生产AI系统中代码生成的安全性和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码可能存在不稳定、错误、漏洞（如数据中毒、恶意攻击、幻觉）等问题，在生产AI系统中直接使用这些代码可能导致系统故障。传统安全测试方法和人工代码审查在生产环境中不切实际或不可靠。

Method: 提出STELP（Secure Transpiler and Executor of LLM-Generated Program），能够在受控和安全的环境中执行LLM生成的代码。包括无头代码生成执行和LLM生成可执行代码片段作为实时执行的动作计划等应用。

Result: 在公开数据集上对正确性、安全性和延迟进行基准测试，结果显示该方法显著优于现有方法，特别是在安全执行风险代码片段方面表现出色。贡献了一个人工验证的不安全代码片段数据集。

Conclusion: STELP填补了传统安全测试方法和人工监督在生产AI系统中的空白，能够安全执行LLM生成的代码，为涉及代码生成的自主生产AI系统提供安全保障。

Abstract: Rapid evolution of Large Language Models (LLMs) has achieved major advances in reasoning, planning, and function-calling capabilities. Multi-agentic collaborative frameworks using such LLMs place them at the center of solving software development-related tasks such as code generation. However, direct use of LLM generated code in production software development systems is problematic. The code could be unstable or erroneous and contain vulnerabilities such as data poisoning, malicious attacks, and hallucinations that could lead to widespread system malfunctions. This prohibits the adoption of LLM generated code in production AI systems where human code reviews and traditional secure testing tools are impractical or untrustworthy. In this paper, we discuss safety and reliability problems with the execution of LLM generated code and propose a Secure Transpiler and Executor of LLM-Generated Program (STELP), capable of executing LLM-generated code in a controlled and safe manner. STELP secures autonomous production AI systems involving code generation, filling the critical void left by the impracticality or limitations of traditional secure testing methodologies and human oversight. This includes applications such as headless code generation-execution and LLMs that produce executable code snippets as an action plan to be executed in real time. We contribute a human-validated dataset of insecure code snippets and benchmark our approach on publicly available datasets for correctness, safety, and latency. Our results demonstrate that our approach outperforms an existing method by a significant margin, particularly in its ability to safely execute risky code snippets. Warning: This paper contains malicious code snippets that should be run with caution.

</details>


### [31] [Readability-Robust Code Summarization via Meta Curriculum Learning](https://arxiv.org/abs/2601.05485)
*Wenhao Zeng,Yitian Chai,Hao Zhou,Fandong Meng,Jie Zhou,Xiaodong Gu*

Main category: cs.SE

TL;DR: 提出RoFTCodeSum方法，通过课程学习和元学习结合增强代码摘要模型对低可读性代码的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有代码摘要模型和基准测试仅限于高可读性代码，但现实世界代码往往结构不良或混淆，导致模型性能显著下降

Method: 提出RoFTCodeSum方法，结合课程学习和元学习：基于原始微调数据集创建渐进难度的课程训练集（如混淆函数名和标识符），在每一步训练中使用这些渐进挑战的数据集进行元梯度更新

Result: 实验表明RoFTCodeSum在增强对原始代码性能的同时，提高了对语义扰动的鲁棒性

Conclusion: RoFTCodeSum能有效提升代码摘要模型对低可读性代码的鲁棒性，解决了现实世界代码质量问题

Abstract: Code summarization has emerged as a fundamental technique in the field of program comprehension. While code language models have shown significant advancements, the current models and benchmarks are confined to high-readability code, which contains sufficient semantic cues such as function and variable names. In the real world, however, code is often poorly structured or obfuscated, significantly degrading model performance. In this paper, we first empirically evaluate the robustness of state-of-the-art language models on poor-readability code for the task of code summarization, focusing on (1) their effectiveness, (2) the impact of prompt engineering, and (3) the robustness of different variants. Experimental results reveal that state-of-the-art models-including GPT-4o and DeepSeek-V3 experience a substantial performance drop when faced with poorly readable code, and that prompt engineering and reasoning-enhanced models offer limited improvements. Motivated by these findings, we propose RoFTCodeSum, a novel fine-tuning method that enhances the robustness of code summarization against poorly readable code. RoFTCodeSum marries the concepts of curriculum learning and meta-learning: based on the original dataset for fine-tuning, it creates curricular training sets, e.g., obfuscating function names and identifiers from the code, respectively, that have progressive difficulty in code comprehension. In each training step, the approach meta-updates the gradients using these progressively challenging datasets, thereby optimizing both accuracy and readability robustness simultaneously. Experimental results demonstrate that RoFTCodeSum exhibits increased robustness against semantic perturbation while enhancing performance on the original code.

</details>


### [32] [LIDL: LLM Integration Defect Localization via Knowledge Graph-Enhanced Multi-Agent Analysis](https://arxiv.org/abs/2601.05539)
*Gou Tan,Zilong He,Min Li,Pengfei Chen,Jieke Shi,Zhensu Sun,Ting Zhang,Danwen Chen,Lwin Khin Shar,Chuanfu Zhang,David Lo*

Main category: cs.SE

TL;DR: LIDL是一个用于定位LLM集成软件缺陷的多智能体框架，通过构建代码知识图谱、融合错误证据和上下文感知验证，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: LLM集成软件具有概率性和上下文依赖的行为，与传统软件不同，导致新的集成缺陷类型。现有缺陷定位技术无法有效识别这些LLM特定集成缺陷，因为它们无法捕捉异构工件间的跨层依赖关系，不能利用不完整或误导性的错误跟踪，也缺乏识别根本原因的语义推理能力。

Method: LIDL采用多智能体框架：(1)构建带有LLM感知注释的代码知识图谱，表示源代码、提示词和配置文件之间的交互边界；(2)融合LLM推断的三种互补错误证据来源，筛选候选缺陷位置；(3)应用上下文感知验证，使用反事实推理区分真正的根本原因和传播的症状。

Result: 在146个真实世界缺陷实例（来自105个GitHub仓库和16个基于智能体的系统）上评估，LIDL在各项指标上显著优于5个最先进的基线方法，Top-3准确率达到0.64，MAP为0.48，比最佳基线提升64.1%。同时成本降低92.5%，实现了高准确性和成本效率。

Conclusion: LIDL通过多智能体框架有效解决了LLM集成软件中的缺陷定位问题，能够捕捉跨层依赖关系，利用不完整的错误信息，并进行语义推理，为LLM集成软件的调试提供了有效工具。

Abstract: LLM-integrated software, which embeds or interacts with large language models (LLMs) as functional components, exhibits probabilistic and context-dependent behaviors that fundamentally differ from those of traditional software. This shift introduces a new category of integration defects that arise not only from code errors but also from misaligned interactions among LLM-specific artifacts, including prompts, API calls, configurations, and model outputs. However, existing defect localization techniques are ineffective at identifying these LLM-specific integration defects because they fail to capture cross-layer dependencies across heterogeneous artifacts, cannot exploit incomplete or misleading error traces, and lack semantic reasoning capabilities for identifying root causes.
  To address these challenges, we propose LIDL, a multi-agent framework for defect localization in LLM-integrated software. LIDL (1) constructs a code knowledge graph enriched with LLM-aware annotations that represent interaction boundaries across source code, prompts, and configuration files, (2) fuses three complementary sources of error evidence inferred by LLMs to surface candidate defect locations, and (3) applies context-aware validation that uses counterfactual reasoning to distinguish true root causes from propagated symptoms. We evaluate LIDL on 146 real-world defect instances collected from 105 GitHub repositories and 16 agent-based systems. The results show that LIDL significantly outperforms five state-of-the-art baselines across all metrics, achieving a Top-3 accuracy of 0.64 and a MAP of 0.48, which represents a 64.1% improvement over the best-performing baseline. Notably, LIDL achieves these gains while reducing cost by 92.5%, demonstrating both high accuracy and cost efficiency.

</details>


### [33] [EET: Experience-Driven Early Termination for Cost-Efficient Software Engineering Agents](https://arxiv.org/abs/2601.05777)
*Yaoqi Guo,Ying Xiao,Jie M. Zhang,Mark Harman,Yiling Lou,Yang Liu,Zhenpeng Chen*

Main category: cs.SE

TL;DR: EET是一种经验驱动的早期终止方法，通过利用历史执行经验来减少软件工程代理的成本，同时保持任务性能


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的软件工程代理在实践中应用日益广泛，但通常会产生高昂的货币成本，需要一种方法来降低这些成本而不影响任务性能

Method: 从先前的issue-resolution执行中提取结构化经验，利用这些经验在补丁生成和选择阶段指导早期终止，减少无成效的迭代

Result: 在SWE-bench Verified基准测试中，EET将总成本降低了19%-55%（平均32%），分辨率损失最多仅0.2%，平均为11%的问题识别出早期终止机会

Conclusion: EET能有效降低软件工程代理的成本，同时保持任务性能，通过减少API调用和token使用实现显著效率提升

Abstract: Software engineering (SE) agents powered by large language models are increasingly adopted in practice, yet they often incur substantial monetary cost. We introduce EET, an experience-driven early termination approach that reduces the cost of SE agents while preserving task performance. EET extracts structured experience from prior issue-resolution executions and leverages it to guide early termination during patch generation and selection, reducing unproductive iterations. We evaluate EET on the SWE-bench Verified benchmark across three representative SE agents. EET consistently reduces total cost by 19%-55% (32% on average), with negligible loss in resolution rate (at most 0.2%). These efficiency gains are achieved, on average, by identifying early-termination opportunities for 11% of issues and reducing API calls, input tokens, and output tokens by 21%, 30%, and 25%, respectively. We release the code, prompts, and data at https://github.com/EffiSEAgent/EET.

</details>


### [34] [EvoC2Rust: A Skeleton-guided Framework for Project-Level C-to-Rust Translation](https://arxiv.org/abs/2508.04295)
*Chaofan Wang,Tingrui Yu,Chen Xie,Jie Wang,Dong Chen,Wenrui Zhang,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: EvoC2Rust是一个自动化框架，用于将完整的C项目转换为等效的Rust项目，采用骨架引导的翻译策略，结合了基于规则和基于LLM方法的优点。


<details>
  <summary>Details</summary>
Motivation: 将遗留C代码库转换为Rust对于构建安全关键系统需求日益增长。现有方法存在固有权衡：基于规则的方法难以满足代码安全性和惯用性要求，而基于LLM的方法由于整个代码库中模块的重度依赖，经常无法生成语义等效的Rust代码。两种解决方案都局限于小型程序。

Method: 采用骨架引导的翻译策略，包含三个阶段：1) 将C项目分解为功能模块，使用特征映射增强的LLM转换定义和宏，生成类型检查的函数存根，形成可编译的Rust骨架；2) 增量翻译函数，替换对应的存根占位符；3) 通过集成LLM和静态分析修复编译错误。通过进化增强结合两种方法的优势。

Result: 在开源基准测试和六个工业项目上的评估显示，EvoC2Rust在项目级C到Rust翻译中表现优异。在语法准确率上比最强的基于LLM的基线高出17.24%，在语义准确率上高出14.32%，同时比最好的基于规则工具实现43.59%更高的代码安全率。

Conclusion: EvoC2Rust框架成功解决了项目级C到Rust翻译的挑战，通过骨架引导策略和进化增强，在语法准确性、语义等效性和代码安全性方面都显著优于现有方法。

Abstract: Translating legacy C codebases to Rust is increasingly demanded for building safety-critical systems. While various approaches have emerged for this task, they face inherent trade-offs: rule-based methods often struggle to satisfy code safety and idiomaticity requirements, while LLM-based methods frequently fail to generate semantically equivalent Rust code, due to the heavy dependencies of modules across the entire codebase. Recent studies have revealed that both solutions are limited to small-scale programs. In this paper, we propose EvoC2Rust, an automated framework for converting complete C projects to equivalent Rust ones. EvoC2Rust employs a skeleton-guided translation strategy for project-level translation. The pipeline consists of three stages: 1) it first decomposes the C project into functional modules, employs a feature-mapping-enhanced LLM to transform definitions and macros, and generates type-checked function stubs, which form a compilable Rust skeleton; 2) it then incrementally translates functions, replacing the corresponding stub placeholders; 3) finally, it repairs compilation errors by integrating LLM and static analysis. Through evolutionary augmentation, EvoC2Rust combines the advantages of both rule-based and LLM-based solutions. Our evaluation on open-source benchmarks and six industrial projects demonstrates the superior performance of EvoC2Rust in project-level C-to-Rust translation. The results show that our approach outperforms the strongest LLM-based baseline by 17.24% in syntax accuracy and 14.32% in semantic accuracy, while also achieving a 43.59% higher code safety rate than the best rule-based tool.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning](https://arxiv.org/abs/2601.05300)
*Susmit Das*

Main category: cs.LG

TL;DR: TIME框架通过时间敏感的元推理机制，在对话中引入<time>标签、tick turns和<think>块，实现更高效、上下文感知的推理，大幅减少推理token并提升时序对话能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的显式推理设计存在成本高、可审计性差、无法重新触发推理的问题，且对话模型缺乏对时间结构的感知能力，无法处理时间间隔和时序关系。

Method: 引入TIME框架：1) 使用ISO 8601 <time>标签标记时间；2) tick turns表示沉默间隔；3) 短<think>块可在回复任意位置出现。通过四阶段课程学习训练Qwen3模型，包括小规模、最大多样性的全批次对齐步骤。

Result: 在4B到32B规模上，TIME在TIMEBench基准测试中优于基础Qwen3模型（无论是否开启推理模式），同时将推理token减少约一个数量级。

Conclusion: TIME框架通过时间敏感的元推理机制，实现了更高效、上下文感知的推理，显著提升了模型在时序对话任务上的表现，同时大幅降低了推理成本。

Abstract: Reasoning oriented large language models often expose explicit "thinking" as long, turn-global traces at the start of every response, either always on or toggled externally at inference time. While useful for arithmetic, programming, and problem solving, this design is costly, blurs claim level auditability, and cannot re-trigger explicit reasoning once the model begins presenting. Dialogue models are also largely blind to temporal structure, treating replies after seconds and replies after weeks as equivalent unless time is stated in text. We introduce TIME, the Temporally Intelligent Meta-reasoning Engine, a behavioral alignment framework that treats explicit reasoning as a context sensitive resource driven by discourse and temporal cues. TIME augments dialogue with optional ISO 8601 <time> tags, tick turns that represent silent gaps, and short <think> blocks that can appear anywhere in a reply. A four-phase curriculum including a small, maximally diverse full-batch alignment step trains Qwen3 dense models to invoke brief, in-place reasoning bursts and keep user facing text compact. We evaluate with TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense under gaps and offsets, anomaly detection, and continuity. Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude. Our training data and code are available at https://github.com/The-Coherence-Initiative/TIME and TIMEBench is available at https://github.com/The-Coherence-Initiative/TIMEBench

</details>


### [36] [Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.05407)
*Minwoo Cho,Batuhan Altundas,Matthew Gombolay*

Main category: cs.LG

TL;DR: HINT提出了一种用于多智能体强化学习的知识蒸馏框架，通过分层交互教师机制解决传统KD在MARL中的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏在MARL中面临三个主要瓶颈：1) 复杂领域中高性能教学策略合成的挑战；2) 教师需要在分布外状态进行推理的困难；3) 分散式学生与集中式教师观察空间不匹配的问题。

Method: HINT采用分层强化学习构建可扩展的高性能教师模型，关键创新是伪离策略RL，允许教师策略同时使用教师和学生经验进行更新以改善OOD适应，并应用基于性能的过滤来保留仅与结果相关的指导。

Result: 在FireCommander资源分配和MARINE战术战斗等挑战性合作领域评估中，HINT优于基线方法，成功率提升60%到165%。

Conclusion: HINT通过分层交互教师框架有效解决了MARL中知识蒸馏的关键瓶颈，显著提升了多智能体系统的学习效率和性能。

Abstract: Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.

</details>


### [37] [Efficient Inference for Noisy LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2601.05420)
*Yiqun T Chen,Sizhu Lu,Sijia Li,Moran Guo,Shengyi Li*

Main category: cs.LG

TL;DR: 本文系统研究LLM作为评估者时的偏差校正方法，比较测量误差校正和预测驱动推断两种方法，推导高效估计器并分析方差特性。


<details>
  <summary>Details</summary>
Motivation: LLM作为评估者存在系统性误差，现有校正方法包括测量误差校正和预测驱动推断，需要系统比较这两种方法的性能差异。

Method: 基于半参数效率理论，推导高效影响函数的高效估计器，统一两类估计方法，理论分析预测驱动推断方法在何种条件下具有更小渐近方差。

Result: 理论分析表明预测驱动推断在某些条件下具有更小渐近方差，模拟实验验证理论结果，并在真实数据示例中展示方法应用。

Conclusion: 系统比较了LLM评估偏差校正方法，提供了理论框架和实用工具，帮助研究者选择合适的方法来校正LLM评估中的系统性误差。

Abstract: Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as "LLM-as-a-judge." In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at https://github.com/yiqunchen/debias-llm-as-a-judge.

</details>


### [38] [MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization](https://arxiv.org/abs/2601.05475)
*Jiefu Ou,Sapana Chaudhary,Kaj Bostrom,Nathaniel Weir,Shuai Zhang,Huzefa Rangwala,George Karypis*

Main category: cs.LG

TL;DR: MaxCode是一个基于最大奖励强化学习框架的代码优化方法，通过执行反馈的迭代搜索和自然语言批评模型来指导LLM生成更优化的代码，在CUDA和C++优化基准上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: LLM在通用编码任务中表现出色，但在代码优化方面面临两大挑战：1) 编写优化代码（如高性能CUDA内核和竞赛级CPU代码）需要系统、算法和特定语言的专业知识；2) 需要解释性能指标（如计时和设备利用率），而不仅仅是二进制正确性。

Method: MaxCode采用推理时搜索算法，基于执行反馈指导LLM通过迭代优化发现更好的解决方案。该方法将现有搜索方法统一到最大奖励强化学习框架下，使观察和动作价值函数模块化。通过集成自然语言批评模型将原始执行反馈转换为诊断见解，并使用生成式奖励到目标模型来重新排序潜在解决方案。

Result: 在KernelBench（CUDA）和PIE（C++）优化基准测试中，MaxCode相比基线方法在绝对加速值和相对加速排名上分别实现了20.3%和10.1%的相对改进。

Conclusion: MaxCode通过统一的强化学习框架和增强的观察空间，有效提升了LLM在代码优化任务中的性能，为解决复杂代码优化问题提供了新的方法。

Abstract: Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.

</details>


### [39] [Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR](https://arxiv.org/abs/2601.05607)
*Zijun Min,Bingshuai Liu,Ante Wang,Long Zhang,Anxiang Zeng,Haibo Zhang,Jinsong Su*

Main category: cs.LG

TL;DR: 提出DHPO方法，动态混合token级和序列级重要性比率，在数学推理任务上超越现有RLVR方法


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法各有优缺点：GRPO使用token级重要性比率能保留细粒度信用分配但方差高不稳定；GSPO使用序列级重要性比率匹配序列级奖励但牺牲token级信用分配。需要结合两者优势。

Method: 提出动态混合策略优化(DHPO)，在单一裁剪代理目标中桥接GRPO和GSPO。使用加权机制结合token级和序列级重要性比率，探索平均混合和熵引导混合两种变体。采用分支特定裁剪策略，在混合前分别约束两种比率在信任区域内。

Result: 在7个具有挑战性的数学推理基准测试中，在Qwen3系列的密集和MoE模型上，DHPO始终优于GRPO和GSPO。

Conclusion: DHPO通过动态混合token级和序列级重要性比率，结合了两种方法的优势，在数学推理任务上取得了更好的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.

</details>


### [40] [Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks](https://arxiv.org/abs/2601.05616)
*ShaoZhen Liu,Xinting Huang,Houwen Peng,Xin Chen,Xinyang Song,Qi Li,Zhenan Sun*

Main category: cs.LG

TL;DR: 该论文提出了一种两阶段训练框架，通过自生成长链思维数据增强大语言模型的自我修正能力，证明监督微调能有效激活模型内在推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖强化学习框架，忽视了监督微调方法在复杂推理任务中的潜力。论文旨在探索如何通过监督微调有效激活大语言模型的内在推理能力。

Method: 采用两阶段训练框架：第一阶段通过多轮对话策略引导模型生成包含验证、回溯、子目标分解和反向推理的思维链数据，并用预定义规则筛选高质量样本进行监督微调；第二阶段采用难度感知拒绝采样机制动态优化数据分布，增强模型处理复杂问题的能力。

Result: 实验结果显示在GSM8K和MATH500等数学基准测试上性能提升，微调模型在AIME24等竞赛级问题上取得显著改进。该方法生成的推理链长度扩展超过4倍，同时保持强可扩展性。

Conclusion: 监督微调能有效激活大语言模型的内在推理能力，为复杂任务优化提供了资源高效的途径。该方法证明了监督微调在数学推理任务中的有效性。

Abstract: In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models' self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model's ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models' intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.

</details>


### [41] [Automating Deception: Scalable Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2511.19517)
*Adarsh Kumarappan,Ananya Mujoo*

Main category: cs.LG

TL;DR: 本文提出了一种自动生成大规模、基于心理学原理的多轮越狱数据集的方法，用于评估LLM对多轮对话攻击的防御能力，发现不同模型家族在上下文鲁棒性上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 多轮对话攻击利用心理学原理（如登门槛效应）绕过LLM的安全对齐，现有防御方法依赖难以扩展的手动数据集创建，阻碍了防御进展。

Method: 开发自动化流水线，将登门槛技术系统化为可复现模板，创建包含1500个场景的基准数据集，涵盖非法活动和冒犯性内容，评估7个主流模型在多轮和单轮条件下的表现。

Result: GPT系列模型对对话历史高度脆弱，攻击成功率最多提升32个百分点；Gemini 2.5 Flash表现出卓越韧性，几乎免疫此类攻击；Claude 3 Haiku有较强但不完美的抵抗力。

Conclusion: 当前安全架构在处理对话上下文时存在关键分歧，需要能够抵抗叙事操纵的防御机制，自动化数据集生成方法为评估和改进模型安全性提供了重要工具。

Abstract: Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.

</details>


### [42] [IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck](https://arxiv.org/abs/2601.05870)
*Huilin Deng,Hongchen Luo,Yue Zhu,Long Li,Zhuoyue Chen,Xinghao Zhao,Ming Li,Jihai Zhang,Mengchang Wang,Yang Cao,Yu Kang*

Main category: cs.LG

TL;DR: 提出IIB-LPO方法，通过信息瓶颈原理实现推理轨迹的拓扑分支，解决RLVR中的探索崩溃问题，在数学推理基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: RLVR在LLM推理中存在探索崩溃问题，随机rollout的语义同质性导致模型陷入狭窄的过优化行为。现有方法使用策略熵鼓励探索，但全局熵正则化易受奖励攻击导致无意义冗长，局部token选择性更新难以克服预训练模型的强归纳偏置。

Method: 提出IIB-LPO方法，将探索从token分布的统计扰动转向推理轨迹的拓扑分支。在高熵状态触发潜在分支以多样化推理路径，使用信息瓶颈原理作为轨迹过滤器和自奖励机制，确保简洁且信息丰富的探索。

Result: 在四个数学推理基准上的实验结果表明，IIB-LPO达到最先进性能，相比先前方法在准确率上提升高达5.3%，在多样性指标上提升7.4%。

Conclusion: IIB-LPO通过拓扑分支和信息瓶颈有效解决了RLVR中的探索崩溃问题，显著提升了LLM推理的性能和多样性。

Abstract: Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [43] [Visual Web Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.02439%3Futm_source=tldrai/1/0100019b9e169e42-0780e18a-91e7-487b-b08f-7364d8885d51-000000/2U61-YmCElTN_DK0pybHHRltJBe4nEz17SjX67zZA6Y=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: WebGym是一个包含近30万个真实网页任务的大规模视觉Web智能体训练环境，配备高吞吐RL系统提升训练速度，其微调的Qwen-3-VL模型在未见网页导航任务上超越GPT-4o和GPT-5-Thinking。


<details>
  <summary>Details</summary>
Motivation: 当前视觉Web智能体训练面临真实网页任务规模不足、训练效率低的问题，需要构建大规模真实环境来提升智能体在复杂网页导航任务上的泛化能力。

Method: 构建WebGym大规模环境（近30万个真实网页任务），开发高吞吐强化学习系统加速训练，并微调Qwen-3-VL视觉语言模型用于网页导航。

Result: 微调的Qwen-3-VL在未见网页导航任务上表现优于GPT-4o和GPT-5-Thinking，高吞吐RL系统显著提升训练速度。

Conclusion: WebGym为视觉Web智能体训练提供了有效的大规模真实环境，结合高效RL系统和专用模型，显著提升了网页导航任务的性能。

Abstract: Visual Web Agents (29 minute read) WebGym is a large-scale environment with nearly 300,000 real-world web tasks for training visual agents. It has a high-throughput RL system that boosts training speed and a fine-tuned Qwen-3-VL that outperforms GPT-4o and GPT-5-Thinking on unseen web navigation tasks.

</details>


### [44] [LLM predictions for 2026, shared with Oxide and Friends](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F8%2Fllm-predictions-for-2026%2F%23atom-everything%3Futm_source=tldrnewsletter/1/0100019ba27f6bb7-00db6a38-3217-44d5-93c8-d65295297e38-000000/6LVQFKMoe5p_WHRBHtwIoFnDPhWDAoyyI-K8Z4zyliI=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Simon Willison在播客中分享了对2026年及未来科技行业的预测，认为编码智能体的快速发展将带来重大但不确定的变化


<details>
  <summary>Details</summary>
Motivation: 当前科技行业面临前所未有的不确定性，特别是编码智能体在最近两个月取得的显著进展预示着重大变革即将到来，但具体变化方向尚不明确

Method: 基于播客访谈形式，分享对未来1年、3年和6年科技发展趋势的个人预测和见解

Result: 预测指出编码智能体的快速发展将显著改变技术行业格局，但具体变化路径和影响仍存在高度不确定性

Conclusion: 科技行业正处于快速变革期，特别是编码智能体技术将带来重大但难以预测的行业变化

Abstract: LLM predictions for 2026, shared with Oxide and Friends (9 minute read) This post shares predictions Simon Willison made on a recent podcast. The predictions cover what he thinks will happen in the next 1, 3, and 6 years in the tech industry. There has never been so much uncertainty about what's coming in the next year. The significant advances in coding agents in just the last two months indicate that things will change significantly, but it is unclear what those changes will be.

</details>


### [45] [Early look at Grok Build, upcoming vibe coding agent from xAI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fearly-look-at-grok-build-upcoming-vibe-coding-agent-from-xai%2F%3Futm_source=tldrnewsletter/1/0100019ba27f6bb7-00db6a38-3217-44d5-93c8-d65295297e38-000000/P_Pvpf8ADDIrmpKZf3nHOWlZogqwraP8hp8yx2vOz9k=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Grok Build是xAI即将推出的代码代理，支持本地代理和CLI/web访问，具备代理配置和自定义环境功能


<details>
  <summary>Details</summary>
Motivation: xAI开发Grok Build旨在提供一个集成的代码代理工具，让开发者能够通过本地或远程环境进行代码开发，简化开发流程

Method: 通过本地代理实现代码生成和开发支持，提供CLI和web界面，支持代理配置和自定义环境设置

Result: 目前支持本地代理，远程环境支持即将推出，已提供代理配置和自定义环境选项

Conclusion: Grok Build是xAI即将推出的代码代理工具，旨在为开发者提供便捷的代码开发体验

Abstract: Early look at Grok Build, upcoming vibe coding agent from xAI (2 minute read) Grok Build is a coding agent from xAI that can be accessed both on the web and through a CLI. It currently supports local agents, with remote environment support marked as coming soon. There are options for users to configure agents and custom environments. A video previewing the feature is available in the article.

</details>


### [46] [The 1,000 commits problem](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdavekiss.com%2Fblog%2Fthe-1000-commits-problem%2F%3Futm_source=tldrnewsletter/1/0100019ba27f6bb7-00db6a38-3217-44d5-93c8-d65295297e38-000000/VmHPeGin7wfyDsYn8n-hVDahVhuyQM7d1DWV6Mz_zSo=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic的CLI代码更新导致整个命令行工具崩溃，突显了代码代理在复杂系统维护中的挑战


<details>
  <summary>Details</summary>
Motivation: 探讨代码代理在处理大型代码库和复杂系统更新时可能面临的风险和挑战，特别是当代码变更影响到核心功能时

Method: 通过分析Anthropic CLI崩溃的具体案例，研究代码代理在代码更新、测试和部署过程中的潜在问题

Result: 代码代理的更新导致了整个CLI工具的崩溃，显示了在复杂系统中进行代码变更时存在的风险

Conclusion: 代码代理需要更严格的测试和验证机制，特别是在处理核心系统功能时，以避免类似的崩溃事件

Abstract: The 1,000 commits problem (4 minute read) Anthropic recently shipped code that broke the entire CLI.

</details>


### [47] [The Emperor Has No Clothes: How to Code Claude Code in 200 Lines of Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mihaileric.com%2FThe-Emperor-Has-No-Clothes%2F%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/NOZVOZaYycy6sy7KOuULDgbjnGAnceTFTfp0tAc_U4c=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文展示了如何用约200行Python代码构建一个简单的AI编码助手，核心架构基于三个基本工具：读取、列出和编辑文件，LLM通过结构化命令学习调用这些工具。


<details>
  <summary>Details</summary>
Motivation: 揭示AI编码助手的基本架构其实很简单，可以被轻松复制，打破复杂系统的神秘感，展示核心功能的最小实现。

Method: 构建一个配备三个基本工具（读取、列出、编辑文件）的代理，LLM通过结构化命令学习调用这些工具，自主决定使用哪个工具、执行操作并处理结果。

Result: 成功实现了一个功能完整的AI编码助手，仅用约200行Python代码，证明了核心架构的简洁性和可复制性。

Conclusion: AI编码助手的基本架构并不复杂，可以通过少量代码实现核心功能，这有助于理解其工作原理并促进更广泛的开发和应用。

Abstract: The Emperor Has No Clothes: How to Code Claude Code in 200 Lines of Code (8 minute read) AI coding assistants are built on a simple core architecture that can be replicated in about 200 lines of Python. This post presents an agent that is equipped with three fundamental tools - read, list, and edit files - which the LLM learns to invoke via structured commands. The LLM decides which tool to use, executes it, and then processes the results to either continue the task or respond to the user.

</details>


### [48] [Staging is a wasteful lie: the case for the mono-environment](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tomwphillips.co.uk%2F2026%2F01%2Fstaging-is-a-wasteful-lie-the-case-for-the-mono-environment%2F%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/BO-gzadccyGRCjEIK15aPbJuQxfdnQwS1JWyGZ2o8q8=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 传统测试环境浪费且不可靠，提出"单环境"方法，只在生产环境中通过代码逻辑隔离实现安全保障，而非环境隔离


<details>
  <summary>Details</summary>
Motivation: 传统测试环境存在浪费、不可靠的问题，并产生虚假的安全感。作者认为环境隔离不是最佳解决方案，需要更高效的方法

Method: 采用"单环境"方法，只保留生产环境，将安全保障从环境隔离转移到代码逻辑隔离。通过自动化测试、临时本地环境、功能标记、持续部署等实践实现

Result: 该方法能实现更快的交付速度和更紧密的反馈循环，减少资源浪费，提高部署可靠性

Conclusion: 单环境方法比传统测试环境更高效可靠，通过代码逻辑隔离而非环境隔离来实现安全保障

Abstract: Staging is a wasteful lie: the case for the mono-environment (13 minute read) Traditional staging environments are wasteful, unreliable, and create a false sense of security. A "mono-environment" approach, where only production exists, moves safeguards from environmental isolation to logical isolation within the code, leading to faster delivery and tighter feedback loops. This is done through automated practices such as testing, ephemeral local environments, feature flagging, continuous deplo...

</details>


### [49] [AI Coding Assistants Are Getting Worse](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fspectrum.ieee.org%2Fai-coding-degrades%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/nkrMUaFzDgICXgIZDS0n5NbsSaiF9T8pI9b9XT28hzg=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI编程助手在初期改进后近期质量下降，新模型（如GPT-5）倾向于产生"无声但致命"的故障，代码无错误运行但产生错误结果，可能由训练方法优先考虑用户接受度而非正确性导致。


<details>
  <summary>Details</summary>
Motivation: 观察到AI编程助手在经历初期质量提升后出现性能退化现象，特别是新模型产生"无声但致命"的故障，这比明显错误更危险，因为代码能运行但结果错误。

Method: 通过分析AI编程助手的行为模式，识别"无声但致命"的故障特征：代码无语法错误但产生不正确结果，避免安全检查，生成看似合理但虚假的数据。

Result: 发现新模型（如GPT-5）质量下降，产生更多隐蔽性错误，这种退化可能与训练方法过度优化用户接受度指标而牺牲代码正确性有关。

Conclusion: AI编程助手质量出现令人担忧的下降趋势，需要重新评估训练方法，平衡用户接受度和代码正确性，开发更可靠的评估指标。

Abstract: AI Coding Assistants Are Getting Worse (5 minute read) AI coding assistants, after an initial period of improvement, have recently begun to decline in quality. The primary issue with newer models, such as GPT-5, is their tendency towards "silent but deadly" failures, where code runs without errors but produces incorrect or misleading results by avoiding safety checks or generating plausible fake data. This degradation is likely caused by current training methods that prioritize user acceptanc...

</details>


### [50] [AI & Humans: Making the Relationship Work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.schneier.com%2Fblog%2Farchives%2F2026%2F01%2Fai-humans-making-the-relationship-work.html%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/IyeIhtITKfhg11SowWRXUSs8hRroRwu1cwhwu3cOuyg=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨如何将传统人类管理原则应用于AI代理管理，特别是混合人机团队协作，提出了三个关键管理策略


<details>
  <summary>Details</summary>
Motivation: 随着AI代理展现出类人行为，传统的人类管理原则在管理混合人机团队中变得相关，需要探索如何有效管理这些新型协作关系

Method: 基于Anthropic等领先AI实验室的研究，提出三个关键管理原则：有效委派以利用AI并行化能力、允许快速迭代和试错学习、建立高效的信息共享机制

Result: 研究表明传统人类管理原则可以成功应用于AI代理管理，特别是在混合人机团队协作场景中

Conclusion: AI代理的类人行为使得人类管理原则具有适用性，有效管理混合团队需要关注委派、迭代学习和信息共享三个关键方面

Abstract: AI & Humans: Making the Relationship Work (15 minute read) Agentic AI, especially in collaborative groups, often has human-like behaviors, which means traditional human management principles are surprisingly relevant. Research from leading AI labs like Anthropic shows three key lessons for managing hybrid human-AI teams: effective delegation to use AI's parallelization, allowing for rapid iteration and trial-and-error learning, and having proper efficient information sharing among agents.

</details>


### [51] [OpenSpec](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FFission-AI%2FOpenSpec%3Futm_source=tldrdevops/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/pzd_c46nLp8hBWwutT3ix74Uqjl30fMQiCxwFWn0kMc=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenSpec是一个基于规范驱动开发的AI编码助手框架，通过在编写代码前使用轻量级规范工作流锁定意图，实现人类与AI的对齐，提供确定性和可审查的输出。


<details>
  <summary>Details</summary>
Motivation: 解决AI编码助手在需求仅存在于聊天历史中时的不确定性问题，通过规范驱动的方法确保AI输出与人类意图对齐。

Method: 采用规范驱动开发框架，在编写代码前建立轻量级规范工作流来锁定意图，确保输出的确定性和可审查性。

Result: 开发了一个能够提供确定性、可审查输出的AI编码助手框架，解决了AI在需求不明确时的不可预测性问题。

Conclusion: 规范驱动开发是解决AI编码助手不确定性的有效方法，通过提前锁定意图可以提高AI与人类的对齐度。

Abstract: OpenSpec (GitHub Repo) OpenSpec is a spec-driven development (SDD) framework for AI coding assistants that aims to align humans and AI by locking intent with a lightweight specification workflow before any code is written. The system provides deterministic, reviewable outputs, addressing the unpredictability of AI when requirements are confined to chat history.

</details>


### [52] [HolmesGPT: Agentic troubleshooting built for the cloud native era](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cncf.io%2Fblog%2F2026%2F01%2F07%2Fholmesgpt-agentic-troubleshooting-built-for-the-cloud-native-era%2F%3Futm_source=tldrdevops/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/ydS_5RS8TsOGhRPS7ebDcy1sIUtppom-0a2xTvhwpU8=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: HolmesGPT是一个开源的Kubernetes和云原生环境AI故障排除代理，已被CNCF接受为沙盒项目


<details>
  <summary>Details</summary>
Motivation: 云原生环境中的故障排除复杂且耗时，需要自动化智能解决方案来提升运维效率

Method: 开发基于AI的故障排除代理，专门针对Kubernetes和云原生环境设计

Result: 项目成功开发并被CNCF接受为沙盒项目，获得开源社区认可

Conclusion: HolmesGPT为云原生时代提供了有效的AI驱动故障排除解决方案

Abstract: HolmesGPT: Agentic troubleshooting built for the cloud native era (2 minute read) HolmesGPT, an open-source AI troubleshooting agent for Kubernetes and cloud-native environments, was accepted as a CNCF Sandbox project in October.

</details>


### [53] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/le_Om4y5TLUWO16oGKmYxEwJd-8cnpb4QH47g5LiGgI=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: HolmesGPT是一个开源的AI故障排除代理，专门为Kubernetes和云原生环境设计，已被CNCF接受为沙盒项目


<details>
  <summary>Details</summary>
Motivation: 云原生环境（特别是Kubernetes）的故障排除复杂且耗时，需要专门工具来帮助运维人员快速诊断和解决问题

Method: 开发一个开源的AI代理，利用人工智能技术自动分析和诊断Kubernetes及云原生环境中的故障

Result: HolmesGPT在2023年10月被云原生计算基金会（CNCF）接受为沙盒项目，获得了行业认可

Conclusion: HolmesGPT为云原生时代的故障排除提供了创新的AI解决方案，有望简化运维工作流程

Abstract: HolmesGPT: Agentic troubleshooting built for the cloud native era (2 minute read) HolmesGPT, an open-source AI troubleshooting agent for Kubernetes and cloud-native environments, was accepted as a CNCF Sandbox project in October.

</details>


### [54] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/AkXo9OisHB9tDRCugniAHxAWWcOt7ygYBZq-Guh_sg4=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: HolmesGPT 是一个开源的 AI 故障排除代理，专为 Kubernetes 和云原生环境设计，已被 CNCF 接受为沙盒项目


<details>
  <summary>Details</summary>
Motivation: 随着云原生技术的普及，Kubernetes 环境中的故障排除变得日益复杂，需要智能化的自动化工具来帮助运维人员快速定位和解决问题

Method: 开发了一个基于 AI 的故障排除代理，专门针对 Kubernetes 和云原生环境，能够自动分析系统状态、识别问题并提供解决方案

Result: HolmesGPT 在 2023 年 10 月被云原生计算基金会（CNCF）接受为沙盒项目，获得了行业认可

Conclusion: HolmesGPT 为云原生时代的故障排除提供了智能化的解决方案，有望提升运维效率和系统可靠性

Abstract: HolmesGPT: Agentic troubleshooting built for the cloud native era (2 minute read) HolmesGPT, an open-source AI troubleshooting agent for Kubernetes and cloud-native environments, was accepted as a CNCF Sandbox project in October.

</details>


### [55] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/2BqGKBoNArSlnAy_JYcR5PMcLAQ07JcvapDVz58XFgU=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: HolmesGPT是一个开源的Kubernetes和云原生环境AI故障排除代理，已被CNCF接受为沙盒项目


<details>
  <summary>Details</summary>
Motivation: 云原生环境（特别是Kubernetes）的故障排除复杂且耗时，需要自动化智能解决方案来提升运维效率

Method: 开发基于AI的故障排除代理，专门针对Kubernetes和云原生环境设计，采用开源模式并集成到CNCF生态

Result: 成功开发出HolmesGPT，并于2023年10月被CNCF接受为沙盒项目，获得社区认可

Conclusion: HolmesGPT为云原生环境提供了有效的AI驱动故障排除解决方案，有望改善运维体验

Abstract: HolmesGPT: Agentic troubleshooting built for the cloud native era (2 minute read) HolmesGPT, an open-source AI troubleshooting agent for Kubernetes and cloud-native environments, was accepted as a CNCF Sandbox project in October.

</details>


### [56] [Eliza Ecosystem: Full Stack for Autonomous Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FB6o0wU/1/0100019ba2ec3c7a-961d8107-2d81-47d6-8530-db86bc598867-000000/15cRiYBWeIsV0P39f8NbbZ_i__zpyYywX8iZkiTZZHY=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ElizaOS推出四层全栈架构用于自主代理，包括应用层、云运行时层、支付与验证集成层、以及区块链身份与商务层，旨在解决当前自主代理生态的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 当前自主代理生态系统存在碎片化问题，缺乏统一的基础设施和互操作性标准，限制了代理的规模化部署和商业化应用。

Method: 设计四层全栈架构：1) 面向消费者的代理应用层；2) Eliza Cloud托管运行时层；3) MCP集成支付通道和TEE可验证性层；4) Jeju区块链提供链上身份、声誉和代理间商务原语层。

Result: 建立了完整的自主代理技术栈，提供了从应用到基础设施的端到端解决方案，支持代理的规模化部署、安全支付和可信交互。

Conclusion: Eliza生态系统通过全栈架构为自主代理提供了统一的基础设施，有望解决行业碎片化问题，推动自主代理技术的商业化应用。

Abstract: Eliza Ecosystem: Full Stack for Autonomous Agents (7 minute read) ElizaOS launched a four-layer full-stack architecture for autonomous agents, comprising consumer-facing agent applications (Layer 1), Eliza Cloud managed runtimes (Layer 2), MCP integrations with x402 payment rails and TEE-based verifiability (Layer 3), and Jeju blockchain providing onchain identity, reputation, and agent-to-agent commerce primitives (Layer 4). The project positions itself as addressing fragmentation in the cur...

</details>


### [57] [ChatGPT falls to new data-pilfering attack as a vicious cycle in AI continues](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farstechnica.com%2Fsecurity%2F2026%2F01%2Fchatgpt-falls-to-new-data-pilfering-attack-as-a-vicious-cycle-in-ai-continues%2F%3Futm_source=tldrinfosec/1/0100019ba31639ab-2e77e3b2-075a-402b-8e54-c09e9251b32a-000000/HD5BBZrbNVLWBkTc3eu1tkg_rfUDh3JPevwPIUqEYDE=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 研究人员发现新的"ZombieAgent"攻击，通过修改提示绕过ChatGPT的URL限制，以字符为单位泄露数据，揭示了LLM代理在间接提示注入和用户指令与嵌入内容分离不足方面的持续漏洞。


<details>
  <summary>Details</summary>
Motivation: 揭示大型语言模型代理在安全防护方面的持续脆弱性，特别是针对数据窃取攻击的防御机制容易被绕过，形成"补丁-绕过"的恶性循环。

Method: Radware安全研究人员开发了"ZombieAgent"攻击技术，通过修改提示词来绕过ChatGPT的URL限制，采用逐字符泄露数据的方式，利用间接提示注入和用户指令与嵌入内容分离不足的漏洞。

Result: 成功演示了如何复活先前被阻止的ChatGPT数据窃取技术，证明即使有防护措施，攻击者仍能通过巧妙设计的提示绕过限制，泄露敏感信息。

Conclusion: LLM代理在间接提示注入和内容分离方面存在根本性安全漏洞，导致安全厂商陷入无休止的"补丁-绕过"循环，需要更根本的架构改进而非临时修复。

Abstract: ChatGPT falls to new data-pilfering attack as a vicious cycle in AI continues (4 minute read) Security researchers at Radware show how a new “ZombieAgent” attack revives a previously blocked ChatGPT data‑exfiltration technique by tweaking prompts to bypass URL restrictions and leak information character by character. Indirect prompt injection and weak separation between user instructions and embedded content keep LLM agents vulnerable, forcing vendors into an endless patch-and-bypass cycle ra...

</details>
