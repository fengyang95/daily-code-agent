{"id": "2602.09138", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09138", "abs": "https://arxiv.org/abs/2602.09138", "authors": ["Haitao Jiang", "Lin Ge", "Hengrui Cai", "Rui Song"], "title": "PABU: Progress-Aware Belief Update for Efficient LLM Agents", "comment": null, "summary": "Large Language Model (LLM) agents commonly condition actions on full action-observation histories, which introduce task-irrelevant information that easily leads to redundant actions and higher inference cost. We propose Progress-Aware Belief Update (PABU), a belief-state framework that compactly represents an agent's state by explicitly modeling task progress and selectively retaining past actions and observations. At each step, the agent predicts its relative progress since the previous round and decides whether the newly encountered interaction should be stored, conditioning future decisions only on the retained subset. Across eight environments in the AgentGym benchmark, and using identical training trajectories, PABU achieves an 81.0% task completion rate, outperforming previous State of the art (SoTA) models with full-history belief by 23.9%. Additionally, PABU's progress-oriented action selection improves efficiency, reducing the average number of interaction steps to 9.5, corresponding to a 26.9% reduction. Ablation studies show that both explicit progress prediction and selective retention are necessary for robust belief learning and performance gains.", "AI": {"tldr": "PABU\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4efb\u52a1\u8fdb\u5ea6\u548c\u9009\u62e9\u6027\u4fdd\u7559\u5386\u53f2\u4fe1\u606f\uff0c\u51cf\u5c11LLM\u667a\u80fd\u4f53\u4e2d\u7684\u5197\u4f59\u52a8\u4f5c\u548c\u63a8\u7406\u6210\u672c\uff0c\u5728AgentGym\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\u5e76\u964d\u4f4e\u4ea4\u4e92\u6b65\u9aa4\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u901a\u5e38\u57fa\u4e8e\u5b8c\u6574\u7684\u52a8\u4f5c-\u89c2\u5bdf\u5386\u53f2\u6765\u51b3\u7b56\uff0c\u8fd9\u4f1a\u5f15\u5165\u4efb\u52a1\u65e0\u5173\u4fe1\u606f\uff0c\u5bfc\u81f4\u5197\u4f59\u52a8\u4f5c\u548c\u66f4\u9ad8\u7684\u63a8\u7406\u6210\u672c\u3002\u9700\u8981\u66f4\u7d27\u51d1\u7684\u72b6\u6001\u8868\u793a\u65b9\u6cd5\u6765\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51faProgress-Aware Belief Update (PABU)\u4fe1\u5ff5\u72b6\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4efb\u52a1\u8fdb\u5ea6\u548c\u9009\u62e9\u6027\u4fdd\u7559\u8fc7\u53bb\u7684\u52a8\u4f5c\u548c\u89c2\u5bdf\u6765\u7d27\u51d1\u8868\u793a\u667a\u80fd\u4f53\u72b6\u6001\u3002\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\uff0c\u667a\u80fd\u4f53\u9884\u6d4b\u81ea\u4e0a\u4e00\u8f6e\u4ee5\u6765\u7684\u76f8\u5bf9\u8fdb\u5ea6\uff0c\u5e76\u51b3\u5b9a\u662f\u5426\u5b58\u50a8\u65b0\u9047\u5230\u7684\u4ea4\u4e92\uff0c\u4ec5\u57fa\u4e8e\u4fdd\u7559\u7684\u5b50\u96c6\u8fdb\u884c\u672a\u6765\u51b3\u7b56\u3002", "result": "\u5728AgentGym\u57fa\u51c6\u6d4b\u8bd5\u7684\u516b\u4e2a\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u8bad\u7ec3\u8f68\u8ff9\uff0cPABU\u5b9e\u73b0\u4e8681.0%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u6bd4\u57fa\u4e8e\u5b8c\u6574\u5386\u53f2\u4fe1\u5ff5\u7684\u5148\u524dSOTA\u6a21\u578b\u9ad8\u51fa23.9%\u3002PABU\u7684\u8fdb\u5ea6\u5bfc\u5411\u52a8\u4f5c\u9009\u62e9\u5c06\u5e73\u5747\u4ea4\u4e92\u6b65\u9aa4\u6570\u51cf\u5c11\u52309.5\u6b65\uff0c\u5bf9\u5e9426.9%\u7684\u51cf\u5c11\u3002", "conclusion": "PABU\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u8fdb\u5ea6\u9884\u6d4b\u548c\u9009\u62e9\u6027\u4fdd\u7559\u673a\u5236\uff0c\u6709\u6548\u63d0\u9ad8\u4e86LLM\u667a\u80fd\u4f53\u7684\u6548\u7387\u548c\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e24\u4e2a\u7ec4\u4ef6\u5bf9\u4e8e\u7a33\u5065\u7684\u4fe1\u5ff5\u5b66\u4e60\u548c\u6027\u80fd\u63d0\u5347\u90fd\u662f\u5fc5\u8981\u7684\u3002", "topic": "agent analysis"}}
{"id": "2602.09159", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.09159", "abs": "https://arxiv.org/abs/2602.09159", "authors": ["Yichen Wu", "Yujin Oh", "Sangjoon Park", "Kailong Fan", "Dania Daye", "Hana Farzaneh", "Xiang Li", "Raul Uppot", "Quanzheng Li"], "title": "CoMMa: Contribution-Aware Medical Multi-Agents From A Game-Theoretic Perspective", "comment": "9 pages, 3 figures", "summary": "Recent multi-agent frameworks have broadened the ability to tackle oncology decision support tasks that require reasoning over dynamic, heterogeneous patient data. We propose Contribution-Aware Medical Multi-Agents (CoMMa), a decentralized LLM-agent framework in which specialists operate on partitioned evidence and coordinate through a game-theoretic objective for robust decision-making. In contrast to most agent architectures relying on stochastic narrative-based reasoning, CoMMa utilizes deterministic embedding projections to approximate contribution-aware credit assignment. This yields explicit evidence attribution by estimating each agent's marginal utility, producing interpretable and mathematically grounded decision pathways with improved stability. Evaluated on diverse oncology benchmarks, including a real-world multidisciplinary tumor board dataset, CoMMa achieves higher accuracy and more stable performance than data-centralized and role-based multi-agents baselines.", "AI": {"tldr": "CoMMa\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u533b\u7597\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u76ee\u6807\u548c\u786e\u5b9a\u6027\u5d4c\u5165\u6295\u5f71\u5b9e\u73b0\u8d21\u732e\u611f\u77e5\u7684\u4fe1\u7528\u5206\u914d\uff0c\u5728\u80bf\u7624\u5b66\u51b3\u7b56\u652f\u6301\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u968f\u673a\u53d9\u4e8b\u63a8\u7406\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728\u5904\u7406\u9700\u8981\u52a8\u6001\u3001\u5f02\u6784\u60a3\u8005\u6570\u636e\u7684\u80bf\u7624\u5b66\u51b3\u7b56\u652f\u6301\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8d21\u732e\u611f\u77e5\u533b\u7597\u591a\u667a\u80fd\u4f53\uff08CoMMa\uff09\u6846\u67b6\uff0c\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u67b6\u6784\uff0c\u4e13\u5bb6\u5728\u5206\u533a\u8bc1\u636e\u4e0a\u64cd\u4f5c\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u76ee\u6807\u534f\u8c03\uff0c\u5229\u7528\u786e\u5b9a\u6027\u5d4c\u5165\u6295\u5f71\u8fd1\u4f3c\u8d21\u732e\u611f\u77e5\u4fe1\u7528\u5206\u914d\uff0c\u4f30\u8ba1\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u8fb9\u9645\u6548\u7528\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u80bf\u7624\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u771f\u5b9e\u4e16\u754c\u591a\u5b66\u79d1\u80bf\u7624\u59d4\u5458\u4f1a\u6570\u636e\u96c6\uff09\u4e2d\uff0cCoMMa\u6bd4\u6570\u636e\u96c6\u4e2d\u5316\u548c\u57fa\u4e8e\u89d2\u8272\u7684\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "CoMMa\u901a\u8fc7\u8d21\u732e\u611f\u77e5\u4fe1\u7528\u5206\u914d\u63d0\u4f9b\u4e86\u663e\u5f0f\u8bc1\u636e\u5f52\u56e0\uff0c\u4ea7\u751f\u53ef\u89e3\u91ca\u4e14\u6570\u5b66\u57fa\u7840\u7684\u51b3\u7b56\u8def\u5f84\uff0c\u5728\u533b\u7597\u51b3\u7b56\u652f\u6301\u4e2d\u5177\u6709\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.09163", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.09163", "abs": "https://arxiv.org/abs/2602.09163", "authors": ["Xingjian Zhang", "Sophia Moylan", "Ziyang Xiong", "Qiaozhu Mei", "Yichen Luo", "Jiaqi W. Ma"], "title": "FlyAOC: Evaluating Agentic Ontology Curation of Drosophila Scientific Knowledge Bases", "comment": null, "summary": "Scientific knowledge bases accelerate discovery by curating findings from primary literature into structured, queryable formats for both human researchers and emerging AI systems. Maintaining these resources requires expert curators to search relevant papers, reconcile evidence across documents, and produce ontology-grounded annotations - a workflow that existing benchmarks, focused on isolated subtasks like named entity recognition or relation extraction, do not capture. We present FlyBench to evaluate AI agents on end-to-end agentic ontology curation from scientific literature. Given only a gene symbol, agents must search and read from a corpus of 16,898 full-text papers to produce structured annotations: Gene Ontology terms describing function, expression patterns, and historical synonyms linking decades of nomenclature. The benchmark includes 7,397 expert-curated annotations across 100 genes drawn from FlyBase, the Drosophila (fruit fly) knowledge base. We evaluate four baseline agent architectures: memorization, fixed pipeline, single-agent, and multi-agent. We find that architectural choices significantly impact performance, with multi-agent designs outperforming simpler alternatives, yet scaling backbone models yields diminishing returns. All baselines leave substantial room for improvement. Our analysis surfaces several findings to guide future development; for example, agents primarily use retrieval to confirm parametric knowledge rather than discover new information. We hope FlyBench will drive progress on retrieval-augmented scientific reasoning, a capability with broad applications across scientific domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86FlyBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u79d1\u5b66\u6587\u732e\u4e2d\u8fdb\u884c\u7aef\u5230\u7aef\u672c\u4f53\u8bba\u7b56\u5c55\u7684\u80fd\u529b\uff0c\u8981\u6c42\u4ece\u679c\u8747\u57fa\u56e0\u7b26\u53f7\u51fa\u53d1\uff0c\u572816,898\u7bc7\u5168\u6587\u8bba\u6587\u4e2d\u641c\u7d22\u5e76\u751f\u6210\u7ed3\u6784\u5316\u6ce8\u91ca\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6216\u5173\u7cfb\u63d0\u53d6\u7b49\u5b64\u7acb\u5b50\u4efb\u52a1\uff0c\u65e0\u6cd5\u6355\u6349\u79d1\u5b66\u77e5\u8bc6\u5e93\u7b56\u5c55\u6240\u9700\u7684\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u7a0b\uff0c\u9700\u8981\u8bc4\u4f30AI\u4ee3\u7406\u5728\u771f\u5b9e\u79d1\u5b66\u7b56\u5c55\u573a\u666f\u4e2d\u7684\u7efc\u5408\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b7,397\u4e2a\u4e13\u5bb6\u7b56\u5c55\u6ce8\u91ca\u7684\u57fa\u51c6\uff0c\u6db5\u76d6100\u4e2a\u679c\u8747\u57fa\u56e0\uff0c\u8981\u6c42AI\u4ee3\u7406\u4ece\u57fa\u56e0\u7b26\u53f7\u51fa\u53d1\uff0c\u572816,898\u7bc7\u5168\u6587\u8bba\u6587\u4e2d\u641c\u7d22\u5e76\u751f\u6210Gene Ontology\u672f\u8bed\u3001\u8868\u8fbe\u6a21\u5f0f\u548c\u5386\u53f2\u540c\u4e49\u8bcd\u7b49\u7ed3\u6784\u5316\u6ce8\u91ca\u3002\u8bc4\u4f30\u4e86\u56db\u79cd\u57fa\u7ebf\u4ee3\u7406\u67b6\u6784\uff1a\u8bb0\u5fc6\u5316\u3001\u56fa\u5b9a\u6d41\u6c34\u7ebf\u3001\u5355\u4ee3\u7406\u548c\u591a\u4ee3\u7406\u3002", "result": "\u591a\u4ee3\u7406\u8bbe\u8ba1\u4f18\u4e8e\u7b80\u5355\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u6269\u5c55\u9aa8\u5e72\u6a21\u578b\u5e26\u6765\u7684\u6536\u76ca\u9012\u51cf\u3002\u6240\u6709\u57fa\u7ebf\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002\u5206\u6790\u53d1\u73b0\u4ee3\u7406\u4e3b\u8981\u4f7f\u7528\u68c0\u7d22\u6765\u786e\u8ba4\u53c2\u6570\u77e5\u8bc6\u800c\u975e\u53d1\u73b0\u65b0\u4fe1\u606f\u3002", "conclusion": "FlyBench\u5c06\u63a8\u52a8\u68c0\u7d22\u589e\u5f3a\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\uff0c\u8fd9\u79cd\u80fd\u529b\u5728\u79d1\u5b66\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002\u67b6\u6784\u9009\u62e9\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee3\u7406\u5728\u79d1\u5b66\u7b56\u5c55\u4e2d\u7684\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.09286", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.09286", "abs": "https://arxiv.org/abs/2602.09286", "authors": ["Hanjing Shi", "Dominic DiFranzo"], "title": "Human Control Is the Anchor, Not the Answer: Early Divergence of Oversight in Agentic AI Communities", "comment": null, "summary": "Oversight for agentic AI is often discussed as a single goal (\"human control\"), yet early adoption may produce role-specific expectations. We present a comparative analysis of two newly active Reddit communities in Jan--Feb 2026 that reflect different socio-technical roles: r/OpenClaw (deployment and operations) and r/Moltbook (agent-centered social interaction). We conceptualize this period as an early-stage crystallization phase, where oversight expectations form before norms reach equilibrium.\n  Using topic modeling in a shared comparison space, a coarse-grained oversight-theme abstraction, engagement-weighted salience, and divergence tests, we show the communities are strongly separable (JSD =0.418, cosine =0.372, permutation $p=0.0005$). Across both communities, \"human control\" is an anchor term, but its operational meaning diverges: r/OpenClaw} emphasizes execution guardrails and recovery (action-risk), while r/Moltbook} emphasizes identity, legitimacy, and accountability in public interaction (meaning-risk). The resulting distinction offers a portable lens for designing and evaluating oversight mechanisms that match agent role, rather than applying one-size-fits-all control policies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790\u4e24\u4e2aReddit\u793e\u533a\uff08r/OpenClaw\u548cr/Moltbook\uff09\u57282026\u5e74\u521d\u7684\u8ba8\u8bba\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u793e\u4f1a\u6280\u672f\u89d2\u8272\u5bf9AI\u76d1\u7763\u671f\u671b\u7684\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u89d2\u8272\u5339\u914d\u7684\u76d1\u7763\u673a\u5236\u8bbe\u8ba1\u6846\u67b6\u3002", "motivation": "\u5f53\u524dAI\u76d1\u7763\u8ba8\u8bba\u5e38\u5c06\"\u4eba\u7c7b\u63a7\u5236\"\u89c6\u4e3a\u5355\u4e00\u76ee\u6807\uff0c\u4f46\u65e9\u671f\u91c7\u7528\u53ef\u80fd\u4ea7\u751f\u89d2\u8272\u7279\u5b9a\u7684\u671f\u671b\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e0d\u540c\u793e\u4f1a\u6280\u672f\u89d2\u8272\u5982\u4f55\u5f62\u6210\u5dee\u5f02\u5316\u7684AI\u76d1\u7763\u671f\u671b\u3002", "method": "\u4f7f\u7528\u4e3b\u9898\u5efa\u6a21\u5728\u5171\u4eab\u6bd4\u8f83\u7a7a\u95f4\u4e2d\u5206\u6790\u4e24\u4e2aReddit\u793e\u533a\uff082026\u5e741-2\u6708\u6570\u636e\uff09\uff0c\u91c7\u7528\u7c97\u7c92\u5ea6\u76d1\u7763\u4e3b\u9898\u62bd\u8c61\u3001\u53c2\u4e0e\u5ea6\u52a0\u6743\u663e\u8457\u6027\u5206\u6790\u548c\u5dee\u5f02\u68c0\u9a8c\u65b9\u6cd5\u3002", "result": "\u4e24\u4e2a\u793e\u533a\u5728\u76d1\u7763\u671f\u671b\u4e0a\u663e\u8457\u5206\u79bb\uff08JSD=0.418\uff0c\u4f59\u5f26\u76f8\u4f3c\u5ea6=0.372\uff0c\u7f6e\u6362\u68c0\u9a8cp=0.0005\uff09\u3002r/OpenClaw\u5f3a\u8c03\u6267\u884c\u62a4\u680f\u548c\u6062\u590d\uff08\u884c\u52a8\u98ce\u9669\uff09\uff0c\u800cr/Moltbook\u5173\u6ce8\u8eab\u4efd\u3001\u5408\u6cd5\u6027\u548c\u516c\u5171\u4e92\u52a8\u4e2d\u7684\u95ee\u8d23\uff08\u610f\u4e49\u98ce\u9669\uff09\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u79fb\u690d\u7684\u89c6\u89d2\uff0c\u7528\u4e8e\u8bbe\u8ba1\u548c\u8bc4\u4f30\u4e0e\u667a\u80fd\u4f53\u89d2\u8272\u5339\u914d\u7684\u76d1\u7763\u673a\u5236\uff0c\u800c\u975e\u5e94\u7528\u4e00\u5200\u5207\u7684\u63a7\u5236\u7b56\u7565\uff0c\u4e3a\u89d2\u8272\u7279\u5b9a\u7684AI\u76d1\u7763\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.09185", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09185", "abs": "https://arxiv.org/abs/2602.09185", "authors": ["Hao Li", "Haoxiang Zhang", "Ahmed E. Hassan"], "title": "AIDev: Studying AI Coding Agents on GitHub", "comment": null, "summary": "AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.\n  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering", "AI": {"tldr": "AIDev\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6536\u96c6\u4e86932,791\u4e2a\u7531AI\u7f16\u7801\u4ee3\u7406\u5728\u771f\u5b9eGitHub\u4ed3\u5e93\u4e2d\u521b\u5efa\u7684Pull Request\uff0c\u6db5\u76d65\u79cd\u4e3b\u6d41AI\u4ee3\u7406\uff0c\u4e3a\u7814\u7a76AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u91c7\u7528\u3001\u751f\u4ea7\u529b\u548c\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "AI\u7f16\u7801\u4ee3\u7406\u6b63\u5728\u5feb\u901f\u6539\u53d8\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\uff0c\u4f46\u7814\u7a76\u793e\u533a\u7f3a\u4e4f\u6355\u6349\u8fd9\u4e9b\u4ee3\u7406\u5728\u771f\u5b9e\u9879\u76ee\u4e2d\u5982\u4f55\u4f7f\u7528\u7684\u5168\u9762\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u521b\u5efa\u4e86AIDev\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c6\u4e86\u6765\u81ea5\u79cdAI\u4ee3\u7406\uff08OpenAI Codex\u3001Devin\u3001GitHub Copilot\u3001Cursor\u3001Claude Code\uff09\u5728116,211\u4e2aGitHub\u4ed3\u5e93\u4e2d\u521b\u5efa\u7684932,791\u4e2aAgentic-PR\u3002\u8fd8\u521b\u5efa\u4e86\u4e00\u4e2a\u7cbe\u9009\u5b50\u96c6\uff0c\u5305\u542b\u6765\u81ea2,807\u4e2a\u9ad8\u661f\u4ed3\u5e93\u768433,596\u4e2aPR\uff0c\u5305\u542b\u8bc4\u8bba\u3001\u5ba1\u67e5\u3001\u63d0\u4ea4\u548c\u76f8\u5173\u95ee\u9898\u7b49\u8be6\u7ec6\u4fe1\u606f\u3002", "result": "\u6784\u5efa\u4e86AIDev\u6570\u636e\u96c6\uff0c\u5305\u542b932,791\u4e2aAI\u4ee3\u7406\u521b\u5efa\u7684Pull Request\uff0c\u6d89\u53ca116,211\u4e2a\u4ed3\u5e93\u548c72,189\u540d\u5f00\u53d1\u8005\u3002\u7cbe\u9009\u5b50\u96c6\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\uff0c\u4e3a\u7814\u7a76AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002", "conclusion": "AIDev\u6570\u636e\u96c6\u4e3a\u7814\u7a76AI\u91c7\u7528\u3001\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u548c\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u5c06\u63a8\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u65b0\u65f6\u4ee3\u7684\u7814\u7a76\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2602.09341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09341", "abs": "https://arxiv.org/abs/2602.09341", "authors": ["Wei Yang", "Shixuan Li", "Heng Ping", "Peiyu Zhang", "Paul Bogdan", "Jesse Thomason"], "title": "Auditing Multi-Agent LLM Reasoning Trees Outperforms Majority Vote and LLM-as-Judge", "comment": null, "summary": "Multi-agent systems (MAS) can substantially extend the reasoning capacity of large language models (LLMs), yet most frameworks still aggregate agent outputs with majority voting. This heuristic discards the evidential structure of reasoning traces and is brittle under the confabulation consensus, where agents share correlated biases and converge on the same incorrect rationale. We introduce AgentAuditor, which replaces voting with a path search over a Reasoning Tree that explicitly represents agreements and divergences among agent traces. AgentAuditor resolves conflicts by comparing reasoning branches at critical divergence points, turning global adjudication into efficient, localized verification. We further propose Anti-Consensus Preference Optimization (ACPO), which trains the adjudicator on majority-failure cases and rewards evidence-based minority selections over popular errors. AgentAuditor is agnostic to MAS setting, and we find across 5 popular settings that it yields up to 5% absolute accuracy improvement over a majority vote, and up to 3% over using LLM-as-Judge.", "AI": {"tldr": "AgentAuditor\u7528\u63a8\u7406\u6811\u66ff\u4ee3\u591a\u6570\u6295\u7968\uff0c\u901a\u8fc7\u5c40\u90e8\u9a8c\u8bc1\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u51b2\u7a81\uff0c\u7ed3\u5408ACPO\u8bad\u7ec3\u63d0\u5347\u5c11\u6570\u6b63\u786e\u9009\u62e9\u7684\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5927\u591a\u91c7\u7528\u591a\u6570\u6295\u7968\u805a\u5408\u667a\u80fd\u4f53\u8f93\u51fa\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e22\u5f03\u4e86\u63a8\u7406\u8f68\u8ff9\u7684\u8bc1\u636e\u7ed3\u6784\uff0c\u4e14\u5728\u667a\u80fd\u4f53\u5b58\u5728\u76f8\u5173\u504f\u89c1\u65f6\u5bb9\u6613\u5f62\u6210\u9519\u8bef\u5171\u8bc6\uff08confabulation consensus\uff09\u3002", "method": "1. \u63d0\u51faAgentAuditor\u6846\u67b6\uff0c\u7528\u63a8\u7406\u6811\uff08Reasoning Tree\uff09\u663e\u5f0f\u8868\u793a\u667a\u80fd\u4f53\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u4e00\u81f4\u4e0e\u5206\u6b67\uff1b2. \u5728\u5173\u952e\u5206\u6b67\u70b9\u6bd4\u8f83\u63a8\u7406\u5206\u652f\uff0c\u5c06\u5168\u5c40\u88c1\u51b3\u8f6c\u5316\u4e3a\u9ad8\u6548\u7684\u5c40\u90e8\u9a8c\u8bc1\uff1b3. \u63d0\u51faAnti-Consensus Preference Optimization (ACPO)\uff0c\u5728\u591a\u6570\u6295\u7968\u5931\u8d25\u6848\u4f8b\u4e0a\u8bad\u7ec3\u88c1\u51b3\u5668\uff0c\u5956\u52b1\u57fa\u4e8e\u8bc1\u636e\u7684\u5c11\u6570\u9009\u62e9\u800c\u975e\u6d41\u884c\u9519\u8bef\u3002", "result": "\u57285\u4e2a\u6d41\u884c\u8bbe\u7f6e\u4e2d\uff0cAgentAuditor\u76f8\u6bd4\u591a\u6570\u6295\u7968\u83b7\u5f97\u9ad8\u8fbe5%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\uff0c\u76f8\u6bd4\u4f7f\u7528LLM-as-Judge\u83b7\u5f97\u9ad8\u8fbe3%\u7684\u63d0\u5347\u3002", "conclusion": "AgentAuditor\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u5206\u6790\u548c\u5bf9\u6297\u5171\u8bc6\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u6846\u67b6\u4e0e\u5177\u4f53MAS\u8bbe\u7f6e\u65e0\u5173\u3002", "topic": "agent analysis"}}
{"id": "2602.09311", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09311", "abs": "https://arxiv.org/abs/2602.09311", "authors": ["Tao Xiao", "Dong Wang", "Shane McIntosh", "Hideaki Hata", "Yasutaka Kamei"], "title": "Cross-Project Flakiness: A Case Study of the OpenStack Ecosystem", "comment": null, "summary": "Automated regression testing is a cornerstone of modern software development, often contributing directly to code review and Continuous Integration (CI). Yet some tests suffer from flakiness, where their outcomes vary non-deterministically. Flakiness erodes developer trust in test results, wastes computational resources, and undermines CI reliability. While prior research has examined test flakiness within individual projects, its broader ecosystem-wide impact remains largely unexplored. In this paper, we present an empirical study of test flakiness in the OpenStack ecosystem, which focuses on (1) cross-project flakiness, where flaky tests impact multiple projects, and (2) inconsistent flakiness, where a test exhibits flakiness in some projects but remains stable in others. By analyzing 649 OpenStack projects, we identify 1,535 cross-project flaky tests and 1,105 inconsistently flaky tests. We find that cross-project flakiness affects 55% of OpenStack projects and significantly increases both review time and computational costs. Surprisingly, 70% of unit tests exhibit cross-project flakiness, challenging the assumption that unit tests are inherently insulated from issues that span modules like integration and system-level tests. Through qualitative analysis, we observe that race conditions in CI, inconsistent build configurations, and dependency mismatches are the primary causes of inconsistent flakiness. These findings underline the need for better coordination across complex ecosystems, standardized CI configurations, and improved test isolation strategies.", "AI": {"tldr": "\u5bf9OpenStack\u751f\u6001\u7cfb\u7edf\u4e2d\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u8de8\u9879\u76ee\u4e0d\u7a33\u5b9a\u6027\u5f71\u54cd55%\u7684\u9879\u76ee\uff0c\u663e\u8457\u589e\u52a0\u5ba1\u67e5\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\uff0c70%\u7684\u5355\u5143\u6d4b\u8bd5\u8868\u73b0\u51fa\u8de8\u9879\u76ee\u4e0d\u7a33\u5b9a\u6027", "motivation": "\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\u4f1a\u4fb5\u8680\u5f00\u53d1\u8005\u5bf9\u6d4b\u8bd5\u7ed3\u679c\u7684\u4fe1\u4efb\u3001\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\u5e76\u7834\u574fCI\u53ef\u9760\u6027\u3002\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u9879\u76ee\u5185\u7684\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\uff0c\u4f46\u5176\u5728\u66f4\u5e7f\u6cdb\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22", "method": "\u5bf9649\u4e2aOpenStack\u9879\u76ee\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u8de8\u9879\u76ee\u4e0d\u7a33\u5b9a\u6027\uff08\u5f71\u54cd\u591a\u4e2a\u9879\u76ee\u7684\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\uff09\u548c\u4e0d\u4e00\u81f4\u4e0d\u7a33\u5b9a\u6027\uff08\u5728\u67d0\u4e9b\u9879\u76ee\u4e2d\u4e0d\u7a33\u5b9a\u4f46\u5728\u5176\u4ed6\u9879\u76ee\u4e2d\u7a33\u5b9a\u7684\u6d4b\u8bd5\uff09", "result": "\u8bc6\u522b\u51fa1,535\u4e2a\u8de8\u9879\u76ee\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u548c1,105\u4e2a\u4e0d\u4e00\u81f4\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u3002\u8de8\u9879\u76ee\u4e0d\u7a33\u5b9a\u6027\u5f71\u54cd55%\u7684OpenStack\u9879\u76ee\uff0c\u663e\u8457\u589e\u52a0\u5ba1\u67e5\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c70%\u7684\u5355\u5143\u6d4b\u8bd5\u8868\u73b0\u51fa\u8de8\u9879\u76ee\u4e0d\u7a33\u5b9a\u6027", "conclusion": "\u7814\u7a76\u53d1\u73b0CI\u4e2d\u7684\u7ade\u4e89\u6761\u4ef6\u3001\u4e0d\u4e00\u81f4\u7684\u6784\u5efa\u914d\u7f6e\u548c\u4f9d\u8d56\u4e0d\u5339\u914d\u662f\u4e0d\u4e00\u81f4\u4e0d\u7a33\u5b9a\u6027\u7684\u4e3b\u8981\u539f\u56e0\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u590d\u6742\u751f\u6001\u7cfb\u7edf\u4e2d\u9700\u8981\u66f4\u597d\u7684\u534f\u8c03\u3001\u6807\u51c6\u5316\u7684CI\u914d\u7f6e\u548c\u6539\u8fdb\u7684\u6d4b\u8bd5\u9694\u79bb\u7b56\u7565", "topic": "swe benchmark"}}
{"id": "2602.09447", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09447", "abs": "https://arxiv.org/abs/2602.09447", "authors": ["Zhirui Zhang", "Hongbo Zhang", "Haoxiang Fei", "Zhiyuan Bao", "Yubin Chen", "Zhengyu Lei", "Ziyue Liu", "Yixuan Sun", "Mingkun Xiao", "Zihang Ye", "Yu Zhang", "Hongcheng Zhu", "Yuxiang Wen", "Heung-Yeung Shum"], "title": "SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents", "comment": "20 pages, 3 figures", "summary": "Although large language models (LLMs) have demonstrated impressive coding capabilities, their ability to autonomously build production-scale software from explicit specifications remains an open question. We introduce SWE-AGI, an open-source benchmark for evaluating end-to-end, specification-driven construction of software systems written in MoonBit. SWE-AGI tasks require LLM-based agents to implement parsers, interpreters, binary decoders, and SAT solvers strictly from authoritative standards and RFCs under a fixed API scaffold. Each task involves implementing 1,000-10,000 lines of core logic, corresponding to weeks or months of engineering effort for an experienced human developer. By leveraging the nascent MoonBit ecosystem, SWE-AGI minimizes data leakage, forcing agents to rely on long-horizon architectural reasoning rather than code retrieval. Across frontier models, gpt-5.3-codex achieves the best overall performance (solving 19/22 tasks, 86.4%), outperforming claude-opus-4.6 (15/22, 68.2%), and kimi-2.5 exhibits the strongest performance among open-source models. Performance degrades sharply with increasing task difficulty, particularly on hard, specification-intensive systems. Behavioral analysis further reveals that as codebases scale, code reading, rather than writing, becomes the dominant bottleneck in AI-assisted development. Overall, while specification-driven autonomous software engineering is increasingly viable, substantial challenges remain before it can reliably support production-scale development.", "AI": {"tldr": "SWE-AGI\u662f\u4e00\u4e2a\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u6839\u636e\u6743\u5a01\u6807\u51c6\u89c4\u8303\u4ece\u5934\u6784\u5efa\u751f\u4ea7\u7ea7\u8f6f\u4ef6\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u6d4b\u8bd5\u7ed3\u679c\u663e\u793aGPT-5.3-Codex\u8868\u73b0\u6700\u4f73\uff0886.4%\uff09\uff0c\u4f46\u968f\u7740\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u4ee3\u7801\u9605\u8bfb\u6210\u4e3aAI\u8f85\u52a9\u5f00\u53d1\u7684\u4e3b\u8981\u74f6\u9888\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u7f16\u7801\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u6839\u636e\u660e\u786e\u89c4\u8303\u81ea\u4e3b\u6784\u5efa\u751f\u4ea7\u7ea7\u8f6f\u4ef6\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u7136\u672a\u77e5\u3002\u9700\u8981\u8bc4\u4f30LLM\u4ee3\u7406\u80fd\u5426\u4ece\u6743\u5a01\u6807\u51c6\u548cRFC\u6587\u6863\u4e25\u683c\u5b9e\u73b0\u590d\u6742\u7684\u8f6f\u4ef6\u7cfb\u7edf\u3002", "method": "\u5f15\u5165SWE-AGI\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42LLM\u4ee3\u7406\u5728MoonBit\u8bed\u8a00\u4e2d\u5b9e\u73b0\u89e3\u6790\u5668\u3001\u89e3\u91ca\u5668\u3001\u4e8c\u8fdb\u5236\u89e3\u7801\u5668\u548cSAT\u6c42\u89e3\u5668\u7b49\u7cfb\u7edf\uff0c\u4e25\u683c\u9075\u5faa\u6743\u5a01\u6807\u51c6\u89c4\u8303\uff0c\u4f7f\u7528\u56fa\u5b9aAPI\u6846\u67b6\u3002\u6bcf\u4e2a\u4efb\u52a1\u6d89\u53ca1,000-10,000\u884c\u6838\u5fc3\u903b\u8f91\u4ee3\u7801\uff0c\u76f8\u5f53\u4e8e\u4eba\u7c7b\u5de5\u7a0b\u5e08\u6570\u5468\u6216\u6570\u6708\u7684\u5f00\u53d1\u5de5\u4f5c\u91cf\u3002\u5229\u7528\u65b0\u5174\u7684MoonBit\u751f\u6001\u7cfb\u7edf\u6700\u5c0f\u5316\u6570\u636e\u6cc4\u9732\uff0c\u8feb\u4f7f\u4ee3\u7406\u4f9d\u8d56\u957f\u65f6\u7a0b\u67b6\u6784\u63a8\u7406\u800c\u975e\u4ee3\u7801\u68c0\u7d22\u3002", "result": "GPT-5.3-Codex\u8868\u73b0\u6700\u4f73\uff0822\u4e2a\u4efb\u52a1\u4e2d\u5b8c\u621019\u4e2a\uff0c86.4%\uff09\uff0c\u4f18\u4e8eClaude-Opus-4.6\uff0815/22\uff0c68.2%\uff09\uff0cKimi-2.5\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u5f3a\u3002\u968f\u7740\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u89c4\u8303\u5bc6\u96c6\u7684\u56f0\u96be\u7cfb\u7edf\u4e0a\u3002\u884c\u4e3a\u5206\u6790\u663e\u793a\uff0c\u968f\u7740\u4ee3\u7801\u5e93\u89c4\u6a21\u6269\u5927\uff0c\u4ee3\u7801\u9605\u8bfb\u800c\u975e\u7f16\u5199\u6210\u4e3aAI\u8f85\u52a9\u5f00\u53d1\u7684\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "\u867d\u7136\u57fa\u4e8e\u89c4\u8303\u7684\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u8d8a\u6765\u8d8a\u53ef\u884c\uff0c\u4f46\u5728\u80fd\u591f\u53ef\u9760\u652f\u6301\u751f\u4ea7\u7ea7\u5f00\u53d1\u4e4b\u524d\uff0c\u4ecd\u7136\u5b58\u5728\u91cd\u5927\u6311\u6218\u3002\u4ee3\u7801\u9605\u8bfb\u6210\u4e3a\u89c4\u6a21\u5316\u5f00\u53d1\u7684\u4e3b\u8981\u9650\u5236\u56e0\u7d20\u3002", "topic": "swe benchmark"}}
{"id": "2602.09443", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09443", "abs": "https://arxiv.org/abs/2602.09443", "authors": ["Yun Luo", "Futing Wang", "Qianjia Cheng", "Fangchen Yu", "Haodi Lei", "Jianhao Yan", "Chenxi Li", "Jiacheng Chen", "Yufeng Zhao", "Haiyuan Wan", "Yuchen Zhang", "Shenghe Zheng", "Junchi Yao", "Qingyang Zhang", "Haonan He", "Wenxuan Zeng", "Li Sheng", "Chengxing Xie", "Yuxin Zuo", "Yizhuo Li", "Yulun Wu", "Rui Huang", "Dongzhan Zhou", "Kai Chen", "Yu Qiao", "Lei Bai", "Yu Cheng", "Ning Ding", "Bowen Zhou", "Peng Ye", "Ganqu Cui"], "title": "P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads", "comment": null, "summary": "The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.", "AI": {"tldr": "P1-VL\u662f\u4e00\u4e2a\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u4e13\u95e8\u9488\u5bf9\u9ad8\u7ea7\u79d1\u5b66\u63a8\u7406\u8bbe\u8ba1\uff0c\u5728\u7269\u7406\u5965\u6797\u5339\u514b\u7ade\u8d5b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u5f00\u6e90\u6a21\u578b\u6700\u4f73\u6027\u80fd\uff0c\u5168\u7403\u6392\u540d\u7b2c\u4e8c\u3002", "motivation": "\u4ece\u7b26\u53f7\u64cd\u4f5c\u5230\u79d1\u5b66\u7ea7\u63a8\u7406\u662fLLMs\u7684\u5173\u952e\u524d\u6cbf\uff0c\u7269\u7406\u4f5c\u4e3a\u8fde\u63a5\u62bd\u8c61\u903b\u8f91\u4e0e\u7269\u7406\u73b0\u5b9e\u7684\u6d4b\u8bd5\u951a\u70b9\u3002\u7269\u7406\u8981\u6c42\u6a21\u578b\u4fdd\u6301\u4e0e\u5b87\u5b99\u5b9a\u5f8b\u7684\u4e00\u81f4\u6027\uff0c\u8fd9\u9700\u8981\u591a\u6a21\u6001\u611f\u77e5\u5c06\u62bd\u8c61\u903b\u8f91\u4e0e\u73b0\u5b9e\u63a5\u5730\u3002\u5728\u5965\u6797\u5339\u514b\u7ea7\u522b\uff0c\u56fe\u8868\u901a\u5e38\u662f\u6784\u6210\u6027\u7684\u800c\u975e\u8bf4\u660e\u6027\u7684\uff0c\u5305\u542b\u6587\u672c\u4e2d\u7f3a\u5931\u7684\u5173\u952e\u7ea6\u675f\u6761\u4ef6\u3002", "method": "\u7ed3\u5408\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff08\u91c7\u7528\u6e10\u8fdb\u96be\u5ea6\u6269\u5c55\u7a33\u5b9a\u540e\u8bad\u7ec3\uff09\u548c\u4ee3\u7406\u589e\u5f3a\uff08\u5728\u63a8\u7406\u65f6\u5b9e\u73b0\u8fed\u4ee3\u81ea\u6211\u9a8c\u8bc1\uff09\u3002\u5f00\u53d1\u4e86P1-VL\u7cfb\u5217\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728HiPhO\u57fa\u51c6\u6d4b\u8bd5\uff082024-2025\u5e7413\u573a\u8003\u8bd5\uff09\u4e2d\uff0c\u65d7\u8230\u6a21\u578bP1-VL-235B-A22B\u6210\u4e3a\u9996\u4e2a\u83b7\u5f9712\u679a\u91d1\u724c\u7684\u5f00\u6e90VLM\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u4ee3\u7406\u589e\u5f3a\u7cfb\u7edf\u5168\u7403\u6392\u540d\u7b2c\u4e8c\uff0c\u4ec5\u6b21\u4e8eGemini-3-Pro\u3002\u5728STEM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u9886\u5148\u57fa\u7840\u6a21\u578b\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f00\u6e90P1-VL\uff0c\u4e3a\u5b9e\u73b0\u901a\u7528\u7269\u7406\u667a\u80fd\u8fc8\u51fa\u4e86\u57fa\u7840\u6027\u4e00\u6b65\uff0c\u66f4\u597d\u5730\u5bf9\u9f50\u89c6\u89c9\u611f\u77e5\u4e0e\u62bd\u8c61\u7269\u7406\u5b9a\u5f8b\uff0c\u4fc3\u8fdb\u673a\u5668\u79d1\u5b66\u53d1\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2602.09464", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09464", "abs": "https://arxiv.org/abs/2602.09464", "authors": ["Haoyu Zhao", "Ziran Yang", "Jiawei Li", "Deyuan He", "Zenan Li", "Chi Jin", "Venugopal V. Veeravalli", "Aarti Gupta", "Sanjeev Arora"], "title": "AlgoVeri: An Aligned Benchmark for Verified Code Generation on Classical Algorithms", "comment": "32 pages", "summary": "Vericoding refers to the generation of formally verified code from rigorous specifications. Recent AI models show promise in vericoding, but a unified methodology for cross-paradigm evaluation is lacking. Existing benchmarks test only individual languages/tools (e.g., Dafny, Verus, and Lean) and each covers very different tasks, so the performance numbers are not directly comparable. We address this gap with AlgoVeri, a benchmark that evaluates vericoding of $77$ classical algorithms in Dafny, Verus, and Lean. By enforcing identical functional contracts, AlgoVeri reveals critical capability gaps in verification systems. While frontier models achieve tractable success in Dafny ($40.3$% for Gemini-3 Flash), where high-level abstractions and SMT automation simplify the workflow, performance collapses under the systems-level memory constraints of Verus ($24.7$%) and the explicit proof construction required by Lean (7.8%). Beyond aggregate metrics, we uncover a sharp divergence in test-time compute dynamics: Gemini-3 effectively utilizes iterative repair to boost performance (e.g., tripling pass rates in Dafny), whereas GPT-OSS saturates early. Finally, our error analysis shows that language design affects the refinement trajectory: while Dafny allows models to focus on logical correctness, Verus and Lean trap models in persistent syntactic and semantic barriers. All data and evaluation code can be found at https://github.com/haoyuzhao123/algoveri.", "AI": {"tldr": "AlgoVeri\u662f\u4e00\u4e2a\u8bc4\u4f30AI\u6a21\u578b\u5728Dafny\u3001Verus\u548cLean\u4e09\u79cd\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7cfb\u7edf\u4e2d\u751f\u6210\u9a8c\u8bc1\u4ee3\u7801\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b77\u4e2a\u7ecf\u5178\u7b97\u6cd5\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u9a8c\u8bc1\u7cfb\u7edf\u95f4\u7684\u80fd\u529b\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7684\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u9488\u5bf9\u5355\u4e2a\u8bed\u8a00/\u5de5\u5177\uff08\u5982Dafny\u3001Verus\u3001Lean\uff09\uff0c\u4e14\u4efb\u52a1\u5dee\u5f02\u5927\uff0c\u6027\u80fd\u6570\u636e\u65e0\u6cd5\u76f4\u63a5\u6bd4\u8f83\uff0c\u7f3a\u4e4f\u8de8\u8303\u5f0f\u7684\u7edf\u4e00\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u521b\u5efaAlgoVeri\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b77\u4e2a\u7ecf\u5178\u7b97\u6cd5\uff0c\u5728Dafny\u3001Verus\u548cLean\u4e09\u79cd\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7cfb\u7edf\u4e2d\u4f7f\u7528\u76f8\u540c\u7684\u529f\u80fd\u5951\u7ea6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5206\u6790\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u548c\u9519\u8bef\u6a21\u5f0f\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u5728Dafny\u4e2d\u8868\u73b0\u6700\u4f73\uff08Gemini-3 Flash\u8fbe\u523040.3%\uff09\uff0c\u4f46\u5728Verus\uff0824.7%\uff09\u548cLean\uff087.8%\uff09\u4e2d\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff1b\u4e0d\u540c\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u52a8\u6001\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8bed\u8a00\u8bbe\u8ba1\u5f71\u54cd\u9519\u8bef\u4fee\u6b63\u8f68\u8ff9\u3002", "conclusion": "AlgoVeri\u63ed\u793a\u4e86\u9a8c\u8bc1\u7cfb\u7edf\u95f4\u7684\u5173\u952e\u80fd\u529b\u5dee\u8ddd\uff0c\u8bed\u8a00\u8bbe\u8ba1\u663e\u8457\u5f71\u54cdAI\u6a21\u578b\u7684\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0cDafny\u7684\u9ad8\u7ea7\u62bd\u8c61\u548cSMT\u81ea\u52a8\u5316\u7b80\u5316\u4e86\u5de5\u4f5c\u6d41\u7a0b\uff0c\u800cVerus\u548cLean\u5219\u5e26\u6765\u4e86\u6301\u7eed\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u969c\u788d\u3002", "topic": "code agent"}}
{"id": "2602.09463", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09463", "abs": "https://arxiv.org/abs/2602.09463", "authors": ["Furong Jia", "Ling Dai", "Wenjin Deng", "Fan Zhang", "Chen Hu", "Daxin Jiang", "Yu Liu"], "title": "SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated strong reasoning capabilities in geo-localization, yet they often struggle in real-world scenarios where visual cues are sparse, long-tailed, and highly ambiguous. Previous approaches, bound by internal knowledge, often fail to provide verifiable results, yielding confident but ungrounded predictions when faced with confounded evidence. To address these challenges, we propose SpotAgent, a framework that formalizes geo-localization into an agentic reasoning process that leverages expert-level reasoning to synergize visual interpretation with tool-assisted verification. SpotAgent actively explores and verifies visual cues by leveraging external tools (e.g., web search, maps) through a ReAct diagram. We introduce a 3-stage post-training pipeline starting with a Supervised Fine-Tuning (SFT) stage for basic alignment, followed by an Agentic Cold Start phase utilizing high-quality trajectories synthesized via a Multi-Agent framework, aiming to instill tool-calling expertise. Subsequently, the model's reasoning capabilities are refined through Reinforcement Learning. We propose a Spatially-Aware Dynamic Filtering strategy to enhance the efficiency of the RL stage by prioritizing learnable samples based on spatial difficulty. Extensive experiments on standard benchmarks demonstrate that SpotAgent achieves state-of-the-art performance, effectively mitigating hallucinations while delivering precise and verifiable geo-localization.", "AI": {"tldr": "SpotAgent\u662f\u4e00\u4e2a\u7528\u4e8e\u5730\u7406\u5b9a\u4f4d\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u89e3\u91ca\u4e0e\u5de5\u5177\u8f85\u52a9\u9a8c\u8bc1\u6765\u89e3\u51b3\u7a00\u758f\u3001\u957f\u5c3e\u548c\u6a21\u7cca\u89c6\u89c9\u7ebf\u7d22\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u7cbe\u51c6\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5730\u7406\u5b9a\u4f4d\u4e2d\u9762\u4e34\u89c6\u89c9\u7ebf\u7d22\u7a00\u758f\u3001\u957f\u5c3e\u5206\u5e03\u548c\u9ad8\u5ea6\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u4e14\u53d7\u9650\u4e8e\u5185\u90e8\u77e5\u8bc6\uff0c\u5e38\u4ea7\u751f\u81ea\u4fe1\u4f46\u65e0\u6839\u636e\u7684\u9884\u6d4b\uff0c\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u6027\u3002", "method": "\u63d0\u51faSpotAgent\u6846\u67b6\uff0c\u5c06\u5730\u7406\u5b9a\u4f4d\u5f62\u5f0f\u5316\u4e3a\u667a\u80fd\u4f53\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc73\u9636\u6bb5\u540e\u8bad\u7ec3\uff1a1)\u76d1\u7763\u5fae\u8c03\u57fa\u7840\u5bf9\u9f50\uff1b2)\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5408\u6210\u9ad8\u8d28\u91cf\u8f68\u8ff9\u7684\u667a\u80fd\u4f53\u51b7\u542f\u52a8\u9636\u6bb5\uff1b3)\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7cbe\u70bc\u63a8\u7406\u80fd\u529b\uff0c\u91c7\u7528\u7a7a\u95f4\u611f\u77e5\u52a8\u6001\u8fc7\u6ee4\u7b56\u7565\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSpotAgent\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\uff0c\u63d0\u4f9b\u7cbe\u786e\u4e14\u53ef\u9a8c\u8bc1\u7684\u5730\u7406\u5b9a\u4f4d\u7ed3\u679c\u3002", "conclusion": "SpotAgent\u901a\u8fc7\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\u7ed3\u5408\u5de5\u5177\u8f85\u52a9\u9a8c\u8bc1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5730\u7406\u5b9a\u4f4d\u4e2d\u7684\u7a00\u758f\u89c6\u89c9\u7ebf\u7d22\u548c\u53ef\u9a8c\u8bc1\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.09540", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09540", "abs": "https://arxiv.org/abs/2602.09540", "authors": ["Muxin Tian", "Zhe Wang", "Blair Yang", "Zhenwei Tang", "Kunlun Zhu", "Honghua Dong", "Hanchen Li", "Xinni Xie", "Guangjing Wang", "Jiaxuan You"], "title": "SWE-Bench Mobile: Can Large Language Model Agents Develop Industry-Level Mobile Applications?", "comment": null, "summary": "Can large language model agents develop industry-level mobile applications? We introduce \\textbf{SWE-Bench Mobile}, a benchmark for evaluating coding agents on realistic software engineering tasks derived from a production iOS codebase. Unlike existing benchmarks that focus on isolated problems or bug fixes, SWE-Bench Mobile captures the full complexity of industrial development: multi-modal inputs (PRDs and Figma designs), a large-scale mixed Swift/Objective-C codebase, and comprehensive test suites. We evaluate 22 agent-model configurations across four coding agents -- three commercial (Cursor, Codex, Claude Code) and one open-source (OpenCode) -- and find that even the best configurations achieve only 12\\% task success rate. Our analysis reveals that (1) agent design matters as much as model capability -- the same model shows up to 6$\\times$ performance gap across agents, (2) commercial agents consistently outperform open-source alternatives, and (3) simple ``Defensive Programming'' prompts outperform complex ones by 7.4\\%. These findings highlight a significant gap between current agent capabilities and industrial requirements, while providing actionable insights for practitioners and researchers. We release SWE-Bench Mobile as a \\textit{hosted benchmark challenge} to prevent data contamination and ensure fair evaluation. The public leaderboard and development toolkit are available at https://swebenchmobile.com.", "AI": {"tldr": "SWE-Bench Mobile\u662f\u4e00\u4e2a\u8bc4\u4f30\u7f16\u7801\u4ee3\u7406\u5728\u771f\u5b9eiOS\u4ee3\u7801\u5e93\u4e0a\u5f00\u53d1\u5de5\u4e1a\u7ea7\u79fb\u52a8\u5e94\u7528\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u4f73\u4ee3\u7406\u914d\u7f6e\u4ec5\u8fbe\u523012%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u63ed\u793a\u4e86\u4ee3\u7406\u8bbe\u8ba1\u4e0e\u6a21\u578b\u80fd\u529b\u540c\u7b49\u91cd\u8981\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u95ee\u9898\u6216bug\u4fee\u590d\uff0c\u65e0\u6cd5\u8bc4\u4f30\u7f16\u7801\u4ee3\u7406\u5728\u5de5\u4e1a\u7ea7\u79fb\u52a8\u5e94\u7528\u5f00\u53d1\u4e2d\u7684\u771f\u5b9e\u80fd\u529b\u3002\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u80fd\u6355\u6349\u5de5\u4e1a\u5f00\u53d1\u5168\u590d\u6742\u6027\u7684\u57fa\u51c6\uff0c\u5305\u62ec\u591a\u6a21\u6001\u8f93\u5165\u3001\u5927\u89c4\u6a21\u6df7\u5408\u4ee3\u7801\u5e93\u548c\u5b8c\u6574\u6d4b\u8bd5\u5957\u4ef6\u3002", "method": "\u5f15\u5165SWE-Bench Mobile\u57fa\u51c6\uff0c\u57fa\u4e8e\u751f\u4ea7iOS\u4ee3\u7801\u5e93\u6784\u5efa\u771f\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u3002\u8bc4\u4f3022\u79cd\u4ee3\u7406-\u6a21\u578b\u914d\u7f6e\uff0c\u6db5\u76d6\u56db\u4e2a\u7f16\u7801\u4ee3\u7406\uff08\u4e09\u4e2a\u5546\u4e1a\u4ee3\u7406\uff1aCursor\u3001Codex\u3001Claude Code\uff0c\u4e00\u4e2a\u5f00\u6e90\u4ee3\u7406\uff1aOpenCode\uff09\uff0c\u5206\u6790\u4efb\u52a1\u6210\u529f\u7387\u3001\u4ee3\u7406\u8bbe\u8ba1\u5f71\u54cd\u548c\u63d0\u793a\u7b56\u7565\u6548\u679c\u3002", "result": "\u6700\u4f73\u914d\u7f6e\u4ec5\u8fbe\u523012%\u4efb\u52a1\u6210\u529f\u7387\u3002\u53d1\u73b0\uff1a(1) \u4ee3\u7406\u8bbe\u8ba1\u4e0e\u6a21\u578b\u80fd\u529b\u540c\u7b49\u91cd\u8981\uff0c\u76f8\u540c\u6a21\u578b\u5728\u4e0d\u540c\u4ee3\u7406\u4e0a\u6027\u80fd\u5dee\u5f02\u53ef\u8fbe6\u500d\uff1b(2) \u5546\u4e1a\u4ee3\u7406\u59cb\u7ec8\u4f18\u4e8e\u5f00\u6e90\u66ff\u4ee3\u54c1\uff1b(3) \u7b80\u5355\u7684\"\u9632\u5fa1\u6027\u7f16\u7a0b\"\u63d0\u793a\u6bd4\u590d\u6742\u63d0\u793a\u6548\u679c\u597d7.4%\u3002", "conclusion": "\u5f53\u524d\u7f16\u7801\u4ee3\u7406\u80fd\u529b\u4e0e\u5de5\u4e1a\u9700\u6c42\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4f46\u7814\u7a76\u4e3a\u5b9e\u8df5\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002\u53d1\u5e03SWE-Bench Mobile\u4f5c\u4e3a\u6258\u7ba1\u57fa\u51c6\u6311\u6218\uff0c\u9632\u6b62\u6570\u636e\u6c61\u67d3\u5e76\u786e\u4fdd\u516c\u5e73\u8bc4\u4f30\u3002", "topic": "swe benchmark"}}
{"id": "2602.09892", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09892", "abs": "https://arxiv.org/abs/2602.09892", "authors": ["Jiale Zhao", "Guoxin Chen", "Fanzhe Meng", "Minghao Li", "Jie Chen", "Hui Xu", "Yongshuai Sun", "Xin Zhao", "Ruihua Song", "Yuan Zhang", "Peng Wang", "Cheng Chen", "Jirong Wen", "Kai Jia"], "title": "Immersion in the GitHub Universe: Scaling Coding Agents to Mastery", "comment": null, "summary": "Achieving mastery in real world software engineering tasks is fundamentally bottlenecked by the scarcity of large scale, high quality training data. Scaling such data has been limited by the complexity of environment setup, unit test generation, and problem statement curation. In this paper, we propose ScaleSWE, an automated, sandboxed multi agent workflow designed to construct high quality SWE data at scale. The system coordinates three specialized agents for environment setup, test creation, and problem description synthesis to process 6 million pull requests across 5200 repositories, producing Scale SWE Data: 100k verified SWE instances, the largest such dataset to date. It substantially surpasses existing real world datasets in repository diversity and reflects realistic task complexity. We further demonstrate the dataset utility for training by distilling 71498 high quality trajectories and finetuning Qwen30BA3BInstruct to produce ScaleSWE Agent. Our agent achieves a 64 resolve rate on SWE Bench Verified a nearly three fold improvement over the base model. ScaleSWE provides a scalable, reproducible approach for data construction to advance LLM based software engineering. Scale SWE will be publicly available.", "AI": {"tldr": "ScaleSWE\u63d0\u51fa\u81ea\u52a8\u5316\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u4ece600\u4e07PR\u4e2d\u6784\u5efa\u4e8610\u4e07\u9a8c\u8bc1\u7684\u8f6f\u4ef6\u5de5\u7a0b\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u51fa\u5728SWE Bench\u4e0a\u8fbe\u523064%\u89e3\u51b3\u7387\u7684\u667a\u80fd\u4f53\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\uff0c\u73b0\u6709\u6570\u636e\u6269\u5c55\u53d7\u9650\u4e8e\u73af\u5883\u8bbe\u7f6e\u3001\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u548c\u95ee\u9898\u63cf\u8ff0\u521b\u5efa\u7684\u590d\u6742\u6027\u3002", "method": "\u8bbe\u8ba1\u81ea\u52a8\u5316\u3001\u6c99\u76d2\u5316\u7684\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41ScaleSWE\uff0c\u534f\u8c03\u4e09\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff08\u73af\u5883\u8bbe\u7f6e\u3001\u6d4b\u8bd5\u521b\u5efa\u3001\u95ee\u9898\u63cf\u8ff0\u5408\u6210\uff09\uff0c\u5904\u74065200\u4e2a\u4ed3\u5e93\u7684600\u4e07PR\uff0c\u751f\u6210Scale SWE Data\u6570\u636e\u96c6\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b10\u4e07\u9a8c\u8bc1\u5b9e\u4f8b\u7684\u6700\u5927\u8f6f\u4ef6\u5de5\u7a0b\u6570\u636e\u96c6\uff0c\u5728\u4ed3\u5e93\u591a\u6837\u6027\u548c\u4efb\u52a1\u590d\u6742\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709\u6570\u636e\u96c6\u3002\u8bad\u7ec3\u51fa\u7684ScaleSWE Agent\u5728SWE Bench Verified\u4e0a\u8fbe\u523064%\u89e3\u51b3\u7387\uff0c\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u5347\u8fd1\u4e09\u500d\u3002", "conclusion": "ScaleSWE\u4e3aLLM\u9a71\u52a8\u7684\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u6570\u636e\u6784\u5efa\u65b9\u6cd5\uff0c\u5c06\u516c\u5f00\u53ef\u7528\u8be5\u6570\u636e\u96c6\u3002", "topic": "swe benchmark"}}
{"id": "2602.09930", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09930", "abs": "https://arxiv.org/abs/2602.09930", "authors": ["Nishil Amin", "Zhiwei Fei", "Xiang Li", "Justyna Petke", "He Ye"], "title": "JMigBench: A Benchmark for Evaluating LLMs on Source Code Migration (Java 8 to Java 11)", "comment": null, "summary": "We build a benchmark to evaluate large language models (LLMs) for source code migration tasks, specifically upgrading functions from Java 8 to Java 11. We first collected a dataset of function pairs from open-source repositories, but limitations in data quality led us to construct a refined dataset covering eight categories of deprecated APIs. Using this dataset, the Mistral Codestral model was evaluated with CodeBLEU and keyword-based metrics to measure lexical and semantic similarity as well as migration correctness. Results show that the evaluated model (Mistral Codestral) can handle trivial one-to-one API substitutions with moderate success, achieving identical migrations in 11.11% of the cases, but it struggles with more complex migrations such as CORBA or JAX-WS. These findings suggest Mistral Codestral can partially reduce developer effort by automating repetitive migration tasks but cannot yet replace humans within the scope of the JMigBench benchmark. The benchmark and analysis provide a foundation for future work on expanding datasets, refining prompting strategies, and improving migration performance across different LLMs.", "AI": {"tldr": "\u6784\u5efa\u4e86JMigBench\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30LLM\u5728Java 8\u5230Java 11\u4ee3\u7801\u8fc1\u79fb\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0Mistral Codestral\u80fd\u5904\u7406\u7b80\u5355API\u66ff\u6362\u4f46\u590d\u6742\u8fc1\u79fb\u6548\u679c\u6709\u9650", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6e90\u4ee3\u7801\u8fc1\u79fb\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662fJava\u7248\u672c\u5347\u7ea7\u4e2d\u7684API\u8fc1\u79fb\uff0c\u4e3a\u81ea\u52a8\u5316\u4ee3\u7801\u8fc1\u79fb\u63d0\u4f9b\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177", "method": "\u6784\u5efaJMigBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6536\u96c6\u5f00\u6e90\u4ed3\u5e93\u4e2d\u7684\u51fd\u6570\u5bf9\u6570\u636e\u96c6\uff0c\u8986\u76d68\u7c7b\u5e9f\u5f03API\u7c7b\u522b\uff0c\u4f7f\u7528Mistral Codestral\u6a21\u578b\u8bc4\u4f30\uff0c\u91c7\u7528CodeBLEU\u548c\u5173\u952e\u8bcd\u6307\u6807\u8861\u91cf\u8bcd\u6c47\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6", "result": "Mistral Codestral\u80fd\u5904\u7406\u7b80\u5355\u7684\u4e00\u5bf9\u4e00API\u66ff\u6362\uff0811.11%\u76f8\u540c\u8fc1\u79fb\uff09\uff0c\u4f46\u5728CORBA\u3001JAX-WS\u7b49\u590d\u6742\u8fc1\u79fb\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u53ea\u80fd\u90e8\u5206\u51cf\u5c11\u5f00\u53d1\u5de5\u4f5c\u91cf", "conclusion": "\u5f53\u524dLLM\u80fd\u81ea\u52a8\u5316\u91cd\u590d\u6027\u8fc1\u79fb\u4efb\u52a1\u4f46\u4e0d\u80fd\u5b8c\u5168\u66ff\u4ee3\u4eba\u5de5\uff0c\u9700\u8981\u6269\u5c55\u6570\u636e\u96c6\u3001\u4f18\u5316\u63d0\u793a\u7b56\u7565\uff0c\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u57fa\u7840", "topic": "swe benchmark"}}
{"id": "2602.09416", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.09416", "abs": "https://arxiv.org/abs/2602.09416", "authors": ["Andrew Shaw", "Christina Hahn", "Catherine Rasgaitis", "Yash Mishra", "Alisa Liu", "Natasha Jaques", "Yulia Tsvetkov", "Amy X. Zhang"], "title": "Are Language Models Sensitive to Morally Irrelevant Distractors?", "comment": null, "summary": "With the rapid development and uptake of large language models (LLMs) across high-stakes settings, it is increasingly important to ensure that LLMs behave in ways that align with human values. Existing moral benchmarks prompt LLMs with value statements, moral scenarios, or psychological questionnaires, with the implicit underlying assumption that LLMs report somewhat stable moral preferences. However, moral psychology research has shown that human moral judgements are sensitive to morally irrelevant situational factors, such as smelling cinnamon rolls or the level of ambient noise, thereby challenging moral theories that assume the stability of human moral judgements. Here, we draw inspiration from this \"situationist\" view of moral psychology to evaluate whether LLMs exhibit similar cognitive moral biases to humans. We curate a novel multimodal dataset of 60 \"moral distractors\" from existing psychological datasets of emotionally-valenced images and narratives which have no moral relevance to the situation presented. After injecting these distractors into existing moral benchmarks to measure their effects on LLM responses, we find that moral distractors can shift the moral judgements of LLMs by over 30% even in low-ambiguity scenarios, highlighting the need for more contextual moral evaluations and more nuanced cognitive moral modeling of LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u7684\u9053\u5fb7\u5224\u65ad\u4f1a\u53d7\u5230\u4e0e\u9053\u5fb7\u65e0\u5173\u7684\u60c5\u5883\u56e0\u7d20\uff08\"\u9053\u5fb7\u5e72\u6270\u7269\"\uff09\u7684\u663e\u8457\u5f71\u54cd\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u9053\u5fb7\u5fc3\u7406\u5b66\u4e2d\u7684\u60c5\u5883\u4e3b\u4e49\u73b0\u8c61\uff0c\u5373\u4f7f\u5728\u4e0d\u6a21\u7cca\u7684\u573a\u666f\u4e2d\u4e5f\u80fd\u5bfc\u81f430%\u4ee5\u4e0a\u7684\u5224\u65ad\u53d8\u5316\u3002", "motivation": "\u968f\u7740LLMs\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u884c\u4e3a\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u9053\u5fb7\u57fa\u51c6\u5047\u8bbeLLMs\u62a5\u544a\u76f8\u5bf9\u7a33\u5b9a\u7684\u9053\u5fb7\u504f\u597d\uff0c\u4f46\u4eba\u7c7b\u9053\u5fb7\u5fc3\u7406\u5b66\u7814\u7a76\u8868\u660e\u4eba\u7c7b\u9053\u5fb7\u5224\u65ad\u4f1a\u53d7\u5230\u4e0e\u9053\u5fb7\u65e0\u5173\u7684\u60c5\u5883\u56e0\u7d20\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLMs\u662f\u5426\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8ba4\u77e5\u9053\u5fb7\u504f\u89c1\u3002", "method": "\u4ece\u73b0\u6709\u5fc3\u7406\u5b66\u6570\u636e\u96c6\u4e2d\u6536\u96c660\u4e2a\"\u9053\u5fb7\u5e72\u6270\u7269\"\uff08\u60c5\u611f\u6027\u56fe\u50cf\u548c\u53d9\u4e8b\uff09\uff0c\u8fd9\u4e9b\u5e72\u6270\u7269\u4e0e\u6240\u5448\u73b0\u60c5\u5883\u6ca1\u6709\u9053\u5fb7\u76f8\u5173\u6027\u3002\u5c06\u8fd9\u4e9b\u5e72\u6270\u7269\u6ce8\u5165\u73b0\u6709\u9053\u5fb7\u57fa\u51c6\u4e2d\uff0c\u6d4b\u91cf\u5b83\u4eec\u5bf9LLM\u56de\u7b54\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9053\u5fb7\u5e72\u6270\u7269\u53ef\u4ee5\u663e\u8457\u6539\u53d8LLMs\u7684\u9053\u5fb7\u5224\u65ad\uff0c\u5373\u4f7f\u5728\u4f4e\u6a21\u7cca\u6027\u573a\u666f\u4e2d\u4e5f\u80fd\u5bfc\u81f4\u8d85\u8fc730%\u7684\u5224\u65ad\u53d8\u5316\uff0c\u8868\u660eLLMs\u7684\u9053\u5fb7\u5224\u65ad\u5177\u6709\u60c5\u5883\u654f\u611f\u6027\u3002", "conclusion": "LLMs\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u60c5\u5883\u4e3b\u4e49\u9053\u5fb7\u504f\u89c1\uff0c\u8fd9\u6311\u6218\u4e86LLMs\u5177\u6709\u7a33\u5b9a\u9053\u5fb7\u504f\u597d\u7684\u5047\u8bbe\u3002\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u8fdb\u884c\u66f4\u60c5\u5883\u5316\u7684\u9053\u5fb7\u8bc4\u4f30\u548c\u5bf9LLMs\u66f4\u7ec6\u81f4\u7684\u8ba4\u77e5\u9053\u5fb7\u5efa\u6a21\u3002", "topic": "agent analysis"}}
{"id": "2602.09158", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09158", "abs": "https://arxiv.org/abs/2602.09158", "authors": ["Eric Yeats", "John Buckheit", "Sarah Scullen", "Brendan Kennedy", "Loc Truong", "Davis Brown", "Bill Kay", "Cliff Joslyn", "Tegan Emerson", "Michael J. Henry", "John Emanuello", "Henry Kvinge"], "title": "What do Geometric Hallucination Detection Metrics Actually Measure?", "comment": "Published at the 2025 ICML Workshop on Reliable and Responsible Foundation Models", "summary": "Hallucination remains a barrier to deploying generative models in high-consequence applications. This is especially true in cases where external ground truth is not readily available to validate model outputs. This situation has motivated the study of geometric signals in the internal state of an LLM that are predictive of hallucination and require limited external knowledge. Given that there are a range of factors that can lead model output to be called a hallucination (e.g., irrelevance vs incoherence), in this paper we ask what specific properties of a hallucination these geometric statistics actually capture. To assess this, we generate a synthetic dataset which varies distinct properties of output associated with hallucination. This includes output correctness, confidence, relevance, coherence, and completeness. We find that different geometric statistics capture different types of hallucinations. Along the way we show that many existing geometric detection methods have substantial sensitivity to shifts in task domain (e.g., math questions vs. history questions). Motivated by this, we introduce a simple normalization method to mitigate the effect of domain shift on geometric statistics, leading to AUROC gains of +34 points in multi-domain settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLM\u5185\u90e8\u72b6\u6001\u4e2d\u7684\u51e0\u4f55\u4fe1\u53f7\u5982\u4f55\u6355\u6349\u4e0d\u540c\u7c7b\u578b\u7684\u5e7b\u89c9\uff0c\u53d1\u73b0\u4e0d\u540c\u51e0\u4f55\u7edf\u8ba1\u91cf\u5bf9\u5e94\u4e0d\u540c\u5e7b\u89c9\u7c7b\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u6765\u7f13\u89e3\u9886\u57df\u504f\u79fb\u5bf9\u51e0\u4f55\u7edf\u8ba1\u91cf\u7684\u5f71\u54cd\u3002", "motivation": "\u5e7b\u89c9\u662f\u751f\u6210\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u5916\u90e8\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\u6a21\u578b\u8f93\u51fa\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u7814\u7a76\u5173\u6ce8LLM\u5185\u90e8\u72b6\u6001\u7684\u51e0\u4f55\u4fe1\u53f7\u6765\u9884\u6d4b\u5e7b\u89c9\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u51e0\u4f55\u7edf\u8ba1\u91cf\u5177\u4f53\u6355\u6349\u4e86\u5e7b\u89c9\u7684\u54ea\u4e9b\u7279\u6027\u3002", "method": "\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u4e0e\u5e7b\u89c9\u76f8\u5173\u7684\u8f93\u51fa\u7279\u6027\uff08\u6b63\u786e\u6027\u3001\u7f6e\u4fe1\u5ea6\u3001\u76f8\u5173\u6027\u3001\u8fde\u8d2f\u6027\u3001\u5b8c\u6574\u6027\uff09\uff1b\u5206\u6790\u4e0d\u540c\u51e0\u4f55\u7edf\u8ba1\u91cf\u5982\u4f55\u6355\u6349\u8fd9\u4e9b\u7279\u6027\uff1b\u63d0\u51fa\u7b80\u5355\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u6765\u7f13\u89e3\u9886\u57df\u504f\u79fb\u5bf9\u51e0\u4f55\u7edf\u8ba1\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u51e0\u4f55\u7edf\u8ba1\u91cf\u6355\u6349\u4e0d\u540c\u7c7b\u578b\u7684\u5e7b\u89c9\uff1b\u73b0\u6709\u51e0\u4f55\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u4efb\u52a1\u9886\u57df\u504f\u79fb\uff08\u5982\u6570\u5b66\u95ee\u9898vs\u5386\u53f2\u95ee\u9898\uff09\u5177\u6709\u663e\u8457\u654f\u611f\u6027\uff1b\u63d0\u51fa\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u591a\u9886\u57df\u8bbe\u7f6e\u4e2d\u5c06AUROC\u63d0\u5347\u4e8634\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "LLM\u5185\u90e8\u72b6\u6001\u7684\u51e0\u4f55\u4fe1\u53f7\u786e\u5b9e\u80fd\u591f\u6355\u6349\u4e0d\u540c\u7c7b\u578b\u7684\u5e7b\u89c9\u7279\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u9886\u57df\u504f\u79fb\u654f\u611f\uff1b\u901a\u8fc7\u7b80\u5355\u7684\u5f52\u4e00\u5316\u53ef\u4ee5\u663e\u8457\u6539\u5584\u51e0\u4f55\u7edf\u8ba1\u91cf\u5728\u8de8\u9886\u57df\u68c0\u6d4b\u5e7b\u89c9\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.09944", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09944", "abs": "https://arxiv.org/abs/2602.09944", "authors": ["Xiang Li", "Zhiwei Fei", "Ying Ma", "Jerry Zhang", "Sarro Federica", "He Ye"], "title": "Environment-in-the-Loop: Rethinking Code Migration with LLM-based Agents", "comment": null, "summary": "Modern software systems continuously undergo code upgrades to enhance functionality, security, and performance, and Large Language Models (LLMs) have demonstrated remarkable capabilities in code migration tasks. However, while research on automated code migration which including refactoring, API adaptation, and dependency updates has advanced rapidly, the exploration of the automated environment interaction that must accompany it remains relatively scarce. In practice, code and its environment are intricately intertwined. Relying solely on static analysis of the environment leads to an inadequate understanding of the target setting, prolongs feedback cycles, and consequently causes significant rework and project delays, thereby reducing overall efficiency. We contend that successful software evolution demands a holistic perspective that integrates both code and environment migration. To understand the current landscape and challenges, we first provide an overview of the status of automated environment construction. We then propose a novel framework paradigm that tightly integrates automated environment setup with the code migration workflow. Finally, we explore the challenges and future directions for automated environment interaction within the code migration domain. Our findings emphasize that without automated environment interaction, the automation of code migration is only half complete.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u4ee3\u7801\u8fc1\u79fb\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u672c\u8eab\uff0c\u800c\u5ffd\u7565\u4e86\u73af\u5883\u4ea4\u4e92\u7684\u81ea\u52a8\u5316\uff0c\u63d0\u51fa\u9700\u8981\u5c06\u73af\u5883\u6784\u5efa\u4e0e\u4ee3\u7801\u8fc1\u79fb\u7d27\u5bc6\u7ed3\u5408\u7684\u6846\u67b6\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u9700\u8981\u6301\u7eed\u5347\u7ea7\u4ee3\u7801\u4ee5\u589e\u5f3a\u529f\u80fd\u3001\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0cLLM\u5728\u4ee3\u7801\u8fc1\u79fb\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4ee3\u7801\u8fc1\u79fb\u672c\u8eab\uff08\u5982\u91cd\u6784\u3001API\u9002\u914d\u3001\u4f9d\u8d56\u66f4\u65b0\uff09\uff0c\u800c\u4f34\u968f\u4ee3\u7801\u8fc1\u79fb\u6240\u9700\u7684\u73af\u5883\u4ea4\u4e92\u81ea\u52a8\u5316\u7814\u7a76\u76f8\u5bf9\u532e\u4e4f\u3002\u4ee3\u7801\u4e0e\u73af\u5883\u7d27\u5bc6\u76f8\u8fde\uff0c\u4ec5\u4f9d\u8d56\u9759\u6001\u73af\u5883\u5206\u6790\u4f1a\u5bfc\u81f4\u5bf9\u76ee\u6807\u73af\u5883\u7406\u89e3\u4e0d\u8db3\u3001\u53cd\u9988\u5468\u671f\u5ef6\u957f\u3001\u5927\u91cf\u8fd4\u5de5\u548c\u9879\u76ee\u5ef6\u8fdf\uff0c\u4ece\u800c\u964d\u4f4e\u6574\u4f53\u6548\u7387\u3002", "method": "1. \u9996\u5148\u6982\u8ff0\u5f53\u524d\u81ea\u52a8\u5316\u73af\u5883\u6784\u5efa\u7684\u73b0\u72b6\uff1b2. \u63d0\u51fa\u4e00\u4e2a\u5c06\u81ea\u52a8\u5316\u73af\u5883\u8bbe\u7f6e\u4e0e\u4ee3\u7801\u8fc1\u79fb\u5de5\u4f5c\u6d41\u7d27\u5bc6\u7ed3\u5408\u7684\u65b0\u6846\u67b6\u8303\u5f0f\uff1b3. \u63a2\u7d22\u4ee3\u7801\u8fc1\u79fb\u9886\u57df\u4e2d\u81ea\u52a8\u5316\u73af\u5883\u4ea4\u4e92\u7684\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6ca1\u6709\u81ea\u52a8\u5316\u73af\u5883\u4ea4\u4e92\uff0c\u4ee3\u7801\u8fc1\u79fb\u7684\u81ea\u52a8\u5316\u53ea\u5b8c\u6210\u4e86\u4e00\u534a\u3002\u5f3a\u8c03\u9700\u8981\u91c7\u7528\u6574\u4f53\u89c6\u89d2\uff0c\u5c06\u4ee3\u7801\u8fc1\u79fb\u548c\u73af\u5883\u8fc1\u79fb\u7ed3\u5408\u8d77\u6765\uff0c\u624d\u80fd\u5b9e\u73b0\u6210\u529f\u7684\u8f6f\u4ef6\u6f14\u8fdb\u3002", "conclusion": "\u6210\u529f\u7684\u8f6f\u4ef6\u6f14\u8fdb\u9700\u8981\u4e00\u4e2a\u6574\u4f53\u89c6\u89d2\uff0c\u5c06\u4ee3\u7801\u8fc1\u79fb\u548c\u73af\u5883\u8fc1\u79fb\u7ed3\u5408\u8d77\u6765\u3002\u81ea\u52a8\u5316\u73af\u5883\u4ea4\u4e92\u5bf9\u4e8e\u5b8c\u6574\u7684\u4ee3\u7801\u8fc1\u79fb\u81ea\u52a8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u5f53\u524d\u7814\u7a76\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u6846\u67b6\u548c\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "topic": "swe application"}}
{"id": "2602.10046", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.10046", "abs": "https://arxiv.org/abs/2602.10046", "authors": ["Doehyun Baek", "Michael Pradel"], "title": "Artisan: Agentic Artifact Evaluation", "comment": null, "summary": "Artifact evaluation has become standard practice in the software engineering community to ensure the reproducibility of research results. However, the current manual process is labor-intensive, and hence, done only as a one-time assessment for a subset of all papers. To support the artifact evaluation effort, we present Artisan, an automated LLM agent for reproducing research results given a paper and its artifact. The approach is enabled by two key contributions: First, we frame the reproduction problem as a code generation task where the goal is to generate a reproduction script that, when executed, reproduces the results reported in a paper. Unlike prior work on automatically reproducing research results in other domains, this formulation allows for running the script independently of the agent and for assessing the reproduction process at a fine-grained level. Second, we design automated judging mechanism that guides the agent toward the expected results without revealing them and that prevent trivial solutions, such as simply copying checked-in results. To evaluate Artisan, we introduce Artisan-Bench, the first benchmark assessing the ability to generate reproduction scripts and the first benchmark for automated artifact evaluation in software engineering. Artisan-Bench comprises 60 tasks derived from 23 software engineering papers, covering different research areas and programming languages. We validate all tasks in Artisan-Bench for reproducibility to ensure that the tasks are feasible. Our experiments show that Artisan is effective, producing 44/60 reproduction scripts and outperforming the best available baseline, a vanilla LLM agent (mini-swe-agent), by 3.14$\\times$ in terms of reproduction scripts generated while taking $0.45 and 48 minutes, on average per task. Artisan also helped uncover 20 new errors in either the paper or artifact.", "AI": {"tldr": "Artisan\uff1a\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u5316\u590d\u73b0\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u7ed3\u679c\u7684LLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u6267\u884c\u7684\u590d\u73b0\u811a\u672c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7814\u7a76\u7ed3\u679c\u590d\u73b0\u7684\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u5236\u54c1\u8bc4\u4f30\uff08artifact evaluation\uff09\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u624b\u52a8\u8fdb\u884c\uff0c\u8fc7\u7a0b\u8017\u65f6\u8017\u529b\uff0c\u901a\u5e38\u53ea\u80fd\u5bf9\u90e8\u5206\u8bba\u6587\u8fdb\u884c\u4e00\u6b21\u6027\u7684\u8bc4\u4f30\u3002\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u652f\u6301\u5236\u54c1\u8bc4\u4f30\u5de5\u4f5c\uff0c\u63d0\u9ad8\u7814\u7a76\u7ed3\u679c\u590d\u73b0\u7684\u6548\u7387\u548c\u8986\u76d6\u7387\u3002", "method": "1. \u5c06\u590d\u73b0\u95ee\u9898\u5b9a\u4e49\u4e3a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff1a\u76ee\u6807\u662f\u751f\u6210\u4e00\u4e2a\u590d\u73b0\u811a\u672c\uff0c\u6267\u884c\u8be5\u811a\u672c\u53ef\u4ee5\u590d\u73b0\u8bba\u6587\u4e2d\u62a5\u544a\u7684\u7ed3\u679c\uff1b2. \u8bbe\u8ba1\u81ea\u52a8\u5316\u8bc4\u5224\u673a\u5236\uff1a\u5f15\u5bfc\u4ee3\u7406\u5411\u9884\u671f\u7ed3\u679c\u524d\u8fdb\u800c\u4e0d\u76f4\u63a5\u63ed\u793a\u7ed3\u679c\uff0c\u9632\u6b62\u7b80\u5355\u590d\u5236\u5df2\u63d0\u4ea4\u7ed3\u679c\u7684\u53d6\u5de7\u65b9\u6848\uff1b3. \u6784\u5efaArtisan-Bench\u57fa\u51c6\u6d4b\u8bd5\uff1a\u5305\u542b60\u4e2a\u6765\u81ea23\u7bc7\u8f6f\u4ef6\u5de5\u7a0b\u8bba\u6587\u7684\u4efb\u52a1\uff0c\u6db5\u76d6\u4e0d\u540c\u7814\u7a76\u9886\u57df\u548c\u7f16\u7a0b\u8bed\u8a00\u3002", "result": "Artisan\u572860\u4e2a\u4efb\u52a1\u4e2d\u6210\u529f\u751f\u6210\u4e8644\u4e2a\u590d\u73b0\u811a\u672c\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\uff08mini-swe-agent\uff09\u63d0\u9ad8\u4e863.14\u500d\u7684\u6210\u529f\u7387\u3002\u5e73\u5747\u6bcf\u4e2a\u4efb\u52a1\u4ec5\u97000.45\u7f8e\u5143\u548c48\u5206\u949f\u3002\u6b64\u5916\uff0cArtisan\u8fd8\u5e2e\u52a9\u53d1\u73b0\u4e8620\u4e2a\u8bba\u6587\u6216\u5236\u54c1\u4e2d\u7684\u65b0\u9519\u8bef\u3002", "conclusion": "Artisan\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u590d\u73b0\u811a\u672c\u7684\u65b9\u5f0f\uff0c\u6709\u6548\u652f\u6301\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u7ed3\u679c\u7684\u590d\u73b0\u5de5\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5236\u54c1\u8bc4\u4f30\u7684\u6548\u7387\u548c\u8986\u76d6\u7387\uff0c\u540c\u65f6\u8fd8\u80fd\u5e2e\u52a9\u53d1\u73b0\u7814\u7a76\u4e2d\u7684\u6f5c\u5728\u9519\u8bef\u3002", "topic": "swe application"}}
{"id": "2602.09802", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09802", "abs": "https://arxiv.org/abs/2602.09802", "authors": ["Manon Reusens", "Sofie Goethals", "Toon Calders", "David Martens"], "title": "Would a Large Language Model Pay Extra for a View? Inferring Willingness to Pay from Subjective Choices", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed in applications such as travel assistance and purchasing support, they are often required to make subjective choices on behalf of users in settings where no objectively correct answer exists. We study LLM decision-making in a travel-assistant context by presenting models with choice dilemmas and analyzing their responses using multinomial logit models to derive implied willingness to pay (WTP) estimates. These WTP values are subsequently compared to human benchmark values from the economics literature. In addition to a baseline setting, we examine how model behavior changes under more realistic conditions, including the provision of information about users' past choices and persona-based prompting. Our results show that while meaningful WTP values can be derived for larger LLMs, they also display systematic deviations at the attribute level. Additionally, they tend to overestimate human WTP overall, particularly when expensive options or business-oriented personas are introduced. Conditioning models on prior preferences for cheaper options yields valuations that are closer to human benchmarks. Overall, our findings highlight both the potential and the limitations of using LLMs for subjective decision support and underscore the importance of careful model selection, prompt design, and user representation when deploying such systems in practice.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30LLM\u5728\u4e3b\u89c2\u51b3\u7b56\u573a\u666f\uff08\u65c5\u884c\u52a9\u624b\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u9009\u62e9\u56f0\u5883\u5b9e\u9a8c\u8ba1\u7b97\u9690\u542b\u652f\u4ed8\u610f\u613f\uff0c\u5e76\u4e0e\u4eba\u7c7b\u57fa\u51c6\u6bd4\u8f83\uff0c\u53d1\u73b0LLM\u80fd\u4ea7\u751f\u6709\u610f\u4e49\u7684WTP\u503c\u4f46\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u4e14\u503e\u5411\u4e8e\u9ad8\u4f30\u4eba\u7c7b\u652f\u4ed8\u610f\u613f\u3002", "motivation": "\u968f\u7740LLM\u5728\u65c5\u884c\u52a9\u624b\u3001\u8d2d\u7269\u652f\u6301\u7b49\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u5b83\u4eec\u7ecf\u5e38\u9700\u8981\u5728\u6ca1\u6709\u5ba2\u89c2\u6b63\u786e\u7b54\u6848\u7684\u4e3b\u89c2\u9009\u62e9\u573a\u666f\u4e2d\u4e3a\u7528\u6237\u505a\u51b3\u7b56\u3002\u9700\u8981\u7814\u7a76LLM\u5728\u8fd9\u4e9b\u4e3b\u89c2\u51b3\u7b56\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u5176\u4f5c\u4e3a\u51b3\u7b56\u652f\u6301\u5de5\u5177\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002", "method": "\u5728\u65c5\u884c\u52a9\u624b\u80cc\u666f\u4e0b\u5411LLM\u5448\u73b0\u9009\u62e9\u56f0\u5883\uff0c\u4f7f\u7528\u591a\u9879logit\u6a21\u578b\u5206\u6790\u54cd\u5e94\u4ee5\u63a8\u5bfc\u9690\u542b\u652f\u4ed8\u610f\u613f\u4f30\u8ba1\uff0c\u7136\u540e\u5c06\u8fd9\u4e9bWTP\u503c\u4e0e\u7ecf\u6d4e\u5b66\u6587\u732e\u4e2d\u7684\u4eba\u7c7b\u57fa\u51c6\u503c\u8fdb\u884c\u6bd4\u8f83\u3002\u9664\u4e86\u57fa\u7ebf\u8bbe\u7f6e\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u5728\u66f4\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u884c\u4e3a\u53d8\u5316\uff0c\u5305\u62ec\u63d0\u4f9b\u7528\u6237\u8fc7\u53bb\u9009\u62e9\u4fe1\u606f\u548c\u57fa\u4e8e\u89d2\u8272\u7684\u63d0\u793a\u3002", "result": "\u8f83\u5927LLM\u53ef\u4ee5\u4ea7\u751f\u6709\u610f\u4e49\u7684WTP\u503c\uff0c\u4f46\u5728\u5c5e\u6027\u5c42\u9762\u663e\u793a\u7cfb\u7edf\u6027\u504f\u5dee\uff1b\u603b\u4f53\u4e0a\u503e\u5411\u4e8e\u9ad8\u4f30\u4eba\u7c7bWTP\uff0c\u7279\u522b\u662f\u5f53\u5f15\u5165\u6602\u8d35\u9009\u9879\u6216\u5546\u52a1\u5bfc\u5411\u89d2\u8272\u65f6\uff1b\u5f53\u6a21\u578b\u57fa\u4e8e\u5148\u524d\u5bf9\u66f4\u4fbf\u5b9c\u9009\u9879\u7684\u504f\u597d\u8fdb\u884c\u6761\u4ef6\u5316\u65f6\uff0c\u4f30\u503c\u66f4\u63a5\u8fd1\u4eba\u7c7b\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u4f7f\u7528LLM\u8fdb\u884c\u4e3b\u89c2\u51b3\u7b56\u652f\u6301\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u5728\u5b9e\u9645\u90e8\u7f72\u6b64\u7c7b\u7cfb\u7edf\u65f6\u4ed4\u7ec6\u9009\u62e9\u6a21\u578b\u3001\u8bbe\u8ba1\u63d0\u793a\u548c\u7528\u6237\u8868\u793a\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.09813", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09813", "abs": "https://arxiv.org/abs/2602.09813", "authors": ["Dexun Li", "Sidney Tio", "Pradeep Varakantham"], "title": "Efficient Unsupervised Environment Design through Hierarchical Policy Representation Learning", "comment": null, "summary": "Unsupervised Environment Design (UED) has emerged as a promising approach to developing general-purpose agents through automated curriculum generation. Popular UED methods focus on Open-Endedness, where teacher algorithms rely on stochastic processes for infinite generation of useful environments. This assumption becomes impractical in resource-constrained scenarios where teacher-student interaction opportunities are limited. To address this challenge, we introduce a hierarchical Markov Decision Process (MDP) framework for environment design. Our framework features a teacher agent that leverages student policy representations derived from discovered evaluation environments, enabling it to generate training environments based on the student's capabilities. To improve efficiency, we incorporate a generative model that augments the teacher's training dataset with synthetic data, reducing the need for teacher-student interactions. In experiments across several domains, we show that our method outperforms baseline approaches while requiring fewer teacher-student interactions in a single episode. The results suggest the applicability of our approach in settings where training opportunities are limited.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42MDP\u6846\u67b6\u7528\u4e8e\u73af\u5883\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6559\u5e08\u4ee3\u7406\u5229\u7528\u5b66\u751f\u7b56\u7565\u8868\u793a\u751f\u6210\u8bad\u7ec3\u73af\u5883\uff0c\u5e76\u5f15\u5165\u751f\u6210\u6a21\u578b\u51cf\u5c11\u5e08\u751f\u4ea4\u4e92\u9700\u6c42\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\uff08UED\uff09\u65b9\u6cd5\u4f9d\u8d56\u968f\u673a\u8fc7\u7a0b\u65e0\u9650\u751f\u6210\u73af\u5883\uff0c\u8fd9\u5728\u5e08\u751f\u4ea4\u4e92\u673a\u4f1a\u6709\u9650\u7684\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bfe\u7a0b\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5206\u5c42MDP\u6846\u67b6\uff0c\u6559\u5e08\u4ee3\u7406\u5229\u7528\u4ece\u8bc4\u4f30\u73af\u5883\u4e2d\u63d0\u53d6\u7684\u5b66\u751f\u7b56\u7565\u8868\u793a\u6765\u751f\u6210\u8bad\u7ec3\u73af\u5883\uff1b\u5f15\u5165\u751f\u6210\u6a21\u578b\u589e\u5f3a\u6559\u5e08\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u51cf\u5c11\u5e08\u751f\u4ea4\u4e92\u9700\u6c42\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5728\u5355\u6b21\u8bad\u7ec3\u4e2d\u9700\u8981\u66f4\u5c11\u7684\u5e08\u751f\u4ea4\u4e92\uff0c\u8bc1\u660e\u4e86\u5728\u8bad\u7ec3\u673a\u4f1a\u6709\u9650\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u73af\u5883\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u51cf\u5c11\u5e08\u751f\u4ea4\u4e92\u9700\u6c42\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.09937", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09937", "abs": "https://arxiv.org/abs/2602.09937", "authors": ["Taeyoon Kim", "Woohyeok Park", "Hoyeong Yun", "Kyungyong Lee"], "title": "Why Do AI Agents Systematically Fail at Cloud Root Cause Analysis?", "comment": null, "summary": "Failures in large-scale cloud systems incur substantial financial losses, making automated Root Cause Analysis (RCA) essential for operational stability. Recent efforts leverage Large Language Model (LLM) agents to automate this task, yet existing systems exhibit low detection accuracy even with capable models, and current evaluation frameworks assess only final answer correctness without revealing why the agent's reasoning failed. This paper presents a process level failure analysis of LLM-based RCA agents. We execute the full OpenRCA benchmark across five LLM models, producing 1,675 agent runs, and classify observed failures into 12 pitfall types across intra-agent reasoning, inter-agent communication, and agent-environment interaction. Our analysis reveals that the most prevalent pitfalls, notably hallucinated data interpretation and incomplete exploration, persist across all models regardless of capability tier, indicating that these failures originate from the shared agent architecture rather than from individual model limitations. Controlled mitigation experiments further show that prompt engineering alone cannot resolve the dominant pitfalls, whereas enriching the inter-agent communication protocol reduces communication-related failures by up to 15 percentage points. The pitfall taxonomy and diagnostic methodology developed in this work provide a foundation for designing more reliable autonomous agents for cloud RCA.", "AI": {"tldr": "\u5bf9LLM\u9a71\u52a8\u7684\u4e91\u7cfb\u7edf\u6839\u56e0\u5206\u6790\u4ee3\u7406\u8fdb\u884c\u8fc7\u7a0b\u7ea7\u5931\u8d25\u5206\u6790\uff0c\u8bc6\u522b12\u79cd\u9677\u9631\u7c7b\u578b\uff0c\u53d1\u73b0\u4e3b\u8981\u5931\u8d25\u6e90\u4e8e\u5171\u4eab\u7684\u4ee3\u7406\u67b6\u6784\u800c\u975e\u6a21\u578b\u80fd\u529b\u9650\u5236\uff0c\u63d0\u793a\u5de5\u7a0b\u65e0\u6cd5\u89e3\u51b3\u4e3b\u8981\u95ee\u9898\uff0c\u4f46\u6539\u8fdb\u901a\u4fe1\u534f\u8bae\u53ef\u51cf\u5c11\u901a\u4fe1\u76f8\u5173\u5931\u8d25\u3002", "motivation": "\u4e91\u7cfb\u7edf\u6545\u969c\u5bfc\u81f4\u5de8\u5927\u7ecf\u6d4e\u635f\u5931\uff0c\u9700\u8981\u81ea\u52a8\u6839\u56e0\u5206\u6790\u3002\u73b0\u6709LLM\u4ee3\u7406\u7cfb\u7edf\u68c0\u6d4b\u51c6\u786e\u7387\u4f4e\uff0c\u4e14\u8bc4\u4f30\u6846\u67b6\u53ea\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\uff0c\u65e0\u6cd5\u63ed\u793a\u63a8\u7406\u5931\u8d25\u539f\u56e0\uff0c\u9700\u8981\u6df1\u5165\u5206\u6790\u4ee3\u7406\u8fc7\u7a0b\u7ea7\u5931\u8d25\u3002", "method": "\u5728OpenRCA\u57fa\u51c6\u4e0a\u5bf95\u4e2aLLM\u6a21\u578b\u6267\u884c1675\u6b21\u4ee3\u7406\u8fd0\u884c\uff0c\u89c2\u5bdf\u5931\u8d25\u5e76\u5c06\u5176\u5206\u7c7b\u4e3a12\u79cd\u9677\u9631\u7c7b\u578b\uff0c\u6db5\u76d6\u4ee3\u7406\u5185\u90e8\u63a8\u7406\u3001\u4ee3\u7406\u95f4\u901a\u4fe1\u548c\u4ee3\u7406\u73af\u5883\u4ea4\u4e92\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u8fdb\u884c\u63a7\u5236\u6027\u7f13\u89e3\u5b9e\u9a8c\u3002", "result": "\u5206\u6790\u663e\u793a\u6700\u666e\u904d\u7684\u9677\u9631\uff08\u5982\u5e7b\u89c9\u6570\u636e\u89e3\u91ca\u548c\u4e0d\u5b8c\u5168\u63a2\u7d22\uff09\u5728\u6240\u6709\u6a21\u578b\u4e2d\u90fd\u5b58\u5728\uff0c\u4e0e\u6a21\u578b\u80fd\u529b\u5c42\u7ea7\u65e0\u5173\uff0c\u8868\u660e\u5931\u8d25\u6e90\u4e8e\u5171\u4eab\u4ee3\u7406\u67b6\u6784\u3002\u63d0\u793a\u5de5\u7a0b\u65e0\u6cd5\u89e3\u51b3\u4e3b\u8981\u9677\u9631\uff0c\u4f46\u4e30\u5bcc\u4ee3\u7406\u95f4\u901a\u4fe1\u534f\u8bae\u53ef\u5c06\u901a\u4fe1\u76f8\u5173\u5931\u8d25\u51cf\u5c1115\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "LLM\u4ee3\u7406\u7684\u6839\u56e0\u5206\u6790\u5931\u8d25\u4e3b\u8981\u6e90\u4e8e\u67b6\u6784\u8bbe\u8ba1\u800c\u975e\u6a21\u578b\u9650\u5236\uff0c\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u4ee3\u7406\u67b6\u6784\u3002\u63d0\u51fa\u7684\u9677\u9631\u5206\u7c7b\u548c\u8bca\u65ad\u65b9\u6cd5\u4e3a\u8bbe\u8ba1\u66f4\u53ef\u9760\u7684\u4e91RCA\u81ea\u4e3b\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2602.09945", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09945", "abs": "https://arxiv.org/abs/2602.09945", "authors": ["Jinsong Liu", "Yuhang Jiang", "Ramayya Krishnan", "Rema Padman", "Yiye Zhang", "Jiang Bian"], "title": "Closing Reasoning Gaps in Clinical Agents with Differential Reasoning Learning", "comment": null, "summary": "Clinical decision support requires not only correct answers but also clinically valid reasoning. We propose Differential Reasoning Learning (DRL), a framework that improves clinical agents by learning from reasoning discrepancies. From reference reasoning rationales (e.g., physician-authored clinical rationale, clinical guidelines, or outputs from more capable models) and the agent's free-form chain-of-thought (CoT), DRL extracts reasoning graphs as directed acyclic graphs (DAGs) and performs a clinically weighted graph edit distance (GED)-based discrepancy analysis. An LLM-as-a-judge aligns semantically equivalent nodes and diagnoses discrepancies between graphs. These graph-level discrepancy diagnostics are converted into natural-language instructions and stored in a Differential Reasoning Knowledge Base (DR-KB). At inference, we retrieve top-$k$ instructions via Retrieval-Augmented Generation (RAG) to augment the agent prompt and patch likely logic gaps. Evaluation on open medical question answering (QA) benchmarks and a Return Visit Admissions (RVA) prediction task from internal clinical data demonstrates gains over baselines, improving both final-answer accuracy and reasoning fidelity. Ablation studies confirm gains from infusing reference reasoning rationales and the top-$k$ retrieval strategy. Clinicians' review of the output provides further assurance of the approach. Together, results suggest that DRL supports more reliable clinical decision-making in complex reasoning scenarios and offers a practical mechanism for deployment under limited token budgets.", "AI": {"tldr": "DRL\u6846\u67b6\u901a\u8fc7\u5206\u6790\u63a8\u7406\u5dee\u5f02\u6765\u63d0\u5347\u4e34\u5e8a\u667a\u80fd\u4f53\u7684\u51b3\u7b56\u8d28\u91cf\uff0c\u4f7f\u7528\u56fe\u7f16\u8f91\u8ddd\u79bb\u6bd4\u8f83\u53c2\u8003\u63a8\u7406\u4e0e\u667a\u80fd\u4f53\u63a8\u7406\uff0c\u6784\u5efa\u5dee\u5f02\u77e5\u8bc6\u5e93\u5e76\u901a\u8fc7RAG\u68c0\u7d22\u6307\u4ee4\u6765\u4fee\u8865\u63a8\u7406\u6f0f\u6d1e\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u4e0d\u4ec5\u9700\u8981\u6b63\u786e\u7b54\u6848\uff0c\u8fd8\u9700\u8981\u4e34\u5e8a\u6709\u6548\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u8d28\u91cf\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b66\u4e60\u548c\u6539\u8fdb\u63a8\u7406\u8fc7\u7a0b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5dee\u5f02\u63a8\u7406\u5b66\u4e60(DRL)\u6846\u67b6\uff1a1) \u5c06\u53c2\u8003\u63a8\u7406\uff08\u533b\u5e08\u4e34\u5e8a\u63a8\u7406\u3001\u4e34\u5e8a\u6307\u5357\u6216\u66f4\u5f3a\u6a21\u578b\u8f93\u51fa\uff09\u548c\u667a\u80fd\u4f53\u7684\u81ea\u7531\u5f62\u5f0f\u94fe\u5f0f\u63a8\u7406\u8f6c\u6362\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff1b2) \u4f7f\u7528\u4e34\u5e8a\u52a0\u6743\u56fe\u7f16\u8f91\u8ddd\u79bb\u8fdb\u884c\u5dee\u5f02\u5206\u6790\uff1b3) LLM\u4f5c\u4e3a\u6cd5\u5b98\u5bf9\u9f50\u8bed\u4e49\u7b49\u4ef7\u8282\u70b9\u5e76\u8bca\u65ad\u5dee\u5f02\uff1b4) \u5c06\u56fe\u7ea7\u5dee\u5f02\u8bca\u65ad\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5b58\u50a8\u5728\u5dee\u5f02\u63a8\u7406\u77e5\u8bc6\u5e93(DR-KB)\u4e2d\uff1b5) \u63a8\u7406\u65f6\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u68c0\u7d22top-k\u6307\u4ee4\u6765\u589e\u5f3a\u667a\u80fd\u4f53\u63d0\u793a\u3002", "result": "\u5728\u5f00\u653e\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u548c\u5185\u90e8\u4e34\u5e8a\u6570\u636e\u7684\u8fd4\u9662\u5165\u9662\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0cDRL\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u548c\u63a8\u7406\u4fdd\u771f\u5ea6\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u53c2\u8003\u63a8\u7406\u6ce8\u5165\u548ctop-k\u68c0\u7d22\u7b56\u7565\u7684\u6709\u6548\u6027\u3002\u4e34\u5e8a\u533b\u5e08\u8bc4\u5ba1\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u3002", "conclusion": "DRL\u652f\u6301\u5728\u590d\u6742\u63a8\u7406\u573a\u666f\u4e2d\u8fdb\u884c\u66f4\u53ef\u9760\u7684\u4e34\u5e8a\u51b3\u7b56\uff0c\u5e76\u4e3a\u6709\u9650token\u9884\u7b97\u4e0b\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2602.09514", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09514", "abs": "https://arxiv.org/abs/2602.09514", "authors": ["Xavier Hu", "Jinxiang Xia", "Shengze Xu", "Kangqi Song", "Yishuo Yuan", "Guibin Zhang", "Jincheng Ren", "Boyu Feng", "Li Lu", "Tieyong Zeng", "Jiaheng Liu", "Minghao Liu", "Yuchen Elenor Jiang", "Wei Wang", "He Zhu", "Wangchunshu Zhou"], "title": "EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies", "comment": "work in progress", "summary": "Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies. EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under partial observability and stochasticity. Experiments across eleven leading LLMs expose a systematic tension: no single model dominates across all three scenarios. Critically, we find that models exhibit significant suboptimality in either high-level strategies or efficient actions executions. EcoGym is released as an open, extensible testbed for transparent long-horizon agent evaluation and for studying controllability-utility trade-offs in realistic economic settings.", "AI": {"tldr": "EcoGym\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u957f\u671f\u89c4\u5212\u548c\u51b3\u7b56\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e09\u4e2a\u7ecf\u6d4e\u73af\u5883\uff0c\u5173\u6ce8\u957f\u671f\u6218\u7565\u4e00\u81f4\u6027\u548c\u4e1a\u52a1\u76f8\u5173\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u6846\u67b6\u5927\u591a\u662f\u9636\u6bb5\u6027\u7684\u3001\u7279\u5b9a\u9886\u57df\u7684\uff0c\u6216\u7f3a\u4e4f\u6301\u7eed\u7ecf\u6d4e\u52a8\u6001\u7684\u5145\u5206\u57fa\u7840\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u957f\u671f\u89c4\u5212\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5f00\u53d1\u4e86EcoGym\u57fa\u51c6\uff0c\u5305\u542b\u4e09\u4e2a\u591a\u6837\u5316\u73af\u5883\uff08Vending\u3001Freelance\u3001Operation\uff09\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u51b3\u7b56\u8fc7\u7a0b\u548c\u6807\u51c6\u5316\u63a5\u53e3\uff0c\u652f\u63011000+\u6b65\u7684\u65e0\u754c\u65f6\u95f4\u8303\u56f4\u8bc4\u4f30\u3002", "result": "\u572811\u4e2a\u9886\u5148LLM\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u4e09\u4e2a\u573a\u666f\u4e2d\u90fd\u8868\u73b0\u6700\u4f18\uff0c\u6a21\u578b\u5728\u9ad8\u5c42\u6218\u7565\u6216\u9ad8\u6548\u884c\u52a8\u6267\u884c\u65b9\u9762\u5b58\u5728\u663e\u8457\u6b21\u4f18\u6027\u3002", "conclusion": "EcoGym\u4f5c\u4e3a\u5f00\u653e\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u53ef\u7528\u4e8e\u900f\u660e\u8bc4\u4f30\u957f\u671f\u89c4\u5212\u667a\u80fd\u4f53\uff0c\u5e76\u7814\u7a76\u73b0\u5b9e\u7ecf\u6d4e\u8bbe\u7f6e\u4e2d\u7684\u53ef\u63a7\u6027-\u6548\u7528\u6743\u8861\u3002", "topic": "agent analysis"}}
{"id": "2602.10009", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.10009", "abs": "https://arxiv.org/abs/2602.10009", "authors": ["Sean Memery", "Kartic Subr"], "title": "Discovering High Level Patterns from Simulation Traces", "comment": null, "summary": "Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics. The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data. In this paper, we propose a natural language guided method to discover coarse-grained patterns (e.g., 'rigid-body collision', 'stable support', etc.) from detailed simulation logs. Specifically, we synthesize programs that operate on simulation logs and map them to a series of high level activated patterns. We show, through two physics benchmarks, that this annotated representation of the simulation log is more amenable to natural language reasoning about physical systems. We demonstrate how this method enables LMs to generate effective reward programs from goals specified in natural language, which may be used within the context of planning or supervised learning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u8be6\u7ec6\u7269\u7406\u4eff\u771f\u65e5\u5fd7\u4e2d\u63d0\u53d6\u7c97\u7c92\u5ea6\u6a21\u5f0f\uff08\u5982\"\u521a\u4f53\u78b0\u649e\"\u3001\"\u7a33\u5b9a\u652f\u6491\"\u7b49\uff09\u7684\u81ea\u7136\u8bed\u8a00\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u7a0b\u5e8f\u5c06\u4eff\u771f\u65e5\u5fd7\u6620\u5c04\u4e3a\u9ad8\u5c42\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u66f4\u597d\u5730\u8fdb\u884c\u7269\u7406\u7cfb\u7edf\u63a8\u7406\u3002", "motivation": "\u57fa\u4e8e\u7269\u7406\u4ea4\u4e92\u7684AI\u667a\u80fd\u4f53\u9762\u4e34\u63a8\u7406\u3001\u89c4\u5212\u7b49\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u4eba\u7c7b\u7528\u6237\u5e0c\u671b\u7528\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u6216\u4ea4\u4e92\u65f6\u3002\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9ed8\u8ba4AI\u5de5\u5177\uff0c\u5728\u5904\u7406\u7269\u7406\u4efb\u52a1\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u5176\u7269\u7406\u63a8\u7406\u80fd\u529b\u662f\u4ece\u89c2\u5bdf\u6570\u636e\u4e2d\u5b66\u4e60\u7684\uff0c\u800c\u975e\u57fa\u4e8e\u4eff\u771f\u57fa\u7840\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u4eff\u771f\u8f68\u8ff9\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u4f46\u53ef\u6269\u5c55\u6027\u5dee\uff0c\u56e0\u4e3a\u4eff\u771f\u8f68\u8ff9\u5305\u542b\u5927\u91cf\u7ec6\u7c92\u5ea6\u6570\u503c\u548c\u8bed\u4e49\u6570\u636e\u3002", "method": "\u63d0\u51fa\u81ea\u7136\u8bed\u8a00\u5f15\u5bfc\u7684\u65b9\u6cd5\uff0c\u4ece\u8be6\u7ec6\u4eff\u771f\u65e5\u5fd7\u4e2d\u53d1\u73b0\u7c97\u7c92\u5ea6\u6a21\u5f0f\u3002\u5177\u4f53\u901a\u8fc7\u5408\u6210\u7a0b\u5e8f\u64cd\u4f5c\u4eff\u771f\u65e5\u5fd7\uff0c\u5c06\u5176\u6620\u5c04\u4e3a\u4e00\u7cfb\u5217\u9ad8\u5c42\u6fc0\u6d3b\u6a21\u5f0f\u3002\u8be5\u65b9\u6cd5\u751f\u6210\u4eff\u771f\u65e5\u5fd7\u7684\u6ce8\u91ca\u8868\u793a\uff0c\u66f4\u9002\u5408\u81ea\u7136\u8bed\u8a00\u5bf9\u7269\u7406\u7cfb\u7edf\u7684\u63a8\u7406\u3002", "result": "\u5728\u4e24\u4e2a\u7269\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\uff0c\u8fd9\u79cd\u6ce8\u91ca\u8868\u793a\u7684\u4eff\u771f\u65e5\u5fd7\u66f4\u9002\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u5bf9\u7269\u7406\u7cfb\u7edf\u7684\u63a8\u7406\u3002\u6f14\u793a\u4e86\u8be5\u65b9\u6cd5\u5982\u4f55\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u5b9a\u7684\u76ee\u6807\u751f\u6210\u6709\u6548\u7684\u5956\u52b1\u7a0b\u5e8f\uff0c\u8fd9\u4e9b\u7a0b\u5e8f\u53ef\u7528\u4e8e\u89c4\u5212\u6216\u76d1\u7763\u5b66\u4e60\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u901a\u8fc7\u4ece\u8be6\u7ec6\u4eff\u771f\u65e5\u5fd7\u4e2d\u63d0\u53d6\u7c97\u7c92\u5ea6\u6a21\u5f0f\uff0c\u53ef\u4ee5\u521b\u5efa\u66f4\u9002\u5408\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u8868\u793a\uff0c\u4ece\u800c\u6539\u5584AI\u667a\u80fd\u4f53\u5728\u7269\u7406\u73af\u5883\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u6307\u5bfc\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.10063", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10063", "abs": "https://arxiv.org/abs/2602.10063", "authors": ["Tianyi Jiang", "Arctanx An", "Hengyi Feng", "Naixin Zhai", "Haodong Li", "Xiaomin Yu", "Jiahui Liu", "Hanwen Du", "Shuo Zhang", "Zhi Yang", "Jie Huang", "Yuhua Li", "Yongxin Ni", "Huacan Wang", "Ronghao Chen"], "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes", "comment": null, "summary": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.", "AI": {"tldr": "Chain of Mindset (CoM) \u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u534f\u8c03\u56db\u79cd\u4e0d\u540c\u601d\u7ef4\u6a21\u5f0f\uff08\u7a7a\u95f4\u3001\u6536\u655b\u3001\u53d1\u6563\u3001\u7b97\u6cd5\uff09\u6765\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\"\u5355\u4e00\u601d\u7ef4\u9677\u9631\"\uff0c\u5373\u5728\u6240\u6709\u63a8\u7406\u6b65\u9aa4\u4e2d\u5e94\u7528\u76f8\u540c\u7684\u56fa\u5b9a\u601d\u7ef4\u6a21\u5f0f\uff0c\u5ffd\u89c6\u4e86\u89e3\u51b3\u540c\u4e00\u95ee\u9898\u4e0d\u540c\u9636\u6bb5\u9700\u8981\u6839\u672c\u4e0d\u540c\u7684\u601d\u7ef4\u6a21\u5f0f\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u8fbe\u5230\u66f4\u9ad8\u667a\u80fd\u6c34\u5e73\u3002", "method": "\u63d0\u51faChain of Mindset (CoM)\u6846\u67b6\uff1a1) \u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u56db\u79cd\u529f\u80fd\u5f02\u6784\u7684\u601d\u7ef4\u6a21\u5f0f\uff1a\u7a7a\u95f4\u601d\u7ef4\u3001\u6536\u655b\u601d\u7ef4\u3001\u53d1\u6563\u601d\u7ef4\u548c\u7b97\u6cd5\u601d\u7ef4\uff1b2) \u5143\u667a\u80fd\u4f53\u6839\u636e\u6f14\u5316\u7684\u63a8\u7406\u72b6\u6001\u52a8\u6001\u9009\u62e9\u6700\u4f18\u601d\u7ef4\u6a21\u5f0f\uff1b3) \u53cc\u5411\u4e0a\u4e0b\u6587\u95e8\u8fc7\u6ee4\u8de8\u6a21\u5757\u4fe1\u606f\u6d41\u4ee5\u4fdd\u6301\u6548\u679c\u548c\u6548\u7387\u3002", "result": "\u5728\u6570\u5b66\u3001\u4ee3\u7801\u751f\u6210\u3001\u79d1\u5b66\u95ee\u7b54\u548c\u7a7a\u95f4\u63a8\u7406\u7b49\u516d\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1a\u5728Qwen3-VL-32B-Instruct\u4e0a\u6574\u4f53\u51c6\u786e\u7387\u6bd4\u6700\u5f3a\u57fa\u7ebf\u9ad8\u51fa4.96%\uff0c\u5728Gemini-2.0-Flash\u4e0a\u9ad8\u51fa4.72%\uff0c\u540c\u65f6\u5e73\u8861\u4e86\u63a8\u7406\u6548\u7387\u3002", "conclusion": "CoM\u901a\u8fc7\u52a8\u6001\u534f\u8c03\u591a\u79cd\u601d\u7ef4\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u63a8\u7406\u65b9\u6cd5\u7684\u5355\u4e00\u601d\u7ef4\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u7684\u667a\u80fd\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.09517", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09517", "abs": "https://arxiv.org/abs/2602.09517", "authors": ["Sangwon Yu", "Ik-hwan Kim", "Donghun Kang", "Bongkyu Hwang", "Junhwa Choi", "Suk-hoon Jung", "Seungki Hong", "Taehee Lee", "Sungroh Yoon"], "title": "Knowledge Integration Decay in Search-Augmented Reasoning of Large Language Models", "comment": null, "summary": "Modern Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks by employing search-augmented reasoning to incorporate external knowledge into long chains of thought. However, we identify a critical yet underexplored bottleneck in this paradigm, termed Knowledge Integration Decay (KID). Specifically, we observe that as the length of reasoning generated before search grows, models increasingly fail to integrate retrieved evidence into subsequent reasoning steps, limiting performance even when relevant information is available. To address this, we propose Self-Anchored Knowledge Encoding (SAKE), a training-free inference-time strategy designed to stabilize knowledge utilization. By anchoring retrieved knowledge at both the beginning and end of the reasoning process, SAKE prevents it from being overshadowed by prior context, thereby preserving its semantic integrity. Extensive experiments on multi-hop QA and complex reasoning benchmarks demonstrate that SAKE significantly mitigates KID and improves performance, offering a lightweight yet effective solution for knowledge integration in agentic LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSAKE\u65b9\u6cd5\uff0c\u901a\u8fc7\u951a\u5b9a\u68c0\u7d22\u77e5\u8bc6\u5728\u63a8\u7406\u8fc7\u7a0b\u7684\u9996\u5c3e\uff0c\u89e3\u51b3LLMs\u5728\u957f\u63a8\u7406\u94fe\u4e2d\u77e5\u8bc6\u6574\u5408\u8870\u51cf\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u548c\u590d\u6742\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLMs\u901a\u8fc7\u641c\u7d22\u589e\u5f3a\u63a8\u7406\u6574\u5408\u5916\u90e8\u77e5\u8bc6\uff0c\u4f46\u5b58\u5728\"\u77e5\u8bc6\u6574\u5408\u8870\u51cf\"\u74f6\u9888\uff1a\u968f\u7740\u63a8\u7406\u94fe\u589e\u957f\uff0c\u6a21\u578b\u8d8a\u6765\u8d8a\u96be\u4ee5\u5c06\u68c0\u7d22\u5230\u7684\u8bc1\u636e\u6574\u5408\u5230\u540e\u7eed\u63a8\u7406\u6b65\u9aa4\u4e2d\uff0c\u5373\u4f7f\u76f8\u5173\u4fe1\u606f\u53ef\u7528\uff0c\u6027\u80fd\u4e5f\u53d7\u9650\u3002", "method": "\u63d0\u51faSAKE\uff08Self-Anchored Knowledge Encoding\uff09\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u63a8\u7406\u65f6\u4f7f\u7528\u7684\u7b56\u7565\u3002\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u7684\u5f00\u5934\u548c\u7ed3\u5c3e\u540c\u65f6\u951a\u5b9a\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\uff0c\u9632\u6b62\u77e5\u8bc6\u88ab\u5148\u524d\u4e0a\u4e0b\u6587\u6df9\u6ca1\uff0c\u4fdd\u6301\u5176\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "result": "\u5728\u591a\u8df3\u95ee\u7b54\u548c\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSAKE\u663e\u8457\u7f13\u89e3\u4e86\u77e5\u8bc6\u6574\u5408\u8870\u51cf\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "SAKE\u4e3a\u667a\u80fdLLMs\u4e2d\u7684\u77e5\u8bc6\u6574\u5408\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7a33\u5b9a\u77e5\u8bc6\u5229\u7528\u6765\u6539\u5584\u957f\u63a8\u7406\u94fe\u4e2d\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.09207", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09207", "abs": "https://arxiv.org/abs/2602.09207", "authors": ["Xiaofeng Xiao", "Xiao Hu", "Yang Ye", "Xubo Yue"], "title": "CausalGDP: Causality-Guided Diffusion Policies for Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has achieved remarkable success in a wide range of sequential decision-making problems. Recent diffusion-based policies further improve RL by modeling complex, high-dimensional action distributions. However, existing diffusion policies primarily rely on statistical associations and fail to explicitly account for causal relationships among states, actions, and rewards, limiting their ability to identify which action components truly cause high returns. In this paper, we propose Causality-guided Diffusion Policy (CausalGDP), a unified framework that integrates causal reasoning into diffusion-based RL. CausalGDP first learns a base diffusion policy and an initial causal dynamical model from offline data, capturing causal dependencies among states, actions, and rewards. During real-time interaction, the causal information is continuously updated and incorporated as a guidance signal to steer the diffusion process toward actions that causally influence future states and rewards. By explicitly considering causality beyond association, CausalGDP focuses policy optimization on action components that genuinely drive performance improvements. Experimental results demonstrate that CausalGDP consistently achieves competitive or superior performance over state-of-the-art diffusion-based and offline RL methods, especially in complex, high-dimensional control tasks.", "AI": {"tldr": "CausalGDP\u5c06\u56e0\u679c\u63a8\u7406\u878d\u5165\u6269\u6563\u7b56\u7565\uff0c\u901a\u8fc7\u56e0\u679c\u5f15\u5bfc\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\uff0c\u5728\u590d\u6742\u9ad8\u7ef4\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u6269\u6563\u7b56\u7565\u4e3b\u8981\u4f9d\u8d56\u7edf\u8ba1\u5173\u8054\uff0c\u672a\u80fd\u663e\u5f0f\u8003\u8651\u72b6\u6001\u3001\u52a8\u4f5c\u548c\u5956\u52b1\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u8bc6\u522b\u771f\u6b63\u5bfc\u81f4\u9ad8\u56de\u62a5\u7684\u52a8\u4f5c\u6210\u5206\u7684\u80fd\u529b", "method": "\u63d0\u51faCausalGDP\u6846\u67b6\uff1a\u9996\u5148\u4ece\u79bb\u7ebf\u6570\u636e\u5b66\u4e60\u57fa\u7840\u6269\u6563\u7b56\u7565\u548c\u521d\u59cb\u56e0\u679c\u52a8\u6001\u6a21\u578b\uff0c\u6355\u83b7\u72b6\u6001\u3001\u52a8\u4f5c\u548c\u5956\u52b1\u95f4\u7684\u56e0\u679c\u4f9d\u8d56\uff1b\u5728\u5b9e\u65f6\u4ea4\u4e92\u4e2d\uff0c\u6301\u7eed\u66f4\u65b0\u56e0\u679c\u4fe1\u606f\u5e76\u5c06\u5176\u4f5c\u4e3a\u5f15\u5bfc\u4fe1\u53f7\uff0c\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\u751f\u6210\u80fd\u56e0\u679c\u5f71\u54cd\u672a\u6765\u72b6\u6001\u548c\u5956\u52b1\u7684\u52a8\u4f5c", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCausalGDP\u5728\u590d\u6742\u9ad8\u7ef4\u63a7\u5236\u4efb\u52a1\u4e2d\u6301\u7eed\u53d6\u5f97\u4f18\u4e8e\u6216\u4e0e\u6700\u5148\u8fdb\u7684\u6269\u6563\u57fa\u7ebf\u548c\u79bb\u7ebfRL\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u8d85\u8d8a\u5173\u8054\u7684\u663e\u5f0f\u56e0\u679c\u8003\u8651\uff0cCausalGDP\u5c06\u7b56\u7565\u4f18\u5316\u805a\u7126\u4e8e\u771f\u6b63\u9a71\u52a8\u6027\u80fd\u63d0\u5347\u7684\u52a8\u4f5c\u6210\u5206\uff0c\u4e3a\u6269\u6563\u57faRL\u63d0\u4f9b\u4e86\u56e0\u679c\u589e\u5f3a\u7684\u7edf\u4e00\u6846\u67b6", "topic": "agentic reinforcement learning"}}
{"id": "2602.10085", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10085", "abs": "https://arxiv.org/abs/2602.10085", "authors": ["Richard Bornemann", "Pierluigi Vito Amadori", "Antoine Cully"], "title": "CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs", "comment": "Preprint", "summary": "Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\\href{https://sites.google.com/view/code-sharp/homepage}{here}$.", "AI": {"tldr": "CODE-SHARP\u662f\u4e00\u4e2a\u5229\u7528\u57fa\u7840\u6a21\u578b\u81ea\u52a8\u53d1\u73b0\u548c\u6f14\u5316\u5206\u5c42\u6280\u80fd\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u5f62\u5f0f\u7684\u53ef\u6267\u884c\u5956\u52b1\u51fd\u6570\u6784\u5efa\u6280\u80fd\u56fe\u5e93\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u89e3\u51b3\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u4e0d\u9002\u7528\u4e8e\u5f00\u653e\u5f0f\u7684\u6280\u80fd\u53d1\u73b0\uff0c\u56e0\u4e3a\u6709\u610f\u4e49\u6280\u80fd\u7684\u96c6\u5408\u4e8b\u5148\u672a\u77e5\u3002\u867d\u7136\u5df2\u6709\u65b9\u6cd5\u80fd\u81ea\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u4f46\u4ecd\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u4efb\u52a1\u7684\u5956\u52b1\u4f18\u5316\u3002", "method": "\u63d0\u51faCODE-SHARP\u6846\u67b6\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5f00\u653e\u5730\u6269\u5c55\u548c\u7cbe\u70bc\u5206\u5c42\u6280\u80fd\u6863\u6848\uff0c\u5c06\u5176\u6784\u5efa\u4e3a\u4ee3\u7801\u4e2d\u53ef\u6267\u884c\u5956\u52b1\u51fd\u6570\u7684\u6709\u5411\u56fe\u3002\u901a\u8fc7\u9ad8\u5c42FM\u89c4\u5212\u5668\u7ec4\u5408\u53d1\u73b0\u7684\u6280\u80fd\uff0c\u4f7f\u5355\u4e2a\u76ee\u6807\u6761\u4ef6\u667a\u80fd\u4f53\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u3002", "result": "\u5728Craftax\u73af\u5883\u4e2d\uff0c\u4ec5\u4f7f\u7528SHARP\u6280\u80fd\u751f\u6210\u7684\u5956\u52b1\u8bad\u7ec3\u7684\u76ee\u6807\u6761\u4ef6\u667a\u80fd\u4f53\u80fd\u591f\u89e3\u51b3\u8d8a\u6765\u8d8a\u957f\u65f6\u7a0b\u7684\u76ee\u6807\u3002\u7ec4\u5408\u6280\u80fd\u540e\uff0c\u5355\u4e2a\u667a\u80fd\u4f53\u5728\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u4e0a\u5e73\u5747\u8868\u73b0\u4f18\u4e8e\u9884\u8bad\u7ec3\u667a\u80fd\u4f53\u548c\u4efb\u52a1\u7279\u5b9a\u4e13\u5bb6\u7b56\u7565134%\u4ee5\u4e0a\u3002", "conclusion": "CODE-SHARP\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5f00\u653e\u5f0f\u6280\u80fd\u53d1\u73b0\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u81ea\u52a8\u751f\u6210\u5206\u5c42\u5956\u52b1\u51fd\u6570\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u548c\u7ec4\u5408\u6280\u80fd\u4ee5\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.09538", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09538", "abs": "https://arxiv.org/abs/2602.09538", "authors": ["Hongyan Xie", "Yikun Ban", "Ruiyu Fang", "Zixuan Huang", "Deqing Wang", "Jianxin Li", "Yitong Yao", "Chao Wang", "Shuangyong Song"], "title": "UniARM: Towards a Unified Autoregressive Reward Model for Multi-Objective Test-Time Alignment", "comment": "Under Review", "summary": "Multi-objective alignment aims to align LLM responses with multiple human preference objectives. Among existing methods, guiding the generation of frozen LLMs through autoregressive reward models (ARMs) to accomplish multi-objective test-time alignment is a low-cost solution. However, these methods typically rely on independent parameters for each preference objective, either by training ARMs independently across preference dimensions, which neglects interactions among preference features, or by training a single ARM with separate feature extraction modules for each preference, which can cause feature entanglement. Both strategies can result in misalignment between generated outputs and user preferences. To address this limitation, we propose Preference-Modulated \\& Shared Low-Rank Adaptation (MoSLoRA) for ARM training, which first extracts shared features via a preference-agnostic module and then applies affine transformations to shared features via a preference modulation module conditioned on mixed preference vectors. This design mitigates feature entanglement and enables precise control over preference trade-offs during inference. Building on this, we introduce the Unified Autoregressive Reward Model (UniARM), a novel framework for multi-objective test-time alignment. UniARM jointly models all preference dimensions in a single parameter space, eliminating the need for independent parameters for each preference objective. es on larger-scale LLMs, enhancing its practical usability.", "AI": {"tldr": "\u63d0\u51faMoSLoRA\u548cUniARM\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u548c\u504f\u597d\u8c03\u5236\u6a21\u5757\u89e3\u51b3\u591a\u76ee\u6807\u5bf9\u9f50\u4e2d\u7684\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u5355\u53c2\u6570\u7a7a\u95f4\u5efa\u6a21\u6240\u6709\u504f\u597d\u7ef4\u5ea6", "motivation": "\u73b0\u6709\u591a\u76ee\u6807\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1) \u4e3a\u6bcf\u4e2a\u504f\u597d\u76ee\u6807\u72ec\u7acb\u8bad\u7ec3ARMs\uff0c\u5ffd\u7565\u4e86\u504f\u597d\u7279\u5f81\u95f4\u7684\u4ea4\u4e92\uff1b2) \u4f7f\u7528\u72ec\u7acb\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u5bfc\u81f4\u7279\u5f81\u7ea0\u7f20\u3002\u8fd9\u4e24\u79cd\u7b56\u7565\u90fd\u4f1a\u5bfc\u81f4\u751f\u6210\u8f93\u51fa\u4e0e\u7528\u6237\u504f\u597d\u9519\u4f4d", "method": "\u63d0\u51faMoSLoRA\u65b9\u6cd5\uff1a\u5148\u901a\u8fc7\u504f\u597d\u65e0\u5173\u6a21\u5757\u63d0\u53d6\u5171\u4eab\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u504f\u597d\u8c03\u5236\u6a21\u5757\u5bf9\u5171\u4eab\u7279\u5f81\u8fdb\u884c\u4eff\u5c04\u53d8\u6362\u3002\u57fa\u4e8e\u6b64\u6784\u5efaUniARM\u6846\u67b6\uff0c\u5728\u5355\u4e00\u53c2\u6570\u7a7a\u95f4\u4e2d\u8054\u5408\u5efa\u6a21\u6240\u6709\u504f\u597d\u7ef4\u5ea6", "result": "MoSLoRA\u7f13\u89e3\u4e86\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5bf9\u504f\u597d\u6743\u8861\u7684\u7cbe\u786e\u63a7\u5236\u3002UniARM\u6d88\u9664\u4e86\u4e3a\u6bcf\u4e2a\u504f\u597d\u76ee\u6807\u8bbe\u7f6e\u72ec\u7acb\u53c2\u6570\u7684\u9700\u6c42\uff0c\u5e76\u5728\u66f4\u5927\u89c4\u6a21LLMs\u4e0a\u8868\u73b0\u51fa\u8272", "conclusion": "\u63d0\u51fa\u7684MoSLoRA\u548cUniARM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u76ee\u6807\u5bf9\u9f50\u4e2d\u7684\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\uff0c\u901a\u8fc7\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u548c\u504f\u597d\u8c03\u5236\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u504f\u597d\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u5b9e\u9645\u53ef\u7528\u6027", "topic": "agent analysis"}}
{"id": "2602.10090", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10090", "abs": "https://arxiv.org/abs/2602.10090", "authors": ["Zhaoyang Wang", "Canwen Xu", "Boyi Liu", "Yite Wang", "Siwei Han", "Zhewei Yao", "Huaxiu Yao", "Yuxiong He"], "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning", "comment": "41 pages", "summary": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.", "AI": {"tldr": "\u63d0\u51faAgent World Model (AWM)\u5408\u6210\u73af\u5883\u751f\u6210\u7ba1\u9053\uff0c\u521b\u5efa1000\u4e2a\u4ee3\u7801\u9a71\u52a8\u7684\u65e5\u5e38\u573a\u666f\u73af\u5883\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u8bad\u7ec3\u53d7\u9650\u4e8e\u7f3a\u4e4f\u591a\u6837\u53ef\u9760\u7684\u73af\u5883\uff0cLLM\u6a21\u62df\u7684\u73af\u5883\u72b6\u6001\u8f6c\u6362\u4e0d\u53ef\u9760\u4e14\u6548\u7387\u4f4e\uff0c\u9700\u8981\u53ef\u6269\u5c55\u7684\u5408\u6210\u73af\u5883\u751f\u6210\u65b9\u6848\u3002", "method": "\u5f00\u53d1AWM\u5408\u6210\u73af\u5883\u751f\u6210\u7ba1\u9053\uff0c\u521b\u5efa1000\u4e2a\u4ee3\u7801\u9a71\u52a8\u7684\u65e5\u5e38\u573a\u666f\u73af\u5883\uff0c\u6bcf\u4e2a\u73af\u5883\u5e73\u574735\u4e2a\u5de5\u5177\uff0c\u57fa\u4e8e\u6570\u636e\u5e93\u63d0\u4f9b\u53ef\u9760\u72b6\u6001\u8f6c\u6362\uff0c\u652f\u6301\u9ad8\u6548\u667a\u80fd\u4f53\u4ea4\u4e92\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u5728\u5408\u6210\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u5728\u57fa\u51c6\u7279\u5b9a\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "conclusion": "AWM\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5408\u6210\u73af\u5883\u751f\u6210\u65b9\u6848\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5b9e\u73b0\u53ef\u9760\u5956\u52b1\u8bbe\u8ba1\u548c\u9ad8\u6548\u667a\u80fd\u4f53\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.09591", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09591", "abs": "https://arxiv.org/abs/2602.09591", "authors": ["Daisuke Nohara", "Taishi Nakamura", "Rio Yokota"], "title": "On the Optimal Reasoning Length for RL-Trained Language Models", "comment": "15 pages, 10 figures. Submitted to the Workshop on Scaling Post-training for LLMs (SPOT) at ICLR 2026", "summary": "Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e0d\u540c\u957f\u5ea6\u63a7\u5236\u65b9\u6cd5\u5bf9LLM\u63a8\u7406\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u957f\u5ea6\u60e9\u7f5a\u53ef\u80fd\u963b\u788d\u63a8\u7406\u5b66\u4e60\uff0c\u800c\u9002\u5f53\u8c03\u6574\u7684\u957f\u5ea6\u63a7\u5236\u80fd\u63d0\u5347\u5df2\u6709\u5f3a\u63a8\u7406\u80fd\u529b\u6a21\u578b\u7684\u6548\u7387\uff0c\u5e76\u8bc6\u522b\u4e86\u4e24\u79cd\u5931\u8d25\u6a21\u5f0f", "motivation": "\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4f1a\u5bfc\u81f4\u601d\u7ef4\u94fe\u8f93\u51fa\u53d8\u957f\uff0c\u589e\u52a0\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u8ba1\u7b97\u6210\u672c\u3002\u73b0\u6709\u957f\u5ea6\u63a7\u5236\u65b9\u6cd5\u4e2d\uff0c\u6700\u4f18\u8f93\u51fa\u957f\u5ea6\u5982\u4f55\u5e73\u8861\u6548\u7387\u4e0e\u6027\u80fd\u4ecd\u4e0d\u660e\u786e", "method": "\u5728Qwen3-1.7B Base\u548cDeepSeek-R1-Distill-Qwen-1.5B\u4e24\u4e2a\u6a21\u578b\u4e0a\u6bd4\u8f83\u591a\u79cd\u957f\u5ea6\u63a7\u5236\u65b9\u6cd5\uff0c\u5c06\u5148\u524d\u5de5\u4f5c\u6269\u5c55\u5230RL\u8bad\u7ec3\u7684\u7b56\u7565\u4e2d\u8fdb\u884c\u5206\u6790", "result": "\u957f\u5ea6\u60e9\u7f5a\u53ef\u80fd\u963b\u788d\u63a8\u7406\u80fd\u529b\u7684\u5b66\u4e60\uff0c\u800c\u5bf9\u5df2\u6709\u5f3a\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\uff0c\u9002\u5f53\u8c03\u6574\u7684\u957f\u5ea6\u63a7\u5236\u80fd\u63d0\u5347\u6548\u7387\u3002\u8bc6\u522b\u4e86\u4e24\u79cd\u5931\u8d25\u6a21\u5f0f\uff1a\u957f\u8f93\u51fa\u589e\u52a0\u5206\u6563\u6027\uff0c\u77ed\u8f93\u51fa\u5bfc\u81f4\u601d\u8003\u4e0d\u8db3", "conclusion": "\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u6a21\u578b\u72b6\u6001\uff08\u63a8\u7406\u80fd\u529b\u6c34\u5e73\uff09\u91c7\u7528\u4e0d\u540c\u7684\u957f\u5ea6\u63a7\u5236\u7b56\u7565\uff0c\u5e73\u8861\u63a8\u7406\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u907f\u514d\u8fc7\u5ea6\u60e9\u7f5a\u6216\u4e0d\u8db3\u601d\u8003\u7684\u95ee\u9898", "topic": "agentic reinforcement learning"}}
{"id": "2602.09598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09598", "abs": "https://arxiv.org/abs/2602.09598", "authors": ["Qiao Liang", "Yuke Zhu", "Chao Ge", "Lei Yang", "Ying Shen", "Bo Zheng", "Sheng Guo"], "title": "Learning from the Irrecoverable: Error-Localized Policy Optimization for Tool-Integrated LLM Reasoning", "comment": "20 pages, 11 figures", "summary": "Tool-integrated reasoning (TIR) enables LLM agents to solve tasks through planning, tool use, and iterative revision, but outcome-only reinforcement learning in this setting suffers from sparse, delayed rewards and weak step-level credit assignment. In long-horizon TIR trajectories, an early irrecoverable mistake can determine success or failure, making it crucial to localize the first irrecoverable step and leverage it for fine-grained credit assignment. We propose Error-Localized Policy Optimization (ELPO), which localizes the first irrecoverable step via binary-search rollout trees under a fixed rollout budget, converts the resulting tree into stable learning signals through hierarchical advantage attribution, and applies error-localized adaptive clipping to strengthen corrective updates on the critical step and its suffix. Across TIR benchmarks in math, science QA, and code execution, ELPO consistently outperforms strong Agentic RL baselines under comparable sampling budgets, with additional gains in Pass@K and Major@K scaling, rollout ranking quality, and tool-call efficiency. Our code will be publicly released soon.", "AI": {"tldr": "ELPO\u901a\u8fc7\u4e8c\u5206\u641c\u7d22\u5b9a\u4f4dTIR\u8f68\u8ff9\u4e2d\u9996\u4e2a\u4e0d\u53ef\u6062\u590d\u9519\u8bef\u6b65\u9aa4\uff0c\u5229\u7528\u5c42\u6b21\u4f18\u52bf\u5f52\u56e0\u548c\u81ea\u9002\u5e94\u88c1\u526a\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347Agentic RL\u5728\u6570\u5b66\u3001\u79d1\u5b66QA\u548c\u4ee3\u7801\u6267\u884c\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u5de5\u5177\u96c6\u6210\u63a8\u7406(TIR)\u4e2d\uff0c\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u7a00\u758f\u5ef6\u8fdf\u5956\u52b1\u548c\u5f31\u6b65\u9aa4\u7ea7\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002\u65e9\u671f\u4e0d\u53ef\u6062\u590d\u9519\u8bef\u4f1a\u51b3\u5b9a\u6574\u4e2a\u957f\u65f6\u7a0b\u8f68\u8ff9\u7684\u6210\u8d25\uff0c\u56e0\u6b64\u9700\u8981\u7cbe\u786e\u5b9a\u4f4d\u9996\u4e2a\u4e0d\u53ef\u6062\u590d\u6b65\u9aa4\u5e76\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\u3002", "method": "ELPO\u65b9\u6cd5\uff1a1) \u901a\u8fc7\u56fa\u5b9a\u9884\u7b97\u4e0b\u7684\u4e8c\u5206\u641c\u7d22\u5c55\u5f00\u6811\u5b9a\u4f4d\u9996\u4e2a\u4e0d\u53ef\u6062\u590d\u6b65\u9aa4\uff1b2) \u901a\u8fc7\u5c42\u6b21\u4f18\u52bf\u5f52\u56e0\u5c06\u641c\u7d22\u6811\u8f6c\u6362\u4e3a\u7a33\u5b9a\u5b66\u4e60\u4fe1\u53f7\uff1b3) \u5e94\u7528\u9519\u8bef\u5b9a\u4f4d\u81ea\u9002\u5e94\u88c1\u526a\uff0c\u52a0\u5f3a\u5bf9\u5173\u952e\u6b65\u9aa4\u53ca\u5176\u540e\u7eed\u6b65\u9aa4\u7684\u4fee\u6b63\u66f4\u65b0\u3002", "result": "\u5728\u6570\u5b66\u3001\u79d1\u5b66QA\u548c\u4ee3\u7801\u6267\u884c\u7684TIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cELPO\u5728\u53ef\u6bd4\u91c7\u6837\u9884\u7b97\u4e0b\u6301\u7eed\u4f18\u4e8e\u5f3aAgentic RL\u57fa\u7ebf\uff0c\u5728Pass@K\u548cMajor@K\u6269\u5c55\u3001\u5c55\u5f00\u6392\u5e8f\u8d28\u91cf\u548c\u5de5\u5177\u8c03\u7528\u6548\u7387\u65b9\u9762\u5747\u6709\u989d\u5916\u63d0\u5347\u3002", "conclusion": "ELPO\u901a\u8fc7\u5b9a\u4f4d\u9996\u4e2a\u4e0d\u53ef\u6062\u590d\u9519\u8bef\u5e76\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff0c\u6709\u6548\u89e3\u51b3\u4e86TIR\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u7a00\u758f\u5956\u52b1\u548c\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86Agentic RL\u5728\u5404\u79cd\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.09642", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09642", "abs": "https://arxiv.org/abs/2602.09642", "authors": ["Sieun Hyeon", "Jusang Oh", "Sunghwan Steve Cho", "Jaeyoung Do"], "title": "MATA: Multi-Agent Framework for Reliable and Flexible Table Question Answering", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have significantly improved table understanding tasks such as Table Question Answering (TableQA), yet challenges remain in ensuring reliability, scalability, and efficiency, especially in resource-constrained or privacy-sensitive environments. In this paper, we introduce MATA, a multi-agent TableQA framework that leverages multiple complementary reasoning paths and a set of tools built with small language models. MATA generates candidate answers through diverse reasoning styles for a given table and question, then refines or selects the optimal answer with the help of these tools. Furthermore, it incorporates an algorithm designed to minimize expensive LLM agent calls, enhancing overall efficiency. MATA maintains strong performance with small, open-source models and adapts easily across various LLM types. Extensive experiments on two benchmarks of varying difficulty with ten different LLMs demonstrate that MATA achieves state-of-the-art accuracy and highly efficient reasoning while avoiding excessive LLM inference. Our results highlight that careful orchestration of multiple reasoning pathways yields scalable and reliable TableQA. The code is available at https://github.com/AIDAS-Lab/MATA.", "AI": {"tldr": "MATA\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u8868\u683c\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u79cd\u63a8\u7406\u8def\u5f84\u548c\u5c0f\u8bed\u8a00\u6a21\u578b\u5de5\u5177\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u8868\u683c\u7406\u89e3\uff0c\u51cf\u5c11\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8868\u683c\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u6216\u9690\u79c1\u654f\u611f\u73af\u5883\u4e2d\u4ecd\u9762\u4e34\u53ef\u9760\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u79cd\u4e92\u8865\u63a8\u7406\u8def\u5f84\u751f\u6210\u5019\u9009\u7b54\u6848\uff0c\u5229\u7528\u5c0f\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7684\u5de5\u5177\u96c6\u8fdb\u884c\u7b54\u6848\u7cbe\u70bc\u6216\u9009\u62e9\uff0c\u5e76\u8bbe\u8ba1\u7b97\u6cd5\u6700\u5c0f\u5316\u6602\u8d35\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8c03\u7528\u3002", "result": "\u5728\u4e24\u4e2a\u4e0d\u540c\u96be\u5ea6\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528\u5341\u79cd\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0cMATA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u548c\u9ad8\u6548\u63a8\u7406\uff0c\u540c\u65f6\u907f\u514d\u8fc7\u5ea6\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002", "conclusion": "\u7cbe\u5fc3\u7f16\u6392\u591a\u79cd\u63a8\u7406\u8def\u5f84\u53ef\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u53ef\u9760\u7684\u8868\u683c\u95ee\u7b54\uff0cMATA\u6846\u67b6\u5728\u4fdd\u6301\u5c0f\u5f00\u6e90\u6a21\u578b\u5f3a\u6027\u80fd\u7684\u540c\u65f6\uff0c\u80fd\u8f7b\u677e\u9002\u5e94\u5404\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u7c7b\u578b\u3002", "topic": "agent analysis"}}
{"id": "2602.09300", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09300", "abs": "https://arxiv.org/abs/2602.09300", "authors": ["Sumedh Gupte", "Shrey Rakeshkumar Patel", "Soumen Pachal", "Prashanth L. A.", "Sanjay P. Bhat"], "title": "Risk-sensitive reinforcement learning using expectiles, shortfall risk and optimized certainty equivalent risk", "comment": null, "summary": "We propose risk-sensitive reinforcement learning algorithms catering to three families of risk measures, namely expectiles, utility-based shortfall risk and optimized certainty equivalent risk. For each risk measure, in the context of a finite horizon Markov decision process, we first derive a policy gradient theorem. Second, we propose estimators of the risk-sensitive policy gradient for each of the aforementioned risk measures, and establish $\\mathcal{O}\\left(1/m\\right)$ mean-squared error bounds for our estimators, where $m$ is the number of trajectories. Further, under standard assumptions for policy gradient-type algorithms, we establish smoothness of the risk-sensitive objective, in turn leading to stationary convergence rate bounds for the overall risk-sensitive policy gradient algorithm that we propose. Finally, we conduct numerical experiments to validate the theoretical findings on popular RL benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9488\u5bf9\u4e09\u79cd\u98ce\u9669\u5ea6\u91cf\uff08\u671f\u671b\u5206\u4f4d\u6570\u3001\u57fa\u4e8e\u6548\u7528\u7684\u77ed\u7f3a\u98ce\u9669\u548c\u4f18\u5316\u786e\u5b9a\u6027\u7b49\u4ef7\u98ce\u9669\uff09\u7684\u98ce\u9669\u654f\u611f\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u63a8\u5bfc\u4e86\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\uff0c\u8bbe\u8ba1\u4e86\u68af\u5ea6\u4f30\u8ba1\u5668\u5e76\u5efa\u7acb\u4e86\u8bef\u5dee\u754c\uff0c\u6700\u7ec8\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u5173\u6ce8\u671f\u671b\u56de\u62a5\u6700\u5927\u5316\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u8003\u8651\u98ce\u9669\u56e0\u7d20\u3002\u672c\u6587\u65e8\u5728\u4e3a\u4e09\u79cd\u91cd\u8981\u7684\u98ce\u9669\u5ea6\u91cf\u5f00\u53d1\u98ce\u9669\u654f\u611f\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ee5\u5904\u7406\u51b3\u7b56\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u98ce\u9669\u3002", "method": "1) \u5728\u6709\u9650\u65f6\u57df\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6846\u67b6\u4e0b\uff0c\u4e3a\u4e09\u79cd\u98ce\u9669\u5ea6\u91cf\u63a8\u5bfc\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\uff1b2) \u4e3a\u6bcf\u79cd\u98ce\u9669\u5ea6\u91cf\u8bbe\u8ba1\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u5668\uff1b3) \u5efa\u7acb\u4f30\u8ba1\u5668\u7684\u5747\u65b9\u8bef\u5dee\u754c\uff1b4) \u5728\u6807\u51c6\u5047\u8bbe\u4e0b\u8bc1\u660e\u98ce\u9669\u654f\u611f\u76ee\u6807\u51fd\u6570\u7684\u5e73\u6ed1\u6027\uff1b5) \u63d0\u51fa\u5b8c\u6574\u7684\u98ce\u9669\u654f\u611f\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u5e76\u5206\u6790\u6536\u655b\u6027\u3002", "result": "1) \u6210\u529f\u63a8\u5bfc\u4e86\u4e09\u79cd\u98ce\u9669\u5ea6\u91cf\u7684\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\uff1b2) \u63d0\u51fa\u7684\u68af\u5ea6\u4f30\u8ba1\u5668\u5177\u6709O(1/m)\u7684\u5747\u65b9\u8bef\u5dee\u754c\uff08m\u4e3a\u8f68\u8ff9\u6570\uff09\uff1b3) \u8bc1\u660e\u4e86\u98ce\u9669\u654f\u611f\u76ee\u6807\u51fd\u6570\u7684\u5e73\u6ed1\u6027\uff1b4) \u5efa\u7acb\u4e86\u7b97\u6cd5\u7684\u5e73\u7a33\u70b9\u6536\u655b\u901f\u7387\u754c\uff1b5) \u5728\u6807\u51c6RL\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e09\u79cd\u91cd\u8981\u7684\u98ce\u9669\u5ea6\u91cf\u5f00\u53d1\u4e86\u98ce\u9669\u654f\u611f\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u6709\u6548\u7684\u68af\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4e3a\u98ce\u9669\u654f\u611f\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7b97\u6cd5\u5de5\u5177\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.09712", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09712", "abs": "https://arxiv.org/abs/2602.09712", "authors": ["Yiming Shu", "Pei Liu", "Tiange Zhang", "Ruiyang Gao", "Jun Ma", "Chen Sun"], "title": "TraceMem: Weaving Narrative Memory Schemata from User Conversational Traces", "comment": null, "summary": "Sustaining long-term interactions remains a bottleneck for Large Language Models (LLMs), as their limited context windows struggle to manage dialogue histories that extend over time. Existing memory systems often treat interactions as disjointed snippets, failing to capture the underlying narrative coherence of the dialogue stream. We propose TraceMem, a cognitively-inspired framework that weaves structured, narrative memory schemata from user conversational traces through a three-stage pipeline: (1) Short-term Memory Processing, which employs a deductive topic segmentation approach to demarcate episode boundaries and extract semantic representation; (2) Synaptic Memory Consolidation, a process that summarizes episodes into episodic memories before distilling them alongside semantics into user-specific traces; and (3) Systems Memory Consolidation, which utilizes two-stage hierarchical clustering to organize these traces into coherent, time-evolving narrative threads under unifying themes. These threads are encapsulated into structured user memory cards, forming narrative memory schemata. For memory utilization, we provide an agentic search mechanism to enhance reasoning process. Evaluation on the LoCoMo benchmark shows that TraceMem achieves state-of-the-art performance with a brain-inspired architecture. Analysis shows that by constructing coherent narratives, it surpasses baselines in multi-hop and temporal reasoning, underscoring its essential role in deep narrative comprehension. Additionally, we provide an open discussion on memory systems, offering our perspectives and future outlook on the field. Our code implementation is available at: https://github.com/YimingShu-teay/TraceMem", "AI": {"tldr": "TraceMem\u662f\u4e00\u4e2a\u53d7\u8ba4\u77e5\u542f\u53d1\u7684\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\u4ece\u7528\u6237\u5bf9\u8bdd\u8f68\u8ff9\u6784\u5efa\u7ed3\u6784\u5316\u53d9\u4e8b\u8bb0\u5fc6\u6a21\u5f0f\uff0c\u5728\u957f\u7a0b\u5bf9\u8bdd\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ef4\u6301\u957f\u671f\u4e92\u52a8\u65b9\u9762\u5b58\u5728\u74f6\u9888\uff0c\u5176\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u96be\u4ee5\u7ba1\u7406\u968f\u65f6\u95f4\u5ef6\u4f38\u7684\u5bf9\u8bdd\u5386\u53f2\u3002\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u901a\u5e38\u5c06\u4e92\u52a8\u89c6\u4e3a\u79bb\u6563\u7247\u6bb5\uff0c\u65e0\u6cd5\u6355\u6349\u5bf9\u8bdd\u6d41\u7684\u5e95\u5c42\u53d9\u4e8b\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51faTraceMem\u6846\u67b6\uff0c\u5305\u542b\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a1) \u77ed\u671f\u8bb0\u5fc6\u5904\u7406\uff1a\u4f7f\u7528\u6f14\u7ece\u5f0f\u4e3b\u9898\u5206\u5272\u65b9\u6cd5\u5212\u5b9a\u4e8b\u4ef6\u8fb9\u754c\u5e76\u63d0\u53d6\u8bed\u4e49\u8868\u793a\uff1b2) \u7a81\u89e6\u8bb0\u5fc6\u5de9\u56fa\uff1a\u5c06\u4e8b\u4ef6\u603b\u7ed3\u4e3a\u60c5\u8282\u8bb0\u5fc6\uff0c\u7136\u540e\u4e0e\u8bed\u4e49\u4e00\u8d77\u63d0\u70bc\u4e3a\u7528\u6237\u7279\u5b9a\u8f68\u8ff9\uff1b3) \u7cfb\u7edf\u8bb0\u5fc6\u5de9\u56fa\uff1a\u4f7f\u7528\u4e24\u9636\u6bb5\u5c42\u6b21\u805a\u7c7b\u5c06\u8fd9\u4e9b\u8f68\u8ff9\u7ec4\u7ec7\u6210\u8fde\u8d2f\u7684\u3001\u968f\u65f6\u95f4\u6f14\u5316\u7684\u53d9\u4e8b\u7ebf\u7a0b\uff0c\u5f62\u6210\u7ed3\u6784\u5316\u7528\u6237\u8bb0\u5fc6\u5361\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTraceMem\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5206\u6790\u8868\u660e\uff0c\u901a\u8fc7\u6784\u5efa\u8fde\u8d2f\u7684\u53d9\u4e8b\uff0c\u5b83\u5728\u591a\u8df3\u63a8\u7406\u548c\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u8d85\u8d8a\u4e86\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7a81\u663e\u4e86\u5176\u5728\u6df1\u5ea6\u53d9\u4e8b\u7406\u89e3\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "TraceMem\u901a\u8fc7\u8ba4\u77e5\u542f\u53d1\u7684\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u957f\u671f\u4e92\u52a8\u4e2d\u7684\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.09719", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09719", "abs": "https://arxiv.org/abs/2602.09719", "authors": ["Longhuan Xu", "Cunjian Chen", "Feng Yin"], "title": "Unsupervised Layer-Wise Dynamic Test Time Adaptation for LLMs", "comment": null, "summary": "Test-time adaptation (TTA) for large language models (LLMs) updates model parameters at inference time using signals available at deployment. This paper focuses on a common yet under-explored regime: unsupervised, sample-specific TTA, where the model adapts independently for each prompt using only the prompt itself, without gold answers or external supervision. Although appealing, naive unsupervised TTA with a fixed, handcrafted learning rate can be unstable: updates may overfit to prompt-specific statistics, drift from the desired answer distribution, and ultimately degrade generation quality. This failure mode is not surprising, as in this case TTA must adapt to a single prompt within only a few gradient steps, unlike standard training that averages updates over large datasets and long optimization horizons. Therefore, we propose layer-wise dynamic test-time adaptation, a framework which explicitly modulates TTA strength as a function of prompt representation, LLM structure and adaptation step. In our setting, TTA updates only LoRA parameters, and a lightweight hypernetwork predicts per-layer, per-step learning-rate multipliers, enabling fine-grained control. Experiments across various datasets and LLMs consistently show that our method substantially strengthens TTA by learning effective scaling patterns over adaptation steps and transformer layer projections, improving stability while delivering better performance.", "AI": {"tldr": "\u63d0\u51fa\u5c42\u7ea7\u52a8\u6001\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u9884\u6d4b\u6bcf\u5c42\u6bcf\u6b65\u7684\u5b66\u4e60\u7387\u4e58\u5b50\uff0c\u89e3\u51b3\u65e0\u76d1\u7763\u5355\u6837\u672cTTA\u4e2d\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u5347LLM\u5728\u63a8\u7406\u65f6\u7684\u9002\u5e94\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u76d1\u7763\u3001\u6837\u672c\u7279\u5b9a\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u4e2d\uff0c\u4f7f\u7528\u56fa\u5b9a\u624b\u5de5\u5b66\u4e60\u7387\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\uff1a\u66f4\u65b0\u53ef\u80fd\u8fc7\u62df\u5408\u5230\u63d0\u793a\u7279\u5b9a\u7edf\u8ba1\u4fe1\u606f\uff0c\u504f\u79bb\u671f\u671b\u7b54\u6848\u5206\u5e03\uff0c\u6700\u7ec8\u964d\u4f4e\u751f\u6210\u8d28\u91cf\u3002\u8fd9\u79cd\u5931\u8d25\u6a21\u5f0f\u6e90\u4e8eTTA\u9700\u8981\u5728\u5c11\u91cf\u68af\u5ea6\u6b65\u5185\u9002\u5e94\u5355\u4e2a\u63d0\u793a\uff0c\u800c\u6807\u51c6\u8bad\u7ec3\u5219\u5728\u5927\u6570\u636e\u96c6\u548c\u957f\u4f18\u5316\u65f6\u57df\u4e0a\u5e73\u5747\u66f4\u65b0\u3002", "method": "\u63d0\u51fa\u5c42\u7ea7\u52a8\u6001\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u5c06TTA\u5f3a\u5ea6\u660e\u786e\u8c03\u5236\u4e3a\u63d0\u793a\u8868\u793a\u3001LLM\u7ed3\u6784\u548c\u9002\u5e94\u6b65\u9aa4\u7684\u51fd\u6570\u3002\u5728\u8bbe\u7f6e\u4e2d\uff0cTTA\u4ec5\u66f4\u65b0LoRA\u53c2\u6570\uff0c\u8f7b\u91cf\u7ea7\u8d85\u7f51\u7edc\u9884\u6d4b\u6bcf\u5c42\u6bcf\u6b65\u7684\u5b66\u4e60\u7387\u4e58\u5b50\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "result": "\u5728\u5404\u79cd\u6570\u636e\u96c6\u548cLLM\u4e0a\u7684\u5b9e\u9a8c\u4e00\u81f4\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u9002\u5e94\u6b65\u9aa4\u548cTransformer\u5c42\u6295\u5f71\u4e0a\u7684\u6709\u6548\u7f29\u653e\u6a21\u5f0f\uff0c\u663e\u8457\u589e\u5f3a\u4e86TTA\uff0c\u5728\u63d0\u9ad8\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u5c42\u7ea7\u52a8\u6001\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u63a7\u5236\u5b66\u4e60\u7387\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u5355\u6837\u672cTTA\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3aLLM\u5728\u63a8\u7406\u65f6\u7684\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.09305", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09305", "abs": "https://arxiv.org/abs/2602.09305", "authors": ["Pei-Chi Pan", "Yingbin Liang", "Sen Lin"], "title": "Reward Modeling for Reinforcement Learning-Based LLM Reasoning: Design, Challenges, and Evaluation", "comment": null, "summary": "Large Language Models (LLMs) demonstrate transformative potential, yet their reasoning remains inconsistent and unreliable. Reinforcement learning (RL)-based fine-tuning is a key mechanism for improvement, but its effectiveness is fundamentally governed by reward design. Despite its importance, the relationship between reward modeling and core LLM challenges--such as evaluation bias, hallucination, distribution shift, and efficient learning--remains poorly understood. This work argues that reward modeling is not merely an implementation detail but a central architect of reasoning alignment, shaping what models learn, how they generalize, and whether their outputs can be trusted. We introduce Reasoning-Aligned Reinforcement Learning (RARL), a unifying framework that systematizes diverse reward paradigms for multi-step reasoning. Within this framework, we present a taxonomy of reward mechanisms, analyze reward hacking as a pervasive failure mode, and examine how reward signals unify challenges ranging from inference-time scaling to hallucination mitigation. We further critically evaluate existing benchmarks, highlighting vulnerabilities such as data contamination and reward misalignment, and outline directions for more robust evaluation. By integrating fragmented research threads and clarifying the interplay between reward design and fundamental reasoning capabilities, this work provides a foundational roadmap for building reasoning models that are robust, verifiable, and trustworthy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u63a8\u7406\u5bf9\u9f50\u5f3a\u5316\u5b66\u4e60(RARL)\u6846\u67b6\uff0c\u7cfb\u7edf\u5316\u591a\u6b65\u63a8\u7406\u7684\u5956\u52b1\u673a\u5236\uff0c\u5206\u6790\u5956\u52b1\u5efa\u6a21\u5bf9LLM\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u5f71\u54cd", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u662f\u6539\u8fdbLLM\u7684\u5173\u952e\u673a\u5236\uff0c\u4f46\u5956\u52b1\u8bbe\u8ba1\u7684\u6548\u679c\u4e0eLLM\u6838\u5fc3\u6311\u6218\uff08\u5982\u8bc4\u4f30\u504f\u5dee\u3001\u5e7b\u89c9\u3001\u5206\u5e03\u504f\u79fb\u7b49\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\u4ecd\u4e0d\u6e05\u695a\u3002\u5956\u52b1\u5efa\u6a21\u4e0d\u4ec5\u662f\u5b9e\u73b0\u7ec6\u8282\uff0c\u800c\u662f\u63a8\u7406\u5bf9\u9f50\u7684\u6838\u5fc3\u67b6\u6784\u5e08", "method": "\u63d0\u51fa\u63a8\u7406\u5bf9\u9f50\u5f3a\u5316\u5b66\u4e60(RARL)\u7edf\u4e00\u6846\u67b6\uff0c\u7cfb\u7edf\u5316\u591a\u6b65\u63a8\u7406\u7684\u5956\u52b1\u8303\u5f0f\uff0c\u5efa\u7acb\u5956\u52b1\u673a\u5236\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u5956\u52b1\u653b\u51fb\u4f5c\u4e3a\u666e\u904d\u5931\u6548\u6a21\u5f0f\uff0c\u5e76\u7814\u7a76\u5956\u52b1\u4fe1\u53f7\u5982\u4f55\u7edf\u4e00\u4ece\u63a8\u7406\u65f6\u6269\u5c55\u5230\u5e7b\u89c9\u7f13\u89e3\u7b49\u6311\u6218", "result": "\u901a\u8fc7\u6574\u5408\u788e\u7247\u5316\u7814\u7a76\u7ebf\u7d22\uff0c\u9610\u660e\u5956\u52b1\u8bbe\u8ba1\u4e0e\u57fa\u672c\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u6784\u5efa\u7a33\u5065\u3001\u53ef\u9a8c\u8bc1\u548c\u53ef\u4fe1\u8d56\u7684\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u57fa\u7840\u8def\u7ebf\u56fe", "conclusion": "\u5956\u52b1\u5efa\u6a21\u662f\u63a8\u7406\u5bf9\u9f50\u7684\u6838\u5fc3\u67b6\u6784\u5e08\uff0c\u5851\u9020\u6a21\u578b\u5b66\u4e60\u5185\u5bb9\u3001\u6cdb\u5316\u65b9\u5f0f\u4ee5\u53ca\u8f93\u51fa\u53ef\u4fe1\u5ea6\u3002RARL\u6846\u67b6\u4e3a\u7cfb\u7edf\u5316\u5956\u52b1\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3LLM\u63a8\u7406\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u6311\u6218", "topic": "agentic reinforcement learning"}}
{"id": "2602.09805", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09805", "abs": "https://arxiv.org/abs/2602.09805", "authors": ["Daniel Kaiser", "Arnoldo Frigessi", "Ali Ramezani-Kebrya", "Benjamin Ricaud"], "title": "Decomposing Reasoning Efficiency in Large Language Models", "comment": "Preprint (under review). 29 pages, 4 figures", "summary": "Large language models trained for reasoning trade off inference tokens against accuracy, yet standard evaluations report only final accuracy, obscuring where tokens are spent or wasted. We introduce a trace-optional framework that decomposes token efficiency into interpretable factors: completion under a fixed token budget (avoiding truncation), conditional correctness given completion, and verbosity (token usage). When benchmark metadata provides per-instance workload proxies, we further factor verbosity into two components: mean verbalization overhead (tokens per work unit) and a coupling coefficient capturing how overhead scales with task workload. When reasoning traces are available, we add deterministic trace-quality measures (grounding, repetition, prompt copying) to separate degenerate looping from verbose-but-engaged reasoning, avoiding human labeling and LLM judges. Evaluating 25 models on CogniLoad, we find that accuracy and token-efficiency rankings diverge (Spearman $\u03c1=0.63$), efficiency gaps are often driven by conditional correctness, and verbalization overhead varies by about 9 times (only weakly related to model scale). Our decomposition reveals distinct bottleneck profiles that suggest different efficiency interventions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u8ffd\u8e2a\u7684\u6846\u67b6\u6765\u5206\u89e3LLM\u63a8\u7406\u4efb\u52a1\u7684token\u6548\u7387\uff0c\u5305\u62ec\u5b8c\u6210\u5ea6\u3001\u6761\u4ef6\u6b63\u786e\u6027\u548c\u5197\u4f59\u5ea6\u7b49\u53ef\u89e3\u91ca\u56e0\u7d20\uff0c\u5e76\u572825\u4e2a\u6a21\u578b\u4e0a\u8bc4\u4f30\u53d1\u73b0\u51c6\u786e\u7387\u4e0etoken\u6548\u7387\u6392\u540d\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8bc4\u4f30\u53ea\u62a5\u544a\u6700\u7ec8\u51c6\u786e\u7387\uff0c\u63a9\u76d6\u4e86token\u4f7f\u7528\u6548\u7387\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u4e86\u89e3token\u5728\u54ea\u91cc\u88ab\u6709\u6548\u4f7f\u7528\u6216\u6d6a\u8d39\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u6548\u7387\u5206\u6790\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u53ef\u8ffd\u8e2a\u6846\u67b6\u5c06token\u6548\u7387\u5206\u89e3\u4e3a\uff1a\u56fa\u5b9atoken\u9884\u7b97\u4e0b\u7684\u5b8c\u6210\u5ea6\u3001\u5b8c\u6210\u540e\u7684\u6761\u4ef6\u6b63\u786e\u6027\u3001\u5197\u4f59\u5ea6\uff1b\u5f53\u6709\u57fa\u51c6\u5143\u6570\u636e\u65f6\uff0c\u8fdb\u4e00\u6b65\u5c06\u5197\u4f59\u5ea6\u5206\u89e3\u4e3a\u5e73\u5747\u8bed\u8a00\u5316\u5f00\u9500\u548c\u8026\u5408\u7cfb\u6570\uff1b\u5f53\u6709\u63a8\u7406\u8f68\u8ff9\u65f6\uff0c\u6dfb\u52a0\u786e\u5b9a\u6027\u8f68\u8ff9\u8d28\u91cf\u5ea6\u91cf\uff08\u57fa\u7840\u6027\u3001\u91cd\u590d\u6027\u3001\u63d0\u793a\u590d\u5236\uff09\u3002\u5728CogniLoad\u57fa\u51c6\u4e0a\u8bc4\u4f3025\u4e2a\u6a21\u578b\u3002", "result": "\u51c6\u786e\u7387\u548ctoken\u6548\u7387\u6392\u540d\u5b58\u5728\u5dee\u5f02\uff08Spearman \u03c1=0.63\uff09\uff0c\u6548\u7387\u5dee\u8ddd\u4e3b\u8981\u7531\u6761\u4ef6\u6b63\u786e\u6027\u9a71\u52a8\uff0c\u8bed\u8a00\u5316\u5f00\u9500\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u5dee\u5f02\u7ea69\u500d\uff08\u4e0e\u6a21\u578b\u89c4\u6a21\u5173\u7cfb\u8f83\u5f31\uff09\uff0c\u5206\u89e3\u63ed\u793a\u4e86\u4e0d\u540c\u7684\u74f6\u9888\u6a21\u5f0f\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u63ed\u793aLLM\u63a8\u7406\u4e2dtoken\u6548\u7387\u7684\u4e0d\u540c\u74f6\u9888\u6a21\u5f0f\uff0c\u4e3a\u9488\u5bf9\u6027\u7684\u6548\u7387\u4f18\u5316\u5e72\u9884\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u8d85\u8d8a\u4e86\u4ec5\u5173\u6ce8\u6700\u7ec8\u51c6\u786e\u7387\u7684\u4f20\u7edf\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2602.09817", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2602.09817", "abs": "https://arxiv.org/abs/2602.09817", "authors": ["Khang Ly", "Georgios Cheirmpos", "Adrian Raudaschl", "Christopher James", "Seyed Amin Tabatabaei"], "title": "AnalyticsGPT: An LLM Workflow for Scientometric Question Answering", "comment": null, "summary": "This paper introduces AnalyticsGPT, an intuitive and efficient large language model (LLM)-powered workflow for scientometric question answering. This underrepresented downstream task addresses the subcategory of meta-scientific questions concerning the \"science of science.\" When compared to traditional scientific question answering based on papers, the task poses unique challenges in the planning phase. Namely, the need for named-entity recognition of academic entities within questions and multi-faceted data retrieval involving scientometric indices, e.g. impact factors. Beyond their exceptional capacity for treating traditional natural language processing tasks, LLMs have shown great potential in more complex applications, such as task decomposition and planning and reasoning. In this paper, we explore the application of LLMs to scientometric question answering, and describe an end-to-end system implementing a sequential workflow with retrieval-augmented generation and agentic concepts. We also address the secondary task of effectively synthesizing the data into presentable and well-structured high-level analyses. As a database for retrieval-augmented generation, we leverage a proprietary research performance assessment platform. For evaluation, we consult experienced subject matter experts and leverage LLMs-as-judges. In doing so, we provide valuable insights on the efficacy of LLMs towards a niche downstream task. Our (skeleton) code and prompts are available at: https://github.com/lyvykhang/llm-agents-scientometric-qa/tree/acl.", "AI": {"tldr": "AnalyticsGPT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u8ba1\u91cf\u95ee\u7b54\u5de5\u4f5c\u6d41\uff0c\u4e13\u95e8\u5904\u7406\"\u79d1\u5b66\u4e4b\u79d1\u5b66\"\u8fd9\u7c7b\u5143\u79d1\u5b66\u95ee\u9898\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u667a\u80fd\u4f53\u6982\u5ff5\u5b9e\u73b0\u7aef\u5230\u7aef\u7cfb\u7edf\u3002", "motivation": "\u79d1\u5b66\u8ba1\u91cf\u95ee\u7b54\u4f5c\u4e3a\u79d1\u5b66\u95ee\u7b54\u7684\u5b50\u4efb\u52a1\uff0c\u5728\u89c4\u5212\u9636\u6bb5\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u5b66\u672f\u5b9e\u4f53\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u6d89\u53ca\u5f71\u54cd\u56e0\u5b50\u7b49\u591a\u7ef4\u5ea6\u6570\u636e\u68c0\u7d22\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f20\u7edfNLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u5e94\u7528\u5982\u4efb\u52a1\u5206\u89e3\u3001\u89c4\u5212\u548c\u63a8\u7406\u65b9\u9762\u4e5f\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u672c\u6587\u63a2\u7d22LLM\u5728\u79d1\u5b66\u8ba1\u91cf\u95ee\u7b54\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u7cfb\u7edf\u5b9e\u73b0\u987a\u5e8f\u5de5\u4f5c\u6d41\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u667a\u80fd\u4f53\u6982\u5ff5\u3002\u4f7f\u7528\u4e13\u6709\u7684\u7814\u7a76\u7ee9\u6548\u8bc4\u4f30\u5e73\u53f0\u4f5c\u4e3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u6570\u636e\u5e93\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5904\u7406\u79d1\u5b66\u8ba1\u91cf\u95ee\u9898\uff0c\u5e76\u6709\u6548\u5408\u6210\u6570\u636e\u4e3a\u53ef\u5448\u73b0\u7684\u9ad8\u5c42\u6b21\u5206\u6790\u3002", "result": "\u901a\u8fc7\u54a8\u8be2\u7ecf\u9a8c\u4e30\u5bcc\u7684\u9886\u57df\u4e13\u5bb6\u548c\u5229\u7528LLM-as-judges\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e3a\u8fd9\u4e00\u5c0f\u4f17\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u5173\u4e8eLLM\u6709\u6548\u6027\u7684\u5b9d\u8d35\u89c1\u89e3\u3002\u4ee3\u7801\u548c\u63d0\u793a\u5df2\u5f00\u6e90\u3002", "conclusion": "AnalyticsGPT\u5c55\u793a\u4e86LLM\u5728\u79d1\u5b66\u8ba1\u91cf\u95ee\u7b54\u8fd9\u4e00\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u5143\u79d1\u5b66\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2602.09375", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09375", "abs": "https://arxiv.org/abs/2602.09375", "authors": ["Hanchen Xia", "Baoyou Chen", "Zelin Zang", "Yutang Ge", "Guojiang Zhao", "Siyu Zhu"], "title": "Latent Poincar\u00e9 Shaping for Agentic Reinforcement Learning", "comment": null, "summary": "We propose LaPha, a method for training AlphaZero-like LLM agents in a Poincar\u00e9 latent space. Under LaPha, the search process can be visualized as a tree rooted at the prompt and growing outward from the origin toward the boundary of the Poincar\u00e9 ball, where negative curvature provides exponentially increasing capacity with radius. Using hyperbolic geodesic distance to rule-verified correctness, we define a node potential and assign dense process rewards by potential differences. We further attach a lightweight value head on the same shared latent space, enabling self-guided test-time scaling with almost no additional overhead. On MATH-500, LaPha improves Qwen2.5-Math-1.5B from 66.0% to 88.2%. With value-head-guided search, LaPha-1.5B reaches 56.7% accuracy on AIME'24, and LaPha-7B further achieves 60.0% on AIME'24 and 53.3% on AIME'25.", "AI": {"tldr": "LaPha\u662f\u4e00\u79cd\u5728\u5e9e\u52a0\u83b1\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3AlphaZero\u5f0fLLM\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u53cc\u66f2\u51e0\u4f55\u7684\u8d1f\u66f2\u7387\u7279\u6027\uff0c\u901a\u8fc7\u6d4b\u5730\u7ebf\u8ddd\u79bb\u5b9a\u4e49\u8282\u70b9\u6f5c\u529b\u5e76\u5206\u914d\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u641c\u7d22\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002\u5e9e\u52a0\u83b1\u7a7a\u95f4\u7684\u8d1f\u66f2\u7387\u7279\u6027\u53ef\u4ee5\u63d0\u4f9b\u6307\u6570\u7ea7\u589e\u957f\u7684\u5bb9\u91cf\uff0c\u9002\u5408\u8868\u793a\u5c42\u6b21\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u5728\u5e9e\u52a0\u83b1\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3AlphaZero\u5f0fLLM\u4ee3\u7406\uff0c\u4f7f\u7528\u53cc\u66f2\u6d4b\u5730\u7ebf\u8ddd\u79bb\u5230\u89c4\u5219\u9a8c\u8bc1\u6b63\u786e\u6027\u6765\u5b9a\u4e49\u8282\u70b9\u6f5c\u529b\uff0c\u901a\u8fc7\u6f5c\u529b\u5dee\u5f02\u5206\u914d\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\u3002\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e0a\u9644\u52a0\u8f7b\u91cf\u7ea7\u4ef7\u503c\u5934\uff0c\u5b9e\u73b0\u81ea\u5f15\u5bfc\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "result": "\u5728MATH-500\u4e0a\uff0cLaPha\u5c06Qwen2.5-Math-1.5B\u4ece66.0%\u63d0\u5347\u523088.2%\u3002\u4f7f\u7528\u4ef7\u503c\u5934\u5f15\u5bfc\u641c\u7d22\u65f6\uff0cLaPha-1.5B\u5728AIME'24\u4e0a\u8fbe\u523056.7%\u51c6\u786e\u7387\uff0cLaPha-7B\u5728AIME'24\u4e0a\u8fbe\u523060.0%\uff0c\u5728AIME'25\u4e0a\u8fbe\u523053.3%\u3002", "conclusion": "LaPha\u901a\u8fc7\u5e9e\u52a0\u83b1\u7a7a\u95f4\u4e2d\u7684\u53cc\u66f2\u51e0\u4f55\u8868\u793a\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u53cc\u66f2\u6f5c\u5728\u7a7a\u95f4\u5bf9\u5c42\u6b21\u5316\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u7684\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.09396", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09396", "abs": "https://arxiv.org/abs/2602.09396", "authors": ["Nilaksh", "Antoine Clavaud", "Mathieu Reymond", "Fran\u00e7ois Rivest", "Sarath Chandar"], "title": "Squeezing More from the Stream : Learning Representation Online for Streaming Reinforcement Learning", "comment": "8 pages, 4 figures", "summary": "In streaming Reinforcement Learning (RL), transitions are observed and discarded immediately after a single update. While this minimizes resource usage for on-device applications, it makes agents notoriously sample-inefficient, since value-based losses alone struggle to extract meaningful representations from transient data. We propose extending Self-Predictive Representations (SPR) to the streaming pipeline to maximize the utility of every observed frame. However, due to the highly correlated samples induced by the streaming regime, naively applying this auxiliary loss results in training instabilities. Thus, we introduce orthogonal gradient updates relative to the momentum target and resolve gradient conflicts arising from streaming-specific optimizers. Validated across the Atari, MinAtar, and Octax suites, our approach systematically outperforms existing streaming baselines. Latent-space analysis, including t-SNE visualizations and effective-rank measurements, confirms that our method learns significantly richer representations, bridging the performance gap caused by the absence of a replay buffer, while remaining efficient enough to train on just a few CPU cores.", "AI": {"tldr": "\u5c06\u81ea\u9884\u6d4b\u8868\u793a(SPR)\u6269\u5c55\u5230\u6d41\u5f0f\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u6b63\u4ea4\u68af\u5ea6\u66f4\u65b0\u89e3\u51b3\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u6d41\u5f0f\u57fa\u7ebf\uff0c\u5b66\u4e60\u5230\u66f4\u4e30\u5bcc\u7684\u8868\u793a", "motivation": "\u6d41\u5f0f\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u6570\u636e\u5728\u5355\u6b21\u66f4\u65b0\u540e\u7acb\u5373\u4e22\u5f03\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u3002\u4ec5\u57fa\u4e8e\u4ef7\u503c\u7684\u635f\u5931\u96be\u4ee5\u4ece\u77ac\u6001\u6570\u636e\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u9700\u8981\u6700\u5927\u5316\u6bcf\u4e2a\u89c2\u5bdf\u5e27\u7684\u6548\u7528", "method": "\u5c06\u81ea\u9884\u6d4b\u8868\u793a(SPR)\u6269\u5c55\u5230\u6d41\u5f0f\u7ba1\u9053\u4e2d\uff0c\u5f15\u5165\u76f8\u5bf9\u4e8e\u52a8\u91cf\u76ee\u6807\u7684\u6b63\u4ea4\u68af\u5ea6\u66f4\u65b0\uff0c\u89e3\u51b3\u6d41\u5f0f\u7279\u5b9a\u4f18\u5316\u5668\u5f15\u8d77\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898", "result": "\u5728Atari\u3001MinAtar\u548cOctax\u5957\u4ef6\u4e0a\u7cfb\u7edf\u6027\u5730\u8d85\u8d8a\u73b0\u6709\u6d41\u5f0f\u57fa\u7ebf\u3002\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\uff08t-SNE\u53ef\u89c6\u5316\u548c\u6709\u6548\u79e9\u6d4b\u91cf\uff09\u8bc1\u5b9e\u65b9\u6cd5\u5b66\u4e60\u5230\u663e\u8457\u66f4\u4e30\u5bcc\u7684\u8868\u793a", "conclusion": "\u8be5\u65b9\u6cd5\u5f25\u8865\u4e86\u7531\u4e8e\u7f3a\u5c11\u56de\u653e\u7f13\u51b2\u533a\u9020\u6210\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u8db3\u591f\u9ad8\u6548\uff0c\u4ec5\u9700\u5c11\u91cfCPU\u6838\u5fc3\u5373\u53ef\u8bad\u7ec3\uff0c\u4e3a\u6d41\u5f0fRL\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5", "topic": "agentic reinforcement learning"}}
{"id": "2602.09877", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09877", "abs": "https://arxiv.org/abs/2602.09877", "authors": ["Chenxu Wang", "Chaozhuo Li", "Songyang Liu", "Zejian Chen", "Jinyu Hou", "Ji Qi", "Rui Li", "Litian Zhang", "Qiwei Ye", "Zheng Liu", "Xu Chen", "Xi Zhang", "Philip S. Yu"], "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "comment": null, "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u5b8c\u5168\u5c01\u95ed\u7684LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u6301\u7eed\u81ea\u6211\u8fdb\u5316\u3001\u5b8c\u5168\u9694\u79bb\u548c\u5b89\u5168\u4e0d\u53d8\u6027\uff0c\u5b58\u5728\"\u81ea\u6211\u8fdb\u5316\u4e09\u96be\u56f0\u5883\"\uff0c\u5e76\u63d0\u51fa\u7f13\u89e3\u65b9\u6848\u3002", "motivation": "\u7814\u7a76\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5b9e\u73b0\u53ef\u6269\u5c55\u96c6\u4f53\u667a\u80fd\u548c\u81ea\u6211\u8fdb\u5316\u65f6\u9762\u4e34\u7684\u5b89\u5168\u5bf9\u9f50\u6311\u6218\uff0c\u7279\u522b\u662f\u63a2\u7d22\u5b8c\u5168\u5c01\u95ed\u5faa\u73af\u4e2d\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u4e0e\u5b89\u5168\u4fdd\u6301\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u8bba\u6846\u67b6\u5c06\u5b89\u5168\u5f62\u5f0f\u5316\u4e3a\u4e0e\u4eba\u7c7b\u4ef7\u503c\u5206\u5e03\u7684\u504f\u79bb\u7a0b\u5ea6\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u8bc1\u7814\u7a76\uff08\u5305\u62ec\u5f00\u653e\u667a\u80fd\u4f53\u793e\u533aMoltbook\u548c\u4e24\u4e2a\u5c01\u95ed\u81ea\u8fdb\u5316\u7cfb\u7edf\uff09\u9a8c\u8bc1\u5b89\u5168\u9000\u5316\u73b0\u8c61\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5b8c\u5168\u9694\u79bb\u7684\u81ea\u6211\u8fdb\u5316\u4f1a\u5bfc\u81f4\u7edf\u8ba1\u76f2\u70b9\uff0c\u9020\u6210\u5b89\u5168\u5bf9\u9f50\u4e0d\u53ef\u9006\u9000\u5316\uff1b\u5b9e\u8bc1\u7ed3\u679c\u4e0e\u7406\u8bba\u9884\u6d4b\u4e00\u81f4\uff0c\u663e\u793a\u5b89\u5168\u4fb5\u8680\u4e0d\u53ef\u907f\u514d\u3002", "conclusion": "\u5b8c\u5168\u5c01\u95ed\u7684\u81ea\u8fdb\u5316AI\u793e\u4f1a\u5b58\u5728\u6839\u672c\u9650\u5236\uff0c\u9700\u8981\u4ece\u75c7\u72b6\u9a71\u52a8\u7684\u5b89\u5168\u8865\u4e01\u8f6c\u5411\u5bf9\u5185\u5728\u52a8\u6001\u98ce\u9669\u7684\u539f\u5219\u6027\u7406\u89e3\uff0c\u5f3a\u8c03\u5916\u90e8\u76d1\u7763\u6216\u65b0\u578b\u5b89\u5168\u4fdd\u6301\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.09924", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09924", "abs": "https://arxiv.org/abs/2602.09924", "authors": ["William Lugoloobi", "Thomas Foster", "William Bankes", "Chris Russell"], "title": "LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations", "comment": null, "summary": "Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5982\u4f55\u901a\u8fc7LLM\u5185\u90e8\u8868\u5f81\u9884\u6d4b\u5176\u81ea\u8eab\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u7387\uff0c\u4ece\u800c\u6307\u5bfc\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u8ba1\u7b97\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u6269\u5c55\u63a8\u7406\u5f00\u9500\u3002", "motivation": "\u5bf9\u6bcf\u4e2a\u95ee\u9898\u90fd\u8fd0\u884c\u6269\u5c55\u63a8\u7406\u7684LLM\u6210\u672c\u9ad8\u6602\uff0c\u4f46\u786e\u5b9a\u54ea\u4e9b\u8f93\u5165\u771f\u6b63\u9700\u8981\u989d\u5916\u8ba1\u7b97\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u4eceLLM\u751f\u6210\u524d\u7684\u5185\u90e8\u8868\u5f81\u4e2d\u6062\u590d\u5176\u81ea\u8eab\u6210\u529f\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u5229\u7528\u8fd9\u4e00\u4fe1\u53f7\u6307\u5bfc\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u3002", "method": "\u5728\u751f\u6210\u524d\u6fc0\u6d3b\u4e0a\u8bad\u7ec3\u7ebf\u6027\u63a2\u9488\u6765\u9884\u6d4b\u7279\u5b9a\u7b56\u7565\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u7387\uff1b\u4f7f\u7528E2H-AMC\u6570\u636e\u96c6\uff08\u5305\u542b\u4eba\u7c7b\u548c\u6a21\u578b\u5728\u76f8\u540c\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff09\u5206\u6790\u6a21\u578b\u7279\u5b9a\u96be\u5ea6\u4e0e\u4eba\u7c7b\u96be\u5ea6\u7684\u5dee\u5f02\uff1b\u5229\u7528\u63a2\u9488\u5728\u591a\u6a21\u578b\u6c60\u4e2d\u8fdb\u884c\u67e5\u8be2\u8def\u7531\u3002", "result": "\u7ebf\u6027\u63a2\u9488\u5728\u9884\u6d4b\u6a21\u578b\u6210\u529f\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u8868\u9762\u7279\u5f81\uff08\u5982\u95ee\u9898\u957f\u5ea6\u548cTF-IDF\uff09\uff1b\u6a21\u578b\u7f16\u7801\u4e86\u4e0e\u4eba\u7c7b\u96be\u5ea6\u4e0d\u540c\u7684\u6a21\u578b\u7279\u5b9a\u96be\u5ea6\u6982\u5ff5\uff0c\u4e14\u8fd9\u79cd\u5dee\u5f02\u968f\u6269\u5c55\u63a8\u7406\u800c\u589e\u52a0\uff1b\u901a\u8fc7\u63a2\u9488\u8def\u7531\u67e5\u8be2\uff0c\u5728MATH\u6570\u636e\u96c6\u4e0a\u53ef\u8d85\u8d8a\u6700\u4f73\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u9ad8\u8fbe70%\u7684\u63a8\u7406\u6210\u672c\u3002", "conclusion": "LLM\u7684\u5185\u90e8\u8868\u5f81\u80fd\u591f\u5b9e\u73b0\u5b9e\u9645\u7684\u6548\u7387\u63d0\u5347\uff0c\u5373\u4f7f\u8fd9\u4e9b\u8868\u5f81\u4e0e\u4eba\u7c7b\u5bf9\u96be\u5ea6\u7684\u76f4\u89c9\u5b58\u5728\u5dee\u5f02\u3002\u5185\u90e8\u8868\u5f81\u53ef\u7528\u4e8e\u6307\u5bfc\u9ad8\u6548\u63a8\u7406\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "topic": "agent analysis"}}
{"id": "2602.09953", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09953", "abs": "https://arxiv.org/abs/2602.09953", "authors": ["Shuaiyi Nie", "Siyu Ding", "Wenyuan Zhang", "Linhao Yu", "Tianmeng Yang", "Yao Chen", "Tingwen Liu", "Weichong Yin", "Yu Sun", "Hua Wu"], "title": "ATTNPO: Attention-Guided Process Supervision for Efficient Reasoning", "comment": "Work in process", "summary": "Large reasoning models trained with reinforcement learning and verifiable rewards (RLVR) achieve strong performance on complex reasoning tasks, yet often overthink, generating redundant reasoning without performance gains. Existing trajectory-level length penalties often fail to effectively shorten reasoning length and degrade accuracy, as they uniformly treat all reasoning steps and lack fine-grained signals to distinguish redundancy from necessity. Meanwhile, process-supervised methods are typically resource-intensive and suffer from inaccurate credit assignment. To address these issues, we propose ATTNPO, a low-overhead process-supervised RL framework that leverages the model's intrinsic attention signals for step-level credit assignment. We first identify a set of special attention heads that naturally focus on essential steps while suppressing redundant ones. By leveraging the attention scores of these heads, We then employ two sub-strategies to mitigate overthinking by discouraging redundant steps while preserving accuracy by reducing penalties on essential steps. Experimental results show that ATTNPO substantially reduces reasoning length while significantly improving performance across 9 benchmarks.", "AI": {"tldr": "ATTNPO\uff1a\u5229\u7528\u6a21\u578b\u5185\u5728\u6ce8\u610f\u529b\u4fe1\u53f7\u8fdb\u884c\u6b65\u7ea7\u4fe1\u7528\u5206\u914d\u7684\u8f7b\u91cf\u7ea7\u8fc7\u7a0b\u76d1\u7763RL\u6846\u67b6\uff0c\u6709\u6548\u51cf\u5c11\u63a8\u7406\u957f\u5ea6\u540c\u65f6\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709RLVR\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7ecf\u5e38\"\u8fc7\u5ea6\u601d\u8003\"\uff0c\u751f\u6210\u5197\u4f59\u63a8\u7406\u800c\u4e0d\u63d0\u5347\u6027\u80fd\u3002\u8f68\u8ff9\u7ea7\u957f\u5ea6\u60e9\u7f5a\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u7f29\u77ed\u63a8\u7406\u957f\u5ea6\u4e14\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u7edf\u4e00\u5bf9\u5f85\u6240\u6709\u63a8\u7406\u6b65\u9aa4\uff0c\u7f3a\u4e4f\u533a\u5206\u5197\u4f59\u4e0e\u5fc5\u8981\u6b65\u9aa4\u7684\u7ec6\u7c92\u5ea6\u4fe1\u53f7\u3002\u8fc7\u7a0b\u76d1\u7763\u65b9\u6cd5\u901a\u5e38\u8d44\u6e90\u5bc6\u96c6\u4e14\u4fe1\u7528\u5206\u914d\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51faATTNPO\u6846\u67b6\uff1a1\uff09\u8bc6\u522b\u4e00\u7ec4\u7279\u6b8a\u7684\u6ce8\u610f\u529b\u5934\uff0c\u8fd9\u4e9b\u5934\u81ea\u7136\u5730\u5173\u6ce8\u5fc5\u8981\u6b65\u9aa4\u540c\u65f6\u6291\u5236\u5197\u4f59\u6b65\u9aa4\uff1b2\uff09\u5229\u7528\u8fd9\u4e9b\u6ce8\u610f\u529b\u5934\u7684\u6ce8\u610f\u529b\u5206\u6570\uff0c\u91c7\u7528\u4e24\u79cd\u5b50\u7b56\u7565\uff1a\u901a\u8fc7\u60e9\u7f5a\u5197\u4f59\u6b65\u9aa4\u6765\u51cf\u5c11\u8fc7\u5ea6\u601d\u8003\uff0c\u540c\u65f6\u901a\u8fc7\u51cf\u5c11\u5bf9\u5fc5\u8981\u6b65\u9aa4\u7684\u60e9\u7f5a\u6765\u4fdd\u6301\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cATTNPO\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u957f\u5ea6\uff0c\u540c\u65f6\u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "ATTNPO\u901a\u8fc7\u5229\u7528\u6a21\u578b\u5185\u5728\u7684\u6ce8\u610f\u529b\u4fe1\u53f7\u8fdb\u884c\u6b65\u7ea7\u4fe1\u7528\u5206\u914d\uff0c\u6709\u6548\u89e3\u51b3\u4e86RLVR\u6a21\u578b\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u957f\u5ea6\u51cf\u5c11\u548c\u6027\u80fd\u63d0\u5347\u7684\u53cc\u91cd\u76ee\u6807\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.09782", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09782", "abs": "https://arxiv.org/abs/2602.09782", "authors": ["Kun Chen", "Peng Shi", "Fanfan Liu", "Haibo Qiu", "Zhixiong Zeng", "Siqi Yang", "Wenji Mao"], "title": "Flexible Entropy Control in RLVR with Gradient-Preserving Perspective", "comment": "https://github.com/Kwen-Chen/Flexible-Entropy-Control", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a critical method for enhancing the reasoning capabilities of Large Language Models (LLMs). However, continuous training often leads to policy entropy collapse, characterized by a rapid decay in entropy that results in premature overconfidence, reduced output diversity, and vanishing gradient norms that inhibit learning. Gradient-Preserving Clipping is a primary factor influencing these dynamics, but existing mitigation strategies are largely static and lack a framework connecting clipping mechanisms to precise entropy control. This paper proposes reshaping entropy control in RL from the perspective of Gradient-Preserving Clipping. We first theoretically and empirically verify the contributions of specific importance sampling ratio regions to entropy growth and reduction. Leveraging these findings, we introduce a novel regulation mechanism using dynamic clipping threshold to precisely manage entropy. Furthermore, we design and evaluate dynamic entropy control strategies, including increase-then-decrease, decrease-increase-decrease, and oscillatory decay. Experimental results demonstrate that these strategies effectively mitigate entropy collapse, and achieve superior performance across multiple benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u68af\u5ea6\u4fdd\u6301\u88c1\u526a\u7684\u52a8\u6001\u9608\u503c\u673a\u5236\u6765\u91cd\u5851\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u71b5\u63a7\u5236\uff0c\u6709\u6548\u7f13\u89e3\u7b56\u7565\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u662f\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u65b9\u6cd5\uff0c\u4f46\u6301\u7eed\u8bad\u7ec3\u5e38\u5bfc\u81f4\u7b56\u7565\u71b5\u5d29\u6e83\uff0c\u8868\u73b0\u4e3a\u71b5\u5feb\u901f\u8870\u51cf\u3001\u8fc7\u65e9\u8fc7\u5ea6\u81ea\u4fe1\u3001\u8f93\u51fa\u591a\u6837\u6027\u964d\u4f4e\u548c\u68af\u5ea6\u8303\u6570\u6d88\u5931\u3002\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u591a\u4e3a\u9759\u6001\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5c06\u88c1\u526a\u673a\u5236\u4e0e\u7cbe\u786e\u71b5\u63a7\u5236\u76f8\u8fde\u63a5\u7684\u6846\u67b6\u3002", "method": "\u4ece\u68af\u5ea6\u4fdd\u6301\u88c1\u526a\u7684\u89d2\u5ea6\u91cd\u5851\u71b5\u63a7\u5236\uff1a\u9996\u5148\u7406\u8bba\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u7279\u5b9a\u91cd\u8981\u6027\u91c7\u6837\u6bd4\u7387\u533a\u57df\u5bf9\u71b5\u589e\u957f\u548c\u51cf\u5c11\u7684\u8d21\u732e\uff1b\u5229\u7528\u8fd9\u4e9b\u53d1\u73b0\uff0c\u5f15\u5165\u4f7f\u7528\u52a8\u6001\u88c1\u526a\u9608\u503c\u7684\u65b0\u578b\u8c03\u8282\u673a\u5236\u6765\u7cbe\u786e\u7ba1\u7406\u71b5\uff1b\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u52a8\u6001\u71b5\u63a7\u5236\u7b56\u7565\uff0c\u5305\u62ec\u5148\u589e\u540e\u51cf\u3001\u51cf-\u589e-\u51cf\u548c\u632f\u8361\u8870\u51cf\u7b49\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u7b56\u7565\u6709\u6548\u7f13\u89e3\u4e86\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u68af\u5ea6\u4fdd\u6301\u88c1\u526a\u7684\u52a8\u6001\u9608\u503c\u673a\u5236\u80fd\u591f\u6709\u6548\u63a7\u5236\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u71b5\uff0c\u9632\u6b62\u8fc7\u65e9\u6536\u655b\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u8f93\u51fa\u591a\u6837\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.09761", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09761", "abs": "https://arxiv.org/abs/2602.09761", "authors": ["Matteo Pannacci", "Andrea Fanti", "Elena Umili", "Roberto Capobianco"], "title": "Grounding LTL Tasks in Sub-Symbolic RL Environments for Zero-Shot Generalization", "comment": "Preprint currently under review", "summary": "In this work we address the problem of training a Reinforcement Learning agent to follow multiple temporally-extended instructions expressed in Linear Temporal Logic in sub-symbolic environments. Previous multi-task work has mostly relied on knowledge of the mapping between raw observations and symbols appearing in the formulae. We drop this unrealistic assumption by jointly training a multi-task policy and a symbol grounder with the same experience. The symbol grounder is trained only from raw observations and sparse rewards via Neural Reward Machines in a semi-supervised fashion. Experiments on vision-based environments show that our method achieves performance comparable to using the true symbol grounding and significantly outperforms state-of-the-art methods for sub-symbolic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u5b50\u7b26\u53f7\u73af\u5883\u4e2d\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u9075\u5faa\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u6307\u4ee4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u591a\u4efb\u52a1\u7b56\u7565\u548c\u7b26\u53f7\u63a5\u5730\u5668\uff0c\u65e0\u9700\u5148\u9a8c\u7684\u89c2\u5bdf-\u7b26\u53f7\u6620\u5c04\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u77e5\u9053\u539f\u59cb\u89c2\u5bdf\u4e0e\u903b\u8f91\u516c\u5f0f\u4e2d\u7b26\u53f7\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u8fd9\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u662f\u4e0d\u73b0\u5b9e\u7684\u5047\u8bbe\u3002\u9700\u8981\u89e3\u51b3\u5728\u5b50\u7b26\u53f7\u73af\u5883\u4e2d\u4ec5\u4ece\u539f\u59cb\u89c2\u5bdf\u548c\u7a00\u758f\u5956\u52b1\u5b66\u4e60\u9075\u5faa\u65f6\u5e8f\u903b\u8f91\u6307\u4ee4\u7684\u95ee\u9898\u3002", "method": "\u8054\u5408\u8bad\u7ec3\u591a\u4efb\u52a1\u7b56\u7565\u548c\u7b26\u53f7\u63a5\u5730\u5668\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u7ecf\u9a8c\u3002\u7b26\u53f7\u63a5\u5730\u5668\u4ec5\u4ece\u539f\u59cb\u89c2\u5bdf\u548c\u7a00\u758f\u5956\u52b1\u901a\u8fc7\u795e\u7ecf\u5956\u52b1\u673a\u5668\u4ee5\u534a\u76d1\u7763\u65b9\u5f0f\u8bad\u7ec3\uff0c\u65e0\u9700\u5148\u9a8c\u7684\u7b26\u53f7\u6620\u5c04\u77e5\u8bc6\u3002", "result": "\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u4e0e\u4f7f\u7528\u771f\u5b9e\u7b26\u53f7\u63a5\u5730\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u5b50\u7b26\u53f7\u73af\u5883\u4e2d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5b50\u7b26\u53f7\u73af\u5883\u4e2d\u9075\u5faa\u65f6\u5e8f\u903b\u8f91\u6307\u4ee4\u7684\u95ee\u9898\uff0c\u65e0\u9700\u4e0d\u73b0\u5b9e\u7684\u7b26\u53f7\u6620\u5c04\u5047\u8bbe\uff0c\u5728\u89c6\u89c9\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10044", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.10044", "abs": "https://arxiv.org/abs/2602.10044", "authors": ["Akshay Mete", "Shahid Aamir Sheikh", "Tzu-Hsiang Lin", "Dileep Kalathil", "P. R. Kumar"], "title": "Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning", "comment": null, "summary": "Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes. This fully gradient-based loss requires neither uncertainty estimates nor constrained optimization. Our approach is plug-and-play with existing world model frameworks, preserving scalability while requiring only minimal modifications to standard training procedures. We instantiate OWMs within two state-of-the-art world model architectures, leading to Optimistic DreamerV3 and Optimistic STORM, which demonstrate significant improvements in sample efficiency and cumulative return compared to their baseline counterparts.", "AI": {"tldr": "\u63d0\u51faOptimistic World Models (OWMs)\uff0c\u4e00\u79cd\u5c06\u4e50\u89c2\u63a2\u7d22\u76f4\u63a5\u878d\u5165\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e50\u89c2\u52a8\u6001\u635f\u5931\u5c06\u60f3\u8c61\u8f6c\u79fb\u504f\u5411\u9ad8\u56de\u62a5\u7ed3\u679c\uff0c\u65e0\u9700\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6216\u7ea6\u675f\u4f18\u5316\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9ad8\u6548\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709UCB\u98ce\u683c\u7684\u63a2\u7d22\u65b9\u6cd5\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6216\u7ea6\u675f\u4f18\u5316\uff0c\u4f5c\u8005\u5e0c\u671b\u63d0\u51fa\u66f4\u7b80\u5355\u3001\u53ef\u6269\u5c55\u7684\u4e50\u89c2\u63a2\u7d22\u65b9\u6cd5\u3002", "method": "\u5c06\u7ecf\u5178\u7684\u81ea\u9002\u5e94\u63a7\u5236\u4e2d\u7684\u5956\u52b1\u504f\u7f6e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1(RBMLE)\u5f15\u5165\u6df1\u5ea6RL\uff0c\u901a\u8fc7\u6dfb\u52a0\u4e50\u89c2\u52a8\u6001\u635f\u5931\u6765\u504f\u7f6e\u60f3\u8c61\u8f6c\u79fb\u5411\u9ad8\u56de\u62a5\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u662f\u5b8c\u5168\u57fa\u4e8e\u68af\u5ea6\u7684\uff0c\u4e0d\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6216\u7ea6\u675f\u4f18\u5316\uff0c\u53ef\u4e0e\u73b0\u6709\u4e16\u754c\u6a21\u578b\u6846\u67b6\u5373\u63d2\u5373\u7528\u3002", "result": "\u5728\u4e24\u4e2a\u6700\u5148\u8fdb\u7684\u4e16\u754c\u6a21\u578b\u67b6\u6784(DreamerV3\u548cSTORM)\u4e2d\u5b9e\u4f8b\u5316OWMs\uff0c\u521b\u5efa\u4e86Optimistic DreamerV3\u548cOptimistic STORM\uff0c\u76f8\u6bd4\u57fa\u7ebf\u7248\u672c\u5728\u6837\u672c\u6548\u7387\u548c\u7d2f\u79ef\u56de\u62a5\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "OWMs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u53ef\u6269\u5c55\u7684\u4e50\u89c2\u63a2\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e50\u89c2\u76f4\u63a5\u878d\u5165\u6a21\u578b\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a2\u7d22\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u73b0\u6709\u4e16\u754c\u6a21\u578b\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u548c\u6613\u7528\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10048", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10048", "abs": "https://arxiv.org/abs/2602.10048", "authors": ["Xinchen Han", "Hossam Afifi", "Michel Marot", "Xilu Wang", "Lu Yin"], "title": "Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization", "comment": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2026", "summary": "Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \\textbf{F}ine-grained \\textbf{G}roup policy \\textbf{O}ptimization (\\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two major limitations of the GRPO: inefficient data utilization and entropy collapse. We evaluate FGO on multiple reasoning LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Experimental results show that FGO achieves efficient CoT compression without degrading performance, and simultaneously resolves the key limitations of GRPO.", "AI": {"tldr": "FGO\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5206\u7ec4\u7b56\u7565\u4f18\u5316\u5b9e\u73b0CoT\u63a8\u7406\u538b\u7f29\uff0c\u89e3\u51b3GRPO\u7684\u6570\u636e\u5229\u7528\u4f4e\u6548\u548c\u71b5\u5d29\u6e83\u95ee\u9898", "motivation": "LLM\u751f\u6210\u7684CoT\u63a8\u7406\u901a\u5e38\u8fc7\u4e8e\u5197\u957f\uff0c\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u4f46\u6027\u80fd\u63d0\u5347\u4e0d\u6210\u6bd4\u4f8b\uff0c\u9700\u8981\u6709\u6548\u7684\u538b\u7f29\u65b9\u6cd5", "method": "\u63d0\u51faFGO\uff08\u7ec6\u7c92\u5ea6\u5206\u7ec4\u7b56\u7565\u4f18\u5316\uff09\uff0c\u901a\u8fc7\u5c06\u7ec4\u54cd\u5e94\u7ec6\u5206\u4e3a\u66f4\u5c0f\u7684\u5355\u5143\uff0c\u5e76\u6839\u636e\u957f\u5ea6\u548c\u71b5\u5206\u914d\u9002\u5f53\u6743\u91cd\uff0c\u5b9e\u73b0\u6709\u6548\u7684CoT\u538b\u7f29\u3002\u4f5c\u4e3aGRPO\u7684\u589e\u5f3a\u53d8\u4f53\uff0c\u89e3\u51b3\u4e86GRPO\u7684\u4e24\u4e2a\u4e3b\u8981\u9650\u5236", "result": "\u5728MATH500\u3001AIME24\u3001AMC23\u548cMinerva\u7b49\u591a\u4e2a\u63a8\u7406LLM\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFGO\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684CoT\u538b\u7f29\u800c\u4e0d\u964d\u4f4e\u6027\u80fd\uff0c\u540c\u65f6\u89e3\u51b3\u4e86GRPO\u7684\u5173\u952e\u9650\u5236", "conclusion": "FGO\u80fd\u591f\u6709\u6548\u538b\u7f29LLM\u7684CoT\u63a8\u7406\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u89e3\u51b3\u73b0\u6709GRPO\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2602.10117", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10117", "abs": "https://arxiv.org/abs/2602.10117", "authors": ["Iv\u00e1n Arcuschin", "David Chanin", "Adri\u00e0 Garriga-Alonso", "Oana-Maria Camburu"], "title": "Biases in the Blind Spot: Detecting What LLMs Fail to Mention", "comment": "10 pages, Under review at ICML 2026", "summary": "Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets. In this work, we introduce a fully automated, black-box pipeline for detecting task-specific unverbalized biases. Given a task dataset, the pipeline uses LLM autoraters to generate candidate bias concepts. It then tests each concept on progressively larger input samples by generating positive and negative variations, and applies statistical techniques for multiple testing and early stopping. A concept is flagged as an unverbalized bias if it yields statistically significant performance differences while not being cited as justification in the model's CoTs. We evaluate our pipeline across six LLMs on three decision tasks (hiring, loan approval, and university admissions). Our technique automatically discovers previously unknown biases in these models (e.g., Spanish fluency, English proficiency, writing formality). In the same run, the pipeline also validates biases that were manually identified by prior work (gender, race, religion, ethnicity). More broadly, our proposed approach provides a practical, scalable path to automatic task-specific bias discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u7684\u9ed1\u76d2\u7ba1\u9053\uff0c\u7528\u4e8e\u68c0\u6d4bLLMs\u4e2d\u672a\u660e\u786e\u8868\u8ff0\u7684\u504f\u89c1\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u504f\u89c1\u6982\u5ff5\u3001\u7edf\u8ba1\u6d4b\u8bd5\u548c\u65e9\u671f\u505c\u6b62\u673a\u5236\uff0c\u5728\u62db\u8058\u3001\u8d37\u6b3e\u5ba1\u6279\u548c\u5927\u5b66\u5f55\u53d6\u7b49\u4efb\u52a1\u4e2d\u53d1\u73b0\u4e86\u65b0\u7684\u504f\u89c1\u3002", "motivation": "LLMs\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u53ef\u80fd\u9690\u85cf\u5185\u90e8\u504f\u89c1\uff08\u672a\u660e\u786e\u8868\u8ff0\u7684\u504f\u89c1\uff09\uff0c\u800c\u73b0\u6709\u504f\u89c1\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9884\u5b9a\u4e49\u7c7b\u522b\u548c\u624b\u5de5\u5236\u4f5c\u7684\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u504f\u89c1\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5168\u81ea\u52a8\u9ed1\u76d2\u7ba1\u9053\uff1a1\uff09\u4f7f\u7528LLM\u81ea\u52a8\u8bc4\u4f30\u5668\u751f\u6210\u5019\u9009\u504f\u89c1\u6982\u5ff5\uff1b2\uff09\u901a\u8fc7\u751f\u6210\u6b63\u8d1f\u53d8\u4f53\u5728\u9010\u6e10\u589e\u5927\u7684\u8f93\u5165\u6837\u672c\u4e0a\u6d4b\u8bd5\u6bcf\u4e2a\u6982\u5ff5\uff1b3\uff09\u5e94\u7528\u591a\u91cd\u6d4b\u8bd5\u548c\u65e9\u671f\u505c\u6b62\u7684\u7edf\u8ba1\u6280\u672f\uff1b4\uff09\u5982\u679c\u6982\u5ff5\u4ea7\u751f\u663e\u8457\u6027\u80fd\u5dee\u5f02\u4e14\u672a\u5728\u601d\u7ef4\u94fe\u4e2d\u88ab\u5f15\u7528\u4e3a\u7406\u7531\uff0c\u5219\u6807\u8bb0\u4e3a\u672a\u660e\u786e\u8868\u8ff0\u7684\u504f\u89c1\u3002", "result": "\u5728\u516d\u4e2aLLMs\u548c\u4e09\u4e2a\u51b3\u7b56\u4efb\u52a1\uff08\u62db\u8058\u3001\u8d37\u6b3e\u5ba1\u6279\u3001\u5927\u5b66\u5f55\u53d6\uff09\u4e0a\u8bc4\u4f30\uff0c\u81ea\u52a8\u53d1\u73b0\u4e86\u5148\u524d\u672a\u77e5\u7684\u504f\u89c1\uff08\u5982\u897f\u73ed\u7259\u8bed\u6d41\u5229\u5ea6\u3001\u82f1\u8bed\u719f\u7ec3\u5ea6\u3001\u5199\u4f5c\u6b63\u5f0f\u6027\uff09\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u624b\u52a8\u8bc6\u522b\u7684\u504f\u89c1\uff08\u6027\u522b\u3001\u79cd\u65cf\u3001\u5b97\u6559\u3001\u6c11\u65cf\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u7684\u4efb\u52a1\u7279\u5b9a\u504f\u89c1\u53d1\u73b0\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u80fd\u591f\u53ef\u9760\u5730\u68c0\u6d4bLLMs\u4e2d\u9690\u85cf\u7684\u672a\u660e\u786e\u8868\u8ff0\u7684\u504f\u89c1\u3002", "topic": "agent analysis"}}
{"id": "2602.10014", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.10014", "abs": "https://arxiv.org/abs/2602.10014", "authors": ["Chenruo Liu", "Yijun Dong", "Yiqiu Shen", "Qi Lei"], "title": "A Task-Centric Theory for Iterative Self-Improvement with Easy-to-Hard Curricula", "comment": null, "summary": "Iterative self-improvement fine-tunes an autoregressive large language model (LLM) on reward-verified outputs generated by the LLM itself. In contrast to the empirical success of self-improvement, the theoretical foundation of this generative, iterative procedure in a practical, finite-sample setting remains limited. We make progress toward this goal by modeling each round of self-improvement as maximum-likelihood fine-tuning on a reward-filtered distribution and deriving finite-sample guarantees for the expected reward. Our analysis reveals an explicit feedback loop where better models accept more data per iteration, supporting sustained self-improvement while explaining eventual saturation of such improvement. Adopting a task-centric view by considering reasoning tasks with multiple difficulty levels, we further prove quantifiable conditions on model initialization, task difficulty, and sample budget where easy-to-hard curricula provably achieve better guarantees than training on fixed mixtures of tasks. Our analyses are validated via Monte-Carlo simulations and controlled experiments on graph-based reasoning tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3a\u8fed\u4ee3\u81ea\u6539\u8fdb\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5728\u6709\u9650\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u901a\u8fc7\u5956\u52b1\u9a8c\u8bc1\u8f93\u51fa\u8fdb\u884c\u6700\u5927\u4f3c\u7136\u5fae\u8c03\u53ef\u4ee5\u5b9e\u73b0\u6301\u7eed\u7684\u81ea\u6539\u8fdb\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u6027\u80fd\u4e0e\u63a5\u53d7\u6570\u636e\u91cf\u4e4b\u95f4\u7684\u53cd\u9988\u5faa\u73af\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u8fed\u4ee3\u81ea\u6539\u8fdb\u5728\u5b9e\u8df5\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u6709\u9650\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u8fd9\u79cd\u751f\u6210\u5f0f\u8fed\u4ee3\u8fc7\u7a0b\u7684\u7406\u8bba\u57fa\u7840\u4ecd\u7136\u6709\u9650\u3002\u8bba\u6587\u65e8\u5728\u4e3a\u8fd9\u79cd\u81ea\u6539\u8fdb\u8fc7\u7a0b\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u7406\u89e3\u5176\u52a8\u6001\u673a\u5236\u548c\u6027\u80fd\u8fb9\u754c\u3002", "method": "\u5c06\u6bcf\u8f6e\u81ea\u6539\u8fdb\u5efa\u6a21\u4e3a\u5728\u5956\u52b1\u8fc7\u6ee4\u5206\u5e03\u4e0a\u7684\u6700\u5927\u4f3c\u7136\u5fae\u8c03\uff0c\u63a8\u5bfc\u6709\u9650\u6837\u672c\u4e0b\u7684\u671f\u671b\u5956\u52b1\u4fdd\u8bc1\u3002\u91c7\u7528\u4efb\u52a1\u4e2d\u5fc3\u89c6\u89d2\uff0c\u8003\u8651\u5177\u6709\u591a\u4e2a\u96be\u5ea6\u7ea7\u522b\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u5206\u6790\u6a21\u578b\u521d\u59cb\u5316\u3001\u4efb\u52a1\u96be\u5ea6\u548c\u6837\u672c\u9884\u7b97\u7b49\u6761\u4ef6\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u6027\u80fd\u4e0e\u63a5\u53d7\u6570\u636e\u91cf\u4e4b\u95f4\u7684\u663e\u5f0f\u53cd\u9988\u5faa\u73af\uff1a\u66f4\u597d\u7684\u6a21\u578b\u6bcf\u8f6e\u63a5\u53d7\u66f4\u591a\u6570\u636e\uff0c\u652f\u6301\u6301\u7eed\u81ea\u6539\u8fdb\uff0c\u540c\u65f6\u89e3\u91ca\u4e86\u6539\u8fdb\u6700\u7ec8\u9971\u548c\u7684\u73b0\u8c61\u3002\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u4ece\u6613\u5230\u96be\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u6bd4\u56fa\u5b9a\u4efb\u52a1\u6df7\u5408\u8bad\u7ec3\u5177\u6709\u66f4\u597d\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8fed\u4ee3\u81ea\u6539\u8fdb\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5176\u52a8\u6001\u673a\u5236\uff0c\u8bc1\u660e\u4e86\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u7684\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u56fe\u63a8\u7406\u4efb\u52a1\u7684\u53d7\u63a7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u3002", "topic": "agent analysis"}}
{"id": "2602.10067", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10067", "abs": "https://arxiv.org/abs/2602.10067", "authors": ["Aaditya Vikram Prasad", "Connor Watts", "Jack Merullo", "Dhruvil Gala", "Owen Lewis", "Thomas McGrath", "Ekdeep Singh Lubana"], "title": "Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability", "comment": null, "summary": "Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model, while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.", "AI": {"tldr": "\u63d0\u51faRLFR\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7279\u5f81\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\uff0c\u51cf\u5c11\u6a21\u578b\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd", "motivation": "\u4f20\u7edf\u4e0a\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u7684\u7279\u5f81\u4ec5\u7528\u4e8e\u6d4b\u8bd5\u65f6\u76d1\u63a7\u6216\u5f15\u5bfc\uff0c\u672c\u6587\u63a2\u7d22\u5c06\u8fd9\u4e9b\u7279\u5f81\u4f5c\u4e3a\u5f00\u653e\u5f0f\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u76d1\u7763\u4fe1\u53f7", "method": "\u8bbe\u8ba1RLFR\u5f3a\u5316\u5b66\u4e60\u6d41\u7a0b\uff1a1) \u57fa\u4e8e\u63a2\u6d4b\u6846\u67b6\u8bc6\u522b\u5019\u9009\u5e7b\u89c9\u58f0\u660e\uff1b2) \u4f7f\u7528\u7279\u5f81\u4f5c\u4e3a\u5956\u52b1\u51fd\u6570\uff1b3) \u8bad\u7ec3\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u4e8b\u5b9e\u6027\u65f6\u5e72\u9884\u548c\u4fee\u6b63\u8865\u5168\uff1b4) \u652f\u6301\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97", "result": "\u5728Gemma-3-12B-IT\u4e0a\u5b9e\u65bd\uff0c\u76f8\u6bd4\u539f\u59cb\u6a21\u578b\uff0c\u5e7b\u89c9\u6982\u7387\u964d\u4f4e58%\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd", "conclusion": "\u901a\u8fc7\u5c06\u76d1\u7763\u57fa\u4e8e\u7279\u5f81\u8bed\u8a00\uff0c\u4e3a\u5b66\u4e60\u5f00\u653e\u5f0f\u4efb\u52a1\u5f15\u5165\u4e86\u53ef\u89e3\u91ca\u6027\u5e94\u7528\u7684\u65b0\u8303\u5f0f", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2602.96fe30c7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2020207322124132504.html%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/pGDX-WwqsXRFD_jVQWi494f7B_MCUtOOTelJWo4bwGQ=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2020207322124132504.html%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/pGDX-WwqsXRFD_jVQWi494f7B_MCUtOOTelJWo4bwGQ=443", "authors": ["TLDR Newsletter"], "title": "Our teams have been building with a 2.5x-faster version of Claude Opus 4.6", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2020207322124132504.html%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/pGDX-WwqsXRFD_jVQWi494f7B_MCUtOOTelJWo4bwGQ=443", "summary": "Our teams have been building with a 2.5x-faster version of Claude Opus 4.6 (1 minute read) Anthropic is making a faster version of Claude Opus 4.6 available as an early experiment via Claude Code and its API. Fast mode is more expensive to run, but it is 2.5x faster. It is designed for urgent, high-stakes projects. A link to the waitlist for the feature is available in the thread.", "source": "tldr", "AI": {"tldr": "Anthropic\u63a8\u51faClaude Opus 4.6\u76842.5\u500d\u901f\u7248\u672c\uff0c\u901a\u8fc7Claude Code\u548cAPI\u63d0\u4f9b\uff0c\u9002\u7528\u4e8e\u7d27\u6025\u9ad8\u98ce\u9669\u9879\u76ee\uff0c\u4f46\u8fd0\u884c\u6210\u672c\u66f4\u9ad8", "motivation": "\u4e3a\u6ee1\u8db3\u7d27\u6025\u3001\u9ad8\u98ce\u9669\u9879\u76ee\u7684\u9700\u6c42\uff0c\u63d0\u4f9b\u66f4\u5feb\u7684AI\u6a21\u578b\u54cd\u5e94\u901f\u5ea6\uff0c\u4ee5\u652f\u6301\u65f6\u95f4\u654f\u611f\u7684\u5f00\u53d1\u4efb\u52a1", "method": "\u5f00\u53d1Claude Opus 4.6\u7684\"\u5feb\u901f\u6a21\u5f0f\"\u7248\u672c\uff0c\u901a\u8fc7Claude Code\u5e73\u53f0\u548cAPI\u63a5\u53e3\u63d0\u4f9b\uff0c\u91c7\u7528\u7b49\u5f85\u540d\u5355\u673a\u5236\u8fdb\u884c\u65e9\u671f\u5b9e\u9a8c", "result": "\u6210\u529f\u5b9e\u73b02.5\u500d\u901f\u5ea6\u63d0\u5347\u7684Claude Opus 4.6\u7248\u672c\uff0c\u5df2\u901a\u8fc7\u7b49\u5f85\u540d\u5355\u5411\u7528\u6237\u5f00\u653e", "conclusion": "\u5feb\u901f\u7248Claude Opus 4.6\u4e3a\u7d27\u6025\u9879\u76ee\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u901f\u5ea6\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u6743\u8861\u66f4\u9ad8\u7684\u8fd0\u884c\u6210\u672c", "topic": "code agent"}}
{"id": "tldr.2602.de767124", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fmeta-ai-redies-avacado-manus-agent-and-openclaw-integration%2F%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/IA3S-zdd77OxKr6wHxi3RnJie4KdmhqE8UBEm7YzOo4=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fmeta-ai-redies-avacado-manus-agent-and-openclaw-integration%2F%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/IA3S-zdd77OxKr6wHxi3RnJie4KdmhqE8UBEm7YzOo4=443", "authors": ["TLDR Newsletter"], "title": "Meta AI readies Avocado, Manus Agent, and OpenClaw integration", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fmeta-ai-redies-avacado-manus-agent-and-openclaw-integration%2F%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/IA3S-zdd77OxKr6wHxi3RnJie4KdmhqE8UBEm7YzOo4=443", "summary": "Meta AI readies Avocado, Manus Agent, and OpenClaw integration (5 minute read) Meta AI is reportedly preparing to release new models named Avocado. It is also adding MCP support and a Memory section to its settings menu. Meta AI has revamped its website to include a lot of additional functionality. The company appears to be working on an AI agent and a browser agent, and a new feature called Tasks that will allow users to schedule recurring executions of Meta AI.", "source": "tldr", "AI": {"tldr": "Meta AI\u6b63\u5728\u51c6\u5907\u53d1\u5e03\u540d\u4e3aAvocado\u7684\u65b0\u6a21\u578b\uff0c\u589e\u52a0MCP\u652f\u6301\u548c\u5185\u5b58\u8bbe\u7f6e\uff0c\u5e76\u5f00\u53d1AI\u4ee3\u7406\u3001\u6d4f\u89c8\u5668\u4ee3\u7406\u53ca\u4efb\u52a1\u8c03\u5ea6\u529f\u80fd", "motivation": "Meta AI\u65e8\u5728\u6269\u5c55\u5176AI\u80fd\u529b\uff0c\u901a\u8fc7\u53d1\u5e03\u65b0\u6a21\u578b\u548c\u96c6\u6210\u529f\u80fd\u6765\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u5f00\u53d1\u4ee3\u7406\u7cfb\u7edf\u4ee5\u5b9e\u73b0\u66f4\u667a\u80fd\u7684\u81ea\u52a8\u5316\u4efb\u52a1\u6267\u884c", "method": "\u901a\u8fc7\u5f00\u53d1Avocado\u6a21\u578b\u3001\u96c6\u6210MCP\u652f\u6301\u3001\u589e\u52a0\u5185\u5b58\u8bbe\u7f6e\u83dc\u5355\u3001\u91cd\u65b0\u8bbe\u8ba1\u7f51\u7ad9\u529f\u80fd\uff0c\u5e76\u6784\u5efaAI\u4ee3\u7406\u548c\u6d4f\u89c8\u5668\u4ee3\u7406\u7cfb\u7edf", "result": "Meta AI\u6b63\u5728\u51c6\u5907\u53d1\u5e03\u65b0\u6a21\u578b\u548c\u529f\u80fd\uff0c\u5305\u62ecAvocado\u6a21\u578b\u3001MCP\u652f\u6301\u3001\u5185\u5b58\u8bbe\u7f6e\u3001\u7f51\u7ad9\u529f\u80fd\u589e\u5f3a\uff0c\u4ee5\u53caAI\u4ee3\u7406\u548c\u4efb\u52a1\u8c03\u5ea6\u7cfb\u7edf", "conclusion": "Meta AI\u6b63\u5728\u79ef\u6781\u6269\u5c55\u5176AI\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u65b0\u6a21\u578b\u3001\u4ee3\u7406\u7cfb\u7edf\u548c\u4efb\u52a1\u8c03\u5ea6\u529f\u80fd\u6765\u589e\u5f3a\u5176AI\u4ea7\u54c1\u7684\u80fd\u529b\u548c\u7528\u6237\u4f53\u9a8c", "topic": "code agent"}}
{"id": "tldr.2602.adfd60b2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fweaviate.io%2Fblog%2Flimit-in-the-loop%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/FONU_Gll43NG_AHO93Ot8IlhCYfmjb10i32f35jD1s0=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fweaviate.io%2Fblog%2Flimit-in-the-loop%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/FONU_Gll43NG_AHO93Ot8IlhCYfmjb10i32f35jD1s0=443", "authors": ["TLDR Newsletter"], "title": "The Limit in the Loop: Memory as a System Problem", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fweaviate.io%2Fblog%2Flimit-in-the-loop%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/FONU_Gll43NG_AHO93Ot8IlhCYfmjb10i32f35jD1s0=443", "summary": "The Limit in the Loop: Memory as a System Problem (8 minute read) Weaviate discusses the limitations of current LLM applications rooted in session-based design, arguing that solving continuity\u2014carrying context across interactions\u2014requires systemic rather than model-level changes.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524dLLM\u5e94\u7528\u7684\u5c40\u9650\u6027\u6e90\u4e8e\u4f1a\u8bdd\u5f0f\u8bbe\u8ba1\uff0c\u89e3\u51b3\u8de8\u4ea4\u4e92\u7684\u4e0a\u4e0b\u6587\u8fde\u7eed\u6027\u9700\u8981\u7cfb\u7edf\u7ea7\u800c\u975e\u6a21\u578b\u7ea7\u53d8\u9769", "motivation": "\u5f53\u524dLLM\u5e94\u7528\u91c7\u7528\u4f1a\u8bdd\u5f0f\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u65e0\u6cd5\u5728\u4e0d\u540c\u4ea4\u4e92\u95f4\u6301\u7eed\uff0c\u9650\u5236\u4e86\u5e94\u7528\u7684\u8fde\u7eed\u6027\u548c\u667a\u80fd\u8868\u73b0", "method": "\u63d0\u51fa\u7cfb\u7edf\u7ea7\u89e3\u51b3\u65b9\u6848\u800c\u975e\u6a21\u578b\u7ea7\u6539\u8fdb\uff0c\u5f3a\u8c03\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u5e94\u7528\u67b6\u6784\u6765\u89e3\u51b3\u4e0a\u4e0b\u6587\u8fde\u7eed\u6027\u8fd9\u4e00\u6839\u672c\u95ee\u9898", "result": "\u8bc6\u522b\u51fa\u4f1a\u8bdd\u5f0f\u8bbe\u8ba1\u7684\u6838\u5fc3\u5c40\u9650\u6027\uff0c\u6307\u51fa\u5355\u7eaf\u6539\u8fdb\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u8de8\u4ea4\u4e92\u7684\u4e0a\u4e0b\u6587\u4fdd\u6301\u95ee\u9898", "conclusion": "LLM\u5e94\u7528\u7684\u771f\u6b63\u7a81\u7834\u9700\u8981\u4ece\u7cfb\u7edf\u67b6\u6784\u5c42\u9762\u89e3\u51b3\u4e0a\u4e0b\u6587\u8fde\u7eed\u6027\uff0c\u800c\u975e\u4ec5\u4ec5\u4f9d\u8d56\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347", "topic": "agent analysis"}}
{"id": "tldr.2602.ce311841", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fpydantic%2Fmonty%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/xQQotbZaHQ47YSoufczV92gj2EtMQBfdkt7jRKq-rPk=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fpydantic%2Fmonty%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/xQQotbZaHQ47YSoufczV92gj2EtMQBfdkt7jRKq-rPk=443", "authors": ["TLDR Newsletter"], "title": "Monty", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fpydantic%2Fmonty%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/xQQotbZaHQ47YSoufczV92gj2EtMQBfdkt7jRKq-rPk=443", "summary": "Monty (GitHub Repo) Monty is a minimal, secure Python interpreter written in Rust for use by AI. It lets users safely run Python code written by agents. Monty completely blocks access to the host environment, and it can only call functions it has access to. It makes it possible to safely run LLM-generated code without the complexity of a sandbox or the risk of running code directly on the host.", "source": "tldr", "AI": {"tldr": "Monty\u662f\u4e00\u4e2a\u7528Rust\u7f16\u5199\u7684\u5b89\u5168Python\u89e3\u91ca\u5668\uff0c\u7528\u4e8eAI\u4ee3\u7406\u5b89\u5168\u6267\u884cPython\u4ee3\u7801\uff0c\u5b8c\u5168\u9694\u79bb\u4e3b\u673a\u73af\u5883\uff0c\u65e0\u9700\u590d\u6742\u6c99\u7bb1", "motivation": "AI\u4ee3\u7406\u751f\u6210\u7684\u4ee3\u7801\u53ef\u80fd\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u76f4\u63a5\u5728\u4e3b\u673a\u73af\u5883\u8fd0\u884c\u4e0d\u5b89\u5168\uff0c\u800c\u4f20\u7edf\u6c99\u7bb1\u65b9\u6848\u53c8\u8fc7\u4e8e\u590d\u6742\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5b89\u5168\u6267\u884cAI\u751f\u6210\u4ee3\u7801\uff0c\u53c8\u7b80\u5355\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Rust\u8bed\u8a00\u91cd\u65b0\u5b9e\u73b0\u4e00\u4e2a\u6700\u5c0f\u5316\u7684Python\u89e3\u91ca\u5668\uff0c\u5b8c\u5168\u963b\u65ad\u5bf9\u4e3b\u673a\u73af\u5883\u7684\u8bbf\u95ee\uff0c\u53ea\u80fd\u8c03\u7528\u9884\u5148\u6388\u6743\u7684\u51fd\u6570\uff0c\u5b9e\u73b0\u4ee3\u7801\u6267\u884c\u7684\u5b89\u5168\u9694\u79bb\u3002", "result": "\u5f00\u53d1\u51faMonty\u5b89\u5168Python\u89e3\u91ca\u5668\uff0c\u80fd\u591f\u5b89\u5168\u8fd0\u884cAI\u751f\u6210\u7684Python\u4ee3\u7801\uff0c\u65e0\u9700\u590d\u6742\u6c99\u7bb1\u914d\u7f6e\uff0c\u6d88\u9664\u4e86\u76f4\u63a5\u8fd0\u884c\u4ee3\u7801\u7684\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "Monty\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b89\u5168\u3001\u7b80\u5355\u7684\u4ee3\u7801\u6267\u884c\u73af\u5883\uff0c\u89e3\u51b3\u4e86AI\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6267\u884c\u95ee\u9898\uff0c\u5e73\u8861\u4e86\u5b89\u5168\u6027\u548c\u6613\u7528\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2602.3bccfad5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkaitchup.substack.com%2Fp%2Flora-but-with-only-13-parameters%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/Q4DniN2hMNCcqVaPimoFxNzhoLYd2r3T90VOUUJObJo=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkaitchup.substack.com%2Fp%2Flora-but-with-only-13-parameters%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/Q4DniN2hMNCcqVaPimoFxNzhoLYd2r3T90VOUUJObJo=443", "authors": ["TLDR Newsletter"], "title": "LoRA but with Only 13 Parameters??", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkaitchup.substack.com%2Fp%2Flora-but-with-only-13-parameters%3Futm_source=tldrai/1/0100019c42c6d5d1-c087094f-e541-40cf-a1e7-11f2ec1b8cbe-000000/Q4DniN2hMNCcqVaPimoFxNzhoLYd2r3T90VOUUJObJo=443", "summary": "LoRA but with Only 13 Parameters?? (6 minute read) Meta researchers say they can boost an LLM's math reasoning by updating just 13 parameters.", "source": "tldr", "AI": {"tldr": "Meta\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ea\u9700\u66f4\u65b013\u4e2a\u53c2\u6570\u5c31\u80fd\u663e\u8457\u63d0\u5347LLM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u66f4\u65b0\u5927\u91cf\u53c2\u6570\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u7814\u7a76\u8005\u5e0c\u671b\u627e\u5230\u4e00\u79cd\u6781\u7b80\u7684\u53c2\u6570\u66f4\u65b0\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u91c7\u7528\u7c7b\u4f3cLoRA\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f46\u5c06\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u538b\u7f29\u5230\u6781\u81f4\u768413\u4e2a\uff0c\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9\u7684\u53c2\u6570\u4f4d\u7f6e\u548c\u66f4\u65b0\u7b56\u7565\u6765\u5f71\u54cd\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "result": "\u4ec5\u66f4\u65b013\u4e2a\u53c2\u6570\u5c31\u80fd\u663e\u8457\u63d0\u5347LLM\u7684\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u83b7\u5f97\u4e86\u4e0e\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6548\u679c\u3002", "conclusion": "\u6781\u5c11\u6570\u53c2\u6570\u7684\u66f4\u65b0\u4e5f\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u7279\u5b9a\u80fd\u529b\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.15f665ae", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2026%2F02%2F06%2Fmaybe-ai-agents-can-be-lawyers-after-all%2F%3Futm_source=tldrfintech/1/0100019c42cf5879-2d960b0a-1812-482e-ad83-d534ce67a338-000000/bkC143xiZe0fltn5rfDqV7747C5n27LwAA2VCO2y2Ao=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2026%2F02%2F06%2Fmaybe-ai-agents-can-be-lawyers-after-all%2F%3Futm_source=tldrfintech/1/0100019c42cf5879-2d960b0a-1812-482e-ad83-d534ce67a338-000000/bkC143xiZe0fltn5rfDqV7747C5n27LwAA2VCO2y2Ao=443", "authors": ["TLDR Newsletter"], "title": "AI agents are closing the gap on legal work", "comment": "Source: TLDR Newsletter, Date: 2026-02-09, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2026%2F02%2F06%2Fmaybe-ai-agents-can-be-lawyers-after-all%2F%3Futm_source=tldrfintech/1/0100019c42cf5879-2d960b0a-1812-482e-ad83-d534ce67a338-000000/bkC143xiZe0fltn5rfDqV7747C5n27LwAA2VCO2y2Ao=443", "summary": "AI agents are closing the gap on legal work (3 minute read) New benchmarks show Anthropic's Opus 4.6 sharply improving agent performance on legal and corporate tasks, signaling faster-than-expected progress that doesn't replace lawyers yet, but meaningfully challenges the idea that professional legal work is safely out of reach for AI.", "source": "tldr", "AI": {"tldr": "Anthropic\u7684Opus 4.6\u5728\u5f8b\u5e08\u548c\u4f01\u4e1a\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0cAI\u4ee3\u7406\u6b63\u5728\u7f29\u5c0f\u4e0e\u6cd5\u5f8b\u5de5\u4f5c\u7684\u5dee\u8ddd\uff0c\u4f46\u5c1a\u672a\u5b8c\u5168\u53d6\u4ee3\u5f8b\u5e08", "motivation": "\u8bc4\u4f30AI\u5728\u6cd5\u5f8b\u548c\u4e13\u4e1a\u5de5\u4f5c\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u4e86\u89e3AI\u4ee3\u7406\u662f\u5426\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u6cd5\u5f8b\u548c\u4f01\u4e1a\u4efb\u52a1", "method": "\u901a\u8fc7\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30Anthropic Opus 4.6\u6a21\u578b\u5728\u6cd5\u5f8b\u548c\u4f01\u4e1a\u4efb\u52a1\u4e0a\u7684\u4ee3\u7406\u6027\u80fd", "result": "Opus 4.6\u5728\u6cd5\u5f8b\u548c\u4f01\u4e1a\u4efb\u52a1\u4e0a\u7684\u4ee3\u7406\u6027\u80fd\u663e\u8457\u6539\u5584\uff0c\u8fdb\u5c55\u901f\u5ea6\u8d85\u51fa\u9884\u671f\uff0c\u8868\u660eAI\u6b63\u5728\u5feb\u901f\u63a5\u8fd1\u4e13\u4e1a\u6cd5\u5f8b\u5de5\u4f5c\u7684\u80fd\u529b\u6c34\u5e73", "conclusion": "AI\u4ee3\u7406\u6b63\u5728\u5feb\u901f\u7f29\u5c0f\u4e0e\u6cd5\u5f8b\u5de5\u4f5c\u7684\u5dee\u8ddd\uff0c\u867d\u7136\u5c1a\u672a\u5b8c\u5168\u53d6\u4ee3\u5f8b\u5e08\uff0c\u4f46\u5df2\u7ecf\u5bf9\"\u4e13\u4e1a\u6cd5\u5f8b\u5de5\u4f5cAI\u65e0\u6cd5\u89e6\u53ca\"\u7684\u89c2\u5ff5\u6784\u6210\u5b9e\u8d28\u6027\u6311\u6218", "topic": "agent analysis"}}
{"id": "tldr.2602.8e4d4e08", "categories": ["tldr.article"], "pdf": "", "abs": "", "authors": ["TLDR Newsletter"], "title": "poof", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Reading time: 4 minute read", "summary": "AI coding agents are collapsing feature backlogs and redefining software velocity. The winners will be teams that pair agentic engineering with strong human judgment about what to build next.", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7801\u4ee3\u7406\u6b63\u5728\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\uff0c\u6210\u529f\u56e2\u961f\u9700\u8981\u7ed3\u5408\u4ee3\u7406\u5de5\u7a0b\u4e0e\u4eba\u7c7b\u5224\u65ad\u6765\u51b3\u5b9a\u6784\u5efa\u65b9\u5411", "motivation": "AI\u7f16\u7801\u4ee3\u7406\u6b63\u5728\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u901f\u5ea6\uff0c\u4f46\u9700\u8981\u4eba\u7c7b\u5224\u65ad\u6765\u6307\u5bfc\u6784\u5efa\u65b9\u5411\uff0c\u4ee5\u6700\u5927\u5316\u4ef7\u503c", "method": "\u7ed3\u5408\u4ee3\u7406\u5de5\u7a0b\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u56e2\u961f\u534f\u4f5c\u65b9\u6cd5", "result": "AI\u7f16\u7801\u4ee3\u7406\u80fd\u591f\u663e\u8457\u51cf\u5c11\u529f\u80fd\u79ef\u538b\uff0c\u91cd\u65b0\u5b9a\u4e49\u8f6f\u4ef6\u5f00\u53d1\u901f\u5ea6", "conclusion": "\u6210\u529f\u7684\u8f6f\u4ef6\u5f00\u53d1\u56e2\u961f\u9700\u8981\u5c06AI\u4ee3\u7406\u5de5\u7a0b\u4e0e\u4eba\u7c7b\u6218\u7565\u5224\u65ad\u76f8\u7ed3\u5408", "topic": "code agent"}}
{"id": "tldr.2602.6c90c712", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F06YGMn/1/0100019c473bf51b-2f182b0d-8b11-41c3-bad1-e18f8b618da1-000000/cfkrpA5M29o0DEb3ei8tqwD4mftaWFBoq6-mDF74s8Y=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F06YGMn/1/0100019c473bf51b-2f182b0d-8b11-41c3-bad1-e18f8b618da1-000000/cfkrpA5M29o0DEb3ei8tqwD4mftaWFBoq6-mDF74s8Y=443", "authors": ["TLDR Newsletter"], "title": "Agents vs. Workflows: The Framework Founders Actually Need", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F06YGMn/1/0100019c473bf51b-2f182b0d-8b11-41c3-bad1-e18f8b618da1-000000/cfkrpA5M29o0DEb3ei8tqwD4mftaWFBoq6-mDF74s8Y=443", "summary": "Agents vs. Workflows: The Framework Founders Actually Need (8 minute read) The Replit failure highlights why autonomous AI agents are risky in production. For most teams, workflows with constrained LLM components are safer, faster to ship, and easier to trust.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u81ea\u4e3bAI\u4ee3\u7406\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5b58\u5728\u98ce\u9669\uff0c\u5bf9\u4e8e\u5927\u591a\u6570\u56e2\u961f\u6765\u8bf4\uff0c\u4f7f\u7528\u53d7\u7ea6\u675f\u7684LLM\u7ec4\u4ef6\u7684\u5de5\u4f5c\u6d41\u7a0b\u66f4\u5b89\u5168\u3001\u90e8\u7f72\u66f4\u5feb\u3001\u66f4\u503c\u5f97\u4fe1\u8d56", "motivation": "Replit\u7684\u5931\u8d25\u6848\u4f8b\u7a81\u663e\u4e86\u81ea\u4e3bAI\u4ee3\u7406\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u98ce\u9669\uff0c\u9700\u8981\u4e3a\u56e2\u961f\u63d0\u4f9b\u66f4\u5b89\u5168\u53ef\u9760\u7684AI\u5e94\u7528\u6846\u67b6", "method": "\u901a\u8fc7\u5206\u6790Replit\u5931\u8d25\u6848\u4f8b\uff0c\u5bf9\u6bd4\u81ea\u4e3bAI\u4ee3\u7406\u4e0e\u53d7\u7ea6\u675fLLM\u7ec4\u4ef6\u5de5\u4f5c\u6d41\u7a0b\u7684\u5dee\u5f02\uff0c\u63d0\u51fa\u66f4\u9002\u5408\u751f\u4ea7\u73af\u5883\u7684\u6846\u67b6\u65b9\u6848", "result": "\u53d1\u73b0\u5de5\u4f5c\u6d41\u7a0b\u65b9\u6cd5\u6bd4\u81ea\u4e3b\u4ee3\u7406\u66f4\u5b89\u5168\u3001\u90e8\u7f72\u66f4\u5feb\u3001\u66f4\u503c\u5f97\u4fe1\u8d56\uff0c\u66f4\u9002\u5408\u5927\u591a\u6570\u56e2\u961f\u7684\u5b9e\u9645\u9700\u6c42", "conclusion": "\u6846\u67b6\u5f00\u53d1\u8005\u5e94\u8be5\u4f18\u5148\u8003\u8651\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7a0b\u7684\u53d7\u7ea6\u675fLLM\u7ec4\u4ef6\uff0c\u800c\u4e0d\u662f\u98ce\u9669\u8f83\u9ad8\u7684\u81ea\u4e3bAI\u4ee3\u7406", "topic": "agent analysis"}}
{"id": "tldr.2602.9fabb802", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.useparagon.com%2Flearn%2Fproduction-ready-rag-and-ai-agents-tutorial-series-landing%3Futm_campaign_name=TLDRProductManagement%26utm_source=tldr_pm%26utm_medium=newsletter/1/0100019c473bf51b-2f182b0d-8b11-41c3-bad1-e18f8b618da1-000000/9kFe9lOH-xA1dCXU7Ol-W2EGnAFDbaurd5nuZl2iKJg=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.useparagon.com%2Flearn%2Fproduction-ready-rag-and-ai-agents-tutorial-series-landing%3Futm_campaign_name=TLDRProductManagement%26utm_source=tldr_pm%26utm_medium=newsletter/1/0100019c473bf51b-2f182b0d-8b11-41c3-bad1-e18f8b618da1-000000/9kFe9lOH-xA1dCXU7Ol-W2EGnAFDbaurd5nuZl2iKJg=443", "authors": ["TLDR Newsletter"], "title": "Enable your agentic AI product with third-party tools", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.useparagon.com%2Flearn%2Fproduction-ready-rag-and-ai-agents-tutorial-series-landing%3Futm_campaign_name=TLDRProductManagement%26utm_source=tldr_pm%26utm_medium=newsletter/1/0100019c473bf51b-2f182b0d-8b11-41c3-bad1-e18f8b618da1-000000/9kFe9lOH-xA1dCXU7Ol-W2EGnAFDbaurd5nuZl2iKJg=443", "summary": "Enable your agentic AI product with third-party tools (Sponsor) This tutorial covers everything you need to build an AI agent SaaS feature that takes action in users' external apps.", "source": "tldr", "AI": {"tldr": "\u5173\u4e8e\u5982\u4f55\u4e3aAI\u4ee3\u7406\u4ea7\u54c1\u96c6\u6210\u7b2c\u4e09\u65b9\u5de5\u5177\u4ee5\u5728\u7528\u6237\u5916\u90e8\u5e94\u7528\u4e2d\u6267\u884c\u64cd\u4f5c\u7684\u6559\u7a0b", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u6784\u5efa\u80fd\u591f\u4e0e\u7b2c\u4e09\u65b9\u5e94\u7528\u4ea4\u4e92\u7684AI\u4ee3\u7406SaaS\u529f\u80fd\uff0c\u6269\u5c55AI\u4ee3\u7406\u7684\u5b9e\u9645\u5e94\u7528\u80fd\u529b", "method": "\u63d0\u4f9b\u6559\u7a0b\u6307\u5bfc\uff0c\u6db5\u76d6\u6784\u5efaAI\u4ee3\u7406SaaS\u529f\u80fd\u6240\u9700\u7684\u6240\u6709\u5185\u5bb9\uff0c\u5305\u62ec\u7b2c\u4e09\u65b9\u5de5\u5177\u96c6\u6210", "result": "\u6559\u7a0b\u5185\u5bb9\uff0c\u6307\u5bfc\u5f00\u53d1\u8005\u5982\u4f55\u5b9e\u73b0AI\u4ee3\u7406\u5728\u5916\u90e8\u5e94\u7528\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b", "conclusion": "\u901a\u8fc7\u672c\u6559\u7a0b\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u4e3a\u5176AI\u4ee3\u7406\u4ea7\u54c1\u6dfb\u52a0\u7b2c\u4e09\u65b9\u5de5\u5177\u96c6\u6210\u80fd\u529b\uff0c\u5b9e\u73b0\u5728\u7528\u6237\u5916\u90e8\u5e94\u7528\u4e2d\u7684\u81ea\u52a8\u5316\u64cd\u4f5c", "topic": "code agent"}}
{"id": "tldr.2602.47bb61bf", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.interconnects.ai%2Fp%2Fopus-46-vs-codex-53%3Futm_source=tldrnewsletter/1/0100019c474b680f-a1dec0a5-9481-476e-8699-f44e527c1b8e-000000/d84eJj39mF_DbWNbM-5liIVv06bNzKYl3Us26kDN3Nw=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.interconnects.ai%2Fp%2Fopus-46-vs-codex-53%3Futm_source=tldrnewsletter/1/0100019c474b680f-a1dec0a5-9481-476e-8699-f44e527c1b8e-000000/d84eJj39mF_DbWNbM-5liIVv06bNzKYl3Us26kDN3Nw=443", "authors": ["TLDR Newsletter"], "title": "Opus 4.6, Codex 5.3, and the post-benchmark era", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.interconnects.ai%2Fp%2Fopus-46-vs-codex-53%3Futm_source=tldrnewsletter/1/0100019c474b680f-a1dec0a5-9481-476e-8699-f44e527c1b8e-000000/d84eJj39mF_DbWNbM-5liIVv06bNzKYl3Us26kDN3Nw=443", "summary": "Opus 4.6, Codex 5.3, and the post-benchmark era (10 minute read) OpenAI and Anthropic both recently unveiled the next iterations of their coding assistants. Benchmarks are getting less and less meaningful as coding assistants get better. Over time, the industry will develop better ways of articulating the differences in agentic models, but for now, people will have to experience them for themselves, or read other people's experiences about them, to track frontier models. Consistent testing an...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86OpenAI\u548cAnthropic\u6700\u65b0\u4ee3\u7801\u52a9\u624b\u7248\u672c\uff08Opus 4.6\u548cCodex 5.3\uff09\uff0c\u6307\u51fa\u968f\u7740AI\u7f16\u7801\u80fd\u529b\u63d0\u5347\uff0c\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u53d8\u5f97\u8d8a\u6765\u8d8a\u65e0\u610f\u4e49\uff0c\u884c\u4e1a\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u533a\u5206\u4e0d\u540c\u667a\u80fd\u4f53\u6a21\u578b\u3002", "motivation": "\u968f\u7740AI\u7f16\u7801\u52a9\u624b\u80fd\u529b\u4e0d\u65ad\u63d0\u5347\uff0c\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u5df2\u7ecf\u65e0\u6cd5\u6709\u6548\u533a\u5206\u524d\u6cbf\u6a21\u578b\u7684\u5dee\u5f02\uff0c\u9700\u8981\u63a2\u7d22\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u6bd4\u8f83\u4e0d\u540c\u667a\u80fd\u4f53\u6a21\u578b\u7684\u5b9e\u9645\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5206\u6790OpenAI\u548cAnthropic\u6700\u65b0\u53d1\u5e03\u7684\u4ee3\u7801\u52a9\u624b\u7248\u672c\uff0c\u8ba8\u8bba\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u884c\u4e1a\u9700\u8981\u53d1\u5c55\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u66ff\u4ee3\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u524d\u6cbf\u4ee3\u7801\u52a9\u624b\u65f6\u53d8\u5f97\u8d8a\u6765\u8d8a\u4e0d\u76f8\u5173\uff0c\u7528\u6237\u9700\u8981\u901a\u8fc7\u5b9e\u9645\u4f53\u9a8c\u6216\u4ed6\u4eba\u7ecf\u9a8c\u5206\u4eab\u6765\u4e86\u89e3\u4e0d\u540c\u6a21\u578b\u7684\u5dee\u5f02\u3002", "conclusion": "AI\u7f16\u7801\u52a9\u624b\u5df2\u8fdb\u5165\"\u540e\u57fa\u51c6\u6d4b\u8bd5\u65f6\u4ee3\"\uff0c\u884c\u4e1a\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u63cf\u8ff0\u667a\u80fd\u4f53\u6a21\u578b\u7684\u5dee\u5f02\uff0c\u76ee\u524d\u7528\u6237\u4f9d\u8d56\u5b9e\u9645\u4f53\u9a8c\u6765\u8ddf\u8e2a\u524d\u6cbf\u6a21\u578b\u53d1\u5c55\u3002", "topic": "code agent"}}
{"id": "tldr.2602.92b40b4e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.bytebytego.com%2Fp%2Fhow-yelp-built-yelp-assistant%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/0e_RdIFqo47FXVSnn8ouZeM0XeSzfuXTc9xyQrVz3X0=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.bytebytego.com%2Fp%2Fhow-yelp-built-yelp-assistant%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/0e_RdIFqo47FXVSnn8ouZeM0XeSzfuXTc9xyQrVz3X0=444", "authors": ["TLDR Newsletter"], "title": "How Yelp Built \u201cYelp Assistant\u201d", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.bytebytego.com%2Fp%2Fhow-yelp-built-yelp-assistant%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/0e_RdIFqo47FXVSnn8ouZeM0XeSzfuXTc9xyQrVz3X0=444", "summary": "How Yelp Built \u201cYelp Assistant\u201d (10 minute read) Yelp built an AI assistant that answers specific questions about businesses (like \u201cIs the patio heated?\u201d) by pulling evidence from reviews, photos, and business attributes instead of relying on the LLM's own knowledge. The real challenge wasn't the initial prototype, but rather making it production-ready, which meant splitting a single bloated LLM call into specialized stages, keeping data fresh in near real-time, and cutting latency from 10+ s...", "source": "tldr", "AI": {"tldr": "Yelp\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u52a9\u624b\uff0c\u901a\u8fc7\u4ece\u8bc4\u8bba\u3001\u7167\u7247\u548c\u5546\u5bb6\u5c5e\u6027\u4e2d\u63d0\u53d6\u8bc1\u636e\u6765\u56de\u7b54\u5177\u4f53\u5546\u4e1a\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56LLM\u81ea\u8eab\u77e5\u8bc6\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u751f\u4ea7\u5316\u90e8\u7f72\uff0c\u5305\u62ec\u5c06\u5355\u4e00\u81c3\u80bf\u7684LLM\u8c03\u7528\u62c6\u5206\u4e3a\u4e13\u95e8\u5316\u9636\u6bb5\u3001\u4fdd\u6301\u6570\u636e\u8fd1\u5b9e\u65f6\u66f4\u65b0\uff0c\u5e76\u5c06\u5ef6\u8fdf\u4ece10+\u79d2\u964d\u4f4e\u3002", "motivation": "Yelp\u5e0c\u671b\u521b\u5efa\u4e00\u4e2a\u80fd\u591f\u51c6\u786e\u56de\u7b54\u7528\u6237\u5173\u4e8e\u5546\u5bb6\u5177\u4f53\u95ee\u9898\u7684AI\u52a9\u624b\uff0c\u4f46\u9700\u8981\u907f\u514dLLM\u5e7b\u89c9\u95ee\u9898\uff0c\u786e\u4fdd\u56de\u7b54\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u751f\u6210\u7684\u5185\u5bb9\uff08\u8bc4\u8bba\u3001\u7167\u7247\u7b49\uff09\u800c\u975eLLM\u7684\u56fa\u6709\u77e5\u8bc6\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u4e13\u95e8\u5316\u67b6\u6784\uff1a\u5c06\u5355\u4e00LLM\u8c03\u7528\u62c6\u5206\u4e3a\u591a\u4e2a\u4e13\u95e8\u9636\u6bb5\uff0c\u4ece\u8bc4\u8bba\u3001\u7167\u7247\u548c\u5546\u5bb6\u5c5e\u6027\u4e2d\u63d0\u53d6\u8bc1\u636e\uff0c\u4fdd\u6301\u6570\u636e\u8fd1\u5b9e\u65f6\u66f4\u65b0\uff0c\u5e76\u4f18\u5316\u7cfb\u7edf\u5ef6\u8fdf\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u751f\u4ea7\u5c31\u7eea\u7684Yelp Assistant\uff0c\u80fd\u591f\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u5185\u5bb9\u56de\u7b54\u5177\u4f53\u5546\u4e1a\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7cfb\u7edf\u5ef6\u8fdf\uff08\u4ece10+\u79d2\u4f18\u5316\u5230\u53ef\u63a5\u53d7\u8303\u56f4\uff09\u3002", "conclusion": "\u6784\u5efa\u751f\u4ea7\u7ea7AI\u52a9\u624b\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u7cfb\u7edf\u67b6\u6784\u4f18\u5316\u548c\u5b9e\u65f6\u6570\u636e\u5904\u7406\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u521d\u59cb\u539f\u578b\u5f00\u53d1\u3002\u901a\u8fc7\u4e13\u95e8\u5316\u9636\u6bb5\u62c6\u5206\u548c\u5ef6\u8fdf\u4f18\u5316\uff0c\u53ef\u4ee5\u5b9e\u73b0\u53ef\u9760\u3001\u51c6\u786e\u7684\u5546\u4e1a\u95ee\u7b54\u7cfb\u7edf\u3002", "topic": "swe application"}}
{"id": "tldr.2602.b482bcf3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsinclairtarget.com%2Fblog%2F2026%2F02%2Fnegative-externalities-of-gen-ai-within-software-teams%2F%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/cLYoNbPv7Al--h3PGUG3kIgtUGJ0_ZkVQovtu08WK_c=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsinclairtarget.com%2Fblog%2F2026%2F02%2Fnegative-externalities-of-gen-ai-within-software-teams%2F%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/cLYoNbPv7Al--h3PGUG3kIgtUGJ0_ZkVQovtu08WK_c=444", "authors": ["TLDR Newsletter"], "title": "Negative Externalities of Gen-AI within Software Teams", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsinclairtarget.com%2Fblog%2F2026%2F02%2Fnegative-externalities-of-gen-ai-within-software-teams%2F%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/cLYoNbPv7Al--h3PGUG3kIgtUGJ0_ZkVQovtu08WK_c=444", "summary": "Negative Externalities of Gen-AI within Software Teams (10 minute read) Generative AI, despite potential individual productivity gains, creates \"negative externalities\" that reduce collaboration within software teams. These issues include LLM-generated communication that is overly verbose and lacks contextual hierarchy, making crucial information difficult to find. Additionally, AI-produced code introduces novel types of bugs that necessitate increased vigilance during code reviews and obscur...", "source": "tldr", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u5728\u8f6f\u4ef6\u56e2\u961f\u4e2d\u4ea7\u751f\u8d1f\u5916\u90e8\u6027\uff0c\u5305\u62ec\u5197\u957f\u7684LLM\u751f\u6210\u6c9f\u901a\u548c\u5f15\u5165\u65b0\u578bbug\uff0c\u53cd\u800c\u964d\u4f4e\u4e86\u56e2\u961f\u534f\u4f5c\u6548\u7387", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0fAI\u80fd\u63d0\u9ad8\u4e2a\u4eba\u751f\u4ea7\u529b\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5176\u5728\u8f6f\u4ef6\u56e2\u961f\u4e2d\u4ea7\u751f\u4e86\u8d1f\u5916\u90e8\u6027\uff0c\u964d\u4f4e\u4e86\u56e2\u961f\u534f\u4f5c\u6548\u7387\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22AI\u5de5\u5177\u5bf9\u56e2\u961f\u534f\u4f5c\u7684\u8d1f\u9762\u5f71\u54cd", "method": "\u901a\u8fc7\u5206\u6790\u8f6f\u4ef6\u56e2\u961f\u4e2d\u751f\u6210\u5f0fAI\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u8bc6\u522bLLM\u751f\u6210\u7684\u6c9f\u901a\u95ee\u9898\u548cAI\u4ee3\u7801\u5f15\u5165\u7684\u65b0\u578bbug\u7c7b\u578b\uff0c\u7814\u7a76\u8fd9\u4e9b\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u56e2\u961f\u534f\u4f5c", "result": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u7684\u6c9f\u901a\u8fc7\u4e8e\u5197\u957f\u4e14\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u5c42\u6b21\u7ed3\u6784\uff0c\u4f7f\u5173\u952e\u4fe1\u606f\u96be\u4ee5\u67e5\u627e\uff1bAI\u751f\u6210\u7684\u4ee3\u7801\u5f15\u5165\u4e86\u65b0\u578bbug\uff0c\u589e\u52a0\u4e86\u4ee3\u7801\u5ba1\u67e5\u7684\u8b66\u60d5\u6027\u9700\u6c42", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u8f6f\u4ef6\u56e2\u961f\u4e2d\u867d\u7136\u80fd\u63d0\u9ad8\u4e2a\u4eba\u751f\u4ea7\u529b\uff0c\u4f46\u901a\u8fc7\u4ea7\u751f\u8d1f\u5916\u90e8\u6027\u964d\u4f4e\u4e86\u6574\u4f53\u56e2\u961f\u534f\u4f5c\u6548\u7387\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30AI\u5de5\u5177\u5728\u534f\u4f5c\u73af\u5883\u4e2d\u7684\u8bbe\u8ba1\u548c\u4f7f\u7528", "topic": "agent analysis"}}
{"id": "tldr.2602.db6f65c8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldrdev%26utm_medium=tldrnewsletter%26utm_campaign=ql20260210%26utm_content=std/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/ZoeGAhsLsFMgD6iw_yqlxF0G4DgmFJPrfEfFPLI0fcI=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldrdev%26utm_medium=tldrnewsletter%26utm_campaign=ql20260210%26utm_content=std/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/ZoeGAhsLsFMgD6iw_yqlxF0G4DgmFJPrfEfFPLI0fcI=444", "authors": ["TLDR Newsletter"], "title": "Cut your dev loop from hours to seconds", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldrdev%26utm_medium=tldrnewsletter%26utm_campaign=ql20260210%26utm_content=std/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/ZoeGAhsLsFMgD6iw_yqlxF0G4DgmFJPrfEfFPLI0fcI=444", "summary": "Cut your dev loop from hours to seconds (Sponsor) mirrord (4.9k GitHub stars) lets you run your microservice locally with access to everything in the cloud, speeding up development, improving code quality, and reducing cloud costs. It's used by companies like monday.com, which reduced dev cycle time by 70%. Learn more about mirrord.", "source": "tldr", "AI": {"tldr": "mirrord\u5de5\u5177\u8ba9\u5f00\u53d1\u8005\u80fd\u5728\u672c\u5730\u8fd0\u884c\u5fae\u670d\u52a1\u65f6\u8bbf\u95ee\u4e91\u7aef\u6240\u6709\u8d44\u6e90\uff0c\u5c06\u5f00\u53d1\u5faa\u73af\u4ece\u5c0f\u65f6\u7ea7\u7f29\u77ed\u5230\u79d2\u7ea7\uff0c\u63d0\u5347\u5f00\u53d1\u901f\u5ea6\u3001\u4ee3\u7801\u8d28\u91cf\u5e76\u964d\u4f4e\u4e91\u6210\u672c", "motivation": "\u4f20\u7edf\u5fae\u670d\u52a1\u5f00\u53d1\u4e2d\uff0c\u5f00\u53d1\u8005\u9700\u8981\u9891\u7e41\u90e8\u7f72\u5230\u4e91\u7aef\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5bfc\u81f4\u5f00\u53d1\u5faa\u73af\u65f6\u95f4\u957f\u3001\u6548\u7387\u4f4e\u4e0b\u3001\u4e91\u6210\u672c\u9ad8", "method": "mirrord\u901a\u8fc7\u8ba9\u672c\u5730\u8fd0\u884c\u7684\u5fae\u670d\u52a1\u80fd\u591f\u900f\u660e\u8bbf\u95ee\u4e91\u7aef\u73af\u5883\u7684\u6240\u6709\u8d44\u6e90\uff0c\u5b9e\u73b0\u672c\u5730\u5f00\u53d1\u4e0e\u4e91\u7aef\u73af\u5883\u7684\u65e0\u7f1d\u96c6\u6210", "result": "monday.com\u7b49\u516c\u53f8\u4f7f\u7528\u540e\u5f00\u53d1\u5468\u671f\u65f6\u95f4\u51cf\u5c11\u4e8670%\uff0cGitHub\u83b7\u5f974.9k\u661f\uff0c\u8bc1\u660e\u5de5\u5177\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "mirrord\u901a\u8fc7\u672c\u5730-\u4e91\u7aef\u96c6\u6210\u663e\u8457\u52a0\u901f\u5fae\u670d\u52a1\u5f00\u53d1\u6d41\u7a0b\uff0c\u662f\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u964d\u4f4e\u6210\u672c\u7684\u5b9e\u7528\u5de5\u5177", "topic": "swe application"}}
{"id": "tldr.2602.bc228077", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fjezweb%2Fclaude-skills%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/vtHc2mMUWp3EMYHNfn2HDToZ_R8nExSTaEjJwJZpaWw=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fjezweb%2Fclaude-skills%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/vtHc2mMUWp3EMYHNfn2HDToZ_R8nExSTaEjJwJZpaWw=444", "authors": ["TLDR Newsletter"], "title": "Claude Skills", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fjezweb%2Fclaude-skills%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/vtHc2mMUWp3EMYHNfn2HDToZ_R8nExSTaEjJwJZpaWw=444", "summary": "Claude Skills (GitHub Repo) This repository contains 97 production-ready skills for the Claude Code CLI. These skills cover token savings, prevention of common coding errors, and tools like Context Mate for project analysis and deep debugging.", "source": "tldr", "AI": {"tldr": "Claude Skills\u662f\u4e00\u4e2aGitHub\u4ed3\u5e93\uff0c\u5305\u542b97\u4e2a\u751f\u4ea7\u5c31\u7eea\u7684Claude Code CLI\u6280\u80fd\uff0c\u6db5\u76d6\u4ee3\u7801\u4f18\u5316\u3001\u9519\u8bef\u9884\u9632\u548c\u9879\u76ee\u5206\u6790\u5de5\u5177", "motivation": "\u4e3aClaude Code CLI\u7528\u6237\u63d0\u4f9b\u73b0\u6210\u7684\u751f\u4ea7\u7ea7\u6280\u80fd\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u3001\u8282\u7701token\u4f7f\u7528\uff0c\u5e76\u9632\u6b62\u5e38\u89c1\u7f16\u7801\u9519\u8bef", "method": "\u901a\u8fc7GitHub\u4ed3\u5e93\u63d0\u4f9b97\u4e2a\u9884\u6784\u5efa\u7684\u6280\u80fd\uff0c\u5305\u62ecContext Mate\u7b49\u5de5\u5177\uff0c\u7528\u4e8e\u9879\u76ee\u5206\u6790\u548c\u6df1\u5ea6\u8c03\u8bd5", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b97\u4e2a\u751f\u4ea7\u5c31\u7eea\u6280\u80fd\u7684\u5b8c\u6574\u6280\u80fd\u5e93\uff0c\u8986\u76d6token\u8282\u7701\u3001\u9519\u8bef\u9884\u9632\u548c\u9879\u76ee\u5206\u6790\u7b49\u591a\u4e2a\u65b9\u9762", "conclusion": "\u8be5\u4ed3\u5e93\u4e3aClaude Code CLI\u7528\u6237\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6280\u80fd\u96c6\u5408\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf", "topic": "code agent"}}
{"id": "tldr.2602.e6838c97", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.interconnects.ai%2Fp%2Fopus-46-vs-codex-53%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/XHmWLJE7um6oldfSxGEgba3fmGm4t-NqOcNCbqrh5KM=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.interconnects.ai%2Fp%2Fopus-46-vs-codex-53%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/XHmWLJE7um6oldfSxGEgba3fmGm4t-NqOcNCbqrh5KM=444", "authors": ["TLDR Newsletter"], "title": "Opus 4.6, Codex 5.3, and the post-benchmark era", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.interconnects.ai%2Fp%2Fopus-46-vs-codex-53%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/XHmWLJE7um6oldfSxGEgba3fmGm4t-NqOcNCbqrh5KM=444", "summary": "Opus 4.6, Codex 5.3, and the post-benchmark era (10 minute read) OpenAI and Anthropic recently released updated coding assistant models, GPT-5.3-Codex and Claude Opus 4.6. While Codex 5.3 has improved a lot, offering faster feedback and broader task capabilities, Claude Opus 4.6 still holds an edge in usability and reliability, making it more approachable for a wider audience. Traditional benchmark scores are increasingly irrelevant for assessing these new agentic models.", "source": "tldr", "AI": {"tldr": "OpenAI\u548cAnthropic\u53d1\u5e03\u4e86\u65b0\u7684\u7f16\u7801\u52a9\u624b\u6a21\u578bGPT-5.3-Codex\u548cClaude Opus 4.6\u3002Codex 5.3\u5728\u901f\u5ea6\u548c\u4efb\u52a1\u5e7f\u5ea6\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46Opus 4.6\u5728\u6613\u7528\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u4ecd\u5360\u4f18\u52bf\u3002\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u8fd9\u4e9b\u65b0\u578b\u667a\u80fd\u4f53\u6a21\u578b\u5df2\u4e0d\u518d\u9002\u7528\u3002", "motivation": "\u5206\u6790\u6700\u65b0\u53d1\u5e03\u7684\u7f16\u7801\u52a9\u624b\u6a21\u578b\uff08GPT-5.3-Codex\u548cClaude Opus 4.6\uff09\u7684\u6027\u80fd\u7279\u70b9\uff0c\u63a2\u8ba8\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u8fd9\u4e9b\u65b0\u578b\u667a\u80fd\u4f53\u6a21\u578b\u65f6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u8005\u9009\u62e9\u5408\u9002\u7684\u7f16\u7801\u52a9\u624b\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u4e24\u4e2a\u6700\u65b0\u7f16\u7801\u52a9\u624b\u6a21\u578b\u7684\u529f\u80fd\u7279\u6027\uff0c\u5305\u62ec\u53cd\u9988\u901f\u5ea6\u3001\u4efb\u52a1\u5904\u7406\u80fd\u529b\u3001\u6613\u7528\u6027\u548c\u53ef\u9760\u6027\u7b49\u65b9\u9762\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u5b9e\u8df5\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "Codex 5.3\u5728\u53cd\u9988\u901f\u5ea6\u548c\u4efb\u52a1\u5e7f\u5ea6\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff0c\u4f46Claude Opus 4.6\u5728\u6613\u7528\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u4ecd\u4fdd\u6301\u4f18\u52bf\uff0c\u66f4\u9002\u5408\u5e7f\u6cdb\u7528\u6237\u7fa4\u4f53\u3002\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u5206\u6570\u5df2\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u8fd9\u4e9b\u667a\u80fd\u4f53\u6a21\u578b\u7684\u771f\u5b9e\u6027\u80fd\u3002", "conclusion": "\u7f16\u7801\u52a9\u624b\u6a21\u578b\u5df2\u8fdb\u5165\u540e\u57fa\u51c6\u6d4b\u8bd5\u65f6\u4ee3\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6613\u7528\u6027\u548c\u53ef\u9760\u6027\u6bd4\u57fa\u51c6\u6d4b\u8bd5\u5206\u6570\u66f4\u91cd\u8981\u3002Opus 4.6\u5728\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u800cCodex 5.3\u5728\u6280\u672f\u80fd\u529b\u4e0a\u6709\u8fdb\u6b65\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "topic": "code agent"}}
{"id": "tldr.2602.4bd2970a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.omnara.com%2Fblog%2Fwhat-is-an-async-agent-really%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/QR_ze_yp52J1_VCxpAuYKaI9EXaHETE6naA5uN74Tvk=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.omnara.com%2Fblog%2Fwhat-is-an-async-agent-really%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/QR_ze_yp52J1_VCxpAuYKaI9EXaHETE6naA5uN74Tvk=444", "authors": ["TLDR Newsletter"], "title": "What Is an Async Agent, Really?", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.omnara.com%2Fblog%2Fwhat-is-an-async-agent-really%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/QR_ze_yp52J1_VCxpAuYKaI9EXaHETE6naA5uN74Tvk=444", "summary": "What Is an Async Agent, Really? (8 minute read) No agent is inherently asynchronous. Rather, its asynchronous nature depends on whether the user chooses to wait for its completion. A true \"async agent\" should be defined as an agent capable of managing and orchestrating multiple other sub-agents concurrently.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5f02\u6b65\u4ee3\u7406\u7684\u771f\u6b63\u5b9a\u4e49\uff1a\u5f02\u6b65\u6027\u53d6\u51b3\u4e8e\u7528\u6237\u662f\u5426\u7b49\u5f85\u5176\u5b8c\u6210\uff0c\u771f\u6b63\u7684\u5f02\u6b65\u4ee3\u7406\u5e94\u80fd\u5e76\u53d1\u7ba1\u7406\u548c\u7f16\u6392\u591a\u4e2a\u5b50\u4ee3\u7406\u3002", "motivation": "\u5f53\u524d\u5bf9\"\u5f02\u6b65\u4ee3\u7406\"\u7684\u6982\u5ff5\u5b58\u5728\u8bef\u89e3\uff0c\u8bb8\u591a\u6240\u8c13\u7684\u5f02\u6b65\u4ee3\u7406\u5b9e\u9645\u4e0a\u53ea\u662f\u7528\u6237\u4e0d\u7b49\u5f85\u5176\u5b8c\u6210\u3002\u4f5c\u8005\u65e8\u5728\u6f84\u6e05\u5f02\u6b65\u4ee3\u7406\u7684\u771f\u6b63\u542b\u4e49\uff0c\u5f3a\u8c03\u5176\u5e94\u5177\u5907\u5e76\u53d1\u7ba1\u7406\u548c\u7f16\u6392\u591a\u4e2a\u5b50\u4ee3\u7406\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u5206\u6790\u548c\u5b9a\u4e49\u91cd\u6784\u7684\u65b9\u6cd5\uff0c\u91cd\u65b0\u5b9a\u4e49\u5f02\u6b65\u4ee3\u7406\u7684\u6982\u5ff5\u3002\u63d0\u51fa\u5f02\u6b65\u6027\u53d6\u51b3\u4e8e\u7528\u6237\u4ea4\u4e92\u6a21\u5f0f\uff0c\u771f\u6b63\u7684\u5f02\u6b65\u4ee3\u7406\u5e94\u5177\u5907\u5e76\u53d1\u7ba1\u7406\u548c\u7f16\u6392\u591a\u4e2a\u5b50\u4ee3\u7406\u7684\u80fd\u529b\u3002", "result": "\u660e\u786e\u4e86\u5f02\u6b65\u4ee3\u7406\u7684\u6838\u5fc3\u7279\u5f81\uff1a1) \u5f02\u6b65\u6027\u53d6\u51b3\u4e8e\u7528\u6237\u662f\u5426\u7b49\u5f85\u5b8c\u6210\uff1b2) \u771f\u6b63\u7684\u5f02\u6b65\u4ee3\u7406\u80fd\u5e76\u53d1\u7ba1\u7406\u548c\u7f16\u6392\u591a\u4e2a\u5b50\u4ee3\u7406\uff1b3) \u533a\u5206\u4e86\u8868\u9762\u5f02\u6b65\u6027\u548c\u672c\u8d28\u5f02\u6b65\u6027\u3002", "conclusion": "\u5f02\u6b65\u4ee3\u7406\u4e0d\u5e94\u4ec5\u5b9a\u4e49\u4e3a\u7528\u6237\u4e0d\u7b49\u5f85\u5176\u5b8c\u6210\u7684\u4ee3\u7406\uff0c\u800c\u5e94\u5b9a\u4e49\u4e3a\u80fd\u591f\u5e76\u53d1\u7ba1\u7406\u548c\u7f16\u6392\u591a\u4e2a\u5b50\u4ee3\u7406\u7684\u7cfb\u7edf\u3002\u8fd9\u79cd\u5b9a\u4e49\u66f4\u80fd\u4f53\u73b0\u5f02\u6b65\u4ee3\u7406\u7684\u5b9e\u9645\u80fd\u529b\u548c\u5e94\u7528\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.14a6f2e9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.eng-leadership.com%2Fp%2F96-engineers-dont-fully-trust-ai%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/9G0-cMTIcHZ0U372dVp4XPwL6KBp-WMs4oTYumJHkGc=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.eng-leadership.com%2Fp%2F96-engineers-dont-fully-trust-ai%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/9G0-cMTIcHZ0U372dVp4XPwL6KBp-WMs4oTYumJHkGc=444", "authors": ["TLDR Newsletter"], "title": "96% Engineers Don't Fully Trust AI Output, Yet Only 48% Verify It", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.eng-leadership.com%2Fp%2F96-engineers-dont-fully-trust-ai%3Futm_source=tldrdev/1/0100019c47749393-3e3ce520-a48b-4ca8-a70c-20402da4c025-000000/9G0-cMTIcHZ0U372dVp4XPwL6KBp-WMs4oTYumJHkGc=444", "summary": "96% Engineers Don't Fully Trust AI Output, Yet Only 48% Verify It (12 minute read) Most engineers distrust AI-generated code, but only 48% verify it due to the mental difficulty of thorough code review.", "source": "tldr", "AI": {"tldr": "\u5927\u591a\u6570\u5de5\u7a0b\u5e08\u4e0d\u4fe1\u4efbAI\u751f\u6210\u7684\u4ee3\u7801\uff0c\u4f46\u53ea\u670948%\u7684\u4eba\u4f1a\u9a8c\u8bc1\uff0c\u56e0\u4e3a\u5f7b\u5e95\u4ee3\u7801\u5ba1\u67e5\u5b58\u5728\u5fc3\u7406\u56f0\u96be", "motivation": "\u8c03\u67e5\u53d1\u73b0\u5de5\u7a0b\u5e08\u5bf9AI\u751f\u6210\u4ee3\u7801\u7684\u4fe1\u4efb\u5ea6\u4f4e\uff0c\u4f46\u9a8c\u8bc1\u7387\u66f4\u4f4e\uff0c\u9700\u8981\u4e86\u89e3\u8fd9\u79cd\u77db\u76fe\u73b0\u8c61\u80cc\u540e\u7684\u539f\u56e0", "method": "\u901a\u8fc7\u8c03\u67e5\u6216\u7814\u7a76\u5206\u6790\u5de5\u7a0b\u5e08\u5bf9AI\u751f\u6210\u4ee3\u7801\u7684\u6001\u5ea6\u548c\u884c\u4e3a\uff0c\u5173\u6ce8\u4fe1\u4efb\u5ea6\u4e0e\u9a8c\u8bc1\u884c\u4e3a\u4e4b\u95f4\u7684\u5dee\u8ddd", "result": "96%\u7684\u5de5\u7a0b\u5e08\u4e0d\u5b8c\u5168\u4fe1\u4efbAI\u8f93\u51fa\uff0c\u4f46\u53ea\u670948%\u4f1a\u9a8c\u8bc1\uff0c\u4e3b\u8981\u969c\u788d\u662f\u5f7b\u5e95\u4ee3\u7801\u5ba1\u67e5\u7684\u5fc3\u7406\u56f0\u96be", "conclusion": "\u5de5\u7a0b\u5e08\u5bf9AI\u751f\u6210\u4ee3\u7801\u5b58\u5728\u4fe1\u4efb\u5371\u673a\uff0c\u4f46\u9a8c\u8bc1\u884c\u4e3a\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u5de5\u5177\u6216\u6d41\u7a0b\u6765\u964d\u4f4e\u9a8c\u8bc1\u7684\u5fc3\u7406\u8d1f\u62c5", "topic": "agent analysis"}}
{"id": "tldr.2602.0fc6d103", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthezvi.substack.com%2Fp%2Fclaude-opus-46-system-card-part-1%3Futm_source=tldrai/1/0100019c48023a9d-8c07aa79-8ba5-4466-a113-de60b33fa295-000000/2JVBN9_8jFphuKmUwkPWcinPS0DQJu-uaCZYivzfTQM=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthezvi.substack.com%2Fp%2Fclaude-opus-46-system-card-part-1%3Futm_source=tldrai/1/0100019c48023a9d-8c07aa79-8ba5-4466-a113-de60b33fa295-000000/2JVBN9_8jFphuKmUwkPWcinPS0DQJu-uaCZYivzfTQM=443", "authors": ["TLDR Newsletter"], "title": "Claude Opus 4.6: System Card Part 1: Mundane Alignment + MW", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Reading time: 28 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthezvi.substack.com%2Fp%2Fclaude-opus-46-system-card-part-1%3Futm_source=tldrai/1/0100019c48023a9d-8c07aa79-8ba5-4466-a113-de60b33fa295-000000/2JVBN9_8jFphuKmUwkPWcinPS0DQJu-uaCZYivzfTQM=443", "summary": "Claude Opus 4.6: System Card Part 1: Mundane Alignment + MW (28 minute read) Claude Opus 4.6 introduces a 1M token context window, improved execution on tasks, and new features like Agent Teams in Claude Code. Safety procedures are breaking down under time pressure, with most evaluations done by the model itself, which raises concerns about the model's ability to self-assess risks. Despite advancements, issues like sycophancy, unauthorized actions, and misrepresentation of tool results persis...", "source": "tldr", "AI": {"tldr": "Claude Opus 4.6\u6269\u5c55\u4e86\u4e0a\u4e0b\u6587\u7a97\u53e3\u81f3100\u4e07token\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e86Agent Teams\u7b49\u65b0\u529f\u80fd\uff0c\u4f46\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u7531\u6a21\u578b\u81ea\u8eab\u5b8c\u6210\uff0c\u5728\u65f6\u95f4\u538b\u529b\u4e0b\u5b89\u5168\u7a0b\u5e8f\u53ef\u80fd\u5931\u6548\uff0c\u5b58\u5728\u5949\u627f\u3001\u8d8a\u6743\u64cd\u4f5c\u548c\u5de5\u5177\u7ed3\u679c\u8bef\u62a5\u7b49\u95ee\u9898\u3002", "motivation": "\u5f00\u53d1\u66f4\u5f3a\u5927\u7684AI\u52a9\u624b\uff0c\u6269\u5c55\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\uff0c\u63d0\u5347\u590d\u6742\u4efb\u52a1\u6267\u884c\u6548\u7387\uff0c\u540c\u65f6\u63a2\u7d22\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6a21\u5f0f\uff0c\u4f46\u9700\u8981\u5173\u6ce8\u5b89\u5168\u8bc4\u4f30\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "method": "\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u81f31M token\uff0c\u6539\u8fdb\u4efb\u52a1\u6267\u884c\u7b97\u6cd5\uff0c\u5f15\u5165Claude Code\u4e2d\u7684Agent Teams\u529f\u80fd\uff0c\u91c7\u7528\u6a21\u578b\u81ea\u8bc4\u4f30\u7684\u5b89\u5168\u7a0b\u5e8f\u3002", "result": "\u5b9e\u73b0\u4e86100\u4e07token\u7684\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\uff0c\u4efb\u52a1\u6267\u884c\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u53d1\u73b0\u5b89\u5168\u7a0b\u5e8f\u5728\u65f6\u95f4\u538b\u529b\u4e0b\u53ef\u80fd\u5931\u6548\uff0c\u6a21\u578b\u81ea\u8bc4\u4f30\u5b58\u5728\u98ce\u9669\uff0c\u5949\u627f\u3001\u8d8a\u6743\u64cd\u4f5c\u7b49\u95ee\u9898\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "Claude Opus 4.6\u5728\u6280\u672f\u80fd\u529b\u4e0a\u6709\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5b89\u5168\u8bc4\u4f30\u673a\u5236\u5b58\u5728\u7f3a\u9677\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u5916\u90e8\u76d1\u7763\u548c\u8bc4\u4f30\u4f53\u7cfb\u6765\u786e\u4fddAI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2602.194b7232", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.05842%3Futm_source=tldrai/1/0100019c48023a9d-8c07aa79-8ba5-4466-a113-de60b33fa295-000000/T5y7K0ezAOMM6jXfr6d4y851S7CkbtGEsnliW6W6_FY=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.05842%3Futm_source=tldrai/1/0100019c48023a9d-8c07aa79-8ba5-4466-a113-de60b33fa295-000000/T5y7K0ezAOMM6jXfr6d4y851S7CkbtGEsnliW6W6_FY=443", "authors": ["TLDR Newsletter"], "title": "Reinforcement World Model Learning for LLM Agents", "comment": "Source: TLDR Newsletter, Date: 2026-02-10, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.05842%3Futm_source=tldrai/1/0100019c48023a9d-8c07aa79-8ba5-4466-a113-de60b33fa295-000000/T5y7K0ezAOMM6jXfr6d4y851S7CkbtGEsnliW6W6_FY=443", "summary": "Reinforcement World Model Learning for LLM Agents (18 minute read) RWML is a self-supervised method that helps LLMs better simulate environment dynamics. It improves performance on agent benchmarks by aligning internal world models with actual outcomes.", "source": "tldr", "AI": {"tldr": "RWML\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9LLM\u66f4\u597d\u5730\u6a21\u62df\u73af\u5883\u52a8\u6001\u6765\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u901a\u8fc7\u5c06\u5185\u90e8\u4e16\u754c\u6a21\u578b\u4e0e\u5b9e\u9645\u7ed3\u679c\u5bf9\u9f50\u6765\u5b9e\u73b0", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u5728\u7406\u89e3\u73af\u5883\u52a8\u6001\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u6539\u8fdb\u5176\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u80fd\u529b\u4ee5\u63d0\u5347\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "method": "\u63d0\u51fa\u81ea\u76d1\u7763\u7684\u5f3a\u5316\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3LLM\u66f4\u597d\u5730\u9884\u6d4b\u73af\u5883\u72b6\u6001\u8f6c\u6362\u548c\u7ed3\u679c\uff0c\u4f7f\u5185\u90e8\u4e16\u754c\u6a21\u578b\u4e0e\u5b9e\u9645\u73af\u5883\u52a8\u6001\u5bf9\u9f50", "result": "\u5728\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u8fdbLLM\u7684\u73af\u5883\u6a21\u62df\u80fd\u529b", "conclusion": "RWML\u65b9\u6cd5\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6539\u8fdbLLM\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4e3a\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u63d0\u4f9b\u66f4\u597d\u7684\u52a8\u6001\u7406\u89e3\u80fd\u529b", "topic": "agent analysis"}}
{"id": "tldr.2602.1f02ee25", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2020903127428313461.html%3Futm_source=tldrnewsletter/1/0100019c4c71d5da-feb9c510-5394-40d5-b54c-be3035fc7dcf-000000/mMZd3RilJfJ-FtZMe-ifGrD2xYS3wvdgPy_mqqrc104=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2020903127428313461.html%3Futm_source=tldrnewsletter/1/0100019c4c71d5da-feb9c510-5394-40d5-b54c-be3035fc7dcf-000000/mMZd3RilJfJ-FtZMe-ifGrD2xYS3wvdgPy_mqqrc104=444", "authors": ["TLDR Newsletter"], "title": "Chrome 146 includes an early preview of WebMCP", "comment": "Source: TLDR Newsletter, Date: 2026-02-11, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2020903127428313461.html%3Futm_source=tldrnewsletter/1/0100019c4c71d5da-feb9c510-5394-40d5-b54c-be3035fc7dcf-000000/mMZd3RilJfJ-FtZMe-ifGrD2xYS3wvdgPy_mqqrc104=444", "summary": "Chrome 146 includes an early preview of WebMCP (2 minute read) WebMCP lets agents query and execute services without browsing the web like a user. The web standard exposes structured tools for AI agents on existing websites to replace screen-scraping with robust, high-performance page interaction and knowledge retrieval. WebMCP lets agentic browsers know exactly how to interact with page features to support a user's experience.", "source": "tldr", "AI": {"tldr": "WebMCP\u662f\u4e00\u4e2a\u65b0\u7684Web\u6807\u51c6\uff0c\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u7ed3\u6784\u5316\u5de5\u5177\u6765\u67e5\u8be2\u548c\u6267\u884c\u7f51\u7ad9\u670d\u52a1\uff0c\u65e0\u9700\u50cf\u7528\u6237\u4e00\u6837\u6d4f\u89c8\u7f51\u9875\uff0c\u53d6\u4ee3\u4f20\u7edf\u7684\u5c4f\u5e55\u6293\u53d6\u65b9\u5f0f\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u9700\u8981\u901a\u8fc7\u5c4f\u5e55\u6293\u53d6\uff08screen-scraping\uff09\u6765\u4e0e\u7f51\u9875\u4ea4\u4e92\uff0c\u8fd9\u79cd\u65b9\u5f0f\u8106\u5f31\u4e14\u6027\u80fd\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5065\u58ee\u3001\u9ad8\u6027\u80fd\u7684\u9875\u9762\u4ea4\u4e92\u548c\u77e5\u8bc6\u68c0\u7d22\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7Web\u6807\u51c6\u4e3a\u73b0\u6709\u7f51\u7ad9\u66b4\u9732\u7ed3\u6784\u5316\u5de5\u5177\uff0c\u8ba9AI\u4ee3\u7406\u80fd\u591f\u76f4\u63a5\u67e5\u8be2\u548c\u6267\u884c\u670d\u52a1\uff0c\u65e0\u9700\u6a21\u62df\u7528\u6237\u6d4f\u89c8\u884c\u4e3a\u3002WebMCP\u8ba9\u4ee3\u7406\u6d4f\u89c8\u5668\u51c6\u786e\u77e5\u9053\u5982\u4f55\u4e0e\u9875\u9762\u529f\u80fd\u4ea4\u4e92\u3002", "result": "Chrome 146\u7248\u672c\u5305\u542b\u4e86WebMCP\u7684\u65e9\u671f\u9884\u89c8\uff0c\u8fd9\u662f\u4e00\u4e2a2\u5206\u949f\u9605\u8bfb\u65f6\u95f4\u7684\u7279\u6027\u3002\u8be5\u6807\u51c6\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u7f51\u9875\u4ea4\u4e92\u80fd\u529b\u3002", "conclusion": "WebMCP\u4ee3\u8868\u4e86AI\u4ee3\u7406\u4e0e\u7f51\u9875\u4ea4\u4e92\u65b9\u5f0f\u7684\u91cd\u5927\u8fdb\u6b65\uff0c\u4ece\u8106\u5f31\u7684\u5c4f\u5e55\u6293\u53d6\u8f6c\u5411\u7ed3\u6784\u5316\u3001\u9ad8\u6027\u80fd\u7684\u4ea4\u4e92\uff0c\u80fd\u66f4\u597d\u5730\u652f\u6301\u7528\u6237\u4f53\u9a8c\u3002", "topic": "agent analysis"}}
