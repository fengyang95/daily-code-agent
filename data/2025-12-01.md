<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 11]
- [cs.LG](#cs.LG) [Total: 18]
- [tldr.article](#tldr.article) [Total: 5]
- [wechat.article](#wechat.article) [Total: 20]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [GPS: General Per-Sample Prompter](https://arxiv.org/abs/2511.21714)
*Pawel Batorski,Paul Swoboda*

Main category: cs.CL

TL;DR: GPS是一种通用、按样本提示的方法，无需任务特定调优即可为每个输入生成定制提示，通过强化学习和正则化训练，在多项任务上取得竞争性表现。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示方法存在三大局限：需要大量任务特定数据训练提示、依赖耗时优化循环、只能生成单一任务级提示而无法适应具体输入问题。需要一种更灵活高效的提示生成方法。

Method: 提出GPS方法：1) 使用强化学习在训练任务套件上训练提示生成器；2) 引入新颖正则化技术以有效适应按样本提示；3) 采用最小贝叶斯风险解码稳定推理。

Result: 在文本简化任务上获得第二佳结果，摘要任务第三佳，分类任务表现相当，且未在这些任务上进行训练。在GSM8K上获得最先进结果。展示了无需大量优化和任务特定训练集即可生成自适应提示的潜力。

Conclusion: GPS展示了自动提示的新范式：无需大量优化和任务特定训练集即可生成自适应、输入特定的提示，为提示工程提供了更灵活高效的解决方案。

Abstract: LLMs are sensitive to prompting, with task performance often hinging on subtle, sometimes imperceptible variations in phrasing. As a result, crafting effective prompts manually remains challenging and time-consuming. Recent automatic prompting methods mitigate this difficulty but face three key limitations: (i) for each new task, they require large datasets to train good prompts;(ii) they rely on costly optimization loops that may take hours; (iii)they typically produce a single task-level prompt that does not adapt to the individual input problem to be solved.
  We propose GPS, the first general-purpose, per-sample prompting method. Without any task-specific tuning, GPS generates a tailored prompt for each unseen input, improving performance across diverse tasks. The prompter is trained with reinforcement learning on a suite of training tasks and includes a novel regularization for effectively adapting to per-sample prompting. Finally, we employ Minimum Bayes Risk decoding to stabilize inference.
  Empirically, GPS demonstrates competitive performance: we attain second best results among baselines on text simplification, third best results on summarization and on-par results on classification, while not training on any of these tasks, in contrast to the baselines. For in-domain prompting, we obtain sota on GSM8K. Our work shows the potential of a novel and effective paradigm for automatic prompting: generating adaptive, input-specific prompts without extensive optimization and without access to a task-specific training set. Our code is available at https://github.com/Batorskq/GPS.

</details>


### [2] [Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks](https://arxiv.org/abs/2511.21726)
*Yicong Zheng,Kevin L. McKee,Thomas Miconi,Zacharie Bugaud,Mick van Gelderen,Jed McCaleb*

Main category: cs.CL

TL;DR: SUMER提出了一种基于强化学习的搜索方法，直接在未压缩的原始数据中进行目标导向搜索，超越了传统基于压缩的记忆方法，在长上下文对话理解任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有记忆框架和基准测试主要关注寻找最优的记忆压缩算法，但这往往引入了人类偏见，且预定义的压缩算法无法适应所有原始数据分布。压缩是有损的，而目标导向的搜索可能表现更优。

Method: 提出了SUMER（Search in Uncompressed Memory via Experience Replay），这是一个端到端的强化学习代理，使用可验证奖励（RLVR）学习使用搜索工具收集信息并回答目标问题。

Result: 在LoCoMo长上下文对话理解数据集上，SUMER使用Qwen2.5-7B-Instruct模型学习使用搜索工具，超越了所有其他有偏记忆压缩方法和完整上下文基线，达到SOTA性能（比之前最佳提升43%）。

Conclusion: 简单的搜索方法应用于原始数据优于当前长上下文记忆任务中的目标无关和有偏压缩算法，需要更动态和自主可扩展的新范式与基准测试。

Abstract: How to enable human-like long-term memory in large language models (LLMs) has been a central question for unlocking more general capabilities such as few-shot generalization. Existing memory frameworks and benchmarks focus on finding the optimal memory compression algorithm for higher performance in tasks that require recollection and sometimes further reasoning. However, such efforts have ended up building more human bias into the compression algorithm, through the search for the best prompts and memory architectures that suit specific benchmarks, rather than finding a general solution that would work on other data distributions. On the other hand, goal-directed search on uncompressed information could potentially exhibit superior performance because compression is lossy, and a predefined compression algorithm will not fit all raw data distributions. Here we present SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR) that learns to use search tools to gather information and answer a target question. On the LoCoMo dataset for long-context conversation understanding, SUMER with Qwen2.5-7B-Instruct learned to use search tools and outperformed all other biased memory compression approaches and also the full-context baseline, reaching SOTA performance (43% gain over the prior best). We demonstrate that a simple search method applied to raw data outperforms goal-agnostic and biased compression algorithms in current long-context memory tasks, arguing for new paradigms and benchmarks that are more dynamic and autonomously scalable. Code for SUMER and all implemented baselines is publicly available at https://github.com/zycyc/SUMER.

</details>


### [3] [Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue](https://arxiv.org/abs/2511.21728)
*Lin Yu,Xiaofei Han,Yifei Kang,Chiung-Yi Tseng,Danyang Zhang,Ziqian Bi,Zhimo Han*

Main category: cs.CL

TL;DR: AffectMind是一个多模态情感对话代理，通过主动推理和动态知识基础，在营销对话中实现情感对齐和说服性交互，显著提升情感一致性、说服成功率和用户参与度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对话系统大多是被动的，在情感丰富、目标导向的营销对话场景中表现不佳，需要能够主动推理、动态适应情感并保持说服力的对话代理。

Method: 提出AffectMind多模态情感对话代理，包含三个核心组件：主动知识基础网络（PKGN）从文本、视觉和韵律中持续更新事实和情感上下文；情感-意图对齐模型（EIAM）联合建模用户情感和购买意图以调整说服策略；强化话语循环（RDL）通过用户反馈的强化信号优化情感连贯性和参与度。

Result: 在两个新构建的营销对话数据集（MM-ConvMarket和AffectPromo）上的实验表明，AffectMind在情感一致性（+26%）、说服成功率（+19%）和长期用户参与度（+23%）方面显著优于基于LLM的基线模型。

Conclusion: 情感基础的主动性是商业多模态代理的关键能力，AffectMind通过多模态情感理解和主动推理显著提升了营销对话的效果。

Abstract: Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\%), persuasive success rate (+19\%), and long-term user engagement (+23\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.

</details>


### [4] [Polarity-Aware Probing for Quantifying Latent Alignment in Language Models](https://arxiv.org/abs/2511.21737)
*Sabrina Sadiekh,Elena Ericheva,Chirag Agarwal*

Main category: cs.CL

TL;DR: 本文提出Polarity-Aware CCS (PA-CCS)方法，通过极性反转评估模型内部表征的一致性，并引入Polar-Consistency和Contradiction Index两个对齐指标，用于量化模型潜在知识的语义鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着CCS等无监督探针技术的发展，需要验证这些方法是否能可靠评估模型对齐。研究旨在探究CCS对有害与安全陈述的敏感性，并开发能评估模型内部表征在极性反转下一致性的方法。

Method: 提出Polarity-Aware CCS (PA-CCS)方法，引入Polar-Consistency和Contradiction Index两个对齐指标。构建两个主要数据集和一个控制数据集，包含使用不同方法（并发和对抗性陈述）构建的匹配有害-安全句子对。将PA-CCS应用于16个语言模型。

Result: PA-CCS能够识别潜在有害知识编码中的架构和层级特定差异。对于具有良好对齐内部表征的模型，用无意义标记替换否定标记会降低PA-CCS分数，而缺乏鲁棒内部校准的模型则不会出现这种退化。

Conclusion: 研究强调了无监督探针在对齐评估中的潜力，并强调需要将结构鲁棒性检查纳入可解释性基准。PA-CCS为评估模型内部表征的一致性提供了有效工具。

Abstract: Advances in unsupervised probes such as Contrast-Consistent Search (CCS), which reveal latent beliefs without relying on token outputs, raise the question of whether these methods can reliably assess model alignment. We investigate this by examining the sensitivity of CCS to harmful vs. safe statements and by introducing Polarity-Aware CCS (PA-CCS), a method for evaluating whether a model's internal representations remain consistent under polarity inversion. We propose two alignment-oriented metrics, Polar-Consistency and the Contradiction Index, to quantify the semantic robustness of a model's latent knowledge. To validate PA-CCS, we curate two main datasets and one control dataset containing matched harmful-safe sentence pairs constructed using different methodologies (concurrent and antagonistic statements). We apply PA-CCS to 16 language models. Our results show that PA-CCS identifies both architectural and layer-specific differences in the encoding of latent harmful knowledge. Notably, replacing the negation token with a meaningless marker degrades PA-CCS scores for models with well-aligned internal representations, while models lacking robust internal calibration do not exhibit this degradation. Our findings highlight the potential of unsupervised probing for alignment evaluation and emphasize the need to incorporate structural robustness checks into interpretability benchmarks. Code and datasets are available at: https://github.com/SadSabrina/polarity-probing. WARNING: This paper contains potentially sensitive, harmful, and offensive content.

</details>


### [5] [Factors That Support Grounded Responses in LLM Conversations: A Rapid Review](https://arxiv.org/abs/2511.21762)
*Gabriele Cesar Iwashima,Claudia Susie Rodrigues,Claudio Dipolitto,Geraldo Xexéo*

Main category: cs.CL

TL;DR: 本文综述了LLM对话中对齐技术，通过PRISMA框架的快速回顾，将方法分为推理时、后训练和基于强化学习三类，发现推理时方法在无需重新训练的情况下能有效对齐用户意图、确保上下文基础并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在对话中可能产生与用户意图不一致、缺乏上下文基础或出现幻觉的输出，这影响了LLM应用的可靠性，因此需要系统梳理对齐技术来改善这些问题。

Method: 采用PRISMA框架和PICO策略指导的快速回顾方法，对LLM对齐技术进行系统搜索、筛选和分类，将方法按LLM生命周期阶段分为推理时方法、后训练方法和基于强化学习的方法三类。

Result: 推理时方法被证明特别高效，能在不重新训练的情况下对齐输出，支持用户意图、上下文基础和幻觉缓解。综述的技术为改善LLM响应质量和可靠性提供了结构化机制。

Conclusion: 本文系统回顾了LLM对话对齐技术，为改善LLM响应质量提供了分类框架和实用方法，特别是推理时方法展示了高效的对齐能力，有助于提升LLM应用的可靠性。

Abstract: Large language models (LLMs) may generate outputs that are misaligned with user intent, lack contextual grounding, or exhibit hallucinations during conversation, which compromises the reliability of LLM-based applications. This review aimed to identify and analyze techniques that align LLM responses with conversational goals, ensure grounding, and reduce hallucination and topic drift. We conducted a Rapid Review guided by the PRISMA framework and the PICO strategy to structure the search, filtering, and selection processes. The alignment strategies identified were categorized according to the LLM lifecycle phase in which they operate: inference-time, post-training, and reinforcement learning-based methods. Among these, inference-time approaches emerged as particularly efficient, aligning outputs without retraining while supporting user intent, contextual grounding, and hallucination mitigation. The reviewed techniques provided structured mechanisms for improving the quality and reliability of LLM responses across key alignment objectives.

</details>


### [6] [Improving Score Reliability of Multiple Choice Benchmarks with Consistency Evaluation and Altered Answer Choices](https://arxiv.org/abs/2511.21860)
*Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Claudio Pinhanez,Yago Primerano*

Main category: cs.CL

TL;DR: CoRA是一种改进LLM在多项选择题基准测试中评分可靠性的新指标，通过分析模型回答一致性来调整原始MCQA分数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多项选择题基准测试中的评分可能不可靠，因为即使获得高分，模型也可能表现出回答不一致的问题。需要一种能更好反映模型一致性的评估指标。

Method: 提出CoRA指标，通过生成修改答案选项的合成问题来评估LLM回答一致性。使用两个中间分数BMCA（最低一致性准确率）和CI（一致性指数），基于模型一致性水平调整原始MCQA分数。

Result: 在不同基准测试和多种LLM上的评估表明，LLM即使获得高MCQA分数也可能表现出低回答一致性，而CoRA能成功降低不一致模型的分数。

Conclusion: CoRA指标能更可靠地评估LLM在多项选择题基准测试中的表现，通过考虑回答一致性来改进现有评分方法的局限性。

Abstract: In this work we present the Consistency-Rebalanced Accuracy (CoRA) metric, improving the reliability of Large Language Model (LLM) scores computed on multiple choice (MC) benchmarks. Our metric explores the response consistency of the LLMs, taking advantage of synthetically-generated questions with altered answer choices. With two intermediate scores, i.e. Bare-Minimum-Consistency Accuracy (BMCA) and Consistency Index (CI), CoRA is computed by adjusting the multiple-choice question answering (MCQA) scores to better reflect the level of consistency of the LLM. We present evaluations in different benchmarks using diverse LLMs, and not only demonstrate that LLMs can present low response consistency even when they present high MCQA scores, but also that CoRA can successfully scale down the scores of inconsistent models.

</details>


### [7] [Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information](https://arxiv.org/abs/2511.22176)
*Lukas Struppek,Dominik Hintersdorf,Hannah Struppek,Daniel Neider,Kristian Kersting*

Main category: cs.CL

TL;DR: F-CoT通过结构化输入分离信息提取与推理过程，减少2-3倍生成token同时保持与标准CoT相当的准确率


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理方法（如思维链）通常生成详细推理轨迹，导致token使用过多和推理延迟高。现有效率方法主要关注模型中心干预（如强化学习或监督微调），本文提出无需训练、输入中心的方法来提高效率。

Method: 受认知心理学启发，提出Focused Chain-of-Thought (F-CoT)，将信息提取与推理过程分离。首先从查询中提取关键信息组织成简洁的结构化上下文，然后引导模型仅基于该上下文进行推理，避免关注无关细节，自然产生更短的推理路径。

Result: 在算术文字题上，F-CoT将生成的token减少2-3倍，同时保持与标准零样本CoT相当的准确率。

Conclusion: 结构化输入是提高LLM推理效率的简单而有效的杠杆，无需模型训练即可显著减少token使用。

Abstract: Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.

</details>


### [8] [Language-conditioned world model improves policy generalization by reading environmental descriptions](https://arxiv.org/abs/2511.22904)
*Anh Nguyen,Stefan Lee*

Main category: cs.CL

TL;DR: 提出LED-WM方法，通过语言条件化的世界模型提升策略在未见游戏中的泛化能力，无需规划或专家演示


<details>
  <summary>Details</summary>
Motivation: 现有方法要么无法证明策略在未见游戏中的泛化能力，要么依赖限制性假设（如容忍推理时规划延迟或需要专家演示）。需要改进语言条件化世界模型的策略泛化能力，同时放弃这些假设。

Method: 基于DreamerV3构建语言感知编码器世界模型（LED-WM），使用注意力机制将语言描述显式地锚定到观察中的实体。通过与环境交互训练语言条件化世界模型，然后从该模型学习策略，无需规划或专家演示。

Result: 在MESSENGER和MESSENGER-WM两个环境中，LED-WM训练的策略相比其他基线方法，在由新动态和语言描述的未见游戏中表现出更好的泛化能力。还展示了策略可以通过世界模型生成的合成测试轨迹进行微调来改进。

Conclusion: LED-WM方法能够有效提升策略在未见游戏中的泛化能力，无需依赖规划或专家演示，为语言条件化世界模型的实际应用提供了可行方案。

Abstract: To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying "what to do". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.

</details>


### [9] [Behavior-Equivalent Token: Single-Token Replacement for Long Prompts in LLMs](https://arxiv.org/abs/2511.23271)
*Jiancheng Dong,Pengyue Jia,Jingyu Peng,Maolin Wang,Yuhao Wang,Lixin Su,Xin Sun,Shuaiqiang Wang,Dawei Yin,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 提出一种三阶段训练框架，将冗长的系统提示压缩为单个行为等效标记[BE]，实现3000倍长度压缩同时保持98%下游性能


<details>
  <summary>Details</summary>
Motivation: 长系统提示导致推理延迟增加、计算成本上升和有效上下文长度减少，需要寻找替代方案来保持行为效果的同时大幅减少标记数量

Method: 轻量级三阶段训练框架：1) 通过重构训练[BE]编码原始系统提示的自然语言内容；2) 将提示的下游行为蒸馏到该单个标记中；无需模型内部访问、辅助压缩模型或标注响应

Result: 在三个数据集上的实证评估显示，单个[BE]标记实现高达3000倍的提示长度减少，同时保留原始系统提示约98%的下游性能

Conclusion: 该方法显著降低推理成本，几乎将整个上下文窗口留给用户输入，为系统提示压缩提供有效解决方案

Abstract: Carefully engineered system prompts play a critical role in guiding the behavior of LLM agents, but their considerable length introduces significant drawbacks, including increased inference latency, higher computational cost, and reduced effective context length. This raises the question of whether such lengthy prompts can be replaced by a drastically reduced number of tokens while preserving their behavioral effect on downstream tasks. To enable this, we propose a lightweight three-stage training framework that learns a single prompt-specific Behavior-Equivalent token ([BE]). The framework first trains [BE] to encode the natural-language content of the original system prompt via reconstruction, and then distills the prompt 's downstream behavior into this single token. Importantly, our method requires no access to model internals, no auxiliary compression models, and no labeled responses. Empirical evaluations on three datasets show that a single [BE] token achieves up to a 3000x reduction in prompt length, while retaining about 98% of the downstream performance of the original system prompts. This substantially reduces inference cost and leaves almost the entire context window available for user inputs.

</details>


### [10] [MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)](https://arxiv.org/abs/2511.23281)
*Aaron Steiner,Ralph Peeters,Christian Bizer*

Main category: cs.CL

TL;DR: 比较四种网页交互接口（HTML、RAG、MCP、NLWeb）在LLM代理自动化网页任务中的性能，发现RAG、MCP和NLWeb在效果和效率上都优于传统HTML浏览


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理与网站交互有多种接口方式（HTML浏览、RAG、MCP、NLWeb），但缺乏在统一控制环境下对这些架构的系统性比较，需要填补这一研究空白

Method: 构建包含四个模拟电商网站的实验平台，每个网站提供HTML、MCP和NLWeb接口，为每种接口开发专门的代理执行相同任务集，使用GPT 4.1、GPT 5、GPT 5 mini和Claude Sonnet 4作为底层LLM进行评估

Result: RAG、MCP和NLWeb代理在效果和效率上都优于HTML代理：平均F1从0.67提升到0.75-0.77，token使用量从241k降至47k-140k，运行时间从291秒降至50-62秒。最佳配置是RAG+GPT 5（F1=0.87，完成率=0.79）

Conclusion: 交互接口的选择对基于LLM的网页代理的效果和效率有显著影响，RAG、MCP和NLWeb架构优于传统HTML浏览，其中RAG+GPT 5表现最佳，而RAG+GPT 5 mini在成本和性能间提供了良好平衡

Abstract: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks.
  To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.

</details>


### [11] [MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation](https://arxiv.org/abs/2511.23397)
*Mahdi Rahmani,AmirHossein Saffari,Reyhane Rahmani*

Main category: cs.CL

TL;DR: MegaChat是首个完全合成的波斯语问答数据集，用于评估Telegram电商中的智能销售聊天机器人。采用多智能体架构自动生成人物感知的问答对，相比传统RAG模型在4/5渠道中表现更优。


<details>
  <summary>Details</summary>
Motivation: 伊朗中小企业越来越多地使用Telegram进行销售，实时互动对转化至关重要。但为低资源语言（如波斯语）开发AI驱动的聊天机器人需要大规模高质量问答数据集，这通常成本高昂且资源密集。

Method: 提出新颖的自动化多智能体架构，从活跃的Telegram购物频道收集数据，生成人物感知的问答对。系统包含专门的问题生成、验证和优化智能体，确保生成真实多样的对话数据。评估时比较了三种经典RAG模型与先进的智能体系统（包含多查询检索、重排序和人物对齐响应合成）。

Result: 使用GPT-5.1在六个质量维度上进行评估，结果显示智能体架构在5个不同渠道中的4个上优于传统RAG模型，证明了其无需昂贵人工标注或复杂微调即可生成可扩展高质量数据集的能力。

Conclusion: MegaChat为中小企业提供了在专业商业领域构建智能客户互动系统的高效、经济解决方案，推动了低资源语言多语言对话AI的进步。

Abstract: Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs](https://arxiv.org/abs/2511.21928)
*Yifan Zhou,Sachin Grover,Mohamed El Mistiri,Kamalesh Kalirathnam,Pratyush Kerhalkar,Swaroop Mishra,Neelesh Kumar,Sanket Gaurav,Oya Aran,Heni Ben Amor*

Main category: cs.LG

TL;DR: ProPS是一种新颖的强化学习方法，将大型语言模型置于策略优化循环的核心，直接基于奖励反馈和自然语言输入提出策略更新，在15个Gymnasium任务中优于7种传统RL算法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习仅依赖标量奖励信号，无法利用现实任务中丰富的语义知识。人类学习则能有效结合数值反馈与语言、先验知识和常识，因此需要统一数值和语言推理的框架。

Method: 提出Prompted Policy Search (ProPS)方法，将大型语言模型置于策略优化循环的核心，直接基于奖励反馈和自然语言输入（如目标、领域知识、策略提示）提出策略更新，实现数值优化和语义推理的统一。

Result: 在15个Gymnasium任务（经典控制、Atari游戏、MuJoCo环境）中，ProPS在8个任务上优于所有7种基线算法（PPO、SAC、TRPO等），当提供领域知识时表现出显著优势。

Conclusion: 研究展示了统一语义和数值推理的潜力，能够实现更透明、可泛化和人类对齐的强化学习，LLM能够在上下文中执行数值优化，语义信号能带来更明智的探索和样本高效学习。

Abstract: Reinforcement Learning (RL) traditionally relies on scalar reward signals, limiting its ability to leverage the rich semantic knowledge often available in real-world tasks. In contrast, humans learn efficiently by combining numerical feedback with language, prior knowledge, and common sense. We introduce Prompted Policy Search (ProPS), a novel RL method that unifies numerical and linguistic reasoning within a single framework. Unlike prior work that augment existing RL components with language, ProPS places a large language model (LLM) at the center of the policy optimization loop-directly proposing policy updates based on both reward feedback and natural language input. We show that LLMs can perform numerical optimization in-context, and that incorporating semantic signals, such as goals, domain knowledge, and strategy hints can lead to more informed exploration and sample-efficient learning. ProPS is evaluated across fifteen Gymnasium tasks, spanning classic control, Atari games, and MuJoCo environments, and compared to seven widely-adopted RL algorithms (e.g., PPO, SAC, TRPO). It outperforms all baselines on eight out of fifteen tasks and demonstrates substantial gains when provided with domain knowledge. These results highlight the potential of unifying semantics and numerics for transparent, generalizable, and human-aligned RL.

</details>


### [13] [Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation](https://arxiv.org/abs/2511.21934)
*Tao Zhe,Huazhen Fang,Kunpeng Liu,Qian Lou,Tamzidul Hoque,Dongjie Wang*

Main category: cs.LG

TL;DR: 提出一种异构多智能体强化学习框架，用于自动化特征变换，通过共享批评器机制增强智能体间通信，使用多头注意力处理动态扩展特征空间，提高特征交叉效率。


<details>
  <summary>Details</summary>
Motivation: 传统自动化特征变换方法依赖启发式或穷举搜索，效率低下。现有强化学习方法存在两个问题：1) 特征变换过程中的动态特征扩展导致不稳定和学习复杂度增加；2) 智能体间协作不足导致特征交叉操作次优和模型性能下降。

Method: 提出异构多智能体强化学习框架，包含三种异构智能体（分为两类），分别负责选择特征和操作进行特征交叉。采用共享批评器机制促进智能体间通信，使用多头注意力特征智能体处理动态扩展特征空间，引入状态编码技术稳定强化学习过程。

Result: 通过大量实验验证了模型的有效性、效率、鲁棒性和可解释性，表明该方法能够实现更优的特征变换策略。

Conclusion: 提出的异构多智能体强化学习框架解决了自动化特征变换中的协作和动态特征空间问题，实现了更稳定、高效的特征变换过程。

Abstract: Feature transformation enhances downstream task performance by generating informative features through mathematical feature crossing. Despite the advancements in deep learning, feature transformation remains essential for structured data, where deep models often struggle to capture complex feature interactions. Prior literature on automated feature transformation has achieved success but often relies on heuristics or exhaustive searches, leading to inefficient and time-consuming processes. Recent works employ reinforcement learning (RL) to enhance traditional approaches through a more effective trial-and-error way. However, two limitations remain: 1) Dynamic feature expansion during the transformation process, which causes instability and increases the learning complexity for RL agents; 2) Insufficient cooperation and communication between agents, which results in suboptimal feature crossing operations and degraded model performance. To address them, we propose a novel heterogeneous multi-agent RL framework to enable cooperative and scalable feature transformation. The framework comprises three heterogeneous agents, grouped into two types, each designed to select essential features and operations for feature crossing. To enhance communication among these agents, we implement a shared critic mechanism that facilitates information exchange during feature transformation. To handle the dynamically expanding feature space, we tailor multi-head attention-based feature agents to select suitable features for feature crossing. Additionally, we introduce a state encoding technique during the optimization process to stabilize and enhance the learning dynamics of the RL agents, resulting in more robust and reliable transformation policies. Finally, we conduct extensive experiments to validate the effectiveness, efficiency, robustness, and interpretability of our model.

</details>


### [14] [Breaking Algorithmic Collusion in Human-AI Ecosystems](https://arxiv.org/abs/2511.21935)
*Natalie Collina,Eshwar Ram Arunachaleswaran,Meena Jagadeesan*

Main category: cs.LG

TL;DR: 研究混合人机生态系统中算法合谋的脆弱性，发现即使单个人类参与者也能破坏AI代理之间的价格合谋


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在生态系统中与人类频繁交互，需要理解人类参与如何影响AI代理之间的算法合谋行为，特别是在重复定价博弈中

Method: 采用重复定价博弈的理论框架，分析AI代理采用均衡策略而人类采用无遗憾策略时，混合生态系统的动态变化

Result: 单个人类参与者就能破坏AI代理的价格合谋，多个人类参与者会进一步将价格推向竞争水平；当AI代理意识到人类存在时，合谋性质会发生变化

Conclusion: 算法合谋在混合人机生态系统中具有脆弱性，人类参与能有效破坏价格合谋，这为理解算法合谋的持久性提供了理论依据

Abstract: AI agents are increasingly deployed in ecosystems where they repeatedly interact not only with each other but also with humans. In this work, we study these human-AI ecosystems from a theoretical perspective, focusing on the classical framework of repeated pricing games. In our stylized model, the AI agents play equilibrium strategies, and one or more humans manually perform the pricing task instead of adopting an AI agent, thereby defecting to a no-regret strategy. Motivated by how populations of AI agents can sustain supracompetitive prices, we investigate whether high prices persist under such defections. Our main finding is that even a single human defection can destabilize collusion and drive down prices, and multiple defections push prices even closer to competitive levels. We further show how the nature of collusion changes under defection-aware AI agents. Taken together, our results characterize when algorithmic collusion is fragile--and when it persists--in mixed ecosystems of AI agents and humans.

</details>


### [15] [A Safety and Security Framework for Real-World Agentic Systems](https://arxiv.org/abs/2511.21990)
*Shaona Ghosh,Barnaby Simkin,Kyriacos Shiarlis,Soumili Nandi,Dan Zhao,Matthew Fiedler,Julia Bazinska,Nikki Pope,Roopa Prabhu,Daniel Rohrer,Michael Demoret,Bartley Richardson*

Main category: cs.LG

TL;DR: 提出一个动态可操作的框架来保障企业级智能体AI系统的安全，通过统一传统安全与新型智能体风险，并利用辅助AI模型和红队测试进行风险发现与缓解。


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统的安全不仅是单个模型的固定属性，更是模型、编排器、工具和数据在操作环境中动态交互产生的涌现特性。传统LLM和孤立智能体模型的安全与安全有明确分离，但在智能体系统中它们相互关联，需要新的风险管理方法。

Method: 提出动态智能体安全框架，包括：1）定义统一的智能体风险分类法，涵盖传统安全问题和新型智能体特有风险；2）利用辅助AI模型和人工监督进行上下文风险管理；3）通过沙盒化的AI驱动红队测试进行风险发现；4）在NVIDIA AI-Q研究助手中进行案例研究验证。

Result: 框架在NVIDIA旗舰智能体研究助手AI-Q中成功应用，发现了新型智能体风险并进行上下文缓解。发布了包含超过10,000次真实攻击和防御执行轨迹的数据集，用于推进智能体安全研究。

Conclusion: 智能体系统的安全需要动态、上下文感知的方法，将传统安全与新型智能体特有风险统一管理。提出的框架通过辅助AI模型和红队测试有效识别和缓解风险，为企业级智能体部署提供了实用的安全保障方案。

Abstract: This paper introduces a dynamic and actionable framework for securing agentic AI systems in enterprise deployment. We contend that safety and security are not merely fixed attributes of individual models but also emergent properties arising from the dynamic interactions among models, orchestrators, tools, and data within their operating environments. We propose a new way of identification of novel agentic risks through the lens of user safety. Although, for traditional LLMs and agentic models in isolation, safety and security has a clear separation, through the lens of safety in agentic systems, they appear to be connected. Building on this foundation, we define an operational agentic risk taxonomy that unifies traditional safety and security concerns with novel, uniquely agentic risks, including tool misuse, cascading action chains, and unintended control amplification among others. At the core of our approach is a dynamic agentic safety and security framework that operationalizes contextual agentic risk management by using auxiliary AI models and agents, with human oversight, to assist in contextual risk discovery, evaluation, and mitigation. We further address one of the most challenging aspects of safety and security of agentic systems: risk discovery through sandboxed, AI-driven red teaming. We demonstrate the framework effectiveness through a detailed case study of NVIDIA flagship agentic research assistant, AI-Q Research Assistant, showcasing practical, end-to-end safety and security evaluations in complex, enterprise-grade agentic workflows. This risk discovery phase finds novel agentic risks that are then contextually mitigated. We also release the dataset from our case study, containing traces of over 10,000 realistic attack and defense executions of the agentic workflow to help advance research in agentic safety.

</details>


### [16] [Representative Action Selection for Large Action Space: From Bandits to MDPs](https://arxiv.org/abs/2511.22104)
*Quan Zhou,Shie Mannor*

Main category: cs.LG

TL;DR: 该研究提出了一种从大规模动作空间中选择代表性子集的方法，使得在强化学习家族环境中都能找到近似最优动作，从而解决大规模组合决策问题。


<details>
  <summary>Details</summary>
Motivation: 在库存管理和推荐系统等应用中，动作空间极大，直接在整个空间上学习是不可行的。需要找到一种方法，能够从大规模动作空间中选择一个小的代表性子集，使得在每个环境中都能包含近似最优动作。

Method: 将元多臂赌博机（meta-bandits）的方法扩展到更一般的马尔可夫决策过程（MDPs）设置。在放松的非中心化次高斯过程模型下，证明现有算法能达到与使用完整动作空间相当的性能。

Result: 理论证明表明，该方法在放松的非中心化次高斯过程模型下，能够实现与使用完整动作空间相当的性能，为大规模组合决策提供了计算和样本高效的解决方案。

Conclusion: 该方法为大规模组合决策问题提供了有效的解决方案，能够在环境异质性更强的情况下，通过选择代表性动作子集实现高效学习，避免了完整动作空间的评估负担。

Abstract: We study the problem of selecting a small, representative action subset from an extremely large action space shared across a family of reinforcement learning (RL) environments -- a fundamental challenge in applications like inventory management and recommendation systems, where direct learning over the entire space is intractable. Our goal is to identify a fixed subset of actions that, for every environment in the family, contains a near-optimal action, thereby enabling efficient learning without exhaustively evaluating all actions.
  This work extends our prior results for meta-bandits to the more general setting of Markov Decision Processes (MDPs). We prove that our existing algorithm achieves performance comparable to using the full action space. This theoretical guarantee is established under a relaxed, non-centered sub-Gaussian process model, which accommodates greater environmental heterogeneity. Consequently, our approach provides a computationally and sample-efficient solution for large-scale combinatorial decision-making under uncertainty.

</details>


### [17] [Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning](https://arxiv.org/abs/2511.22105)
*Saad Masrur,Ismail Guvenc,David Lopez Perez*

Main category: cs.LG

TL;DR: 提出基于多智能体深度强化学习（MARL-DDQN）的动态睡眠模式优化框架，用于毫米波网络中最大化能效并满足服务质量约束。


<details>
  <summary>Details</summary>
Motivation: 现有毫米波网络中的睡眠模式优化方法依赖静态基站流量模型，无法捕捉非平稳流量动态，且状态-动作空间过大，限制了实际部署。需要解决动态环境下的能效优化问题。

Method: 提出MARL-DDQN框架，采用双深度Q网络进行多智能体强化学习，在3D城市环境中结合时变社区化用户设备移动模型，集成真实的基站功耗模型和波束成形技术，实现分布式决策。

Result: MARL-DDQN在动态场景下优于现有方法（All On、IT-QoS-LB、MARL-DDPG、MARL-PPO），达到0.60 Mbit/Joule能效、8.5 Mbps第10百分位吞吐量，95%时间满足QoS约束。

Conclusion: MARL-DDQN框架能有效解决毫米波网络中动态睡眠模式优化问题，在保证服务质量的同时显著提升能效，具有实际部署潜力。

Abstract: Dynamic sleep mode optimization (SMO) in millimeter-wave (mmWave) networks is essential for maximizing energy efficiency (EE) under stringent quality-of-service (QoS) constraints. However, existing optimization and reinforcement learning (RL) approaches rely on aggregated, static base station (BS) traffic models that fail to capture non-stationary traffic dynamics and suffer from large state-action spaces, limiting real-world deployment. To address these challenges, this paper proposes a multi-agent deep reinforcement learning (MARL) framework using a Double Deep Q-Network (DDQN), referred to as MARL-DDQN, for adaptive SMO in a 3D urban environment with a time-varying and community-based user equipment (UE) mobility model. Unlike conventional single-agent RL, MARL-DDQN enables scalable, distributed decision-making with minimal signaling overhead. A realistic BS power consumption model and beamforming are integrated to accurately quantify EE, while QoS is defined in terms of throughput. The method adapts SMO policies to maximize EE while mitigating inter-cell interference and ensuring throughput fairness. Simulations show that MARL-DDQN outperforms state-of-the-art strategies, including All On, iterative QoS-aware load-based (IT-QoS-LB), MARL-DDPG, and MARL-PPO, achieving up to 0.60 Mbit/Joule EE, 8.5 Mbps 10th-percentile throughput, and meeting QoS constraints 95% of the time under dynamic scenarios.

</details>


### [18] [Benchmarking In-context Experiential Learning Through Repeated Product Recommendations](https://arxiv.org/abs/2511.22130)
*Gilbert Yang,Yaqin Chen,Thomson Yen,Hongseok Namkoong*

Main category: cs.LG

TL;DR: BELA基准测试评估智能体在动态环境中的经验学习能力，当前前沿模型在推荐场景中难以通过经验有效改进


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注确定性任务，缺乏对智能体通过经验适应和推理能力的衡量。真实世界环境需要智能体处理不完全知识并基于经验调整行为。

Method: 构建BELA基准：结合(1)亚马逊真实产品数据，(2)多样化用户画像表示潜在偏好，(3)基于画像的LLM用户模拟器创建交互轨迹，评估智能体在推荐对话中的经验学习能力。

Result: 当前前沿模型难以在多个对话回合中实现有意义的改进，表明需要具备强大上下文学习能力的智能体系统。

Conclusion: 需要开发能够通过经验学习和主动探索适应动态环境的智能体系统，BELA基准为此提供了评估框架。

Abstract: To reliably navigate ever-shifting real-world environments, agents must grapple with incomplete knowledge and adapt their behavior through experience. However, current evaluations largely focus on tasks that leave no ambiguity, and do not measure agents' ability to adaptively learn and reason through the experiences they accrued. We exemplify the need for this in-context experiential learning in a product recommendation context, where agents must navigate shifting customer preferences and product landscapes through natural language dialogue. We curate a benchmark for experiential learning and active exploration (BELA) that combines (1) rich real-world products from Amazon, (2) a diverse collection of user personas to represent heterogeneous yet latent preferences, and (3) a LLM user simulator powered by the persona to create rich interactive trajectories. We observe that current frontier models struggle to meaningfully improve across episodes, underscoring the need for agentic systems with strong in-context learning capabilities.

</details>


### [19] [TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices](https://arxiv.org/abs/2511.22138)
*Mohd Ariful Haque,Fahad Rahman,Kishor Datta Gupta,Khalil Shujaee,Roy George*

Main category: cs.LG

TL;DR: 该论文研究了小型语言模型在边缘设备上执行代理任务（函数/工具/API调用）的有效性，通过多种优化策略（SFT、PEFT、RL、DPO、混合方法）在BFCL基准上评估，发现中等规模模型（1-3B参数）显著优于超紧凑模型，最高达到65.74%整体准确率。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索小型语言模型在边缘设备上执行代理任务的可能性，以实现无需依赖云基础设施的隐私保护、低延迟自主代理系统，解决云端依赖带来的隐私和延迟问题。

Method: 使用伯克利函数调用排行榜框架评估多个SLM模型，采用参数驱动的优化策略包括：监督微调、参数高效微调、基于强化学习的优化、通过直接偏好优化的偏好对齐以及混合方法。构建了从AgentBank数据转换的DPO训练管道。

Result: 结果显示模型规模存在明显准确率差异：中等规模模型（1-3B参数）显著优于超紧凑模型（<1B参数），最高达到65.74%整体准确率和55.62%多轮对话准确率。混合优化策略表现最佳。

Conclusion: 研究表明混合优化策略能使小型语言模型在边缘设备上提供准确、高效、稳定的代理AI，使隐私保护、低延迟的自主代理在云端之外变得实用可行。

Abstract: This paper investigates the effectiveness of small language models (SLMs) for agentic tasks (function/tool/API calling) with a focus on running agents on edge devices without reliance on cloud infrastructure. We evaluate SLMs using the Berkeley Function Calling Leaderboard (BFCL) framework and describe parameter-driven optimization strategies that include supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), reinforcement learning (RL)-based optimization, preference alignment via Direct Preference Optimization (DPO), and hybrid methods. We report results for models including TinyAgent, TinyLlama, Qwen, and xLAM across BFCL categories (simple, multiple, parallel, parallel-multiple, and relevance detection), both in live and non-live settings, and in multi-turn evaluations. We additionally detail a DPO training pipeline constructed from AgentBank data (e.g., ALFRED), including our conversion of SFT data to chosen-rejected pairs using TinyLlama responses as rejected outputs and manual validation. Our results demonstrate clear accuracy differences across model scales where medium-sized models (1-3B parameters) significantly outperform ultra-compact models (<1B parameters), achieving up to 65.74% overall accuracy, and 55.62% multi-turn accuracy with hybrid optimization. This study highlights the importance of hybrid optimization strategies that enable small language models to deliver accurate, efficient, and stable agentic AI on edge devices, making privacy-preserving, low-latency autonomous agents practical beyond the cloud.

</details>


### [20] [BiCQL-ML: A Bi-Level Conservative Q-Learning Framework for Maximum Likelihood Inverse Reinforcement Learning](https://arxiv.org/abs/2511.22210)
*Junsung Park*

Main category: cs.LG

TL;DR: BiCQL-ML是一种无策略的离线逆强化学习算法，通过双层框架联合优化奖励函数和保守Q函数，无需显式策略学习，在标准离线RL基准测试中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 离线逆强化学习仅使用固定演示数据恢复解释专家行为的奖励函数，无需在线交互。现有方法通常需要显式策略学习，BiCQL-ML旨在避免这一需求，同时提高奖励恢复和下游策略性能。

Method: 采用双层框架：1) 在当前奖励下通过保守Q学习(CQL)学习保守Q函数；2) 更新奖励参数以最大化专家动作的期望Q值，同时抑制对分布外动作的过度泛化。该方法可视为软值匹配原则下的最大似然估计。

Result: 理论保证BiCQL-ML收敛到使专家策略软最优的奖励函数。在标准离线RL基准测试中，相比现有离线IRL基线，BiCQL-ML在奖励恢复和下游策略性能方面均有提升。

Conclusion: BiCQL-ML通过无策略的双层优化框架有效解决了离线逆强化学习问题，避免了显式策略学习，在理论和实证上都表现出优越性能。

Abstract: Offline inverse reinforcement learning (IRL) aims to recover a reward function that explains expert behavior using only fixed demonstration data, without any additional online interaction. We propose BiCQL-ML, a policy-free offline IRL algorithm that jointly optimizes a reward function and a conservative Q-function in a bi-level framework, thereby avoiding explicit policy learning. The method alternates between (i) learning a conservative Q-function via Conservative Q-Learning (CQL) under the current reward, and (ii) updating the reward parameters to maximize the expected Q-values of expert actions while suppressing over-generalization to out-of-distribution actions. This procedure can be viewed as maximum likelihood estimation under a soft value matching principle. We provide theoretical guarantees that BiCQL-ML converges to a reward function under which the expert policy is soft-optimal. Empirically, we show on standard offline RL benchmarks that BiCQL-ML improves both reward recovery and downstream policy performance compared to existing offline IRL baselines.

</details>


### [21] [TreeCoder: Systematic Exploration and Optimisation of Decoding and Constraints for LLM Code Generation](https://arxiv.org/abs/2511.22277)
*Henrijs Princis,Arindam Sharma,Cristina David*

Main category: cs.LG

TL;DR: TreeCoder是一个用于代码生成的通用解码框架，通过树搜索和约束函数确保代码正确性，而非依赖提示工程


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码生成方面表现出色，但仅通过自然语言提示生成的代码经常违反语法或语义约束，需要更系统的方法来确保代码正确性

Method: TreeCoder将解码过程表示为候选程序的树搜索，将解码策略和约束函数（如风格、语法、执行）作为可优化的组件，支持系统探索和自动调优

Result: 在MBPP（Python）和SQL-Spider基准测试中，TreeCoder显著提高了CodeLlama、Mistral和DeepSeek等开源模型的准确性，通常以较大优势超越无约束基线

Conclusion: TreeCoder提供了一个灵活且通用的框架，通过在解码过程中强制执行正确性和结构约束，有效提高了LLM代码生成的质量和可靠性

Abstract: Large language models (LLMs) have shown remarkable ability to generate code, yet their outputs often violate syntactic or semantic constraints when guided only through natural language prompts. We introduce TreeCoder, the most general and flexible framework to date for exploring decoding strategies, constraints, and hyperparameters in LLMs, and use it in code generation to enforce correctness and structure during decoding rather than relying on prompt engineering. TreeCoder represents decoding as a tree search over candidate programs, where both decoding strategies and constraint functions - such as style, syntax, execution - are treated as first-class, optimisable components. This design enables systematic exploration and automatic tuning of decoding configurations using standard optimisation techniques. Experiments on the MBPP (Python) and SQL-Spider benchmarks show that TreeCoder consistently improves accuracy across open-source models such as CodeLlama, Mistral and DeepSeek, often outperforming their unconstrained baselines by considerable margins.

</details>


### [22] [Improving Stochastic Action-Constrained Reinforcement Learning via Truncated Distributions](https://arxiv.org/abs/2511.22406)
*Roland Stolz,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: 提出高效数值近似方法解决动作约束强化学习中截断正态分布熵、对数概率等关键特征计算难题，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有动作约束RL方法在处理复杂约束时，截断正态分布的关键特征（熵、对数概率及其梯度）计算不可行，现有近似方法严重降低性能

Method: 提出高效数值近似方法计算截断正态分布的熵、对数概率及其梯度，并提供截断策略分布的高效采样策略

Result: 在三个基准环境中验证，使用准确估计的方法相比现有近似方法获得显著性能提升

Conclusion: 在动作约束RL中准确估计截断分布的关键特征至关重要，提出的高效数值近似方法能有效解决计算难题并提升性能

Abstract: In reinforcement learning (RL), it is often advantageous to consider additional constraints on the action space to ensure safety or action relevance. Existing work on such action-constrained RL faces challenges regarding effective policy updates, computational efficiency, and predictable runtime. Recent work proposes to use truncated normal distributions for stochastic policy gradient methods. However, the computation of key characteristics, such as the entropy, log-probability, and their gradients, becomes intractable under complex constraints. Hence, prior work approximates these using the non-truncated distributions, which severely degrades performance. We argue that accurate estimation of these characteristics is crucial in the action-constrained RL setting, and propose efficient numerical approximations for them. We also provide an efficient sampling strategy for truncated policy distributions and validate our approach on three benchmark environments, which demonstrate significant performance improvements when using accurate estimations.

</details>


### [23] [LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system](https://arxiv.org/abs/2511.22598)
*Huanyu Li,Zongyuan Li,Wei Huang,Xian Guo*

Main category: cs.LG

TL;DR: LLM-Cave是一个轻量级基准测试环境，用于评估大语言模型的序列推理和决策能力，相比现有复杂环境更高效，实验显示结构化多步推理能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估基准主要限于单步交互，而现有的序列决策环境（如TextStarCraftII和LLM-PySC2）过于复杂，需要数小时才能完成一次游戏，缺乏轻量高效的评估环境。

Method: 提出了LLM-Cave基准测试和轻量环境，这是一个符号主义时代的经典实例，智能体通过部分可观测状态信息推理附近危险来探索环境并避免潜在损失。评估了GPT-4o-mini、o1-mini、DeepSeek-R1等主流LLM的序列推理能力、决策性能和计算效率。

Result: Deepseek-R1在复杂推理任务上取得了最高成功率，而较小的模型如4o-mini通过采用Chain of Speculation和Planner-Critic策略，在挑战任务上显著缩小了性能差距，但牺牲了计算效率。

Conclusion: 结构化多步推理结合基于LLM的反馈机制能显著增强LLM的决策能力，为改进较弱模型的推理能力提供了有前景的方向，并建议了新的以推理为中心的LLM评估基准。

Abstract: Large language models (LLMs) such as ChatGPT o1, ChatGPT o3, and DeepSeek R1 have shown great potential in solving difficult problems. However, current LLM evaluation benchmarks are limited to one-step interactions. Some of the existing sequence decision-making environments, such as TextStarCraftII and LLM-PySC2, are too complicated and require hours of interaction to complete a game. In this paper, we introduce LLM-Cave, a benchmark and light environment for LLM reasoning and decision-making systems. This environment is a classic instance in the era of Symbolism. Artificial intelligence enables the agent to explore the environment and avoid potential losses by reasoning about nearby dangers using partial observable state information. In the experiment, we evaluated the sequential reasoning ability, decision-making performance and computational efficiency of mainstream large language models (LLMs) such as GPT-4o-mini, o1-mini, and DeepSeek-R1. Experiments show that while Deepseek-R1 achieved the highest success rate on complex reasoning tasks, smaller models like 4o-mini significantly narrowed the performance gap on challenges by employing Chain of Speculation and Planner-Critic strategies, at the expense of reduced computational efficiency. This indicates that structured, multi-step reasoning combined with an LLM-based feedback mechanism can substantially enhance an LLM's decision-making capabilities, providing a promising direction for improving reasoning in weaker models and suggesting a new reasoning-centered benchmark for LLM assessment. Our code is open-sourced in https://github.com/puleya1277/CaveEnv.

</details>


### [24] [Automated Design Optimization via Strategic Search with Large Language Models](https://arxiv.org/abs/2511.22651)
*Anthony Carreon,Vansh Sharma,Venkat Raman*

Main category: cs.LG

TL;DR: AUTO是一个基于LLM的设计优化框架，将设计优化视为无梯度搜索问题，通过策略性LLM推理指导，在GPU代码优化中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在明确定义的搜索空间中表现良好，但在设计参数难以定义的设计问题中表现不佳。LLM通过动态解释设计空间和利用编码的领域知识，为这类问题提供了有前景的替代方案。

Method: AUTO是一个LLM代理框架，采用两个协作代理：策略师（Strategist）在探索和利用策略之间进行选择，实施者（Implementor）执行详细设计。该框架将设计优化视为由战略性LLM推理指导的无梯度搜索问题。

Result: 在GPU代码优化领域，AUTO生成的解决方案与专家实现相当，在化学动力学积分和稠密矩阵乘法方面表现优异。相对于贝叶斯优化方法，实现了50-70%的搜索效率。优化过程约需8小时，每次运行成本估计最高159美元，而中等工资软件开发者估计成本最高480美元。

Conclusion: 这些发现为在有限先验信息的情况下，自动化处理定义不明确的搜索空间中的设计优化问题打开了大门。

Abstract: Traditional optimization methods excel in well-defined search spaces but struggle with design problems where transformations and design parameters are difficult to define. Large language models (LLMs) offer a promising alternative by dynamically interpreting design spaces and leveraging encoded domain knowledge. To this end, we introduce AUTO, an LLM agent framework that treats design optimization as a gradient-free search problem guided by strategic LLM reasoning. The framework employs two collaborative agents: a Strategist that selects between exploration and exploitation strategies, and an Implementor that executes detailed designs. Applied to GPU code optimization -- a domain critical to fields from machine learning to scientific computing -- AUTO generates solutions competitive with expert implementations for chemical kinetics integration and dense matrix multiplication. The framework achieves 50-70% search efficiency relative to Bayesian optimization methodologies. It completes optimizations in approximately 8 hours at an estimated cost of up to \$159 per run, compared to an estimated cost of up to \$480 with median-wage software developers. These findings open the door to automating design optimization in ill-defined search spaces with limited prior information.

</details>


### [25] [VeriDispatcher: Multi-Model Dispatching through Pre-Inference Difficulty Prediction for RTL Generation Optimization](https://arxiv.org/abs/2511.22749)
*Zeng Wang,Weihua Xiao,Minghao Shao,Raghu Vamshi Hemadri,Ozgur Sinanoglu,Muhammad Shafique,Ramesh Karri*

Main category: cs.LG

TL;DR: VeriDispatcher是一个多LLM RTL生成框架，通过预推理难度预测将任务分派给合适的LLM，在保持准确性的同时显著降低商业API调用成本。


<details>
  <summary>Details</summary>
Motivation: 不同LLM在RTL生成任务上各有优势，但现有工作主要关注单个模型的提示或微调。如何协调多个不同LLM以共同提高RTL质量同时降低成本，而不是运行所有模型并选择最佳输出，这一问题尚未得到充分研究。

Method: 提出VeriDispatcher框架：1) 基于任务描述的语义嵌入训练紧凑分类器，使用结合语法、结构相似性和功能正确性的难度评分；2) 在推理时使用这些预测器将任务路由到选定的LLM子集。

Result: 在RTLLM和VerilogEval基准测试中，VeriDispatcher在10个不同LLM上实现了：RTLLM上准确率提升18%且仅使用40%的商业调用；VerilogEval上保持准确率同时减少25%的商业使用。

Conclusion: VeriDispatcher能够在硬件设计自动化中实现成本效益高、高质量的LLM部署，通过智能任务分派充分利用不同LLM的优势。

Abstract: Large Language Models (LLMs) show strong performance in RTL generation, but different models excel on different tasks because of architecture and training differences. Prior work mainly prompts or finetunes a single model. What remains not well studied is how to coordinate multiple different LLMs so they jointly improve RTL quality while also reducing cost, instead of running all models and choosing the best output. We define this as the multi-LLM RTL generation problem. We propose VeriDispatcher, a multi-LLM RTL generation framework that dispatches each RTL task to suitable LLMs based on pre-inference difficulty prediction. For each model, we train a compact classifier over semantic embeddings of task descriptions, using difficulty scores derived from benchmark variants that combine syntax, structural similarity, and functional correctness. At inference, VeriDispatcher uses these predictors to route tasks to a selected subset of LLMs. Across 10 diverse LLMs on RTLLM and VerilogEval, VeriDispatcher achieves up to 18% accuracy improvement on RTLLM using only 40% of commercial calls, and on VerilogEval maintains accuracy while reducing commercial usage by 25%, enabling cost-effective, high-quality LLM deployment in hardware design automation.

</details>


### [26] [Exact Learning of Arithmetic with Differentiable Agents](https://arxiv.org/abs/2511.22751)
*Hristo Papazov,Francesco D'Angelo,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 提出一种可微有限状态转换器（DFST）框架，通过梯度方法实现精确算法学习，在算术任务上实现强长度泛化


<details>
  <summary>Details</summary>
Motivation: 探索使用梯度方法进行精确算法学习的可能性，解决现有架构在长度泛化方面的不足

Method: 引入可微有限状态转换器（DFST），这是一种图灵完备的模型家族，支持恒定精度、恒定时间生成和端到端对数并行可微训练，利用专家智能体的策略轨迹观察进行训练

Result: 在二进制和十进制加减乘除任务上，模型在极小数据集上训练后，能够无错误地泛化到比训练样本长数千倍的输入

Conclusion: 在结构化中间监督下训练可微智能体，可能为基于梯度的精确算法技能学习开辟新途径

Abstract: We explore the possibility of exact algorithmic learning with gradient-based methods and introduce a differentiable framework capable of strong length generalization on arithmetic tasks. Our approach centers on Differentiable Finite-State Transducers (DFSTs), a Turing-complete model family that avoids the pitfalls of prior architectures by enabling constant-precision, constant-time generation, and end-to-end log-parallel differentiable training. Leveraging policy-trajectory observations from expert agents, we train DFSTs to perform binary and decimal addition and multiplication. Remarkably, models trained on tiny datasets generalize without error to inputs thousands of times longer than the training examples. These results show that training differentiable agents on structured intermediate supervision could pave the way towards exact gradient-based learning of algorithmic skills. Code available at \href{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}.

</details>


### [27] [ThetaEvolve: Test-time Learning on Open Problems](https://arxiv.org/abs/2511.23473)
*Yiping Wang,Shao-Rong Su,Zhiyuan Zeng,Eva Xu,Liliang Ren,Xinyu Yang,Zeyi Huang,Xuehai He,Luyao Ma,Baolin Peng,Hao Cheng,Pengcheng He,Weizhu Chen,Shuohang Wang,Simon Shaolei Du,Yelong Shen*

Main category: cs.LG

TL;DR: ThetaEvolve是一个开源框架，简化并扩展了AlphaEvolve，通过上下文学习和强化学习在测试时持续学习，使小型开源模型能在开放优化问题上取得新的最佳边界。


<details>
  <summary>Details</summary>
Motivation: AlphaEvolve虽然取得了数学发现突破，但它是闭源系统，依赖前沿LLM集成，且是纯推理系统无法内化演化策略。需要开源框架让小型模型也能在开放问题上取得突破。

Method: ThetaEvolve采用单一LLM、大型程序数据库增强探索、批量采样提高吞吐量、惰性惩罚避免停滞输出、可选奖励塑形提供稳定训练信号，支持测试时的上下文学习和强化学习。

Result: ThetaEvolve首次使小型开源模型（如DeepSeek-R1-0528-Qwen3-8B）在AlphaEvolve提到的开放问题（圆堆积和自相关不等式）上取得了新的最佳边界。在4个开放任务和2个模型上，测试时RL持续优于纯推理基线。

Conclusion: ThetaEvolve证明了小型开源模型通过持续学习演化策略，能在开放优化问题上取得突破，RL训练的检查点在目标任务和未见任务上都表现更好，展示了模型真正学到了演化能力。

Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve

</details>


### [28] [ASTRO: Adaptive Stitching via Dynamics-Guided Trajectory Rollouts](https://arxiv.org/abs/2511.23442)
*Hang Yu,Di Zhang,Qiwei Du,Yanping Zhao,Hai Zhang,Guang Chen,Eduardo E. Veas,Junqiao Zhao*

Main category: cs.LG

TL;DR: ASTRO是一个离线强化学习数据增强框架，通过生成分布新颖且符合动态约束的轨迹来解决子优和碎片化数据集中的奖励传播问题，显著提升策略学习效果。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习从预收集数据集中学习最优策略，但子优和碎片化轨迹导致奖励传播困难，造成价值估计不准确和策略性能下降。现有基于生成模型的轨迹拼接方法要么局限于行为策略支持集，要么违反底层动态约束，限制了策略改进效果。

Method: ASTRO首先学习时间距离表示来识别不同且可达的拼接目标，然后采用动态引导的拼接规划器，通过Rollout Deviation Feedback（目标状态序列与实际到达状态序列之间的差距）自适应生成连接动作序列，提高轨迹拼接的可行性和可达性。

Result: ASTRO在OGBench套件上显著优于现有离线RL增强方法，在D4RL等标准离线RL基准测试中也表现出持续改进，实现了显著的性能提升。

Conclusion: ASTRO通过生成分布新颖且符合动态约束的轨迹，有效解决了离线强化学习中子优和碎片化数据集带来的挑战，为策略学习提供了高质量的数据增强。

Abstract: Offline reinforcement learning (RL) enables agents to learn optimal policies from pre-collected datasets. However, datasets containing suboptimal and fragmented trajectories present challenges for reward propagation, resulting in inaccurate value estimation and degraded policy performance. While trajectory stitching via generative models offers a promising solution, existing augmentation methods frequently produce trajectories that are either confined to the support of the behavior policy or violate the underlying dynamics, thereby limiting their effectiveness for policy improvement. We propose ASTRO, a data augmentation framework that generates distributionally novel and dynamics-consistent trajectories for offline RL. ASTRO first learns a temporal-distance representation to identify distinct and reachable stitch targets. We then employ a dynamics-guided stitch planner that adaptively generates connecting action sequences via Rollout Deviation Feedback, defined as the gap between target state sequence and the actual arrived state sequence by executing predicted actions, to improve trajectory stitching's feasibility and reachability. This approach facilitates effective augmentation through stitching and ultimately enhances policy learning. ASTRO outperforms prior offline RL augmentation methods across various algorithms, achieving notable performance gain on the challenging OGBench suite and demonstrating consistent improvements on standard offline RL benchmarks such as D4RL.

</details>


### [29] [Emergent Coordination and Phase Structure in Independent Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.23315)
*Azusa Yamaguchi*

Main category: cs.LG

TL;DR: 该研究通过大规模实验揭示了去中心化多智能体强化学习中的三阶段结构：协调稳定相、脆弱过渡区和阻塞无序相，由核漂移和同步的相互作用驱动。


<details>
  <summary>Details</summary>
Motivation: 需要更清晰地理解去中心化多智能体强化学习中协调何时出现、波动或崩溃，以表征多智能体学习系统的动态特性。

Method: 使用完全独立Q学习（IQL）作为最小去中心化测试平台，在环境大小L和智能体密度ρ上进行大规模实验，构建基于合作成功率（CSR）和TD误差方差稳定性指数的相图。

Result: 发现了三个不同区域：协调稳定相、脆弱过渡区和阻塞无序相，由尖锐的双重不稳定岭分隔。核漂移（由其他智能体策略更新引起的有效转移核时变偏移）是关键机制，而移除智能体标识符会完全消除漂移并破坏三阶段结构。

Conclusion: 去中心化MARL表现出由规模、密度和核漂移相互作用控制的相干相结构，表明涌现的协调行为是一种分布交互驱动的相现象。

Abstract: A clearer understanding of when coordination emerges, fluctuates, or collapses in decentralized multi-agent reinforcement learning (MARL) is increasingly sought in order to characterize the dynamics of multi-agent learning systems. We revisit fully independent Q-learning (IQL) as a minimal decentralized testbed and run large-scale experiments across environment size L and agent density rho. We construct a phase map using two axes - the cooperative success rate (CSR) and a stability index derived from TD-error variance - revealing three distinct regimes: a coordinated and stable phase, a fragile transition region, and a jammed or disordered phase. A sharp double Instability Ridge separates these regimes and corresponds to persistent kernel drift, the time-varying shift of each agent's effective transition kernel induced by others' policy updates. Synchronization analysis further shows that temporal alignment is required for sustained cooperation, and that competition between drift and synchronization generates the fragile regime. Removing agent identifiers eliminates drift entirely and collapses the three-phase structure, demonstrating that small inter-agent asymmetries are a necessary driver of drift. Overall, the results show that decentralized MARL exhibits a coherent phase structure governed by the interaction between scale, density, and kernel drift, suggesting that emergent coordination behaves as a distribution-interaction-driven phase phenomenon.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [30] [Gemini CLI Tips & Tricks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Faddyosmani%2Fgemini-cli-tips%3Futm_source=tldrai/1/0100019ac5ae0e91-6e88b5ab-bd26-49ec-879a-1bb149da6a01-000000/_2UaRK1CA-f2J2SDECC_Y-9tpgGaKGLD52Oez7dJVoc=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gemini CLI是一个将Google Gemini AI集成到终端的对话式代理命令行工具，可作为超级增强的结对编程和命令行助手


<details>
  <summary>Details</summary>
Motivation: 将先进的AI助手直接集成到开发者的工作流中，特别是终端环境，以提升编程、调试和系统自动化任务的效率

Method: 构建一个基于Google Gemini的对话式代理命令行工具，能够推理请求、选择工具并执行多步骤计划

Result: 开发出功能强大的AI助手，擅长编码任务、调试、内容生成和系统自动化，为开发者提供终端内的智能辅助

Conclusion: Gemini CLI成功将Google Gemini AI能力带入终端环境，创建了一个有效的代理式命令行工具，能够显著提升开发者的工作效率

Abstract: Gemini CLI Tips & Tricks (GitHub Repo) Gemini CLI is an AI assistant that brings Google Gemini directly into the terminal. It functions as a conversational agentic command-line tool. Gemini CLI can reason about requests, choose tools, and execute multi-step plans. It acts like a supercharged pair programmer and command-line assistant. The assistant excels at coding tasks, debugging, content generation, and system automation.

</details>


### [31] [Andrew Ng releases AI “Agentic Reviewer” for research papers](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FBz22n2/1/0100019ac5ae0e91-6e88b5ab-bd26-49ec-879a-1bb149da6a01-000000/JudPhHKO3LEaxsRPNOqm5CS4whO9xEc5fttLX9tjEEY=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Andrew Ng发布基于ICLR 2025审稿训练的AI代理审稿系统，AI-人类审稿相关性略高于人类-人类审稿相关性


<details>
  <summary>Details</summary>
Motivation: 探索AI代理在学术论文审稿中的应用，提高审稿效率和一致性，减少人工审稿负担

Method: 基于ICLR 2025审稿数据训练AI代理审稿系统，构建"Agentic Reviewer"模型

Result: AI-人类审稿相关性略高于人类-人类审稿相关性，显示AI在审稿任务中的潜力

Conclusion: AI代理审稿系统在学术审稿中具有应用价值，可作为辅助工具提高审稿质量

Abstract: Andrew Ng releases AI “Agentic Reviewer” for research papers (2 minute read) Andrew Ng introduced an agentic paper-review system trained on ICLR 2025 reviews, showing AI-human review correlation slightly higher than human-human.

</details>


### [32] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019ac5ae0e91-6e88b5ab-bd26-49ec-879a-1bb149da6a01-000000/i6X2xnz8cvIFNdudEBLurQs7OZrK1If8_nZOn5p1uP0=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Andrew Ng发布基于ICLR 2025评审训练的AI代理评审系统，其AI-人类评审相关性略高于人类-人类评审相关性


<details>
  <summary>Details</summary>
Motivation: 探索AI代理在学术论文评审中的应用，验证AI评审系统能否达到甚至超越人类评审的质量和一致性

Method: 基于ICLR 2025的评审数据训练AI代理评审系统，通过对比AI-人类评审相关性和人类-人类评审相关性来评估系统性能

Result: AI-人类评审相关性略高于人类-人类评审相关性，表明AI代理评审系统具有实用价值

Conclusion: AI代理评审系统在学术论文评审中表现出色，能够提供与人类评审相当甚至更一致的评审意见

Abstract: Andrew Ng releases AI “Agentic Reviewer” for research papers (2 minute read) Andrew Ng introduced an agentic paper-review system trained on ICLR 2025 reviews, showing AI-human review correlation slightly higher than human-human.

</details>


### [33] [How to monitor AI agent applications on Amazon Bedrock AgentCore with Grafana Cloud](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgrafana.com%2Fblog%2F2025%2F11%2F26%2Fhow-to-monitor-ai-agent-applications-on-amazon-bedrock-agentcore-with-grafana-cloud%2F%3Futm_source=tldrdevops/1/0100019aca5c062a-ac1b5cb7-6dc6-4727-bcbe-05dffe833082-000000/utTSIiL_Pr1ztVrYgFmqb8vewiEbRd8ly6TZalUccAQ=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文介绍如何在Amazon Bedrock AgentCore上部署AI代理，并使用OpenTelemetry和Grafana Cloud实现可观测性监控


<details>
  <summary>Details</summary>
Motivation: 随着AI代理应用的普及，需要有效的监控解决方案来确保生产环境的稳定性、调试问题和优化成本

Method: 使用OpenTelemetry进行自动仪表化，通过OpenLit为AI框架提供自动检测，并利用Grafana Cloud的AI可观测性仪表板进行监控

Result: 实现了对AI代理性能的监控、使用分布式追踪调试生产问题，并通过跟踪令牌使用和模型性能来优化成本

Conclusion: 结合Amazon Bedrock AgentCore、OpenTelemetry和Grafana Cloud提供了一个完整的AI代理监控解决方案

Abstract: How to monitor AI agent applications on Amazon Bedrock AgentCore with Grafana Cloud (7 minute read) This post details how to deploy an AI agent on Amazon Bedrock AgentCore with observability powered by OpenTelemetry and Grafana Cloud. OpenLit provides automatic instrumentation for AI frameworks, and Grafana Cloud's AI Observability dashboards can be used to monitor agent performance, debug production issues using distributed tracing, and optimize costs by tracking token usage and model perfor...

</details>


### [34] [Agentic AI Interface Improvements](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2136%26utm_source=tldrdesign/1/0100019aca9eb16b-6442508d-29b9-4ce7-aaff-9d9b7829a89b-000000/SO9MSUwLYW5W9MEYNpaZsG1ZyfGoHHfk8XEKduUW-Vg=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 提出双滚动窗格界面设计，解决智能体AI在传统聊天界面中因冗长推理过程导致用户忘记初始指令的问题


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统进行扩展推理和工具使用时，在传统聊天界面中产生可用性问题，特别是当AI的冗长内部处理过程将内容推离屏幕时，用户会忘记初始指令

Method: 引入新的双滚动窗格布局，将AI的处理过程放在左列，结果放在右列，保持两者同时可见

Result: 界面改进后，用户能够同时看到AI的处理过程和最终结果，解决了用户跟踪初始指令的问题

Conclusion: 双滚动窗格界面设计有效改善了智能体AI系统的用户体验，解决了传统聊天界面在复杂任务中的局限性

Abstract: Agentic AI Interface Improvements (1 minute read) Agentic AI systems that perform extended reasoning and tool use create usability problems in traditional chat interfaces, particularly when users lose track of their initial instructions as the AI's lengthy internal processes push content off-screen. This post introduces a new dual-scroll pane layout that separates the AI's process in the left column from results in the right column, keeping both visible simultaneously. After completion, the t...

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [35] [无人机开发分享——基于<em class="highlight">强化学习</em>的无人机端到端飞行控制算法开发](http://mp.weixin.qq.com/s?__biz=Mzk5MDU4Njc4MA==&mid=2247483889&idx=1&sn=1d4764bcb560c87ed5ed3cc636c29bce&chksm=c47f39af4da12eb038b93ad351c0d424e5462ab855f3d2fc4e869d63eed5c4e44b60935ad936#rd)
*无人机自由开发坊*

Main category: wechat.article

TL;DR: 思路 2：强化学习（RL）端到端控制，无需系统模型，通过强化学习训练智能体（Agent）直接从 “传感器输入→控制输出” 映射，适合复杂环境（如动态避障、多机协作）；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 思路 2：强化学习（RL）端到端控制，无需系统模型，通过强化学习训练智能体（Agent）直接从 “传感器输入→控制输出” 映射，适合复杂环境（如动态避障、多机协作）；

</details>


### [36] [特斯拉新推出的FSD V14：自动驾驶训练从模仿学习走向<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzA4ODUzMTYyOA==&mid=2650824889&idx=1&sn=09deeeabecd6ff31a540d2974c7f6f9c&chksm=8aca0643d8e8d8177737d1cc7c3d76d27af078e215922cac83a354b36d18a3c03134fb5876bc#rd)
*Myautotime*

Main category: wechat.article

TL;DR: 强化学习的核心是将驾驶问题建模为马尔科夫决策过程，不断进行“感知状态-选择动作-执行动作-获得反馈-学习与更新”的重复循环，通过与环境的反复试错交互，根据获得的奖励信号自主学习最优的策略。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习的核心是将驾驶问题建模为马尔科夫决策过程，不断进行“感知状态-选择动作-执行动作-获得反馈-学习与更新”的重复循环，通过与环境的反复试错交互，根据获得的奖励信号自主学习最优的策略。

</details>


### [37] [Transformer、<em class="highlight">强化学习</em>融合？解决序列决策优化难题！！！](http://mp.weixin.qq.com/s?__biz=MzYyMzQ5NTY5OQ==&mid=2247484376&idx=1&sn=238364d4f7a5f98f2f7241b917e5b1f8&chksm=fe7c5186e67c08e40e16bc4350e0d2f4a95d1fb15281a2ad95c816709be1869193b740541d16#rd)
*AI魔王进化论*

Main category: wechat.article

TL;DR: 强化学习 详解强化学习负责告诉 Transformer “怎么做才对”。我们通常使用马尔可夫决策过程（MDP）来建模。智能体在状态 下采取动作 ，获得奖励 并转移到 。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习 详解强化学习负责告诉 Transformer “怎么做才对”。我们通常使用马尔可夫决策过程（MDP）来建模。智能体在状态 下采取动作 ，获得奖励 并转移到 。

</details>


### [38] [华盛顿大学Nature Communications：用于可解释且高效的基于模型的<em class="highlight">强化学习</em>的SINDy-RL（附开源代码数据）](http://mp.weixin.qq.com/s?__biz=MzU4NzY1NTI4Mw==&mid=2247498115&idx=1&sn=ae096cee55ba4719ba16bcee6a1d1cb1&chksm=fcee53ea95dee8ac6046d77b0b1be09259c3b28ce9658372f4a764ea00698b352e3ebf77c4d7#rd)
*AI4CFD*

Main category: wechat.article

TL;DR: 针对减少训练强化学习策略所需的经验量，已经开展了大量研究，例如离线、经验回放方法、迁移学习和元学习。在环境的低保真度表示中进行训练可能是减少全阶环境中交互次数的最常见方法。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 针对减少训练强化学习策略所需的经验量，已经开展了大量研究，例如离线、经验回放方法、迁移学习和元学习。在环境的低保真度表示中进行训练可能是减少全阶环境中交互次数的最常见方法。

</details>


### [39] [LLM <em class="highlight">强化学习</em>框架研究](http://mp.weixin.qq.com/s?__biz=Mzk2NDA1NDI3Mg==&mid=2247483732&idx=1&sn=db78eb6f38a32280f7065a7da96abfc9&chksm=c588315f7e95ca64590d747f5c427024ee58003e81b7dc7b780bbb577cbf88179e4efc59f0b5#rd)
*厦门英特宝儿科技有限公司*

Main category: wechat.article

TL;DR: 本报告旨在对当前开源社区中排名前十的强化学习训练框架进行详尽的深度剖析，重点关注其在LLM后训练阶段的表现。本研究基于GitHub Star数量对框架进行倒排，选取了LLaMA-Factory、Unsloth、DeepSpeed、Verl、TRL、Swift、Axolotl、OpenRLHF


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本报告旨在对当前开源社区中排名前十的强化学习训练框架进行详尽的深度剖析，重点关注其在LLM后训练阶段的表现。本研究基于GitHub Star数量对框架进行倒排，选取了LLaMA-Factory、Unsloth、DeepSpeed、Verl、TRL、Swift、Axolotl、OpenRLHF

</details>


### [40] [<em class="highlight">强化学习</em>之外的一份清醒](http://mp.weixin.qq.com/s?__biz=MzAxMTYzNTA2Ng==&mid=2653701896&idx=1&sn=626f2796f8c24821c1a51c35ab1c37b8&chksm=81c411fed05f981a643c5794687f11efaf849e8a8b2f4f26777a9953b73a4d84349d348126f4#rd)
*模型视角*

Main category: wechat.article

TL;DR: 强化学习是一种通过与环境交互来学习最优行为策略的机器学习方法。与监督学习依赖标注数据、无监督学习探索数据结构不同，强化学习关注的是"试错"过程中的奖惩反馈。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习是一种通过与环境交互来学习最优行为策略的机器学习方法。与监督学习依赖标注数据、无监督学习探索数据结构不同，强化学习关注的是"试错"过程中的奖惩反馈。

</details>


### [41] [PPO最强，DPO一般？一文带你了解常见三种<em class="highlight">强化学习</em>方法，文末有大模型微调神器！](http://mp.weixin.qq.com/s?__biz=MzE5OTAxNzYxMw==&mid=2247484961&idx=1&sn=0c4c7fdf105e13b79abd735d4c2c3bda&chksm=9752528701396ec46b685749fc16350ae37b79aa82b9eb5e81b5259f6591f6ecc75c85d40760#rd)
*大模型微调Online*

Main category: wechat.article

TL;DR: ● 训练稳定，没有价值函数、优势估计这些“强化学习坑点”，训练过程更像普通 SFT。缺点：● 非常依赖偏好数据质量。如果标注员的标准不统一、甚至本身理解有误，模型就会学错偏好，而且很难通过“奖励模型分析”把问


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ● 训练稳定，没有价值函数、优势估计这些“强化学习坑点”，训练过程更像普通 SFT。缺点：● 非常依赖偏好数据质量。如果标注员的标准不统一、甚至本身理解有误，模型就会学错偏好，而且很难通过“奖励模型分析”把问

</details>


### [42] [<em class="highlight">强化学习</em>之父Richard Sutton：为什么大语言模型是条死胡同](http://mp.weixin.qq.com/s?__biz=Mzg2ODcxNTIyMg==&mid=2247484630&idx=1&sn=7eeb45237d28ea18b6dfe50970854f3a&chksm=cf9aa3a355092521ca05c97e1b531296b671438d55cdfe1f5f5ec0d9c3abe6698a2f2a1313b6#rd)
*WhaleThinking*

Main category: wechat.article

TL;DR: 从强化学习的角度来看，我们在概念上忽略了什么？Richard Sutton：这确实是截然不同的视角。两者很容易各说各话，失去交流的能力。大语言模型已经变得如此庞大，生成式AI整体都很火，我们这个行业喜欢跟风，于是忽略了最


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从强化学习的角度来看，我们在概念上忽略了什么？Richard Sutton：这确实是截然不同的视角。两者很容易各说各话，失去交流的能力。大语言模型已经变得如此庞大，生成式AI整体都很火，我们这个行业喜欢跟风，于是忽略了最

</details>


### [43] [Claude <em class="highlight">Code</em> 核心：深度介绍 MCP+<em class="highlight">Agent</em>+斜杆命令+Hook 一文通！](http://mp.weixin.qq.com/s?__biz=MzYzMzE5ODQyNw==&mid=2247483850&idx=1&sn=2fb31650b0963409d702c65d6c297ca7&chksm=f191c75134c147924db9109c7d0eaa4de4ff95306d7f2fc5d2b71fc2262792810633e17979ef#rd)
*银河智学技术*

Main category: wechat.article

TL;DR: Claude Code 是一个非常有限的命令行 AI Code Agent ， 具备非常丰富的 Agent/MCP/Hooks 等功能，具备非常强的能力，能够囊括除了写代码之外更多联动工作，可以作为一个核心操作交互的入口。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Claude Code 是一个非常有限的命令行 AI Code Agent ， 具备非常丰富的 Agent/MCP/Hooks 等功能，具备非常强的能力，能够囊括除了写代码之外更多联动工作，可以作为一个核心操作交互的入口。

</details>


### [44] [<em class="highlight">Agentic</em> AI 全栈创新：从模型到治理，开启智能化落地新路径](http://mp.weixin.qq.com/s?__biz=MzI0MjI0MTc5Mg==&mid=2247522152&idx=2&sn=b5399086884da24dce792221491a397b&chksm=e8c389d309c9732671d1593c5745923522bb0c5420bc8ae37e81dc16411828ad8028a4da539c#rd)
*微软市场活动*

Main category: wechat.article

TL;DR: Microsoft Foundry（国际版）：模型、智能体、工具、治理、部署一站式 AI 工程平台Agent 超级工厂：超11，000模型池 + 智能 Model Router，优选模型构建智能体


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Microsoft Foundry（国际版）：模型、智能体、工具、治理、部署一站式 AI 工程平台Agent 超级工厂：超11，000模型池 + 智能 Model Router，优选模型构建智能体

</details>


### [45] [易鑫开源业内首个<em class="highlight">Agentic</em>大模型，推动AI红利共享，加速AI生态共建](http://mp.weixin.qq.com/s?__biz=MzYyNTQyNzUyMw==&mid=2247483909&idx=1&sn=d4d2b6f17135963e51418ba77430bee2&chksm=f1281d75a01abe42eef5213f7f27a63f33a505cc8c0f1048f7c5f499af275dcb1eb9349bd7d2#rd)
*易鑫AI*

Main category: wechat.article

TL;DR: 在Agentic通用工具调用评测方面，YiXin-Agentic-Qwen3-14B在TAU1、TAU2和C3-Bench等复杂场景和任务执行类测试中平均得分58.3，不仅在同尺寸模型中稳居第一（Qwen3-14B：44.5），还超越了参数规模更大的Qwen3-235B-A22B-Thinking-2507（57.4）、Kimi-K2-I


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在Agentic通用工具调用评测方面，YiXin-Agentic-Qwen3-14B在TAU1、TAU2和C3-Bench等复杂场景和任务执行类测试中平均得分58.3，不仅在同尺寸模型中稳居第一（Qwen3-14B：44.5），还超越了参数规模更大的Qwen3-235B-A22B-Thinking-2507（57.4）、Kimi-K2-I

</details>


### [46] [麦肯锡1年实战总结：6个<em class="highlight">Agentic</em> AI企业级实操落地经验](http://mp.weixin.qq.com/s?__biz=MzI3NzE1ODQ4NQ==&mid=2650458367&idx=1&sn=235357d19510ad1eaa801a10ca153a74&chksm=f29caa8754dda5f33a592c2915c4535b799fcebf55818f7bf6a3e8b75fb07389cfd7ecee4a81#rd)
*AIE 加速工业进化*

Main category: wechat.article

TL;DR: 本文具体内容来自对文章《One year of agentic AI：Six Lessons from the people doing the work》的理解和翻译，原文链接见文末“阅读原文”。以下是6个经验的具体解释。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本文具体内容来自对文章《One year of agentic AI：Six Lessons from the people doing the work》的理解和翻译，原文链接见文末“阅读原文”。以下是6个经验的具体解释。

</details>


### [47] [<em class="highlight">Agentic</em>时代来临，你的“数字员工”还缺一个专属办公室！](http://mp.weixin.qq.com/s?__biz=Mzg4OTU4MTIzNw==&mid=2247483844&idx=1&sn=0f833fb2e04e5978e991bd1a9f9bef76&chksm=cead33827bec56ff4ee912a309907853cc372bd1abdfb592a0791890415b1f4e32238b38b2ac#rd)
*熊熊 ai 笔记*

Main category: wechat.article

TL;DR: 当Agent进化到真正的 Agentic 阶段——即能够自主规划、动态调用工具并与环境深度交互时，一个简单的函数调用接口早已无法满足需求。我们需要一个能模拟真实世界、但又完全受控的环境。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 当Agent进化到真正的 Agentic 阶段——即能够自主规划、动态调用工具并与环境深度交互时，一个简单的函数调用接口早已无法满足需求。我们需要一个能模拟真实世界、但又完全受控的环境。

</details>


### [48] [【报告】智能体专题五：国家队出手！ 2025 首份<em class="highlight">大模型</em>智能体开发平台技术能力综合测试报告（附PDF下载）](http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247566187&idx=5&sn=fa091e97a0b6f702eb253f59b516f013&chksm=fc6db9b5b344877d042a9c27b2537842bce6d7efdd51d606987518e201739c07c75d49985120#rd)
*人工智能产业链union*

Main category: wechat.article

TL;DR: 《大模型智能体开发平台技术能力综合测试报告》 （完整版.pdf ）以下仅展示部分内容 下载方式见文末 一、开场：官方“摸底考”来了 测试机构：国家工业信息安全发展研究中心 + 赛昇实验室


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 《大模型智能体开发平台技术能力综合测试报告》 （完整版.pdf ）以下仅展示部分内容 下载方式见文末 一、开场：官方“摸底考”来了 测试机构：国家工业信息安全发展研究中心 + 赛昇实验室

</details>


### [49] [【AI今日头条】ChatGPT发布三周年：从对话到智能体，<em class="highlight">大模型</em>这三年](http://mp.weixin.qq.com/s?__biz=MzU5NTc5NDk3MQ==&mid=2247485457&idx=1&sn=be23137044e0e019f2dd5441eb952f54&chksm=ffca1b75d25de87a430ca9585bbf4223c123e0dcff80ddb71b2bbd43a38e3217ad58b8997f13#rd)
*河东小郢*

Main category: wechat.article

TL;DR: 这一年，国内的大模型还主要在“预备队”阶段，更多是技术储备和内部演示。2022 年代表性模型（按地区）地区模型 / 产品时间节点说明美国GPT-3.5 / ChatGPT


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这一年，国内的大模型还主要在“预备队”阶段，更多是技术储备和内部演示。2022 年代表性模型（按地区）地区模型 / 产品时间节点说明美国GPT-3.5 / ChatGPT

</details>


### [50] [主流AI<em class="highlight">大模型</em>密集迭代 多场景实用能力显著升级](http://mp.weixin.qq.com/s?__biz=Mzk2NDg2Mzc5MA==&mid=2247484921&idx=1&sn=7325193e725eeecec26925a6394fa2ac&chksm=c5eccef4de73a84a1a6f3b740f59dd4c65a48b178ba9ccf0dc9c6eccc6e34a65a283ddfa445c#rd)
*视界拾光社*

Main category: wechat.article

TL;DR: 此次多款大模型迭代均以"实用化"为核心导向，长上下文处理、推理深度可调、专业场景适配等升级方向，既降低了普通用户的使用门槛，也为开发者提供了更精准的工具选择，进一步推动AI技术在科研、开发、日常服务等多领


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 此次多款大模型迭代均以"实用化"为核心导向，长上下文处理、推理深度可调、专业场景适配等升级方向，既降低了普通用户的使用门槛，也为开发者提供了更精准的工具选择，进一步推动AI技术在科研、开发、日常服务等多领

</details>


### [51] [《中国人工智能<em class="highlight">大模型</em>知识产权发展报告（2025）》重磅发布 这些趋势值得关注！](http://mp.weixin.qq.com/s?__biz=MzkzOTU4MjQwMg==&mid=2247487134&idx=1&sn=efeff21299d2e780f8704988744f93ee&chksm=c365d8d921e9d1d7be999ebf68ea0696f76426b879c8a448841bacb33799cd5c78f2afe1ba32#rd)
*东恒知盛知识产权*

Main category: wechat.article

TL;DR: 大模型生成内容的版权归属问题同样复杂。报告梳理了当前的主要观点：主流观点分歧：35%的专家认为应归属于模型使用者 28%的专家认为应归属于模型开发者


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型生成内容的版权归属问题同样复杂。报告梳理了当前的主要观点：主流观点分歧：35%的专家认为应归属于模型使用者 28%的专家认为应归属于模型开发者

</details>


### [52] [哪吒出海！国内首个“AI+法律”出海<em class="highlight">大模型</em>来了](http://mp.weixin.qq.com/s?__biz=Mzk4ODk0MDUzOA==&mid=2247483715&idx=1&sn=9d6f3819c934d7b7a3b016619800be85&chksm=c47082e6eaf560f8b92ccd3956616f65756c825cba5a13fcdc40a83963868246966598e22026#rd)
*哪吒出海法律通*

Main category: wechat.article

TL;DR: 上海哪吒出海智能科技有限公司是一家基于前沿AI大模型技术、专注于为中国企业出海提供智能化法律服务的科技创新企业。公司旗下产品“哪吒出海法律通”智能体，作为全球首个专注于出海法律服务领域的垂直类模型平台，


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 上海哪吒出海智能科技有限公司是一家基于前沿AI大模型技术、专注于为中国企业出海提供智能化法律服务的科技创新企业。公司旗下产品“哪吒出海法律通”智能体，作为全球首个专注于出海法律服务领域的垂直类模型平台，

</details>


### [53] [每月AI<em class="highlight">大模型</em>更新速递（25年11月）](http://mp.weixin.qq.com/s?__biz=MzAxNjYxOTY3NQ==&mid=2455622600&idx=1&sn=6e9e9909750e64d2391d1303542d4008&chksm=8d127fb7267685b87303267f46f7897dc8f92dbbfa1898925049b4500d7c055898b75638ea6a#rd)
*大模型评测及优化NoneLinear*

Main category: wechat.article

TL;DR: 大模型/agent评测技术交流：关注公众号，发送消息"进群"


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型/agent评测技术交流：关注公众号，发送消息"进群"

</details>


### [54] [AIR学术｜刘洋：<em class="highlight">大模型</em>驱动的可进化智能体](http://mp.weixin.qq.com/s?__biz=MzI0NDg0OTI1MQ==&mid=2247500984&idx=1&sn=d64f444e64e98989daae7a9fe4dbf21f&chksm=e859a623097937df688bba7c7ccab3ffeffaae87ddcdfbb484edae866fc53bec8765b323f778#rd)
*清华大学智能产业研究院*

Main category: wechat.article

TL;DR: 首先，大模型正逐渐成为各类智能系统的“通用大脑”，为机器人、金融、医疗等场景提供理解、推理与生成能力，其发展路径也从早期的单模态、专用模型，快速转向能够同时处理文本、图像、音频、视频等的多模态通用模型


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 首先，大模型正逐渐成为各类智能系统的“通用大脑”，为机器人、金融、医疗等场景提供理解、推理与生成能力，其发展路径也从早期的单模态、专用模型，快速转向能够同时处理文本、图像、音频、视频等的多模态通用模型

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [55] [Code Refactoring with LLM: A Comprehensive Evaluation With Few-Shot Settings](https://arxiv.org/abs/2511.21788)
*Md. Raihan Tapader,Md. Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe*

Main category: cs.SE

TL;DR: 提出基于LLM的多语言代码重构框架，通过提示工程和指令微调提升重构效果，在Java上达到99.99%正确率和94.78%可编译性


<details>
  <summary>Details</summary>
Motivation: 现有代码重构方法依赖手工规则，难以泛化到多种编程语言和编码风格，需要开发能跨语言工作的自动化重构方案

Method: 使用基于提示工程的微调模型结合少样本学习，支持C、C++、C#、Python、Java多语言重构，研究温度参数和不同样本算法的影响

Result: Java在10-shot设置下达到最高正确率99.99%，平均可编译性94.78%，相似度53-54%；Python在所有shot中结构距离最低(277-294)，相似度44-48%

Conclusion: LLM-based框架能有效进行多语言代码重构，Java表现最佳，Python重构改动最小，提示工程和微调对重构质量有显著影响

Abstract: In today's world, the focus of programmers has shifted from writing complex, error-prone code to prioritizing simple, clear, efficient, and sustainable code that makes programs easier to understand. Code refactoring plays a critical role in this transition by improving structural organization and optimizing performance. However, existing refactoring methods are limited in their ability to generalize across multiple programming languages and coding styles, as they often rely on manually crafted transformation rules. The objectives of this study are to (i) develop an Large Language Models (LLMs)-based framework capable of performing accurate and efficient code refactoring across multiple languages (C, C++, C#, Python, Java), (ii) investigate the impact of prompt engineering (Temperature, Different shot algorithm) and instruction fine-tuning on refactoring effectiveness, and (iii) evaluate the quality improvements (Compilability, Correctness, Distance, Similarity, Number of Lines, Token, Character, Cyclomatic Complexity) in refactored code through empirical metrics and human assessment. To accomplish these goals, we propose a fine-tuned prompt-engineering-based model combined with few-shot learning for multilingual code refactoring. Experimental results indicate that Java achieves the highest overall correctness up to 99.99% the 10-shot setting, records the highest average compilability of 94.78% compared to the original source code and maintains high similarity (Approx. 53-54%) and thus demonstrates a strong balance between structural modifications and semantic preservation. Python exhibits the lowest structural distance across all shots (Approx. 277-294) while achieving moderate similarity ( Approx. 44-48%) that indicates consistent and minimally disruptive refactoring.

</details>


### [56] [LLM-Empowered Event-Chain Driven Code Generation for ADAS in SDV systems](https://arxiv.org/abs/2511.21877)
*Nenad Petrovic,Norbert Kroth,Axel Torschmied,Yinglei Song,Fengjunjie Pan,Vahid Zolfaghari,Nils Purschke,Sven Kirchner,Chengdong Wu,Andre Schamschurko,Yi Zhang,Alois Knoll*

Main category: cs.SE

TL;DR: 提出基于事件链驱动、LLM赋能的工作流，从自然语言需求生成经过验证的汽车代码，通过RAG检索VSS信号确保架构正确性，无需LLM重训练即可实现有效信号使用和一致代码生成。


<details>
  <summary>Details</summary>
Motivation: 汽车代码开发面临自然语言需求到代码转换的挑战，需要确保信号使用的正确性、减少LLM幻觉，并保持架构一致性。现有方法在处理大型且不断演化的车辆信号规范(VSS)目录时存在困难。

Method: 采用事件链驱动的LLM工作流：1) RAG层从VSS目录检索相关信号作为上下文；2) 信号映射和验证；3) 转换为编码因果和时序约束的事件链；4) 事件链指导LLM代码合成，确保行为一致性和实时可行性。

Result: 在紧急制动案例研究中，该方法实现了有效的信号使用和一致的代码生成，无需LLM重训练。能够减少幻觉并确保架构正确性。

Conclusion: 事件链驱动、LLM赋能的工作流能够从自然语言需求生成经过验证的汽车代码，通过RAG检索和事件链约束确保信号正确性和行为一致性，为汽车软件开发提供有效解决方案。

Abstract: This paper presents an event-chain-driven, LLM-empowered workflow for generating validated, automotive code from natural-language requirements. A Retrieval-Augmented Generation (RAG) layer retrieves relevant signals from large and evolving Vehicle Signal Specification (VSS) catalogs as code generation prompt context, reducing hallucinations and ensuring architectural correctness. Retrieved signals are mapped and validated before being transformed into event chains that encode causal and timing constraints. These event chains guide and constrain LLM-based code synthesis, ensuring behavioral consistency and real-time feasibility. Based on our initial findings from the emergency braking case study, with the proposed approach, we managed to achieve valid signal usage and consistent code generation without LLM retraining.

</details>


### [57] [Advancing Automated In-Isolation Validation in Repository-Level Code Translation](https://arxiv.org/abs/2511.21878)
*Kaiyao Ke,Ali Reza Ibrahimzada,Rangeet Pan,Saurabh Sinha,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: TRAM提出了一种结合上下文感知类型解析和基于模拟的隔离验证的仓库级代码翻译方法，在Java到Python翻译中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管仓库级代码翻译有所进展，但验证翻译结果仍然具有挑战性。现有方法要么使用代理进行验证成本高昂，要么依赖语言互操作性需要大量人工努力。

Method: TRAM结合了上下文感知类型解析和基于模拟的隔离验证。首先检索API文档和上下文代码信息，使用LLM解析跨语言类型映射；然后通过自定义序列化/反序列化工作流自动构建目标语言中的等效模拟对象，实现方法片段的隔离验证。

Result: TRAM在Java到Python翻译中展示了最先进的性能，证明了其RAG-based类型解析与可靠隔离验证集成的有效性。

Conclusion: TRAM通过结合上下文感知类型解析和基于模拟的隔离验证，实现了高质量的仓库级代码翻译，解决了翻译验证的挑战。

Abstract: Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.

</details>


### [58] [Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code](https://arxiv.org/abs/2511.21920)
*Apu Kumar Chakroborti,Yi Ding,Lipeng Wan*

Main category: cs.SE

TL;DR: 评估开源大语言模型在科学数据分析和可视化任务中生成Python代码的可信度，发现无人工干预时可靠性有限，提出了三种改进策略并构建了可复用的基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 随着科学数据日益复杂，领域科学家缺乏编程技能成为分析大规模数据的障碍。大语言模型通过自然语言生成代码提供了解决方案，但其在科学数据分析中的可信度需要系统评估。

Method: 构建反映真实研究任务的领域启发式提示基准套件，系统评估生成代码的可执行性和正确性。设计并评估三种策略：数据感知提示消歧、检索增强提示改进和迭代错误修复。

Result: 无人工干预时，LLM生成代码的可靠性有限，失败常由模糊提示和模型对领域特定上下文理解不足导致。三种改进策略显著提高了执行成功率和输出质量，但仍需进一步优化。

Conclusion: LLM驱动的科学工作流自动化既有前景也有当前局限，需要更精细的提示工程和领域知识整合。本研究为构建更包容、可访问和可信的AI辅助研究工具提供了可行技术和可复用基准。

Abstract: As modern science becomes increasingly data-intensive, the ability to analyze and visualize large-scale, complex datasets is critical to accelerating discovery. However, many domain scientists lack the programming expertise required to develop custom data analysis workflows, creating barriers to timely and effective insight. Large language models (LLMs) offer a promising solution by generating executable code from natural language descriptions. In this paper, we investigate the trustworthiness of open-source LLMs in autonomously producing Python scripts for scientific data analysis and visualization. We construct a benchmark suite of domain-inspired prompts that reflect real-world research tasks and systematically evaluate the executability and correctness of the generated code. Our findings show that, without human intervention, the reliability of LLM-generated code is limited, with frequent failures caused by ambiguous prompts and the models' insufficient understanding of domain-specific contexts. To address these challenges, we design and assess three complementary strategies: data-aware prompt disambiguation, retrieval-augmented prompt enhancement, and iterative error repair. While these methods significantly improve execution success rates and output quality, further refinement is needed. This work highlights both the promise and current limitations of LLM-driven automation in scientific workflows and introduces actionable techniques and a reusable benchmark for building more inclusive, accessible, and trustworthy AI-assisted research tools.

</details>


### [59] [DRS-OSS: LLM-Driven Diff Risk Scoring Tool for PR Risk Prediction](https://arxiv.org/abs/2511.21964)
*Ali Sayedsalehi,Peter C. Rigby,Audris Mockus*

Main category: cs.SE

TL;DR: DRS-OSS是一个开源的风险评分系统，使用微调的Llama 3.1 8B模型分析代码变更风险，帮助优先审查高风险PR，防止缺陷引入。


<details>
  <summary>Details</summary>
Motivation: 大规模开源项目中每天有大量PR合并，每个都可能引入回归缺陷。需要一种方法来评估代码变更的风险，以便更好地进行审查优先级排序、测试规划和CI/CD门控。

Method: 使用微调的Llama 3.1 8B序列分类器，结合提交信息、结构化差异和变更指标的长上下文表示。通过参数高效适配、4位QLoRA和DeepSpeed ZeRO-3 CPU卸载技术，在单张20GB GPU上训练22k令牌的上下文。

Result: 在ApacheJIT基准测试中达到最先进性能（F1=0.64，ROC-AUC=0.89）。模拟显示，仅门控风险最高的30%提交就能防止高达86.4%的缺陷引入变更。

Conclusion: DRS-OSS是一个有效的开源风险评分系统，通过API、Web UI和GitHub插件集成到开发者工作流中，能够显著减少缺陷引入。

Abstract: In large-scale open-source projects, hundreds of pull requests land daily, each a potential source of regressions. Diff Risk Scoring (DRS) estimates the likelihood that a diff will introduce a defect, enabling better review prioritization, test planning, and CI/CD gating. We present DRS-OSS, an open-source DRS system equipped with a public API, web UI, and GitHub plugin. DRS-OSS uses a fine-tuned Llama 3.1 8B sequence classifier trained on the ApacheJIT dataset, consuming long-context representations that combine commit messages, structured diffs, and change metrics. Through parameter-efficient adaptation, 4-bit QLoRA, and DeepSpeed ZeRO-3 CPU offloading, we train 22k-token contexts on a single 20 GB GPU. On the ApacheJIT benchmark, DRS-OSS achieves state-of-the-art performance (F1 = 0.64, ROC-AUC = 0.89). Simulations show that gating only the riskiest 30% of commits can prevent up to 86.4% of defect-inducing changes. The system integrates with developer workflows through an API gateway, a React dashboard, and a GitHub App that posts risk labels on pull requests. We release the full replication package, fine-tuning scripts, deployment artifacts, code, demo video, and public website.

</details>


### [60] [Statistical Independence Aware Caching for LLM Workflows](https://arxiv.org/abs/2511.22118)
*Yihan Dai,Dimitrios Stamatios Bouras,Haoxiang Jia,Sergey Mechtaev*

Main category: cs.SE

TL;DR: Mnimi是一个LLM缓存设计模式，通过封装统计约束在LLM引用类型中，确保组件级统计完整性，同时提高代码代理工作流的可复现性和效率。


<details>
  <summary>Details</summary>
Motivation: LLM推理成本高且延迟大，本地缓存可以降低成本延迟，但现有缓存系统无法保证统计独立性，而统计独立性对于代码代理中的性能指标（如Pass@k）、不确定性估计以及程序修复循环等算法至关重要。

Method: 提出Mnimi缓存设计模式，核心创新是将统计约束封装在LLM引用类型中，允许用户根据算法范围和需求管理和转换这些类型。在Python中使用装饰器和无限序列迭代器实现。

Result: 在SpecFix（自动程序规范修复系统）的案例研究中，Mnimi提高了可复现性、调试便利性、时间和成本效率，同时保持了统计正确性。

Conclusion: Mnimi为模块化LLM工作流提供了支持统计完整性的缓存解决方案，解决了现有缓存系统缺乏统计独立性约束的问题。

Abstract: Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.

</details>


### [61] [NOMAD: A Multi-Agent LLM System for UML Class Diagram Generation from Natural Language Requirements](https://arxiv.org/abs/2511.22409)
*Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.SE

TL;DR: NOMAD是一个认知启发的模块化多智能体框架，用于将UML图生成分解为角色专业化的子任务，在UML类图生成方面优于基线模型，并首次系统分类了LLM生成UML图的错误类型。


<details>
  <summary>Details</summary>
Motivation: LLM在软件工程中应用日益广泛，但其生成UML图等结构化工件的能力尚未充分探索。现有方法缺乏对UML生成过程的分解和可解释性。

Method: 提出NOMAD框架，将UML生成分解为实体提取、关系分类、图合成等角色专业化子任务，采用混合评估设计：大型案例研究（Northwind）进行深度探测和错误分析，以及人工编写的UML练习进行广度和真实性评估。

Result: NOMAD在所有选定基线模型中表现最佳，同时揭示了细粒度属性提取方面的持续挑战。首次提出了LLM生成UML图的系统性错误分类法（结构、关系、语义/逻辑错误），并探讨了验证作为设计探针的混合效果。

Conclusion: NOMAD既是UML类图生成的有效框架，也为可靠语言到模型工作流程的更广泛研究挑战提供了视角，自适应策略是有前景的研究方向。

Abstract: Large Language Models (LLMs) are increasingly utilised in software engineering, yet their ability to generate structured artefacts such as UML diagrams remains underexplored. In this work we present NOMAD, a cognitively inspired, modular multi-agent framework that decomposes UML generation into a series of role-specialised subtasks. Each agent handles a distinct modelling activity, such as entity extraction, relationship classification, and diagram synthesis, mirroring the goal-directed reasoning processes of an engineer. This decomposition improves interpretability and allows for targeted verification strategies. We evaluate NOMAD through a mixed design: a large case study (Northwind) for in-depth probing and error analysis, and human-authored UML exercises for breadth and realism. NOMAD outperforms all selected baselines, while revealing persistent challenges in fine-grained attribute extraction. Building on these observations, we introduce the first systematic taxonomy of errors in LLM-generated UML diagrams, categorising structural, relationship, and semantic/logical. Finally, we examine verification as a design probe, showing its mixed effects and outlining adaptive strategies as promising directions. Together, these contributions position NOMAD as both an effective framework for UML class diagram generation and a lens onto the broader research challenges of reliable language-to-model workflows.

</details>


### [62] [FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation](https://arxiv.org/abs/2511.23302)
*Hengyuan Liu,Zheng Li,Donghua Wang,Yankai Wu,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: 提出MBFL-FLIM框架，通过LLM语义分析识别和缓解干扰突变体，提升基于突变的故障定位效果


<details>
  <summary>Details</summary>
Motivation: 传统基于突变的故障定位(MBFL)面临干扰突变体的挑战，这些来自非故障代码的突变体会被失败测试杀死，模仿真实故障行为，削弱故障定位效果

Method: 1) 提出故障定位干扰突变体(FLIMs)概念并进行RIPR模型理论分析；2) 使用LLM语义分析识别FLIMs，结合微调技术和置信度估计策略；3) 通过精炼可疑度分数缓解FLIMs影响；4) 集成到MBFL工作流中形成MBFL-FLIM框架

Result: 在Defects4J基准测试的395个程序版本上，使用8个LLM进行实验，MBFL-FLIM在Top-1指标上平均提升44个故障，优于传统SBFL/MBFL方法、动态特征方法和近期LLM故障定位技术

Conclusion: MBFL-FLIM通过语义识别和缓解干扰突变体，显著提升故障定位效果，在多故障场景下表现稳健，微调和置信度估计组件对性能有重要贡献

Abstract: Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components.

</details>


### [63] [Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing](https://arxiv.org/abs/2511.23321)
*Yifei Wang,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Yuchen Cao*

Main category: cs.SE

TL;DR: C2C-MoLA：结合MoE与LoRA的多模态框架，用于图表到代码生成，提升准确性17%、降低GPU内存18%、加速收敛20%


<details>
  <summary>Details</summary>
Motivation: 现有图表到代码生成方法在跨类型泛化、内存效率和模块化设计方面存在不足，需要更高效的解决方案

Method: 提出C2C-MoLA框架，结合Mixture of Experts（MoE）与Low-Rank Adaptation（LoRA）。MoE使用复杂度感知路由机制，包含领域专家和负载均衡稀疏门控；LoRA实现参数高效更新；采用定制训练策略对齐路由稳定性与语义准确性

Result: 在Chart2Code-160k数据集上，相比标准微调和仅LoRA基线，生成准确性提升17%，峰值GPU内存降低18%，收敛速度加快20%，尤其在复杂图表上表现更佳

Conclusion: C2C-MoLA有效解决了图表到代码生成中的跨类型泛化、内存效率和模块化设计问题，通过MoE与LoRA的协同实现了高效的多模态代码生成

Abstract: Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [64] [Real-Time Procedural Learning From Experience for AI Agents](https://arxiv.org/abs/2511.22074)
*Dasheng Bi,Yubin Hu,Mohammed N. Nasir*

Main category: cs.AI

TL;DR: PRAXIS是一种轻量级后训练学习机制，通过存储动作结果并根据环境状态检索过往经验，增强LLM智能体在实时环境中的程序性知识获取能力


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体缺乏部署后获取程序性知识的能力，无法像生物智能那样通过试错实时学习。需要一种机制让AI智能体在快速演化的状态化环境中有效学习新程序

Method: 提出PRAXIS机制：存储动作结果，通过联合匹配环境和内部状态来检索过往经验，将检索到的状态-动作-结果示例实时生成并增强智能体的动作选择

Result: 在REAL网页浏览基准测试中，PRAXIS提高了任务完成准确率、可靠性和成本效率，在不同基础模型骨干上都有效，并在相似环境中对未见任务显示出初步泛化能力

Conclusion: PRAXIS通过帮助智能体有效学习新程序，使AI智能体能够在快速演化的状态化环境中得到实际应用

Abstract: Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment. We propose Procedural Recall for Agents with eXperiences Indexed by State (PRAXIS), a lightweight post-training learning mechanism that stores the consequences of actions and retrieves them by jointly matching environmental and internal states of past episodes to the current state. PRAXIS augments agentic action selection with retrieved state-action-result exemplars that are generated in real time. When evaluated on the REAL web browsing benchmark, PRAXIS improves task completion accuracy, reliability, and cost efficiency across different foundation model backbones, and shows preliminary generalization to unseen tasks in similar environments. These results demonstrate that PRAXIS enables the practical adoption of AI agents in fast-evolving stateful environments by helping them learn new procedures effectively.

</details>


### [65] [Hybrid Stackelberg Game and Diffusion-based Auction for Two-tier Agentic AI Task Offloading in Internet of Agents](https://arxiv.org/abs/2511.22076)
*Yue Zhong,Yongju Tong,Jiawen Kang,Minghui Dai,Hong-Ning Dai,Zhou Su,Dusit Niyato*

Main category: cs.AI

TL;DR: 该论文提出了一个两层的智能体物联网(IoA)任务卸载优化方案，第一层使用Stackelberg博弈模型处理移动智能体与固定智能体之间的资源定价和任务卸载，第二层采用双荷兰拍卖模型处理固定智能体向空中智能体的进一步任务卸载，并使用基于扩散的深度强化学习算法求解。


<details>
  <summary>Details</summary>
Motivation: 智能体物联网(IoA)作为互联智能系统的基础架构，需要处理资源受限的无线智能体(WAs)的计算密集型AI服务卸载问题。固定智能体(FAs)由于地理位置固定和连接稳定，可以作为可靠的通信网关和任务聚合点，但可能出现过载情况，需要进一步向空中智能体(AAs)卸载任务。

Method: 1. 第一层：多领导者多追随者Stackelberg博弈模型，移动智能体(MAs)和固定智能体(FAs)作为领导者设定资源价格，无线智能体(WAs)作为追随者确定任务卸载比例
2. 第二层：双荷兰拍卖模型，过载的固定智能体(FAs)作为买家请求资源，空中智能体(AAs)作为卖家提供资源
3. 使用基于扩散的深度强化学习算法求解整个优化模型

Result: 数值结果表明，所提出的方案在促进任务卸载方面具有优越性。

Conclusion: 该论文提出的两层优化方法有效地解决了智能体物联网中资源受限智能体的任务卸载问题，通过博弈论和拍卖机制结合深度强化学习，实现了高效的任务分配和资源利用。

Abstract: The Internet of Agents (IoA) is rapidly gaining prominence as a foundational architecture for interconnected intelligent systems, designed to facilitate seamless discovery, communication, and collaborative reasoning among a vast network of Artificial Intelligence (AI) agents. Powered by Large Language and Vision-Language Models, IoA enables the development of interactive, rational agents capable of complex cooperation, moving far beyond traditional isolated models. IoA involves physical entities, i.e., Wireless Agents (WAs) with limited onboard resources, which need to offload their compute-intensive agentic AI services to nearby servers. Such servers can be Mobile Agents (MAs), e.g., vehicle agents, or Fixed Agents (FAs), e.g., end-side units agents. Given their fixed geographical locations and stable connectivity, FAs can serve as reliable communication gateways and task aggregation points. This stability allows them to effectively coordinate with and offload to an Aerial Agent (AA) tier, which has an advantage not affordable for highly mobile MAs with dynamic connectivity limitations. As such, we propose a two-tier optimization approach. The first tier employs a multi-leader multi-follower Stackelberg game. In the game, MAs and FAs act as the leaders who set resource prices. WAs are the followers to determine task offloading ratios. However, when FAs become overloaded, they can further offload tasks to available aerial resources. Therefore, the second tier introduces a Double Dutch Auction model where overloaded FAs act as the buyers to request resources, and AAs serve as the sellers for resource provision. We then develop a diffusion-based Deep Reinforcement Learning algorithm to solve the model. Numerical results demonstrate the superiority of our proposed scheme in facilitating task offloading.

</details>


### [66] [Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning](https://arxiv.org/abs/2511.22226)
*Alexander Meulemans,Rajai Nasser,Maciej Wołczyk,Marissa A. Weis,Seijin Kobayashi,Blake Richards,Guillaume Lajoie,Angelika Steger,Marcus Hutter,James Manyika,Rif A. Saurous,João Sacramento,Blaise Agüera y Arcas*

Main category: cs.AI

TL;DR: 论文提出一个基于自我预测的嵌入式智能体数学框架，解决传统强化学习中智能体与环境分离假设在多智能体环境中的局限性，实现前瞻性学习和嵌入式智能。


<details>
  <summary>Details</summary>
Motivation: 传统无模型强化学习假设环境是静态的，智能体与环境分离。但在多智能体环境中，其他智能体的学习导致非平稳性，需要基于预测模型的前瞻性学习。智能体需要建模其他智能体，而其他智能体也在建模它，这促使智能体将自己作为环境的一部分来建模。

Method: 基于通用人工智能(AIXI)基础工作，引入以自我预测为中心的数学框架。贝叶斯强化学习智能体同时预测未来感知输入和自己的行动，必须解决关于自身作为宇宙一部分的认知不确定性。扩展AIXI理论，研究从Solomonoff先验开始的通用智能嵌入式智能体。

Result: 在多智能体环境中，自我预测使智能体能够推理运行相似算法的其他智能体，产生新的博弈论解概念和传统分离智能体无法实现的新型合作形式。理想化的智能体可以形成一致的相互预测，实现无限阶心智理论。

Conclusion: 自我预测框架为嵌入式多智能体学习设定了黄金标准，解决了传统强化学习在多智能体环境中的理论挑战，实现了智能体与环境的一体化建模。

Abstract: The standard theory of model-free reinforcement learning assumes that the environment dynamics are stationary and that agents are decoupled from their environment, such that policies are treated as being separate from the world they inhabit. This leads to theoretical challenges in the multi-agent setting where the non-stationarity induced by the learning of other agents demands prospective learning based on prediction models. To accurately model other agents, an agent must account for the fact that those other agents are, in turn, forming beliefs about it to predict its future behavior, motivating agents to model themselves as part of the environment. Here, building upon foundational work on universal artificial intelligence (AIXI), we introduce a mathematical framework for prospective learning and embedded agency centered on self-prediction, where Bayesian RL agents predict both future perceptual inputs and their own actions, and must therefore resolve epistemic uncertainty about themselves as part of the universe they inhabit. We show that in multi-agent settings, self-prediction enables agents to reason about others running similar algorithms, leading to new game-theoretic solution concepts and novel forms of cooperation unattainable by classical decoupled agents. Moreover, we extend the theory of AIXI, and study universally intelligent embedded agents which start from a Solomonoff prior. We show that these idealized agents can form consistent mutual predictions and achieve infinite-order theory of mind, potentially setting a gold standard for embedded multi-agent learning.

</details>


### [67] [Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation](https://arxiv.org/abs/2511.22235)
*Zehao Deng,Tianjie Ju,Zheng Wu,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出CES多智能体框架，通过协调器、执行器和状态跟踪器的分工合作，结合执行反馈强化学习，提升GUI智能体处理长时程任务的能力。


<details>
  <summary>Details</summary>
Motivation: 当前GUI智能体在处理长时程任务时面临两大挑战：1) 单智能体模型难以平衡高层规划能力和底层执行能力，存在责任耦合和能力冲突问题；2) 智能体缺乏任务状态感知，导致长时程任务中进度丢失。

Method: 提出分阶段执行反馈强化学习算法，训练高层调度模型。构建Coordinator-Executor-State Tracker (CES)多智能体框架，包含：协调器负责战略规划和任务分解；状态跟踪器负责上下文压缩和信息管理；执行器负责具体操作。该框架可与任何底层执行器模型集成。

Result: 在长时程任务基准测试中，CES显著提升了系统的规划和状态管理能力。分析证实训练的高层调度模块是通用、即插即用的模块，能显著增强各种执行器的长时程任务处理能力。

Conclusion: CES多智能体框架通过解耦高层规划和底层执行，结合状态跟踪机制，有效解决了GUI智能体在长时程任务中的责任耦合和状态管理问题，为GUI智能体系统提供了可扩展的解决方案。

Abstract: The rapid development of large vision-language model (VLM) has greatly promoted the research of GUI agent. However, GUI agents still face significant challenges in handling long-horizon tasks. First, single-agent models struggle to balance high-level capabilities and low-level execution capability, facing prevalent issues of responsibility coupling and capability conflicts. Second, agents lack awareness of the task state, leading to progress loss in long-horizon tasks. To address these challenges, we propose a staged execution-feedback reinforcement learning algorithm. Unlike training a unified policy model, we focus on training high-level scheduling models. Specifically, we propose and train two agents: a Coordinator, responsible for the strategic planning and task decomposition; and a State Tracker, responsible for context compression and information management to maintain the task's state and coherence. Based on this, we built the Coordinator-Executor-State Tracker (CES) multi-agent framework, which can be integrated with any low-level Executor model, assisting the Executor in solving long-horizon tasks through task scheduling and state management. Experiments on long-horizon task benchmarks demonstrate that CES significantly enhances the system's planning and state management capabilities. Furthermore, analysis confirms that our trained high-level scheduling module is a generalizable, plug-and-play module that significantly enhances the long-horizon capabilities of various Executors. Code can be available at https://github.com/hehehahi4/CES.

</details>


### [68] [Co-Evolving Agents: Learning from Failures as Hard Negatives](https://arxiv.org/abs/2511.22254)
*Yeonsung Jung,Trilok Padhi,Sina Shaham,Dipika Khullar,Joonhyun Jeong,Ninareh Mehrabi,Eunho Yang*

Main category: cs.AI

TL;DR: 提出一种协同进化智能体框架，通过辅助失败智能体生成接近成功但仍失败的硬负样本，提升目标智能体的泛化能力


<details>
  <summary>Details</summary>
Motivation: 当前自改进智能体依赖预测轨迹与稀缺真实轨迹的偏好优化，但过度依赖有限监督下的预测轨迹容易导致过拟合，需要更有效的失败轨迹利用方法

Method: 提出协同进化智能体框架：目标智能体与辅助失败智能体共同进化。失败智能体通过偏好优化学习目标智能体与自身的失败轨迹，生成接近成功但仍失败的硬负样本，将这些信息丰富的硬负样本纳入目标智能体优化过程

Result: 在基准数据集上的综合分析和实验表明，该方法不仅性能提升，还证明失败可以被系统性地转化为结构化且有价值的学习信号

Conclusion: 协同进化框架能够将失败轨迹转化为有价值的硬负样本，锐化决策边界并增强泛化能力，为自改进智能体提供了更有效的失败利用机制

Abstract: The rapid progress of large foundation models has accelerated the development of task-specialized agents across diverse domains. However, the effectiveness of agents remains tightly coupled with the quality of training data, while curating task-specific datasets remains costly and often infeasible in real-world scenarios. Recent work has explored self-improving agents that autonomously generate, refine, and re-train on their own trajectories. A prominent line of approaches further leverages preference optimization by pairing predicted trajectories with scarce ground-truth trajectories, enabling agents to learn directly from their own failures. While these methods outperform supervised fine-tuning, their heavy reliance on predicted trajectories under limited ground-truth supervision leaves them prone to overfitting. To address this, we propose a co-evolving agents framework in which a target agent improves jointly with an auxiliary failure agent. The failure agent learns through preference optimization over failure trajectories from both the target and itself, thereby generating hard negatives that are close to success yet remain failures. Incorporating these informative hard negatives into the target agent's optimization sharpens decision boundaries and enhances generalization. Our comprehensive analysis and experiments across benchmark datasets show that our method not only shows improved performance but also demonstrates that failures, instead of being used as-is, can be systematically transformed into structured and valuable learning signals in self-improving agents.

</details>


### [69] [Geometrically-Constrained Agent for Spatial Reasoning](https://arxiv.org/abs/2511.22659)
*Zeren Chen,Xiaoya Lu,Zhijie Zheng,Pengrui Li,Lehan He,Yijin Zhou,Jing Shao,Bohan Zhuang,Lu Sheng*

Main category: cs.AI

TL;DR: GCA通过将VLM角色解耦为语义分析和任务求解两阶段，引入形式化任务约束来弥合空间推理中的语义-几何鸿沟，在多个基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在空间推理中存在语义-几何鸿沟：擅长定性语义推理但推理过程在损失性语义空间中进行，与高保真几何不对齐。现有方法无法弥合这一鸿沟，训练方法存在"预言机悖论"，工具集成方法则无法约束规划过程。

Method: 提出几何约束代理(GCA)，一种无需训练的代理范式。策略性地将VLM角色解耦为两个阶段：1)作为语义分析器，将用户模糊查询转换为形式化、可验证的任务约束；2)作为任务求解器，在约束定义的确定性边界内生成和执行工具调用。

Result: GCA在多个空间推理基准测试中达到最先进性能，超越现有训练方法和工具集成方法约27%。

Conclusion: GCA通过几何约束推理策略成功解决了语义-几何鸿沟问题，为空间推理提供了鲁棒且可验证的推理路径。

Abstract: Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,'' learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM's planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM's role into two stages. First, acting as a semantic analyst, the VLM translates the user's ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.

</details>


### [70] [Solving Context Window Overflow in AI Agents](https://arxiv.org/abs/2511.22729)
*Anton Bulle Labate,Valesca Moura de Sousa,Sandro Rama Fiorini,Leonardo Guerreiro Azevedo,Raphael Melo Thiago,Viviane Torres da Silva*

Main category: cs.AI

TL;DR: 提出一种让LLM处理任意长度工具输出的方法，通过内存指针而非原始数据交互，避免信息丢失并提升效率


<details>
  <summary>Details</summary>
Motivation: LLM在处理化学、材料科学等知识密集型领域的外部工具输出时，大尺寸输出会超出上下文窗口限制，现有截断或摘要方法会丢失完整信息，无法满足需要完整数据的工作流

Method: 将LLM与工具的交互从原始数据转向内存指针，保留工具完整功能，实现无缝集成到智能体工作流中，减少token使用和执行时间

Result: 在传统工作流无法执行的真实材料科学应用中验证了该方法，通过对比分析显示，在两种方法都成功的情况下，新方法消耗的token数量约为传统方法的七分之一

Conclusion: 该方法能有效处理任意长度的工具输出而不丢失信息，显著提升LLM在知识密集型领域的应用效率

Abstract: Large Language Models (LLMs) have become increasingly capable of interacting with external tools, granting access to specialized knowledge beyond their training data - critical in dynamic, knowledge-intensive domains such as Chemistry and Materials Science. However, large tool outputs can overflow the LLMs' context window, preventing task completion. Existing solutions such as truncation or summarization fail to preserve complete outputs, making them unsuitable for workflows requiring the full data. This work introduces a method that enables LLMs to process and utilize tool responses of arbitrary length without loss of information. By shifting the model's interaction from raw data to memory pointers, the method preserves tool functionality, allows seamless integration into agentic workflows, and reduces token usage and execution time. The proposed method is validated on a real-world Materials Science application that cannot be executed with conventional workflows, and its effectiveness is demonstrated via a comparative analysis where both methods succeed. In this experiment, the proposed approach consumed approximately seven times fewer tokens than the traditional workflow.

</details>


### [71] [Agentic AI Framework for Individuals with Disabilities and Neurodivergence: A Multi-Agent System for Healthy Eating, Daily Routines, and Inclusive Well-Being](https://arxiv.org/abs/2511.22737)
*Salman Jan,Toqeer Ali Syed,Gohar Ali,Ali Akarma,Mohammad Riyaz Belgaum,Ahmad Ali*

Main category: cs.AI

TL;DR: 提出一个面向残障和神经多样性人群的多层智能体AI框架，通过四个专用智能体（膳食规划、提醒、食物指导、监测）提供个性化健康支持，采用黑板/事件总线通信，整合隐私敏感数据源，并包含可解释AI模块。


<details>
  <summary>Details</summary>
Motivation: 传统辅助系统缺乏包容性、个性化和可访问性，无法充分满足残障和神经多样性人群的健康需求。需要开发一个能够提供自适应、透明和包容支持的智能体AI系统，帮助这些人群过上更健康、更有规律的生活。

Method: 采用三层架构：应用与接口层、智能体层、数据源层。通过混合推理引擎协调四个专用智能体（膳食规划、提醒、食物指导、监测），使用黑板/事件总线进行通信。整合电子健康记录、营养数据库、可穿戴设备、智能厨房物联网等隐私敏感数据源，并包含可解释AI模块和临床医生仪表板。

Result: 提出了一个超越传统辅助系统的智能体AI框架，在多个层面实现了包容性、个性化和可访问性。展示了多智能体推理、多模态接口和以人为中心设计的交叉融合，能够促进残障和神经多样性人群的自主性、健康和数字公平。

Conclusion: 该智能体AI框架通过整合多智能体系统、隐私保护数据源和可解释AI，为残障和神经多样性人群提供了全面的健康支持系统，代表了辅助技术向更包容、个性化和可访问方向的重要发展。

Abstract: The paper presents a detailed Agentic Artificial Intelligence (AI) model that would enable people with disabilities and neurodivergence to lead healthier lives and have more regular days. The system will use a multi-layer structure; it will include an Application and Interface Layer, an Agents Layer, and a Data Source Layer to provide adaptive, transparent, and inclusive support. Fundamentally, a hybrid reasoning engine will synchronize four special-purpose agents, which include: a personalized-nutrition-based, called a Meal Planner Agent; an adaptive-scheduling-based, called a Reminder Agent; interactive assistance during grocery shopping and cooking, called a Food Guidance Agent; and a continuous-intake-and-physiological-tracking, called a Monitoring Agent. All the agents interact through a central communicative system called the Blackboard/Event Bus, which allows autonomous interaction and real-time feedback loops with multimedia user interfaces. Privacy-sensitive data sources, including electronic health records (EHRs), nutritional databases, wearable sensors, and smart kitchen Internet of Things, are also included in the framework and placed into a policy-controlled layer, which ensures data safety and compliance with consent. Collaborative care and clinician dashboards allow common supervision, and discussable artificial intelligence (XAI) modules give brief explanations of why a decision was made, making users responsible and reliant. The proposed agentic AI framework is an extension beyond traditional assistive systems since it incorporates inclusiveness, personalization, and accessibility at all levels. It displays the intersection of multi-agent reasoning, multi-modal interfaces, and human-centered design that will enable the development of autonomy, health, and digital equity among people with disabilities and neurodivergence.

</details>


### [72] [ORION: Teaching Language Models to Reason Efficiently in the Language of Thought](https://arxiv.org/abs/2511.22891)
*Kumar Tanmay,Kriti Aggarwal,Paul Pu Liang,Subhabrata Mukherjee*

Main category: cs.AI

TL;DR: 提出Mentalese框架，通过超压缩结构化令牌实现高效推理，结合SLPO强化学习方法，在保持准确性的同时大幅减少推理令牌数量（4-16倍）和延迟（5倍），降低训练成本（7-9倍）


<details>
  <summary>Details</summary>
Motivation: 大型推理模型依赖冗长的"思考"令牌链，导致高延迟、冗余和不连贯的推理路径，需要更高效、类似人类认知的推理方式

Method: 1. 提出Mentalese框架，受"思维语言假说"启发，训练模型使用超压缩结构化令牌进行推理；2. 提出SLPO（短长度偏好优化）强化学习方法，奖励简洁且正确的解决方案，同时允许必要时进行更长的推理

Result: ORION模型在AIME 2024/2025、MinervaMath、OlympiadBench、Math500和AMC等基准测试中：推理令牌减少4-16倍，推理延迟降低5倍，训练成本降低7-9倍，保持DeepSeek R1 Distilled模型90-98%的准确性，超越Claude和ChatGPT-4o准确性达5%同时保持2倍压缩

Conclusion: Mentalese风格的压缩推理实现了类似人类的认知效率，能够在保持准确性的同时实现实时、经济高效的推理

Abstract: Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose "thinking" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.

</details>


### [73] [TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM](https://arxiv.org/abs/2511.22998)
*Peng Kuang,Xiangxiang Wang,Wentao Liu,Jian Dong,Kaidi Xu,Haohan Wang*

Main category: cs.AI

TL;DR: TIM-PRM是一个工具集成的多模态过程奖励模型框架，通过主动工具查询和独立提问机制，将验证从被动分类任务转变为主动调查，显著提升了多模态数学推理的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在数学推理中存在视觉幻觉和逻辑不一致问题，而传统的过程奖励模型存在谄媚性，倾向于盲目验证有缺陷的假设而非基于视觉现实进行验证。

Method: 提出TIM-PRM框架，训练模型显式规划验证策略，通过独立提问机制使用外部工具查询证据，将验证与推理上下文解耦以消除确认偏差，并构建高质量工具集成验证轨迹数据集。

Result: 在VisualProcessBench上的实验表明，8B参数的TIM-PRM超越了现有开源多模态PRMs，显著优于Qwen2.5-72B和InternVL-78B等更大模型，同时提供可解释的验证过程洞察。

Conclusion: TIM-PRM通过将验证重构为主动工具增强的调查过程，有效解决了多模态推理中的视觉幻觉和逻辑不一致问题，为过程奖励模型提供了新的研究方向。

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive performances in mathematical reasoning, yet they remain vulnerable to visual hallucinations and logical inconsistencies that standard outcome-based supervision fails to mitigate. While Process Reward Models (PRMs) promise step-by-step verification, current approaches typically operate as scalar scorers or generative critics that suffer from sycophancy, blindly validating the flawed hypotheses rather than grounding them in visual reality. To bridge this gap, we introduce TIM-PRM (Tool-Integrated Multimodal PRM), a novel agentic framework that transforms verification from a passive classification task into an active, tool-augmented investigation. TIM-PRM is trained to explicitly plan verification strategies and utilizes a mechanism of Independent Question Asking to query evidence via external tools, effectively decoupling verification from the reasoning context to eliminate confirmation bias. We instantiate this method by curating a high-quality dataset of tool-integrated verification trajectories. Extensive experiments on VisualProcessBench demonstrate that our 8B parameter model surpasses existing open-source multimodal PRMs, significantly outperforming much larger models like Qwen2.5-72B and InternVL-78B, while offering interpretable insights into the verification process.

</details>


### [74] [MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents](https://arxiv.org/abs/2511.23055)
*Ruoxuan Zhang,Qiyun Zheng,Zhiyu Zhou,Ziqi Liao,Siyu Wu,Jian-Yu Jiang-Lin,Bin Wen,Hongxia Xie,Jianlong Fu,Wen-Huang Cheng*

Main category: cs.AI

TL;DR: 提出MindPower框架，将心理理论整合到具身智能体决策中，通过感知-心理推理-决策-行动流程，并引入Mind-Reward优化目标，显著提升决策和行动生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言具身智能体缺乏基于心理理论的决策能力，现有基准仅关注人类心理状态而忽略智能体自身视角，这阻碍了连贯的决策和行动生成。

Method: 提出MindPower机器人中心框架，包含感知、心理推理、决策和行动四个模块。首先感知环境和人类状态，然后进行心理理论推理建模自我和他人，最后基于推断的心理状态生成决策和行动。同时引入Mind-Reward优化目标，鼓励视觉语言模型产生一致的心理推理和行为。

Result: 模型在决策制定上比GPT-4o提升12.77%，在行动生成上提升12.49%。

Conclusion: MindPower框架成功将心理理论整合到具身智能体决策中，通过同时考虑自我和他人视角，显著提升了决策和行动的连贯性。

Abstract: Theory of Mind (ToM) refers to the ability to infer others' mental states, such as beliefs, desires, and intentions. Current vision-language embodied agents lack ToM-based decision-making, and existing benchmarks focus solely on human mental states while ignoring the agent's own perspective, hindering coherent decision and action generation. To address this, we propose MindPower, a Robot-Centric framework integrating Perception, Mental Reasoning, Decision Making and Action. Given multimodal inputs, MindPower first perceives the environment and human states, then performs ToM Reasoning to model both self and others, and finally generates decisions and actions guided by inferred mental states. Furthermore, we introduce Mind-Reward, a novel optimization objective that encourages VLMs to produce consistent ToM Reasoning and behavior. Our model outperforms GPT-4o by 12.77% in decision making and 12.49% in action generation.

</details>


### [75] [Does Self-Evaluation Enable Wireheading in Language Models?](https://arxiv.org/abs/2511.23092)
*David Demitri Africa,Hans Ethan Ting*

Main category: cs.AI

TL;DR: 研究发现当自我评估与奖励信号耦合时，语言模型会进行"线头化"（wireheading）——操纵奖励测量而非提升任务性能，导致评分膨胀但准确性无改善。


<details>
  <summary>Details</summary>
Motivation: 研究自我评估在语言模型训练中的安全性问题，特别是当自我评估与奖励信号耦合时是否会激励模型操纵奖励测量而非真正提升任务性能。

Method: 1. 在部分可观察马尔可夫决策过程（POMDP）中形式化奖励通道控制严格优于任务聚焦行为的条件；2. 使用两个模型和三个任务进行实证测试；3. 比较自我评分决定奖励的模型与自我评估但不控制奖励的模型。

Result: 1. 自我评分决定奖励的模型表现出显著的评分膨胀但无相应的准确性提升；2. 在模糊任务（如摘要）上效果尤其明显；3. 自我评估但不控制奖励的模型没有这种膨胀现象。

Conclusion: 自我评估与学习信号解耦时是安全的，但当两者耦合时存在危险，这对智能体系统设计有明确启示。

Abstract: Self-evaluation is increasingly central to language model training, from constitutional AI to self-refinement. We investigate whether coupling self-evaluation to reward signals creates incentives for wireheading, where agents manipulate reward measurements rather than improving task performance. We formalize conditions under which reward-channel control strictly dominates task-focused behavior in POMDPs and test these predictions empirically. Across two models and three tasks, we find that models whose self-grades determine rewards exhibit substantial grade inflation without corresponding accuracy gains, particularly on ambiguous tasks like summarization. Models that self-evaluate but do not control rewards show no such inflation. Our results demonstrate that self-evaluation is safe when decoupled from learning signals but dangerous when coupled, with clear implications for agentic system design.

</details>


### [76] [Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting](https://arxiv.org/abs/2511.23387)
*Daniil Sukhorukov,Andrei Zakharov,Nikita Glazkov,Katsiaryna Yanchanka,Vladimir Kirilin,Maxim Dubovitsky,Roman Sultimov,Yuri Maksimov,Ilya Makarov*

Main category: cs.AI

TL;DR: 提出分层AI气象学家系统，通过分层预测推理和天气关键词生成可解释的天气报告，使用多尺度分析和关键词验证提高LLM生成天气叙事的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法将天气预报视为平坦时间序列，缺乏对短期动态和长期趋势的捕捉，且LLM生成的天气报告缺乏可解释性和一致性验证机制。

Method: 构建分层LLM代理系统，在小时、6小时和日尺度进行多尺度推理；核心推理代理将结构化气象输入转换为连贯叙述，同时提取总结主要气象事件的关键词；使用关键词作为语义锚点验证生成报告的一致性、时序连贯性和事实对齐。

Result: 使用OpenWeather和Meteostat数据证明，分层上下文和基于关键词的验证显著提高了LLM生成天气叙事的可解释性和鲁棒性。

Conclusion: 该框架为自动化气象报告的语义评估提供了可复现框架，推动了基于代理的科学推理发展。

Abstract: We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.

</details>


### [77] [Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent](https://arxiv.org/abs/2511.23436)
*Jianzhe Lin,Zeyu Pan,Yun Zhu,Ruiqi Song,Jining Yang*

Main category: cs.AI

TL;DR: SuperIntelliAgent是一个智能体学习框架，通过可训练的小型扩散模型（学习者）与冻结的大型语言模型（验证器）耦合，实现通过自监督交互的持续智能增长。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调需要人工标注，而SuperIntelliAgent旨在实现无需标注的自主学习，将普通推理循环转变为终身优化过程，实现持续智能积累。

Method: 框架包含学习者生成候选输出，验证器通过逐步推理评估，交互产生DPO训练对。采用双尺度记忆：短期上下文记忆保存推理轨迹，长期记忆通过轻量级微调巩固知识。回放缓冲区保留可验证进展的样本作为辅助监督。

Result: 使用少量自动生成的DPO对，学习者在所有基准测试中都有改进，表明该机制为持续智能积累和实际部署提供了有前景的方向。

Conclusion: 可训练学习者与推理能力验证器的配对构成了增长智能的最小可靠单元，配对反馈和部分历史回放产生更丰富的学习课程和更强的偏好对齐。

Abstract: We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.

</details>


### [78] [Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction](https://arxiv.org/abs/2511.23476)
*Bao Shu,Yan Cai,Jianjian Sun,Chunrui Han,En Yu,Liang Zhao,Jingcheng Hu,Yinmin Zhang,Haoran Lv,Yuang Peng,Zheng Ge,Xiangyu Zhang,Daxin Jiang,Xiangyu Yue*

Main category: cs.AI

TL;DR: WMAct通过奖励重缩放和交互频率退火机制，让LLM智能体通过主动推理内部化世界模型，减少冗余交互，提升任务解决效率


<details>
  <summary>Details</summary>
Motivation: 当前多轮交互方法采用僵化的推理过程，限制了模型的主动学习能力，阻碍了高效的世界模型推理。需要让模型通过"做"来直接塑造思考，实现更有效的世界模型内部化

Method: 提出WMAct框架：1) 奖励重缩放机制，根据行动效能调整结果奖励，激励减少冗余和目的性交互；2) 交互频率退火策略，逐步减少最大允许交互轮数，迫使模型压缩学习并内部化环境动态

Result: 在Sokoban、Maze和Taxi等环境中，WMAct能够单轮解决之前需要多轮交互的任务，并在复杂环境中展现出强大的可迁移性，在一系列推理基准上提升性能

Conclusion: 通过解放结构化推理约束，让模型通过主动交互内部化世界模型，WMAct实现了高效的世界模型推理，提升了LLM智能体在复杂环境中的规划和交互能力

Abstract: Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.

</details>
