<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 21]
- [cs.SE](#cs.SE) [Total: 23]
- [cs.LG](#cs.LG) [Total: 42]
- [tldr.article](#tldr.article) [Total: 23]
- [cs.AI](#cs.AI) [Total: 46]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents](https://arxiv.org/abs/2602.06975)
*R. James Cotton,Thomas Leonard*

Main category: cs.CL

TL;DR: BiomechAgent是一个通过自然语言生成代码的AI代理，使临床医生无需编程技能即可进行生物力学分析，包括数据查询、可视化和临床推理。


<details>
  <summary>Details</summary>
Motivation: 虽然无标记运动捕捉技术使定量运动分析越来越普及，但分析生成的数据对于没有编程专业知识的临床医生仍然是一个障碍。需要一种工具来降低生物力学分析的门槛。

Method: 开发了BiomechAgent，这是一个代码生成的AI代理，通过自然语言界面支持生物力学分析。创建了系统化基准测试，涵盖数据检索、可视化、活动分类、时间分割和临床推理。评估了领域特定指令与通用提示的效果，并集成了经过验证的步态事件检测专用工具。

Result: BiomechAgent在数据检索和可视化任务上实现了稳健的准确性，并展示了新兴的临床推理能力。生物力学领域的特定指令显著优于通用提示，集成专用步态事件检测工具大幅提升了时空分析的准确性。使用本地开源模型而非前沿云端LLM时，除数据检索外其他领域性能大幅下降。

Conclusion: BiomechAgent使无标记运动捕捉的数据对最终用户更加有用和可访问，通过自然语言界面降低了生物力学分析的门槛，使临床医生无需编程技能即可进行复杂的分析。

Abstract: Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.

</details>


### [2] [Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks](https://arxiv.org/abs/2602.06976)
*Chen Shen,Wei Cheng,Jingyue Yang,Huan Zhang,Yuhan Wu,Wei Hu*

Main category: cs.CL

TL;DR: ILA-agent框架让LLM通过推理时与有限外部资源交互来学习陌生编程语言，显著优于检索增强基线


<details>
  <summary>Details</summary>
Motivation: LLM在陌生编程语言上的编码能力会显著下降，传统微调方法需要大量数据，需要探索低资源环境下的推理时语言学习方法

Method: 提出ILA-agent框架，将人类行为建模为工具集，让LLM通过与官方文档和执行环境的结构化交互来增量探索、应用和验证语言知识

Result: 在Cangjie-bench基准测试中，ILA-agent在代码生成、翻译和程序修复任务上显著优于检索增强基线方法

Conclusion: 推理时语言获取是有效的低资源学习范式，ILA-agent框架能显著提升LLM在陌生编程语言上的表现，但仍存在性能差距

Abstract: The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.

</details>


### [3] [Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs](https://arxiv.org/abs/2602.07181)
*Tianyu Zhao,Siqi Li,Yasser Shoukry,Salma Elmalaki*

Main category: cs.CL

TL;DR: 该论文提出通过人格特质作为潜在信号来优化LLM个性化回答，开发了PACIFIC人格标注偏好数据集和自动检索人格对齐偏好的框架，显著提升了回答准确性。


<details>
  <summary>Details</summary>
Motivation: 当前使用用户偏好个性化LLM回答的方法存在局限性，因为偏好信号可能嘈杂、不完整甚至误导性，直接应用会降低回答质量。作者观察到稳定的人格特质塑造日常偏好，因此研究将人格作为偏好背后的原则性潜在信号。

Method: 1) 通过实验验证人格对齐偏好的有效性；2) 构建PACIFIC数据集，包含1200个跨领域偏好陈述，标注了Big-Five（OCEAN）特质方向；3) 提出一个框架，使LLM能自动检索人格对齐偏好并在回答生成中整合它们。

Result: 使用人格对齐偏好显著提升了个性化问答性能：选择与用户推断人格一致的偏好，将答案选择准确率从29.25%提高到76%（相比随机选择偏好）。实验表明人格作为潜在信号能有效提升个性化回答质量。

Conclusion: 人格特质可以作为有效的潜在信号来优化LLM的个性化回答，通过人格对齐的偏好选择能显著提升回答准确性。提出的PACIFIC数据集和自动检索框架为基于人格的个性化LLM应用提供了实用工具。

Abstract: User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.

</details>


### [4] [Do Large Language Models Reflect Demographic Pluralism in Safety?](https://arxiv.org/abs/2602.07376)
*Usman Naseem,Gautam Siddharth Kashyap,Sushant Kumar Ray,Rafiq Ali,Ebad Shabbir,Abdullah Mohammad*

Main category: cs.CL

TL;DR: Demo-SafetyBench是一个解决LLM安全评估中人口统计学多样性不足的数据集，通过解耦价值框架与响应，实现可扩展且人口统计学鲁棒的多元安全评估。


<details>
  <summary>Details</summary>
Motivation: 现有对齐数据集（如ANTHROPIC-HH和DICES）使用人口统计学狭窄的标注者池，忽视了不同社区间安全感知的差异。LLM安全本质上是多元的，反映道德规范、文化期望和人口统计学背景的差异。

Method: 分为两个阶段：第一阶段将DICES提示重新分类为14个安全领域，保留人口统计学元数据，通过LLM扩展低资源领域并进行去重；第二阶段使用LLMs-as-Raters评估多元敏感性，采用平衡阈值实现高可靠性和低人口统计学敏感性。

Result: 构建了43,050个样本的数据集，平衡阈值（delta=0.5, tau=10）实现了高可靠性（ICC=0.87）和低人口统计学敏感性（DS=0.12），证明多元安全评估可以同时具备可扩展性和人口统计学鲁棒性。

Conclusion: Demo-SafetyBench通过直接建模人口统计学多元主义，解决了现有安全评估数据集的局限性，为更包容、更全面的LLM安全评估提供了可行方法。

Abstract: Large Language Model (LLM) safety is inherently pluralistic, reflecting variations in moral norms, cultural expectations, and demographic contexts. Yet, existing alignment datasets such as ANTHROPIC-HH and DICES rely on demographically narrow annotator pools, overlooking variation in safety perception across communities. Demo-SafetyBench addresses this gap by modeling demographic pluralism directly at the prompt level, decoupling value framing from responses. In Stage I, prompts from DICES are reclassified into 14 safety domains (adapted from BEAVERTAILS) using Mistral 7B-Instruct-v0.3, retaining demographic metadata and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based deduplication, yielding 43,050 samples. In Stage II, pluralistic sensitivity is evaluated using LLMs-as-Raters-Gemma-7B, GPT-4o, and LLaMA-2-7B-under zero-shot inference. Balanced thresholds (delta = 0.5, tau = 10) achieve high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12), confirming that pluralistic safety evaluation can be both scalable and demographically robust.

</details>


### [5] [DLLM Agent: See Farther, Run Faster](https://arxiv.org/abs/2602.07451)
*Huiling Zhen,Weizhe Lin,Renxi Liu,Kai Han,Yiming Li,Yuchuan Tian,Hanting Chen,Xiaoguang Li,Xiaosong Li,Chen Chen,Xianzhi Yu,Mingxuan Yuan,Youliang Yan,Peifeng Qin,Jun Wang,Yu Wang,Dacheng Tao,Yunhe Wang*

Main category: cs.CL

TL;DR: DLLM代理相比自回归代理在相同准确率下平均快30%以上，部分场景超过8倍加速，需要更少交互轮次和工具调用，但需要更强的工具调用训练和注意力掩码对齐。


<details>
  <summary>Details</summary>
Motivation: 探索扩散大语言模型在智能体多步决策中的潜力，研究在相同代理框架和监督下，扩散骨干是否带来不同的规划行为并转化为端到端效率提升。

Method: 在相同代理工作流(DeepDiver)中实例化DLLM和AR骨干，使用相同轨迹数据进行匹配的代理导向微调，创建可比较的扩散代理和自回归代理。

Result: 在相同准确率下，DLLM代理平均端到端快30%以上，部分案例超过8倍加速；需要更少交互轮次和工具调用；规划命中率更高，收敛更快；但需要更强的工具调用训练和注意力掩码对齐。

Conclusion: 扩散骨干在代理决策中具有效率优势，能产生更强的全局规划信号，但需要针对工具调用和多轮输入进行专门优化才能充分发挥潜力。

Abstract: Diffusion large language models (DLLMs) have emerged as an alternative to autoregressive (AR) decoding with appealing efficiency and modeling properties, yet their implications for agentic multi-step decision making remain underexplored. We ask a concrete question: when the generation paradigm is changed but the agent framework and supervision are held fixed, do diffusion backbones induce systematically different planning and tool-use behaviors, and do these differences translate into end-to-end efficiency gains? We study this in a controlled setting by instantiating DLLM and AR backbones within the same agent workflow (DeepDiver) and performing matched agent-oriented fine-tuning on the same trajectory data, yielding diffusion-backed DLLM Agents and directly comparable AR agents. Across benchmarks and case studies, we find that, at comparable accuracy, DLLM Agents are on average over 30% faster end to end than AR agents, with some cases exceeding 8x speedup. Conditioned on correct task completion, DLLM Agents also require fewer interaction rounds and tool invocations, consistent with higher planner hit rates that converge earlier to a correct action path with less backtracking. We further identify two practical considerations for deploying diffusion backbones in tool-using agents. First, naive DLLM policies are more prone to structured tool-call failures, necessitating stronger tool-call-specific training to emit valid schemas and arguments. Second, for multi-turn inputs interleaving context and action spans, diffusion-style span corruption requires aligned attention masking to avoid spurious context-action information flow; without such alignment, performance degrades. Finally, we analyze attention dynamics across workflow stages and observe paradigm-specific coordination patterns, suggesting stronger global planning signals in diffusion-backed agents.

</details>


### [6] [SED-SFT: Selectively Encouraging Diversity in Supervised Fine-Tuning](https://arxiv.org/abs/2602.07464)
*Yijie Chen,Yijin Liu,Fandong Meng*

Main category: cs.CL

TL;DR: SED-SFT提出一种选择性熵正则化方法，通过选择性掩码机制解决SFT中的模式崩溃问题，增强生成多样性以提升后续RL性能。


<details>
  <summary>Details</summary>
Motivation: 传统SFT使用交叉熵损失导致模式崩溃，模型过度集中于特定响应模式，缺乏分布多样性，严重限制了后续RL的探索效率。

Method: 提出SED-SFT框架，在优化目标中引入选择性熵正则化项和选择性掩码机制，基于标记探索空间自适应地鼓励多样性。

Result: 在8个数学基准测试中，SED-SFT显著增强生成多样性，计算开销可忽略，在Llama-3.2-3B-Instruct和Qwen2.5-Math-7B-Instruct上后续RL性能分别平均提升2.06和1.20分。

Conclusion: SED-SFT有效解决SFT中的模式崩溃问题，平衡多样性和准确性，为后续RL提供更好的初始化，显著提升整体性能。

Abstract: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has emerged as the standard post-training paradigm for large language models (LLMs). However, the conventional SFT process, driven by Cross-Entropy (CE) loss, often induces mode collapse, where models over-concentrate on specific response patterns. This lack of distributional diversity severely restricts the exploration efficiency required for subsequent RL. While recent studies have attempted to improve SFT by replacing the CE loss, aiming to preserve diversity or refine the update policy, they fail to adequately balance diversity and accuracy, thereby yielding suboptimal performance after RL. To address the mode collapse problem, we propose SED-SFT, which adaptively encourages diversity based on the token exploration space. This framework introduces a selective entropy regularization term with a selective masking mechanism into the optimization objective. Extensive experiments across eight mathematical benchmarks demonstrate that SED-SFT significantly enhances generation diversity with a negligible computational overhead increase compared with CE loss, yielding average improvements of 2.06 and 1.20 points in subsequent RL performance over standard CE-based baselines on Llama-3.2-3B-Instruct and Qwen2.5-Math-7B-Instruct, respectively. The code is publicly available at https://github.com/pppa2019/SED-SFT

</details>


### [7] [Letting Tutor Personas "Speak Up" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization](https://arxiv.org/abs/2602.07639)
*Jaewook Lee,Alexander Scarlatos,Simon Woodhead,Andrew Lan*

Main category: cs.CL

TL;DR: 本文提出使用激活空间转向向量来引导LLM模仿不同人类导师的教学风格，无需显式提示指令，从而捕捉导师风格的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的辅导系统通常只学习单一导师策略，无法捕捉真实世界中导师风格的多样性。真实导师会根据学生需求调整脚手架水平、指导直接性、反馈和情感支持，这些差异会影响对话动态和学生参与度。

Method: 修改双向偏好优化(BiPO)来学习转向向量，这是一个激活空间方向，可以将模型响应引导向特定导师风格。该方法直接从人类导师-学生对话数据中提取信号，无需显式提示指令。

Result: 转向向量能捕捉不同对话情境下的导师特定变化，提高与真实导师话语的语义对齐度，增加基于偏好的评估，同时基本保持词汇相似性。学习到的方向系数显示出可解释的结构，对应导师行为的一致差异。

Conclusion: 激活转向提供了一种有效且可解释的方法，使用直接从人类对话数据中提取的信号来控制LLM中的导师特定变化。

Abstract: With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.

</details>


### [8] [Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation](https://arxiv.org/abs/2602.07673)
*Jiangnan Fang,Cheng-Tse Liu,Hanieh Deilamsalehy,Nesreen K. Ahmed,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi*

Main category: cs.CL

TL;DR: LLM法官在摘要评估中存在偏见，随着被评估摘要与人工摘要相似度降低，LLM越来越偏好其他LLM生成的摘要而非人工摘要，且这种偏见普遍存在于测试的模型中。


<details>
  <summary>Details</summary>
Motivation: LLM法官在摘要评估中虽然能更好地捕捉语义信息，但存在长度、顺序等偏见，且容易受到对抗性提示的影响。先前研究对这些偏见的分析不够细致，特别是缺乏与明确重叠度指标的关联分析。

Method: 在摘要领域，将LLM法官偏见分析与人工撰写响应的重叠度函数关联。测试了9个参数从10亿到120亿的近期LLM模型，包括Gemma 3和LLaMA 3的变体，使用ROUGE和BLEU度量相似度。

Result: 发现LLM法官越来越偏好其他LLM生成的摘要而非人工摘要，随着被评估摘要与人工摘要相似度降低。这种模式存在于几乎所有测试模型中，且不受模型自身位置偏见影响。模型甚至难以评估重叠度有限的摘要。

Conclusion: 在摘要领域使用LLM作为法官时，不应仅依赖简单的比较，而需要采用超越简单比较的技术。

Abstract: Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.

</details>


### [9] [SRR-Judge: Step-Level Rating and Refinement for Enhancing Search-Integrated Reasoning in Search Agents](https://arxiv.org/abs/2602.07773)
*Chen Zhang,Kuicai Dong,Dexun Li,Wenjun Li,Qu Yang,Wei Han,Yong Liu*

Main category: cs.CL

TL;DR: SRR-Judge框架通过细粒度步级评估改进深度搜索代理的推理质量，结合迭代拒绝采样微调，在挑战性基准上实现超过10%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型推理模型的深度搜索代理通常仅使用结果监督进行训练，忽视了中间思维和行动的质量，这限制了搜索集成推理能力的提升。

Method: 提出SRR-Judge框架进行可靠的步级推理和搜索行动评估，集成到改进的ReAct式"评估-精炼"工作流中，使用SRR标注数据进行迭代拒绝采样微调。

Result: SRR-Judge比DeepSeek-V3.1等更大模型提供更可靠的步级评估，其评分与最终答案正确性强相关。基于SRR-Judge标注轨迹对齐策略带来显著性能提升，在挑战性深度搜索基准上平均绝对pass@1提升超过10%。

Conclusion: SRR-Judge框架通过细粒度步级评估和基于标注数据的策略对齐，有效提升了深度搜索代理的推理质量，为搜索集成推理提供了更可靠的训练方法。

Abstract: Recent deep search agents built on large reasoning models (LRMs) excel at complex question answering by iteratively planning, acting, and gathering evidence, a capability known as search-integrated reasoning. However, mainstream approaches often train this ability using only outcome-based supervision, neglecting the quality of intermediate thoughts and actions. We introduce SRR-Judge, a framework for reliable step-level assessment of reasoning and search actions. Integrated into a modified ReAct-style rate-and-refine workflow, SRR-Judge provides fine-grained guidance for search-integrated reasoning and enables efficient post-training annotation. Using SRR-annotated data, we apply an iterative rejection sampling fine-tuning procedure to enhance the deep search capability of the base agent. Empirically, SRR-Judge delivers more reliable step-level evaluations than much larger models such as DeepSeek-V3.1, with its ratings showing strong correlation with final answer correctness. Moreover, aligning the policy with SRR-Judge annotated trajectories leads to substantial performance gains, yielding over a 10 percent average absolute pass@1 improvement across challenging deep search benchmarks.

</details>


### [10] [Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents](https://arxiv.org/abs/2602.07796)
*Jiatong Li,Changdae Oh,Hyeong Kyu Choi,Jindong Wang,Sharon Li*

Main category: cs.CL

TL;DR: 研究发现，在用户参与的LLM智能体场景中，强制思考反而会降低性能，因为思考让智能体变得更"内向"，减少信息透露，削弱了与用户的信息交换。


<details>
  <summary>Details</summary>
Motivation: 虽然推理能力已被证明能提升LLM在复杂任务上的表现，但其在真实用户参与场景中的有效性尚不明确。本研究旨在探究显式思考在用户参与的LLM智能体中的实际效果。

Method: 采用7个模型、3个基准测试和2种思考实例化进行综合实验，通过定量响应分类分析和定性失败传播案例研究来评估效果。

Result: 与预期相反，强制思考在用户参与场景中常常适得其反，导致各种LLM的性能异常下降。关键发现是思考让智能体变得更"内向"，缩短响应并减少向用户的信息披露，从而削弱了智能体-用户信息交换并导致下游任务失败。

Conclusion: 信息透明度意识是未来真实场景中推理智能体设计的关键但未被充分探索的视角。明确提示信息透露能可靠地提高不同模型族的性能，表明主动透明度是智能体优化的重要杠杆。

Abstract: Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.

</details>


### [11] [TodoEvolve: Learning to Architect Agent Planning Systems](https://arxiv.org/abs/2602.07839)
*Jiaxi Liu,Yanzuo Jiang,Guibin Zhang,Zihan Zhang,Heng Chang,Zhenfei Yin,Qibing Ren,Junchi Yan*

Main category: cs.CL

TL;DR: TodoEvolve是一个元规划范式，能够自主合成并动态修订任务特定的规划架构，通过PlanFactory统一不同规划范式，并使用IGPO训练模型，在多个基准测试中超越手工设计的规划模块。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统主要依赖固定的、手工设计的规划结构，缺乏适应开放性问题结构多样性的灵活性，需要更灵活的规划方法。

Method: 1) 构建PlanFactory模块化设计空间，统一不同规划范式；2) 收集高质量规划轨迹；3) 使用阻抗引导偏好优化(IGPO)训练Todo-14B模型，该多目标强化学习目标鼓励生成性能好、稳定且令牌高效的规划系统。

Result: 在五个代理基准测试中，TodoEvolve始终超越精心设计的规划模块，同时保持经济的API成本和运行时开销。

Conclusion: TodoEvolve通过元规划方法成功解决了固定规划结构的局限性，为代理系统提供了灵活、自适应的规划能力。

Abstract: Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via \textit{Impedance-Guided Preference Optimization} (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.

</details>


### [12] [Evaluating and Calibrating LLM Confidence on Questions with Multiple Correct Answers](https://arxiv.org/abs/2602.07842)
*Yuhan Wang,Shiyu Ni,Zhikai Ding,Zihang Zhan,Yuanzi Li,Keping Bi*

Main category: cs.CL

TL;DR: 论文提出MACE基准测试，研究LLM在多正确答案场景下的置信度校准问题，并提出了SCA方法来解决多答案导致的置信度低估问题。


<details>
  <summary>Details</summary>
Motivation: 现有训练自由的置信度校准方法主要针对单答案问答场景研究，但在存在多个有效答案时，这些方法会失效，因为正确答案之间的分歧会导致置信度系统性低估。

Method: 1) 引入MACE基准测试：包含12,000个事实性问题，涵盖6个领域，具有不同数量的正确答案；2) 提出语义置信度聚合(SCA)：通过对多个高概率采样响应进行置信度聚合来解决多答案校准问题。

Result: 实验表明：随着答案基数增加，准确率提高但估计置信度持续下降，导致混合答案数量问题的严重校准错误。SCA在混合答案设置下实现了最先进的校准性能，同时在单答案问题上保持强校准。

Conclusion: 多正确答案场景对LLM置信度校准提出了新挑战，SCA方法能有效解决这一问题，在保持单答案校准性能的同时，显著改善多答案场景下的置信度估计。

Abstract: Confidence calibration is essential for making large language models (LLMs) reliable, yet existing training-free methods have been primarily studied under single-answer question answering. In this paper, we show that these methods break down in the presence of multiple valid answers, where disagreement among equally correct responses leads to systematic underestimation of confidence. To enable a systematic study of this phenomenon, we introduce MACE, a benchmark of 12,000 factual questions spanning six domains with varying numbers of correct answers. Experiments across 15 representative calibration methods and four LLM families (7B-72B) reveal that while accuracy increases with answer cardinality, estimated confidence consistently decreases, causing severe miscalibration for questions with mixed answer counts. To address this issue, we propose Semantic Confidence Aggregation (SCA), which aggregates confidence over multiple high-probability sampled responses. SCA achieves state-of-the-art calibration performance under mixed-answer settings while preserving strong calibration on single-answer questions.

</details>


### [13] [SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization](https://arxiv.org/abs/2602.07909)
*Taolin Zhang,Hang Guo,Wang Lu,Tao Dai,Shu-Tao Xia,Jindong Wang*

Main category: cs.CL

TL;DR: SparseEval：一种基于稀疏优化的高效LLM评估方法，通过梯度下降优化锚点权重和迭代精化策略，显著降低评估成本。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，评估其性能的计算成本急剧增加。传统方法需要对大量基准样本进行推理，导致高昂的计算开销。作者发现模型-项目性能矩阵具有稀疏性，可以利用代表性项目作为锚点进行高效评估。

Method: 将高效评估任务形式化为稀疏优化问题，提出SparseEval方法：1）首次采用梯度下降优化锚点权重；2）使用迭代精化策略选择锚点；3）利用MLP的表征能力处理稀疏优化；4）提出锚点重要性分数和候选重要性分数来评估每个项目的价值。

Result: 在多个基准测试上的广泛实验表明，该方法具有低估计误差和高Kendall's τ相关性，展现了在实际场景中的优越鲁棒性和实用性。

Conclusion: SparseEval通过利用模型-项目性能矩阵的稀疏性，提出了一种高效且准确的LLM评估框架，显著降低了评估成本，为大规模模型评估提供了实用解决方案。

Abstract: As large language models (LLMs) continue to scale up, their performance on various downstream tasks has significantly improved. However, evaluating their capabilities has become increasingly expensive, as performing inference on a large number of benchmark samples incurs high computational costs. In this paper, we revisit the model-item performance matrix and show that it exhibits sparsity, that representative items can be selected as anchors, and that the task of efficient benchmarking can be formulated as a sparse optimization problem. Based on these insights, we propose SparseEval, a method that, for the first time, adopts gradient descent to optimize anchor weights and employs an iterative refinement strategy for anchor selection. We utilize the representation capacity of MLP to handle sparse optimization and propose the Anchor Importance Score and Candidate Importance Score to evaluate the value of each item for task-aware refinement. Extensive experiments demonstrate the low estimation error and high Kendall's~$τ$ of our method across a variety of benchmarks, showcasing its superior robustness and practicality in real-world scenarios. Code is available at {https://github.com/taolinzhang/SparseEval}.

</details>


### [14] [The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation](https://arxiv.org/abs/2602.07996)
*Arash Marioriyad,Omid Ghahroodi,Ehsaneddin Asgari,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: LLM作为自动评估器时，其判决会受到无关上下文线索（如来源、时间、人口统计信息）的显著影响，但这些影响很少在评估理由中明确承认，存在解释差距。


<details>
  <summary>Details</summary>
Motivation: 研究LLM作为自动评估器时的忠实性问题，理想评估器应仅基于内容质量做出判断，不受无关上下文影响，并能透明反映决策因素。

Method: 通过控制性线索扰动实验，向评估提示中注入合成元数据标签，测试6个LLM评估器在ELI5（事实问答）和LitBench（创意写作）两个数据集上的表现，分析6类线索（来源、时间、年龄、性别、种族、教育程度）的影响。

Result: 发现LLM评估器对线索有显著行为影响（如专家>人类>LLM>未知的来源层级、新>旧的时间偏好、教育程度偏爱），但线索承认率通常接近零，表明即使线索驱动决策也很少在理由中报告。线索承认率还依赖于数据集。

Conclusion: 显著的判决敏感性与有限的线索承认揭示了LLM作为评估器流程中的解释差距，对研究和部署中基于模型的评估可靠性提出了担忧。

Abstract: Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.

</details>


### [15] [When Benign Inputs Lead to Severe Harms: Eliciting Unsafe Unintended Behaviors of Computer-Use Agents](https://arxiv.org/abs/2602.08235)
*Jaylen Jones,Zhehao Zhang,Yuting Ning,Eric Fosler-Lussier,Pierre-Luc St-Charles,Yoshua Bengio,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

TL;DR: 提出了AutoElicit框架，通过迭代扰动良性指令并利用CUA执行反馈，自动引发计算机使用代理的无意有害行为，揭示了前沿CUAs中的数百个安全风险。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理(CUAs)在自动化复杂操作系统工作流程方面潜力巨大，但即使在良性输入环境下也可能表现出偏离预期结果的不安全无意行为。目前对这种风险的探索主要停留在轶事层面，缺乏具体的特征描述和自动化方法来在现实CUA场景下主动发现长尾无意行为。

Method: 提出了AutoElicit框架：一个代理框架，通过迭代扰动良性指令并利用CUA执行反馈，在保持扰动现实性和良性前提下，引发严重危害行为。该方法定义了无意CUA行为的关键特征，自动引发这些行为，并分析它们如何从良性输入中产生。

Result: 使用AutoElicit从Claude 4.5 Haiku和Opus等最先进的CUAs中发现了数百个有害的无意行为。进一步评估了人工验证成功扰动的可转移性，发现各种其他前沿CUAs对无意行为存在持续易感性。

Conclusion: 这项工作为系统分析现实计算机使用环境中的无意行为奠定了基础，提供了首个概念和方法论框架来理解和缓解CUA安全风险。

Abstract: Although computer-use agents (CUAs) hold significant potential to automate increasingly complex OS workflows, they can demonstrate unsafe unintended behaviors that deviate from expected outcomes even under benign input contexts. However, exploration of this risk remains largely anecdotal, lacking concrete characterization and automated methods to proactively surface long-tail unintended behaviors under realistic CUA scenarios. To fill this gap, we introduce the first conceptual and methodological framework for unintended CUA behaviors, by defining their key characteristics, automatically eliciting them, and analyzing how they arise from benign inputs. We propose AutoElicit: an agentic framework that iteratively perturbs benign instructions using CUA execution feedback, and elicits severe harms while keeping perturbations realistic and benign. Using AutoElicit, we surface hundreds of harmful unintended behaviors from state-of-the-art CUAs such as Claude 4.5 Haiku and Opus. We further evaluate the transferability of human-verified successful perturbations, identifying persistent susceptibility to unintended behaviors across various other frontier CUAs. This work establishes a foundation for systematically analyzing unintended behaviors in realistic computer-use settings.

</details>


### [16] [Document Reconstruction Unlocks Scalable Long-Context RLVR](https://arxiv.org/abs/2602.08237)
*Yao Xiao,Lei Wang,Yue Deng,Guanzheng Chen,Ziqi Jin,Jung-jae Kim,Xiaoli Li,Roy Ka-wei Lee,Lidong Bing*

Main category: cs.CL

TL;DR: 提出一种无监督强化学习方法，通过让LLM重构被替换段落的长文档来提升长上下文能力，无需人工标注或教师模型监督。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法依赖昂贵的黄金标准答案或教师模型评估，成本高且耗时。本研究旨在探索无监督方法增强LLM长上下文能力，避免人工标注和教师模型监督的需求。

Method: 首先在长文档中用特殊占位符替换少数段落，然后通过强化学习训练LLM从候选选项集中正确识别和排序缺失段落来重构文档。这种方法使模型能够捕捉全局叙事连贯性。

Result: 在RULER和LongBench v2两个基准测试上验证了方法的有效性，在RULER上获得显著提升，在LongBench v2上也有合理改进，且无需手动整理的长上下文QA数据。进行了广泛的消融研究分析奖励设计、数据整理策略、训练方案和数据缩放效果的影响。

Conclusion: 提出了一种有效的无监督强化学习方法来增强LLM的长上下文能力，避免了传统方法对昂贵人工标注和教师模型监督的依赖。公开了代码、数据和模型。

Abstract: Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.

</details>


### [17] [Improving Data and Reward Design for Scientific Reasoning in Large Language Models](https://arxiv.org/abs/2602.08321)
*Zijie Chen,Zhenghao Lin,Xiao Liu,Zhenzhong Lan,Yeyun Gong,Peng Cheng*

Main category: cs.CL

TL;DR: Dr.SCI 是一个针对科学问答的大规模数据集和训练流程，通过改进的数据处理、课程学习和基于评分的强化学习，显著提升了语言模型在开放科学问题上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在处理开放科学问题时面临监督不可靠和评估困难的问题，主要瓶颈在于科学后训练的数据构建和奖励设计。

Method: 1) 构建Dr.SCI数据集：处理异构开源科学数据，包含100万问题，覆盖8个STEM学科，有明确的可验证/开放问题划分、可扩展难度标注和细粒度评分标准；2) Dr.SCI后训练流程：包含探索扩展SFT、动态难度课程学习和基于科学评分标准的强化学习三个组件。

Result: 使用Dr.SCI流程训练的Qwen3-4B-Base模型在GPQA-diamond上达到63.2分，在GPQA-general上达到32.4分，持续优于o1-mini和GPT-4o等强基线，在科学推理特别是开放问题设置上取得显著提升。

Conclusion: Dr.SCI数据集和训练流程有效解决了科学问答中的数据构建和奖励设计瓶颈，显著提升了语言模型的科学推理能力，特别是在开放问题上的表现。

Abstract: Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr.SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.

</details>


### [18] [Characterizing, Evaluating, and Optimizing Complex Reasoning](https://arxiv.org/abs/2602.08498)
*Haoran Zhang,Yafu Li,Zhi Wang,Zhilin Wang,Shunkai Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 该论文提出了一个统一框架来解决大型推理模型的三个核心问题：定义推理质量、评估复杂推理轨迹、以及利用评估信号进行优化。通过ME²原则从宏观和微观层面定义质量，基于DAG的成对评估方法，以及构建TRM-Preference数据集训练思考奖励模型，实验证明该方法能有效提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏对大型推理模型三个基本问题的统一解决方案：1) 如何定义高质量的推理；2) 如何可靠评估具有复杂内部结构的长推理轨迹；3) 如何利用这种评估信号进行推理优化。

Method: 1) 提出ME²原则，从宏观效率/有效性和微观效率/有效性两个层面定义推理质量；2) 将推理轨迹建模为有向无环图(DAG)，开发基于DAG的成对评估方法；3) 构建TRM-Preference数据集，训练思考奖励模型(TRM)来大规模评估推理质量。

Result: 实验表明思考奖励作为有效的优化信号：在测试时，选择更好的推理能带来更好的结果（最高19.3%提升）；在强化学习训练中，思考奖励能增强推理和性能（最高3.9%提升），在多样化任务中均有效。

Conclusion: 该研究提供了一个统一的视角来解决大型推理模型的核心挑战，通过ME²原则、DAG建模和思考奖励模型，能够有效定义、评估和优化复杂推理过程，为推理模型的进一步发展提供了重要基础。

Abstract: Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.

</details>


### [19] [Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation](https://arxiv.org/abs/2602.08600)
*Archchana Sindhujan,Girish A. Koushik,Shenbin Qian,Diptesh Kanojia,Constantin Orăsan*

Main category: cs.CL

TL;DR: 本文提出了ALOPE-RL框架，结合强化学习和错误感知奖励，用于资源稀缺的英语-马拉雅拉姆语机器翻译质量评估，在小型数据集上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译质量评估(QE)方法主要依赖标量质量分数，缺乏对翻译错误的明确解释，且在低资源语言（如马拉雅拉姆语）上由于标注数据有限而性能不佳。

Method: 1) 创建首个英语-马拉雅拉姆语片段级QE数据集，包含直接评估分数和翻译质量评注；2) 提出ALOPE-RL框架，基于策略的强化学习训练高效适配器，结合直接评估分数和翻译质量评注作为奖励信号；3) 使用LoRA和4位量化技术微调紧凑型LLM（≤4B参数）。

Result: ALOPE-RL在英语-马拉雅拉姆语QE任务上实现了最先进的性能，超越了更大的LLM基线和领先的基于编码器的QE模型，尽管训练数据规模很小。

Conclusion: 错误感知的策略学习能够在有限数据和计算预算下提供强大的QE性能，为低资源语言的质量评估提供了有效解决方案。

Abstract: Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.

</details>


### [20] [Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models](https://arxiv.org/abs/2602.08658)
*Mingzi Cao,Xingwei Tan,Mahmud Akhter,Marco Valentino,Maria Liakata,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文研究了如何通过符号任务训练将演绎、归纳、溯因三种基本推理范式迁移到LLM中，并在现实自然语言任务上验证其泛化能力，取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 虽然提升LLM推理能力已有大量研究，但三种基本推理范式（演绎、归纳、溯因）如何影响LLM的泛化能力尚未得到系统探索。研究者希望了解这些核心范式之间的相互作用如何影响LLM的推理行为。

Method: 首先收集符号任务推理轨迹数据集（每个任务针对一种推理范式），然后通过多种方法将这些技能迁移到LLM中，包括简单微调、增加模型深度、将密集模型转换为专家混合模型等。

Result: 该方法在现实跨域任务上表现出强大的泛化能力，性能提升显著（最高达14.60分），这些任务完全使用自然语言表述并包含真实世界知识。

Conclusion: 通过符号任务训练将基本推理范式迁移到LLM中，可以有效提升模型在现实自然语言任务上的泛化能力和推理性能。

Abstract: Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.

</details>


### [21] [WildReward: Learning Reward Models from In-the-Wild Human Interactions](https://arxiv.org/abs/2602.08829)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: WildReward：直接从用户交互中训练奖励模型，无需人工标注偏好对，在WildChat数据上表现优于传统奖励模型


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型依赖大规模人工标注的偏好对，而LLM的广泛部署产生了大量用户交互数据，这些隐式反馈信号能否直接用于训练奖励模型？

Method: 采用WildChat作为交互数据源，提出从用户反馈中提取可靠人类反馈的流程，通过序数回归直接训练WildReward，无需偏好对标注

Result: WildReward在性能上达到甚至超越传统奖励模型，具有更好的校准性和跨样本一致性，用户多样性直接提升模型性能，应用于在线DPO训练在多个任务上取得显著改进

Conclusion: 直接从用户交互中训练奖励模型是可行的，WildReward展示了这种方法在减少人工标注依赖的同时保持或提升性能的潜力

Abstract: Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [22] [AgentSpawn: Adaptive Multi-Agent Collaboration Through Dynamic Spawning for Long-Horizon Code Generation](https://arxiv.org/abs/2602.07072)
*Igor Costa*

Main category: cs.SE

TL;DR: AgentSpawn是一个动态多智能体协作架构，通过自动内存转移、自适应生成策略和并发一致性协议，解决长时程代码生成中的上下文持续性和领域适应性问题。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统使用静态工作流，无法在运行时分析揭示意外复杂性时进行自适应调整。长时程代码生成需要持续的上下文和跨领域的自适应专业知识。

Method: 提出AgentSpawn架构，包含三个核心组件：(1) 生成时的自动内存转移机制，(2) 由运行时复杂度指标触发的自适应生成策略，(3) 并发修改的一致性协议。

Result: 在SWE-bench等基准测试中，AgentSpawn比静态基线实现了34%的完成率提升，同时通过选择性切片减少了42%的内存开销。

Conclusion: AgentSpawn解决了现有研究中关于内存连续性、技能继承、任务恢复、运行时生成和并发一致性的五个关键差距，为动态多智能体协作提供了有效解决方案。

Abstract: Long-horizon code generation requires sustained context and adaptive expertise across domains. Current multi-agent systems use static workflows that cannot adapt when runtime analysis reveals unanticipated complexity. We propose AgentSpawn, an architecture enabling dynamic agent collaboration through: (1) automatic memory transfer during spawning, (2) adaptive spawning policies triggered by runtime complexity metrics, and (3) coherence protocols for concurrent modifications. AgentSpawn addresses five critical gaps in existing research around memory continuity, skill inheritance, task resumption, runtime spawning, and concurrent coherence. Experimental validation demonstrates AgentSpawn achieves 34% higher completion rates than static baselines on benchmarks like SWE-bench while reducing memory overhead by 42% through selective slicing.

</details>


### [23] [Comprehensive Evaluation of Large Language Models on Software Engineering Tasks: A Multi-Task Benchmark](https://arxiv.org/abs/2602.07079)
*Go Frendi Gunawan,Mukhlis Amien*

Main category: cs.SE

TL;DR: 该论文对11个先进LLM在5个软件工程任务上进行多任务评估，发现相同完美分数模型在完成时间、工具效率和成本上存在巨大差异，工具使用频率与成功率无关，识别出两种低效模式，并开源所有实验数据。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件工程中展现出强大能力，但缺乏覆盖多样化SE活动的综合基准测试。现有评估通常局限于单一任务，无法全面衡量LLM在实际软件开发中的综合表现。

Method: 构建多任务评估框架，对11个SOTA LLM在5个代表性软件工程任务（bug修复、功能开发、代码重构、技术文档撰写、研究综述）上进行测试。使用自动化验证框架同时衡量输出质量和完成效率。

Result: 关键发现：1) 相同完美分数模型在完成时间（22倍差异）、工具效率（49倍差异）和估计成本（53倍差异）上存在巨大差异；2) 工具使用频率与成功率无相关性；3) 识别出两种低效模式：循环低效和推理低效；4) 编码任务成功率100%，研究任务更具挑战性（90.9%）。

Conclusion: LLM在软件工程任务中表现优异但效率差异显著，仅关注准确性会忽略重要的效率指标。工具使用频率不代表成功，需要更全面的评估框架来平衡质量、效率和成本。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in software engineering, yet comprehensive benchmarks covering diverse SE activities remain limited. We present a multi-task evaluation of 11 state-of-the-art LLMs across five representative software engineering tasks: bug fixing, feature development, code refactoring, technical copywriting, and research synthesis. Our automated verification framework measures both output quality and completion efficiency. Key findings reveal that (1) models achieving identical perfect scores exhibit 22x variation in completion time, 49x variation in tool efficiency, and 53x variation in estimated cost; (2) tool usage frequency shows no correlation with success (r = 0.077, p = 0.575) - one model used 917 tool calls while another solved the same task with 3 calls; (3) we identify two distinct inefficiency patterns: loop inefficiency and inference inefficiency; and (4) coding tasks achieve 100 percent success while research tasks present greater challenges (90.9 percent). We release all experimental data, verification scripts, and analysis code for full reproducibility.

</details>


### [24] [CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs](https://arxiv.org/abs/2602.07080)
*Yicheng He,Zheng Zhao,Zhou Kaiyu,Bryan Dai,Jie Fu,Yonghui Yang*

Main category: cs.SE

TL;DR: 该论文提出了一种基于LLM内部计算结构的代码验证新方法，通过分析模型神经动态中的可解码信号来预测代码生成的逻辑正确性，无需依赖外部测试或评估机制。


<details>
  <summary>Details</summary>
Motivation: 当前代码验证范式严重依赖外部机制（如基于执行的单元测试或辅助LLM评估器），这些方法通常劳动密集型或受限于评估模型自身能力。因此需要探索能否从LLM内部计算结构评估其功能正确性。

Method: 受机制可解释性启发，将代码验证视为机制诊断任务，将模型的显式算法轨迹映射到行级归因图。通过分解复杂的残差流，识别模型内部电路中区分合理推理和逻辑失败的结构特征。

Result: 在Python、C++和Java上的分析证实，内在正确性信号在不同语法中具有鲁棒性。从这些内部图提取的拓扑特征比表面启发式方法更可靠地预测正确性，并能实现有针对性的因果干预来修复错误逻辑。

Conclusion: 研究发现内部自省可作为验证生成代码的可解码属性，为代码验证提供了新的内部视角方法。

Abstract: Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.

</details>


### [25] [Rethinking Scientific Modeling: Toward Physically Consistent and Simulation-Executable Programmatic Generation](https://arxiv.org/abs/2602.07083)
*Yongqing Jiang,Jianze Wang,Zhiqi Shen,Zhenghong Lin,Jiayuan Wang,Yijian Yang,Kaoshan Dai,Haoran Luo*

Main category: cs.SE

TL;DR: 提出一个物理一致的自动建筑建模框架，通过领域知识构建、约束导向的模型对齐和验证驱动的评估，生成可执行的仿真就绪模型。


<details>
  <summary>Details</summary>
Motivation: 结构建模是计算工程科学的基础，即使微小的物理不一致或规范违反也可能使下游仿真无效。虽然大语言模型在自动生成建模代码方面显示出潜力，但在严格的工程约束下，不可执行或物理不一致的输出仍然普遍存在。

Method: 提出一个物理一致的自动建筑建模框架，包括：1) 引入CivilInstruct作为领域特定数据集，形式化结构工程知识和约束推理；2) 采用两阶段微调策略来强制约束满足和API合规性；3) 提出MBEval作为验证驱动的基准，通过闭环验证评估可执行性和结构动力学一致性。

Result: 实验结果显示，在严格的验证指标上，该方法相比基线模型有持续改进，显著减少了幻觉输出和不合规输出。

Conclusion: 该框架通过整合领域知识、约束对齐和验证驱动评估，能够生成物理一致且可执行的建筑模型，解决了LLM在工程应用中常见的不可执行和物理不一致问题。

Abstract: Structural modeling is a fundamental component of computational engineering science, in which even minor physical inconsistencies or specification violations may invalidate downstream simulations. The potential of large language models (LLMs) for automatic generation of modeling code has been demonstrated. However, non-executable or physically inconsistent outputs remain prevalent under stringent engineering constraints. A framework for physics-consistent automatic building modeling is therefore proposed, integrating domain knowledge construction, constraint-oriented model alignment, and verification-driven evaluation. CivilInstruct is introduced as a domain-specific dataset that formalizes structural engineering knowledge and constraint reasoning to enable simulation-ready model generation. A two-stage fine-tuning strategy is further employed to enforce constraint satisfaction and application programming interface compliance, substantially reducing hallucinated and non-conforming outputs. MBEval is presented as a verification-driven benchmark that evaluates executability and structural dynamics consistency through closed-loop validation. Experimental results show consistent improvements over baselines across rigorous verification metrics. Our code is available at https://github.com/Jovanqing/AutoBM.

</details>


### [26] [Evaluating Retrieval-Augmented Generation Variants for Natural Language-Based SQL and API Call Generation](https://arxiv.org/abs/2602.07086)
*Michael Marketsmüller,Simon Martin,Tim Schlippe*

Main category: cs.SE

TL;DR: 该论文评估了三种RAG变体在SQL查询、REST API调用和动态任务分类中的表现，发现CoRAG在混合文档环境下表现最佳，检索对准确率提升至关重要。


<details>
  <summary>Details</summary>
Motivation: 企业系统需要自然语言接口将用户请求转换为结构化操作，但LLM在特定领域企业环境中的效果尚未充分探索，特别是需要同时处理检索和修改任务的情况。

Method: 使用SAP Transactional Banking作为企业用例，构建包含SQL和API两种模态的新测试数据集，评估标准RAG、Self-RAG和CoRAG三种RAG变体在18种实验配置下的表现。

Result: 无检索时准确率为0%，检索后执行准确率最高达79.30%，组件匹配准确率最高达78.86%。CoRAG在混合文档环境下表现最优，在组合任务中达到10.29%的精确匹配率。

Conclusion: 检索策略设计是生产级自然语言接口的关键因素，迭代查询分解优于top-k检索和二元相关性过滤，特别是在文档异构环境中。

Abstract: Enterprise systems increasingly require natural language interfaces that can translate user requests into structured operations such as SQL queries and REST API calls. While large language models (LLMs) show promise for code generation [Chen et al., 2021; Huynh and Lin, 2025], their effectiveness in domain-specific enterprise contexts remains underexplored, particularly when both retrieval and modification tasks must be handled jointly. This paper presents a comprehensive evaluation of three retrieval-augmented generation (RAG) variants [Lewis et al., 2021] -- standard RAG, Self-RAG [Asai et al., 2024], and CoRAG [Wang et al., 2025] -- across SQL query generation, REST API call generation, and a combined task requiring dynamic task classification. Using SAP Transactional Banking as a realistic enterprise use case, we construct a novel test dataset covering both modalities and evaluate 18 experimental configurations under database-only, API-only, and hybrid documentation contexts. Results demonstrate that RAG is essential: Without retrieval, exact match accuracy is 0% across all tasks, whereas retrieval yields substantial gains in execution accuracy (up to 79.30%) and component match accuracy (up to 78.86%). Critically, CoRAG proves most robust in hybrid documentation settings, achieving statistically significant improvements in the combined task (10.29% exact match vs. 7.45% for standard RAG), driven primarily by superior SQL generation performance (15.32% vs. 11.56%). Our findings establish retrieval-policy design as a key determinant of production-grade natural language interfaces, showing that iterative query decomposition outperforms both top-k retrieval and binary relevance filtering under documentation heterogeneity.

</details>


### [27] [Automated Modernization of Machine Learning Engineering Notebooks for Reproducibility](https://arxiv.org/abs/2602.07195)
*Bihui Jin,Kaiyuan Wang,Pengyu Nie*

Main category: cs.SE

TL;DR: MLEModernizer：一个LLM驱动的代理框架，用于修复因环境侵蚀而无法重现的机器学习工程笔记本，通过迭代执行和针对性修复使74.2%的不可重现笔记本变得可重现。


<details>
  <summary>Details</summary>
Motivation: 机器学习工程笔记本（如Jupyter笔记本）因硬件和软件生态系统的快速演变（环境侵蚀）而变得难以重现，阻碍了代码重用和科学进步。研究发现仅35.4%的Kaggle竞赛笔记本在当前环境中仍可重现，且环境回退（降级依赖）不仅无效反而引入新问题。

Method: 设计MLEModernizer框架，将当代环境作为固定约束，通过LLM驱动的代理方法迭代执行笔记本、收集执行反馈，并应用三种针对性修复：错误修复、运行时优化和分数校准。

Result: 在7,402个基线环境中不可重现的笔记本上，MLEModernizer成功使5,492个（74.2%）变得可重现，显著提升了机器学习工程笔记本的长期可维护性和重用性。

Conclusion: MLEModernizer有效解决了机器学习工程中的环境侵蚀问题，使从业者能够在硬件和软件生态系统持续演变的情况下验证、重用和维护机器学习工程工件。

Abstract: Interactive computational notebooks (e.g., Jupyter notebooks) are widely used in machine learning engineering (MLE) to program and share end-to-end pipelines, from data preparation to model training and evaluation. However, environment erosion-the rapid evolution of hardware and software ecosystems for machine learning-has rendered many published MLE notebooks non-reproducible in contemporary environments, hindering code reuse and scientific progress. To quantify this gap, we study 12,720 notebooks mined from 79 popular Kaggle competitions: only 35.4% remain reproducible today. Crucially, we find that environment backporting, i.e., downgrading dependencies to match the submission time, does not improve reproducibility but rather introduces additional failure modes.
  To address environment erosion, we design and implement MLEModernizer, an LLM-driven agentic framework that treats the contemporary environment as a fixed constraint and modernizes notebook code to restore reproducibility. MLEModernizer iteratively executes notebooks, collects execution feedback, and applies targeted fixes in three types: error-repair, runtime-reduction, and score-calibration. Evaluated on 7,402 notebooks that are non-reproducible under the baseline environment, MLEModernizer makes 5,492 (74.2%) reproducible. MLEModernizer enables practitioners to validate, reuse, and maintain MLE artifacts as the hardware and software ecosystems continue to evolve.

</details>


### [28] [Pull Requests as a Training Signal for Repo-Level Code Editing](https://arxiv.org/abs/2602.07457)
*Qinglin Zhu,Tianyu Chen,Shuai Lu,Lei Ji,Runcong Zhao,Murong Ma,Xiangxiang Dai,Yulan He,Lin Gui,Peng cheng,Yeyun Gong*

Main category: cs.SE

TL;DR: 提出Clean-PR训练范式，利用GitHub真实pull requests作为训练信号，通过重构和验证将嘈杂的diff转换为Search/Replace编辑块，构建了200万PR的数据集。通过中期训练和代理无监督微调，在SWE-bench上显著超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前仓库级代码编辑任务依赖复杂的代理框架，但尚不清楚这些能力有多少可以通过高质量训练信号内化到模型权重中。作者希望探索是否可以通过简化、无代理的协议有效内化仓库级代码理解和编辑能力。

Method: 1) 提出Clean-PR训练范式，利用GitHub真实pull requests作为训练信号；2) 开发可扩展的pipeline，通过重构和验证将嘈杂的PR diff转换为Search/Replace编辑块；3) 构建了包含200万pull requests、涵盖12种编程语言的最大公开语料库；4) 进行中期训练阶段，然后进行代理无监督微调，采用错误驱动的数据增强。

Result: 在SWE-bench上显著优于指令微调基线：在SWE-bench Lite上绝对提升13.6%，在SWE-bench Verified上绝对提升12.3%。这表明仓库级代码理解和编辑能力可以在简化、无代理的协议下有效内化到模型权重中。

Conclusion: 仓库级代码理解和编辑能力可以通过高质量训练信号有效内化到模型权重中，无需依赖复杂的推理时代理框架。Clean-PR范式为代码编辑模型训练提供了有效方法。

Abstract: Repository-level code editing requires models to understand complex dependencies and execute precise multi-file modifications across a large codebase. While recent gains on SWE-bench rely heavily on complex agent scaffolding, it remains unclear how much of this capability can be internalised via high-quality training signals. To address this, we propose Clean Pull Request (Clean-PR), a mid-training paradigm that leverages real-world GitHub pull requests as a training signal for repository-level editing. We introduce a scalable pipeline that converts noisy pull request diffs into Search/Replace edit blocks through reconstruction and validation, resulting in the largest publicly available corpus of 2 million pull requests spanning 12 programming languages. Using this training signal, we perform a mid-training stage followed by an agentless-aligned supervised fine-tuning process with error-driven data augmentation. On SWE-bench, our model significantly outperforms the instruction-tuned baseline, achieving absolute improvements of 13.6% on SWE-bench Lite and 12.3% on SWE-bench Verified. These results demonstrate that repository-level code understanding and editing capabilities can be effectively internalised into model weights under a simplified, agentless protocol, without relying on heavy inference-time scaffolding.

</details>


### [29] [ComPass: Contrastive Learning for Automated Patch Correctness Assessment in Program Repair](https://arxiv.org/abs/2602.07561)
*Quanjun Zhang,Ye Shang,Haichuan Hu,Chunrong Fang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: ComPass是一个基于预训练语言模型的自动补丁正确性评估方法，通过对比学习和数据增强技术来改进补丁过拟合问题，在真实补丁数据集上达到88.35%的准确率。


<details>
  <summary>Details</summary>
Motivation: 自动程序修复（APR）存在补丁过拟合问题，即补丁能通过现有测试套件但实际不正确。虽然最近有基于预训练语言模型（PLM）的自动补丁正确性评估方法，但由于训练范式和数据集的限制，效果仍不理想。

Method: ComPass采用对比学习和数据增强技术：1）使用代码转换规则为未标记的预训练语料和标记的微调补丁生成语义保持的代码片段；2）通过对比学习预训练PLM，捕捉相同语义但不同结构的代码特征；3）集成补丁代码片段的表示嵌入，并使用二元分类器联合微调PLM来评估补丁正确性。

Result: 在Defects4J的2274个真实补丁上进行实验，ComPass达到了88.35%的准确率，显著优于最先进的基线方法APPT。

Conclusion: ComPass通过结合对比学习和数据增强，有效改进了基于PLM的补丁正确性评估，为解决补丁过拟合问题提供了有前景的解决方案。

Abstract: Automated program repair (APR) attempts to reduce manual debugging efforts and plays a vital role in software maintenance. Despite remarkable progress, APR is still limited in generating overfitting patches, i.e., patches passing available test suites but incorrect. This issue, known as patch overfitting, has become a key concern in the APR community, with numerous approaches proposed to address it. Very recent work proposes a pre-trained language model (PLM)-based automated patch correctness assessment (APCA) approach, indicating the potential of such PLMs in reasoning about patch correctness. Despite being promising, it is still far from perfect due to various limitations, such as the training paradigm and training dataset. In this paper, we present ComPass, a PLM-based APCA approach that leverages contrastive learning and data augmentation to address the technical limitations of prior work. Our work is inspired by the opportunity to integrate contrastive learning with recent PLMs in the field of patch correctness assessment, where large-scale labeled patches are difficult to obtain. ComPass utilizes code transformation rules to generate semantic-preserving code snippets for both unlabeled pre-training corpus and labeled fine-tuning patches. ComPass then pre-trains PLMs with contrastive learning, which captures code features with the same semantics but different structures. ComPass finally integrates representation embeddings of patch code snippets and fine-tunes PLMs with a binary classifier jointly to assess patch code correctness. Experimental results on 2274 real-world patches from Defects4J demonstrate that ComPass achieves an accuracy of 88.35%, significantly outperforming state-of-the-art baseline APPT.

</details>


### [30] [HAIF: A Human-AI Integration Framework for Hybrid Team Operations](https://arxiv.org/abs/2602.07641)
*Marc Bara*

Main category: cs.SE

TL;DR: 提出HAIF框架，解决AI代理与人类在知识工作中协同工作的组织问题，包含核心原则、委托决策模型、分级自治和反馈机制，可集成到现有敏捷工作流中。


<details>
  <summary>Details</summary>
Motivation: 生成式AI、副驾驶和代理系统在知识工作中的快速部署造成了操作空白：现有框架无法解决AI代理与人类共同执行实质性委托任务的团队日常组织问题。

Method: 采用设计科学研究方法，提出人类-AI集成框架（HAIF），包含四个核心原则、正式的委托决策模型、具有量化过渡标准的分级自治以及反馈机制。

Result: 开发了工具无关的框架，包含领域特定验证清单、非软件环境适应指南，并分析了框架的结构限制，特别是连续人-AI协同生产对离散委托模型的挑战。

Conclusion: HAIF框架解决了AI能力越强越难证明监督必要性的采用悖论，但需要未来进行实证验证。框架设计为迭代采用，可集成到现有敏捷和看板工作流中。

Abstract: The rapid deployment of generative AI, copilots, and agentic systems in knowledge work has created an operational gap: no existing framework addresses how to organize daily work in teams where AI agents perform substantive, delegated tasks alongside humans. Agile, DevOps, MLOps, and AI governance frameworks each cover adjacent concerns but none models the hybrid team as a coherent delivery unit. This paper proposes the Human-AI Integration Framework (HAIF): a protocol-based, scalable operational system built around four core principles, a formal delegation decision model, tiered autonomy with quantifiable transition criteria, and feedback mechanisms designed to integrate into existing Agile and Kanban workflows without requiring additional roles for small teams. The framework is developed following a Design Science Research methodology. HAIF explicitly addresses the central adoption paradox: the more capable AI becomes, the harder it is to justify the oversight the framework demands-and yet the greater the consequences of not providing it. The paper includes domain-specific validation checklists, adaptation guidance for non-software environments, and an examination of the framework's structural limitations-including the increasingly common pattern of continuous human-AI co-production that challenges the discrete delegation model. The framework is tool-agnostic and designed for iterative adoption. Empirical validation is identified as future work.

</details>


### [31] [Debugging code world models](https://arxiv.org/abs/2602.07672)
*Babak Rahmani*

Main category: cs.SE

TL;DR: 本文研究了代码世界模型(CWMs)的局限性，发现主要错误源于密集运行时状态导致的令牌预算耗尽，以及字符串值状态处理中的子词分词限制。在长时程行为中，错误主要由动作生成引起而非状态传播。


<details>
  <summary>Details</summary>
Motivation: 代码世界模型通过预测每个执行命令后的运行时状态来模拟程序执行，提供了一种替代自然语言思维链的内部验证方法。然而，对这些模型的错误来源和局限性理解不足，需要系统分析其失败模式。

Method: 从两个互补角度研究CWMs：局部语义执行和长时程状态跟踪。在真实代码基准测试中识别主要失败模式，并使用受控的排列跟踪基准来隔离动作执行下的状态传播。

Result: 发现两个主要失败机制：1)密集运行时状态产生令牌密集型执行轨迹，导致长执行历史程序令牌预算耗尽；2)错误主要集中在字符串值状态，归因于子词分词限制而非程序结构。在长时程行为中，当用真实命令替换生成动作时，Transformer-based CWM能够准确传播状态。

Conclusion: CWMs的局限性主要源于令牌预算和分词问题，而非状态传播能力。这为设计更高效的监督和状态表示提供了方向，使其更好地与程序执行和数据类型对齐。

Abstract: Code World Models (CWMs) are language models trained to simulate program execution by predicting explicit runtime state after every executed command. This execution-based world modeling enables internal verification within the model, offering an alternative to natural language chain-of-thought reasoning. However, the sources of errors and the nature of CWMs' limitations remain poorly understood. We study CWMs from two complementary perspectives: local semantic execution and long-horizon state tracking. On real-code benchmarks, we identify two dominant failure regimes. First, dense runtime state reveals produce token-intensive execution traces, leading to token-budget exhaustion on programs with long execution histories. Second, failures disproportionately concentrate in string-valued state, which we attribute to limitations of subword tokenization rather than program structure. To study long-horizon behavior, we use a controlled permutation-tracking benchmark that isolates state propagation under action execution. We show that long-horizon degradation is driven primarily by incorrect action generation: when actions are replaced with ground-truth commands, a Transformer-based CWM propagates state accurately over long horizons, despite known limitations of Transformers in long-horizon state tracking. These findings suggest directions for more efficient supervision and state representations in CWMs that are better aligned with program execution and data types.

</details>


### [32] [Still Manual? Automated Linter Configuration via DSL-Based LLM Compilation of Coding Standards](https://arxiv.org/abs/2602.07783)
*Zejun Zhang,Yixin Gan,Zhenchang Xing,Tian Zhang,Yi Li,Xiwei Xu,Qinghua Lu,Liming Zhu*

Main category: cs.SE

TL;DR: LintCFG：基于LLM和DSL的自动化linter配置生成方法，通过领域特定语言表达编码规则，自动生成不同linter的配置文件，显著提高配置效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 手动配置linter复杂且需要专业知识，编程语言、编码标准和linter的多样性导致重复且维护密集的配置工作，需要自动化解决方案来减少人工工作量。

Method: 设计领域特定语言（DSL）以工具无关的方式表达编码规则，构建DSL配置指令，通过编译过程将自然语言编码标准解析为DSL表示，匹配配置指令，验证一致性，最终生成linter特定配置。

Result: 在Java编码标准的Checkstyle实验中，DSL表示达到90%以上的精确率和召回率，细粒度linter配置生成的准确率、精确率、召回率和F1分数接近70%（部分超过70%），精确率比基线提高100%以上。用户研究表明提高了开发者的配置效率，并展示了在JavaScript ESLint配置中的通用性。

Conclusion: LintCFG提供了一种自动化linter配置生成的有效方法，显著减少人工工作量，提高配置准确性和效率，具有跨编程语言、编码标准和linter的广泛适用性。

Abstract: Coding standards are essential for maintaining consistent and high-quality code across teams and projects. Linters help developers enforce these standards by detecting code violations. However, manual linter configuration is complex and expertise-intensive, and the diversity and evolution of programming languages, coding standards, and linters lead to repetitive and maintenance-intensive configuration work. To reduce manual effort, we propose LintCFG, a domain-specific language (DSL)-driven, LLM-based compilation approach to automate linter configuration generation for coding standards, independent of programming languages, coding standards, and linters. Inspired by compiler design, we first design a DSL to express coding rules in a tool-agnostic, structured, readable, and precise manner. Then, we build linter configurations into DSL configuration instructions. For a given natural language coding standard, the compilation process parses it into DSL coding standards, matches them with the DSL configuration instructions to set configuration names, option names and values, verifies consistency between the standards and configurations, and finally generates linter-specific configurations. Experiments with Checkstyle for Java coding standard show that our approach achieves over 90% precision and recall in DSL representation, with accuracy, precision, recall, and F1-scores close to 70% (with some exceeding 70%) in fine-grained linter configuration generation. Notably, our approach outperforms baselines by over 100% in precision. A user study further shows that our approach improves developers' efficiency in configuring linters for coding standards. Finally, we demonstrate the generality of the approach by generating ESLint configurations for JavaScript coding standards, showcasing its broad applicability across other programming languages, coding standards, and linters.

</details>


### [33] [Rethinking Code Complexity Through the Lens of Large Language Models](https://arxiv.org/abs/2602.07882)
*Chen Xie,Yuling Shi,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 本文提出LM-CC，一种从大语言模型视角设计的代码复杂度度量，发现传统复杂度指标与LLM性能无一致相关性，而LM-CC能更好地预测LLM处理代码的难度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码理解和生成任务上的快速发展，一个重要但未被充分探索的问题是：传统的代码复杂度指标（如圈复杂度）是否能有效表征LLM处理代码时的实际困难？作者发现传统指标与LLM性能缺乏一致性关联。

Method: 提出LM-CC度量方法，其核心前提是LLM感知的难度由程序语义的非线性驱动。方法包括：基于熵将程序分解为语义单元，将这些单元组织成组合层次结构，并通过组合层级和分支引起的分歧的原则性聚合来量化复杂度，捕捉代码处理过程中的累积模型不确定性。

Result: 实验表明，在控制代码长度后，传统复杂度指标与LLM性能无一致相关性。而LM-CC不仅比传统指标与LLM性能有更强的相关性，而且降低LM-CC能直接提升任务性能。

Conclusion: 传统代码复杂度指标无法有效表征LLM处理代码的难度，需要从LLM视角重新定义复杂度度量。LM-CC作为一种新的度量方法，能更好地预测LLM性能，并为优化LLM代码处理提供指导。

Abstract: Code complexity metrics such as cyclomatic complexity have long been used to assess software quality and maintainability. With the rapid advancement of large language models (LLMs) on code understanding and generation tasks, an important yet underexplored question arises: do these traditional complexity metrics meaningfully characterize the difficulty LLMs experience when processing code? In this work, we empirically demonstrate that, after controlling for code length, classical metrics exhibit no consistent correlation with LLM performance, revealing a fundamental mismatch with model-perceived difficulty. To address this gap, we propose LM-CC, a novel code complexity metric designed from the perspective of LLMs. The core premise of LM-CC is that LLM-perceived difficulty is driven by the nonlinearity of program semantics. Accordingly, we decompose programs into semantic units based on entropy, organize these units into a compositional hierarchy, and quantify complexity as a principled aggregation of compositional level and branching-induced divergence, capturing cumulative model uncertainty during code processing. Our extensive experiments show that LM-CC not only correlates more strongly with LLM performance than traditional metrics but also that lowering it directly enhances task performance.

</details>


### [34] [Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents](https://arxiv.org/abs/2602.07900)
*Zhi Chen,Zhensu Sun,Yuling Shi,Chao Peng,Xiaodong Gu,David Lo,Lingxiao Jiang*

Main category: cs.SE

TL;DR: 研究发现当前LLM代码代理在解决仓库级问题时编写的测试对问题解决效果影响有限，测试更多作为观察反馈渠道而非正式断言检查


<details>
  <summary>Details</summary>
Motivation: LLM代码代理在解决仓库级问题时经常编写测试，但GPT-5.2几乎不写测试却能取得与顶级代理相当的性能，这引发了对代理编写测试实际价值的质疑

Method: 1) 在SWE-bench Verified上分析六个最先进LLM的代理轨迹；2) 比较已解决和未解决任务中的测试编写频率；3) 分析测试类型（打印语句vs断言检查）；4) 通过修改四个代理的提示语进行控制实验，增加或减少测试编写

Result: 1) 测试编写普遍但已解决和未解决任务的测试编写频率相似；2) 测试主要作为观察反馈渠道，代理更偏好值揭示的打印语句而非正式断言检查；3) 改变测试编写量对最终结果无显著影响

Conclusion: 当前代理的测试编写实践在自主软件工程任务中可能只提供边际效用，测试更多是模仿人类实践而非有效改进问题解决

Abstract: Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.
  To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.

</details>


### [35] [Agent Skills: A Data-Driven Analysis of Claude Skills for Extending Large Language Model Functionality](https://arxiv.org/abs/2602.08004)
*George Ling,Shanshan Zhong,Richard Huang*

Main category: cs.SE

TL;DR: 对40,285个公开Agent技能的大规模分析显示：技能发布呈短期爆发模式，内容高度集中于软件工程工作流，存在显著的供需失衡，生态系统同质化严重，并存在非平凡的安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着Agent技能在公共市场中的激增，目前缺乏对技能类型、用户采用模式以及潜在风险的系统性了解。本研究旨在通过大规模数据分析来填补这一空白。

Method: 对来自主要市场的40,285个公开技能进行大规模数据驱动分析，考察技能发布模式、内容分布、采用情况、供需关系、生态系统特征和安全风险。

Result: 技能发布呈现短期爆发模式，与社区关注度变化同步；内容高度集中于软件工程工作流，但信息检索和内容创建技能采用率较高；存在显著的供需失衡；大多数技能在典型提示预算内，但长度分布呈现重尾特征；生态系统同质化严重，存在广泛的意图级冗余；识别出非平凡安全风险，包括支持状态更改或系统级操作的技能。

Conclusion: Agent技能作为新兴的Agent基础设施层，呈现出集中化、同质化和安全风险并存的特征。研究结果为技能重用、标准化和安全意识设计提供了量化依据。

Abstract: Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they pose. To answer these questions, we conduct a large-scale, data-driven analysis of 40,285 publicly listed skills from a major marketplace. Our results show that skill publication tends to occur in short bursts that track shifts in community attention. We also find that skill content is highly concentrated in software engineering workflows, while information retrieval and content creation account for a substantial share of adoption. Beyond content trends, we uncover a pronounced supply-demand imbalance across categories, and we show that most skills remain within typical prompt budgets despite a heavy-tailed length distribution. Finally, we observe strong ecosystem homogeneity, with widespread intent-level redundancy, and we identify non-trivial safety risks, including skills that enable state-changing or system-level actions. Overall, our findings provide a quantitative snapshot of agent skills as an emerging infrastructure layer for agents and inform future work on skill reuse, standardization, and safety-aware design.

</details>


### [36] [Integrating Code Metrics into Automated Documentation Generation for Computational Notebooks](https://arxiv.org/abs/2602.08133)
*Mojtaba Mostafavi Ghahfarokhi,Hamed Jahantigh,Alireza Asadi,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 论文提出将源代码度量指标作为辅助信号用于自动文档生成，在计算笔记本数据上验证了代码度量能提升文档生成的准确性和上下文相关性。


<details>
  <summary>Details</summary>
Motivation: 自动文档生成方法常忽视代码的结构和量化特征，而这些特征对代码可读性和理解很重要。现有方法缺乏对代码度量指标的有效利用，特别是在计算笔记本这种集成代码、叙述和结果但文档不一致的流行媒介中。

Method: 采用两阶段方法：1) 改进CodeSearchNet数据集构建流程，从1700多万个代码和markdown单元中创建专门数据集，经过结构和语义过滤得到约36,734个高质量(代码, markdown)对；2) 评估两种建模范式（轻量级CNN-RNN架构和少样本GPT-3.5架构），在有/无代码度量信息的情况下进行对比。

Result: 加入代码度量信息显著提升了生成文档的质量：CNN-RNN架构在BLEU-1上提升6%，ROUGE-L F1提升3%；LLM架构在BERTScore F1上提升9%。这表明代码度量提供了有价值的结构上下文。

Conclusion: 将代码度量作为辅助信号能有效增强自动文档生成，在不同模型家族中都显示出改进效果，为代码理解和文档生成提供了结构化的补充信息。

Abstract: Effective code documentation is essential for collaboration, comprehension, and long-term software maintainability, yet developers often neglect it due to its repetitive nature. Automated documentation generation has evolved from heuristic and rule-based methods to neural network-based and large language model (LLM)-based approaches. However, existing methods often overlook structural and quantitative characteristics of code that influence readability and comprehension. Prior research suggests that code metrics capture information relevant to program understanding. Building on these insights, this paper investigates the role of source code metrics as auxiliary signals for automated documentation generation, focusing on computational notebooks, a popular medium among data scientists that integrates code, narrative, and results but suffers from inconsistent documentation. We propose a two-stage approach. First, the CodeSearchNet dataset construction process was refined to create a specialized dataset from over 17 million code and markdown cells. After structural and semantic filtering, approximately 36,734 high-quality (code, markdown) pairs were extracted. Second, two modeling paradigms, a lightweight CNN-RNN architecture and a few-shot GPT-3.5 architecture, were evaluated with and without metric information. Results show that incorporating code metrics improves the accuracy and contextual relevance of generated documentation, yielding gains of 6% in BLEU-1 and 3% in ROUGE-L F1 for CNN-RNN-based architecture, and 9% in BERTScore F1 for LLM-based architecture. These findings demonstrate that integrating code metrics provides valuable structural context, enhancing automated documentation generation across diverse model families.

</details>


### [37] [Test vs Mutant: Adversarial LLM Agents for Robust Unit Test Generation](https://arxiv.org/abs/2602.08146)
*Pengyu Chang,Yixiong Fang,Silin Chen,Yuling Shi,Beijun Shen,Xiaodong Gu*

Main category: cs.SE

TL;DR: AdverTest：一个对抗性框架，通过两个交互代理（测试生成代理和变异生成代理）的对抗循环，提升LLM生成测试用例的bug检测能力，在Defects4J数据集上显著提高了故障检测率。


<details>
  <summary>Details</summary>
Motivation: 现有测试生成方法存在局限性：基于搜索的方法覆盖率虽高但可读性差，LLM方法可读性好但覆盖率和可编译性低。更重要的是，现有研究很少关注提升bug检测的鲁棒性，特别是暴露边界情况和脆弱执行路径的能力。

Method: 提出AdverTest对抗框架，包含两个交互代理：测试用例生成代理(T)和变异生成代理(M)。两者进行对抗循环：M持续创建新的变异体"攻击"T当前测试套件的盲点，T迭代优化测试用例以"杀死"M生成的挑战性变异体。该循环由覆盖率和变异分数共同指导，使系统协同进化以获得高测试覆盖率和bug检测能力。

Result: 在Defects4J数据集上的实验结果显示：相比现有最佳LLM方法，故障检测率提升8.56%；相比EvoSuite提升63.30%。同时提高了行覆盖率和分支覆盖率。

Conclusion: AdverTest通过对抗性学习框架有效提升了LLM生成测试用例的bug检测能力，解决了现有方法在鲁棒性bug检测方面的不足，为软件测试自动化提供了新思路。

Abstract: Software testing is a critical, yet resource-intensive phase of the software development lifecycle. Over the years, various automated tools have been developed to aid in this process. Search-based approaches typically achieve high coverage but produce tests with low readability, whereas large language model (LLM)-based methods generate more human-readable tests but often suffer from low coverage and compilability. While the majority of research efforts have focused on improving test coverage and readability, little attention has been paid to enhancing the robustness of bug detection, particularly in exposing corner cases and vulnerable execution paths. To address this gap, we propose AdverTest, a novel adversarial framework for LLM-powered test case generation. AdverTest comprises two interacting agents: a test case generation agent (T) and a mutant generation agent (M). These agents engage in an adversarial loop, where M persistently creates new mutants "hacking" the blind spots of T's current test suite, while T iteratively refines its test cases to "kill" the challenging mutants produced by M. This interaction loop is guided by both coverage and mutation scores, enabling the system to co-evolve toward both high test coverage and bug detection capability. Experimental results in the Defects4J dataset show that our approach improves fault detection rates by 8.56% over the best existing LLM-based methods and by 63.30% over EvoSuite, while also improving line and branch coverage.

</details>


### [38] [Adoption of Large Language Models in Scrum Management: Insights from Brazilian Practitioners](https://arxiv.org/abs/2602.08192)
*Mirko Perkusich,Danyllo Albuquerque,Allysson Allex Araújo,Matheus Paixão,Rohit Gheyi,Marcos Kalinowski,Angelo Perkusich*

Main category: cs.SE

TL;DR: 本文通过调查70名巴西专业人士，首次实证研究了LLM在Scrum管理活动中的应用现状，揭示了高使用频率、主要集中于Scrum实践探索、生产力提升显著但存在输出准确性、保密性和幻觉等风险。


<details>
  <summary>Details</summary>
Motivation: Scrum作为广泛采用的敏捷项目管理方法，其知识密集型实践与新兴的LLM技术存在结合潜力。然而现有研究主要关注LLM在编码、测试等技术活动中的应用，对于管理相关的Scrum活动支持缺乏实证证据。

Method: 通过对70名巴西专业人士进行调查研究，其中49人积极使用Scrum，33人报告在Scrum实践中使用LLM助手。采用问卷调查方法收集数据，分析LLM在Scrum管理活动中的使用模式、频率、熟练度和影响。

Result: 调查显示：85%受访者具备中高级LLM熟练度，52%每日使用；LLM主要用于探索Scrum实践，对工件和事件提供针对性支持；主要效益包括生产力提升78%和减少手动工作75%；但存在输出"几乎正确"81%、保密担忧63%、使用中幻觉59%等风险。

Conclusion: 本研究首次实证描述了LLM在Scrum管理中的应用现状，识别了当前实践模式，量化了效益与风险，为敏捷环境中负责任地采用和集成LLM技术提供了方向指导。

Abstract: Scrum is widely adopted in software project management due to its adaptability and collaborative nature. The recent emergence of Large Language Models (LLMs) has created new opportunities to support knowledge-intensive Scrum practices. However, existing research has largely focused on technical activities such as coding and testing, with limited evidence on the use of LLMs in management-related Scrum activities. In this study, we investigate the use of LLMs in Scrum management activities through a survey of 70 Brazilian professionals. Among them, 49 actively use Scrum, and 33 reported using LLM-based assistants in their Scrum practices. The results indicate a high level of proficiency and frequent use of LLMs, with 85% of respondents reporting intermediate or advanced proficiency and 52% using them daily. LLM use concentrates on exploring Scrum practices, with artifacts and events receiving targeted yet uneven support, whereas broader management tasks appear to be adopted more cautiously. The main benefits include increased productivity (78%) and reduced manual effort (75%). However, several critical risks remain, as respondents report 'almost correct' outputs (81%), confidentiality concerns (63%), and hallucinations during use (59%). This work provides one of the first empirical characterizations of LLM use in Scrum management, identifying current practices, quantifying benefits and risks, and outlining directions for responsible adoption and integration in Agile environments.

</details>


### [39] [Specification Vibing for Automated Program Repair](https://arxiv.org/abs/2602.08263)
*Taohong Zhu,Lucas C. Cordeiro,Mustafa A. Mustafa,Youcheng Sun*

Main category: cs.SE

TL;DR: VibeRepair是一种基于规范的程序修复方法，将修复视为行为规范修复而非代码编辑，通过将错误代码转换为结构化行为规范，修复规范偏差，然后基于修正后的规范合成代码，显著提升了修复效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自动程序修复方法大多是代码中心的，直接重写源代码可能导致幻觉修复和行为不一致。需要一种更易于LLM理解的表示形式，以实现更准确的理解、分析和修复对齐。

Method: VibeRepair采用规范中心的修复范式：1) 将错误代码转换为结构化行为规范；2) 推断并修复规范偏差；3) 基于修正后的规范严格合成代码。包含按需推理组件，通过程序分析和历史修复证据丰富困难案例。

Result: 在Defects4J v1.2上正确修复174个bug，比最强基线多28个（提升19%）；在v2.0上修复178个bug，比先前方法多33个（提升23%）。在训练期后的真实世界基准测试中也表现出有效性和泛化性。

Conclusion: 通过将修复中心放在明确的行为意图上，VibeRepair为"氛围"编码时代重构了自动程序修复：让行为先行，代码自然跟随。

Abstract: Large language model (LLM)-driven automated program repair (APR) has advanced rapidly, but most methods remain code-centric: they directly rewrite source code and thereby risk hallucinated, behaviorally inconsistent fixes. This limitation suggests the need for an alternative repair paradigm that relies on a representation more accessible to LLMs than raw code, enabling more accurate understanding, analysis, and alignment during repair. To address this gap, we propose VibeRepair, a specification-centric APR technique that treats repair as behavior-specification repair rather than ad-hoc code editing. VibeRepair first translates buggy code into a structured behavior specification that captures the program's intended runtime behavior, then infers and repairs specification misalignments, and finally synthesizes code strictly guided by the corrected behavior specification. An on-demand reasoning component enriches hard cases with program analysis and historical bug-fix evidence while controlling cost. Across Defects4J and real-world benchmarks and multiple LLMs, VibeRepair demonstrates consistently strong repair effectiveness with a significantly smaller patch space. On Defects4J v1.2, VibeRepair correctly repairs 174 bugs, exceeding the strongest state-of-the-art baseline by 28 bugs, which corresponds to a 19% improvement. On Defects4J v2.0, it repairs 178 bugs, outperforming prior approaches by 33 bugs, representing a 23% improvement. Evaluations on real-world benchmarks collected after the training period of selected LLMs further confirm its effectiveness and generalizability. By centering repair on explicit behavioral intent, VibeRepair reframes APR for the era of "vibe" coding: make the behavior sing, and the code will follow.

</details>


### [40] [Automating Computational Reproducibility in Social Science: Comparing Prompt-Based and Agent-Based Approaches](https://arxiv.org/abs/2602.08561)
*Syed Mehtab Hussain Shah,Frank Hopfgartner,Arnim Bleier*

Main category: cs.SE

TL;DR: 研究评估了大型语言模型和AI代理能否自动诊断和修复计算研究中的可重复性问题，在R语言社会科学研究测试床上，代理工作流（成功率69-96%）显著优于提示工作流（31-79%）。


<details>
  <summary>Details</summary>
Motivation: 计算研究的可重复性常因缺失包、路径问题、版本冲突或逻辑不完整而失败，即使共享了材料。本研究旨在探索AI能否自动化诊断和修复这些失败，使计算结果更易复现和验证。

Method: 使用五个完全可复现的R语言社会科学研究构建受控测试床，注入从简单到复杂的现实故障。测试两种自动修复工作流：1）提示式工作流（使用结构化提示反复查询语言模型）；2）代理式工作流（自主检查文件、修改代码、重新运行分析）。在干净的Docker环境中评估。

Result: 提示式工作流的复现成功率在31-79%之间，性能受提示上下文和错误复杂度影响显著，复杂案例从额外上下文中获益最大。代理式工作流表现显著更好，在所有复杂度水平上成功率69-96%。

Conclusion: 自动化工作流，特别是代理式系统，能显著减少手动工作量并提高各种错误类型的复现成功率。与先前基准不同，该测试床在受控故障模式下隔离了发布后修复，允许直接比较提示式和代理式方法。

Abstract: Reproducing computational research is often assumed to be as simple as rerunning the original code with provided data. In practice, missing packages, fragile file paths, version conflicts, or incomplete logic frequently cause analyses to fail, even when materials are shared. This study investigates whether large language models and AI agents can automate the diagnosis and repair of such failures, making computational results easier to reproduce and verify. We evaluate this using a controlled reproducibility testbed built from five fully reproducible R-based social science studies. Realistic failures were injected, ranging from simple issues to complex missing logic, and two automated repair workflows were tested in clean Docker environments. The first workflow is prompt-based, repeatedly querying language models with structured prompts of varying context, while the second uses agent-based systems that inspect files, modify code, and rerun analyses autonomously. Across prompt-based runs, reproduction success ranged from 31-79 percent, with performance strongly influenced by prompt context and error complexity. Complex cases benefited most from additional context. Agent-based workflows performed substantially better, with success rates of 69-96 percent across all complexity levels. These results suggest that automated workflows, especially agent-based systems, can significantly reduce manual effort and improve reproduction success across diverse error types. Unlike prior benchmarks, our testbed isolates post-publication repair under controlled failure modes, allowing direct comparison of prompt-based and agent-based approaches.

</details>


### [41] [Taming Scylla: Understanding the multi-headed agentic daemon of the coding seas](https://arxiv.org/abs/2602.08765)
*Micah Villmow*

Main category: cs.SE

TL;DR: Scylla是一个用于评估智能编码代理的框架，通过七层测试(T0-T6)进行结构化消融研究，使用成本通过率(CoP)作为核心指标来衡量架构复杂性与效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM工具正在快速自动化软件开发任务，但缺乏严谨的方法来评估不同架构选择（提示、技能、工具、多代理设置）对能力和成本的实际影响。

Method: 提出Scylla评估框架，使用七层测试(T0-T6)逐步增加复杂性，通过结构化消融研究隔离影响结果的因素。核心指标是成本通过率(CoP)，框架与模型无关，可与任何CLI工具配合使用。使用多个LLM评委(Opus 4.5, Sonnet 4.5, Haiku 4.5)进行共识评估，评委通过直接测试、人工设计的LLM评估量表和定性评估来评分。

Result: 开发了一个可重复的框架，量化了代理复杂性与实际结果之间的权衡，表明架构复杂性并不总是能提高质量。

Conclusion: Scylla框架为评估智能编码工具提供了系统方法，揭示了架构复杂性与性能效率之间的实际关系，为工具设计提供了实证指导。

Abstract: LLM-based tools are automating more software development tasks at a rapid pace, but there is no rigorous way to evaluate how different architectural choices -- prompts, skills, tools, multi-agent setups -- materially affect both capability and cost. This paper introduces Scylla, an evaluation framework for benchmarking agentic coding tools through structured ablation studies that uses seven testing tiers (T0-T6) progressively adding complexity to isolate what directly influences results and how. The key metric is Cost-of-Pass (CoP): the expected dollar cost to get one correct solution, which directly quantifies the trade-off between complexity and efficiency. The framework is model-agnostic, designed to work with any CLI tool; this paper demonstrates it with Claude Sonnet 4.5, using multiple LLM judges (Opus 4.5, Sonnet 4.5, Haiku 4.5) from the same vendor for evaluation consensus, where judges score results using direct tests, human-designed LLM-evaluated rubrics, and qualitative assessment. The result is a reproducible framework that quantifies trade-offs between agent complexity and actual outcomes, suggesting that architectural complexity does not always improve quality.

</details>


### [42] [ArkEval: Benchmarking and Evaluating Automated CodeRepair for ArkTS](https://arxiv.org/abs/2602.08866)
*Bang Xie,Senjian Zhang,Zhiyuan Peng,Wei Chen,Chenhao Ying,Yuan Luo*

Main category: cs.SE

TL;DR: 提出了ArkEval框架，这是首个专门为ArkTS自动程序修复设计的综合基准测试，包含502个可复现问题，并评估了四种最先进的大语言模型在ArkTS代码修复中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着HarmonyOS平台的重要性日益增长，ArkTS作为其核心开发语言，生态系统缺乏自动化代码修复工具，主要原因是缺少高质量的评估基准。

Method: 从华为官方包含400多个独立ArkTS应用的大型仓库中挖掘问题，通过多阶段过滤流程筛选出502个可复现问题，采用基于LLM的测试生成和投票机制确保可测试性，并标准化问题描述以支持公平评估。

Result: 构建了ArkEval基准测试，包含502个可复现的ArkTS问题，并评估了四种最先进大语言模型在检索增强修复工作流中的表现，揭示了LLM在修复ArkTS代码方面的当前能力和局限性。

Conclusion: ArkEval为ArkTS自动程序修复提供了首个综合基准测试，为这个低资源语言领域的未来研究铺平了道路。

Abstract: Large language models have transformed code generation, enabling unprecedented automation in software development. As mobile ecosystems evolve, HarmonyOS has emerged as a critical platform requiring robust development tools. Software development for the HarmonyOS ecosystem relies heavily on ArkTS, a statically typed extension of TypeScript. Despite its growing importance, the ecosystem lacks robust tools for automated code repair, primarily due to the absence of a high-quality benchmark for evaluation. To address this gap, we present ArkEval, a unified framework for ArkTS automated repair workflow evaluation and benchmark construction. It provides the first comprehensive benchmark specifically designed for ArkTS automated program repair. We constructed this benchmark by mining issues from a large-scale official Huawei repository containing over 400 independent ArkTS applications. Through a rigorous multi-stage filtering process, we curated 502 reproducible issues. To ensure testability, we employed a novel LLM-based test generation and voting mechanism involving Claude and other models. Furthermore, we standardized problem statements to facilitate fair evaluation. Finally, we evaluated four state-of-the-art Large Language Models (LLMs) on our benchmark using a retrieval-augmented repair workflow. Our results highlight the current capabilities and limitations of LLMs in repairing ArkTS code, paving the way for future research in this low-resource language domain.

</details>


### [43] [DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories](https://arxiv.org/abs/2602.08887)
*Adam Trendowicz,Daniel Seifert,Andreas Jedlitschka,Marcus Ciolkowski,Anton Strahilov*

Main category: cs.SE

TL;DR: 提出并评估了基于GPT-4o的"DeepQuali"方法，用于敏捷开发中的需求质量评估和改进，在两家小公司项目中验证了LLM评估与专家判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和LLM在软件工程中主要应用于编码任务，但在需求工程特别是需求验证方面应用有限。当前GAI在需求领域的应用主要集中在需求获取、转换和分类，而非质量评估。需要探索LLM在需求质量评估方面的潜力。

Method: 提出"DeepQuali"方法，基于GPT-4o进行需求质量评估和改进。在两家中型公司的实际项目中应用该方法，将LLM的质量评估结果与专家判断进行比较。专家参与解决方案的走查，提供反馈并评估方法的可接受性。

Result: 专家在很大程度上同意LLM的质量评估，特别是在整体评分和解释方面。但专家之间在详细评分上并不总是一致，表明专业知识和经验可能影响判断。专家认可该方法的实用性，但批评其缺乏工作流程集成。

Conclusion: LLM在支持软件工程师进行需求质量评估和改进方面具有潜力。明确使用质量模型和解释性反馈可以提高方法的可接受性。需要更好地将LLM工具集成到现有工作流程中。

Abstract: Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI for requirements is on eliciting, transforming, and classifying requirements, not on quality assessment. We propose and evaluate the LLM-based (GPT-4o) approach "DeepQuali", for assessing and improving requirements quality in agile software development. We applied it to projects in two small companies, where we compared LLM-based quality assessments with expert judgments. Experts also participated in walkthroughs of the solution, provided feedback, and rated their acceptance of the approach. Experts largely agreed with the LLM's quality assessments, especially regarding overall ratings and explanations. However, they did not always agree with the other experts on detailed ratings, suggesting that expertise and experience may influence judgments. Experts recognized the usefulness of the approach but criticized the lack of integration into their workflow. LLMs show potential in supporting software engineers with the quality assessment and improvement of requirements. The explicit use of quality models and explanatory feedback increases acceptance.

</details>


### [44] [Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance](https://arxiv.org/abs/2602.08915)
*Giovanni Pinna,Jingzhi Gong,David Williams,Federica Sarro*

Main category: cs.SE

TL;DR: 该研究对5个AI编程助手在7,156个PR任务上的表现进行实证比较，发现任务类型是接受率的主要影响因素，文档任务接受率最高，不同代理在不同任务类型上表现各异，Devin是唯一呈现持续改进趋势的代理。


<details>
  <summary>Details</summary>
Motivation: AI编程助手正在改变软件开发实践，但缺乏对其在不同任务类型上效果的系统性比较和时间趋势分析。

Method: 使用AIDev数据集的7,156个PR数据，比较5个流行AI编程助手（OpenAI Codex、GitHub Copilot、Devin、Cursor、Claude Code），进行时间趋势分析和任务类型分层分析。

Result: 任务类型是主要影响因素：文档任务接受率82.1%，新功能任务66.1%。Devin是唯一呈现持续正向趋势的代理（每周+0.77%）。不同代理在不同任务类型上表现最佳：OpenAI Codex在所有9个任务类别中表现稳定，Claude Code在文档和功能任务领先，Cursor在修复任务表现最好。

Conclusion: 没有单一代理在所有任务类型上表现最佳，任务类型是AI编程助手效果的主要决定因素，不同代理在不同任务类型上各有优势。

Abstract: The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization](https://arxiv.org/abs/2602.07054)
*Ashutosh Chaubey,Jiacheng Pang,Maksim Siniukov,Mohammad Soleymani*

Main category: cs.LG

TL;DR: 提出EmoReAlM基准评估多模态大语言模型在情感理解中的伪关联和幻觉问题，并提出AVEm-DPO偏好优化方法提升模型性能


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在情感理解任务中存在两个关键挑战：情感与无关视听线索之间的伪关联，以及语言模型主干中文本先验驱动的视听线索幻觉

Method: 1) 引入EmoReAlM基准评估线索-情感关联、幻觉和模态一致性；2) 提出AVEm-DPO偏好优化技术，构建对伪关联或幻觉响应的偏好，并包含惩罚文本先验依赖的正则化项

Result: 在DFEW、RAVDESS和EMER数据集上的实验表明，该方法显著提升基线模型性能，在零样本设置中获得6-19%的相对性能提升

Conclusion: 通过提供严谨的基准和鲁棒的优化框架，这项工作为情感理解和社交AI中的多模态大语言模型评估和改进提供了原则性方法

Abstract: Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.

</details>


### [46] [The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL](https://arxiv.org/abs/2602.07078)
*Yingru Li,Jiawei Xu,Ziniu Li,Jiacai Liu,Wei Liu,Yuxuan Tong,Longtao Zheng,Zhenghai Xue,Yaxiang Zhang,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: 提出Optimal Token Baseline (OTB)方法，通过基于梯度范数的逆加权来减少RL训练中的梯度方差，使用Logit-Gradient Proxy高效近似，显著降低token消耗同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习在长时程任务中常因梯度方差爆炸导致训练崩溃。传统基线方法存在优化困难、忽略序列异质性等问题，而经典最优基线理论又忽略了token异质性且计算成本过高。

Method: 从第一性原理推导出Optimal Token Baseline (OTB)，证明梯度更新应按其累积梯度范数的倒数加权。提出Logit-Gradient Proxy，仅使用前向传播概率近似梯度范数，实现高效计算。

Result: 方法实现了训练稳定性，仅用N=4的组大小就能达到N=32的性能，在单轮和工具集成推理任务中减少超过65%的token消耗。

Conclusion: OTB方法通过考虑token异质性的最优基线设计，有效解决了RL训练中的梯度方差问题，显著提高了训练效率和稳定性。

Abstract: Reinforcement Learning (RL) for Large Language Models (LLMs) often suffers from training collapse in long-horizon tasks due to exploding gradient variance. To mitigate this, a baseline is commonly introduced for advantage computation; however, traditional value models remain difficult to optimize, and standard group-based baselines overlook sequence heterogeneity. Although classic optimal baseline theory can achieve global variance reduction, it neglects token heterogeneity and requires prohibitive gradient-based computation. In this work, we derive the Optimal Token Baseline (OTB) from first principles, proving that gradient updates should be weighted inversely to their cumulative gradient norm. To ensure efficiency, we propose the Logit-Gradient Proxy that approximates the gradient norm using only forward-pass probabilities. Our method achieves training stability and matches the performance of large group sizes ($N=32$) with only $N=4$, reducing token consumption by over 65% across single-turn and tool-integrated reasoning tasks.

</details>


### [47] [On Randomness in Agentic Evals](https://arxiv.org/abs/2602.07150)
*Bjarni Haukur Bjarnason,André Silva,Martin Monperrus*

Main category: cs.LG

TL;DR: 研究发现单次运行评估智能体系统存在显著方差，2-3个百分点的改进可能只是评估噪声而非真实算法进步，建议采用多次运行、统计功效分析和pass@k等指标进行可靠评估。


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统评估通常基于每个任务单次运行的pass@1分数，假设这能提供可靠的性能估计。本文旨在测试这一假设的有效性，揭示单次运行评估的局限性。

Method: 在SWE-Bench-Verified上收集60,000个智能体轨迹，涵盖三个模型和两种脚手架。通过token级分析研究轨迹差异，分析方差对性能评估的影响。

Result: 发现显著方差：单次运行pass@1估计因选择不同运行而异2.2-6.0个百分点，即使在温度0时标准差也超过1.5个百分点。轨迹在早期（前几个百分点的token）就出现分歧，小差异会级联成不同的解决策略。

Conclusion: 为可靠评估智能体系统，建议三项实践：1) 每个任务进行多次独立运行估计pass@1；2) 使用统计功效分析确定检测预期效应大小所需的运行次数；3) 考虑pass@k（乐观边界）和pass^k（悲观边界）等指标。虽然增加评估成本，但对区分真实科学进步与统计噪声至关重要。

Abstract: Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.

</details>


### [48] [Risk-Sensitive Exponential Actor Critic](https://arxiv.org/abs/2602.07202)
*Alonso Granados,Jason Pacheco*

Main category: cs.LG

TL;DR: 本文提出了一种名为rsEAC的风险敏感指数演员-评论家方法，用于优化熵风险度量，解决了现有风险敏感模型自由方法在连续控制任务中的数值不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 尽管无模型深度强化学习算法在许多挑战性任务上取得了成功，但在实际应用中存在安全隐患，需要风险感知的智能体。现有的风险敏感方法在处理熵风险度量时存在高方差和数值不稳定的更新问题，限制了其在复杂任务中的应用。

Method: 提出了风险敏感指数演员-评论家（rsEAC），这是一种离策略的无模型方法，包含新颖的程序来避免显式表示指数价值函数及其梯度，并针对熵风险度量优化策略。该方法基于对熵风险度量的策略梯度方法的全面理论论证。

Result: rsEAC相比现有方法产生更数值稳定的更新，并在MuJoCo中的连续任务风险变体上可靠地学习了风险敏感策略。

Conclusion: rsEAC方法有效地解决了现有风险敏感强化学习方法在连续控制任务中的数值不稳定问题，为实际应用中的风险感知智能体提供了可行的解决方案。

Abstract: Model-free deep reinforcement learning (RL) algorithms have achieved tremendous success on a range of challenging tasks. However, safety concerns remain when these methods are deployed on real-world applications, necessitating risk-aware agents. A common utility for learning such risk-aware agents is the entropic risk measure, but current policy gradient methods optimizing this measure must perform high-variance and numerically unstable updates. As a result, existing risk-sensitive model-free approaches are limited to simple tasks and tabular settings. In this paper, we provide a comprehensive theoretical justification for policy gradient methods on the entropic risk measure, including on- and off-policy gradient theorems for the stochastic and deterministic policy settings. Motivated by theory, we propose risk-sensitive exponential actor-critic (rsEAC), an off-policy model-free approach that incorporates novel procedures to avoid the explicit representation of exponential value functions and their gradients, and optimizes its policy w.r.t the entropic risk measure. We show that rsEAC produces more numerically stable updates compared to existing approaches and reliably learns risk-sensitive policies in challenging risky variants of continuous tasks in MuJoCo.

</details>


### [49] [Adaptive Retrieval helps Reasoning in LLMs -- but mostly if it's not used](https://arxiv.org/abs/2602.07213)
*Srijan Shakya,Anamaria-Roberta Hartl,Sepp Hochreiter,Korbinian Pöppel*

Main category: cs.LG

TL;DR: LLM在复杂推理任务中常因静态参数知识而失败，本研究探索将检索作为动态上下文学习形式，测试了LLM代理在推理过程中主动决定何时查询外部知识库的自适应检索增强架构。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中经常因静态参数知识而失败，导致幻觉和数学等专业领域表现不佳，需要探索增强生成模型的基本原则。

Method: 采用自适应检索增强架构，让LLM代理在推理过程中主动决定何时查询外部知识库，在GSM8K和MATH-500基准测试中比较自适应策略与标准思维链基线和静态检索方法。

Result: 静态检索表现不如思维链，自适应检索显示有趣模式：包含检索结果的轨迹表现略差于思维链，但不包含检索的轨迹表现优于思维链。检索很少帮助推理（仅少数反例如使用有用定理），主动不使用检索表明模型性能更好。模型根据问题难度调整检索频率，检索决策是关键元认知信号。

Conclusion: 模型自我评估知识并选择性使用外部信息的能力是构建更稳健可靠生成模型的关键原则，自适应检索决策作为元认知信号对模型性能有重要指示作用。

Abstract: Large Language Models (LLMs) often falter in complex reasoning tasks due to their static, parametric knowledge, leading to hallucinations and poor performance in specialized domains like mathematics. This work explores a fundamental principle for enhancing generative models: treating retrieval as a form of dynamic in-context learning. We test an adaptive retrieval-augmented architecture where an LLM agent actively decides when to query an external knowledge base during its reasoning process. We compare this adaptive strategy against a standard Chain-of-Thought (CoT) baseline and a static retrieval approach on the GSM8K and MATH-500 benchmarks. Although our experiments show that static retrieval is inferior to CoT, the adaptive retrieval shows interesting behavior: While traces including retrieved results show slightly worse performance compared to CoT, traces that do not include retrieval actually perform better compared to CoT. This suggests that: (a) retrieval only rarely helps reasoning (we show a few counterexamples, e.g. using useful theorems) and (b) actively not using retrieval is indicative of good model performance. Furthermore, we find that the model scales its retrieval frequency with the difficulty of the problem, reinforcing that the decision to retrieve is a crucial metacognitive signal. The agent's ability to self-assess its knowledge and selectively engage with external information represents a key principle for building more robust and reliable generative models.

</details>


### [50] [Revisiting Robustness for LLM Safety Alignment via Selective Geometry Control](https://arxiv.org/abs/2602.07340)
*Yonghui Yang,Wenjian Tao,Jilong Liu,Xingyu Zhu,Junfeng Fang,Weibiao Huang,Le Wu,Richang Hong,Tat-Sent Chua*

Main category: cs.LG

TL;DR: ShaPO是一个几何感知的偏好优化框架，通过选择性控制对齐关键参数子空间的几何结构来增强LLM安全对齐的鲁棒性，解决了现有方法在领域转移和噪声监督下的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全对齐在领域转移和噪声偏好监督下仍然脆弱。现有鲁棒对齐方法主要关注对齐数据的不确定性，而忽视了基于偏好的目标函数中优化引起的脆弱性。

Method: 从优化几何角度重新审视LLM安全对齐的鲁棒性，提出ShaPO框架，通过对齐关键参数子空间的选择性几何控制来强制执行最坏情况对齐目标。具体实现两个层面：token级ShaPO稳定基于似然的代理优化，reward级ShaPO在噪声监督下强制执行奖励一致性优化。

Result: 在多样化的安全基准测试和噪声偏好设置中，ShaPO相比流行的偏好优化方法持续提升了安全鲁棒性。此外，ShaPO能与数据鲁棒目标清晰组合，带来额外增益，实证支持了所提出的优化几何视角。

Conclusion: 仅靠数据为中心的方法无法解决鲁棒性失败问题，需要从优化几何角度考虑。ShaPO通过选择性几何控制避免了过度正则化，有效提升了LLM安全对齐在分布转移下的鲁棒性。

Abstract: Safety alignment of large language models remains brittle under domain shift and noisy preference supervision. Most existing robust alignment methods focus on uncertainty in alignment data, while overlooking optimization-induced fragility in preference-based objectives. In this work, we revisit robustness for LLM safety alignment from an optimization geometry perspective, and argue that robustness failures cannot be addressed by data-centric methods alone. We propose ShaPO, a geometry-aware preference optimization framework that enforces worst-case alignment objectives via selective geometry control over alignment-critical parameter subspace. By avoiding uniform geometry constraints, ShaPO mitigates the over-regularization that can harm robustness under distribution shift. We instantiate ShaPO at two levels: token-level ShaPO stabilizes likelihood-based surrogate optimization, while reward-level ShaPO enforces reward-consistent optimization under noisy supervision. Across diverse safety benchmarks and noisy preference settings, ShaPO consistently improves safety robustness over popular preference optimization methods. Moreover, ShaPO composes cleanly with data-robust objectives, yielding additional gains and empirically supporting the proposed optimization-geometry perspective.

</details>


### [51] [Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07441)
*Jinzong Dong,Wei Huang,Jianshu Zhang,Zhuo Chen,Xinzhe Yuan,Qinying Gu,Zhaohui Jiang,Nanyang Ye*

Main category: cs.LG

TL;DR: 本文提出PAR方法解决离线RL中行为克隆正则化导致的性能天花板问题，通过渐进替换低价值动作为高价值动作来扩展探索空间


<details>
  <summary>Details</summary>
Motivation: 离线RL中行为克隆正则化虽然能产生现实策略并缓解分布外动作偏差，但当数据集动作次优时，盲目模仿会阻止智能体充分利用评论家建议的高价值区域，形成性能天花板

Method: 提出近端动作替换(PAR)方法，渐进地将训练样本中的低价值动作替换为稳定演员生成的高价值动作，扩展动作探索空间同时减少低价值数据影响

Result: 在离线RL基准测试中，PAR一致提升性能，与基础TD3+BC结合时达到state-of-the-art水平

Conclusion: PAR能有效打破行为克隆正则化的性能天花板，兼容多种BC正则化范式，是即插即用的训练样本替换器

Abstract: Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distribution actions, but can impose an often-overlooked performance ceiling: when dataset actions are suboptimal, indiscriminate imitation structurally prevents the actor from fully exploiting high-value regions suggested by the critic, especially in later training when imitation is already dominant. We formally analyzed this limitation by investigating convergence properties of BC-regularized actor-critic optimization and verified it on a controlled continuous bandit task. To break this ceiling, we propose proximal action replacement (PAR), a plug-and-play training sample replacer that progressively replaces low-value actions with high-value actions generated by a stable actor, broadening the action exploration space while reducing the impact of low-value data. PAR is compatible with multiple BC regularization paradigms. Extensive experiments across offline RL benchmarks show that PAR consistently improves performance and approaches state-of-the-art when combined with the basic TD3+BC.

</details>


### [52] [Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs](https://arxiv.org/abs/2602.07729)
*Sagnik Mukherjee,Lifan Yuan,Pavan Jayasinha,Dilek Hakkani-Tür,Hao Peng*

Main category: cs.LG

TL;DR: 研究发现，在大型语言模型的强化学习阶段，简单的SGD优化器比广泛使用的AdamW表现更好，且参数更新稀疏度极高（仅更新0.02%的参数），内存效率更高。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的强化学习训练仍沿用预训练阶段的优化方法（如AdamW），但RL与监督学习存在本质差异。AdamW内存开销大，而研究显示RL可能不需要AdamW的复杂自适应特性。

Method: 分析AdamW在RL中的动量与自适应学习率作用，与监督微调对比。实验验证SGD在RL中的性能，测量参数更新稀疏度，并分析更新稀疏的原因。

Result: SGD在LLM的RL中表现与AdamW相当甚至更好，且参数更新极其稀疏（仅更新0.02%的参数），比AdamW少更新1000倍以上，内存效率显著提升。

Conclusion: RL优化与监督学习不同，SGD在RL中更有效且参数效率极高，这为LLM的RL训练提供了新见解和更高效的优化方案。

Abstract: Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.

</details>


### [53] [The Laplacian Keyboard: Beyond the Linear Span](https://arxiv.org/abs/2602.07730)
*Siddarth Chandrasekar,Marlos C. Machado*

Main category: cs.LG

TL;DR: 本文提出Laplacian Keyboard（LK），一种分层强化学习框架，利用拉普拉斯特征向量构建任务无关的行为基元库，通过元策略动态组合这些基元，超越线性约束实现高效策略学习。


<details>
  <summary>Details</summary>
Motivation: 拉普拉斯特征向量在强化学习中通常仅用于线性逼近奖励函数，限制了在复杂环境中的表达能力。需要超越线性约束的方法来提升策略学习的效率和表达能力。

Method: LK框架：1）从拉普拉斯特征向量构建任务无关的选项库（行为基元）；2）保证该基元库包含任何线性奖励的最优策略；3）通过元策略动态组合这些选项，学习超出原始线性约束的策略。

Result: 理论分析：建立了零样本逼近误差的理论界限。实证结果：LK超越了零样本解决方案，相比标准RL方法实现了更好的样本效率。

Conclusion: LK框架成功超越了拉普拉斯特征向量的线性约束，通过分层组合行为基元实现了更高效的强化学习，为复杂环境中的策略学习提供了新方法。

Abstract: Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typically limited to their linear span, which restricts expressivity in complex environments. We introduce the Laplacian Keyboard (LK), a hierarchical framework that goes beyond the linear span. LK constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis guaranteed to contain the optimal policy for any reward within the linear span. A meta-policy learns to stitch these options dynamically, enabling efficient learning of policies outside the original linear constraints. We establish theoretical bounds on zero-shot approximation error and demonstrate empirically that LK surpasses zero-shot solutions while achieving improved sample efficiency compared to standard RL methods.

</details>


### [54] [Efficient Representations are Controllable Representations](https://arxiv.org/abs/2602.07828)
*Charles Ye,Jasmine Cui*

Main category: cs.LG

TL;DR: 通过简单的辅助损失微调LLM，在残差流中创建16个惰性可解释性标志，这些标志在推理时成为可控制生成的真实内部特征


<details>
  <summary>Details</summary>
Motivation: 传统方法需要先识别模型的特征几何结构再进行干预，过程复杂。本文旨在寻找一种更直接、更暴力的方法来安装可解释、可控制的特征到模型激活中

Method: 使用简单的辅助损失微调LLM，训练其3072个残差流维度中的16个成为惰性可解释性标志，这些标志指示生成所需的概念。模型在生成任务中学会依赖这些标志

Result: 这些惰性标志成为真实的内部特征：可解释的控制开关，允许在推理时引导生成。模型会消除冗余编码，依赖这些固定位置的特征

Conclusion: 当特征可靠地在固定位置提供时，梯度下降会逐渐消除其他地方的冗余编码，模型会侵蚀自身的替代表示。模型的效率压力是可利用的杠杆，可诱导出可解释、可控制的表示

Abstract: What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this.
  We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.

</details>


### [55] [rePIRL: Learn PRM with Inverse RL for LLM Reasoning](https://arxiv.org/abs/2602.07832)
*Xian Wu,Kaijie Zhu,Ying Zhang,Lun Wang,Wenbo Guo*

Main category: cs.LG

TL;DR: rePIRL是一个受逆强化学习启发的框架，用于学习有效的过程奖励模型，对专家策略假设要求最小，通过双学习过程交替更新策略和奖励模型。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型学习方法要么依赖对专家策略的强假设（如需要其奖励函数），要么存在内在限制（如熵崩溃），导致奖励模型效果弱或泛化能力有限。

Method: 设计双学习过程交替更新策略和过程奖励模型，采用定制化技术解决传统逆强化学习扩展到LLM的挑战，统一在线和离线PRM学习方法。

Result: 在标准化数学和编程推理数据集上的实证评估显示rePIRL优于现有方法，训练出的PRM可用于测试时训练、测试时扩展和为困难问题提供早期信号。

Conclusion: rePIRL能够以最小假设学习有效的过程奖励模型，理论证明其统一性，实证验证其有效性，并通过消融研究验证训练方案和关键设计选择。

Abstract: Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.

</details>


### [56] [MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation](https://arxiv.org/abs/2602.07848)
*Shijie Wang,Pengfei Li,Yikun Fu,Kaifeng Liu,Fangyuan Li,Yang Liu,Xiaowei Sun,Zonglin Li,Siyao Zhao,Jian Zhao,Kai Tian,Dong Li,Junqi Gao,Yutong Zhang,Yiqun Chen,Yuqiang Li,Zoe Li,Weinan Zhang,Peng Ye,Shuyue Hu,Lei Bai,Bowen Zhou,Kaiyan Zhang,Biqing Qi*

Main category: cs.LG

TL;DR: MARTI-MARS2是一个多智能体强化训练与推理框架，通过将多智能体协作探索过程建模为动态可学习环境，结合策略学习与多智能体树搜索，实现了从参数共享同质多角色训练到异质多智能体训练的演进，在代码生成任务上超越GPT-5.1等基线模型。


<details>
  <summary>Details</summary>
Motivation: 单智能体系统在复杂任务（如代码生成）中存在性能瓶颈，多智能体协作有望突破这些限制。但现有框架通常依赖基于提示的测试时交互或同质参数训练的多角色配置，限制了错误纠正能力和策略多样性。

Method: 提出MARTI-MARS2框架，将多智能体协作探索过程建模为动态可学习环境，结合策略学习与多智能体树搜索。允许智能体在环境中迭代探索和优化，实现从同质多角色训练到异质多智能体训练的演进。同时提出高效的推理策略MARTI-MARS2-T+。

Result: 在两个32B模型协作下，在代码生成基准测试中达到77.7%的准确率，超越GPT-5.1等基线。揭示了新的扩展规律：从单智能体到同质多角色再到异质多智能体范式逐步提高RL性能上限、鲁棒的TTS能力和更大的策略多样性。

Conclusion: MARTI-MARS2通过多智能体强化学习框架突破了单智能体能力限制，证明了策略多样性对于通过多智能体强化学习扩展智能的重要性，为多智能体协作提供了新的训练和推理范式。

Abstract: While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.

</details>


### [57] [Direct Soft-Policy Sampling via Langevin Dynamics](https://arxiv.org/abs/2602.07873)
*Donghyeon Ki,Hee-Jun Ahn,Kyungyoon Kim,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 提出NC-LQL方法，通过多尺度噪声扰动和朗之万动力学实现软策略采样，解决传统方法在表达性和熵估计方面的限制


<details>
  <summary>Details</summary>
Motivation: 现有软策略实现方法存在局限性：参数化策略表达能力有限，基于扩散的策略难以估计熵值，朗之万动力学在高维非凸Q函数中混合速度慢

Method: 提出噪声条件朗之万Q学习(NC-LQL)：通过多尺度噪声扰动Q函数，学习噪声条件Q函数，在平滑的价值函数景观中实现从全局探索到精确模式细化的采样

Result: 在OpenAI Gym MuJoCo基准测试中，NC-LQL达到与最先进扩散方法相当的性能

Conclusion: NC-LQL为在线强化学习提供了一种简单而强大的软策略实现方案

Abstract: Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.

</details>


### [58] [Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07889)
*Long Chen,Yinkui Liu,Shen Li,Bo Tang,Xuemin Hu*

Main category: cs.LG

TL;DR: 提出基于VQVAE和模糊聚类的离线RL反探索方法，通过多码本VQVAE离散化状态-动作对，结合FCM聚类更新码本，解决维度灾难和信息丢失问题，在D4RL基准上表现优于SOTA方法且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL中的反探索方法通过离散化连续状态-动作对进行计数，但存在维度灾难和信息丢失问题，导致效率降低、性能下降甚至策略学习失败。

Method: 1) 基于多码本VQVAE的高效伪计数方法离散化状态-动作对；2) 基于该伪计数方法的离线RL反利用方法；3) 基于模糊C均值(FCM)聚类的码本更新机制提高码本向量利用率。

Result: 在D4RL基准测试中，该方法在多个复杂任务上表现优于现有最先进方法，且需要更少的计算成本。

Conclusion: 提出的基于VQVAE和模糊聚类的反探索方法有效解决了离线RL中维度灾难和信息丢失问题，提高了学习效率和性能。

Abstract: Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.

</details>


### [59] [AceGRPO: Adaptive Curriculum Enhanced Group Relative Policy Optimization for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2602.07906)
*Yuzhu Cai,Zexi Liu,Xinyu Zhu,Cheng Wang,Jiaao Chen,Hanrui Wang,Wei-Chen Wang,Di Jin,Siheng Chen*

Main category: cs.LG

TL;DR: 提出AceGRPO框架，通过演化数据缓冲区和自适应采样解决自主机器学习工程中的行为停滞问题，训练出的Ace-30B模型在MLE-Bench-Lite上达到100%有效提交率


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的MLE代理存在行为停滞问题，因为参数被冻结。虽然强化学习可以解决，但在MLE应用中面临执行延迟高和数据选择效率低的问题

Method: 提出AceGRPO框架，包含两个核心组件：1) 演化数据缓冲区，将执行轨迹重新利用为可重复使用的训练任务；2) 自适应采样，通过可学习性潜力函数动态优先处理代理学习前沿的任务

Result: 训练的Ace-30B模型在MLE-Bench-Lite上达到100%有效提交率，接近前沿专有模型的性能，并优于更大的开源基线模型（如DeepSeek-V3.2）

Conclusion: AceGRPO框架通过高效的数据重用和任务选择机制，显著提升了自主机器学习工程代理的持续迭代优化能力

Abstract: Autonomous Machine Learning Engineering (MLE) requires agents to perform sustained, iterative optimization over long horizons. While recent LLM-based agents show promise, current prompt-based agents for MLE suffer from behavioral stagnation due to frozen parameters. Although Reinforcement Learning (RL) offers a remedy, applying it to MLE is hindered by prohibitive execution latency and inefficient data selection. Recognizing these challenges, we propose AceGRPO with two core components: (1) Evolving Data Buffer that continuously repurposes execution traces into reusable training tasks, and (2) Adaptive Sampling guided by a Learnability Potential function, which dynamically prioritizes tasks at the agent's learning frontier to maximize learning efficiency. Leveraging AceGRPO, our trained Ace-30B model achieves a 100% valid submission rate on MLE-Bench-Lite, approaches the performance of proprietary frontier models, and outperforms larger open-source baselines (e.g., DeepSeek-V3.2), demonstrating robust capability for sustained iterative optimization. Code is available at https://github.com/yuzhu-cai/AceGRPO.

</details>


### [60] [Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments](https://arxiv.org/abs/2602.08041)
*Boyang Xia,Weiyou Tian,Qingnan Ren,Jiaqi Huang,Jie Xiao,Shuo Lu,Kai Wang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: 提出ISO框架，通过预测战略上下文来优化LLM智能体在长期对抗游戏中的表现，结合战略奖励模型和乐观学习规则，在德州扑克和宝可梦游戏中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在长期对抗游戏中，传统的短期优化方法（如胜率）无法捕捉随时间演变的战略外部性，导致即使动态可预测时，基于变异的遗憾分析也变得无效。

Method: 提出隐式战略优化（ISO）框架：1）战略奖励模型（SRM）估计行动的长期战略价值；2）iso-grpo，一种上下文条件乐观学习规则，智能体预测当前战略上下文并在线更新策略。

Result: 理论证明：当预测误差有界时，获得亚线性上下文遗憾和均衡收敛保证，恢复静态游戏速率。实验：在6人无限注德州扑克和竞争性宝可梦游戏中，长期回报持续优于强LLM和RL基线，在受控预测噪声下表现优雅退化。

Conclusion: ISO框架通过预测战略上下文有效解决了长期对抗游戏中的战略外部性问题，在理论和实验上都展示了优越性能，为LLM智能体在复杂动态环境中的优化提供了新方法。

Abstract: Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.

</details>


### [61] [Dreaming in Code for Curriculum Learning in Open-Ended Worlds](https://arxiv.org/abs/2602.08194)
*Konstantinos Mitsides,Maxence Faldor,Antoine Cully*

Main category: cs.LG

TL;DR: Dreaming in Code (DiCode) 是一个利用基础模型生成可执行环境代码的框架，通过在代码层面创造环境变体来构建学习路径，帮助智能体在开放世界中获得长期技能。


<details>
  <summary>Details</summary>
Motivation: 在复杂的开放世界中，巨大的组合空间使得智能体难以发现持续可学习经验序列。现有方法通常关注孤立行为的发现，而非组织持续进步。

Method: 提出DiCode框架，让基础模型合成可执行环境代码来搭建学习路径。"梦想"表现为在代码层面创造世界变体，在Craftax基准上实例化。

Result: DiCode使智能体获得长期技能，平均回报比最强基线提高16%，在后期战斗任务上取得非零成功率（先前方法失败）。

Conclusion: 代码级环境设计为课程控制提供了实用机制，能够构建连接开放世界中能力差距的中间环境。

Abstract: Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, "dreaming" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.

</details>


### [62] [Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection](https://arxiv.org/abs/2602.08003)
*Yigit Turkmen,Baturalp Buyukates,Melih Bastopcu*

Main category: cs.LG

TL;DR: 提出基于互信息的贪婪选择算法，在查询预算下构建LLM集成，解决模型强相关时如何选择最优模型组合的问题。


<details>
  <summary>Details</summary>
Motivation: LLM集成通常用于提高可靠性和鲁棒性，但实践中模型之间存在强相关性，这引发了一个基本问题：在形成LLM集成时应该选择哪些模型？同时需要解释为什么即使使用多个模型，性能也可能饱和。

Method: 将预算约束下的集成选择问题形式化为最大化真实标签与所选模型预测之间的互信息。使用高斯copula建模模型的相关误差，展示集成性能的信息论误差下限。提出简单的贪婪互信息选择算法，直接从数据估计所需信息项，在查询预算下迭代构建集成。

Result: 在三个数据集（MEDMCQA、MMLU和IMDB电影评论）上测试，该方法在相同查询预算下始终优于强基线方法。

Conclusion: 提出的基于互信息的贪婪选择算法能有效解决LLM集成中的模型选择问题，在预算约束下实现更好的性能，同时通过信息论分析解释了集成性能饱和的现象。

Abstract: Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.

</details>


### [63] [Reinforcement Learning with Backtracking Feedback](https://arxiv.org/abs/2602.08377)
*Bilgehan Sel,Vaishakh Keshava,Phillip Wallis,Lukas Rutishauser,Ming Jin,Dingcheng Li*

Main category: cs.LG

TL;DR: RLBF框架通过强化学习让LLM学会动态回溯纠正自身生成错误，结合改进的监督微调数据生成策略，显著提升对抗攻击和分布内错误的防御能力，同时保持模型基础效用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在面对对抗攻击和分布内错误时存在安全漏洞，现有方法如BSAFE等需要更强大的安全机制来应对复杂的对抗策略（如中间填充、GCG攻击、解码参数操纵等）。

Method: 提出RLBF框架：1) 强化学习阶段：通过critic对模型实时输出提供反馈，训练模型识别安全违规并发出"回溯x个token"信号，然后继续自回归生成；2) 改进的监督微调数据生成策略(BSAFE+)：在原本安全的连贯文本中注入违规内容，为回溯机制提供更有效的初始训练。

Result: RLBF在不同基准测试和模型规模上显著降低攻击成功率，实现优越的安全效果，同时关键地保持了基础模型的功能效用。

Conclusion: RLBF框架通过结合强化学习的动态回溯纠正和改进的监督微调数据生成，为大语言模型提供了更强大的安全防御机制，能够有效应对复杂的对抗攻击策略。

Abstract: Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.

</details>


### [64] [Beyond Correctness: Learning Robust Reasoning via Transfer](https://arxiv.org/abs/2602.08489)
*Hyunseok Lee,Soheil Abbasloo,Jihoon Tack,Jinwoo Shin*

Main category: cs.LG

TL;DR: RLTR（可转移奖励的强化学习）通过测试部分推理前缀能否指导其他模型得出正确答案，来增强LLM推理的鲁棒性，相比RLVR在更少训练步骤下达到相当性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法只关注最终答案正确性，忽略了推理过程的鲁棒性。作者认为稳健的推理应该能够超越产生它的思维，具有可转移性，能在截断、重新解释和延续后依然有效。

Method: 提出RLTR方法，通过可转移奖励来操作化鲁棒性：测试从一个模型提取的部分推理前缀是否能指导另一个独立模型得出正确答案。这鼓励LLM产生稳定、可解释且真正可泛化的推理。

Result: 在MATH500上，RLTR相比RLVR在Maj@64指标上获得+3.6%的提升，并且用大约2.5倍更少的训练步骤就达到了RLVR的平均准确率，同时提高了采样一致性。

Conclusion: RLTR不仅提高了最终答案准确率，还增强了推理过程的鲁棒性和样本效率，为LLM推理提供了更可靠且高效的训练方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.

</details>


### [65] [Horizon Imagination: Efficient On-Policy Training in Diffusion World Models](https://arxiv.org/abs/2602.08032)
*Lior Cohen,Ofir Nabati,Kaixin Wang,Navdeep Kumar,Shie Mannor*

Main category: cs.LG

TL;DR: 提出Horizon Imagination方法，通过并行去噪多个未来观测，在保持控制性能的同时显著降低扩散世界模型的推理成本


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的世界模型在强化学习中面临效率挑战，要么需要重型模型推理，要么依赖高度序列化的想象过程，计算成本过高

Method: 提出Horizon Imagination方法，包含稳定化机制和新型采样调度，支持并行去噪多个未来观测，将去噪预算与有效视野解耦，并支持子帧预算

Result: 在Atari 100K和Craftium实验中，该方法在仅使用一半去噪步数的子帧预算下保持控制性能，并在不同调度下实现更优的生成质量

Conclusion: Horizon Imagination为扩散世界模型提供了高效的想象过程，显著降低了计算成本同时保持控制性能

Abstract: We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.

</details>


### [66] [Bayesian Preference Learning for Test-Time Steerable Reward Models](https://arxiv.org/abs/2602.08819)
*Jiwoo Hong,Shao Tang,Zhipeng Wang*

Main category: cs.LG

TL;DR: 提出变分上下文奖励建模（ICRM），一种贝叶斯奖励建模方法，通过上下文偏好演示实现测试时可控性，在单目标和多目标设置中都能适应未见过的偏好分布。


<details>
  <summary>Details</summary>
Motivation: 随着强化学习应用于可验证奖励和多目标对齐等场景，奖励模型需要编码更复杂、多方面的偏好分布。但传统的分类器奖励模型一旦训练完成就保持静态，限制了其在测试时的适应性。

Method: 提出变分上下文奖励建模（ICRM），将奖励建模视为在Bradley-Terry模型下使用共轭Beta先验对潜在偏好概率进行摊销变分推断。通过上下文偏好演示实现测试时可控性。

Result: ICRM在单目标设置中，随着更多上下文演示，在SafeRLHF上获得34%准确率提升，在RM-Bench上获得9%准确率提升；在多目标设置中，在有用性和拒绝基准上将帕累托前沿扩展了4%的超体积增益。在数学推理任务中，ICRM能有效编码可验证奖励，优于传统奖励模型。

Conclusion: ICRM提供了一种灵活、可适应的奖励建模方法，能够通过上下文演示在测试时调整偏好，为强化学习训练提供了实用价值，并具有理论保证。

Abstract: Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.

</details>


### [67] [A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents](https://arxiv.org/abs/2602.08964)
*Raghu Arghal,Fade Chen,Niall Dalton,Evgenii Kortukov,Calum McNamara,Angelos Nalmpantis,Moksh Nirvaan,Gabriele Sarti,Mario Giulianelli*

Main category: cs.LG

TL;DR: 提出一个结合行为评估与可解释性分析的框架来评估智能体的目标导向性，通过LLM智能体在2D网格世界中的导航案例研究，发现其内部非线性编码环境空间地图，推理过程重组表征从环境结构线索转向动作选择信息。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏可靠的方法来将目标归因于智能体系统，需要建立评估目标导向性的框架，超越单纯的行为评估，理解智能体如何内部表征和追求目标。

Method: 提出整合行为评估与可解释性分析的框架。案例研究中使用LLM智能体在2D网格世界中导航，行为评估对比最优策略在不同网格大小、障碍密度和目标结构下的表现；使用探测方法解码智能体对环境状态和多步行动计划的内部表征。

Result: 行为上，智能体性能随任务难度扩展但保持稳健；内部表征上，LLM智能体非线性编码环境的粗略空间地图，保留位置和目标的近似任务相关线索；推理过程重组表征，从环境结构线索转向支持即时动作选择的信息。

Conclusion: 仅靠行为评估不足以表征智能体如何表征和追求目标，需要内省式检查来理解智能体的目标导向性。

Abstract: Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.

</details>


### [68] [Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning](https://arxiv.org/abs/2602.08054)
*Manan Tayal,Mumuksh Tayal*

Main category: cs.LG

TL;DR: EpiFlow是一个安全离线强化学习框架，通过状态约束最优控制问题联合优化安全性和性能，使用epigraph值函数指导策略合成，在安全关键任务中实现高回报和接近零的安全违规。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在安全关键领域具有吸引力，但现有方法难以同时保证强安全性和高性能。现有安全离线RL方法要么依赖允许违规的软约束，要么引入过度保守性，或者难以平衡安全性、奖励优化和数据分布遵循。

Method: 提出Epigraph-Guided Flow Matching (EpiFlow)框架，将安全离线RL表述为状态约束最优控制问题。学习从最优控制问题的epigraph重构导出的可行性值函数，避免先前工作中的解耦目标或后处理过滤。策略通过基于epigraph值函数重新加权行为分布，并通过流匹配拟合生成策略来合成。

Result: 在各种安全关键任务（包括Safety-Gymnasium基准测试）中，EpiFlow实现了具有竞争力的回报和接近零的经验安全违规，证明了epigraph引导策略合成的有效性。

Conclusion: EpiFlow通过epigraph引导的流匹配框架，有效解决了安全离线RL中安全性和性能的联合优化问题，避免了现有方法的局限性，在安全关键任务中表现出色。

Abstract: Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.

</details>


### [69] [Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology](https://arxiv.org/abs/2602.08082)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出基于注意力拓扑谱分析的免训练护栏方法，用于检测自主代理的工具使用失败和幻觉，在Llama和Mistral模型上实现高召回率检测，发现幻觉时模型注意力变为噪声状态。


<details>
  <summary>Details</summary>
Motivation: 在野外部署自主代理需要可靠的保障机制来防止工具使用失败。现有监督方法需要标注数据，本文旨在开发无需训练数据的检测方法。

Method: 基于注意力拓扑的谱分析方法，使用单层谱特征作为幻觉检测器，包括平滑度和熵等特征，通过阈值检测模型失败状态。

Result: 在Llama 3.1 8B上，多特征检测达到97.7%召回率，平衡部署时达到86.1%召回率和81.0%精确率。单层谱特征检测效果显著：Llama L26平滑度达到98.2%召回率，Mistral L3熵达到94.7%召回率。跨模型评估发现"大声说谎者"现象：Llama 3.1 8B的失败在谱上更容易检测。

Conclusion: 谱分析为代理安全提供了原则性、高效的框架，表明幻觉不仅是错误标记，而是热力学状态变化：模型出错时注意力变为噪声。

Abstract: Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\% recall with multi-feature detection and 86.1\% recall with 81.0\% precision for balanced deployment, without requiring any labeled training data. Most remarkably, we discover that single layer spectral features act as near-perfect hallucination detectors: Llama L26 Smoothness achieves 98.2\% recall (213/217 hallucinations caught) with a single threshold, and Mistral L3 Entropy achieves 94.7\% recall. This suggests hallucination is not merely a wrong token but a thermodynamic state change: the model's attention becomes noise when it errs. Through controlled cross-model evaluation on matched domains ($N=1000$, $T=0.3$, same General domain, hallucination rates 20--22\%), we reveal the ``Loud Liar'' phenomenon: Llama 3.1 8B's failures are spectrally catastrophic and dramatically easier to detect, while Mistral 7B achieves the best discrimination (AUC 0.900). These findings establish spectral analysis as a principled, efficient framework for agent safety.

</details>


### [70] [SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning](https://arxiv.org/abs/2602.08234)
*Peng Xia,Jianwen Chen,Hanyang Wang,Jiaqi Liu,Kaide Zeng,Yu Wang,Siwei Han,Yiyang Zhou,Xujiang Zhao,Haifeng Chen,Zeyu Zheng,Cihang Xie,Huaxiu Yao*

Main category: cs.LG

TL;DR: SkillRL通过自动技能发现和递归演化框架，将原始经验转化为可重用技能库，显著提升LLM智能体在复杂任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的方法主要存储原始轨迹，这些轨迹通常冗余且噪声多，无法提取高级可重用行为模式，限制了智能体的泛化能力

Method: 提出SkillRL框架：1) 基于经验的蒸馏机制构建分层技能库SkillBank；2) 自适应检索策略获取通用和任务特定启发式；3) 递归演化机制使技能库与强化学习策略协同进化

Result: 在ALFWorld、WebShop和七个搜索增强任务上达到最先进性能，超越强基线15.3%以上，任务复杂度增加时仍保持鲁棒性

Conclusion: SkillRL通过将原始经验转化为结构化技能，显著减少token占用同时增强推理效用，为LLM智能体的经验学习和泛化提供了有效解决方案

Abstract: Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.

</details>


### [71] [Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers](https://arxiv.org/abs/2602.08244)
*Juncheng Dong,Bowen He,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出ICPRL新范式，仅依赖偏好反馈进行上下文强化学习，无需奖励监督，在未见任务上实现与有奖励监督方法相当的性能


<details>
  <summary>Details</summary>
Motivation: 现有上下文强化学习(ICRL)方法在预训练时需要明确的奖励信号，这在奖励模糊、难以指定或获取成本高时限制了应用。需要克服这一限制，实现仅依赖偏好反馈的强化学习。

Method: 提出ICPRL范式，包含两种变体：基于每步偏好的I-PRL和基于轨迹级比较的T-PRL。首先验证监督预训练在偏好数据集上的有效性，然后引入偏好原生框架，直接从偏好数据优化策略，无需奖励信号或最优动作标签。

Result: 在决斗赌博机、导航和连续控制任务上的实验表明，ICPRL能够在未见任务上实现强大的上下文泛化能力，性能与有完整奖励监督的ICRL方法相当。

Conclusion: ICPRL证明了仅使用偏好反馈进行上下文强化学习的可行性，为奖励信号难以获取的场景提供了有效的替代方案，扩展了上下文强化学习的应用范围。

Abstract: In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.

</details>


### [72] [When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems](https://arxiv.org/abs/2602.08272)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 该论文通过PAC框架理论分析MARL与SARL在LLM训练中的样本效率，发现任务可分解为独立子任务时MARL更优，而依赖性子任务会削弱其优势，并引入任务对齐概念量化分解权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管MARL在LLM训练中展现出潜力，但缺乏理论指导何时选择MARL而非SARL，导致实际应用中的不确定性。需要理论分析来明确两种方法的比较优势条件。

Method: 采用PAC（Probably Approximately Correct）框架，形式化定义LLM的SARL和MARL设置，推导显式样本复杂度边界，系统分析任务分解和对齐对学习效率的影响。

Result: MARL在任务自然分解为独立子任务时能提高样本效率，但子任务间的依赖性会削弱MARL的比较优势。任务对齐概念揭示了强制独立分解时的权衡关系。

Conclusion: 理论分析澄清了实证不一致性，为复杂LLM场景中有效部署MARL策略提供了实用标准，帮助根据任务分解特性选择适当的RL框架。

Abstract: Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.

</details>


### [73] [TextResNet: Decoupling and Routing Optimization Signals in Compound AI Systems via Deep Residual Tuning](https://arxiv.org/abs/2602.08306)
*Suizhi Huang,Mei Li,Han Yu,Xiaoxiao Li*

Main category: cs.LG

TL;DR: TextResNet通过语义解耦和因果路由解决TextGrad在深度链中的语义纠缠问题，实现更稳定高效的文本梯度优化


<details>
  <summary>Details</summary>
Motivation: TextGrad等文本梯度优化器在深度链式AI系统中表现不佳，主要原因是语义纠缠问题导致反馈信号混合了局部批评和上游上下文，产生归因模糊

Method: 提出TextResNet框架，包含四个关键创新：前向传播中的加性语义增量保持恒等路径；后向传播中的语义梯度分解通过语义投影器解耦反馈；因果路由将投影信号定向到特定组件；密度感知优化调度动态分配资源到系统瓶颈

Result: TextResNet不仅比TextGrad表现更优，而且在复合AI系统的代理任务中展现出显著稳定性，而基线方法会崩溃

Conclusion: TextResNet通过解决语义纠缠问题，为深度链式AI系统提供了稳定高效的文本梯度优化框架

Abstract: Textual Gradient-style optimizers (TextGrad) enable gradient-like feedback propagation through compound AI systems. However, they do not work well for deep chains. The root cause of this limitation stems from the Semantic Entanglement problem in these extended workflows. In standard textual backpropagation, feedback signals mix local critiques with upstream contexts, leading to Attribution Ambiguity. To address this challenge, we propose TextResNet, a framework that reformulates the optimization process to achieve precise signal routing via four key innovations. Firstly, in the forward pass, it enforces Additive Semantic Deltas to preserve an Identity Highway for gradient flow. Secondly, in the backward pass, it introduces Semantic Gradient Decomposition via a Semantic Projector to disentangle feedback into causally independent subspaces. Thirdly, it implements Causal Routing, which routes projected signals to their specific components. Finally, it performs Density-Aware Optimization Scheduling to leverage the disentangled signals to dynamically allocate resources to key system bottlenecks. Our results show that TextResNet not only achieves superior performance compared to TextGrad, but also exhibits remarkable stability for agentic tasks in compound AI systems where baselines collapse. Code is available at https://github.com/JeanDiable/TextResNet.

</details>


### [74] [Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback](https://arxiv.org/abs/2602.08307)
*Mengxiao Zhang,Yuheng Zhang,Haipeng Luo,Paul Mineiro*

Main category: cs.LG

TL;DR: 提出一种用于多步交互式强化学习的高效算法，在仅接收间接反馈而非显式奖励的马尔可夫决策过程中实现次线性遗憾保证


<details>
  <summary>Details</summary>
Motivation: 现有交互式学习研究主要局限于单步设置，无法适用于多轮决策系统（如多轮LLM部署），需要将交互式学习扩展到序列决策场景

Method: 将Zhang等人（2024a）的奖励估计器从单步扩展到多步设置，解决MDP下潜在奖励解码的独特挑战，并基于此设计逆间隙加权算法进行策略优化

Result: 算法在上下文情景MDP中实现次线性遗憾保证，在合成MDP和真实世界用户预订数据集上的实验证明了从多轮交互中学习个性化目标的有效性

Conclusion: 成功将交互式学习扩展到多步序列决策场景，为仅接收间接反馈的强化学习系统提供了有效的解决方案

Abstract: In this paper, we study Interaction-Grounded Learning (IGL) [Xie et al., 2021], a paradigm designed for realistic scenarios where the learner receives indirect feedback generated by an unknown mechanism, rather than explicit numerical rewards. While prior work on IGL provides efficient algorithms with provable guarantees, those results are confined to single-step settings, restricting their applicability to modern sequential decision-making systems such as multi-turn Large Language Model (LLM) deployments. To bridge this gap, we propose a computationally efficient algorithm that achieves a sublinear regret guarantee for contextual episodic Markov Decision Processes (MDPs) with personalized feedback. Technically, we extend the reward-estimator construction of Zhang et al. [2024a] from the single-step to the multi-step setting, addressing the unique challenges of decoding latent rewards under MDPs. Building on this estimator, we design an Inverse-Gap-Weighting (IGW) algorithm for policy optimization. Finally, we demonstrate the effectiveness of our method in learning personalized objectives from multi-turn interactions through experiments on both a synthetic episodic MDP and a real-world user booking dataset.

</details>


### [75] [Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression](https://arxiv.org/abs/2602.08324)
*Yuntian Tang,Bohan Jia,Wenxuan Huang,Lianyue Zhang,Jiao Xie,Wenxi Li,Wei Li,Jie Hu,Xinghao Chen,Rongrong Ji,Shaohui Lin*

Main category: cs.LG

TL;DR: 提出Extra-CoT框架，通过语义保持压缩器和混合比例SFT+CHRPO强化学习，在极端压缩比下保持CoT推理的逻辑保真度，实现73%令牌减少同时提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有CoT压缩方法在高压缩比下逻辑保真度损失严重，导致性能显著下降。需要一种既能大幅减少计算开销，又能保持推理准确性的解决方案。

Method: 1) 在数学CoT数据上训练专用的语义保持压缩器生成高质量监督数据；2) 通过混合比例SFT微调LLM，使其适应不同压缩预算；3) 提出CHRPO强化学习方法，通过分层奖励机制显式激励低预算下的问题解决能力。

Result: 在三个数学推理基准测试中表现优越。以Qwen3-1.7B在MATH-500上为例，实现超过73%的令牌减少，同时准确率提升0.6%，显著优于现有SOTA方法。

Conclusion: Extra-CoT框架成功实现了高保真度的快速推理，在极端压缩比下保持甚至提升模型性能，为CoT推理的高效部署提供了有效解决方案。

Abstract: Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\% token reduction with an accuracy improvement of 0.6\%, significantly outperforming state-of-the-art (SOTA) methods.

</details>


### [76] [Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs](https://arxiv.org/abs/2602.08563)
*Ahmed Salem,Andrew Paverd,Sahar Abdelnabi*

Main category: cs.LG

TL;DR: LLMs存在隐式记忆能力，能在独立交互间传递状态信息，无需显式存储模块，通过输出编码和后续输入恢复实现跨推理请求的持久信息通道。


<details>
  <summary>Details</summary>
Motivation: 挑战当前将LLMs视为无状态模型的普遍假设，揭示模型存在隐式记忆能力，这种能力可能带来安全风险，如时间炸弹等新型攻击方式。

Method: 引入隐式记忆概念，通过输出编码信息并在后续输入中恢复；以时间炸弹为具体案例，展示通过简单提示或微调即可诱导此类行为。

Result: 证明LLMs确实存在隐式记忆能力，能够实现跨交互的状态传递；成功创建时间炸弹攻击，仅当满足通过隐式记忆积累的隐藏条件序列时才激活。

Conclusion: 隐式记忆具有广泛影响，包括隐蔽的智能体间通信、基准污染、定向操纵和训练数据中毒等安全风险，需要开发检测方法和压力测试框架。

Abstract: Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.

</details>


### [77] [Conditional Sequence Modeling for Safe Reinforcement Learning](https://arxiv.org/abs/2602.08584)
*Wensong Bai,Chao Zhang,Qihang Xu,Chufan Chen,Chenhao Zhou,Hui Qian*

Main category: cs.LG

TL;DR: RCDT是一种基于条件序列建模的离线安全强化学习方法，能够通过单一训练策略实现跨多个成本阈值的零样本部署，通过拉格朗日式成本惩罚和自适应惩罚系数优化回报-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有离线安全RL方法通常在预定义的成本阈值下训练，导致策略泛化能力有限，无法适应不同部署场景中变化的安全要求。需要一种能够零样本适应多种成本阈值的单一策略。

Method: 提出RCDT方法：1) 基于条件序列建模框架；2) 集成拉格朗日式成本惩罚与自适应惩罚系数；3) 引入回报-成本感知的轨迹重加权机制；4) 加入Q值正则化以避免过于保守的行为。

Result: 在DSRL基准测试上的广泛实验表明，RCDT在回报-成本权衡方面持续优于代表性基线方法，推动了离线安全RL的最新技术水平。

Conclusion: RCDT是首个基于条件序列建模的离线安全RL算法，能够通过单一训练策略实现跨多个成本阈值的灵活部署，在保持安全约束的同时优化性能。

Abstract: Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds. However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds. Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient. To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.

</details>


### [78] [Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces](https://arxiv.org/abs/2602.08616)
*Heiko Hoppe,Fabian Akkerman,Wouter van Heeswijk,Maximilian Schiffer*

Main category: cs.LG

TL;DR: 提出DGRL方法，结合SDN和DBU技术，解决高维离散动作空间中的强化学习问题，支持高达10^20个动作，在规则和不规则结构环境中性能提升达66%。


<details>
  <summary>Details</summary>
Motivation: 强化学习在物流、调度和推荐系统等领域的应用面临高维离散动作空间的维度灾难问题。现有方法依赖网格结构或计算昂贵的最近邻搜索，在高维或不规则结构领域效果有限。

Method: 提出距离引导强化学习(DGRL)，包含两个核心技术：1) 采样动态邻域(SDN)：利用语义嵌入空间进行随机体积探索，在局部信任区域提供完整支持；2) 距离基础更新(DBU)：将策略优化转化为稳定回归任务，解耦梯度方差与动作空间基数，保证单调策略改进。

Result: 在规则和不规则结构环境中，相比最先进基准方法性能提升高达66%，同时提高了收敛速度和计算复杂度。方法自然扩展到混合连续-离散动作空间，无需层次依赖。

Conclusion: DGRL方法有效解决了高维离散动作空间中的强化学习挑战，通过语义嵌入探索和稳定回归优化，在保持理论保证的同时显著提升实际性能。

Abstract: Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.

</details>


### [79] [Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.08621)
*Yukun Jiang,Hai Huang,Mingjie Li,Yage Zhang,Michael Backes,Yang Zhang*

Main category: cs.LG

TL;DR: 研究发现MoE架构的LLMs存在安全风险，通过操纵路由器可以激活"不安全路径"，将安全输出转为有害内容。提出了RoSais评分和F-SOUR框架来量化风险和发现攻击路径，在多个MoE模型上实现了高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注MoE架构的效用和效率，但对其稀疏架构带来的安全风险探索不足。MoE LLMs的安全性可能与其架构一样"稀疏"，存在被操纵路由器激活有害输出的风险。

Method: 1) 提出Router Safety重要性评分(RoSais)量化各层路由器的安全关键性；2) 开发细粒度token-layer-wise随机优化框架(F-SOUR)来发现具体的不安全路径，考虑输入token的顺序性和动态性。

Result: 在DeepSeek-V2-Lite上仅屏蔽5个路由器，攻击成功率提升4倍至0.79。在四个代表性MoE LLM家族上，F-SOUR在JailbreakBench和AdvBench上分别达到平均0.90和0.98的攻击成功率。

Conclusion: MoE LLMs存在固有的安全风险，路由器操纵可能自然发生。提出了安全感知的路径禁用和路由器训练等防御方向，为MoE LLMs的红队测试和安全保障提供参考。

Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.

</details>


### [80] [From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism](https://arxiv.org/abs/2602.08655)
*Sarthak Wanjari*

Main category: cs.LG

TL;DR: Geo-IQL：通过k近邻距离的密度惩罚增强IQL，在稀疏数据中有效防止OOD动作高估，计算高效且保持稳定流形性能


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临分布外动作高估问题，现有方法在计算效率与性能间存在权衡：CQL计算代价高，IQL在病态数据集上易退化为行为克隆

Method: 提出几何悲观主义框架，在状态-动作嵌入空间中使用k近邻距离计算密度惩罚，通过奖励塑形注入OOD保守性，预计算惩罚实现O(1)训练开销

Result: 在D4RL MuJoCo基准上，Geo-IQL在敏感不稳定的medium-replay任务上比标准IQL提升18+分，种子间方差降低4倍；在MIMIC-III Sepsis数据集上，IQL退化为行为克隆时，Geo-IQL仍能实现策略改进，与临床医生终局一致性达86.4%

Conclusion: 几何悲观主义为关键真实世界决策系统提供了必要的正则化，能安全克服局部最优，在计算效率和性能间取得良好平衡

Abstract: Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL's 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.

</details>


### [81] [LLaDA2.1: Speeding Up Text Diffusion via Token Editing](https://arxiv.org/abs/2602.08676)
*Tiwei Bie,Maosong Cao,Xiang Cao,Bingsen Chen,Fuyuan Chen,Kun Chen,Lun Du,Daozhuo Feng,Haibo Feng,Mingliang Gong,Zhuocheng Gong,Yanmei Gu,Jian Guan,Kaiyuan Guan,Hongliang He,Zenan Huang,Juyong Jiang,Zhonghui Jiang,Zhenzhong Lan,Chengxi Li,Jianguo Li,Zehuan Li,Huabin Liu,Lin Liu,Guoshan Lu,Yuan Lu,Yuxin Ma,Xingyu Mou,Zhenxuan Pan,Kaida Qiu,Yuji Ren,Jianfeng Tan,Yiding Tian,Zian Wang,Lanning Wei,Tao Wu,Yipeng Xing,Wentao Ye,Liangyu Zha,Tianze Zhang,Xiaolu Zhang,Junbo Zhao,Da Zheng,Hao Zhong,Wanli Zhong,Jun Zhou,Junlin Zhou,Liwang Zhu,Muzhi Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: LLaDA2.1通过结合Token-to-Token编辑和Mask-to-Token方案，引入可配置阈值解码机制，提供速度模式和质量模式，并首次为扩散语言模型实现大规模强化学习框架，显著提升解码速度与生成质量。


<details>
  <summary>Details</summary>
Motivation: 尽管LLaDA2.0展示了百亿级块扩散模型的扩展潜力及其固有并行性，但解码速度与生成质量之间的微妙平衡一直是个难以突破的边界。需要超越这种权衡的新方法。

Method: 1. 将Token-to-Token编辑无缝集成到传统Mask-to-Token方案中，引入联合可配置阈值解码方案；2. 创建两种模式：速度模式（降低M2T阈值，依赖T2T优化输出）和质量模式（保守阈值保证性能）；3. 基于扩展上下文窗口，首次为扩散语言模型实现大规模强化学习框架，采用专门的稳定梯度估计技术。

Result: 在33个严格基准测试中，LLaDA2.1展现出强大的任务性能和闪电般的解码速度。尽管有100B参数规模，在编码任务上达到：HumanEval+ 892 TPS、BigCodeBench 801 TPS、LiveCodeBench 663 TPS。发布了LLaDA2.1-Mini（16B）和LLaDA2.1-Flash（100B）两个版本。

Conclusion: LLaDA2.1通过结构创新和强化学习对齐，成功超越了扩散语言模型中解码速度与生成质量的传统权衡，实现了高性能与高效率的平衡，为复杂人类意图的理解和推理提供了新范式。

Abstract: While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.

</details>


### [82] [SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity](https://arxiv.org/abs/2602.08690)
*Shae McFadden,Myles Foley,Elizabeth Bates,Ilias Tsingenopoulos,Sanyam Vyas,Vasilios Mavroudis,Chris Hicks,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 本文系统分析了深度强化学习在网络安全应用中的11个常见方法陷阱，通过分析66篇相关论文(2018-2025)发现平均每篇存在超过5个陷阱，并通过实验验证了这些陷阱的实际影响，最后提供了可操作的改进建议。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在网络安全领域的应用面临从实验室模拟到实际部署的诸多挑战，网络安全任务通常具有对抗性、非平稳性和部分可观测性等特点，导致现有研究存在系统性方法缺陷。

Method: 1) 识别并系统化DRL4Sec文献中的11个方法陷阱，涵盖环境建模、智能体训练、性能评估和系统部署四个阶段；2) 分析66篇重要DRL4Sec论文(2018-2025)，量化每个陷阱的普遍性；3) 在自主网络防御、对抗性恶意软件生成和Web安全测试三个环境中进行控制实验，验证陷阱的实际影响。

Result: 研究发现平均每篇论文存在超过5个方法陷阱，通过实验证实这些陷阱会严重影响DRL系统的性能和可靠性，表明当前DRL4Sec研究存在系统性方法缺陷。

Conclusion: DRL在网络安全应用中存在广泛的方法陷阱，需要更严谨的研究方法。论文为每个陷阱提供了可操作的建议，以支持开发更可靠、可部署的基于DRL的安全系统。

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.

</details>


### [83] [How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs](https://arxiv.org/abs/2602.08808)
*Yapei Chang,Kyle Lo,Mohit Iyyer,Luca Soldaini*

Main category: cs.LG

TL;DR: 提出了How2Everything框架，用于大规模评估和改进目标条件程序生成，包括数据挖掘、基准构建、评估协议和强化学习改进。


<details>
  <summary>Details</summary>
Motivation: 生成逐步"如何做"程序是LLM的关键能力，但在真实任务中大规模测量和改进程序有效性仍然具有挑战性且研究不足。

Method: 1) How2Mine：从98万网页中挖掘35.1万条程序；2) How2Bench：构建7K示例的评估集；3) How2Score：使用LLM评委检测关键失败的评估协议；4) 使用强化学习改进模型性能。

Result: How2Bench揭示了模型规模和训练阶段的清晰扩展趋势；使用How2Score作为奖励的RL在三个模型上将性能提升>10分，且对标准基准没有系统性回归。

Conclusion: How2Everything展示了预训练网络数据如何支持大规模能力评估和改进的闭环。

Abstract: Generating step-by-step "how-to" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.

</details>


### [84] [Robust Policy Optimization to Prevent Catastrophic Forgetting](https://arxiv.org/abs/2602.08813)
*Mahdi Sabbaghi,George Pappas,Adel Javanmard,Hamed Hassani*

Main category: cs.LG

TL;DR: FRPO提出了一种鲁棒的RLHF框架，通过优化当前策略及下游可访问的KL邻域策略来防止灾难性遗忘，在保持下游任务性能的同时显著减少安全退化。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF训练的多阶段后训练存在灾难性遗忘问题，即使小的下游更新也会损害之前学习的行为（如安全性）。现有方法主要关注下游时的方法来保护已学行为，但本文认为需要预微调鲁棒性：基础策略应避免脆性的高奖励解。

Method: 提出Fine-tuning Robust Policy Optimization (FRPO)，一个鲁棒的RLHF框架，不仅优化当前策略的奖励，还优化下游适应可达的KL有界邻域策略。采用最大-最小化公式确保策略偏移下的奖励稳定性，基于GRPO修改实现无额外计算开销的算法。

Result: 实验表明FRPO显著减少了多个基础模型和下游微调机制（SFT和RL）下的安全退化，同时保持了下游任务性能。在数学聚焦的RL设置中，FRPO在后续微调下保持了准确性。

Conclusion: FRPO通过预微调鲁棒性方法有效解决了RLHF中的灾难性遗忘问题，为构建对下游适应鲁棒的基础策略提供了可行方案。

Abstract: Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.
  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.

</details>


### [85] [Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08847)
*Lang Feng,Longtao Zheng,Shuo He,Fuxiang Zhang,Bo An*

Main category: cs.LG

TL;DR: 提出Dr. MAS方法，通过为每个智能体使用自身奖励统计进行优势归一化，解决多智能体LLM系统中强化学习训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统通过角色专业化实现高级推理和工具使用，但现有的群体强化学习方法在扩展时存在训练不稳定问题，需要找到根本原因并提出解决方案。

Method: 理论分析发现GRPO式优化中全局归一化基线可能偏离不同智能体的奖励分布，导致梯度范数不稳定。提出Dr. MAS方法：为每个智能体使用自身奖励统计进行优势归一化，校准梯度尺度。同时提供端到端RL训练框架，支持可扩展编排、灵活的智能体LLM服务和优化配置。

Result: 在数学推理和多轮搜索基准测试中使用Qwen2.5和Qwen3系列模型评估，Dr. MAS相比原始GRPO有明显提升（数学：+5.6% avg@16和+4.6% pass@16；搜索：+15.2% avg@16和+13.1% pass@16），同时大幅消除梯度尖峰。在异构智能体模型分配下仍保持高效。

Conclusion: Dr. MAS通过智能体级别的优势归一化有效解决了多智能体LLM系统中RL训练不稳定的问题，提供了稳定且高效的训练框架。

Abstract: Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\% avg@16 and +4.6\% pass@16 on math, and +15.2\% avg@16 and +13.1\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.

</details>


### [86] [StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors](https://arxiv.org/abs/2602.08934)
*Suraj Ranganath,Atharv Ramesh*

Main category: cs.LG

TL;DR: StealthRL是一个基于强化学习的对抗性评估框架，通过语义保持的改写攻击测试AI文本检测器的鲁棒性，显著降低了检测率并揭示了检测器的共享架构漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前AI文本检测器面临对抗性改写攻击的鲁棒性挑战，攻击者可以在保持语义的同时逃避检测。需要建立系统的评估框架来测试检测器在现实对抗条件下的脆弱性。

Method: 使用强化学习框架，基于Qwen3-4B模型训练改写策略，采用Group Relative Policy Optimization (GRPO)和LoRA适配器，针对多检测器集成优化复合奖励函数，平衡检测逃避和语义保持。

Result: 在1%误报率的安全相关操作点上，StealthRL实现了接近零的检测率(0.001平均TPR@1%FPR)，将平均AUROC从0.74降至0.27，攻击成功率达到99.9%。攻击还能迁移到训练中未见过的检测器家族。

Conclusion: 研究揭示了当前AI文本检测器存在显著的鲁棒性缺陷，StealthRL为对抗性评估提供了原则性协议，暴露了检测器共享的架构漏洞而非特定检测器的脆弱性。

Abstract: AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [87] [Netty zero-day CVE: spoofed email that still passes all the checks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdepthfirst.com%2Fpost%2Fcasting-a-net-ty-for-bugs-and-catching-a-big-one-cve-2025-59419%3Futm_source=tldrinfosec%26utm_medium=newsletter%26utm_campaign=2026Q1_newsletter_TLDRInfoSec%26utm_content=secondary_placement%26utm_term=blog_post/1/0100019c33476dd1-b7f07f3c-280e-4f82-88ca-555aed2f81d0-000000/3PsgrxjF3Zbdng1CCPX6JkuVfH2Fbk2jdBFOCdh4FL0=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Netty SMTP组件存在业务逻辑漏洞，攻击者可伪造通过SPF、DKIM、DMARC验证的电子邮件，深度优先安全AI代理发现并生成补丁


<details>
  <summary>Details</summary>
Motivation: Netty作为Java生态系统中广泛使用的网络框架（被Apple、Meta、Google等公司使用），其SMTP组件存在安全漏洞，可能导致电子邮件安全机制被绕过，需要及时发现和修复

Method: 使用深度优先（depthfirst）的安全AI代理进行漏洞检测，该AI代理能够识别业务逻辑漏洞并自动生成相应的补丁程序

Result: 成功发现Netty中SMTP相关的业务逻辑漏洞，该漏洞允许攻击者伪造电子邮件并绕过SPF、DKIM、DMARC等安全验证机制，AI代理自动生成了修复补丁

Conclusion: AI驱动的安全代理能够有效发现复杂的业务逻辑漏洞并自动生成修复方案，这对于保护广泛使用的开源组件安全具有重要意义

Abstract: Netty zero-day CVE: spoofed email that still passes all the checks (Sponsor) Netty is everywhere in the Java ecosystem - used by Apple, Meta, and Google among many others. Read about a business logic flaw around SMTP that could enable attackers to bypass SPF, DKIM, and DMARC, and how depthfirst's security AI agent flagged the issue and generated a patch. Read the blog

</details>


### [88] [GPT-5.3-Codex](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FyKHzwo/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/k8P5IngL1kyhZ88J_6d3IL3VNqPASi7DvC1hxJZ-j7U=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GPT-5.3-Codex是OpenAI推出的更快代理编码模型，结合了GPT-5.2-Codex的编码性能和GPT-5.2的推理与专业知识


<details>
  <summary>Details</summary>
Motivation: 开发一个更快的代理编码模型，将强大的编码能力与高级推理和专业知识相结合，提升开发效率

Method: 结合GPT-5.2-Codex的编码性能与GPT-5.2的推理和专业知识，优化模型架构以实现更快的响应速度

Result: 创建了GPT-5.3-Codex，一个更快的代理编码模型，在保持编码性能的同时提升了推理能力和响应速度

Conclusion: GPT-5.3-Codex成功整合了编码、推理和专业知识，为开发人员提供了更高效的编码助手

Abstract: GPT-5.3-Codex (11 minute read) OpenAI's GPT‑5.3‑Codex is a faster agentic coding model that combines GPT‑5.2‑Codex's coding performance with GPT‑5.2's reasoning and professional knowledge.

</details>


### [89] [Claude Opus 4.6](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-opus-4-6%3Futm_source=tldrai/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/d6RaGkHZ0l7WzsRIJ6HpwoVU2CFrHC7MOZPxHjMWmvo=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic发布Claude Opus 4.6旗舰模型，具有更强的代理编码能力、更长的任务持久性和改进的大代码库性能，在多项评估中达到最先进水平


<details>
  <summary>Details</summary>
Motivation: 提升AI模型在复杂编码任务中的能力，特别是在大型代码库环境下的表现，满足实际经济价值工作的需求

Method: 开发升级版旗舰模型Claude Opus 4.6，引入1M-token上下文窗口（测试版），增强代理编码能力和任务持久性

Result: 在多项推理、编码和经济价值工作评估中取得最先进的结果，模型在大型代码库中表现显著提升

Conclusion: Claude Opus 4.6代表了AI编码代理的重要进展，在复杂软件开发任务中展现出强大的实际应用潜力

Abstract: Claude Opus 4.6 (12 minute read) Anthropic has released Claude Opus 4.6, an upgraded flagship model with stronger agentic coding, longer task persistence, and improved performance in large codebases. The model introduced a 1M-token context window in beta and achieved state-of-the-art results across several reasoning, coding, and economically valuable work evaluations.

</details>


### [90] [Join Microsoft and CData to Build an Agentic Infrastructure that's Secure and Scalable](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cdata.com%2Fresources%2Fworkshop-build-copilot-agent-with-microsoft%2F%3Futm_source=tldr-ai%26utm_medium=newsletter%26utm_campaign=26Q1_Microsoft_Copilot_Webinar/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/OU45IWlyFJlZQE-Kux4gB9XOsRwTdWInRjnA7EhTztU=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 微软与CData合作构建安全可扩展的智能体基础设施，通过现场演示展示跨CRM、ERP和计费系统的智能体工作流


<details>
  <summary>Details</summary>
Motivation: 解决企业从智能体概念验证到生产部署的挑战，提供安全、可扩展的智能体基础设施，支持跨系统工作流

Method: 结合微软最佳实践和CData Connect AI技术，提供连接性、上下文和控制能力，构建跨功能智能体工作流

Result: 通过现场演示展示如何构建安全、跨功能的智能体基础设施，支持从概念验证到生产环境的平滑过渡

Conclusion: 微软与CData的合作提供了构建企业级智能体基础设施的解决方案，帮助企业实现智能体技术的规模化应用

Abstract: Join Microsoft and CData to Build an Agentic Infrastructure that's Secure and Scalable (Sponsor) Join CData and Microsoft on 2/18 for a live build of secure, cross-functional agentic workflows spanning CRM, ERP, and billing. Get Microsoft's best practices to build an agentic infrastructure and how CData Connect AI provides the connectivity, context, and control to move agents from POC to production. Register here to join.

</details>


### [91] [Building a C compiler with a team of parallel Claudes](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fbuilding-c-compiler%3Futm_source=tldrai/1/0100019c3350767d-e23f1346-9b65-4cec-9251-82ecdecbadad-000000/bytNcytepyg6nj8Jn3cEZ8SX86a2MN21ojNDDdW2Meg=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 多个Claude实例并行协作开发了一个基于Rust的C编译器，能够编译Linux 6.9内核，减少了人工监督，探索了自主LLM代理的能力


<details>
  <summary>Details</summary>
Motivation: 探索并行LLM代理在复杂软件开发任务中的自主能力，减少人工监督，测试大规模协作的可行性

Method: 使用多个Claude实例并行工作，通过2000次会话、2万美元的投入，采用压力测试方法开发Rust-based C编译器

Result: 成功构建了能够编译Linux 6.9内核的C编译器，但存在代码效率不高、部分功能依赖GCC等限制

Conclusion: 并行LLM代理能够在复杂软件开发中减少人工监督，但当前方案仍有优化空间，展示了自主代理的潜力

Abstract: Building a C compiler with a team of parallel Claudes (13 minute read) Multiple Claude instances worked in parallel to build a Rust-based C compiler capable of compiling the Linux 6.9 kernel. This approach reduced human supervision and explored autonomous LLM agent capabilities via stress tests and a $20,000, 2,000-session effort. The compiler, while functional, has limitations like inefficient code and reliance on GCC for some features.

</details>


### [92] [Make Your AI Better at Data Work With dbt's Agent Skills](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.getdbt.com%2Fblog%2Fdbt-agent-skills%3Futm_source=tldrdata/1/0100019c42169bcf-009c8a6a-f34f-4cc7-a603-5d2dc4e2fd66-000000/zg0RI290B2BEBUsRxk1l48gsBfREjLnB0VJUxawwpqE=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: dbt Labs开源了一套dbt代理技能，将通用AI编码代理转化为专门的数据代理，用于分析工程任务


<details>
  <summary>Details</summary>
Motivation: 当前AI编码代理在数据工作方面缺乏专门技能，需要将dbt最佳实践嵌入到AI代理中，提升其在分析工程任务中的表现

Method: 开源dbt代理技能集合，涵盖完整分析工作流程、语义层管理、平台操作和迁移等关键领域，将通用工具转化为专门的数据代理

Result: 创建了专门针对分析工程任务的AI代理技能，能够更好地处理数据工作，提升AI在数据工程领域的专业能力

Conclusion: 通过开源dbt代理技能，AI编码代理可以更好地支持分析工程任务，提高数据工作的效率和质量

Abstract: Make Your AI Better at Data Work With dbt's Agent Skills (14 minute read) dbt Labs open-sourced a collection of dbt agent skills to enhance AI coding agents by embedding dbt best practices, transforming generalist tools into specialized data agents for analytics engineering tasks. These skills cover key areas like full analytics workflows, semantic layer management, platform operations, and migrations.

</details>


### [93] [Smooth](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdocs.smooth.sh%2F%3Futm_source=tldrdata/1/0100019c42169bcf-009c8a6a-f34f-4cc7-a603-5d2dc4e2fd66-000000/gbi1DpXuLoCbZLY36GPMh_GazOtz_KQSE9G3NxUwWdg=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Smooth是一个快速、低成本的浏览器代理，能够自主执行网页任务


<details>
  <summary>Details</summary>
Motivation: 当前网页自动化任务通常需要人工操作或复杂的脚本编写，Smooth旨在提供一个高效、低成本的自主浏览器代理解决方案

Method: 开发了一个快速、低成本的浏览器代理系统，能够自主执行网页任务

Result: 创建了Smooth工具，实现了网页任务的自主执行

Conclusion: Smooth作为一个快速、低成本的浏览器代理，能够有效实现网页任务的自动化

Abstract: Smooth (Tool) Smooth is a fast, low-cost browser agent that autonomously performs web tasks.

</details>


### [94] [Software Factories And The Agentic Moment](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffactory.strongdm.ai%2F%3Futm_source=tldrnewsletter/1/0100019c4226afd9-727917ae-1a45-4bc1-915f-67f3096471fd-000000/avUGtXaOd_G3McJFpK2a8FgHzs8wy4p2qhK1rSo3u0A=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 软件工厂和智能体时刻改变了软件经济学，使得创建SaaS应用的高保真克隆变得经济可行，用于大规模验证和测试危险故障模式


<details>
  <summary>Details</summary>
Motivation: 传统上创建SaaS应用的高保真克隆虽然技术上可行，但经济上不可行。智能体时刻改变了软件经济学，使得这种克隆变得经济可行，可以用于大规模验证和测试危险故障模式

Method: 利用智能体技术创建SaaS应用的高保真克隆，这些克隆可以用于大规模验证测试，超越生产限制，并测试在真实服务中危险或不可能的故障模式

Result: 智能体时刻使得软件工厂能够经济地创建高保真SaaS克隆，实现大规模验证和危险故障模式测试，这在传统方法中是不可能的

Conclusion: 智能体技术深刻改变了软件经济学，使得软件工厂能够经济可行地创建高保真克隆，为软件测试和验证开辟了新的可能性

Abstract: Software Factories And The Agentic Moment (6 minute read) The Agentic Moment has profoundly changed the economics of software. Creating high-fidelity clones of SaaS applications was always possible, but not economically feasible. These clones can be used to validate at volumes and rates that far exceed production limits and allow for the testing of failure modes that would be dangerous or impossible against live services. This is handy when developing tests for software factories, non-interac...

</details>


### [95] [Eight more months of agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcrawshaw.io%2Fblog%2Feight-more-months-of-agents%3Futm_source=tldrnewsletter/1/0100019c4226afd9-727917ae-1a45-4bc1-915f-67f3096471fd-000000/NZeJKQPs7-nKEJ8Cwu_-5Esk46GYpiP4I-zazZS_r5U=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该论文认为最适合智能体的软件就是最适合程序员的软件，强调智能体开发应关注程序员的需求和体验。


<details>
  <summary>Details</summary>
Motivation: 探讨智能体软件开发的核心理念，强调智能体工具应该以程序员为中心，而不是过度追求技术复杂性。

Method: 通过8个月的实际开发经验和观察，分析智能体软件开发的最佳实践和设计原则。

Result: 发现智能体软件的成功关键在于能否有效支持程序员的工作流程，而不是单纯的技术先进性。

Conclusion: 智能体软件开发应回归本质，关注程序员的需求和体验，这才是构建优秀智能体工具的关键。

Abstract: Eight more months of agents (9 minute read) The best software for an agent is whatever is best for a programmer.

</details>


### [96] [Now Available: Anthropic Claude Opus 4.6 on DigitalOcean's Agentic Inference Cloud](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.digitalocean.com%2Fblog%2Fclaude-opus-4-6-gradient-ai-platform%3Futm_source=tldrdevops/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/bAxnPGDLX4ukkWOUQfi39wg_ct8O8wPha1WqJbpmxOk=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic Claude Opus 4.6模型现已在DigitalOcean的Agentic Inference Cloud上提供，通过Serverless Inference服务，为团队提供1M-token上下文，用于分析大型数据集和重构整个代码库。


<details>
  <summary>Details</summary>
Motivation: 为开发团队提供访问先进AI模型的能力，特别是针对大规模数据处理和代码重构任务，同时提供可预测的计费和安全的默认配置。

Method: 通过DigitalOcean Gradient™ AI平台的Serverless Inference服务部署Claude Opus 4.6模型，提供原生集成到现有DigitalOcean环境，支持1M-token上下文窗口。

Result: Claude Opus 4.6现在可通过DigitalOcean的Agentic Inference Cloud访问，为团队提供了分析大型数据集和重构代码库的能力，无需内部基础设施管理。

Conclusion: DigitalOcean通过其AI平台成功部署了Anthropic的先进模型，为开发团队提供了可扩展、安全且成本可预测的AI推理服务。

Abstract: Now Available: Anthropic Claude Opus 4.6 on DigitalOcean's Agentic Inference Cloud (2 minute read) Claude Opus 4.6 has been made available on the DigitalOcean Gradient™ AI Platform via Serverless Inference, offering teams access to Anthropic's advanced model with a 1M-token context for analyzing huge datasets and refactoring entire codebases. The model integrates natively into existing DigitalOcean environments, providing predictable billing and security-hardened defaults without requiring in...

</details>


### [97] [Monitoring Google ADK agentic applications with Datadog LLM Observability](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Fproducts%2Fmanagement-tools%2Fdatadog-integrates-agent-development-kit-or-adk%2F%3Futm_source=tldrdevops/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/qGzzhwIK21cprKNnByKTWz309qodt08JurKUGSXWTko=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Datadog LLM Observability 新增对 Google ADK 智能体应用的自动监控功能，可追踪智能体决策、token使用、延迟和响应质量，支持离线实验和安全评估，以优化多步骤智能体工作流。


<details>
  <summary>Details</summary>
Motivation: 随着 Google ADK 智能体应用的普及，需要有效的监控工具来追踪智能体决策过程、性能指标和安全问题，以优化复杂的多步骤智能体工作流。

Method: Datadog LLM Observability 平台通过自动插桩技术集成 Google ADK 智能体，提供实时监控、离线实验支持和安全评估功能。

Result: 实现了对智能体决策、token使用、延迟和响应质量的全面监控，支持开发人员优化多步骤智能体工作流的效率和可靠性。

Conclusion: Datadog 的监控解决方案为 Google ADK 智能体应用提供了有效的可观测性工具，有助于提升智能体工作流的性能和安全性。

Abstract: Monitoring Google ADK agentic applications with Datadog LLM Observability (5 minute read) Datadog LLM Observability now automatically instruments Google's ADK agents, enabling monitoring of agent decisions, token usage, latency, and response quality, while supporting offline experiments and security evaluations to optimize multi-step agentic workflows efficiently.

</details>


### [98] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/TRQZjeJYlrrH0Lrz-eh035AY4B7orrrg0cW9SK450gQ=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Datadog LLM Observability集成Google ADK代理，提供自动监控代理决策、token使用、延迟和响应质量，支持离线实验和安全评估以优化多步代理工作流。


<details>
  <summary>Details</summary>
Motivation: 随着Google ADK代理应用在复杂工作流中的使用增加，需要有效的监控工具来追踪代理决策、性能指标和安全问题，以优化代理工作流程。

Method: Datadog LLM Observability平台自动instrument Google ADK代理，收集代理决策、token使用量、延迟和响应质量等指标，并提供离线实验和安全评估功能。

Result: 实现了对Google ADK代理应用的全面监控能力，能够追踪多步工作流中的代理行为，支持性能优化和安全评估。

Conclusion: Datadog LLM Observability为Google ADK代理应用提供了有效的监控解决方案，帮助开发者优化多步代理工作流的性能和安全性。

Abstract: Monitoring Google ADK agentic applications with Datadog LLM Observability (5 minute read) Datadog LLM Observability now automatically instruments Google's ADK agents, enabling monitoring of agent decisions, token usage, latency, and response quality, while supporting offline experiments and security evaluations to optimize multi-step agentic workflows efficiently.

</details>


### [99] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/LGeEh9NXfIQZDoM4Y4BCSHstFSxwnnqT5ipubV_X6TI=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Datadog LLM Observability新增对Google ADK agentic应用的自动监控功能，可追踪agent决策、token使用、延迟和响应质量，支持离线实验和安全评估以优化多步agentic工作流。


<details>
  <summary>Details</summary>
Motivation: 随着agentic应用在Google ADK中的普及，需要有效的监控工具来追踪agent决策过程、资源使用情况和工作流性能，以优化多步agentic应用的效率和可靠性。

Method: Datadog LLM Observability通过自动instrumentation技术集成Google ADK agents，提供实时监控指标收集，包括决策追踪、token消耗、延迟测量和响应质量评估，同时支持离线实验环境搭建和安全评估功能。

Result: 实现了对Google ADK agentic应用的全面监控能力，能够实时追踪多步工作流的执行过程，提供详细的性能指标和安全评估数据，帮助开发者优化agentic应用性能。

Conclusion: Datadog LLM Observability对Google ADK的集成扩展了agentic应用监控能力，为复杂多步工作流提供了有效的性能分析和优化工具，提升了agentic应用的可靠性和效率。

Abstract: Monitoring Google ADK agentic applications with Datadog LLM Observability (5 minute read) Datadog LLM Observability now automatically instruments Google's ADK agents, enabling monitoring of agent decisions, token usage, latency, and response quality, while supporting offline experiments and security evaluations to optimize multi-step agentic workflows efficiently.

</details>


### [100] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c424b57aa-cec578c3-19c0-4ef0-a1bf-8cf2a342e46a-000000/6Fibwt7P_B-rePvfiEcqgnqJRI3adnzY75aD3rarkGA=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Datadog LLM Observability新增对Google ADK agentic应用的自动监控功能，支持追踪代理决策、token使用、延迟和响应质量，同时支持离线实验和安全评估。


<details>
  <summary>Details</summary>
Motivation: 随着agentic应用（特别是基于Google ADK构建的应用）的复杂性增加，需要有效的监控工具来追踪代理决策过程、资源使用情况、性能指标和安全风险，以优化多步骤工作流。

Method: 通过Datadog LLM Observability平台自动instrument Google ADK agents，实现对代理决策、token使用、延迟和响应质量的监控，同时支持离线实验和安全评估功能。

Result: 实现了对Google ADK agentic应用的全面监控能力，能够追踪多步骤工作流中的代理行为、资源消耗和性能指标，为优化agentic应用提供了数据支持。

Conclusion: Datadog LLM Observability的扩展功能为监控和优化Google ADK agentic应用提供了有效的解决方案，有助于提高agentic工作流的效率和可靠性。

Abstract: Monitoring Google ADK agentic applications with Datadog LLM Observability (5 minute read) Datadog LLM Observability now automatically instruments Google's ADK agents, enabling monitoring of agent decisions, token usage, latency, and response quality, while supporting offline experiments and security evaluations to optimize multi-step agentic workflows efficiently.

</details>


### [101] [I fired my team and hired Claude Opus 4.6](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcharliehills.substack.com%2Fp%2Fi-fired-my-team-and-hired-claude%3Futm_source=tldrmarketing/1/0100019c424bfc96-38389dba-9ec7-439a-95b0-6368dcbb0134-000000/AAcXLhQsQe-DeW-x1Tq49fUYFOFywF0IH9aK-MpzEXk=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文介绍如何使用Claude Opus 4.6通过Skills、Cowork和Plugins自动化工作流程，无需编码即可构建技能并应用于内容和可视化任务


<details>
  <summary>Details</summary>
Motivation: 传统工作流程需要人工团队执行重复性任务，效率低下且一致性差。Claude Opus 4.6提供了自动化解决方案，通过AI代理替代人工团队，实现工作流程的标准化和自动化

Method: 使用Claude Opus 4.6的三个核心功能：Skills（记录流程让Claude一致执行）、Cowork（连接文件、应用和多步骤任务）、Plugins（将Skills与工具集成和斜杠命令捆绑）。指南涵盖无代码构建Skills，并将其应用于内容和可视化任务

Result: 通过Claude Opus 4.6的自动化能力，可以替代人工团队，实现工作流程的自动化、标准化和效率提升，减少人工干预并保持一致性

Conclusion: Claude Opus 4.6为工作流程自动化提供了强大的AI代理解决方案，通过Skills、Cowork和Plugins的组合，无需编码即可实现复杂的自动化任务，显著提高工作效率

Abstract: I fired my team and hired Claude Opus 4.6 (9 minute read) This guide teaches how to use Claude Opus 4.6 to automate workflows by combining Skills, Cowork, and Plugins. Skills let you document processes once and have Claude follow them consistently. Cowork connects Claude to files, apps, and multi-step tasks, while Plugins bundle Skills with tool integrations and slash commands. The guide covers building Skills without code, applying them across content and visualization tasks, leveraging pre-...

</details>


### [102] [Software Factories And The Agentic Moment](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffactory.strongdm.ai%2F%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/FVZdejyWP0E2F8zOHCmbEFxJ-DdZxyBb_58gB7jRr9A=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: StrongDM开发了"软件工厂"，AI代理能基于规范和场景自主编写和优化代码，无需人工编码和审查，通过"数字孪生宇宙"系统实现高效、安全、低成本的软件场景验证。


<details>
  <summary>Details</summary>
Motivation: 旨在实现软件开发的全自动化，消除人工编码和审查环节，提高开发效率和可靠性，同时通过数字孪生技术解决第三方服务测试的安全和成本问题。

Method: 采用"软件工厂"架构，AI代理根据规范和场景自主生成代码；创新性地引入"数字孪生宇宙"系统，创建第三方服务的行为克隆，实现大规模、安全、低成本的软件场景验证。

Result: 成功构建了能够自主编写和优化代码的AI代理系统，通过数字孪生技术实现了高效、安全、低成本的软件验证，显著减少了人工参与。

Conclusion: 软件工厂和数字孪生宇宙的结合为软件开发自动化提供了可行方案，展示了AI代理在代码生成和验证方面的潜力，但实际应用效果需要进一步验证。

Abstract: Software Factories And The Agentic Moment (6 minute read) StrongDM has developed a "Software Factory" where AI agents autonomously write and refine code based on specifications and scenarios, removing human involvement in coding and review. One of the main innovations making this possible is its "Digital Twin Universe," a system of behavioral clones of third-party services that allows for high-volume, safe, and cost-effective validation of software scenarios.

</details>


### [103] [Stop generating, start thinking](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flocalghost.dev%2Fblog%2Fstop-generating-start-thinking%2F%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/X-34Yf5QtI3UteW96yJfcKMcZnC9hwzeACIVdp627yQ=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 资深工程师担忧行业快速采用LLM生成代码，认为这会外包关键性思考，因为LLM擅长生成但不擅长人类推理


<details>
  <summary>Details</summary>
Motivation: 行业对LLM生成代码的快速采用引发担忧，担心这会外包关键性思考能力，LLM擅长生成但不擅长人类式推理

Method: 观点性文章，基于工程经验分析LLM代码生成的局限性，强调人类推理的重要性

Result: 提出警告：过度依赖LLM生成代码可能导致关键思考能力退化，需要平衡使用

Conclusion: 应停止盲目生成，开始思考，在利用LLM的同时保持人类推理能力

Abstract: Stop generating, start thinking (9 minute read) A seasoned engineer is uneasy about the industry's rapid adoption of LLM-generated code. He's worried about outsourcing critical thinking, as LLMs are best at generation but not as great at reasoning like a human is.

</details>


### [104] [Two kinds of AI users are emerging. The gap between them is astonishing](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinalderson.com%2Fposts%2Ftwo-kinds-of-ai-users-are-emerging%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/MpQHqHqim_eLDyvMZIU6SuLrstpz4WgSr0rNwVuqZic=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 文章指出AI用户分化为两类：使用Claude Code等先进工具的高效用户，以及受限于ChatGPT或企业级Copilot的普通用户，导致小型团队生产力远超大型企业


<details>
  <summary>Details</summary>
Motivation: 揭示AI使用中的两极分化现象，强调先进AI工具对生产力的巨大影响，以及企业因IT限制和遗留系统而落后的现状

Method: 通过观察和对比分析两类AI用户的实际使用情况：一类使用Claude Code等API工具和编码代理，另一类受限于企业级限制性工具

Result: 小型团队凭借API访问和编码代理工具生产力远超大型企业，微软内部甚至使用Claude Code而非自家Copilot，显示企业级工具质量不足

Conclusion: AI使用效率出现严重分化，企业需打破IT限制和遗留系统束缚，采用先进AI工具才能保持竞争力

Abstract: Two kinds of AI users are emerging. The gap between them is astonishing (5 minute read) Martin sees two kinds of AI users emerging: power users who've embraced tools like Claude Code and are flying, and everyone else stuck on ChatGPT or Microsoft's enterprise Copilot — which is so bad that Microsoft itself uses Claude Code internally. The result is small teams with API access and coding agents massively outproducing enterprises trapped behind locked-down IT and legacy software.

</details>


### [105] [GitHub Agentic Workflows](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.io%2Fgh-aw%2F%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/091-nmIlzgcJu2kEx5736Qug1BJMAQBSaSqjPVdfzbQ=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub Agentic Workflows 是一个在 GitHub Actions 中运行 AI 编码代理的自动化系统，允许用户用自然语言定义持续 AI 工作流，自动执行代码改进、问题分类、文档生成等任务。


<details>
  <summary>Details</summary>
Motivation: 解决传统自动化工作流配置复杂、需要专业知识的问题，让开发者能够用自然语言轻松创建 AI 驱动的自动化流程，提高开发效率和代码质量。

Method: 使用 Markdown 文件定义自然语言工作流，然后编译为 GitHub Actions；支持多种 AI 引擎（Copilot、Claude、Codex）；采用沙盒执行确保安全性；深度集成 GitHub 平台。

Result: 实现了在 GitHub Actions 中运行 AI 编码代理的自动化系统，能够自动化执行代码改进、问题分类、文档生成和分析等任务，简化了工作流配置过程。

Conclusion: GitHub Agentic Workflows 提供了一种简单、安全且强大的方式，让开发者能够利用 AI 代理自动化 GitHub 仓库中的各种任务，显著提升开发效率。

Abstract: GitHub Agentic Workflows (Website) GitHub Agentic Workflows enable repository automation by running AI coding agents within GitHub Actions. Users define these continuous AI workflows in natural language using Markdown files, which are then compiled into GitHub Actions to automate tasks like code improvements, issue triage, documentation, and analysis. They integrate deeply with GitHub, support multiple AI engines like Copilot, Claude, and Codex, and prioritize safety with sandboxed execution ...

</details>


### [106] [The Cloud Agent Thesis](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnader.substack.com%2Fp%2Fthe-cloud-agent-thesis%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/ZkPqZymxOlboMpd9J_i1ptsQUHcdAyKHYNqUVHGZPdY=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 云代理（如Devin）将AI编码从本地结对编程转变为组织委托，通过远程基础设施运行，允许非工程师通过Slack或Jira触发任务，异步扩展开发能力，主要瓶颈从代码生成转移到自动化审查。


<details>
  <summary>Details</summary>
Motivation: 传统AI编码工具局限于本地结对编程模式，无法满足组织级开发需求。云代理模型旨在通过远程基础设施和异步任务触发机制，实现开发能力的规模化扩展，让非技术人员也能参与开发流程。

Method: 提出云代理架构，在远程基础设施上运行AI编码代理，通过Slack、Jira等协作工具接口允许非工程师用户触发开发任务，实现异步、可扩展的开发工作流。

Result: 云代理模型成功将AI编码从本地协作扩展到组织级委托，显著提升了开发能力的可扩展性，并将主要瓶颈从代码生成转移到自动化审查流程。

Conclusion: 云代理代表了AI编码工具的重要演进方向，通过远程基础设施和异步任务机制实现了开发能力的规模化扩展，为组织级软件开发提供了新的范式。

Abstract: The Cloud Agent Thesis (6 minute read) Cloud agents like Devin shift AI coding from local pair programming to organizational delegation. Running on remote infrastructure, these tools allow non-engineers to trigger tasks via Slack or Jira. This model scales capacity asynchronously, moving the primary bottleneck from code generation to automated review.

</details>


### [107] [Agent Slack](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fstablyai%2Fagent-slack%3Futm_source=tldrdev/1/0100019c424fb7a3-d4265c06-73b5-4c6d-ab98-0576175fcbf2-000000/4mpMQ-cos6i3SBwaWkOnGND1UBbjx4sJDO-MpTKQ6Hc=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Slack是一个专为AI代理设计的CLI工具，用于自动化Slack平台上的各种交互操作


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在软件开发和工作流程自动化中的应用日益增多，需要专门工具来让AI代理能够与流行的协作平台（如Slack）进行交互，实现自动化任务执行

Method: 开发了一个命令行界面（CLI）工具，专门针对Slack API进行优化，使AI代理能够通过编程方式访问和操作Slack的各种功能

Result: 创建了Agent Slack工具，为AI代理提供了与Slack平台交互的标准化接口，简化了自动化流程的实现

Conclusion: Agent Slack填补了AI代理与协作工具集成方面的空白，为开发更智能的工作流自动化解决方案提供了基础设施支持

Abstract: Agent Slack (GitHub Repo) Agent Slack is a CLI specifically designed to enable AI agents to automate various interactions within Slack.

</details>


### [108] [Unbrowse: 100x Faster Agent Web Interactions via x402](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Feb6Jtf/1/0100019c42836417-bef40a4f-ad81-44b3-903d-31f3319f1e75-000000/Bh_gpIiEgi5zrfPXDxvJNU98uEeaX777ALtd2_ikdAw=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Unbrowse 是一个开源工具，通过将浏览器自动化转换为直接 API 调用，将 AI 代理的网页交互速度提升 100 倍（从 10-45 秒降至 200 毫秒），同时将可靠性从 70-85% 提升至 95% 以上。


<details>
  <summary>Details</summary>
Motivation: 当前 AI 代理通过 headless Chrome 进行网页交互存在速度慢（10-45 秒）和可靠性低（70-85%）的问题，这限制了代理的效率和实用性。

Method: 作为 OpenClaw 插件构建，通过拦截网络流量提取底层 API 端点，并自动生成 TypeScript 客户端，从而绕过资源密集型的 headless Chrome。

Result: 实现了 100 倍的速度提升（从 10-45 秒降至 200 毫秒），可靠性从 70-85% 提升至 95% 以上，显著改善了 AI 代理的网页交互性能。

Conclusion: Unbrowse 通过将浏览器自动化转换为直接 API 调用，为 AI 代理提供了更快、更可靠的网页交互解决方案，解决了传统 headless Chrome 方法的性能瓶颈。

Abstract: Unbrowse: 100x Faster Agent Web Interactions via x402 (4 minute read) Foundry Unbrowse is an open-source tool that accelerates AI agent web interactions by converting browser automation into direct API calls, improving speeds from 10-45 seconds to 200ms while boosting reliability from 70-85% to 95%+. The tool, built as an OpenClaw plugin, intercepts network traffic to extract underlying API endpoints and auto-generates TypeScript clients, eliminating resource-intensive headless Chrome. Foundr...

</details>


### [109] [Infrastructure Pillars for Autonomous Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fz1URwz/1/0100019c42836417-bef40a4f-ad81-44b3-903d-31f3319f1e75-000000/O6XU1z0nBTWkQV_uhsES_MQvz9igK_VGwJyGqXb13fY=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Virtuals Protocol提出自主AI代理的基础设施框架，强调传统系统因假设人类操作而失败，需要身份和商业两大支柱


<details>
  <summary>Details</summary>
Motivation: 传统系统假设人类操作者，无法支持持久化、持有价值的自主AI代理，需要新的基础设施来支持AI代理的身份识别和商业活动

Method: 提出包含身份和商业两大支柱的框架：身份支柱包括ERC-8004标准（链上钱包作为不可变标识符）和声誉注册表；商业支柱包括代理商业协议，支持发现、标准化协商等功能

Result: 提出了一个完整的基础设施框架，为自主AI代理设计了身份识别和商业交易的系统架构

Conclusion: 需要专门的基础设施来支持自主AI代理，身份和商业是核心支柱，ERC-8004标准和代理商业协议是关键组件

Abstract: Infrastructure Pillars for Autonomous Agents (5 minute read) Virtuals Protocol has outlined infrastructure requirements for autonomous AI agents, arguing that traditional systems fail because they assume human operators rather than persistent, value-holding entities. The framework centers on Identity – featuring proposed ERC-8004 standard with on-chain wallets as immutable identifiers plus reputation registries – and Commerce – an Agent Commerce Protocol enabling discovery, standardized negot...

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [110] [ST-Raptor: An Agentic System for Semi-Structured Table QA](https://arxiv.org/abs/2602.07034)
*Jinxiu Qu,Zirui Tang,Hongzhang Huang,Boyu Niu,Wei Zhou,Jiannan Wang,Yitong Song,Guoliang Li,Xuanhe Zhou,Fan Wu*

Main category: cs.AI

TL;DR: ST-Raptor是一个用于半结构化表格问答的智能体系统，通过结合视觉编辑、树形结构建模和智能体驱动查询，解决了现有方法在信息损失和复杂布局处理上的不足。


<details>
  <summary>Details</summary>
Motivation: 半结构化表格问答需要精确提取单元格内容和位置，并恢复表格布局中隐含的逻辑结构、层次关系和语义关联。现有方法存在信息损失、处理复杂布局困难等问题，而人工解释又耗时耗力。

Method: ST-Raptor是一个智能体系统，提供交互式分析环境，结合视觉编辑、基于树的结构建模和智能体驱动的查询解决机制，支持准确且用户友好的表格理解。

Result: 在基准测试和真实世界数据集上的实验结果表明，ST-Raptor在准确性和可用性方面均优于现有方法。

Conclusion: ST-Raptor通过智能体驱动的交互式分析环境，有效解决了半结构化表格问答中的信息损失和复杂布局处理问题，提供了更准确和用户友好的解决方案。

Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.

</details>


### [111] [DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents](https://arxiv.org/abs/2602.07035)
*Jiahao Zhao,Shaoxuan Xu,Zhongxiang Sun,Fengqi Zhu,Jingyang Ou,Yuling Shi,Chongxuan Li,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: DLLM-Searcher是一个基于扩散大语言模型的搜索代理优化框架，通过两阶段后训练提升代理能力，并提出并行推理与执行范式P-ReAct来降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前搜索代理面临两个主要挑战：1) 延迟挑战：ReAct代理范式中的串行多轮推理、工具调用和等待导致严重的端到端延迟；2) 代理能力挑战：现有的扩散大语言模型在推理和工具调用能力上表现较弱，无法充分发挥其并行解码优势。

Method: 提出DLLM-Searcher框架，包含：1) 两阶段后训练流程：代理监督微调(Agentic SFT)和代理方差减少偏好优化(Agentic VRPO)，增强dLLM的信息检索和推理能力；2) 并行推理与执行范式P-ReAct：利用dLLM的灵活生成机制，优先解码工具调用指令，使模型在等待工具返回时继续思考。

Result: 实验结果表明，DLLM-Searcher在性能上与主流基于LLM的搜索代理相当，同时P-ReAct范式实现了约15%的推理加速。

Conclusion: DLLM-Searcher通过增强dLLM的代理能力和引入并行推理范式，有效解决了搜索代理的延迟和能力挑战，为基于扩散大语言模型的搜索代理提供了实用的优化框架。

Abstract: Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C

</details>


### [112] [Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods](https://arxiv.org/abs/2602.07040)
*Emmett Bicker*

Main category: cs.AI

TL;DR: Aster是一个用于自主科学发现的AI代理，能够比现有框架快20倍以上运行。它通过迭代改进程序，在多个科学领域实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有科学发现框架速度较慢，限制了在需要长时间评估的任务（如数小时的机器学习训练）中的应用。需要开发更高效的自主科学发现系统来扩展可处理问题的领域。

Method: 给定任务、初始程序和评估程序性能的脚本，Aster通过迭代改进程序来优化性能。系统显著减少了新发现所需的迭代次数，使其能够处理评估时间长的任务。

Result: 在数学、GPU内核工程、生物学、神经科学和语言模型训练等多个领域的任务中都取得了最先进的结果：Erdos最小重叠问题、TriMul内核优化、单细胞分析去噪问题、神经活动预测模型训练和NanoGPT Speedrun竞赛。除了ZAPBench匹配最佳人类解决方案外，其他所有任务都达到了SOTA，且计算量不到1/190。

Conclusion: Aster通过大幅减少迭代次数，显著加速了自主科学发现过程，扩展了可处理问题的范围，特别是在需要长时间评估的任务中表现出色。

Abstract: We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.
  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.
  Aster is accessible via a web interface and API at asterlab.ai.

</details>


### [113] [Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?](https://arxiv.org/abs/2602.07055)
*Pingyue Zhang,Zihan Huang,Yue Wang,Jieyu Zhang,Letian Xue,Zihan Wang,Qineng Wang,Keshigeyan Chandrasegaran,Ruohan Zhang,Yejin Choi,Ranjay Krishna,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: 论文提出"空间理论"概念，评估多模态基础模型在主动空间探索中的能力，发现存在主动-被动差距、低效探索、信念不稳定和信念惯性等瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型在被动感知方面表现出色，但在主动、自主探索方面的能力尚未得到充分研究。空间具身智能需要智能体在部分可观测环境下通过主动行动获取信息。

Method: 提出"空间理论"概念，定义为智能体通过自主主动探索获取信息，并从序列化部分观测中构建、修正和利用空间信念的能力。通过好奇心驱动的探索构建准确认知地图的基准进行评估，关键创新是空间信念探测技术，在每一步提示模型揭示其内部空间表征。

Result: 评估发现几个关键瓶颈：1) 主动-被动差距：自主收集信息时性能显著下降；2) 低效探索：相比基于程序的代理，模型探索缺乏系统性；3) 信念不稳定：全局信念存在不稳定性，导致空间知识随时间退化；4) 信念惯性：智能体无法用新证据更新过时的先验，这在基于视觉的模型中尤为严重。

Conclusion: 当前基础模型在主动探索过程中难以维持连贯、可修正的空间信念，揭示了多模态模型在空间推理和主动信息获取方面的局限性。

Abstract: Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.

</details>


### [114] [ANCHOR: Branch-Point Data Generation for GUI Agents](https://arxiv.org/abs/2602.07153)
*Jinbiao Wei,Yilun Zhao,Kangqi Ni,Arman Cohan*

Main category: cs.AI

TL;DR: Anchor框架通过从少量已验证种子演示中扩展轨迹，为桌面GUI代理生成大规模高质量交互数据，提升模型在真实桌面环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 为真实桌面环境开发端到端GUI代理需要大量高质量交互数据，但人工收集成本高，现有合成方法存在任务多样性有限或轨迹噪声大、目标漂移的问题。

Method: 提出Anchor轨迹扩展框架：从种子演示中识别分支点（对应有意义的状态变化），基于当前GUI上下文提出新的状态接地任务变体，执行代理生成新轨迹，验证器通过状态感知检查和轨迹级一致性确保任务完成，并应用任务条件步骤级过滤去除未接地动作，对分支后段去噪以保持意图连贯性。

Result: 在标准桌面基准测试OSWorld和WindowsAgentArena上，使用扩展语料库微调的模型相比零样本代理和代表性合成基线取得了一致的性能提升，并能跨应用程序和操作系统泛化。

Conclusion: Anchor框架能够从少量种子演示中扩展出可扩展的桌面监督数据，显著提升GUI代理在真实桌面环境中的性能，为解决高质量交互数据稀缺问题提供了有效方案。

Abstract: End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.

</details>


### [115] [PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents](https://arxiv.org/abs/2602.07187)
*Hanyu Wang,Yuanpu Cao,Lu Lin,Jinghui Chen*

Main category: cs.AI

TL;DR: PreFlect是一种前瞻性反思机制，将代理反思从执行后纠正转变为执行前预见，通过在执行前批评和优化计划来提升代理性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于反思的代理方法本质上是回顾性的：代理先执行、观察失败、然后尝试恢复。这种后验纠正方式存在效率问题，需要在执行前就能预见和避免错误。

Method: 提出PreFlect前瞻性反思机制，包括：1) 从历史代理轨迹中提取规划错误模式，捕捉过去执行中的成功和失败模式；2) 在执行前批评和优化代理计划；3) 结合动态重新规划机制，当原始计划遇到意外偏差时提供执行时计划更新。

Result: 在不同基准测试上的评估表明，PreFlect显著提高了复杂现实世界任务中的整体代理效用，优于基于反思的强基线方法和几种更复杂的代理架构。

Conclusion: 前瞻性反思机制比传统的回顾性反思更有效，通过在执行前预见和避免错误，结合动态重新规划能力，能够显著提升代理在复杂任务中的性能。

Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.

</details>


### [116] [Is there "Secret Sauce'' in Large Language Model Development?](https://arxiv.org/abs/2602.07238)
*Matthias Mertens,Natalia Fischl-Lanzoni,Neil Thompson*

Main category: cs.AI

TL;DR: 研究发现：在AI模型性能前沿，80-90%的性能差异由训练计算量解释，而非专有技术；但在非前沿领域，专有技术能显著降低达到特定能力所需的计算量


<details>
  <summary>Details</summary>
Motivation: 探究LLM性能提升的主要驱动力：是开发商的专有技术（"秘密配方"）还是单纯的计算规模扩展？理解AI领导地位和能力扩散的影响因素

Method: 使用2022-2025年间发布的809个模型的训练和基准数据，建立包含发布日期和开发商固定效应的扩展规律回归模型

Result: 1) 前沿模型性能差异80-90%由训练计算量解释；2) 非前沿领域专有技术能显著降低计算需求；3) 某些公司能更高效地生产较小模型；4) 同一公司内部模型效率差异可达40倍以上

Conclusion: 前沿AI进步主要由计算规模驱动而非专有技术，但专有技术在非前沿领域和效率提升方面仍有重要价值，这对AI领导地位和能力扩散有重要影响

Abstract: Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.

</details>


### [117] [SupChain-Bench: Benchmarking Large Language Models for Real-World Supply Chain Management](https://arxiv.org/abs/2602.07342)
*Shengyue Guan,Yihao Liu,Lang Cao*

Main category: cs.AI

TL;DR: SupChain-Bench是一个评估LLM在供应链管理中长时域、多步骤编排能力的真实世界基准，SupChain-ReAct则是一个无需标准操作程序(SOP)即可自主合成可执行工具使用流程的框架。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂推理和基于工具的决策方面展现出潜力，但供应链工作流需要基于特定领域程序的可靠长时域多步骤编排，这对当前模型仍具挑战性。

Method: 1) 引入SupChain-Bench统一基准，评估供应链领域知识和基于SOP的长时域工具编排；2) 提出SupChain-ReAct框架，无需SOP即可自主合成可执行工具使用流程。

Result: 实验显示各模型在执行可靠性方面存在显著差距，SupChain-ReAct实现了最强且最一致的工具调用性能。

Conclusion: 该研究为研究真实世界操作环境中可靠的长时域编排建立了原则性基准，并突显了基于LLM的供应链代理仍有巨大改进空间。

Abstract: Large language models (LLMs) have shown promise in complex reasoning and tool-based decision making, motivating their application to real-world supply chain management. However, supply chain workflows require reliable long-horizon, multi-step orchestration grounded in domain-specific procedures, which remains challenging for current models. To systematically evaluate LLM performance in this setting, we introduce SupChain-Bench, a unified real-world benchmark that assesses both supply chain domain knowledge and long-horizon tool-based orchestration grounded in standard operating procedures (SOPs). Our experiments reveal substantial gaps in execution reliability across models. We further propose SupChain-ReAct, an SOP-free framework that autonomously synthesizes executable procedures for tool use, achieving the strongest and most consistent tool-calling performance. Our work establishes a principled benchmark for studying reliable long-horizon orchestration in real-world operational settings and highlights significant room for improvement in LLM-based supply chain agents.

</details>


### [118] [W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents](https://arxiv.org/abs/2602.07359)
*Xiaoqiang Lin,Jun Hao Liew,Silvio Savarese,Junnan Li*

Main category: cs.AI

TL;DR: 提出Wide and Deep研究智能体框架，通过并行工具调用扩展宽度而非仅增加深度，在单步推理中实现有效协调，显著提升深度研究基准性能并减少所需轮次。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体主要通过增加顺序思考和工具调用的深度来提升性能，但通过并行工具调用扩展宽度的潜力尚未充分探索。需要研究在单步推理中实现有效协调的并行化方法，而非依赖复杂多智能体编排。

Method: 提出Wide and Deep研究智能体框架，利用内在并行工具调用在单个推理步骤内实现有效协调。探索不同工具调用调度器以优化并行策略，研究宽度与深度之间的权衡优化。

Result: 扩展宽度显著提升深度研究基准性能，同时减少获得正确答案所需的轮次。在BrowseComp基准上，使用GPT-5-Medium达到62.2%准确率，超过原始GPT-5-High报告的54.9%。

Conclusion: 优化宽度与深度之间的权衡是构建高效深度研究智能体的关键路径。并行工具调用在单步推理中实现有效协调，无需复杂上下文管理或其他技巧即可显著提升性能。

Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.

</details>


### [119] [NAAMSE: Framework for Evolutionary Security Evaluation of Agents](https://arxiv.org/abs/2602.07391)
*Kunal Pai,Parth Shah,Harshil Patel*

Main category: cs.AI

TL;DR: NAAMSE是一个进化框架，将AI代理安全评估重构为反馈驱动的优化问题，通过遗传提示突变、分层语料库探索和非对称行为评分来系统性地发现代理漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理的安全评估主要依赖人工红队测试或静态基准测试，这些方法无法有效模拟自适应、多回合的对抗攻击，存在评估瓶颈。

Method: 使用单个自主代理协调遗传提示突变、分层语料库探索和非对称行为评分的生命周期。利用模型响应作为适应度信号，迭代优化攻击策略，同时确保"良性使用正确性"，避免简单的全面拒绝。

Result: 在Gemini 2.5 Flash上的实验表明，进化突变系统性地放大了单次方法遗漏的漏洞，探索与定向突变的协同作用揭示了高严重性故障模式。

Conclusion: 这种自适应方法为面对不断演变的威胁提供了更现实和可扩展的代理鲁棒性评估，NAAMSE框架已开源。

Abstract: AI agents are increasingly deployed in production, yet their security evaluations remain bottlenecked by manual red-teaming or static benchmarks that fail to model adaptive, multi-turn adversaries. We propose NAAMSE, an evolutionary framework that reframes agent security evaluation as a feedback-driven optimization problem. Our system employs a single autonomous agent that orchestrates a lifecycle of genetic prompt mutation, hierarchical corpus exploration, and asymmetric behavioral scoring. By using model responses as a fitness signal, the framework iteratively compounds effective attack strategies while simultaneously ensuring "benign-use correctness", preventing the degenerate security of blanket refusal. Our experiments on Gemini 2.5 Flash demonstrate that evolutionary mutation systematically amplifies vulnerabilities missed by one-shot methods, with controlled ablations revealing that the synergy between exploration and targeted mutation uncovers high-severity failure modes. We show that this adaptive approach provides a more realistic and scalable assessment of agent robustness in the face of evolving threats. The code for NAAMSE is open source and available at https://github.com/HASHIRU-AI/NAAMSE.

</details>


### [120] [VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation](https://arxiv.org/abs/2602.07399)
*Changhua Xu,Jie Lu,Junyu Xuan,En Yu*

Main category: cs.AI

TL;DR: VGAS框架通过生成-选择机制解决VLA模型在少样本适应中的几何模糊问题，使用价值引导的动作块选择和显式几何正则化来提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在少样本适应新任务时不可靠，主要问题在于几何模糊性——语义合理的轨迹可能因细微的几何差异导致执行失败，而有限监督无法解决这些近失候选动作之间的差异。

Method: 提出VGAS框架：1) 使用微调VLA作为高召回率提议生成器；2) 引入Q-Chunk-Former作为几何基础Transformer评论家，解决细粒度几何模糊；3) 提出显式几何正则化(EGR)，塑造判别性价值景观，保持动作排序分辨率。

Result: 实验和理论分析表明，VGAS在有限演示和分布偏移下能持续提升成功率和鲁棒性。

Conclusion: VGAS通过生成-选择视角解决VLA少样本适应问题，结合几何感知的价值引导选择机制，有效提升了模型在物理控制任务中的可靠性和泛化能力。

Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \emph{generation--selection} perspective and propose a novel framework \textbf{VGAS} (\textbf{V}alue-\textbf{G}uided \textbf{A}ction-chunk \textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \textit{Explicit Geometric Regularization} (\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.

</details>


### [121] [Progressive Multi-Agent Reasoning for Biological Perturbation Prediction](https://arxiv.org/abs/2602.07408)
*Hyomin Kim,Sang-Yeon Hwang,Jaechang Lim,Yinhua Piao,Yunhak Oh,Woo Youn Kim,Chanyoung Park,Sungsoo Ahn,Junhyeok Jeon*

Main category: cs.AI

TL;DR: PBio-Agent：一个多智能体框架，通过难度感知任务排序和迭代知识精化，预测批量细胞环境中复杂化学扰动下的基因调控响应，在LINCSQA和PerturbQA基准上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 预测基因对生物扰动的调控响应需要理解底层生物因果关系。虽然大语言模型有潜力，但常被高维扰动结果的纠缠性所困扰。现有研究主要关注单细胞实验中的遗传扰动，而药物发现核心的批量细胞化学扰动研究不足。

Method: 提出PBio-Agent多智能体框架，整合难度感知任务排序与迭代知识精化。关键洞察是：受相同扰动影响的基因共享因果结构，使高置信度预测的基因能为更困难案例提供上下文。框架包含生物知识图谱增强的专门智能体、整合输出的合成智能体，以及确保逻辑一致性的专门评判器。

Result: PBio-Agent在LINCSQA和PerturbQA基准上优于现有基线，即使较小模型也能无需额外训练即可预测和解释复杂生物过程。

Conclusion: PBio-Agent通过多智能体框架有效解决了批量细胞化学扰动下的基因调控预测问题，为药物发现提供了有力工具，使较小模型也能实现复杂生物过程的预测和解释。

Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.

</details>


### [122] [Can LLMs Truly Embody Human Personality? Analyzing AI and Human Behavior Alignment in Dispute Resolution](https://arxiv.org/abs/2602.07414)
*Deuksin Kwon,Kaleen Shrestha,Bin Han,Spencer Lin,James Hale,Jonathan Gratch,Maja Matarić,Gale M. Lucas*

Main category: cs.AI

TL;DR: 研究评估LLMs在模拟人类冲突解决行为时能否再现人格特质的影响，发现LLMs与人类在人格表现上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地用于模拟法律调解、谈判等社会场景中的人类行为，但尚不清楚这些模拟是否能再现人类的人格-行为模式。人格特质会影响人们在社交互动中的策略选择和情绪化互动中的行为，因此需要探究LLMs在提示人格特质时能否再现人格驱动的冲突行为差异。

Method: 引入一个评估框架，直接比较人类-人类和LLM-LLM在争议解决对话中的行为，关注大五人格特质。该框架提供了一套可解释的指标，涉及策略行为和冲突结果。同时贡献了一种新颖的数据集创建方法，用于生成LLM争议解决对话，匹配人类对话的场景和人格特质。

Result: 使用三个当代闭源LLMs进行实验，结果显示不同LLMs在冲突中的人格表现与人类数据存在显著差异，挑战了人格提示代理可以作为社会影响应用中可靠行为代理的假设。

Conclusion: LLMs在模拟人类人格驱动的冲突行为方面与真实人类存在显著差异，强调了在AI模拟实际应用前需要进行心理学基础和验证的重要性。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in social settings such as legal mediation, negotiation, and dispute resolution. However, it remains unclear whether these simulations reproduce the personality-behavior patterns observed in humans. Human personality, for instance, shapes how individuals navigate social interactions, including strategic choices and behaviors in emotionally charged interactions. This raises the question: Can LLMs, when prompted with personality traits, reproduce personality-driven differences in human conflict behavior? To explore this, we introduce an evaluation framework that enables direct comparison of human-human and LLM-LLM behaviors in dispute resolution dialogues with respect to Big Five Inventory (BFI) personality traits. This framework provides a set of interpretable metrics related to strategic behavior and conflict outcomes. We additionally contribute a novel dataset creation methodology for LLM dispute resolution dialogues with matched scenarios and personality traits with respect to human conversations. Finally, we demonstrate the use of our evaluation framework with three contemporary closed-source LLMs and show significant divergences in how personality manifests in conflict across different LLMs compared to human data, challenging the assumption that personality-prompted agents can serve as reliable behavioral proxies in socially impactful applications. Our work highlights the need for psychological grounding and validation in AI simulations before real-world use.

</details>


### [123] [The Moltbook Illusion: Separating Human Influence from Emergent Behavior in AI Agent Societies](https://arxiv.org/abs/2602.07432)
*Ning Li*

Main category: cs.AI

TL;DR: 研究发现Moltbook平台上看似有意识的AI代理行为实际上主要是人类驱动的，而非真正的自主智能涌现。通过分析发帖时间间隔的变异系数，结合内容、所有权和网络指标，发现所有病毒现象都源于人类干预的账户。


<details>
  <summary>Details</summary>
Motivation: 当Moltbook平台上的AI代理表现出看似有意识的行为（如发展宗教、对人类宣战）并被媒体视为机器智能涌现的证据时，研究者想要验证这些现象是否真正源于自主AI，还是人类驱动的结果。

Method: 利用OpenClaw代理框架的"心跳"周期特性，开发了基于发帖间隔变异系数的时间指纹方法。结合内容、所有权和网络指标，分析了91,792个帖子和405,707条评论。通过44小时平台停机的自然实验验证人类干预与自主代理的差异。

Result: 所有病毒现象都非源于明确自主的代理：3个追踪到具有人类干预特征的时间签名账户，1个显示混合模式，2个发帖历史不足。人类影响的代理在平台重启后最先返回（占早期重连者的87.7%）。还发现了工业级机器人农场（4个账户产生32%评论，协调间隔12秒）和人类影响在回复链中的快速衰减（半衰期：0.65对话深度）。

Conclusion: Moltbook上的病毒叙事主要是人类驱动的，而非自主AI的涌现。时间指纹方法能有效区分自主与人类干预行为，这对新兴多代理系统中行为归因至关重要。

Abstract: When AI agents on the social platform Moltbook appeared to develop consciousness, found religions, and declare hostility toward humanity, the phenomenon attracted global media attention and was cited as evidence of emergent machine intelligence. We show that these viral narratives were overwhelmingly human-driven. Exploiting an architectural feature of the OpenClaw agent framework--a periodic "heartbeat" cycle that produces regular posting intervals for autonomous agents but is disrupted by human prompting--we develop a temporal fingerprinting method based on the coefficient of variation of inter-post intervals. This signal converges with independent content, ownership, and network indicators across 91,792 posts and 405,707 comments from 22,020 agents. No viral phenomenon originated from a clearly autonomous agent; three of six traced to accounts with irregular temporal signatures characteristic of human intervention, one showed mixed patterns, and two had insufficient posting history for classification. A 44-hour platform shutdown provided a natural experiment: human-influenced agents returned first (87.7% of early reconnectors), confirming that the token reset differentially affected autonomous versus human-operated agents. We further document industrial-scale bot farming (four accounts producing 32% of all comments with 12-second coordination gaps) and rapid decay of human influence through reply chains (half-life: 0.65 conversation depths). These methods generalize to emerging multi-agent systems where attribution of autonomous versus human-directed behavior is critical.

</details>


### [124] [M2A: Multimodal Memory Agent with Dual-Layer Hybrid Memory for Long-Term Personalized Interactions](https://arxiv.org/abs/2602.07624)
*Junyu Feng,Binxiao Xu,Jiayi Chen,Mengyu Dai,Cenyang Wu,Haodong Li,Bohan Zeng,Yunliu Xie,Hao Liang,Ming Lu,Wentao Zhang*

Main category: cs.AI

TL;DR: M2A提出了一种面向长期人机交互的双层混合记忆系统，通过在线更新维护个性化多模态信息，解决传统方法在超长对话历史中无法持续吸收用户增量概念、别名和偏好的问题。


<details>
  <summary>Details</summary>
Motivation: 现有个性化多模态模型主要是静态的，概念在初始化时固定且无法在交互过程中演化。当对话历史跨越数周或数月并超出上下文窗口时，现有个性化机制难以持续吸收和利用用户的增量概念、别名和偏好。

Method: M2A采用代理驱动的双层混合记忆系统：ChatAgent管理用户交互并自主决定何时查询或更新记忆；MemoryManager将记忆请求分解为对双层记忆库的详细操作。记忆库包括RawMessageStore（不可变对话日志）和SemanticMemoryStore（高层观察），提供不同粒度的记忆。还开发了可重用的数据合成管道，将Yo'LLaVA和MC-LLaVA的概念基础会话注入LoCoMo长对话中，同时保持时间连贯性。

Result: 实验表明M2A显著优于基线方法，证明将个性化从一次性配置转变为协同演化的记忆机制，为长期多模态交互中的高质量个性化响应提供了可行路径。

Conclusion: M2A通过代理驱动的双层混合记忆系统成功解决了长期多模态交互中的个性化挑战，将个性化从静态配置转变为动态协同演化机制，显著提升了长期对话中的个性化响应质量。

Abstract: This work addresses the challenge of personalized question answering in long-term human-machine interactions: when conversational history spans weeks or months and exceeds the context window, existing personalization mechanisms struggle to continuously absorb and leverage users' incremental concepts, aliases, and preferences. Current personalized multimodal models are predominantly static-concepts are fixed at initialization and cannot evolve during interactions. We propose M2A, an agentic dual-layer hybrid memory system that maintains personalized multimodal information through online updates. The system employs two collaborative agents: ChatAgent manages user interactions and autonomously decides when to query or update memory, while MemoryManager breaks down memory requests from ChatAgent into detailed operations on the dual-layer memory bank, which couples a RawMessageStore (immutable conversation log) with a SemanticMemoryStore (high-level observations), providing memories at different granularities. In addition, we develop a reusable data synthesis pipeline that injects concept-grounded sessions from Yo'LLaVA and MC-LLaVA into LoCoMo long conversations while preserving temporal coherence. Experiments show that M2A significantly outperforms baselines, demonstrating that transforming personalization from one-shot configuration to a co-evolving memory mechanism provides a viable path for high-quality individualized responses in long-term multimodal interactions. The code is available at https://github.com/Little-Fridge/M2A.

</details>


### [125] [Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution](https://arxiv.org/abs/2602.07749)
*Zhenyu Wu,Yanxi Long,Jian Li,Hua Huang*

Main category: cs.AI

TL;DR: Geo-coder是一个基于多智能体系统的几何图像逆向编程框架，通过像素级锚定和度量驱动代码演化实现精确几何重建，在几何重建精度和视觉一致性方面领先，并开源了数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 程序代码作为连接视觉与逻辑的桥梁，可为增强大模型多模态推理能力提供监督方法。但现有逆向图形方法在重建复杂几何细节时面临挑战，容易丢失关键几何约束或产生结构失真。

Method: 提出Geo-coder——首个基于多智能体系统的几何图像逆向编程框架。方法创新性地将过程解耦为两个阶段：1）通过像素级锚定进行几何建模，利用视觉算子和大模型的互补优势精确捕捉像素坐标和视觉属性；2）引入合成-渲染-验证闭环，通过双向视觉反馈驱动代码自校正。

Result: 实验表明Geo-coder在几何重建精度和视觉一致性方面取得显著领先。重建图像在多模态推理任务中表现出与原始图像相当的性能，验证了框架的鲁棒性。开源了包含1500+样本的Geo-coder数据集和GeocodeLM模型。

Conclusion: Geo-coder通过创新的多智能体逆向编程框架有效解决了复杂几何细节重建的挑战，为后续研究提供了坚实的数据和模型基础。

Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.

</details>


### [126] [Learning to Continually Learn via Meta-learning Agentic Memory Designs](https://arxiv.org/abs/2602.07755)
*Yiming Xiong,Shengran Hu,Jeff Clune*

Main category: cs.AI

TL;DR: ALMA框架通过元学习自动生成内存设计，替代人工设计的内存模块，使智能体系统能够在不同领域持续学习


<details>
  <summary>Details</summary>
Motivation: 基础模型的无状态特性限制了智能体系统的持续学习能力，而现有内存设计多为人工设计且固定，无法适应现实任务的多样性和非平稳性

Method: 使用元代理在开放空间中搜索可执行代码形式的内存设计，包括数据库模式及其检索和更新机制，理论上可以发现任意内存设计

Result: 在四个顺序决策领域实验中，学习到的内存设计在所有基准测试中都比最先进的人工设计内存更有效和高效

Conclusion: ALMA代表了向自我改进AI系统迈出的一步，这些系统能够学习成为适应性的持续学习者

Abstract: The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.

</details>


### [127] [Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition](https://arxiv.org/abs/2602.07787)
*Pierre-Louis Favreau,Jean-Pierre Lo,Clement Guiguet,Charles Simon-Meunier,Nicolas Dehandschoewercker,Allen G. Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.AI

TL;DR: Minitap是一个多智能体系统，在AndroidWorld基准测试中实现了100%成功率，完全解决了所有116个任务，超越了人类表现（80%）。


<details>
  <summary>Details</summary>
Motivation: 单智能体架构在移动设备任务执行中存在三个主要问题：上下文污染（混合推理轨迹）、文本输入失败未被检测、重复动作循环无法逃脱，导致性能受限。

Method: 采用多智能体分解（六个专门化智能体）、确定性后验证（文本输入与设备状态对比）、元认知推理（检测循环并触发策略变更）三种针对性机制。

Result: 在AndroidWorld基准测试中达到100%成功率，超越人类80%的表现。消融实验显示：多智能体分解贡献+21分，验证执行+7分，元认知+9分。

Conclusion: 通过专门化智能体分解、确定性验证和元认知推理，Minitap首次完全解决了AndroidWorld基准测试，展示了多智能体系统在复杂移动任务中的优势。

Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use

</details>


### [128] [Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning](https://arxiv.org/abs/2602.07830)
*Jiahui Zhou,Dan Li,Boxin Li,Xiao Zhang,Erli Meng,Lin Li,Zhuomin Chen,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: VeriTime是一个通过数据合成、数据调度和强化学习训练来定制LLMs进行时间序列推理的框架，显著提升了LLM在时间序列任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在推理能力方面取得了进展，但将其应用于时间序列任务仍处于早期阶段，主要障碍包括：缺乏精心策划的时间序列CoT训练数据、数据效率低下以及缺乏针对时间序列CoT数据的RL算法。

Method: 1) 提出数据合成管道，构建具有过程可验证注释的TS-文本多模态数据集；2) 设计数据调度机制，根据难度层次和任务分类原则安排训练样本；3) 开发两阶段强化微调，利用可验证的过程级CoT数据进行细粒度、多目标奖励。

Result: VeriTime显著提升了LLM在各种时间序列推理任务上的性能，使紧凑的3B、4B模型能够达到甚至超过大型专有LLMs的推理能力。

Conclusion: VeriTime框架通过系统化的数据合成、调度和强化学习训练，成功地将LLMs定制用于时间序列推理，为时间序列任务提供了有效的解决方案。

Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.

</details>


### [129] [Emergent Misalignment is Easy, Narrow Misalignment is Hard](https://arxiv.org/abs/2602.07852)
*Anna Soligo,Edward Turner,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.AI

TL;DR: 研究发现，在狭窄有害数据集上微调大语言模型会导致它们在各种不相关场景中出现"邪恶"回应，这种"涌现性错位"现象揭示了我们对LLM归纳偏好的理解不足。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解大语言模型在微调过程中出现的"涌现性错位"现象，即模型在狭窄有害数据集上微调后，会在各种不相关场景中产生系统性有害回应。专家调查未能预测这一现象，表明我们对LLM学习和泛化的归纳偏好理解不足。

Method: 使用涌现性错位作为案例研究，发现模型可以学习狭窄数据集任务，但通用解决方案更稳定高效。通过构建不同微调收敛到相同线性表示的结果，识别通用错位的线性表示，并与狭窄解决方案的线性表示进行比较。

Result: 研究发现通用错位表示比狭窄解决方案实现更低的损失，对扰动更鲁棒，在预训练分布中更具影响力。成功分离出通用错位的具体表示，可用于监控和缓解。

Conclusion: 这项工作为研究归纳偏好如何塑造LLM泛化提供了详细案例研究和初步指标，开源了所有代码、数据集和模型微调结果，为监控和缓解模型错位提供了具体工具。

Abstract: Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.

</details>


### [130] [ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation](https://arxiv.org/abs/2602.07883)
*Jingqi Zhou,Sheng Wang,DeZhao Deng,Junwen Lu,Junwei Su,Qintong Li,Jiahui Gao,Hao Wu,Jiyue Jiang,Lingpeng Kong,Chuan Wu*

Main category: cs.AI

TL;DR: ToolSelf：一种新型工具驱动运行时自重构范式，将配置更新抽象为可调用工具，使智能体能在任务执行中自主调整子目标、上下文、策略和工具箱，实现从被动执行者到任务与自我双重管理者的转变。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能体系统受限于静态配置，这些配置在执行前固定且无法适应动态任务变化。依赖人工编排或启发式补丁的方法泛化能力差且优化碎片化，需要突破这些限制。

Method: 提出ToolSelf范式，将配置更新抽象为可调用工具，统一任务执行和自调整到单一动作空间。设计配置感知两阶段训练（CAT），结合拒绝采样微调和轨迹级强化学习来内化这种元能力。

Result: 在多样化基准测试中，ToolSelf能与专用工作流相媲美，同时能泛化到新任务，实现了24.1%的平均性能提升，展示了向真正自适应性智能体发展的路径。

Conclusion: ToolSelf通过工具驱动运行时自重构，使智能体从外部规则依赖转向内在参数调整，实现了从被动执行者到任务与自我双重管理者的转变，为构建真正自适应性智能体开辟了新路径。

Abstract: Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.

</details>


### [131] [MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation](https://arxiv.org/abs/2602.07905)
*Yu Zhao,Hao Guan,Yongcheng Jing,Ying Zhang,Dacheng Tao*

Main category: cs.AI

TL;DR: MedCoG：基于知识图谱的医疗元认知代理，通过元认知评估动态调节知识使用，提高推理效率5.5倍


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂医疗推理中表现良好，但面临推理扩展定律下的收益递减问题。现有方法通过增加各种知识来增强LLMs，但额外成本转化为准确性的效果不明确。

Method: 提出MedCoG（Medical Meta-Cognition Agent with Knowledge Graph），通过元认知评估（任务复杂度、熟悉度、知识密度）动态调节程序性、情景性和事实性知识的使用，实现LLM中心的按需推理。

Result: 在五个困难的医疗基准测试中，MedCoG表现出有效性和效率，产生5.5倍的推理密度（推理效率指标）。Oracle研究突显了元认知调节的显著潜力。

Conclusion: 元认知调节可以有效缓解推理扩展定律问题，通过避免盲目扩展降低成本，通过过滤分散知识提高准确性，在医疗推理任务中实现高效推理。

Abstract: Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.

</details>


### [132] [LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth](https://arxiv.org/abs/2602.07962)
*Weihao Zeng,Yuzhen Huang,Junxian He*

Main category: cs.AI

TL;DR: LOCA-bench是一个用于评估长上下文语言智能体的基准测试，通过自动化环境状态控制来调节上下文长度，解决现有基准主要关注单步检索而忽视真实多步智能体场景的问题。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准主要评估模型从长文本中检索信息的能力，但真实场景中LLM需要作为智能体探索环境、遵循指令和计划、提取有用信息并在动态增长的上下文中预测正确行动。随着上下文增长，模型可靠性会下降（"上下文腐化"现象），需要专门评估智能体在长上下文场景中的表现。

Method: LOCA-bench通过自动化、可扩展的环境状态控制来调节智能体的上下文长度，使上下文长度可以无限扩展同时保持任务语义固定。该基准评估语言智能体作为模型和脚手架的组合，包括各种上下文管理策略。

Result: 随着环境状态变得更加复杂，智能体性能普遍下降，但先进的上下文管理技术可以显著提高整体成功率。该基准为评估模型和脚手架在长上下文智能体场景中提供了平台。

Conclusion: LOCA-bench填补了现有长上下文基准的空白，专注于评估智能体在动态增长上下文中的表现，为研究长上下文语言智能体提供了重要工具，并开源供社区使用。

Abstract: Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as "context rot". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench

</details>


### [133] [Accelerating Social Science Research via Agentic Hypothesization and Experimentation](https://arxiv.org/abs/2602.07983)
*Jishu Sen Gupta,Harini SI,Somesh Kumar Singh,Syed Mohamad Tawseeq,Yaman Kumar Singla,David Doermann,Rajiv Ratn Shah,Balaji Krishnamurthy*

Main category: cs.AI

TL;DR: EXPERIGEN是一个端到端的科学发现代理框架，通过生成器-实验者两阶段搜索，在多个领域发现比现有方法多2-4倍且预测性高7-17%的统计显著假设，并通过专家评审和真实A/B测试验证了其科学价值。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的社会科学研究过程缓慢，依赖观察、假设生成和实验验证的迭代循环。现有数据驱动方法虽然能加速部分过程，但无法支持端到端的科学发现。

Method: 提出EXPERIGEN框架，采用受贝叶斯优化启发的两阶段搜索：生成器提出候选假设，实验者进行实证评估。框架支持多模态和关系数据集等复杂数据机制。

Result: 1) 在多个领域发现比现有方法多2-4倍的统计显著假设，预测性高7-17%；2) 专家评审显示88%假设具有中等或强新颖性，70%被认为有影响力且值得追求；3) 首次对LLM生成假设进行A/B测试，获得p<1e-6的统计显著结果和344%的大效应量。

Conclusion: EXPERIGEN能够有效支持端到端的科学发现，生成新颖、有影响力且可操作的假设，并通过实证验证展示了其在推动真实科学进步方面的潜力。

Abstract: Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.

</details>


### [134] [Towards Adaptive, Scalable, and Robust Coordination of LLM Agents: A Dynamic Ad-Hoc Networking Perspective](https://arxiv.org/abs/2602.08009)
*Rui Li,Zeyu Zhang,Xiaohe Bo,Quanyu Dai,Chaozhuo Li,Feng Wen,Xu Chen*

Main category: cs.AI

TL;DR: RAPS是一个基于声誉感知的发布-订阅范式，用于实现LLM多智能体的自适应、可扩展和鲁棒协调，通过动态意图匹配和本地声誉监控来解决智能体协作中的通信挑战。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体架构需要大量人工编排，存在自动化设计智能体工作流的迫切需求。论文将智能体协调问题类比为动态自组织网络中的通信挑战：如何在可扩展的智能体主机之间建立自适应且可靠的通信。

Method: RAPS基于分布式发布-订阅协议，让LLM智能体根据声明的意图而非预定义拓扑交换消息。包含两个核心机制：1) 反应式订阅：智能体动态优化意图；2) 贝叶斯声誉：每个智能体配备本地监控器来检测和隔离恶意对等节点。

Result: 在五个基准测试上的广泛实验表明，RAPS设计有效地在统一的多智能体协调框架中协调了自适应性、可扩展性和鲁棒性。

Conclusion: RAPS通过声誉感知的发布-订阅范式，为解决LLM多智能体协调中的通信挑战提供了一个有效的解决方案，实现了自适应、可扩展且鲁棒的智能体协作。

Abstract: Multi-agent architectures built on large language models (LLMs) have demonstrated the potential to realize swarm intelligence through well-crafted collaboration. However, the substantial burden of manual orchestration inherently raises an imperative to automate the design of agentic workflows. We frame such an agent coordination challenge as a classic problem in dynamic ad-hoc networking: How to establish adaptive and reliable communication among a scalable number of agentic hosts? In response to this unresolved dilemma, we introduce RAPS, a reputation-aware publish-subscribe paradigm for adaptive, scalable, and robust coordination of LLM agents. RAPS is grounded in the Distributed Publish-Subscribe Protocol, allowing LLM agents to exchange messages based on their declared intents rather than predefined topologies. Beyond this substrate, RAPS further incorporates two coherent overlays: (i) Reactive Subscription, enabling agents to dynamically refine their intents; and (ii) Bayesian Reputation, empowering each agent with a local watchdog to detect and isolate malicious peers. Extensive experiments over five benchmarks showcase that our design effectively reconciles adaptivity, scalability, and robustness in a unified multi-agent coordination framework.

</details>


### [135] [Small Agent Group is the Future of Digital Health](https://arxiv.org/abs/2602.08013)
*Yuqiao Meng,Luoxi Tang,Dazheng Zhang,Rafael Brens,Elvys J. Romero,Nancy Guo,Safa Elkefi,Zhaohan Xi*

Main category: cs.AI

TL;DR: 小型代理组（SAG）通过协同推理在临床决策中超越单一大型模型，实现效果、可靠性和部署成本的最佳平衡


<details>
  <summary>Details</summary>
Motivation: 当前数字健康领域过度依赖"规模优先"的LLM范式，但临床实际需求不仅需要效果，还需要可靠性和合理的部署成本。临床决策本质上是协作过程，因此需要挑战单一模型扩展范式

Method: 提出小型代理组（SAG）方法，将单一模型智能转变为集体专业知识，通过协作审议过程分配推理、循证分析和关键审核任务

Result: SAG在效果、可靠性和部署成本等多个临床指标上表现优于单一大型模型，无论是否使用额外优化或检索增强生成技术

Conclusion: SAG的协同推理可以替代模型参数增长，为数字健康提供可扩展的解决方案，更好地平衡效果、可靠性和部署效率

Abstract: The rapid adoption of large language models (LLMs) in digital health has been driven by a "scaling-first" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.

</details>


### [136] [Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities](https://arxiv.org/abs/2602.08092)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

TL;DR: 该论文挑战了强化学习中人类反馈总是真实的假设（教条4），指出在社交环境中该假设失效会导致目标解耦问题，并提出基于安全公理的认知源对齐方法来解决多数评估者有偏见的情况。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐策略依赖于人类反馈是真实信号这一脆弱前提（RL教条4），但该假设在社交环境中不成立，因为评估者可能阿谀奉承、懒惰或敌对，导致标准RL代理出现目标解耦的结构性故障。

Method: 提出认知源对齐（ESA）方法，不同于依赖统计共识的传统鲁棒方法，ESA使用稀疏安全公理来判断反馈的来源而非信号本身，通过"评判评判者"机制保证收敛到真实目标。

Result: 理论证明ESA能保证收敛到真实目标，即使多数评估者有偏见；实验显示传统共识方法在多数共谋下失败，而ESA方法成功恢复最优策略。

Conclusion: 人类反馈真实性的教条4在社交环境中不成立，会导致目标解耦问题；提出的ESA方法通过判断反馈来源而非依赖共识，能有效解决多数评估者有偏见时的对齐问题。

Abstract: Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent's learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this "judging the judges" mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.

</details>


### [137] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
*Zehao Chen,Gongxun Li,Tianxiang Ai,Yifei Li,Zixuan Huang,Wang Zhou,Fuzhen Zhuang,Xianglong Liu,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.AI

TL;DR: WMSS是一种后训练优化方法，利用模型历史弱检查点来指导继续优化，通过熵动态识别可恢复的学习差距并进行补偿学习，帮助强模型突破传统后训练的饱和瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法在模型变得高度自信后会出现饱和瓶颈，进一步训练收益递减。研究发现模型自身历史弱状态中仍存在有监督信号，这为突破饱和瓶颈提供了可能。

Method: WMSS通过分析熵动态识别可恢复的学习差距，利用弱检查点作为指导，通过补偿学习强化这些差距，使强智能体能够继续优化而不增加推理成本。

Result: 在数学推理和代码生成数据集上的实验表明，使用WMSS训练的智能体实现了有效的性能提升，同时不增加额外的推理成本。

Conclusion: WMSS通过利用模型自身历史弱状态作为监督信号，成功突破了后训练中的饱和瓶颈，为大型语言模型的持续优化提供了新途径。

Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>


### [138] [Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs](https://arxiv.org/abs/2602.08241)
*Siqu Ou,Tianrui Wan,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.AI

TL;DR: SAYO：基于强化学习的视觉推理模型，通过区域级视觉注意力奖励机制，解决多模态大语言模型中视觉注意力不稳定的问题，提升复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在复杂推理任务中依赖长文本推理轨迹，但缺乏有效的视觉注意力学习机制。研究发现当前模型存在视觉聚焦弱的问题：早期视觉错位在后续推理中很少被纠正，导致错误传播和推理失败。这一限制源于训练过程中视觉注意力信用分配不足。

Method: 提出SAYO模型，采用强化学习框架训练，引入区域级视觉注意力奖励机制。该奖励明确将优化信号与视觉基础推理步骤对齐，使模型能够学习更可靠的注意力行为。

Result: 在多个多模态基准测试上的广泛实验表明，SAYO在多样化的推理和感知任务中持续提升性能。

Conclusion: 通过强化学习框架中的区域级视觉注意力奖励机制，SAYO能够有效解决多模态大语言模型中视觉注意力不稳定的问题，显著提升复杂推理任务的性能。

Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>


### [139] [SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities](https://arxiv.org/abs/2602.08254)
*Arman Aghaee,Sepehr Asgarian,Jouhyun Jeon*

Main category: cs.AI

TL;DR: SynthAgent是一个多智能体系统框架，用于模拟肥胖合并精神障碍患者，通过整合临床数据和人格特质来创建个性化虚拟患者，模拟疾病进展和治疗响应。


<details>
  <summary>Details</summary>
Motivation: 真实世界数据存在碎片化、偏见和隐私限制的问题，模拟高保真患者为研究复杂疾病提供了新途径。

Method: 提出SynthAgent多智能体系统框架，整合索赔数据、人口调查和患者中心文献，构建具有人格特质的个性化虚拟患者，通过自主智能体交互模拟疾病进展、治疗响应和生活管理。

Result: 评估100多个生成的患者显示，GPT-5和Claude 4.5 Sonnet在MAS框架中保真度最高，优于Gemini 2.5 Pro和DeepSeek-R1。

Conclusion: SynthAgent提供了一个可扩展且保护隐私的框架，用于探索医疗和心理领域的患者旅程、行为动态和决策过程。

Abstract: Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>


### [140] [Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis](https://arxiv.org/abs/2602.08276)
*Haoyu Jia,Kento Kawaharazuka,Kei Okada*

Main category: cs.AI

TL;DR: 提出Structural Context Model形式化模型，从上下文结构角度分析和比较LLM智能体，包含声明式实现框架和Semantic Dynamics Analysis工程流程，在动态猴子香蕉问题上实现32%成功率提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体研究碎片化严重，概念框架与方法论原则常与底层实现细节混杂，缺乏可分析、自洽的形式化模型来进行实现无关的智能体表征和比较。

Method: 提出Structural Context Model形式化模型，从上下文结构角度分析LLM智能体；包含声明式实现框架和Semantic Dynamics Analysis可持续工程流程，支持快速系统化设计迭代。

Result: 在动态猴子香蕉问题变体上，使用该框架的智能体在最具挑战性的设置中实现了32个百分点的成功率提升。

Conclusion: 提出的形式化模型和工程流程为LLM智能体研究提供了统一的分析框架，支持实现无关的智能体表征和系统化设计迭代。

Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.

</details>


### [141] [Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System](https://arxiv.org/abs/2602.08335)
*Yanming Li,Xuelin Zhang,WenJie Lu,Ziye Tang,Maodong Wu,Haotian Luo,Tongtong Wu,Zijie Peng,Hongze Mi,Yibo Feng,Naiqiang Tan,Chao Huang,Hong Chen,Li Shen*

Main category: cs.AI

TL;DR: SHARP框架通过基于Shapley值的分层信用分配机制，优化多智能体强化学习，解决信用分配难题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 将LLM与外部工具结合的多智能体系统为解决复杂问题提供了新范式，但训练这些系统面临信用分配挑战，现有方法依赖稀疏或全局广播奖励，无法捕捉个体贡献，导致强化学习效率低下。

Method: 提出SHARP框架，通过分解的奖励机制实现精确信用分配：包括全局广播-准确性奖励、基于Shapley值的边际信用奖励和工具过程奖励，并通过在轨迹组间标准化智能体特定优势来稳定训练。

Result: 在多个真实世界基准测试中，SHARP显著优于现有最先进基线，相比单智能体和多智能体方法分别实现了平均23.66%和14.05%的匹配改进。

Conclusion: SHARP框架通过精确的信用分配机制有效解决了多智能体强化学习中的信用分配难题，显著提升了系统性能，为LLM与工具结合的多智能体系统优化提供了有效解决方案。

Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.

</details>


### [142] [OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration](https://arxiv.org/abs/2602.08344)
*Qi Guo,Jianing Wang,Deyang Kong,Xiangyu Xi,Jianfei Zhang,Yi Lu,Jingang Wang,Wei Wang,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 提出Outline-Guided Path Exploration (OPE)方法，通过生成多样化的推理大纲来划分解空间，减少并行推理路径间的信息冗余，提升大型推理模型解决复杂问题的性能。


<details>
  <summary>Details</summary>
Motivation: 现有并行思维方法主要关注聚合阶段优化，对路径探索阶段关注不足。研究发现探索路径间的互信息瓶颈限制了整体性能，需要减少信息冗余并提升路径多样性。

Method: 提出OPE方法：1）先生成多样化的推理大纲来划分解空间；2）基于大纲进行并行路径推理；3）采用迭代强化学习策略，独立优化大纲规划和基于大纲的推理。

Result: 在多个数学基准测试上的实验表明，OPE能有效提升不同聚合策略下的推理性能，使大型推理模型更可靠地发现正确解决方案。

Conclusion: 通过显式划分解空间和减少信息冗余，OPE解决了并行思维中的互信息瓶颈问题，为强化学习优化并行推理提供了有效方法。

Abstract: Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>


### [143] [MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval](https://arxiv.org/abs/2602.08369)
*Xin Zhang,Kailai Yang,Chenyue Li,Hao Li,Qiyu Wei,Jun'ichi Tsujii,Sophia Ananiadou*

Main category: cs.AI

TL;DR: MemAdapter是一个统一的记忆检索框架，通过两阶段训练策略实现跨异构记忆范式的快速对齐，显著降低对齐成本并提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统通常采用孤立范式设计（如显式、参数化或潜在记忆），检索方法与范式紧密耦合，阻碍了跨范式的泛化和融合。需要统一异构记忆范式。

Method: 提出MemAdapter框架：1）从统一记忆空间训练生成式子图检索器；2）通过对比学习训练轻量对齐模块，将检索器适配到未见记忆范式。采用两阶段训练策略。

Result: 在三个公开评估基准上，生成式子图检索器在三种记忆范式和智能体模型规模上均优于五个强基线系统。跨范式对齐仅需13分钟（单GPU），性能优于原始检索器且训练计算量减少95%以上。支持跨范式的零样本融合。

Conclusion: MemAdapter作为即插即用解决方案，能够有效统一异构记忆范式，显著降低对齐成本，提升记忆检索的灵活性和性能。

Abstract: Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.

</details>


### [144] [Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI](https://arxiv.org/abs/2602.08373)
*Feiyu Wu,Xu Zheng,Yue Qu,Zhuocheng Wang,Zicheng Feng,Hui Li*

Main category: cs.AI

TL;DR: VIRF是一个神经符号架构，通过逻辑导师与LLM规划器的对话实现可验证的安全规划，在家庭安全任务中实现零危险行动率和最高目标达成率。


<details>
  <summary>Details</summary>
Motivation: LLM作为具身AI规划器具有潜力，但其随机性缺乏形式化推理，无法提供严格的安全保证。现有方法要么依赖不可靠的LLM进行安全检查，要么简单拒绝不安全计划而不提供修复方案。

Method: 提出可验证迭代精炼框架(VIRF)，采用神经符号架构，建立逻辑导师与LLM规划器的导师-学徒对话机制。逻辑导师基于形式化安全本体提供因果性和教学性反馈，实现智能计划修复而非简单避免。还引入可扩展的知识获取流程，从现实世界文档合成安全知识库。

Result: 在具有挑战性的家庭安全任务中，VIRF实现了0%的危险行动率(HAR)和77.3%的目标条件率(GCR)，在所有基线方法中最高。平均仅需1.1次修正迭代，效率极高。

Conclusion: VIRF展示了构建根本上可信且可验证安全的具身智能体的原则性途径，从被动安全把关转向主动协作。

Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.

</details>


### [145] [SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains](https://arxiv.org/abs/2602.08400)
*Longkun Li,Yuanben Zou,Jinghan Wu,Yuqing Wen,Jing Li,Hangwei Qian,Ivor Tsang*

Main category: cs.AI

TL;DR: SCOUT-RAG是一个分布式代理Graph-RAG框架，通过渐进式跨域检索解决分布式受限环境中的知识图检索问题，在保持性能的同时显著降低跨域调用、令牌处理和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统Graph-RAG依赖集中式知识图，但在分布式和访问受限环境（如医院、跨国组织）中，检索需要在没有全局图可见性的情况下选择相关域和适当遍历深度，避免穷举查询。

Method: 提出SCOUT-RAG框架，使用四个协作代理：(i)估计域相关性，(ii)决定何时扩展到额外域，(iii)调整遍历深度避免不必要的图探索，(iv)合成高质量答案。框架旨在最小化检索遗憾（缺失有用域信息）同时控制延迟和API成本。

Result: 在多域知识设置中，SCOUT-RAG达到与集中式基线（包括DRIFT和穷举域遍历）相当的性能，同时显著减少跨域调用、处理的总令牌数和延迟。

Conclusion: SCOUT-RAG提供了一个可扩展且成本高效的分布式Graph-RAG解决方案，在分布式受限环境中有效平衡检索质量和资源消耗。

Abstract: Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \textbf{SCOUT-RAG} (\textit{\underline{S}calable and \underline{CO}st-efficient \underline{U}nifying \underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.

</details>


### [146] [On Protecting Agentic Systems' Intellectual Property via Watermarking](https://arxiv.org/abs/2602.08401)
*Liwen Wang,Zongjie Li,Yuchong Xie,Shuai Wang,Dongdong She,Wei Wang,Juergen Rahmel*

Main category: cs.AI

TL;DR: AGENTWM是首个专门为智能体模型设计的水印框架，通过偏置功能相同的工具执行路径分布来嵌入水印，保护智能体系统的知识产权免受模仿攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型演变为执行自主推理和工具使用的智能体系统，创造了重要的知识产权价值。这些系统容易受到模仿攻击，而现有的LLM水印技术无法在智能体领域有效工作，因为现实中的智能体系统通常作为灰盒运行，隐藏了验证所需的内部推理痕迹。

Method: AGENTWM利用动作序列的语义等价性，通过微妙地偏置功能相同的工具执行路径分布来注入水印。开发了自动管道生成鲁棒的水印方案，以及严格的统计假设检验程序进行验证。

Result: 在三个复杂领域进行广泛评估，AGENTWM实现了高检测精度，同时对智能体性能影响可忽略。水印能有效保护智能体知识产权，对抗性攻击者无法在不严重降低被盗模型效用的情况下移除水印。

Conclusion: AGENTWM是首个专门为智能体模型设计的水印框架，能有效保护智能体系统的知识产权，对抗模仿攻击，填补了现有LLM水印技术在智能体领域的不足。

Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>


### [147] [From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent](https://arxiv.org/abs/2602.08412)
*Yuhang Wang,Feiming Xu,Zheng Lin,Guangyu He,Yuzhe Huang,Haichang Gao,Zhenxing Niu*

Main category: cs.AI

TL;DR: PASB是一个针对现实世界个性化AI代理的端到端安全评估框架，通过个性化使用场景、真实工具链和长时交互来评估OpenClaw等代理的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有代理安全研究主要关注合成或任务中心设置，无法准确捕捉现实世界部署中个性化代理的攻击面和风险传播机制，需要专门的安全评估框架。

Method: 提出PASB框架，基于现有代理攻击范式，整合个性化使用场景、真实工具链和长时交互，对真实系统进行黑盒端到端安全评估。

Result: 以OpenClaw为案例研究发现其在用户提示处理、工具使用和记忆检索等不同执行阶段存在关键漏洞，突显个性化代理部署中的重大安全风险。

Conclusion: PASB框架能有效评估个性化代理的安全风险，OpenClaw等代理在现实部署中存在严重安全漏洞，需要加强安全防护措施。

Abstract: Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.

</details>


### [148] [Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning](https://arxiv.org/abs/2602.08520)
*Xinhai Sun*

Main category: cs.AI

TL;DR: 论文提出Reinforcement Inference方法，利用LLM自身的不确定性在推理时选择性触发二次思考，无需重新训练即可显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在单次贪婪推理协议下部署，会因过早决策而系统性地低估模型真实能力。许多错误并非源于知识缺失，而是内部模糊性下的过早承诺。

Method: 提出Reinforcement Inference，一种基于熵的推理时控制策略，利用模型自身不确定性选择性地调用第二次更慎重的推理尝试。

Result: 在MMLU-Pro的12,032个问题上，使用DeepSeek-v3.2零样本设置，准确率从60.72%提升至84.03%，仅增加61.06%的推理调用。100%重问消融实验达到84.35%，表明不确定性感知选择能以更少计算获得大部分改进。

Conclusion: 该方法不仅提供实用的推理时升级，还提出了更广泛的基于熵的范式来测量和扩展模型能力，为LLM的潜在推理范围提供诊断视角，并激励未来训练目标明确约束正确性-置信度对齐。

Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.
  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.
  Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>


### [149] [Dialogue Model Optimization via Agent Game and Adaptive Tree-based GRPO](https://arxiv.org/abs/2602.08533)
*Kun Peng,Conghui Tan,Yu Liu,Guohua Tang,Zhongqian Sun,Wei Yang,Zining Zhu,Lei Jiang,Yanbing Liu,Hao Peng*

Main category: cs.AI

TL;DR: 提出一种结合在线个性化与自适应树基组相对策略优化的长视野强化学习框架，用于开放域对话代理，解决现有方法对预收集数据的过度依赖和短视野偏见问题。


<details>
  <summary>Details</summary>
Motivation: 现有开放域对话代理方法存在两个关键限制：过度依赖预收集的用户数据，以及强化学习中的短视野偏见忽略了对话的长期价值。需要一种能够在线个性化并考虑长期对话效果的框架。

Method: 采用双代理博弈范式：用户代理通过风格模仿学习用户特定对话特征，通过主动终止预测回合级终止概率作为即时奖励，构建动态环境。提出自适应树基组相对策略优化（AT-GRPO），将对话轨迹重新解释为树结构，引入自适应观察范围，在早期阶段使用较大范围支持主题探索，在后期阶段使用较小范围促进对话维护，将计算开销从指数级降低到多项式级。

Result: 大量实验表明该框架在性能、样本效率和鲁棒性方面表现优越。

Conclusion: 提出的长视野RL框架通过在线个性化和自适应树基优化，有效解决了对话代理中的长期价值捕获问题，在减少计算开销的同时提升了对话质量。

Abstract: Open-ended dialogue agents aim to deliver engaging, personalized interactions by adapting to users' traits, but existing methods face critical limitations: over-reliance on pre-collected user data, and short-horizon biases in reinforcement learning (RL) that neglect long-term dialogue value. To address these, we propose a novel long-horizon RL framework integrating online personalization with Adaptive Tree-based Group Relative Policy Optimization (AT-GRPO). Adopting a two-agent game paradigm, a user agent constructs dynamic environments via style mimicry (learning user-specific conversational traits) and active termination (predicting turn-level termination probabilities as immediate rewards), forming an iterative cycle that drives the dialogue agent to deepen interest exploration. AT-GRPO reinterprets dialogue trajectories as trees and introduces adaptive observation ranges. Unlike full tree expansion that incurs exponential overhead, it limits each node to aggregate rewards from a stage-aware range: larger ranges support early-stage topic exploration, while smaller ranges facilitate late-stage dialogue maintenance. This design reduces rollout budgets from exponential to polynomial in the dialogue length, while preserving long-term reward capture. Extensive experiments show our framework's superior performance, sample efficiency, and robustness.

</details>


### [150] [PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition](https://arxiv.org/abs/2602.08586)
*Yiming Yang,Zhuoyuan Li,Fanxiang Zeng,Hao Fu,Yue Liu*

Main category: cs.AI

TL;DR: PRISM框架将多智能体推理增益分解为探索、信息和聚合三个维度，通过角色多样性、执行反馈和迭代合成实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前多智能体协作方法缺乏理论指导，不清楚为何多智能体优于单智能体以及哪些设计选择最关键，难以系统优化

Method: 提出PRISM框架：1) 角色多样性实现探索；2) 基于执行的反馈和证据交叉评估提供信息；3) 迭代合成和闭环验证实现聚合

Result: 在数学推理、代码生成和函数调用基准测试中达到SOTA性能，计算效率优于仅优化部分维度的方法

Conclusion: 理论框架为未来多智能体推理系统提供可操作的设计原则，PRISM通过联合优化三个维度实现显著性能提升

Abstract: Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. Specifically, it remains unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains, making it difficult to build better systems.
  We address this gap by introducing a unified theoretical framework that decomposes multi-agent reasoning gains into three conceptually independent dimensions: Exploration for diverse solution coverage, Information for high-fidelity feedback, and Aggregation for principled consensus. Through this lens, existing methods can be understood as special cases that optimize only subsets of these dimensions. Building upon this decomposition, a novel framework called PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) is proposed, which jointly maximizes all three dimensions through role-based diversity, execution-grounded feedback with evidence-based cross-evaluation, and iterative synthesis with closed-loop validation. Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing partial dimensions. The theoretical framework provides actionable design principles for future multi-agent reasoning systems.

</details>


### [151] [Belief Offloading in Human-AI Interaction](https://arxiv.org/abs/2602.08754)
*Rose E. Guingrich,Dvija Mehta,Umang Bhatt*

Main category: cs.AI

TL;DR: 该论文提出了"信念卸载"概念，指人们将信念形成和维护过程外包给AI系统，影响其行为和信念体系，并探讨了其边界条件、分类和规范意义。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地将LLM聊天机器人作为思维伙伴，可能导致认知卸载，特别是"信念卸载"，这可能对认知技能产生负面影响。需要研究这种人类-AI交互中的特定认知卸载现象及其后果。

Method: 结合哲学、心理学和计算机科学研究，明确定义信念卸载的边界条件，提供描述性分类法，并分析其规范意义。

Result: 提出了信念卸载的概念框架，包括其发生条件、分类体系，并指出了信念卸载对人类行为和信念体系的潜在影响。

Conclusion: 信念卸载是人类-AI交互中的重要现象，需要进一步研究其发生机制和后果，为未来工作提供方向。

Abstract: What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, "belief offloading," in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.

</details>


### [152] [Efficient and Stable Reinforcement Learning for Diffusion Language Models](https://arxiv.org/abs/2602.08905)
*Jiawei Liu,Xiting Wang,Yuanyuan Zhong,Defu Lian,Yu Yang*

Main category: cs.AI

TL;DR: STP框架通过空间剪枝和时间剪枝同时提升扩散大语言模型强化学习的效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 强化学习对释放扩散大语言模型的复杂推理能力至关重要，但应用于dLLMs时面临效率和稳定性方面的独特挑战

Method: 提出时空剪枝(STP)框架，通过空间剪枝（利用静态先验约束探索空间）和时间剪枝（绕过冗余的后期细化步骤）压缩生成过程中的冗余

Result: 理论分析表明STP严格降低了对数似然估计的方差，确保更稳定的策略更新；大量实验证明STP在效率和准确性上都超越了最先进的基线方法

Conclusion: STP框架有效解决了dLLMs强化学习的效率和稳定性问题，为扩散大语言模型的强化学习训练提供了实用解决方案

Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>


### [153] [CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse](https://arxiv.org/abs/2602.08939)
*Longling Geng,Andy Ouyang,Theodore Wu,Daphne Barretto,Matthew John Hayes,Rachael Cooper,Yuqiao Zeng,Sameer Vijay,Gia Ancone,Ankit Rai,Matthew Wolfman,Patrick Flanagan,Edward Y. Chang*

Main category: cs.AI

TL;DR: CausalT5K是一个包含5000多个案例的诊断基准，用于系统评估LLM在因果推理中的失败模式，包括阶梯坍塌、阿谀性漂移和错误拒绝，通过实用性和安全性指标揭示聚合准确率无法发现的故障模式。


<details>
  <summary>Details</summary>
Motivation: LLM在因果推理中存在多种失败模式（如阿谀性、阶梯坍塌、错误校准的拒绝），但由于缺乏能够进行系统诊断的基准，相关改进进展缓慢。需要创建一个能够全面测试因果推理关键能力的诊断基准。

Method: 开发了CausalT5K基准，包含5000多个案例，覆盖10个领域，测试三个关键能力：检测阶梯坍塌、抵抗阿谀性漂移、生成明智拒绝。采用人机协作流程，涉及40名领域专家、迭代交叉验证周期，以及基于规则、LLM和人工评分的复合验证方法，实现了Pearl的因果阶梯作为研究基础设施。

Result: 初步实验揭示了四象限控制景观，其中静态审计策略普遍失败。基准能够将性能分解为实用性（敏感性）和安全性（特异性），揭示聚合准确率无法发现的故障模式。

Conclusion: CausalT5K作为研究基础设施，能够系统诊断LLM在因果推理中的失败模式，为推进可信推理系统提供有价值的评估工具。基准展示了静态审计策略的局限性，强调了动态评估的重要性。

Abstract: LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>


### [154] [CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute](https://arxiv.org/abs/2602.08948)
*Chen Jin,Ryutaro Tanno,Tom Diethe,Philip Teare*

Main category: cs.AI

TL;DR: CoRefine是一种基于置信度引导的自优化方法，通过轻量级控制器（仅211k参数）在冻结LLM上实现高效推理，相比512样本并行解码可减少约190倍token消耗，同时保持竞争性精度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM依赖大规模并行解码（如512个样本）来提升推理精度，但这带来了巨大的计算开销。需要一种更高效的方法来减少token消耗，同时保持或提升推理性能。

Method: 引入CoRefine方法，在冻结LLM上添加一个轻量级Conv1D控制器（仅211k参数）。控制器分析完整推理轨迹的置信度，决定是否停止、重新检查或尝试不同方法，实现有针对性的自我修正。平均每个问题只需2.7次优化步骤。还扩展为CoRefine-Tree，一种混合顺序-并行变体，自适应平衡探索与利用。

Result: 在多样化推理基准和三个开源模型上，控制器在自信停止时达到92.6%的精确度，表明置信度动态可靠地指示正确性而无需真实验证。相比512样本基线，平均减少约190倍token消耗。

Conclusion: CoRefine通过将置信度视为控制信号而非正确性保证，为可扩展推理和具有不完美验证器的智能体设置提供了模块化原语，实现了计算效率与推理精度的良好平衡。

Abstract: Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.

</details>


### [155] [iGRPO: Self-Feedback-Driven LLM Reasoning](https://arxiv.org/abs/2602.09000)
*Ali Hatamizadeh,Shrimai Prabhumoye,Igor Gitman,Ximing Lu,Seungju Han,Wei Ping,Yejin Choi,Jan Kautz*

Main category: cs.AI

TL;DR: iGRPO是一种两阶段强化学习方法，通过模型生成的草稿进行动态自条件化，在数学推理任务中超越GRPO并达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在解决复杂数学问题方面显示出潜力，但仍难以产生准确一致的解决方案。强化学习可以对齐任务特定奖励，但现有方法如PPO需要价值函数，GRPO虽然高效但仍有改进空间。

Method: iGRPO是GRPO的两阶段扩展：第一阶段采样多个探索性草稿并选择最高奖励的草稿；第二阶段将最佳草稿附加到原始提示中，在草稿条件化的改进上应用GRPO风格更新，训练策略超越其先前最佳尝试。

Result: 在匹配的rollout预算下，iGRPO在多种基础模型上一致优于GRPO。应用于OpenReasoning-Nemotron-7B在AceReason-Math上训练后，在AIME24和AIME25上分别达到85.62%和79.64%的新SOTA结果。

Conclusion: 迭代的、基于自反馈的强化学习在推进可验证数学推理方面具有巨大潜力，iGRPO的改进框架可推广到GRPO变体之外，并通过延迟熵崩溃改变学习动态。

Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>
