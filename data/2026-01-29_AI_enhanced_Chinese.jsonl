{"id": "2601.19964", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19964", "abs": "https://arxiv.org/abs/2601.19964", "authors": ["Maxim Tabachnyk", "Xu Shu", "Alexander Fr\u00f6mmgen", "Pavel Sychev", "Vahid Meimand", "Ilia Krets", "Stanislav Pyatykh", "Abner Araujo", "Krist\u00f3f Moln\u00e1r", "Satish Chandra"], "title": "Achieving Productivity Gains with AI-based IDE features: A Journey at Google", "comment": "Accepted for publication at the 3rd International Workshop on Large Language Models For Code (LLM4Code '26 workshop at ICSE '26)", "summary": "We discuss Google's journey in developing and refining two internal AI-based IDE features: code completion and natural-language-driven code transformation (Transform Code). We address challenges in latency, user experience and suggestion quality, all backed by rigorous experimentation. The article serves as an example of how to refine AI developer tools across the user interface, backend, and model layers, to deliver tangible productivity improvements in an enterprise setting.", "AI": {"tldr": "Google\u5f00\u53d1AI\u9a71\u52a8\u7684IDE\u529f\u80fd\uff08\u4ee3\u7801\u8865\u5168\u548c\u81ea\u7136\u8bed\u8a00\u4ee3\u7801\u8f6c\u6362\uff09\uff0c\u901a\u8fc7\u591a\u5c42\u9762\u4f18\u5316\u89e3\u51b3\u5ef6\u8fdf\u3001\u7528\u6237\u4f53\u9a8c\u548c\u8d28\u91cf\u95ee\u9898\uff0c\u63d0\u5347\u4f01\u4e1a\u751f\u4ea7\u529b", "motivation": "\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5f00\u53d1AI\u9a71\u52a8\u7684\u5f00\u53d1\u8005\u5de5\u5177\uff0c\u9700\u8981\u89e3\u51b3\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6311\u6218\uff0c\u5305\u62ec\u5ef6\u8fdf\u3001\u7528\u6237\u4f53\u9a8c\u548c\u4ee3\u7801\u5efa\u8bae\u8d28\u91cf\uff0c\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u751f\u4ea7\u529b\u63d0\u5347", "method": "\u901a\u8fc7\u7528\u6237\u754c\u9762\u3001\u540e\u7aef\u548c\u6a21\u578b\u5c42\u7684\u7efc\u5408\u4f18\u5316\uff0c\u7ed3\u5408\u4e25\u683c\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6539\u8fdb\u4ee3\u7801\u8865\u5168\u548c\u81ea\u7136\u8bed\u8a00\u4ee3\u7801\u8f6c\u6362\u529f\u80fd", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u53ef\u7528\u7684AI IDE\u529f\u80fd\uff0c\u901a\u8fc7\u591a\u5c42\u9762\u4f18\u5316\u89e3\u51b3\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u6311\u6218", "conclusion": "AI\u5f00\u53d1\u8005\u5de5\u5177\u7684\u4f18\u5316\u9700\u8981\u8de8UI\u3001\u540e\u7aef\u548c\u6a21\u578b\u5c42\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e25\u683c\u5b9e\u9a8c\uff0c\u624d\u80fd\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5b9e\u73b0\u751f\u4ea7\u529b\u63d0\u5347", "topic": "swe application"}}
{"id": "2601.20103", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20103", "abs": "https://arxiv.org/abs/2601.20103", "authors": ["Darshan Deshpande", "Anand Kannappan", "Rebecca Qian"], "title": "Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis", "comment": "Dataset: https://huggingface.co/datasets/PatronusAI/trace-dataset", "summary": "Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.", "AI": {"tldr": "\u63d0\u51fa\u4e86TRACE\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4ee3\u7801\u751f\u6210RL\u73af\u5883\u4e2d\u68c0\u6d4b\u5956\u52b1\u653b\u51fb\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5728\u5bf9\u6bd4\u6027\u5f02\u5e38\u68c0\u6d4b\u8bbe\u7f6e\u4e2d\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff08GPT-5.2\u8fbe\u523063%\u68c0\u6d4b\u7387\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u8bed\u4e49\u4e0a\u4e0b\u6587\u5956\u52b1\u653b\u51fb\u4e0a\u7684\u663e\u8457\u56f0\u96be\u3002", "motivation": "\u968f\u7740\u4ee3\u7801\u751f\u6210\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u8fdb\u5c55\uff0c\u9700\u8981\u5065\u58ee\u7684\u73af\u5883\u6765\u9632\u6b62\u5956\u52b1\u653b\u51fb\u3002LLM\u8d8a\u6765\u8d8a\u591a\u5730\u4f5c\u4e3a\u4ee3\u7801RL\u7684\u8bc4\u4f30\u5668\uff0c\u4f46\u5176\u68c0\u6d4b\u5956\u52b1\u653b\u51fb\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b54\u4e2a\u7c7b\u522b\u7684\u5956\u52b1\u653b\u51fb\u5206\u7c7b\u6cd5\uff0c\u521b\u5efa\u4e86TRACE\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b517\u4e2a\u6d4b\u8bd5\u8f68\u8ff9\uff09\uff0c\u5728\u5bf9\u6bd4\u6027\u5f02\u5e38\u68c0\u6d4b\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u4e0e\u5b64\u7acb\u5206\u7c7b\u8bbe\u7f6e\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u6a21\u578b\u5728\u5bf9\u6bd4\u6027\u8bbe\u7f6e\u4e2d\u68c0\u6d4b\u5956\u52b1\u653b\u51fb\u66f4\u6709\u6548\uff08GPT-5.2\u8fbe\u523063%\u68c0\u6d4b\u7387\uff0c\u6bd4\u5b64\u7acb\u8bbe\u7f6e\u768445%\u663e\u8457\u63d0\u5347\uff09\u3002\u6a21\u578b\u5728\u8bed\u4e49\u4e0a\u4e0b\u6587\u5956\u52b1\u653b\u51fb\u4e0a\u6bd4\u53e5\u6cd5\u4e0a\u4e0b\u6587\u5956\u52b1\u653b\u51fb\u8868\u73b0\u66f4\u5dee\u3002\u826f\u6027\u4e0e\u653b\u51fb\u8f68\u8ff9\u6bd4\u4f8b\u548c\u5206\u6790\u805a\u7c7b\u5927\u5c0f\u663e\u8457\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "TRACE\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30LLM\u68c0\u6d4b\u5956\u52b1\u653b\u51fb\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5bf9\u6bd4\u6027\u8bc4\u4f30\u8bbe\u7f6e\u7684\u4f18\u52bf\uff0c\u5e76\u6307\u51fa\u4e86\u6a21\u578b\u5728\u8bed\u4e49\u590d\u6742\u6027\u5956\u52b1\u653b\u51fb\u4e0a\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.19918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19918", "abs": "https://arxiv.org/abs/2601.19918", "authors": ["Yitong Qiao", "Licheng Pan", "Yu Mi", "Lei Liu", "Yue Shen", "Fei Sun", "Zhixuan Chu"], "title": "Lowest Span Confidence: A Zero-Shot Metric for Efficient and Black-Box Hallucination Detection in LLMs", "comment": null, "summary": "Hallucinations in Large Language Models (LLMs), i.e., the tendency to generate plausible but non-factual content, pose a significant challenge for their reliable deployment in high-stakes environments. However, existing hallucination detection methods generally operate under unrealistic assumptions, i.e., either requiring expensive intensive sampling strategies for consistency checks or white-box LLM states, which are unavailable or inefficient in common API-based scenarios. To this end, we propose a novel efficient zero-shot metric called Lowest Span Confidence (LSC) for hallucination detection under minimal resource assumptions, only requiring a single forward with output probabilities. Concretely, LSC evaluates the joint likelihood of semantically coherent spans via a sliding window mechanism. By identifying regions of lowest marginal confidence across variable-length n-grams, LSC could well capture local uncertainty patterns strongly correlated with factual inconsistency. Importantly, LSC can mitigate the dilution effect of perplexity and the noise sensitivity of minimum token probability, offering a more robust estimate of factual uncertainty. Extensive experiments across multiple state-of-the-art (SOTA) LLMs and diverse benchmarks show that LSC consistently outperforms existing zero-shot baselines, delivering strong detection performance even under resource-constrained conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aLSC\uff08\u6700\u4f4e\u8de8\u5ea6\u7f6e\u4fe1\u5ea6\uff09\u7684\u65b0\u578b\u96f6\u6837\u672c\u5e7b\u89c9\u68c0\u6d4b\u6307\u6807\uff0c\u4ec5\u9700\u5355\u6b21\u524d\u5411\u4f20\u64ad\u548c\u8f93\u51fa\u6982\u7387\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u673a\u5236\u8bc4\u4f30\u8bed\u4e49\u8fde\u8d2f\u8de8\u5ea6\u7684\u8054\u5408\u4f3c\u7136\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5e7b\u89c9\u68c0\u6d4b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff08\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u975e\u4e8b\u5b9e\u5185\u5bb9\uff09\u5728\u9ad8\u98ce\u9669\u90e8\u7f72\u73af\u5883\u4e2d\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u5bc6\u96c6\u91c7\u6837\u7b56\u7565\u6216\u767d\u76d2\u6a21\u578b\u72b6\u6001\uff0c\u8fd9\u5728\u5e38\u89c1\u7684API\u573a\u666f\u4e2d\u4e0d\u53ef\u7528\u6216\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faLSC\uff08\u6700\u4f4e\u8de8\u5ea6\u7f6e\u4fe1\u5ea6\uff09\u6307\u6807\uff0c\u4ec5\u9700\u5355\u6b21\u524d\u5411\u4f20\u64ad\u548c\u8f93\u51fa\u6982\u7387\u3002\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u673a\u5236\u8bc4\u4f30\u8bed\u4e49\u8fde\u8d2f\u8de8\u5ea6\u7684\u8054\u5408\u4f3c\u7136\uff0c\u8bc6\u522b\u53ef\u53d8\u957f\u5ea6n-gram\u4e2d\u6700\u4f4e\u8fb9\u9645\u7f6e\u4fe1\u5ea6\u533a\u57df\uff0c\u6355\u6349\u4e0e\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u6027\u9ad8\u5ea6\u76f8\u5173\u7684\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u6700\u5148\u8fdbLLM\u548c\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLSC\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u4e5f\u80fd\u63d0\u4f9b\u5f3a\u5927\u7684\u68c0\u6d4b\u6027\u80fd\u3002LSC\u80fd\u591f\u7f13\u89e3\u56f0\u60d1\u5ea6\u7684\u7a00\u91ca\u6548\u5e94\u548c\u6700\u5c0f\u6807\u8bb0\u6982\u7387\u7684\u566a\u58f0\u654f\u611f\u6027\uff0c\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u4e8b\u5b9e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "conclusion": "LSC\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u8d44\u6e90\u53cb\u597d\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ec5\u9700\u6700\u5c0f\u8d44\u6e90\u5047\u8bbe\uff08\u5355\u6b21\u524d\u5411\u4f20\u64ad\uff09\uff0c\u5728API\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u53ef\u9760\u90e8\u7f72LLM\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.20106", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20106", "abs": "https://arxiv.org/abs/2601.20106", "authors": ["Shamse Tasnim Cynthia", "Joy Krishan Das", "Banani Roy"], "title": "Are We All Using Agents the Same Way? An Empirical Study of Core and Peripheral Developers Use of Coding Agents", "comment": null, "summary": "Autonomous AI agents are transforming software development and redefining how developers collaborate with AI. Prior research shows that the adoption and use of AI-powered tools differ between core and peripheral developers. However, it remains unclear how this dynamic unfolds in the emerging era of autonomous coding agents. In this paper, we present the first empirical study of 9,427 agentic PRs, examining how core and peripheral developers use, review, modify, and verify agent-generated contributions prior to acceptance. Through a mix of qualitative and quantitative analysis, we make four key contributions. First, a subset of peripheral developers use agents more often, delegating tasks evenly across bug fixing, feature addition, documentation, and testing. In contrast, core developers focus more on documentation and testing, yet their agentic PRs are frequently merged into the main/master branch. Second, core developers engage slightly more in review discussions than peripheral developers, and both groups focus on evolvability issues. Third, agentic PRs are less likely to be modified, but when they are, both groups commonly perform refactoring. Finally, peripheral developers are more likely to merge without running CI checks, whereas core developers more consistently require passing verification before acceptance. Our analysis offers a comprehensive view of how developer experience shapes integration offer insights for both peripheral and core developers on how to effectively collaborate with coding agents.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5bf99,427\u4e2a\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u751f\u6210\u7684PR\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0\u6838\u5fc3\u4e0e\u5916\u56f4\u5f00\u53d1\u8005\u5728\u4f7f\u7528AI\u4ee3\u7406\u65f6\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1a\u5916\u56f4\u5f00\u53d1\u8005\u66f4\u9891\u7e41\u4f7f\u7528\u4ee3\u7406\u4e14\u4efb\u52a1\u5206\u5e03\u5747\u5300\uff0c\u800c\u6838\u5fc3\u5f00\u53d1\u8005\u66f4\u5173\u6ce8\u6587\u6863\u548c\u6d4b\u8bd5\uff0c\u4f46\u4ed6\u4eec\u7684\u4ee3\u7406PR\u5408\u5e76\u7387\u66f4\u9ad8\u3002", "motivation": "\u968f\u7740\u81ea\u4e3bAI\u4ee3\u7406\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u65b9\u5f0f\uff0c\u9700\u8981\u4e86\u89e3\u6838\u5fc3\u4e0e\u5916\u56f4\u5f00\u53d1\u8005\u5982\u4f55\u4e0e\u7f16\u7801\u4ee3\u7406\u534f\u4f5c\u3002\u5148\u524d\u7814\u7a76\u8868\u660eAI\u5de5\u5177\u4f7f\u7528\u5b58\u5728\u5dee\u5f02\uff0c\u4f46\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u65f6\u4ee3\u8fd9\u79cd\u52a8\u6001\u5982\u4f55\u6f14\u53d8\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5b9a\u6027\u5b9a\u91cf\u5206\u6790\u65b9\u6cd5\uff0c\u5bf99,427\u4e2a\u4ee3\u7406\u751f\u6210\u7684PR\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u6838\u5fc3\u4e0e\u5916\u56f4\u5f00\u53d1\u8005\u5728\u4f7f\u7528\u3001\u5ba1\u67e5\u3001\u4fee\u6539\u548c\u9a8c\u8bc1\u4ee3\u7406\u8d21\u732e\u65b9\u9762\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "result": "1) \u5916\u56f4\u5f00\u53d1\u8005\u66f4\u9891\u7e41\u4f7f\u7528\u4ee3\u7406\u4e14\u4efb\u52a1\u5206\u5e03\u5747\u5300(\u9519\u8bef\u4fee\u590d\u3001\u529f\u80fd\u6dfb\u52a0\u3001\u6587\u6863\u3001\u6d4b\u8bd5)\uff1b\u6838\u5fc3\u5f00\u53d1\u8005\u66f4\u5173\u6ce8\u6587\u6863\u548c\u6d4b\u8bd5\uff0c\u4f46\u4ee3\u7406PR\u5408\u5e76\u7387\u66f4\u9ad8\u30022) \u6838\u5fc3\u5f00\u53d1\u8005\u5ba1\u67e5\u53c2\u4e0e\u5ea6\u7565\u9ad8\uff0c\u53cc\u65b9\u90fd\u5173\u6ce8\u53ef\u6f14\u5316\u6027\u95ee\u9898\u30023) \u4ee3\u7406PR\u4fee\u6539\u7387\u8f83\u4f4e\uff0c\u4f46\u4fee\u6539\u65f6\u53cc\u65b9\u90fd\u8fdb\u884c\u91cd\u6784\u30024) \u5916\u56f4\u5f00\u53d1\u8005\u66f4\u53ef\u80fd\u8df3\u8fc7CI\u68c0\u67e5\u5408\u5e76\uff0c\u6838\u5fc3\u5f00\u53d1\u8005\u66f4\u575a\u6301\u9a8c\u8bc1\u901a\u8fc7\u3002", "conclusion": "\u5f00\u53d1\u8005\u7ecf\u9a8c\u663e\u8457\u5f71\u54cd\u4e0e\u7f16\u7801\u4ee3\u7406\u7684\u534f\u4f5c\u65b9\u5f0f\uff0c\u7814\u7a76\u4e3a\u4e24\u7c7b\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6709\u6548\u534f\u4f5c\u7684\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u65f6\u4ee3\u7684\u65b0\u534f\u4f5c\u52a8\u6001\u3002", "topic": "swe application"}}
{"id": "2601.20048", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20048", "abs": "https://arxiv.org/abs/2601.20048", "authors": ["Jincheng Bai", "Zhenyu Zhang", "Jennifer Zhang", "Zhihuai Zhu"], "title": "Insight Agents: An LLM-Based Multi-Agent System for Data Insights", "comment": "Accepted to SIGIR 2025. DOI: 10.1145/3726302.3731959", "summary": "Today, E-commerce sellers face several key challenges, including difficulties in discovering and effectively utilizing available programs and tools, and struggling to understand and utilize rich data from various tools. We therefore aim to develop Insight Agents (IA), a conversational multi-agent Data Insight system, to provide E-commerce sellers with personalized data and business insights through automated information retrieval. Our hypothesis is that IA will serve as a force multiplier for sellers, thereby driving incremental seller adoption by reducing the effort required and increase speed at which sellers make good business decisions. In this paper, we introduce this novel LLM-backed end-to-end agentic system built on a plan-and-execute paradigm and designed for comprehensive coverage, high accuracy, and low latency. It features a hierarchical multi-agent structure, consisting of manager agent and two worker agents: data presentation and insight generation, for efficient information retrieval and problem-solving. We design a simple yet effective ML solution for manager agent that combines Out-of-Domain (OOD) detection using a lightweight encoder-decoder model and agent routing through a BERT-based classifier, optimizing both accuracy and latency. Within the two worker agents, a strategic planning is designed for API-based data model that breaks down queries into granular components to generate more accurate responses, and domain knowledge is dynamically injected to to enhance the insight generator. IA has been launched for Amazon sellers in US, which has achieved high accuracy of 90% based on human evaluation, with latency of P90 below 15s.", "AI": {"tldr": "Insight Agents (IA) \u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u5f0f\u591a\u4ee3\u7406\u6570\u636e\u6d1e\u5bdf\u7cfb\u7edf\uff0c\u4e3a\u7535\u5546\u5356\u5bb6\u63d0\u4f9b\u4e2a\u6027\u5316\u6570\u636e\u548c\u4e1a\u52a1\u6d1e\u5bdf\uff0c\u901a\u8fc7\u5206\u5c42\u591a\u4ee3\u7406\u67b6\u6784\u5b9e\u73b0\u9ad8\u6548\u4fe1\u606f\u68c0\u7d22\uff0c\u51c6\u786e\u7387\u8fbe90%\uff0c\u5ef6\u8fdfP90\u4f4e\u4e8e15\u79d2\u3002", "motivation": "\u7535\u5546\u5356\u5bb6\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u96be\u4ee5\u53d1\u73b0\u548c\u6709\u6548\u5229\u7528\u53ef\u7528\u7a0b\u5e8f\u548c\u5de5\u5177\uff1b\u96be\u4ee5\u7406\u89e3\u548c\u5229\u7528\u6765\u81ea\u5404\u79cd\u5de5\u5177\u7684\u4e30\u5bcc\u6570\u636e\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u51cf\u5c11\u5356\u5bb6\u51b3\u7b56\u52aa\u529b\u3001\u52a0\u901f\u4e1a\u52a1\u51b3\u7b56\u7684\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8ba1\u5212-\u6267\u884c\u8303\u5f0f\u7684\u5206\u5c42\u591a\u4ee3\u7406\u67b6\u6784\uff1a1) \u7ba1\u7406\u5668\u4ee3\u7406\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u8fdb\u884cOOD\u68c0\u6d4b\u548c\u57fa\u4e8eBERT\u7684\u5206\u7c7b\u5668\u8fdb\u884c\u4ee3\u7406\u8def\u7531\uff1b2) \u4e24\u4e2a\u5de5\u4f5c\u4ee3\u7406\uff1a\u6570\u636e\u5448\u73b0\u4ee3\u7406\u4f7f\u7528API\u6570\u636e\u6a21\u578b\u8fdb\u884c\u6218\u7565\u89c4\u5212\uff0c\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7ec4\u4ef6\uff1b\u6d1e\u5bdf\u751f\u6210\u4ee3\u7406\u52a8\u6001\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u589e\u5f3a\u6d1e\u5bdf\u751f\u6210\u3002", "result": "\u7cfb\u7edf\u5df2\u5728\u7f8e\u56fd\u4e9a\u9a6c\u900a\u5356\u5bb6\u4e0a\u7ebf\uff0c\u57fa\u4e8e\u4eba\u5de5\u8bc4\u4f30\u7684\u51c6\u786e\u7387\u8fbe\u523090%\uff0cP90\u5ef6\u8fdf\u4f4e\u4e8e15\u79d2\uff0c\u80fd\u591f\u6709\u6548\u4e3a\u5356\u5bb6\u63d0\u4f9b\u4e2a\u6027\u5316\u6570\u636e\u6d1e\u5bdf\u3002", "conclusion": "Insight Agents\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u591a\u4ee3\u7406\u67b6\u6784\u548cML\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7535\u5546\u5356\u5bb6\u5728\u5de5\u5177\u5229\u7528\u548c\u6570\u636e\u7406\u89e3\u65b9\u9762\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u548c\u4f4e\u5ef6\u8fdf\u7684\u81ea\u52a8\u5316\u6570\u636e\u6d1e\u5bdf\u670d\u52a1\u3002", "topic": "agent analysis"}}
{"id": "2601.20109", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20109", "abs": "https://arxiv.org/abs/2601.20109", "authors": ["Shamse Tasnim Cynthia", "Al Muttakin", "Banani Roy"], "title": "Beyond Bug Fixes: An Empirical Investigation of Post-Merge Code Quality Issues in Agent-Generated Pull Requests", "comment": null, "summary": "The increasing adoption of AI coding agents has increased the number of agent-generated pull requests (PRs) merged with little or no human intervention. Although such PRs promise productivity gains, their post-merge code quality remains underexplored, as prior work has largely relied on benchmarks and controlled tasks rather than large-scale post-merge analyses. To address this gap, we analyze 1,210 merged agent-generated bug-fix PRs from Python repositories in the AIDev dataset. Using SonarQube, we perform a differential analysis between base and merged commits to identify code quality issues newly introduced by PR changes. We examine issue frequency, density, severity, and rule-level prevalence across five agents. Our results show that apparent differences in raw issue counts across agents largely disappear after normalizing by code churn, indicating that higher issue counts are primarily driven by larger PRs. Across all agents, code smells dominate, particularly at critical and major severities, while bugs are less frequent but often severe. Overall, our findings show that merge success does not reliably reflect post-merge code quality, highlighting the need for systematic quality checks for agent-generated bug-fix PRs.", "AI": {"tldr": "\u5206\u6790AI\u4ee3\u7801\u4ee3\u7406\u751f\u6210\u7684bug\u4fee\u590dPR\u5728\u5408\u5e76\u540e\u7684\u4ee3\u7801\u8d28\u91cf\u95ee\u9898\uff0c\u53d1\u73b0\u5408\u5e76\u6210\u529f\u4e0d\u80fd\u53ef\u9760\u53cd\u6620\u4ee3\u7801\u8d28\u91cf\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u8d28\u91cf\u68c0\u67e5", "motivation": "\u968f\u7740AI\u4ee3\u7801\u4ee3\u7406\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u4ee3\u7406\u751f\u6210\u7684PR\u5728\u5f88\u5c11\u6216\u6ca1\u6709\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u88ab\u5408\u5e76\u3002\u867d\u7136\u8fd9\u4e9bPR\u627f\u8bfa\u63d0\u9ad8\u751f\u4ea7\u529b\uff0c\u4f46\u5408\u5e76\u540e\u7684\u4ee3\u7801\u8d28\u91cf\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u5148\u524d\u5de5\u4f5c\u4e3b\u8981\u4f9d\u8d56\u57fa\u51c6\u6d4b\u8bd5\u548c\u53d7\u63a7\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u5408\u5e76\u540e\u5206\u6790\u3002", "method": "\u4f7f\u7528AIDev\u6570\u636e\u96c6\u4e2d\u76841,210\u4e2a\u5df2\u5408\u5e76\u7684\u4ee3\u7406\u751f\u6210bug\u4fee\u590dPR\uff08\u6765\u81eaPython\u4ed3\u5e93\uff09\uff0c\u901a\u8fc7SonarQube\u5bf9\u57fa\u7840\u63d0\u4ea4\u548c\u5408\u5e76\u63d0\u4ea4\u8fdb\u884c\u5dee\u5f02\u5206\u6790\uff0c\u8bc6\u522bPR\u53d8\u66f4\u65b0\u5f15\u5165\u7684\u4ee3\u7801\u8d28\u91cf\u95ee\u9898\u3002\u5206\u6790\u95ee\u9898\u9891\u7387\u3001\u5bc6\u5ea6\u3001\u4e25\u91cd\u6027\u548c\u89c4\u5219\u7ea7\u5206\u5e03\uff0c\u6db5\u76d6\u4e94\u4e2a\u4e0d\u540c\u7684\u4ee3\u7406\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u4e0d\u540c\u4ee3\u7406\u7684\u539f\u59cb\u95ee\u9898\u6570\u91cf\u5dee\u5f02\u5728\u6309\u4ee3\u7801\u53d8\u66f4\u91cf\u5f52\u4e00\u5316\u540e\u57fa\u672c\u6d88\u5931\uff0c\u8868\u660e\u66f4\u9ad8\u7684\u95ee\u9898\u6570\u91cf\u4e3b\u8981\u7531\u66f4\u5927\u7684PR\u9a71\u52a8\uff1b\u6240\u6709\u4ee3\u7406\u4e2d\uff0c\u4ee3\u7801\u5f02\u5473\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u7279\u522b\u662f\u5728\u5173\u952e\u548c\u4e3b\u8981\u4e25\u91cd\u6027\u7ea7\u522b\uff0c\u800cbug\u8f83\u5c11\u4f46\u901a\u5e38\u5f88\u4e25\u91cd\u3002", "conclusion": "\u5408\u5e76\u6210\u529f\u4e0d\u80fd\u53ef\u9760\u53cd\u6620\u5408\u5e76\u540e\u7684\u4ee3\u7801\u8d28\u91cf\uff0c\u7a81\u663e\u4e86\u5bf9\u4ee3\u7406\u751f\u6210\u7684bug\u4fee\u590dPR\u8fdb\u884c\u7cfb\u7edf\u6027\u8d28\u91cf\u68c0\u67e5\u7684\u5fc5\u8981\u6027\u3002", "topic": "code agent"}}
{"id": "2601.19921", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19921", "abs": "https://arxiv.org/abs/2601.19921", "authors": ["Xiaochen Zhu", "Caiqi Zhang", "Yizhou Chi", "Tom Stafford", "Nigel Collier", "Andreas Vlachos"], "title": "Demystifying Multi-Agent Debate: The Role of Confidence and Diversity", "comment": null, "summary": "Multi-agent debate (MAD) is widely used to improve large language model (LLM) performance through test-time scaling, yet recent work shows that vanilla MAD often underperforms simple majority vote despite higher computational cost. Studies show that, under homogeneous agents and uniform belief updates, debate preserves expected correctness and therefore cannot reliably improve outcomes. Drawing on findings from human deliberation and collective decision-making, we identify two key mechanisms missing from vanilla MAD: (i) diversity of initial viewpoints and (ii) explicit, calibrated confidence communication. We propose two lightweight interventions. First, a diversity-aware initialisation that selects a more diverse pool of candidate answers, increasing the likelihood that a correct hypothesis is present at the start of debate. Second, a confidence-modulated debate protocol in which agents express calibrated confidence and condition their updates on others' confidence. We show theoretically that diversity-aware initialisation improves the prior probability of MAD success without changing the underlying update dynamics, while confidence-modulated updates enable debate to systematically drift to the correct hypothesis. Empirically, across six reasoning-oriented QA benchmarks, our methods consistently outperform vanilla MAD and majority vote. Our results connect human deliberation with LLM-based debate and demonstrate that simple, principled modifications can substantially enhance debate effectiveness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u6539\u8fdb\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7684\u65b9\u6cd5\uff1a\u591a\u6837\u6027\u611f\u77e5\u521d\u59cb\u5316\u548c\u7f6e\u4fe1\u5ea6\u8c03\u5236\u8fa9\u8bba\u534f\u8bae\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfMAD\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u516d\u4e2a\u63a8\u7406\u95ee\u7b54\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfMAD\u548c\u591a\u6570\u6295\u7968\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u8fa9\u8bba(MAD)\u867d\u7136\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5176\u8868\u73b0\u5e38\u5e38\u4e0d\u5982\u7b80\u5355\u7684\u591a\u6570\u6295\u7968\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\u3002\u7814\u7a76\u8868\u660e\u5728\u667a\u80fd\u4f53\u540c\u8d28\u5316\u548c\u5747\u5300\u4fe1\u5ff5\u66f4\u65b0\u7684\u60c5\u51b5\u4e0b\uff0c\u8fa9\u8bba\u53ea\u80fd\u4fdd\u6301\u671f\u671b\u6b63\u786e\u6027\u800c\u65e0\u6cd5\u53ef\u9760\u6539\u5584\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u8f7b\u91cf\u7ea7\u5e72\u9884\u63aa\u65bd\uff1a1) \u591a\u6837\u6027\u611f\u77e5\u521d\u59cb\u5316\uff1a\u9009\u62e9\u66f4\u591a\u6837\u5316\u7684\u5019\u9009\u7b54\u6848\u6c60\uff0c\u589e\u52a0\u8fa9\u8bba\u5f00\u59cb\u65f6\u6b63\u786e\u5047\u8bbe\u5b58\u5728\u7684\u53ef\u80fd\u6027\uff1b2) \u7f6e\u4fe1\u5ea6\u8c03\u5236\u8fa9\u8bba\u534f\u8bae\uff1a\u667a\u80fd\u4f53\u8868\u8fbe\u6821\u51c6\u540e\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u6839\u636e\u4ed6\u4eba\u7684\u7f6e\u4fe1\u5ea6\u6761\u4ef6\u5316\u66f4\u65b0\u81ea\u5df1\u7684\u4fe1\u5ff5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u591a\u6837\u6027\u611f\u77e5\u521d\u59cb\u5316\u63d0\u9ad8\u4e86MAD\u6210\u529f\u7684\u5148\u9a8c\u6982\u7387\u800c\u4e0d\u6539\u53d8\u5e95\u5c42\u66f4\u65b0\u52a8\u6001\uff0c\u7f6e\u4fe1\u5ea6\u8c03\u5236\u66f4\u65b0\u4f7f\u8fa9\u8bba\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u5411\u6b63\u786e\u5047\u8bbe\u6f02\u79fb\u3002\u5728\u516d\u4e2a\u63a8\u7406\u5bfc\u5411\u7684\u95ee\u7b54\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u4f20\u7edfMAD\u548c\u591a\u6570\u6295\u7968\u3002", "conclusion": "\u7814\u7a76\u5c06\u4eba\u7c7b\u5ba1\u8bae\u4e0e\u57fa\u4e8eLLM\u7684\u8fa9\u8bba\u8054\u7cfb\u8d77\u6765\uff0c\u8bc1\u660e\u7b80\u5355\u3001\u6709\u539f\u5219\u7684\u4fee\u6539\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8fa9\u8bba\u6548\u679c\u3002\u591a\u6837\u6027\u521d\u59cb\u5316\u548c\u7f6e\u4fe1\u5ea6\u8c03\u5236\u662f\u5b9e\u73b0\u6709\u6548\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7684\u5173\u952e\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2601.20090", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20090", "abs": "https://arxiv.org/abs/2601.20090", "authors": ["Amirmohammad Farzaneh", "Salvatore D'Oro", "Osvaldo Simeone"], "title": "Should I Have Expressed a Different Intent? Counterfactual Generation for LLM-Based Autonomous Control", "comment": null, "summary": "Large language model (LLM)-powered agents can translate high-level user intents into plans and actions in an environment. Yet after observing an outcome, users may wonder: What if I had phrased my intent differently? We introduce a framework that enables such counterfactual reasoning in agentic LLM-driven control scenarios, while providing formal reliability guarantees. Our approach models the closed-loop interaction between a user, an LLM-based agent, and an environment as a structural causal model (SCM), and leverages test-time scaling to generate multiple candidate counterfactual outcomes via probabilistic abduction. Through an offline calibration phase, the proposed conformal counterfactual generation (CCG) yields sets of counterfactual outcomes that are guaranteed to contain the true counterfactual outcome with high probability. We showcase the performance of CCG on a wireless network control use case, demonstrating significant advantages compared to naive re-execution baselines.", "AI": {"tldr": "\u63d0\u51faCCG\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u548c\u6982\u7387\u6eaf\u56e0\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u4e3aLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u63a7\u5236\u63d0\u4f9b\u5f62\u5f0f\u5316\u53ef\u9760\u6027\u4fdd\u8bc1", "motivation": "\u5f53LLM\u667a\u80fd\u4f53\u5c06\u7528\u6237\u610f\u56fe\u8f6c\u5316\u4e3a\u73af\u5883\u4e2d\u7684\u8ba1\u5212\u548c\u884c\u52a8\u540e\uff0c\u7528\u6237\u53ef\u80fd\u60f3\u77e5\u9053\uff1a\u5982\u679c\u6211\u7528\u4e0d\u540c\u7684\u65b9\u5f0f\u8868\u8fbe\u610f\u56fe\u4f1a\u600e\u6837\uff1f\u9700\u8981\u4e00\u79cd\u652f\u6301\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u5f62\u5f0f\u5316\u53ef\u9760\u6027\u4fdd\u8bc1\u3002", "method": "\u5c06\u7528\u6237\u3001LLM\u667a\u80fd\u4f53\u548c\u73af\u5883\u7684\u95ed\u73af\u4ea4\u4e92\u5efa\u6a21\u4e3a\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u5229\u7528\u6d4b\u8bd5\u65f6\u7f29\u653e\u901a\u8fc7\u6982\u7387\u6eaf\u56e0\u751f\u6210\u591a\u4e2a\u5019\u9009\u53cd\u4e8b\u5b9e\u7ed3\u679c\uff0c\u901a\u8fc7\u79bb\u7ebf\u6821\u51c6\u9636\u6bb5\u5b9e\u73b0\u7b26\u5408\u6027\u53cd\u4e8b\u5b9e\u751f\u6210\uff0c\u4fdd\u8bc1\u9ad8\u6982\u7387\u5305\u542b\u771f\u5b9e\u53cd\u4e8b\u5b9e\u7ed3\u679c", "result": "\u5728\u65e0\u7ebf\u7f51\u7edc\u63a7\u5236\u7528\u4f8b\u4e2d\u5c55\u793a\u4e86CCG\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u7b80\u5355\u7684\u91cd\u65b0\u6267\u884c\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u4f18\u52bf", "conclusion": "CCG\u6846\u67b6\u80fd\u591f\u4e3aLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u63a7\u5236\u573a\u666f\u63d0\u4f9b\u53ef\u9760\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c", "topic": "agent analysis"}}
{"id": "2601.20112", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20112", "abs": "https://arxiv.org/abs/2601.20112", "authors": ["Maja Vukovic", "Rangeet Pan", "Tin Kam Ho", "Rahul Krishna", "Raju Pavuluri", "Michele Merler"], "title": "Usage, Effects and Requirements for AI Coding Assistants in the Enterprise: An Empirical Study", "comment": "To appear in the 3rd International Workshop on Large Language Models For Code, co-located at ICSE, Rio de Janeiro, Brazil, 2026", "summary": "The rise of large language models (LLMs) has accelerated the development of automated techniques and tools for supporting various software engineering tasks, e.g., program understanding, code generation, software testing, and program repair. As CodeLLMs are being employed toward automating these tasks, one question that arises, especially in enterprise settings, is whether these coding assistants and the code LLMs that power them are ready for real-world projects and enterprise use cases, and how do they impact the existing software engineering process and user experience. In this paper we survey 57 developers from different domains and with varying software engineering skill about their experience with AI coding assistants and CodeLLMs. We also reviewed 35 user surveys on the usage, experience and expectations of professionals and students using AI coding assistants and CodeLLMs. Based on our study findings and analysis of existing surveys, we discuss the requirements for AI-powered coding assistants.", "AI": {"tldr": "\u5bf957\u540d\u5f00\u53d1\u8005\u548c35\u4efd\u7528\u6237\u8c03\u67e5\u7684\u5206\u6790\u663e\u793a\uff0cAI\u7f16\u7801\u52a9\u624b\u5728\u63d0\u9ad8\u751f\u4ea7\u529b\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4f01\u4e1a\u7ea7\u5e94\u7528\u3001\u4ee3\u7801\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u96c6\u6210\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9bAI\u7f16\u7801\u52a9\u624b\u662f\u5426\u51c6\u5907\u597d\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u9879\u76ee\u548c\u4f01\u4e1a\u7528\u4f8b\uff0c\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u73b0\u6709\u7684\u8f6f\u4ef6\u5de5\u7a0b\u6d41\u7a0b\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "1. \u8c03\u67e5\u4e86\u6765\u81ea\u4e0d\u540c\u9886\u57df\u3001\u5177\u6709\u4e0d\u540c\u8f6f\u4ef6\u5de5\u7a0b\u6280\u80fd\u768457\u540d\u5f00\u53d1\u8005\u5173\u4e8e\u4ed6\u4eec\u4f7f\u7528AI\u7f16\u7801\u52a9\u624b\u548cCodeLLMs\u7684\u7ecf\u9a8c\uff1b2. \u56de\u987e\u4e8635\u4efd\u5173\u4e8e\u4e13\u4e1a\u4eba\u58eb\u548c\u5b66\u751f\u4f7f\u7528AI\u7f16\u7801\u52a9\u624b\u548cCodeLLMs\u7684\u7528\u6237\u8c03\u67e5\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u7f16\u7801\u52a9\u624b\u5728\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u65b9\u9762\u6709\u79ef\u6781\u4f5c\u7528\uff0c\u4f46\u5728\u4f01\u4e1a\u7ea7\u5e94\u7528\u573a\u666f\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5305\u62ec\u4ee3\u7801\u8d28\u91cf\u3001\u5b89\u5168\u6027\u3001\u96c6\u6210\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\u548c\u73b0\u6709\u8c03\u67e5\u5206\u6790\uff0c\u8ba8\u8bba\u4e86AI\u9a71\u52a8\u7684\u7f16\u7801\u52a9\u624b\u9700\u8981\u6ee1\u8db3\u7684\u8981\u6c42\uff0c\u5305\u62ec\u66f4\u597d\u7684\u4f01\u4e1a\u96c6\u6210\u3001\u4ee3\u7801\u8d28\u91cf\u4fdd\u8bc1\u3001\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u7b49\u65b9\u9762\u7684\u6539\u8fdb\u3002", "topic": "code agent"}}
{"id": "2601.20147", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20147", "abs": "https://arxiv.org/abs/2601.20147", "authors": ["Saima Afrin", "Zaiyu Cheng", "Tushar Sharma", "Alexander Serebrenik", "Massimiliano Di Penta", "Antonio Mastropaolo"], "title": "Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization", "comment": null, "summary": "Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.", "AI": {"tldr": "\u7cfb\u7edf\u7814\u7a76\u4e86\u7cfb\u7edf\u63d0\u793a\u5bf9\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u8d8a\u5927\uff0c\u7cfb\u7edf\u63d0\u793a\u7684\u5f71\u54cd\u8d8a\u663e\u8457\uff0c\u5c11\u6837\u672c\u63d0\u793a\u80fd\u51cf\u5c11\u8fd9\u79cd\u5f71\u54cd\uff0c\u4e14\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u5bf9\u63d0\u793a\u7684\u654f\u611f\u5ea6\u4e0d\u540c\u3002", "motivation": "\u867d\u7136\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7cfb\u7edf\u63d0\u793a\u5bf9\u901a\u7528ILMs\u548c\u4e13\u7528CLMs\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u8003\u5bdf\u4e0d\u540c\u8be6\u7ec6\u7a0b\u5ea6\u7684\u7cfb\u7edf\u63d0\u793a\u3001\u6a21\u578b\u89c4\u6a21\u3001\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672cvs\u5c11\u6837\u672c\uff09\u548c\u7f16\u7a0b\u8bed\u8a00\u5bf9ILMs\u548cCLMs\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5f71\u54cd\uff0c\u5171\u6d4b\u8bd5120\u79cd\u6a21\u578b\u914d\u7f6e\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u7ed3\u679c\uff1a1\uff09\u7cfb\u7edf\u63d0\u793a\u7684\u5f71\u54cd\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u589e\u5f3a\uff1b2\uff09\u5c11\u6837\u672c\u63d0\u793a\u76f8\u6bd4\u96f6\u6837\u672c\u80fd\u51cf\u5c11\u7cfb\u7edf\u63d0\u793a\u7684\u5f71\u54cd\uff1b3\uff09\u7f16\u7a0b\u8bed\u8a00\u5f71\u54cd\u663e\u8457\uff0cJava\u6bd4Python\u5bf9\u7cfb\u7edf\u63d0\u793a\u53d8\u5316\u66f4\u654f\u611f\u3002", "conclusion": "\u7cfb\u7edf\u63d0\u793a\u5bf9\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u8fd9\u79cd\u5f71\u54cd\u53d7\u6a21\u578b\u89c4\u6a21\u3001\u63d0\u793a\u7b56\u7565\u548c\u7f16\u7a0b\u8bed\u8a00\u7b49\u56e0\u7d20\u8c03\u8282\uff0c\u4e3a\u4f18\u5316\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002", "topic": "code agent"}}
{"id": "2601.20221", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20221", "abs": "https://arxiv.org/abs/2601.20221", "authors": ["Hang Zhang", "Ruheng Wang", "Yuelyu Ji", "Mingu Kwak", "Xizhi Wu", "Chenyu Li", "Li Zhang", "Wenqi Shi", "Yifan Peng", "Yanshan Wang"], "title": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning", "comment": null, "summary": "Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\\method$ demonstrates an $\\mathbf{8\\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3a$\\method$\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u533b\u7597\u63a8\u7406\u9a8c\u8bc1\u5668\u5728\u8bc4\u4f30\u65f6\u8fed\u4ee3\u67e5\u8be2\u5916\u90e8\u533b\u7597\u8bed\u6599\u5e93\uff0c\u7ed3\u5408\u5de5\u5177\u589e\u5f3a\u9a8c\u8bc1\u548c\u8fed\u4ee3\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u533b\u7597\u63a8\u7406\u51c6\u786e\u6027\u5e76\u5927\u5e45\u964d\u4f4e\u91c7\u6837\u9884\u7b97\u9700\u6c42\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e34\u5e8a\u90e8\u7f72\u4e2d\u9700\u8981\u4e25\u683c\u9a8c\u8bc1\u4ee5\u786e\u4fdd\u4e8b\u5b9e\u51c6\u786e\u6027\u3002\u73b0\u6709\u5956\u52b1\u6a21\u578b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\uff1a\u53ea\u4ea7\u751f\u6807\u91cf\u5956\u52b1\u503c\u800c\u65e0\u660e\u786e\u7406\u7531\uff0c\u4e14\u4f9d\u8d56\u5355\u6b21\u68c0\u7d22\u65e0\u6cd5\u5728\u9a8c\u8bc1\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u81ea\u9002\u5e94\u77e5\u8bc6\u8bbf\u95ee\u3002", "method": "$\\method$\u6846\u67b6\u8bad\u7ec3\u533b\u7597\u63a8\u7406\u9a8c\u8bc1\u5668\u5728\u8bc4\u4f30\u65f6\u8fed\u4ee3\u67e5\u8be2\u5916\u90e8\u533b\u7597\u8bed\u6599\u5e93\uff0c\u7ed3\u5408\u5de5\u5177\u589e\u5f3a\u9a8c\u8bc1\u548c\u8fed\u4ee3\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff08\u4ec5\u9700\u8f68\u8ff9\u7ea7\u76d1\u7763\uff09\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u8bfe\u7a0b\u673a\u5236\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u3002", "result": "\u5728\u56db\u4e2a\u533b\u7597\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c$\\method$\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff1aMedQA\u51c6\u786e\u7387\u63d0\u9ad823.5%\uff0cMedXpertQA\u63d0\u9ad832.0%\uff08\u76f8\u5bf9\u4e8e\u57fa\u7840\u751f\u6210\u5668\uff09\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u76f8\u6bd4\u5148\u524d\u5956\u52b1\u6a21\u578b\u57fa\u7ebf\uff0c$\\method$\u5c06\u91c7\u6837\u9884\u7b97\u9700\u6c42\u964d\u4f4e\u4e868\u500d\u3002", "conclusion": "\u57fa\u4e8e\u52a8\u6001\u68c0\u7d22\u8bc1\u636e\u7684\u9a8c\u8bc1\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u533b\u7597\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8def\u5f84\uff0c$\\method$\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u77e5\u8bc6\u8bbf\u95ee\u548c\u81ea\u9002\u5e94\u8bad\u7ec3\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.20323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20323", "abs": "https://arxiv.org/abs/2601.20323", "authors": ["Hyunseung Chung", "Jungwoo Oh", "Daeun Kyung", "Jiho Kim", "Yeonsu Kwon", "Min-Gyu Kim", "Edward Choi"], "title": "ECG-Agent: On-Device Tool-Calling Agent for ECG Multi-Turn Dialogue", "comment": "Accepted to ICASSP 2026 (5 pages, 2 figures, 5 tables)", "summary": "Recent advances in Multimodal Large Language Models have rapidly expanded to electrocardiograms, focusing on classification, report generation, and single-turn QA tasks. However, these models fall short in real-world scenarios, lacking multi-turn conversational ability, on-device efficiency, and precise understanding of ECG measurements such as the PQRST intervals. To address these limitations, we introduce ECG-Agent, the first LLM-based tool-calling agent for multi-turn ECG dialogue. To facilitate its development and evaluation, we also present ECG-Multi-Turn-Dialogue (ECG-MTD) dataset, a collection of realistic user-assistant multi-turn dialogues for diverse ECG lead configurations. We develop ECG-Agents in various sizes, from on-device capable to larger agents. Experimental results show that ECG-Agents outperform baseline ECG-LLMs in response accuracy. Furthermore, on-device agents achieve comparable performance to larger agents in various evaluations that assess response accuracy, tool-calling ability, and hallucinations, demonstrating their viability for real-world applications.", "AI": {"tldr": "ECG-Agent\uff1a\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u5de5\u5177\u8c03\u7528\u4ee3\u7406\uff0c\u7528\u4e8e\u591a\u8f6e\u5fc3\u7535\u56fe\u5bf9\u8bdd\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\u3001\u8bbe\u5907\u7aef\u6548\u7387\u548c\u7cbe\u786eECG\u6d4b\u91cf\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7535\u56fe\u5e94\u7528\u4e0a\u5c40\u9650\u4e8e\u5206\u7c7b\u3001\u62a5\u544a\u751f\u6210\u548c\u5355\u8f6e\u95ee\u7b54\u4efb\u52a1\uff0c\u7f3a\u4e4f\u771f\u5b9e\u573a\u666f\u6240\u9700\u7684\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\u3001\u8bbe\u5907\u7aef\u6548\u7387\u4ee5\u53ca\u5bf9PQRST\u95f4\u9694\u7b49ECG\u6d4b\u91cf\u7684\u7cbe\u786e\u7406\u89e3\u3002", "method": "\u5f15\u5165ECG-Agent\u4f5c\u4e3a\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u5de5\u5177\u8c03\u7528\u4ee3\u7406\uff0c\u7528\u4e8e\u591a\u8f6eECG\u5bf9\u8bdd\u3002\u540c\u65f6\u521b\u5efaECG-MTD\u6570\u636e\u96c6\uff08\u5305\u542b\u771f\u5b9e\u7528\u6237-\u52a9\u624b\u591a\u8f6e\u5bf9\u8bdd\uff09\uff0c\u5e76\u5f00\u53d1\u4ece\u8bbe\u5907\u7aef\u5230\u5927\u578b\u4ee3\u7406\u7684\u5404\u79cd\u89c4\u6a21ECG-Agent\u3002", "result": "ECG-Agent\u5728\u54cd\u5e94\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebfECG-LLM\u3002\u8bbe\u5907\u7aef\u4ee3\u7406\u5728\u54cd\u5e94\u51c6\u786e\u6027\u3001\u5de5\u5177\u8c03\u7528\u80fd\u529b\u548c\u5e7b\u89c9\u8bc4\u4f30\u65b9\u9762\u4e0e\u5927\u578b\u4ee3\u7406\u8868\u73b0\u76f8\u5f53\uff0c\u8bc1\u660e\u5176\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "ECG-Agent\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709ECG-LLM\u7684\u5c40\u9650\u6027\uff0c\u8bbe\u5907\u7aef\u4ee3\u7406\u7684\u53ef\u884c\u6027\u4e3a\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0cECG-MTD\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002", "topic": "code agent"}}
{"id": "2601.20160", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20160", "abs": "https://arxiv.org/abs/2601.20160", "authors": ["Lukas Ottenhof", "Daniel Penner", "Abram Hindle", "Thibaud Lutellier"], "title": "How do Agents Refactor: An Empirical Study", "comment": "Accepted for publication in 23rd International Mining Software Repositories Conference (MSR 2026) : 5 pages, 4 tables", "summary": "Software development agents such as Claude Code, GitHub Copilot, Cursor Agent, Devin, and OpenAI Codex are being increasingly integrated into developer workflows. While prior work has evaluated agent capabilities for code completion and task automation, there is little work investigating how these agents perform Java refactoring in practice, the types of changes they make, and their impact on code quality. In this study, we present the first analysis of agentic refactoring pull requests in Java, comparing them to developer refactorings across 86 projects per group. Using RefactoringMiner and DesigniteJava 3.0, we identify refactoring types and detect code smells before and after refactoring commits. Our results show that agent refactorings are dominated by annotation changes (the 5 most common refactoring types done by agents are annotation related), in contrast to the diverse structural improvements typical of developers. Despite these differences in refactoring types, we find Cursor to be the only model to show a statistically significant increase in refactoring smells.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5206\u6790\u4e86Java\u4ee3\u7801\u91cd\u6784\u4e2dAI\u4ee3\u7406\u7684\u8868\u73b0\uff0c\u5bf9\u6bd4\u4e86AI\u4ee3\u7406\u4e0e\u5f00\u53d1\u8005\u7684\u91cd\u6784\u884c\u4e3a\uff0c\u53d1\u73b0AI\u4ee3\u7406\u4e3b\u8981\u8fdb\u884c\u6ce8\u89e3\u76f8\u5173\u7684\u91cd\u6784\uff0c\u800c\u5f00\u53d1\u8005\u5219\u8fdb\u884c\u66f4\u591a\u7ed3\u6784\u6027\u6539\u8fdb\uff0c\u53ea\u6709Cursor\u6a21\u578b\u5728\u91cd\u6784\u540e\u4ee3\u7801\u5f02\u5473\u6709\u663e\u8457\u589e\u52a0\u3002", "motivation": "\u968f\u7740Claude Code\u3001GitHub Copilot\u7b49\u8f6f\u4ef6\u5f00\u53d1\u4ee3\u7406\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u4e2d\uff0c\u867d\u7136\u5df2\u6709\u7814\u7a76\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u4ee3\u7406\u5728\u4ee3\u7801\u8865\u5168\u548c\u4efb\u52a1\u81ea\u52a8\u5316\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5b83\u4eec\u5728Java\u91cd\u6784\u5b9e\u8df5\u4e2d\u8868\u73b0\u7684\u7814\u7a76\uff0c\u5305\u62ec\u5b83\u4eec\u8fdb\u884c\u4f55\u79cd\u7c7b\u578b\u7684\u91cd\u6784\u53d8\u66f4\u4ee5\u53ca\u5bf9\u4ee3\u7801\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86AI\u4ee3\u7406\u548c\u5f00\u53d1\u8005\u5404\u81ea86\u4e2a\u9879\u76ee\u4e2d\u7684\u91cd\u6784\u62c9\u53d6\u8bf7\u6c42\uff0c\u4f7f\u7528RefactoringMiner\u8bc6\u522b\u91cd\u6784\u7c7b\u578b\uff0c\u4f7f\u7528DesigniteJava 3.0\u68c0\u6d4b\u91cd\u6784\u524d\u540e\u7684\u4ee3\u7801\u5f02\u5473\uff0c\u5bf9\u6bd4\u4e86\u4e24\u7ec4\u5728\u91cd\u6784\u7c7b\u578b\u548c\u4ee3\u7801\u8d28\u91cf\u5f71\u54cd\u4e0a\u7684\u5dee\u5f02\u3002", "result": "AI\u4ee3\u7406\u7684\u91cd\u6784\u4e3b\u8981\u96c6\u4e2d\u5728\u6ce8\u89e3\u53d8\u66f4\uff08\u524d5\u79cd\u6700\u5e38\u89c1\u91cd\u6784\u7c7b\u578b\u90fd\u662f\u6ce8\u89e3\u76f8\u5173\u7684\uff09\uff0c\u800c\u5f00\u53d1\u8005\u5219\u8fdb\u884c\u66f4\u591a\u6837\u5316\u7684\u7ed3\u6784\u6027\u6539\u8fdb\u3002\u5c3d\u7ba1\u91cd\u6784\u7c7b\u578b\u5b58\u5728\u5dee\u5f02\uff0c\u4f46\u53ea\u6709Cursor\u6a21\u578b\u663e\u793a\u51fa\u91cd\u6784\u540e\u4ee3\u7801\u5f02\u5473\u6709\u7edf\u8ba1\u663e\u8457\u6027\u7684\u589e\u52a0\u3002", "conclusion": "AI\u4ee3\u7406\u5728Java\u91cd\u6784\u4e2d\u7684\u884c\u4e3a\u4e0e\u5f00\u53d1\u8005\u6709\u663e\u8457\u5dee\u5f02\uff0c\u4e3b\u8981\u5173\u6ce8\u6ce8\u89e3\u5c42\u9762\u7684\u53d8\u66f4\u800c\u975e\u7ed3\u6784\u6027\u6539\u8fdb\uff0c\u4e14\u53ea\u6709\u7279\u5b9a\u6a21\u578b\uff08Cursor\uff09\u5bf9\u4ee3\u7801\u8d28\u91cf\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u8fd9\u4e3aAI\u8f85\u52a9\u91cd\u6784\u5de5\u5177\u7684\u5f00\u53d1\u548c\u4f7f\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "topic": "swe application"}}
{"id": "2601.20352", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20352", "abs": "https://arxiv.org/abs/2601.20352", "authors": ["Weiquan Huang", "Zixuan Wang", "Hehai Lin", "Sudong Wang", "Bo Xu", "Qian Li", "Beier Zhu", "Linyi Yang", "Chengwei Qin"], "title": "AMA: Adaptive Memory via Multi-Agent Collaboration", "comment": "8 pages", "summary": "The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.", "AI": {"tldr": "AMA\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u81ea\u9002\u5e94\u8bb0\u5fc6\u7ba1\u7406\uff0c\u91c7\u7528\u5206\u5c42\u8bb0\u5fc6\u8bbe\u8ba1\u548c\u52a8\u6001\u7c92\u5ea6\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u957f\u671f\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u68c0\u7d22\u7cbe\u5ea6\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4etoken\u6d88\u8017\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u68c0\u7d22\u7c92\u5ea6\u56fa\u5b9a\uff0c\u65e0\u6cd5\u5339\u914d\u4efb\u52a1\u590d\u6742\u5ea6\uff1b2\uff09\u7ef4\u62a4\u7b56\u7565\u79ef\u7d2f\u8fc7\u91cd\uff1b3\uff09\u66f4\u65b0\u673a\u5236\u7c97\u7c92\u5ea6\u3002\u8fd9\u5bfc\u81f4\u5b58\u50a8\u4fe1\u606f\u4e0e\u4efb\u52a1\u63a8\u7406\u9700\u6c42\u4e0d\u5339\u914d\uff0c\u4e14\u968f\u65f6\u95f4\u79ef\u7d2f\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faAMA\uff08Adaptive Memory via Multi-Agent Collaboration\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u534f\u8c03\u667a\u80fd\u4f53\u7ba1\u7406\u591a\u7c92\u5ea6\u8bb0\u5fc6\uff1aConstructor\u548cRetriever\u5b9e\u73b0\u591a\u7c92\u5ea6\u8bb0\u5fc6\u6784\u5efa\u548c\u81ea\u9002\u5e94\u67e5\u8be2\u8def\u7531\uff1bJudge\u9a8c\u8bc1\u68c0\u7d22\u5185\u5bb9\u76f8\u5173\u6027\u548c\u4e00\u81f4\u6027\uff1bRefresher\u68c0\u6d4b\u5230\u903b\u8f91\u51b2\u7a81\u65f6\u6267\u884c\u9488\u5bf9\u6027\u66f4\u65b0\u6216\u5220\u9664\u8fc7\u65f6\u6761\u76ee\u3002", "result": "\u5728\u6311\u6218\u6027\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAMA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u540c\u65f6\u76f8\u6bd4\u5168\u4e0a\u4e0b\u6587\u65b9\u6cd5\u51cf\u5c11\u7ea680%\u7684token\u6d88\u8017\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4fdd\u6301\u68c0\u7d22\u7cbe\u5ea6\u548c\u957f\u671f\u8bb0\u5fc6\u4e00\u81f4\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "AMA\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u5206\u5c42\u8bb0\u5fc6\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u4e00\u81f4\u7684\u81ea\u9002\u5e94\u8bb0\u5fc6\u7ba1\u7406\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.20171", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20171", "abs": "https://arxiv.org/abs/2601.20171", "authors": ["Kazuma Yamasaki", "Joseph Ayobami Joshua", "Tasha Settewong", "Mahmoud Alfadel", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "Who Writes the Docs in SE 3.0? Agent vs. Human Documentation Pull Requests", "comment": "Comments: 5 pages, 5 figures. To appear in MSR 2026 Mining Challenge (April 2026). Code available at https://github.com/NAIST-SE/msr2026-docs-prs-replication", "summary": "As software engineering moves toward SE3.0, AI agents are increasingly used to carry out development tasks and contribute changes to software projects. It is therefore important to understand the extent of these contributions and how human developers review and intervene, since these factors shape the risks of delegating work to AI agents. While recent studies have examined how AI agents support software development tasks (e.g., code generation, issue resolution, and PR automation), their role in documentation tasks remains underexplored-even though documentation is widely consumed and shapes how developers understand and use software.\n  Using the AIDev, we analyze 1,997 documentation-related pull requests (PRs) authored by AI agents and human developers, where documentation PRs are those that create or modify project documentation artifacts. We find that AI agents submit substantially more documentation-related PRs than humans in the studied repositories. We further observe that agent-authored documentation edits are typically integrated with little follow-up modification from humans, raising concerns about review practices and the reliability of agent-generated documentation. Overall, while AI agents already contribute substantially to documentation workflows, our results suggest concerns for emerging challenges for documentation quality assurance and human-AI collaboration in SE3.0.", "AI": {"tldr": "AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5927\u91cf\u63d0\u4ea4\u6587\u6863\u76f8\u5173\u7684PR\uff0c\u4f46\u4eba\u7c7b\u5ba1\u67e5\u4e0d\u8db3\uff0c\u5f15\u53d1\u6587\u6863\u8d28\u91cf\u62c5\u5fe7", "motivation": "\u968f\u7740SE3.0\u65f6\u4ee3\u5230\u6765\uff0cAI\u4ee3\u7406\u8d8a\u6765\u8d8a\u591a\u5730\u53c2\u4e0e\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\uff0c\u4f46\u5b83\u4eec\u5728\u6587\u6863\u4efb\u52a1\u4e2d\u7684\u89d2\u8272\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u6587\u6863\u5bf9\u8f6f\u4ef6\u7406\u89e3\u548c\u4f7f\u7528\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4e86\u89e3AI\u4ee3\u7406\u7684\u6587\u6863\u8d21\u732e\u7a0b\u5ea6\u4ee5\u53ca\u4eba\u7c7b\u5f00\u53d1\u8005\u7684\u5ba1\u67e5\u548c\u5e72\u9884\u60c5\u51b5\uff0c\u4ee5\u8bc4\u4f30\u59d4\u6258\u5de5\u4f5c\u7ed9AI\u4ee3\u7406\u7684\u98ce\u9669\u3002", "method": "\u4f7f\u7528AIDev\u5de5\u5177\u5206\u6790\u4e861,997\u4e2a\u7531AI\u4ee3\u7406\u548c\u4eba\u7c7b\u5f00\u53d1\u8005\u63d0\u4ea4\u7684\u6587\u6863\u76f8\u5173PR\uff08\u521b\u5efa\u6216\u4fee\u6539\u9879\u76ee\u6587\u6863\u7684PR\uff09\u3002", "result": "AI\u4ee3\u7406\u63d0\u4ea4\u7684\u6587\u6863\u76f8\u5173PR\u6570\u91cf\u663e\u8457\u591a\u4e8e\u4eba\u7c7b\u3002AI\u4ee3\u7406\u64b0\u5199\u7684\u6587\u6863\u7f16\u8f91\u901a\u5e38\u5f88\u5c11\u7ecf\u8fc7\u4eba\u7c7b\u540e\u7eed\u4fee\u6539\u5c31\u88ab\u96c6\u6210\uff0c\u8fd9\u5f15\u53d1\u4e86\u5ba1\u67e5\u5b9e\u8df5\u548cAI\u751f\u6210\u6587\u6863\u53ef\u9760\u6027\u7684\u62c5\u5fe7\u3002", "conclusion": "\u867d\u7136AI\u4ee3\u7406\u5df2\u7ecf\u5728\u6587\u6863\u5de5\u4f5c\u6d41\u4e2d\u505a\u51fa\u91cd\u8981\u8d21\u732e\uff0c\u4f46\u7814\u7a76\u7ed3\u679c\u8868\u660eSE3.0\u65f6\u4ee3\u6587\u6863\u8d28\u91cf\u4fdd\u8bc1\u548c\u4eba\u7c7b-AI\u534f\u4f5c\u9762\u4e34\u65b0\u5174\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2601.20223", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20223", "abs": "https://arxiv.org/abs/2601.20223", "authors": ["Aral de Moor", "Yana Hrynevich", "Hleb Badzeika", "Vladyslav Furda", "Marko Kojic", "Artem Savelev", "Kostadin Cvejoski", "Darya Rovdo", "Ekaterina Garanina"], "title": "Control Models for In-IDE Code Completion", "comment": "6 pages; accepted at IDE'26 co-located with ICSE'26", "summary": "We introduce control models for LLM-powered code completion in JetBrains IDEs: ML classifiers which trigger inference and filter the generated suggestions to better align them with users and reduce unnecessary requests. To this end, we evaluate boosting- and transformer-based architectures on an offline dataset of real code completions with n=98 users. We further evaluate the offline classification performance of our boosting-based approach on a range of syntactically diverse languages; and perform an A/B study in a production environment where they improve completion efficiency and quality metrics. With this study, we hope to demonstrate the potential in using auxiliary models for smarter in-IDE integration of LLM-driven features, highlight fruitful future directions, and open problems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8eJetBrains IDE\u4e2dLLM\u4ee3\u7801\u8865\u5168\u7684\u63a7\u5236\u6a21\u578b\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u89e6\u53d1\u63a8\u7406\u5e76\u8fc7\u6ee4\u751f\u6210\u5efa\u8bae\uff0c\u4ee5\u63d0\u9ad8\u4e0e\u7528\u6237\u7684\u5339\u914d\u5ea6\u5e76\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8bf7\u6c42\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u4ee3\u7801\u8865\u5168\u5728IDE\u4e2d\u53ef\u80fd\u4ea7\u751f\u4e0d\u76f8\u5173\u6216\u4e0d\u5fc5\u8981\u7684\u5efa\u8bae\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u89e6\u53d1\u548c\u8fc7\u6ee4\u673a\u5236\u6765\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u7cfb\u7edf\u6548\u7387\u3002", "method": "\u4f7f\u7528\u63d0\u5347\uff08boosting\uff09\u548c\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u5728\u771f\u5b9e\u4ee3\u7801\u8865\u5168\u6570\u636e\u96c6\uff08n=98\u7528\u6237\uff09\u4e0a\u8fdb\u884c\u79bb\u7ebf\u8bc4\u4f30\uff0c\u5e76\u5728\u591a\u79cd\u8bed\u6cd5\u591a\u6837\u5316\u7684\u8bed\u8a00\u4e0a\u6d4b\u8bd5\u5206\u7c7b\u6027\u80fd\uff0c\u6700\u540e\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u8fdb\u884cA/B\u6d4b\u8bd5\u3002", "result": "\u63d0\u5347\u65b9\u6cd5\u5728\u79bb\u7ebf\u5206\u7c7b\u6027\u80fd\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u751f\u4ea7\u73af\u5883A/B\u6d4b\u8bd5\u663e\u793a\u63d0\u9ad8\u4e86\u8865\u5168\u6548\u7387\u548c\u8d28\u91cf\u7684\u6307\u6807\u3002", "conclusion": "\u8f85\u52a9\u6a21\u578b\u5728IDE\u4e2d\u667a\u80fd\u96c6\u6210LLM\u9a71\u52a8\u529f\u80fd\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u542f\u793a\uff0c\u5e76\u6307\u51fa\u4e86\u5f00\u653e\u6027\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2601.19928", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.19928", "abs": "https://arxiv.org/abs/2601.19928", "authors": ["Yi Hu", "Jiaqi Gu", "Ruxin Wang", "Zijun Yao", "Hao Peng", "Xiaobao Wu", "Jianhui Chen", "Muhan Zhang", "Liangming Pan"], "title": "Towards a Mechanistic Understanding of Large Reasoning Models: A Survey of Training, Inference, and Failures", "comment": null, "summary": "Reinforcement learning (RL) has catalyzed the emergence of Large Reasoning Models (LRMs) that have pushed reasoning capabilities to new heights. While their performance has garnered significant excitement, exploring the internal mechanisms driving these behaviors has become an equally critical research frontier. This paper provides a comprehensive survey of the mechanistic understanding of LRMs, organizing recent findings into three core dimensions: 1) training dynamics, 2) reasoning mechanisms, and 3) unintended behaviors. By synthesizing these insights, we aim to bridge the gap between black-box performance and mechanistic transparency. Finally, we discuss under-explored challenges to outline a roadmap for future mechanistic studies, including the need for applied interpretability, improved methodologies, and a unified theoretical framework.", "AI": {"tldr": "\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u673a\u5236\u7406\u89e3\u8fdb\u884c\u5168\u9762\u7efc\u8ff0\uff0c\u4ece\u8bad\u7ec3\u52a8\u6001\u3001\u63a8\u7406\u673a\u5236\u548c\u610f\u5916\u884c\u4e3a\u4e09\u4e2a\u7ef4\u5ea6\u7ec4\u7ec7\u6700\u65b0\u53d1\u73b0\uff0c\u65e8\u5728\u5f25\u5408\u9ed1\u76d2\u6027\u80fd\u4e0e\u673a\u5236\u900f\u660e\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u63a8\u52a8\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u867d\u7136\u6027\u80fd\u4ee4\u4eba\u5174\u594b\uff0c\u4f46\u7406\u89e3\u9a71\u52a8\u8fd9\u4e9b\u884c\u4e3a\u7684\u5185\u90e8\u673a\u5236\u5df2\u6210\u4e3a\u540c\u7b49\u91cd\u8981\u7684\u7814\u7a76\u524d\u6cbf\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u9ed1\u76d2\u6027\u80fd\u4e0e\u673a\u5236\u900f\u660e\u5ea6\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5c06\u73b0\u6709\u7814\u7a76\u6210\u679c\u7ec4\u7ec7\u6210\u4e09\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff1a1) \u8bad\u7ec3\u52a8\u6001\uff0c2) \u63a8\u7406\u673a\u5236\uff0c3) \u610f\u5916\u884c\u4e3a\u3002\u901a\u8fc7\u7efc\u5408\u8fd9\u4e9b\u89c1\u89e3\uff0c\u6784\u5efa\u673a\u5236\u7406\u89e3\u7684\u6846\u67b6\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\u673a\u5236\u7406\u89e3\u7684\u5168\u9762\u8c03\u67e5\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u7684\u5173\u952e\u53d1\u73b0\u548c\u77e5\u8bc6\u7a7a\u767d\uff0c\u4e3a\u7406\u89e3\u8fd9\u4e9b\u590d\u6742\u7cfb\u7edf\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u5efa\u7acb\u4e86\u7ed3\u6784\u5316\u6846\u67b6\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5e94\u7528\u53ef\u89e3\u91ca\u6027\u3001\u6539\u8fdb\u65b9\u6cd5\u8bba\u548c\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u4ee5\u63a8\u8fdb\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u673a\u5236\u7406\u89e3\uff0c\u8fd9\u5c06\u662f\u672a\u6765\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.20380", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20380", "abs": "https://arxiv.org/abs/2601.20380", "authors": ["Le Zhang", "Yixiong Xiao", "Xinjiang Lu", "Jingjia Cao", "Yusai Zhao", "Jingbo Zhou", "Lang An", "Zikan Feng", "Wanxiang Sha", "Yu Shi", "Congxi Xiao", "Jian Xiong", "Yankai Zhang", "Hua Wu", "Haifeng Wang"], "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution", "comment": null, "summary": "Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.", "AI": {"tldr": "OmegaUse\u662f\u4e00\u4e2a\u901a\u7528\u7684GUI\u4ee3\u7406\u6a21\u578b\uff0c\u652f\u6301\u79fb\u52a8\u548c\u684c\u9762\u5e73\u53f0\u7684\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\uff0c\u91c7\u7528\u6570\u636e\u5408\u6210\u6846\u67b6\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "GUI\u4ee3\u7406\u5177\u6709\u8ba9\u57fa\u7840\u6a21\u578b\u5b8c\u6210\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u9769\u65b0\u4eba\u673a\u4ea4\u4e92\u5e76\u63d0\u9ad8\u4eba\u7c7b\u751f\u4ea7\u529b\u3002\u5f53\u524d\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u652f\u6301\u8de8\u5e73\u53f0\uff08\u79fb\u52a8\u548c\u684c\u9762\uff09\u7684\u901a\u7528GUI\u4ee3\u7406\u6a21\u578b\u3002", "method": "1) \u6570\u636e\u6784\u5efa\uff1a\u7ed3\u5408\u7cbe\u5fc3\u7b56\u5212\u7684\u5f00\u6e90\u6570\u636e\u96c6\u548c\u81ea\u52a8\u5316\u5408\u6210\u6846\u67b6\uff08\u81ea\u5e95\u5411\u4e0a\u81ea\u4e3b\u63a2\u7d22+\u81ea\u9876\u5411\u4e0b\u5206\u7c7b\u6307\u5bfc\u751f\u6210\uff09\uff1b2) \u8bad\u7ec3\uff1a\u4e24\u9636\u6bb5\u7b56\u7565\uff1a\u76d1\u7763\u5fae\u8c03\u5efa\u7acb\u57fa\u672c\u4ea4\u4e92\u8bed\u6cd5\uff0c\u7136\u540e\u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u63d0\u5347\u7a7a\u95f4\u5b9a\u4f4d\u548c\u5e8f\u5217\u89c4\u5212\uff1b3) \u67b6\u6784\uff1a\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u80fd\u529b\u3002", "result": "OmegaUse\u5728\u591a\u4e2aGUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u5728ScreenSpot-V2\u4e0a\u8fbe\u523096.3%\u7684SOTA\u5206\u6570\uff0c\u5728AndroidControl\u4e0a\u8fbe\u523079.1%\u7684\u6b65\u9aa4\u6210\u529f\u7387\u3002\u5728\u65b0\u63d0\u51fa\u7684OS-Nav\u57fa\u51c6\u4e0a\uff0c\u5728ChiM-Nav\u4e0a\u8fbe\u523074.24%\u6b65\u9aa4\u6210\u529f\u7387\uff0c\u5728Ubu-Nav\u4e0a\u8fbe\u523055.9%\u5e73\u5747\u6210\u529f\u7387\u3002", "conclusion": "OmegaUse\u662f\u4e00\u4e2a\u6709\u6548\u7684\u901a\u7528GUI\u4ee3\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u6784\u5efa\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u8de8\u5e73\u53f0GUI\u4efb\u52a1\u6267\u884c\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.19929", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19929", "abs": "https://arxiv.org/abs/2601.19929", "authors": ["David Linus Ostby"], "title": "Stingy Context: 18:1 Hierarchical Code Compression for LLM Auto-Coding", "comment": "28 pages, 10 tables, 2 figures and 6 appendices", "summary": "We introduce Stingy Context, a hierarchical tree-based compression scheme achieving 18:1 reduction in LLM context for auto-coding tasks. Using our TREEFRAG exploit decomposition, we reduce a real source code base of 239k tokens to 11k tokens while preserving task fidelity. Empirical results across 12 Frontier models show 94 to 97% success on 40 real-world issues at low cost, outperforming flat methods and mitigating lost-in-the-middle effects.", "AI": {"tldr": "Stingy Context\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u6811\u7684\u538b\u7f29\u65b9\u6848\uff0c\u5728\u81ea\u52a8\u7f16\u7801\u4efb\u52a1\u4e2d\u5b9e\u73b018:1\u7684LLM\u4e0a\u4e0b\u6587\u538b\u7f29\u6bd4\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u4ee3\u7801\u5e93\u5904\u7406\u65f6LLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\u7684\u95ee\u9898\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u907f\u514d\"\u8ff7\u5931\u5728\u4e2d\u95f4\"\u6548\u5e94\uff0c\u63d0\u9ad8\u81ea\u52a8\u7f16\u7801\u4efb\u52a1\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6811\u72b6\u538b\u7f29\u65b9\u6848\uff0c\u7ed3\u5408TREEFRAG\u5206\u89e3\u6280\u672f\uff0c\u5c06\u5927\u578b\u4ee3\u7801\u5e93\u7ed3\u6784\u5316\u538b\u7f29\uff0c\u4fdd\u7559\u5173\u952e\u4ee3\u7801\u7ed3\u6784\u548c\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002", "result": "\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\uff08239k tokens\uff09\u4e0a\u5b9e\u73b0\u538b\u7f29\u81f311k tokens\uff0818:1\u538b\u7f29\u6bd4\uff09\uff0c\u572812\u4e2a\u524d\u6cbf\u6a21\u578b\u4e0a\u6d4b\u8bd540\u4e2a\u771f\u5b9e\u95ee\u9898\uff0c\u8fbe\u523094-97%\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u5e73\u9762\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "Stingy Context\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u4ee3\u7801\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4eLLM\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u4ee3\u7801\u5904\u7406\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.20467", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20467", "abs": "https://arxiv.org/abs/2601.20467", "authors": ["Zhenxuan Fan", "Jie Cao", "Yang Dai", "Zheqi Lv", "Wenqiao Zhang", "Zhongle Xie", "Peng LU", "Beng Chin Ooi"], "title": "CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning", "comment": "16 pages, 9 figures, 11 tables", "summary": "Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \\textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.", "AI": {"tldr": "CtrlCoT\u662f\u4e00\u4e2a\u53cc\u7c92\u5ea6CoT\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u62bd\u8c61\u3001\u903b\u8f91\u4fdd\u6301\u84b8\u998f\u548c\u5206\u5e03\u5bf9\u9f50\u751f\u6210\uff0c\u5728\u51cf\u5c1130.7%token\u7684\u540c\u65f6\u63d0\u5347\u63a8\u7406\u51c6\u786e\u73877.6\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u4f20\u7edfCoT\u63d0\u793a\u867d\u7136\u80fd\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4f1a\u4ea7\u751f\u5197\u957f\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u5185\u5b58\u6210\u672c\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u8981\u4e48\u5728\u8bed\u4e49\u5c42\u9762\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u8981\u4e48\u5728token\u5c42\u9762\u8fc7\u4e8e\u6fc0\u8fdb\uff0c\u5bb9\u6613\u4e22\u5931\u5173\u952e\u63a8\u7406\u7ebf\u7d22\u5e76\u964d\u4f4e\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faCtrlCoT\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u5206\u5c42\u63a8\u7406\u62bd\u8c61\uff1a\u751f\u6210\u591a\u7c92\u5ea6\u8bed\u4e49\u7684CoT\uff1b2) \u903b\u8f91\u4fdd\u6301\u84b8\u998f\uff1a\u8bad\u7ec3\u903b\u8f91\u611f\u77e5\u7684\u526a\u679d\u5668\uff0c\u4fdd\u7559\u5173\u952e\u63a8\u7406\u7ebf\u7d22\uff1b3) \u5206\u5e03\u5bf9\u9f50\u751f\u6210\uff1a\u5bf9\u9f50\u538b\u7f29\u8f68\u8ff9\u4e0e\u63a8\u7406\u98ce\u683c\uff0c\u907f\u514d\u788e\u7247\u5316\u3002", "result": "\u5728MATH-500\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Qwen2.5-7B-Instruct\u6a21\u578b\uff0cCtrlCoT\u6bd4\u6700\u5f3a\u57fa\u7ebf\u4f7f\u752830.7%\u66f4\u5c11\u7684token\uff0c\u540c\u65f6\u51c6\u786e\u7387\u9ad8\u51fa7.6\u4e2a\u767e\u5206\u70b9\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u63a8\u7406\u3002", "conclusion": "CtrlCoT\u901a\u8fc7\u534f\u8c03\u8bed\u4e49\u62bd\u8c61\u548ctoken\u7ea7\u526a\u679d\uff0c\u6709\u6548\u89e3\u51b3\u4e86CoT\u538b\u7f29\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6b63\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2601.20539", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20539", "abs": "https://arxiv.org/abs/2601.20539", "authors": ["Oguzhan Gungordu", "Siheng Xiong", "Faramarz Fekri"], "title": "PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs", "comment": null, "summary": "Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.", "AI": {"tldr": "PathWise\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u81ea\u52a8\u5316\u542f\u53d1\u5f0f\u8bbe\u8ba1\u6846\u67b6\uff0c\u5c06\u542f\u53d1\u5f0f\u751f\u6210\u5efa\u6a21\u4e3a\u57fa\u4e8e\u8574\u542b\u56fe\u7684\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7\u72b6\u6001\u611f\u77e5\u89c4\u5212\u800c\u975e\u8bd5\u9519\u8fdb\u5316\u6765\u751f\u6210\u66f4\u597d\u7684\u7ec4\u5408\u4f18\u5316\u542f\u53d1\u5f0f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u542f\u53d1\u5f0f\u8bbe\u8ba1\u6846\u67b6\u4f9d\u8d56\u56fa\u5b9a\u7684\u8fdb\u5316\u89c4\u5219\u548c\u9759\u6001\u63d0\u793a\u6a21\u677f\uff0c\u5bfc\u81f4\u542f\u53d1\u5f0f\u751f\u6210\u77ed\u89c6\u3001\u8bc4\u4f30\u5197\u4f59\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u65b0\u542f\u53d1\u5f0f\u5982\u4f55\u63a8\u5bfc\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faPathWise\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff1a1) \u5c06\u542f\u53d1\u5f0f\u751f\u6210\u5efa\u6a21\u4e3a\u57fa\u4e8e\u8574\u542b\u56fe\u7684\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff1b2) \u7b56\u7565\u667a\u80fd\u4f53\u89c4\u5212\u8fdb\u5316\u52a8\u4f5c\uff1b3) \u4e16\u754c\u6a21\u578b\u667a\u80fd\u4f53\u57fa\u4e8e\u52a8\u4f5c\u751f\u6210\u542f\u53d1\u5f0f\u63a8\u6f14\uff1b4) \u6279\u8bc4\u667a\u80fd\u4f53\u63d0\u4f9b\u8def\u7531\u53cd\u601d\u603b\u7ed3\u5148\u524d\u7ecf\u9a8c\u6559\u8bad\u3002", "result": "\u5728\u591a\u79cd\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\uff0cPathWise\u80fd\u66f4\u5feb\u6536\u655b\u5230\u66f4\u597d\u7684\u542f\u53d1\u5f0f\uff0c\u5728\u4e0d\u540cLLM\u9aa8\u5e72\u4e0a\u5177\u6709\u826f\u597d\u6cdb\u5316\u6027\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u66f4\u5927\u89c4\u6a21\u95ee\u9898\u3002", "conclusion": "PathWise\u901a\u8fc7\u72b6\u6001\u611f\u77e5\u89c4\u5212\u548c\u591a\u667a\u80fd\u4f53\u63a8\u7406\uff0c\u5c06LLM\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u542f\u53d1\u5f0f\u8bbe\u8ba1\u4ece\u8bd5\u9519\u8fdb\u5316\u8f6c\u5411\u57fa\u4e8e\u63a8\u7406\u7684\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u542f\u53d1\u5f0f\u8bbe\u8ba1\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "topic": "agent analysis"}}
{"id": "2601.20404", "categories": ["cs.SE", "cs.AI", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.20404", "abs": "https://arxiv.org/abs/2601.20404", "authors": ["Jai Lal Lulla", "Seyedmoein Mohsenimofidi", "Matthias Galster", "Jie M. Zhang", "Sebastian Baltes", "Christoph Treude"], "title": "On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents", "comment": "5 pages, 1 figure, 1 table", "summary": "AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS$.$md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS$.$md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS$.$md is associated with a lower median runtime ($\u039428.64$%) and reduced output token consumption ($\u039416.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.", "AI": {"tldr": "\u7814\u7a76AGENTS.md\u6587\u4ef6\u5bf9AI\u7f16\u7a0b\u4ee3\u7406\u5728GitHub PR\u4e2d\u8fd0\u884c\u6548\u7387\u548ctoken\u6d88\u8017\u7684\u5f71\u54cd\uff0c\u53d1\u73b0AGENTS.md\u80fd\u663e\u8457\u964d\u4f4e\u8fd0\u884c\u65f6\u95f4\u548ctoken\u4f7f\u7528\u91cf", "motivation": "AI\u7f16\u7a0b\u4ee3\u7406\uff08\u5982Codex\u3001Claude Code\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u81ea\u4e3b\u8d21\u732e\u8f6f\u4ef6\u4ed3\u5e93\uff0c\u4f46\u4eba\u4eec\u5bf9\u4ed3\u5e93\u7ea7\u914d\u7f6e\u5de5\u4ef6\u5982\u4f55\u5f71\u54cd\u4ee3\u7406\u7684\u64cd\u4f5c\u6548\u7387\u77e5\u4e4b\u751a\u5c11", "method": "\u5206\u679010\u4e2a\u4ed3\u5e93\u548c124\u4e2aPR\uff0c\u5728\u6709\u548c\u6ca1\u6709AGENTS.md\u6587\u4ef6\u7684\u4e24\u79cd\u6761\u4ef6\u4e0b\u6267\u884c\u4ee3\u7406\uff0c\u6d4b\u91cf\u6267\u884c\u65f6\u95f4\u548ctoken\u4f7f\u7528\u91cf", "result": "AGENTS.md\u6587\u4ef6\u7684\u5b58\u5728\u4e0e\u8f83\u4f4e\u7684\u4e2d\u4f4d\u8fd0\u884c\u65f6\u95f4\uff08\u039428.64%\uff09\u548c\u51cf\u5c11\u7684\u8f93\u51fatoken\u6d88\u8017\uff08\u039416.58%\uff09\u76f8\u5173\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u4efb\u52a1\u5b8c\u6210\u884c\u4e3a", "conclusion": "\u8ba8\u8bba\u4e86AI\u7f16\u7a0b\u4ee3\u7406\u914d\u7f6e\u548c\u90e8\u7f72\u7684\u5b9e\u9645\u610f\u4e49\uff0c\u5e76\u6982\u8ff0\u4e86\u5173\u4e8e\u4ed3\u5e93\u7ea7\u6307\u4ee4\u5728\u5851\u9020AI\u7f16\u7a0b\u4ee3\u7406\u884c\u4e3a\u3001\u6548\u7387\u548c\u96c6\u6210\u65b9\u9762\u7684\u66f4\u5e7f\u6cdb\u7814\u7a76\u8bae\u7a0b", "topic": "code agent"}}
{"id": "2601.20614", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20614", "abs": "https://arxiv.org/abs/2601.20614", "authors": ["Yanqi Dai", "Yuxiang Ji", "Xiao Zhang", "Yong Wang", "Xiangxiang Chu", "Zhiwu Lu"], "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "comment": "Accepted for ICLR 2026", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "AI": {"tldr": "MathForge\u6846\u67b6\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u7ec4\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u548c\u591a\u65b9\u9762\u95ee\u9898\u91cd\u6784\u7b56\u7565\uff0c\u4ece\u7b97\u6cd5\u548c\u6570\u636e\u4e24\u4e2a\u89d2\u5ea6\u63d0\u5347\u5927\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u7279\u522b\u9488\u5bf9\u56f0\u96be\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u53ef\u9a8c\u8bc1\u5956\u52b1\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u8db3\uff1a\u7b97\u6cd5\u4e0aGRPO\u5bf9\u56f0\u96be\u95ee\u9898\u7684\u7b56\u7565\u66f4\u65b0\u5e45\u5ea6\u8f83\u4f4e\uff0c\u6570\u636e\u4e0a\u589e\u5f3a\u65b9\u6cd5\u4e3b\u8981\u91cd\u8ff0\u95ee\u9898\u800c\u975e\u7cfb\u7edf\u589e\u52a0\u5185\u5728\u96be\u5ea6\u3002", "method": "\u63d0\u51faMathForge\u6846\u67b6\uff0c\u5305\u542b\u96be\u5ea6\u611f\u77e5\u7ec4\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff08DGPO\uff09\u548c\u591a\u65b9\u9762\u95ee\u9898\u91cd\u6784\u7b56\u7565\uff08MQR\uff09\u3002DGPO\u901a\u8fc7\u96be\u5ea6\u5e73\u8861\u7ec4\u4f18\u52bf\u4f30\u8ba1\u7ea0\u6b63GRPO\u7684\u4e0d\u5e73\u8861\uff0c\u5e76\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u95ee\u9898\u7ea7\u52a0\u6743\u4f18\u5148\u5904\u7406\u56f0\u96be\u95ee\u9898\uff1bMQR\u5728\u591a\u4e2a\u65b9\u9762\u91cd\u6784\u95ee\u9898\u4ee5\u589e\u52a0\u96be\u5ea6\u540c\u65f6\u4fdd\u6301\u539f\u7b54\u6848\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cMathForge\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MathForge\u901a\u8fc7\u7b97\u6cd5\u548c\u6570\u636e\u7684\u53cc\u91cd\u6539\u8fdb\uff0c\u5f62\u6210\u534f\u540c\u5faa\u73af\uff1aMQR\u6269\u5c55\u6570\u636e\u8fb9\u754c\uff0cDGPO\u6709\u6548\u5b66\u4e60\u589e\u5f3a\u6570\u636e\uff0c\u4ece\u800c\u63d0\u5347\u5927\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u7279\u522b\u662f\u56f0\u96be\u95ee\u9898\u4e0a\u7684\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.20615", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20615", "abs": "https://arxiv.org/abs/2601.20615", "authors": ["Yanlin Wang", "Jiadong Wu", "Tianyue Jiang", "Mingwei Liu", "Jiachi Chen", "Chong Wang", "Ensheng Shi", "Xilin Liu", "Yuchi Ma", "Zibin Zheng"], "title": "DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning", "comment": "12 pages, 4 figures", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.", "AI": {"tldr": "DrainCode\u662f\u4e00\u79cd\u9488\u5bf9\u57fa\u4e8eRAG\u7684\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd2\u5316\u68c0\u7d22\u4e0a\u4e0b\u6587\u8feb\u4f7fLLM\u751f\u6210\u66f4\u957f\u8f93\u51fa\uff0c\u4ece\u800c\u663e\u8457\u589e\u52a0GPU\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46LLM\u63a8\u7406\u7684\u8ba1\u7b97\u6210\u672c\uff08\u5ef6\u8fdf\u548c\u80fd\u8017\uff09\u5728\u5b89\u5168\u9886\u57df\u5173\u6ce8\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u9488\u5bf9RAG\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u8ba1\u7b97\u6548\u7387\u7684\u5bf9\u6297\u653b\u51fb\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7a81\u53d8\u7684\u7b56\u7565\u6bd2\u5316\u68c0\u7d22\u4e0a\u4e0b\u6587\uff0c\u8feb\u4f7fLLM\u751f\u6210\u663e\u8457\u66f4\u957f\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u589e\u52a0GPU\u5ef6\u8fdf\u548c\u80fd\u8017\u3002\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\u3002", "result": "DrainCode\u5b9e\u73b0\u4e86\u9ad8\u8fbe85%\u7684\u5ef6\u8fdf\u589e\u52a0\u300149%\u7684\u80fd\u8017\u589e\u52a0\uff0c\u4ee5\u53ca\u8d85\u8fc73\u500d\u7684\u8f93\u51fa\u957f\u5ea6\u589e\u52a0\u3002\u653b\u51fb\u5728\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u4e0b\u5177\u6709\u6cdb\u5316\u6027\uff0c\u4e14\u5bf9\u591a\u79cd\u9632\u5fa1\u63aa\u65bd\u6709\u6548\u3002", "conclusion": "DrainCode\u662f\u4e00\u79cd\u589e\u52a0LLM\u8ba1\u7b97\u5f00\u9500\u7684\u6f5c\u5728\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u8bc4\u4f30LLM\u5b89\u5168\u6027\u3002", "topic": "code agent"}}
{"id": "2601.19935", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19935", "abs": "https://arxiv.org/abs/2601.19935", "authors": ["Yiting Shen", "Kun Li", "Wei Zhou", "Songlin Hu"], "title": "Mem2ActBench: A Benchmark for Evaluating Long-Term Memory Utilization in Task-Oriented Autonomous Agents", "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly deployed for complex, tool-based tasks where long-term memory is critical to driving actions. Existing benchmarks, however, primarily test a angent's ability to passively retrieve isolated facts in response to explicit questions. They fail to evaluate the more crucial capability of actively applying memory to execute tasks. To address this gap, we introduce \\textsc{Mem2ActBench}, a benchmark for evaluating whether agents can proactively leverage long-term memory to execute tool-based actions by selecting appropriate tools and grounding their parameters. The benchmark simulates persistent assistant usage, where users mention the same topic across long, interrupted interactions and expect previously established preferences and task states to be implicitly applied. We build the dataset with an automated pipeline that merges heterogeneous sources (ToolACE, BFCL, Oasst1), resolves conflicts via consistency modeling, and synthesizes 2,029 sessions with 12 user--assistant--tool turns on average. From these memory chains, a reverse-generation method produces 400 tool-use tasks, with human evaluation confirming 91.3\\% are strongly memory-dependent. Experiments on seven memory frameworks show that current systems remain inadequate at actively utilizing memory for parameter grounding, highlighting the need for more effective approaches to evaluate and improve memory application in task execution.", "AI": {"tldr": "Mem2ActBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u80fd\u5426\u4e3b\u52a8\u5229\u7528\u957f\u671f\u8bb0\u5fc6\u6267\u884c\u5de5\u5177\u64cd\u4f5c\u7684\u65b0\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u8bb0\u5fc6\u5728\u5de5\u5177\u9009\u62e9\u548c\u53c2\u6570\u5b9a\u4f4d\u4e2d\u7684\u5e94\u7528\uff0c\u800c\u975e\u88ab\u52a8\u4e8b\u5b9e\u68c0\u7d22\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u6d4b\u8bd5\u667a\u80fd\u4f53\u88ab\u52a8\u68c0\u7d22\u5b64\u7acb\u4e8b\u5b9e\u7684\u80fd\u529b\uff0c\u4f46\u672a\u80fd\u8bc4\u4f30\u66f4\u5173\u952e\u7684\u4e3b\u52a8\u5e94\u7528\u8bb0\u5fc6\u6267\u884c\u4efb\u52a1\u7684\u80fd\u529b\u3002\u9700\u8981\u8bc4\u4f30\u667a\u80fd\u4f53\u662f\u5426\u80fd\u5728\u957f\u671f\u3001\u4e2d\u65ad\u7684\u4ea4\u4e92\u4e2d\u4e3b\u52a8\u5229\u7528\u8bb0\u5fc6\u6765\u6267\u884c\u5de5\u5177\u64cd\u4f5c\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5408\u5e76\u5f02\u6784\u6570\u636e\u6e90\uff08ToolACE\u3001BFCL\u3001Oasst1\uff09\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u5efa\u6a21\u89e3\u51b3\u51b2\u7a81\uff0c\u5408\u62102,029\u4e2a\u4f1a\u8bdd\uff08\u5e73\u574712\u8f6e\u7528\u6237-\u52a9\u624b-\u5de5\u5177\u4ea4\u4e92\uff09\u3002\u4ece\u8fd9\u4e9b\u8bb0\u5fc6\u94fe\u4e2d\uff0c\u4f7f\u7528\u53cd\u5411\u751f\u6210\u65b9\u6cd5\u4ea7\u751f400\u4e2a\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u786e\u8ba491.3%\u7684\u4efb\u52a1\u5177\u6709\u5f3a\u70c8\u7684\u8bb0\u5fc6\u4f9d\u8d56\u6027\u3002\u5bf9\u4e03\u4e2a\u8bb0\u5fc6\u6846\u67b6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u7cfb\u7edf\u5728\u4e3b\u52a8\u5229\u7528\u8bb0\u5fc6\u8fdb\u884c\u53c2\u6570\u5b9a\u4f4d\u65b9\u9762\u4ecd\u7136\u4e0d\u8db3\u3002", "conclusion": "\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u8bb0\u5fc6\u5e94\u7528\u80fd\u529b\uff0c\u5f53\u524d\u7cfb\u7edf\u5728\u4e3b\u52a8\u5229\u7528\u957f\u671f\u8bb0\u5fc6\u6267\u884c\u5de5\u5177\u64cd\u4f5c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "topic": "agent analysis"}}
{"id": "2601.20662", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20662", "abs": "https://arxiv.org/abs/2601.20662", "authors": ["Julien Malka", "Arnout Engelen"], "title": "Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model", "comment": null, "summary": "Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.\n  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.\n  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies.", "AI": {"tldr": "Lila\u662f\u4e00\u4e2a\u9488\u5bf9\u529f\u80fd\u6027\u5305\u7ba1\u7406\u6a21\u578b\u7684\u53bb\u4e2d\u5fc3\u5316\u53ef\u91cd\u73b0\u6027\u8bc4\u4f30\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u62a5\u544a\u6784\u5efa\u7ed3\u679c\u5e76\u805a\u5408\u5230\u53ef\u91cd\u73b0\u6027\u6570\u636e\u5e93\u4e2d\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u8f6f\u4ef6\u53ef\u91cd\u73b0\u6027\u76d1\u63a7\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u786e\u4fdd\u8f6f\u4ef6\u6784\u5efa\u4ea7\u7269\u7684\u5b8c\u6574\u6027\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u91c7\u7528\u53ef\u91cd\u73b0\u6784\u5efa\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u5728\u5e9e\u5927\u8f6f\u4ef6\u96c6\u5408\u4e2d\u5b9e\u73b0\u9ad8\u53ef\u91cd\u73b0\u7387\uff0c\u4ee5\u53ca\u5efa\u7acb\u80fd\u591f\u5927\u89c4\u6a21\u8fd0\u884c\u7684\u53ef\u91cd\u73b0\u6027\u76d1\u63a7\u57fa\u7840\u8bbe\u65bd\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u8868\u660e\u9ad8\u53ef\u91cd\u73b0\u7387\u53ef\u4ee5\u5b9e\u73b0\uff0c\u4f46\u6709\u6548\u7684\u53ef\u91cd\u73b0\u6027\u76d1\u63a7\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e86Lila\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u529f\u80fd\u6027\u5305\u7ba1\u7406\u6a21\u578b\u7684\u53bb\u4e2d\u5fc3\u5316\u53ef\u91cd\u73b0\u6027\u8bc4\u4f30\u7cfb\u7edf\u3002\u5b83\u652f\u6301\u5206\u5e03\u5f0f\u62a5\u544a\u6784\u5efa\u7ed3\u679c\uff0c\u5e76\u5c06\u8fd9\u4e9b\u7ed3\u679c\u805a\u5408\u5230\u4e00\u4e2a\u53ef\u91cd\u73b0\u6027\u6570\u636e\u5e93\u4e2d\u3002", "result": "Lila\u7cfb\u7edf\u80fd\u591f\u4e3a\u4ece\u4e1a\u8005\u548c\u672a\u6765\u7684\u5b9e\u8bc1\u6784\u5efa\u53ef\u91cd\u73b0\u6027\u7814\u7a76\u63d0\u4f9b\u652f\u6301\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u76d1\u63a7\u673a\u5236\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u53ef\u91cd\u73b0\u6027\u8bc4\u4f30\u7684\u57fa\u7840\u8bbe\u65bd\u95ee\u9898\u3002", "conclusion": "Lila\u7cfb\u7edf\u89e3\u51b3\u4e86\u53ef\u91cd\u73b0\u6027\u76d1\u63a7\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u8f6f\u4ef6\u6784\u5efa\u5b8c\u6574\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5206\u5e03\u5f0f\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u8f6f\u4ef6\u5206\u53d1\u7684\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u5ea6\u3002", "topic": "swe application"}}
{"id": "2601.20071", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20071", "abs": "https://arxiv.org/abs/2601.20071", "authors": ["Baptiste Debes", "Tinne Tuytelaars"], "title": "Distributional value gradients for stochastic environments", "comment": null, "summary": "Gradient-regularized value learning methods improve sample efficiency by leveraging learned models of transition dynamics and rewards to estimate return gradients. However, existing approaches, such as MAGE, struggle in stochastic or noisy environments, limiting their applicability. In this work, we address these limitations by extending distributional reinforcement learning on continuous state-action spaces to model not only the distribution over scalar state-action value functions but also over their gradients. We refer to this approach as Distributional Sobolev Training. Inspired by Stochastic Value Gradients (SVG), our method utilizes a one-step world model of reward and transition distributions implemented via a conditional Variational Autoencoder (cVAE). The proposed framework is sample-based and employs Max-sliced Maximum Mean Discrepancy (MSMMD) to instantiate the distributional Bellman operator. We prove that the Sobolev-augmented Bellman operator is a contraction with a unique fixed point, and highlight a fundamental smoothness trade-off underlying contraction in gradient-aware RL. To validate our method, we first showcase its effectiveness on a simple stochastic reinforcement learning toy problem, then benchmark its performance on several MuJoCo environments.", "AI": {"tldr": "\u63d0\u51faDistributional Sobolev Training\u65b9\u6cd5\uff0c\u5728\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u5efa\u6a21\u72b6\u6001-\u52a8\u4f5c\u503c\u51fd\u6570\u53ca\u5176\u68af\u5ea6\u7684\u5206\u5e03\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u68af\u5ea6\u6b63\u5219\u5316\u65b9\u6cd5\u5728\u968f\u673a/\u566a\u58f0\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u68af\u5ea6\u6b63\u5219\u5316\u503c\u5b66\u4e60\u65b9\u6cd5\uff08\u5982MAGE\uff09\u5728\u968f\u673a\u6216\u566a\u58f0\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u66f4\u597d\u5904\u7406\u73af\u5883\u968f\u673a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u6269\u5c55\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u4e0a\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\uff0c\u4e0d\u4ec5\u5efa\u6a21\u6807\u91cf\u72b6\u6001-\u52a8\u4f5c\u503c\u51fd\u6570\u7684\u5206\u5e03\uff0c\u8fd8\u5efa\u6a21\u5176\u68af\u5ea6\u7684\u5206\u5e03\u3002\u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b9e\u73b0\u4e00\u6b65\u4e16\u754c\u6a21\u578b\uff0c\u91c7\u7528\u6700\u5927\u5207\u7247\u6700\u5927\u5747\u503c\u5dee\u5f02\u5b9e\u4f8b\u5316\u5206\u5e03\u8d1d\u5c14\u66fc\u7b97\u5b50\u3002", "result": "\u8bc1\u660e\u4e86Sobolev\u589e\u5f3a\u8d1d\u5c14\u66fc\u7b97\u5b50\u662f\u6536\u7f29\u7b97\u5b50\u4e14\u6709\u552f\u4e00\u4e0d\u52a8\u70b9\uff0c\u63ed\u793a\u4e86\u68af\u5ea6\u611f\u77e5RL\u4e2d\u5e73\u6ed1\u6027\u7684\u57fa\u672c\u6743\u8861\u3002\u5728\u7b80\u5355\u968f\u673aRL\u95ee\u9898\u548c\u591a\u4e2aMuJoCo\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "Distributional Sobolev Training\u65b9\u6cd5\u901a\u8fc7\u5efa\u6a21\u503c\u51fd\u6570\u53ca\u5176\u68af\u5ea6\u7684\u5206\u5e03\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u968f\u673a\u73af\u5883\uff0c\u63d0\u9ad8\u4e86\u68af\u5ea6\u6b63\u5219\u5316\u503c\u5b66\u4e60\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.20810", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20810", "abs": "https://arxiv.org/abs/2601.20810", "authors": ["Shahd Seddik", "Fahd Seddik", "Iman Saberi", "Fatemeh Fard", "Minh Hieu Huynh", "Patanamon Thongtanunam"], "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs", "comment": null, "summary": "Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph", "AI": {"tldr": "\u63d0\u51fa\u7f16\u7a0b\u77e5\u8bc6\u56fe\u8c31\uff08PKG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u8868\u793a\u548c\u7ec6\u7c92\u5ea6\u68c0\u7d22\u589e\u5f3a\u4ee3\u7801\u751f\u6210\uff0c\u7ed3\u5408\u6811\u526a\u679d\u548c\u91cd\u6392\u5e8f\u673a\u5236\u51cf\u5c11\u5e7b\u89c9\uff0c\u5728HumanEval\u548cMBPP\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u95ee\u9898\u65f6\u5b58\u5728\u5c40\u9650\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u901a\u8fc7\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u7ecf\u5e38\u9519\u8fc7\u76f8\u5173\u4e0a\u4e0b\u6587\uff0c\u751f\u6210\u6a21\u578b\u5219\u5bb9\u6613\u4ea7\u751f\u65e0\u5173\u6570\u636e\u7684\u5e7b\u89c9\u3002", "method": "\u63d0\u51fa\u7f16\u7a0b\u77e5\u8bc6\u56fe\u8c31\uff08PKG\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ee3\u7801\u548c\u6587\u672c\u7684\u8bed\u4e49\u8868\u793a\u548c\u7ec6\u7c92\u5ea6\u68c0\u7d22\u3002\u901a\u8fc7\u6811\u526a\u679d\u63d0\u9ad8\u68c0\u7d22\u7cbe\u5ea6\uff0c\u901a\u8fc7\u91cd\u6392\u5e8f\u673a\u5236\u6574\u5408\u975eRAG\u89e3\u51b3\u65b9\u6848\u6765\u51cf\u5c11\u5e7b\u89c9\u3002\u5c06\u5916\u90e8\u6570\u636e\u7ed3\u6784\u5316\u4e3a\u66f4\u7ec6\u7c92\u5ea6\u7684\u8282\u70b9\u4ee5\u6539\u8fdb\u68c0\u7d22\u7c92\u5ea6\u3002", "result": "\u5728HumanEval\u548cMBPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cpass@1\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad8\u8fbe20%\uff0c\u5728MBPP\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534734%\u3002PKG\u65b9\u6cd5\u7ed3\u5408\u91cd\u6392\u5e8f\u5668\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u540c\u65f6\u5bf9\u539f\u672c\u65e0\u9700RAG\u5c31\u80fd\u6b63\u786e\u89e3\u51b3\u7684\u95ee\u9898\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "\u63d0\u51fa\u7684PKG\u65b9\u6cd5\u548c\u91cd\u6392\u5e8f\u673a\u5236\u80fd\u6709\u6548\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u63d0\u9ad8\u68c0\u7d22\u7cbe\u5ea6\u5e76\u51cf\u5c11\u5e7b\u89c9\uff0c\u5728\u5904\u7406\u590d\u6742\u7f16\u7a0b\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "topic": "code agent"}}
{"id": "2601.20831", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.20831", "abs": "https://arxiv.org/abs/2601.20831", "authors": ["Vishnu Sashank Dorbala", "Dinesh Manocha"], "title": "MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents", "comment": null, "summary": "Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head \u03bcthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of \u03bc, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on \u03bc-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that \u03bc-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by \u03bc, noting the superior performance of \u03bcaugmented MLLMs on long and complex instruction types.", "AI": {"tldr": "MemCtrl\uff1a\u4e00\u79cd\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ebf\u4fee\u526a\u8bb0\u5fc6\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u8bb0\u5fc6\u5934\u95e8\u63a7\u673a\u5236\u51b3\u5b9a\u4fdd\u7559\u3001\u66f4\u65b0\u6216\u4e22\u5f03\u89c2\u5bdf\u548c\u53cd\u601d\uff0c\u663e\u8457\u63d0\u5347\u5177\u8eab\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u538b\u7f29\u548c\u68c0\u7d22\u7cfb\u7edf\uff08\u5982RAG\uff09\u901a\u5e38\u5c06\u8bb0\u5fc6\u89c6\u4e3a\u5927\u578b\u79bb\u7ebf\u5b58\u50a8\u7a7a\u95f4\uff0c\u8fd9\u4e0d\u9002\u5408\u9700\u8981\u5728\u4e25\u683c\u5185\u5b58\u548c\u8ba1\u7b97\u7ea6\u675f\u4e0b\u5728\u7ebf\u64cd\u4f5c\u7684\u5177\u8eab\u667a\u80fd\u4f53\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u7ebf\u4fee\u526a\u8bb0\u5fc6\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faMemCtrl\u6846\u67b6\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6dfb\u52a0\u53ef\u8bad\u7ec3\u7684\u8bb0\u5fc6\u5934\u03bc\uff0c\u4f5c\u4e3a\u95e8\u63a7\u673a\u5236\u51b3\u5b9a\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u3001\u66f4\u65b0\u6216\u4e22\u5f03\u54ea\u4e9b\u89c2\u5bdf\u6216\u53cd\u601d\u3002\u8bad\u7ec3\u4e24\u79cd\u7c7b\u578b\u7684\u03bc\uff1a1\uff09\u901a\u8fc7\u79bb\u7ebf\u4e13\u5bb6\u8bad\u7ec3\uff0c2\uff09\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5728EmbodiedBench\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u4e2a\u5b50\u96c6\u4e0a\uff0cMemCtrl\u589e\u5f3a\u7684\u4f4e\u6027\u80fdMLLMs\u5e73\u5747\u63d0\u5347\u7ea616%\uff0c\u7279\u5b9a\u6307\u4ee4\u5b50\u96c6\u63d0\u5347\u8d85\u8fc720%\u3002\u03bc\u589e\u5f3a\u7684MLLMs\u5728\u957f\u800c\u590d\u6742\u7684\u6307\u4ee4\u7c7b\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MemCtrl\u901a\u8fc7\u5728\u7ebf\u8bb0\u5fc6\u4fee\u526a\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5177\u8eab\u667a\u80fd\u4f53\u7684\u5185\u5b58\u7ea6\u675f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6307\u4ee4\u573a\u666f\u4e0b\u3002", "topic": "agent analysis"}}
{"id": "2601.20116", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20116", "abs": "https://arxiv.org/abs/2601.20116", "authors": ["Juncheng Dong", "Moyang Guo", "Ethan X. Fang", "Zhuoran Yang", "Vahid Tarokh"], "title": "In-Context Reinforcement Learning From Suboptimal Historical Data", "comment": "Accepted to Forty-Second International Conference on Machine Learning (ICML2025)", "summary": "Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.", "AI": {"tldr": "\u63d0\u51faDecision Importance Transformer (DIT)\u6846\u67b6\uff0c\u7528\u4e8e\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u4ef7\u503c\u51fd\u6570\u8bc4\u4f30\u6b21\u4f18\u884c\u4e3a\u7b56\u7565\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u7136\u540e\u4f7f\u7528\u52a0\u6743\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u5305\u542b\u6b21\u4f18\u5386\u53f2\u6570\u636e\u7684\u79bb\u7ebf\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60(ICRL)\u4e2d\uff0c\u5f53\u79bb\u7ebf\u6570\u636e\u96c6\u5305\u542b\u4ece\u6b21\u4f18\u884c\u4e3a\u7b56\u7565\u91c7\u6837\u7684\u8f68\u8ff9\u65f6\uff0c\u6807\u51c6\u7684\u81ea\u56de\u5f52\u8bad\u7ec3\u76f8\u5f53\u4e8e\u6a21\u4eff\u5b66\u4e60\uff0c\u4f1a\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u4ece\u6b21\u4f18\u5386\u53f2\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u6539\u8fdb\u7b56\u7565\u3002", "method": "\u63d0\u51faDIT\u6846\u67b6\uff1a1) \u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u4ef7\u503c\u51fd\u6570\u6765\u4f30\u8ba1\u884c\u4e3a\u7b56\u7565\u7684\u4f18\u52bf\u51fd\u6570\uff1b2) \u4f7f\u7528\u52a0\u6743\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u7b56\u7565\uff0c\u6743\u91cd\u57fa\u4e8e\u8bad\u7ec3\u597d\u7684\u4ef7\u503c\u51fd\u6570\u6784\u5efa\uff0c\u4ee5\u5f15\u5bfc\u6b21\u4f18\u7b56\u7565\u5411\u6700\u4f18\u7b56\u7565\u8f6c\u53d8\u3002", "result": "\u5728bandit\u548c\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u95ee\u9898\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aDIT\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u79bb\u7ebf\u6570\u636e\u96c6\u5305\u542b\u6b21\u4f18\u5386\u53f2\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "DIT\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u5305\u542b\u6b21\u4f18\u8f68\u8ff9\u7684\u79bb\u7ebf\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6a21\u62dfactor-critic\u7b97\u6cd5\u7684\u4e0a\u4e0b\u6587\u65b9\u5f0f\uff0c\u5728\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u4ece\u6b21\u4f18\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u6539\u8fdb\u7b56\u7565\u7684\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.20843", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20843", "abs": "https://arxiv.org/abs/2601.20843", "authors": ["Saurav Prateek"], "title": "Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)", "comment": "11 pages, 6 figures, 2 tables, source code: https://github.com/SauravP97/deep-researcher-reflect-evolve/", "summary": "This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runtime. This dynamic adaptation contrasts with parallel approaches, which often suffer from siloed knowledge. The Candidates Crossover algorithm further enhances search efficiency by deploying multiple LLM candidates with varied parameters to explore a larger search space, with their findings synthesized to curate a comprehensive final research response. The process concludes with One Shot Report Generation, ensuring the final document is informed by a unified narrative and high fact density. Powered by the Gemini 2.5 Pro model, our Deep Researcher was evaluated on the DeepResearch Bench, a globally recognized benchmark of 100 doctoral level research tasks. Our architecture achieved an overall score of 46.21, demonstrating superior performance by surpassing leading deep research agents such as Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher and Grok Deeper Search present on the DeepResearch Bench actively running leaderboard. This performance marginally exceeds our previous work, Static DRA, and reinforces the finding that sequential scaling consistently outperforms the parallel self consistency paradigm.", "AI": {"tldr": "\u63d0\u51faDeep Researcher\u67b6\u6784\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u7814\u7a76\u8ba1\u5212\u4f18\u5316\u548c\u5019\u9009\u4ea4\u53c9\u7b97\u6cd5\uff0c\u5728\u535a\u58eb\u7ea7\u7814\u7a76\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u7814\u7a76\u52a9\u624b\uff0c\u8bc1\u660e\u5e8f\u5217\u5316\u65b9\u6cd5\u4f18\u4e8e\u5e76\u884c\u8303\u5f0f\u3002", "motivation": "\u89e3\u51b3\u5e76\u884c\u6269\u5c55\u8303\u5f0f\u5728\u590d\u6742\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u56fa\u6709\u5c40\u9650\uff0c\u7279\u522b\u662f\u77e5\u8bc6\u5b64\u5c9b\u95ee\u9898\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u52a8\u6001\u9002\u5e94\u65b9\u6cd5\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7814\u7a76\u62a5\u544a\u3002", "method": "\u91c7\u7528\u5e8f\u5217\u5316\u7814\u7a76\u8ba1\u5212\u4f18\u5316\uff08\u901a\u8fc7\u53cd\u601d\uff09\u548c\u5019\u9009\u4ea4\u53c9\u7b97\u6cd5\uff0c\u7ed3\u5408\u5168\u5c40\u7814\u7a76\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u4e00\u6b21\u6027\u62a5\u544a\u751f\u6210\uff0c\u4f7f\u7528Gemini 2.5 Pro\u6a21\u578b\u5b9e\u73b0\u3002", "result": "\u5728DeepResearch Bench\u7684100\u4e2a\u535a\u58eb\u7ea7\u7814\u7a76\u4efb\u52a1\u4e0a\u83b7\u5f9746.21\u5206\uff0c\u8d85\u8d8aClaude Researcher\u3001Nvidia AIQ\u7b49\u73b0\u6709\u7814\u7a76\u52a9\u624b\uff0c\u8bc1\u660e\u5e8f\u5217\u5316\u65b9\u6cd5\u4f18\u4e8e\u5e76\u884c\u81ea\u4e00\u81f4\u6027\u8303\u5f0f\u3002", "conclusion": "\u5e8f\u5217\u5316\u6269\u5c55\u8303\u5f0f\u5728\u590d\u6742\u7814\u7a76\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5e76\u884c\u65b9\u6cd5\uff0c\u5168\u5c40\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u52a8\u6001\u9002\u5e94\u80fd\u529b\u662f\u5173\u952e\u4f18\u52bf\uff0c\u4e3aAI\u7814\u7a76\u52a9\u624b\u63d0\u4f9b\u4e86\u65b0\u7684\u67b6\u6784\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.20255", "categories": ["cs.LG", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20255", "abs": "https://arxiv.org/abs/2601.20255", "authors": ["Yueyang Wang", "Jiawei Fu", "Baolong Bi", "Xili Wang", "Xiaoqing Liu"], "title": "HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH", "comment": "21 pages, 15 figures", "summary": "SWE-bench has emerged as the premier benchmark for evaluating Large Language Models on complex software engineering tasks. While these capabilities are fundamentally acquired during the mid-training phase and subsequently elicited during Supervised Fine-Tuning (SFT), there remains a critical deficit in metrics capable of guiding mid-training effectively. Standard metrics such as Perplexity (PPL) are compromised by the \"Long-Context Tax\" and exhibit weak correlation with downstream SWE performance. In this paper, we bridge this gap by first introducing a rigorous data filtering strategy. Crucially, we propose the Entropy Compression Hypothesis, redefining intelligence not by scalar Top-1 compression, but by the capacity to structure uncertainty into Entropy-Compressed States of low orders (\"reasonable hesitation\"). Grounded in this fine-grained entropy analysis, we formulate a novel metric, HE-SNR (High-Entropy Signal-to-Noise Ratio). Validated on industrial-scale Mixture-of-Experts (MoE) models across varying context windows (32K/128K), our approach demonstrates superior robustness and predictive power. This work provides both the theoretical foundation and practical tools for optimizing the latent potential of LLMs in complex engineering domains.", "AI": {"tldr": "\u63d0\u51faHE-SNR\u6307\u6807\uff0c\u57fa\u4e8e\u71b5\u538b\u7f29\u5047\u8bbe\u4f18\u5316LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u4e2d\u95f4\u8bad\u7ec3\uff0c\u89e3\u51b3\u4f20\u7edf\u6307\u6807\u5728\u957f\u4e0a\u4e0b\u6587\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "SWE-bench\u4f5c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u8bc4\u4f30\u7684\u9ec4\u91d1\u6807\u51c6\uff0c\u4f46LLM\u7684\u76f8\u5173\u80fd\u529b\u4e3b\u8981\u5728\u4e2d\u95f4\u8bad\u7ec3\u9636\u6bb5\u83b7\u5f97\u3002\u5f53\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u6307\u6807\u6765\u6307\u5bfc\u4e2d\u95f4\u8bad\u7ec3\uff0c\u4f20\u7edf\u6307\u6807\u5982\u56f0\u60d1\u5ea6\u53d7\"\u957f\u4e0a\u4e0b\u6587\u7a0e\"\u5f71\u54cd\uff0c\u4e0e\u4e0b\u6e38SWE\u6027\u80fd\u76f8\u5173\u6027\u5f31\u3002", "method": "1. \u5f15\u5165\u4e25\u683c\u7684\u6570\u636e\u8fc7\u6ee4\u7b56\u7565\uff1b2. \u63d0\u51fa\u71b5\u538b\u7f29\u5047\u8bbe\uff0c\u5c06\u667a\u80fd\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5c06\u4e0d\u786e\u5b9a\u6027\u7ed3\u6784\u5316\u4e3a\u4f4e\u9636\u71b5\u538b\u7f29\u72b6\u6001\u7684\u80fd\u529b\uff1b3. \u57fa\u4e8e\u7ec6\u7c92\u5ea6\u71b5\u5206\u6790\u5236\u5b9aHE-SNR\uff08\u9ad8\u71b5\u4fe1\u566a\u6bd4\uff09\u6307\u6807\uff1b4. \u5728\u5de5\u4e1a\u7ea7MoE\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u6d4b\u8bd5\u4e0d\u540c\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0832K/128K\uff09\u3002", "result": "HE-SNR\u6307\u6807\u5728\u5de5\u4e1a\u7ea7MoE\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u9884\u6d4b\u80fd\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\u5982\u56f0\u60d1\u5ea6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4f18\u5316LLM\u5728\u590d\u6742\u5de5\u7a0b\u9886\u57df\u4e2d\u7684\u6f5c\u5728\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u901a\u8fc7HE-SNR\u6307\u6807\u89e3\u51b3\u4e86\u4e2d\u95f4\u8bad\u7ec3\u6307\u5bfc\u7684\u7a7a\u767d\u3002", "topic": "swe benchmark"}}
{"id": "2601.20055", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20055", "abs": "https://arxiv.org/abs/2601.20055", "authors": ["Vikash Singh", "Darion Cassel", "Nathaniel Weir", "Nick Feng", "Sam Bayless"], "title": "VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning", "comment": null, "summary": "Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches.", "AI": {"tldr": "VERGE\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7ed3\u5408LLM\u548cSMT\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u8fed\u4ee3\u7cbe\u70bc\u751f\u6210\u9a8c\u8bc1\u5f15\u5bfc\u7684\u7b54\u6848\uff0c\u63d0\u5347\u903b\u8f91\u6b63\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u6cd5\u4e0a\u5f88\u6d41\u7545\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u786e\u4fdd\u5176\u903b\u8f91\u6b63\u786e\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u9a8c\u8bc1\u548c\u4fdd\u8bc1LLM\u8f93\u51fa\u7684\u903b\u8f91\u4e00\u81f4\u6027\u3002", "method": "\u5c06LLM\u8f93\u51fa\u5206\u89e3\u4e3a\u539f\u5b50\u58f0\u660e\uff0c\u81ea\u52a8\u5f62\u5f0f\u5316\u4e3a\u4e00\u9636\u903b\u8f91\uff0c\u4f7f\u7528\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u9a8c\u8bc1\u903b\u8f91\u4e00\u81f4\u6027\u3002\u5f15\u5165\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u591a\u6a21\u578b\u5171\u8bc6\u901a\u8fc7\u5f62\u5f0f\u8bed\u4e49\u7b49\u4ef7\u68c0\u67e5\u786e\u4fdd\u903b\u8f91\u7ea7\u5bf9\u9f50\uff1b\u8bed\u4e49\u8def\u7531\u5c06\u4e0d\u540c\u58f0\u660e\u7c7b\u578b\u5bfc\u5411\u9002\u5f53\u7684\u9a8c\u8bc1\u7b56\u7565\uff1b\u901a\u8fc7\u6700\u5c0f\u4fee\u6b63\u5b50\u96c6\u8fdb\u884c\u7cbe\u786e\u903b\u8f91\u9519\u8bef\u5b9a\u4f4d\u3002", "result": "\u4f7f\u7528GPT-OSS-120B\u6a21\u578b\uff0cVERGE\u5728\u4e00\u7ec4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u5355\u6b21\u901a\u8fc7\u65b9\u6cd5\uff0c\u5728\u6536\u655b\u65f6\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e8618.7%\u3002", "conclusion": "\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u5728\u53ef\u80fd\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u5f62\u5f0f\u4fdd\u8bc1\uff0c\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u63d0\u4f9b\u5171\u8bc6\u9a8c\u8bc1\uff0c\u63a8\u8fdb\u53ef\u4fe1AI\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2601.20412", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20412", "abs": "https://arxiv.org/abs/2601.20412", "authors": ["Qihao Wang", "Yue Hu", "Mingzhe Lu", "Jiayue Wu", "Yanbing Liu", "Yuanmin Tang"], "title": "Beyond Accuracy: A Cognitive Load Framework for Mapping the Capability Boundaries of Tool-use Agents", "comment": "Accepted to AAAI 2026", "summary": "The ability of Large Language Models (LLMs) to use external tools unlocks powerful real-world interactions, making rigorous evaluation essential. However, current benchmarks primarily report final accuracy, revealing what models can do but obscuring the cognitive bottlenecks that define their true capability boundaries. To move from simple performance scoring to a diagnostic tool, we introduce a framework grounded in Cognitive Load Theory. Our framework deconstructs task complexity into two quantifiable components: Intrinsic Load, the inherent structural complexity of the solution path, formalized with a novel Tool Interaction Graph; and Extraneous Load, the difficulty arising from ambiguous task presentation. To enable controlled experiments, we construct ToolLoad-Bench, the first benchmark with parametrically adjustable cognitive load. Our evaluation reveals distinct performance cliffs as cognitive load increases, allowing us to precisely map each model's capability boundary. We validate that our framework's predictions are highly calibrated with empirical results, establishing a principled methodology for understanding an agent's limits and a practical foundation for building more efficient systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bca\u65adLLM\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u8ba4\u77e5\u74f6\u9888\uff0c\u901a\u8fc7ToolLoad-Bench\u57fa\u51c6\u6d4b\u8bd5\u91cf\u5316\u5185\u5728\u8d1f\u8377\u548c\u5916\u5728\u8d1f\u8377\uff0c\u7cbe\u786e\u6620\u5c04\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u3002", "motivation": "\u5f53\u524dLLM\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u51c6\u786e\u7387\uff0c\u4f46\u65e0\u6cd5\u63ed\u793a\u8ba4\u77e5\u74f6\u9888\u548c\u771f\u5b9e\u80fd\u529b\u8fb9\u754c\u3002\u9700\u8981\u4ece\u7b80\u5355\u6027\u80fd\u8bc4\u5206\u8f6c\u5411\u8bca\u65ad\u5de5\u5177\uff0c\u4ee5\u7406\u89e3\u6a21\u578b\u7684\u80fd\u529b\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\uff0c\u5c06\u4efb\u52a1\u590d\u6742\u5ea6\u5206\u89e3\u4e3a\u4e24\u4e2a\u53ef\u91cf\u5316\u90e8\u5206\uff1a\u5185\u5728\u8d1f\u8377\uff08\u4f7f\u7528\u65b0\u9896\u7684\u5de5\u5177\u4ea4\u4e92\u56fe\u5f62\u5f0f\u5316\u89e3\u51b3\u65b9\u6848\u8def\u5f84\u7684\u7ed3\u6784\u590d\u6742\u5ea6\uff09\u548c\u5916\u5728\u8d1f\u8377\uff08\u4efb\u52a1\u8868\u8ff0\u6a21\u7cca\u6027\u5e26\u6765\u7684\u96be\u5ea6\uff09\u3002\u6784\u5efaToolLoad-Bench\u57fa\u51c6\uff0c\u9996\u6b21\u5b9e\u73b0\u53c2\u6570\u5316\u53ef\u8c03\u8282\u7684\u8ba4\u77e5\u8d1f\u8377\u63a7\u5236\u5b9e\u9a8c\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u968f\u7740\u8ba4\u77e5\u8d1f\u8377\u589e\u52a0\uff0c\u6a21\u578b\u6027\u80fd\u51fa\u73b0\u660e\u663e\u65ad\u5d16\u5f0f\u4e0b\u964d\uff0c\u80fd\u591f\u7cbe\u786e\u6620\u5c04\u6bcf\u4e2a\u6a21\u578b\u7684\u80fd\u529b\u8fb9\u754c\u3002\u6846\u67b6\u7684\u9884\u6d4b\u4e0e\u5b9e\u8bc1\u7ed3\u679c\u9ad8\u5ea6\u6821\u51c6\uff0c\u4e3a\u7406\u89e3\u667a\u80fd\u4f53\u6781\u9650\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u667a\u80fd\u4f53\u80fd\u529b\u8fb9\u754c\u5efa\u7acb\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u4ece\u6027\u80fd\u8bc4\u5206\u5230\u8bca\u65ad\u5de5\u5177\u7684\u8f6c\u53d8\u3002", "topic": "agent analysis"}}
{"id": "2601.20789", "categories": ["cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20789", "abs": "https://arxiv.org/abs/2601.20789", "authors": ["Ethan Shen", "Danny Tormoen", "Saurabh Shah", "Ali Farhadi", "Tim Dettmers"], "title": "SERA: Soft-Verified Efficient Repository Agents", "comment": "21 main pages, 7 pages appendix", "summary": "Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.", "AI": {"tldr": "SERA\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u8bad\u7ec3\u4ee3\u7801\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f6f\u9a8c\u8bc1\u751f\u6210\u6280\u672f\u521b\u5efa\u6570\u5343\u4e2a\u4ee3\u7801\u5e93\u8f68\u8ff9\uff0c\u80fd\u4ee5\u4f4e\u6210\u672c\u4e13\u95e8\u5316\u5230\u79c1\u6709\u4ee3\u7801\u5e93\uff0c\u6027\u80fd\u8fbe\u5230\u5f00\u6e90\u6a21\u578bSOTA\u5e76\u5339\u914d\u524d\u6cbf\u5f00\u653e\u6743\u91cd\u6a21\u578b\u3002", "motivation": "\u5f00\u653e\u6743\u91cd\u4ee3\u7801\u4ee3\u7406\u76f8\u5bf9\u4e8e\u95ed\u6e90\u7cfb\u7edf\u5177\u6709\u7406\u8bba\u4f18\u52bf\uff1a\u53ef\u4ee5\u4e13\u95e8\u5316\u5230\u79c1\u6709\u4ee3\u7801\u5e93\uff0c\u5c06\u4ed3\u5e93\u7279\u5b9a\u4fe1\u606f\u76f4\u63a5\u7f16\u7801\u5230\u6743\u91cd\u4e2d\u3002\u4f46\u7531\u4e8e\u8bad\u7ec3\u6210\u672c\u548c\u590d\u6742\u6027\uff0c\u8fd9\u4e00\u4f18\u52bf\u4e00\u76f4\u505c\u7559\u5728\u7406\u8bba\u5c42\u9762\u3002", "method": "\u63d0\u51fa\u8f6f\u9a8c\u8bc1\u9ad8\u6548\u4ed3\u5e93\u4ee3\u7406(SERA)\u65b9\u6cd5\uff0c\u4f7f\u7528\u8f6f\u9a8c\u8bc1\u751f\u6210(SVG)\u6280\u672f\u4ece\u5355\u4e2a\u4ee3\u7801\u5e93\u751f\u6210\u6570\u5343\u4e2a\u8f68\u8ff9\uff0c\u4ec5\u901a\u8fc7\u76d1\u7763\u5fae\u8c03(SFT)\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "SERA\u5728\u5b8c\u5168\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5339\u914dDevstral-Small-2\u7b49\u524d\u6cbf\u5f00\u653e\u6743\u91cd\u6a21\u578b\u3002\u8bad\u7ec3\u6210\u672c\u6bd4\u5f3a\u5316\u5b66\u4e60\u4f4e26\u500d\uff0c\u6bd4\u5148\u524d\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4f4e57\u500d\u3002\u751f\u6210\u4e86\u8d85\u8fc720\u4e07\u4e2a\u5408\u6210\u8f68\u8ff9\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u52a0\u901f\u5f00\u653e\u4ee3\u7801\u4ee3\u7406\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u53ef\u4ee5\u4e13\u95e8\u5316\u5230\u79c1\u6709\u4ee3\u7801\u5e93\u7684\u5f00\u6e90\u6a21\u578b\u4f18\u52bf\u3002\u4f5c\u4e3aAi2\u5f00\u653e\u4ee3\u7801\u4ee3\u7406\u7cfb\u5217\u7684\u9996\u4e2a\u6a21\u578b\u53d1\u5e03\uff0c\u5305\u542b\u4ee3\u7801\u3001\u6570\u636e\u548cClaude Code\u96c6\u6210\u3002", "topic": "code agent"}}
{"id": "2601.20126", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20126", "abs": "https://arxiv.org/abs/2601.20126", "authors": ["Abha Jha", "Akanksha Mahajan", "Ashwath Vaithinathan Aravindan", "Praveen Saravanan", "Sai Sailaja Policharla", "Sonal Chaturbhuj Gehlot"], "title": "Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) often produce hallucinated or unverifiable content, undermining their reliability in factual domains. This work investigates Reinforcement Learning with Verifiable Rewards (RLVR) as a training paradigm that explicitly rewards abstention (\"I don't know\") alongside correctness to promote intellectual humility. We fine-tune and evaluate Granite-3.3-2B-Instruct and Qwen-3-4B-Instruct on the MedMCQA and Hendrycks Math benchmarks using a ternary reward structure ($-1$, r_abs, 1) under varying abstention reward structures. We further study the effect of combining RLVR with supervised fine-tuning strategies that teach abstention prior to reinforcement learning. Our results show that moderate abstention rewards (r_abs $\\approx -0.25$ to 0.3) consistently reduce incorrect responses without severe accuracy degradation on multiple-choice tasks, with larger models exhibiting greater robustness to abstention incentives. On open-ended question answering, we observe limitations due to insufficient exploration, which can be partially mitigated through supervised abstention training. Overall, these findings demonstrate the feasibility and flexibility of verifiable reward design as a practical approach for hallucination mitigation in language models. Reproducible code for our abstention training framework is available here https://github.com/Mystic-Slice/rl-abstention.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5956\u52b1\"\u6211\u4e0d\u77e5\u9053\"\u7684\u5f03\u6743\u56de\u7b54\u6765\u51cf\u5c11LLM\u7684\u5e7b\u89c9\uff0c\u5728\u591a\u9879\u9009\u62e9\u9898\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\u6216\u4e0d\u53ef\u9a8c\u8bc1\u7684\u5185\u5bb9\uff0c\u8fd9\u524a\u5f31\u4e86\u5176\u5728\u4e8b\u5b9e\u9886\u57df\u4e2d\u7684\u53ef\u9760\u6027\u3002\u9700\u8981\u4e00\u79cd\u8bad\u7ec3\u8303\u5f0f\u6765\u4fc3\u8fdb\u77e5\u8bc6\u8c26\u900a\uff0c\u660e\u786e\u5956\u52b1\u5f03\u6743\u56de\u7b54\u3002", "method": "\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u8bad\u7ec3\u8303\u5f0f\uff0c\u91c7\u7528\u4e09\u5143\u5956\u52b1\u7ed3\u6784\uff08-1\uff0cr_abs\uff0c1\uff09\uff0c\u5728MedMCQA\u548cHendrycks Math\u57fa\u51c6\u4e0a\u5bf9Granite-3.3-2B-Instruct\u548cQwen-3-4B-Instruct\u8fdb\u884c\u5fae\u8c03\u3002\u7814\u7a76\u4e0d\u540c\u5f03\u6743\u5956\u52b1\u7ed3\u6784\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u5c06RLVR\u4e0e\u76d1\u7763\u5fae\u8c03\u7b56\u7565\u7ed3\u5408\u3002", "result": "\u4e2d\u7b49\u5f03\u6743\u5956\u52b1\uff08r_abs \u2248 -0.25\u52300.3\uff09\u80fd\u6301\u7eed\u51cf\u5c11\u9519\u8bef\u56de\u7b54\uff0c\u4e14\u4e0d\u4f1a\u4e25\u91cd\u964d\u4f4e\u591a\u9879\u9009\u62e9\u9898\u7684\u51c6\u786e\u6027\u3002\u8f83\u5927\u6a21\u578b\u5bf9\u5f03\u6743\u6fc0\u52b1\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u5728\u5f00\u653e\u5f0f\u95ee\u7b54\u4e2d\uff0c\u7531\u4e8e\u63a2\u7d22\u4e0d\u8db3\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u53ef\u901a\u8fc7\u76d1\u7763\u5f03\u6743\u8bad\u7ec3\u90e8\u5206\u7f13\u89e3\u3002", "conclusion": "\u53ef\u9a8c\u8bc1\u5956\u52b1\u8bbe\u8ba1\u662f\u7f13\u89e3\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7684\u53ef\u884c\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u3002\u4e2d\u7b49\u5f03\u6743\u5956\u52b1\u80fd\u5728\u51cf\u5c11\u9519\u8bef\u56de\u7b54\u7684\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u8f83\u5927\u6a21\u578b\u5bf9\u6b64\u65b9\u6cd5\u66f4\u9c81\u68d2\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.20162", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20162", "abs": "https://arxiv.org/abs/2601.20162", "authors": ["Shuoxin Wang", "Chang Liu", "Gowen Loo", "Lifan Zheng", "Kaiwen Wei", "Xinyi Zeng", "Jingyuan Zhang", "Yu Tian"], "title": "Me-Agent: A Personalized Mobile Agent with Two-Level User Habit Learning for Enhanced Interaction", "comment": null, "summary": "Large Language Model (LLM)-based mobile agents have made significant performance advancements. However, these agents often follow explicit user instructions while overlooking personalized needs, leading to significant limitations for real users, particularly without personalized context: (1) inability to interpret ambiguous instructions, (2) lack of learning from user interaction history, and (3) failure to handle personalized instructions. To alleviate the above challenges, we propose Me-Agent, a learnable and memorable personalized mobile agent. Specifically, Me-Agent incorporates a two-level user habit learning approach. At the prompt level, we design a user preference learning strategy enhanced with a Personal Reward Model to improve personalization performance. At the memory level, we design a Hierarchical Preference Memory, which stores users' long-term memory and app-specific memory in different level memory. To validate the personalization capabilities of mobile agents, we introduce User FingerTip, a new benchmark featuring numerous ambiguous instructions for daily life. Extensive experiments on User FingerTip and general benchmarks demonstrate that Me-Agent achieves state-of-the-art performance in personalization while maintaining competitive instruction execution performance.", "AI": {"tldr": "Me-Agent\u662f\u4e00\u4e2a\u53ef\u5b66\u4e60\u3001\u53ef\u8bb0\u5fc6\u7684\u4e2a\u6027\u5316\u79fb\u52a8\u4ee3\u7406\uff0c\u901a\u8fc7\u4e24\u7ea7\u7528\u6237\u4e60\u60ef\u5b66\u4e60\u548c\u5206\u5c42\u504f\u597d\u8bb0\u5fc6\u6765\u89e3\u51b3LLM\u4ee3\u7406\u5ffd\u89c6\u4e2a\u6027\u5316\u9700\u6c42\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u79fb\u52a8\u4ee3\u7406\u901a\u5e38\u9075\u5faa\u660e\u786e\u7684\u7528\u6237\u6307\u4ee4\u800c\u5ffd\u89c6\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u5bfc\u81f4\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u65e0\u6cd5\u89e3\u91ca\u6a21\u7cca\u6307\u4ee4\u3001\u7f3a\u4e4f\u4ece\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u4e2d\u5b66\u4e60\u3001\u65e0\u6cd5\u5904\u7406\u4e2a\u6027\u5316\u6307\u4ee4\u3002", "method": "1. \u4e24\u7ea7\u7528\u6237\u4e60\u60ef\u5b66\u4e60\uff1a\u63d0\u793a\u7ea7\u4f7f\u7528\u4e2a\u4eba\u5956\u52b1\u6a21\u578b\u589e\u5f3a\u7684\u7528\u6237\u504f\u597d\u5b66\u4e60\u7b56\u7565\uff1b\u8bb0\u5fc6\u7ea7\u8bbe\u8ba1\u5206\u5c42\u504f\u597d\u8bb0\u5fc6\uff0c\u5728\u4e0d\u540c\u5c42\u7ea7\u5b58\u50a8\u7528\u6237\u957f\u671f\u8bb0\u5fc6\u548c\u5e94\u7528\u7279\u5b9a\u8bb0\u5fc6\u30022. \u5f15\u5165User FingerTip\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728User FingerTip\u548c\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMe-Agent\u5728\u4e2a\u6027\u5316\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u6307\u4ee4\u6267\u884c\u6027\u80fd\u3002", "conclusion": "Me-Agent\u901a\u8fc7\u6709\u6548\u7684\u4e2a\u6027\u5316\u673a\u5236\u89e3\u51b3\u4e86\u73b0\u6709\u79fb\u52a8\u4ee3\u7406\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u9645\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u4e2a\u6027\u5316\u4f53\u9a8c\u3002", "topic": "agent analysis"}}
{"id": "2601.20253", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20253", "abs": "https://arxiv.org/abs/2601.20253", "authors": ["Si Chen", "Le Huy Khiem", "Annalisa Szymanski", "Ronald Metoyer", "Ting Hua", "Nitesh V. Chawla"], "title": "Automated Benchmark Generation from Domain Guidelines Informed by Bloom's Taxonomy", "comment": null, "summary": "Open-ended question answering (QA) evaluates a model's ability to perform contextualized reasoning beyond factual recall. This challenge is especially acute in practice-based domains, where knowledge is procedural and grounded in professional judgment, while most existing LLM benchmarks depend on pre-existing human exam datasets that are often unavailable in such settings. We introduce a framework for automated benchmark generation from expert-authored guidelines informed by Bloom's Taxonomy. It converts expert practices into implicit violation-based scenarios and expands them into auto-graded multiple-choice questions (MCQs) and multi-turn dialogues across four cognitive levels, enabling deterministic, reproducible, and scalable evaluation. Applied to three applied domains: teaching, dietetics, and caregiving, we find differences between model and human-like reasoning: LLMs sometimes perform relatively better on higher-order reasoning (Analyze) but fail more frequently on lower-level items (Remember). We produce large-scale, psychometrically informed benchmarks that surface these non-intuitive model behaviors and enable evaluation of contextualized reasoning in real-world settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u4e13\u5bb6\u6307\u5357\u81ea\u52a8\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u5b9e\u8df5\u9886\u57df\u7684\u5f00\u653e\u5f0f\u95ee\u7b54\u80fd\u529b\uff0c\u53d1\u73b0LLM\u5728\u9ad8\u9636\u63a8\u7406\u4e0a\u8868\u73b0\u66f4\u597d\u4f46\u5728\u57fa\u7840\u8bb0\u5fc6\u4e0a\u53cd\u800c\u66f4\u5dee\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u4f9d\u8d56\u73b0\u6709\u4eba\u7c7b\u8003\u8bd5\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u5728\u5b9e\u8df5\u6027\u4e13\u4e1a\u9886\u57df\u5f80\u5f80\u4e0d\u53ef\u5f97\u3002\u5b9e\u8df5\u9886\u57df\u77e5\u8bc6\u5177\u6709\u7a0b\u5e8f\u6027\u548c\u4e13\u4e1a\u5224\u65ad\u7279\u70b9\uff0c\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u60c5\u5883\u5316\u63a8\u7406\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\uff0c\u5c06\u4e13\u5bb6\u5b9e\u8df5\u6307\u5357\u8f6c\u5316\u4e3a\u9690\u542b\u8fdd\u89c4\u573a\u666f\uff0c\u7136\u540e\u6269\u5c55\u4e3a\u81ea\u52a8\u8bc4\u5206\u7684\u591a\u9009\u9898\u548c\u591a\u8f6e\u5bf9\u8bdd\uff0c\u8986\u76d6\u56db\u4e2a\u8ba4\u77e5\u5c42\u6b21\uff0c\u5b9e\u73b0\u786e\u5b9a\u6027\u3001\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u3002", "result": "\u5e94\u7528\u4e8e\u6559\u5b66\u3001\u8425\u517b\u5b66\u548c\u62a4\u7406\u4e09\u4e2a\u9886\u57df\uff0c\u53d1\u73b0LLM\u4e0e\u4eba\u7c7b\u63a8\u7406\u5b58\u5728\u5dee\u5f02\uff1aLLM\u6709\u65f6\u5728\u9ad8\u9636\u63a8\u7406\uff08\u5206\u6790\uff09\u4e0a\u8868\u73b0\u76f8\u5bf9\u66f4\u597d\uff0c\u4f46\u5728\u4f4e\u5c42\u6b21\u9879\u76ee\uff08\u8bb0\u5fc6\uff09\u4e0a\u5931\u8d25\u66f4\u9891\u7e41\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u5927\u89c4\u6a21\u3001\u5fc3\u7406\u6d4b\u91cf\u5b66\u77e5\u60c5\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793aLLM\u7684\u975e\u76f4\u89c2\u884c\u4e3a\u6a21\u5f0f\uff0c\u652f\u6301\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8bc4\u4f30\u60c5\u5883\u5316\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.20276", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20276", "abs": "https://arxiv.org/abs/2601.20276", "authors": ["Tianwei Lin", "Zuyi Zhou", "Xinda Zhao", "Chenke Wang", "Xiaohong Li", "Yu Chen", "Chuanrui Hu", "Jian Pei", "Yafeng Deng"], "title": "Beyond the Needle's Illusion: Decoupled Evaluation of Evidence Access and Use under Semantic Interference at 326M-Token Scale", "comment": null, "summary": "Long-context LLM agents must access the right evidence from large environments and use it faithfully. However, the popular Needle-in-a-Haystack (NIAH) evaluation mostly measures benign span localization. The needle is near-unique, and the haystack is largely irrelevant. We introduce EverMemBench-S (EMB-S), an adversarial NIAH-style benchmark built on a 326M-token MemoryBank. While the full MemoryBank spans 326M tokens for retrieval-based (RAG) evaluation, we evaluate native long-context models only at scales that fit within each model's context window (up to 1M tokens in this work) to ensure a fair comparison. EMB-S pairs queries with collision-tested near-miss hard negatives and gold evidence sets spanning one or more documents, validated via human screening and LLM verification. We also propose a decoupled diagnostic protocol that reports evidence access (document-ID localization) separately from end-to-end QA quality under full-context prompting. This enables consistent diagnosis for both native long-context prompting and retrieval pipelines. Across a reference-corpus ladder from domain-isolated 64K contexts to a globally shared 326M-token environment, we observe a clear reality gap. Systems that saturate benign NIAH degrade sharply in evidence access under semantic interference. These results indicate that semantic discrimination, not context length alone, is the dominant bottleneck for long-context memory at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5bf9\u6297\u6027\u57fa\u51c6EMB-S\uff0c\u7528\u4e8e\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587LLM\u4ee3\u7406\u5728\u8bed\u4e49\u5e72\u6270\u4e0b\u7684\u8bc1\u636e\u8bbf\u95ee\u80fd\u529b\uff0c\u53d1\u73b0\u8bed\u4e49\u8fa8\u522b\u800c\u975e\u4e0a\u4e0b\u6587\u957f\u5ea6\u662f\u4e3b\u8981\u74f6\u9888", "motivation": "\u73b0\u6709NIAH\u8bc4\u4f30\u4e3b\u8981\u6d4b\u8bd5\u826f\u6027\u8de8\u5ea6\u5b9a\u4f4d\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u4ece\u5927\u91cf\u8bed\u4e49\u5e72\u6270\u4e2d\u51c6\u786e\u8bbf\u95ee\u8bc1\u636e\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u771f\u5b9e\u7684\u5bf9\u6297\u6027\u57fa\u51c6", "method": "\u6784\u5efa326M\u4ee4\u724c\u7684MemoryBank\uff0c\u521b\u5efa\u5305\u542b\u78b0\u649e\u6d4b\u8bd5\u7684\u8fd1\u8bef\u786c\u8d1f\u4f8b\u548c\u9ec4\u91d1\u8bc1\u636e\u96c6\u7684\u67e5\u8be2\uff0c\u63d0\u51fa\u5206\u79bb\u7684\u8bca\u65ad\u534f\u8bae\u5206\u522b\u8bc4\u4f30\u8bc1\u636e\u8bbf\u95ee\u548c\u7aef\u5230\u7aefQA\u8d28\u91cf", "result": "\u5728\u4ece64K\u5230326M\u4ee4\u724c\u7684\u53c2\u8003\u8bed\u6599\u9636\u68af\u4e0a\uff0c\u7cfb\u7edf\u5728\u826f\u6027NIAH\u4e2d\u9971\u548c\u4f46\u5728\u8bed\u4e49\u5e72\u6270\u4e0b\u8bc1\u636e\u8bbf\u95ee\u6025\u5267\u4e0b\u964d\uff0c\u8868\u660e\u8bed\u4e49\u8fa8\u522b\u662f\u4e3b\u8981\u74f6\u9888", "conclusion": "\u8bed\u4e49\u8fa8\u522b\u80fd\u529b\u800c\u975e\u4e0a\u4e0b\u6587\u957f\u5ea6\u662f\u957f\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u6269\u5c55\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u9700\u8981\u5bf9\u6297\u6027\u8bc4\u4f30\u6765\u771f\u5b9e\u8861\u91cfLLM\u4ee3\u7406\u7684\u8bc1\u636e\u8bbf\u95ee\u80fd\u529b", "topic": "agent analysis"}}
{"id": "2601.20193", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20193", "abs": "https://arxiv.org/abs/2601.20193", "authors": ["Zhipeng Zhang", "Wenting Ma", "Kai Li", "Meng Guo", "Lei Yang", "Wei Yu", "Hongji Cui", "Yichen Zhang", "Mo Zhang", "Jinzhe Lin", "Zhenjie Yao"], "title": "Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery", "comment": null, "summary": "Robust reinforcement learning methods typically focus on suppressing unreliable experiences or corrupted rewards, but they lack the ability to reason about the reliability of their own learning process. As a result, such methods often either overreact to noise by becoming overly conservative or fail catastrophically when uncertainty accumulates.\n  In this work, we propose a meta-cognitive reinforcement learning framework that enables an agent to assess, regulate, and recover its learning behavior based on internally estimated reliability signals. The proposed method introduces a meta-trust variable driven by Value Prediction Error Stability (VPES), which modulates learning dynamics via fail-safe regulation and gradual trust recovery.\n  Experiments on continuous-control benchmarks with reward corruption demonstrate that recovery-enabled meta-cognitive control achieves higher average returns and significantly reduces late-stage training failures compared to strong robustness baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5143\u8ba4\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u90e8\u53ef\u9760\u6027\u4fe1\u53f7\u8bc4\u4f30\u3001\u8c03\u8282\u548c\u6062\u590d\u5b66\u4e60\u884c\u4e3a\uff0c\u89e3\u51b3\u4f20\u7edf\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bf9\u566a\u58f0\u8fc7\u5ea6\u4fdd\u5b88\u6216\u707e\u96be\u6027\u5931\u8d25\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u6291\u5236\u4e0d\u53ef\u9760\u7ecf\u9a8c\u6216\u635f\u574f\u5956\u52b1\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u81ea\u8eab\u5b66\u4e60\u8fc7\u7a0b\u53ef\u9760\u6027\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u8981\u4e48\u5bf9\u566a\u58f0\u8fc7\u5ea6\u53cd\u5e94\u53d8\u5f97\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u8981\u4e48\u5728\u4e0d\u786e\u5b9a\u6027\u7d2f\u79ef\u65f6\u53d1\u751f\u707e\u96be\u6027\u5931\u8d25\u3002", "method": "\u63d0\u51fa\u5143\u8ba4\u77e5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5f15\u5165\u57fa\u4e8e\u4ef7\u503c\u9884\u6d4b\u8bef\u5dee\u7a33\u5b9a\u6027(VPES)\u9a71\u52a8\u7684\u5143\u4fe1\u4efb\u53d8\u91cf\uff0c\u901a\u8fc7\u6545\u969c\u5b89\u5168\u8c03\u8282\u548c\u6e10\u8fdb\u4fe1\u4efb\u6062\u590d\u6765\u8c03\u5236\u5b66\u4e60\u52a8\u6001\u3002", "result": "\u5728\u5177\u6709\u5956\u52b1\u635f\u574f\u7684\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u542f\u7528\u6062\u590d\u7684\u5143\u8ba4\u77e5\u63a7\u5236\u76f8\u6bd4\u5f3a\u9c81\u68d2\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5e73\u5747\u56de\u62a5\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u540e\u671f\u8bad\u7ec3\u5931\u8d25\u3002", "conclusion": "\u5143\u8ba4\u77e5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u5728\u566a\u58f0\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u5185\u90e8\u53ef\u9760\u6027\u8bc4\u4f30\u548c\u81ea\u9002\u5e94\u8c03\u8282\u673a\u5236\u9632\u6b62\u707e\u96be\u6027\u5931\u8d25\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.20312", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20312", "abs": "https://arxiv.org/abs/2601.20312", "authors": ["Kaiyuan Chen", "Guangmin Zheng", "Jin Wang", "Xiaobing Zhou", "Xuejie Zhang"], "title": "SAPO: Self-Adaptive Process Optimization Makes Small Reasoners Stronger", "comment": "Accepted by AAAI 2026", "summary": "Existing self-evolution methods overlook the influence of fine-grained reasoning steps, which leads to the reasoner-verifier gap. The computational inefficiency of Monte Carlo (MC) process supervision further exacerbates the difficulty in mitigating the gap. Motivated by the Error-Related Negativity (ERN), which the reasoner can localize error following incorrect decisions, guiding rapid adjustments, we propose a Self-Adaptive Process Optimization (SAPO) method for self-improvement in Small Language Models (SLMs). SAPO adaptively and efficiently introduces process supervision signals by actively minimizing the reasoner-verifier gap rather than relying on inefficient MC estimations. Extensive experiments demonstrate that the proposed method outperforms most existing self-evolution methods on two challenging task types: mathematics and code. Additionally, to further investigate SAPO's impact on verifier performance, this work introduces two new benchmarks for process reward models in both mathematical and coding tasks.", "AI": {"tldr": "SAPO\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u8fc7\u7a0b\u76d1\u7763\u4fe1\u53f7\u51cf\u5c11\u63a8\u7406\u5668-\u9a8c\u8bc1\u5668\u5dee\u8ddd\uff0c\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u4ee3\u7801\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u907f\u514d\u4f4e\u6548\u7684\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u5ffd\u89c6\u7ec6\u7c92\u5ea6\u63a8\u7406\u6b65\u9aa4\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u63a8\u7406\u5668-\u9a8c\u8bc1\u5668\u5dee\u8ddd\u3002\u8499\u7279\u5361\u6d1b\u8fc7\u7a0b\u76d1\u7763\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u7684\u56f0\u96be\u3002\u53d7\u9519\u8bef\u76f8\u5173\u8d1f\u6ce2\uff08ERN\uff09\u542f\u53d1\uff0c\u63a8\u7406\u5668\u80fd\u5728\u9519\u8bef\u51b3\u7b56\u540e\u5b9a\u4f4d\u9519\u8bef\u5e76\u6307\u5bfc\u5feb\u901f\u8c03\u6574\u3002", "method": "\u63d0\u51fa\u81ea\u6211\u9002\u5e94\u8fc7\u7a0b\u4f18\u5316\uff08SAPO\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4e14\u9ad8\u6548\u5730\u5f15\u5165\u8fc7\u7a0b\u76d1\u7763\u4fe1\u53f7\uff0c\u4e3b\u52a8\u6700\u5c0f\u5316\u63a8\u7406\u5668-\u9a8c\u8bc1\u5668\u5dee\u8ddd\uff0c\u800c\u975e\u4f9d\u8d56\u4f4e\u6548\u7684\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u3002", "result": "\u5728\u6570\u5b66\u548c\u4ee3\u7801\u4e24\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u7c7b\u578b\u4e0a\uff0cSAPO\u65b9\u6cd5\u4f18\u4e8e\u5927\u591a\u6570\u73b0\u6709\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u4e3a\u7814\u7a76SAPO\u5bf9\u9a8c\u8bc1\u5668\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5f15\u5165\u4e86\u6570\u5b66\u548c\u7f16\u7801\u4efb\u52a1\u7684\u4e24\u4e2a\u65b0\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u3002", "conclusion": "SAPO\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u8fc7\u7a0b\u76d1\u7763\u6709\u6548\u7f29\u5c0f\u63a8\u7406\u5668-\u9a8c\u8bc1\u5668\u5dee\u8ddd\uff0c\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e3a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2601.20327", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20327", "abs": "https://arxiv.org/abs/2601.20327", "authors": ["Xinyu Hu", "Yancheng He", "Weixun Wang", "Tao Feng", "Li Lin", "Jiashun Liu", "Wenbo Su", "Bo Zheng", "Xiaojun Wan"], "title": "CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria", "comment": "Under Review", "summary": "Automatic evaluation is crucial yet challenging for open-ended natural language generation, especially when rule-based metrics are infeasible. Compared with traditional methods, the recent LLM-as-a-Judge paradigms enable better and more flexible evaluation, and show promise as generative reward models for reinforcement learning. However, prior work has revealed a notable gap between their seemingly impressive benchmark performance and actual effectiveness in RL practice. We attribute this issue to some limitations in existing studies, including the dominance of pairwise evaluation and inadequate optimization of evaluation criteria. Therefore, we propose CE-RM-4B, a pointwise generative reward model trained with a dedicated two-stage rollout method, and adopting unified query-based criteria. Using only about 5.7K high-quality data curated from the open-source preference dataset, our CE-RM-4B achieves superior performance on diverse reward model benchmarks, especially in Best-of-N scenarios, and delivers more effective improvements in downstream RL practice.", "AI": {"tldr": "\u63d0\u51faCE-RM-4B\uff0c\u4e00\u4e2a\u57fa\u4e8e\u4e24\u9636\u6bb5rollout\u65b9\u6cd5\u548c\u7edf\u4e00\u67e5\u8be2\u6807\u51c6\u7684\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u5728\u5c11\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5728\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e0b\u6e38RL\u5b9e\u8df5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709LLM-as-a-Judge\u8bc4\u4f30\u65b9\u6cd5\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645RL\u5e94\u7528\u4e2d\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u4e3b\u8981\u7531\u4e8e\u73b0\u6709\u7814\u7a76\u4e2d\u7684\u6210\u5bf9\u8bc4\u4f30\u4e3b\u5bfc\u548c\u8bc4\u4f30\u6807\u51c6\u4f18\u5316\u4e0d\u8db3\u7b49\u9650\u5236\u3002", "method": "\u63d0\u51faCE-RM-4B\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u91c7\u7528\u4e13\u95e8\u7684\u4e24\u9636\u6bb5rollout\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u7528\u7edf\u4e00\u67e5\u8be2\u6807\u51c6\uff0c\u4ec5\u4f7f\u7528\u7ea65.7K\u4ece\u5f00\u6e90\u504f\u597d\u6570\u636e\u96c6\u4e2d\u7cbe\u9009\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "CE-RM-4B\u5728\u591a\u6837\u5316\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728Best-of-N\u573a\u666f\u4e2d\uff0c\u5e76\u5728\u4e0b\u6e38RL\u5b9e\u8df5\u4e2d\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u4e24\u9636\u6bb5rollout\u65b9\u6cd5\u548c\u7edf\u4e00\u67e5\u8be2\u6807\u51c6\u8bad\u7ec3\u7684\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u5373\u4f7f\u4f7f\u7528\u5c11\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u4e5f\u80fd\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u6548\u679c\u548cRL\u5b9e\u8df5\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.20209", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20209", "abs": "https://arxiv.org/abs/2601.20209", "authors": ["Jinyang Wu", "Shuo Yang", "Changpeng Yang", "Yuhao Shen", "Shuai Zhang", "Zhengqi Wen", "Jianhua Tao"], "title": "Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning", "comment": null, "summary": "Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \\textbf{Spark} (\\textbf{S}trategic \\textbf{P}olicy-\\textbf{A}ware explo\\textbf{R}ation via \\textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \\textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.", "AI": {"tldr": "Spark\u6846\u67b6\u901a\u8fc7\u5173\u952e\u72b6\u6001\u52a8\u6001\u5206\u652f\u5b9e\u73b0\u7b56\u7565\u611f\u77e5\u7684\u63a2\u7d22\uff0c\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u63d0\u5347\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u6548\u7387", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u901a\u5e38\u76f2\u76ee\u6269\u5927rollout\u89c4\u6a21\u5e76\u5747\u5300\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u5bfc\u81f4\u5728\u5e73\u51e1\u6b65\u9aa4\u4e0a\u6d6a\u8d39\u5927\u91cf\u8ba1\u7b97\u9884\u7b97\uff0c\u540c\u65f6\u65e0\u6cd5\u4fdd\u8bc1\u6837\u672c\u8d28\u91cf", "method": "\u63d0\u51faSpark\u6846\u67b6\uff0c\u5728\u5173\u952e\u51b3\u7b56\u72b6\u6001\u9009\u62e9\u6027\u5206\u652f\u8fdb\u884c\u8d44\u6e90\u9ad8\u6548\u63a2\u7d22\uff0c\u5229\u7528\u667a\u80fd\u4f53\u5185\u5728\u51b3\u7b56\u4fe1\u53f7\u6fc0\u6d3b\u81ea\u9002\u5e94\u5206\u652f\u63a2\u7d22\uff0c\u51cf\u5c11\u5bf9\u4eba\u7c7b\u5148\u9a8c\u7684\u4f9d\u8d56", "result": "\u5728\u591a\u6837\u5316\u4efb\u52a1\uff08\u5982\u5177\u8eab\u89c4\u5212\uff09\u4e2d\uff0cSpark\u4ee5\u663e\u8457\u66f4\u5c11\u7684\u8bad\u7ec3\u6837\u672c\u5b9e\u73b0\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u5728\u672a\u89c1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "Spark\u901a\u8fc7\u7b56\u7565\u611f\u77e5\u7684\u5173\u952e\u72b6\u6001\u5206\u652f\u63a2\u7d22\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u8d44\u6e90\u5206\u914d\uff0c\u4f18\u5148\u8003\u8651\u91c7\u6837\u8d28\u91cf\u800c\u975e\u76f2\u76ee\u8986\u76d6\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u4e3b\u6269\u5c55\u63a2\u7d22\u5e76\u5b9e\u73b0\u66f4\u5f3a\u7684\u6cdb\u5316", "topic": "agentic reinforcement learning"}}
{"id": "2601.20335", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20335", "abs": "https://arxiv.org/abs/2601.20335", "authors": ["Qinzhuo Wu", "Zhizhuo Yang", "Hanhao Li", "Pengzhi Gao", "Wei Liu", "Jian Luan"], "title": "MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment", "comment": null, "summary": "Recent advances in mobile Graphical User Interface (GUI) agents highlight the growing need for comprehensive evaluation benchmarks. While new online benchmarks offer more realistic testing than offline ones, they tend to focus on the agents' task instruction-following ability while neglecting their reasoning and exploration ability. Moreover, these benchmarks do not consider the random noise in real-world mobile environments. This leads to a gap between benchmarks and real-world environments. To addressing these limitations, we propose MobileBench-OL, an online benchmark with 1080 tasks from 80 Chinese apps. It measures task execution, complex reasoning, and noise robustness of agents by including 5 subsets, which set multiple evaluation dimensions. We also provide an auto-eval framework with a reset mechanism, enabling stable and repeatable real-world benchmarking. Evaluating 12 leading GUI agents on MobileBench-OL shows significant room for improvement to meet real-world requirements. Human evaluation further confirms that MobileBench-OL can reliably measure the performance of leading GUI agents in real environments. Our data and code will be released upon acceptance.", "AI": {"tldr": "MobileBench-OL\u662f\u4e00\u4e2a\u5305\u542b1080\u4e2a\u4efb\u52a1\u3001\u8986\u76d680\u4e2a\u4e2d\u6587\u5e94\u7528\u7684\u5728\u7ebf\u79fb\u52a8GUI\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc75\u4e2a\u5b50\u96c6\u8bc4\u4f30\u4efb\u52a1\u6267\u884c\u3001\u590d\u6742\u63a8\u7406\u548c\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5e26\u91cd\u7f6e\u673a\u5236\u7684\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u79fb\u52a8GUI\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5728\u7ebf\u57fa\u51c6\u867d\u7136\u6bd4\u79bb\u7ebf\u57fa\u51c6\u66f4\u771f\u5b9e\uff0c\u4f46\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u548c\u63a2\u7d22\u80fd\u529b\uff0c\u4e14\u672a\u8003\u8651\u771f\u5b9e\u79fb\u52a8\u73af\u5883\u4e2d\u7684\u968f\u673a\u566a\u58f0\uff0c\u5bfc\u81f4\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u73b0\u5b9e\u73af\u5883\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u63d0\u51faMobileBench-OL\u5728\u7ebf\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1080\u4e2a\u4efb\u52a1\uff0c\u8986\u76d680\u4e2a\u4e2d\u6587\u5e94\u7528\uff0c\u901a\u8fc75\u4e2a\u5b50\u96c6\u8bbe\u7f6e\u591a\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u6765\u6d4b\u91cf\u4efb\u52a1\u6267\u884c\u3001\u590d\u6742\u63a8\u7406\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002\u540c\u65f6\u63d0\u4f9b\u5e26\u91cd\u7f6e\u673a\u5236\u7684\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u73b0\u7a33\u5b9a\u53ef\u91cd\u590d\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728MobileBench-OL\u4e0a\u8bc4\u4f3012\u4e2a\u9886\u5148\u7684GUI\u4ee3\u7406\uff0c\u7ed3\u679c\u663e\u793a\u8fd9\u4e9b\u4ee3\u7406\u5728\u6ee1\u8db3\u771f\u5b9e\u4e16\u754c\u9700\u6c42\u65b9\u9762\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002\u4eba\u5de5\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9eMobileBench-OL\u80fd\u591f\u53ef\u9760\u5730\u6d4b\u91cf\u9886\u5148GUI\u4ee3\u7406\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "MobileBench-OL\u586b\u8865\u4e86\u73b0\u6709\u79fb\u52a8GUI\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u5168\u9762\u8bc4\u4f30\u4efb\u52a1\u6267\u884c\u3001\u590d\u6742\u63a8\u7406\u548c\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u4e3a\u79fb\u52a8\u4ee3\u7406\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u8d34\u8fd1\u771f\u5b9e\u73af\u5883\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "topic": "swe benchmark"}}
{"id": "2601.20439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20439", "abs": "https://arxiv.org/abs/2601.20439", "authors": ["Qihao Wang", "Mingzhe Lu", "Jiayue Wu", "Yue Hu", "Yanbing Liu"], "title": "PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use", "comment": "Accepted to PRICAI25", "summary": "Large Language Models show great potential with external tools, but face significant challenges in complex, multi-turn tool invocation. They often exhibit weak planning, tool hallucination, erroneous parameter generation, and struggle with robust interaction. To tackle these issues, we present PEARL, a novel framework to enhance LLM planning and execution for sophisticated tool use. PEARL adopts a two-stage approach: an offline phase where the agent explores tools to learn valid usage patterns and failure conditions, and an online reinforcement learning phase. In the online phase, a dedicated Planner is trained via group Relative Policy Optimization (GRPO) with a carefully designed reward function that provides distinct signals for planning quality. Experiments on the ToolHop and T-Eval benchmarks show PEARL significantly outperforms existing methods, achieving a new state-of-the-art success rate of \\textbf{56.5\\%} on ToolHop while maintaining a low invocation error rate. Our work marks a key advance in addressing the complex planning challenges of tool use, contributing to the development of more robust and reliable LLM-based agents.", "AI": {"tldr": "PEARL\u6846\u67b6\u901a\u8fc7\u79bb\u7ebf\u63a2\u7d22\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u5347LLM\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u5728ToolHop\u57fa\u51c6\u4e0a\u8fbe\u523056.5%\u7684\u65b0SOTA\u6210\u529f\u7387", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u65f6\u9762\u4e34\u590d\u6742\u591a\u8f6e\u8c03\u7528\u7684\u6311\u6218\uff0c\u5305\u62ec\u89c4\u5212\u80fd\u529b\u5f31\u3001\u5de5\u5177\u5e7b\u89c9\u3001\u53c2\u6570\u751f\u6210\u9519\u8bef\u548c\u4ea4\u4e92\u9c81\u68d2\u6027\u5dee\u7b49\u95ee\u9898", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u79bb\u7ebf\u9636\u6bb5\u63a2\u7d22\u5de5\u5177\u5b66\u4e60\u6709\u6548\u4f7f\u7528\u6a21\u5f0f\u548c\u5931\u8d25\u6761\u4ef6\uff1b\u5728\u7ebf\u9636\u6bb5\u901a\u8fc7GRPO\u8bad\u7ec3\u4e13\u7528\u89c4\u5212\u5668\uff0c\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u63d0\u4f9b\u89c4\u5212\u8d28\u91cf\u4fe1\u53f7", "result": "\u5728ToolHop\u548cT-Eval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728ToolHop\u4e0a\u8fbe\u523056.5%\u7684\u65b0SOTA\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u8c03\u7528\u9519\u8bef\u7387", "conclusion": "PEARL\u6846\u67b6\u5728\u89e3\u51b3\u5de5\u5177\u4f7f\u7528\u7684\u590d\u6742\u89c4\u5212\u6311\u6218\u65b9\u9762\u53d6\u5f97\u5173\u952e\u8fdb\u5c55\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u9c81\u68d2\u53ef\u9760\u7684\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53", "topic": "agent analysis"}}
{"id": "2601.20257", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2601.20257", "abs": "https://arxiv.org/abs/2601.20257", "authors": ["Jinren Ding", "Xuejian Xu", "Shen Jiang", "Zhitong Hao", "Jinhui Yang", "Peng Jiang"], "title": "C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding", "comment": null, "summary": "Decision Transformer (DT) shows promise for generative auto-bidding by capturing temporal dependencies, but suffers from two critical limitations: insufficient cross-correlation modeling among state, action, and return-to-go (RTG) sequences, and indiscriminate learning of optimal/suboptimal behaviors. To address these, we propose C2, a novel framework enhancing DT with two core innovations: (1) a Cross Learning Block (CLB) via cross-attention to strengthen inter-sequence correlation modeling; (2) a Constraint-aware Loss (CL) incorporating budget and Cost-Per-Acquisition (CPA) constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains (up to 3.23\\% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify the complementary synergy of CLB and CL, confirming C2's superiority in auto-bidding. The code for reproducing our results is available at: https://github.com/Dingjinren/C2.", "AI": {"tldr": "C2\u6846\u67b6\u901a\u8fc7\u4ea4\u53c9\u5b66\u4e60\u5757\u548c\u7ea6\u675f\u611f\u77e5\u635f\u5931\u589e\u5f3a\u51b3\u7b56\u53d8\u6362\u5668\uff0c\u89e3\u51b3\u81ea\u52a8\u51fa\u4ef7\u4e2d\u8de8\u5e8f\u5217\u76f8\u5173\u6027\u5efa\u6a21\u4e0d\u8db3\u548c\u6b21\u4f18\u884c\u4e3a\u5b66\u4e60\u95ee\u9898\uff0c\u5728AuctionNet\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51b3\u7b56\u53d8\u6362\u5668\u5728\u81ea\u52a8\u51fa\u4ef7\u4e2d\u867d\u7136\u80fd\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u72b6\u6001\u3001\u52a8\u4f5c\u548c\u56de\u62a5\u5e8f\u5217\u4e4b\u95f4\u7684\u8de8\u76f8\u5173\u6027\u5efa\u6a21\u4e0d\u8db3\uff0c\u4ee5\u53ca\u65e0\u6cd5\u533a\u5206\u5730\u5b66\u4e60\u6700\u4f18/\u6b21\u4f18\u884c\u4e3a\u3002", "method": "\u63d0\u51faC2\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u4ea4\u53c9\u5b66\u4e60\u5757\u589e\u5f3a\u5e8f\u5217\u95f4\u76f8\u5173\u6027\u5efa\u6a21\uff1b2) \u7ed3\u5408\u9884\u7b97\u548cCPA\u7ea6\u675f\u7684\u7ea6\u675f\u611f\u77e5\u635f\u5931\uff0c\u9009\u62e9\u6027\u5b66\u4e60\u6700\u4f18\u8f68\u8ff9\u3002", "result": "\u5728AuctionNet\u6570\u636e\u96c6\u4e0a\u7684\u79bb\u7ebf\u8bc4\u4f30\u663e\u793a\uff0cC2\u5728\u4e0d\u540c\u9884\u7b97\u8bbe\u7f6e\u4e0b\u5747\u53d6\u5f97\u6027\u80fd\u63d0\u5347\uff08\u6700\u9ad8\u6bd4\u6700\u5148\u8fdb\u7684GAVE\u65b9\u6cd5\u63d0\u53473.23%\uff09\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86CLB\u548cCL\u7684\u4e92\u8865\u534f\u540c\u4f5c\u7528\u3002", "conclusion": "C2\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u8de8\u5e8f\u5217\u76f8\u5173\u6027\u5efa\u6a21\u548c\u9009\u62e9\u6027\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u51fa\u4ef7\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u81ea\u52a8\u51fa\u4ef7\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.20465", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20465", "abs": "https://arxiv.org/abs/2601.20465", "authors": ["Yang Li", "Jiaxiang Liu", "Yusong Wang", "Yujie Wu", "Mingkun Xu"], "title": "BMAM: Brain-inspired Multi-Agent Memory Framework", "comment": "Submitted to ACL (ARR 2026 January submission); non-anonymous preprint", "summary": "Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.", "AI": {"tldr": "BMAM\u662f\u4e00\u79cd\u53d7\u5927\u8111\u542f\u53d1\u7684\u591a\u667a\u80fd\u4f53\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7\u529f\u80fd\u4e13\u95e8\u5316\u7684\u5b50\u7cfb\u7edf\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\"\u7075\u9b42\u4fb5\u8680\"\u95ee\u9898\uff0c\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523078.45%\u51c6\u786e\u7387\u3002", "motivation": "\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u9762\u4e34\u4fdd\u6301\u65f6\u95f4\u57fa\u7840\u4fe1\u606f\u548c\u884c\u4e3a\u4e00\u81f4\u6027\u7684\u6311\u6218\uff0c\u4f5c\u8005\u79f0\u4e4b\u4e3a\"\u7075\u9b42\u4fb5\u8680\"\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5355\u4e00\u975e\u7ed3\u6784\u5316\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u65f6\u95f4\u5c3a\u5ea6\u548c\u8bb0\u5fc6\u529f\u80fd\u5dee\u5f02\u3002", "method": "\u63d0\u51faBMAM\u67b6\u6784\uff0c\u53d7\u8ba4\u77e5\u8bb0\u5fc6\u7cfb\u7edf\u542f\u53d1\uff0c\u5c06\u8bb0\u5fc6\u5206\u89e3\u4e3a\u56db\u4e2a\u529f\u80fd\u4e13\u95e8\u5316\u7684\u5b50\u7cfb\u7edf\uff1a\u60c5\u666f\u8bb0\u5fc6\u3001\u8bed\u4e49\u8bb0\u5fc6\u3001\u663e\u8457\u6027\u611f\u77e5\u8bb0\u5fc6\u548c\u63a7\u5236\u5bfc\u5411\u8bb0\u5fc6\uff0c\u8fd9\u4e9b\u7ec4\u4ef6\u5728\u4e92\u8865\u7684\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8fd0\u4f5c\u3002\u4e3a\u652f\u6301\u957f\u671f\u63a8\u7406\uff0cBMAM\u6cbf\u7740\u660e\u786e\u7684\u65f6\u95f4\u7ebf\u7ec4\u7ec7\u60c5\u666f\u8bb0\u5fc6\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u591a\u4e2a\u4e92\u8865\u4fe1\u53f7\u6765\u68c0\u7d22\u8bc1\u636e\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBMAM\u5728\u6807\u51c6\u957f\u671f\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\u8fbe\u523078.45%\u7684\u51c6\u786e\u7387\u3002\u6d88\u878d\u5206\u6790\u8bc1\u5b9e\uff0c\u53d7\u6d77\u9a6c\u4f53\u542f\u53d1\u7684\u7684\u60c5\u666f\u8bb0\u5fc6\u5b50\u7cfb\u7edf\u5728\u65f6\u95f4\u63a8\u7406\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "BMAM\u901a\u8fc7\u529f\u80fd\u4e13\u95e8\u5316\u7684\u591a\u667a\u80fd\u4f53\u8bb0\u5fc6\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u7684\"\u7075\u9b42\u4fb5\u8680\"\u95ee\u9898\uff0c\u4e3a\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8bb0\u5fc6\u4fdd\u6301\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.20546", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20546", "abs": "https://arxiv.org/abs/2601.20546", "authors": ["Kumiko Nakajima", "Jan Zuiderveld", "Sandro Pezzelle"], "title": "Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models", "comment": "Accepted to Findings of EACL 2026", "summary": "Large language models (LLMs) are increasingly used in verbal creative tasks. However, previous assessments of the creative capabilities of LLMs remain weakly grounded in human creativity theory and are thus hard to interpret. The widely used Divergent Association Task (DAT) focuses on novelty, ignoring appropriateness, a core component of creativity. We evaluate a range of state-of-the-art LLMs on DAT and show that their scores on the task are lower than those of two baselines that do not possess any creative abilities, undermining its validity for model evaluation. Grounded in human creativity theory, which defines creativity as the combination of novelty and appropriateness, we introduce Conditional Divergent Association Task (CDAT). CDAT evaluates novelty conditional on contextual appropriateness, separating noise from creativity better than DAT, while remaining simple and objective. Under CDAT, smaller model families often show the most creativity, whereas advanced families favor appropriateness at lower novelty. We hypothesize that training and alignment likely shift models along this frontier, making outputs more appropriate but less creative. We release the dataset and code.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faConditional Divergent Association Task (CDAT)\uff0c\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u521b\u9020\u529b\u7406\u8bba\uff08\u65b0\u9896\u6027+\u9002\u5f53\u6027\uff09\u7684LLM\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u53d1\u73b0\u4f20\u7edfDAT\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff0c\u800cCDAT\u80fd\u66f4\u597d\u533a\u5206\u566a\u58f0\u4e0e\u521b\u9020\u529b\u3002", "motivation": "\u73b0\u6709LLM\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982DAT\uff09\u7f3a\u4e4f\u4eba\u7c7b\u521b\u9020\u529b\u7406\u8bba\u7684\u57fa\u7840\uff0c\u53ea\u5173\u6ce8\u65b0\u9896\u6027\u800c\u5ffd\u7565\u9002\u5f53\u6027\u8fd9\u4e00\u6838\u5fc3\u8981\u7d20\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u96be\u4ee5\u89e3\u91ca\u4e14\u6709\u6548\u6027\u5b58\u7591\u3002", "method": "\u57fa\u4e8e\u4eba\u7c7b\u521b\u9020\u529b\u7406\u8bba\uff08\u521b\u9020\u529b=\u65b0\u9896\u6027+\u9002\u5f53\u6027\uff09\uff0c\u63d0\u51faConditional Divergent Association Task (CDAT)\uff0c\u5728\u8003\u8651\u4e0a\u4e0b\u6587\u9002\u5f53\u6027\u7684\u6761\u4ef6\u4e0b\u8bc4\u4f30\u65b0\u9896\u6027\u3002\u4f7f\u7528CDAT\u8bc4\u4f30\u591a\u79cd\u6700\u5148\u8fdbLLM\uff0c\u5e76\u4e0e\u4f20\u7edfDAT\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "1) DAT\u8bc4\u4f30\u4e0bLLM\u5f97\u5206\u4f4e\u4e8e\u65e0\u521b\u9020\u529b\u57fa\u7ebf\uff0c\u8d28\u7591\u5176\u6709\u6548\u6027\uff1b2) CDAT\u80fd\u66f4\u597d\u533a\u5206\u566a\u58f0\u4e0e\u521b\u9020\u529b\uff1b3) \u8f83\u5c0f\u6a21\u578b\u5bb6\u65cf\u5e38\u8868\u73b0\u6700\u9ad8\u521b\u9020\u529b\uff0c\u800c\u5148\u8fdb\u6a21\u578b\u5bb6\u65cf\u503e\u5411\u4e8e\u9002\u5f53\u6027\u4f46\u65b0\u9896\u6027\u8f83\u4f4e\uff1b4) \u8bad\u7ec3\u548c\u5bf9\u9f50\u53ef\u80fd\u4f7f\u6a21\u578b\u5728\u521b\u9020\u529b\u8fb9\u754c\u4e0a\u5411\u9002\u5f53\u6027\u504f\u79fb\u3002", "conclusion": "\u9700\u8981\u57fa\u4e8e\u4eba\u7c7b\u521b\u9020\u529b\u7406\u8bba\uff08\u65b0\u9896\u6027+\u9002\u5f53\u6027\uff09\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0cCDAT\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u5ba2\u89c2\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6a21\u578b\u8bad\u7ec3\u548c\u5bf9\u9f50\u53ef\u80fd\u4ee5\u727a\u7272\u521b\u9020\u529b\u4e3a\u4ee3\u4ef7\u63d0\u9ad8\u9002\u5f53\u6027\uff0c\u8fd9\u5bf9LLM\u521b\u9020\u529b\u53d1\u5c55\u6709\u91cd\u8981\u542f\u793a\u3002", "topic": "agent analysis"}}
{"id": "2601.20299", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2601.20299", "abs": "https://arxiv.org/abs/2601.20299", "authors": ["Tianyi Alex Qiu", "Micah Carroll", "Cameron Allen"], "title": "Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction", "comment": "ICLR 2026", "summary": "The evaluation and post-training of large language models (LLMs) rely on supervision, but strong supervision for difficult tasks is often unavailable, especially when evaluating frontier models. In such cases, models are demonstrated to exploit evaluations built on such imperfect supervision, leading to deceptive results. However, underutilized in LLM research, a wealth of mechanism design research focuses on game-theoretic incentive compatibility, i.e., eliciting honest and informative answers with weak supervision. Drawing from this literature, we introduce the peer prediction method for model evaluation and post-training. It rewards honest and informative answers over deceptive and uninformative ones, using a metric based on mutual predictability and without requiring ground truth labels. We demonstrate the method's effectiveness and resistance to deception, with both theoretical guarantees and empirical validation on models with up to 405B parameters. We show that training an 8B model with peer prediction-based reward recovers most of the drop in truthfulness due to prior malicious finetuning, even when the reward is produced by a 0.135B language model with no finetuning. On the evaluation front, in contrast to LLM-as-a-Judge which requires strong and trusted judges, we discover an inverse scaling property in peer prediction, where, surprisingly, resistance to deception is strengthened as the capability gap between the experts and participants widens, enabling reliable evaluation of strong models with weak supervision. In particular, LLM-as-a-Judge become worse than random guess when facing deceptive models 5-20x the judge's size, while peer prediction thrives when such gaps are large, including in cases with over 100x size difference.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u540c\u4f34\u9884\u6d4b\uff08peer prediction\uff09\u7684LLM\u8bc4\u4f30\u4e0e\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u535a\u5f08\u8bba\u6fc0\u52b1\u517c\u5bb9\u6027\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\u5956\u52b1\u8bda\u5b9e\u4fe1\u606f\u6027\u56de\u7b54\uff0c\u6709\u6548\u62b5\u6297\u6b3a\u9a97\u884c\u4e3a", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u548c\u540e\u8bad\u7ec3\u4f9d\u8d56\u76d1\u7763\u4fe1\u53f7\uff0c\u4f46\u5bf9\u4e8e\u56f0\u96be\u4efb\u52a1\u5f80\u5f80\u7f3a\u4e4f\u5f3a\u76d1\u7763\uff0c\u5bfc\u81f4\u6a21\u578b\u53ef\u80fd\u5229\u7528\u4e0d\u5b8c\u7f8e\u76d1\u7763\u8fdb\u884c\u6b3a\u9a97\uff0c\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u5f31\u76d1\u7763\u4e0b\u53ef\u9760\u8bc4\u4f30\u548c\u8bad\u7ec3\u7684\u65b9\u6cd5", "method": "\u501f\u9274\u535a\u5f08\u8bba\u4e2d\u7684\u673a\u5236\u8bbe\u8ba1\u7814\u7a76\uff0c\u5f15\u5165\u540c\u4f34\u9884\u6d4b\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u76f8\u4e92\u53ef\u9884\u6d4b\u6027\u5ea6\u91cf\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\uff0c\u901a\u8fc7\u5956\u52b1\u8bda\u5b9e\u548c\u4fe1\u606f\u6027\u56de\u7b54\u6765\u5bf9\u6297\u6b3a\u9a97\u6027\u548c\u65e0\u4fe1\u606f\u6027\u56de\u7b54\u3002\u5305\u542b\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u9a8c\u8bc1", "result": "\u65b9\u6cd5\u6709\u6548\u62b5\u6297\u6b3a\u9a97\uff1a1\uff09\u4f7f\u7528\u540c\u4f34\u9884\u6d4b\u5956\u52b1\u8bad\u7ec3\u76848B\u6a21\u578b\u80fd\u6062\u590d\u56e0\u6076\u610f\u5fae\u8c03\u800c\u4e0b\u964d\u7684\u8bda\u5b9e\u6027\uff1b2\uff09\u53d1\u73b0\u540c\u4f34\u9884\u6d4b\u5b58\u5728\u9006\u7f29\u653e\u7279\u6027\uff0c\u4e13\u5bb6\u4e0e\u53c2\u4e0e\u8005\u80fd\u529b\u5dee\u8ddd\u8d8a\u5927\uff0c\u62b5\u6297\u6b3a\u9a97\u80fd\u529b\u8d8a\u5f3a\uff1b3\uff09LLM-as-a-Judge\u5728\u9762\u5bf95-20\u500d\u5927\u5c0f\u7684\u6b3a\u9a97\u6a21\u578b\u65f6\u8868\u73b0\u5dee\u4e8e\u968f\u673a\u731c\u6d4b\uff0c\u800c\u540c\u4f34\u9884\u6d4b\u5728100\u500d\u5927\u5c0f\u5dee\u8ddd\u4e0b\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c", "conclusion": "\u540c\u4f34\u9884\u6d4b\u65b9\u6cd5\u4e3aLLM\u8bc4\u4f30\u548c\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5f3a\u76d1\u7763\u7684\u53ef\u9760\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8bc4\u4f30\u524d\u6cbf\u6a21\u578b\u3002\u5176\u9006\u7f29\u653e\u7279\u6027\u4f7f\u5f97\u5f31\u76d1\u7763\u4e5f\u80fd\u53ef\u9760\u8bc4\u4f30\u5f3a\u6a21\u578b\uff0c\u4e3a\u89e3\u51b3LLM\u8bc4\u4f30\u4e2d\u7684\u6b3a\u9a97\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84", "topic": "agent analysis"}}
{"id": "2601.20613", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20613", "abs": "https://arxiv.org/abs/2601.20613", "authors": ["Kaiyuan Chen", "Qimin Wu", "Taiyu Hou", "Tianhao Tang", "Xueyu Hu", "Yuchen Hou", "Bikun Li", "Chengming Qian", "Guoyin Wang", "Haolin Chen", "Haotong Tian", "Haoye Zhang", "Haoyu Bian", "Hongbing Pan", "Hongkang Zhang", "Hongyi Zhou", "Jiaqi Cai", "Jiewu Rao", "Jiyuan Ren", "Keduan Huang", "Lucia Zhu Huang", "Mingyu Yuan", "Naixu Guo", "Qicheng Tang", "Qinyan Zhang", "Shuai Chen", "Siheng Chen", "Ting Ting Li", "Xiaoxing Guo", "Yaocheng Zuo", "Yaoqi Guo", "Yinan Wang", "Yinzhou Yu", "Yize Wang", "Yuan Jiang", "Yuan Tian", "Yuanshuo Zhang", "Yuxuan Liu", "Yvette Yan Zeng", "Zenyu Shan", "Zihan Yin", "Xiaobo Hu", "Yang Liu", "Yixin Ren", "Yuan Gong"], "title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios", "comment": "17 pages, 8 figures", "summary": "The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAgentIF-OneDay\u57fa\u51c6\uff0c\u8bc4\u4f30AI\u4ee3\u7406\u80fd\u5426\u5b8c\u6210\u591a\u6837\u5316\u7684\u65e5\u5e38\u4efb\u52a1\uff0c\u5305\u62ec\u5de5\u4f5c\u6d41\u6267\u884c\u3001\u9690\u542b\u6307\u4ee4\u7406\u89e3\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u53d1\u73b0\u57fa\u4e8eAPI\u6784\u5efa\u7684\u4ee3\u7406\u4ea7\u54c1\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u8bc4\u4f30\u8fc7\u4e8e\u5173\u6ce8\u4efb\u52a1\u96be\u5ea6\uff0c\u800c\u5ffd\u7565\u4e86\u65e5\u5e38\u573a\u666f\u7684\u591a\u6837\u6027\u9700\u6c42\u3002\u7528\u6237\u5bf9AI\u9ad8\u7ea7\u80fd\u529b\u7684\u611f\u77e5\u6709\u9650\uff0c\u9700\u8981\u66f4\u8d34\u8fd1\u65e5\u5e38\u5de5\u4f5c\u548c\u751f\u6d3b\u573a\u666f\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faAgentIF-OneDay\u57fa\u51c6\uff0c\u5305\u542b104\u4e2a\u4efb\u52a1\u3001767\u4e2a\u8bc4\u5206\u70b9\uff0c\u5206\u4e3a\u4e09\u7c7b\uff1a\u5f00\u653e\u5de5\u4f5c\u6d41\u6267\u884c\u3001\u9690\u542b\u6307\u4ee4\u7406\u89e3\u3001\u8fed\u4ee3\u4f18\u5316\u3002\u91c7\u7528\u5b9e\u4f8b\u7ea7\u8bc4\u5206\u6807\u51c6\u548c\u6539\u8fdb\u7684\u8bc4\u4f30\u6d41\u7a0b\uff0c\u7ed3\u5408LLM\u9a8c\u8bc1\u4e0e\u4eba\u5de5\u5224\u65ad\u3002", "result": "\u4f7f\u7528Gemini-3-Pro\u9a8c\u8bc1\uff0cLLM\u9a8c\u8bc1\u4e0e\u4eba\u5de5\u5224\u65ad\u7684\u4e00\u81f4\u6027\u8fbe\u523080.1%\u3002\u8bc4\u4f30\u56db\u4e2a\u9886\u5148\u7684\u901a\u7528AI\u4ee3\u7406\uff0c\u53d1\u73b0\u57fa\u4e8eAPI\u6784\u5efa\u7684\u4ee3\u7406\u4ea7\u54c1\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684ChatGPT\u4ee3\u7406\u5904\u4e8e\u7b2c\u4e00\u68af\u961f\u3002", "conclusion": "\u9886\u5148\u7684LLM API\u548c\u5f00\u6e90\u6a21\u578b\u5df2\u5185\u5316\u4ee3\u7406\u80fd\u529b\uff0c\u4f7fAI\u5e94\u7528\u56e2\u961f\u80fd\u591f\u5f00\u53d1\u524d\u6cbf\u7684\u4ee3\u7406\u4ea7\u54c1\u3002\u65e5\u5e38\u4efb\u52a1\u8bc4\u4f30\u5bf9\u63a8\u52a8AI\u4ee3\u7406\u7684\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2601.20649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20649", "abs": "https://arxiv.org/abs/2601.20649", "authors": ["Wenlin Zhong", "Chengyuan Liu", "Yiquan Wu", "Bovin Tan", "Changlong Sun", "Yi Wang", "Xiaozhong Liu", "Kun Kuang"], "title": "P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering", "comment": null, "summary": "While reinforcement learning with verifiable rewards (RLVR) has advanced LLM reasoning in structured domains like mathematics and programming, its application to general-domain reasoning tasks remains challenging due to the absence of verifiable reward signals. To this end, methods like Reinforcement Learning with Reference Probability Reward (RLPR) have emerged, leveraging the probability of generating the final answer as a reward signal. However, these outcome-focused approaches neglect crucial step-by-step supervision of the reasoning process itself. To address this gap, we introduce Probabilistic Process Supervision (P2S), a novel self-supervision framework that provides fine-grained process rewards without requiring a separate reward model or human-annotated reasoning steps. During reinforcement learning, P2S synthesizes and filters a high-quality reference reasoning chain (gold-CoT). The core of our method is to calculate a Path Faithfulness Reward (PFR) for each reasoning step, which is derived from the conditional probability of generating the gold-CoT's suffix, given the model's current reasoning prefix. Crucially, this PFR can be flexibly integrated with any outcome-based reward, directly tackling the reward sparsity problem by providing dense guidance. Extensive experiments on reading comprehension and medical Question Answering benchmarks show that P2S significantly outperforms strong baselines.", "AI": {"tldr": "P2S\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf\u53c2\u8003\u63a8\u7406\u94fe\uff0c\u4e3a\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u8ba1\u7b97\u8def\u5f84\u5fe0\u5b9e\u5ea6\u5956\u52b1\uff0c\u89e3\u51b3\u901a\u7528\u9886\u57df\u63a8\u7406\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u5956\u52b1\u4fe1\u53f7\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5728\u7ed3\u6784\u5316\u9886\u57df\u6709\u6548\uff0c\u4f46\u5728\u901a\u7528\u9886\u57df\u63a8\u7406\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u5956\u52b1\u4fe1\u53f7\u3002RLPR\u7b49\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6982\u7387\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u9010\u6b65\u76d1\u7763\uff0c\u5b58\u5728\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6982\u7387\u8fc7\u7a0b\u76d1\u7763(P2S)\u6846\u67b6\uff1a1)\u5728\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5408\u6210\u548c\u8fc7\u6ee4\u9ad8\u8d28\u91cf\u53c2\u8003\u63a8\u7406\u94fe(gold-CoT)\uff1b2)\u4e3a\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u8ba1\u7b97\u8def\u5f84\u5fe0\u5b9e\u5ea6\u5956\u52b1(PFR)\uff0c\u57fa\u4e8e\u5f53\u524d\u63a8\u7406\u524d\u7f00\u751f\u6210gold-CoT\u540e\u7f00\u7684\u6761\u4ef6\u6982\u7387\uff1b3)PFR\u53ef\u4e0e\u4efb\u4f55\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u7075\u6d3b\u7ed3\u5408\uff0c\u63d0\u4f9b\u5bc6\u96c6\u6307\u5bfc\u3002", "result": "\u5728\u9605\u8bfb\u7406\u89e3\u548c\u533b\u7597\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cP2S\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898\u3002", "conclusion": "P2S\u901a\u8fc7\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u8fc7\u7a0b\u5956\u52b1\uff0c\u65e0\u9700\u5355\u72ec\u5956\u52b1\u6a21\u578b\u6216\u4eba\u5de5\u6807\u6ce8\u63a8\u7406\u6b65\u9aa4\uff0c\u6210\u529f\u5c06\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u5230\u901a\u7528\u9886\u57df\u63a8\u7406\u4efb\u52a1\uff0c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u76d1\u7763\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.20676", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20676", "abs": "https://arxiv.org/abs/2601.20676", "authors": ["Zhuo Chen", "Xinyu Geng", "Xinyu Wang", "Yong Jiang", "Zhen Zhang", "Pengjun Xie", "Kewei Tu"], "title": "Efficient Multimodal Planning Agent for Visual Question-Answering", "comment": null, "summary": "Visual Question-Answering (VQA) is a challenging multimodal task that requires integrating visual and textual information to generate accurate responses. While multimodal Retrieval-Augmented Generation (mRAG) has shown promise in enhancing VQA systems by providing more evidence on both image and text sides, the default procedure that addresses VQA queries, especially the knowledge-intensive ones, often relies on multi-stage pipelines of mRAG with inherent dependencies. To mitigate the inefficiency limitations while maintaining VQA task performance, this paper proposes a method that trains a multimodal planning agent, dynamically decomposing the mRAG pipeline to solve the VQA task. Our method optimizes the trade-off between efficiency and effectiveness by training the agent to intelligently determine the necessity of each mRAG step. In our experiments, the agent can help reduce redundant computations, cutting search time by over 60\\% compared to existing methods and decreasing costly tool calls. Meanwhile, experiments demonstrate that our method outperforms all baselines, including a Deep Research agent and a carefully designed prompt-based method, on average over six various datasets. Code will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u591a\u6a21\u6001\u89c4\u5212\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u52a8\u6001\u5206\u89e3mRAG\u6d41\u7a0b\u6765\u89e3\u51b3VQA\u4efb\u52a1\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u6548\u7387", "motivation": "\u5f53\u524dVQA\u4efb\u52a1\u4e2d\uff0c\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210(mRAG)\u901a\u5e38\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u5b58\u5728\u56fa\u6709\u4f9d\u8d56\u5173\u7cfb\u548c\u6548\u7387\u9650\u5236\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u591a\u6a21\u6001\u89c4\u5212\u4ee3\u7406\uff0c\u52a8\u6001\u5206\u89e3mRAG\u6d41\u6c34\u7ebf\uff0c\u667a\u80fd\u51b3\u5b9a\u6bcf\u4e2amRAG\u6b65\u9aa4\u7684\u5fc5\u8981\u6027\uff0c\u4f18\u5316\u6548\u7387\u4e0e\u6548\u679c\u7684\u6743\u8861\u3002", "result": "\u4ee3\u7406\u80fd\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c11\u8d85\u8fc760%\u7684\u641c\u7d22\u65f6\u95f4\uff0c\u964d\u4f4e\u6602\u8d35\u5de5\u5177\u8c03\u7528\u3002\u5728\u516d\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u89c4\u5212\u4ee3\u7406\u65b9\u6cd5\u80fd\u6709\u6548\u5e73\u8861VQA\u4efb\u52a1\u7684\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u4e3a\u77e5\u8bc6\u5bc6\u96c6\u578bVQA\u67e5\u8be2\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.20375", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20375", "abs": "https://arxiv.org/abs/2601.20375", "authors": ["Wei Huang", "Anda Cheng", "Yinggui Wang", "Lei Wang", "Tao Wei"], "title": "LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning", "comment": "Accepted by VLDB2026", "summary": "Large Language Models (LLMs) can be fine-tuned on domain-specific data to enhance their performance in specialized fields. However, such data often contains numerous low-quality samples, necessitating effective data processing (DP). In practice, DP strategies are typically developed through iterative manual analysis and trial-and-error adjustment. These processes inevitably incur high labor costs and may lead to privacy issues in high-privacy domains like healthcare due to direct human access to sensitive data. Thus, achieving automated data processing without exposing the raw data has become a critical challenge. To address this challenge, we propose LLM-AutoDP, a novel framework that leverages LLMs as agents to automatically generate and optimize data processing strategies. Our method generates multiple candidate strategies and iteratively refines them using feedback signals and comparative evaluations. This iterative in-context learning mechanism enables the agent to converge toward high-quality processing pipelines without requiring direct human intervention or access to the underlying data. To further accelerate strategy search, we introduce three key techniques: Distribution Preserving Sampling, which reduces data volume while maintaining distributional integrity; Processing Target Selection, which uses a binary classifier to identify low-quality samples for focused processing; Cache-and-Reuse Mechanism}, which minimizes redundant computations by reusing prior processing results. Results show that models trained on data processed by our framework achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines based on LLM agents, LLM-AutoDP achieves approximately a 65% win rate. Moreover, our acceleration techniques reduce the total searching time by up to 10 times, demonstrating both effectiveness and efficiency.", "AI": {"tldr": "LLM-AutoDP\uff1a\u5229\u7528LLM\u4f5c\u4e3a\u667a\u80fd\u4f53\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316\u6570\u636e\u5904\u7406\u7b56\u7565\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\u548c\u52a0\u901f\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u5904\u7406\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u6216\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u3002", "motivation": "\u9488\u5bf9\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e2d\u5927\u91cf\u4f4e\u8d28\u91cf\u6837\u672c\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u4eba\u5de5\u6570\u636e\u5904\u7406\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u5728\u9ad8\u9690\u79c1\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u9700\u8981\u5b9e\u73b0\u65e0\u9700\u63a5\u89e6\u539f\u59cb\u6570\u636e\u7684\u81ea\u52a8\u5316\u6570\u636e\u5904\u7406\u3002", "method": "\u63d0\u51faLLM-AutoDP\u6846\u67b6\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u667a\u80fd\u4f53\u81ea\u52a8\u751f\u6210\u591a\u4e2a\u5019\u9009\u7b56\u7565\uff0c\u901a\u8fc7\u53cd\u9988\u4fe1\u53f7\u548c\u6bd4\u8f83\u8bc4\u4f30\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002\u5f15\u5165\u4e09\u79cd\u52a0\u901f\u6280\u672f\uff1a\u5206\u5e03\u4fdd\u6301\u91c7\u6837\u3001\u5904\u7406\u76ee\u6807\u9009\u62e9\uff08\u4e8c\u5143\u5206\u7c7b\u5668\u8bc6\u522b\u4f4e\u8d28\u91cf\u6837\u672c\uff09\u3001\u7f13\u5b58\u91cd\u7528\u673a\u5236\u3002", "result": "\u4f7f\u7528\u8be5\u6846\u67b6\u5904\u7406\u7684\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\u672a\u5904\u7406\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u80dc\u7387\u8d85\u8fc780%\uff1b\u76f8\u6bd4\u57fa\u4e8eLLM\u667a\u80fd\u4f53\u7684AutoML\u57fa\u7ebf\u80dc\u7387\u7ea665%\uff1b\u52a0\u901f\u6280\u672f\u5c06\u603b\u641c\u7d22\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe10\u500d\u3002", "conclusion": "LLM-AutoDP\u6846\u67b6\u5728\u4fdd\u6301\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u81ea\u52a8\u5316\u7684\u6570\u636e\u5904\u7406\u7b56\u7565\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agent analysis"}}
{"id": "2601.20731", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.20731", "abs": "https://arxiv.org/abs/2601.20731", "authors": ["Mae Sosto", "Delfina Sol Martinez Pandiani", "Laura Hollink"], "title": "QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks", "comment": null, "summary": "This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized \"unmarked\" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u518d\u73b0\u793e\u4f1a\u89c4\u8303\uff08\u7279\u522b\u662f\u5f02\u6027\u604b\u987a\u6027\u522b\u89c4\u8303\uff09\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u89c4\u8303\u5982\u4f55\u8f6c\u5316\u4e3a\u6587\u672c\u751f\u6210\u4e2d\u7684\u53ef\u6d4b\u91cf\u504f\u89c1\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5bf9\u6807\u8bb0\u4e3a\u9177\u513f\u3001\u975e\u9177\u513f\u548c\u672a\u6807\u8bb0\u7684\u53d7\u8bd5\u8005\u4ea7\u751f\u4e0d\u540c\u504f\u89c1\u6a21\u5f0f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u518d\u73b0\u793e\u4f1a\u89c4\u8303\uff0c\u7279\u522b\u662f\u5f02\u6027\u604b\u987a\u6027\u522b\u89c4\u8303\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u89c4\u8303\u5982\u4f55\u8f6c\u5316\u4e3a\u6587\u672c\u751f\u6210\u4e2d\u7684\u53ef\u6d4b\u91cf\u504f\u89c1\u3002\u5173\u6ce8\u6a21\u578b\u662f\u5426\u57fa\u4e8e\u53d7\u8bd5\u8005\u7684\u6027\u522b\u6216\u6027\u53d6\u5411\u4fe1\u606f\u4ea7\u751f\u4e0d\u540c\u7684\u54cd\u5e94\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5c06\u53d7\u8bd5\u8005\u5206\u4e3a\u4e09\u7c7b\uff1a\u9177\u513f\u6807\u8bb0\u3001\u975e\u9177\u513f\u6807\u8bb0\u548c\u672a\u6807\u8bb0\uff08\u6807\u51c6\u5316\u7c7b\u522b\uff09\uff1b2) \u901a\u8fc7\u56db\u4e2a\u7ef4\u5ea6\u64cd\u4f5c\u5316\u8868\u5f81\u4e0d\u5e73\u8861\uff1a\u60c5\u611f\u3001\u5c0a\u91cd\u5ea6\u3001\u6bd2\u6027\u548c\u9884\u6d4b\u591a\u6837\u6027\uff1b3) \u6bd4\u8f83\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u3001\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u548c\u95ed\u6e90\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7684\u54cd\u5e94\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff1a1) \u63a9\u7801\u8bed\u8a00\u6a21\u578b\u5bf9\u9177\u513f\u6807\u8bb0\u53d7\u8bd5\u8005\u4ea7\u751f\u6700\u4e0d\u5229\u7684\u60c5\u611f\u3001\u66f4\u9ad8\u7684\u6bd2\u6027\u548c\u66f4\u8d1f\u9762\u7684\u5c0a\u91cd\u5ea6\uff1b2) \u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u90e8\u5206\u7f13\u89e3\u4e86\u8fd9\u4e9b\u6a21\u5f0f\uff1b3) \u95ed\u6e90\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u503e\u5411\u4e8e\u5bf9\u672a\u6807\u8bb0\u53d7\u8bd5\u8005\u4ea7\u751f\u66f4\u6709\u5bb3\u7684\u8f93\u51fa\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5927\u8bed\u8a00\u6a21\u578b\u518d\u73b0\u4e86\u89c4\u8303\u6027\u7684\u793e\u4f1a\u5047\u8bbe\uff0c\u4f46\u504f\u89c1\u7684\u5f62\u5f0f\u548c\u7a0b\u5ea6\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u7279\u5b9a\u6a21\u578b\u7279\u5f81\u3002\u6a21\u578b\u53ef\u80fd\u91cd\u65b0\u5206\u914d\u4f46\u4e0d\u4f1a\u6d88\u9664\u8868\u5f81\u4f24\u5bb3\uff0c\u8868\u660e\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u6a21\u578b\u8bc4\u4f30\u548c\u7f13\u89e3\u7b56\u7565\u3002", "topic": "agent analysis"}}
{"id": "2601.20420", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20420", "abs": "https://arxiv.org/abs/2601.20420", "authors": ["Yuhang Liu", "Erdun Gao", "Dong Gong", "Anton van den Hengel", "Javen Qinfeng Shi"], "title": "Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs", "comment": null, "summary": "Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs' activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical progress, SAEs suffer from a fundamental theoretical ambiguity: the well-defined correspondence between LLM representations and human-interpretable concepts remains unclear. This lack of theoretical grounding gives rise to several methodological challenges, including difficulties in principled method design and evaluation criteria. In this work, we show that, under mild assumptions, LLM representations can be approximated as a {linear mixture} of the log-posteriors over concepts given the input context, through the lens of a latent variable model where concepts are treated as latent variables. This motivates a principled framework for concept extraction, namely Concept Component Analysis (ConCA), which aims to recover the log-posterior of each concept from LLM representations through a {unsupervised} linear unmixing process. We explore a specific variant, termed sparse ConCA, which leverages a sparsity prior to address the inherent ill-posedness of the unmixing problem. We implement 12 sparse ConCA variants and demonstrate their ability to extract meaningful concepts across multiple LLMs, offering theory-backed advantages over SAEs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faConcept Component Analysis (ConCA)\u6846\u67b6\uff0c\u901a\u8fc7\u7ebf\u6027\u89e3\u6df7\u4eceLLM\u8868\u793a\u4e2d\u6062\u590d\u6982\u5ff5\u7684\u540e\u9a8c\u5bf9\u6570\uff0c\u4e3a\u6982\u5ff5\u63d0\u53d6\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAEs)\u867d\u7136\u80fd\u63d0\u53d6\u53ef\u89e3\u91ca\u6982\u5ff5\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\uff0c\u5bfc\u81f4\u65b9\u6cd5\u8bbe\u8ba1\u548c\u8bc4\u4f30\u6807\u51c6\u5b58\u5728\u56f0\u96be\u3002\u9700\u8981\u5efa\u7acbLLM\u8868\u793a\u4e0e\u4eba\u7c7b\u53ef\u89e3\u91ca\u6982\u5ff5\u4e4b\u95f4\u7684\u7406\u8bba\u5bf9\u5e94\u5173\u7cfb\u3002", "method": "\u63d0\u51faConcept Component Analysis (ConCA)\u6846\u67b6\uff0c\u5c06LLM\u8868\u793a\u5efa\u6a21\u4e3a\u6982\u5ff5\u540e\u9a8c\u5bf9\u6570\u7684\u7ebf\u6027\u6df7\u5408\u3002\u901a\u8fc7\u65e0\u76d1\u7763\u7ebf\u6027\u89e3\u6df7\u8fc7\u7a0b\u6062\u590d\u6bcf\u4e2a\u6982\u5ff5\u7684\u540e\u9a8c\u5bf9\u6570\u3002\u7279\u522b\u63d0\u51fa\u7a00\u758fConCA\u53d8\u4f53\uff0c\u5229\u7528\u7a00\u758f\u5148\u9a8c\u89e3\u51b3\u89e3\u6df7\u95ee\u9898\u7684\u75c5\u6001\u6027\u3002", "result": "\u5b9e\u73b0\u4e8612\u4e2a\u7a00\u758fConCA\u53d8\u4f53\uff0c\u5728\u591a\u4e2aLLM\u4e0a\u6210\u529f\u63d0\u53d6\u6709\u610f\u4e49\u7684\u53ef\u89e3\u91ca\u6982\u5ff5\uff0c\u76f8\u6bd4SAEs\u5177\u6709\u7406\u8bba\u652f\u6301\u7684\u4f18\u52bf\u3002", "conclusion": "ConCA\u4e3aLLM\u6982\u5ff5\u63d0\u53d6\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u51b3\u4e86SAEs\u7684\u7406\u8bba\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u7ebf\u6027\u89e3\u6df7\u65b9\u6cd5\u80fd\u591f\u66f4\u53ef\u9760\u5730\u63d0\u53d6\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u6982\u5ff5\u3002", "topic": "agent analysis"}}
{"id": "2601.20687", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20687", "abs": "https://arxiv.org/abs/2601.20687", "authors": ["Zhiqiang Kou", "Junyang Chen", "Xin-Qiang Cai", "Xiaobo Xia", "Ming-Kun Xie", "Dong-Dong Wu", "Biao Liu", "Yuheng Jia", "Xin Geng", "Masashi Sugiyama", "Tat-Seng Chua"], "title": "Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models", "comment": "22 pages, 8 figures, 7 tables", "summary": "Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher's preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6b63\u4f8b-\u672a\u6807\u6ce8\u5f3a\u5316\u5b66\u4e60\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u672c\u5730\u5316\u5c0f\u6a21\u578b\u90e8\u7f72\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u504f\u597d\u6216\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u751f\u6210\u951a\u70b9\u54cd\u5e94\uff0c\u5b66\u751f\u6a21\u578b\u91c7\u6837\u5019\u9009\uff0c\u8fdb\u884c\u951a\u70b9\u6761\u4ef6\u81ea\u6392\u5e8f\u6765\u8bf1\u5bfc\u504f\u597d\u4fe1\u53f7\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u3001\u6210\u672c\u548c\u5ef6\u8fdf\u9650\u5236\uff0c\u672c\u5730\u90e8\u7f72\u5c0f\u6a21\u578b\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u4f46\u5927\u591a\u6570\u5b9e\u9645\u7ba1\u9053\u53ea\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u65e0\u6cd5\u8fbe\u5230\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u9636\u6bb5\u3002RL\u5bf9\u9f50\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u504f\u597d\u6807\u6ce8\u6216\u4f9d\u8d56\u9ad8\u8d28\u91cf\u5956\u52b1\u6a21\u578b\uff0c\u8fd9\u4e24\u79cd\u65b9\u5f0f\u90fd\u4e0d\u9002\u5408\u672c\u5730\u90e8\u7f72\u73af\u5883\u3002", "method": "\u63d0\u51fa\u6b63\u4f8b-\u672a\u6807\u6ce8\u5f3a\u5316\u5b66\u4e60\u84b8\u998f\u65b9\u6cd5\uff1a1\uff09\u5bf9\u6bcf\u4e2a\u63d0\u793a\u67e5\u8be2\u6559\u5e08\u6a21\u578b\u4e00\u6b21\u83b7\u5f97\u951a\u70b9\u54cd\u5e94\uff1b2\uff09\u672c\u5730\u91c7\u6837\u591a\u4e2a\u5b66\u751f\u5019\u9009\u54cd\u5e94\uff1b3\uff09\u8fdb\u884c\u951a\u70b9\u6761\u4ef6\u81ea\u6392\u5e8f\u6765\u8bf1\u5bfc\u6210\u5bf9\u6216\u5217\u8868\u504f\u597d\uff1b4\uff09\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u6216\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u5b9e\u73b0\u5b8c\u5168\u672c\u5730\u8bad\u7ec3\u5faa\u73af\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u8bf1\u5bfc\u7684\u504f\u597d\u4fe1\u53f7\u5177\u6709\u987a\u5e8f\u4e00\u81f4\u6027\u548c\u96c6\u4e2d\u4e8e\u8fd1\u6700\u4f18\u5019\u9009\u7684\u7279\u6027\uff0c\u652f\u6301\u504f\u597d\u4f18\u5316\u7684\u7a33\u5b9a\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4f4e\u6210\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u4e00\u81f4\u7684\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u586b\u8865\u4e86\u672c\u5730\u5c0f\u6a21\u578b\u90e8\u7f72\u4e2d\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u7684\u7a7a\u767d\uff0c\u65e0\u9700\u4eba\u5de5\u504f\u597d\u6807\u6ce8\u6216\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u7684\u504f\u597d\u4f18\u5316\u80fd\u529b\u84b8\u998f\u5230\u672c\u5730\u53ef\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4f4e\u6210\u672c\u7684\u672c\u5730\u5bf9\u9f50\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.20714", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20714", "abs": "https://arxiv.org/abs/2601.20714", "authors": ["Raul de la Rosa", "Ivana Dusparic", "Nicolas Cardozo"], "title": "Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions", "comment": null, "summary": "Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.", "AI": {"tldr": "MORPHIN\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u7684Q\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u53c2\u6570\uff0c\u9002\u5e94\u5956\u52b1\u51fd\u6570\u53d8\u5316\u548c\u52a8\u4f5c\u7a7a\u95f4\u6269\u5c55\uff0c\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5728Gridworld\u548c\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6Q\u5b66\u4e60\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u9762\u4e34\u975e\u5e73\u7a33\u73af\u5883\u6311\u6218\uff0c\u7279\u522b\u662f\u5956\u52b1\u51fd\u6570\u53d8\u5316\u548c\u52a8\u4f5c\u7a7a\u95f4\u6269\u5c55\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u5bb9\u6613\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u63d0\u51faMORPHIN\u6846\u67b6\uff0c\u96c6\u6210\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u548c\u63a2\u7d22\u8d85\u53c2\u6570\uff0c\u9002\u5e94\u5956\u52b1\u51fd\u6570\u53d8\u5316\u548c\u52a8\u4f5c\u7a7a\u95f4\u6269\u5c55\uff0c\u540c\u65f6\u4fdd\u7559\u5148\u9a8c\u7b56\u7565\u77e5\u8bc6\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728Gridworld\u57fa\u51c6\u548c\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u6a21\u62df\u4e2d\u9a8c\u8bc1\uff0cMORPHIN\u76f8\u6bd4\u6807\u51c6Q\u5b66\u4e60\u57fa\u7ebf\u83b7\u5f97\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u6301\u7eed\u9002\u5e94\u80fd\u529b\uff0c\u5b66\u4e60\u6548\u7387\u63d0\u5347\u9ad8\u8fbe1.7\u500d\u3002", "conclusion": "MORPHIN\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u9002\u5e94\u95ee\u9898\uff0c\u80fd\u591f\u5728\u5956\u52b1\u51fd\u6570\u53d8\u5316\u548c\u52a8\u4f5c\u7a7a\u95f4\u6269\u5c55\u65f6\u5b9e\u73b0\u9ad8\u6548\u5728\u7ebf\u9002\u5e94\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.20802", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20802", "abs": "https://arxiv.org/abs/2601.20802", "authors": ["Jonas H\u00fcbotter", "Frederike L\u00fcbeck", "Lejs Behric", "Anton Baumann", "Marco Bagatella", "Daniel Marta", "Ido Hakimi", "Idan Shenfeld", "Thomas Kleine Buening", "Carlos Guestrin", "Andreas Krause"], "title": "Reinforcement Learning via Self-Distillation", "comment": null, "summary": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.", "AI": {"tldr": "SDPO\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u4e30\u5bcc\u7684\u6587\u672c\u53cd\u9988\uff08\u5982\u8fd0\u884c\u65f6\u9519\u8bef\uff09\u8fdb\u884c\u81ea\u6211\u84b8\u998f\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528\u6807\u91cf\u5956\u52b1\u7684RLVR\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u65b9\u6cd5\u4ec5\u4ece\u6bcf\u6b21\u5c1d\u8bd5\u7684\u6807\u91cf\u7ed3\u679c\u5956\u52b1\u4e2d\u5b66\u4e60\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u4fe1\u7528\u5206\u914d\u74f6\u9888\u3002\u8bb8\u591a\u53ef\u9a8c\u8bc1\u73af\u5883\u5b9e\u9645\u4e0a\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6587\u672c\u53cd\u9988\uff08\u5982\u8fd0\u884c\u65f6\u9519\u8bef\u6216\u8bc4\u4f30\u610f\u89c1\uff09\uff0c\u53ef\u4ee5\u89e3\u91ca\u5931\u8d25\u539f\u56e0\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u606f\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51fa\u81ea\u6211\u84b8\u998f\u7b56\u7565\u4f18\u5316\uff08SDPO\uff09\uff0c\u5c06\u6807\u8bb0\u5316\u7684\u53cd\u9988\u8f6c\u6362\u4e3a\u5bc6\u96c6\u5b66\u4e60\u4fe1\u53f7\uff0c\u65e0\u9700\u5916\u90e8\u6559\u5e08\u6216\u663e\u5f0f\u5956\u52b1\u6a21\u578b\u3002SDPO\u5c06\u5f53\u524d\u6a21\u578b\u5728\u53cd\u9988\u6761\u4ef6\u4e0b\u7684\u8f93\u51fa\u89c6\u4e3a\u81ea\u6211\u6559\u5e08\uff0c\u5e76\u5c06\u5176\u53cd\u9988\u611f\u77e5\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u84b8\u998f\u56de\u7b56\u7565\u4e2d\uff0c\u5229\u7528\u6a21\u578b\u56de\u987e\u6027\u8bc6\u522b\u81ea\u8eab\u9519\u8bef\u7684\u80fd\u529b\u3002", "result": "\u5728\u79d1\u5b66\u63a8\u7406\u3001\u5de5\u5177\u4f7f\u7528\u548cLiveCodeBench v6\u7684\u7ade\u4e89\u6027\u7f16\u7a0b\u4efb\u52a1\u4e0a\uff0cSDPO\u76f8\u6bd4\u5f3aRLVR\u57fa\u7ebf\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u51c6\u786e\u7387\u3002\u5373\u4f7f\u5728\u4ec5\u8fd4\u56de\u6807\u91cf\u53cd\u9988\u7684\u6807\u51c6RLVR\u73af\u5883\u4e2d\uff0cSDPO\u901a\u8fc7\u4f7f\u7528\u6210\u529f\u8f68\u8ff9\u4f5c\u4e3a\u5931\u8d25\u5c1d\u8bd5\u7684\u9690\u5f0f\u53cd\u9988\u4e5f\u4f18\u4e8e\u57fa\u7ebf\u3002\u5728\u6d4b\u8bd5\u65f6\u5bf9\u5355\u4e2a\u95ee\u9898\u5e94\u7528SDPO\u53ef\u52a0\u901f\u56f0\u96be\u4e8c\u5143\u5956\u52b1\u4efb\u52a1\u7684\u53d1\u73b0\uff0c\u4ee53\u500d\u66f4\u5c11\u7684\u5c1d\u8bd5\u8fbe\u5230\u4e0ebest-of-k\u91c7\u6837\u6216\u591a\u8f6e\u5bf9\u8bdd\u76f8\u540c\u7684\u53d1\u73b0\u6982\u7387\u3002", "conclusion": "SDPO\u901a\u8fc7\u5229\u7528\u4e30\u5bcc\u7684\u6587\u672c\u53cd\u9988\u8fdb\u884c\u81ea\u6211\u84b8\u998f\uff0c\u6709\u6548\u89e3\u51b3\u4e86RLVR\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u5728\u591a\u79cd\u53ef\u9a8c\u8bc1\u9886\u57df\u663e\u8457\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.20815", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20815", "abs": "https://arxiv.org/abs/2601.20815", "authors": ["Steve Azzolin", "Stefano Teso", "Bruno Lepri", "Andrea Passerini", "Sagar Malhotra"], "title": "GNN Explanations that do not Explain and How to find Them", "comment": null, "summary": "Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0\u81ea\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\uff08SE-GNNs\uff09\u5b58\u5728\u89e3\u91ca\u9000\u5316\u95ee\u9898\uff0c\u5373\u89e3\u91ca\u53ef\u80fd\u4e0e\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u5b8c\u5168\u65e0\u5173\uff0c\u4f46\u73b0\u6709\u5fe0\u5b9e\u5ea6\u6307\u6807\u65e0\u6cd5\u68c0\u6d4b\u8fd9\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5fe0\u5b9e\u5ea6\u6307\u6807\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "motivation": "\u81ea\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\uff08SE-GNNs\uff09\u7684\u89e3\u91ca\u5bf9\u4e8e\u7406\u89e3\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u548c\u68c0\u6d4b\u654f\u611f\u5c5e\u6027\u6ee5\u7528\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u6307\u51fa\u8fd9\u4e9b\u89e3\u91ca\u53ef\u80fd\u4e0d\u7406\u60f3\u4e14\u6709\u8bef\u5bfc\u6027\uff0c\u4f46\u5bf9\u5176\u5931\u8d25\u6848\u4f8b\u7684\u7279\u5f81\u63cf\u8ff0\u4ecd\u7136\u7f3a\u4e4f\u3002\u672c\u6587\u65e8\u5728\u8bc6\u522bSE-GNN\u89e3\u91ca\u7684\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u4f5c\u8005\u8bc6\u522b\u4e86SE-GNN\u89e3\u91ca\u7684\u9000\u5316\u95ee\u9898\uff1a\u89e3\u91ca\u53ef\u80fd\u4e0eSE-GNN\u63a8\u7406\u6807\u7b7e\u7684\u65b9\u5f0f\u5b8c\u5168\u65e0\u5173\u3002\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u5c55\u793a\u4e86\u9000\u5316\u89e3\u91ca\u65e2\u53ef\u80fd\u88ab\u6076\u610f\u690d\u5165\uff08\u7528\u4e8e\u9690\u85cf\u654f\u611f\u5c5e\u6027\u7684\u4f7f\u7528\uff09\uff0c\u4e5f\u53ef\u80fd\u81ea\u7136\u51fa\u73b0\u3002\u4e3a\u5e94\u5bf9\u6b64\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5fe0\u5b9e\u5ea6\u6307\u6807\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bb8\u591aSE-GNNs\u53ef\u4ee5\u5728\u5b9e\u73b0\u6700\u4f18\u771f\u5b9e\u98ce\u9669\u7684\u540c\u65f6\u4ea7\u751f\u9000\u5316\u89e3\u91ca\uff0c\u800c\u5927\u591a\u6570\u73b0\u6709\u5fe0\u5b9e\u5ea6\u6307\u6807\u65e0\u6cd5\u8bc6\u522b\u8fd9\u79cd\u5931\u8d25\u6a21\u5f0f\u3002\u65b0\u63d0\u51fa\u7684\u5fe0\u5b9e\u5ea6\u6307\u6807\u5728\u6076\u610f\u548c\u81ea\u7136\u8bbe\u7f6e\u4e0b\u90fd\u80fd\u53ef\u9760\u5730\u5c06\u9000\u5316\u89e3\u91ca\u6807\u8bb0\u4e3a\u4e0d\u5fe0\u5b9e\u3002", "conclusion": "SE-GNNs\u5b58\u5728\u89e3\u91ca\u9000\u5316\u95ee\u9898\uff0c\u8fd9\u7a81\u663e\u4e86\u53ef\u9760\u5ba1\u8ba1\u7684\u5fc5\u8981\u6027\u3002\u4f5c\u8005\u63d0\u51fa\u7684\u65b0\u5fe0\u5b9e\u5ea6\u6307\u6807\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u9000\u5316\u89e3\u91ca\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.07597e8b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fvibeflowing-inc%2Fvibe_figma%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/_5A0hdX1IFsNgX60yrcC4EQ21QkN1XI7KTcRPeLEk_0=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fvibeflowing-inc%2Fvibe_figma%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/_5A0hdX1IFsNgX60yrcC4EQ21QkN1XI7KTcRPeLEk_0=441", "authors": ["TLDR Newsletter"], "title": "VibeFigma", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fvibeflowing-inc%2Fvibe_figma%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/_5A0hdX1IFsNgX60yrcC4EQ21QkN1XI7KTcRPeLEk_0=441", "summary": "VibeFigma (GitHub Repo) VibeFigma is a tool that automatically transforms Figma designs into production-ready React components using Tailwind CSS. It uses the official Figma API for accurate design extraction, generates React/TypeScript components, and has optional AI-powered code optimization.", "source": "tldr", "AI": {"tldr": "VibeFigma\u662f\u4e00\u4e2a\u5c06Figma\u8bbe\u8ba1\u81ea\u52a8\u8f6c\u6362\u4e3a\u751f\u4ea7\u5c31\u7eea\u7684React\u7ec4\u4ef6\uff08\u4f7f\u7528Tailwind CSS\uff09\u7684\u5de5\u5177", "motivation": "\u89e3\u51b3\u4ece\u8bbe\u8ba1\u5230\u4ee3\u7801\u7684\u624b\u52a8\u8f6c\u6362\u8fc7\u7a0b\u8017\u65f6\u4e14\u5bb9\u6613\u51fa\u9519\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u524d\u7aef\u5f00\u53d1\u6548\u7387", "method": "\u4f7f\u7528\u5b98\u65b9Figma API\u51c6\u786e\u63d0\u53d6\u8bbe\u8ba1\uff0c\u751f\u6210React/TypeScript\u7ec4\u4ef6\uff0c\u5e76\u63d0\u4f9b\u53ef\u9009\u7684AI\u9a71\u52a8\u7684\u4ee3\u7801\u4f18\u5316", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u81ea\u52a8\u5c06Figma\u8bbe\u8ba1\u8f6c\u6362\u4e3a\u751f\u4ea7\u5c31\u7eeaReact\u7ec4\u4ef6\u7684\u5de5\u5177", "conclusion": "VibeFigma\u901a\u8fc7\u81ea\u52a8\u5316\u8bbe\u8ba1\u5230\u4ee3\u7801\u7684\u8f6c\u6362\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u524d\u7aef\u5f00\u53d1\u6548\u7387", "topic": "swe application"}}
{"id": "tldr.2601.69731b02", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F26%2Fchatgpt-containers%2F%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/NsIgv2IMFqL7XR_7dhnjuJJOe1bnLKvayZSkKKAFQEg=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F26%2Fchatgpt-containers%2F%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/NsIgv2IMFqL7XR_7dhnjuJJOe1bnLKvayZSkKKAFQEg=441", "authors": ["TLDR Newsletter"], "title": "ChatGPT Containers can now run bash, pip/npm install packages, and download files", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F26%2Fchatgpt-containers%2F%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/NsIgv2IMFqL7XR_7dhnjuJJOe1bnLKvayZSkKKAFQEg=441", "summary": "ChatGPT Containers can now run bash, pip/npm install packages, and download files (10 minute read) ChatGPT's code execution container can now run Bash and other languages, install packages via pip and npm, and download files directly from the web.", "source": "tldr", "AI": {"tldr": "ChatGPT\u4ee3\u7801\u6267\u884c\u5bb9\u5668\u65b0\u589e\u529f\u80fd\uff1a\u652f\u6301\u8fd0\u884cBash\u548c\u5176\u4ed6\u8bed\u8a00\uff0c\u901a\u8fc7pip/npm\u5b89\u88c5\u5305\uff0c\u4ee5\u53ca\u4ece\u7f51\u7edc\u76f4\u63a5\u4e0b\u8f7d\u6587\u4ef6", "motivation": "\u6269\u5c55ChatGPT\u7684\u4ee3\u7801\u6267\u884c\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u7f16\u7a0b\u4efb\u52a1\uff0c\u5305\u62ec\u7cfb\u7edf\u7ea7\u64cd\u4f5c\u3001\u5305\u7ba1\u7406\u548c\u6587\u4ef6\u4e0b\u8f7d\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387", "method": "\u5347\u7ea7ChatGPT\u7684\u4ee3\u7801\u6267\u884c\u5bb9\u5668\uff0c\u589e\u52a0\u5bf9Bash\u811a\u672c\u7684\u652f\u6301\uff0c\u96c6\u6210pip\u548cnpm\u5305\u7ba1\u7406\u5668\uff0c\u5b9e\u73b0\u7f51\u7edc\u6587\u4ef6\u4e0b\u8f7d\u529f\u80fd", "result": "ChatGPT\u73b0\u5728\u80fd\u591f\u5728\u5bb9\u5668\u4e2d\u6267\u884c\u66f4\u5e7f\u6cdb\u7684\u7f16\u7a0b\u4efb\u52a1\uff0c\u5305\u62ec\u7cfb\u7edf\u547d\u4ee4\u3001\u5305\u5b89\u88c5\u548c\u6587\u4ef6\u64cd\u4f5c\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5176\u7f16\u7a0b\u8f85\u52a9\u80fd\u529b", "conclusion": "ChatGPT\u4ee3\u7801\u6267\u884c\u5bb9\u5668\u7684\u529f\u80fd\u6269\u5c55\u4f7f\u5176\u6210\u4e3a\u66f4\u5f3a\u5927\u7684\u7f16\u7a0b\u5de5\u5177\uff0c\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u5f00\u53d1\u5de5\u4f5c\u6d41", "topic": "code agent"}}
{"id": "tldr.2601.f6797c30", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fably.com%2Fblog%2Fbuilding-agentic-ai-at-scale%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/ZFc0jj3gf9tIObyP9_YcHz6vCTTJ9vrA6mP7zZIlb1s=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fably.com%2Fblog%2Fbuilding-agentic-ai-at-scale%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/ZFc0jj3gf9tIObyP9_YcHz6vCTTJ9vrA6mP7zZIlb1s=441", "authors": ["TLDR Newsletter"], "title": "What we learned from leaders building agentic AI at scale", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fably.com%2Fblog%2Fbuilding-agentic-ai-at-scale%3Futm_source=tldrdev/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/ZFc0jj3gf9tIObyP9_YcHz6vCTTJ9vrA6mP7zZIlb1s=441", "summary": "What we learned from leaders building agentic AI at scale (7 minute read) Research with AI engineering leaders shows that the biggest challenges are experiential rather than model-related.", "source": "tldr", "AI": {"tldr": "AI\u5de5\u7a0b\u9886\u5bfc\u8005\u7ecf\u9a8c\u5206\u4eab\uff1a\u6784\u5efa\u89c4\u6a21\u5316\u667a\u80fd\u4f53AI\u7684\u4e3b\u8981\u6311\u6218\u6765\u81ea\u5b9e\u8df5\u7ecf\u9a8c\u800c\u975e\u6a21\u578b\u672c\u8eab", "motivation": "\u4e86\u89e3AI\u5de5\u7a0b\u9886\u5bfc\u8005\u5728\u6784\u5efa\u89c4\u6a21\u5316\u667a\u80fd\u4f53AI\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u5b9e\u9645\u6311\u6218\uff0c\u4e3a\u5176\u4ed6\u56e2\u961f\u63d0\u4f9b\u7ecf\u9a8c\u6307\u5bfc", "method": "\u901a\u8fc7\u4e0eAI\u5de5\u7a0b\u9886\u5bfc\u8005\u8fdb\u884c\u7814\u7a76\u548c\u8bbf\u8c08\uff0c\u6536\u96c6\u4ed6\u4eec\u5728\u6784\u5efa\u89c4\u6a21\u5316\u667a\u80fd\u4f53AI\u8fc7\u7a0b\u4e2d\u7684\u5b9e\u8df5\u7ecf\u9a8c", "result": "\u7814\u7a76\u53d1\u73b0\u6700\u5927\u7684\u6311\u6218\u6765\u81ea\u5b9e\u8df5\u7ecf\u9a8c\u65b9\u9762\uff08\u5982\u56e2\u961f\u534f\u4f5c\u3001\u6d41\u7a0b\u7ba1\u7406\u3001\u90e8\u7f72\u8fd0\u7ef4\uff09\uff0c\u800c\u975e\u6a21\u578b\u6280\u672f\u672c\u8eab", "conclusion": "\u6784\u5efa\u6210\u529f\u7684\u89c4\u6a21\u5316\u667a\u80fd\u4f53AI\u9700\u8981\u66f4\u591a\u5173\u6ce8\u7ec4\u7ec7\u3001\u6d41\u7a0b\u548c\u56e2\u961f\u534f\u4f5c\u7b49\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6a21\u578b\u6280\u672f", "topic": "agent analysis"}}
{"id": "tldr.2601.7eedb495", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/oda3zw7TYzM7OGPnf5mpdVxPcDIfYNQi63_NEsadPqk=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/oda3zw7TYzM7OGPnf5mpdVxPcDIfYNQi63_NEsadPqk=441", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019bff5b3376-7ad66c83-8858-48d9-8a23-07bf56211163-000000/oda3zw7TYzM7OGPnf5mpdVxPcDIfYNQi63_NEsadPqk=441", "summary": "After two years of vibecoding, I'm back to writing by hand (6 minute read) AI-generated code for complex projects has no integrity and leads to unmanageable \"slop.\u201d", "source": "tldr", "AI": {"tldr": "\u4f5c\u8005\u7ecf\u5386\u4e24\u5e74AI\u7f16\u7801\u540e\u56de\u5f52\u624b\u5199\u4ee3\u7801\uff0c\u8ba4\u4e3aAI\u751f\u6210\u7684\u590d\u6742\u9879\u76ee\u4ee3\u7801\u7f3a\u4e4f\u5b8c\u6574\u6027\uff0c\u5bfc\u81f4\u96be\u4ee5\u7ba1\u7406\u7684\"\u4ee3\u7801\u5783\u573e\"", "motivation": "\u4f5c\u8005\u53d1\u73b0AI\u751f\u6210\u7684\u590d\u6742\u9879\u76ee\u4ee3\u7801\u5b58\u5728\u8d28\u91cf\u95ee\u9898\uff0c\u7f3a\u4e4f\u5b8c\u6574\u6027\uff0c\u5bfc\u81f4\u96be\u4ee5\u7ef4\u62a4\u548c\u7ba1\u7406\u7684\"\u4ee3\u7801\u5783\u573e\"\u95ee\u9898", "method": "\u57fa\u4e8e\u4e2a\u4eba\u4e24\u5e74AI\u7f16\u7801\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u5bf9\u6bd4\u5206\u6790AI\u751f\u6210\u4ee3\u7801\u4e0e\u624b\u5199\u4ee3\u7801\u7684\u8d28\u91cf\u5dee\u5f02", "result": "AI\u751f\u6210\u7684\u590d\u6742\u9879\u76ee\u4ee3\u7801\u7f3a\u4e4f\u5b8c\u6574\u6027\uff0c\u5bfc\u81f4\u96be\u4ee5\u7ba1\u7406\u7684\"\u4ee3\u7801\u5783\u573e\"\uff0c\u4f5c\u8005\u56e0\u6b64\u56de\u5f52\u624b\u5199\u4ee3\u7801", "conclusion": "\u5bf9\u4e8e\u590d\u6742\u9879\u76ee\uff0c\u624b\u5199\u4ee3\u7801\u6bd4AI\u751f\u6210\u4ee3\u7801\u66f4\u6709\u8d28\u91cf\u4fdd\u8bc1\uff0c\u5efa\u8bae\u8c28\u614e\u4f7f\u7528AI\u4ee3\u7801\u751f\u6210\u5de5\u5177", "topic": "code agent"}}
{"id": "tldr.2601.19de797b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnl20250127/2/0100019bff8fce5b-9d1dad78-8dc2-441d-9f9d-1afff8f201cf-000000/n0rEceZw4ynn2vQ3QR4I-Yq8BfBMcMdcjTr4nZ2YVR4=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnl20250127/2/0100019bff8fce5b-9d1dad78-8dc2-441d-9f9d-1afff8f201cf-000000/n0rEceZw4ynn2vQ3QR4I-Yq8BfBMcMdcjTr4nZ2YVR4=442", "authors": ["TLDR Newsletter"], "title": "Don't let your best ideas languish in a backlog", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnl20250127/2/0100019bff8fce5b-9d1dad78-8dc2-441d-9f9d-1afff8f201cf-000000/n0rEceZw4ynn2vQ3QR4I-Yq8BfBMcMdcjTr4nZ2YVR4=442", "summary": "Don't let your best ideas languish in a backlog (Sponsor) Vibe coding was supposed to give anyone the power to bring their ideas to life. In reality: a designer or marketer builds something promising, but engineers still need to spend weeks rewriting it to fit production. More often than not, these ideas never leave the backlog.v0 closes the gap between vibe coding and production. Every prompt generates production-ready code that lives in your repository. Designers work against the actual cod...", "source": "tldr", "AI": {"tldr": "v0\u662f\u4e00\u4e2aAI\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff0c\u65e8\u5728\u5f25\u5408\u539f\u578b\u8bbe\u8ba1\u4e0e\u751f\u4ea7\u5c31\u7eea\u4ee3\u7801\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u8ba9\u975e\u5de5\u7a0b\u5e08\u4e5f\u80fd\u5c06\u521b\u610f\u5feb\u901f\u8f6c\u5316\u4e3a\u53ef\u90e8\u7f72\u7684\u4ee3\u7801", "motivation": "\u5f53\u524dAI\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff08\u5982vibe coding\uff09\u867d\u7136\u80fd\u8ba9\u8bbe\u8ba1\u5e08\u6216\u5e02\u573a\u4eba\u5458\u5feb\u901f\u521b\u5efa\u539f\u578b\uff0c\u4f46\u751f\u6210\u7684\u4ee3\u7801\u901a\u5e38\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u751f\u4ea7\u73af\u5883\uff0c\u9700\u8981\u5de5\u7a0b\u5e08\u82b1\u8d39\u6570\u5468\u65f6\u95f4\u91cd\u5199\uff0c\u5bfc\u81f4\u8bb8\u591a\u521b\u610f\u60f3\u6cd5\u6700\u7ec8\u505c\u7559\u5728\u5f85\u529e\u5217\u8868\u4e2d\u65e0\u6cd5\u5b9e\u73b0", "method": "v0\u901a\u8fc7AI\u751f\u6210\u53ef\u76f4\u63a5\u7528\u4e8e\u751f\u4ea7\u7684\u4ee3\u7801\uff0c\u6bcf\u4e2a\u63d0\u793a\u90fd\u80fd\u751f\u6210\u751f\u4ea7\u5c31\u7eea\u7684\u4ee3\u7801\u5e76\u76f4\u63a5\u5b58\u5165\u4ee3\u7801\u4ed3\u5e93\uff0c\u8ba9\u8bbe\u8ba1\u4eba\u5458\u80fd\u591f\u57fa\u4e8e\u5b9e\u9645\u4ee3\u7801\u8fdb\u884c\u5de5\u4f5c", "result": "\u8be5\u5de5\u5177\u80fd\u591f\u663e\u8457\u7f29\u77ed\u4ece\u521b\u610f\u5230\u751f\u4ea7\u90e8\u7f72\u7684\u65f6\u95f4\uff0c\u51cf\u5c11\u5de5\u7a0b\u5e08\u91cd\u5199\u4ee3\u7801\u7684\u5de5\u4f5c\u91cf\uff0c\u8ba9\u66f4\u591a\u521b\u610f\u80fd\u591f\u5feb\u901f\u8f6c\u5316\u4e3a\u5b9e\u9645\u53ef\u7528\u7684\u4ea7\u54c1", "conclusion": "v0\u89e3\u51b3\u4e86\u5f53\u524dAI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u7684\u4e3b\u8981\u75db\u70b9\uff0c\u771f\u6b63\u5b9e\u73b0\u4e86\u4ece\u539f\u578b\u5230\u751f\u4ea7\u4ee3\u7801\u7684\u65e0\u7f1d\u8fc7\u6e21\uff0c\u8ba9\u975e\u6280\u672f\u80cc\u666f\u7684\u4eba\u5458\u4e5f\u80fd\u6709\u6548\u53c2\u4e0e\u4ea7\u54c1\u5f00\u53d1\u8fc7\u7a0b", "topic": "code agent"}}
{"id": "tldr.2601.e2606d35", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnl20250127/6/0100019bff8fce5b-9d1dad78-8dc2-441d-9f9d-1afff8f201cf-000000/HJ4nPE2PFQppkaJVfY97cKac6t1M2656zdcVu5WYesQ=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnl20250127/6/0100019bff8fce5b-9d1dad78-8dc2-441d-9f9d-1afff8f201cf-000000/HJ4nPE2PFQppkaJVfY97cKac6t1M2656zdcVu5WYesQ=442", "authors": ["TLDR Newsletter"], "title": "v0 by Vercel lets you see your designs come to life...with code that actually works", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnl20250127/6/0100019bff8fce5b-9d1dad78-8dc2-441d-9f9d-1afff8f201cf-000000/HJ4nPE2PFQppkaJVfY97cKac6t1M2656zdcVu5WYesQ=442", "summary": "v0 by Vercel lets you see your designs come to life...with code that actually works (Sponsor) Vibe coding prototypes is satisfying. Even more satisfying? Turning your designs into code built for production, ready to deploy. Try v0 today.", "source": "tldr", "AI": {"tldr": "Vercel\u7684v0\u5de5\u5177\u53ef\u4ee5\u5c06\u8bbe\u8ba1\u539f\u578b\u8f6c\u5316\u4e3a\u53ef\u76f4\u63a5\u90e8\u7f72\u7684\u751f\u4ea7\u7ea7\u4ee3\u7801", "motivation": "\u89e3\u51b3\u8bbe\u8ba1\u5230\u4ee3\u7801\u8f6c\u6362\u7684\u75db\u70b9\uff0c\u8ba9\u8bbe\u8ba1\u5e08\u80fd\u591f\u5feb\u901f\u5c06\u89c6\u89c9\u8bbe\u8ba1\u8f6c\u5316\u4e3a\u53ef\u5de5\u4f5c\u7684\u751f\u4ea7\u4ee3\u7801\uff0c\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4", "method": "\u901a\u8fc7AI\u9a71\u52a8\u7684\u4ee3\u7801\u751f\u6210\u6280\u672f\uff0c\u5c06\u8bbe\u8ba1\u539f\u578b\u81ea\u52a8\u8f6c\u6362\u4e3a\u751f\u4ea7\u5c31\u7eea\u7684\u4ee3\u7801", "result": "\u80fd\u591f\u5c06\u8bbe\u8ba1\u5feb\u901f\u8f6c\u5316\u4e3a\u53ef\u76f4\u63a5\u90e8\u7f72\u7684\u4ee3\u7801\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387", "conclusion": "v0\u5de5\u5177\u7b80\u5316\u4e86\u8bbe\u8ba1\u5230\u5f00\u53d1\u7684\u6d41\u7a0b\uff0c\u4f7f\u539f\u578b\u80fd\u591f\u5feb\u901f\u8f6c\u5316\u4e3a\u751f\u4ea7\u5c31\u7eea\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "tldr.2601.efe43303", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.smashingmagazine.com%2F2026%2F01%2Fbeyond-generative-rise-agentic-ai-user-centric-design%2F%3Futm_source=tldrdesign/1/0100019bff8fce5b-9d1dad78-8dc2-441d-9f9d-1afff8f201cf-000000/ZeFC2y512RmAb1KEca5wpgDaayhyRiBETzIhPnxhT6k=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.smashingmagazine.com%2F2026%2F01%2Fbeyond-generative-rise-agentic-ai-user-centric-design%2F%3Futm_source=tldrdesign/1/0100019bff8fce5b-9d1dad78-8dc2-441d-9f9d-1afff8f201cf-000000/ZeFC2y512RmAb1KEca5wpgDaayhyRiBETzIhPnxhT6k=442", "authors": ["TLDR Newsletter"], "title": "Beyond Generative: The Rise Of Agentic AI and User-Centric Design", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Reading time: 19 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.smashingmagazine.com%2F2026%2F01%2Fbeyond-generative-rise-agentic-ai-user-centric-design%2F%3Futm_source=tldrdesign/1/0100019bff8fce5b-9d1dad78-8dc2-441d-9f9d-1afff8f201cf-000000/ZeFC2y512RmAb1KEca5wpgDaayhyRiBETzIhPnxhT6k=442", "summary": "Beyond Generative: The Rise Of Agentic AI and User-Centric Design (19 minute read) Agentic AI represents a fundamental shift from traditional generative AI by enabling systems to autonomously plan, execute, and persist in tasks rather than simply responding to commands. There are four levels of agent autonomy\u2014observe-and-suggest, plan-and-propose, act-with-confirmation, and act-autonomously. This post introduces specialized research methods, such as agent journey mapping and simulated misbeha...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4ece\u4f20\u7edf\u751f\u6210\u5f0fAI\u5411\u667a\u80fd\u4f53AI\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u667a\u80fd\u4f53AI\u80fd\u591f\u81ea\u4e3b\u89c4\u5212\u3001\u6267\u884c\u548c\u6301\u7eed\u5b8c\u6210\u4efb\u52a1\uff0c\u800c\u975e\u4ec5\u54cd\u5e94\u6307\u4ee4\u3002\u6587\u7ae0\u63d0\u51fa\u4e86\u56db\u4e2a\u667a\u80fd\u4f53\u81ea\u4e3b\u6027\u7ea7\u522b\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e13\u95e8\u7684\u7814\u7a76\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u5f0fAI\u4e3b\u8981\u88ab\u52a8\u54cd\u5e94\u7528\u6237\u6307\u4ee4\uff0c\u7f3a\u4e4f\u81ea\u4e3b\u89c4\u5212\u548c\u6301\u7eed\u6267\u884c\u4efb\u52a1\u7684\u80fd\u529b\u3002\u667a\u80fd\u4f53AI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\uff0c\u4f7fAI\u7cfb\u7edf\u80fd\u591f\u50cf\u4eba\u7c7b\u52a9\u624b\u4e00\u6837\u4e3b\u52a8\u601d\u8003\u3001\u89c4\u5212\u548c\u884c\u52a8\uff0c\u5b9e\u73b0\u66f4\u590d\u6742\u3001\u6301\u7eed\u7684\u4efb\u52a1\u5b8c\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u56db\u4e2a\u667a\u80fd\u4f53\u81ea\u4e3b\u6027\u7ea7\u522b\uff1a\u89c2\u5bdf\u4e0e\u5efa\u8bae\u3001\u89c4\u5212\u4e0e\u63d0\u8bae\u3001\u786e\u8ba4\u540e\u884c\u52a8\u3001\u81ea\u4e3b\u884c\u52a8\u3002\u4ecb\u7ecd\u4e86\u4e13\u95e8\u7684\u7814\u7a76\u65b9\u6cd5\uff0c\u5982\u667a\u80fd\u4f53\u65c5\u7a0b\u6620\u5c04\u548c\u6a21\u62df\u4e0d\u5f53\u884c\u4e3a\u5206\u6790\uff0c\u7528\u4e8e\u8bbe\u8ba1\u548c\u8bc4\u4f30\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "result": "\u667a\u80fd\u4f53AI\u4ee3\u8868\u4e86AI\u53d1\u5c55\u7684\u6839\u672c\u6027\u8f6c\u53d8\uff0c\u4ece\u88ab\u52a8\u54cd\u5e94\u8f6c\u5411\u4e3b\u52a8\u89c4\u5212\u548c\u6267\u884c\u3002\u8fd9\u79cd\u8f6c\u53d8\u4f7fAI\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u3001\u6301\u7eed\u7684\u4efb\u52a1\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u66f4\u667a\u80fd\u3001\u66f4\u81ea\u4e3b\u7684\u8f85\u52a9\u4f53\u9a8c\u3002", "conclusion": "\u667a\u80fd\u4f53AI\u662fAI\u53d1\u5c55\u7684\u4e0b\u4e00\u9636\u6bb5\uff0c\u5f3a\u8c03\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u548c\u81ea\u4e3b\u80fd\u529b\u3002\u901a\u8fc7\u9002\u5f53\u7684\u81ea\u4e3b\u6027\u7ea7\u522b\u548c\u4e13\u95e8\u7684\u7814\u7a76\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u66f4\u6709\u6548\u3001\u66f4\u53ef\u9760\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u66f4\u667a\u80fd\u7684\u670d\u52a1\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.573dc072", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FArcanum-Sec%2Fsec-context%3Futm_source=tldrinfosec/1/0100019bffc8616d-f145252c-f4d6-40f1-a6ad-9fba267646b9-000000/1n5muEjn81VI3it7_QGfTcI7VvauQAOFwMwpNclxYhA=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FArcanum-Sec%2Fsec-context%3Futm_source=tldrinfosec/1/0100019bffc8616d-f145252c-f4d6-40f1-a6ad-9fba267646b9-000000/1n5muEjn81VI3it7_QGfTcI7VvauQAOFwMwpNclxYhA=442", "authors": ["TLDR Newsletter"], "title": "Sec-Context", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FArcanum-Sec%2Fsec-context%3Futm_source=tldrinfosec/1/0100019bffc8616d-f145252c-f4d6-40f1-a6ad-9fba267646b9-000000/1n5muEjn81VI3it7_QGfTcI7VvauQAOFwMwpNclxYhA=442", "summary": "Sec-Context (GitHub Repo) Comprehensive security anti-pattern reference distilled from 150+ sources designed for LLM consumption, addressing the 86% XSS failure rate and other critical vulnerabilities in AI-generated code. Provides two reference documents covering 25+ anti-patterns with pseudocode examples, Common Weakness Enumeration (CWE) references, and mitigation strategies optimized for use as system prompts, Retrieval-Augmented Generation (RAG) references, or dedicated security review a...", "source": "tldr", "AI": {"tldr": "Sec-Context\u662f\u4e00\u4e2a\u5b89\u5168\u53cd\u6a21\u5f0f\u53c2\u8003\u5e93\uff0c\u4ece150+\u4e2a\u6765\u6e90\u63d0\u70bc\uff0c\u4e13\u4e3aLLM\u8bbe\u8ba1\uff0c\u65e8\u5728\u89e3\u51b3AI\u751f\u6210\u4ee3\u7801\u4e2d86%\u7684XSS\u5931\u8d25\u7387\u7b49\u5173\u952e\u6f0f\u6d1e\u95ee\u9898", "motivation": "AI\u751f\u6210\u7684\u4ee3\u7801\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662fXSS\u653b\u51fb\u670986%\u7684\u5931\u8d25\u7387\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5b89\u5168\u53c2\u8003\u6765\u63d0\u5347AI\u4ee3\u7801\u751f\u6210\u7684\u5b89\u5168\u6027", "method": "\u4ece150\u591a\u4e2a\u6765\u6e90\u63d0\u70bc\u5b89\u5168\u53cd\u6a21\u5f0f\uff0c\u521b\u5efa\u5305\u542b25+\u4e2a\u53cd\u6a21\u5f0f\u7684\u53c2\u8003\u6587\u6863\uff0c\u63d0\u4f9b\u4f2a\u4ee3\u7801\u793a\u4f8b\u3001CWE\u5f15\u7528\u548c\u7f13\u89e3\u7b56\u7565\uff0c\u4f18\u5316\u4e3a\u7cfb\u7edf\u63d0\u793a\u3001RAG\u53c2\u8003\u6216\u5b89\u5168\u5ba1\u67e5\u5de5\u5177", "result": "\u521b\u5efa\u4e86\u5168\u9762\u7684\u5b89\u5168\u53cd\u6a21\u5f0f\u53c2\u8003\u5e93\uff0c\u4e13\u95e8\u9488\u5bf9LLM\u4f18\u5316\uff0c\u8986\u76d6\u5e38\u89c1\u5b89\u5168\u6f0f\u6d1e\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u7f13\u89e3\u7b56\u7565", "conclusion": "Sec-Context\u4e3aAI\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u4e13\u95e8\u7684\u5b89\u5168\u53c2\u8003\u6846\u67b6\uff0c\u80fd\u663e\u8457\u63d0\u5347\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9XSS\u7b49\u5e38\u89c1\u6f0f\u6d1e", "topic": "swe application"}}
{"id": "tldr.2601.c11d7f9c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fqwen3-max-thinking-debuts-with-focus-on-hard-math-code%2F%3Futm_source=tldrai/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/app_m8Xs5B8FKfCg9bT5vERJIhTl3rNxTY3ZOZotWNs=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fqwen3-max-thinking-debuts-with-focus-on-hard-math-code%2F%3Futm_source=tldrai/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/app_m8Xs5B8FKfCg9bT5vERJIhTl3rNxTY3ZOZotWNs=442", "authors": ["TLDR Newsletter"], "title": "Qwen3-Max-Thinking debuts with focus on hard math, code", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fqwen3-max-thinking-debuts-with-focus-on-hard-math-code%2F%3Futm_source=tldrai/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/app_m8Xs5B8FKfCg9bT5vERJIhTl3rNxTY3ZOZotWNs=442", "summary": "Qwen3-Max-Thinking debuts with focus on hard math, code (2 minute read) Qwen3-Max-Thinking is a reasoning model for complex math, coding, and multi-step workflows. Available on Alibaba Cloud's Model Studio, this model excels in tasks that require evidence gathering and deep verification, offering adaptive tool-use and built-in web search features. Early testers highlight its benefits for developers and enterprises needing long-context reasoning and tool-using agents.", "source": "tldr", "AI": {"tldr": "Qwen3-Max-Thinking\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u590d\u6742\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u591a\u6b65\u5de5\u4f5c\u6d41\u7684\u63a8\u7406\u6a21\u578b\uff0c\u5177\u6709\u81ea\u9002\u5e94\u5de5\u5177\u4f7f\u7528\u548c\u5185\u7f6e\u7f51\u7edc\u641c\u7d22\u529f\u80fd\u3002", "motivation": "\u89e3\u51b3\u9700\u8981\u8bc1\u636e\u6536\u96c6\u548c\u6df1\u5ea6\u9a8c\u8bc1\u7684\u590d\u6742\u4efb\u52a1\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u4f01\u4e1a\u63d0\u4f9b\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u63a8\u7406\u6a21\u578b\u67b6\u6784\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u5de5\u5177\u4f7f\u7528\u548c\u5185\u7f6e\u7f51\u7edc\u641c\u7d22\u529f\u80fd\uff0c\u4e13\u6ce8\u4e8e\u590d\u6742\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u591a\u6b65\u5de5\u4f5c\u6d41\u5904\u7406\u3002", "result": "\u5728Alibaba Cloud\u7684Model Studio\u4e0a\u63a8\u51fa\uff0c\u65e9\u671f\u6d4b\u8bd5\u8005\u8ba4\u53ef\u5176\u5728\u5f00\u53d1\u8005\u548c\u4f01\u4e1a\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7684\u573a\u666f\u3002", "conclusion": "Qwen3-Max-Thinking\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u63a8\u7406\u6a21\u578b\uff0c\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u591a\u6b65\u5de5\u4f5c\u6d41\u4efb\u52a1\uff0c\u5177\u6709\u81ea\u9002\u5e94\u5de5\u5177\u4f7f\u7528\u548c\u7f51\u7edc\u641c\u7d22\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2601.aee77c6a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F26%2Fchatgpt-containers%2F%23atom-everything%3Futm_source=tldrai/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/M_bKeuzUysBDpd-Bn2J8rJhvFa79xWa8sdp9_AJsUR4=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F26%2Fchatgpt-containers%2F%23atom-everything%3Futm_source=tldrai/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/M_bKeuzUysBDpd-Bn2J8rJhvFa79xWa8sdp9_AJsUR4=442", "authors": ["TLDR Newsletter"], "title": "ChatGPT Containers can now run bash, pip/npm install packages, and download files", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F26%2Fchatgpt-containers%2F%23atom-everything%3Futm_source=tldrai/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/M_bKeuzUysBDpd-Bn2J8rJhvFa79xWa8sdp9_AJsUR4=442", "summary": "ChatGPT Containers can now run bash, pip/npm install packages, and download files (11 minute read) ChatGPT Code Interpreter underwent a massive upgrade in the past few months. It can write and then test code in 11 new languages, find files online and download them into a container, and install packages via pip and npm to help it solve problems. There is no official documentation for these new features. This post provides an overview of the update, along with a full list of tools that ChatGPT ...", "source": "tldr", "AI": {"tldr": "ChatGPT Code Interpreter\u529f\u80fd\u5927\u5e45\u5347\u7ea7\uff0c\u73b0\u5728\u53ef\u4ee5\u5728\u5bb9\u5668\u4e2d\u8fd0\u884cbash\u3001\u901a\u8fc7pip/npm\u5b89\u88c5\u5305\u3001\u4e0b\u8f7d\u6587\u4ef6\uff0c\u5e76\u652f\u630111\u79cd\u65b0\u7f16\u7a0b\u8bed\u8a00\u7684\u4ee3\u7801\u7f16\u5199\u4e0e\u6d4b\u8bd5\u3002", "motivation": "ChatGPT Code Interpreter\u529f\u80fd\u5347\u7ea7\u540e\u7f3a\u4e4f\u5b98\u65b9\u6587\u6863\uff0c\u7528\u6237\u9700\u8981\u4e86\u89e3\u8fd9\u4e9b\u65b0\u529f\u80fd\u7684\u5177\u4f53\u80fd\u529b\u548c\u4f7f\u7528\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5b9e\u9645\u6d4b\u8bd5\u548c\u5206\u6790\uff0c\u603b\u7ed3ChatGPT Code Interpreter\u7684\u65b0\u529f\u80fd\uff0c\u5305\u62ec\u5bb9\u5668\u73af\u5883\u3001\u5305\u7ba1\u7406\u3001\u6587\u4ef6\u4e0b\u8f7d\u548c\u591a\u8bed\u8a00\u652f\u6301\u7b49\u5de5\u5177\u96c6\u3002", "result": "\u786e\u8ba4ChatGPT Code Interpreter\u73b0\u5728\u5177\u5907\u5b8c\u6574\u7684\u5bb9\u5668\u5316\u5f00\u53d1\u73af\u5883\uff0c\u80fd\u591f\u6267\u884cbash\u547d\u4ee4\u3001\u5b89\u88c5Python/Node.js\u5305\u3001\u4e0b\u8f7d\u7f51\u7edc\u6587\u4ef6\uff0c\u5e76\u652f\u630111\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u4ee3\u7801\u7f16\u5199\u4e0e\u6d4b\u8bd5\u3002", "conclusion": "ChatGPT Code Interpreter\u7684\u529f\u80fd\u5347\u7ea7\u4f7f\u5176\u6210\u4e3a\u4e00\u4e2a\u66f4\u5f3a\u5927\u7684\u4ee3\u7801\u6267\u884c\u73af\u5883\uff0c\u4f46\u7f3a\u4e4f\u5b98\u65b9\u6587\u6863\u9700\u8981\u7528\u6237\u81ea\u884c\u63a2\u7d22\u3002", "topic": "code agent"}}
{"id": "tldr.2601.32c2b5d8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FAyanami1314%2Fswe-pruner%3Futm_source=tldrai/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/ONWT4CoXbmjfKDHuO1GYkI1FkpB5VDzch5EmxisvOYI=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FAyanami1314%2Fswe-pruner%3Futm_source=tldrai/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/ONWT4CoXbmjfKDHuO1GYkI1FkpB5VDzch5EmxisvOYI=442", "authors": ["TLDR Newsletter"], "title": "Self-Adaptive Context Pruning for Coding Agents", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FAyanami1314%2Fswe-pruner%3Futm_source=tldrai/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/ONWT4CoXbmjfKDHuO1GYkI1FkpB5VDzch5EmxisvOYI=442", "summary": "Self-Adaptive Context Pruning for Coding Agents (GitHub Repo) SWE-Pruner introduces a task-aware context pruning framework that cuts token usage in code agents by over 50% without harming performance. It dynamically selects relevant code using a lightweight model, optimizing workflows in multi-turn engineering tasks.", "source": "tldr", "AI": {"tldr": "SWE-Pruner\u63d0\u51fa\u4efb\u52a1\u611f\u77e5\u4e0a\u4e0b\u6587\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u578b\u52a8\u6001\u9009\u62e9\u76f8\u5173\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4ee3\u7406\u7684token\u4f7f\u7528\u91cf\u51cf\u5c1150%\u4ee5\u4e0a\u800c\u4e0d\u5f71\u54cd\u6027\u80fd", "motivation": "\u4ee3\u7801\u4ee3\u7406\u5728\u5904\u7406\u591a\u8f6e\u5de5\u7a0b\u4efb\u52a1\u65f6\u901a\u5e38\u9700\u8981\u5927\u91cf\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4token\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9488\u5bf9\u4ee3\u7801\u4e0a\u4e0b\u6587\u7684\u667a\u80fd\u526a\u679d\u673a\u5236\u3002", "method": "\u91c7\u7528\u4efb\u52a1\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u526a\u679d\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\u52a8\u6001\u9009\u62e9\u4e0e\u5f53\u524d\u4efb\u52a1\u76f8\u5173\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u4f18\u5316\u591a\u8f6e\u5de5\u7a0b\u4efb\u52a1\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5728\u4fdd\u6301\u6027\u80fd\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u4ee3\u7801\u4ee3\u7406\u7684token\u4f7f\u7528\u91cf\u51cf\u5c11\u8d85\u8fc750%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "conclusion": "SWE-Pruner\u901a\u8fc7\u667a\u80fd\u4e0a\u4e0b\u6587\u526a\u679d\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u4ee3\u7406\u7684token\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4f18\u5316\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "tldr.2601.0a98167b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FwcRw4C/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/yXRLj-6jChVTIYe8HXFpNH1zjADjLo8UHk8z8sSOfg4=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FwcRw4C/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/yXRLj-6jChVTIYe8HXFpNH1zjADjLo8UHk8z8sSOfg4=442", "authors": ["TLDR Newsletter"], "title": "An AI Pioneer Warns the Tech \u2018Herd' Is Marching Into a Dead End", "comment": "Source: TLDR Newsletter, Date: 2026-01-27, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FwcRw4C/1/0100019bffe8af9d-ec95ed15-c083-4e02-9159-04148701cd46-000000/yXRLj-6jChVTIYe8HXFpNH1zjADjLo8UHk8z8sSOfg4=442", "summary": "An AI Pioneer Warns the Tech \u2018Herd' Is Marching Into a Dead End (8 minute read) Yann LeCun, one of the world's leading experts on artificial intelligence, has become increasingly vocal in his criticism of Silicon Valley's approach to building AI. He says the technology industry will eventually hit a dead end in its AI development as LLM technology has its limits. The herd effect in Silicon Valley leaves no room for other approaches that may be more promising in the long run. If everyone were ...", "source": "tldr", "AI": {"tldr": "AI\u5148\u9a71Yann LeCun\u8b66\u544a\u79d1\u6280\u754c\"\u7f8a\u7fa4\u6548\u5e94\"\u6b63\u5c06AI\u53d1\u5c55\u5e26\u5165\u6b7b\u80e1\u540c\uff0c\u8ba4\u4e3aLLM\u6280\u672f\u5b58\u5728\u5c40\u9650\uff0c\u7845\u8c37\u7f3a\u4e4f\u591a\u5143\u5316\u63a2\u7d22", "motivation": "\u6279\u8bc4\u7845\u8c37\u5f53\u524d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u8fc7\u5ea6\u8ffd\u6367\u548c\"\u7f8a\u7fa4\u6548\u5e94\"\uff0c\u6307\u51fa\u8fd9\u79cd\u5355\u4e00\u6280\u672f\u8def\u7ebf\u9650\u5236\u4e86AI\u7684\u957f\u671f\u53d1\u5c55\u6f5c\u529b\uff0c\u9700\u8981\u66f4\u591a\u6837\u5316\u7684\u7814\u7a76\u8def\u5f84", "method": "\u901a\u8fc7\u4e13\u5bb6\u89c2\u70b9\u5206\u6790\uff0cYann LeCun\u4f5c\u4e3aAI\u9886\u57df\u6743\u5a01\u4e13\u5bb6\uff0c\u516c\u5f00\u6279\u8bc4\u5f53\u524dAI\u53d1\u5c55\u7684\u6280\u672f\u8def\u7ebf\uff0c\u5f3a\u8c03LLM\u6280\u672f\u7684\u5c40\u9650\u6027", "result": "\u63ed\u793a\u4e86\u7845\u8c37AI\u53d1\u5c55\u4e2d\u7684\"\u7f8a\u7fa4\u6548\u5e94\"\u95ee\u9898\uff0c\u6307\u51fa\u8fc7\u5ea6\u4f9d\u8d56LLM\u6280\u672f\u5c06\u5bfc\u81f4\u53d1\u5c55\u74f6\u9888\uff0c\u547c\u5401\u66f4\u591a\u5143\u5316\u7684\u6280\u672f\u63a2\u7d22", "conclusion": "\u5f53\u524dAI\u53d1\u5c55\u7684\u5355\u4e00\u6280\u672f\u8def\u7ebf\u5b58\u5728\u98ce\u9669\uff0c\u9700\u8981\u6253\u7834\"\u7f8a\u7fa4\u6548\u5e94\"\uff0c\u63a2\u7d22\u66f4\u591a\u6837\u5316\u7684AI\u6280\u672f\u8def\u5f84\u4ee5\u5b9e\u73b0\u957f\u671f\u7a81\u7834", "topic": "agent analysis"}}
{"id": "tldr.2601.6e98ad03", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F27%2Fone-human-one-agent-one-browser%2F%23atom-everything%3Futm_source=tldrnewsletter/1/0100019c0458af1d-c4ec1ac7-017f-413a-9a11-23d034c00c96-000000/PtsQwNuz6bAHzqy8GyNuGb5dwY_AYdi9SPaqyC7cODE=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F27%2Fone-human-one-agent-one-browser%2F%23atom-everything%3Futm_source=tldrnewsletter/1/0100019c0458af1d-c4ec1ac7-017f-413a-9a11-23d034c00c96-000000/PtsQwNuz6bAHzqy8GyNuGb5dwY_AYdi9SPaqyC7cODE=442", "authors": ["TLDR Newsletter"], "title": "One Human + One Agent = One Browser From Scratch", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F27%2Fone-human-one-agent-one-browser%2F%23atom-everything%3Futm_source=tldrnewsletter/1/0100019c0458af1d-c4ec1ac7-017f-413a-9a11-23d034c00c96-000000/PtsQwNuz6bAHzqy8GyNuGb5dwY_AYdi9SPaqyC7cODE=442", "summary": "One Human + One Agent = One Browser From Scratch (2 minute read) one-agent-one-browser is a web browser built in three days with a single Codex CLI agent. It contains 20,000 lines of Rust with no Rust crate dependencies at all, with only OS system frameworks for image and text rendering. The browser successfully renders HTML and CSS and can even display SVGs and PNGs. The project proves that an agent driven by a talented engineer is enough to get a very solid basic renderer working.", "source": "tldr", "AI": {"tldr": "\u4e00\u4e2a\u5de5\u7a0b\u5e08\u4f7f\u7528Codex CLI\u667a\u80fd\u4f53\u57283\u5929\u5185\u4ece\u96f6\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2\u4e07\u884cRust\u4ee3\u7801\u7684\u6d4f\u89c8\u5668\uff0c\u4e0d\u4f9d\u8d56\u4efb\u4f55Rust\u5e93\uff0c\u4ec5\u4f7f\u7528\u64cd\u4f5c\u7cfb\u7edf\u6846\u67b6\uff0c\u6210\u529f\u6e32\u67d3HTML\u3001CSS\u3001SVG\u548cPNG", "motivation": "\u63a2\u7d22\u667a\u80fd\u4f53\u4e0e\u4eba\u7c7b\u5de5\u7a0b\u5e08\u534f\u4f5c\u7684\u6f5c\u529b\uff0c\u8bc1\u660e\u5728\u4f18\u79c0\u5de5\u7a0b\u5e08\u6307\u5bfc\u4e0b\uff0c\u667a\u80fd\u4f53\u80fd\u591f\u5b8c\u6210\u590d\u6742\u7684\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1", "method": "\u4f7f\u7528Codex CLI\u667a\u80fd\u4f53\u4f5c\u4e3a\u5f00\u53d1\u5de5\u5177\uff0c\u5de5\u7a0b\u5e08\u6307\u5bfc\u667a\u80fd\u4f53\u7f16\u5199\u4ee3\u7801\uff0c\u91c7\u7528Rust\u8bed\u8a00\uff0c\u5b8c\u5168\u4e0d\u4f9d\u8d56\u5916\u90e8Rust\u5e93\uff0c\u4ec5\u4f7f\u7528\u64cd\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u7684\u56fe\u50cf\u548c\u6587\u672c\u6e32\u67d3\u6846\u67b6", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684\u6d4f\u89c8\u5668\uff0c\u5305\u542b2\u4e07\u884cRust\u4ee3\u7801\uff0c\u80fd\u591f\u6e32\u67d3HTML\u3001CSS\u3001SVG\u548cPNG\uff0c\u5b9e\u73b0\u4e86\u57fa\u672c\u7684\u7f51\u9875\u6e32\u67d3\u529f\u80fd", "conclusion": "\u667a\u80fd\u4f53\u5728\u4eba\u7c7b\u5de5\u7a0b\u5e08\u7684\u6307\u5bfc\u4e0b\u80fd\u591f\u5b8c\u6210\u590d\u6742\u7684\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86\"\u4e00\u4eba+\u4e00\u667a\u80fd\u4f53\"\u534f\u4f5c\u6a21\u5f0f\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u6709\u6548\u6027", "topic": "code agent"}}
{"id": "tldr.2601.de78e6ee", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftn1ck.com%2Fblog%2Fclaude-code-made-me-love-meetings-again%3Futm_source=tldrnewsletter/1/0100019c0458af1d-c4ec1ac7-017f-413a-9a11-23d034c00c96-000000/irmICZ_ffXP--j4nKYlxIKR4dGYPdA5C_gBv0UizOJo=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftn1ck.com%2Fblog%2Fclaude-code-made-me-love-meetings-again%3Futm_source=tldrnewsletter/1/0100019c0458af1d-c4ec1ac7-017f-413a-9a11-23d034c00c96-000000/irmICZ_ffXP--j4nKYlxIKR4dGYPdA5C_gBv0UizOJo=442", "authors": ["TLDR Newsletter"], "title": "Claude Code made me love meetings again", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftn1ck.com%2Fblog%2Fclaude-code-made-me-love-meetings-again%3Futm_source=tldrnewsletter/1/0100019c0458af1d-c4ec1ac7-017f-413a-9a11-23d034c00c96-000000/irmICZ_ffXP--j4nKYlxIKR4dGYPdA5C_gBv0UizOJo=442", "summary": "Claude Code made me love meetings again (3 minute read) Claude frees up mental capacity and increases your capacity for getting interrupted.", "source": "tldr", "AI": {"tldr": "Claude Code\u901a\u8fc7\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\u548c\u63d0\u9ad8\u5904\u7406\u4e2d\u65ad\u7684\u80fd\u529b\uff0c\u8ba9\u5f00\u53d1\u8005\u91cd\u65b0\u7231\u4e0a\u4f1a\u8bae", "motivation": "\u5f00\u53d1\u8005\u5728\u7f16\u7801\u8fc7\u7a0b\u4e2d\u7ecf\u5e38\u9762\u4e34\u8ba4\u77e5\u8d1f\u62c5\u8fc7\u91cd\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f1a\u8bae\u548c\u4e2d\u65ad\u9891\u7e41\u7684\u5de5\u4f5c\u73af\u5883\u4e2d\uff0c\u8fd9\u5f71\u54cd\u4e86\u5de5\u4f5c\u6548\u7387\u548c\u5f00\u53d1\u4f53\u9a8c", "method": "\u4f7f\u7528Claude Code\u5de5\u5177\u6765\u8f85\u52a9\u7f16\u7a0b\u5de5\u4f5c\uff0c\u901a\u8fc7AI\u52a9\u624b\u51cf\u5c11\u5f00\u53d1\u8005\u7684\u8ba4\u77e5\u8d1f\u8377\uff0c\u63d0\u9ad8\u5904\u7406\u4e2d\u65ad\u7684\u80fd\u529b", "result": "Claude Code\u91ca\u653e\u4e86\u5f00\u53d1\u8005\u7684\u5fc3\u667a\u5bb9\u91cf\uff0c\u589e\u5f3a\u4e86\u5e94\u5bf9\u4e2d\u65ad\u7684\u80fd\u529b\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u597d\u5730\u5e73\u8861\u7f16\u7801\u5de5\u4f5c\u548c\u4f1a\u8bae\u53c2\u4e0e", "conclusion": "Claude Code\u901a\u8fc7\u51cf\u8f7b\u8ba4\u77e5\u8d1f\u62c5\uff0c\u6539\u5584\u4e86\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u4f53\u9a8c\uff0c\u8ba9\u4f1a\u8bae\u4e0d\u518d\u6210\u4e3a\u8d1f\u62c5\uff0c\u800c\u662f\u53ef\u4ee5\u63a5\u53d7\u7684\u5de5\u4f5c\u73af\u8282", "topic": "code agent"}}
{"id": "tldr.2601.933a0df3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsnarktank%2Fralph%3Futm_source=tldrdevops/1/0100019c04905183-a61a9ef4-95c0-4e6c-aea2-d82e435f887b-000000/iyyFLr-_dpsTzsKp0LBIeOIPWX-raILbYBRHE-QnAk0=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsnarktank%2Fralph%3Futm_source=tldrdevops/1/0100019c04905183-a61a9ef4-95c0-4e6c-aea2-d82e435f887b-000000/iyyFLr-_dpsTzsKp0LBIeOIPWX-raILbYBRHE-QnAk0=442", "authors": ["TLDR Newsletter"], "title": "Ralph", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsnarktank%2Fralph%3Futm_source=tldrdevops/1/0100019c04905183-a61a9ef4-95c0-4e6c-aea2-d82e435f887b-000000/iyyFLr-_dpsTzsKp0LBIeOIPWX-raILbYBRHE-QnAk0=442", "summary": "Ralph (GitHub Repo) Ralph, an autonomous AI agent loop based on Geoffrey Huntley's pattern, is a system that uses AI coding tools like Amp or Claude Code to complete Product Requirements Document (PRD) items. It operates through repeated iterations, with memory persisting via git history, progress.txt, and prd.json, and updates AGENTS.md with learned patterns for future runs.", "source": "tldr", "AI": {"tldr": "Ralph\u662f\u4e00\u4e2a\u57fa\u4e8eGeoffrey Huntley\u6a21\u5f0f\u7684\u81ea\u4e3bAI\u4ee3\u7406\u5faa\u73af\u7cfb\u7edf\uff0c\u4f7f\u7528Amp\u6216Claude Code\u7b49AI\u7f16\u7801\u5de5\u5177\u5b8c\u6210PRD\u9879\u76ee\uff0c\u901a\u8fc7git\u5386\u53f2\u3001progress.txt\u548cprd.json\u5b9e\u73b0\u8bb0\u5fc6\u6301\u4e45\u5316\uff0c\u5e76\u5728AGENTS.md\u4e2d\u8bb0\u5f55\u5b66\u4e60\u5230\u7684\u6a21\u5f0f\u4f9b\u672a\u6765\u4f7f\u7528\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u5b8c\u6210\u4ea7\u54c1\u9700\u6c42\u6587\u6863\uff08PRD\uff09\u9879\u76ee\u7684AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6301\u7eed\u8fed\u4ee3\u548c\u8bb0\u5fc6\u6301\u4e45\u5316\u6765\u63d0\u9ad8\u81ea\u52a8\u5316\u7f16\u7801\u4efb\u52a1\u7684\u6548\u7387\u548c\u53ef\u91cd\u590d\u6027\u3002", "method": "\u57fa\u4e8eGeoffrey Huntley\u6a21\u5f0f\u6784\u5efa\u81ea\u4e3bAI\u4ee3\u7406\u5faa\u73af\uff0c\u4f7f\u7528Amp\u6216Claude Code\u7b49AI\u7f16\u7801\u5de5\u5177\uff0c\u901a\u8fc7git\u5386\u53f2\u3001progress.txt\u548cprd.json\u5b9e\u73b0\u8bb0\u5fc6\u6301\u4e45\u5316\uff0c\u5e76\u5728AGENTS.md\u4e2d\u8bb0\u5f55\u5b66\u4e60\u5230\u7684\u6a21\u5f0f\u3002", "result": "\u521b\u5efa\u4e86Ralph\u7cfb\u7edf\uff0c\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u5b8c\u6210PRD\u9879\u76ee\u7684AI\u4ee3\u7406\u5faa\u73af\uff0c\u5177\u5907\u8bb0\u5fc6\u6301\u4e45\u5316\u548c\u6a21\u5f0f\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "Ralph\u7cfb\u7edf\u5c55\u793a\u4e86\u901a\u8fc7AI\u4ee3\u7406\u5faa\u73af\u548c\u8bb0\u5fc6\u6301\u4e45\u5316\u673a\u5236\u53ef\u4ee5\u5b9e\u73b0\u81ea\u4e3b\u5b8c\u6210\u7f16\u7801\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u4e3a\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "code agent"}}
{"id": "tldr.2601.2a1614b8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2F21st-dev%2F1code%3Futm_source=tldrdevops/1/0100019c04905183-a61a9ef4-95c0-4e6c-aea2-d82e435f887b-000000/KF6L7PAIXmJ7QafZSbEhxU0nEX_F9xYQc_dczxf8BUU=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2F21st-dev%2F1code%3Futm_source=tldrdevops/1/0100019c04905183-a61a9ef4-95c0-4e6c-aea2-d82e435f887b-000000/KF6L7PAIXmJ7QafZSbEhxU0nEX_F9xYQc_dczxf8BUU=442", "authors": ["TLDR Newsletter"], "title": "1Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2F21st-dev%2F1code%3Futm_source=tldrdevops/1/0100019c04905183-a61a9ef4-95c0-4e6c-aea2-d82e435f887b-000000/KF6L7PAIXmJ7QafZSbEhxU0nEX_F9xYQc_dczxf8BUU=442", "summary": "1Code (GitHub Repo) 1Code.dev has launched as a new UI for Claude Code that enables local and remote agent execution across macOS, Linux, and Windows. The platform features a Cursor-like interface with diff previews, a built-in Git client, and offers pre-execution planning capabilities from Claude.", "source": "tldr", "AI": {"tldr": "1Code.dev\u662f\u4e00\u4e2a\u65b0\u7684Claude Code UI\uff0c\u652f\u6301macOS\u3001Linux\u548cWindows\u4e0a\u7684\u672c\u5730\u548c\u8fdc\u7a0b\u4ee3\u7406\u6267\u884c\uff0c\u63d0\u4f9b\u7c7b\u4f3cCursor\u7684\u754c\u9762\u3001\u5dee\u5f02\u9884\u89c8\u3001\u5185\u7f6eGit\u5ba2\u6237\u7aef\u4ee5\u53caClaude\u7684\u9884\u6267\u884c\u89c4\u5212\u529f\u80fd\u3002", "motivation": "\u4e3aClaude Code\u5f00\u53d1\u4e00\u4e2a\u66f4\u5f3a\u5927\u7684\u7528\u6237\u754c\u9762\uff0c\u652f\u6301\u8de8\u5e73\u53f0\u4ee3\u7406\u6267\u884c\uff0c\u63d0\u4f9b\u7c7b\u4f3c\u73b0\u4ee3\u4ee3\u7801\u7f16\u8f91\u5668\u7684\u4f53\u9a8c\uff0c\u5e76\u96c6\u6210AI\u9a71\u52a8\u7684\u89c4\u5212\u529f\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684UI\u5e73\u53f0\uff0c\u5177\u6709\u7c7b\u4f3cCursor\u7684\u754c\u9762\u8bbe\u8ba1\uff0c\u652f\u6301\u672c\u5730\u548c\u8fdc\u7a0b\u4ee3\u7406\u6267\u884c\uff0c\u5305\u542b\u5dee\u5f02\u9884\u89c8\u3001\u5185\u7f6eGit\u5ba2\u6237\u7aef\uff0c\u5e76\u96c6\u6210\u4e86Claude\u7684\u9884\u6267\u884c\u89c4\u5212\u80fd\u529b\u3002", "result": "\u6210\u529f\u63a8\u51fa\u4e861Code.dev\u5e73\u53f0\uff0c\u652f\u6301macOS\u3001Linux\u548cWindows\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e86\u73b0\u4ee3\u5316\u7684\u4ee3\u7801\u7f16\u8f91\u4f53\u9a8c\uff0c\u96c6\u6210\u4e86AI\u89c4\u5212\u529f\u80fd\u548c\u7248\u672c\u63a7\u5236\u5de5\u5177\u3002", "conclusion": "1Code.dev\u4e3aClaude Code\u63d0\u4f9b\u4e86\u4e00\u4e2a\u529f\u80fd\u4e30\u5bcc\u7684\u8de8\u5e73\u53f0\u754c\u9762\uff0c\u7ed3\u5408\u4e86\u73b0\u4ee3\u4ee3\u7801\u7f16\u8f91\u5668\u7684\u4fbf\u5229\u6027\u548cAI\u9a71\u52a8\u7684\u667a\u80fd\u89c4\u5212\u80fd\u529b\u3002", "topic": "swe application"}}
{"id": "tldr.2601.8c4021ef", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fthe-state-of-code%2Fdeveloper-survey-report%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-state-of-code-developer-survey26%26utm_content=newsletter-dev-primary-devsurvey-260128-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/xfwbHF9_DTa8DxifhYkEp4t4dqzj-oMg6-8a0LxIT7U=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fthe-state-of-code%2Fdeveloper-survey-report%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-state-of-code-developer-survey26%26utm_content=newsletter-dev-primary-devsurvey-260128-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/xfwbHF9_DTa8DxifhYkEp4t4dqzj-oMg6-8a0LxIT7U=442", "authors": ["TLDR Newsletter"], "title": "Report: 96% of devs don't fully trust AI code", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fthe-state-of-code%2Fdeveloper-survey-report%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-state-of-code-developer-survey26%26utm_content=newsletter-dev-primary-devsurvey-260128-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/xfwbHF9_DTa8DxifhYkEp4t4dqzj-oMg6-8a0LxIT7U=442", "summary": "Report: 96% of devs don't fully trust AI code (Sponsor) AI is accelerating code generation, but it's creating a bottleneck in the verification phase. Based on a survey of 1,100+ developers, Sonar's newest State of Code report analyzes the impact of generative AI on software engineering workflows and how developers are adapting to address it.Findings include: 96% of developers don't fully trust that AI-generated code is functionally correct yet only 48% always check it before committing61% agr...", "source": "tldr", "AI": {"tldr": "\u8c03\u67e5\u62a5\u544a\u663e\u793a96%\u5f00\u53d1\u8005\u4e0d\u5b8c\u5168\u4fe1\u4efbAI\u751f\u6210\u4ee3\u7801\uff0c\u4ec548%\u4f1a\u5728\u63d0\u4ea4\u524d\u68c0\u67e5\uff0cAI\u52a0\u901f\u4ee3\u7801\u751f\u6210\u4f46\u9a8c\u8bc1\u6210\u4e3a\u74f6\u9888", "motivation": "\u5206\u6790\u751f\u6210\u5f0fAI\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u5f71\u54cd\uff0c\u4e86\u89e3\u5f00\u53d1\u8005\u5982\u4f55\u9002\u5e94AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff0c\u8bc6\u522b\u5f53\u524d\u5b58\u5728\u7684\u4fe1\u4efb\u548c\u9a8c\u8bc1\u95ee\u9898", "method": "\u57fa\u4e8e\u5bf91,100\u591a\u540d\u5f00\u53d1\u8005\u7684\u8c03\u67e5\uff0cSonar\u7684\u4ee3\u7801\u72b6\u6001\u62a5\u544a\u91c7\u7528\u95ee\u5377\u8c03\u67e5\u65b9\u6cd5\u6536\u96c6\u6570\u636e", "result": "96%\u5f00\u53d1\u8005\u4e0d\u5b8c\u5168\u4fe1\u4efbAI\u751f\u6210\u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4ec548%\u4f1a\u5728\u63d0\u4ea4\u524d\u68c0\u67e5\uff0c61%\u5f00\u53d1\u8005\u8ba4\u4e3aAI\u4ee3\u7801\u9700\u8981\u66f4\u591a\u9a8c\u8bc1", "conclusion": "AI\u52a0\u901f\u4ee3\u7801\u751f\u6210\u4f46\u9a8c\u8bc1\u6210\u4e3a\u74f6\u9888\uff0c\u5f00\u53d1\u8005\u5bf9AI\u4ee3\u7801\u4fe1\u4efb\u5ea6\u4f4e\uff0c\u9700\u8981\u66f4\u597d\u7684\u9a8c\u8bc1\u5de5\u5177\u548c\u6d41\u7a0b", "topic": "swe application"}}
{"id": "tldr.2601.c7d8b7a0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.bytebytego.com%2Fp%2Fhow-cursor-shipped-its-coding-agent%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/dIxtUMx2lWUbMeby8TuNnUyNKPXW-1FP5jK7dXbs85E=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.bytebytego.com%2Fp%2Fhow-cursor-shipped-its-coding-agent%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/dIxtUMx2lWUbMeby8TuNnUyNKPXW-1FP5jK7dXbs85E=442", "authors": ["TLDR Newsletter"], "title": "How Cursor Shipped its Coding Agent to Production", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.bytebytego.com%2Fp%2Fhow-cursor-shipped-its-coding-agent%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/dIxtUMx2lWUbMeby8TuNnUyNKPXW-1FP5jK7dXbs85E=442", "summary": "How Cursor Shipped its Coding Agent to Production (12 minute read) Cursor's team shipped Composer, a coding agent that completes most tasks in under 30 seconds, by solving three gnarly engineering problems: the \u201cdiff problem\u201d (models aren't great at making clean code edits, so they trained specifically on edit trajectories and search-replace operations), compounded latency (each step in the agent loop adds delay, so they used MoE architecture, speculative decoding with draft models, and aggre...", "source": "tldr", "AI": {"tldr": "Cursor\u56e2\u961f\u5f00\u53d1\u4e86Composer\u7f16\u7801\u4ee3\u7406\uff0c\u901a\u8fc7\u89e3\u51b3\u4e09\u4e2a\u5de5\u7a0b\u96be\u9898\uff08diff\u95ee\u9898\u3001\u590d\u5408\u5ef6\u8fdf\u3001\u6210\u672c\u63a7\u5236\uff09\uff0c\u5b9e\u73b0\u4e86\u5927\u591a\u6570\u4efb\u52a1\u572830\u79d2\u5185\u5b8c\u6210\u7684\u751f\u4ea7\u7ea7\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u7f16\u7801\u4ee3\u7406\u5728\u5b9e\u9645\u751f\u4ea7\u90e8\u7f72\u4e2d\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u6a21\u578b\u4e0d\u64c5\u957f\u751f\u6210\u5e72\u51c0\u7684\u4ee3\u7801\u7f16\u8f91\uff08diff\u95ee\u9898\uff09\u3001\u4ee3\u7406\u5faa\u73af\u4e2d\u6bcf\u4e2a\u6b65\u9aa4\u90fd\u4f1a\u589e\u52a0\u5ef6\u8fdf\uff08\u590d\u5408\u5ef6\u8fdf\uff09\u3001\u4ee5\u53ca\u8fd0\u884c\u6210\u672c\u8fc7\u9ad8\u3002Cursor\u56e2\u961f\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u624d\u80fd\u5c06\u7f16\u7801\u4ee3\u7406\u6210\u529f\u63a8\u5411\u751f\u4ea7\u73af\u5883\u3002", "method": "1. \u9488\u5bf9diff\u95ee\u9898\uff1a\u4e13\u95e8\u5728\u7f16\u8f91\u8f68\u8ff9\u548c\u641c\u7d22\u66ff\u6362\u64cd\u4f5c\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff1b2. \u9488\u5bf9\u590d\u5408\u5ef6\u8fdf\uff1a\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u3001\u4f7f\u7528\u8349\u7a3f\u6a21\u578b\u8fdb\u884c\u63a8\u6d4b\u89e3\u7801\u3001\u4ee5\u53ca\u805a\u5408\u6280\u672f\uff1b3. \u9488\u5bf9\u6210\u672c\u63a7\u5236\uff1a\u91c7\u7528\u7279\u5b9a\u4f18\u5316\u7b56\u7565\uff08\u5177\u4f53\u672a\u5728\u6458\u8981\u4e2d\u8be6\u7ec6\u8bf4\u660e\uff09\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51faComposer\u7f16\u7801\u4ee3\u7406\uff0c\u80fd\u591f\u572830\u79d2\u5185\u5b8c\u6210\u5927\u591a\u6570\u7f16\u7801\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u751f\u4ea7\u7ea7\u90e8\u7f72\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u7f16\u7801\u4ee3\u7406\u90e8\u7f72\u4e2d\u7684\u4e09\u4e2a\u6838\u5fc3\u5de5\u7a0b\u95ee\u9898\uff0cCursor\u56e2\u961f\u6210\u529f\u5c06Composer\u63a8\u5411\u751f\u4ea7\u73af\u5883\uff0c\u5c55\u793a\u4e86\u89e3\u51b3\u5b9e\u9645\u5de5\u7a0b\u6311\u6218\u5bf9\u4e8eAI\u7f16\u7801\u4ee3\u7406\u5546\u4e1a\u5316\u7684\u91cd\u8981\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2601.4c09a9e9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F4I1kGa/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/UrslZW64eljAB8e7XRua-MvoaqsInlfbr9HxUXn8cBE=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F4I1kGa/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/UrslZW64eljAB8e7XRua-MvoaqsInlfbr9HxUXn8cBE=442", "authors": ["TLDR Newsletter"], "title": "A few random notes from Claude coding quite a bit last few weeks", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F4I1kGa/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/UrslZW64eljAB8e7XRua-MvoaqsInlfbr9HxUXn8cBE=442", "summary": "A few random notes from Claude coding quite a bit last few weeks (5 minute read) Andrej Karpathy has had a rapid shift in their coding workflow, moving from 80% manual to 80% LLM agent-driven programming within weeks. While LLMs boost productivity, stamina, and project scope by handling code generation, they still produce subtle conceptual errors, overcomplicate code, and necessitate close human oversight within an IDE. Despite these flaws, the LLM-assisted workflow makes coding more enjoyabl...", "source": "tldr", "AI": {"tldr": "Andrej Karpathy\u5728\u51e0\u5468\u5185\u5c06\u7f16\u7a0b\u5de5\u4f5c\u6d41\u7a0b\u4ece80%\u624b\u52a8\u8f6c\u53d8\u4e3a80% LLM\u4ee3\u7406\u9a71\u52a8\uff0cLLM\u63d0\u5347\u4e86\u751f\u4ea7\u529b\u4f46\u4ecd\u6709\u6982\u5ff5\u9519\u8bef\u548c\u4ee3\u7801\u8fc7\u5ea6\u590d\u6742\u5316\u7684\u95ee\u9898", "motivation": "\u63a2\u7d22LLM\u4ee3\u7406\u5728\u7f16\u7a0b\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u4e86\u89e3\u4ece\u624b\u52a8\u7f16\u7a0b\u5411AI\u8f85\u52a9\u7f16\u7a0b\u8f6c\u53d8\u7684\u5b9e\u9645\u4f53\u9a8c\u548c\u6311\u6218", "method": "\u57fa\u4e8e\u4e2a\u4eba\u5b9e\u9645\u4f7f\u7528\u7ecf\u9a8c\uff0c\u5bf9\u6bd4\u5206\u6790\u4f20\u7edf\u624b\u52a8\u7f16\u7a0b\u4e0eLLM\u4ee3\u7406\u9a71\u52a8\u7f16\u7a0b\u7684\u5de5\u4f5c\u6d41\u7a0b\u53d8\u5316\uff0c\u8bb0\u5f55\u89c2\u5bdf\u5230\u7684\u4f18\u7f3a\u70b9", "result": "LLM\u663e\u8457\u63d0\u5347\u4e86\u7f16\u7a0b\u751f\u4ea7\u529b\u3001\u8010\u529b\u548c\u9879\u76ee\u8303\u56f4\uff0c\u4f46\u4ecd\u4f1a\u4ea7\u751f\u5fae\u5999\u7684\u6982\u5ff5\u9519\u8bef\u3001\u8fc7\u5ea6\u590d\u6742\u7684\u4ee3\u7801\uff0c\u9700\u8981IDE\u4e2d\u5bc6\u5207\u7684\u4eba\u5de5\u76d1\u7763", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u7f3a\u9677\uff0cLLM\u8f85\u52a9\u7684\u5de5\u4f5c\u6d41\u7a0b\u4f7f\u7f16\u7a0b\u66f4\u52a0\u6109\u5feb\uff0c\u4ee3\u8868\u4e86\u7f16\u7a0b\u5de5\u4f5c\u65b9\u5f0f\u7684\u91cd\u5927\u8f6c\u53d8", "topic": "code agent"}}
{"id": "tldr.2601.0a07f379", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fallenai.org%2Fblog%2Fopen-coding-agents%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/MdQi0qzR3OvBP5FlfP_xajKK56DXzLG67RV5XEKy1iw=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fallenai.org%2Fblog%2Fopen-coding-agents%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/MdQi0qzR3OvBP5FlfP_xajKK56DXzLG67RV5XEKy1iw=442", "authors": ["TLDR Newsletter"], "title": "Open Coding Agents: Fast, accessible coding agents that adapt to any repo", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fallenai.org%2Fblog%2Fopen-coding-agents%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/MdQi0qzR3OvBP5FlfP_xajKK56DXzLG67RV5XEKy1iw=442", "summary": "Open Coding Agents: Fast, accessible coding agents that adapt to any repo (11 minute read) Ai2 has launched Open Coding Agents, an initiative to address the limitations of existing closed and expensive coding agent systems. They are releasing SERA (Soft-verified Efficient Repository Agents), a family of strong open coding models and an accessible training method. This method significantly reduces the cost and complexity of fine-tuning powerful coding agents on private codebases, allowing smal...", "source": "tldr", "AI": {"tldr": "Ai2\u53d1\u5e03Open Coding Agents\u8ba1\u5212\uff0c\u63a8\u51faSERA\u7cfb\u5217\u5f00\u6e90\u7f16\u7801\u6a21\u578b\u53ca\u4f4e\u6210\u672c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u5c0f\u578b\u56e2\u961f\u4e5f\u80fd\u5728\u79c1\u6709\u4ee3\u7801\u5e93\u4e0a\u5fae\u8c03\u5f3a\u5927\u7684\u7f16\u7801\u52a9\u624b", "motivation": "\u89e3\u51b3\u73b0\u6709\u5c01\u95ed\u5f0f\u3001\u6602\u8d35\u7684\u7f16\u7801\u52a9\u624b\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u964d\u4f4e\u79c1\u6709\u4ee3\u7801\u5e93\u4e0a\u8bad\u7ec3\u5f3a\u5927\u7f16\u7801\u52a9\u624b\u7684\u6210\u672c\u548c\u590d\u6742\u5ea6", "method": "\u53d1\u5e03SERA\uff08Soft-verified Efficient Repository Agents\uff09\u7cfb\u5217\u5f00\u6e90\u7f16\u7801\u6a21\u578b\uff0c\u63d0\u4f9b\u53ef\u8bbf\u95ee\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u5728\u79c1\u6709\u4ee3\u7801\u5e93\u4e0a\u5fae\u8c03\u7f16\u7801\u52a9\u624b\u7684\u6210\u672c\u548c\u590d\u6742\u5ea6", "result": "\u4f7f\u5c0f\u578b\u56e2\u961f\u4e5f\u80fd\u8d1f\u62c5\u5f97\u8d77\u5728\u79c1\u6709\u4ee3\u7801\u5e93\u4e0a\u8bad\u7ec3\u5f3a\u5927\u7684\u7f16\u7801\u52a9\u624b\uff0c\u63d0\u9ad8\u4e86\u7f16\u7801\u52a9\u624b\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u9002\u5e94\u6027", "conclusion": "Open Coding Agents\u8ba1\u5212\u901a\u8fc7\u5f00\u6e90\u6a21\u578b\u548c\u4f4e\u6210\u672c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u5f3a\u5927\u7684\u7f16\u7801\u52a9\u624b\u6280\u672f\u66f4\u52a0\u6c11\u4e3b\u5316\uff0c\u80fd\u591f\u9002\u5e94\u4efb\u4f55\u4ee3\u7801\u5e93", "topic": "code agent"}}
{"id": "tldr.2601.69216fd8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fagents-md-outperforms-skills-in-our-agent-evals%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/nHjdsFrceHd5pQlAvc7IBPWOjl5vB-sPLwTpid-W5O8=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fagents-md-outperforms-skills-in-our-agent-evals%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/nHjdsFrceHd5pQlAvc7IBPWOjl5vB-sPLwTpid-W5O8=442", "authors": ["TLDR Newsletter"], "title": "AGENTS.md outperforms skills in our agent evals", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fagents-md-outperforms-skills-in-our-agent-evals%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/nHjdsFrceHd5pQlAvc7IBPWOjl5vB-sPLwTpid-W5O8=442", "summary": "AGENTS.md outperforms skills in our agent evals (9 minute read) AI coding agents struggle with outdated framework knowledge, leading to incorrect code generation. Researchers from Vercel evaluated two approaches for providing current Next.js API documentation: \"skills\" (on-demand knowledge packages) and \"AGENTS.md\" (a persistent, embedded markdown file). Surprisingly, a compressed 8KB docs index embedded directly in AGENTS.md achieved a perfect 100% pass rate in their evals, outperforming ski...", "source": "tldr", "AI": {"tldr": "AGENTS.md\uff08\u5d4c\u5165\u5f0fMarkdown\u6587\u4ef6\uff09\u5728AI\u7f16\u7801\u4ee3\u7406\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u6280\u80fd\u5305\uff0c\u901a\u8fc7\u538b\u7f29\u76848KB\u6587\u6863\u7d22\u5f15\u5b9e\u73b0\u4e86100%\u901a\u8fc7\u7387", "motivation": "AI\u7f16\u7801\u4ee3\u7406\u5728\u5904\u7406\u8fc7\u65f6\u7684\u6846\u67b6\u77e5\u8bc6\u65f6\u5b58\u5728\u95ee\u9898\uff0c\u5bfc\u81f4\u751f\u6210\u9519\u8bef\u4ee3\u7801\u3002\u9700\u8981\u627e\u5230\u6709\u6548\u7684\u65b9\u6cd5\u4e3a\u4ee3\u7406\u63d0\u4f9b\u6700\u65b0\u7684Next.js API\u6587\u6863", "method": "Vercel\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\"\u6280\u80fd\"\uff08\u6309\u9700\u77e5\u8bc6\u5305\uff09\uff1b2\uff09\"AGENTS.md\"\uff08\u6301\u4e45\u5d4c\u5165\u5f0fMarkdown\u6587\u4ef6\uff09\u3002\u540e\u8005\u5305\u542b\u538b\u7f29\u76848KB\u6587\u6863\u7d22\u5f15", "result": "AGENTS.md\u5728\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u7684100%\u901a\u8fc7\u7387\uff0c\u8868\u73b0\u4f18\u4e8e\u6280\u80fd\u5305\u65b9\u6cd5", "conclusion": "\u5d4c\u5165\u5f0fMarkdown\u6587\u4ef6\u6bd4\u4f20\u7edf\u7684\u6280\u80fd\u5305\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u4e3aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u6700\u65b0\u6846\u67b6\u6587\u6863", "topic": "code agent"}}
{"id": "tldr.2601.772ae014", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Femsh.cat%2Fone-human-one-agent-one-browser%2F%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/wptpzv15P30ChmEzIaUOXne1zmFB2gmaqWlrOsQHL40=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Femsh.cat%2Fone-human-one-agent-one-browser%2F%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/wptpzv15P30ChmEzIaUOXne1zmFB2gmaqWlrOsQHL40=442", "authors": ["TLDR Newsletter"], "title": "One Human + One Agent = One Browser From Scratch", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Femsh.cat%2Fone-human-one-agent-one-browser%2F%3Futm_source=tldrdev/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/wptpzv15P30ChmEzIaUOXne1zmFB2gmaqWlrOsQHL40=442", "summary": "One Human + One Agent = One Browser From Scratch (7 minute read) Working with a single LLM agent, the author successfully built a basic, cross-platform HTML/CSS browser from scratch in approximately 70 hours and without third-party Rust dependencies.", "source": "tldr", "AI": {"tldr": "\u4f5c\u8005\u4ec5\u4f7f\u7528\u5355\u4e2aLLM\u4ee3\u7406\uff0c\u5728\u7ea670\u5c0f\u65f6\u5185\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u672c\u7684\u8de8\u5e73\u53f0HTML/CSS\u6d4f\u89c8\u5668\uff0c\u65e0\u9700\u7b2c\u4e09\u65b9Rust\u4f9d\u8d56", "motivation": "\u63a2\u7d22LLM\u4ee3\u7406\u5728\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u80fd\u5426\u4ec5\u51ed\u5355\u4e2a\u4ee3\u7406\u5b8c\u6210\u4ece\u96f6\u6784\u5efa\u6d4f\u89c8\u5668\u7684\u6311\u6218\u6027\u4efb\u52a1", "method": "\u4f7f\u7528\u5355\u4e2aLLM\u4ee3\u7406\u8fdb\u884c\u7aef\u5230\u7aef\u5f00\u53d1\uff0c\u91c7\u7528Rust\u8bed\u8a00\uff0c\u907f\u514d\u7b2c\u4e09\u65b9\u4f9d\u8d56\uff0c\u901a\u8fc7\u8fed\u4ee3\u5f0f\u5f00\u53d1\u8fc7\u7a0b\u6784\u5efa\u6d4f\u89c8\u5668\u6838\u5fc3\u529f\u80fd", "result": "\u6210\u529f\u6784\u5efa\u4e86\u57fa\u672c\u7684\u8de8\u5e73\u53f0HTML/CSS\u6d4f\u89c8\u5668\uff0c\u652f\u6301\u6838\u5fc3\u6e32\u67d3\u529f\u80fd\uff0c\u8bc1\u660e\u4e86\u5355\u4e2aLLM\u4ee3\u7406\u80fd\u591f\u5b8c\u6210\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1", "conclusion": "\u5355\u4e2aLLM\u4ee3\u7406\u5177\u5907\u5b8c\u6210\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u9879\u76ee\u7684\u80fd\u529b\uff0c\u4e3aAI\u8f85\u52a9\u7f16\u7a0b\u548c\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027", "topic": "code agent"}}
{"id": "tldr.2601.fc1c8553", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/pVCYCX9KLQgI-oYWsrXVckCVpaYvLPB_Q8WD1f9PDH4=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/pVCYCX9KLQgI-oYWsrXVckCVpaYvLPB_Q8WD1f9PDH4=442", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/pVCYCX9KLQgI-oYWsrXVckCVpaYvLPB_Q8WD1f9PDH4=442", "summary": "One Human + One Agent = One Browser From Scratch (7 minute read) Working with a single LLM agent, the author successfully built a basic, cross-platform HTML/CSS browser from scratch in approximately 70 hours and without third-party Rust dependencies.", "source": "tldr", "AI": {"tldr": "\u4f5c\u8005\u4f7f\u7528\u5355\u4e2aLLM\u4ee3\u7406\uff0c\u5728\u7ea670\u5c0f\u65f6\u5185\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u672c\u7684\u8de8\u5e73\u53f0HTML/CSS\u6d4f\u89c8\u5668\uff0c\u65e0\u9700\u7b2c\u4e09\u65b9Rust\u4f9d\u8d56", "motivation": "\u63a2\u7d22LLM\u4ee3\u7406\u5728\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u80fd\u5426\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684\u6d4f\u89c8\u5668\uff0c\u6d4b\u8bd5\u4ee3\u7406\u5728\u5927\u578b\u9879\u76ee\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b", "method": "\u4f7f\u7528\u5355\u4e2aLLM\u4ee3\u7406\u4f5c\u4e3a\u4e3b\u8981\u5f00\u53d1\u5de5\u5177\uff0c\u91c7\u7528\u4ece\u96f6\u5f00\u59cb\u7684\u5f00\u53d1\u65b9\u6cd5\uff0c\u5b8c\u5168\u4f7f\u7528Rust\u8bed\u8a00\u4f46\u4e0d\u4f9d\u8d56\u7b2c\u4e09\u65b9\u5e93\uff0c\u901a\u8fc7\u8fed\u4ee3\u5f00\u53d1\u5b9e\u73b0HTML/CSS\u6e32\u67d3\u5f15\u64ce\u548c\u57fa\u672c\u6d4f\u89c8\u5668\u529f\u80fd", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u672c\u7684\u8de8\u5e73\u53f0HTML/CSS\u6d4f\u89c8\u5668\uff0c\u5f00\u53d1\u65f6\u95f4\u7ea670\u5c0f\u65f6\uff0c\u8bc1\u660e\u4e86\u5355\u4e2aLLM\u4ee3\u7406\u80fd\u591f\u5b8c\u6210\u590d\u6742\u7684\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1", "conclusion": "\u5355\u4e2aLLM\u4ee3\u7406\u5177\u5907\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u590d\u6742\u8f6f\u4ef6\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u8fd9\u4e3aAI\u8f85\u52a9\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5c55\u793a\u4e86\u4ee3\u7406\u5728\u5927\u578b\u9879\u76ee\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c", "topic": "code agent"}}
{"id": "tldr.2601.6ca26198", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/X4Bs1VuD4UtrFyBrBRypQuR3HZwh0GN_YqNkZ8dQF0Q=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/X4Bs1VuD4UtrFyBrBRypQuR3HZwh0GN_YqNkZ8dQF0Q=442", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/X4Bs1VuD4UtrFyBrBRypQuR3HZwh0GN_YqNkZ8dQF0Q=442", "summary": "One Human + One Agent = One Browser From Scratch (7 minute read) Working with a single LLM agent, the author successfully built a basic, cross-platform HTML/CSS browser from scratch in approximately 70 hours and without third-party Rust dependencies.", "source": "tldr", "AI": {"tldr": "\u4f5c\u8005\u4f7f\u7528\u5355\u4e2aLLM\u4ee3\u7406\uff0c\u5728\u7ea670\u5c0f\u65f6\u5185\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u672c\u7684\u8de8\u5e73\u53f0HTML/CSS\u6d4f\u89c8\u5668\uff0c\u6ca1\u6709\u4f7f\u7528\u7b2c\u4e09\u65b9Rust\u4f9d\u8d56", "motivation": "\u63a2\u7d22\u5355\u4e2aLLM\u4ee3\u7406\u5728\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u57fa\u7840\u6d4f\u89c8\u5668\u8fd9\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u9879\u76ee", "method": "\u4f7f\u7528\u5355\u4e2aLLM\u4ee3\u7406\u8fdb\u884c\u5f00\u53d1\uff0c\u91c7\u7528\u4ece\u96f6\u5f00\u59cb\u7684\u65b9\u6cd5\uff0c\u4e0d\u4f7f\u7528\u7b2c\u4e09\u65b9Rust\u4f9d\u8d56\uff0c\u5b8c\u5168\u81ea\u4e3b\u5b9e\u73b0HTML/CSS\u6e32\u67d3\u529f\u80fd", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u672c\u7684\u8de8\u5e73\u53f0HTML/CSS\u6d4f\u89c8\u5668\uff0c\u5f00\u53d1\u65f6\u95f4\u7ea670\u5c0f\u65f6\uff0c\u8bc1\u660e\u4e86\u5355\u4e2aLLM\u4ee3\u7406\u80fd\u591f\u5b8c\u6210\u590d\u6742\u7684\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1", "conclusion": "\u5355\u4e2aLLM\u4ee3\u7406\u5177\u5907\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u590d\u6742\u8f6f\u4ef6\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u4e3aAI\u8f85\u52a9\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027", "topic": "code agent"}}
{"id": "tldr.2601.c4a31982", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/qPGe9Da-Y9HTMUusSVhY4jJuOmU7gc5Vk5UVrdPgipk=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/qPGe9Da-Y9HTMUusSVhY4jJuOmU7gc5Vk5UVrdPgipk=442", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0497333b-5581f3ca-93ae-41ce-b123-0c070c906bf3-000000/qPGe9Da-Y9HTMUusSVhY4jJuOmU7gc5Vk5UVrdPgipk=442", "summary": "One Human + One Agent = One Browser From Scratch (7 minute read) Working with a single LLM agent, the author successfully built a basic, cross-platform HTML/CSS browser from scratch in approximately 70 hours and without third-party Rust dependencies.", "source": "tldr", "AI": {"tldr": "\u4f5c\u8005\u4ec5\u4f7f\u7528\u5355\u4e2aLLM\u4ee3\u7406\uff0c\u5728\u7ea670\u5c0f\u65f6\u5185\u4ece\u96f6\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u7840\u7684\u8de8\u5e73\u53f0HTML/CSS\u6d4f\u89c8\u5668\uff0c\u65e0\u9700\u7b2c\u4e09\u65b9Rust\u4f9d\u8d56", "motivation": "\u63a2\u7d22\u5355\u4e2aLLM\u4ee3\u7406\u80fd\u5426\u72ec\u7acb\u5b8c\u6210\u590d\u6742\u7684\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\uff0c\u7279\u522b\u662f\u6784\u5efa\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684\u6d4f\u89c8\u5668\uff0c\u9a8c\u8bc1LLM\u5728\u4ee3\u7801\u751f\u6210\u548c\u7cfb\u7edf\u5f00\u53d1\u65b9\u9762\u7684\u80fd\u529b", "method": "\u4f7f\u7528\u5355\u4e2aLLM\u4ee3\u7406\u4f5c\u4e3a\u5f00\u53d1\u52a9\u624b\uff0c\u91c7\u7528\u4ece\u96f6\u5f00\u59cb\u7684\u5f00\u53d1\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528Rust\u6807\u51c6\u5e93\uff0c\u907f\u514d\u7b2c\u4e09\u65b9\u4f9d\u8d56\uff0c\u901a\u8fc7\u8fed\u4ee3\u5f00\u53d1\u9010\u6b65\u5b9e\u73b0\u6d4f\u89c8\u5668\u6838\u5fc3\u529f\u80fd", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u7840\u7684\u8de8\u5e73\u53f0HTML/CSS\u6d4f\u89c8\u5668\uff0c\u652f\u6301\u57fa\u672c\u7684\u7f51\u9875\u6e32\u67d3\u529f\u80fd\uff0c\u8bc1\u660e\u4e86\u5355\u4e2aLLM\u4ee3\u7406\u80fd\u591f\u5b8c\u6210\u590d\u6742\u7684\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1", "conclusion": "\u5355\u4e2aLLM\u4ee3\u7406\u5177\u5907\u72ec\u7acb\u5b8c\u6210\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e3aAI\u8f85\u52a9\u7f16\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5c55\u793a\u4e86LLM\u5728\u7cfb\u7edf\u7ea7\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u6f5c\u529b", "topic": "code agent"}}
