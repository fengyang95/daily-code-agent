<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.SE](#cs.SE) [Total: 3]
- [tldr.article](#tldr.article) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and Multi-Domain Contexts](https://arxiv.org/abs/2601.15479)
*Sydney Anuyah,Sneha Shajee-Mohan,Ankit-Singh Chauhan,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: 论文评估了13个开源LLM在文本中因果发现任务上的表现，发现现有模型在因果检测和提取方面存在显著缺陷，最佳模型准确率不足50%，尤其在复杂场景下表现更差。


<details>
  <summary>Details</summary>
Motivation: 为了安全地将大语言模型部署到生物医学等高风险领域，需要确保它们能够进行因果推理。论文旨在评估LLM在文本中因果发现这一基础任务上的能力。

Method: 使用12个多样化数据集构建基准测试，评估两种核心技能：因果检测（判断文本是否包含因果链接）和因果提取（提取具体的因果短语）。测试了多种提示方法，包括零样本、思维链和少样本上下文学习。

Result: 现有模型表现严重不足：最佳因果检测模型DeepSeek-R1-Distill-Llama-70B平均准确率仅49.57%，最佳因果提取模型Qwen2.5-Coder-32B-Instruct仅47.12%。模型在简单、显式、单句关系上表现最好，但在隐式关系、跨多句链接和多因果对等复杂场景下表现急剧下降。

Conclusion: 当前LLM在文本因果发现任务上存在重大缺陷，特别是在复杂现实场景中。论文提供了统一的评估框架和公开数据集，以促进该领域的进一步研究。

Abstract: The safe deployment of large language models (LLMs) in high-stakes fields like biomedicine, requires them to be able to reason about cause and effect. We investigate this ability by testing 13 open-source LLMs on a fundamental task: pairwise causal discovery (PCD) from text. Our benchmark, using 12 diverse datasets, evaluates two core skills: 1) \textbf{Causal Detection} (identifying if a text contains a causal link) and 2) \textbf{Causal Extraction} (pulling out the exact cause and effect phrases). We tested various prompting methods, from simple instructions (zero-shot) to more complex strategies like Chain-of-Thought (CoT) and Few-shot In-Context Learning (FICL).
  The results show major deficiencies in current models. The best model for detection, DeepSeek-R1-Distill-Llama-70B, only achieved a mean score of 49.57\% ($C_{detect}$), while the best for extraction, Qwen2.5-Coder-32B-Instruct, reached just 47.12\% ($C_{extract}$). Models performed best on simple, explicit, single-sentence relations. However, performance plummeted for more difficult (and realistic) cases, such as implicit relationships, links spanning multiple sentences, and texts containing multiple causal pairs. We provide a unified evaluation framework, built on a dataset validated with high inter-annotator agreement ($κ\ge 0.758$), and make all our data, code, and prompts publicly available to spur further research. \href{https://github.com/sydneyanuyah/CausalDiscovery}{Code available here: https://github.com/sydneyanuyah/CausalDiscovery}

</details>


### [2] [AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains](https://arxiv.org/abs/2601.15511)
*Adam Szelestey,Sofie van Engelen,Tianhao Huang,Justin Snelders,Qintao Zeng,Songgaojun Deng*

Main category: cs.CL

TL;DR: AdversaRiskQA：首个针对健康、金融和法律领域的对抗性事实性基准，评估LLM在对抗性虚假信息下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: LLM幻觉问题严重，特别是在高风险领域可能导致错误信息传播。现有研究缺乏高质量、领域特定的资源来评估模型在对抗性条件下的鲁棒性，且没有研究考察注入虚假信息对长文本事实性的影响。

Method: 引入AdversaRiskQA基准，包含健康和金融法律两个难度级别，提出两种自动化方法评估对抗性攻击成功率和长文本事实性，评估了六个开源和闭源LLM。

Result: Qwen3 (80B)在排除无意义响应后获得最高平均准确率，GPT-5保持稳定高准确率。性能随模型规模非线性增长，不同领域表现不同，难度级别差距随模型增大而缩小。长文本评估显示注入虚假信息与模型事实输出无显著相关性。

Conclusion: AdversaRiskQA为识别LLM弱点和开发更可靠的高风险应用模型提供了有价值的基准。

Abstract: Hallucination in large language models (LLMs) remains an acute concern, contributing to the spread of misinformation and diminished public trust, particularly in high-risk domains. Among hallucination types, factuality is crucial, as it concerns a model's alignment with established world knowledge. Adversarial factuality, defined as the deliberate insertion of misinformation into prompts with varying levels of expressed confidence, tests a model's ability to detect and resist confidently framed falsehoods. Existing work lacks high-quality, domain-specific resources for assessing model robustness under such adversarial conditions, and no prior research has examined the impact of injected misinformation on long-form text factuality.
  To address this gap, we introduce AdversaRiskQA, the first verified and reliable benchmark systematically evaluating adversarial factuality across Health, Finance, and Law. The benchmark includes two difficulty levels to test LLMs' defensive capabilities across varying knowledge depths. We propose two automated methods for evaluating the adversarial attack success and long-form factuality. We evaluate six open- and closed-source LLMs from the Qwen, GPT-OSS, and GPT families, measuring misinformation detection rates. Long-form factuality is assessed on Qwen3 (30B) under both baseline and adversarial conditions. Results show that after excluding meaningless responses, Qwen3 (80B) achieves the highest average accuracy, while GPT-5 maintains consistently high accuracy. Performance scales non-linearly with model size, varies by domains, and gaps between difficulty levels narrow as models grow. Long-form evaluation reveals no significant correlation between injected misinformation and the model's factual output. AdversaRiskQA provides a valuable benchmark for pinpointing LLM weaknesses and developing more reliable models for high-stakes applications.

</details>


### [3] [Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind](https://arxiv.org/abs/2601.15715)
*Zhitao He,Zongwei Lyu,Yi R Fung*

Main category: cs.CL

TL;DR: 提出了首个基于心理理论的学术反驳框架RebuttalAgent，通过建模审稿人心理状态、制定说服策略并生成策略驱动的回应，显著提升了学术反驳质量。


<details>
  <summary>Details</summary>
Motivation: 学术反驳是研究流程中重要但未被充分探索的挑战，现有方法仅模仿表面语言而缺乏换位思考能力，无法在信息不对称情况下进行有效的策略性沟通。

Method: 提出ToM-Strategy-Response（TSR）管道，通过心理理论建模审稿人心理状态；构建RebuttalBench大规模数据集；采用两阶段训练：监督微调+基于自奖励机制的强化学习；开发Rebuttal-RM专业评估器。

Result: RebuttalAgent在自动指标上比基础模型平均提升18.3%，在自动和人工评估中都优于先进专有模型；Rebuttal-RM评估器在评分一致性上超越GPT-4.1。

Conclusion: 基于心理理论的学术反驳框架能有效提升反驳质量，但生成内容仅供作者参考和启发，不能替代作者的批判性分析和回应。

Abstract: Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.

</details>


### [4] [Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model](https://arxiv.org/abs/2601.15892)
*Chenghao Fan,Wen Heng,Bo Li,Sichen Liu,Yuxuan Song,Jing Su,Xiaoye Qu,Kai Shen,Wei Wei*

Main category: cs.CL

TL;DR: Stable-DiffCoder是一个基于块扩散的代码模型，通过改进的训练策略在代码生成任务上超越了自回归模型，并在代码编辑、推理和低资源语言方面表现出优势。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLLMs）相比自回归模型具有非顺序生成和更好的数据复用优势，但现有代码DLLMs在相同预算下仍落后于AR模型。本文旨在探索扩散模型在代码生成任务上的潜力，并证明其可以超越AR模型。

Method: 提出Stable-DiffCoder，基于Seed-Coder架构，采用块扩散持续预训练（CPT）阶段，结合定制化的预热策略和块级裁剪噪声调度，实现高效知识学习和稳定训练。仅使用CPT和监督微调阶段。

Result: 在相同数据和架构下，Stable-DiffCoder在广泛的代码基准测试中整体优于其AR对应模型。仅使用CPT和SFT阶段，就超越了多种约8B参数的AR和DLLMs模型，证明扩散训练可以提升代码建模质量。

Conclusion: 扩散训练不仅能超越AR训练，其任意顺序建模能力还能改进结构化代码的编辑和推理，并通过数据增强有益于低资源编程语言。

Abstract: Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.

</details>


### [5] [LLM-in-Sandbox Elicits General Agentic Intelligence](https://arxiv.org/abs/2601.16206)
*Daixuan Cheng,Shaohan Huang,Yuxian Gu,Huatong Song,Guoxin Chen,Li Dong,Wayne Xin Zhao,Ji-Rong Wen,Furu Wei*

Main category: cs.CL

TL;DR: LLM-in-Sandbox 让大语言模型在代码沙盒（虚拟计算机）中探索，以激发非代码领域的通用智能。无需额外训练，强LLMs就能利用沙盒处理非代码任务，如获取外部知识、处理长上下文、执行脚本等。通过LLM-in-Sandbox-RL强化学习，仅使用非智能数据训练模型进行沙盒探索，可增强这些智能能力。


<details>
  <summary>Details</summary>
Motivation: 探索如何让大语言模型在非代码领域展现通用智能。通过代码沙盒环境，使LLMs能够像在真实计算机中一样操作，从而扩展其在数学、物理、化学、生物医学等专业领域的能力。

Method: 提出LLM-in-Sandbox框架，让LLMs在代码沙盒（虚拟计算机）中自主探索。包括两种方式：1) 无需训练的零样本方法，强LLMs自发利用沙盒功能；2) LLM-in-Sandbox-RL，仅使用非智能数据训练模型进行沙盒探索的强化学习方法。

Result: LLM-in-Sandbox在数学、物理、化学、生物医学、长上下文理解和指令跟随等任务上展现出强大的泛化能力。训练免费和训练后设置都取得了稳健表现。框架已开源为Python包，便于实际部署。

Conclusion: LLM-in-Sandbox成功地将代码沙盒作为通用智能的激发环境，使LLMs能够处理复杂的非代码任务。该方法为LLMs在专业领域的应用提供了新途径，并展示了通过沙盒环境增强模型能力的潜力。

Abstract: We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents](https://arxiv.org/abs/2601.15322)
*Raffi Khatchadourian*

Main category: cs.AI

TL;DR: 本文提出DFAH框架，用于评估金融领域LLM代理的轨迹确定性和证据条件忠实性，发现模型确定性与忠实性正相关，并提供三个金融基准测试


<details>
  <summary>Details</summary>
Motivation: LLM代理在监管审计回放中存在一致性问题：当要求用相同输入重现被标记的交易决策时，大多数部署无法返回一致结果。金融服务的工具使用代理需要确定性和忠实性保证。

Method: 引入确定性-忠实性保证框架（DFAH），测量工具使用代理的轨迹确定性和证据条件忠实性。在74种配置（12个模型、4个提供商）中进行实验，提供三个金融基准测试（合规分类、投资组合约束、数据运维异常）。

Result: 7-20B参数模型在非代理基线实验中达到100%确定性，而120B+模型需要3.7倍更大的验证样本才能达到同等统计可靠性。代理工具使用引入额外方差。确定性与忠实性呈正相关（r=0.45）。Tier 1模型在DFAH评估设置下达到审计回放要求的确定性水平。

Conclusion: DFAH框架能有效评估金融LLM代理的确定性和忠实性，发现确定性与能力正相关而非负相关，为金融监管审计提供了实用评估工具和基准。

Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.
  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.
  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.

</details>


### [7] [Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity](https://arxiv.org/abs/2601.15728)
*Hangle Hu,Chenyu Hou,Bin Cao,Ruizhe Li*

Main category: cs.AI

TL;DR: BIRD-Python 基准测试揭示Text-to-Python在数据检索中的可靠性问题，提出逻辑补全框架解决歧义，最终实现与Text-to-SQL相当的性能


<details>
  <summary>Details</summary>
Motivation: 现实数据分析越来越需要Python等通用编程语言处理文件数据和复杂工作流，但Text-to-Python在核心数据检索方面的可靠性相对成熟的SQL生态系统尚未充分探索

Method: 引入BIRD-Python基准进行跨范式评估，系统优化原始数据集减少标注噪声并统一执行语义；提出逻辑补全框架(LCF)，通过融入潜在领域知识解决歧义

Result: 性能差异主要源于缺失领域上下文而非代码生成固有局限；当这些差距被填补后，Text-to-Python能达到与Text-to-SQL相当的性能水平

Conclusion: Python可作为分析代理的可行基础，前提是系统能有效将模糊的自然语言输入锚定到可执行的逻辑规范中

Abstract: While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.

</details>


### [8] [MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation](https://arxiv.org/abs/2601.15487)
*Chandan Kumar Sahu,Premith Kumar Chilukuri,Matthew Hetrich*

Main category: cs.AI

TL;DR: 提出了MiRAGE框架，使用多智能体协作生成领域特定、多模态、多跳的RAG评估数据集，解决现有基准无法捕捉专业文档复杂性的问题。


<details>
  <summary>Details</summary>
Motivation: RAG技术向多模态、高风险企业应用的快速发展超过了领域特定评估基准的开发。现有数据集通常依赖通用领域语料库或纯文本检索，无法捕捉专业技术文档的复杂性，其中信息是多模态的，推理需要综合分散的证据。

Method: 引入MiRAGE多智能体框架，通过协作的专门智能体群生成经过验证的领域特定、多模态、多跳问答数据集。包括：递归上下文优化循环聚合分散证据、对抗性验证智能体保证事实基础、识别专家角色和相关领域的智能体来模拟专家认知工作流程。

Result: 在四个不同领域（法规、金融、定量生物学和新闻）的实证评估表明，MiRAGE生成的数据集具有显著更高的推理复杂性（>2.3平均跳数）和事实忠实度。消融研究表明，如果有图像文本描述，MiRAGE可以由LLM驱动，但视觉基础仍是前沿挑战。

Conclusion: 通过自动化创建反映专有语料库潜在主题结构的黄金标准评估数据集，MiRAGE为严格基准测试下一代信息检索系统提供了必要的基础设施。

Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.

</details>


### [9] [Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge](https://arxiv.org/abs/2601.15495)
*Yiyang Feng,Zeming Chen,Haotian Wu,Jiawei Zhou,Antoine Bosselut*

Main category: cs.AI

TL;DR: TRACK是一个新的基准测试，用于评估LLMs在多步推理中如何处理与参数知识冲突的新知识，发现提供更新事实反而会降低推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要关注单知识更新和事实回忆，缺乏评估知识更新如何影响下游推理，特别是当新知识与模型参数知识冲突时的传播问题。

Method: 提出TRACK基准测试，涵盖三个推理密集型场景（WIKI、CODE、MATH），引入多个现实冲突来模拟真实世界复杂性，评估LLMs在多步推理中传播冲突知识的能力。

Result: 实验结果显示，为模型提供更新事实进行推理反而比不提供更新事实表现更差，且随着提供更多更新事实，性能退化加剧。失败原因包括无法忠实整合更新事实，以及即使知识整合成功也存在有缺陷的推理。

Conclusion: TRACK为测量和指导未来在多步推理中传播冲突知识的研究提供了严谨的新基准测试。

Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.

</details>


### [10] [ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance](https://arxiv.org/abs/2601.15551)
*Bismack Tokoli,Luis Jaimes,Ayesha S. Dina*

Main category: cs.AI

TL;DR: ALIGNAgent是一个多智能体教育框架，通过集成知识估计、技能差距识别和针对性资源推荐，为个性化学习提供自适应循环。


<details>
  <summary>Details</summary>
Motivation: 现有个性化学习系统通常只专注于知识追踪、诊断建模或资源推荐中的某一项，缺乏将这些组件整合成连贯自适应循环的系统。

Method: 提出ALIGNAgent多智能体框架，包含技能差距智能体和推荐智能体。首先处理学生测验表现、成绩数据和偏好，使用概念级诊断推理生成主题级熟练度估计，识别具体误解和知识缺陷，然后检索与诊断缺陷对齐的偏好感知学习材料，实现持续反馈循环。

Result: 在两个本科计算机科学课程的真实数据集上进行实证评估，基于GPT-4o的智能体在知识熟练度估计方面达到0.87-0.90的精确度和0.84-0.87的F1分数，与实际考试表现验证一致。

Conclusion: ALIGNAgent通过集成知识估计、技能差距识别和资源推荐，有效实现了个性化学习，实证结果验证了其有效性。

Abstract: Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.

</details>


### [11] [CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models](https://arxiv.org/abs/2601.15628)
*Haibo Tong,Zeyang Yue,Feifei Zhao,Erliang Lin,Lu Jia,Ruolin Chen,Yinqian Sun,Qian Zhang,Yi Zeng*

Main category: cs.AI

TL;DR: CogToM是一个全面的、理论基础的基准测试，包含8000多个双语实例，涵盖46种范式，用于评估LLMs是否具有类似人类的心理理论能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要局限于错误信念任务等狭窄范式，无法全面捕捉人类认知机制的全貌，需要更全面的评估工具来研究LLMs是否真正具备人类心理理论能力。

Method: 开发CogToM基准测试，包含8000多个双语实例，涵盖46种范式，由49名人类标注者验证。系统评估22个代表性模型，包括GPT-5.1和Qwen3-Max等前沿模型。

Result: 评估显示模型性能存在显著异质性，在特定维度上存在持续瓶颈。基于人类认知模式的分析表明LLMs与人类认知结构可能存在差异。

Conclusion: CogToM为研究LLMs不断演化的认知边界提供了强有力的工具和视角，揭示了LLMs心理理论能力的局限性及其与人类认知的差异。

Abstract: Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.

</details>


### [12] [Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2601.15652)
*Manish Bhatt*

Main category: cs.AI

TL;DR: 提出基于神经科学启发的信号设计与监督学习的混合检测框架，用于检测LLM幻觉，相比现有方法数据效率高75倍、推理速度快1000倍，同时保持完全可解释性。


<details>
  <summary>Details</summary>
Motivation: LLM幻觉（看似合理但事实不忠实的生成）是高风险部署的关键障碍。现有检测方法依赖计算昂贵的外部检索循环或不透明的黑盒LLM法官（需要70B+参数），需要更高效、可解释的解决方案。

Method: 引入混合检测框架，结合神经科学启发的信号设计与监督学习。提取基于预测编码（量化对内部先验的惊讶）和信息瓶颈（测量扰动下信号保留）的可解释信号。通过系统消融研究，开发三个关键增强：实体聚焦吸收、上下文依从性、可证伪性评分。

Result: 在HaluBench（n=200，完美平衡）上，理论指导基线达到0.8017 AUROC，基础监督模型达到0.8274 AUROC，改进特征提升至0.8669 AUROC（4.95%增益）。使用比Lynx少75倍的训练数据（200 vs 15,000样本），推理速度快1000倍（5ms vs 5s），同时保持完全可解释性。

Conclusion: 信号架构中编码的领域知识相比扩展LLM法官提供更优的数据效率，通过轻量级（少于1M参数）、可解释的模型实现强性能，适合生产部署。关键负面发现：合理化信号无法区分幻觉，表明LLM为虚假前提生成连贯推理（"奉承"现象）。

Abstract: Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).
  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises ("Sycophancy").
  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.

</details>


### [13] [Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats](https://arxiv.org/abs/2601.15679)
*Ee Wei Seah,Yongsen Zheng,Naga Nikshith,Mahran Morsidi,Gabriel Waikin Loh Matienzo,Nigel Gay,Akriti Vij,Benjamin Chua,En Qi Ng,Sharmini Johnson,Vanessa Wilfred,Wan Sie Lee,Anna Davidson,Catherine Devine,Erin Zorer,Gareth Holvey,Harry Coppock,James Walpole,Jerome Wynee,Magda Dubois,Michael Schmatz,Patrick Keane,Sam Deverett,Bill Black,Bo Yan,Bushra Sabir,Frank Sun,Hao Zhang,Harriet Farlow,Helen Zhou,Lingming Dong,Qinghua Lu,Seung Jang,Sharif Abuadbba,Simon O'Callaghan,Suyu Ma,Tom Howroyd,Cyrus Fung,Fatemeh Azadi,Isar Nejadgholi,Krishnapriya Vishnubhotla,Pulei Xiong,Saeedeh Lohrasbi,Scott Buffett,Shahrear Iqbal,Sowmya Vajjala,Anna Safont-Andreu,Luca Massarelli,Oskar van der Wal,Simon Möller,Agnes Delaborde,Joris Duguépéroux,Nicolas Rolin,Romane Gallienne,Sarah Behanzin,Tom Seimandi,Akiko Murakami,Takayuki Semitsu,Teresa Tsukiji,Angela Kinuthia,Michael Michie,Stephanie Kasaon,Jean Wangari,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim*

Main category: cs.AI

TL;DR: 多国AI安全机构联合开展第三次智能体评估测试，重点关注方法学问题而非模型性能，涵盖信息泄露、欺诈等通用风险和网络安全两大领域。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统的快速发展和智能体能力的提升，由于现实世界交互的监督减少，引入了新的风险。智能体测试仍处于早期发展阶段，需要建立跨语言和文化的准确安全评估方法。

Method: 国际网络成员（新加坡、日本、澳大利亚、加拿大、欧盟委员会、法国、肯尼亚、韩国、英国）联合开展第三次测试，分为两个方向：(1) 通用风险（敏感信息泄露、欺诈），由新加坡AISI领导；(2) 网络安全，由英国AISI领导。使用公开和闭源模型，基于公共智能体基准任务进行评估。

Result: 由于智能体测试仍处于早期阶段，研究主要关注进行此类测试的方法学问题，而不是检查测试结果或模型能力。这次合作标志着参与者共同努力推进智能体评估科学的重要一步。

Conclusion: 多国合作开展智能体评估测试是推进智能体评估科学发展的重要步骤，重点关注方法学建设，为未来全球AI系统的安全部署奠定基础。

Abstract: The rapid rise of autonomous AI systems and advancements in agent capabilities are introducing new risks due to reduced oversight of real-world interactions. Yet agent testing remains nascent and is still a developing science. As AI agents begin to be deployed globally, it is important that they handle different languages and cultures accurately and securely.
  To address this, participants from The International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the European Commission, France, Kenya, South Korea, and the United Kingdom have come together to align approaches to agentic evaluations.
  This is the third exercise, building on insights from two earlier joint testing exercises conducted by the Network in November 2024 and February 2025. The objective is to further refine best practices for testing advanced AI systems.
  The exercise was split into two strands: (1) common risks, including leakage of sensitive information and fraud, led by Singapore AISI; and (2) cybersecurity, led by UK AISI. A mix of open and closed-weight models were evaluated against tasks from various public agentic benchmarks. Given the nascency of agentic testing, our primary focus was on understanding methodological issues in conducting such tests, rather than examining test results or model capabilities. This collaboration marks an important step forward as participants work together to advance the science of agentic evaluations.

</details>


### [14] [Agentic Uncertainty Quantification](https://arxiv.org/abs/2601.15703)
*Jiaxin Zhang,Prafulla Kumar Choubey,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 提出双过程代理不确定性量化框架，将语言化不确定性转化为主动控制信号，解决AI代理在长程推理中的"幻觉螺旋"问题


<details>
  <summary>Details</summary>
Motivation: AI代理在长程推理中表现出色，但可靠性受"幻觉螺旋"严重影响——早期认知错误会不可逆地传播。现有方法面临两难：不确定性量化方法通常只作为被动传感器诊断风险而不解决问题，而自我反思机制则存在持续或漫无目的的修正问题

Method: 提出统一的Dual-Process Agentic UQ框架，包含两个互补机制：系统1（不确定性感知记忆UAM）隐式传播语言化置信度和语义解释以防止盲目决策；系统2（不确定性感知反思UAR）利用这些解释作为理性线索，仅在必要时触发有针对性的推理时解决

Result: 在闭环基准测试和开放式深度研究任务上的广泛实验表明，这种无需训练的方法实现了优越的性能和轨迹级校准

Conclusion: AUQ框架代表了向可靠代理迈出的重要一步，使代理能够动态平衡高效执行和深度思考

Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.

</details>


### [15] [AgentSM: Semantic Memory for Agentic Text-to-SQL](https://arxiv.org/abs/2601.15709)
*Asim Biswal,Chuan Lei,Xiao Qin,Aodong Li,Balakrishnan Narayanaswamy,Tim Kraska*

Main category: cs.AI

TL;DR: AgentSM是一个用于Text-to-SQL的智能体框架，通过构建可解释的语义记忆来提升复杂企业场景下的效率和稳定性，相比现有方法减少25%的token使用和35%的轨迹长度，在Spider 2.0 Lite上达到44.8%的SOTA准确率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based Text-to-SQL系统在真实企业环境中面临挑战：大规模复杂模式、多样化SQL方言、昂贵的多步推理。现有智能体方法存在效率低下和不稳定的问题，如重复数据库交互、输出不一致、偶尔无法生成有效答案。

Method: 提出Agent Semantic Memory (AgentSM)框架，通过构建可解释的语义记忆来指导推理。不同于原始草稿或向量检索，AgentSM捕获先前的执行轨迹（或合成精心策划的轨迹）作为结构化程序，系统性地重用推理路径。

Result: 在Spider 2.0基准测试中，AgentSM平均减少25%的token使用和35%的轨迹长度。在Spider 2.0 Lite基准测试中达到44.8%的SOTA准确率，显著提升了执行准确率。

Conclusion: AgentSM通过语义记忆机制有效解决了Text-to-SQL智能体在复杂企业环境中的可扩展性、效率和稳定性问题，实现了更高效可靠的推理。

Abstract: Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.

</details>


### [16] [PhysProver: Advancing Automatic Theorem Proving for Physics](https://arxiv.org/abs/2601.15737)
*Hanning Zhang,Ruida Wang,Rui Pan,Wenyuan Wang,Bingxu Meng,Tong Zhang*

Main category: cs.AI

TL;DR: 该论文提出了首个增强物理领域形式化定理证明的方法，通过构建PhysLeanData数据集和基于RLVR训练的PhysProver模型，在物理子领域取得2.4%提升，并在数学基准上展现1.3%的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然可验证语言与LLMs的结合在数学定理证明领域取得显著进展，但形式化物理推理领域却鲜有关注。物理领域同样依赖类似的问题解决和定理证明框架，需要专门的方法来提升形式化物理定理证明能力。

Method: 1) 构建PhysLeanData数据集，包含从PhysLean采样的定理和基于猜想的形式化数据生成管道生成的数据；2) 基于DeepSeek-Prover-V2-7B数学定理证明器，应用可验证奖励的强化学习(RLVR)训练PhysProver模型。

Result: 仅使用约5K训练样本，PhysProver在多个物理子领域实现总体2.4%的改进。在MiniF2F-Test数学基准上获得1.3%提升，表明模型在物理领域之外的泛化能力和形式化数学能力的增强。

Conclusion: 该方法为将形式化证明器扩展到数学领域之外提供了范例，证明了方法的有效性和效率。作者将发布数据集和模型以促进进一步研究。

Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.

</details>


### [17] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: SigEnt-SAC是一种从零开始学习的离策略actor-critic方法，仅需单条专家轨迹，通过sigmoid有界熵项防止负熵驱动的OOD动作优化，减少Q函数振荡，在真实世界机器人任务中实现低成本RL部署。


<details>
  <summary>Details</summary>
Motivation: 真实世界强化学习面临样本效率低、奖励稀疏和视觉观测噪声等挑战。现有方法需要大量数据或大规模预训练，缺乏低成本、数据需求少的实用解决方案。

Method: 提出SigEnt-SAC离策略actor-critic方法，核心设计是sigmoid有界熵项，防止负熵驱动的分布外动作优化，减少Q函数振荡，仅需单条专家轨迹从零开始学习。

Result: 在D4RL基准测试中显著减轻Q函数振荡，比现有方法更快达到100%成功率。在四种真实世界机器人任务中，仅需少量真实交互就能学习成功策略。

Conclusion: SigEnt-SAC为真实世界RL部署提供了低成本、实用的途径，仅需少量数据和交互就能学习有效策略。

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [18] [Agentic Confidence Calibration](https://arxiv.org/abs/2601.15778)
*Jiaxin Zhang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 论文提出首个代理置信度校准问题，并开发了HTC框架，通过提取代理执行轨迹的宏观动态和微观稳定性特征，显著提升AI代理在复杂任务中的可靠性校准。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理从被动语言模型向自主系统演进，但在高风险场景部署面临过度自信问题。现有校准方法针对静态单轮输出设计，无法解决代理系统中的独特挑战，如轨迹中的错误累积、外部工具的不确定性以及不透明的失败模式。

Method: 提出整体轨迹校准（HTC）诊断框架，从代理的整个执行轨迹中提取丰富的流程级特征，涵盖宏观动态到微观稳定性。采用简单可解释的模型，并开发通用代理校准器（GAC）实现跨域泛化。

Result: HTC在8个基准测试、多个LLM和不同代理框架中，在校准和判别方面均超越强基线。GAC在跨域GAIA基准上实现了最佳校准（最低ECE）。

Conclusion: 建立了一个新的以流程为中心的置信度校准范式，为诊断和增强AI代理的可靠性提供了框架，实现了可解释性、可迁移性和泛化能力三大进展。

Abstract: AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.

</details>


### [19] [Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification](https://arxiv.org/abs/2601.15808)
*Yuxuan Wan,Tianqing Fang,Zaitang Li,Yintong Huo,Wenxuan Wang,Haitao Mi,Dong Yu,Michael R. Lyu*

Main category: cs.AI

TL;DR: 提出DeepVerifier：基于规则的结果奖励验证器，通过推理时验证实现智能体自我进化，在GAIA和XBench-DeepResearch上提升8%-11%准确率


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体（DRAs）主要关注通过后训练增强策略能力，但缺乏在推理时自我进化的机制。需要一种能够系统评估智能体输出并产生迭代反馈的方法。

Method: 1. 基于自动构建的DRA失败分类法制定评估规则；2. 开发DeepVerifier规则验证器，利用验证的不对称性；3. 作为即插即用模块在推理时集成；4. 生成详细规则反馈供智能体迭代优化；5. 发布DeepVerifier-4K高质量监督微调数据集

Result: 1. DeepVerifier在元评估F1分数上比基准方法提升12%-48%；2. 在GAIA和XBench-DeepResearch的挑战性子集上实现8%-11%准确率提升；3. 支持开源模型发展验证能力

Conclusion: 提出了一种新的智能体自我进化范式，通过推理时验证实现能力提升，无需额外训练。验证器的高效性和数据集的开源发布为深度研究智能体发展提供了新方向。

Abstract: Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

</details>


### [20] [ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models](https://arxiv.org/abs/2601.15812)
*Shir Ashury-Tahan,Yifan Mai,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.AI

TL;DR: ErrorMap是一种分析LLM失败原因的方法，通过提取模型的"失败签名"来识别错误来源，创建了ErrorAtlas错误分类法，揭示了当前研究未充分探索的错误类型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试只能告诉我们模型何时失败，但不能解释为什么失败。错误的答案可能源于格式问题、计算错误或数据集噪声，而非推理能力弱。如果不区分这些原因，基准测试就不完整，无法可靠指导模型改进。

Method: ErrorMap方法提取模型的独特"失败签名"，澄清基准测试测量内容，扩大错误识别范围以减少盲点。该方法适用于任何模型或数据集，使用相同逻辑。作者将其应用于35个数据集和83个模型，生成了ErrorAtlas错误分类法。

Result: ErrorAtlas揭示了模型错误的重复模式，突出了当前LLM研究中未充分探索的错误类型，如输出中遗漏必要细节和问题误解。通过将焦点从模型成功之处转移到失败原因，实现了更高级的评估。

Conclusion: ErrorMap和ErrorAtlas实现了超越任务级指标的成功衡量，引入了可在模型和任务间全局应用的更深层评估，提供了对模型行为和局限性的更丰富洞察。

Abstract: Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique "failure signature", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.

</details>


### [21] [EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience](https://arxiv.org/abs/2601.15876)
*Taofeng Xue,Chong Peng,Mianqiu Huang,Linsen Guo,Tiancheng Han,Haozhe Wang,Jianing Wang,Xiaocheng Zhang,Xin Yang,Dengchang Zhao,Jinrui Ding,Xiandi Ma,Yuchen Xie,Peng Pei,Xunliang Cai,Xipeng Qiu*

Main category: cs.AI

TL;DR: EvoCUA：通过进化学习循环（数据生成+策略优化）突破静态数据限制，实现原生计算机使用代理，在OSWorld基准上达到56.7%成功率，创开源SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前原生计算机使用代理（CUA）受限于静态数据扩展瓶颈，依赖静态数据集被动模仿难以捕捉长时程计算机任务中的复杂因果动态。

Method: 1）可验证合成引擎自动生成多样化任务及可执行验证器；2）可扩展基础设施协调数万个异步沙箱环境；3）迭代进化学习策略，通过识别能力边界动态调节策略更新，将失败轨迹转化为监督信号。

Result: 在OSWorld基准上达到56.7%成功率，超越之前最佳开源模型OpenCUA-72B（45.0%）和领先闭源模型UI-TARS-2（53.1%）。进化范式在不同规模基础模型上均带来一致性能提升。

Conclusion: 基于经验学习的进化范式为提升原生代理能力提供了稳健可扩展的路径，突破了静态模仿的限制。

Abstract: The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.

</details>


### [22] [Decoupling Return-to-Go for Efficient Decision Transformer](https://arxiv.org/abs/2601.15953)
*Yongyi Wang,Hanyu Liu,Lingfeng Li,Bozhou Chen,Ang Li,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 本文提出解耦决策变换器（DDT），通过消除RTG序列冗余，仅使用最新RTG指导动作预测，从而提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 决策变换器（DT）在离线强化学习中采用序列建模方法，但存在设计冗余：将整个RTG序列输入变换器理论上是不必要的，因为只有最新的RTG影响动作预测。这种冗余可能损害DT的性能。

Method: 提出解耦决策变换器（DDT），简化架构：仅通过变换器处理观测和动作序列，使用最新的RTG来指导动作预测，从而消除RTG序列的冗余。

Result: 实验表明，DDT显著优于原始DT，并在多个离线RL任务中与最先进的DT变体相比具有竞争力。同时减少了计算成本。

Conclusion: 通过消除RTG序列冗余，DDT提供了一种更高效、性能更好的决策变换器设计，为离线强化学习的序列建模方法提供了改进方向。

Abstract: The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.

</details>


### [23] [Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics](https://arxiv.org/abs/2601.16087)
*Sukesh Subaharan*

Main category: cs.AI

TL;DR: 该研究探索了在LLM代理中引入外部情感动态子系统（基于VAD模型）来增强多轮对话中的时间一致性和可控恢复能力，通过一阶和二阶动态规则实现情感状态的持续演化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在长时间交互中经常出现语气和角色的突然转变，缺乏明确的时间结构来管理代理层面的状态。虽然先前工作关注回合层面的情感或静态情感分类，但显式情感动态在塑造长期代理行为中的作用尚未充分探索。

Method: 引入代理层面的情感子系统，维护外部于语言模型的连续VAD（效价-唤醒-支配）状态，通过一阶和二阶更新规则进行管理。使用固定的无记忆估计器提取瞬时情感信号，并通过指数平滑或基于动量的动态进行时间积分。生成过程中注入情感状态而不修改模型参数。

Result: 使用固定的25轮对话协议比较无状态、一阶和二阶情感动态：无状态代理无法展现连贯轨迹或恢复；状态持续性支持延迟响应和可靠恢复；二阶动态引入情感惯性和滞后效应，随着动量增加，揭示了稳定性与响应性之间的权衡。

Conclusion: 在LLM代理中施加明确的情感动态结构可以增强多轮对话的时间一致性和可控恢复能力，二阶动态提供了情感惯性和滞后效应，但需要在稳定性和响应性之间进行权衡。

Abstract: Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.

</details>


### [24] [Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning](https://arxiv.org/abs/2601.16163)
*Moo Jin Kim,Yihuai Gao,Tsung-Yi Lin,Yen-Chen Lin,Yunhao Ge,Grace Lam,Percy Liang,Shuran Song,Ming-Yu Liu,Chelsea Finn,Jinwei Gu*

Main category: cs.AI

TL;DR: Cosmos Policy：一种简单方法，通过单阶段后训练将预训练视频模型（Cosmos-Predict2）适配为机器人策略，无需架构修改，直接生成编码为潜在帧的机器人动作，并在LIBERO和RoboCasa基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型已能捕捉复杂物理交互和场景演化，但将其用于机器人策略学习需要多阶段后训练和新架构组件，过程复杂。本文旨在简化这一过程，利用预训练视频模型的时空先验进行高效策略学习。

Method: 通过单阶段后训练将预训练视频模型（Cosmos-Predict2）适配为机器人策略，无需架构修改。模型学习直接生成编码为潜在帧的机器人动作，同时生成未来状态图像和值函数（预期累积奖励），支持测试时规划高成功率动作轨迹。

Result: 在LIBERO和RoboCasa仿真基准测试中分别达到98.5%和67.1%的平均成功率，在真实世界双手操作任务中获得最高平均分数，优于从头训练的扩散策略、基于视频模型的策略以及在同一演示数据上微调的VLA模型。

Conclusion: Cosmos Policy证明了通过简单后训练即可有效利用预训练视频模型的时空先验进行机器人策略学习，无需复杂架构修改，且在仿真和真实世界任务中均表现出色，还能通过经验学习改进世界模型和值函数。

Abstract: Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding](https://arxiv.org/abs/2601.15482)
*Huayu Li,ZhengXiao He,Siyuan Tian,Jinghao Wen,Ao Li*

Main category: cs.LG

TL;DR: MFS将LLM解码重构为识别最优随机过程的问题，利用鞅理论设计理论基础的算法，在推理基准上超越SOTA方法同时提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 标准自回归解码是短视的，无法找到全局最优推理路径。现有的前瞻采样方法依赖启发式机制进行路径评估和剪枝，缺乏理论依据。

Method: 提出鞅前瞻采样(MFS)框架，将LLM解码建模为识别最优随机过程的问题。利用鞅理论：1) 基于Doob分解定理推导步骤评估；2) 使用可选停止理论进行路径选择；3) 基于鞅收敛定理设计自适应停止规则。

Result: 在六个推理基准上的实验表明，MFS在准确性上超越了最先进的方法，同时显著提高了计算效率。

Conclusion: MFS为LLM解码提供了一个理论基础的框架，用概率论原理替代启发式机制，在推理任务中实现了更好的性能和效率。

Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

</details>


### [26] [Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors](https://arxiv.org/abs/2601.15625)
*Zhiwei Zhang,Fei Zhao,Rui Wang,Zezhong Wang,Bin Liang,Jiakang Wang,Yao Hu,Shaosheng Cao,Kam-Fai Wong*

Main category: cs.LG

TL;DR: 提出Fission-GRPO框架，通过将执行错误转化为纠正监督，解决LLM在多轮工具调用中遇到错误后无法有效恢复的问题，显著提升错误恢复率和整体准确率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在工具调用中遇到错误时表现脆弱，容易陷入重复无效调用而无法根据错误反馈进行自我纠正。标准RL方法将错误视为稀疏负奖励，缺乏恢复指导；预收集的错误纠正数据集存在分布不匹配问题。

Method: 提出Fission-GRPO框架，核心机制是将每个失败轨迹分裂为新的训练实例：通过微调的错误模拟器提供诊断反馈，然后在策略上重新采样恢复轨迹，使模型能从探索过程中产生的具体错误中学习。

Result: 在BFCL v4 Multi-Turn基准上，Fission-GRPO将Qwen3-8B的错误恢复率提升5.7%，整体准确率从42.75%提升至46.75%，优于GRPO和专门的工具使用代理。

Conclusion: Fission-GRPO通过将执行错误转化为纠正监督，有效解决了LLM在多轮工具调用中的错误恢复问题，为可靠的实际部署提供了解决方案。

Abstract: Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.

</details>


### [27] [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](https://arxiv.org/abs/2601.15801)
*Fengheng Chu,Jiahao Chen,Yuhong Wang,Jun Wang,Zhihui Fu,Shouling Ji,Songze Li*

Main category: cs.LG

TL;DR: 提出GOSV框架，通过全局优化识别安全关键注意力头，发现对齐LLMs中存在恶意注入向量和安全抑制向量两种空间分离的安全向量，并基于此开发了新的白盒越狱攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖局部贪心归因，假设组件独立贡献，忽视了LLM中不同组件（如注意力头）之间的协同交互对安全机制的影响，导致对安全组件理解有限。

Method: 提出GOSV框架，通过全局优化同时对所有注意力头进行优化，采用有害补丁和零消融两种互补的激活重补丁策略，识别安全关键注意力头。

Result: 发现对齐LLMs中存在空间分离的两组安全向量（恶意注入向量和安全抑制向量），当约30%的总头被重补丁时会发生完全安全崩溃，基于此开发的白盒越狱攻击在所有测试模型上显著优于现有方法。

Conclusion: GOSV框架有效提升了LLM安全可解释性，揭示了LLMs通过分离功能通路维护安全机制，为安全分析和防御提供了新见解。

Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for \textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.

</details>


### [28] [Learning to Discover at Test Time](https://arxiv.org/abs/2601.16175)
*Mert Yuksekgonul,Daniel Koceja,Xinhao Li,Federico Bianchi,Jed McCaleb,Xiaolong Wang,Jan Kautz,Yejin Choi,James Zou,Carlos Guestrin,Yu Sun*

Main category: cs.LG

TL;DR: TTT-Discover：通过测试时强化学习让LLM针对特定问题持续训练，在数学、GPU内核、算法设计和生物学等多个领域实现SOTA突破。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法（如AlphaEvolve）使用冻结的LLM进行搜索，无法针对特定测试问题持续学习。需要一种方法让LLM在测试时通过强化学习获得特定问题的经验，专注于生成单个优秀解决方案而非平均表现。

Method: 提出TTT-Discover方法：在测试时进行强化学习，设计学习目标和搜索子程序以优先考虑最有希望的解决方案。使用开放模型OpenAI gpt-oss-120b，通过Tinker API进行测试时训练，成本仅每问题几百美元。

Result: 在几乎所有尝试的问题上都实现了新的SOTA：1) Erdős最小重叠问题和自相关不等式；2) GPUMode内核竞赛（比现有技术快达2倍）；3) 过去的AtCoder算法竞赛；4) 单细胞分析中的去噪问题。所有结果均可通过公开代码复现。

Conclusion: TTT-Discover通过测试时训练成功发现多个科学问题的新SOTA解决方案，证明了测试时强化学习在科学发现中的有效性，且使用开放模型和可复现代码，相比依赖封闭前沿模型的方法更具优势。

Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [29] [Lost in Transcription: How Speech-to-Text Errors Derail Code Understanding](https://arxiv.org/abs/2601.15339)
*Jayant Havare,Ashish Mittal,Srikanth Tamilselvam,Ganesh Ramakrishnan*

Main category: cs.SE

TL;DR: 开发了一个多语言语音驱动的代码理解框架，支持用户用母语进行语音查询，通过ASR转录和LLM引导的代码感知ASR输出优化，显著提升了代码问答和检索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码理解系统主要面向英语用户和键盘交互，限制了多语言和语音优先场景的可访问性，特别是在印度等地区。语音界面虽然更具包容性，但涉及代码的语音查询面临非标准英语用法、领域特定词汇和自定义标识符等独特挑战。

Method: 开发了一个多语言语音驱动的代码理解框架：1）接受用户母语的语音查询；2）使用自动语音识别（ASR）进行转录；3）利用大型语言模型（LLMs）进行代码感知的ASR输出优化；4）与代码模型接口，在CodeSearchNet、CoRNStack和CodeQA等基准上执行代码问答和检索任务。

Result: 系统分析了四种广泛使用的印度语言和英语，系统性地描述了转录错误对下游任务性能的影响。识别了ASR在代码处理中的关键失败模式，并证明LLM引导的优化显著提升了转录和代码理解阶段的性能。

Conclusion: 研究强调了语音界面需要代码敏感的适应性改进，并为构建鲁棒的多语言语音驱动编程工具提供了实用解决方案。

Abstract: Code understanding is a foundational capability in software engineering tools and developer workflows. However, most existing systems are designed for English-speaking users interacting via keyboards, which limits accessibility in multilingual and voice-first settings, particularly in regions like India. Voice-based interfaces offer a more inclusive modality, but spoken queries involving code present unique challenges due to the presence of non-standard English usage, domain-specific vocabulary, and custom identifiers such as variable and function names, often combined with code-mixed expressions. In this work, we develop a multilingual speech-driven framework for code understanding that accepts spoken queries in a user native language, transcribes them using Automatic Speech Recognition (ASR), applies code-aware ASR output refinement using Large Language Models (LLMs), and interfaces with code models to perform tasks such as code question answering and code retrieval through benchmarks such as CodeSearchNet, CoRNStack, and CodeQA. Focusing on four widely spoken Indic languages and English, we systematically characterize how transcription errors impact downstream task performance. We also identified key failure modes in ASR for code and demonstrated that LLM-guided refinement significantly improves performance across both transcription and code understanding stages. Our findings underscore the need for code-sensitive adaptations in speech interfaces and offer a practical solution for building robust, multilingual voice-driven programming tools.

</details>


### [30] [A Prompt-Based Framework for Loop Vulnerability Detection Using Local LLMs](https://arxiv.org/abs/2601.15352)
*Adeyemi Adeseye,Aisvarya Adeseye*

Main category: cs.SE

TL;DR: 提出基于提示的本地LLM框架，用于检测Python代码中的循环漏洞，包括控制逻辑错误、安全风险和资源管理问题，测试显示Phi模型优于LLaMA。


<details>
  <summary>Details</summary>
Motivation: 循环漏洞是软件开发中的主要风险构造，容易导致无限循环、资源耗尽或逻辑错误，传统静态分析器基于语法模式难以检测语义缺陷，而本地LLM能解决隐私、延迟和依赖问题。

Method: 设计基于提示的框架，利用本地部署的LLM（LLaMA 3.2 3B和Phi 3.5 4B），通过迭代提示引导模型行为，框架包含语言特定意识、代码感知基础、版本敏感性和幻觉预防等安全特性。

Result: Phi模型在精确率、召回率和F1分数上优于LLaMA，验证了本地LLM在代码漏洞分析中的有效性，强调了设计有效提示的重要性。

Conclusion: 研究表明本地LLM能够有效检测循环漏洞，设计良好的提示框架对于实现安全准确的代码漏洞分析至关重要。

Abstract: Loop vulnerabilities are one major risky construct in software development. They can easily lead to infinite loops or executions, exhaust resources, or introduce logical errors that degrade performance and compromise security. The problem are often undetected by traditional static analyzers because such tools rely on syntactic patterns, which makes them struggle to detect semantic flaws. Consequently, Large Language Models (LLMs) offer new potential for vulnerability detection because of their ability to understand code contextually. Moreover, local LLMs unlike commercial ones like ChatGPT or Gemini addresses issues such as privacy, latency, and dependency concerns by facilitating efficient offline analysis. Consequently, this study proposes a prompt-based framework that utilize local LLMs for the detection of loop vulnerabilities within Python 3.7+ code. The framework targets three categories of loop-related issues, such as control and logic errors, security risks inside loops, and resource management inefficiencies. A generalized and structured prompt-based framework was designed and tested with two locally deployed LLMs (LLaMA 3.2; 3B and Phi 3.5; 4B) by guiding their behavior via iterative prompting. The designed prompt-based framework included key safeguarding features such as language-specific awareness, code-aware grounding, version sensitivity, and hallucination prevention. The LLM results were validated against a manually established baseline truth, and the results indicate that Phi outperforms LLaMA in precision, recall, and F1-score. The findings emphasize the importance of designing effective prompts for local LLMs to perform secure and accurate code vulnerability analysis.

</details>


### [31] [Evaluating and Achieving Controllable Code Completion in Code LLM](https://arxiv.org/abs/2601.15879)
*Jiajun Zhang,Zeyu Cui,Lei Zhang,Jian Yang,Jiaxi Yang,Qiang Liu,Zilei Wang,Binyuan Hui,Liang Wang,Junyang Lin*

Main category: cs.SE

TL;DR: C3-Bench：首个指令引导的代码补全基准，包含2195个任务，评估LLM在代码补全中的指令跟随能力，发现开源与专有模型存在显著差距，并提出数据合成方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码补全基准主要关注功能正确性，忽略了LLM在代码补全中遵循用户指令的能力，而这是LLM辅助编程的常见场景。

Method: 构建C3-Bench基准（2195个任务），评估40多个主流LLM，开发基于Qwen2.5-Coder的数据合成管道生成高质量指令-补全对进行SFT微调。

Result: 开源与先进专有模型在代码补全的指令跟随能力上存在显著差距；Qwen2.5-Coder-C3模型在C3-Bench上达到SOTA性能。

Conclusion: 研究为增强LLM的代码补全和指令跟随能力提供了宝贵见解，建立了代码LLM未来研究的新方向，所有代码、数据集和模型均已开源。

Abstract: Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [32] [AI code review with comments you'll actually implement](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview_260122secondary/1/0100019be5739d20-829217f4-610a-460f-80d6-19705caddc21-000000/9a3lTqUbjFVR3AJa9m4A7PbkLTPWODaMRx48rI_JLA8=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Unblocked是一款AI代码审查工具，专注于识别真实问题和提供有价值的反馈，而非琐碎的风格建议


<details>
  <summary>Details</summary>
Motivation: 解决现有AI代码审查工具过度关注代码风格细节，导致PR中充斥低价值评论的问题，提供真正有意义的上下文感知反馈

Method: 开发了能够理解完整代码库上下文的AI系统，专注于识别实际问题和提供可实施的建议

Result: 获得了开发者的积极评价，特别是资深开发者认为该工具提供了只有了解完整代码库的人才能给出的上下文建议

Conclusion: Unblocked通过专注于真实问题和上下文感知的反馈，有效解决了AI代码审查中的价值问题，改变了开发者对AI工具的疲劳态度

Abstract: AI code review with comments you'll actually implement (Sponsor) Unblocked is the AI code review that surfaces real issues and meaningful feedback instead of flooding your PRs with stylistic nitpicks and low-value comments. “Unblocked made me reconsider my AI fatigue. Finally, a tool that surfaces context only someone with a full view of the codebase could provide.” - Senior developer, Clio Try now for free

</details>


### [33] [Korey is the AI agent for engineering work that ISN'T coding](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.korey.ai%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=jan-26/2/0100019be59bb3fe-fdc5bab6-3ab9-43dc-b96b-cf0f94e7a428-000000/0fSgOq9OvYpa77BcvlrXRmhhLJ3p8U62KBsktGVXGnU=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Korey是一个专注于非编码工程工作的AI代理，通过优化开发流程来帮助工程师更快交付产品


<details>
  <summary>Details</summary>
Motivation: 当前AI工具主要关注代码编写，但工程师工作中存在大量非编码的行政和管理任务（如编写规范、创建工单、状态更新等），这些任务占据了大量时间，影响了开发效率

Method: Korey从开发者已使用的工具中提取上下文信息，自动化处理非编码任务，包括：编写和更新规范、创建工单、起草状态更新等

Result: Korey帮助工程师减少行政任务负担，让他们专注于更有价值的编码工作，从而提高整体开发效率

Conclusion: 专注于非编码工程任务的AI代理能够有效优化开发流程，解决工程师面临的行政负担问题，提升团队生产力

Abstract: Korey is the AI agent for engineering work that ISN'T coding (Sponsor) Ever feel like AI is trying to automate the best part of your job, and leaving you with piles of admin?Korey is the AI agent that helps you ship faster by optimizing your process, not writing your code. Korey pulls context from the developer tools you're already using and handles all of the non-glamorous stuff: Writing and updating specs Creating tickets Drafting status updates Basically - everything that fills up your day...

</details>


### [34] [Agent Skills vs. Rules vs. Commands](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fagent-skills-rules-commands%3Futm_source=tldrdev/1/0100019be59bb3fe-fdc5bab6-3ab9-43dc-b96b-cf0f94e7a428-000000/wSG7PZ3jIBuxRreapq10KLS2DUFTEgBQHUwh8okDRvs=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该文章区分了AI代理的三种指令类型：技能（按需加载的Markdown文件）、规则（始终适用的硬性约束）和命令（用户输入的特定快捷指令。


<details>
  <summary>Details</summary>
Motivation: 为了澄清AI代理系统中不同类型的指令和约束，帮助用户更好地理解和使用代理功能，避免混淆技能、规则和命令的概念。

Method: 通过概念分析和分类方法，将AI代理的指令系统分为三类：技能（动态加载的特定能力）、规则（始终生效的系统约束）和命令（用户触发的快捷操作）。

Result: 提出了清晰的AI代理指令分类框架，明确了技能、规则和命令在代理系统中的不同作用和实现方式。

Conclusion: 理解这三种指令类型的区别对于有效使用AI代理系统至关重要，每种类型在代理工作流中扮演不同的角色。

Abstract: Agent Skills vs. Rules vs. Commands (10 minute read) Agent skills are markdown files that the AI only loads when it actually needs them (like lazy-loading, but for AI instructions). Rules are the hard rules that always apply (like “don't commit .env files”). Commands are shortcuts you type when you want something specific (like `/release`).

</details>


### [35] [Making AI agents work in the real world ⚙️](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frefactoring.fm%2Fp%2Fmaking-ai-agents-work-in-the-real%3Futm_source=tldrdev/1/0100019be59bb3fe-fdc5bab6-3ab9-43dc-b96b-cf0f94e7a428-000000/-qhtLm66wDMzOebVOsGBIAQh1wJcUu79hb2SzUfCvsU=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI智能体在实际部署中常因多步工作流中的累积错误等问题而失败，需要构建包含模型、编排器和外部工具的完整系统，而不仅仅是强大的"大脑"


<details>
  <summary>Details</summary>
Motivation: 许多AI智能体部署未能产生实质性影响，经常停留在试点阶段，主要挑战包括多步工作流中的累积错误、缺乏完整的系统架构等

Method: 构建包含模型、编排器和外部工具的完整AI智能体系统，从狭窄和具体的应用场景开始，采用渐进式方法，而非一次性构建复杂系统

Result: 通过构建完整的系统架构和采用渐进式部署策略，可以提高AI智能体在实际生产环境中的成功率和影响力

Conclusion: AI智能体要在实际应用中取得成功，需要超越单纯依赖强大模型，构建包含编排、工具集成和错误处理的完整系统，并采用务实、渐进式的部署策略

Abstract: Making AI agents work in the real world ⚙️ (15 minute read) Many AI agent deployments fail to achieve material impact, often remaining stuck in pilot mode due to challenges like compounding errors in multi-step workflows. AI agents, unlike chatbots or copilots, take autonomous actions and require a robust system encompassing the model, orchestrator, and external tools, not just a capable "brain." To succeed in production, teams must be realistic about agent capabilities, start with narrow and...

</details>


### [36] [AI is writing more code. Someone still has to review it.](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2F%3Futm_source=tldr_ai%26utm_medium=sponsorship%26utm_campaign=tldr_ai/2/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/K_aa9jNRNGmh5Gb6Oyvwjs3D9dOF1mwKG8KAzkVDs3Q=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Greptile是一款AI代码审查工具，利用完整仓库上下文学习团队规范，自动标记问题并提供符合团队习惯的修复建议


<details>
  <summary>Details</summary>
Motivation: 随着AI编写代码增多，人工代码审查成为瓶颈，容易遗漏bug、丢失上下文、降低团队效率，需要自动化工具来改进审查流程

Method: 通过分析完整仓库上下文，从团队评论、反应和合并记录中学习团队规范，提供个性化的代码审查建议

Result: 已集成到Claude Code插件中，开源项目免费使用，被NVIDIA等工程团队采用

Conclusion: Greptile通过AI驱动的个性化代码审查，解决了传统代码审查中的效率和质量问题

Abstract: AI is writing more code. Someone still has to review it. (Sponsor) That review step is where bugs slip through, context gets lost, and teams slow down.Greptile reviews each PR with full repo context and learns your team's conventions over time from comments, reactions, and what gets merged. It flags issues and suggests fixes that match your team, not generic best practices. ✅ Now integrated with Claude Code: install via /plugin. ✅ Free for open source. ✅ Trusted by engineering teams at NVIDIA...

</details>


### [37] [GLM4-MoE Inference with SGLang](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flmsys.org%2Fblog%2F2026-01-21-novita-glm4%2F%3Futm_source=tldrai/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/tw4kFFK_Yz_c5DbbK1f9ioZtIZiLUJWQe_0V0RGrLIs=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Novita AI使用SGLang优化GLM4-MoE模型推理，在代理编码工作负载下实现了更快的首次令牌时间和更好的令牌生成速度


<details>
  <summary>Details</summary>
Motivation: GLM4-MoE模型在代理编码任务中需要更高效的推理性能，特别是减少首次令牌延迟和提高令牌生成速度，以改善用户体验和系统响应性

Method: 采用SGLang框架对GLM4-MoE模型进行性能优化，通过特定的推理优化技术提升模型在代理编码工作负载下的执行效率

Result: 实现了更快的Time-to-First-Token和更好的token生成速度，在代理编码工作负载下显著提升了模型推理性能

Conclusion: SGLang框架能有效优化GLM4-MoE模型的推理性能，特别是在代理编码场景下，为实际应用提供了更好的响应速度和吞吐量

Abstract: GLM4-MoE Inference with SGLang (11 minute read) Novita AI introduced performance optimizations for GLM4-MoE models using SGLang, achieving faster Time-to-First-Token and better token generation speed under agentic coding workloads.

</details>


### [38] [Claude Codes #3](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthezvi.substack.com%2Fp%2Fclaude-codes-3%3Futm_source=tldrai/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/ySHdE6Si8nUFwN5sOe5ZE1WZGdDWw9v3XZIQ18LN5Uw=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 这是一篇关于Claude Code的综述性文章，包含新闻、教程、技巧和工具介绍，旨在帮助读者提升Claude Code技能并了解技术发展趋势。


<details>
  <summary>Details</summary>
Motivation: 帮助开发者了解和掌握Claude Code技术，提供全面的学习资源和行业动态，促进AI编程助手技术的应用和发展。

Method: 通过整理和精选Claude Code相关的新闻、教程、技巧和工具，构建一个综合性的学习资源集合，包括技术更新、配套工具介绍以及技能提升建议。

Result: 提供了一个全面的Claude Code学习指南，包含最新技术动态、实用工具推荐、技能提升方法和未来趋势预测，为开发者提供了有价值的学习资源。

Conclusion: Claude Code作为AI编程助手正在快速发展，通过系统学习和掌握相关技能，开发者可以显著提升编程效率，该文章为这一学习过程提供了重要支持。

Abstract: Claude Codes #3 (24 minute read) This post contains a curated list of news, tutorials, tips, and articles on Claude Code. It covers recent upgrades, tools that complement Claude Code, and more. The post provides advice on how to skill up with Claude Code as well as predictions on where the technology is headed.

</details>


### [39] [Devin Review: AI to Stop Slop](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fdevin-review%3Futm_source=tldrai/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/QVSftfsEAmQcvG4W02ArMKLXdIPAw6GKQcWc2G4-sM8=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Devin Review是一款利用AI和用户体验设计来扩展人类对复杂代码差异理解的代码审查工具，支持任何GitHub PR，帮助开发者在不离开审查界面的情况下讨论变更。


<details>
  <summary>Details</summary>
Motivation: 解决代码审查过程中理解复杂代码差异的挑战，提高代码审查效率，让开发者能够更轻松地讨论和审查代码变更。

Method: 结合人工智能和用户体验设计，创建集成到GitHub PR流程中的代码审查工具，支持开发者直接在审查界面中进行聊天讨论。

Result: 目前免费提供，支持任何公共或私有GitHub PR，帮助开发者完成PR流程的每一步，提供无缝的代码审查体验。

Conclusion: Devin Review通过AI辅助的代码审查工具，简化了代码审查流程，提高了开发效率，为团队协作提供了更好的支持。

Abstract: Devin Review: AI to Stop Slop (4 minute read) Devin Review is a code review tool that uses AI and UX to scale human understanding of complex code diffs. It is currently free and works on any public or private GitHub PR. The tool helps in every step of the PR process. It allows developers to chat about changes without leaving the review.

</details>


### [40] [A detailed guide to building agentic systems](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.algolia.com%2Fresources%2Fasset%2Fbuilding-agentic-ai%3Futm_campaign=tldr_global_b2x_ecomm_ecomm_tof_reach%26utm_medium=display%26utm_source=tldr%26utm_content=tldr_global_b2x_ecomm%26utm_term=ebo-building-agentic-ai%26utm_camp_parent=b2x_ecomm%26utm_2nd_camp=ecomm_tof%26utm_region=global%26utm_goal=reach%26utm_creative_format=prmrynwsl%26utm_model=cpm%26utm_marketing_tactic=reach/1/0100019be613364d-f3ffbaab-52d1-44bd-b68b-eb4f9a949698-000000/ddn_Ojo8eb7pBB6iXz3LIGMuCvR6GSlPqkAdXLE28Wc=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Algolia发布白皮书，提供构建AI代理系统的详细指南，帮助开发者在截止日期前完成项目


<details>
  <summary>Details</summary>
Motivation: AI代理系统开发中常遇到定制集成复杂、耗时的问题，需要实用的构建指南来加速开发进程

Method: 通过白皮书形式提供详细的构建指南，分析如何避免定制集成的困扰，提供系统化的开发方法

Result: 提供了实用的AI代理系统构建指南，帮助开发者更高效地完成项目开发

Conclusion: AI代理系统开发需要系统化的指导，Algolia的白皮书为开发者提供了实用的解决方案

Abstract: A detailed guide to building agentic systems (Sponsor) Is agentic AI your next step? Beware of getting bogged down with custom integrations. Algolia's white paper breaks down how to build AI agents on a deadline. Read it now

</details>


### [41] [Threat Hunting with Claude Code and MCP](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.detectionatscale.com%2Fp%2Fai-threat-hunting-mcp-workflows%3Futm_source=tldrinfosec/1/0100019be619f6bb-d58c16ae-c07d-4029-84c5-a5571612f3dc-000000/MMwwdxxpHx69MPX2zM6vuAEGh_oZMjEqFN75ha0Ufrg=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用Claude Code和MCP协议进行威胁狩猎，将数天的手动调查压缩到数小时内完成


<details>
  <summary>Details</summary>
Motivation: 传统威胁狩猎需要大量手动工作，耗时数天。通过AI代理直接连接安全数据湖，可以显著提高调查效率

Method: 使用Claude Code与Model Context Protocol服务器，通过可复用的"技能"教导AI代理威胁狩猎方法，如pivot loop模式进行关联分析

Result: 能够将数天的威胁狩猎工作压缩到数小时内完成，实现假设驱动的调查

Conclusion: AI代理与安全数据湖的直接连接可以大幅提升威胁狩猎效率，通过标准化的工作流程和可复用的技能实现快速调查

Abstract: Threat Hunting with Claude Code and MCP (7 minute read) This guide demonstrates how to use Claude Code with Model Context Protocol servers to compress days of manual threat hunting into hours by connecting AI agents directly to security data lakes for hypothesis-driven investigations. The workflow involves stakeholder alignment on three to five priority threats from threat models, then deploying reusable "Skills" that teach agents hunting methodologies, such as the pivot loop pattern for corr...

</details>
