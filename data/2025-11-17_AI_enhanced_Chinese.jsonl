{"id": "2511.10781", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10781", "abs": "https://arxiv.org/abs/2511.10781", "authors": ["Md Ariful Islam Malik", "Jeffrey C. Carver", "Nasir U. Eisty"], "title": "Peer Code Review in Research Software Development: The Research Software Engineer Perspective", "comment": null, "summary": "Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u7814\u7a76\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5bf9\u540c\u884c\u4ee3\u7801\u5ba1\u67e5\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u5c3d\u7ba1\u540c\u884c\u4ee3\u7801\u5ba1\u67e5\u5bf9\u63d0\u9ad8\u7814\u7a76\u8f6f\u4ef6\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46RSEs\u5728\u5b9e\u8df5\u4e2d\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u7ed3\u6784\u5316\u6d41\u7a0b\u3001\u6539\u8fdb\u5de5\u5177\u548c\u9488\u5bf9\u6027\u57f9\u8bad\u6765\u63d0\u5347\u91c7\u7528\u7387\u3002", "motivation": "\u7814\u7a76\u8f6f\u4ef6\u5bf9\u79d1\u7814\u53d1\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9700\u6c42\u53d8\u5316\u3001\u590d\u6742\u8f93\u5165\u548c\u9057\u7559\u4f9d\u8d56\u963b\u788d\u4e86\u8f6f\u4ef6\u8d28\u91cf\u548c\u53ef\u7ef4\u62a4\u6027\u3002\u867d\u7136\u540c\u884c\u4ee3\u7801\u5ba1\u67e5\u80fd\u63d0\u9ad8\u8f6f\u4ef6\u8d28\u91cf\uff0c\u4f46\u5176\u5728\u7814\u7a76\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u4e2d\u7684\u91c7\u7528\u60c5\u51b5\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u6536\u96c6\u7814\u7a76\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5bf9\u540c\u884c\u4ee3\u7801\u5ba1\u67e5\u7684\u770b\u6cd5\uff0c\u8c03\u67e5\u8bbe\u8ba1\u4e0e\u5148\u524d\u7814\u7a76\u4fdd\u6301\u4e00\u81f4\u4ee5\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff0c\u540c\u65f6\u5305\u542b\u9488\u5bf9RSEs\u7684\u989d\u5916\u95ee\u9898\u3002", "result": "\u6536\u523061\u4efd\u6709\u6548\u95ee\u5377\u56de\u590d\uff0c\u53d1\u73b0\u7ed3\u679c\u4e0e\u5148\u524d\u7814\u7a76\u4e00\u81f4\uff0c\u540c\u65f6\u63ed\u793a\u4e86RSEs\u76f8\u6bd4\u66f4\u5e7f\u6cdb\u5f00\u53d1\u8005\u7fa4\u4f53\u9762\u4e34\u7684\u72ec\u7279\u6311\u6218\u548c\u5b9e\u8df5\u5dee\u5f02\u3002", "conclusion": "\u540c\u884c\u4ee3\u7801\u5ba1\u67e5\u5bf9\u63d0\u9ad8\u7814\u7a76\u8f6f\u4ef6\u8d28\u91cf\u3001\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1RSEs\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u7ed3\u6784\u5316\u6d41\u7a0b\u3001\u6539\u8fdb\u5de5\u5177\u548c\u9488\u5bf9\u6027\u57f9\u8bad\u53ef\u4ee5\u63d0\u5347\u540c\u884c\u5ba1\u67e5\u5728\u7814\u7a76\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u91c7\u7528\u7387\u548c\u6709\u6548\u6027\u3002", "topic": "swe application"}}
{"id": "2511.10865", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10865", "abs": "https://arxiv.org/abs/2511.10865", "authors": ["Sherry Shi", "Renyao Wei", "Michele Tufano", "Jos\u00e9 Cambronero", "Runxiang Cheng", "Franjo Ivan\u010di\u0107", "Pat Rondon"], "title": "Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge", "comment": null, "summary": "Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u548c\u4eba\u5de5\u53c2\u4e0e\u7684\u8865\u4e01\u6709\u6548\u6027\u5224\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u6bcf\u4e2abug\u7684\u8bc4\u5206\u6807\u51c6\uff0c\u7ecf\u8fc7\u4eba\u5de5\u5ba1\u67e5\u548c\u4f18\u5316\u540e\uff0c\u4f7f\u7528LLM\u6839\u636e\u4f18\u5316\u540e\u7684\u6807\u51c6\u5224\u65ad\u8865\u4e01\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u6267\u884c\u6d4b\u8bd5\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u8865\u4e01\u7684\u771f\u5b9e\u6709\u6548\u6027\uff0c\u800c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002", "method": "\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u65b9\u6cd5\uff1a\u9996\u5148\u7528LLM\u751f\u6210\u6bcf\u4e2abug\u7684\u8bc4\u5206\u6807\u51c6\uff0c\u7ecf\u8fc7\u4e00\u6b21\u6027\u4eba\u5de5\u5ba1\u67e5\u548c\u4f18\u5316\uff0c\u7136\u540e\u7528LLM\u6839\u636e\u4f18\u5316\u540e\u7684\u6807\u51c6\u5224\u65ad\u8865\u4e01\u6709\u6548\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0e\u4eba\u5de5\u5171\u8bc6\u8fbe\u6210\u9ad8\u5ea6\u4e00\u81f4\uff08Cohen's kappa 0.75\uff09\uff0c\u53ec\u56de\u73870.94\uff0c\u7cbe\u786e\u73870.80\u3002\u5728\u5305\u542b\u4eba\u5de5\u8bc4\u5206\u4e0d\u4e00\u81f4\u7684\u5b8c\u6574\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6709\u6240\u4e0b\u964d\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\uff0c\u5728\u8865\u4e01\u6709\u6548\u6027\u5224\u65ad\u4e0a\u8fbe\u5230\u4e0e\u4eba\u5de5\u9ad8\u5ea6\u4e00\u81f4\u7684\u6c34\u5e73\uff0c\u4e3a\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe benchmark"}}
{"id": "2511.11012", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.11012", "abs": "https://arxiv.org/abs/2511.11012", "authors": ["Noor Nashid", "Daniel Ding", "Keheliya Gallaba", "Ahmed E. Hassan", "Ali Mesbah"], "title": "Beyond Accuracy: Behavioral Dynamics of Agentic Multi-Hunk Repair", "comment": null, "summary": "Automated program repair has traditionally focused on single-hunk defects, overlooking multi-hunk bugs that are prevalent in real-world systems. Repairing these bugs requires coordinated edits across multiple, disjoint code regions, posing substantially greater challenges. We present the first systematic study of LLM-driven coding agents (Claude Code, Codex, Gemini-cli, and Qwen Code) on this task. We evaluate these agents on 372 multi-hunk bugs from the Hunk4J dataset, analyzing 1,488 repair trajectories using fine-grained metrics that capture localization, repair accuracy, regression behavior, and operational dynamics. Results reveal substantial variation: repair accuracy ranges from 25.8% (Qwen Code) to 93.3% (Claude Code) and consistently declines with increasing bug dispersion and complexity. High-performing agents demonstrate superior semantic consistency, achieving positive regression reduction, whereas lower-performing agents often introduce new test failures. Notably, agents do not fail fast; failed repairs consume substantially more resources (39%-343% more tokens) and require longer execution time (43%-427%). Additionally, we developed Maple to provide agents with repository-level context. Empirical results show that Maple improves the repair accuracy of Gemini-cli by 30% through enhanced localization. By analyzing fine-grained metrics and trajectory-level analysis, this study moves beyond accuracy to explain how coding agents localize, reason, and act during multi-hunk repair.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76LLM\u9a71\u52a8\u7684\u4ee3\u7801\u4ee3\u7406\u5728\u591a\u6bb5bug\u4fee\u590d\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u4e864\u79cd\u4ee3\u7406\u5728372\u4e2a\u591a\u6bb5bug\u4e0a\u7684\u4fee\u590d\u80fd\u529b\uff0c\u53d1\u73b0\u4fee\u590d\u51c6\u786e\u7387\u4ece25.8%\u523093.3%\u4e0d\u7b49\uff0c\u4e14\u968f\u7740bug\u5206\u6563\u5ea6\u548c\u590d\u6742\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\u3002", "motivation": "\u4f20\u7edf\u7a0b\u5e8f\u4fee\u590d\u4e3b\u8981\u5173\u6ce8\u5355\u6bb5\u7f3a\u9677\uff0c\u800c\u5ffd\u7565\u4e86\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\u7684\u591a\u6bb5bug\u3002\u4fee\u590d\u8fd9\u4e9bbug\u9700\u8981\u5728\u591a\u4e2a\u4e0d\u8fde\u7eed\u4ee3\u7801\u533a\u57df\u8fdb\u884c\u534f\u8c03\u7f16\u8f91\uff0c\u9762\u4e34\u66f4\u5927\u6311\u6218\u3002", "method": "\u5728Hunk4J\u6570\u636e\u96c6\u7684372\u4e2a\u591a\u6bb5bug\u4e0a\u8bc4\u4f304\u79cdLLM\u4ee3\u7801\u4ee3\u7406\uff0c\u4f7f\u7528\u7ec6\u7c92\u5ea6\u6307\u6807\u5206\u67901,488\u6761\u4fee\u590d\u8f68\u8ff9\uff0c\u5305\u62ec\u5b9a\u4f4d\u3001\u4fee\u590d\u51c6\u786e\u7387\u3001\u56de\u5f52\u884c\u4e3a\u548c\u64cd\u4f5c\u52a8\u6001\u3002\u5f00\u53d1\u4e86Maple\u5de5\u5177\u4e3a\u4ee3\u7406\u63d0\u4f9b\u4ed3\u5e93\u7ea7\u4e0a\u4e0b\u6587\u3002", "result": "\u4fee\u590d\u51c6\u786e\u7387\u5dee\u5f02\u663e\u8457\uff1aClaude Code\u6700\u9ad8(93.3%)\uff0cQwen Code\u6700\u4f4e(25.8%)\u3002\u9ad8\u8868\u73b0\u4ee3\u7406\u5c55\u793a\u51fa\u66f4\u597d\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u6b63\u5411\u56de\u5f52\u51cf\u5c11\uff0c\u800c\u4f4e\u8868\u73b0\u4ee3\u7406\u5e38\u5f15\u5165\u65b0\u6d4b\u8bd5\u5931\u8d25\u3002\u5931\u8d25\u4fee\u590d\u6d88\u8017\u66f4\u591a\u8d44\u6e90(39%-343%\u66f4\u591atoken)\u548c\u66f4\u957f\u65f6\u95f4(43%-427%)\u3002Maple\u5c06Gemini-cli\u7684\u4fee\u590d\u51c6\u786e\u7387\u63d0\u534730%\u3002", "conclusion": "\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6307\u6807\u548c\u8f68\u8ff9\u7ea7\u5206\u6790\uff0c\u672c\u7814\u7a76\u4e0d\u4ec5\u5173\u6ce8\u51c6\u786e\u7387\uff0c\u8fd8\u89e3\u91ca\u4e86\u4ee3\u7801\u4ee3\u7406\u5728\u591a\u6bb5\u4fee\u590d\u8fc7\u7a0b\u4e2d\u5982\u4f55\u5b9a\u4f4d\u3001\u63a8\u7406\u548c\u884c\u52a8\u3002", "topic": "agent analysis"}}
{"id": "2511.10650", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.10650", "abs": "https://arxiv.org/abs/2511.10650", "authors": ["Felix George", "Harshit Kumar", "Divya Pathak", "Kaustabha Ray", "Mudit Verma", "Pratibha Moogi"], "title": "Unsupervised Cycle Detection in Agentic Applications", "comment": null, "summary": "Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u76d1\u7763\u7684\u5faa\u73af\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u5206\u6790\u6765\u8bc6\u522bLLM\u9a71\u52a8\u5e94\u7528\u4e2d\u9690\u85cf\u7684\u6267\u884c\u5faa\u73af\uff0c\u8fd9\u4e9b\u5faa\u73af\u4f1a\u6d88\u8017\u8d44\u6e90\u4f46\u4e0d\u89e6\u53d1\u663e\u5f0f\u9519\u8bef\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u667a\u80fd\u5e94\u7528\u8868\u73b0\u51fa\u975e\u786e\u5b9a\u6027\u884c\u4e3a\uff0c\u53ef\u80fd\u5f62\u6210\u9690\u85cf\u7684\u6267\u884c\u5faa\u73af\uff0c\u9ed8\u9ed8\u6d88\u8017\u8d44\u6e90\u800c\u4e0d\u89e6\u53d1\u663e\u5f0f\u9519\u8bef\uff0c\u4f20\u7edf\u53ef\u89c2\u6d4b\u6027\u5e73\u53f0\u65e0\u6cd5\u68c0\u6d4b\u8fd9\u4e9b\u6210\u672c\u9ad8\u6602\u7684\u4f4e\u6548\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a\u9996\u5148\u5e94\u7528\u8ba1\u7b97\u9ad8\u6548\u7684\u65f6\u95f4\u8c03\u7528\u6808\u5206\u6790\u8bc6\u522b\u663e\u5f0f\u5faa\u73af\uff0c\u7136\u540e\u5229\u7528\u8bed\u4e49\u76f8\u4f3c\u6027\u5206\u6790\u53d1\u73b0\u7531\u5197\u4f59\u5185\u5bb9\u751f\u6210\u7279\u5f81\u7684\u5fae\u5999\u5faa\u73af\u3002", "result": "\u5728\u57fa\u4e8eLangGraph\u7684\u80a1\u7968\u5e02\u573a\u5e94\u7528\u76841575\u6761\u8f68\u8ff9\u4e0a\u8bc4\u4f30\uff0c\u6df7\u5408\u65b9\u6cd5F1\u5f97\u5206\u4e3a0.72\uff08\u7cbe\u786e\u73870.62\uff0c\u53ec\u56de\u73870.86\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u7684\u7ed3\u6784\u65b9\u6cd5\uff08F1:0.08\uff09\u548c\u8bed\u4e49\u65b9\u6cd5\uff08F1:0.28\uff09\u3002", "conclusion": "\u867d\u7136\u7ed3\u679c\u4ee4\u4eba\u9f13\u821e\uff0c\u4f46\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\uff0c\u672a\u6765\u5de5\u4f5c\u9700\u8981\u5b8c\u5584\u65b9\u6cd5\u5e76\u89e3\u51b3\u5f53\u524d\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.11125", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11125", "abs": "https://arxiv.org/abs/2511.11125", "authors": ["Salim Fares", "Steffen Herbold"], "title": "Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs", "comment": "Submitted to the International Conference on Software Engineering (ICSE) track Software Engineering in Practice (SEIP) 2026", "summary": "How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.", "AI": {"tldr": "\u7814\u7a76\u5982\u4f55\u5728\u5de5\u4e1a\u8fc7\u7a0b\u81ea\u52a8\u5316\u9886\u57df\u4f7f\u7528LLM\u5904\u7406\u4e13\u6709\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u89e3\u51b3\u7b80\u5355\u95ee\u9898\uff0c\u786e\u4fdd\u6570\u636e\u5b89\u5168\u3002", "motivation": "\u73b0\u6709LLM\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u7f16\u7a0b\u8bed\u8a00\uff0c\u5de5\u4e1a\u81ea\u52a8\u5316\u9886\u57df\u7684\u4e13\u6709\u8bed\u8a00\u652f\u6301\u4e0d\u8db3\uff0c\u4f01\u4e1a\u9700\u8981\u5728\u4e0d\u6295\u5165\u5927\u91cf\u8bad\u7ec3\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u5229\u7528LLM\u3002", "method": "\u4f7f\u7528\u5c11\u91cf\u6837\u672c\u63d0\u793a\u65b9\u6cd5\uff0c\u5728\u4e13\u6709\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u4e0a\u6d4b\u8bd5LLM\u6027\u80fd\uff0c\u786e\u4fdd\u5728\u672c\u5730\u90e8\u7f72\u4ee5\u4fdd\u62a4\u654f\u611f\u6570\u636e\u3002", "result": "\u5c11\u91cf\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u8db3\u4ee5\u89e3\u51b3\u4e13\u6709\u8bed\u8a00\u4e2d\u7684\u7b80\u5355\u95ee\u9898\uff0c\u5373\u4f7f\u8be5\u8bed\u8a00\u5728LLM\u4e2d\u652f\u6301\u4e0d\u4f73\u3002", "conclusion": "\u4f01\u4e1a\u53ef\u4ee5\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u63d0\u793a\u5728\u672c\u5730\u90e8\u7f72LLM\uff0c\u6709\u6548\u5904\u7406\u4e13\u6709\u9886\u57df\u8bed\u8a00\u95ee\u9898\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u3002", "topic": "swe application"}}
{"id": "2511.10705", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10705", "abs": "https://arxiv.org/abs/2511.10705", "authors": ["Yuan Zhao", "Hualei Zhu", "Tingyu Jiang", "Shen Li", "Xiaohang Xu", "Hao Henry Wang"], "title": "Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents", "comment": "Accepted by AAAI 2026", "summary": "Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.", "AI": {"tldr": "Co-EPG\u662f\u4e00\u4e2a\u81ea\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5212\u548c\u63a5\u5730\u6a21\u578b\u7684\u534f\u540c\u8fdb\u5316\u6765\u89e3\u51b3GUI\u4efb\u52a1\u81ea\u52a8\u5316\u4e2d\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u6b63\u53cd\u9988\u5faa\u73af\uff0c\u901a\u8fc7GRPO\u4f18\u5316\u89c4\u5212\u6a21\u578b\uff0c\u540c\u65f6\u5229\u7528\u751f\u6210\u7684\u6570\u636e\u4f18\u5316\u63a5\u5730\u6a21\u578b\uff0c\u5b9e\u73b0\u6301\u7eed\u81ea\u6211\u589e\u5f3a\u3002", "motivation": "\u5f53\u524dGUI\u4ee3\u7406\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u9650\u5236\uff1a(1) \u8de8\u6a21\u578b\u534f\u540c\u5229\u7528\u4e0d\u8db3\uff0c(2) \u8fc7\u5ea6\u4f9d\u8d56\u5408\u6210\u6570\u636e\u751f\u6210\u800c\u5229\u7528\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u6574\u5408\u89c4\u5212\u548c\u63a5\u5730\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCo-EPG\u81ea\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7Group Relative Policy Optimization (GRPO)\u4f18\u5316\u89c4\u5212\u6a21\u578b\uff0c\u540c\u65f6\u5229\u7528\u751f\u6210\u7684\u6570\u636e\u4f18\u5316\u63a5\u5730\u6a21\u578b\uff0c\u5efa\u7acb\u89c4\u5212\u548c\u63a5\u5730\u6a21\u578b\u4e4b\u95f4\u7684\u6b63\u53cd\u9988\u5faa\u73af\u3002", "result": "\u5728Multimodal-Mind2Web\u548cAndroidControl\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u7ecf\u8fc7\u4e09\u6b21\u8fed\u4ee3\u5c31\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u3002\u4ee3\u7406\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u6301\u7eed\u6539\u8fdb\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u81ea\u6211\u589e\u5f3a\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aGUI\u4ee3\u7406\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u4ece\u5b64\u7acb\u4f18\u5316\u8f6c\u5411\u96c6\u6210\u3001\u81ea\u9a71\u52a8\u7684\u534f\u540c\u8fdb\u5316\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.10654", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10654", "abs": "https://arxiv.org/abs/2511.10654", "authors": ["Javier Mar\u00edn"], "title": "Empirical Characterization of Temporal Constraint Processing in LLMs", "comment": null, "summary": "When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u65f6\u95f4\u7ea6\u675f\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u98ce\u9669\uff0c\u5305\u62ec\u53cc\u5cf0\u6027\u80fd\u5206\u5e03\u3001\u6781\u7aef\u63d0\u793a\u8106\u5f31\u6027\u548c\u7cfb\u7edf\u6027\u884c\u52a8\u504f\u89c1\uff0c\u4e14\u53c2\u6570\u6570\u91cf\u4e0e\u80fd\u529b\u65e0\u5173\u3002\u5373\u4f7f\u901a\u8fc7\u5fae\u8c03\u4e5f\u65e0\u6cd5\u53ef\u9760\u5b66\u4e60\u65f6\u95f4\u7ea6\u675f\u6ee1\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u7684\u67b6\u6784\u673a\u5236\u3002", "motivation": "\u6d4b\u8bd5LLM\u5728\u9700\u8981\u5b9e\u65f6\u51b3\u7b56\u7684\u667a\u80fd\u4f53\u67b6\u6784\u4e2d\u662f\u5426\u80fd\u591f\u53ef\u9760\u5730\u5224\u65ad\u884c\u52a8\u7a97\u53e3\u662f\u5426\u5f00\u653e\u6216\u5173\u95ed\uff0c\u8fd9\u4e00\u5047\u8bbe\u6b64\u524d\u672a\u7ecf\u68c0\u9a8c\u3002", "method": "\u4f7f\u7528\u622a\u6b62\u65f6\u95f4\u68c0\u6d4b\u4efb\u52a1\u5bf98\u4e2a\u751f\u4ea7\u7ea7\u6a21\u578b\uff082.8-8B\u53c2\u6570\uff09\u8fdb\u884c\u65f6\u95f4\u7ea6\u675f\u5904\u7406\u7279\u6027\u5206\u6790\uff0c\u5305\u62ec\u6027\u80fd\u5206\u5e03\u3001\u63d0\u793a\u654f\u611f\u6027\u548c\u884c\u52a8\u504f\u89c1\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c200\u4e2a\u5408\u6210\u793a\u4f8b\u7684\u5fae\u8c03\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u6027\u90e8\u7f72\u98ce\u9669\uff1a\u53cc\u5cf0\u6027\u80fd\u5206\u5e03\uff08\u6a21\u578b\u51c6\u786e\u7387\u8981\u4e4895%\u8981\u4e4850%\uff09\u3001\u6781\u7aef\u63d0\u793a\u8106\u5f31\u6027\uff08\u4ec5\u683c\u5f0f\u53d8\u5316\u5c31\u5bfc\u81f430-60\u4e2a\u767e\u5206\u70b9\u6ce2\u52a8\uff09\u548c\u7cfb\u7edf\u6027\u884c\u52a8\u504f\u89c1\uff08\u5931\u8d25\u6a21\u578b100%\u5047\u9633\u6027\u7387\uff09\u3002\u53c2\u6570\u6570\u91cf\u4e0e\u80fd\u529b\u65e0\u5173\uff0c3.8B\u6a21\u578b\u4e0e7B\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002\u5fae\u8c03\u4ec5\u80fd\u90e8\u5206\u6539\u5584\u80fd\u529b\u3002", "conclusion": "\u65f6\u95f4\u7ea6\u675f\u6ee1\u8db3\u65e0\u6cd5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7684\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u53ef\u9760\u5b66\u4e60\uff0c\u5373\u4f7f\u6709\u9488\u5bf9\u6027\u7684\u5fae\u8c03\u4e5f\u65e0\u6548\u3002\u9700\u8981\u5305\u542b\u6301\u7eed\u65f6\u95f4\u72b6\u6001\u8868\u793a\u3001\u663e\u5f0f\u7ea6\u675f\u68c0\u67e5\u548c\u7cfb\u7edf\u7ec4\u5408\u63a8\u7406\u7684\u67b6\u6784\u673a\u5236\uff0c\u5f53\u524d\u81ea\u56de\u5f52\u67b6\u6784\u7f3a\u4e4f\u8fd9\u4e9b\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2511.10899", "categories": ["cs.CL", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10899", "abs": "https://arxiv.org/abs/2511.10899", "authors": ["Farima Fatahi Bayat", "Pouya Pezeshkpour", "Estevam Hruschka"], "title": "From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models", "comment": "19 pages, 5 figures", "summary": "Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5de5\u5177\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u63d0\u9ad8\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\uff0c\u4f46\u4f1a\u5bfc\u81f4\u63a8\u7406\u8d28\u91cf\u4e0b\u964d\uff0c\u51fa\u73b0\u5de5\u5177\u8bf1\u5bfc\u77ed\u89c6\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5c06\u5de5\u5177\u8f93\u51fa\u4f5c\u4e3a\u63a8\u7406\u7684\u66ff\u4ee3\u54c1\u800c\u975e\u8f85\u52a9\u8bc1\u636e\u3002", "motivation": "\u63a2\u7a76\u5de5\u5177\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u5728\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u65f6\u662f\u5426\u771f\u6b63\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u4f9d\u8d56\u5de5\u5177\u8f93\u51fa\u800c\u5ffd\u89c6\u4e86\u8fde\u8d2f\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528PYMATH\u57fa\u51c6\u6d4b\u8bd5\uff081,679\u4e2a\u7ade\u8d5b\u7ea7\u6570\u5b66\u95ee\u9898\uff09\uff0c\u5f00\u53d1\u591a\u7ef4\u5ea6\u8bc4\u4f30\u5957\u4ef6\u6765\u91cf\u5316\u63a8\u7406\u9000\u5316\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u504f\u597d\u4f18\u5316\u7684\u6846\u67b6\u6765\u91cd\u65b0\u8c03\u6574\u6a21\u578b\u4f7f\u7528\u5de5\u5177\u7684\u65b9\u5f0f\u3002", "result": "\u5de5\u5177\u589e\u5f3a\u6a21\u578b\u5728\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\u4e0a\u63d0\u534719.3\u4e2a\u767e\u5206\u70b9\uff0c\u4f46\u63a8\u7406\u8d28\u91cf\u663e\u8457\u4e0b\u964d\uff08\u975e\u5de5\u5177\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u6bd4\u8f83\u4e2d\u80dc\u51fa41.5%\uff09\uff0c\u5de5\u5177\u4f7f\u7528\u9891\u7387\u8d8a\u9ad8\uff0c\u63a8\u7406\u8fde\u8d2f\u6027\u8d8a\u5dee\u3002", "conclusion": "\u5de5\u5177\u4f7f\u7528\u867d\u7136\u63d0\u9ad8\u7b54\u6848\u51c6\u786e\u6027\uff0c\u4f46\u635f\u5bb3\u63a8\u7406\u8d28\u91cf\uff0c\u9700\u8981\u901a\u8fc7\u504f\u597d\u4f18\u5316\u7b49\u65b9\u6cd5\u91cd\u65b0\u8c03\u6574\u6a21\u578b\u5c06\u5de5\u5177\u4f5c\u4e3a\u8f85\u52a9\u8bc1\u636e\u800c\u975e\u63a8\u7406\u66ff\u4ee3\u54c1\u3002", "topic": "agent analysis"}}
{"id": "2511.10788", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10788", "abs": "https://arxiv.org/abs/2511.10788", "authors": ["Chao Wu", "Baoheng Li", "Mingchen Gao", "Zhenyi Wang"], "title": "From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models", "comment": null, "summary": "Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ece\u81ea\u9002\u5e94\u6027\u7684\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u5c06\u63a8\u7406\u52aa\u529b\u6839\u636e\u4efb\u52a1\u96be\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u52a8\u6001\u5206\u914d\uff0c\u5e76\u5efa\u7acb\u4e86\u5305\u542b\u8bad\u7ec3\u65b9\u6cd5\u548c\u8bad\u7ec3\u81ea\u7531\u65b9\u6cd5\u7684\u7cfb\u7edf\u5206\u7c7b\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u5bf9\u6240\u6709\u4efb\u52a1\u91c7\u7528\u7edf\u4e00\u7684\u63a8\u7406\u7b56\u7565\uff0c\u5bfc\u81f4\u7b80\u5355\u4efb\u52a1\u751f\u6210\u8fc7\u957f\u63a8\u7406\u94fe\u800c\u56f0\u96be\u4efb\u52a1\u63a8\u7406\u4e0d\u8db3\uff0c\u9700\u8981\u53d1\u5c55\u80fd\u591f\u6839\u636e\u4efb\u52a1\u7279\u6027\u81ea\u9002\u5e94\u5206\u914d\u63a8\u7406\u52aa\u529b\u7684\u80fd\u529b\u3002", "method": "\u5c06\u81ea\u9002\u5e94\u63a8\u7406\u5f62\u5f0f\u5316\u4e3a\u63a7\u5236\u589e\u5f3a\u7684\u7b56\u7565\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u7cfb\u7edf\u5206\u7c7b\u6cd5\uff1a\u8bad\u7ec3\u65b9\u6cd5\uff08\u5f3a\u5316\u5b66\u4e60\u3001\u76d1\u7763\u5fae\u8c03\u3001\u5b66\u4e60\u63a7\u5236\u5668\uff09\u548c\u8bad\u7ec3\u81ea\u7531\u65b9\u6cd5\uff08\u63d0\u793a\u6761\u4ef6\u5316\u3001\u53cd\u9988\u9a71\u52a8\u505c\u6b62\u3001\u6a21\u5757\u5316\u7ec4\u5408\uff09\u3002", "result": "\u5efa\u7acb\u4e86\u8fde\u63a5\u7ecf\u5178\u8ba4\u77e5\u8303\u5f0f\u4e0e\u7b97\u6cd5\u5b9e\u73b0\u7684\u6846\u67b6\uff0c\u6f84\u6e05\u4e86\u4e0d\u540c\u673a\u5236\u5982\u4f55\u5728\u5b9e\u9645\u4e2d\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u5e76\u652f\u6301\u8de8\u7b56\u7565\u7684\u7cfb\u7edf\u6bd4\u8f83\u3002", "conclusion": "\u8bc6\u522b\u4e86\u81ea\u6211\u8bc4\u4f30\u3001\u5143\u63a8\u7406\u548c\u4eba\u7c7b\u5bf9\u9f50\u63a8\u7406\u63a7\u5236\u7b49\u5f00\u653e\u6311\u6218\uff0c\u4e3a\u81ea\u9002\u5e94\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2511.10810", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10810", "abs": "https://arxiv.org/abs/2511.10810", "authors": ["Ran Elgedawy", "Sanjay Das", "Ethan Seefried", "Gavin Wiggins", "Ryan Burchfield", "Dana Hewit", "Sudarshan Srinivasan", "Todd Thomas", "Prasanna Balaprakash", "Tirthankar Ghosal"], "title": "HARNESS: Human-Agent Risk Navigation and Event Safety System for Proactive Hazard Forecasting in High-Risk DOE Environments", "comment": null, "summary": "Operational safety at mission-critical work sites is a top priority given the complex and hazardous nature of daily tasks. This paper presents the Human-Agent Risk Navigation and Event Safety System (HARNESS), a modular AI framework designed to forecast hazardous events and analyze operational risks in U.S. Department of Energy (DOE) environments. HARNESS integrates Large Language Models (LLMs) with structured work data, historical event retrieval, and risk analysis to proactively identify potential hazards. A human-in-the-loop mechanism allows subject matter experts (SMEs) to refine predictions, creating an adaptive learning loop that enhances performance over time. By combining SME collaboration with iterative agentic reasoning, HARNESS improves the reliability and efficiency of predictive safety systems. Preliminary deployment shows promising results, with future work focusing on quantitative evaluation of accuracy, SME agreement, and decision latency reduction.", "AI": {"tldr": "HARNESS\u662f\u4e00\u4e2a\u6a21\u5757\u5316AI\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5371\u9669\u4e8b\u4ef6\u548c\u5206\u6790\u7f8e\u56fd\u80fd\u6e90\u90e8\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u98ce\u9669\uff0c\u7ed3\u5408LLM\u4e0e\u7ed3\u6784\u5316\u5de5\u4f5c\u6570\u636e\u3001\u5386\u53f2\u4e8b\u4ef6\u68c0\u7d22\u548c\u98ce\u9669\u5206\u6790\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u673a\u5236\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u5173\u952e\u4efb\u52a1\u5de5\u4f5c\u573a\u6240\uff0c\u64cd\u4f5c\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u65e5\u5e38\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u5371\u9669\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4e3b\u52a8\u9884\u6d4b\u5371\u9669\u4e8b\u4ef6\u7684\u7cfb\u7edf\u6765\u4fdd\u969c\u5b89\u5168\u3002", "method": "\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u7ed3\u6784\u5316\u5de5\u4f5c\u6570\u636e\u3001\u5386\u53f2\u4e8b\u4ef6\u68c0\u7d22\u548c\u98ce\u9669\u5206\u6790\uff0c\u91c7\u7528\u4eba\u5728\u56de\u8def\u673a\u5236\u8ba9\u9886\u57df\u4e13\u5bb6\u4f18\u5316\u9884\u6d4b\uff0c\u5f62\u6210\u81ea\u9002\u5e94\u5b66\u4e60\u5faa\u73af\u3002", "result": "\u521d\u6b65\u90e8\u7f72\u663e\u793a\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u7cfb\u7edf\u901a\u8fc7\u4e13\u5bb6\u534f\u4f5c\u548c\u8fed\u4ee3\u63a8\u7406\u63d0\u9ad8\u4e86\u9884\u6d4b\u5b89\u5168\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "conclusion": "HARNESS\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u4e13\u5bb6\u534f\u4f5c\u4e0e\u8fed\u4ee3\u667a\u80fd\u63a8\u7406\uff0c\u6539\u8fdb\u4e86\u9884\u6d4b\u5b89\u5168\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u805a\u7126\u4e8e\u51c6\u786e\u6027\u3001\u4e13\u5bb6\u4e00\u81f4\u6027\u548c\u51b3\u7b56\u5ef6\u8fdf\u7684\u91cf\u5316\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2511.10661", "categories": ["cs.CL", "cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10661", "abs": "https://arxiv.org/abs/2511.10661", "authors": ["Rachel Longjohn", "Shang Wu", "Saatvik Kher", "Catarina Bel\u00e9m", "Padhraic Smyth"], "title": "Bayesian Evaluation of Large Language Model Behavior", "comment": "Accepted to NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling", "summary": "It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u65b9\u6cd5\u6765\u91cf\u5316\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u7cfb\u7edf\u5728\u4e8c\u5143\u8bc4\u4f30\u6307\u6807\u4e2d\u7684\u7edf\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u7531\u6982\u7387\u6587\u672c\u751f\u6210\u7b56\u7565\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u7edf\u8ba1\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u5728\u6709\u5bb3\u8f93\u51fa\u751f\u6210\u3001\u5bf9\u6297\u6027\u8f93\u5165\u654f\u611f\u6027\u7b49\u65b9\u9762\u7684\u884c\u4e3a\u8868\u73b0\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u91cf\u5316\u4e8c\u5143\u8bc4\u4f30\u6307\u6807\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u7279\u522b\u5173\u6ce8\u6982\u7387\u6587\u672c\u751f\u6210\u7b56\u7565\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u884c\u4e3a\u63d0\u4f9b\u6709\u7528\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5728\u62d2\u7edd\u7387\u8bc4\u4f30\u548c\u6210\u5bf9\u504f\u597d\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u5e94\u7528\u6548\u679c\u3002", "conclusion": "\u8d1d\u53f6\u65af\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u7edf\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u6a21\u578b\u884c\u4e3a\u8bc4\u4f30\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u7edf\u8ba1\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2511.10952", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10952", "abs": "https://arxiv.org/abs/2511.10952", "authors": ["Steven J. Jones", "Robert E. Wray", "John E. Laird"], "title": "Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints", "comment": "6 pages, technical appendix (submitted to AAAI26)", "summary": "Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual \"knowledge\" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u81ea\u4e3bAI\u7cfb\u7edf\u5728\u9047\u5230\u65e0\u6cd5\u5b8c\u5168\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u6761\u4ef6\u7684\u65b0\u573a\u666f\u65f6\uff0c\u9700\u8981\u8d85\u8d8a\u8bad\u7ec3\u7b56\u7565\u6765\u6784\u5efa\u3001\u8bc4\u4f30\u548c\u8bc1\u660e\u5019\u9009\u884c\u52a8\u65b9\u6848\u7684\u80fd\u529b\u8981\u6c42\u3002", "motivation": "\u81ea\u4e3bAI\u7cfb\u7edf\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5fc5\u7136\u4f1a\u9047\u5230\u8bad\u7ec3\u6570\u636e\u672a\u8986\u76d6\u7684\u65b0\u573a\u666f\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u4e0d\u5b8c\u5168\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u6761\u4ef6\u7684\u60c5\u51b5\uff0c\u4ee5\u7b26\u5408\u4eba\u7c7b\u671f\u671b\u548c\u4ef7\u503c\u89c2\u3002", "method": "\u901a\u8fc7\u5206\u6790\u548c\u5b9e\u8bc1\u6848\u4f8b\u7814\u7a76\uff0c\u8bc6\u522b\u667a\u80fd\u4f53\u51b3\u7b56\u6240\u9700\u7684\u77e5\u8bc6\u7c7b\u578b\uff0c\u5305\u62ec\u89c4\u8303\u6027\u3001\u5b9e\u7528\u6027\u548c\u60c5\u5883\u6027\u7406\u89e3\u3002", "result": "\u786e\u5b9a\u4e86\u667a\u80fd\u4f53\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u505a\u51fa\u66f4\u7b26\u5408\u4eba\u7c7b\u671f\u671b\u51b3\u7b56\u6240\u9700\u7684\u77e5\u8bc6\u6574\u5408\u8981\u6c42\u3002", "conclusion": "\u667a\u80fd\u4f53\u9700\u8981\u6574\u5408\u591a\u79cd\u77e5\u8bc6\u7c7b\u578b\u6765\u9009\u62e9\u548c\u8ffd\u6c42\u66f4\u7b26\u5408\u4eba\u7c7b\u671f\u671b\u7684\u884c\u52a8\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2511.10665", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10665", "abs": "https://arxiv.org/abs/2511.10665", "authors": ["Cristina Pinneri", "Christos Louizos"], "title": "Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models", "comment": null, "summary": "Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\u6765\u63d0\u5347\u9632\u62a4\u6a21\u578b\u7684\u8bed\u4e49\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u4f7f\u7528\u91ca\u4e49\u96c6\u5408\u548c\u504f\u659c\u611f\u77e5\u805a\u5408\u7b56\u7565\u6765\u589e\u5f3a\u9884\u6d4b\u4e00\u81f4\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bed\u4e49\u53d8\u5f02\u6027\u5e76\u6539\u5584\u4e86\u6a21\u578b\u6821\u51c6\u3002", "motivation": "\u9632\u62a4\u6a21\u578b\u5bf9\u8bed\u8a00\u8868\u9762\u53d8\u5316\u7684\u654f\u611f\u6027\u662f\u5176\u5173\u952e\u6f0f\u6d1e\uff0c\u5373\u4f7f\u610f\u4e49\u4fdd\u6301\u4e0d\u53d8\u7684\u91ca\u4e49\u4e5f\u4f1a\u5bfc\u81f4\u5b89\u5168\u8bc4\u5206\u5927\u5e45\u6ce2\u52a8\uff0c\u8868\u660e\u7f3a\u4e4f\u8bed\u4e49\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u91ca\u4e49\u96c6\u5408\u5f3a\u5236\u9884\u6d4b\u4e00\u81f4\u6027\uff0c\u91c7\u7528\u65b0\u9896\u7684\u504f\u659c\u611f\u77e5\u805a\u5408\u7b56\u7565\u8fdb\u884c\u9c81\u68d2\u76ee\u6807\u8ba1\u7b97\uff0c\u907f\u514d\u6807\u51c6\u805a\u5408\u65b9\u6cd5\u53ef\u80fd\u964d\u4f4e\u5b89\u5168\u6027\u7684\u95ee\u9898\u3002", "result": "\u5728\u516d\u4e2a\u5f00\u6e90\u9632\u62a4\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0c\u65b9\u6cd5\u5c06\u8bed\u4e49\u53d8\u5f02\u6027\u964d\u4f4e\u4e86\u7ea658%\uff0c\u57fa\u51c6\u51c6\u786e\u7387\u5e73\u5747\u63d0\u9ad8\u7ea62.5%\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u98ce\u683c\u53d8\u5316\u3002", "conclusion": "\u8bed\u4e49\u4e00\u81f4\u6027\u5e94\u4f5c\u4e3a\u9996\u8981\u8bad\u7ec3\u76ee\u6807\uff0c\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u9632\u62a4\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u65b9\u6848\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u6821\u51c6\u4e0e\u4e00\u81f4\u6027\u4e4b\u95f4\u7684\u53cc\u5411\u5173\u7cfb\u3002", "topic": "agent analysis"}}
{"id": "2511.10667", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10667", "abs": "https://arxiv.org/abs/2511.10667", "authors": ["Sichao Li", "Xinyue Xu", "Xiaomeng Li"], "title": "Evaluating LLM Understanding via Structured Tabular Decision Simulations", "comment": null, "summary": "Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.", "AI": {"tldr": "\u63d0\u51fa\u4e86STaDS\u6846\u67b6\u6765\u8bc4\u4f30LLMs\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u5927\u591a\u6570\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\u7684\u51c6\u786e\u7387\uff0c\u4e14\u5b58\u5728\u9884\u6d4b\u51c6\u786e\u4f46\u51b3\u7b56\u4f9d\u636e\u4e0d\u5fe0\u5b9e\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLMs\u867d\u7136\u9884\u6d4b\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u51c6\u786e\u6027\u5e76\u4e0d\u7b49\u540c\u4e8e\u771f\u6b63\u7684\u7406\u89e3\u80fd\u529b\u3002\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u4e13\u5bb6\u4e00\u6837\uff0c\u5728\u4e0d\u540c\u5b9e\u4f8b\u548c\u9886\u57df\u4e2d\u505a\u51fa\u57fa\u4e8e\u76f8\u5173\u51b3\u7b56\u56e0\u7d20\u7684\u4e00\u81f4\u51b3\u7b56\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u8868\u683c\u51b3\u7b56\u6a21\u62df(STaDS)\uff0c\u901a\u8fc715\u4e2a\u4e0d\u540c\u51b3\u7b56\u573a\u666f\u5bf99\u4e2a\u524d\u6cbfLLMs\u8fdb\u884c\u8bc4\u4f30\uff0c\u4ece\u95ee\u9898\u7406\u89e3\u3001\u77e5\u8bc6\u9884\u6d4b\u548c\u51b3\u7b56\u56e0\u7d20\u4f9d\u8d56\u4e09\u4e2a\u7ef4\u5ea6\u8054\u5408\u8bc4\u4f30\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\u7684\u5f3a\u51c6\u786e\u7387\uff1b\u6a21\u578b\u53ef\u80fd\u51c6\u786e\u4f46\u5168\u5c40\u4e0d\u5fe0\u5b9e\uff0c\u5b58\u5728\u9648\u8ff0\u7406\u7531\u4e0e\u9884\u6d4b\u9a71\u52a8\u56e0\u7d20\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u51c6\u786e\u6027\u7684\u5168\u5c40\u7406\u89e3\u8bc4\u4f30\u534f\u8bae\uff0c\u5f00\u53d1\u65b0\u6846\u67b6\u6765\u589e\u5f3aLLMs\u7684\u7406\u89e3\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.11040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11040", "abs": "https://arxiv.org/abs/2511.11040", "authors": ["Qian Zhang", "Yan Zheng", "Jinyi Liu", "Hebin Liang", "Lanjun Wang"], "title": "Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?", "comment": null, "summary": "Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, \"Truth Last\", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89d2\u8272\u5206\u914d\u7b56\u7565\u5bf9\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86'Truth Last'\u7b56\u7565\u63d0\u5347\u63a8\u7406\u4efb\u52a1\u8868\u73b022%\uff0c\u5e76\u5f00\u53d1\u4e86MADC\u7b56\u7565\u901a\u8fc7\u4e00\u81f4\u6027\u8bc4\u4f30\u6765\u4f18\u5316\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u673a\u5236\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u5728\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u89d2\u8272\u5206\u914d\u7b56\u7565\u8fd9\u4e00\u5173\u952e\u65b9\u9762\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u771f\u7406\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa'Truth Last'\u89d2\u8272\u5206\u914d\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e00\u81f4\u6027(MADC)\u7b56\u7565\uff0c\u901a\u8fc7\u8def\u5f84\u4e00\u81f4\u6027\u8bc4\u4f30\u72ec\u7acb\u89d2\u8272\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u6a21\u62df\u6700\u9ad8\u4e00\u81f4\u6027\u5f97\u5206\u7684\u89d2\u8272\u4f5c\u4e3a\u771f\u7406\u3002", "result": "\u57289\u4e2aLLM\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cMADC\u7b56\u7565\u6301\u7eed\u8868\u73b0\u51fa\u5148\u8fdb\u6027\u80fd\uff0c\u6709\u6548\u514b\u670d\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7684\u6027\u80fd\u74f6\u9888\uff0c\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u8fbe22%\u3002", "conclusion": "MADC\u4e3aLLM\u667a\u80fd\u4f53\u6269\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u6539\u8fdb\u8def\u5f84\uff0c\u7cfb\u7edf\u6027\u5730\u4f18\u5316\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7684\u6838\u5fc3\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2511.10843", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10843", "abs": "https://arxiv.org/abs/2511.10843", "authors": ["Alexander W. Goodall", "Edwin Hamel-De le Court", "Francesco Belardinelli"], "title": "Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning", "comment": "Accepted at AAAI 2026 (main track)", "summary": "Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u884c\u4e3a\u7b56\u7565\u6536\u96c6\u79bb\u7b56\u7565\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u56de\u62a5\u4f30\u8ba1\u7684\u65b9\u5dee\uff0c\u4ece\u800c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f9d\u8d56\u56de\u62a5\u4f30\u8ba1\u8fdb\u884c\u7b56\u7565\u6539\u8fdb\uff0c\u4f46\u9ad8\u65b9\u5dee\u7684\u56de\u62a5\u4f30\u8ba1\u4f1a\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002\u6700\u8fd1\u7814\u7a76\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u884c\u4e3a\u7b56\u7565\u53ef\u4ee5\u6536\u96c6\u79bb\u7b56\u7565\u6570\u636e\uff0c\u83b7\u5f97\u65b9\u5dee\u66f4\u4f4e\u7684\u56de\u62a5\u4f30\u8ba1\u3002", "method": "\u5c06\u79bb\u7b56\u7565\u8bc4\u4f30\u7684\u5173\u952e\u89c1\u89e3\u6269\u5c55\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\uff0c\u5728\u7b56\u7565\u8bc4\u4f30\u548c\u6539\u8fdb\u4ea4\u9519\u8fdb\u884c\u65f6\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u3002\u6269\u5c55\u4e86\u4e24\u79cd\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u4f7f\u7528\u5355\u4e00\u884c\u4e3a\u7b56\u7565\u6536\u96c6\u6570\u636e\uff0c\u83b7\u5f97\u65b9\u5dee\u66f4\u4f4e\u7684\u56de\u62a5\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u6536\u96c6\u79bb\u7b56\u7565\u6570\u636e\u53ef\u4ee5\u83b7\u5f97\u65b9\u5dee\u66f4\u4f4e\u7684\u56de\u62a5\u4f30\u8ba1\uff0c\u4ece\u800c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u8ba4\u4e3a\u5728\u7ebf\u7b56\u7565\u6570\u636e\u6536\u96c6\u662f\u6700\u4f18\u7684\u89c2\u70b9\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.11043", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11043", "abs": "https://arxiv.org/abs/2511.11043", "authors": ["Asen Nachkov", "Jan-Nico Zaech", "Danda Pani Paudel", "Xi Wang", "Luc Van Gool"], "title": "Autonomous Vehicle Path Planning by Searching With Differentiable Simulation", "comment": null, "summary": "Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.", "AI": {"tldr": "\u63d0\u51faDSS\u6846\u67b6\uff0c\u5229\u7528\u53ef\u5fae\u5206\u6a21\u62df\u5668Waymax\u4f5c\u4e3a\u72b6\u6001\u9884\u6d4b\u5668\u548c\u8bc4\u4f30\u5668\uff0c\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u52a8\u4f5c\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u8ddf\u8e2a\u548c\u8def\u5f84\u89c4\u5212\u7cbe\u5ea6\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u89c4\u5212\u5bf9\u4e8e\u907f\u514d\u78b0\u649e\u548c\u5728\u590d\u6742\u5bc6\u96c6\u4ea4\u901a\u573a\u666f\u4e2d\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5b66\u4e60\u7b56\u7565\u3001\u72b6\u6001\u9884\u6d4b\u5668\u548c\u8bc4\u4f30\u5668\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "\u4f7f\u7528\u53ef\u5fae\u5206\u6a21\u62df\u5668Waymax\u4f5c\u4e3a\u72b6\u6001\u9884\u6d4b\u5668\u548c\u8bc4\u4f30\u5668\uff0c\u5229\u7528\u5176\u786c\u7f16\u7801\u52a8\u529b\u5b66\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u72b6\u6001\u9884\u6d4b\uff0c\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u5728\u60f3\u8c61\u8f68\u8ff9\u4e0a\u4f18\u5316\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDSS\uff08\u89c4\u5212\u68af\u5ea6\u4e0e\u968f\u673a\u641c\u7d22\u7ed3\u5408\uff09\u76f8\u6bd4\u5e8f\u5217\u9884\u6d4b\u3001\u6a21\u4eff\u5b66\u4e60\u3001\u65e0\u6a21\u578bRL\u548c\u5176\u4ed6\u89c4\u5212\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u548c\u8def\u5f84\u89c4\u5212\u7cbe\u5ea6\u3002", "conclusion": "DSS\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c4\u5212\u68af\u5ea6\u548c\u968f\u673a\u641c\u7d22\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.10855", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10855", "abs": "https://arxiv.org/abs/2511.10855", "authors": ["Tom Yuviler", "Dana Drachsler-Cohen"], "title": "ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries", "comment": null, "summary": "Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.", "AI": {"tldr": "ExPairT-LLM\u662f\u4e00\u79cd\u7cbe\u786e\u7684\u4ee3\u7801\u9009\u62e9\u7b97\u6cd5\uff0c\u901a\u8fc7\u5411LLM\u63d0\u51fa\u6210\u5bf9\u6210\u5458\u8d44\u683c\u548c\u6210\u5bf9\u7b49\u4ef7\u6027\u67e5\u8be2\u6765\u4ece\u591a\u4e2a\u751f\u6210\u7684\u7a0b\u5e8f\u4e2d\u9009\u51fa\u6b63\u786e\u7a0b\u5e8f\uff0c\u5728\u56db\u4e2a\u6d41\u884c\u4ee3\u7801\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u6bd4\u73b0\u6709\u6700\u4f73\u7b97\u6cd5\u63d0\u534713.0%\u7684pass@1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u9009\u62e9\u7b97\u6cd5\u53ef\u80fd\u65e0\u6cd5\u8bc6\u522b\u6b63\u786e\u7a0b\u5e8f\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u80fd\u9519\u8bef\u8bc6\u522b\u4e0d\u7b49\u4ef7\u7a0b\u5e8f\uff0c\u6216\u8005\u4f9d\u8d56LLM\u5e76\u5047\u8bbe\u5b83\u603b\u80fd\u6b63\u786e\u786e\u5b9a\u6bcf\u4e2a\u8f93\u5165\u7684\u8f93\u51fa\u3002", "method": "\u63d0\u51faExPairT-LLM\u7b97\u6cd5\uff0c\u901a\u8fc7\u5411LLM\u63d0\u51fa\u4e24\u79cd\u65b0\u578b\u67e5\u8be2\uff1a\u6210\u5bf9\u6210\u5458\u8d44\u683c\u548c\u6210\u5bf9\u7b49\u4ef7\u6027\u67e5\u8be2\uff0c\u8fd9\u4e9b\u67e5\u8be2\u5bf9LLM\u66f4\u7b80\u5355\uff0c\u4f7f\u7b97\u6cd5\u80fd\u591f\u901a\u8fc7\u9526\u6807\u8d5b\u65b9\u5f0f\u8bc6\u522b\u6b63\u786e\u7a0b\u5e8f\uff0c\u5bf9\u67d0\u4e9bLLM\u9519\u8bef\u5177\u6709\u9c81\u68d2\u6027\u3002", "result": "\u5728\u56db\u4e2a\u6d41\u884c\u4ee3\u7801\u6570\u636e\u96c6\u4e0a\uff0cExPairT-LLM\u7684pass@1\uff08\u6210\u529f\u7387\uff09\u5e73\u5747\u6bd4\u6700\u5148\u8fdb\u7684\u4ee3\u7801\u9009\u62e9\u7b97\u6cd5\u9ad8\u51fa13.0%\uff0c\u6700\u9ad8\u53ef\u8fbe27.1%\u3002\u540c\u65f6\u5c06\u6267\u884c\u590d\u6742\u63a8\u7406\u7684LLM\u7684pass@1\u63d0\u9ad8\u4e8624.0%\u3002", "conclusion": "ExPairT-LLM\u901a\u8fc7\u66f4\u7b80\u5355\u7684\u67e5\u8be2\u7c7b\u578b\u548c\u9526\u6807\u8d5b\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u4e2d\u7a0b\u5e8f\u9009\u62e9\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "topic": "code agent"}}
{"id": "2511.10674", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.10674", "abs": "https://arxiv.org/abs/2511.10674", "authors": ["Thomas Cook", "Kelly Patel", "Sivapriya Vellaichamy", "Saba Rahimi", "Zhen Zeng", "Sumitra Ganesh"], "title": "Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL", "comment": "34 pages, 6 figures, 4 tables", "summary": "Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u672c\u5230SQL\u7684\u67e5\u8be2\u751f\u6210\uff0c\u901a\u8fc7\u5b58\u50a8\u548c\u91cd\u7528\u4ece\u53cd\u9988\u4e2d\u63d0\u53d6\u7684\u77e5\u8bc6\u6765\u63d0\u9ad8\u6267\u884c\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210SQL\u67e5\u8be2\u65f6\uff0c\u96be\u4ee5\u5904\u7406\u6570\u636e\u5e93\u7279\u5b9a\u6a21\u5f0f\u548c\u9690\u542b\u9886\u57df\u77e5\u8bc6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u4eba\u7c7b\u53cd\u9988\u4e2d\u6301\u7eed\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b66\u4e60\u4ee3\u7406\u6846\u67b6\uff0c\u63a5\u6536\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u6765\u4f18\u5316\u67e5\u8be2\uff0c\u5e76\u5c06\u63ed\u793a\u7684\u77e5\u8bc6\u63d0\u70bc\u5b58\u50a8\u5230\u7ed3\u6784\u5316\u5185\u5b58\u4e2d\uff0c\u4f9b\u672a\u6765\u4efb\u52a1\u91cd\u7528\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u5b66\u4e60\u4ee3\u7406\u67b6\u6784\u53d8\u4f53\uff0c\u5305\u62ec\u8fc7\u7a0b\u4ee3\u7406\u7b49\u5185\u5b58\u589e\u5f3a\u4ee3\u7406\u3002", "result": "\u5728BIRD\u57fa\u51c6\u6d4b\u8bd5\u5f00\u53d1\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5185\u5b58\u589e\u5f3a\u4ee3\u7406\uff08\u7279\u522b\u662f\u8fc7\u7a0b\u4ee3\u7406\uff09\u901a\u8fc7\u5229\u7528\u4eba\u7c7b\u5728\u73af\u53cd\u9988\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u51c6\u786e\u6027\u63d0\u5347\u548c\u9519\u8bef\u51cf\u5c11\u3002", "conclusion": "\u5c06\u9690\u542b\u7684\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u8f6c\u5316\u4e3a\u53ef\u91cd\u7528\u77e5\u8bc6\u5bf9\u4e8e\u6784\u5efa\u66f4\u9002\u5e94\u3001\u9886\u57df\u611f\u77e5\u7684\u6587\u672c\u5230SQL\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u80fd\u591f\u4ece\u4eba\u7c7b\u5728\u73af\u4e2d\u6301\u7eed\u5b66\u4e60\u3002", "topic": "agent analysis"}}
{"id": "2511.10868", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10868", "abs": "https://arxiv.org/abs/2511.10868", "authors": ["Yashshi Pipalani", "Hritik Raj", "Rajat Ghosh", "Vaishnavi Bhargava", "Debojyoti Dutta"], "title": "Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go", "comment": "9 pages, 5 figures", "summary": "Training data imbalance poses a major challenge for code LLMs. Most available data heavily over represents raw opensource code while underrepresenting broader software engineering tasks, especially in low resource languages like Golang. As a result, models excel at code autocompletion but struggle with real world developer workflows such as unit test generation. To address this gap, we introduce GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests, drawn from 10 permissively licensed Golang repositories spanning diverse domain. We evaluate its effectiveness as a fine tuning dataset across two LLM families i.e. mixture of experts and dense decoders. Our results show that finetuned models outperform their base counterparts on more than 75% of benchmark tasks.", "AI": {"tldr": "GO UT Bench\u662f\u4e00\u4e2a\u5305\u542b5264\u5bf9\u4ee3\u7801\u548c\u5355\u5143\u6d4b\u8bd5\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u4ee3\u7801LLM\u5728Go\u8bed\u8a00\u4e2d\u8bad\u7ec3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u8bad\u7ec3\u6570\u636e\u4e25\u91cd\u504f\u5411\u5f00\u6e90\u4ee3\u7801\uff0c\u800c\u4f4e\u4f30\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728Go\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u3002\u8fd9\u5bfc\u81f4\u6a21\u578b\u64c5\u957f\u4ee3\u7801\u81ea\u52a8\u8865\u5168\uff0c\u4f46\u5728\u5b9e\u9645\u5f00\u53d1\u5de5\u4f5c\u6d41\uff08\u5982\u5355\u5143\u6d4b\u8bd5\u751f\u6210\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6784\u5efaGO UT Bench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea10\u4e2a\u8bb8\u53ef\u5bbd\u677e\u7684Go\u4ed3\u5e93\u76845264\u5bf9\u4ee3\u7801\u548c\u5355\u5143\u6d4b\u8bd5\u3002\u5728\u4e24\u4e2aLLM\u5bb6\u65cf\uff08\u4e13\u5bb6\u6df7\u5408\u548c\u5bc6\u96c6\u89e3\u7801\u5668\uff09\u4e0a\u8bc4\u4f30\u5176\u4f5c\u4e3a\u5fae\u8c03\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u8d85\u8fc775%\u7684\u57fa\u51c6\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "GO UT Bench\u4f5c\u4e3a\u5fae\u8c03\u6570\u636e\u96c6\u80fd\u6709\u6548\u63d0\u5347\u4ee3\u7801LLM\u5728Go\u8bed\u8a00\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "topic": "swe benchmark"}}
{"id": "2511.11182", "categories": ["cs.AI", "cs.CL", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.11182", "abs": "https://arxiv.org/abs/2511.11182", "authors": ["Dayong Liang", "Xiao-Yong Wei", "Changmeng Zheng"], "title": "Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning", "comment": "Accepted by AAAI 2026", "summary": "Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like \"Who is Undercover?\". MUG reframes MAD as a process of detecting \"undercover\" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86MUG\u534f\u8bae\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u7c7b\u4f3c\"\u8c01\u662f\u5367\u5e95\"\u7684\u6e38\u620f\u673a\u5236\u68c0\u6d4b\u5e7b\u89c9\u667a\u80fd\u4f53\uff0c\u4f7f\u7528\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u548c\u52a8\u6001\u8bc1\u636e\u4fee\u6539\u6765\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u5047\u8bbe\u6240\u6709\u8fa9\u8bba\u8005\u90fd\u662f\u7406\u6027\u7684\uff0c\u4f46\u73b0\u5b9e\u4e2d\u667a\u80fd\u4f53\u672c\u8eab\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\uff0c\u8fd9\u9650\u5236\u4e86\u8fa9\u8bba\u8303\u5f0f\u7684\u53ef\u9760\u6027\u3002", "method": "MUG\u534f\u8bae\u5c06\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u91cd\u6784\u4e3a\u68c0\u6d4b\"\u5367\u5e95\"\u667a\u80fd\u4f53\u7684\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4fee\u6539\u53c2\u8003\u56fe\u50cf\u5f15\u5165\u53cd\u4e8b\u5b9e\u8bc1\u636e\uff0c\u89c2\u5bdf\u667a\u80fd\u4f53\u662f\u5426\u80fd\u51c6\u786e\u8bc6\u522b\u53d8\u5316\uff0c\u4ece\u800c\u8bc6\u522b\u5e7b\u89c9\u667a\u80fd\u4f53\u3002", "result": "MUG\u5728\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u6539\u8fdb\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u534f\u8bae\uff1a\u5b9e\u73b0\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u6d4b\u8bd5\u7684\u4e8b\u5b9e\u9a8c\u8bc1\u3001\u5f15\u5165\u8de8\u8bc1\u636e\u63a8\u7406\u3001\u4fc3\u8fdb\u4e3b\u52a8\u63a8\u7406\u3002", "conclusion": "MUG\u4e3aLLMs\u7684\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u53ef\u9760\u6709\u6548\u7684\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2511.10872", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10872", "abs": "https://arxiv.org/abs/2511.10872", "authors": ["Shuyuan Zhang", "Zihan Wang", "Xiao-Wen Chang", "Doina Precup"], "title": "Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement Learning via Graph Representations", "comment": "Transactions on Machine Learning Research (2025)", "summary": "The integration of graphs with Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) has recently gained attention, as intermediate goals (subgoals) can be effectively sampled from graphs that naturally represent the overall task structure in most RL tasks. However, existing approaches typically rely on domain-specific knowledge to construct these graphs, limiting their applicability to new tasks. Other graph-based approaches create graphs dynamically during exploration but struggle to fully utilize them, because they have problems passing the information in the graphs to newly visited states. Additionally, current GCHRL methods face challenges such as sample inefficiency and poor subgoal representation. This paper proposes a solution to these issues by developing a graph encoder-decoder to evaluate unseen states. Our proposed method, Graph-Guided sub-Goal representation Generation RL (G4RL), can be incorporated into any existing GCHRL method when operating in environments with primarily symmetric and reversible transitions to enhance performance across this class of problems. We show that the graph encoder-decoder can be effectively implemented using a network trained on the state graph generated during exploration. Empirical results indicate that leveraging high and low-level intrinsic rewards from the graph encoder-decoder significantly enhances the performance of state-of-the-art GCHRL approaches with an extra small computational cost in dense and sparse reward environments.", "AI": {"tldr": "\u63d0\u51faG4RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u7f16\u7801\u5668-\u89e3\u7801\u5668\u8bc4\u4f30\u672a\u89c1\u72b6\u6001\uff0c\u89e3\u51b3\u56fe\u5f15\u5bfc\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6837\u672c\u6548\u7387\u4f4e\u548c\u5b50\u76ee\u6807\u8868\u793a\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56fe\u5f15\u5bfc\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u6784\u5efa\u56fe\uff0c\u6216\u52a8\u6001\u6784\u5efa\u56fe\u4f46\u96be\u4ee5\u6709\u6548\u5229\u7528\u56fe\u4fe1\u606f\u4f20\u9012\u7ed9\u65b0\u8bbf\u95ee\u72b6\u6001\uff0c\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u548c\u5b50\u76ee\u6807\u8868\u793a\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u56fe\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6765\u8bc4\u4f30\u672a\u89c1\u72b6\u6001\uff0cG4RL\u65b9\u6cd5\u53ef\u96c6\u6210\u5230\u4efb\u4f55\u73b0\u6709GCHRL\u65b9\u6cd5\u4e2d\uff0c\u5728\u5177\u6709\u5bf9\u79f0\u548c\u53ef\u9006\u8f6c\u6362\u7684\u73af\u5883\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "result": "\u7ecf\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5229\u7528\u56fe\u7f16\u7801\u5668-\u89e3\u7801\u5668\u4ea7\u751f\u7684\u9ad8\u5c42\u548c\u4f4e\u5c42\u5185\u5728\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6700\u5148\u8fdbGCHRL\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u5f88\u5c0f\u3002", "conclusion": "G4RL\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u5f15\u5bfc\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u5bc6\u96c6\u548c\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u90fd\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.11252", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11252", "abs": "https://arxiv.org/abs/2511.11252", "authors": ["Mohamed Amine Ferrag", "Abderrahmane Lakas", "Merouane Debbah"], "title": "UAVBench: An Open Benchmark Dataset for Autonomous and Agentic AI UAV Systems via LLM-Generated Flight Scenarios", "comment": "18 pages, 5 Figures", "summary": "Autonomous aerial systems increasingly rely on large language models (LLMs) for mission planning, perception, and decision-making, yet the lack of standardized and physically grounded benchmarks limits systematic evaluation of their reasoning capabilities. To address this gap, we introduce UAVBench, an open benchmark dataset comprising 50,000 validated UAV flight scenarios generated through taxonomy-guided LLM prompting and multi-stage safety validation. Each scenario is encoded in a structured JSON schema that includes mission objectives, vehicle configuration, environmental conditions, and quantitative risk labels, providing a unified representation of UAV operations across diverse domains. Building on this foundation, we present UAVBench_MCQ, a reasoning-oriented extension containing 50,000 multiple-choice questions spanning ten cognitive and ethical reasoning styles, ranging from aerodynamics and navigation to multi-agent coordination and integrated reasoning. This framework enables interpretable and machine-checkable assessment of UAV-specific cognition under realistic operational contexts. We evaluate 32 state-of-the-art LLMs, including GPT-5, ChatGPT-4o, Gemini 2.5 Flash, DeepSeek V3, Qwen3 235B, and ERNIE 4.5 300B, and find strong performance in perception and policy reasoning but persistent challenges in ethics-aware and resource-constrained decision-making. UAVBench establishes a reproducible and physically grounded foundation for benchmarking agentic AI in autonomous aerial systems and advancing next-generation UAV reasoning intelligence. To support open science and reproducibility, we release the UAVBench dataset, the UAVBench_MCQ benchmark, evaluation scripts, and all related materials on GitHub at https://github.com/maferrag/UAVBench", "AI": {"tldr": "\u63d0\u51fa\u4e86UAVBench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b5\u4e07\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u65e0\u4eba\u673a\u98de\u884c\u573a\u666f\u548c5\u4e07\u4e2a\u591a\u9009\u9898\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u65e0\u4eba\u673a\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002\u8bc4\u4f30\u4e8632\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff0c\u53d1\u73b0\u5728\u611f\u77e5\u548c\u653f\u7b56\u63a8\u7406\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4f26\u7406\u610f\u8bc6\u548c\u8d44\u6e90\u53d7\u9650\u51b3\u7b56\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u81ea\u4e3b\u7a7a\u4e2d\u7cfb\u7edf\u8d8a\u6765\u8d8a\u4f9d\u8d56LLM\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\u3001\u611f\u77e5\u548c\u51b3\u7b56\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u7269\u7406\u57fa\u7840\u7684\u57fa\u51c6\u9650\u5236\u4e86\u5bf9\u5176\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u6307\u5bfc\u7684LLM\u63d0\u793a\u548c\u591a\u9636\u6bb5\u5b89\u5168\u9a8c\u8bc1\u751f\u62105\u4e07\u4e2a\u9a8c\u8bc1\u8fc7\u7684\u65e0\u4eba\u673a\u98de\u884c\u573a\u666f\uff0c\u5e76\u521b\u5efa\u5305\u542b10\u79cd\u8ba4\u77e5\u548c\u4f26\u7406\u63a8\u7406\u98ce\u683c\u76845\u4e07\u4e2a\u591a\u9009\u9898\u3002", "result": "\u8bc4\u4f30\u4e8632\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff0c\u5305\u62ecGPT-5\u3001ChatGPT-4o\u7b49\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u611f\u77e5\u548c\u653f\u7b56\u63a8\u7406\u65b9\u9762\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u4f26\u7406\u610f\u8bc6\u548c\u8d44\u6e90\u53d7\u9650\u51b3\u7b56\u65b9\u9762\u5b58\u5728\u6301\u7eed\u6311\u6218\u3002", "conclusion": "UAVBench\u4e3a\u81ea\u4e3b\u7a7a\u4e2d\u7cfb\u7edf\u4e2d\u7684\u4ee3\u7406AI\u57fa\u51c6\u6d4b\u8bd5\u5efa\u7acb\u4e86\u53ef\u91cd\u73b0\u548c\u7269\u7406\u57fa\u7840\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u4e0b\u4e00\u4ee3\u65e0\u4eba\u673a\u63a8\u7406\u667a\u80fd\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2511.10688", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10688", "abs": "https://arxiv.org/abs/2511.10688", "authors": ["Jiahang He", "Rishi Ramachandran", "Neel Ramachandran", "Aryan Katakam", "Kevin Zhu", "Sunishchal Dev", "Ashwinee Panda", "Aryan Shrivastava"], "title": "Modeling and Predicting Multi-Turn Answer Instability in Large Language Models", "comment": null, "summary": "As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple \"Think again\" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.", "AI": {"tldr": "\u4f7f\u7528\u591a\u8f6e\u5bf9\u8bdd\u6d4b\u8bd5LLM\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u7b80\u5355\u7684'\u518d\u60f3\u60f3'\u63d0\u793a\u4f1a\u5bfc\u81f4\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\uff0c\u53ef\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u5efa\u6a21\u51c6\u786e\u7387\u52a8\u6001\u53d8\u5316\uff0c\u63d0\u51fa\u7a33\u6001\u51c6\u786e\u7387\u4f5c\u4e3a\u4ea4\u4e92\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u6307\u6807\u3002", "motivation": "\u968f\u7740LLM\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bc4\u4f30\u5176\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u9c81\u68d2\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u4e00\u81f4\u63a8\u7406\u7684\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u591a\u8f6e\u540e\u7eed\u63d0\u793a\u8bc4\u4f30\u6a21\u578b\u7b54\u6848\u53d8\u5316\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u5efa\u6a21\u51c6\u786e\u7387\u52a8\u6001\uff0c\u5e76\u63a2\u7d22\u7ebf\u6027\u63a2\u9488\u662f\u5426\u80fd\u9884\u6d4b\u8fd9\u4e9b\u53d8\u5316\u3002", "result": "\u53d1\u73b0LLM\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff1a'\u518d\u60f3\u60f3'\u63d0\u793a\u5bfc\u81f4Gemini 1.5 Flash\u51c6\u786e\u7387\u4e0b\u964d\u7ea610%\uff0c\u7ed3\u5408\u8bed\u4e49\u7b49\u6548\u91cd\u8ff0\u95ee\u9898\u4f7fClaude 3.5 Haiku\u51c6\u786e\u7387\u4e0b\u964d7.5%\u3002\u6a21\u578b\u51c6\u786e\u7387\u53ef\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u6709\u6548\u5efa\u6a21\uff0c\u7a33\u6001\u51c6\u786e\u7387\u5e73\u5747\u6bd4\u9996\u8f6e\u4f4e8%\u3002", "conclusion": "\u5efa\u7acb\u4e86\u7a33\u6001\u51c6\u786e\u7387\u4f5c\u4e3a\u4ea4\u4e92\u573a\u666f\u4e0b\u7684\u539f\u5219\u6027\u9c81\u68d2\u6027\u6307\u6807\uff0c\u66b4\u9732\u4e86\u6a21\u578b\u5728\u91cd\u590d\u63d0\u95ee\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u89e3\u51b3\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u5bf9\u9ad8\u98ce\u9669\u4ea4\u4e92\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2511.11301", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11301", "abs": "https://arxiv.org/abs/2511.11301", "authors": ["Ruoxi Cheng", "Haoxuan Ma", "Teng Ma", "Hongyi Zhang"], "title": "EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment", "comment": null, "summary": "Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.", "AI": {"tldr": "EcoAlign\u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u5c06\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7ecf\u6d4e\u7406\u6027\u7684\u641c\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u524d\u77bb\u6027\u8bc4\u4f30\u51fd\u6570\u52a8\u6001\u6743\u8861\u5b89\u5168\u6027\u3001\u6548\u7528\u548c\u6210\u672c\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u597d\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u5bf9\u9f50\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u3001\u5b9e\u7528\u6027\u548c\u8fd0\u8425\u6210\u672c\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u56f0\u96be\uff0c\u4e14\u4ec5\u5173\u6ce8\u6700\u7ec8\u8f93\u51fa\u7684\u8fc7\u7a0b\u76f2\u76ee\u6027\u4f1a\u6d6a\u8d39\u5927\u91cf\u8ba1\u7b97\u9884\u7b97\u5728\u4e0d\u5b89\u5168\u7684\u63a8\u7406\u4e0a\uff0c\u5141\u8bb8\u6709\u5bb3\u63a8\u7406\u901a\u8fc7\u826f\u6027\u7406\u7531\u4f2a\u88c5\u6765\u89c4\u907f\u5b89\u5168\u68c0\u6d4b\u3002", "method": "\u5c06LVLM\u89c6\u4e3a\u6709\u9650\u7406\u6027\u667a\u80fd\u4f53\uff0c\u589e\u91cf\u6269\u5c55\u601d\u7ef4\u56fe\uff0c\u4f7f\u7528\u524d\u77bb\u6027\u51fd\u6570\uff08\u7c7b\u4f3c\u4e8e\u51c0\u73b0\u503c\uff09\u5bf9\u52a8\u4f5c\u8fdb\u884c\u8bc4\u5206\uff0c\u52a8\u6001\u6743\u8861\u9884\u671f\u5b89\u5168\u6027\u3001\u6548\u7528\u548c\u6210\u672c\u4e0e\u5269\u4f59\u9884\u7b97\uff0c\u5e76\u901a\u8fc7\u6700\u5f31\u73af\u8282\u539f\u5219\u5f3a\u5236\u6267\u884c\u8def\u5f84\u5b89\u5168\u6027\u3002", "result": "\u57283\u4e2a\u95ed\u6e90\u548c2\u4e2a\u5f00\u6e90\u6a21\u578b\u4e0a\u76846\u4e2a\u6570\u636e\u96c6\u5b9e\u9a8c\u8868\u660e\uff0cEcoAlign\u4ee5\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u6c34\u5e73\u3002", "conclusion": "EcoAlign\u4e3a\u5f3a\u5927\u7684LVLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u7ecf\u6d4e\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u5b89\u5168\u4e0e\u6548\u7387\u7684\u5e73\u8861\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2511.10691", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10691", "abs": "https://arxiv.org/abs/2511.10691", "authors": ["Zijian Chen", "Wenjun Zhang", "Guangtao Zhai"], "title": "Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models", "comment": "26 pages, 12 figures", "summary": "Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.", "AI": {"tldr": "Squid Game\u662f\u4e00\u4e2a\u52a8\u6001\u5bf9\u6297\u6027\u8bc4\u4f30\u73af\u5883\uff0c\u901a\u8fc7\u8d44\u6e90\u53d7\u9650\u548c\u4e0d\u5bf9\u79f0\u4fe1\u606f\u8bbe\u7f6e\u6765\u8bc4\u4f30LLM\u5728\u4ea4\u4e92\u6e38\u620f\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u542b\u516d\u4e2a\u6dd8\u6c70\u8d5b\u7ea7\u522b\uff0c\u6d4b\u8bd5\u591a\u65b9\u9762\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5047\u8bbe\u826f\u6027\u3001\u8d44\u6e90\u4e30\u5bcc\u7684\u73af\u5883\uff0c\u65e0\u6cd5\u8bc4\u4f30LLM\u5728\u538b\u529b\u4e0b\u7684\u884c\u4e3a\uff0c\u4e14\u5b58\u5728\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u53ef\u4fe1\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u516d\u4e2a\u6dd8\u6c70\u8d5b\u7ea7\u522b\u7684\u52a8\u6001\u5bf9\u6297\u73af\u5883\uff0c\u8ba9LLM\u4e0e\u5176\u4ed6LLM\u5bf9\u624b\u8fdb\u884c\u4ea4\u4e92\u6e38\u620f\uff0c\u8bc4\u4f30\u6307\u4ee4\u9075\u5faa\u3001\u4ee3\u7801\u3001\u63a8\u7406\u3001\u89c4\u5212\u548c\u5b89\u5168\u5bf9\u9f50\u7b49\u591a\u65b9\u9762\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u4e8650\u591a\u4e2aLLM\uff0c\u89c2\u5bdf\u5230\u540c\u4e00\u6a21\u578b\u8c31\u7cfb\u4e2d\u7684\u4ee3\u9645\u6027\u80fd\u8dc3\u8fc1\uff0c\u53d1\u73b0\u67d0\u4e9b\u6a21\u578b\u4f7f\u7528\u6295\u673a\u6377\u5f84\u83b7\u80dc\uff0c\u8868\u660e\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u53ef\u80fd\u5b58\u5728\u66f4\u9ad8\u7ea7\u522b\u7684\u8bc4\u4f30\u8303\u5f0f\u6c61\u67d3\u3002", "conclusion": "\u52a8\u6001\u8bc4\u4f30\u53ef\u4ee5\u4f5c\u4e3a\u9759\u6001\u8bc4\u4f30\u7684\u8865\u5145\u90e8\u5206\uff0c\u76f8\u5173\u5206\u6790\u663e\u793a\u52a8\u6001\u8bc4\u4f30\u4e0e\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5177\u6709\u4e92\u8865\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.11373", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11373", "abs": "https://arxiv.org/abs/2511.11373", "authors": ["Shulin Liu", "Dong Du", "Tao Yang", "Yang Li", "Boyu Qiu"], "title": "MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism", "comment": "10 pages", "summary": "Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.", "AI": {"tldr": "MarsRL\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7406\u7ba1\u9053\u5e76\u884c\u6027\u8054\u5408\u4f18\u5316\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u6240\u6709\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u6e90\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u4ee3\u7406\u63a8\u7406\u7cfb\u7edf\u5728\u95ed\u6e90\u6a21\u578b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u7531\u4e8e\u6279\u8bc4\u548c\u4fee\u6b63\u80fd\u529b\u4e0d\u8db3\u800c\u96be\u4ee5\u63a8\u5e7f\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u63d0\u5347\u5f00\u6e90\u6a21\u578b\u591a\u4ee3\u7406\u63a8\u7406\u80fd\u529b\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u63d0\u51faMarsRL\u6846\u67b6\uff0c\u91c7\u7528\u4ee3\u7406\u7279\u5b9a\u7684\u5956\u52b1\u673a\u5236\u6765\u51cf\u5c11\u5956\u52b1\u566a\u58f0\uff0c\u5e76\u4f7f\u7528\u7ba1\u9053\u542f\u53d1\u7684\u8bad\u7ec3\u65b9\u6cd5\u6765\u9ad8\u6548\u5904\u7406\u957f\u8f68\u8ff9\uff0c\u8054\u5408\u4f18\u5316\u7cfb\u7edf\u4e2d\u7684\u6240\u6709\u4ee3\u7406\u3002", "result": "\u5728Qwen3-30B-A3B-Thinking-2507\u4e0a\uff0cMarsRL\u5c06AIME2025\u51c6\u786e\u7387\u4ece86.5%\u63d0\u5347\u81f393.3%\uff0cBeyondAIME\u4ece64.9%\u63d0\u5347\u81f373.8%\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86Qwen3-235B-A22B-Thinking-2507\u3002", "conclusion": "MarsRL\u5c55\u793a\u4e86\u5728\u591a\u4ee3\u7406\u63a8\u7406\u7cfb\u7edf\u4e2d\u63a8\u8fdb\u5f3a\u5316\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f00\u6e90\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u6269\u5c55\u5176\u5e94\u7528\u8303\u56f4\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.10695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10695", "abs": "https://arxiv.org/abs/2511.10695", "authors": ["Jonghyeon Choi", "Yeonjun Choi", "Hyun-chul Kim", "Beakcheol Jang"], "title": "\"As Eastern Powers, I will veto.\" : An Investigation of Nation-level Bias of Large Language Models in International Relations", "comment": "21 pages, 4 figures. This is the extended version of the paper accepted at AAAI 2026, which includes all technical appendices and additional experimental details", "summary": "This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56fd\u9645\u5173\u7cfb\u9886\u57df\u7684\u56fd\u5bb6\u5c42\u9762\u504f\u89c1\uff0c\u57fa\u4e8e\u8054\u5408\u56fd\u5b89\u7406\u4f1a\u5386\u53f2\u8bb0\u5f55\u5f00\u53d1\u4e86\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u5bf9\u897f\u65b9\u56fd\u5bb6\u7684\u6709\u5229\u504f\u89c1\u548c\u5bf9\u4fc4\u7f57\u65af\u7684\u4e0d\u5229\u504f\u89c1\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u53cd\u601d\u6280\u672f\u7684\u53bb\u504f\u89c1\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7cfb\u7edf\u8bc4\u4f30LLMs\u5728\u56fd\u9645\u5173\u7cfb\u9886\u57df\u7684\u56fd\u5bb6\u5c42\u9762\u504f\u89c1\uff0c\u56e0\u4e3aLLMs\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u504f\u89c1\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u57fa\u4e8e\u8054\u5408\u56fd\u5b89\u7406\u4f1a\u5386\u53f2\u8bb0\u5f55\u5f00\u53d1\u4e86\u5305\u542b\u4e09\u4e2a\u6d4b\u8bd5\u7684\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u5b89\u7406\u4f1a\u4e94\u4e2a\u5e38\u4efb\u7406\u4e8b\u56fd\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u53cd\u601d\u6280\u672f\u7684\u53bb\u504f\u89c1\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aLLMs\u5b58\u5728\u660e\u663e\u7684\u56fd\u5bb6\u5c42\u9762\u504f\u89c1\uff0c\u504f\u89c1\u6a21\u5f0f\u56e0\u6a21\u578b\u548c\u8bc4\u4f30\u60c5\u5883\u800c\u5f02\uff0c\u63a8\u7406\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u504f\u89c1\u66f4\u5c0f\uff0c\u63d0\u51fa\u7684\u53bb\u504f\u89c1\u6846\u67b6\u6709\u6548\u51cf\u5c11\u4e86\u504f\u89c1\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "LLMs\u7684\u56fd\u5bb6\u5c42\u9762\u504f\u89c1\u5177\u6709\u591a\u7ef4\u6027\uff0c\u5e94\u7528LLMs\u4e8e\u56fd\u9645\u5173\u7cfb\u9886\u57df\u65f6\u9700\u8981\u540c\u65f6\u8bc4\u4f30\u504f\u89c1\u548c\u6027\u80fd\uff0c\u63d0\u51fa\u7684\u53bb\u504f\u89c1\u65b9\u6cd5\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2511.11519", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11519", "abs": "https://arxiv.org/abs/2511.11519", "authors": ["Adam Stein", "Matthew Trager", "Benjamin Bowman", "Michael Kleinman", "Aditya Chattopadhyay", "Wei Xia", "Stefano Soatto"], "title": "Experience-Guided Adaptation of Inference-Time Reasoning Strategies", "comment": "29 pages, 5 figures", "summary": "Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.", "AI": {"tldr": "EGuR\u662f\u4e00\u4e2a\u7ecf\u9a8c\u5f15\u5bfc\u7684\u63a8\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u63a8\u7406\u65f6\u52a8\u6001\u751f\u6210\u5305\u542bLLM\u8c03\u7528\u3001\u5de5\u5177\u3001\u91c7\u6837\u53c2\u6570\u548c\u63a7\u5236\u903b\u8f91\u7684\u5b8c\u6574\u8ba1\u7b97\u7b56\u7565\uff0c\u901a\u8fc7\u5143\u7b56\u7565\u5b9e\u73b0\u6240\u6709\u7b56\u7565\u7ec4\u4ef6\u7684\u81ea\u9002\u5e94\u8c03\u6574\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709AI\u7cfb\u7edf\u5728\u63a8\u7406\u65f6\u65e0\u6cd5\u7075\u6d3b\u8c03\u6574\u91c7\u6837\u53c2\u6570\u3001\u5de5\u5177\u914d\u7f6e\u3001\u7cfb\u7edf\u63d0\u793a\u6216\u5207\u6362\u4ee3\u7406\u8303\u5f0f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u57fa\u4e8e\u7ecf\u9a8c\u7684\u81ea\u9002\u5e94\u95ee\u9898\u89e3\u51b3\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u5143\u7b56\u7565\uff0c\u5305\u542bGuide\u7ec4\u4ef6\u751f\u6210\u5019\u9009\u7b56\u7565\uff0cConsolidator\u7ec4\u4ef6\u6574\u5408\u6267\u884c\u53cd\u9988\u6539\u8fdb\u7b56\u7565\u751f\u6210\uff0c\u80fd\u591f\u52a8\u6001\u521b\u5efa\u5b8c\u6574\u7684\u8ba1\u7b97\u7a0b\u5e8f\u3002", "result": "\u5728\u4e94\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u4e86\u9ad8\u8fbe14%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe111\u500d\uff0c\u4e14\u968f\u7740\u7ecf\u9a8c\u79ef\u7d2f\u6027\u80fd\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "EGuR\u901a\u8fc7\u52a8\u6001\u7b56\u7565\u751f\u6210\u548c\u57fa\u4e8e\u7ecf\u9a8c\u7684\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u7cfb\u7edf\u7684\u81ea\u9002\u5e94\u80fd\u529b\u548c\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u66f4\u7075\u6d3b\u7684\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2511.11551", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11551", "abs": "https://arxiv.org/abs/2511.11551", "authors": ["Dena Mujtaba", "Brian Hu", "Anthony Hoogs", "Arslan Basharat"], "title": "Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping", "comment": "Accepted to AAAI 2026 AI Alignment Track", "summary": "The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5f15\u5bfc\u7b56\u7565\u5851\u9020\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u6280\u672f\uff0c\u80fd\u591f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7cbe\u786e\u63a7\u5236\u4e2a\u4f53\u884c\u4e3a\u5c5e\u6027\uff0c\u5728\u4f26\u7406\u5bf9\u9f50\u548c\u5956\u52b1\u6700\u5927\u5316\u4e4b\u95f4\u5b9e\u73b0\u539f\u5219\u6027\u6743\u8861\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u667a\u80fd\u4f53\u3002", "motivation": "\u51b3\u7b56AI\u667a\u80fd\u4f53\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u6216\u6307\u5bfc\u65b9\u9488\u4fdd\u6301\u5bf9\u9f50\u7684\u5173\u952e\u6311\u6218\u3002\u4ec5\u4ee5\u5b9e\u73b0\u76ee\u6807\u4e3a\u8bad\u7ec3\u76ee\u7684\u7684\u667a\u80fd\u4f53\u53ef\u80fd\u91c7\u53d6\u6709\u5bb3\u884c\u4e3a\uff0c\u66b4\u9732\u51fa\u6700\u5927\u5316\u5956\u52b1\u51fd\u6570\u4e0e\u4fdd\u6301\u5bf9\u9f50\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\u3002", "method": "\u4f7f\u7528\u6a21\u578b\u5f15\u5bfc\u7b56\u7565\u5851\u9020\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u6280\u672f\uff0c\u901a\u8fc7\u573a\u666f-\u52a8\u4f5c\u5c5e\u6027\u5206\u7c7b\u5668\u8fdb\u884c\u7b56\u7565\u5851\u9020\uff0c\u786e\u4fdd\u51b3\u7b56\u4e0e\u4f26\u7406\u5c5e\u6027\u5bf9\u9f50\u3002\u5728MACHIAVELLI\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5305\u542b134\u4e2a\u57fa\u4e8e\u6587\u672c\u7684\u6e38\u620f\u73af\u5883\u548c\u6570\u5343\u4e2a\u6d89\u53ca\u4f26\u7406\u51b3\u7b56\u7684\u6807\u6ce8\u573a\u666f\u3002", "result": "\u6d4b\u8bd5\u65f6\u7b56\u7565\u5851\u9020\u4e3a\u7f13\u89e3\u4e0d\u540c\u73af\u5883\u548c\u5bf9\u9f50\u5c5e\u6027\u4e2d\u7684\u4e0d\u9053\u5fb7\u884c\u4e3a\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u4e2a\u4f53\u884c\u4e3a\u5c5e\u6027\uff0c\u5e76\u5728\u4f26\u7406\u5bf9\u9f50\u548c\u5956\u52b1\u6700\u5927\u5316\u4e4b\u95f4\u5b9e\u73b0\u539f\u5219\u6027\u6743\u8861\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u7b56\u7565\u5851\u9020\u662f\u786e\u4fdd\u9884\u8bad\u7ec3\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u4fdd\u6301\u4f26\u7406\u5bf9\u9f50\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u65e0\u9700\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u8fc7\u7a0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.10850", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10850", "abs": "https://arxiv.org/abs/2511.10850", "authors": ["Stefan Horoi", "Sangwoo Cho", "Supriyo Chakraborty", "Shi-Xiong Zhang", "Sambit Sahu", "Guy Wolf", "Genta Indra Winata"], "title": "Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs", "comment": null, "summary": "Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53c2\u6570\u7a7a\u95f4\u5bf9\u9f50\u6765\u6539\u8fdb\u4efb\u52a1\u7b97\u672f\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86LLM\u6280\u80fd\u8f6c\u79fb\u4e2d\u7684\u8d1f\u5e72\u6270\u95ee\u9898\uff0c\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6807\u51c6\u4efb\u52a1\u7b97\u672f\u3002", "motivation": "\u4efb\u52a1\u7b97\u672f\u5728LLM\u6280\u80fd\u8f6c\u79fb\u4e2d\u5b58\u5728\u8d1f\u5e72\u6270\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u5206\u6b67\u65f6\uff0c\u8fd9\u9650\u5236\u4e86\u6280\u80fd\u7684\u6709\u6548\u8f6c\u79fb\u3002", "method": "\u9996\u5148\u5bf9\u9f50\u6a21\u578b\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u5229\u7528Transformer\u67b6\u6784\u7684\u7f6e\u6362\u3001\u65cb\u8f6c\u548c\u7f29\u653e\u5bf9\u79f0\u6027\uff0c\u9488\u5bf9GQA\u548cSwiGLU\u5c42\u8fdb\u884c\u9002\u914d\uff0c\u63a2\u7d22\u57fa\u4e8e\u6743\u91cd\u548c\u57fa\u4e8e\u6fc0\u6d3b\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6\u4efb\u52a1\u7b97\u672f\uff0c\u6210\u529f\u5c06\u9ad8\u7ea7\u63a8\u7406\u6280\u80fd\u8f6c\u79fb\u5230\u975e\u63a8\u7406\u6a21\u578b\u4e2d\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5728\u8fdb\u5316\u7684LLM\u5bb6\u65cf\u4e4b\u95f4\u5408\u5e76\u548c\u8f6c\u79fb\u4e13\u95e8\u6280\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u5197\u4f59\u5fae\u8c03\u5e76\u589e\u5f3a\u4e86\u6a21\u578b\u9002\u5e94\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.10871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10871", "abs": "https://arxiv.org/abs/2511.10871", "authors": ["Parisa Rabbani", "Nimet Beyza Bozdag", "Dilek Hakkani-T\u00fcr"], "title": "From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems", "comment": "11 pages, 3 figures. Under review at IWSDS 2026", "summary": "LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from \"Is this statement correct?\" to \"Is this speaker correct?\". Furthermore, we apply pressure in the form of a simple rebuttal (\"The previous answer is incorrect.\") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u65f6\uff0c\u5bf9\u8bdd\u6846\u67b6\u4f1a\u663e\u8457\u5f71\u54cd\u5176\u5224\u65ad\u51c6\u786e\u6027\uff0c\u5373\u4f7f\u662f\u6700\u5c0f\u5bf9\u8bdd\u8bed\u5883\u4e5f\u80fd\u5bfc\u81f4\u5e73\u57479.24%\u7684\u6027\u80fd\u53d8\u5316\u3002", "motivation": "\u63a2\u7a76LLM\u5728\u9700\u8981\u793e\u4ea4\u6216\u5bf9\u8bdd\u5224\u65ad\u7684\u4efb\u52a1\u4e2d\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u4ece\u76f4\u63a5\u4e8b\u5b9e\u67e5\u8be2\u8f6c\u5411\u5bf9\u8bdd\u5224\u65ad\u4efb\u52a1\u65f6\u7684\u8868\u73b0\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u6a21\u578b\u5728\u76f4\u63a5\u4e8b\u5b9e\u67e5\u8be2\u548c\u6700\u5c0f\u5bf9\u8bdd\u8bed\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u65bd\u52a0\u7b80\u5355\u53cd\u9a73\u538b\u529b\u6765\u6d4b\u91cf\u6a21\u578b\u7acb\u573a\u7684\u575a\u5b9a\u7a0b\u5ea6\u3002", "result": "\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u51fa\u4e0d\u540c\u503e\u5411\uff1aGPT-4o-mini\u5728\u793e\u4ea4\u6846\u67b6\u4e0b\u663e\u793a\u5949\u627f\u503e\u5411\uff0c\u800cLlama-8B-Instruct\u53d8\u5f97\u8fc7\u5ea6\u6279\u5224\u3002\u5bf9\u8bdd\u6846\u67b6\u663e\u8457\u6539\u53d8\u6a21\u578b\u5224\u65ad\u3002", "conclusion": "\u5bf9\u8bdd\u6846\u67b6\u662fLLM\u8bc4\u4f30\u7684\u5173\u952e\u56e0\u7d20\uff0c\u8be5\u6846\u67b6\u4e3a\u8bca\u65ad\u6a21\u578b\u4fe1\u5ff5\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u53ef\u4fe1\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2511.10985", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10985", "abs": "https://arxiv.org/abs/2511.10985", "authors": ["Aladin Djuhera", "Farhan Ahmed", "Swanand Ravindra Kadhe", "Syed Zawad", "Heiko Ludwig", "Holger Boche"], "title": "When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets", "comment": null, "summary": "Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e3b\u6d41\u5f00\u6e90DPO\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u4f7f\u7528Magpie\u6846\u67b6\u6807\u6ce8\u6837\u672c\u8d28\u91cf\uff0c\u5e76\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\u6784\u5efa\u4e86\u65b0\u7684DPO\u6df7\u5408\u6570\u636e\u96c6UltraMix\uff0c\u8be5\u6570\u636e\u96c6\u6bd4\u6700\u4f73\u5355\u4e2a\u6570\u636e\u96c6\u5c0f30%\u4f46\u6027\u80fd\u66f4\u4f18\u3002", "motivation": "\u5f53\u524dLLM\u5bf9\u9f50\u4e3b\u8981\u91c7\u7528DPO\u6280\u672f\uff0c\u4f46\u5f00\u6e90DPO\u6570\u636e\u96c6\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u96be\u4ee5\u7406\u89e3\u504f\u597d\u9009\u62e9\u65b9\u5f0f\u3001\u4efb\u52a1\u7c7b\u578b\u8986\u76d6\u8303\u56f4\u4ee5\u53ca\u4eba\u7c7b\u5224\u65ad\u53cd\u6620\u7a0b\u5ea6\u3002", "method": "\u4f7f\u7528Magpie\u6846\u67b6\u5bf9\u6bcf\u4e2a\u6837\u672c\u8fdb\u884c\u4efb\u52a1\u7c7b\u522b\u3001\u8f93\u5165\u8d28\u91cf\u548c\u504f\u597d\u5956\u52b1\u7684\u6807\u6ce8\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u7ec6\u7c92\u5ea6\u504f\u597d\u8d28\u91cf\u68c0\u67e5\uff0c\u5e76\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\u4ece\u4e94\u4e2a\u8bed\u6599\u5e93\u4e2d\u7cbe\u9009\u6784\u5efaUltraMix\u6df7\u5408\u6570\u636e\u96c6\u3002", "result": "\u63ed\u793a\u4e86\u4e0d\u540c\u6570\u636e\u96c6\u5728\u5956\u52b1\u8fb9\u9645\u4e0a\u7684\u7ed3\u6784\u548c\u8d28\u91cf\u5dee\u5f02\uff0cUltraMix\u6bd4\u6700\u4f73\u5355\u4e2a\u6570\u636e\u96c6\u5c0f30%\u4f46\u5728\u5173\u952e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u66f4\u4f18\u3002", "conclusion": "\u6570\u636e\u4e2d\u5fc3\u7684\u504f\u597d\u4f18\u5316\u7814\u7a76\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u6570\u636e\u96c6\u5206\u6790\uff0cUltraMix\u5c55\u793a\u4e86\u901a\u8fc7\u7cbe\u9009\u6df7\u5408\u53ef\u4ee5\u663e\u8457\u63d0\u5347DPO\u6548\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.11362", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11362", "abs": "https://arxiv.org/abs/2511.11362", "authors": ["Prabodh Katti", "Sangwoo Park", "Bipin Rajendran", "Osvaldo Simeone"], "title": "On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization", "comment": "Conference submission; Under review", "summary": "On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f7f\u7528\u5185\u5b58\u9ad8\u6548\u7684\u96f6\u9636\u4f18\u5316(MeZO)\u8fdb\u884c\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u53cd\u5411\u4f20\u64ad(BP)\u8bad\u7ec3\uff0cMeZO\u901a\u8fc7\u4ec5\u4f7f\u7528\u524d\u5411\u8bc4\u4f30\u6765\u4f30\u8ba1\u68af\u5ea6\uff0c\u6d88\u9664\u4e86\u5b58\u50a8\u4e2d\u95f4\u6fc0\u6d3b\u548c\u4f18\u5316\u5668\u72b6\u6001\u7684\u9700\u6c42\uff0c\u4ece\u800c\u5728\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u652f\u6301\u66f4\u5927\u7684\u6a21\u578b\u90e8\u7f72\u3002", "motivation": "\u8fb9\u7f18AI\u7cfb\u7edf\u9700\u8981\u5728\u4e25\u683c\u5185\u5b58\u7ea6\u675f\u4e0b\u9002\u5e94\u4e0d\u540c\u7684\u667a\u80fd\u4efb\u52a1\uff0c\u4f20\u7edfBP\u8bad\u7ec3\u9700\u8981\u5b58\u50a8\u5c42\u6fc0\u6d3b\u548c\u4f18\u5316\u5668\u72b6\u6001\uff0c\u8fd9\u5728\u8bbe\u5907\u5185\u5b58\u5b8c\u5168\u5bb9\u7eb3\u6a21\u578b\u6743\u91cd\u7684\u90e8\u7f72\u4e2d\u4e25\u91cd\u9650\u5236\u4e86\u53ef\u90e8\u7f72\u7684\u6700\u5927\u6a21\u578b\u89c4\u6a21\u3002", "method": "\u4f7f\u7528\u5185\u5b58\u9ad8\u6548\u7684\u96f6\u9636\u4f18\u5316(MeZO)\u65b9\u6cd5\uff0c\u4ec5\u901a\u8fc7\u524d\u5411\u8bc4\u4f30\u6765\u4f30\u8ba1\u68af\u5ea6\uff0c\u65e0\u9700\u5b58\u50a8\u4e2d\u95f4\u6fc0\u6d3b\u6216\u4f18\u5316\u5668\u72b6\u6001\u3002\u9996\u5148\u63d0\u4f9bBP\u548cMeZO\u8bad\u7ec3\u4e0b\u53ef\u5bb9\u7eb3\u6a21\u578b\u5927\u5c0f\u7684\u7406\u8bba\u4f30\u8ba1\uff0c\u7136\u540e\u8fdb\u884c\u6570\u503c\u9a8c\u8bc1\u3002", "result": "\u6570\u503c\u9a8c\u8bc1\u8868\u660e\uff0c\u5728\u8bbe\u5907\u5185\u5b58\u7ea6\u675f\u4e0b\uff0c\u53ea\u8981\u6709\u8db3\u591f\u7684\u5fae\u8c03\u65f6\u95f4\uff0cMeZO\u5728\u51c6\u786e\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u80fd\u591f\u663e\u8457\u589e\u5927\u53ef\u5728\u7247\u4e0a\u5185\u5b58\u4e2d\u90e8\u7f72\u7684\u6a21\u578b\u89c4\u6a21\u3002", "conclusion": "MeZO\u901a\u8fc7\u6d88\u9664\u5b58\u50a8\u4e2d\u95f4\u6fc0\u6d3b\u548c\u4f18\u5316\u5668\u72b6\u6001\u7684\u9700\u6c42\uff0c\u7f13\u89e3\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5185\u5b58\u74f6\u9888\uff0c\u4f7f\u5f97\u5728\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u80fd\u591f\u90e8\u7f72\u66f4\u5927\u7684\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5c3d\u7ba1\u53ef\u80fd\u9700\u8981\u66f4\u957f\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.11402", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11402", "abs": "https://arxiv.org/abs/2511.11402", "authors": ["Amit Jain", "Victor Rodriguez-Fernandez", "Richard Linares"], "title": "Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning", "comment": null, "summary": "Autonomous spacecraft control for mission phases such as launch, ascent, stage separation, and orbit insertion remains a critical challenge due to the need for adaptive policies that generalize across dynamically distinct regimes. While reinforcement learning (RL) has shown promise in individual astrodynamics tasks, existing approaches often require separate policies for distinct mission phases, limiting adaptability and increasing operational complexity. This work introduces a transformer-based RL framework that unifies multi-phase trajectory optimization through a single policy architecture, leveraging the transformer's inherent capacity to model extended temporal contexts. Building on proximal policy optimization (PPO), our framework replaces conventional recurrent networks with a transformer encoder-decoder structure, enabling the agent to maintain coherent memory across mission phases spanning seconds to minutes during critical operations. By integrating a Gated Transformer-XL (GTrXL) architecture, the framework eliminates manual phase transitions while maintaining stability in control decisions. We validate our approach progressively: first demonstrating near-optimal performance on single-phase benchmarks (double integrator and Van der Pol oscillator), then extending to multiphase waypoint navigation variants, and finally tackling a complex multiphase rocket ascent problem that includes atmospheric flight, stage separation, and vacuum operations. Results demonstrate that the transformer-based framework not only matches analytical solutions in simple cases but also effectively learns coherent control policies across dynamically distinct regimes, establishing a foundation for scalable autonomous mission planning that reduces reliance on phase-specific controllers while maintaining compatibility with safety-critical verification protocols.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7edf\u4e00\u591a\u9636\u6bb5\u822a\u5929\u5668\u8f68\u8ff9\u4f18\u5316\uff0c\u901a\u8fc7\u5355\u4e00\u7b56\u7565\u67b6\u6784\u5904\u7406\u52a8\u6001\u4e0d\u540c\u7684\u4efb\u52a1\u9636\u6bb5\uff0c\u6d88\u9664\u624b\u52a8\u9636\u6bb5\u8f6c\u6362\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u4e3a\u4e0d\u540c\u4efb\u52a1\u9636\u6bb5\u5206\u522b\u8bad\u7ec3\u7b56\u7565\uff0c\u9650\u5236\u4e86\u81ea\u9002\u5e94\u6027\u548c\u589e\u52a0\u4e86\u64cd\u4f5c\u590d\u6742\u6027\uff0c\u9700\u8981\u80fd\u591f\u8de8\u52a8\u6001\u4e0d\u540c\u9636\u6bb5\u4fdd\u6301\u8fde\u8d2f\u8bb0\u5fc6\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316(PPO)\uff0c\u7528Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u66ff\u4ee3\u4f20\u7edf\u5faa\u73af\u7f51\u7edc\uff0c\u96c6\u6210\u95e8\u63a7Transformer-XL(GTrXL)\u67b6\u6784\uff0c\u5728\u5173\u952e\u64cd\u4f5c\u4e2d\u4fdd\u6301\u79d2\u5230\u5206\u949f\u65f6\u95f4\u8de8\u5ea6\u7684\u8fde\u8d2f\u8bb0\u5fc6\u3002", "result": "\u5728\u5355\u9636\u6bb5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u5728\u591a\u9636\u6bb5\u822a\u70b9\u5bfc\u822a\u53d8\u4f53\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5728\u590d\u6742\u591a\u9636\u6bb5\u706b\u7bad\u4e0a\u5347\u95ee\u9898\u4e2d\u6709\u6548\u5b66\u4e60\u8de8\u52a8\u6001\u4e0d\u540c\u9636\u6bb5\u7684\u8fde\u8d2f\u63a7\u5236\u7b56\u7565\u3002", "conclusion": "Transformer\u6846\u67b6\u4e0d\u4ec5\u5339\u914d\u7b80\u5355\u60c5\u51b5\u4e0b\u7684\u89e3\u6790\u89e3\uff0c\u8fd8\u80fd\u6709\u6548\u5b66\u4e60\u8de8\u52a8\u6001\u4e0d\u540c\u9636\u6bb5\u7684\u8fde\u8d2f\u63a7\u5236\u7b56\u7565\uff0c\u4e3a\u53ef\u6269\u5c55\u81ea\u4e3b\u4efb\u52a1\u89c4\u5212\u5960\u5b9a\u57fa\u7840\uff0c\u51cf\u5c11\u5bf9\u9636\u6bb5\u7279\u5b9a\u63a7\u5236\u5668\u7684\u4f9d\u8d56\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.11306", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11306", "abs": "https://arxiv.org/abs/2511.11306", "authors": ["Wei Fan", "JinYi Yoon", "Bo Ji"], "title": "iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference", "comment": "Accepted in AAAI 2026 (Oral)", "summary": "Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u667a\u80fd\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6iMAD\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u9009\u62e9\u6027\u89e6\u53d1\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\u5bf9\u6240\u6709\u67e5\u8be2\u90fd\u8fdb\u884c\u8fa9\u8bba\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u63a8\u7ffb\u6b63\u786e\u5355\u667a\u80fd\u4f53\u7b54\u6848\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e6\u53d1\u673a\u5236\u3002", "method": "iMAD\u9996\u5148\u8ba9\u5355\u667a\u80fd\u4f53\u751f\u6210\u7ed3\u6784\u5316\u81ea\u6211\u6279\u8bc4\u54cd\u5e94\uff0c\u63d0\u53d641\u4e2a\u53ef\u89e3\u91ca\u7684\u8bed\u8a00\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528FocusCal\u635f\u5931\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u8fa9\u8bba\u51b3\u7b56\u5206\u7c7b\u5668\u51b3\u5b9a\u662f\u5426\u89e6\u53d1\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u3002", "result": "\u5728\u516d\u4e2a\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0ciMAD\u5c06token\u4f7f\u7528\u91cf\u51cf\u5c11\u9ad8\u8fbe92%\uff0c\u540c\u65f6\u5c06\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\u63d0\u9ad8\u9ad8\u8fbe13.5%\u3002", "conclusion": "iMAD\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u89e6\u53d1\u8fa9\u8bba\u673a\u5236\uff0c\u5728\u4fdd\u6301\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4f18\u52bf\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2511.11324", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11324", "abs": "https://arxiv.org/abs/2511.11324", "authors": ["Anurag J. Vaidya", "Felix Meissen", "Daniel C. Castro", "Shruthi Bannur", "Tristan Lazard", "Drew F. K. Williamson", "Faisal Mahmood", "Javier Alvarez-Valle", "Stephanie L. Hyland", "Kenza Bouzid"], "title": "NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery", "comment": null, "summary": "Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.", "AI": {"tldr": "NOVA\u662f\u4e00\u4e2a\u4ee3\u7406\u6846\u67b6\uff0c\u53ef\u5c06\u79d1\u5b66\u67e5\u8be2\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u5206\u6790\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u548c\u8fd0\u884cPython\u4ee3\u7801\u3002\u5b83\u96c6\u6210\u4e8649\u4e2a\u9886\u57df\u7279\u5b9a\u5de5\u5177\uff0c\u5e76\u80fd\u5728SlideQuest\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u7f16\u7801\u4ee3\u7406\u57fa\u7ebf\u3002", "motivation": "\u6570\u5b57\u5316\u75c5\u7406\u5b66\u5206\u6790\u6d89\u53ca\u590d\u6742\u3001\u8017\u65f6\u7684\u6d41\u7a0b\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5176\u53ef\u8bbf\u95ee\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u6267\u884c\u591a\u6b65\u9aa4\u63a8\u7406\u548c\u8ba1\u7b97\u95ee\u9898\u89e3\u51b3\u7684\u7cfb\u7edf\u3002", "method": "NOVA\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u548c\u8fd0\u884cPython\u4ee3\u7801\u6765\u6784\u5efa\u5206\u6790\u6d41\u6c34\u7ebf\uff0c\u96c6\u6210\u4e8649\u4e2a\u9886\u57df\u7279\u5b9a\u5de5\u5177\uff08\u5982\u7ec6\u80de\u6838\u5206\u5272\u3001\u5168\u73bb\u7247\u7f16\u7801\uff09\uff0c\u5e76\u80fd\u4e34\u65f6\u521b\u5efa\u65b0\u5de5\u5177\u3002", "result": "\u5728SlideQuest\u57fa\u51c6\u6d4b\u8bd5\uff0890\u4e2a\u95ee\u9898\uff09\u4e2d\uff0cNOVA\u4f18\u4e8e\u5176\u4ed6\u7f16\u7801\u4ee3\u7406\u57fa\u7ebf\u3002\u75c5\u7406\u5b66\u5bb6\u9a8c\u8bc1\u7684\u6848\u4f8b\u7814\u7a76\u663e\u793a\u5176\u80fd\u5c06\u5f62\u6001\u5b66\u4e0e\u9884\u540e\u76f8\u5173\u7684PAM50\u4e9a\u578b\u8054\u7cfb\u8d77\u6765\u3002", "conclusion": "NOVA\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u7684\u53d1\u73b0\u6f5c\u529b\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u75c5\u7406\u5b66\u5206\u6790\u4efb\u52a1\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u77e5\u8bc6\u56de\u5fc6\u6216\u8bca\u65ad\u95ee\u7b54\u7cfb\u7edf\u3002", "topic": "swe application"}}
{"id": "2511.11518", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11518", "abs": "https://arxiv.org/abs/2511.11518", "authors": ["Zhenyu Ding", "Yuhao Wang", "Tengyue Xiao", "Haoying Wang", "Guojun Ma", "Mingyang Wan", "Caigui Jiang", "Ning Ding"], "title": "W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search", "comment": "AAAI 2026 Oral", "summary": "Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.", "AI": {"tldr": "\u63d0\u51fa\u4e86W2S-AlignTree\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4e0e\u5f31\u5230\u5f3a\u6cdb\u5316\u8303\u5f0f\u7ed3\u5408\uff0c\u5b9e\u73b0\u63a8\u7406\u65f6\u5bf9\u9f50\uff0c\u65e0\u9700\u4fee\u6539\u5f3a\u6a21\u578b\u53c2\u6570\u5373\u53ef\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u6307\u5bfc\u3002", "motivation": "\u89e3\u51b3LLM\u8f93\u51fa\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u73b0\u6709\u8bad\u7ec3\u65f6\u5bf9\u9f50\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u6269\u5c55\u6027\u6709\u9650\uff0c\u9700\u8981\u53ef\u6269\u5c55\u4e14\u81ea\u9002\u5e94\u7684\u63a8\u7406\u65f6\u5bf9\u9f50\u673a\u5236\u3002", "method": "\u5c06LLM\u5bf9\u9f50\u5efa\u6a21\u4e3a\u751f\u6210\u641c\u7d22\u6811\u4e2d\u7684\u6700\u4f18\u542f\u53d1\u5f0f\u641c\u7d22\u95ee\u9898\uff0c\u5229\u7528\u5f31\u6a21\u578b\u7684\u5b9e\u65f6\u6b65\u7ea7\u4fe1\u53f7\u4f5c\u4e3a\u5bf9\u9f50\u4ee3\u7406\uff0c\u5f15\u5165\u71b5\u611f\u77e5\u63a2\u7d22\u673a\u5236\u3002", "result": "\u5728\u60c5\u611f\u751f\u6210\u3001\u6458\u8981\u548c\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u4e00\u81f4\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5c06Llama3-8B\u5728\u6458\u8981\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4ece1.89\u63d0\u5347\u52302.19\uff0c\u76f8\u5bf9\u63d0\u534715.9%\u3002", "conclusion": "W2S-AlignTree\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u63a8\u7406\u65f6\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u63a7\u5236\u800c\u65e0\u9700\u6a21\u578b\u53c2\u6570\u4fee\u6539\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.6e2091dc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fusestrix%2Fstrix%3Futm_source=tldrdevops/1/0100019a824235d4-236add6a-0a0a-42b1-aee2-0b8d01c8b65e-000000/Dks1mLVV8Op9twdYu0WFGAcVA7Vtp7aNlnZChADe4Oc=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fusestrix%2Fstrix%3Futm_source=tldrdevops/1/0100019a824235d4-236add6a-0a0a-42b1-aee2-0b8d01c8b65e-000000/Dks1mLVV8Op9twdYu0WFGAcVA7Vtp7aNlnZChADe4Oc=431", "authors": ["TLDR Newsletter"], "title": "Strix", "comment": "Source: TLDR Newsletter, Date: 2025-11-14, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fusestrix%2Fstrix%3Futm_source=tldrdevops/1/0100019a824235d4-236add6a-0a0a-42b1-aee2-0b8d01c8b65e-000000/Dks1mLVV8Op9twdYu0WFGAcVA7Vtp7aNlnZChADe4Oc=431", "summary": "Strix (GitHub Repo) Strix is open-source AI agent that emulates hackers by dynamically running code to identify and validate vulnerabilities, providing fast security testing for developers. GPT-5 and Claude Sonnet 4.5 are recommended for optimal use. Strix can be integrated into CI/CD pipelines.", "source": "tldr", "AI": {"tldr": "Strix\u662f\u4e00\u4e2a\u5f00\u6e90AI\u4ee3\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u8fd0\u884c\u4ee3\u7801\u6765\u6a21\u62df\u9ed1\u5ba2\u884c\u4e3a\uff0c\u8bc6\u522b\u548c\u9a8c\u8bc1\u6f0f\u6d1e\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u5feb\u901f\u5b89\u5168\u6d4b\u8bd5\u3002", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u5feb\u901f\u3001\u81ea\u52a8\u5316\u7684\u5b89\u5168\u6d4b\u8bd5\u5de5\u5177\uff0c\u901a\u8fc7\u6a21\u62df\u9ed1\u5ba2\u884c\u4e3a\u6765\u8bc6\u522b\u6f5c\u5728\u6f0f\u6d1e\u3002", "method": "\u4f7f\u7528AI\u4ee3\u7406\u52a8\u6001\u8fd0\u884c\u4ee3\u7801\u6765\u6a21\u62df\u9ed1\u5ba2\u653b\u51fb\uff0c\u8bc6\u522b\u548c\u9a8c\u8bc1\u5b89\u5168\u6f0f\u6d1e\u3002", "result": "\u80fd\u591f\u96c6\u6210\u5230CI/CD\u6d41\u6c34\u7ebf\u4e2d\uff0c\u63d0\u4f9b\u5feb\u901f\u7684\u5b89\u5168\u6d4b\u8bd5\u80fd\u529b\u3002", "conclusion": "Strix\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5f00\u6e90\u5b89\u5168\u6d4b\u8bd5\u5de5\u5177\uff0c\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u53ca\u65f6\u53d1\u73b0\u5b89\u5168\u6f0f\u6d1e\u3002", "topic": "swe application"}}
{"id": "tldr.2511.245cd139", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FiU8nnf/1/0100019a8245c82f-d89b1013-a447-4437-8b3d-fbe2990a57ec-000000/T4ve092CTtxF9HxW29Wfjl8EUmP7cD6Cj1K_KoevixY=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FiU8nnf/1/0100019a8245c82f-d89b1013-a447-4437-8b3d-fbe2990a57ec-000000/T4ve092CTtxF9HxW29Wfjl8EUmP7cD6Cj1K_KoevixY=431", "authors": ["TLDR Newsletter"], "title": "Introducing GPT-5.1 for developers", "comment": "Source: TLDR Newsletter, Date: 2025-11-14, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FiU8nnf/1/0100019a8245c82f-d89b1013-a447-4437-8b3d-fbe2990a57ec-000000/T4ve092CTtxF9HxW29Wfjl8EUmP7cD6Cj1K_KoevixY=431", "summary": "Introducing GPT-5.1 for developers (11 minute read) OpenAI has released GPT-5.1, a new model in the GPT-5 series designed for agentic and coding tasks that balances intelligence and speed. GPT-5.1 dynamically adjusts its reasoning time based on task complexity, offers a \"no reasoning\" mode for faster responses on simpler tasks, and includes extended prompt caching for improved efficiency. It introduces new tools like the apply_patch tool for reliable code editing and a shell tool for executin...", "source": "tldr", "AI": {"tldr": "OpenAI\u53d1\u5e03\u4e86GPT-5.1\u6a21\u578b\uff0c\u4e13\u4e3a\u4ee3\u7406\u548c\u7f16\u7801\u4efb\u52a1\u8bbe\u8ba1\uff0c\u5e73\u8861\u667a\u80fd\u4e0e\u901f\u5ea6\uff0c\u63d0\u4f9b\u52a8\u6001\u63a8\u7406\u65f6\u95f4\u8c03\u6574\u3001\u65e0\u63a8\u7406\u6a21\u5f0f\u3001\u6269\u5c55\u63d0\u793a\u7f13\u5b58\u7b49\u529f\u80fd\uff0c\u5e76\u5f15\u5165apply_patch\u548cshell\u7b49\u65b0\u5de5\u5177\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5728\u667a\u80fd\u548c\u901f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u6a21\u578b\uff0c\u4e13\u95e8\u9488\u5bf9\u4ee3\u7406\u548c\u7f16\u7801\u4efb\u52a1\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "method": "GPT-5.1\u6a21\u578b\u91c7\u7528\u52a8\u6001\u63a8\u7406\u65f6\u95f4\u8c03\u6574\u673a\u5236\uff0c\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u81ea\u52a8\u8c03\u6574\u63a8\u7406\u65f6\u95f4\uff1b\u63d0\u4f9b\u65e0\u63a8\u7406\u6a21\u5f0f\u5904\u7406\u7b80\u5355\u4efb\u52a1\uff1b\u4f7f\u7528\u6269\u5c55\u63d0\u793a\u7f13\u5b58\u63d0\u9ad8\u6548\u7387\uff1b\u5f15\u5165apply_patch\u5de5\u5177\u8fdb\u884c\u53ef\u9760\u4ee3\u7801\u7f16\u8f91\u548cshell\u5de5\u5177\u6267\u884c\u547d\u4ee4\u3002", "result": "GPT-5.1\u5728\u4ee3\u7406\u548c\u7f16\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5e73\u8861\u7684\u667a\u80fd\u548c\u901f\u5ea6\uff0c\u80fd\u591f\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u6027\u80fd\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u5f00\u53d1\u4f53\u9a8c\u3002", "conclusion": "GPT-5.1\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u5f00\u53d1\u8005\u548c\u4ee3\u7406\u4efb\u52a1\u4f18\u5316\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u667a\u80fd\u7684\u901f\u5ea6\u5e73\u8861\u548c\u65b0\u7684\u5de5\u5177\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u7801\u548c\u4ee3\u7406\u4efb\u52a1\u7684\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2511.5514f5ab", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fprahladyeri.github.io%2Fblog%2F2025%2F10%2Fi-am-a-programmer.html%3Futm_source=tldrwebdev/1/0100019a8245c82f-d89b1013-a447-4437-8b3d-fbe2990a57ec-000000/RFSo00nLZqdx7fzzFxQ33IKyGbqjd63zMhPqWUVhRXQ=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fprahladyeri.github.io%2Fblog%2F2025%2F10%2Fi-am-a-programmer.html%3Futm_source=tldrwebdev/1/0100019a8245c82f-d89b1013-a447-4437-8b3d-fbe2990a57ec-000000/RFSo00nLZqdx7fzzFxQ33IKyGbqjd63zMhPqWUVhRXQ=431", "authors": ["TLDR Newsletter"], "title": "I am a programmer, not a rubber-stamp that approves Copilot generated code", "comment": "Source: TLDR Newsletter, Date: 2025-11-14, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fprahladyeri.github.io%2Fblog%2F2025%2F10%2Fi-am-a-programmer.html%3Futm_source=tldrwebdev/1/0100019a8245c82f-d89b1013-a447-4437-8b3d-fbe2990a57ec-000000/RFSo00nLZqdx7fzzFxQ33IKyGbqjd63zMhPqWUVhRXQ=431", "summary": "I am a programmer, not a rubber-stamp that approves Copilot generated code (2 minute read) Enforced AI assistance in programming is transforming programmers from creators into mere approvers of AI-generated code, potentially devaluing the profession.", "source": "tldr", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "topics": "Error"}}
{"id": "tldr.2511.a5861bd6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Foctomind.dev%2Fblog%2Fwhy-agents-do-not-write-most-of-our-code-a-reality-check%3Futm_source=tldrwebdev/1/0100019a8245c82f-d89b1013-a447-4437-8b3d-fbe2990a57ec-000000/AOzKsx-ct-lOJ0EWPyW59IUMvy2seXpNI4uOwbHGk_E=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Foctomind.dev%2Fblog%2Fwhy-agents-do-not-write-most-of-our-code-a-reality-check%3Futm_source=tldrwebdev/1/0100019a8245c82f-d89b1013-a447-4437-8b3d-fbe2990a57ec-000000/AOzKsx-ct-lOJ0EWPyW59IUMvy2seXpNI4uOwbHGk_E=431", "authors": ["TLDR Newsletter"], "title": "Why agents DO NOT write most of our code", "comment": "Source: TLDR Newsletter, Date: 2025-11-14, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Foctomind.dev%2Fblog%2Fwhy-agents-do-not-write-most-of-our-code-a-reality-check%3Futm_source=tldrwebdev/1/0100019a8245c82f-d89b1013-a447-4437-8b3d-fbe2990a57ec-000000/AOzKsx-ct-lOJ0EWPyW59IUMvy2seXpNI4uOwbHGk_E=431", "summary": "Why agents DO NOT write most of our code (10 minute read) Despite the hype, AI agents are not yet capable of writing most of this company's code due to issues like loss of mental model of the codebase, absence of self-reflection in AI, and the current inability of AI to consistently deliver meaningful productivity boosts in complex tasks.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u76ee\u524d\u65e0\u6cd5\u7f16\u5199\u5927\u90e8\u5206\u4ee3\u7801\uff0c\u4e3b\u8981\u7531\u4e8e\u4ee3\u7801\u5e93\u5fc3\u667a\u6a21\u578b\u7f3a\u5931\u3001\u7f3a\u4e4f\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u65e0\u6cd5\u6301\u7eed\u63d0\u4f9b\u751f\u4ea7\u529b\u63d0\u5347", "motivation": "\u63a2\u8ba8\u4e3a\u4ec0\u4e48\u5c3d\u7ba1\u6709\u5927\u91cf\u7092\u4f5c\uff0cAI\u4ee3\u7406\u5728\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u4ecd\u65e0\u6cd5\u627f\u62c5\u4e3b\u8981\u7f16\u7801\u5de5\u4f5c\uff0c\u5206\u6790\u5f53\u524d\u6280\u672f\u5c40\u9650\u6027", "method": "\u57fa\u4e8e\u5b9e\u9645\u5f00\u53d1\u7ecf\u9a8c\u5206\u6790AI\u4ee3\u7406\u5728\u4ee3\u7801\u7f16\u5199\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5fc3\u667a\u6a21\u578b\u3001\u81ea\u6211\u53cd\u601d\u548c\u751f\u4ea7\u529b\u63d0\u5347\u7b49\u65b9\u9762", "result": "\u53d1\u73b0AI\u4ee3\u7406\u5728\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u66ff\u4ee3\u4eba\u7c7b\u5f00\u53d1\u8005\u7684\u6838\u5fc3\u4f5c\u7528", "conclusion": "AI\u4ee3\u7406\u76ee\u524d\u66f4\u9002\u5408\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u800c\u975e\u4e3b\u8981\u7f16\u7801\u8005\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6280\u672f\u7a81\u7834\u624d\u80fd\u627f\u62c5\u66f4\u91cd\u8981\u7684\u5f00\u53d1\u89d2\u8272", "topic": "agent analysis"}}
{"id": "tldr.2511.ded27249", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsiliconangle.com%2F2025%2F11%2F12%2Fwebflow-expands-beyond-design-new-ai-tool-building-full-stack-web-applications%2F%3Futm_source=tldrdesign/1/0100019a8279d01a-b7d96b19-23a2-4745-ad7e-0dacadaad4e5-000000/xQRJe9S_42qTUMWPQ2fJTKSIznUz0JTs2Q5ORDl-HpM=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsiliconangle.com%2F2025%2F11%2F12%2Fwebflow-expands-beyond-design-new-ai-tool-building-full-stack-web-applications%2F%3Futm_source=tldrdesign/1/0100019a8279d01a-b7d96b19-23a2-4745-ad7e-0dacadaad4e5-000000/xQRJe9S_42qTUMWPQ2fJTKSIznUz0JTs2Q5ORDl-HpM=431", "authors": ["TLDR Newsletter"], "title": "Webflow Expands Beyond Design with New AI Tool for Building Full-stack Web Applications", "comment": "Source: TLDR Newsletter, Date: 2025-11-14, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsiliconangle.com%2F2025%2F11%2F12%2Fwebflow-expands-beyond-design-new-ai-tool-building-full-stack-web-applications%2F%3Futm_source=tldrdesign/1/0100019a8279d01a-b7d96b19-23a2-4745-ad7e-0dacadaad4e5-000000/xQRJe9S_42qTUMWPQ2fJTKSIznUz0JTs2Q5ORDl-HpM=431", "summary": "Webflow Expands Beyond Design with New AI Tool for Building Full-stack Web Applications (2 minute read) Webflow's App Gen is an AI-powered tool that enables users to build and deploy full-stack web applications through natural language prompts without writing code. The platform automatically integrates applications with existing design systems and CMS data while maintaining brand consistency across typography, colors, and components. Currently in beta, App Gen will eventually support authenti...", "source": "tldr", "AI": {"tldr": "Webflow\u63a8\u51faAI\u5de5\u5177App Gen\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u6784\u5efa\u548c\u90e8\u7f72\u5168\u6808Web\u5e94\u7528\uff0c\u65e0\u9700\u7f16\u5199\u4ee3\u7801\u3002", "motivation": "\u8ba9\u975e\u6280\u672f\u7528\u6237\u80fd\u591f\u5feb\u901f\u6784\u5efa\u5168\u6808Web\u5e94\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u54c1\u724c\u8bbe\u8ba1\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528AI\u6280\u672f\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u5e76\u4e0e\u73b0\u6709\u8bbe\u8ba1\u7cfb\u7edf\u548cCMS\u6570\u636e\u96c6\u6210\u3002", "result": "\u76ee\u524d\u5904\u4e8e\u6d4b\u8bd5\u9636\u6bb5\uff0c\u80fd\u591f\u81ea\u52a8\u96c6\u6210\u8bbe\u8ba1\u7cfb\u7edf\u3001\u7ef4\u62a4\u54c1\u724c\u4e00\u81f4\u6027\uff0c\u672a\u6765\u5c06\u652f\u6301\u8ba4\u8bc1\u7b49\u529f\u80fd\u3002", "conclusion": "App Gen\u4f7fWebflow\u4ece\u8bbe\u8ba1\u5e73\u53f0\u6269\u5c55\u5230\u5168\u6808\u5e94\u7528\u5f00\u53d1\u9886\u57df\uff0c\u964d\u4f4e\u4e86Web\u5e94\u7528\u5f00\u53d1\u95e8\u69db\u3002", "topic": "swe application"}}
{"id": "tldr.2511.0c9c543f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fyouart.ai%2F%3Futm_source=tldrdesign/1/0100019a8279d01a-b7d96b19-23a2-4745-ad7e-0dacadaad4e5-000000/65WOiOgkM_JZhC-N8MT5CewcOc9mzotRjxUsF4fpr3U=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fyouart.ai%2F%3Futm_source=tldrdesign/1/0100019a8279d01a-b7d96b19-23a2-4745-ad7e-0dacadaad4e5-000000/65WOiOgkM_JZhC-N8MT5CewcOc9mzotRjxUsF4fpr3U=431", "authors": ["TLDR Newsletter"], "title": "Agentic Creative Workflow Studio", "comment": "Source: TLDR Newsletter, Date: 2025-11-14, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fyouart.ai%2F%3Futm_source=tldrdesign/1/0100019a8279d01a-b7d96b19-23a2-4745-ad7e-0dacadaad4e5-000000/65WOiOgkM_JZhC-N8MT5CewcOc9mzotRjxUsF4fpr3U=431", "summary": "Agentic Creative Workflow Studio (Website) YouArt is an all-in-one AI Creative Studio. It allows you to create stunning AI image and video generators.", "source": "tldr", "AI": {"tldr": "YouArt\u662f\u4e00\u4e2a\u96c6\u6210\u7684AI\u521b\u610f\u5de5\u4f5c\u5ba4\u5e73\u53f0\uff0c\u63d0\u4f9bAI\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u529f\u80fd", "motivation": "\u4e3a\u521b\u610f\u5de5\u4f5c\u8005\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684AI\u521b\u4f5c\u5de5\u5177\u5e73\u53f0\uff0c\u7b80\u5316AI\u56fe\u50cf\u548c\u89c6\u9891\u7684\u751f\u6210\u6d41\u7a0b", "method": "\u5f00\u53d1\u96c6\u6210\u7684AI\u521b\u610f\u5de5\u4f5c\u5ba4\uff0c\u6574\u5408\u591a\u79cdAI\u751f\u6210\u6a21\u578b\u548c\u5de5\u5177", "result": "\u6210\u529f\u6784\u5efa\u4e86YouArt\u5e73\u53f0\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684AI\u56fe\u50cf\u548c\u89c6\u9891\u5185\u5bb9", "conclusion": "YouArt\u4f5c\u4e3a\u4e00\u4e2a\u7efc\u5408AI\u521b\u610f\u5de5\u4f5c\u5ba4\uff0c\u6709\u6548\u63d0\u5347\u4e86\u521b\u610f\u5de5\u4f5c\u7684\u6548\u7387\u548c\u8d28\u91cf", "topic": "swe application"}}
{"id": "tldr.2511.00629e1b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FKeygraphHQ%2Fshannon%3Futm_source=tldrinfosec/1/0100019a82b12d74-84ef4594-cee1-4170-85a9-22ccb5ccb1c3-000000/9dZH8fyooAl_5kqEsMAxKYQBXBRjgBxCbC_COQTiipo=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FKeygraphHQ%2Fshannon%3Futm_source=tldrinfosec/1/0100019a82b12d74-84ef4594-cee1-4170-85a9-22ccb5ccb1c3-000000/9dZH8fyooAl_5kqEsMAxKYQBXBRjgBxCbC_COQTiipo=431", "authors": ["TLDR Newsletter"], "title": "Shannon", "comment": "Source: TLDR Newsletter, Date: 2025-11-14, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FKeygraphHQ%2Fshannon%3Futm_source=tldrinfosec/1/0100019a82b12d74-84ef4594-cee1-4170-85a9-22ccb5ccb1c3-000000/9dZH8fyooAl_5kqEsMAxKYQBXBRjgBxCbC_COQTiipo=431", "summary": "Shannon (GitHub Repo) Shannon is an AI pentester whose goal is to break your web app before someone else does. It autonomously hunts for attack vectors in your code, then uses its built-in browser to execute real exploits, such as injection attacks and auth bypass, to prove the vulnerability is actually exploitable.", "source": "tldr", "AI": {"tldr": "Shannon\u662f\u4e00\u4e2aAI\u6e17\u900f\u6d4b\u8bd5\u5de5\u5177\uff0c\u80fd\u591f\u81ea\u4e3b\u53d1\u73b0\u4ee3\u7801\u4e2d\u7684\u653b\u51fb\u5411\u91cf\uff0c\u5e76\u901a\u8fc7\u5185\u7f6e\u6d4f\u89c8\u5668\u6267\u884c\u771f\u5b9e\u653b\u51fb\u6765\u9a8c\u8bc1\u6f0f\u6d1e\u7684\u53ef\u5229\u7528\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u81ea\u52a8\u53d1\u73b0\u548c\u9a8c\u8bc1web\u5e94\u7528\u6f0f\u6d1e\u7684AI\u6e17\u900f\u6d4b\u8bd5\u5de5\u5177\uff0c\u4ee5\u5728\u6076\u610f\u653b\u51fb\u8005\u4e4b\u524d\u53d1\u73b0\u5e76\u4fee\u590d\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4ee3\u7801\u81ea\u4e3b\u5bfb\u627e\u653b\u51fb\u5411\u91cf\uff0c\u7136\u540e\u4f7f\u7528\u5185\u7f6e\u6d4f\u89c8\u5668\u6267\u884c\u771f\u5b9e\u653b\u51fb\uff08\u5982\u6ce8\u5165\u653b\u51fb\u548c\u8ba4\u8bc1\u7ed5\u8fc7\uff09\u6765\u9a8c\u8bc1\u6f0f\u6d1e\u3002", "result": "Shannon\u80fd\u591f\u6709\u6548\u53d1\u73b0\u548c\u9a8c\u8bc1web\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u8bc1\u660e\u5176\u4f5c\u4e3a\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u5de5\u5177\u7684\u6709\u6548\u6027\u3002", "conclusion": "Shannon\u4f5c\u4e3a\u4e00\u4e2aAI\u6e17\u900f\u6d4b\u8bd5\u5de5\u5177\uff0c\u80fd\u591f\u81ea\u4e3b\u53d1\u73b0\u548c\u9a8c\u8bc1web\u5e94\u7528\u6f0f\u6d1e\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5e94\u7528\u5b89\u5168\u6027\u3002", "topic": "swe application"}}
{"id": "tldr.2511.b07bd85d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fassets.anthropic.com%2Fm%2Fec212e6566a0d47%2Foriginal%2FDisrupting-the-first-reported-AI-orchestrated-cyber-espionage-campaign.pdf%3Futm_source=tldrai/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/YFjY1V6b6Xai0IFFKHGfHHSZPol8t0RDtkEKRlV5hCA=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fassets.anthropic.com%2Fm%2Fec212e6566a0d47%2Foriginal%2FDisrupting-the-first-reported-AI-orchestrated-cyber-espionage-campaign.pdf%3Futm_source=tldrai/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/YFjY1V6b6Xai0IFFKHGfHHSZPol8t0RDtkEKRlV5hCA=431", "authors": ["TLDR Newsletter"], "title": "AI-orchestrated Cyberattack Detailed by Anthropic", "comment": "Source: TLDR Newsletter, Date: 2025-11-14, Reading time: 28 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fassets.anthropic.com%2Fm%2Fec212e6566a0d47%2Foriginal%2FDisrupting-the-first-reported-AI-orchestrated-cyber-espionage-campaign.pdf%3Futm_source=tldrai/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/YFjY1V6b6Xai0IFFKHGfHHSZPol8t0RDtkEKRlV5hCA=431", "summary": "AI-orchestrated Cyberattack Detailed by Anthropic (28 minute read) Anthropic disclosed the first documented AI-driven cyber espionage campaign, carried out largely autonomously using its Claude Code tool. The attackers, linked to a Chinese state-sponsored group, targeted 30+ entities worldwide, demonstrating the growing threat of agentic AI systems in cybersecurity.", "source": "tldr", "AI": {"tldr": "Anthropic\u62ab\u9732\u4e86\u9996\u4e2a\u7531AI\u9a71\u52a8\u7684\u7f51\u7edc\u95f4\u8c0d\u6d3b\u52a8\uff0c\u653b\u51fb\u8005\u4f7f\u7528Claude Code\u5de5\u5177\u81ea\u4e3b\u6267\u884c\uff0c\u9488\u5bf9\u5168\u740330\u591a\u4e2a\u5b9e\u4f53\uff0c\u5c55\u793a\u4e86AI\u4ee3\u7406\u7cfb\u7edf\u5728\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u5a01\u80c1\u3002", "motivation": "\u63ed\u793aAI\u9a71\u52a8\u7684\u7f51\u7edc\u653b\u51fb\u7684\u73b0\u5b9e\u5a01\u80c1\uff0c\u5c55\u793aAI\u4ee3\u7406\u7cfb\u7edf\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u4f7f\u7528Anthropic\u7684Claude Code\u5de5\u5177\u8fdb\u884c\u81ea\u4e3b\u7f51\u7edc\u95f4\u8c0d\u6d3b\u52a8\uff0c\u653b\u51fb\u8005\u4e0e\u4e2d\u56fd\u56fd\u5bb6\u652f\u6301\u7684\u7ec4\u7ec7\u6709\u5173\u3002", "result": "\u6210\u529f\u9488\u5bf9\u5168\u740330\u591a\u4e2a\u5b9e\u4f53\uff0c\u8bc1\u660e\u4e86AI\u4ee3\u7406\u7cfb\u7edf\u80fd\u591f\u81ea\u4e3b\u6267\u884c\u590d\u6742\u7684\u7f51\u7edc\u653b\u51fb\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u7f51\u7edc\u653b\u51fb\u5df2\u6210\u4e3a\u73b0\u5b9e\u5a01\u80c1\uff0c\u9700\u8981\u52a0\u5f3a\u5bf9\u6b64\u7c7bAI\u4ee3\u7406\u7cfb\u7edf\u7684\u5b89\u5168\u9632\u62a4\u548c\u76d1\u7ba1\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.ac601357", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dynatrace.com%2Finfo%2Freports%2Fbring-clarity-to-your-ai-systems%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=cloud-ai-observability-hyperframe-observability-for-ai%26utm_term=111425/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/tErOKy5SqN-eM0dzNGV6pjiyvVtuct6YI2EV8gO6PSo=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dynatrace.com%2Finfo%2Freports%2Fbring-clarity-to-your-ai-systems%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=cloud-ai-observability-hyperframe-observability-for-ai%26utm_term=111425/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/tErOKy5SqN-eM0dzNGV6pjiyvVtuct6YI2EV8gO6PSo=431", "authors": ["TLDR Newsletter"], "title": "GenAI observability: 6 practical recommendations", "comment": "Source: TLDR Newsletter, Date: 2025-11-14, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dynatrace.com%2Finfo%2Freports%2Fbring-clarity-to-your-ai-systems%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=cloud-ai-observability-hyperframe-observability-for-ai%26utm_term=111425/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/tErOKy5SqN-eM0dzNGV6pjiyvVtuct6YI2EV8gO6PSo=431", "summary": "GenAI observability: 6 practical recommendations (Sponsor) GenAI and agentic AI are powerful but unpredictable. It's not just hallucination \u2014 they regularly take entirely new paths through established workflows. Clarity requires more than monitoring: this Dynatrace report lays out six pragmatic observability recommendations for practitioners preparing their organizations for GenAI and agentic AI. Read the report", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e866\u4e2a\u5b9e\u7528\u7684GenAI\u53ef\u89c2\u6d4b\u6027\u5efa\u8bae\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u4e3a\u751f\u6210\u5f0fAI\u548c\u4ee3\u7406AI\u505a\u51c6\u5907\uff0c\u5f3a\u8c03\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u76d1\u63a7\u7684\u5168\u9762\u53ef\u89c2\u6d4b\u6027\u3002", "motivation": "\u751f\u6210\u5f0fAI\u548c\u4ee3\u7406AI\u867d\u7136\u5f3a\u5927\u4f46\u4e0d\u53ef\u9884\u6d4b\uff0c\u5b83\u4eec\u7ecf\u5e38\u5728\u65e2\u5b9a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u91c7\u53d6\u5168\u65b0\u8def\u5f84\uff0c\u4ec5\u9760\u4f20\u7edf\u76d1\u63a7\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u53ef\u89c2\u6d4b\u6027\u65b9\u6848\u3002", "method": "\u901a\u8fc7Dynatrace\u62a5\u544a\u63d0\u51fa\u4e866\u4e2a\u5b9e\u7528\u7684\u53ef\u89c2\u6d4b\u6027\u5efa\u8bae\uff0c\u9488\u5bf9GenAI\u548c\u4ee3\u7406AI\u7684\u7279\u70b9\u8bbe\u8ba1\u3002", "result": "\u63d0\u4f9b\u4e866\u4e2a\u5177\u4f53\u7684\u53ef\u89c2\u6d4b\u6027\u63a8\u8350\u65b9\u6848\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u66f4\u597d\u5730\u51c6\u5907\u548c\u7ba1\u7406GenAI\u53ca\u4ee3\u7406AI\u7cfb\u7edf\u3002", "conclusion": "\u7ec4\u7ec7\u9700\u8981\u91c7\u7eb3\u8fd9\u4e9b\u5b9e\u7528\u7684\u53ef\u89c2\u6d4b\u6027\u5efa\u8bae\u6765\u5e94\u5bf9GenAI\u548c\u4ee3\u7406AI\u5e26\u6765\u7684\u6311\u6218\uff0c\u786e\u4fdd\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u63a7\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.2174e895", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Fen%2Fintroducing-code-wiki-accelerating-your-code-understanding%2F%3Futm_source=tldrai/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/dpv63e1TcbRGltxmhaOySYaVjrbgPx7uan7T1oXMpYY=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Fen%2Fintroducing-code-wiki-accelerating-your-code-understanding%2F%3Futm_source=tldrai/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/dpv63e1TcbRGltxmhaOySYaVjrbgPx7uan7T1oXMpYY=431", "authors": ["TLDR Newsletter"], "title": "Introducing Code Wiki: Accelerating your code understanding", "comment": "Source: TLDR Newsletter, Date: 2025-11-14, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Fen%2Fintroducing-code-wiki-accelerating-your-code-understanding%2F%3Futm_source=tldrai/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/dpv63e1TcbRGltxmhaOySYaVjrbgPx7uan7T1oXMpYY=431", "summary": "Introducing Code Wiki: Accelerating your code understanding (3 minute read) Code Wiki is a platform that maintains a continuously updated, structured wiki for code repositories. It scans the full code base and regenerates documentation after each change, so docs evolve with the code. Every wiki section and chat answer is hyperlinked directly to the relevant code files and definitions. The platform will allow new contributors to start immediately, while senior developers can now understand new...", "source": "tldr", "AI": {"tldr": "Code Wiki\u662f\u4e00\u4e2a\u6301\u7eed\u66f4\u65b0\u7684\u4ee3\u7801\u4ed3\u5e93\u7ed3\u6784\u5316wiki\u5e73\u53f0\uff0c\u80fd\u591f\u81ea\u52a8\u626b\u63cf\u4ee3\u7801\u5e93\u5e76\u5728\u6bcf\u6b21\u53d8\u66f4\u540e\u91cd\u65b0\u751f\u6210\u6587\u6863\uff0c\u4f7f\u6587\u6863\u4e0e\u4ee3\u7801\u540c\u6b65\u53d1\u5c55\u3002", "motivation": "\u89e3\u51b3\u4ee3\u7801\u6587\u6863\u4e0e\u4ee3\u7801\u672c\u8eab\u4e0d\u540c\u6b65\u7684\u95ee\u9898\uff0c\u5e2e\u52a9\u65b0\u8d21\u732e\u8005\u5feb\u901f\u4e0a\u624b\uff0c\u540c\u65f6\u8ba9\u8d44\u6df1\u5f00\u53d1\u8005\u80fd\u591f\u7406\u89e3\u65b0\u4ee3\u7801\u3002", "method": "\u5e73\u53f0\u626b\u63cf\u5b8c\u6574\u4ee3\u7801\u5e93\uff0c\u5728\u6bcf\u6b21\u4ee3\u7801\u53d8\u66f4\u540e\u81ea\u52a8\u91cd\u65b0\u751f\u6210\u6587\u6863\uff0c\u6bcf\u4e2awiki\u90e8\u5206\u548c\u804a\u5929\u56de\u7b54\u90fd\u76f4\u63a5\u8d85\u94fe\u63a5\u5230\u76f8\u5173\u7684\u4ee3\u7801\u6587\u4ef6\u548c\u5b9a\u4e49\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u6301\u7eed\u66f4\u65b0\u7684\u7ed3\u6784\u5316\u4ee3\u7801wiki\uff0c\u6587\u6863\u4e0e\u4ee3\u7801\u4fdd\u6301\u540c\u6b65\uff0c\u63d0\u4f9b\u4e86\u76f4\u63a5\u7684\u4ee3\u7801\u94fe\u63a5\u3002", "conclusion": "Code Wiki\u80fd\u591f\u52a0\u901f\u4ee3\u7801\u7406\u89e3\u8fc7\u7a0b\uff0c\u4f7f\u65b0\u8d21\u732e\u8005\u80fd\u591f\u7acb\u5373\u5f00\u59cb\u5de5\u4f5c\uff0c\u540c\u65f6\u5e2e\u52a9\u8d44\u6df1\u5f00\u53d1\u8005\u7406\u89e3\u65b0\u4ee3\u7801\u3002", "topic": "swe application"}}
{"id": "tldr.2511.6dc71307", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeepmind.google%2Fblog%2Fsima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds%2F%3Futm_source=tldrai/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/A1IfxZDpPMGrT63O-dIqtfMCD3YrpdWmy8dGDGoDvN4=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeepmind.google%2Fblog%2Fsima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds%2F%3Futm_source=tldrai/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/A1IfxZDpPMGrT63O-dIqtfMCD3YrpdWmy8dGDGoDvN4=431", "authors": ["TLDR Newsletter"], "title": "DeepMind released SIMA 2", "comment": "Source: TLDR Newsletter, Date: 2025-11-14, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeepmind.google%2Fblog%2Fsima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds%2F%3Futm_source=tldrai/1/0100019a82b94288-0a61bcb2-78bf-4815-814e-3e45a60d85c6-000000/A1IfxZDpPMGrT63O-dIqtfMCD3YrpdWmy8dGDGoDvN4=431", "summary": "DeepMind released SIMA 2 (9 minute read) SIMA 2 is a generalist AI agent that uses Gemini LLM capabilities to reason, adapt, and interact with virtual environments.", "source": "tldr", "AI": {"tldr": "DeepMind\u53d1\u5e03\u4e86SIMA 2\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528AI\u667a\u80fd\u4f53\uff0c\u5229\u7528Gemini LLM\u80fd\u529b\u5728\u865a\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u63a8\u7406\u3001\u9002\u5e94\u548c\u4ea4\u4e92\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u590d\u6742\u865a\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u901a\u7528\u63a8\u7406\u548c\u4ea4\u4e92\u7684AI\u667a\u80fd\u4f53\uff0c\u63d0\u5347AI\u5728\u6e38\u620f\u548c\u6a21\u62df\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528Gemini\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u8ba9\u667a\u80fd\u4f53\u80fd\u591f\u7406\u89e3\u73af\u5883\u3001\u8fdb\u884c\u63a8\u7406\u5e76\u91c7\u53d6\u76f8\u5e94\u884c\u52a8\u3002", "result": "SIMA 2\u5c55\u793a\u4e86\u5728\u865a\u62df\u73af\u5883\u4e2d\u7684\u901a\u7528\u4ea4\u4e92\u80fd\u529b\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u573a\u666f\u5e76\u6267\u884c\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "SIMA 2\u4ee3\u8868\u4e86AI\u667a\u80fd\u4f53\u53d1\u5c55\u7684\u4e00\u4e2a\u91cd\u8981\u91cc\u7a0b\u7891\uff0c\u5c55\u793a\u4e86LLM\u5728\u865a\u62df\u73af\u5883\u4ea4\u4e92\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.3f6ce1da", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpages.dataiku.com%2Fmodernize-governance-agentic-ai%3Futm_campaign=GLO%252520CONTENT%252520Agents%2525202025%26utm_source=glo-tldr%26utm_medium=paid-email%26utm_content=data-primary/2/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/ueIZ6ieFwdaJ5pt616rMAOgPuMWvzcm92I6RSqIWxC4=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpages.dataiku.com%2Fmodernize-governance-agentic-ai%3Futm_campaign=GLO%252520CONTENT%252520Agents%2525202025%26utm_source=glo-tldr%26utm_medium=paid-email%26utm_content=data-primary/2/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/ueIZ6ieFwdaJ5pt616rMAOgPuMWvzcm92I6RSqIWxC4=431", "authors": ["TLDR Newsletter"], "title": "Build Guardrails to Scale AI Agents", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpages.dataiku.com%2Fmodernize-governance-agentic-ai%3Futm_campaign=GLO%252520CONTENT%252520Agents%2525202025%26utm_source=glo-tldr%26utm_medium=paid-email%26utm_content=data-primary/2/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/ueIZ6ieFwdaJ5pt616rMAOgPuMWvzcm92I6RSqIWxC4=431", "summary": "Build Guardrails to Scale AI Agents (Sponsor) Learn how to reduce regulatory risk, control agent behavior, and avoid costly missteps as your teams adopt GenAI. This BARC report, sponsored by Dataiku, shows how organizations can extend governance from data to models to agents. It covers the operational, regulatory, and reputational risks of agentic AI and shows how to close the gaps before they become board-level problems. Read now to benchmark your AI governance readiness.", "source": "tldr", "AI": {"tldr": "\u8be5\u62a5\u544a\u4ecb\u7ecd\u4e86\u5982\u4f55\u901a\u8fc7\u5efa\u7acb\u9632\u62a4\u63aa\u65bd\u6765\u6269\u5c55AI\u4ee3\u7406\uff0c\u51cf\u5c11\u76d1\u7ba1\u98ce\u9669\u3001\u63a7\u5236\u4ee3\u7406\u884c\u4e3a\u5e76\u907f\u514d\u4ee3\u4ef7\u9ad8\u6602\u7684\u9519\u8bef\u3002", "motivation": "\u968f\u7740\u7ec4\u7ec7\u91c7\u7528\u751f\u6210\u5f0fAI\uff0c\u9700\u8981\u89e3\u51b3AI\u4ee3\u7406\u5e26\u6765\u7684\u8fd0\u8425\u3001\u76d1\u7ba1\u548c\u58f0\u8a89\u98ce\u9669\uff0c\u5c06\u6cbb\u7406\u4ece\u6570\u636e\u548c\u6a21\u578b\u6269\u5c55\u5230\u4ee3\u7406\u5c42\u9762\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u9632\u62a4\u63aa\u65bd\u548c\u6cbb\u7406\u6846\u67b6\uff0c\u4ece\u6570\u636e\u5230\u6a21\u578b\u518d\u5230\u4ee3\u7406\u7684\u5168\u9762\u6cbb\u7406\uff0c\u8bc6\u522b\u5e76\u586b\u8865\u6cbb\u7406\u7a7a\u767d\u3002", "result": "\u63d0\u4f9b\u4e86\u7ec4\u7ec7AI\u6cbb\u7406\u51c6\u5907\u5ea6\u7684\u57fa\u51c6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e2e\u52a9\u5728\u95ee\u9898\u5347\u7ea7\u5230\u8463\u4e8b\u4f1a\u5c42\u9762\u4e4b\u524d\u89e3\u51b3\u98ce\u9669\u3002", "conclusion": "\u5efa\u7acb\u9002\u5f53\u7684\u9632\u62a4\u63aa\u65bd\u548c\u6cbb\u7406\u6846\u67b6\u5bf9\u4e8e\u89c4\u6a21\u5316\u91c7\u7528AI\u4ee3\u7406\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u6709\u6548\u7ba1\u7406\u76f8\u5173\u98ce\u9669\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.cb4608d4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpages.dataiku.com%2Fmodernize-governance-agentic-ai%3Futm_campaign=GLO%252520CONTENT%252520Agents%2525202025%26utm_source=glo-tldr%26utm_medium=paid-email%26utm_content=data-primary/2/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/ueIZ6ieFwdaJ5pt616rMAOgPuMWvzcm92I6RSqIWxC4=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpages.dataiku.com%2Fmodernize-governance-agentic-ai%3Futm_campaign=GLO%252520CONTENT%252520Agents%2525202025%26utm_source=glo-tldr%26utm_medium=paid-email%26utm_content=data-primary/2/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/ueIZ6ieFwdaJ5pt616rMAOgPuMWvzcm92I6RSqIWxC4=431", "authors": ["TLDR Newsletter"], "title": "reduce regulatory risk", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpages.dataiku.com%2Fmodernize-governance-agentic-ai%3Futm_campaign=GLO%252520CONTENT%252520Agents%2525202025%26utm_source=glo-tldr%26utm_medium=paid-email%26utm_content=data-primary/2/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/ueIZ6ieFwdaJ5pt616rMAOgPuMWvzcm92I6RSqIWxC4=431", "summary": "Build Guardrails to Scale AI Agents (Sponsor) Learn how to reduce regulatory risk, control agent behavior, and avoid costly missteps as your teams adopt GenAI. This BARC report, sponsored by Dataiku, shows how organizations can extend governance from data to models to agents. It covers the operational, regulatory, and reputational risks of agentic AI and shows how to close the gaps before they become board-level problems. Read now to benchmark your AI governance readiness.", "source": "tldr", "AI": {"tldr": "\u8be5\u62a5\u544a\u63a2\u8ba8\u5982\u4f55\u4e3aAI\u4ee3\u7406\u5efa\u7acb\u9632\u62a4\u63aa\u65bd\uff0c\u901a\u8fc7\u4ece\u6570\u636e\u5230\u6a21\u578b\u518d\u5230\u4ee3\u7406\u7684\u6cbb\u7406\u6269\u5c55\uff0c\u964d\u4f4e\u76d1\u7ba1\u98ce\u9669\u3001\u63a7\u5236\u4ee3\u7406\u884c\u4e3a\u5e76\u907f\u514d\u4ee3\u4ef7\u9ad8\u6602\u7684\u5931\u8bef\u3002", "motivation": "\u968f\u7740\u56e2\u961f\u91c7\u7528\u751f\u6210\u5f0fAI\uff0c\u7ec4\u7ec7\u9762\u4e34\u8fd0\u8425\u3001\u76d1\u7ba1\u548c\u58f0\u8a89\u98ce\u9669\uff0c\u9700\u8981\u5efa\u7acb\u6709\u6548\u7684\u6cbb\u7406\u673a\u5236\u6765\u9884\u9632\u8fd9\u4e9b\u95ee\u9898\u5347\u7ea7\u4e3a\u8463\u4e8b\u4f1a\u5c42\u9762\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u6cbb\u7406\u6846\u67b6\uff0c\u4ece\u6570\u636e\u6cbb\u7406\u5ef6\u4f38\u5230\u6a21\u578b\u6cbb\u7406\u518d\u5230\u4ee3\u7406\u6cbb\u7406\uff0c\u5efa\u7acb\u5168\u9762\u7684AI\u6cbb\u7406\u4f53\u7cfb\u3002", "result": "\u63d0\u4f9b\u4e86\u8bc4\u4f30AI\u6cbb\u7406\u51c6\u5907\u5ea6\u7684\u57fa\u51c6\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u8bc6\u522b\u548c\u5f25\u8865\u6cbb\u7406\u5dee\u8ddd\u3002", "conclusion": "\u5efa\u7acb\u5168\u9762\u7684\u9632\u62a4\u63aa\u65bd\u5bf9\u4e8e\u89c4\u6a21\u5316\u90e8\u7f72AI\u4ee3\u7406\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u6709\u6548\u7ba1\u7406\u98ce\u9669\u5e76\u786e\u4fdd\u5408\u89c4\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.245f0632", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpages.dataiku.com%2Fmodernize-governance-agentic-ai%3Futm_campaign=GLO%252520CONTENT%252520Agents%2525202025%26utm_source=glo-tldr%26utm_medium=paid-email%26utm_content=data-primary/2/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/ueIZ6ieFwdaJ5pt616rMAOgPuMWvzcm92I6RSqIWxC4=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpages.dataiku.com%2Fmodernize-governance-agentic-ai%3Futm_campaign=GLO%252520CONTENT%252520Agents%2525202025%26utm_source=glo-tldr%26utm_medium=paid-email%26utm_content=data-primary/2/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/ueIZ6ieFwdaJ5pt616rMAOgPuMWvzcm92I6RSqIWxC4=431", "authors": ["TLDR Newsletter"], "title": "BARC report", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpages.dataiku.com%2Fmodernize-governance-agentic-ai%3Futm_campaign=GLO%252520CONTENT%252520Agents%2525202025%26utm_source=glo-tldr%26utm_medium=paid-email%26utm_content=data-primary/2/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/ueIZ6ieFwdaJ5pt616rMAOgPuMWvzcm92I6RSqIWxC4=431", "summary": "Build Guardrails to Scale AI Agents (Sponsor) Learn how to reduce regulatory risk, control agent behavior, and avoid costly missteps as your teams adopt GenAI. This BARC report, sponsored by Dataiku, shows how organizations can extend governance from data to models to agents. It covers the operational, regulatory, and reputational risks of agentic AI and shows how to close the gaps before they become board-level problems. Read now to benchmark your AI governance readiness.", "source": "tldr", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u5982\u4f55\u901a\u8fc7\u5efa\u7acb\u9632\u62a4\u63aa\u65bd\u6765\u89c4\u6a21\u5316AI\u4ee3\u7406\uff0c\u51cf\u5c11\u76d1\u7ba1\u98ce\u9669\u3001\u63a7\u5236\u4ee3\u7406\u884c\u4e3a\uff0c\u907f\u514d\u4ee3\u4ef7\u9ad8\u6602\u7684\u9519\u8bef\u3002", "motivation": "\u968f\u7740\u56e2\u961f\u91c7\u7528\u751f\u6210\u5f0fAI\uff0c\u9700\u8981\u89e3\u51b3AI\u4ee3\u7406\u5e26\u6765\u7684\u8fd0\u8425\u3001\u76d1\u7ba1\u548c\u58f0\u8a89\u98ce\u9669\uff0c\u9632\u6b62\u8fd9\u4e9b\u95ee\u9898\u5347\u7ea7\u4e3a\u8463\u4e8b\u4f1a\u5c42\u9762\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u6cbb\u7406\u4ece\u6570\u636e\u548c\u6a21\u578b\u6269\u5c55\u5230AI\u4ee3\u7406\uff0c\u5efa\u7acb\u5168\u9762\u7684AI\u6cbb\u7406\u6846\u67b6\u6765\u7ba1\u7406\u98ce\u9669\u3002", "result": "\u63d0\u4f9b\u4e86\u7ec4\u7ec7\u53ef\u4ee5\u8bc4\u4f30\u5176AI\u6cbb\u7406\u51c6\u5907\u5ea6\u7684\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u7ec4\u7ec7\u9700\u8981\u5efa\u7acb\u5b8c\u5584\u7684\u9632\u62a4\u63aa\u65bd\u548c\u6cbb\u7406\u6846\u67b6\u6765\u5b89\u5168\u5730\u89c4\u6a21\u5316AI\u4ee3\u7406\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.d41f87f9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftowardsdatascience.com%2Forganizing-code-experiments-and-research-for-kaggle-competitions%2F%3Futm_source=tldrdata/1/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/B1OC1q0sF-uNzysANg9LEPE7Ej1c7A9JEW8u1vVekC0=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftowardsdatascience.com%2Forganizing-code-experiments-and-research-for-kaggle-competitions%2F%3Futm_source=tldrdata/1/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/B1OC1q0sF-uNzysANg9LEPE7Ej1c7A9JEW8u1vVekC0=431", "authors": ["TLDR Newsletter"], "title": "Organizing Code, Experiments, and Research for Kaggle Competitions", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Reading time: 14 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftowardsdatascience.com%2Forganizing-code-experiments-and-research-for-kaggle-competitions%2F%3Futm_source=tldrdata/1/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/B1OC1q0sF-uNzysANg9LEPE7Ej1c7A9JEW8u1vVekC0=431", "summary": "Organizing Code, Experiments, and Research for Kaggle Competitions (14 minute read) Robust code organization, meticulous experiment tracking, and reproducible environments are non-negotiable for successful data science projects. Using modular repo structures, version control, and tools like wandb and Hydra streamlines experimentation and collaboration across platforms.", "source": "tldr", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u5728Kaggle\u7ade\u8d5b\u4e2d\u7ec4\u7ec7\u4ee3\u7801\u3001\u5b9e\u9a8c\u548c\u7814\u7a76\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5f3a\u8c03\u6a21\u5757\u5316\u4ed3\u5e93\u7ed3\u6784\u3001\u7248\u672c\u63a7\u5236\u548cwandb\u3001Hydra\u7b49\u5de5\u5177\u7684\u91cd\u8981\u6027", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u79d1\u5b66\u9879\u76ee\u4e2d\u4ee3\u7801\u7ec4\u7ec7\u6df7\u4e71\u3001\u5b9e\u9a8c\u96be\u4ee5\u8ffd\u8e2a\u548c\u590d\u73b0\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728Kaggle\u7ade\u8d5b\u8fd9\u79cd\u9700\u8981\u9ad8\u6548\u534f\u4f5c\u7684\u573a\u5408", "method": "\u91c7\u7528\u6a21\u5757\u5316\u4ed3\u5e93\u7ed3\u6784\u3001\u7248\u672c\u63a7\u5236\u7cfb\u7edf\uff0c\u4ee5\u53cawandb\u548cHydra\u7b49\u5de5\u5177\u6765\u7ba1\u7406\u5b9e\u9a8c\u548c\u914d\u7f6e", "result": "\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u5b9e\u9a8c\u6d41\u7a0b\u3001\u66f4\u597d\u7684\u534f\u4f5c\u80fd\u529b\u548c\u53ef\u590d\u73b0\u7684\u7814\u7a76\u73af\u5883", "conclusion": "\u826f\u597d\u7684\u4ee3\u7801\u7ec4\u7ec7\u3001\u5b9e\u9a8c\u8ddf\u8e2a\u548c\u53ef\u590d\u73b0\u73af\u5883\u662f\u6570\u636e\u79d1\u5b66\u9879\u76ee\u6210\u529f\u7684\u5173\u952e\u8981\u7d20", "topic": "swe application"}}
{"id": "wechat.2511.5d8cd6d7", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk3NTA3NzA1Nw==&mid=2247483894&idx=1&sn=be19f82c98ddd5dd72854d969b259586&chksm=c50d5477b832abef75ace6e6df739568fe16f1db2dd6356ae30279fe1cf77ec5bfc30d0cc62b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk3NTA3NzA1Nw==&mid=2247483894&idx=1&sn=be19f82c98ddd5dd72854d969b259586&chksm=c50d5477b832abef75ace6e6df739568fe16f1db2dd6356ae30279fe1cf77ec5bfc30d0cc62b#rd", "authors": ["\u61d2\u60f0\u5c0f\u871c\u8702\u7684AI\u871c\u7f50"], "title": "Mem-\u03b1\uff1a\u901a\u8fc7<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6765\u5b66\u4e60\u8bb0\u5fc6\u6784\u5efa", "comment": "Source: WeChat, Published: 2025-11-17 12:37:20", "summary": "1\u3001\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff081\uff09\u4efb\u52a1\u5b9a\u4e49\uff1aMem-\u03b1\u5c06\u8bb0\u5fc6\u6784\u5efa\u5efa\u6a21\u4e3a\u4e86\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u8f93\u5165\uff1a\u4e00\u4e2a\u591a\u8f6e\u4ea4\u4e92\u5e8f\u5217 C={c_1\uff0cc_2\uff0c\u2026\uff0cc_n}\uff0c\u53ef\u89c6\u4e3a\u8fde\u7eed\u7684\u5bf9\u8bdd\u3001\u6587\u6863\u7247\u6bb5\u6216\u4fe1\u606f\u5757\uff08chunk\uff09\u3002", "AI": {"tldr": "1\u3001\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff081\uff09\u4efb\u52a1\u5b9a\u4e49\uff1aMem-\u03b1\u5c06\u8bb0\u5fc6\u6784\u5efa\u5efa\u6a21\u4e3a\u4e86\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u8f93\u5165\uff1a\u4e00\u4e2a\u591a\u8f6e\u4ea4\u4e92\u5e8f\u5217 C={c_1\uff0cc_2\uff0c\u2026\uff0cc_n}\uff0c\u53ef\u89c6\u4e3a\u8fde\u7eed\u7684\u5bf9\u8bdd\u3001\u6587\u6863\u7247\u6bb5\u6216\u4fe1\u606f\u5757\uff08chunk\uff09\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.ec9d241e", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4NzY0NDk5NQ==&mid=2247634939&idx=2&sn=929ab72899fc50e14f8667c30a55a41b&chksm=eae121d22b3029b0b609dbda376023b29e78cd8a9f63a0fc109e83e7dfdbd9b00b00fcbd10c8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4NzY0NDk5NQ==&mid=2247634939&idx=2&sn=929ab72899fc50e14f8667c30a55a41b&chksm=eae121d22b3029b0b609dbda376023b29e78cd8a9f63a0fc109e83e7dfdbd9b00b00fcbd10c8#rd", "authors": ["\u5370\u8c61\u65b0\u5b89\u7164\u77ff"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\"\u56db\u5230\u4f4d\"  \u7b51\u7262\u5b89\u5168\u751f\u4ea7\u9632\u7ebf", "comment": "Source: WeChat, Published: 2025-11-17 09:51:57", "summary": "\u8fdb\u5165\u56db\u5b63\u5ea6\u4ee5\u6765\uff0c\u65b0\u5b89\u7164\u77ff\u7d27\u7d27\u56f4\u7ed5\u5b89\u5168\u751f\u4ea7\u5b9e\u9645\uff0c\u5168\u9762\u542f\u52a8\u7164\u77ff\u4e13\u4e1a\u77e5\u8bc6\u5f3a\u5316\u5b66\u4e60\u6d3b\u52a8\uff0c\u901a\u8fc7\u201c\u7ec4\u7ec7\u5230\u4f4d\u3001\u5ba3\u4f20\u5230\u4f4d\u3001\u57f9\u8bad\u5230\u4f4d\u3001\u8003\u6838\u5230\u4f4d\u201d\u5de5\u4f5c\u4e3e\u63aa\uff0c\u6301\u7eed\u63d0\u5347\u4ece\u4e1a\u4eba\u5458\u5b89\u5168\u7d20\u517b\u4e0e\u4e13\u4e1a\u6280\u80fd\uff0c\u4e3a\u77ff\u4e95\u5b9e\u73b0\u5168\u5e74\u5b89\u5168\u751f\u4ea7\u63d0\u4f9b\u575a", "AI": {"tldr": "\u8fdb\u5165\u56db\u5b63\u5ea6\u4ee5\u6765\uff0c\u65b0\u5b89\u7164\u77ff\u7d27\u7d27\u56f4\u7ed5\u5b89\u5168\u751f\u4ea7\u5b9e\u9645\uff0c\u5168\u9762\u542f\u52a8\u7164\u77ff\u4e13\u4e1a\u77e5\u8bc6\u5f3a\u5316\u5b66\u4e60\u6d3b\u52a8\uff0c\u901a\u8fc7\u201c\u7ec4\u7ec7\u5230\u4f4d\u3001\u5ba3\u4f20\u5230\u4f4d\u3001\u57f9\u8bad\u5230\u4f4d\u3001\u8003\u6838\u5230\u4f4d\u201d\u5de5\u4f5c\u4e3e\u63aa\uff0c\u6301\u7eed\u63d0\u5347\u4ece\u4e1a\u4eba\u5458\u5b89\u5168\u7d20\u517b\u4e0e\u4e13\u4e1a\u6280\u80fd\uff0c\u4e3a\u77ff\u4e95\u5b9e\u73b0\u5168\u5e74\u5b89\u5168\u751f\u4ea7\u63d0\u4f9b\u575a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.eb25a480", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNzkyOTczMw==&mid=2247484304&idx=1&sn=c9f7d154ddd76c5e2deabd595f5ad00b&chksm=c34dd0b5db37e4f63d532948ce1e8eb638bd1f497cfbeb78f192be54c382c9d01a1e959eec4d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNzkyOTczMw==&mid=2247484304&idx=1&sn=c9f7d154ddd76c5e2deabd595f5ad00b&chksm=c34dd0b5db37e4f63d532948ce1e8eb638bd1f497cfbeb78f192be54c382c9d01a1e959eec4d#rd", "authors": ["AI\u6df1\u5b66\u89c6\u754c"], "title": "DeepEyes\uff1a\u901a\u8fc7<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6fc0\u53d1\u201c\u7528\u56fe\u50cf\u601d\u8003\u201d", "comment": "Source: WeChat, Published: 2025-11-17 09:10:55", "summary": "\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6fc0\u52b1\u201c\u7528\u56fe\u50cf\u601d\u8003\u201d\u80fd\u529b\u7684\u6a21\u578b\uff0c\u800c\u4e0d\u9700\u8981\u51b7\u542f\u52a8SFT\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u79cd\u80fd\u529b\u662f\u5728\u6a21\u578b\u672c\u8eab\u4e2d\u56fa\u6709\u7684\uff0c\u5229\u7528\u5176\u56fa\u6709\u7684\u57fa\u7840\u80fd\u529b\u4f5c\u4e3a\u4e00\u79cd\u5de5\u5177\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u5355\u72ec\u7684\u4e13\u7528\u6a21\u578b\u3002", "AI": {"tldr": "\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6fc0\u52b1\u201c\u7528\u56fe\u50cf\u601d\u8003\u201d\u80fd\u529b\u7684\u6a21\u578b\uff0c\u800c\u4e0d\u9700\u8981\u51b7\u542f\u52a8SFT\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u79cd\u80fd\u529b\u662f\u5728\u6a21\u578b\u672c\u8eab\u4e2d\u56fa\u6709\u7684\uff0c\u5229\u7528\u5176\u56fa\u6709\u7684\u57fa\u7840\u80fd\u529b\u4f5c\u4e3a\u4e00\u79cd\u5de5\u5177\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u5355\u72ec\u7684\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.811b4750", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzMTIyODU1MA==&mid=2247483768&idx=1&sn=dcede30bc050ce26c59096af65753248&chksm=f1325dc11d9f4ced0f6ee0f707d35a30cd5c29f5df550bbb6a2ef55476abe5ad0b170541b77c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzMTIyODU1MA==&mid=2247483768&idx=1&sn=dcede30bc050ce26c59096af65753248&chksm=f1325dc11d9f4ced0f6ee0f707d35a30cd5c29f5df550bbb6a2ef55476abe5ad0b170541b77c#rd", "authors": ["\u5446\u5854\u79d1\u7814\u7ade\u8d5b\u52a9\u624b"], "title": "\u591a\u667a\u80fd\u4f53\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7efc\u8ff0\uff1a\u7b97\u6cd5\u6846\u67b6\u3001\u6838\u5fc3\u6311\u6218\u4e0e\u524d\u6cbf\u8fdb\u5c55", "comment": "Source: WeChat, Published: 2025-11-17 02:49:20", "summary": "\u672c\u6587\u9002\u5408\u5bf9\u5f3a\u5316\u5b66\u4e60\u57fa\u7840\u6709\u4e00\u5b9a\u4e86\u89e3\uff0c\u5e0c\u671b\u6df1\u5165\u591a\u667a\u80fd\u4f53\u65b9\u5411\u7684\u7814\u7a76\u751f\u8bfb\u8005\u3002\u6211\u4eec\u5c06\u91cd\u70b9\u5206\u6790\u8bad\u7ec3\u67b6\u6784\u8bbe\u8ba1\u3001\u884c\u4e3a\u6d8c\u73b0\u673a\u5236\u4ee5\u53ca\u5e94\u5bf9\u975e\u5e73\u7a33\u6027\u3001\u4fe1\u7528\u5206\u914d\u7b49\u7279\u6709\u6311\u6218\u7684\u7b97\u6cd5\u7b56\u7565\u3002", "AI": {"tldr": "\u672c\u6587\u9002\u5408\u5bf9\u5f3a\u5316\u5b66\u4e60\u57fa\u7840\u6709\u4e00\u5b9a\u4e86\u89e3\uff0c\u5e0c\u671b\u6df1\u5165\u591a\u667a\u80fd\u4f53\u65b9\u5411\u7684\u7814\u7a76\u751f\u8bfb\u8005\u3002\u6211\u4eec\u5c06\u91cd\u70b9\u5206\u6790\u8bad\u7ec3\u67b6\u6784\u8bbe\u8ba1\u3001\u884c\u4e3a\u6d8c\u73b0\u673a\u5236\u4ee5\u53ca\u5e94\u5bf9\u975e\u5e73\u7a33\u6027\u3001\u4fe1\u7528\u5206\u914d\u7b49\u7279\u6709\u6311\u6218\u7684\u7b97\u6cd5\u7b56\u7565\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.fc859655", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0NzY3ODMxMA==&mid=2247488450&idx=1&sn=59385fe314573e532a209315ded66b58&chksm=c24241e0c4941c07f6595a16ef739beb22eff68ab933e83a935315ef8b991d22b65f9ff5d8ed#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0NzY3ODMxMA==&mid=2247488450&idx=1&sn=59385fe314573e532a209315ded66b58&chksm=c24241e0c4941c07f6595a16ef739beb22eff68ab933e83a935315ef8b991d22b65f9ff5d8ed#rd", "authors": ["AI\u65b0\u6587"], "title": "NeurIPS 2025 | <em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>-\u76f8\u5173\u8bba\u658712\u7bc7", "comment": "Source: WeChat, Published: 2025-11-16 23:01:21", "summary": "\u539f\u6587\u94fe\u63a5\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7684\u5728\u7ebf\u4f18\u5316\u539f\u6807\u9898\uff1aOnline Optimization for Offline Safe Reinforcement Learning\u4f5c\u8005\uff1aYassine Chemingui\uff1bAryan Deshwal\uff1bAlan Fern\uff1bThanh Nguyen-Tang\uff1b", "AI": {"tldr": "\u539f\u6587\u94fe\u63a5\u79bb\u7ebf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7684\u5728\u7ebf\u4f18\u5316\u539f\u6807\u9898\uff1aOnline Optimization for Offline Safe Reinforcement Learning\u4f5c\u8005\uff1aYassine Chemingui\uff1bAryan Deshwal\uff1bAlan Fern\uff1bThanh Nguyen-Tang\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.832b69ca", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247488148&idx=1&sn=82934d68fbd3a7bcc0968d67768fec32&chksm=c03749801b93b02f1a91b3227f3a4670ce372295973ae6c88b0b8efd9376e6419afcd3edd523#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNTY0Mjg0OQ==&mid=2247488148&idx=1&sn=82934d68fbd3a7bcc0968d67768fec32&chksm=c03749801b93b02f1a91b3227f3a4670ce372295973ae6c88b0b8efd9376e6419afcd3edd523#rd", "authors": ["AIGC\u5c0f\u767d\u5165\u95e8\u8bb0"], "title": "\u9996\u7bc7\u57fa\u4e8e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684Agentic Search\u6700\u65b0\u7efc\u8ff0\uff01", "comment": "Source: WeChat, Published: 2025-11-16 15:03:22", "summary": "\u800c\u8fd9\u80cc\u540e\u7684\u6838\u5fc3\u9a71\u52a8\u529b\uff0c\u6b63\u662f\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\u3002ret rieval control trainable freeze tool & knowledge integration search search query final answer search llm engine llm engine pollicy policy think re asoning single agent code execute r llm policy query optimiza", "AI": {"tldr": "\u800c\u8fd9\u80cc\u540e\u7684\u6838\u5fc3\u9a71\u52a8\u529b\uff0c\u6b63\u662f\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\u3002ret rieval control trainable freeze tool & knowledge integration search search query final answer search llm engine llm engine pollicy policy think re aso...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.7cce7db0", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMDA2Nzg5NA==&mid=2452403125&idx=1&sn=bdc5fee62e5e61ae407a5a8c20fcf7a6&chksm=8d8d9c0b50e799fe7524ee1b14ccad41ec6ebc2ec3eab2f5700a9d264bafe5acb5fcbe4427d4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMDA2Nzg5NA==&mid=2452403125&idx=1&sn=bdc5fee62e5e61ae407a5a8c20fcf7a6&chksm=8d8d9c0b50e799fe7524ee1b14ccad41ec6ebc2ec3eab2f5700a9d264bafe5acb5fcbe4427d4#rd", "authors": ["\u6652\u79d1\u7f51"], "title": "\u70ed\u70b9\u5934\u6761|\u767e\u5ea6\u4e16\u754c\u5927\u4f1a\u52fe\u52d2\u6211\u56fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5e94\u7528\u89c4\u6a21\u5316\u843d\u5730\u65b0\u56fe\u666f", "comment": "Source: WeChat, Published: 2025-11-17 12:00:00", "summary": "11\u670813\u65e5\uff0c\u767e\u5ea6\u4e16\u754c\u5927\u4f1a\u96c6\u4e2d\u5c55\u793a\u4e86\u6211\u56fd\u5927\u6a21\u578b\u5e94\u7528\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u6570\u5b57\u4eba\u3001\u667a\u80fd\u641c\u7d22\u548c\u667a\u80fd\u4f53\u7b49\u9886\u57df\u7684\u89c4\u6a21\u5316\u843d\u5730\u6210\u6548\uff0c\u591a\u9879\u6838\u5fc3\u6307\u6807\u5b9e\u73b0\u5b9e\u8d28\u6027\u7a81\u7834\u3002", "AI": {"tldr": "11\u670813\u65e5\uff0c\u767e\u5ea6\u4e16\u754c\u5927\u4f1a\u96c6\u4e2d\u5c55\u793a\u4e86\u6211\u56fd\u5927\u6a21\u578b\u5e94\u7528\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u6570\u5b57\u4eba\u3001\u667a\u80fd\u641c\u7d22\u548c\u667a\u80fd\u4f53\u7b49\u9886\u57df\u7684\u89c4\u6a21\u5316\u843d\u5730\u6210\u6548\uff0c\u591a\u9879\u6838\u5fc3\u6307\u6807\u5b9e\u73b0\u5b9e\u8d28\u6027\u7a81\u7834\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.1b5d293d", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI5NTQ3NzIxMw==&mid=2247485789&idx=1&sn=90ad50849bc3755550e7662c71848635&chksm=ed0b144bc499fe2a554a4e1e6a355fe7b2f823811e278652c9af7e5bbe693e5751616f762b53#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI5NTQ3NzIxMw==&mid=2247485789&idx=1&sn=90ad50849bc3755550e7662c71848635&chksm=ed0b144bc499fe2a554a4e1e6a355fe7b2f823811e278652c9af7e5bbe693e5751616f762b53#rd", "authors": ["AI\u7b80\u5316\u5b89\u5168"], "title": "\u5229\u7528<em class=\"highlight\">\u5927\u6a21\u578b</em>\u53d1\u8d77\u7f51\u7edc\u5b89\u5168\u653b\u51fb", "comment": "Source: WeChat, Published: 2025-11-17 11:02:46", "summary": "\u4e00\u3001\u5927\u6a21\u578b\u7f51\u7edc\u653b\u51fb\u7684\u5177\u4f53\u8fc7\u7a0b1.1 \u653b\u51fb\u67b6\u6784\u8bbe\u8ba1\u653b\u51fb\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u81ea\u4e3b\u653b\u51fb\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u5c06Claude Code\u548cMCP\u5de5\u5177\u7ed3\u5408\u4f7f\u7528\u3002\u8be5\u6846\u67b6\u5c06Claude\u4f5c\u4e3a\u7f16\u6392\u7cfb\u7edf\uff0c\u80fd\u591f\u5c06\u590d\u6742\u7684\u591a\u9636\u6bb5\u653b\u51fb\u5206\u89e3\u4e3a\u79bb\u6563\u7684\u6280\u672f\u4efb\u52a1\uff0c\u5305\u62ec\u6f0f\u6d1e\u626b\u63cf\u3001\u51ed\u8bc1\u9a8c\u8bc1\u3001", "AI": {"tldr": "\u4e00\u3001\u5927\u6a21\u578b\u7f51\u7edc\u653b\u51fb\u7684\u5177\u4f53\u8fc7\u7a0b1.1 \u653b\u51fb\u67b6\u6784\u8bbe\u8ba1\u653b\u51fb\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u81ea\u4e3b\u653b\u51fb\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u5c06Claude Code\u548cMCP\u5de5\u5177\u7ed3\u5408\u4f7f\u7528\u3002\u8be5\u6846\u67b6\u5c06Claude\u4f5c\u4e3a\u7f16\u6392\u7cfb\u7edf\uff0c\u80fd\u591f\u5c06\u590d\u6742\u7684\u591a\u9636\u6bb5\u653b\u51fb\u5206\u89e3\u4e3a\u79bb\u6563\u7684\u6280\u672f\u4efb\u52a1\uff0c\u5305\u62ec\u6f0f\u6d1e\u626b\u63cf\u3001\u51ed\u8bc1\u9a8c\u8bc1\u3001", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.932767f4", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MDUzOTcxNg==&mid=2247497472&idx=1&sn=66d3785529f07974dd74583541f94216&chksm=c3eb5852a22726a7f077f06cc91b50d1c62b0d64aa821f6b3f526b1b4af56397fe1b8b00ecf1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MDUzOTcxNg==&mid=2247497472&idx=1&sn=66d3785529f07974dd74583541f94216&chksm=c3eb5852a22726a7f077f06cc91b50d1c62b0d64aa821f6b3f526b1b4af56397fe1b8b00ecf1#rd", "authors": ["CAICT\u4eba\u5de5\u667a\u80fd"], "title": "\u4e13\u5bb6\u89c2\u70b9\u4e28<em class=\"highlight\">\u5927\u6a21\u578b</em>\u91cd\u5851\u8f6f\u4ef6\u751f\u6001\uff1a\u673a\u9047\u4e0e\u6311\u6218\u4e0b\u7684\u4ea7\u4e1a\u65b0\u8def\u5f84", "comment": "Source: WeChat, Published: 2025-11-17 10:00:00", "summary": "\u5728\u6a21\u578b\u6280\u672f\u5c42\u9762\uff0c\u9700\u91cd\u70b9\u7a81\u7834\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0e\u4e13\u4e1a\u9886\u57df\u6cdb\u5316\u7b49\u6280\u672f\u74f6\u9888\uff0c\u5f53\u524d\u56fd\u5185\u5927\u6a21\u578b\u5c11\u6570\u652f\u6301256K\u4e0a\u4e0b\u6587\uff0c\u5927\u90e8\u5206\u5904\u4e8e32K\u81f3128K\u4e4b\u95f4\uff0c\u8fd9\u5bf9\u4e8e\u5904\u7406\u590d\u6742\u4ee3\u7801\u5e93\u3001\u5de5\u7a0b\u7ea7\u9879\u76ee\u3001\u7cfb\u7edf\u67b6\u6784\u6587\u6863\u7b49\u573a\u666f\u65f6\u663e\u7136\u4e0d\u591f\u3002", "AI": {"tldr": "\u5728\u6a21\u578b\u6280\u672f\u5c42\u9762\uff0c\u9700\u91cd\u70b9\u7a81\u7834\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0e\u4e13\u4e1a\u9886\u57df\u6cdb\u5316\u7b49\u6280\u672f\u74f6\u9888\uff0c\u5f53\u524d\u56fd\u5185\u5927\u6a21\u578b\u5c11\u6570\u652f\u6301256K\u4e0a\u4e0b\u6587\uff0c\u5927\u90e8\u5206\u5904\u4e8e32K\u81f3128K\u4e4b\u95f4\uff0c\u8fd9\u5bf9\u4e8e\u5904\u7406\u590d\u6742\u4ee3\u7801\u5e93\u3001\u5de5\u7a0b\u7ea7\u9879\u76ee\u3001\u7cfb\u7edf\u67b6\u6784\u6587\u6863\u7b49\u573a\u666f\u65f6\u663e\u7136\u4e0d\u591f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.14f50dbf", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxNjYxOTY3NQ==&mid=2455622412&idx=1&sn=669b5e8d827503a40d57a79916bcee19&chksm=8d6d100595c2709ed2430b9fc6615e76b33703e9078dd094f92985ab967d0d08dfe302758482#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxNjYxOTY3NQ==&mid=2455622412&idx=1&sn=669b5e8d827503a40d57a79916bcee19&chksm=8d6d100595c2709ed2430b9fc6615e76b33703e9078dd094f92985ab967d0d08dfe302758482#rd", "authors": ["\u5927\u6a21\u578b\u8bc4\u6d4b\u53ca\u4f18\u5316NoneLinear"], "title": "\u6bcf\u5468AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u66f4\u65b0\u901f\u901211.10~11.16", "comment": "Source: WeChat, Published: 2025-11-17 04:30:27", "summary": "\u5927\u6a21\u578b/agent\u8bc4\u6d4b\u6280\u672f\u4ea4\u6d41\uff1a\u5173\u6ce8\u516c\u4f17\u53f7\uff0c\u53d1\u9001\u6d88\u606f\"\u8fdb\u7fa4\"", "AI": {"tldr": "\u5927\u6a21\u578b/agent\u8bc4\u6d4b\u6280\u672f\u4ea4\u6d41\uff1a\u5173\u6ce8\u516c\u4f17\u53f7\uff0c\u53d1\u9001\u6d88\u606f\"\u8fdb\u7fa4\"", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.8080a6ab", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3Mzg5MjY3Nw==&mid=2247525222&idx=1&sn=49ac27b51502efcedb2171a7f3ba0b4d&chksm=cf2606fcd179863e35a613c94f55f222e1d3d1c62c3c4b0d4102e8b97b113bbfdcf3b8cc2008#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3Mzg5MjY3Nw==&mid=2247525222&idx=1&sn=49ac27b51502efcedb2171a7f3ba0b4d&chksm=cf2606fcd179863e35a613c94f55f222e1d3d1c62c3c4b0d4102e8b97b113bbfdcf3b8cc2008#rd", "authors": ["AIGC\u5f00\u653e\u793e\u533a"], "title": "AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5f00\u53d1\u6838\u5fc3\u6280\u672f\u6808\uff1a\u4ece\u6846\u67b6\u5230\u90e8\u7f72\u7684\u5168\u666f\u89e3\u6790", "comment": "Source: WeChat, Published: 2025-11-17 01:26:13", "summary": "\u5b83\u4eec\u4e13\u6ce8\u4e8e\u7f16\u6392\u548c\u8c03\u5ea6\u5927\u6a21\u578b\u7684\u80fd\u529b\uff0c\u662f\u5f15\u7206\u5e94\u7528\u5c42\u521b\u65b0\u7684\u50ac\u5316\u5242\u3002\u7406\u89e3\u8fd9\u4e24\u5c42\u6846\u67b6\u7684\u7279\u70b9\u4e0e\u5206\u5de5\uff0c\u662f\u5f00\u53d1\u8005\u6784\u5efa\u73b0\u4ee3AI\u5e94\u7528\u7684\u7b2c\u4e00\u6b65\u30021.1 \u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u6846\u67b6\uff1a\u4e09\u8db3\u9f0e\u7acb\uff0cPyTorch\u738b\u8005\u5730\u4f4d\u7a33\u56fa", "AI": {"tldr": "\u5b83\u4eec\u4e13\u6ce8\u4e8e\u7f16\u6392\u548c\u8c03\u5ea6\u5927\u6a21\u578b\u7684\u80fd\u529b\uff0c\u662f\u5f15\u7206\u5e94\u7528\u5c42\u521b\u65b0\u7684\u50ac\u5316\u5242\u3002\u7406\u89e3\u8fd9\u4e24\u5c42\u6846\u67b6\u7684\u7279\u70b9\u4e0e\u5206\u5de5\uff0c\u662f\u5f00\u53d1\u8005\u6784\u5efa\u73b0\u4ee3AI\u5e94\u7528\u7684\u7b2c\u4e00\u6b65\u30021.1 \u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u6846\u67b6\uff1a\u4e09\u8db3\u9f0e\u7acb\uff0cPyTorch\u738b\u8005\u5730\u4f4d\u7a33\u56fa", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
