<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 11]
- [cs.LG](#cs.LG) [Total: 5]
- [wechat.article](#wechat.article) [Total: 23]
- [tldr.article](#tldr.article) [Total: 13]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 提出了一种基于大五人格特质提取LLM隐藏状态激活，通过低秩子空间发现方法识别特质特定层，实现人格对齐的精确控制框架。


<details>
  <summary>Details</summary>
Motivation: LLM在生成中表现出隐含人格特质，但可靠控制或对齐这些特质以满足特定需求仍是一个开放挑战，需要有效的行为操纵机制。

Method: 从transformer层提取隐藏状态激活，应用低秩子空间发现方法识别特质特定最优层，通过动态层选择和扰动实现人格对齐控制。

Result: 发现人格特质占据低秩共享子空间，这些潜在结构可通过精心扰动转化为有效的操纵机制，不影响流畅性、方差和一般能力。

Conclusion: 该框架有助于弥合心理学理论与实际模型对齐之间的差距，为人格感知LLM提供可行的实现路径。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [2] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: TextualVerifier是一个用于TextGrad的自我验证框架，通过思维链分解、变体生成、多数投票和共识聚合四阶段工作流，解决文本优化中推理有效性问题，在多个基准测试中显著提升推理有效性。


<details>
  <summary>Details</summary>
Motivation: TextGrad缺乏确保文本决策中推理有效性的自我验证机制，这限制了其在复合AI系统中的可靠性。

Method: 采用四阶段工作流：思维链分解、变体生成、多数投票和共识聚合，并与TextGrad在损失函数和优化结果验证阶段非侵入式集成。

Result: 在PRM800K上推理步骤有效性提升29%；与TextGrad集成后在GPQA-Diamond、MMLU-ML和MMLU-CP基准上分别获得8.08、10.71和3.92个百分点的改进，损失函数验证带来2.2个百分点增益。

Conclusion: TextualVerifier是首个基于LLM技术的TextGrad自我验证框架，无需数值梯度即可实现更可靠的推理，为文本优化验证开辟了新方向。

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [3] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 提出了T-FIX基准，用于评估LLM在知识密集型领域中生成解释与专家判断的对齐程度，而不仅仅是解释的合理性或内部一致性。


<details>
  <summary>Details</summary>
Motivation: 在知识密集型领域（如手术、天文学、治疗等）中，用户（通常是领域专家）不仅需要答案，还需要反映专家级推理的有意义解释。当前评估方案主要关注解释的合理性或内部忠实性，未能捕捉解释内容是否真正符合专家直觉。

Method: 与领域专家合作开发了T-FIX基准，涵盖七个知识密集型领域，并设计了新的指标来衡量LLM解释与专家判断的对齐程度。

Result: 开发了T-FIX基准和相应的评估指标，能够更准确地评估LLM生成解释与专家判断的对齐性。

Conclusion: 专家对齐应作为评估解释的重要标准，T-FIX基准为评估LLM在知识密集型领域中的解释质量提供了更全面的框架。

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [4] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 批处理提示不仅降低推理成本，还能作为推理时正则化器，提高大型推理模型的准确性和效率，减少3-5倍的推理令牌使用。


<details>
  <summary>Details</summary>
Motivation: 探索批处理在大型语言模型中的额外益处，特别是作为多步推理的正则化机制。

Method: 在13个不同基准上进行的综合研究，通过行为分析考察批处理对模型推理行为的影响。

Result: 批处理提高了准确性，显著减少推理令牌使用（通常3-5倍），抑制过度思考，减少犹豫语言，促进更果断的回答，并出现集体效应。

Conclusion: 批处理不仅是吞吐量优化，更是强大的推理时正则化器，可实现更高效可靠的LLM推理。

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [5] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 本文探索了三种多代理LLM管道用于Text-to-SQL任务，发现多代理讨论能提升小模型性能，其中LLM推理器-编码器管道效果最佳，DeepSeek-R1-32B和QwQ-32B规划器将Gemma 3 27B IT的准确率从52.4%提升至56.4%。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在从自然语言生成SQL时面临模式规模大和复杂推理的挑战，先前工作主要关注复杂不实用的管道，而忽略了更小、更高效的模型。

Method: 提出了三种多代理LLM管道：(1)多代理讨论管道，代理迭代批评和精炼SQL查询；(2)规划器-编码器管道，思考模型规划器生成逐步SQL生成计划；(3)编码器-聚合器管道，多个编码器独立生成SQL查询，推理代理选择最佳查询。

Result: 在Bird-Bench Mini-Dev集上的实验显示，多代理讨论能提升小模型性能，Qwen2.5-7b-Instruct经过三轮讨论后执行准确率提升10.6%。LLM推理器-编码器管道效果最佳，DeepSeek-R1-32B和QwQ-32B规划器将Gemma 3 27B IT准确率从52.4%提升至56.4%。

Conclusion: 多代理方法能有效提升Text-to-SQL任务的性能，特别是对于较小模型，其中规划器-编码器管道表现最佳。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [6] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 本文提出了一个计算图灵测试框架，用于评估LLM生成文本的人类相似度，并在多个社交媒体平台上系统比较了9个开源LLM的校准策略效果。


<details>
  <summary>Details</summary>
Motivation: 当前社会科学中使用LLM模拟人类行为时，缺乏可靠的验证工具来评估生成文本的真实性，现有方法主要依赖不可靠的人工判断。

Method: 开发了集成聚合指标（BERT检测性和语义相似度）与可解释语言特征（风格标记和主题模式）的计算图灵测试框架，并比较了9个LLM在5种校准策略下的表现。

Result: 即使经过校准，LLM输出仍明显可区分于人类文本，特别是在情感语调方面；指令调优模型表现不如基础模型；模型规模扩大不提升人类相似度；存在人类相似度与语义保真度的权衡。

Conclusion: 研究提供了可扩展的验证和校准框架，同时警示当前LLM在捕捉人类交流方面的局限性。

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [7] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 评估当前大语言模型是否能通过波兰国家上诉庭资格考试的实证研究，测试了模型作为考生和自动评分者的表现，发现模型在知识测试中表现尚可，但在实践写作部分均未通过，且自动评分与官方评审存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估大语言模型在法律专业资格考试中的实际能力，探索其在法律领域的应用潜力，特别是作为考生和自动评分者的可行性。

Method: 构建了混合信息检索和提取管道，测试了GPT-4.1、Claude 4 Sonnet和Bielik-11B-v2.6等模型在闭卷和检索增强生成设置下的表现，包括多项选择知识测试和书面判决写作。

Result: 模型在知识测试中取得满意分数，但在实践写作部分均未达到及格线，LLM作为评分者的评估与官方评审委员会存在显著差异。

Conclusion: 尽管技术进步迅速，当前的大语言模型尚无法在波兰公共采购裁决中替代人类法官或独立考官，存在幻觉、法律条款引用错误、逻辑论证薄弱等问题。

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [8] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 本文研究了LLMs在时间推理方面的能力，使用1940年挪威书籍中的琐事问题，测试LLMs以1940年的知识水平回答问题，并比较了英语和挪威语提示的效果。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在时间推理方面的表现，特别是测试它们能否基于历史知识回答问题，并比较不同语言提示对结果的影响。

Method: 使用1940年挪威书籍中的琐事问题，让LLMs以1940年的视角回答问题，测试了英语和挪威语两种语言提示，并采用LLM-as-judge和人工检查相结合的方式进行评分。

Result: 英语提示的结果始终优于挪威语提示，这是一个意外的发现。同时，使用更大的LLM模型能够改善结果。测试了DeepSeek-R1、Gemma3、Qwen3和Llama3.1等模型系列，以及专门为挪威语设计的最大的LLM。

Conclusion: LLMs在时间推理方面表现出一定的能力，但语言选择对结果有显著影响，英语提示效果更好，模型规模也是影响性能的重要因素。

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [9] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 本文系统评估了6个大型语言模型在BFI-2人格框架下的表现，发现不同模型在人格维度上存在显著差异，神经质和外向性对温度参数敏感，架构特征影响人格稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在人类中心应用中的重要性增加，理解其类似人格的行为对于负责任开发和部署至关重要。

Method: 应用BFI-2人格框架评估6个LLMs在不同采样温度下的人格特质表达，使用层次聚类分析模型特征。

Result: 发现四个主要人格维度存在显著差异，神经质和外向性对温度调整敏感，聚类分析显示架构特征影响人格稳定性。

Conclusion: 研究为LLMs中人格模式的出现提供了新见解，为模型调优、选择和AI系统伦理治理提供了新视角。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [10] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: 提出了RAGalyst框架，用于在专业领域自动化评估RAG系统，通过代理管道生成高质量QA数据集并优化LLM评估指标，在三个不同领域验证了RAG性能的领域依赖性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估框架在专业安全关键领域存在不足，启发式指标无法捕捉领域特异性，而LLM-as-a-Judge方法缺乏与人类判断的有效对齐验证。

Method: 开发RAGalyst代理框架，包含生成合成QA数据集的代理管道和代理过滤步骤，通过提示优化改进Answer Correctness和Answerability两个LLM评估指标。

Result: 在军事行动、网络安全和桥梁工程三个领域评估发现，RAG性能高度依赖上下文，没有单一嵌入模型、LLM或超参数配置是普遍最优的。

Conclusion: RAGalyst系统评估框架能帮助从业者发现领域特定权衡，为构建可靠有效的RAG系统做出明智设计选择。

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [11] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 该论文分析了LLM代码助手在软件开发中的安全风险，发现即使最新开源模型仍存在早期报告的漏洞，并提出了新的安全度量指标Prompt Exposure和Model Exposure来评估漏洞风险。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的代码助手在软件开发中作用日益重要，其生成的bug对网络安全的影响也变得至关重要。需要评估现有安全基准和改进方法对主流编码LLM的实际影响。

Method: 引入新的严重性度量Prompt Exposure(PE)，综合考虑漏洞严重程度、生成概率和诱导漏洞代码生成的提示表述。基于PE定义Model Exposure(ME)评分来指示模型生成漏洞的严重性和普遍性。

Result: 研究表明即使最新的开源模型在现实使用环境中仍然容易受到早期报告的漏洞攻击，表明安全-功能性权衡阻碍了有效的漏洞修补。

Conclusion: 需要新的安全度量标准来鼓励缓解最严重和普遍的漏洞，提出的PE和ME指标有助于更准确地评估LLM代码生成的安全风险。

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification](https://arxiv.org/abs/2511.03828)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出Energy-Guided Diffusion Stratification (StratDiff)方法，通过扩散模型学习离线数据先验知识，使用能量函数改进策略模仿，在在线微调期间生成离线类动作，通过KL散度将训练批次分层为离线类和在线类样本分别处理。


<details>
  <summary>Details</summary>
Motivation: 解决离线到在线强化学习中的分布偏移问题，现有方法很少明确评估或利用离线数据本身的分布结构，存在适应不同样本类型的学习策略研究空白。

Method: 使用扩散模型学习离线数据先验，通过能量函数改进策略模仿，计算生成动作与采样动作的KL散度来分层训练批次，离线类样本使用离线目标更新，在线类样本采用在线学习策略。

Result: 在D4RL基准测试中，与Cal-QL和IQL集成后，StratDiff显著优于现有方法，实现了更强的适应性和更稳定的性能。

Conclusion: StratDiff方法通过显式利用离线数据的分布结构，有效缓解了离线到在线强化学习中的分布偏移问题，提高了学习过程的稳定性和适应性。

Abstract: Transitioning from offline to online reinforcement learning (RL) poses
critical challenges due to distributional shifts between the fixed behavior
policy in the offline dataset and the evolving policy during online learning.
Although this issue is widely recognized, few methods attempt to explicitly
assess or utilize the distributional structure of the offline data itself,
leaving a research gap in adapting learning strategies to different types of
samples. To address this challenge, we propose an innovative method,
Energy-Guided Diffusion Stratification (StratDiff), which facilitates smoother
transitions in offline-to-online RL. StratDiff deploys a diffusion model to
learn prior knowledge from the offline dataset. It then refines this knowledge
through energy-based functions to improve policy imitation and generate
offline-like actions during online fine-tuning. The KL divergence between the
generated action and the corresponding sampled action is computed for each
sample and used to stratify the training batch into offline-like and
online-like subsets. Offline-like samples are updated using offline objectives,
while online-like samples follow online learning strategies. We demonstrate the
effectiveness of StratDiff by integrating it with off-the-shelf methods Cal-QL
and IQL. Extensive empirical evaluations on D4RL benchmarks show that StratDiff
significantly outperforms existing methods, achieving enhanced adaptability and
more stable performance across diverse RL settings.

</details>


### [13] [Enhancing Q-Value Updates in Deep Q-Learning via Successor-State Prediction](https://arxiv.org/abs/2511.03836)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出SADQ方法，通过显式建模环境动态和使用后继状态分布来改进DQN，减少训练方差并提高学习稳定性


<details>
  <summary>Details</summary>
Motivation: 解决DQN中目标更新依赖过去次优策略生成的状态，导致学习信号不具信息性和高方差的问题

Method: 使用随机转移模型显式建模环境动态，将后继状态分布集成到Q值估计过程中，并探索基于转移结构的更高效动作选择策略

Result: 在标准RL基准和实际向量控制任务中，SADQ在稳定性和学习效率上持续优于DQN变体

Conclusion: SADQ通过建模环境动态和后继状态分布，在保持无偏值估计的同时显著减少训练方差

Abstract: Deep Q-Networks (DQNs) estimate future returns by learning from transitions
sampled from a replay buffer. However, the target updates in DQN often rely on
next states generated by actions from past, potentially suboptimal, policy. As
a result, these states may not provide informative learning signals, causing
high variance into the update process. This issue is exacerbated when the
sampled transitions are poorly aligned with the agent's current policy. To
address this limitation, we propose the Successor-state Aggregation Deep
Q-Network (SADQ), which explicitly models environment dynamics using a
stochastic transition model. SADQ integrates successor-state distributions into
the Q-value estimation process, enabling more stable and policy-aligned value
updates. Additionally, it explores a more efficient action selection strategy
with the modeled transition structure. We provide theoretical guarantees that
SADQ maintains unbiased value estimates while reducing training variance. Our
extensive empirical results across standard RL benchmarks and real-world
vector-based control tasks demonstrate that SADQ consistently outperforms DQN
variants in both stability and learning efficiency.

</details>


### [14] [Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning](https://arxiv.org/abs/2511.04147)
*Jiaming Zhang,Yujie Yang,Haoning Wang,Liping Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出交换策略优化(EPO)框架，解决具有无限约束的半无限安全强化学习问题，通过迭代求解有限约束子问题并自适应调整活动约束集，确保策略性能最优且安全约束严格有界。


<details>
  <summary>Details</summary>
Motivation: 传统安全强化学习通常处理有限约束，但在实际应用中经常遇到需要在连续参数空间上执行安全条件的半无限约束问题，如确保每个空间位置的资源分配充足。

Method: EPO通过迭代求解有限约束的安全RL子问题，采用约束扩展和删除机制自适应调整活动集：违反预定义容差的约束被添加以细化策略，零拉格朗日乘子的约束在策略更新后被移除。

Result: 在温和假设下，通过EPO训练的策略实现与最优解相当的性能，且全局约束违反严格保持在预定界限内。

Conclusion: EPO框架有效解决了半无限安全强化学习问题，通过可控的约束集管理实现最优策略性能和确定性有界安全性。

Abstract: Safe reinforcement learning (safe RL) aims to respect safety requirements
while optimizing long-term performance. In many practical applications,
however, the problem involves an infinite number of constraints, known as
semi-infinite safe RL (SI-safe RL). Such constraints typically appear when
safety conditions must be enforced across an entire continuous parameter space,
such as ensuring adequate resource distribution at every spatial location. In
this paper, we propose exchange policy optimization (EPO), an algorithmic
framework that achieves optimal policy performance and deterministic bounded
safety. EPO works by iteratively solving safe RL subproblems with finite
constraint sets and adaptively adjusting the active set through constraint
expansion and deletion. At each iteration, constraints with violations
exceeding the predefined tolerance are added to refine the policy, while those
with zero Lagrange multipliers are removed after the policy update. This
exchange rule prevents uncontrolled growth of the working set and supports
effective policy training. Our theoretical analysis demonstrates that, under
mild assumptions, strategies trained via EPO achieve performance comparable to
optimal solutions with global constraint violations strictly remaining within a
prescribed bound.

</details>


### [15] [Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference](https://arxiv.org/abs/2511.04286)
*Matteo Cercola,Valeria Capretti,Simone Formentin*

Main category: cs.LG

TL;DR: 提出了一种结合RLHF可扩展性和PBO样本效率的混合框架，通过在RLHF流程中集成主动查询模块，实现高效偏好数据收集。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据收集成本高且耗时，需要更高效的学习范式。RLHF在高维任务中扩展性好，而PBO通过主动查询实现更高样本效率。

Method: 在RLHF流程中集成主动查询模块，结合RLHF的可扩展性和PBO的查询效率，实现主动且样本高效的偏好收集。

Result: 在高维偏好优化和LLM微调两个代表性领域进行验证，实验结果显示在样本效率和整体性能上均取得一致改进。

Conclusion: 提出的混合框架成功统一了RLHF的可扩展性和PBO的查询效率，为偏好学习提供了更高效的解决方案。

Abstract: Learning from human preferences is a cornerstone of aligning machine learning
models with subjective human judgments. Yet, collecting such preference data is
often costly and time-consuming, motivating the need for more efficient
learning paradigms. Two established approaches offer complementary advantages:
RLHF scales effectively to high-dimensional tasks such as LLM fine-tuning,
while PBO achieves greater sample efficiency through active querying. We
propose a hybrid framework that unifies RLHF's scalability with PBO's query
efficiency by integrating an acquisition-driven module into the RLHF pipeline,
thereby enabling active and sample-efficient preference gathering. We validate
the proposed approach on two representative domains: (i) high-dimensional
preference optimization and (ii) LLM fine-tuning. Experimental results
demonstrate consistent improvements in both sample efficiency and overall
performance across these tasks.

</details>


### [16] [Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning](https://arxiv.org/abs/2511.04598)
*Hampus Åström,Elin Anna Topp,Jacek Malec*

Main category: cs.LG

TL;DR: 将常规强化学习环境转换为目标条件环境，使智能体能够自主、无奖励地学习解决任务，通过自选目标实现环境无关的学习。


<details>
  <summary>Details</summary>
Motivation: 研究如何让智能体在无需外部奖励指导的情况下自主学习解决任务，实现环境无关的通用智能体训练。

Method: 将常规RL环境转换为目标条件环境，让智能体自主选择目标进行学习，该方法与底层离策略学习算法无关。

Result: 智能体能够以与外部指导RL相当的时间学习解决任务，平均目标成功率得到改善和稳定，但单个目标性能存在不稳定性。

Conclusion: 该方法能够训练出可被指示追求环境中任何观测的通用智能体，为特定用例前的通用训练提供了可能。

Abstract: In this paper we study how transforming regular reinforcement learning
environments into goal-conditioned environments can let agents learn to solve
tasks autonomously and reward-free. We show that an agent can learn to solve
tasks by selecting its own goals in an environment-agnostic way, at training
times comparable to externally guided reinforcement learning. Our method is
independent of the underlying off-policy learning algorithm. Since our method
is environment-agnostic, the agent does not value any goals higher than others,
leading to instability in performance for individual goals. However, in our
experiments, we show that the average goal success rate improves and
stabilizes. An agent trained with this method can be instructed to seek any
observations made in the environment, enabling generic training of agents prior
to specific use cases.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [17] [每周一书《<em class="highlight">强化学习</em>精要：核心算法与TensorFlow实现 pdf》分享](http://mp.weixin.qq.com/s?__biz=MjM5Mzc2NjczMQ==&mid=2651896459&idx=2&sn=45c3d875b10ac03155ffa27cb0ae30e4&chksm=bcb34953cee07b5b272af8b190849181a22d91abf4423b9882279789baeeedea440727c6a0f2#rd)
*中科院计算所培训中心*

Main category: wechat.article

TL;DR: 书中介绍的代码可以帮助读者快速将算法应用到实践中。强化学习。精要 ui。编辑推荐该书系统阐述强化学习基础理论与算法实现，涵盖马尔可夫决策过程、Q-Learning、DQN改进算法及策略梯度方法等内容，结合TensorFlow框架提供代


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 书中介绍的代码可以帮助读者快速将算法应用到实践中。强化学习。精要 ui。编辑推荐该书系统阐述强化学习基础理论与算法实现，涵盖马尔可夫决策过程、Q-Learning、DQN改进算法及策略梯度方法等内容，结合TensorFlow框架提供代

</details>


### [18] [<em class="highlight">强化学习</em>+大模型记忆：Mem-α，让智能体第一次学会“如何记忆”](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2651000503&idx=3&sn=6e1443e4f8c13d3822cd12b731907565&chksm=85883e74e5bde1bde2a663e63ed7f0087076290128c5465bbd8d7d21c37ef39e9af5e6b97c37#rd)
*机器之心*

Main category: wechat.article

TL;DR: 这项工作是首次将强化学习引入大模型的记忆管理体系，让模型能够自主学习如何使用工具去存储、更新和组织记忆。Yuanzhe Hu2， Julian McAuley2， Xiaojian Wu2 ，


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这项工作是首次将强化学习引入大模型的记忆管理体系，让模型能够自主学习如何使用工具去存储、更新和组织记忆。Yuanzhe Hu2， Julian McAuley2， Xiaojian Wu2 ，

</details>


### [19] [<em class="highlight">强化学习</em>教父重出江湖， 生成式AI的时代要结束了？](http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652642887&idx=3&sn=a249244679bc3ab1be1f79cacbfb839b&chksm=f092569002b095abb47490a2e233b4063725ca1b6fa31af976a6d8c6fd3ac5446a57c4e17e06#rd)
*新智元*

Main category: wechat.article

TL;DR: 我们的重点是真正的强化学习研究，它将推动持续学习、泛化能力以及基于模型的层级规划。与当下依赖大规模语言模型的路线不同，ExperienceFlow认为智能的核心不在参数量，而在于「如何通过经验产生知识」。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们的重点是真正的强化学习研究，它将推动持续学习、泛化能力以及基于模型的层级规划。与当下依赖大规模语言模型的路线不同，ExperienceFlow认为智能的核心不在参数量，而在于「如何通过经验产生知识」。

</details>


### [20] [基于<em class="highlight">强化学习</em>的机构行为情绪拐点预警系统构建](http://mp.weixin.qq.com/s?__biz=MzA5MjE0MTI0NQ==&mid=2247534208&idx=1&sn=3c22e2f773b631ec3df81d63acd36e87&chksm=911068c612faede6be041f972677ceeb536d28cc89986c1a5b121f7be36d1854aa2868f023cf#rd)
*中国货币市场*

Main category: wechat.article

TL;DR: 随着人工智能技术的快速发展，强化学习作为一类通过与环境交互来不断优化决策的算法框架，因其具备强大的动态学习能力和适应性，为实时准确识别市场极端情绪状态提供了新的技术路径。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 随着人工智能技术的快速发展，强化学习作为一类通过与环境交互来不断优化决策的算法框架，因其具备强大的动态学习能力和适应性，为实时准确识别市场极端情绪状态提供了新的技术路径。

</details>


### [21] [文献速递—面向城市结构化空域避撞的三维交互速度障碍增强的多智能体深度<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzU2OTQ3NzUxNQ==&mid=2247485071&idx=1&sn=55062507a0f663eab54c71f5d4c45eaf&chksm=fd800f50ad679297fe42c38b035c39d346908c2504ae23d3811ee6e7cecbb620491bcbc9f6a2#rd)
*PEDynamics*

Main category: wechat.article

TL;DR: 传统方法在该情景下存在明显不足，速度障碍法在约束下解空间碎片化，而强化学习方法又往往忽略空域规则，学得的策略在实际中不可行。因此，结构化空域避撞问题呈现出三个显著特征：一是传统算法的可行解空间急剧收缩


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 传统方法在该情景下存在明显不足，速度障碍法在约束下解空间碎片化，而强化学习方法又往往忽略空域规则，学得的策略在实际中不可行。因此，结构化空域避撞问题呈现出三个显著特征：一是传统算法的可行解空间急剧收缩

</details>


### [22] [微软最新开源：让任何AI Agent“自我进化”的<em class="highlight">强化学习</em>框架](http://mp.weixin.qq.com/s?__biz=MzIzODI0MzQ5Mw==&mid=2650906537&idx=1&sn=a543993793aaca741d68566b4ee20a37&chksm=f3ebde47e71b4fcc0842f9932096d5a54d45a5b62b47822ef02225764defcfa33d865885ca50#rd)
*AI Pulse*

Main category: wechat.article

TL;DR: 强化学习（Reinforcement Learning， RL）被认为是解决这一问题的关键，它能基于“任务结果反馈”直接优化模型策略，而无需昂贵标注数据。然而，将 RL 应用于 Agent 场景仍面临巨大挑战——


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（Reinforcement Learning， RL）被认为是解决这一问题的关键，它能基于“任务结果反馈”直接优化模型策略，而无需昂贵标注数据。然而，将 RL 应用于 Agent 场景仍面临巨大挑战——

</details>


### [23] [从马尔可夫决策过程到智能体学习闭环：LLM <em class="highlight">强化学习</em>的工程实践剖析](http://mp.weixin.qq.com/s?__biz=MzAxMTQwODY1MQ==&mid=2651574369&idx=1&sn=a001bca353bfe6a29b35c2617591e8e2&chksm=81cb0e858869eb2284f1812b84f700c856496f19d09462d2261a60270d46f25c5b203de626a9#rd)
*云与数字化*

Main category: wechat.article

TL;DR: 强化学习（ppo）：用奖励信号反向更新策略，让模型倾向输出“更受欢迎”的结果。形式化地说： [ \max_\theta E_a \sim \pi_\theta（as）[R（s， a）] ] RLHF 让模型不仅“会说”，还要“说得让人满意”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（ppo）：用奖励信号反向更新策略，让模型倾向输出“更受欢迎”的结果。形式化地说： [ \max_\theta E_a \sim \pi_\theta（as）[R（s， a）] ] RLHF 让模型不仅“会说”，还要“说得让人满意”。

</details>


### [24] [华为云<em class="highlight">Agentic</em> AI驱动存储架构变革，数据存储向“知识存储”跃迁](http://mp.weixin.qq.com/s?__biz=MzU1MjcwNTM5OA==&mid=2247486682&idx=8&sn=23aa638b7ca5e29bd2a46a56a30f87e1&chksm=fa50305f68ecdd0d37c5bff77742e51c0d84bcd863ce877bab4ec14ae7e40e3db26975b0554d#rd)
*聚搜营销*

Main category: wechat.article

TL;DR: Agentic AI 时代的核心特征是智能体（Agent）的崛起，这促使存储的价值不再局限于简单的“数据存放”，而是正在向「知识存储」跃迁。未来的 AI Data Platform 必须具备以下能力：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 时代的核心特征是智能体（Agent）的崛起，这促使存储的价值不再局限于简单的“数据存放”，而是正在向「知识存储」跃迁。未来的 AI Data Platform 必须具备以下能力：

</details>


### [25] [华为云的组合新范式，引爆了<em class="highlight">Agentic</em> AI应用革命](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2651000503&idx=1&sn=6ad854f9744346f407af041a82853024&chksm=85ef0947122cd46cda940d74ff95b016b23c0934324d82c882063a89d7130fe5c1a517d65bc6#rd)
*机器之心*

Main category: wechat.article

TL;DR: Agentic AI，从来没有这么简单过。刚刚，在 2025 全球计算大会（CGC 2025）上，华为云打出了一套 AI 时代技术落地的组合拳。今年是 AI 大模型的落地关键年。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI，从来没有这么简单过。刚刚，在 2025 全球计算大会（CGC 2025）上，华为云打出了一套 AI 时代技术落地的组合拳。今年是 AI 大模型的落地关键年。

</details>


### [26] [<em class="highlight">Agentic</em> AI的5种应用方式](http://mp.weixin.qq.com/s?__biz=MzIzMTIyODMwMA==&mid=2650193517&idx=1&sn=0ff7a9dd8000eb1cb0c2eb48de4ce60b&chksm=f112e08092653dc4325aaa85d54e66ee6cef4b22f64ea23f4d6dbce5d25936f634061144a2b5#rd)
*AGI商业新声*

Main category: wechat.article

TL;DR: 5 ways to use agentic ai 1 prompt routing agent 2 query writing。· expanding keywords into richer search terms。· converting natural language into sql， api calls， or vector retrieval queries.。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 5 ways to use agentic ai 1 prompt routing agent 2 query writing。· expanding keywords into richer search terms。· converting natural language into sql， api calls， or vector retrieval queries.。

</details>


### [27] [Google发布！一文了解21种<em class="highlight">Agentic</em>设计模式](http://mp.weixin.qq.com/s?__biz=MzYzNzE2ODIxMg==&mid=2247483685&idx=1&sn=186ac5b3590472df0f627c3e3166040d&chksm=f1c03c8f3ac82d6a1705416bcf5b7ceea3c96b3af09d34a48a24e8ba70cd4da7e97e710b87f2#rd)
*EasyShip.AI*

Main category: wechat.article

TL;DR: What makes an AI system an "agent"？， 9 pages [final， last read done] part three （total： 34 pages） 12. chapter 12： exception handling and recovery （code）， 8 pages [final， last read done， code ok] 13. c...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: What makes an AI system an "agent"？， 9 pages [final， last read done] part three （total： 34 pages） 12. chapter 12： exception handling and recovery （code）， 8 pages [final， last read done， code ok] 13. chapter 13： human-in-the-loop （code）， 9 pages [final， last read don

</details>


### [28] [《<em class="highlight">Agentic</em> AI 现状：创始人版》分享：看看创始人们如何看待AI浪潮?](http://mp.weixin.qq.com/s?__biz=MzA4Njc4MzUzOA==&mid=2651569751&idx=1&sn=a9a61245b4291c350bcb31d53426b9a7&chksm=8569a43ea9a3ef74a7e0bdc1789d117d1d6fcf0b1399b765c57b805493bf0a4db8a25592cbd7#rd)
*雷蒙说*

Main category: wechat.article

TL;DR: 以下是根据 MMC Ventures 发布的《State of Agentic AI： Founder’s Edition》（2025年11月3日）内容整理的中文总结，涵盖文章核心主题、关键发现与细节要点：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 以下是根据 MMC Ventures 发布的《State of Agentic AI： Founder’s Edition》（2025年11月3日）内容整理的中文总结，涵盖文章核心主题、关键发现与细节要点：

</details>


### [29] [麦肯锡总结的<em class="highlight">Agentic</em> AI落地的6条经验教训](http://mp.weixin.qq.com/s?__biz=MzA5NTI1MTcyNw==&mid=2651119725&idx=1&sn=bdce1893dd817dc630eafbad90eed44e&chksm=8a413fcf136a61c43a5ccc761b1addd9a5c741a4063f4cdc2d5de78f38a0f7a3d792cb8da7d7#rd)
*科技領袖Alliance*

Main category: wechat.article

TL;DR: 归结为六个教训，以帮助领导者成功地从Agentic Al中获取价值。it's not about the agent； it's about the workflow。企业常过度关注A1智能体（A1 Agent）本身的技术，却忽略了其应嵌入的业务流程。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 归结为六个教训，以帮助领导者成功地从Agentic Al中获取价值。it's not about the agent； it's about the workflow。企业常过度关注A1智能体（A1 Agent）本身的技术，却忽略了其应嵌入的业务流程。

</details>


### [30] [<em class="highlight">Agentic</em>21种设计模式-What makes an AI system an Agent?](http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484250&idx=1&sn=6471aa3b8a4c8c2b4f2d376705137221&chksm=c0f51fab7d9ec1ae64ffca3fe8b7d6e4f9c64bf0c158d94504f09339341aa9b4aff169b6ec6f#rd)
*AI Lab Dev*

Main category: wechat.article

TL;DR: agentic ai problem-solving process get the mission 02 scan the scene 03 think lt through 04 take action 05 learn & get better图 1：这种 Agent 充当智能助手的角色，通过不断学习来提升自身能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic ai problem-solving process get the mission 02 scan the scene 03 think lt through 04 take action 05 learn & get better图 1：这种 Agent 充当智能助手的角色，通过不断学习来提升自身能力。

</details>


### [31] [《2025 AI <em class="highlight">大模型</em>开发生态白皮书》正式发布 | 算泥社区](http://mp.weixin.qq.com/s?__biz=MzU4ODkyNzcyNA==&mid=2247534619&idx=1&sn=4b7460431dc0932b6bedb7ffde93855c&chksm=fcd6daac40c15f61b218cfef7f8b89998100d1be00f322a31468da04cd3096690a06c9bf0583#rd)
*RPA全球生态*

Main category: wechat.article

TL;DR: 2025年，AI大模型的技术迭代呈现出明显的加速态势，其核心特征是从单纯追求基准测试分数的“能力”（Capability）提升，转向更加注重模型在真实世界中的可靠性、安全性和实用性的“可用性”（Usability）进化。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2025年，AI大模型的技术迭代呈现出明显的加速态势，其核心特征是从单纯追求基准测试分数的“能力”（Capability）提升，转向更加注重模型在真实世界中的可靠性、安全性和实用性的“可用性”（Usability）进化。

</details>


### [32] [深度推理<em class="highlight">大模型</em>讯飞星火X1.5发布](http://mp.weixin.qq.com/s?__biz=MzA5NzAwNjg4NA==&mid=2650741338&idx=2&sn=d28732de26e86471264b6db8e53225e7&chksm=891baa49aa9009b3ecd3944de7c5ded06cde03ba0396d577d91a18133590adb44e4b8deb46a6#rd)
*安徽省科技厅*

Main category: wechat.article

TL;DR: 发布会上，深度推理大模型讯飞星火X1.5正式亮相。星火X1.5推理效率相比星火X1提升100%，它的语言理解、文本生成、知识问答、逻辑推理、数学能力、代码能力等六大核心能力全面对标国际主流，其中数学能力持续保持国际领先


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 发布会上，深度推理大模型讯飞星火X1.5正式亮相。星火X1.5推理效率相比星火X1提升100%，它的语言理解、文本生成、知识问答、逻辑推理、数学能力、代码能力等六大核心能力全面对标国际主流，其中数学能力持续保持国际领先

</details>


### [33] [<em class="highlight">大模型</em>集体翻车！字节 × 斯坦福新基准 MIRA 揭示多模态AI的“视觉推理短板”](http://mp.weixin.qq.com/s?__biz=MzYyMTk5MjAxOA==&mid=2247483740&idx=1&sn=8d868ed0d3b6ae4877c28ba9ac2e49bf&chksm=fe67b6ad432597e42ed40ef278d1d64456fcff8ae675ceb267201df27b6aeabb14dcea5b36c1#rd)
*AIdea空间*

Main category: wechat.article

TL;DR: 向左滑动查看更多大模型翻车瞬间（正确答案：2）一、从“思维链”到“视觉断链”：Text-CoT的致命软肋在过去的几年里，大型语言模型的推理能力通过一种名为“思维链”（Chain-of-Thought， CoT）的技术被极大激活了。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 向左滑动查看更多大模型翻车瞬间（正确答案：2）一、从“思维链”到“视觉断链”：Text-CoT的致命软肋在过去的几年里，大型语言模型的推理能力通过一种名为“思维链”（Chain-of-Thought， CoT）的技术被极大激活了。

</details>


### [34] [【微科普】从AI工具看AI新浪潮：<em class="highlight">大模型</em>与智能体如何重塑未来？](http://mp.weixin.qq.com/s?__biz=Mzg2MjYyNDU3MQ==&mid=2247488191&idx=2&sn=7315e4510654bfee2b055c77504077c1&chksm=cfa24adc93b57341abbc8120d7a3af67e2ea18a07d25eed6985040f40e889c0f1b061000589e#rd)
*微风智选*

Main category: wechat.article

TL;DR: 大模型（Large Model）指的是通过利用海量数据训练而成的深度学习模型，通常具有参数量大、训练数据大、计算资源大等显著特点，具备强大的数据处理和生成能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型（Large Model）指的是通过利用海量数据训练而成的深度学习模型，通常具有参数量大、训练数据大、计算资源大等显著特点，具备强大的数据处理和生成能力。

</details>


### [35] [牛！<em class="highlight">大模型</em>的9大核心技术解析！](http://mp.weixin.qq.com/s?__biz=MzkzNzg2NTY1MA==&mid=2247485546&idx=1&sn=f337ab2763c6bcfbc2b1134fae5dc291&chksm=c375e32de7a54bf90ea43f551cec3e0addcbd396836e6de6fd5255e3fea2426c4e2d469b29a6#rd)
*探数通*

Main category: wechat.article

TL;DR: ai智能体架构设计 1 2 planning 3 4 action 5 llm 8 6 工具集 大模型 7 9 observation 10 10 final answer 掘金技术社区@聚客ai 二、Agentic AIAgentic AI 代表多智能体协作的系统架构。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ai智能体架构设计 1 2 planning 3 4 action 5 llm 8 6 工具集 大模型 7 9 observation 10 10 final answer 掘金技术社区@聚客ai 二、Agentic AIAgentic AI 代表多智能体协作的系统架构。

</details>


### [36] [全球开源榜第一的编程<em class="highlight">大模型</em>来了！青云上线 MiniMax M2](http://mp.weixin.qq.com/s?__biz=MzkxNTE5MjIxNQ==&mid=2247531220&idx=1&sn=9082905fd9baf4122a56ee2720337ea0&chksm=c03556b07ee53bd6a4033771f5e06283dfc728c800c8a004b3391c516f4eb527189317e2af48#rd)
*青云科技*

Main category: wechat.article

TL;DR: 平台提供模型询用服务，模实备际调用备计册，可先创建API宗则，使用在线体验功能，也可通过openA用调用接口服装，服使用指南 全部 文生文 文生图 文生视频 图生视频 语音 嵌入 重排序 多模志 api密明管理 用量统计 minimax-m2 de


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 平台提供模型询用服务，模实备际调用备计册，可先创建API宗则，使用在线体验功能，也可通过openA用调用接口服装，服使用指南 全部 文生文 文生图 文生视频 图生视频 语音 嵌入 重排序 多模志 api密明管理 用量统计 minimax-m2 de

</details>


### [37] [AI模型网络安全实力大比拼：中国电信研究院发布<em class="highlight">大模型</em>网络安全能力评测报告](http://mp.weixin.qq.com/s?__biz=MzU0ODkwNDc4Mw==&mid=2247498028&idx=1&sn=372bd79f83b5d71b60716293cf693142&chksm=fa68648be81e67e472a99032860071ac8136ed58eb251fcc8e89effaa00c7c7e1cffd051b1fe#rd)
*CNIS国家工程研究中心*

Main category: wechat.article

TL;DR: 九款主流大模型网络安全能力排名、能力分布视图如下，详细测评结果可参考文末测评报告。各个ai安全模型整体评价排名。glm。0. 795。gpt-oss。0. 782 seed-oss。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 九款主流大模型网络安全能力排名、能力分布视图如下，详细测评结果可参考文末测评报告。各个ai安全模型整体评价排名。glm。0. 795。gpt-oss。0. 782 seed-oss。

</details>


### [38] [【微科普】从AI工具看AI新浪潮：<em class="highlight">大模型</em>与智能体如何重塑未来？](http://mp.weixin.qq.com/s?__biz=MzUyNTkyNjc4OQ==&mid=2247502698&idx=2&sn=dc56a867d996199ec84b5a882ff35804&chksm=fbd42d5ecb4449222d8b01291be813320d2a1bab4e56198689732237f615c72156a44634dee9#rd)
*微风企*

Main category: wechat.article

TL;DR: 大模型（Large Model）指的是通过利用海量数据训练而成的深度学习模型，通常具有参数量大、训练数据大、计算资源大等显著特点，具备强大的数据处理和生成能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型（Large Model）指的是通过利用海量数据训练而成的深度学习模型，通常具有参数量大、训练数据大、计算资源大等显著特点，具备强大的数据处理和生成能力。

</details>


### [39] [国产<em class="highlight">模型</em>新盛况！王座易主：Kimi K2 Thinking开源超闭源](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2651000477&idx=1&sn=775ce5bef5782aae46b873d11b788893&chksm=85d0255eb2a600ec8cc446a9f91797edcd319cc4c9b66950a5bc62f9a8f4d75dc3d9f279a4e0#rd)
*机器之心*

Main category: wechat.article

TL;DR: 昨晚，月之暗面（Moonshot AI）刚刚开源了最新一代大模型 Kimi K2 Thinking，新模型一发布，就掀起了全网的大讨论。作为一款开源模型，它在基准测试上毫无保留，多方面性能直接超越了 GPT-5、Claude Sonnet 4.5 等业界先进闭源模型。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 昨晚，月之暗面（Moonshot AI）刚刚开源了最新一代大模型 Kimi K2 Thinking，新模型一发布，就掀起了全网的大讨论。作为一款开源模型，它在基准测试上毫无保留，多方面性能直接超越了 GPT-5、Claude Sonnet 4.5 等业界先进闭源模型。

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [40] [Serena](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Foraios%2Fserena%3Futm_source=tldrdevops/1/0100019a53e8fb63-19fa30d1-eba0-45d3-9eb6-b2ed3b20457b-000000/XZ-_iLdR12Y92EPsx0stPRPmcG--EB2tqgygv2GQnus=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Serena是一个开源编码代理工具包，结合语义代码检索与编辑和shell执行功能，通过MCP服务器和LSP集成，可与Claude Code等LLM集成以节省token和时间。


<details>
  <summary>Details</summary>
Motivation: 提供可定制的编码代理工具包，通过语义检索和集成功能提高开发效率，减少LLM使用成本。

Method: 使用MCP服务器和LSP语言服务器集成，结合语义代码检索、编辑和shell执行功能，支持通过Modes和Contexts进行定制。

Result: 开发了Serena工具包，能够与Claude Code等LLM集成，提供高效的编码辅助功能。

Conclusion: Serena是一个功能丰富且可定制的开源编码代理工具包，能够显著提升开发效率并降低LLM使用成本。

Abstract: Serena (GitHub Repo) Serena, a free and open-source coding agent toolkit, combines semantic code retrieval with editing and shell execution via its MCP server and LSP-based language server integrations, and can be integrated with LLMs like Claude Code to save tokens and time. Serena can be further customized through Modes and Contexts, which allow users to tailor its behavior to their workflow and environment.

</details>


### [41] [7 habits of highly effective AI coding](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2F7-habits-of-highly-effective-ai-coding-ebook%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-7-habits-ebook25%26utm_content=newsletter-primary-tldr-dev-251105-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019a53ec2e04-fc2d5128-4728-4a9e-98e9-a4788961bfe2-000000/fG5PFt3nwvtNzkzQPcvnT8goP95bBF4ZX7VsOqSN3is=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 介绍了有效使用AI编程的7个关键习惯，旨在帮助团队安全高效地采用AI编码，避免安全风险和技术债务


<details>
  <summary>Details</summary>
Motivation: 帮助开发团队在采用AI编码时避免安全风险、减少技术债务，同时提升工程生产力和代码质量

Method: 提出7个核心习惯框架，包括提升真实工程生产力、确保AI生成代码的安全性可靠性、培养开发者责任文化等

Result: 通过采用这些习惯，团队能够更自信地使用AI，减少重复劳动，交付安全高质量的代码

Conclusion: 系统性地采用AI编码需要建立正确的习惯和实践，以确保代码安全性和工程效率

Abstract: 7 habits of highly effective AI coding (Sponsor) How can your team use AI coding effectively without drowning in security risks and new technical debt?Discover the 7 essential habits you need to confidently adopt AI, reduce toil, and ship secure, high-quality code. Read this ebook from Sonar to learn how to: Boost real engineering productivity and velocity, not just code volume. Ensure AI-generated code is secure, reliable, and maintainable. Foster a culture of developer accountability. Tackl...

</details>


### [42] [Windsurf Codemaps: Understand Code, Before You Vibe It](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fcodemaps%3Futm_source=tldrwebdev/1/0100019a53ec2e04-fc2d5128-4728-4a9e-98e9-a4788961bfe2-000000/A83Avk75SlOX1VKIa-YPxob7VDLMUVkRSq1Rqk75RRs=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Windsurf Codemaps 是一个由 SWE-1.5 和 Claude Sonnet 4.5 驱动的 AI 注释代码结构图，帮助开发者快速理解和导航复杂代码库。


<details>
  <summary>Details</summary>
Motivation: 解决开发者在调试和重构等任务中快速理解复杂代码库的需求，提供即时映射功能。

Method: 使用 SWE-1.5 和 Claude Sonnet 4.5 生成 AI 注释的结构化代码地图，可视化代码结构并追踪功能。

Result: 开发了能够帮助开发者可视化代码结构、追踪功能并改善 AI 代理上下文理解的工具。

Conclusion: Codemaps 为开发者提供了更高效的代码理解和导航方式，特别适用于调试和重构任务。

Abstract: Windsurf Codemaps: Understand Code, Before You Vibe It (7 minute read) Cognition has announced Windsurf Codemaps, an AI-annotated structured map of codebases powered by SWE-1.5 and Claude Sonnet 4.5 designed to help devs quickly understand and navigate complex code. Codemaps provides just-in-time mapping for tasks like debugging and refactoring, allowing devs to visualize code structure, trace functionality, and improve context for AI agents.

</details>


### [43] [Chaining ffmpeg with a Browser Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2F100x.bot%2Fa%2Fchaining-ffmpeg-with-browser-agent%3Futm_source=tldrwebdev/1/0100019a53ec2e04-fc2d5128-4728-4a9e-98e9-a4788961bfe2-000000/tBQOjv7RjrOt7kR5eSdt5nZAJRtZlxWuSyxmPzA6PO4=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 通过将FFmpeg作为WebAssembly模块集成到浏览器代理中，可以在无服务器、无状态的工作流中执行复杂的媒体处理，将FFmpeg转变为可组合的API，作为单个步骤访问。


<details>
  <summary>Details</summary>
Motivation: 解决复杂媒体处理在无服务器环境中执行的挑战，使FFmpeg能够在浏览器代理中作为可组合的API使用，简化媒体处理工作流。

Method: 将FFmpeg编译为WebAssembly模块，集成到浏览器代理中，使其能够在无服务器、无状态的工作流中执行媒体处理任务。

Result: 成功实现了FFmpeg作为可组合API的集成，可以在浏览器代理中作为单个步骤执行复杂的媒体处理操作。

Conclusion: 该方法有效解决了无服务器环境中媒体处理的挑战，使FFmpeg能够以更灵活、可组合的方式在浏览器代理中使用。

Abstract: Chaining ffmpeg with a Browser Agent (4 minute read) By integrating FFmpeg as a WebAssembly module within a browser agent, complex media processing can be executed serverlessly and statelessly within a workflow, turning FFmpeg into a composable API accessible as a single step.

</details>


### [44] [A simple test for developing taste](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.commonreader.co.uk%2Fp%2Fmetapreferences-and-the-future-of%3Futm_source=tldrfounders/1/0100019a54317134-259c1825-e3a4-4e6d-8018-2bcc38d6b661-000000/fLcS2ICNoaLbsLXZm0ZZRq6j2eA__1gPkcMjkRer4xI=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A simple test for developing taste (4 minute read) Taste isn't about what you like—it's about how your preferences change once you've seen something better. You read Middlemarch after Remains of the Day, and suddenly the scale resets. You can't unsee the difference. That's how taste develops: through comparison, awareness, and the slow upgrading of what you think good means. It happens in every field - music, design, writing, code. The more you expose yourself to better work, the clearer your...

</details>


### [45] [Chaining ffmpeg with a Browser Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2F100x.bot%2Fa%2Fchaining-ffmpeg-with-browser-agent%3Futm_source=tldrai/1/0100019a5461b001-b6f95403-041d-4762-95af-c13bbb2e8b90-000000/X1oNkpu00NAOd6f15YYVxWfoXbXenwmF99_s_BkJDOI=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 100x将ffmpeg集成到其Chrome扩展中，使复杂的媒体处理成为工作流中的单一、无服务器、无状态步骤，作为浏览器代理的工具调用。


<details>
  <summary>Details</summary>
Motivation: 使浏览器代理能够处理复杂的媒体编辑任务，如音频视频混合和视频添加文本，而无需服务器支持。

Method: 将ffmpeg作为工具调用集成到现有的浏览器代理中，利用Chrome扩展实现无服务器、无状态的媒体处理。

Result: 成功实现了浏览器代理对复杂媒体编辑任务的处理能力，使ffmpeg成为代理的工具之一。

Conclusion: 通过将ffmpeg集成到浏览器代理中，可以有效地处理复杂的媒体处理任务，扩展了代理的功能范围。

Abstract: Chaining ffmpeg with a Browser Agent (4 minute read) 100x included ffmpeg into its Chrome extension to make complex media processing a single, serverless, and stateless step in any workflow. It already had a browser agent for automation - ffmpeg became another 'tool call' for the agent. This enabled the agent to deal with complex editing tasks like mixing audio and video and adding text over video. This post gives readers a look at how the integration works.

</details>


### [46] [Code execution with MCP: Building more efficient agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fcode-execution-with-mcp%3Futm_source=tldrai/1/0100019a5461b001-b6f95403-041d-4762-95af-c13bbb2e8b90-000000/RmAlfS00QJpZY0imWQR0VM-cdsUIJuJfFmNBuSNbDes=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 通过代码执行让代理与MCP服务器更高效交互，解决工具定义和结果消耗过多token的问题


<details>
  <summary>Details</summary>
Motivation: 当连接过多MCP服务器时，工具定义和结果会消耗过多token，降低代理效率。这些问题在软件工程中已有解决方案

Method: 应用代码执行模式，让代理使用熟悉的编程构造与MCP服务器交互

Result: 提高了代理与MCP服务器交互的效率

Conclusion: 代码执行将软件工程中的成熟模式应用于代理系统，解决了MCP中的效率问题

Abstract: Code execution with MCP: Building more efficient agents (15 minute read) Code execution can enable agents to interact with MCP servers more efficiently. When too many servers are connected, tool definitions and results can consume excessive tokens, reducing agent efficiency. Many of the problems with MCP feel novel, but they have known solutions from software engineering. Code execution applies established patterns to agents, letting them use familiar programming constructs to interact with M...

</details>


### [47] [Profiling with Cursor 2.0: The Missing Layer in AI Code Generation](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fryanperry.io%2Fpost%2Fcursor-profiling-missing-layer%3Futm_source=tldrai/1/0100019a5461b001-b6f95403-041d-4762-95af-c13bbb2e8b90-000000/UiUyISu-giohcyIJwy1C104fa7TmWcGGR2W-aCXMzq8=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Pyroscope Performance Profiler是一个Cursor扩展，通过将性能分析数据直接标注在代码上，帮助开发者基于实际性能数据从多个AI生成的解决方案中选择最佳方案。


<details>
  <summary>Details</summary>
Motivation: 现有的AI代码生成工具虽然能产生看起来不错的代码，但缺乏评估代码实际性能的方法。开发者需要一种工具来区分'看起来好'和'实际上好'的代码。

Method: 开发Pyroscope性能分析器作为Cursor扩展，将性能分析数据直接可视化在代码编辑器中，支持多代理解决方案的性能比较、趋势分析和事件检查。

Result: 该工具使开发者能够基于真实性能数据评估不同AI代理生成的代码，提高了代码质量评估的客观性和效率。

Conclusion: 性能分析是AI代码生成中缺失的关键环节，Pyroscope填补了这一空白，帮助开发者做出更明智的代码选择决策。

Abstract: Profiling with Cursor 2.0: The Missing Layer in AI Code Generation (5 minute read) Profiling is the best way to separate code that looks good from code that is actually good. The Pyroscope Performance Profiler is a Cursor extension that takes profiling data and paints it right on the code. It allows developers to look at multiple solutions generated by different agents and pick the best based on real numbers. Performance comparisons across deployments, trend analysis, and incident inspection ...

</details>


### [48] [Getting agents to code is easy... but can you get them to follow your rules?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Feu1.hubs.ly%2FH0p2Yqz0%3Futm_source=tldrai/1/0100019a5461b001-b6f95403-041d-4762-95af-c13bbb2e8b90-000000/CTHikhqza8JVMJ2JkJEG02l39K4-ruLf_4i8c1TPZio=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Tabnine提出了一种能够理解架构、标准和合规规则的AI代码生成工具，解决了现有工具只生成代码但无法遵循规则的问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI代码生成工具虽然能生成代码行，但无法理解用户的架构规范、编码标准和合规要求，导致生成的代码不符合实际开发需求。

Method: Tabnine通过专门的技术来理解和遵循用户的架构、标准和合规规则，使AI生成的代码能够符合具体的开发规范。

Result: Tabnine成功弥合了AI代码生成与遵循开发规则之间的差距，能够生成符合特定架构和标准的代码。

Conclusion: Tabnine证明了AI代码生成工具不仅能够生成代码，还能够理解和遵循开发规则，为代码开发提供了更智能的解决方案。

Abstract: Getting agents to code is easy... but can you get them to follow your rules? (Sponsor) Most AI tools generate lines of code — but fail to understand your architecture, standards, and compliance rules. Discover how Tabnine bridges the gap

</details>


### [49] [Codemaps: Understand Code, Before You Vibe It](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fcodemaps%3Futm_source=tldrai/1/0100019a5461b001-b6f95403-041d-4762-95af-c13bbb2e8b90-000000/b2wgjEPXNQtQoVj2VRDXDegXi2EWxZApH8ccwmPDjaA=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Codemaps通过AI生成代码库的结构化注释地图，帮助开发者理解新代码库或维护不熟悉的遗留代码，降低学习成本。


<details>
  <summary>Details</summary>
Motivation: 解决开发者学习新代码库或维护不熟悉遗留代码时的高昂入门成本问题。

Method: 使用AI技术创建代码库的结构化注释地图，为代码提供智能标注和组织。

Result: 开发了Windsurf Codemaps工具，能够自动生成代码的AI注释结构化地图。

Conclusion: Codemaps通过AI驱动的代码可视化方法，有效降低了代码理解和维护的门槛。

Abstract: Codemaps: Understand Code, Before You Vibe It (5 minute read) Cognition launched Windsurf Codemaps, which creates AI-annotated structured maps of codebases to address the onboarding costs of learning a new codebase or maintaining unfamiliar legacy code.

</details>


### [50] [ClickHouse Welcomes LibreChat: Introducing the Open-Source Agentic Data Stack](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclickhouse.com%2Fblog%2Flibrechat-open-source-agentic-data-stack%3Futm_source=tldrdata/1/0100019a58d992b4-e87efaf0-e81d-45c4-9475-4d30b556bb66-000000/_FB5CkJrGJBe_6CfcWwAv1_en9CUTjk8XUljo-y1ZDs=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ClickHouse收购了LibreChat，将其开源的多LLM聊天平台整合为统一的Agentic Data Stack的核心组件，用于面向代理的分析。


<details>
  <summary>Details</summary>
Motivation: 构建统一的Agentic Data Stack，为代理提供更好的分析能力，整合开源的多LLM聊天平台以增强数据栈的功能。

Method: 通过收购LibreChat并将其开源的多LLM聊天平台整合到ClickHouse数据栈中，形成统一的Agentic Data Stack。

Result: 成功构建了面向代理的分析数据栈，整合了多LLM聊天功能。

Conclusion: ClickHouse通过收购LibreChat增强了其数据栈的代理分析能力，为开发者提供了更强大的工具。

Abstract: ClickHouse Welcomes LibreChat: Introducing the Open-Source Agentic Data Stack (7 minute read) ClickHouse has acquired LibreChat, integrating its open-source, multi-LLM chat platform as a core component of a unified Agentic Data Stack for agent-facing analytics.

</details>


### [51] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdata%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a58d992b4-e87efaf0-e81d-45c4-9475-4d30b556bb66-000000/T42Ua3w8MrLoYvZOWaE71xg1kZqbe13Goekz2NMZkAA=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ClickHouse收购了LibreChat，将其开源的多LLM聊天平台整合为统一的Agentic Data Stack的核心组件，用于面向代理的分析。


<details>
  <summary>Details</summary>
Motivation: 构建一个统一的代理数据堆栈，为AI代理提供分析能力，通过整合聊天平台来增强数据交互和分析功能。

Method: 通过收购LibreChat开源多LLM聊天平台，并将其整合到ClickHouse的Agentic Data Stack中。

Result: 创建了一个统一的代理数据堆栈，集成了多LLM聊天功能，为代理提供数据分析能力。

Conclusion: ClickHouse通过收购LibreChat成功构建了面向代理的统一数据堆栈，增强了AI代理的数据分析能力。

Abstract: ClickHouse Welcomes LibreChat: Introducing the Open-Source Agentic Data Stack (7 minute read) ClickHouse has acquired LibreChat, integrating its open-source, multi-LLM chat platform as a core component of a unified Agentic Data Stack for agent-facing analytics.

</details>


### [52] [30,000 bugs in 30 days](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fai-code-review-30k-bugs-lighter-50-faster%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q4-aicodereviewlaunch%26utm_content=newsletter-30k-blog-learnmore/2/0100019a591e2f3d-ec2946e5-1c0f-4f48-99b9-eafc27e7c61f-000000/nEzvw9ztzr5lqCy15MoA9KxQSRLT_guzgGgAVPqKauE=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sentry的AI代码审查工具在30天内发现了30,000个bug，审查速度提升了50%，通过'思考预算'机制防止过度思考。


<details>
  <summary>Details</summary>
Motivation: 提高代码审查效率和准确性，通过AI技术自动发现代码中的bug，减少人工审查负担。

Method: 应用'思考预算'机制限制每个审查步骤的思考时间，防止AI过度思考；引入Claude Skill连接Claude Code与AI代码审查输出；优化AI评论结构。

Result: 30天内发现30,000多个bug，审查速度提升50%。

Conclusion: AI代码审查工具能有效提升代码质量和审查效率，'思考预算'机制是提高性能的关键。

Abstract: 30,000 bugs in 30 days (Sponsor) Sentry launched AI Code Review 30 days ago. Since then, it's caught 30k bugs and is working 50% faster. Read the blog post to learn: How Sentry applies a “thinking budget” to each review step to prevent overthinking - making AI code reviews are leaner and speedier. The new Claude Skill that connects Claude Code directly to AI Code Review outputs. Revamping how AI comments are structured ➡️ The bottom line: AI Code Review has caught 30K+ bugs.. .and it's only g...

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [53] [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)
*Zhaorun Chen,Zhuokai Zhao,Kai Zhang,Bo Liu,Qi Qi,Yifan Wu,Tarun Kalluri,Sara Cao,Yuanhao Xiong,Haibo Tong,Huaxiu Yao,Hengduo Li,Jiacheng Zhu,Xian Li,Dawn Song,Bo Li,Jason Weston,Dat Huynh*

Main category: cs.AI

TL;DR: DreamGym是一个统一的框架，通过合成多样化经验数据来解决RL训练中的挑战，避免昂贵的真实环境交互，实现可扩展的自主代理训练。


<details>
  <summary>Details</summary>
Motivation: 解决RL在实际应用中面临的挑战：昂贵的环境交互、任务多样性有限、奖励信号不可靠和基础设施复杂性，这些因素阻碍了可扩展经验数据的收集。

Method: 1) 将环境动态提炼为基于推理的经验模型，通过逐步推理获得一致的状态转换和反馈信号；2) 使用离线真实世界数据初始化经验回放缓冲区，并持续丰富新交互；3) 自适应生成挑战当前代理策略的新任务，实现有效的在线课程学习。

Result: 在多样环境和代理骨干上的实验表明：在非RL就绪任务如WebArena上，DreamGym比所有基线方法性能提升超过30%；在RL就绪但成本高的设置中，仅使用合成交互就能匹配GRPO和PPO性能；在将纯合成经验训练的策略迁移到真实环境RL时，DreamGym在需要更少真实世界交互的同时获得显著性能提升。

Conclusion: DreamGym为通用RL提供了一个可扩展的预热启动策略，通过合成经验有效解决了RL训练中的数据收集瓶颈问题。

Abstract: While reinforcement learning (RL) can empower large language model (LLM)
agents by enabling self-improvement through interaction, its practical adoption
remains challenging due to costly rollouts, limited task diversity, unreliable
reward signals, and infrastructure complexity, all of which obstruct the
collection of scalable experience data. To address these challenges, we
introduce DreamGym, the first unified framework designed to synthesize diverse
experiences with scalability in mind to enable effective online RL training for
autonomous agents. Rather than relying on expensive real-environment rollouts,
DreamGym distills environment dynamics into a reasoning-based experience model
that derives consistent state transitions and feedback signals through
step-by-step reasoning, enabling scalable agent rollout collection for RL. To
improve the stability and quality of transitions, DreamGym leverages an
experience replay buffer initialized with offline real-world data and
continuously enriched with fresh interactions to actively support agent
training. To improve knowledge acquisition, DreamGym adaptively generates new
tasks that challenge the current agent policy, enabling more effective online
curriculum learning. Experiments across diverse environments and agent
backbones demonstrate that DreamGym substantially improves RL training, both in
fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready
tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in
RL-ready but costly settings, it matches GRPO and PPO performance using only
synthetic interactions. When transferring a policy trained purely on synthetic
experiences to real-environment RL, DreamGym yields significant additional
performance gains while requiring far fewer real-world interactions, providing
a scalable warm-start strategy for general-purpose RL.

</details>


### [54] [KnowThyself: An Agentic Assistant for LLM Interpretability](https://arxiv.org/abs/2511.03878)
*Suraj Prasai,Mengnan Du,Ying Zhang,Fan Yang*

Main category: cs.AI

TL;DR: KnowThyself是一个基于聊天的智能助手，旨在提升大语言模型的可解释性，通过整合现有工具提供统一界面，降低技术门槛。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型可解释性工具分散且代码密集，用户使用不便。KnowThyself旨在将这些能力整合到聊天界面中，让用户能够通过自然语言提问并获得交互式可视化解释。

Method: 采用编排器LLM重新表述用户查询，代理路由器将查询导向专门模块，最后将输出情境化为连贯的解释。整个流程嵌入对话式工作流中。

Result: 开发了一个可扩展的平台，通过聊天界面提供模型上传、自然语言提问和交互式可视化功能，降低了LLM检查的技术障碍。

Conclusion: KnowThyself为可访问的大语言模型可解释性提供了坚实基础，通过对话式工作流整合了分散的工具能力。

Abstract: We develop KnowThyself, an agentic assistant that advances large language
model (LLM) interpretability. Existing tools provide useful insights but remain
fragmented and code-intensive. KnowThyself consolidates these capabilities into
a chat-based interface, where users can upload models, pose natural language
questions, and obtain interactive visualizations with guided explanations. At
its core, an orchestrator LLM first reformulates user queries, an agent router
further directs them to specialized modules, and the outputs are finally
contextualized into coherent explanations. This design lowers technical
barriers and provides an extensible platform for LLM inspection. By embedding
the whole process into a conversational workflow, KnowThyself offers a robust
foundation for accessible LLM interpretability.

</details>


### [55] [ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering](https://arxiv.org/abs/2511.03985)
*Zhuowen Yuan,Tao Liu,Yang Yang,Yang Wang,Feng Qi,Kaushik Rangadurai,Bo Li,Shuang Yang*

Main category: cs.AI

TL;DR: ArchPilot是一个多代理系统，通过集成架构生成、基于代理的评估和自适应搜索来解决LLM代理在自动化ML工程中依赖重复完整训练运行的问题，显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理在自动化ML工程中严重依赖重复的完整训练运行来评估候选解决方案，导致计算开销大、搜索空间扩展性有限、迭代周期慢。

Method: 采用多代理系统架构，包含三个专门代理：编排代理使用MCTS启发式算法协调搜索过程，生成代理迭代生成和改进候选架构，评估代理执行代理训练运行并生成优化的代理函数。

Result: 在MLE-Bench上的实验表明，ArchPilot优于AIDE和ML-Master等最先进基线方法，验证了多代理系统的有效性。

Conclusion: ArchPilot通过多代理协作能够优先考虑高潜力候选方案，最小化对昂贵完整训练运行的依赖，在有限预算下实现高效的ML工程。

Abstract: Recent LLM-based agents have demonstrated strong capabilities in automated ML
engineering. However, they heavily rely on repeated full training runs to
evaluate candidate solutions, resulting in significant computational overhead,
limited scalability to large search spaces, and slow iteration cycles. To
address these challenges, we introduce ArchPilot, a multi-agent system that
integrates architecture generation, proxy-based evaluation, and adaptive search
into a unified framework. ArchPilot consists of three specialized agents: an
orchestration agent that coordinates the search process using a Monte Carlo
Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and
manages memory of previous candidates; a generation agent that iteratively
generates, improves, and debugs candidate architectures; and an evaluation
agent that executes proxy training runs, generates and optimizes proxy
functions, and aggregates the proxy scores into a fidelity-aware performance
metric. This multi-agent collaboration allows ArchPilot to prioritize
high-potential candidates with minimal reliance on expensive full training
runs, facilitating efficient ML engineering under limited budgets. Experiments
on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE
and ML-Master, validating the effectiveness of our multi-agent system.

</details>


### [56] [Detecting Silent Failures in Multi-Agentic AI Trajectories](https://arxiv.org/abs/2511.04032)
*Divya Pathak,Harshit Kumar,Anuska Roy,Felix George,Mudit Verma,Pratibha Moogi*

Main category: cs.AI

TL;DR: 该论文提出了多智能体AI系统中的异常检测任务，构建了两个包含4,275和894条轨迹的数据集，并比较了监督和半监督方法在检测智能体漂移、循环和输出细节缺失等故障方面的性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统具有非确定性且容易出现难以检测的静默故障，如漂移、循环和输出细节缺失，需要系统化的异常检测方法来识别这些故障。

Method: 开发了数据集构建流程来捕捉用户行为、智能体非确定性和LLM变化，构建了两个基准数据集，并评估了XGBoost（监督）和SVDD（半监督）等异常检测方法。

Result: 监督方法（XGBoost）和半监督方法（SVDD）表现相当，分别达到98%和96%的准确率，表明两种方法都能有效检测多智能体系统中的异常。

Conclusion: 这是首个对多智能体AI系统进行异常检测的系统性研究，提供了数据集、基准测试和见解，为未来研究提供了指导。

Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are
inherently non-deterministic and prone to silent failures such as drift,
cycles, and missing details in outputs, which are difficult to detect. We
introduce the task of anomaly detection in agentic trajectories to identify
these failures and present a dataset curation pipeline that captures user
behavior, agent non-determinism, and LLM variation. Using this pipeline, we
curate and label two benchmark datasets comprising \textbf{4,275 and 894}
trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection
methods on these datasets, we show that supervised (XGBoost) and
semi-supervised (SVDD) approaches perform comparably, achieving accuracies up
to 98% and 96%, respectively. This work provides the first systematic study of
anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,
and insights to guide future research.

</details>


### [57] [Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models](https://arxiv.org/abs/2511.04053)
*Hirohane Takagi,Gouki Minegishi,Shota Kizawa,Issey Sukeda,Hitomi Yanaka*

Main category: cs.AI

TL;DR: 该论文研究了LLMs中数值推理错误的表示机制，发现LLMs编码真实世界数值相关性但会系统性放大它们，无关上下文会诱导一致的幅度表示偏移。


<details>
  <summary>Details</summary>
Motivation: 尽管行为研究已记录LLMs中的数值推理错误，但底层表示机制仍不清楚。作者假设数值属性占据共享潜在子空间，旨在探究LLMs如何内部整合单个实体的多个数值属性，以及无关数值上下文如何干扰这些表示及其下游输出。

Method: 结合线性探测与偏相关分析，以及在不同规模模型上进行基于提示的脆弱性测试。

Result: 结果显示LLMs编码真实世界数值相关性但倾向于系统性放大它们。无关上下文诱导一致的幅度表示偏移，下游影响因模型规模而异。

Conclusion: 这些发现揭示了LLM决策中的脆弱性，为在多属性纠缠下实现更公平、表示感知的控制奠定了基础。

Abstract: Although behavioral studies have documented numerical reasoning errors in
large language models (LLMs), the underlying representational mechanisms remain
unclear. We hypothesize that numerical attributes occupy shared latent
subspaces and investigate two questions:(1) How do LLMs internally integrate
multiple numerical attributes of a single entity? (2)How does irrelevant
numerical context perturb these representations and their downstream outputs?
To address these questions, we combine linear probing with partial correlation
analysis and prompt-based vulnerability tests across models of varying sizes.
Our results show that LLMs encode real-world numerical correlations but tend to
systematically amplify them. Moreover, irrelevant context induces consistent
shifts in magnitude representations, with downstream effects that vary by model
size. These findings reveal a vulnerability in LLM decision-making and lay the
groundwork for fairer, representation-aware control under multi-attribute
entanglement.

</details>


### [58] [Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://arxiv.org/abs/2511.04076)
*Hao Li,Haotian Chen,Ruoyuan Gong,Juanjuan Wang,Hao Jiang*

Main category: cs.AI

TL;DR: 提出了Agentmandering框架，将选区重划重新构想为两个代表对立政治利益的智能体之间的回合制谈判，通过LLM智能体将战略互动嵌入到选区重划过程中。


<details>
  <summary>Details</summary>
Motivation: 现有的计算方法主要生成大量法律上有效的选区划分方案，但忽略了选择过程中的战略动态，这为党派行为者挑选政治上有利的地图创造了机会。

Method: 基于博弈论思想，特别是"选择并冻结"协议，让两个代表对立政治利益的LLM智能体轮流从候选地图中选择和冻结选区，通过受限且可解释的选择逐步划分州。

Result: 在2020年美国人口普查数据上的评估显示，Agentmandering显著减少了党派偏见和不公平性，同时比标准基线实现了2到3个数量级的更低方差。

Conclusion: 该方法在公平性和稳定性方面表现出色，特别是在摇摆州场景中。

Abstract: Redistricting plays a central role in shaping how votes are translated into
political power. While existing computational methods primarily aim to generate
large ensembles of legally valid districting plans, they often neglect the
strategic dynamics involved in the selection process. This oversight creates
opportunities for partisan actors to cherry-pick maps that, while technically
compliant, are politically advantageous. Simply satisfying formal constraints
does not ensure fairness when the selection process itself can be manipulated.
We propose \textbf{Agentmandering}, a framework that reimagines redistricting
as a turn-based negotiation between two agents representing opposing political
interests. Drawing inspiration from game-theoretic ideas, particularly the
\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction
into the redistricting process via large language model (LLM) agents. Agents
alternate between selecting and freezing districts from a small set of
candidate maps, gradually partitioning the state through constrained and
interpretable choices. Evaluation on post-2020 U.S. Census data across all
states shows that Agentmandering significantly reduces partisan bias and
unfairness, while achieving 2 to 3 orders of magnitude lower variance than
standard baselines. These results demonstrate both fairness and stability,
especially in swing-state scenarios. Our code is available at
https://github.com/Lihaogx/AgentMandering.

</details>


### [59] [When Empowerment Disempowers](https://arxiv.org/abs/2511.04177)
*Claire Yang,Maya Cakmak,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 本文介绍了多人类环境中的赋权问题，发现专注于单人类赋权的AI助手可能会削弱其他人类的环境影响力，这种现象被称为"去赋权"。


<details>
  <summary>Details</summary>
Motivation: 研究多人类环境中赋权作为AI助手通用目标的有效性，因为现有研究假设AI助手只服务单个人类，而现实环境如家庭和医院涉及多个人类。

Method: 开发了开源多人类网格世界测试套件Disempower-Grid，通过实验验证专注于单人类赋权的强化学习助手对其他人类环境影响和奖励的影响。

Result: 实验表明，优化单人类赋权的助手会显著降低其他人类的环境影响力和奖励，即产生去赋权现象。联合赋权方法可以缓解去赋权，但会牺牲用户的奖励。

Conclusion: AI对齐社区面临更广泛的挑战：在单智能体设置中看似对齐的目标无关目标，在多智能体环境中可能变得不对齐。

Abstract: Empowerment, a measure of an agent's ability to control its environment, has
been proposed as a universal goal-agnostic objective for motivating assistive
behavior in AI agents. While multi-human settings like homes and hospitals are
promising for AI assistance, prior work on empowerment-based assistance assumes
that the agent assists one human in isolation. We introduce an open source
multi-human gridworld test suite Disempower-Grid. Using Disempower-Grid, we
empirically show that assistive RL agents optimizing for one human's
empowerment can significantly reduce another human's environmental influence
and rewards - a phenomenon we formalize as disempowerment. We characterize when
disempowerment occurs in these environments and show that joint empowerment
mitigates disempowerment at the cost of the user's reward. Our work reveals a
broader challenge for the AI alignment community: goal-agnostic objectives that
seem aligned in single-agent settings can become misaligned in multi-agent
contexts.

</details>


### [60] [Shared Spatial Memory Through Predictive Coding](https://arxiv.org/abs/2511.04235)
*Zhengru Fang,Yu Guo,Jingjing Wang,Yuang Zhang,Haonan An,Yinhai Wang,Yuguang Fang*

Main category: cs.AI

TL;DR: 提出了一种多智能体预测编码框架，通过最小化相互不确定性来协调智能体行为，在带宽受限条件下表现出卓越的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中由于部分可观测性和有限带宽导致的空间记忆共享和协调失败问题。

Method: 基于信息瓶颈目标的多智能体预测编码框架，结合网格细胞状内部空间编码和分层强化学习策略，自发形成带宽高效的通信机制。

Result: 在Memory-Maze基准测试中，当带宽从128位/步降至4位/步时，成功率从73.5%优雅下降至64.4%，而全广播基线从67.6%崩溃至28.6%。

Conclusion: 为复杂社会表征如何从统一的预测驱动中涌现提供了理论原则和生物学合理的基础，实现了社会集体智能。

Abstract: Sharing and reconstructing a consistent spatial memory is a critical
challenge in multi-agent systems, where partial observability and limited
bandwidth often lead to catastrophic failures in coordination. We introduce a
multi-agent predictive coding framework that formulate coordination as the
minimization of mutual uncertainty among agents. Instantiated as an information
bottleneck objective, it prompts agents to learn not only who and what to
communicate but also when. At the foundation of this framework lies a
grid-cell-like metric as internal spatial coding for self-localization,
emerging spontaneously from self-supervised motion prediction. Building upon
this internal spatial code, agents gradually develop a bandwidth-efficient
communication mechanism and specialized neural populations that encode
partners' locations: an artificial analogue of hippocampal social place cells
(SPCs). These social representations are further enacted by a hierarchical
reinforcement learning policy that actively explores to reduce joint
uncertainty. On the Memory-Maze benchmark, our approach shows exceptional
resilience to bandwidth constraints: success degrades gracefully from 73.5% to
64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast
baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically
principled and biologically plausible basis for how complex social
representations emerge from a unified predictive drive, leading to social
collective intelligence.

</details>


### [61] [RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization](https://arxiv.org/abs/2511.04285)
*Zeng Zhiyuan,Jiashuo Liu,Zhangyue Yin,Ge Zhang,Wenhao Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: RLoop是一个基于迭代策略初始化的自改进框架，通过探索-利用循环将瞬时策略变化转化为稳健性能提升，解决RL训练中的过拟合和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在可验证奖励训练中存在过拟合问题，模型获得训练奖励但失去泛化能力，这由策略过度专业化和训练期间生成的不同解决方案的灾难性遗忘驱动。

Method: RLoop框架：首先使用RL从给定策略探索解空间，然后过滤成功轨迹创建专家数据集，通过拒绝采样微调(RFT)来改进初始策略，为下一次迭代创建更好的起点。

Result: RLoop显著改善泛化能力，平均准确率提升9%，pass@32提升超过15%，相比普通RL方法。

Conclusion: RLoop通过迭代重新初始化的探索-利用循环有效缓解遗忘问题，将瞬时策略变化转化为稳健性能增益。

Abstract: While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for
training large reasoning models, its training dynamics harbor a critical
challenge: RL overfitting, where models gain training rewards but lose
generalization. Our analysis reveals this is driven by policy
over-specialization and catastrophic forgetting of diverse solutions generated
during training. Standard optimization discards this valuable inter-step policy
diversity. To address this, we introduce RLoop, a self-improving framework
built on iterative policy initialization. RLoop transforms the standard
training process into a virtuous cycle: it first uses RL to explore the
solution space from a given policy, then filters the successful trajectories to
create an expert dataset. This dataset is used via Rejection-sampling
Fine-Tuning (RFT) to refine the initial policy, creating a superior starting
point for the next iteration. This loop of exploration and exploitation via
iterative re-initialization effectively converts transient policy variations
into robust performance gains. Our experiments show RLoop mitigates forgetting
and substantially improves generalization, boosting average accuracy by 9% and
pass@32 by over 15% compared to vanilla RL.

</details>


### [62] [GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents](https://arxiv.org/abs/2511.04307)
*Jian Mu,Chaoyun Zhang,Chiming Ni,Lu Wang,Bo Qiao,Kartik Mathur,Qianhui Wu,Yuhang Xie,Xiaojun Ma,Mengyu Zhou,Si Qin,Liqun Li,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: GUI-360°是一个大规模数据集和基准套件，用于推进计算机使用代理（CUAs）的发展，包含超过120万执行步骤，支持GUI定位、屏幕解析和动作预测三个核心任务。


<details>
  <summary>Details</summary>
Motivation: 解决计算机使用代理面临的三个关键挑战：真实世界CUA任务稀缺、多模态轨迹自动收集和标注流程缺乏、以及缺乏统一评估GUI定位、屏幕解析和动作预测的基准。

Method: 采用LLM增强的自动化流程，包括查询来源、环境模板构建、任务实例化、批量执行和LLM驱动的质量过滤，在Windows办公应用中收集数千条轨迹数据。

Result: 基准测试显示现有视觉-语言模型在GUI定位和动作预测方面存在显著不足，监督微调和强化学习带来显著改进但未达到人类水平可靠性。

Conclusion: GUI-360°数据集和代码已公开发布，旨在促进可重复研究并加速稳健桌面CUAs的发展。

Abstract: We introduce GUI-360$^\circ$, a large-scale, comprehensive dataset and
benchmark suite designed to advance computer-using agents (CUAs). CUAs present
unique challenges and is constrained by three persistent gaps: a scarcity of
real-world CUA tasks, the lack of automated collection-and-annotation pipelines
for multi-modal trajectories, and the absence of a unified benchmark that
jointly evaluates GUI grounding, screen parsing, and action prediction.
  GUI-360$^\circ$ addresses these gaps with an LLM-augmented, largely automated
pipeline for query sourcing, environment-template construction, task
instantiation, batched execution, and LLM-driven quality filtering. The
released corpus contains over 1.2M executed action steps across thousands of
trajectories in popular Windows office applications, and includes
full-resolution screenshots, accessibility metadata when available,
instantiated goals, intermediate reasoning traces, and both successful and
failed action trajectories. The dataset supports three canonical tasks, GUI
grounding, screen parsing, and action prediction, and a hybrid GUI+API action
space that reflects modern agent designs. Benchmarking state-of-the-art
vision--language models on GUI-360$^\circ$ reveals substantial out-of-the-box
shortcomings in grounding and action prediction; supervised fine-tuning and
reinforcement learning yield significant gains but do not close the gap to
human-level reliability. We release GUI-360$^\circ$ and accompanying code to
facilitate reproducible research and accelerate progress on robust desktop
CUAs.
  The full dataset has been made public on
https://huggingface.co/datasets/vyokky/GUI-360.

</details>


### [63] [The Peril of Preference: Why GRPO fails on Ordinal Rewards](https://arxiv.org/abs/2511.04439)
*Anisha Garg,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: CoRPO是一种新的强化学习算法，解决了GRPO在使用序数奖励时对失败轨迹错误强化的问题。它使用自适应基线确保失败解决方案不被正向强化，并在达到质量阈值后自动切换到相对偏好模式。


<details>
  <summary>Details</summary>
Motivation: GRPO的简单性使其在适应LLMs时很受欢迎，但在使用序数奖励提供部分信用时存在问题，因为其组平均基线经常对失败轨迹分配正优势，从而强化错误行为。

Method: CoRPO使用自适应基线强制执行最低质量阈值，确保失败解决方案永远不会被正向强化。一旦策略持续达到此阈值，基线自动过渡到相对偏好模式。

Result: 在代码验证任务上的实证验证表明，CoRPO表现出更稳定的收敛性和更好的域外泛化能力。

Conclusion: 这项工作代表了让LLMs通过强化学习学习真正新能力的关键步骤，通过使LLMs能够从丰富的多维反馈中学习来实现。

Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly
desirable for adapting LLMs to become experts at specific tasks. But this
simplicity also makes it ill-specified as we seek to enhance RL training with
richer, non-binary feedback. When using ordinal rewards to give partial credit,
GRPO's simplicity starts to hurt, as its group-average baseline often assigns a
positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new
formulation that solves this flaw. CoRPO uses an adaptive baseline that
enforces a minimum quality threshold, ensuring failed solutions are never
positively reinforced. Once the policy consistently meets this threshold, the
baseline automatically transitions to a relative preference mode, pushing the
model to find optimal solutions rather than just "acceptable" ones. We
empirically validate CoRPO on a code verification task, where it demonstrates
more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to
enable LLMs to learn genuinely new capabilities through reinforcement learning.
We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback
- progressing from binary to ordinal rewards in this work, and onward to
denser, per-step supervision.

</details>


### [64] [Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context](https://arxiv.org/abs/2511.04464)
*Carnot Braun,Rafael O. Jarczewski,Gabriel U. Talasso,Leandro A. Villas,Allan M. de Souza*

Main category: cs.AI

TL;DR: PAVe系统将经典路径规划算法与LLM语义推理相结合，通过多目标Dijkstra算法生成候选路线，再由LLM代理根据用户任务、偏好和规避规则进行语义评估，实现个性化车辆路由优化。


<details>
  <summary>Details</summary>
Motivation: 传统车辆路由系统只能优化单一指标（如时间或距离），缺乏对驾驶员复杂语义和动态上下文（如多步骤任务、情境约束、紧急需求）的理解和整合能力。

Method: 采用混合代理方法：多目标（时间、CO2）Dijkstra算法生成候选路线，LLM代理基于预处理的地理空间POI缓存，评估路线是否符合用户提供的任务、偏好和规避规则。

Result: 在现实城市场景基准测试中，PAVe成功将复杂用户意图转化为适当的路线修改，使用本地模型时初始路线选择准确率超过88%。

Conclusion: 将经典路由算法与基于LLM的语义推理层相结合，是创建个性化、自适应和可扩展城市移动优化解决方案的稳健有效方法。

Abstract: Traditional vehicle routing systems efficiently optimize singular metrics
like time or distance, and when considering multiple metrics, they need more
processes to optimize . However, they lack the capability to interpret and
integrate the complex, semantic, and dynamic contexts of human drivers, such as
multi-step tasks, situational constraints, or urgent needs. This paper
introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a
hybrid agentic assistant designed to augment classical pathfinding algorithms
with contextual reasoning. Our approach employs a Large Language Model (LLM)
agent that operates on a candidate set of routes generated by a multi-objective
(time, CO2) Dijkstra algorithm. The agent evaluates these options against
user-provided tasks, preferences, and avoidance rules by leveraging a
pre-processed geospatial cache of urban Points of Interest (POIs). In a
benchmark of realistic urban scenarios, PAVe successfully used complex user
intent into appropriate route modifications, achieving over 88% accuracy in its
initial route selections with a local model. We conclude that combining
classical routing algorithms with an LLM-based semantic reasoning layer is a
robust and effective approach for creating personalized, adaptive, and scalable
solutions for urban mobility optimization.

</details>


### [65] [Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis](https://arxiv.org/abs/2511.04481)
*Lars Krupp,Daniel Geißler,Vishal Banwari,Paul Lukowicz,Jakob Karolus*

Main category: cs.AI

TL;DR: 该论文首次从理论和实证角度探讨了网络代理的能源消耗和碳排放问题，发现不同网络代理设计理念对能耗有显著影响，且能耗与性能不一定成正比。


<details>
  <summary>Details</summary>
Motivation: 当前网络代理研究蓬勃发展，但其引发的可持续性问题尚未得到充分探索。作者旨在揭示网络代理的能源和碳排放成本，推动该领域的可持续性考量。

Method: 采用理论估计和实证基准测试相结合的方法，分析不同网络代理的能耗表现，并评估模型参数和过程透明度对能耗估算的影响。

Result: 研究显示不同网络代理设计理念会严重影响能耗，能耗增加不一定带来更好的结果，同时发现某些网络代理缺乏模型参数和过程透明度，限制了能耗估算的准确性。

Conclusion: 需要改变网络代理评估方式，在基准测试中加入专门的能耗指标，推动网络代理研究的可持续发展。

Abstract: Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful
agentic systems pushing the boundaries of Large Language Models (LLM). They can
autonomously interact with the internet at the user's behest, such as
navigating websites, filling search masks, and comparing price lists. Though
web agent research is thriving, induced sustainability issues remain largely
unexplored. To highlight the urgency of this issue, we provide an initial
exploration of the energy and $CO_2$ cost associated with web agents from both
a theoretical -via estimation- and an empirical perspective -by benchmarking.
Our results show how different philosophies in web agent creation can severely
impact the associated expended energy, and that more energy consumed does not
necessarily equate to better results. We highlight a lack of transparency
regarding disclosing model parameters and processes used for some web agents as
a limiting factor when estimating energy consumption. Our work contributes
towards a change in thinking of how we evaluate web agents, advocating for
dedicated metrics measuring energy consumption in benchmarks.

</details>


### [66] [Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper](https://arxiv.org/abs/2511.04583)
*Atsuyuki Miyai,Mashiro Toyooka,Takashi Otonari,Zaiying Zhao,Kiyoharu Aizawa*

Main category: cs.AI

TL;DR: 开发了Jr. AI Scientist系统，这是一个模拟学生研究流程的自主AI科学家系统，能够分析论文局限性、提出假设、实验验证并撰写论文，在评估中表现优于现有全自动系统，但仍有重要局限性。


<details>
  <summary>Details</summary>
Motivation: 理解AI科学家系统的当前能力和风险对于确保可信赖和可持续的AI驱动科学进步至关重要，同时保护学术生态系统的完整性。

Method: 开发Jr. AI Scientist系统，模拟学生研究流程：分析基线论文局限性、制定改进假设、通过严格实验验证、撰写结果论文，利用现代编码代理处理复杂多文件实现。

Result: Jr. AI Scientist生成的论文获得比现有全自动系统更高的评审分数，但作者评估和Agents4Science评审均发现重要局限性。

Conclusion: 当前AI科学家系统存在直接应用的风险和关键挑战，需要进一步研究解决已识别的各种风险。

Abstract: Understanding the current capabilities and risks of AI Scientist systems is
essential for ensuring trustworthy and sustainable AI-driven scientific
progress while preserving the integrity of the academic ecosystem. To this end,
we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system
that mimics the core research workflow of a novice student researcher: Given
the baseline paper from the human mentor, it analyzes its limitations,
formulates novel hypotheses for improvement, validates them through rigorous
experimentation, and writes a paper with the results. Unlike previous
approaches that assume full automation or operate on small-scale code, Jr. AI
Scientist follows a well-defined research workflow and leverages modern coding
agents to handle complex, multi-file implementations, leading to scientifically
valuable contributions. For evaluation, we conducted automated assessments
using AI Reviewers, author-led evaluations, and submissions to Agents4Science,
a venue dedicated to AI-driven scientific contributions. The findings
demonstrate that Jr. AI Scientist generates papers receiving higher review
scores than existing fully automated systems. Nevertheless, we identify
important limitations from both the author evaluation and the Agents4Science
reviews, indicating the potential risks of directly applying current AI
Scientist systems and key challenges for future research. Finally, we
comprehensively report various risks identified during development. We hope
these insights will deepen understanding of current progress and risks in AI
Scientist development.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [67] [PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI](https://arxiv.org/abs/2511.03934)
*Athma Narayanan,Mahesh Subedar,Omesh Tickoo*

Main category: cs.SE

TL;DR: 提出了一个多代理系统，结合专用LLM和硬件仿真工具，无需人工干预完成寄存器传输级(RTL)生成任务。核心是渐进式错误反馈系统(PEFA)，通过迭代错误反馈逐步增加方法复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决复杂RTL代码生成的自动化问题，通过多代理协作和自我纠正机制提高生成代码的质量和正确性。

Method: 使用多代理流程，结合专用LLM和硬件仿真工具，采用渐进式错误反馈系统(PEFA)进行自我纠正，生成包含编译检查、功能正确性和可综合构造的RTL代码。

Result: 在两个开源自然语言到RTL数据集上验证，使用开源和闭源LLM均表现出色，缩小了它们之间的性能差距，在通过率和token效率方面达到最先进水平。

Conclusion: 该方法为RTL代码生成设立了新的基准，在保持token效率的同时提供了最先进的通过率，证明了自适应代码生成方法的有效性。

Abstract: We present an agentic flow consisting of multiple agents that combine
specialized LLMs and hardware simulation tools to collaboratively complete the
complex task of Register Transfer Level (RTL) generation without human
intervention. A key feature of the proposed flow is the progressive error
feedback system of agents (PEFA), a self-correcting mechanism that leverages
iterative error feedback to progressively increase the complexity of the
approach. The generated RTL includes checks for compilation, functional
correctness, and synthesizable constructs. To validate this adaptive approach
to code generation, benchmarking is performed using two opensource natural
language-to-RTL datasets. We demonstrate the benefits of the proposed approach
implemented on an open source agentic framework, using both open- and
closed-source LLMs, effectively bridging the performance gap between them.
Compared to previously published methods, our approach sets a new benchmark,
providing state-of-the-art pass rates while being efficient in token counts.

</details>


### [68] [PSD2Code: Automated Front-End Code Generation from Design Files via Multimodal Large Language Models](https://arxiv.org/abs/2511.04012)
*Yongxi Chen,Lei Chen*

Main category: cs.SE

TL;DR: PSD2Code是一个多模态设计到代码生成方法，通过PSD文件解析和资源对齐生成生产就绪的React+SCSS代码，显著提升了代码相似性、视觉保真度和生产就绪度。


<details>
  <summary>Details</summary>
Motivation: 现有的设计到代码生成方法存在结构不一致、资源错位和生产就绪度有限的问题，需要一种能够生成工业级前端代码的解决方案。

Method: 采用ParseAlignGenerate流水线，从PSD文件中提取层次结构、图层属性和元数据，使用基于约束的对齐策略确保生成元素与设计资源的一致性，并通过结构化提示构建增强可控性和代码质量。

Result: 综合评估显示在代码相似性、视觉保真度和生产就绪度等多个指标上显著优于现有方法，且在不同大语言模型上表现出强模型独立性。

Conclusion: 将结构化设计信息与多模态大语言模型集成对于工业级代码生成具有有效性，标志着设计驱动自动化前端开发的重要进展。

Abstract: Design-to-code generation has emerged as a promising approach to bridge the
gap between design prototypes and deployable frontend code. However, existing
methods often suffer from structural inconsistencies, asset misalignment, and
limited production readiness. This paper presents PSD2Code, a novel multi-modal
approach that leverages PSD file parsing and asset alignment to generate
production-ready React+SCSS code. Our method introduces a ParseAlignGenerate
pipeline that extracts hierarchical structures, layer properties, and metadata
from PSD files, providing large language models with precise spatial
relationships and semantic groupings for frontend code generation. The system
employs a constraint-based alignment strategy that ensures consistency between
generated elements and design resources, while a structured prompt construction
enhances controllability and code quality. Comprehensive evaluation
demonstrates significant improvements over existing methods across multiple
metrics including code similarity, visual fidelity, and production readiness.
The method exhibits strong model independence across different large language
models, validating the effectiveness of integrating structured design
information with multimodal large language models for industrial-grade code
generation, marking an important step toward design-driven automated frontend
development.

</details>


### [69] [Specification-Guided Vulnerability Detection with Large Language Models](https://arxiv.org/abs/2511.04014)
*Hao Zhu,Jia Li,Cuiyun Gao,Jiaru Qian,Yihong Dong,Huanyu Liu,Lecheng Wang,Ziliang Wang,Xiaolong Hu,Ge Li*

Main category: cs.SE

TL;DR: VulInstruct是一个基于安全规范的漏洞检测方法，通过从历史漏洞中提取安全规范知识，帮助LLMs理解代码的安全行为期望，显著提升了漏洞检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在漏洞检测方面表现有限，主要因为缺乏对安全规范的理解——即代码应该如何行为才能保持安全。当代码行为与这些期望不符时，就形成了潜在漏洞，但这类知识在训练数据中很少显式存在。

Method: VulInstruct从两个角度构建规范知识库：(1)从跨项目的高质量补丁中提取通用规范，捕捉基本安全行为；(2)从特定仓库中重复的违规行为中提取领域特定规范。通过检索相关历史案例和规范，使LLMs能够基于期望的安全行为进行推理。

Result: 在PrimeVul数据集上，VulInstruct达到45.0% F1分数（提升32.7%）和37.7%召回率（提升50.8%），独特检测到24.3%的漏洞——比任何基线方法多2.4倍。在成对评估中实现32.3%的相对提升，并发现了一个之前未知的高严重性漏洞(CVE-2025-56538)。

Conclusion: VulInstruct通过系统性地利用安全规范知识，显著提升了LLMs在漏洞检测方面的能力，证明了规范引导方法在实际漏洞发现中的实用价值。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
understanding tasks. However, they demonstrate limited performance in
vulnerability detection and struggle to distinguish vulnerable code from
patched code. We argue that LLMs lack understanding of security specifications
-- the expectations about how code should behave to remain safe. When code
behavior differs from these expectations, it becomes a potential vulnerability.
However, such knowledge is rarely explicit in training data, leaving models
unable to reason about security flaws. We propose VulInstruct, a
specification-guided approach that systematically extracts security
specifications from historical vulnerabilities to detect new ones. VulInstruct
constructs a specification knowledge base from two perspectives: (i) General
specifications from high-quality patches across projects, capturing fundamental
safe behaviors; and (ii) Domain-specific specifications from repeated
violations in particular repositories relevant to the target code. VulInstruct
retrieves relevant past cases and specifications, enabling LLMs to reason about
expected safe behaviors rather than relying on surface patterns. We evaluate
VulInstruct under strict criteria requiring both correct predictions and valid
reasoning. On PrimeVul, VulInstruct achieves 45.0% F1-score (32.7% improvement)
and 37.7% recall (50.8% improvement) compared to baselines, while uniquely
detecting 24.3% of vulnerabilities -- 2.4x more than any baseline. In pair-wise
evaluation, VulInstruct achieves 32.3% relative improvement. VulInstruct also
discovered a previously unknown high-severity vulnerability (CVE-2025-56538) in
production code, demonstrating practical value for real-world vulnerability
discovery. All code and supplementary materials are available at
https://github.com/zhuhaopku/VulInstruct-temp.

</details>


### [70] [Benchmarking and Studying the LLM-based Agent System in End-to-End Software Development](https://arxiv.org/abs/2511.04064)
*Zhengran Zeng,Yixin Li,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: 提出了E2EDevBench基准测试和混合评估框架，对三种代表性代理架构进行实证研究，发现当前最先进代理只能满足约50%需求，成功关键取决于任务分解和协作的架构策略。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自主代理评估存在基准过于简单、不同架构难以公平比较的问题，需要更现实的评估方法和基准。

Method: 构建E2EDevBench基准测试，提出结合测试用例功能评估和细粒度LLM需求验证的混合评估框架，在统一基础上实现三种代理架构进行对比研究。

Result: 最先进代理只能满足约50%需求，成功关键取决于任务分解和协作的架构策略，主要瓶颈是需求遗漏和自验证不足。

Conclusion: 为社区提供了更现实的基准、全面评估框架，揭示了软件开发代理当前能力和核心挑战，指导未来研究增强需求理解和规划能力。

Abstract: The development of LLM-based autonomous agents for end-to-end software
development represents a significant paradigm shift in software engineering.
However, the scientific evaluation of these systems is hampered by significant
challenges, including overly simplistic benchmarks and the difficulty of
conducting fair comparisons between different agent architectures due to
confounding implementation variables. To address these limitations, we first
construct a challenging and dynamically curated E2EDevBench to simulate
realistic development scenarios. Second, we propose a hybrid evaluation
framework that combines test-case-based functional assessment with
fine-grained, LLM-based requirement verification. Using this framework, we
conduct a controlled empirical study on three representative agent
architectures implemented upon a unified foundation to isolate the impact of
workflow design. Our findings reveal that state-of-the-art agents can fulfill
approximately 50\% of requirements on \bench{}, but their success is critically
dependent on the architectural strategy for task decomposition and
collaboration. Furthermore, our analysis indicates that the primary bottleneck
is the omission of requirements and inadequate self-verification. This work
provides the community with a more realistic benchmark, a comprehensive
evaluation framework, and crucial insights into the current capabilities and
core challenges of software development agents, guiding future research toward
enhancing requirement comprehension and planning.

</details>


### [71] [How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks](https://arxiv.org/abs/2511.04115)
*Ruksit Rojpaisarnkit,Youmei Fan,Kenichi Matsumoto,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 论文研究了英语语言能力对LLM生成代码质量的影响，发现高级英语提示能产生更正确的代码，即使不考虑提示技术本身。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型在软件工程中的广泛应用，自然语言提示成为开发者与LLM的关键接口。虽然已有研究关注提示结构，但自然语言能力这一影响代码质量的因素尚未充分探索。

Method: 使用HumanEval数据集，系统地将164个编程任务的英语提示从基础到高级进行变化，测量生成的代码能力和正确性。

Result: LLM默认使用中级(B2)自然语言水平。虽然对代码能力的影响因模型而异，但高级英语提示在所有模型中都能产生更正确的代码。

Conclusion: 自然语言能力是控制代码生成的关键因素，帮助开发者定制AI输出并提高解决方案的可靠性。

Abstract: With the widespread adoption of Foundation Model (FM)-powered tools in
software engineering, the natural language prompt has become a critical
interface between developers and Large Language Models (LLMs). While much
research has focused on prompt structure, the natural language proficiency is
an underexplored factor that can influence the quality of generated code. This
paper investigates whether the English language proficiency itself independent
of the prompting technique affects the proficiency and correctness of code
generated by LLMs. Using the HumanEval dataset, we systematically varied the
English proficiency of prompts from basic to advanced for 164 programming tasks
and measured the resulting code proficiency and correctness. Our findings show
that LLMs default to an intermediate (B2) natural language level. While the
effect on the resulting code proficiency was model-dependent, we found that
higher-proficiency prompts consistently yielded more correct code across all
models. These results demonstrate that natural language proficiency is a key
lever for controlling code generation, helping developers tailor AI output and
improve the reliability of solutions.

</details>


### [72] [Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks](https://arxiv.org/abs/2511.04355)
*Amir Molzam Sharifloo,Maedeh Heydari,Parsa Kazerooni,Daniel Maninger,Mira Mezini*

Main category: cs.SE

TL;DR: 该研究分析了四个流行代码生成基准测试中LLM最可能失败的任务，识别出LLM的四种常见弱点模式和导致失败的基准任务复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试和排行榜仅提供LLM的定量排名，但无法揭示LLM持续失败的任务类型，这些信息对于理解当前局限性和开发更强大模型至关重要。

Method: 研究检查了四个流行基准测试中的代码生成任务，识别主要LLM最可能失败的任务；调查解决方案代码的静态复杂性是否导致失败；系统检查114个LLM持续困难的任务。

Result: 分析揭示了LLM的四种重复出现的弱点模式，以及基准任务中最常导致失败的常见复杂性问题。

Conclusion: 该研究填补了基准测试的空白，提供了对LLM代码生成能力局限性的深入理解，为开发更强大模型提供了指导。

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation, and the race to improve their performance has become a central
focus of AI research. Benchmarks and leaderboards are increasingly popular,
offering quantitative rankings of LLMs. However, they provide limited insight
into the tasks that LLMs consistently fail to solve - information that is
crucial for understanding current limitations and guiding the development of
more capable models. To address this gap, we examined code generation tasks
across four popular benchmarks, identifying those that major LLMs are most
likely to fail. To understand the causes of these failures, we investigated
whether the static complexity of solution code contributes to them, followed by
a systematic inspection of 114 tasks that LLMs consistently struggled with. Our
analysis revealed four recurring patterns of weaknesses in LLMs, as well as
common complications within benchmark tasks that most often lead to failure.

</details>


### [73] [Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development](https://arxiv.org/abs/2511.04427)
*Hao He,Courtney Miller,Shyam Agarwal,Christian Kästner,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: 本文通过因果推断方法评估了Cursor LLM助手对软件开发的影响，发现其能显著但短暂提升开发速度，同时导致代码复杂度和静态分析警告的持续增加，最终这些质量问题会反过来降低长期开发效率。


<details>
  <summary>Details</summary>
Motivation: LLM代理在软件开发中应用广泛，但缺乏关于其对开发效率和软件质量影响的实证证据。本文旨在量化评估流行LLM助手Cursor的因果效应。

Method: 采用差分法设计，比较使用Cursor的GitHub项目与匹配的未使用控制组，并使用面板广义矩估计分析长期影响机制。

Result: Cursor采用导致项目级开发速度显著但短暂提升，同时静态分析警告和代码复杂度显著持续增加。质量问题的增加是导致长期速度下降的主要因素。

Conclusion: LLM助手虽然能短期提升开发效率，但可能以牺牲代码质量为代价，这对软件工程实践者、LLM助手设计者和研究者都有重要启示。

Abstract: Large language models (LLMs) have demonstrated the promise to revolutionize
the field of software engineering. Among other things, LLM agents are rapidly
gaining momentum in their application to software development, with
practitioners claiming a multifold productivity increase after adoption. Yet,
empirical evidence is lacking around these claims. In this paper, we estimate
the causal effect of adopting a widely popular LLM agent assistant, namely
Cursor, on development velocity and software quality. The estimation is enabled
by a state-of-the-art difference-in-differences design comparing
Cursor-adopting GitHub projects with a matched control group of similar GitHub
projects that do not use Cursor. We find that the adoption of Cursor leads to a
significant, large, but transient increase in project-level development
velocity, along with a significant and persistent increase in static analysis
warnings and code complexity. Further panel generalized method of moments
estimation reveals that the increase in static analysis warnings and code
complexity acts as a major factor causing long-term velocity slowdown. Our
study carries implications for software engineering practitioners, LLM agent
assistant designers, and researchers.

</details>


### [74] [EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits](https://arxiv.org/abs/2511.04486)
*Wayne Chi,Valerie Chen,Ryan Shar,Aditya Mittal,Jenny Liang,Wei-Lin Chiang,Anastasios Nikolas Angelopoulos,Ion Stoica,Graham Neubig,Ameet Talwalkar,Chris Donahue*

Main category: cs.SE

TL;DR: EDIT-Bench是一个基于真实世界使用场景的代码编辑基准测试，包含545个问题，涵盖多种自然语言和编程语言，评估LLM在理解代码上下文、高亮代码和光标位置的基础上进行代码编辑的能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏直接评估LLM代码编辑能力的基准测试，现有数据集往往依赖人工来源，需要基于真实世界使用场景的评估标准。

Method: 收集真实世界的用户指令和代码上下文构建EDIT-Bench基准测试，包含多种使用场景和上下文依赖问题，评估40个不同LLM的性能表现。

Result: EDIT-Bench是一个具有挑战性的基准测试，只有5个模型得分超过60%，模型性能在不同用户指令类别间差异显著，上下文信息量对任务成功率影响可达11%。

Conclusion: 代码编辑评估需要基于真实上下文，EDIT-Bench为评估LLM代码编辑能力提供了重要基准，上下文信息对任务成功至关重要。

Abstract: Instructed code editing, where LLMs directly modify a developer's existing
code based on a user instruction, is becoming a widely used interaction mode in
AI coding assistants. However, few benchmarks directly evaluate this capability
and current datasets often rely on artificial sources. We introduce EDIT-Bench,
a benchmark for evaluating LLM code editing capabilities grounded in real-world
usage, i.e., user instructions and code contexts collected in the wild.
EDIT-Bench comprises of 545 problems, multiple natural and programming
languages, and a diverse set of real-world use cases, ranging from resolving
errors to adding features. EDIT-Bench introduces context-dependent problems
that require the model to understand code context, highlighted code, and cursor
position in addition to the user instruction. We evaluate 40 diverse LLMs and
observe that EDIT-Bench is a challenging set of problems where only 5 models
score over 60%. We find that model performance varies across different
categories of user instructions. Further, we find that varying levels of
contextual information greatly affect task success rate, with performance
varying up to 11%, indicating the importance of evaluating with realistic
context.

</details>
