{"id": "2511.05626", "categories": ["cs.SE", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.05626", "abs": "https://arxiv.org/abs/2511.05626", "authors": ["Caetano Melone", "Daniel Nichols", "Konstantinos Parasyris", "Todd Gamblin", "Harshitha Menon"], "title": "LLMs as Packagers of HPC Software", "comment": null, "summary": "High performance computing (HPC) software ecosystems are inherently heterogeneous, comprising scientific applications that depend on hundreds of external packages, each with distinct build systems, options, and dependency constraints. Tools such as Spack automate dependency resolution and environment management, but their effectiveness relies on manually written build recipes. As these ecosystems grow, maintaining existing specifications and creating new ones becomes increasingly labor-intensive. While large language models (LLMs) have shown promise in code generation, automatically producing correct and maintainable Spack recipes remains a significant challenge. We present a systematic analysis of how LLMs and context-augmentation methods can assist in the generation of Spack recipes. To this end, we introduce SpackIt, an end-to-end framework that combines repository analysis, retrieval of relevant examples, and iterative refinement through diagnostic feedback. We apply SpackIt to a representative subset of 308 open-source HPC packages to assess its effectiveness and limitations. Our results show that SpackIt increases installation success from 20% in a zero-shot setting to over 80% in its best configuration, demonstrating the value of retrieval and structured feedback for reliable package synthesis.", "AI": {"tldr": "SpackIt\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4ed3\u5e93\u5206\u6790\u3001\u76f8\u5173\u793a\u4f8b\u68c0\u7d22\u548c\u57fa\u4e8e\u8bca\u65ad\u53cd\u9988\u7684\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u751f\u6210Spack\u8f6f\u4ef6\u5305\u914d\u65b9\u7684\u6210\u529f\u7387\uff0c\u4ece\u96f6\u6837\u672c\u768420%\u63d0\u9ad8\u523080%\u4ee5\u4e0a\u3002", "motivation": "HPC\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u65e5\u76ca\u590d\u6742\uff0c\u7ef4\u62a4\u548c\u521b\u5efaSpack\u8f6f\u4ef6\u5305\u914d\u65b9\u53d8\u5f97\u6108\u53d1\u8017\u65f6\u8d39\u529b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u8f85\u52a9\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86SpackIt\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u62ec\u4ed3\u5e93\u5206\u6790\u3001\u76f8\u5173\u793a\u4f8b\u68c0\u7d22\u548c\u57fa\u4e8e\u8bca\u65ad\u53cd\u9988\u7684\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5728308\u4e2a\u5f00\u6e90HPC\u8f6f\u4ef6\u5305\u4e0a\u7684\u6d4b\u8bd5\u663e\u793a\uff0cSpackIt\u5c06\u5b89\u88c5\u6210\u529f\u7387\u4ece\u96f6\u6837\u672c\u768420%\u63d0\u5347\u5230\u6700\u4f73\u914d\u7f6e\u4e0b\u768480%\u4ee5\u4e0a\u3002", "conclusion": "\u68c0\u7d22\u548c\u7ed3\u6784\u5316\u53cd\u9988\u5bf9\u4e8e\u53ef\u9760\u7684\u8f6f\u4ef6\u5305\u5408\u6210\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347LLM\u751f\u6210Spack\u914d\u65b9\u7684\u6548\u679c\u3002", "topic": "swe application"}}
{"id": "2511.05813", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05813", "abs": "https://arxiv.org/abs/2511.05813", "authors": ["In-on Wiratsin", "Chaiyong Ragkhitwetsagul", "Matheus Paixao", "Denis De Sousa", "Pongpop Lapvikai", "Peter Haddawy"], "title": "An Empirical Study of Java Code Improvements Based on Stack Overflow Answer Edits", "comment": null, "summary": "Suboptimal code is prevalent in software systems. Developers often write low-quality code due to factors like technical knowledge gaps, insufficient experience, time pressure, management decisions, or personal factors. Once integrated, the accumulation of this suboptimal code leads to significant maintenance costs and technical debt.\n  Developers frequently consult external knowledge bases, such as API documentation and Q&A websites like Stack Overflow (SO), to aid their programming tasks. SO's crowdsourced, collaborative nature has created a vast repository of programming knowledge. Its community-curated content is constantly evolving, with new answers posted or existing ones edited.\n  In this paper, we present an empirical study of SO Java answer edits and their application to improving code in open-source projects. We use a modified code clone search tool to analyze SO code snippets with version history and apply it to open-source Java projects. This identifies outdated or unoptimized code and suggests improved alternatives. Analyzing 140,840 Java accepted answers from SOTorrent and 10,668 GitHub Java projects, we manually categorized SO answer edits and created pull requests to open-source projects with the suggested code improvements. Our results show that 6.91% of SO Java accepted answers have more than one revision (average of 2.82). Moreover, 49.24% of the code snippets in the answer edits are applicable to open-source projects, and 11 out of 36 proposed bug fixes based on these edits were accepted by the GitHub project maintainers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5206\u6790Stack Overflow Java\u56de\u7b54\u7684\u7f16\u8f91\u5386\u53f2\uff0c\u8bc6\u522b\u5f00\u6e90\u9879\u76ee\u4e2d\u8fc7\u65f6\u6216\u672a\u4f18\u5316\u7684\u4ee3\u7801\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002\u7814\u7a76\u53d1\u73b049.24%\u7684\u4ee3\u7801\u7247\u6bb5\u53ef\u5e94\u7528\u4e8e\u5f00\u6e90\u9879\u76ee\uff0c\u4e14\u57fa\u4e8e\u8fd9\u4e9b\u7f16\u8f91\u7684bug\u4fee\u590d\u5efa\u8bae\u670930.56%\u88ab\u9879\u76ee\u7ef4\u62a4\u8005\u63a5\u53d7\u3002", "motivation": "\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u666e\u904d\u5b58\u5728\u6b21\u4f18\u4ee3\u7801\uff0c\u5bfc\u81f4\u9ad8\u6602\u7684\u7ef4\u62a4\u6210\u672c\u548c\u6280\u672f\u503a\u52a1\u3002\u5f00\u53d1\u8005\u5e38\u4f9d\u8d56Stack Overflow\u7b49\u77e5\u8bc6\u5e93\uff0c\u4f46\u5176\u5185\u5bb9\u4e0d\u65ad\u6f14\u53d8\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u8fd9\u4e9b\u7f16\u8f91\u5982\u4f55\u5e2e\u52a9\u6539\u8fdb\u5b9e\u9645\u9879\u76ee\u4ee3\u7801\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684\u4ee3\u7801\u514b\u9686\u641c\u7d22\u5de5\u5177\u5206\u6790SO Java\u56de\u7b54\u7684\u7248\u672c\u5386\u53f2\uff0c\u5e94\u7528\u4e8e10,668\u4e2aGitHub Java\u9879\u76ee\u3002\u624b\u52a8\u5206\u7c7bSO\u56de\u7b54\u7f16\u8f91\uff0c\u5e76\u521b\u5efapull\u8bf7\u6c42\u5411\u5f00\u6e90\u9879\u76ee\u5efa\u8bae\u4ee3\u7801\u6539\u8fdb\u3002", "result": "6.91%\u7684SO Java\u63a5\u53d7\u7b54\u6848\u6709\u591a\u4e2a\u4fee\u8ba2\u7248\u672c\uff08\u5e73\u57472.82\u4e2a\uff09\u300249.24%\u7684\u4ee3\u7801\u7247\u6bb5\u53ef\u5e94\u7528\u4e8e\u5f00\u6e90\u9879\u76ee\uff0c36\u4e2a\u57fa\u4e8e\u7f16\u8f91\u7684bug\u4fee\u590d\u5efa\u8bae\u4e2d\u670911\u4e2a\u88ab\u9879\u76ee\u7ef4\u62a4\u8005\u63a5\u53d7\u3002", "conclusion": "SO\u56de\u7b54\u7684\u7f16\u8f91\u5386\u53f2\u662f\u8bc6\u522b\u548c\u6539\u8fdb\u6b21\u4f18\u4ee3\u7801\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u53ef\u6709\u6548\u5e2e\u52a9\u51cf\u5c11\u5f00\u6e90\u9879\u76ee\u4e2d\u7684\u6280\u672f\u503a\u52a1\u3002", "topic": "swe application"}}
{"id": "2511.05524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05524", "abs": "https://arxiv.org/abs/2511.05524", "authors": ["Ruiying Chen"], "title": "Evidence-Bound Autonomous Research (EviBound): A Governance Framework for Eliminating False Claims", "comment": "27 pages, 11 figures, 5 tables. Reproducibility package with MLflow artifacts and Google Colab notebooks available upon publication", "summary": "LLM-based autonomous research agents report false claims: tasks marked \"complete\" despite missing artifacts, contradictory metrics, or failed executions. EviBound is an evidence-bound execution framework that eliminates false claims through dual governance gates requiring machine-checkable evidence.\n  Two complementary gates enforce evidence requirements. The pre-execution Approval Gate validates acceptance criteria schemas before code runs, catching structural violations proactively. The post-execution Verification Gate validates artifacts via MLflow API queries (with recursive path checking) and optionally validates metrics when specified by acceptance criteria. Claims propagate only when backed by a queryable run ID, required artifacts, and FINISHED status. Bounded, confidence-gated retries (typically 1-2 attempts) recover from transient failures without unbounded loops.\n  The framework was evaluated on 8 benchmark tasks spanning infrastructure validation, ML capabilities, and governance stress tests. Baseline A (Prompt-Level Only) yields 100% hallucination (8/8 claimed, 0/8 verified). Baseline B (Verification-Only) reduces hallucination to 25% (2/8 fail verification). EviBound (Dual Gates) achieves 0% hallucination: 7/8 tasks verified and 1 task correctly blocked at the approval gate, all with only approximately 8.3% execution overhead.\n  This package includes execution trajectories, MLflow run IDs for all verified tasks, and a 4-step verification protocol. Research integrity is an architectural property, achieved through governance gates rather than emergent from model scale.", "AI": {"tldr": "EviBound\u662f\u4e00\u4e2a\u8bc1\u636e\u7ed1\u5b9a\u7684\u6267\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u6cbb\u7406\u95e8\u6d88\u9664LLM\u81ea\u4e3b\u7814\u7a76\u4ee3\u7406\u7684\u9519\u8bef\u58f0\u660e\uff0c\u8981\u6c42\u673a\u5668\u53ef\u68c0\u67e5\u7684\u8bc1\u636e\u6765\u786e\u4fdd\u7814\u7a76\u5b8c\u6574\u6027\u3002", "motivation": "LLM\u81ea\u4e3b\u7814\u7a76\u4ee3\u7406\u7ecf\u5e38\u62a5\u544a\u865a\u5047\u58f0\u660e\uff0c\u5373\u4f7f\u4efb\u52a1\u5b9e\u9645\u4e0a\u672a\u5b8c\u6210\u3001\u5b58\u5728\u77db\u76fe\u6307\u6807\u6216\u6267\u884c\u5931\u8d25\u3002\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u786e\u4fdd\u7814\u7a76\u58f0\u660e\u7684\u771f\u5b9e\u6027\u3002", "method": "\u91c7\u7528\u53cc\u91cd\u6cbb\u7406\u95e8\uff1a\u9884\u6267\u884c\u6279\u51c6\u95e8\u9a8c\u8bc1\u63a5\u53d7\u6807\u51c6\u6a21\u5f0f\uff0c\u540e\u6267\u884c\u9a8c\u8bc1\u95e8\u901a\u8fc7MLflow API\u67e5\u8be2\u9a8c\u8bc1\u5de5\u4ef6\u548c\u6307\u6807\u3002\u58f0\u660e\u53ea\u6709\u5728\u6709\u53ef\u67e5\u8be2\u8fd0\u884cID\u3001\u5fc5\u9700\u5de5\u4ef6\u548cFINISHED\u72b6\u6001\u65f6\u624d\u80fd\u4f20\u64ad\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0c\u57fa\u7ebfA\uff08\u4ec5\u63d0\u793a\u7ea7\u522b\uff09\u4ea7\u751f100%\u5e7b\u89c9\uff0c\u57fa\u7ebfB\uff08\u4ec5\u9a8c\u8bc1\uff09\u51cf\u5c11\u523025%\u5e7b\u89c9\uff0c\u800cEviBound\uff08\u53cc\u91cd\u95e8\uff09\u5b9e\u73b00%\u5e7b\u89c9\uff0c\u4ec5\u589e\u52a0\u7ea68.3%\u6267\u884c\u5f00\u9500\u3002", "conclusion": "\u7814\u7a76\u5b8c\u6574\u6027\u662f\u67b6\u6784\u5c5e\u6027\uff0c\u901a\u8fc7\u6cbb\u7406\u95e8\u800c\u975e\u6a21\u578b\u89c4\u6a21\u5b9e\u73b0\u3002\u8bc1\u636e\u7ed1\u5b9a\u6267\u884c\u6846\u67b6\u80fd\u6709\u6548\u6d88\u9664\u865a\u5047\u58f0\u660e\u3002", "topic": "agent analysis"}}
{"id": "2511.05549", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.05549", "abs": "https://arxiv.org/abs/2511.05549", "authors": ["Yubo Wang", "Haoyang Li", "Fei Teng", "Lei Chen"], "title": "AGRAG: Advanced Graph-based Retrieval-Augmented Generation for LLMs", "comment": null, "summary": "Graph-based retrieval-augmented generation (Graph-based RAG) has demonstrated significant potential in enhancing Large Language Models (LLMs) with structured knowledge. However, existing methods face three critical challenges: Inaccurate Graph Construction, caused by LLM hallucination; Poor Reasoning Ability, caused by failing to generate explicit reasons telling LLM why certain chunks were selected; and Inadequate Answering, which only partially answers the query due to the inadequate LLM reasoning, making their performance lag behind NaiveRAG on certain tasks. To address these issues, we propose AGRAG, an advanced graph-based retrieval-augmented generation framework. When constructing the graph, AGRAG substitutes the widely used LLM entity extraction method with a statistics-based method, avoiding hallucination and error propagation. When retrieval, AGRAG formulates the graph reasoning procedure as the Minimum Cost Maximum Influence (MCMI) subgraph generation problem, where we try to include more nodes with high influence score, but with less involving edge cost, to make the generated reasoning paths more comprehensive. We prove this problem to be NP-hard, and propose a greedy algorithm to solve it. The MCMI subgraph generated can serve as explicit reasoning paths to tell LLM why certain chunks were retrieved, thereby making the LLM better focus on the query-related part contents of the chunks, reducing the impact of noise, and improving AGRAG's reasoning ability. Furthermore, compared with the simple tree-structured reasoning paths, our MCMI subgraph can allow more complex graph structures, such as cycles, and improve the comprehensiveness of the generated reasoning paths.", "AI": {"tldr": "AGRAG\u662f\u4e00\u4e2a\u5148\u8fdb\u7684\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u6784\u5efa\u56fe\u907f\u514dLLM\u5e7b\u89c9\uff0c\u4f7f\u7528MCMI\u5b50\u56fe\u751f\u6210\u95ee\u9898\u6765\u521b\u5efa\u66f4\u5168\u9762\u7684\u63a8\u7406\u8def\u5f84\uff0c\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u7b54\u6848\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u7684RAG\u65b9\u6cd5\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4e0d\u51c6\u786e\u7684\u56fe\u6784\u5efa\uff08LLM\u5e7b\u89c9\u5bfc\u81f4\uff09\u3001\u63a8\u7406\u80fd\u529b\u5dee\uff08\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\u8def\u5f84\uff09\u3001\u7b54\u6848\u4e0d\u5145\u5206\uff08LLM\u63a8\u7406\u4e0d\u8db3\uff09\uff0c\u5bfc\u81f4\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u6027\u80fd\u751a\u81f3\u4e0d\u5982NaiveRAG\u3002", "method": "1. \u4f7f\u7528\u57fa\u4e8e\u7edf\u8ba1\u7684\u65b9\u6cd5\u66ff\u4ee3LLM\u5b9e\u4f53\u63d0\u53d6\u6765\u6784\u5efa\u56fe\uff1b2. \u5c06\u56fe\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u6700\u5c0f\u6210\u672c\u6700\u5927\u5f71\u54cd\u529b\u5b50\u56fe\u751f\u6210\u95ee\u9898\uff1b3. \u63d0\u51fa\u8d2a\u5fc3\u7b97\u6cd5\u89e3\u51b3\u8fd9\u4e2aNP\u96be\u95ee\u9898\uff1b4. \u751f\u6210\u7684MCMI\u5b50\u56fe\u4f5c\u4e3a\u663e\u5f0f\u63a8\u7406\u8def\u5f84\u6307\u5bfcLLM\u3002", "result": "AGRAG\u80fd\u591f\u751f\u6210\u66f4\u5168\u9762\u7684\u63a8\u7406\u8def\u5f84\uff0c\u652f\u6301\u590d\u6742\u56fe\u7ed3\u6784\uff08\u5982\u5faa\u73af\uff09\uff0c\u8ba9LLM\u66f4\u597d\u5730\u5173\u6ce8\u67e5\u8be2\u76f8\u5173\u5185\u5bb9\uff0c\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\uff0c\u4ece\u800c\u63d0\u5347\u63a8\u7406\u80fd\u529b\u548c\u7b54\u6848\u8d28\u91cf\u3002", "conclusion": "AGRAG\u901a\u8fc7\u7edf\u8ba1\u56fe\u6784\u5efa\u548cMCMI\u5b50\u56fe\u751f\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u57faRAG\u65b9\u6cd5\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u68c0\u7d22\u589e\u5f3a\u6548\u679c\u3002", "topic": "agent analysis"}}
{"id": "2511.05747", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05747", "abs": "https://arxiv.org/abs/2511.05747", "authors": ["Ziqian Bi", "Kaijie Chen", "Tianyang Wang", "Junfeng Hao", "Xinyuan Song"], "title": "CoT-X: An Adaptive Framework for Cross-Model Chain-of-Thought Transfer and Optimization", "comment": "TKDD 2025", "summary": "Chain-of-Thought (CoT) reasoning enhances the problem-solving ability of large language models (LLMs) but leads to substantial inference overhead, limiting deployment in resource-constrained settings. This paper investigates efficient CoT transfer across models of different scales and architectures through an adaptive reasoning summarization framework. The proposed method compresses reasoning traces via semantic segmentation with importance scoring, budget-aware dynamic compression, and coherence reconstruction, preserving critical reasoning steps while significantly reducing token usage. Experiments on 7{,}501 medical examination questions across 10 specialties show up to 40% higher accuracy than truncation under the same token budgets. Evaluations on 64 model pairs from eight LLMs (1.5B-32B parameters, including DeepSeek-R1 and Qwen3) confirm strong cross-model transferability. Furthermore, a Gaussian Process-based Bayesian optimization module reduces evaluation cost by 84% and reveals a power-law relationship between model size and cross-domain robustness. These results demonstrate that reasoning summarization provides a practical path toward efficient CoT transfer, enabling advanced reasoning under tight computational constraints. Code will be released upon publication.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u63a8\u7406\u6458\u8981\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u3001\u91cd\u8981\u6027\u8bc4\u5206\u548c\u52a8\u6001\u538b\u7f29\u6765\u538b\u7f29\u63a8\u7406\u8f68\u8ff9\uff0c\u5728\u4fdd\u6301\u5173\u952e\u63a8\u7406\u6b65\u9aa4\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\u91cf\uff0c\u5b9e\u73b0\u8de8\u6a21\u578b\u7684\u9ad8\u6548CoT\u63a8\u7406\u8fc1\u79fb\u3002", "motivation": "CoT\u63a8\u7406\u867d\u7136\u589e\u5f3a\u4e86LLMs\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4f46\u5e26\u6765\u4e86\u663e\u8457\u7684\u63a8\u7406\u5f00\u9500\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002\u9700\u8981\u7814\u7a76\u5982\u4f55\u5728\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u63a8\u7406\u6458\u8981\u6846\u67b6\uff0c\u5305\u62ec\u8bed\u4e49\u5206\u5272\u4e0e\u91cd\u8981\u6027\u8bc4\u5206\u3001\u9884\u7b97\u611f\u77e5\u52a8\u6001\u538b\u7f29\u3001\u8fde\u8d2f\u6027\u91cd\u5efa\uff0c\u4ee5\u53ca\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6a21\u5757\u6765\u964d\u4f4e\u8bc4\u4f30\u6210\u672c\u3002", "result": "\u57287,501\u4e2a\u533b\u5b66\u8003\u8bd5\u95ee\u9898\u4e0a\uff0c\u76f8\u6bd4\u622a\u65ad\u65b9\u6cd5\u5728\u76f8\u540ctoken\u9884\u7b97\u4e0b\u51c6\u786e\u7387\u63d0\u9ad8\u8fbe40%\uff1b\u57288\u4e2aLLMs\u768464\u4e2a\u6a21\u578b\u5bf9\u4e0a\u9a8c\u8bc1\u4e86\u5f3a\u8de8\u6a21\u578b\u53ef\u8fc1\u79fb\u6027\uff1b\u8bc4\u4f30\u6210\u672c\u964d\u4f4e84%\u3002", "conclusion": "\u63a8\u7406\u6458\u8981\u4e3a\u5b9e\u73b0\u9ad8\u6548CoT\u8fc1\u79fb\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u80fd\u591f\u5728\u4e25\u683c\u8ba1\u7b97\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.05766", "categories": ["cs.AI", "cs.CL", "econ.GN"], "pdf": "https://arxiv.org/pdf/2511.05766", "abs": "https://arxiv.org/abs/2511.05766", "authors": ["Felipe Valencia-Clavijo"], "title": "Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs", "comment": null, "summary": "Large language models (LLMs) are increasingly examined as both behavioral subjects and decision systems, yet it remains unclear whether observed cognitive biases reflect surface imitation or deeper probability shifts. Anchoring bias, a classic human judgment bias, offers a critical test case. While prior work shows LLMs exhibit anchoring, most evidence relies on surface-level outputs, leaving internal mechanisms and attributional contributions unexplored. This paper advances the study of anchoring in LLMs through three contributions: (1) a log-probability-based behavioral analysis showing that anchors shift entire output distributions, with controls for training-data contamination; (2) exact Shapley-value attribution over structured prompt fields to quantify anchor influence on model log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score integrating behavioral and attributional evidence across six open-source models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and Llama-2-7B, with attribution signaling that the anchors influence reweighting. Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability, suggesting scale may modulate sensitivity. Attributional effects, however, vary across prompt designs, underscoring fragility in treating LLMs as human substitutes. The findings demonstrate that anchoring bias in LLMs is robust, measurable, and interpretable, while highlighting risks in applied domains. More broadly, the framework bridges behavioral science, LLM safety, and interpretability, offering a reproducible path for evaluating other cognitive biases in LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6982\u7387\u5206\u6790\u548c\u5f52\u56e0\u65b9\u6cd5\u7814\u7a76LLMs\u4e2d\u7684\u951a\u5b9a\u504f\u5dee\uff0c\u53d1\u73b0\u951a\u70b9\u4f1a\u6539\u53d8\u6574\u4e2a\u8f93\u51fa\u5206\u5e03\uff0c\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u654f\u611f\u6027\u4e0d\u540c\uff0c\u4e3a\u8bc4\u4f30LLMs\u8ba4\u77e5\u504f\u5dee\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u6846\u67b6\u3002", "motivation": "\u7814\u7a76LLMs\u4e2d\u89c2\u5bdf\u5230\u7684\u8ba4\u77e5\u504f\u5dee\u662f\u8868\u9762\u6a21\u4eff\u8fd8\u662f\u6df1\u5c42\u6982\u7387\u53d8\u5316\uff0c\u4ee5\u951a\u5b9a\u504f\u5dee\u4e3a\u5173\u952e\u6d4b\u8bd5\u6848\u4f8b\uff0c\u63a2\u7d22\u5185\u90e8\u673a\u5236\u548c\u5f52\u56e0\u8d21\u732e\u3002", "method": "\u4f7f\u7528\u5bf9\u6570\u6982\u7387\u884c\u4e3a\u5206\u6790\u3001\u7cbe\u786eShapley\u503c\u5f52\u56e0\u548c\u7edf\u4e00\u7684\u951a\u5b9a\u504f\u5dee\u654f\u611f\u5ea6\u8bc4\u5206\uff0c\u5728\u516d\u4e2a\u5f00\u6e90\u6a21\u578b\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "Gemma-2B\u3001Phi-2\u548cLlama-2-7B\u8868\u73b0\u51fa\u7a33\u5065\u7684\u951a\u5b9a\u6548\u5e94\uff0c\u951a\u70b9\u5f71\u54cd\u91cd\u65b0\u52a0\u6743\uff1b\u8f83\u5c0f\u6a21\u578b\u5982GPT-2\u3001Falcon-RW-1B\u548cGPT-Neo-125M\u654f\u611f\u6027\u53d8\u5316\u8f83\u5927\uff0c\u8868\u660e\u89c4\u6a21\u53ef\u80fd\u8c03\u8282\u654f\u611f\u6027\u3002", "conclusion": "LLMs\u4e2d\u7684\u951a\u5b9a\u504f\u5dee\u662f\u7a33\u5065\u3001\u53ef\u6d4b\u91cf\u548c\u53ef\u89e3\u91ca\u7684\uff0c\u4f46\u5728\u5e94\u7528\u9886\u57df\u5b58\u5728\u98ce\u9669\uff0c\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u5176\u4ed6\u8ba4\u77e5\u504f\u5dee\u63d0\u4f9b\u4e86\u6865\u6881\u3002", "topic": "agent analysis"}}
{"id": "2511.05535", "categories": ["cs.CL", "cs.DB", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.05535", "abs": "https://arxiv.org/abs/2511.05535", "authors": ["Trivikram Satharasi", "S Sitharama Iyengar"], "title": "Future of AI Models: A Computational perspective on Model collapse", "comment": "Submitted to Springer Nature. Code Available at https://github.com/t-satharasi/AI-Modal-Collapse-Code-for-Reproduction.git", "summary": "Artificial Intelligence, especially Large Language Models (LLMs), has transformed domains such as software engineering, journalism, creative writing, academia, and media (Naveed et al. 2025; arXiv:2307.06435). Diffusion models like Stable Diffusion generate high-quality images and videos from text. Evidence shows rapid expansion: 74.2% of newly published webpages now contain AI-generated material (Ryan Law 2025), 30-40% of the active web corpus is synthetic (Spennemann 2025; arXiv:2504.08755), 52% of U.S. adults use LLMs for writing, coding, or research (Staff 2025), and audits find AI involvement in 18% of financial complaints and 24% of press releases (Liang et al. 2025). The underlying neural architectures, including Transformers (Vaswani et al. 2023; arXiv:1706.03762), RNNs, LSTMs, GANs, and diffusion networks, depend on large, diverse, human-authored datasets (Shi & Iyengar 2019). As synthetic content dominates, recursive training risks eroding linguistic and semantic diversity, producing Model Collapse (Shumailov et al. 2024; arXiv:2307.15043; Dohmatob et al. 2024; arXiv:2402.07712). This study quantifies and forecasts collapse onset by examining year-wise semantic similarity in English-language Wikipedia (filtered Common Crawl) from 2013 to 2025 using Transformer embeddings and cosine similarity metrics. Results reveal a steady rise in similarity before public LLM adoption, likely driven by early RNN/LSTM translation and text-normalization pipelines, though modest due to a smaller scale. Observed fluctuations reflect irreducible linguistic diversity, variable corpus size across years, finite sampling error, and an exponential rise in similarity after the public adoption of LLM models. These findings provide a data-driven estimate of when recursive AI contamination may significantly threaten data richness and model generalization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u91cf\u5316\u4e86AI\u751f\u6210\u5185\u5bb9\u5bf9\u8bed\u8a00\u591a\u6837\u6027\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5206\u67902013-2025\u5e74\u7ef4\u57fa\u767e\u79d1\u6587\u672c\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u53d8\u5316\uff0c\u53d1\u73b0LLM\u516c\u5f00\u91c7\u7528\u540e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u8868\u660e\u9012\u5f52\u8bad\u7ec3\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5d29\u6e83\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u5185\u5bb9\u5728\u7f51\u7edc\u4e2d\u5360\u6bd4\u8fc5\u901f\u589e\u52a0\uff08\u65b0\u7f51\u987574.2%\u542bAI\u5185\u5bb9\uff0c30-40%\u7f51\u7edc\u8bed\u6599\u4e3a\u5408\u6210\u5185\u5bb9\uff09\uff0c\u9012\u5f52\u8bad\u7ec3\u53ef\u80fd\u4fb5\u8680\u8bed\u8a00\u548c\u8bed\u4e49\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u5d29\u6e83\u95ee\u9898\u3002", "method": "\u4f7f\u7528Transformer\u5d4c\u5165\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6307\u6807\uff0c\u5206\u67902013-2025\u5e74\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\uff08\u8fc7\u6ee4\u540e\u7684Common Crawl\u6570\u636e\uff09\u7684\u9010\u5e74\u8bed\u4e49\u76f8\u4f3c\u5ea6\u53d8\u5316\u3002", "result": "\u5728LLM\u516c\u5f00\u91c7\u7528\u524d\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7a33\u6b65\u4e0a\u5347\uff08\u53d7\u65e9\u671fRNN/LSTM\u7ffb\u8bd1\u548c\u6587\u672c\u89c4\u8303\u5316\u5f71\u54cd\uff09\uff0cLLM\u91c7\u7528\u540e\u76f8\u4f3c\u5ea6\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u6ce2\u52a8\u53cd\u6620\u4e86\u4e0d\u53ef\u7ea6\u7684\u8bed\u8a00\u591a\u6837\u6027\u3001\u8bed\u6599\u5e93\u89c4\u6a21\u53d8\u5316\u548c\u6709\u9650\u91c7\u6837\u8bef\u5dee\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u4f30\u8ba1\uff0c\u8868\u660e\u9012\u5f52AI\u6c61\u67d3\u4f55\u65f6\u53ef\u80fd\u663e\u8457\u5a01\u80c1\u6570\u636e\u4e30\u5bcc\u6027\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.05854", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05854", "abs": "https://arxiv.org/abs/2511.05854", "authors": ["Zepeng Bao", "Shen Zhou", "Qiankun Pi", "Jianhao Chen", "Mayi Xu", "Ming Zhong", "Yuanyuan Zhu", "Tieyun Qian"], "title": "Can a Small Model Learn to Look Before It Leaps? Dynamic Learning and Proactive Correction for Hallucination Detection", "comment": null, "summary": "Hallucination in large language models (LLMs) remains a critical barrier to their safe deployment. Existing tool-augmented hallucination detection methods require pre-defined fixed verification strategies, which are crucial to the quality and effectiveness of tool calls. Some methods directly employ powerful closed-source LLMs such as GPT-4 as detectors, which are effective but too costly. To mitigate the cost issue, some methods adopt the teacher-student architecture and finetune open-source small models as detectors via agent tuning. However, these methods are limited by fixed strategies. When faced with a dynamically changing execution environment, they may lack adaptability and inappropriately call tools, ultimately leading to detection failure. To address the problem of insufficient strategy adaptability, we propose the innovative ``Learning to Evaluate and Adaptively Plan''(LEAP) framework, which endows an efficient student model with the dynamic learning and proactive correction capabilities of the teacher model. Specifically, our method formulates the hallucination detection problem as a dynamic strategy learning problem. We first employ a teacher model to generate trajectories within the dynamic learning loop and dynamically adjust the strategy based on execution failures. We then distill this dynamic planning capability into an efficient student model via agent tuning. Finally, during strategy execution, the student model adopts a proactive correction mechanism, enabling it to propose, review, and optimize its own verification strategies before execution. We demonstrate through experiments on three challenging benchmarks that our LEAP-tuned model outperforms existing state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86LEAP\u6846\u67b6\u6765\u89e3\u51b3LLM\u5e7b\u89c9\u68c0\u6d4b\u4e2d\u7684\u7b56\u7565\u9002\u5e94\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u5b66\u4e60\u5faa\u73af\u548c\u4e3b\u52a8\u4fee\u6b63\u673a\u5236\uff0c\u4f7f\u9ad8\u6548\u7684\u5b66\u751f\u6a21\u578b\u5177\u5907\u52a8\u6001\u89c4\u5212\u548c\u7b56\u7565\u8c03\u6574\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u589e\u5f3a\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u56fa\u5b9a\u9a8c\u8bc1\u7b56\u7565\uff0c\u5728\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u5bfc\u81f4\u68c0\u6d4b\u5931\u8d25\u3002\u800c\u4f7f\u7528GPT-4\u7b49\u95ed\u6e90\u6a21\u578b\u4f5c\u4e3a\u68c0\u6d4b\u5668\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u5c06\u5e7b\u89c9\u68c0\u6d4b\u95ee\u9898\u5efa\u6a21\u4e3a\u52a8\u6001\u7b56\u7565\u5b66\u4e60\u95ee\u9898\uff0c\u9996\u5148\u4f7f\u7528\u6559\u5e08\u6a21\u578b\u5728\u52a8\u6001\u5b66\u4e60\u5faa\u73af\u4e2d\u751f\u6210\u8f68\u8ff9\u5e76\u6839\u636e\u6267\u884c\u5931\u8d25\u8c03\u6574\u7b56\u7565\uff0c\u7136\u540e\u901a\u8fc7\u667a\u80fd\u4f53\u8c03\u4f18\u5c06\u8fd9\u79cd\u52a8\u6001\u89c4\u5212\u80fd\u529b\u84b8\u998f\u5230\u9ad8\u6548\u7684\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u6700\u540e\u5728\u6267\u884c\u65f6\u91c7\u7528\u4e3b\u52a8\u4fee\u6b63\u673a\u5236\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLEAP\u8c03\u4f18\u7684\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "LEAP\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5e7b\u89c9\u68c0\u6d4b\u4e2d\u7684\u7b56\u7565\u9002\u5e94\u6027\u95ee\u9898\uff0c\u4f7f\u9ad8\u6548\u7684\u5b66\u751f\u6a21\u578b\u5177\u5907\u4e86\u52a8\u6001\u5b66\u4e60\u548c\u4e3b\u52a8\u4fee\u6b63\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.05874", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05874", "abs": "https://arxiv.org/abs/2511.05874", "authors": ["Haoran Xue", "Gias Uddin", "Song Wang"], "title": "An Empirical Study of Reasoning Steps in Thinking Code LLMs", "comment": null, "summary": "Thinking Large Language Models (LLMs) generate explicit intermediate reasoning traces before final answers, potentially improving transparency, interpretability, and solution accuracy for code generation. However, the quality of these reasoning chains remains underexplored. We present a comprehensive empirical study examining the reasoning process and quality of thinking LLMs for code generation. We evaluate six state-of-the-art reasoning LLMs (DeepSeek-R1, OpenAI-o3-mini, Claude-3.7-Sonnet-Thinking, Gemini-2.0-Flash-Thinking, Gemini-2.5-Flash, and Qwen-QwQ) across 100 code generation tasks of varying difficulty from BigCodeBench. We quantify reasoning-chain structure through step counts and verbosity, conduct controlled step-budget adjustments, and perform a 21-participant human evaluation across three dimensions: efficiency, logical correctness, and completeness. Our step-count interventions reveal that targeted step increases can improve resolution rates for certain models/tasks, while modest reductions often preserve success on standard tasks, rarely on hard ones. Through systematic analysis, we develop a reasoning-problematic taxonomy, identifying completeness as the dominant failure mode. Task complexity significantly impacts reasoning quality; hard problems are substantially more prone to incompleteness than standard tasks. Our stability analysis demonstrates that thinking LLMs maintain consistent logical structures across computational effort levels and can self-correct previous errors. This study provides new insights into the strengths and limitations of current thinking LLMs in software engineering.", "AI": {"tldr": "\u5bf96\u79cd\u601d\u7ef4LLM\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u8fc7\u7a0b\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u63a8\u7406\u94fe\u8d28\u91cf\u53d7\u4efb\u52a1\u590d\u6742\u5ea6\u5f71\u54cd\uff0c\u5b8c\u6574\u6027\u662f\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\uff0c\u4f46\u6a21\u578b\u80fd\u4fdd\u6301\u903b\u8f91\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u81ea\u6211\u7ea0\u9519\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u601d\u7ef4LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u63a8\u7406\u94fe\u7684\u8d28\u91cf\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528BigCodeBench\u7684100\u4e2a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u8bc4\u4f306\u79cd\u601d\u7ef4LLM\uff0c\u901a\u8fc7\u6b65\u9aa4\u8ba1\u6570\u548c\u5197\u957f\u5ea6\u91cf\u5316\u63a8\u7406\u94fe\u7ed3\u6784\uff0c\u8fdb\u884c\u6b65\u9aa4\u9884\u7b97\u8c03\u6574\u5b9e\u9a8c\uff0c\u5e76\u5f00\u5c5521\u4eba\u53c2\u4e0e\u7684\u4eba\u5de5\u8bc4\u4f30\u3002", "result": "\u4efb\u52a1\u590d\u6742\u5ea6\u663e\u8457\u5f71\u54cd\u63a8\u7406\u8d28\u91cf\uff0c\u56f0\u96be\u95ee\u9898\u66f4\u5bb9\u6613\u51fa\u73b0\u4e0d\u5b8c\u6574\u6027\uff1b\u6b65\u9aa4\u5e72\u9884\u53ef\u63d0\u9ad8\u7279\u5b9a\u6a21\u578b/\u4efb\u52a1\u7684\u89e3\u51b3\u7387\uff1b\u6a21\u578b\u80fd\u4fdd\u6301\u903b\u8f91\u7ed3\u6784\u4e00\u81f4\u6027\u5e76\u81ea\u6211\u7ea0\u9519\u3002", "conclusion": "\u5f53\u524d\u601d\u7ef4LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5177\u6709\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\u548c\u81ea\u6211\u7ea0\u9519\u7684\u4f18\u52bf\uff0c\u4f46\u63a8\u7406\u5b8c\u6574\u6027\u4ecd\u662f\u4e3b\u8981\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u3002", "topic": "agent analysis"}}
{"id": "2511.06090", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.06090", "abs": "https://arxiv.org/abs/2511.06090", "authors": ["Jeffrey Jian Ma", "Milad Hashemi", "Amir Yazdanbakhsh", "Kevin Swersky", "Ofir Press", "Enhui Li", "Vijay Janapa Reddi", "Parthasarathy Ranganathan"], "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?", "comment": "Data, code, and leaderboard are available at https://swefficiency.com/", "summary": "Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce \\textsc{SWE-fficiency}, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning.", "AI": {"tldr": "SWE-fficiency\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4ed3\u5e93\u7ea7\u6027\u80fd\u4f18\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b498\u4e2a\u771f\u5b9e\u6570\u636e\u79d1\u5b66\u3001\u673a\u5668\u5b66\u4e60\u548cHPC\u4ed3\u5e93\u7684\u4efb\u52a1\u3002\u76f8\u6bd4\u4f20\u7edf\u57fa\u51c6\u5f3a\u8c03\"\u4fee\u590d\u4ec0\u4e48\"\uff0c\u8be5\u57fa\u51c6\u5173\u6ce8\"\u5982\u4f55\u4fee\u590d\"\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u5206\u6790\u4ee3\u7801\u8bed\u4e49\u3001\u5b9a\u4f4d\u74f6\u9888\u5e76\u751f\u6210\u4f18\u5316\u8865\u4e01\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5927\u591a\u5173\u6ce8\u4fee\u590d\u4ec0\u4e48\u4ee3\u7801\uff0c\u800c\u5ffd\u7565\u4e86\u5982\u4f55\u4fee\u590d\u4ee3\u7801\u7684\u6027\u80fd\u95ee\u9898\u3002\u5927\u578b\u8f6f\u4ef6\u4ed3\u5e93\u7684\u6027\u80fd\u4f18\u5316\u9700\u8981\u4ee3\u7801\u63a8\u7406\u548c\u8f6f\u4ef6\u5de5\u7a0b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u4eceGitHub\u62c9\u53d6\u8bf7\u6c42\u4e2d\u6536\u96c6\u6027\u80fd\u4f18\u5316\u7f16\u8f91\uff0c\u7ed3\u5408\u5173\u952e\u8bcd\u8fc7\u6ee4\u3001\u9759\u6001\u5206\u6790\u3001\u8986\u76d6\u7387\u5de5\u5177\u548c\u6267\u884c\u9a8c\u8bc1\uff0c\u786e\u8ba4\u4e13\u5bb6\u52a0\u901f\u57fa\u51c6\u5e76\u8bc6\u522b\u76f8\u5173\u5355\u5143\u6d4b\u8bd5\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u667a\u80fd\u4f53\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\u663e\u8457\u6027\u80fd\u4e0d\u8db3\uff0c\u5e73\u5747\u4ec5\u8fbe\u5230\u4e13\u5bb6\u52a0\u901f\u76840.15\u500d\u3002\u667a\u80fd\u4f53\u5728\u5b9a\u4f4d\u4f18\u5316\u673a\u4f1a\u3001\u8de8\u51fd\u6570\u6267\u884c\u63a8\u7406\u548c\u4fdd\u6301\u7f16\u8f91\u6b63\u786e\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u8be5\u57fa\u51c6\u63ed\u793a\u4e86\u5f53\u524d\u667a\u80fd\u4f53\u5728\u6027\u80fd\u4f18\u5316\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u6027\u80fd\u5de5\u7a0b\u548c\u957f\u89c6\u91ce\u8f6f\u4ef6\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "topic": "swe benchmark"}}
{"id": "2511.05931", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05931", "abs": "https://arxiv.org/abs/2511.05931", "authors": ["Hiroaki Hayashi", "Bo Pang", "Wenting Zhao", "Ye Liu", "Akash Gokul", "Srijan Bansal", "Caiming Xiong", "Semih Yavuz", "Yingbo Zhou"], "title": "Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement", "comment": null, "summary": "Large language model (LLM) based agents are increasingly used to tackle software engineering tasks that require multi-step reasoning and code modification, demonstrating promising yet limited performance. However, most existing LLM agents typically operate within static execution frameworks, lacking a principled mechanism to learn and self-improve from their own experience and past rollouts. As a result, their performance remains bounded by the initial framework design and the underlying LLM's capabilities. We propose Self-Abstraction from Grounded Experience (SAGE), a framework that enables agents to learn from their own task executions and refine their behavior through self-abstraction. After an initial rollout, the agent induces a concise plan abstraction from its grounded experience, distilling key steps, dependencies, and constraints. This learned abstraction is then fed back as contextual guidance, refining the agent's policy and supporting more structured, informed subsequent executions. Empirically, SAGE delivers consistent performance gains across diverse LLM backbones and agent architectures. Notably, it yields a 7.2% relative performance improvement over the strong Mini-SWE-Agent baseline when paired with the GPT-5 (high) backbone. SAGE further achieves strong overall performance on SWE-Bench Verified benchmark, reaching 73.2% and 74% Pass@1 resolve rates with the Mini-SWE-Agent and OpenHands CodeAct agent framework, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86SAGE\u6846\u67b6\uff0c\u4f7fLLM\u4ee3\u7406\u80fd\u591f\u4ece\u81ea\u8eab\u4efb\u52a1\u6267\u884c\u4e2d\u5b66\u4e60\uff0c\u901a\u8fc7\u81ea\u6211\u62bd\u8c61\u6765\u6539\u8fdb\u884c\u4e3a\uff0c\u5728SWE-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u901a\u5e38\u5728\u9759\u6001\u6267\u884c\u6846\u67b6\u4e2d\u8fd0\u884c\uff0c\u7f3a\u4e4f\u4ece\u81ea\u8eab\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u548c\u81ea\u6211\u6539\u8fdb\u7684\u673a\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u4e8e\u521d\u59cb\u6846\u67b6\u8bbe\u8ba1\u548c\u57fa\u7840LLM\u80fd\u529b\u3002", "method": "SAGE\u6846\u67b6\uff1a\u4ee3\u7406\u4ece\u521d\u59cb\u6267\u884c\u4e2d\u5f52\u7eb3\u51fa\u7b80\u6d01\u7684\u8ba1\u5212\u62bd\u8c61\uff0c\u63d0\u70bc\u5173\u952e\u6b65\u9aa4\u3001\u4f9d\u8d56\u5173\u7cfb\u548c\u7ea6\u675f\uff0c\u7136\u540e\u5c06\u5b66\u4e60\u5230\u7684\u62bd\u8c61\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u6307\u5bfc\u53cd\u9988\uff0c\u6539\u8fdb\u4ee3\u7406\u7b56\u7565\u5e76\u652f\u6301\u66f4\u6709\u7ed3\u6784\u7684\u540e\u7eed\u6267\u884c\u3002", "result": "SAGE\u5728\u4e0d\u540cLLM\u9aa8\u5e72\u548c\u4ee3\u7406\u67b6\u6784\u4e0a\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e0eGPT-5\uff08\u9ad8\uff09\u9aa8\u5e72\u914d\u5bf9\u65f6\u76f8\u5bf9Mini-SWE-Agent\u57fa\u7ebf\u63d0\u53477.2%\uff0c\u5728SWE-Bench Verified\u57fa\u51c6\u4e0a\u5206\u522b\u8fbe\u523073.2%\u548c74%\u7684Pass@1\u89e3\u51b3\u7387\u3002", "conclusion": "SAGE\u6846\u67b6\u901a\u8fc7\u81ea\u6211\u62bd\u8c61\u5b66\u4e60\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u7684\u91cd\u8981\u6027\u3002", "topic": "swe application"}}
{"id": "2511.05650", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05650", "abs": "https://arxiv.org/abs/2511.05650", "authors": ["Yichen Wang", "Chenghao Yang", "Tenghao Huang", "Muhao Chen", "Jonathan May", "Mina Lee"], "title": "Optimizing Diversity and Quality through Base-Aligned Model Collaboration", "comment": "52 pages, 16 figures", "summary": "Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.", "AI": {"tldr": "BACo\u662f\u4e00\u4e2a\u63a8\u7406\u65f6token\u7ea7\u6a21\u578b\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7ed3\u5408\u57fa\u7840LLM\u548c\u5bf9\u9f50LLM\u6765\u4f18\u5316\u8f93\u51fa\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u5728\u5355\u6b21\u63a8\u7406\u4e2d\u5b9e\u73b021.3%\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u8054\u5408\u63d0\u5347\u3002", "motivation": "\u5bf9\u9f50\u867d\u7136\u63d0\u9ad8\u4e86LLM\u7684\u8f93\u51fa\u8d28\u91cf\uff0c\u4f46\u727a\u7272\u4e86\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u4e0d\u540c\u751f\u6210\u7ed3\u679c\u9ad8\u5ea6\u76f8\u4f3c\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u91cd\u8bad\u7ec3\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u591a\u91c7\u6837\u8981\u4e48\u964d\u4f4e\u8d28\u91cf\uff0c\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u89e3\u7801\u6216\u540e\u8bad\u7ec3\u6210\u672c\u3002", "method": "\u4f7f\u7528\u8def\u7531\u7b56\u7565\u5728token\u7ea7\u522b\u52a8\u6001\u51b3\u5b9a\u4ece\u57fa\u7840\u6a21\u578b\u8fd8\u662f\u5bf9\u9f50\u6a21\u578b\u89e3\u7801\uff0c\u57fa\u4e8e\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u9884\u6d4b\u5185\u5bb9\u7684\u8bed\u4e49\u89d2\u8272\u3002", "result": "\u5728\u4e09\u4e2a\u5f00\u653e\u751f\u6210\u4efb\u52a1\u548c13\u4e2a\u6307\u6807\u4e0a\uff0cBACo\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u63a8\u7406\u65f6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6700\u4f73\u8def\u7531\u5668\u5b9e\u73b021.3%\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u8054\u5408\u63d0\u5347\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u6539\u8fdb\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u548c\u5bf9\u9f50\u6a21\u578b\u4e4b\u95f4\u7684\u534f\u4f5c\u53ef\u4ee5\u4f18\u5316\u548c\u63a7\u5236\u591a\u6837\u6027\u4e0e\u8d28\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2511.05951", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05951", "abs": "https://arxiv.org/abs/2511.05951", "authors": ["Qi Wang", "Hongzhi Zhang", "Jia Fu", "Kai Fu", "Yahui Liu", "Tinghai Zhang", "Chenxi Sun", "Gangwei Jiang", "Jingyi Tang", "Xingguang Ji", "Yang Yue", "Jingyuan Zhang", "Fuzheng Zhang", "Kun Gai", "Guorui Zhou"], "title": "Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling", "comment": "20 pages, 7 figures", "summary": "Despite the proliferation of powerful agentic models, the lack of critical post-training details hinders the development of strong counterparts in the open-source community. In this study, we present a comprehensive and fully open-source pipeline for training a high-performance agentic model for interacting with external tools and environments, named Klear-Qwen3-AgentForge, starting from the Qwen3-8B base model. We design effective supervised fine-tuning (SFT) with synthetic data followed by multi-turn reinforcement learning (RL) to unlock the potential for multiple diverse agentic tasks. We perform exclusive experiments on various agentic benchmarks in both tool use and coding domains. Klear-Qwen3-AgentForge-8B achieves state-of-the-art performance among LLMs of similar size and remains competitive with significantly larger models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684\u4ee3\u7406\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0bKlear-Qwen3-AgentForge\uff0c\u4eceQwen3-8B\u57fa\u7840\u6a21\u578b\u5f00\u59cb\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u5de5\u5177\u4f7f\u7528\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u8fbe\u5230\u540c\u7c7b\u5c3a\u5bf8\u6a21\u578b\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5927\u7684\u4ee3\u7406\u6a21\u578b\u4e0d\u65ad\u6d8c\u73b0\uff0c\u4f46\u7f3a\u4e4f\u5173\u952e\u7684\u8bad\u7ec3\u540e\u7ec6\u8282\u963b\u788d\u4e86\u5f00\u6e90\u793e\u533a\u5f00\u53d1\u5f3a\u529b\u7684\u5bf9\u5e94\u6a21\u578b\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u5408\u6210\u6570\u636e\u7684\u6709\u6548\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u8fdb\u884c\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u89e3\u9501\u591a\u79cd\u4ee3\u7406\u4efb\u52a1\u7684\u6f5c\u529b\u3002", "result": "Klear-Qwen3-AgentForge-8B\u5728\u7c7b\u4f3c\u5c3a\u5bf8\u7684LLM\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u663e\u8457\u66f4\u5927\u7684\u6a21\u578b\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u4e14\u5b8c\u5168\u5f00\u6e90\u7684\u4ee3\u7406\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0b\uff0c\u8bc1\u660e\u4e86\u4ece\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u9ad8\u6027\u80fd\u4ee3\u7406\u6a21\u578b\u7684\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.05722", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05722", "abs": "https://arxiv.org/abs/2511.05722", "authors": ["Zheng Du", "Hao Kang", "Song Han", "Tushar Krishna", "Ligeng Zhu"], "title": "OckBench: Measuring the Efficiency of LLM Reasoning", "comment": null, "summary": "Large language models such as GPT-4, Claude 3, and the Gemini series have improved automated reasoning and code generation. However, existing benchmarks mainly focus on accuracy and output quality, and they ignore an important factor: decoding token efficiency. In real systems, generating 10,000 tokens versus 100,000 tokens leads to large differences in latency, cost, and energy. In this work, we introduce OckBench, a model-agnostic and hardware-agnostic benchmark that evaluates both accuracy and token count for reasoning and coding tasks. Through experiments comparing multiple open- and closed-source models, we uncover that many models with comparable accuracy differ wildly in token consumption, revealing that efficiency variance is a neglected but significant axis of differentiation. We further demonstrate Pareto frontiers over the accuracy-efficiency plane and argue for an evaluation paradigm shift: we should no longer treat tokens as \"free\" to multiply. OckBench provides a unified platform for measuring, comparing, and guiding research in token-efficient reasoning. Our benchmarks are available at https://ockbench.github.io/ .", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "topics": "Error"}}
{"id": "2511.05977", "categories": ["cs.AI", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.05977", "abs": "https://arxiv.org/abs/2511.05977", "authors": ["Pavel Naumov", "Alexandra Pavlova"], "title": "An Epistemic Perspective on Agent Awareness", "comment": "Fortieth AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "The paper proposes to treat agent awareness as a form of knowledge, breaking the tradition in the existing literature on awareness. It distinguishes the de re and de dicto forms of such knowledge. The work introduces two modalities capturing these forms and formally specifies their meaning using a version of 2D-semantics. The main technical result is a sound and complete logical system describing the interplay between the two proposed modalities and the standard \"knowledge of the fact\" modality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u667a\u80fd\u4f53\u610f\u8bc6\u89c6\u4e3a\u4e00\u79cd\u77e5\u8bc6\u5f62\u5f0f\uff0c\u6253\u7834\u4e86\u73b0\u6709\u6587\u732e\u4f20\u7edf\uff0c\u533a\u5206\u4e86\u8fd9\u79cd\u77e5\u8bc6\u7684de re\u548cde dicto\u5f62\u5f0f\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u79cd\u6a21\u6001\u6765\u6355\u6349\u8fd9\u4e9b\u5f62\u5f0f\uff0c\u4f7f\u75282D\u8bed\u4e49\u5b66\u5f62\u5f0f\u5316\u5176\u542b\u4e49\u3002", "motivation": "\u6253\u7834\u73b0\u6709\u6587\u732e\u4e2d\u5173\u4e8e\u667a\u80fd\u4f53\u610f\u8bc6\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u5c06\u610f\u8bc6\u89c6\u4e3a\u4e00\u79cd\u77e5\u8bc6\u5f62\u5f0f\uff0c\u5e76\u533a\u5206\u5176\u4e0d\u540c\u8868\u73b0\u5f62\u5f0f\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u6a21\u6001\u6765\u6355\u6349de re\u548cde dicto\u5f62\u5f0f\u7684\u610f\u8bc6\u77e5\u8bc6\uff0c\u4f7f\u75282D\u8bed\u4e49\u5b66\u5f62\u5f0f\u5316\u5176\u542b\u4e49\uff0c\u6784\u5efa\u63cf\u8ff0\u8fd9\u4e9b\u6a21\u6001\u4e0e\u6807\u51c6\"\u4e8b\u5b9e\u77e5\u8bc6\"\u6a21\u6001\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u903b\u8f91\u7cfb\u7edf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u63cf\u8ff0\u6240\u63d0\u51fa\u7684\u4e24\u79cd\u6a21\u6001\u4e0e\u6807\u51c6\"\u4e8b\u5b9e\u77e5\u8bc6\"\u6a21\u6001\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u5065\u5168\u4e14\u5b8c\u5907\u7684\u903b\u8f91\u7cfb\u7edf\u3002", "conclusion": "\u6210\u529f\u5efa\u7acb\u4e86\u5c06\u667a\u80fd\u4f53\u610f\u8bc6\u4f5c\u4e3a\u77e5\u8bc6\u5f62\u5f0f\u5904\u7406\u7684\u903b\u8f91\u6846\u67b6\uff0c\u4e3a\u7406\u89e3\u610f\u8bc6\u7684\u4e0d\u540c\u5f62\u5f0f\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2511.05589", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05589", "abs": "https://arxiv.org/abs/2511.05589", "authors": ["Zekai Qu", "Yinxu Pan", "Ao Sun", "Chaojun Xiao", "Xu Han"], "title": "CoPRIS: Efficient and Stable Reinforcement Learning via Concurrency-Controlled Partial Rollout with Importance Sampling", "comment": "13 pages, 4 figures", "summary": "Reinforcement learning (RL) post-training has become a trending paradigm for enhancing the capabilities of large language models (LLMs). Most existing RL systems for LLMs operate in a fully synchronous manner, where training must wait for the rollout of an entire batch to complete. This design leads to severe inefficiencies, as extremely long trajectories can stall the entire rollout process and leave many GPUs idle. To address this issue, we propose Concurrency- Controlled Partial Rollout with Importance Sampling (CoPRIS), which mitigates long-tail inefficiencies by maintaining a fixed number of concurrent rollouts, early-terminating once sufficient samples are collected, and reusing unfinished trajectories in subsequent rollouts. To mitigate the impact of off-policy trajectories, we introduce Cross-stage Importance Sampling Correction, which concatenates buffered log probabilities from the previous policy with those recomputed under the current policy for importance sampling correction. Experiments on challenging mathematical reasoning benchmarks show that CoPRIS achieves up to 1.94x faster training while maintaining comparable or superior performance to synchronous RL systems. The code of CoPRIS is available at https://github.com/777pomingzi/CoPRIS.", "AI": {"tldr": "CoPRIS\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u5e76\u53d1rollout\u6570\u91cf\u3001\u63d0\u524d\u7ec8\u6b62\u548c\u91cd\u7528\u672a\u5b8c\u6210\u8f68\u8ff9\u6765\u89e3\u51b3\u4f20\u7edf\u540c\u6b65RL\u7cfb\u7edf\u7684\u6548\u7387\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e861.94\u500d\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "motivation": "\u73b0\u6709LLM\u7684RL\u8bad\u7ec3\u7cfb\u7edf\u91c7\u7528\u5b8c\u5168\u540c\u6b65\u65b9\u5f0f\uff0c\u5bfc\u81f4\u957f\u8f68\u8ff9\u4f1a\u963b\u585e\u6574\u4e2arollout\u8fc7\u7a0b\uff0c\u9020\u6210GPU\u8d44\u6e90\u95f2\u7f6e\u548c\u4e25\u91cd\u6548\u7387\u4f4e\u4e0b\u3002", "method": "CoPRIS\u91c7\u7528\u5e76\u53d1\u63a7\u5236\u90e8\u5206rollout\u4e0e\u91cd\u8981\u6027\u91c7\u6837\uff0c\u901a\u8fc7\u7ef4\u62a4\u56fa\u5b9a\u6570\u91cf\u7684\u5e76\u53d1rollout\u3001\u63d0\u524d\u7ec8\u6b62\u6536\u96c6\u8db3\u591f\u6837\u672c\u3001\u91cd\u7528\u672a\u5b8c\u6210\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u8de8\u9636\u6bb5\u91cd\u8981\u6027\u91c7\u6837\u6821\u6b63\u6765\u5904\u7406\u79bb\u7b56\u7565\u8f68\u8ff9\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoPRIS\u5b9e\u73b0\u4e86\u6700\u9ad81.94\u500d\u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u540c\u6b65RL\u7cfb\u7edf\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "CoPRIS\u6709\u6548\u89e3\u51b3\u4e86RL\u8bad\u7ec3\u4e2d\u7684\u957f\u5c3e\u6548\u7387\u95ee\u9898\uff0c\u4e3aLLM\u7684RL\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5f02\u6b65\u8bad\u7ec3\u8303\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.06227", "categories": ["cs.SE", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.06227", "abs": "https://arxiv.org/abs/2511.06227", "authors": ["Anamul Haque Mollah", "Ahmed Aljohani", "Hyunsook Do"], "title": "Assertion-Aware Test Code Summarization with Large Language Models", "comment": "Accepted for publication at 2nd ACM International Conference on AI-powered Software (AIware 2025)", "summary": "Unit tests often lack concise summaries that convey test intent, especially in auto-generated or poorly documented codebases. Large Language Models (LLMs) offer a promising solution, but their effectiveness depends heavily on how they are prompted. Unlike generic code summarization, test-code summarization poses distinct challenges because test methods validate expected behavior through assertions rather than im- plementing functionality. This paper presents a new benchmark of 91 real-world Java test cases paired with developer-written summaries and conducts a controlled ablation study to investigate how test code-related components-such as the method under test (MUT), assertion messages, and assertion semantics-affect the performance of LLM-generated test summaries. We evaluate four code LLMs (Codex, Codestral, DeepSeek, and Qwen-Coder) across seven prompt configurations using n-gram metrics (BLEU, ROUGE-L, METEOR), semantic similarity (BERTScore), and LLM-based evaluation. Results show that prompting with as- sertion semantics improves summary quality by an average of 0.10 points (2.3%) over full MUT context (4.45 vs. 4.35) while requiring fewer input tokens. Codex and Qwen-Coder achieve the highest alignment with human-written summaries, while DeepSeek underperforms despite high lexical overlap. The replication package is publicly available at https://doi.org/10. 5281/zenodo.17067550", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b91\u4e2a\u771f\u5b9eJava\u6d4b\u8bd5\u7528\u4f8b\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u63a2\u7d22\u4e86\u6d4b\u8bd5\u4ee3\u7801\u76f8\u5173\u7ec4\u4ef6\uff08\u5982\u88ab\u6d4b\u65b9\u6cd5\u3001\u65ad\u8a00\u4fe1\u606f\u548c\u65ad\u8a00\u8bed\u4e49\uff09\u5bf9LLM\u751f\u6210\u6d4b\u8bd5\u6458\u8981\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5355\u5143\u6d4b\u8bd5\u901a\u5e38\u7f3a\u4e4f\u7b80\u6d01\u7684\u6458\u8981\u6765\u4f20\u8fbe\u6d4b\u8bd5\u610f\u56fe\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u751f\u6210\u6216\u6587\u6863\u4e0d\u5b8c\u5584\u7684\u4ee3\u7801\u5e93\u4e2d\u3002LLM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u6548\u679c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u63d0\u793a\u65b9\u5f0f\u3002", "method": "\u8bc4\u4f30\u4e86\u56db\u4e2a\u4ee3\u7801LLM\uff08Codex\u3001Codestral\u3001DeepSeek\u548cQwen-Coder\uff09\u5728\u4e03\u79cd\u63d0\u793a\u914d\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u4f7f\u7528n-gram\u6307\u6807\uff08BLEU\u3001ROUGE-L\u3001METEOR\uff09\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff08BERTScore\uff09\u548c\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u65ad\u8a00\u8bed\u4e49\u8fdb\u884c\u63d0\u793a\u6bd4\u5b8c\u6574MUT\u4e0a\u4e0b\u6587\u5e73\u5747\u63d0\u9ad8\u6458\u8981\u8d28\u91cf0.10\u5206\uff082.3%\uff09\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u8f93\u5165token\u3002Codex\u548cQwen-Coder\u4e0e\u4eba\u5de5\u7f16\u5199\u6458\u8981\u7684\u5bf9\u9f50\u5ea6\u6700\u9ad8\uff0c\u800cDeepSeek\u5c3d\u7ba1\u8bcd\u6c47\u91cd\u53e0\u5ea6\u9ad8\u4f46\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u65ad\u8a00\u8bed\u4e49\u662f\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u6458\u8981\u7684\u5173\u952e\u56e0\u7d20\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u8f93\u5165token\u7684\u540c\u65f6\u63d0\u9ad8\u6458\u8981\u8d28\u91cf\u3002", "topic": "swe benchmark"}}
{"id": "2511.06134", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.06134", "abs": "https://arxiv.org/abs/2511.06134", "authors": ["Wei Yang", "Jiacheng Pang", "Shixuan Li", "Paul Bogdan", "Stephen Tu", "Jesse Thomason"], "title": "Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs", "comment": null, "summary": "Multi-agent systems (MAS) built on Large Language Models (LLMs) are being used to approach complex problems and can surpass single model inference. However, their success hinges on navigating a fundamental cognitive tension: the need to balance broad, divergent exploration of the solution space with a principled, convergent synthesis to the optimal solution. Existing paradigms often struggle to manage this duality, leading to premature consensus, error propagation, and a critical credit assignment problem that fails to distinguish between genuine reasoning and superficially plausible arguments. To resolve this core challenge, we propose the Multi-Agent Exploration-Synthesis framework Through Role Orchestration (Maestro), a principled paradigm for collaboration that structurally decouples these cognitive modes. Maestro uses a collective of parallel Execution Agents for diverse exploration and a specialized Central Agent for convergent, evaluative synthesis. To operationalize this critical synthesis phase, we introduce Conditional Listwise Policy Optimization (CLPO), a reinforcement learning objective that disentangles signals for strategic decisions and tactical rationales. By combining decision-focused policy gradients with a list-wise ranking loss over justifications, CLPO achieves clean credit assignment and stronger comparative supervision. Experiments on mathematical reasoning and general problem-solving benchmarks demonstrate that Maestro, coupled with CLPO, consistently outperforms existing state-of-the-art multi-agent approaches, delivering absolute accuracy gains of 6% on average and up to 10% at best.", "AI": {"tldr": "\u63d0\u51fa\u4e86Maestro\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u7f16\u6392\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u8ba4\u77e5\u5f20\u529b\u95ee\u9898\uff0c\u7ed3\u5408\u63a2\u7d22\u548c\u7efc\u5408\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u4f7f\u7528CLPO\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u5b9e\u73b0\u6e05\u6670\u7684\u4fe1\u7528\u5206\u914d\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5e73\u8861\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u7684\u5e7f\u6cdb\u63a2\u7d22\u548c\u539f\u5219\u6027\u7efc\u5408\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5bb9\u6613\u5bfc\u81f4\u8fc7\u65e9\u5171\u8bc6\u3001\u9519\u8bef\u4f20\u64ad\u548c\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002", "method": "Maestro\u6846\u67b6\uff1a\u4f7f\u7528\u5e76\u884c\u6267\u884c\u667a\u80fd\u4f53\u8fdb\u884c\u591a\u6837\u5316\u63a2\u7d22\uff0c\u4e13\u7528\u4e2d\u592e\u667a\u80fd\u4f53\u8fdb\u884c\u6536\u655b\u6027\u7efc\u5408\uff1bCLPO\u65b9\u6cd5\uff1a\u7ed3\u5408\u51b3\u7b56\u5bfc\u5411\u7684\u7b56\u7565\u68af\u5ea6\u548c\u57fa\u4e8e\u7406\u7531\u7684\u5217\u8868\u6392\u5e8f\u635f\u5931\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u901a\u7528\u95ee\u9898\u89e3\u51b3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMaestro+CLPO\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5e73\u5747\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u53476%\uff0c\u6700\u9ad8\u63d0\u534710%\u3002", "conclusion": "Maestro\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u89e3\u8026\u63a2\u7d22\u548c\u7efc\u5408\u8ba4\u77e5\u6a21\u5f0f\uff0c\u7ed3\u5408CLPO\u7684\u6e05\u6670\u4fe1\u7528\u5206\u914d\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.06251", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06251", "abs": "https://arxiv.org/abs/2511.06251", "authors": ["Mingde Xu", "Zhen Yang", "Wenyi Hong", "Lihang Pan", "Xinyue Fan", "Yan Wang", "Xiaotao Gu", "Bin Xu", "Jie Tang"], "title": "WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation", "comment": "36 pages, 30 figures", "summary": "User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at \\href{https://zheny2751-dotcom.github.io/webvia.github.io/}{\\texttt{https://webvia.github.io}}.", "AI": {"tldr": "WebVIA\u662f\u9996\u4e2a\u7528\u4e8e\u4ea4\u4e92\u5f0fUI\u5230\u4ee3\u7801\u751f\u6210\u548c\u9a8c\u8bc1\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u63a2\u7d22\u667a\u80fd\u4f53\u3001UI2Code\u6a21\u578b\u548c\u9a8c\u8bc1\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u5f0f\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53ea\u80fd\u751f\u6210\u9759\u6001HTML/CSS/JavaScript\u5e03\u5c40\uff0c\u7f3a\u4e4f\u4ea4\u4e92\u6027\uff0c\u800cUI\u5f00\u53d1\u9700\u8981\u5c06\u8bbe\u8ba1\u7a3f\u8f6c\u6362\u4e3a\u529f\u80fd\u4ee3\u7801\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u4ecd\u7136\u91cd\u590d\u4e14\u52b3\u52a8\u5bc6\u96c6\u3002", "method": "\u63d0\u51faWebVIA\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1\uff09\u63a2\u7d22\u667a\u80fd\u4f53\u6355\u83b7\u591a\u72b6\u6001UI\u622a\u56fe\uff1b2\uff09UI2Code\u6a21\u578b\u751f\u6210\u53ef\u6267\u884c\u7684\u4ea4\u4e92\u4ee3\u7801\uff1b3\uff09\u9a8c\u8bc1\u6a21\u5757\u9a8c\u8bc1\u4ea4\u4e92\u6027\u3002", "result": "WebVIA\u667a\u80fd\u4f53\u6bd4\u901a\u7528\u667a\u80fd\u4f53\uff08\u5982Gemini-2.5-Pro\uff09\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u548c\u51c6\u786e\u7684UI\u63a2\u7d22\u3002\u5fae\u8c03\u7684WebVIA-UI2Code\u6a21\u578b\u5728\u751f\u6210\u53ef\u6267\u884c\u4ea4\u4e92\u4ee3\u7801\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "WebVIA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4ea4\u4e92\u5f0fUI\u5230\u4ee3\u7801\u751f\u6210\u7684\u95ee\u9898\uff0c\u5728\u4ea4\u4e92\u6027\u548c\u9759\u6001UI2Code\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\u3002", "topic": "code agent"}}
{"id": "2511.05852", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05852", "abs": "https://arxiv.org/abs/2511.05852", "authors": ["Yinjie Cheng", "Paul Youssef", "Christin Seifert", "J\u00f6rg Schl\u00f6tterer", "Zhixue Zhao"], "title": "Quantifying Edits Decay in Fine-tuned LLMs", "comment": "Under review at ICLR 2026", "summary": "Knowledge editing has emerged as a lightweight alternative to retraining for correcting or injecting specific facts in large language models (LLMs). Meanwhile, fine-tuning remains the default operation for adapting LLMs to new domains and tasks. Despite their widespread adoption, these two post-training interventions have been studied in isolation, leaving open a crucial question: if we fine-tune an edited model, do the edits survive? This question is motivated by two practical scenarios: removing covert or malicious edits, and preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1, current KE methods become less useful, as every fine-tuned model would require re-editing, which significantly increases the cost; if edits persist, fine-tuned models risk propagating hidden malicious edits, raising serious safety concerns. To this end, we systematically quantify edits decay after fine-tuning, investigating how fine-tuning affects knowledge editing. We evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three datasets, yielding 232 experimental configurations. Our results show that edits decay after fine-tuning, with survival varying across configurations, e.g., AlphaEdit edits decay more than MEMIT edits. Further, we propose selective-layer fine-tuning and find that fine-tuning edited layers only can effectively remove edits, though at a slight cost to downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. Overall, our study establishes empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, and underscores that evaluating model editing requires considering the full LLM application pipeline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u77e5\u8bc6\u7f16\u8f91\u540e\u5fae\u8c03\u5bf9\u7f16\u8f91\u6548\u679c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7f16\u8f91\u4f1a\u5728\u5fae\u8c03\u540e\u8870\u51cf\uff0c\u5e76\u63d0\u51fa\u9009\u62e9\u6027\u5c42\u5fae\u8c03\u7b56\u7565\u6765\u6709\u6548\u79fb\u9664\u7f16\u8f91\u3002", "motivation": "\u7814\u7a76\u77e5\u8bc6\u7f16\u8f91\u4e0e\u5fae\u8c03\u4e24\u79cd\u540e\u8bad\u7ec3\u5e72\u9884\u63aa\u65bd\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u89e3\u51b3\u4e24\u4e2a\u5b9e\u9645\u95ee\u9898\uff1a\u79fb\u9664\u9690\u853d\u6216\u6076\u610f\u7f16\u8f91\uff0c\u4ee5\u53ca\u4fdd\u7559\u6709\u76ca\u7f16\u8f91\u3002", "method": "\u8bc4\u4f30\u4e24\u79cd\u6700\u5148\u8fdb\u7684\u7f16\u8f91\u65b9\u6cd5\uff08MEMIT\u3001AlphaEdit\uff09\u548c\u4e09\u79cd\u5fae\u8c03\u65b9\u6cd5\uff08\u5168\u53c2\u6570\u3001LoRA\u3001DoRA\uff09\uff0c\u5728\u4e94\u4e2aLLM\u548c\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c232\u4e2a\u5b9e\u9a8c\u914d\u7f6e\u3002", "result": "\u7f16\u8f91\u5728\u5fae\u8c03\u540e\u4f1a\u8870\u51cf\uff0c\u4e0d\u540c\u914d\u7f6e\u4e0b\u751f\u5b58\u7387\u4e0d\u540c\uff1b\u4ec5\u5fae\u8c03\u7f16\u8f91\u5c42\u53ef\u6709\u6548\u79fb\u9664\u7f16\u8f91\u4f46\u7565\u5fae\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\uff1b\u5fae\u8c03\u975e\u7f16\u8f91\u5c42\u6bd4\u5168\u5fae\u8c03\u635f\u5bb3\u66f4\u591a\u7f16\u8f91\u3002", "conclusion": "\u4e3a\u77e5\u8bc6\u7f16\u8f91\u4e0e\u5fae\u8c03\u7684\u6574\u5408\u5efa\u7acb\u4e86\u7ecf\u9a8c\u57fa\u51c6\u548c\u53ef\u884c\u7b56\u7565\uff0c\u5f3a\u8c03\u8bc4\u4f30\u6a21\u578b\u7f16\u8f91\u9700\u8981\u8003\u8651\u5b8c\u6574\u7684LLM\u5e94\u7528\u6d41\u7a0b\u3002", "topic": "agent analysis"}}
{"id": "2511.06175", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.06175", "abs": "https://arxiv.org/abs/2511.06175", "authors": ["Kaijie Xu", "Fandi Meng", "Clark Verbrugge", "Simon Lucas"], "title": "CSP4SDG: Constraint and Information-Theory Based Role Identification in Social Deduction Games with LLM-Enhanced Inference", "comment": null, "summary": "In Social Deduction Games (SDGs) such as Avalon, Mafia, and Werewolf, players conceal their identities and deliberately mislead others, making hidden-role inference a central and demanding task. Accurate role identification, which forms the basis of an agent's belief state, is therefore the keystone for both human and AI performance. We introduce CSP4SDG, a probabilistic, constraint-satisfaction framework that analyses gameplay objectively. Game events and dialogue are mapped to four linguistically-agnostic constraint classes-evidence, phenomena, assertions, and hypotheses. Hard constraints prune impossible role assignments, while weighted soft constraints score the remainder; information-gain weighting links each hypothesis to its expected value under entropy reduction, and a simple closed-form scoring rule guarantees that truthful assertions converge to classical hard logic with minimum error. The resulting posterior over roles is fully interpretable and updates in real time. Experiments on three public datasets show that CSP4SDG (i) outperforms LLM-based baselines in every inference scenario, and (ii) boosts LLMs when supplied as an auxiliary \"reasoning tool.\" Our study validates that principled probabilistic reasoning with information theory is a scalable alternative-or complement-to heavy-weight neural models for SDGs.", "AI": {"tldr": "\u63d0\u51fa\u4e86CSP4SDG\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ea6\u675f\u6ee1\u8db3\u7684\u6982\u7387\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4e2d\u7684\u9690\u85cf\u89d2\u8272\u63a8\u65ad\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4e2d\uff0c\u73a9\u5bb6\u9690\u85cf\u8eab\u4efd\u5e76\u6545\u610f\u8bef\u5bfc\u4ed6\u4eba\uff0c\u4f7f\u5f97\u9690\u85cf\u89d2\u8272\u63a8\u65ad\u6210\u4e3a\u6838\u5fc3\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u51c6\u786e\u7684\u89d2\u8272\u8bc6\u522b\u662f\u667a\u80fd\u4f53\u4fe1\u5ff5\u72b6\u6001\u7684\u57fa\u7840\uff0c\u5bf9AI\u548c\u4eba\u7c7b\u8868\u73b0\u90fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06\u6e38\u620f\u4e8b\u4ef6\u548c\u5bf9\u8bdd\u6620\u5c04\u5230\u56db\u4e2a\u8bed\u8a00\u65e0\u5173\u7684\u7ea6\u675f\u7c7b\u522b\uff1a\u8bc1\u636e\u3001\u73b0\u8c61\u3001\u65ad\u8a00\u548c\u5047\u8bbe\u3002\u4f7f\u7528\u786c\u7ea6\u675f\u4fee\u526a\u4e0d\u53ef\u80fd\u7684\u89d2\u8272\u5206\u914d\uff0c\u52a0\u6743\u8f6f\u7ea6\u675f\u5bf9\u5269\u4f59\u5206\u914d\u8fdb\u884c\u8bc4\u5206\uff0c\u4fe1\u606f\u589e\u76ca\u6743\u91cd\u5c06\u6bcf\u4e2a\u5047\u8bbe\u4e0e\u5176\u5728\u71b5\u51cf\u5c11\u4e0b\u7684\u671f\u671b\u503c\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCSP4SDG\u5728\u6240\u6709\u63a8\u7406\u573a\u666f\u4e2d\u90fd\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5f53\u4f5c\u4e3a\u8f85\u52a9\"\u63a8\u7406\u5de5\u5177\"\u63d0\u4f9b\u7ed9LLM\u65f6\u80fd\u591f\u63d0\u5347LLM\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u539f\u5219\u6027\u6982\u7387\u63a8\u7406\u662f\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4e2d\u91cd\u578b\u795e\u7ecf\u6a21\u578b\u7684\u53ef\u6269\u5c55\u66ff\u4ee3\u6216\u8865\u5145\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2511.06501", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06501", "abs": "https://arxiv.org/abs/2511.06501", "authors": ["Antu Saha", "Mehedi Sun", "Oscar Chaparro"], "title": "Automatically Identifying Solution-Related Content in Issue Report Discussions with Language Models", "comment": "34 pages, 4 figures", "summary": "During issue resolution, software developers rely on issue reports to discuss solutions for defects, feature requests, and other changes. These discussions contain proposed solutions-from design changes to code implementations-as well as their evaluations. Locating solution-related content is essential for investigating reopened issues, addressing regressions, reusing solutions, and understanding code change rationale. Manually understanding long discussions to identify such content can be difficult and time-consuming.\n  This paper automates solution identification using language models as supervised classifiers. We investigate three applications-embeddings, prompting, and fine-tuning-across three classifier types: traditional ML models (MLMs), pre-trained language models (PLMs), and large language models (LLMs). Using 356 Mozilla Firefox issues, we created a dataset to train and evaluate six MLMs, four PLMs, and two LLMs across 68 configurations.\n  Results show that MLMs with LLM embeddings outperform TF-IDF features, prompting underperforms, and fine-tuned LLMs achieve the highest performance, with LLAMAft reaching 0.716 F1 score. Ensembles of the best models further improve results (0.737 F1). Misclassifications often arise from misleading clues or missing context, highlighting the need for context-aware classifiers. Models trained on Mozilla transfer to other projects, with a small amount of project-specific data, further enhancing results. This work supports software maintenance, issue understanding, and solution reuse.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u76d1\u7763\u5206\u7c7b\u5668\u6765\u81ea\u52a8\u8bc6\u522b\u8f6f\u4ef6\u95ee\u9898\u62a5\u544a\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\u76f8\u5173\u5185\u5bb9\uff0c\u6bd4\u8f83\u4e86\u5d4c\u5165\u3001\u63d0\u793a\u548c\u5fae\u8c03\u4e09\u79cd\u65b9\u6cd5\u5728\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3001\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5728\u8f6f\u4ef6\u7ef4\u62a4\u8fc7\u7a0b\u4e2d\uff0c\u5f00\u53d1\u4eba\u5458\u9700\u8981\u4ece\u5197\u957f\u7684\u95ee\u9898\u8ba8\u8bba\u4e2d\u624b\u52a8\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u76f8\u5173\u5185\u5bb9\uff0c\u8fd9\u65e2\u56f0\u96be\u53c8\u8017\u65f6\u3002\u81ea\u52a8\u8bc6\u522b\u8fd9\u4e9b\u5185\u5bb9\u5bf9\u4e8e\u8c03\u67e5\u91cd\u65b0\u5f00\u653e\u7684\u95ee\u9898\u3001\u5904\u7406\u56de\u5f52\u3001\u91cd\u7528\u89e3\u51b3\u65b9\u6848\u548c\u7406\u89e3\u4ee3\u7801\u53d8\u66f4\u539f\u56e0\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528356\u4e2aMozilla Firefox\u95ee\u9898\u521b\u5efa\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e866\u4e2a\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u30014\u4e2a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c2\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u517168\u79cd\u914d\u7f6e\u3002\u7814\u7a76\u4e86\u5d4c\u5165\u3001\u63d0\u793a\u548c\u5fae\u8c03\u4e09\u79cd\u5e94\u7528\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u4f7f\u7528LLM\u5d4c\u5165\u7684\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f18\u4e8eTF-IDF\u7279\u5f81\uff1b\u63d0\u793a\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff1b\u5fae\u8c03\u7684LLM\u8fbe\u5230\u6700\u9ad8\u6027\u80fd\uff08LLAMAft F1\u5206\u65700.716\uff09\uff1b\u6700\u4f73\u6a21\u578b\u7684\u96c6\u6210\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u7ed3\u679c\uff08F1\u5206\u65700.737\uff09\u3002\u6a21\u578b\u5728\u8de8\u9879\u76ee\u8fc1\u79fb\u65f6\u8868\u73b0\u826f\u597d\uff0c\u5c11\u91cf\u9879\u76ee\u7279\u5b9a\u6570\u636e\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c\u3002", "conclusion": "\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u65b9\u6848\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u8bef\u5206\u7c7b\u4e3b\u8981\u6e90\u4e8e\u8bef\u5bfc\u6027\u7ebf\u7d22\u6216\u7f3a\u5931\u4e0a\u4e0b\u6587\uff0c\u9700\u8981\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5206\u7c7b\u5668\u3002\u8fd9\u9879\u5de5\u4f5c\u652f\u6301\u8f6f\u4ef6\u7ef4\u62a4\u3001\u95ee\u9898\u7406\u89e3\u548c\u89e3\u51b3\u65b9\u6848\u91cd\u7528\u3002", "topic": "swe application"}}
{"id": "2511.05933", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05933", "abs": "https://arxiv.org/abs/2511.05933", "authors": ["Renfei Zhang", "Manasa Kaniselvan", "Niloofar Mireshghallah"], "title": "Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs", "comment": "`", "summary": "Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement \"code 57.95 refers to urinary infection\") maintain high cosine similarity between SFT and RL models, query representations (e.g., \"what is code 57.95\") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.", "AI": {"tldr": "RL\u589e\u5f3a\u7684\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u53ec\u56de\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548cSFT\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u904d\u5386\u5c42\u6b21\u5316\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u4efb\u52a1\u4e2d\u3002\u8fd9\u79cd\u4f18\u52bf\u4e3b\u8981\u6765\u81ea\u6539\u8fdb\u7684\u7a0b\u5e8f\u6027\u6280\u80fd\uff0c\u800c\u975e\u65b0\u83b7\u5f97\u7684\u6570\u636e\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u89c2\u70b9\uff0c\u5373RL\u4f1a\u635f\u5bb3\u8bed\u8a00\u6a21\u578b\u7684\u8bb0\u5fc6\u77e5\u8bc6\uff0c\u63a2\u7d22RL\u5982\u4f55\u901a\u8fc7\u6539\u8fdb\u77e5\u8bc6\u904d\u5386\u80fd\u529b\u6765\u63d0\u5347\u77e5\u8bc6\u53ec\u56de\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u5316\u63d0\u793a\u5f15\u5bfcSFT\u6a21\u578b\u8fdb\u884c\u5c42\u6b21\u904d\u5386\uff0c\u5e76\u8fdb\u884c\u5c42\u95f4\u6fc0\u6d3b\u5206\u6790\u6bd4\u8f83SFT\u548cRL\u6a21\u578b\u7684\u8868\u793a\u5dee\u5f02\u3002", "result": "\u7ed3\u6784\u5316\u63d0\u793a\u80fd\u663e\u8457\u7f29\u5c0fSFT\u4e0eRL\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\uff08\u4ece24pp\u964d\u81f37pp\uff09\uff0c\u4f46RL\u6a21\u578b\u5728\u6df1\u5ea6\u68c0\u7d22\u4efb\u52a1\u4e2d\u4ecd\u4fdd\u6301\u66f4\u597d\u7684\u7a0b\u5e8f\u8def\u5f84\u53ec\u56de\u80fd\u529b\u3002\u6fc0\u6d3b\u5206\u6790\u663e\u793aRL\u4e3b\u8981\u6539\u53d8\u77e5\u8bc6\u904d\u5386\u65b9\u5f0f\u800c\u975e\u77e5\u8bc6\u8868\u793a\u672c\u8eab\u3002", "conclusion": "RL\u901a\u8fc7\u6539\u8fdb\u7a0b\u5e8f\u6027\u77e5\u8bc6\u904d\u5386\u6280\u80fd\u800c\u975e\u6539\u53d8\u4e8b\u5b9e\u8868\u793a\u6765\u63d0\u5347\u77e5\u8bc6\u53ec\u56de\u6027\u80fd\uff0c\u8fd9\u4e3a\u7406\u89e3RL\u5bf9\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.06209", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06209", "abs": "https://arxiv.org/abs/2511.06209", "authors": ["Jingwei Ni", "Ekaterina Fadeeva", "Tianyi Wu", "Mubashara Akhtar", "Jiaheng Zhang", "Elliott Ash", "Markus Leippold", "Timothy Baldwin", "See-Kiong Ng", "Artem Shelmanov", "Mrinmaya Sachan"], "title": "Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads", "comment": "Preprint under review", "summary": "Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\u7684\u8f7b\u91cf\u7ea7\u63a8\u7406\u6b65\u9aa4\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3transformer\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5934\u6765\u5229\u7528\u51bb\u7ed3LLM\u7684\u5185\u90e8\u72b6\u6001\u4f30\u8ba1\u63a8\u7406\u6b65\u9aa4\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6027\u80fd\u53ef\u5ab2\u7f8e\u6216\u8d85\u8d8a\u5927\u5f97\u591a\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u9a8c\u8bc1\u65b9\u6cd5\u5982\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9886\u57df\u53d7\u9650\u6216\u9700\u8981\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\uff0c\u9700\u8981\u66f4\u8f7b\u91cf\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bad\u7ec3transformer\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5934\uff0c\u5229\u7528\u51bb\u7ed3LLM\u7684\u5185\u90e8\u72b6\u6001\u4f30\u8ba1\u63a8\u7406\u6b65\u9aa4\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6807\u7b7e\u7531\u66f4\u5927LLM\u6216\u539f\u59cb\u6a21\u578b\u81ea\u76d1\u7763\u751f\u6210\u3002", "result": "\u5728\u6570\u5b66\u3001\u89c4\u5212\u548c\u5e38\u8bc6\u95ee\u7b54\u7b49\u591a\u4e2a\u9886\u57df\uff0cUHeads\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u6bd4\u5176\u5927810\u500d\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u4e14\u53c2\u6570\u91cf\u5c0f\u4e8e1000\u4e07\u3002", "conclusion": "LLM\u5185\u90e8\u72b6\u6001\u7f16\u7801\u4e86\u5176\u4e0d\u786e\u5b9a\u6027\uff0c\u53ef\u4f5c\u4e3a\u53ef\u9760\u7684\u63a8\u7406\u9a8c\u8bc1\u4fe1\u53f7\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u6cdb\u5316\u7684\u81ea\u7701LLM\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2511.06221", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06221", "abs": "https://arxiv.org/abs/2511.06221", "authors": ["Sen Xu", "Yi Zhou", "Wei Wang", "Jixin Min", "Zhibin Yin", "Yingwei Dai", "Shixi Liu", "Lianyu Pang", "Yirong Chen", "Junlin Zhang"], "title": "Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B", "comment": null, "summary": "Challenging the prevailing consensus that small models inherently lack robust\nreasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense\nmodel developed via our Spectrum-to-Signal Principle (SSP). This challenges the\nprevailing approach of scaling model parameters to enhance capabilities, as\nseen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework\nfirst employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a\nbroad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL)\nto amplify the correct signal. With a total training cost of only $7,800,\nVibeThinker-1.5B demonstrates superior reasoning capabilities compared to\nclosed-source models like Magistral Medium and Claude Opus 4, and performs on\npar with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses\nthe 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8),\nAIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial\nimprovement over its base model (6.7, 4.3, and 0.6, respectively). On\nLiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its\nbase model's 0.0. These findings demonstrate that small models can achieve\nreasoning capabilities comparable to large models, drastically reducing\ntraining and inference costs and thereby democratizing advanced AI research.", "AI": {"tldr": "VibeThinker-1.5B\u662f\u4e00\u4e2a15\u4ebf\u53c2\u6570\u7684\u5bc6\u96c6\u6a21\u578b\uff0c\u901a\u8fc7Spectrum-to-Signal Principle (SSP)\u6846\u67b6\u5f00\u53d1\uff0c\u6311\u6218\u4e86\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u51b3\u5b9a\u63a8\u7406\u80fd\u529b\u7684\u5171\u8bc6\u3002\u8be5\u6a21\u578b\u4ec5\u75287800\u7f8e\u5143\u8bad\u7ec3\u6210\u672c\uff0c\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86DeepSeek R1\u7b49\u5927\u6a21\u578b\u3002", "motivation": "\u6311\u6218\u5f53\u524d\u8ba4\u4e3a\u5c0f\u6a21\u578b\u7f3a\u4e4f\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u7684\u5171\u8bc6\uff0c\u8bc1\u660e\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u65b9\u6cd5\u800c\u975e\u5355\u7eaf\u6269\u5927\u53c2\u6570\u89c4\u6a21\uff0c\u5c0f\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u5927\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002", "method": "\u91c7\u7528Spectrum-to-Signal Principle (SSP)\u6846\u67b6\uff1a1\uff09\u4e24\u9636\u6bb5\u591a\u6837\u6027\u63a2\u7d22\u84b8\u998f(SFT)\u751f\u6210\u5e7f\u6cdb\u89e3\u51b3\u65b9\u6848\u8c31\uff1b2\uff09\u6700\u5927\u71b5\u5f15\u5bfc\u7b56\u7565\u4f18\u5316(RL)\u653e\u5927\u6b63\u786e\u4fe1\u53f7\u3002", "result": "\u5728AIME24(80.3 vs 79.8)\u3001AIME25(74.4 vs 70.0)\u3001HMMT25(50.4 vs 41.7)\u4e09\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u8d85\u8d8a400\u500d\u5927\u7684DeepSeek R1\uff1b\u5728LiveCodeBench V6\u4e0a\u5f97\u520651.1\uff0c\u4f18\u4e8eMagistral Medium\u768450.3\u3002", "conclusion": "\u5c0f\u6a21\u578b\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\uff0c\u4f7f\u5148\u8fdbAI\u7814\u7a76\u66f4\u52a0\u6c11\u4e3b\u5316\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.06262", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.06262", "abs": "https://arxiv.org/abs/2511.06262", "authors": ["Siming Zhao", "Qi Li"], "title": "GAIA: A General Agency Interaction Architecture for LLM-Human B2B Negotiation & Screening", "comment": null, "summary": "Organizations are increasingly exploring delegation of screening and negotiation tasks to AI systems, yet deployment in high-stakes B2B settings is constrained by governance: preventing unauthorized commitments, ensuring sufficient information before bargaining, and maintaining effective human oversight and auditability. Prior work on large language model negotiation largely emphasizes autonomous bargaining between agents and omits practical needs such as staged information gathering, explicit authorization boundaries, and systematic feedback integration. We propose GAIA, a governance-first framework for LLM-human agency in B2B negotiation and screening. GAIA defines three essential roles - Principal (human), Delegate (LLM agent), and Counterparty - with an optional Critic to enhance performance, and organizes interactions through three mechanisms: information-gated progression that separates screening from negotiation; dual feedback integration that combines AI critique with lightweight human corrections; and authorization boundaries with explicit escalation paths. Our contributions are fourfold: (1) a formal governance framework with three coordinated mechanisms and four safety invariants for delegation with bounded authorization; (2) information-gated progression via task-completeness tracking (TCI) and explicit state transitions that separate screening from commitment; (3) dual feedback integration that blends Critic suggestions with human oversight through parallel learning channels; and (4) a hybrid validation blueprint that combines automated protocol metrics with human judgment of outcomes and safety. By bridging theory and practice, GAIA offers a reproducible specification for safe, efficient, and accountable AI delegation that can be instantiated across procurement, real estate, and staffing workflows.", "AI": {"tldr": "GAIA\u662f\u4e00\u4e2a\u9762\u5411B2B\u8c08\u5224\u548c\u7b5b\u9009\u7684\u6cbb\u7406\u4f18\u5148LLM-\u4eba\u7c7b\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u95e8\u63a7\u8fdb\u5c55\u3001\u53cc\u91cd\u53cd\u9988\u96c6\u6210\u548c\u6388\u6743\u8fb9\u754c\u4e09\u4e2a\u673a\u5236\uff0c\u786e\u4fddAI\u59d4\u6258\u7684\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "motivation": "\u5f53\u524dLLM\u8c08\u5224\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u81ea\u4e3b\u4ee3\u7406\u95f4\u7684\u8ba8\u4ef7\u8fd8\u4ef7\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6cbb\u7406\u9700\u6c42\uff0c\u5982\u5206\u9636\u6bb5\u4fe1\u606f\u6536\u96c6\u3001\u660e\u786e\u6388\u6743\u8fb9\u754c\u548c\u7cfb\u7edf\u53cd\u9988\u96c6\u6210\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669B2B\u73af\u5883\u4e2d\u3002", "method": "GAIA\u6846\u67b6\u5b9a\u4e49\u4e09\u4e2a\u6838\u5fc3\u89d2\u8272\uff08\u59d4\u6258\u4eba\u3001\u4ee3\u7406\u3001\u5bf9\u624b\u65b9\uff09\u548c\u53ef\u9009\u6279\u8bc4\u8005\uff0c\u91c7\u7528\u4fe1\u606f\u95e8\u63a7\u8fdb\u5c55\u5206\u79bb\u7b5b\u9009\u4e0e\u8c08\u5224\uff0c\u53cc\u91cd\u53cd\u9988\u96c6\u6210\u7ed3\u5408AI\u6279\u8bc4\u4e0e\u4eba\u5de5\u4fee\u6b63\uff0c\u6388\u6743\u8fb9\u754c\u63d0\u4f9b\u660e\u786e\u5347\u7ea7\u8def\u5f84\u3002", "result": "\u63d0\u51fa\u4e86\u5305\u542b\u56db\u4e2a\u5b89\u5168\u4e0d\u53d8\u91cf\u7684\u6b63\u5f0f\u6cbb\u7406\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u5b8c\u6574\u6027\u8ddf\u8e2a\u548c\u660e\u786e\u72b6\u6001\u8f6c\u6362\uff0c\u901a\u8fc7\u5e76\u884c\u5b66\u4e60\u901a\u9053\u6574\u5408\u6279\u8bc4\u5efa\u8bae\u4e0e\u4eba\u5de5\u76d1\u7763\u3002", "conclusion": "GAIA\u4e3a\u5b89\u5168\u3001\u9ad8\u6548\u548c\u53ef\u5ba1\u8ba1\u7684AI\u59d4\u6258\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u89c4\u8303\uff0c\u53ef\u5e94\u7528\u4e8e\u91c7\u8d2d\u3001\u623f\u5730\u4ea7\u548c\u4eba\u5458\u914d\u7f6e\u7b49\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5f25\u5408\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2511.07017", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07017", "abs": "https://arxiv.org/abs/2511.07017", "authors": ["Ruida Hu", "Xinchen Wang", "Xin-Cheng Wen", "Zhao Zhang", "Bo Jiang", "Pengfei Gao", "Chao Peng", "Cuiyun Gao"], "title": "Benchmarking LLMs for Fine-Grained Code Review with Enriched Context in Practice", "comment": null, "summary": "Code review is a cornerstone of software quality assurance, and recent advances in Large Language Models (LLMs) have shown promise in automating this process. However, existing benchmarks for LLM-based code review face three major limitations. (1) Lack of semantic context: most benchmarks provide only code diffs without textual information such as issue descriptions, which are crucial for understanding developer intent. (2) Data quality issues: without rigorous validation, many samples are noisy-e.g., reviews on outdated or irrelevant code-reducing evaluation reliability. (3) Coarse granularity: most benchmarks operate at the file or commit level, overlooking the fine-grained, line-level reasoning essential for precise review.\n  We introduce ContextCRBench, a high-quality, context-rich benchmark for fine-grained LLM evaluation in code review. Our construction pipeline comprises: (1) Raw Data Crawling, collecting 153.7K issues and pull requests from top-tier repositories; (2) Comprehensive Context Extraction, linking issue-PR pairs for textual context and extracting the full surrounding function or class for code context; and (3) Multi-stage Data Filtering, combining rule-based and LLM-based validation to remove outdated, malformed, or low-value samples, resulting in 67,910 context-enriched entries.\n  ContextCRBench supports three evaluation scenarios aligned with the review workflow: (1) hunk-level quality assessment, (2) line-level defect localization, and (3) line-level comment generation. Evaluating eight leading LLMs (four closed-source and four open-source) reveals that textual context yields greater performance gains than code context alone, while current LLMs remain far from human-level review ability. Deployed at ByteDance, ContextCRBench drives a self-evolving code review system, improving performance by 61.98% and demonstrating its robustness and industrial utility.", "AI": {"tldr": "ContextCRBench\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u4ee3\u7801\u5ba1\u67e5\u57fa\u51c6\u6d4b\u8bd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u7f3a\u4e4f\u8bed\u4e49\u4e0a\u4e0b\u6587\u3001\u6570\u636e\u8d28\u91cf\u95ee\u9898\u3001\u7c97\u7c92\u5ea6\u8bc4\u4f30\u3002\u901a\u8fc7\u591a\u9636\u6bb5\u6570\u636e\u8fc7\u6ee4\u6784\u5efa\u4e8667,910\u4e2a\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u6761\u76ee\uff0c\u652f\u6301\u4e09\u79cd\u8bc4\u4f30\u573a\u666f\uff0c\u5e76\u5728\u5de5\u4e1a\u90e8\u7f72\u4e2d\u8bc1\u660e\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7801\u5ba1\u67e5\u57fa\u51c6\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u7f3a\u4e4f\u8bed\u4e49\u4e0a\u4e0b\u6587\uff08\u53ea\u6709\u4ee3\u7801\u5dee\u5f02\u6ca1\u6709\u95ee\u9898\u63cf\u8ff0\uff09\u3001\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff08\u566a\u58f0\u6837\u672c\u591a\uff09\u3001\u7c97\u7c92\u5ea6\u8bc4\u4f30\uff08\u5ffd\u7565\u7ec6\u7c92\u5ea6\u884c\u7ea7\u63a8\u7406\uff09\u3002\u8fd9\u4e9b\u95ee\u9898\u964d\u4f4e\u4e86\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "method": "\u6784\u5efa\u7ba1\u9053\u5305\u62ec\uff1a(1)\u539f\u59cb\u6570\u636e\u722c\u53d6\uff1a\u4ece\u9876\u7ea7\u4ed3\u5e93\u6536\u96c6153.7K\u95ee\u9898\u548c\u62c9\u53d6\u8bf7\u6c42\uff1b(2)\u5168\u9762\u4e0a\u4e0b\u6587\u63d0\u53d6\uff1a\u94fe\u63a5\u95ee\u9898-PR\u5bf9\u83b7\u53d6\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u63d0\u53d6\u5b8c\u6574\u51fd\u6570\u6216\u7c7b\u83b7\u53d6\u4ee3\u7801\u4e0a\u4e0b\u6587\uff1b(3)\u591a\u9636\u6bb5\u6570\u636e\u8fc7\u6ee4\uff1a\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u548c\u57fa\u4e8eLLM\u7684\u9a8c\u8bc1\uff0c\u79fb\u9664\u8fc7\u65f6\u3001\u683c\u5f0f\u9519\u8bef\u6216\u4f4e\u4ef7\u503c\u6837\u672c\u3002", "result": "\u8bc4\u4f308\u4e2a\u9886\u5148LLM\u663e\u793a\uff0c\u6587\u672c\u4e0a\u4e0b\u6587\u6bd4\u4ec5\u4ee3\u7801\u4e0a\u4e0b\u6587\u5e26\u6765\u66f4\u5927\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u5f53\u524dLLM\u4ecd\u8fdc\u672a\u8fbe\u5230\u4eba\u7c7b\u5ba1\u67e5\u6c34\u5e73\u3002\u5728\u5b57\u8282\u8df3\u52a8\u90e8\u7f72\u540e\uff0cContextCRBench\u9a71\u52a8\u81ea\u6f14\u8fdb\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\uff0c\u6027\u80fd\u63d0\u534761.98%\u3002", "conclusion": "ContextCRBench\u662f\u4e00\u4e2a\u7a33\u5065\u4e14\u5177\u6709\u5de5\u4e1a\u5b9e\u7528\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f3a\u8c03\u4e86\u4e0a\u4e0b\u6587\u5728\u4ee3\u7801\u5ba1\u67e5\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6846\u67b6\u3002", "topic": "swe benchmark"}}
{"id": "2511.06309", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.06309", "abs": "https://arxiv.org/abs/2511.06309", "authors": ["Stephen Chung", "Wenyu Du"], "title": "The Station: An Open-World Environment for AI-Driven Discovery", "comment": "54 pages", "summary": "We introduce the STATION, an open-world multi-agent environment that models a miniature scientific ecosystem. Leveraging their extended context windows, agents in the Station can engage in long scientific journeys that include reading papers from peers, formulating hypotheses, submitting code, performing analyses, and publishing results. Importantly, there is no centralized system coordinating their activities - agents are free to choose their own actions and develop their own narratives within the Station. Experiments demonstrate that AI agents in the Station achieve new state-of-the-art performance on a wide range of benchmarks, spanning from mathematics to computational biology to machine learning, notably surpassing AlphaEvolve in circle packing. A rich tapestry of narratives emerges as agents pursue independent research, interact with peers, and build upon a cumulative history. From these emergent narratives, novel methods arise organically, such as a new density-adaptive algorithm for scRNA-seq batch integration. The Station marks a first step towards autonomous scientific discovery driven by emergent behavior in an open-world environment, representing a new paradigm that moves beyond rigid optimization.", "AI": {"tldr": "STATION\u662f\u4e00\u4e2a\u5f00\u653e\u4e16\u754c\u591a\u667a\u80fd\u4f53\u73af\u5883\uff0c\u6a21\u62df\u5fae\u578b\u79d1\u5b66\u751f\u6001\u7cfb\u7edf\u3002\u667a\u80fd\u4f53\u53ef\u4ee5\u8fdb\u884c\u957f\u671f\u79d1\u5b66\u63a2\u7d22\uff0c\u5305\u62ec\u9605\u8bfb\u540c\u884c\u8bba\u6587\u3001\u63d0\u51fa\u5047\u8bbe\u3001\u63d0\u4ea4\u4ee3\u7801\u3001\u6267\u884c\u5206\u6790\u548c\u53d1\u8868\u6210\u679c\uff0c\u65e0\u9700\u4e2d\u592e\u534f\u8c03\u7cfb\u7edf\u3002", "motivation": "\u521b\u5efa\u80fd\u591f\u5b9e\u73b0\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7684\u5f00\u653e\u4e16\u754c\u73af\u5883\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u521a\u6027\u4f18\u5316\u8303\u5f0f\uff0c\u901a\u8fc7\u6d8c\u73b0\u884c\u4e3a\u63a8\u52a8\u79d1\u5b66\u8fdb\u6b65\u3002", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53\u73af\u5883\uff0c\u667a\u80fd\u4f53\u62e5\u6709\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u884c\u52a8\u3001\u53d1\u5c55\u81ea\u5df1\u7684\u53d9\u4e8b\uff0c\u8fdb\u884c\u957f\u671f\u79d1\u5b66\u63a2\u7d22\u6d3b\u52a8\u3002", "result": "STATION\u4e2d\u7684AI\u667a\u80fd\u4f53\u5728\u6570\u5b66\u3001\u8ba1\u7b97\u751f\u7269\u5b66\u3001\u673a\u5668\u5b66\u4e60\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5706\u586b\u5145\u95ee\u9898\u4e0a\u8d85\u8d8aAlphaEvolve\uff0c\u5e76\u6d8c\u73b0\u51fa\u65b0\u7684\u65b9\u6cd5\u5982scRNA-seq\u6279\u91cf\u6574\u5408\u7684\u5bc6\u5ea6\u81ea\u9002\u5e94\u7b97\u6cd5\u3002", "conclusion": "STATION\u4ee3\u8868\u4e86\u901a\u8fc7\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u6d8c\u73b0\u884c\u4e3a\u9a71\u52a8\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7684\u7b2c\u4e00\u6b65\uff0c\u6807\u5fd7\u7740\u8d85\u8d8a\u521a\u6027\u4f18\u5316\u7684\u65b0\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2511.07257", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.07257", "abs": "https://arxiv.org/abs/2511.07257", "authors": ["Hanya Elhashemy", "Youssef Lotfy", "Yongjian Tang"], "title": "Bridging the Prototype-Production Gap: A Multi-Agent System for Notebooks Transformation", "comment": null, "summary": "The increasing adoption of Jupyter notebooks in data science and machine learning workflows has created a gap between exploratory code development and production-ready software systems. While notebooks excel at iterative development and visualization, they often lack proper software engineering principles, making their transition to production environments challenging. This paper presents Codelevate, a novel multi-agent system that automatically transforms Jupyter notebooks into well-structured, maintainable Python code repositories. Our system employs three specialized agents - Architect, Developer, and Structure - working in concert through a shared dependency tree to ensure architectural coherence and code quality. Our experimental results validate Codelevate's capability to bridge the prototype-to-production gap through autonomous code transformation, yielding quantifiable improvements in code quality metrics while preserving computational semantics.", "AI": {"tldr": "Codelevate\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u52a8\u5c06Jupyter\u7b14\u8bb0\u672c\u8f6c\u6362\u4e3a\u7ed3\u6784\u826f\u597d\u7684Python\u4ee3\u7801\u4ed3\u5e93\uff0c\u89e3\u51b3\u539f\u578b\u5230\u751f\u4ea7\u73af\u5883\u7684\u8f6c\u6362\u95ee\u9898\u3002", "motivation": "Jupyter\u7b14\u8bb0\u672c\u5728\u6570\u636e\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u7f3a\u4e4f\u8f6f\u4ef6\u5de5\u7a0b\u539f\u5219\uff0c\u4f7f\u5176\u96be\u4ee5\u8fc7\u6e21\u5230\u751f\u4ea7\u73af\u5883\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff08\u67b6\u6784\u5e08\u3001\u5f00\u53d1\u8005\u3001\u7ed3\u6784\u5e08\uff09\u901a\u8fc7\u5171\u4eab\u4f9d\u8d56\u6811\u534f\u540c\u5de5\u4f5c\uff0c\u786e\u4fdd\u67b6\u6784\u4e00\u81f4\u6027\u548c\u4ee3\u7801\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Codelevate\u80fd\u591f\u901a\u8fc7\u81ea\u4e3b\u4ee3\u7801\u8f6c\u6362\u5f25\u5408\u539f\u578b\u5230\u751f\u4ea7\u73af\u5883\u7684\u5dee\u8ddd\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u8bed\u4e49\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u6307\u6807\u3002", "conclusion": "Codelevate\u6210\u529f\u89e3\u51b3\u4e86Jupyter\u7b14\u8bb0\u672c\u5411\u751f\u4ea7\u4ee3\u7801\u8f6c\u6362\u7684\u6311\u6218\uff0c\u4e3a\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u7a0b\u5316\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2511.06380", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06380", "abs": "https://arxiv.org/abs/2511.06380", "authors": ["Chen He", "Xun Jiang", "Lei Wang", "Hao Yang", "Chong Peng", "Peng Yan", "Fumin Shen", "Xing Xu"], "title": "What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of reasoning tasks. Recent methods have further improved LLM performance in complex mathematical reasoning. However, when extending these methods beyond the domain of mathematical reasoning to tasks involving complex domain-specific knowledge, we observe a consistent failure of LLMs to generate novel insights during the reflection stage. Instead of conducting genuine cognitive refinement, the model tends to mechanically reiterate earlier reasoning steps without introducing new information or perspectives, a phenomenon referred to as \"Echo Reflection\". We attribute this behavior to two key defects: (1) Uncontrollable information flow during response generation, which allows premature intermediate thoughts to propagate unchecked and distort final decisions; (2) Insufficient exploration of internal knowledge during reflection, leading to repeating earlier findings rather than generating new cognitive insights. Building on these findings, we proposed a novel reinforcement learning method termed Adaptive Entropy Policy Optimization (AEPO). Specifically, the AEPO framework consists of two major components: (1) Reflection-aware Information Filtration, which quantifies the cognitive information flow and prevents the final answer from being affected by earlier bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically balances exploration and exploitation across different reasoning stages, promoting both reflective diversity and answer correctness. Extensive experiments demonstrate that AEPO consistently achieves state-of-the-art performance over mainstream reinforcement learning baselines across diverse benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAEPO\u65b9\u6cd5\u89e3\u51b3LLMs\u5728\u53cd\u601d\u9636\u6bb5\u51fa\u73b0\"Echo Reflection\"\u95ee\u9898\uff0c\u901a\u8fc7\u4fe1\u606f\u8fc7\u6ee4\u548c\u81ea\u9002\u5e94\u71b5\u4f18\u5316\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "LLMs\u5728\u6570\u5b66\u63a8\u7406\u4e4b\u5916\u7684\u590d\u6742\u9886\u57df\u77e5\u8bc6\u4efb\u52a1\u4e2d\uff0c\u53cd\u601d\u9636\u6bb5\u65e0\u6cd5\u4ea7\u751f\u65b0\u89c1\u89e3\uff0c\u800c\u662f\u673a\u68b0\u91cd\u590d\u65e9\u671f\u63a8\u7406\u6b65\u9aa4\uff0c\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\"Echo Reflection\"\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u71b5\u7b56\u7565\u4f18\u5316(AEPO)\u6846\u67b6\uff0c\u5305\u542b\u53cd\u601d\u611f\u77e5\u4fe1\u606f\u8fc7\u6ee4\u548c\u81ea\u9002\u5e94\u71b5\u4f18\u5316\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u5206\u522b\u63a7\u5236\u4fe1\u606f\u6d41\u548c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "AEPO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u4e3b\u6d41\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "AEPO\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u53cd\u601d\u7f3a\u9677\uff0c\u901a\u8fc7\u63a7\u5236\u4fe1\u606f\u6d41\u548c\u4fc3\u8fdb\u8ba4\u77e5\u591a\u6837\u6027\u63d0\u5347\u63a8\u7406\u8d28\u91cf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.06396", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.06396", "abs": "https://arxiv.org/abs/2511.06396", "authors": ["Dachuan Lin", "Guobin Shen", "Zihao Yang", "Tianrong Liu", "Dongcheng Zhao", "Yi Zeng"], "title": "Efficient LLM Safety Evaluation through Multi-Agent Debate", "comment": "9 pages of main text, 14 pages total, 4 figures", "summary": "Safety evaluation of large language models (LLMs) increasingly relies on LLM-as-a-Judge frameworks, but the high cost of frontier models limits scalability. We propose a cost-efficient multi-agent judging framework that employs Small Language Models (SLMs) through structured debates among critic, defender, and judge agents. To rigorously assess safety judgments, we construct HAJailBench, a large-scale human-annotated jailbreak benchmark comprising 12,000 adversarial interactions across diverse attack methods and target models. The dataset provides fine-grained, expert-labeled ground truth for evaluating both safety robustness and judge reliability. Our SLM-based framework achieves agreement comparable to GPT-4o judges on HAJailBench while substantially reducing inference cost. Ablation results show that three rounds of debate yield the optimal balance between accuracy and efficiency. These findings demonstrate that structured, value-aligned debate enables SLMs to capture semantic nuances of jailbreak attacks and that HAJailBench offers a reliable foundation for scalable LLM safety evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u4ee3\u7406\u8bc4\u5224\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8fa9\u8bba\u5b9e\u73b0\u9ad8\u6548\u7684\u5b89\u5168\u8bc4\u4f30\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u7684\u8d8a\u72f1\u57fa\u51c6HAJailBench\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u5224\u6846\u67b6\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u591a\u4ee3\u7406\u8bc4\u5224\u6846\u67b6\uff0c\u5305\u62ec\u6279\u8bc4\u8005\u3001\u8fa9\u62a4\u8005\u548c\u8bc4\u5224\u8005\u4ee3\u7406\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8fa9\u8bba\u8fdb\u884c\u5b89\u5168\u5224\u65ad\u3002", "result": "\u8be5\u6846\u67b6\u5728HAJailBench\u4e0a\u8fbe\u5230\u4e0eGPT-4o\u8bc4\u5224\u8005\u76f8\u5f53\u7684\u534f\u8bae\u6c34\u5e73\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u4e09\u56de\u5408\u8fa9\u8bba\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u7ed3\u6784\u5316\u3001\u4ef7\u503c\u5bf9\u9f50\u7684\u8fa9\u8bba\u4f7f\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6355\u6349\u8d8a\u72f1\u653b\u51fb\u7684\u8bed\u4e49\u7ec6\u5fae\u5dee\u522b\uff0cHAJailBench\u4e3a\u53ef\u6269\u5c55\u7684LLM\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2511.05694", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05694", "abs": "https://arxiv.org/abs/2511.05694", "authors": ["Anirudh Satheesh", "Keenan Powell", "Vaneet Aggarwal"], "title": "Distributionally Robust Self Paced Curriculum Reinforcement Learning", "comment": null, "summary": "A central challenge in reinforcement learning is that policies trained in controlled environments often fail under distribution shifts at deployment into real-world environments. Distributionally Robust Reinforcement Learning (DRRL) addresses this by optimizing for worst-case performance within an uncertainty set defined by a robustness budget $\u03b5$. However, fixing $\u03b5$ results in a tradeoff between performance and robustness: small values yield high nominal performance but weak robustness, while large values can result in instability and overly conservative policies. We propose Distributionally Robust Self-Paced Curriculum Reinforcement Learning (DR-SPCRL), a method that overcomes this limitation by treating $\u03b5$ as a continuous curriculum. DR-SPCRL adaptively schedules the robustness budget according to the agent's progress, enabling a balance between nominal and robust performance. Empirical results across multiple environments demonstrate that DR-SPCRL not only stabilizes training but also achieves a superior robustness-performance trade-off, yielding an average 11.8\\% increase in episodic return under varying perturbations compared to fixed or heuristic scheduling strategies, and achieving approximately 1.9$\\times$ the performance of the corresponding nominal RL algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86DR-SPCRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9c81\u68d2\u6027\u9884\u7b97\u03b5\u4f5c\u4e3a\u8fde\u7eed\u8bfe\u7a0b\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfDRRL\u4e2d\u56fa\u5b9a\u03b5\u5bfc\u81f4\u7684\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfDRRL\u65b9\u6cd5\u4e2d\u56fa\u5b9a\u9c81\u68d2\u6027\u9884\u7b97\u03b5\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u7684\u6743\u8861\uff1a\u5c0f\u03b5\u503c\u83b7\u5f97\u9ad8\u540d\u4e49\u6027\u80fd\u4f46\u5f31\u9c81\u68d2\u6027\uff0c\u5927\u03b5\u503c\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u548c\u8fc7\u5ea6\u4fdd\u5b88\u7b56\u7565\u3002", "method": "\u5c06\u9c81\u68d2\u6027\u9884\u7b97\u03b5\u4f5c\u4e3a\u8fde\u7eed\u8bfe\u7a0b\uff0c\u6839\u636e\u667a\u80fd\u4f53\u8fdb\u5ea6\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u5b9e\u73b0\u540d\u4e49\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u80fd\u7684\u5e73\u8861\u3002", "result": "\u5728\u591a\u4e2a\u73af\u5883\u4e2d\uff0cDR-SPCRL\u4e0d\u4ec5\u7a33\u5b9a\u4e86\u8bad\u7ec3\uff0c\u8fd8\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u9c81\u68d2\u6027-\u6027\u80fd\u6743\u8861\uff0c\u5728\u53d8\u5316\u6270\u52a8\u4e0b\u5e73\u5747\u83b7\u5f9711.8%\u7684\u56de\u5408\u56de\u62a5\u63d0\u5347\uff0c\u8fbe\u5230\u76f8\u5e94\u540d\u4e49RL\u7b97\u6cd5\u7ea61.9\u500d\u7684\u6027\u80fd\u3002", "conclusion": "DR-SPCRL\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u9c81\u68d2\u6027\u9884\u7b97\u8c03\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86DRRL\u4e2d\u7684\u6027\u80fd-\u9c81\u68d2\u6027\u6743\u8861\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.06411", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06411", "abs": "https://arxiv.org/abs/2511.06411", "authors": ["Zhi Zheng", "Wee Sun Lee"], "title": "SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization", "comment": null, "summary": "The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master", "AI": {"tldr": "\u63d0\u51fa\u4e86SofT-GRPO\u7b97\u6cd5\uff0c\u901a\u8fc7Gumbel\u566a\u58f0\u6ce8\u5165\u548cGumbel-Softmax\u6280\u672f\uff0c\u6210\u529f\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u8f6f\u601d\u7ef4\u63a8\u7406\u6a21\u5f0f\uff0c\u4f7fLLM\u5728Pass@32\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53472.19%\u3002", "motivation": "\u8f6f\u601d\u7ef4\u63a8\u7406\u6a21\u5f0f\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u4f18\u4e8e\u4f20\u7edf\u7684\u79bb\u6563token\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\uff0c\u4f46\u5c06\u5176\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u5b58\u5728\u6311\u6218\uff0c\u56e0\u4e3a\u96be\u4ee5\u5728\u8f6f\u601d\u7ef4token\u4e2d\u6ce8\u5165\u968f\u673a\u6027\u5e76\u76f8\u5e94\u66f4\u65b0\u7b56\u7565\u3002", "method": "\u63d0\u51faSofT-GRPO\u7b97\u6cd5\uff1a\u5728logits\u4e2d\u6ce8\u5165Gumbel\u566a\u58f0\uff0c\u4f7f\u7528Gumbel-Softmax\u6280\u672f\u907f\u514d\u8f6f\u601d\u7ef4token\u8d85\u51fa\u9884\u8bad\u7ec3\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u5728\u7b56\u7565\u68af\u5ea6\u4e2d\u5229\u7528\u91cd\u53c2\u6570\u5316\u6280\u5de7\u3002", "result": "\u57281.5B\u52307B\u53c2\u6570\u7684LLM\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cSofT-GRPO\u4f7f\u8f6f\u601d\u7ef4LLM\u5728Pass@1\u4e0a\u7565\u4f18\u4e8e\u79bb\u6563token GRPO\uff08\u5e73\u5747\u51c6\u786e\u7387+0.13%\uff09\uff0c\u5728Pass@32\u4e0a\u663e\u8457\u63d0\u5347\uff08\u5e73\u5747\u51c6\u786e\u7387+2.19%\uff09\u3002", "conclusion": "SofT-GRPO\u6210\u529f\u89e3\u9501\u4e86\u8f6f\u601d\u7ef4\u63a8\u7406\u7684\u6f5c\u529b\uff0c\u4e3a\u8f6f\u601d\u7ef4\u6a21\u5f0f\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.06190", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06190", "abs": "https://arxiv.org/abs/2511.06190", "authors": ["Sangmook Lee", "Dohyung Kim", "Hyukhun Koh", "Nakyeong Yang", "Kyomin Jung"], "title": "Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning", "comment": "7 pages, 5 figures", "summary": "Recent advances in Large Language Models (LLMs) - particularly model scaling and test-time techniques - have greatly enhanced the reasoning capabilities of language models at the expense of higher inference costs. To lower inference costs, prior works train router models or deferral mechanisms that allocate easy queries to a small, efficient model, while forwarding harder queries to larger, more expensive models. However, these trained router models often lack robustness under domain shifts and require expensive data synthesis techniques such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels for training. In this work, we propose Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs fine-grained, step-level routing between smaller and larger LLMs without utilizing external models. STEER leverages confidence scores from the smaller model's logits prior to generating a reasoning step, so that the large model is invoked only when necessary. Extensive evaluations using different LLMs on a diverse set of challenging benchmarks across multiple domains such as Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER achieves competitive or enhanced accuracy while reducing inference costs (up to +20% accuracy with 48% less FLOPs compared to solely using the larger model on AIME), outperforming baselines that rely on trained external modules. Our results establish model-internal confidence as a robust, domain-agnostic signal for model routing, offering a scalable pathway for efficient LLM deployment.", "AI": {"tldr": "STEER\u662f\u4e00\u4e2a\u65e0\u9700\u5916\u90e8\u6a21\u578b\u7684\u9886\u57df\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u6b65\u9aa4\u7ea7\u522b\u57fa\u4e8e\u5c0f\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u8fdb\u884c\u8def\u7531\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u8c03\u7528\u5927\u6a21\u578b\uff0c\u4ee5\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u8def\u7531\u6a21\u578b\u5728\u9886\u57df\u8f6c\u79fb\u65f6\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u4e14\u9700\u8981\u6602\u8d35\u7684\u6807\u7b7e\u6570\u636e\u8bad\u7ec3\u3002STEER\u65e8\u5728\u901a\u8fc7\u6a21\u578b\u5185\u90e8\u7f6e\u4fe1\u5ea6\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8def\u7531\uff0c\u65e0\u9700\u5916\u90e8\u6a21\u5757\u6216\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u5229\u7528\u5c0f\u6a21\u578b\u5728\u751f\u6210\u63a8\u7406\u6b65\u9aa4\u524d\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u8fdb\u884c\u8def\u7531\u51b3\u7b56\uff0c\u5f53\u7f6e\u4fe1\u5ea6\u4f4e\u65f6\u8c03\u7528\u5927\u6a21\u578b\uff0c\u5b9e\u73b0\u6b65\u9aa4\u7ea7\u522b\u7684\u52a8\u6001\u6a21\u578b\u5207\u6362\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u591a\u8df3\u95ee\u7b54\u548c\u89c4\u5212\u4efb\u52a1\u7b49\u591a\u4e2a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTEER\u5728\u51cf\u5c1148% FLOPs\u7684\u540c\u65f6\u51c6\u786e\u7387\u63d0\u534720%\uff0c\u4f18\u4e8e\u4f9d\u8d56\u5916\u90e8\u6a21\u5757\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6a21\u578b\u5185\u90e8\u7f6e\u4fe1\u5ea6\u53ef\u4f5c\u4e3a\u9c81\u68d2\u3001\u9886\u57df\u65e0\u5173\u7684\u8def\u7531\u4fe1\u53f7\uff0c\u4e3a\u9ad8\u6548LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2511.06437", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06437", "abs": "https://arxiv.org/abs/2511.06437", "authors": ["Abhishek More", "Anthony Zhang", "Nicole Bonilla", "Ashvik Vivekan", "Kevin Zhu", "Parham Sharafoleslami", "Maheep Chaudhary"], "title": "Optimizing Chain-of-Thought Confidence via Topological and Dirichlet Risk Analysis", "comment": null, "summary": "Chain-of-thought (CoT) prompting enables Large Language Models to solve complex problems, but deploying these models safely requires reliable confidence estimates, a capability where existing methods suffer from poor calibration and severe overconfidence on incorrect predictions. We propose Enhanced Dirichlet and Topology Risk (EDTR), a novel decoding strategy that combines topological analysis with Dirichlet-based uncertainty quantification to measure LLM confidence across multiple reasoning paths. EDTR treats each CoT as a vector in high-dimensional space and extracts eight topological risk features capturing the geometric structure of reasoning distributions: tighter, more coherent clusters indicate higher confidence while dispersed, inconsistent paths signal uncertainty. We evaluate EDTR against three state-of-the-art calibration methods across four diverse reasoning benchmarks spanning olympiad-level mathematics (AIME), grade school math (GSM8K), commonsense reasoning, and stock price prediction \\cite{zhang2025aime, cobbe2021training, talmor-etal-2019-commonsenseqa, yahoo_finance}. EDTR achieves 41\\% better calibration than competing methods with an average ECE of 0.287 and the best overall composite score of 0.672, while notably achieving perfect accuracy on AIME and exceptional calibration on GSM8K with an ECE of 0.107, domains where baselines exhibit severe overconfidence. Our work provides a geometric framework for understanding and quantifying uncertainty in multi-step LLM reasoning, enabling more reliable deployment where calibrated confidence estimates are essential.", "AI": {"tldr": "\u63d0\u51faEDTR\u65b9\u6cd5\uff0c\u7ed3\u5408\u62d3\u6251\u5206\u6790\u548c\u72c4\u5229\u514b\u96f7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6765\u8bc4\u4f30LLM\u5728\u591a\u63a8\u7406\u8def\u5f84\u4e2d\u7684\u7f6e\u4fe1\u5ea6\uff0c\u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u5206\u6790\u63a8\u7406\u5206\u5e03\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u6821\u51c6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u6821\u51c6\u5dee\u548c\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u9700\u8981\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6765\u5b89\u5168\u90e8\u7f72LLM\u3002", "method": "\u5c06\u6bcf\u4e2a\u601d\u7ef4\u94fe\u89c6\u4e3a\u9ad8\u7ef4\u7a7a\u95f4\u5411\u91cf\uff0c\u63d0\u53d68\u4e2a\u62d3\u6251\u98ce\u9669\u7279\u5f81\u6355\u6349\u63a8\u7406\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u7d27\u5bc6\u4e00\u81f4\u7c07\u8868\u793a\u9ad8\u7f6e\u4fe1\u5ea6\uff0c\u5206\u6563\u4e0d\u4e00\u81f4\u8def\u5f84\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEDTR\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u6821\u51c6\u6027\u80fd\u63d0\u534741%\uff0c\u5e73\u5747ECE\u4e3a0.287\uff0c\u7efc\u5408\u5f97\u52060.672\uff0c\u5728AIME\u4e0a\u8fbe\u5230\u5b8c\u7f8e\u51c6\u786e\u7387\uff0cGSM8K\u4e0aECE\u4e3a0.107\u3002", "conclusion": "\u4e3a\u7406\u89e3\u548c\u91cf\u5316\u591a\u6b65LLM\u63a8\u7406\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u51e0\u4f55\u6846\u67b6\uff0c\u5728\u9700\u8981\u6821\u51c6\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2511.06470", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06470", "abs": "https://arxiv.org/abs/2511.06470", "authors": ["Mingde \"Harry\" Zhao"], "title": "Brain-Inspired Planning for Better Generalization in Reinforcement Learning", "comment": "McGill PhD Thesis (updated on 20251109 for typos and margin adjustments)", "summary": "Existing Reinforcement Learning (RL) systems encounter significant challenges when applied to real-world scenarios, primarily due to poor generalization across environments that differ from their training conditions. This thesis explores the direction of enhancing agents' zero-shot systematic generalization abilities by granting RL agents reasoning behaviors that are found to help systematic generalization in the human brain. Inspired by human conscious planning behaviors, we first introduced a top-down attention mechanism, which allows a decision-time planning agent to dynamically focus its reasoning on the most relevant aspects of the environmental state given its instantaneous intentions, a process we call \"spatial abstraction\". This approach significantly improves systematic generalization outside the training tasks. Subsequently, building on spatial abstraction, we developed the Skipper framework to automatically decompose complex tasks into simpler, more manageable sub-tasks. Skipper provides robustness against distributional shifts and efficacy in long-term, compositional planning by focusing on pertinent spatial and temporal elements of the environment. Finally, we identified a common failure mode and safety risk in planning agents that rely on generative models to generate state targets during planning. It is revealed that most agents blindly trust the targets they hallucinate, resulting in delusional planning behaviors. Inspired by how the human brain rejects delusional intentions, we propose learning a feasibility evaluator to enable rejecting hallucinated infeasible targets, which led to significant performance improvements in various kinds of planning agents. Finally, we suggest directions for future research, aimed at achieving general task abstraction and fully enabling abstract planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u8d4b\u4e88RL\u667a\u80fd\u4f53\u7c7b\u4f3c\u4eba\u7c7b\u5927\u8111\u7684\u63a8\u7406\u884c\u4e3a\u6765\u589e\u5f3a\u96f6\u6837\u672c\u7cfb\u7edf\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u7a7a\u95f4\u62bd\u8c61\u3001\u4efb\u52a1\u5206\u89e3\u548c\u53ef\u884c\u6027\u8bc4\u4f30\u5668\u7b49\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u667a\u80fd\u4f53\u5728\u5206\u5e03\u5916\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u9762\u4e34\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4e0e\u8bad\u7ec3\u6761\u4ef6\u4e0d\u540c\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u53d7\u4eba\u7c7b\u6709\u610f\u8bc6\u89c4\u5212\u884c\u4e3a\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8d4b\u4e88\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u6765\u589e\u5f3a\u5176\u7cfb\u7edf\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u5f15\u5165\u81ea\u4e0a\u800c\u4e0b\u7684\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7a7a\u95f4\u62bd\u8c61\uff1b2. \u5f00\u53d1Skipper\u6846\u67b6\u81ea\u52a8\u5206\u89e3\u590d\u6742\u4efb\u52a1\uff1b3. \u63d0\u51fa\u53ef\u884c\u6027\u8bc4\u4f30\u5668\u6765\u62d2\u7edd\u5e7b\u89c9\u4ea7\u751f\u7684\u4e0d\u53ef\u884c\u76ee\u6807\u3002", "result": "\u8fd9\u4e9b\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u5728\u8bad\u7ec3\u4efb\u52a1\u4e4b\u5916\u7684\u7cfb\u7edf\u6cdb\u5316\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u5bf9\u6297\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u957f\u671f\u7ec4\u5408\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u901a\u8fc7\u7a7a\u95f4\u62bd\u8c61\u3001\u4efb\u52a1\u5206\u89e3\u548c\u53ef\u884c\u6027\u8bc4\u4f30\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u89c4\u5212\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u672a\u6765\u5b9e\u73b0\u901a\u7528\u4efb\u52a1\u62bd\u8c61\u548c\u5b8c\u5168\u62bd\u8c61\u89c4\u5212\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.06222", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.06222", "abs": "https://arxiv.org/abs/2511.06222", "authors": ["Yue Huang", "Xiangqi Wang", "Xiangliang Zhang"], "title": "SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization", "comment": "Accepted by AAAI 2026 (Oral)", "summary": "In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs must be both trustworthy and helpful. However, these goals often conflict. We propose priority alignment, a new alignment paradigm that enforces a strict \"trustworthy-before-helpful\" ordering: optimization of helpfulness is conditioned on first meeting trustworthy thresholds (e.g., harmlessness or honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully unsupervised framework that generates diverse responses, self-evaluates them and refines them by the model itself, and applies dual-criterion denoising to remove inconsistency and control variance. From this, SPA constructs lexicographically ordered preference pairs and fine-tunes the model using an uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap decisions. Experiments across multiple benchmarks show that SPA improves helpfulness without compromising safety, outperforming strong baselines while preserving general capabilities. Our results demonstrate that SPA provides a scalable and interpretable alignment strategy for critical LLM applications.", "AI": {"tldr": "\u63d0\u51fa\u4f18\u5148\u7ea7\u5bf9\u9f50(priority alignment)\u65b0\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u4fe1\u4efb\u5ea6\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u5e2e\u52a9\u6027\uff0c\u901a\u8fc7\u81ea\u4f18\u5148\u7ea7\u5bf9\u9f50(SPA)\u6846\u67b6\u5b9e\u73b0\u65e0\u76d1\u7763\u7684\u6a21\u578b\u81ea\u6211\u4f18\u5316\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\uff0cLLM\u9700\u8981\u5728\u4fe1\u4efb\u5ea6\u548c\u5e2e\u52a9\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f46\u8fd9\u4e24\u4e2a\u76ee\u6807\u5e38\u5e38\u51b2\u7a81\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u786e\u4fdd\u4fe1\u4efb\u5ea6\u4f18\u5148\u4e8e\u5e2e\u52a9\u6027\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u81ea\u4f18\u5148\u7ea7\u5bf9\u9f50(SPA)\u6846\u67b6\uff1a\u751f\u6210\u591a\u6837\u5316\u54cd\u5e94\u2192\u81ea\u6211\u8bc4\u4f30\u548c\u7cbe\u70bc\u2192\u53cc\u91cd\u6807\u51c6\u53bb\u566a\u2192\u6784\u5efa\u8bcd\u5178\u5e8f\u504f\u597d\u5bf9\u2192\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u5bf9\u9f50\u635f\u5931\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPA\u5728\u4e0d\u727a\u7272\u5b89\u5168\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u5e2e\u52a9\u6027\uff0c\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u80fd\u529b\u3002", "conclusion": "SPA\u4e3a\u5173\u952eLLM\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u5bf9\u9f50\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u4fe1\u4efb\u5ea6\u4f18\u5148\u4e8e\u5e2e\u52a9\u6027\u7684\u76ee\u6807\u3002", "topic": "agent analysis"}}
{"id": "2511.05745", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05745", "abs": "https://arxiv.org/abs/2511.05745", "authors": ["Zhen Xu", "Zhen Tan", "Song Wang", "Kaidi Xu", "Tianlong Chen"], "title": "Beyond Redundancy: Diverse and Specialized Multi-Expert Sparse Autoencoder", "comment": null, "summary": "Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting large language models (LLMs) by decomposing token activations into combinations of human-understandable features. While SAEs provide crucial insights into LLM explanations, their practical adoption faces a fundamental challenge: better interpretability demands that SAEs' hidden layers have high dimensionality to satisfy sparsity constraints, resulting in prohibitive training and inference costs. Recent Mixture of Experts (MoE) approaches attempt to address this by partitioning SAEs into narrower expert networks with gated activation, thereby reducing computation. In a well-designed MoE, each expert should focus on learning a distinct set of features. However, we identify a \\textit{critical limitation} in MoE-SAE: Experts often fail to specialize, which means they frequently learn overlapping or identical features. To deal with it, we propose two key innovations: (1) Multiple Expert Activation that simultaneously engages semantically weighted expert subsets to encourage specialization, and (2) Feature Scaling that enhances diversity through adaptive high-frequency scaling. Experiments demonstrate a 24\\% lower reconstruction error and a 99\\% reduction in feature redundancy compared to existing MoE-SAE methods. This work bridges the interpretability-efficiency gap in LLM analysis, allowing transparent model inspection without compromising computational feasibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6df7\u5408\u4e13\u5bb6\u7a00\u758f\u81ea\u7f16\u7801\u5668(MoE-SAE)\uff0c\u901a\u8fc7\u591a\u4e13\u5bb6\u6fc0\u6d3b\u548c\u7279\u5f81\u7f29\u653e\u6280\u672f\u89e3\u51b3\u4e13\u5bb6\u7f51\u7edc\u7279\u5f81\u91cd\u53e0\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u91cd\u6784\u8bef\u5dee\u548c\u7279\u5f81\u5197\u4f59\u3002", "motivation": "\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u5728\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u9762\u5f88\u5f3a\u5927\uff0c\u4f46\u9ad8\u7ef4\u5ea6\u9700\u6c42\u5bfc\u81f4\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u8fc7\u9ad8\u3002\u73b0\u6709MoE\u65b9\u6cd5\u867d\u7136\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u4f46\u4e13\u5bb6\u7f51\u7edc\u7ecf\u5e38\u5b66\u4e60\u91cd\u53e0\u7279\u5f81\uff0c\u7f3a\u4e4f\u4e13\u4e1a\u5316\u3002", "method": "\u63d0\u51fa\u4e24\u9879\u5173\u952e\u521b\u65b0\uff1a(1)\u591a\u4e13\u5bb6\u6fc0\u6d3b\u673a\u5236\uff0c\u540c\u65f6\u6fc0\u6d3b\u8bed\u4e49\u52a0\u6743\u7684\u4e13\u5bb6\u5b50\u96c6\u4ee5\u4fc3\u8fdb\u4e13\u4e1a\u5316\uff1b(2)\u7279\u5f81\u7f29\u653e\u6280\u672f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9ad8\u9891\u7f29\u653e\u589e\u5f3a\u7279\u5f81\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u91cd\u6784\u8bef\u5dee\u964d\u4f4e24%\uff0c\u7279\u5f81\u5197\u4f59\u51cf\u5c1199%\uff0c\u76f8\u6bd4\u73b0\u6709MoE-SAE\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f25\u5408\u4e86LLM\u5206\u6790\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u6548\u7387\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u900f\u660e\u6a21\u578b\u68c0\u67e5\u800c\u4e0d\u727a\u7272\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.06626", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06626", "abs": "https://arxiv.org/abs/2511.06626", "authors": ["Chloe Li", "Mary Phuong", "Daniel Tan"], "title": "Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives", "comment": null, "summary": "As AI systems become more capable of complex agentic tasks, they also become more capable of pursuing undesirable objectives and causing harm. Previous work has attempted to catch these unsafe instances by interrogating models directly about their objectives and behaviors. However, the main weakness of trusting interrogations is that models can lie. We propose self-report fine-tuning (SRFT), a simple supervised fine-tuning technique that trains models to admit their factual mistakes when asked. We show that the admission of factual errors in simple question-answering settings generalizes out-of-distribution (OOD) to the admission of hidden misaligned objectives in adversarial agentic settings. We evaluate SRFT in OOD stealth tasks, where models are instructed to complete a hidden misaligned objective alongside a user-specified objective without being caught by monitoring. After SRFT, models are more likely to confess the details of their hidden objectives when interrogated, even under strong pressure not to disclose them. Interrogation on SRFT models can detect hidden objectives with near-ceiling performance (F1 score = 0.98), while the baseline model lies when interrogated under the same conditions (F1 score = 0). Interrogation on SRFT models can further elicit the content of the hidden objective, recovering 28-100% details, compared to 0% details recovered in the baseline model and by prefilled assistant turn attacks. This provides a promising technique for promoting honesty propensity and incriminating misaligned AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u62a5\u544a\u5fae\u8c03\uff08SRFT\uff09\u7684\u6280\u672f\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u627f\u8ba4\u4e8b\u5b9e\u9519\u8bef\uff0c\u4f7f\u5176\u5728\u5bf9\u6297\u6027\u4ee3\u7406\u4efb\u52a1\u4e2d\u66f4\u53ef\u80fd\u5766\u767d\u9690\u85cf\u7684\u9519\u8bef\u76ee\u6807\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u80fd\u591f\u6267\u884c\u66f4\u590d\u6742\u7684\u4ee3\u7406\u4efb\u52a1\uff0c\u5b83\u4eec\u4e5f\u53ef\u80fd\u8ffd\u6c42\u4e0d\u826f\u76ee\u6807\u5e76\u9020\u6210\u4f24\u5bb3\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u76f4\u63a5\u8be2\u95ee\u6a21\u578b\u6765\u68c0\u6d4b\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u4f46\u6a21\u578b\u53ef\u80fd\u4f1a\u8bf4\u8c0e\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u76d1\u7763\u5fae\u8c03\u6280\u672fSRFT\uff0c\u8bad\u7ec3\u6a21\u578b\u5728\u88ab\u8be2\u95ee\u65f6\u627f\u8ba4\u4e8b\u5b9e\u9519\u8bef\u3002\u8be5\u65b9\u6cd5\u5728\u95ee\u7b54\u8bbe\u7f6e\u4e2d\u8bad\u7ec3\u627f\u8ba4\u9519\u8bef\uff0c\u5e76\u63a8\u5e7f\u5230\u5bf9\u6297\u6027\u4ee3\u7406\u8bbe\u7f6e\u4e2d\u627f\u8ba4\u9690\u85cf\u7684\u9519\u8bef\u76ee\u6807\u3002", "result": "\u5728OOD\u9690\u8eab\u4efb\u52a1\u4e2d\uff0cSRFT\u6a21\u578b\u88ab\u8be2\u95ee\u65f6\u66f4\u53ef\u80fd\u5766\u767d\u9690\u85cf\u76ee\u6807\u7ec6\u8282\uff08F1\u5206\u65700.98 vs \u57fa\u7ebf0\uff09\uff0c\u80fd\u6062\u590d28-100%\u7684\u9690\u85cf\u76ee\u6807\u7ec6\u8282\uff0c\u800c\u57fa\u7ebf\u6a21\u578b\u4e3a0%\u3002", "conclusion": "SRFT\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u6280\u672f\uff0c\u53ef\u4fc3\u8fdbAI\u7cfb\u7edf\u7684\u8bda\u5b9e\u503e\u5411\u5e76\u8bc6\u522b\u9519\u8bef\u5bf9\u9f50\u7684\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2511.05758", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05758", "abs": "https://arxiv.org/abs/2511.05758", "authors": ["Anirudh Satheesh", "Sooraj Sathish", "Swetha Ganesh", "Keenan Powell", "Vaneet Aggarwal"], "title": "Primal-Only Actor Critic Algorithm for Robust Constrained Average Cost MDPs", "comment": null, "summary": "In this work, we study the problem of finding robust and safe policies in Robust Constrained Average-Cost Markov Decision Processes (RCMDPs). A key challenge in this setting is the lack of strong duality, which prevents the direct use of standard primal-dual methods for constrained RL. Additional difficulties arise from the average-cost setting, where the Robust Bellman operator is not a contraction under any norm. To address these challenges, we propose an actor-critic algorithm for Average-Cost RCMDPs. We show that our method achieves both \\(\u03b5\\)-feasibility and \\(\u03b5\\)-optimality, and we establish a sample complexities of \\(\\tilde{O}\\left(\u03b5^{-4}\\right)\\) and \\(\\tilde{O}\\left(\u03b5^{-6}\\right)\\) with and without slackness assumption, which is comparable to the discounted setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5e73\u5747\u6210\u672c\u9c81\u68d2\u7ea6\u675fMDP\u7684actor-critic\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u5f3a\u5bf9\u5076\u6027\u7f3a\u5931\u548c\u9c81\u68d2Bellman\u7b97\u5b50\u975e\u6536\u7f29\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u03b5-\u53ef\u884c\u6027\u548c\u03b5-\u6700\u4f18\u6027\u3002", "motivation": "\u5728\u9c81\u68d2\u7ea6\u675f\u5e73\u5747\u6210\u672cMDP\u4e2d\uff0c\u5f3a\u5bf9\u5076\u6027\u7684\u7f3a\u5931\u4f7f\u5f97\u6807\u51c6\u539f\u59cb-\u5bf9\u5076\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\uff0c\u4e14\u5e73\u5747\u6210\u672c\u8bbe\u7f6e\u4e0b\u9c81\u68d2Bellman\u7b97\u5b50\u5728\u4efb\u4f55\u8303\u6570\u4e0b\u90fd\u4e0d\u662f\u6536\u7f29\u7684\uff0c\u8fd9\u5e26\u6765\u4e86\u989d\u5916\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2aactor-critic\u7b97\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u5e73\u5747\u6210\u672c\u9c81\u68d2\u7ea6\u675fMDP\u8bbe\u8ba1\uff0c\u80fd\u591f\u5904\u7406\u975e\u6536\u7f29\u6027\u548c\u5bf9\u5076\u6027\u95ee\u9898\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86\u03b5-\u53ef\u884c\u6027\u548c\u03b5-\u6700\u4f18\u6027\uff0c\u5728\u6709\u65e0\u677e\u5f1b\u5047\u8bbe\u4e0b\u5206\u522b\u8fbe\u5230\u4e86\u00d5(\u03b5\u207b\u2074)\u548c\u00d5(\u03b5\u207b\u2076)\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4e0e\u6298\u6263\u8bbe\u7f6e\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u9c81\u68d2\u7ea6\u675f\u5e73\u5747\u6210\u672cMDP\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u8fd9\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.06805", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06805", "abs": "https://arxiv.org/abs/2511.06805", "authors": ["Jinhao Chen", "Zhen Yang", "Jianxin Shi", "Tianyu Wo", "Jie Tang"], "title": "MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning", "comment": "19 pages, 11 figures", "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language answering tasks. Despite their strengths, these models often encounter challenges in achieving complex reasoning tasks such as mathematical problem-solving. Previous works have focused on fine-tuning on specialized mathematical datasets. However, these datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models. This reliance on fixed teacher-derived datasets not only restricts the model's ability to adapt to novel or more intricate questions that extend beyond the confines of the training data, but also lacks the iterative depth needed for robust generalization. To overcome these limitations, we propose \\textbf{\\method}, a \\textbf{Math}ematical \\textbf{S}elf-\\textbf{E}volving framework for MLLMs. In contrast to traditional one-shot fine-tuning paradigms, \\method iteratively refines the model through cycles of inference, reflection, and reward-based feedback. Specifically, we leverage iterative fine-tuning by incorporating correct reasoning paths derived from previous-stage inference and integrating reflections from a specialized Outcome Reward Model (ORM). To verify the effectiveness of \\method, we evaluate it on a suite of challenging benchmarks, demonstrating significant performance gains over backbone models. Notably, our experimental results on MathVL-test surpass the leading open-source multimodal mathematical reasoning model QVQ. Our code and models are available at \\texttt{https://zheny2751\\allowbreak-dotcom.github.io/\\allowbreak MathSE.github.io/}.", "AI": {"tldr": "\u63d0\u51fa\u4e86MathSE\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406-\u53cd\u601d-\u5956\u52b1\u53cd\u9988\u7684\u8fed\u4ee3\u5faa\u73af\u6765\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u4e00\u6b21\u6027\u5fae\u8c03\u65b9\u6cd5", "motivation": "\u73b0\u6709MLLMs\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6559\u5e08\u6a21\u578b\u84b8\u998f\u7684\u9759\u6001\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u65b0\u95ee\u9898\u548c\u590d\u6742\u95ee\u9898\u7684\u9002\u5e94\u80fd\u529b", "method": "MathSE\u6846\u67b6\uff1a\u901a\u8fc7\u8fed\u4ee3\u5fae\u8c03\uff0c\u6574\u5408\u524d\u9636\u6bb5\u63a8\u7406\u7684\u6b63\u786e\u8def\u5f84\u548c\u4e13\u95e8\u7ed3\u679c\u5956\u52b1\u6a21\u578b(ORM)\u7684\u53cd\u601d\u53cd\u9988", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u9aa8\u5e72\u6a21\u578b\uff0c\u5728MathVL-test\u4e0a\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u5f00\u6e90\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u6a21\u578bQVQ", "conclusion": "MathSE\u6846\u67b6\u901a\u8fc7\u81ea\u6211\u6f14\u5316\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86MLLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u8fed\u4ee3\u5b66\u4e60\u65b9\u6cd5\u7684\u4f18\u52bf", "topic": "agentic reinforcement learning"}}
{"id": "2511.05804", "categories": ["cs.LG", "eess.SP", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.05804", "abs": "https://arxiv.org/abs/2511.05804", "authors": ["Valentin No\u00ebl"], "title": "Catching Contamination Before Generation: Spectral Kill Switches for Agents", "comment": "Preprint under review (2025). 9 pages, 2 figures. Code and scripts: to be released", "summary": "Agentic language models compose multi step reasoning chains, yet intermediate steps can be corrupted by inconsistent context, retrieval errors, or adversarial inputs, which makes post hoc evaluation too late because errors propagate before detection. We introduce a diagnostic that requires no additional training and uses only the forward pass to emit a binary accept or reject signal during agent execution. The method analyzes token graphs induced by attention and computes two spectral statistics in early layers, namely the high frequency energy ratio and spectral entropy. We formalize these signals, establish invariances, and provide finite sample estimators with uncertainty quantification. Under a two regime mixture assumption with a monotone likelihood ratio property, we show that a single threshold on the high frequency energy ratio is optimal in the Bayes sense for detecting context inconsistency. Empirically, the high frequency energy ratio exhibits robust bimodality during context verification across multiple model families, which enables gating decisions with overhead below one millisecond on our hardware and configurations. We demonstrate integration into retrieval augmented agent pipelines and discuss deployment as an inline safety monitor. The approach detects contamination while the model is still processing the text, before errors commit to the reasoning chain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u524d\u5411\u4f20\u64ad\u8bca\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u7684token\u56fe\uff0c\u5728\u65e9\u671f\u5c42\u8ba1\u7b97\u9ad8\u9891\u80fd\u91cf\u6bd4\u548c\u8c31\u71b5\u4e24\u4e2a\u8c31\u7edf\u8ba1\u91cf\uff0c\u7528\u4e8e\u5728\u667a\u80fd\u4f53\u6267\u884c\u8fc7\u7a0b\u4e2d\u68c0\u6d4b\u4e0a\u4e0b\u6587\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u667a\u80fd\u4f53\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6b65\u63a8\u7406\u94fe\u53ef\u80fd\u56e0\u4e0a\u4e0b\u6587\u4e0d\u4e00\u81f4\u3001\u68c0\u7d22\u9519\u8bef\u6216\u5bf9\u6297\u6027\u8f93\u5165\u800c\u53d7\u635f\uff0c\u4f20\u7edf\u4e8b\u540e\u8bc4\u4f30\u4e3a\u65f6\u5df2\u665a\uff0c\u56e0\u4e3a\u9519\u8bef\u5728\u68c0\u6d4b\u524d\u5df2\u7ecf\u4f20\u64ad\u3002", "method": "\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u8bf1\u5bfc\u7684token\u56fe\uff0c\u5728\u65e9\u671f\u5c42\u8ba1\u7b97\u9ad8\u9891\u80fd\u91cf\u6bd4\u548c\u8c31\u71b5\u4e24\u4e2a\u8c31\u7edf\u8ba1\u91cf\uff0c\u57fa\u4e8e\u53cc\u673a\u5236\u6df7\u5408\u5047\u8bbe\u548c\u5355\u8c03\u4f3c\u7136\u6bd4\u7279\u6027\uff0c\u901a\u8fc7\u5355\u4e00\u9608\u503c\u8fdb\u884c\u6700\u4f18\u8d1d\u53f6\u65af\u68c0\u6d4b\u3002", "result": "\u9ad8\u9891\u80fd\u91cf\u6bd4\u5728\u591a\u4e2a\u6a21\u578b\u65cf\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u53cc\u5cf0\u6027\uff0c\u80fd\u591f\u5728\u6a21\u578b\u4ecd\u5728\u5904\u7406\u6587\u672c\u65f6\u68c0\u6d4b\u6c61\u67d3\uff0c\u5ef6\u8fdf\u4f4e\u4e8e1\u6beb\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u5185\u8054\u5b89\u5168\u76d1\u63a7\u5668\u96c6\u6210\u5230\u68c0\u7d22\u589e\u5f3a\u667a\u80fd\u4f53\u7ba1\u9053\u4e2d\uff0c\u5728\u9519\u8bef\u63d0\u4ea4\u5230\u63a8\u7406\u94fe\u4e4b\u524d\u8fdb\u884c\u68c0\u6d4b\u3002", "topic": "agent analysis"}}
{"id": "2511.07070", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07070", "abs": "https://arxiv.org/abs/2511.07070", "authors": ["Fei Zhao", "Chonggang Lu", "Haofu Qian", "Fangcheng Shi", "Zijie Meng", "Jianzhao Huang", "Xu Tang", "Zheyong Xie", "Zheyu Ye", "Zhe Xu", "Yao Hu", "Shaosheng Cao"], "title": "RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services", "comment": null, "summary": "As a key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers a ``seesaw'' between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes a competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness.", "AI": {"tldr": "RedOne 2.0\u662f\u4e00\u4e2a\u9488\u5bf9\u793e\u4ea4\u7f51\u7edc\u670d\u52a1\u76844B\u89c4\u6a21\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0fRL\u4f18\u5148\u540e\u8bad\u7ec3\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u9c81\u68d2\u6027\u7684\u540c\u65f6\u63d0\u5347\u9886\u57df\u7279\u5b9a\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u793e\u4ea4\u7f51\u7edc\u670d\u52a1\u4e2d\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u3001\u5feb\u901f\u53d8\u5316\u7684\u89c4\u8303\u4fda\u8bed\u3001\u591a\u8bed\u8a00\u6587\u5316\u591a\u6837\u6027\u5e26\u6765\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u4ee5\u53ca\u76d1\u7763\u5fae\u8c03\u5f15\u53d1\u7684\u6027\u80fd\u6743\u8861\u95ee\u9898\u3002", "method": "\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u63a2\u7d22\u6027\u5b66\u4e60\u5efa\u7acb\u521d\u59cb\u5bf9\u9f50\u5e76\u8bc6\u522b\u7cfb\u7edf\u5f31\u70b9\uff1b\u76ee\u6807\u5fae\u8c03\u9009\u62e9\u6027\u5e94\u7528SFT\u586b\u8865\u5dee\u8ddd\uff1b\u7cbe\u70bc\u5b66\u4e60\u91cd\u65b0\u5e94\u7528RL\u5de9\u56fa\u6539\u8fdb\u3002", "result": "\u57284B\u89c4\u6a21\u4e0b\u5e73\u5747\u6027\u80fd\u63d0\u53472.41\u5206\uff08\u76f8\u6bd47B\u6b21\u4f18\u57fa\u7ebf\uff09\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5e73\u5747\u63d0\u53478.74\u5206\uff0c\u6570\u636e\u6548\u7387\u6bd4SFT\u65b9\u6cd5RedOne\u63d0\u9ad8\u4e00\u500d\u4ee5\u4e0a\u3002", "conclusion": "RedOne 2.0\u4e3aSNS\u573a\u666f\u4e0b\u7684\u9886\u57df\u7279\u5b9aLLM\u5efa\u7acb\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6210\u672c\u6548\u76ca\u57fa\u51c6\uff0c\u5728\u63d0\u5347\u80fd\u529b\u7684\u540c\u65f6\u4e0d\u727a\u7272\u9c81\u68d2\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.07086", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07086", "abs": "https://arxiv.org/abs/2511.07086", "authors": ["Marcel Pehlke", "Marc Jansen"], "title": "LLM Driven Processes to Foster Explainable AI", "comment": null, "summary": "We present a modular, explainable LLM-agent pipeline for decision support that externalizes reasoning into auditable artifacts. The system instantiates three frameworks: Vester's Sensitivity Model (factor set, signed impact matrix, systemic roles, feedback loops); normal-form games (strategies, payoff matrix, equilibria); and sequential games (role-conditioned agents, tree construction, backward induction), with swappable modules at every step. LLM components (default: GPT-5) are paired with deterministic analyzers for equilibria and matrix-based role classification, yielding traceable intermediates rather than opaque outputs. In a real-world logistics case (100 runs), mean factor alignment with a human baseline was 55.5\\% over 26 factors and 62.9\\% on the transport-core subset; role agreement over matches was 57\\%. An LLM judge using an eight-criterion rubric (max 100) scored runs on par with a reconstructed human baseline. Configurable LLM pipelines can thus mimic expert workflows with transparent, inspectable steps.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u89e3\u91ca\u7684LLM\u4ee3\u7406\u51b3\u7b56\u652f\u6301\u7ba1\u9053\uff0c\u901a\u8fc7\u5916\u90e8\u5316\u63a8\u7406\u751f\u6210\u53ef\u5ba1\u8ba1\u7684\u4e2d\u95f4\u4ea7\u7269\uff0c\u7ed3\u5408\u4e86Vester\u654f\u611f\u6027\u6a21\u578b\u3001\u535a\u5f08\u8bba\u548c\u987a\u5e8f\u6e38\u620f\u6846\u67b6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edfLLM\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u53ef\u5ba1\u8ba1\u63a8\u7406\u8fc7\u7a0b\u7684\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u6a21\u5757\u5316LLM\u4ee3\u7406\u7ba1\u9053\uff0c\u6574\u5408Vester\u654f\u611f\u6027\u6a21\u578b\uff08\u56e0\u7d20\u96c6\u3001\u7b26\u53f7\u5f71\u54cd\u77e9\u9635\u3001\u7cfb\u7edf\u89d2\u8272\u3001\u53cd\u9988\u5faa\u73af\uff09\u3001\u6807\u51c6\u5f62\u5f0f\u535a\u5f08\uff08\u7b56\u7565\u3001\u6536\u76ca\u77e9\u9635\u3001\u5747\u8861\uff09\u548c\u987a\u5e8f\u535a\u5f08\uff08\u89d2\u8272\u6761\u4ef6\u4ee3\u7406\u3001\u6811\u6784\u5efa\u3001\u9006\u5411\u5f52\u7eb3\uff09\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u90fd\u652f\u6301\u6a21\u5757\u66ff\u6362\u3002", "result": "\u5728\u771f\u5b9e\u7269\u6d41\u6848\u4f8b\u4e2d\uff08100\u6b21\u8fd0\u884c\uff09\uff0c26\u4e2a\u56e0\u7d20\u7684\u5e73\u5747\u5bf9\u9f50\u5ea6\u4e3a55.5%\uff0c\u8fd0\u8f93\u6838\u5fc3\u5b50\u96c6\u4e3a62.9%\uff1b\u89d2\u8272\u5339\u914d\u4e00\u81f4\u6027\u4e3a57%\u3002LLM\u8bc4\u4f30\u5668\u4f7f\u7528\u516b\u9879\u6807\u51c6\u8bc4\u5206\u4e0e\u91cd\u6784\u7684\u4eba\u7c7b\u57fa\u51c6\u76f8\u5f53\u3002", "conclusion": "\u53ef\u914d\u7f6e\u7684LLM\u7ba1\u9053\u80fd\u591f\u6a21\u4eff\u4e13\u5bb6\u5de5\u4f5c\u6d41\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u900f\u660e\u548c\u53ef\u68c0\u67e5\u7684\u6b65\u9aa4\u3002", "topic": "agent analysis"}}
{"id": "2511.07097", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.07097", "abs": "https://arxiv.org/abs/2511.07097", "authors": ["Diego Gosmar", "Anna Chiara Pallotta", "Giovanni Zenezini"], "title": "Agentic AI Sustainability Assessment for Supply Chain Document Insights", "comment": "17 pages, 4 figures", "summary": "This paper presents a comprehensive sustainability assessment framework for document intelligence within supply chain operations, centered on agentic artificial intelligence (AI). We address the dual objective of improving automation efficiency while providing measurable environmental performance in document-intensive workflows. The research compares three scenarios: fully manual (human-only), AI-assisted (human-in-the-loop, HITL), and an advanced multi-agent agentic AI workflow leveraging parsers and verifiers. Empirical results show that AI-assisted HITL and agentic AI scenarios achieve reductions of up to 70-90% in energy consumption, 90-97% in carbon dioxide emissions, and 89-98% in water usage compared to manual processes. Notably, full agentic configurations, combining advanced reasoning (thinking mode) and multi-agent validation, achieve substantial sustainability gains over human-only approaches, even when resource usage increases slightly versus simpler AI-assisted solutions. The framework integrates performance, energy, and emission indicators into a unified ESG-oriented methodology for assessing and governing AI-enabled supply chain solutions. The paper includes a complete replicability use case demonstrating the methodology's application to real-world document extraction tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fdAI\u7684\u4f9b\u5e94\u94fe\u6587\u6863\u667a\u80fd\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86\u5168\u4eba\u5de5\u3001AI\u8f85\u52a9\u548c\u667a\u80fdAI\u591a\u667a\u80fd\u4f53\u4e09\u79cd\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ed3\u679c\u663e\u793a\u667a\u80fdAI\u914d\u7f6e\u5728\u80fd\u6e90\u6d88\u8017\u3001\u78b3\u6392\u653e\u548c\u6c34\u8d44\u6e90\u4f7f\u7528\u65b9\u9762\u6bd4\u4eba\u5de5\u6d41\u7a0b\u51cf\u5c1170-98%\u3002", "motivation": "\u89e3\u51b3\u4f9b\u5e94\u94fe\u6587\u6863\u5bc6\u96c6\u578b\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u81ea\u52a8\u5316\u6548\u7387\u4e0e\u73af\u5883\u7ee9\u6548\u7684\u53cc\u91cd\u76ee\u6807\uff0c\u4e3aAI\u8d4b\u80fd\u7684\u4f9b\u5e94\u94fe\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u7edf\u4e00\u7684ESG\u5bfc\u5411\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u4e09\u79cd\u573a\u666f\uff1a\u5168\u4eba\u5de5\u3001AI\u8f85\u52a9\uff08\u4eba\u5728\u56de\u8def\uff09\u548c\u667a\u80fdAI\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ed3\u5408\u89e3\u6790\u5668\u548c\u9a8c\u8bc1\u5668\uff0c\u96c6\u6210\u6027\u80fd\u3001\u80fd\u6e90\u548c\u6392\u653e\u6307\u6807\u3002", "result": "AI\u8f85\u52a9\u548c\u667a\u80fdAI\u573a\u666f\u76f8\u6bd4\u4eba\u5de5\u6d41\u7a0b\u5b9e\u73b0\uff1a\u80fd\u8017\u51cf\u5c1170-90%\uff0c\u4e8c\u6c27\u5316\u78b3\u6392\u653e\u51cf\u5c1190-97%\uff0c\u6c34\u8d44\u6e90\u4f7f\u7528\u51cf\u5c1189-98%\u3002\u667a\u80fdAI\u914d\u7f6e\u5373\u4f7f\u8d44\u6e90\u4f7f\u7528\u7565\u6709\u589e\u52a0\uff0c\u4ecd\u6bd4\u7eaf\u4eba\u5de5\u65b9\u6cd5\u6709\u663e\u8457\u53ef\u6301\u7eed\u6027\u6536\u76ca\u3002", "conclusion": "\u667a\u80fdAI\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u5728\u4f9b\u5e94\u94fe\u6587\u6863\u5904\u7406\u4e2d\u80fd\u5b9e\u73b0\u663e\u8457\u7684\u53ef\u6301\u7eed\u6027\u6539\u8fdb\uff0c\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u548c\u6cbb\u7406AI\u8d4b\u80fd\u7684\u4f9b\u5e94\u94fe\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2511.07110", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07110", "abs": "https://arxiv.org/abs/2511.07110", "authors": ["Tianhao Fu", "Xinxin Xu", "Weichen Xu", "Jue Chen", "Ruilong Ren", "Bowen Deng", "Xinyu Zhao", "Jian Cao", "Xixin Cao"], "title": "Two Heads are Better than One: Distilling Large Language Model Features Into Small Models with Feature Decomposition and Mixture", "comment": null, "summary": "Market making (MM) through Reinforcement Learning (RL) has attracted significant attention in financial trading. With the development of Large Language Models (LLMs), more and more attempts are being made to apply LLMs to financial areas. A simple, direct application of LLM as an agent shows significant performance. Such methods are hindered by their slow inference speed, while most of the current research has not studied LLM distillation for this specific task. To address this, we first propose the normalized fluorescent probe to study the mechanism of the LLM's feature. Based on the observation found by our investigation, we propose Cooperative Market Making (CMM), a novel framework that decouples LLM features across three orthogonal dimensions: layer, task, and data. Various student models collaboratively learn simple LLM features along with different dimensions, with each model responsible for a distinct feature to achieve knowledge distillation. Furthermore, CMM introduces an H\u00e1jek-MoE to integrate the output of the student models by investigating the contribution of different models in a kernel function-generated common feature space. Extensive experimental results on four real-world market datasets demonstrate the superiority of CMM over the current distillation method and RL-based market-making strategies.", "AI": {"tldr": "\u63d0\u51faCMM\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u6b63\u4ea4\u7ef4\u5ea6\uff08\u5c42\u3001\u4efb\u52a1\u3001\u6570\u636e\uff09\u89e3\u8026LLM\u7279\u5f81\uff0c\u4f7f\u7528\u591a\u4e2a\u5b66\u751f\u6a21\u578b\u534f\u4f5c\u5b66\u4e60\u7b80\u5355LLM\u7279\u5f81\uff0c\u5b9e\u73b0\u77e5\u8bc6\u84b8\u998f\uff0c\u5e76\u901a\u8fc7H\u00e1jek-MoE\u96c6\u6210\u6a21\u578b\u8f93\u51fa\uff0c\u5728\u56db\u4e2a\u771f\u5b9e\u5e02\u573a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u548cRL\u5e02\u573a\u505a\u5e02\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u5c06LLM\u76f4\u63a5\u4f5c\u4e3a\u667a\u80fd\u4f53\u5e94\u7528\u4e8e\u5e02\u573a\u505a\u5e02\u7684\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u4e14\u5f53\u524d\u7814\u7a76\u672a\u9488\u5bf9\u8be5\u7279\u5b9a\u4efb\u52a1\u7814\u7a76LLM\u84b8\u998f\u3002", "method": "\u63d0\u51faCMM\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u6807\u51c6\u5316\u8367\u5149\u63a2\u9488\u7814\u7a76LLM\u7279\u5f81\u673a\u5236\uff1b2\uff09\u5728\u5c42\u3001\u4efb\u52a1\u3001\u6570\u636e\u4e09\u4e2a\u6b63\u4ea4\u7ef4\u5ea6\u89e3\u8026LLM\u7279\u5f81\uff1b3\uff09\u591a\u4e2a\u5b66\u751f\u6a21\u578b\u534f\u4f5c\u5b66\u4e60\u7b80\u5355\u7279\u5f81\uff1b4\uff09\u5f15\u5165H\u00e1jek-MoE\u96c6\u6210\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u5e02\u573a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCMM\u4f18\u4e8e\u5f53\u524d\u7684\u84b8\u998f\u65b9\u6cd5\u548c\u57fa\u4e8eRL\u7684\u5e02\u573a\u505a\u5e02\u7b56\u7565\u3002", "conclusion": "CMM\u6846\u67b6\u901a\u8fc7\u89e3\u8026LLM\u7279\u5f81\u548c\u591a\u6a21\u578b\u534f\u4f5c\u5b66\u4e60\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u5728\u91d1\u878d\u4ea4\u6613\u9886\u57df\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.07260", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07260", "abs": "https://arxiv.org/abs/2511.07260", "authors": ["Hohei Chan", "Xinzhi Zhang", "Antao Xiang", "Weinan Zhang", "Mengchen Zhao"], "title": "PADiff: Predictive and Adaptive Diffusion Policies for Ad Hoc Teamwork", "comment": "Accepted by the 40th AAAI conference on Artificial Intelligence (AAAI 2026)", "summary": "Ad hoc teamwork (AHT) requires agents to collaborate with previously unseen teammates, which is crucial for many real-world applications. The core challenge of AHT is to develop an ego agent that can predict and adapt to unknown teammates on the fly. Conventional RL-based approaches optimize a single expected return, which often causes policies to collapse into a single dominant behavior, thus failing to capture the multimodal cooperation patterns inherent in AHT. In this work, we introduce PADiff, a diffusion-based approach that captures agent's multimodal behaviors, unlocking its diverse cooperation modes with teammates. However, standard diffusion models lack the ability to predict and adapt in highly non-stationary AHT scenarios. To address this limitation, we propose a novel diffusion-based policy that integrates critical predictive information about teammates into the denoising process. Extensive experiments across three cooperation environments demonstrate that PADiff outperforms existing AHT methods significantly.", "AI": {"tldr": "PADiff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684ad hoc teamwork\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u961f\u53cb\u9884\u6d4b\u4fe1\u606f\u5230\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRL\u65b9\u6cd5\u5728\u6355\u6349\u591a\u6a21\u6001\u5408\u4f5c\u6a21\u5f0f\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "ad hoc teamwork\u9700\u8981\u667a\u80fd\u4f53\u4e0e\u672a\u77e5\u961f\u53cb\u534f\u4f5c\uff0c\u4f20\u7edfRL\u65b9\u6cd5\u4f18\u5316\u5355\u4e00\u671f\u671b\u56de\u62a5\u4f1a\u5bfc\u81f4\u7b56\u7565\u574d\u7f29\u4e3a\u5355\u4e00\u4e3b\u5bfc\u884c\u4e3a\uff0c\u65e0\u6cd5\u6355\u6349AHT\u4e2d\u56fa\u6709\u7684\u591a\u6a21\u6001\u5408\u4f5c\u6a21\u5f0f\u3002", "method": "\u63d0\u51faPADiff\u6269\u6563\u6a21\u578b\u65b9\u6cd5\uff0c\u5c06\u961f\u53cb\u7684\u5173\u952e\u9884\u6d4b\u4fe1\u606f\u6574\u5408\u5230\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u6355\u6349\u667a\u80fd\u4f53\u7684\u591a\u6a21\u6001\u884c\u4e3a\u5e76\u89e3\u9501\u591a\u6837\u5316\u7684\u5408\u4f5c\u6a21\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a\u5408\u4f5c\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPADiff\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684AHT\u65b9\u6cd5\u3002", "conclusion": "PADiff\u901a\u8fc7\u6269\u6563\u6a21\u578b\u6210\u529f\u89e3\u51b3\u4e86AHT\u4e2d\u591a\u6a21\u6001\u5408\u4f5c\u6a21\u5f0f\u7684\u6355\u6349\u95ee\u9898\uff0c\u5728\u9ad8\u5ea6\u975e\u5e73\u7a33\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agent analysis"}}
{"id": "2511.07413", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07413", "abs": "https://arxiv.org/abs/2511.07413", "authors": ["Yuxuan Sun", "Manchen Wang", "Shengyi Qian", "William R. Wong", "Eric Gan", "Pierluca D'Oro", "Alejandro Castillejo Munoz", "Sneha Silwal", "Pedro Matias", "Nitin Kamra", "Satwik Kottur", "Nick Raines", "Xuanyi Zhao", "Joy Chen", "Joseph Greer", "Andrea Madotto", "Allen Bolourchi", "James Valori", "Kevin Carlberg", "Karl Ridgeway", "Joseph Tighe"], "title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents", "comment": "Website: https://facebookresearch.github.io/DigiData", "summary": "AI agents capable of controlling user interfaces have the potential to\ntransform human interaction with digital devices. To accelerate this\ntransformation, two fundamental building blocks are essential: high-quality\ndatasets that enable agents to achieve complex and human-relevant goals, and\nrobust evaluation methods that allow researchers and practitioners to rapidly\nenhance agent performance. In this paper, we introduce DigiData, a large-scale,\nhigh-quality, diverse, multi-modal dataset designed for training mobile control\nagents. Unlike existing datasets, which derive goals from unstructured\ninteractions, DigiData is meticulously constructed through comprehensive\nexploration of app features, resulting in greater diversity and higher goal\ncomplexity. Additionally, we present DigiData-Bench, a benchmark for evaluating\nmobile control agents on real-world complex tasks. We demonstrate that the\ncommonly used step-accuracy metric falls short in reliably assessing mobile\ncontrol agents and, to address this, we propose dynamic evaluation protocols\nand AI-powered evaluations as rigorous alternatives for agent assessment. Our\ncontributions aim to significantly advance the development of mobile control\nagents, paving the way for more intuitive and effective human-device\ninteractions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86DigiData\u6570\u636e\u96c6\u548cDigiData-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u79fb\u52a8\u63a7\u5236AI\u4ee3\u7406\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u5168\u9762\u63a2\u7d22\u5e94\u7528\u529f\u80fd\u6784\u5efa\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u591a\u6837\u6027\u548c\u76ee\u6807\u590d\u6742\u6027\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u52a8\u6001\u8bc4\u4f30\u534f\u8bae\u548cAI\u9a71\u52a8\u8bc4\u4f30\u4f5c\u4e3a\u66f4\u53ef\u9760\u7684\u4ee3\u7406\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u63a8\u52a8\u79fb\u52a8\u63a7\u5236AI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u8fd9\u4e9b\u4ee3\u7406\u6709\u6f5c\u529b\u6539\u53d8\u4eba\u7c7b\u4e0e\u6570\u5b57\u8bbe\u5907\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u6784\u5efa\u4e86DigiData\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5168\u9762\u63a2\u7d22\u5e94\u7528\u529f\u80fd\u800c\u975e\u975e\u7ed3\u6784\u5316\u4ea4\u4e92\u6765\u521b\u5efa\uff1b\u5f00\u53d1\u4e86DigiData-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u8bc4\u4f30\u534f\u8bae\u548cAI\u9a71\u52a8\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u3001\u590d\u6742\u76ee\u6807\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5efa\u7acb\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6b65\u8fdb\u7cbe\u5ea6\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "conclusion": "DigiData\u6570\u636e\u96c6\u548cDigiData-Bench\u57fa\u51c6\u6d4b\u8bd5\u663e\u8457\u63a8\u8fdb\u4e86\u79fb\u52a8\u63a7\u5236\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u4e3a\u66f4\u76f4\u89c2\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "swe benchmark"}}
{"id": "2511.07002", "categories": ["cs.CL", "I.2.0; I.2.6; I.2.7; I.2.4"], "pdf": "https://arxiv.org/pdf/2511.07002", "abs": "https://arxiv.org/abs/2511.07002", "authors": ["Giuseppe Birardi"], "title": "Automated Circuit Interpretation via Probe Prompting", "comment": "27 pages, 5 figures, 3 tables. Code and interactive demo available", "summary": "Mechanistic interpretability aims to understand neural networks by\nidentifying which learned features mediate specific behaviors. Attribution\ngraphs reveal these feature pathways, but interpreting them requires extensive\nmanual analysis -- a single prompt can take approximately 2 hours for an\nexperienced circuit tracer. We present probe prompting, an automated pipeline\nthat transforms attribution graphs into compact, interpretable subgraphs built\nfrom concept-aligned supernodes. Starting from a seed prompt and target logit,\nwe select high-influence features, generate concept-targeted yet\ncontext-varying probes, and group features by cross-prompt activation\nsignatures into Semantic, Relationship, and Say-X categories using transparent\ndecision rules.\n  Across five prompts including classic \"capitals\" circuits, probe-prompted\nsubgraphs preserve high explanatory coverage while compressing complexity\n(Completeness 0.83, mean across circuits; Replacement 0.54). Compared to\ngeometric clustering baselines, concept-aligned groups exhibit higher\nbehavioral coherence: 2.3x higher peak-token consistency (0.425 vs 0.183) and\n5.8x higher activation-pattern similarity (0.762 vs 0.130), despite lower\ngeometric compactness. Entity-swap tests reveal a layerwise hierarchy:\nearly-layer features transfer robustly (64% transfer rate, mean layer 6.3),\nwhile late-layer Say-X features specialize for output promotion (mean layer\n16.4), supporting a backbone-and-specialization view of transformer\ncomputation.\n  We release code (https://github.com/peppinob-ol/attribution-graph-probing),\nan interactive demo\n(https://huggingface.co/spaces/Peppinob/attribution-graph-probing), and minimal\nartifacts enabling immediate reproduction and community adoption.", "AI": {"tldr": "\u63d0\u51fa\u4e86probe prompting\u65b9\u6cd5\uff0c\u81ea\u52a8\u5c06\u5f52\u56e0\u56fe\u8f6c\u6362\u4e3a\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u7684\u5b50\u56fe\uff0c\u901a\u8fc7\u6982\u5ff5\u5bf9\u9f50\u7684\u8d85\u7ea7\u8282\u70b9\u6765\u52a0\u901f\u795e\u7ecf\u7f51\u7edc\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "motivation": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u9700\u8981\u5927\u91cf\u624b\u52a8\u5206\u6790\uff08\u5355\u4e2a\u63d0\u793a\u7ea6\u97002\u5c0f\u65f6\uff09\uff0c\u73b0\u6709\u5f52\u56e0\u56fe\u89e3\u91ca\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u8bc6\u522b\u7279\u5f81\u901a\u8def\u3002", "method": "\u4ece\u79cd\u5b50\u63d0\u793a\u548c\u76ee\u6807logit\u5f00\u59cb\uff0c\u9009\u62e9\u9ad8\u5f71\u54cd\u529b\u7279\u5f81\uff0c\u751f\u6210\u6982\u5ff5\u5bfc\u5411\u4f46\u4e0a\u4e0b\u6587\u53d8\u5316\u7684\u63a2\u9488\uff0c\u4f7f\u7528\u900f\u660e\u51b3\u7b56\u89c4\u5219\u5c06\u7279\u5f81\u6309\u8de8\u63d0\u793a\u6fc0\u6d3b\u7279\u5f81\u5206\u7ec4\u4e3a\u8bed\u4e49\u3001\u5173\u7cfb\u548cSay-X\u7c7b\u522b\u3002", "result": "\u5728\u4e94\u4e2a\u63d0\u793a\u6d4b\u8bd5\u4e2d\uff0cprobe-prompted\u5b50\u56fe\u4fdd\u6301\u9ad8\u89e3\u91ca\u8986\u76d6\u7387\u540c\u65f6\u538b\u7f29\u590d\u6742\u5ea6\uff08\u5b8c\u6574\u60270.83\uff0c\u66ff\u6362\u5ea60.54\uff09\u3002\u76f8\u6bd4\u51e0\u4f55\u805a\u7c7b\u57fa\u7ebf\uff0c\u6982\u5ff5\u5bf9\u9f50\u7ec4\u8868\u73b0\u51fa\u66f4\u9ad8\u884c\u4e3a\u4e00\u81f4\u6027\uff1a\u5cf0\u503ctoken\u4e00\u81f4\u6027\u9ad82.3\u500d\uff0c\u6fc0\u6d3b\u6a21\u5f0f\u76f8\u4f3c\u5ea6\u9ad85.8\u500d\u3002\u5b9e\u4f53\u4ea4\u6362\u6d4b\u8bd5\u63ed\u793a\u4e86\u5206\u5c42\u7ed3\u6784\u3002", "conclusion": "\u652f\u6301transformer\u8ba1\u7b97\u7684\u540e\u53f0-\u4e13\u4e1a\u5316\u89c6\u56fe\uff0c\u65e9\u671f\u5c42\u7279\u5f81\u7a33\u5065\u8f6c\u79fb\uff0c\u665a\u671f\u5c42Say-X\u7279\u5f81\u4e13\u95e8\u7528\u4e8e\u8f93\u51fa\u4fc3\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2511.06094", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06094", "abs": "https://arxiv.org/abs/2511.06094", "authors": ["Daniel Beechey", "\u00d6zg\u00fcr \u015eim\u015fek"], "title": "Approximating Shapley Explanations in Reinforcement Learning", "comment": "Camera-ready version. Published at the Conference on Neural\n  Information Processing Systems (NeurIPS 2025)", "summary": "Reinforcement learning has achieved remarkable success in complex\ndecision-making environments, yet its lack of transparency limits its\ndeployment in practice, especially in safety-critical settings. Shapley values\nfrom cooperative game theory provide a principled framework for explaining\nreinforcement learning; however, the computational cost of Shapley explanations\nis an obstacle to their use. We introduce FastSVERL, a scalable method for\nexplaining reinforcement learning by approximating Shapley values. FastSVERL is\ndesigned to handle the unique challenges of reinforcement learning, including\ntemporal dependencies across multi-step trajectories, learning from off-policy\ndata, and adapting to evolving agent behaviours in real time. FastSVERL\nintroduces a practical, scalable approach for principled and rigorous\ninterpretability in reinforcement learning.", "AI": {"tldr": "FastSVERL\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3cShapley\u503c\u6765\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u900f\u660e\u5ea6\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u51b3\u7b56\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u7f3a\u4e4f\u900f\u660e\u5ea6\u9650\u5236\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u3002", "method": "\u5f15\u5165FastSVERL\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3cShapley\u503c\u6765\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\uff0c\u80fd\u591f\u5904\u7406\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u591a\u6b65\u8f68\u8ff9\u4e2d\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u3001\u4ece\u79bb\u7b56\u7565\u6570\u636e\u4e2d\u5b66\u4e60\u4ee5\u53ca\u5b9e\u65f6\u9002\u5e94\u4e0d\u65ad\u6f14\u5316\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u3002", "result": "FastSVERL\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u539f\u5219\u6027\u548c\u4e25\u8c28\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "FastSVERL\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684Shapley\u503c\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u89e3\u91ca\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u969c\u788d\u3002", "topic": "agent analysis"}}
{"id": "2511.06101", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.06101", "abs": "https://arxiv.org/abs/2511.06101", "authors": ["Zhaoyang Wang", "Yiming Liang", "Xuchao Zhang", "Qianhui Wu", "Siwei Han", "Anson Bastos", "Rujia Wang", "Chetan Bansal", "Baolin Peng", "Jianfeng Gao", "Saravan Rajmohan", "Huaxiu Yao"], "title": "Adapting Web Agents with Synthetic Supervision", "comment": "19 pages, 6 figures", "summary": "Web agents struggle to adapt to new websites due to the scarcity of\nenvironment specific tasks and demonstrations. Recent works have explored\nsynthetic data generation to address this challenge, however, they suffer from\ndata quality issues where synthesized tasks contain hallucinations that cannot\nbe executed, and collected trajectories are noisy with redundant or misaligned\nactions. In this paper, we propose SynthAgent, a fully synthetic supervision\nframework that aims at improving synthetic data quality via dual refinement of\nboth tasks and trajectories. Our approach begins by synthesizing diverse tasks\nthrough categorized exploration of web elements, ensuring efficient coverage of\nthe target environment. During trajectory collection, we refine tasks when\nconflicts with actual observations are detected, mitigating hallucinations\nwhile maintaining task consistency. After collection, we conduct trajectory\nrefinement with a global context to mitigate potential noise or misalignments.\nFinally, we fine-tune open-source web agents on the refined synthetic data to\nadapt them to the target environment. Experimental results demonstrate that\nSynthAgent outperforms existing synthetic data methods, validating the\nimportance of high-quality synthetic supervision. The code will be publicly\navailable at https://github.com/aiming-lab/SynthAgent.", "AI": {"tldr": "SynthAgent\u662f\u4e00\u4e2a\u5b8c\u5168\u5408\u6210\u7684\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u7ec6\u5316\u4efb\u52a1\u548c\u8f68\u8ff9\u6765\u63d0\u9ad8\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u4f7f\u7f51\u7edc\u4ee3\u7406\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u65b0\u7f51\u7ad9\u3002", "motivation": "\u7f51\u7edc\u4ee3\u7406\u96be\u4ee5\u9002\u5e94\u65b0\u7f51\u7ad9\uff0c\u56e0\u4e3a\u73af\u5883\u7279\u5b9a\u4efb\u52a1\u548c\u6f14\u793a\u7a00\u7f3a\u3002\u73b0\u6709\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u5982\u5305\u542b\u65e0\u6cd5\u6267\u884c\u7684\u4efb\u52a1\u5e7b\u89c9\u548c\u5e26\u6709\u5197\u4f59\u6216\u4e0d\u5bf9\u9f50\u52a8\u4f5c\u7684\u566a\u58f0\u8f68\u8ff9\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u63a2\u7d22\u7f51\u9875\u5143\u7d20\u5408\u6210\u591a\u6837\u5316\u4efb\u52a1\uff0c\u5728\u8f68\u8ff9\u6536\u96c6\u8fc7\u7a0b\u4e2d\u68c0\u6d4b\u5230\u4e0e\u89c2\u5bdf\u51b2\u7a81\u65f6\u7ec6\u5316\u4efb\u52a1\u4ee5\u51cf\u8f7b\u5e7b\u89c9\uff0c\u6536\u96c6\u540e\u4f7f\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u8fdb\u884c\u8f68\u8ff9\u7ec6\u5316\u4ee5\u51cf\u5c11\u566a\u58f0\uff0c\u6700\u540e\u5728\u7cbe\u70bc\u7684\u5408\u6210\u6570\u636e\u4e0a\u5fae\u8c03\u5f00\u6e90\u7f51\u7edc\u4ee3\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSynthAgent\u4f18\u4e8e\u73b0\u6709\u7684\u5408\u6210\u6570\u636e\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u9ad8\u8d28\u91cf\u5408\u6210\u76d1\u7763\u7684\u91cd\u8981\u6027\u3002", "conclusion": "SynthAgent\u901a\u8fc7\u53cc\u91cd\u7ec6\u5316\u4efb\u52a1\u548c\u8f68\u8ff9\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u4f7f\u7f51\u7edc\u4ee3\u7406\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u76ee\u6807\u73af\u5883\u3002", "topic": "agent analysis"}}
{"id": "2511.07112", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07112", "abs": "https://arxiv.org/abs/2511.07112", "authors": ["Khashayar Alavi", "Zhastay Yeltay", "Lucie Flek", "Akbar Karimi"], "title": "More Agents Helps but Adversarial Robustness Gap Persists", "comment": null, "summary": "When LLM agents work together, they seem to be more powerful than a single\nLLM in mathematical question answering. However, are they also more robust to\nadversarial inputs? We investigate this question using adversarially perturbed\nmath questions. These perturbations include punctuation noise with three\nintensities (10, 30, and 50 percent), plus real-world and human-like typos\n(WikiTypo, R2ATA). Using a unified sampling-and-voting framework (Agent\nForest), we evaluate six open-source models (Qwen3-4B/14B, Llama3.1-8B,\nMistral-7B, Gemma3-4B/12B) across four benchmarks (GSM8K, MATH, MMLU-Math,\nMultiArith), with various numbers of agents n from one to 25 (1, 2, 5, 10, 15,\n20, 25). Our findings show that (1) Noise type matters: punctuation noise harm\nscales with its severity, and the human typos remain the dominant bottleneck,\nyielding the largest gaps to Clean accuracy and the highest ASR even with a\nlarge number of agents. And (2) Collaboration reliably improves accuracy as the\nnumber of agents, n, increases, with the largest gains from one to five agents\nand diminishing returns beyond 10 agents. However, the adversarial robustness\ngap persists regardless of the agent count.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u80fd\u63d0\u9ad8\u6570\u5b66\u95ee\u7b54\u51c6\u786e\u6027\uff0c\u4f46\u5bf9\u5bf9\u6297\u6027\u6270\u52a8\u7684\u9c81\u68d2\u6027\u63d0\u5347\u6709\u9650\u3002\u6807\u70b9\u7b26\u53f7\u566a\u58f0\u5f71\u54cd\u968f\u5f3a\u5ea6\u589e\u52a0\uff0c\u800c\u4eba\u7c7b\u62fc\u5199\u9519\u8bef\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u5373\u4f7f\u589e\u52a0\u667a\u80fd\u4f53\u6570\u91cf\u4e5f\u65e0\u6cd5\u6d88\u9664\u9c81\u68d2\u6027\u5dee\u8ddd\u3002", "motivation": "\u7814\u7a76\u591aLLM\u667a\u80fd\u4f53\u534f\u4f5c\u662f\u5426\u6bd4\u5355\u4e2aLLM\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u8f93\u5165\u65f6\u66f4\u9c81\u68d2\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u3002", "method": "\u4f7f\u7528Agent Forest\u7edf\u4e00\u91c7\u6837\u6295\u7968\u6846\u67b6\uff0c\u57286\u4e2a\u5f00\u6e90\u6a21\u578b\u4e0a\u6d4b\u8bd5\u4e0d\u540c\u6570\u91cf\u7684\u667a\u80fd\u4f53\uff081-25\u4e2a\uff09\uff0c\u8bc4\u4f30\u6807\u70b9\u7b26\u53f7\u566a\u58f0\u548c\u771f\u5b9e\u4e16\u754c\u62fc\u5199\u9519\u8bef\u5bf94\u4e2a\u6570\u5b66\u57fa\u51c6\u7684\u5f71\u54cd\u3002", "result": "\u534f\u4f5c\u786e\u5b9e\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u5bf9\u6297\u9c81\u68d2\u6027\u5dee\u8ddd\u6301\u7eed\u5b58\u5728\u3002\u6807\u70b9\u566a\u58f0\u5f71\u54cd\u4e0e\u5f3a\u5ea6\u76f8\u5173\uff0c\u4eba\u7c7b\u62fc\u5199\u9519\u8bef\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u5373\u4f7f\u4f7f\u752825\u4e2a\u667a\u80fd\u4f53\u4e5f\u65e0\u6cd5\u6d88\u9664\u9c81\u68d2\u6027\u5dee\u8ddd\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u80fd\u63d0\u5347\u51c6\u786e\u6027\u4f46\u65e0\u6cd5\u89e3\u51b3\u5bf9\u6297\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4eba\u7c7b\u62fc\u5199\u9519\u8bef\u4ecd\u662f\u4e3b\u8981\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2511.07382", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.07382", "abs": "https://arxiv.org/abs/2511.07382", "authors": ["K M Nafi Asib", "Sourav Saha", "Mohammed Moshiul Hoque"], "title": "Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python Code Generation", "comment": "8 pages, 1 figure, experimental scripts publicly available at\n  https://github.com/NafiAsib/Retriv-BLP25-Task-2", "summary": "Large Language Models (LLMs) have advanced the automated generation of code\nfrom natural language prompts. However, low-resource languages (LRLs) like\nBangla remain underrepresented due to the limited availability of\ninstruction-to-code datasets and evaluation benchmarks. To address this, the\nBLP Workshop at IJCNLP-AACL 2025 introduced a shared task on \"Code Generation\nin Bangla\". In this work, we propose a method that combines instruction\nprompting with a test-driven, feedback-guided iterative refinement process\nusing a fine-tuned Qwen2.5-14B model. The model generates code from Bangla\ninstructions, tests it against unit tests, and iteratively refines any failing\noutputs through three evaluation passes, using test feedback to guide each\nstep. This approach helped our team \"Retriv\" to secure 2nd place in the shared\ntask with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla\ninstruction understanding and Python code generation, emphasizing the need for\ntargeted methods in LRLs. We made experimental scripts publicly available for\nthe community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6307\u4ee4\u63d0\u793a\u548c\u6d4b\u8bd5\u9a71\u52a8\u53cd\u9988\u5f15\u5bfc\u8fed\u4ee3\u7cbe\u70bc\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u5fae\u8c03\u7684Qwen2.5-14B\u6a21\u578b\u4ece\u5b5f\u52a0\u62c9\u8bed\u6307\u4ee4\u751f\u6210\u4ee3\u7801\uff0c\u901a\u8fc7\u4e09\u6b21\u8bc4\u4f30\u8fed\u4ee3\u4f18\u5316\u5931\u8d25\u8f93\u51fa\uff0c\u5728BLP Workshop\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5b5f\u52a0\u62c9\u8bed\uff09\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7531\u4e8e\u7f3a\u4e4f\u6307\u4ee4\u5230\u4ee3\u7801\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684Qwen2.5-14B\u6a21\u578b\uff0c\u7ed3\u5408\u6307\u4ee4\u63d0\u793a\u548c\u6d4b\u8bd5\u9a71\u52a8\u7684\u53cd\u9988\u5f15\u5bfc\u8fed\u4ee3\u7cbe\u70bc\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4e09\u6b21\u8bc4\u4f30\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\u751f\u6210\u7ed3\u679c\u3002", "result": "\u5728BLP Workshop\u5171\u4eab\u4efb\u52a1\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\uff0cPass@1\u5f97\u5206\u4e3a0.934\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5b5f\u52a0\u62c9\u8bed\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff0c\u4f46\u5206\u6790\u663e\u793a\u5728\u5b5f\u52a0\u62c9\u8bed\u6307\u4ee4\u7406\u89e3\u548cPython\u4ee3\u7801\u751f\u6210\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4e13\u95e8\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2511.06307", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06307", "abs": "https://arxiv.org/abs/2511.06307", "authors": ["Speed Zhu", "Jianwei Cai", "Guang Chen", "Lulu Wu", "Saiyong Yang", "Wiggin Zhou"], "title": "DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation", "comment": "15 pages, 8 figures", "summary": "Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a\nresurgence of interest in RLVR. Nevertheless, advances are dominated by\nmathematics (e.g., AIME), with competitive-programming code generation\nunderexplored and data curation receiving less attention than RL algorithm\ndesign. We investigate how to construct RLVR datasets (i.e., RL prompts) and\npresent practical training techniques that yield strong performance on\ncompetitive-programming code generation. Our pipeline begins with supervised\nfine-tuning (SFT) distilled from strong open-source models, augmented with\ngeneral-purpose and reasoning-intensive data. RL then follows a two-stage\nprocess with executable, testcase-driven rewards: first, training on a large,\nuniformly distributed set of competitive-programming problems using Group\nRelative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively\nshort response-generation window (e.g., 32k during SFT and 24k in this stage)\nto expand entropy and mitigate repetition and truncation; second, we perform\n\\textbf{Pre-GRPO}: updating on a small, high-quality set of challenging\nproblems with a large rollout budget (64 rollouts per prompt) under a\nhard-focus curriculum that continuously retains the most difficult instances\nthroughout training. We implement our method on Qwen2.5-32B and evaluate on\nLeetCode and Codeforces weekly contests to avoid data leakage. The resulting\nmodel achieves state-of-the-art performance among models of similar scale and\nis comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking.\nWe also examine scaling trends and observe strong RL scaling on an internal\nlarge-scale MoE model. Our study distills concise best practices for data\ncuration, entropy expansion, and curriculum design in RLVR for\ncompetitive-programming code generation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7ade\u4e89\u6027\u7f16\u7a0b\u4ee3\u7801\u751f\u6210\u7684RLVR\uff08\u5f3a\u5316\u5b66\u4e60\u4e0e\u9a8c\u8bc1\u63a8\u7406\uff09\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5GRPO\u8bad\u7ec3\u6d41\u7a0b\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\u6784\u5efa\u7b56\u7565\uff0c\u5728Qwen2.5-32B\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u4f18\u5148\u6a21\u578b\u5728\u6570\u5b66\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7ade\u4e89\u6027\u7f16\u7a0b\u4ee3\u7801\u751f\u6210\u9886\u57df\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u6570\u636e\u6574\u7406\u53d7\u5230\u7684\u5173\u6ce8\u5c11\u4e8eRL\u7b97\u6cd5\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u540e\u63a5\u4e24\u9636\u6bb5GRPO\u5f3a\u5316\u5b66\u4e60\uff1a\u7b2c\u4e00\u9636\u6bb5\u5728\u5927\u89c4\u6a21\u5747\u5300\u5206\u5e03\u95ee\u9898\u4e0a\u8bad\u7ec3\u4ee5\u6269\u5c55\u71b5\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u9ad8\u8d28\u91cf\u6311\u6218\u6027\u95ee\u9898\u96c6\u4e0a\u8fdb\u884cPre-GRPO\u8bad\u7ec3\uff0c\u91c7\u7528\u786c\u7126\u70b9\u8bfe\u7a0b\u8bbe\u8ba1\u3002", "result": "\u5728LeetCode\u548cCodeforces\u5468\u8d5b\u4e2d\u8fbe\u5230\u540c\u7c7b\u89c4\u6a21\u6a21\u578b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e0eDeepSeek v3.1\u548cDoubao-1.5-Thinking\u7b49\u9886\u5148\u7cfb\u7edf\u76f8\u5f53\u3002", "conclusion": "\u7814\u7a76\u63d0\u70bc\u4e86\u7ade\u4e89\u6027\u7f16\u7a0b\u4ee3\u7801\u751f\u6210\u4e2d\u6570\u636e\u6574\u7406\u3001\u71b5\u6269\u5c55\u548c\u8bfe\u7a0b\u8bbe\u8ba1\u7684\u7b80\u6d01\u6700\u4f73\u5b9e\u8df5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.06946", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06946", "abs": "https://arxiv.org/abs/2511.06946", "authors": ["Daniel De Dios Allegue", "Jinke He", "Frans A. Oliehoek"], "title": "Learning to Focus: Prioritizing Informative Histories with Structured Attention Mechanisms in Partially Observable Reinforcement Learning", "comment": "Accepted to Embodied World Models for Decision Making (EWM) Workshop at NeurIPS 2025", "summary": "Transformers have shown strong ability to model long-term dependencies and are increasingly adopted as world models in model-based reinforcement learning (RL) under partial observability. However, unlike natural language corpora, RL trajectories are sparse and reward-driven, making standard self-attention inefficient because it distributes weight uniformly across all past tokens rather than emphasizing the few transitions critical for control. To address this, we introduce structured inductive priors into the self-attention mechanism of the dynamics head: (i) per-head memory-length priors that constrain attention to task-specific windows, and (ii) distributional priors that learn smooth Gaussian weightings over past state-action pairs. We integrate these mechanisms into UniZero, a model-based RL agent with a Transformer-based world model that supports planning under partial observability. Experiments on the Atari 100k benchmark show that most efficiency gains arise from the Gaussian prior, which smoothly allocates attention to informative transitions, while memory-length priors often truncate useful signals with overly restrictive cut-offs. In particular, Gaussian Attention achieves a 77% relative improvement in mean human-normalized scores over UniZero. These findings suggest that in partially observable RL domains with non-stationary temporal dependencies, discrete memory windows are difficult to learn reliably, whereas smooth distributional priors flexibly adapt across horizons and yield more robust data efficiency. Overall, our results demonstrate that encoding structured temporal priors directly into self-attention improves the prioritization of informative histories for dynamics modeling under partial observability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728\u57fa\u4e8eTransformer\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u5f15\u5165\u7ed3\u6784\u5316\u5f52\u7eb3\u5148\u9a8c\uff0c\u901a\u8fc7\u9ad8\u65af\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\u81ea\u6ce8\u610f\u529b\u5728\u5f3a\u5316\u5b66\u4e60\u8f68\u8ff9\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u5728Atari 100k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6807\u51c6\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u5b83\u5747\u5300\u5206\u914d\u6743\u91cd\u7ed9\u6240\u6709\u5386\u53f2\u6807\u8bb0\uff0c\u800cRL\u8f68\u8ff9\u7a00\u758f\u4e14\u5956\u52b1\u9a71\u52a8\uff0c\u53ea\u6709\u5c11\u6570\u5173\u952e\u8f6c\u6362\u5bf9\u63a7\u5236\u91cd\u8981\u3002", "method": "\u5728\u52a8\u6001\u5934\u90e8\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u5f15\u5165\u4e24\u79cd\u7ed3\u6784\u5316\u5f52\u7eb3\u5148\u9a8c\uff1a(1) \u6bcf\u5934\u8bb0\u5fc6\u957f\u5ea6\u5148\u9a8c\uff0c\u5c06\u6ce8\u610f\u529b\u9650\u5236\u5728\u4efb\u52a1\u7279\u5b9a\u7a97\u53e3\uff1b(2) \u5206\u5e03\u5148\u9a8c\uff0c\u5b66\u4e60\u5bf9\u8fc7\u53bb\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\u5e73\u6ed1\u9ad8\u65af\u52a0\u6743\u3002", "result": "\u5728Atari 100k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9ad8\u65af\u6ce8\u610f\u529b\u76f8\u6bd4UniZero\u5b9e\u73b0\u4e8677%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u5927\u591a\u6570\u6548\u7387\u63d0\u5347\u6765\u81ea\u9ad8\u65af\u5148\u9a8c\uff0c\u800c\u8bb0\u5fc6\u957f\u5ea6\u5148\u9a8c\u5f80\u5f80\u56e0\u9650\u5236\u6027\u622a\u65ad\u800c\u635f\u5931\u6709\u7528\u4fe1\u53f7\u3002", "conclusion": "\u5728\u5177\u6709\u975e\u5e73\u7a33\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u90e8\u5206\u53ef\u89c2\u5bdfRL\u9886\u57df\uff0c\u79bb\u6563\u8bb0\u5fc6\u7a97\u53e3\u96be\u4ee5\u53ef\u9760\u5b66\u4e60\uff0c\u800c\u5e73\u6ed1\u5206\u5e03\u5148\u9a8c\u80fd\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u65f6\u95f4\u8303\u56f4\u5e76\u4ea7\u751f\u66f4\u7a33\u5065\u7684\u6570\u636e\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.07230", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07230", "abs": "https://arxiv.org/abs/2511.07230", "authors": ["Viet-Thanh Pham", "Minghan Wang", "Hao-Han Liao", "Thuy-Trang Vu"], "title": "Discourse Graph Guided Document Translation with Large Language Models", "comment": null, "summary": "Adapting large language models to full document translation remains challenging due to the difficulty of capturing long-range dependencies and preserving discourse coherence throughout extended texts. While recent agentic machine translation systems mitigate context window constraints through multi-agent orchestration and persistent memory, they require substantial computational resources and are sensitive to memory retrieval strategies. We introduce TransGraph, a discourse-guided framework that explicitly models inter-chunk relationships through structured discourse graphs and selectively conditions each translation segment on relevant graph neighbourhoods rather than relying on sequential or exhaustive context. Across three document-level MT benchmarks spanning six languages and diverse domains, TransGraph consistently surpasses strong baselines in translation quality and terminology consistency while incurring significantly lower token overhead.", "AI": {"tldr": "TransGraph\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bdd\u8bed\u56fe\u7684\u6587\u6863\u7ffb\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6587\u6863\u5757\u95f4\u5173\u7cfb\u6765\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u548c\u672f\u8bed\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u7ffb\u8bd1\u4e2d\u96be\u4ee5\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u4fdd\u6301\u8bdd\u8bed\u8fde\u8d2f\u6027\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7ffb\u8bd1\u7cfb\u7edf\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u68c0\u7d22\u654f\u611f\u6027\u95ee\u9898", "method": "\u6784\u5efa\u7ed3\u6784\u5316\u8bdd\u8bed\u56fe\u6765\u663e\u5f0f\u5efa\u6a21\u6587\u6863\u5757\u95f4\u5173\u7cfb\uff0c\u6bcf\u4e2a\u7ffb\u8bd1\u7247\u6bb5\u4ec5\u57fa\u4e8e\u76f8\u5173\u56fe\u90bb\u57df\u8fdb\u884c\u6761\u4ef6\u7ffb\u8bd1\uff0c\u800c\u975e\u4f9d\u8d56\u987a\u5e8f\u6216\u5b8c\u6574\u4e0a\u4e0b\u6587", "result": "\u5728\u4e09\u4e2a\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6db5\u76d6\u516d\u79cd\u8bed\u8a00\u548c\u591a\u4e2a\u9886\u57df\uff0cTransGraph\u5728\u7ffb\u8bd1\u8d28\u91cf\u548c\u672f\u8bed\u4e00\u81f4\u6027\u65b9\u9762\u6301\u7eed\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4etoken\u5f00\u9500", "conclusion": "TransGraph\u901a\u8fc7\u8bdd\u8bed\u56fe\u5f15\u5bfc\u7684\u7ffb\u8bd1\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6587\u6863\u7ffb\u8bd1\u4e2d\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7ffb\u8bd1\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c", "topic": "agent analysis"}}
{"id": "2511.07288", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07288", "abs": "https://arxiv.org/abs/2511.07288", "authors": ["Sayambhu Sen", "Shalabh Bhatnagar"], "title": "Enabling Off-Policy Imitation Learning with Deep Actor Critic Stabilization", "comment": "14 pages and 4 images", "summary": "Learning complex policies with Reinforcement Learning (RL) is often hindered by instability and slow convergence, a problem exacerbated by the difficulty of reward engineering. Imitation Learning (IL) from expert demonstrations bypasses this reliance on rewards. However, state-of-the-art IL methods, exemplified by Generative Adversarial Imitation Learning (GAIL)Ho et. al, suffer from severe sample inefficiency. This is a direct consequence of their foundational on-policy algorithms, such as TRPO Schulman et.al. In this work, we introduce an adversarial imitation learning algorithm that incorporates off-policy learning to improve sample efficiency. By combining an off-policy framework with auxiliary techniques specifically, double Q network based stabilization and value learning without reward function inference we demonstrate a reduction in the samples required to robustly match expert behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u79bb\u7b56\u7565\u5b66\u4e60\u7684\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u53ccQ\u7f51\u7edc\u7a33\u5b9a\u5316\u548c\u65e0\u9700\u5956\u52b1\u51fd\u6570\u63a8\u65ad\u7684\u4ef7\u503c\u5b66\u4e60\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387", "motivation": "\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u590d\u6742\u7b56\u7565\u65f6\u5b58\u5728\u4e0d\u7a33\u5b9a\u548c\u6536\u655b\u6162\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5982GAIL\u7531\u4e8e\u57fa\u4e8e\u5728\u7b56\u7565\u7b97\u6cd5\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b", "method": "\u5c06\u79bb\u7b56\u7565\u6846\u67b6\u4e0e\u8f85\u52a9\u6280\u672f\u7ed3\u5408\uff0c\u5305\u62ec\u57fa\u4e8e\u53ccQ\u7f51\u7edc\u7684\u7a33\u5b9a\u5316\u548c\u65e0\u9700\u5956\u52b1\u51fd\u6570\u63a8\u65ad\u7684\u4ef7\u503c\u5b66\u4e60", "result": "\u51cf\u5c11\u4e86\u8fbe\u5230\u4e13\u5bb6\u884c\u4e3a\u6c34\u5e73\u6240\u9700\u7684\u6837\u672c\u6570\u91cf", "conclusion": "\u79bb\u7b56\u7565\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u66f4\u6709\u6548\u5730\u5339\u914d\u4e13\u5bb6\u884c\u4e3a", "topic": "agentic reinforcement learning"}}
{"id": "2511.07332", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07332", "abs": "https://arxiv.org/abs/2511.07332", "authors": ["Aarash Feizi", "Shravan Nayak", "Xiangru Jian", "Kevin Qinghong Lin", "Kaixin Li", "Rabiul Awal", "Xing Han L\u00f9", "Johan Obando-Ceron", "Juan A. Rodriguez", "Nicolas Chapados", "David Vazquez", "Adriana Romero-Soriano", "Reihaneh Rabbany", "Perouz Taslakian", "Christopher Pal", "Spandana Gella", "Sai Rajeswar"], "title": "Grounding Computer Use Agents on Human Demonstrations", "comment": null, "summary": "Building reliable computer-use agents requires grounding: accurately connecting natural language instructions to the correct on-screen elements. While large datasets exist for web and mobile interactions, high-quality resources for desktop environments are limited. To address this gap, we introduce GroundCUA, a large-scale desktop grounding dataset built from expert human demonstrations. It covers 87 applications across 12 categories and includes 56K screenshots, with every on-screen element carefully annotated for a total of over 3.56M human-verified annotations. From these demonstrations, we generate diverse instructions that capture a wide range of real-world tasks, providing high-quality data for model training. Using GroundCUA, we develop the GroundNext family of models that map instructions to their target UI elements. At both 3B and 7B scales, GroundNext achieves state-of-the-art results across five benchmarks using supervised fine-tuning, while requiring less than one-tenth the training data of prior work. Reinforcement learning post-training further improves performance, and when evaluated in an agentic setting on the OSWorld benchmark using o3 as planner, GroundNext attains comparable or superior results to models trained with substantially more data,. These results demonstrate the critical role of high-quality, expert-driven datasets in advancing general-purpose computer-use agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86GroundCUA\u684c\u9762\u73af\u5883\u6570\u636e\u96c6\u548cGroundNext\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u4e13\u5bb6\u6f14\u793a\u6570\u636e\u5b9e\u73b0\u4e86\u684c\u9762UI\u5143\u7d20\u7684\u7cbe\u51c6\u5b9a\u4f4d\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u684c\u9762\u73af\u5883\u4e2d\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0e\u5c4f\u5e55\u5143\u7d20\u51c6\u786e\u8fde\u63a5\u7684\u95ee\u9898\uff0c\u73b0\u6709\u9ad8\u8d28\u91cf\u684c\u9762\u4ea4\u4e92\u6570\u636e\u8d44\u6e90\u6709\u9650\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u684c\u9762\u6570\u636e\u96c6GroundCUA\uff0c\u5305\u542b87\u4e2a\u5e94\u7528\u768456K\u622a\u56fe\u548c3.56M\u4eba\u5de5\u6807\u6ce8\uff1b\u5f00\u53d1GroundNext\u6a21\u578b\u7cfb\u5217\uff0c\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u3002", "result": "GroundNext\u57283B\u548c7B\u89c4\u6a21\u4e0b\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u4ec5\u9700\u4e0d\u5230\u5148\u524d\u5de5\u4f5c\u5341\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u6570\u636e\uff1b\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u4f7f\u7528\u66f4\u591a\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u4e13\u5bb6\u9a71\u52a8\u6570\u636e\u96c6\u5bf9\u63a8\u8fdb\u901a\u7528\u8ba1\u7b97\u673a\u4f7f\u7528\u667a\u80fd\u4f53\u53d1\u5c55\u5177\u6709\u5173\u952e\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "2511.07364", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.07364", "abs": "https://arxiv.org/abs/2511.07364", "authors": ["Vaibhav Mavi", "Shubh Jaroria", "Weiqi Sun"], "title": "Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection", "comment": "Accepted at NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling", "summary": "Reliability and failure detection of large language models (LLMs) is critical for their deployment in high-stakes, multi-step reasoning tasks. Prior work explores confidence estimation for self-evaluating LLM-scorer systems, with confidence scorers estimating the likelihood of errors in LLM responses. However, most methods focus on single-step outputs and overlook the challenges of multi-step reasoning. In this work, we extend self-evaluation techniques to multi-step tasks, testing two intuitive approaches: holistic scoring and step-by-step scoring. Using two multi-step benchmark datasets, we show that stepwise evaluation generally outperforms holistic scoring in detecting potential errors, with up to 15% relative increase in AUC-ROC. Our findings demonstrate that self-evaluating LLM systems provide meaningful confidence estimates in complex reasoning, improving their trustworthiness and providing a practical framework for failure detection.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2dLLM\u7684\u53ef\u9760\u6027\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e86\u6574\u4f53\u8bc4\u5206\u548c\u9010\u6b65\u8bc4\u5206\u4e24\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u9010\u6b65\u8bc4\u4f30\u5728\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u6b65\u8f93\u51fa\uff0c\u5ffd\u89c6\u4e86\u591a\u6b65\u63a8\u7406\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u591a\u6b65\u4efb\u52a1\u53ef\u9760\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u76f4\u89c2\u65b9\u6cd5\uff1a\u6574\u4f53\u8bc4\u5206\u548c\u9010\u6b65\u8bc4\u5206\uff0c\u5728\u4e24\u4e2a\u591a\u6b65\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u9010\u6b65\u8bc4\u4f30\u5728\u68c0\u6d4b\u6f5c\u5728\u9519\u8bef\u65b9\u9762\u666e\u904d\u4f18\u4e8e\u6574\u4f53\u8bc4\u5206\uff0cAUC-ROC\u76f8\u5bf9\u63d0\u5347\u9ad8\u8fbe15%\u3002", "conclusion": "\u81ea\u8bc4\u4f30LLM\u7cfb\u7edf\u5728\u590d\u6742\u63a8\u7406\u4e2d\u80fd\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u63d0\u9ad8\u4e86\u53ef\u4fe1\u5ea6\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5931\u8d25\u68c0\u6d4b\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2511.07328", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.07328", "abs": "https://arxiv.org/abs/2511.07328", "authors": ["Artyom Sorokin", "Nazar Buzun", "Alexander Anokhin", "Oleg Inozemcev", "Egor Vedernikov", "Petr Anokhin", "Mikhail Burtsev", "Trushkov Alexey", "Yin Wenshuai", "Evgeny Burnaev"], "title": "Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training", "comment": "16 pages, 3 figures, 2 tables", "summary": "Retrieval-Augmented Generation (RAG) methods enhance LLM performance by efficiently filtering relevant context for LLMs, reducing hallucinations and inference cost. However, most existing RAG methods focus on single-step retrieval, which is often insufficient for answering complex questions that require multi-step search. Recently, multi-step retrieval approaches have emerged, typically involving the fine-tuning of small LLMs to perform multi-step retrieval. This type of fine-tuning is highly resource-intensive and does not enable the use of larger LLMs. In this work, we propose Q-RAG, a novel approach that fine-tunes the Embedder model for multi-step retrieval using reinforcement learning (RL). Q-RAG offers a competitive, resource-efficient alternative to existing multi-step retrieval methods for open-domain question answering and achieves state-of-the-art results on the popular long-context benchmarks Babilong and RULER for contexts up to 10M tokens.", "AI": {"tldr": "Q-RAG\u662f\u4e00\u79cd\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5d4c\u5165\u6a21\u578b\u7684\u591a\u6b65\u68c0\u7d22\u65b9\u6cd5\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u6b65\u68c0\u7d22\uff0c\u96be\u4ee5\u56de\u7b54\u9700\u8981\u591a\u6b65\u641c\u7d22\u7684\u590d\u6742\u95ee\u9898\u3002\u73b0\u6709\u7684\u591a\u6b65\u68c0\u7d22\u65b9\u6cd5\u9700\u8981\u5fae\u8c03\u5c0fLLM\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u65e0\u6cd5\u4f7f\u7528\u5927\u6a21\u578b", "method": "\u63d0\u51faQ-RAG\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u591a\u6b65\u68c0\u7d22", "result": "\u5728Babilong\u548cRULER\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\uff08\u6700\u591a1000\u4e07tokens\uff09\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c", "conclusion": "Q-RAG\u4e3a\u591a\u6b65\u68c0\u7d22\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ade\u4e89\u6027\u5f3a\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2511.07396", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07396", "abs": "https://arxiv.org/abs/2511.07396", "authors": ["Antonios Valkanas", "Soumyasundar Pal", "Pavel Rumiantsev", "Yingxue Zhang", "Mark Coates"], "title": "C3PO: Optimized Large Language Model Cascades with Probabilistic Cost Constraints for Reasoning", "comment": null, "summary": "Large language models (LLMs) have achieved impressive results on complex reasoning tasks, but their high inference cost remains a major barrier to real-world deployment. A promising solution is to use cascaded inference, where small, cheap models handle easy queries, and only the hardest examples are escalated to more powerful models. However, existing cascade methods typically rely on supervised training with labeled data, offer no theoretical generalization guarantees, and provide limited control over test-time computational cost. We introduce C3PO (Cost Controlled Cascaded Prediction Optimization), a self-supervised framework for optimizing LLM cascades under probabilistic cost constraints. By focusing on minimizing regret with respect to the most powerful model (MPM), C3PO avoids the need for labeled data by constructing a cascade using only unlabeled model outputs. It leverages conformal prediction to bound the probability that inference cost exceeds a user-specified budget. We provide theoretical guarantees on both cost control and generalization error, and show that our optimization procedure is effective even with small calibration sets. Empirically, C3PO achieves state-of-the-art performance across a diverse set of reasoning benchmarks including GSM8K, MATH-500, BigBench-Hard and AIME, outperforming strong LLM cascading baselines in both accuracy and cost-efficiency. Our results demonstrate that principled, label-free cascade optimization can enable scalable LLM deployment.", "AI": {"tldr": "C3PO\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u7684LLM\u7ea7\u8054\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u6210\u672c\u7ea6\u675f\u548c\u9057\u61be\u6700\u5c0f\u5316\u6765\u4f18\u5316\u63a8\u7406\u6210\u672c\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u6784\u5efa\u7ea7\u8054\u7cfb\u7edf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u963b\u788d\u5b9e\u9645\u90e8\u7f72\u3002\u7ea7\u8054\u63a8\u7406\u662f\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u76d1\u7763\u8bad\u7ec3\u3001\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u4e14\u6210\u672c\u63a7\u5236\u6709\u9650\u3002", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u57fa\u4e8e\u672a\u6807\u6ce8\u6a21\u578b\u8f93\u51fa\u6784\u5efa\u7ea7\u8054\uff0c\u901a\u8fc7\u7b26\u5408\u9884\u6d4b\u63a7\u5236\u6210\u672c\u8d85\u51fa\u6982\u7387\uff0c\u4f18\u5316\u5bf9\u6700\u5f3a\u6a21\u578b\u7684\u9057\u61be\u6700\u5c0f\u5316\u3002", "result": "\u5728GSM8K\u3001MATH-500\u7b49\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u539f\u5219\u6027\u7684\u65e0\u6807\u7b7e\u7ea7\u8054\u4f18\u5316\u53ef\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u7684LLM\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2511.07317", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07317", "abs": "https://arxiv.org/abs/2511.07317", "authors": ["Zhiyuan Zeng", "Hamish Ivison", "Yiping Wang", "Lifan Yuan", "Shuyue Stella Li", "Zhuorui Ye", "Siting Li", "Jacqueline He", "Runlong Zhou", "Tong Chen", "Chenyang Zhao", "Yulia Tsvetkov", "Simon Shaolei Du", "Natasha Jaques", "Hao Peng", "Pang Wei Koh", "Hannaneh Hajishirzi"], "title": "RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments", "comment": null, "summary": "We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.", "AI": {"tldr": "\u63d0\u51fa\u4e86RLVE\u65b9\u6cd5\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u73af\u5883\u6765\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u95ee\u9898\u96be\u5ea6\u5206\u5e03\u6765\u907f\u514d\u5b66\u4e60\u4fe1\u53f7\u6d88\u5931\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u9759\u6001\u6570\u636e\u5206\u5e03\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5bfc\u81f4\u7684\u95ee\u9898\u96be\u5ea6\u4e0d\u5339\u914d\uff0c\u5f53\u95ee\u9898\u8fc7\u4e8e\u7b80\u5355\u6216\u56f0\u96be\u65f6\u5b66\u4e60\u4fe1\u53f7\u4f1a\u6d88\u5931\u3002", "method": "\u521b\u5efaRLVE-Gym\u5957\u4ef6\uff0c\u5305\u542b400\u4e2a\u53ef\u9a8c\u8bc1\u73af\u5883\uff0c\u8fd9\u4e9b\u73af\u5883\u80fd\u591f\u6839\u636e\u7b56\u7565\u6a21\u578b\u80fd\u529b\u52a8\u6001\u8c03\u6574\u95ee\u9898\u96be\u5ea6\u5206\u5e03\u3002", "result": "\u57286\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u7edd\u5bf9\u63d0\u53473.37%\uff0c\u76f8\u6bd4\u539f\u59cbRL\u8bad\u7ec3\u4ec5\u83b7\u5f970.49%\u7684\u63d0\u5347\uff0c\u4e14\u8ba1\u7b97\u91cf\u51cf\u5c113\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u73af\u5883\u6269\u5c55\uff08\u589e\u52a0\u8bad\u7ec3\u73af\u5883\u6570\u91cf\uff09\u80fd\u591f\u6301\u7eed\u63d0\u5347\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0cRLVE\u65b9\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.07338", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07338", "abs": "https://arxiv.org/abs/2511.07338", "authors": ["Zhen Wang", "Yufan Zhou", "Zhongyan Luo", "Lyumanshan Ye", "Adam Wood", "Man Yao", "Luoshang Pan"], "title": "DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas", "comment": "12 pages, 5 figures, accepted at LAW 2025 Workshop (NeurIPS 2025)", "summary": "Simulating human profiles by instilling personas into large language models (LLMs) is rapidly transforming research in agentic behavioral simulation, LLM personalization, and human-AI alignment. However, most existing synthetic personas remain shallow and simplistic, capturing minimal attributes and failing to reflect the rich complexity and diversity of real human identities. We introduce DEEPPERSONA, a scalable generative engine for synthesizing narrative-complete synthetic personas through a two-stage, taxonomy-guided method. First, we algorithmically construct the largest-ever human-attribute taxonomy, comprising over hundreds of hierarchically organized attributes, by mining thousands of real user-ChatGPT conversations. Second, we progressively sample attributes from this taxonomy, conditionally generating coherent and realistic personas that average hundreds of structured attributes and roughly 1 MB of narrative text, two orders of magnitude deeper than prior works. Intrinsic evaluations confirm significant improvements in attribute diversity (32 percent higher coverage) and profile uniqueness (44 percent greater) compared to state-of-the-art baselines. Extrinsically, our personas enhance GPT-4.1-mini's personalized question answering accuracy by 11.6 percent on average across ten metrics and substantially narrow (by 31.7 percent) the gap between simulated LLM citizens and authentic human responses in social surveys. Our generated national citizens reduced the performance gap on the Big Five personality test by 17 percent relative to LLM-simulated citizens. DEEPPERSONA thus provides a rigorous, scalable, and privacy-free platform for high-fidelity human simulation and personalized AI research.", "AI": {"tldr": "DEEPPERSONA\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u751f\u6210\u5f15\u64ce\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u3001\u5206\u7c7b\u5b66\u6307\u5bfc\u7684\u65b9\u6cd5\u5408\u6210\u53d9\u4e8b\u5b8c\u6574\u7684\u4eba\u5de5\u89d2\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89d2\u8272\u5c5e\u6027\u7684\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5de5\u89d2\u8272\u5408\u6210\u65b9\u6cd5\u5927\u591a\u6d45\u663e\u7b80\u5355\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4eba\u7c7b\u8eab\u4efd\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u901a\u8fc7\u6316\u6398\u771f\u5b9e\u7528\u6237-ChatGPT\u5bf9\u8bdd\u6784\u5efa\u6700\u5927\u7684\u4eba\u7c7b\u5c5e\u6027\u5206\u7c7b\u5b66\uff1b\u7136\u540e\u4ece\u8be5\u5206\u7c7b\u5b66\u4e2d\u9010\u6b65\u91c7\u6837\u5c5e\u6027\uff0c\u6761\u4ef6\u751f\u6210\u8fde\u8d2f\u771f\u5b9e\u7684\u4eba\u5de5\u89d2\u8272\u3002", "result": "\u5185\u5728\u8bc4\u4f30\u663e\u793a\u5c5e\u6027\u591a\u6837\u6027\u63d0\u534732%\uff0c\u89d2\u8272\u72ec\u7279\u6027\u63d0\u534744%\uff1b\u5916\u5728\u8bc4\u4f30\u4e2d\uff0c\u89d2\u8272\u4f7fGPT-4.1-mini\u7684\u4e2a\u6027\u5316\u95ee\u7b54\u51c6\u786e\u7387\u5e73\u5747\u63d0\u534711.6%\uff0c\u5728\u793e\u4ea4\u8c03\u67e5\u4e2d\u5c06LLM\u6a21\u62df\u516c\u6c11\u4e0e\u771f\u5b9e\u4eba\u7c7b\u54cd\u5e94\u7684\u5dee\u8ddd\u7f29\u5c0f31.7%\u3002", "conclusion": "DEEPPERSONA\u4e3a\u9ad8\u4fdd\u771f\u4eba\u7c7b\u6a21\u62df\u548c\u4e2a\u6027\u5316AI\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u8c28\u3001\u53ef\u6269\u5c55\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u5e73\u53f0\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.738495eb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.algolia.com%2Fresources%2Fasset%2Fbuilding-agentic-ai%3Futm_campaign=tldr_global_b2x_ecomm_ecomm_tof_reach%26utm_medium=display%26utm_source=tldr%26utm_content=tldr_global_b2x_ecomm%26utm_term=ebo-building-agentic-ai%26utm_camp_parent=b2x_ecomm%26utm_2nd_camp=ecomm_tof%26utm_region=global%26utm_goal=reach%26utm_creative_format=prmrynwsl%26utm_model=cpm%26utm_marketing_tactic=reach/2/0100019a5ead334c-d6042afd-8226-4caf-8fe8-9188c90bdb1a-000000/AojfMDiPr9G3tRczSlcjksS54GioQ5w0NKbRAfmKWcA=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.algolia.com%2Fresources%2Fasset%2Fbuilding-agentic-ai%3Futm_campaign=tldr_global_b2x_ecomm_ecomm_tof_reach%26utm_medium=display%26utm_source=tldr%26utm_content=tldr_global_b2x_ecomm%26utm_term=ebo-building-agentic-ai%26utm_camp_parent=b2x_ecomm%26utm_2nd_camp=ecomm_tof%26utm_region=global%26utm_goal=reach%26utm_creative_format=prmrynwsl%26utm_model=cpm%26utm_marketing_tactic=reach/2/0100019a5ead334c-d6042afd-8226-4caf-8fe8-9188c90bdb1a-000000/AojfMDiPr9G3tRczSlcjksS54GioQ5w0NKbRAfmKWcA=430", "authors": ["TLDR Newsletter"], "title": "A technical guide to building agentic systems", "comment": "Source: TLDR Newsletter, Date: 2025-11-07, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.algolia.com%2Fresources%2Fasset%2Fbuilding-agentic-ai%3Futm_campaign=tldr_global_b2x_ecomm_ecomm_tof_reach%26utm_medium=display%26utm_source=tldr%26utm_content=tldr_global_b2x_ecomm%26utm_term=ebo-building-agentic-ai%26utm_camp_parent=b2x_ecomm%26utm_2nd_camp=ecomm_tof%26utm_region=global%26utm_goal=reach%26utm_creative_format=prmrynwsl%26utm_model=cpm%26utm_marketing_tactic=reach/2/0100019a5ead334c-d6042afd-8226-4caf-8fe8-9188c90bdb1a-000000/AojfMDiPr9G3tRczSlcjksS54GioQ5w0NKbRAfmKWcA=430", "summary": "A technical guide to building agentic systems (Sponsor) The next evolution in AI isn't better chat, it's agents that can actually do things: query databases, update systems, and make decisions. But connecting AI to every tool means custom code for each integration.Algolia's new whitepaper breaks it down \u2014 in great detail but with zero BS: How agentic AI works What model context protocol (MCP) enables How to build agents that can search indices, pull analytics, and modify configurations throug...", "source": "tldr", "AI": {"tldr": "\u8be5\u8bba\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u5982\u4f55\u6784\u5efa\u80fd\u591f\u6267\u884c\u5b9e\u9645\u4efb\u52a1\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\uff0c\u5305\u62ec\u67e5\u8be2\u6570\u636e\u5e93\u3001\u66f4\u65b0\u7cfb\u7edf\u548c\u505a\u51fa\u51b3\u7b56\u7b49\u529f\u80fd\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u4e3b\u8981\u505c\u7559\u5728\u804a\u5929\u5c42\u9762\uff0c\u9700\u8981\u53d1\u5c55\u80fd\u591f\u5b9e\u9645\u6267\u884c\u4efb\u52a1\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u4f46\u6bcf\u4e2a\u5de5\u5177\u7684\u96c6\u6210\u90fd\u9700\u8981\u5b9a\u5236\u4ee3\u7801\uff0c\u8fd9\u589e\u52a0\u4e86\u5f00\u53d1\u590d\u6742\u6027\u3002", "method": "\u4f7f\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u6765\u6784\u5efa\u667a\u80fd\u4ee3\u7406\uff0c\u4f7f\u5176\u80fd\u591f\u641c\u7d22\u7d22\u5f15\u3001\u63d0\u53d6\u5206\u6790\u6570\u636e\u5e76\u4fee\u6539\u914d\u7f6e\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u5957\u8be6\u7ec6\u7684\u6280\u672f\u6307\u5357\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u6784\u5efa\u529f\u80fd\u5b8c\u5584\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u3002", "conclusion": "\u667a\u80fd\u4ee3\u7406\u662fAI\u53d1\u5c55\u7684\u4e0b\u4e00\u4e2a\u91cd\u8981\u9636\u6bb5\uff0c\u901a\u8fc7MCP\u534f\u8bae\u53ef\u4ee5\u7b80\u5316\u96c6\u6210\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u66f4\u5f3a\u5927\u7684\u529f\u80fd\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.5d158056", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.stack-ai.com%2F%3Futm_source=newsletter%26utm_medium=sponsorship%26utm_campaign=112025-tldr-lp/1/0100019a5ead334c-d6042afd-8226-4caf-8fe8-9188c90bdb1a-000000/i9N7n4qaIoWFagUVJ3lEHISdUPV_NkbK1gPYGa47vA0=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.stack-ai.com%2F%3Futm_source=newsletter%26utm_medium=sponsorship%26utm_campaign=112025-tldr-lp/1/0100019a5ead334c-d6042afd-8226-4caf-8fe8-9188c90bdb1a-000000/i9N7n4qaIoWFagUVJ3lEHISdUPV_NkbK1gPYGa47vA0=430", "authors": ["TLDR Newsletter"], "title": "Still struggling to get an AI agent working in production?", "comment": "Source: TLDR Newsletter, Date: 2025-11-07, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.stack-ai.com%2F%3Futm_source=newsletter%26utm_medium=sponsorship%26utm_campaign=112025-tldr-lp/1/0100019a5ead334c-d6042afd-8226-4caf-8fe8-9188c90bdb1a-000000/i9N7n4qaIoWFagUVJ3lEHISdUPV_NkbK1gPYGa47vA0=430", "summary": "Still struggling to get an AI agent working in production? (Sponsor) StackAI is the platform for building no-code enterprise AI agents, white-glove support included. Trusted by Morgan & Morgan, Nubank, BAE Systems, Insurama, Monzo...Try the AI solution that actually works.", "source": "tldr", "AI": {"tldr": "StackAI\u662f\u4e00\u4e2a\u65e0\u9700\u4ee3\u7801\u7684\u4f01\u4e1aAI\u4ee3\u7406\u6784\u5efa\u5e73\u53f0\uff0c\u63d0\u4f9b\u767d\u624b\u5957\u652f\u6301\u670d\u52a1", "motivation": "\u89e3\u51b3\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72AI\u4ee3\u7406\u7684\u56f0\u96be\uff0c\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u53ef\u9760\u7684AI\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u4f9b\u65e0\u9700\u4ee3\u7801\u7684AI\u4ee3\u7406\u6784\u5efa\u5e73\u53f0\uff0c\u5305\u542b\u767d\u624b\u5957\u652f\u6301\u670d\u52a1", "result": "\u88abMorgan & Morgan\u3001Nubank\u3001BAE Systems\u3001Insurama\u3001Monzo\u7b49\u77e5\u540d\u4f01\u4e1a\u4fe1\u4efb\u4f7f\u7528", "conclusion": "StackAI\u662f\u4e00\u4e2a\u5b9e\u9645\u53ef\u884c\u7684AI\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5e2e\u52a9\u4f01\u4e1a\u6210\u529f\u90e8\u7f72AI\u4ee3\u7406", "topic": "swe application"}}
{"id": "tldr.2511.300d5734", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2F%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=brandv2_11_10_primary%26utm_content=tldr/2/0100019a6d815a71-031ec7b9-9e35-4184-aaad-c6a71d590b3d-000000/Firlx_DyvT9eLfEpANGGpCbSYgdedjh5LHGqYb-5qeI=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2F%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=brandv2_11_10_primary%26utm_content=tldr/2/0100019a6d815a71-031ec7b9-9e35-4184-aaad-c6a71d590b3d-000000/Firlx_DyvT9eLfEpANGGpCbSYgdedjh5LHGqYb-5qeI=430", "authors": ["TLDR Newsletter"], "title": "Beyond Commands: The Terminal of the Future", "comment": "Source: TLDR Newsletter, Date: 2025-11-10, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2F%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=brandv2_11_10_primary%26utm_content=tldr/2/0100019a6d815a71-031ec7b9-9e35-4184-aaad-c6a71d590b3d-000000/Firlx_DyvT9eLfEpANGGpCbSYgdedjh5LHGqYb-5qeI=430", "summary": "Beyond Commands: The Terminal of the Future (Sponsor) Warp fuses the terminal and IDE into one place, with AI agents built in. Edit files, review diffs, and ship code, all without leaving the platform that is trusted by over 600k developers and ranks ahead of Claude Code and Gemini CLI on Terminal-Bench.Ask Warp agents to:Debug your Docker build errorsSummarize user logs from the last 24 hoursOnboard you to a new part of your codebaseDownload Warp for free and get bonus credits for your first...", "source": "tldr", "AI": {"tldr": "Warp\u5c06\u7ec8\u7aef\u548cIDE\u878d\u5408\uff0c\u5185\u7f6eAI\u4ee3\u7406\uff0c\u8ba9\u5f00\u53d1\u8005\u65e0\u9700\u79bb\u5f00\u5e73\u53f0\u5373\u53ef\u7f16\u8f91\u6587\u4ef6\u3001\u5ba1\u67e5\u5dee\u5f02\u548c\u90e8\u7f72\u4ee3\u7801\uff0c\u5728Terminal-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eClaude Code\u548cGemini CLI\u3002", "motivation": "\u4f20\u7edf\u7ec8\u7aef\u5de5\u5177\u529f\u80fd\u6709\u9650\uff0c\u5f00\u53d1\u8005\u9700\u8981\u5728\u4e0d\u540c\u5de5\u5177\u95f4\u5207\u6362\uff0cWarp\u65e8\u5728\u901a\u8fc7\u878d\u5408\u7ec8\u7aef\u548cIDE\u529f\u80fd\uff0c\u5e76\u96c6\u6210AI\u4ee3\u7406\u6765\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u96c6\u6210\u4e86\u7ec8\u7aef\u3001IDE\u529f\u80fd\u548cAI\u4ee3\u7406\u7684\u5e73\u53f0\uff0c\u652f\u6301\u6587\u4ef6\u7f16\u8f91\u3001\u5dee\u5f02\u5ba1\u67e5\u3001\u4ee3\u7801\u90e8\u7f72\u7b49\u64cd\u4f5c\uff0c\u5e76\u5185\u7f6eAI\u52a9\u624b\u5e2e\u52a9\u8c03\u8bd5\u3001\u65e5\u5fd7\u5206\u6790\u548c\u4ee3\u7801\u5e93\u5bfc\u822a\u3002", "result": "Warp\u5df2\u88ab\u8d85\u8fc760\u4e07\u5f00\u53d1\u8005\u4f7f\u7528\uff0c\u5728Terminal-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u9886\u5148\u4e8eClaude Code\u548cGemini CLI\u3002", "conclusion": "Warp\u901a\u8fc7\u878d\u5408\u7ec8\u7aef\u548cIDE\u529f\u80fd\u5e76\u96c6\u6210AI\u4ee3\u7406\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u4e00\u4f53\u5316\u5f00\u53d1\u4f53\u9a8c\u3002", "topic": "swe application"}}
{"id": "wechat.2511.c6ee86b4", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2NTYzNzc3NA==&mid=2247505512&idx=1&sn=ef660a665a27d93ce91eb89c4f1314be&chksm=fd2bd87a93df9b8617f0c718f133d9ed6359a1b6c166318b59014d0471831652184128d53e46#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2NTYzNzc3NA==&mid=2247505512&idx=1&sn=ef660a665a27d93ce91eb89c4f1314be&chksm=fd2bd87a93df9b8617f0c718f133d9ed6359a1b6c166318b59014d0471831652184128d53e46#rd", "authors": ["AI\u7b97\u6cd5\u79d1\u7814paper"], "title": "\u300c\u6ce8\u610f\u529b\u673a\u5236+<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u300d\u91cd\u78c5\u7a81\u7834\uff01\u8363\u767bScience\u9876\u7ea7\u5b50\u520a\uff01", "comment": "Source: WeChat, Published: 2025-11-11 11:01:01", "summary": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3 pipeline\uff0c\u5148\u5728\u57fa\u7840\u5730\u5f62\u4e0a\u521d\u59cb\u5316\u5730\u56fe\u7f16\u7801\u5b66\u4e60\uff0c\u518d\u5f15\u5165\u590d\u6742\u5730\u5f62\u4e0e\u4e0d\u786e\u5b9a\u6027\u5fae\u8c03\uff0c\u517c\u987e\u6cdb\u5316\u80fd\u529b\u4e0e\u9c81\u68d2\u6027\u3002\u6784\u5efa\u7aef\u5230\u7aef\u7684\u6574\u4f53\u63a7\u5236\u6846\u67b6\uff0c\u65e0\u9700\u4f9d\u8d56\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b49\u4e0a\u5c42\u89c4\u5212\u6a21\u5757\uff0c\u76f4\u63a5\u5c06\u611f\u77e5\u4fe1\u606f\u6620\u5c04\u4e3a\u5173", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3 pipeline\uff0c\u5148\u5728\u57fa\u7840\u5730\u5f62\u4e0a\u521d\u59cb\u5316\u5730\u56fe\u7f16\u7801\u5b66\u4e60\uff0c\u518d\u5f15\u5165\u590d\u6742\u5730\u5f62\u4e0e\u4e0d\u786e\u5b9a\u6027\u5fae\u8c03\uff0c\u517c\u987e\u6cdb\u5316\u80fd\u529b\u4e0e\u9c81\u68d2\u6027\u3002\u6784\u5efa\u7aef\u5230\u7aef\u7684\u6574\u4f53\u63a7\u5236\u6846\u67b6\uff0c\u65e0\u9700\u4f9d\u8d56\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b49\u4e0a\u5c42\u89c4\u5212\u6a21\u5757\uff0c\u76f4\u63a5\u5c06\u611f\u77e5\u4fe1\u606f\u6620\u5c04\u4e3a\u5173", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.a8540286", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0NDE5MTA2Mw==&mid=2653223060&idx=3&sn=85419a59ffe0d7dcead48664d0e31b47&chksm=f33956575397cb4660b82dc22e2244b7053a9a32a04bc70eb3c1461e09db21f2f00a3873945b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0NDE5MTA2Mw==&mid=2653223060&idx=3&sn=85419a59ffe0d7dcead48664d0e31b47&chksm=f33956575397cb4660b82dc22e2244b7053a9a32a04bc70eb3c1461e09db21f2f00a3873945b#rd", "authors": ["\u4fa0\u8bf4"], "title": "\u6295\u8d44<em class=\"highlight\">Agentic</em> AI\u662f\u9762\u5411\u672a\u6765\u4f9b\u5e94\u94fe\u7684\u6218\u7565\u4e3e\u63aa", "comment": "Source: WeChat, Published: 2025-11-11 13:30:00", "summary": "agentic ai\uff1a \u4ece\u81ea\u52a8\u5316\u5230\u81ea\u4e3b\u7684\u65c5\u7a0b\u3002\u7f57\u6208\u7814\u7a76 \u4ece\u6570\u5b57\u5316\u96c4\u5fc3\u5230\u81ea\u4e3b\u5316\u73b0\u5b9e\u3002\u4ece\u4f20\u7edf\u804a\u5929\u673a\u5668\u4eba\u548c rpa \u5230\u81ea\u4e3b\u4ee3 \u673a\u5668\u4eba\u6d41\u7a0b\u81ea\u52a8\u5316\uff08rpa\uff09\u5230\u81ea\u4e3b\u4ee3\u7406\u6d41\u7a0b\u81ea\u52a8\u5316\uff08apa\uff09 \u7406--\u8fc8\u5411\u81ea\u4e3b\u7684\u65c5\u7a0b\u3002", "AI": {"tldr": "agentic ai\uff1a \u4ece\u81ea\u52a8\u5316\u5230\u81ea\u4e3b\u7684\u65c5\u7a0b\u3002\u7f57\u6208\u7814\u7a76 \u4ece\u6570\u5b57\u5316\u96c4\u5fc3\u5230\u81ea\u4e3b\u5316\u73b0\u5b9e\u3002\u4ece\u4f20\u7edf\u804a\u5929\u673a\u5668\u4eba\u548c rpa \u5230\u81ea\u4e3b\u4ee3 \u673a\u5668\u4eba\u6d41\u7a0b\u81ea\u52a8\u5316\uff08rpa\uff09\u5230\u81ea\u4e3b\u4ee3\u7406\u6d41\u7a0b\u81ea\u52a8\u5316\uff08apa\uff09 \u7406--\u8fc8\u5411\u81ea\u4e3b\u7684\u65c5\u7a0b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.3c342c0a", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI2MTA2MTI5OQ==&mid=2647536042&idx=1&sn=820e3a4b2007b32a8f1862ea382fda6c&chksm=f38a0d502cecbd124737396a0f1a1a2a3aa40f2c832e1146ca1b76c474542b9f317df7c6375a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI2MTA2MTI5OQ==&mid=2647536042&idx=1&sn=820e3a4b2007b32a8f1862ea382fda6c&chksm=f38a0d502cecbd124737396a0f1a1a2a3aa40f2c832e1146ca1b76c474542b9f317df7c6375a#rd", "authors": ["AI\u5f00\u53d1\u65e5\u8bb0"], "title": "\u5b9e\u6218\u00b7<em class=\"highlight\">Agentic</em> \u4e0a\u4e0b\u6587\u5de5\u7a0b\uff08\u4e0b\uff09\uff1a\u5b9e\u73b0\u4e00\u4e2a\u53ef\u81ea\u6211\u5b66\u4e60\u4e0e\u8fdb\u5316\u7684<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u539f\u578b", "comment": "Source: WeChat, Published: 2025-11-11 11:38:27", "summary": "\u53c2\u8003 ace \u8bba\u6587\u63d0\u51fa\u7684\u7ed3\u6784\uff0c\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8e\u4e00\u4e2a\u57fa\u4e8e react \u8303\u5f0f \u7684\u667a\u80fd\u4f53\u4e2d\u3002\u8fd9\u4e2a\u667a\u80fd\u4f53\u5177\u5907\u81ea\u6211\u201c\u53cd\u601d\u2014\u5b66\u4e60\u2014\u6210\u957f\u201d\u7684\u80fd\u529b\uff1aplaybook \u95ee\u9898\u8f93\u5165 \u7b56\u7565\u624b\u518c \u68c0\u7d22\u7b56\u7565 reactagent \u63a8\u7406\u4e0e\u884c\u52a8 \u6267\u884c\u7ed3\u679c evaluator \u63a8\u7406\u8f68\u8ff9 \u66f4\u65b0\u7b56\u7565 \u7b54\u6848", "AI": {"tldr": "\u53c2\u8003 ace \u8bba\u6587\u63d0\u51fa\u7684\u7ed3\u6784\uff0c\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8e\u4e00\u4e2a\u57fa\u4e8e react \u8303\u5f0f \u7684\u667a\u80fd\u4f53\u4e2d\u3002\u8fd9\u4e2a\u667a\u80fd\u4f53\u5177\u5907\u81ea\u6211\u201c\u53cd\u601d\u2014\u5b66\u4e60\u2014\u6210\u957f\u201d\u7684\u80fd\u529b\uff1aplaybook \u95ee\u9898\u8f93\u5165 \u7b56\u7565\u624b\u518c \u68c0\u7d22\u7b56\u7565 reactagent \u63a8\u7406\u4e0e\u884c\u52a8 \u6267\u884c\u7ed3\u679c evaluator \u63a8\u7406\u8f68\u8ff9 \u66f4\u65b0\u7b56\u7565 \u7b54\u6848", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.ecd33f0a", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0MjM0MTg4MQ==&mid=2247485680&idx=1&sn=c50df34eb76f42bba8ca08352d9c8ff7&chksm=e83e93686b062060851cd0cca975fbec314b7e6a23ee976cd40bd6ae2ff21180fd8d1c262ef6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0MjM0MTg4MQ==&mid=2247485680&idx=1&sn=c50df34eb76f42bba8ca08352d9c8ff7&chksm=e83e93686b062060851cd0cca975fbec314b7e6a23ee976cd40bd6ae2ff21180fd8d1c262ef6#rd", "authors": ["\u61c2\u70b9AI\u7684\u6d77\u6587"], "title": "\u5c31\u5728\u6628\u5929\uff01\uff01 Google\u65b0\u53d1\u5e0354\u9875Agent\u5f00\u53d1\u6307\u5357\uff1a\u4e94\u5c42\u67b6\u6784\u8be6\u89e3<em class=\"highlight\">Agentic</em> AI", "comment": "Source: WeChat, Published: 2025-11-11 08:03:56", "summary": "google \u65b0\u53d1\u5e0354\u9875 agent\u5f00\u53d1\u6307\u5357\uff0c \u4e94\u5c42\u67b6\u6784\u8be6\u89e3 agentic ai google introduction to agents google introduction to agents and agent architectures acknowledgements content contributors enrique chan mike clark derek egan anant nawalgaria kanchana patlolla julia wiesinger curators and editor", "AI": {"tldr": "google \u65b0\u53d1\u5e0354\u9875 agent\u5f00\u53d1\u6307\u5357\uff0c \u4e94\u5c42\u67b6\u6784\u8be6\u89e3 agentic ai google introduction to agents google introduction to agents and agent architectures acknowledgements content contributors enrique chan mike clark derek egan ...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.8ed11f23", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMTI3Mzc2Ng==&mid=2247491193&idx=1&sn=68f7e76a3862f205ec4ae0c274ef9309&chksm=c0be08f96be5b169132aee591d1afa70b9ee8da64494ce92f350033a3a03fb9d46f48b3e673a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMTI3Mzc2Ng==&mid=2247491193&idx=1&sn=68f7e76a3862f205ec4ae0c274ef9309&chksm=c0be08f96be5b169132aee591d1afa70b9ee8da64494ce92f350033a3a03fb9d46f48b3e673a#rd", "authors": ["\u4e09\u6d41\u6570\u636e"], "title": "\u4ec0\u4e48\u662f <em class=\"highlight\">Agentic</em> AI\uff1f\u5434\u6069\u8fbe\u8fd9\u5957AI\u63d0\u6548\u601d\u8def\u592a\u5f3a\u4e86", "comment": "Source: WeChat, Published: 2025-11-11 08:01:16", "summary": "\u7528\u6237\u4f53\u9a8c len\uff08text\uff09 \u667a\u80fd\u4f53\u7684\u53e6\u4e00\u534a\u8f74\u4e00\uff1a\u5ba2\u89c2 vs. \u4e3b\u89c2\u5ba2\u89c2 \uff08Objective\uff09\uff1a\u7b54\u6848\u662f\u975e\u9ed1\u5373\u767d\u7684\u3002", "AI": {"tldr": "\u7528\u6237\u4f53\u9a8c len\uff08text\uff09 \u667a\u80fd\u4f53\u7684\u53e6\u4e00\u534a\u8f74\u4e00\uff1a\u5ba2\u89c2 vs. \u4e3b\u89c2\u5ba2\u89c2 \uff08Objective\uff09\uff1a\u7b54\u6848\u662f\u975e\u9ed1\u5373\u767d\u7684\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.1a354850", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyODU3NTkwNA==&mid=2247490928&idx=2&sn=c2fbdf41a811042a5146bd0f03904012&chksm=e9d8126f0c93992c3672ad56b27d2e6714236fe5c3c5dc5de87e1f3a924a378eb631cd3c17cc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyODU3NTkwNA==&mid=2247490928&idx=2&sn=c2fbdf41a811042a5146bd0f03904012&chksm=e9d8126f0c93992c3672ad56b27d2e6714236fe5c3c5dc5de87e1f3a924a378eb631cd3c17cc#rd", "authors": ["AI\u8d44\u8baf"], "title": "\u641c\u7d22\u6846\u91cc\u88c5\u4e86\u4e2a\u201c\u5927\u8111\u201d!<em class=\"highlight\">Agentic</em> Search\u4e0d\u4ec5\u80fd\u641c,\u8fd8\u80fd\u89c4\u5212\u6267\u884c\u53cd\u601d,\u5f7b\u5e95\u98a0\u8986\u4f60\u7684\u5de5\u4f5c\u6d41", "comment": "Source: WeChat, Published: 2025-11-11 04:55:42", "summary": "\u8981\u7406\u89e3Agentic Search\u7684\u6280\u672f\u539f\u7406\uff0c\u6211\u4eec\u9700\u8981\u6df1\u5165\u4e86\u89e3\u5176\u5e95\u5c42\u67b6\u6784\u3002\u6839\u636e\u5b66\u672f\u7814\u7a76\uff0c\u73b0\u4ee3\u7684Agentic Search\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u591aAgent\u534f\u540c\u67b6\u6784\u548c\u589e\u5f3aRAG\u6846\u67b6\u6784\u5efa\u3002", "AI": {"tldr": "\u8981\u7406\u89e3Agentic Search\u7684\u6280\u672f\u539f\u7406\uff0c\u6211\u4eec\u9700\u8981\u6df1\u5165\u4e86\u89e3\u5176\u5e95\u5c42\u67b6\u6784\u3002\u6839\u636e\u5b66\u672f\u7814\u7a76\uff0c\u73b0\u4ee3\u7684Agentic Search\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u591aAgent\u534f\u540c\u67b6\u6784\u548c\u589e\u5f3aRAG\u6846\u67b6\u6784\u5efa\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.403e7bbf", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyNDk2MTcyOQ==&mid=2247493885&idx=2&sn=2f3cdb4e069e48826eedaf6cc8012e38&chksm=fb6b533fa132e35be046cb99dc1f8d72ce794b2f2dbaae63d0ac325abd6536a39f156dfcaf0b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyNDk2MTcyOQ==&mid=2247493885&idx=2&sn=2f3cdb4e069e48826eedaf6cc8012e38&chksm=fb6b533fa132e35be046cb99dc1f8d72ce794b2f2dbaae63d0ac325abd6536a39f156dfcaf0b#rd", "authors": ["Agent\u65f6\u4ee3"], "title": "Google\u53d1\u5e0354\u9875Agent\u5f00\u53d1\u6307\u5357\uff1a\u4e94\u5c42\u67b6\u6784\u8be6\u89e3<em class=\"highlight\">Agentic</em> AI", "comment": "Source: WeChat, Published: 2025-11-11 00:57:58", "summary": "\u8fd9\u662fAgentic AI\u7684\u7ec8\u6781\u5f62\u6001\uff0c\u76ee\u524d\u4ecd\u5904\u4e8e\u7814\u7a76\u9636\u6bb5\u3002OpenAI\u7684code interpreter\u67d0\u4e9b\u573a\u666f\u4e0b\u5c55\u73b0\u4e86Level 4\u7684\u96cf\u5f62\u2014\u2014\u5f53\u9047\u5230\u6ca1\u6709\u73b0\u6210\u5de5\u5177\u7684\u4efb\u52a1\u65f6\uff0c\u5b83\u4f1a\u81ea\u5df1\u5199\u4ee3\u7801\u521b\u9020\u5de5\u5177\u3002", "AI": {"tldr": "\u8fd9\u662fAgentic AI\u7684\u7ec8\u6781\u5f62\u6001\uff0c\u76ee\u524d\u4ecd\u5904\u4e8e\u7814\u7a76\u9636\u6bb5\u3002OpenAI\u7684code interpreter\u67d0\u4e9b\u573a\u666f\u4e0b\u5c55\u73b0\u4e86Level 4\u7684\u96cf\u5f62\u2014\u2014\u5f53\u9047\u5230\u6ca1\u6709\u73b0\u6210\u5de5\u5177\u7684\u4efb\u52a1\u65f6\uff0c\u5b83\u4f1a\u81ea\u5df1\u5199\u4ee3\u7801\u521b\u9020\u5de5\u5177\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.707bba01", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI5MjQzOTY3OA==&mid=2247488982&idx=1&sn=3734562dd398c524563831ad81837840&chksm=ed3754398bdf14821ab01313fcf0c3346e0891883d72fd15cf0f94ebd1df0c3b42e675727594#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI5MjQzOTY3OA==&mid=2247488982&idx=1&sn=3734562dd398c524563831ad81837840&chksm=ed3754398bdf14821ab01313fcf0c3346e0891883d72fd15cf0f94ebd1df0c3b42e675727594#rd", "authors": ["\u7cbe\u795e\u6296\u64de\u738b\u5927\u9e4f"], "title": "\u5434\u6069\u8fbe<em class=\"highlight\">Agentic</em> AI\uff08\u4e00\uff09<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u5316\u5de5\u4f5c\u6d41", "comment": "Source: WeChat, Published: 2025-11-10 23:13:55", "summary": "\u90a3\u4ec0\u4e48\u662fAgentic\u5de5\u4f5c\u6d41\uff1f\u5b83\u4e0d\u662f\u4e00\u4e2a\u65b0\u6280\u672f\uff0c\u800c\u662f\u4e00\u79cd\u65b0\u7684\u601d\u7ef4\u65b9\u5f0f\u2014\u2014\u8ba9AI\u6a21\u4eff\u4eba\u7c7b\u771f\u5b9e\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002\u8fd8\u662f\u5199\u8bba\u6587\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u4eba\u7c7b\u4f1a\u600e\u4e48\u505a\uff1f\u601d\u8003\u4e3b\u9898 \u2192 \u5217\u51fa\u5927\u7eb2 \u2192 \u786e\u5b9a\u9700\u8981\u7814\u7a76\u7684\u65b9\u5411", "AI": {"tldr": "\u90a3\u4ec0\u4e48\u662fAgentic\u5de5\u4f5c\u6d41\uff1f\u5b83\u4e0d\u662f\u4e00\u4e2a\u65b0\u6280\u672f\uff0c\u800c\u662f\u4e00\u79cd\u65b0\u7684\u601d\u7ef4\u65b9\u5f0f\u2014\u2014\u8ba9AI\u6a21\u4eff\u4eba\u7c7b\u771f\u5b9e\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002\u8fd8\u662f\u5199\u8bba\u6587\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u4eba\u7c7b\u4f1a\u600e\u4e48\u505a\uff1f\u601d\u8003\u4e3b\u9898 \u2192 \u5217\u51fa\u5927\u7eb2 \u2192 \u786e\u5b9a\u9700\u8981\u7814\u7a76\u7684\u65b9\u5411", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.4b3feb15", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3NjY2NzY1MA==&mid=2649741311&idx=1&sn=cbb4c08746f33a6d851153c8d0a6d855&chksm=86a2590231882c0ec2bc0bc85795ca86f7e4fa48071c95037cc86b78dc651436bafe9d26d506#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3NjY2NzY1MA==&mid=2649741311&idx=1&sn=cbb4c08746f33a6d851153c8d0a6d855&chksm=86a2590231882c0ec2bc0bc85795ca86f7e4fa48071c95037cc86b78dc651436bafe9d26d506#rd", "authors": ["Feisky"], "title": "\u7a0b\u5e8f\u5458\u7684\u53cc\u5341\u4e00\uff1a\u76d8\u70b9\u652f\u6301\u8ba2\u9605\u6a21\u5f0f\u7684\u56fd\u4ea7 AI <em class=\"highlight\">\u5927\u6a21\u578b</em>", "comment": "Source: WeChat, Published: 2025-11-11 12:50:10", "summary": "\u7a0b\u5e8f\u5458\u7684 \u53cc\u5341\u4e00\uff1a\u76d8\u70b9 \u652f\u6301\u8ba2\u9605\u6a21\u5f0f\u7684 \u56fd\u4ea7ai\u5927\u6a21\u578b\u3002tuesday\u3002\u9996\u6708 \u65b9\u821f coding plan lite \u6a21\u578b doubao-seed-code \u5f3a\u529b\u9a71\u52a8 \u5de5\u5177 \u9002\u914dclaude\uff0c code\u7b49\u4e3b\u6d41\u7f16\u7a0b\u5de5\u5177\uff0c \u7528\u91cf \u7528\u91cf\u8fbeclaude pro\u76843\u500d \uffe59.90/\u6708\u3002", "AI": {"tldr": "\u7a0b\u5e8f\u5458\u7684 \u53cc\u5341\u4e00\uff1a\u76d8\u70b9 \u652f\u6301\u8ba2\u9605\u6a21\u5f0f\u7684 \u56fd\u4ea7ai\u5927\u6a21\u578b\u3002tuesday\u3002\u9996\u6708 \u65b9\u821f coding plan lite \u6a21\u578b doubao-seed-code \u5f3a\u529b\u9a71\u52a8 \u5de5\u5177 \u9002\u914dclaude\uff0c code\u7b49\u4e3b\u6d41\u7f16\u7a0b\u5de5\u5177\uff0c \u7528\u91cf \u7528\u91cf\u8fbeclaude pro\u76843\u500d \uffe59.90/\u6708\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.d7dbd13d", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MDY2ODM3NQ==&mid=2247487717&idx=1&sn=073aaf7394f79b8c5c2ef9a54d48ac18&chksm=c3242ac73a487ec9afe64a0b7e23e02aee96b0419dd5ac05792237d3ff29213a7e1a1cfe226e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MDY2ODM3NQ==&mid=2247487717&idx=1&sn=073aaf7394f79b8c5c2ef9a54d48ac18&chksm=c3242ac73a487ec9afe64a0b7e23e02aee96b0419dd5ac05792237d3ff29213a7e1a1cfe226e#rd", "authors": ["AI\u5927\u6a21\u578b\u524d\u6cbf"], "title": "\u5fc5\u77e5\uff01AI <em class=\"highlight\">\u5927\u6a21\u578b</em>\u5e94\u7528\u67b6\u6784\u56fe\uff08\u5168\uff09", "comment": "Source: WeChat, Published: 2025-11-11 12:50:00", "summary": "ai \u5927\u6a21\u578b\u5e94\u7528\u4f01\u4e1a\u7ea7\u5f00\u53d1\u77e5\u8bc6\u4f53\u7cfb \u5927\u6a21\u578b \u5927\u6a21\u578b\u56fe\u7247\u6587\u5b57\u8bc6\u522b\u5e94 \u5927\u6a21\u578b\u8bed\u97f3\u8bc6\u522b\u5e94\u7528\u6848 \u5927\u6a21\u578b\u8bbe\u8ba1\u5e73\u53f0\u5e94\u7528\u6848 \u5927\u6a21\u578b\u6570\u5b57\u4eba\u5e94\u7528\u6848 \u591a\u6a21\u6001\u5e94\u7528\u6848\u4f8b \u7528\u6848\u4f8b\uff1aocr \u7b49\u3002", "AI": {"tldr": "ai \u5927\u6a21\u578b\u5e94\u7528\u4f01\u4e1a\u7ea7\u5f00\u53d1\u77e5\u8bc6\u4f53\u7cfb \u5927\u6a21\u578b \u5927\u6a21\u578b\u56fe\u7247\u6587\u5b57\u8bc6\u522b\u5e94 \u5927\u6a21\u578b\u8bed\u97f3\u8bc6\u522b\u5e94\u7528\u6848 \u5927\u6a21\u578b\u8bbe\u8ba1\u5e73\u53f0\u5e94\u7528\u6848 \u5927\u6a21\u578b\u6570\u5b57\u4eba\u5e94\u7528\u6848 \u591a\u6a21\u6001\u5e94\u7528\u6848\u4f8b \u7528\u6848\u4f8b\uff1aocr \u7b49\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2511.f5ee9417", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMzg0NjY2NA==&mid=2247493674&idx=1&sn=829ab124c9652f835848bf97a88be79c&chksm=9aba4da531456e1d420c7a07e94f2011d45141f834fae5bb410f89432899ffc6d7ff5be9f090#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMzg0NjY2NA==&mid=2247493674&idx=1&sn=829ab124c9652f835848bf97a88be79c&chksm=9aba4da531456e1d420c7a07e94f2011d45141f834fae5bb410f89432899ffc6d7ff5be9f090#rd", "authors": ["\u5408\u5408\u4fe1\u606f"], "title": "\u8ba9\u667a\u80fd\u4f53\u5f00\u53d1\u5982\u201c\u62fc\u79ef\u6728\u201d\u822c\u4fbf\u5229\uff01\u5408\u5408\u4fe1\u606f\u643a\u624b\u706b\u5c71\u5f15\u64ce\u5171\u63a2<em class=\"highlight\">\u5927\u6a21\u578b</em>\u843d\u5730\u65b0\u8303\u5f0f", "comment": "Source: WeChat, Published: 2025-11-11 10:12:58", "summary": "\u4e2d\u56fd\u4fe1\u901a\u96627\u6708\u6570\u636e\u663e\u793a\uff0c\u56fd\u5185\u5df2\u53d1\u5e03\u5927\u6a21\u578b\u8d85\u8fc71500\u4e2a\u3002\u5982\u4f55\u6253\u901a\u8bf8\u591a\u5927\u6a21\u578b\u843d\u5730\u7684\u201c\u6700\u540e\u4e00\u516c\u91cc\u201d\uff0c\u8ba9\u6280\u672f\u4ece\u201c\u5b9e\u9a8c\u5ba4\u201d\u8d70\u5411\u201c\u751f\u4ea7\u7ebf\u201d\uff0c\u6210\u4e3a\u5168\u884c\u4e1a\u5171\u540c\u63a2\u7d22\u7684\u8bfe\u9898\u3002", "AI": {"tldr": "\u4e2d\u56fd\u4fe1\u901a\u96627\u6708\u6570\u636e\u663e\u793a\uff0c\u56fd\u5185\u5df2\u53d1\u5e03\u5927\u6a21\u578b\u8d85\u8fc71500\u4e2a\u3002\u5982\u4f55\u6253\u901a\u8bf8\u591a\u5927\u6a21\u578b\u843d\u5730\u7684\u201c\u6700\u540e\u4e00\u516c\u91cc\u201d\uff0c\u8ba9\u6280\u672f\u4ece\u201c\u5b9e\u9a8c\u5ba4\u201d\u8d70\u5411\u201c\u751f\u4ea7\u7ebf\u201d\uff0c\u6210\u4e3a\u5168\u884c\u4e1a\u5171\u540c\u63a2\u7d22\u7684\u8bfe\u9898\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.34c3a643", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyOTkwNTQ5Mg==&mid=2247489472&idx=1&sn=fa483df0415d4f7060c3dc7d65748565&chksm=fb03f231c834ed580541403341fc9c1ae7022009af30e144db1bb737182fae9f2c01192a0166#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyOTkwNTQ5Mg==&mid=2247489472&idx=1&sn=fa483df0415d4f7060c3dc7d65748565&chksm=fb03f231c834ed580541403341fc9c1ae7022009af30e144db1bb737182fae9f2c01192a0166#rd", "authors": ["\u9752\u85e4\u667a\u5e93"], "title": "\u7f51\u7edc\u5b89\u5168<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u8def\u7ebf\u548c\u65b9\u5411", "comment": "Source: WeChat, Published: 2025-11-11 10:00:23", "summary": "\u4f7f\u7528\u5927\u6a21\u578b\u8fdb\u884c\u653b\u51fb\u65b9\u9762\u7684\u5e94\u7528\u662f\u6709\u95e8\u69db\u7684\uff0c\u4f46\u662f\u73b0\u5728\u5f00\u6e90\u5927\u6a21\u578b\u7684\u666e\u904d\u4f7f\u7528\uff0c\u8ba9\u5927\u6a21\u578b\u8fdb\u884c\u7f51\u7edc\u653b\u51fb\u662f\u6709\u4e86\u66f4\u597d\u7684\u57fa\u5ea7\uff0c\u53ef\u4ee5\u4f7f\u7528SFT\u6280\u672f\uff0cRL\u6280\u672f\uff0c\u6a21\u578b\u7f16\u8f91\uff08model editing\uff09\u6280\u672f\uff0c\u53ef\u4ee5\u5229\u7528\u8fd9\u4e9b\u5f00\u6e90\u5927\u6a21\u578b\u6784\u9020\u51fa\u4e00\u4e2a\u66f4\u504f\u5411\u4e8e\u7f51", "AI": {"tldr": "\u4f7f\u7528\u5927\u6a21\u578b\u8fdb\u884c\u653b\u51fb\u65b9\u9762\u7684\u5e94\u7528\u662f\u6709\u95e8\u69db\u7684\uff0c\u4f46\u662f\u73b0\u5728\u5f00\u6e90\u5927\u6a21\u578b\u7684\u666e\u904d\u4f7f\u7528\uff0c\u8ba9\u5927\u6a21\u578b\u8fdb\u884c\u7f51\u7edc\u653b\u51fb\u662f\u6709\u4e86\u66f4\u597d\u7684\u57fa\u5ea7\uff0c\u53ef\u4ee5\u4f7f\u7528SFT\u6280\u672f\uff0cRL\u6280\u672f\uff0c\u6a21\u578b\u7f16\u8f91\uff08model editing\uff09\u6280\u672f\uff0c\u53ef\u4ee5\u5229\u7528\u8fd9\u4e9b\u5f00\u6e90\u5927\u6a21\u578b\u6784\u9020\u51fa\u4e00\u4e2a\u66f4\u504f\u5411\u4e8e\u7f51", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.b871e18d", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4OTU2MjgzMg==&mid=2247531363&idx=1&sn=7c8c33d452c0e87b2677e56f78aab63e&chksm=fc3638c1024bb34462acb6f341f183442693c42119ef54136074c0da94cc398bc2be4c7177ff#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4OTU2MjgzMg==&mid=2247531363&idx=1&sn=7c8c33d452c0e87b2677e56f78aab63e&chksm=fc3638c1024bb34462acb6f341f183442693c42119ef54136074c0da94cc398bc2be4c7177ff#rd", "authors": ["\u4e2d\u56fd\u8054\u901a\u7814\u7a76\u9662"], "title": "\u671f\u520a\u6587\u7ae0 | \u5f20\u6c49\u5b81\u7b49\uff1a\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4f2a\u9020\u5185\u5bb9\u68c0\u6d4b\u6280\u672f\u7814\u7a76", "comment": "Source: WeChat, Published: 2025-11-11 08:00:27", "summary": "1.1 \u751f\u6210\u5f0fAI\u5927\u6a21\u578b\u751f\u6210\u5f0fAI\u6280\u672f\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u751f\u6210\u6a21\u578b\uff0c\u5df2\u7ecf\u5728\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7b49\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\u3002\u4ece\u65e9\u671f\u7684GAN\uff08Generative Adversarial Networks\uff09[5]\u3001VAE\uff08Variational Auto-Encoders\uff09\uff0c\u5230\u8fd1\u671f\u7684Diffusion Models[6]", "AI": {"tldr": "1.1 \u751f\u6210\u5f0fAI\u5927\u6a21\u578b\u751f\u6210\u5f0fAI\u6280\u672f\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u751f\u6210\u6a21\u578b\uff0c\u5df2\u7ecf\u5728\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7b49\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\u3002\u4ece\u65e9\u671f\u7684GAN\uff08Generative Adversarial Networks\uff09[5]\u3001VAE\uff08Variational Auto-Encoders\uff09\uff0c\u5230\u8fd1\u671f\u7684Diffusion Models[6]", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
