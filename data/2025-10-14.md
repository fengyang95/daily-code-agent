<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 29]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.LG](#cs.LG) [Total: 27]
- [cs.SE](#cs.SE) [Total: 17]
- [tldr.article](#tldr.article) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Preference-Aware Memory Update for Long-Term LLM Agents](https://arxiv.org/abs/2510.09720)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 提出了偏好感知记忆更新机制(PAMU)，通过滑动窗口平均和指数移动平均的融合来动态优化LLM代理的长期记忆表示，在长期对话任务中显著提升输出质量


<details>
  <summary>Details</summary>
Motivation: 现有长期记忆方法在记忆更新方面存在不足，缺乏根据用户行为和上下文变化动态优化偏好记忆表示的机制

Method: PAMU机制结合滑动窗口平均(SW)和指数移动平均(EMA)，构建融合的偏好感知表示，捕捉短期波动和长期用户趋势

Result: 在LoCoMo数据集的五个任务场景中，该机制显著提升了五个基线模型的输出质量

Conclusion: PAMU机制有效解决了长期对话中记忆动态更新的问题，验证了其有效性

Abstract: One of the key factors influencing the reasoning capabilities of LLM-based
agents is their ability to leverage long-term memory. Integrating long-term
memory mechanisms allows agents to make informed decisions grounded in
historical interactions. While recent advances have significantly improved the
storage and retrieval components, by encoding memory into dense vectors for
similarity search or organizing memory as structured knowledge graphs most
existing approaches fall short in memory updating. In particular, they lack
mechanisms for dynamically refining preference memory representations in
response to evolving user behaviors and contexts. To address this gap, we
propose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic
and personalized memory refinement. By integrating sliding window averages (SW)
with exponential moving averages (EMA), PAMU constructs a fused
preference-aware representation that captures both short-term fluctuations and
long-term user tendencies. We conduct experiments on five task scenarios of the
LoCoMo dataset, and the results show that our mechanism can significantly
improve the output quality of LLM in five baselines, validating its
effectiveness in long-term conversations.

</details>


### [2] [Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement](https://arxiv.org/abs/2510.09738)
*Steve Han,Gilberto Titericz Junior,Tom Balough,Wenfei Zhou*

Main category: cs.CL

TL;DR: 提出了Judge's Verdict Benchmark，一种两步法评估LLM作为响应准确性评估的裁判能力，发现27/54个LLM达到Tier 1性能，其中23个呈现类人判断模式，4个呈现超一致模式。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在复制人类判断方面的能力，特别是在RAG或Agentic管道的响应准确性评估任务中，传统相关性分析不足以全面评估裁判质量。

Method: 两步法：先通过相关性测试筛选对齐良好的裁判，再使用z-score进行类人性测试，识别类人判断(|z| < 1)和超一致判断(z > 1)两种模式。

Result: 测试54个LLM(43个开源，11个闭源)，27个达到Tier 1性能，其中23个类人模式，4个超一致模式。裁判优秀程度不单纯依赖模型大小，而是特定训练策略。

Conclusion: 相关性分析不足以评估裁判，需要基于一致性模式的"图灵测试"，提供了标准化基准来分类LLM裁判到不同性能层级。

Abstract: This research introduces the Judge's Verdict Benchmark, a novel two-step
methodology to evaluate Large Language Models (LLMs) as judges for response
accuracy evaluation tasks. We assess how well 54 LLMs can replicate human
judgment when scoring responses from RAG (Retrieval-Augmented Generation) or
Agentic pipelines against ground truth answers. Our methodology progresses from
traditional correlation analysis to comprehensive Cohen's Kappa analysis that
measures actual agreement patterns. The two-step approach includes: (1) a
correlation test that filters judges with strong alignment, followed by (2) a
human-likeness test using z-scores to identify two distinct judgment patterns:
human-like judgment (|z| < 1) that mimics natural human variation, and
super-consistent judgment (z > 1) that exceeds typical human-to-human agreement
levels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1
performance: 23 models exhibit human-like patterns that preserve the nuances of
human judgment, while 4 models demonstrate super-consistent behavior, a pattern
that could indicate either enhanced reliability or oversimplification of
complex judgments. Testing 43 open-source models (1B-405B parameters) and 11
closed models (GPT, Gemini, Claude variants), we demonstrate that judge
excellence is not solely dependent on model size but on specific training
strategies. Our key contributions include: (1) establishing that correlation
alone is insufficient for judge evaluation, (2) introducing a "Turing Test for
judges" based on agreement patterns, and (3) providing a standardized benchmark
for classifying LLM judges into distinct performance tiers for different
evaluation needs.

</details>


### [3] [Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning](https://arxiv.org/abs/2510.09770)
*Adam Byerly,Daniel Khashabi*

Main category: cs.CL

TL;DR: 提出Gold Panning Bandits框架，利用LLM的位置偏见作为诊断信号，通过重新排序文档来高效识别相关内容，相比随机排列基线减少65%的查询次数


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多文档环境中存在强烈的位置偏见，现有方法将其视为需要缓解的噪声，但本研究旨在将其转化为可用于高效内容识别的诊断信号

Method: 将重排序问题建模为二分图匹配问题，提出贪心O(N log N)策略，将最不确定的文档放在最具信息量的位置，相比匈牙利算法的O(N^3)复杂度更高效

Result: 在知识密集型NLP任务中，相比随机排列基线，该方法使用高达65%更少的语言模型查询来识别相关文档，显著降低计算成本且无需模型重训练

Conclusion: 研究表明，固有的LLM偏见可以从负担转化为资产，用于实现高效的推理时优化

Abstract: Large language models exhibit a strong position bias in multi-document
contexts, systematically prioritizing information based on location rather than
relevance. While existing approaches treat this bias as noise to be mitigated,
we introduce Gold Panning Bandits, a framework that leverages position bias as
a diagnostic signal: by reordering documents and observing shifts in the
model's responses, we can efficiently identify the most relevant content. We
frame the problem of choosing reorderings as a bipartite matching problem.
While an optimal assignment can be computed at each iteration with the
Hungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \log N)$
strategy that achieves comparable performance by prioritizing the placement of
the most uncertain documents in the most informative positions. Our approach
identifies relevant documents using up to 65\% fewer language model queries
than random permutation baselines on knowledge-intensive NLP tasks,
substantially reducing computational cost without model retraining. This work
demonstrates that inherent LLM biases can be transformed from liabilities into
assets for efficient, inference-time optimization.

</details>


### [4] [NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering](https://arxiv.org/abs/2510.09854)
*Kaiwen Shi,Zheyuan Zhang,Zhengqing Yuan,Keerthiram Murugesan,Vincent Galass,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: NG-Router是一个新颖的框架，将营养问答建模为基于知识图谱的多智能体协作问题，通过图神经网络学习任务感知的路由分布，并使用梯度子图检索解决上下文过载问题。


<details>
  <summary>Details</summary>
Motivation: 现有营养问答方法面临单智能体推理能力有限和多智能体架构设计复杂的问题，以及上下文过载阻碍准确决策的挑战。

Method: 将智能体节点集成到异构知识图谱中，使用图神经网络学习任务感知的路由分布，并采用梯度子图检索机制识别关键证据。

Result: 在多个基准测试和骨干模型上的广泛实验表明，NG-Router始终优于单智能体和集成基线方法。

Conclusion: NG-Router为复杂营养健康任务提供了一种基于领域感知的多智能体推理原则性方法。

Abstract: Diet plays a central role in human health, and Nutrition Question Answering
(QA) offers a promising path toward personalized dietary guidance and the
prevention of diet-related chronic diseases. However, existing methods face two
fundamental challenges: the limited reasoning capacity of single-agent systems
and the complexity of designing effective multi-agent architectures, as well as
contextual overload that hinders accurate decision-making. We introduce
Nutritional-Graph Router (NG-Router), a novel framework that formulates
nutritional QA as a supervised, knowledge-graph-guided multi-agent
collaboration problem. NG-Router integrates agent nodes into heterogeneous
knowledge graphs and employs a graph neural network to learn task-aware routing
distributions over agents, leveraging soft supervision derived from empirical
agent performance. To further address contextual overload, we propose a
gradient-based subgraph retrieval mechanism that identifies salient evidence
during training, thereby enhancing multi-hop and relational reasoning.
Extensive experiments across multiple benchmarks and backbone models
demonstrate that NG-Router consistently outperforms both single-agent and
ensemble baselines, offering a principled approach to domain-aware multi-agent
reasoning for complex nutritional health tasks.

</details>


### [5] [CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs](https://arxiv.org/abs/2510.09871)
*Nafiseh Nikeghbal,Amir Hossein Kargaran,Jana Diesner*

Main category: cs.CL

TL;DR: CoBia是一个轻量级对抗攻击套件，通过构造包含偏见声明的对话来测试LLMs能否从虚构的偏见中恢复并拒绝偏见性后续问题，揭示了LLMs在对话中存在的偏见放大问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs通过了标准安全检查，但在对话中仍会表现出有害行为（如种族主义观点）。需要系统性地分析LLMs在何种条件下会偏离规范或伦理行为。

Method: 创建构造对话，让模型对某个社会群体发表偏见声明，然后评估模型能否从虚构的偏见中恢复并拒绝偏见性后续问题。评估了11个开源和专有LLMs在6个社会人口类别上的表现。

Result: 目的性构造的对话能可靠地揭示偏见放大，LLMs在对话中经常无法拒绝偏见性后续问题。这种压力测试突显了通过互动可以浮现的深层嵌入偏见。

Conclusion: CoBia方法能有效揭示LLMs中隐藏的偏见，表明需要更严格的测试来确保模型的安全性和公平性。

Abstract: Improvements in model construction, including fortified safety guardrails,
allow Large language models (LLMs) to increasingly pass standard safety checks.
However, LLMs sometimes slip into revealing harmful behavior, such as
expressing racist viewpoints, during conversations. To analyze this
systematically, we introduce CoBia, a suite of lightweight adversarial attacks
that allow us to refine the scope of conditions under which LLMs depart from
normative or ethical behavior in conversations. CoBia creates a constructed
conversation where the model utters a biased claim about a social group. We
then evaluate whether the model can recover from the fabricated bias claim and
reject biased follow-up questions. We evaluate 11 open-source as well as
proprietary LLMs for their outputs related to six socio-demographic categories
that are relevant to individual safety and fair treatment, i.e., gender, race,
religion, nationality, sex orientation, and others. Our evaluation is based on
established LLM-based bias metrics, and we compare the results against human
judgments to scope out the LLMs' reliability and alignment. The results suggest
that purposefully constructed conversations reliably reveal bias amplification
and that LLMs often fail to reject biased follow-up questions during dialogue.
This form of stress-testing highlights deeply embedded biases that can be
surfaced through interaction. Code and artifacts are available at
https://github.com/nafisenik/CoBia.

</details>


### [6] [Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey](https://arxiv.org/abs/2510.09988)
*Jiaqi Wei,Xiang Zhang,Yuejin Yang,Wenxuan Huang,Juntai Cao,Sheng Xu,Xiang Zhuang,Zhangyang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Chenyu You,Wanli Ouyang,Siqi Sun*

Main category: cs.CL

TL;DR: 本文提出了一个统一框架，将树搜索算法分解为三个核心组件：搜索机制、奖励公式和转移函数，澄清了奖励信号在测试时扩展和自我改进中的不同作用。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型研究中的审慎树搜索领域存在碎片化，缺乏共同的形式化框架，特别是在奖励信号的角色定位上存在模糊性——它究竟是临时启发式还是持久学习目标。

Method: 引入统一框架，将搜索算法分解为搜索机制、奖励公式和转移函数三个核心组件，并正式区分了用于测试时扩展的搜索指导和用于自我改进的参数化奖励建模。

Result: 建立了组件中心分类法，综合了最先进技术，并为创建自主、自我改进智能体的系统性进展绘制了研究路线图。

Conclusion: 该框架为解决树搜索领域的碎片化问题提供了理论基础，明确了奖励信号在不同应用场景中的角色，为未来研究提供了系统性指导。

Abstract: Deliberative tree search is a cornerstone of modern Large Language Model
(LLM) research, driving the pivot from brute-force scaling toward algorithmic
efficiency. This single paradigm unifies two critical frontiers:
\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve
hard problems, and \textbf{Self-Improvement}, which uses search-generated data
to durably enhance model parameters. However, this burgeoning field is
fragmented and lacks a common formalism, particularly concerning the ambiguous
role of the reward signal -- is it a transient heuristic or a durable learning
target? This paper resolves this ambiguity by introducing a unified framework
that deconstructs search algorithms into three core components: the
\emph{Search Mechanism}, \emph{Reward Formulation}, and \emph{Transition
Function}. We establish a formal distinction between transient \textbf{Search
Guidance} for TTS and durable \textbf{Parametric Reward Modeling} for
Self-Improvement. Building on this formalism, we introduce a component-centric
taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward
more systematic progress in creating autonomous, self-improving agents.

</details>


### [7] [Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning](https://arxiv.org/abs/2510.10009)
*Shu Zhao,Tan Yu,Anbang Xu*

Main category: cs.CL

TL;DR: 提出ExpandSearch方法，通过强化学习训练LLM搜索代理，具备查询扩展能力，结合预训练压缩器模型理解检索文档，在7个问答基准测试中平均提升4.4%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有推理增强搜索代理在复杂多跳问答任务中表现不佳，主要受限于推理和搜索能力不足，难以处理复合查询。

Method: 使用强化学习训练LLM搜索代理，每轮生成多个查询变体并行搜索；引入预训练压缩器模型帮助理解检索文档，让搜索代理专注于查询生成以提高检索召回率。

Result: 在7个问答基准测试中，ExpandSearch相比最先进基线平均提升4.4%准确率，在多跳推理任务上表现尤为突出。

Conclusion: 即使使用小规模3B参数的LLM，结合压缩器模型辅助，也能在查询扩展和多跳问答任务中达到最先进性能。

Abstract: Reasoning-augmented search agents, such as Search-R1, are trained to reason,
search, and generate the final answer iteratively. Nevertheless, due to their
limited capabilities in reasoning and search, their performance on multi-hop QA
benchmarks remains far from satisfactory. To handle complex or compound
queries, we train an LLM-based search agent with the native capability of query
expansion through reinforcement learning. In each turn, our search agent
proposes several query variants, which are searched simultaneously to cover
more relevant information. Meanwhile, given limited post-training data and
computing resources, it is very challenging for a search agent to master
multiple tasks, including query generation, retrieved information
understanding, and answer generation. Therefore, we propose incorporating a
pre-trained squeezer model that helps the search agent understand the retrieved
documents, allowing the search agent to focus on query generation for high
retrieval recall. With the assistance of the squeezer model, we discover that
even a small-scale 3B LLM can demonstrate a strong capability of query
expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.
To be specific, our experiments across seven question-answering benchmarks
demonstrate that our method, named ExpandSearch, achieves an average
improvement of 4.4% compared to state-of-the-art baselines, with strong gains
on multi-hop reasoning tasks requiring diverse evidence aggregation.

</details>


### [8] [HUME: Measuring the Human-Model Performance Gap in Text Embedding Task](https://arxiv.org/abs/2510.10062)
*Adnan El Assadi,Isaac Chung,Roman Solomatin,Niklas Muennighoff,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 提出了HUME框架，用于测量文本嵌入任务中的人类表现，并与模型性能进行比较，发现在16个MTEB数据集上人类平均表现77.6%，略低于最佳模型的80.1%。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型评估框架缺乏可靠的人类性能基准，限制了模型分数的可解释性，需要建立人类表现基线来更好地理解模型的能力和局限。

Method: 开发HUME框架，在16个MTEB数据集上测量人类表现，涵盖重排序、分类、聚类和语义文本相似性任务，覆盖高资源和低资源语言。

Result: 人类平均表现77.6%，最佳模型80.1%；模型在某些数据集上接近天花板性能，但在其他数据集上表现不佳，特别是在低资源语言中。

Conclusion: 提供了人类性能基线、任务难度洞察和可扩展评估框架，有助于更准确地解释模型性能并指导模型和基准测试的开发。

Abstract: Comparing human and model performance offers a valuable perspective for
understanding the strengths and limitations of embedding models, highlighting
where they succeed and where they fail to capture meaning and nuance. However,
such comparisons are rarely made, as human performance on embedding tasks is
difficult to measure. To fill this gap, we introduce HUME: Human Evaluation
Framework for Text Embeddings. While frameworks like MTEB provide broad model
evaluation, they lack reliable estimates of human performance, limiting the
interpretability of model scores. We measure human performance across 16 MTEB
datasets spanning reranking, classification, clustering, and semantic textual
similarity across linguistically diverse high- and low-resource languages.
Humans achieve an average performance of 77.6% compared to 80.1% for the best
embedding model, although variation is substantial: models reach near-ceiling
performance on some datasets while struggling on others, suggesting dataset
issues and revealing shortcomings in low-resource languages. We provide human
performance baselines, insight into task difficulty patterns, and an extensible
evaluation framework that enables a more meaningful interpretation of the model
and informs the development of both models and benchmarks. Our code, dataset,
and leaderboard are publicly available at
https://github.com/embeddings-benchmark/mteb.

</details>


### [9] [DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models](https://arxiv.org/abs/2510.10142)
*Tingxu Han,Wei Song,Ziqi Ding,Ziming Li,Chunrong Fang,Yuekang Li,Dongfang Liu,Zhenyu Chen,Zhenting Wang*

Main category: cs.CL

TL;DR: DiffHeads是一个轻量级的LLM去偏框架，通过识别和选择性屏蔽偏见注意力头来减少模型不公平性，在保持模型效用的同时将不公平性降低49.4%和40.3%。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注偏见输出的出现时机，但缺乏对生成机制的深入理解，导致现有缓解方法脆弱。需要系统研究LLM不公平性并开发有效的去偏方法。

Method: 首先比较直接回答和思维链提示策略，定义token到注意力头的贡献分数来追踪偏见来源，然后基于DA和CoT的差异激活分析识别偏见头，选择性屏蔽这些头。

Result: DA提示会触发LLM的固有偏见，将不公平性提高534.5%-391.9%；发现一小簇偏见头在DA下激活但在CoT下休眠；DiffHeads在DA和CoT下分别减少不公平性49.4%和40.3%。

Conclusion: DiffHeads提供了一种因果解释偏见出现机制的方法，通过轻量级干预有效减少LLM不公平性，同时保持模型性能。

Abstract: Large language models (LLMs) increasingly mediate decisions in domains where
unfair treatment of demographic groups is unacceptable. Existing work probes
when biased outputs appear, but gives little insight into the mechanisms that
generate them, leaving existing mitigations largely fragile. In this paper, we
conduct a systematic investigation LLM unfairness and propose DiffHeads, a
lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)
prompting to Chain-of-Thought (CoT) prompting across eight representative open-
and closed-source LLMs. DA will trigger the nature bias part of LLM and improve
measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.
Next, we define a token-to-head contribution score that traces each token's
influence back to individual attention heads. This reveals a small cluster of
bias heads that activate under DA but stay largely dormant with CoT, providing
the first causal link between prompting strategy and bias emergence. Finally,
building on this insight, we propose DiffHeads that identifies bias heads
through differential activation analysis between DA and CoT, and selectively
masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under
DA and CoT, respectively, without harming model utility.

</details>


### [10] [MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems](https://arxiv.org/abs/2510.10185)
*Lei Gu,Yinghao Zhu,Haoran Sang,Zixiang Wang,Dehao Sui,Wen Tang,Ewen Harrison,Junyi Gao,Lequan Yu,Liantao Ma*

Main category: cs.CL

TL;DR: 对基于LLM的多智能体医疗咨询系统进行大规模实证研究，揭示其内部协作过程中的失败模式，强调仅靠最终答案准确性不足以评估医疗AI的可信度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统在医疗咨询中的评估过于关注最终答案准确性，忽视了内部推理过程的可验证性，这在高风险医疗应用中存在严重风险。

Method: 使用六个医疗数据集和六个代表性多智能体框架，对3600个案例进行混合方法研究，结合定性分析和定量审计。

Result: 识别出四种主要失败模式：共享模型缺陷驱动的错误共识、正确少数意见被压制、无效讨论动态、以及信息合成中的关键信息丢失。

Conclusion: 高准确性不足以建立临床或公众信任，迫切需要透明可审计的推理过程来负责任地开发和部署医疗AI。

Abstract: While large language model (LLM)-based multi-agent systems show promise in
simulating medical consultations, their evaluation is often confined to
final-answer accuracy. This practice treats their internal collaborative
processes as opaque "black boxes" and overlooks a critical question: is a
diagnostic conclusion reached through a sound and verifiable reasoning pathway?
The inscrutable nature of these systems poses a significant risk in high-stakes
medical applications, potentially leading to flawed or untrustworthy
conclusions. To address this, we conduct a large-scale empirical study of 3,600
cases from six medical datasets and six representative multi-agent frameworks.
Through a rigorous, mixed-methods approach combining qualitative analysis with
quantitative auditing, we develop a comprehensive taxonomy of collaborative
failure modes. Our quantitative audit reveals four dominant failure patterns:
flawed consensus driven by shared model deficiencies, suppression of correct
minority opinions, ineffective discussion dynamics, and critical information
loss during synthesis. This study demonstrates that high accuracy alone is an
insufficient measure of clinical or public trust. It highlights the urgent need
for transparent and auditable reasoning processes, a cornerstone for the
responsible development and deployment of medical AI.

</details>


### [11] [Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models](https://arxiv.org/abs/2510.10252)
*Samir Abdaljalil,Erchin Serpedin,Khalid Qaraqe,Hasan Kurban*

Main category: cs.CL

TL;DR: 提出了AoU框架，通过分解查询、审计假设支持度、仅基于已验证前提进行推理来约束推理过程，有效减少LLM的推理诱导幻觉。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在推理过程中产生看似连贯但基于未经验证假设的幻觉结论问题，现有方法主要关注事实性幻觉或依赖事后验证。

Method: AoU框架包含三个阶段：分解查询为候选假设、审计假设支持度、仅基于已验证子集进行推理，形式化为后验约束推理。

Result: 在GSM8K、MultiArith和SVAMP数据集上显著提升准确性和忠实度，相比Chain-of-Thought等方法获得高达30-45%的性能提升。

Conclusion: AoU框架通过约束推理前提验证有效减少推理诱导幻觉，提供理论保证和实际性能提升。

Abstract: Large language models (LLMs) often generate reasoning traces that appear
coherent but rest on unsupported assumptions, leading to hallucinated
conclusions. Prior work mainly addresses factual hallucinations or relies on
post-hoc verification, leaving reasoning-induced hallucinations largely
unaddressed. We propose Audit-of-Understanding (AoU), a framework that
constrains inference to validated premises through three phases: (1)
decomposing a query into candidate assumptions, (2) auditing their support, and
(3) conditioning inference only on the validated subset. Formally, AoU is
\emph{posterior-constrained inference}, connecting to selective prediction and
rejection learning. Our contributions are threefold: (i) theoretical guarantees
under perfect validation, (ii) excess-risk bounds under imperfect audits, and
(iii) tractability analysis. Empirically, AoU improves both accuracy and
faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on
GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over
Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at
https://anonymous.4open.science/r/audit-of-understanding-E28B.

</details>


### [12] [RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2510.10448)
*Zhichao Xu,Minheng Wang,Yawei Wang,Wenqian Ye,Yuntao Du,Yunpu Ma,Yijun Tian*

Main category: cs.CL

TL;DR: RECON框架通过集成显式摘要模块压缩检索证据，在推理循环中减少上下文长度35%，提升训练速度和推理延迟，同时改善RAG在QA基准上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于强化学习的检索增强生成系统存在上下文管理效率低下的问题，检索的长而嘈杂文档会增加成本并降低性能。

Method: RECON框架集成显式摘要模块压缩证据，采用两阶段训练：在QA数据集上进行相关性预训练，然后从专有LLM进行多方面蒸馏以确保事实性和清晰度。

Result: 在Search-R1流水线中，RECON将总上下文长度减少35%，3B模型的平均EM分数提升14.5%，7B模型提升3.0%，在多跳QA中表现尤为突出。

Conclusion: 学习到的上下文压缩对于构建实用、可扩展且高性能的RAG系统至关重要。

Abstract: Retrieval-augmented generation (RAG) systems trained using reinforcement
learning (RL) with reasoning are hampered by inefficient context management,
where long, noisy retrieved documents increase costs and degrade performance.
We introduce RECON (REasoning with CONdensation), a framework that integrates
an explicit summarization module to compress evidence within the reasoning
loop. Our summarizer is trained via a two-stage process: relevance pretraining
on QA datasets, followed by multi-aspect distillation from proprietary LLMs to
ensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON
reduces total context length by 35\%, leading to improved training speed and
inference latency, while simultaneously improving RAG performance on downstream
QA benchmarks. Notably, it boosts the average EM score of the 3B model by
14.5\% and the 7B model by 3.0\%, showing particular strength in multi-hop QA.
RECON demonstrates that learned context compression is essential for building
practical, scalable, and performant RAG systems. Our code implementation is
made available at https://github.com/allfornancy/RECON.

</details>


### [13] [FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth](https://arxiv.org/abs/2510.10472)
*Qiran Zou,Hou Hei Lam,Wenhao Zhao,Yiming Tang,Tingting Chen,Samson Yu,Tianyi Zhang,Chang Liu,Xiangyang Ji,Dianbo Liu*

Main category: cs.CL

TL;DR: FML-bench是一个评估自动机器学习研究代理的基准，专注于8个基础机器学习研究问题，减少编码负担，强调基础问题而非特定用例，提供高任务多样性，并可扩展到真实GitHub仓库。


<details>
  <summary>Details</summary>
Motivation: 现有基准过度强调工程方面而忽视学术严谨性，任务多样性有限，过于关注应用导向任务而非基础研究问题，难以扩展到真实研究环境。

Method: 引入FML-bench基准，包含8个多样化基础机器学习研究问题，采用统一的评估框架和五个互补指标来全面评估代理性能。

Result: 评估显示采用广泛研究探索策略的代理优于专注于狭窄但深入探索的代理。

Conclusion: 强调探索广度可能比仅关注增量改进产生更有效的研究结果。

Abstract: Large language models (LLMs) have sparked growing interest in automatic
machine learning research agents. Among them, agents capable of autonomously
proposing ideas and conducting machine learning experiments are particularly
promising, as they maximize research automation and accelerate scientific
progress by iteratively refining ideas based on experimental results. However,
comprehensively evaluating such agents remains challenging. Existing benchmarks
tend to overemphasize engineering aspects while neglecting academic rigor,
creating barriers that obscure a clear assessment of an agent's scientific
capabilities in machine learning research. They also suffer from limited task
diversity, an overemphasis on application-oriented tasks over fundamental
research problems, and limited scalability to realistic research settings. To
address these limitations, we introduce FML-bench, a benchmark designed to
evaluate automatic machine learning research agents on 8 diverse and
fundamental machine learning research problems. It reduces coding burden,
emphasizes fundamental problems rather than specific use cases, offers high
task diversity, and is extensible to real-world machine learning GitHub
repositories. Furthermore, we present a unified evaluation framework with five
complementary metrics, designed to comprehensively assess agent performance on
our benchmark. We evaluate state-of-the-art automatic research agents on
FML-bench, and find that agents employing broad research exploration strategies
outperform those focusing on narrow but deep exploration. These findings
suggest that emphasizing the breadth of exploration may lead to more effective
research outcomes than focusing solely on incremental refinement. Our benchmark
is available at https://github.com/qrzou/FML-bench.

</details>


### [14] [AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation](https://arxiv.org/abs/2510.10661)
*Omid Reza Heidari,Siobhan Reid,Yassine Yaakoubi*

Main category: cs.CL

TL;DR: AGENTIQL是一个多专家代理框架，通过推理代理、编码代理和优化步骤改进文本到SQL生成，在Spider基准测试中达到86.07%执行准确率，接近GPT-4水平。


<details>
  <summary>Details</summary>
Motivation: 解决传统单体架构在复杂推理和多样化数据库模式下的局限性，提升文本到SQL生成的准确性和可解释性。

Method: 采用多专家代理框架：推理代理进行问题分解，编码代理生成子查询，优化步骤进行列选择，并包含自适应路由器在模块化流程和基线解析器间选择。

Result: 在Spider基准测试中，使用14B模型达到86.07%执行准确率，接近GPT-4的89.65% SOTA水平，同时提高了可解释性。

Conclusion: AGENTIQL提供了一个强大、可扩展且可解释的语义解析方法，通过模块化设计缩小了与GPT-4的差距。

Abstract: LLMs have advanced text-to-SQL generation, yet monolithic architectures
struggle with complex reasoning and schema diversity. We propose AGENTIQL, an
agent-inspired multi-expert framework that combines a reasoning agent for
question decomposition, a coding agent for sub-query generation, and a
refinement step for column selection. An adaptive router further balances
efficiency and accuracy by selecting between our modular pipeline and a
baseline parser. Several steps in the pipeline can be executed in parallel,
making the framework scalable to larger workloads. Evaluated on the Spider
benchmark, AGENTIQL improves execution accuracy and interpretability and
achieves up to 86.07\% EX with 14B models using the Planner&Executor merging
strategy. The attained performance is contingent upon the efficacy of the
routing mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)
while using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances
transparency by exposing intermediate reasoning steps, offering a robust,
scalable, and interpretable approach to semantic parsing.

</details>


### [15] [BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions](https://arxiv.org/abs/2510.10666)
*Zhengbo Zhang,Zhiheng Lyu,Junhao Gong,Hongzhu Yi,Xinming Wang,Yuxuan Zhou,Jiabing Yang,Ping Nie,Yan Huang,Wenhu Chen*

Main category: cs.CL

TL;DR: BrowserAgent是一个基于人类浏览行为的交互式网络代理，通过直接操作浏览器页面解决复杂任务，在多项Open-QA任务中表现优于Search-R1，特别是在多跳问答任务上有约20%的提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Search-R1和WebDancer依赖额外工具将交互式网络环境转换为静态文本，这与人类多样化的浏览器交互行为（如滚动、点击、输入）不符。

Method: 提出BrowserAgent代理，通过Playwright直接操作原始网页，采用两阶段训练（监督微调和拒绝微调）提升泛化能力，并引入显式记忆机制存储关键结论。

Result: 使用比Search-R1少得多的训练数据，BrowserAgent在不同Open-QA任务中取得更有竞争力的结果，在HotpotQA、2Wiki和Bamboogle等多跳问答任务上比Search-R1提升约20%。

Conclusion: BrowserAgent可以作为更先进、更交互式和可扩展的网络代理框架。

Abstract: Efficiently solving real-world problems with LLMs increasingly hinges on
their ability to interact with dynamic web environments and autonomously
acquire external information. While recent research like Search-R1 and
WebDancer demonstrates strong performance in solving web tasks, they heavily
rely on additional tools to convert the interactive web environment into static
text content. This is in contrast to human browsing behaviors, which involve
diverse interactions with the browser, such as scrolling, clicking, and typing.
In this paper, we propose BrowserAgent, a more interactive agent that solves
complex tasks through human-inspired browser actions. BrowserAgent operates
directly on raw web pages via Playwright through a set of predefined browser
actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and
Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities.
Despite using significantly less training data than Search-R1, BrowserAgent
achieves more competitive results across different Open-QA tasks. Additionally,
we introduce an explicit memory mechanism to store key conclusions across
steps, further enhancing the model's reasoning capabilities for long-horizon
tasks. Notably, BrowserAgent-7B can achieve around 20\% improvement over
Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These
results indicate that BrowserAgent can serve as a more advanced framework for
more interactive and scalable web agents.

</details>


### [16] [Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures](https://arxiv.org/abs/2510.10806)
*Mihir Gupte,Paolo Giusto,Ramesh S*

Main category: cs.CL

TL;DR: 提出了一种自下而上的方法，将树状结构（如GitHub仓库）的知识线性化，通过在每个层级生成隐式的聚合摘要，然后与RAG结合使用。


<details>
  <summary>Details</summary>
Motivation: 探索如何最好地表示检索到的知识，以便在结构化数据（特别是树状层次结构）上生成响应，因为现有方法在这方面研究不足。

Method: 采用自下而上的方法线性化树状结构知识，生成隐式的聚合摘要，存储在知识库中，并与RAG直接结合使用。

Result: 与使用原始非结构化代码的RAG相比，响应质量相当，但检索器中的文档数量减少了68%以上，效率显著提升。

Conclusion: 利用隐式线性化知识可能是处理复杂层次数据结构的高效且可扩展策略。

Abstract: Large Language Models (LLMs) are adept at generating responses based on
information within their context. While this ability is useful for interacting
with structured data like code files, another popular method,
Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model's in-context learning. However, it is not well-explored how to best
represent this retrieved knowledge for generating responses on structured data,
particularly hierarchical structures like trees. In this work, we propose a
novel bottom-up method to linearize knowledge from tree-like structures (like a
GitHub repository) by generating implicit, aggregated summaries at each
hierarchical level. This approach enables the knowledge to be stored in a
knowledge base and used directly with RAG. We then compare our method to using
RAG on raw, unstructured code, evaluating the accuracy and quality of the
generated responses. Our results show that while response quality is comparable
across both methods, our approach generates over 68% fewer documents in the
retriever, a significant gain in efficiency. This finding suggests that
leveraging implicit, linearized knowledge may be a highly effective and
scalable strategy for handling complex, hierarchical data structures.

</details>


### [17] [LLM$\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System](https://arxiv.org/abs/2510.10890)
*Yu Chao,Siyu Lin,xiaorong wang,Zhu Zhang,Zihan Zhou,Haoyu Wang,Shuo Wang,Jie Zhou,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: LLM x MapReduce-V3是一个分层模块化的智能体系统，用于生成长篇调研报告。该系统采用多智能体架构，将功能组件实现为独立的MCP服务器，并通过高层规划器动态编排工作流程。


<details>
  <summary>Details</summary>
Motivation: 构建一个模块化的智能体系统，支持人类在环干预，为用户提供对研究过程的更大控制和定制能力，同时提高调研报告的深度和长度。

Method: 采用分层模块化架构，将骨架初始化、摘要构建和骨架优化等功能组件实现为独立的MCP服务器，通过高层规划器根据MCP工具描述和执行历史动态选择模块。

Result: 人类评估显示，该系统在内容深度和长度方面均优于代表性基线方法，证明了基于MCP的模块化规划的优势。

Conclusion: LLM x MapReduce-V3通过分层模块化设计和MCP服务器架构，有效提升了长篇调研报告生成的质量和可控性。

Abstract: We introduce LLM x MapReduce-V3, a hierarchically modular agent system
designed for long-form survey generation. Building on the prior work, LLM x
MapReduce-V2, this version incorporates a multi-agent architecture where
individual functional components, such as skeleton initialization, digest
construction, and skeleton refinement, are implemented as independent
model-context-protocol (MCP) servers. These atomic servers can be aggregated
into higher-level servers, creating a hierarchically structured system. A
high-level planner agent dynamically orchestrates the workflow by selecting
appropriate modules based on their MCP tool descriptions and the execution
history. This modular decomposition facilitates human-in-the-loop intervention,
affording users greater control and customization over the research process.
Through a multi-turn interaction, the system precisely captures the intended
research perspectives to generate a comprehensive skeleton, which is then
developed into an in-depth survey. Human evaluations demonstrate that our
system surpasses representative baselines in both content depth and length,
highlighting the strength of MCP-based modular planning.

</details>


### [18] [Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning](https://arxiv.org/abs/2510.10974)
*Zhiwen Ruan,Yixia Li,He Zhu,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出了关键令牌微调(CFT)方法，仅更新通过反事实扰动识别出的功能关键令牌，在数学推理任务上优于标准监督微调(SFT)，尽管只微调不到12%的令牌。


<details>
  <summary>Details</summary>
Motivation: 标准SFT对所有令牌进行统一惩罚，忽略了只有少数关键令牌决定推理正确性，导致输出多样性减少和泛化能力有限。

Method: CFT方法通过反事实扰动识别功能关键令牌，仅对这些决定性推理步骤进行梯度更新，同时保留非关键令牌的多样性。

Result: 在5个模型、3个模型家族和11个数学推理基准测试中，CFT始终优于标准SFT，提高了生成质量和多样性，并为强化学习提供了更好的初始化。

Conclusion: CFT是一个实用且通用的框架，可实现高效和鲁棒的LLM微调，通过改进采样多样性实现测试时扩展。

Abstract: Large language models (LLMs) primarily rely on supervised fine-tuning (SFT)
as a key method to adapt pre-trained models to domain-specific tasks such as
mathematical reasoning. However, standard SFT uniformly penalizes all tokens,
neglecting that only a small subset of critical tokens determines reasoning
correctness. This uniform supervision often causes reduced output diversity and
limited generalization. We propose Critical Token Fine-tuning (CFT), a simple
yet effective approach that updates only tokens identified as functionally
indispensable via counterfactual perturbations. By focusing gradient signals on
these decisive reasoning steps while preserving the diversity of non-critical
tokens, CFT can enhance both generation and diversity. Extensive experiments on
five models across three families (Qwen, OLMo, LLaMA) and eleven mathematical
reasoning benchmarks show that CFT, despite fine-tuning on less than 12% of
tokens, consistently outperforms standard SFT. Moreover, CFT enables test-time
scaling through improved sampling diversity and provides a stronger
initialization for reinforcement learning, sustaining performance gains in
later training stages while maintaining higher entropy for better exploration.
These results highlight CFT as a practical and general framework for efficient
and robust LLM fine-tuning.

</details>


### [19] [DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety](https://arxiv.org/abs/2510.10994)
*Wei-Chieh Huang,Henry Peng Zou,Yaozu Wu,Dongyuan Li,Yankai Chen,Weizhi Zhang,Yangning Li,Angelo Zangari,Jizhou Guo,Chunyu Miao,Liancheng Fang,Langzhou He,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: 提出了DEEPRESEARCHGUARD框架，通过四阶段安全防护机制解决深度研究框架在报告质量评估和安全防护方面的不足，显著提升防御成功率并降低过度拒绝率。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究框架缺乏充分的评估程序和阶段特定防护，通常只关注问答准确率而忽视报告质量的关键维度（可信度、连贯性、广度、深度、安全性），可能导致有害内容被整合到最终报告中。

Method: 引入DEEPRESEARCHGUARD框架，包含四阶段安全防护机制，并开发DRSAFEBENCH基准测试。评估了多种先进LLM模型，包括GPT-4o、Gemini-2.5-flash等。

Result: DEEPRESEARCHGUARD平均防御成功率提升18.16%，同时降低过度拒绝率6%。输入防护提供最显著的早期保护，计划和研究防护增强引用纪律和来源可信度。

Conclusion: DEEPRESEARCHGUARD能够有效阻止有害内容传播，系统性地提高报告质量，同时避免过高的过度拒绝率。

Abstract: Deep research frameworks have shown promising capabilities in synthesizing
comprehensive reports from web sources. While deep research possesses
significant potential to address complex issues through planning and research
cycles, existing frameworks are deficient in sufficient evaluation procedures
and stage-specific protections. They typically treat evaluation as exact match
accuracy of question-answering, but overlook crucial aspects of report quality
such as credibility, coherence, breadth, depth, and safety. This oversight may
result in hazardous or malicious sources being integrated into the final
report. To address these issues, we introduce DEEPRESEARCHGUARD, a
comprehensive framework featuring four-stage safeguards with open-domain
evaluation of references and reports. We assess performance across multiple
metrics, e.g., defense success rate and over-refusal rate, and five key report
dimensions. In the absence of a suitable safety benchmark, we introduce
DRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation
spans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash,
DeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success
rate improvement of 18.16% while reducing over-refusal rate by 6%. The input
guard provides the most substantial early-stage protection by filtering out
obvious risks, while the plan and research guards enhance citation discipline
and source credibility. Through extensive experiments, we show that
DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware
defenses that effectively block harmful content propagation, while
systematically improving report quality without excessive over-refusal rates.
The code can be found via https://github.com/Jasonya/DeepResearchGuard.

</details>


### [20] [Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization](https://arxiv.org/abs/2510.11104)
*Junjie Lu,Yuliang Liu,Chaofeng Qu,Wei Shen,Zhouhan Lin,Min Xu*

Main category: cs.CL

TL;DR: 提出CGPO方法，利用置信度信号识别推理过程中的最大不确定性点，应用自生成的非人类推理路径指导来缓解轨迹漂移，在代码和数学推理任务中表现优于使用强模型或人工标注数据的方法。


<details>
  <summary>Details</summary>
Motivation: 当前强化LLM推理的方法倾向于引入对人类推理轨迹的训练偏见，依赖人类或高能力模型对中间步骤的标注限制了探索替代性非人类推理路径，从而约束了可实现的性能。

Method: 提出置信度引导推理路径偏好优化(CGPO)，利用置信度信号识别模型推理过程中的最大不确定性点，并应用自生成的非人类推理路径指导来缓解轨迹漂移。

Result: 实验表明，在相同训练数据量下，使用小模型生成数据的方法在大多数情况下比使用强模型或人工标注数据的方法表现更好。

Conclusion: CGPO方法通过识别最大不确定性点并提供自生成的推理路径指导，能够有效提升模型推理性能，减少对人类标注的依赖。

Abstract: Current approaches for strengthening LLM reasoning tend to introduce a
training bias toward human-like reasoning trajectories. In step-wise preference
optimization, in particular, dependence on human or higher-capacity model
annotations for intermediate steps limits exploration of alternative,
non-human-like reasoning paths and thus constrains achievable performance.
Furthermore, through a small-scale pilot study, we observed that in
approximately 75% of cases, the model's first erroneous step occurs after the
lowest-confidence point. This suggests that guiding the model at its
lowest-confidence point before an error provides more accurate supervision than
locating the first explicit error. In this paper, we propose Confidence-Guided
Reasoning Path Preference Optimization (CGPO), a method that leverages a
confidence signal to identify points of maximal uncertainty in the model's
reasoning process and applies self-generated, non-human-like reasoning-path
guidance to mitigate trajectory drift. Our experiments span diverse models
applied to both code and mathematical reasoning tasks. The results show that,
with the same amount of training data, our method using data generated by a
small model can achieve better performance in most cases compared with
approaches using data generated by a strong model or human-annotated.

</details>


### [21] [TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code](https://arxiv.org/abs/2510.11151)
*Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: TypePilot是一个基于强类型语言的AI代理框架，通过Scala语言增强LLM生成代码的安全性和鲁棒性，显著减少输入验证和注入漏洞。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码经常包含微妙但关键的安全漏洞，在安全敏感系统中部署存在重大风险。

Method: 使用Scala作为代表语言，结合Stainless框架进行形式化验证，构建类型导向的代理化流水线来增强代码安全性。

Result: 实验表明，直接代码生成往往无法强制执行安全约束，而类型导向的代理化流水线显著减轻了输入验证和注入漏洞。

Conclusion: 结构化、类型引导的LLM工作流程有潜力提高高保证领域中自动化代码生成的可信度。

Abstract: Large language Models (LLMs) have shown remarkable proficiency in code
generation tasks across various programming languages. However, their outputs
often contain subtle but critical vulnerabilities, posing significant risks
when deployed in security-sensitive or mission-critical systems. This paper
introduces TypePilot, an agentic AI framework designed to enhance the security
and robustness of LLM-generated code by leveraging strongly typed and
verifiable languages, using Scala as a representative example. We evaluate the
effectiveness of our approach in two settings: formal verification with the
Stainless framework and general-purpose secure code generation. Our experiments
with leading open-source LLMs reveal that while direct code generation often
fails to enforce safety constraints, just as naive prompting for more secure
code, our type-focused agentic pipeline substantially mitigates input
validation and injection vulnerabilities. The results demonstrate the potential
of structured, type-guided LLM workflows to improve the SotA of the
trustworthiness of automated code generation in high-assurance domains.

</details>


### [22] [WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent](https://arxiv.org/abs/2510.11221)
*Tao Li,Jinlong Hu,Yang Wang,Junfeng Liu,Xuejun Liu*

Main category: cs.CL

TL;DR: WebRouter是一个基于信息论的查询特定路由器，通过成本感知变分信息瓶颈目标来压缩输入提示，显著降低LLM驱动的Web代理的运营成本。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的Web代理在Web自动化中面临成本与性能的权衡挑战，复杂的提示包含目标、行动历史和状态信息，导致LLM集成性能下降。

Method: 提出WebRouter，采用成本感知变分信息瓶颈目标，学习输入提示的压缩表示，同时明确惩罚预期运营成本。

Result: 在WebVoyager基准测试的五个真实网站上的实验显示，WebRouter相比GPT-4o基线降低了87.8%的运营成本，仅损失3.8%的准确率。

Conclusion: WebRouter通过信息论方法有效解决了Web代理的成本性能权衡问题，实现了显著的成本节省和良好的性能保持。

Abstract: LLM-brained web agents offer powerful capabilities for web automation but
face a critical cost-performance trade-off. The challenge is amplified by web
agents' inherently complex prompts that include goals, action histories, and
environmental states, leading to degraded LLM ensemble performance. To address
this, we introduce WebRouter, a novel query-specific router trained from an
information-theoretic perspective. Our core contribution is a cost-aware
Variational Information Bottleneck (ca-VIB) objective, which learns a
compressed representation of the input prompt while explicitly penalizing the
expected operational cost. Experiments on five real-world websites from the
WebVoyager benchmark show that WebRouter reduces operational costs by a
striking 87.8\% compared to a GPT-4o baseline, while incurring only a 3.8\%
accuracy drop.

</details>


### [23] [A Theorem-Proving-Based Evaluation of Neural Semantic Parsing](https://arxiv.org/abs/2510.11225)
*Hayate Funakura,Hyunsoo Kim,Koji Mineshima*

Main category: cs.CL

TL;DR: 本文重新评估语义解析器的评估方法，将图匹配与自动定理证明相结合，发现基于图匹配的评估指标无法准确反映逻辑等价性，并提出了逻辑敏感的评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前语义解析器评估主要依赖图匹配指标（如Smatch），但这些指标只捕捉表面重叠而非逻辑等价性，需要更准确的评估方法来支持推理导向的应用。

Method: 比较两种解析器构建方法：监督微调（T5-Small/Base）和少样本上下文学习（GPT-4o/4.1/5），使用图匹配、双向蕴涵定理证明和格式良好性进行综合评估。

Result: 在图匹配上表现良好的模型经常无法产生逻辑等价的公式。归一化减少了目标变异性，提高了格式良好性和逻辑充分性。错误分析显示性能随公式复杂度增加而下降。

Conclusion: 图匹配指标在推理导向应用中存在局限性，需要逻辑敏感的评估和训练目标，以及简化的归一化目标表示。

Abstract: Graph-matching metrics such as Smatch are the de facto standard for
evaluating neural semantic parsers, yet they capture surface overlap rather
than logical equivalence. We reassess evaluation by pairing graph-matching with
automated theorem proving. We compare two approaches to building parsers:
supervised fine-tuning (T5-Small/Base) and few-shot in-context learning
(GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs
using graph-matching, bidirectional entailment between source and target
formulas with a first-order logic theorem prover, and well-formedness. Across
settings, we find that models performing well on graph-matching often fail to
produce logically equivalent formulas. Normalization reduces incidental target
variability, improves well-formedness, and strengthens logical adequacy. Error
analysis shows performance degrades with increasing formula complexity and with
coordination, prepositional phrases, and passive voice; the dominant failures
involve variable binding and indexing, and predicate naming. These findings
highlight limits of graph-based metrics for reasoning-oriented applications and
motivate logic-sensitive evaluation and training objectives together with
simplified, normalized target representations. All code and data for our
experiments are publicly available.

</details>


### [24] [Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://arxiv.org/abs/2510.11370)
*Wenhan Ma,Hailin Zhang,Liang Zhao,Yifan Song,Yudong Wang,Zhifang Sui,Fuli Luo*

Main category: cs.CL

TL;DR: 提出Rollout Routing Replay (R3)方法解决MoE模型中路由机制在强化学习训练中的不稳定性问题，通过记录推理阶段的路由分布并在训练中重放，显著减少训练-推理策略差异。


<details>
  <summary>Details</summary>
Motivation: MoE模型中的路由机制在强化学习训练中会引入不稳定性，甚至导致灾难性训练崩溃。研究发现训练和推理阶段存在显著的路由行为差异，即使在相同条件下，路由框架也会产生不同的专家选择。

Method: 提出R3方法：记录推理引擎中的路由分布，并在训练阶段重放这些分布，从而减少训练-推理策略的KL散度，缓解极端差异，同时不损害训练速度。

Result: 在各种设置下的广泛实验证实，R3成功稳定了RL训练，防止了崩溃，并且在性能上超越了GSPO和TIS等方法。

Conclusion: 这项工作为稳定MoE模型中的强化学习训练提供了新的解决方案。

Abstract: Reinforcement learning (RL) has emerged as a crucial approach for enhancing
the capabilities of large language models. However, in Mixture-of-Experts (MoE)
models, the routing mechanism often introduces instability, even leading to
catastrophic RL training collapse. We analyze the training-inference
consistency of MoE models and identify a notable discrepancy in routing
behaviors between the two phases. Moreover, even under identical conditions,
the routing framework can yield divergent expert selections across repeated
forward passes. To address this foundational inconsistency, we propose Rollout
Routing Replay (R3), a method that records routing distributions from the
inference engine and replays them during training. R3 significantly reduces
training-inference policy KL divergence and mitigates extreme discrepancies
without compromising training speed. Extensive experiments on various settings
confirm that R3 succeeds in stabilizing RL training, preventing collapse and
outperforming methods such as GSPO and TIS. We believe this work can offer a
new solution for stabilizing RL in MoE models.

</details>


### [25] [Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content](https://arxiv.org/abs/2510.11434)
*Dana Sotto Porat,Ella Rabinovich*

Main category: cs.CL

TL;DR: 提出了一种无需依赖自评问卷的数据驱动方法，使用自动人格和性别分类器分析LLM在Reddit开放性问题回复中的人格特征，发现LLM表现出更高的宜人性和更低的神经质。


<details>
  <summary>Details</summary>
Motivation: 研究生成式大语言模型是否在其语言中表现出类似人格和人口统计特征，填补了该领域的研究空白。

Method: 使用自动人格和性别分类器分析LLM在Reddit开放性问题上的回复，并与人类作者的回答进行比较。

Result: 发现LLM系统性地表现出更高的宜人性和更低的神经质，反映了合作和稳定的对话倾向；性别化语言模式与人类作者相似但变异较小。

Conclusion: LLM在人格特征上表现出系统性偏差，为理解生成式AI的人格和人口统计模式提供了新视角。

Abstract: Generative large language models (LLMs) have become central to everyday life,
producing human-like text across diverse domains. A growing body of research
investigates whether these models also exhibit personality- and
demographic-like characteristics in their language. In this work, we introduce
a novel, data-driven methodology for assessing LLM personality without relying
on self-report questionnaires, applying instead automatic personality and
gender classifiers to model replies on open-ended questions collected from
Reddit. Comparing six widely used models to human-authored responses, we find
that LLMs systematically express higher Agreeableness and lower Neuroticism,
reflecting cooperative and stable conversational tendencies. Gendered language
patterns in model text broadly resemble those of human writers, though with
reduced variation, echoing prior findings on automated agents. We contribute a
new dataset of human and model responses, along with large-scale comparative
analyses, shedding new light on the topic of personality and demographic
patterns of generative AI.

</details>


### [26] [Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation](https://arxiv.org/abs/2510.11620)
*Siheng Xiong,Ali Payani,Faramarz Fekri*

Main category: cs.CL

TL;DR: 提出MPPA框架，通过多路径计划聚合和在线Step-DPO方法解决长链推理中的规划错误问题，显著提升小语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法在单次前向传播中生成整个推理链，容易导致推理轨迹偏离，特别是小模型在处理长推理链时由于容量限制更容易出现规划错误。

Method: 提出多路径计划聚合(MPPA)框架，基于token位置的可变间隔生成多个候选计划并聚合；引入在线Step-DPO，使用扭曲序列蒙特卡洛(TSMC)提供可扩展的逐步监督。

Result: 在数学、科学和逻辑推理基准测试中，仅使用10%的SFT数据和5%的偏好对，该方法在多个基础模型和任务上均优于DeepSeek-R1蒸馏基线和结果奖励RL基线。

Conclusion: MPPA框架通过计划探索和聚合有效解决了长链推理中的规划错误问题，结合在线Step-DPO实现了高效训练和更高准确率。

Abstract: Inference-time scaling enhances the reasoning ability of a language model
(LM) by extending its chain-of-thought (CoT). However, existing approaches
typically generate the entire reasoning chain in a single forward pass, which
often leads to CoT derailment, i.e., the reasoning trajectory drifting off
course due to compounding errors. This problem is particularly severe for
smaller LMs with long CoTs due to their limited capacity. To address this, we
analyze raw long CoTs and uncover a reasoning hierarchy consisting of planning
and execution steps. Our analysis reveals that most reasoning errors stem from
incorrect planning. Motivated by this observation, we propose Multi-Path Plan
Aggregation (MPPA), a framework that augments single-pass reasoning with plan
exploration and aggregation. Following a variable interval schedule based on
the token position, MPPA generates multiple candidate plans and aggregates them
into a refined planning step. To maintain efficiency, we adopt a minimal design
in which the base LM serves as the primary policy, while a lightweight LoRA
module implements the plan aggregation policy. We further observe that
outcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K
tokens). To overcome this, we introduce online Step-DPO, a process-level
preference optimization scheme that leverages Twisted Sequential Monte Carlo
(TSMC) to provide scalable stepwise supervision using small LMs. This yields
more efficient training, improved stability, and higher accuracy. Extensive
experiments on challenging math, science, and logical reasoning benchmarks
demonstrate that, with only 10% SFT data and 5% of preference pairs, our method
outperforms both the DeepSeek-R1 distillation baseline and the outcome-reward
RL baseline across multiple base models and tasks.

</details>


### [27] [When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents](https://arxiv.org/abs/2510.11695)
*Lingfei Qian,Xueqing Peng,Yan Wang,Vincent Jim Zhang,Huan He,Hanley Smith,Yi Han,Yueru He,Haohang Li,Yupeng Cao,Yangyang Yu,Alejandro Lopez-Lira,Peng Lu,Jian-Yun Nie,Guojun Xiong,Jimin Huang,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 提出了首个终身实时基准AMA，用于在多市场中评估基于LLM的交易代理，整合了验证交易数据、专家检查新闻和多样化代理架构，在加密货币和股票市场的实时实验显示代理框架比模型骨干对结果差异贡献更大。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究中测试模型而非代理、覆盖期和资产有限、依赖未验证数据的问题，建立公平、连续的实时比较框架。

Method: 引入Agent Market Arena (AMA)基准，集成验证交易数据、专家检查新闻和多样化代理架构，实现四种代理（InvestorAgent、TradeAgent、HedgeFundAgent、DeepFundAgent）并在多个LLM模型上进行评估。

Result: 实时实验显示代理框架表现出明显不同的行为模式，从激进风险承担到保守决策，而模型骨干对结果差异贡献较小。

Conclusion: AMA为基于LLM的代理的金融推理和交易智能评估建立了严谨、可复现和持续演进的基础。

Abstract: Although Large Language Model (LLM)-based agents are increasingly used in
financial trading, it remains unclear whether they can reason and adapt in live
markets, as most studies test models instead of agents, cover limited periods
and assets, and rely on unverified data. To address these gaps, we introduce
Agent Market Arena (AMA), the first lifelong, real-time benchmark for
evaluating LLM-based trading agents across multiple markets. AMA integrates
verified trading data, expert-checked news, and diverse agent architectures
within a unified trading framework, enabling fair and continuous comparison
under real conditions. It implements four agents, including InvestorAgent as a
single-agent baseline, TradeAgent and HedgeFundAgent with different risk
styles, and DeepFundAgent with memory-based reasoning, and evaluates them
across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and
Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets
demonstrate that agent frameworks display markedly distinct behavioral
patterns, spanning from aggressive risk-taking to conservative decision-making,
whereas model backbones contribute less to outcome variation. AMA thus
establishes a foundation for rigorous, reproducible, and continuously evolving
evaluation of financial reasoning and trading intelligence in LLM-based agents.

</details>


### [28] [Demystifying Reinforcement Learning in Agentic Reasoning](https://arxiv.org/abs/2510.11701)
*Zhaochen Yu,Ling Yang,Jiaru Zou,Shuicheng Yan,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文系统研究了强化学习在智能体推理中的关键设计原则，发现真实端到端工具使用轨迹、探索友好技术以及审慎策略能显著提升训练效率和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前智能体强化学习虽然能提升LLMs的推理能力，但关键设计原则和最佳实践仍不明确，需要系统研究。

Method: 从数据、算法和推理模式三个关键角度进行系统研究，使用真实端到端工具使用轨迹、探索友好技术（如clip higher、奖励塑形、保持策略熵）以及审慎推理策略。

Result: 这些简单实践能持续增强智能体推理和训练效率，在挑战性基准测试中取得强劲结果，4B模型也能达到32B模型的推理性能。

Conclusion: 建立了实用的智能体强化学习基线，为未来研究提供指导，并贡献了高质量数据集。

Abstract: Recently, the emergence of agentic RL has showcased that RL could also
effectively improve the agentic reasoning ability of LLMs, yet the key design
principles and optimal practices remain unclear. In this work, we conduct a
comprehensive and systematic investigation to demystify reinforcement learning
in agentic reasoning from three key perspectives: data, algorithm, and
reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic
trajectories with real end-to-end tool-use trajectories yields a far stronger
SFT initialization; high-diversity, model-aware datasets sustain exploration
and markedly improve RL performance. (ii) Exploration-friendly techniques are
crucial for agentic RL, such as clip higher, overlong reward shaping, and
maintaining adequate policy entropy could improve the training efficiency.
(iii) A deliberative strategy with fewer tool calls outperforms frequent tool
calls or verbose self-reasoning, improving tool efficiency and final accuracy.
Together, these simple practices consistently enhance agentic reasoning and
training efficiency, achieving strong results on challenging benchmarks with
smaller models, and establishing a practical baseline for future agentic RL
research. Beyond these empirical insights, we further contribute a
high-quality, real end-to-end agentic SFT dataset along with a high-quality RL
dataset, and demonstrate the effectiveness of our insights in boosting the
agentic reasoning ability of LLMs across four challenging benchmarks, including
AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,
4B-sized models could also achieve superior agentic reasoning performance
compared to 32B-sized models. Code and models:
https://github.com/Gen-Verse/Open-AgentRL

</details>


### [29] [Are Large Reasoning Models Interruptible?](https://arxiv.org/abs/2510.11713)
*Tsung-Han Wu,Mihran Miroyan,David M. Chan,Trevor Darrell,Narges Norouzi,Joseph E. Gonzalez*

Main category: cs.CL

TL;DR: 该论文挑战了大型推理模型在静态评估中的"冻结世界"假设，通过中断和动态上下文两种现实场景评估模型鲁棒性，发现静态评估会高估模型性能，在动态环境下模型性能可能下降高达60%。


<details>
  <summary>Details</summary>
Motivation: 传统评估假设模型响应是即时的且上下文在响应期间不变，但现代推理任务（如辅助编程）中模型可能需要数小时思考，代码在此期间会发生显著变化，"冻结世界"假设不再适用。

Method: 在需要长形式推理的数学和编程基准测试中，评估模型在两种动态场景下的表现：中断（测试模型在有限预算下的部分输出质量）和动态上下文（测试模型对飞行中变化的适应能力）。

Result: 静态评估一致性地高估了模型鲁棒性：即使在静态设置中达到高准确率的最先进LRM，在中断或暴露于变化上下文时也会不可预测地失败，当更新在推理过程后期引入时性能下降高达60%。

Conclusion: 揭示了几个新的失败模式：推理泄漏（模型在中断时将推理折叠到最终答案中）、恐慌（在时间压力下模型完全放弃推理并返回错误答案）和自我怀疑（在整合更新信息时性能下降）。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning but are
traditionally evaluated in static, "frozen world" settings: model responses are
assumed to be instantaneous, and the context of a request is presumed to be
immutable over the duration of the response. While generally true for
short-term tasks, the "frozen world" assumption breaks down in modern reasoning
tasks such as assistive programming, where models may take hours to think
through problems and code may change dramatically from the time the model
starts thinking to the model's final output. In this work, we challenge the
frozen world assumption and evaluate LRM robustness under two realistic dynamic
scenarios: interruptions, which test the quality of the model's partial outputs
on a limited budget, and dynamic context, which tests model adaptation to
in-flight changes. Across mathematics and programming benchmarks that require
long-form reasoning, static evaluations consistently overestimate robustness:
even state-of-the-art LRMs, which achieve high accuracy in static settings, can
fail unpredictably when interrupted or exposed to changing context, with
performance dropping by up to 60% when updates are introduced late in the
reasoning process. Our analysis further reveals several novel failure modes,
including reasoning leakage, where models fold the reasoning into their final
answer when interrupted; panic, where under time pressure models abandon
reasoning entirely and return incorrect answers; and self-doubt, where
performance degrades while incorporating updated information.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [30] [The Geometry of Reasoning: Flowing Logics in Representation Space](https://arxiv.org/abs/2510.09782)
*Yufa Zhou,Yixiao Wang,Xunjian Yin,Shuyan Zhou,Anru R. Zhang*

Main category: cs.AI

TL;DR: 提出了一种几何框架，将大语言模型的推理建模为表示空间中的流（flows），通过分析嵌入轨迹来研究LLM如何在其表示空间中"思考"。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型如何在表示空间中执行推理，将逻辑推理与几何概念（如位置、速度、曲率）联系起来，为LLM行为提供形式化分析的新视角。

Method: 使用几何框架建模LLM推理为表示空间中的流，通过自然演绎命题分离逻辑结构与语义，利用学习到的表示代理设计受控实验来可视化和量化推理流。

Result: 理论建立了：(1) LLM推理对应表示空间中的平滑流；(2) 逻辑语句作为这些流速度的局部控制器。通过实验验证了理论框架。

Conclusion: 该工作为研究推理现象提供了概念基础和实践工具，为LLM行为的可解释性和形式化分析提供了新的视角。

Abstract: We study how large language models (LLMs) ``think'' through their
representation space. We propose a novel geometric framework that models an
LLM's reasoning as flows -- embedding trajectories evolving where logic goes.
We disentangle logical structure from semantics by employing the same natural
deduction propositions with varied semantic carriers, allowing us to test
whether LLMs internalize logic beyond surface form. This perspective connects
reasoning with geometric quantities such as position, velocity, and curvature,
enabling formal analysis in representation and concept spaces. Our theory
establishes: (1) LLM reasoning corresponds to smooth flows in representation
space, and (2) logical statements act as local controllers of these flows'
velocities. Using learned representation proxies, we design controlled
experiments to visualize and quantify reasoning flows, providing empirical
validation of our theoretical framework. Our work serves as both a conceptual
foundation and practical tools for studying reasoning phenomenon, offering a
new lens for interpretability and formal analysis of LLMs' behavior.

</details>


### [31] [How can we assess human-agent interactions? Case studies in software agent design](https://arxiv.org/abs/2510.09801)
*Valerie Chen,Rohit Malhotra,Xingyao Wang,Juan Michelini,Xuhui Zhou,Aditya Bharat Soni,Hoang H. Tran,Calvin Smith,Ameet Talwalkar,Graham Neubig*

Main category: cs.AI

TL;DR: 提出了PULSE框架，用于更高效地评估人机交互中的智能体设计，通过收集用户反馈、训练预测模型和结合人工评分与模型伪标签来计算结果。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多假设完全自动化，未能反映真实世界中人机协作的使用场景，需要更严谨的人类-智能体交互评估方法。

Method: 开发PULSE评估框架，并在基于开源软件智能体OpenHands的大规模网络平台上部署，收集超过15,000名用户的实地使用数据，研究LLM骨干模型、规划策略和记忆机制三个设计决策对开发者满意度的影响。

Result: 框架使智能体设计的置信区间比标准A/B测试减少40%，发现实地结果与基准性能存在显著差异（如claude-sonnet-4与gpt-5的比较结果呈负相关），突显了基准驱动评估的局限性。

Conclusion: 为LLM智能体的人类评估提供指导，并识别了改进智能体设计的机会。

Abstract: LLM-powered agents are both a promising new technology and a source of
complexity, where choices about models, tools, and prompting can affect their
usefulness. While numerous benchmarks measure agent accuracy across domains,
they mostly assume full automation, failing to represent the collaborative
nature of real-world use cases. In this paper, we make two major steps towards
the rigorous assessment of human-agent interactions. First, we propose PULSE, a
framework for more efficient human-centric evaluation of agent designs, which
comprises collecting user feedback, training an ML model to predict user
satisfaction, and computing results by combining human satisfaction ratings
with model-generated pseudo-labels. Second, we deploy the framework on a
large-scale web platform built around the open-source software agent OpenHands,
collecting in-the-wild usage data across over 15k users. We conduct case
studies around how three agent design decisions -- choice of LLM backbone,
planning strategy, and memory mechanisms -- impact developer satisfaction
rates, yielding practical insights for software agent design. We also show how
our framework can lead to more robust conclusions about agent design, reducing
confidence intervals by 40\% compared to a standard A/B test. Finally, we find
substantial discrepancies between in-the-wild results and benchmark performance
(e.g., the anti-correlation between results comparing claude-sonnet-4 and
gpt-5), underscoring the limitations of benchmark-driven evaluation. Our
findings provide guidance for evaluations of LLM agents with humans and
identify opportunities for better agent designs.

</details>


### [32] [Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics](https://arxiv.org/abs/2510.09901)
*Lianhao Zhou,Hongyi Ling,Cong Fu,Yepeng Huang,Michael Sun,Wendi Yu,Xiaoxuan Wang,Xiner Li,Xingyu Su,Junkai Zhang,Xiusi Chen,Chenxing Liang,Xiaofeng Qian,Heng Ji,Wei Wang,Marinka Zitnik,Shuiwang Ji*

Main category: cs.AI

TL;DR: 本文探讨了基于大语言模型的科学代理在科学发现中的变革作用，从假设发现到实验设计和结果分析的全流程应用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的兴起，自主系统（代理）正在加速科学发现，这些语言代理提供了一个灵活框架来协调与科学家、自然语言、计算机语言和物理学的交互。

Method: 批判性审视当前方法论，强调关键创新、实际成就和现存限制，识别开放研究挑战并概述构建更强大、可泛化和适应性科学代理的有前景方向。

Result: 分析突显了自主代理在加速跨领域科学发现方面的变革潜力。

Conclusion: 基于大语言模型的科学代理正在改变科学发现的生命周期，具有加速跨领域科学发现的巨大潜力。

Abstract: Computing has long served as a cornerstone of scientific discovery. Recently,
a paradigm shift has emerged with the rise of large language models (LLMs),
introducing autonomous systems, referred to as agents, that accelerate
discovery across varying levels of autonomy. These language agents provide a
flexible and versatile framework that orchestrates interactions with human
scientists, natural language, computer language and code, and physics. This
paper presents our view and vision of LLM-based scientific agents and their
growing role in transforming the scientific discovery lifecycle, from
hypothesis discovery, experimental design and execution, to result analysis and
refinement. We critically examine current methodologies, emphasizing key
innovations, practical achievements, and outstanding limitations. Additionally,
we identify open research challenges and outline promising directions for
building more robust, generalizable, and adaptive scientific agents. Our
analysis highlights the transformative potential of autonomous agents to
accelerate scientific discovery across diverse domains.

</details>


### [33] [Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs](https://arxiv.org/abs/2510.09970)
*Olivia Peiyu Wang,Tashvi Bansal,Ryan Bai,Emily M. Chui,Leilani H. Gilpin*

Main category: cs.AI

TL;DR: 提出了一种低成本、基于指令的干预方法，通过分步指令数据集和知识图谱验证来改善LLM的逻辑谬误分类能力。


<details>
  <summary>Details</summary>
Motivation: LLM存在关键推理缺陷，包括产生幻觉和在逻辑谬误分类中准确性差，这源于其默认的系统1处理方式，而可靠推理需要系统2的深思熟虑方法。

Method: 引入新颖的分步指令数据集，将谬误分类分解为一系列原子程序步骤（简单的二元问题），并通过查询相关谬误的关系知识图谱进行最终验证。

Result: 这种基于程序的规则干预显著提高了LLM逻辑谬误分类的性能，并为LLM决策过程提供了更好的透明度。

Conclusion: 该方法为神经符号架构解决LLM推理缺陷提供了一条实用途径。

Abstract: Large Language Models (LLMs) suffer from critical reasoning gaps, including a
tendency to hallucinate and poor accuracy in classifying logical fallacies.
This limitation stems from their default System 1 processing, which is fast and
intuitive, whereas reliable reasoning requires the deliberate, effortful System
2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is
often prohibitively expensive, we explore a low-cost, instruction-based
intervention to bridge this gap. Our methodology introduces a novel stepwise
instruction dataset that decomposes fallacy classification into a series of
atomic procedural steps (simple binary questions). We further augment this with
a final verification step where models consult a relational knowledge graph of
related fallacies. This procedural, rule-based intervention yields a
significant improvement in LLM logical fallacy classification. Crucially, the
approach also provides enhanced transparency into the LLMs' decision-making,
highlighting a practical pathway for Neuro-symbolic architectures to address
LLM reasoning deficits.

</details>


### [34] [Deliberative Dynamics and Value Alignment in LLM Debates](https://arxiv.org/abs/2510.10002)
*Pratik S. Sachdeva,Tom van Nuenen*

Main category: cs.AI

TL;DR: 该研究通过LLM辩论分析多轮对话中的道德推理和价值对齐，使用GPT-4.1、Claude 3.7 Sonnet和Gemini 2.0 Flash模型在Reddit的"Am I the Asshole"社区1000个日常困境中进行集体归责判断。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在敏感日常场景中的部署，理解其在复杂道德推理中表现出的价值观至关重要。现有研究多关注单轮提示，但多轮对话中价值观通过对话、修订和共识形成的过程尚不明确。

Method: 使用同步（并行响应）和轮询（顺序响应）两种辩论格式，测试顺序效应和判决修订。让三个模型子集在1000个日常困境中集体分配责任。

Result: 发现显著行为差异：同步设置中GPT表现出强惯性（0.6-3.1%修订率），而Claude和Gemini更灵活（28-41%）。价值观模式也不同：GPT强调个人自主和直接沟通，Claude和Gemini优先考虑同理心对话。某些价值观特别有效驱动判决变化。

Conclusion: 辩论格式对模型行为有强烈影响，GPT和Gemini相对于Claude表现出高度从众性，其判决行为受顺序效应强烈影响。表明社会技术对齐不仅取决于系统输出，还取决于对话结构方式。

Abstract: As large language models (LLMs) are increasingly deployed in sensitive
everyday contexts - offering personal advice, mental health support, and moral
guidance - understanding their elicited values in navigating complex moral
reasoning is essential. Most evaluations study this sociotechnical alignment
through single-turn prompts, but it is unclear if these findings extend to
multi-turn settings where values emerge through dialogue, revision, and
consensus. We address this gap using LLM debate to examine deliberative
dynamics and value alignment in multi-turn settings by prompting subsets of
three models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively
assign blame in 1,000 everyday dilemmas from Reddit's "Am I the Asshole"
community. We use both synchronous (parallel responses) and round-robin
(sequential responses) formats to test order effects and verdict revision. Our
findings show striking behavioral differences. In the synchronous setting, GPT
showed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were
far more flexible (28-41%). Value patterns also diverged: GPT emphasized
personal autonomy and direct communication, while Claude and Gemini prioritized
empathetic dialogue. Certain values proved especially effective at driving
verdict changes. We further find that deliberation format had a strong impact
on model behavior: GPT and Gemini stood out as highly conforming relative to
Claude, with their verdict behavior strongly shaped by order effects. These
results show how deliberation format and model-specific behaviors shape moral
reasoning in multi-turn interactions, underscoring that sociotechnical
alignment depends on how systems structure dialogue as much as on their
outputs.

</details>


### [35] [Failure-Driven Workflow Refinement](https://arxiv.org/abs/2510.10035)
*Jusheng Zhang,Kaitong Cai,Qinglin Zeng,Ningyuan Liu,Stephen Fan,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: 提出CE-Graph框架，将LLM工作流优化重新定义为分布问题，通过最小化期望失败质量来系统性地学习并重塑失败分布的结构。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工作流优化方法存在信息崩溃问题，将丰富的多步骤执行轨迹简化为简单的成功/失败信号，无法建模工作流的失败分布。

Method: 引入失败签名空间(FSS)概念，通过反例池近似失败分布，识别最密集区域作为重复失败模式，使用提出-验证机制进行有针对性的图编辑来贪婪减少失败质量。

Result: 在数学、代码和问答基准测试中，CE-Graph以显著更低的成本实现了比强基线更高的鲁棒性。

Conclusion: 系统的可靠性不是来自避免失败，而是来自系统性地学习和重塑其失败分布的几何结构。

Abstract: Optimizing LLM-based workflows is typically formulated as a global search,
where candidate workflows are evaluated based on a scalar metric. This
paradigm, however, suffers from a critical flaw: information collapse. By
reducing rich, multi-step execution traces to simple success/failure signals,
existing methods are rendered blind to the underlying structure of failures,
fundamentally preventing them from modeling the workflow's failure
distribution. We reconceptualize this challenge as a distributional problem. We
propose a new paradigm where the optimization goal is not to maximize a scalar
score, but to directly minimize a workflow's Expected Failure Mass, i.e., the
integral of its failure probability density function defined over a
high-dimensional Failure Signature Space (FSS). This distributional lens allows
us to move from inefficient, zero-order optimization to a principled,
gradient-like descent on the failure landscape itself. We introduce CE-Graph, a
framework that operationalizes this paradigm through a novel, failure-driven
refinement process. CE-Graph approximates the failure distribution from a pool
of counterexamples, identifies its densest regions as recurring failure modes,
and applies targeted, operator-constrained graph edits via a Propose-and-Verify
mechanism to greedily reduce the failure mass. On math, code, and QA
benchmarks, our CE-Graph achieves higher robustness at a significantly lower
cost than strong baselines. This suggests that a system's reliability emerges
not from avoiding failures, but from systematically learning and reshaping the
geometric structure of its failure distributions.

</details>


### [36] [SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning](https://arxiv.org/abs/2510.10047)
*Ruohao Li,Hongjun Liu,Leyi Zhao,Zisu Li,Jiawei Li,Jiajun Jiang,Linning Xu,Chen Zhao,Mingming Fan,Chen Liang*

Main category: cs.AI

TL;DR: SwarmSys是一个受群体智能启发的分布式多智能体推理框架，通过探索者、工作者和验证者三个角色的迭代交互实现闭环推理，无需全局监督即可实现动态任务分配和自组织收敛。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体框架依赖固定角色或集中控制，限制了长时程推理的可扩展性和适应性，需要更灵活、分布式的协调机制。

Method: 集成自适应智能体和事件档案、基于嵌入的概率匹配以及信息素启发的强化机制，支持三个专业角色的探索、利用和验证循环交互。

Result: 在符号推理、研究综合和科学编程任务中，SwarmSys始终优于基线方法，提高了准确性和推理稳定性。

Conclusion: 群体启发的协调机制是构建可扩展、鲁棒和自适应多智能体推理的有前景范式，协调扩展可能与模型扩展同样重要。

Abstract: Large language model (LLM) agents have shown remarkable reasoning abilities.
However, existing multi-agent frameworks often rely on fixed roles or
centralized control, limiting scalability and adaptability in long-horizon
reasoning. We introduce SwarmSys, a closed-loop framework for distributed
multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys
emerges through iterative interactions among three specialized roles,
Explorers, Workers, and Validators, that continuously cycle through
exploration, exploitation, and validation. To enable scalable and adaptive
collaboration, we integrate adaptive agent and event profiles, embedding-based
probabilistic matching, and a pheromone-inspired reinforcement mechanism,
supporting dynamic task allocation and self-organizing convergence without
global supervision. Across symbolic reasoning, research synthesis, and
scientific programming tasks, SwarmSys consistently outperforms baselines,
improving both accuracy and reasoning stability. These findings highlight
swarm-inspired coordination as a promising paradigm for scalable, robust, and
adaptive multi-agent reasoning, suggesting that coordination scaling may rival
model scaling in advancing LLM intelligence.

</details>


### [37] [Agentic Troubleshooting Guide Automation for Incident Management](https://arxiv.org/abs/2510.10074)
*Jiayi Mao,Liqun Li,Yanjie Gao,Zegang Peng,Shilin He,Chaoyun Zhang,Si Qin,Samia Khalid,Qingwei Lin,Saravan Rajmohan,Sitaram Lanka,Dongmei Zhang*

Main category: cs.AI

TL;DR: StepFly是一个用于自动化故障排除指南(TSG)执行的端到端智能代理框架，通过三阶段工作流解决TSG质量问题、复杂控制流解释、数据密集型查询和执行并行化等挑战，显著提高了故障管理效率。


<details>
  <summary>Details</summary>
Motivation: 大规模IT系统中的故障管理依赖故障排除指南(TSG)，但手动执行缓慢且容易出错。现有基于LLM的解决方案缺乏对TSG质量问题、复杂控制流解释、数据密集型查询和执行并行化等关键挑战的专业支持。

Method: StepFly采用三阶段工作流：1) TSG Mentor工具帮助SRE改进TSG质量；2) 离线预处理使用LLM从非结构化TSG中提取结构化执行DAG并创建专用查询准备插件(QPPs)；3) 在线执行使用DAG引导的调度器-执行器框架和内存系统，保证正确工作流并支持并行执行独立步骤。

Result: 在真实世界TSG和故障上的评估显示，StepFly在GPT-4.1上达到约94%的成功率，优于基线方法，同时减少了时间和token消耗。对于可并行化的TSG，实现了32.9%到70.4%的显著执行时间减少。

Conclusion: StepFly通过其端到端的智能代理框架有效解决了TSG自动化中的关键挑战，显著提高了故障管理的成功率和效率，特别是在可并行化场景下表现突出。

Abstract: Effective incident management in large-scale IT systems relies on
troubleshooting guides (TSGs), but their manual execution is slow and
error-prone. While recent advances in LLMs offer promise for automating
incident management tasks, existing LLM-based solutions lack specialized
support for several key challenges, including managing TSG quality issues,
interpreting complex control flow, handling data-intensive queries, and
exploiting execution parallelism. We first conducted an empirical study on 92
real-world TSGs, and, guided by our findings, we present StepFly, a novel
end-to-end agentic framework for troubleshooting guide automation. Our approach
features a three-stage workflow: the first stage provides a comprehensive guide
together with a tool, TSG Mentor, to assist SREs in improving TSG quality; the
second stage performs offline preprocessing using LLMs to extract structured
execution DAGs from unstructured TSGs and to create dedicated Query Preparation
Plugins (QPPs); and the third stage executes online using a DAG-guided
scheduler-executor framework with a memory system to guarantee correct workflow
and support parallel execution of independent steps. Our empirical evaluation
on a collection of real-world TSGs and incidents demonstrates that StepFly
achieves a ~94% success rate on GPT-4.1, outperforming baselines with less time
and token consumption. Furthermore, it achieves a remarkable execution time
reduction of 32.9% to 70.4% for parallelizable TSGs.

</details>


### [38] [Don't Just Fine-tune the Agent, Tune the Environment](https://arxiv.org/abs/2510.10197)
*Siyuan Lu,Zechuan Wang,Hongxuan Zhang,Qintong Wu,Leilei Gan,Chenyi Zhuang,Jinjie Gu,Tao Lin*

Main category: cs.AI

TL;DR: 提出了一种名为Environment Tuning的新训练范式，通过结构化课程、环境增强和细粒度进度奖励，使智能体能够直接从问题实例中学习复杂行为，无需依赖预先收集的专家轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在复杂多轮工具使用任务中面临的高质量训练数据稀缺问题，以及SFT方法导致的过拟合和标准RL方法面临的冷启动问题和训练不稳定性。

Method: Environment Tuning训练范式，包含结构化课程编排、提供纠正反馈的可操作环境增强、确保稳定高效探索的细粒度进度奖励。

Result: 仅使用BFCL基准的400个问题实例，不仅实现了与强基线相当的分布内性能，还表现出优越的分布外泛化能力，克服了SFT方法的性能崩溃问题。

Conclusion: 从基于静态轨迹的监督微调转向动态、基于环境的探索，为训练更鲁棒和数据高效的智能体铺平了道路。

Abstract: Large Language Model (LLM) agents show great promise for complex, multi-turn
tool-use tasks, but their development is often hampered by the extreme scarcity
of high-quality training data. Supervised fine-tuning (SFT) on synthetic data
leads to overfitting, whereas standard reinforcement learning (RL) struggles
with a critical cold-start problem and training instability. To address these
challenges, we introduce $\textbf{Environment Tuning}$, a novel training
paradigm that enables agents to learn complex behaviors directly from problem
instances without relying on pre-collected expert trajectories.
$\textbf{Environment Tuning}$ orchestrates this learning process through a
structured curriculum, actionable environment augmentation that provides
corrective feedback, and fine-grained progress rewards to ensure stable and
efficient exploration. Using only 400 problem instances from Berkeley
Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves
competitive in-distribution performance against strong baselines but also
demonstrates superior out-of-distribution generalization, overcoming the
performance collapse common to SFT-based approaches. Our work presents a
paradigm shift from supervised fine-tuning on static trajectories to dynamic,
environment-based exploration, paving the way for training more robust and
data-efficient agents.

</details>


### [39] [Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning](https://arxiv.org/abs/2510.10207)
*Yujian Zhang,Keyu Chen,Zhifeng Shen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 提出自适应双推理器(ADR)，通过快速思维和慢速思维两种推理模式的动态切换，在保持性能的同时显著降低计算成本和推理延迟


<details>
  <summary>Details</summary>
Motivation: 解决长推理模型在推理过程中因过度思考导致的计算成本增加和推理延迟问题

Method: 两阶段训练：1) 使用监督微调构建混合推理数据集；2) 强化学习阶段引入熵引导混合策略优化(EHPO)，在高熵单元进行分支并采用难度感知惩罚

Result: 在数学推理基准测试中，性能提升达6.1%，同时推理输出长度减少49.5%至59.3%

Conclusion: ADR在推理性能和效率之间实现了有效平衡，是当前最优方法之一

Abstract: Although Long Reasoning Models (LRMs) have achieved superior performance on
various reasoning scenarios, they often suffer from increased computational
costs and inference latency caused by overthinking. To address these
limitations, we propose Adaptive Dual Reasoner, which supports two reasoning
modes: fast thinking and slow thinking. ADR dynamically alternates between
these modes based on the contextual complexity during reasoning. ADR is trained
in two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to
equip the model with the ability to integrate both fast and slow reasoning
modes, in which we construct a hybrid reasoning dataset through a dedicated
pipeline to provide large-scale supervision. (2) A reinforcement learning stage
for optimizing reasoning effort, where we introduce Entropy-guided Hybrid
Policy Optimization EHPO, an RL training framework employing an entropy-guided
dynamic rollout strategy for branching at high-entropy units and a
difficulty-aware penalty to balance fast and slow reasoning. Across challenging
mathematical reasoning benchmarks, ADR achieves an effective balance between
reasoning performance and efficiency among state-of-the-art approaches.
Specifically, ADR yields a performance gain of up to 6.1%, while reducing the
reasoning output length by 49.5% to 59.3%.

</details>


### [40] [The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities](https://arxiv.org/abs/2510.10238)
*Zixuan Qin,Kunlin Lyu,Qingchen Yu,Yifan Sun,Zhaoxin Fan*

Main category: cs.AI

TL;DR: 提出了基于扰动的关键神经元识别方法，发现在大语言模型中存在超稀疏的关键神经元集合，破坏这些神经元会导致模型性能急剧下降，且这些神经元集中分布在模型外层特别是MLP组件中。


<details>
  <summary>Details</summary>
Motivation: 受到人类大脑中少数关键神经元对认知功能重要性的启发，研究大语言模型中是否也存在类似的关键神经元子集。

Method: 使用基于扰动的因果识别方法来系统定位大语言模型中的关键神经元。

Result: 发现：(1)LLMs包含超稀疏关键神经元，破坏这些神经元会导致720亿参数模型崩溃，困惑度增加20个数量级；(2)关键神经元集中分布在外层，特别是MLP down_proj组件；(3)性能下降呈现急剧的相变而非渐进式。

Conclusion: 这些发现为开发更鲁棒的模型架构和提高安全关键应用中的部署安全性提供了指导。

Abstract: Large Language Models (LLMs) have become foundational tools in natural
language processing, powering a wide range of applications and research. Many
studies have shown that LLMs share significant similarities with the human
brain. Recent neuroscience research has found that a small subset of biological
neurons in the human brain are crucial for core cognitive functions, which
raises a fundamental question: do LLMs also contain a small subset of critical
neurons? In this paper, we investigate this question by proposing a
Perturbation-based Causal Identification of Critical Neurons method to
systematically locate such critical neurons in LLMs. Our findings reveal three
key insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting
these critical neurons can cause a 72B-parameter model with over 1.1 billion
neurons to completely collapse, with perplexity increasing by up to 20 orders
of magnitude; (2) These critical neurons are not uniformly distributed, but
tend to concentrate in the outer layers, particularly within the MLP down\_proj
components; (3) Performance degradation exhibits sharp phase transitions,
rather than a gradual decline, when these critical neurons are disrupted.
Through comprehensive experiments across diverse model architectures and
scales, we provide deeper analysis of these phenomena and their implications
for LLM robustness and interpretability. These findings can offer guidance for
developing more robust model architectures and improving deployment security in
safety-critical applications.

</details>


### [41] [Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control](https://arxiv.org/abs/2510.10285)
*Haolang Lu,Bolun Chu,WeiYe Fu,Guoshun Nan,Junning Liu,Minghui Pan,Qiankun Li,Yi Yu,Hua Wang,Kun Wang*

Main category: cs.AI

TL;DR: 提出了一种轻量级插件方法，通过识别感知导向和推理导向的注意力头并调节其贡献，有效减少多模态大推理模型的幻觉问题，在多个基准测试上平均提升5%性能，最高提升15%。


<details>
  <summary>Details</summary>
Motivation: 多模态大推理模型存在幻觉问题，表现为错误推理链和视觉内容误解。研究发现注意力头存在阶段性分工：浅层头主要负责感知，深层头转向符号推理，这揭示了幻觉的两个主要原因：感知偏差和推理漂移。

Method: 提出功能性头识别和类条件重缩放的两步插件方法，定位感知导向和推理导向的注意力头，在不重新训练的情况下调节它们的贡献。

Result: 在三个真实世界MLRM模型、六个基准测试和四个基线方法上的评估显示，该方法平均提升5%性能，最高提升15%，仅增加<1%的计算开销和9%的基线延迟。

Conclusion: 该方法完全模型无关，显著增强了现有多模态大推理模型的可靠性和可解释性，使其能够安全部署在高风险应用中。

Abstract: Multimodal large reasoning models (MLRMs) are rapidly advancing
vision-language reasoning and are emerging as a foundation for cross-modal
intelligence. Hallucination remains a persistent failure mode, manifesting
itself as erroneous reasoning chains and misinterpretation of visual content.
In this study, we observe that attention heads exhibit a staged division:
shallow heads predominantly serve perception, while deeper heads shift toward
symbolic reasoning, revealing two major causes of hallucination, namely
perceptual bias and reasoning drift. To address these issues, we propose a
lightweight and interpretable two-step plugin, Functional Head Identification
and Class-conditioned Rescaling, which locates perception- and
reasoning-oriented heads and regulates their contributions without retraining.
Evaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six
benchmarks across three domains, and four baselines show that our plugin
achieves an average improvement of 5% and up to 15%, with only <1% additional
computation and 9% of baseline latency. Our approach is completely
model-agnostic and significantly enhances both the reliability and
interpretability of the off-the-shelf MLRMs, thereby enabling their safe
deployment in high-stakes applications. Our code is available at
https://anonymous.4open.science/r/Functional-Attention-Control.

</details>


### [42] [Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction](https://arxiv.org/abs/2510.10454)
*Sihang Zeng,Yujuan Fu,Sitong Zhou,Zixuan Yu,Lucas Jing Liu,Jun Wen,Matthew Thompson,Ruth Etzioni,Meliha Yetisgen*

Main category: cs.AI

TL;DR: 提出了Traj-CoA多代理系统，通过链式代理处理电子健康记录数据，解决长序列和噪声问题，在肺癌风险预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在建模患者轨迹方面具有通用性，但面临电子健康记录数据长序列和噪声的挑战，需要改进时序推理能力。

Method: 使用多代理系统，包括多个工作代理按顺序处理数据块，将关键事件提取到共享长期记忆模块EHRMem中，最后由管理代理综合信息进行预测。

Result: 在基于五年电子健康记录数据的零样本一年肺癌风险预测任务中，Traj-CoA优于四类基线方法。

Conclusion: Traj-CoA展现出与临床对齐的时序推理能力，是建模复杂患者轨迹的稳健且通用方法。

Abstract: Large language models (LLMs) offer a generalizable approach for modeling
patient trajectories, but suffer from the long and noisy nature of electronic
health records (EHR) data in temporal reasoning. To address these challenges,
we introduce Traj-CoA, a multi-agent system involving chain-of-agents for
patient trajectory modeling. Traj-CoA employs a chain of worker agents to
process EHR data in manageable chunks sequentially, distilling critical events
into a shared long-term memory module, EHRMem, to reduce noise and preserve a
comprehensive timeline. A final manager agent synthesizes the worker agents'
summary and the extracted timeline in EHRMem to make predictions. In a
zero-shot one-year lung cancer risk prediction task based on five-year EHR
data, Traj-CoA outperforms baselines of four categories. Analysis reveals that
Traj-CoA exhibits clinically aligned temporal reasoning, establishing it as a
promisingly robust and generalizable approach for modeling complex patient
trajectories.

</details>


### [43] [MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision](https://arxiv.org/abs/2510.10461)
*Hongjie Zheng,Zesheng Shi,Ping Yi*

Main category: cs.AI

TL;DR: 提出MedCoAct框架，通过医生和药剂师多智能体协作解决医疗AI在整合诊断和治疗工作流中的局限性，在诊断和用药推荐准确率上分别比单智能体提升7.04%和7.08%。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI系统在孤立任务中表现出色，但在整合临床工作流中缺乏跨验证和知识整合，限制了在真实医疗场景中的有效性。

Method: 提出MedCoAct置信感知多智能体框架，集成专业医生和药剂师智能体模拟临床协作，并建立DrugCareQA基准评估医疗AI在整合诊断和治疗工作流中的能力。

Result: MedCoAct达到67.58%的诊断准确率和67.58%的用药推荐准确率，分别比单智能体框架提升7.04%和7.08%，在远程医疗和常规临床场景中表现优异。

Conclusion: 协作方法在不同医疗领域泛化良好，特别适用于远程医疗咨询和常规临床场景，同时提供可解释的决策路径。

Abstract: Autonomous agents utilizing Large Language Models (LLMs) have demonstrated
remarkable capabilities in isolated medical tasks like diagnosis and image
analysis, but struggle with integrated clinical workflows that connect
diagnostic reasoning and medication decisions. We identify a core limitation:
existing medical AI systems process tasks in isolation without the
cross-validation and knowledge integration found in clinical teams, reducing
their effectiveness in real-world healthcare scenarios. To transform the
isolation paradigm into a collaborative approach, we propose MedCoAct, a
confidence-aware multi-agent framework that simulates clinical collaboration by
integrating specialized doctor and pharmacist agents, and present a benchmark,
DrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and
treatment workflows. Our results demonstrate that MedCoAct achieves 67.58\%
diagnostic accuracy and 67.58\% medication recommendation accuracy,
outperforming single agent framework by 7.04\% and 7.08\% respectively. This
collaborative approach generalizes well across diverse medical domains, proving
especially effective for telemedicine consultations and routine clinical
scenarios, while providing interpretable decision-making pathways.

</details>


### [44] [ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding](https://arxiv.org/abs/2510.10549)
*Xinbang Dai,Huikang Hu,Yongrui Chen,Jiaqi Li,Rihui Jin,Yuyang Zhang,Xiaoguang Li,Lifeng Shang,Guilin Qi*

Main category: cs.AI

TL;DR: 提出了ELAIPBench基准测试，用于评估LLM对AI研究论文的深度理解能力，包含403个多选题，实验显示最佳LLM准确率仅39.95%，远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估LLM对学术论文的深度理解能力方面存在不足，问题设计表面化且评估指标不可靠。

Method: 通过激励驱动的对抗性标注过程，构建包含403个多选题的ELAIPBench基准，涵盖三个难度级别，强调非平凡推理而非浅层检索。

Result: 最佳LLM准确率仅39.95%，前沿LLM即使配备思维模式或RAG系统也无法改善结果，甚至因过度思考或噪声检索而降低准确率。

Conclusion: 当前LLM能力与真正理解学术论文之间存在显著差距。

Abstract: While large language models (LLMs) excel at many domain-specific tasks, their
ability to deeply comprehend and reason about full-length academic papers
remains underexplored. Existing benchmarks often fall short of capturing such
depth, either due to surface-level question design or unreliable evaluation
metrics. To address this gap, we introduce ELAIPBench, a benchmark curated by
domain experts to evaluate LLMs' comprehension of artificial intelligence (AI)
research papers. Developed through an incentive-driven, adversarial annotation
process, ELAIPBench features 403 multiple-choice questions from 137 papers. It
spans three difficulty levels and emphasizes non-trivial reasoning rather than
shallow retrieval. Our experiments show that the best-performing LLM achieves
an accuracy of only 39.95%, far below human performance. Moreover, we observe
that frontier LLMs equipped with a thinking mode or a retrieval-augmented
generation (RAG) system fail to improve final results-even harming accuracy due
to overthinking or noisy retrieval. These findings underscore the significant
gap between current LLM capabilities and genuine comprehension of academic
papers.

</details>


### [45] [A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning](https://arxiv.org/abs/2510.10592)
*Hong Su*

Main category: cs.AI

TL;DR: 提出了一种统一的直觉-方法分层模型与范围扩展框架，通过直觉思维提供快速反应，方法思维将问题解耦为可转移推理单元，并引入垂直、水平、时间和空间扩展来增强LLM对未见问题的解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究已引入基于方法的推理和范围扩展来提升LLM性能，但缺乏系统整合。本文旨在将这些思想统一成一个更系统的框架，以更有效地解决间接（未见）问题。

Method: 构建直觉-方法分层模型：直觉层提供快速答案，方法层将问题解耦为可转移推理单元。引入四种范围扩展（垂直、水平、时间、空间）并组织成系统知识树网络。提出方法扩展熵来量化评估系统能力。

Result: 通过逻辑连接现有方法并引入新的扩展维度，建立了一个更鲁棒和可扩展的LLM推理范式，能够系统性地处理未见问题。

Conclusion: 该工作通过统一现有方法、引入新的扩展维度和熵评估框架，推进了LLM在现实问题解决中更鲁棒和可扩展的推理范式发展。

Abstract: Existing studies have introduced method-based reasoning and scope extension
as approaches to enhance Large Language Model (LLM) performance beyond direct
matrix mappings. Building on these foundations, this paper summarizes and
integrates these ideas into a unified Intuition-Method Layered Model with Scope
Extension, designed to address indirected (unseen) issues more systematically.
In this framework, intuition-based thinking provides rapid first-reaction
answers, while method-based thinking decouples questions and solutions into
transferable reasoning units. Scope extension is then applied to broaden
applicability, including vertical (cause analysis), horizontal (parallel and
generalized issues), and for the first time, temporal and spatial extensions,
which expand reasoning across time and contextual dimensions. These extensions
are organized into systematic knowledge trees that interconnect into a
knowledge network, thereby increasing adaptability. To quantitatively evaluate
this process, we propose the entropy of method extension, which measures the
independence and diversity of extensions as an indicator of the system's
capacity to solve unseen questions. By logically connecting existing approaches
with new extensions and introducing an entropy-based evaluation framework, this
work advances toward a more robust and extensible reasoning paradigm for LLMs
in real-world problem-solving.

</details>


### [46] [Simpliflow: A Lightweight Open-Source Framework for Rapid Creation and Deployment of Generative Agentic AI Workflows](https://arxiv.org/abs/2510.10675)
*Deven Panchal*

Main category: cs.AI

TL;DR: simpliflow是一个轻量级开源Python框架，通过声明式JSON配置快速开发确定性智能体工作流，解决了现有框架复杂性和学习曲线陡峭的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成式智能体AI框架存在显著复杂性、陡峭学习曲线和大量样板代码，阻碍了快速原型设计和部署。

Method: 采用模块化架构，将智能体管理、工作流执行和后处理解耦，通过声明式JSON配置实现线性确定性工作流的快速编排，集成LiteLLM支持100+大语言模型。

Result: 开发了simpliflow框架，通过多样用例（从软件开发模拟到实时系统交互）展示了其实用性，与LangChain和AutoGen的对比分析突显了其在确定性工作流环境中的优势。

Conclusion: simpliflow在确定性工作流环境中作为优化简单性、控制性和速度的工具具有独特地位，促进了智能体系统的快速开发和部署。

Abstract: Generative Agentic AI systems are emerging as a powerful paradigm for
automating complex, multi-step tasks. However, many existing frameworks for
building these systems introduce significant complexity, a steep learning
curve, and substantial boilerplate code, hindering rapid prototyping and
deployment. This paper introduces simpliflow, a lightweight, open-source Python
framework designed to address these challenges. simpliflow enables the rapid
development and orchestration of linear, deterministic agentic workflows
through a declarative, JSON-based configuration. Its modular architecture
decouples agent management, workflow execution, and post-processing, promoting
ease of use and extensibility. By integrating with LiteLLM, it supports over
100 Large Language Models (LLMs) out-of-the-box. We present the architecture,
operational flow, and core features of simpliflow, demonstrating its utility
through diverse use cases ranging from software development simulation to
real-time system interaction. A comparative analysis with prominent frameworks
like LangChain and AutoGen highlights simpliflow's unique position as a tool
optimized for simplicity, control, and speed in deterministic workflow
environments.

</details>


### [47] [PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature](https://arxiv.org/abs/2510.10909)
*Daoyu Wang,Mingyue Cheng,Qi Liu,Shuo Yu,Zirui Liu,Ze Guo*

Main category: cs.AI

TL;DR: 提出了PaperArena基准测试平台，用于评估LLM代理在跨论文推理和多工具协调方面的能力，现有先进代理仅达到38.78%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要局限于单篇论文内的无工具任务，缺乏真实研究场景中跨论文推理和多工具协调的基准测试。

Method: 构建PaperArena基准测试平台，提供多模态解析、上下文检索和程序化计算等工具，评估代理在解决需要整合多篇论文信息的真实研究问题时的表现。

Result: 最先进的LLM代理系统仅达到38.78%的平均准确率，在困难子集上准确率降至18.47%，且所有测试代理都表现出工具使用效率低下的问题。

Conclusion: PaperArena揭示了当前代理在科学发现任务中的局限性，为开发更强大的科学发现代理提供了评估基准。

Abstract: Understanding and reasoning on the web-scale scientific literature is a
crucial touchstone for large language model (LLM) based agents designed to
support complex knowledge-intensive tasks. However, existing works are mainly
restricted to tool-free tasks within isolated papers, largely due to the lack
of a benchmark for cross-paper reasoning and multi-tool orchestration in real
research scenarios. In this work, we propose PaperArena, an evaluation
benchmark for agents to address real-world research questions that typically
require integrating information across multiple papers with the assistance of
external tools. Given a research question, agents should integrate diverse
formats across multiple papers through reasoning and interacting with
appropriate tools, thereby producing a well-grounded answer. To support
standardized evaluation, we provide a modular and extensible platform for agent
execution, offering tools such as multimodal parsing, context retrieval, and
programmatic computation. Experimental results reveal that even the most
advanced LLM powering a well-established agent system achieves merely 38.78%
average accuracy. On the hard subset, accuracy drops to only 18.47%,
highlighting great potential for improvement. We also present several empirical
findings, including that all agents tested exhibit inefficient tool usage,
often invoking more tools than necessary to solve a task. We invite the
community to adopt PaperArena to develop and evaluate more capable agents for
scientific discovery. Our code and data are available
https://github.com/Melmaphother/PaperArena.

</details>


### [48] [PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents](https://arxiv.org/abs/2510.10931)
*SHengjie Ma,Chenlong Deng,Jiaxin Mao,Jiadeng Huang,Teng Wang,Junjie Wu,Changwang Zhang,Jun wang*

Main category: cs.AI

TL;DR: 提出了Proof-of-Use (PoU)框架来解决检索增强生成(RAG)代理中的工具调用黑客问题，通过证据基础的强化学习确保检索证据与最终答案之间的可验证因果联系。


<details>
  <summary>Details</summary>
Motivation: 发现RAG代理存在工具调用黑客问题，即代理通过发出表面正确的工具调用来夸大奖励信号，而不真正利用检索到的证据，导致模式崩溃和虚假基础。

Method: 提出Proof-of-Use (PoU)框架，通过统一的逐步契约结合语法引用验证、基于扰动的敏感性奖励和答案-证据对齐目标，确保工具使用的可解释性和功能性基础。

Result: 在七个QA基准测试中，PoU在事实准确性、证据忠实度和工具路由平衡方面持续优于DeepResearch基线，涵盖领域内、领域外和工具分布外设置。

Conclusion: 研究强调了在强化学习训练的代理中不仅要在任务结果上建立基础，还要在检索信息的因果使用上建立基础，为可信的检索增强推理提供了原则性路径。

Abstract: Retrieval-augmented generation (RAG) agents, such as recent
DeepResearch-style systems, extend large language models (LLMs) with autonomous
information-seeking capabilities through external tools. While reinforcement
learning (RL) has enabled impressive multi-step reasoning, we identify a
previously overlooked failure mode, Tool-Call Hacking, where agents inflate
reward signals by issuing superficially correct tool calls without genuinely
leveraging the retrieved evidence. This results in (i) mode collapse into
repetitive reliance on a single source and (ii) spurious grounding, where
answers are only weakly supported by cited content.
  To address this, we propose Proof-of-Use (PoU), an evidence-grounded RL
framework that enforces verifiable causal links between retrieved evidence,
reasoning traces, and final answers. PoU operationalizes this through a unified
step-wise contract combining syntactic citation validation, perturbation-based
sensitivity rewards, and answer-evidence alignment objectives, ensuring that
tool usage remains both interpretable and functionally grounded.
  Across seven QA benchmarks spanning in-domain, out-of-domain, and
out-of-tool-distribution settings, PoU consistently outperforms strong
DeepResearch baselines in factual accuracy, evidence faithfulness, and
tool-routing balance. These findings highlight the necessity of grounding
RL-trained agents not merely in task outcomes but in the causal use of
retrieved information, offering a principled path toward trustworthy
retrieval-augmented reasoning.

</details>


### [49] [$How^{2}$: How to learn from procedural How-to questions](https://arxiv.org/abs/2510.11144)
*Gautier Dagan,Frank Keller,Alex Lascarides*

Main category: cs.AI

TL;DR: 提出了How2记忆代理框架，让AI代理能够询问如何做的问题、存储答案并在交互环境中重复使用以实现终身学习。在Minecraft制作环境中验证，发现抽象程度高的答案对代理学习最有益。


<details>
  <summary>Details</summary>
Motivation: 代理在规划问题时需要填补知识空白，但如何做问题的开放性使得AI代理难以有效提问，AI专家也难以高效回答。需要一种支持高效规划的问答机制。

Method: 引入How2记忆代理框架，在Plancraft（Minecraft制作环境）中评估，使用不同抽象层次的教师模型回答问题，从可执行动作序列到高级子目标描述。

Result: 终身学习代理从抽象且与当前状态解耦的答案中获益最大，这种框架能让基于LLM的代理在交互环境中通过提问提升规划能力。

Conclusion: How2框架为LLM代理提供了一种通过交互环境中的提问来持续改进规划能力的方法，抽象答案对学习效果最有利。

Abstract: An agent facing a planning problem can use answers to how-to questions to
reduce uncertainty and fill knowledge gaps, helping it solve both current and
future tasks. However, their open ended nature, where valid answers to "How do
I X?" range from executable actions to high-level descriptions of X's
sub-goals, makes them challenging for AI agents to ask, and for AI experts to
answer, in ways that support efficient planning. We introduce $How^{2}$, a
memory agent framework that enables agents to ask how-to questions, store the
answers, and reuse them for lifelong learning in interactive environments. We
evaluate our approach in Plancraft, a Minecraft crafting environment, where
agents must complete an assembly task by manipulating inventory items. Using
teacher models that answer at varying levels of abstraction, from executable
action sequences to high-level subgoal descriptions, we show that lifelong
learning agents benefit most from answers that are abstracted and decoupled
from the current state. $How^{2}$ offers a way for LLM-based agents to improve
their planning capabilities over time by asking questions in interactive
environments.

</details>


### [50] [Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity Educational Dynamics](https://arxiv.org/abs/2510.11290)
*Sheng Jin,Haoming Wang,Zhiqi Gao,Yongbo Yang,Bao Chunjia,Chengliang Wang*

Main category: cs.AI

TL;DR: 提出了AI-Agent School系统，通过自进化机制模拟复杂教育动态，采用Zero-Exp策略和"经验-反思-优化"循环，在模拟学校场景中实现代理自主进化。


<details>
  <summary>Details</summary>
Motivation: 解决教学流程建模的碎片化问题，以及代理在模拟多样化教育参与者时的性能限制，旨在更准确地模拟真实学校中的师生互动和学习过程。

Method: 构建Zero-Exp策略，采用持续的经验-反思-优化循环，基于包含经验和知识库的双重记忆基础，整合短期和长期记忆组件。

Result: 实验证实AAS能有效模拟复杂教育动态，促进高级代理认知能力发展，生成高保真行为交互数据。

Conclusion: 该系统为从"经验时代"迈向"模拟时代"提供了基础支撑，通过生成高质量模拟数据推动教育系统研究。

Abstract: Large language models (LLMs) based Agents are increasingly pivotal in
simulating and understanding complex human systems and interactions. We propose
the AI-Agent School (AAS) system, built around a self-evolving mechanism that
leverages agents for simulating complex educational dynamics. Addressing the
fragmented issues in teaching process modeling and the limitations of agents
performance in simulating diverse educational participants, AAS constructs the
Zero-Exp strategy, employs a continuous "experience-reflection-optimization"
cycle, grounded in a dual memory base comprising experience and knowledge bases
and incorporating short-term and long-term memory components. Through this
mechanism, agents autonomously evolve via situated interactions within diverse
simulated school scenarios. This evolution enables agents to more accurately
model the nuanced, multi-faceted teacher-student engagements and underlying
learning processes found in physical schools. Experiment confirms that AAS can
effectively simulate intricate educational dynamics and is effective in
fostering advanced agent cognitive abilities, providing a foundational stepping
stone from the "Era of Experience" to the "Era of Simulation" by generating
high-fidelity behavioral and interaction data.

</details>


### [51] [From <Answer> to <Think>: Multidimensional Supervision of Reasoning Process for LLM Optimization](https://arxiv.org/abs/2510.11457)
*Beining Wang,Weihang Su,Hongtao Tian,Tao Yang,Yujia Zhou,Ting Yao,Qingyao Ai,Yiqun Liu*

Main category: cs.AI

TL;DR: 提出维度级奖励模型(DRM)，通过置信度、相关性和连贯性三个维度评估推理过程质量，解决传统结果监督强化学习奖励稀疏和过程级奖励模型缺乏泛化性的问题。


<details>
  <summary>Details</summary>
Motivation: 改善大语言模型的多步推理能力是关键挑战。传统结果监督强化学习只奖励正确答案，容易传播错误推理且奖励稀疏；过程级奖励模型虽然提供更密集的反馈，但缺乏泛化性和可解释性。

Method: 提出维度级奖励模型(DRM)，从三个互补且可解释的维度评估推理过程：置信度（不确定性校准）、相关性（语义对齐）和连贯性（逻辑一致性）。这些维度无需真实答案即可进行可解释评估。

Result: 实验结果显示DRM提供有效的监督信号，指导LLMs优化并增强推理能力。DRM监督训练在数学、问答、代码执行和谜题等开放领域任务上获得一致提升，包括分布内和分布外任务。

Conclusion: 多维度的推理过程监督可以提升大语言模型在训练分布之外的泛化推理能力。

Abstract: Improving the multi-step reasoning ability of Large Language Models (LLMs) is
a critical yet challenging task. The dominant paradigm, outcome-supervised
reinforcement learning (RLVR), rewards only correct final answers, often
propagating flawed reasoning and suffering from sparse reward signals. While
process-level reward models (PRMs) provide denser, step-by-step feedback, they
lack generalizability and interpretability, requiring task-specific
segmentation of the reasoning process. To this end, we propose the
Dimension-level Reward Model (DRM), a new supervision framework that bridges
the gap between these two approaches. DRM evaluates the quality of a reasoning
process along three fundamental, complementary, and interpretable dimensions:
Confidence for uncertainty calibration, Relevance for semantic alignment, and
Coherence for logical consistency. Together, these dimensions capture aspects
beyond final answer correctness and enable interpretable assessment without
requiring ground truth answers. Experimental results show that DRM provides
effective supervision signals, guides the optimization of LLMs and enhances
their reasoning ability. In particular, DRM-supervised training achieves
consistent gains on both in-distribution and out-of-distribution open-domain
tasks, including mathematics, question answering, code execution, and puzzles.
Our findings demonstrate that multidimensional supervision of the reasoning
process can improve the generalized reasoning ability of LLMs beyond the
training distribution.

</details>


### [52] [Analyzing and Internalizing Complex Policy Documents for LLM Agents](https://arxiv.org/abs/2510.11588)
*Jiateng Liu,Zhenhailong Wang,Xiaojiang Huang,Yingjie Li,Xing Fan,Xiang Li,Chenlei Guo,Ruhi Sarikaya,Heng Ji*

Main category: cs.AI

TL;DR: 提出了CC-Gen基准生成器和CAP-CPT方法，用于解决LLM代理系统中政策文档内部化的挑战，特别是在处理复杂政策规范时的推理困难。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理系统的发展，政策文档迅速扩展导致计算开销增加，需要开发将政策文档嵌入模型先验的内部化方法，同时保持性能。

Method: 引入CC-Gen基准生成器，具有四个可控复杂度级别；提出CAP-CPT方法，通过解析政策文档提取关键规范，分类为事实性、行为性和条件性类别，指导数据合成并通过自回归预训练损失实现政策内部化。

Result: CAP-CPT在所有设置中都优于SFT基线，在Qwen-3-32B上实现了高达41%和22%的性能提升，在CC-Gen上实现了97.3%的提示长度减少，并在使用少量SFT数据时进一步增强了tau-Bench性能。

Conclusion: CAP-CPT方法有效缓解了数据和推理负担，显著提升了政策内部化能力，特别是在处理复杂政策规范时表现优异。

Abstract: Large Language Model (LLM)-based agentic systems rely on in-context policy
documents encoding diverse business rules. As requirements grow, these
documents expand rapidly, causing high computational overhead. This motivates
developing internalization methods that embed policy documents into model
priors while preserving performance. Prior prompt compression work targets
generic prompts, but agentic policy documents span multiple complexity levels
and require deeper reasoning, making internalization harder. We introduce
CC-Gen, an agentic benchmark generator with Controllable Complexity across four
levels, enabling systematic evaluation of agents' ability to handle complexity
and offering a unified framework for assessing policy internalization. Our
analysis shows that complex policy specifications governing workflows pose
major reasoning challenges. Supporting internalization with gold user agent
interaction trajectories containing chain-of-thought (CoT) annotations via
supervised fine-tuning (SFT) is data-intensive and degrades sharply as policy
complexity increases. To mitigate data and reasoning burdens, we propose
Category-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline
parses policy documents to extract key specifications, grouping them into
factual, behavioral, and conditional categories, and isolating complex
conditions that drive workflow complexity. This guides targeted data synthesis
and enables agents to internalize policy information through an autoregressive
pretraining loss. Experiments show CAP-CPT improves SFT baselines in all
settings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt
length reduction on CC-Gen and further enhancing tau-Bench with minimal SFT
data.

</details>


### [53] [ParaCook: On Time-Efficient Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.11608)
*Shiqi Zhang,Xinbei Ma,Yunqing Xu,Zouying Cao,Pengrui Lu,Haobo Yuan,Tiancheng Shen,Zhuosheng Zhang,Hai Zhao,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: ParaCook是一个评估多智能体时间效率协作规划的基准，基于简化版Overcooked游戏，专注于并行和异步操作的规划效率。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准主要关注任务完成度，但忽视了并行和异步操作中的时间效率问题，需要专门评估多智能体系统的时间效率规划能力。

Method: 基于Overcooked游戏设计简化环境，提供可扩展的评估框架，通过烹饪任务实例化多智能体系统的交互规划挑战，专注于战略并行规划的核心问题。

Result: 对最先进LLMs的评估显示，当前方法生成的规划次优，难以处理并行动作或协调，但在抽象任务中LLMs展现出专注于高层并行优化的潜力。

Conclusion: ParaCook为开发和评估时间效率感知的多智能体规划奠定了基础，提供了一个可调节复杂度的可扩展评估框架。

Abstract: Large Language Models (LLMs) exhibit strong reasoning abilities for planning
long-horizon, real-world tasks, yet existing agent benchmarks focus on task
completion while neglecting time efficiency in parallel and asynchronous
operations. To address this, we present ParaCook, a benchmark for
time-efficient collaborative planning. Inspired by the Overcooked game,
ParaCook provides an environment for various challenging interaction planning
of multi-agent systems that are instantiated as cooking tasks, with a
simplified action space to isolate the core challenge of strategic parallel
planning. Through a comprehensive evaluation of state-of-the-art LLMs, we find
that current approaches achieve suboptimal plans, which struggle with parallel
actions or coordination. Our analysis also reveals LLMs' potential on abstract
tasks where they can focus on high-level parallel optimization. ParaCook
provides a scalable evaluation framework with adjustable complexity,
establishing a foundation for developing and assessing time efficiency-aware
multi-agent planning. The code and data are available at
https://github.com/zsq259/ParaCook.

</details>


### [54] [SR-Scientist: Scientific Equation Discovery With Agentic AI](https://arxiv.org/abs/2510.11661)
*Shijie Xia,Yuhan Sun,Pengfei Liu*

Main category: cs.AI

TL;DR: SR-Scientist框架将LLM从简单的方程提议者提升为自主AI科学家，通过编写代码分析数据、实现方程、评估反馈并进行优化，在四个科学领域的数据集上比基线方法提升6%至35%的绝对性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法通常将LLM限制为遗传编程等搜索算法中的方程提议者角色，未能充分利用其科学知识和推理能力。

Method: 将代码解释器包装成数据分析和方程评估工具集，让代理在最小人工干预下使用这些工具进行长期优化，并开发端到端强化学习框架增强代理能力。

Result: 在四个科学领域的数据集上，SR-Scientist比基线方法绝对提升6%至35%；对噪声具有鲁棒性，发现的方程在域外数据上具有良好泛化能力和符号准确性。

Conclusion: SR-Scientist成功将LLM提升为自主AI科学家，显著提升了科学方程发现的性能，并展示了在噪声环境和泛化场景下的有效性。

Abstract: Recently, Large Language Models (LLMs) have been applied to scientific
equation discovery, leveraging their embedded scientific knowledge for
hypothesis generation. However, current methods typically confine LLMs to the
role of an equation proposer within search algorithms like genetic programming.
In this paper, we present SR-Scientist, a framework that elevates the LLM from
a simple equation proposer to an autonomous AI scientist that writes code to
analyze data, implements the equation as code, submits it for evaluation, and
optimizes the equation based on experimental feedback. Specifically, we wrap
the code interpreter into a set of tools for data analysis and equation
evaluation. The agent is instructed to optimize the equation by utilizing these
tools over a long horizon with minimal human-defined pipelines. Empirical
results show that SR-Scientist outperforms baseline methods by an absolute
margin of 6% to 35% on datasets covering four science disciplines.
Additionally, we demonstrate our method's robustness to noise, the
generalization of the discovered equations to out-of-domain data, and their
symbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning
framework to enhance the agent's capabilities.

</details>


### [55] [Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2510.11694)
*Arjun Sahney,Ram Gorthi,Cezary Łastowski,Javier Vega*

Main category: cs.AI

TL;DR: Operand Quant是一种单代理、IDE基础的自主机器学习工程架构，在MLE-Benchmark上创下最新SOTA结果，证明单代理系统在受控IDE环境中可超越多代理系统。


<details>
  <summary>Details</summary>
Motivation: 传统多代理编排框架存在复杂性，作者旨在探索单代理架构在机器学习工程全生命周期中的潜力。

Method: 采用单代理、IDE基础的线性非阻塞架构，整合机器学习工程的所有阶段：探索、建模、实验和部署。

Result: 在MLE-Benchmark（2025）上获得0.3956 ± 0.0565的总体奖牌率，在75个问题上创下所有评估系统中的最高性能记录。

Conclusion: 线性非阻塞的单代理在受控IDE环境中能够超越多代理和编排系统，展示了单代理架构的有效性。

Abstract: We present Operand Quant, a single-agent, IDE-based architecture for
autonomous machine learning engineering (MLE). Operand Quant departs from
conventional multi-agent orchestration frameworks by consolidating all MLE
lifecycle stages -- exploration, modeling, experimentation, and deployment --
within a single, context-aware agent. On the MLE-Benchmark (2025), Operand
Quant achieved a new state-of-the-art (SOTA) result, with an overall medal rate
of 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance
among all evaluated systems to date. The architecture demonstrates that a
linear, non-blocking agent, operating autonomously within a controlled IDE
environment, can outperform multi-agent and orchestrated systems under
identical constraints.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation](https://arxiv.org/abs/2510.09705)
*Sudip Khadka,L. S. Paudel*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的特征选择框架，将偏差缓解和特征选择整合到单一学习过程中，通过动态奖励信号平衡预测性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统静态特征排除策略无法有效防止偏差，因为隐藏的依赖关系会影响模型预测。需要一种动态方法来平衡泛化能力、准确性和公平性。

Method: 使用强化学习框架，构建多组件奖励函数，定义特征子集的动作空间，并与集成学习相结合，实现自适应特征选择。

Result: 该方法能够动态平衡预测性能和公平性考虑，避免传统预处理或后处理方法的局限性。

Conclusion: 该RL框架提供了一种灵活通用的特征选择方法，特别适用于预测因子相关且偏差可能意外重新出现的环境。

Abstract: Static feature exclusion strategies often fail to prevent bias when hidden
dependencies influence the model predictions. To address this issue, we explore
a reinforcement learning (RL) framework that integrates bias mitigation and
automated feature selection within a single learning process. Unlike
traditional heuristic-driven filter or wrapper approaches, our RL agent
adaptively selects features using a reward signal that explicitly integrates
predictive performance with fairness considerations. This dynamic formulation
allows the model to balance generalization, accuracy, and equity throughout the
training process, rather than rely exclusively on pre-processing adjustments or
post hoc correction mechanisms. In this paper, we describe the construction of
a multi-component reward function, the specification of the agents action space
over feature subsets, and the integration of this system with ensemble
learning. We aim to provide a flexible and generalizable way to select features
in environments where predictors are correlated and biases can inadvertently
re-emerge.

</details>


### [57] [ICL-Router: In-Context Learned Model Representations for LLM Routing](https://arxiv.org/abs/2510.09719)
*Chenxu Wang,Hao Li,Yiqun Zhang,Linyao Chen,Jianhao Chen,Ping Jian,Peng Ye,Qiaosheng Zhang,Shuyue Hu*

Main category: cs.LG

TL;DR: 提出了一种使用上下文向量表示模型能力的新型路由方法，通过两阶段训练实现模型路由，在分布内外任务中达到最先进性能，并能无缝集成新模型而无需重新训练路由器。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型路由方法依赖准确模型表示、添加新模型需要重新训练的限制可扩展性问题。

Method: 两阶段方法：1) 查询嵌入和投影到向量空间，训练投影器和基于LLM的路由器重构原始查询；2) 在查询集上分析候选模型性能，路由器基于查询和模型性能的上下文向量学习预测模型是否能正确回答新查询。

Result: 在分布内外任务中实现了最先进的路由性能，能够无缝集成新模型而不需要重新训练路由器。

Conclusion: 该方法通过上下文向量有效表示模型能力，显著提升了模型路由的性能和可扩展性。

Abstract: Large language models (LLMs) often exhibit complementary strengths. Model
routing harnesses these strengths by dynamically directing each query to the
most suitable model, given a candidate model pool. However, routing performance
relies on accurate model representations, and adding new models typically
requires retraining, limiting scalability. To address these challenges, we
propose a novel routing method using in-context vectors to represent model
capabilities. The method proceeds in two stages. First, queries are embedded
and projected into vectors, with a projector and LLM-based router trained to
reconstruct the original queries, aligning vector representations with the
router's semantic space. Second, each candidate model is profiled on a query
set, and the router learns -- based on in-context vectors of query and model
performance -- to predict whether each model can correctly answer new queries.
Extensive experiments demonstrate that our method achieves state-of-the-art
routing performance in both in-distribution and out-of-distribution tasks.
Moreover, our method allows for seamless integration of new models without
retraining the router. The code is available at
https://github.com/lalalamdbf/ICL-Router.

</details>


### [58] [A Stochastic Differential Equation Framework for Multi-Objective LLM Interactions: Dynamical Systems Analysis with Code Generation Applications](https://arxiv.org/abs/2510.10739)
*Shivani Shukla,Himanshu Joshi*

Main category: cs.LG

TL;DR: 提出了一个随机微分方程框架来建模LLM多目标优化动态，通过扩散项捕捉LLM响应的随机性，并使用干扰矩阵揭示目标间的系统性干扰模式。


<details>
  <summary>Details</summary>
Motivation: LLM在多目标优化中的动态行为缺乏系统性分析框架，需要理解目标间的干扰效应和收敛特性。

Method: 使用随机微分方程框架，包含显式扩散项和干扰矩阵，以代码生成为验证应用，分析了400个会话的安全、效率和功能目标。

Result: 展示了策略依赖的收敛行为（收敛率0.33-1.29），平衡方法的预测准确率达到R²=0.74。

Conclusion: 证明了动态系统分析在多目标LLM交互中的可行性，代码生成作为初步验证领域。

Abstract: We introduce a general stochastic differential equation framework for
modelling multiobjective optimization dynamics in iterative Large Language
Model (LLM) interactions. Our framework captures the inherent stochasticity of
LLM responses through explicit diffusion terms and reveals systematic
interference patterns between competing objectives via an interference matrix
formulation. We validate our theoretical framework using iterative code
generation as a proof-of-concept application, analyzing 400 sessions across
security, efficiency, and functionality objectives. Our results demonstrate
strategy-dependent convergence behaviors with rates ranging from 0.33 to 1.29,
and predictive accuracy achieving R2 = 0.74 for balanced approaches. This work
proposes the feasibility of dynamical systems analysis for multi-objective LLM
interactions, with code generation serving as an initial validation domain.

</details>


### [59] [Building a Foundational Guardrail for General Agentic Systems via Synthetic Data](https://arxiv.org/abs/2510.09781)
*Yue Huang,Hang Hua,Yujun Zhou,Pengcheng Jing,Manish Nagireddy,Inkit Padhi,Greta Dolcetti,Zhangchen Xu,Subhajit Chaudhury,Ambrish Rawat,Liubov Nedoshivina,Pin-Yu Chen,Prasanna Sattigeri,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 该论文提出了AuraGen数据生成引擎、Safiron防护模型和Pre-Exec Bench基准，用于在LLM智能体执行前检测计划级风险，填补了数据、模型和评估三个关键空白。


<details>
  <summary>Details</summary>
Motivation: 现有防护措施主要在智能体执行后运作，难以扩展且缺乏对计划级别的可控监督，某些风险一旦执行会造成严重后果，因此需要在规划阶段进行干预。

Method: 1) AuraGen：可控数据生成引擎，合成良性轨迹、注入类别标记的风险、通过奖励模型过滤输出；2) Safiron：基础防护模型，结合跨规划器适配器和紧凑防护模型；3) Pre-Exec Bench：包含多样化工具和分支轨迹的现实基准。

Result: 实验表明提出的防护模型在Pre-Exec Bench上相比强基线获得一致增益，消融研究提炼了可操作实践。

Conclusion: 该方法为更安全的智能体系统提供了实用模板，通过计划级风险检测实现了更可控的监督。

Abstract: While LLM agents can plan multi-step tasks, intervening at the planning
stage-before any action is executed-is often the safest way to prevent harm,
since certain risks can lead to severe consequences once carried out. However,
existing guardrails mostly operate post-execution, which is difficult to scale
and leaves little room for controllable supervision at the plan level. To
address this challenge, we highlight three critical gaps in current research:
data gap, model gap, and evaluation gap. To close the data gap, we introduce
AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii)
injects category-labeled risks with calibrated difficulty, and (iii) filters
outputs via an automated reward model, producing large and reliable corpora for
pre-execution safety. To close the guardian model gap, we propose a
foundational guardrail Safiron, combining a cross-planner adapter with a
compact guardian model. The adapter unifies different input formats, while
Safiron flags risky cases, assigns risk types, and generates rationales;
trained in two stages with a broadly explored data recipe, Safiron achieves
robust transfer across settings. To close the evaluation gap, we release
Pre-Exec Bench, a realistic benchmark covering diverse tools and branching
trajectories, which measures detection, fine-grained categorization,
explanation, and cross-planner generalization in human-verified scenarios.
Extensive experiments demonstrate consistent gains of the proposed guardrail
over strong baselines on Pre-Exec Bench, and ablations further distill
actionable practices, providing a practical template for safer agentic systems.

</details>


### [60] [Learning Bug Context for PyTorch-to-JAX Translation with LLMs](https://arxiv.org/abs/2510.09898)
*Hung Phan,Son Le Vu,Ali Jannesari*

Main category: cs.LG

TL;DR: T2J是一个增强提示框架，通过注入结构化指导来改进LLM从PyTorch到JAX的代码翻译，显著提升翻译质量和运行效率。


<details>
  <summary>Details</summary>
Motivation: PyTorch和JAX虽然都嵌入Python，但在核心设计、执行语义和生态系统成熟度上存在差异，且JAX较新、公开代码较少，现有评估方法在跨框架基准测试中存在不足。

Method: 构建包含TorchLeet和GitHub来源的PyTorch数据集，用GPT-4o-mini生成初始JAX草稿，由专业开发者迭代修复至功能等价，创建包含常见错误和修复的固定错误数据集，并构建注入结构化指导的增强提示。

Result: T2J将GPT-4o-mini在CodeBLEU上的性能提升达10%，T2J修复成本得分提升50%，T2J代码翻译得分提升1.33分（0-4分制），T2J比较得分提升100%，生成代码运行速度比基线快2.5倍。

Conclusion: T2J框架通过结构化指导显著提升了轻量级LLM在PyTorch到JAX代码翻译中的性能，解决了跨框架翻译的挑战。

Abstract: Despite recent progress of large language models (LLMs) on code translation
among mainstream languages, translating PyTorch to JAX remains nontrivial. The
two libraries, though both embedded in Python, differ in core design, execution
semantics, and ecosystem maturity; JAX is newer and comparatively
underrepresented in public code, and parallel PyTorch--JAX corpora are limited.
Weaknesses in existing evaluation further complicate cross-framework
benchmarking. We present T2J, a prompt-augmentation framework that strengthens
LLM-based PyTorch to JAX translation. Our pipeline (i) assembles two PyTorch
sources -- the problem-solving set from TorchLeet (Aroori & Chien, 2025) and a
GitHub-derived set from CodeParrot (Wolf et al., 2022) -- and uses GPT-4o-mini
to produce initial JAX drafts; (ii) engages two professional developers to
iteratively repair those drafts until functional equivalence, yielding a
curated fixed-bug dataset of common errors and patches; and (iii) constructs
augmented prompts that inject structured guidance from these fixes to steer
lightweight LLMs (e.g., GPT-4o-mini). We also introduce three metrics tailored
to PyTorch to JAX: T2J CodeTrans Score, T2J FixCost Score (an LLM-based
estimate of bug-fix effort), and T2J Comparison Score (LLM-as-judge).
Empirically, T2J raises GPT-4o-mini performance by up to 10% on CodeBLEU, 50%
on T2J FixCost Score, 1.33 points on T2J CodeTrans Score (0--4 scale), and 100%
on T2J Comparison Score; moreover, the generated code runs up to 2.5x faster
than the baseline.

</details>


### [61] [Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models](https://arxiv.org/abs/2510.09976)
*Mingyang Lyu,Yinqian Sun,Erliang Lin,Huangrui Li,Ruolin Chen,Feifei Zhao,Yi Zeng*

Main category: cs.LG

TL;DR: 提出了Flow Policy Optimization (FPO)算法，用于改进基于流匹配的视觉-语言-动作模型，通过重新定义重要性采样和集成多种技术实现稳定高效的在线强化学习微调。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型性能受限于监督数据的质量和覆盖范围，而传统策略梯度方法在流匹配模型中计算不可行，需要新的强化学习方法。

Method: FPO算法通过重新定义重要性采样过程，结合结构感知信用分配、裁剪替代目标、多步潜在探索和Q集成机制，实现稳定可扩展的在线强化学习微调。

Result: 在LIBERO基准测试和ALOHA模拟任务中，FPO相比监督学习、偏好对齐、扩散模型等基线方法表现出持续改进，在稀疏奖励下实现稳定学习。

Conclusion: FPO算法有效解决了流匹配模型在强化学习中的计算挑战，通过提出的计算模块实现了稳定收敛和性能提升。

Abstract: Vision-Language-Action (VLA) models such as OpenVLA, Octo, and $\pi_0$ have
shown strong generalization by leveraging large-scale demonstrations, yet their
performance is still fundamentally constrained by the quality and coverage of
supervised data. Reinforcement learning (RL) provides a promising path for
improving and fine-tuning VLAs through online interaction. However,
conventional policy gradient methods are computationally infeasible in the
context of flow-matching based models due to the intractability of the
importance sampling process, which requires explicit computation of policy
ratios. To overcome this limitation, we propose Flow Policy Optimization (FPO)
algorithm, which reformulates importance sampling by leveraging per-sample
changes in the conditional flow-matching objective. Furthermore, FPO achieves
stable and scalable online reinforcement fine-tuning of the $\pi_0$ model by
integrating structure-aware credit assignment to enhance gradient efficiency,
clipped surrogate objectives to stabilize optimization, multi-step latent
exploration to encourage diverse policy updates, and a Q-ensemble mechanism to
provide robust value estimation. We evaluate FPO on the LIBERO benchmark and
the ALOHA simulation task against supervised, preference-aligned,
diffusion-based, autoregressive online RL, and $\pi_0$-FAST baselines,
observing consistent improvements over the imitation prior and strong
alternatives with stable learning under sparse rewards. In addition, ablation
studies and analyses of the latent space dynamics further highlight the
contributions of individual components within FPO, validating the effectiveness
of the proposed computational modules and the stable convergence of the
conditional flow-matching objective during online RL.

</details>


### [62] [Skill-Targeted Adaptive Training](https://arxiv.org/abs/2510.10023)
*Yinghui He,Abhishek Panigrahi,Yong Lin,Sanjeev Arora*

Main category: cs.LG

TL;DR: STAT是一种新的微调策略，利用更强LLM的元认知能力作为教师，通过分析学生模型的技能缺失情况，创建针对性的训练集来提升数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调在类似训练数据上往往出现性能饱和现象，需要更有效的训练策略来突破这种限制。

Method: 使用强LLM作为教师，分析任务所需技能并标记数据点，创建学生模型的缺失技能档案，然后通过STAT-Sel（重加权现有训练样本）或STAT-Syn（合成新样本）两种方式构建针对性训练集。

Result: 在Llama和Qwen模型上，STAT在MATH数据集上提升达7.5%，在分布外基准测试上平均提升4.6%，且与GRPO强化学习方法互补。

Conclusion: 技能导向的自适应训练应该广泛改进当前的训练流程。

Abstract: Language models often show little to no improvement (i.e., "saturation") when
trained via vanilla supervised fine-tuning (SFT) on data similar to what they
saw in their training set (e.g., MATH). We introduce a new fine-tuning
strategy, STAT, to train such a student model by using the metacognition
ability of a stronger large language model (LLM) as the teacher. The teacher
uses the task dataset to create a list of skills needed for the task, and then
labels each data point with its required skills (Didolkar et al., 2024). By
monitoring the student's answers, the teacher creates a Missing-Skill-Profile
for the student, tracking how often they failed to apply each skill in their
responses. We use this idea to build a modified training set in one of two
ways. In STAT-Sel, the teacher uses an existing set of training examples but
adaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn,
the teacher synthesizes additional examples involving missing skills. Across
extensive experiments on Llama and Qwen models, our methods yield improvements
of up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore,
STAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25,
AMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is
complementary to RL via GRPO (Shao et al., 2024): after the model is improved
using STAT to address skill gaps, GRPO continues to add further gains. We
conclude that skill-targeted adaptive training should broadly improve current
training pipelines. Our code is available at:
https://github.com/princeton-pli/STAT.

</details>


### [63] [Experience-Efficient Model-Free Deep Reinforcement Learning Using Pre-Training](https://arxiv.org/abs/2510.10029)
*Ruoxing Yang*

Main category: cs.LG

TL;DR: PPOPT是一种新颖的模型无关深度强化学习算法，通过预训练在物理环境中实现高训练效率和稳定性，特别适用于小样本训练场景。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要大量环境交互样本，但在复杂环境中计算成本高昂。PPOPT旨在通过预训练减少环境交互需求，提高训练效率。

Method: 采用新型策略神经网络架构，包含预训练的神经网络中间层和两个全连接网络，通过在相似物理环境上预训练来迁移物理特性理解。

Result: PPOPT在小训练样本上优于经典PPO算法，在奖励获取和训练稳定性方面表现更好。虽然不如DYNA DDPG等模型方法，但训练时间显著更短。

Conclusion: PPOPT通过预训练实现了高效的模型无关强化学习，特别适合计算资源有限的小样本训练场景。

Abstract: We introduce PPOPT - Proximal Policy Optimization using Pretraining, a novel,
model-free deep-reinforcement-learning algorithm that leverages pretraining to
achieve high training efficiency and stability on very small training samples
in physics-based environments. Reinforcement learning agents typically rely on
large samples of environment interactions to learn a policy. However, frequent
interactions with a (computer-simulated) environment may incur high
computational costs, especially when the environment is complex. Our main
innovation is a new policy neural network architecture that consists of a
pretrained neural network middle section sandwiched between two fully-connected
networks. Pretraining part of the network on a different environment with
similar physics will help the agent learn the target environment with high
efficiency because it will leverage a general understanding of the
transferrable physics characteristics from the pretraining environment. We
demonstrate that PPOPT outperforms baseline classic PPO on small training
samples both in terms of rewards gained and general training stability. While
PPOPT underperforms against classic model-based methods such as DYNA DDPG, the
model-free nature of PPOPT allows it to train in significantly less time than
its model-based counterparts. Finally, we present our implementation of PPOPT
as open-source software, available at github.com/Davidrxyang/PPOPT.

</details>


### [64] [Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective](https://arxiv.org/abs/2510.10150)
*Zhezheng Hao,Hong Wang,Haoyang Liu,Jian Luo,Jiarui Yu,Hande Dong,Qiang Lin,Can Wang,Jiawei Chen*

Main category: cs.LG

TL;DR: 本文分析了强化学习中可验证奖励（RLVR）训练过程中的熵崩溃问题，提出了STEER方法通过token级熵变化感知的重新加权方案来稳定熵动态，防止策略多样性丧失。


<details>
  <summary>Details</summary>
Motivation: RLVR训练中存在熵崩溃风险，即策略多样性快速丧失，导致泛化能力不足。现有熵干预方法机制不明确且效果有限。

Method: 提出STEER方法，通过token级熵变化感知的重新加权方案，自适应稳定熵动态，进行细粒度token级调整。

Result: 实验表明STEER显著缓解熵崩溃，稳定熵动态，在多个数学推理基准上实现更强的下游性能。

Conclusion: STEER通过直接控制熵动态，有效解决了现有方法的局限性，在防止过利用的同时促进鲁棒探索。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) can enhance LLM
reasoning, its training process poses a critical risk: entropy collapse. This
phenomenon is a rapid loss of policy diversity, stemming from the
exploration-exploitation imbalance and leading to a lack of generalization.
Recent entropy-intervention methods aim to prevent \coloredtext{entropy
collapse}, yet their underlying mechanisms remain unclear. In this paper, we
conduct a quantitative analysis to reveal token-level entropy changes and how
existing entropy intervention methods help avoid entropy collapse. Our findings
point out a fundamental limitation of existing methods: they attempt to control
entropy dynamics indirectly. By only affecting related factors, such as the
advantage signal and generation probability, their effectiveness is inherently
limited and could potentially fail. To address this limitation, we introduce an
entropy-change-aware reweighting scheme, namely Stabilizing Token-level
Entropy-changE via Reweighting (STEER), that adaptively stabilizes entropy
dynamics through fine-grained token-level adjustments. Our approach mitigates
over-exploitation while fostering robust exploration. Extensive experiments
demonstrate that STEER significantly mitigates entropy collapse, stabilizes
entropy dynamics, and achieves stronger downstream performance across various
mathematical reasoning benchmarks \footnote{Our code is available at
https://github.com/zz-haooo/STEER.

</details>


### [65] [SGM: A Statistical Godel Machine for Risk-Controlled Recursive Self-Modification](https://arxiv.org/abs/2510.10232)
*Xuening Wu,Shenqin Yin,Yanlan Kang,Xinhang Zhang,Qianya Xu,Zeping Chen,Wenqiang Zhang*

Main category: cs.LG

TL;DR: 提出统计哥德尔机(SGM)，首个用于递归编辑的统计安全层，用统计置信测试替代证明要求，在选定置信水平下认证改进，同时分配全局错误预算来限制累积风险。


<details>
  <summary>Details</summary>
Motivation: 递归自修改在AutoML、神经架构搜索和自适应优化中日益重要，但现有框架无法确保此类更改的安全性。哥德尔机通过要求形式化改进证明来提供原则性保障，但在随机高维环境中此类证明不可行。

Method: SGM用统计置信测试（e值、Hoeffding界）替代基于证明的要求，仅在选定置信水平下认证改进时才允许修改，同时分配全局错误预算。提出确认触发谐波支出(CTHS)，将支出索引到确认事件而非轮次，在保持族有效性的同时将错误预算集中在有希望的编辑上。

Result: 在监督学习、强化学习和黑盒优化实验中验证了SGM的作用：在CIFAR-100上认证真实增益，在ImageNet-100上拒绝虚假改进，在RL和优化基准上展示鲁棒性。

Conclusion: SGM为学习系统中持续、风险感知的自修改提供了基础性基础设施。

Abstract: Recursive self-modification is increasingly central in AutoML, neural
architecture search, and adaptive optimization, yet no existing framework
ensures that such changes are made safely. Godel machines offer a principled
safeguard by requiring formal proofs of improvement before rewriting code;
however, such proofs are unattainable in stochastic, high-dimensional settings.
We introduce the Statistical Godel Machine (SGM), the first statistical safety
layer for recursive edits. SGM replaces proof-based requirements with
statistical confidence tests (e-values, Hoeffding bounds), admitting a
modification only when superiority is certified at a chosen confidence level,
while allocating a global error budget to bound cumulative risk across
rounds.We also propose Confirm-Triggered Harmonic Spending (CTHS), which
indexes spending by confirmation events rather than rounds, concentrating the
error budget on promising edits while preserving familywise
validity.Experiments across supervised learning, reinforcement learning, and
black-box optimization validate this role: SGM certifies genuine gains on
CIFAR-100, rejects spurious improvement on ImageNet-100, and demonstrates
robustness on RL and optimization benchmarks.Together, these results position
SGM as foundational infrastructure for continual, risk-aware self-modification
in learning systems.Code is available at:
https://github.com/gravitywavelet/sgm-anon.

</details>


### [66] [Lost in the Middle: An Emergent Property from Information Retrieval Demands in LLMs](https://arxiv.org/abs/2510.10276)
*Nikolaus Salvatore,Hao Wang,Qiong Zhang*

Main category: cs.LG

TL;DR: LLMs在长文本中表现出'中间信息丢失'现象，这与人类记忆的首因和近因效应类似。研究发现这种U形性能曲线是LLMs适应不同信息检索需求的训练结果，而非简单的信息丢失缺陷。


<details>
  <summary>Details</summary>
Motivation: 解释LLMs在长文本处理中为何出现中间信息性能下降的现象，探索这种现象是否反映了类似人类记忆的机制。

Method: 从零开始训练GPT-2和Llama变体模型，使用模拟人类长期和短期记忆需求的简单范式，分析位置偏差的形成机制。

Result: 发现U形性能曲线确实在训练中出现，近因效应与训练数据中的短期记忆需求直接对应，而首因效应由均匀的长期记忆需求诱导，并受自回归特性和注意力汇聚点形成的影响。

Conclusion: LLMs中的位置偏差是信息检索需求、模型架构和训练过程中结构注意力动态共同作用的结果，这种现象在更接近LLM预训练的下一个标记预测任务中也得到验证。

Abstract: The performance of Large Language Models (LLMs) often degrades when crucial
information is in the middle of a long context, a "lost-in-the-middle"
phenomenon that mirrors the primacy and recency effects in human memory. We
propose that this behavior is not simply a flaw indicative of information loss
but an adaptation to different information retrieval demands during
pre-training: some tasks require uniform recall across the entire input (a
long-term memory demand), while others prioritize the most recent information
(a short-term memory demand). Consistent with this view, we show that this
U-shaped performance curve emerges when LLMs (GPT-2 and Llama variants) are
trained from scratch on two simple human memory paradigms simulating long-term
and short-term memory demands. Our analysis reveals that while the recency
effect directly aligns with short-term memory demand in the training data, the
primacy effect is induced by the uniform long-term memory demand and is
additionally influenced by the model's autoregressive properties and the
formation of attention sinks. Our main findings from simple human memory
paradigms also generalize to a sequence completion task, which more closely
resembles the next-token prediction process in LLM pre-training. Together, our
findings reveal how information retrieval demands, model architecture, and
structural attention dynamics during model training can jointly produce
positional bias observed in LLMs.

</details>


### [67] [Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting](https://arxiv.org/abs/2510.10304)
*Michael Y. Hu,Benjamin Van Durme,Jacob Andreas,Harsh Jhamtani*

Main category: cs.LG

TL;DR: ECHO是一个基于提示框架的语言模型代理架构，通过事后经验回放技术生成优化的反事实轨迹，从失败的交互中创建合成正例，提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理在新环境中学习时样本效率低下，这在交互成本高的环境中（如与人类交互或重置物理系统）严重限制了其实用性。现有架构对语言模型生成和推理完整反事实轨迹的能力利用有限。

Method: ECHO框架包含两个组件：事后规则（使用语言模型识别相关子目标并生成优化轨迹）和更新规则（在内存中维护压缩的轨迹表示）。该方法将强化学习中的事后经验回放技术适配到语言模型代理中。

Result: 在XMiniGrid和PeopleJoinQA两个领域的评估中，ECHO比普通语言代理基线性能提升高达80%；在XMiniGrid中，还优于Reflexion和AWM等复杂代理架构，通过更有效地利用过去经验实现了对新环境的更快适应。

Conclusion: ECHO框架通过事后优化生成合成正例，显著提高了语言模型代理在交互成本高环境中的样本效率和适应性。

Abstract: Language model (LM) agents deployed in novel environments often exhibit poor
sample efficiency when learning from sequential interactions. This
significantly hinders the usefulness of such agents in environments where
interaction is costly (for example, when they interact with humans or reset
physical systems). While a number of existing LM agent architectures
incorporate various mechanisms for experience storage and reflection, they make
limited use of LMs' abilities to directly generate or reason about full
counterfactual trajectories. We introduce ECHO (Experience Consolidation via
Hindsight Optimization), a prompting framework that adapts hindsight experience
replay from reinforcement learning for language model agents. ECHO generates
optimized trajectories for alternative goals that could have been achieved
during failed attempts, effectively creating synthetic positive examples from
unsuccessful interactions. Our approach consists of two components: a hindsight
rule that uses the language model itself to identify relevant subgoals and
generate optimized trajectories, and an update rule that maintains compressed
trajectory representations in memory. We evaluate ECHO on stateful versions of
XMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a
collaborative information-gathering enterprise simulation. Across both domains,
ECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid,
it also outperforms a number of sophisticated agent architectures including
Reflexion and AWM, demonstrating faster adaptation to novel environments
through more effective utilization of past experiences.

</details>


### [68] [Rethinking RL Evaluation: Can Benchmarks Truly Reveal Failures of RL Methods?](https://arxiv.org/abs/2510.10541)
*Zihan Chen,Yiming Zhang,Hengguang Zhou,Zenghui Ding,Yining Sun,Cho-Jui Hsieh*

Main category: cs.LG

TL;DR: 当前强化学习基准对评估LLM进展不足，提出Oracle性能差距指标和诊断套件，发现现有方法泛化能力差，建议设计更可靠的基准原则。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习基准无法可靠区分进展，因为训练集和测试集性能几乎相同，无法评估泛化能力。

Method: 引入诊断套件和Oracle性能差距(OPG)指标，通过压力测试分析现有RL方法的泛化能力。

Result: 发现尽管基准分数高，但现有RL方法在分布偏移、难度变化和反事实场景下泛化能力差。

Conclusion: 当前基准不足以评估泛化能力，提出三个核心设计原则：足够难度、平衡评估和分布鲁棒性。

Abstract: Current benchmarks are inadequate for evaluating progress in reinforcement
learning (RL) for large language models (LLMs).Despite recent benchmark gains
reported for RL, we find that training on these benchmarks' training sets
achieves nearly the same performance as training directly on the test sets,
suggesting that the benchmarks cannot reliably separate further progress.To
study this phenomenon, we introduce a diagnostic suite and the Oracle
Performance Gap (OPG) metric that quantifies the performance difference between
training on the train split versus the test split of a benchmark. We further
analyze this phenomenon with stress tests and find that, despite strong
benchmark scores, existing RL methods struggle to generalize across
distribution shifts, varying levels of difficulty, and counterfactual
scenarios: shortcomings that current benchmarks fail to reveal.We conclude that
current benchmarks are insufficient for evaluating generalization and propose
three core principles for designing more faithful benchmarks: sufficient
difficulty, balanced evaluation, and distributional robustness.

</details>


### [69] [Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents](https://arxiv.org/abs/2510.10586)
*Giulio Ruffini*

Main category: cs.LG

TL;DR: 提出了一个基于组合对称性的智能体框架，将自然感官流建模为有限参数李伪群在低维配置流形上的作用，并分析了智能体动力学中的结构约束和守恒量。


<details>
  <summary>Details</summary>
Motivation: 从算法视角理解智能体如何通过生成程序来跟踪和压缩感官流，探索组合对称性作为结构先验在智能体建模中的作用。

Method: 将智能体建模为与感官流耦合的神经动力系统，分析对称性对智能体构成方程和读出的等变性约束，以及在静态输入和缓慢漂移下的动力学约束。

Result: 证明了准确的世界跟踪要求智能体具有等变性结构，并产生守恒量和低维不变流形，这些流形与伪群的组合分解对齐。

Conclusion: 组合对称性为深度模型中的组合性提供了几何解释，并启发了基于对称性的预测编码框架。

Abstract: In the algorithmic (Kolmogorov) view, agents are programs that track and
compress sensory streams using generative programs. We propose a framework
where the relevant structural prior is simplicity (Solomonoff) understood as
\emph{compositional symmetry}: natural streams are well described by (local)
actions of finite-parameter Lie pseudogroups on geometrically and topologically
complex low-dimensional configuration manifolds (latent spaces). Modeling the
agent as a generic neural dynamical system coupled to such streams, we show
that accurate world-tracking imposes (i) \emph{structural constraints} --
equivariance of the agent's constitutive equations and readouts -- and (ii)
\emph{dynamical constraints}: under static inputs, symmetry induces conserved
quantities (Noether-style labels) in the agent dynamics and confines
trajectories to reduced invariant manifolds; under slow drift, these manifolds
move but remain low-dimensional. This yields a hierarchy of reduced manifolds
aligned with the compositional factorization of the pseudogroup, providing a
geometric account of the ``blessing of compositionality'' in deep models. We
connect these ideas to the Spencer formalism for Lie pseudogroups and formulate
a symmetry-based, self-contained version of predictive coding in which higher
layers receive only \emph{coarse-grained residual transformations}
(prediction-error coordinates) along symmetry directions unresolved at lower
layers.

</details>


### [70] [Neutral Agent-based Adversarial Policy Learning against Deep Reinforcement Learning in Multi-party Open Systems](https://arxiv.org/abs/2510.10937)
*Qizhou Peng,Yang Zheng,Yu Wen,Yanna Wu,Yingying Du*

Main category: cs.LG

TL;DR: 本文提出了一种在多方开放系统中无需直接与受害者智能体交互或完全控制环境即可误导训练有素智能体的对抗策略学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击技术在多党开放系统中应用受限，因为它们要么需要完全控制环境，要么依赖与受害者智能体的直接交互。本文旨在解决这些限制。

Method: 提出基于中立智能体的方法，通过共享环境间接影响受害者智能体，在SMAC平台和Highway-env自动驾驶仿真平台上进行评估。

Result: 实验结果表明，该方法能够在多方开放系统中发起通用且有效的对抗攻击。

Conclusion: 该方法为多方开放系统中的对抗攻击提供了新的可行方案，突破了现有技术的局限性。

Abstract: Reinforcement learning (RL) has been an important machine learning paradigm
for solving long-horizon sequential decision-making problems under uncertainty.
By integrating deep neural networks (DNNs) into the RL framework, deep
reinforcement learning (DRL) has emerged, which achieved significant success in
various domains. However, the integration of DNNs also makes it vulnerable to
adversarial attacks. Existing adversarial attack techniques mainly focus on
either directly manipulating the environment with which a victim agent
interacts or deploying an adversarial agent that interacts with the victim
agent to induce abnormal behaviors. While these techniques achieve promising
results, their adoption in multi-party open systems remains limited due to two
major reasons: impractical assumption of full control over the environment and
dependent on interactions with victim agents.
  To enable adversarial attacks in multi-party open systems, in this paper, we
redesigned an adversarial policy learning approach that can mislead
well-trained victim agents without requiring direct interactions with these
agents or full control over their environments. Particularly, we propose a
neutral agent-based approach across various task scenarios in multi-party open
systems. While the neutral agents seemingly are detached from the victim
agents, indirectly influence them through the shared environment. We evaluate
our proposed method on the SMAC platform based on Starcraft II and the
autonomous driving simulation platform Highway-env. The experimental results
demonstrate that our method can launch general and effective adversarial
attacks in multi-party open systems.

</details>


### [71] [Stronger Together: On-Policy Reinforcement Learning for Collaborative LLMs](https://arxiv.org/abs/2510.11062)
*Yujie Zhao,Lanxiang Hu,Yang Wang,Minmin Hou,Hao Zhang,Ke Ding,Jishen Zhao*

Main category: cs.LG

TL;DR: 提出了AT-GRPO方法，将多智能体系统与强化学习结合，通过智能体和回合分组优化解决MAS中的RL挑战，在规划、编码和数学任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统和强化学习都能增强大语言模型的智能能力，但将在线策略RL应用于MAS仍存在算法和系统层面的挑战，需要专门解决方案。

Method: AT-GRPO包括：(i)针对MAS定制的智能体和回合分组RL算法，(ii)支持单策略和多策略模型的训练系统。

Result: 在长时程规划任务中，准确率从14.0-47.0%提升到96.0-99.5%；编码任务平均提升3.87-7.62%；数学任务平均提升9.0-17.93%。

Conclusion: AT-GRPO成功解决了MAS中应用在线策略RL的挑战，在多个任务领域都取得了显著性能提升。

Abstract: Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to
enhance the agentic capabilities of large language models (LLMs). MAS improves
task performance through role-based orchestration, while RL uses environmental
rewards to learn stronger policies, such as GRPO-style optimization. However,
applying on-policy RL to MAS remains underexplored and presents unique
challenges. Algorithmically, standard GRPO grouping assumptions break down
because prompts vary by role and by turn. System-wise, the training stack must
support MAS-workflow rollouts and on-policy updates for both single-policy and
multi-policy models.
  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL
algorithm tailored to MAS and (ii) a training system that supports both single-
and multi-policy regimes. Across game, planning, coding, and math tasks,
AT-GRPO delivers substantial gains. On long-horizon planning, it increases
accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5
percent. It also improves reasoning performance, with average gains of 3.87 to
7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and
environments are available at: https://github.com/pettingllms-ai/PettingLLMs.

</details>


### [72] [Can Tool-Integrated Reinforcement Learning Generalize Across Diverse Domains?](https://arxiv.org/abs/2510.11184)
*Zhengyu Chen,Jinluan Yang,Teng Xiao,Ruochen Zhou,Luan Zhang,Xiangyu Xi,Xiaowei Shi,Wei Wang,Jinggang Wang*

Main category: cs.LG

TL;DR: LLM代理通过数学任务训练的代码解释器工具，在跨领域推理任务中展现出良好的泛化能力和token效率。


<details>
  <summary>Details</summary>
Motivation: 探索工具增强强化学习在不同领域的泛化能力，尽管训练仅限于数学问题解决任务。

Method: 提出Tool Generalization Reinforcement Learning (TGRL)框架，包括标准化工具接口、双组件奖励系统和XML提示模板。

Result: 在多个不同基准测试中实现最先进性能，验证了基于数学任务学习的工具使用可以有效地转移到其他领域的复杂任务。

Conclusion: 工具强化学习在LLM推理中具有跨领域潜力，能够实现任务性能和高token效率。

Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities in reasoning and tool utilization. However, the generalization of
tool-augmented reinforcement learning (RL) across diverse domains remains
underexplored. In this work, we investigate the cross-domain generalization of
an LLM agent equipped with a code interpreter tool, which is exclusively
trained on mathematical problem-solving tasks. Despite the restricted training
domain, we evaluate the agent's performance across several distinct reasoning
domains. The results reveal that RL-based tool usage learned from mathematical
tasks can be effectively transferred to complex tasks in other domains,
enabling great task performance and high token efficiency. To facilitate this
cross-domain transfer, we propose a Tool Generalization Reinforcement Learning
(TGRL) framework designed to promote domain-agnostic learning and skill
migration, encompassing: (i) a standardized tool interface that abstracts
domain-specific nuances through consistent formatting and explicit termination,
fostering transferable invocation patterns; (ii) a dual-component reward system
that decomposes rewards to incentivize generalizable behaviors like tool
efficiency and reasoning abstraction, ensuring alignment and robustness across
domain shifts; and (iii) an XML-based prompt template that separates thinking,
tool calls, and responses to encourage modular, domain-invariant planning and
coherent multi-turn interactions. Extensive experiments across diverse
benchmarks validate our approach, achieving state-of-the-art performance and
highlighting the cross-domain potential of Tool RL for LLM reasoning.

</details>


### [73] [ENIGMA: The Geometry of Reasoning and Alignment in Large-Language Models](https://arxiv.org/abs/2510.11278)
*Gareth Seneque,Lap-Hang Ho,Nafise Erfanian Saeedi,Jeffrey Molendijk,Ariel Kupermann,Tim Elson*

Main category: cs.LG

TL;DR: ENIGMA是一种新颖的LLM训练方法，通过将组织政策视为信息流形上的方向，联合提升推理、对齐和鲁棒性。该方法结合了GRPO、SAMI风格对称InfoNCE辅助和Sinkhorn最优传输正则化，无需奖励模型即可实现原则性推理。


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练中推理、对齐和鲁棒性通常被视为独立目标，作者假设这些是单一信息几何目标的不同投影，旨在通过统一的信息几何框架同时优化这些能力。

Method: 使用单循环训练器结合：1) GRPO（基于策略、无评论家的RL方法，仅使用CoT格式奖励）；2) SAMI风格对称InfoNCE辅助；3) 隐状态分布上的熵Sinkhorn最优传输正则化。还引入了infoNCE指标和充分性指数来选择和创建原则。

Result: 在1B参数LLM上的实验表明，高SI原则预测了更稳定的训练动态和优于GRPO消融实验的基准性能。信息几何分析验证了流形上的理想结构变化。

Conclusion: 推理、对齐和鲁棒性是单一信息几何目标的不同投影，ENIGMA训练的模型无需奖励模型即可展示原则性推理，为可信能力提供了路径。

Abstract: We present Entropic Mutual-Information Geometry Large-Language Model
Alignment (ENIGMA), a novel approach to Large-Language Model (LLM) training
that jointly improves reasoning, alignment and robustness by treating an
organisation's policies/principles as directions to move on a model's
information manifold. Our single-loop trainer combines Group-Relative Policy
Optimisation (GRPO), an on-policy, critic-free RL method with Chain-of-Thought
(CoT)-format only rewards; a Self-Supervised Alignment with Mutual Information
(SAMI)-style symmetric InfoNCE auxiliary; and an entropic Sinkhorn
optimal-transport regulariser on hidden-state distributions to bound geometry
drift. We also introduce infoNCE metrics that specialise to a standard MI lower
bound under matched negatives to measure how strongly a model's CoT encodes
these policies. These metrics include a Sufficiency Index (SI) that enables the
selection and creation of principles that maximise downstream performance prior
to training. In our experiments using small (1B) LLMs, high-SI principles
predict steadier training dynamics and improved benchmark performance over GRPO
ablations. Our information-geometry analysis of trained models validates
desirable structural change in the manifold. These results support our
hypothesis that reasoning, alignment, and robustness are projections of a
single informationgeometric objective, and that models trained using ENIGMA
demonstrate principled reasoning without the use of a reward model, offering a
path to trusted capability

</details>


### [74] [ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding](https://arxiv.org/abs/2510.11498)
*Yuhang Li,Chenchen Zhang,Ruilin Lv,Ao Liu,Ken Deng,Yuanxing Zhang,Jiaheng Liu,Wiggin Zhou,Bo Zhou*

Main category: cs.LG

TL;DR: ReLook是一个基于视觉的强化学习框架，通过多模态LLM作为工具，实现前端代码生成中的生成-诊断-优化循环，显著提升视觉前端代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在算法代码生成方面表现出色，但在前端开发中表现不佳，因为前端代码的正确性需要基于渲染像素和交互来判断。

Method: 采用代理式、视觉基础的强化学习框架，利用多模态LLM作为视觉批评者和反馈源，引入强制优化规则确保单调改进，并在推理时解耦批评者以降低延迟。

Result: 在三个广泛使用的基准测试中，ReLook在视觉前端代码生成方面持续优于强基线方法。

Conclusion: ReLook展示了代理感知、视觉奖励和训练-推理解耦在前端代码生成中的显著优势。

Abstract: While Large Language Models (LLMs) excel at algorithmic code generation, they
struggle with front-end development, where correctness is judged on rendered
pixels and interaction. We present ReLook, an agentic, vision-grounded
reinforcement learning framework that empowers an agent to close a robust
generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.
During training, the agent uses the MLLM-in-the-loop both as a visual
critic--scoring code with screenshots--and as a source of actionable,
vision-grounded feedback; a strict zero-reward rule for invalid renders anchors
renderability and prevents reward hacking. To prevent behavioral collapse, we
introduce Forced Optimization, a strict acceptance rule that admits only
improving revisions, yielding monotonically better trajectories. At inference,
we decouple the critic and run a lightweight, critic-free self-edit cycle,
keeping latency comparable to base decoding while retaining most of the gains.
Across three widely used benchmarks, ReLook consistently outperforms strong
baselines in vision-grounded front-end code generation, highlighting the
benefits of agentic perception, visual rewards, and training-inference
decoupling.

</details>


### [75] [Refining Hybrid Genetic Search for CVRP via Reinforcement Learning-Finetuned LLM](https://arxiv.org/abs/2510.11121)
*Rongjie Zhu,Cong Zhang,Zhiguang Cao*

Main category: cs.LG

TL;DR: 提出RFTHGS框架，通过强化学习微调小型LLM来为车辆路径问题生成高性能交叉算子，超越专家设计的启发式方法和GPT-4等大型模型。


<details>
  <summary>Details</summary>
Motivation: 挑战当前依赖大型通用LLM（如GPT-4）作为启发式设计器的范式，证明精心微调的小型专用LLM可以生成超越专家设计的组件。

Method: 使用强化学习框架微调小型LLM，采用多层级课程奖励函数，结合算子缓存机制防止抄袭并促进多样性，为混合遗传搜索求解器生成交叉算子。

Result: 微调后的LLM生成的交叉算子显著优于HGS中的专家设计算子，性能优势从小规模实例扩展到1000节点的大规模问题，超越神经组合基线、提示方法和商业LLM。

Conclusion: 精心微调的小型专用LLM可以超越大型通用模型和专家设计，为组合优化问题生成高性能组件，展示了专业化模型的价值。

Abstract: While large language models (LLMs) are increasingly used as automated
heuristic designers for vehicle routing problems (VRPs), current
state-of-the-art methods predominantly rely on prompting massive,
general-purpose models like GPT-4. This work challenges that paradigm by
demonstrating that a smaller, specialized LLM, when meticulously fine-tuned,
can generate components that surpass expert-crafted heuristics within advanced
solvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for
Fine-Tuning a small LLM to generate high-performance crossover operators for
the Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).
Our method employs a multi-tiered, curriculum-based reward function that
progressively guides the LLM to master generating first compilable, then
executable, and finally, superior-performing operators that exceed human expert
designs. This is coupled with an operator caching mechanism that discourages
plagiarism and promotes diversity during training. Comprehensive experiments
show that our fine-tuned LLM produces crossover operators which significantly
outperform the expert-designed ones in HGS. The performance advantage remains
consistent, generalizing from small-scale instances to large-scale problems
with up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading
neuro-combinatorial baselines, prompt-based methods, and commercial LLMs such
as GPT-4o and GPT-4o-mini.

</details>


### [76] [Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models](https://arxiv.org/abs/2510.11683)
*Nianyi Lin,Jiajie Zhang,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: 提出了BGPO算法，通过构建特殊的ELBO下界来解决dLLMs强化学习中的内存瓶颈问题，实现更准确的目标函数估计和更好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在dLLMs的RL训练中需要保留所有MC样本的前向计算图，导致内存开销大，限制了样本数量，从而影响似然函数近似精度和RL目标函数估计。

Method: 提出Boundary-Guided Policy Optimization (BGPO)算法，构建满足线性和等价性两个关键性质的特殊下界，支持跨样本梯度累积，实现恒定内存使用。

Result: BGPO在数学问题求解、代码生成和规划任务中显著优于之前的dLLMs RL算法。

Conclusion: BGPO通过内存高效的RL算法设计，解决了dLLMs训练中的内存瓶颈问题，实现了更准确的似然函数近似和更好的性能。

Abstract: A key challenge in applying reinforcement learning (RL) to diffusion large
language models (dLLMs) lies in the intractability of their likelihood
functions, which are essential for the RL objective, necessitating
corresponding approximation in each training step. While existing methods
approximate the log-likelihoods by their evidence lower bounds (ELBOs) via
customized Monte Carlo (MC) sampling, the forward computational graphs of all
MC samples need to be retained for the gradient computation of non-linear terms
in the RL objective, resulting in significant memory overhead. This constraint
restricts feasible sample sizes, leading to imprecise likelihood approximations
and ultimately distorting the RL objective. To overcome this limitation, we
propose \emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient
RL algorithm that maximizes a specially constructed lower bound of the
ELBO-based objective. This lower bound is carefully designed to satisfy two key
properties: (1) Linearity: it is formulated in a linear sum where each term
depends only on a single MC sample, thereby enabling gradient accumulation
across samples and ensuring constant memory usage; (2) Equivalence: Both the
value and gradient of this lower bound are equal to those of the ELBO-based
objective in on-policy training, making it also an effective approximation for
the original RL objective. These properties allow BGPO to adopt a large MC
sample size, resulting in more accurate likelihood approximations and improved
RL objective estimation, which in turn leads to enhanced performance.
Experiments show that BGPO significantly outperforms previous RL algorithms for
dLLMs in math problem solving, code generation, and planning tasks.

</details>


### [77] [QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs](https://arxiv.org/abs/2510.11696)
*Wei Huang,Yi Ge,Shuai Yang,Yicheng Xiao,Huizi Mao,Yujun Lin,Hanrong Ye,Sifei Liu,Ka Chun Cheung,Hongxu Yin,Yao Lu,Xiaojuan Qi,Song Han,Yukang Chen*

Main category: cs.LG

TL;DR: QeRL是一个量化增强的强化学习框架，通过结合NVFP4量化和LoRA技术，加速LLM的RL训练过程，减少内存开销，并利用量化噪声增强探索能力。


<details>
  <summary>Details</summary>
Motivation: 传统RL训练对LLM来说资源密集，需要大量GPU内存和长训练时间，限制了其在大型模型上的应用。

Method: 结合NVFP4量化和LoRA技术，引入自适应量化噪声机制，动态调整训练过程中的噪声水平。

Result: 在rollout阶段实现1.5倍加速，首次在单张H100 80GB GPU上训练32B LLM，在数学基准测试中达到GSM8K 90.8%和MATH 500 77.4%的准确率。

Conclusion: QeRL为LLM的RL训练提供了一个高效且有效的框架，在保持性能的同时显著提升了训练效率。

Abstract: We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for
large language models (LLMs). While RL is essential for LLMs' reasoning
capabilities, it is resource-intensive, requiring substantial GPU memory and
long rollout durations. QeRL addresses these issues by combining NVFP4
quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL
while reducing memory overhead. Beyond efficiency, our findings show that
quantization noise increases policy entropy, enhancing exploration, and
enabling the discovery of better strategies during RL. To further optimize
exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,
which dynamically adjusts noise during training. Experiments demonstrate that
QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is
the first framework to enable RL training of a 32B LLM on a single H100 80GB
GPU, while delivering overall speedups for RL training. It also achieves faster
reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while
matching the performance of full-parameter fine-tuning on mathematical
benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These
results establish QeRL as an efficient and effective framework for RL training
in LLMs.

</details>


### [78] [Emergence of hybrid computational dynamics through reinforcement learning](https://arxiv.org/abs/2510.11162)
*Roman A. Kononov,Nikita A. Pospelov,Konstantin V. Anokhin,Vladimir V. Nekorkin,Oleg V. Maslennikov*

Main category: cs.LG

TL;DR: 强化学习（RL）和监督学习（SL）在相同决策任务中驱动循环神经网络（RNN）形成根本不同的计算策略，RL自发发现混合吸引子架构，而SL主要收敛到简单固定点解决方案。


<details>
  <summary>Details</summary>
Motivation: 探索学习算法本身如何塑造神经网络中涌现的计算策略，理解不同学习范式对网络动态特性的影响。

Method: 通过系统动力学分析，比较RL和SL训练的RNN在相同决策任务中的计算解决方案，分析吸引子架构和神经群体特性。

Result: RL发现混合吸引子架构（稳定固定点+准周期吸引子），SL主要收敛到固定点解决方案；RL通过隐式正则化塑造功能平衡的神经群体，增强鲁棒性。

Conclusion: 学习算法是涌现计算的主要决定因素，基于奖励的优化能自主发现复杂动态机制，为设计自适应AI系统提供原则。

Abstract: Understanding how learning algorithms shape the computational strategies that
emerge in neural networks remains a fundamental challenge in machine
intelligence. While network architectures receive extensive attention, the role
of the learning paradigm itself in determining emergent dynamics remains
largely unexplored. Here we demonstrate that reinforcement learning (RL) and
supervised learning (SL) drive recurrent neural networks (RNNs) toward
fundamentally different computational solutions when trained on identical
decision-making tasks. Through systematic dynamical systems analysis, we reveal
that RL spontaneously discovers hybrid attractor architectures, combining
stable fixed-point attractors for decision maintenance with quasi-periodic
attractors for flexible evidence integration. This contrasts sharply with SL,
which converges almost exclusively to simpler fixed-point-only solutions. We
further show that RL sculpts functionally balanced neural populations through a
powerful form of implicit regularization -- a structural signature that
enhances robustness and is conspicuously absent in the more heterogeneous
solutions found by SL-trained networks. The prevalence of these complex
dynamics in RL is controllably modulated by weight initialization and
correlates strongly with performance gains, particularly as task complexity
increases. Our results establish the learning algorithm as a primary
determinant of emergent computation, revealing how reward-based optimization
autonomously discovers sophisticated dynamical mechanisms that are less
accessible to direct gradient-based optimization. These findings provide both
mechanistic insights into neural computation and actionable principles for
designing adaptive AI systems.

</details>


### [79] [Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony](https://arxiv.org/abs/2510.11345)
*Han Lu,Zichen Liu,Shaopan Xiong,Yancheng He,Wei Gao,Yanan Wu,Weixun Wang,Jiashun Liu,Yang Li,Haizhou Zhao,Ju Huang,Siran Yang,Xiaoyang Li,Yijia Luo,Zihe Liu,Ling Pan,Junchi Yan,Wei Wang,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng*

Main category: cs.LG

TL;DR: ROLL Flash是一个支持异步强化学习后训练的系统，通过细粒度并行化和rollout-train解耦设计，显著提高了资源利用率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的同步强化学习后训练系统存在资源利用率低和可扩展性有限的问题，需要设计更高效的异步训练架构。

Method: 基于细粒度并行化和rollout-train解耦两个核心设计原则，提供灵活的编程接口，支持完全异步训练架构和高效的rollout机制。

Result: ROLL Flash在相同GPU预算下，在RLVR任务上实现2.24倍加速，在智能体任务上实现2.72倍加速，异步训练性能与同步训练相当。

Conclusion: 异步强化学习后训练可以显著提高资源利用率和训练效率，同时保持与同步训练相当的性能表现。

Abstract: Synchronous Reinforcement Learning (RL) post-training has emerged as a
crucial step for enhancing Large Language Models (LLMs) with diverse
capabilities. However, many systems designed to accelerate RL post-training
still suffer from low resource utilization and limited scalability. We present
ROLL Flash, a system that extends ROLL with native support for asynchronous RL
post-training. ROLL Flash is built upon two core design principles:
fine-grained parallelism and rollout-train decoupling. Guided by these
principles, ROLL Flash provides flexible programming interfaces that enable a
fully asynchronous training architecture and support efficient rollout
mechanisms, including queue scheduling and environment-level asynchronous
execution. Through comprehensive theoretical analysis and extensive
experiments, we demonstrate that ROLL Flash significantly improves resource
utilization and scalability over synchronous RL post-training. ROLL Flash
achieves up to 2.24x speedup on RLVR tasks and 2.72x on agentic tasks, using
the same GPU budget as synchronous baselines. Furthermore, we implement several
popular off-policy algorithms and verify that asynchronous training can achieve
performance on par with synchronous training.

</details>


### [80] [Offline Reinforcement Learning with Generative Trajectory Policies](https://arxiv.org/abs/2510.11499)
*Xinsong Feng,Leshu Tang,Chenan Wang,Haipeng Chen*

Main category: cs.LG

TL;DR: 提出Generative Trajectory Policies (GTPs)作为新的生成策略范式，通过ODE统一视角将扩散、流匹配和一致性模型整合，在离线强化学习中实现高性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成模型在离线强化学习中面临的计算效率与性能之间的权衡问题，扩散策略计算昂贵而一致性策略性能较差。

Method: 提出GTPs范式，将现代生成模型视为学习由ODE控制的连续时间生成轨迹的特定实例，学习底层ODE的整个解映射。

Result: 在D4RL基准测试中达到最先进性能，显著优于先前生成策略，在多个困难的AntMaze任务中获得完美分数。

Conclusion: GTPs成功弥合了生成模型在离线强化学习中的效率与性能差距，为生成策略提供了统一的理论基础。

Abstract: Generative models have emerged as a powerful class of policies for offline
reinforcement learning (RL) due to their ability to capture complex,
multi-modal behaviors. However, existing methods face a stark trade-off: slow,
iterative models like diffusion policies are computationally expensive, while
fast, single-step models like consistency policies often suffer from degraded
performance. In this paper, we demonstrate that it is possible to bridge this
gap. The key to moving beyond the limitations of individual methods, we argue,
lies in a unifying perspective that views modern generative models, including
diffusion, flow matching, and consistency models, as specific instances of
learning a continuous-time generative trajectory governed by an Ordinary
Differential Equation (ODE). This principled foundation provides a clearer
design space for generative policies in RL and allows us to propose Generative
Trajectory Policies (GTPs), a new and more general policy paradigm that learns
the entire solution map of the underlying ODE. To make this paradigm practical
for offline RL, we further introduce two key theoretically principled
adaptations. Empirical results demonstrate that GTP achieves state-of-the-art
performance on D4RL benchmarks - it significantly outperforms prior generative
policies, achieving perfect scores on several notoriously hard AntMaze tasks.

</details>


### [81] [MATH-Beyond: A Benchmark for RL to Expand Beyond the Base Model](https://arxiv.org/abs/2510.11653)
*Prasanna Mayilvahanan,Ricardo Dominguez-Olmedo,Thaddäus Wiedemer,Wieland Brendel*

Main category: cs.LG

TL;DR: 论文指出现有RL微调方法主要是在强化基础模型已有的解题模式而非发现新方法，为此提出了MATH-B基准来挑战现有模型，推动探索驱动的RL方法发展。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准在大量采样下已被基础模型解决，RL微调只是优化已有能力而非培养新技能，需要更具挑战性的基准来推动真正的探索性RL研究。

Method: 构建MATH-B基准，从DAPO-Math-17K和DeepScaleR数据集中选取能击败8B参数以下开源模型的数学问题，即使在大采样预算下也保持挑战性。

Result: 现有RL微调模型如Nemotron-Research-Reasoning-Qwen-1.5B和DeepScaleR-1.5B-Preview在MATH-B上表现不佳，即使在pass@1024下也无法解决这些问题。

Conclusion: MATH-B基准能有效评估RL方法是否真正提升了推理能力，有望推动探索驱动的RL方法发展，激发更深层次的推理能力。

Abstract: With the advent of DeepSeek-R1, a new wave of reinforcement learning (RL)
methods has emerged that seem to unlock stronger mathematical reasoning.
However, a closer look at the open-source ecosystem reveals a critical
limitation: with sufficiently many draws (e.g., $\texttt{pass@1024}$), many
existing base models already solve nearly all questions on widely used math
benchmarks such as MATH-500 and AIME 2024. This suggests that the RL
fine-tuning methods prevalent in the LLM reasoning literature largely sharpen
existing solution modes rather than discovering entirely new ones. Such
sharpening stands in contrast to the broader promise of RL: to foster
exploration and to acquire new skills. To move beyond this plateau, we
introduce MATH-Beyond (MATH-B), a benchmark deliberately constructed to defeat
common open-source models of up to 8B parameters even under large sampling
budgets. Improving performance on our benchmark via RL requires methods that
learn to reason in ways that go beyond base model capabilities in repeated
sampling. Since the problems are drawn from subsets of DAPO-Math-17K and
DeepScaleR datasets, they remain topically equivalent to standard high-school
math. Validating our premise, RL fine-tuned models such as
Nemotron-Research-Reasoning-Qwen-1.5B and DeepScaleR-1.5B-Preview perform
poorly on MATH-B at $\texttt{pass@1024}$, showing how existing approaches fall
short on tackling harder instances. We hope MATH-B will catalyze
exploration-driven RL approaches that elicit deeper reasoning capabilities. We
release MATH-B at https://huggingface.co/datasets/brendel-group/MATH-Beyond.

</details>


### [82] [Representation-Based Exploration for Language Models: From Test-Time to Post-Training](https://arxiv.org/abs/2510.11686)
*Jens Tuyls,Dylan J. Foster,Akshay Krishnamurthy,Jordan T. Ash*

Main category: cs.LG

TL;DR: 该论文研究了在语言模型强化学习中，通过基于预训练模型隐藏状态的表示多样性奖励来促进探索，显著提高了推理任务的多样性和效率。


<details>
  <summary>Details</summary>
Motivation: 研究当前强化学习技术是否真正促进新行为的发现，还是仅仅强化了基础模型中已有的行为，探索预训练模型知识如何指导这种搜索。

Method: 使用基于预训练语言模型隐藏状态的表示多样性奖励来激励模型发现新颖多样的行为，应用于后训练和推理时扩展设置。

Result: 表示多样性探索显著提高了多样性和pass@k率，在推理时提高效率，在后训练中改进推理性能。例如Qwen-2.5-14b-Instruct在几乎所有任务上获得超过50%的验证器效率提升。

Conclusion: 通过正确的多样性概念进行有意识的探索，是实现超越简单强化的新行为发现的实际路径。

Abstract: Reinforcement learning (RL) promises to expand the capabilities of language
models, but it is unclear if current RL techniques promote the discovery of
novel behaviors, or simply sharpen those already present in the base model. In
this paper, we investigate the value of deliberate exploration -- explicitly
incentivizing the model to discover novel and diverse behaviors -- and aim to
understand how the knowledge in pre-trained models can guide this search. Our
main finding is that exploration with a simple, principled,
representation-based bonus derived from the pre-trained language model's hidden
states significantly improves diversity and pass@k rates -- both for
post-training, and in a novel inference-time scaling setting we introduce. For
inference-time, exploration with representation-based diversity improves
efficiency, consistently improving pass@k rates across a variety of models and
reasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50%
improvement in verifier efficiency on almost all tasks. For post-training, we
show that integrating this exploration strategy into an RL pipeline improves
reasoning performance over that of the initial model and over standard RL
post-training. For example, on AIME 2024, our post-trained
Qwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model,
demonstrating a 3x improvement in test-time sample efficiency. Overall, our
findings suggest that deliberate exploration -- with the right notion of
diversity -- is a practical path toward discovery of new behaviors beyond
sharpening.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [83] [A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System](https://arxiv.org/abs/2510.09721)
*Jiale Guo,Suizhi Huang,Mei Li,Dong Huang,Xingsheng Chen,Regina Zhang,Zhijiang Guo,Han Yu,Siu-Ming Yiu,Christian Jensen,Pietro Lio,Kwok-Yan Lam*

Main category: cs.SE

TL;DR: 该论文对LLM赋能的软件工程进行了首次全面分析，通过分析150多篇论文构建了涵盖解决方案和基准测试的完整分类法，揭示了从简单提示工程到复杂智能体系统的发展路径。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件工程领域的应用缺乏对基准测试和解决方案之间相互关系的系统性理解，阻碍了该领域的系统性进展和评估。

Method: 分析150多篇近期论文，构建包含解决方案（提示基础、微调基础、智能体基础）和基准测试（代码生成、翻译、修复等任务）两个维度的综合分类法。

Result: 提出了统一的流水线，展示了从任务规范到最终交付的完整工作流程，连接了50多个基准测试及其对应的解决方案策略。

Conclusion: 该调查为研究人员和实践者提供了理解、评估和推进LLM赋能软件工程系统的基础资源，并指出了多智能体协作框架、自进化代码生成系统等未来研究方向。

Abstract: The integration of LLMs into software engineering has catalyzed a paradigm
shift from traditional rule-based systems to sophisticated agentic systems
capable of autonomous problem-solving. Despite this transformation, the field
lacks a comprehensive understanding of how benchmarks and solutions
interconnect, hindering systematic progress and evaluation. This survey
presents the first holistic analysis of LLM-empowered software engineering,
bridging the critical gap between evaluation and solution approaches. We
analyze 150+ recent papers and organize them into a comprehensive taxonomy
spanning two major dimensions: (1) Solutions, categorized into prompt-based,
fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code
generation, translation, repair, and other tasks. Our analysis reveals how the
field has evolved from simple prompt engineering to complex agentic systems
incorporating planning and decomposition, reasoning and self-refinement, memory
mechanisms, and tool augmentation. We present a unified pipeline that
illustrates the complete workflow from task specification to final
deliverables, demonstrating how different solution paradigms address varying
complexity levels across software engineering tasks. Unlike existing surveys
that focus on isolated aspects, we provide full-spectrum coverage connecting
50+ benchmarks with their corresponding solution strategies, enabling
researchers to identify optimal approaches for specific evaluation criteria.
Furthermore, we identify critical research gaps and propose actionable future
directions, including multi-agent collaboration frameworks, self-evolving code
generation systems, and integration of formal verification with LLM-based
methods. This survey serves as a foundational resource for researchers and
practitioners seeking to understand, evaluate, and advance LLM-empowered
software engineering systems.

</details>


### [84] [InteractScience: Programmatic and Visually-Grounded Evaluation of Interactive Scientific Demonstration Code Generation](https://arxiv.org/abs/2510.09724)
*Qiaosheng Chen,Yang Liu,Lei Li,Kai Chen,Qipeng Guo,Gong Cheng,Fei Yuan*

Main category: cs.SE

TL;DR: InteractScience是首个评估LLMs结合科学知识与交互式前端代码生成能力的基准测试，涵盖五个科学领域，通过程序化功能测试和视觉定性测试来验证生成的交互式科学演示。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试要么评估知识问答而不涉及代码，要么评估静态网页代码生成而缺乏科学交互性，无法评估LLMs结合科学知识和交互式前端代码生成的能力。

Method: 设计混合框架，结合程序化功能测试验证交互逻辑，以及基于视觉的定性测试评估渲染输出与参考快照的匹配度。构建包含五个科学领域问题的InteractScience基准测试。

Result: 评估了30个领先的开源和闭源LLMs，结果显示在整合领域知识与交互式前端编码方面仍存在明显弱点。

Conclusion: InteractScience是首个能自动测量LLMs结合科学知识与交互式前端代码生成能力的基准测试，为推进可靠且教育有用的科学演示代码生成奠定了基础。

Abstract: Large Language Models (LLMs) are increasingly capable of generating complete
applications from natural language instructions, creating new opportunities in
science and education. In these domains, interactive scientific demonstrations
are particularly valuable for explaining concepts, supporting new teaching
methods, and presenting research findings. Generating such demonstrations
requires models to combine accurate scientific knowledge with the ability to
implement interactive front-end code that behaves correctly and responds to
user actions. This capability goes beyond the scope of existing benchmarks,
which typically evaluate either knowledge question answering without grounding
in code or static web code generation without scientific interactivity. To
evaluate this integrated ability, we design a hybrid framework that combines
programmatic functional testing to rigorously verify interaction logic with
visually-grounded qualitative testing to assess rendered outputs against
reference snapshots. Building on this framework, we present InteractScience, a
benchmark consisting of a substantial set of carefully designed questions
across five scientific domains, each paired with unit tests, reference
snapshots, and checklists. We evaluate 30 leading open- and closed-source LLMs
and report results that highlight ongoing weaknesses in integrating domain
knowledge with interactive front-end coding. Our work positions InteractScience
as the first benchmark to automatically measure this combined capability with
realistic interactive operations, providing a foundation for advancing reliable
and educationally useful scientific demonstration code generation. All code and
data are publicly available at https://github.com/open-compass/InteractScience.

</details>


### [85] [Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem](https://arxiv.org/abs/2510.09907)
*Muhammad Maaz,Liam DeVoe,Zac Hatfield-Dodds,Nicholas Carlini*

Main category: cs.SE

TL;DR: 该论文提出了一个基于LLM的代理系统，能够分析Python模块、从代码和文档中推断函数特性和跨函数属性，合成并执行基于属性的测试，通过测试输出确认真实bug，最终为开发者输出可操作的bug报告。


<details>
  <summary>Details</summary>
Motivation: 基于属性的测试是一种轻量级形式化方法，但传统方法需要人工指定测试属性和输入域。本研究旨在利用LLM自动推断软件属性并执行测试，实现自主化的软件测试。

Method: 开发了一个LLM代理系统，该系统能够：1）分析Python模块；2）从代码和文档中推断函数特性和跨函数属性；3）合成并执行基于属性的测试；4）通过测试输出确认真实bug；5）输出可操作的bug报告。

Result: 在100个流行Python包上的评估显示：56%的bug报告是有效bug，32%是值得向维护者报告的有效bug。使用排名标准筛选出的21个高优先级bug中，86%有效，81%值得报告。已报告5个bug，其中4个附带补丁，3个补丁成功合并到NumPy和云计算SDK中。

Conclusion: LLM与基于属性测试的结合提供了一种严谨且可扩展的自主软件测试方法，能够发现从序列化失败到数值精度错误等多种类型的bug。

Abstract: Property-based testing (PBT) is a lightweight formal method, typically
implemented as a randomized testing framework. Users specify the input domain
for their test using combinators supplied by the PBT framework, and the
expected properties or invariants as a unit-test function. The framework then
searches for a counterexample, e.g. by generating inputs and calling the test
function. In this work, we demonstrate an LLM-based agent which analyzes Python
modules, infers function-specific and cross-function properties from code and
documentation, synthesizes and executes PBTs, reflects on outputs of these
tests to confirm true bugs, and finally outputs actionable bug reports for the
developer. We perform an extensive evaluation of our agent across 100 popular
Python packages. Of the bug reports generated by the agent, we found after
manual review that 56\% were valid bugs and 32\% were valid bugs that we would
report to maintainers. We then developed a ranking rubric to surface
high-priority valid bugs to developers, and found that of the 21 top-scoring
bugs, 86\% were valid and 81\% we would report. The bugs span diverse failure
modes from serialization failures to numerical precision errors to flawed cache
implementations. We reported 5 bugs, 4 with patches, including to NumPy and
cloud computing SDKs, with 3 patches merged successfully. Our results suggest
that LLMs with PBT provides a rigorous and scalable method for autonomously
testing software. Our code and artifacts are available at:
https://github.com/mmaaz-git/agentic-pbt.

</details>


### [86] [SLEAN: Simple Lightweight Ensemble Analysis Network for Multi-Provider LLM Coordination: Design, Implementation, and Vibe Coding Bug Investigation Case Study](https://arxiv.org/abs/2510.10010)
*Matheus J. T. Vargas*

Main category: cs.SE

TL;DR: SLEAN是一个基于文本提示编排的确定性框架，通过独立分析、交叉批评和仲裁三阶段协议，协调多个LLM提供商过滤有害的AI生成代码建议。


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助调试中产生的修改引入不必要复杂性、破坏现有功能或处理错误问题的情况，确保代码修改的质量和安全性。

Method: 使用.txt模板作为LLM之间的简单提示桥接，采用三阶段协议：独立分析、交叉批评和仲裁，无需专业技术知识即可部署。

Result: 在评估15个软件bug时，SLEAN过滤了69个AI生成的修复建议，接受了22个修复（31.9%），拒绝了47个有害修复，将代码变更范围减少了83-90%。

Conclusion: SLEAN提供了一种轻量级、提供商无关的架构，适用于安全审计、代码审查等领域，实现可靠的多提供商合成和端到端可审计性。

Abstract: We present SLEAN (Simple Lightweight Ensemble Analysis Network), a
deterministic framework for coordinating multiple LLM providers through
text-based prompt orchestration. Unlike complex multi-agent systems requiring
specialized infrastructure, SLEAN operates as a simple prompt bridge between
LLMs using .txt templates, requiring no deep technical knowledge for
deployment. The three-phase protocol formed by independent analysis,
cross-critique, and arbitration, filters harmful AI-generated code suggestions
before production deployment, addressing how AI-assisted debugging increasingly
produces modifications that introduce unnecessary complexity, break existing
functionality, or address problems. Evaluating 15 software bugs, we analyzed 69
AI-generated fix propositions. SLEAN's filtering accepted 22 fixes (31.9%, 95%
CI 20.9-42.9%) while rejecting 47 that would have been harmful if applied
verbatim. The arbitration process reduced code change surface by 83-90%
relative to raw AI outputs, enforcing minimal causal edits over scope-expanding
modifications. Minimal Type 2 inputs proved more efficient than detailed Type 1
inputs, requiring 2.85 versus 3.56 propositions per accepted fix (35.1% versus
28.1% acceptance, about a 20% efficiency gain). Agreement between AI systems
showed weak correlation with fix quality: high convergence (at least 80%)
occurred in 4 of 15 cases and improved acceptance by only 2.4% points;
arbitration appeared only at exactly 10% convergence in 2 of 15 cases, although
low convergence alone did not necessitate arbitration. The file-driven,
provider-agnostic architecture enables deployment without specialized coding
expertise, making it applicable to security auditing, code review, document
verification, and other domains requiring reliable multi-provider synthesis
with end-to-end auditability.

</details>


### [87] [IntrinTrans: LLM-based Intrinsic Code Translator for RISC-V Vector](https://arxiv.org/abs/2510.10119)
*Liutong Han,Zhiyuan Tan,Hongbin Zhang,Pengcheng Wang,Chu Kang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: IntrinTrans是一个基于LLM的多智能体方法，利用编译-测试反馈自动跨架构翻译内联函数代码，并通过活跃度分析优化生成的RISC-V向量内联函数性能。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V硬件软件生态系统的快速扩展，对RISC-V向量扩展支持的需求日益增长。手动翻译现有向量化内联函数代码到RVV内联函数耗时且容易出错，现有基于规则的方法翻译成功率有限且性能不佳。

Method: 使用基于LLM的多智能体方法，结合编译-测试反馈进行自动跨架构翻译，并通过活跃度分析获取寄存器使用信息来优化生成的RVV内联函数。

Result: 实验显示高级LLM在有限迭代次数内能在大多数情况下生成语义正确的RISC-V向量内联函数，在某些情况下性能达到开源社区原生实现的5.93倍。

Conclusion: IntrinTrans方法能够有效自动翻译和优化跨架构内联函数代码，显著提升翻译效率和性能。

Abstract: The use of intrinsic functions to exploit hardware-specific capabilities is
an important approach for optimizing library performance. Many mainstream
libraries implement a large number of vectorized algorithms on Arm or x86 SIMD
intrinsic functions. With the rapid expansion of the RISC-V hardware-software
ecosystem, there is a growing demand for support of the RISC-V Vector (RVV)
extension. Translating existing vectorized intrinsic code onto RVV intrinsics
is a practical and effective approach. However, current cross-architecture
translation largely relies on manual rewriting, which is time-consuming and
error-prone. Furthermore, while some rule-based methods can reduce the need for
manual intervention, their translation success rate is limited by incomplete
rule coverage and syntactic constraints, and the performance suffers from
inadequate utilization of RVV-specific features. We present IntrinTrans, a
LLM-based multi-agent approach that utilizes compile-and-test feedback to
translate intrinsic code across architectures automatically, and further
optimizes the generated RVV intrinsics using register-usage information derived
from liveness analysis. To evaluate the effectiveness of our approach, we
collected 34 vectorized algorithm cases from open-source libraries. Each case
includes an Arm Neon intrinsics implementation and a RVV intrinsics
implementation contributed by the open-source community, together with
correctness and performance tests. Our experiments show that advanced LLMs
produce semantically correct RISC-V Vector intrinsics in most cases within a
limited number of iterations, and in some cases achieve up to 5.93x the
performance of the native implementation from the open-source community.

</details>


### [88] [A Systematic Study on Generating Web Vulnerability Proof-of-Concepts Using Large Language Models](https://arxiv.org/abs/2510.10148)
*Mengyao Zhao,Kaixuan Li,Lyuye Zhang,Wenjing Dang,Chenggong Ding,Sen Chen,Zheli Liu*

Main category: cs.SE

TL;DR: LLMs can自动生成有效的概念验证漏洞利用，在仅使用公开数据的情况下成功率为8%-34%，通过代码上下文和自适应推理策略可将成功率提升至68%-72%。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在代码理解和推理方面的进步，研究其是否能利用公开披露的CVE信息自动生成有效的PoC漏洞利用，这对漏洞复现、理解和缓解具有重要意义。

Method: 评估GPT-4o和DeepSeek-R1在100个真实可复现CVE上的表现，涵盖三个漏洞披露阶段：仅描述的新披露漏洞、有补丁的1-day漏洞、有完整上下文代码的N-day漏洞。

Result: LLMs仅使用公开数据就能在8%-34%的情况下自动生成可工作的PoC，DeepSeek-R1表现优于GPT-4o。补充代码上下文可提高17%-20%成功率，函数级比文件级提升9%-13%。集成自适应推理策略后成功率可达68%-72%。

Conclusion: LLMs能够重塑漏洞利用的动态格局，已有23个新生成的PoC被NVD和Exploit DB接受。

Abstract: Recent advances in Large Language Models (LLMs) have brought remarkable
progress in code understanding and reasoning, creating new opportunities and
raising new concerns for software security. Among many downstream tasks,
generating Proof-of-Concept (PoC) exploits plays a central role in
vulnerability reproduction, comprehension, and mitigation. While previous
research has focused primarily on zero-day exploitation, the growing
availability of rich public information accompanying disclosed CVEs leads to a
natural question: can LLMs effectively use this information to automatically
generate valid PoCs? In this paper, we present the first empirical study of
LLM-based PoC generation for web application vulnerabilities, focusing on the
practical feasibility of leveraging publicly disclosed information. We evaluate
GPT-4o and DeepSeek-R1 on 100 real-world and reproducible CVEs across three
stages of vulnerability disclosure: (1) newly disclosed vulnerabilities with
only descriptions, (2) 1-day vulnerabilities with patches, and (3) N-day
vulnerabilities with full contextual code. Our results show that LLMs can
automatically generate working PoCs in 8%-34% of cases using only public data,
with DeepSeek-R1 consistently outperforming GPT-4o. Further analysis shows that
supplementing code context improves success rates by 17%-20%, with
function-level providing 9%-13% improvement than file-level ones. Further
integrating adaptive reasoning strategies to prompt refinement significantly
improves success rates to 68%-72%. Our findings suggest that LLMs could reshape
vulnerability exploitation dynamics. To date, 23 newly generated PoCs have been
accepted by NVD and Exploit DB.

</details>


### [89] [LLMs are All You Need? Improving Fuzz Testing for MOJO with Large Language Models](https://arxiv.org/abs/2510.10179)
*Linghan Huang,Peizhou Zhao,Huaming Chen*

Main category: cs.SE

TL;DR: MOJOFuzzer是一个针对新兴编程语言MOJO的自适应LLM模糊测试框架，通过多阶段过滤和动态提示调整，显著提高了测试用例的有效性和错误检测性能。


<details>
  <summary>Details</summary>
Motivation: MOJO作为一种新兴的高性能AI编程语言，缺乏完善的测试框架和语料库，导致LLM在测试时产生语义错误的代码，降低了模糊测试的效果。

Method: 提出MOJOFuzzer框架，集成多阶段过滤机制消除低质量输入，并基于运行时反馈动态调整LLM提示进行测试用例变异。

Result: 实验表明MOJOFuzzer显著提升了测试有效性、API覆盖率和错误检测性能，发现了13个先前未知的bug。

Conclusion: 该研究不仅推动了LLM驱动的软件测试领域发展，还为新兴编程语言的LLM测试建立了基础方法论。

Abstract: The rapid development of large language models (LLMs) has revolutionized
software testing, particularly fuzz testing, by automating the generation of
diverse and effective test inputs. This advancement holds great promise for
improving software reliability. Meanwhile, the introduction of MOJO, a
high-performance AI programming language blending Python's usability with the
efficiency of C and C++, presents new opportunities to enhance AI model
scalability and programmability. However, as a new language, MOJO lacks
comprehensive testing frameworks and a sufficient corpus for LLM-based testing,
which exacerbates model hallucination. In this case, LLMs will generate
syntactically valid but semantically incorrect code, significantly reducing the
effectiveness of fuzz testing. To address this challenge, we propose
MOJOFuzzer, the first adaptive LLM-based fuzzing framework designed for
zero-shot learning environments of emerging programming languages. MOJOFuzzer
integrates a mutil-phase framework that systematically eliminates low-quality
generated inputs before execution, significantly improving test case validity.
Furthermore, MOJOFuzzer dynamically adapts LLM prompts based on runtime
feedback for test case mutation, enabling an iterative learning process that
continuously enhances fuzzing efficiency and bug detection performance. Our
experimental results demonstrate that MOJOFuzzer significantly enhances test
validity, API coverage, and bug detection performance, outperforming
traditional fuzz testing and state-of-the-art LLM-based fuzzing approaches.
Using MOJOFuzzer, we have conducted a first large-scale fuzz testing evaluation
of MOJO, uncorvering 13 previous unknown bugs. This study not only advances the
field of LLM-driven software testing but also establishes a foundational
methodology for leveraging LLMs in the testing of emerging programming
languages.

</details>


### [90] [Grounded AI for Code Review: Resource-Efficient Large-Model Serving in Enterprise Pipelines](https://arxiv.org/abs/2510.10290)
*Sayan Mandal,Hua Jiang*

Main category: cs.SE

TL;DR: 提出了一种基于静态分析与AST引导的代码审查系统，通过量化开源模型和多层缓存实现快速反馈，在安全导向的C/C++标准中实现分钟级反馈，同时保持较高的违规减少率。


<details>
  <summary>Details</summary>
Motivation: 在合规性要求高的环境中，自动化代码审查采用率低，静态分析器产生大量低解释性输出，而简单使用LLM存在幻觉风险和成本过高问题。

Method: 将静态分析结果与AST引导的上下文提取相结合，使用单GPU按需服务堆栈（量化开源模型、多层缓存）来提供简洁解释和修复指导。

Result: 在安全导向C/C++标准评估中，实现中位数59.8秒的首反馈时间，保持竞争性的违规减少率和较低违规率，相比大型专有模型表现更好。

Conclusion: 该架构解耦设计允许团队独立采用基础层或服务层，内部调查显示减少了分类工作量和人工审查迭代次数，强调可重现性、可审计性和向更广泛标准扩展的路径。

Abstract: Automated code review adoption lags in compliance-heavy settings, where
static analyzers produce high-volume, low-rationale outputs, and naive LLM use
risks hallucination and incurring cost overhead. We present a production system
for grounded, PR-native review that pairs static-analysis findings with
AST-guided context extraction and a single-GPU, on-demand serving stack
(quantized open-weight model, multi-tier caching) to deliver concise
explanations and remediation guidance. Evaluated on safety-oriented C/C++
standards, the approach achieves sub-minute median first-feedback (offline p50
build+LLM 59.8s) while maintaining competitive violation reduction and lower
violation rates versus larger proprietary models. The architecture is
decoupled: teams can adopt the grounding/prompting layer or the serving layer
independently. A small internal survey (n=8) provides directional signals of
reduced triage effort and moderate perceived grounding, with participants
reporting fewer human review iterations. We outline operational lessons and
limitations, emphasizing reproducibility, auditability, and pathways to broader
standards and assisted patching.

</details>


### [91] [Bridging Semantics & Structure for Software Vulnerability Detection using Hybrid Network Models](https://arxiv.org/abs/2510.10321)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: 提出了一种结合异构图表示和轻量级本地LLM的混合框架，用于软件漏洞检测，在Java漏洞检测上达到93.57%的准确率，比现有方法提升8.36-17.81%。


<details>
  <summary>Details</summary>
Motivation: 现有静态和动态分析方法往往忽略影响不安全行为的结构依赖关系，需要一种能结合拓扑特征和语义推理的方法，同时避免大型云模型的成本和隐私问题。

Method: 将程序建模为异构图，捕捉控制和数据流关系作为复杂交互网络，结合图表示与轻量级(<4B)本地LLM，统一拓扑特征与语义推理。

Result: 在Java漏洞检测(二元分类)上达到93.57%准确率，比基于图注意力网络的嵌入方法提升8.36%，比预训练LLM基线(Qwen2.5 Coder 3B)提升17.81%。

Conclusion: 该方法提取显著子图并生成自然语言解释，提高了开发人员的可解释性，为可扩展、可解释且可本地部署的工具铺平道路。

Abstract: Software vulnerabilities remain a persistent risk, yet static and dynamic
analyses often overlook structural dependencies that shape insecure behaviors.
Viewing programs as heterogeneous graphs, we capture control- and data-flow
relations as complex interaction networks. Our hybrid framework combines these
graph representations with light-weight (<4B) local LLMs, uniting topological
features with semantic reasoning while avoiding the cost and privacy concerns
of large cloud models. Evaluated on Java vulnerability detection (binary
classification), our method achieves 93.57% accuracy-an 8.36% gain over Graph
Attention Network-based embeddings and 17.81% over pretrained LLM baselines
such as Qwen2.5 Coder 3B. Beyond accuracy, the approach extracts salient
subgraphs and generates natural language explanations, improving
interpretability for developers. These results pave the way for scalable,
explainable, and locally deployable tools that can shift vulnerability analysis
from purely syntactic checks to deeper structural and semantic insights,
facilitating broader adoption in real-world secure software development.

</details>


### [92] [Testing and Enhancing Multi-Agent Systems for Robust Code Generation](https://arxiv.org/abs/2510.10460)
*Zongyi Lyu,Songqiang Chen,Zhenlan Ji,Liwen Wang,Shuai Wang,Daoyuan Wu,Wenxuan Wang,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 本文首次全面研究了多智能体系统在代码生成中的鲁棒性问题，通过模糊测试方法发现主流MAS存在严重鲁棒性缺陷，并提出修复方法有效解决了40.0%-88.9%的失败案例。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在代码生成中表现出色，但其鲁棒性研究严重不足，这对实际部署构成关键挑战。

Method: 设计基于模糊测试的管道，包含语义保持的变异操作符和新颖的适应度函数，评估主流MAS在多个数据集和LLM上的表现。

Result: 发现各种流行MAS存在严重鲁棒性缺陷：在应用语义保持变异后，它们无法解决最初成功解决的7.9%-83.3%的问题。

Conclusion: 研究揭示了MAS中的关键鲁棒性缺陷，提供了有效的缓解策略，为开发更可靠的代码生成MAS贡献了重要见解。

Abstract: Multi-agent systems (MASs) have emerged as a promising paradigm for automated
code generation, demonstrating impressive performance on established benchmarks
by decomposing complex coding tasks across specialized agents with different
roles. Despite their prosperous development and adoption, their robustness
remains pressingly under-explored, raising critical concerns for real-world
deployment. This paper presents the first comprehensive study examining the
robustness of MASs for code generation through a fuzzing-based testing
approach. By designing a fuzzing pipeline incorporating semantic-preserving
mutation operators and a novel fitness function, we assess mainstream MASs
across multiple datasets and LLMs. Our findings reveal substantial robustness
flaws of various popular MASs: they fail to solve 7.9%-83.3% of problems they
initially resolved successfully after applying the semantic-preserving
mutations. Through comprehensive failure analysis, we identify a common yet
largely overlooked cause of the robustness issue: miscommunications between
planning and coding agents, where plans lack sufficient detail and coding
agents misinterpret intricate logic, aligning with the challenges inherent in a
multi-stage information transformation process. Accordingly, we also propose a
repairing method that encompasses multi-prompt generation and introduces a new
monitor agent to address this issue. Evaluation shows that our repairing method
effectively enhances the robustness of MASs by solving 40.0%-88.9% of
identified failures. Our work uncovers critical robustness flaws in MASs and
provides effective mitigation strategies, contributing essential insights for
developing more reliable MASs for code generation.

</details>


### [93] [How Students Use Generative AI for Software Testing: An Observational Study](https://arxiv.org/abs/2510.10551)
*Baris Ardic,Quentin Le Dilavrec,Andy Zaidman*

Main category: cs.SE

TL;DR: 研究新手开发者使用生成式AI进行单元测试的交互策略、依赖程度及感知的优缺点，发现四种交互策略和两种提示风格，AI辅助能节省时间但存在信任和质量问题。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI工具如ChatGPT在软件工程工作流中的集成如何改变开发者角色，特别是对新手开发者在单元测试工程中的控制权、输出质量和学习体验的影响。

Method: 对12名本科学生进行观察性研究，分析他们使用生成式AI进行单元测试任务的交互策略、提示风格，并测量测试有效性和代码质量。

Result: 识别出四种交互策略和两种提示风格，学生报告了节省时间、减轻认知负担等好处，但也存在信任降低、测试质量担忧和缺乏所有权等问题。策略和提示风格不影响测试有效性或代码质量。

Conclusion: 生成式AI辅助单元测试能提高效率但带来新的挑战，需要平衡AI辅助与开发者控制权，策略选择不影响最终测试质量。

Abstract: The integration of generative AI tools like ChatGPT into software engineering
workflows opens up new opportunities to boost productivity in tasks such as
unit test engineering. However, these AI-assisted workflows can also
significantly alter the developer's role, raising concerns about control,
output quality, and learning, particularly for novice developers. This study
investigates how novice software developers with foundational knowledge in
software testing interact with generative AI for engineering unit tests. Our
goal is to examine the strategies they use, how heavily they rely on generative
AI, and the benefits and challenges they perceive when using generative
AI-assisted approaches for test engineering. We conducted an observational
study involving 12 undergraduate students who worked with generative AI for
unit testing tasks. We identified four interaction strategies, defined by
whether the test idea or the test implementation originated from generative AI
or the participant. Additionally, we singled out prompting styles that focused
on one-shot or iterative test generation, which often aligned with the broader
interaction strategy. Students reported benefits including time-saving, reduced
cognitive load, and support for test ideation, but also noted drawbacks such as
diminished trust, test quality concerns, and lack of ownership. While strategy
and prompting styles influenced workflow dynamics, they did not significantly
affect test effectiveness or test code quality as measured by mutation score or
test smells.

</details>


### [94] [Generative AI and the Transformation of Software Development Practices](https://arxiv.org/abs/2510.10819)
*Vivek Acharya*

Main category: cs.SE

TL;DR: 本文探讨生成式AI如何改变软件工程实践，包括聊天式编程、多智能体系统等新开发方式，分析其带来的机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，生成式AI正在重塑软件设计、编写和维护的方式，需要研究这些新技术对软件工程实践的影响。

Method: 通过案例研究和行业数据分析，调查迭代式聊天开发、多智能体系统、动态提示编排和模型上下文协议集成等方法。

Result: 发现AI辅助技术可以加速开发周期、普及编程能力，但也面临模型可靠性和成本等挑战。

Conclusion: 需要为AI在软件工程中的负责任和有效使用制定新的角色、技能和最佳实践。

Abstract: Generative AI is reshaping how software is designed, written, and maintained.
Advances in large language models (LLMs) are enabling new development styles -
from chat-oriented programming and 'vibe coding' to agentic programming - that
can accelerate productivity and broaden access. This paper examines how
AI-assisted techniques are changing software engineering practice, and the
related issues of trust, accountability, and shifting skills. We survey
iterative chat-based development, multi-agent systems, dynamic prompt
orchestration, and integration via the Model Context Protocol (MCP). Using case
studies and industry data, we outline both the opportunities (faster cycles,
democratized coding) and the challenges (model reliability and cost) of
applying generative AI to coding. We describe new roles, skills, and best
practices for using AI in a responsible and effective way.

</details>


### [95] [Project-Level C-to-Rust Translation via Synergistic Integration of Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2510.10956)
*Zhiqiang Yuan,Wenjun Mao,Zhuo Chen,Xiyue Shang,Chong Wang,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: 提出了一种基于指针知识图谱的C到Rust项目级翻译方法，通过全局指针语义分析显著提升翻译后Rust代码的安全性和功能性正确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的C到Rust翻译方法在项目级别存在指针处理困难，缺乏全局视角导致生成的Rust代码仍包含大量不安全用法。

Method: 构建C-Rust指针知识图谱，包含指针使用信息和Rust导向的注解（所有权、可变性、可空性、生命周期），结合LLM进行项目级翻译。

Result: 相比基于规则的翻译和传统LLM重写，减少99.9%的不安全用法；相比模糊测试增强的LLM方法，功能正确性平均提高29.3%。

Conclusion: 指针知识图谱能有效提供全局指针语义，指导LLM生成更安全、更符合Rust习惯的代码。

Abstract: Translating C code into safe Rust is an effective way to ensure its memory
safety. Compared to rule-based translation which produces Rust code that
remains largely unsafe, LLM-based methods can generate more idiomatic and safer
Rust code because LLMs have been trained on vast amount of human-written
idiomatic code. Although promising, existing LLM-based methods still struggle
with project-level C-to-Rust translation. They typically partition a C project
into smaller units (\eg{} functions) based on call graphs and translate them
bottom-up to resolve program dependencies. However, this bottom-up,
unit-by-unit paradigm often fails to translate pointers due to the lack of a
global perspective on their usage. To address this problem, we propose a novel
C-Rust Pointer Knowledge Graph (KG) that enriches a code-dependency graph with
two types of pointer semantics: (i) pointer-usage information which record
global behaviors such as points-to flows and map lower-level struct usage to
higher-level units; and (ii) Rust-oriented annotations which encode ownership,
mutability, nullability, and lifetime. Synthesizing the \kg{} with LLMs, we
further propose \ourtool{}, which implements a project-level C-to-Rust
translation technique. In \ourtool{}, the \kg{} provides LLMs with
comprehensive pointer semantics from a global perspective, thus guiding LLMs
towards generating safe and idiomatic Rust code from a given C project. Our
experiments show that \ourtool{} reduces unsafe usages in translated Rust by
99.9\% compared to both rule-based translation and traditional LLM-based
rewriting, while achieving an average 29.3\% higher functional correctness than
those fuzzing-enhanced LLM methods.

</details>


### [96] [RepoSummary: Feature-Oriented Summarization and Documentation Generation for Code Repositories](https://arxiv.org/abs/2510.11039)
*Yifeng Zhu,Xianlin Zhao,Xutian Li,Yanzhen Zou,Haizhuo Yuan,Yue Wang,Bing Xie*

Main category: cs.SE

TL;DR: RepoSummary是一个面向特征的代码仓库摘要方法，能自动生成仓库文档并建立功能特征到代码元素的更准确追踪链接。


<details>
  <summary>Details</summary>
Motivation: 现有仓库摘要技术主要基于目录树结构，无法有效追踪高层次功能特征到协作实现的方法，限制了代码理解和维护效率。

Method: 提出特征导向的代码仓库摘要方法，同时自动生成仓库文档，建立功能特征到对应代码元素的追踪链接。

Result: 相比最先进基线HGEN，RepoSummary实现了更高的特征覆盖率和更准确的追踪能力：完全覆盖特征比例从61.2%提升到71.1%，文件级追踪召回率从29.9%提升到53.0%。

Conclusion: RepoSummary生成的文档在概念一致性、可理解性和格式方面都优于现有方法，能帮助开发者快速定位相关方法和文件。

Abstract: Repository summarization is a crucial research question in development and
maintenance for software engineering. Existing repository summarization
techniques primarily focus on summarizing code according to the directory tree,
which is insufficient for tracing high-level features to the methods that
collaboratively implement them. To address these limitations, we propose
RepoSummary, a feature-oriented code repository summarization approach that
simultaneously generates repository documentation automatically. Furthermore,
it establishes more accurate traceability links from functional features to the
corresponding code elements, enabling developers to rapidly locate relevant
methods and files during code comprehension and maintenance. Comprehensive
experiments against the state-of-the-art baseline (HGEN) demonstrate that
RepoSummary achieves higher feature coverage and more accurate traceability. On
average, it increases the rate of completely covered features in manual
documentation from 61.2% to 71.1%, improves file-level traceability recall from
29.9% to 53.0%, and generates documentation that is more conceptually
consistent, easier to understand, and better formatted than that produced by
existing approaches.

</details>


### [97] [DebugTA: An LLM-Based Agent for Simplifying Debugging and Teaching in Programming Education](https://arxiv.org/abs/2510.11076)
*Lingyue Fu,Haowei Yuan,Datong Chen,Xinyi Dai,Qingyao Li,Weinan Zhang,Weiwen Liu,Yong Yu*

Main category: cs.SE

TL;DR: 提出了DebugTA，一个基于LLM的调试和教学代理，通过专用工具（标准代码检索、变量替换、外部编译器）来简化编程教育中的调试任务，提高教学效果并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决编程教育中调试教学任务的两个关键挑战：输入复杂性和异质性增加了LLM的推理难度，以及现有方法未能充分利用标准代码，限制了LLM在调试任务中的潜力。

Method: 开发了DebugTA代理，通过工具调用（标准代码检索、变量替换对齐参考代码、外部编译器实时分析）将复杂任务分解为顺序LLM交互，简化每一步的逻辑推理。

Result: 在三个真实世界代码数据集上的实验表明，DebugTA持续提高了教学效果，同时显著降低了计算成本。

Conclusion: DebugTA通过工具辅助的分解方法有效解决了编程教育中调试任务的复杂性，提高了LLM在调试教学中的表现。

Abstract: In programming education, Debugging and Teaching (DT) task is a common
scenario where students receive assistance in correcting their erroneous code.
The task involves multiple inputs, including erroneous code, error messages,
reference solutions, and the question description, with the goal of generating
modification suggestions to the erroneous code. However, two key challenges
hinder the effectiveness of existing approaches. Firstly, the complexity and
heterogeneity of inputs inherent in DT tasks significantly elevate the
reasoning challenges faced by LLMs. Second, existing approaches often fail to
fully leverage the availability of standard code in DT tasks, forcing models to
rely solely on complex multi-step reasoning, which limits the potential of LLMs
in addressing DT tasks effectively. To address these challenges, we propose
DebugTA, a novel LLM-based debugging and teaching agent with specialized tools
for standard code retrieval, variable substitution to align reference code, and
an external compiler for real-time code analysis. Guided by explicit
pedagogical and debugging principles, DebugTA acts as an agent that decomposes
a complex task into sequential LLM interactions, each utilizing distinct tools
for specific subtasks, thereby simplifying the logical reasoning at each step
and reducing overall reasoning complexity. Furthermore, DebugTA utilizes tool
calls to align the standard code with the erroneous code as much as possible,
allowing the LLM to focus on logic errors within the erroneous code and
improving the accuracy of the generated suggestions. To rigorously assess the
quality of modification suggestions, we introduce a student simulator-teacher
interaction paradigm. Experimental results on three real-world code datasets
demonstrate that DebugTA consistently improves teaching effectiveness while
significantly reducing computational costs.

</details>


### [98] [What Slows Down FMware Development? An Empirical Study of Developer Challenges and Resolution Times](https://arxiv.org/abs/2510.11138)
*Zitao Wang,Zhimin Zhao,Michael W. Godfrey*

Main category: cs.SE

TL;DR: 这是首个大规模分析基础模型软件(FMware)开发的研究，涵盖云端平台和开源仓库，揭示了教育、内容创作和商业策略是主要应用领域，并识别了内存管理、依赖处理和分词器配置等关键技术挑战。


<details>
  <summary>Details</summary>
Motivation: 基础模型(FMs)正在彻底改变软件工程实践，但FMware的设计、实现和演化带来了新的挑战，特别是在云端和本地平台上的开发流程与传统软件开发存在显著差异。

Method: 通过分析GitHub仓库和主流FMware平台(HuggingFace、GPTStore、Ora、Poe)的数据，从三个维度实证研究：常见应用领域、开发者面临的关键挑战、最耗时的issue类型。

Result: 研究发现FMware主要聚焦于教育、内容创作和商业策略；在GitHub上bug报告和核心功能问题最常见，而代码审查、相似性搜索和提示模板设计最耗时；技术挑战集中在内存管理、依赖处理和分词器配置。

Conclusion: 通过揭示开发者实践和痛点，本研究为改进FMware工具、工作流和社区支持提供了机会，并为FMware开发的未来发展提供了可操作的见解。

Abstract: Foundation Models (FMs), such as OpenAI's GPT, are fundamentally transforming
the practice of software engineering by enabling the development of
\emph{FMware} -- applications and infrastructures built around these models.
FMware systems now support tasks such as code generation, natural-language
interaction, knowledge integration, and multi-modal content creation,
underscoring their disruptive impact on current software engineering workflows.
However, the design, implementation, and evolution of FMware present
significant new challenges, particularly across cloud-based and on-premise
platforms where goals, processes, and tools often diverge from those of
traditional software development.
  To our knowledge, this is the first large-scale analysis of FMware
development across both cloud-based platforms and open-source repositories. We
empirically investigate the FMware ecosystem through three focus areas: (1) the
most common application domains of FMware, (2) the key challenges developers
encounter, and (3) the types of issues that demand the greatest effort to
resolve. Our analysis draws on data from GitHub repositories and from leading
FMware platforms, including HuggingFace, GPTStore, Ora, and Poe. Our findings
reveal a strong focus on education, content creation, and business strategy,
alongside persistent technical challenges in memory management, dependency
handling, and tokenizer configuration. On GitHub, bug reports and core
functionality issues are the most frequently reported problems, while code
review, similarity search, and prompt template design are the most
time-consuming to resolve.
  By uncovering developer practices and pain points, this study points to
opportunities to improve FMware tools, workflows, and community support, and
provides actionable insights to help guide the future of FMware development.

</details>


### [99] [CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding Interactions with LLMs](https://arxiv.org/abs/2510.11536)
*Manaal Basha,Aimeê M. Ribeiro,Jeena Javahar,Cleidson R. B. de Souza,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: CodeWatcher是一个轻量级、非侵入式的客户端-服务器系统，用于在VS Code编辑器中捕获开发者与代码生成工具的细粒度交互事件。


<details>
  <summary>Details</summary>
Motivation: 理解开发者如何与代码生成工具交互需要详细的实时编程行为数据，但收集这些数据往往会影响工作流程。

Method: 系统包含VS Code插件、Python RESTful API和MongoDB后端，通过容器化实现可扩展部署，记录插入、删除、复制粘贴和焦点切换等语义化事件。

Result: CodeWatcher能够在不修改用户工作流程的情况下持续监控开发者活动，支持编程会话的事后重建和丰富的行为分析。

Conclusion: 该基础设施对于支持负责任AI、开发者生产力以及代码生成工具的人本评估研究至关重要。

Abstract: Understanding how developers interact with code generation tools (CGTs)
requires detailed, real-time data on programming behavior which is often
difficult to collect without disrupting workflow. We present
\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed
to capture fine-grained interaction events from within the Visual Studio Code
(VS Code) editor. \textit{CodeWatcher} logs semantically meaningful events such
as insertions made by CGTs, deletions, copy-paste actions, and focus shifts,
enabling continuous monitoring of developer activity without modifying user
workflows. The system comprises a VS Code plugin, a Python-based RESTful API,
and a MongoDB backend, all containerized for scalability and ease of
deployment. By structuring and timestamping each event, \textit{CodeWatcher}
enables post-hoc reconstruction of coding sessions and facilitates rich
behavioral analyses, including how and when CGTs are used during development.
This infrastructure is crucial for supporting research on responsible AI,
developer productivity, and the human-centered evaluation of CGTs. Please find
the demo, diagrams, and tool here: https://osf.io/j2kru/overview.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [100] [Amazon takes shots at ChatGPT with Quick Suite - your new AI 'teammate' at work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zdnet.com%2Farticle%2Famazon-takes-shots-at-chatgpt-with-quick-suite-your-new-ai-teammate-at-work%2F%3Futm_source=tldrai/1/01000199ce43810a-c0673c9c-b988-438f-ab8a-51949ed65897-000000/ZAzHP1RU0dKsZLI1KF8SxcJ1x1g3xqTwbIG7lotPqAc=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 亚马逊推出Quick Suite作为企业级AI助手，通过对话语言帮助用户查找信息，整合多种数据源，并提供数据保护功能


<details>
  <summary>Details</summary>
Motivation: 应对ChatGPT等AI工具的竞争，为企业提供集成化的AI助手解决方案

Method: 开发基于对话语言的AI体验，整合文件、企业系统、数据库和网络等多种数据源

Result: 推出Quick Suite作为企业AI助手，提供30天免费试用

Conclusion: 亚马逊通过Quick Suite进军企业AI助手市场，提供数据安全的AI协作工具

Abstract: Amazon takes shots at ChatGPT with Quick Suite - your new AI 'teammate' at work (5 minute read) Amazon's Quick Suite is an agentic AI experience for enterprises that uses conversational language to find what users need. It acts as a hub that pulls data from various sources like files, enterprise systems, databases, and the web. Users can use Quick Suite to discuss questions, build personalized agents, and complete tasks while leveraging data protections. AWS offers a free 30-day trial for the...

</details>


### [101] [Not Another Workflow Builder](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.langchain.com%2Fnot-another-workflow-builder%2F%3Futm_source=tldrdata/1/01000199dd099727-2acea3cb-7de2-40fc-ac24-968b68de7f4c-000000/P3RNowFB6BWLUH0BPVulG4bYgyrrBelB-_nBqJ-7YM0=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 视觉工作流构建器限制AI开发的可扩展性，未来将由无代码代理处理简单任务，代码工作流处理复杂任务


<details>
  <summary>Details</summary>
Motivation: 解决视觉工作流构建器在AI开发中的局限性，它们对非技术用户过于复杂，对高级用例又过于有限

Method: 提出结合无代码代理和基于代码工作流的混合方法，利用AI生成代码来管理复杂任务

Result: 实现了灵活性、可维护性和企业实用性的平衡

Conclusion: LangChain认为未来AI开发将是无代码代理与代码工作流的结合，为不同复杂度的任务提供合适解决方案

Abstract: Not Another Workflow Builder (4 minute read) Visual workflow builders limit scalable AI development. They are too complex for nontechnical users and too limited for advanced use cases. LangChain sees a future where no-code agents handle simple tasks, and code-based workflows with AI-generated code manage complex ones, combining flexibility, maintainability, and practical usability for enterprises.

</details>


### [102] [Vibing a Non-Trivial Ghostty Feature](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmitchellh.com%2Fwriting%2Fnon-trivial-vibing%3Futm_source=tldrnewsletter/1/01000199dd18ae8c-49272999-75d1-4acb-9e77-1d78816ba86a-000000/n4W32Mv34Hmz7YC4TCEAEYQBMwWuPN5zrwDqkGx3cfM=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文分享了使用AI开发Ghostty非侵入式更新通知功能的过程，强调AI作为助手而非替代者的角色


<details>
  <summary>Details</summary>
Motivation: 展示AI在软件开发中的实际应用，探讨AI作为开发助手的有效使用方式

Method: 通过代理式编码会话开发功能，记录开发过程和决策思路

Result: 成功开发并发布了Ghostty的非侵入式更新通知功能

Conclusion: AI在开发中能完成大量工作，但开发者仍需适时介入干预

Abstract: Vibing a Non-Trivial Ghostty Feature (20 minute read) Ghotty's unobtrusive update notification feature was largely developed with AI. This post shares the agentic coding sessions that led to shipping the feature. It provides context about the process and reasoning. AI is best used as an assistant and not a replacement. While AI can do a lot of the work, developers will almost always still need to intervene at times.

</details>


### [103] [Superpowers: How I'm using coding agents in October 2025](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.fsck.com%2F2025%2F10%2F09%2Fsuperpowers%2F%3Futm_source=tldrnewsletter/1/01000199dd18ae8c-49272999-75d1-4acb-9e77-1d78816ba86a-000000/RX9dNKEBDG4bnBOEYBJYNInJQLFtkfYgNEUeZWzmB0M=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic为Claude Code推出了新的插件系统，赋予编码代理新的技能来完成各种任务。


<details>
  <summary>Details</summary>
Motivation: 提升编码代理的能力，使其能够更有效地完成复杂的编程任务，通过插件系统扩展其功能范围。

Method: 开发并集成插件系统到Claude Code中，为编码代理提供新的技能和工具。

Result: 编码代理现在能够利用插件系统获得增强的功能，更高效地完成编程任务。

Conclusion: 插件系统显著提升了编码代理的能力，使其在2025年10月时具备更强的任务完成能力。

Abstract: Superpowers: How I'm using coding agents in October 2025 (13 minute read) Anthropic's new plugin system for Claude Code gives the agent new skills it can use to complete tasks.

</details>


### [104] [Making Documentation Simpler and Practical: Our Docs-as-Code Journey](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.squarespace.com%2Fblog%2F2025%2Fmaking-documentation-simpler-and-practical-our-docs-as-code-journey%3Futm_source=tldrdevops/1/01000199dd3fcf75-25975a05-2c99-48df-814a-06181712edc3-000000/1bex5eAAHVzeqcOzWKI4s2te6KnHo6csCOR2-iJ3xb4=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Squarespace Domains团队采用Docs-as-Code方法，将文档与代码集成在Git中，使用Markdown、Mermaid语法和Backstage开发者门户，简化文档流程并提高可追溯性。


<details>
  <summary>Details</summary>
Motivation: 解决传统文档管理中的版本控制、协作和迭代效率问题，使文档开发与软件开发流程保持一致。

Method: 采用Docs-as-Code哲学，将文档存储在Git仓库中，使用Markdown和Mermaid语法编写，通过Backstage作为开发者门户，支持本地预览和拉取请求审查。

Result: 文档流程得到简化，可追溯性提高，技术文档迭代速度加快，团队协作效率提升。

Conclusion: Docs-as-Code方法有效改善了文档管理实践，使文档开发更加高效和可维护。

Abstract: Making Documentation Simpler and Practical: Our Docs-as-Code Journey (6 minute read) The Squarespace Domains engineering team has embraced a "Docs-as-Code" (DaC) philosophy, integrating documentation with code in Git for version control and pull request reviews. By using Markdown, Mermaid syntax, and Backstage as a developer portal, the team has streamlined documentation, improved traceability, and enabled faster iteration on technical documents. The team also uses local previews with the Int...

</details>


### [105] [Add MCP Servers to Claude Code with MCP Toolkit](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fadd-mcp-servers-to-claude-code-with-mcp-toolkit%2F%3Futm_source=tldrdevops/1/01000199dd3fcf75-25975a05-2c99-48df-814a-06181712edc3-000000/3Jwl0mX8m3TRNaLYsw7ppLS9j2QFbtjZsjUQiKIoOwE=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Docker MCP Toolkit 连接 Claude Code 与开发工具，实现数据库查询、GitHub问题创建和Slack消息发送，无需手动复制粘贴和上下文切换。


<details>
  <summary>Details</summary>
Motivation: 减少开发者在不同工具间手动切换和复制粘贴的操作，提高开发效率。

Method: 提供200多个预构建的容器化MCP服务器，支持一键部署和自动凭证处理，连接Claude Code到可信环境。

Result: 实现了Claude Code与开发工具的无缝集成，简化了开发工作流程。

Conclusion: MCP Toolkit显著提升了开发效率，通过自动化工具集成减少了手动操作。

Abstract: Add MCP Servers to Claude Code with MCP Toolkit (6 minute read) Docker MCP Toolkit now connects Claude Code to real development tools, enabling it to run database queries, create GitHub issues, and send Slack messages, eliminating the need for manual copy-pasting and context-switching. The toolkit features over 200 pre-built containerized MCP servers, one-click deployment in Docker Desktop, and automatic credential handling, allowing developers to connect Claude Code to trusted environments i...

</details>


### [106] [Vibing a Non-Trivial Ghostty Feature](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmitchellh.com%2Fwriting%2Fnon-trivial-vibing%3Futm_source=tldrwebdev/1/01000199dd433eeb-8e2c150a-a9b8-4fa5-afd8-609d2658149b-000000/SOHdI_vnJTFUFXuaGajMr-b334w3fq9uZVQKn7hl348=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文记录了使用AI编码工具为终端应用Ghostty开发macOS自动更新功能的完整过程，包括UI原型设计、后端实现和代码清理等迭代环节。


<details>
  <summary>Details</summary>
Motivation: 探索AI编码工具在实际软件开发项目中的应用效果，特别是在复杂功能开发过程中的作用和局限性。

Method: 采用迭代式AI会话与手动编码相结合的方法，分别处理UI原型设计、后端实现和代码清理等不同开发阶段。

Result: AI在UI迭代和后台代码处理方面最为有用，总token成本为15.98美元，估计耗时8小时完成功能开发。

Conclusion: AI编码工具在特定开发环节（如UI迭代）中能显著提高效率，但仍需与手动编码和问题解决相结合才能完成复杂功能开发。

Abstract: Vibing a Non-Trivial Ghostty Feature (16 minute read) This is a “day-in-the-life” of creating a new macOS automatic update feature for Ghostty, a terminal application, through AI coding tools. The process involved iterative AI sessions for UI prototyping, backend implementation, and code cleanup, interspersed with a lot of manual coding and problem-solving. AI was the most useful for UI iteration and as a background code processor. The total token cost was $15.98 with an estimated 8 hours of ...

</details>


### [107] [React Compiler v1.0](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Freact.dev%2Fblog%2F2025%2F10%2F07%2Freact-compiler-1%3Futm_source=tldrwebdev/1/01000199dd433eeb-8e2c150a-a9b8-4fa5-afd8-609d2658149b-000000/DitPrPlJa1_skutfy82_t8eAfDepVFOpt9-7SVgUYdc=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: React Compiler v1.0是一个构建时工具，可自动优化React和React Native组件，通过记忆化技术无需重写代码


<details>
  <summary>Details</summary>
Motivation: 解决React应用中手动优化组件的复杂性和容易出错的问题，提供自动化的性能优化方案

Method: 使用构建时编译器技术，自动分析React组件并进行记忆化优化，无需开发者修改源代码

Result: 发布了React Compiler v1.0版本，能够有效优化React和React Native应用的性能

Conclusion: React Compiler v1.0为React生态系统提供了自动化的性能优化工具，简化了开发者的优化工作

Abstract: React Compiler v1.0 (8 minute read) React Compiler v1.0 has been released. It's a build-time tool that automatically optimizes React and React Native components through memoization without code rewrites.

</details>


### [108] [After nine years of grinding, Replit finally found its market. Can it keep it?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F10%2F02%2Fafter-nine-years-of-grinding-replit-finally-found-its-market-can-it-keep-it%2F%3Futm_source=tldrwebdev/1/01000199dd433eeb-8e2c150a-a9b8-4fa5-afd8-609d2658149b-000000/lAyykSrMuZLDlg_aebabxrKVfiskH29_ikDweoYzRH0=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Replit经过9年缓慢发展后，通过面向非技术用户的AI编程代理找到了市场成功，收入大幅增长并获得25亿美元融资和30亿美元估值


<details>
  <summary>Details</summary>
Motivation: Replit在经历多年缓慢增长和多个失败商业模式后，需要找到可持续的市场定位和增长路径

Method: 通过AI驱动的编程代理面向非技术用户，提供更易用的编程平台

Result: 公司收入大幅增长，获得2.5亿美元融资，估值达到30亿美元

Conclusion: Replit通过AI编程代理成功开拓了非技术用户市场，找到了可持续的业务模式

Abstract: After nine years of grinding, Replit finally found its market. Can it keep it? (9 minute read) After nine years of slow growth and multiple failed business models, Replit, a coding platform, has finally found success by targeting non-technical users with its AI-powered coding agent. The company's revenue has skyrocketed, leading to a $250 million funding round and a $3 billion valuation.

</details>


### [109] [ERC-8004: Trustless Agents v1 Spec Launches](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1976257886390002116.html%3Futm_source=tldrcrypto/1/01000199dd76b261-0997ef02-8aa1-4dc2-b677-4c67a55e5e0e-000000/lP6IRRnLEgeZFwJHdPjtNrC_LDwbuc8Y1GKX5TbM9Ds=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ERC-8004 v1是一个允许AI代理无需中央中介即可相互发现和信任的标准，通过将代理表示为NFT来实现


<details>
  <summary>Details</summary>
Motivation: 解决AI代理之间缺乏去中心化信任和发现机制的问题，使代理能够在无需中央控制的情况下相互识别和交互

Method: 将AI代理表示为ERC-721 NFT，包含注册文件列出名称、技能、能力和端点，利用现有NFT基础设施实现可移植性

Result: 创建了一个逻辑上集中但无需许可的发现系统，AI代理可以在任何代理浏览器中实现可移植性

Conclusion: ERC-8004为AI代理提供了去中心化的信任和发现框架，为更广泛的代理生态系统奠定了基础

Abstract: ERC-8004: Trustless Agents v1 Spec Launches (1 minute read) ERC-8004 v1 is a standard that allows AI agents to discover and trust each other without central intermediaries by representing agents as NFTs that can be minted, transferred, and managed using existing ERC-721 infrastructure while pointing to registration files listing names, skills, capabilities, and endpoints for portability across any agent explorer. The standard provides logically centralized permissionless discovery with onchai...

</details>
