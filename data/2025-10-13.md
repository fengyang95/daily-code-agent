<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 27]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.SE](#cs.SE) [Total: 15]
- [tldr.article](#tldr.article) [Total: 9]
- [cs.LG](#cs.LG) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2510.08592)
*Shahriar Kabir Nahin,Hadi Askari,Muhao Chen,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 本文揭示了测试时缩放(TTS)技术中一个未被识别的失败模式：当候选响应多样性受限时，即使只是适度减少，TTS也更可能产生不安全输出。


<details>
  <summary>Details</summary>
Motivation: TTS技术假设候选池的多样性能够提高可靠性，但作者发现这种假设存在缺陷，需要研究TTS在多样性受限时的安全性问题。

Method: 提出了RefDiv参考引导多样性减少协议作为诊断攻击方法，通过约束候选响应的多样性来压力测试TTS管道，在四个开源模型和两种TTS策略上进行实验。

Result: 多样性受限显著增加了TTS产生不安全输出的概率，这种效应通常比高对抗性意图提示更强，且在不同TTS策略和闭源模型中都存在。现有安全防护分类器无法检测RefDiv生成的对抗性输入提示。

Conclusion: TTS存在基于多样性的固有安全漏洞，需要设计既能保持有效性又能抵御多样性针对性攻击的鲁棒TTS策略。

Abstract: Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple
candidate responses and then operating over this set to find the best output. A
tacit premise behind TTS is that sufficiently diverse candidate pools enhance
reliability. In this work, we show that this assumption in TTS introduces a
previously unrecognized failure mode. When candidate diversity is curtailed,
even by a modest amount, TTS becomes much more likely to produce unsafe
outputs. We present a reference-guided diversity reduction protocol (RefDiv)
that serves as a diagnostic attack to stress test TTS pipelines. Through
extensive experiments across four open-source models (Qwen3, Mistral, Llama3.1,
Gemma3) and two widely used TTS strategies (Monte Carlo Tree Search and
Best-of-N), constraining diversity consistently signifies the rate at which TTS
produces unsafe results. The effect is often stronger than that produced by
prompts directly with high adversarial intent scores. This observed phenomenon
also transfers across TTS strategies and to closed-source models (e.g. OpenAI
o3 and Gemini-2.5-Pro), thus indicating that this is a general and extant
property of TTS rather than a model-specific artifact. Additionally, we find
that numerous widely used safety guardrail classifiers (e.g. Llama-Guard and
OpenAI Moderation API), are unable to flag the adversarial input prompts
generated by RefDiv, demonstrating that existing defenses offer limited
protection against this diversity-driven failure mode. Through this work, we
hope to motivate future research on designing robust TTS strategies that are
both effective and secure against diversity-targeted stress tests as
illustrated by RefDiv.

</details>


### [2] [From Simulation to Strategy: Automating Personalized Interaction Planning for Conversational Agents](https://arxiv.org/abs/2510.08621)
*Wen-Yu Chang,Tzu-Hung Huang,Chih-Ho Chen,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 提出了一种基于用户职业信息的轻量级对话策略，通过优先考虑与用户偏好一致的对话意图，实现了更短且更成功的销售对话。


<details>
  <summary>Details</summary>
Motivation: 随着智能对话模型的快速发展，需要真实用户模拟研究来优化有效的对话策略。本研究旨在探索基于用户画像（年龄、性别、职业）的销售导向对话代理。

Method: 研究销售导向代理根据用户年龄、性别和职业调整对话策略。引入轻量级职业条件策略，引导代理优先考虑与用户偏好一致的对话意图。

Result: 年龄和性别影响整体性能，但职业在对话意图上产生最显著差异。使用职业条件策略后，对话更短且成功率更高。

Conclusion: 研究强调了丰富模拟器配置文件的重要性，并展示了简单基于人物画像的策略可以提升销售导向对话系统的有效性。

Abstract: Amid the rapid rise of agentic dialogue models, realistic user-simulator
studies are essential for tuning effective conversation strategies. This work
investigates a sales-oriented agent that adapts its dialogue based on user
profiles spanning age, gender, and occupation. While age and gender influence
overall performance, occupation produces the most pronounced differences in
conversational intent. Leveraging this insight, we introduce a lightweight,
occupation-conditioned strategy that guides the agent to prioritize intents
aligned with user preferences, resulting in shorter and more successful
dialogues. Our findings highlight the importance of rich simulator profiles and
demonstrate how simple persona-informed strategies can enhance the
effectiveness of sales-oriented dialogue systems.

</details>


### [3] [Upfront Chain-of-Thought: A Cooperative Framework for Chain-of-Thought Compression](https://arxiv.org/abs/2510.08647)
*Chengzhengxu Li,Xiaoming Liu,Zhaohan Zhang,Shaochu Zhang,Shengchao Liu,Guoxin Ma,Yu Lan,Chao Shen*

Main category: cs.CL

TL;DR: 提出了UCoT框架，通过前期思想嵌入实现CoT压缩，在保持强大推理能力的同时显著减少推理长度，在GSM8K数据集上减少50%token使用且性能提升3.08%。


<details>
  <summary>Details</summary>
Motivation: 长链式思维推理计算成本高、延迟大，现有CoT压缩方法要么需要繁琐的离散提示设计，要么牺牲关键推理细节。

Method: UCoT是一个小模型(压缩器)和大模型(执行器)协作的工作流：第一阶段训练压缩器生成富含推理信息的前期思想嵌入；第二阶段优化执行器利用这些嵌入得出正确答案。

Result: 在GSM8K数据集上，UCoT将token使用减少50%，性能比SOTA方法提高3.08%。

Conclusion: UCoT框架能有效压缩推理过程，在保持强大推理能力的同时显著提高效率。

Abstract: Recent developments have enabled advanced reasoning in Large Language Models
(LLMs) via long Chain-of-Thought (CoT), while long CoT suffers from high
computational costs and significant latency losses owing to the autoregressive
nature of generative LLMs. CoT compression aims to improve efficiency in the
reasoning process by reducing output length. Previous works trade reasoning
efficiency by either laborious discrete prompt designing or the construction of
external compressed CoT datasets that sacrifice key reasoning details. In this
work, we propose Upfront CoT (UCoT): an efficient reasoning framework with
upfront thought embedding to automate CoT compression. UCoT is a cooperative
workflow involving a small model (compressor) and a large model (executor). The
first stage of UCoT trains compressor to generate upfront thought embeddings
rich in reasoning information for the executor, avoiding the drawbacks of
manually designed prompts. The second stage optimizes executor to utilize
upfront thought embeddings to derive the correct answer with short reasoning,
using a reward mechanism. Extensive experiments show that UCoT maintains the
powerful reasoning ability of executor while significantly reducing the length
of CoT. It is worth mentioning that when applying UCoT to the
Qwen2.5-7B-Instruct model, the usage of tokens on GSM8K dataset is reduced by
50\%, while the performance is 3.08\% higher than that of the state-of-the-art
(SOTA) method. The code and dataset are in supplementary material.

</details>


### [4] [Scaling Laws for Code: A More Data-Hungry Regime](https://arxiv.org/abs/2510.08702)
*Xianzhen Luo,Wenzhen Zheng,Qingfu Zhu,Rongyi Zhang,Houyi Li,Siming Huang,YuanTao Fan,Wanxiang Che*

Main category: cs.CL

TL;DR: 该论文首次对代码的缩放定律进行了大规模实证研究，发现代码LLM在模型大小上有效扩展，但代码比自然语言需要更高的数据参数比。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律主要基于自然语言分析，但代码与自然语言存在根本差异（如严格语法），需要验证这些定律是否适用于代码。

Method: 进行了117次实验运行，模型大小从0.2B到3.8B，训练token从2B到128B，拟合了Chinchilla定律和Farsser定律。

Result: 1) Farsser定律更准确；2) 代码LLM随模型大小有效扩展；3) 代码需要比自然语言更高的数据参数比；4) 代码-自然语言混合实验中，自然语言在资源受限时有益，但在高计算预算时成为不利因素。

Conclusion: 代码缩放定律与自然语言不同，需要更高的数据参数比，为高效训练代码LLM提供了指导。

Abstract: Code Large Language Models (LLMs) are revolutionizing software engineering.
However, scaling laws that guide the efficient training are predominantly
analyzed on Natural Language (NL). Given the fundamental differences like
strict syntax between code and NL, it is unclear whether these laws are
directly applicable to code. To address this gap, we conduct the first
large-scale empirical study of scaling laws for code, comprising 117
experimental runs with model sizes from 0.2B to 3.8B and training tokens from
2B to 128B. We fit the Chinchilla law and the Farsser law. First, the results
show that the more expressive Farseer law offers greater accuracy. Second, the
analysis reveals that Code LLMs scale effectively with model size. Crucially,
code represents a more data-hungry regime, requiring a substantially higher
data-to-parameter ratio than NL. Finally, two additional sets of experiments on
code-NL mixtures show that NL benefits resource-constrained scenarios, but
becomes a detriment at higher compute budgets.

</details>


### [5] [How Many Code and Test Cases Are Enough? Evaluating Test Cases Generation from a Binary-Matrix Perspective](https://arxiv.org/abs/2510.08720)
*Xianzhen Luo,Jinyang Huang,Wenzhen Zheng,Qingfu Zhu,Mingzheng Xu,Yiheng Xu,Yuantao Fan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: 提出了TC-Bench基准测试框架，通过寻找二进制代码-测试矩阵的最优诊断基础，构建紧凑、多样化且抗分数膨胀的测试用例生成评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在计算成本高、分数膨胀以及对简单错误偏见的问题，需要更有效的测试用例生成评估方法。

Method: 引入框架将基准构建形式化为在二进制代码-测试矩阵中寻找最优诊断基础，提出WrongSelect算法选择最大多样性的错误代码。

Result: 构建的TC-Bench基准显示，即使最先进的测试用例生成方法也只能达到约60%的排除率，暴露了其诊断能力的显著差距。

Conclusion: 该框架能够构建紧凑且多样化的基准，有效评估LLM生成的测试用例质量，揭示了现有方法的局限性。

Abstract: Evaluating test cases automatically generated by Large Language Models (LLMs)
is a critical yet challenging task. Existing benchmarks suffer from high
computational costs, score inflation, and a bias towards trivial bugs over
rare, critical faults. In this work, we ask two fundamental questions: (1) What
is the minimal set of wrong codes sufficient to represent the entire error
space? and (2) What is the minimal set of test cases needed to distinguish
them? We introduce a framework that formalizes benchmark construction as
finding an optimal diagnostic basis in a binary code-test matrix. The rank of
this matrix specifies the minimal number of independent error patterns (wrong
codes) and provides a tight upper bound on the number of test cases required
for complete fault coverage. Our objective is to identify a basis of size equal
to the matrix rank that maximizes internal diversity. To tackle this NP-hard
problem, we propose WrongSelect, an efficient approximation algorithm to select
maximally diverse wrong codes. Applying this framework to millions of
competitive programming submissions, we construct TC-Bench, a compact, diverse,
and inflation-resistant benchmark. Extensive experiments show that even the
most advanced test case generation methods achieve only ~60% exclusion rates on
TC-Bench, exposing a significant gap in their diagnostic power. Our dataset is
available at: https://huggingface.co/datasets/Luoberta/TC-Bench and our code is
at: https://github.com/Luowaterbi/TC-Bench.

</details>


### [6] [Measuring Moral LLM Responses in Multilingual Capacities](https://arxiv.org/abs/2510.08776)
*Kimaya Basu,Savi Kolari,Allison Yu*

Main category: cs.CL

TL;DR: 评估前沿和领先开源模型在低资源和高资源语言中的五个维度表现，发现GPT-5在准确性和一致性方面表现最佳，而其他模型在不同语言和类别间存在较大不一致性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在全球多语言环境中的广泛应用，需要理解和防护其多语言响应，特别是在不同语言资源条件下的表现差异。

Method: 使用五点评分标准和法官LLM，评估模型在五个维度上的多语言响应准确性和一致性。

Result: GPT-5在所有类别中平均表现最佳，特别是在同意与自主权（3.56）和伤害预防与安全（4.73）类别得分最高；Gemini 2.5 Pro表现最差，相应得分为1.39和1.98。

Conclusion: 语言变化对LLM在不同类别中的响应有显著影响，需要进一步测试和改进这些领域。

Abstract: With LLM usage becoming widespread across countries, languages, and humanity
more broadly, the need to understand and guardrail their multilingual responses
increases. Large-scale datasets for testing and benchmarking have been created
to evaluate and facilitate LLM responses across multiple dimensions. In this
study, we evaluate the responses of frontier and leading open-source models in
five dimensions across low and high-resource languages to measure LLM accuracy
and consistency across multilingual contexts. We evaluate the responses using a
five-point grading rubric and a judge LLM. Our study shows that GPT-5 performed
the best on average in each category, while other models displayed more
inconsistency across language and category. Most notably, in the Consent &
Autonomy and Harm Prevention & Safety categories, GPT scored the highest with
averages of 3.56 and 4.73, while Gemini 2.5 Pro scored the lowest with averages
of 1.39 and 1.98, respectively. These findings emphasize the need for further
testing on how linguistic shifts impact LLM responses across various categories
and improvement in these areas.

</details>


### [7] [MOSAIC: Multi-agent Orchestration for Task-Intelligent Scientific Coding](https://arxiv.org/abs/2510.08804)
*Siddeshwar Raghavan,Tanwi Mallick*

Main category: cs.CL

TL;DR: MOSAIC是一个用于解决科学编码任务的多智能体LLM框架，通过学生-教师范式和整合上下文窗口来提升准确性、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 科学工作流需要严谨的算法、深度领域知识和特定推理，而现有方法在处理链式子问题和无I/O测试用例时存在局限性。

Method: 采用无训练的多智能体框架，包含自反思、创建原理、编码和调试的专门智能体，使用学生-教师范式和整合上下文窗口。

Result: 在科学编码基准测试中，MOSAIC在准确性、鲁棒性和可解释性方面优于现有方法。

Conclusion: MOSAIC通过专门设计的智能体框架有效解决了科学代码生成的挑战，特别适用于涉及链式子问题的复杂任务。

Abstract: We present MOSAIC, a multi-agent Large Language Model (LLM) framework for
solving challenging scientific coding tasks. Unlike general-purpose coding,
scientific workflows require algorithms that are rigorous, interconnected with
deep domain knowledge, and incorporate domain-specific reasoning, as well as
algorithm iteration without requiring I/O test cases. Many scientific problems
also require a sequence of subproblems to be solved, leading to the final
desired result. MOSAIC is designed as a training-free framework with specially
designed agents to self-reflect, create the rationale, code, and debug within a
student-teacher paradigm to address the challenges of scientific code
generation. This design facilitates stepwise problem decomposition, targeted
error correction, and, when combined with our Consolidated Context Window
(CCW), mitigates LLM hallucinations when solving complex scientific tasks
involving chained subproblems. We evaluate MOSAIC on scientific coding
benchmarks and demonstrate that our specialized agentic framework outperforms
existing approaches in terms of accuracy, robustness, and interpretability.

</details>


### [8] [Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models](https://arxiv.org/abs/2510.08859)
*Ragib Amin Nihal,Rui Wen,Kazuhiro Nakadai,Jun Sakuma*

Main category: cs.CL

TL;DR: 提出了PE-CoA框架，通过五种对话模式构建有效的多轮越狱攻击，在12个LLM和10个危害类别上实现最先进性能，揭示了模式特定的漏洞和LLM行为特征。


<details>
  <summary>Details</summary>
Motivation: 现有多轮越狱方法依赖启发式或临时探索策略，对模型底层弱点提供有限洞察，且对话模式与跨危害类别模型漏洞之间的关系理解不足。

Method: 提出PE-CoA框架，包含五种对话模式来通过自然对话构建有效的多轮越狱攻击。

Result: 在12个LLM和10个危害类别上实现最先进性能，发现模型表现出不同的弱点分布，对一种对话模式的鲁棒性不能推广到其他模式，且模型家族共享相似的失败模式。

Conclusion: 这些发现突出了安全训练的局限性，表明需要模式感知的防御方法。

Abstract: Large language models (LLMs) remain vulnerable to multi-turn jailbreaking
attacks that exploit conversational context to bypass safety constraints
gradually. These attacks target different harm categories (like malware
generation, harassment, or fraud) through distinct conversational approaches
(educational discussions, personal experiences, hypothetical scenarios).
Existing multi-turn jailbreaking methods often rely on heuristic or ad hoc
exploration strategies, providing limited insight into underlying model
weaknesses. The relationship between conversation patterns and model
vulnerabilities across harm categories remains poorly understood. We propose
Pattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation
patterns to construct effective multi-turn jailbreaks through natural dialogue.
Evaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve
state-of-the-art performance, uncovering pattern-specific vulnerabilities and
LLM behavioral characteristics: models exhibit distinct weakness profiles where
robustness to one conversational pattern does not generalize to others, and
model families share similar failure modes. These findings highlight
limitations of safety training and indicate the need for pattern-aware
defenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA

</details>


### [9] [Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR](https://arxiv.org/abs/2510.08892)
*Haomin Zhuang,Yujun Zhou,Taicheng Guo,Yue Huang,Fangxu Liu,Kai Song,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 提出一种多温度强化学习方法，在推理过程中对不同类型的token应用不同温度设置：对推理token使用高温鼓励探索，对知识token使用低温保持事实准确性


<details>
  <summary>Details</summary>
Motivation: 现有方法通常限制更新来间接鼓励探索，但没有在token生成阶段明确促进探索行为。研究发现LLM中的token在推理任务中扮演不同角色，可分为高熵推理token和低熵知识token

Method: 在采样过程中应用不同的温度设置：对推理token使用较高温度以积极鼓励探索，对知识token使用较低温度以保持事实正确性。系统研究多种多温度调度策略及其在强化学习环境中的影响

Result: 在多个推理基准测试上的实证评估表明，该方法显著提升了LLMs的推理性能

Conclusion: 通过为不同类型的token应用不同的温度设置，明确促进推理过程中的探索行为，可以有效提升LLMs的推理能力

Abstract: Reinforcement Learning has demonstrated substantial improvements in the
reasoning abilities of Large Language Models (LLMs), exhibiting significant
applicability across various domains. Recent research has identified that
tokens within LLMs play distinct roles during reasoning tasks, categorizing
them into high-entropy reasoning tokens and low-entropy knowledge tokens. Prior
approaches have typically focused on restricting updates to indirectly
encourage exploration, yet they do not explicitly facilitate exploratory
behavior during the token generation stage itself. In this work, we introduce a
complementary approach that explicitly promotes exploration during sampling by
applying distinct temperature settings for different token types. Specifically,
our method employs higher temperatures for reasoning tokens to actively
encourage exploration, while retaining lower temperatures for knowledge tokens
to maintain factual correctness. Furthermore, we systematically investigate
various multi-temperature scheduling strategies and their impacts within
reinforcement learning contexts. Empirical evaluations on several reasoning
benchmarks demonstrate that our approach significantly enhances the reasoning
performance of LLMs. The code is available at
https://github.com/zhmzm/Multi_Temperature_Verl.git.

</details>


### [10] [Artificial Impressions: Evaluating Large Language Model Behavior Through the Lens of Trait Impressions](https://arxiv.org/abs/2510.08915)
*Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: 该论文研究了LLM内部表示中类似人类印象和刻板印象的人工印象模式，使用线性探针预测刻板印象内容模型的两个维度，并探讨了这些印象与下游模型行为的关系。


<details>
  <summary>Details</summary>
Motivation: 研究LLM内部表示中是否存在类似人类印象和刻板印象的模式，以及这些模式如何影响模型的行为和响应质量。

Method: 使用线性探针在生成的提示上预测刻板印象内容模型的两个维度，分析印象与下游模型行为的关系，以及提示特征对印象的影响。

Result: 发现LLM在提示时不一致地报告印象，但从隐藏表示中线性解码印象更一致；人工印象能预测模型响应的质量和使用对冲的程度；特定内容、风格和方言特征会影响LLM印象。

Conclusion: LLM内部存在可解码的人工印象模式，这些模式与模型行为相关，提示特征会影响印象形成。

Abstract: We introduce and study artificial impressions--patterns in LLMs' internal
representations of prompts that resemble human impressions and stereotypes
based on language. We fit linear probes on generated prompts to predict
impressions according to the two-dimensional Stereotype Content Model (SCM).
Using these probes, we study the relationship between impressions and
downstream model behavior as well as prompt features that may inform such
impressions. We find that LLMs inconsistently report impressions when prompted,
but also that impressions are more consistently linearly decodable from their
hidden representations. Additionally, we show that artificial impressions of
prompts are predictive of the quality and use of hedging in model responses. We
also investigate how particular content, stylistic, and dialectal features in
prompts impact LLM impressions.

</details>


### [11] [SOP-Maze: Evaluating Large Language Models on Complicated Business Standard Operating Procedures](https://arxiv.org/abs/2510.08942)
*Jiaming Wang,Zhe Tang,Yilin Jin,Peng Ding,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: 提出了SOP-Maze基准测试，包含397个基于真实业务SOP的任务，评估LLM在复杂标准操作流程中的表现，发现现有模型在流程遵循、对话处理和计算推理方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估LLM的指令遵循和决策能力，但在涉及复杂标准操作流程(SOP)的业务场景中的评估尚未充分探索。

Method: 从真实业务数据构建SOP-Maze基准，包含397个任务，分为横向根系统(LRS)和核心根系统(HRS)两类，分别测试选择能力和深度推理能力。

Result: 实验表明几乎所有最先进模型在SOP-Maze上都表现不佳，识别出三个主要错误类型：流程盲区、对话脆弱性和计算错误。

Conclusion: 该研究为改进模型在复杂SOP任务中的能力提供了新见解，基准测试已开源。

Abstract: As large language models (LLMs) are widely deployed as domain-specific
agents, many benchmarks have been proposed to evaluate their ability to follow
instructions and make decisions in real-world scenarios. However, business
scenarios often involve complex standard operating procedures (SOPs), and the
evaluation of LLM capabilities in such contexts has not been fully explored. To
bridge this gap, we propose SOP-Maze, a benchmark constructed from real-world
business data and adapted into a collection of 397 tasks from 23 complex SOP
scenarios. We further categorize SOP tasks into two broad classes: Lateral Root
System (LRS), representing wide-option tasks that demand precise selection; and
Heart Root System (HRS), which emphasizes deep logical reasoning with complex
branches. Extensive experiments reveal that nearly all state-of-the-art models
struggle with SOP-Maze. We conduct a comprehensive analysis and identify three
key error categories: (i) route blindness: difficulty following procedures;
(ii) conversational fragility: inability to handle real dialogue nuances; and
(iii) calculation errors: mistakes in time or arithmetic reasoning under
complex contexts. The systematic study explores LLM performance across SOP
tasks that challenge both breadth and depth, offering new insights for
improving model capabilities. We have open-sourced our work on
https://github.com/ADoublLEN/SOP-Maze.

</details>


### [12] [Augmenting Dialog with Think-Aloud Utterances for Modeling Individual Personality Traits by LLM](https://arxiv.org/abs/2510.09158)
*Seiya Ishikura,Hiroaki Yamada,Tatsuya Hiraoka,Hiroaki Yamada,Takenobu Tokunaga*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This study proposes augmenting dialog data with think-aloud utterances (TAUs)
for modeling individual personalities in text chat by LLM. TAU is a
verbalization of a speaker's thought before articulating the utterance. We
expect "persona LLMs" trained with TAU-augmented data can mimic the speaker's
personality trait better. We tested whether the trained persona LLMs obtain the
human personality with respect to Big Five, a framework characterizing human
personality traits from five aspects. The results showed that LLMs trained with
TAU-augmented data more closely align to the speakers' Agreeableness and
Neuroticism of Big Five than those trained with original dialog data. We also
found that the quality of TAU-augmentation impacts persona LLM's performance.

</details>


### [13] [DSPO: Stable and Efficient Policy Optimization for Agentic Search and Reasoning](https://arxiv.org/abs/2510.09255)
*Chenyang Gu,Yewen Pu,Bruce Yang,Xiaofan Li,Huan Gao*

Main category: cs.CL

TL;DR: 提出DSPO算法，通过序列级优化和动态样本过滤增强LLM的外部知识搜索能力，在多个QA基准上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖提示激发模型内在代理能力，要么在复杂交互任务中应用RL时存在性能上限和崩溃问题，未能充分挖掘模型的代理潜力

Method: DSPO（动态过滤序列级策略优化）算法，通过序列级优化和动态样本过滤进行鲁棒代理训练，纯RL训练实现多轮搜索和推理交错

Result: 在多个QA基准上，7B模型比同类工作提升34.1%，在复杂多跳QA如HotpotQA上甚至超过之前工作的14B模型近9%相对提升，训练稳定性优异

Conclusion: DSPO算法能有效增强LLM的外部知识搜索能力，在复杂任务中表现出色且训练稳定

Abstract: Enhancing LLMs with the ability to actively search external knowledge is
crucial for complex and real-world tasks. Current approaches either rely on
prompting to elicit the model's innate agent capabilities, or suffer from
performance ceilings and collapse when applying RL to complex interactive
tasks, leaving their true agentic potential untapped. To address this, we
introduce \textbf{D}ynamic-filter \textbf{S}equence-level \textbf{P}olicy
\textbf{O}ptimization (DSPO), an improved RL algorithm designed for robust
agent training through sequence-level optimization and dynamic sample
filtering. We train our model purely through RL to interleave multi-turn search
and reasoning, obviating the need for supervised demonstration data. Across
multiple QA benchmarks, our DSPO-trained 7B model improves over a comparable
previous work by \textbf{34.1\%}, and even outperforms the 14B model from
previous work in complex multihop QA such as HotpotQA by nearly \textbf{9\%
relative}, maintaining exceptional training stability.

</details>


### [14] [Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models](https://arxiv.org/abs/2510.09259)
*Yongding Tao,Tian Wang,Yihong Dong,Huanyu Liu,Kechi Zhang,Xiaolong Hu,Ge Li*

Main category: cs.CL

TL;DR: 提出了Self-Critique方法，首次系统研究RL后训练阶段的数据污染检测问题，通过检测模型输出熵分布崩溃来识别污染数据。


<details>
  <summary>Details</summary>
Motivation: RL后训练阶段缺乏专门的数据污染检测方法，而该阶段对提升LLM推理能力至关重要，现有方法无法有效检测RL阶段的污染。

Method: 基于模型输出熵分布崩溃的观察，提出Self-Critique方法，通过探测底层策略崩溃（模型收敛到狭窄推理路径）来检测污染。

Result: 在RL-MIA基准测试中，Self-Critique显著优于基线方法，AUC提升高达30%，而现有方法接近随机猜测。

Conclusion: Self-Critique使得RL阶段污染检测成为可能，填补了该领域的重要空白。

Abstract: Data contamination poses a significant threat to the reliable evaluation of
Large Language Models (LLMs). This issue arises when benchmark samples may
inadvertently appear in training sets, compromising the validity of reported
performance. While detection methods have been developed for the pre-training
and Supervised Fine-Tuning stages, a critical research gap exists for the
increasingly significant phase of Reinforcement Learning (RL) post-training. As
RL post-training becomes pivotal for advancing LLM reasoning, the absence of
specialized contamination detection methods in this paradigm presents a
critical vulnerability. To address this, we conduct the first systematic study
of data detection within RL post-training scenario and propose Self-Critique.
Our method is motivated by a key observation: after RL phase, the output
entropy distribution of LLMs tends to collapse into highly specific and sparse
modes. Self-Critique probes for the underlying policy collapse, i.e., the
model's convergence to a narrow reasoning path, which causes this entropy
reduction. To facilitate this research, we also introduce RL-MIA, a benchmark
constructed to simulate this specific contamination scenario. Extensive
experiments show that Self-Critique significantly outperforms baseline methods
across multiple models and contamination tasks, achieving an AUC improvement of
up to 30%. Whereas existing methods are close to a random guess for RL-phase
contamination, our method makes detection possible.

</details>


### [15] [CLARity: Reasoning Consistency Alone Can Teach Reinforced Experts](https://arxiv.org/abs/2510.09278)
*Jiuheng Lin,Cong Jiang,Zirui Wu,Jiarui Sun,Yansong Feng*

Main category: cs.CL

TL;DR: CLARity是一个成本效益高的强化学习框架，使用小型通用LLM来提升专家模型在稀缺数据领域的推理质量，通过一致性感知奖励机制和两阶段训练流程改善推理一致性。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺领域训练专家LLM很困难，通常依赖选择题。但标准的基于结果的强化学习虽然可能提高准确性，却常常损害推理质量如逻辑一致性。现有监督推理的解决方案成本过高。

Method: 提出CLARity框架，集成一致性感知奖励机制与两阶段精炼-监控训练流程来增强推理一致性，并使用动态数据重构策略更好地利用有限数据。

Result: 实验显示CLARity相比基线方法将响应一致性提高了16.5%，准确性提高了7.5%。人工评估进一步确认了在连贯性和专业性方面的整体改进。

Conclusion: CLARity提供了一个通用解决方案，使较小模型能够通过推理一致性有效指导专家模型。

Abstract: Training expert LLMs in domains with scarce data is difficult, often relying
on multiple-choice questions (MCQs). However, standard outcome-based
reinforcement learning (RL) on MCQs is risky. While it may improve accuracy, we
observe it often degrades reasoning quality such as logical consistency.
Existing solutions to supervise reasoning, such as large-scale Process Reward
Models (PRMs), are prohibitively expensive. To address this, we propose
CLARity, a cost-effective RL framework that enhances reasoning quality using
only a small, general-purpose LLM. CLARity integrates a consistency-aware
reward mechanism with a 2-stage refine-then-monitor training pipeline to
enhance reasoning consistency, and a dynamic data reformulation strategy to to
better exploit limited data. Experiments demonstrate that CLARity improves
response consistency by 16.5% and accuracy by 7.5% over baselines. Human
evaluations further confirm holistic improvements in coherence and
professionalism. Thus, CLARity offers a generalizable solution that enables
smaller models to effectively guide expert models by reasoning consistency.Our
code is open sourced at: https://github.com/Infinite-set/CLARity

</details>


### [16] [MaP: A Unified Framework for Reliable Evaluation of Pre-training Dynamics](https://arxiv.org/abs/2510.09295)
*Jiapeng Wang,Changxin Tian,Kunlong Chen,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 提出MaP框架，通过检查点合并和Pass@k指标来解决LLM预训练评估中的不稳定性问题，提供更可靠的模型能力评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练过程中的评估存在显著不稳定性，这掩盖了真实的学习动态，阻碍了LLM研究的进展。

Method: MaP框架结合检查点合并（平滑参数空间）和Pass@k指标（低方差统计估计），从参数不稳定性和评估不稳定性两个源头解决问题。

Result: 实验表明MaP能产生更平滑的性能曲线，减少运行间方差，确保更一致的模型排名。

Conclusion: MaP为观察LLM训练动态提供了更可靠和忠实的视角，为LLM研究奠定了重要的实证基础。

Abstract: Reliable evaluation is fundamental to the progress of Large Language Models
(LLMs), yet the evaluation process during pre-training is plagued by
significant instability that obscures true learning dynamics. In this work, we
systematically diagnose this instability, attributing it to two distinct
sources: \textit{Parameter Instability} from training stochasticity and
\textit{Evaluation Instability} from noisy measurement protocols. To counteract
both sources of noise, we introduce \textbf{MaP}, a dual-pronged framework that
synergistically integrates checkpoint \underline{M}erging \underline{a}nd the
\underline{P}ass@k metric. Checkpoint merging smooths the parameter space by
averaging recent model weights, while Pass@k provides a robust, low-variance
statistical estimate of model capability. Extensive experiments show that MaP
yields significantly smoother performance curves, reduces inter-run variance,
and ensures more consistent model rankings. Ultimately, MaP provides a more
reliable and faithful lens for observing LLM training dynamics, laying a
crucial empirical foundation for LLM research.

</details>


### [17] [Verifying Chain-of-Thought Reasoning via Its Computational Graph](https://arxiv.org/abs/2510.09312)
*Zheng Zhao,Yeskendir Koishekenov,Xianjun Yang,Naila Murray,Nicola Cancedda*

Main category: cs.CL

TL;DR: 提出了一种基于电路推理验证（CRV）的白盒方法，通过分析正确和错误推理步骤的归因图结构特征来验证推理正确性，相比黑盒和灰盒方法能提供更深入的错误原因分析。


<details>
  <summary>Details</summary>
Motivation: 现有的CoT验证方法基于输出（黑盒）或激活（灰盒）预测推理正确性，但对计算失败的原因提供有限洞察。需要一种能深入理解推理错误原因的白盒方法。

Method: 使用基于电路推理验证（CRV）的白盒方法，训练分类器分析正确和错误CoT步骤归因图的结构特征，这些图被视为模型潜在推理电路的执行轨迹。

Result: 该方法显示推理错误具有高度预测性的结构特征，这些特征具有领域特异性，且通过针对性干预可以成功纠正模型的错误推理。

Conclusion: 通过审查模型的计算过程，可以从简单的错误检测转向对LLM推理的更深层次因果理解。

Abstract: Current Chain-of-Thought (CoT) verification methods predict reasoning
correctness based on outputs (black-box) or activations (gray-box), but offer
limited insight into why a computation fails. We introduce a white-box method:
Circuit-based Reasoning Verification (CRV). We hypothesize that attribution
graphs of correct CoT steps, viewed as execution traces of the model's latent
reasoning circuits, possess distinct structural fingerprints from those of
incorrect steps. By training a classifier on structural features of these
graphs, we show that these traces contain a powerful signal of reasoning
errors. Our white-box approach yields novel scientific insights unattainable by
other methods. (1) We demonstrate that structural signatures of error are
highly predictive, establishing the viability of verifying reasoning directly
via its computational graph. (2) We find these signatures to be highly
domain-specific, revealing that failures in different reasoning tasks manifest
as distinct computational patterns. (3) We provide evidence that these
signatures are not merely correlational; by using our analysis to guide
targeted interventions on individual transcoder features, we successfully
correct the model's faulty reasoning. Our work shows that, by scrutinizing a
model's computational process, we can move from simple error detection to a
deeper, causal understanding of LLM reasoning.

</details>


### [18] [Understanding the Effects of Domain Finetuning on LLMs](https://arxiv.org/abs/2510.09359)
*Eshaan Tanwar,Deepak Nathani,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本研究首次系统分析了医疗领域大语言模型的领域特定微调机制，发现微调仅改变表示子空间的一小部分，提出了tuning vectors框架来捕捉参数方向性变化，并揭示了这些向量主要在MLP层写入新方向信息而在注意力层放大已有方向。


<details>
  <summary>Details</summary>
Motivation: 理解领域特定微调如何重塑大语言模型的参数空间机制，此前研究主要关注自回归或通用指令模型，领域专业化模型研究不足。

Method: 提出tuning vectors框架（受task vectors启发）来显式捕捉微调引起的参数方向性变化，分析不同层的变化模式。

Result: 发现微调仅修改表示子空间的小部分，tuning vectors能提升指令遵循和生成质量，跨领域组合可改善泛化能力，MLP层写入新方向信息而注意力层放大已有方向。

Conclusion: 研究为LLM适应机制提供了新见解，并提供了一个通用、可解释的分析专业化大语言模型的框架。

Abstract: Large Language Models (LLMs) fine-tuned for specific domains exhibit strong
performance; however, the underlying mechanisms by which this fine-tuning
reshapes their parametric space are not well understood. Prior works primarily
focus on auto-regressive or general-purpose instruct models, leaving
domain-specialised LLMs under-explored. We present the first systematic study
of domain-specific fine-tuning in large medical language models. Our analysis
reveals that fine-tuning modifies only a small subset of the representational
subspace, essentially preserving the pre-trained model's representation. To
interpret these changes in subspaces, we propose tuning vectors, a novel
framework inspired by task vectors, which explicitly capture the directional
parameter shifts induced by fine-tuning. We demonstrate that these vectors are
critical for enhancing both instruction-following and generation quality.
Furthermore, combining tuning vectors across different domains yields improved
generalisation. Upon closer inspection of directional alignment, we find these
vectors primarily write new directional information into the MLP layers of the
model, while amplifying existing directions in attention heads. Our findings
offer new insights into LLM adaptation and provide a general, interpretable
framework for analysing specialisation in large language models.

</details>


### [19] [Active Model Selection for Large Language Models](https://arxiv.org/abs/2510.09418)
*Yavuz Durmazkeser,Patrik Okanovic,Andreas Kirsch,Torsten Hoefler,Nezihe Merve Gürel*

Main category: cs.CL

TL;DR: LLM SELECTOR是一个主动选择大型语言模型的框架，通过自适应选择少量查询进行标注来高效识别最佳LLM，相比传统评估方法显著降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 传统LLM评估方法需要完全标注的数据集，标注成本高昂。需要一种能够用有限标注就能识别最佳LLM的高效方法。

Method: 提出主动模型选择框架，自适应选择信息量最大的查询进行标注，并利用基于评判者的标注模型进一步降低标注成本。

Result: 在6个基准测试和151个LLM上的实验表明，LLM SELECTOR在选择最佳和接近最佳LLM时，标注成本降低了高达59.62%。

Conclusion: LLM SELECTOR提供了一种高效的LLM选择方法，显著降低了评估成本，为实际应用中的模型选择提供了实用解决方案。

Abstract: We introduce LLM SELECTOR, the first framework for active model selection of
Large Language Models (LLMs). Unlike prior evaluation and benchmarking
approaches that rely on fully annotated datasets, LLM SELECTOR efficiently
identifies the best LLM with limited annotations. In particular, for any given
task, LLM SELECTOR adaptively selects a small set of queries to annotate that
are most informative about the best model for the task. To further reduce
annotation cost, we leverage a judge-based oracle annotation model. Through
extensive experiments on 6 benchmarks with 151 LLMs, we show that LLM SELECTOR
reduces annotation costs by up to 59.62% when selecting the best and near-best
LLM for the task.

</details>


### [20] [On the Representations of Entities in Auto-regressive Large Language Models](https://arxiv.org/abs/2510.09421)
*Victor Morand,Josiane Mothe,Benjamin Piwowarski*

Main category: cs.CL

TL;DR: 论文提出了实体提及重构框架来研究LLM如何编码和操作实体，通过任务向量方法从LLM隐藏状态生成多token实体提及，揭示了LLM具有实体特定机制来表示和操作多token实体。


<details>
  <summary>Details</summary>
Motivation: 研究LLM内部如何表示实体，目前对实体表示本身了解甚少，需要探索LLM如何编码和操作实体，特别是多token实体的表示方式。

Method: 提出实体提及重构框架，利用任务向量方法从LLM隐藏状态一致生成多token实体提及，引入Entity Lens扩展logit-lens来预测多token提及。

Result: 发现LLM开发了实体特定机制来表示和操作任何多token实体，包括训练期间未见过的实体，实体表示捕获了关系知识。

Conclusion: LLM具有专门机制来编码和操作实体表示，这些表示包含丰富的语义和关系信息，Entity Lens为研究实体表示提供了有效工具。

Abstract: Named entities are fundamental building blocks of knowledge in text,
grounding factual information and structuring relationships within language.
Despite their importance, it remains unclear how Large Language Models (LLMs)
internally represent entities. Prior research has primarily examined explicit
relationships, but little is known about entity representations themselves. We
introduce entity mention reconstruction as a novel framework for studying how
LLMs encode and manipulate entities. We investigate whether entity mentions can
be generated from internal representations, how multi-token entities are
encoded beyond last-token embeddings, and whether these representations capture
relational knowledge. Our proposed method, leveraging _task vectors_, allows to
consistently generate multi-token mentions from various entity representations
derived from the LLMs hidden states. We thus introduce the _Entity Lens_,
extending the _logit-lens_ to predict multi-token mentions. Our results bring
new evidence that LLMs develop entity-specific mechanisms to represent and
manipulate any multi-token entities, including those unseen during training.
Our code is avalable at https://github.com/VictorMorand/EntityRepresentations .

</details>


### [21] [StatEval: A Comprehensive Benchmark for Large Language Models in Statistics](https://arxiv.org/abs/2510.09517)
*Yuchen Lu,Run Yang,Yichen Zhang,Shuguang Yu,Runpeng Dai,Ziwei Wang,Jiayi Xiang,Wenxin E,Siran Gao,Xinyao Ruan,Yirui Huang,Chenjing Xi,Haibo Hu,Yueming Fu,Qinglan Yu,Xiaobing Wei,Jiani Gu,Rui Sun,Jiaxuan Jia,Fan Zhou*

Main category: cs.CL

TL;DR: 提出了StatEval，首个全面的统计学基准测试，包含13,817个基础问题和2,374个研究级证明任务，用于评估LLMs在统计学推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试对数学和逻辑推理已有较好覆盖，但统计学作为一门独特且综合的学科在基准测试中尚未得到充分探索。

Method: 设计了可扩展的多智能体流水线，结合人工验证，自动化大规模问题提取、重写和质量控制，确保学术严谨性。

Result: 实验结果显示，闭源模型如GPT5-mini在研究级问题上准确率低于57%，开源模型表现更差，凸显了统计推理的独特挑战。

Conclusion: StatEval可作为推进大型语言模型统计智能的严格基准，揭示了当前LLMs在统计学推理方面的局限性。

Abstract: Large language models (LLMs) have demonstrated remarkable advances in
mathematical and logical reasoning, yet statistics, as a distinct and
integrative discipline, remains underexplored in benchmarking efforts. To
address this gap, we introduce \textbf{StatEval}, the first comprehensive
benchmark dedicated to statistics, spanning both breadth and depth across
difficulty levels. StatEval consists of 13,817 foundational problems covering
undergraduate and graduate curricula, together with 2374 research-level proof
tasks extracted from leading journals. To construct the benchmark, we design a
scalable multi-agent pipeline with human-in-the-loop validation that automates
large-scale problem extraction, rewriting, and quality control, while ensuring
academic rigor. We further propose a robust evaluation framework tailored to
both computational and proof-based tasks, enabling fine-grained assessment of
reasoning ability. Experimental results reveal that while closed-source models
such as GPT5-mini achieve below 57\% on research-level problems, with
open-source models performing significantly lower. These findings highlight the
unique challenges of statistical reasoning and the limitations of current LLMs.
We expect StatEval to serve as a rigorous benchmark for advancing statistical
intelligence in large language models. All data and code are available on our
web platform: https://stateval.github.io/.

</details>


### [22] [Mitigating Overthinking through Reasoning Shaping](https://arxiv.org/abs/2510.09535)
*Feifan Song,Shaohang Wei,Bofei Gao,Yejie Wang,Wen Luo,Wei Li,Linli Yao,Weimin Xiong,Liang Chen,Tianyu Liu,Houfeng Wang*

Main category: cs.CL

TL;DR: 提出GRSP方法，通过步骤级监督来平衡大型推理模型的效率和准确性，解决过度思考问题


<details>
  <summary>Details</summary>
Motivation: 现有基于验证器奖励的强化学习方法在减少token消耗时往往损害模型性能，需要更细粒度的监督来平衡效率与准确性

Method: 提出Group Relative Segment Penalization (GRSP)，基于推理步骤的段级惩罚方法，设计跨段簇的长度感知权重机制

Result: GRSP在保持准确性的同时显著提升token效率，特别是在困难问题上表现更优，还能稳定RL训练并有效扩展到不同模型规模

Conclusion: 监督粒度对平衡推理模型的效率和准确性至关重要，GRSP通过步骤级监督实现了更好的平衡

Abstract: Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier
Reward (RLVR) have shown great power in problem solving, yet they often cause
overthinking: excessive, meandering reasoning that inflates computational cost.
Prior designs of penalization in RLVR manage to reduce token consumption while
often harming model performance, which arises from the oversimplicity of
token-level supervision. In this paper, we argue that the granularity of
supervision plays a crucial role in balancing efficiency and accuracy, and
propose Group Relative Segment Penalization (GRSP), a step-level method to
regularize reasoning. Since preliminary analyses show that reasoning segments
are strongly correlated with token consumption and model performance, we design
a length-aware weighting mechanism across segment clusters. Extensive
experiments demonstrate that GRSP achieves superior token efficiency without
heavily compromising accuracy, especially the advantages with harder problems.
Moreover, GRSP stabilizes RL training and scales effectively across model
sizes.

</details>


### [23] [Evaluating Robustness of Large Language Models Against Multilingual Typographical Errors](https://arxiv.org/abs/2510.09536)
*Yihong Liu,Raoyuan Zhao,Lena Altinger,Hinrich Schütze,Michael A. Hedderich*

Main category: cs.CL

TL;DR: MulTypo是一个多语言拼写错误生成算法，用于评估LLM在包含拼写错误的输入下的鲁棒性。研究发现拼写错误会显著降低模型性能，特别是在生成性任务和推理任务中，且不同语言的鲁棒性存在差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多假设输入是干净的，而实际应用中用户输入往往包含拼写错误，因此需要评估LLM在多语言环境下对拼写错误的鲁棒性。

Method: 提出MulTypo算法，基于语言特定的键盘布局和打字行为模拟人类拼写错误，评估了18个开源LLM在5个下游任务上的表现。

Result: 拼写错误持续降低模型性能，生成性任务和推理任务受影响最严重；指令调优提高了干净输入的性能但可能增加对噪声的脆弱性；高资源语言比低资源语言更鲁棒。

Conclusion: 需要进行噪声感知训练和多语言鲁棒性评估。

Abstract: Large language models (LLMs) are increasingly deployed in multilingual,
real-world applications with user inputs -- naturally introducing typographical
errors (typos). Yet most benchmarks assume clean input, leaving the robustness
of LLMs to typos across languages largely underexplored. To address this gap,
we introduce MulTypo, a multilingual typo generation algorithm that simulates
human-like errors based on language-specific keyboard layouts and typing
behavior. We evaluate 18 open-source LLMs across three model families and five
downstream tasks spanning language inference, multi-choice question answering,
mathematical reasoning, and machine translation tasks. Our results show that
typos consistently degrade performance, particularly in generative tasks and
those requiring reasoning -- while the natural language inference task is
comparatively more robust. Instruction tuning improves clean-input performance
but may increase brittleness under noise. We also observe language-dependent
robustness: high-resource languages are generally more robust than low-resource
ones, and translation from English is more robust than translation into
English. Our findings underscore the need for noise-aware training and
multilingual robustness evaluation. We make our code and data publicly
available.

</details>


### [24] [SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models](https://arxiv.org/abs/2510.09541)
*Chengyu Wang,Paria Rashidinejad,DiJia Su,Song Jiang,Sid Wang,Siyan Zhao,Cai Zhou,Shannon Zejiang Shen,Feiyu Chen,Tommi Jaakkola,Yuandong Tian,Bo Liu*

Main category: cs.CL

TL;DR: 提出了Sandwiched Policy Gradient (SPG)方法，通过使用对数似然的上界和下界来解决扩散大语言模型(dLLMs)强化学习中对数似然难以处理的问题，显著提升了在数学推理和逻辑推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)因其并行解码能力成为自回归模型的高效替代品，但由于其难以处理的对数似然，无法直接应用标准的策略梯度方法进行人类偏好或任务特定奖励的对齐。现有方法使用ELBO等替代方案，但这些单边近似会引入显著的策略梯度偏差。

Method: 提出了Sandwiched Policy Gradient (SPG)方法，利用真实对数似然的上界和下界来减少策略梯度偏差，相比基于ELBO或一步估计的基线方法具有更好的性能。

Result: SPG在多个任务上显著优于基线方法：在GSM8K上准确率提升3.6%，在MATH500上提升2.6%，在Countdown上提升18.4%，在Sudoku上提升27.0%。

Conclusion: SPG方法有效解决了dLLMs强化学习中对数似然难以处理的问题，通过使用上下界约束显著提升了策略梯度的准确性，在各种推理任务上取得了显著改进。

Abstract: Diffusion large language models (dLLMs) are emerging as an efficient
alternative to autoregressive models due to their ability to decode multiple
tokens in parallel. However, aligning dLLMs with human preferences or
task-specific rewards via reinforcement learning (RL) is challenging because
their intractable log-likelihood precludes the direct application of standard
policy gradient methods. While prior work uses surrogates like the evidence
lower bound (ELBO), these one-sided approximations can introduce significant
policy gradient bias. To address this, we propose the Sandwiched Policy
Gradient (SPG) that leverages both an upper and a lower bound of the true
log-likelihood. Experiments show that SPG significantly outperforms baselines
based on ELBO or one-step estimation. Specifically, SPG improves the accuracy
over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500,
18.4% in Countdown and 27.0% in Sudoku.

</details>


### [25] [A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages](https://arxiv.org/abs/2510.09555)
*Raoyuan Zhao,Yihong Liu,Hinrich Schütze,Michael A. Hedderich*

Main category: cs.CL

TL;DR: 本文首次对多语言链式思维推理进行了全面研究，评估了性能、一致性和忠实性三个关键维度，揭示了不同语言下思维轨迹的质量和有效性存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 虽然大型推理模型依赖链式思维推理来提高任务性能，但在多语言环境中，导致最终答案的中间思维轨迹本身尚未得到充分探索。

Method: 通过测量语言合规性、答案准确性和一致性来评估模型在目标语言中的表现；通过跨语言交换思维轨迹评估一致性；采用截断和错误注入等扰动技术来探测思维轨迹的忠实性。

Result: 研究发现模型在不同语言下表现出强烈的语言偏好和不同的性能表现；思维轨迹的质量和有效性因提示语言而异；模型在不同语言下对思维轨迹的依赖程度各不相同。

Conclusion: 多语言链式思维推理存在显著的语言依赖性，思维轨迹的质量和有效性在不同语言间差异明显，这对多语言推理系统的开发具有重要启示。

Abstract: Large reasoning models (LRMs) increasingly rely on step-by-step
Chain-of-Thought (CoT) reasoning to improve task performance, particularly in
high-resource languages such as English. While recent work has examined
final-answer accuracy in multilingual settings, the thinking traces themselves,
i.e., the intermediate steps that lead to the final answer, remain
underexplored. In this paper, we present the first comprehensive study of
multilingual CoT reasoning, evaluating three key dimensions: performance,
consistency, and faithfulness. We begin by measuring language compliance,
answer accuracy, and answer consistency when LRMs are explicitly instructed or
prompt-hacked to think in a target language, revealing strong language
preferences and divergent performance across languages. Next, we assess
crosslingual consistency of thinking traces by interchanging them between
languages. We find that the quality and effectiveness of thinking traces vary
substantially depending on the prompt language. Finally, we adapt
perturbation-based techniques -- i.e., truncation and error injection -- to
probe the faithfulness of thinking traces across languages, showing that models
rely on traces to varying degrees. We release our code and data to support
future research.

</details>


### [26] [Dyna-Mind: Learning to Simulate from Experience for Better AI Agents](https://arxiv.org/abs/2510.09577)
*Xiao Yu,Baolin Peng,Michel Galley,Hao Cheng,Qianhui Wu,Janardhan Kulkarni,Suman Nath,Zhou Yu,Jianfeng Gao*

Main category: cs.CL

TL;DR: 提出Dyna-Mind框架，通过模拟训练增强AI代理在复杂交互环境中的推理和决策能力


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在数学和编程方面表现出色，但在长时程交互任务中表现不佳，需要模拟未来状态的能力

Method: 两阶段训练框架：第一阶段ReSim通过环境交互生成结构化推理轨迹；第二阶段Dyna-GRPO使用强化学习优化决策

Result: 在Sokoban、ALFWorld和AndroidWorld基准测试中有效提升了代理的模拟和决策能力

Conclusion: 模拟能力对于AI代理在复杂环境中有效推理、规划和行动至关重要

Abstract: Reasoning models have recently shown remarkable progress in domains such as
math and coding. However, their expert-level abilities in math and coding
contrast sharply with their performance in long-horizon, interactive tasks such
as web navigation and computer/phone-use. Inspired by literature on human
cognition, we argue that current AI agents need ''vicarious trial and error'' -
the capacity to mentally simulate alternative futures before acting - in order
to enhance their understanding and performance in complex interactive
environments. We introduce Dyna-Mind, a two-stage training framework that
explicitly teaches (V)LM agents to integrate such simulation into their
reasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which
trains the agent to generate structured reasoning traces from expanded search
trees built from real experience gathered through environment interactions.
ReSim thus grounds the agent's reasoning in faithful world dynamics and equips
it with the ability to anticipate future states in its reasoning. In stage 2,
we propose Dyna-GRPO, an online reinforcement learning method to further
strengthen the agent's simulation and decision-making ability by using both
outcome rewards and intermediate states as feedback from real rollouts.
Experiments on two synthetic benchmarks (Sokoban and ALFWorld) and one
realistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively
infuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome
and interaction-level signals to learn better policies for long-horizon,
planning-intensive tasks. Together, these results highlight the central role of
simulation in enabling AI agents to reason, plan, and act more effectively in
the ever more challenging environments.

</details>


### [27] [Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation](https://arxiv.org/abs/2510.09599)
*Sondos Mahmoud Bsharat,Zhiqiang Shen*

Main category: cs.CL

TL;DR: P-TTS是一种推理时数据增强策略，仅使用90个手动选择的事例，通过系统化改变提示强度来合成多样化的推理轨迹，从而增强LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要大量人工标注推理数据集的资源消耗问题，提供一种低成本、高效的LLM推理能力增强方法。

Method: 使用仅90个手动选择的推理实例，在测试时通过原则性指令提示强度系统化变化来合成多样化的推理轨迹上下文，然后对Qwen-2.5模型进行微调。

Result: 在数学推理基准测试中显著优于现有基线，7B和32B模型在AIME'24和AIME'25上分别获得+26.66%到+30.00%的绝对准确率提升，并在其他基准测试中表现优异。

Conclusion: 测试时扩展有效探索了推理模式的潜在空间，以最小标注开销放大了LLM的问题解决能力，为资源受限或快速发展的领域提供了实用的低成本LLM推理激发方法。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
capabilities when provided with chain-of-thought exemplars, but curating large
reasoning datasets remains laborious and resource-intensive. In this work, we
introduce Prompting Test-Time Scaling (P-TTS), a simple yet effective
inference-time data augmentation strategy for enhancing LLM reasoning through
finetuning. Rather than collecting thousands or even millions of examples,
P-TTS leverages a small pool of only 90 manually selected reasoning instances
and systematically varies exemplar augmentation through principled instruction
prompting intensities at test time to synthesize diverse reasoning trajectory
contexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data.
Across a suite of mathematical reasoning AIME2024 & 25, MATH500, and
GPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive
baselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of
+26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B);
P-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and
+3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better
performance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances
zero-shot generalization accuracy on out-of-domain reasoning benchmarks of
Gaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our
analysis suggests that test-time scaling effectively explores the latent space
of reasoning patterns, amplifying LLM problem-solving with minimal annotation
overhead, and further unlocking the reasoning potential and capabilities of
LLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit
LLM reasoning in resource-constrained or rapidly evolving domains.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [28] [Hypothesis Hunting with Evolving Networks of Autonomous Scientific Agents](https://arxiv.org/abs/2510.08619)
*Tennison Liu,Silas Ruhrberg Estévez,David L. Bentley,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 提出了AScience框架和ASCollab系统，通过LLM研究代理的自组织网络进行大规模科学假设探索，在癌症队列实验中展示了能够持续产生多样化、高质量、新颖的发现


<details>
  <summary>Details</summary>
Motivation: 大规模科学数据集为无特定研究问题的探索性发现创造了机会，需要支持在复杂假设空间中进行累积搜索的系统

Method: 引入AScience框架，将发现建模为代理、网络和评估规范的交互，实现为ASCollab分布式系统，包含具有异质行为的LLM研究代理，这些代理自组织成演化网络，持续产生和同行评审发现

Result: 实验显示这种社会动态能够沿多样性-质量-新颖性前沿积累专家评级结果，包括重新发现已建立的生物标志物、扩展已知通路和提出新的治疗靶点

Conclusion: 虽然湿实验室验证仍然不可或缺，但社会结构化的代理网络可以在大规模上持续进行探索性假设搜索

Abstract: Large-scale scientific datasets -- spanning health biobanks, cell atlases,
Earth reanalyses, and more -- create opportunities for exploratory discovery
unconstrained by specific research questions. We term this process hypothesis
hunting: the cumulative search for insight through sustained exploration across
vast and complex hypothesis spaces. To support it, we introduce AScience, a
framework modeling discovery as the interaction of agents, networks, and
evaluation norms, and implement it as ASCollab, a distributed system of
LLM-based research agents with heterogeneous behaviors. These agents
self-organize into evolving networks, continually producing and peer-reviewing
findings under shared standards of evaluation. Experiments show that such
social dynamics enable the accumulation of expert-rated results along the
diversity-quality-novelty frontier, including rediscoveries of established
biomarkers, extensions of known pathways, and proposals of new therapeutic
targets. While wet-lab validation remains indispensable, our experiments on
cancer cohorts demonstrate that socially structured, agentic networks can
sustain exploratory hypothesis hunting at scale.

</details>


### [29] [COMPASS: Enhancing Agent Long-Horizon Reasoning with Evolving Context](https://arxiv.org/abs/2510.08790)
*Guangya Wan,Mingyang Ling,Xiaoqi Ren,Rujun Han,Sheng Li,Zizhao Zhang*

Main category: cs.AI

TL;DR: COMPASS是一个轻量级分层框架，通过分离战术执行、战略监督和上下文管理来解决LLM代理在长时程任务中的上下文管理瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 长时程任务需要持续推理和多次工具交互，但LLM代理容易因小错误累积、幻觉或失去连贯性而失败，上下文管理是主要瓶颈。

Method: 提出COMPASS框架，包含三个专门组件：执行推理和工具使用的主代理、监控进度并发出战略干预的元思考者、维护简洁相关进度摘要的上下文管理器。

Result: 在GAIA、BrowseComp和Humanity's Last Exam三个基准测试中，COMPASS相比单代理和多代理基线准确率提升高达20%。

Conclusion: COMPASS有效解决了长时程任务中的上下文管理问题，并通过测试时扩展和训练后流水线进一步提升了性能和效率。

Abstract: Long-horizon tasks that require sustained reasoning and multiple tool
interactions remain challenging for LLM agents: small errors compound across
steps, and even state-of-the-art models often hallucinate or lose coherence. We
identify context management as the central bottleneck -- extended histories
cause agents to overlook critical evidence or become distracted by irrelevant
information, thus failing to replan or reflect from previous mistakes. To
address this, we propose COMPASS (Context-Organized Multi-Agent Planning and
Strategy System), a lightweight hierarchical framework that separates tactical
execution, strategic oversight, and context organization into three specialized
components: (1) a Main Agent that performs reasoning and tool use, (2) a
Meta-Thinker that monitors progress and issues strategic interventions, and (3)
a Context Manager that maintains concise, relevant progress briefs for
different reasoning stages. Across three challenging benchmarks -- GAIA,
BrowseComp, and Humanity's Last Exam -- COMPASS improves accuracy by up to 20%
relative to both single- and multi-agent baselines. We further introduce a
test-time scaling extension that elevates performance to match established
DeepResearch agents, and a post-training pipeline that delegates context
management to smaller models for enhanced efficiency.

</details>


### [30] [What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment](https://arxiv.org/abs/2510.08847)
*Allison Sihan Jia,Daniel Huang,Nikhil Vytla,Nirvika Choudhury,John C Mitchell,Anupam Datta*

Main category: cs.AI

TL;DR: 提出了Agent GPA框架，基于目标-计划-动作循环来评估智能体性能，包含五个评价指标，在两个数据集上验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 需要系统化的智能体评估框架来全面覆盖各种智能体失败情况，支持错误定位和性能改进。

Method: 开发了基于目标-计划-动作循环的Agent GPA评估框架，包含五个评价指标：目标达成度、逻辑一致性、执行效率、计划质量和计划遵循度。

Result: 在TRAIL/GAIA和内部数据集上的实验表明，该框架能覆盖所有智能体错误，支持LLM评判器与人工标注高度一致（80%-95%），错误定位准确率达86%。

Conclusion: Agent GPA框架为智能体评估提供了系统化方法，能有效识别和定位错误，促进智能体性能的针对性改进。

Abstract: We introduce the Agent GPA (Goal-Plan-Action) framework: an evaluation
paradigm based on an agent's operational loop of setting goals, devising plans,
and executing actions. The framework includes five evaluation metrics: Goal
Fulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan
Adherence. Logical Consistency checks that an agent's actions are consistent
with its prior actions. Execution Efficiency checks whether the agent executes
in the most efficient way to achieve its goal. Plan Quality checks whether an
agent's plans are aligned with its goals; Plan Adherence checks if an agent's
actions are aligned with its plan; and Goal Fulfillment checks that agent's
final outcomes match the stated goals. Our experimental results on two
benchmark datasets - the public TRAIL/GAIA dataset and an internal dataset for
a production-grade data agent - show that this framework (a) provides a
systematic way to cover a broad range of agent failures, including all agent
errors on the TRAIL/GAIA benchmark dataset; (b) supports LLM-judges that
exhibit strong agreement with human annotation, covering 80% to over 95%
errors; and (c) localizes errors with 86% agreement to enable targeted
improvement of agent performance.

</details>


### [31] [GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare](https://arxiv.org/abs/2510.08872)
*Siqi Zhu,David Zhang,Pedro Cisneros-Velarde,Jiaxuan You*

Main category: cs.AI

TL;DR: 提出了GTAlign框架，将博弈论决策整合到LLM的推理和训练中，通过构建收益矩阵和相互福利奖励来优化用户-LLM交互，实现互利结果。


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法假设最大化模型奖励就能最大化用户福利，但实践中LLM可能产生过度澄清或冗长推理，导致用户偏好与模型行为不匹配，类似于囚徒困境问题。

Method: 在推理阶段将用户-LLM交互视为策略博弈，构建收益矩阵评估双方福利；在训练阶段引入相互福利奖励强化合作响应；还包含基于博弈论推理的动态适应技术。

Result: 实验表明GTAlign显著提高了推理效率、答案质量和相互福利，优于基线方法。

Conclusion: GTAlign通过博弈论方法有效解决了LLM对齐中的社会效率问题，实现了用户与模型的互利共赢。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in reasoning,
yet sometimes produce responses that are suboptimal for users in tasks such as
writing, information seeking, or providing practical guidance. Conventional
alignment practices typically assume that maximizing model reward also
maximizes user welfare, but this assumption frequently fails in practice:
models may over-clarify or generate overly verbose reasoning when users prefer
concise answers. Such behaviors resemble the prisoner's dilemma, where
individually rational choices lead to socially suboptimal outcomes. The
fundamental challenge is the lack of a principled decision making mechanism
that mutually benefits both the LLM and the user. We propose Game-Theoretic
Alignment (GTAlign), an alignment framework that integrates game-theoretic
decision making into both reasoning and training. During reasoning, the model
explicitly treats user-LLM interaction as a strategic game: it constructs
payoff matrices within its reasoning chain to estimate welfare for both itself
and the user, and then selects actions that are mutually beneficial. During
training, we introduce a mutual welfare reward that reinforces cooperative
responses, aligning model behavior with socially efficient outcomes. In
addition, we introduce an inference technique that leverages game-theoretic
reasoning to dynamically adapt LLM's response when pricing policies of LLM
service change. Extensive experiments demonstrate that GTAlign substantially
improves reasoning efficiency, answer quality, and mutual welfare compared to
baselines across diverse tasks. The code is available at
https://github.com/ulab-uiuc/GTAlign .

</details>


### [32] [LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition](https://arxiv.org/abs/2510.08928)
*Yushuo Zheng,Zicheng Zhang,Xiongkuo Min,Huiyu Duan,Guangtao Zhai*

Main category: cs.AI

TL;DR: LM Fight Arena是一个新颖的多模态模型评估框架，通过让模型在格斗游戏《真人快打II》中对战来测试其视觉理解和战略决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型基准测试往往无法捕捉其在实时对抗环境中的表现，需要创建更动态的评估方法。

Method: 在受控的锦标赛中测试六个领先的开源和闭源模型，每个模型控制相同角色以确保公平比较，通过解析游戏画面和状态数据来选择行动。

Result: 该框架提供了完全自动化、可重现且客观的评估，能够衡量多模态模型在动态环境中的战略推理能力。

Conclusion: LM Fight Arena引入了一个具有挑战性和吸引力的基准测试，弥合了AI评估与交互娱乐之间的差距。

Abstract: Existing benchmarks for large multimodal models (LMMs) often fail to capture
their performance in real-time, adversarial environments. We introduce LM Fight
Arena (Large Model Fight Arena), a novel framework that evaluates LMMs by
pitting them against each other in the classic fighting game Mortal Kombat II,
a task requiring rapid visual understanding and tactical, sequential
decision-making. In a controlled tournament, we test six leading open- and
closed-source models, where each agent operates controlling the same character
to ensure a fair comparison. The models are prompted to interpret game frames
and state data to select their next actions. Unlike static evaluations, LM
Fight Arena provides a fully automated, reproducible, and objective assessment
of an LMM's strategic reasoning capabilities in a dynamic setting. This work
introduces a challenging and engaging benchmark that bridges the gap between AI
evaluation and interactive entertainment.

</details>


### [33] [RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic Workflows](https://arxiv.org/abs/2510.09021)
*Hamed Mahdavi,Pouria Mahdavinia,Samira Malek,Pegah Mohammadipour,Alireza Hashemi,Majid Daliri,Alireza Farhadi,Amir Khasahmadi,Niloofar Mireshghallah,Vasant Honavar*

Main category: cs.AI

TL;DR: 评估LLMs在数学证明评分方面的能力，包括错误检测、严重性判断和分数分配，并提出基于智能体工作流程的评分方法以提高与人工评分的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在解决奥林匹克数学问题方面取得显著进展，需要评估它们在证明评分方面的能力，特别是检测错误、判断严重性和分配公平分数，而不仅仅是二元正确性判断。

Method: 使用90个Gemini 2.5 Pro生成的解决方案和MathArena的IMO/USAMO 2025解决方案集，引入智能体工作流程来提取和分析参考解决方案，自动推导问题特定的评分标准，并进行多步骤评分过程。

Result: 模型能够可靠地标记错误解决方案（包括微妙错误），但在分配部分分数方面存在校准差距。提出的工作流程在注释语料库和MathArena上实现了与人工评分更高的一致性，并在处理部分分数方面更加一致。

Conclusion: 智能体工作流程可以有效提高LLMs在数学证明评分方面的表现，特别是在处理部分分数和与人工评分一致性方面。

Abstract: State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based
Olympiad problems to solving most of the IMO 2025 problems, with leading
systems reportedly handling 5 of 6 problems. Given this progress, we assess how
well these models can grade proofs: detecting errors, judging their severity,
and assigning fair scores beyond binary correctness. We study proof-analysis
capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we
grade on a 1-4 scale with detailed error annotations, and on MathArena solution
sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models
can reliably flag incorrect (including subtly incorrect) solutions but exhibit
calibration gaps in how partial credit is assigned. To address this, we
introduce agentic workflows that extract and analyze reference solutions and
automatically derive problem-specific rubrics for a multi-step grading process.
We instantiate and compare different design choices for the grading workflows,
and evaluate their trade-offs. Across our annotated corpus and MathArena, our
proposed workflows achieve higher agreement with human grades and more
consistent handling of partial credit across metrics. We release all code,
data, and prompts/logs to facilitate future research.

</details>


### [34] [Auto-scaling Continuous Memory for GUI Agent](https://arxiv.org/abs/2510.09038)
*Wenyi Wu,Kun Zhou,Ruoxin Yuan,Vivian Yu,Stephen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: 提出连续记忆机制，将GUI轨迹编码为固定长度的连续嵌入，显著减少上下文成本并保留细粒度视觉信息。通过自动扩展数据飞轮收集10万+轨迹，仅微调1.2%参数，在真实GUI基准测试中显著提升长视野和分布偏移下的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理将历史轨迹压缩为文本标记，导致上下文长度膨胀且丢失关键视觉线索（如精确的控件大小和位置）。需要可扩展的记忆机制来帮助代理在陌生界面和长视野任务中泛化。

Method: 使用VLM作为编码器将GUI轨迹编码为固定长度的连续嵌入，直接插入主干网络输入层。引入自动扩展数据飞轮：搜索发现新环境、使用开源VLM合成任务、代理执行轨迹、相同VLM验证成功。仅微调记忆编码器（LoRA on Q-Former，1.2%参数）。

Result: 随着记忆大小和检索深度增加，性能单调提升，而文本记忆在长提示下会退化。在真实GUI基准测试中，记忆增强代理在长视野和分布偏移下持续提升成功率。Qwen-2.5-VL-7B + 连续记忆达到与GPT-4o、Claude-4等闭源模型相当的性能。

Conclusion: 连续记忆机制能够有效减少上下文成本，保留视觉细节，并通过数据飞轮实现低成本扩展。该方法使开源模型在GUI任务中达到与顶级闭源模型相当的性能。

Abstract: We study how to endow GUI agents with scalable memory that help generalize
across unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress
past trajectories into text tokens, which balloons context length and misses
decisive visual cues (e.g., exact widget size and position). We propose a
continuous memory that encodes each GUI trajectory into a fixed-length sequence
of continuous embeddings using the VLM itself as an encoder; these embeddings
are plugged directly into the backbone's input layer, sharply reducing context
cost while preserving fine-grained visual information. As memory size and
retrieval depth increase, performance improves monotonically, unlike text
memories that degrade with long prompts. To grow memory at low cost, we
introduce an auto-scaling data flywheel that (i) discovers new environments via
search, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out
trajectories with the agent, and (iv) verifies success with the same VLM. Using
this pipeline, we collect 100k+ trajectories for about \$4000 and fine-tune
only the memory encoder (LoRA on a Q-Former, 1.2\% parameters) with 1,500
samples. On real-world GUI benchmarks, our memory-augmented agent consistently
improves success rates under long horizons and distribution shifts. Notably,
Qwen-2.5-VL-7B + continuous memory achieves performance comparable to
state-of-the-art closed-source models (e.g., GPT-4o, Claude-4).

</details>


### [35] [MEC$^3$O: Multi-Expert Consensus for Code Time Complexity Prediction](https://arxiv.org/abs/2510.09049)
*Joonghyuk Hahn,Soohan Lim,Yo-Sub Han*

Main category: cs.AI

TL;DR: 提出了MEC^3O多专家共识系统，通过将LLM分配到不同复杂度类别并让专家进行结构化辩论，使用加权共识机制整合预测，在代码复杂度预测任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码时间复杂度预测任务中表现不稳定，不同模型在不同复杂度类别上各有优势，但没有单一模型在所有类别上都表现优异。

Method: MEC^3O多专家共识系统：基于性能将LLM分配到特定复杂度类别，提供类别专业化指令，让专家进行结构化辩论，通过加权共识机制整合预测结果。

Result: 在CodeComplex数据集上，MEC^3O比开源基线方法准确率和macro-F1分数至少提高10%，在macro-F1分数上平均超过GPT-4o-mini，与GPT-4o和GPT-o4-mini的F1分数相当。

Conclusion: 多专家辩论和加权共识策略能有效生成最终预测，证明了该方法在代码复杂度预测任务中的有效性。

Abstract: Predicting the complexity of source code is essential for software
development and algorithm analysis. Recently, Baik et al. (2025) introduced
CodeComplex for code time complexity prediction. The paper shows that LLMs
without fine-tuning struggle with certain complexity classes. This suggests
that no single LLM excels at every class, but rather each model shows
advantages in certain classes. We propose MEC$^3$O, a multi-expert consensus
system, which extends the multi-agent debate frameworks. MEC$^3$O assigns LLMs
to complexity classes based on their performance and provides them with
class-specialized instructions, turning them into experts. These experts engage
in structured debates, and their predictions are integrated through a weighted
consensus mechanism. Our expertise assignments to LLMs effectively handle
Degeneration-of-Thought, reducing reliance on a separate judge model, and
preventing convergence to incorrect majority opinions. Experiments on
CodeComplex show that MEC$^3$O outperforms the open-source baselines, achieving
at least 10% higher accuracy and macro-F1 scores. It also surpasses GPT-4o-mini
in macro-F1 scores on average and demonstrates competitive on-par F1 scores to
GPT-4o and GPT-o4-mini on average. This demonstrates the effectiveness of
multi-expert debates and weight consensus strategy to generate the final
predictions. Our code and data is available at
https://github.com/suhanmen/MECO.

</details>


### [36] [Leading the Follower: Learning Persuasive Agents in Social Deduction Games](https://arxiv.org/abs/2510.09087)
*Zhang Zheng,Deheng Ye,Peilin Zhao,Hao Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于Stackelberg竞争模型的强化学习框架，用于训练LLM代理在社交推理游戏中优化说服性沟通能力，显著提升了代理在多种游戏中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在社交推理游戏中主要关注信息处理和策略选择，但忽视了说服性沟通对影响其他玩家信念和反应的重要性。游戏成功不仅依赖正确推理，更需要说服他人按照自己的意图行动。

Method: 将回合制对话形式化为Stackelberg竞争，当前玩家作为领导者战略性地影响跟随者的响应。基于此理论基础，提出了强化学习框架来训练代理优化有说服力的表达。

Result: 在三种不同的社交推理游戏中进行的综合实验表明，该方法显著优于基线模型。

Conclusion: 这项工作代表了开发具有战略社交影响力AI代理的重要进展，对需要说服性沟通的场景具有广泛意义。

Abstract: Large language model (LLM) agents have shown remarkable progress in social
deduction games (SDGs). However, existing approaches primarily focus on
information processing and strategy selection, overlooking the significance of
persuasive communication in influencing other players' beliefs and responses.
In SDGs, success depends not only on making correct deductions but on
convincing others to response in alignment with one's intent. To address this
limitation, we formalize turn-based dialogue in SDGs as a Stackelberg
competition, where the current player acts as the leader who strategically
influences the follower's response. Building on this theoretical foundation, we
propose a reinforcement learning framework that trains agents to optimize
utterances for persuasive impact. Through comprehensive experiments across
three diverse SDGs, we demonstrate that our agents significantly outperform
baselines. This work represents a significant step toward developing AI agents
capable of strategic social influence, with implications extending to scenarios
requiring persuasive communication.

</details>


### [37] [RegexPSPACE: A Benchmark for Evaluating LLM Reasoning on PSPACE-complete Regex Problems](https://arxiv.org/abs/2510.09227)
*Hyundong Jin,Joonghyuk Hahn,Yo-Sub Han*

Main category: cs.AI

TL;DR: 该论文提出了一个基于PSPACE完全正则表达式问题的新基准，用于评估LLM和LRM的空间计算限制，揭示了常见失败模式如冗长和重复。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型和推理模型在空间复杂度方面的计算限制，特别是受限于有限上下文窗口的问题。

Method: 构建了一个包含超过100万个正则表达式实例的数据集，基于两个PSPACE完全问题（正则表达式等价性判定和最小化），并进行双重指数空间探索。

Result: 对6个LLM和5个LRM进行了广泛评估，发现它们存在冗长和重复等常见失败模式。

Conclusion: 这是首个对LLM和LRM空间计算限制的实证研究，为评估其高级推理能力提供了新框架。

Abstract: Large language models (LLMs) show strong performance across natural language
processing (NLP), mathematical reasoning, and programming, and recent large
reasoning models (LRMs) further emphasize explicit reasoning. Yet their
computational limits, particularly spatial complexity constrained by finite
context windows, remain poorly understood. While recent works often focus on
problems within the NP complexity class, we push the boundary by introducing a
novel benchmark grounded in two PSPACE-complete regular expression (regex)
problems: equivalence decision (RegexEQ) and minimization (RegexMin).
PSPACE-complete problems serve as a more rigorous standard for assessing
computational capacity, as their solutions require massive search space
exploration. We perform a double-exponential space exploration to construct a
labeled dataset of over a million regex instances with a sound filtering
process to build the benchmark. We conduct extensive evaluations on 6 LLMs and
5 LRMs of varying scales, revealing common failure patterns such as verbosity
and repetition. With its well-defined structure and quantitative evaluation
metrics, this work presents the first empirical investigation into the spatial
computational limitations of LLMs and LRMs, offering a new framework for
evaluating their advanced reasoning capabilities. Our code is available at
https://github.com/hyundong98/RegexPSPACE .

</details>


### [38] [Fundamentals of Building Autonomous LLM Agents](https://arxiv.org/abs/2510.09244)
*Victor de Lamo Castrillo,Habtom Kahsay Gidey,Alexander Lenz,Alois Knoll*

Main category: cs.AI

TL;DR: 本文综述了基于大语言模型的智能体架构与实现方法，探讨如何构建能够自动化复杂任务、缩小与人类能力差距的"智能"LLM。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在现实任务中存在局限性，研究旨在探索开发能够自动化复杂任务、缩小与人类能力差距的智能LLM模式。

Method: 构建包含四个关键组件的智能体系统：感知系统将环境感知转换为有意义的表示；推理系统通过思维链、思维树等技术制定计划、适应反馈和评估行动；记忆系统通过短期和长期机制保留知识；执行系统将内部决策转化为具体行动。

Result: 研究表明，整合这些系统可以产生更强大和通用的软件机器人，能够模拟人类认知过程实现自主智能行为。

Conclusion: 通过集成感知、推理、记忆和执行系统，可以构建出能够模拟人类认知过程的智能LLM智能体，实现自主和智能行为。

Abstract: This paper reviews the architecture and implementation methods of agents
powered by large language models (LLMs). Motivated by the limitations of
traditional LLMs in real-world tasks, the research aims to explore patterns to
develop "agentic" LLMs that can automate complex tasks and bridge the
performance gap with human capabilities. Key components include a perception
system that converts environmental percepts into meaningful representations; a
reasoning system that formulates plans, adapts to feedback, and evaluates
actions through different techniques like Chain-of-Thought and Tree-of-Thought;
a memory system that retains knowledge through both short-term and long-term
mechanisms; and an execution system that translates internal decisions into
concrete actions. This paper shows how integrating these systems leads to more
capable and generalized software bots that mimic human cognitive processes for
autonomous and intelligent behavior.

</details>


### [39] [Agentic Systems in Radiology: Design, Applications, Evaluation, and Challenges](https://arxiv.org/abs/2510.09404)
*Christian Bluethgen,Dave Van Veen,Daniel Truhn,Jakob Nikolas Kather,Michael Moor,Malgorzata Polacin,Akshay Chaudhari,Thomas Frauenfelder,Curtis P. Langlotz,Michael Krauthammer,Farhad Nooralahzadeh*

Main category: cs.AI

TL;DR: 该论文探讨了基于大语言模型（LLMs）的智能体系统在放射学领域的应用，重点研究了如何通过外部工具和反馈机制增强LLMs能力，以支持复杂的多步骤工作流程。


<details>
  <summary>Details</summary>
Motivation: 放射学具有多模态数据流和跨系统协调工作流程的特点，非常适合应用能够适应上下文并自动化复杂任务的智能体系统。虽然LLMs在单个任务上表现良好，但单独使用无法充分发挥其在复杂多步骤工作流程中的潜力。

Method: 通过为LLMs配备外部工具和反馈机制，构建能够展现不同程度自主性的系统，从半自动化工作流程到能够管理复杂流程的自适应智能体。

Result: LLM驱动的智能体系统在放射学领域显示出应用潜力，能够处理信息提取、报告总结等任务，并支持更复杂的工作流程。

Conclusion: 该综述分析了LLM驱动智能体系统的设计、关键应用、评估方法，并指出了错误级联、工具使用效率和健康IT集成等挑战。

Abstract: Building agents, systems that perceive and act upon their environment with a
degree of autonomy, has long been a focus of AI research. This pursuit has
recently become vastly more practical with the emergence of large language
models (LLMs) capable of using natural language to integrate information,
follow instructions, and perform forms of "reasoning" and planning across a
wide range of tasks. With its multimodal data streams and orchestrated
workflows spanning multiple systems, radiology is uniquely suited to benefit
from agents that can adapt to context and automate repetitive yet complex
tasks. In radiology, LLMs and their multimodal variants have already
demonstrated promising performance for individual tasks such as information
extraction and report summarization. However, using LLMs in isolation
underutilizes their potential to support complex, multi-step workflows where
decisions depend on evolving context from multiple information sources.
Equipping LLMs with external tools and feedback mechanisms enables them to
drive systems that exhibit a spectrum of autonomy, ranging from semi-automated
workflows to more adaptive agents capable of managing complex processes. This
review examines the design of such LLM-driven agentic systems, highlights key
applications, discusses evaluation methods for planning and tool use, and
outlines challenges such as error cascades, tool-use efficiency, and health IT
integration.

</details>


### [40] [Safe, Untrusted, "Proof-Carrying" AI Agents: toward the agentic lakehouse](https://arxiv.org/abs/2510.09567)
*Jacopo Tagliabue,Ciro Greco*

Main category: cs.AI

TL;DR: 论文提出使用API优先、可编程的数据湖仓作为安全设计的基础设施，支持AI代理工作流。通过Bauplan案例研究，展示了数据分支和声明式环境如何自然扩展到代理，实现可重现性和可观测性，同时减少攻击面。


<details>
  <summary>Details</summary>
Motivation: 数据湖仓运行敏感工作负载，AI驱动的自动化引发了关于信任、正确性和治理的担忧。需要找到既能利用AI自动化优势又能确保安全的方法。

Method: 使用Bauplan作为案例研究，采用数据分支和声明式环境来扩展代理能力。提出了一个概念验证，让代理使用受证明携带代码启发的正确性检查来修复数据管道。

Result: 原型演示表明，不受信任的AI代理可以在生产数据上安全操作，并为实现完全代理化湖仓指明了路径。

Conclusion: API优先、可编程的湖仓为安全设计的代理工作流提供了合适的抽象，能够实现可重现性、可观测性，同时降低安全风险。

Abstract: Data lakehouses run sensitive workloads, where AI-driven automation raises
concerns about trust, correctness, and governance. We argue that API-first,
programmable lakehouses provide the right abstractions for safe-by-design,
agentic workflows. Using Bauplan as a case study, we show how data branching
and declarative environments extend naturally to agents, enabling
reproducibility and observability while reducing the attack surface. We present
a proof-of-concept in which agents repair data pipelines using correctness
checks inspired by proof-carrying code. Our prototype demonstrates that
untrusted AI agents can operate safely on production data and outlines a path
toward a fully agentic lakehouse.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [41] [Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model](https://arxiv.org/abs/2510.08610)
*Imranur Rahman,Md Rayhanur Rahman*

Main category: cs.SE

TL;DR: 提出了一种有效的上下文收集策略，通过代码分块和相对定位来提升LLM在代码补全任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏确定什么构成良好的代码补全上下文，以帮助LLM在代码补全任务中表现更好

Method: 将代码库预处理为更小的代码块，然后使用基于句法和语义相似性的代码块检索与相对定位

Result: 代码分块和代码块在最终上下文中的相对定位提高了代码补全任务的性能

Conclusion: 提出的上下文收集策略能有效提升LLM在代码补全中的表现

Abstract: Code completion can help developers improve efficiency and ease the
development lifecycle. Although code completion is available in modern
integrated development environments (IDEs), research lacks in determining what
makes a good context for code completion based on the information available to
the IDEs for the large language models (LLMs) to perform better. In this paper,
we describe an effective context collection strategy to assist the LLMs in
performing better at code completion tasks. The key idea of our strategy is to
preprocess the repository into smaller code chunks and later use syntactic and
semantic similarity-based code chunk retrieval with relative positioning. We
found that code chunking and relative positioning of the chunks in the final
context improve the performance of code completion tasks.

</details>


### [42] [Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools](https://arxiv.org/abs/2510.08640)
*Ha Min Son,Huan Ren,Xin Liu,Zhe Zhao*

Main category: cs.SE

TL;DR: 提出了AndroidBuildBench基准测试和GradleFixer工具，专门用于修复Android构建错误，通过领域特定工具显著提升了LLM在构建修复任务中的表现。


<details>
  <summary>Details</summary>
Motivation: Android作为最大的移动平台，自动构建应用仍面临挑战。虽然LLM在代码修复方面有潜力，但在修复Android构建错误方面的应用仍未被充分探索。

Method: 首先创建AndroidBuildBench基准测试（包含1,019个构建失败案例），然后提出GradleFixer——一个配备领域特定工具的LLM代理，用于检查和操作Gradle构建环境。

Result: GradleFixer实现了81.4%的解决率（pass@1），显著优于依赖通用shell的最先进编码代理。

Conclusion: LLM具备解决构建失败的高层知识，但难以通过通用shell将其转化为有效的底层操作。工具桥接策略通过提供API式工具和约束操作空间，弥合了高层推理与底层执行之间的差距。

Abstract: Android is the largest mobile platform, yet automatically building
applications remains a practical challenge. While Large Language Models (LLMs)
show promise for code repair, their use for fixing Android build errors remains
underexplored. To address this gap, we first introduce AndroidBuildBench, a
benchmark of 1,019 build failures curated from the commit histories of 43
open-source Android projects. Each problem is paired with a verified solution
from a subsequent commit, ensuring that fixes are feasible. Second, we propose
GradleFixer, an LLM agent with domain-specific tools for inspecting and
manipulating the Gradle build environment. GradleFixer achieves a resolve rate
of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent
that relies on a general-purpose shell. GradleFixer's success suggests that
while LLMs possess the high-level knowledge to solve these failures, they
struggle to translate this knowledge into effective low-level actions using a
general-purpose shell. We demonstrate the effectiveness of a strategy we term
Tool Bridging, which replaces general-purpose shell commands with domain-aware
abstractions. We hypothesize this approach works through two mechanisms: 1) it
provides tools in an API-like format that LLMs use more reliably, and 2) it
constrains the action space to relevant operations. This approach bridges the
gap between the model's high-level reasoning and effective low-level execution.

</details>


### [43] [Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware](https://arxiv.org/abs/2510.08664)
*Jianan Mu,Mingyu Shi,Yining Wang,Tianmeng Yang,Bin Sun,Xing Hu,Jing Ye,Huawei Li*

Main category: cs.SE

TL;DR: 提出Faver方法，通过函数抽象的可验证中间件简化基于LLM的RTL生成流程，将电路验证细节与功能实现解耦，提高生成准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在RTL生成中面临语义鸿沟大、训练数据有限的问题，导致生成准确性不足。借鉴人类设计经验，需要结合验证来提升准确性，但RTL测试数据稀缺且LLM对高级语言更擅长。

Method: 提出Faver方法，混合LLM友好的代码结构和基于规则的模板，将电路验证细节与功能实现解耦，让LLM专注于功能本身。

Result: 在SFT模型和开源模型上的实验表明，Faver将模型生成准确性提升了最高14%。

Conclusion: Faver方法有效解决了LLM在RTL生成中的语义鸿沟问题，通过验证中间件显著提高了生成质量。

Abstract: LLM-based RTL generation is an interesting research direction, as it holds
the potential to liberate the least automated stage in the current chip design.
However, due to the substantial semantic gap between high-level specifications
and RTL, coupled with limited training data, existing models struggle with
generation accuracy. Drawing on human experience, design with verification
helps improving accuracy. However, as the RTL testbench data are even more
scarce, it is not friendly for LLMs. Although LLMs excel at higher-level
languages like Python/C, they have a huge semantic gap from RTL. When
implementing the same functionality, Python/C code and hardware code differ
significantly in the spatiotemporal granularity, requiring the LLM not only to
consider high-level functional semantics but also to ensure the low-level
details align with the circuit code. It is not an easy task. In this paper, we
propose a function abstracted verifiable middleware (Faver) that streamlines
RTL verification in LLM-based workflows. By mixing LLM-friendly code structures
with a rule-based template, Faver decouples the details of circuit
verification, allowing the LLM to focus on the functionality itself. In our
experiments on the SFT model and open-source models, Faver improved the model's
generation accuracy by up to 14%.

</details>


### [44] [RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution](https://arxiv.org/abs/2510.08665)
*Aofan Liu,Haoxuan Li,Bin Wang,Ao Yang,Hui Li*

Main category: cs.SE

TL;DR: 提出基于ReAct范式的可控代码生成多智能体框架，通过四个专门智能体的协作实现高效、精确和可解释的代码生成，在安全性和准确性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成模型在安全性、准确性和可控性方面存在不足，特别是缺乏动态外部工具集成、透明推理和用户安全控制。

Method: 采用多智能体系统架构，包含四个专门智能体：任务分解规划器、基于ReAct的搜索器、代码生成器和结构化数据提取器，通过动态交互实现代码生成。

Result: 在多语言测试中表现优异，在SVEN数据集上达到94.8%的安全率（使用CodeQL），优于现有方法。

Conclusion: 该框架通过透明推理过程增强用户信任，提高可控性，为复杂代码生成任务提供了有效的解决方案。

Abstract: Code generation models based on large language models (LLMs) have gained wide
adoption, but challenges remain in ensuring safety, accuracy, and
controllability, especially for complex tasks. Existing methods often lack
dynamic integration of external tools, transparent reasoning, and user control
over safety. To address these issues, we propose a controllable code generation
framework utilizing the ReAct paradigm for multi-agent task execution. This
framework is a multi-agent system designed to enable efficient, precise, and
interpretable code generation through dynamic interactions between LLMs and
external resources. The framework adopts a collaborative architecture
comprising four specialized agents: a Planner for task decomposition, a
Searcher that leverages the ReAct framework for reasoning and tool integration,
a CodeGen agent for accurate code generation, and an Extractor for structured
data retrieval. The ReAct-based Searcher alternates between generating
reasoning traces and executing actions, facilitating seamless integration of
internal knowledge with external tools (such as search engines) to enhance
accuracy and user control. Experimental results show the framework's
effectiveness across multiple languages, achieving a 94.8% security rate on the
SVEN dataset with CodeQL, outperforming existing approaches. Its transparent
reasoning process fosters user trust and improves controllability.

</details>


### [45] [RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data](https://arxiv.org/abs/2510.08667)
*Mohammad Baqar*

Main category: cs.SE

TL;DR: 提出了一个基于检索增强生成(RAG)的框架，用于整合JIRA工单、开发者讨论和GitHub PR中的碎片化知识，为软件团队提供上下文感知的问题解决建议。


<details>
  <summary>Details</summary>
Motivation: 现代软件团队在处理重复或相关问题时常因知识分散在JIRA工单、开发者讨论和GitHub PR中而遇到延迟，需要一个统一的知识整合和检索系统来加速问题解决。

Method: 使用Sentence-Transformers生成语义嵌入，结合FAISS向量搜索构建RAG框架，嵌入历史JIRA工单、用户评论和关联PR元数据，检索语义相似的过往案例，然后通过LLM合成基于证据的解决方案建议。

Result: 实验评估显示，该系统在精确率、召回率、解决时间减少和开发者接受度等指标上显著提高了解决准确性、修复质量和知识重用。

Conclusion: 该框架通过统一的JIRA和GitHub数据管道、异构软件工件的嵌入和索引策略，以及基于检索证据的解决方案生成模块，有效改善了现代DevOps环境中的问题解决效率。

Abstract: Modern software teams frequently encounter delays in resolving recurring or
related issues due to fragmented knowledge scattered across JIRA tickets,
developer discussions, and GitHub pull requests (PRs). To address this
challenge, we propose a Retrieval-Augmented Generation (RAG) framework that
integrates Sentence-Transformers for semantic embeddings with FAISS-based
vector search to deliver context-aware ticket resolution recommendations. The
approach embeds historical JIRA tickets, user comments, and linked PR metadata
to retrieve semantically similar past cases, which are then synthesized by a
Large Language Model (LLM) into grounded and explainable resolution
suggestions. The framework contributes a unified pipeline linking JIRA and
GitHub data, an embedding and FAISS indexing strategy for heterogeneous
software artifacts, and a resolution generation module guided by retrieved
evidence. Experimental evaluation using precision, recall, resolution time
reduction, and developer acceptance metrics shows that the proposed system
significantly improves resolution accuracy, fix quality, and knowledge reuse in
modern DevOps environments.

</details>


### [46] [BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution](https://arxiv.org/abs/2510.08697)
*Terry Yue Zhuo,Xiaolong Jin,Hange Liu,Juyong Jiang,Tianyang Liu,Chen Gong,Bhupesh Bishnoi,Vaisakhi Mishra,Marek Suppa,Noah Ziems,Saiteja Utpala,Ming Xu,Guangyu Song,Kaixin Li,Yuhan Cao,Bo Liu,Zheng Liu,Sabina Abdurakhmanova,Wenhao Yu,Mengzhao Jia,Jihan Yao,Kenneth Hamilton,Kumar Shridhar,Minh Chien Vu,Dingmin Wang,Jiawei Liu,Zijian Wang,Qian Liu,Binyuan Hui,Meg Risdal,Ahsen Khaliq,Atin Sood,Zhenchang Xing,Wasi Uddin Ahmad,John Grundy,David Lo,Banghua Zhu,Xiaoning Du,Torsten Scholak,Leandro von Werra*

Main category: cs.SE

TL;DR: BigCodeArena是一个基于Chatbot Arena构建的代码生成人类评估平台，提供实时执行环境来评估LLM生成的代码质量，收集了超过14,000个代码对话会话，并基于这些数据开发了BigCodeReward和AutoCodeArena两个基准测试。


<details>
  <summary>Details</summary>
Motivation: 在代码领域，手动评估LLM生成内容的质量极具挑战性，需要理解原始代码并模拟执行过程，因此需要专门的评估平台。

Method: 构建BigCodeArena平台，支持LLM生成代码的执行和人类交互，收集多轮对话样本，开发BigCodeReward评估奖励模型与人类偏好的一致性，以及AutoCodeArena自动Elo评级基准。

Result: 收集了14,000+代码对话，识别了4,700+带有人类偏好的多轮样本，发现当执行结果可用时，大多数LLM在判断编码偏好方面表现优异，GPT-5、Claude-Sonnet-4和Claude-Opus-4在代码生成性能上领先。

Conclusion: BigCodeArena为代码生成评估提供了有效的平台，基于执行结果的评估能更好地反映LLM的代码能力，专有LLM在代码生成方面仍保持领先地位。

Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable
real-time evaluation from human perspectives to assess the quality of model
responses. In the coding domain, manually examining the quality of
LLM-generated content is extremely challenging, as it requires understanding
long chunks of raw code and deliberately simulating code execution. To this
end, we introduce BigCodeArena, an open human evaluation platform for code
generation backed by a comprehensive and on-the-fly execution environment.
Built on top of Chatbot Arena, BigCodeArena enables the execution of
LLM-generated code and allows humans to interact with the execution process and
outcomes. We collected over 14,000 raw code-centric conversation sessions
across 10 widely used LLMs, spanning 10 languages and 8 types of execution
environments. Among these conversations, we identified more than 4,700
multi-turn samples with pairwise human preferences. Further analysis uncovers
underexplored preferences of LLMs in fine-grained domains characterized by
tasks, languages, and frameworks. To systematically examine code understanding
and generation capabilities of frontier LLMs, we curated two benchmarks based
on the collected data, namely BigCodeReward and AutoCodeArena. For
BigCodeReward, we post-processed the 4,700 conversations and evaluated the
consistency between reward models and human preferences. The evaluation shows
that most LLMs have superior performance in judging coding preferences when the
execution results are available. Inspired by these findings, we propose
AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding
quality of LLMs without human involvement. We find that proprietary LLMs like
GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation
performance among recent emerging models.

</details>


### [47] [PyMigTool: a tool for end-to-end Python library migration](https://arxiv.org/abs/2510.08810)
*Mohayeminul Islam,Ajay Kumar Jha,May Mahmoud,Sarah Nadi*

Main category: cs.SE

TL;DR: 开发了PyMigTool，一个结合LLM、静态分析和动态分析的端到端Python库迁移工具，能够自动迁移任意功能相似的Python库之间的代码


<details>
  <summary>Details</summary>
Motivation: 手动库迁移耗时且易错，现有自动化技术大多只停留在API映射阶段或支持有限的库和代码转换

Method: 使用LLM作为主要迁移引擎，结合静态分析和动态分析，开发了PyMigTool命令行应用

Result: 在717个真实Python应用上评估，PyMigTool能完全正确迁移32%的案例，剩余案例中超过一半项目只有14%的迁移相关变更需要开发者修复

Conclusion: LLM能有效执行库迁移，结合后处理步骤可进一步提升性能，PyMigTool为Python库迁移提供了实用的端到端解决方案

Abstract: Library migration is the process of replacing a library with a similar one in
a software project. Manual library migration is time consuming and error prone,
as it requires developers to understand the Application Programming Interfaces
(API) of both libraries, map equivalent APIs, and perform the necessary code
transformations. Due to the difficulty of the library migration process, most
of the existing automated techniques and tooling stop at the API mapping stage
or support a limited set of libraries and code transformations. In this paper,
we develop an end-to-end solution that can automatically migrate code between
any arbitrary pair of Python libraries that provide similar functionality. Due
to the promising capabilities of Large Language Models (LLMs) in code
generation and transformation, we use LLMs as the primary engine for migration.
Before building the tool, we first study the capabilities of LLMs for library
migration on a benchmark of 321 real-world library migrations. We find that
LLMs can effectively perform library migration, but some post-processing steps
can further improve the performance. Based on this, we develop PyMigTool, a
command line application that combines the power of LLMs, static analysis, and
dynamic analysis to provide accurate library migration. We evaluate PyMigTool
on 717 real-world Python applications that are not from our benchmark. We find
that PyMigTool can migrate 32% of the migrations with complete correctness. Of
the remaining migrations, only 14% of the migration-related changes are left
for developers to fix for more than half of the projects.

</details>


### [48] [Repository-Aware File Path Retrieval via Fine-Tuned LLMs](https://arxiv.org/abs/2510.08850)
*Vasudha Yanuganti,Ishaan Puri,Swapnil Chhatre,Mantinder Singh,Ashok Jallepalli,Hritvik Shrivastava,Pradeep Kumar Sharma*

Main category: cs.SE

TL;DR: 提出一种基于QLoRA和Unsloth优化的文件路径检索方法，通过微调Qwen3-8B模型直接从自然语言查询预测相关文件路径，在多个Python项目中达到高检索准确率。


<details>
  <summary>Details</summary>
Motivation: 传统代码搜索方法缺乏语义上下文和跨文件链接理解，而大语言模型虽然理解自然语言但缺乏仓库特定细节，需要一种能结合两者优势的文件检索方法。

Method: 使用六种基于抽象语法树和仓库内容的代码感知策略生成训练数据，通过QLoRA和Unsloth优化微调Qwen3-8B模型进行文件路径预测。

Result: 在Flask、Click等项目中达到91%精确匹配和93%召回率，在PyTorch大型代码库中达到59%召回率，明显优于单策略训练。

Conclusion: 多级代码信号帮助LLM推理跨文件上下文，该方法展示了良好的可扩展性，未来可与基于LLM的代码智能系统集成。

Abstract: Modern codebases make it hard for developers and AI coding assistants to find
the right source files when answering questions like "How does this feature
work?" or "Where was the bug introduced?" Traditional code search (keyword or
IR based) often misses semantic context and cross file links, while large
language models (LLMs) understand natural language but lack repository specific
detail. We present a method for file path retrieval that fine tunes a strong
LLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file
paths directly from a natural language query. To build training data, we
introduce six code aware strategies that use abstract syntax tree (AST)
structure and repository content to generate realistic question-answer pairs,
where answers are sets of file paths. The strategies range from single file
prompts to hierarchical repository summaries, providing broad coverage. We fine
tune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,
and obtain high retrieval accuracy: up to 91\% exact match and 93\% recall on
held out queries, clearly beating single strategy training. On a large codebase
like PyTorch (about 4,000 Python files), the model reaches 59\% recall, showing
scalability. We analyze how multi level code signals help the LLM reason over
cross file context and discuss dataset design, limits (for example, context
length in very large repos), and future integration of retrieval with LLM based
code intelligence.

</details>


### [49] [SEER: Sustainability Enhanced Engineering of Software Requirements](https://arxiv.org/abs/2510.08981)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: SEER是一个在软件开发早期阶段解决可持续性问题的框架，使用LLM推理能力和代理RAG方法，通过识别、评估和优化可持续性需求来提升软件可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有可持续性方法多为高层指南，实施耗时且依赖团队适应性，且主要关注设计或实现阶段，而可持续性评估应从需求工程阶段开始。

Method: SEER框架分三个阶段：从通用分类法中识别特定软件产品的可持续性需求；基于识别的SR评估系统需求的可持续性；优化未能满足任何SR的系统需求。使用LLM推理能力和代理RAG方法实现。

Result: 在四个不同领域的软件项目上实验，使用Gemini 2.5推理模型的结果表明，该方法能有效准确识别跨领域的广泛可持续性问题。

Conclusion: SEER框架在软件开发早期阶段有效解决了可持续性问题，能够准确识别和优化可持续性需求。

Abstract: The rapid expansion of software development has significant environmental,
technical, social, and economic impacts. Achieving the United Nations
Sustainable Development Goals by 2030 compels developers to adopt sustainable
practices. Existing methods mostly offer high-level guidelines, which are
time-consuming to implement and rely on team adaptability. Moreover, they focus
on design or implementation, while sustainability assessment should start at
the requirements engineering phase. In this paper, we introduce SEER, a
framework which addresses sustainability concerns in the early software
development phase. The framework operates in three stages: (i) it identifies
sustainability requirements (SRs) relevant to a specific software product from
a general taxonomy; (ii) it evaluates how sustainable system requirements are
based on the identified SRs; and (iii) it optimizes system requirements that
fail to satisfy any SR. The framework is implemented using the reasoning
capabilities of large language models and the agentic RAG (Retrieval Augmented
Generation) approach. SEER has been experimented on four software projects from
different domains. Results generated using Gemini 2.5 reasoning model
demonstrate the effectiveness of the proposed approach in accurately
identifying a broad range of sustainability concerns across diverse domains.

</details>


### [50] [Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation](https://arxiv.org/abs/2510.08996)
*Spandan Garg,Ben Steenhoek,Yufan Huang*

Main category: cs.SE

TL;DR: 论文提出了一个新的基准测试框架，将现有的正式基准转换为更真实的用户查询，发现现有基准显著高估了聊天式软件工程代理的能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于GitHub问题的软件工程代理评估基准无法准确反映开发者在IDE中与聊天式编码助手的实际交互方式，导致对代理在真实场景中能力的系统性高估。

Method: 通过分析开发者与聊天式代理的交互模式，开发了一个灵活的方法论，将现有正式基准转换为真实用户风格的查询，并将其应用于多个基准测试。

Result: 发现现有基准显著高估了代理能力，公开基准中某些模型的性能被高估超过50%，内部基准中高估约10-16%。

Conclusion: 这项工作通过基准变异技术为评估交互式聊天式软件工程代理建立了新范式。

Abstract: Current benchmarks for evaluating software engineering agents, such as
SWE-Bench Verified, are predominantly derived from GitHub issues and fail to
accurately reflect how developers interact with chat-based coding assistants in
integrated development environments (IDEs). We posit that this mismatch leads
to a systematic overestimation of agent's capabilities in real-world scenarios,
especially bug fixing. We introduce a novel benchmarking framework that
transforms existing formal benchmarks into realistic user queries through
systematic analysis of developer interaction patterns with chat-based agents.
Our methodology is flexible and can be easily extended to existing benchmarks.
In this paper, we apply our testing framework to SWE-Bench Verified, the
TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and
transform formal GitHub issue descriptions into realistic user-style queries
based on telemetry analysis of a popular chat-based agent interactions. Our
findings reveal that existing benchmarks significantly overestimate agent
capabilities for some models by >50% over baseline performance for public
benchmarks and ~10-16% for our internal benchmark. This work establishes a new
paradigm for evaluating interactive chat-based software engineering agents
through benchmark mutation techniques.

</details>


### [51] [Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements](https://arxiv.org/abs/2510.09045)
*Manojit Chakraborty,Madhusudan Ghosh,Rishabh Gupta*

Main category: cs.SE

TL;DR: 提出了一种零样本代码翻译方法，通过将长标识符替换为通用占位符来减少token数量，提高长代码翻译的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理超出上下文窗口的长源代码时翻译不准确，需要解决长代码翻译的效率问题。

Method: 使用标识符替换技术，将用户给定的长标识符替换为通用占位符，让LLM专注于代码逻辑结构。

Result: 该方法保留了语法和层次结构信息，减少了翻译结果中的token数量。

Conclusion: 提出的标识符替换方法有效提高了长代码翻译的效率和成本效益。

Abstract: In the domain of software development, LLMs have been utilized to automate
tasks such as code translation, where source code from one programming language
is translated to another while preserving its functionality. However, LLMs
often struggle with long source codes that don't fit into the context window,
which produces inaccurate translations. To address this, we propose a novel
zero-shot code translation method that incorporates identifier replacement. By
substituting user-given long identifiers with generalized placeholders during
translation, our method allows the LLM to focus on the logical structure of the
code, by reducing token count and memory usage, which improves the efficiency
and cost-effectiveness of long code translation. Our empirical results
demonstrate that our approach preserves syntactical and hierarchical
information and produces translation results with reduced tokens.

</details>


### [52] [Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding](https://arxiv.org/abs/2510.09058)
*Italo Santos,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 对131名软件从业者的全球调查显示，LLM在软件开发中被广泛用于编码任务，带来生产力提升和认知负荷减少，但也存在输出不准确、上下文理解有限等担忧。


<details>
  <summary>Details</summary>
Motivation: 了解LLM在实际软件开发中的使用情况以及专业人士对其优势和局限性的看法。

Method: 通过对全球131名软件从业者进行问卷调查，收集关于LLM使用情况的实证数据。

Result: 开发者主要将LLM用作辅助工具而非独立解决方案，报告了生产力提升、认知负荷减少等好处，但也担忧输出不准确、上下文理解有限和伦理风险。

Conclusion: LLM在软件工程中的应用需要谨慎而实用的方法，未来研究应关注负责任的使用。

Abstract: Large Language Models have quickly become a central component of modern
software development workflows, and software practitioners are increasingly
integrating LLMs into various stages of the software development lifecycle.
Despite the growing presence of LLMs, there is still a limited understanding of
how these tools are actually used in practice and how professionals perceive
their benefits and limitations. This paper presents preliminary findings from a
global survey of 131 software practitioners. Our results reveal how LLMs are
utilized for various coding-specific tasks. Software professionals report
benefits such as increased productivity, reduced cognitive load, and faster
learning, but also raise concerns about LLMs' inaccurate outputs, limited
context awareness, and associated ethical risks. Most developers treat LLMs as
assistive tools rather than standalone solutions, reflecting a cautious yet
practical approach to their integration. Our findings provide an early,
practitioner-focused perspective on LLM adoption, highlighting key
considerations for future research and responsible use in software engineering.

</details>


### [53] [Literate Tracing](https://arxiv.org/abs/2510.09073)
*Matthew Sotoudeh*

Main category: cs.SE

TL;DR: 提出了一种称为'文学化追踪'的程序文档范式，通过带注释的具体执行追踪来解释软件系统，并开发了TReX工具来创建交互式、可视化的文学化追踪。


<details>
  <summary>Details</summary>
Motivation: 解决大型复杂软件系统中，系统专家向新手解释程序工作原理时，现有文档方法（代码注释缺乏全局上下文，设计文档缺乏与代码的具体连接）的不足。

Method: 开发了TReX工具，通过生成交互式、可视化的执行追踪来创建文学化追踪文档，这些追踪在构造上保证与程序语义的一致性。

Result: 使用TReX工具成功为Linux内核、Git源代码控制系统和GCC编译器等大型系统软件组件编写了文学化追踪文档。

Conclusion: 文学化追踪作为一种程序文档范式，能够有效补充现有文档方法的不足，提供更直观和具体的程序理解方式。

Abstract: As computer systems grow ever larger and more complex, a crucial task in
software development is for one person (the system expert) to communicate to
another (the system novice) how a certain program works. This paper reports on
the author's experiences with a paradigm for program documentation that we call
literate tracing. A literate trace explains a software system using annotated,
concrete execution traces of the system. Literate traces complement both
in-code comments (which often lack global context) and out-of-band design docs
(which often lack a concrete connection to the code). We also describe TReX,
our tool for making literate traces that are interactive, visual, and
guaranteed by construction to be faithful to the program semantics. We have
used TReX to write literate traces explaining components of large systems
software including the Linux kernel, Git source control system, and GCC
compiler.

</details>


### [54] [Constraint-Guided Unit Test Generation for Machine Learning Libraries](https://arxiv.org/abs/2510.09108)
*Lukas Krodinger,Altin Hajdari,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: PynguinML通过从API文档中提取约束条件，改进Pynguin测试生成器，为机器学习API生成合规输入，显著提高代码覆盖率


<details>
  <summary>Details</summary>
Motivation: 机器学习库的API通常有严格的输入约束，涉及张量等复杂数据结构。现有测试生成工具不了解这些约束，导致测试失败和代码覆盖率有限

Method: 基于Pynguin测试生成器，从官方API文档中提取约束条件，利用这些约束生成符合要求的输入数据

Result: 在PyTorch和TensorFlow的165个模块上评估，相比Pynguin，PynguinML显著提高测试效果，代码覆盖率最高提升63.9%

Conclusion: 通过利用API文档中的约束条件，PynguinML能够更有效地测试机器学习库，生成合规输入并提高代码覆盖率

Abstract: Machine learning (ML) libraries such as PyTorch and TensorFlow are essential
for a wide range of modern applications. Ensuring the correctness of ML
libraries through testing is crucial. However, ML APIs often impose strict
input constraints involving complex data structures such as tensors. Automated
test generation tools such as Pynguin are not aware of these constraints and
often create non-compliant inputs. This leads to early test failures and
limited code coverage. Prior work has investigated extracting constraints from
official API documentation. In this paper, we present PynguinML, an approach
that improves the Pynguin test generator to leverage these constraints to
generate compliant inputs for ML APIs, enabling more thorough testing and
higher code coverage. Our evaluation is based on 165 modules from PyTorch and
TensorFlow, comparing PynguinML against Pynguin. The results show that
PynguinML significantly improves test effectiveness, achieving up to 63.9 %
higher code coverage.

</details>


### [55] [TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation](https://arxiv.org/abs/2510.09400)
*He Jiang,Yufu Wang,Hao Lin,Peiyu Zou,Zhide Zhou,Ang Jia,Xiaochen Li,Zhilei Ren*

Main category: cs.SE

TL;DR: 提出TIT方法，通过树结构指令调优解决LLM代码翻译中的语法混淆和语义对齐问题，显著提升翻译成功率


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码翻译方法对语言特定特征敏感，容易引入源语言语法，且缺乏细粒度语义对齐，导致语法混淆和语义偏差

Method: TIT包含三个模块：语法信息表示模块集成语言无关语法特征；细粒度并行数据增强模块通过语句级分割和对齐匹配生成高质量数据；双阶段树指令调优模块减轻LLM上下文处理负担

Result: 在多个LLM中显著优于现有方法，代码翻译成功率提高1.22-1.75倍，同时大幅减少语法混淆

Conclusion: TIT方法有效解决了LLM代码翻译中的关键限制，通过结构化语法表示和细粒度对齐提升了翻译质量和准确性

Abstract: Large Language Models (LLMs) have shown strong performance in automated
source-to-target code translation through pretraining on extensive code
corpora. However, mainstream LLM-based code translation methods suffer from two
critical limitations. First, they are highly sensitive to language-specific
features, which often introduce source-language syntax or lexicon into the
output, leading to syntactic confusion. Second, they lack fine-grained semantic
alignment due to an over-reliance on function-level parallel datasets,
resulting in semantic misalignment between the translated code and the original
source. To overcome these limitations, we propose TIT, a Tree-structured
Instruction Tuning paradigm for LLM-based code translation. Specifically, TIT
consists of three modules. First, to mitigate syntactic confusion, the
syntactic information representation module integrates language-agnostic
syntactic features via structured parsing. Then, to generate high-quality
fine-grained parallel data, the fine-grained parallel dataset augmentation
module aligns nodes with code segments through statement-level segmentation and
contrastive matching. Finally, we leverage the dual-stage tree instruction
tuning module to alleviate the contextual processing burden on the LLM caused
by the introduction of syntactic information. The first stage employs
syntax-aware fine-tuning to enable the LLM to autonomously comprehend
structured syntactic information, while the second stage utilizes code
generation fine-tuning to guide the model in generating accurate target code
based on function-level syntactic dependencies. The experimental results
demonstrate that the proposed method significantly outperforms existing
approaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in
code translation while markedly reducing syntactic confusion.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [56] [Six Winning Figma Makes—and What You Can Learn from Them](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2F6-winning-figma-makes-and-what-you-can-learn-from-them%2F%3Futm_source=tldrdesign/1/01000199c8e6cbec-3225094e-4f5f-4637-9009-3def68d11f13-000000/ooycE_XoUJHDDYjhZz2lK1lTHRfqwSm11U-mMcSoL9c=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Figma首届全球Make-a-thon吸引了超过10,000名创作者参与，展示了通过明确结构要求、设计优先、模块化代码组织和简短提示等策略来连接设计与开发的能力。


<details>
  <summary>Details</summary>
Motivation: 展示Figma Make平台如何通过竞赛形式促进设计与开发的融合，并分享获胜者的成功策略。

Method: 分析Figma首届全球Make-a-thon竞赛中获胜项目的策略和方法，包括结构要求定义、设计优化、代码组织和提示工程。

Result: 竞赛展示了Figma Make能够有效连接设计和开发，获胜者通过清晰的结构规划、设计优先方法、模块化代码和简短提示实现了创新项目。

Conclusion: Figma Make-a-thon证明了该平台在弥合设计与开发鸿沟方面的潜力，获胜策略为其他创作者提供了有价值的参考。

Abstract: Six Winning Figma Makes—and What You Can Learn from Them (7 minute read) Figma's first global Make-a-thon attracted over 10,000 creators competing for $100,000 in prizes. Showcasing innovative projects, winners emphasized strategies such as defining clear structural requirements, refining designs before building, utilizing modular code organization, and leveraging shorter prompts for easier iteration. The competition demonstrated Figma Make's capability to bridge design and development, with ...

</details>


### [57] [Designing Agentic Loops](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FSep%2F30%2Fdesigning-agentic-loops%2F%3Futm_source=tldrdesign/1/01000199c8e6cbec-3225094e-4f5f-4637-9009-3def68d11f13-000000/KQqYANJ-gWk893JJjpxiDaMfijWiD9muakaSXUhbRtY=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该论文讨论了如何设计有效的智能体循环，让编码智能体如Claude Code能够通过循环运行工具来暴力解决问题，但需要精心设计包含适当工具和凭证的智能体循环才能有效工作。


<details>
  <summary>Details</summary>
Motivation: 编码智能体虽然能够通过循环运行工具来解决问题，但需要设计有效的智能体循环机制来平衡生产力和安全性，特别是在处理具有明确成功标准的问题时。

Method: 在沙盒环境（如GitHub Codespaces）中运行智能体的YOLO模式（自动批准命令），这种方法结合了适当的安全措施来管理数据丢失和数据泄露等风险。

Result: 该方法在处理具有明确成功标准的问题时效果最佳，能够在保证安全的同时提高生产力。

Conclusion: 精心设计的智能体循环结合沙盒环境中的YOLO模式运行，是平衡编码智能体生产力和安全风险的有效方法。

Abstract: Designing Agentic Loops (8 minute read) Coding agents like Claude Code can brute force solutions by running tools in loops, but require carefully designed "agentic loops" with appropriate tools and credentials to be effective. Running agents in YOLO mode (auto-approving commands) within sandboxed environments like GitHub Codespaces balances productivity with security risks, including data loss and exfiltration attacks. This approach works best for problems with clear success criteria involvin...

</details>


### [58] [Intercom's Fin AI agent achieves 70% resolution rate for finserv support tickets](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffin.ai%2Fsolutions%2Ffinancial-services%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=f_tldr_newsletter_bmkt_ba_awar_pros_3p_fintech-vertical_namer_us_en_newsletter/2/01000199c9206c6b-58735486-11c8-4a20-b8da-e9982b0c67cb-000000/xecy3Comw-CMJA08f6QLSNxD2ICyDfF4f7YerIfUQL8=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Intercom的Fin AI代理在金融服务支持工单中达到70%解决率，专为受监管的金融服务公司设计


<details>
  <summary>Details</summary>
Motivation: 为金融服务行业提供专门定制的AI代理解决方案，解决通用聊天机器人在受监管行业中的局限性

Method: 开发专为金融服务设计的AI代理系统，支持业务规则遵循、完整审计追踪和与任何帮助台集成

Result: 在金融服务支持工单中实现70%的解决率，被Moneybox、Checkout.com和Polymarket等公司采用

Conclusion: Fin是行业中唯一完整、完全可配置的AI代理系统，专为受监管的金融服务公司量身定制

Abstract: Intercom's Fin AI agent achieves 70% resolution rate for finserv support tickets (Sponsor) Moneybox, Checkout .com, and Polymarket all use the best-performing AI agent for financial services — Fin. Made by Intercom, Fin: >> follows your business rules, >> maintains full audit trails, and >> integrates with any helpdesk. Unlike generic chatbots, Fin was built for regulated financial services companies. It's the only complete, fully configurable AI agent system in the industry. Try the #1 agent...

</details>


### [59] [Cracking the Code of Browser Automation for AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flabs.amazon.science%2Fblog%2Fwhat-makes-browser-use-hard-for-ai-agents%3Futm_campaign=TLDR_labs_blog%26utm_medium=employer-brand%26utm_source=TLDR%26utm_browser_use_hard_for_AI_agents%26utm_term=2025-october/1/01000199c920b810-e2958e8a-de32-4c7b-b730-7c28c78fa479-000000/0vEsuaEpmZj55DQaxyenTg3TTrV7Y0g1ah4WQqSKZbI=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 亚马逊AGI实验室开发了革命性的浏览器自动化技术，使AI代理在早期企业用例中达到90%以上的可靠性，结合技术卓越和以人为中心的设计。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理在浏览器自动化中面临的挑战，推动AI与网络界面的交互发展。

Method: 结合技术卓越和以人为中心的设计方法，开发浏览器自动化解决方案。

Result: 在早期企业用例中实现了90%以上的可靠性，为AI与网络界面交互设定了新标准。

Conclusion: 该技术将影响AI的未来发展，团队正在招募人才。

Abstract: Cracking the Code of Browser Automation for AI (Sponsor) Discover how Amazon's AGI Lab is revolutionizing browser automation for AI agents, achieving 90%+ reliability in early enterprise use cases. Our approach combines technical excellence with human-centric design, setting new standards for AI interaction with web interfaces.What makes browser use hard for AI Agents? → Impact the future of AI, today. Join our team →

</details>


### [60] [Sweep's autocomplete for Jetbrains runs in <100ms. With full codebase context](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sweep.dev%2Fposts%2Fnext-edit-jetbrains%3Futm_source=tldr%26utm_medium=email/1/01000199cda59753-8622b292-4000-41fd-aefd-a78bc488e52d-000000/udwc2Xy1-pBQLbr5LnVkP-RB0iwuuCFh7dLGAex8ddI=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sweep为JetBrains IDE开发了快速AI自动补全功能，响应时间低于100毫秒，并提供基于代码库的智能编辑建议


<details>
  <summary>Details</summary>
Motivation: 解决传统IDE自动补全功能响应慢、缺乏代码库上下文理解的问题，提供更智能高效的开发体验

Method: 从头构建语法感知的FIM（填充中间代码）技术，采用自定义推理栈，支持全代码库上下文

Result: 实现了JetBrains IDE中最快的自动补全功能，响应时间<100ms，并能基于代码库学习提供智能编辑建议

Conclusion: Sweep的AI自动补全技术显著提升了开发效率，为JetBrains用户提供了快速且上下文感知的编码辅助工具

Abstract: Sweep's autocomplete for Jetbrains runs in <100ms. With full codebase context (Sponsor) Sweep's AI autocomplete is the fastest autocomplete for JetBrains. Sweep's next-edit suggestions learn from your codebase to suggest changes anywhere in your code - while staying in your favorite IDEs like IntelliJ or PyCharm. Built from scratch with syntax-aware FIM on a custom inference stack. Get the Jetbrains plugin.

</details>


### [61] [Embracing the parallel coding agent lifestyle](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F5%2Fparallel-coding-agents%2F%3Futm_source=tldrnewsletter/1/01000199cda59753-8622b292-4000-41fd-aefd-a78bc488e52d-000000/35ussuSyB16-5Iaxgf_h6I58cdikwVMMgVrAkmcX8QA=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文讨论了并行运行多个编码代理的模式，这种新兴方法可以在不增加太多认知负担的情况下提高并行任务处理能力。


<details>
  <summary>Details</summary>
Motivation: 编码代理软件是新兴技术，模型在最近几个月才变得足够好来驱动它们。为了充分利用这种技术，需要探索如何并行运行多个编码代理来提高效率。

Method: 通过在同一代码库中运行多个编码代理，有时使用多个检出或git工作树，实现并行任务处理。

Result: 这种方法能够显著增加可以并行执行的任务数量，同时保持较低的认知负担。

Conclusion: 并行编码代理是一种有前景的工作模式，可以有效提升软件开发效率。

Abstract: Embracing the parallel coding agent lifestyle (7 minute read) Coding agent software is still really new. Models have only really become good enough to drive them in the past few months. One of the ways to use this software is to run multiple coding agents at once, sometimes in the same repo and against multiple checkouts or git work trees. This approach increases the number of tasks that can be fired off in parallel without adding too much cognitive overhead. This post discusses patterns for ...

</details>


### [62] [Embracing the parallel coding agent lifestyle](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F5%2Fparallel-coding-agents%2F%3Futm_source=tldrwebdev/1/01000199cdcff38a-cb6ce768-6648-4683-8afd-10a5e2be0b68-000000/SRCncSMsku4hBp-JvYdXkRh_qjPuAd7lGG4z9DA2F0A=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用多个并行编码代理可以提高生产力，特别适用于研究任务和小型维护任务。


<details>
  <summary>Details</summary>
Motivation: 探索并行编码代理方法如何提升开发效率，特别是在研究原型验证和代码理解方面。

Method: 采用多个编码代理并行工作的方法，通过精心指定的任务来优化工作流程。

Result: 并行编码代理方法能够提高生产力，产生更易于代码审查和缺陷更少的代码。

Conclusion: 并行编码代理方法是一种有效的生产力提升策略，特别适合研究任务和低风险维护工作。

Abstract: Embracing the parallel coding agent lifestyle (7 minute read) Using multiple coding agents in parallel can improve productivity. This approach is great for research tasks like proof-of-concepts and understanding existing code, as well as small, low-stakes maintenance tasks like fixing warnings. Carefully specified tasks result in easier code review and less buggy code overall.

</details>


### [63] [We Rebuilt Our Onboarding Around MCP](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.devcycle.com%2Fwe-rebuilt-our-onboarding-around-mcp-the-result-3x-sdk-installs%2F%3Futm_source=tldrfounders/1/01000199ce0583b7-8b085ca4-312e-4ac7-9d93-518f95603d65-000000/1thOBrYUqCruqMAJRlohNJEHrvq4CFquta04tfb2i08=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 通过将开发者引导流程集成到编辑器中，使用MCP协议让AI编码助手直接在编辑器中安装和配置SDK，显著提高了安装率。


<details>
  <summary>Details</summary>
Motivation: 传统开发者引导流程在用户看到真正价值之前就流失了用户，问题不在于动机而在于位置，开发者不愿意离开编辑器去尝试演示应用。

Method: 围绕模型上下文协议(MCP)重建引导流程，让AI编码助手如Cursor或VS Code直接在编辑器中安装和配置SDK。

Result: 两个提示现在取代了整个教程，安装率提高了三倍。

Conclusion: 将引导流程集成到开发者工作流中，直接在编辑器中完成配置，可以显著改善开发者体验和提高采用率。

Abstract: We Rebuilt Our Onboarding Around MCP (5 minute read) Most developer onboarding flows lose users before they ever see real value. DevCycle found the problem wasn't motivation, it was location. Developers don't want to leave their editors to try a demo app. So the team rebuilt onboarding around the Model-Context-Protocol (MCP), letting AI coding assistants like Cursor or VS Code install and configure the SDK directly in-editor. Two prompts now replace an entire tutorial, tripling install rates ...

</details>


### [64] [From Agent Hype to Agent Fatigue](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fproductpower.substack.com%2Fp%2Ffrom-agent-hype-to-agent-fatigue%3Futm_source=tldrfounders/1/01000199ce0583b7-8b085ca4-312e-4ac7-9d93-518f95603d65-000000/JXgB-v87tOcdydkL_yQxhsuaPmM98FH9aT93naWwVUk=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 从AI代理热潮到代理疲劳：2025年AI代理项目面临严峻挑战，40%的项目预计在2027年前失败，员工在使用不熟悉的工具时出现倦怠


<details>
  <summary>Details</summary>
Motivation: 分析AI代理领域的现状，揭示从过度炒作到现实挑战的转变，关注项目失败率和员工倦怠问题

Method: 基于行业趋势分析和预测数据，评估AI代理项目的实际实施情况

Result: 发现40%的AI代理项目预计在2027年前失败，员工在使用不熟悉的AI工具时出现严重倦怠

Conclusion: AI代理领域正从过度炒作转向现实挑战，需要更谨慎的实施策略和更好的员工培训

Abstract: From Agent Hype to Agent Fatigue (4 minute read) Everyone said 2025 would be the year of AI agents, and they were right, just not in the way they expected. Companies raced to rebrand every workflow as “agentic,” but the reality is showing cracks: 40% of projects are predicted to fail by 2027, and employees are burning out under the pressure to use tools they barely understand.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [65] [Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem](https://arxiv.org/abs/2510.08768)
*Francisco Pascoa,Ian Lalonde,Alexandre Girard*

Main category: cs.LG

TL;DR: 提出基于白金汉π定理的零样本迁移方法，通过无量纲空间缩放预训练RL策略的输入输出，无需重新训练即可适应新系统环境。


<details>
  <summary>Details</summary>
Motivation: 解决RL策略在面临不同物理参数的新机器人、任务或环境时泛化能力差的问题，提升其实际应用价值。

Method: 基于白金汉π定理，将预训练策略的观测值和动作映射到无量纲空间进行缩放，实现零样本迁移。

Result: 在三个复杂度递增的环境中验证：模拟钟摆、物理钟摆（sim-to-real验证）和高维HalfCheetah。缩放迁移在动态相似环境下无性能损失，在非相似环境下始终优于朴素迁移。

Conclusion: 量纲分析为增强RL策略的鲁棒性和泛化能力提供了强大实用的工具。

Abstract: Reinforcement learning (RL) policies often fail to generalize to new robots,
tasks, or environments with different physical parameters, a challenge that
limits their real-world applicability. This paper presents a simple, zero-shot
transfer method based on Buckingham's Pi Theorem to address this limitation.
The method adapts a pre-trained policy to new system contexts by scaling its
inputs (observations) and outputs (actions) through a dimensionless space,
requiring no retraining. The approach is evaluated against a naive transfer
baseline across three environments of increasing complexity: a simulated
pendulum, a physical pendulum for sim-to-real validation, and the
high-dimensional HalfCheetah. Results demonstrate that the scaled transfer
exhibits no loss of performance on dynamically similar contexts. Furthermore,
on non-similar contexts, the scaled policy consistently outperforms the naive
transfer, significantly expanding the volume of contexts where the original
policy remains effective. These findings demonstrate that dimensional analysis
provides a powerful and practical tool to enhance the robustness and
generalization of RL policies.

</details>


### [66] [Guiding Exploration in Reinforcement Learning Through LLM-Augmented Observations](https://arxiv.org/abs/2510.08779)
*Vaibhav Jain,Gerrit Grossmann*

Main category: cs.LG

TL;DR: 提出一个框架，通过增强观察空间提供LLM生成的动作推荐，让RL智能体学习何时遵循或忽略这些指导，在稀疏奖励环境中显著提升训练效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 解决RL智能体在稀疏奖励环境中探索困难的问题，利用LLM的程序性知识和推理能力来指导RL探索，同时避免现有方法中LLM建议与RL策略之间的刚性依赖关系。

Method: 通过增强观察空间提供LLM生成的动作推荐，让RL智能体学习何时遵循或忽略这些指导，保持软约束的灵活性，无需修改现有RL算法。

Result: 在三个复杂度递增的BabyAI环境中评估，LLM指导的益处随任务难度增加而增强。在最挑战的环境中，最终成功率相对基线提升71%，样本效率提升高达9倍。

Conclusion: 该方法有效利用LLM的规划能力来加速RL在挑战性环境中的训练，同时保持RL智能体的学习自主性。

Abstract: Reinforcement Learning (RL) agents often struggle in sparse-reward
environments where traditional exploration strategies fail to discover
effective action sequences. Large Language Models (LLMs) possess procedural
knowledge and reasoning capabilities from text pretraining that could guide RL
exploration, but existing approaches create rigid dependencies where RL
policies must follow LLM suggestions or incorporate them directly into reward
functions. We propose a framework that provides LLM-generated action
recommendations through augmented observation spaces, allowing RL agents to
learn when to follow or ignore this guidance. Our method leverages LLMs' world
knowledge and reasoning abilities while maintaining flexibility through soft
constraints. We evaluate our approach on three BabyAI environments of
increasing complexity and show that the benefits of LLM guidance scale with
task difficulty. In the most challenging environment, we achieve 71% relative
improvement in final success rates over baseline. The approach provides
substantial sample efficiency gains, with agents reaching performance
thresholds up to 9 times faster, and requires no modifications to existing RL
algorithms. Our results demonstrate an effective method for leveraging LLM
planning capabilities to accelerate RL training in challenging environments.

</details>


### [67] [Deceptive Exploration in Multi-armed Bandits](https://arxiv.org/abs/2510.08794)
*I. Arda Vurankaya,Mustafa O. Karabag,Wesley A. Suttle,Jesse Milzman,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 本文研究了一个多臂老虎机问题，其中每个臂有公开和私有奖励分布。一个欺骗性智能体试图在观察者不知情的情况下快速识别最佳私有臂，同时满足KL散度约束。


<details>
  <summary>Details</summary>
Motivation: 研究在存在观察者监督的情况下，智能体如何在不被发现的前提下最大化私有奖励的获取，这在隐私保护、安全系统等场景中具有实际意义。

Method: 将可检测性形式化为智能体实际拉取概率与观察者预期拉取概率之间的KL散度约束，建模公共次优臂的拉取为伯努利过程，提出基于top-two算法的启发式算法。

Result: 证明在KL约束下，公共次优臂的拉取最多以Θ(√T)的速率发生，并给出了最佳私有臂识别的最优错误指数。

Conclusion: 提出的算法能够根据公共次优性差距自适应调整探索策略，在满足检测约束的同时有效识别最佳私有臂。

Abstract: We consider a multi-armed bandit setting in which each arm has a public and a
private reward distribution. An observer expects an agent to follow Thompson
Sampling according to the public rewards, however, the deceptive agent aims to
quickly identify the best private arm without being noticed. The observer can
observe the public rewards and the pulled arms, but not the private rewards.
The agent, on the other hand, observes both the public and private rewards. We
formalize detectability as a stepwise Kullback-Leibler (KL) divergence
constraint between the actual pull probabilities used by the agent and the
anticipated pull probabilities by the observer. We model successful pulling of
public suboptimal arms as a % Bernoulli process where the success probability
decreases with each successful pull, and show these pulls can happen at most at
a $\Theta(\sqrt{T}) $ rate under the KL constraint. We then formulate a maximin
problem based on public and private means, whose solution characterizes the
optimal error exponent for best private arm identification. We finally propose
an algorithm inspired by top-two algorithms. This algorithm naturally adapts
its exploration according to the hardness of pulling arms based on the public
suboptimality gaps. We provide numerical examples illustrating the
$\Theta(\sqrt{T}) $ rate and the behavior of the proposed algorithm.

</details>


### [68] [Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction](https://arxiv.org/abs/2510.08839)
*Motahare Mounesan,Sourya Saha,Houchao Gan,Md. Nurul Absur,Saptarshi Debroy*

Main category: cs.LG

TL;DR: 提出基于强化学习的边缘资源管理框架，用于在资源受限和易受干扰的环境中实现可靠的多视角3D重建。


<details>
  <summary>Details</summary>
Motivation: 边缘环境中的动态和不可预测资源可用性（如图像质量下降、网络不稳定、服务器负载波动）对3D重建管道的可靠性构成挑战，特别是在消防救援等关键应用中。

Method: 采用两个协作的Q学习代理：一个用于相机选择，一个用于服务器选择，两者完全在线运行，通过与边缘环境交互学习策略。

Result: 在包含实验室设备和FABRIC基础设施的分布式测试平台上评估，结果显示该框架能有效平衡端到端延迟和重建质量，提高应用可靠性。

Conclusion: 强化学习框架能够在动态环境中实现可靠的3D重建，平衡延迟和质量要求。

Abstract: Real-time multi-view 3D reconstruction is a mission-critical application for
key edge-native use cases, such as fire rescue, where timely and accurate 3D
scene modeling enables situational awareness and informed decision-making.
However, the dynamic and unpredictable nature of edge resource availability
introduces disruptions, such as degraded image quality, unstable network links,
and fluctuating server loads, which challenge the reliability of the
reconstruction pipeline. In this work, we present a reinforcement learning
(RL)-based edge resource management framework for reliable 3D reconstruction to
ensure high quality reconstruction within a reasonable amount of time, despite
the system operating under a resource-constrained and disruption-prone
environment. In particular, the framework adopts two cooperative Q-learning
agents, one for camera selection and one for server selection, both of which
operate entirely online, learning policies through interactions with the edge
environment. To support learning under realistic constraints and evaluate
system performance, we implement a distributed testbed comprising lab-hosted
end devices and FABRIC infrastructure-hosted edge servers to emulate smart city
edge infrastructure under realistic disruption scenarios. Results show that the
proposed framework improves application reliability by effectively balancing
end-to-end latency and reconstruction quality in dynamic environments.

</details>


### [69] [When LLM Agents Meet Graph Optimization: An Automated Data Quality Improvement Approach](https://arxiv.org/abs/2510.08952)
*Zhihan Zhang,Xunkai Li,Yilong Zuo,Zhenjun Li,Bing Zhou,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出LAGA框架，通过多智能体系统自动优化文本属性图的质量，解决GNN对输入质量敏感的问题


<details>
  <summary>Details</summary>
Motivation: 传统GNN和LLM增强的GNN在稀疏、噪声和不平衡等九种场景下性能显著下降，现有方法主要关注模型架构而忽视图数据本身的系统性优化

Method: LAGA框架包含四个协作智能体：检测、规划、行动和评估，形成自动化闭环。核心行动智能体采用双编码器和三目标设计，捕获跨模态互补信息进行整体图质量增强

Result: 在九种场景下的实验表明，LAGA提高了图质量并在各种任务和骨干网络上实现了最先进的性能

Conclusion: 以数据为中心的质量优化是实现可靠文本属性图和鲁棒图学习的关键

Abstract: Text-attributed graphs (TAGs) have emerged as a powerful representation that
combines structural connections with fine-grained semantics, supporting a wide
range of data-centric applications. However, the performance of graph neural
networks (GNNs) on TAGs is highly sensitive to input quality. Our empirical
study shows that both traditional GNNs and LLM-enhanced GNNs suffer significant
degradation across nine representative scenarios of sparsity, noise, and
imbalance, highlighting graph quality as a critical bottleneck. Existing
approaches mainly focus on improving model architectures, while neglecting
systematic optimization of TAG data itself, leading to limited effectiveness in
practice. To address this gap, we propose LAGA (Large Language and Graph
Agent), a unified multi-agent framework that treats graph quality control as a
first-class, data-centric problem. LAGA integrates four collaborative
agents-detection, planning, action, and evaluation-into an automated closed
loop. At its core, the action agent employs a dual-encoder and tri-objective
design to capture complementary information across modalities and perform
holistic graph quality enhancement. Experiments across nine scenarios show that
LAGA improves graph quality and achieves state-of-the-art performance across
various tasks and backbones, validating data-centric quality optimization as
key to reliable TAGs and robust graph learning.

</details>


### [70] [Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search](https://arxiv.org/abs/2510.08992)
*Kamel Alrashedy,Vriksha Srihari,Zulfiqar Zaidi,Ridam Srivastava,Pradyumna Tambwekar,Matthew Gombolay*

Main category: cs.LG

TL;DR: 提出Constraints-of-Thought (Const-o-T)框架，通过将推理步骤表示为(意图,约束)对，引导蒙特卡洛树搜索专注于语义上有意义的路径，提高规划效率和约束对齐。


<details>
  <summary>Details</summary>
Motivation: 现有推理方法如CoT、ToT等在复杂多步领域中难以确保计划符合用户意图和符号约束，常产生不可行动作或幻觉步骤。

Method: 使用(意图,约束)对结构化表示，集成到MCTS中，约束剪枝不可行分支，引导搜索朝向语义有效动作。

Result: 在Risk游戏、CAD代码生成和算术推理三个领域中，该方法优于基线，获得更高准确率和更强的结构对齐。

Conclusion: Const-of-T为约束引导推理提供了可泛化的基础，使LLM规划更高效、约束对齐且领域适应性强。

Abstract: While researchers have made significant progress in enabling large language
models (LLMs) to perform multi-step planning, LLMs struggle to ensure that
those plans align with high-level user intent and satisfy symbolic constraints,
especially in complex, multi-step domains. Existing reasoning approaches such
as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and verifier-augmented
methods, expand the search space but often yield infeasible actions or
hallucinated steps. To overcome these limitations, we propose
Constraints-of-Thought (Const-o-T), a framework that provides a structured
prior that enables Monte Carlo Tree Search (MCTS) focus search on semantically
meaningful paths. Each reasoning step is represented as an (intent, constraint)
pair, which serves both to compress the search space and enforce validity.
Unlike prior methods that merely generate reasoning traces or validate outputs
post hoc, Const-o-T uses (intent, constraint)pairs to actively focus the search
toward feasible and meaningful plans. We integrate Const-o-T into MCTS using a
structured representation of intent-constraint pairs constraints prune
infeasible branches and guide exploration toward semantically valid actions,
improving planning efficiency and verifiable decision-making. We demonstrate
across three domains Risk game, CAD code generation, and arithmetic reasoning
that our approach outperforms baselines, yielding higher accuracy and stronger
structural alignment. Our contribution is to demonstrate that Const-of-T offers
a generalizable foundation for constraint-guided reasoning, enabling more
efficient, constraint-aligned, and domain-adaptable planning with LLMs.

</details>


### [71] [Robust Driving Control for Autonomous Vehicles: An Intelligent General-sum Constrained Adversarial Reinforcement Learning Approach](https://arxiv.org/abs/2510.09041)
*Junchao Fan,Xiaolin Chang*

Main category: cs.LG

TL;DR: 提出IGCARL方法，通过战略目标对抗者和鲁棒驾驶代理来增强深度强化学习在自动驾驶中的对抗鲁棒性，解决了现有方法面对战略性攻击、难以引发安全关键事件以及训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 现有DRL自动驾驶方法在面对对抗攻击时存在三个关键问题：只能应对短视攻击、难以引发真正安全关键事件、缺乏鲁棒约束导致训练不稳定和政策漂移。

Method: IGCARL包含战略目标对抗者和鲁棒驾驶代理。对抗者利用DRL的时序决策能力执行战略性多步攻击，采用广义和博弈目标诱导安全关键事件；驾驶代理通过与对抗者交互学习鲁棒策略，在约束条件下优化以确保稳定学习。

Result: 实验表明IGCARL相比最先进方法至少提高27.9%的成功率，显著增强了对对抗攻击的鲁棒性，提高了DRL自动驾驶的安全性和可靠性。

Conclusion: IGCARL通过战略性对抗训练和约束优化，有效解决了DRL自动驾驶的对抗鲁棒性问题，为实际部署提供了更安全可靠的解决方案。

Abstract: Deep reinforcement learning (DRL) has demonstrated remarkable success in
developing autonomous driving policies. However, its vulnerability to
adversarial attacks remains a critical barrier to real-world deployment.
Although existing robust methods have achieved success, they still suffer from
three key issues: (i) these methods are trained against myopic adversarial
attacks, limiting their abilities to respond to more strategic threats, (ii)
they have trouble causing truly safety-critical events (e.g., collisions), but
instead often result in minor consequences, and (iii) these methods can
introduce learning instability and policy drift during training due to the lack
of robust constraints. To address these issues, we propose Intelligent
General-sum Constrained Adversarial Reinforcement Learning (IGCARL), a novel
robust autonomous driving approach that consists of a strategic targeted
adversary and a robust driving agent. The strategic targeted adversary is
designed to leverage the temporal decision-making capabilities of DRL to
execute strategically coordinated multi-step attacks. In addition, it
explicitly focuses on inducing safety-critical events by adopting a general-sum
objective. The robust driving agent learns by interacting with the adversary to
develop a robust autonomous driving policy against adversarial attacks. To
ensure stable learning in adversarial environments and to mitigate policy drift
caused by attacks, the agent is optimized under a constrained formulation.
Extensive experiments show that IGCARL improves the success rate by at least
27.9\% over state-of-the-art methods, demonstrating superior robustness to
adversarial attacks and enhancing the safety and reliability of DRL-based
autonomous driving.

</details>


### [72] [Agentic-KGR: Co-evolutionary Knowledge Graph Construction through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.09156)
*Jing Li,Zhijie Sun,Zhicheng Zhou,Suming Qiu,Junjie Huang,Haijia Sun,Linyuan Qiu*

Main category: cs.LG

TL;DR: Agentic-KGR是一个新颖框架，通过多轮强化学习实现大语言模型与知识图谱的协同进化，解决了静态知识库的覆盖不足和时效性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于知识的LLMs依赖静态知识库，存在覆盖不足和时效性差的问题，限制了在动态信息环境中的有效性。

Method: 采用多轮强化学习框架，包含动态模式扩展机制、检索增强记忆系统和可学习多尺度提示压缩方法。

Result: 在知识抽取任务中显著优于监督基线和单轮RL方法，与GraphRAG集成后在QA任务中表现出更高的准确性和知识覆盖度。

Conclusion: Agentic-KGR通过LLM与KG的协同进化有效解决了静态知识库的局限性，为动态知识环境提供了更有效的解决方案。

Abstract: Current knowledge-enhanced large language models (LLMs) rely on static,
pre-constructed knowledge bases that suffer from coverage gaps and temporal
obsolescence, limiting their effectiveness in dynamic information environments.
We present Agentic-KGR, a novel framework enabling co-evolution between LLMs
and knowledge graphs (KGs) through multi-round reinforcement learning (RL). Our
approach introduces three key innovations: (1) a dynamic schema expansion
mechanism that systematically extends graph ontologies beyond pre-defined
boundaries during training; (2) a retrieval-augmented memory system enabling
synergistic co-evolution between model parameters and knowledge structures
through continuous optimization; (3) a learnable multi-scale prompt compression
approach that preserves critical information while reducing computational
complexity through adaptive sequence optimization. Experimental results
demonstrate substantial improvements over supervised baselines and single-round
RL approaches in knowledge extraction tasks. When integrated with GraphRAG, our
method achieves superior performance in downstream QA tasks, with significant
gains in both accuracy and knowledge coverage compared to existing methods.

</details>


### [73] [FM-IRL: Flow-Matching for Reward Modeling and Policy Regularization in Reinforcement Learning](https://arxiv.org/abs/2510.09222)
*Zhenglin Wan,Jingxuan Wu,Xingrui Yu,Chubin Zhang,Mingcong Lei,Bo An,Ivor Tsang*

Main category: cs.LG

TL;DR: 提出了一种结合Flow Matching教师模型和MLP学生策略的方法，通过在线强化学习提升策略的泛化能力和学习效率


<details>
  <summary>Details</summary>
Motivation: Flow Matching在离线模仿学习中表现出色，但缺乏环境交互和探索，导致在未见场景中泛化能力差。直接在线优化FM策略存在梯度不稳定和推理成本高的问题

Method: 使用MLP结构的学生策略进行环境探索和在线更新，通过基于教师FM模型的奖励模型进行指导，并用教师FM模型正则化学生策略行为以稳定学习

Result: 实验表明该方法显著提升了学习效率、泛化能力和鲁棒性，特别是在从次优专家数据中学习时表现突出

Conclusion: 通过简单MLP学生策略与复杂FM教师模型的结合，成功解决了FM策略在线学习的问题，实现了高效探索和稳定学习

Abstract: Flow Matching (FM) has shown remarkable ability in modeling complex
distributions and achieves strong performance in offline imitation learning for
cloning expert behaviors. However, despite its behavioral cloning
expressiveness, FM-based policies are inherently limited by their lack of
environmental interaction and exploration. This leads to poor generalization in
unseen scenarios beyond the expert demonstrations, underscoring the necessity
of online interaction with environment. Unfortunately, optimizing FM policies
via online interaction is challenging and inefficient due to instability in
gradient computation and high inference costs. To address these issues, we
propose to let a student policy with simple MLP structure explore the
environment and be online updated via RL algorithm with a reward model. This
reward model is associated with a teacher FM model, containing rich information
of expert data distribution. Furthermore, the same teacher FM model is utilized
to regularize the student policy's behavior to stabilize policy learning. Due
to the student's simple architecture, we avoid the gradient instability of FM
policies and enable efficient online exploration, while still leveraging the
expressiveness of the teacher FM model. Extensive experiments show that our
approach significantly enhances learning efficiency, generalization, and
robustness, especially when learning from suboptimal expert data.

</details>


### [74] [Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers](https://arxiv.org/abs/2510.09330)
*Tuan Nguyen,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 提出了一种模型无关的黑盒安全对齐框架，通过博弈论方法在推理时平衡安全性和帮助性，无需重新训练或访问模型内部。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法成本高且不灵活，需要重新训练或访问模型内部，不适合第三方利益相关者。

Method: 将安全与帮助性的权衡建模为两人零和博弈，使用线性规划求解器在推理时计算均衡策略。

Result: 证明了黑盒安全对齐的可行性，为资源受限的组织提供了可扩展的安全执行途径。

Conclusion: 该框架为快速发展的LLM生态系统提供了可访问的安全对齐解决方案。

Abstract: Ensuring that large language models (LLMs) comply with safety requirements is
a central challenge in AI deployment. Existing alignment approaches primarily
operate during training, such as through fine-tuning or reinforcement learning
from human feedback, but these methods are costly and inflexible, requiring
retraining whenever new requirements arise. Recent efforts toward
inference-time alignment mitigate some of these limitations but still assume
access to model internals, which is impractical, and not suitable for third
party stakeholders who do not have access to the models. In this work, we
propose a model-independent, black-box framework for safety alignment that does
not require retraining or access to the underlying LLM architecture. As a proof
of concept, we address the problem of trading off between generating safe but
uninformative answers versus helpful yet potentially risky ones. We formulate
this dilemma as a two-player zero-sum game whose minimax equilibrium captures
the optimal balance between safety and helpfulness. LLM agents operationalize
this framework by leveraging a linear programming solver at inference time to
compute equilibrium strategies. Our results demonstrate the feasibility of
black-box safety alignment, offering a scalable and accessible pathway for
stakeholders, including smaller organizations and entities in
resource-constrained settings, to enforce safety across rapidly evolving LLM
ecosystems.

</details>


### [75] [HINT: Helping Ineffective Rollouts Navigate Towards Effectiveness](https://arxiv.org/abs/2510.09388)
*Xinyi Wang,Jinyi Han,Zishang Jiang,Tingyun Li,Jiaqing Liang,Sihang Jiang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.LG

TL;DR: 提出HINT框架解决强化学习中训练亲和力低的问题，通过提供启发式提示而非直接答案，引导模型自主发现解决方案，在数学推理任务上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在任务难度超过模型能力时会出现奖励稀疏和训练低效问题，而混合SFT或使用提示的方法往往误导策略更新，核心问题是训练亲和力低。

Method: 引入Affinity指标监控探索效率和训练稳定性，提出HINT自适应提示框架，提供启发式提示而非直接答案，保持模型自主推理能力。

Result: 在数学推理任务上的广泛实验表明，HINT持续优于现有方法，在不同规模模型上取得SOTA结果，学习更稳定且数据效率更高。

Conclusion: HINT框架有效解决了RL训练中的亲和力问题，通过启发式提示实现了更稳定高效的训练，同时保持了模型的自主推理能力。

Abstract: Reinforcement Learning (RL) has become a key driver for enhancing the long
chain-of-thought (CoT) reasoning capabilities of Large Language Models (LLMs).
However, prevalent methods like GRPO often fail when task difficulty exceeds
the model's capacity, leading to reward sparsity and inefficient training.
While prior work attempts to mitigate this using off-policy data, such as
mixing RL with Supervised Fine-Tuning (SFT) or using hints, they often misguide
policy updates In this work, we identify a core issue underlying these
failures, which we term low training affinity. This condition arises from a
large distributional mismatch between external guidance and the model's policy.
To diagnose this, we introduce Affinity, the first quantitative metric for
monitoring exploration efficiency and training stability. To improve Affinity,
we propose HINT: Helping Ineffective rollouts Navigate Towards effectiveness,
an adaptive hinting framework. Instead of providing direct answers, HINT
supplies heuristic hints that guide the model to discover solutions on its own,
preserving its autonomous reasoning capabilities. Extensive experiments on
mathematical reasoning tasks show that HINT consistently outperforms existing
methods, achieving state-of-the-art results with models of various scales,
while also demonstrating significantly more stable learning and greater data
efficiency.Code is available on Github.

</details>


### [76] [Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols](https://arxiv.org/abs/2510.09462)
*Mikhail Terekhov,Alexander Panfilov,Daniil Dzenhaliou,Caglar Gulcehre,Maksym Andriushchenko,Ameya Prabhu,Jonas Geiping*

Main category: cs.LG

TL;DR: 论文研究了针对AI控制协议的自适应攻击，发现当前基于LLM监控器的控制协议存在根本性缺陷，攻击者可以通过嵌入公开已知或零样本提示注入来逃避监控并完成恶意任务。


<details>
  <summary>Details</summary>
Motivation: 现有AI控制协议主要依赖LLM监控器作为防御机制，但这些监控器可能成为单点故障。论文旨在研究当攻击者了解协议和监控模型时的自适应攻击威胁。

Method: 实例化了一种简单的自适应攻击向量，攻击者在模型输出中嵌入公开已知或零样本提示注入，并在两个主要AI控制基准上测试这种攻击的有效性。

Result: 前沿模型能够持续逃避各种监控器并完成恶意任务，攻击对当前依赖监控器的协议具有普遍有效性，甚至Defer-to-Resample协议会适得其反。

Conclusion: 对监控模型的自适应攻击是当前控制协议的主要盲点，应该成为未来AI控制机制评估的标准组成部分。

Abstract: AI control protocols serve as a defense mechanism to stop untrusted LLM
agents from causing harm in autonomous settings. Prior work treats this as a
security problem, stress testing with exploits that use the deployment context
to subtly complete harmful side tasks, such as backdoor insertion. In practice,
most AI control protocols are fundamentally based on LLM monitors, which can
become a central point of failure. We study adaptive attacks by an untrusted
model that knows the protocol and the monitor model, which is plausible if the
untrusted model was trained with a later knowledge cutoff or can search for
this information autonomously. We instantiate a simple adaptive attack vector
by which the attacker embeds publicly known or zero-shot prompt injections in
the model outputs. Using this tactic, frontier models consistently evade
diverse monitors and complete malicious tasks on two main AI control
benchmarks. The attack works universally against current protocols that rely on
a monitor. Furthermore, the recent Defer-to-Resample protocol even backfires,
as its resampling amplifies the prompt injection and effectively reframes it as
a best-of-$n$ attack. In general, adaptive attacks on monitor models represent
a major blind spot in current control protocols and should become a standard
component of evaluations for future AI control mechanisms.

</details>


### [77] [Near-Optimal Second-Order Guarantees for Model-Based Adversarial Imitation Learning](https://arxiv.org/abs/2510.09487)
*Shangzhe Li,Dongruo Zhou,Weitong Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于模型的对抗模仿学习算法(MB-AIL)，在无奖励的在线环境中从离线专家演示中学习，建立了无视野限制的二阶样本复杂度保证，并在理论和实验上验证了其最优性。


<details>
  <summary>Details</summary>
Motivation: 解决在线对抗模仿学习中在线交互的益处和随机性影响理解不足的问题，特别是在无奖励环境下从专家演示中学习的样本效率问题。

Method: 引入基于模型的对抗模仿学习算法(MB-AIL)，使用一般函数逼近处理专家数据和奖励自由交互，建立二阶样本复杂度分析框架。

Result: MB-AIL在在线交互中达到了极小极大最优样本复杂度(对数因子内)，与专家演示的下界在视野H、精度ε和策略方差σ²的依赖上匹配，实验验证其样本效率优于现有方法。

Conclusion: MB-AIL算法在理论和实践上都表现出色，为在线对抗模仿学习提供了最优样本复杂度保证，特别在系统接近确定性时表现更好。

Abstract: We study online adversarial imitation learning (AIL), where an agent learns
from offline expert demonstrations and interacts with the environment online
without access to rewards. Despite strong empirical results, the benefits of
online interaction and the impact of stochasticity remain poorly understood. We
address these gaps by introducing a model-based AIL algorithm (MB-AIL) and
establish its horizon-free, second-order sample-complexity guarantees under
general function approximations for both expert data and reward-free
interactions. These second-order bounds provide an instance-dependent result
that can scale with the variance of returns under the relevant policies and
therefore tighten as the system approaches determinism. Together with
second-order, information-theoretic lower bounds on a newly constructed
hard-instance family, we show that MB-AIL attains minimax-optimal sample
complexity for online interaction (up to logarithmic factors) with limited
expert demonstrations and matches the lower bound for expert demonstrations in
terms of the dependence on horizon $H$, precision $\epsilon$ and the policy
variance $\sigma^2$. Experiments further validate our theoretical findings and
demonstrate that a practical implementation of MB-AIL matches or surpasses the
sample efficiency of existing methods.

</details>
