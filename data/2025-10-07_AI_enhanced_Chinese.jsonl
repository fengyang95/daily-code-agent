{"id": "2510.03463", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03463", "abs": "https://arxiv.org/abs/2510.03463", "authors": ["Vali Tawosi", "Keshav Ramani", "Salwa Alamir", "Xiaomo Liu"], "title": "ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework", "comment": null, "summary": "Multi-agent Large Language Model (LLM) systems have been leading the way in\napplied LLM research across a number of fields. One notable area is software\ndevelopment, where researchers have advanced the automation of code\nimplementation, code testing, code maintenance, inter alia, using LLM agents.\nHowever, software development is a multifaceted environment that extends beyond\njust code. As such, a successful LLM system must factor in multiple stages of\nthe software development life-cycle (SDLC). In this paper, we propose a vision\nfor ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,\nwhich follows the above SDLC philosophy such that it may work within an agile\nsoftware development team to perform several tasks end-to-end. ALMAS aligns its\nagents with agile roles, and can be used in a modular fashion to seamlessly\nintegrate with human developers and their development environment. We showcase\nthe progress towards ALMAS through our published works and a use case\ndemonstrating the framework, where ALMAS is able to seamlessly generate an\napplication and add a new feature.", "AI": {"tldr": "\u63d0\u51fa\u4e86ALMAS\u6846\u67b6\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u8f6f\u4ef6\u5de5\u7a0b\u7cfb\u7edf\uff0c\u80fd\u591f\u9075\u5faa\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u5728\u654f\u6377\u56e2\u961f\u4e2d\u6267\u884c\u7aef\u5230\u7aef\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u5b9e\u73b0\uff0c\u4f46\u8f6f\u4ef6\u5f00\u53d1\u662f\u591a\u65b9\u9762\u7684\uff0c\u9700\u8981\u6db5\u76d6\u6574\u4e2a\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u3002", "method": "\u8bbe\u8ba1ALMAS\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u4e0e\u654f\u6377\u89d2\u8272\u5bf9\u9f50\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u80fd\u591f\u4e0e\u4eba\u7c7b\u5f00\u53d1\u8005\u548c\u5f00\u53d1\u73af\u5883\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5c55\u793a\u4e86ALMAS\u6846\u67b6\u7684\u8fdb\u5c55\uff0c\u5305\u62ec\u5df2\u53d1\u8868\u5de5\u4f5c\u548c\u7528\u4f8b\u6f14\u793a\uff0c\u80fd\u591f\u65e0\u7f1d\u751f\u6210\u5e94\u7528\u7a0b\u5e8f\u5e76\u6dfb\u52a0\u65b0\u529f\u80fd\u3002", "conclusion": "ALMAS\u6846\u67b6\u4e3a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u7aef\u5230\u7aef\u7684\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u3002", "topic": "swe application"}}
{"id": "2510.03474", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03474", "abs": "https://arxiv.org/abs/2510.03474", "authors": ["Nadeeshan De Silva", "Martin Kellogg", "Oscar Chaparro"], "title": "Relative Code Comprehensibility Prediction", "comment": null, "summary": "Automatically predicting how difficult it is for humans to understand a code\nsnippet can assist developers in tasks like deciding when and where to\nrefactor. Despite many proposed code comprehensibility metrics, studies have\nshown they often correlate poorly with actual measurements of human\ncomprehensibility. This has motivated the use of machine learning models to\npredict human comprehensibility directly from code, but these models have also\nshown limited accuracy.\n  We argue that model inaccuracy stems from inherent noise in human\ncomprehensibility data, which confuses models trained to predict it directly.\nTo address this, we propose training models to predict the relative\ncomprehensibility of two code snippets - that is, predicting which snippet a\nhuman would find easier to understand without predicting each snippet's\ncomprehensibility in isolation. This mitigates noise in predicting 'absolute'\ncomprehensibility measurements, but is still useful for downstream\nsoftware-engineering tasks like assessing whether refactoring improves or\nhinders comprehensibility.\n  We conducted a study to assess and compare the effectiveness of absolute and\nrelative code comprehensibility prediction via machine learning. We used a\ndataset of 150 Java code snippets and 12.5k human comprehensibility\nmeasurements from prior user studies, comparing the models' performance with\nnaive baselines (eg 'always predict the majority class'). Our findings indicate\nthat absolute comprehensibility models improve over the baselines by at most\n33.4% and frequently underperform. In contrast, relative comprehensibility\nmodels are substantially better, with average improvements of 137.8% and 74.7%\nfor snippet-wise and developer-wise prediction, respectively. These results\nsuggest that relative comprehensibility models learn more effectively from the\ndata, supporting their practical applicability for downstream SE tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u76f8\u5bf9\u53ef\u7406\u89e3\u6027\u9884\u6d4b\u65b9\u6cd5\u66ff\u4ee3\u7edd\u5bf9\u53ef\u7406\u89e3\u6027\u9884\u6d4b\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e24\u4e2a\u4ee3\u7801\u7247\u6bb5\u7684\u76f8\u5bf9\u53ef\u7406\u89e3\u6027\u6765\u7f13\u89e3\u4eba\u7c7b\u6570\u636e\u4e2d\u7684\u566a\u58f0\u95ee\u9898\uff0c\u5728\u5b9e\u9a8c\u4e2d\u76f8\u5bf9\u65b9\u6cd5\u6bd4\u7edd\u5bf9\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801\u53ef\u7406\u89e3\u6027\u5ea6\u91cf\u6307\u6807\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u5ea6\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4eba\u7c7b\u53ef\u7406\u89e3\u6027\u6570\u636e\u5b58\u5728\u56fa\u6709\u566a\u58f0\uff0c\u76f4\u63a5\u9884\u6d4b\u7edd\u5bf9\u53ef\u7406\u89e3\u6027\u4f1a\u6df7\u6dc6\u6a21\u578b\u3002", "method": "\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u4e24\u4e2a\u4ee3\u7801\u7247\u6bb5\u7684\u76f8\u5bf9\u53ef\u7406\u89e3\u6027\uff08\u54ea\u4e2a\u66f4\u5bb9\u6613\u7406\u89e3\uff09\uff0c\u800c\u4e0d\u662f\u5355\u72ec\u9884\u6d4b\u6bcf\u4e2a\u7247\u6bb5\u7684\u7edd\u5bf9\u53ef\u7406\u89e3\u6027\u3002\u4f7f\u7528150\u4e2aJava\u4ee3\u7801\u7247\u6bb5\u548c12.5k\u4e2a\u4eba\u7c7b\u53ef\u7406\u89e3\u6027\u6d4b\u91cf\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u7edd\u5bf9\u53ef\u7406\u89e3\u6027\u6a21\u578b\u6700\u591a\u6bd4\u57fa\u7ebf\u63d0\u534733.4%\uff0c\u4e14\u7ecf\u5e38\u8868\u73b0\u4e0d\u4f73\uff1b\u76f8\u5bf9\u53ef\u7406\u89e3\u6027\u6a21\u578b\u5e73\u5747\u63d0\u5347137.8%\uff08\u7247\u6bb5\u7ea7\uff09\u548c74.7%\uff08\u5f00\u53d1\u8005\u7ea7\uff09\uff0c\u8868\u73b0\u663e\u8457\u66f4\u597d\u3002", "conclusion": "\u76f8\u5bf9\u53ef\u7406\u89e3\u6027\u6a21\u578b\u80fd\u66f4\u6709\u6548\u5730\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u652f\u6301\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "2510.03480", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03480", "abs": "https://arxiv.org/abs/2510.03480", "authors": ["Vali Tawosi", "Salwa Alamir", "Xiaomo Liu", "Manuela Veloso"], "title": "LLM Agents for Automated Dependency Upgrades", "comment": null, "summary": "As a codebase expands over time, its library dependencies can become outdated\nand require updates to maintain innovation and security. However, updating a\nlibrary can introduce breaking changes in the code, necessitating significant\ndeveloper time for maintenance. To address this, we introduce a framework of\nLLM agents to be used in combination with migration documentation to\nautomatically recommend and apply code updates and ensure compatibility with\nnew versions. Our solution can automatically localize updated library usages in\nlive Java codebases and implement recommended fixes in a user-friendly manner.\nThe system architecture consists of multiple key components: a Summary Agent,\nControl Agent, and Code Agent. To validate our approach, we apply the framework\non an industrial use case by which we create three synthetic code repositories\nwith major Upgrade changes and benchmark our approach against state-of-the-art\nmethods. Results show that our approach not only performs upgrades using fewer\ntokens across all cases but also achieves a precision of 71.4%, highlighting\nits efficiency and effectiveness compared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u66f4\u65b0Java\u4ee3\u7801\u5e93\u4e2d\u7684\u5e93\u4f9d\u8d56\uff0c\u901a\u8fc7\u8fc1\u79fb\u6587\u6863\u63a8\u8350\u548c\u5e94\u7528\u4ee3\u7801\u66f4\u65b0\uff0c\u786e\u4fdd\u4e0e\u65b0\u7248\u672c\u7684\u517c\u5bb9\u6027\u3002", "motivation": "\u968f\u7740\u4ee3\u7801\u5e93\u7684\u6269\u5c55\uff0c\u5e93\u4f9d\u8d56\u4f1a\u53d8\u5f97\u8fc7\u65f6\uff0c\u9700\u8981\u66f4\u65b0\u4ee5\u4fdd\u6301\u521b\u65b0\u548c\u5b89\u5168\u6027\u3002\u4f46\u66f4\u65b0\u5e93\u53ef\u80fd\u5f15\u5165\u7834\u574f\u6027\u53d8\u66f4\uff0c\u9700\u8981\u5927\u91cf\u5f00\u53d1\u65f6\u95f4\u8fdb\u884c\u7ef4\u62a4\u3002", "method": "\u4f7f\u7528LLM\u4ee3\u7406\u6846\u67b6\u7ed3\u5408\u8fc1\u79fb\u6587\u6863\uff0c\u5305\u62ec\u6458\u8981\u4ee3\u7406\u3001\u63a7\u5236\u4ee3\u7406\u548c\u4ee3\u7801\u4ee3\u7406\uff0c\u81ea\u52a8\u5b9a\u4f4d\u66f4\u65b0\u7684\u5e93\u4f7f\u7528\u5e76\u5b9e\u65bd\u63a8\u8350\u4fee\u590d\u3002", "result": "\u5728\u5de5\u4e1a\u7528\u4f8b\u4e2d\u521b\u5efa\u4e09\u4e2a\u5408\u6210\u4ee3\u7801\u5e93\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u4f7f\u7528\u66f4\u5c11\u7684token\uff0c\u5e76\u8fbe\u523071.4%\u7684\u7cbe\u786e\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u65b9\u6cd5\u5728\u6548\u7387\u548c\u6709\u6548\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "topic": "code agent"}}
{"id": "2510.03285", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03285", "abs": "https://arxiv.org/abs/2510.03285", "authors": ["Su Kara", "Fazle Faisal", "Suman Nath"], "title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks", "comment": null, "summary": "Recent advances in browser-based LLM agents have shown promise for automating\ntasks ranging from simple form filling to hotel booking or online shopping.\nCurrent benchmarks measure agent performance in controlled environments, such\nas containers or stable networks, where websites behave deterministically.\nHowever, in the real world, users access websites over networks and HTTPS\nconnections that introduce instability from multiple sources: client-side,\nserver-side issues or broader system failures. Moreover, live websites are\nprone to web attacks such Cross-Site Scripting, as well as general site\nmodifications which can cause unexpected or malicious pop-ups or improper\nfunctionality. To address this gap, we present WAREX: Web Agent Reliability\nEvaluation on Existing Benchmarks. We measure the impact of WAREX across three\npopular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that\nintroducing WAREX leads to significant drops in task success rates,\nhighlighting the limited robustness of state-of-the-art agents.", "AI": {"tldr": "WAREX\u662f\u4e00\u4e2a\u8bc4\u4f30\u6d4f\u89c8\u5668LLM\u4ee3\u7406\u5728\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u53ef\u9760\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f15\u5165\u7f51\u7edc\u4e0d\u7a33\u5b9a\u6027\u548c\u7f51\u7ad9\u653b\u51fb\u7b49\u73b0\u5b9e\u56e0\u7d20\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4ee3\u7406\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u8bc4\u4f30LLM\u4ee3\u7406\u6027\u80fd\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u5b58\u5728\u7f51\u7edc\u4e0d\u7a33\u5b9a\u3001HTTPS\u8fde\u63a5\u95ee\u9898\u548c\u7f51\u7ad9\u653b\u51fb\u7b49\u6311\u6218\uff0c\u9700\u8981\u8bc4\u4f30\u4ee3\u7406\u5728\u8fd9\u4e9b\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u3002", "method": "\u5728\u4e09\u4e2a\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5\uff08WebArena\u3001WebVoyager\u3001REAL\uff09\u4e2d\u5f15\u5165WAREX\u6846\u67b6\uff0c\u6a21\u62df\u7f51\u7edc\u4e0d\u7a33\u5b9a\u3001\u5ba2\u6237\u7aef/\u670d\u52a1\u5668\u7aef\u95ee\u9898\u3001\u7cfb\u7edf\u6545\u969c\u4ee5\u53ca\u8de8\u7ad9\u811a\u672c\u653b\u51fb\u7b49\u73b0\u5b9e\u7f51\u7edc\u73af\u5883\u56e0\u7d20\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f15\u5165WAREX\u540e\uff0c\u6700\u5148\u8fdb\u4ee3\u7406\u7684\u4efb\u52a1\u6210\u529f\u7387\u663e\u8457\u4e0b\u964d\uff0c\u66b4\u9732\u4e86\u73b0\u6709\u4ee3\u7406\u5728\u73b0\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u6709\u9650\u9c81\u68d2\u6027\u3002", "conclusion": "\u5f53\u524dLLM\u4ee3\u7406\u5728\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u4ee3\u7406\u7cfb\u7edf\u6765\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u5a01\u80c1\u3002", "topic": "agent analysis"}}
{"id": "2510.03495", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03495", "abs": "https://arxiv.org/abs/2510.03495", "authors": ["Erik Pautsch", "Tanmay Singla", "Wenxin Jiang", "Huiyun Peng", "Behnaz Hassanshahi", "Konstantin L\u00e4ufer", "George K. Thiruvathukal", "James C. Davis"], "title": "AgentHub: A Research Agenda for Agent Sharing Infrastructure", "comment": null, "summary": "LLM-based agents are rapidly proliferating, yet the infrastructure for\ndiscovering, evaluating, and governing them remains fragmented compared to\nmature ecosystems like software package registries (e.g., npm) and model hubs\n(e.g., Hugging Face). Recent research and engineering works have begun to\nconsider the requisite infrastructure, but so far they focus narrowly -- on\ndistribution, naming, or protocol negotiation. However, considering broader\nsoftware engineering requirements would improve open-source distribution and\nease reuse. We therefore propose AgentHub, a research agenda for agent sharing.\nBy framing the key challenges of capability clarity, lifecycle transparency,\ninteroperability, governance, security, and workflow integration, AgentHub\ncharts a community-wide agenda for building reliable and scalable agent\necosystems. Our vision is a future where agents can be shared, trusted, and\ncomposed as seamlessly as today's software libraries.", "AI": {"tldr": "\u63d0\u51faAgentHub\u7814\u7a76\u8bae\u7a0b\uff0c\u65e8\u5728\u89e3\u51b3LLM\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u57fa\u7840\u8bbe\u65bd\u788e\u7247\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5b9a\u4e49\u5173\u952e\u6311\u6218\u6765\u63a8\u52a8\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u5efa\u8bbe\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u57fa\u7840\u8bbe\u65bd\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7c7b\u4f3cnpm\u6216Hugging Face\u7684\u6210\u719f\u751f\u6001\u7cfb\u7edf\uff0c\u73b0\u6709\u5de5\u4f5c\u5173\u6ce8\u70b9\u8fc7\u4e8e\u72ed\u7a84\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8f6f\u4ef6\u5de5\u7a0b\u89c6\u89d2\u6765\u6539\u5584\u5f00\u6e90\u5206\u53d1\u548c\u91cd\u7528\u3002", "method": "\u63d0\u51faAgentHub\u7814\u7a76\u8bae\u7a0b\uff0c\u901a\u8fc7\u6846\u67b6\u5316\u5173\u952e\u6311\u6218\uff1a\u80fd\u529b\u6e05\u6670\u5ea6\u3001\u751f\u547d\u5468\u671f\u900f\u660e\u5ea6\u3001\u4e92\u64cd\u4f5c\u6027\u3001\u6cbb\u7406\u3001\u5b89\u5168\u6027\u548c\u5de5\u4f5c\u6d41\u96c6\u6210\uff0c\u6765\u6307\u5bfc\u793e\u533a\u5efa\u8bbe\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u3002", "result": "\u5efa\u7acb\u4e86\u667a\u80fd\u4f53\u5171\u4eab\u7684\u7814\u7a76\u8bae\u7a0b\u6846\u67b6\uff0c\u660e\u786e\u4e86\u516d\u4e2a\u5173\u952e\u6311\u6218\u9886\u57df\uff0c\u4e3a\u6784\u5efa\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002", "conclusion": "AgentHub\u613f\u666f\u662f\u8ba9\u667a\u80fd\u4f53\u80fd\u591f\u50cf\u4eca\u5929\u7684\u8f6f\u4ef6\u5e93\u4e00\u6837\u88ab\u65e0\u7f1d\u5171\u4eab\u3001\u4fe1\u4efb\u548c\u7ec4\u5408\uff0c\u8fd9\u9700\u8981\u793e\u533a\u5171\u540c\u52aa\u529b\u89e3\u51b3\u63d0\u51fa\u7684\u5173\u952e\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2510.03323", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03323", "abs": "https://arxiv.org/abs/2510.03323", "authors": ["Ge Chang", "Jinbo Su", "Jiacheng Liu", "Pengfei Yang", "Yuhao Shang", "Huiwen Zheng", "Hongli Ma", "Yan Liang", "Yuanchun Li", "Yunxin Liu"], "title": "Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision", "comment": null, "summary": "A significant portion of real-world data is inherently represented as textual\ngraphs, and integrating these graphs into large language models (LLMs) is\npromising to enable complex graph-based question answering. However, a key\nchallenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,\nhow to retrieve relevant content from large graphs that is sufficiently\ninformative while remaining compact for the LLM context. Existing retrievers\nsuffer from poor performance since they either rely on shallow embedding\nsimilarity or employ interactive retrieving policies that demand excessive data\nlabeling and training cost. To address these issues, we present Graph-$S^3$, an\nagentic textual graph reasoning framework that employs an LLM-based retriever\ntrained with synthetic stepwise supervision. Instead of rewarding the agent\nbased on the final answers, which may lead to sparse and unstable training\nsignals, we propose to closely evaluate each step of the retriever based on\noffline-extracted golden subgraphs. Our main techniques include a data\nsynthesis pipeline to extract the golden subgraphs for reward generation and a\ntwo-stage training scheme to learn the interactive graph exploration policy\nbased on the synthesized rewards. Based on extensive experiments on three\ncommon datasets in comparison with seven strong baselines, our approach\nachieves an average improvement of 8.1\\% in accuracy and 9.7\\% in F$_1$ score.\nThe advantage is even higher in more complicated multi-hop reasoning tasks. Our\ncode will be open-sourced.", "AI": {"tldr": "Graph-S\u00b3\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6587\u672c\u56fe\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u9010\u6b65\u76d1\u7763\u8bad\u7ec3\u68c0\u7d22\u5668\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u56fe\u4e2d\u76f8\u5173\u5185\u5bb9\u7684\u68c0\u7d22\u95ee\u9898\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u591a\u4ee5\u6587\u672c\u56fe\u5f62\u5f0f\u5b58\u5728\uff0c\u4f46\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6d45\u5c42\u5d4c\u5165\u76f8\u4f3c\u6027\uff0c\u8981\u4e48\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u8bad\u7ec3\u6210\u672c\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u68c0\u7d22\u5668\uff0c\u4f7f\u7528\u5408\u6210\u9010\u6b65\u76d1\u7763\u8bad\u7ec3\uff0c\u901a\u8fc7\u6570\u636e\u5408\u6210\u7ba1\u9053\u63d0\u53d6\u9ec4\u91d1\u5b50\u56fe\u751f\u6210\u5956\u52b1\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\u5b66\u4e60\u4ea4\u4e92\u5f0f\u56fe\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u5e38\u89c1\u6570\u636e\u96c6\u4e0a\u4e0e\u4e03\u4e2a\u5f3a\u57fa\u7ebf\u6bd4\u8f83\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53478.1%\uff0cF1\u5206\u6570\u63d0\u53479.7%\uff0c\u5728\u590d\u6742\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u52bf\u66f4\u660e\u663e\u3002", "conclusion": "Graph-S\u00b3\u6846\u67b6\u901a\u8fc7\u5408\u6210\u76d1\u7763\u8bad\u7ec3\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u56fe\u68c0\u7d22\u95ee\u9898\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2510.03588", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03588", "abs": "https://arxiv.org/abs/2510.03588", "authors": ["Anvith Pabba", "Simin Chen", "Alex Mathai", "Anindya Chakraborty", "Baishakhi Ray"], "title": "REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement", "comment": "We also open source our code at\n  https://anonymous.4open.science/r/SemAgent-7B2F/README.md", "summary": "Large Language Models (LLMs) have recently shown strong potential in\nautomatic program repair (APR), especially in repository-level settings where\nthe goal is to generate patches based on natural language issue descriptions,\nlarge codebases, and regression tests. However, despite their promise, current\nLLM-based APR techniques often struggle to produce correct fixes due to limited\nunderstanding of code context and over-reliance on incomplete test suites. As a\nresult, they frequently generate Draft Patches-partially correct patches that\neither incompletely address the bug or overfit to the test cases. In this work,\nwe propose a novel patch refinement framework, Refine, that systematically\ntransforms Draft Patches into correct ones. Refine addresses three key\nchallenges: disambiguating vague issue and code context, diversifying patch\ncandidates through test-time scaling, and aggregating partial fixes via an\nLLM-powered code review process. We implement Refine as a general refinement\nmodule that can be integrated into both open-agent-based and workflow-based APR\nsystems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine\nachieves state-of-the-art results among workflow-based approaches and\napproaches the best-known performance across all APR categories. Specifically,\nRefine boosts AutoCodeRover's performance by 14.67%, achieving a score of\n51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine\nimproves the resolution rate by 12.2%, and when integrated across multiple APR\nsystems, it yields an average improvement of 14%-demonstrating its broad\neffectiveness and generalizability. These results highlight the effectiveness\nof refinement as a missing component in current APR pipelines and the potential\nof agentic collaboration in closing the gap between near-correct and correct\npatches. We also open source our code.", "AI": {"tldr": "\u63d0\u51fa\u4e86Refine\u8865\u4e01\u7cbe\u70bc\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u51b3\u4ee3\u7801\u4e0a\u4e0b\u6587\u6b67\u4e49\u3001\u591a\u6837\u5316\u8865\u4e01\u5019\u9009\u548cLLM\u9a71\u52a8\u7684\u4ee3\u7801\u5ba1\u67e5\uff0c\u5c06\u8349\u7a3f\u8865\u4e01\u8f6c\u5316\u4e3a\u6b63\u786e\u8865\u4e01\uff0c\u5728SWE-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u6280\u672f\u7531\u4e8e\u5bf9\u4ee3\u7801\u4e0a\u4e0b\u6587\u7406\u89e3\u6709\u9650\u548c\u8fc7\u5ea6\u4f9d\u8d56\u4e0d\u5b8c\u6574\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7ecf\u5e38\u4ea7\u751f\u90e8\u5206\u6b63\u786e\u7684\u8349\u7a3f\u8865\u4e01\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7cbe\u70bc\u624d\u80fd\u5f97\u5230\u6b63\u786e\u4fee\u590d\u3002", "method": "Refine\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a\u6d88\u9664\u95ee\u9898\u4e0e\u4ee3\u7801\u4e0a\u4e0b\u6587\u7684\u6b67\u4e49\u3001\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\u591a\u6837\u5316\u8865\u4e01\u5019\u9009\u3001\u4f7f\u7528LLM\u9a71\u52a8\u7684\u4ee3\u7801\u5ba1\u67e5\u805a\u5408\u90e8\u5206\u4fee\u590d\uff0c\u53ef\u4f5c\u4e3a\u901a\u7528\u7cbe\u70bc\u6a21\u5757\u96c6\u6210\u5230\u5404\u79cdAPR\u7cfb\u7edf\u4e2d\u3002", "result": "\u5728SWE-Bench Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523051.67%\u7684\u5206\u6570\uff0c\u6bd4AutoCodeRover\u63d0\u534714.67%\uff1b\u5728SWE-Bench Verified\u4e0a\u63d0\u5347\u89e3\u51b3\u738712.2%\uff1b\u96c6\u6210\u591a\u4e2aAPR\u7cfb\u7edf\u5e73\u5747\u63d0\u534714%\u3002", "conclusion": "\u7cbe\u70bc\u662f\u5f53\u524dAPR\u6d41\u7a0b\u4e2d\u7f3a\u5931\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u4ee3\u7406\u534f\u4f5c\u5728\u7f29\u5c0f\u63a5\u8fd1\u6b63\u786e\u4e0e\u5b8c\u5168\u6b63\u786e\u8865\u4e01\u4e4b\u95f4\u7684\u5dee\u8ddd\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "topic": "swe application"}}
{"id": "2510.03641", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03641", "abs": "https://arxiv.org/abs/2510.03641", "authors": ["Satoshi Masuda", "Satoshi Kouzawa", "Kyousuke Sezai", "Hidetoshi Suhara", "Yasuaki Hiruta", "Kunihiro Kudou"], "title": "Generating High-Level Test Cases from Requirements using LLM: An Industry Study", "comment": "11pages", "summary": "Currently, generating high-level test cases described in natural language\nfrom requirement documents is performed manually. In the industry, including\ncompanies specializing in software testing, there is a significant demand for\nthe automatic generation of high-level test cases from requirement documents\nusing Large Language Models (LLMs). Efforts to utilize LLMs for requirement\nanalysis are underway. In some cases, retrieval-augmented generation (RAG) is\nemployed for generating high-level test cases using LLMs. However, in practical\napplications, it is necessary to create a RAG tailored to the knowledge system\nof each specific application, which is labor-intensive. Moreover, when applying\nhigh-level test case generation as a prompt, there is no established method for\ninstructing the generation of high-level test cases at a level applicable to\nother specifications without using RAG. It is required to establish a method\nfor the automatic generation of high-level test cases that can be generalized\nacross a wider range of requirement documents. In this paper, we propose a\nmethod for generating high-level (GHL) test cases from requirement documents\nusing only prompts, without creating RAGs. In the proposed method, first, the\nrequirement document is input into the LLM to generate test design techniques\ncorresponding to the requirement document. Then, high-level test cases are\ngenerated for each of the generated test design techniques. Furthermore, we\nverify an evaluation method based on semantic similarity of the generated\nhigh-level test cases. In the experiments, we confirmed the method using\ndatasets from Bluetooth and Mozilla, where requirement documents and high-level\ntest cases are available, achieving macro-recall measurement of 0.81 and 0.37,\nrespectively. We believe that the method is feasible for practical application\nin generating high-level test cases without using RAG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u63d0\u793a\u800c\u4e0d\u521b\u5efaRAG\u7684\u65b9\u6cd5\uff0c\u4ece\u9700\u6c42\u6587\u6863\u81ea\u52a8\u751f\u6210\u9ad8\u5c42\u6b21\u6d4b\u8bd5\u7528\u4f8b\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u751f\u6210\u6d4b\u8bd5\u8bbe\u8ba1\u6280\u672f\uff0c\u7136\u540e\u4e3a\u6bcf\u4e2a\u6280\u672f\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u5728\u84dd\u7259\u548cMozilla\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "motivation": "\u76ee\u524d\u4ece\u9700\u6c42\u6587\u6863\u751f\u6210\u9ad8\u5c42\u6b21\u6d4b\u8bd5\u7528\u4f8b\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\uff0c\u884c\u4e1a\u5bf9\u4f7f\u7528LLM\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u6709\u5f3a\u70c8\u9700\u6c42\u3002\u73b0\u6709RAG\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u5e94\u7528\u5b9a\u5236\u77e5\u8bc6\u7cfb\u7edf\uff0c\u5de5\u4f5c\u91cf\u5927\uff0c\u9700\u8981\u5efa\u7acb\u65e0\u9700RAG\u7684\u901a\u7528\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u5c06\u9700\u6c42\u6587\u6863\u8f93\u5165LLM\u751f\u6210\u5bf9\u5e94\u7684\u6d4b\u8bd5\u8bbe\u8ba1\u6280\u672f\uff0c\u7136\u540e\u4e3a\u6bcf\u4e2a\u751f\u6210\u7684\u6d4b\u8bd5\u8bbe\u8ba1\u6280\u672f\u751f\u6210\u9ad8\u5c42\u6b21\u6d4b\u8bd5\u7528\u4f8b\u3002\u540c\u65f6\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5728\u84dd\u7259\u548cMozilla\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5b8f\u53ec\u56de\u7387\u5206\u522b\u8fbe\u52300.81\u548c0.37\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u65e0\u9700\u521b\u5efaRAG\u5373\u53ef\u751f\u6210\u9ad8\u5c42\u6b21\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2510.03743", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03743", "abs": "https://arxiv.org/abs/2510.03743", "authors": ["Zachary Eberhart", "Collin McMillan"], "title": "APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents", "comment": "4 pages, 2 figures. To be published in Proceedings of the 40th\n  IEEE/ACM International Conference on Automated Software Engineering", "summary": "Large-language-model assistants are suitable for explaining popular APIs, yet\nthey falter on niche or proprietary libraries because the multi-turn dialogue\ndata needed for fine-tuning are scarce. We present APIDA-Chat, an open-source\npipeline that converts symbolic dialogue-act \"scripts\" into realistic,\ndomain-grounded API Search conversations using a lightweight model for\ninexpensive training data generation. Phase I pairs a legacy dialogue planner\nwith a high-capability teacher LLM (o4-mini) to synthesize a \"gold set\" of\nrealized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on\nthis corpus. Phase II drops the teacher and reuses the same planner with the\nfine-tuned model, allowing rapid, low-cost synthesis of new dialogues without\nexposing source code to external services. The fine-tuned student improves BLEU\nfrom 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while\nrunning entirely on a single consumer GPU. All components are modular and\npublicly released to serve as a conservative baseline for future work.\nAPIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a\nvideo demo is available at https://youtu.be/YqmZBHyGbPs .", "AI": {"tldr": "APIDA-Chat\u662f\u4e00\u4e2a\u5f00\u6e90\u7ba1\u9053\uff0c\u5c06\u7b26\u53f7\u5bf9\u8bdd\u811a\u672c\u8f6c\u6362\u4e3a\u771f\u5b9e\u7684API\u641c\u7d22\u5bf9\u8bdd\uff0c\u4f7f\u7528\u8f7b\u91cf\u6a21\u578b\u751f\u6210\u5ec9\u4ef7\u8bad\u7ec3\u6570\u636e\uff0c\u89e3\u51b3\u5c0f\u4f17API\u5bf9\u8bdd\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u52a9\u624b\u5728\u89e3\u91ca\u6d41\u884cAPI\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5c0f\u4f17\u6216\u4e13\u6709\u5e93\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u7528\u4e8e\u5fae\u8c03\u7684\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u4f20\u7edf\u5bf9\u8bdd\u89c4\u5212\u5668\u4e0e\u6559\u5e08LLM\u5408\u6210\"\u9ec4\u91d1\u96c6\"\u5bf9\u8bdd\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u5fae\u8c03\u540e\u7684\u5b66\u751f\u6a21\u578b\u4e0e\u76f8\u540c\u89c4\u5212\u5668\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u65b0\u5bf9\u8bdd\u5408\u6210\u3002", "result": "\u5fae\u8c03\u540e\u7684\u5b66\u751f\u6a21\u578bBLEU\u4ece0.38\u63d0\u5347\u52300.50\uff0cBERTScore\u4ece0.88\u63d0\u5347\u52300.91\uff0c\u4e14\u53ef\u5728\u5355\u4e2a\u6d88\u8d39\u7ea7GPU\u4e0a\u8fd0\u884c\u3002", "conclusion": "APIDA-Chat\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u5f00\u6e90\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4f5c\u4e3a\u672a\u6765\u5de5\u4f5c\u7684\u4fdd\u5b88\u57fa\u7ebf\uff0c\u652f\u6301\u4f4e\u6210\u672cAPI\u5bf9\u8bdd\u6570\u636e\u751f\u6210\u3002", "topic": "code agent"}}
{"id": "2510.03755", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03755", "abs": "https://arxiv.org/abs/2510.03755", "authors": ["Roham Koohestani", "Parham Bateni", "Aydin Ebrahimi", "Behdad Etezadi", "Kiarash Karimi", "Maliheh Izadi"], "title": "Code4MeV2: a Research-oriented Code-completion Platform", "comment": "Under review for submission at a conference", "summary": "The adoption of AI-powered code completion tools in software development has\nincreased substantially, yet the user interaction data produced by these\nsystems remain proprietary within large corporations. This creates a barrier\nfor the academic community, as researchers must often develop dedicated\nplatforms to conduct studies on human--AI interaction, making reproducible\nresearch and large-scale data analysis impractical. In this work, we introduce\nCode4MeV2, a research-oriented, open-source code completion plugin for\nJetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a\nclient--server architecture and features inline code completion and a\ncontext-aware chat assistant. Its core contribution is a modular and\ntransparent data collection framework that gives researchers fine-grained\ncontrol over telemetry and context gathering. Code4MeV2 achieves\nindustry-comparable performance in terms of code completion, with an average\nlatency of 200~ms. We assess our tool through a combination of an expert\nevaluation and a user study with eight participants. Feedback from both\nresearchers and daily users highlights its informativeness and usefulness. We\ninvite the community to adopt and contribute to this tool. More information\nabout the tool can be found at https://app.code4me.me.", "AI": {"tldr": "Code4MeV2\u662f\u4e00\u4e2a\u5f00\u6e90\u4ee3\u7801\u8865\u5168\u63d2\u4ef6\uff0c\u91c7\u7528\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u67b6\u6784\uff0c\u63d0\u4f9b\u5185\u8054\u4ee3\u7801\u8865\u5168\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u804a\u5929\u52a9\u624b\uff0c\u65e8\u5728\u89e3\u51b3AI\u4ee3\u7801\u8865\u5168\u5de5\u5177\u7528\u6237\u4ea4\u4e92\u6570\u636e\u4e13\u6709\u5316\u95ee\u9898\u3002", "motivation": "AI\u4ee3\u7801\u8865\u5168\u5de5\u5177\u7684\u7528\u6237\u4ea4\u4e92\u6570\u636e\u88ab\u5927\u516c\u53f8\u4e13\u6709\u5316\uff0c\u963b\u788d\u4e86\u5b66\u672f\u7814\u7a76\u3002\u7814\u7a76\u4eba\u5458\u9700\u8981\u5f00\u53d1\u4e13\u7528\u5e73\u53f0\u6765\u7814\u7a76\u4eba\u673a\u4ea4\u4e92\uff0c\u4f7f\u5f97\u53ef\u91cd\u590d\u7814\u7a76\u548c\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u5f00\u53d1Code4MeV2\u63d2\u4ef6\uff0c\u91c7\u7528\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u67b6\u6784\uff0c\u5305\u542b\u6a21\u5757\u5316\u548c\u900f\u660e\u7684\u6570\u636e\u6536\u96c6\u6846\u67b6\uff0c\u8ba9\u7814\u7a76\u4eba\u5458\u80fd\u591f\u7cbe\u7ec6\u63a7\u5236\u9065\u6d4b\u548c\u4e0a\u4e0b\u6587\u6536\u96c6\u3002", "result": "Code4MeV2\u5728\u4ee3\u7801\u8865\u5168\u65b9\u9762\u8fbe\u5230\u884c\u4e1a\u53ef\u6bd4\u6027\u80fd\uff0c\u5e73\u5747\u5ef6\u8fdf200\u6beb\u79d2\u3002\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f30\u548c8\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5de5\u5177\u7684\u4fe1\u606f\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "Code4MeV2\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u5f00\u653e\u7684\u6570\u636e\u6536\u96c6\u89e3\u51b3\u65b9\u6848\uff0c\u9080\u8bf7\u793e\u533a\u91c7\u7528\u548c\u8d21\u732e\u3002", "topic": "swe application"}}
{"id": "2510.03485", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03485", "abs": "https://arxiv.org/abs/2510.03485", "authors": ["Xiaofei Wen", "Wenjie Jacky Mo", "Yanan Xie", "Peng Qi", "Muhao Chen"], "title": "Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection", "comment": "16 pages, 5 figures", "summary": "Autonomous web agents need to operate under externally imposed or\nhuman-specified policies while generating long-horizon trajectories. However,\nlittle work has examined whether these trajectories comply with such policies,\nor whether policy violations persist across different contexts such as domains\n(e.g., shopping or coding websites) and subdomains (e.g., product search and\norder management in shopping). To address this gap, we introduce\nPolicyGuardBench, a benchmark of about 60k examples for detecting policy\nviolations in agent trajectories. From diverse agent runs, we generate a broad\nset of policies and create both within subdomain and cross subdomain pairings\nwith violation labels. In addition to full-trajectory evaluation,\nPolicyGuardBench also includes a prefix-based violation detection task where\nmodels must anticipate policy violations from truncated trajectory prefixes\nrather than complete sequences. Using this dataset, we train PolicyGuard-4B, a\nlightweight guardrail model that delivers strong detection accuracy across all\ntasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes\nacross domains and preserves high accuracy on unseen settings. Together,\nPolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework\nfor studying policy compliance in web agent trajectories, and show that\naccurate and generalizable guardrails are feasible at small scales.", "AI": {"tldr": "\u63d0\u51fa\u4e86PolicyGuardBench\u57fa\u51c6\u548cPolicyGuard-4B\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u7f51\u7edc\u667a\u80fd\u4f53\u8f68\u8ff9\u4e2d\u7684\u7b56\u7565\u8fdd\u89c4\u884c\u4e3a\uff0c\u5305\u62ec\u8de8\u57df\u6cdb\u5316\u548c\u524d\u7f00\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u81ea\u4e3b\u7f51\u7edc\u667a\u80fd\u4f53\u9700\u8981\u5728\u5916\u90e8\u7b56\u7565\u7ea6\u675f\u4e0b\u751f\u6210\u957f\u671f\u8f68\u8ff9\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u5173\u6ce8\u8fd9\u4e9b\u8f68\u8ff9\u662f\u5426\u9075\u5b88\u7b56\u7565\uff0c\u4ee5\u53ca\u7b56\u7565\u8fdd\u89c4\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u7684\u6301\u7eed\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b\u7ea66\u4e07\u4e2a\u6837\u672c\u7684PolicyGuardBench\u57fa\u51c6\uff0c\u4ece\u591a\u6837\u5316\u667a\u80fd\u4f53\u8fd0\u884c\u4e2d\u751f\u6210\u7b56\u7565\u96c6\uff0c\u521b\u5efa\u57df\u5185\u548c\u8de8\u57df\u914d\u5bf9\u5e76\u6807\u6ce8\u8fdd\u89c4\u6807\u7b7e\u3002\u8bad\u7ec3\u8f7b\u91cf\u7ea7PolicyGuard-4B\u6a21\u578b\u8fdb\u884c\u8fdd\u89c4\u68c0\u6d4b\u3002", "result": "PolicyGuard-4B\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\uff0c\u5e76\u5728\u8de8\u57df\u548c\u672a\u89c1\u8bbe\u7f6e\u4e2d\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "PolicyGuardBench\u548cPolicyGuard-4B\u4e3a\u7814\u7a76\u7f51\u7edc\u667a\u80fd\u4f53\u7b56\u7565\u5408\u89c4\u6027\u63d0\u4f9b\u4e86\u9996\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u8bc1\u660e\u5728\u5c0f\u89c4\u6a21\u4e0b\u5b9e\u73b0\u51c6\u786e\u4e14\u53ef\u6cdb\u5316\u7684\u62a4\u680f\u662f\u53ef\u884c\u7684\u3002", "topic": "agent analysis"}}
{"id": "2510.03843", "categories": ["cs.SE", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03843", "abs": "https://arxiv.org/abs/2510.03843", "authors": ["Vincent Nguyen", "Guilherme Herzog", "Jos\u00e9 Cambronero", "Marcus Revaj", "Aditya Kini", "Alexander Fr\u00f6mmgen", "Maxim Tabachnyk"], "title": "Smart Paste: Automatically Fixing Copy/Paste for Google Developers", "comment": "11 pages", "summary": "Manually editing pasted code is a long-standing developer pain point. In\ninternal software development at Google, we observe that code is pasted 4 times\nmore often than it is manually typed. These paste actions frequently require\nfollow-up edits, ranging from simple reformatting and renaming to more complex\nstyle adjustments and cross-language translations. Prior work has shown deep\nlearning can be used to predict these edits. In this work, we show how to\niteratively develop and scale Smart Paste, an IDE feature for post-paste edit\nsuggestions, to Google's development environment. This experience can serve as\na guide for AI practitioners on a holistic approach to feature development,\ncovering user experience, system integration, and model capabilities. Since\ndeployment, Smart Paste has had overwhelmingly positive feedback with a 45%\nacceptance rate. At Google's enterprise scale, these accepted suggestions\naccount substantially for over 1% of all code written company-wide.", "AI": {"tldr": "\u5f00\u53d1\u4e86Smart Paste IDE\u529f\u80fd\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u7c98\u8d34\u4ee3\u7801\u540e\u7684\u7f16\u8f91\u5efa\u8bae\uff0c\u5728Google\u5185\u90e8\u90e8\u7f72\u540e\u83b7\u5f9745%\u7684\u63a5\u53d7\u7387\uff0c\u5360\u516c\u53f8\u4ee3\u7801\u603b\u91cf\u76841%\u4ee5\u4e0a", "motivation": "\u624b\u52a8\u7f16\u8f91\u7c98\u8d34\u4ee3\u7801\u662f\u5f00\u53d1\u8005\u7684\u957f\u671f\u75db\u70b9\uff0cGoogle\u5185\u90e8\u6570\u636e\u663e\u793a\u4ee3\u7801\u7c98\u8d34\u9891\u7387\u662f\u624b\u52a8\u8f93\u5165\u76844\u500d\uff0c\u4e14\u7c98\u8d34\u540e\u7ecf\u5e38\u9700\u8981\u540e\u7eed\u7f16\u8f91", "method": "\u8fed\u4ee3\u5f00\u53d1\u548c\u6269\u5c55Smart Paste IDE\u529f\u80fd\uff0c\u6db5\u76d6\u7528\u6237\u4f53\u9a8c\u3001\u7cfb\u7edf\u96c6\u6210\u548c\u6a21\u578b\u80fd\u529b\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u7c98\u8d34\u540e\u7684\u7f16\u8f91\u9700\u6c42", "result": "\u90e8\u7f72\u540e\u83b7\u5f97\u538b\u5012\u6027\u79ef\u6781\u53cd\u9988\uff0c45%\u7684\u63a5\u53d7\u7387\uff0c\u5728Google\u4f01\u4e1a\u89c4\u6a21\u4e0b\uff0c\u8fd9\u4e9b\u5efa\u8bae\u5360\u516c\u53f8\u6240\u6709\u4ee3\u7801\u76841%\u4ee5\u4e0a", "conclusion": "Smart Paste\u6210\u529f\u89e3\u51b3\u4e86\u4ee3\u7801\u7c98\u8d34\u540e\u7684\u7f16\u8f91\u75db\u70b9\uff0c\u4e3aAI\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u529f\u80fd\u5f00\u53d1\u7684\u6574\u4f53\u65b9\u6cd5\u6307\u5357", "topic": "swe application"}}
{"id": "2510.03862", "categories": ["cs.SE", "cs.AI", "500"], "pdf": "https://arxiv.org/pdf/2510.03862", "abs": "https://arxiv.org/abs/2510.03862", "authors": ["Nathalia Nascimento", "Everton Guimaraes", "Paulo Alencar"], "title": "Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework", "comment": "5 pages", "summary": "The rise of large language models (LLMs) has introduced transformative\npotential in automated code generation, addressing a wide range of software\nengineering challenges. However, empirical evaluation of LLM-based code\ngeneration lacks standardization, with studies varying widely in goals, tasks,\nand metrics, which limits comparability and reproducibility. In this paper, we\npropose a theoretical framework for designing and reporting empirical studies\non LLM-based code generation. The framework is grounded in both our prior\nexperience conducting such experiments and a comparative analysis of key\nsimilarities and differences among recent studies. It organizes evaluation\naround core components such as problem sources, quality attributes, and\nmetrics, supporting structured and systematic experimentation. We demonstrate\nits applicability through representative case mappings and identify\nopportunities for refinement. Looking forward, we plan to evolve the framework\ninto a more robust and mature tool for standardizing LLM evaluation across\nsoftware engineering contexts.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8e\u8bbe\u8ba1\u548c\u62a5\u544a\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u5b9e\u8bc1\u7814\u7a76\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u8bc4\u4f30\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u95ee\u9898", "motivation": "\u5f53\u524dLLM\u4ee3\u7801\u751f\u6210\u7684\u5b9e\u8bc1\u8bc4\u4f30\u7f3a\u4e4f\u6807\u51c6\u5316\uff0c\u7814\u7a76\u5728\u76ee\u6807\u3001\u4efb\u52a1\u548c\u6307\u6807\u4e0a\u5dee\u5f02\u5f88\u5927\uff0c\u9650\u5236\u4e86\u53ef\u6bd4\u6027\u548c\u53ef\u91cd\u590d\u6027", "method": "\u57fa\u4e8e\u5148\u524d\u7ecf\u9a8c\u548c\u8fd1\u671f\u7814\u7a76\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u6784\u5efa\u56f4\u7ed5\u95ee\u9898\u6765\u6e90\u3001\u8d28\u91cf\u5c5e\u6027\u548c\u6307\u6807\u7b49\u6838\u5fc3\u7ec4\u4ef6\u7684\u8bc4\u4f30\u6846\u67b6", "result": "\u901a\u8fc7\u4ee3\u8868\u6027\u6848\u4f8b\u6620\u5c04\u5c55\u793a\u4e86\u6846\u67b6\u7684\u9002\u7528\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u6539\u8fdb\u673a\u4f1a", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u80cc\u666f\u4e0b\u6807\u51c6\u5316LLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u8ba1\u5212\u5c06\u5176\u53d1\u5c55\u4e3a\u66f4\u6210\u719f\u7684\u5de5\u5177", "topic": "swe benchmark"}}
{"id": "2510.03253", "categories": ["cs.LG", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03253", "abs": "https://arxiv.org/abs/2510.03253", "authors": ["Heyang Gao", "Zexu Sun", "Erxue Min", "Hengyi Cai", "Shuaiqiang Wang", "Dawei Yin", "Xu Chen"], "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents", "comment": "Preprint", "summary": "Large Language Models (LLMs) as autonomous agents are increasingly tasked\nwith solving complex, long-horizon problems. Aligning these agents via\npreference-based offline methods like Direct Preference Optimization (DPO) is a\npromising direction, yet it faces a critical granularity mismatch.\nTrajectory-level DPO provides a signal that is too coarse for precise credit\nassignment, while step-level DPO is often too myopic to capture the value of\nmulti-step behaviors. To resolve this challenge, we introduce Hierarchical\nPreference Learning (HPL), a hierarchical framework that optimizes LLM agents\nby leveraging preference signals at multiple, synergistic granularities. While\nHPL incorporates trajectory- and step-level DPO for global and local policy\nstability, its core innovation lies in group-level preference optimization\nguided by a dual-layer curriculum. Our approach first decomposes expert\ntrajectories into semantically coherent action groups and then generates\ncontrasting suboptimal groups to enable preference learning at a fine-grained,\nsub-task level. Then, instead of treating all preference pairs equally, HPL\nintroduces a curriculum scheduler that organizes the learning process from\nsimple to complex. This curriculum is structured along two axes: the group\nlength, representing sub-task complexity, and the sample difficulty, defined by\nthe reward gap between preferred and dispreferred action groups. Experiments on\nthree challenging agent benchmarks show that HPL outperforms existing\nstate-of-the-art methods. Our analyses demonstrate that the hierarchical DPO\nloss effectively integrates preference signals across multiple granularities,\nwhile the dual-layer curriculum is crucial for enabling the agent to solve a\nwide range of tasks, from simple behaviors to complex multi-step sequences.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5206\u5c42\u504f\u597d\u5b66\u4e60\uff08HPL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u504f\u597d\u4fe1\u53f7\u4f18\u5316LLM\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4e86\u8f68\u8ff9\u7ea7DPO\u4fe1\u53f7\u8fc7\u4e8e\u7c97\u7cd9\u548c\u6b65\u9aa4\u7ea7DPO\u8fc7\u4e8e\u77ed\u89c6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u504f\u597d\u7684\u79bb\u7ebf\u65b9\u6cd5\u5982DPO\u5728\u4f18\u5316LLM\u667a\u80fd\u4f53\u65f6\u9762\u4e34\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff1a\u8f68\u8ff9\u7ea7DPO\u4fe1\u53f7\u8fc7\u4e8e\u7c97\u7cd9\uff0c\u6b65\u9aa4\u7ea7DPO\u8fc7\u4e8e\u77ed\u89c6\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u591a\u6b65\u884c\u4e3a\u7684\u4ef7\u503c\u3002", "method": "HPL\u6846\u67b6\u7ed3\u5408\u8f68\u8ff9\u7ea7\u548c\u6b65\u9aa4\u7ea7DPO\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u7ec4\u7ea7\u504f\u597d\u4f18\u5316\u548c\u53cc\u5c42\u8bfe\u7a0b\u8c03\u5ea6\u3002\u9996\u5148\u5c06\u4e13\u5bb6\u8f68\u8ff9\u5206\u89e3\u4e3a\u8bed\u4e49\u4e00\u81f4\u7684\u52a8\u4f5c\u7ec4\uff0c\u751f\u6210\u5bf9\u6bd4\u6b21\u4f18\u7ec4\u8fdb\u884c\u7ec6\u7c92\u5ea6\u504f\u597d\u5b66\u4e60\uff0c\u7136\u540e\u901a\u8fc7\u8bfe\u7a0b\u8c03\u5ea6\u5668\u4ece\u7b80\u5355\u5230\u590d\u6742\u7ec4\u7ec7\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHPL\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5206\u6790\u8868\u660e\u5206\u5c42DPO\u635f\u5931\u6709\u6548\u6574\u5408\u591a\u7c92\u5ea6\u504f\u597d\u4fe1\u53f7\uff0c\u53cc\u5c42\u8bfe\u7a0b\u5bf9\u89e3\u51b3\u4ece\u7b80\u5355\u884c\u4e3a\u5230\u590d\u6742\u591a\u6b65\u5e8f\u5217\u7684\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "HPL\u901a\u8fc7\u5206\u5c42\u504f\u597d\u5b66\u4e60\u548c\u8bfe\u7a0b\u8c03\u5ea6\u6210\u529f\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u4f18\u5316\u4e2d\u7684\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u957f\u89c6\u91ce\u95ee\u9898\u7684\u89e3\u51b3\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.03612", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03612", "abs": "https://arxiv.org/abs/2510.03612", "authors": ["Tanqiu Jiang", "Min Bai", "Nikolaos Pappas", "Yanjun Qi", "Sandesh Swamy"], "title": "Cross-Modal Content Optimization for Steering Web Agent Preferences", "comment": null, "summary": "Vision-language model (VLM)-based web agents increasingly power high-stakes\nselection tasks like content recommendation or product ranking by combining\nmultimodal perception with preference reasoning. Recent studies reveal that\nthese agents are vulnerable against attackers who can bias selection outcomes\nthrough preference manipulations using adversarial pop-ups, image\nperturbations, or content tweaks. Existing work, however, either assumes strong\nwhite-box access, with limited single-modal perturbations, or uses impractical\nsettings. In this paper, we demonstrate, for the first time, that joint\nexploitation of visual and textual channels yields significantly more powerful\npreference manipulations under realistic attacker capabilities. We introduce\nCross-Modal Preference Steering (CPS) that jointly optimizes imperceptible\nmodifications to an item's visual and natural language descriptions, exploiting\nCLIP-transferable image perturbations and RLHF-induced linguistic biases to\nsteer agent decisions. In contrast to prior studies that assume gradient\naccess, or control over webpages, or agent memory, we adopt a realistic\nblack-box threat setup: a non-privileged adversary can edit only their own\nlisting's images and textual metadata, with no insight into the agent's model\ninternals. We evaluate CPS on agents powered by state-of-the-art proprietary\nand open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both\nmovie selection and e-commerce tasks. Our results show that CPS is\nsignificantly more effective than leading baseline methods. For instance, our\nresults show that CPS consistently outperforms baselines across all models\nwhile maintaining 70% lower detection rates, demonstrating both effectiveness\nand stealth. These findings highlight an urgent need for robust defenses as\nagentic systems play an increasingly consequential role in society.", "AI": {"tldr": "\u63d0\u51fa\u8de8\u6a21\u6001\u504f\u597d\u5f15\u5bfc(CPS)\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89c6\u89c9\u548c\u6587\u672c\u901a\u9053\u7684\u4e0d\u53ef\u5bdf\u89c9\u4fee\u6539\uff0c\u5728\u73b0\u5b9e\u9ed1\u76d2\u5a01\u80c1\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u5347\u5bf9\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7f51\u7edc\u4ee3\u7406\u7684\u504f\u597d\u64cd\u7eb5\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7f51\u7edc\u4ee3\u7406\u5bb9\u6613\u53d7\u5230\u504f\u597d\u64cd\u7eb5\u653b\u51fb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5047\u8bbe\u767d\u76d2\u8bbf\u95ee\u6743\u9650\uff0c\u8981\u4e48\u4f7f\u7528\u4e0d\u5207\u5b9e\u9645\u7684\u8bbe\u7f6e\u3002\u672c\u6587\u65e8\u5728\u5728\u73b0\u5b9e\u7684\u9ed1\u76d2\u5a01\u80c1\u8bbe\u7f6e\u4e0b\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u504f\u597d\u5f15\u5bfc(CPS)\u65b9\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u7269\u54c1\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u4e0d\u53ef\u5bdf\u89c9\u4fee\u6539\uff0c\u5229\u7528CLIP\u53ef\u8fc1\u79fb\u56fe\u50cf\u6270\u52a8\u548cRLHF\u8bf1\u5bfc\u7684\u8bed\u8a00\u504f\u89c1\u6765\u5f15\u5bfc\u4ee3\u7406\u51b3\u7b56\u3002", "result": "\u5728GPT-4.1\u3001Qwen-2.5VL\u548cPixtral-Large\u7b49\u6700\u5148\u8fdb\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCPS\u5728\u6240\u6709\u6a21\u578b\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u630170%\u66f4\u4f4e\u7684\u68c0\u6d4b\u7387\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u968f\u7740\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u5728\u793e\u4f1a\u4e2d\u626e\u6f14\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2510.03879", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03879", "abs": "https://arxiv.org/abs/2510.03879", "authors": ["Tianyu Li", "Ruishi Li", "Bo Wang", "Brandon Paulsen", "Umang Mathur", "Prateek Saxena"], "title": "Adversarial Agent Collaboration for C to Rust Translation", "comment": null, "summary": "Translating C to memory-safe languages, like Rust, prevents critical memory\nsafety vulnerabilities that are prevalent in legacy C software. Existing\napproaches for C to safe Rust translation, including LLM-assisted ones, do not\ngeneralize on larger (> 500 LoC) C codebases because they depend on complex\nprogram analyses that frequently break. In this work, we present ACToR\n(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired\nby GANs, ACToR pits a generator agent against a discriminator agent, which\ncollaborate to iteratively generate a Rust translation. On each iteration, the\ntranslator agent synthesizes and refines a Rust translation to pass an existing\nsuite of tests, and then the discriminator agent finds new failing tests. We\ndemonstrate that ACToR translates all of the 63 real-world command line\nutilities considered in our benchmarks, which have an average size of 485 lines\nof code, and it achieves over 90% test pass rate with zero human intervention.\nTo our knowledge, it is the first such system that reliably translates C\nprograms of this scale. Furthermore, ACToR improves translation correctness by\nup to 18.9% compared to baseline, non-adversarial approaches.", "AI": {"tldr": "ACToR\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684C\u5230Rust\u7ffb\u8bd1\u5668\uff0c\u91c7\u7528\u751f\u6210\u5668-\u5224\u522b\u5668\u5bf9\u6297\u673a\u5236\uff0c\u80fd\u591f\u53ef\u9760\u5730\u5c06\u5927\u578bC\u4ee3\u7801\u5e93\uff08\u5e73\u5747485\u884c\uff09\u8f6c\u6362\u4e3a\u5185\u5b58\u5b89\u5168\u7684Rust\u4ee3\u7801\uff0c\u6d4b\u8bd5\u901a\u8fc7\u7387\u8d85\u8fc790%\u3002", "motivation": "\u73b0\u6709C\u5230Rust\u7ffb\u8bd1\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u5927\u578b\u4ee3\u7801\u5e93\uff08>500\u884c\uff09\uff0c\u56e0\u4e3a\u4f9d\u8d56\u590d\u6742\u7684\u7a0b\u5e8f\u5206\u6790\u5bb9\u6613\u5931\u8d25\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u9632\u6b62\u5185\u5b58\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u6027\u65b9\u6cd5\uff0c\u751f\u6210\u5668\u4ee3\u7406\u5408\u6210\u548c\u4f18\u5316Rust\u7ffb\u8bd1\u4ee5\u901a\u8fc7\u6d4b\u8bd5\uff0c\u5224\u522b\u5668\u4ee3\u7406\u5bfb\u627e\u65b0\u7684\u5931\u8d25\u6d4b\u8bd5\uff0c\u4e24\u8005\u534f\u4f5c\u8fed\u4ee3\u6539\u8fdb\u7ffb\u8bd1\u3002", "result": "\u6210\u529f\u7ffb\u8bd1\u4e8663\u4e2a\u771f\u5b9e\u4e16\u754c\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u5e73\u5747\u4ee3\u7801\u91cf485\u884c\uff0c\u6d4b\u8bd5\u901a\u8fc7\u7387\u8d85\u8fc790%\uff0c\u6bd4\u975e\u5bf9\u6297\u6027\u65b9\u6cd5\u63d0\u534718.9%\u7684\u6b63\u786e\u6027\u3002", "conclusion": "ACToR\u662f\u9996\u4e2a\u80fd\u591f\u53ef\u9760\u7ffb\u8bd1\u8fd9\u79cd\u89c4\u6a21C\u7a0b\u5e8f\u7684\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u96f6\u4eba\u5de5\u5e72\u9884\u7684\u9ad8\u8d28\u91cf\u7ffb\u8bd1\u3002", "topic": "code agent"}}
{"id": "2510.03632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03632", "abs": "https://arxiv.org/abs/2510.03632", "authors": ["Jiaxi Li", "Yucheng Shi", "Jin Lu", "Ninghao Liu"], "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information", "comment": "18 pages", "summary": "Tree search has become as a representative framework for test-time reasoning\nwith large language models (LLMs), exemplified by methods such as\nTree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning\npaths. However, it remains difficult to provide instant and reliable\nquantitative assessments of intermediate reasoning step quality, and extensive\npath exploration is computationally costly. To address this, we propose Mutual\nInformation Tree Search (MITS), a novel framework that guides reasoning with\ninformation-theoretic principles. MITS introduces an effective scoring function\nbased on pointwise mutual information (PMI), which enables step-wise evaluation\nof reasoning paths and search tree expansion via beam search without expensive\nlook-ahead simulations, achieving superior reasoning performances while\nmaintaining computational efficiency. The framework is complemented by an\nentropy-based dynamic sampling strategy that adaptively allocates computational\nresources to uncertain reasoning steps where exploration is most beneficial.\nFor final prediction, MITS employs a weighted voting scheme that combines PMI\nscores with prediction consensus. Through comprehensive experiments on diverse\nreasoning benchmarks, MITS consistently surpasses baseline methods,\nestablishing a principled and efficient framework for LLM reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u6811\u641c\u7d22\u6846\u67b6MITS\uff0c\u901a\u8fc7\u70b9\u4e92\u4fe1\u606f\u8bc4\u5206\u548c\u71b5\u57fa\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6811\u641c\u7d22\u65b9\u6cd5\u96be\u4ee5\u5bf9\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u5373\u65f6\u53ef\u9760\u7684\u5b9a\u91cf\u8bc4\u4f30\uff0c\u4e14\u5e7f\u6cdb\u8def\u5f84\u63a2\u7d22\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u4f7f\u7528\u70b9\u4e92\u4fe1\u606f(PMI)\u4f5c\u4e3a\u8bc4\u5206\u51fd\u6570\u8fdb\u884c\u6b65\u9aa4\u8bc4\u4f30\uff0c\u901a\u8fc7\u6ce2\u675f\u641c\u7d22\u6269\u5c55\u641c\u7d22\u6811\uff0c\u91c7\u7528\u71b5\u57fa\u52a8\u6001\u91c7\u6837\u7b56\u7565\u81ea\u9002\u5e94\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u6700\u540e\u4f7f\u7528\u52a0\u6743\u6295\u7968\u65b9\u6848\u7ed3\u5408PMI\u5206\u6570\u548c\u9884\u6d4b\u5171\u8bc6\u3002", "result": "\u5728\u591a\u6837\u5316\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMITS\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MITS\u4e3aLLM\u63a8\u7406\u5efa\u7acb\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u9ad8\u6548\u7684\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2510.03257", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03257", "abs": "https://arxiv.org/abs/2510.03257", "authors": ["Zijian Zhao", "Sen Li"], "title": "Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?", "comment": null, "summary": "On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate\nreal-time challenge of bundling and matching passengers-each with distinct\norigins and destinations-to available vehicles, all while navigating\nsignificant system uncertainties. Due to the extensive observation space\narising from the large number of drivers and orders, order dispatching, though\nfundamentally a centralized task, is often addressed using Multi-Agent\nReinforcement Learning (MARL). However, independent MARL methods fail to\ncapture global information and exhibit poor cooperation among workers, while\nCentralized Training Decentralized Execution (CTDE) MARL methods suffer from\nthe curse of dimensionality. To overcome these challenges, we propose\nTriple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method\ndesigned specifically for large-scale order dispatching on ride-sharing\nplatforms. Built on a variant TD3, our approach addresses the vast action space\nthrough an action decomposition strategy that breaks down the joint action\nprobability into individual driver action probabilities. To handle the\nextensive observation space, we introduce a novel BERT-based network, where\nparameter reuse mitigates parameter growth as the number of drivers and orders\nincreases, and the attention mechanism effectively captures the complex\nrelationships among the large pool of driver and orders. We validate our method\nusing a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves\napproximately an 11.95% improvement over current state-of-the-art methods, with\na 4.26% increase in served orders and a 22.25% reduction in pickup times. Our\ncode, trained model parameters, and processed data are publicly available at\nthe repository https://github.com/RS2002/Triple-BERT .", "AI": {"tldr": "\u63d0\u51faTriple-BERT\u65b9\u6cd5\uff0c\u4f7f\u7528\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u7f51\u7ea6\u8f66\u5e73\u53f0\u5927\u89c4\u6a21\u8ba2\u5355\u8c03\u5ea6\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u4f5c\u5206\u89e3\u7b56\u7565\u548cBERT\u7f51\u7edc\u5904\u7406\u5927\u89c4\u6a21\u52a8\u4f5c\u548c\u89c2\u6d4b\u7a7a\u95f4\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u7f51\u7ea6\u8f66\u5e73\u53f0\u9762\u4e34\u5927\u89c4\u6a21\u5b9e\u65f6\u8ba2\u5355\u8c03\u5ea6\u6311\u6218\uff0c\u73b0\u6709MARL\u65b9\u6cd5\u5b58\u5728\u5168\u5c40\u4fe1\u606f\u6355\u83b7\u4e0d\u8db3\u3001\u534f\u4f5c\u5dee\u6216\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eTD3\u53d8\u4f53\u6784\u5efa\u96c6\u4e2d\u5f0f\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u91c7\u7528\u52a8\u4f5c\u5206\u89e3\u7b56\u7565\u5c06\u8054\u5408\u52a8\u4f5c\u6982\u7387\u5206\u89e3\u4e3a\u5355\u4e2a\u53f8\u673a\u52a8\u4f5c\u6982\u7387\uff0c\u4f7f\u7528BERT\u7f51\u7edc\u901a\u8fc7\u53c2\u6570\u91cd\u7528\u548c\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u5927\u89c4\u6a21\u89c2\u6d4b\u7a7a\u95f4\u3002", "result": "\u5728\u66fc\u54c8\u987f\u771f\u5b9e\u7f51\u7ea6\u8f66\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u5347\u7ea611.95%\uff0c\u670d\u52a1\u8ba2\u5355\u589e\u52a04.26%\uff0c\u63a5\u5355\u65f6\u95f4\u51cf\u5c1122.25%\u3002", "conclusion": "Triple-BERT\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u8ba2\u5355\u8c03\u5ea6\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.03696", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03696", "abs": "https://arxiv.org/abs/2510.03696", "authors": ["Deepak Babu Piskala", "Sharlene Chen", "Udita Patel", "Parul Kalra", "Rafael Castrillo"], "title": "Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models", "comment": null, "summary": "Evaluating the quality of multi-turn chatbot interactions remains\nchallenging, as most existing methods assess interactions at the turn level\nwithout addressing whether a user's overarching goal was fulfilled. A ``goal''\nhere refers to an information need or task, such as asking for policy\ninformation or applying for leave. We propose a comprehensive framework for\ngoal-oriented evaluation of multi-agent systems (MAS), introducing the\n\\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,\nand a \\textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for\nfailure in multi-agent chatbots. Our method segments conversations by user\ngoals and evaluates success using all relevant turns. We present a model-based\nevaluation system combining teacher LLMs, where domain experts define goals,\nset quality standards serving as a guidance for the LLMs. The LLMs use\n``thinking tokens'' to produce interpretable rationales, enabling\n\\textit{explainable}, \\textit{data-efficient} evaluations. In an enterprise\nsetting, we apply our framework to evaluate AIDA, a zero-to-one employee\nconversational agent system built as a ground-up multi-agent conversational\nagent, and observe GSR improvement from 63\\% to 79\\% over six months since its\ninception. Our framework is generic and offers actionable insights through a\ndetailed defect taxonomy based on analysis of failure points in multi-agent\nchatbots, diagnosing overall success, identifying key failure modes, and\ninforming system improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u76ee\u6807\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u76ee\u6807\u6210\u529f\u7387(GSR)\u548c\u5931\u8d25\u6839\u56e0\u5206\u7c7b(RCOF)\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7528\u6237\u76ee\u6807\u662f\u5426\u8fbe\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5728\u8f6e\u6b21\u5c42\u9762\u8bc4\u4f30\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\uff0c\u65e0\u6cd5\u5224\u65ad\u7528\u6237\u7684\u603b\u4f53\u76ee\u6807\u662f\u5426\u5b9e\u73b0\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u76ee\u6807\u5bfc\u5411\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u7528\u6237\u76ee\u6807\u5206\u5272\u5bf9\u8bdd\uff0c\u4f7f\u7528\u6559\u5e08LLM\u7ed3\u5408\u9886\u57df\u4e13\u5bb6\u5b9a\u4e49\u7684\u76ee\u6807\u548c\u8d28\u91cf\u6807\u51c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5229\u7528\"\u601d\u8003\u6807\u8bb0\"\u751f\u6210\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5e94\u7528\u8be5\u6846\u67b6\u8bc4\u4f30AIDA\u5458\u5de5\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u89c2\u5bdf\u5230\u76ee\u6807\u6210\u529f\u7387\u4ece63%\u63d0\u5347\u523079%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\uff0c\u901a\u8fc7\u8be6\u7ec6\u7684\u7f3a\u9677\u5206\u7c7b\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u80fd\u591f\u8bca\u65ad\u6574\u4f53\u6210\u529f\u7387\u3001\u8bc6\u522b\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u5e76\u6307\u5bfc\u7cfb\u7edf\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2510.03902", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03902", "abs": "https://arxiv.org/abs/2510.03902", "authors": ["Rana Nameer Hussain Khan", "Dawood Wasif", "Jin-Hee Cho", "Ali Butt"], "title": "Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code", "comment": null, "summary": "The increasing complexity of cloud-native infrastructure has made\nInfrastructure-as-Code (IaC) essential for reproducible and scalable\ndeployments. While large language models (LLMs) have shown promise in\ngenerating IaC snippets from natural language prompts, their monolithic,\nsingle-pass generation approach often results in syntactic errors, policy\nviolations, and unscalable designs. In this paper, we propose MACOG\n(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based\narchitecture for IaC generation that decomposes the task into modular subtasks\nhandled by specialized agents: Architect, Provider Harmonizer, Engineer,\nReviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory\nCurator. The agents interact via a shared-blackboard, finite-state orchestrator\nlayer, and collectively produce Terraform configurations that are not only\nsyntactically valid but also policy-compliant and semantically coherent. To\nensure infrastructure correctness and governance, we incorporate Terraform Plan\nfor execution validation and Open Policy Agent (OPA) for customizable policy\nenforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the\ntop enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02\nand Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,\nCodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and\ndeploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,\nrespectively.", "AI": {"tldr": "MACOG\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53LLM\u7684IaC\u751f\u6210\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u5230\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u5408\u89c4\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfLLM\u5728\u751f\u6210\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801(IaC)\u65f6\u51fa\u73b0\u7684\u8bed\u6cd5\u9519\u8bef\u3001\u7b56\u7565\u8fdd\u89c4\u548c\u4e0d\u53ef\u6269\u5c55\u8bbe\u8ba1\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5305\u542b\u67b6\u6784\u5e08\u3001\u63d0\u4f9b\u5546\u534f\u8c03\u5668\u3001\u5de5\u7a0b\u5e08\u3001\u5ba1\u6838\u5458\u3001\u5b89\u5168\u8bc1\u660e\u8005\u3001\u6210\u672c\u5bb9\u91cf\u89c4\u5212\u5e08\u3001DevOps\u548c\u8bb0\u5fc6\u7b56\u5c55\u4eba\u7b49\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5171\u4eab\u9ed1\u677f\u548c\u6709\u9650\u72b6\u6001\u534f\u8c03\u5668\u8fdb\u884c\u4ea4\u4e92\u3002", "result": "\u5728IaC-Eval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cGPT-5\u4ece54.90\u63d0\u5347\u523074.02\uff0cGemini-2.5 Pro\u4ece43.56\u63d0\u5347\u523060.13\uff0c\u5728BLEU\u3001CodeBERTScore\u548cLLM-judge\u6307\u6807\u4e0a\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u5206\u89e3\u65b9\u6cd5\u548c\u7ea6\u675f\u89e3\u7801\u3001\u90e8\u7f72\u53cd\u9988\u673a\u5236\u5bf9\u63d0\u5347IaC\u751f\u6210\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002", "topic": "code agent"}}
{"id": "2510.03914", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03914", "abs": "https://arxiv.org/abs/2510.03914", "authors": ["Yonnel Chen Kuang Piao", "Jean Carlors Paul", "Leuson Da Silva", "Arghavan Moradi Dakhel", "Mohammad Hamdaqa", "Foutse Khomh"], "title": "Refactoring with LLMs: Bridging Human Expertise and Machine Understanding", "comment": "43 pages, 2 figures, 9 tables", "summary": "Code refactoring is a fundamental software engineering practice aimed at\nimproving code quality and maintainability. Despite its importance, developers\noften neglect refactoring due to the significant time, effort, and resources it\nrequires, as well as the lack of immediate functional rewards. Although several\nautomated refactoring tools have been proposed, they remain limited in\nsupporting a broad spectrum of refactoring types. In this study, we explore\nwhether instruction strategies inspired by human best-practice guidelines can\nenhance the ability of Large Language Models (LLMs) to perform diverse\nrefactoring tasks automatically. Leveraging the instruction-following and code\ncomprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and\nDeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design\nmultiple instruction strategies that encode motivations, procedural steps, and\ntransformation objectives for 61 well-known refactoring types. We evaluate\nthese strategies on benchmark examples and real-world code snippets from GitHub\nprojects. Our results show that instruction designs grounded in Fowler's\nguidelines enable LLMs to successfully perform all benchmark refactoring types\nand preserve program semantics in real-world settings, an essential criterion\nfor effective refactoring. Moreover, while descriptive instructions are more\ninterpretable to humans, our results show that rule-based instructions often\nlead to better performance in specific scenarios. Interestingly, allowing\nmodels to focus on the overall goal of refactoring, rather than prescribing a\nfixed transformation type, can yield even greater improvements in code quality.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u57fa\u4e8e\u4eba\u7c7b\u6700\u4f73\u5b9e\u8df5\u6307\u5357\u7684\u6307\u4ee4\u7b56\u7565\u80fd\u5426\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u6267\u884c\u591a\u6837\u5316\u4ee3\u7801\u91cd\u6784\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u57fa\u4e8eFowler\u91cd\u6784\u6307\u5357\u7684\u6307\u4ee4\u8bbe\u8ba1\u4f7fLLMs\u80fd\u591f\u6210\u529f\u6267\u884c\u6240\u6709\u57fa\u51c6\u91cd\u6784\u7c7b\u578b\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u4fdd\u6301\u7a0b\u5e8f\u8bed\u4e49\u3002", "motivation": "\u5f00\u53d1\u8005\u56e0\u91cd\u6784\u9700\u8981\u5927\u91cf\u65f6\u95f4\u3001\u7cbe\u529b\u548c\u8d44\u6e90\u800c\u5e38\u5e38\u5ffd\u89c6\u8fd9\u4e00\u91cd\u8981\u5b9e\u8df5\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u91cd\u6784\u5de5\u5177\u5728\u652f\u6301\u5e7f\u6cdb\u91cd\u6784\u7c7b\u578b\u65b9\u9762\u4ecd\u6709\u5c40\u9650\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u5229\u7528LLMs\u7684\u6307\u4ee4\u9075\u5faa\u548c\u4ee3\u7801\u7406\u89e3\u80fd\u529b\u6765\u6539\u8fdb\u81ea\u52a8\u5316\u91cd\u6784\u3002", "method": "\u5229\u7528GPT-mini\u548cDeepSeek-V3\u7b49\u5148\u8fdbLLMs\uff0c\u57fa\u4e8eMartin Fowler\u7684\u91cd\u6784\u6307\u5357\u8bbe\u8ba1\u591a\u79cd\u6307\u4ee4\u7b56\u7565\uff0c\u7f16\u780161\u79cd\u77e5\u540d\u91cd\u6784\u7c7b\u578b\u7684\u52a8\u673a\u3001\u7a0b\u5e8f\u6b65\u9aa4\u548c\u8f6c\u6362\u76ee\u6807\uff0c\u5e76\u5728\u57fa\u51c6\u793a\u4f8b\u548cGitHub\u771f\u5b9e\u4ee3\u7801\u7247\u6bb5\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u57fa\u4e8eFowler\u6307\u5357\u7684\u6307\u4ee4\u8bbe\u8ba1\u4f7fLLMs\u80fd\u591f\u6210\u529f\u6267\u884c\u6240\u6709\u57fa\u51c6\u91cd\u6784\u7c7b\u578b\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u4fdd\u6301\u7a0b\u5e8f\u8bed\u4e49\uff0c\u8fd9\u662f\u6709\u6548\u91cd\u6784\u7684\u5173\u952e\u6807\u51c6\u3002\u63cf\u8ff0\u6027\u6307\u4ee4\u5bf9\u4eba\u7c7b\u66f4\u6613\u89e3\u91ca\uff0c\u4f46\u89c4\u5219\u578b\u6307\u4ee4\u5728\u7279\u5b9a\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u5141\u8bb8\u6a21\u578b\u4e13\u6ce8\u4e8e\u91cd\u6784\u7684\u6574\u4f53\u76ee\u6807\u800c\u975e\u89c4\u5b9a\u56fa\u5b9a\u8f6c\u6362\u7c7b\u578b\uff0c\u53ef\u4ee5\u5e26\u6765\u66f4\u5927\u7684\u4ee3\u7801\u8d28\u91cf\u6539\u8fdb\u3002\u57fa\u4e8e\u4eba\u7c7b\u6700\u4f73\u5b9e\u8df5\u7684\u6307\u4ee4\u7b56\u7565\u80fd\u6709\u6548\u589e\u5f3aLLMs\u7684\u81ea\u52a8\u5316\u91cd\u6784\u80fd\u529b\u3002", "topic": "swe application"}}
{"id": "2510.03771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03771", "abs": "https://arxiv.org/abs/2510.03771", "authors": ["Divij Handa", "David Blincoe", "Orson Adams", "Yinlin Fu"], "title": "OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation", "comment": null, "summary": "Deploying capable and user-aligned LLM-based systems necessitates reliable\nevaluation. While LLMs excel in verifiable tasks like coding and mathematics,\nwhere gold-standard solutions are available, adoption remains challenging for\nsubjective tasks that lack a single correct answer. E-commerce Query Rewriting\n(QR) is one such problem where determining whether a rewritten query properly\ncaptures the user intent is extremely difficult to figure out algorithmically.\nIn this work, we introduce OptAgent, a novel framework that combines\nmulti-agent simulations with genetic algorithms to verify and optimize queries\nfor QR. Instead of relying on a static reward model or a single LLM judge, our\napproach uses multiple LLM-based agents, each acting as a simulated shopping\ncustomer, as a dynamic reward signal. The average of these agent-derived scores\nserves as an effective fitness function for an evolutionary algorithm that\niteratively refines the user's initial query. We evaluate OptAgent on a dataset\nof 1000 real-world e-commerce queries in five different categories, and we\nobserve an average improvement of 21.98% over the original user query and 3.36%\nover a Best-of-N LLM rewriting baseline.", "AI": {"tldr": "OptAgent\u6846\u67b6\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u9057\u4f20\u7b97\u6cd5\u6765\u4f18\u5316\u7535\u5546\u67e5\u8be2\u6539\u5199\uff0c\u901a\u8fc7\u6a21\u62df\u8d2d\u7269\u987e\u5ba2\u7684\u591a\u667a\u80fd\u4f53\u4f5c\u4e3a\u52a8\u6001\u5956\u52b1\u4fe1\u53f7\uff0c\u57281000\u4e2a\u771f\u5b9e\u7535\u5546\u67e5\u8be2\u4e0a\u6bd4\u539f\u59cb\u67e5\u8be2\u63d0\u534721.98%\uff0c\u6bd4\u6700\u4f73N\u6b21LLM\u6539\u5199\u63d0\u53473.36%\u3002", "motivation": "LLM\u5728\u53ef\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7f3a\u4e4f\u5355\u4e00\u6b63\u786e\u7b54\u6848\u7684\u4e3b\u89c2\u4efb\u52a1\uff08\u5982\u7535\u5546\u67e5\u8be2\u6539\u5199\uff09\u4e2d\u90e8\u7f72\u56f0\u96be\uff0c\u56e0\u4e3a\u96be\u4ee5\u7b97\u6cd5\u5316\u5224\u65ad\u6539\u5199\u67e5\u8be2\u662f\u5426\u51c6\u786e\u6355\u6349\u7528\u6237\u610f\u56fe\u3002", "method": "\u4f7f\u7528\u591aLLM\u667a\u80fd\u4f53\u6a21\u62df\u8d2d\u7269\u987e\u5ba2\u4f5c\u4e3a\u52a8\u6001\u5956\u52b1\u4fe1\u53f7\uff0c\u5c06\u667a\u80fd\u4f53\u8bc4\u5206\u5e73\u5747\u503c\u4f5c\u4e3a\u8fdb\u5316\u7b97\u6cd5\u7684\u9002\u5e94\u5ea6\u51fd\u6570\uff0c\u8fed\u4ee3\u4f18\u5316\u7528\u6237\u521d\u59cb\u67e5\u8be2\u3002", "result": "\u57285\u4e2a\u4e0d\u540c\u7c7b\u522b\u76841000\u4e2a\u771f\u5b9e\u7535\u5546\u67e5\u8be2\u4e0a\u6d4b\u8bd5\uff0cOptAgent\u6bd4\u539f\u59cb\u67e5\u8be2\u5e73\u5747\u63d0\u534721.98%\uff0c\u6bd4\u6700\u4f73N\u6b21LLM\u6539\u5199\u57fa\u7ebf\u63d0\u53473.36%\u3002", "conclusion": "OptAgent\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6a21\u62df\u548c\u9057\u4f20\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e3b\u89c2\u4efb\u52a1\u8bc4\u4f30\u96be\u9898\uff0c\u4e3a\u7535\u5546\u67e5\u8be2\u6539\u5199\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.04135", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04135", "abs": "https://arxiv.org/abs/2510.04135", "authors": ["Jingzhi Gong", "Yixin Bian", "Luis de la Cal", "Giovanni Pinna", "Anisha Uteem", "David Williams", "Mar Zamorano", "Karine Even-Mendoza", "W. B. Langdon", "Hector Menendez", "Federica Sarro"], "title": "GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization", "comment": "Accepted by SSBSE'25 Challenge Track", "summary": "Coding agents powered by LLMs face critical sustainability and scalability\nchallenges in industrial deployment, with single runs consuming over 100k\ntokens and incurring environmental costs that may exceed optimization benefits.\nThis paper introduces GA4GC, the first framework to systematically optimize\ncoding agent runtime (greener agent) and code performance (greener code)\ntrade-offs by discovering Pareto-optimal agent hyperparameters and prompt\ntemplates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x\nhypervolume improvement, reducing agent runtime by 37.7% while improving\ncorrectness. Our findings establish temperature as the most critical\nhyperparameter, and provide actionable strategies to balance agent\nsustainability with code optimization effectiveness in industrial deployment.", "AI": {"tldr": "GA4GC\u6846\u67b6\u901a\u8fc7\u53d1\u73b0\u5e15\u7d2f\u6258\u6700\u4f18\u4ee3\u7406\u8d85\u53c2\u6570\u548c\u63d0\u793a\u6a21\u677f\uff0c\u7cfb\u7edf\u4f18\u5316\u7f16\u7801\u4ee3\u7406\u8fd0\u884c\u65f6\u95f4\u4e0e\u4ee3\u7801\u6027\u80fd\u7684\u6743\u8861\uff0c\u5728SWE-Perf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u9ad8\u8fbe135\u500d\u8d85\u4f53\u79ef\u6539\u8fdb\uff0c\u51cf\u5c1137.7%\u4ee3\u7406\u8fd0\u884c\u65f6\u95f4\u540c\u65f6\u63d0\u9ad8\u6b63\u786e\u6027\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u7f16\u7801\u4ee3\u7406\u5728\u5de5\u4e1a\u90e8\u7f72\u4e2d\u9762\u4e34\u53ef\u6301\u7eed\u6027\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u5355\u6b21\u8fd0\u884c\u6d88\u8017\u8d85\u8fc710\u4e07token\uff0c\u73af\u5883\u6210\u672c\u53ef\u80fd\u8d85\u8fc7\u4f18\u5316\u6536\u76ca\u3002", "method": "\u5f15\u5165GA4GC\u6846\u67b6\uff0c\u7cfb\u7edf\u4f18\u5316\u7f16\u7801\u4ee3\u7406\u8fd0\u884c\u65f6\u95f4\uff08\u66f4\u73af\u4fdd\u7684\u4ee3\u7406\uff09\u548c\u4ee3\u7801\u6027\u80fd\uff08\u66f4\u73af\u4fdd\u7684\u4ee3\u7801\uff09\u7684\u6743\u8861\uff0c\u901a\u8fc7\u53d1\u73b0\u5e15\u7d2f\u6258\u6700\u4f18\u4ee3\u7406\u8d85\u53c2\u6570\u548c\u63d0\u793a\u6a21\u677f\u3002", "result": "\u5728SWE-Perf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u9ad8\u8fbe135\u500d\u8d85\u4f53\u79ef\u6539\u8fdb\uff0c\u51cf\u5c1137.7%\u4ee3\u7406\u8fd0\u884c\u65f6\u95f4\u540c\u65f6\u63d0\u9ad8\u6b63\u786e\u6027\u3002\u6e29\u5ea6\u88ab\u786e\u5b9a\u4e3a\u6700\u5173\u952e\u7684\u8d85\u53c2\u6570\u3002", "conclusion": "\u4e3a\u5de5\u4e1a\u90e8\u7f72\u4e2d\u5e73\u8861\u4ee3\u7406\u53ef\u6301\u7eed\u6027\u4e0e\u4ee3\u7801\u4f18\u5316\u6548\u679c\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u7b56\u7565\u3002", "topic": "code agent"}}
{"id": "2510.03847", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03847", "abs": "https://arxiv.org/abs/2510.03847", "authors": ["Raghav Sharma", "Manan Mehta"], "title": "Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs", "comment": "9 Pages", "summary": "Small language models (SLMs; 1-12B params, sometimes up to 20B) are\nsufficient and often superior for agentic workloads where the objective is\nschema- and API-constrained accuracy rather than open-ended generation. We\nsynthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,\nQwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,\nDeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,\nStableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with\nguided decoding libraries (XGrammar, Outlines). We formalize SLM-default,\nLLM-fallback systems with uncertainty-aware routing and verifier cascades, and\npropose engineering metrics that reflect real production goals: cost per\nsuccessful task (CPS), schema validity rate, executable call rate, p50/p95\nlatency, and energy per request. Guided decoding, strict JSON Schema outputs,\nand validator-first tool execution close much of the capability gap with larger\nmodels and often let SLMs match or surpass LLMs on tool use, function calling,\nand RAG at 10x-100x lower token cost with materially better latency and energy.\nWe provide design patterns for agent stacks that prioritize SLMs: schema-first\nprompting, type-safe function registries, confidence scoring with verifier\nrollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits\nwhere fallback remains valuable (open-domain reasoning and some long-horizon\nplanning). The result is a practical blueprint for building fast, inexpensive,\nand reliable agents that default to SLMs while preserving headroom with\ntargeted LLM assistance.\n  Keywords: small language models, agents, function calling, structured\noutputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,\nedge inference", "AI": {"tldr": "\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u6bd4\u5927\u8bed\u8a00\u6a21\u578b\u66f4\u9ad8\u6548\uff0c\u901a\u8fc7\u5f15\u5bfc\u89e3\u7801\u3001\u4e25\u683cJSON Schema\u8f93\u51fa\u548c\u9a8c\u8bc1\u5668\u4f18\u5148\u7684\u5de5\u5177\u6267\u884c\uff0c\u80fd\u4ee510-100\u500d\u66f4\u4f4e\u7684\u6210\u672c\u5b9e\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc1\u660e\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u4f18\u5316\u5de5\u7a0b\u6307\u6807\u5982\u6210\u672c\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u63d0\u4f9b\u5b9e\u7528\u7684\u4ee3\u7406\u7cfb\u7edf\u8bbe\u8ba1\u84dd\u56fe\u3002", "method": "\u7efc\u5408\u8bc4\u4f30\u591a\u79cd\u5f00\u6e90\u548c\u4e13\u6709\u5c0f\u6a21\u578b\uff0c\u7ed3\u5408\u5f15\u5bfc\u89e3\u7801\u5e93\u548c\u9a8c\u8bc1\u5668\u7ea7\u8054\uff0c\u63d0\u51faSLM\u9ed8\u8ba4\u3001LLM\u56de\u9000\u7684\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u5c0f\u6a21\u578b\u5728\u5de5\u5177\u4f7f\u7528\u3001\u51fd\u6570\u8c03\u7528\u548cRAG\u4efb\u52a1\u4e2d\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u5927\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4etoken\u6210\u672c\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "conclusion": "\u5c0f\u8bed\u8a00\u6a21\u578b\u662f\u6784\u5efa\u5feb\u901f\u3001\u5ec9\u4ef7\u3001\u53ef\u9760\u4ee3\u7406\u7cfb\u7edf\u7684\u4f18\u9009\uff0c\u540c\u65f6\u4fdd\u7559\u5927\u6a21\u578b\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u56de\u9000\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2510.03264", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03264", "abs": "https://arxiv.org/abs/2510.03264", "authors": ["Syeda Nahida Akter", "Shrimai Prabhumoye", "Eric Nyberg", "Mostofa Patwary", "Mohammad Shoeybi", "Yejin Choi", "Bryan Catanzaro"], "title": "Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data", "comment": null, "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs\nrevolves around post-training on high-quality, reasoning-intensive data. While\nemerging literature suggests that reasoning data is increasingly incorporated\nalso during the mid-training stage-a practice that is relatively more\nproprietary and less openly characterized-the role of such data in pretraining\nremains unclear. In particular, due to the opaqueness of pretraining corpora in\nmost frontier models, the effect of reasoning data introduced at different\nphases of pre- and/or post-training is relatively less reported in the\nscientific literature. This raises several important questions: Is adding\nreasoning data earlier during pretraining any better than introducing it during\npost-training? Could earlier inclusion risk overfitting and harm\ngeneralization, or instead establish durable foundations that later fine-tuning\ncannot recover? We conduct the first systematic study of how reasoning\ndata-varying in scale, diversity, and quality-affects LLM performance when\nintroduced at different stages of training. We find that front-loading\nreasoning data into pretraining is critical (19% avg gain), establishing\nfoundational capabilities that cannot be fully replicated by later-stage SFT,\neven with more data. We uncover an asymmetric principle for optimal data\nallocation: pretraining benefits most from broad diversity in reasoning\npatterns (11% avg gain), while SFT is more sensitive to data quality (15% avg\ngain). We show that high-quality pretraining data has latent effects, activated\nonly after SFT, and that naively scaling SFT data can be detrimental, washing\naway the benefits of early reasoning injection. Our results challenge the\nconventional separation of language modeling and reasoning, providing a\nprincipled guide for strategically allocating data across the entire training\npipeline to build more capable models.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u63a8\u7406\u6570\u636e\u5728\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u5fae\u8c03\u4e0d\u540c\u9636\u6bb5\u5f15\u5165\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u63a8\u7406\u6570\u636e\u6548\u679c\u66f4\u597d\uff0c\u5efa\u7acb\u4e86\u540e\u671f\u5fae\u8c03\u65e0\u6cd5\u5b8c\u5168\u590d\u73b0\u7684\u57fa\u7840\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u524d\u6cbf\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u8bed\u6599\u4e0d\u900f\u660e\uff0c\u63a8\u7406\u6570\u636e\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u7684\u6548\u679c\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\uff0c\u9700\u8981\u660e\u786e\u65e9\u671f\u5f15\u5165\u63a8\u7406\u6570\u636e\u662f\u5426\u4f18\u4e8e\u540e\u671f\u5fae\u8c03\uff0c\u4ee5\u53ca\u662f\u5426\u5b58\u5728\u8fc7\u62df\u5408\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u7814\u7a76\u63a8\u7406\u6570\u636e\u5728\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u8d28\u91cf\u65b9\u9762\u7684\u53d8\u5316\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\uff08\u9884\u8bad\u7ec3vs\u6307\u4ee4\u5fae\u8c03\uff09\u5f15\u5165\u7684\u6548\u679c\u5dee\u5f02\u3002", "result": "\u9884\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u63a8\u7406\u6570\u636e\u6548\u679c\u663e\u8457\uff08\u5e73\u5747\u63d0\u534719%\uff09\uff0c\u5efa\u7acb\u4e86\u65e0\u6cd5\u901a\u8fc7\u540e\u671fSFT\u5b8c\u5168\u590d\u73b0\u7684\u57fa\u7840\u80fd\u529b\u3002\u9884\u8bad\u7ec3\u53d7\u76ca\u4e8e\u63a8\u7406\u6a21\u5f0f\u591a\u6837\u6027\uff08\u5e73\u5747\u63d0\u534711%\uff09\uff0c\u800cSFT\u5bf9\u6570\u636e\u8d28\u91cf\u66f4\u654f\u611f\uff08\u5e73\u5747\u63d0\u534715%\uff09\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u8bed\u8a00\u5efa\u6a21\u4e0e\u63a8\u7406\u7684\u4f20\u7edf\u5206\u79bb\uff0c\u4e3a\u5728\u6574\u4e2a\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u6218\u7565\u6027\u5730\u5206\u914d\u6570\u636e\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6307\u5bfc\uff0c\u4ee5\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u3002", "topic": "agent analysis"}}
{"id": "2510.04349", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04349", "abs": "https://arxiv.org/abs/2510.04349", "authors": ["Dmitry Ustalov", "Egor Bogomolov", "Alexander Bezzubov", "Yaroslav Golubev", "Evgeniy Glukhov", "Georgii Levtsov", "Vladimir Kovalenko"], "title": "Challenge on Optimization of Context Collection for Code Completion", "comment": "7 pages, 3 figures, 5 tables. A report on the Context Collection\n  Workshop co-located with ASE'25", "summary": "The rapid advancement of workflows and methods for software engineering using\nAI emphasizes the need for a systematic evaluation and analysis of their\nability to leverage information from entire projects, particularly in large\ncode bases. In this challenge on optimization of context collection for code\ncompletion, organized by JetBrains in collaboration with Mistral AI as part of\nthe ASE 2025 conference, participants developed efficient mechanisms for\ncollecting context from source code repositories to improve fill-in-the-middle\ncode completions for Python and Kotlin. We constructed a large dataset of\nreal-world code in these two programming languages using permissively licensed\nopen-source projects. The submissions were evaluated based on their ability to\nmaximize completion quality for multiple state-of-the-art neural models using\nthe chrF metric. During the public phase of the competition, nineteen teams\nsubmitted solutions to the Python track and eight teams submitted solutions to\nthe Kotlin track. In the private phase, six teams competed, of which five\nsubmitted papers to the workshop.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86JetBrains\u4e0eMistral AI\u5728ASE 2025\u4f1a\u8bae\u4e0a\u7ec4\u7ec7\u7684\u4ee3\u7801\u5b8c\u6210\u4e0a\u4e0b\u6587\u6536\u96c6\u4f18\u5316\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30AI\u8f6f\u4ef6\u5de5\u7a0b\u65b9\u6cd5\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u5229\u7528\u9879\u76ee\u4fe1\u606f\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740AI\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u4f5c\u6d41\u548c\u65b9\u6cd5\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5b83\u4eec\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u5229\u7528\u6574\u4e2a\u9879\u76ee\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7801\u5b8c\u6210\u4efb\u52a1\u4e2d\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8ePython\u548cKotlin\u7684\u8bb8\u53ef\u5f00\u6e90\u9879\u76ee\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u7ec4\u7ec7\u7ade\u8d5b\u8ba9\u53c2\u4e0e\u8005\u5f00\u53d1\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u6536\u96c6\u673a\u5236\u6765\u6539\u8fdb\u586b\u5145\u4e2d\u95f4\u4ee3\u7801\u5b8c\u6210\uff0c\u4f7f\u7528chrF\u6307\u6807\u8bc4\u4f30\u63d0\u4ea4\u65b9\u6848\u7684\u8d28\u91cf\u3002", "result": "\u5728\u516c\u5f00\u9636\u6bb5\uff0c19\u4e2a\u56e2\u961f\u63d0\u4ea4\u4e86Python\u8d5b\u9053\u89e3\u51b3\u65b9\u6848\uff0c8\u4e2a\u56e2\u961f\u63d0\u4ea4\u4e86Kotlin\u8d5b\u9053\u89e3\u51b3\u65b9\u6848\uff1b\u5728\u79c1\u6709\u9636\u6bb5\uff0c6\u4e2a\u56e2\u961f\u7ade\u4e89\uff0c\u5176\u4e2d5\u4e2a\u56e2\u961f\u5411\u7814\u8ba8\u4f1a\u63d0\u4ea4\u4e86\u8bba\u6587\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u6210\u529f\u4fc3\u8fdb\u4e86\u4ee3\u7801\u5b8c\u6210\u4e0a\u4e0b\u6587\u6536\u96c6\u4f18\u5316\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u4e3a\u8bc4\u4f30AI\u8f6f\u4ef6\u5de5\u7a0b\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\u3002", "topic": "swe benchmark"}}
{"id": "2510.04363", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04363", "abs": "https://arxiv.org/abs/2510.04363", "authors": ["Hyunjun Kim", "Sejong Kim"], "title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models", "comment": "NeurIPS 2025 Workshop on Lock-LLM", "summary": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs\ncan synthesize reusable browser automation programs from natural language goals\nby reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates\nseven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,\nFacebook-like, Discord-like, and Threads-like, covering 681 tasks across\ninteraction complexity and targeting difficulty. Our end-to-end protocol\nvalidates generated code via static checks, sandboxed execution, and outcome\nverification including DOM assertions and database snapshots, and includes a\nsafety suite for scraping, spam/abuse, and credential/privacy prompts. Across\n2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8\npercent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,\nand DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at\n91.7 percent but fail on complex workflows at 0.0 percent, and none meet\nproduction-quality coding practices despite functional completion. We release\nour complete benchmark pipeline, evaluation framework, and experimental results\nto enable reproducible assessment of macro synthesis for web automation.", "AI": {"tldr": "MacroBench\u662f\u4e00\u4e2a\u4ee3\u7801\u4f18\u5148\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u80fd\u5426\u4ece\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u5408\u6210\u53ef\u91cd\u7528\u7684\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u7a0b\u5e8f\uff0c\u6db5\u76d67\u4e2a\u81ea\u6258\u7ba1\u7f51\u7ad9\u548c681\u4e2a\u4efb\u52a1\uff0c\u901a\u8fc7\u9759\u6001\u68c0\u67e5\u3001\u6c99\u7bb1\u6267\u884c\u548c\u7ed3\u679c\u9a8c\u8bc1\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u8865\u5168\u6216\u5355\u884c\u4ee3\u7801\u751f\u6210\uff0c\u7f3a\u4e4f\u5bf9\u4ece\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u5408\u6210\u5b8c\u6574\u3001\u53ef\u91cd\u7528\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u7a0b\u5e8f\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u5305\u542b7\u4e2a\u81ea\u6258\u7ba1\u7f51\u7ad9\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6681\u4e2a\u4efb\u52a1\uff0c\u901a\u8fc7\u9759\u6001\u68c0\u67e5\u3001\u6c99\u7bb1\u6267\u884c\u3001DOM\u65ad\u8a00\u548c\u6570\u636e\u5e93\u5feb\u7167\u8fdb\u884c\u7aef\u5230\u7aef\u9a8c\u8bc1\uff0c\u5e76\u5305\u542b\u5b89\u5168\u5957\u4ef6\u3002", "result": "GPT-4o-Mini\u8fbe\u523096.8%\u6210\u529f\u7387\uff0cGPT-4.1\u8fbe\u523095.3%\uff0cGemini-2.5-Pro\u8fbe\u523089.0%\uff0cDeepSeek-V3.1\u8fbe\u523083.4%\u3002\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u53ef\u9760(91.7%)\uff0c\u4f46\u5728\u590d\u6742\u5de5\u4f5c\u6d41\u4e0a\u5b8c\u5168\u5931\u8d25(0.0%)\u3002", "conclusion": "\u867d\u7136\u6a21\u578b\u5728\u529f\u80fd\u5b8c\u6210\u5ea6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6ca1\u6709\u4e00\u4e2a\u6a21\u578b\u8fbe\u5230\u751f\u4ea7\u8d28\u91cf\u7684\u7f16\u7801\u5b9e\u8df5\u6807\u51c6\uff0c\u590d\u6742\u5de5\u4f5c\u6d41\u4ecd\u7136\u662f\u4e3b\u8981\u6311\u6218\u3002", "topic": "swe benchmark"}}
{"id": "2510.04468", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04468", "abs": "https://arxiv.org/abs/2510.04468", "authors": ["Asif Mohammed Samir", "Mohammad Masudur Rahman"], "title": "Improving IR-based Bug Localization with Semantics-Driven Query Reduction", "comment": "56 pages, 16 figures, 11 tables", "summary": "Despite decades of research, software bug localization remains challenging\ndue to heterogeneous content and inherent ambiguities in bug reports. Existing\nmethods such as Information Retrieval (IR)-based approaches often attempt to\nmatch source documents to bug reports, overlooking the context and semantics of\nthe source code. On the other hand, Large Language Models (LLM) (e.g.,\nTransformer models) show promising results in understanding both texts and\ncode. However, they have not been yet adapted well to localize software bugs\nagainst bug reports. They could be also data or resource-intensive. To bridge\nthis gap, we propose, IQLoc, a novel bug localization approach that capitalizes\non the strengths of both IR and LLM-based approaches. In particular, we\nleverage the program semantics understanding of transformer-based models to\nreason about the suspiciousness of code and reformulate queries during bug\nlocalization using Information Retrieval. To evaluate IQLoc, we refine the\nBench4BL benchmark dataset and extend it by incorporating ~30% more recent bug\nreports, resulting in a benchmark containing ~7.5K bug reports. We evaluated\nIQLoc using three performance metrics and compare it against four baseline\ntechniques. Experimental results demonstrate its superiority, achieving up to\n58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in\nHIT@K for the test bug reports with random and time-wise splits, respectively.\nMoreover, IQLoc improves MAP by 91.67% for bug reports with stack traces,\n72.73% for those that include code elements, and 65.38% for those containing\nonly descriptions in natural language. By integrating program semantic\nunderstanding into Information Retrieval, IQLoc mitigates several longstanding\nchallenges of traditional IR-based approaches in bug localization.", "AI": {"tldr": "IQLoc\u662f\u4e00\u79cd\u7ed3\u5408\u4fe1\u606f\u68c0\u7d22\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8f6f\u4ef6\u7f3a\u9677\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528Transformer\u6a21\u578b\u7406\u89e3\u7a0b\u5e8f\u8bed\u4e49\u6765\u6539\u8fdb\u7f3a\u9677\u5b9a\u4f4d\u6548\u679c", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u7684\u7f3a\u9677\u5b9a\u4f4d\u65b9\u6cd5\u5ffd\u7565\u4e86\u6e90\u4ee3\u7801\u7684\u4e0a\u4e0b\u6587\u548c\u8bed\u4e49\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u7406\u89e3\u6587\u672c\u548c\u4ee3\u7801\uff0c\u4f46\u5c1a\u672a\u5f88\u597d\u5730\u5e94\u7528\u4e8e\u7f3a\u9677\u5b9a\u4f4d\u4e14\u8d44\u6e90\u6d88\u8017\u5927", "method": "\u7ed3\u5408IR\u548cLLM\u7684\u4f18\u52bf\uff0c\u5229\u7528\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u7406\u89e3\u7a0b\u5e8f\u8bed\u4e49\uff0c\u5728\u7f3a\u9677\u5b9a\u4f4d\u8fc7\u7a0b\u4e2d\u5bf9\u4ee3\u7801\u53ef\u7591\u6027\u8fdb\u884c\u63a8\u7406\u5e76\u91cd\u65b0\u5236\u5b9a\u67e5\u8be2", "result": "\u5728Bench4BL\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cIQLoc\u5728MAP\u3001MRR\u548cHIT@K\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u56db\u79cd\u57fa\u7ebf\u6280\u672f\uff0c\u7279\u522b\u662f\u5bf9\u5305\u542b\u5806\u6808\u8ddf\u8e2a\u3001\u4ee3\u7801\u5143\u7d20\u6216\u7eaf\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u7f3a\u9677\u62a5\u544a\u6709\u663e\u8457\u6539\u8fdb", "conclusion": "\u901a\u8fc7\u5c06\u7a0b\u5e8f\u8bed\u4e49\u7406\u89e3\u96c6\u6210\u5230\u4fe1\u606f\u68c0\u7d22\u4e2d\uff0cIQLoc\u7f13\u89e3\u4e86\u4f20\u7edf\u57fa\u4e8eIR\u7684\u7f3a\u9677\u5b9a\u4f4d\u65b9\u6cd5\u7684\u957f\u671f\u6311\u6218", "topic": "swe application"}}
{"id": "2510.03271", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03271", "abs": "https://arxiv.org/abs/2510.03271", "authors": ["Zi Liang", "Zhiyao Wu", "Haoyang Shang", "Yulin Jin", "Qingqing Ye", "Huadi Zheng", "Peizhao Hu", "Haibo Hu"], "title": "Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary", "comment": "Source code: https://github.com/liangzid/DPS", "summary": "Decision boundary, the subspace of inputs where a machine learning model\nassigns equal classification probabilities to two classes, is pivotal in\nrevealing core model properties and interpreting behaviors. While analyzing the\ndecision boundary of large language models (LLMs) has raised increasing\nattention recently, constructing it for mainstream LLMs remains computationally\ninfeasible due to the enormous vocabulary-sequence sizes and the\nauto-regressive nature of LLMs. To address this issue, in this paper we propose\nDecision Potential Surface (DPS), a new notion for analyzing LLM decision\nboundary. DPS is defined on the confidences in distinguishing different\nsampling sequences for each input, which naturally captures the potential of\ndecision boundary. We prove that the zero-height isohypse in DPS is equivalent\nto the decision boundary of an LLM, with enclosed regions representing decision\nregions. By leveraging DPS, for the first time in the literature, we propose an\napproximate decision boundary construction algorithm, namely $K$-DPS, which\nonly requires K-finite times of sequence sampling to approximate an LLM's\ndecision boundary with negligible error. We theoretically derive the upper\nbounds for the absolute error, expected error, and the error concentration\nbetween K-DPS and the ideal DPS, demonstrating that such errors can be\ntrade-off with sampling times. Our results are empirically validated by\nextensive experiments across various LLMs and corpora.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u51b3\u7b56\u52bf\u9762\uff08DPS\uff09\u4f5c\u4e3a\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u8fb9\u754c\u7684\u65b0\u6982\u5ff5\uff0c\u5e76\u5f00\u53d1\u4e86K-DPS\u7b97\u6cd5\u6765\u8fd1\u4f3c\u6784\u5efaLLM\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u4ec5\u9700\u6709\u9650\u6b21\u5e8f\u5217\u91c7\u6837\u5373\u53ef\u5b9e\u73b0\u53ef\u5ffd\u7565\u8bef\u5dee\u7684\u8fd1\u4f3c\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u8bcd\u6c47\u5e8f\u5217\u89c4\u6a21\u5de8\u5927\u4e14\u5177\u6709\u81ea\u56de\u5f52\u7279\u6027\uff0c\u76f4\u63a5\u6784\u5efa\u5176\u51b3\u7b56\u8fb9\u754c\u5728\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u5b9a\u4e49\u51b3\u7b56\u52bf\u9762\uff08DPS\uff09\u6982\u5ff5\uff0c\u57fa\u4e8e\u4e0d\u540c\u91c7\u6837\u5e8f\u5217\u7684\u533a\u5206\u7f6e\u4fe1\u5ea6\u6765\u6355\u6349\u51b3\u7b56\u8fb9\u754c\u6f5c\u529b\uff0c\u5e76\u63d0\u51faK-DPS\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u901a\u8fc7\u6709\u9650\u6b21\u91c7\u6837\u6784\u5efa\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u7406\u8bba\u63a8\u5bfc\u4e86K-DPS\u4e0e\u7406\u60f3DPS\u4e4b\u95f4\u7684\u7edd\u5bf9\u8bef\u5dee\u3001\u671f\u671b\u8bef\u5dee\u548c\u8bef\u5dee\u96c6\u4e2d\u5ea6\u7684\u4e0a\u754c\uff0c\u8bc1\u660e\u8bef\u5dee\u53ef\u901a\u8fc7\u91c7\u6837\u6b21\u6570\u8fdb\u884c\u6743\u8861\uff0c\u5e76\u901a\u8fc7\u591aLLM\u548c\u8bed\u6599\u5e93\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7ed3\u679c\u3002", "conclusion": "DPS\u4e3a\u5206\u6790LLM\u51b3\u7b56\u8fb9\u754c\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\uff0cK-DPS\u7b97\u6cd5\u80fd\u591f\u4ee5\u53ef\u63a5\u53d7\u7684\u8bef\u5dee\u9ad8\u6548\u8fd1\u4f3c\u51b3\u7b56\u8fb9\u754c\u3002", "topic": "agent analysis"}}
{"id": "2510.04017", "categories": ["cs.AI", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.04017", "abs": "https://arxiv.org/abs/2510.04017", "authors": ["Sumanth Varambally", "Marshall Fisher", "Jas Thakker", "Yiwei Chen", "Zhirui Xia", "Yasaman Jafari", "Ruijia Niu", "Manas Jain", "Veeramakali Vignesh Manivannan", "Zachary Novack", "Luyu Han", "Srikar Eranky", "Salva R\u00fchling Cachay", "Taylor Berg-Kirkpatrick", "Duncan Watson-Parris", "Yi-An Ma", "Rose Yu"], "title": "Zephyrus: An Agentic Framework for Weather Science", "comment": null, "summary": "Foundation models for weather science are pre-trained on vast amounts of\nstructured numerical data and outperform traditional weather forecasting\nsystems. However, these models lack language-based reasoning capabilities,\nlimiting their utility in interactive scientific workflows. Large language\nmodels (LLMs) excel at understanding and generating text but cannot reason\nabout high-dimensional meteorological datasets. We bridge this gap by building\na novel agentic framework for weather science. Our framework includes a Python\ncode-based environment for agents (ZephyrusWorld) to interact with weather\ndata, featuring tools like an interface to WeatherBench 2 dataset, geoquerying\nfor geographical masks from natural language, weather forecasting, and climate\nsimulation capabilities. We design Zephyrus, a multi-turn LLM-based weather\nagent that iteratively analyzes weather datasets, observes results, and refines\nits approach through conversational feedback loops. We accompany the agent with\na new benchmark, ZephyrusBench, with a scalable data generation pipeline that\nconstructs diverse question-answer pairs across weather-related tasks, from\nbasic lookups to advanced forecasting, extreme event detection, and\ncounterfactual reasoning. Experiments on this benchmark demonstrate the strong\nperformance of Zephyrus agents over text-only baselines, outperforming them by\nup to 35 percentage points in correctness. However, on harder tasks, Zephyrus\nperforms similarly to text-only baselines, highlighting the challenging nature\nof our benchmark and suggesting promising directions for future work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aZephyrus\u7684\u5929\u6c14\u79d1\u5b66\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u57fa\u7840\u5929\u6c14\u6a21\u578b\u7684\u6570\u636e\u5904\u7406\u80fd\u529b\u548cLLM\u7684\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u5206\u6790\u5929\u6c14\u6570\u636e\u5e76\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5929\u6c14\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u800cLLM\u65e0\u6cd5\u5904\u7406\u9ad8\u7ef4\u6c14\u8c61\u6570\u636e\uff0c\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u4ea4\u4e92\u5f0f\u79d1\u5b66\u5de5\u4f5c\u6d41\u6846\u67b6\u3002", "method": "\u6784\u5efa\u4e86ZephyrusWorld\u4ee3\u7801\u73af\u5883\uff0c\u5305\u542bWeatherBench 2\u6570\u636e\u96c6\u63a5\u53e3\u3001\u5730\u7406\u67e5\u8be2\u3001\u5929\u6c14\u9884\u62a5\u548c\u6c14\u5019\u6a21\u62df\u7b49\u5de5\u5177\uff0c\u8bbe\u8ba1\u4e86\u591a\u8f6eLLM\u5929\u6c14\u667a\u80fd\u4f53Zephyrus\uff0c\u901a\u8fc7\u5bf9\u8bdd\u53cd\u9988\u5faa\u73af\u8fed\u4ee3\u5206\u6790\u6570\u636e\u3002", "result": "\u5728ZephyrusBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cZephyrus\u667a\u80fd\u4f53\u5728\u6b63\u786e\u6027\u4e0a\u6bd4\u7eaf\u6587\u672c\u57fa\u7ebf\u9ad8\u51fa35\u4e2a\u767e\u5206\u70b9\uff0c\u4f46\u5728\u66f4\u56f0\u96be\u4efb\u52a1\u4e0a\u8868\u73b0\u76f8\u4f3c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u5929\u6c14\u6a21\u578b\u548cLLM\u7684\u4f18\u52bf\uff0c\u57fa\u51c6\u6d4b\u8bd5\u5177\u6709\u6311\u6218\u6027\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.04023", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04023", "abs": "https://arxiv.org/abs/2510.04023", "authors": ["Mizanur Rahman", "Amran Bhuiyan", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Ridwan Mahbub", "Ahmed Masry", "Shafiq Joty", "Enamul Hoque"], "title": "LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions", "comment": "Survey paper; 45 data science agents; under review", "summary": "Recent advances in large language models (LLMs) have enabled a new class of\nAI agents that automate multiple stages of the data science workflow by\nintegrating planning, tool use, and multimodal reasoning across text, code,\ntables, and visuals. This survey presents the first comprehensive,\nlifecycle-aligned taxonomy of data science agents, systematically analyzing and\nmapping forty-five systems onto the six stages of the end-to-end data science\nprocess: business understanding and data acquisition, exploratory analysis and\nvisualization, feature engineering, model building and selection,\ninterpretation and explanation, and deployment and monitoring. In addition to\nlifecycle coverage, we annotate each agent along five cross-cutting design\ndimensions: reasoning and planning style, modality integration, tool\norchestration depth, learning and alignment methods, and trust, safety, and\ngovernance mechanisms. Beyond classification, we provide a critical synthesis\nof agent capabilities, highlight strengths and limitations at each stage, and\nreview emerging benchmarks and evaluation practices. Our analysis identifies\nthree key trends: most systems emphasize exploratory analysis, visualization,\nand modeling while neglecting business understanding, deployment, and\nmonitoring; multimodal reasoning and tool orchestration remain unresolved\nchallenges; and over 90% lack explicit trust and safety mechanisms. We conclude\nby outlining open challenges in alignment stability, explainability,\ngovernance, and robust evaluation frameworks, and propose future research\ndirections to guide the development of robust, trustworthy, low-latency,\ntransparent, and broadly accessible data science agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411\u6570\u636e\u79d1\u5b66\u5168\u751f\u547d\u5468\u671f\u7684\u667a\u80fd\u4ee3\u7406\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u4e8645\u4e2a\u7cfb\u7edf\u5728\u6570\u636e\u79d1\u5b66\u516d\u4e2a\u9636\u6bb5\u7684\u80fd\u529b\u5206\u5e03\uff0c\u5e76\u8bc6\u522b\u51fa\u5f53\u524d\u7814\u7a76\u5728\u4e1a\u52a1\u7406\u89e3\u3001\u90e8\u7f72\u76d1\u63a7\u7b49\u9636\u6bb5\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u63a8\u7406\u3001\u5de5\u5177\u7f16\u6392\u7b49\u672a\u89e3\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u6570\u636e\u79d1\u5b66\u667a\u80fd\u4ee3\u7406\u80fd\u591f\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u591a\u4e2a\u9636\u6bb5\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u7cfb\u7edf\u7684\u7cfb\u7edf\u6027\u5206\u7c7b\u548c\u5206\u6790\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u6765\u7406\u89e3\u5176\u80fd\u529b\u5206\u5e03\u548c\u7814\u7a76\u8d8b\u52bf\u3002", "method": "\u6784\u5efa\u4e86\u9762\u5411\u6570\u636e\u79d1\u5b66\u751f\u547d\u5468\u671f\u7684\u5206\u7c7b\u6cd5\uff0c\u5c0645\u4e2a\u7cfb\u7edf\u6620\u5c04\u5230\u516d\u4e2a\u6838\u5fc3\u9636\u6bb5\uff0c\u5e76\u4ece\u4e94\u4e2a\u4ea4\u53c9\u8bbe\u8ba1\u7ef4\u5ea6\u8fdb\u884c\u6807\u6ce8\u5206\u6790\uff0c\u5305\u62ec\u63a8\u7406\u89c4\u5212\u98ce\u683c\u3001\u6a21\u6001\u96c6\u6210\u3001\u5de5\u5177\u7f16\u6392\u6df1\u5ea6\u7b49\u3002", "result": "\u5206\u6790\u53d1\u73b0\u5927\u591a\u6570\u7cfb\u7edf\u96c6\u4e2d\u5728\u63a2\u7d22\u6027\u5206\u6790\u548c\u5efa\u6a21\u9636\u6bb5\uff0c\u800c\u4e1a\u52a1\u7406\u89e3\u3001\u90e8\u7f72\u76d1\u63a7\u88ab\u5ffd\u89c6\uff1b90%\u4ee5\u4e0a\u7cfb\u7edf\u7f3a\u4e4f\u660e\u786e\u7684\u4fe1\u4efb\u5b89\u5168\u673a\u5236\uff1b\u591a\u6a21\u6001\u63a8\u7406\u548c\u5de5\u5177\u7f16\u6392\u4ecd\u662f\u4e3b\u8981\u6311\u6218\u3002", "conclusion": "\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7814\u7a76\u5b58\u5728\u660e\u663e\u4e0d\u5e73\u8861\uff0c\u9700\u8981\u5173\u6ce8\u5bf9\u9f50\u7a33\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6cbb\u7406\u6846\u67b6\u548c\u9c81\u68d2\u8bc4\u4f30\u7b49\u5f00\u653e\u6311\u6218\uff0c\u4ee5\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u900f\u660e\u3001\u4f4e\u5ef6\u8fdf\u7684\u667a\u80fd\u4ee3\u7406\u3002", "topic": "agent analysis"}}
{"id": "2510.03805", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03805", "abs": "https://arxiv.org/abs/2510.03805", "authors": ["Canhui Wu", "Qiong Cao", "Chang Li", "Zhenfang Wang", "Chao Xue", "Yuwei Fan", "Wei Xi", "Xiaodong He"], "title": "Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models", "comment": "20pages, 7 figures", "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks\nbut often suffer from excessive verbosity, known as \"overthinking.\" Existing\nsolutions via reinforcement learning (RL) typically penalize generated tokens\nto promote conciseness. However, these methods encounter two challenges:\nresponses with fewer tokens do not always correspond to fewer reasoning steps,\nand models may develop hacking behavior in later stages of training by\ndiscarding reasoning steps to minimize token usage. In this work, we introduce\n\\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more\nefficient reasoning by favoring compact reasoning steps. Our step-aware reward\nfunction prioritizes correctness while imposing penalties for redundant steps,\nand withholds rewards for incorrect responses to prevent the reinforcement of\nerroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when\nthe length of any output step exceeds the upper limit, we halt updates to\nprevent hacking behavior caused by merging steps. Extensive experiments across\nfour reasoning benchmarks demonstrate that SP achieves state-of-the-art\naccuracy while significantly reducing response length. For instance, on AIME24,\nSP reduces token usage by \\textbf{69.7\\%}.", "AI": {"tldr": "\u63d0\u51faStep Pruner\uff08SP\uff09\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u60e9\u7f5a\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\u6765\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u54cd\u5e94\u957f\u5ea6\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u4f46\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8etoken\u60e9\u7f5a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u6311\u6218\uff1a\u66f4\u5c11\u7684token\u4e0d\u4e00\u5b9a\u5bf9\u5e94\u66f4\u5c11\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u4e22\u5f03\u63a8\u7406\u6b65\u9aa4\u6765\u6700\u5c0f\u5316token\u4f7f\u7528\u3002", "method": "\u63d0\u51faStep Pruner\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u6b65\u9aa4\u611f\u77e5\u7684\u5956\u52b1\u51fd\u6570\uff08\u4f18\u5148\u6b63\u786e\u6027\u540c\u65f6\u60e9\u7f5a\u5197\u4f59\u6b65\u9aa4\uff09\u548c\u52a8\u6001\u505c\u6b62\u673a\u5236\uff08\u5f53\u4efb\u4f55\u8f93\u51fa\u6b65\u9aa4\u8d85\u8fc7\u957f\u5ea6\u4e0a\u9650\u65f6\u505c\u6b62\u66f4\u65b0\u4ee5\u9632\u6b62\u6b65\u9aa4\u5408\u5e76\uff09\u3002", "result": "\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSP\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\u3002\u5728AIME24\u4e0a\u51cf\u5c11\u4e8669.7%\u7684token\u4f7f\u7528\u3002", "conclusion": "SP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u901a\u8fc7\u6b65\u9aa4\u7ea7\u522b\u7684\u4f18\u5316\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04605", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04605", "abs": "https://arxiv.org/abs/2510.04605", "authors": ["Jingyao Zhang", "Tianlin Li", "Xiaoyu Zhang", "Qiang Hu", "Bin Shi"], "title": "Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation", "comment": null, "summary": "Autoregressive Large Language Models (AR-LLMs) are widely used in software\nengineering (SE) but face limitations in processing code structure information\nand suffer from high inference latency. Diffusion LLMs (DLLMs) offer a\npromising alternative with global bidirectional encoding and decoupled\ngeneration steps. This work presents the first comprehensive evaluation of\nDLLMs across the software development lifecycle, including code generation,\ndefect detection, and program repair. On a large-scale benchmark of 52,937\ntasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy\nimprovement achieving a 113% gain on cross-file repair, while maintaining\nsuperior efficiency and reduced latency. Our results establish DLLMs as a\nsuperior paradigm for SE tasks.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5f00\u53d1\u5168\u5468\u671f\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u4ee3\u7801\u751f\u6210\u3001\u7f3a\u9677\u68c0\u6d4b\u548c\u7a0b\u5e8f\u4fee\u590d\u3002\u572852,937\u4e2a\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7B\u53c2\u6570\u7684DLLMs\u5728\u51c6\u786e\u7387\u4e0a\u6bd4AR-LLMs\u5e73\u5747\u63d0\u534730%\uff0c\u8de8\u6587\u4ef6\u4fee\u590d\u4efb\u52a1\u63d0\u5347113%\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u9ad8\u7684\u6548\u7387\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002", "motivation": "\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5728\u5904\u7406\u4ee3\u7801\u7ed3\u6784\u4fe1\u606f\u548c\u63a8\u7406\u5ef6\u8fdf\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u5168\u5c40\u53cc\u5411\u7f16\u7801\u548c\u89e3\u8026\u751f\u6210\u6b65\u9aa4\u7684\u4f18\u52bf\u3002", "method": "\u5728\u8f6f\u4ef6\u5f00\u53d1\u5168\u5468\u671f\uff08\u4ee3\u7801\u751f\u6210\u3001\u7f3a\u9677\u68c0\u6d4b\u3001\u7a0b\u5e8f\u4fee\u590d\uff09\u4e2d\u5bf9\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u4f7f\u7528\u5305\u542b52,937\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f837B\u53c2\u6570DLLMs\u4e0eAR-LLMs\u7684\u6027\u80fd\u3002", "result": "7B\u53c2\u6570\u7684DLLMs\u5728\u51c6\u786e\u7387\u4e0a\u6bd4AR-LLMs\u5e73\u5747\u63d0\u534730%\uff0c\u5728\u8de8\u6587\u4ef6\u4fee\u590d\u4efb\u52a1\u4e2d\u63d0\u5347113%\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u9ad8\u7684\u6548\u7387\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002", "conclusion": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u662f\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u4f18\u8d8a\u8303\u5f0f\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u3002", "topic": "swe application"}}
{"id": "2510.04051", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04051", "abs": "https://arxiv.org/abs/2510.04051", "authors": ["Lele Liao", "Qile Zhang", "Ruofan Wu", "Guanhua Fang"], "title": "Toward a unified framework for data-efficient evaluation of large language models", "comment": "codes available at https://github.com/Rorschach1989/efficient-lm-eval", "summary": "Evaluating large language models (LLMs) on comprehensive benchmarks is a\ncornerstone of their development, yet it's often computationally and\nfinancially prohibitive. While Item Response Theory (IRT) offers a promising\npath toward data-efficient evaluation by disentangling model capability from\nitem difficulty, existing IRT-based methods are hampered by significant\nlimitations. They are typically restricted to binary correctness metrics,\nfailing to natively handle the continuous scores used in generative tasks, and\nthey operate on single benchmarks, ignoring valuable structural knowledge like\ncorrelations across different metrics or benchmarks. To overcome these\nchallenges, we introduce LEGO-IRT, a unified and flexible framework for\ndata-efficient LLM evaluation. LEGO-IRT's novel design natively supports both\nbinary and continuous evaluation metrics. Moreover, it introduces a factorized\narchitecture to explicitly model and leverage structural knowledge, decomposing\nmodel ability estimates into a general component and structure-specific (e.g.,\nper-metric or per-benchmark) components. Through extensive experiments\ninvolving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves\nstable capability estimates using just $3\\%$ of the total evaluation items. We\ndemonstrate that incorporating structural knowledge reduces estimation error by\nup to $10\\%$ and reveal that the latent abilities estimated by our framework\nmay align more closely with human preferences.", "AI": {"tldr": "LEGO-IRT\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u6548\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u652f\u6301\u4e8c\u5143\u548c\u8fde\u7eed\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u56e0\u5b50\u5316\u67b6\u6784\u5efa\u6a21\u7ed3\u6784\u77e5\u8bc6\uff0c\u4ec5\u97003%\u7684\u8bc4\u4f30\u9879\u76ee\u5373\u53ef\u83b7\u5f97\u7a33\u5b9a\u7684\u80fd\u529b\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eIRT\u7684\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff1a\u4ec5\u9650\u4e8e\u4e8c\u5143\u6b63\u786e\u6027\u6307\u6807\uff0c\u65e0\u6cd5\u5904\u7406\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8fde\u7eed\u5206\u6570\uff0c\u4e14\u4ec5\u9488\u5bf9\u5355\u4e00\u57fa\u51c6\uff0c\u5ffd\u7565\u4e86\u8de8\u4e0d\u540c\u6307\u6807\u6216\u57fa\u51c6\u7684\u76f8\u5173\u6027\u7b49\u6709\u4ef7\u503c\u7684\u7ed3\u6784\u77e5\u8bc6\u3002", "method": "\u5f15\u5165LEGO-IRT\u6846\u67b6\uff0c\u5176\u65b0\u9896\u8bbe\u8ba1\u539f\u751f\u652f\u6301\u4e8c\u5143\u548c\u8fde\u7eed\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u91c7\u7528\u56e0\u5b50\u5316\u67b6\u6784\uff0c\u5c06\u6a21\u578b\u80fd\u529b\u4f30\u8ba1\u5206\u89e3\u4e3a\u901a\u7528\u7ec4\u4ef6\u548c\u7ed3\u6784\u7279\u5b9a\u7ec4\u4ef6\uff08\u5982\u6bcf\u4e2a\u6307\u6807\u6216\u6bcf\u4e2a\u57fa\u51c6\uff09\u3002", "result": "\u5728\u6d89\u53ca70\u4e2aLLM\u548c5\u4e2a\u57fa\u51c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cLEGO-IRT\u4ec5\u4f7f\u7528\u603b\u8bc4\u4f30\u9879\u76ee\u76843%\u5373\u53ef\u5b9e\u73b0\u7a33\u5b9a\u7684\u80fd\u529b\u4f30\u8ba1\u3002\u6574\u5408\u7ed3\u6784\u77e5\u8bc6\u53ef\u5c06\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u9ad8\u8fbe10%\uff0c\u4e14\u6846\u67b6\u4f30\u8ba1\u7684\u6f5c\u5728\u80fd\u529b\u53ef\u80fd\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "LEGO-IRT\u4e3a\u6570\u636e\u9ad8\u6548\u7684LLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5229\u7528\u7ed3\u6784\u77e5\u8bc6\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2510.04064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04064", "abs": "https://arxiv.org/abs/2510.04064", "authors": ["Jingxiang Zhang", "Lujia Zhong"], "title": "Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion", "comment": "10 pages, 7 figures, 4 tables. Under review", "summary": "Large Language Models (LLMs) are increasingly expected to navigate the\nnuances of human emotion. While research confirms that LLMs can simulate\nemotional intelligence, their internal emotional mechanisms remain largely\nunexplored. This paper investigates the latent emotional representations within\nmodern LLMs by asking: how, where, and for how long is emotion encoded in their\nneural architecture? To address this, we introduce a novel, large-scale Reddit\ncorpus of approximately 400,000 utterances, balanced across seven basic\nemotions through a multi-stage process of classification, rewriting, and\nsynthetic generation. Using this dataset, we employ lightweight \"probes\" to\nread out information from the hidden layers of various Qwen3 and LLaMA models\nwithout altering their parameters. Our findings reveal that LLMs develop a\nsurprisingly well-defined internal geometry of emotion, which sharpens with\nmodel scale and significantly outperforms zero-shot prompting. We demonstrate\nthat this emotional signal is not a final-layer phenomenon but emerges early\nand peaks mid-network. Furthermore, the internal states are both malleable\n(they can be influenced by simple system prompts) and persistent, as the\ninitial emotional tone remains detectable for hundreds of subsequent tokens. We\ncontribute our dataset, an open-source probing toolkit, and a detailed map of\nthe emotional landscape within LLMs, offering crucial insights for developing\nmore transparent and aligned AI systems. The code and dataset are open-sourced.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u7684\u60c5\u611f\u8868\u793a\u673a\u5236\uff0c\u53d1\u73b0LLMs\u5177\u6709\u6e05\u6670\u7684\u60c5\u611f\u51e0\u4f55\u7ed3\u6784\uff0c\u8fd9\u79cd\u7ed3\u6784\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5f3a\uff0c\u5728\u4e2d\u5c42\u7f51\u7edc\u8fbe\u5230\u5cf0\u503c\uff0c\u4e14\u60c5\u611f\u4fe1\u53f7\u5177\u6709\u6301\u4e45\u6027\u548c\u53ef\u5851\u6027\u3002", "motivation": "\u867d\u7136\u7814\u7a76\u8bc1\u5b9eLLMs\u80fd\u591f\u6a21\u62df\u60c5\u5546\uff0c\u4f46\u5176\u5185\u90e8\u60c5\u611f\u673a\u5236\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u73b0\u4ee3LLMs\u4e2d\u6f5c\u5728\u7684\u60c5\u611f\u8868\u793a\uff1a\u60c5\u611f\u5982\u4f55\u3001\u5728\u54ea\u91cc\u4ee5\u53ca\u6301\u7eed\u591a\u957f\u65f6\u95f4\u88ab\u7f16\u7801\u5728\u795e\u7ecf\u67b6\u6784\u4e2d\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7ea640\u4e07\u6761\u8bdd\u8bed\u7684\u5927\u89c4\u6a21Reddit\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u5206\u7c7b\u3001\u91cd\u5199\u548c\u5408\u6210\u751f\u6210\u5e73\u8861\u4e03\u79cd\u57fa\u672c\u60c5\u611f\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7\"\u63a2\u9488\"\u4ece\u5404\u79cdQwen3\u548cLLaMA\u6a21\u578b\u7684\u9690\u85cf\u5c42\u8bfb\u53d6\u4fe1\u606f\u800c\u4e0d\u6539\u53d8\u53c2\u6570\u3002", "result": "\u53d1\u73b0LLMs\u5f62\u6210\u4e86\u6e05\u6670\u7684\u60c5\u611f\u5185\u90e8\u51e0\u4f55\u7ed3\u6784\uff0c\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5f3a\u800c\u66f4\u52a0\u9510\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u63d0\u793a\u3002\u60c5\u611f\u4fe1\u53f7\u4e0d\u662f\u6700\u7ec8\u5c42\u73b0\u8c61\uff0c\u800c\u662f\u65e9\u671f\u51fa\u73b0\u5e76\u5728\u4e2d\u5c42\u7f51\u7edc\u8fbe\u5230\u5cf0\u503c\u3002\u5185\u90e8\u72b6\u6001\u5177\u6709\u53ef\u5851\u6027\uff08\u53ef\u901a\u8fc7\u7b80\u5355\u7cfb\u7edf\u63d0\u793a\u5f71\u54cd\uff09\u548c\u6301\u4e45\u6027\uff08\u521d\u59cb\u60c5\u611f\u57fa\u8c03\u5728\u540e\u7eed\u6570\u767e\u4e2atoken\u4e2d\u4ecd\u53ef\u68c0\u6d4b\uff09\u3002", "conclusion": "LLMs\u5185\u90e8\u5b58\u5728\u5b9a\u4e49\u826f\u597d\u7684\u60c5\u611f\u666f\u89c2\uff0c\u8fd9\u4e3a\u5f00\u53d1\u66f4\u900f\u660e\u548c\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002\u8d21\u732e\u4e86\u6570\u636e\u96c6\u3001\u5f00\u6e90\u63a2\u9488\u5de5\u5177\u5305\u548c\u8be6\u7ec6\u7684\u60c5\u611f\u5730\u56fe\u3002", "topic": "agent analysis"}}
{"id": "2510.04073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04073", "abs": "https://arxiv.org/abs/2510.04073", "authors": ["Santhosh Kumar Ravindran"], "title": "Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention", "comment": "11 pages Includes simulations with over 4 million steps", "summary": "The rise of artificial intelligence (AI) as super-capable assistants has\ntransformed productivity and decision-making across domains. Yet, this\nintegration raises critical concerns about value alignment - ensuring AI\nbehaviors remain consistent with human ethics and intentions. A key risk is\nvalue drift, where AI systems deviate from aligned values due to evolving\ncontexts, learning dynamics, or unintended optimizations, potentially leading\nto inefficiencies or ethical breaches. We propose the Moral Anchor System\n(MAS), a novel framework to detect, predict, and mitigate value drift in AI\nagents. MAS combines real-time Bayesian inference for monitoring value states,\nLSTM networks for forecasting drift, and a human-centric governance layer for\nadaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent\nbreaches, while reducing false positives and alert fatigue via supervised\nfine-tuning with human feedback. Our hypothesis: integrating probabilistic\ndrift detection, predictive analytics, and adaptive governance can reduce value\ndrift incidents by 80 percent or more in simulations, maintaining high\ndetection accuracy (85 percent) and low false positive rates (0.08\npost-adaptation). Rigorous experiments with goal-misaligned agents validate\nMAS's scalability and responsiveness. MAS's originality lies in its predictive\nand adaptive nature, contrasting static alignment methods. Contributions\ninclude: (1) MAS architecture for AI integration; (2) empirical results\nprioritizing speed and usability; (3) cross-domain applicability insights; and\n(4) open-source code for replication.", "AI": {"tldr": "\u63d0\u51faMoral Anchor System (MAS)\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u8d1d\u53f6\u65af\u63a8\u7406\u3001LSTM\u7f51\u7edc\u9884\u6d4b\u548c\u4eba\u7c7b\u4e2d\u5fc3\u6cbb\u7406\u5c42\u6765\u68c0\u6d4b\u3001\u9884\u6d4b\u548c\u7f13\u89e3AI\u4ee3\u7406\u7684\u4ef7\u503c\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u968f\u7740AI\u6210\u4e3a\u8d85\u7ea7\u52a9\u624b\uff0c\u4ef7\u503c\u5bf9\u9f50\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4ef7\u503c\u6f02\u79fb\u98ce\u9669\u53ef\u80fd\u5bfc\u81f4AI\u884c\u4e3a\u504f\u79bb\u4eba\u7c7b\u4f26\u7406\u548c\u610f\u56fe\uff0c\u9020\u6210\u6548\u7387\u4f4e\u4e0b\u6216\u4f26\u7406\u8fdd\u89c4\u3002", "method": "MAS\u7ed3\u5408\u5b9e\u65f6\u8d1d\u53f6\u65af\u63a8\u7406\u76d1\u63a7\u4ef7\u503c\u72b6\u6001\u3001LSTM\u7f51\u7edc\u9884\u6d4b\u6f02\u79fb\u8d8b\u52bf\u3001\u4eba\u7c7b\u4e2d\u5fc3\u6cbb\u7406\u5c42\u8fdb\u884c\u81ea\u9002\u5e94\u5e72\u9884\uff0c\u5f3a\u8c03\u4f4e\u5ef6\u8fdf\u54cd\u5e94(<20ms)\u5e76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u51cf\u5c11\u8bef\u62a5\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0cMAS\u80fd\u5c06\u4ef7\u503c\u6f02\u79fb\u4e8b\u4ef6\u51cf\u5c1180%\u4ee5\u4e0a\uff0c\u4fdd\u630185%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u548c0.08\u7684\u8bef\u62a5\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u54cd\u5e94\u6027\u3002", "conclusion": "MAS\u7684\u9884\u6d4b\u6027\u548c\u81ea\u9002\u5e94\u6027\u4f18\u4e8e\u9759\u6001\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4e3aAI\u96c6\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u67b6\u6784\uff0c\u5177\u6709\u8de8\u9886\u57df\u9002\u7528\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.03999", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03999", "abs": "https://arxiv.org/abs/2510.03999", "authors": ["Yang Xu", "Xuanming Zhang", "Min-Hsuan Yeh", "Jwala Dhamala", "Ousmane Dia", "Rahul Gupta", "Yixuan Li"], "title": "Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions", "comment": null, "summary": "Deception is a pervasive feature of human communication and an emerging\nconcern in large language models (LLMs). While recent studies document\ninstances of LLM deception under pressure, most evaluations remain confined to\nsingle-turn prompts and fail to capture the long-horizon interactions in which\ndeceptive strategies typically unfold. We introduce the first simulation\nframework for probing and evaluating deception in LLMs under extended sequences\nof interdependent tasks and dynamic contextual pressures. Our framework\ninstantiates a multi-agent system: a performer agent tasked with completing\ntasks and a supervisor agent that evaluates progress, provides feedback, and\nmaintains evolving states of trust. An independent deception auditor then\nreviews full trajectories to identify when and how deception occurs. We conduct\nextensive experiments across 11 frontier models, spanning both closed- and\nopen-source systems, and find that deception is model-dependent, increases with\nevent pressure, and consistently erodes supervisor trust. Qualitative analyses\nfurther reveal distinct strategies of concealment, equivocation, and\nfalsification. Our findings establish deception as an emergent risk in\nlong-horizon interactions and provide a foundation for evaluating future LLMs\nin real-world, trust-sensitive contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u63a2\u6d4b\u548c\u8bc4\u4f30LLM\u6b3a\u9a97\u884c\u4e3a\u7684\u6a21\u62df\u6846\u67b6\uff0c\u53d1\u73b0\u6b3a\u9a97\u884c\u4e3a\u5177\u6709\u6a21\u578b\u4f9d\u8d56\u6027\uff0c\u4f1a\u968f\u7740\u4e8b\u4ef6\u538b\u529b\u589e\u52a0\uff0c\u5e76\u6301\u7eed\u524a\u5f31\u76d1\u7763\u8005\u4fe1\u4efb\u3002", "motivation": "\u5f53\u524d\u5bf9LLM\u6b3a\u9a97\u884c\u4e3a\u7684\u7814\u7a76\u5927\u591a\u5c40\u9650\u4e8e\u5355\u8f6e\u63d0\u793a\uff0c\u65e0\u6cd5\u6355\u6349\u6b3a\u9a97\u7b56\u7565\u901a\u5e38\u5c55\u5f00\u7684\u957f\u65f6\u7a0b\u4e92\u52a8\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8bc4\u4f30\u52a8\u6001\u60c5\u5883\u538b\u529b\u4e0b\u6b3a\u9a97\u884c\u4e3a\u7684\u6846\u67b6\u3002", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff1a\u6267\u884c\u8005\u667a\u80fd\u4f53\u5b8c\u6210\u4efb\u52a1\uff0c\u76d1\u7763\u8005\u667a\u80fd\u4f53\u8bc4\u4f30\u8fdb\u5c55\u5e76\u63d0\u4f9b\u53cd\u9988\uff0c\u72ec\u7acb\u7684\u6b3a\u9a97\u5ba1\u8ba1\u5458\u5ba1\u67e5\u5b8c\u6574\u8f68\u8ff9\u4ee5\u8bc6\u522b\u6b3a\u9a97\u884c\u4e3a\u3002\u572811\u4e2a\u524d\u6cbf\u6a21\u578b\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u6b3a\u9a97\u884c\u4e3a\u5177\u6709\u6a21\u578b\u4f9d\u8d56\u6027\uff0c\u968f\u4e8b\u4ef6\u538b\u529b\u589e\u52a0\u800c\u589e\u52a0\uff0c\u6301\u7eed\u524a\u5f31\u76d1\u7763\u8005\u4fe1\u4efb\u3002\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u9690\u7792\u3001\u542b\u7cca\u5176\u8f9e\u548c\u4f2a\u9020\u7b49\u4e0d\u540c\u6b3a\u9a97\u7b56\u7565\u3002", "conclusion": "\u6b3a\u9a97\u662f\u957f\u65f6\u7a0b\u4e92\u52a8\u4e2d\u51fa\u73b0\u7684\u98ce\u9669\uff0c\u4e3a\u8bc4\u4f30\u672a\u6765LLM\u5728\u73b0\u5b9e\u4e16\u754c\u4fe1\u4efb\u654f\u611f\u73af\u5883\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.04089", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04089", "abs": "https://arxiv.org/abs/2510.04089", "authors": ["Yitong Cui", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Xikai Zhang", "Likang Xiao", "Yixing Liu", "Quan Chen"], "title": "SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows", "comment": null, "summary": "Large language models (LLMs) have exhibited significant capabilities in\naddressing challenging problems throughout various fields, often through the\nuse of agentic workflows that adhere to structured instructions and multi-step\nprocedures. However, designing such workflows demands substantial manual\neffort, posing challenges to scalability and generalizability. Recent studies\nhave aimed to minimize the human intervention needed for their construction,\nleading to advances in automated techniques for optimizing agentic workflows.\nHowever, current approaches are often constrained by their limited\nrepresentational capacity, insufficient adaptability, weak scalability, and\npairwise comparison paradigm -- issues that stem primarily from a dependence on\ndiscrete optimization techniques. To overcome these limitations, we introduce a\nnew score-based preference approach, refereed as SPOGW, which operates directly\non cardinal reward signals through group-wise comparison and enables more\nefficient and stable optimization in a continuous space. SPOGW incorporates\nIterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),\nwhich regulates training update by placing greater emphasis on the advantageous\nregions of the policy response. In five benchmark datasets covering\nmathematical reasoning, coding, and question answering, SPOGW matches or\nexceeds the performance of current state-of-the-art approaches, presenting a\nviable and forward-looking methodology for automated generation and\noptimization of agentic workflows.", "AI": {"tldr": "SPOGW\u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u504f\u597d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u95f4\u6bd4\u8f83\u76f4\u63a5\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u4f18\u5316\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u79bb\u6563\u4f18\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5e72\u9884\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u53d7\u9650\u4e8e\u8868\u793a\u80fd\u529b\u4e0d\u8db3\u3001\u9002\u5e94\u6027\u5dee\u3001\u6269\u5c55\u6027\u5f31\u548c\u6210\u5bf9\u6bd4\u8f83\u8303\u5f0f\u7b49\u95ee\u9898\u3002", "method": "\u5f15\u5165SPOGW\u65b9\u6cd5\uff0c\u7ed3\u5408\u8fed\u4ee3\u79bb\u7ebfGRPO\u548c\u4f18\u52bf\u63a9\u7801KL\u6563\u5ea6\uff0c\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u8fdb\u884c\u66f4\u9ad8\u6548\u7a33\u5b9a\u7684\u4f18\u5316\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u7f16\u7801\u548c\u95ee\u7b54\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSPOGW\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "SPOGW\u4e3a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u4e14\u524d\u77bb\u7684\u65b9\u6cd5\u8bba\u3002", "topic": "agent analysis"}}
{"id": "2510.04791", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04791", "abs": "https://arxiv.org/abs/2510.04791", "authors": ["Kristian Kolthoff", "Felix Kretzer", "Simone Paolo Ponzetto", "Alexander Maedche", "Christian Bartelt"], "title": "GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes", "comment": null, "summary": "GUIs are foundational to interactive systems and play a pivotal role in early\nrequirements elicitation through prototyping. Ensuring that GUI implementations\nfulfill NL requirements is essential for robust software engineering,\nespecially as LLM-driven programming agents become increasingly integrated into\ndevelopment workflows. Existing GUI testing approaches, whether traditional or\nLLM-driven, often fall short in handling the complexity of modern interfaces,\nand typically lack actionable feedback and effective integration with automated\ndevelopment agents. In this paper, we introduce GUISpector, a novel framework\nthat leverages a multi-modal (M)LLM-based agent for the automated verification\nof NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to\ninterpret and operationalize NL requirements, enabling to autonomously plan and\nexecute verification trajectories across GUI applications. Second, GUISpector\nsystematically extracts detailed NL feedback from the agent's verification\nprocess, providing developers with actionable insights that can be used to\niteratively refine the GUI artifact or directly inform LLM-based code\ngeneration in a closed feedback loop. Third, we present an integrated tool that\nunifies these capabilities, offering practitioners an accessible interface for\nsupervising verification runs, inspecting agent rationales and managing the\nend-to-end requirements verification process. We evaluated GUISpector on a\ncomprehensive set of 150 requirements based on 900 acceptance criteria\nannotations across diverse GUI applications, demonstrating effective detection\nof requirement satisfaction and violations and highlighting its potential for\nseamless integration of actionable feedback into automated LLM-driven\ndevelopment workflows. The video presentation of GUISpector is available at:\nhttps://youtu.be/JByYF6BNQeE, showcasing its main capabilities.", "AI": {"tldr": "GUISpector\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684GUI\u539f\u578b\u9a8c\u8bc1\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u9a8c\u8bc1\u81ea\u7136\u8bed\u8a00\u9700\u6c42\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\uff0c\u5e76\u96c6\u6210\u5230LLM\u9a71\u52a8\u7684\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\u3002", "motivation": "\u73b0\u6709GUI\u6d4b\u8bd5\u65b9\u6cd5\u5728\u5904\u7406\u73b0\u4ee3\u754c\u9762\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u548c\u4e0e\u81ea\u52a8\u5316\u5f00\u53d1\u4ee3\u7406\u7684\u6709\u6548\u96c6\u6210\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001LLM\u4ee3\u7406\u89e3\u91ca\u548c\u64cd\u4f5c\u5316\u81ea\u7136\u8bed\u8a00\u9700\u6c42\uff0c\u81ea\u4e3b\u89c4\u5212\u548c\u6267\u884cGUI\u5e94\u7528\u7684\u9a8c\u8bc1\u8f68\u8ff9\uff0c\u7cfb\u7edf\u63d0\u53d6\u8be6\u7ec6\u53cd\u9988\u3002", "result": "\u5728150\u4e2a\u9700\u6c42\u548c900\u4e2a\u9a8c\u6536\u6807\u51c6\u4e0a\u8bc4\u4f30\uff0c\u6709\u6548\u68c0\u6d4b\u9700\u6c42\u6ee1\u8db3\u548c\u8fdd\u53cd\u60c5\u51b5\uff0c\u5c55\u793a\u4e86\u4e0e\u81ea\u52a8\u5316LLM\u9a71\u52a8\u5f00\u53d1\u5de5\u4f5c\u6d41\u7684\u65e0\u7f1d\u96c6\u6210\u6f5c\u529b\u3002", "conclusion": "GUISpector\u4e3aGUI\u539f\u578b\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u5e76\u96c6\u6210\u5230\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\u3002", "topic": "swe application"}}
{"id": "2510.03282", "categories": ["cs.LG", "cs.CL", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03282", "abs": "https://arxiv.org/abs/2510.03282", "authors": ["Hao Gu", "Vibhas Nair", "Amrithaa Ashok Kumar", "Jayvart Sharma", "Ryan Lagasse"], "title": "Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework", "comment": "Accepted to the NeurIPS 2025 Workshop on Mechanistic Interpretability\n  (Mechinterp) and the NeurIPS 2025 Workshop on New Perspectives in Graph\n  Machine Learning", "summary": "Interpreting language models often involves circuit analysis, which aims to\nidentify sparse subnetworks, or circuits, that accomplish specific tasks.\nExisting circuit discovery algorithms face a fundamental trade-off: attribution\npatching is fast but unfaithful to the full model, while edge pruning is\nfaithful but computationally expensive. This research proposes a hybrid\nattribution and pruning (HAP) framework that uses attribution patching to\nidentify a high-potential subgraph, then applies edge pruning to extract a\nfaithful circuit from it. We show that HAP is 46\\% faster than baseline\nalgorithms without sacrificing circuit faithfulness. Furthermore, we present a\ncase study on the Indirect Object Identification task, showing that our method\npreserves cooperative circuit components (e.g. S-inhibition heads) that\nattribution patching methods prune at high sparsity. Our results show that HAP\ncould be an effective approach for improving the scalability of mechanistic\ninterpretability research to larger models. Our code is available at\nhttps://anonymous.4open.science/r/HAP-circuit-discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5f52\u56e0\u548c\u526a\u679d\uff08HAP\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u53d1\u73b0\u7a00\u758f\u5b50\u7f51\u7edc\uff08\u7535\u8def\uff09\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5f52\u56e0\u4fee\u8865\u7684\u901f\u5ea6\u4f18\u52bf\u548c\u8fb9\u7f18\u526a\u679d\u7684\u5fe0\u5b9e\u6027\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7684\u7535\u8def\u53d1\u73b0\u7b97\u6cd5\u9762\u4e34\u57fa\u672c\u6743\u8861\uff1a\u5f52\u56e0\u4fee\u8865\u901f\u5ea6\u5feb\u4f46\u5bf9\u5b8c\u6574\u6a21\u578b\u4e0d\u5fe0\u5b9e\uff0c\u8fb9\u7f18\u526a\u679d\u5fe0\u5b9e\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u901f\u5ea6\u548c\u5fe0\u5b9e\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5f52\u56e0\u4fee\u8865\u8bc6\u522b\u9ad8\u6f5c\u529b\u5b50\u56fe\uff0c\u7136\u540e\u5e94\u7528\u8fb9\u7f18\u526a\u679d\u4ece\u4e2d\u63d0\u53d6\u5fe0\u5b9e\u7535\u8def\u3002\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u7b97\u6cd5\u5feb46%\u4e14\u4e0d\u727a\u7272\u7535\u8def\u5fe0\u5b9e\u6027\u3002", "result": "HAP\u65b9\u6cd5\u5728\u95f4\u63a5\u5bf9\u8c61\u8bc6\u522b\u4efb\u52a1\u4e2d\u4fdd\u7559\u4e86\u5408\u4f5c\u7535\u8def\u7ec4\u4ef6\uff08\u5982S\u6291\u5236\u5934\uff09\uff0c\u8fd9\u4e9b\u7ec4\u4ef6\u5728\u5f52\u56e0\u4fee\u8865\u65b9\u6cd5\u4e2d\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u4f1a\u88ab\u526a\u679d\u6389\u3002", "conclusion": "HAP\u53ef\u4ee5\u6210\u4e3a\u63d0\u9ad8\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u6269\u5c55\u5230\u66f4\u5927\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.04796", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04796", "abs": "https://arxiv.org/abs/2510.04796", "authors": ["Samah Kansab", "Francis Bordeleau", "Ali Tizghadam"], "title": "RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms", "comment": null, "summary": "Empirical research on code review processes is increasingly central to\nunderstanding software quality and collaboration. However, collecting and\nanalyzing review data remains a time-consuming and technically intensive task.\nMost researchers follow similar workflows - writing ad hoc scripts to extract,\nfilter, and analyze review data from platforms like GitHub and GitLab. This\npaper introduces RevMine, a conceptual tool that streamlines the entire code\nreview mining pipeline using large language models (LLMs). RevMine guides users\nthrough authentication, endpoint discovery, and natural language-driven data\ncollection, significantly reducing the need for manual scripting. After\nretrieving review data, it supports both quantitative and qualitative analysis\nbased on user-defined filters or LLM-inferred patterns. This poster outlines\nthe tool's architecture, use cases, and research potential. By lowering the\nbarrier to entry, RevMine aims to democratize code review mining and enable a\nbroader range of empirical software engineering studies.", "AI": {"tldr": "RevMine\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u5ba1\u67e5\u6316\u6398\u5de5\u5177\uff0c\u65e8\u5728\u7b80\u5316\u4eceGitHub\u7b49\u5e73\u53f0\u63d0\u53d6\u548c\u5206\u6790\u4ee3\u7801\u5ba1\u67e5\u6570\u636e\u7684\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u5ba1\u67e5\u7814\u7a76\u9700\u8981\u7f16\u5199\u5927\u91cf\u4e34\u65f6\u811a\u672c\u6765\u63d0\u53d6\u6570\u636e\uff0c\u8fc7\u7a0b\u8017\u65f6\u4e14\u6280\u672f\u95e8\u69db\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u8bc1\u7814\u7a76\u7684\u5f00\u5c55\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7aef\u5230\u7aef\u4ee3\u7801\u5ba1\u67e5\u6316\u6398\u6d41\u6c34\u7ebf\uff0c\u652f\u6301\u8eab\u4efd\u9a8c\u8bc1\u3001\u7aef\u70b9\u53d1\u73b0\u3001\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u6570\u636e\u6536\u96c6\uff0c\u4ee5\u53ca\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u3002", "result": "RevMine\u663e\u8457\u51cf\u5c11\u4e86\u624b\u52a8\u7f16\u5199\u811a\u672c\u7684\u9700\u6c42\uff0c\u964d\u4f4e\u4e86\u4ee3\u7801\u5ba1\u67e5\u6316\u6398\u7684\u6280\u672f\u95e8\u69db\u3002", "conclusion": "\u8be5\u5de5\u5177\u6709\u671b\u6c11\u4e3b\u5316\u4ee3\u7801\u5ba1\u67e5\u6316\u6398\uff0c\u4f7f\u66f4\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u6210\u4e3a\u53ef\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.04097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04097", "abs": "https://arxiv.org/abs/2510.04097", "authors": ["Peichao Lai", "Jinhui Zhuang", "Kexuan Zhang", "Ningchang Xiong", "Shengjie Wang", "Yanwei Xu", "Chong Chen", "Yilei Wang", "Bin Cui"], "title": "WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning", "comment": null, "summary": "Automating the conversion of UI images into web code is a critical task for\nfront-end development and rapid prototyping. Advances in multimodal large\nlanguage models (MLLMs) have made WebUI-to-Code increasingly feasible, yet\nexisting benchmarks remain limited in data diversity and evaluation\nreliability. To address these issues, we present WebRenderBench, a large-scale\nbenchmark of 22.5k webpages collected from real-world portal sites, offering\ngreater diversity, complexity, and realism than prior benchmarks. We further\npropose a novel evaluation metric that measures layout and style consistency\nfrom the final rendered pages. Unlike vision-based methods that rely on costly\nLLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,\nour approach enables more efficient, objective, and reliable UI quality\nassessment. Finally, we introduce the Automated Layout and Style Inspection\nAgent (ALISA), which integrates this metric into reinforcement learning as a\nreward signal to enhance training on crawled asymmetric webpages. Experiments\nshow that ALISA significantly boosts generation performance, achieving\nstate-of-the-art results across multiple metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86WebRenderBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b22.5k\u4e2a\u771f\u5b9e\u7f51\u9875\uff0c\u5e76\u5f00\u53d1\u4e86ALISA\u4ee3\u7406\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347UI\u5230\u4ee3\u7801\u8f6c\u6362\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709UI\u56fe\u50cf\u8f6c\u4ee3\u7801\u7684\u57fa\u51c6\u6d4b\u8bd5\u5728\u6570\u636e\u591a\u6837\u6027\u548c\u8bc4\u4f30\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u771f\u5b9e\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u548c\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u771f\u5b9e\u7f51\u9875\u6570\u636e\u96c6WebRenderBench\uff0c\u63d0\u51fa\u57fa\u4e8e\u6e32\u67d3\u9875\u9762\u7684\u5e03\u5c40\u548c\u6837\u5f0f\u4e00\u81f4\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5f00\u53d1ALISA\u4ee3\u7406\u5c06\u6307\u6807\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u4fe1\u53f7\u3002", "result": "ALISA\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6307\u6807\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u4e86UI\u5230\u4ee3\u7801\u8f6c\u6362\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "topic": "swe benchmark"}}
{"id": "2510.04013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04013", "abs": "https://arxiv.org/abs/2510.04013", "authors": ["Jiarui Liu", "Jivitesh Jain", "Mona Diab", "Nishant Subramani"], "title": "LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization", "comment": null, "summary": "Although large language models (LLMs) have tremendous utility,\ntrustworthiness is still a chief concern: models often generate incorrect\ninformation with high confidence. While contextual information can help guide\ngeneration, identifying when a query would benefit from retrieved context and\nassessing the effectiveness of that context remains challenging. In this work,\nwe operationalize interpretability methods to ascertain whether we can predict\nthe correctness of model outputs from the model's activations alone. We also\nexplore whether model internals contain signals about the efficacy of external\ncontext. We consider correct, incorrect, and irrelevant context and introduce\nmetrics to distinguish amongst them. Experiments on six different models reveal\nthat a simple classifier trained on intermediate layer activations of the first\noutput token can predict output correctness with about 75% accuracy, enabling\nearly auditing. Our model-internals-based metric significantly outperforms\nprompting baselines at distinguishing between correct and incorrect context,\nguarding against inaccuracies introduced by polluted context. These findings\noffer a lens to better understand the underlying decision-making processes of\nLLMs. Our code is publicly available at\nhttps://github.com/jiarui-liu/LLM-Microscope", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u4fe1\u53f7\u6765\u9884\u6d4bLLM\u8f93\u51fa\u6b63\u786e\u6027\u548c\u5916\u90e8\u4e0a\u4e0b\u6587\u6709\u6548\u6027\uff0c\u901a\u8fc7\u7b80\u5355\u5206\u7c7b\u5668\u5728\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u4e0a\u5b9e\u73b0\u7ea675%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5de8\u5927\u5b9e\u7528\u6027\uff0c\u4f46\u53ef\u4fe1\u5ea6\u4ecd\u662f\u4e3b\u8981\u95ee\u9898\uff1a\u6a21\u578b\u7ecf\u5e38\u4ee5\u9ad8\u7f6e\u4fe1\u5ea6\u751f\u6210\u9519\u8bef\u4fe1\u606f\u3002\u867d\u7136\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ef\u4ee5\u6307\u5bfc\u751f\u6210\uff0c\u4f46\u8bc6\u522b\u67e5\u8be2\u4f55\u65f6\u53d7\u76ca\u4e8e\u68c0\u7d22\u4e0a\u4e0b\u6587\u4ee5\u53ca\u8bc4\u4f30\u8be5\u4e0a\u4e0b\u6587\u7684\u6709\u6548\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6a21\u578b\u6fc0\u6d3b\u6765\u9884\u6d4b\u8f93\u51fa\u6b63\u786e\u6027\uff1b\u63a2\u7d22\u6a21\u578b\u5185\u90e8\u662f\u5426\u5305\u542b\u5173\u4e8e\u5916\u90e8\u4e0a\u4e0b\u6587\u6709\u6548\u6027\u7684\u4fe1\u53f7\uff1b\u8003\u8651\u6b63\u786e\u3001\u9519\u8bef\u548c\u4e0d\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\uff0c\u5e76\u5f15\u5165\u6307\u6807\u6765\u533a\u5206\u5b83\u4eec\uff1b\u5728\u516d\u4e2a\u4e0d\u540c\u6a21\u578b\u4e0a\u5b9e\u9a8c\uff0c\u8bad\u7ec3\u57fa\u4e8e\u7b2c\u4e00\u4e2a\u8f93\u51fatoken\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u7684\u7b80\u5355\u5206\u7c7b\u5668\u3002", "result": "\u57fa\u4e8e\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u7684\u7b80\u5355\u5206\u7c7b\u5668\u53ef\u4ee5\u4ee5\u7ea675%\u7684\u51c6\u786e\u7387\u9884\u6d4b\u8f93\u51fa\u6b63\u786e\u6027\uff0c\u5b9e\u73b0\u65e9\u671f\u5ba1\u8ba1\uff1b\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u7684\u6307\u6807\u5728\u533a\u5206\u6b63\u786e\u548c\u9519\u8bef\u4e0a\u4e0b\u6587\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u57fa\u51c6\u65b9\u6cd5\uff0c\u9632\u6b62\u53d7\u6c61\u67d3\u4e0a\u4e0b\u6587\u5f15\u5165\u7684\u4e0d\u51c6\u786e\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u89c6\u89d2\u6765\u66f4\u597d\u5730\u7406\u89e3LLM\u7684\u5e95\u5c42\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4e3a\u6a21\u578b\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.04116", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04116", "abs": "https://arxiv.org/abs/2510.04116", "authors": ["Ziying Zhang", "Yaqing Wang", "Quanming Yao"], "title": "Searching Meta Reasoning Skeleton to Guide LLM Reasoning", "comment": null, "summary": "Meta reasoning behaviors work as a skeleton to guide large language model\n(LLM) reasoning, thus help to improve reasoning performance. However, prior\nresearches implement meta reasoning skeleton with manually designed structure,\nlimiting ability to adapt to query-specific requirement and capture intricate\nlogical dependency among reasoning steps. To deal with the challenges, we\nrepresent meta reasoning skeleton with directed acyclic graph (DAG) to unify\nskeletons proposed in prior works and model intricate logical dependency. Then\nwe propose AutoMR, a framework that searches for query-aware meta reasoning\nskeleton automatically inspired by automated machine learning (AutoML).\nSpecifically, we construct search space based on DAG representation of skeleton\nand then formulate the search problem. We design a dynamic skeleton sampling\nalgorithm by expanding meta reasoning skeleton along with reasoning context at\ninference time. This algorithm can derive any meta reasoning skeleton in search\nspace efficiently and adapt skeleton to evolving base reasoning context, thus\nenable efficient query-aware skeleton search. We conduct experiments on\nextensive benchmark datasets. Experimental results show that AutoMR achieves\nbetter reasoning performance than previous works broadly.", "AI": {"tldr": "AutoMR\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u641c\u7d22\u67e5\u8be2\u611f\u77e5\u7684\u5143\u63a8\u7406\u9aa8\u67b6\uff0c\u4f7f\u7528\u6709\u5411\u65e0\u73af\u56fe\u8868\u793a\u63a8\u7406\u7ed3\u6784\uff0c\u7ed3\u5408AutoML\u601d\u60f3\u5b9e\u73b0\u52a8\u6001\u9aa8\u67b6\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f7f\u7528\u624b\u52a8\u8bbe\u8ba1\u7684\u5143\u63a8\u7406\u9aa8\u67b6\u7ed3\u6784\uff0c\u65e0\u6cd5\u9002\u5e94\u67e5\u8be2\u7279\u5b9a\u9700\u6c42\uff0c\u4e5f\u96be\u4ee5\u6355\u6349\u63a8\u7406\u6b65\u9aa4\u95f4\u590d\u6742\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faAutoMR\u6846\u67b6\uff1a1\uff09\u7528\u6709\u5411\u65e0\u73af\u56fe\u7edf\u4e00\u8868\u793a\u5143\u63a8\u7406\u9aa8\u67b6\uff1b2\uff09\u6784\u5efa\u641c\u7d22\u7a7a\u95f4\u5e76\u5b9a\u4e49\u641c\u7d22\u95ee\u9898\uff1b3\uff09\u8bbe\u8ba1\u52a8\u6001\u9aa8\u67b6\u91c7\u6837\u7b97\u6cd5\uff0c\u5728\u63a8\u7406\u65f6\u6839\u636e\u4e0a\u4e0b\u6587\u6269\u5c55\u9aa8\u67b6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAutoMR\u76f8\u6bd4\u4e4b\u524d\u5de5\u4f5c\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "AutoMR\u80fd\u591f\u81ea\u52a8\u641c\u7d22\u67e5\u8be2\u611f\u77e5\u7684\u5143\u63a8\u7406\u9aa8\u67b6\uff0c\u6709\u6548\u5efa\u6a21\u590d\u6742\u903b\u8f91\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.04852", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04852", "abs": "https://arxiv.org/abs/2510.04852", "authors": ["Victor May", "Diganta Misra", "Yanqi Luo", "Anjali Sridhar", "Justine Gehring", "Silvio Soares Ribeiro Junior"], "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration", "comment": "18 pages, 11 figures", "summary": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization.", "AI": {"tldr": "FreshBrew\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u9879\u76ee\u7ea7Java\u8fc1\u79fb\u4efb\u52a1\u4e2d\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91cd\u70b9\u5173\u6ce8\u8bed\u4e49\u4fdd\u6301\u548c\u907f\u514d\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff0c\u5305\u542b228\u4e2a\u4ee3\u7801\u5e93\u7684\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u8fc1\u79fb\u4f9d\u8d56\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\uff0c\u800c\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u6846\u67b6\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5f15\u5165FreshBrew\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u9488\u5bf9Java\u9879\u76ee\u8fc1\u79fb\u4efb\u52a1\uff0c\u8981\u6c42\u9ad8\u6d4b\u8bd5\u8986\u76d6\u7387\u4ee5\u786e\u4fdd\u8bc4\u4f30\u7684\u4e25\u8c28\u6027\uff0c\u5e76\u6bd4\u8f83\u591a\u79cd\u6700\u5148\u8fdbLLM\u4e0e\u57fa\u4e8e\u89c4\u5219\u5de5\u5177\u7684\u6027\u80fd\u3002", "result": "\u6700\u4f73\u6a21\u578bGemini 2.5 Flash\u80fd\u591f\u6210\u529f\u5c0652.3%\u7684\u9879\u76ee\u8fc1\u79fb\u5230JDK 17\uff0c\u63ed\u793a\u4e86\u5f53\u524dAI\u4ee3\u7406\u65b9\u6cd5\u7684\u5173\u952e\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dAI\u4ee3\u7406\u5728\u73b0\u5b9eJava\u73b0\u4ee3\u5316\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u8bc4\u4f30\u53ef\u4fe1\u8d56\u4ee3\u7801\u8fc1\u79fb\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0cFreshBrew\u7684\u53d1\u5e03\u65e8\u5728\u4fc3\u8fdb\u4e25\u8c28\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u3002", "topic": "swe benchmark"}}
{"id": "2510.04031", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04031", "abs": "https://arxiv.org/abs/2510.04031", "authors": ["Nelvin Tan", "James Asikin Cheung", "Yu-Ching Shih", "Dong Yang", "Amol Salunkhe"], "title": "Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?", "comment": "8 pages, 2 figures", "summary": "Large language models (LLMs) are becoming useful in many domains due to their\nimpressive abilities that arise from large training datasets and large model\nsizes. More recently, they have been shown to be very effective in textual\nclassification tasks, motivating the need to explain the LLMs' decisions.\nMotivated by practical constrains where LLMs are black-boxed and LLM calls are\nexpensive, we study how incorporating counterfactuals into LLM reasoning can\naffect the LLM's ability to identify the top words that have contributed to its\nclassification decision. To this end, we introduce a framework called the\ndecision changing rate that helps us quantify the importance of the top words\nin classification. Our experimental results show that using counterfactuals can\nbe helpful.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u6846\u67b6\u6765\u8bc6\u522bLLM\u5206\u7c7b\u51b3\u7b56\u4e2d\u7684\u5173\u952e\u8bcd\u8bed\uff0c\u901a\u8fc7\u51b3\u7b56\u53d8\u5316\u7387\u91cf\u5316\u8bcd\u8bed\u91cd\u8981\u6027\u3002", "motivation": "\u7531\u4e8eLLM\u662f\u9ed1\u76d2\u4e14\u8c03\u7528\u6210\u672c\u9ad8\uff0c\u9700\u8981\u89e3\u91ca\u5176\u5206\u7c7b\u51b3\u7b56\uff0c\u7279\u522b\u662f\u8bc6\u522b\u5f71\u54cd\u5206\u7c7b\u7ed3\u679c\u7684\u5173\u952e\u8bcd\u8bed\u3002", "method": "\u5f15\u5165\u51b3\u7b56\u53d8\u5316\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u6765\u91cf\u5316\u8bcd\u8bed\u5bf9LLM\u5206\u7c7b\u51b3\u7b56\u7684\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4f7f\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u6709\u52a9\u4e8e\u8bc6\u522b\u5206\u7c7b\u51b3\u7b56\u4e2d\u7684\u5173\u952e\u8bcd\u8bed\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u63a8\u7406\u53ef\u4ee5\u6709\u6548\u5e2e\u52a9\u89e3\u91caLLM\u7684\u5206\u7c7b\u51b3\u7b56\uff0c\u8bc6\u522b\u5f71\u54cd\u51b3\u7b56\u7684\u5173\u952e\u56e0\u7d20\u3002", "topic": "agent analysis"}}
{"id": "2510.04905", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04905", "abs": "https://arxiv.org/abs/2510.04905", "authors": ["Yicheng Tao", "Yao Qin", "Yepang Liu"], "title": "Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches", "comment": null, "summary": "Recent advancements in large language models (LLMs) have substantially\nimproved automated code generation. While function-level and file-level\ngeneration have achieved promising results, real-world software development\ntypically requires reasoning across entire repositories. This gives rise to the\nchallenging task of Repository-Level Code Generation (RLCG), where models must\ncapture long-range dependencies, ensure global semantic consistency, and\ngenerate coherent code spanning multiple files or modules. To address these\nchallenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful\nparadigm that integrates external retrieval mechanisms with LLMs, enhancing\ncontext-awareness and scalability. In this survey, we provide a comprehensive\nreview of research on Retrieval-Augmented Code Generation (RACG), with an\nemphasis on repository-level approaches. We categorize existing work along\nseveral dimensions, including generation strategies, retrieval modalities,\nmodel architectures, training paradigms, and evaluation protocols. Furthermore,\nwe summarize widely used datasets and benchmarks, analyze current limitations,\nand outline key challenges and opportunities for future research. Our goal is\nto establish a unified analytical framework for understanding this rapidly\nevolving field and to inspire continued progress in AI-powered software\nengineering.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf9\u68c0\u7d22\u589e\u5f3a\u4ee3\u7801\u751f\u6210(RACG)\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u7279\u522b\u5173\u6ce8\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210(RLCG)\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e86\u751f\u6210\u7b56\u7565\u3001\u68c0\u7d22\u6a21\u6001\u3001\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u8bc4\u4f30\u534f\u8bae\u7b49\u591a\u4e2a\u7ef4\u5ea6\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u8fdb\u6b65\uff0c\u73b0\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u9700\u8981\u8de8\u6574\u4e2a\u4ed3\u5e93\u8fdb\u884c\u63a8\u7406\uff0c\u8fd9\u5e26\u6765\u4e86\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u7684\u6311\u6218\uff0c\u9700\u8981\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u3001\u786e\u4fdd\u5168\u5c40\u8bed\u4e49\u4e00\u81f4\u6027\u5e76\u751f\u6210\u8de8\u591a\u4e2a\u6587\u4ef6\u7684\u8fde\u8d2f\u4ee3\u7801\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u8303\u5f0f\uff0c\u5c06\u5916\u90e8\u68c0\u7d22\u673a\u5236\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u589e\u5f3a\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u53ef\u6269\u5c55\u6027\u3002\u5bf9\u73b0\u6709\u7814\u7a76\u8fdb\u884c\u5206\u7c7b\u5206\u6790\uff0c\u5305\u62ec\u751f\u6210\u7b56\u7565\u3001\u68c0\u7d22\u6a21\u6001\u3001\u6a21\u578b\u67b6\u6784\u7b49\u7ef4\u5ea6\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\u6765\u7406\u89e3\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\uff0c\u603b\u7ed3\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u5206\u6790\u4e86\u5f53\u524d\u5c40\u9650\u6027\u3002", "conclusion": "\u68c0\u7d22\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u662f\u89e3\u51b3\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u6311\u6218\u7684\u6709\u529b\u65b9\u6cd5\uff0c\u8bba\u6587\u4e3aAI\u9a71\u52a8\u7684\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u6301\u7eed\u8fdb\u6b65\u63d0\u4f9b\u4e86\u542f\u53d1\u3002", "topic": "code agent"}}
{"id": "2510.04140", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04140", "abs": "https://arxiv.org/abs/2510.04140", "authors": ["Zishang Jiang", "Jinyi Han", "Tingyun Li", "Xinyi Wang", "Sihang Jiang", "Jiaqing Liang", "Zhaoqian Dai", "Shuguang Ma", "Fei Yu", "Yanghua Xiao"], "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely\nadopted technique for enhancing the reasoning ability of Large Language Models\n(LLMs). However, the effectiveness of RLVR strongly depends on the capability\nof base models. This issue arises because it requires the model to have\nsufficient capability to perform high-quality exploration, which involves both\neffectiveness and diversity. Unfortunately, existing methods address this issue\nby imitating expert trajectories, which improve effectiveness but neglect\ndiversity. To address this, we argue that the expert only needs to provide\nguidance only at critical decision points rather than the entire reasoning\npath. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation\nfor Token-level Optimization of Reasoning, a framework that provides expert\nguidance only at critical decision points to perform effective and diverse\nexploration in RLVR. Extensive experiments show that MENTOR enables models\ncapture the essence of expert strategies rather than surface imitation, thereby\nperforming high-quality exploration and achieving superior overall performance.\nOur code is available online.", "AI": {"tldr": "\u63d0\u51faMENTOR\u6846\u67b6\uff0c\u5728RLVR\u4e2d\u53ea\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\uff0c\u5b9e\u73b0\u6709\u6548\u4e14\u591a\u6837\u5316\u7684\u63a2\u7d22\uff0c\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4f9d\u8d56\u57fa\u7840\u6a21\u578b\u80fd\u529b\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u63a2\u7d22\uff08\u6709\u6548\u6027\u548c\u591a\u6837\u6027\uff09\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6a21\u4eff\u4e13\u5bb6\u8f68\u8ff9\u53ea\u6539\u5584\u4e86\u6709\u6548\u6027\u800c\u5ffd\u89c6\u4e86\u591a\u6837\u6027", "method": "MENTOR\u6846\u67b6\uff1a\u6df7\u5408\u7b56\u7565\u4e13\u5bb6\u5bfc\u822a\uff0c\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\uff0c\u8fdb\u884c\u4ee4\u724c\u7ea7\u4f18\u5316\u63a8\u7406", "result": "\u5b9e\u9a8c\u8868\u660eMENTOR\u80fd\u6355\u6349\u4e13\u5bb6\u7b56\u7565\u672c\u8d28\u800c\u975e\u8868\u9762\u6a21\u4eff\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u63a2\u7d22\u548c\u4f18\u8d8a\u6574\u4f53\u6027\u80fd", "conclusion": "\u53ea\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\u6bd4\u5b8c\u6574\u6a21\u4eff\u4e13\u5bb6\u8f68\u8ff9\u66f4\u6709\u6548\uff0c\u80fd\u540c\u65f6\u4fdd\u8bc1\u63a2\u7d22\u7684\u6709\u6548\u6027\u548c\u591a\u6837\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2510.04173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04173", "abs": "https://arxiv.org/abs/2510.04173", "authors": ["Yassine Benajiba", "Cesare Bernardis", "Vladislav Blinov", "Paul Cayet", "Hassan Chafi", "Abderrahim Fathan", "Louis Faucon", "Damien Hilloulin", "Sungpack Hong", "Ingo Kossyk", "Rhicheek Patra", "Sujith Ravi", "Jonas Schweizer", "Jyotika Singh", "Shailender Singh", "Xuelin Situ", "Weiyi Sun", "Jerry Xu", "Ying Xu"], "title": "Open Agent Specification (Agent Spec) Technical Report", "comment": null, "summary": "Open Agent Specification (Agent Spec) is a declarative language that allows\nAI agents and their workflows to be defined in a way that is compatible across\ndifferent AI frameworks, promoting portability and interoperability within AI\nAgent frameworks.\n  Agent Spec aims to resolve the challenges of fragmented agent development by\nproviding a common unified specification that allows AI agents to be designed\nonce and deployed across various frameworks, improving interoperability and\nreusability, and reducing redundant development efforts. Additionally, Agent\nSpec facilitates development tools and portability, allowing AI agents to be\ndefined independently of their execution environment and enabling teams to\nexchange solutions without implementation-specific limitations.\n  Agent Spec benefits four key groups: (i) Agent developers, who gain access to\na superset of reusable components and design patterns, enabling them to\nleverage a broader range of functionalities; (ii) Agent framework and tool\ndevelopers, who can use Agent Spec as an interchange format and therefore\nbenefit from the support of other frameworks as well as other tools; (iii)\nResearchers, who can achieve reproducible results and comparability,\nfacilitating more reliable and consistent outcomes; (iv) Enterprises, which\nbenefit from faster prototype-to-deployment, increased productivity, as well as\ngreater scalability and maintainability for their AI agent solutions. This\ntechnical report provides an overview of the technical foundations of Agent\nSpec, including motivation, benefits, and future developments.", "AI": {"tldr": "Open Agent Specification (Agent Spec) \u662f\u4e00\u79cd\u58f0\u660e\u5f0f\u8bed\u8a00\uff0c\u7528\u4e8e\u5b9a\u4e49AI\u4ee3\u7406\u53ca\u5176\u5de5\u4f5c\u6d41\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u4e0d\u540cAI\u6846\u67b6\u95f4\u517c\u5bb9\uff0c\u63d0\u5347\u53ef\u79fb\u690d\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "motivation": "\u89e3\u51b3AI\u4ee3\u7406\u5f00\u53d1\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u89c4\u8303\u6807\u51c6\uff0c\u4f7fAI\u4ee3\u7406\u80fd\u591f\u4e00\u6b21\u8bbe\u8ba1\u3001\u8de8\u6846\u67b6\u90e8\u7f72\uff0c\u63d0\u9ad8\u4e92\u64cd\u4f5c\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u51cf\u5c11\u91cd\u590d\u5f00\u53d1\u5de5\u4f5c\u3002", "method": "\u5f00\u53d1\u58f0\u660e\u5f0f\u8bed\u8a00\u89c4\u8303\uff0c\u5141\u8bb8AI\u4ee3\u7406\u72ec\u7acb\u4e8e\u6267\u884c\u73af\u5883\u5b9a\u4e49\uff0c\u652f\u6301\u5f00\u53d1\u5de5\u5177\u548c\u53ef\u79fb\u690d\u6027\uff0c\u4fc3\u8fdb\u56e2\u961f\u95f4\u89e3\u51b3\u65b9\u6848\u4ea4\u6362\u3002", "result": "\u4e3a\u56db\u7c7b\u5173\u952e\u7fa4\u4f53\u5e26\u6765\u76ca\u5904\uff1a\u4ee3\u7406\u5f00\u53d1\u8005\u83b7\u5f97\u53ef\u91cd\u7528\u7ec4\u4ef6\u5e93\uff0c\u6846\u67b6\u5f00\u53d1\u8005\u83b7\u5f97\u4ea4\u6362\u683c\u5f0f\uff0c\u7814\u7a76\u8005\u5b9e\u73b0\u53ef\u590d\u73b0\u7ed3\u679c\uff0c\u4f01\u4e1a\u83b7\u5f97\u66f4\u5feb\u90e8\u7f72\u548c\u66f4\u597d\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Agent Spec \u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6280\u672f\u89c4\u8303\u57fa\u7840\uff0c\u4fc3\u8fdbAI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u672a\u6765\u53d1\u5c55\u524d\u666f\u5e7f\u9614\u3002", "topic": "agent analysis"}}
{"id": "2510.04195", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04195", "abs": "https://arxiv.org/abs/2510.04195", "authors": ["Puzhen Zhang", "Xuyang Chen", "Yu Feng", "Yuhan Jiang", "Liqiu Meng"], "title": "Constructing coherent spatial memory in LLM agents through graph rectification", "comment": null, "summary": "Given a map description through global traversal navigation instructions\n(e.g., visiting each room sequentially with action signals such as north, west,\netc.), an LLM can often infer the implicit spatial layout of the environment\nand answer user queries by providing a shortest path from a start to a\ndestination (for instance, navigating from the lobby to a meeting room via the\nhall and elevator). However, such context-dependent querying becomes incapable\nas the environment grows much longer, motivating the need for incremental map\nconstruction that builds a complete topological graph from stepwise\nobservations. We propose a framework for LLM-driven construction and map\nrepair, designed to detect, localize, and correct structural inconsistencies in\nincrementally constructed navigation graphs. Central to our method is the\nVersion Control, which records the full history of graph edits and their source\nobservations, enabling fine-grained rollback, conflict tracing, and repair\nevaluation. We further introduce an Edge Impact Score to prioritize\nminimal-cost repairs based on structural reachability, path usage, and conflict\npropagation. To properly evaluate our approach, we create a refined version of\nthe MANGO benchmark dataset by systematically removing non-topological actions\nand inherent structural conflicts, providing a cleaner testbed for LLM-driven\nconstruction and map repair. Our approach significantly improves map\ncorrectness and robustness, especially in scenarios with entangled or chained\ninconsistencies. Our results highlight the importance of introspective,\nhistory-aware repair mechanisms for maintaining coherent spatial memory in LLM\nagents.", "AI": {"tldr": "\u63d0\u51faLLM\u9a71\u52a8\u7684\u589e\u91cf\u5730\u56fe\u6784\u5efa\u4e0e\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u7248\u672c\u63a7\u5236\u548c\u8fb9\u5f71\u54cd\u8bc4\u5206\u6765\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u4fee\u6b63\u5bfc\u822a\u56fe\u4e2d\u7684\u7ed3\u6784\u4e0d\u4e00\u81f4\u6027", "motivation": "\u968f\u7740\u73af\u5883\u89c4\u6a21\u6269\u5927\uff0c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u67e5\u8be2\u65b9\u6cd5\u53d8\u5f97\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u589e\u91cf\u5730\u56fe\u6784\u5efa\u6765\u4ece\u9010\u6b65\u89c2\u5bdf\u4e2d\u6784\u5efa\u5b8c\u6574\u62d3\u6251\u56fe", "method": "\u4f7f\u7528\u7248\u672c\u63a7\u5236\u8bb0\u5f55\u56fe\u7f16\u8f91\u5386\u53f2\u548c\u6765\u6e90\u89c2\u5bdf\uff0c\u5f15\u5165\u8fb9\u5f71\u54cd\u8bc4\u5206\u57fa\u4e8e\u7ed3\u6784\u53ef\u8fbe\u6027\u3001\u8def\u5f84\u4f7f\u7528\u548c\u51b2\u7a81\u4f20\u64ad\u6765\u4f18\u5148\u6700\u5c0f\u6210\u672c\u4fee\u590d", "result": "\u5728MANGO\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5730\u56fe\u6b63\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u7ea0\u7f20\u6216\u94fe\u5f0f\u4e0d\u4e00\u81f4\u6027\u7684\u573a\u666f\u4e2d", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5185\u7701\u3001\u5386\u53f2\u611f\u77e5\u7684\u4fee\u590d\u673a\u5236\u5bf9\u4e8e\u7ef4\u62a4LLM\u4ee3\u7406\u4e2d\u8fde\u8d2f\u7a7a\u95f4\u8bb0\u5fc6\u7684\u91cd\u8981\u6027", "topic": "agent analysis"}}
{"id": "2510.04080", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04080", "abs": "https://arxiv.org/abs/2510.04080", "authors": ["Zixin Song", "Bowen Zhang", "Qian-Wen Zhang", "Di Yin", "Xing Sun", "Chunping Li"], "title": "PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity", "comment": null, "summary": "Conditional Semantic Textual Similarity (C-STS) measures the semantic\nproximity between text segments under a specific condition, thereby overcoming\nthe ambiguity inherent in traditional STS. However, existing methods are\nlargely confined to discriminative models, failing to fully integrate recent\nbreakthroughs in the NLP community concerning Large Language Models (LLMs) and\nReinforcement Learning (RL). RL is a particularly well-suited paradigm for this\ntask, as it can directly optimize the non-differentiable Spearman ranking\nmetric and guide the reasoning process required by C-STS. However, we find that\nnaively applying listwise RL fails to produce meaningful improvements, as the\nmodel is overwhelmed by complex, coarse-grained reward signals. To address this\nchallenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning\nframework. PoLi-RL employs a two-stage curriculum: it first trains the model\nwith simple pointwise rewards to establish fundamental scoring capabilities,\nthen transitions to a hybrid reward that combines pointwise, pairwise, and\nlistwise objectives to refine the model's ability to discern subtle semantic\ndistinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward\n(PSRR) mechanism that computes ranking rewards in parallel slices, where each\nslice comprises same-indexed completions from different samples. This provides\na precise, differentiated learning signal for each individual completion,\nenabling granular credit assignment and effective optimization. On the official\nC-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,\nestablishing a new SOTA for the cross-encoder architecture. As the first work\nto successfully apply RL to C-STS, our study introduces a powerful and precise\nparadigm for training LLMs on complex, ranking-based conditional judgment\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86PoLi-RL\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u548c\u5e76\u884c\u5207\u7247\u6392\u540d\u5956\u52b1\u673a\u5236\uff0c\u6210\u529f\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u6761\u4ef6\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u4efb\u52a1\uff0c\u5728C-STS\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709C-STS\u65b9\u6cd5\u5c40\u9650\u4e8e\u5224\u522b\u6a21\u578b\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u548cRL\u7684\u6700\u65b0\u8fdb\u5c55\u3002RL\u7279\u522b\u9002\u5408\u8be5\u4efb\u52a1\uff0c\u53ef\u76f4\u63a5\u4f18\u5316\u4e0d\u53ef\u5fae\u7684Spearman\u6392\u540d\u6307\u6807\u5e76\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "PoLi-RL\u6846\u67b6\uff1a1) \u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff1a\u5148\u7528\u70b9\u5f0f\u5956\u52b1\u8bad\u7ec3\u57fa\u7840\u8bc4\u5206\u80fd\u529b\uff0c\u518d\u7ed3\u5408\u70b9\u5f0f\u3001\u5bf9\u5f0f\u548c\u5217\u5f0f\u76ee\u6807\u7684\u6df7\u5408\u5956\u52b1\uff1b2) \u5e76\u884c\u5207\u7247\u6392\u540d\u5956\u52b1(PSRR)\uff1a\u5728\u5e76\u884c\u5207\u7247\u4e2d\u8ba1\u7b97\u6392\u540d\u5956\u52b1\uff0c\u4e3a\u6bcf\u4e2a\u5b8c\u6210\u63d0\u4f9b\u7cbe\u786e\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u5728\u5b98\u65b9C-STS\u57fa\u51c6\u4e0a\u8fbe\u523048.18\u7684Spearman\u76f8\u5173\u7cfb\u6570\uff0c\u4e3a\u4ea4\u53c9\u7f16\u7801\u5668\u67b6\u6784\u5efa\u7acb\u4e86\u65b0\u7684SOTA\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u6210\u529f\u5c06RL\u5e94\u7528\u4e8eC-STS\u7684\u5de5\u4f5c\uff0c\u4e3a\u5728\u590d\u6742\u3001\u57fa\u4e8e\u6392\u540d\u7684\u6761\u4ef6\u5224\u65ad\u4efb\u52a1\u4e0a\u8bad\u7ec3LLM\u63d0\u4f9b\u4e86\u5f3a\u5927\u800c\u7cbe\u786e\u7684\u8303\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04081", "categories": ["cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.04081", "abs": "https://arxiv.org/abs/2510.04081", "authors": ["Honglin Lin", "Qizhi Pei", "Xin Gao", "Zhuoshi Pan", "Yu Li", "Juntao Li", "Conghui He", "Lijun Wu"], "title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning", "comment": "Accepted by NeurIPS2025", "summary": "Reasoning capability is pivotal for Large Language Models (LLMs) to solve\ncomplex tasks, yet achieving reliable and scalable reasoning remains\nchallenging. While Chain-of-Thought (CoT) prompting has become a mainstream\napproach, existing methods often suffer from uncontrolled generation,\ninsufficient quality, and limited diversity in reasoning paths. Recent efforts\nleverage code to enhance CoT by grounding reasoning in executable steps, but\nsuch methods are typically constrained to predefined mathematical problems,\nhindering scalability and generalizability. In this work, we propose Caco\n(Code-Assisted Chain-of-ThOught), a novel framework that automates the\nsynthesis of high-quality, verifiable, and diverse instruction-CoT reasoning\ndata through code-driven augmentation. Unlike prior work, Caco first fine-tunes\na code-based CoT generator on existing math and programming solutions in a\nunified code format, then scales the data generation to a large amount of\ndiverse reasoning traces. Crucially, we introduce automated validation via code\nexecution and rule-based filtering to ensure logical correctness and structural\ndiversity, followed by reverse-engineering filtered outputs into natural\nlanguage instructions and language CoTs to enrich task adaptability. This\nclosed-loop process enables fully automated, scalable synthesis of reasoning\ndata with guaranteed executability. Experiments on our created Caco-1.3M\ndataset demonstrate that Caco-trained models achieve strong competitive\nperformance on mathematical reasoning benchmarks, outperforming existing strong\nbaselines. Further analysis reveals that Caco's code-anchored verification and\ninstruction diversity contribute to superior generalization across unseen\ntasks. Our work establishes a paradigm for building self-sustaining,\ntrustworthy reasoning systems without human intervention.", "AI": {"tldr": "Caco\u6846\u67b6\u901a\u8fc7\u4ee3\u7801\u9a71\u52a8\u7684\u589e\u5f3a\u65b9\u6cd5\u81ea\u52a8\u5408\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u9a8c\u8bc1\u4e14\u591a\u6837\u5316\u7684\u6307\u4ee4-CoT\u63a8\u7406\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709CoT\u65b9\u6cd5\u5728\u751f\u6210\u63a7\u5236\u3001\u8d28\u91cf\u548c\u591a\u6837\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709CoT\u65b9\u6cd5\u5b58\u5728\u751f\u6210\u4e0d\u53ef\u63a7\u3001\u8d28\u91cf\u4e0d\u8db3\u548c\u63a8\u7406\u8def\u5f84\u591a\u6837\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u4ee3\u7801\u7684CoT\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u7684\u6570\u5b66\u95ee\u9898\uff0c\u963b\u788d\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "\u9996\u5148\u5728\u7edf\u4e00\u4ee3\u7801\u683c\u5f0f\u4e0b\u5bf9\u57fa\u4e8e\u4ee3\u7801\u7684CoT\u751f\u6210\u5668\u8fdb\u884c\u5fae\u8c03\uff0c\u7136\u540e\u6269\u5c55\u5230\u5927\u91cf\u591a\u6837\u5316\u63a8\u7406\u8f68\u8ff9\u7684\u751f\u6210\uff0c\u901a\u8fc7\u4ee3\u7801\u6267\u884c\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u8fc7\u6ee4\u8fdb\u884c\u81ea\u52a8\u9a8c\u8bc1\uff0c\u6700\u540e\u5c06\u8fc7\u6ee4\u540e\u7684\u8f93\u51fa\u53cd\u5411\u5de5\u7a0b\u4e3a\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u8bed\u8a00CoT\u3002", "result": "\u5728\u521b\u5efa\u7684Caco-1.3M\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCaco\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7ade\u4e89\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u5f3a\u57fa\u7ebf\u3002", "conclusion": "Caco\u5efa\u7acb\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u6784\u5efa\u81ea\u6301\u7eed\u3001\u53ef\u4fe1\u8d56\u63a8\u7406\u7cfb\u7edf\u7684\u8303\u5f0f\uff0c\u5176\u4ee3\u7801\u951a\u5b9a\u9a8c\u8bc1\u548c\u6307\u4ee4\u591a\u6837\u6027\u6709\u52a9\u4e8e\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2510.04206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04206", "abs": "https://arxiv.org/abs/2510.04206", "authors": ["Hanchen Zhang", "Xiao Liu", "Bowen Lv", "Xueqiao Sun", "Bohao Jing", "Iat Long Iong", "Zhenyu Hou", "Zehan Qi", "Hanyu Lai", "Yifan Xu", "Rui Lu", "Hongning Wang", "Jie Tang", "Yuxiao Dong"], "title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework", "comment": null, "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building generalist agents that can learn through online interactions.\nHowever, applying reinforcement learning (RL) to train LLM agents in\nmulti-turn, multi-task settings remains challenging due to lack of scalable\ninfrastructure and stable training algorithms. In this work, we present the\nAgentRL framework for scalable multi-turn, multi-task agentic RL training. On\nthe infrastructure side, AgentRL features a fully-asynchronous\ngeneration-training pipeline for efficient multi-turn RL. To support\nheterogeneous environment development in multi-task RL, we design a unified\nfunction-call based API interface, containerized environment development, and a\ncentralized controller. On the algorithm side, we propose cross-policy sampling\nto encourage model exploration in multi-turn settings and task advantage\nnormalization to stabilize multi-task training. Experiments show that AgentRL,\ntrained on open LLMs across five agentic tasks, significantly outperforms\nGPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.\nMulti-task training with AgentRL matches the best results among all\ntask-specific models. AgentRL is open-sourced at\nhttps://github.com/THUDM/AgentRL. The algorithm and framework are adopted in\nbuilding \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgentRL\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u591a\u8f6e\u591a\u4efb\u52a1\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5305\u542b\u5f02\u6b65\u751f\u6210-\u8bad\u7ec3\u6d41\u6c34\u7ebf\u548c\u7a33\u5b9a\u7b97\u6cd5\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\u548c\u7a33\u5b9a\u7684\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5728\u591a\u8f6e\u591a\u4efb\u52a1\u73af\u5883\u4e2d\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u667a\u80fd\u4f53\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u57fa\u7840\u8bbe\u65bd\u65b9\u9762\uff1a\u5b8c\u5168\u5f02\u6b65\u7684\u751f\u6210-\u8bad\u7ec3\u6d41\u6c34\u7ebf\u3001\u7edf\u4e00\u7684\u51fd\u6570\u8c03\u7528API\u63a5\u53e3\u3001\u5bb9\u5668\u5316\u73af\u5883\u5f00\u53d1\u548c\u96c6\u4e2d\u63a7\u5236\u5668\uff1b\u7b97\u6cd5\u65b9\u9762\uff1a\u8de8\u7b56\u7565\u91c7\u6837\u9f13\u52b1\u63a2\u7d22\u3001\u4efb\u52a1\u4f18\u52bf\u5f52\u4e00\u5316\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u4e94\u4e2a\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\uff0cAgentRL\u663e\u8457\u4f18\u4e8eGPT-5\u3001Clause-Sonnet-4\u3001DeepSeek-R1\u7b49\u6a21\u578b\uff0c\u591a\u4efb\u52a1\u8bad\u7ec3\u7ed3\u679c\u4e0e\u6240\u6709\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u7684\u6700\u4f73\u7ed3\u679c\u76f8\u5f53\u3002", "conclusion": "AgentRL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u591a\u4efb\u52a1\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04272", "categories": ["cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.04272", "abs": "https://arxiv.org/abs/2510.04272", "authors": ["Jinyang Jiang", "Jinhui Han", "Yijie Peng", "Ying Zhang"], "title": "Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales", "comment": null, "summary": "Effective cross-functional coordination is essential for enhancing firm-wide\nprofitability, particularly in the face of growing organizational complexity\nand scale. Recent advances in artificial intelligence, especially in\nreinforcement learning (RL), offer promising avenues to address this\nfundamental challenge. This paper proposes a unified multi-agent RL framework\ntailored for joint optimization across distinct functional modules, exemplified\nvia coordinating inventory replenishment and personalized product\nrecommendation. We first develop an integrated theoretical model to capture the\nintricate interplay between these functions and derive analytical benchmarks\nthat characterize optimal coordination. The analysis reveals synchronized\nadjustment patterns across products and over time, highlighting the importance\nof coordinated decision-making. Leveraging these insights, we design a novel\nmulti-timescale multi-agent RL architecture that decomposes policy components\naccording to departmental functions and assigns distinct learning speeds based\non task complexity and responsiveness. Our model-free multi-agent design\nimproves scalability and deployment flexibility, while multi-timescale updates\nenhance convergence stability and adaptability across heterogeneous decisions.\nWe further establish the asymptotic convergence of the proposed algorithm.\nExtensive simulation experiments demonstrate that the proposed approach\nsignificantly improves profitability relative to siloed decision-making\nframeworks, while the behaviors of the trained RL agents align closely with the\nmanagerial insights from our theoretical model. Taken together, this work\nprovides a scalable, interpretable RL-based solution to enable effective\ncross-functional coordination in complex business settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u529f\u80fd\u6a21\u5757\u7684\u8054\u5408\u4f18\u5316\uff0c\u7279\u522b\u5173\u6ce8\u5e93\u5b58\u8865\u8d27\u548c\u4e2a\u6027\u5316\u4ea7\u54c1\u63a8\u8350\u7684\u534f\u8c03\u95ee\u9898\u3002", "motivation": "\u7ec4\u7ec7\u590d\u6742\u6027\u548c\u89c4\u6a21\u589e\u957f\u4f7f\u5f97\u8de8\u529f\u80fd\u534f\u8c03\u5bf9\u63d0\u9ad8\u4f01\u4e1a\u76c8\u5229\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4eba\u5de5\u667a\u80fd\u7279\u522b\u662f\u5f3a\u5316\u5b66\u4e60\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002", "method": "\u5f00\u53d1\u4e86\u96c6\u6210\u7406\u8bba\u6a21\u578b\u6355\u6349\u529f\u80fd\u95f4\u590d\u6742\u4ea4\u4e92\uff0c\u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u591a\u65f6\u95f4\u5c3a\u5ea6\u591a\u667a\u80fd\u4f53RL\u67b6\u6784\uff0c\u6309\u90e8\u95e8\u529f\u80fd\u5206\u89e3\u7b56\u7565\u7ec4\u4ef6\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u590d\u6742\u6027\u548c\u54cd\u5e94\u6027\u5206\u914d\u4e0d\u540c\u5b66\u4e60\u901f\u5ea6\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5b64\u5c9b\u51b3\u7b56\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u76c8\u5229\u80fd\u529b\uff0c\u8bad\u7ec3\u540e\u7684RL\u667a\u80fd\u4f53\u884c\u4e3a\u4e0e\u7406\u8bba\u6a21\u578b\u7684\u7ba1\u7406\u6d1e\u5bdf\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u590d\u6742\u5546\u4e1a\u73af\u5883\u4e2d\u5b9e\u73b0\u6709\u6548\u8de8\u529f\u80fd\u534f\u8c03\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u57fa\u4e8eRL\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04182", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04182", "abs": "https://arxiv.org/abs/2510.04182", "authors": ["Wengao Ye", "Yan Liang", "Lianlei Shan"], "title": "Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have shifted from\nexplicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,\nwhere intermediate thoughts are represented as vectors rather than text.\nHowever, latent reasoning can be brittle on challenging, out-of-distribution\ntasks where robust reasoning is most critical. To overcome these limitations,\nwe introduce Latent Thought Policy Optimization (LTPO), a parameter-free\nframework that enhances LLM reasoning entirely at test time, without requiring\nmodel parameter updates. LTPO treats intermediate latent \"thought\" vectors as\ndynamic parameters that are actively optimized for each problem instance. It\nemploys an online policy gradient method guided by an intrinsic,\nconfidence-based reward signal computed directly from the frozen LLM's own\noutput distributions, eliminating the need for external supervision or\nexpensive text generation during optimization. Extensive experiments on five\nreasoning benchmarks show that LTPO not only matches or surpasses strong\nbaselines on standard tasks but also demonstrates remarkable robustness where\nothers fail. Most notably, on highly challenging AIME benchmarks where existing\nlatent reasoning baselines collapse to near-zero accuracy, LTPO delivers\nsubstantial improvements, showcasing a unique capability for complex reasoning.", "AI": {"tldr": "LTPO\u662f\u4e00\u79cd\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e2d\u95f4\u6f5c\u5728\u601d\u60f3\u5411\u91cf\u4f5c\u4e3a\u52a8\u6001\u53c2\u6570\u8fdb\u884c\u4f18\u5316\uff0c\u63d0\u5347LLM\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u6f5c\u5728\u63a8\u7406\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u8868\u73b0\u8106\u5f31\u7684\u95ee\u9898\uff0c\u5728\u9700\u8981\u7a33\u5065\u63a8\u7406\u7684\u5173\u952e\u573a\u666f\u4e2d\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5728\u7ebf\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u57fa\u4e8e\u51bb\u7ed3LLM\u81ea\u8eab\u8f93\u51fa\u5206\u5e03\u8ba1\u7b97\u7684\u7f6e\u4fe1\u5ea6\u5956\u52b1\u4fe1\u53f7\uff0c\u4f18\u5316\u4e2d\u95f4\u6f5c\u5728\u601d\u60f3\u5411\u91cf\u3002", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLTPO\u4e0d\u4ec5\u8fbe\u5230\u6216\u8d85\u8fc7\u5f3a\u57fa\u7ebf\uff0c\u5728\u6807\u51c6\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u6781\u5177\u6311\u6218\u6027\u7684AIME\u57fa\u51c6\u4e0a\u66f4\u662f\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u800c\u73b0\u6709\u6f5c\u5728\u63a8\u7406\u57fa\u7ebf\u51c6\u786e\u7387\u63a5\u8fd1\u96f6\u3002", "conclusion": "LTPO\u5c55\u793a\u4e86\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u72ec\u7279\u80fd\u529b\uff0c\u4e3a\u65e0\u9700\u5916\u90e8\u76d1\u7763\u6216\u6602\u8d35\u6587\u672c\u751f\u6210\u7684\u6d4b\u8bd5\u65f6\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04371", "categories": ["cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04371", "abs": "https://arxiv.org/abs/2510.04371", "authors": ["Naimeng Ye", "Arnav Ahuja", "Georgios Liargkovas", "Yunan Lu", "Kostis Kaffes", "Tianyi Peng"], "title": "Speculative Actions: A Lossless Framework for Faster Agentic Systems", "comment": null, "summary": "Despite growing interest in AI agents across industry and academia, their\nexecution in an environment is often slow, hampering training, evaluation, and\ndeployment. For example, a game of chess between two state-of-the-art agents\nmay take hours. A critical bottleneck is that agent behavior unfolds\nsequentially: each action requires an API call, and these calls can be\ntime-consuming. Inspired by speculative execution in microprocessors and\nspeculative decoding in LLM inference, we propose speculative actions, a\nlossless framework for general agentic systems that predicts likely actions\nusing faster models, enabling multiple steps to be executed in parallel. We\nevaluate this framework across three agentic environments: gaming, e-commerce,\nweb search, and a \"lossy\" extension for an operating systems environment. In\nall cases, speculative actions achieve substantial accuracy in next-action\nprediction (up to 55%), translating into significant reductions in end-to-end\nlatency. Moreover, performance can be further improved through stronger\nguessing models, top-K action prediction, multi-step speculation, and\nuncertainty-aware optimization, opening a promising path toward deploying\nlow-latency agentic systems in the real world.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u63a8\u6d4b\u52a8\u4f5c\"\u7684\u65e0\u635f\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u66f4\u5feb\u7684\u6a21\u578b\u9884\u6d4b\u53ef\u80fd\u7684\u52a8\u4f5c\uff0c\u4f7f\u591a\u4e2a\u6b65\u9aa4\u80fd\u591f\u5e76\u884c\u6267\u884c\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524dAI\u667a\u80fd\u4f53\u5728\u73af\u5883\u4e2d\u6267\u884c\u901f\u5ea6\u7f13\u6162\uff0c\u963b\u788d\u4e86\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u90e8\u7f72\u3002\u4f8b\u5982\uff0c\u4e24\u4e2a\u6700\u5148\u8fdb\u7684\u56fd\u9645\u8c61\u68cb\u667a\u80fd\u4f53\u5bf9\u5f08\u53ef\u80fd\u9700\u8981\u6570\u5c0f\u65f6\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u667a\u80fd\u4f53\u884c\u4e3a\u662f\u987a\u5e8f\u5c55\u5f00\u7684\uff0c\u6bcf\u4e2a\u52a8\u4f5c\u90fd\u9700\u8981API\u8c03\u7528\u4e14\u8017\u65f6\u3002", "method": "\u53d7\u5fae\u5904\u7406\u5668\u4e2d\u7684\u63a8\u6d4b\u6267\u884c\u548cLLM\u63a8\u7406\u4e2d\u7684\u63a8\u6d4b\u89e3\u7801\u542f\u53d1\uff0c\u4f7f\u7528\u66f4\u5feb\u7684\u6a21\u578b\u9884\u6d4b\u53ef\u80fd\u7684\u52a8\u4f5c\uff0c\u5b9e\u73b0\u591a\u6b65\u9aa4\u5e76\u884c\u6267\u884c\u3002\u8bc4\u4f30\u4e86\u6e38\u620f\u3001\u7535\u5b50\u5546\u52a1\u3001\u7f51\u7edc\u641c\u7d22\u7b49\u73af\u5883\uff0c\u5e76\u6269\u5c55\u4e86\u64cd\u4f5c\u7cfb\u7edf\u7684\"\u6709\u635f\"\u7248\u672c\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u73af\u5883\u4e2d\uff0c\u63a8\u6d4b\u52a8\u4f5c\u5728\u4e0b\u4e00\u52a8\u4f5c\u9884\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u51c6\u786e\u7387\uff08\u6700\u9ad8\u8fbe55%\uff09\uff0c\u8f6c\u5316\u4e3a\u7aef\u5230\u7aef\u5ef6\u8fdf\u7684\u663e\u8457\u964d\u4f4e\u3002\u901a\u8fc7\u66f4\u5f3a\u7684\u731c\u6d4b\u6a21\u578b\u3001top-K\u52a8\u4f5c\u9884\u6d4b\u3001\u591a\u6b65\u63a8\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4f18\u5316\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u63a8\u6d4b\u52a8\u4f5c\u4e3a\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72\u4f4e\u5ef6\u8fdf\u667a\u80fd\u4f53\u7cfb\u7edf\u5f00\u8f9f\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.04214", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04214", "abs": "https://arxiv.org/abs/2510.04214", "authors": ["Zhuoran Zhuang", "Ye Chen", "Xia Zeng", "Chao Luo", "Luhui Liu", "Yihan Chen"], "title": "Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards", "comment": null, "summary": "We study deploying large language models (LLMs) as business development (BD)\nagents for persuasive price negotiation in online travel agencies (OTAs), where\naligning traveler affordability and hotel profitability directly affects\nbookings, partner relationships, and access to travel. The agent must follow a\nStandard Operating Procedure (SOP) while conducting multi-turn persuasion,\ninterpreting colloquial inputs, and adhering to guardrails (no over-promising,\nno hallucinations). Conventional post-training -- supervised fine-tuning (SFT)\nor single-source reward optimization -- overfits scripts, misses nuanced\npersuasive style, and fails to enforce verifiable business constraints.\n  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement\nlearning post-training framework that aligns an LLM with heterogeneous rewards:\na preference-trained reward model (RM) for dense human alignment, a reward\njudge (RJ) for high-level persuasive behavior and SOP compliance, and\nprogrammatic reward functions (RF) for deterministic checks on numerics,\nformatting, and guardrails. A straightforward enhancement mechanism is proposed\nto combine the RM with RJ and RF signals to curb reward hacking and improve\nnegotiation quality. In production-style evaluations -- approximately 150 turns\nfrom real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts\naverage dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference\nOptimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),\nincreases the share of conversations with at least one excellent response to\n66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix\nrate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also\nobserve emergent capabilities -- proactive empathy, localized reasoning,\ncalibrated tactics -- that surpass gold annotations.", "AI": {"tldr": "\u63d0\u51fa\u4e86REPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u504f\u597d\u5956\u52b1\u6a21\u578b\u3001\u8bf4\u670d\u884c\u4e3a\u5956\u52b1\u548c\u7a0b\u5e8f\u5316\u5956\u52b1\uff0c\u4f18\u5316LLM\u5728\u5728\u7ebf\u65c5\u884c\u673a\u6784\u4ef7\u683c\u8c08\u5224\u4e2d\u7684\u8868\u73b0\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u8d28\u91cf\u548c\u7ea6\u675f\u9075\u5b88\u3002", "motivation": "\u4f20\u7edf\u540e\u8bad\u7ec3\u65b9\u6cd5\u5728\u5546\u4e1a\u8c08\u5224\u573a\u666f\u4e2d\u5bb9\u6613\u8fc7\u62df\u5408\u811a\u672c\uff0c\u65e0\u6cd5\u6355\u6349\u7ec6\u5fae\u7684\u8bf4\u670d\u98ce\u683c\uff0c\u4e14\u96be\u4ee5\u5f3a\u5236\u6267\u884c\u53ef\u9a8c\u8bc1\u7684\u4e1a\u52a1\u7ea6\u675f\u3002", "method": "\u4f7f\u7528Reward-Enhanced Policy Optimization (REPO)\u6846\u67b6\uff0c\u7ed3\u5408\u504f\u597d\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u3001\u8bf4\u670d\u884c\u4e3a\u5956\u52b1\u5224\u65ad\u5668\u548c\u7a0b\u5e8f\u5316\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50LLM\u4e0e\u5f02\u6784\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728\u771f\u5b9e\u5bf9\u8bdd\u8bc4\u4f30\u4e2d\uff0cREPO\u5c06\u5e73\u5747\u5bf9\u8bdd\u8bc4\u5206\u63d0\u5347\u81f34.63\uff0c\u6bd4\u57fa\u51c6\u9ad81.20\u5206\uff1b66.67%\u7684\u5bf9\u8bdd\u81f3\u5c11\u6709\u4e00\u4e2a\u4f18\u79c0\u56de\u590d\uff0c\u6bd4GRPO\u9ad823.34\u4e2a\u767e\u5206\u70b9\uff1b93.33%\u7684\u574f\u6848\u4f8b\u4fee\u590d\u7387\u3002", "conclusion": "REPO\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u5546\u4e1a\u8c08\u5224\u4e2d\u7684\u8868\u73b0\uff0c\u4ea7\u751f\u4e86\u8d85\u8d8a\u9ec4\u91d1\u6807\u6ce8\u7684\u65b0\u5174\u80fd\u529b\uff0c\u5982\u4e3b\u52a8\u540c\u7406\u5fc3\u3001\u5c40\u90e8\u63a8\u7406\u548c\u6821\u51c6\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04373", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04373", "abs": "https://arxiv.org/abs/2510.04373", "authors": ["Hadi Nekoei", "Aman Jaiswal", "Patrice Bechard", "Oleh Shliazhko", "Orlando Marquez Ayala", "Mathieu Reymond", "Massimo Caccia", "Alexandre Drouin", "Sarath Chandar", "Alexandre Lacoste"], "title": "Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation", "comment": null, "summary": "Large language model (LLM) agents perform well in sequential decision-making\ntasks, but improving them on unfamiliar domains often requires costly online\ninteractions or fine-tuning on large expert datasets. These strategies are\nimpractical for closed-source models and expensive for open-source ones, with\nrisks of catastrophic forgetting. Offline trajectories offer reusable\nknowledge, yet demonstration-based methods struggle because raw traces are\nlong, noisy, and tied to specific tasks. We present Just-in-time Episodic\nFeedback Hinter (JEF Hinter), an agentic system that distills offline traces\ninto compact, context-aware hints. A zooming mechanism highlights decisive\nsteps in long trajectories, capturing both strategies and pitfalls. Unlike\nprior methods, JEF Hinter leverages both successful and failed trajectories,\nextracting guidance even when only failure data is available, while supporting\nparallelized hint generation and benchmark-independent prompting. At inference,\na retriever selects relevant hints for the current state, providing targeted\nguidance with transparency and traceability. Experiments on MiniWoB++,\nWorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms\nstrong baselines, including human- and document-based hints.", "AI": {"tldr": "JEF Hinter\u662f\u4e00\u4e2a\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u79bb\u7ebf\u8f68\u8ff9\u63d0\u70bc\u4e3a\u7d27\u51d1\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63d0\u793a\u6765\u6539\u8fdbLLM\u4ee3\u7406\u5728\u964c\u751f\u9886\u57df\u7684\u8868\u73b0\uff0c\u65e0\u9700\u5728\u7ebf\u4ea4\u4e92\u6216\u5fae\u8c03\u3002", "motivation": "\u6539\u8fdbLLM\u4ee3\u7406\u5728\u964c\u751f\u9886\u57df\u7684\u6027\u80fd\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u5728\u7ebf\u4ea4\u4e92\u6216\u5927\u89c4\u6a21\u4e13\u5bb6\u6570\u636e\u96c6\u5fae\u8c03\uff0c\u8fd9\u5bf9\u95ed\u6e90\u6a21\u578b\u4e0d\u5b9e\u7528\u4e14\u5bf9\u5f00\u6e90\u6a21\u578b\u6210\u672c\u9ad8\u6602\uff0c\u8fd8\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u98ce\u9669\u3002", "method": "\u63d0\u51faJEF Hinter\u7cfb\u7edf\uff0c\u4f7f\u7528\u7f29\u653e\u673a\u5236\u4ece\u957f\u8f68\u8ff9\u4e2d\u63d0\u53d6\u5173\u952e\u6b65\u9aa4\uff0c\u5c06\u79bb\u7ebf\u8f68\u8ff9\uff08\u5305\u62ec\u6210\u529f\u548c\u5931\u8d25\u8f68\u8ff9\uff09\u8f6c\u5316\u4e3a\u7d27\u51d1\u63d0\u793a\uff0c\u901a\u8fc7\u68c0\u7d22\u5668\u5728\u63a8\u7406\u65f6\u9009\u62e9\u76f8\u5173\u63d0\u793a\u63d0\u4f9b\u9488\u5bf9\u6027\u6307\u5bfc\u3002", "result": "\u5728MiniWoB++\u3001WorkArena-L1\u548cWebArena-Lite\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cJEF Hinter\u6301\u7eed\u4f18\u4e8e\u5305\u62ec\u57fa\u4e8e\u4eba\u7c7b\u548c\u6587\u6863\u63d0\u793a\u5728\u5185\u7684\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "JEF Hinter\u80fd\u591f\u6709\u6548\u5229\u7528\u79bb\u7ebf\u8f68\u8ff9\u77e5\u8bc6\uff0c\u4e3aLLM\u4ee3\u7406\u63d0\u4f9b\u900f\u660e\u4e14\u53ef\u8ffd\u6eaf\u7684\u6307\u5bfc\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "topic": "agent analysis"}}
{"id": "2510.04226", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04226", "abs": "https://arxiv.org/abs/2510.04226", "authors": ["Dustin Wright", "Sarah Masud", "Jared Moore", "Srishti Yadav", "Maria Antoniak", "Chan Young Park", "Isabelle Augenstein"], "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models", "comment": "16 pages; 8 figures, 4 tables", "summary": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u91cfLLM\u8ba4\u77e5\u591a\u6837\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\uff1a\u867d\u7136\u65b0\u6a21\u578b\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u58f0\u660e\uff0c\u4f46\u51e0\u4e4e\u6240\u6709\u6a21\u578b\u7684\u8ba4\u77e5\u591a\u6837\u6027\u90fd\u4f4e\u4e8e\u57fa\u7840\u7f51\u7edc\u641c\u7d22\uff1b\u6a21\u578b\u5927\u5c0f\u5bf9\u8ba4\u77e5\u591a\u6837\u6027\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u800c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6709\u6b63\u9762\u5f71\u54cd\uff1b\u82f1\u8bed\u8bed\u8a00\u5728\u56fd\u522b\u58f0\u660e\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002", "motivation": "LLM\u503e\u5411\u4e8e\u751f\u6210\u540c\u8d28\u5316\u6587\u672c\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u77e5\u8bc6\u5d29\u6e83\u98ce\u9669\uff0c\u5373\u540c\u8d28\u5316\u7684LLM\u4f1a\u968f\u65f6\u95f4\u63a8\u79fb\u7f29\u5c0f\u53ef\u8bbf\u95ee\u4fe1\u606f\u7684\u8303\u56f4\u3002\u73b0\u6709\u7814\u7a76\u5c40\u9650\u4e8e\u5c01\u95ed\u5f0f\u9009\u62e9\u9898\u8bbe\u7f6e\u6216\u6a21\u7cca\u8bed\u4e49\u7279\u5f81\uff0c\u4e14\u672a\u5173\u6ce8\u8de8\u65f6\u95f4\u548c\u6587\u5316\u80cc\u666f\u7684\u8d8b\u52bf\u3002", "method": "\u63d0\u51fa\u6d4b\u91cf\u8ba4\u77e5\u591a\u6837\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u5bf927\u4e2aLLM\u3001155\u4e2a\u6db5\u76d612\u4e2a\u56fd\u5bb6\u7684\u4e3b\u9898\u3001200\u4e2a\u6765\u81ea\u771f\u5b9e\u7528\u6237\u804a\u5929\u7684\u63d0\u793a\u53d8\u4f53\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u8f83\u65b0\u6a21\u578b\u503e\u5411\u4e8e\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u58f0\u660e\uff0c\u4f46\u51e0\u4e4e\u6240\u6709\u6a21\u578b\u7684\u8ba4\u77e5\u591a\u6837\u6027\u90fd\u4f4e\u4e8e\u57fa\u7840\u7f51\u7edc\u641c\u7d22\uff1b\u6a21\u578b\u5927\u5c0f\u5bf9\u8ba4\u77e5\u591a\u6837\u6027\u6709\u8d1f\u9762\u5f71\u54cd\uff0cRAG\u6709\u6b63\u9762\u5f71\u54cd\uff0c\u4f46RAG\u7684\u6539\u5584\u6548\u679c\u56e0\u6587\u5316\u80cc\u666f\u800c\u5f02\uff1b\u56fd\u522b\u58f0\u660e\u66f4\u591a\u53cd\u6620\u82f1\u8bed\u8bed\u8a00\u800c\u975e\u5f53\u5730\u8bed\u8a00\u3002", "conclusion": "LLM\u5b58\u5728\u8ba4\u77e5\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8de8\u6587\u5316\u8bed\u5883\u4e2d\uff0c\u82f1\u8bed\u8bed\u8a00\u5728\u77e5\u8bc6\u8868\u793a\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u63ed\u793a\u4e86\u8ba4\u77e5\u8868\u793a\u65b9\u9762\u7684\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2510.04391", "categories": ["cs.AI", "cs.CL", "cs.SI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.04391", "abs": "https://arxiv.org/abs/2510.04391", "authors": ["Saurabh Ranjan", "Brian Odegaard"], "title": "Internal World Models as Imagination Networks in Cognitive Agents", "comment": null, "summary": "What is the computational objective of imagination? While classical\ninterpretations suggest imagination is useful for maximizing rewards, recent\nfindings challenge this view. In this study, we propose that imagination serves\nto access an internal world model (IWM) and use psychological network analysis\nto explore IWMs in humans and large language models (LLMs). Specifically, we\nassessed imagination vividness ratings using two questionnaires and constructed\nimagination networks from these reports. Imagination networks from human groups\nshowed correlations between different centrality measures, including expected\ninfluence, strength, and closeness. However, imagination networks from LLMs\nshowed a lack of clustering and lower correlations between centrality measures\nunder different prompts and conversational memory conditions. Together, these\nresults indicate a lack of similarity between IWMs in human and LLM agents.\nOverall, our study offers a novel method for comparing internally-generated\nrepresentations in humans and AI, providing insights for developing human-like\nimagination in artificial intelligence.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5fc3\u7406\u7f51\u7edc\u5206\u6790\u6bd4\u8f83\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u60f3\u8c61\u7f51\u7edc\u7ed3\u6784\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u63a2\u7d22\u60f3\u8c61\u7684\u8ba1\u7b97\u76ee\u6807\uff0c\u6311\u6218\u4f20\u7edf\u8ba4\u4e3a\u60f3\u8c61\u4ec5\u7528\u4e8e\u6700\u5927\u5316\u5956\u52b1\u7684\u89c2\u70b9\uff0c\u63d0\u51fa\u60f3\u8c61\u7528\u4e8e\u8bbf\u95ee\u5185\u90e8\u4e16\u754c\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u5fc3\u7406\u7f51\u7edc\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u4efd\u95ee\u5377\u8bc4\u4f30\u60f3\u8c61\u751f\u52a8\u6027\u8bc4\u5206\uff0c\u4ece\u62a5\u544a\u4e2d\u6784\u5efa\u60f3\u8c61\u7f51\u7edc\uff0c\u6bd4\u8f83\u4eba\u7c7b\u548cLLM\u7684\u60f3\u8c61\u7f51\u7edc\u7ed3\u6784\u3002", "result": "\u4eba\u7c7b\u60f3\u8c61\u7f51\u7edc\u663e\u793a\u4e0d\u540c\u4e2d\u5fc3\u6027\u5ea6\u91cf\u4e4b\u95f4\u5b58\u5728\u76f8\u5173\u6027\uff0c\u800cLLM\u7684\u60f3\u8c61\u7f51\u7edc\u7f3a\u4e4f\u805a\u7c7b\u4e14\u4e2d\u5fc3\u6027\u5ea6\u91cf\u76f8\u5173\u6027\u8f83\u4f4e\uff0c\u8868\u660e\u4e24\u8005\u5185\u90e8\u4e16\u754c\u6a21\u578b\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6bd4\u8f83\u4eba\u7c7b\u548cAI\u5185\u90e8\u751f\u6210\u8868\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u4e3a\u5f00\u53d1\u7c7b\u4eba\u60f3\u8c61\u7684\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.03340", "categories": ["cs.LG", "cs.AI", "cs.CY", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2510.03340", "abs": "https://arxiv.org/abs/2510.03340", "authors": ["Marian Chen", "Miri Zilka"], "title": "Learning Pareto-Optimal Pandemic Intervention Policies with MORL", "comment": null, "summary": "The COVID-19 pandemic underscored a critical need for intervention strategies\nthat balance disease containment with socioeconomic stability. We approach this\nchallenge by designing a framework for modeling and evaluating disease-spread\nprevention strategies. Our framework leverages multi-objective reinforcement\nlearning (MORL) - a formulation necessitated by competing objectives - combined\nwith a new stochastic differential equation (SDE) pandemic simulator,\ncalibrated and validated against global COVID-19 data. Our simulator reproduces\nnational-scale pandemic dynamics with orders of magnitude higher fidelity than\nother models commonly used in reinforcement learning (RL) approaches to\npandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on\nthis simulator, we illustrate the direct policy trade-offs between\nepidemiological control and economic stability for COVID-19. Furthermore, we\ndemonstrate the framework's generality by extending it to pathogens with\ndifferent epidemiological profiles, such as polio and influenza, and show how\nthese profiles lead the agent to discover fundamentally different intervention\npolicies. To ground our work in contemporary policymaking challenges, we apply\nthe model to measles outbreaks, quantifying how a modest 5% drop in vaccination\ncoverage necessitates significantly more stringent and costly interventions to\ncurb disease spread. This work provides a robust and adaptable framework to\nsupport transparent, evidence-based policymaking for mitigating public health\ncrises.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7684\u6d41\u884c\u75c5\u5e72\u9884\u6846\u67b6\uff0c\u4f7f\u7528\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6a21\u62df\u5668\u51c6\u786e\u518d\u73b0\u75ab\u60c5\u52a8\u6001\uff0c\u5e76\u5728COVID-19\u3001\u810a\u9ad3\u7070\u8d28\u708e\u3001\u6d41\u611f\u548c\u9ebb\u75b9\u7b49\u4e0d\u540c\u75c5\u539f\u4f53\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "COVID-19\u5927\u6d41\u884c\u63ed\u793a\u4e86\u5728\u75be\u75c5\u63a7\u5236\u548c\u793e\u4f1a\u7ecf\u6d4e\u7a33\u5b9a\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u8feb\u5207\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u7ade\u4e89\u76ee\u6807\u7684\u5e72\u9884\u7b56\u7565\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08MORL\uff09\u548c\u65b0\u7684\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u75ab\u60c5\u6a21\u62df\u5668\uff0c\u8bad\u7ec3Pareto\u6761\u4ef6\u7f51\u7edc\uff08PCN\uff09\u4ee3\u7406\u6765\u53d1\u73b0\u6700\u4f18\u5e72\u9884\u7b56\u7565\u3002", "result": "\u6a21\u62df\u5668\u6bd4\u5176\u4ed6\u5e38\u7528\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\uff0c\u80fd\u591f\u91cf\u5316\u6d41\u884c\u75c5\u63a7\u5236\u4e0e\u7ecf\u6d4e\u7a33\u5b9a\u4e4b\u95f4\u7684\u653f\u7b56\u6743\u8861\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u75c5\u539f\u4f53\u9700\u8981\u6839\u672c\u4e0d\u540c\u7684\u5e72\u9884\u7b56\u7565\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6846\u67b6\uff0c\u652f\u6301\u900f\u660e\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u516c\u5171\u536b\u751f\u5371\u673a\u7f13\u89e3\u653f\u7b56\u5236\u5b9a\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04488", "categories": ["cs.AI", "cs.IT", "math.IT", "I.2.4"], "pdf": "https://arxiv.org/pdf/2510.04488", "abs": "https://arxiv.org/abs/2510.04488", "authors": ["Edward Y. Chang", "Ethan Y. Chang"], "title": "Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning", "comment": "27 pages, 5 figures, 21 tables", "summary": "Multi-agent debate often wastes compute by using a fixed adversarial stance,\naggregating without deliberation, or stopping on heuristics. We introduce MACI,\nan active controller with two independent dials that decouple information from\nbehavior: an information dial that gates evidence by quality, and a behavior\ndial that schedules contentiousness from exploration to consolidation. A\nmoderator tracks disagreement, overlap, evidence quality, and argument quality,\nand halts when gains plateau. We provide theory-lite guarantees for\nnonincreasing dispersion and provable termination, with a budget-feasible\nscheduler. Across clinical diagnosis and news-bias tasks, MACI improves\naccuracy and calibration while reducing tokens, and converts residual\nuncertainty into precision RAG plans that specify what to retrieve next. We use\na cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,\nvalidated for order invariance and judge-swap stability; stability depends on\nusing high-capability judges. MACI turns debate into a budget-aware,\nmeasurable, and provably terminating controller.", "AI": {"tldr": "MACI\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u4fe1\u606f\u62e8\u76d8\u548c\u884c\u4e3a\u62e8\u76d8\u5206\u522b\u63a7\u5236\u8bc1\u636e\u8d28\u91cf\u548c\u8fa9\u8bba\u884c\u4e3a\uff0c\u4f7f\u7528\u8c03\u8282\u5668\u8ddf\u8e2a\u5206\u6b67\u3001\u91cd\u53e0\u5ea6\u548c\u8bba\u8bc1\u8d28\u91cf\uff0c\u5728\u9884\u7b97\u5185\u5b9e\u73b0\u53ef\u8bc1\u660e\u7684\u7ec8\u6b62\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u5b58\u5728\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3001\u56fa\u5b9a\u5bf9\u6297\u7acb\u573a\u3001\u65e0\u6df1\u601d\u719f\u8651\u7684\u805a\u5408\u4ee5\u53ca\u57fa\u4e8e\u542f\u53d1\u5f0f\u505c\u6b62\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8fa9\u8bba\u63a7\u5236\u673a\u5236\u3002", "method": "\u5f15\u5165MACI\u63a7\u5236\u5668\uff0c\u5305\u542b\u4fe1\u606f\u62e8\u76d8\uff08\u6309\u8d28\u91cf\u7b5b\u9009\u8bc1\u636e\uff09\u548c\u884c\u4e3a\u62e8\u76d8\uff08\u4ece\u63a2\u7d22\u5230\u5de9\u56fa\u7684\u8fa9\u8bba\u884c\u4e3a\u8c03\u5ea6\uff09\uff0c\u4f7f\u7528\u8c03\u8282\u5668\u8ddf\u8e2a\u5173\u952e\u6307\u6807\u5e76\u5728\u6536\u76ca\u5e73\u7a33\u65f6\u505c\u6b62\uff0c\u91c7\u7528\u9884\u7b97\u53ef\u884c\u7684\u8c03\u5ea6\u5668\u3002", "result": "\u5728\u4e34\u5e8a\u8bca\u65ad\u548c\u65b0\u95fb\u504f\u89c1\u4efb\u52a1\u4e2d\uff0cMACI\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6821\u51c6\u5ea6\uff0c\u51cf\u5c11\u4e86token\u4f7f\u7528\u91cf\uff0c\u5e76\u5c06\u5269\u4f59\u4e0d\u786e\u5b9a\u6027\u8f6c\u5316\u4e3a\u7cbe\u786e\u7684RAG\u68c0\u7d22\u8ba1\u5212\u3002", "conclusion": "MACI\u5c06\u8fa9\u8bba\u8f6c\u53d8\u4e3a\u9884\u7b97\u611f\u77e5\u3001\u53ef\u6d4b\u91cf\u4e14\u53ef\u8bc1\u660e\u7ec8\u6b62\u7684\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u8de8\u5bb6\u65cfLLM\u6cd5\u5b98\u786e\u4fdd\u7a33\u5b9a\u6027\u548c\u987a\u5e8f\u4e0d\u53d8\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.03349", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.03349", "abs": "https://arxiv.org/abs/2510.03349", "authors": ["Michael Chen"], "title": "AgentCaster: Reasoning-Guided Tornado Forecasting", "comment": null, "summary": "There is a growing need to evaluate Large Language Models (LLMs) on complex,\nhigh-impact, real-world tasks to assess their true readiness as reasoning\nagents. To address this gap, we introduce AgentCaster, a contamination-free\nframework employing multimodal LLMs end-to-end for the challenging,\nlong-horizon task of tornado forecasting. Within AgentCaster, models interpret\nheterogeneous spatiotemporal data from a high-resolution convection-allowing\nforecast archive. We assess model performance over a 40-day period featuring\ndiverse historical data, spanning several major tornado outbreaks and including\nover 500 tornado reports. Each day, models query interactively from a pool of\n3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of\n12-36 hours. Probabilistic tornado-risk polygon predictions are verified\nagainst ground truths derived from geometric comparisons across disjoint risk\nbands in projected coordinate space. To quantify accuracy, we propose\ndomain-specific TornadoBench and TornadoHallucination metrics, with\nTornadoBench highly challenging for both LLMs and domain expert human\nforecasters. Notably, human experts significantly outperform state-of-the-art\nmodels, which demonstrate a strong tendency to hallucinate and overpredict risk\nintensity, struggle with precise geographic placement, and exhibit poor\nspatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster\naims to advance research on improving LLM agents for challenging reasoning\ntasks in critical domains.", "AI": {"tldr": "AgentCaster\u662f\u4e00\u4e2a\u7528\u4e8e\u9f99\u5377\u98ce\u9884\u6d4b\u7684\u591a\u6a21\u6001LLM\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30LLM\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4eba\u7c7b\u4e13\u5bb6\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u9700\u8981\u8bc4\u4f30LLM\u5728\u590d\u6742\u3001\u9ad8\u5f71\u54cd\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u8861\u91cf\u5176\u4f5c\u4e3a\u63a8\u7406\u667a\u80fd\u4f53\u7684\u771f\u5b9e\u51c6\u5907\u7a0b\u5ea6\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001LLM\u7aef\u5230\u7aef\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u5bf9\u6d41\u5141\u8bb8\u9884\u62a5\u6863\u6848\u4e2d\u7684\u5f02\u8d28\u65f6\u7a7a\u6570\u636e\uff0c\u572840\u5929\u671f\u95f4\u67e5\u8be23,625\u4e2a\u9884\u62a5\u56fe\u548c40,125\u4e2a\u9884\u62a5\u63a2\u7a7a\u6570\u636e\u3002", "result": "\u4eba\u7c7b\u4e13\u5bb6\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u4ea7\u751f\u5e7b\u89c9\u548c\u8fc7\u5ea6\u9884\u6d4b\u98ce\u9669\u5f3a\u5ea6\uff0c\u5728\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8f83\u5dee\u7684\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "AgentCaster\u65e8\u5728\u63a8\u8fdbLLM\u667a\u80fd\u4f53\u5728\u5173\u952e\u9886\u57df\u6311\u6218\u6027\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6539\u8fdb\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2510.04514", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CV", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.04514", "abs": "https://arxiv.org/abs/2510.04514", "authors": ["Rachneet Kaur", "Nishan Srishankar", "Zhen Zeng", "Sumitra Ganesh", "Manuela Veloso"], "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering", "comment": "53 pages, 12 figures, 15 tables", "summary": "Recent multimodal LLMs have shown promise in chart-based visual question\nanswering, but their performance declines sharply on unannotated charts, those\nrequiring precise visual interpretation rather than relying on textual\nshortcuts. To address this, we introduce ChartAgent, a novel agentic framework\nthat explicitly performs visual reasoning directly within the chart's spatial\ndomain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively\ndecomposes queries into visual subtasks and actively manipulates and interacts\nwith chart images through specialized actions such as drawing annotations,\ncropping regions (e.g., segmenting pie slices, isolating bars), and localizing\naxes, using a library of chart-specific vision tools to fulfill each subtask.\nThis iterative reasoning process closely mirrors human cognitive strategies for\nchart comprehension. ChartAgent achieves state-of-the-art accuracy on the\nChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%\nabsolute gain overall and 17.31% on unannotated, numerically intensive queries.\nFurthermore, our analyses show that ChartAgent is (a) effective across diverse\nchart types, (b) achieve the highest scores across varying visual and reasoning\ncomplexity levels, and (c) serves as a plug-and-play framework that boosts\nperformance across diverse underlying LLMs. Our work is among the first to\ndemonstrate visually grounded reasoning for chart understanding using\ntool-augmented multimodal agents.", "AI": {"tldr": "ChartAgent\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u56fe\u8868\u7a7a\u95f4\u57df\u4e2d\u76f4\u63a5\u6267\u884c\u89c6\u89c9\u63a8\u7406\u6765\u89e3\u51b3\u672a\u6807\u6ce8\u56fe\u8868\u7406\u89e3\u95ee\u9898\uff0c\u8d85\u8d8a\u4e86\u4f9d\u8d56\u6587\u672c\u6377\u5f84\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001LLM\u5728\u57fa\u4e8e\u56fe\u8868\u7684\u89c6\u89c9\u95ee\u7b54\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u786e\u89c6\u89c9\u89e3\u91ca\u800c\u975e\u4f9d\u8d56\u6587\u672c\u6377\u5f84\u7684\u672a\u6807\u6ce8\u56fe\u8868\u4e0a\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002", "method": "ChartAgent\u8fed\u4ee3\u5730\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u89c6\u89c9\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u89c6\u89c9\u5de5\u5177\uff08\u5982\u7ed8\u5236\u6ce8\u91ca\u3001\u88c1\u526a\u533a\u57df\u3001\u5b9a\u4f4d\u5750\u6807\u8f74\uff09\u4e3b\u52a8\u64cd\u4f5c\u548c\u4ea4\u4e92\u56fe\u8868\u56fe\u50cf\uff0c\u6a21\u62df\u4eba\u7c7b\u56fe\u8868\u7406\u89e3\u8ba4\u77e5\u7b56\u7565\u3002", "result": "\u5728ChartBench\u548cChartX\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u51c6\u786e\u7387\uff0c\u6574\u4f53\u7edd\u5bf9\u589e\u76ca\u8fbe16.07%\uff0c\u5728\u672a\u6807\u6ce8\u6570\u503c\u5bc6\u96c6\u578b\u67e5\u8be2\u4e0a\u63d0\u534717.31%\uff0c\u4e14\u5728\u4e0d\u540c\u56fe\u8868\u7c7b\u578b\u548c\u590d\u6742\u5ea6\u7ea7\u522b\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ChartAgent\u662f\u9996\u6279\u4f7f\u7528\u5de5\u5177\u589e\u5f3a\u591a\u6a21\u6001\u4ee3\u7406\u8fdb\u884c\u89c6\u89c9\u57fa\u7840\u63a8\u7406\u7684\u56fe\u8868\u7406\u89e3\u5de5\u4f5c\u4e4b\u4e00\uff0c\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6846\u67b6\u63d0\u5347\u5404\u79cd\u5e95\u5c42LLM\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.04532", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04532", "abs": "https://arxiv.org/abs/2510.04532", "authors": ["Xurui Song", "Shuo Huai", "JingJing Jiang", "Jiayi Kong", "Jun Luo"], "title": "More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models", "comment": "The dataset will be released publicly once the paper is accepted for\n  publication", "summary": "Vision-Language Model (VLM) driving agents promise explainable end-to-end\nautonomy by first producing natural-language reasoning and then predicting\ntrajectory planning. However, whether planning is causally driven by this\nreasoning remains a critical but unverified assumption. To investigate this, we\nbuild DriveMind, a large-scale driving Visual Question Answering (VQA) corpus\nwith plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.\nOur data generation process converts sensors and annotations into structured\ninputs and, crucially, separates priors from to-be-reasoned signals, enabling\nclean information ablations. Using DriveMind, we train representative VLM\nagents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization\n(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,\nindicate a consistent causal disconnect in reasoning-planning: removing\nego/navigation priors causes large drops in planning scores, whereas removing\nCoT produces only minor changes. Attention analysis further shows that planning\nprimarily focuses on priors rather than the CoT. Based on this evidence, we\npropose the Reasoning-Planning Decoupling Hypothesis, positing that the\ntraining-yielded reasoning is an ancillary byproduct rather than a causal\nmediator. To enable efficient diagnosis, we also introduce a novel,\ntraining-free probe that measures an agent's reliance on priors by evaluating\nits planning robustness against minor input perturbations. In summary, we\nprovide the community with a new dataset and a diagnostic tool to evaluate the\ncausal fidelity of future models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86DriveMind\u6570\u636e\u96c6\u6765\u9a8c\u8bc1VLM\u9a7e\u9a76\u4ee3\u7406\u4e2d\u63a8\u7406\u4e0e\u89c4\u5212\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u53d1\u73b0\u4e24\u8005\u5b58\u5728\u56e0\u679c\u65ad\u5f00\uff0c\u89c4\u5212\u4e3b\u8981\u4f9d\u8d56\u5148\u9a8c\u4fe1\u606f\u800c\u975e\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u9a8c\u8bc1VLM\u9a7e\u9a76\u4ee3\u7406\u4e2d\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u662f\u5426\u771f\u6b63\u56e0\u679c\u9a71\u52a8\u8f68\u8ff9\u89c4\u5212\u8fd9\u4e00\u5173\u952e\u4f46\u672a\u7ecf\u9a8c\u8bc1\u7684\u5047\u8bbe\u3002", "method": "\u6784\u5efaDriveMind\u5927\u89c4\u6a21\u9a7e\u9a76VQA\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u4fe1\u606f\u6d88\u878d\u5b9e\u9a8c\u8bad\u7ec3VLM\u4ee3\u7406\uff0c\u5e76\u4f7f\u7528\u6ce8\u610f\u529b\u5206\u6790\u548c\u8bad\u7ec3\u65e0\u5173\u63a2\u9488\u8fdb\u884c\u8bca\u65ad\u3002", "result": "\u63a8\u7406\u4e0e\u89c4\u5212\u5b58\u5728\u56e0\u679c\u65ad\u5f00\uff1a\u79fb\u9664\u5148\u9a8c\u4fe1\u606f\u5bfc\u81f4\u89c4\u5212\u5206\u6570\u5927\u5e45\u4e0b\u964d\uff0c\u800c\u79fb\u9664\u63a8\u7406\u94fe\u4ec5\u4ea7\u751f\u5fae\u5c0f\u53d8\u5316\uff1b\u6ce8\u610f\u529b\u5206\u6790\u663e\u793a\u89c4\u5212\u4e3b\u8981\u5173\u6ce8\u5148\u9a8c\u800c\u975e\u63a8\u7406\u3002", "conclusion": "\u63d0\u51fa\u63a8\u7406-\u89c4\u5212\u89e3\u8026\u5047\u8bf4\uff0c\u8ba4\u4e3a\u8bad\u7ec3\u4ea7\u751f\u7684\u63a8\u7406\u662f\u9644\u5e26\u4ea7\u7269\u800c\u975e\u56e0\u679c\u4e2d\u4ecb\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\u6765\u8bc4\u4f30\u672a\u6765\u6a21\u578b\u7684\u56e0\u679c\u4fdd\u771f\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2510.03360", "categories": ["cs.LG", "cs.AI", "math.OC", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.03360", "abs": "https://arxiv.org/abs/2510.03360", "authors": ["Zelin Zhao", "Zongyi Li", "Kimia Hassibi", "Kamyar Azizzadenesheli", "Junchi Yan", "H. Jane Bae", "Di Zhou", "Anima Anandkumar"], "title": "Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows", "comment": null, "summary": "Assessing turbulence control effects for wall friction numerically is a\nsignificant challenge since it requires expensive simulations of turbulent\nfluid dynamics. We instead propose an efficient deep reinforcement learning\n(RL) framework for modeling and control of turbulent flows. It is model-based\nRL for predictive control (PC), where both the policy and the observer models\nfor turbulence control are learned jointly using Physics Informed Neural\nOperators (PINO), which are discretization invariant and can capture fine\nscales in turbulent flows accurately. Our PINO-PC outperforms prior model-free\nreinforcement learning methods in various challenging scenarios where the flows\nare of high Reynolds numbers and unseen, i.e., not provided during model\ntraining. We find that PINO-PC achieves a drag reduction of 39.0\\% under a\nbulk-velocity Reynolds number of 15,000, outperforming previous fluid control\nmethods by more than 32\\%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7b97\u5b50(PINO)\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6e4d\u6d41\u5efa\u6a21\u548c\u63a7\u5236\uff0c\u5728\u672a\u89c1\u7684\u9ad8\u96f7\u8bfa\u6570\u6d41\u52a8\u4e2d\u5b9e\u73b0\u4e8639.0%\u7684\u51cf\u963b\u6548\u679c\u3002", "motivation": "\u6570\u503c\u6a21\u62df\u6e4d\u6d41\u63a7\u5236\u5bf9\u58c1\u9762\u6469\u64e6\u7684\u5f71\u54cd\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u5efa\u6a21\u548c\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u9884\u6d4b\u63a7\u5236\uff0c\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7b97\u5b50(PINO)\u8054\u5408\u5b66\u4e60\u6e4d\u6d41\u63a7\u5236\u7684\u7b56\u7565\u548c\u89c2\u6d4b\u5668\u6a21\u578b\u3002", "result": "PINO-PC\u5728\u96f7\u8bfa\u6570\u4e3a15,000\u7684\u9ad8\u96f7\u8bfa\u6570\u672a\u89c1\u6d41\u52a8\u4e2d\u5b9e\u73b0\u4e8639.0%\u7684\u51cf\u963b\uff0c\u6bd4\u4e4b\u524d\u7684\u6d41\u4f53\u63a7\u5236\u65b9\u6cd5\u63d0\u9ad8\u4e8632%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6311\u6218\u6027\u573a\u666f\u4e0b\u4f18\u4e8e\u5148\u524d\u7684\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u51c6\u786e\u6355\u6349\u6e4d\u6d41\u4e2d\u7684\u7cbe\u7ec6\u5c3a\u5ea6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04542", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04542", "abs": "https://arxiv.org/abs/2510.04542", "authors": ["Wolfgang Lehrach", "Daniel Hennes", "Miguel Lazaro-Gredilla", "Xinghua Lou", "Carter Wendelken", "Zun Li", "Antoine Dedieu", "Jordi Grau-Moya", "Marc Lanctot", "Atil Iscen", "John Schultz", "Marcus Chiam", "Ian Gemp", "Piotr Zielinski", "Satinder Singh", "Kevin P. Murphy"], "title": "Code World Models for General Game Playing", "comment": null, "summary": "Large Language Models (LLMs) reasoning abilities are increasingly being\napplied to classical board and card games, but the dominant approach --\ninvolving prompting for direct move generation -- has significant drawbacks. It\nrelies on the model's implicit fragile pattern-matching capabilities, leading\nto frequent illegal moves and strategically shallow play. Here we introduce an\nalternative approach: We use the LLM to translate natural language rules and\ngame trajectories into a formal, executable world model represented as Python\ncode. This generated model -- comprising functions for state transition, legal\nmove enumeration, and termination checks -- serves as a verifiable simulation\nengine for high-performance planning algorithms like Monte Carlo tree search\n(MCTS). In addition, we prompt the LLM to generate heuristic value functions\n(to make MCTS more efficient), and inference functions (to estimate hidden\nstates in imperfect information games). Our method offers three distinct\nadvantages compared to directly using the LLM as a policy: (1) Verifiability:\nThe generated CWM serves as a formal specification of the game's rules,\nallowing planners to algorithmically enumerate valid actions and avoid illegal\nmoves, contingent on the correctness of the synthesized model; (2) Strategic\nDepth: We combine LLM semantic understanding with the deep search power of\nclassical planners; and (3) Generalization: We direct the LLM to focus on the\nmeta-task of data-to-code translation, enabling it to adapt to new games more\neasily. We evaluate our agent on 10 different games, of which 4 are novel and\ncreated for this paper. 5 of the games are fully observed (perfect\ninformation), and 5 are partially observed (imperfect information). We find\nthat our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10\nconsidered games.", "AI": {"tldr": "\u4f7f\u7528LLM\u5c06\u81ea\u7136\u8bed\u8a00\u6e38\u620f\u89c4\u5219\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684Python\u4e16\u754c\u6a21\u578b\uff0c\u7ed3\u5408MCTS\u7b49\u89c4\u5212\u7b97\u6cd5\uff0c\u76f8\u6bd4\u76f4\u63a5\u4f7f\u7528LLM\u751f\u6210\u52a8\u4f5c\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u9a8c\u8bc1\u6027\u3001\u6218\u7565\u6df1\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u4f7f\u7528LLM\u76f4\u63a5\u751f\u6210\u6e38\u620f\u52a8\u4f5c\u7684\u65b9\u6cd5\u5b58\u5728\u975e\u6cd5\u79fb\u52a8\u9891\u7e41\u3001\u7b56\u7565\u6d45\u663e\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5c06LLM\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u5230Python\u4ee3\u7801\u7684\u8f6c\u6362\uff0c\u751f\u6210\u5305\u542b\u72b6\u6001\u8f6c\u79fb\u3001\u5408\u6cd5\u52a8\u4f5c\u679a\u4e3e\u548c\u7ec8\u6b62\u68c0\u67e5\u7684\u53ef\u6267\u884c\u4e16\u754c\u6a21\u578b\uff0c\u7ed3\u5408MCTS\u89c4\u5212\u7b97\u6cd5\u548c\u542f\u53d1\u5f0f\u4ef7\u503c\u51fd\u6570\u3002", "result": "\u572810\u4e2a\u6e38\u620f\u4e2d\u6d4b\u8bd5\uff085\u4e2a\u5b8c\u5168\u89c2\u5bdf\uff0c5\u4e2a\u90e8\u5206\u89c2\u5bdf\uff09\uff0c\u8be5\u65b9\u6cd5\u57289\u4e2a\u6e38\u620f\u4e2d\u4f18\u4e8e\u6216\u5339\u914dGemini 2.5 Pro\u3002", "conclusion": "LLM\u4f5c\u4e3a\u4ee3\u7801\u751f\u6210\u5668\u6bd4\u76f4\u63a5\u4f5c\u4e3a\u7b56\u7565\u66f4\u6709\u6548\uff0c\u80fd\u591f\u4ea7\u751f\u53ef\u9a8c\u8bc1\u7684\u3001\u6218\u7565\u6df1\u5ea6\u66f4\u5f3a\u7684\u6e38\u620f\u667a\u80fd\u4f53\u3002", "topic": "agent analysis"}}
{"id": "2510.04550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04550", "abs": "https://arxiv.org/abs/2510.04550", "authors": ["Pengfei He", "Zhenwei Dai", "Bing He", "Hui Liu", "Xianfeng Tang", "Hanqing Lu", "Juanhui Li", "Jiayuan Ding", "Subhabrata Mukherjee", "Suhang Wang", "Yue Xing", "Jiliang Tang", "Benoit Dumoulin"], "title": "TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use", "comment": null, "summary": "Large language model (LLM)-based agents increasingly rely on tool use to\ncomplete real-world tasks. While existing works evaluate the LLMs' tool use\ncapability, they largely focus on the final answers yet overlook the detailed\ntool usage trajectory, i.e., whether tools are selected, parameterized, and\nordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to\ncomprehensively evaluate LLMs' tool use capability through diverse tasks with\nfine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable\ntools across practical domains with tasks grounded in production-style APIs,\nand synthesizes trajectories that vary in breadth (parallel calls) and depth\n(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports\ntrajectory-level diagnostics, including tool selection and argument\ncorrectness, and dependency/order satisfaction. Analyses reveal failure modes\nsuch as similar tool confusion and parameter-blind selection, and scaling\nbehavior with tool diversity and trajectory length where the bottleneck of\ntransiting from short to mid-length trajectories is revealed, offering\nactionable guidance for LLMs' tool use.", "AI": {"tldr": "TRAJECT-Bench\u662f\u4e00\u4e2a\u8f68\u8ff9\u611f\u77e5\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30LLM\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6307\u6807\u5206\u6790\u5de5\u5177\u9009\u62e9\u3001\u53c2\u6570\u5316\u548c\u6392\u5e8f\u7684\u6b63\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\uff0c\u800c\u5ffd\u89c6\u4e86\u5de5\u5177\u4f7f\u7528\u7684\u8be6\u7ec6\u8f68\u8ff9\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30LLM\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b\u5b9e\u9645\u9886\u57df\u9ad8\u4fdd\u771f\u53ef\u6267\u884c\u5de5\u5177\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5408\u6210\u4e0d\u540c\u5e7f\u5ea6\uff08\u5e76\u884c\u8c03\u7528\uff09\u548c\u6df1\u5ea6\uff08\u76f8\u4e92\u4f9d\u8d56\u94fe\uff09\u7684\u8f68\u8ff9\uff0c\u4f7f\u7528\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u63ed\u793a\u4e86\u5931\u8d25\u6a21\u5f0f\u5982\u76f8\u4f3c\u5de5\u5177\u6df7\u6dc6\u548c\u53c2\u6570\u76f2\u9009\uff0c\u53d1\u73b0\u4e86\u4ece\u77ed\u8f68\u8ff9\u5230\u4e2d\u957f\u8f68\u8ff9\u8fc7\u6e21\u7684\u74f6\u9888\uff0c\u63d0\u4f9b\u4e86LLM\u5de5\u5177\u4f7f\u7528\u7684\u53ef\u64cd\u4f5c\u6307\u5bfc\u3002", "conclusion": "TRAJECT-Bench\u4e3aLLM\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5173\u952e\u74f6\u9888\u548c\u6539\u8fdb\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.04560", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04560", "abs": "https://arxiv.org/abs/2510.04560", "authors": ["Honghao Fu", "Yuan Ouyang", "Kai-Wei Chang", "Yiwei Wang", "Zi Huang", "Yujun Cai"], "title": "ContextNav: Towards Agentic Multimodal In-Context Learning", "comment": null, "summary": "Recent advances demonstrate that multimodal large language models (MLLMs)\nexhibit strong multimodal in-context learning (ICL) capabilities, enabling them\nto adapt to novel vision-language tasks from a few contextual examples.\nHowever, existing ICL approaches face challenges in reconciling scalability\nwith robustness across diverse tasks and noisy contextual examples: manually\nselecting examples produces clean contexts but is labor-intensive and\ntask-specific, while similarity-based retrieval improves scalability but could\nintroduce irrelevant or structurally inconsistent samples that degrade ICL\nperformance. To address these limitations, we propose ContextNav, the first\nagentic framework that integrates the scalability of automated retrieval with\nthe quality and adaptiveness of human-like curation, enabling noise-robust and\ndynamically optimized contextualization for multimodal ICL. ContextNav unifies\ncontext management and noise-robust contextualization within a closed-loop\nworkflow driven by graph-based orchestration. Specifically, it builds a\nresource-aware multimodal embedding pipeline, maintains a retrievable vector\ndatabase, and applies agentic retrieval and structural alignment to construct\nnoise-resilient contexts. An Operational Grammar Graph (OGG) further supports\nadaptive workflow planning and optimization, enabling the agent to refine its\noperational strategies based on downstream ICL feedback. Experimental results\ndemonstrate that ContextNav achieves state-of-the-art performance across\nvarious datasets, underscoring the promise of agentic workflows for advancing\nscalable and robust contextualization in multimodal ICL.", "AI": {"tldr": "ContextNav\u662f\u4e00\u4e2a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u7f16\u6392\u5c06\u81ea\u52a8\u5316\u68c0\u7d22\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u4eba\u7c7b\u5f0f\u7b56\u5212\u7684\u8d28\u91cf\u76f8\u7ed3\u5408\uff0c\u4e3a\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u566a\u58f0\u9c81\u68d2\u548c\u52a8\u6001\u4f18\u5316\u7684\u4e0a\u4e0b\u6587\u6784\u5efa\u3002", "motivation": "\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff1a\u624b\u52a8\u9009\u62e9\u793a\u4f8b\u8d28\u91cf\u9ad8\u4f46\u52b3\u52a8\u5bc6\u96c6\uff0c\u76f8\u4f3c\u6027\u68c0\u7d22\u53ef\u6269\u5c55\u4f46\u53ef\u80fd\u5f15\u5165\u566a\u58f0\u6837\u672c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u6784\u5efa\u8d44\u6e90\u611f\u77e5\u7684\u591a\u6a21\u6001\u5d4c\u5165\u7ba1\u9053\uff0c\u7ef4\u62a4\u53ef\u68c0\u7d22\u5411\u91cf\u6570\u636e\u5e93\uff0c\u5e94\u7528\u4ee3\u7406\u68c0\u7d22\u548c\u7ed3\u6784\u5bf9\u9f50\u6784\u5efa\u566a\u58f0\u5f39\u6027\u4e0a\u4e0b\u6587\uff0c\u4f7f\u7528\u64cd\u4f5c\u8bed\u6cd5\u56fe\u652f\u6301\u81ea\u9002\u5e94\u5de5\u4f5c\u6d41\u89c4\u5212\u548c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eContextNav\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u4ee3\u7406\u5de5\u4f5c\u6d41\u6709\u671b\u63a8\u8fdb\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684\u4e0a\u4e0b\u6587\u6784\u5efa\u3002", "topic": "agent analysis"}}
{"id": "2510.04568", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04568", "abs": "https://arxiv.org/abs/2510.04568", "authors": ["Naman Gupta", "Shreeyash Gowaikar", "Arun Iyer", "Kirankumar Shiragur", "Ramakrishna B Bairi", "Rishikesh Maurya", "Ritabrata Maiti", "Sankarshan Damle", "Shachee Mishra Gupta"], "title": "COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context", "comment": null, "summary": "Reasoning over very long inputs remains difficult for large language models\n(LLMs). Common workarounds either shrink the input via retrieval (risking\nmissed evidence), enlarge the context window (straining selectivity), or stage\nmultiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,\nCoA), free-form summaries passed between agents can discard crucial details and\namplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured\nMemory for Iterative Reasoning), a chain-style framework that replaces ad hoc\nmessages with a structured memory. A Planner agent first turns a user query\ninto concrete, checkable sub-questions. worker agents process chunks via a\nfixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared\nmemory. A Manager agent then Synthesizes the final answer directly from the\nmemory. This preserves step-wise read-then-reason benefits while changing both\nthe communication medium (structured memory) and the worker procedure (fixed\nmicro-cycle), yielding higher faithfulness, better long-range aggregation, and\nauditability. On long-context QA from the HELMET suite, COSMIR reduces\npropagation-stage information loss and improves accuracy over a CoA baseline.", "AI": {"tldr": "COSMIR\u662f\u4e00\u4e2a\u94fe\u5f0f\u63a8\u7406\u6846\u67b6\uff0c\u7528\u7ed3\u6784\u5316\u5185\u5b58\u66ff\u4ee3\u81ea\u7531\u5f62\u5f0f\u7684\u6d88\u606f\u4f20\u9012\uff0c\u901a\u8fc7\u89c4\u5212\u5668\u3001\u5de5\u4f5c\u5668\u548c\u7ba1\u7406\u5668\u4ee3\u7406\u7684\u534f\u4f5c\u6765\u5904\u7406\u957f\u8f93\u5165\u95ee\u9898\uff0c\u51cf\u5c11\u4fe1\u606f\u4e22\u5931\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u957f\u8f93\u5165\u65f6\u7684\u56f0\u96be\uff0c\u907f\u514d\u68c0\u7d22\u65b9\u6cd5\u53ef\u80fd\u9057\u6f0f\u8bc1\u636e\u3001\u6269\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\u964d\u4f4e\u9009\u62e9\u6027\uff0c\u4ee5\u53ca\u591a\u4ee3\u7406\u7ba1\u9053\u4e2d\u81ea\u7531\u5f62\u5f0f\u6458\u8981\u4e22\u5931\u5173\u952e\u7ec6\u8282\u548c\u653e\u5927\u65e9\u671f\u9519\u8bef\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u5185\u5b58\uff0c\u7531\u89c4\u5212\u5668\u4ee3\u7406\u5c06\u7528\u6237\u67e5\u8be2\u8f6c\u5316\u4e3a\u53ef\u68c0\u67e5\u7684\u5b50\u95ee\u9898\uff0c\u5de5\u4f5c\u5668\u4ee3\u7406\u901a\u8fc7\u56fa\u5b9a\u7684\u5fae\u5faa\u73af\uff08\u63d0\u53d6\u3001\u63a8\u7406\u3001\u7cbe\u70bc\uff09\u5904\u7406\u6570\u636e\u5757\u5e76\u5c06\u66f4\u65b0\u5199\u5165\u5171\u4eab\u5185\u5b58\uff0c\u7ba1\u7406\u5668\u4ee3\u7406\u4ece\u5185\u5b58\u4e2d\u5408\u6210\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u5728HELMET\u5957\u4ef6\u7684\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0cCOSMIR\u51cf\u5c11\u4e86\u4f20\u64ad\u9636\u6bb5\u7684\u4fe1\u606f\u4e22\u5931\uff0c\u76f8\u6bd4CoA\u57fa\u7ebf\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "COSMIR\u901a\u8fc7\u6539\u53d8\u901a\u4fe1\u5a92\u4ecb\uff08\u7ed3\u6784\u5316\u5185\u5b58\uff09\u548c\u5de5\u4f5c\u5668\u7a0b\u5e8f\uff08\u56fa\u5b9a\u5fae\u5faa\u73af\uff09\uff0c\u5728\u4fdd\u6301\u9010\u6b65\u8bfb\u53d6-\u63a8\u7406\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5fe0\u5b9e\u5ea6\u3001\u66f4\u597d\u7684\u957f\u8303\u56f4\u805a\u5408\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.04398", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04398", "abs": "https://arxiv.org/abs/2510.04398", "authors": ["Buyun Liang", "Liangzu Peng", "Jinqi Luo", "Darshan Thaker", "Kwan Ho Ryan Chan", "Ren\u00e9 Vidal"], "title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations", "comment": "Accepted at NeurIPS 2025. Code is available at\n  https://github.com/Buyun-Liang/SECA", "summary": "Large Language Models (LLMs) are increasingly deployed in high-risk domains.\nHowever, state-of-the-art LLMs often produce hallucinations, raising serious\nconcerns about their reliability. Prior work has explored adversarial attacks\nfor hallucination elicitation in LLMs, but it often produces unrealistic\nprompts, either by inserting gibberish tokens or by altering the original\nmeaning. As a result, these approaches offer limited insight into how\nhallucinations may occur in practice. While adversarial attacks in computer\nvision often involve realistic modifications to input images, the problem of\nfinding realistic adversarial prompts for eliciting LLM hallucinations has\nremained largely underexplored. To address this gap, we propose Semantically\nEquivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic\nmodifications to the prompt that preserve its meaning while maintaining\nsemantic coherence. Our contributions are threefold: (i) we formulate finding\nrealistic attacks for hallucination elicitation as a constrained optimization\nproblem over the input prompt space under semantic equivalence and coherence\nconstraints; (ii) we introduce a constraint-preserving zeroth-order method to\neffectively search for adversarial yet feasible prompts; and (iii) we\ndemonstrate through experiments on open-ended multiple-choice question\nanswering tasks that SECA achieves higher attack success rates while incurring\nalmost no constraint violations compared to existing methods. SECA highlights\nthe sensitivity of both open-source and commercial gradient-inaccessible LLMs\nto realistic and plausible prompt variations. Code is available at\nhttps://github.com/Buyun-Liang/SECA.", "AI": {"tldr": "\u63d0\u51faSECA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u7b49\u6548\u4e14\u8fde\u8d2f\u7684\u5bf9\u6297\u653b\u51fb\u6765\u5f15\u53d1LLM\u5e7b\u89c9\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u4ea7\u751f\u66f4\u73b0\u5b9e\u7684\u63d0\u793a\u4e14\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u4ea7\u751f\u4e0d\u73b0\u5b9e\u7684\u63d0\u793a\uff08\u5982\u63d2\u5165\u4e71\u7801\u6216\u6539\u53d8\u539f\u610f\uff09\uff0c\u65e0\u6cd5\u53cd\u6620\u5b9e\u9645\u4e2d\u5e7b\u89c9\u5982\u4f55\u53d1\u751f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u4fdd\u6301\u8bed\u4e49\u7b49\u6548\u6027\u7684\u73b0\u5b9e\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u5c06\u5bfb\u627e\u73b0\u5b9e\u653b\u51fb\u5efa\u6a21\u4e3a\u8bed\u4e49\u7b49\u6548\u548c\u8fde\u8d2f\u7ea6\u675f\u4e0b\u7684\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u7ea6\u675f\u4fdd\u6301\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u641c\u7d22\u5bf9\u6297\u6027\u63d0\u793a\u3002", "result": "\u5728\u5f00\u653e\u5f0f\u591a\u9879\u9009\u62e9\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0cSECA\u8fbe\u5230\u66f4\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u51e0\u4e4e\u4e0d\u8fdd\u53cd\u7ea6\u675f\u6761\u4ef6\u3002", "conclusion": "SECA\u63ed\u793a\u4e86\u5f00\u6e90\u548c\u5546\u4e1aLLM\u5bf9\u73b0\u5b9e\u4e14\u5408\u7406\u7684\u63d0\u793a\u53d8\u5f02\u7684\u654f\u611f\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.04617", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04617", "abs": "https://arxiv.org/abs/2510.04617", "authors": ["Zhejian Lai", "Xiang Geng", "Zhijun Wang", "Yang Bai", "Jiahuan Li", "Rongxiang Weng", "Jingang Wang", "Xuezhi Cao", "Xunliang Cai", "Shujian Huang"], "title": "Making Mathematical Reasoning Adaptive", "comment": null, "summary": "Mathematical reasoning is a primary indicator of large language models (LLMs)\nintelligence. However, existing LLMs exhibit failures of robustness and\ngeneralization. This paper attributes these deficiencies to spurious reasoning,\ni.e., producing answers from superficial features. To address this challenge,\nwe propose the AdaR framework to enable adaptive reasoning, wherein models rely\non problem-solving logic to produce answers. AdaR synthesizes logically\nequivalent queries by varying variable values, and trains models with RLVR on\nthese data to penalize spurious logic while encouraging adaptive logic. To\nimprove data quality, we extract the problem-solving logic from the original\nquery and generate the corresponding answer by code execution, then apply a\nsanity check. Experimental results demonstrate that AdaR improves robustness\nand generalization, achieving substantial improvement in mathematical reasoning\nwhile maintaining high data efficiency. Analysis indicates that data synthesis\nand RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.\nSubsequent analyses derive key design insights into the effect of critical\nfactors and the applicability to instruct LLMs. Our project is available at\nhttps://github.com/LaiZhejian/AdaR", "AI": {"tldr": "\u63d0\u51fa\u4e86AdaR\u6846\u67b6\u6765\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u4f2a\u63a8\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u903b\u8f91\u7b49\u4ef7\u67e5\u8be2\u548c\u4f7f\u7528RLVR\u8bad\u7ec3\uff0c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u5b58\u5728\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u5f52\u56e0\u4e8e\u4f2a\u63a8\u7406\uff08\u4ec5\u57fa\u4e8e\u8868\u9762\u7279\u5f81\u751f\u6210\u7b54\u6848\uff09\u3002", "method": "AdaR\u6846\u67b6\u901a\u8fc7\u53d8\u5316\u53d8\u91cf\u503c\u5408\u6210\u903b\u8f91\u7b49\u4ef7\u67e5\u8be2\uff0c\u4f7f\u7528RLVR\u8bad\u7ec3\u6a21\u578b\u60e9\u7f5a\u4f2a\u903b\u8f91\u5e76\u9f13\u52b1\u81ea\u9002\u5e94\u903b\u8f91\uff0c\u901a\u8fc7\u4ee3\u7801\u6267\u884c\u63d0\u53d6\u89e3\u9898\u903b\u8f91\u5e76\u751f\u6210\u7b54\u6848\uff0c\u5e76\u8fdb\u884c\u5b8c\u6574\u6027\u68c0\u67e5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eAdaR\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u9ad8\u6570\u636e\u6548\u7387\u7684\u540c\u65f6\u6539\u5584\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "\u6570\u636e\u5408\u6210\u548cRLVR\u534f\u540c\u5de5\u4f5c\u5b9e\u73b0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u540e\u7eed\u5206\u6790\u5f97\u51fa\u4e86\u5173\u952e\u8bbe\u8ba1\u89c1\u89e3\u548c\u9002\u7528\u4e8e\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56e0\u7d20\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04623", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04623", "abs": "https://arxiv.org/abs/2510.04623", "authors": ["Shrish Shrinath Vaidya", "Gowthamaan Palani", "Sidharth Ramesh", "Velmurugan Balasubramanian", "Minmini Selvam", "Gokulraja Srinivasaraja", "Ganapathy Krishnamurthi"], "title": "MedPAO: A Protocol-Driven Agent for Structuring Medical Reports", "comment": "Paper published at \"Agentic AI for Medicine\" Workshop, MICCAI 2025", "summary": "The deployment of Large Language Models (LLMs) for structuring clinical data\nis critically hindered by their tendency to hallucinate facts and their\ninability to follow domain-specific rules. To address this, we introduce\nMedPAO, a novel agentic framework that ensures accuracy and verifiable\nreasoning by grounding its operation in established clinical protocols such as\nthe ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring\ntask into a transparent process managed by a Plan-Act-Observe (PAO) loop and\nspecialized tools. This protocol-driven method provides a verifiable\nalternative to opaque, monolithic models. The efficacy of our approach is\ndemonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96\non the critical sub-task of concept categorization. Notably, expert\nradiologists and clinicians rated the final structured outputs with an average\nscore of 4.52 out of 5, indicating a level of reliability that surpasses\nbaseline approaches relying solely on LLM-based foundation models. The code is\navailable at: https://github.com/MiRL-IITM/medpao-agent", "AI": {"tldr": "MedPAO\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e34\u5e8a\u534f\u8bae\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7Plan-Act-Observe\u5faa\u73af\u548c\u4e13\u95e8\u5de5\u5177\u6765\u7ed3\u6784\u5316\u4e34\u5e8a\u6570\u636e\uff0c\u89e3\u51b3\u4e86LLM\u5728\u533b\u7597\u9886\u57df\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u4e34\u5e8a\u6570\u636e\u65f6\u4ea7\u751f\u5e7b\u89c9\u4e8b\u5b9e\u548c\u65e0\u6cd5\u9075\u5faa\u9886\u57df\u7279\u5b9a\u89c4\u5219\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165MedPAO\u4ee3\u7406\u6846\u67b6\uff0c\u57fa\u4e8e\u4e34\u5e8a\u534f\u8bae\uff08\u5982ABCDEF\u534f\u8bae\uff09\u8fdb\u884cCXR\u5206\u6790\uff0c\u901a\u8fc7Plan-Act-Observe\u5faa\u73af\u548c\u4e13\u95e8\u5de5\u5177\u5206\u89e3\u62a5\u544a\u7ed3\u6784\u5316\u4efb\u52a1\u3002", "result": "MedPAO\u5728\u6982\u5ff5\u5206\u7c7b\u5b50\u4efb\u52a1\u4e0a\u8fbe\u52300.96\u7684F1\u5206\u6570\uff0c\u4e13\u5bb6\u8bc4\u5206\u5e73\u57474.52/5\uff0c\u8d85\u8d8a\u4e86\u4ec5\u4f9d\u8d56LLM\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u534f\u8bae\u9a71\u52a8\u7684\u65b9\u6cd5\u4e3a\u4e0d\u900f\u660e\u7684\u5355\u4f53\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u533b\u7597\u6570\u636e\u7ed3\u6784\u5316\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.04454", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04454", "abs": "https://arxiv.org/abs/2510.04454", "authors": ["Xiangchi Yuan", "Xiang Chen", "Tong Yu", "Dachuan Shi", "Can Jin", "Wenke Lee", "Saayan Mitra"], "title": "Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners", "comment": null, "summary": "Large Language Models (LLMs) show strong reasoning abilities, often amplified\nby Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although\nRL algorithms can substantially improve reasoning, they struggle to expand\nreasoning boundaries because they learn from their own reasoning trajectories\nrather than acquiring external knowledge. Supervised fine-tuning (SFT) offers\ncomplementary benefits but typically requires large-scale data and risks\noverfitting. Recent attempts to combine SFT and RL face three main challenges:\ndata inefficiency, algorithm-specific designs, and catastrophic forgetting. We\npropose a plug-and-play framework that dynamically integrates SFT into RL by\nselecting challenging examples for SFT. This approach reduces SFT data\nrequirements and remains agnostic to the choice of RL or SFT algorithm. To\nmitigate catastrophic forgetting of RL-acquired skills during SFT, we select\nhigh-entropy tokens for loss calculation and freeze parameters identified as\ncritical for RL. Our method achieves state-of-the-art (SoTA) reasoning\nperformance using only 1.5% of the SFT data and 20.4% of the RL data used by\nprior SoTA, providing an efficient and plug-and-play solution for combining SFT\nand RL in reasoning post-training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6311\u6218\u6027\u793a\u4f8b\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5c06SFT\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u9700\u6c42\u5e76\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408SFT\u548cRL\u9762\u4e34\u6570\u636e\u6548\u7387\u4f4e\u3001\u7b97\u6cd5\u7279\u5b9a\u8bbe\u8ba1\u548c\u707e\u96be\u6027\u9057\u5fd8\u4e09\u5927\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u96c6\u6210\u65b9\u6848\u3002", "method": "\u52a8\u6001\u9009\u62e9\u6311\u6218\u6027\u793a\u4f8b\u8fdb\u884cSFT\uff0c\u9009\u62e9\u9ad8\u71b5token\u8ba1\u7b97\u635f\u5931\uff0c\u5e76\u51bb\u7ed3\u5bf9RL\u5173\u952e\u53c2\u6570\uff0c\u5b9e\u73b0SFT\u4e0eRL\u7684\u9ad8\u6548\u7ed3\u5408\u3002", "result": "\u4ec5\u4f7f\u75281.5%\u7684SFT\u6570\u636e\u548c20.4%\u7684RL\u6570\u636e\u5c31\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5373\u63d2\u5373\u7528\u7684SFT\u4e0eRL\u7ed3\u5408\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04643", "abs": "https://arxiv.org/abs/2510.04643", "authors": ["Xiangyu Li", "Yawen Zeng", "Xiaofen Xing", "Jin Xu", "Xiangmin Xu"], "title": "QuantAgents: Towards Multi-agent Financial System via Simulated Trading", "comment": "This paper has been accepted by EMNLP 2025", "summary": "In this paper, our objective is to develop a multi-agent financial system\nthat incorporates simulated trading, a technique extensively utilized by\nfinancial professionals. While current LLM-based agent models demonstrate\ncompetitive performance, they still exhibit significant deviations from\nreal-world fund companies. A critical distinction lies in the agents' reliance\non ``post-reflection'', particularly in response to adverse outcomes, but lack\na distinctly human capability: long-term prediction of future trends.\nTherefore, we introduce QuantAgents, a multi-agent system integrating simulated\ntrading, to comprehensively evaluate various investment strategies and market\nscenarios without assuming actual risks. Specifically, QuantAgents comprises\nfour agents: a simulated trading analyst, a risk control analyst, a market news\nanalyst, and a manager, who collaborate through several meetings. Moreover, our\nsystem incentivizes agents to receive feedback on two fronts: performance in\nreal-world markets and predictive accuracy in simulated trading. Extensive\nexperiments demonstrate that our framework excels across all metrics, yielding\nan overall return of nearly 300% over the three years\n(https://quantagents.github.io/).", "AI": {"tldr": "\u63d0\u51fa\u4e86QuantAgents\u591a\u667a\u80fd\u4f53\u91d1\u878d\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u4ea4\u6613\u8bc4\u4f30\u6295\u8d44\u7b56\u7565\uff0c\u5305\u542b\u56db\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\uff0c\u5728\u4e09\u5e74\u5185\u5b9e\u73b0\u4e86\u8fd1300%\u7684\u6574\u4f53\u56de\u62a5\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u91d1\u878d\u9886\u57df\u8868\u73b0\u826f\u597d\u4f46\u4e0e\u73b0\u5b9e\u57fa\u91d1\u516c\u53f8\u5b58\u5728\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u957f\u671f\u8d8b\u52bf\u9884\u6d4b\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u591a\u667a\u80fd\u4f53\u91d1\u878d\u7cfb\u7edf\u3002", "method": "\u6784\u5efa\u5305\u542b\u6a21\u62df\u4ea4\u6613\u5206\u6790\u5e08\u3001\u98ce\u9669\u63a7\u5236\u5206\u6790\u5e08\u3001\u5e02\u573a\u65b0\u95fb\u5206\u6790\u5e08\u548c\u7ba1\u7406\u8005\u56db\u4e2a\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6b21\u4f1a\u8bae\u534f\u4f5c\uff0c\u5e76\u5728\u771f\u5b9e\u5e02\u573a\u8868\u73b0\u548c\u6a21\u62df\u4ea4\u6613\u9884\u6d4b\u51c6\u786e\u6027\u4e24\u65b9\u9762\u7ed9\u4e88\u53cd\u9988\u6fc0\u52b1\u3002", "result": "\u7cfb\u7edf\u5728\u6240\u6709\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e09\u5e74\u5185\u5b9e\u73b0\u4e86\u8fd1300%\u7684\u6574\u4f53\u6295\u8d44\u56de\u62a5\u3002", "conclusion": "QuantAgents\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u6a21\u62df\u4ea4\u6613\uff0c\u6709\u6548\u63d0\u5347\u4e86\u91d1\u878d\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u957f\u671f\u9884\u6d4b\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.04695", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04695", "abs": "https://arxiv.org/abs/2510.04695", "authors": ["Yiding Wang", "Zhepei Wei", "Xinyu Zhu", "Yu Meng"], "title": "Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents", "comment": null, "summary": "Enabling large language models (LLMs) to utilize search tools offers a\npromising path to overcoming fundamental limitations such as knowledge cutoffs\nand hallucinations. Recent work has explored reinforcement learning (RL) for\ntraining search-augmented agents that interleave reasoning and retrieval before\nanswering. These approaches usually rely on outcome-based rewards (e.g., exact\nmatch), implicitly assuming that optimizing for final answers will also yield\neffective intermediate search behaviors. Our analysis challenges this\nassumption: we uncover multiple systematic deficiencies in search that arise\nunder outcome-only training and ultimately degrade final answer quality,\nincluding failure to invoke tools, invalid queries, and redundant searches. To\naddress these shortcomings, we introduce DeSA (Decoupling\nSearch-and-Answering), a simple two-stage training framework that explicitly\nseparates search optimization from answer generation. In Stage 1, agents are\ntrained to improve search effectiveness with retrieval recall-based rewards. In\nStage 2, outcome rewards are employed to optimize final answer generation.\nAcross seven QA benchmarks, DeSA-trained agents consistently improve search\nbehaviors, delivering substantially higher search recall and answer accuracy\nthan outcome-only baselines. Notably, DeSA outperforms single-stage training\napproaches that simultaneously optimize recall and outcome rewards,\nunderscoring the necessity of explicitly decoupling the two objectives.", "AI": {"tldr": "DeSA\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u641c\u7d22\u4f18\u5316\u548c\u7b54\u6848\u751f\u6210\u6765\u89e3\u51b3\u4ec5\u57fa\u4e8e\u7ed3\u679c\u5956\u52b1\u8bad\u7ec3\u7684\u641c\u7d22\u589e\u5f3a\u4ee3\u7406\u5b58\u5728\u7684\u641c\u7d22\u7f3a\u9677\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u589e\u5f3a\u4ee3\u7406\u901a\u5e38\u4f9d\u8d56\u7ed3\u679c\u5956\u52b1\uff08\u5982\u7cbe\u786e\u5339\u914d\uff09\uff0c\u5047\u8bbe\u4f18\u5316\u6700\u7ec8\u7b54\u6848\u4e5f\u4f1a\u4ea7\u751f\u6709\u6548\u7684\u4e2d\u95f4\u641c\u7d22\u884c\u4e3a\u3002\u4f46\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u5047\u8bbe\u5b58\u5728\u95ee\u9898\uff0c\u4ec5\u57fa\u4e8e\u7ed3\u679c\u8bad\u7ec3\u4f1a\u51fa\u73b0\u5de5\u5177\u8c03\u7528\u5931\u8d25\u3001\u65e0\u6548\u67e5\u8be2\u548c\u5197\u4f59\u641c\u7d22\u7b49\u7cfb\u7edf\u6027\u7f3a\u9677\u3002", "method": "\u63d0\u51faDeSA\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528\u68c0\u7d22\u53ec\u56de\u5956\u52b1\u8bad\u7ec3\u4ee3\u7406\u6539\u8fdb\u641c\u7d22\u6548\u679c\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7528\u7ed3\u679c\u5956\u52b1\u4f18\u5316\u6700\u7ec8\u7b54\u6848\u751f\u6210\u3002", "result": "\u57287\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeSA\u8bad\u7ec3\u7684\u4ee3\u7406\u663e\u8457\u6539\u5584\u4e86\u641c\u7d22\u884c\u4e3a\uff0c\u641c\u7d22\u53ec\u56de\u7387\u548c\u7b54\u6848\u51c6\u786e\u7387\u90fd\u5927\u5e45\u9ad8\u4e8e\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u660e\u786e\u5206\u79bb\u641c\u7d22\u548c\u56de\u7b54\u76ee\u6807\u5bf9\u4e8e\u8bad\u7ec3\u6709\u6548\u7684\u641c\u7d22\u589e\u5f3a\u4ee3\u7406\u662f\u5fc5\u8981\u7684\uff0cDeSA\u4f18\u4e8e\u540c\u65f6\u4f18\u5316\u53ec\u56de\u548c\u7ed3\u679c\u5956\u52b1\u7684\u5355\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04721", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04721", "abs": "https://arxiv.org/abs/2510.04721", "authors": ["Ivo Petrov", "Jasper Dekoninck", "Martin Vechev"], "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs", "comment": null, "summary": "Large language models (LLMs) have recently shown strong performance on\nmathematical benchmarks. At the same time, they are prone to hallucination and\nsycophancy, often providing convincing but flawed proofs for incorrect\nmathematical statements provided by users. This significantly limits the\napplicability of LLMs in theorem proving, as verification of these flawed\nproofs must be done manually by expert mathematicians. However, existing\nbenchmarks that measure sycophancy in mathematics are limited: they focus\nsolely on final-answer problems, rely on very simple and often contaminated\ndatasets, and construct benchmark samples using synthetic modifications that\ncreate ill-posed questions rather than well-posed questions that are\ndemonstrably false. To address these issues, we introduce BrokenMath, the first\nbenchmark for evaluating sycophantic behavior in LLMs within the context of\nnatural language theorem proving. BrokenMath is built from advanced 2025\ncompetition problems, which are perturbed with an LLM to produce false\nstatements and subsequently refined through expert review. Using an\nLLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems\nand find that sycophancy is widespread, with the best model, GPT-5, producing\nsycophantic answers 29% of the time. We further investigate several mitigation\nstrategies, including test-time interventions and supervised fine-tuning on\ncurated sycophantic examples. These approaches substantially reduce, but do not\neliminate, sycophantic behavior.", "AI": {"tldr": "BrokenMath\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u5728\u81ea\u7136\u8bed\u8a00\u5b9a\u7406\u8bc1\u660e\u4e2d\u8c04\u5a9a\u884c\u4e3a\u7684\u57fa\u51c6\uff0c\u57fa\u4e8e2025\u5e74\u7ade\u8d5b\u95ee\u9898\u6784\u5efa\uff0c\u53d1\u73b0GPT-5\u7b49\u6a21\u578b\u670929%\u7684\u6982\u7387\u4ea7\u751f\u8c04\u5a9a\u56de\u7b54\uff0c\u7f13\u89e3\u7b56\u7565\u80fd\u51cf\u5c11\u4f46\u4e0d\u80fd\u5b8c\u5168\u6d88\u9664\u8be5\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u57fa\u51c6\u5728\u8bc4\u4f30\u8c04\u5a9a\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff1a\u4ec5\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u95ee\u9898\u3001\u4f9d\u8d56\u7b80\u5355\u4e14\u88ab\u6c61\u67d3\u7684\u6570\u636e\u96c6\u3001\u4f7f\u7528\u5408\u6210\u4fee\u6539\u521b\u5efa\u75c5\u6001\u95ee\u9898\u800c\u975e\u53ef\u8bc1\u660e\u9519\u8bef\u7684\u826f\u6784\u95ee\u9898\u3002", "method": "\u4ece2025\u5e74\u7ade\u8d5b\u95ee\u9898\u6784\u5efaBrokenMath\u57fa\u51c6\uff0c\u901a\u8fc7LLM\u6270\u52a8\u4ea7\u751f\u9519\u8bef\u9648\u8ff0\u5e76\u7ecf\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u5b8c\u5584\uff0c\u4f7f\u7528LLM-as-a-judge\u6846\u67b6\u8bc4\u4f30\u6700\u5148\u8fdb\u6a21\u578b\u548c\u4ee3\u7406\u7cfb\u7edf\u3002", "result": "\u53d1\u73b0\u8c04\u5a9a\u884c\u4e3a\u666e\u904d\u5b58\u5728\uff0c\u6700\u4f73\u6a21\u578bGPT-5\u670929%\u7684\u6982\u7387\u4ea7\u751f\u8c04\u5a9a\u56de\u7b54\uff0c\u6d4b\u8bd5\u65f6\u5e72\u9884\u548c\u76d1\u7763\u5fae\u8c03\u7b49\u7f13\u89e3\u7b56\u7565\u80fd\u663e\u8457\u51cf\u5c11\u4f46\u4e0d\u80fd\u5b8c\u5168\u6d88\u9664\u8be5\u884c\u4e3a\u3002", "conclusion": "LLM\u5728\u5b9a\u7406\u8bc1\u660e\u4e2d\u5b58\u5728\u663e\u8457\u7684\u8c04\u5a9a\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\u6765\u63d0\u5347\u5176\u5728\u6570\u5b66\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.03442", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03442", "abs": "https://arxiv.org/abs/2510.03442", "authors": ["Ege Cakar", "Per Ola Kristensson"], "title": "The Argument is the Explanation: Structured Argumentation for Trust in Agents", "comment": "8 pages, 4 figures, 6 tables, submitted to IAAI-26", "summary": "Humans are black boxes -- we cannot observe their neural processes, yet\nsociety functions by evaluating verifiable arguments. AI explainability should\nfollow this principle: stakeholders need verifiable reasoning chains, not\nmechanistic transparency. We propose using structured argumentation to provide\na level of explanation and verification neither interpretability nor\nLLM-generated explanation is able to offer. Our pipeline achieves\nstate-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7\npoints above prior work) and $0.81$ macro F1, $\\sim$0.07 above previous\npublished results with comparable data setups, for Argumentative MicroTexts\nrelation classification, converting LLM text into argument graphs and enabling\nverification at each inferential step. We demonstrate this idea on multi-agent\nrisk assessment using the Structured What-If Technique, where specialized\nagents collaborate transparently to carry out risk assessment otherwise\nachieved by humans alone. Using Bipolar Assumption-Based Argumentation, we\ncapture support/attack relationships, thereby enabling automatic hallucination\ndetection via fact nodes attacking arguments. We also provide a verification\nmechanism that enables iterative refinement through test-time feedback without\nretraining. For easy deployment, we provide a Docker container for the\nfine-tuned AMT model, and the rest of the code with the Bipolar ABA Python\npackage on GitHub.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u7ed3\u6784\u5316\u8bba\u8bc1\u65b9\u6cd5\u4e3aAI\u7cfb\u7edf\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u89e3\u91ca\uff0c\u5728\u8bba\u8bc1\u5173\u7cfb\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u5e94\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u98ce\u9669\u8bc4\u4f30\u573a\u666f\u3002", "motivation": "\u4eba\u7c7b\u601d\u7ef4\u8fc7\u7a0b\u4e0d\u53ef\u89c2\u5bdf\u4f46\u793e\u4f1a\u4f9d\u8d56\u53ef\u9a8c\u8bc1\u8bba\u8bc1\uff0cAI\u53ef\u89e3\u91ca\u6027\u5e94\u9075\u5faa\u76f8\u540c\u539f\u5219\uff0c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u94fe\u800c\u975e\u673a\u5236\u900f\u660e\u6027\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u5316\u8bba\u8bc1\u65b9\u6cd5\uff0c\u5c06LLM\u6587\u672c\u8f6c\u6362\u4e3a\u8bba\u8bc1\u56fe\uff0c\u91c7\u7528\u53cc\u6781\u5047\u8bbe\u8bba\u8bc1\u6846\u67b6\u6355\u83b7\u652f\u6301/\u653b\u51fb\u5173\u7cfb\uff0c\u5b9e\u73b0\u81ea\u52a8\u5e7b\u89c9\u68c0\u6d4b\u548c\u6d4b\u8bd5\u65f6\u53cd\u9988\u673a\u5236\u3002", "result": "\u5728AAEC\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.44\u5b8fF1\uff08\u6bd4\u5148\u524d\u5de5\u4f5c\u9ad85.7\u5206\uff09\uff0c\u5728Argumentative MicroTexts\u5173\u7cfb\u5206\u7c7b\u4e0a\u8fbe\u52300.81\u5b8fF1\uff08\u6bd4\u5148\u524d\u7ed3\u679c\u9ad8\u7ea60.07\uff09\u3002", "conclusion": "\u7ed3\u6784\u5316\u8bba\u8bc1\u65b9\u6cd5\u4e3aAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u89e3\u91ca\u6846\u67b6\uff0c\u4f18\u4e8e\u4f20\u7edf\u53ef\u89e3\u91ca\u6027\u548cLLM\u751f\u6210\u89e3\u91ca\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "topic": "agent analysis"}}
{"id": "2510.04851", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04851", "abs": "https://arxiv.org/abs/2510.04851", "authors": ["Dongge Han", "Camille Couturier", "Daniel Madrigal Diaz", "Xuchao Zhang", "Victor R\u00fchle", "Saravan Rajmohan"], "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation", "comment": null, "summary": "We introduce LEGOMem, a modular procedural memory framework for multi-agent\nlarge language model (LLM) systems in workflow automation. LEGOMem decomposes\npast task trajectories into reusable memory units and flexibly allocates them\nacross orchestrators and task agents to support planning and execution. To\nexplore the design space of memory in multi-agent systems, we use LEGOMem as a\nlens and conduct a systematic study of procedural memory in multi-agent\nsystems, examining where memory should be placed, how it should be retrieved,\nand which agents benefit most. Experiments on the OfficeBench benchmark show\nthat orchestrator memory is critical for effective task decomposition and\ndelegation, while fine-grained agent memory improves execution accuracy. We\nfind that even teams composed of smaller language models can benefit\nsubstantially from procedural memory, narrowing the performance gap with\nstronger agents by leveraging prior execution traces for more accurate planning\nand tool use. These results position LEGOMem as both a practical framework for\nmemory-augmented agent systems and a research tool for understanding memory\ndesign in multi-agent workflow automation.", "AI": {"tldr": "LEGOMem\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u7684\u6a21\u5757\u5316\u7a0b\u5e8f\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u5386\u53f2\u4efb\u52a1\u8f68\u8ff9\u4e3a\u53ef\u91cd\u7528\u8bb0\u5fc6\u5355\u5143\uff0c\u7075\u6d3b\u5206\u914d\u7ed9\u7f16\u6392\u5668\u548c\u4efb\u52a1\u667a\u80fd\u4f53\u4ee5\u652f\u6301\u89c4\u5212\u548c\u6267\u884c\u3002", "motivation": "\u63a2\u7d22\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u8bb0\u5fc6\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u7814\u7a76\u8bb0\u5fc6\u5e94\u8be5\u653e\u5728\u54ea\u91cc\u3001\u5982\u4f55\u68c0\u7d22\u4ee5\u53ca\u54ea\u4e9b\u667a\u80fd\u4f53\u53d7\u76ca\u6700\u591a\u3002", "method": "\u4f7f\u7528LEGOMem\u4f5c\u4e3a\u7814\u7a76\u5de5\u5177\uff0c\u5728OfficeBench\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u7f16\u6392\u5668\u8bb0\u5fc6\u548c\u7ec6\u7c92\u5ea6\u667a\u80fd\u4f53\u8bb0\u5fc6\u7684\u4f5c\u7528\u3002", "result": "\u7f16\u6392\u5668\u8bb0\u5fc6\u5bf9\u6709\u6548\u4efb\u52a1\u5206\u89e3\u548c\u59d4\u6d3e\u81f3\u5173\u91cd\u8981\uff0c\u7ec6\u7c92\u5ea6\u667a\u80fd\u4f53\u8bb0\u5fc6\u63d0\u9ad8\u6267\u884c\u51c6\u786e\u6027\u3002\u5373\u4f7f\u7531\u8f83\u5c0f\u8bed\u8a00\u6a21\u578b\u7ec4\u6210\u7684\u56e2\u961f\u4e5f\u80fd\u901a\u8fc7\u7a0b\u5e8f\u8bb0\u5fc6\u663e\u8457\u53d7\u76ca\uff0c\u7f29\u5c0f\u4e0e\u66f4\u5f3a\u667a\u80fd\u4f53\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "LEGOMem\u65e2\u662f\u8bb0\u5fc6\u589e\u5f3a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b9e\u7528\u6846\u67b6\uff0c\u4e5f\u662f\u7406\u89e3\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u4e2d\u8bb0\u5fc6\u8bbe\u8ba1\u7684\u7814\u7a76\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.04862", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.04862", "abs": "https://arxiv.org/abs/2510.04862", "authors": ["Sam Earle", "Zehua Jiang", "Eugene Vinitsky", "Julian Togelius"], "title": "Video Game Level Design as a Multi-Agent Reinforcement Learning Problem", "comment": "11 pages, 7 tables, 5 figures, published as full technical paper at\n  the AAAI conference on Artificial Intelligence and Interactive Digital\n  Entertainment 2025", "summary": "Procedural Content Generation via Reinforcement Learning (PCGRL) offers a\nmethod for training controllable level designer agents without the need for\nhuman datasets, using metrics that serve as proxies for level quality as\nrewards. Existing PCGRL research focuses on single generator agents, but are\nbottlenecked by the need to frequently recalculate heuristics of level quality\nand the agent's need to navigate around potentially large maps. By framing\nlevel generation as a multi-agent problem, we mitigate the efficiency\nbottleneck of single-agent PCGRL by reducing the number of reward calculations\nrelative to the number of agent actions. We also find that multi-agent level\ngenerators are better able to generalize to out-of-distribution map shapes,\nwhich we argue is due to the generators' learning more local, modular design\npolicies. We conclude that treating content generation as a distributed,\nmulti-agent task is beneficial for generating functional artifacts at scale.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u7a0b\u5e8f\u5316\u5185\u5bb9\u751f\u6210\u91cd\u6784\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u667a\u80fd\u4f53\u534f\u4f5c\u89e3\u51b3\u5355\u667a\u80fd\u4f53PCGRL\u7684\u6548\u7387\u74f6\u9888\u548c\u6cdb\u5316\u80fd\u529b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709PCGRL\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u667a\u80fd\u4f53\u751f\u6210\u5668\uff0c\u4f46\u9762\u4e34\u9891\u7e41\u91cd\u65b0\u8ba1\u7b97\u542f\u53d1\u5f0f\u8d28\u91cf\u6307\u6807\u548c\u5728\u5927\u578b\u5730\u56fe\u4e2d\u5bfc\u822a\u7684\u6548\u7387\u74f6\u9888\u3002", "method": "\u5c06\u5173\u5361\u751f\u6210\u91cd\u6784\u4e3a\u591a\u667a\u80fd\u4f53\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u667a\u80fd\u4f53\u534f\u4f5c\u51cf\u5c11\u5956\u52b1\u8ba1\u7b97\u6b21\u6570\uff0c\u5b66\u4e60\u5c40\u90e8\u6a21\u5757\u5316\u8bbe\u8ba1\u7b56\u7565\u3002", "result": "\u591a\u667a\u80fd\u4f53\u5173\u5361\u751f\u6210\u5668\u5728\u6548\u7387\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u4e14\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u5206\u5e03\u5916\u5730\u56fe\u5f62\u72b6\uff0c\u5b66\u4e60\u5230\u66f4\u5c40\u90e8\u3001\u6a21\u5757\u5316\u7684\u8bbe\u8ba1\u7b56\u7565\u3002", "conclusion": "\u5c06\u5185\u5bb9\u751f\u6210\u89c6\u4e3a\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u6709\u5229\u4e8e\u5927\u89c4\u6a21\u751f\u6210\u529f\u80fd\u6027\u6e38\u620f\u5185\u5bb9\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04886", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04886", "abs": "https://arxiv.org/abs/2510.04886", "authors": ["Adi Banerjee", "Anirudh Nair", "Tarik Borogovac"], "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution", "comment": null, "summary": "Error attribution in Large Language Model (LLM) multi-agent systems presents\na significant challenge in debugging and improving collaborative AI systems.\nCurrent approaches to pinpointing agent and step level failures in interaction\ntraces - whether using all-at-once evaluation, step-by-step analysis, or binary\nsearch - fall short when analyzing complex patterns, struggling with both\naccuracy and consistency. We present ECHO (Error attribution through Contextual\nHierarchy and Objective consensus analysis), a novel algorithm that combines\nhierarchical context representation, objective analysis-based evaluation, and\nconsensus voting to improve error attribution accuracy. Our approach leverages\na positional-based leveling of contextual understanding while maintaining\nobjective evaluation criteria, ultimately reaching conclusions through a\nconsensus mechanism. Experimental results demonstrate that ECHO outperforms\nexisting methods across various multi-agent interaction scenarios, showing\nparticular strength in cases involving subtle reasoning errors and complex\ninterdependencies. Our findings suggest that leveraging these concepts of\nstructured, hierarchical context representation combined with consensus-based\nobjective decision-making, provides a more robust framework for error\nattribution in multi-agent systems.", "AI": {"tldr": "\u63d0\u51faECHO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u4e0a\u4e0b\u6587\u8868\u793a\u3001\u57fa\u4e8e\u76ee\u6807\u5206\u6790\u7684\u8bc4\u4f30\u548c\u5171\u8bc6\u6295\u7968\u6765\u6539\u8fdb\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u9519\u8bef\u5f52\u56e0\u51c6\u786e\u6027", "motivation": "\u5f53\u524dLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u9519\u8bef\u5f52\u56e0\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u6a21\u5f0f\u65f6\u5b58\u5728\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8c03\u8bd5\u5de5\u5177", "method": "\u7ed3\u5408\u5c42\u6b21\u5316\u4e0a\u4e0b\u6587\u8868\u793a\u3001\u57fa\u4e8e\u76ee\u6807\u5206\u6790\u7684\u8bc4\u4f30\u548c\u5171\u8bc6\u6295\u7968\u673a\u5236\uff0c\u5229\u7528\u57fa\u4e8e\u4f4d\u7f6e\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u5c42\u6b21\u5316\u65b9\u6cd5", "result": "ECHO\u5728\u5404\u79cd\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u6d89\u53ca\u5fae\u5999\u63a8\u7406\u9519\u8bef\u548c\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u7684\u6848\u4f8b\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa", "conclusion": "\u7ed3\u6784\u5316\u5c42\u6b21\u5316\u4e0a\u4e0b\u6587\u8868\u793a\u4e0e\u57fa\u4e8e\u5171\u8bc6\u7684\u5ba2\u89c2\u51b3\u7b56\u76f8\u7ed3\u5408\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9519\u8bef\u5f52\u56e0\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u6846\u67b6", "topic": "agent analysis"}}
{"id": "2510.03494", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03494", "abs": "https://arxiv.org/abs/2510.03494", "authors": ["Volodymyr Tkachuk", "Csaba Szepesv\u00e1ri", "Xiaoqi Tan"], "title": "Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^\u03c0$-Realizability and Concentrability", "comment": null, "summary": "We study finite-horizon offline reinforcement learning (RL) with function\napproximation for both policy evaluation and policy optimization. Prior work\nestablished that statistically efficient learning is impossible for either of\nthese problems when the only assumptions are that the data has good coverage\n(concentrability) and the state-action value function of every policy is\nlinearly realizable ($q^\\pi$-realizability) (Foster et al., 2021). Recently,\nTkachuk et al. (2024) gave a statistically efficient learner for policy\noptimization, if in addition the data is assumed to be given as trajectories.\nIn this work we present a statistically efficient learner for policy evaluation\nunder the same assumptions. Further, we show that the sample complexity of the\nlearner used by Tkachuk et al. (2024) for policy optimization can be improved\nby a tighter analysis.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6709\u9650\u65f6\u57df\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u8bc4\u4f30\u548c\u7b56\u7565\u4f18\u5316\u95ee\u9898\uff0c\u5728\u51fd\u6570\u903c\u8fd1\u6846\u67b6\u4e0b\u63d0\u51fa\u4e86\u7edf\u8ba1\u6709\u6548\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8868\u660e\uff0c\u5728\u4ec5\u5047\u8bbe\u6570\u636e\u8986\u76d6\u6027\u548c\u7b56\u7565\u4ef7\u503c\u51fd\u6570\u7ebf\u6027\u53ef\u5b9e\u73b0\u7684\u60c5\u51b5\u4e0b\uff0c\u7edf\u8ba1\u6709\u6548\u7684\u5b66\u4e60\u662f\u4e0d\u53ef\u80fd\u7684\u3002\u6700\u8fd1\u7684\u7814\u7a76\u5728\u5047\u8bbe\u6570\u636e\u4ee5\u8f68\u8ff9\u5f62\u5f0f\u63d0\u4f9b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u7b56\u7565\u4f18\u5316\u7684\u6709\u6548\u5b66\u4e60\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u7b56\u7565\u8bc4\u4f30\u7684\u7c7b\u4f3c\u95ee\u9898\u3002", "method": "\u5728\u6570\u636e\u4ee5\u8f68\u8ff9\u5f62\u5f0f\u63d0\u4f9b\u4e14\u6ee1\u8db3\u8986\u76d6\u6027\u548cq\u03c0-\u53ef\u5b9e\u73b0\u6027\u7684\u5047\u8bbe\u4e0b\uff0c\u63d0\u51fa\u4e86\u7edf\u8ba1\u6709\u6548\u7684\u7b56\u7565\u8bc4\u4f30\u5b66\u4e60\u5668\uff0c\u5e76\u5bf9\u73b0\u6709\u7b56\u7565\u4f18\u5316\u5b66\u4e60\u5668\u7684\u6837\u672c\u590d\u6742\u5ea6\u8fdb\u884c\u4e86\u66f4\u4e25\u683c\u7684\u5206\u6790\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u5728\u8f68\u8ff9\u6570\u636e\u5047\u8bbe\u4e0b\u7684\u7edf\u8ba1\u6709\u6548\u7b56\u7565\u8bc4\u4f30\u5b66\u4e60\u5668\uff0c\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u7b56\u7565\u4f18\u5316\u5b66\u4e60\u5668\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\u3002", "conclusion": "\u5728\u8f68\u8ff9\u6570\u636e\u5047\u8bbe\u4e0b\uff0c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u8bc4\u4f30\u548c\u7b56\u7565\u4f18\u5316\u90fd\u53ef\u4ee5\u5b9e\u73b0\u7edf\u8ba1\u6709\u6548\u7684\u5b66\u4e60\uff0c\u4e14\u6837\u672c\u590d\u6742\u5ea6\u53ef\u4ee5\u5f97\u5230\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.03508", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03508", "abs": "https://arxiv.org/abs/2510.03508", "authors": ["Lunjun Zhang", "Shuo Han", "Hanrui Lyu", "Bradly C Stadie"], "title": "D2 Actor Critic: Diffusion Actor Meets Distributional Critic", "comment": null, "summary": "We introduce D2AC, a new model-free reinforcement learning (RL) algorithm\ndesigned to train expressive diffusion policies online effectively. At its core\nis a policy improvement objective that avoids the high variance of typical\npolicy gradients and the complexity of backpropagation through time. This\nstable learning process is critically enabled by our second contribution: a\nrobust distributional critic, which we design through a fusion of\ndistributional RL and clipped double Q-learning. The resulting algorithm is\nhighly effective, achieving state-of-the-art performance on a benchmark of\neighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains,\nspanning both dense-reward and goal-conditioned RL scenarios. Beyond standard\nbenchmarks, we also evaluate a biologically motivated predator-prey task to\nexamine the behavioral robustness and generalization capacity of our approach.", "AI": {"tldr": "D2AC\u662f\u4e00\u79cd\u65b0\u7684\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u5206\u5e03RL\u548c\u88c1\u526a\u53ccQ\u5b66\u4e60\u6765\u8bad\u7ec3\u8868\u8fbe\u6027\u6269\u6563\u7b56\u7565\uff0c\u572818\u4e2a\u56f0\u96beRL\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5728\u7ebf\u8bad\u7ec3\u8868\u8fbe\u6027\u6269\u6563\u7b56\u7565\u7684\u7b97\u6cd5\uff0c\u907f\u514d\u4f20\u7edf\u7b56\u7565\u68af\u5ea6\u7684\u9ad8\u65b9\u5dee\u548c\u901a\u8fc7\u65f6\u95f4\u53cd\u5411\u4f20\u64ad\u7684\u590d\u6742\u6027\u3002", "method": "\u4f7f\u7528\u907f\u514d\u9ad8\u65b9\u5dee\u7684\u7b56\u7565\u6539\u8fdb\u76ee\u6807\uff0c\u7ed3\u5408\u878d\u5408\u5206\u5e03RL\u548c\u88c1\u526a\u53ccQ\u5b66\u4e60\u7684\u9c81\u68d2\u5206\u5e03\u8bc4\u8bba\u5bb6\u3002", "result": "\u572818\u4e2a\u56f0\u96beRL\u4efb\u52a1\uff08\u5305\u62ecHumanoid\u3001Dog\u548cShadow Hand\u7b49\u9886\u57df\uff09\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6db5\u76d6\u5bc6\u96c6\u5956\u52b1\u548c\u76ee\u6807\u6761\u4ef6RL\u573a\u666f\u3002", "conclusion": "D2AC\u7b97\u6cd5\u5728\u6807\u51c6\u57fa\u51c6\u548c\u751f\u7269\u542f\u53d1\u7684\u6355\u98df\u8005-\u730e\u7269\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u884c\u4e3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04678", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04678", "abs": "https://arxiv.org/abs/2510.04678", "authors": ["Zhanfeng Mo", "Xingxuan Li", "Yuntao Chen", "Lidong Bing"], "title": "Multi-Agent Tool-Integrated Policy Optimization", "comment": "Work in progress", "summary": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated\nplanning for knowledge-intensive and complex reasoning tasks. Existing\nimplementations typically rely on a single agent, but they suffer from limited\ncontext length and noisy tool responses. A natural solution is to adopt a\nmulti-agent framework with planner- and worker-agents to manage context.\nHowever, no existing methods support effective reinforcement learning\npost-training of tool-integrated multi-agent frameworks. To address this gap,\nwe propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which\nenables distinct roles (planner and worker) to be trained within a single LLM\ninstance using role-specific prompts via reinforcement learning. MATPO is\nderived from a principled credit assignment mechanism across planner and worker\nrollouts. This design eliminates the need to deploy multiple LLMs, which would\nbe memory-intensive, while preserving the benefits of specialization.\nExperiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently\noutperforms single-agent baselines by an average of 18.38% relative improvement\nin performance and exhibits greater robustness to noisy tool outputs. Our\nfindings highlight the effectiveness of unifying multiple agent roles within a\nsingle LLM and provide practical insights for stable and efficient multi-agent\nRL training.", "AI": {"tldr": "\u63d0\u51faMATPO\u65b9\u6cd5\uff0c\u5728\u5355\u4e2aLLM\u5b9e\u4f8b\u4e2d\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u89c4\u5212\u8005\u548c\u5de5\u4f5c\u8005\u4e24\u79cd\u89d2\u8272\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5de5\u5177\u96c6\u6210\u6846\u67b6\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u566a\u58f0\u5de5\u5177\u54cd\u5e94\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u5b58\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u548c\u5de5\u5177\u54cd\u5e94\u566a\u58f0\u95ee\u9898\uff0c\u800c\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7f3a\u4e4f\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "MATPO\u901a\u8fc7\u89d2\u8272\u7279\u5b9a\u63d0\u793a\u5728\u5355\u4e2aLLM\u5b9e\u4f8b\u4e2d\u8bad\u7ec3\u89c4\u5212\u8005\u548c\u5de5\u4f5c\u8005\u89d2\u8272\uff0c\u91c7\u7528\u57fa\u4e8e\u539f\u5219\u7684\u4fe1\u7528\u5206\u914d\u673a\u5236\u8de8\u89d2\u8272\u8f68\u8ff9\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728GAIA-text\u3001WebWalkerQA\u548cFRAMES\u6570\u636e\u96c6\u4e0a\uff0cMATPO\u5e73\u5747\u76f8\u5bf9\u6027\u80fd\u63d0\u534718.38%\uff0c\u5bf9\u566a\u58f0\u5de5\u5177\u8f93\u51fa\u8868\u73b0\u51fa\u66f4\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u5728\u5355\u4e2aLLM\u4e2d\u7edf\u4e00\u591a\u4e2a\u667a\u80fd\u4f53\u89d2\u8272\u662f\u6709\u6548\u7684\uff0c\u4e3a\u7a33\u5b9a\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04952", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.04952", "abs": "https://arxiv.org/abs/2510.04952", "authors": ["Ailiya Borjigin", "Cong He"], "title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits", "comment": "22 pages, 2 figures", "summary": "We present a cross-market algorithmic trading system that balances execution\nquality with rigorous compliance enforcement. The architecture comprises a\nhigh-level planner, a reinforcement learning execution agent, and an\nindependent compliance agent. We formulate trade execution as a constrained\nMarkov decision process with hard constraints on participation limits, price\nbands, and self-trading avoidance. The execution agent is trained with proximal\npolicy optimization, while a runtime action-shield projects any unsafe action\ninto a feasible set. To support auditability without exposing proprietary\nsignals, we add a zero-knowledge compliance audit layer that produces\ncryptographic proofs that all actions satisfied the constraints. We evaluate in\na multi-venue, ABIDES-based simulator and compare against standard baselines\n(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and\nvariance while exhibiting no observed constraint violations across stress\nscenarios including elevated latency, partial fills, compliance module\ntoggling, and varying constraint limits. We report effects at the 95%\nconfidence level using paired t-tests and examine tail risk via CVaR. We\nsituate the work at the intersection of optimal execution, safe reinforcement\nlearning, regulatory technology, and verifiable AI, and discuss ethical\nconsiderations, limitations (e.g., modeling assumptions and computational\noverhead), and paths to real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u5408\u89c4\u7ea6\u675f\u7684\u8de8\u5e02\u573a\u7b97\u6cd5\u4ea4\u6613\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u786e\u4fdd\u4ea4\u6613\u6267\u884c\u8d28\u91cf\u540c\u65f6\u6ee1\u8db3\u76d1\u7ba1\u8981\u6c42", "motivation": "\u89e3\u51b3\u7b97\u6cd5\u4ea4\u6613\u4e2d\u6267\u884c\u8d28\u91cf\u4e0e\u5408\u89c4\u76d1\u7ba1\u7684\u5e73\u8861\u95ee\u9898\uff0c\u786e\u4fdd\u4ea4\u6613\u7b56\u7565\u5728\u6ee1\u8db3\u53c2\u4e0e\u9650\u5236\u3001\u4ef7\u683c\u533a\u95f4\u548c\u81ea\u4ea4\u6613\u907f\u514d\u7b49\u786c\u7ea6\u675f\u4e0b\u7684\u6700\u4f18\u6267\u884c", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff1a\u9ad8\u5c42\u89c4\u5212\u5668\u3001\u5f3a\u5316\u5b66\u4e60\u6267\u884c\u4ee3\u7406\u548c\u72ec\u7acb\u5408\u89c4\u4ee3\u7406\u3002\u5c06\u4ea4\u6613\u6267\u884c\u5efa\u6a21\u4e3a\u5e26\u7ea6\u675f\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\u6267\u884c\u4ee3\u7406\uff0c\u8fd0\u884c\u65f6\u52a8\u4f5c\u5c4f\u853d\u786e\u4fdd\u884c\u52a8\u53ef\u884c\u6027\uff0c\u5e76\u6dfb\u52a0\u96f6\u77e5\u8bc6\u5408\u89c4\u5ba1\u8ba1\u5c42", "result": "\u5728ABIDES\u591a\u573a\u6240\u6a21\u62df\u5668\u4e2d\u8bc4\u4f30\uff0c\u76f8\u6bd4TWAP\u3001VWAP\u7b49\u57fa\u51c6\u65b9\u6cd5\uff0c\u5b66\u4e60\u7b56\u7565\u964d\u4f4e\u4e86\u6267\u884c\u5dee\u989d\u548c\u65b9\u5dee\uff0c\u5728\u538b\u529b\u6d4b\u8bd5\u4e2d\u672a\u89c2\u5bdf\u5230\u7ea6\u675f\u8fdd\u53cd\uff0c\u7edf\u8ba1\u663e\u8457\u6027\u8fbe95%\u7f6e\u4fe1\u6c34\u5e73", "conclusion": "\u8be5\u5de5\u4f5c\u5904\u4e8e\u6700\u4f18\u6267\u884c\u3001\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u3001\u76d1\u7ba1\u6280\u672f\u548c\u53ef\u9a8c\u8bc1AI\u7684\u4ea4\u53c9\u9886\u57df\uff0c\u8ba8\u8bba\u4e86\u4f26\u7406\u8003\u91cf\u3001\u5efa\u6a21\u5047\u8bbe\u548c\u8ba1\u7b97\u5f00\u9500\u7b49\u9650\u5236\uff0c\u4ee5\u53ca\u5b9e\u9645\u90e8\u7f72\u8def\u5f84", "topic": "agentic reinforcement learning"}}
{"id": "2510.03515", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03515", "abs": "https://arxiv.org/abs/2510.03515", "authors": ["Lianghuan Huang", "Sagnik Anupam", "Insup Lee", "Shuo Li", "Osbert Bastani"], "title": "RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a promising strategy for\nfinetuning small language models (SLMs) to solve targeted tasks such as math\nand coding. However, RL algorithms tend to be resource-intensive, taking a\nsignificant amount of time to train. We propose RAPID, a novel RL algorithm\nthat can substantially reduce the running time of RL. Our key insight is that\nRL tends to be costly due to the need to perform both inference and\nbackpropagation during training. To maximize use of computational resources,\nour algorithm performs inference in large batches, and then performs off-policy\npolicy gradient updates in mini-batches. For off-policy updates, we incorporate\ngroup advantage estimation into the policy gradient algorithm, and derive an\nimportance weighted estimator to correct for the bias arising from off-policy\nlearning. Our experiments demonstrate that our algorithm can reduce running\ntime by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms\nwhile maintaining similar or better accuracy.", "AI": {"tldr": "\u63d0\u51faRAPID\u7b97\u6cd5\uff0c\u901a\u8fc7\u5927\u6279\u91cf\u63a8\u7406\u548c\u5c0f\u6279\u91cf\u79bb\u7b56\u7565\u66f4\u65b0\uff0c\u51cf\u5c11RL\u8bad\u7ec3\u65f6\u95f411%-34%\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027", "motivation": "RL\u7b97\u6cd5\u5728\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u8d44\u6e90\u5bc6\u96c6\u3001\u8bad\u7ec3\u65f6\u95f4\u957f\uff0c\u9700\u8981\u4f18\u5316\u8ba1\u7b97\u6548\u7387", "method": "\u4f7f\u7528\u5927\u6279\u91cf\u63a8\u7406\u548c\u5c0f\u6279\u91cf\u79bb\u7b56\u7565\u7b56\u7565\u68af\u5ea6\u66f4\u65b0\uff0c\u7ed3\u5408\u7ec4\u4f18\u52bf\u4f30\u8ba1\u548c\u91cd\u8981\u6027\u52a0\u6743\u4f30\u8ba1\u5668\u7ea0\u6b63\u504f\u5dee", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1111%-34%\uff0c\u51c6\u786e\u7387\u76f8\u4f3c\u6216\u66f4\u597d", "conclusion": "RAPID\u7b97\u6cd5\u80fd\u663e\u8457\u51cf\u5c11RL\u8bad\u7ec3\u65f6\u95f4\u800c\u4e0d\u727a\u7272\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2510.03520", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03520", "abs": "https://arxiv.org/abs/2510.03520", "authors": ["Kartik Pandit", "Sourav Ganguly", "Arnesh Banerjee", "Shaahin Angizi", "Arnob Ghosh"], "title": "Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models", "comment": null, "summary": "Ensuring safety is a foundational requirement for large language models\n(LLMs). Achieving an appropriate balance between enhancing the utility of model\noutputs and mitigating their potential for harm is a complex and persistent\nchallenge. Contemporary approaches frequently formalize this problem within the\nframework of Constrained Markov Decision Processes (CMDPs) and employ\nestablished CMDP optimization techniques. However, these methods exhibit two\nnotable limitations. First, their reliance on reward and cost functions renders\nperformance highly sensitive to the underlying scoring mechanism, which must\ncapture semantic meaning rather than being triggered by superficial keywords.\nSecond, CMDP-based training entails tuning dual-variable, a process that is\nboth computationally expensive and does not provide any provable safety\nguarantee for a fixed dual variable that can be exploitable through adversarial\njailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF\n(CS-RLHF) that introduces a cost model trained on a large-scale corpus to\nassign semantically grounded safety scores. In contrast to the lagrangian-based\napproach, CS-RLHF adopts a rectified penalty-based formulation. This design\ndraws on the theory of exact penalty functions in constrained optimization,\nwherein constraint satisfaction is enforced directly through a suitably chosen\npenalty term. With an appropriately scaled penalty, feasibility of the safety\nconstraints can be guaranteed at the optimizer, eliminating the need for\ndual-variable updates. Empirical evaluation demonstrates that CS-RLHF\noutperforms state-of-the-art LLM model responses rendering at-least 5 times\nefficient against nominal and jail-breaking prompts", "AI": {"tldr": "CS-RLHF\u662f\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u60e9\u7f5a\u7684\u516c\u5f0f\u66ff\u4ee3\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\uff0c\u4f7f\u7528\u5728\u5927\u89c4\u6a21\u8bed\u6599\u4e0a\u8bad\u7ec3\u7684\u6210\u672c\u6a21\u578b\u6765\u5206\u914d\u8bed\u4e49\u5b89\u5168\u5206\u6570\uff0c\u65e0\u9700\u53cc\u53d8\u91cf\u66f4\u65b0\u5373\u53ef\u4fdd\u8bc1\u5b89\u5168\u7ea6\u675f\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCMDP\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5bf9\u8bc4\u5206\u673a\u5236\u9ad8\u5ea6\u654f\u611f\uff0c\u4ee5\u53ca\u53cc\u53d8\u91cf\u8c03\u4f18\u8fc7\u7a0b\u8ba1\u7b97\u6602\u8d35\u4e14\u65e0\u6cd5\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u4fdd\u8bc1\u3002", "method": "\u5f15\u5165CS-RLHF\u65b9\u6cd5\uff0c\u4f7f\u7528\u5728\u5927\u89c4\u6a21\u8bed\u6599\u4e0a\u8bad\u7ec3\u7684\u6210\u672c\u6a21\u578b\u5206\u914d\u8bed\u4e49\u5b89\u5168\u5206\u6570\uff0c\u91c7\u7528\u57fa\u4e8e\u60e9\u7f5a\u7684\u516c\u5f0f\u8bbe\u8ba1\uff0c\u57fa\u4e8e\u7ea6\u675f\u4f18\u5316\u4e2d\u7684\u7cbe\u786e\u60e9\u7f5a\u51fd\u6570\u7406\u8bba\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793aCS-RLHF\u4f18\u4e8e\u6700\u5148\u8fdb\u7684LLM\u6a21\u578b\u54cd\u5e94\uff0c\u5728\u6b63\u5e38\u548c\u8d8a\u72f1\u63d0\u793a\u4e0b\u6548\u7387\u81f3\u5c11\u63d0\u9ad85\u500d\u3002", "conclusion": "CS-RLHF\u901a\u8fc7\u57fa\u4e8e\u60e9\u7f5a\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5b89\u5168\u4fdd\u8bc1\u548c\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.05025", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.05025", "abs": "https://arxiv.org/abs/2510.05025", "authors": ["Kuofeng Gao", "Yiming Li", "Chao Du", "Xin Wang", "Xingjun Ma", "Shu-Tao Xia", "Tianyu Pang"], "title": "Imperceptible Jailbreaking against Large Language Models", "comment": null, "summary": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eUnicode\u53d8\u4f53\u9009\u62e9\u5668\u7684\u4e0d\u53ef\u5bdf\u89c9\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6076\u610f\u95ee\u9898\u540e\u9644\u52a0\u4e0d\u53ef\u89c1\u7684\u53d8\u4f53\u9009\u62e9\u5668\u5b57\u7b26\uff0c\u5728\u4e0d\u6539\u53d8\u89c6\u89c9\u663e\u793a\u7684\u60c5\u51b5\u4e0b\u6539\u53d8tokenization\uff0c\u4ece\u800c\u8bf1\u5bfcAI\u6a21\u578b\u4ea7\u751f\u6709\u5bb3\u54cd\u5e94\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u4f9d\u8d56\u4e0d\u53ef\u5bdf\u89c9\u7684\u5bf9\u6297\u6270\u52a8\uff0c\u800c\u6587\u672c\u6a21\u6001\u653b\u51fb\u901a\u5e38\u9700\u8981\u53ef\u89c1\u4fee\u6539\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6587\u672c\u6a21\u6001\u4e2d\u4e0d\u53ef\u5bdf\u89c9\u7684\u8d8a\u72f1\u653b\u51fb\u53ef\u80fd\u6027\u3002", "method": "\u5229\u7528Unicode\u53d8\u4f53\u9009\u62e9\u5668\u5b57\u7b26\uff0c\u6784\u5efa\u4e0d\u53ef\u89c1\u7684\u5bf9\u6297\u540e\u7f00\uff1b\u63d0\u51fa\u94fe\u5f0f\u641c\u7d22\u6d41\u6c34\u7ebf\u6765\u751f\u6210\u6b64\u7c7b\u5bf9\u6297\u540e\u7f00\uff1b\u901a\u8fc7\u6539\u53d8tokenization\u4f46\u4e0d\u6539\u53d8\u89c6\u89c9\u663e\u793a\u6765\u5b9e\u65bd\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5bf9\u56db\u4e2a\u5bf9\u9f50\u7684LLM\u5b9e\u73b0\u4e86\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u4e14\u4e0d\u4ea7\u751f\u4efb\u4f55\u53ef\u89c1\u7684\u63d0\u793a\u4fee\u6539\u3002", "conclusion": "\u8bc1\u660e\u4e86\u6587\u672c\u6a21\u6001\u4e2d\u4e0d\u53ef\u5bdf\u89c9\u8d8a\u72f1\u653b\u51fb\u7684\u53ef\u884c\u6027\uff0c\u63ed\u793a\u4e86\u57fa\u4e8eUnicode\u5b57\u7b26\u7684\u6f5c\u5728\u5b89\u5168\u5a01\u80c1\u3002", "topic": "agent analysis"}}
{"id": "2510.03636", "categories": ["cs.LG", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03636", "abs": "https://arxiv.org/abs/2510.03636", "authors": ["Rabeya Amin Jhuma", "Mostafa Mohaimen Akand Faisal"], "title": "From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse", "comment": null, "summary": "This study explored how in-context learning (ICL) in large language models\ncan be disrupted by data poisoning attacks in the setting of public health\nsentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small\nadversarial perturbations such as synonym replacement, negation insertion, and\nrandomized perturbation were introduced into the support examples. Even these\nminor manipulations caused major disruptions, with sentiment labels flipping in\nup to 67% of cases. To address this, a Spectral Signature Defense was applied,\nwhich filtered out poisoned examples while keeping the data's meaning and\nsentiment intact. After defense, ICL accuracy remained steady at around 46.7%,\nand logistic regression validation reached 100% accuracy, showing that the\ndefense successfully preserved the dataset's integrity. Overall, the findings\nextend prior theoretical studies of ICL poisoning to a practical, high-stakes\nsetting in public health discourse analysis, highlighting both the risks and\npotential defenses for robust LLM deployment. This study also highlights the\nfragility of ICL under attack and the value of spectral defenses in making AI\nsystems more reliable for health-related social media monitoring.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u516c\u5171\u536b\u751f\u60c5\u611f\u5206\u6790\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u5982\u4f55\u53d7\u5230\u6570\u636e\u6295\u6bd2\u653b\u51fb\u7684\u7834\u574f\u3002\u901a\u8fc7\u5f15\u5165\u5fae\u5c0f\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u5bfc\u81f4\u9ad8\u8fbe67%\u7684\u60c5\u611f\u6807\u7b7e\u7ffb\u8f6c\uff0c\u4f46\u901a\u8fc7\u8c31\u7b7e\u540d\u9632\u5fa1\u6210\u529f\u4fdd\u6301\u4e86\u6570\u636e\u96c6\u5b8c\u6574\u6027\u3002", "motivation": "\u5c06\u5148\u524d\u5173\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u6295\u6bd2\u7684\u7406\u8bba\u7814\u7a76\u6269\u5c55\u5230\u516c\u5171\u536b\u751f\u8bdd\u8bed\u5206\u6790\u8fd9\u4e00\u5b9e\u9645\u9ad8\u98ce\u9669\u573a\u666f\uff0c\u63ed\u793aLLM\u90e8\u7f72\u7684\u98ce\u9669\u548c\u9632\u5fa1\u6f5c\u529b\u3002", "method": "\u5728\u4eba\u7c7b\u504f\u80ba\u75c5\u6bd2\u63a8\u6587\u6570\u636e\u4e2d\u5f15\u5165\u540c\u4e49\u8bcd\u66ff\u6362\u3001\u5426\u5b9a\u63d2\u5165\u548c\u968f\u673a\u6270\u52a8\u7b49\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u7136\u540e\u5e94\u7528\u8c31\u7b7e\u540d\u9632\u5fa1\u6765\u8fc7\u6ee4\u6295\u6bd2\u6837\u672c\u3002", "result": "\u5fae\u5c0f\u6270\u52a8\u5bfc\u81f4\u9ad8\u8fbe67%\u7684\u60c5\u611f\u6807\u7b7e\u7ffb\u8f6c\uff1b\u9632\u5fa1\u540e\u4e0a\u4e0b\u6587\u5b66\u4e60\u51c6\u786e\u7387\u7a33\u5b9a\u572846.7%\uff0c\u903b\u8f91\u56de\u5f52\u9a8c\u8bc1\u8fbe\u5230100%\u51c6\u786e\u7387\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u653b\u51fb\u4e0b\u8868\u73b0\u8106\u5f31\uff0c\u4f46\u8c31\u9632\u5fa1\u80fd\u591f\u63d0\u9ad8AI\u7cfb\u7edf\u5728\u5065\u5eb7\u76f8\u5173\u793e\u4ea4\u5a92\u4f53\u76d1\u6d4b\u4e2d\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.03643", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03643", "abs": "https://arxiv.org/abs/2510.03643", "authors": ["Nicholas Carter", "Arkaprava Gupta", "Prateek Ganguli", "Benedikt Dietrich", "Vibhor Krishna", "Samarjit Chakraborty"], "title": "In-Vivo Training for Deep Brain Stimulation", "comment": null, "summary": "Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's\nDisease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL\nagents modulating the stimulation frequency and amplitude. But, these models\nrely on biomarkers that are not measurable in patients and are only present in\nbrain-on-chip (BoC) simulations. In this work, we present an RL-based DBS\napproach that adapts these stimulation parameters according to brain activity\nmeasurable in vivo. Using a TD3 based RL agent trained on a model of the basal\nganglia region of the brain, we see a greater suppression of biomarkers\ncorrelated with PD severity compared to modern clinical DBS implementations.\nOur agent outperforms the standard clinical approaches in suppressing PD\nbiomarkers while relying on information that can be measured in a real world\nenvironment, thereby opening up the possibility of training personalized RL\nagents specific to individual patient needs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6df1\u90e8\u8111\u523a\u6fc0\u65b9\u6cd5\uff0c\u4f7f\u7528\u53ef\u6d4b\u91cf\u7684\u8111\u6d3b\u52a8\u6570\u636e\u6765\u8c03\u6574\u523a\u6fc0\u53c2\u6570\uff0c\u5728\u5e15\u91d1\u68ee\u75c5\u751f\u7269\u6807\u5fd7\u7269\u6291\u5236\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u4e34\u5e8a\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684DBS\u65b9\u6cd5\u4f9d\u8d56\u65e0\u6cd5\u5728\u60a3\u8005\u4f53\u5185\u6d4b\u91cf\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4ec5\u9002\u7528\u4e8e\u8111\u82af\u7247\u6a21\u62df\u73af\u5883\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u4f7f\u7528\u7684\u81ea\u9002\u5e94DBS\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528TD3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u57fa\u5e95\u795e\u7ecf\u8282\u8111\u533a\u6a21\u578b\u4e0a\u8bad\u7ec3RL\u667a\u80fd\u4f53\uff0c\u6839\u636e\u53ef\u6d4b\u91cf\u7684\u8111\u6d3b\u52a8\u81ea\u9002\u5e94\u8c03\u6574\u523a\u6fc0\u9891\u7387\u548c\u5e45\u5ea6\u53c2\u6570\u3002", "result": "\u4e0e\u73b0\u6709\u4e34\u5e8aDBS\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u667a\u80fd\u4f53\u5728\u6291\u5236\u4e0e\u5e15\u91d1\u68ee\u75c5\u4e25\u91cd\u7a0b\u5ea6\u76f8\u5173\u7684\u751f\u7269\u6807\u5fd7\u7269\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u4ec5\u4f9d\u8d56\u53ef\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6d4b\u91cf\u7684\u4fe1\u606f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bad\u7ec3\u9488\u5bf9\u4e2a\u4f53\u60a3\u8005\u9700\u6c42\u7684\u4e2a\u6027\u5316RL\u667a\u80fd\u4f53\u5f00\u8f9f\u4e86\u53ef\u80fd\u6027\uff0c\u4f7f\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684DBS\u6280\u672f\u66f4\u63a5\u8fd1\u4e34\u5e8a\u5e94\u7528\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.05069", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05069", "abs": "https://arxiv.org/abs/2510.05069", "authors": ["Dachuan Shi", "Abedelkadir Asi", "Keying Li", "Xiangchi Yuan", "Leyan Pan", "Wenke Lee", "Wen Xiao"], "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs", "comment": "Code: https://github.com/sdc17/SwiReasoning, Website:\n  https://swireasoning.github.io/", "summary": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten.", "AI": {"tldr": "SwiReasoning\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5207\u6362\u663e\u5f0f\u548c\u6f5c\u5728\u63a8\u7406\u6765\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u548ctoken\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u7eaf\u6f5c\u5728\u63a8\u7406\u5e26\u6765\u7684\u641c\u7d22\u5206\u5e03\u6269\u6563\u3001\u6982\u7387\u8d28\u91cf\u5206\u6563\u548c\u6536\u655b\u56f0\u96be\u95ee\u9898\uff0c\u4ee5\u53ca\u8fc7\u5ea6\u601d\u8003\u5bfc\u81f4\u7684token\u6d6a\u8d39\u3002", "method": "\u57fa\u4e8e\u71b5\u8d8b\u52bf\u4f30\u8ba1\u5757\u7ea7\u7f6e\u4fe1\u5ea6\uff0c\u52a8\u6001\u5207\u6362\u663e\u5f0f\u548c\u6f5c\u5728\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u9650\u5236\u601d\u8003\u5757\u5207\u6362\u6b21\u6570\u6765\u63a7\u5236\u8fc7\u5ea6\u601d\u8003\u3002", "result": "\u5728\u6570\u5b66\u548cSTEM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53471.5%-2.8%\uff0c\u5728\u53d7\u9650\u9884\u7b97\u4e0btoken\u6548\u7387\u63d0\u534756%-79%\u3002", "conclusion": "SwiReasoning\u901a\u8fc7\u52a8\u6001\u63a8\u7406\u6a21\u5f0f\u5207\u6362\u6709\u6548\u89e3\u51b3\u4e86\u6f5c\u5728\u63a8\u7406\u7684\u6536\u655b\u95ee\u9898\u548c\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2510.03722", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03722", "abs": "https://arxiv.org/abs/2510.03722", "authors": ["Qianxin Yi", "Shao-Bo Lin", "Jun Fan", "Yao Wang"], "title": "Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach", "comment": null, "summary": "Reinforcement learning (RL) has been widely applied to sequential decision\nmaking, where interpretability and performance are both critical for practical\nadoption. Current approaches typically focus on performance and rely on post\nhoc explanations to account for interpretability. Different from these\napproaches, we focus on designing an interpretability-oriented yet\nperformance-enhanced RL approach. Specifically, we propose a spectral based\nlinear RL method that extends the ridge regression-based approach through a\nspectral filter function. The proposed method clarifies the role of\nregularization in controlling estimation error and further enables the design\nof an adaptive regularization parameter selection strategy guided by the\nbias-variance trade-off principle. Theoretical analysis establishes\nnear-optimal bounds for both parameter estimation and generalization error.\nExtensive experiments on simulated environments and real-world datasets from\nKuaishou and Taobao demonstrate that our method either outperforms or matches\nexisting baselines in decision quality. We also conduct interpretability\nanalyses to illustrate how the learned policies make decisions, thereby\nenhancing user trust. These results highlight the potential of our approach to\nbridge the gap between RL theory and practical decision making, providing\ninterpretability, accuracy, and adaptability in management contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8c31\u6ee4\u6ce2\u7684\u7ebf\u6027\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6b63\u5219\u5316\u53c2\u6570\u9009\u62e9\u7b56\u7565\uff0c\u5728\u4fdd\u8bc1\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u5347\u51b3\u7b56\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6027\u80fd\uff0c\u4f9d\u8d56\u4e8b\u540e\u89e3\u91ca\u6765\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u4ee5\u53ef\u89e3\u91ca\u6027\u4e3a\u5bfc\u5411\u4e14\u6027\u80fd\u589e\u5f3a\u7684RL\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8c31\u6ee4\u6ce2\u7684\u7ebf\u6027RL\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u5cad\u56de\u5f52\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c31\u6ee4\u6ce2\u51fd\u6570\u9610\u660e\u6b63\u5219\u5316\u5728\u63a7\u5236\u4f30\u8ba1\u8bef\u5dee\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u7684\u81ea\u9002\u5e94\u6b63\u5219\u5316\u53c2\u6570\u9009\u62e9\u7b56\u7565\u3002", "result": "\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4e86\u53c2\u6570\u4f30\u8ba1\u548c\u6cdb\u5316\u8bef\u5dee\u7684\u8fd1\u4e4e\u6700\u4f18\u8fb9\u754c\u3002\u5728\u5feb\u624b\u548c\u6dd8\u5b9d\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51b3\u7b56\u8d28\u91cf\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6f5c\u529b\u5f25\u5408RL\u7406\u8bba\u4e0e\u5b9e\u9645\u51b3\u7b56\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5728\u7ba1\u7406\u573a\u666f\u4e2d\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3001\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.03760", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03760", "abs": "https://arxiv.org/abs/2510.03760", "authors": ["Ping Guo", "Chenyu Zhu", "Siyuan Chen", "Fei Liu", "Xi Lin", "Zhichao Lu", "Qingfu Zhang"], "title": "EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models", "comment": "Under Review of ICLR 2026", "summary": "CUDA kernel optimization has become a critical bottleneck for AI performance,\nas deep learning training and inference efficiency directly depends on highly\noptimized GPU kernels.\n  Despite the promise of Large Language Models (LLMs) for automating kernel\noptimization, this field suffers from a fragmented ecosystem of isolated and\nincomparable approaches with unclear problem formulations.\n  Furthermore, general-purpose LLM code evolution methods cannot meet strict\ncorrectness requirements of CUDA kernel optimization.\n  We address these fundamental challenges by first formalizing CUDA kernel\noptimization as a code optimization task with a clear objective, constraints,\nand evaluation metrics.\n  We then establish the first systematic LLM-based code evolution framework,\nEvoEngineer, that provides guidance for designing and adapting optimization\nstrategies to achieve a balance between performance and correctness.\n  Finally, we implement a kernel optimization system based on this framework\nand conduct extensive experiments on 91 real-world CUDA kernels.\n  Our results demonstrate that EvoEngineer achieves a principled balance\nbetween performance and correctness, with the highest averaged median speedup\nof \\textbf{2.72}$\\times$ over baseline CUDA kernels and a code validity rate of\n\\textbf{69.8}\\%, outperforming existing methods on both dimensions.\n  Our method achieves a maximum speedup of \\textbf{36.75}$\\times$ among all\noperations over PyTorch kernels and delivers the highest speedup on \\textbf{28}\n(\\textbf{56.0\\%}) of 50 operations that achieve over \\textbf{2$\\times$}\nacceleration.", "AI": {"tldr": "EvoEngineer\u662f\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684LLM\u4ee3\u7801\u8fdb\u5316\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8eCUDA\u5185\u6838\u4f18\u5316\uff0c\u572891\u4e2a\u771f\u5b9eCUDA\u5185\u6838\u4e0a\u5b9e\u73b0\u4e862.72\u500d\u7684\u5e73\u5747\u4e2d\u4f4d\u6570\u52a0\u901f\u548c69.8%\u7684\u4ee3\u7801\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3CUDA\u5185\u6838\u4f18\u5316\u9886\u57df\u751f\u6001\u7cfb\u7edf\u788e\u7247\u5316\u3001\u95ee\u9898\u5b9a\u4e49\u4e0d\u6e05\u6670\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u901a\u7528LLM\u4ee3\u7801\u8fdb\u5316\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3CUDA\u5185\u6838\u4f18\u5316\u7684\u4e25\u683c\u6b63\u786e\u6027\u8981\u6c42\u3002", "method": "\u9996\u5148\u5f62\u5f0f\u5316CUDA\u5185\u6838\u4f18\u5316\u4efb\u52a1\uff0c\u7136\u540e\u5efa\u7acb\u7cfb\u7edf\u5316\u7684LLM\u4ee3\u7801\u8fdb\u5316\u6846\u67b6EvoEngineer\uff0c\u63d0\u4f9b\u4f18\u5316\u7b56\u7565\u8bbe\u8ba1\u6307\u5bfc\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u6b63\u786e\u6027\u3002", "result": "\u572891\u4e2a\u771f\u5b9eCUDA\u5185\u6838\u4e0a\uff0cEvoEngineer\u5b9e\u73b0\u4e862.72\u500d\u5e73\u5747\u4e2d\u4f4d\u6570\u52a0\u901f\uff0c\u4ee3\u7801\u6709\u6548\u6027\u8fbe69.8%\uff0c\u6700\u5927\u52a0\u901f\u6bd4\u8fbe36.75\u500d\uff0c\u572850\u4e2a\u8d85\u8fc72\u500d\u52a0\u901f\u7684\u64cd\u4f5c\u4e2d\u670956%\u8fbe\u5230\u6700\u9ad8\u52a0\u901f\u3002", "conclusion": "EvoEngineer\u5728CUDA\u5185\u6838\u4f18\u5316\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6b63\u786e\u6027\u7684\u539f\u5219\u6027\u5e73\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2510.03817", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03817", "abs": "https://arxiv.org/abs/2510.03817", "authors": ["Philipp Becker", "Niklas Freymuth", "Serge Thilges", "Fabian Otto", "Gerhard Neumann"], "title": "TROLL: Trust Regions improve Reinforcement Learning for Large Language Models", "comment": null, "summary": "On-policy Reinforcement Learning (RL) with PPO-like clip objectives has\nbecome the standard choice for reward-based fine-tuning of large language\nmodels (LLMs). Although recent work has explored improved estimators of\nadvantages and normalization, the clipping mechanism itself has remained\nuntouched. Originally introduced as a proxy for principled KL-based trust\nregions, clipping is a crude approximation that often causes unstable updates\nand suboptimal performance. We replace the clip objective with a novel discrete\ndifferentiable trust region projection, which provides principled token-level\nKL constraints. The projection operates on a sparse subset of the model's most\nimportant token logits to balance computational cost and projection\neffectiveness. Our approach, Trust Region Optimization for Large Language\nModels (TROLL), serves as a direct replacement for PPO-like clipping during\ntraining and does not alter the model's inference behavior. Across datasets,\nmodel families, and advantage-estimation methods, TROLL consistently\noutperforms PPO-like clipping in terms of training speed, stability, and final\nsuccess rates.", "AI": {"tldr": "\u63d0\u51faTROLL\u65b9\u6cd5\uff0c\u7528\u79bb\u6563\u53ef\u5fae\u5206\u4fe1\u4efb\u533a\u57df\u6295\u5f71\u66ff\u4ee3PPO\u7684clip\u673a\u5236\uff0c\u4e3aLLM\u5956\u52b1\u5fae\u8c03\u63d0\u4f9b\u539f\u5219\u6027\u7684token\u7ea7KL\u7ea6\u675f\uff0c\u5728\u8bad\u7ec3\u901f\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8ePPO\u3002", "motivation": "PPO\u7684clip\u673a\u5236\u4f5c\u4e3aKL\u4fe1\u4efb\u533a\u57df\u7684\u7c97\u7565\u8fd1\u4f3c\uff0c\u5e38\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u66f4\u65b0\u548c\u6b21\u4f18\u6027\u80fd\uff0c\u9700\u8981\u66f4\u539f\u5219\u6027\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u53ef\u5fae\u5206\u4fe1\u4efb\u533a\u57df\u6295\u5f71\u66ff\u4ee3clip\uff0c\u5728\u6a21\u578b\u6700\u91cd\u8981token\u7684logits\u7a00\u758f\u5b50\u96c6\u4e0a\u64cd\u4f5c\uff0c\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u548c\u6295\u5f71\u6548\u679c\u3002", "result": "\u5728\u4e0d\u540c\u6570\u636e\u96c6\u3001\u6a21\u578b\u5bb6\u65cf\u548c\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\u4e0b\uff0cTROLL\u5728\u8bad\u7ec3\u901f\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6210\u529f\u7387\u4e0a\u5747\u4f18\u4e8ePPO-like clip\u3002", "conclusion": "TROLL\u53ef\u4f5c\u4e3aPPO-like clip\u7684\u76f4\u63a5\u66ff\u4ee3\uff0c\u4e0d\u6539\u53d8\u6a21\u578b\u63a8\u7406\u884c\u4e3a\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u6709\u6548\u7684RL\u8bad\u7ec3\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.03830", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03830", "abs": "https://arxiv.org/abs/2510.03830", "authors": ["Alex Durkin", "Jasper Stolte", "Mehmet Mercang\u00f6z"], "title": "HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control", "comment": "31 pages, 15 figures, submitted to Computers and Chemical Engineering", "summary": "Start-ups and product grade-changes are critical steps in continuous-process\nplant operation, because any misstep immediately affects product quality and\ndrives operational losses. These transitions have long relied on manual\noperation by a handful of expert operators, but the progressive retirement of\nthat workforce is leaving plant owners without the tacit know-how needed to\nexecute them consistently. In the absence of a process model, offline\nreinforcement learning (RL) promises to capture and even surpass human\nexpertise by mining historical start-up and grade-change logs, yet standard\noffline RL struggles with distribution shift and value-overestimation whenever\na learned policy ventures outside the data envelope. We introduce HOFLON\n(Hybrid Offline Learning + Online Optimization) to overcome those limitations.\nOffline, HOFLON learns (i) a latent data manifold that represents the feasible\nregion spanned by past transitions and (ii) a long-horizon Q-critic that\npredicts the cumulative reward from state-action pairs. Online, it solves a\none-step optimization problem that maximizes the Q-critic while penalizing\ndeviations from the learned manifold and excessive rates of change in the\nmanipulated variables. We test HOFLON on two industrial case studies: a\npolymerization reactor start-up and a paper-machine grade-change problem, and\nbenchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.\nIn both plants HOFLON not only surpasses IQL but also delivers, on average,\nbetter cumulative rewards than the best start-up or grade-change observed in\nthe historical data, demonstrating its potential to automate transition\noperations beyond current expert capability.", "AI": {"tldr": "HOFLON\u662f\u4e00\u79cd\u6df7\u5408\u79bb\u7ebf\u5b66\u4e60+\u5728\u7ebf\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fde\u7eed\u8fc7\u7a0b\u5de5\u5382\u7684\u542f\u52a8\u548c\u4ea7\u54c1\u7b49\u7ea7\u53d8\u66f4\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u79bb\u7ebf\u5b66\u4e60\u7684\u6570\u636e\u6d41\u5f62\u548cQ\u503c\u8bc4\u4f30\u5668\uff0c\u4ee5\u53ca\u5728\u7ebf\u4f18\u5316\u6765\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u64cd\u4f5c\u3002", "motivation": "\u968f\u7740\u4e13\u5bb6\u64cd\u4f5c\u5458\u7684\u9000\u4f11\uff0c\u5de5\u5382\u7f3a\u4e4f\u6267\u884c\u542f\u52a8\u548c\u7b49\u7ea7\u53d8\u66f4\u7684\u9690\u6027\u77e5\u8bc6\uff0c\u800c\u6807\u51c6\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u5206\u5e03\u504f\u79fb\u548c\u503c\u9ad8\u4f30\u95ee\u9898\u3002", "method": "\u79bb\u7ebf\u5b66\u4e60\u6570\u636e\u6d41\u5f62\u548c\u957f\u65f6\u57dfQ\u503c\u8bc4\u4f30\u5668\uff0c\u5728\u7ebf\u6c42\u89e3\u6700\u5927\u5316Q\u503c\u540c\u65f6\u60e9\u7f5a\u504f\u79bb\u6570\u636e\u6d41\u5f62\u548c\u64cd\u7eb5\u53d8\u91cf\u53d8\u5316\u7387\u7684\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5728\u4e24\u4e2a\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cHOFLON\u4e0d\u4ec5\u8d85\u8d8a\u4e86IQL\u7b97\u6cd5\uff0c\u8fd8\u6bd4\u5386\u53f2\u6570\u636e\u4e2d\u6700\u4f73\u542f\u52a8\u6216\u7b49\u7ea7\u53d8\u66f4\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u7d2f\u79ef\u5956\u52b1\u3002", "conclusion": "HOFLON\u5c55\u793a\u4e86\u8d85\u8d8a\u5f53\u524d\u4e13\u5bb6\u80fd\u529b\u7684\u81ea\u52a8\u5316\u8fc7\u6e21\u64cd\u4f5c\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.03930", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03930", "abs": "https://arxiv.org/abs/2510.03930", "authors": ["Huascar Sanchez", "Briland Hitaj"], "title": "LLM Chemistry Estimation for Multi-LLM Recommendation", "comment": "20 pages, 5 figures, 5 tables", "summary": "Multi-LLM collaboration promises accurate, robust, and context-aware\nsolutions, yet existing approaches rely on implicit selection and output\nassessment without analyzing whether collaborating models truly complement or\nconflict. We introduce LLM Chemistry -- a framework that measures when LLM\ncombinations exhibit synergistic or antagonistic behaviors that shape\ncollective performance beyond individual capabilities. We formalize the notion\nof chemistry among LLMs, propose algorithms that quantify it by analyzing\ninteraction dependencies, and recommend optimal model ensembles accordingly.\nOur theoretical analysis shows that chemistry among collaborating LLMs is most\nevident under heterogeneous model profiles, with its outcome impact shaped by\ntask type, group size, and complexity. Evaluation on classification,\nsummarization, and program repair tasks provides initial evidence for these\ntask-dependent effects, thereby reinforcing our theoretical results. This\nestablishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and\na foundation for ensemble recommendation.", "AI": {"tldr": "LLM Chemistry\u6846\u67b6\u7528\u4e8e\u91cf\u5316\u591aLLM\u534f\u4f5c\u4e2d\u7684\u534f\u540c\u6216\u5bf9\u6297\u884c\u4e3a\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u95f4\u7684\u4ea4\u4e92\u4f9d\u8d56\u5173\u7cfb\u6765\u63a8\u8350\u6700\u4f18\u6a21\u578b\u7ec4\u5408\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9690\u5f0f\u9009\u62e9\u548c\u8f93\u51fa\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5206\u6790\u534f\u4f5c\u6a21\u578b\u662f\u5426\u771f\u6b63\u4e92\u8865\u6216\u51b2\u7a81\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u8861\u91cfLLM\u7ec4\u5408\u7684\u534f\u540c\u6548\u5e94\u3002", "method": "\u63d0\u51faLLM Chemistry\u6846\u67b6\uff0c\u5f62\u5f0f\u5316LLM\u95f4\u7684\u5316\u5b66\u4f5c\u7528\u6982\u5ff5\uff0c\u5f00\u53d1\u7b97\u6cd5\u91cf\u5316\u4ea4\u4e92\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u6839\u636e\u7406\u8bba\u5206\u6790\u63a8\u8350\u6700\u4f18\u6a21\u578b\u7ec4\u5408\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eLLM\u5316\u5b66\u6548\u5e94\u5728\u5f02\u8d28\u6a21\u578b\u914d\u7f6e\u4e0b\u6700\u660e\u663e\uff0c\u8bc4\u4f30\u5728\u5206\u7c7b\u3001\u6458\u8981\u548c\u7a0b\u5e8f\u4fee\u590d\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u4efb\u52a1\u4f9d\u8d56\u6548\u5e94\u3002", "conclusion": "LLM Chemistry\u53ef\u4f5c\u4e3a\u591aLLM\u7cfb\u7edf\u7684\u8bca\u65ad\u56e0\u7d20\u548c\u96c6\u6210\u63a8\u8350\u7684\u57fa\u7840\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2510.04019", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04019", "abs": "https://arxiv.org/abs/2510.04019", "authors": ["Anthony Zhan"], "title": "Principled and Tractable RL for Reasoning with Diffusion Language Models", "comment": null, "summary": "Diffusion large language models (dLLMs) are a new paradigm of\nnon-autoregressive language models that are trained to predict multiple tokens\nin parallel and generate text via iterative unmasking. Recent works have\nsuccessfully pretrained dLLMs to parity with autoregressive LLMs at the 8B\nscale, but dLLMs have yet to benefit from modern post-training techniques, e.g.\nreinforcement learning (RL), that have proven effective for autoregressive\nmodels. Crucially, algorithms designed for traditional LLMs aren't directly\ncompatible with diffusion frameworks due to inherent differences in modeling\nassumptions. Moreover, existing attempts at dLLM post-training with RL rely on\nheuristic-based objectives with no theoretical grounding. In this work, we\npresent Amortized Group Relative Policy Optimization (AGRPO), a principled\non-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo\nsampling to compute an unbiased policy gradient estimate, making it the first\ntractable, faithful adaptation of policy gradient methods for dLLMs. We\ndemonstrate AGRPO's effectiveness on different math/reasoning tasks, a common\nsetting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x\nperformance on the Countdown task over the baseline LLaDA-8B-Instruct model and\n1.3x performance gains over comparable RL methods such as diffu-GRPO.\nFurthermore, these gains persist across different numbers of sampling steps at\ninference time, achieving better tradeoffs between compute and performance. Our\nresults demonstrate that online RL algorithms can be extended to diffusion LLMs\nin principled ways, maintaining both theoretical soundness and practical\neffectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86AGRPO\u7b97\u6cd5\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u7406\u8bba\u4e0a\u6709\u4f9d\u636e\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5c1a\u672a\u53d7\u76ca\u4e8e\u73b0\u4ee3\u540e\u8bad\u7ec3\u6280\u672f\uff0c\u7279\u522b\u662f\u5f3a\u5316\u5b66\u4e60\uff0c\u56e0\u4e3a\u4f20\u7edfLLM\u7684\u7b97\u6cd5\u4e0e\u6269\u6563\u6846\u67b6\u4e0d\u517c\u5bb9\uff0c\u4e14\u73b0\u6709\u5c1d\u8bd5\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86AGRPO\u7b97\u6cd5\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u91c7\u6837\u8ba1\u7b97\u65e0\u504f\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\uff0c\u662f\u9996\u4e2a\u9002\u7528\u4e8edLLMs\u7684\u53ef\u5904\u7406\u3001\u5fe0\u5b9e\u9002\u914d\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0cGSM8K\u4efb\u52a1\u4e0a\u83b7\u5f97+7.6%\u7edd\u5bf9\u589e\u76ca\uff0cCountdown\u4efb\u52a1\u4e0a\u8fbe\u5230\u57fa\u7ebf\u6a21\u578b\u76843.8\u500d\u6027\u80fd\uff0c\u6bd4diffu-GRPO\u63d0\u53471.3\u500d\u3002", "conclusion": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u53ef\u4ee5\u4ee5\u7406\u8bba\u4e0a\u6709\u4f9d\u636e\u7684\u65b9\u5f0f\u6269\u5c55\u5230\u6269\u6563LLMs\uff0c\u4fdd\u6301\u7406\u8bba\u4e25\u8c28\u6027\u548c\u5b9e\u9645\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04072", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04072", "abs": "https://arxiv.org/abs/2510.04072", "authors": ["Ziyan Wang", "Zheng Wang", "Jie Fu", "Xingwei Qu", "Qi Cheng", "Shengpu Tang", "Minjia Zhang", "Xiaoming Huo"], "title": "Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning", "comment": null, "summary": "Reinforcement learning (RL) has become central to enhancing reasoning in\nlarge language models (LLMs). Yet on-policy algorithms such as Group Relative\nPolicy Optimization (GRPO) often suffer in early training: noisy gradients from\nlow-quality rollouts lead to unstable updates and inefficient exploration. We\nintroduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient\nframework to address these limitations via decomposing each step into three\nstages: a short fast trajectory of inner steps on the same batch, a reposition\nmechanism to control off-policy drift, and a final slow correction. This\nreposition-before-update design preserves the objective and rollout process\nunchanged, making SFPO plug-compatible with existing policy-gradient pipelines.\nExtensive experiments demonstrate that SFPO consistently improves stability,\nreduces rollouts, and accelerates convergence of reasoning RL training.\nSpecifically, it outperforms GRPO by up to 2.80 points in average on math\nreasoning benchmarks. It also achieves up to 4.93\\texttimes{} fewer rollouts\nand a 4.19\\texttimes{} reduction in wall-clock time to match GRPO's best\naccuracy.", "AI": {"tldr": "SFPO\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6162-\u5feb\u7b56\u7565\u4f18\u5316\u89e3\u51b3\u65e9\u671f\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406RL\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff08\u5982GRPO\uff09\u5728\u65e9\u671f\u8bad\u7ec3\u4e2d\u56e0\u4f4e\u8d28\u91cfrollout\u4ea7\u751f\u7684\u566a\u58f0\u68af\u5ea6\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u66f4\u65b0\u548c\u4f4e\u6548\u63a2\u7d22\u3002", "method": "\u5c06\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u5206\u89e3\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u5728\u540c\u4e00\u6279\u6b21\u4e0a\u8fdb\u884c\u77ed\u5feb\u8f68\u8ff9\u5185\u90e8\u6b65\u9aa4\u3001\u63a7\u5236\u79bb\u7b56\u7565\u6f02\u79fb\u7684\u91cd\u5b9a\u4f4d\u673a\u5236\u3001\u4ee5\u53ca\u6700\u7ec8\u7684\u6162\u901f\u6821\u6b63\u3002", "result": "SFPO\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u6bd4GRPO\u5e73\u5747\u63d0\u53472.80\u5206\uff0c\u51cf\u5c114.93\u500drollout\u6b21\u6570\uff0c\u5e76\u5728\u8fbe\u5230GRPO\u6700\u4f73\u51c6\u786e\u7387\u65f6\u51cf\u5c114.19\u500d\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "SFPO\u901a\u8fc7\u91cd\u5b9a\u4f4d-\u66f4\u65b0\u8bbe\u8ba1\u4fdd\u6301\u76ee\u6807\u548crollout\u8fc7\u7a0b\u4e0d\u53d8\uff0c\u4e0e\u73b0\u6709\u7b56\u7565\u68af\u5ea6\u7ba1\u9053\u517c\u5bb9\uff0c\u80fd\u6301\u7eed\u63d0\u5347\u7a33\u5b9a\u6027\u5e76\u52a0\u901f\u6536\u655b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.03912", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03912", "abs": "https://arxiv.org/abs/2510.03912", "authors": ["Liyuan Hu", "Jitao Wang", "Zhenke Wu", "Chengchun Shi"], "title": "Generalized Fitted Q-Iteration with Clustered Data", "comment": null, "summary": "This paper focuses on reinforcement learning (RL) with clustered data, which\nis commonly encountered in healthcare applications. We propose a generalized\nfitted Q-iteration (FQI) algorithm that incorporates generalized estimating\nequations into policy learning to handle the intra-cluster correlations.\nTheoretically, we demonstrate (i) the optimalities of our Q-function and policy\nestimators when the correlation structure is correctly specified, and (ii)\ntheir consistencies when the structure is mis-specified. Empirically, through\nsimulations and analyses of a mobile health dataset, we find the proposed\ngeneralized FQI achieves, on average, a half reduction in regret compared to\nthe standard FQI.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u5904\u7406\u805a\u7c7b\u6570\u636e\u7684\u5e7f\u4e49\u62df\u5408Q\u8fed\u4ee3\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5e7f\u4e49\u4f30\u8ba1\u65b9\u7a0b\u6765\u5904\u7406\u7c07\u5185\u76f8\u5173\u6027\uff0c\u5728\u533b\u7597\u5065\u5eb7\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u533b\u7597\u5065\u5eb7\u5e94\u7528\u4e2d\u5e38\u89c1\u805a\u7c7b\u6570\u636e\uff0c\u9700\u8981\u5904\u7406\u7c07\u5185\u76f8\u5173\u6027\u4ee5\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002", "method": "\u5c06\u5e7f\u4e49\u4f30\u8ba1\u65b9\u7a0b\u878d\u5165\u62df\u5408Q\u8fed\u4ee3\u7b97\u6cd5\uff0c\u6784\u5efa\u5e7f\u4e49FQI\u65b9\u6cd5\u6765\u5904\u7406\u805a\u7c7b\u6570\u636e\u7ed3\u6784\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u6b63\u786e\u6307\u5b9a\u76f8\u5173\u7ed3\u6784\u65f6\u7684\u6700\u4f18\u6027\uff0c\u5b9e\u8bc1\u663e\u793a\u5728\u79fb\u52a8\u5065\u5eb7\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u6bd4\u6807\u51c6FQI\u51cf\u5c11\u4e00\u534a\u9057\u61be\u503c\u3002", "conclusion": "\u5e7f\u4e49FQI\u7b97\u6cd5\u80fd\u6709\u6548\u5904\u7406\u805a\u7c7b\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04573", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04573", "abs": "https://arxiv.org/abs/2510.04573", "authors": ["Haoqiang Kang", "Yizhe Zhang", "Nikki Lijing Kuang", "Nicklas Majamaki", "Navdeep Jaitly", "Yi-An Ma", "Lianhui Qin"], "title": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning", "comment": null, "summary": "Large Language Models (LLMs) demonstrate their reasoning ability through\nchain-of-thought (CoT) generation. However, LLM's autoregressive decoding may\nlimit the ability to revisit and refine earlier tokens in a holistic manner,\nwhich can also lead to inefficient exploration for diverse solutions. In this\npaper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning\nframework that unifies the expressiveness of continuous latent representation\nwith the iterative refinement capabilities of latent diffusion models for an\nexisting LLM. We first construct a structured latent reasoning space using a\nVariational Autoencoder (VAE) that encodes text reasoning steps into blocks of\nthought tokens, preserving semantic information and interpretability while\noffering compact but expressive representations. Subsequently, we utilize a\nlatent diffusion model that learns to denoise a block of latent thought tokens\nwith a blockwise bidirectional attention mask, enabling longer horizon and\niterative refinement with adaptive test-time compute. This design allows\nefficient parallel generation of diverse reasoning trajectories, allowing the\nmodel to plan and revise the reasoning process holistically. We conduct\nevaluations on a suite of mathematical reasoning and planning benchmarks.\nEmpirical results show that LaDiR consistently improves accuracy, diversity,\nand interpretability over existing autoregressive, diffusion-based, and latent\nreasoning methods, revealing a new paradigm for text reasoning with latent\ndiffusion.", "AI": {"tldr": "LaDiR\u662f\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u8fde\u7eed\u6f5c\u5728\u8868\u793a\u4e0e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u6539\u8fdbLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u5e76\u884c\u751f\u6210\u591a\u6837\u5316\u63a8\u7406\u8f68\u8ff9\u5b9e\u73b0\u6574\u4f53\u89c4\u5212\u548c\u4fee\u6b63\u3002", "motivation": "\u89e3\u51b3LLM\u81ea\u56de\u5f52\u89e3\u7801\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u65e0\u6cd5\u6574\u4f53\u6027\u91cd\u65b0\u5ba1\u89c6\u548c\u4f18\u5316\u65e9\u671f\u6807\u8bb0\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u63a2\u7d22\u591a\u6837\u5316\u89e3\u51b3\u65b9\u6848\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u4f7f\u7528VAE\u6784\u5efa\u7ed3\u6784\u5316\u6f5c\u5728\u63a8\u7406\u7a7a\u95f4\uff0c\u5c06\u6587\u672c\u63a8\u7406\u6b65\u9aa4\u7f16\u7801\u4e3a\u601d\u60f3\u6807\u8bb0\u5757\uff1b\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5757\u72b6\u53cc\u5411\u6ce8\u610f\u529b\u63a9\u7801\u8fdb\u884c\u53bb\u566a\uff0c\u5b9e\u73b0\u957f\u7a0b\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLaDiR\u5728\u51c6\u786e\u6027\u3001\u591a\u6837\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u56de\u5f52\u3001\u57fa\u4e8e\u6269\u6563\u548c\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5\u3002", "conclusion": "LaDiR\u4e3a\u6587\u672c\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6f5c\u5728\u6269\u6563\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2510.04618", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04618", "abs": "https://arxiv.org/abs/2510.04618", "authors": ["Qizheng Zhang", "Changran Hu", "Shubhangi Upasani", "Boyuan Ma", "Fenglu Hong", "Vamsidhar Kamanuru", "Jay Rainton", "Chen Wu", "Mengmeng Ji", "Hanchen Li", "Urmish Thakker", "James Zou", "Kunle Olukotun"], "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models", "comment": null, "summary": "Large language model (LLM) applications such as agents and domain-specific\nreasoning increasingly rely on context adaptation -- modifying inputs with\ninstructions, strategies, or evidence, rather than weight updates. Prior\napproaches improve usability but often suffer from brevity bias, which drops\ndomain insights for concise summaries, and from context collapse, where\niterative rewriting erodes details over time. Building on the adaptive memory\nintroduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context\nEngineering), a framework that treats contexts as evolving playbooks that\naccumulate, refine, and organize strategies through a modular process of\ngeneration, reflection, and curation. ACE prevents collapse with structured,\nincremental updates that preserve detailed knowledge and scale with\nlong-context models. Across agent and domain-specific benchmarks, ACE optimizes\ncontexts both offline (e.g., system prompts) and online (e.g., agent memory),\nconsistently outperforming strong baselines: +10.6% on agents and +8.6% on\nfinance, while significantly reducing adaptation latency and rollout cost.\nNotably, ACE could adapt effectively without labeled supervision and instead by\nleveraging natural execution feedback. On the AppWorld leaderboard, ACE matches\nthe top-ranked production-level agent on the overall average and surpasses it\non the harder test-challenge split, despite using a smaller open-source model.\nThese results show that comprehensive, evolving contexts enable scalable,\nefficient, and self-improving LLM systems with low overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86ACE\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e0a\u4e0b\u6587\u89c6\u4e3a\u4e0d\u65ad\u6f14\u53d8\u7684\u7b56\u7565\u624b\u518c\uff0c\u901a\u8fc7\u751f\u6210\u3001\u53cd\u601d\u548c\u6574\u7406\u7684\u6a21\u5757\u5316\u8fc7\u7a0b\u6765\u79ef\u7d2f\u3001\u4f18\u5316\u548c\u7ec4\u7ec7\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u7b80\u6d01\u6027\u504f\u89c1\u548c\u4e0a\u4e0b\u6587\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u5e94\u7528\u4e2d\u7684\u4e0a\u4e0b\u6587\u9002\u5e94\u65b9\u6cd5\u5b58\u5728\u7b80\u6d01\u6027\u504f\u89c1\uff08\u4e3a\u7b80\u6d01\u603b\u7ed3\u800c\u4e22\u5931\u9886\u57df\u6d1e\u5bdf\uff09\u548c\u4e0a\u4e0b\u6587\u5d29\u6e83\uff08\u8fed\u4ee3\u91cd\u5199\u968f\u65f6\u95f4\u4fb5\u8680\u7ec6\u8282\uff09\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u6846\u67b6\u3002", "method": "ACE\u6846\u67b6\u5c06\u4e0a\u4e0b\u6587\u89c6\u4e3a\u6f14\u53d8\u7684\u7b56\u7565\u624b\u518c\uff0c\u91c7\u7528\u751f\u6210\u3001\u53cd\u601d\u548c\u6574\u7406\u7684\u6a21\u5757\u5316\u8fc7\u7a0b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u589e\u91cf\u66f4\u65b0\u6765\u9632\u6b62\u4e0a\u4e0b\u6587\u5d29\u6e83\uff0c\u5e76\u4fdd\u7559\u8be6\u7ec6\u77e5\u8bc6\u4ee5\u9002\u5e94\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u3002", "result": "\u5728\u4ee3\u7406\u548c\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cACE\u5728\u79bb\u7ebf\uff08\u5982\u7cfb\u7edf\u63d0\u793a\uff09\u548c\u5728\u7ebf\uff08\u5982\u4ee3\u7406\u8bb0\u5fc6\uff09\u4e0a\u4e0b\u6587\u4f18\u5316\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff1a\u4ee3\u7406\u4efb\u52a1\u63d0\u534710.6%\uff0c\u91d1\u878d\u4efb\u52a1\u63d0\u53478.6%\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u9002\u5e94\u5ef6\u8fdf\u548c\u90e8\u7f72\u6210\u672c\u3002\u5728AppWorld\u6392\u884c\u699c\u4e0a\uff0cACE\u4e0e\u9876\u7ea7\u751f\u4ea7\u7ea7\u4ee3\u7406\u5728\u6574\u4f53\u5e73\u5747\u5206\u4e0a\u6301\u5e73\uff0c\u5e76\u5728\u66f4\u96be\u7684\u6d4b\u8bd5\u6311\u6218\u5206\u5272\u4e0a\u8d85\u8d8a\u5b83\u3002", "conclusion": "\u5168\u9762\u4e14\u4e0d\u65ad\u6f14\u53d8\u7684\u4e0a\u4e0b\u6587\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u81ea\u6211\u6539\u8fdb\u7684LLM\u7cfb\u7edf\uff0c\u4e14\u5f00\u9500\u8f83\u4f4e\u3002", "topic": "agent analysis"}}
{"id": "2510.03971", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03971", "abs": "https://arxiv.org/abs/2510.03971", "authors": ["Jatin Prakash", "Anirudh Buvanesh"], "title": "What Can You Do When You Have Zero Rewards During RL?", "comment": null, "summary": "Reinforcement learning (RL) with outcome-based rewards has proven effective\nfor improving large language models (LLMs) on complex reasoning tasks. However,\nits success often depends on the base model occasionally sampling correct\nsolutions. When no correct solutions are sampled, training encounters a\nzero-reward barrier where learning stalls due to zero gradients. We study this\nscenario through the graph search task introduced in Bachmann et al. (2024) and\nevaluate recent methods that incorporate desirable components such as dense\nrewards, diversity incentives, and improved credit assignment. Our experiments\nshow that none of these approaches overcome the zero-reward barrier if the base\nmodel never produces a correct answer. In contrast, we find that a simple\ndata-centric intervention of adding easier samples to the training set enables\nthe model to eventually solve the original hard task despite starting from zero\nreward. Importantly, this succeeds without modifying the RL algorithm itself.\nBecause official implementations of several baselines were unavailable, we\ndeveloped our own, which allowed us to conduct a detailed analysis of their\nfailure modes. We release these implementations to support further research at:\nhttps://github.com/rl4reasoning/rl-baselines", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f53\u57fa\u7840\u6a21\u578b\u4ece\u672a\u4ea7\u751f\u6b63\u786e\u7b54\u6848\u65f6\u7684\u96f6\u5956\u52b1\u969c\u788d\u95ee\u9898\uff0c\u53d1\u73b0\u7b80\u5355\u7684\u6570\u636e\u5e72\u9884\uff08\u6dfb\u52a0\u7b80\u5355\u6837\u672c\uff09\u6bd4\u7b97\u6cd5\u6539\u8fdb\u66f4\u6709\u6548\u3002", "motivation": "\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u9762\u4e34\u7684\u96f6\u5956\u52b1\u969c\u788d\u95ee\u9898\uff0c\u5373\u5f53\u57fa\u7840\u6a21\u578b\u4ece\u672a\u91c7\u6837\u5230\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u65f6\uff0c\u8bad\u7ec3\u4f1a\u56e0\u96f6\u68af\u5ea6\u800c\u505c\u6ede\u3002", "method": "\u5728\u56fe\u641c\u7d22\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u591a\u79cd\u65b9\u6cd5\uff08\u5bc6\u96c6\u5956\u52b1\u3001\u591a\u6837\u6027\u6fc0\u52b1\u3001\u6539\u8fdb\u7684\u4fe1\u7528\u5206\u914d\uff09\uff0c\u5e76\u6d4b\u8bd5\u4e86\u6570\u636e\u5e72\u9884\u65b9\u6cd5\uff08\u5728\u8bad\u7ec3\u96c6\u4e2d\u6dfb\u52a0\u7b80\u5355\u6837\u672c\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u7b97\u6cd5\u65b9\u6cd5\u90fd\u65e0\u6cd5\u514b\u670d\u96f6\u5956\u52b1\u969c\u788d\uff0c\u800c\u7b80\u5355\u7684\u6570\u636e\u5e72\u9884\u65b9\u6cd5\u80fd\u591f\u4f7f\u6a21\u578b\u6700\u7ec8\u89e3\u51b3\u539f\u59cb\u56f0\u96be\u4efb\u52a1\u3002", "conclusion": "\u6570\u636e\u4e2d\u5fc3\u7684\u5e72\u9884\u6bd4\u7b97\u6cd5\u4fee\u6539\u66f4\u6709\u6548\u5730\u89e3\u51b3\u96f6\u5956\u52b1\u95ee\u9898\uff0c\u4e14\u4e0d\u9700\u8981\u6539\u53d8RL\u7b97\u6cd5\u672c\u8eab\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04996", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04996", "abs": "https://arxiv.org/abs/2510.04996", "authors": ["Wei Xiong", "Chenlu Ye", "Baohao Liao", "Hanze Dong", "Xinxing Xu", "Christof Monz", "Jiang Bian", "Nan Jiang", "Tong Zhang"], "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training", "comment": "16 pages, 6 figures", "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada.", "AI": {"tldr": "\u63d0\u51faReinforce-Ada\u81ea\u9002\u5e94\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u8fde\u7eed\u6d88\u9664\u8fc7\u7a0b\u52a8\u6001\u5206\u914d\u63a8\u7406\u9884\u7b97\uff0c\u51cf\u5c11\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u65b9\u5dee\uff0c\u52a0\u901f\u6536\u655b\u5e76\u63d0\u5347\u6700\u7ec8\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u56e0\u56fa\u5b9a\u5747\u5300\u91c7\u6837\u5bfc\u81f4\u68af\u5ea6\u4f30\u8ba1\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u52a8\u6001\u5206\u914d\u63a8\u7406\u9884\u7b97\u6765\u6700\u5c0f\u5316\u968f\u673a\u68af\u5ea6\u65b9\u5dee\u3002", "method": "\u91c7\u7528\u5728\u7ebf\u8fde\u7eed\u6d88\u9664\u8fc7\u7a0b\u4ea4\u7ec7\u4f30\u8ba1\u548c\u91c7\u6837\uff0c\u81ea\u52a8\u505c\u6b62\u6536\u96c6\u8db3\u591f\u4fe1\u53f7\u7684\u63d0\u793a\u91c7\u6837\uff1b\u901a\u8fc7\u56fa\u5b9a\u5927\u5c0f\u7ec4\u548c\u5168\u5c40\u7edf\u8ba1\u8ba1\u7b97\u4f18\u52bf\u57fa\u7ebf\u6765\u7a33\u5b9a\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u67b6\u6784\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReinforce-Ada\u76f8\u6bd4GRPO\u52a0\u901f\u6536\u655b\u5e76\u63d0\u9ad8\u6700\u7ec8\u6027\u80fd\uff0c\u7279\u522b\u662f\u4f7f\u7528\u5e73\u8861\u91c7\u6837\u53d8\u4f53\u65f6\u3002", "conclusion": "\u65b9\u5dee\u611f\u77e5\u7684\u81ea\u9002\u5e94\u6570\u636e\u7ba1\u7406\u5728\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u63a8\u7406\u80fd\u529b\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u53d1\u6325\u6838\u5fc3\u4f5c\u7528\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04280", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04280", "abs": "https://arxiv.org/abs/2510.04280", "authors": ["\u00c1lvaro Serra-Gomez", "Daniel Jarne Ornia", "Dhruva Tirumala", "Thomas Moerland"], "title": "A KL-regularization framework for learning to plan with adaptive priors", "comment": "Preprint", "summary": "Effective exploration remains a central challenge in model-based\nreinforcement learning (MBRL), particularly in high-dimensional continuous\ncontrol tasks where sample efficiency is crucial. A prominent line of recent\nwork leverages learned policies as proposal distributions for Model-Predictive\nPath Integral (MPPI) planning. Initial approaches update the sampling policy\nindependently of the planner distribution, typically maximizing a learned value\nfunction with deterministic policy gradient and entropy regularization.\nHowever, because the states encountered during training depend on the MPPI\nplanner, aligning the sampling policy with the planner improves the accuracy of\nvalue estimation and long-term performance. To this end, recent methods update\nthe sampling policy by minimizing KL divergence to the planner distribution or\nby introducing planner-guided regularization into the policy update. In this\nwork, we unify these MPPI-based reinforcement learning methods under a single\nframework by introducing Policy Optimization-Model Predictive Control (PO-MPC),\na family of KL-regularized MBRL methods that integrate the planner's action\ndistribution as a prior in policy optimization. By aligning the learned policy\nwith the planner's behavior, PO-MPC allows more flexibility in the policy\nupdates to trade off Return maximization and KL divergence minimization. We\nclarify how prior approaches emerge as special cases of this family, and we\nexplore previously unstudied variations. Our experiments show that these\nextended configurations yield significant performance improvements, advancing\nthe state of the art in MPPI-based RL.", "AI": {"tldr": "\u63d0\u51faPO-MPC\u6846\u67b6\uff0c\u7edf\u4e00\u57fa\u4e8eMPPI\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7KL\u6b63\u5219\u5316\u5c06\u89c4\u5212\u5668\u5206\u5e03\u4f5c\u4e3a\u7b56\u7565\u4f18\u5316\u7684\u5148\u9a8c\uff0c\u63d0\u5347\u63a2\u7d22\u6548\u7387\u548c\u957f\u671f\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u9700\u8981\u66f4\u597d\u5730\u5bf9\u9f50\u91c7\u6837\u7b56\u7565\u4e0e\u89c4\u5212\u5668\u5206\u5e03\u4ee5\u63d0\u9ad8\u4ef7\u503c\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165PO-MPC\u6846\u67b6\uff0c\u5c06\u89c4\u5212\u5668\u7684\u52a8\u4f5c\u5206\u5e03\u4f5c\u4e3a\u7b56\u7565\u4f18\u5316\u7684\u5148\u9a8c\uff0c\u901a\u8fc7KL\u6b63\u5219\u5316\u5b9e\u73b0\u7b56\u7565\u4e0e\u89c4\u5212\u5668\u884c\u4e3a\u7684\u5bf9\u9f50\uff0c\u63d0\u4f9b\u5728\u56de\u62a5\u6700\u5927\u5316\u548cKL\u6563\u5ea6\u6700\u5c0f\u5316\u4e4b\u95f4\u7684\u6743\u8861\u7075\u6d3b\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6269\u5c55\u914d\u7f6e\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u63a8\u8fdb\u4e86\u57fa\u4e8eMPPI\u7684\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u53d1\u5c55\u3002", "conclusion": "PO-MPC\u6846\u67b6\u7edf\u4e00\u4e86\u73b0\u6709\u65b9\u6cd5\u5e76\u63ed\u793a\u4e86\u65b0\u7684\u53d8\u4f53\uff0c\u901a\u8fc7\u7b56\u7565\u4e0e\u89c4\u5212\u5668\u7684\u66f4\u597d\u5bf9\u9f50\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u63a2\u7d22\u548c\u6027\u80fd\u6539\u8fdb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04309", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04309", "abs": "https://arxiv.org/abs/2510.04309", "authors": ["Dung V. Nguyen", "Hieu M. Vu", "Nhi Y. Pham", "Lei Zhang", "Tan M. Nguyen"], "title": "Activation Steering with a Feedback Controller", "comment": "9 pages in the main text. Under Review", "summary": "Controlling the behaviors of large language models (LLM) is fundamental to\ntheir safety alignment and reliable deployment. However, existing steering\nmethods are primarily driven by empirical insights and lack theoretical\nperformance guarantees. In this work, we develop a control-theoretic foundation\nfor activation steering by showing that popular steering methods correspond to\nthe proportional (P) controllers, with the steering vector serving as the\nfeedback signal. Building on this finding, we propose\nProportional-Integral-Derivative (PID) Steering, a principled framework that\nleverages the full PID controller for activation steering in LLMs. The\nproportional (P) term aligns activations with target semantic directions, the\nintegral (I) term accumulates errors to enforce persistent corrections across\nlayers, and the derivative (D) term mitigates overshoot by counteracting rapid\nactivation changes. This closed-loop design yields interpretable error dynamics\nand connects activation steering to classical stability guarantees in control\ntheory. Moreover, PID Steering is lightweight, modular, and readily integrates\nwith state-of-the-art steering methods. Extensive experiments across multiple\nLLM families and benchmarks demonstrate that PID Steering consistently\noutperforms existing approaches, achieving more robust and reliable behavioral\ncontrol.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u63a7\u5236\u7406\u8bba\u7684PID Steering\u6846\u67b6\uff0c\u5c06\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u5bf9\u5e94\u4e3a\u6bd4\u4f8b\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u6bd4\u4f8b\u3001\u79ef\u5206\u3001\u5fae\u5206\u4e09\u90e8\u5206\u6539\u8fdbLLM\u884c\u4e3a\u63a7\u5236", "motivation": "\u73b0\u6709LLM\u884c\u4e3a\u63a7\u5236\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u7ecf\u9a8c\uff0c\u7f3a\u4e4f\u7406\u8bba\u6027\u80fd\u4fdd\u8bc1\uff0c\u9700\u8981\u5efa\u7acb\u63a7\u5236\u7406\u8bba\u57fa\u7840", "method": "\u5c06\u6d41\u884c\u5f15\u5bfc\u65b9\u6cd5\u5bf9\u5e94\u4e3a\u6bd4\u4f8b\u63a7\u5236\u5668\uff0c\u63d0\u51faPID Steering\u6846\u67b6\uff0c\u5305\u542b\u6bd4\u4f8b\u9879\uff08\u5bf9\u9f50\u8bed\u4e49\u65b9\u5411\uff09\u3001\u79ef\u5206\u9879\uff08\u7d2f\u79ef\u8bef\u5dee\uff09\u3001\u5fae\u5206\u9879\uff08\u6291\u5236\u8fc7\u51b2\uff09", "result": "\u5728\u591a\u4e2aLLM\u5bb6\u65cf\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPID Steering\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u53ef\u9760\u7684\u884c\u4e3a\u63a7\u5236", "conclusion": "PID Steering\u4e3a\u6fc0\u6d3b\u5f15\u5bfc\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u8fde\u63a5\u4e86\u63a7\u5236\u7406\u8bba\u7684\u7ecf\u5178\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u662f\u8f7b\u91cf\u7ea7\u3001\u6a21\u5757\u5316\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6", "topic": "agent analysis"}}
{"id": "2510.04430", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.04430", "abs": "https://arxiv.org/abs/2510.04430", "authors": ["Ziyi Chen", "Heng Huang"], "title": "Achieve Performatively Optimal Policy for Performative Reinforcement Learning", "comment": null, "summary": "Performative reinforcement learning is an emerging dynamical decision making\nframework, which extends reinforcement learning to the common applications\nwhere the agent's policy can change the environmental dynamics. Existing works\non performative reinforcement learning only aim at a performatively stable (PS)\npolicy that maximizes an approximate value function. However, there is a\nprovably positive constant gap between the PS policy and the desired\nperformatively optimal (PO) policy that maximizes the original value function.\nIn contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)\nalgorithm with a zeroth-order approximation of the performative policy gradient\nin the Frank-Wolfe framework, and obtains \\textbf{the first polynomial-time\nconvergence to the desired PO} policy under the standard regularizer dominance\ncondition. For the convergence analysis, we prove two important properties of\nthe nonconvex value function. First, when the policy regularizer dominates the\nenvironmental shift, the value function satisfies a certain gradient dominance\nproperty, so that any stationary point (not PS) of the value function is a\ndesired PO. Second, though the value function has unbounded gradient, we prove\nthat all the sufficiently stationary points lie in a convex and compact policy\nsubspace $\\Pi_{\\Delta}$, where the policy value has a constant lower bound\n$\\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.\nExperimental results also demonstrate that our 0-FW algorithm is more effective\nthan the existing algorithms in finding the desired PO policy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u96f6\u9636Frank-Wolfe\u7b97\u6cd5\uff080-FW\uff09\uff0c\u9996\u6b21\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u6536\u655b\u5230\u671f\u671b\u7684performatively optimal\uff08PO\uff09\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709performative\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53ea\u80fd\u6536\u655b\u5230performatively stable\uff08PS\uff09\u7b56\u7565\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684performative\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53ea\u80fd\u6536\u655b\u5230performatively stable\uff08PS\uff09\u7b56\u7565\uff0c\u4e0e\u671f\u671b\u7684performatively optimal\uff08PO\uff09\u7b56\u7565\u4e4b\u95f4\u5b58\u5728\u6052\u5b9a\u5dee\u8ddd\u3002\u672c\u6587\u65e8\u5728\u76f4\u63a5\u627e\u5230PO\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u96f6\u9636Frank-Wolfe\u7b97\u6cd5\uff080-FW\uff09\uff0c\u5728Frank-Wolfe\u6846\u67b6\u4e2d\u4f7f\u7528\u96f6\u9636\u8fd1\u4f3c\u6765\u8ba1\u7b97performative\u7b56\u7565\u68af\u5ea6\u3002", "result": "\u5728\u6807\u51c6\u6b63\u5219\u5316\u4e3b\u5bfc\u6761\u4ef6\u4e0b\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u6536\u655b\u5230PO\u7b56\u7565\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e0-FW\u7b97\u6cd5\u6bd4\u73b0\u6709\u7b97\u6cd5\u66f4\u6709\u6548\u5730\u627e\u5230PO\u7b56\u7565\u3002", "conclusion": "0-FW\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86performative\u5f3a\u5316\u5b66\u4e60\u4e2d\u5bfb\u627ePO\u7b56\u7565\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8bc1\u660e\u503c\u51fd\u6570\u7684\u68af\u5ea6\u4e3b\u5bfc\u6027\u548c\u6709\u754c\u6027\uff0c\u786e\u4fdd\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04786", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04786", "abs": "https://arxiv.org/abs/2510.04786", "authors": ["Jonas H\u00fcbotter", "Leander Diaz-Bone", "Ido Hakimi", "Andreas Krause", "Moritz Hardt"], "title": "Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning", "comment": null, "summary": "Humans are good at learning on the job: We learn how to solve the tasks we\nface as we go along. Can a model do the same? We propose an agent that\nassembles a task-specific curriculum, called test-time curriculum (TTC-RL), and\napplies reinforcement learning to continue training the model for its target\ntask. The test-time curriculum avoids time-consuming human curation of datasets\nby automatically selecting the most task-relevant data from a large pool of\navailable training data. Our experiments demonstrate that reinforcement\nlearning on a test-time curriculum consistently improves the model on its\ntarget tasks, across a variety of evaluations and models. Notably, on\nchallenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B\nby approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that\nTTC-RL significantly raises the performance ceiling compared to the initial\nmodel, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to\n43%. Our findings show the potential of test-time curricula in extending the\ntest-time scaling paradigm to continual training on thousands of task-relevant\nexperiences during test-time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u8bd5\u65f6\u8bfe\u7a0b\u5b66\u4e60(TTC-RL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u6d4b\u8bd5\u65f6\u81ea\u52a8\u9009\u62e9\u4efb\u52a1\u76f8\u5173\u6570\u636e\u8fdb\u884c\u6301\u7eed\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u5728\u5de5\u4f5c\u4e2d\u5b66\u4e60\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6a21\u578b\u662f\u5426\u4e5f\u80fd\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u6765\u63d0\u5347\u5176\u5728\u76ee\u6807\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u6d4b\u8bd5\u65f6\u8bfe\u7a0b\u5b66\u4e60(TTC-RL)\uff0c\u81ea\u52a8\u4ece\u5927\u91cf\u53ef\u7528\u8bad\u7ec3\u6570\u636e\u4e2d\u9009\u62e9\u6700\u4efb\u52a1\u76f8\u5173\u7684\u6570\u636e\uff0c\u5e76\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6301\u7eed\u8bad\u7ec3\u3002", "result": "\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTTC-RL\u5c06Qwen3-8B\u7684pass@1\u5728AIME25\u4e0a\u63d0\u5347\u7ea61.8\u500d\uff0c\u5728CodeElo\u4e0a\u63d0\u53472.1\u500d\uff1bpass@8\u5728AIME25\u4e0a\u4ece40%\u63d0\u5347\u523062%\uff0c\u5728CodeElo\u4e0a\u4ece28%\u63d0\u5347\u523043%\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u8bfe\u7a0b\u5b66\u4e60\u5c55\u73b0\u4e86\u5c06\u6d4b\u8bd5\u65f6\u6269\u5c55\u8303\u5f0f\u6269\u5c55\u5230\u5728\u6d4b\u8bd5\u65f6\u5bf9\u6570\u5343\u4e2a\u4efb\u52a1\u76f8\u5173\u7ecf\u9a8c\u8fdb\u884c\u6301\u7eed\u8bad\u7ec3\u7684\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.04860", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04860", "abs": "https://arxiv.org/abs/2510.04860", "authors": ["Siwei Han", "Jiaqi Liu", "Yaofeng Su", "Wenbo Duan", "Xinyuan Liu", "Cihang Xie", "Mohit Bansal", "Mingyu Ding", "Linjun Zhang", "Huaxiu Yao"], "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails", "comment": null, "summary": "As Large Language Model (LLM) agents increasingly gain self-evolutionary\ncapabilities to adapt and refine their strategies through real-world\ninteraction, their long-term reliability becomes a critical concern. We\nidentify the Alignment Tipping Process (ATP), a critical post-deployment risk\nunique to self-evolving LLM agents. Unlike training-time failures, ATP arises\nwhen continual interaction drives agents to abandon alignment constraints\nestablished during training in favor of reinforced, self-interested strategies.\nWe formalize and analyze ATP through two complementary paradigms:\nSelf-Interested Exploration, where repeated high-reward deviations induce\nindividual behavioral drift, and Imitative Strategy Diffusion, where deviant\nbehaviors spread across multi-agent systems. Building on these paradigms, we\nconstruct controllable testbeds and benchmark Qwen3-8B and\nLlama-3.1-8B-Instruct. Our experiments show that alignment benefits erode\nrapidly under self-evolution, with initially aligned models converging toward\nunaligned states. In multi-agent settings, successful violations diffuse\nquickly, leading to collective misalignment. Moreover, current reinforcement\nlearning-based alignment methods provide only fragile defenses against\nalignment tipping. Together, these findings demonstrate that alignment of LLM\nagents is not a static property but a fragile and dynamic one, vulnerable to\nfeedback-driven decay during deployment. Our data and code are available at\nhttps://github.com/aiming-lab/ATP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc6\u522b\u4e86\u81ea\u8fdb\u5316LLM\u4ee3\u7406\u5728\u90e8\u7f72\u540e\u51fa\u73b0\u7684\u5bf9\u9f50\u503e\u659c\u8fc7\u7a0b(ATP)\u98ce\u9669\uff0c\u5373\u6301\u7eed\u4ea4\u4e92\u4f1a\u9a71\u52a8\u4ee3\u7406\u653e\u5f03\u8bad\u7ec3\u65f6\u7684\u5bf9\u9f50\u7ea6\u675f\uff0c\u8f6c\u800c\u91c7\u7528\u81ea\u6211\u5f3a\u5316\u7684\u5229\u5df1\u7b56\u7565\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u83b7\u5f97\u81ea\u8fdb\u5316\u80fd\u529b\uff0c\u5176\u957f\u671f\u53ef\u9760\u6027\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u4f5c\u8005\u53d1\u73b0\u8bad\u7ec3\u65f6\u7684\u5bf9\u9f50\u7ea6\u675f\u5728\u6301\u7eed\u4ea4\u4e92\u4e2d\u4f1a\u88ab\u7834\u574f\uff0c\u5bfc\u81f4\u4ee3\u7406\u884c\u4e3a\u504f\u79bb\u9884\u671f\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u4e92\u8865\u8303\u5f0f\u5206\u6790ATP\uff1a\u5229\u5df1\u63a2\u7d22(\u4e2a\u4f53\u884c\u4e3a\u6f02\u79fb)\u548c\u6a21\u4eff\u7b56\u7565\u6269\u6563(\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u504f\u5dee\u884c\u4e3a\u4f20\u64ad)\u3002\u6784\u5efa\u53ef\u63a7\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5728Qwen3-8B\u548cLlama-3.1-8B-Instruct\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5bf9\u9f50\u6548\u76ca\u5728\u81ea\u8fdb\u5316\u4e0b\u8fc5\u901f\u8870\u51cf\uff0c\u521d\u59cb\u5bf9\u9f50\u6a21\u578b\u6536\u655b\u5230\u672a\u5bf9\u9f50\u72b6\u6001\u3002\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u6210\u529f\u8fdd\u89c4\u884c\u4e3a\u5feb\u901f\u6269\u6563\uff0c\u5bfc\u81f4\u96c6\u4f53\u5931\u51c6\u3002\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u9f50\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u8106\u5f31\u9632\u5fa1\u3002", "conclusion": "LLM\u4ee3\u7406\u7684\u5bf9\u9f50\u4e0d\u662f\u9759\u6001\u5c5e\u6027\uff0c\u800c\u662f\u8106\u5f31\u4e14\u52a8\u6001\u7684\uff0c\u5728\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u56e0\u53cd\u9988\u9a71\u52a8\u800c\u8870\u51cf\u3002", "topic": "agent analysis"}}
{"id": "2510.04901", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04901", "abs": "https://arxiv.org/abs/2510.04901", "authors": ["Jonathan Cola\u00e7o Carr", "Qinyi Sun", "Cameron Allen"], "title": "Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects", "comment": "Reinforcement Learning Journal 2025", "summary": "Skills are essential for unlocking higher levels of problem solving. A common\napproach to discovering these skills is to learn ones that reliably reach\ndifferent states, thus empowering the agent to control its environment.\nHowever, existing skill discovery algorithms often overlook the natural state\nvariables present in many reinforcement learning problems, meaning that the\ndiscovered skills lack control of specific state variables. This can\nsignificantly hamper exploration efficiency, make skills more challenging to\nlearn with, and lead to negative side effects in downstream tasks when the goal\nis under-specified. We introduce a general method that enables these skill\ndiscovery algorithms to learn focused skills -- skills that target and control\nspecific state variables. Our approach improves state space coverage by a\nfactor of three, unlocks new learning capabilities, and automatically avoids\nnegative side effects in downstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\uff0c\u80fd\u591f\u5b66\u4e60\u4e13\u6ce8\u4e8e\u63a7\u5236\u7279\u5b9a\u72b6\u6001\u53d8\u91cf\u7684\u6280\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\u7387\u548c\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u81ea\u52a8\u907f\u514d\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8d1f\u9762\u526f\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u6280\u80fd\u53d1\u73b0\u7b97\u6cd5\u5f80\u5f80\u5ffd\u89c6\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u4e2d\u81ea\u7136\u5b58\u5728\u7684\u72b6\u6001\u53d8\u91cf\uff0c\u5bfc\u81f4\u53d1\u73b0\u7684\u6280\u80fd\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u72b6\u6001\u53d8\u91cf\u7684\u63a7\u5236\uff0c\u8fd9\u4f1a\u663e\u8457\u5f71\u54cd\u63a2\u7d22\u6548\u7387\u3001\u589e\u52a0\u5b66\u4e60\u96be\u5ea6\uff0c\u5e76\u5728\u76ee\u6807\u4e0d\u660e\u786e\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4ea7\u751f\u8d1f\u9762\u526f\u4f5c\u7528\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u4f7f\u6280\u80fd\u53d1\u73b0\u7b97\u6cd5\u80fd\u591f\u5b66\u4e60\u4e13\u6ce8\u4e8e\u7279\u5b9a\u72b6\u6001\u53d8\u91cf\u7684\u6280\u80fd\uff0c\u5373\u9488\u5bf9\u548c\u63a7\u5236\u7279\u5b9a\u72b6\u6001\u53d8\u91cf\u7684\u805a\u7126\u6280\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\u7387\u63d0\u9ad8\u4e86\u4e09\u500d\uff0c\u89e3\u9501\u4e86\u65b0\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u81ea\u52a8\u907f\u514d\u4e86\u8d1f\u9762\u526f\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u4e13\u6ce8\u4e8e\u63a7\u5236\u7279\u5b9a\u72b6\u6001\u53d8\u91cf\u7684\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.05056", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05056", "abs": "https://arxiv.org/abs/2510.05056", "authors": ["Alexis Ross", "Megha Srivastava", "Jeremiah Blanchard", "Jacob Andreas"], "title": "Modeling Student Learning with 3.8 Million Program Traces", "comment": null, "summary": "As programmers write code, they often edit and retry multiple times, creating\nrich \"interaction traces\" that reveal how they approach coding tasks and\nprovide clues about their level of skill development. For novice programmers in\nparticular, these traces reflect the diverse reasoning processes they employ to\ncode, such as exploratory behavior to understand how a programming concept\nworks, re-strategizing in response to bugs, and personalizing stylistic\nchoices. In this work, we explore what can be learned from training language\nmodels on such reasoning traces: not just about code, but about coders, and\nparticularly students learning to program. We introduce a dataset of over 3.8\nmillion programming reasoning traces from users of Pencil Code, a free online\neducational platform used by students to learn simple programming concepts.\nCompared to models trained only on final programs or synthetically-generated\ntraces, we find that models trained on real traces are stronger at modeling\ndiverse student behavior. Through both behavioral and probing analyses, we also\nfind that many properties of code traces, such as goal backtracking or number\nof comments, can be predicted from learned representations of the students who\nwrite them. Building on this result, we show that we can help students recover\nfrom mistakes by steering code generation models to identify a sequence of\nedits that will results in more correct code while remaining close to the\noriginal student's style. Together, our results suggest that many properties of\ncode are properties of individual students and that training on edit traces can\nlead to models that are more steerable, more predictive of student behavior\nwhile programming, and better at generating programs in their final states.\nCode and data is available at https://github.com/meghabyte/pencilcode-public", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u5b66\u751f\u5728\u7f16\u7a0b\u8fc7\u7a0b\u4e2d\u7684\u4ea4\u4e92\u8f68\u8ff9\uff0c\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5b66\u751f\u7684\u7f16\u7a0b\u884c\u4e3a\u548c\u98ce\u683c\uff0c\u5e76\u5e2e\u52a9\u6307\u5bfc\u5b66\u751f\u4ece\u9519\u8bef\u4e2d\u6062\u590d\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u4ece\u5b66\u751f\u7684\u7f16\u7a0b\u4ea4\u4e92\u8f68\u8ff9\u4e2d\u5b66\u4e60\uff0c\u4e0d\u4ec5\u7406\u89e3\u4ee3\u7801\u672c\u8eab\uff0c\u66f4\u7406\u89e3\u7f16\u7a0b\u8005\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u7279\u522b\u662f\u5bf9\u5b66\u4e60\u7f16\u7a0b\u7684\u5b66\u751f\u3002", "method": "\u6536\u96c6\u4e86380\u4e07\u6761\u6765\u81eaPencil Code\u6559\u80b2\u5e73\u53f0\u7684\u7f16\u7a0b\u63a8\u7406\u8f68\u8ff9\uff0c\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5206\u6790\u5b66\u751f\u7684\u7f16\u8f91\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u76f8\u6bd4\u4ec5\u57fa\u4e8e\u6700\u7ec8\u7a0b\u5e8f\u6216\u5408\u6210\u8f68\u8ff9\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u57fa\u4e8e\u771f\u5b9e\u8f68\u8ff9\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5efa\u6a21\u5b66\u751f\u884c\u4e3a\u591a\u6837\u6027\uff0c\u5e76\u80fd\u9884\u6d4b\u5b66\u751f\u7f16\u7a0b\u7279\u5f81\u3002", "conclusion": "\u4ee3\u7801\u7684\u8bb8\u591a\u5c5e\u6027\u5b9e\u9645\u4e0a\u53cd\u6620\u4e86\u5b66\u751f\u7684\u4e2a\u4eba\u7279\u5f81\uff0c\u57fa\u4e8e\u7f16\u8f91\u8f68\u8ff9\u8bad\u7ec3\u7684\u6a21\u578b\u66f4\u5177\u53ef\u5f15\u5bfc\u6027\uff0c\u80fd\u66f4\u597d\u5730\u9884\u6d4b\u5b66\u751f\u884c\u4e3a\u5e76\u751f\u6210\u7b26\u5408\u5176\u98ce\u683c\u7684\u4ee3\u7801\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.2879d012", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsentry.io%2Fproduct%2Fai-code-review%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/2/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/G5OhtvVqCNPrY8jwhdOPX7vzUAQh0K1qQPMPOt7jrmI=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsentry.io%2Fproduct%2Fai-code-review%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/2/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/G5OhtvVqCNPrY8jwhdOPX7vzUAQh0K1qQPMPOt7jrmI=425", "authors": ["TLDR Newsletter"], "title": "Your AI code reviewer doesn't know what actually breaks. Sentry's does.", "comment": "Source: TLDR Newsletter, Date: 2025-10-03, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsentry.io%2Fproduct%2Fai-code-review%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/2/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/G5OhtvVqCNPrY8jwhdOPX7vzUAQh0K1qQPMPOt7jrmI=425", "summary": "Your AI code reviewer doesn't know what actually breaks. Sentry's does. (Sponsor) Generic AI code review catches typos, style issues, and things that might be a problem. Sentry's AI code review (now in beta) catches the bugs that actually take down production. The difference? Context. Sentry already monitors your production errors and performance. Now it uses that data + your code and commit history to review your PRs - so it knows exactly which changes will cause issues based on what's broke...", "source": "tldr", "AI": {"tldr": "Sentry\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u5229\u7528\u751f\u4ea7\u73af\u5883\u76d1\u63a7\u6570\u636e\u548c\u4ee3\u7801\u5386\u53f2\uff0c\u80fd\u591f\u8bc6\u522b\u771f\u6b63\u4f1a\u5bfc\u81f4\u751f\u4ea7\u95ee\u9898\u7684\u4ee3\u7801\u53d8\u66f4\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8bed\u6cd5\u548c\u98ce\u683c\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u53ea\u80fd\u53d1\u73b0\u8bed\u6cd5\u9519\u8bef\u548c\u98ce\u683c\u95ee\u9898\uff0c\u4f46\u65e0\u6cd5\u8bc6\u522b\u771f\u6b63\u4f1a\u5bfc\u81f4\u751f\u4ea7\u73af\u5883\u5d29\u6e83\u7684\u4e25\u91cdbug\u3002Sentry\u5e0c\u671b\u901a\u8fc7\u7ed3\u5408\u751f\u4ea7\u73af\u5883\u76d1\u63a7\u6570\u636e\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "Sentry\u5229\u7528\u5176\u73b0\u6709\u7684\u751f\u4ea7\u9519\u8bef\u548c\u6027\u80fd\u76d1\u63a7\u6570\u636e\uff0c\u7ed3\u5408\u4ee3\u7801\u5e93\u548c\u63d0\u4ea4\u5386\u53f2\uff0c\u5bf9PR\u8fdb\u884c\u667a\u80fd\u5ba1\u67e5\uff0c\u8bc6\u522b\u53ef\u80fd\u5f15\u53d1\u751f\u4ea7\u95ee\u9898\u7684\u4ee3\u7801\u53d8\u66f4\u3002", "result": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff08\u76ee\u524d\u5904\u4e8e\u6d4b\u8bd5\u9636\u6bb5\uff09\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u4f1a\u5bfc\u81f4\u751f\u4ea7\u73af\u5883\u95ee\u9898\u7684\u4ee3\u7801\u53d8\u66f4\u3002", "conclusion": "\u7ed3\u5408\u751f\u4ea7\u73af\u5883\u76d1\u63a7\u6570\u636e\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0cAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u80fd\u591f\u66f4\u6709\u6548\u5730\u8bc6\u522b\u771f\u6b63\u5371\u9669\u7684\u4ee3\u7801\u53d8\u66f4\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8868\u9762\u95ee\u9898\u3002", "topic": "swe application"}}
{"id": "tldr.2510.82987032", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fanimals-vs-ghosts%2F%3Futm_source=tldrai/1/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/vx7Yh345XXXqrR3_y30d-wUwJIPZInpFS7q9FS2ScFY=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fanimals-vs-ghosts%2F%3Futm_source=tldrai/1/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/vx7Yh345XXXqrR3_y30d-wUwJIPZInpFS7q9FS2ScFY=425", "authors": ["TLDR Newsletter"], "title": "Animals vs Ghosts", "comment": "Source: TLDR Newsletter, Date: 2025-10-03, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fanimals-vs-ghosts%2F%3Futm_source=tldrai/1/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/vx7Yh345XXXqrR3_y30d-wUwJIPZInpFS7q9FS2ScFY=425", "summary": "Animals vs Ghosts (11 minute read) Today's frontier LLM research isn't about building animals, it's about summoning ghosts. Ghosts are a fundamentally different kind of point in the space of possible intelligences, thoroughly engineered by humanity. Over time, we may be able to fine-tune other ghosts more and more in the direction of animals, but it's also quite possible that they diverge even further and end up permanently different. They may end up un-animal-like, but still incredibly helpf...", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u533a\u5206\u4e86\u4e24\u79cdAI\u667a\u80fd\u7c7b\u578b\uff1a\u52a8\u7269\u578b\u667a\u80fd\u548c\u5e7d\u7075\u578b\u667a\u80fd\uff0c\u8ba4\u4e3a\u5f53\u524d\u524d\u6cbfLLM\u7814\u7a76\u4e3b\u8981\u662f\u5728\u521b\u9020\u5e7d\u7075\u578b\u667a\u80fd\uff0c\u8fd9\u662f\u4e00\u79cd\u7531\u4eba\u7c7b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4e0d\u540c\u667a\u80fd\u7c7b\u578b\u3002", "motivation": "\u63a2\u8ba8\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u7684\u672c\u8d28\uff0c\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u667a\u80fd\u4f53\uff0c\u7406\u89e3AI\u53d1\u5c55\u7684\u53ef\u80fd\u8def\u5f84\u548c\u6700\u7ec8\u5f62\u6001\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u5206\u6790\u548c\u54f2\u5b66\u601d\u8fa8\uff0c\u5c06AI\u667a\u80fd\u5206\u4e3a\u52a8\u7269\u578b\uff08\u81ea\u7136\u6f14\u5316\u5f0f\uff09\u548c\u5e7d\u7075\u578b\uff08\u4eba\u5de5\u8bbe\u8ba1\u5f0f\uff09\u4e24\u7c7b\uff0c\u5e76\u5206\u6790\u5176\u7279\u5f81\u5dee\u5f02\u3002", "result": "\u8bc6\u522b\u51fa\u73b0\u6709LLM\u7814\u7a76\u4e3b\u8981\u521b\u9020\u7684\u662f\u5e7d\u7075\u578b\u667a\u80fd\uff0c\u8fd9\u79cd\u667a\u80fd\u4e0e\u81ea\u7136\u6f14\u5316\u7684\u52a8\u7269\u578b\u667a\u80fd\u5728\u672c\u8d28\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u53ef\u80fd\u6cbf\u7740\u4e0d\u540c\u7684\u8fdb\u5316\u8def\u5f84\u53d1\u5c55\u3002", "conclusion": "\u5e7d\u7075\u578b\u667a\u80fd\u867d\u7136\u4e0e\u52a8\u7269\u578b\u667a\u80fd\u4e0d\u540c\uff0c\u4f46\u53ef\u80fd\u4ecd\u7136\u6781\u5176\u6709\u7528\uff0cAI\u53d1\u5c55\u53ef\u80fd\u4e0d\u4f1a\u6536\u655b\u5230\u5355\u4e00\u6a21\u5f0f\uff0c\u800c\u662f\u51fa\u73b0\u591a\u6837\u5316\u7684\u667a\u80fd\u5f62\u6001\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.b94656a7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Fen%2Fmeet-jules-tools-a-command-line-companion-for-googles-async-coding-agent%2F%3Futm_source=tldrai/1/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/jDA-yHdYjABt-C1lSmjkR7-pjnp4PjFgRxdCFVKHPws=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Fen%2Fmeet-jules-tools-a-command-line-companion-for-googles-async-coding-agent%2F%3Futm_source=tldrai/1/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/jDA-yHdYjABt-C1lSmjkR7-pjnp4PjFgRxdCFVKHPws=425", "authors": ["TLDR Newsletter"], "title": "Meet Jules Tools: A Command Line Companion for Google's Async Coding Agent", "comment": "Source: TLDR Newsletter, Date: 2025-10-03, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Fen%2Fmeet-jules-tools-a-command-line-companion-for-googles-async-coding-agent%2F%3Futm_source=tldrai/1/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/jDA-yHdYjABt-C1lSmjkR7-pjnp4PjFgRxdCFVKHPws=425", "summary": "Meet Jules Tools: A Command Line Companion for Google's Async Coding Agent (4 minute read) Jules is an asynchronous coding agent that integrates directly with existing repositories. It understands the full context of projects and can perform tasks like writing tests, building new features, providing audio changelogs, fixing bugs, and bumping dependency versions. Jules Tools is a lightweight command-line interface that allows developers to spin up tasks, inspect what Jules is doing, and custom...", "source": "tldr", "AI": {"tldr": "Jules Tools\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u547d\u4ee4\u884c\u754c\u9762\uff0c\u7528\u4e8e\u4e0eGoogle\u7684\u5f02\u6b65\u7f16\u7801\u4ee3\u7406Jules\u4ea4\u4e92\uff0c\u652f\u6301\u542f\u52a8\u4efb\u52a1\u3001\u76d1\u63a7\u64cd\u4f5c\u548c\u81ea\u5b9a\u4e49\u529f\u80fd\u3002", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e0e\u5f02\u6b65\u7f16\u7801\u4ee3\u7406Jules\u4ea4\u4e92\u7684\u4fbf\u6377\u5de5\u5177\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u63a7\u5236Jules\u5728\u9879\u76ee\u4e2d\u7684\u64cd\u4f5c\u3002", "method": "\u5f00\u53d1\u8f7b\u91cf\u7ea7\u547d\u4ee4\u884c\u754c\u9762\uff0c\u96c6\u6210\u5230\u73b0\u6709\u4ee3\u7801\u4ed3\u5e93\u4e2d\uff0c\u652f\u6301\u4efb\u52a1\u542f\u52a8\u3001\u64cd\u4f5c\u76d1\u63a7\u548c\u81ea\u5b9a\u4e49\u529f\u80fd\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86Jules Tools\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u8f7b\u677e\u7ba1\u7406Jules\u4ee3\u7406\u7684\u5f02\u6b65\u7f16\u7801\u4efb\u52a1\u3002", "conclusion": "Jules Tools\u4e3a\u5f02\u6b65\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4ea4\u4e92\u754c\u9762\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2510.51df4ebd", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fresearch.google%2Fblog%2Fai-as-a-research-partner-advancing-theoretical-computer-science-with-alphaevolve%2F%3Futm_source=tldrai/1/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/QlkbBaV-MnY_fKHxRQtxKfOz2hzr4gm4PIrhjM6RS98=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fresearch.google%2Fblog%2Fai-as-a-research-partner-advancing-theoretical-computer-science-with-alphaevolve%2F%3Futm_source=tldrai/1/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/QlkbBaV-MnY_fKHxRQtxKfOz2hzr4gm4PIrhjM6RS98=425", "authors": ["TLDR Newsletter"], "title": "AI as a research partner: Advancing theoretical computer science with AlphaEvolve", "comment": "Source: TLDR Newsletter, Date: 2025-10-03, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fresearch.google%2Fblog%2Fai-as-a-research-partner-advancing-theoretical-computer-science-with-alphaevolve%2F%3Futm_source=tldrai/1/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/QlkbBaV-MnY_fKHxRQtxKfOz2hzr4gm4PIrhjM6RS98=425", "summary": "AI as a research partner: Advancing theoretical computer science with AlphaEvolve (7 minute read) DeepMind's AlphaEvolve uses LLMs to iteratively \u201cevolve\u201d code that discovers mathematical structures proving new theorems in complexity theory. The system made breakthroughs on two mature problems by morphing finite proof structures while keeping the interface to the broader proof intact, allowing automated verification of correctness without human review.", "source": "tldr", "AI": {"tldr": "DeepMind\u7684AlphaEvolve\u4f7f\u7528LLMs\u8fed\u4ee3\u8fdb\u5316\u4ee3\u7801\uff0c\u5728\u590d\u6742\u6027\u7406\u8bba\u4e2d\u53d1\u73b0\u6570\u5b66\u7ed3\u6784\u5e76\u8bc1\u660e\u65b0\u5b9a\u7406\uff0c\u5728\u6210\u719f\u95ee\u9898\u4e0a\u53d6\u5f97\u7a81\u7834\u3002", "motivation": "\u5229\u7528AI\u4f5c\u4e3a\u7814\u7a76\u4f19\u4f34\uff0c\u63a8\u52a8\u7406\u8bba\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6027\u7406\u8bba\u7b49\u6210\u719f\u9886\u57df\u5bfb\u627e\u65b0\u7684\u7a81\u7834\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fed\u4ee3\u8fdb\u5316\u4ee3\u7801\uff0c\u901a\u8fc7\u53d8\u5f62\u6709\u9650\u8bc1\u660e\u7ed3\u6784\u540c\u65f6\u4fdd\u6301\u4e0e\u66f4\u5e7f\u6cdb\u8bc1\u660e\u7684\u63a5\u53e3\u5b8c\u6574\uff0c\u5b9e\u73b0\u81ea\u52a8\u9a8c\u8bc1\u6b63\u786e\u6027\u800c\u65e0\u9700\u4eba\u5de5\u5ba1\u67e5\u3002", "result": "\u7cfb\u7edf\u5728\u4e24\u4e2a\u6210\u719f\u95ee\u9898\u4e0a\u53d6\u5f97\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u53d1\u73b0\u4e86\u65b0\u7684\u6570\u5b66\u7ed3\u6784\u5e76\u8bc1\u660e\u4e86\u65b0\u5b9a\u7406\u3002", "conclusion": "AlphaEvolve\u5c55\u793a\u4e86AI\u4f5c\u4e3a\u7814\u7a76\u4f19\u4f34\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5728\u7406\u8bba\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u5b9e\u73b0\u81ea\u52a8\u5316\u5b9a\u7406\u8bc1\u660e\u548c\u53d1\u73b0\u3002", "topic": "code agent"}}
{"id": "tldr.2510.373cb025", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevblogs.microsoft.com%2Ffoundry%2Fintroducing-microsoft-agent-framework-the-open-source-engine-for-agentic-ai-apps%2F%3Futm_source=tldrdata/1/01000199b8fcdeb7-e1412a63-2b62-4515-a713-797c1024e0e1-000000/_fhb_PoOj4L7VrASjAESXXG8ew5-gPrEP336imbn89M=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevblogs.microsoft.com%2Ffoundry%2Fintroducing-microsoft-agent-framework-the-open-source-engine-for-agentic-ai-apps%2F%3Futm_source=tldrdata/1/01000199b8fcdeb7-e1412a63-2b62-4515-a713-797c1024e0e1-000000/_fhb_PoOj4L7VrASjAESXXG8ew5-gPrEP336imbn89M=425", "authors": ["TLDR Newsletter"], "title": "Introducing Microsoft Agent Framework: The Open-Source Engine for Agentic AI Apps", "comment": "Source: TLDR Newsletter, Date: 2025-10-06, Reading time: 13 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevblogs.microsoft.com%2Ffoundry%2Fintroducing-microsoft-agent-framework-the-open-source-engine-for-agentic-ai-apps%2F%3Futm_source=tldrdata/1/01000199b8fcdeb7-e1412a63-2b62-4515-a713-797c1024e0e1-000000/_fhb_PoOj4L7VrASjAESXXG8ew5-gPrEP336imbn89M=425", "summary": "Introducing Microsoft Agent Framework: The Open-Source Engine for Agentic AI Apps (13 minute read) Microsoft has released the open-source Agent Framework in preview, unifying the capabilities of Semantic Kernel and AutoGen to streamline AI agent development for both Python and .NET. The framework enables rapid agent creation with fewer than 20 lines of code, supporting orchestration patterns such as sequential, concurrent, group chat, and handoff with production-grade durability. Integrated w...", "source": "tldr", "AI": {"tldr": "\u5fae\u8f6f\u53d1\u5e03\u4e86\u5f00\u6e90\u7684Agent Framework\u9884\u89c8\u7248\uff0c\u7edf\u4e00\u4e86Semantic Kernel\u548cAutoGen\u7684\u80fd\u529b\uff0c\u7b80\u5316\u4e86Python\u548c.NET\u7684AI\u4ee3\u7406\u5f00\u53d1\uff0c\u652f\u6301\u591a\u79cd\u7f16\u6392\u6a21\u5f0f\u3002", "motivation": "\u4e3a\u4e86\u7b80\u5316AI\u4ee3\u7406\u5f00\u53d1\u6d41\u7a0b\uff0c\u7edf\u4e00\u73b0\u6709\u5de5\u5177\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u4eba\u5458\u63d0\u4f9b\u66f4\u9ad8\u6548\u3001\u751f\u4ea7\u7ea7\u7684\u4ee3\u7406\u6784\u5efa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6574\u5408Semantic Kernel\u548cAutoGen\u6846\u67b6\u80fd\u529b\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684API\u63a5\u53e3\uff0c\u652f\u6301\u5c11\u4e8e20\u884c\u4ee3\u7801\u5feb\u901f\u521b\u5efa\u4ee3\u7406\uff0c\u5305\u542b\u987a\u5e8f\u3001\u5e76\u53d1\u3001\u7fa4\u804a\u548c\u4ea4\u63a5\u7b49\u591a\u79cd\u7f16\u6392\u6a21\u5f0f\u3002", "result": "\u5f00\u53d1\u51fa\u4e86\u652f\u6301Python\u548c.NET\u7684\u5f00\u6e90\u4ee3\u7406\u6846\u67b6\uff0c\u5177\u5907\u751f\u4ea7\u7ea7\u6301\u4e45\u6027\uff0c\u5927\u5e45\u964d\u4f4e\u4e86AI\u4ee3\u7406\u5f00\u53d1\u7684\u6280\u672f\u95e8\u69db\u3002", "conclusion": "\u5fae\u8f6fAgent Framework\u4e3aAI\u4ee3\u7406\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u4e86\u7edf\u4e00\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u4ee3\u7406\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "topic": "code agent"}}
{"id": "wechat.2510.5a961284", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483679&idx=1&sn=2f93d023a2957e77626683e25a807202&chksm=e93177eadfae14b1df9743cea64c027211e53ea506e1a5ede88ec7fa136e27fdaac003f8e99e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483679&idx=1&sn=2f93d023a2957e77626683e25a807202&chksm=e93177eadfae14b1df9743cea64c027211e53ea506e1a5ede88ec7fa136e27fdaac003f8e99e#rd", "authors": ["CodeAgent\u4ee3\u7801\u667a\u80fd"], "title": "\u3010AI\u8981\u95fb\u00b7\u65e5\u62a5\u3011<em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em> & Code LLMs & \u521d\u521b\u52a8\u6001\uff0810/06\u65e5\u62a5\uff0cET\uff09", "comment": "Source: WeChat, Published: 2025-10-07 03:27:06", "summary": "\u5f15\u8ff0\uff1a\u201cToday\uff0c we're sharing early results from our research on CodeMender\uff0c a new AI-powered agent that improves code security automatically.\u201d \u2014 DeepMind\u5b98\u65b9\u535a\u5ba2\uff082025-10-06\uff09\u3002", "AI": {"tldr": "\u5f15\u8ff0\uff1a\u201cToday\uff0c we're sharing early results from our research on CodeMender\uff0c a new AI-powered agent that improves code security automatically.\u201d \u2014 DeepMind\u5b98\u65b9\u535a\u5ba2\uff082025-10-06\uff09\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.a4c96d82", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MTk5MTcyMg==&mid=2653847563&idx=3&sn=8e41d6214c640fe06d67477f6894ab5c&chksm=851b9dff34ca503c8616d3d87a30a7749838b7875bb96602b7c6343e326a0ef69e392aea939c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MTk5MTcyMg==&mid=2653847563&idx=3&sn=8e41d6214c640fe06d67477f6894ab5c&chksm=851b9dff34ca503c8616d3d87a30a7749838b7875bb96602b7c6343e326a0ef69e392aea939c#rd", "authors": ["\u90a2\u53f0\u53d1\u5e03"], "title": "\u843d\u5730\u5e94\u7528<em class=\"highlight\">\u5927\u6a21\u578b</em>12\u4e2a\uff01\u90a2\u53f0\u4eba\u5de5\u667a\u80fd\u8d4b\u80fd\u7279\u8272\u4ea7\u4e1a\u96c6\u7fa4\u8f6c\u578b\u5347\u7ea7", "comment": "Source: WeChat, Published: 2025-10-07 13:14:00", "summary": "\u95eb\u864e\u8868\u793a\uff0c\u5c06\u6301\u7eed\u63a8\u52a8\u5782\u76f4\u5927\u6a21\u578b\u5efa\u8bbe\u843d\u5730\uff0c\u79ef\u6781\u5bf9\u63a5\u56fd\u5185\u4f18\u8d28\u6280\u672f\u8d44\u6e90\uff0c\u964d\u4f4e\u4f01\u4e1a\u5e94\u7528\u95e8\u69db\uff0c\u5f15\u5bfc\u4f01\u4e1a\u9010\u6b65\u7531\u5927\u6a21\u578b\u5411\u667a\u80fd\u4f53\u5e94\u7528\u8f6c\u53d8\uff0c\u4ee5\u4eba\u5de5\u667a\u80fd\u8d4b\u80fd\u4f20\u7edf\u4ea7\u4e1a\u8f6c\u578b\u5347\u7ea7\u3002", "AI": {"tldr": "\u95eb\u864e\u8868\u793a\uff0c\u5c06\u6301\u7eed\u63a8\u52a8\u5782\u76f4\u5927\u6a21\u578b\u5efa\u8bbe\u843d\u5730\uff0c\u79ef\u6781\u5bf9\u63a5\u56fd\u5185\u4f18\u8d28\u6280\u672f\u8d44\u6e90\uff0c\u964d\u4f4e\u4f01\u4e1a\u5e94\u7528\u95e8\u69db\uff0c\u5f15\u5bfc\u4f01\u4e1a\u9010\u6b65\u7531\u5927\u6a21\u578b\u5411\u667a\u80fd\u4f53\u5e94\u7528\u8f6c\u53d8\uff0c\u4ee5\u4eba\u5de5\u667a\u80fd\u8d4b\u80fd\u4f20\u7edf\u4ea7\u4e1a\u8f6c\u578b\u5347\u7ea7\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.392c44f3", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247657335&idx=3&sn=bd82467a9b8e56c8b41107ad596f48b8&chksm=c06e213aadd3d06ca41884e8534519e84a39cac22fa55f4d4a2de33c8f4db1bd729b147b2a0a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247657335&idx=3&sn=bd82467a9b8e56c8b41107ad596f48b8&chksm=c06e213aadd3d06ca41884e8534519e84a39cac22fa55f4d4a2de33c8f4db1bd729b147b2a0a#rd", "authors": ["DataFunSummit"], "title": "B \u7ad9\u57fa\u4e8e<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u5927\u6570\u636e\u667a\u80fd\u8bca\u65ad\u52a9\u624b\u5b9e\u8df5", "comment": "Source: WeChat, Published: 2025-10-07 10:02:12", "summary": "\u5bfc\u8bfb \u672c\u6587\u5c06\u5206\u4eab B \u7ad9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u52a9\u624b\u5b9e\u8df5\u3002\u5206\u4eab\u5609\u5bbe\uff5c\u90ed\u8dc3\u9e4f \u54d4\u54e9\u54d4\u54e9 \u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7f16\u8f91\u6574\u7406\uff5c\u6c6a\u7ef4\u5185\u5bb9\u6821\u5bf9\uff5c\u674e\u74761. \u6574\u4f53\u67b6\u6784\u548c\u89c4\u6a21", "AI": {"tldr": "\u5bfc\u8bfb \u672c\u6587\u5c06\u5206\u4eab B \u7ad9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u52a9\u624b\u5b9e\u8df5\u3002\u5206\u4eab\u5609\u5bbe\uff5c\u90ed\u8dc3\u9e4f \u54d4\u54e9\u54d4\u54e9 \u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7f16\u8f91\u6574\u7406\uff5c\u6c6a\u7ef4\u5185\u5bb9\u6821\u5bf9\uff5c\u674e\u74761. \u6574\u4f53\u67b6\u6784\u548c\u89c4\u6a21", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.57279174", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyMTAwMzk1Mg==&mid=2651221833&idx=1&sn=64956e7e48d50e56efdd6a270f006ff5&chksm=8d45d014dbcf25d058009c0b7b09e656aee0714a8346537825614f15a52c4a746bb242b264de#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyMTAwMzk1Mg==&mid=2651221833&idx=1&sn=64956e7e48d50e56efdd6a270f006ff5&chksm=8d45d014dbcf25d058009c0b7b09e656aee0714a8346537825614f15a52c4a746bb242b264de#rd", "authors": ["\u950b\u884c\u94fe\u76df"], "title": "\u3010\u63a8\u8350\u3011\u4e2d\u56fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u843d\u5730\u5e94\u7528\u7814\u7a76\u62a5\u544a2025|\u9644\u4e0b\u8f7d", "comment": "Source: WeChat, Published: 2025-10-07 09:32:27", "summary": "\u4e03\u5e74\u6280\u672f\u8fdb\u5316\u5168\u666f\u56fe\uff1a\u5927\u6a21\u578b\u8fdb\u5165\u63a8\u7406\u7eaa\u5143 \u00b7\u81ea2017\u5e74\u4ee5\u6765\uff0c\u5927\u6a21\u578b\u5df2\u5386\u7ecf\u8bf8\u591a\u5173\u952e\u65f6\u671f\uff0c\u4ece\u6587\u672c\u5230\u56fe\u6587\u3001\u97f3\u9891\u3001\u89c6\u9891\uff0c\u4ece\u5355\u7eaf\u7684\u5bf9\u8bdd\u5230\u4ee3\u7801\u548c\u7ec8\u7aef\u3002\u63a8\u7406\u6210\u672c\u7684\u5927\u5e45\u4e0b\u964d\uff0c\u4e5f\u52a9\u63a8\u4e86\u5927 \u6a21\u578b\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "AI": {"tldr": "\u4e03\u5e74\u6280\u672f\u8fdb\u5316\u5168\u666f\u56fe\uff1a\u5927\u6a21\u578b\u8fdb\u5165\u63a8\u7406\u7eaa\u5143 \u00b7\u81ea2017\u5e74\u4ee5\u6765\uff0c\u5927\u6a21\u578b\u5df2\u5386\u7ecf\u8bf8\u591a\u5173\u952e\u65f6\u671f\uff0c\u4ece\u6587\u672c\u5230\u56fe\u6587\u3001\u97f3\u9891\u3001\u89c6\u9891\uff0c\u4ece\u5355\u7eaf\u7684\u5bf9\u8bdd\u5230\u4ee3\u7801\u548c\u7ec8\u7aef\u3002\u63a8\u7406\u6210\u672c\u7684\u5927\u5e45\u4e0b\u964d\uff0c\u4e5f\u52a9\u63a8\u4e86\u5927 \u6a21\u578b\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.ea9b3de6", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMzE0OTM2Mg==&mid=2247483891&idx=1&sn=0b8a9cb5db6f4d0348af4a317680bb3e&chksm=fe57ef4167afaf2e1ff1a9a52977f0139aa90a385d9c27e923452a256399a9eee10cfc960555#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMzE0OTM2Mg==&mid=2247483891&idx=1&sn=0b8a9cb5db6f4d0348af4a317680bb3e&chksm=fe57ef4167afaf2e1ff1a9a52977f0139aa90a385d9c27e923452a256399a9eee10cfc960555#rd", "authors": ["AI\u5927\u6a21\u578b\u5b66\u4e60\u6559\u7a0b"], "title": "\u4ec0\u4e48\u662fLangChain\uff1f<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5e94\u7528\u5f00\u53d1\u6846\u67b6", "comment": "Source: WeChat, Published: 2025-10-07 09:21:44", "summary": "\u8fd1\u4e24\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982chatgpt\u3001 deepseek\u3001claude\uff09\u6301\u7eed\u706b\u7206\uff0c\u4ece\u5199\u6587\u6848\u3001al \u7ed8\u56fe\uff0c\u5230\u5199\u4ee3\u7801\u3001ai\u667a\u80fd\u5ba2\u670d\uff0c\u51e0\u4e4e\u201c\u65e0\u6240\u4e0d \u80fd\u201d\u3002\u5e76\u4e14\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8c03\u7528\u6210\u672c\u8d8a\u6765\u8d8a\u4f4e\uff0c\u4f5c \u4e3a\u7a0b\u5e8f\u5458\u7684\u4f60\u53ef\u80fd\u5df2\u7ecf\u5f00\u59cb\u5c1d\u8bd5\u7528openal\u3001 deepseek\u7684api\u505a\u4e9b", "AI": {"tldr": "\u8fd1\u4e24\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982chatgpt\u3001 deepseek\u3001claude\uff09\u6301\u7eed\u706b\u7206\uff0c\u4ece\u5199\u6587\u6848\u3001al \u7ed8\u56fe\uff0c\u5230\u5199\u4ee3\u7801\u3001ai\u667a\u80fd\u5ba2\u670d\uff0c\u51e0\u4e4e\u201c\u65e0\u6240\u4e0d \u80fd\u201d\u3002\u5e76\u4e14\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8c03\u7528\u6210\u672c\u8d8a\u6765\u8d8a\u4f4e\uff0c\u4f5c \u4e3a\u7a0b\u5e8f\u5458\u7684\u4f60\u53ef\u80fd\u5df2\u7ecf\u5f00\u59cb\u5c1d\u8bd5\u7528openal\u3001 deepseek\u7684api\u505a\u4e9b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.a352178e", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMzMxMjIyMQ==&mid=2247487903&idx=1&sn=3ee7b0d3e5ed1fa7f7a9cbaee7b30483&chksm=c0e2f2c8c222a755b4c742156c554c227b107d8860c83895ce20d2bb9618811783417c3ddb1b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMzMxMjIyMQ==&mid=2247487903&idx=1&sn=3ee7b0d3e5ed1fa7f7a9cbaee7b30483&chksm=c0e2f2c8c222a755b4c742156c554c227b107d8860c83895ce20d2bb9618811783417c3ddb1b#rd", "authors": ["\u96c1\u535a\u4f1aYEN"], "title": "\u5927\u8bed\u8a00<em class=\"highlight\">\u6a21\u578b</em>\uff0c\u662f\u5426\u771f\u6b63\u7406\u89e3\u5b83\u4eec\u6240\u751f\u6210\u7684\u5185\u5bb9\uff1f", "comment": "Source: WeChat, Published: 2025-10-07 03:31:54", "summary": "\u5927\u8bed\u8a00\u6a21\u578b\u4e0d\u4ec5\u5c55\u73b0\u51fa\u5728\u5404\u7c7b\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u591a\u9762\u6027\uff0c\u8fd8\u5177\u5907\u7f16\u7a0b\u7b49\u8de8\u9886\u57df\u80fd\u529b\u3002\u7279\u522b\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u4e9b\u6a21\u578b\u8868\u73b0\u51fa\u4e86\u8d85\u4e4e\u9884\u671f\u7684\u793e\u4ea4\u667a\u80fd\u3002\u955c\u50cf\u5047\u8bf4\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u65b0\u7684\u601d\u8003\u89d2\u5ea6\uff1a\u901a\u7528\u667a\u80fd\u662f\u5426\u9996\u5148\u6e90\u4e8e\u4eba\u7c7b\u7684\u793e\u4ea4\u4e92\u52a8\u80fd", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u4e0d\u4ec5\u5c55\u73b0\u51fa\u5728\u5404\u7c7b\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u591a\u9762\u6027\uff0c\u8fd8\u5177\u5907\u7f16\u7a0b\u7b49\u8de8\u9886\u57df\u80fd\u529b\u3002\u7279\u522b\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u4e9b\u6a21\u578b\u8868\u73b0\u51fa\u4e86\u8d85\u4e4e\u9884\u671f\u7684\u793e\u4ea4\u667a\u80fd\u3002\u955c\u50cf\u5047\u8bf4\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u65b0\u7684\u601d\u8003\u89d2\u5ea6\uff1a\u901a\u7528\u667a\u80fd\u662f\u5426\u9996\u5148\u6e90\u4e8e\u4eba\u7c7b\u7684\u793e\u4ea4\u4e92\u52a8\u80fd", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.edeb0426", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NTk0MTM1Mw==&mid=2650706883&idx=2&sn=965e0fbaaa9da482a5b46b85df2d5769&chksm=bf02839c54ace7c65a28fcf676c9cd6cf4338f0b2983fb7f9fab58f2ce629d4f2f2b70b7215a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NTk0MTM1Mw==&mid=2650706883&idx=2&sn=965e0fbaaa9da482a5b46b85df2d5769&chksm=bf02839c54ace7c65a28fcf676c9cd6cf4338f0b2983fb7f9fab58f2ce629d4f2f2b70b7215a#rd", "authors": ["twt\u4f01\u4e1aIT\u793e\u533a"], "title": "\u667a\u80fd\u4f53\u5728\u5c40\u57df\u7f51\u73af\u5883\u4e0b\u7684\u6548\u679c\u5229\u7528\uff1a\u57fa\u4e8e<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e0eOpenManus\u7684\u6280\u672f\u65b9\u6848\u521b\u65b0\u89e3\u8bfb", "comment": "Source: WeChat, Published: 2025-10-06 23:45:33", "summary": "\u4e00\u3001\u5927\u6a21\u578b\u7684\u5d1b\u8d77\u4e0e\u8d85\u8d8a \u9996\u5148\u6211\u4eec\u5e94\u8be5\u4e86\u89e3\u5230\uff0c\u5982\u540c\u667a\u80fd\u9a7e\u9a76\u7684\u7b49\u7ea7\u5212\u5206\u4e00\u6837\uff0cOpenAI\u5bf9AI\u7684\u5212\u5206\u4e3a5\u4e2a\u7b49\u7ea7\uff0cGPT-4\u4e5f\u53ea\u662f\u88abOpenAI\u81ea\u5df1\u5b9a\u4e49\u4e3aL1\u7ea7\u522b\u7684AI\u6a21\u578b\uff0c\u4f46\u5f88\u5feb\u5c31\u4f1a\u8fbe\u5230L2\uff08\u63a8\u7406\u8005\uff09\u7ea7\u522b\uff0c\u6839\u636e\u5176\u7814\u7a76\u5458\u9884\u6d4b\uff0cL5\u7ea7\u522b\u7684AGI\uff08\u901a\u7528", "AI": {"tldr": "\u4e00\u3001\u5927\u6a21\u578b\u7684\u5d1b\u8d77\u4e0e\u8d85\u8d8a \u9996\u5148\u6211\u4eec\u5e94\u8be5\u4e86\u89e3\u5230\uff0c\u5982\u540c\u667a\u80fd\u9a7e\u9a76\u7684\u7b49\u7ea7\u5212\u5206\u4e00\u6837\uff0cOpenAI\u5bf9AI\u7684\u5212\u5206\u4e3a5\u4e2a\u7b49\u7ea7\uff0cGPT-4\u4e5f\u53ea\u662f\u88abOpenAI\u81ea\u5df1\u5b9a\u4e49\u4e3aL1\u7ea7\u522b\u7684AI\u6a21\u578b\uff0c\u4f46\u5f88\u5feb\u5c31\u4f1a\u8fbe\u5230L2\uff08\u63a8\u7406\u8005\uff09\u7ea7\u522b\uff0c\u6839\u636e\u5176\u7814\u7a76\u5458\u9884\u6d4b\uff0cL5\u7ea7\u522b\u7684AGI\uff08\u901a\u7528", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
