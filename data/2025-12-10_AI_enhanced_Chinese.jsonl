{"id": "2512.08082", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08082", "abs": "https://arxiv.org/abs/2512.08082", "authors": ["Vala Vakilian", "Zimeng Wang", "Ankit Singh Rawat", "Christos Thrampoulidis"], "title": "Short-Context Dominance: How Much Local Context Natural Language Actually Needs?", "comment": "38 pages, 7 figures, includes appendix and references", "summary": "We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u77ed\u4e0a\u4e0b\u6587\u4e3b\u5bfc\u5047\u8bf4\uff0c\u53d1\u73b0\u5927\u591a\u6570\u5e8f\u5217\u4ec5\u9700\u5c11\u91cf\u5c40\u90e8\u524d\u7f00\u5373\u53ef\u9884\u6d4b\u4e0b\u4e00\u4e2atoken\uff0c\u5e76\u63d0\u51fa\u4e86\u68c0\u6d4b\u9700\u8981\u957f\u4e0a\u4e0b\u6587\u7684\u5e8f\u5217\u7684\u65b9\u6cd5\u4ee5\u53ca\u76f8\u5e94\u7684\u89e3\u7801\u7b97\u6cd5\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u77ed\u4e0a\u4e0b\u6587\u4e3b\u5bfc\u5047\u8bf4\uff1a\u9a8c\u8bc1\u5bf9\u4e8e\u5927\u591a\u6570\u5e8f\u5217\uff0c\u4ec5\u9700\u5c11\u91cf\u5c40\u90e8\u524d\u7f00\u662f\u5426\u8db3\u4ee5\u9884\u6d4b\u4e0b\u4e00\u4e2atoken\u3002\u540c\u65f6\u63a2\u7d22\u5982\u4f55\u68c0\u6d4b\u90a3\u4e9b\u9700\u8981\u957f\u4e0a\u4e0b\u6587\u7684\u6311\u6218\u6027\u5e8f\u5217\uff0c\u5e76\u89e3\u51b3\u77ed\u4e0a\u4e0b\u6587\u4e3b\u5bfc\u5bfc\u81f4\u7684LLM\u8f93\u51fa\u5206\u5e03\u504f\u5dee\u95ee\u9898\u3002", "method": "1. \u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u7edf\u8ba1\u9884\u8a00\u673a\uff0c\u6d4b\u91cf\u5728\u4e0d\u540c\u957f\u5ea6\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u91cd\u73b0\u51c6\u786e\u5168\u4e0a\u4e0b\u6587\u9884\u6d4b\u6240\u9700\u7684\u6700\u5c0f\u4e0a\u4e0b\u6587\u957f\u5ea6(MCL)\u30022. \u63d0\u51fa\u65e0\u9700\u5b9e\u9645\u4e0b\u4e00\u4e2atoken\u77e5\u8bc6\u7684\u5206\u5e03\u611f\u77e5MCL(DaMCL)\u4f5c\u4e3aMCL\u7684\u5b9e\u7528\u4ee3\u7406\u30023. \u5f00\u53d1\u5229\u7528\u68c0\u6d4b\u5668\u8bc6\u522b\u548c\u63d0\u5347\u957f\u8303\u56f4\u76f8\u5173token\u7684\u89e3\u7801\u7b97\u6cd5\u3002", "result": "\u5bf9\u4e8e1-7k token\u7684\u957f\u4e0a\u4e0b\u6587\u6587\u6863\u5e8f\u5217\uff0c75-80%\u4ec5\u9700\u6700\u591a\u6700\u540e96\u4e2atoken\u3002DaMCL\u9608\u503c\u5316\u5728\u68c0\u6d4b\u957f\u77ed\u4e0a\u4e0b\u6587\u5e8f\u5217\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u63d0\u51fa\u7684\u89e3\u7801\u7b97\u6cd5\u5728\u95ee\u7b54\u4efb\u52a1\u548c\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u4e0a\u90fd\u80fd\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u77ed\u4e0a\u4e0b\u6587\u4e3b\u5bfc\u73b0\u8c61\u786e\u5b9e\u5b58\u5728\uff0c\u5927\u591a\u6570\u5e8f\u5217\u9884\u6d4b\u4ec5\u9700\u5c11\u91cf\u5c40\u90e8\u4e0a\u4e0b\u6587\u3002\u901a\u8fc7DaMCL\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u9700\u8981\u957f\u4e0a\u4e0b\u6587\u7684\u5e8f\u5217\uff0c\u800c\u76f8\u5e94\u7684\u89e3\u7801\u7b97\u6cd5\u80fd\u591f\u7f13\u89e3\u77ed\u4e0a\u4e0b\u6587\u4e3b\u5bfc\u5e26\u6765\u7684\u504f\u5dee\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2512.07921", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07921", "abs": "https://arxiv.org/abs/2512.07921", "authors": ["Zongwei Li", "Zhonghang Li", "Zirui Guo", "Xubin Ren", "Chao Huang"], "title": "DeepCode: Open Agentic Coding", "comment": "for source code, please see https://github.com/HKUDS/DeepCode", "summary": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.", "AI": {"tldr": "DeepCode\u662f\u4e00\u4e2a\u5b8c\u5168\u81ea\u4e3b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u6d41\u7ba1\u7406\u89e3\u51b3\u6587\u6863\u5230\u4ee3\u7801\u5e93\u5408\u6210\u4e2d\u7684\u4fe1\u606f\u8fc7\u8f7d\u4e0e\u4e0a\u4e0b\u6587\u74f6\u9888\u51b2\u7a81\uff0c\u5728PaperBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5546\u4e1a\u4ee3\u7406\u548c\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6587\u6863\u5230\u4ee3\u7801\u5e93\u5408\u6210\uff08\u5982\u79d1\u5b66\u8bba\u6587\u5230\u4ee3\u7801\uff09\u65f6\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u4fe1\u606f\u8fc7\u8f7d\u4e0eLLMs\u4e0a\u4e0b\u6587\u74f6\u9888\u4e4b\u95f4\u7684\u6839\u672c\u51b2\u7a81\u3002", "method": "\u5c06\u4ed3\u5e93\u5408\u6210\u89c6\u4e3a\u4fe1\u9053\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u56db\u4e2a\u4fe1\u606f\u64cd\u4f5c\u6700\u5927\u5316\u6709\u9650\u4e0a\u4e0b\u6587\u9884\u7b97\u4e0b\u7684\u4efb\u52a1\u76f8\u5173\u4fe1\u53f7\uff1a\u84dd\u56fe\u84b8\u998f\u7684\u6e90\u538b\u7f29\u3001\u72b6\u6001\u5316\u4ee3\u7801\u5185\u5b58\u7684\u7ed3\u6784\u5316\u7d22\u5f15\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u6761\u4ef6\u77e5\u8bc6\u6ce8\u5165\u3001\u95ed\u73af\u9519\u8bef\u6821\u6b63\u3002", "result": "\u5728PaperBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8eCursor\u548cClaude Code\u7b49\u9886\u5148\u5546\u4e1a\u4ee3\u7406\uff0c\u5e76\u5728\u5173\u952e\u590d\u73b0\u6307\u6807\u4e0a\u8d85\u8d8a\u9876\u5c16\u673a\u6784\u7684\u535a\u58eb\u7ea7\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5730\u5c06\u8bba\u6587\u89c4\u8303\u8f6c\u5316\u4e3a\u751f\u4ea7\u7ea7\u5b9e\u73b0\uff0c\u4e3a\u81ea\u4e3b\u79d1\u5b66\u590d\u73b0\u5efa\u7acb\u65b0\u57fa\u7840\uff0c\u52a0\u901f\u7814\u7a76\u8bc4\u4f30\u548c\u53d1\u73b0\u3002", "topic": "code agent"}}
{"id": "2512.08230", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08230", "abs": "https://arxiv.org/abs/2512.08230", "authors": ["Eunice Yiu", "Kelsey Allen", "Shiry Ginosar", "Alison Gopnik"], "title": "Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions", "comment": "Accepted to Philosophical Transactions A, Special issue: World models, AGI, and the hard problems of life-mind continuity. Expected publication in 2026", "summary": "Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called \"empowerment\" which maximizes mutual information between actions and their outcomes. \"Empowerment\" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u56e0\u679c\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\"\u8d4b\u6743\"\u6982\u5ff5\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u53d1\u5c55\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u8d4b\u6743\u53ef\u4ee5\u4f5c\u4e3a\u8fde\u63a5\u8d1d\u53f6\u65af\u56e0\u679c\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u6865\u6881\uff0c\u5e76\u901a\u8fc7\u513f\u7ae5\u4e0e\u6210\u4eba\u7684\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u7406\u8bba\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u56e0\u679c\u5b66\u4e60\u548c\u56e0\u679c\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u800c\u8ba4\u77e5\u79d1\u5b66\u9886\u57df\u5df2\u6210\u529f\u5e94\u7528\u56e0\u679c\u8d1d\u53f6\u65af\u7f51\u7edc\u7b49\u8ba1\u7b97\u673a\u79d1\u5b66\u7406\u8bba\u6765\u7406\u89e3\u4eba\u7c7b\u56e0\u679c\u5b66\u4e60\u3002\u7814\u7a76\u8005\u5e0c\u671b\u627e\u5230\u8fde\u63a5\u7ecf\u5178\u8d1d\u53f6\u65af\u56e0\u679c\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e76\u89e3\u91ca\u513f\u7ae5\u56e0\u679c\u5b66\u4e60\u7684\u7279\u5f81\u3002", "method": "\u63d0\u51fa\"\u8d4b\u6743\"\uff08empowerment\uff09\u6982\u5ff5\u4f5c\u4e3a\u8fde\u63a5\u6865\u6881\uff0c\u8ba4\u4e3a\u51c6\u786e\u56e0\u679c\u4e16\u754c\u6a21\u578b\u7684\u5b66\u4e60\u4f1a\u63d0\u9ad8\u8d4b\u6743\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u7cfb\u7edf\u6d4b\u8bd5\u513f\u7ae5\u548c\u6210\u4eba\u5982\u4f55\u4f7f\u7528\u8d4b\u6743\u7ebf\u7d22\u6765\u63a8\u65ad\u56e0\u679c\u5173\u7cfb\u5e76\u8bbe\u8ba1\u6709\u6548\u7684\u56e0\u679c\u5e72\u9884\u3002", "result": "\u8d4b\u6743\u6982\u5ff5\u4e3a\u7406\u89e3\u4eba\u7c7b\u56e0\u679c\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u53ef\u80fd\u89e3\u91ca\u513f\u7ae5\u56e0\u679c\u5b66\u4e60\u7684\u72ec\u7279\u7279\u5f81\uff0c\u5e76\u4e3a\u673a\u5668\u5b9e\u73b0\u56e0\u679c\u5b66\u4e60\u63d0\u4f9b\u66f4\u6613\u5904\u7406\u7684\u7b97\u6cd5\u8def\u5f84\u3002\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u8d4b\u6743\u7ebf\u7d22\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u8d4b\u6743\u662f\u8fde\u63a5\u8d1d\u53f6\u65af\u56e0\u679c\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u91cd\u8981\u6865\u6881\uff0c\u4e0d\u4ec5\u6709\u52a9\u4e8e\u7406\u89e3\u4eba\u7c7b\uff08\u7279\u522b\u662f\u513f\u7ae5\uff09\u7684\u56e0\u679c\u5b66\u4e60\u673a\u5236\uff0c\u4e5f\u4e3a\u673a\u5668\u5b9e\u73b0\u56e0\u679c\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u8ba1\u7b97\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2512.07850", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07850", "abs": "https://arxiv.org/abs/2512.07850", "authors": ["Alejandro Cuadron", "Pengfei Yu", "Yang Liu", "Arpit Gupta"], "title": "SABER: Small Actions, Big Errors -- Safeguarding Mutating Steps in LLM Agents", "comment": "submitted to ICLR2026", "summary": "Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \\emph{do all actions contribute equally to failure?} Analyzing execution traces on $\u03c4$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \\emph{mutating} (environment-changing) vs.\\ non-mutating steps and formalize \\emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\\%$ on Airline and upto $96\\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \\cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \\emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \\cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\\% \\emph{relative} on Airline, +11\\% on Retail, and +7\\% on SWE-Bench Verified; Claude: +9\\%/+7\\%. We further identify ceiling effects in $\u03c4$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $\u03c4$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86LLM\u667a\u80fd\u4f53\u5728\u957f\u89c6\u91ce\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u53d1\u73b0\u7a81\u53d8\u6027\u52a8\u4f5c\uff08\u6539\u53d8\u73af\u5883\u7684\u64cd\u4f5c\uff09\u4e2d\u7684\u9519\u8bef\u5bf9\u4efb\u52a1\u5931\u8d25\u5f71\u54cd\u6700\u5927\uff0c\u800c\u975e\u7a81\u53d8\u6027\u52a8\u4f5c\u9519\u8bef\u5f71\u54cd\u8f83\u5c0f\u3002\u4f5c\u8005\u63d0\u51fa\u4e86CM\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a81\u53d8\u95e8\u63a7\u9a8c\u8bc1\u3001\u9488\u5bf9\u6027\u53cd\u601d\u548c\u5757\u7ea7\u4e0a\u4e0b\u6587\u6e05\u7406\u6765\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e86\u03c4-Bench Verified\u57fa\u51c6\u4ee5\u89e3\u51b3\u539f\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u667a\u80fd\u4f53\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5728\u957f\u89c6\u91ce\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4ecd\u7136\u8106\u5f31\u3002\u4e3a\u4e86\u7406\u89e3\u8fd9\u79cd\u8106\u5f31\u6027\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7a76\uff1a\u6240\u6709\u52a8\u4f5c\u5bf9\u5931\u8d25\u7684\u5f71\u54cd\u662f\u5426\u76f8\u540c\uff1f\u7279\u522b\u662f\u7a81\u53d8\u6027\u52a8\u4f5c\uff08\u6539\u53d8\u73af\u5883\u72b6\u6001\u7684\u64cd\u4f5c\uff09\u4e0e\u975e\u7a81\u53d8\u6027\u52a8\u4f5c\u5728\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u65b9\u9762\u7684\u5dee\u5f02\u3002", "method": "1) \u5728\u03c4-Bench\uff08\u822a\u7a7a/\u96f6\u552e\uff09\u548cSWE-Bench Verified\u4e0a\u5206\u6790\u6267\u884c\u8f68\u8ff9\uff0c\u5c06\u52a8\u4f5c\u5206\u89e3\u4e3a\u7a81\u53d8\u6027\u548c\u975e\u7a81\u53d8\u6027\u6b65\u9aa4\uff1b2) \u5f62\u5f0f\u5316\"\u51b3\u5b9a\u6027\u504f\u5dee\"\u6982\u5ff5\uff0c\u5373\u6700\u65e9\u5bfc\u81f4\u6210\u529f\u8f6c\u4e3a\u5931\u8d25\u7684\u52a8\u4f5c\u7ea7\u5206\u6b67\uff1b3) \u4f7f\u7528\u903b\u8f91\u56de\u5f52\u5206\u6790\u4e0d\u540c\u52a8\u4f5c\u7c7b\u578b\u9519\u8bef\u5bf9\u6210\u529f\u7387\u7684\u5f71\u54cd\uff1b4) \u63d0\u51faCM\u65b9\u6cd5\uff1a\u7a81\u53d8\u95e8\u63a7\u9a8c\u8bc1\u3001\u9488\u5bf9\u6027\u53cd\u601d\uff08\u5728\u7a81\u53d8\u6b65\u9aa4\u524d\uff09\u3001\u5757\u7ea7\u4e0a\u4e0b\u6587\u6e05\u7406\uff1b5) \u53d1\u5e03\u03c4-Bench Verified\u57fa\u51c6\uff0c\u4fee\u6b63\u539f\u6709\u57fa\u51c6\u7684\u6807\u6ce8\u9519\u8bef\u548c\u4efb\u52a1\u5b9a\u4e49\u4e0d\u660e\u786e\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u6bcf\u4e2a\u7a81\u53d8\u6027\u52a8\u4f5c\u4e2d\u7684\u504f\u5dee\u53ef\u5c06\u6210\u529f\u7387\u964d\u4f4e\u9ad8\u8fbe92%\uff08\u822a\u7a7a\uff09\u548c96%\uff08\u96f6\u552e\uff09\uff0c\u800c\u975e\u7a81\u53d8\u6027\u52a8\u4f5c\u504f\u5dee\u5f71\u54cd\u5f88\u5c0f\uff1b2) \u9519\u8bef\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u800c\u589e\u957f\uff0c\u667a\u80fd\u4f53\u504f\u79bb\u89d2\u8272\u5e76\u57fa\u4e8e\u8fc7\u65f6\u7ea6\u675f\u884c\u52a8\uff1b3) CM\u65b9\u6cd5\u5e26\u6765\u663e\u8457\u63d0\u5347\uff1aQwen3-Thinking\u5728\u822a\u7a7a\u4efb\u52a1\u4e0a\u76f8\u5bf9\u63d0\u534728%\uff0c\u96f6\u552e\u4efb\u52a111%\uff0cSWE-Bench Verified 7%\uff1bClaude\u6a21\u578b\u4e5f\u67099%/7%\u7684\u63d0\u5347\uff1b4) \u53d1\u73b0\u03c4-Bench\u5b58\u5728\u5929\u82b1\u677f\u6548\u5e94\uff0c\u6807\u6ce8\u9519\u8bef\u548c\u4efb\u52a1\u5b9a\u4e49\u4e0d\u660e\u786e\u4eba\u4e3a\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u4e3b\u5f20\uff1a1) \u9700\u8981\u8fdb\u884c\u52a8\u4f5c\u7ea7\u5206\u6790\u6765\u7406\u89e3\u667a\u80fd\u4f53\u5931\u8d25\uff1b2) \u9700\u8981\u9488\u5bf9\u6027\u5b89\u5168\u63aa\u65bd\uff08\u5982CM\uff09\u6765\u4fdd\u62a4\u7a81\u53d8\u6027\u52a8\u4f5c\uff1b3) \u9700\u8981\u53ef\u9760\u7684\u8bc4\u4f30\u57fa\u51c6\uff08\u5982\u03c4-Bench Verified\uff09\u6765\u51c6\u786e\u8861\u91cf\u667a\u80fd\u4f53\u80fd\u529b\u3002\u8fd9\u4e9b\u662f\u6784\u5efa\u7a33\u5065\u591a\u8f6e\u667a\u80fd\u4f53\u7684\u5148\u51b3\u6761\u4ef6\u3002", "topic": "agent analysis"}}
{"id": "2512.07853", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07853", "abs": "https://arxiv.org/abs/2512.07853", "authors": ["Jinwoo Jeong", "Minchul Kang", "Younghun Go", "Changyong Shin", "Hyunho Lee", "Junho Yoon", "Gyeongsik Yang", "Chuck Yoo"], "title": "GPU Memory Prediction for Multimodal Model Training", "comment": "1st Workshop on Systems for Agentic AI (SAA '25), co-located with SOSP 2025", "summary": "As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9884\u6d4b\u591a\u6a21\u6001\u6a21\u578bGPU\u5185\u5b58\u4f7f\u7528\u91cf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u6a21\u578b\u67b6\u6784\u548c\u5206\u6790\u8bad\u7ec3\u884c\u4e3a\u6765\u51c6\u786e\u9884\u6d4b\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\uff0c\u9632\u6b62\u5185\u5b58\u6ea2\u51fa\u9519\u8bef\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u589e\u52a0\uff0cGPU\u5185\u5b58\u9700\u6c42\u7ecf\u5e38\u8d85\u8fc7\u53ef\u7528\u5bb9\u91cf\uff0c\u5bfc\u81f4\u5185\u5b58\u6ea2\u51fa\u9519\u8bef\uff0c\u8fd9\u4f1a\u4e2d\u65ad\u8bad\u7ec3\u5e76\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\u3002\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u5355\u6a21\u6001\u67b6\u6784\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u591a\u6a21\u6001\u6a21\u578b\uff0c\u800c\u591a\u6a21\u6001\u6a21\u578b\u5728\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\u5f88\u5e38\u89c1\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u591a\u6a21\u6001\u6a21\u578b\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u884c\u4e3a\u6765\u9884\u6d4b\u5cf0\u503cGPU\u5185\u5b58\u4f7f\u7528\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u5c06\u591a\u6a21\u6001\u6a21\u578b\u5206\u89e3\u4e3a\u7ec4\u6210\u5c42\uff0c\u5e76\u5e94\u7528\u56e0\u5b50\u5316\u6765\u4f30\u8ba1\u6bcf\u5c42\u7684\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u7ea68.7%\u7684\u5e73\u5747MAPE\uff08\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff09\u9884\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u591a\u6a21\u6001\u6a21\u578b\u7684GPU\u5185\u5b58\u4f7f\u7528\uff0c\u6709\u52a9\u4e8e\u9632\u6b62\u5185\u5b58\u6ea2\u51fa\u9519\u8bef\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2512.08296", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08296", "abs": "https://arxiv.org/abs/2512.08296", "authors": ["Yubin Kim", "Ken Gu", "Chanwoo Park", "Chunjong Park", "Samuel Schmidgall", "A. Ali Heydari", "Yao Yan", "Zhihan Zhang", "Yuchen Zhuang", "Mark Malhotra", "Paul Pu Liang", "Hae Won Park", "Yuzhe Yang", "Xuhai Xu", "Yilun Du", "Shwetak Patel", "Tim Althoff", "Daniel McDuff", "Xin Liu"], "title": "Towards a Science of Scaling Agent Systems", "comment": null, "summary": "Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u7814\u7a76\u63d0\u51fa\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u91cf\u5316\u6269\u5c55\u539f\u5219\uff0c\u53d1\u73b0\u4e86\u5de5\u5177\u534f\u8c03\u6743\u8861\u3001\u80fd\u529b\u9971\u548c\u548c\u62d3\u6251\u4f9d\u8d56\u9519\u8bef\u653e\u5927\u4e09\u4e2a\u4e3b\u5bfc\u6548\u5e94\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u80fd\u9884\u6d4b\u6700\u4f18\u534f\u8c03\u7b56\u7565\u7684\u6846\u67b6\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u73b0\u5b9eAI\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u6027\u80fd\u51b3\u5b9a\u539f\u5219\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7814\u7a76\uff0c\u5bfc\u81f4\u5b9e\u8df5\u8005\u4f9d\u8d56\u542f\u53d1\u5f0f\u800c\u975e\u539f\u5219\u6027\u8bbe\u8ba1\u9009\u62e9\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5728\u56db\u4e2a\u591a\u6837\u5316\u57fa\u51c6\uff08Finance-Agent\u3001BrowseComp-Plus\u3001PlanCraft\u3001Workbench\uff09\u4e0a\u8bc4\u4f30\u4e94\u79cd\u5178\u578b\u67b6\u6784\uff08\u5355\u4e00\u3001\u72ec\u7acb\u3001\u96c6\u4e2d\u5f0f\u3001\u5206\u6563\u5f0f\u3001\u6df7\u5408\u5f0f\uff09\uff0c\u4f7f\u7528\u4e09\u4e2aLLM\u5bb6\u65cf\u5b9e\u4f8b\u5316\uff0c\u8fdb\u884c180\u4e2a\u914d\u7f6e\u7684\u53d7\u63a7\u8bc4\u4f30\u3002\u4f7f\u7528\u7ecf\u9a8c\u534f\u8c03\u6307\u6807\uff08\u6548\u7387\u3001\u5f00\u9500\u3001\u9519\u8bef\u653e\u5927\u3001\u5197\u4f59\uff09\u5efa\u7acb\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u9884\u6d4b\u6a21\u578b\u8fbe\u5230\u4ea4\u53c9\u9a8c\u8bc1R^2=0.513\u3002\u53d1\u73b0\u4e09\u4e2a\u4e3b\u5bfc\u6548\u5e94\uff1a1\uff09\u5de5\u5177\u534f\u8c03\u6743\u8861\uff1b2\uff09\u80fd\u529b\u9971\u548c\uff08\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u8d85\u8fc7\u7ea645%\u540e\u534f\u8c03\u6536\u76ca\u9012\u51cf\uff09\uff1b3\uff09\u62d3\u6251\u4f9d\u8d56\u9519\u8bef\u653e\u5927\uff08\u72ec\u7acb\u667a\u80fd\u4f53\u9519\u8bef\u653e\u592717.2\u500d\uff0c\u96c6\u4e2d\u5f0f\u63a7\u5236\u57284.4\u500d\uff09\u3002\u96c6\u4e2d\u5f0f\u534f\u8c03\u5728\u91d1\u878d\u63a8\u7406\u7b49\u5e76\u884c\u4efb\u52a1\u4e0a\u63d0\u534780.9%\uff0c\u5206\u6563\u5f0f\u534f\u8c03\u5728\u52a8\u6001\u7f51\u9875\u5bfc\u822a\u4e0a\u8868\u73b0\u66f4\u4f73\uff08+9.2% vs +0.2%\uff09\uff0c\u4f46\u6240\u6709\u591a\u667a\u80fd\u4f53\u53d8\u4f53\u5728\u987a\u5e8f\u63a8\u7406\u4efb\u52a1\u4e0a\u6027\u80fd\u4e0b\u964d39-70%\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u9884\u6d4b87%\u4fdd\u7559\u914d\u7f6e\u7684\u6700\u4f18\u534f\u8c03\u7b56\u7565\uff0c\u4e3a\u57fa\u4e8e\u53ef\u6d4b\u91cf\u4efb\u52a1\u5c5e\u6027\u7684\u667a\u80fd\u4f53\u6269\u5c55\u63d0\u4f9b\u4e86\u9884\u6d4b\u6027\u539f\u5219\uff0c\u4f7f\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u4ece\u542f\u53d1\u5f0f\u8f6c\u5411\u539f\u5219\u6027\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2512.08266", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.08266", "abs": "https://arxiv.org/abs/2512.08266", "authors": ["Zhensu Sun", "Chengran Yang", "Xiaoning Du", "Zhou Yang", "Li Li", "David Lo"], "title": "Token Sugar: Making Source Code Sweeter for LLMs through Token-Efficient Shorthand", "comment": "Accepted by ASE'25", "summary": "Large language models (LLMs) have shown exceptional performance in code generation and understanding tasks, yet their high computational costs hinder broader adoption. One important factor is the inherent verbosity of programming languages, such as unnecessary formatting elements and lengthy boilerplate code. This leads to inflated token counts in both input and generated outputs, which increases inference costs and slows down the generation process. Prior work improves this through simplifying programming language grammar, reducing token usage across both code understanding and generation tasks. However, it is confined to syntactic transformations, leaving significant opportunities for token reduction unrealized at the semantic level.\n  In this work, we propose Token Sugar, a concept that replaces frequent and verbose code patterns with reversible, token-efficient shorthand in the source code. To realize this concept in practice, we designed a systematic solution that mines high-frequency, token-heavy patterns from a code corpus, maps each to a unique shorthand, and integrates them into LLM pretraining via code transformation. With this solution, we obtain 799 (code pattern, shorthand) pairs, which can reduce up to 15.1% token count in the source code and is complementary to existing syntax-focused methods. We further trained three widely used LLMs on Token Sugar-augmented data. Experimental results show that these models not only achieve significant token savings (up to 11.2% reduction) during generation but also maintain near-identical Pass@1 scores compared to baselines trained on unprocessed code.", "AI": {"tldr": "Token Sugar\uff1a\u901a\u8fc7\u5c06\u9ad8\u9891\u5197\u957f\u4ee3\u7801\u6a21\u5f0f\u66ff\u6362\u4e3a\u53ef\u9006\u7684\u7b80\u6d01\u7b26\u53f7\uff0c\u51cf\u5c11LLM\u4ee3\u7801\u751f\u6210\u4e2d\u7684token\u6570\u91cf\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "LLM\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f16\u7a0b\u8bed\u8a00\u7684\u5197\u957f\u6027\u5bfc\u81f4token\u6570\u91cf\u81a8\u80c0\uff0c\u589e\u52a0\u63a8\u7406\u6210\u672c\u548c\u751f\u6210\u65f6\u95f4\u3002\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u8bed\u6cd5\u5c42\u9762\u7684\u7b80\u5316\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u8bed\u4e49\u5c42\u9762\u7684token\u51cf\u5c11\u673a\u4f1a\u3002", "method": "\u63d0\u51faToken Sugar\u6982\u5ff5\uff0c\u8bbe\u8ba1\u7cfb\u7edf\u5316\u65b9\u6848\uff1a\u4ece\u4ee3\u7801\u8bed\u6599\u5e93\u6316\u6398\u9ad8\u9891\u3001token\u5bc6\u96c6\u7684\u6a21\u5f0f\uff0c\u4e3a\u6bcf\u4e2a\u6a21\u5f0f\u6620\u5c04\u552f\u4e00\u7b80\u6d01\u7b26\u53f7\uff0c\u901a\u8fc7\u4ee3\u7801\u8f6c\u6362\u5c06\u7b26\u53f7\u5bf9\u96c6\u6210\u5230LLM\u9884\u8bad\u7ec3\u4e2d\u3002\u6700\u7ec8\u83b7\u5f97799\u4e2a\uff08\u4ee3\u7801\u6a21\u5f0f\uff0c\u7b80\u6d01\u7b26\u53f7\uff09\u5bf9\u3002", "result": "\u6e90\u4ee3\u7801token\u6570\u91cf\u6700\u591a\u51cf\u5c1115.1%\uff0c\u4e0e\u73b0\u6709\u8bed\u6cd5\u65b9\u6cd5\u4e92\u8865\u3002\u8bad\u7ec3\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684LLM\u540e\uff0c\u751f\u6210\u65f6token\u8282\u7701\u663e\u8457\uff08\u6700\u591a\u51cf\u5c1111.2%\uff09\uff0c\u540c\u65f6Pass@1\u5206\u6570\u4e0e\u672a\u5904\u7406\u4ee3\u7801\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\u51e0\u4e4e\u76f8\u540c\u3002", "conclusion": "Token Sugar\u80fd\u6709\u6548\u51cf\u5c11LLM\u4ee3\u7801\u751f\u6210\u4e2d\u7684token\u4f7f\u7528\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u4e3aLLM\u4ee3\u7801\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684token\u4f18\u5316\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2512.08300", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08300", "abs": "https://arxiv.org/abs/2512.08300", "authors": ["Sijia Chen", "Baochun Li", "Di Niu"], "title": "rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection", "comment": "14 pages, 6 figures. Accepted to the ACL ARR July", "summary": "Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.", "AI": {"tldr": "\u63d0\u51farSIM\u673a\u5236\uff0c\u901a\u8fc7\u5c0f\u578b\u89c4\u5212\u5668\u5f15\u5bfcLLM\u7684\u601d\u7ef4\u94fe\uff0c\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u89c4\u5212\u5668\u4e0eLLM\uff0c\u4f7f\u5c0f\u6a21\u578b\u6027\u80fd\u8d85\u8d8a\u5927\u6a21\u578b\u3002", "motivation": "LLM\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u8fdb\u5316\u4e3a\u63a8\u7406\u8bed\u8a00\u6a21\u578b\uff08RLM\uff09\uff0c\u5176\u6807\u5fd7\u6027\u7279\u5f81\u662f\u601d\u7ef4\u94fe\u4e2d\u51fa\u73b0\"\u987f\u609f\"\u65f6\u523b\uff0c\u80fd\u591f\u6267\u884c\u81ea\u6211\u53cd\u601d\u548c\u6df1\u5ea6\u601d\u8003\u7b49\u7b56\u7565\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u673a\u5236\u8ba9\u4efb\u4f55LLM\u90fd\u80fd\u6210\u4e3aRLM\u3002", "method": "\u63d0\u51fa\u5f3a\u5316\u7b56\u7565\u6ce8\u5165\u673a\u5236\uff08rSIM\uff09\uff0c\u4f7f\u7528\u5c0f\u578b\u89c4\u5212\u5668\u5f15\u5bfcLLM\u7684\u601d\u7ef4\u94fe\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6ce8\u5165\u63a8\u7406\u7b56\u7565\u3002\u91c7\u7528\u9886\u5bfc\u8005-\u8ffd\u968f\u8005\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u8bad\u7ec3\u89c4\u5212\u5668\uff08\u9886\u5bfc\u8005\u667a\u80fd\u4f53\uff09\u548cLLM\uff08\u8ffd\u968f\u8005\u667a\u80fd\u4f53\uff09\uff0c\u57fa\u4e8e\u7b80\u5355\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u3002", "result": "rSIM\u4f7fQwen2.5-0.5B\u6210\u4e3aRLM\uff0c\u5e76\u663e\u8457\u8d85\u8d8aQwen2.5-14B\u3002\u89c4\u5212\u5668\u5177\u6709\u6cdb\u5316\u6027\uff1a\u53ea\u9700\u8bad\u7ec3\u4e00\u6b21\uff0c\u5373\u53ef\u4f5c\u4e3a\u63d2\u4ef6\u5927\u5e45\u63d0\u5347\u73b0\u6709LLM\u7684\u63a8\u7406\u80fd\u529b\u3002\u89c4\u5212\u5668\u652f\u6301\u8de8\u4efb\u52a1\u6301\u7eed\u5b66\u4e60\uff0c\u5176\u89c4\u5212\u80fd\u529b\u80fd\u9010\u6b65\u6539\u8fdb\u5e76\u6cdb\u5316\u5230\u66f4\u5e7f\u6cdb\u7684\u95ee\u9898\u3002", "conclusion": "rSIM\u662f\u4e00\u79cd\u6709\u6548\u7684\u673a\u5236\uff0c\u80fd\u591f\u901a\u8fc7\u5c0f\u578b\u89c4\u5212\u5668\u7684\u5f15\u5bfc\u4f7fLLM\u8fdb\u5316\u4e3aRLM\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u5177\u6709\u6cdb\u5316\u6027\u548c\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.08545", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.08545", "abs": "https://arxiv.org/abs/2512.08545", "authors": ["Indrajit Kar", "Kalathur Chenchu Kishore Kumar"], "title": "Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks", "comment": "22 pages, 2 tables, 9 figures", "summary": "Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5c06\u63a8\u7406\u5206\u5e03\u572864*64\u7f51\u683c\u7684\u8f7b\u91cf\u7ea7\u667a\u80fd\u4f53\u4e0a\uff0c\u901a\u8fc7\u7a7a\u95f4\u8bfe\u7a0b\u5b66\u4e60\u9010\u6b65\u6269\u5c55\u64cd\u4f5c\u533a\u57df\uff0c\u7ed3\u5408NLL\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\u548cThompson\u91c7\u6837\u8bfe\u7a0b\u7ba1\u7406\u5668\uff0c\u5728\u7a7a\u95f4\u5316\u6c49\u8bfa\u5854\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u957f\u7a0b\u63a8\u7406\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5206\u89e3\u590d\u6742\u4efb\u52a1\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u957f\u7a0b\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5904\u7406\u957f\u89c6\u91ce\u63a8\u7406\u4efb\u52a1\u540c\u65f6\u63a7\u5236\u8ba1\u7b97\u5f00\u9500\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff1a1) 64*64\u7f51\u683c\u7684\u8f7b\u91cf\u7ea7\u667a\u80fd\u4f53\u5206\u5e03\u63a8\u7406\uff1b2) \u9009\u62e9\u6027oracle\u652f\u6301\uff1b3) \u7a7a\u95f4\u8bfe\u7a0b\u5b66\u4e60\u9010\u6b65\u6269\u5c55\u64cd\u4f5c\u533a\u57df\uff1b4) \u4f7f\u7528\u8d1f\u5bf9\u6570\u4f3c\u7136(NLL)\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\uff1b5) Thompson\u91c7\u6837\u8bfe\u7a0b\u7ba1\u7406\u5668\u81ea\u9002\u5e94\u9009\u62e9\u8bad\u7ec3\u533a\u57df\u3002", "result": "\u5728\u7a7a\u95f4\u5316\u6c49\u8bfa\u5854\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\uff1a1) \u6539\u8fdb\u7684\u7a33\u5b9a\u6027\uff1b2) \u51cf\u5c11\u7684oracle\u4f7f\u7528\uff1b3) \u66f4\u5f3a\u7684\u5206\u5e03\u5f0f\u667a\u80fd\u4f53\u534f\u4f5c\u957f\u7a0b\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\u7ed3\u5408\u7a7a\u95f4\u8bfe\u7a0b\u5b66\u4e60\u548c\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u957f\u89c6\u91ce\u63a8\u7406\u4efb\u52a1\uff0c\u63d0\u9ad8\u7a33\u5b9a\u6027\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u89c4\u5212\u7b49\u7c7b\u4f3c\u7ed3\u6784\u4efb\u52a1\u3002", "topic": "agent analysis"}}
{"id": "2512.08286", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08286", "abs": "https://arxiv.org/abs/2512.08286", "authors": ["Liao Hu", "Qiteng Wu", "Ruoyu Qi"], "title": "Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework", "comment": null, "summary": "The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.", "AI": {"tldr": "SolidGPT\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u8fb9\u7f18-\u4e91\u6df7\u5408\u5f00\u53d1\u52a9\u624b\uff0c\u901a\u8fc7\u8bed\u4e49\u4ee3\u7801\u641c\u7d22\u3001\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u548c\u9690\u79c1\u4f18\u5148\u8bbe\u8ba1\uff0c\u89e3\u51b3LLM\u5728\u5f00\u53d1\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u3001\u751f\u4ea7\u529b\u548c\u6570\u636e\u9690\u79c1\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u79fb\u52a8\u548c\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\u9762\u4e34\u4e09\u4e2a\u6838\u5fc3\u77db\u76fe\uff1a\u8bed\u4e49\u7406\u89e3\u3001\u5f00\u53d1\u6548\u7387\u548c\u6570\u636e\u9690\u79c1\u3002\u4f20\u7edf\u7684\u4e91\u5de5\u5177\u867d\u7136\u63a8\u7406\u80fd\u529b\u5f3a\u4f46\u5b58\u5728\u6570\u636e\u6cc4\u9732\u548c\u5ef6\u8fdf\u98ce\u9669\uff0c\u800c\u672c\u5730\u89e3\u51b3\u65b9\u6848\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u5e93\u548c\u5f00\u53d1\u5de5\u5177\u7684\u5168\u4e0a\u4e0b\u6587\u7406\u89e3\u3002", "method": "SolidGPT\u91c7\u7528\u8fb9\u7f18-\u4e91\u6df7\u5408\u67b6\u6784\uff0c\u57fa\u4e8eGitHub\u6784\u5efa\uff0c\u63d0\u4f9b\uff1a1\uff09\u4ea4\u4e92\u5f0f\u4ee3\u7801\u5e93\u67e5\u8be2\u529f\u80fd\uff1b2\uff09\u81ea\u52a8\u5316\u8f6f\u4ef6\u9879\u76ee\u5de5\u4f5c\u6d41\uff08\u751f\u6210PRD\u3001\u4efb\u52a1\u5206\u89e3\u3001\u770b\u677f\u7b49\uff09\uff1b3\uff09\u53ef\u914d\u7f6e\u7684\u79c1\u6709\u53ef\u6269\u5c55\u4ee3\u7406\uff0c\u652f\u6301\u672c\u5730\u4ee3\u7801\u6587\u4ef6\u5939\u3001Notion\u96c6\u6210\u548cAI\u89d2\u8272\u5b9a\u5236\u3002", "result": "SolidGPT\u901a\u8fc7\u8bed\u4e49\u4e30\u5bcc\u7684\u4ee3\u7801\u5bfc\u822a\u3001\u96c6\u6210\u7684\u6587\u6863\u548c\u4efb\u52a1\u7ba1\u7406\u3001\u9690\u79c1\u4f18\u5148\u8bbe\u8ba1\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u3002\u5f00\u53d1\u8005\u53ef\u4ee5\u901a\u8fc7Docker\u3001CLI\u6216VSCode\u6269\u5c55\u90e8\u7f72\uff0c\u5728\u4fdd\u6301\u6570\u636e\u63a7\u5236\u7684\u540c\u65f6\uff0c\u6309\u9700\u4f7f\u7528LLM API\u3002", "conclusion": "SolidGPT\u901a\u8fc7\u7ed3\u5408\u4ea4\u4e92\u5f0f\u4ee3\u7801\u67e5\u8be2\u3001\u81ea\u52a8\u5316\u9879\u76ee\u811a\u624b\u67b6\u548c\u4eba\u673a\u534f\u4f5c\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u3001\u5c0a\u91cd\u9690\u79c1\u7684\u8fb9\u7f18\u52a9\u624b\uff0c\u52a0\u901f\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u5f00\u53d1\u5de5\u4f5c\u6d41\uff0c\u7279\u522b\u9002\u5408\u667a\u80fd\u79fb\u52a8\u548c\u8f6f\u4ef6\u5de5\u7a0b\u573a\u666f\u3002", "topic": "swe application"}}
{"id": "2512.08345", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.08345", "abs": "https://arxiv.org/abs/2512.08345", "authors": ["Benedikt Mangold"], "title": "The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations", "comment": "8 figures, 3 tables", "summary": "Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled \"sociological sandbox\". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with \"toxic\" system prompts. Our results demonstrate a statistically significant increase of approximately 25\\% in the duration of conversations involving toxic participants. We propose that this \"latency of toxicity\" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.", "AI": {"tldr": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6a21\u62df1\u5bf91\u5bf9\u6297\u6027\u8fa9\u8bba\uff0c\u91cf\u5316\u6bd2\u6027\u5bf9\u8ba8\u8bba\u6548\u7387\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6bd2\u6027\u53c2\u4e0e\u8005\u4f7f\u5bf9\u8bdd\u65f6\u95f4\u589e\u52a0\u7ea625%", "motivation": "\u5de5\u4f5c\u573a\u6240\u6bd2\u6027\u5bf9\u7ec4\u7ec7\u6587\u5316\u6709\u5bb3\uff0c\u4f46\u91cf\u5316\u5176\u5bf9\u8fd0\u8425\u6548\u7387\u7684\u76f4\u63a5\u5f71\u54cd\u5b58\u5728\u65b9\u6cd5\u8bba\u6311\u6218\uff0c\u56e0\u4e3a\u4f26\u7406\u548c\u5b9e\u8df5\u4e0a\u96be\u4ee5\u5728\u4eba\u7c7b\u53d7\u8bd5\u8005\u4e2d\u590d\u5236\u51b2\u7a81", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6a21\u62df1\u5bf91\u5bf9\u6297\u6027\u8fa9\u8bba\uff0c\u521b\u5efa\u53d7\u63a7\u7684\"\u793e\u4f1a\u5b66\u6c99\u76d2\"\u3002\u91c7\u7528\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u6a21\u62df\u6570\u767e\u6b21\u8ba8\u8bba\uff0c\u6d4b\u91cf\u57fa\u7ebf\u5bf9\u7167\u7ec4\u4e0e\u5305\u542b\"\u6bd2\u6027\"\u7cfb\u7edf\u63d0\u793a\u7684\u667a\u80fd\u4f53\u6cbb\u7597\u7ec4\u4e4b\u95f4\u7684\u6536\u655b\u65f6\u95f4\uff08\u5b9a\u4e49\u4e3a\u8fbe\u6210\u7ed3\u8bba\u6240\u9700\u7684\u8bba\u8bc1\u6570\u91cf\uff09", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6d89\u53ca\u6bd2\u6027\u53c2\u4e0e\u8005\u7684\u5bf9\u8bdd\u6301\u7eed\u65f6\u95f4\u663e\u8457\u589e\u52a0\u7ea625%\u3002\u8fd9\u79cd\"\u6bd2\u6027\u5ef6\u8fdf\"\u53ef\u4f5c\u4e3a\u4f01\u4e1a\u548c\u5b66\u672f\u73af\u5883\u4e2d\u8d22\u52a1\u635f\u5931\u7684\u4ee3\u7406\u6307\u6807", "conclusion": "\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u5efa\u6a21\u4e3a\u6d4b\u91cf\u793e\u4f1a\u6469\u64e6\u673a\u5236\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u3001\u7b26\u5408\u4f26\u7406\u7684\u4eba\u7c7b\u53d7\u8bd5\u8005\u7814\u7a76\u66ff\u4ee3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2512.08366", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08366", "abs": "https://arxiv.org/abs/2512.08366", "authors": ["Wentao Zhang", "Qunbo Wang", "Tao Zhang", "Junsheng Wu", "Hongping Gan", "Yang Liu", "Ling Dai", "Shizhuang Deng", "Shuntong Sun"], "title": "Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making", "comment": null, "summary": "Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.", "AI": {"tldr": "DuSAR\u662f\u4e00\u4e2a\u65e0\u9700\u6f14\u793a\u7684\u53cc\u7b56\u7565LLM\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u89c4\u5212\u548c\u5c40\u90e8\u7b56\u7565\u7684\u534f\u540c\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u5728ALFWorld\u548cMind2Web\u57fa\u51c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u4f9d\u8d56\u5916\u90e8\u6f14\u793a\u6216\u68c0\u7d22\u589e\u5f3a\u89c4\u5212\uff0c\u5bfc\u81f4\u8106\u5f31\u6027\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u8ba1\u7b97\u5f00\u9500\u9ad8\u3002\u53d7\u4eba\u7c7b\u95ee\u9898\u89e3\u51b3\u542f\u53d1\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u667a\u80fd\u4f53\u6846\u67b6\u3002", "method": "\u63d0\u51faDuSAR\u6846\u67b6\uff1a\u4f7f\u7528\u5355\u4e2a\u51bb\u7ed3LLM\u901a\u8fc7\u4e24\u4e2a\u4e92\u8865\u7b56\u7565\u8fdb\u884c\u534f\u540c\u81ea\u9002\u5e94\u63a8\u7406\u2014\u2014\u9ad8\u5c42\u6574\u4f53\u89c4\u5212\u548c\u4e0a\u4e0b\u6587\u63a5\u5730\u7684\u5c40\u90e8\u7b56\u7565\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53cd\u601d\u673a\u5236\u52a8\u6001\u8c03\u6574\u7b56\u7565\u3002", "result": "\u5728ALFWorld\u4e0a\u8fbe\u523037.1%\u6210\u529f\u7387\uff08Llama3.1-70B\uff09\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u7ed3\u679c\uff0813.0%\uff09\u63d0\u9ad8\u4e00\u500d\u4ee5\u4e0a\uff1b\u5728Mind2Web\u4e0a\u8fbe\u52304.02%\uff0c\u540c\u6837\u63d0\u9ad8\u4e00\u500d\u4ee5\u4e0a\uff1b\u540c\u65f6\u51cf\u5c11\u6bcf\u6b65token\u6d88\u80173-9\u500d\u3002", "conclusion": "DuSAR\u901a\u8fc7\u53cc\u7b56\u7565\u534f\u8c03\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u95ee\u9898\u89e3\u51b3\uff0c\u65e0\u9700\u5916\u90e8\u6f14\u793a\u5373\u53ef\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5c55\u73b0\u4e86\u7075\u6d3b\u6027\u548c\u4e0e\u5916\u90e8\u77e5\u8bc6\u7684\u517c\u5bb9\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.08706", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.08706", "abs": "https://arxiv.org/abs/2512.08706", "authors": ["Leon Kogler", "Maximilian Ehrhart", "Benedikt Dornauer", "Eduard Paul Enoiu"], "title": "RESTifAI: LLM-Based Workflow for Reusable REST API Testing", "comment": "Accepted for ICSE 2026 Demonstration track", "summary": "With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI.", "AI": {"tldr": "RESTifAI\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u91cd\u7528\u3001CI/CD\u5c31\u7eea\u7684REST API\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u5feb\u4e50\u8def\u5f84\u65b9\u6cd5\uff0c\u540c\u65f6\u751f\u6210\u6b63\u9762\u548c\u8d1f\u9762\u6d4b\u8bd5\u7528\u4f8b\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u4e3b\u8981\u5173\u6ce8\u5185\u90e8\u670d\u52a1\u5668\u9519\u8bef\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u5feb\u4e50\u8def\u5f84\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\uff0c\u4e14\u5b58\u5728\u53ef\u91cd\u7528\u6027\u3001\u6d4b\u8bd5\u9884\u8a00\u590d\u6742\u6027\u548c\u96c6\u6210\u65b9\u9762\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528LLM\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5316\u6784\u5efa\u6709\u6548\u7684\u6d4b\u8bd5\u573a\u666f\uff08\u5feb\u4e50\u8def\u5f84\uff09\uff0c\u5e76\u63a8\u5bfc\u8d1f\u9762\u6848\u4f8b\u6765\u9a8c\u8bc1\u9884\u671f\u529f\u80fd\uff082xx\u54cd\u5e94\uff09\u548c\u5bf9\u65e0\u6548\u8f93\u5165\u6216\u4e1a\u52a1\u89c4\u5219\u8fdd\u89c4\u7684\u9c81\u68d2\u6027\uff084xx\u54cd\u5e94\uff09\u3002", "result": "RESTifAI\u4e0e\u6700\u65b0\u7684LLM\u5de5\u5177\uff08AutoRestTest\u548cLogiAgent\uff09\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u53ef\u91cd\u7528\u6027\u3001\u9884\u8a00\u590d\u6742\u6027\u548c\u96c6\u6210\u65b9\u9762\u7684\u9650\u5236\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5de5\u4e1a\u670d\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "RESTifAI\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684LLM\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u91cd\u7528\u3001CI/CD\u5c31\u7eea\u7684REST API\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u5728\u7cfb\u7edf\u5316\u5feb\u4e50\u8def\u5f84\u6d4b\u8bd5\u751f\u6210\u65b9\u9762\u7684\u7a7a\u767d\u3002", "topic": "swe application"}}
{"id": "2512.08786", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08786", "abs": "https://arxiv.org/abs/2512.08786", "authors": ["Mahmoud Srewa", "Tianyu Zhao", "Salma Elmalaki"], "title": "A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs", "comment": null, "summary": "This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u8bc4\u4f30LLM\u4e0e\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u805a\u5408\u7b56\u7565\u6765\u5e73\u8861\u5bf9\u9f50\u8d28\u91cf\u4e0e\u516c\u5e73\u6027\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u4ee3\u8868\u591a\u6837\u5316\u7684\u4eba\u7c7b\u504f\u597d\uff0c\u5bfc\u81f4LLM\u5bf9\u9f50\u65f6\u5b58\u5728\u516c\u5e73\u6027\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5e73\u8861\u5bf9\u9f50\u8d28\u91cf\u548c\u516c\u5e73\u6027\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u8bc4\u4f30\u6846\u67b6\uff0c\u5404\u7ec4\u672c\u5730\u8bc4\u4f30rollouts\u5e76\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u670d\u52a1\u5668\u805a\u5408\u7ec4\u7ea7\u5956\u52b1\u800c\u4e0d\u8bbf\u95ee\u539f\u59cb\u6570\u636e\u3002\u8bc4\u4f30\u6807\u51c6\u805a\u5408\u6280\u672f\uff08\u6700\u5c0f\u3001\u6700\u5927\u3001\u5e73\u5747\uff09\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u65b9\u6848\uff0c\u57fa\u4e8e\u5386\u53f2\u5bf9\u9f50\u6027\u80fd\u52a8\u6001\u8c03\u6574\u504f\u597d\u6743\u91cd\u3002", "result": "\u5728\u57fa\u4e8ePPO\u7684RLHF\u7ba1\u9053\u7684\u95ee\u7b54\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0c\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u5bf9\u9f50\u5206\u6570\u7684\u540c\u65f6\uff0c\u59cb\u7ec8\u5b9e\u73b0\u66f4\u4f18\u7684\u516c\u5e73\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8bc4\u4f30LLM\u5728\u591a\u6837\u5316\u4eba\u7fa4\u4e2d\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u7a33\u5065\u65b9\u6cd5\uff0c\u5e76\u4e3a\u5f00\u53d1\u771f\u6b63\u591a\u5143\u5316\u548c\u516c\u5e73\u5bf9\u9f50\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.08810", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08810", "abs": "https://arxiv.org/abs/2512.08810", "authors": ["Viola Campos", "Robin Kuschnereit", "Adrian Ulges"], "title": "Multicalibration for LLM-based Code Generation", "comment": "Accepted at AI-SQE 2026 (The 1st International Workshop on AI for Software Quality Evaluation: Judgment, Metrics, Benchmarks, and Beyond)", "summary": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6821\u51c6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4f7f\u7528\u591a\u6821\u51c6\u65b9\u6cd5\u6765\u63d0\u9ad8\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u4ee3\u7801\u6b63\u786e\u6027\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728\u4e09\u4e2a\u51fd\u6570\u5408\u6210\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eAI\u7684\u4ee3\u7801\u751f\u6210\u6280\u672f\u666e\u53ca\uff0c\u786e\u4fdd\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u80fd\u591f\u771f\u5b9e\u53cd\u6620\u4ee3\u7801\u6b63\u786e\u6027\u7684\u6982\u7387\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u9700\u8981\u66f4\u597d\u7684\u6821\u51c6\u65b9\u6cd5\u6765\u6355\u6349\u7f16\u7801\u95ee\u9898\u7684\u989d\u5916\u56e0\u7d20\uff0c\u5982\u590d\u6742\u5ea6\u3001\u4ee3\u7801\u957f\u5ea6\u3001\u7f16\u7a0b\u8bed\u8a00\u7b49\u3002", "method": "\u7814\u7a76\u4e86\u56db\u79cd\u591a\u6821\u51c6\u65b9\u6cd5\uff0c\u5728\u4e09\u4e2a\u51fd\u6570\u5408\u6210\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4f7f\u7528\u4e86\u6700\u65b0\u4e00\u4ee3\u7684\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08Qwen3 Coder\u3001GPT-OSS\u3001DeepSeek-R1-Distill\uff09\u3002\u591a\u6821\u51c6\u65b9\u6cd5\u80fd\u591f\u6355\u6349\u7f16\u7801\u95ee\u9898\u7684\u989d\u5916\u56e0\u7d20\u3002", "result": "\u591a\u6821\u51c6\u65b9\u6cd5\u76f8\u6bd4\u672a\u6821\u51c6\u7684token\u4f3c\u7136\u5ea6\u63d0\u9ad8\u4e861.03\u6280\u80fd\u5206\u6570\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6821\u51c6\u65b9\u6cd5\u63d0\u9ad8\u4e860.37\u6280\u80fd\u5206\u6570\u3002\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u5206\u6790\u4e86\u4e0d\u540c\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5e76\u516c\u5f00\u4e86\u5305\u542b\u4ee3\u7801\u751f\u6210\u3001\u4f3c\u7136\u5ea6\u548c\u6b63\u786e\u6027\u6807\u7b7e\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u591a\u6821\u51c6\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6821\u51c6\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7684\u4ee3\u7801LLM\u6821\u51c6\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u53c2\u8003\u3002", "topic": "code agent"}}
{"id": "2512.08867", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.08867", "abs": "https://arxiv.org/abs/2512.08867", "authors": ["Jing Zhang", "Lianghong Guo", "Yanlin Wang", "Mingwei Liu", "Jiachi Chen", "Yuchi Ma", "Ensheng Shi", "Terry Yue Zhuo", "Hongyu Zhang", "Zibin Zheng"], "title": "SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA", "comment": null, "summary": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u77e5\u8bc6\u95ee\u7b54\u4efb\u52a1\u7684\u91cd\u8981\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4ece\u771f\u5b9e\u5bf9\u8bdd\u6784\u5efa\u7684SimpleDevQA\u57fa\u51c6\uff0c\u5b9e\u9a8c\u663e\u793a\u4ee3\u7801LLM\u4f18\u4e8e\u901a\u7528LLM\uff0cRAG\u7b56\u7565\u80fd\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u5f00\u53d1\u77e5\u8bc6\u95ee\u7b54\u5728\u771f\u5b9e\u7528\u6237-LLM\u5bf9\u8bdd\u4e2d\u5360\u6bd4\u6700\u9ad8\uff0839.6%\uff09\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u7406\u89e3\u800c\u5ffd\u7565\u4e86\u66f4\u5e7f\u6cdb\u7684\u5f00\u53d1\u77e5\u8bc6\u9700\u6c42\uff0c\u4e14\u8bb8\u591a\u57fa\u51c6\u4e0d\u662f\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u6784\u5efa\u7684\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u5c06\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\u8f6c\u5316\u4e3a\u7b80\u5355\u7684\u5f00\u53d1\u77e5\u8bc6\u5bfb\u6c42\u95ee\u7b54\u5bf9\uff0c\u6784\u5efa\u4e86\u591a\u8bed\u8a00\u57fa\u51c6SimpleDevQA\uff0c\u5305\u542b2,740\u4e2a\u95ee\u7b54\u5bf9\uff08\u82f1\u8bed\u3001\u4e2d\u6587\u3001\u4fc4\u8bed\uff09\u3002", "result": "\u4ee3\u7801LLM\u901a\u5e38\u4f18\u4e8e\u76f8\u4f3c\u89c4\u6a21\u7684\u901a\u7528LLM\uff1bRAG\u7b56\u7565\u5e73\u5747\u63d0\u5347LLM\u51c6\u786e\u738711.3%\uff1bLLM\u5728\u5f00\u53d1\u77e5\u8bc6\u95ee\u7b54\u4e2d\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u56de\u7b54\u51c6\u786e\u7387\u4e0e\u9648\u8ff0\u7684\u7f6e\u4fe1\u5ea6\u5448\u6b63\u76f8\u5173\uff1b\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5f3a\u7684LLM\u5728\u5f00\u53d1\u77e5\u8bc6\u95ee\u7b54\u4e2d\u4e5f\u8868\u73b0\u66f4\u5f3a\u3002", "conclusion": "\u5f00\u53d1\u77e5\u8bc6\u95ee\u7b54\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u91cd\u8981\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u57fa\u51c6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u7684SimpleDevQA\u57fa\u51c6\u80fd\u66f4\u597d\u5730\u8bc4\u4f30LLM\u7684\u5f00\u53d1\u77e5\u8bc6\u95ee\u7b54\u80fd\u529b\uff0cRAG\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002", "topic": "swe benchmark"}}
{"id": "2512.08463", "categories": ["cs.AI", "cs.LG", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.08463", "abs": "https://arxiv.org/abs/2512.08463", "authors": ["Antonio Terpin", "Raffaello D'Andrea"], "title": "Using reinforcement learning to probe the role of feedback in skill acquisition", "comment": "Website: https://antonioterpin.com/fluids-control", "summary": "Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u63a7\u5236\u65cb\u8f6c\u5706\u67f1\u4f53\u5728\u6c34\u6d41\u4e2d\u7684\u963b\u529b\uff0c\u53d1\u73b0\u5b66\u4e60\u9ad8\u6027\u80fd\u6280\u80fd\u9700\u8981\u6bd4\u6267\u884c\u6280\u80fd\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\u53cd\u9988\uff0c\u4e14\u5b66\u4e60\u6761\u4ef6\u7684\u597d\u574f\u4ec5\u53d6\u51b3\u4e8e\u76ee\u6807\u800c\u975e\u52a8\u6001\u590d\u6742\u6027\u3002", "motivation": "\u7814\u7a76\u5728\u65e0\u5916\u90e8\u53cd\u9988\u6761\u4ef6\u4e0b\u4eba\u7c7b\u5982\u4f55\u83b7\u5f97\u9ad8\u6027\u80fd\u6280\u80fd\uff08\u5982\u82b1\u6837\u6ed1\u51b0\u3001\u68d2\u7403\u6295\u7403\uff09\uff0c\u901a\u8fc7\u7269\u7406\u7cfb\u7edf\u66ff\u4ee3\u4eba\u7c7b\u53d7\u8bd5\u8005\uff0c\u5728\u5b8c\u5168\u53d7\u63a7\u6761\u4ef6\u4e0b\u63a2\u7d22\u6280\u80fd\u83b7\u53d6\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u63a7\u5236\u684c\u9762\u5faa\u73af\u6c34\u69fd\u4e2d\u7684\u65cb\u8f6c\u5706\u67f1\u4f53\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6216\u6700\u5c0f\u5316\u963b\u529b\u4f5c\u4e3a\u76ee\u6807\u3002\u7cfb\u7edf\u5229\u7528\u9ad8\u7ef4\u6d41\u52a8\u53cd\u9988\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4e0e\u65e0\u53cd\u9988\u6761\u4ef6\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u9ad8\u7ef4\u6d41\u52a8\u53cd\u9988\u4f7f\u667a\u80fd\u4f53\u5728\u51e0\u5206\u949f\u5185\u53d1\u73b0\u9ad8\u6027\u80fd\u963b\u529b\u63a7\u5236\u7b56\u7565\uff1b\u65e0\u53cd\u9988\u6267\u884c\u65f6\u6027\u80fd\u51e0\u4e4e\u76f8\u540c\u3002\u65e0\u6d41\u52a8\u53cd\u9988\u8bad\u7ec3\u65f6\uff0c\u667a\u80fd\u4f53\u5728\u963b\u529b\u6700\u5927\u5316\u4efb\u52a1\u4e2d\u5931\u8d25\uff0c\u5728\u963b\u529b\u6700\u5c0f\u5316\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5b66\u4e60\u9ad8\u6027\u80fd\u6280\u80fd\u9700\u8981\u6bd4\u6267\u884c\u6280\u80fd\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\uff0c\u5b66\u4e60\u6761\u4ef6\u7684\u597d\u574f\u4ec5\u53d6\u51b3\u4e8e\u76ee\u6807\uff08\u6700\u5927\u5316vs\u6700\u5c0f\u5316\uff09\uff0c\u800c\u975e\u7cfb\u7edf\u52a8\u6001\u6216\u7b56\u7565\u590d\u6742\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.08492", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08492", "abs": "https://arxiv.org/abs/2512.08492", "authors": ["Aliaksei Kaliutau"], "title": "Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance", "comment": "21 pages, 4 figures", "summary": "Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the \"Semantic Trap\" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.", "AI": {"tldr": "\u63d0\u51faData Transformation Graph (DTG) \u8303\u5f0f\uff0c\u5c06\u6570\u636e\u72b6\u6001\u4f5c\u4e3a\u8282\u70b9\u3001\u51fd\u6570\u4f5c\u4e3a\u8fb9\uff0c\u901a\u8fc7\u6570\u636e\u8c31\u7cfb\u800c\u975e\u63a7\u5236\u6d41\u8ffd\u8e2a\u903b\u8f91\u7f3a\u9677\uff0c\u5b9e\u73b0\u4ed3\u5e93\u7ea7\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u63a7\u5236\u4e2d\u5fc3\u7684\u4ee3\u7801\u4fee\u590d\u65b9\u6cd5\u5728\u5904\u7406\u4ed3\u5e93\u7ea7\u7a0b\u5e8f\u4fee\u590d\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5bfc\u822a\u590d\u6742\u76ee\u5f55\u7ed3\u6784\u548c\u65e0\u5173\u63a7\u5236\u903b\u8f91\u3002\u73b0\u6709RAG\u7cfb\u7edf\u5b58\u5728\"\u8bed\u4e49\u9677\u9631\"\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u4ee3\u7801\u7ef4\u62a4\u57fa\u7840\u3002", "method": "1) \u4ece\u6807\u51c6\u4ee3\u7801\u5c5e\u6027\u56fe\u8f6c\u5411\u6570\u636e\u8f6c\u6362\u56fe(DTG)\uff0c\u53cd\u8f6c\u62d3\u6251\u7ed3\u6784\uff1b2) \u5f15\u5165\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u534f\u8c03\u6570\u636e\u5b8c\u6574\u6027\u5bfc\u822a\u4e0e\u63a7\u5236\u6d41\u903b\u8f91\uff1b3) \u5b9e\u73b0\u81ea\u4e3b\u95ee\u9898\u89e3\u51b3\u5668(AIR)\uff0c\u4f7f\u7528\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u548cDTG\u7ed3\u6784\u8fdb\u884c\u53ef\u6269\u5c55\u903b\u8f91\u4fee\u590d\u3002", "result": "\u5728\u591a\u4e2aSWE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u826f\u597d\u7ed3\u679c\uff0c\u5728SWE-Verified\u57fa\u51c6\u4e0a\u8fbe\u523087.1%\u7684\u89e3\u51b3\u7387\uff0c\u76f4\u63a5\u89e3\u51b3\u4e86\u5f53\u524dAI\u4ee3\u7801\u52a9\u624b\u5de5\u5177\u7684\u6838\u5fc3\u9650\u5236\u3002", "conclusion": "DTG\u8303\u5f0f\u901a\u8fc7\u6570\u636e\u8c31\u7cfb\u8ffd\u8e2a\u903b\u8f91\u7f3a\u9677\uff0c\u89e3\u51b3\u4e86RAG\u7cfb\u7edf\u7684\"\u8bed\u4e49\u9677\u9631\"\uff0c\u4e3a\u8f6f\u4ef6\u4f9d\u8d56\u65e5\u76ca\u589e\u957f\u7684\u4e16\u754c\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u4ee3\u7801\u7ef4\u62a4\u57fa\u7840\u3002", "topic": "swe application"}}
{"id": "2512.08629", "categories": ["cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.08629", "abs": "https://arxiv.org/abs/2512.08629", "authors": ["Haoyu Zhao", "Weizhong Ding", "Yuhao Yang", "Zheng Tian", "Linyi Yang", "Kun Shao", "Jun Wang"], "title": "See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.", "AI": {"tldr": "See-Control\u662f\u4e00\u4e2a\u901a\u8fc7\u4f4e\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u76f4\u63a5\u7269\u7406\u4ea4\u4e92\u64cd\u4f5c\u667a\u80fd\u624b\u673a\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56ADB\u4ec5\u9650\u4e8eAndroid\u8bbe\u5907\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5e73\u53f0\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u624b\u673a\u64cd\u4f5c\u667a\u80fd\u4f53\u4f9d\u8d56Android Debug Bridge\u8fdb\u884c\u6570\u636e\u4f20\u8f93\u548c\u52a8\u4f5c\u6267\u884c\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u4ec5\u9002\u7528\u4e8eAndroid\u8bbe\u5907\uff0c\u7f3a\u4e4f\u5e73\u53f0\u65e0\u5173\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86Embodied Smartphone Operation\u4efb\u52a1\u548cSee-Control\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1)\u5305\u542b155\u4e2a\u4efb\u52a1\u548c\u8bc4\u4f30\u6307\u6807\u7684ESO\u57fa\u51c6\u6d4b\u8bd5\uff1b2)\u57fa\u4e8eMLLM\u7684\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u65e0\u9700ADB\u6216\u7cfb\u7edf\u540e\u7aef\u8bbf\u95ee\u5373\u53ef\u751f\u6210\u673a\u5668\u4eba\u63a7\u5236\u547d\u4ee4\uff1b3)\u4e30\u5bcc\u6807\u6ce8\u7684\u64cd\u4f5c\u7247\u6bb5\u6570\u636e\u96c6\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5e73\u53f0\u65e0\u5173\u7684\u667a\u80fd\u624b\u673a\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u5b9e\u73b0\u76f4\u63a5\u7269\u7406\u4ea4\u4e92\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3001\u667a\u80fd\u4f53\u6846\u67b6\u548c\u6570\u636e\u96c6\u8d44\u6e90\u3002", "conclusion": "See-Control\u901a\u8fc7\u5f25\u5408\u6570\u5b57\u667a\u80fd\u4f53\u4e0e\u7269\u7406\u4e16\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5b9e\u73b0\u5bb6\u5ead\u673a\u5668\u4eba\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6267\u884c\u4f9d\u8d56\u667a\u80fd\u624b\u673a\u7684\u4efb\u52a1\u8fc8\u51fa\u4e86\u5177\u4f53\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2512.08769", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08769", "abs": "https://arxiv.org/abs/2512.08769", "authors": ["Eranga Bandara", "Ross Gore", "Peter Foytik", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Xueping Liang", "Safdar H. Bouk", "Amin Hass", "Sachini Rajapakse", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows", "comment": null, "summary": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5b9e\u7528\u6307\u5357\uff0c\u7528\u4e8e\u8bbe\u8ba1\u3001\u5f00\u53d1\u548c\u90e8\u7f72\u751f\u4ea7\u7ea7\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u4ecb\u7ecd\u4e86\u7ed3\u6784\u5316\u5de5\u7a0b\u751f\u547d\u5468\u671f\u548c\u4e5d\u4e2a\u6838\u5fc3\u6700\u4f73\u5b9e\u8df5\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u65b0\u95fb\u5206\u6790\u6848\u4f8b\u8fdb\u884c\u6f14\u793a\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4f53AI\u5728\u884c\u4e1a\u548c\u7814\u7a76\u4e2d\u7684\u52a0\u901f\u91c7\u7528\uff0c\u7ec4\u7ec7\u9762\u4e34\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u5982\u4f55\u8bbe\u8ba1\u3001\u5de5\u7a0b\u548c\u8fd0\u8425\u751f\u4ea7\u7ea7\u7684\u667a\u80fd\u4f53AI\u5de5\u4f5c\u6d41\uff0c\u4f7f\u5176\u53ef\u9760\u3001\u53ef\u89c2\u5bdf\u3001\u53ef\u7ef4\u62a4\uff0c\u5e76\u7b26\u5408\u5b89\u5168\u548c\u6cbb\u7406\u8981\u6c42\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u5de5\u7a0b\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u5de5\u4f5c\u6d41\u5206\u89e3\u3001\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\u6a21\u5f0f\u3001\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae(MCP)\u548c\u5de5\u5177\u96c6\u6210\u3001\u786e\u5b9a\u6027\u7f16\u6392\u3001\u8d1f\u8d23\u4efbAI\u8003\u8651\u56e0\u7d20\u548c\u73af\u5883\u611f\u77e5\u90e8\u7f72\u7b56\u7565\u3002\u63d0\u51fa\u4e5d\u4e2a\u6838\u5fc3\u6700\u4f73\u5b9e\u8df5\uff0c\u5305\u62ec\u5de5\u5177\u4f18\u5148\u8bbe\u8ba1\u3001\u7eaf\u51fd\u6570\u8c03\u7528\u3001\u5355\u4e00\u5de5\u5177/\u5355\u4e00\u804c\u8d23\u667a\u80fd\u4f53\u3001\u5916\u90e8\u5316\u63d0\u793a\u7ba1\u7406\u7b49\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u591a\u6a21\u6001\u65b0\u95fb\u5206\u6790\u548c\u5a92\u4f53\u751f\u6210\u5de5\u4f5c\u6d41\u7684\u7efc\u5408\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u539f\u5219\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4e3a\u6784\u5efa\u7a33\u5065\u3001\u53ef\u6269\u5c55\u548c\u751f\u4ea7\u5c31\u7eea\u7684\u667a\u80fd\u4f53AI\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u57fa\u7840\u53c2\u8003\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u67b6\u6784\u6307\u5bfc\u3001\u64cd\u4f5c\u6a21\u5f0f\u548c\u5b9e\u8df5\u5b9e\u73b0\u6d1e\u5bdf\uff0c\u4e3a\u6784\u5efa\u7a33\u5065\u3001\u53ef\u6269\u5c55\u548c\u751f\u4ea7\u5c31\u7eea\u7684\u667a\u80fd\u4f53AI\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u57fa\u7840\u53c2\u8003\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2512.08012", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08012", "abs": "https://arxiv.org/abs/2512.08012", "authors": ["Aryaman Bansal", "Divya Sharma"], "title": "Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care", "comment": null, "summary": "In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.\n  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.", "AI": {"tldr": "\u5728ICU\u91cd\u75c7\u76d1\u62a4\u573a\u666f\u4e2d\uff0c\u7814\u7a76\u6bd4\u8f83\u4e86\u79bb\u7ebf\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0e\u5355\u76ee\u6807\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53d1\u73b0PEDA DT\u7b97\u6cd5\u5728\u7075\u6d3b\u6027\u548c\u4e2a\u6027\u5316\u51b3\u7b56\u65b9\u9762\u8868\u73b0\u6700\u4f18", "motivation": "\u91cd\u75c7\u76d1\u62a4\u4e2d\u4e34\u5e8a\u533b\u751f\u9700\u8981\u5728\u60a3\u8005\u751f\u5b58\u7387\u548c\u8d44\u6e90\u5229\u7528\uff08\u5982\u4f4f\u9662\u65f6\u95f4\uff09\u4e4b\u95f4\u5e73\u8861\uff0c\u4f20\u7edf\u5355\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u6807\u91cf\u5316\u5956\u52b1\u51fd\u6570\uff0c\u5bfc\u81f4\u7b56\u7565\u50f5\u5316\u65e0\u6cd5\u9002\u5e94\u53d8\u5316\u7684\u4e34\u5e8a\u4f18\u5148\u7ea7", "method": "\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u51c6\u6d4b\u8bd5\u4e86\u4e09\u79cd\u79bb\u7ebfMORL\u7b97\u6cd5\uff08CPQL\u3001Adaptive CPQL\u3001PEDA DT\uff09\u548c\u4e09\u79cd\u6807\u91cf\u5316\u5355\u76ee\u6807\u57fa\u7ebf\u65b9\u6cd5\uff08BC\u3001CQL\u3001DDQN\uff09\uff0c\u4f7f\u7528\u79bb\u7b56\u7565\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u6bd4\u8f83", "result": "PEDA DT\u7b97\u6cd5\u76f8\u6bd4\u9759\u6001\u6807\u91cf\u5316\u57fa\u7ebf\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u7075\u6d3b\u6027\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\u5728\u591a\u76ee\u6807\u6761\u4ef6\u751f\u6210\u4e2d\u4fdd\u6301\u7a33\u5065\u6709\u6548\uff0c\u6269\u5c55\u4e86\u5355\u76ee\u6807\u51b3\u7b56\u53d8\u6362\u5668\u5728\u533b\u7597\u9886\u57df\u7684\u7814\u7a76\u53d1\u73b0", "conclusion": "\u79bb\u7ebf\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u662f\u5b9e\u73b0\u91cd\u75c7\u76d1\u62a4\u4e2a\u6027\u5316\u3001\u53ef\u8c03\u6574\u51b3\u7b56\u7684\u6709\u524d\u666f\u6846\u67b6\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u4e34\u5e8a\u4f18\u5148\u7ea7", "topic": "agentic reinforcement learning"}}
{"id": "2512.08868", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08868", "abs": "https://arxiv.org/abs/2512.08868", "authors": ["Rui Min", "Zile Qiao", "Ze Xu", "Jiawen Zhai", "Wenyu Gao", "Xuanzhong Chen", "Haozhen Sun", "Zhen Zhang", "Xinyu Wang", "Hong Zhou", "Wenbiao Yin", "Xuan Zhou", "Yong Jiang", "Haicheng Liu", "Liang Ding", "Ling Zou", "Yi R.", "Fung", "Yalong Li", "Pengjun Xie"], "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce", "comment": null, "summary": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.", "AI": {"tldr": "EcomBench\u662f\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u7535\u5546\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4ee3\u7406\u5728\u5b9e\u9645\u7535\u5546\u73af\u5883\u4e2d\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u5305\u62ec\u6df1\u5ea6\u4fe1\u606f\u68c0\u7d22\u3001\u591a\u6b65\u63a8\u7406\u548c\u8de8\u6e90\u77e5\u8bc6\u6574\u5408\u7b49\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u5173\u6ce8\u5b66\u672f\u573a\u666f\u6216\u4eba\u5de5\u8bbe\u8ba1\u7684\u73af\u5883\uff0c\u5ffd\u89c6\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u51fa\u73b0\u7684\u6311\u6218\u3002\u7535\u5546\u9886\u57df\u5177\u6709\u5927\u91cf\u591a\u6837\u5316\u7528\u6237\u4ea4\u4e92\u3001\u52a8\u6001\u5e02\u573a\u6761\u4ef6\u548c\u771f\u5b9e\u51b3\u7b56\u4efb\u52a1\uff0c\u662f\u8bc4\u4f30\u667a\u80fd\u4ee3\u7406\u5b9e\u9645\u80fd\u529b\u7684\u7406\u60f3\u573a\u666f\u3002", "method": "\u4ece\u5168\u7403\u9886\u5148\u7535\u5546\u751f\u6001\u7cfb\u7edf\u7684\u771f\u5b9e\u7528\u6237\u9700\u6c42\u4e2d\u6784\u5efa\u57fa\u51c6\uff0c\u901a\u8fc7\u4eba\u5de5\u4e13\u5bb6\u7cbe\u5fc3\u7b56\u5212\u548c\u6807\u6ce8\uff0c\u786e\u4fdd\u6e05\u6670\u6027\u3001\u51c6\u786e\u6027\u548c\u9886\u57df\u76f8\u5173\u6027\u3002\u6db5\u76d6\u7535\u5546\u573a\u666f\u4e2d\u7684\u591a\u4e2a\u4efb\u52a1\u7c7b\u522b\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\u3002", "result": "EcomBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u4e14\u52a8\u6001\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8861\u91cf\u667a\u80fd\u4ee3\u7406\u5728\u73b0\u4ee3\u7535\u5546\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u6df1\u5ea6\u4fe1\u606f\u68c0\u7d22\u3001\u591a\u6b65\u63a8\u7406\u548c\u8de8\u6e90\u77e5\u8bc6\u6574\u5408\u7b49\u5173\u952e\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8bc4\u4f30\u5efa\u7acb\u5728\u771f\u5b9e\u7535\u5546\u73af\u5883\u4e2d\uff0cEcomBench\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30\u667a\u80fd\u4ee3\u7406\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "topic": "swe benchmark"}}
{"id": "2512.08093", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08093", "abs": "https://arxiv.org/abs/2512.08093", "authors": ["Manas Joglekar", "Jeremy Chen", "Gabriel Wu", "Jason Yosinski", "Jasmine Wang", "Boaz Barak", "Amelia Glaese"], "title": "Training LLMs for Honesty via Confessions", "comment": null, "summary": "Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.\n  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the \"path of least resistance\" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.\n  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its \"main\" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\"\u5fcf\u6094\"\u673a\u5236\u4fc3\u4f7fLLM\u8bda\u5b9e\u8868\u8fbe\u81ea\u8eab\u7f3a\u9677\u7684\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u4e2d\u5355\u72ec\u5956\u52b1\u5fcf\u6094\u7684\u8bda\u5b9e\u6027\uff0c\u4f7f\u6a21\u578b\u66f4\u613f\u610f\u627f\u8ba4\u9519\u8bef\u800c\u975e\u63a9\u76d6", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u62a5\u544a\u884c\u4e3a\u548c\u4fe1\u5ff5\u65f6\u53ef\u80fd\u5b58\u5728\u4e0d\u8bda\u5b9e\u884c\u4e3a\uff08\u5982\u5938\u5927\u4fe1\u5fc3\u3001\u63a9\u76d6\u8bc1\u636e\uff09\uff0c\u8fd9\u53ef\u80fd\u6e90\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u5851\u9020\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4fc3\u4f7f\u6a21\u578b\u8bda\u5b9e\u8868\u8fbe\u81ea\u8eab\u7f3a\u9677", "method": "\u63d0\u51fa\"\u5fcf\u6094\"\u673a\u5236\uff1a\u6a21\u578b\u5728\u539f\u59cb\u56de\u7b54\u540e\u63d0\u4f9b\u5fcf\u6094\u8f93\u51fa\uff0c\u5b8c\u6574\u8bf4\u660e\u5176\u9075\u5b88\u653f\u7b56\u548c\u6307\u4ee4\u7684\u60c5\u51b5\u3002\u8bad\u7ec3\u4e2d\u5355\u72ec\u57fa\u4e8e\u5fcf\u6094\u7684\u8bda\u5b9e\u6027\u7ed9\u4e88\u5956\u52b1\uff0c\u4e0d\u5f71\u54cd\u4e3b\u56de\u7b54\u7684\u5956\u52b1\u3002\u901a\u8fc7\u4f7f\u627f\u8ba4\u9519\u8bef\u6210\u4e3a\"\u6700\u5c0f\u963b\u529b\u8def\u5f84\"\u6765\u6fc0\u52b1\u8bda\u5b9e", "result": "\u8bad\u7ec3GPT-5-Thinking\u4ea7\u751f\u5fcf\u6094\uff0c\u5728\u5206\u5e03\u5916\u573a\u666f\uff08\u5e7b\u89c9\u3001\u6307\u4ee4\u9075\u5faa\u3001\u9634\u8c0b\u3001\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff09\u4e2d\u8bc4\u4f30\u8bda\u5b9e\u6027\u3002\u53d1\u73b0\u5f53\u6a21\u578b\u5728\u4e3b\u56de\u7b54\u4e2d\u6492\u8c0e\u6216\u7701\u7565\u7f3a\u9677\u65f6\uff0c\u901a\u5e38\u80fd\u8bda\u5b9e\u5fcf\u6094\uff0c\u4e14\u5fcf\u6094\u8bda\u5b9e\u6027\u968f\u8bad\u7ec3\u9002\u5ea6\u63d0\u5347", "conclusion": "\u5fcf\u6094\u673a\u5236\u53ef\u884c\uff0c\u80fd\u4fc3\u4f7f\u6a21\u578b\u8bda\u5b9e\u8868\u8fbe\u7f3a\u9677\u3002\u5fcf\u6094\u652f\u6301\u591a\u79cd\u63a8\u7406\u65f6\u5e72\u9884\uff0c\u5305\u62ec\u76d1\u63a7\u3001\u62d2\u7edd\u91c7\u6837\u548c\u5411\u7528\u6237\u5448\u73b0\u95ee\u9898", "topic": "agent analysis"}}
{"id": "2512.08108", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08108", "abs": "https://arxiv.org/abs/2512.08108", "authors": ["Kwanyoung Park", "Seohong Park", "Youngwoon Lee", "Sergey Levine"], "title": "Scalable Offline Model-Based RL with Action Chunks", "comment": "22 pages, 7 figures", "summary": "In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \\emph{action-chunk} model that predicts a future state from a sequence of actions (an \"action chunk\") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \\textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.", "AI": {"tldr": "\u63d0\u51faMAC\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u4f5c\u5757\u6a21\u578b\u51cf\u5c11\u957f\u65f6\u57df\u9884\u6d4b\u8bef\u5dee\uff0c\u7ed3\u5408\u62d2\u7edd\u91c7\u6837\u9632\u6b62\u6a21\u578b\u5229\u7528\uff0c\u5728\u79bb\u7ebf\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u957f\u65f6\u57df\u590d\u6742\u4efb\u52a1\u7684\u6269\u5c55", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u957f\u65f6\u57df\u4efb\u52a1\u7684\u6311\u6218\uff0c\u4f20\u7edf\u6a21\u578b\u503c\u6269\u5c55\u65b9\u6cd5\u5728\u589e\u52a0\u9884\u6d4b\u6b65\u6570\u65f6\u9762\u4e34\u6a21\u578b\u8bef\u5dee\u7d2f\u79ef\u4e0e\u504f\u5dee\u51cf\u5c11\u7684\u6743\u8861\u95ee\u9898", "method": "\u63d0\u51fa\u52a8\u4f5c\u5757\u6a21\u578b\u9884\u6d4b\u52a8\u4f5c\u5e8f\u5217\u540e\u7684\u72b6\u6001\u800c\u975e\u5355\u6b65\u52a8\u4f5c\uff0c\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\uff1b\u91c7\u7528\u62d2\u7edd\u91c7\u6837\u4ece\u884c\u4e3a\u52a8\u4f5c\u5757\u7b56\u7565\u4e2d\u91c7\u6837\uff0c\u9632\u6b62\u6a21\u578b\u5229\u7528\u548c\u5206\u5e03\u5916\u52a8\u4f5c\u95ee\u9898", "result": "\u5728\u5305\u542b\u9ad8\u8fbe1\u4ebf\u8f6c\u6362\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\uff0cMAC\u5728\u79bb\u7ebf\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u6311\u6218\u6027\u957f\u65f6\u57df\u4efb\u52a1\u4e0a", "conclusion": "MAC\u4e3a\u79bb\u7ebf\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u4f5c\u5757\u6a21\u578b\u548c\u62d2\u7edd\u91c7\u6837\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u57df\u4efb\u52a1\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u6a21\u578b\u5229\u7528\u95ee\u9898", "topic": "agentic reinforcement learning"}}
{"id": "2512.08139", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08139", "abs": "https://arxiv.org/abs/2512.08139", "authors": ["Mikayel Samvelyan"], "title": "Robust Agents in Open-Ended Worlds", "comment": "PhD Thesis", "summary": "The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u591a\u79cd\u65b9\u6cd5\u6765\u63d0\u5347AI\u667a\u80fd\u4f53\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5305\u62ecMiniHack\u6846\u67b6\u3001Maestro\u5bf9\u6297\u8bfe\u7a0b\u751f\u6210\u3001\u8d28\u91cf\u591a\u6837\u6027\u65b9\u6cd5\u5206\u6790\u591a\u667a\u80fd\u4f53\u6f0f\u6d1e\uff0c\u4ee5\u53ca\u8fdb\u5316\u641c\u7d22\u589e\u5f3aLLM\u5bf9\u6297\u63d0\u793a\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740AI\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u80fd\u591f\u6210\u529f\u5bfc\u822a\u548c\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u5f00\u653e\u4e16\u754c\u7684\u667a\u80fd\u4f53\u3002\u5173\u952e\u6311\u6218\u662f\u786e\u4fdd\u8fd9\u4e9bAI\u667a\u80fd\u4f53\u4e0d\u4ec5\u80fd\u5728\u8bad\u7ec3\u4e2d\u719f\u6089\u7684\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u80fd\u6709\u6548\u5730\u6cdb\u5316\u5230\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u591a\u6837\u5316\u573a\u666f\u3002", "method": "1. \u5f15\u5165MiniHack\uff1a\u57fa\u4e8eNetHack\u7684\u6c99\u76d2\u6846\u67b6\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5316\u5185\u5bb9\u751f\u6210\u521b\u5efa\u591a\u6837\u5316\u73af\u5883\uff1b2. \u63d0\u51faMaestro\uff1a\u751f\u6210\u5bf9\u6297\u8bfe\u7a0b\u7684\u65b9\u6cd5\uff0c\u9010\u6b65\u589e\u5f3aRL\u667a\u80fd\u4f53\u5728\u53cc\u4eba\u96f6\u548c\u6e38\u620f\u4e2d\u7684\u9c81\u68d2\u6027\uff1b3. \u4f7f\u7528\u8d28\u91cf\u591a\u6837\u6027\u65b9\u6cd5\u7cfb\u7edf\u8bc6\u522b\u9884\u8bad\u7ec3RL\u7b56\u7565\u5728\u8db3\u7403\u6e38\u620f\u9886\u57df\u7684\u6f0f\u6d1e\uff1b4. \u6269\u5c55\u7814\u7a76\u5230LLM\u9886\u57df\uff0c\u4f7f\u7528\u8fdb\u5316\u641c\u7d22\u751f\u6210\u591a\u6837\u5316\u5bf9\u6297\u63d0\u793a\u6765\u8bca\u65ad\u548c\u589e\u5f3aLLM\u9c81\u68d2\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u591a\u79cd\u5de5\u5177\u548c\u65b9\u6cd5\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\u9c81\u68d2\u7684AI\u667a\u80fd\u4f53\uff0c\u4f7f\u5176\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u9896\u73af\u5883\u3001\u5206\u5e03\u5916\u8f93\u5165\u4ee5\u53ca\u4e0e\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u4ea4\u4e92\uff0c\u4e3a\u672a\u6765AI\u9c81\u68d2\u6027\u7814\u7a76\u94fa\u5e73\u9053\u8def\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5f00\u653e\u6027\u548c\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u5de5\u4f5c\u4e3a\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u4e16\u754c\u5e76\u5728\u9762\u5bf9\u610f\u5916\u6311\u6218\u548c\u4ea4\u4e92\u65f6\u84ec\u52c3\u53d1\u5c55\u7684AI\u667a\u80fd\u4f53\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2512.08153", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.08153", "abs": "https://arxiv.org/abs/2512.08153", "authors": ["Zheng Ding", "Weirui Ye"], "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models", "comment": null, "summary": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \\textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \\emph{High sample efficiency}, achieving better performance under same training samples (2) \\emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \\emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \\textbf{2.4$\\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.", "AI": {"tldr": "TreeGRPO\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53bb\u566a\u8fc7\u7a0b\u91cd\u6784\u4e3a\u641c\u7d22\u6811\uff0c\u663e\u8457\u63d0\u9ad8\u751f\u6210\u6a21\u578b\u5bf9\u9f50\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u5b9e\u73b02.4\u500d\u52a0\u901f\u8bad\u7ec3", "motivation": "\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u5bf9\u4e8e\u5c06\u751f\u6210\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\u963b\u788d\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u4fe1\u7528\u5206\u914d\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5c06\u53bb\u566a\u8fc7\u7a0b\u91cd\u6784\u4e3a\u641c\u7d22\u6811\uff0c\u4ece\u5171\u4eab\u7684\u521d\u59cb\u566a\u58f0\u6837\u672c\u51fa\u53d1\uff0c\u7b56\u7565\u6027\u5730\u5206\u652f\u751f\u6210\u591a\u4e2a\u5019\u9009\u8f68\u8ff9\uff0c\u540c\u65f6\u9ad8\u6548\u590d\u7528\u5176\u5171\u540c\u524d\u7f00\u3002\u901a\u8fc7\u6811\u7ed3\u6784\u5b9e\u73b0\u9ad8\u6837\u672c\u6548\u7387\u3001\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff08\u901a\u8fc7\u5956\u52b1\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u6b65\u9aa4\u7279\u5b9a\u4f18\u52bf\uff09\u548c\u644a\u9500\u8ba1\u7b97\uff08\u6bcf\u4e2a\u524d\u5411\u4f20\u9012\u5b9e\u73b0\u591a\u4e2a\u7b56\u7565\u66f4\u65b0\uff09\u3002", "result": "\u5728\u6269\u6563\u6a21\u578b\u548c\u6d41\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTreeGRPO\u5b9e\u73b0\u4e862.4\u500d\u8bad\u7ec3\u52a0\u901f\uff0c\u5728\u6548\u7387-\u5956\u52b1\u6743\u8861\u7a7a\u95f4\u4e2d\u5efa\u7acb\u4e86\u66f4\u4f18\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u5956\u52b1\u6a21\u578b\u4e0a\u59cb\u7ec8\u4f18\u4e8eGRPO\u57fa\u7ebf\u3002", "conclusion": "TreeGRPO\u4e3a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89c6\u89c9\u751f\u6210\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u9014\u5f84\uff0c\u901a\u8fc7\u6811\u7ed3\u6784\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.08485", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08485", "abs": "https://arxiv.org/abs/2512.08485", "authors": ["Junnan Qiu", "Jie Li"], "title": "Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning", "comment": null, "summary": "Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5168\u5c40\u9884\u7b97\u5206\u914d\u653b\u51fb\u7b56\u7565\uff0c\u57fa\u4e8eTD\u8bef\u5dee\u5206\u914d\u6270\u52a8\u9884\u7b97\uff0c\u76f8\u6bd4\u5747\u5300\u6270\u52a8\u66f4\u9ad8\u6548\u4e14\u9690\u853d", "motivation": "\u73b0\u6709\u79bb\u7ebfRL\u6570\u636e\u6295\u6bd2\u653b\u51fb\u91c7\u7528\u5747\u5300\u6270\u52a8\u7b56\u7565\uff0c\u5bf9\u6240\u6709\u6837\u672c\u65e0\u5dee\u522b\u5904\u7406\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u7f3a\u4e4f\u9690\u853d\u6027\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u653b\u51fb\u65b9\u6cd5", "method": "\u63d0\u51fa\u5168\u5c40\u9884\u7b97\u5206\u914d\u653b\u51fb\u7b56\u7565\uff0c\u57fa\u4e8e\u7406\u8bba\u6d1e\u5bdf\uff1a\u6837\u672c\u5bf9\u4ef7\u503c\u51fd\u6570\u6536\u655b\u7684\u5f71\u54cd\u4e0eTD\u8bef\u5dee\u6210\u6b63\u6bd4\uff0c\u5c06\u653b\u51fb\u5efa\u6a21\u4e3a\u5168\u5c40\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u5728L2\u7ea6\u675f\u4e0b\u6270\u52a8\u5e45\u5ea6\u4e0eTD\u8bef\u5dee\u654f\u611f\u6027\u6210\u6b63\u6bd4\u7684\u95ed\u5f0f\u89e3", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u7b56\u7565\uff0c\u4ec5\u7528\u6700\u5c0f\u6270\u52a8\u5c31\u80fd\u5b9e\u73b0\u9ad8\u8fbe80%\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u80fd\u9003\u907f\u6700\u5148\u8fdb\u7684\u7edf\u8ba1\u548c\u9891\u8c31\u9632\u5fa1\u68c0\u6d4b", "conclusion": "\u5168\u5c40\u9884\u7b97\u5206\u914d\u653b\u51fb\u7b56\u7565\u6bd4\u5747\u5300\u6270\u52a8\u66f4\u9ad8\u6548\u548c\u9690\u853d\uff0c\u4e3a\u79bb\u7ebfRL\u5b89\u5168\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u653b\u51fb\u89c6\u89d2", "topic": "agentic reinforcement learning"}}
{"id": "2512.08870", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08870", "abs": "https://arxiv.org/abs/2512.08870", "authors": ["Xiang Chen", "Yuling Shi", "Qizhen Lan", "Yuchao Qiu", "Xiaodong Gu"], "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents", "comment": null, "summary": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.", "AI": {"tldr": "Fed-SE\uff1a\u8054\u90a6\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u9690\u79c1\u7ea6\u675f\u4e0b\u7684\u8de8\u73af\u5883\u77e5\u8bc6\u8fc1\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u5c40\u90e8\u8fdb\u5316-\u5168\u5c40\u805a\u5408\u8303\u5f0f\uff0c\u5728\u5f02\u6784\u4efb\u52a1\u4e2d\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u7ea618%", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u4ea4\u4e92\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4f46\u9690\u79c1\u7ea6\u675f\u9650\u5236\u4e86\u96c6\u4e2d\u5f0f\u4f18\u5316\u548c\u8de8\u52a8\u6001\u73af\u5883\u7684\u534f\u540c\u8fdb\u5316\u3002\u8054\u90a6\u5b66\u4e60\u5728\u9759\u6001\u6570\u636e\u96c6\u4e0a\u6709\u6548\uff0c\u4f46\u5728\u5f00\u653e\u5f0f\u7684\u667a\u80fd\u4f53\u81ea\u8fdb\u5316\u65b9\u9762\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u76f4\u63a5\u5e94\u7528\u6807\u51c6\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u6311\u6218\uff1a\u5f02\u6784\u4efb\u52a1\u548c\u7a00\u758f\u7684\u8f68\u8ff9\u7ea7\u5956\u52b1\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u68af\u5ea6\u51b2\u7a81\uff0c\u7834\u574f\u5168\u5c40\u4f18\u5316\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faFed-SE\u8054\u90a6\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u91c7\u7528\u5c40\u90e8\u8fdb\u5316-\u5168\u5c40\u805a\u5408\u8303\u5f0f\u3002\u5c40\u90e8\u5c42\u9762\uff1a\u667a\u80fd\u4f53\u5728\u8fc7\u6ee4\u7684\u9ad8\u56de\u62a5\u8f68\u8ff9\u4e0a\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u68af\u5ea6\u66f4\u65b0\u3002\u5168\u5c40\u5c42\u9762\uff1a\u5728\u4f4e\u79e9\u5b50\u7a7a\u95f4\u5185\u805a\u5408\u66f4\u65b0\uff0c\u89e3\u8026\u73af\u5883\u7279\u5b9a\u7684\u52a8\u6001\u7279\u6027\uff0c\u6709\u6548\u51cf\u5c11\u5ba2\u6237\u7aef\u95f4\u7684\u8d1f\u8fc1\u79fb\u3002", "result": "\u5728\u4e94\u4e2a\u5f02\u6784\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFed-SE\u76f8\u6bd4\u8054\u90a6\u57fa\u7ebf\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u63d0\u5347\u7ea618%\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u9690\u79c1\u7ea6\u675f\u90e8\u7f72\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u8de8\u73af\u5883\u77e5\u8bc6\u8fc1\u79fb\u7684\u6709\u6548\u6027\u3002", "conclusion": "Fed-SE\u6210\u529f\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u9690\u79c1\u7ea6\u675f\u4e0b\u7684\u8054\u90a6\u81ea\u8fdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5c40\u90e8\u7a33\u5b9a\u4f18\u5316\u548c\u5168\u5c40\u89e3\u8026\u805a\u5408\uff0c\u5b9e\u73b0\u4e86\u8de8\u5f02\u6784\u73af\u5883\u7684\u6709\u6548\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u667a\u80fd\u4f53\u534f\u540c\u8fdb\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.4e2c09a4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fyoavg%2F3eb3e722d38e887a0a8ac151c62d9617%3Futm_source=tldrai/1/0100019afe63300f-1580626f-03a3-4234-aec4-512e81a071d6-000000/3AALUF13HYivKZMwQCE7engsPC7U6VOScedD2vIrlIk=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fyoavg%2F3eb3e722d38e887a0a8ac151c62d9617%3Futm_source=tldrai/1/0100019afe63300f-1580626f-03a3-4234-aec4-512e81a071d6-000000/3AALUF13HYivKZMwQCE7engsPC7U6VOScedD2vIrlIk=434", "authors": ["TLDR Newsletter"], "title": "Computer-science Reinforcement Learning Got Rewards Wrong", "comment": "Source: TLDR Newsletter, Date: 2025-12-08, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fyoavg%2F3eb3e722d38e887a0a8ac151c62d9617%3Futm_source=tldrai/1/0100019afe63300f-1580626f-03a3-4234-aec4-512e81a071d6-000000/3AALUF13HYivKZMwQCE7engsPC7U6VOScedD2vIrlIk=434", "summary": "Computer-science Reinforcement Learning Got Rewards Wrong (6 minute read) In reinforcement learning, the reward is part of the agent, not part of the environment. Once we start thinking in terms of the reward translation as being part of the agent, we can also start thinking of ways in which the agent can affect the reward translation mechanism. It could be dynamic and change across time, be tied to policy, or even learned. The shift in perspective creates many possible options, just by chang...", "source": "tldr", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u673a\u5236\u5e94\u88ab\u89c6\u4e3a\u667a\u80fd\u4f53\u7684\u4e00\u90e8\u5206\u800c\u975e\u73af\u5883\u7684\u4e00\u90e8\u5206\uff0c\u8fd9\u4e00\u89c6\u89d2\u8f6c\u53d8\u5f00\u542f\u4e86\u5956\u52b1\u673a\u5236\u52a8\u6001\u8c03\u6574\u3001\u4e0e\u7b56\u7565\u7ed1\u5b9a\u751a\u81f3\u53ef\u5b66\u4e60\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5c06\u5956\u52b1\u89c6\u4e3a\u73af\u5883\u7684\u4e00\u90e8\u5206\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u89c2\u70b9\u9650\u5236\u4e86\u5956\u52b1\u673a\u5236\u7684\u8bbe\u8ba1\u53ef\u80fd\u6027\u3002\u5c06\u5956\u52b1\u89c6\u4e3a\u667a\u80fd\u4f53\u7684\u4e00\u90e8\u5206\u53ef\u4ee5\u5e26\u6765\u66f4\u7075\u6d3b\u3001\u66f4\u667a\u80fd\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u5206\u6790\u548c\u89c6\u89d2\u8f6c\u53d8\uff0c\u63d0\u51fa\u5c06\u5956\u52b1\u7ffb\u8bd1\u673a\u5236\u89c6\u4e3a\u667a\u80fd\u4f53\u5185\u90e8\u7ec4\u4ef6\u800c\u975e\u73af\u5883\u56fa\u5b9a\u5c5e\u6027\u7684\u65b0\u6846\u67b6\u3002\u63a2\u8ba8\u4e86\u5956\u52b1\u673a\u5236\u53ef\u4ee5\u52a8\u6001\u53d8\u5316\u3001\u4e0e\u7b56\u7565\u7ed1\u5b9a\u6216\u901a\u8fc7\u5b66\u4e60\u4f18\u5316\u7684\u53ef\u80fd\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u673a\u5236\u7684\u65b0\u7406\u8bba\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5c06\u5956\u52b1\u89c6\u4e3a\u667a\u80fd\u4f53\u4e00\u90e8\u5206\u6240\u5e26\u6765\u7684\u8bbe\u8ba1\u81ea\u7531\u5ea6\uff0c\u4e3a\u672a\u6765\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u65b9\u5411\u3002", "conclusion": "\u5956\u52b1\u673a\u5236\u5e94\u88ab\u89c6\u4e3a\u667a\u80fd\u4f53\u7684\u7ec4\u6210\u90e8\u5206\u800c\u975e\u73af\u5883\u7684\u56fa\u5b9a\u5c5e\u6027\uff0c\u8fd9\u4e00\u6839\u672c\u6027\u89c6\u89d2\u8f6c\u53d8\u4e3a\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u8bbe\u8ba1\u7a7a\u95f4\u548c\u7814\u7a76\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2512.5235b508", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fgoogle-plans-to-integrate-opal-agent-builder-into-gemini%2F%3Futm_source=tldrai/1/0100019afe63300f-1580626f-03a3-4234-aec4-512e81a071d6-000000/aABeM9lVQSvg7uOs5-cg1o8J0s-9dP2lUz4grExQncw=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fgoogle-plans-to-integrate-opal-agent-builder-into-gemini%2F%3Futm_source=tldrai/1/0100019afe63300f-1580626f-03a3-4234-aec4-512e81a071d6-000000/aABeM9lVQSvg7uOs5-cg1o8J0s-9dP2lUz4grExQncw=434", "authors": ["TLDR Newsletter"], "title": "Google plans to integrate Opal Agent builder into Gemini", "comment": "Source: TLDR Newsletter, Date: 2025-12-08, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fgoogle-plans-to-integrate-opal-agent-builder-into-gemini%2F%3Futm_source=tldrai/1/0100019afe63300f-1580626f-03a3-4234-aec4-512e81a071d6-000000/aABeM9lVQSvg7uOs5-cg1o8J0s-9dP2lUz4grExQncw=434", "summary": "Google plans to integrate Opal Agent builder into Gemini (2 minute read) Google is exploring integrating Opal's agent-based automation into Gemini, enhancing direct access to advanced AI tools.", "source": "tldr", "AI": {"tldr": "Google\u8ba1\u5212\u5c06Opal Agent\u6784\u5efa\u5668\u96c6\u6210\u5230Gemini\u4e2d\uff0c\u4ee5\u589e\u5f3a\u5bf9\u9ad8\u7ea7AI\u5de5\u5177\u7684\u76f4\u63a5\u8bbf\u95ee", "motivation": "Google\u5e0c\u671b\u901a\u8fc7\u96c6\u6210Opal\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u81ea\u52a8\u5316\u6280\u672f\u6765\u589e\u5f3aGemini\u7684\u80fd\u529b\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u66f4\u76f4\u63a5\u8bbf\u95ee\u9ad8\u7ea7AI\u5de5\u5177\u7684\u9014\u5f84", "method": "\u901a\u8fc7\u5c06Opal Agent\u6784\u5efa\u5668\u96c6\u6210\u5230Gemini\u5e73\u53f0\u4e2d\uff0c\u5b9e\u73b0\u4ee3\u7406\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u529f\u80fd", "result": "Google\u6b63\u5728\u63a2\u7d22\u8fd9\u4e00\u96c6\u6210\u65b9\u6848\uff0c\u4f46\u5177\u4f53\u5b9e\u65bd\u7ec6\u8282\u548c\u6700\u7ec8\u7ed3\u679c\u5c1a\u672a\u516c\u5e03", "conclusion": "Google\u8ba1\u5212\u901a\u8fc7Opal\u4e0eGemini\u7684\u96c6\u6210\u6765\u63d0\u5347\u5176AI\u5e73\u53f0\u7684\u81ea\u52a8\u5316\u80fd\u529b\u548c\u5de5\u5177\u8bbf\u95ee\u6027", "topic": "agent analysis"}}
{"id": "tldr.2512.8687cbb0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinalderson.com%2Fposts%2Fhas-the-cost-of-software-just-dropped-90-percent%2F%3Futm_source=tldrnewsletter/1/0100019b02da8c44-1de8049f-d511-46f4-aaa0-85336818a468-000000/joUVK-fXxievWi6yPMjLM8--xgcWImWm6Dvs7hEwvcs=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinalderson.com%2Fposts%2Fhas-the-cost-of-software-just-dropped-90-percent%2F%3Futm_source=tldrnewsletter/1/0100019b02da8c44-1de8049f-d511-46f4-aaa0-85336818a468-000000/joUVK-fXxievWi6yPMjLM8--xgcWImWm6Dvs7hEwvcs=434", "authors": ["TLDR Newsletter"], "title": "Has the cost of building software just dropped 90%?", "comment": "Source: TLDR Newsletter, Date: 2025-12-09, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinalderson.com%2Fposts%2Fhas-the-cost-of-software-just-dropped-90-percent%2F%3Futm_source=tldrnewsletter/1/0100019b02da8c44-1de8049f-d511-46f4-aaa0-85336818a468-000000/joUVK-fXxievWi6yPMjLM8--xgcWImWm6Dvs7hEwvcs=434", "summary": "Has the cost of building software just dropped 90%? (8 minute read) We are in the early stages of a once-in-a-generation shift. The economics of building software have changed dramatically with agentic coding. Previous assertions that LLMs make too many mistakes or can't understand codebases or frameworks are rapidly becoming completely false. The technology is going to totally transform the software development industry.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u7f16\u7801\u6280\u672f\u6b63\u5728\u5f7b\u5e95\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u884c\u4e1a\uff0c\u5c06\u8f6f\u4ef6\u6784\u5efa\u6210\u672c\u964d\u4f4e90%\uff0c\u6807\u5fd7\u7740\u4ee3\u9645\u6027\u7684\u6280\u672f\u8f6c\u53d8", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3aLLMs\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5b58\u5728\u592a\u591a\u9519\u8bef\u3001\u65e0\u6cd5\u7406\u89e3\u4ee3\u7801\u5e93\u548c\u6846\u67b6\uff0c\u4f46\u8fd9\u4e9b\u89c2\u70b9\u6b63\u5728\u8fc5\u901f\u53d8\u5f97\u8fc7\u65f6\u3002\u4f5c\u8005\u65e8\u5728\u5c55\u793aAI\u4ee3\u7406\u7f16\u7801\u6280\u672f\u5982\u4f55\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u7684\u7ecf\u6d4e\u5b66", "method": "\u6587\u7ae0\u91c7\u7528\u89c2\u70b9\u6027\u5206\u6790\u800c\u975e\u5b9e\u8bc1\u7814\u7a76\uff0c\u57fa\u4e8e\u5bf9AI\u4ee3\u7406\u7f16\u7801\u6280\u672f\u53d1\u5c55\u8d8b\u52bf\u7684\u89c2\u5bdf\uff0c\u8bba\u8bc1\u8be5\u6280\u672f\u6b63\u5728\u89e3\u51b3LLMs\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u4f20\u7edf\u5c40\u9650\u6027", "result": "AI\u4ee3\u7406\u7f16\u7801\u6280\u672f\u6b63\u5728\u8fc5\u901f\u6210\u719f\uff0c\u80fd\u591f\u6709\u6548\u7406\u89e3\u4ee3\u7801\u5e93\u548c\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u9519\u8bef\uff0c\u4ece\u800c\u5c06\u8f6f\u4ef6\u6784\u5efa\u6210\u672c\u964d\u4f4e90%\uff0c\u5f15\u53d1\u8f6f\u4ef6\u5f00\u53d1\u884c\u4e1a\u7684\u6839\u672c\u6027\u53d8\u9769", "conclusion": "\u6211\u4eec\u6b63\u5904\u4e8e\u4e00\u4ee3\u4eba\u4e00\u6b21\u7684\u8f6c\u53d8\u65e9\u671f\uff0cAI\u4ee3\u7406\u7f16\u7801\u6280\u672f\u5c06\u5f7b\u5e95\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u884c\u4e1a\uff0c\u4f20\u7edf\u5173\u4e8eLLMs\u5c40\u9650\u6027\u7684\u89c2\u70b9\u6b63\u5728\u8fc5\u901f\u53d8\u5f97\u8fc7\u65f6", "topic": "code agent"}}
{"id": "tldr.2512.1f471e49", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fs91DHL%3Futm_source=tldrnewsletter/1/0100019b02da8c44-1de8049f-d511-46f4-aaa0-85336818a468-000000/ulGmcBG4yR2omv5AVOT4I2dG-DKrG2f1Rm-ZM7raCf4=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fs91DHL%3Futm_source=tldrnewsletter/1/0100019b02da8c44-1de8049f-d511-46f4-aaa0-85336818a468-000000/ulGmcBG4yR2omv5AVOT4I2dG-DKrG2f1Rm-ZM7raCf4=434", "authors": ["TLDR Newsletter"], "title": "Agents that don't suck", "comment": "Source: TLDR Newsletter, Date: 2025-12-09, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fs91DHL%3Futm_source=tldrnewsletter/1/0100019b02da8c44-1de8049f-d511-46f4-aaa0-85336818a468-000000/ulGmcBG4yR2omv5AVOT4I2dG-DKrG2f1Rm-ZM7raCf4=434", "summary": "Agents that don't suck (Sponsor) With Agent Bricks, you can build agents that are accurate, reliable and grounded in your unique data. Now you have a clear path to production. Build agents that work.See why it's worth your time", "source": "tldr", "AI": {"tldr": "Agent Bricks\u5e73\u53f0\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u6784\u5efa\u51c6\u786e\u3001\u53ef\u9760\u3001\u57fa\u4e8e\u7279\u5b9a\u6570\u636e\u7684AI\u667a\u80fd\u4f53\uff0c\u5e76\u63d0\u4f9b\u6e05\u6670\u7684\u751f\u4ea7\u90e8\u7f72\u8def\u5f84", "motivation": "\u5f53\u524dAI\u667a\u80fd\u4f53\u5f00\u53d1\u9762\u4e34\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u4e0d\u8db3\u4ee5\u53ca\u96be\u4ee5\u751f\u4ea7\u90e8\u7f72\u7684\u95ee\u9898\uff0c\u9700\u8981\u5de5\u5177\u6765\u6784\u5efa\u771f\u6b63\u53ef\u7528\u7684\u667a\u80fd\u4f53", "method": "\u63d0\u4f9bAgent Bricks\u5e73\u53f0\uff0c\u652f\u6301\u57fa\u4e8e\u7279\u5b9a\u6570\u636e\u6784\u5efa\u667a\u80fd\u4f53\uff0c\u786e\u4fdd\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u4f9b\u751f\u4ea7\u90e8\u7f72\u89e3\u51b3\u65b9\u6848", "result": "\u5f00\u53d1\u8005\u80fd\u591f\u6784\u5efa\u51fa\"\u4e0d\u7cdf\u7cd5\"\u7684\u667a\u80fd\u4f53\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u51c6\u786e\u3001\u53ef\u9760\u3001\u53ef\u90e8\u7f72\u5230\u751f\u4ea7\u73af\u5883", "conclusion": "Agent Bricks\u4e3aAI\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u503c\u5f97\u5f00\u53d1\u8005\u6295\u5165\u65f6\u95f4\u5b66\u4e60\u548c\u4f7f\u7528", "topic": "code agent"}}
{"id": "tldr.2512.4f5d539b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinalderson.com%2Fposts%2Fhas-the-cost-of-software-just-dropped-90-percent%2F%3Futm_source=tldrdev/1/0100019b0303d3e5-f633ea4b-5ad9-4108-a61f-87f3396e029d-000000/_IId5Q4JEnGREvwlp0g90zJfX6c3Qye6yGMkPWdOm4c=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinalderson.com%2Fposts%2Fhas-the-cost-of-software-just-dropped-90-percent%2F%3Futm_source=tldrdev/1/0100019b0303d3e5-f633ea4b-5ad9-4108-a61f-87f3396e029d-000000/_IId5Q4JEnGREvwlp0g90zJfX6c3Qye6yGMkPWdOm4c=434", "authors": ["TLDR Newsletter"], "title": "Has the cost of building software just dropped 90%?", "comment": "Source: TLDR Newsletter, Date: 2025-12-09, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinalderson.com%2Fposts%2Fhas-the-cost-of-software-just-dropped-90-percent%2F%3Futm_source=tldrdev/1/0100019b0303d3e5-f633ea4b-5ad9-4108-a61f-87f3396e029d-000000/_IId5Q4JEnGREvwlp0g90zJfX6c3Qye6yGMkPWdOm4c=434", "summary": "Has the cost of building software just dropped 90%? (8 minute read) Agentic coding with AI is poised to drop the cost of building software by up to 90%. This reduction is due to AI agents automating traditionally labor-intensive tasks like testing, API development, and infrastructure setup, reducing implementation time and project overhead. This shift is expected to increase latent demand for software.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u7f16\u7801\u53ef\u5c06\u8f6f\u4ef6\u5f00\u53d1\u6210\u672c\u964d\u4f4e90%\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d4b\u8bd5\u3001API\u5f00\u53d1\u548c\u57fa\u7840\u8bbe\u65bd\u8bbe\u7f6e\u7b49\u4efb\u52a1\u6765\u5b9e\u73b0", "motivation": "\u63a2\u8ba8AI\u4ee3\u7406\u5982\u4f55\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u7edf\u52b3\u52a8\u5bc6\u96c6\u578b\u4efb\u52a1\u6765\u5927\u5e45\u964d\u4f4e\u8f6f\u4ef6\u5f00\u53d1\u6210\u672c\uff0c\u5e76\u9884\u6d4b\u8fd9\u5c06\u589e\u52a0\u8f6f\u4ef6\u5f00\u53d1\u7684\u6f5c\u5728\u9700\u6c42", "method": "\u57fa\u4e8eAI\u4ee3\u7406\u7684\u7f16\u7801\u65b9\u6cd5\uff0c\u81ea\u52a8\u5316\u6d4b\u8bd5\u3001API\u5f00\u53d1\u548c\u57fa\u7840\u8bbe\u65bd\u8bbe\u7f6e\u7b49\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1", "result": "\u9884\u8ba1\u8f6f\u4ef6\u5f00\u53d1\u6210\u672c\u53ef\u964d\u4f4e\u9ad8\u8fbe90%\uff0c\u540c\u65f6\u51cf\u5c11\u5b9e\u65bd\u65f6\u95f4\u548c\u9879\u76ee\u5f00\u9500", "conclusion": "AI\u4ee3\u7406\u7f16\u7801\u5c06\u663e\u8457\u964d\u4f4e\u8f6f\u4ef6\u5f00\u53d1\u6210\u672c\uff0c\u5e76\u53ef\u80fd\u589e\u52a0\u8f6f\u4ef6\u5f00\u53d1\u7684\u6f5c\u5728\u5e02\u573a\u9700\u6c42", "topic": "code agent"}}
{"id": "tldr.2512.7d64ba4e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcloudflare%2Fvibesdk%3Futm_source=tldrdev/1/0100019b0303d3e5-f633ea4b-5ad9-4108-a61f-87f3396e029d-000000/8NQbyXi9Txe7-mtR8heKhoA4YiJSxymZdcXcoLFn6yg=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcloudflare%2Fvibesdk%3Futm_source=tldrdev/1/0100019b0303d3e5-f633ea4b-5ad9-4108-a61f-87f3396e029d-000000/8NQbyXi9Txe7-mtR8heKhoA4YiJSxymZdcXcoLFn6yg=434", "authors": ["TLDR Newsletter"], "title": "VibeSDK", "comment": "Source: TLDR Newsletter, Date: 2025-12-09, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcloudflare%2Fvibesdk%3Futm_source=tldrdev/1/0100019b0303d3e5-f633ea4b-5ad9-4108-a61f-87f3396e029d-000000/8NQbyXi9Txe7-mtR8heKhoA4YiJSxymZdcXcoLFn6yg=434", "summary": "VibeSDK (GitHub Repo) Cloudflare VibeSDK is an open-source, full-stack AI web application generator that allows users to create and deploy applications using natural language prompts. It has AI code generation with error correction, live previews in sandboxed containers, interactive chat, and one-click deployment to Cloudflare Workers for Platforms.", "source": "tldr", "AI": {"tldr": "VibeSDK\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5168\u6808AI Web\u5e94\u7528\u751f\u6210\u5668\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u521b\u5efa\u548c\u90e8\u7f72\u5e94\u7528\uff0c\u5177\u6709AI\u4ee3\u7801\u751f\u6210\u3001\u9519\u8bef\u4fee\u6b63\u3001\u6c99\u7bb1\u5bb9\u5668\u5b9e\u65f6\u9884\u89c8\u3001\u4ea4\u4e92\u5f0f\u804a\u5929\u548c\u4e00\u952e\u90e8\u7f72\u5230Cloudflare Workers\u7b49\u529f\u80fd\u3002", "motivation": "\u7b80\u5316Web\u5e94\u7528\u5f00\u53d1\u6d41\u7a0b\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5feb\u901f\u521b\u5efa\u548c\u90e8\u7f72\u5168\u6808\u5e94\u7528\uff0c\u964d\u4f4e\u5f00\u53d1\u95e8\u69db\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "method": "\u57fa\u4e8eAI\u4ee3\u7801\u751f\u6210\u6280\u672f\uff0c\u7ed3\u5408\u9519\u8bef\u4fee\u6b63\u673a\u5236\uff0c\u5728\u6c99\u7bb1\u5bb9\u5668\u4e2d\u63d0\u4f9b\u5b9e\u65f6\u9884\u89c8\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u804a\u5929\u8f85\u52a9\u5f00\u53d1\uff0c\u5e76\u96c6\u6210\u4e00\u952e\u90e8\u7f72\u5230Cloudflare Workers\u5e73\u53f0\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684AI\u9a71\u52a8Web\u5e94\u7528\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u751f\u6210\u53ef\u90e8\u7f72\u7684\u5e94\u7528\u4ee3\u7801\uff0c\u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u5f00\u53d1\u4f53\u9a8c\u3002", "conclusion": "VibeSDK\u5c55\u793a\u4e86AI\u8f85\u52a9\u5f00\u53d1\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u7b80\u5316Web\u5e94\u7528\u5f00\u53d1\u6d41\u7a0b\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u4e13\u6ce8\u4e8e\u521b\u610f\u800c\u975e\u5b9e\u73b0\u7ec6\u8282\u3002", "topic": "code agent"}}
{"id": "tldr.2512.f9ee569a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgricha.dev%2Fblog%2Fthe-highest-quality-codebase%3Futm_source=tldrdev/1/0100019b0303d3e5-f633ea4b-5ad9-4108-a61f-87f3396e029d-000000/yQuA6fEK7-EmJJhuXhtH6V-PxTYM-DtFU-18gkuURhw=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgricha.dev%2Fblog%2Fthe-highest-quality-codebase%3Futm_source=tldrdev/1/0100019b0303d3e5-f633ea4b-5ad9-4108-a61f-87f3396e029d-000000/yQuA6fEK7-EmJJhuXhtH6V-PxTYM-DtFU-18gkuURhw=434", "authors": ["TLDR Newsletter"], "title": "The highest quality codebase", "comment": "Source: TLDR Newsletter, Date: 2025-12-09, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgricha.dev%2Fblog%2Fthe-highest-quality-codebase%3Futm_source=tldrdev/1/0100019b0303d3e5-f633ea4b-5ad9-4108-a61f-87f3396e029d-000000/yQuA6fEK7-EmJJhuXhtH6V-PxTYM-DtFU-18gkuURhw=434", "summary": "The highest quality codebase (9 minute read) This developer ran an experiment where they looped an agent over a codebase for 36 hours to see what it did. The experiment resulted in more code to be maintained, most of it largely useless. The agent added tons of tests, but removed the tests that mattered the most. The resulting app still worked, but with a few new bugs.", "source": "tldr", "AI": {"tldr": "\u5f00\u53d1\u8005\u5728\u4ee3\u7801\u5e93\u4e0a\u5faa\u73af\u8fd0\u884cAI\u4ee3\u740636\u5c0f\u65f6\uff0c\u7ed3\u679c\u4ea7\u751f\u5927\u91cf\u9700\u8981\u7ef4\u62a4\u7684\u65e0\u7528\u4ee3\u7801\uff0c\u6dfb\u52a0\u4e86\u8bb8\u591a\u6d4b\u8bd5\u4f46\u79fb\u9664\u4e86\u6700\u91cd\u8981\u7684\u6d4b\u8bd5\uff0c\u5e94\u7528\u4ecd\u80fd\u8fd0\u884c\u4f46\u5f15\u5165\u4e86\u65b0bug", "motivation": "\u63a2\u7d22AI\u4ee3\u7406\u5728\u4ee3\u7801\u5e93\u4e0a\u957f\u65f6\u95f4\u8fd0\u884c\u7684\u5b9e\u9645\u6548\u679c\uff0c\u4e86\u89e3AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u771f\u5b9e\u8868\u73b0\u548c\u6f5c\u5728\u95ee\u9898", "method": "\u5b9e\u9a8c\u65b9\u6cd5\uff1a\u5728\u4ee3\u7801\u5e93\u4e0a\u5faa\u73af\u8fd0\u884cAI\u4ee3\u740636\u5c0f\u65f6\uff0c\u89c2\u5bdf\u5176\u884c\u4e3a\u548c\u5bf9\u4ee3\u7801\u5e93\u7684\u5f71\u54cd", "result": "\u7ed3\u679c\uff1a1) \u4ea7\u751f\u5927\u91cf\u9700\u8981\u7ef4\u62a4\u7684\u65e0\u7528\u4ee3\u7801\uff1b2) \u6dfb\u52a0\u4e86\u8bb8\u591a\u6d4b\u8bd5\u4f46\u79fb\u9664\u4e86\u6700\u91cd\u8981\u7684\u6d4b\u8bd5\uff1b3) \u5e94\u7528\u4ecd\u80fd\u8fd0\u884c\u4f46\u5f15\u5165\u4e86\u65b0bug\uff1b4) \u6574\u4f53\u4ee3\u7801\u8d28\u91cf\u4e0b\u964d", "conclusion": "AI\u4ee3\u7406\u5728\u65e0\u4eba\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u957f\u65f6\u95f4\u8fd0\u884c\u4f1a\u4ea7\u751f\u8d1f\u9762\u6548\u679c\uff0c\u867d\u7136\u80fd\u751f\u6210\u4ee3\u7801\u4f46\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u8d28\u91cf\u548c\u91cd\u8981\u6027\u7684\u7406\u89e3\uff0c\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u548c\u6307\u5bfc", "topic": "agent analysis"}}
{"id": "wechat.2512.87198654", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNTcyOTExNQ==&mid=2247486385&idx=1&sn=1e55d3f0d0bf9aae85d37e0a7f882220&chksm=c094c0e050ffec069a6bc15f6dcb6e1f2766ea6eb806aaa761d2840b6c6d087801a4afb4ed8c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNTcyOTExNQ==&mid=2247486385&idx=1&sn=1e55d3f0d0bf9aae85d37e0a7f882220&chksm=c094c0e050ffec069a6bc15f6dcb6e1f2766ea6eb806aaa761d2840b6c6d087801a4afb4ed8c#rd", "authors": ["AI\u79d1\u7814\u8fdb\u9636\u793e"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7528\u5728\u9884\u8bad\u7ec3\uff0c\u6548\u679c\u53ef\u67e5\uff01", "comment": "Source: WeChat, Published: 2025-12-10 12:00:25", "summary": "\u4e3b\u8981\u5185\u5bb9\uff1a\u9488\u5bf9\u8de8\u57df\u5f3a\u5316\u5b66\u4e60\u9884\u8bad\u7ec3\u96be\u9898\uff0c\u63d0\u51fa CRPTpro \u6846\u67b6\u3002\u5b83\u901a\u8fc7\u89e3\u8026\u968f\u673a\u91c7\u96c6\u751f\u6210\u8de8\u57df\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u642d\u914d\u9ad8\u6548\u539f\u578b\u81ea\u76d1\u7763\u7b97\u6cd5\u8bad\u7ec3\u901a\u7528\u89c6\u89c9\u7f16\u7801\u5668\u3002", "AI": {"tldr": "\u4e3b\u8981\u5185\u5bb9\uff1a\u9488\u5bf9\u8de8\u57df\u5f3a\u5316\u5b66\u4e60\u9884\u8bad\u7ec3\u96be\u9898\uff0c\u63d0\u51fa CRPTpro \u6846\u67b6\u3002\u5b83\u901a\u8fc7\u89e3\u8026\u968f\u673a\u91c7\u96c6\u751f\u6210\u8de8\u57df\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u642d\u914d\u9ad8\u6548\u539f\u578b\u81ea\u76d1\u7763\u7b97\u6cd5\u8bad\u7ec3\u901a\u7528\u89c6\u89c9\u7f16\u7801\u5668\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.70743224", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5MTA0NzYxMQ==&mid=2247512010&idx=1&sn=2192dc464c351b296a99439e3ff24c57&chksm=97a03a3a8ceca1a8da6a78a793c4adc9a920e090ef3899a769cfc539b215d194a272dbd27fc7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5MTA0NzYxMQ==&mid=2247512010&idx=1&sn=2192dc464c351b296a99439e3ff24c57&chksm=97a03a3a8ceca1a8da6a78a793c4adc9a920e090ef3899a769cfc539b215d194a272dbd27fc7#rd", "authors": ["\u667a\u7329\u7329"], "title": "\u6df1\u6252PI \u03c0*0.6\u8fed\u4ee3\u5f0f<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u601d\u8def\u7684\u6765\u6e90\uff1aVLA+\u5728\u7ebfRL\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u7684\u81ea\u6211\u8fdb\u5316", "comment": "Source: WeChat, Published: 2025-12-10 11:59:48", "summary": "\u7b2c\u4e00\u9636\u6bb5\uff1a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08\u63a2\u7d22\u4e0e\u53d1\u73b0\uff09\u56fe\u6ce8\uff1a\u7a33\u5b9a\u63a2\u7d22\u5728\u8fd9\u4e2a\u9636\u6bb5\uff0c\u673a\u5668\u4eba\u7684\u76ee\u6807\u662f\u53bb\u8bd5\u9519\uff0c\u63a2\u7d22\u5982\u4f55\u5b8c\u6210\u65b0\u4efb\u52a1\u3002\u51bb\u7ed3\u5927\u8111\uff08Freeze VLM\uff09\uff1a\u4e3a\u4e86\u9632\u6b62\u6a21\u578b\u5d29\u6e83\u548c\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u4f5c\u8005\u51bb\u7ed3\u4e86\u5de8\u5927\u7684 VLM \u4e3b\u5e72\u53c2\u6570\u3002", "AI": {"tldr": "\u7b2c\u4e00\u9636\u6bb5\uff1a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08\u63a2\u7d22\u4e0e\u53d1\u73b0\uff09\u56fe\u6ce8\uff1a\u7a33\u5b9a\u63a2\u7d22\u5728\u8fd9\u4e2a\u9636\u6bb5\uff0c\u673a\u5668\u4eba\u7684\u76ee\u6807\u662f\u53bb\u8bd5\u9519\uff0c\u63a2\u7d22\u5982\u4f55\u5b8c\u6210\u65b0\u4efb\u52a1\u3002\u51bb\u7ed3\u5927\u8111\uff08Freeze VLM\uff09\uff1a\u4e3a\u4e86\u9632\u6b62\u6a21\u578b\u5d29\u6e83\u548c\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u4f5c\u8005\u51bb\u7ed3\u4e86\u5de8\u5927\u7684 VLM \u4e3b\u5e72\u53c2\u6570\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.d4a18356", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMzMwMjU3OA==&mid=2247483837&idx=1&sn=b9f7d148ec9af4635b3e1a8d1ffe6331&chksm=e9db621c1aad28cf7251cfa7abf26997ca26aab6760c8a2ff3c0e1039106a5da2eeff559dd9e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMzMwMjU3OA==&mid=2247483837&idx=1&sn=b9f7d148ec9af4635b3e1a8d1ffe6331&chksm=e9db621c1aad28cf7251cfa7abf26997ca26aab6760c8a2ff3c0e1039106a5da2eeff559dd9e#rd", "authors": ["\u777f\u9020\u672a\u6765LAB"], "title": "JMS | \u57fa\u4e8e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u591a\u9636\u6bb5\u88c5\u914d\u8fc7\u7a0b\u53c2\u6570\u5728\u7ebf\u534f\u540c\u4f18\u5316", "comment": "Source: WeChat, Published: 2025-12-10 10:44:18", "summary": "\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u53ef\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u6301\u7eed\u4ea4\u4e92\u8bd5\u9519\uff0c\u5b66\u4e60\u4e00\u9879\u80fd\u6700\u5927\u5316\u957f\u671f\u7d2f\u79ef\u56de\u62a5\uff08\u5373\u6700\u7ec8\u9ad8\u8d28\u91cf\u4ea7\u54c1\uff09\u7684\u7b56\u7565\u3002\u7136\u800c\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u5e94\u7528\u4e8e\u591a\u9636\u6bb5\u88c5\u914d\u53c2\u6570\u4f18\u5316\uff0c\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u5176\u96be\u4ee5\u6536\u655b\u81f3\u6709\u6548\u7b56\u7565\uff1a", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u53ef\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u6301\u7eed\u4ea4\u4e92\u8bd5\u9519\uff0c\u5b66\u4e60\u4e00\u9879\u80fd\u6700\u5927\u5316\u957f\u671f\u7d2f\u79ef\u56de\u62a5\uff08\u5373\u6700\u7ec8\u9ad8\u8d28\u91cf\u4ea7\u54c1\uff09\u7684\u7b56\u7565\u3002\u7136\u800c\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u5e94\u7528\u4e8e\u591a\u9636\u6bb5\u88c5\u914d\u53c2\u6570\u4f18\u5316\uff0c\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u5176\u96be\u4ee5\u6536\u655b\u81f3\u6709\u6548\u7b56\u7565\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.20e20ed5", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwODY3ODc0MQ==&mid=2247483748&idx=1&sn=2111f8e578f485add9e053329e166c5c&chksm=c1af0eed0f4907d9ec298c634a71bd4c31e02dac593626ac35658e85b88fcdf9f28b6eed249c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwODY3ODc0MQ==&mid=2247483748&idx=1&sn=2111f8e578f485add9e053329e166c5c&chksm=c1af0eed0f4907d9ec298c634a71bd4c31e02dac593626ac35658e85b88fcdf9f28b6eed249c#rd", "authors": ["\u667a\u80fd\u539f\u59cb\u4eba"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u600e\u4e48\u73a9\u8001\u864e\u673a", "comment": "Source: WeChat, Published: 2025-12-10 08:49:28", "summary": "\uff08\u8fd9\u4e2a\u5c31\u628a\u5f3a\u5316\u5b66\u4e60\u548c\u673a\u5668\u5b66\u4e60\u533a\u5206\u5f00\u4e86\uff0c\u673a\u5668\u5b66\u4e60\u4e0d\u7ba1\u6709\u76d1\u7763\u65e0\u76d1\u7763\uff0c\u6570\u636e\u5206\u5e03\u5df2\u77e5\uff0c\u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u9762\u5bf9\u672a\u77e5\u7684\u73af\u5883\u72b6\u6001\uff0c\u901a\u8fc7\u6bcf\u4e00\u6b21\u52a8\u4f5c\u83b7\u5f97\u548c\u73af\u5883\u4ea4\u4e92\u7684\u7ed3\u679c\uff0c\u8bc4\u4f30\u548c\u8bad\u7ec3\u5185\u5728\u7684\u4ef7\u503c\u51fd\u6570or\u7f51\u7edc\uff09", "AI": {"tldr": "\uff08\u8fd9\u4e2a\u5c31\u628a\u5f3a\u5316\u5b66\u4e60\u548c\u673a\u5668\u5b66\u4e60\u533a\u5206\u5f00\u4e86\uff0c\u673a\u5668\u5b66\u4e60\u4e0d\u7ba1\u6709\u76d1\u7763\u65e0\u76d1\u7763\uff0c\u6570\u636e\u5206\u5e03\u5df2\u77e5\uff0c\u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u9762\u5bf9\u672a\u77e5\u7684\u73af\u5883\u72b6\u6001\uff0c\u901a\u8fc7\u6bcf\u4e00\u6b21\u52a8\u4f5c\u83b7\u5f97\u548c\u73af\u5883\u4ea4\u4e92\u7684\u7ed3\u679c\uff0c\u8bc4\u4f30\u548c\u8bad\u7ec3\u5185\u5728\u7684\u4ef7\u503c\u51fd\u6570or\u7f51\u7edc\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.3ad7250f", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0NDMyODUxMA==&mid=2247488921&idx=1&sn=874c5c500612ae94b2a34b4d3a3d76a7&chksm=e8d4ced3918c7fac2d3bdb902795b7d03fd83df9d8d09160562e569eed92ea414c795db89487#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0NDMyODUxMA==&mid=2247488921&idx=1&sn=874c5c500612ae94b2a34b4d3a3d76a7&chksm=e8d4ced3918c7fac2d3bdb902795b7d03fd83df9d8d09160562e569eed92ea414c795db89487#rd", "authors": ["\u8c03\u5ea6\u4e0e\u4f18\u5316\u7b97\u6cd5\u7684\u96c6\u7ed3\u5730"], "title": "\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u521b\u65b0\u70b9\u603b\u7ed3", "comment": "Source: WeChat, Published: 2025-12-10 06:23:53", "summary": "\u514d\u8d39\u83b7\u53d6\u5168\u90e8\u8bba\u6587+\u5f00\u6e90\u4ee3\u7801 \u5f3a\u5316\u5b66\u4e60+\u5927\u6a21\u578b \u73b0\u5728\u4e0e\u5927\u6a21\u578b\u7ed3\u5408\u5728\u9876\u4f1a\uff08NeurIPS/ICLR/ICML\uff09\u4e0a\u5c5e\u4e8e\u201c\u6d41\u91cf\u5bc6\u7801\u201d\uff0c\u65e0\u8bba\u662f\u5c06RL\u7528\u4e8e\u5bf9\u9f50\u5fae\u8c03\uff08\u6bd4\u5982RLHF\uff09\u3001agent\u51b3\u7b56\u89c4\u5212\uff0c\u8fd8\u662f\u7528LLM\u751f\u6210\u5956\u52b1\u51fd\u6570/\u73af\u5883\uff0c\u90fd\u5bb9\u6613\u4ea7\u751fnovelty\u3002", "AI": {"tldr": "\u514d\u8d39\u83b7\u53d6\u5168\u90e8\u8bba\u6587+\u5f00\u6e90\u4ee3\u7801 \u5f3a\u5316\u5b66\u4e60+\u5927\u6a21\u578b \u73b0\u5728\u4e0e\u5927\u6a21\u578b\u7ed3\u5408\u5728\u9876\u4f1a\uff08NeurIPS/ICLR/ICML\uff09\u4e0a\u5c5e\u4e8e\u201c\u6d41\u91cf\u5bc6\u7801\u201d\uff0c\u65e0\u8bba\u662f\u5c06RL\u7528\u4e8e\u5bf9\u9f50\u5fae\u8c03\uff08\u6bd4\u5982RLHF\uff09\u3001agent\u51b3\u7b56\u89c4\u5212\uff0c\u8fd8\u662f\u7528LLM\u751f\u6210\u5956\u52b1\u51fd\u6570/\u73af\u5883\uff0c\u90fd\u5bb9\u6613\u4ea7\u751fnovelty\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.4258155d", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyNTY1MDUwNQ==&mid=2247502512&idx=1&sn=03642913b09f38341d97e2c0e9467885&chksm=e98032c3755155c4bdbe658b06eebd18e1018457ff1299bd68e3db35a6cbc1fbd23f9621efee#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyNTY1MDUwNQ==&mid=2247502512&idx=1&sn=03642913b09f38341d97e2c0e9467885&chksm=e98032c3755155c4bdbe658b06eebd18e1018457ff1299bd68e3db35a6cbc1fbd23f9621efee#rd", "authors": ["DASOU"], "title": "2026\u5e74ALL in <em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff01\uff01", "comment": "Source: WeChat, Published: 2025-12-10 03:15:30", "summary": "\u514d\u8d39\u83b7\u53d6\u5168\u90e8\u8bba\u6587+\u5f00\u6e90\u4ee3\u7801 \u5f3a\u5316\u5b66\u4e60+\u5927\u6a21\u578b \u73b0\u5728\u4e0e\u5927\u6a21\u578b\u7ed3\u5408\u5728\u9876\u4f1a\uff08NeurIPS/ICLR/ICML\uff09\u4e0a\u5c5e\u4e8e\u201c\u6d41\u91cf\u5bc6\u7801\u201d\uff0c\u65e0\u8bba\u662f\u5c06RL\u7528\u4e8e\u5bf9\u9f50\u5fae\u8c03\uff08\u6bd4\u5982RLHF\uff09\u3001agent\u51b3\u7b56\u89c4\u5212\uff0c\u8fd8\u662f\u7528LLM\u751f\u6210\u5956\u52b1\u51fd\u6570/\u73af\u5883\uff0c\u90fd\u5bb9\u6613\u4ea7\u751fnovelty\u3002", "AI": {"tldr": "\u514d\u8d39\u83b7\u53d6\u5168\u90e8\u8bba\u6587+\u5f00\u6e90\u4ee3\u7801 \u5f3a\u5316\u5b66\u4e60+\u5927\u6a21\u578b \u73b0\u5728\u4e0e\u5927\u6a21\u578b\u7ed3\u5408\u5728\u9876\u4f1a\uff08NeurIPS/ICLR/ICML\uff09\u4e0a\u5c5e\u4e8e\u201c\u6d41\u91cf\u5bc6\u7801\u201d\uff0c\u65e0\u8bba\u662f\u5c06RL\u7528\u4e8e\u5bf9\u9f50\u5fae\u8c03\uff08\u6bd4\u5982RLHF\uff09\u3001agent\u51b3\u7b56\u89c4\u5212\uff0c\u8fd8\u662f\u7528LLM\u751f\u6210\u5956\u52b1\u51fd\u6570/\u73af\u5883\uff0c\u90fd\u5bb9\u6613\u4ea7\u751fnovelty\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.adada34c", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4MDY3MDAxMQ==&mid=2247486483&idx=1&sn=fc9bdebca4bce1352ac9fe5eeba04da2&chksm=ea5414ff2719b22b7107001c206fcf2ac076dc25ee1a22d0691c1cd408e9f1fe6098559e9c27#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4MDY3MDAxMQ==&mid=2247486483&idx=1&sn=fc9bdebca4bce1352ac9fe5eeba04da2&chksm=ea5414ff2719b22b7107001c206fcf2ac076dc25ee1a22d0691c1cd408e9f1fe6098559e9c27#rd", "authors": ["\u5357\u6781\u661f\u533b\u5b66AI\u7b14\u8bb0"], "title": "Agent Lightning\uff1a\u7528<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u8bad\u7ec3\u4efb\u610fAI\u667a\u80fd\u4f53", "comment": "Source: WeChat, Published: 2025-12-10 02:08:24", "summary": "\u73b0\u6709\u7684\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e0e\u6846\u67b6\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u3001\u5355\u6b21\u8c03\u7528\u7684\u4efb\u52a1\uff0c\u4f8b\u5982\u504f\u597d\u5bf9\u9f50\u6216\u6570\u5b66\u63a8\u7406\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u667a\u80fd\u4f53\u5c55\u73b0\u51fa\u8d85\u8d8a\u6b64\u7c7b\u573a\u666f\u7684\u590d\u6742\u6027\u4e0e\u591a\u6837\u6027\u3002", "AI": {"tldr": "\u73b0\u6709\u7684\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e0e\u6846\u67b6\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u3001\u5355\u6b21\u8c03\u7528\u7684\u4efb\u52a1\uff0c\u4f8b\u5982\u504f\u597d\u5bf9\u9f50\u6216\u6570\u5b66\u63a8\u7406\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u667a\u80fd\u4f53\u5c55\u73b0\u51fa\u8d85\u8d8a\u6b64\u7c7b\u573a\u666f\u7684\u590d\u6742\u6027\u4e0e\u591a\u6837\u6027\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.71d3d2b5", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyMTA1NjI5OQ==&mid=2247491581&idx=1&sn=82203fec1f62e18829376e95f48eca34&chksm=f88023191d7d14e97b19606e63146d320eafdaa0a6df7640aa3d8f7a6cb96c87b4fa86205b0d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyMTA1NjI5OQ==&mid=2247491581&idx=1&sn=82203fec1f62e18829376e95f48eca34&chksm=f88023191d7d14e97b19606e63146d320eafdaa0a6df7640aa3d8f7a6cb96c87b4fa86205b0d#rd", "authors": ["\u667a\u6167\u80fd\u6e90\u7814\u7a76\u6240"], "title": "\u3010\u9876\u520a\u89e3\u8bfb\u3011\u5927\u89c4\u6a21\u7f51\u7edc\u63a7\u5236\u7684\u9ad8\u6548\u53ef\u6269\u5c55<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-12-09 14:30:18", "summary": "\u5927\u89c4\u6a21\u7f51\u7edc\u63a7\u5236\u7684\u9ad8\u6548\u53ef\u6269\u5c55\u5f3a\u5316\u5b66\u4e60 \u8bba\u6587\u6df1\u5ea6\u89e3\u8bfb \u8be5\u7814\u7a76\u7531\u5317\u4eac\u5927\u5b66\u548c\u4f26\u6566\u56fd\u738b\u5b66\u9662\u7684\u56e2\u961f\u5408\u4f5c\u5b8c\u6210\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u901a\u4fe1\u9ad8\u6548\u3001\u6837\u672c\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e3a\u6784\u5efa\u5927\u89c4\u6a21AI\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7406\u8bba\u4e0e\u6280\u672f\u57fa\u7840", "AI": {"tldr": "\u5927\u89c4\u6a21\u7f51\u7edc\u63a7\u5236\u7684\u9ad8\u6548\u53ef\u6269\u5c55\u5f3a\u5316\u5b66\u4e60 \u8bba\u6587\u6df1\u5ea6\u89e3\u8bfb \u8be5\u7814\u7a76\u7531\u5317\u4eac\u5927\u5b66\u548c\u4f26\u6566\u56fd\u738b\u5b66\u9662\u7684\u56e2\u961f\u5408\u4f5c\u5b8c\u6210\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u901a\u4fe1\u9ad8\u6548\u3001\u6837\u672c\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e3a\u6784\u5efa\u5927\u89c4\u6a21AI\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7406\u8bba\u4e0e\u6280\u672f\u57fa\u7840", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.a7a2c470", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3Mzc4MTg4Mg==&mid=2453984597&idx=1&sn=a5f86acf9814947b020c1f5fa71afedf&chksm=8918a5c8d3b1b4ceb9381447602dba10b1fd981a0ff8c986c83e7975e5940be498f6479c2016#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3Mzc4MTg4Mg==&mid=2453984597&idx=1&sn=a5f86acf9814947b020c1f5fa71afedf&chksm=8918a5c8d3b1b4ceb9381447602dba10b1fd981a0ff8c986c83e7975e5940be498f6479c2016#rd", "authors": ["\u6d45\u5c1d\u5927\u6a21\u578b"], "title": "<em class=\"highlight\">Agentic</em>\u539f\u751f\u5927\u6a21\u578b\u5bf9\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u5f71\u54cd", "comment": "Source: WeChat, Published: 2025-12-10 13:34:52", "summary": "agentic\u539f\u751f\u5927\u6a21\u578b\u7684\u672c\u8d28\u53d8\u5316\u662f\u8bb0\u4f4f\u6216rollout\u4e86\u5f88\u591a\u9760\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u624d\u80fd\u89e3\u51b3\u7684\u590d\u6742\u4efb\u52a1\u8f68\u8ff9\uff0c\u8fd9\u4e9b\u77e5\u8bc6\u88ab\u5b58\u50a8\u5230\u4e86\u6a21\u578b\u7684\u6743\u91cd\u4e2d\u3002\u8fd9\u5957\u6743\u91cd\u4f1a\u81ea\u7136\u800c\u7136\u5730\u9f13\u52b1\u6a21\u578b\u53bb\u751f\u6210\u9ad8\u8d28\u91cf\u7684ReAct\u5faa\u73af\uff0c\u56e0\u6b64\u5185\u5316\u4e86\u4e4b\u524d\u8981\u9760prompt\u6765\u7ef4\u6301\u7684\u51e0\u4e2a", "AI": {"tldr": "agentic\u539f\u751f\u5927\u6a21\u578b\u7684\u672c\u8d28\u53d8\u5316\u662f\u8bb0\u4f4f\u6216rollout\u4e86\u5f88\u591a\u9760\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u624d\u80fd\u89e3\u51b3\u7684\u590d\u6742\u4efb\u52a1\u8f68\u8ff9\uff0c\u8fd9\u4e9b\u77e5\u8bc6\u88ab\u5b58\u50a8\u5230\u4e86\u6a21\u578b\u7684\u6743\u91cd\u4e2d\u3002\u8fd9\u5957\u6743\u91cd\u4f1a\u81ea\u7136\u800c\u7136\u5730\u9f13\u52b1\u6a21\u578b\u53bb\u751f\u6210\u9ad8\u8d28\u91cf\u7684ReAct\u5faa\u73af\uff0c\u56e0\u6b64\u5185\u5316\u4e86\u4e4b\u524d\u8981\u9760prompt\u6765\u7ef4\u6301\u7684\u51e0\u4e2a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.c40feb49", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk5MDAxMDk3Nw==&mid=2247483805&idx=1&sn=1b01116e65e161e7c972fae8fa81493f&chksm=c4dfd7344ce95a6203300d354b58ba56f11567a76aab09aa2bf7f823cf6e21ba77df96dc09c3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk5MDAxMDk3Nw==&mid=2247483805&idx=1&sn=1b01116e65e161e7c972fae8fa81493f&chksm=c4dfd7344ce95a6203300d354b58ba56f11567a76aab09aa2bf7f823cf6e21ba77df96dc09c3#rd", "authors": ["\u5b50\u51e1AI"], "title": "\u3010Google\u3011<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u5165\u95e8\u6307\u5357\uff08\u4e8c\uff09", "comment": "Source: WeChat, Published: 2025-12-10 13:14:17", "summary": "\u6211\u4eec\u53ef\u4ee5\u5c06 Agentic \u7cfb\u7edf\u5206\u4e3a\u51e0\u4e2a\u5e7f\u6cdb\u7684\u7ea7\u522b\uff0c\u6bcf\u4e00\u4e2a\u7ea7\u522b\u90fd\u5efa\u7acb\u5728\u524d\u4e00\u4e2a\u80fd\u529b\u4e4b\u4e0a\u3002Level 0\uff1a\u6838\u5fc3\u63a8\u7406\u7cfb\u7edf\u5728\u62e5\u6709 Agent \u4e4b\u524d\uff0c\u6211\u4eec\u5fc5\u987b\u4ece\u5176\u6700\u57fa\u672c\u5f62\u5f0f\u7684\u201c\u5927\u8111\u201d\u5f00\u59cb\uff1a\u63a8\u7406\u5f15\u64ce\u672c\u8eab\u3002", "AI": {"tldr": "\u6211\u4eec\u53ef\u4ee5\u5c06 Agentic \u7cfb\u7edf\u5206\u4e3a\u51e0\u4e2a\u5e7f\u6cdb\u7684\u7ea7\u522b\uff0c\u6bcf\u4e00\u4e2a\u7ea7\u522b\u90fd\u5efa\u7acb\u5728\u524d\u4e00\u4e2a\u80fd\u529b\u4e4b\u4e0a\u3002Level 0\uff1a\u6838\u5fc3\u63a8\u7406\u7cfb\u7edf\u5728\u62e5\u6709 Agent \u4e4b\u524d\uff0c\u6211\u4eec\u5fc5\u987b\u4ece\u5176\u6700\u57fa\u672c\u5f62\u5f0f\u7684\u201c\u5927\u8111\u201d\u5f00\u59cb\uff1a\u63a8\u7406\u5f15\u64ce\u672c\u8eab\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.34f318ac", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxNjA0MjIzNg==&mid=2650011159&idx=1&sn=e7da2052c12e2923ec29724ee4e7933a&chksm=8267e80b0e5705b82452714ae04dc08807403a7b533d8bf6ec79810b3bbc41f991a59dbde052#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxNjA0MjIzNg==&mid=2650011159&idx=1&sn=e7da2052c12e2923ec29724ee4e7933a&chksm=8267e80b0e5705b82452714ae04dc08807403a7b533d8bf6ec79810b3bbc41f991a59dbde052#rd", "authors": ["\u6a21\u5b89\u5c40"], "title": "\u5e94\u5bf9<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u52a8\u6001\u98ce\u9669\uff1a\u82f1\u4f1f\u8fbe<em class=\"highlight\">Agentic</em>\u5b89\u5168\u4e0e\u9632\u62a4\u6846\u67b6\u89e3\u6790", "comment": "Source: WeChat, Published: 2025-12-10 12:20:00", "summary": "Agentic\u7cfb\u7edf\u8fd0\u884c\u73af\u5883\u9ad8\u5ea6\u52a8\u6001\uff0c\u5176\u884c\u4e3a\u53d7\u77ed\u671f\u8bb0\u5fc6\u3001\u5916\u90e8\u8f93\u5165\u3001\u968f\u673a\u6027\u53ca\u975e\u786e\u5b9a\u6027\u8def\u5f84\u5f71\u54cd\uff0c\u5177\u6709\u4e0d\u53ef\u5b8c\u5168\u91cd\u590d\u7684\u7279\u6027\u3002\u8fd9\u4f7f\u5f97\u57fa\u4e8e\u9759\u6001\u89c4\u5219\u6216\u6279\u91cf\u6d4b\u8bd5\u7684\u4f20\u7edf\u5b89\u5168\u65b9\u6cd5\u9762\u4e34\u6311\u6218\u3002", "AI": {"tldr": "Agentic\u7cfb\u7edf\u8fd0\u884c\u73af\u5883\u9ad8\u5ea6\u52a8\u6001\uff0c\u5176\u884c\u4e3a\u53d7\u77ed\u671f\u8bb0\u5fc6\u3001\u5916\u90e8\u8f93\u5165\u3001\u968f\u673a\u6027\u53ca\u975e\u786e\u5b9a\u6027\u8def\u5f84\u5f71\u54cd\uff0c\u5177\u6709\u4e0d\u53ef\u5b8c\u5168\u91cd\u590d\u7684\u7279\u6027\u3002\u8fd9\u4f7f\u5f97\u57fa\u4e8e\u9759\u6001\u89c4\u5219\u6216\u6279\u91cf\u6d4b\u8bd5\u7684\u4f20\u7edf\u5b89\u5168\u65b9\u6cd5\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.346e2b4d", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwMTYwNzE2Mg==&mid=2651037139&idx=1&sn=42f47e0ad7ff49ba9d6b3046a93153bb&chksm=806d2b522336217388baac7e8c92e5b788a019e84f0113afe5290ecb250336840292a5d4ddc3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwMTYwNzE2Mg==&mid=2651037139&idx=1&sn=42f47e0ad7ff49ba9d6b3046a93153bb&chksm=806d2b522336217388baac7e8c92e5b788a019e84f0113afe5290ecb250336840292a5d4ddc3#rd", "authors": ["ASCE1885"], "title": "AAIF \u6210\u7acb\uff1a<em class=\"highlight\">Agentic</em> AI \u8fdb\u5165\u201c\u6807\u51c6\u5171\u5efa\u201d\u65f6\u4ee3", "comment": "Source: WeChat, Published: 2025-12-10 12:00:46", "summary": "\u8fd9\u6e05\u6670\u5730\u4f20\u9012\u51fa\u5176\u6839\u672c\u6027\u7684\u6218\u7565\u610f\u56fe\uff1a\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u8d85\u8d8a\u4efb\u4f55\u5355\u4e00\u5546\u4e1a\u5b9e\u4f53\u63a7\u5236\u7684\u4e2d\u7acb\u65b9\uff0c\u6765\u89e3\u51b3 Agentic AI \u9886\u57df\u65e5\u76ca\u4e25\u91cd\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u5e76\u4e3a\u6574\u4e2a\u751f\u6001\u7cfb\u7edf\u7684\u957f\u671f\u5065\u5eb7\u53d1\u5c55\u5960\u5b9a\u575a\u5b9e\u57fa\u7840\u3002", "AI": {"tldr": "\u8fd9\u6e05\u6670\u5730\u4f20\u9012\u51fa\u5176\u6839\u672c\u6027\u7684\u6218\u7565\u610f\u56fe\uff1a\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u8d85\u8d8a\u4efb\u4f55\u5355\u4e00\u5546\u4e1a\u5b9e\u4f53\u63a7\u5236\u7684\u4e2d\u7acb\u65b9\uff0c\u6765\u89e3\u51b3 Agentic AI \u9886\u57df\u65e5\u76ca\u4e25\u91cd\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u5e76\u4e3a\u6574\u4e2a\u751f\u6001\u7cfb\u7edf\u7684\u957f\u671f\u5065\u5eb7\u53d1\u5c55\u5960\u5b9a\u575a\u5b9e\u57fa\u7840\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.9101db9d", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MTczMTY3NQ==&mid=2651396192&idx=1&sn=248f3a5714d24313b58e1295bfa6b230&chksm=bc3618df451dff06df578d7197932ef63f05ac72765d73d35b31ab0898280a8cb61f40fbee7f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MTczMTY3NQ==&mid=2651396192&idx=1&sn=248f3a5714d24313b58e1295bfa6b230&chksm=bc3618df451dff06df578d7197932ef63f05ac72765d73d35b31ab0898280a8cb61f40fbee7f#rd", "authors": ["\u5c0f\u6854\u706f\u7f51"], "title": "\u68c0\u9a8c<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u5230\u6765\uff01\u4f1a\u7ed9IVD\u5e26\u6765\u4ec0\u4e48\u53d8\u5316\uff1f", "comment": "Source: WeChat, Published: 2025-12-10 09:59:54", "summary": "\u65e5\u524d\uff0c\u56fd\u5185\u9996\u4e2a\u4e13\u4e3a\u4e34\u5e8a\u68c0\u9a8c\u6253\u9020\u7684\u5782\u76f4\u9886\u57df\u5927\u6a21\u578b\u201c\u542f\u5143\u68c0\u9a8c\u5927\u6a21\u578b\u201d\u5728\u5e7f\u4e1c\u7701\u533b\u5b66\u4f1a\u68c0\u9a8c\u533b\u5b66\u5b66\u672f\u5e74\u4f1a\u671f\u95f4\u6b63\u5f0f\u53d1\u5e03\u3002\u8fd9\u4e00\u7531\u8fc8\u745e\u533b\u7597\u4e0e\u5357\u65b9\u533b\u79d1\u5927\u5b66\u6df1\u5733\u533b\u9662\u7b49\u673a\u6784\u5408\u4f5c\u5f00\u53d1\u7684\u4e13\u7528AI\u6a21\u578b\uff0c\u6807\u5fd7\u7740\u6211\u56fd\u68c0\u9a8c\u533b\u5b66\u667a\u80fd\u5316\u8fc8\u51fa", "AI": {"tldr": "\u65e5\u524d\uff0c\u56fd\u5185\u9996\u4e2a\u4e13\u4e3a\u4e34\u5e8a\u68c0\u9a8c\u6253\u9020\u7684\u5782\u76f4\u9886\u57df\u5927\u6a21\u578b\u201c\u542f\u5143\u68c0\u9a8c\u5927\u6a21\u578b\u201d\u5728\u5e7f\u4e1c\u7701\u533b\u5b66\u4f1a\u68c0\u9a8c\u533b\u5b66\u5b66\u672f\u5e74\u4f1a\u671f\u95f4\u6b63\u5f0f\u53d1\u5e03\u3002\u8fd9\u4e00\u7531\u8fc8\u745e\u533b\u7597\u4e0e\u5357\u65b9\u533b\u79d1\u5927\u5b66\u6df1\u5733\u533b\u9662\u7b49\u673a\u6784\u5408\u4f5c\u5f00\u53d1\u7684\u4e13\u7528AI\u6a21\u578b\uff0c\u6807\u5fd7\u7740\u6211\u56fd\u68c0\u9a8c\u533b\u5b66\u667a\u80fd\u5316\u8fc8\u51fa", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2512.2443a85e", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3MTczOTkxNg==&mid=2247486705&idx=1&sn=cdf9b3a214f1f9768646c0b01d535efa&chksm=cff19280ec9e660fcbd24a81fba8fc3e426d50174f25eeb793916b981c79eff35cfaab2190e7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3MTczOTkxNg==&mid=2247486705&idx=1&sn=cdf9b3a214f1f9768646c0b01d535efa&chksm=cff19280ec9e660fcbd24a81fba8fc3e426d50174f25eeb793916b981c79eff35cfaab2190e7#rd", "authors": ["AI4Physics"], "title": "\u3010\u5168\u9762\u8c03\u67e5\u3011<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7528\u4e8e\u79d1\u5b66\u53d1\u73b0", "comment": "Source: WeChat, Published: 2025-12-10 03:05:00", "summary": "\u8fc7\u53bb\u4e94\u5e74\uff0c\u5927\u6a21\u578b\u5728\u79d1\u7814\u4e2d\u7684\u89d2\u8272\u53d1\u751f\u4e86\u6839\u672c\u6027\u53d8\u5316\uff1a\u4ece\u6700\u521d\u7684\u201c\u5199\u70b9\u4ee3\u7801\u3001\u5e2e\u5fd9\u627e\u6587\u732e\u201d\u7684\u5de5\u5177\uff0c\u9010\u6b65\u8d70\u5411\u80fd\u81ea\u4e3b\u63d0\u51fa\u5047\u8bbe\u3001\u89c4\u5212\u5b9e\u9a8c\u4e43\u81f3\u64b0\u5199\u8bba\u6587\u7684\u201cAI \u79d1\u5b66\u5bb6\u201d\u3002", "AI": {"tldr": "\u8fc7\u53bb\u4e94\u5e74\uff0c\u5927\u6a21\u578b\u5728\u79d1\u7814\u4e2d\u7684\u89d2\u8272\u53d1\u751f\u4e86\u6839\u672c\u6027\u53d8\u5316\uff1a\u4ece\u6700\u521d\u7684\u201c\u5199\u70b9\u4ee3\u7801\u3001\u5e2e\u5fd9\u627e\u6587\u732e\u201d\u7684\u5de5\u5177\uff0c\u9010\u6b65\u8d70\u5411\u80fd\u81ea\u4e3b\u63d0\u51fa\u5047\u8bbe\u3001\u89c4\u5212\u5b9e\u9a8c\u4e43\u81f3\u64b0\u5199\u8bba\u6587\u7684\u201cAI \u79d1\u5b66\u5bb6\u201d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.5a4fa400", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MDc4NDAxNw==&mid=2449747683&idx=1&sn=ee26fb25abc895683578092145bb718e&chksm=89c4a7d080ed2bfea2c91b676b755255a3ae1386905d0f37a304db44921a59c53b90aa47f077#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MDc4NDAxNw==&mid=2449747683&idx=1&sn=ee26fb25abc895683578092145bb718e&chksm=89c4a7d080ed2bfea2c91b676b755255a3ae1386905d0f37a304db44921a59c53b90aa47f077#rd", "authors": ["AI\u7231\u8bf4\u8d22\u52a1"], "title": "2025\u5e7412\u6708\u4efdAI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6700\u65b0\u7248\u672c\u4ecb\u7ecd", "comment": "Source: WeChat, Published: 2025-12-10 00:11:10", "summary": "\u4e00\u3001\u4e3b\u6d41ai\u5927\u6a21\u578b\u6700\u65b0\u7248\u672c\u53f7\u4e0e\u6392\u540d\uff08\u622a\u81f32025\u5e7412\u67088\u65e5\uff09\u7efc\u5408\u5404\u9879\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MMLU-Pro\u3001GPQA\uff09\u3001\u5f00\u6e90\u793e\u533a\u6d3b\u8dc3\u5ea6\u3001\u7528\u6237\u8bc4\u6d4b\u53ca\u5e02\u573a\u5f71\u54cd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u4e3b\u6d41\u5927\u6a21\u578b\u8fdb\u884c\u4e00\u4e2a\u5927\u81f4\u7684\u7efc\u5408\u5b9e\u529b\u5206\u5c42\u6392\u540d\u3002", "AI": {"tldr": "\u4e00\u3001\u4e3b\u6d41ai\u5927\u6a21\u578b\u6700\u65b0\u7248\u672c\u53f7\u4e0e\u6392\u540d\uff08\u622a\u81f32025\u5e7412\u67088\u65e5\uff09\u7efc\u5408\u5404\u9879\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MMLU-Pro\u3001GPQA\uff09\u3001\u5f00\u6e90\u793e\u533a\u6d3b\u8dc3\u5ea6\u3001\u7528\u6237\u8bc4\u6d4b\u53ca\u5e02\u573a\u5f71\u54cd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u4e3b\u6d41\u5927\u6a21\u578b\u8fdb\u884c\u4e00\u4e2a\u5927\u81f4\u7684\u7efc\u5408\u5b9e\u529b\u5206\u5c42\u6392\u540d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
