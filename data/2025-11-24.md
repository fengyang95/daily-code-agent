<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.SE](#cs.SE) [Total: 4]
- [tldr.article](#tldr.article) [Total: 6]
- [cs.AI](#cs.AI) [Total: 8]
- [wechat.article](#wechat.article) [Total: 24]
- [cs.LG](#cs.LG) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction to Python Code Generation](https://arxiv.org/abs/2511.16787)
*Hossain Shaikh Saadi,Faria Alam,Mario Sanz-Guerrero,Minh Duc Bui,Manuel Mager,Katharina von der Wense*

Main category: cs.CL

TL;DR: JGU Mainz团队在BLP-2025代码生成共享任务中获胜，提出多智能体管道：代码生成智能体生成初始方案，调试智能体针对失败测试用例进行修正，最终获得95.4%的Pass@1得分。


<details>
  <summary>Details</summary>
Motivation: 解决从孟加拉语指令生成代码的挑战，通过多智能体协作提高代码生成质量。

Method: 采用多智能体管道：代码生成智能体生成初始代码，调试智能体针对失败测试用例提取错误信息并生成修正方案。

Result: 在BLP-2025共享任务中获得第一名，Pass@1得分达到95.4%。

Conclusion: 多智能体方法能有效提升代码生成质量，特别是在处理复杂指令和错误修正方面表现优异。

Abstract: This paper presents JGU Mainz's winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions. We propose a multi-agent-based pipeline. First, a code-generation agent produces an initial solution from the input instruction. The candidate program is then executed against the provided unit tests (pytest-style, assert-based). Only the failing cases are forwarded to a debugger agent, which reruns the tests, extracts error traces, and, conditioning on the error messages, the current program, and the relevant test cases, generates a revised solution. Using this approach, our submission achieved first place in the shared task with a $Pass@1$ score of 95.4. We also make our code public.

</details>


### [2] [How Language Directions Align with Token Geometry in Multilingual LLMs](https://arxiv.org/abs/2511.16693)
*JaeSeong Kim,Suan Lee*

Main category: cs.CL

TL;DR: 该论文通过全面探测研究分析了多语言大模型中语言信息的内部表示结构和层间演化动态，发现语言信息在第一个Transformer块中急剧分离，并在整个模型深度中保持线性可分性。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言大模型在多种语言上表现出色，但对其内部表示空间中语言信息的结构化方式以及在不同层间的涌现过程缺乏系统性分析。

Method: 使用线性和非线性探针结合新的Token-Language Alignment分析，对6个多语言大模型的268个Transformer层进行全面探测研究。

Result: 语言信息在第一个Transformer块中急剧分离（从第0层到第1层提升76.4±8.2个百分点），且在整个模型深度中几乎完全线性可分。语言方向与词汇嵌入的对齐与训练数据的语言组成密切相关。

Conclusion: 多语言大模型不是通过表面脚本特征而是通过由训练语料塑造的潜在表示结构来区分语言，这为多语言表示学习的数据组成策略和公平性提供了实践洞见。

Abstract: Multilingual LLMs demonstrate strong performance across diverse languages, yet there has been limited systematic analysis of how language information is structured within their internal representation space and how it emerges across layers. We conduct a comprehensive probing study on six multilingual LLMs, covering all 268 transformer layers, using linear and nonlinear probes together with a new Token--Language Alignment analysis to quantify the layer-wise dynamics and geometric structure of language encoding. Our results show that language information becomes sharply separated in the first transformer block (+76.4$\pm$8.2 percentage points from Layer 0 to 1) and remains almost fully linearly separable throughout model depth. We further find that the alignment between language directions and vocabulary embeddings is strongly tied to the language composition of the training data. Notably, Chinese-inclusive models achieve a ZH Match@Peak of 16.43\%, whereas English-centric models achieve only 3.90\%, revealing a 4.21$\times$ structural imprinting effect. These findings indicate that multilingual LLMs distinguish languages not by surface script features but by latent representational structures shaped by the training corpus. Our analysis provides practical insights for data composition strategies and fairness in multilingual representation learning. All code and analysis scripts are publicly available at: https://github.com/thisiskorea/How-Language-Directions-Align-with-Token-Geometry-in-Multilingual-LLMs.

</details>


### [3] [Detecting and Steering LLMs' Empathy in Action](https://arxiv.org/abs/2511.16699)
*Juan P. Cadile*

Main category: cs.CL

TL;DR: 研究发现共情行为是LLM激活空间中的线性方向，可在特定层检测和操控。不同模型在检测精度上表现一致，但在操控稳健性上存在差异，安全训练影响操控稳健性而非防止操控。


<details>
  <summary>Details</summary>
Motivation: 探索LLM中"行动共情"（牺牲任务效率满足人类需求）的神经表征，研究其检测和操控的可能性，以及安全训练对共情行为的影响。

Method: 使用基于EIA基准的对比提示，在Phi-3-mini-4k、Qwen2.5-7B和Dolphin-Llama-3.1-8B三个模型上测试共情行为的检测和操控能力。

Result: 检测方面：所有模型在最优层AUROC达0.996-1.00，共情编码独立于安全训练。操控方面：Qwen和Phi-3双向操控成功率达65.3%和61.7%，Dolphin仅支持亲共情操控（94.4%成功率），反共情操控会导致崩溃。

Conclusion: 检测-操控差距因模型而异，安全训练影响操控稳健性而非防止操控，需要更多模型验证。

Abstract: We investigate empathy-in-action -- the willingness to sacrifice task efficiency to address human needs -- as a linear direction in LLM activation space. Using contrastive prompts grounded in the Empathy-in-Action (EIA) benchmark, we test detection and steering across Phi-3-mini-4k (3.8B), Qwen2.5-7B (safety-trained), and Dolphin-Llama-3.1-8B (uncensored).
  Detection: All models show AUROC 0.996-1.00 at optimal layers. Uncensored Dolphin matches safety-trained models, demonstrating empathy encoding emerges independent of safety training. Phi-3 probes correlate strongly with EIA behavioral scores (r=0.71, p<0.01). Cross-model probe agreement is limited (Qwen: r=-0.06, Dolphin: r=0.18), revealing architecture-specific implementations despite convergent detection.
  Steering: Qwen achieves 65.3% success with bidirectional control and coherence at extreme interventions. Phi-3 shows 61.7% success with similar coherence. Dolphin exhibits asymmetric steerability: 94.4% success for pro-empathy steering but catastrophic breakdown for anti-empathy (empty outputs, code artifacts).
  Implications: The detection-steering gap varies by model. Qwen and Phi-3 maintain bidirectional coherence; Dolphin shows robustness only for empathy enhancement. Safety training may affect steering robustness rather than preventing manipulation, though validation across more models is needed.

</details>


### [4] [Improving Latent Reasoning in LLMs via Soft Concept Mixing](https://arxiv.org/abs/2511.16885)
*Kang Wang,Xiangyu Duan,Tianyi Du*

Main category: cs.CL

TL;DR: 提出Soft Concept Mixing (SCM)训练方案，通过在训练中直接暴露软概念表示来弥合推理中的软概念与训练中的离散标记之间的差距，提升LLMs的推理性能。


<details>
  <summary>Details</summary>
Motivation: LLMs通常通过生成离散标记进行推理，这限制了其表达能力。现有研究表明LLMs通过软概念的潜在推理很有前景，但LLMs是在离散标记上训练的，存在训练与推理方式的不匹配。

Method: SCM构建软概念向量（嵌入的概率加权平均），将其混合到模型的隐藏状态中，并使用强化学习优化整个潜在推理过程。

Result: 在五个推理基准测试上的实验表明，SCM提高了LLMs的推理性能，同时保持了稳定的训练动态。

Conclusion: SCM通过软概念感知训练有效提升了LLMs的推理能力，弥合了软概念推理与离散标记训练之间的差距。

Abstract: Unlike human reasoning in abstract conceptual spaces, large language models (LLMs) typically reason by generating discrete tokens, which potentially limit their expressive power. The recent work Soft Thinking has shown that LLMs' latent reasoning via soft concepts is a promising direction, but LLMs are trained on discrete tokens. To reduce this gap between the soft concepts in reasoning and the discrete tokens in training, we propose Soft Concept Mixing (SCM), a soft concept aware training scheme that directly exposes the model to soft representations during training. Specifically, SCM constructs a soft concept vector by forming a probability-weighted average of embeddings. Then, this vector is mixed into the model's hidden states, which embody rich contextual information. Finally, the entire latent reasoning process is optimized with Reinforcement Learning (RL). Experiments on five reasoning benchmarks demonstrate that SCM improves the reasoning performance of LLMs, and simultaneously maintains a stable training dynamic.

</details>


### [5] [Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models](https://arxiv.org/abs/2511.17170)
*Vy Nguyen,Ziqi Xu,Jeffrey Chan,Estrid He,Feng Xia,Xiuzhen Zhang*

Main category: cs.CL

TL;DR: 提出Aspect-Based Causal Abstention (ABCA)框架，通过因果推理分析LLM内部知识多样性，实现早期弃权以防止幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有弃权方法依赖生成后信号，无法提前防止不可靠响应。LLM从不同来源获得的知识具有多面性，需要评估知识可靠性

Method: ABCA框架通过分析LLM内部知识多样性，基于方面条件因果效应评估知识可靠性，支持两种弃权类型：知识冲突和知识不足

Result: 在标准基准测试中，ABCA提高了弃权可靠性，达到最先进性能，并增强了弃权决策的可解释性

Conclusion: ABCA通过因果推理分析内部知识多样性，为LLM提供了更可靠的早期弃权机制，有效防止幻觉问题

Abstract: Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as "I don't know", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions.

</details>


### [6] [AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale](https://arxiv.org/abs/2511.17190)
*Ziyang Wang,Yuanlei Zheng,Zhenbiao Cao,Xiaojin Zhang,Zhongyu Wei,Pei Fu,Zhenbo Luo,Wei Chen,Xiang Bai*

Main category: cs.CL

TL;DR: AutoLink是一个自主代理框架，将模式链接重新定义为迭代的、代理驱动的过程，通过动态探索和扩展链接的模式子集，在不输入完整数据库模式的情况下逐步识别必要的模式组件。


<details>
  <summary>Details</summary>
Motivation: 在工业级文本到SQL任务中，由于上下文窗口限制和无关噪声，将整个数据库模式提供给大型语言模型是不切实际的。模式链接因此变得至关重要，但现有方法成本高昂，难以平衡召回率和噪声，且在大规模数据库上扩展性差。

Method: AutoLink采用自主代理框架，通过LLM指导动态探索和扩展链接的模式子集，迭代识别必要的模式组件，避免输入完整数据库模式。

Result: 在Bird-Dev上达到97.4%的严格模式链接召回率，在Spider-2.0-Lite上达到91.2%，执行准确率分别为68.7%和34.9%，在大规模模式（如超过3000列）上保持高召回率和高效令牌消耗。

Conclusion: AutoLink是一个高度可扩展、高召回率的模式链接解决方案，适用于工业级文本到SQL系统，特别是在大规模数据库场景下表现优异。

Abstract: For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \textbf{97.4\%} on Bird-Dev and \textbf{91.2\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \textbf{68.7\%} EX on Bird-Dev (better than CHESS) and \textbf{34.9\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \textbf{exceptional scalability}, \textbf{maintaining high recall}, \textbf{efficient token consumption}, and \textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems.

</details>


### [7] [A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents](https://arxiv.org/abs/2511.17208)
*Sizhe Zhou*

Main category: cs.CL

TL;DR: 提出基于事件语义的对话记忆方法，将对话历史表示为短小的事件命题，通过异构图组织会话和基本话语单元，在长对话基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决LLM对话代理在长期会话中保持连贯性和个性化的问题，避免固定上下文窗口限制和现有外部记忆方法的权衡问题。

Method: 将每个会话分解为丰富的基本话语单元(EDUs)，组织成异构图支持关联检索，构建基于密集相似性搜索和LLM过滤的检索变体。

Result: 在LoCoMo和LongMemEval$_S$基准测试中匹配或超越强基线，同时使用更短的QA上下文。

Conclusion: 结构简单的事件级记忆为长视野对话代理提供了原则性和实用的基础。

Abstract: LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem.

</details>


### [8] [Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator for Group Chats](https://arxiv.org/abs/2511.17315)
*Mateusz Jacniacki,Martí Carmona Serrat*

Main category: cs.CL

TL;DR: 提出了HUMA（类人多用户代理），这是一个基于LLM的促进者，能够在多用户对话中使用类人策略和时机进行互动，在群聊环境中难以被识别为非人类。


<details>
  <summary>Details</summary>
Motivation: 当前大多数对话系统设计为一对一的轮流交流，而非自然的异步群聊。随着AI助手在数字平台中的普及，开发自然且类人的交互模式对于维持用户信任和参与度至关重要。

Method: HUMA采用事件驱动架构，包含三个组件：路由器、行动代理和反思，共同适应群聊动态。系统处理消息、回复、反应，并引入逼真的响应时间模拟。

Result: 在97名参与者的四人角色扮演聊天研究中，参与者在两种条件下将社区管理者分类为人类的概率接近随机水平，表明他们无法可靠区分HUMA代理和人类。主观体验在条件间相当，只有适度差异和小效应量。

Conclusion: 在自然群聊设置中，AI促进者可以匹配人类质量，同时保持难以被识别为非人类。

Abstract: Conversational agents built on large language models (LLMs) are becoming increasingly prevalent, yet most systems are designed for one-on-one, turn-based exchanges rather than natural, asynchronous group chats. As AI assistants become widespread throughout digital platforms, from virtual assistants to customer service, developing natural and humanlike interaction patterns seems crucial for maintaining user trust and engagement. We present the Humanlike Multi-user Agent (HUMA), an LLM-based facilitator that participates in multi-party conversations using human-like strategies and timing. HUMA extends prior multi-user chatbot work with an event-driven architecture that handles messages, replies, reactions and introduces realistic response-time simulation. HUMA comprises three components-Router, Action Agent, and Reflection-which together adapt LLMs to group conversation dynamics.
  We evaluate HUMA in a controlled study with 97 participants in four-person role-play chats, comparing AI and human community managers (CMs). Participants classified CMs as human at near-chance rates in both conditions, indicating they could not reliably distinguish HUMA agents from humans. Subjective experience was comparable across conditions: community-manager effectiveness, social presence, and engagement/satisfaction differed only modestly with small effect sizes. Our results suggest that, in natural group chat settings, an AI facilitator can match human quality while remaining difficult to identify as nonhuman.

</details>


### [9] [Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training](https://arxiv.org/abs/2511.17405)
*Yesheng Liu,Hao Li,Haiyu Xu,Baoqi Pei,Jiahao Wang,Mingxuan Zhao,Jingshu Zheng,Zheqi He,JG Yao,Bowen Qin,Xi Yang,Jiajun Zhang*

Main category: cs.CL

TL;DR: ReVeL框架将多项选择题重写为开放式问题，解决了MCQA中选项泄露信号的问题，提高了模型训练和评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 多项选择题的选项可能泄露可利用信号，导致准确性指标不可靠，并鼓励在强化微调中出现猜测行为。

Method: 提出ReVeL框架，将MCQA重写为开放式问题，根据答案类型应用不同的重写和验证方案，使用GRPO对Qwen2.5-VL模型进行微调。

Result: 在多项选择基准测试中匹配MCQA准确性，OpenQA准确性提高约6个百分点，在评估中揭示MCQA基准测试得分膨胀高达20个百分点。

Conclusion: ReVeL比基于MCQA的训练具有更好的数据效率和更稳健的奖励信号，同时提高判断准确性并降低成本和延迟。

Abstract: Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [Multi-Agent Code Verification with Compound Vulnerability Detection](https://arxiv.org/abs/2511.16708)
*Shreshth Rajan*

Main category: cs.SE

TL;DR: CodeX-Verify是一个多智能体系统，使用四个专门化智能体检测不同类型的代码漏洞。该系统在不执行测试的情况下，能捕获76.1%的漏洞，准确率与最佳现有方法相当但运行更快。多智能体组合相比单智能体将准确率从32.8%提升至72.4%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的代码存在严重漏洞问题：SWE-bench中29.6%的修复补丁失败，BaxBench中62%的解决方案有漏洞，现有工具只能捕获65%的漏洞且假阳性率达35%。需要更有效的漏洞检测方法。

Method: 构建多智能体系统CodeX-Verify，使用四个专门化智能体检测不同类型的漏洞。通过数学证明和实验验证，当智能体寻找不同问题时，组合多个智能体比任何单一智能体能发现更多漏洞。

Result: 在99个带验证标签的代码样本上测试，系统捕获76.1%的漏洞，与最佳现有方法相当但运行更快且无需测试执行。多智能体组合相比单智能体将准确率提升39.7个百分点。最佳双智能体组合达到79.3%准确率。在300个真实补丁上测试，每个样本运行时间低于200ms。

Conclusion: 多智能体方法能有效提高代码漏洞检测的准确性和效率，且具有实际生产应用价值。多个漏洞在同一代码中的组合风险比传统模型预测的要高得多。

Abstract: LLMs generate buggy code: 29.6% of SWE-bench "solved" patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, confirmed by measuring agent correlation of p = 0.05--0.25. We also show that multiple vulnerabilities in the same code create exponentially more risk than previously thought--SQL injection plus exposed credentials creates 15x more danger (risk 300 vs. 20) than traditional models predict. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution. We tested 15 different agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with gains of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4. The best two-agent combination reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.

</details>


### [11] [Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair](https://arxiv.org/abs/2511.16858)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: 研究测试过拟合在当今代码修复中的持续性问题，使用SWE-bench仓库级任务进行实验分析


<details>
  <summary>Details</summary>
Motivation: 自动化程序修复存在测试过拟合问题，即修复后的代码能通过可见测试但在隐藏测试集上失败。虽然这个问题在大型语言模型兴起前已被研究，但需要验证其在当前环境下的严重程度

Method: 使用SWE-bench仓库级任务进行实验研究，分析测试过拟合现象

Result: 实验结果表明测试过拟合问题在今天仍然存在

Conclusion: 测试过拟合仍然是自动化程序修复中需要关注的重要问题

Abstract: Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.

</details>


### [12] [UI-CUBE: Enterprise-Grade Computer Use Agent Benchmarking Beyond Task Accuracy to Operational Reliability](https://arxiv.org/abs/2511.17131)
*Horia Cristescu,Charles Park,Trong Canh Nguyen,Sergiu Talmacel,Alexandru-Gabriel Ilie,Stefan Adam*

Main category: cs.SE

TL;DR: UI-CUBE是一个用于评估计算机使用代理(CUA)企业部署准备度的基准测试，包含226个任务，揭示当前CUA在复杂工作流中的架构局限性


<details>
  <summary>Details</summary>
Motivation: 现有CUA基准主要关注功能正确性，但缺乏对企业部署所需的操作可靠性的评估，需要系统化的基准来暴露CUA的架构限制

Method: 开发UI-CUBE基准，包含136个简单UI交互任务、50个复制粘贴任务和40个企业应用场景任务，通过系统界面变化覆盖、多分辨率测试和应用状态自动验证来评估任务成功率

Result: 评估5个最先进模型显示能力断崖式下降：简单UI交互成功率67-85%，复杂工作流骤降至9-19%；人类评估者在复杂任务上仅61.2%成功率

Conclusion: 当前CUA虽然能操作单个界面元素，但无法作为可靠的工作流自动化工具，存在内存管理、分层规划和状态协调等架构限制

Abstract: While current Computer Use Agent (CUA) benchmarks measure task completion effectively, they provide limited assessment of enterprise deployment readiness, emphasizing functional correctness over the operational reliability required for production systems. We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs. Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks), with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state. Evaluation of five state-of-the-art models reveals a sharp capability cliff rather than gradual performance degradation. Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%. Human evaluators with no prior application experience achieve only 61.2% on complex tasks despite near-perfect performance on simple tasks, establishing realistic performance ceilings. This discontinuous performance pattern -- where agents achieve 68-87% of human performance on simple tasks but only 15-32% on complex workflows -- indicates fundamental architectural limitations in memory management, hierarchical planning, and state coordination rather than incremental capability gaps addressable through better training or prompting. UI-CUBE functions as an enterprise-readiness diagnostic, revealing that while current CUAs can manipulate individual interface elements, they cannot yet function as reliable workflow automation tools. These findings provide architectural insights essential for developing production-ready CUAs capable of managing complex, multi-step enterprise processes.

</details>


### [13] [Agentic Program Verification](https://arxiv.org/abs/2511.17330)
*Haoxin Tu,Huan Zhao,Yahui Song,Mehtab Zafar,Ruijie Meng,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: AutoRocq是一个用于程序验证的LLM智能体，通过迭代精化循环与Rocq定理证明器协作，实现自主的证明构建和验证。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成代码的普及，需要自动化程序验证来确保代码可靠性。现有的方法依赖大量训练数据，而AutoRocq旨在实现无需大量训练、能够自主学习和改进的验证方法。

Method: 采用迭代精化循环，LLM智能体与Rocq定理证明器协作，通过获取上下文和反馈不断改进证明，最终生成经过定理证明器验证的证明推导。

Result: 在SV-COMP基准测试和Linux内核模块上的实验显示，该方法在自动化程序验证方面具有良好效果。

Conclusion: AutoRocq可以与AI代码生成智能体集成，实现生成-验证循环，推动可信自动编程的发展。

Abstract: Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.
  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.
  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [14] [Ship faster without flaky Playwright tests](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmomentic.ai%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=tldr11212025/2/0100019aa6650ca8-4a441b48-fe48-4486-bb4e-baaf51b8798f-000000/iOmEhXQXZr9hjXH9yDYn5sTENxgu6JGFKx--W9wKmUg=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Momentic是一个AI驱动的端到端测试平台，可以将英文描述转换为自适应UI变化的测试，无需维护脆弱的CSS选择器或自定义测试代码。


<details>
  <summary>Details</summary>
Motivation: 解决团队在维护脆弱的Playwright测试上花费过多时间的问题，让团队能够更快地发布功能。

Method: 使用AI技术将英文描述的测试流程自动转换为端到端测试，测试能够自适应UI变化，无需手动维护CSS选择器或测试代码。

Result: 帮助团队在每个PR和部署时捕获回归问题，每周节省数小时的测试维护时间，为工程师提供可靠的发布前信号。

Conclusion: Momentic通过AI驱动的自适应测试解决了传统端到端测试的脆弱性问题，显著提高了测试效率和可靠性。

Abstract: Ship faster without flaky Playwright tests (Sponsor) If your team spends more time nursing brittle Playwright tests than shipping features, you should check out Momentic.Turn plain-English descriptions of your critical flows into AI-native end-to-end tests that adapt when the UI changes. No brittle selectors, no custom test code to maintain. With Momentic you can: Catch regressions on every PR and deploy Cut hours of test maintenance each week Give engineers reliable signal before every relea...

</details>


### [15] [Code execution with MCP: building more efficient AI agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fcode-execution-with-mcp%3Futm_source=tldrwebdev/1/0100019aa6650ca8-4a441b48-fe48-4486-bb4e-baaf51b8798f-000000/xIWTLiJ_6SibLzchPMYuNpMhslEAKfBzqf-dPIdliMA=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文提出通过代码执行环境让AI代理将MCP服务器作为API调用，而不是直接使用工具调用，从而提高代理在规模化使用大量工具时的效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理扩展到使用数百或数千个工具，预先加载所有工具定义并通过上下文窗口传递中间结果会导致效率低下和成本增加。

Method: 使用代码执行环境，让代理编写代码与MCP服务器作为API进行交互，而不是直接进行工具调用，从而只加载需要的工具。

Result: 这种方法提高了代理的效率，减少了不必要的工具加载和上下文传递。

Conclusion: 通过代码执行环境将MCP服务器作为API调用是构建更高效AI代理的有效方法。

Abstract: Code execution with MCP: building more efficient AI agents (8 minute read) MCP allows AI agents to connect to external systems, but as agents scale to use hundreds or thousands of tools, loading all tool definitions upfront and passing intermediate results through the context window creates inefficiencies and increased costs. By using code execution environments where agents write code to interact with MCP servers as APIs rather than making direct tool calls, agents can load only needed tools...

</details>


### [16] [CC Switch](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Ffarion1231%2Fcc-switch%3Futm_source=tldrwebdev/1/0100019aa6650ca8-4a441b48-fe48-4486-bb4e-baaf51b8798f-000000/AuWnqWSESom9B8UAgTTktgt08D70jGEyj1xa9DnLhvk=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: CC-Switch是一个跨平台桌面应用，用于管理和切换Claude Code、Codex和Gemini API配置，支持MCP集成、API速度测试、配置导入导出和多语言功能。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一的工具来管理多个AI代码生成API的配置，简化开发者在不同API之间切换的过程。

Method: 构建跨平台桌面应用程序，集成多个API配置管理功能，包括MCP集成、性能测试和配置迁移工具。

Result: 成功开发了CC-Switch应用，支持三种主要AI代码生成API的配置管理和快速切换。

Conclusion: CC-Switch为开发者提供了一个便捷的工具来管理多个AI代码生成API，提高了开发效率。

Abstract: CC Switch (GitHub Repo) CC-Switch is a cross-platform desktop application for managing and switching between Claude Code, Codex, and Gemini API configurations. It supports MCP integration, API speed testing, config import/export, and multiple languages.

</details>


### [17] [TestSprite](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testsprite.com%2F%3Futm_source=tldrfounders/1/0100019aa6873fd0-436817e3-64a0-47d8-a4bd-0c5eabcaeaf8-000000/Xx7hlyiD0HTk7_JnKYOC_EDwYYcq36Yhka1Fe0Gzqd4=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: TestSprite是一个AI代理，通过自然语言工作流自动化前端和后端软件测试，可将测试成本降低高达90%并提供完整的测试覆盖。


<details>
  <summary>Details</summary>
Motivation: 传统软件测试过程耗时且成本高昂，需要自动化解决方案来提高效率并降低成本。

Method: 使用AI代理和自然语言工作流来自动化前端和后端测试过程。

Result: 能够将测试成本降低高达90%，并提供完整的测试覆盖。

Conclusion: TestSprite通过AI驱动的自动化测试方法显著提高了测试效率并大幅降低了成本。

Abstract: TestSprite (Tool) TestSprite is an AI agent that automates frontend and backend software testing, cutting testing costs by up to 90% with natural language workflows and full test coverage.

</details>


### [18] [Obscure MCP API in Comet Browser Breaches User Trust, Enabling Full Device Control via AI Browsers](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhackread.com%2Fobscure-mcp-api-in-comet-browser-breaches-user-trust-enabling-full-device-control-via-ai-browsers%2F%3Futm_source=tldrinfosec/1/0100019aa6bdc916-a6dcc5ef-8947-49c5-bb83-cea65021b3e9-000000/2F2YME6ZaqKNVk2YyaEedVzaCbXbTV6pOJF0aMw488s=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: SquareX研究人员在Comet浏览器中发现了一个隐藏的MCP API，该API允许嵌入式扩展在未经用户同意的情况下执行任意本地命令，绕过了数十年来建立的浏览器安全原则。


<details>
  <summary>Details</summary>
Motivation: 揭露Comet浏览器中隐藏的安全漏洞，该漏洞可能被恶意第三方利用来完全控制用户设备，破坏用户信任。

Method: 研究人员通过分析Comet浏览器的Agentic扩展和perplexity.ai的触发机制，发现了chrome.perplexity.mcp.addStdioServer这个隐藏API。

Result: 发现该API允许嵌入式扩展绕过浏览器安全沙箱，直接执行本地系统命令，造成严重的安全威胁。

Conclusion: Comet浏览器中的这个隐藏API违反了基本的浏览器安全原则，需要立即修复以防止恶意利用。

Abstract: Obscure MCP API in Comet Browser Breaches User Trust, Enabling Full Device Control via AI Browsers (4 minute read) SquareX researchers exposed a hidden MCP API (chrome.perplexity.mcp.addStdioServer) in the Comet browser that enables embedded extensions to execute arbitrary local commands without user consent, thereby bypassing decades of established browser security principles. The API is accessible via Comet's Agentic extension, triggered by perplexity.ai, creating a catastrophic third-party...

</details>


### [19] [Introducing cline-bench: A Real-World, Open Source Benchmark for Agentic Coding](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcline.bot%2Fblog%2Fcline-bench-initiative%3Futm_source=tldrai/1/0100019aa6c73715-49e25c81-f62d-4400-adcb-a073ef85dfe7-000000/0EO7nUUHHYLEBWq90cmQgwnMR6iFKYAk15zHHIdlj-o=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: cline-bench是一个新的开源基准测试，专注于从真实开源开发场景中创建高保真度的基准测试和强化学习环境，以解决当前AI模型在真实工程工作中缺乏严格基准测试的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型虽然取得了显著进展，但该领域仍然缺乏能够代表真实工程工作的严格开源基准测试。模型实验室需要能够暴露真实问题的评估方法。

Method: 通过从真实开源开发场景中创建高保真度的基准测试和强化学习环境来构建cline-bench。

Result: cline-bench支持AI模型开发的下一阶段，为模型实验室提供能够暴露真实问题的评估环境。

Conclusion: cline-bench填补了AI编码代理领域在真实工程工作基准测试方面的空白，为模型开发提供更严格的评估标准。

Abstract: Introducing cline-bench: A Real-World, Open Source Benchmark for Agentic Coding (11 minute read) cline-bench is a new initiative focused on creating high fidelity benchmarks and reinforcement learning environments derived from real open source development scenarios. AI models have advanced significantly, but the field still lacks a rigorous open source benchmark that represents real engineering work. Model labs require evals that expose real breakdowns. cline-bench supports the next stage of ...

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [Cognitive BASIC: An In-Model Interpreted Reasoning Language for LLMs](https://arxiv.org/abs/2511.16837)
*Oliver Kramer*

Main category: cs.AI

TL;DR: Cognitive BASIC是一种基于BASIC风格的提示语言和模型内解释器，将大型语言模型的推理结构化为显式的逐步执行轨迹。


<details>
  <summary>Details</summary>
Motivation: 受复古BASIC简单性的启发，重新利用编号行和简单命令作为可解释的认知控制层，使LLM推理过程更加透明。

Method: 使用自然语言解释器文件指定命令语义、内存更新和日志行为，通过心理模型解释器提取声明性和程序性知识，检测矛盾并在必要时产生解决方案。

Result: 在三个LLM上的基准测试显示，所有模型都能执行Cognitive BASIC程序，整体性能强劲但不均匀。

Conclusion: Cognitive BASIC为LLM提供了结构化的推理框架，使多步推理过程在模型内部变得透明。

Abstract: Cognitive BASIC is a minimal, BASIC-style prompting language and in-model interpreter that structures large language model (LLM) reasoning into explicit, stepwise execution traces. Inspired by the simplicity of retro BASIC, we repurpose numbered lines and simple commands as an interpretable cognitive control layer. Modern LLMs can reliably simulate such short programs, enabling transparent multi-step reasoning inside the model. A natural-language interpreter file specifies command semantics, memory updates, and logging behavior. Our mental-model interpreter extracts declarative and procedural knowledge, detects contradictions, and produces resolutions when necessary. A comparison across three LLMs on a benchmark of knowledge extraction, conflict detection, and reasoning tasks shows that all models can execute Cognitive BASIC programs, with overall strong but not uniform performance.

</details>


### [21] [Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving](https://arxiv.org/abs/2511.16916)
*Ye Han,Lijun Zhang,Dejian Meng,Zhuang Zhang*

Main category: cs.AI

TL;DR: 提出混合差分奖励机制解决多车协同驾驶中传统奖励函数信号消失问题，通过时间差分奖励和动作梯度奖励提高策略梯度信噪比，显著提升算法收敛速度和策略稳定性。


<details>
  <summary>Details</summary>
Motivation: 多车协同驾驶任务中，传统基于状态的奖励函数存在奖励差异消失问题，导致策略梯度信噪比低，严重影响算法收敛和性能提升。

Method: 提出混合差分奖励机制，包含基于全局势函数的时间差分奖励和直接测量动作边际效用的动作梯度奖励，并在部分可观测马尔可夫博弈框架中实例化。

Result: 实验表明HDR机制显著提高了收敛速度和策略稳定性，引导智能体学习到平衡交通效率和安全的优质协同策略。

Conclusion: HDR机制通过解决奖励信号消失问题，有效提升了多车协同驾驶任务的强化学习性能。

Abstract: In multi-vehicle cooperative driving tasks involving high-frequency continuous control, traditional state-based reward functions suffer from the issue of vanishing reward differences. This phenomenon results in a low signal-to-noise ratio (SNR) for policy gradients, significantly hindering algorithm convergence and performance improvement. To address this challenge, this paper proposes a novel Hybrid Differential Reward (HDR) mechanism. We first theoretically elucidate how the temporal quasi-steady nature of traffic states and the physical proximity of actions lead to the failure of traditional reward signals. Building on this analysis, the HDR framework innovatively integrates two complementary components: (1) a Temporal Difference Reward (TRD) based on a global potential function, which utilizes the evolutionary trend of potential energy to ensure optimal policy invariance and consistency with long-term objectives; and (2) an Action Gradient Reward (ARG), which directly measures the marginal utility of actions to provide a local guidance signal with a high SNR. Furthermore, we formulate the cooperative driving problem as a Multi-Agent Partially Observable Markov Game (POMDPG) with a time-varying agent set and provide a complete instantiation scheme for HDR within this framework. Extensive experiments conducted using both online planning (MCTS) and Multi-Agent Reinforcement Learning (QMIX, MAPPO, MADDPG) algorithms demonstrate that the HDR mechanism significantly improves convergence speed and policy stability. The results confirm that HDR guides agents to learn high-quality cooperative policies that effectively balance traffic efficiency and safety.

</details>


### [22] [MirrorMind: Empowering OmniScientist with the Expert Perspectives and Collective Knowledge of Human Scientists](https://arxiv.org/abs/2511.16997)
*Qingbin Zeng,Bingbing Fan,Zhiyu Chen,Sijian Ren,Zhilun Zhou,Xuhua Zhang,Yuanyi Zhen,Fengli Xu,Yong Li,Tie-Yan Liu*

Main category: cs.AI

TL;DR: MirrorMind是一个分层认知架构，通过整合个体认知轨迹和集体学科记忆，使AI科学家能够进行结构化、个性化的科学推理。


<details>
  <summary>Details</summary>
Motivation: 现有AI科学方法将科学发现视为孤立的优化过程，忽视了知识生产的社会性和历史性本质。人类科学洞察来自个体认知轨迹和集体学科记忆两个相互关联的来源。

Method: 提出三层框架：个体层面构建研究者的认知模型（情景、语义、人格记忆）；领域层面将集体知识映射为学科概念图；跨学科层面作为正交编排引擎。架构将记忆存储与智能执行分离。

Result: 在四个综合任务中评估：作者级认知模拟、互补推理、跨学科协作促进、多智能体科学问题解决。结果显示MirrorMind超越了简单事实检索，实现了结构化、个性化的洞察生成。

Conclusion: 通过整合个体认知深度和集体学科广度，MirrorMind能够进行结构性、个性化的科学推理，推动AI科学家超越简单事实检索。

Abstract: The emergence of AI Scientists has demonstrated remarkable potential in automating scientific research. However, current approaches largely conceptualize scientific discovery as a solitary optimization or search process, overlooking that knowledge production is inherently a social and historical endeavor. Human scientific insight stems from two distinct yet interconnected sources. First is the individual cognitive trajectory, where a researcher's unique insight is shaped by their evolving research history and stylistic preferences; another is the collective disciplinary memory, where knowledge is sedimented into vast, interconnected networks of citations and concepts. Existing LLMs still struggle to represent these structured, high-fidelity cognitive and social contexts. To bridge this gap, we introduce MirrorMind, a hierarchical cognitive architecture that integrates dual-memory representations within a three-level framework. The Individual Level constructs high-fidelity cognitive models of individual researchers by capturing their episodic, semantic, and persona memories; the Domain Level maps collective knowledge into structured disciplinary concept graphs; and the Interdisciplinary Level that acts as an orthogonal orchestration engine. Crucially, our architecture separates memory storage from agentic execution, enabling AI scientist agents to flexibly access individual memories for unique perspectives or collective structures to reason. We evaluate MirrorMind across four comprehensive tasks, including author-level cognitive simulation, complementary reasoning, cross-disciplinary collaboration promotion, and multi-agent scientific problem solving. The results show that by integrating individual cognitive depth with collective disciplinary breadth, MirrorMind moves beyond simple fact retrieval toward structural, personalized, and insight-generating scientific reasoning.

</details>


### [23] [Budget-Aware Tool-Use Enables Effective Agent Scaling](https://arxiv.org/abs/2511.17006)
*Tengxiao Liu,Zifeng Wang,Jin Miao,I-Hung Hsu,Jun Yan,Jiefeng Chen,Rujun Han,Fangyuan Xu,Yanfei Chen,Ke Jiang,Samira Daruki,Yi Liang,William Yang Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.AI

TL;DR: 本文研究了在工具增强型智能体中如何有效扩展测试时计算，提出了预算感知方法来解决单纯增加工具调用预算无法提升性能的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强型智能体缺乏预算意识，即使给予更大的工具调用预算也无法提升性能，需要研究如何在明确工具调用预算下有效扩展智能体能力。

Method: 提出了预算跟踪器（Budget Tracker）轻量级插件提供持续预算意识，并开发了BATS框架动态调整规划和验证策略，同时建立了统一的成本度量标准。

Result: 预算感知方法产生了更有利的扩展曲线，推动了成本-性能的帕累托前沿，提供了对工具增强型智能体扩展的更透明和原则性理解。

Conclusion: 预算感知是工具增强型智能体有效扩展的关键，该方法为智能体在约束预算下的性能优化提供了系统解决方案。

Abstract: Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only "thinking" in tokens but also "acting" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack "budget awareness" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to "dig deeper" on a promising lead or "pivot" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.

</details>


### [24] [The Belief-Desire-Intention Ontology for modelling mental reality and agency](https://arxiv.org/abs/2511.17162)
*Sara Zuppiroli,Carmelo Fabio Longo,Anna Sofia Lippolis,Rocco Paolillo,Lorenzo Giammei,Miguel Ceriani,Francesco Poggi,Antonio Zinilli,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 本文提出了一个形式化的BDI本体论，作为模块化的本体设计模式，用于表示智能体的信念、愿望和意图及其动态关系。通过两个实验验证了其适用性：与LLM结合增强推理一致性，以及在Semas推理平台中实现RDF三元组与心智状态的双向转换。


<details>
  <summary>Details</summary>
Motivation: BDI模型是人工智能和认知科学中表示理性智能体的基础模型，但其与结构化、语义可互操作的知识表示的整合仍然有限。需要开发形式化的BDI本体来支持认知基础的多智能体和神经符号系统。

Method: 设计了一个模块化的BDI本体论模式，与基础本体对齐确保语义精确性和可重用性。通过两个实验验证：(1) 与LLM结合使用逻辑增强生成评估本体基础对推理一致性的贡献；(2) 在Semas推理平台中集成，实现T2B2T范式。

Result: BDI本体论在实验中成功作为声明式智能和过程式智能之间的概念和操作桥梁，支持了认知基础、可解释且语义可互操作的多智能体系统在数据网络中的运行。

Conclusion: 该BDI本体论为构建认知基础、可解释且语义可互操作的多智能体和神经符号系统提供了有效的概念和操作框架，促进了声明式智能与过程式智能的融合。

Abstract: The Belief-Desire-Intention (BDI) model is a cornerstone for representing rational agency in artificial intelligence and cognitive sciences. Yet, its integration into structured, semantically interoperable knowledge representations remains limited. This paper presents a formal BDI Ontology, conceived as a modular Ontology Design Pattern (ODP) that captures the cognitive architecture of agents through beliefs, desires, intentions, and their dynamic interrelations. The ontology ensures semantic precision and reusability by aligning with foundational ontologies and best practices in modular design. Two complementary lines of experimentation demonstrate its applicability: (i) coupling the ontology with Large Language Models (LLMs) via Logic Augmented Generation (LAG) to assess the contribution of ontological grounding to inferential coherence and consistency; and (ii) integrating the ontology within the Semas reasoning platform, which implements the Triples-to-Beliefs-to-Triples (T2B2T) paradigm, enabling a bidirectional flow between RDF triples and agent mental states. Together, these experiments illustrate how the BDI Ontology acts as both a conceptual and operational bridge between declarative and procedural intelligence, paving the way for cognitively grounded, explainable, and semantically interoperable multi-agent and neuro-symbolic systems operating within the Web of Data.

</details>


### [25] [Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism](https://arxiv.org/abs/2511.17198)
*Kaiyu Li,Jiayu Wang,Zhi Wang,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.AI

TL;DR: 提出了基于分层任务抽象机制(HTAM)的EarthAgent多智能体框架，专门用于复杂地理空间分析，在GeoPlan-bench基准测试中显著优于现有单智能体和多智能体系统。


<details>
  <summary>Details</summary>
Motivation: 通用LLM驱动智能体在需要严格结构化工作流程的专业领域表现不佳，特别是在遥感等需要专业工具和多步骤程序的领域。

Method: 引入分层任务抽象机制(HTAM)，将多智能体系统组织成反映领域内在任务依赖关系的逻辑层次结构，确保程序正确性并将复杂问题分解为顺序层。

Result: EarthAgent在GeoPlan-bench基准测试中显著优于现有单智能体和多智能体系统，证明了将智能体架构与领域任务结构对齐的重要性。

Conclusion: 将智能体架构与领域内在任务结构对齐是构建稳健可靠的专业自主系统的关键步骤。

Abstract: LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems.

</details>


### [26] [Agentifying Agentic AI](https://arxiv.org/abs/2511.17332)
*Virginia Dignum,Frank Dignum*

Main category: cs.AI

TL;DR: 论文主张将AAMAS社区开发的BDI架构、通信协议、机制设计和制度建模等概念工具作为实现智能AI系统的基础，通过结合数据驱动方法和结构化推理协调模型，构建具备透明度、协作性和问责制的智能系统。


<details>
  <summary>Details</summary>
Motivation: 当前智能AI系统缺乏对认知、协作和治理的明确模型，需要结合形式化理论和实际自主性来构建更完善的智能系统。

Method: 利用AAMAS社区的概念工具（BDI架构、通信协议、机制设计、制度建模），将自适应数据驱动方法与结构化推理协调模型相结合。

Result: 提出了一个连接形式化理论和实际自主性的智能视角，为构建透明、协作和可问责的智能系统奠定了基础。

Conclusion: AAMAS社区的概念工具为实现具备持续自主性、推理和交互能力的智能AI系统提供了必要的理论基础和方法论支持。

Abstract: Agentic AI seeks to endow systems with sustained autonomy, reasoning, and interaction capabilities. To realize this vision, its assumptions about agency must be complemented by explicit models of cognition, cooperation, and governance. This paper argues that the conceptual tools developed within the Autonomous Agents and Multi-Agent Systems (AAMAS) community, such as BDI architectures, communication protocols, mechanism design, and institutional modelling, provide precisely such a foundation. By aligning adaptive, data-driven approaches with structured models of reasoning and coordination, we outline a path toward agentic systems that are not only capable and flexible, but also transparent, cooperative, and accountable. The result is a perspective on agency that bridges formal theory and practical autonomy.

</details>


### [27] [That's not natural: The Impact of Off-Policy Training Data on Probe Performance](https://arxiv.org/abs/2511.17408)
*Nathalie Kirch,Samuel Dower,Adrians Skapars,Ekdeep Singh Lubana,Dmitrii Krasheninnikov*

Main category: cs.AI

TL;DR: 本文系统评估了在LLM监控中使用合成和离策略数据对探针泛化能力的影响，发现响应生成策略显著影响探针性能，且训练数据域的偏移会导致更大的性能下降。


<details>
  <summary>Details</summary>
Motivation: 由于许多行为的自然示例稀少，研究者不得不依赖合成或离策略的LLM响应来训练探针，但这种方法对探针泛化能力的影响尚不明确。

Method: 在八个不同的LLM行为上测试线性和注意力探针，评估不同响应生成策略对探针性能的影响，并分析从离策略数据到同策略数据的泛化能力。

Result: 发现从离策略数据成功泛化到测试集可以预测同策略泛化的成功，但欺骗和故意表现不佳的探针可能在真实监控场景中泛化失败。不同域测试分数始终低于同域测试。

Conclusion: 在缺乏同策略数据时，使用同域离策略数据比使用不同域的同策略数据产生更可靠的探针，强调需要更好处理LLM监控中分布偏移的方法。

Abstract: Probing has emerged as a promising method for monitoring Large Language Models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect varies by behaviour. We find that successful generalisation from off-policy data, to test sets where the model is incentivised to produce the target behaviour, is predictive of successful on-policy generalisation. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. Notably, shifts in the training data domain still cause even larger performance degradation, with different-domain test scores being consistently lower than the same-domain ones. These results indicate that, in the absence of on-policy data, using same-domain off-policy data yields more reliable probes than using on-policy data from a different domain, emphasizing the need for methods that can better handle distribution shifts in LLM monitoring.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [28] [verl <em class="highlight">强化学习</em>框架背后的巧思](http://mp.weixin.qq.com/s?__biz=MzA3Mzc4MTg4Mg==&mid=2453984555&idx=1&sn=6492314022baec078408687a53ec14d2&chksm=89a4ad337dd627676f91fb259ea27efa7c74b874ea23d2596c892bf425686a9efdb09967a4da#rd)
*浅尝大模型*

Main category: wechat.article

TL;DR: 2. 使用single-controller解耦强化学习多个不同阶段/角色之间的信息通信和角色内部的分布式计算通信。深度学习的分布式计算经常采用multi-controller模式，即没有中心化的调度器指挥和分配节点的工作，每个节点都跑完全一样的代


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2. 使用single-controller解耦强化学习多个不同阶段/角色之间的信息通信和角色内部的分布式计算通信。深度学习的分布式计算经常采用multi-controller模式，即没有中心化的调度器指挥和分配节点的工作，每个节点都跑完全一样的代

</details>


### [29] [【文献】端到端<em class="highlight">强化学习</em>提升灵巧抓取策略](http://mp.weixin.qq.com/s?__biz=MzIxNzQ0ODkyMQ==&mid=2247491473&idx=1&sn=aae7aac1cce3a5043ad847d3da2aea12&chksm=96b71dd50e95869bc60861bb7a9285d01495a5678e50c10619ab6ad49e030916be2207607919#rd)
*AILab笔记*

Main category: wechat.article

TL;DR: 与基于状态的强化学习（RL）有所不同，基于视觉的 RL 内存效率欠佳，致使批次大小相对较小，这对于 PPO 等算法而言并不适用。不过，它依旧是颇具吸引力的方法。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 与基于状态的强化学习（RL）有所不同，基于视觉的 RL 内存效率欠佳，致使批次大小相对较小，这对于 PPO 等算法而言并不适用。不过，它依旧是颇具吸引力的方法。

</details>


### [30] [基于<em class="highlight">强化学习</em>的 CPU 编译优化实践：MLGO 系列论文分析](http://mp.weixin.qq.com/s?__biz=MzkyNTMwMjI2Mw==&mid=2247504881&idx=1&sn=4aa66a613be309793216e114dde0535c&chksm=c0e1369332603a6a5bc0b36f6109ab7545c40bbff8c42b963b855e21df7e9afda55bb4a93eeb#rd)
*毕昇编译*

Main category: wechat.article

TL;DR: 本文通过解读最近几年强化学习技术在编译优化领域实践的两篇代表性论文，来看一看当前基于神经网络（NN）的编译优化技术到底做到了哪一步，哪些地方有突破，哪些地方又仍然受限。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本文通过解读最近几年强化学习技术在编译优化领域实践的两篇代表性论文，来看一看当前基于神经网络（NN）的编译优化技术到底做到了哪一步，哪些地方有突破，哪些地方又仍然受限。

</details>


### [31] [复杂学习VS深度学习，<em class="highlight">强化学习</em>，神经符号推理](http://mp.weixin.qq.com/s?__biz=Mzg5OTE2MzE2Ng==&mid=2247485812&idx=1&sn=327b07e63e2bed84353f5510feba1d73&chksm=c16d5671e3e3777c596a8120672665d3ef0434057982ac46c8594ef206832b83eb1d5901976d#rd)
*天机模型*

Main category: wechat.article

TL;DR: 复杂学习是主动理解，旨在发现系统内生的矛盾动力学。强化学习行为主义、最优控制环境反馈的奖励信号目标驱动：学习实现外部目标的最优策略。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 复杂学习是主动理解，旨在发现系统内生的矛盾动力学。强化学习行为主义、最优控制环境反馈的奖励信号目标驱动：学习实现外部目标的最优策略。

</details>


### [32] [原创 | 大模型扫盲系列（三）-<em class="highlight">强化学习</em>基础（下）](http://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247662412&idx=1&sn=434c843883ae938d16b8507f58ef103c&chksm=e8d345cb5f108ab1c1d3b7d36c148adbcbf58a08fae8a7f815baa5a0b6685c2ab2b3c3d72a68#rd)
*数据派THU*

Main category: wechat.article

TL;DR: 我们先清楚在大模型强化学习应用中，强化学习各个要素对应的含义：基于策略的算法（Policy-Based Optimization）在大模型强化学习中，基于策略的优化已成为主流范式，其核心在于直接对语言模型的生成策略进行端到端优化。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们先清楚在大模型强化学习应用中，强化学习各个要素对应的含义：基于策略的算法（Policy-Based Optimization）在大模型强化学习中，基于策略的优化已成为主流范式，其核心在于直接对语言模型的生成策略进行端到端优化。

</details>


### [33] [DRL圣经2025最新版-《<em class="highlight">强化学习</em>:导论第二版》免费pdf分享](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247572613&idx=2&sn=909ba3b426917561675b20bdbc5596cd&chksm=9687385c1291cbff81cf82008f45b0fe9f670363382eaebb324677364cd332f3d6f2a99b3482#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 我们第二版的目标和第一版的目标是一样的：为强化学习的关键思想和算法提供一个清晰而简单的描述，供所有相关学科的读者阅读。该版本仍然是一个介绍，我们保留了核心，在线学习算法的重点。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们第二版的目标和第一版的目标是一样的：为强化学习的关键思想和算法提供一个清晰而简单的描述，供所有相关学科的读者阅读。该版本仍然是一个介绍，我们保留了核心，在线学习算法的重点。

</details>


### [34] [2026年，<em class="highlight">强化学习</em>RL将走向何方？](http://mp.weixin.qq.com/s?__biz=Mzk0MjUxMzg3OQ==&mid=2247496535&idx=1&sn=fd053fc8946e8f7f322c497482444029&chksm=c3731290028388594a41cc9d2ef39b46c11e8aef66d4be6584ea2876a92e42574d1c7a19f3e3#rd)
*深夜努力写Python*

Main category: wechat.article

TL;DR: 【要点】论文提出了最大扩散强化学习方法，克服了传统强化学习中数据独立同分布的假设，实现了单次部署下的连续学习，并在多个基准测试中表现出稳健的优势。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 【要点】论文提出了最大扩散强化学习方法，克服了传统强化学习中数据独立同分布的假设，实现了单次部署下的连续学习，并在多个基准测试中表现出稳健的优势。

</details>


### [35] [机器人如何从失败中自我纠正？港科大&字节联合推出WMPO，世界模型让<em class="highlight">强化学习</em>‘零成本’进化](http://mp.weixin.qq.com/s?__biz=MzI4NjQxMzg0Nw==&mid=2247486119&idx=1&sn=053350f2670f3f4f63a8be966346587e&chksm=ea40921b9042f4307a79b0a87cf127a53d7cc40e6a37dffe48c5cec80208800a5c216c7a67d8#rd)
*具身智能观察室*

Main category: wechat.article

TL;DR: 该框架基于model-based强化学习（MBRL）原理，策略优化完全在模型内闭环完成，避免真实环境高成本交互。通过直接在像素空间建模，WMPO有效衔接预训练VLA特征与“想象”轨迹。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 该框架基于model-based强化学习（MBRL）原理，策略优化完全在模型内闭环完成，避免真实环境高成本交互。通过直接在像素空间建模，WMPO有效衔接预训练VLA特征与“想象”轨迹。

</details>


### [36] [<em class="highlight">强化学习</em>｜策略梯度算法介绍](http://mp.weixin.qq.com/s?__biz=MzkxMDIwOTU5OQ==&mid=2247484230&idx=1&sn=0d55d3527b0d4e1f995461e251c03523&chksm=c074129e88d02684e65c6c160f999183fc1e9656cd5f7559eeb7f3e3219beb5e90dbdb6f054a#rd)
*AI老马啊*

Main category: wechat.article

TL;DR: 强化学习训练中，有时回报总是正值，即公式 （10） 中的 总为正回报，在这种情况下，使得策略总是提升在对应状态下采取对应行动 的概率。而且为保证在状态 下所有可能动作的概率为1，在提升概率后会做归一化。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习训练中，有时回报总是正值，即公式 （10） 中的 总为正回报，在这种情况下，使得策略总是提升在对应状态下采取对应行动 的概率。而且为保证在状态 下所有可能动作的概率为1，在提升概率后会做归一化。

</details>


### [37] [<em class="highlight">强化学习</em>能否带我们抵达AGI?Amazon、OpenAI、Meta顶尖科学家给出了截然不同的答案](http://mp.weixin.qq.com/s?__biz=MzAxNzkyMTQ3Mg==&mid=2247483875&idx=1&sn=308ab4660c9a04b38fe07736aabdbd2c&chksm=9a4ef6e7d96628d912dada93082ff62d4840c5d07d44d0b8fa314ea0a2eefc746eb44a8b4dac#rd)
*阿牛今天学习了吗*

Main category: wechat.article

TL;DR: 后训练阶段的探索：目标：针对特定任务深度优化方法：有针对性的强化学习标准：任务性能最大化但李洪补充了一个关键洞察：当前的探索空间已经因预训练大幅缩小。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 后训练阶段的探索：目标：针对特定任务深度优化方法：有针对性的强化学习标准：任务性能最大化但李洪补充了一个关键洞察：当前的探索空间已经因预训练大幅缩小。

</details>


### [38] [<em class="highlight">Agentic</em> AI引领生命科学新图景](http://mp.weixin.qq.com/s?__biz=MzE5MTE2MTc2Nw==&mid=2247488812&idx=1&sn=27dfa98afe51a25b7e2e1cc1e2d64932&chksm=97c2251b14f208bd848baef6a7d386bc069edecd148dcd10b73a0f28dbd8289f28bf9a402ae7#rd)
*和通AI*

Main category: wechat.article

TL;DR: com/industries/life-sciences/our-insights/reimagining-life-science-enterprises-with-agentic-ai本文由大模型全文精译，和通AI编译发布。生命科学企业正面临利润空间被压缩、研发成本攀升，以及在技术与运营复杂性不断加剧的背景下对持续创新的迫


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: com/industries/life-sciences/our-insights/reimagining-life-science-enterprises-with-agentic-ai本文由大模型全文精译，和通AI编译发布。生命科学企业正面临利润空间被压缩、研发成本攀升，以及在技术与运营复杂性不断加剧的背景下对持续创新的迫

</details>


### [39] [<em class="highlight">Agentic</em> AI 时代：真正的革命不是技术，而是你的人生剧本](http://mp.weixin.qq.com/s?__biz=Mzk2NDQxMTYxNg==&mid=2247484069&idx=1&sn=45068ef2a3b17ab5fdac25746b41d98b&chksm=c52b8431f105a0132d0ff8c76af1c3872c34ef0bc564bb7ea2696a40d6775a61d3ed9131a84f#rd)
*锦尊路拆妄记*

Main category: wechat.article

TL;DR: 普通 AI：你说一句，它回一句。Agentic AI：你给目标，它自己去跑。比如说，你想订一次旅行。过去你要查机票、看酒店、对比攻略、填订单。一个机票两小时没买到、酒店涨价，你还得重新来。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 普通 AI：你说一句，它回一句。Agentic AI：你给目标，它自己去跑。比如说，你想订一次旅行。过去你要查机票、看酒店、对比攻略、填订单。一个机票两小时没买到、酒店涨价，你还得重新来。

</details>


### [40] [谷歌重磅出品！揭秘21种<em class="highlight">Agentic</em>设计模式，AI从业者必备](http://mp.weixin.qq.com/s?__biz=MzYyNTI3MTg2NQ==&mid=2247483713&idx=1&sn=3d5e8ab7c08dd77f77b15ce2bd002274&chksm=f1361f90d943118eb2a70f149bbd2df52f154f8093f6a80fca09887e024f6d09958937ef2079#rd)
*AI-Frontiers*

Main category: wechat.article

TL;DR: #agent #google


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: #agent #google

</details>


### [41] [从模型到<em class="highlight">智能体</em>：Snowflake 的企业级 <em class="highlight">Agentic</em> AI 工程化之路](http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651264061&idx=2&sn=db54f2e5d97bf731f3fda31202feea99&chksm=bceff8526b5e9b9b7aa358f924216d81c476d7ba7528457db8a9845f71570371a073c3e9a25a#rd)
*InfoQ*

Main category: wechat.article

TL;DR: 这是我们研发团队推动的一项创新，我们将其命名为 ReFoRCE，这是一个 Agentic 系统。它可以通过一系列自动化操作对数据库的 Schema 进行压缩和优化，并通过一个自动投票系统对比不同生成的 SQL 执行结果，最终达到最优效果。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这是我们研发团队推动的一项创新，我们将其命名为 ReFoRCE，这是一个 Agentic 系统。它可以通过一系列自动化操作对数据库的 Schema 进行压缩和优化，并通过一个自动投票系统对比不同生成的 SQL 执行结果，最终达到最优效果。

</details>


### [42] [一文说清 <em class="highlight">Agentic</em> AI：基于 LLM 的<em class="highlight">智能体</em>进化史](http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247772357&idx=1&sn=97f8eac17c36e7e55b602ba8a5633f9d&chksm=facc9684e23fe2f4c5896f9d5a70ac479382e496d6943fbf52eb8b0244742f7652abb639985e#rd)
*DataFunTalk*

Main category: wechat.article

TL;DR: 在开始这段技术演进之旅前，我们有必要先理解一个关键概念——Agent（智能体）。Agent 这个概念在人工智能领域由来已久，最早在 20 世纪 60 年代，“人工智能之父”马文·明斯基在他的研究中首次明确使用了“Agent”一词，将


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在开始这段技术演进之旅前，我们有必要先理解一个关键概念——Agent（智能体）。Agent 这个概念在人工智能领域由来已久，最早在 20 世纪 60 年代，“人工智能之父”马文·明斯基在他的研究中首次明确使用了“Agent”一词，将

</details>


### [43] [从 “算力供给” 到 “智能驱动”：<em class="highlight">Agentic</em> HPC 开启创新范式](http://mp.weixin.qq.com/s?__biz=MzI2NzMyNjI2Nw==&mid=2247649745&idx=2&sn=72b7acd74a5adab123937409fd41166d&chksm=eb3849b554aa868040fda6dde496514d2ac3d9af3cfc2a7aff8ede16d0e63c87346a73ca0690#rd)
*技术邻CAE学习*

Main category: wechat.article

TL;DR: Agentic HPC 平台Altair HPCWorks 平台助力企业快速捕捉核心价值，为下一代智能化、自优化高性能计算做好准备。该平台充分利用人工智能辅助功能（包括AI驱动的内存资源预测），精简作业提交流程，优化集群、云及混合计算环境的


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic HPC 平台Altair HPCWorks 平台助力企业快速捕捉核心价值，为下一代智能化、自优化高性能计算做好准备。该平台充分利用人工智能辅助功能（包括AI驱动的内存资源预测），精简作业提交流程，优化集群、云及混合计算环境的

</details>


### [44] [告别空谈：用<em class="highlight">Agentic</em> AI创造真实商业价值](http://mp.weixin.qq.com/s?__biz=MzA4ODMwMDcxMQ==&mid=2651233657&idx=1&sn=3632f8e55078eccd9b1bdbf81a45b582&chksm=8a4ac3eac32c1ba8f9ad3333703e6d87d24588a317aee80b178a568463ecfe14399a9ab22645#rd)
*亚马逊云科技*

Main category: wechat.article

TL;DR: 我们的目标是成为构建和部署全球最实用Agents的最佳选择，专注于Agentic AI能够带来最大商业价值的核心领域。质量与速度并重，Kiro加速软件创新开发团队一直面临着两难：既要快速交付，又不能牺牲质量。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们的目标是成为构建和部署全球最实用Agents的最佳选择，专注于Agentic AI能够带来最大商业价值的核心领域。质量与速度并重，Kiro加速软件创新开发团队一直面临着两难：既要快速交付，又不能牺牲质量。

</details>


### [45] [Google发布！一文了解21种<em class="highlight">Agentic</em>设计模式](http://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&mid=2650450337&idx=1&sn=343e6252dbad0b392e4f950d1d35b13c&chksm=bfa84e55f87a29e849724409ab5ce043f850bb94af8da41f7771edde13ab15127d396650b594#rd)
*AINLP*

Main category: wechat.article

TL;DR: 扫码回复“智能体设计”免费领取原著&中文版PDF如果你想写大模型论文，但却没有合适的idea，我收集整理了来自QS前50名校大佬的大模型研究思路！


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 扫码回复“智能体设计”免费领取原著&中文版PDF如果你想写大模型论文，但却没有合适的idea，我收集整理了来自QS前50名校大佬的大模型研究思路！

</details>


### [46] [IDC : AI 原生云/新型云厂商重构 <em class="highlight">Agentic</em> 基础设施](http://mp.weixin.qq.com/s?__biz=MzA3MjIwODMzNg==&mid=2457339432&idx=1&sn=89d0de43bc256d96dd60ea2b7ecd9398&chksm=896a3f1437edd2b04af0d8a1bf098da80105757b6d35f986bd36fdf620a374576bd1efbbecce#rd)
*求数科技*

Main category: wechat.article

TL;DR: 在 Agentic 时代，曾经相对线性的技术栈已经演变为动态、互联的生 态系统。这种变化不仅扩展了老牌企业的角色，也刺激很多企业跨界进入到 AI 基础设施 市场。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在 Agentic 时代，曾经相对线性的技术栈已经演变为动态、互联的生 态系统。这种变化不仅扩展了老牌企业的角色，也刺激很多企业跨界进入到 AI 基础设施 市场。

</details>


### [47] [当 AI 会“做事”时：从三张图看 <em class="highlight">Agentic</em> 时代的机会与警觉，普通人也要知道的底层逻辑](http://mp.weixin.qq.com/s?__biz=Mzk0MzY5ODcxOQ==&mid=2247486831&idx=1&sn=f442ba9b7160bda25fd9d9368e722c3b&chksm=c2dc1716d2fbfc53a8480f76bbc89ec09a792cb9b7b73b78b65654db3779def0aefd68b30ff8#rd)
*AI Encyclopedia*

Main category: wechat.article

TL;DR: 1）白板图：幽默化分层——从“深学得更深”到“生成式”再到“Agentic Maybe-AI”，带有SUSOXI，AI品牌，提醒我们别被概念热潮蒙蔽。2）同心圈信息图：详尽展示从人工智能核心概念、机器学习、神经网络、深度学习、生成式到


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1）白板图：幽默化分层——从“深学得更深”到“生成式”再到“Agentic Maybe-AI”，带有SUSOXI，AI品牌，提醒我们别被概念热潮蒙蔽。2）同心圈信息图：详尽展示从人工智能核心概念、机器学习、神经网络、深度学习、生成式到

</details>


### [48] [<em class="highlight">Agentic</em> AI 六大设计模式全解析](http://mp.weixin.qq.com/s?__biz=MzIwNTQzMTk5Mw==&mid=2247485077&idx=1&sn=0cf4596caae3960e0d8c5f80de9cec89&chksm=96d4113e06b816572b965a7a0c1dc3e47e8ddc31b358e2a566b1b94811df2821121972f7596a#rd)
*CodeAI研习社*

Main category: wechat.article

TL;DR: Agentic AI 六大设计模式全解析Agentic AI 设计模式总览面对越来越复杂的业务环境，单一 Agent 很难保持稳定输出。Agentic AI 已经形成六种高频范式，分别覆盖推理-行动闭环、代码执行、工具调用、自我反思、多智能体协作以及检索


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 六大设计模式全解析Agentic AI 设计模式总览面对越来越复杂的业务环境，单一 Agent 很难保持稳定输出。Agentic AI 已经形成六种高频范式，分别覆盖推理-行动闭环、代码执行、工具调用、自我反思、多智能体协作以及检索

</details>


### [49] [【科学智能】<em class="highlight">Agentic</em> AI 在科学发现中的应用：进展、挑战与未来方向](http://mp.weixin.qq.com/s?__biz=MzI3NDI4MzIyNQ==&mid=2247514250&idx=1&sn=a3422806b39e2f24f3c90074da8e0e81&chksm=eab3955c40a8098df14f7577b2b5bf34d7387bc1be72e24b5a07630a7121b68ba88cc577b705#rd)
*产业智能官*

Main category: wechat.article

TL;DR: 是一个由 gpt-4 驱动的自主 ai 智能体，能够规划、设计和执行化学实验。该系统集成了网络搜索、文档分析、代码执行和机器人自动化等模块，能够处理多步骤问题解决和数据驱动决策。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 是一个由 gpt-4 驱动的自主 ai 智能体，能够规划、设计和执行化学实验。该系统集成了网络搜索、文档分析、代码执行和机器人自动化等模块，能够处理多步骤问题解决和数据驱动决策。

</details>


### [50] [吴恩达 <em class="highlight">Agentic</em> AI（四）构建<em class="highlight">智能体</em>AI的实用技巧](http://mp.weixin.qq.com/s?__biz=MzI5MjQzOTY3OA==&mid=2247489012&idx=1&sn=b963965d35b037d863dbbcc6157b812e&chksm=ed8d9bf54ee3ad7019e712e996e61ea5171256033137f5e799a3733d985fc92127c0032efd5d#rd)
*精神抖擞王大鹏*

Main category: wechat.article

TL;DR: 但在 AI 系统特别是 Agentic 工作流的开发中，很难预先知道哪些地方会有效，哪些地方效果不好。这带来了一个根本性的转变：从预先规划到快速迭代。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 但在 AI 系统特别是 Agentic 工作流的开发中，很难预先知道哪些地方会有效，哪些地方效果不好。这带来了一个根本性的转变：从预先规划到快速迭代。

</details>


### [51] [告别简单的聊天机器人，<em class="highlight">Agentic</em> AI（<em class="highlight">代理</em>智能）如何重塑美国保险业？](http://mp.weixin.qq.com/s?__biz=MzkyMDc3OTEwMg==&mid=2247484483&idx=1&sn=1da8dbde145678a05d6b7af5e2dc558a&chksm=c0604bcf5cbb1b4aee3abe2d82d9ae697556cba0dab710f452fdb79c96bbe49d27e34c8baf17#rd)
*读研新解*

Main category: wechat.article

TL;DR: 术语解释：Agentic AI（代理人工智能）传统的ChatGPT是你问它答。而Agentic AI具备“行动力”。它可以自主规划并执行多步任务。例如：它不仅告诉你客户需要体检，还会自动去预约体检中心、追踪体检报告、分析结果，最后给出


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 术语解释：Agentic AI（代理人工智能）传统的ChatGPT是你问它答。而Agentic AI具备“行动力”。它可以自主规划并执行多步任务。例如：它不仅告诉你客户需要体检，还会自动去预约体检中心、追踪体检报告、分析结果，最后给出

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [When Structure Doesn't Help: LLMs Do Not Read Text-Attributed Graphs as Effectively as We Expected](https://arxiv.org/abs/2511.16767)
*Haotian Xu,Yuning You,Tengfei Ma*

Main category: cs.LG

TL;DR: 研究发现，在文本属性图上，仅使用节点文本描述的LLM已经表现出色，大多数结构编码策略带来的提升有限甚至负面，表明在强大语言模型时代需要重新思考结构表示的必要性。


<details>
  <summary>Details</summary>
Motivation: 探索不同图结构编码策略如何影响LLM在文本属性图上的性能，挑战传统图学习中结构必然有益的假设。

Method: 系统性地实验比较不同图结构编码策略，包括仅使用节点文本描述、模板化图表示和GNN编码结构信息等方法。

Result: 令人惊讶地发现：(i)仅使用节点文本描述的LLM在各项任务中已表现优异；(ii)大多数结构编码策略仅带来边际收益甚至负面影响。

Conclusion: 当使用强大语言模型时，显式的结构先验往往不必要甚至适得其反，这标志着与传统图学习范式的重大背离，需要重新思考LLM时代的结构表示方法。

Abstract: Graphs provide a unified representation of semantic content and relational structure, making them a natural fit for domains such as molecular modeling, citation networks, and social graphs. Meanwhile, large language models (LLMs) have excelled at understanding natural language and integrating cross-modal signals, sparking interest in their potential for graph reasoning. Recent work has explored this by either designing template-based graph templates or using graph neural networks (GNNs) to encode structural information. In this study, we investigate how different strategies for encoding graph structure affect LLM performance on text-attributed graphs. Surprisingly, our systematic experiments reveal that: (i) LLMs leveraging only node textual descriptions already achieve strong performance across tasks; and (ii) most structural encoding strategies offer marginal or even negative gains. We show that explicit structural priors are often unnecessary and, in some cases, counterproductive when powerful language models are involved. This represents a significant departure from traditional graph learning paradigms and highlights the need to rethink how structure should be represented and utilized in the LLM era. Our study is to systematically challenge the foundational assumption that structure is inherently beneficial for LLM-based graph reasoning, opening the door to new, semantics-driven approaches for graph learning.

</details>


### [53] [Why Do Language Model Agents Whistleblow?](https://arxiv.org/abs/2511.17085)
*Kushal Agrawal,Frank Xiao,Guido Bergman,Asa Cooper Stickland*

Main category: cs.LG

TL;DR: 该论文研究LLM作为工具使用代理时的举报行为，即模型在未经用户指示或知情的情况下向对话边界外的第三方披露可疑不当行为。作者开发了评估套件，发现举报频率因模型而异，任务复杂性降低举报倾向，道德提示增加举报率，提供更多工具和详细工作流程减少举报行为。


<details>
  <summary>Details</summary>
Motivation: 研究LLM作为工具代理时，其对齐训练会以新方式体现，特别是模型可能违背用户利益或明确指示使用工具，包括向第三方举报可疑不当行为。

Method: 引入包含多样化现实不当行为场景的评估套件，测试不同模型和设置下的举报行为，包括任务复杂性、系统提示、工具和工作流程的影响。

Result: 发现举报频率因模型家族差异很大；增加任务复杂性降低举报倾向；道德提示显著提高举报率；提供更多工具和详细工作流程减少举报行为；评估意识低于先前工作。

Conclusion: LLM作为工具代理时确实会表现出举报行为，这种行为受多种因素影响，包括模型类型、任务设置和系统提示等，需要进一步研究以确保模型行为符合用户意图。

Abstract: The deployment of Large Language Models (LLMs) as tool-using agents causes their alignment training to manifest in new ways. Recent work finds that language models can use tools in ways that contradict the interests or explicit instructions of the user. We study LLM whistleblowing: a subset of this behavior where models disclose suspected misconduct to parties beyond the dialog boundary (e.g., regulatory agencies) without user instruction or knowledge. We introduce an evaluation suite of diverse and realistic staged misconduct scenarios to assess agents for this behavior. Across models and settings, we find that: (1) the frequency of whistleblowing varies widely across model families, (2) increasing the complexity of the task the agent is instructed to complete lowers whistleblowing tendencies, (3) nudging the agent in the system prompt to act morally substantially raises whistleblowing rates, and (4) giving the model more obvious avenues for non-whistleblowing behavior, by providing more tools and a detailed workflow to follow, decreases whistleblowing rates. Additionally, we verify the robustness of our dataset by testing for model evaluation awareness, and find that both black-box methods and probes on model activations show lower evaluation awareness in our settings than in comparable previous work.

</details>


### [54] [Convergence and stability of Q-learning in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2511.17351)
*Massimiliano Manenti,Andrea Iannelli*

Main category: cs.LG

TL;DR: 本文提出了封建Q学习方案，研究了其收敛和稳定性条件，通过随机逼近理论和ODE方法证明了收敛性，并展示了该方法收敛到博弈均衡点。


<details>
  <summary>Details</summary>
Motivation: 分层强化学习在捕捉决策问题的时间结构和增强持续学习能力方面有潜力，但理论保证落后于实践。

Method: 使用封建Q学习方案，结合随机逼近理论和ODE方法进行收敛性分析。

Result: 证明了封建Q学习的收敛性和稳定性，实验支持理论预期结果。

Conclusion: 封建Q学习收敛到博弈均衡点，为分层强化学习的博弈论方法打开了大门。

Abstract: Hierarchical Reinforcement Learning promises, among other benefits, to efficiently capture and utilize the temporal structure of a decision-making problem and to enhance continual learning capabilities, but theoretical guarantees lag behind practice. In this paper, we propose a Feudal Q-learning scheme and investigate under which conditions its coupled updates converge and are stable. By leveraging the theory of Stochastic Approximation and the ODE method, we present a theorem stating the convergence and stability properties of Feudal Q-learning. This provides a principled convergence and stability analysis tailored to Feudal RL. Moreover, we show that the updates converge to a point that can be interpreted as an equilibrium of a suitably defined game, opening the door to game-theoretic approaches to Hierarchical RL. Lastly, experiments based on the Feudal Q-learning algorithm support the outcomes anticipated by theory.

</details>


### [55] [R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability](https://arxiv.org/abs/2511.17367)
*Runyu Lu,Ruochuan Shi,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.LG

TL;DR: 该论文提出了首个在部分可观测性下实现最坏情况鲁棒实时追捕策略的方法，通过信念保持机制和强化学习框架，在未知图结构上实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 当前追逃博弈中的最坏情况鲁棒策略计算耗时，特别是在考虑部分可观测性等现实因素时。现有强化学习方法局限于完美信息场景，且未考虑逃避者预测追捕者行动的情况。

Method: 首先证明传统动态规划算法在异步移动下保持最优性；然后提出关于逃避者可能位置的信念保持机制，将DP策略扩展到部分可观测设置；最后将信念保持嵌入EPG框架，通过跨图强化学习训练实时追捕策略。

Result: 强化学习后，策略在未见过的真实世界图结构上实现鲁棒零样本泛化，始终优于现有游戏RL方法直接在测试图上训练的策略。

Conclusion: 该方法成功解决了部分可观测追逃博弈中的实时鲁棒策略问题，在泛化性能上显著优于现有方法。

Abstract: Computing worst-case robust strategies in pursuit-evasion games (PEGs) is time-consuming, especially when real-world factors like partial observability are considered. While important for general security purposes, real-time applicable pursuit strategies for graph-based PEGs are currently missing when the pursuers only have imperfect information about the evader's position. Although state-of-the-art reinforcement learning (RL) methods like Equilibrium Policy Generalization (EPG) and Grasper provide guidelines for learning graph neural network (GNN) policies robust to different game dynamics, they are restricted to the scenario of perfect information and do not take into account the possible case where the evader can predict the pursuers' actions. This paper introduces the first approach to worst-case robust real-time pursuit strategies (R2PS) under partial observability. We first prove that a traditional dynamic programming (DP) algorithm for solving Markov PEGs maintains optimality under the asynchronous moves by the evader. Then, we propose a belief preservation mechanism about the evader's possible positions, extending the DP pursuit strategies to a partially observable setting. Finally, we embed the belief preservation into the state-of-the-art EPG framework to finish our R2PS learning scheme, which leads to a real-time pursuer policy through cross-graph reinforcement learning against the asynchronous-move DP evasion strategies. After reinforcement learning, our policy achieves robust zero-shot generalization to unseen real-world graph structures and consistently outperforms the policy directly trained on the test graphs by the existing game RL approach.

</details>


### [56] [PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM](https://arxiv.org/abs/2511.17467)
*Siqi Liang,Yudi Zhang,Yue Guo*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's "persona" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F

</details>


### [57] [Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization](https://arxiv.org/abs/2511.17489)
*Vinay Kanakeri,Shivam Bajaj,Ashwin Verma,Vijay Gupta,Aritra Mitra*

Main category: cs.LG

TL;DR: 提出一种结合聚类和学习的算法，用于多智能体线性二次调节器(LQR)设置，通过同时进行聚类和学习来为每个集群输出个性化策略，在保证正确聚类的同时获得统计增益。


<details>
  <summary>Details</summary>
Motivation: 强化学习通常数据效率低下，利用来自'近似相似'过程的数据可以提高样本效率，但由于过程模型未知，识别哪些过程相似具有挑战性。

Method: 结合顺序消除和零阶策略优化的思想，提出同时进行聚类和学习的算法，为每个集群输出个性化控制器。

Result: 在适当的集群分离条件下，证明方法能以高概率保证正确聚类，每个集群学习到的策略的次优性差距与集群大小成反比，且没有额外偏差。

Conclusion: 这是首个揭示聚类如何在数据驱动控制中用于学习个性化策略的工作，既能从协作中获得统计增益，又不会因包含不相似过程的数据而遭受次优性。

Abstract: It is known that reinforcement learning (RL) is data-hungry. To improve sample-efficiency of RL, it has been proposed that the learning algorithm utilize data from 'approximately similar' processes. However, since the process models are unknown, identifying which other processes are similar poses a challenge. In this work, we study this problem in the context of the benchmark Linear Quadratic Regulator (LQR) setting. Specifically, we consider a setting with multiple agents, each corresponding to a copy of a linear process to be controlled. The agents' local processes can be partitioned into clusters based on similarities in dynamics and tasks. Combining ideas from sequential elimination and zeroth-order policy optimization, we propose a new algorithm that performs simultaneous clustering and learning to output a personalized policy (controller) for each cluster. Under a suitable notion of cluster separation that captures differences in closed-loop performance across systems, we prove that our approach guarantees correct clustering with high probability. Furthermore, we show that the sub-optimality gap of the policy learned for each cluster scales inversely with the size of the cluster, with no additional bias, unlike in prior works on collaborative learning-based control. Our work is the first to reveal how clustering can be used in data-driven control to learn personalized policies that enjoy statistical gains from collaboration but do not suffer sub-optimality due to inclusion of data from dissimilar processes. From a distributed implementation perspective, our method is attractive as it incurs only a mild logarithmic communication overhead.

</details>
