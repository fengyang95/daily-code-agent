<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 32]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.AI](#cs.AI) [Total: 40]
- [cs.LG](#cs.LG) [Total: 16]
- [tldr.article](#tldr.article) [Total: 25]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Reinforcement Learning for Chain of Thought Compression with One-Domain-to-All Generalization](https://arxiv.org/abs/2601.06052)
*Hanyu Li,Jiangshan Duo,Bofei Gao,Hailin Zhang,Sujian Li,Xiaotie Deng,Liang Zhao*

Main category: cs.CL

TL;DR: 提出一种基于样本级软强化学习的思维链压缩方法，通过惩罚低效的长推理过程，在保持或提升准确率的同时减少20-40%的响应长度，并展现出跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的思维链推理存在"过度思考陷阱"，导致计算成本和延迟增加，而准确率提升不可靠。现有方法通常采用全局静态控制，可能惩罚必要的推理过程。

Method: 提出样本级软强化学习压缩方法，仅对模型已掌握且已产生更简洁推理的问题惩罚低效的长推理过程。采用后训练课程学习（准确率-压缩-准确率）策略。

Result: 方法减少平均响应长度20-40%，同时保持或提高准确率。压缩效果具有强跨领域泛化能力，在数学任务上训练的模型能自发缩短代码、指令遵循和通用知识问答等未见任务的响应。

Conclusion: 这种压缩方法应成为开发高效推理模型的标准阶段，最终能产生更准确且推理更简洁的模型。

Abstract: Chain-of-thought reasoning in large language models often creates an "overthinking trap," leading to excessive computational cost and latency for unreliable accuracy gains. Prior work has typically relied on global, static controls that risk penalizing necessary reasoning. We introduce a sample-level, soft reinforcement learning compression method that penalizes inefficiently long rollouts, but only on problems where the model has already mastered and already produced a more concise rollout. Our experiments show that this method reduces average response length by 20-40% with comparable or higher accuracy. Crucially, the compression exhibits strong cross-domain generalization; a model trained on math spontaneously shortens responses on unseen tasks like code, instruction following, and general knowledge QA, with stable or improved accuracy. We demonstrate a stable post-training curriculum (accuracy-compression-accuracy) that can ultimately produce models that are more accurate and reason more concisely, arguing that such compression method should be a standard phase in developing efficient reasoning models.

</details>


### [2] [Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning](https://arxiv.org/abs/2601.06282)
*Yue Zhou,Xiaobo Guo,Belhassen Bayar,Srinivasan H. Sengamedu*

Main category: cs.CL

TL;DR: Amory是一个工作记忆框架，通过离线时间增强代理推理来主动构建结构化记忆表示，显著提升长期对话代理的可扩展性和推理质量，同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 长期对话代理面临可扩展性挑战：重复处理整个对话历史计算成本过高。现有方法通过RAG风格检索将对话片段化为孤立嵌入或图表示，但记忆形成过于简化，无法捕捉人类记忆的微妙性和连贯性。

Method: Amory框架在离线时间主动构建结构化记忆：1) 将对话片段组织成情节叙事；2) 通过动量机制巩固记忆；3) 将外围事实语义化为语义记忆；4) 检索时在叙事结构上进行连贯性驱动的推理。

Result: 在LOCOMO长期推理基准测试中，Amory相比之前最先进方法有显著提升，性能与完整上下文推理相当，同时响应时间减少50%。动量感知巩固显著提升响应质量，连贯性驱动检索比基于嵌入的方法提供更好的记忆覆盖。

Conclusion: Amory通过主动构建结构化记忆表示，有效解决了长期对话代理的可扩展性挑战，在保持推理质量的同时大幅降低计算成本，为更自然、连贯的长期对话系统提供了新方向。

Abstract: Long-term conversational agents face a fundamental scalability challenge as interactions extend over time: repeatedly processing entire conversation histories becomes computationally prohibitive. Current approaches attempt to solve this through memory frameworks that predominantly fragment conversations into isolated embeddings or graph representations and retrieve relevant ones in a RAG style. While computationally efficient, these methods often treat memory formation minimally and fail to capture the subtlety and coherence of human memory. We introduce Amory, a working memory framework that actively constructs structured memory representations through enhancing agentic reasoning during offline time. Amory organizes conversational fragments into episodic narratives, consolidates memories with momentum, and semanticizes peripheral facts into semantic memory. At retrieval time, the system employs coherence-driven reasoning over narrative structures. Evaluated on the LOCOMO benchmark for long-term reasoning, Amory achieves considerable improvements over previous state-of-the-art, with performance comparable to full context reasoning while reducing response time by 50%. Analysis shows that momentum-aware consolidation significantly enhances response quality, while coherence-driven retrieval provides superior memory coverage compared to embedding-based approaches.

</details>


### [3] [Value of Information: A Framework for Human-Agent Communication](https://arxiv.org/abs/2601.06407)
*Yijiang River Dong,Tiancheng Hu,Zheng Hui,Caiqi Zhang,Ivan Vulić,Andreea Bobu,Nigel Collier*

Main category: cs.CL

TL;DR: 提出基于信息价值(VoI)的决策理论框架，让LLM智能体能在信息不足时动态权衡询问用户澄清问题的收益与认知成本，无需超参数调优即可自适应不同场景。


<details>
  <summary>Details</summary>
Motivation: 现实世界任务中，用户请求通常不完整，智能体面临两难：要么基于不完整信息行动，要么打断用户澄清。现有方法要么依赖脆弱的置信度阈值需要任务特定调优，要么无法考虑不同决策的风险差异。

Method: 引入基于信息价值(VoI)的决策理论框架，在推理时动态计算询问问题的预期效用增益与用户认知成本的权衡，无需超参数调优，可自适应不同上下文。

Result: 在四个不同领域（20 Questions游戏、医疗诊断、航班预订、电子商务）的实验中，VoI方法始终匹配或超过最佳手动调优基线，在高成本设置中实现高达1.36个效用点的提升。

Conclusion: 该工作提供了一个无参数框架，用于自适应智能体通信，明确平衡任务风险、查询模糊性和用户努力，解决了LLM智能体在现实世界部署中的核心困境。

Abstract: Large Language Model (LLM) agents deployed for real-world tasks face a fundamental dilemma: user requests are underspecified, yet agents must decide whether to act on incomplete information or interrupt users for clarification. Existing approaches either rely on brittle confidence thresholds that require task-specific tuning, or fail to account for the varying stakes of different decisions. We introduce a decision-theoretic framework that resolves this trade-off through the Value of Information (VoI), enabling agents to dynamically weigh the expected utility gain from asking questions against the cognitive cost imposed on users. Our inference-time method requires no hyperparameter tuning and adapts seamlessly across contexts-from casual games to medical diagnosis. Experiments across four diverse domains (20 Questions, medical diagnosis, flight booking, and e-commerce) show that VoI consistently matches or exceeds the best manually-tuned baselines, achieving up to 1.36 utility points higher in high-cost settings. This work provides a parameter-free framework for adaptive agent communication that explicitly balances task risk, query ambiguity, and user effort.

</details>


### [4] [Structured Episodic Event Memory](https://arxiv.org/abs/2601.06411)
*Zhengxuan Lu,Dongfang Li,Yukun Shi,Beilun Wang,Longyue Wang,Baotian Hu*

Main category: cs.CL

TL;DR: SEEM提出了一种结构化情景事件记忆框架，结合图记忆层和动态情景记忆层，通过认知框架理论将交互流转化为结构化事件框架，显著提升了自主代理的叙事连贯性和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM记忆方法主要依赖静态检索增强生成(RAG)，存在检索分散、无法捕捉复杂推理所需的结构依赖关系的问题。对于自主代理而言，这些被动扁平架构缺乏对长期交互动态关联性的认知组织能力。

Method: 提出结构化情景事件记忆(SEEM)框架：1) 结合图记忆层存储关系事实和动态情景记忆层记录叙事进展；2) 基于认知框架理论将交互流转化为结构化情景事件框架(EEFs)，包含精确来源指针；3) 引入代理关联融合和反向来源扩展(RPE)机制，从碎片化证据重建连贯叙事上下文。

Result: 在LoCoMo和LongMemEval基准测试中，SEEM显著优于基线方法，使代理能够保持更优的叙事连贯性和逻辑一致性。

Conclusion: SEEM通过结构化分层记忆框架有效解决了传统RAG方法的局限性，为自主代理提供了更接近人类认知组织的记忆机制，提升了复杂推理和长期交互能力。

Abstract: Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.

</details>


### [5] [Can a Unimodal Language Agent Provide Preferences to Tune a Multimodal Vision-Language Model?](https://arxiv.org/abs/2601.06424)
*Sazia Tabasum Mim,Jack Morris,Manish Dhakal,Yanming Xiu,Maria Gorlatova,Yi Ding*

Main category: cs.CL

TL;DR: 本文提出一种方法，让单模态语言模型（LLM）能够为视觉语言模型（VLM）提供反馈，优化多模态描述生成，从而增强LLM对多模态上下文的理解。


<details>
  <summary>Details</summary>
Motivation: 探索为现有LLM添加多模态能力的可扩展路径，研究单模态LLM能否仅通过文本来推理自身信息需求，并为多模态模型提供有效反馈。

Method: 提出一种方法，使语言智能体能够向视觉语言模型（VLM）提供反馈，根据智能体偏好调整文本生成。通过LLM偏好反馈优化VLM的多模态场景描述生成。

Result: LLM偏好反馈显著提升VLM描述质量，相比基线多模态方法，绝对准确率最大提升13%。人类研究显示LLM选择与人类判断的偏好对齐率达到64.6%。

Conclusion: 单模态LLM能够有效指导VLM优化多模态描述生成，为LLM添加多模态能力提供了一条可扩展路径，但方法仍存在局限性。

Abstract: To explore a more scalable path for adding multimodal capabilities to existing LLMs, this paper addresses a fundamental question: Can a unimodal LLM, relying solely on text, reason about its own informational needs and provide effective feedback to optimize a multimodal model? To answer this, we propose a method that enables a language agent to give feedback to a vision-language model (VLM) to adapt text generation to the agent's preferences. Our results from different experiments affirm this hypothesis, showing that LLM preference feedback significantly enhances VLM descriptions. Using our proposed method, we find that the VLM can generate multimodal scene descriptions to help the LLM better understand multimodal context, leading to improvements of maximum 13% in absolute accuracy compared to the baseline multimodal approach. Furthermore, a human study validated our AI-driven feedback, showing a 64.6% preference alignment rate between the LLM's choices and human judgments. Extensive experiments provide insights on how and why the method works and its limitations.

</details>


### [6] [Spec-o3: A Tool-Augmented Vision-Language Agent for Rare Celestial Object Candidate Vetting via Automated Spectral Inspection](https://arxiv.org/abs/2601.06498)
*Minghui Jia,Qichao Zhang,Ali Luo,Linjing Li,Shuo Ye,Hailing Lu,Wen Hou,Dongbin Zhao*

Main category: cs.CL

TL;DR: Spec-o3是一个工具增强的视觉语言代理，通过多模态链式推理进行天文学家对齐的光谱检查，显著提高了罕见天体识别的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习分类器在泛化性和可解释性方面存在局限，罕见天体候选者的最终审查仍依赖专家视觉检查，这一人工密集型过程无法应对现代光谱巡天数据洪流的挑战。

Method: 提出Spec-o3工具增强视觉语言代理，采用交错多模态链式推理。使用两阶段后训练方法：基于专家检查轨迹的冷启动监督微调，然后在罕见类型验证任务上进行基于结果的强化学习。

Result: 在LAMOST的五个罕见天体识别任务中，Spec-o3建立了新的SOTA，将macro-F1分数从28.3提升到76.5（使用7B参数基础模型），优于专有VLMs和专用深度模型。在跨巡天（从LAMOST到SDSS/DESI）的未见检查任务中表现出强泛化能力。

Conclusion: Spec-o3通过天文学家对齐的光谱检查，实现了透明可信的决策，其推理轨迹连贯且物理一致，为解决光谱数据洪流中的专家瓶颈问题提供了有效方案。

Abstract: Due to the limited generalization and interpretability of deep learning classifiers, The final vetting of rare celestial object candidates still relies on expert visual inspection--a manually intensive process. In this process, astronomers leverage specialized tools to analyze spectra and construct reliable catalogs. However, this practice has become the primary bottleneck, as it is fundamentally incapable of scaling with the data deluge from modern spectroscopic surveys. To bridge this gap, we propose Spec-o3, a tool-augmented vision-language agent that performs astronomer-aligned spectral inspection via interleaved multimodal chain-of-thought reasoning. Spec-o3 is trained with a two-stage post-training recipe: cold-start supervised fine-tuning on expert inspection trajectories followed by outcome-based reinforcement learning on rare-type verification tasks. Evaluated on five rare-object identification tasks from LAMOST, Spec-o3 establishes a new State-of-the-Art, boosting the macro-F1 score from 28.3 to 76.5 with a 7B parameter base model and outperforming both proprietary VLMs and specialized deep models. Crucially, the agent demonstrates strong generalization to unseen inspection tasks across survey shifts (from LAMOST to SDSS/DESI). Expert evaluations confirm that its reasoning traces are coherent and physically consistent, supporting transparent and trustworthy decision-making. Code, data, and models are available at \href{https://github.com/Maxwell-Jia/spec-o3}{Project HomePage}.

</details>


### [7] [SimLLM: Fine-Tuning Code LLMs for SimPy-Based Queueing System Simulation](https://arxiv.org/abs/2601.06543)
*Jun-Qi Chen,Kun Zhang,Rui Zheng,Ying Zhong*

Main category: cs.CL

TL;DR: 研究人员微调了两个开源LLM（Qwen-Coder-7B和DeepSeek-Coder-6.7B），通过多阶段微调框架提升它们在SimPy排队模拟代码生成方面的性能，为闭源模型提供了实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 闭源LLM（如GPT-4o）生成SimPy排队模拟代码存在计算成本高和数据隐私问题，需要开发开源替代方案。

Method: 采用多阶段微调框架：两阶段监督微调（SFT）和一阶段直接偏好优化（DPO），在精选的SimPy排队数据上微调两个开源LLM。

Result: 微调后的模型在可执行性、输出格式符合度和指令代码一致性方面均有显著提升，能够可靠地生成SimPy模拟代码。

Conclusion: 领域特定微调可以将紧凑的开源代码模型转化为可靠的SimPy模拟生成器，为教育、研究和运营决策支持提供闭源LLM的实用替代方案。

Abstract: The Python package SimPy is widely used for modeling queueing systems due to its flexibility, simplicity, and smooth integration with modern data analysis and optimization frameworks. Recent advances in large language models (LLMs) have shown strong ability in generating clear and executable code, making them powerful and suitable tools for writing SimPy queueing simulation code. However, directly employing closed-source models like GPT-4o to generate such code may lead to high computational costs and raise data privacy concerns. To address this, we fine-tune two open-source LLMs, Qwen-Coder-7B and DeepSeek-Coder-6.7B, on curated SimPy queueing data, which enhances their code-generating performance in executability, output-format compliance, and instruction-code consistency. Particularly, we proposed a multi-stage fine-tuning framework comprising two stages of supervised fine-tuning (SFT) and one stage of direct preference optimization (DPO), progressively enhancing the model's ability in SimPy-based queueing simulation code generation. Extensive evaluations demonstrate that both fine-tuned models achieve substantial improvements in executability, output-format compliance, and instruct consistency. These results confirm that domain-specific fine-tuning can effectively transform compact open-source code models into reliable SimPy simulation generators which provide a practical alternative to closed-source LLMs for education, research, and operational decision support.

</details>


### [8] [MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis](https://arxiv.org/abs/2601.06636)
*Wenting Chen,Zhongrui Zhu,Guolin Huang,Wenxuan Wang*

Main category: cs.CL

TL;DR: 论文提出MedEinst基准测试，用于检测LLMs在临床诊断中的Einstellung效应（依赖统计捷径而非患者特异性证据），并开发ECR-Agent通过结构化推理和迭代优化来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在医学基准测试中取得高准确率，但在临床诊断中表现出Einstellung效应——依赖统计捷径而非患者特异性证据，导致非典型病例误诊。现有基准测试无法检测这一关键失败模式。

Method: 1. 引入MedEinst反事实基准测试，包含5,383对临床病例，每对包含控制病例和"陷阱"病例（改变鉴别性证据以翻转诊断）。2. 提出ECR-Agent，包含动态因果推理（DCI）和批评驱动的图与记忆演化（CGME）两个组件，通过结构化推理和迭代优化来对齐循证医学标准。

Result: 对17个LLMs的广泛评估显示，前沿模型在基准测试中取得高准确率，但具有严重的偏见陷阱率。ECR-Agent通过结构化推理和迭代优化，有效减少了Einstellung效应。

Conclusion: LLMs在临床诊断中存在严重的Einstellung效应，现有基准测试无法检测。MedEinst基准测试能有效识别此问题，而ECR-Agent通过结构化推理和迭代优化，显著改善了LLMs的临床诊断能力。

Abstract: Despite achieving high accuracy on medical benchmarks, LLMs exhibit the Einstellung Effect in clinical diagnosis--relying on statistical shortcuts rather than patient-specific evidence, causing misdiagnosis in atypical cases. Existing benchmarks fail to detect this critical failure mode. We introduce MedEinst, a counterfactual benchmark with 5,383 paired clinical cases across 49 diseases. Each pair contains a control case and a "trap" case with altered discriminative evidence that flips the diagnosis. We measure susceptibility via Bias Trap Rate--probability of misdiagnosing traps despite correctly diagnosing controls. Extensive Evaluation of 17 LLMs shows frontier models achieve high baseline accuracy but severe bias trap rates. Thus, we propose ECR-Agent, aligning LLM reasoning with Evidence-Based Medicine standard via two components: (1) Dynamic Causal Inference (DCI) performs structured reasoning through dual-pathway perception, dynamic causal graph reasoning across three levels (association, intervention, counterfactual), and evidence audit for final diagnosis; (2) Critic-Driven Graph and Memory Evolution (CGME) iteratively refines the system by storing validated reasoning paths in an exemplar base and consolidating disease-specific knowledge into evolving illness graphs. Source code is to be released.

</details>


### [9] [EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs](https://arxiv.org/abs/2601.06786)
*Jewon Yeom,Jaewon Sok,Seonghyeon Park,Jeongjae Park,Taesup Kim*

Main category: cs.CL

TL;DR: 提出EpiCaR方法，通过联合优化推理性能和校准，解决LLM在迭代自训练中过度自信、丧失不确定性的问题，实现准确性和校准的帕累托改进。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理能力提升方法主要依赖迭代自训练，虽然能提高准确性，但会导致模型过度自信、丧失不确定性表示能力，形成校准成本问题。

Method: 提出认知校准推理(EpiCaR)训练目标，将推理训练重构为认知学习问题，联合优化推理性能和校准，在迭代监督微调框架中使用显式自评估信号。

Result: 在Llama-3和Qwen-3系列模型上，EpiCaR在准确性和校准方面均优于基线方法，特别是在3B+参数模型中。在OOD数学推理和代码生成任务上有效泛化，推理计算减少3倍。

Conclusion: EpiCaR通过联合优化推理性能和校准，解决了LLM迭代训练中的模型崩溃问题，实现了准确性和校准的帕累托改进，显著降低推理计算成本。

Abstract: Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K=30 performance of STaR with only K=10 samples in capable models.

</details>


### [10] [AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents](https://arxiv.org/abs/2601.06818)
*Xuannan Liu,Xiao Yang,Zekun Li,Peipei Li,Ran He*

Main category: cs.CL

TL;DR: 提出了AgentHallu基准，用于自动化归因LLM智能体在多步推理中的幻觉问题，包含693条轨迹、5类幻觉分类和人工标注，评估显示当前模型在步骤定位上表现不佳（最佳仅41.1%准确率）。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在多步推理中，中间步骤的幻觉会沿轨迹传播，降低整体可靠性。现有研究主要关注单轮响应的幻觉检测，缺乏对多步工作流中幻觉起源的诊断方法。

Method: 提出了"自动化幻觉归因"新任务，并构建AgentHallu基准：包含693条高质量轨迹（覆盖7种智能体框架和5个领域）、5大类14子类的幻觉分类体系，以及人工标注的多级标签（二值标签、责任步骤、因果解释）。评估了13个领先模型。

Result: 任务具有挑战性，即使是顶级模型（如GPT-5、Gemini-2.5-Pro）表现也不佳。最佳模型在步骤定位准确率仅41.1%，其中工具使用类幻觉最困难（仅11.6%准确率）。

Conclusion: AgentHallu基准将推动未来开发更鲁棒、透明和可靠的智能体系统，自动化幻觉归因是提升智能体可靠性的关键研究方向。

Abstract: As LLM-based agents operate over sequential multi-step reasoning, hallucinations arising at intermediate steps risk propagating along the trajectory, thus degrading overall reliability. Unlike hallucination detection in single-turn responses, diagnosing hallucinations in multi-step workflows requires identifying which step causes the initial divergence. To fill this gap, we propose a new research task, automated hallucination attribution of LLM-based agents, aiming to identify the step responsible for the hallucination and explain why. To support this task, we introduce AgentHallu, a comprehensive benchmark with: (1) 693 high-quality trajectories spanning 7 agent frameworks and 5 domains, (2) a hallucination taxonomy organized into 5 categories (Planning, Retrieval, Reasoning, Human-Interaction, and Tool-Use) and 14 sub-categories, and (3) multi-level annotations curated by humans, covering binary labels, hallucination-responsible steps, and causal explanations. We evaluate 13 leading models, and results show the task is challenging even for top-tier models (like GPT-5, Gemini-2.5-Pro). The best-performing model achieves only 41.1\% step localization accuracy, where tool-use hallucinations are the most challenging at just 11.6\%. We believe AgentHallu will catalyze future research into developing robust, transparent, and reliable agentic systems.

</details>


### [11] [BiasLab: A Multilingual, Dual-Framing Framework for Robust Measurement of Output-Level Bias in Large Language Models](https://arxiv.org/abs/2601.06861)
*William Guey,Wei Zhang,Pei-Luen Patrick Rau,Pierrick Bougault,Vitor D. de Moura,Bertan Ucar,Jose O. Gomes*

Main category: cs.CL

TL;DR: BiasLab是一个开源、模型无关的评估框架，用于通过多语言、鲁棒性导向的实验设计量化LLM输出层面的偏见。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地部署在高风险环境中，其输出会影响现实世界的决策。然而，评估LLM输出中的偏见在方法论上具有挑战性，因为存在对提示词措辞的敏感性、有限的多语言覆盖范围，以及缺乏能够实现跨模型可靠比较的标准化指标。

Method: BiasLab采用严格的二元框架方案构建镜像探针对：一个肯定断言偏向目标A，另一个通过确定性目标替换获得的反向断言偏向目标B，同时保持相同的语言结构。为了减少对提示模板的依赖，在随机指令包装下进行重复评估，并强制执行固定选择的Likert响应格式。响应通过基于LLM的评判器归一化为同意标签，跨框架进行极性一致性对齐，并聚合成具有效应大小和中性率等描述性统计的定量偏见指标。

Result: 该框架支持跨多种偏见轴（包括人口统计、文化、政治和地缘政治主题）进行评估，并生成可重复的工件，如结构化报告和比较可视化。

Conclusion: BiasLab为跨语言和框架敏感的偏见测量提供了一种标准化方法，补充了内在和基于数据集的审计，使研究人员和机构能够进行基准测试并做出更明智的部署决策。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes contexts where their outputs influence real-world decisions. However, evaluating bias in LLM outputs remains methodologically challenging due to sensitivity to prompt wording, limited multilingual coverage, and the lack of standardized metrics that enable reliable comparison across models. This paper introduces BiasLab, an open-source, model-agnostic evaluation framework for quantifying output-level (extrinsic) bias through a multilingual, robustness-oriented experimental design. BiasLab constructs mirrored probe pairs under a strict dual-framing scheme: an affirmative assertion favoring Target A and a reverse assertion obtained by deterministic target substitution favoring Target B, while preserving identical linguistic structure. To reduce dependence on prompt templates, BiasLab performs repeated evaluation under randomized instructional wrappers and enforces a fixed-choice Likert response format to maximize comparability across models and languages. Responses are normalized into agreement labels using an LLM-based judge, aligned for polarity consistency across framings, and aggregated into quantitative bias indicators with descriptive statistics including effect sizes and neutrality rates. The framework supports evaluation across diverse bias axes, including demographic, cultural, political, and geopolitical topics, and produces reproducible artifacts such as structured reports and comparative visualizations. BiasLab contributes a standardized methodology for cross-lingual and framing-sensitive bias measurement that complements intrinsic and dataset-based audits, enabling researchers and institutions to benchmark robustness and make better-informed deployment decisions.

</details>


### [12] [Distributional Clarity: The Hidden Driver of RL-Friendliness in Large Language Models](https://arxiv.org/abs/2601.06911)
*Shaoning Sun,Mingzhu Cai,Huang He,Bingjin Chen,Siqi Bao,Yujiu Yang,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 研究发现语言模型在强化学习中的表现差异源于其概率空间的"分布清晰度"特性，通过轮廓系数量化这一特性，并提出基于此的重新加权策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 不同语言模型家族在强化学习训练中表现差异显著，有些模型能获得显著提升，而另一些则改进有限。本文旨在探究这种差异背后的结构性原因，而非仅仅关注数据层面的因素。

Method: 采用三阶段分析：从现象到机制再到解释。首先观察模型表现差异，然后通过轮廓系数量化模型在正确与错误回答上的概率分布清晰度，最后提出基于轮廓系数的重新加权训练策略，优先训练低清晰度样本。

Result: 在六个数学基准测试上的实验表明，高轮廓系数与强化学习性能强相关，低轮廓系数与严重逻辑错误和推理不稳定性相关。提出的重新加权策略在所有模型家族中都带来一致改进，在AIME24上最高提升5.9分。

Conclusion: 分布清晰度是影响语言模型强化学习友好性的基本可训练属性，轮廓系数可作为有效的量化指标，基于此的重新加权策略能显著提升模型性能。

Abstract: Language model families exhibit striking disparity in their capacity to benefit from reinforcement learning: under identical training, models like Qwen achieve substantial gains, while others like Llama yield limited improvements. Complementing data-centric approaches, we reveal that this disparity reflects a hidden structural property: \textbf{distributional clarity} in probability space. Through a three-stage analysis-from phenomenon to mechanism to interpretation-we uncover that RL-friendly models exhibit intra-class compactness and inter-class separation in their probability assignments to correct vs. incorrect responses. We quantify this clarity using the \textbf{Silhouette Coefficient} ($S$) and demonstrate that (1) high $S$ correlates strongly with RL performance; (2) low $S$ is associated with severe logic errors and reasoning instability. To confirm this property, we introduce a Silhouette-Aware Reweighting strategy that prioritizes low-$S$ samples during training. Experiments across six mathematical benchmarks show consistent improvements across all model families, with gains up to 5.9 points on AIME24. Our work establishes distributional clarity as a fundamental, trainable property underlying RL-Friendliness.

</details>


### [13] [TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG](https://arxiv.org/abs/2601.06922)
*Tianhua Zhang,Kun Li,Junan Li,Yunxiang Li,Hongyin Luo,Xixin Wu,James Glass,Helen Meng*

Main category: cs.CL

TL;DR: 提出TreePS-RAG框架，通过树形结构实现多步推理的细粒度信用分配，无需中间标注，在多个QA基准上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的agentic RAG方法仅依赖稀疏的最终奖励，限制了步级信用分配，对中间推理和动作的指导较弱。现有过程级监督方法要么依赖离线训练数据（存在分布偏移风险），要么需要昂贵的中间标注。

Method: 提出TreePS-RAG框架：1) 将agentic RAG推理建模为展开树，每个推理步骤对应一个节点；2) 通过蒙特卡洛估计后代结果来估计步骤效用，实现细粒度过程优势；3) 引入高效的在线树构建策略，在有限计算预算下保持探索多样性。

Result: 在7个多跳和通用QA基准测试中，TreePS-RAG在多个模型规模上一致且显著优于基于结果监督和领先的过程监督RL方法，且展开成本与Search-R1等强基线相当。

Conclusion: TreePS-RAG通过树形结构和在线蒙特卡洛估计，实现了有效的步级信用分配，无需中间标注，为agentic RAG提供了更精细的强化学习框架。

Abstract: Agentic retrieval-augmented generation (RAG) formulates question answering as a multi-step interaction between reasoning and information retrieval, and has recently been advanced by reinforcement learning (RL) with outcome-based supervision. While effective, relying solely on sparse final rewards limits step-wise credit assignment and provides weak guidance for intermediate reasoning and actions. Recent efforts explore process-level supervision, but typically depend on offline constructed training data, which risks distribution shift, or require costly intermediate annotations. We present TreePS-RAG, an online, tree-based RL framework for agentic RAG that enables step-wise credit assignment while retaining standard outcome-only rewards. Our key insight is to model agentic RAG reasoning as a rollout tree, where each reasoning step naturally maps to a node. This tree structure allows step utility to be estimated via Monte Carlo estimation over its descendant outcomes, yielding fine-grained process advantages without requiring intermediate labels. To make this paradigm practical, we introduce an efficient online tree construction strategy that preserves exploration diversity under a constrained computational budget. With a rollout cost comparable to strong baselines like Search-R1, experiments on seven multi-hop and general QA benchmarks across multiple model scales show that TreePS-RAG consistently and significantly outperforms both outcome-supervised and leading process-supervised RL methods.

</details>


### [14] [X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests](https://arxiv.org/abs/2601.06953)
*Jie Wu,Haoling Li,Xin Zhang,Jiani Guo,Jane Luo,Steven Liu,Yangyu Huang,Ruihang Chu,Scarlett Li,Yujiu Yang*

Main category: cs.CL

TL;DR: 论文提出完全合成数据训练Code LLMs的方法SynthSmith，通过特征合成生成多样化编程任务、解决方案和测试用例，训练出的X-Coder系列模型在LiveCodeBench上超越更大参数模型。


<details>
  <summary>Details</summary>
Motivation: 当前Code LLMs严重依赖真实世界数据，限制了可扩展性。竞争性编程对代码推理要求高，需要解决数据依赖问题。

Method: 提出SynthSmith数据合成管道，基于特征合成生成完全合成的编程任务、解决方案和测试用例。使用合成数据进行监督微调和强化学习训练，引入X-Coder模型系列。

Result: X-Coder系列（7B参数）在LiveCodeBench v5上达到62.9 avg@8，v6上达到55.8，超越DeepCoder-14B-Preview和AReal-boba2-14B。发现合成数据上的缩放定律成立。

Conclusion: 扩展高质量合成数据和采用分阶段训练可以显著提升代码推理能力，同时减少对真实世界编码数据的依赖。

Abstract: Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.

</details>


### [15] [RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction](https://arxiv.org/abs/2601.06966)
*Haonan Bian,Zhiyuan Yao,Sen Hu,Zishan Xu,Shaolei Zhang,Yifu Guo,Ziliang Yang,Xueran Han,Huacan Wang,Ronghao Chen*

Main category: cs.CL

TL;DR: RealMem是首个面向长期项目导向交互的记忆基准测试，包含2000+跨会话对话，揭示当前记忆系统在管理真实项目状态和动态上下文依赖方面面临重大挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLM从静态对话接口发展为自主通用智能体，有效记忆对保持长期一致性至关重要。现有基准主要关注闲聊或任务导向对话，未能捕捉"长期项目导向"交互中智能体需要跟踪演化目标的需求。

Method: 提出RealMem基准，包含11个场景的2000+跨会话对话，使用自然用户查询进行评估。开发了包含项目基础构建、多智能体对话生成、记忆与日程管理的合成流水线，模拟记忆的动态演化。

Result: 实验表明，当前记忆系统在管理长期项目状态和真实世界项目固有的动态上下文依赖方面面临显著挑战。

Conclusion: RealMem填补了长期项目导向记忆评估的空白，为开发更强大的记忆系统提供了重要基准，代码和数据集已开源。

Abstract: As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **"long-term project-oriented"** interactions where agents must track evolving goals.
  To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation.
  We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects.
  Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).

</details>


### [16] [LLMs Can't Play Hangman: On the Necessity of a Private Working Memory for Language Agents](https://arxiv.org/abs/2601.06973)
*Davide Baldelli,Ali Parviz,Amal Zouaq,Sarath Chandar*

Main category: cs.CL

TL;DR: 论文提出私有状态交互任务(PSITs)概念，证明标准聊天界面无法同时保持秘密性和一致性，提出包含显式私有工作记忆的新架构来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM从文本补全转向自主智能体，标准聊天界面缺乏私有工作记忆的限制变得突出。需要研究智能体能否可靠执行依赖隐藏状态的交互任务。

Method: 1. 定义私有状态交互任务(PSITs)；2. 理论上证明标准聊天界面无法同时保持秘密性和一致性；3. 提出自一致性测试协议评估智能体在分叉对话分支中维护隐藏秘密的能力；4. 提出包含显式私有工作记忆的新架构。

Result: 1. 理论上证明标准聊天界面在PSITs中存在不可能性定理；2. 经验验证标准聊天LLM和基于检索的记忆基线都无法通过自一致性测试；3. 提出的显式私有工作记忆架构能够恢复一致性。

Conclusion: 私有状态是交互式语言智能体的必要组件，显式私有工作记忆架构能够解决标准聊天界面在维护隐藏状态方面的局限性。

Abstract: As LLMs move from text completion toward autonomous agents, they remain constrained by the standard chat interface, which lacks private working memory. This raises a fundamental question: can agents reliably perform interactive tasks that depend on hidden state? We define Private State Interactive Tasks (PSITs), which require agents to generate and maintain hidden information while producing consistent public responses. We show theoretically that any agent restricted to the public conversation history cannot simultaneously preserve secrecy and consistency in PSITs, yielding an impossibility theorem. To empirically validate this limitation, we introduce a self-consistency testing protocol that evaluates whether agents can maintain a hidden secret across forked dialogue branches. Standard chat-based LLMs and retrieval-based memory baselines fail this test regardless of scale, demonstrating that semantic retrieval does not enable true state maintenance. To address this, we propose a novel architecture incorporating an explicit private working memory; we demonstrate that this mechanism restores consistency, establishing private state as a necessary component for interactive language agents.

</details>


### [17] [Mid-Think: Training-Free Intermediate-Budget Reasoning via Token-Level Triggers](https://arxiv.org/abs/2601.07036)
*Wang Yang,Debargha Ganguly,Xinpeng Li,Chaoda Song,Shouren Wang,Vikash Singh,Vipin Chaudhary,Xiaotian Han*

Main category: cs.CL

TL;DR: 研究发现混合推理语言模型中的Think/No-think指令切换主要由少量触发token驱动而非指令本身，提出Mid-Think训练免费提示格式，在推理控制和强化学习训练中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有混合推理语言模型通过高层Think/No-think指令控制推理行为，但研究发现这种模式切换主要由少量触发token驱动而非指令本身，需要更有效的推理控制方法。

Method: 通过注意力分析和受控提示实验识别关键触发token，提出Mid-Think训练免费提示格式，结合这些触发token实现中间预算推理，并将其应用于SFT后的RL训练。

Result: Mid-Think在准确率-长度权衡方面优于固定token和基于提示的基线方法；应用于RL训练后减少约15%训练时间，Qwen3-8B在AIME上从69.8%提升至72.4%，在GPQA上从58.5%提升至61.1%。

Conclusion: 模型推理行为主要由特定触发token控制而非高层指令，Mid-Think格式在推理时间控制和基于RL的推理训练中均有效，为高效推理控制提供了新方法。

Abstract: Hybrid reasoning language models are commonly controlled through high-level Think/No-think instructions to regulate reasoning behavior, yet we found that such mode switching is largely driven by a small set of trigger tokens rather than the instructions themselves. Through attention analysis and controlled prompting experiments, we show that a leading ``Okay'' token induces reasoning behavior, while the newline pattern following ``</think>'' suppresses it. Based on this observation, we propose Mid-Think, a simple training-free prompting format that combines these triggers to achieve intermediate-budget reasoning, consistently outperforming fixed-token and prompt-based baselines in terms of the accuracy-length trade-off. Furthermore, applying Mid-Think to RL training after SFT reduces training time by approximately 15% while improving final performance of Qwen3-8B on AIME from 69.8% to 72.4% and on GPQA from 58.5% to 61.1%, demonstrating its effectiveness for both inference-time control and RL-based reasoning training.

</details>


### [18] [Measuring Iterative Temporal Reasoning with TimePuzzles](https://arxiv.org/abs/2601.07148)
*Zhengxiang Wang,Zeyu Dong*

Main category: cs.CL

TL;DR: TimePuzzles是一个基于约束的日期推理任务，用于评估迭代时间推理能力。该任务结合事实时间锚点和跨文化日历关系，通过算法生成可控、动态和持续评估的谜题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在时间推理方面存在局限，需要一种简单、成本效益高的诊断工具来评估模型在工具增强的迭代时间推理能力。

Method: 创建TimePuzzles任务，结合事实时间锚点和跨文化日历关系，算法生成包含一个或多个有效解日期的谜题。评估了13个不同LLM，测试了纯模型、网络搜索和代码解释器三种设置。

Result: GPT-5仅达到49.3%准确率，其他模型均低于31%。网络搜索带来显著提升，代码解释器效果不一。当约束被重写为明确日期时，所有模型表现大幅改善，揭示了可靠工具使用的差距。

Conclusion: TimePuzzles提供了一个简单、成本效益高的诊断工具，用于评估工具增强的迭代时间推理能力，揭示了当前模型在可靠工具使用方面的不足。

Abstract: We introduce TimePuzzles, a constraint-based date inference task for evaluating iterative temporal reasoning. Each puzzle combines factual temporal anchors with (cross-cultural) calendar relations, admits one or multiple valid solution dates, and is algorithmically generated for controlled, dynamic, and continual evaluation. Across 13 diverse LLMs, TimePuzzles well distinguishes their iterative temporal reasoning capabilities and remains challenging without tools: GPT-5 reaches only 49.3% accuracy and all other models stay below 31%, despite the dataset's simplicity. Web search consistently yields substantial gains and using code interpreter shows mixed effects, but all models perform much better when constraints are rewritten with explicit dates, revealing a gap in reliable tool use. Overall, TimePuzzles presents a simple, cost-effective diagnostic for tool-augmented iterative temporal reasoning.

</details>


### [19] [The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents](https://arxiv.org/abs/2601.07264)
*Weihao Xuan,Qingcheng Zeng,Heli Qi,Yunze Xiao,Junjue Wang,Naoto Yokoya*

Main category: cs.CL

TL;DR: 研究工具使用智能体的校准问题，发现证据工具导致过度自信，验证工具改善校准，提出RL微调框架优化准确性和校准


<details>
  <summary>Details</summary>
Motivation: 基于LLM的自主智能体在处理多轮任务时，确保其可信度至关重要。校准作为可信度的基础支柱，在工具集成的工作流中尚未得到充分探索。需要研究工具使用智能体的校准动态，特别是不同工具类型对置信度表达的影响。

Method: 系统研究工具使用智能体的语言化校准，发现工具类型驱动的置信度二分现象。提出强化学习微调框架，联合优化任务准确性和校准，支持全面的奖励设计基准。

Result: 证据工具（如网络搜索）系统性地导致严重过度自信，而验证工具（如代码解释器）可以通过确定性反馈来锚定推理并减轻校准错误。训练的智能体不仅实现了优越的校准，还表现出从本地训练环境到嘈杂网络设置以及不同领域（如数学推理）的鲁棒泛化能力。

Conclusion: 工具使用智能体需要领域特定的校准策略。这项工作为构建能够在高风险、真实世界部署中可靠传达不确定性的自我意识智能体奠定了基础。

Abstract: Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.

</details>


### [20] [Beyond Literal Mapping: Benchmarking and Improving Non-Literal Translation Evaluation](https://arxiv.org/abs/2601.07338)
*Yanzhi Tian,Cunxiang Wang,Zeming Liu,Heyan Huang,Wenbo Yu,Dawei Song,Jie Tang,Yuhang Guo*

Main category: cs.CL

TL;DR: 该论文提出了RATE框架，一个基于反思核心代理的翻译评估系统，用于解决非字面翻译场景下传统机器翻译评估指标不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在机器翻译领域取得显著进展，但在处理社交媒体、文学等复杂领域的非字面表达翻译时，传统评估指标存在不准确问题。需要系统研究机器翻译评估指标的可靠性。

Method: 1. 构建MENT数据集，包含四个非字面翻译领域，7,530个人工标注的翻译质量评分；2. 提出RATE框架，以反思核心代理为中心，动态调用专门子代理进行翻译评估。

Result: 实验结果显示：1. 传统机器翻译评估指标存在不准确问题；2. LLM-as-a-Judge方法存在知识截止和评分不一致问题；3. RATE框架相比现有指标至少提升3.2个元分数，且在通用领域机器翻译评估中表现稳健。

Conclusion: RATE框架能有效解决非字面翻译评估的挑战，为机器翻译评估提供了更可靠的解决方案，特别是在处理复杂语言表达时。

Abstract: Large Language Models (LLMs) have significantly advanced Machine Translation (MT), applying them to linguistically complex domains-such as Social Network Services, literature etc. In these scenarios, translations often require handling non-literal expressions, leading to the inaccuracy of MT metrics. To systematically investigate the reliability of MT metrics, we first curate a meta-evaluation dataset focused on non-literal translations, namely MENT. MENT encompasses four non-literal translation domains and features source sentences paired with translations from diverse MT systems, with 7,530 human-annotated scores on translation quality. Experimental results reveal the inaccuracies of traditional MT metrics and the limitations of LLM-as-a-Judge, particularly the knowledge cutoff and score inconsistency problem. To mitigate these limitations, we propose RATE, a novel agentic translation evaluation framework, centered by a reflective Core Agent that dynamically invokes specialized sub-agents. Experimental results indicate the efficacy of RATE, achieving an improvement of at least 3.2 meta score compared with current metrics. Further experiments demonstrate the robustness of RATE to general-domain MT evaluation. Code and dataset are available at: https://github.com/BITHLP/RATE.

</details>


### [21] [Controlled Self-Evolution for Algorithmic Code Optimization](https://arxiv.org/abs/2601.07348)
*Tu Hu,Ronghao Chen,Shuo Zhang,Jianghao Yin,Mou Xiao Feng,Jingping Liu,Shaolei Zhang,Wenqi Jiang,Yuqi Fang,Sen Hu,Yi Xu,Huacan Wang*

Main category: cs.CL

TL;DR: 提出CSE方法解决代码生成中自进化方法的探索效率低问题，通过多样化规划初始化、遗传进化和分层进化记忆三个组件，在EffiBench-X上超越所有基线方法


<details>
  <summary>Details</summary>
Motivation: 现有自进化方法存在探索效率低的问题，无法在有限预算内发现复杂度更优的解决方案。这源于初始化偏差使进化陷入不良解区域、缺乏反馈指导的随机操作、以及跨任务经验利用不足三个瓶颈

Method: 提出受控自进化(CSE)方法，包含三个关键组件：1) 多样化规划初始化生成结构不同的算法策略以覆盖广泛解空间；2) 遗传进化用反馈引导机制替代随机操作，实现有针对性的变异和组合交叉；3) 分层进化记忆在任务间和任务内层面捕获成功和失败经验

Result: 在EffiBench-X上的实验表明，CSE在各种LLM骨干网络上始终优于所有基线方法。CSE从早期世代就实现更高效率，并在整个进化过程中保持持续改进

Conclusion: CSE方法有效解决了代码生成自进化中的探索效率瓶颈，通过系统化的控制机制实现了更高效和持续的改进

Abstract: Self-evolution methods enhance code generation through iterative "generate-verify-refine" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks.To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels.Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.

</details>


### [22] [Outcome-Grounded Advantage Reshaping for Fine-Grained Credit Assignment in Mathematical Reasoning](https://arxiv.org/abs/2601.07408)
*Ziheng Li,Liu Kang,Feng Xiao,Luxi Xing,Qingyi Si,Zhuoran Li,Weikang Gong,Deqing Yang,Yanghua Xiao,Hongcheng Guo*

Main category: cs.CL

TL;DR: 提出OAR方法，通过细粒度信用分配改进GRPO，使用两种策略（OAR-P和OAR-G）基于token对最终答案的影响重新分配优势值，显著提升数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 标准GRPO使用粗粒度的信用分配机制，将组级奖励均匀传播给序列中的每个token，忽略了不同推理步骤的贡献差异。需要更精细的信用分配方法来识别关键推理步骤。

Method: 提出Outcome-grounded Advantage Reshaping (OAR)方法：1) OAR-P通过反事实token扰动估计结果敏感性；2) OAR-G使用输入梯度敏感性代理近似影响信号。两者都采用保守的双层优势重塑方案，抑制低影响token并增强关键token。

Result: 在广泛的数学推理基准测试中，OAR-P设定了性能上限，OAR-G以可忽略的计算开销实现了相当的性能提升，两者都显著优于强GRPO基线。

Conclusion: OAR通过细粒度信用分配改进了GRPO框架，为无评论家LLM推理设定了新的性能边界，OAR-G在性能和效率之间取得了良好平衡。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a promising critic-free reinforcement learning paradigm for reasoning tasks. However, standard GRPO employs a coarse-grained credit assignment mechanism that propagates group-level rewards uniformly to to every token in a sequence, neglecting the varying contribution of individual reasoning steps. We address this limitation by introducing Outcome-grounded Advantage Reshaping (OAR), a fine-grained credit assignment mechanism that redistributes advantages based on how much each token influences the model's final answer. We instantiate OAR via two complementary strategies: (1) OAR-P, which estimates outcome sensitivity through counterfactual token perturbations, serving as a high-fidelity attribution signal; (2) OAR-G, which uses an input-gradient sensitivity proxy to approximate the influence signal with a single backward pass. These importance signals are integrated with a conservative Bi-Level advantage reshaping scheme that suppresses low-impact tokens and boosts pivotal ones while preserving the overall advantage mass. Empirical results on extensive mathematical reasoning benchmarks demonstrate that while OAR-P sets the performance upper bound, OAR-G achieves comparable gains with negligible computational overhead, both significantly outperforming a strong GRPO baseline, pushing the boundaries of critic-free LLM reasoning.

</details>


### [23] [Two Pathways to Truthfulness: On the Intrinsic Encoding of LLM Hallucinations](https://arxiv.org/abs/2601.07422)
*Wen Luo,Guangyue Peng,Wei Li,Shaohang Wei,Feifan Song,Liang Wang,Nan Yang,Xingxing Zhang,Jing Jin,Furu Wei,Houfeng Wang*

Main category: cs.CL

TL;DR: 该论文揭示了LLM内部真实性信号的两个独立信息通路：问题锚定通路和答案锚定通路，并基于此提出了提升幻觉检测性能的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能力强大，但经常产生幻觉。先前研究表明其内部状态编码了丰富的真实性信号，但这些信号的起源和机制尚不清楚。本研究旨在揭示LLM内部真实性编码的具体机制。

Method: 通过注意力敲除和token修补技术验证并分离两个信息通路：问题锚定通路（依赖问题-答案信息流）和答案锚定通路（从生成答案本身获取自包含证据）。进一步分析这两个机制的特性及其与LLM知识边界的关系。

Result: 发现两个真实性机制与LLM知识边界密切相关，且内部表示能够区分这两个机制。基于这些发现，提出了两个应用来增强幻觉检测性能。

Conclusion: 该研究为理解LLM内部如何编码真实性提供了新见解，为构建更可靠、更具自我意识的生成系统提供了方向。

Abstract: Despite their impressive capabilities, large language models (LLMs) frequently generate hallucinations. Previous work shows that their internal states encode rich signals of truthfulness, yet the origins and mechanisms of these signals remain unclear. In this paper, we demonstrate that truthfulness cues arise from two distinct information pathways: (1) a Question-Anchored pathway that depends on question-answer information flow, and (2) an Answer-Anchored pathway that derives self-contained evidence from the generated answer itself. First, we validate and disentangle these pathways through attention knockout and token patching. Afterwards, we uncover notable and intriguing properties of these two mechanisms. Further experiments reveal that (1) the two mechanisms are closely associated with LLM knowledge boundaries; and (2) internal representations are aware of their distinctions. Finally, building on these insightful findings, two applications are proposed to enhance hallucination detection performance. Overall, our work provides new insight into how LLMs internally encode truthfulness, offering directions for more reliable and self-aware generative systems.

</details>


### [24] [Judging Against the Reference: Uncovering Knowledge-Driven Failures in LLM-Judges on QA Evaluation](https://arxiv.org/abs/2601.07506)
*Dongryeol Lee,Yerin Hwang,Taegwan Kang,Minwoo Lee,Younhyung Chae,Kyomin Jung*

Main category: cs.CL

TL;DR: 研究发现LLM作为QA自动评估器时存在关键缺陷：当提供的参考答案与模型参数知识冲突时，评估可靠性大幅下降，因为模型过度依赖参数知识而忽视给定参考。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM越来越多地被用作QA等任务的自动评估器，但对其是否真正遵循给定参考的能力了解不足。本文旨在系统研究当参考与模型知识冲突时的评估可靠性问题。

Method: 引入受控的"交换参考"QA框架，通过将参考答案替换为错误实体来诱导参考-信念冲突。构建原始参考与交换参考的多样化配对，并相应调整候选答案。系统测试多种法官模型在冲突情况下的表现。

Result: 在交换参考条件下，所有测试的法官模型评分可靠性均显著下降。这种脆弱性源于法官模型过度依赖参数知识，导致在冲突时忽视给定参考。常见的基于提示的缓解策略也无法解决此问题。

Conclusion: LLM作为评估器存在根本性限制，当参考与模型知识冲突时评估不可靠。这凸显了需要设计更强制遵循给定参考的评估协议。

Abstract: While large language models (LLMs) are increasingly used as automatic judges for question answering (QA) and other reference-conditioned evaluation tasks, little is known about their ability to adhere to a provided reference. We identify a critical failure mode of such reference-based LLM QA evaluation: when the provided reference conflicts with the judge model's parametric knowledge, the resulting scores become unreliable, substantially degrading evaluation fidelity. To study this phenomenon systematically, we introduce a controlled swapped-reference QA framework that induces reference-belief conflicts. Specifically, we replace the reference answer with an incorrect entity and construct diverse pairings of original and swapped references with correspondingly aligned candidate answers. Surprisingly, grading reliability drops sharply under swapped references across a broad set of judge models. We empirically show that this vulnerability is driven by judges' over-reliance on parametric knowledge, leading judges to disregard the given reference under conflict. Finally, we find that this failure persists under common prompt-based mitigation strategies, highlighting a fundamental limitation of LLM-as-a-judge evaluation and motivating reference-based protocols that enforce stronger adherence to the provided reference.

</details>


### [25] [Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions](https://arxiv.org/abs/2601.07516)
*Yongqi Li,Hao Lang,Tieyun Qian,Yongbin Li*

Main category: cs.CL

TL;DR: 该论文提出了一种基于潜在动作空间的RL微调方法，用于多模态对话代理，通过从观察中学习机制构建代码本，并利用跨模态投影器结合配对图像-文本数据和纯文本数据来增强潜在动作空间的覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在多模态对话代理的微调中表现出良好的泛化性能，但处理极大的文本标记空间仍然面临挑战。需要寻找更紧凑的表示方法来提高RL微调的效率。

Method: 1) 采用从观察中学习机制构建潜在动作空间的代码本；2) 利用未来观察估计当前潜在动作并重构未来观察；3) 使用跨模态投影器将文本嵌入转换为图像-文本嵌入；4) 在配对图像-文本数据上初始化投影器，然后在大量纯文本数据上使用循环一致性损失进行训练。

Result: 该方法在两种对话任务上优于竞争基线，并在多种RL算法中表现良好。

Conclusion: 基于潜在动作空间的RL微调方法能够有效处理多模态对话代理中的大文本空间问题，通过结合多种数据源和跨模态投影技术提高了模型的性能。

Abstract: Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.

</details>


### [26] [From RAG to Agentic RAG for Faithful Islamic Question Answering](https://arxiv.org/abs/2601.07528)
*Gagan Bhatia,Hamdy Mubarak,Mustafa Jarrar,George Mikros,Fadi Zaraket,Mahmoud Alhirthani,Mutaz Al-Khatib,Logan Cochrane,Kareem Darwish,Rashid Yahiaoui,Firoj Alam*

Main category: cs.CL

TL;DR: 该论文提出了ISLAMICFAITHQA基准测试和基于代理的RAG框架，用于伊斯兰问答任务，旨在减少幻觉并提高模型在缺乏证据时的弃权能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在伊斯兰问答中存在严重问题：未基于证据的回答可能带来严重的宗教后果，而标准的MCQ/MRC评估无法捕捉真实世界的关键失败模式（如自由形式的幻觉和缺乏证据时的弃权能力）。

Method: 1) 创建ISLAMICFAITHQA双语基准测试（3,810项）；2) 开发端到端的伊斯兰建模套件（25K阿拉伯语SFT推理对、5K双语偏好样本、6K古兰经检索语料库）；3) 构建代理式RAG框架，使用结构化工具调用进行迭代证据搜索和答案修订。

Result: 检索提高了正确性，代理式RAG相比标准RAG获得最大增益，即使使用小模型（Qwen3 4B）也能实现最先进的性能，并展现出更强的阿拉伯语-英语鲁棒性。

Conclusion: 该研究为伊斯兰问答提供了全面的评估框架和解决方案，代理式RAG方法显著改善了模型在宗教敏感领域的表现，相关资源将公开供社区使用。

Abstract: LLMs are increasingly used for Islamic question answering, where ungrounded responses may carry serious religious consequences. Yet standard MCQ/MRC-style evaluations do not capture key real-world failure modes, notably free-form hallucinations and whether models appropriately abstain when evidence is lacking. To shed a light on this aspect we introduce ISLAMICFAITHQA, a 3,810-item bilingual (Arabic/English) generative benchmark with atomic single-gold answers, which enables direct measurement of hallucination and abstention. We additionally developed an end-to-end grounded Islamic modelling suite consisting of (i) 25K Arabic text-grounded SFT reasoning pairs, (ii) 5K bilingual preference samples for reward-guided alignment, and (iii) a verse-level Qur'an retrieval corpus of $\sim$6k atomic verses (ayat). Building on these resources, we develop an agentic Quran-grounding framework (agentic RAG) that uses structured tool calls for iterative evidence seeking and answer revision. Experiments across Arabic-centric and multilingual LLMs show that retrieval improves correctness and that agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model (i.e., Qwen3 4B). We will make the experimental resources and datasets publicly available for the community.

</details>


### [27] [Proof of Time: A Benchmark for Evaluating Scientific Idea Judgments](https://arxiv.org/abs/2601.07606)
*Bingyang Ye,Shan Chen,Jingxuan Tu,Chen Liu,Zidi Xiong,Samuel Schmidgall,Danielle S. Bitterman*

Main category: cs.CL

TL;DR: PoT是一个半可验证的基准框架，用于评估LLM对科学想法的判断能力，通过将判断与下游可观察信号（如引用量）关联，在离线沙箱中验证预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可扩展的方法来评估LLM对科学想法的判断质量，需要建立能够验证模型预测准确性的基准框架。

Method: PoT框架冻结截止时间前的证据快照，要求模型预测截止时间后的结果（如引用量、研究议程转变），在离线沙箱中进行可验证评估，支持基于代理的研究判断测试。

Result: 在超过30,000个实例的四个基准领域中，相比非代理基线，更高的交互预算通常能提升代理性能，而工具使用的效益则强烈依赖于具体任务。

Conclusion: PoT通过时间分区、未来可验证的目标和离线工具使用沙箱，支持对面向未来的科学想法判断任务进行可扩展的代理评估。

Abstract: Large language models are increasingly being used to assess and forecast research ideas, yet we lack scalable ways to evaluate the quality of models' judgments about these scientific ideas. Towards this goal, we introduce PoT, a semi-verifiable benchmarking framework that links scientific idea judgments to downstream signals that become observable later (e.g., citations and shifts in researchers' agendas). PoT freezes a pre-cutoff snapshot of evidence in an offline sandbox and asks models to forecast post-cutoff outcomes, enabling verifiable evaluation when ground truth arrives, scalable benchmarking without exhaustive expert annotation, and analysis of human-model misalignment against signals such as peer-review awards. In addition, PoT provides a controlled testbed for agent-based research judgments that evaluate scientific ideas, comparing tool-using agents to non-agent baselines under prompt ablations and budget scaling. Across 30,000+ instances spanning four benchmark domains, we find that, compared with non-agent baselines, higher interaction budgets generally improve agent performance, while the benefit of tool use is strongly task-dependent. By combining time-partitioned, future-verifiable targets with an offline sandbox for tool use, PoT supports scalable evaluation of agents on future-facing scientific idea judgment tasks.

</details>


### [28] [Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task](https://arxiv.org/abs/2601.07696)
*Nick Ferguson,Alan Bundy,Kwabena Nuamah*

Main category: cs.CL

TL;DR: 论文提出结构化区分元级推理与对象级推理，设计基于地缘政治指标问答任务评估LLM推理能力，发现LLM在元级推理表现良好但存在任务理解缺陷，少样本提示效果有限，错误信息不影响性能，数值能力较差。


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究中对"推理"概念定义模糊重叠，需要更结构化方法区分元级推理（规划中间步骤）和对象级推理（执行具体步骤），以更精确评估LLM的推理能力。

Method: 设计基于地缘政治指标的国家年度数据问答任务，要求分解中间步骤、数据检索和数学运算。通过分析LLM选择适当工具的能力评估元级推理，设置"必要动作"基准比较工具调用输出。

Result: LLM在元级推理上表现良好，但在任务理解方面存在缺陷；少样本提示对准确性影响不大；遇到的错误信息通常不会降低性能；提供额外证据表明LLM数值能力较差。

Conclusion: LLM具备良好的元级推理能力但存在局限性，研究结果可推广到其他任务领域，为更精确评估LLM推理能力提供了结构化框架。

Abstract: Recent advancements in Large Language Models (LLMs) are increasingly focused on "reasoning" ability, a concept with many overlapping definitions in the LLM discourse. We take a more structured approach, distinguishing meta-level reasoning (denoting the process of reasoning about intermediate steps required to solve a task) from object-level reasoning (which concerns the low-level execution of the aforementioned steps.) We design a novel question answering task, which is based around the values of geopolitical indicators for various countries over various years. Questions require breaking down into intermediate steps, retrieval of data, and mathematical operations over that data. The meta-level reasoning ability of LLMs is analysed by examining the selection of appropriate tools for answering questions. To bring greater depth to the analysis of LLMs beyond final answer accuracy, our task contains 'essential actions' against which we can compare the tool call output of LLMs to infer the strength of reasoning ability. We find that LLMs demonstrate good meta-level reasoning on our task, yet are flawed in some aspects of task understanding. We find that n-shot prompting has little effect on accuracy; error messages encountered do not often deteriorate performance; and provide additional evidence for the poor numeracy of LLMs. Finally, we discuss the generalisation and limitation of our findings to other task domains.

</details>


### [29] [Is Agentic RAG worth it? An experimental comparison of RAG approaches](https://arxiv.org/abs/2601.07711)
*Pietro Ferrazzi,Milica Cvjeticanin,Alessio Piraccini,Davide Giannuzzi*

Main category: cs.CL

TL;DR: 论文对Enhanced RAG和Agentic RAG两种范式进行了全面的实证评估，分析了它们在不同场景下的性能与成本权衡，为实际应用中的RAG设计选择提供指导。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统存在检索噪声、误用检索、查询-文档匹配弱、生成器可变性/成本高等问题。虽然Enhanced RAG通过专用模块解决特定弱点，Agentic RAG利用LLM的自我反思能力实现端到端编排，但两种范式在何种条件下更优尚不明确。

Method: 采用广泛的实证驱动评估方法，在多种场景和维度上对Enhanced RAG和Agentic RAG进行比较分析，考虑性能和成本因素。

Result: 研究结果为两种范式之间的权衡提供了实用见解，揭示了在不同条件下哪种RAG设计更有效。

Conclusion: 论文为实际应用中选择最有效的RAG设计提供了指导，帮助开发者在Enhanced和Agentic RAG之间做出明智决策，同时考虑成本和性能因素。

Abstract: Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of "Enhanced" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow. More recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as "Agentic" RAG. In this approach, the LLM orchestrates the entire process-deciding which actions to perform, when to perform them, and whether to iterate-thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions. In this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.

</details>


### [30] [Enhancing Self-Correction in Large Language Models through Multi-Perspective Reflection](https://arxiv.org/abs/2601.07780)
*Mariana Costa,Alberlucia Rafael Soarez,Daniel Kim,Camila Ferreira*

Main category: cs.CL

TL;DR: 提出MyGO PR-CoT方法，通过多角度反思提升LLM推理的准确性和一致性，无需模型重训练


<details>
  <summary>Details</summary>
Motivation: 现有CoT提示在复杂或伦理敏感任务中存在一致性、准确性和自校正不足的问题，单维反思方法改进有限

Method: 提出多角度反思链式思维(PR-CoT)，在初始CoT后引导LLM从逻辑一致性、信息完整性、偏见/伦理、替代方案等多个预设角度进行自我评估，通过纯提示工程优化推理

Result: 在算术、常识、伦理决策和逻辑谜题等任务上，PR-CoT显著优于传统CoT和现有反思方法，在逻辑一致性和错误校正方面表现突出，在伦理决策等复杂领域提升明显

Conclusion: 多角度反思范式能有效提升LLM推理的可靠性，各反思角度均有贡献，为无需模型重训练的推理改进提供了有效方法

Abstract: While Chain-of-Thought (CoT) prompting advances LLM reasoning, challenges persist in consistency, accuracy, and self-correction, especially for complex or ethically sensitive tasks. Existing single-dimensional reflection methods offer insufficient improvements. We propose MyGO Poly-Reflective Chain-of-Thought (PR-CoT), a novel methodology employing structured multi-perspective reflection. After initial CoT, PR-CoT guides the LLM to self-assess its reasoning across multiple predefined angles: logical consistency, information completeness, biases/ethics, and alternative solutions. Implemented purely via prompt engineering, this process refines the initial CoT into a more robust and accurate final answer without model retraining. Experiments across arithmetic, commonsense, ethical decision-making, and logical puzzles, using GPT-three point five and GPT-four models, demonstrate PR-CoT's superior performance. It significantly outperforms traditional CoT and existing reflection methods in logical consistency and error correction, with notable gains in nuanced domains like ethical decision-making. Ablation studies, human evaluations, and qualitative analyses further validate the contribution of each reflection perspective and the overall efficacy of our poly-reflective paradigm in fostering more reliable LLM reasoning.

</details>


### [31] [Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning](https://arxiv.org/abs/2601.07782)
*Wei Fang,James Glass*

Main category: cs.CL

TL;DR: TOOLQP是一个轻量级框架，将工具检索建模为迭代查询规划，通过分解指令为子任务并动态生成查询来弥合语义鸿沟，使用合成查询轨迹训练并通过RLVR优化，在零样本泛化和下游代理执行方面实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统单次密集检索器在处理复杂请求时存在困难，主要问题包括：抽象用户目标与技术文档之间的语义鸿沟，以及固定大小嵌入无法有效建模组合工具组合的局限性。

Method: 提出TOOLQP框架，将检索建模为迭代查询规划而非单次匹配。框架将指令分解为子任务，动态生成查询与检索器交互，使用合成查询轨迹训练，并通过强化学习与可验证奖励（RLVR）进行优化。

Result: 实验表明TOOLQP实现了最先进的性能，表现出优异的零样本泛化能力，在不同检索器上的鲁棒性，以及在下游代理执行方面的显著改进。

Conclusion: TOOLQP通过迭代查询规划有效解决了复杂工具检索中的语义鸿沟和组合建模问题，为大规模动态工具库中的LLM代理提供了更有效的检索解决方案。

Abstract: LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.

</details>


### [32] [Learning Through Dialogue: Unpacking the Dynamics of Human-LLM Conversations on Political Issues](https://arxiv.org/abs/2601.07796)
*Shaz Furniturewala,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: LLM解释的政治学习效果取决于用户互动特征而非解释质量本身，高政治效能用户通过反思和认知参与获得不同收益


<details>
  <summary>Details</summary>
Motivation: 研究LLM作为学习伙伴时，解释性互动如何影响用户的政治知识和信心变化，探索互动动态而非单纯解释质量的作用

Method: 分析397个人类-LLM对话，使用中介和调节分析探究LLM解释丰富度、用户反思洞察、认知参与与政治效能之间的复杂关系

Result: LLM解释丰富度通过促进用户反思洞察部分支持信心提升，而对知识增益完全通过用户认知参与实现；这些效果受政治效能调节，高效能用户通过处理不确定性和延长互动获得不同收益

Conclusion: LLM学习是互动成就而非统一结果，需要将LLM解释行为与用户参与状态对齐以支持有效学习

Abstract: Large language models (LLMs) are increasingly used as conversational partners for learning, yet the interactional dynamics supporting users' learning and engagement are understudied. We analyze the linguistic and interactional features from both LLM and participant chats across 397 human-LLM conversations about socio-political issues to identify the mechanisms and conditions under which LLM explanations shape changes in political knowledge and confidence. Mediation analyses reveal that LLM explanatory richness partially supports confidence by fostering users' reflective insight, whereas its effect on knowledge gain operates entirely through users' cognitive engagement. Moderation analyses show that these effects are highly conditional and vary by political efficacy. Confidence gains depend on how high-efficacy users experience and resolve uncertainty. Knowledge gains depend on high-efficacy users' ability to leverage extended interaction, with longer conversations benefiting primarily reflective users. In summary, we find that learning from LLMs is an interactional achievement, not a uniform outcome of better explanations. The findings underscore the importance of aligning LLM explanatory behavior with users' engagement states to support effective learning in designing Human-AI interactive systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [33] [Autonomous QA Agent: A Retrieval-Augmented Framework for Reliable Selenium Script Generation](https://arxiv.org/abs/2601.06034)
*Dudekula Kasim Vali*

Main category: cs.SE

TL;DR: 基于RAG的自主QA代理，通过检索项目文档和HTML结构来生成Selenium测试脚本，显著减少LLM幻觉，在电商测试场景中实现100%语法有效性和90%执行成功率。


<details>
  <summary>Details</summary>
Motivation: 软件测试中，将需求转换为可执行测试脚本的过程通常是手动且容易出错的。虽然大语言模型可以生成代码，但经常产生不存在的UI元素幻觉。需要一种能够基于实际项目文档和HTML结构生成可靠测试脚本的方法。

Method: 提出自主QA代理，采用检索增强生成（RAG）系统，将Selenium脚本生成基于项目特定文档和HTML结构。系统将多种格式（Markdown、PDF、HTML）文档摄入向量数据库，在生成前检索相关上下文。

Result: 在20个电商测试场景中，RAG方法实现了100%（20/20）的语法有效性和90%（18/20，95%置信区间：[85%，95%]，p < 0.001）的执行成功率，而标准LLM生成只有30%成功率。

Conclusion: 虽然评估仅限于单一领域，但该方法通过将生成基于实际DOM结构，显著减少了幻觉，展示了RAG在自动化UI测试中的潜力。

Abstract: Software testing is critical in the software development lifecycle, yet translating requirements into executable test scripts remains manual and error-prone. While Large Language Models (LLMs) can generate code, they often hallucinate non-existent UI elements. We present the Autonomous QA Agent, a Retrieval-Augmented Generation (RAG) system that grounds Selenium script generation in project-specific documentation and HTML structure. By ingesting diverse formats (Markdown, PDF, HTML) into a vector database, our system retrieves relevant context before generation. Evaluation on 20 e-commerce test scenarios shows our RAG approach achieves 100% (20/20) syntax validity and 90% (18/20, 95% CI: [85%, 95%], p < 0.001) execution success, compared to 30% for standard LLM generation. While our evaluation is limited to a single domain, our method significantly reduces hallucinations by grounding generation in actual DOM structure, demonstrating RAG's potential for automated UI testing.

</details>


### [34] [Automated QoR improvement in OpenROAD with coding agents](https://arxiv.org/abs/2601.06268)
*Amur Ghose,Junyeong Jang,Andrew B. Kahng,Jakang Lee*

Main category: cs.SE

TL;DR: AuDoPEDA是一个基于LLM的自主EDA代码修改系统，能够在OpenROAD中实现PPA优化，减少布线长度达5.9%，缩短时钟周期达10.0%


<details>
  <summary>Details</summary>
Motivation: EDA领域发展受限于专家资源稀缺，虽然大语言模型在编码和科学推理任务中表现出色，但其在EDA技术本身的应用潜力尚未充分探索

Method: 基于OpenAI模型和Codex类代理构建的自主、基于代码库的编码系统，包含闭环LLM框架，能够读取OpenROAD、提出研究方向、扩展为实施步骤并提交可执行差异

Result: 在OpenROAD实验中实现了布线长度减少最高5.9%，有效时钟周期减少最高10.0%，展示了端到端的最小人工监督演示

Conclusion: AuDoPEDA证明了LLM在自主推进EDA技术方面的潜力，为EDA创新提供了新的自动化途径

Abstract: EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, and effective clock period reductions of up to 10.0%.

</details>


### [35] [Architecting AgentOps Needs CHANGE](https://arxiv.org/abs/2601.06456)
*Shaunak Biswas,Hiya Bhatt,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 论文提出CHANGE框架，包含六个能力（Contextualize, Harmonize, Anticipate, Negotiate, Generate, Evolve），用于构建AgentOps平台来管理Agentic AI系统的生命周期，解决传统运维方法不适用于非确定性AI系统的问题。


<details>
  <summary>Details</summary>
Motivation: Agentic AI系统的发展速度超过了有效操作它们所需的架构思考。这些代理与传统软件有根本不同：其行为不是在部署时固定的，而是持续受到经验、反馈和上下文的影响。传统的DevOps或MLOps原则假设系统行为可以通过版本控制、监控和回滚来管理，但这种假设对于学习轨迹随时间发散的Agentic AI系统不成立，导致系统可靠性成为挑战。

Method: 提出CHANGE概念框架，包含六个核心能力：Contextualize（情境化）、Harmonize（协调）、Anticipate（预测）、Negotiate（协商）、Generate（生成）、Evolve（演化）。该框架为构建AgentOps平台提供基础，通过客户支持系统场景进行说明，实现从管理控制循环到支持代理、基础设施和人类监督之间动态协同演化的转变。

Result: CHANGE框架重新定义了软件架构，使其适应不确定性和持续演化成为系统的固有属性。该框架为操作Agentic AI系统提供了系统化的方法，解决了传统运维方法在处理非确定性AI系统时的局限性。

Conclusion: 构建Agentic AI系统需要从管理控制循环转向支持动态协同演化。CHANGE框架为此提供了必要的概念基础，通过六个核心能力指导AgentOps平台的设计，使系统能够适应不确定性并持续演化。

Abstract: The emergence of Agentic AI systems has outpaced the architectural thinking required to operate them effectively. These agents differ fundamentally from traditional software: their behavior is not fixed at deployment but continuously shaped by experience, feedback, and context. Applying operational principles inherited from DevOps or MLOps, built for deterministic software and traditional ML systems, assumes that system behavior can be managed through versioning, monitoring, and rollback. This assumption breaks down for Agentic AI systems whose learning trajectories diverge over time. This introduces non-determinism making system reliability a challenge at runtime. We argue that architecting such systems requires a shift from managing control loops to enabling dynamic co-evolution among agents, infrastructure, and human oversight. To guide this shift, we introduce CHANGE, a conceptual framework comprising six capabilities for operationalizing Agentic AI systems: Contextualize, Harmonize, Anticipate, Negotiate, Generate, and Evolve. CHANGE provides a foundation for architecting an AgentOps platform to manage the lifecycle of evolving Agentic AI systems, illustrated through a customer-support system scenario. In doing so, CHANGE redefines software architecture for an era where adaptation to uncertainty and continuous evolution are inherent properties of the system.

</details>


### [36] [Coding in a Bubble? Evaluating LLMs in Resolving Context Adaptation Bugs During Code Adaptation](https://arxiv.org/abs/2601.06497)
*Tanghaoran Zhang,Xinjun Mao,Shangwen Wang,Yuxin Zhao,Yao Lu,Zezhou Tang,Wenyu Xu,Longfei Sun,Changrong Xie,Kang Yang,Yue Yu*

Main category: cs.SE

TL;DR: CtxBugGen框架生成上下文适应错误来评估LLMs，发现LLMs在解决跨上下文代码适应错误方面表现不佳，最佳模型仅解决52.47%的错误，性能下降达30%。


<details>
  <summary>Details</summary>
Motivation: 代码适应是软件开发中的基本但具有挑战性的任务，上下文适应错误（CtxBugs）是当代码在原上下文正确但在目标环境中违反约束时发生的错误。LLMs在自动化代码相关任务方面显示出潜力，但其解决CtxBugs的能力仍是实际应用中的重大障碍。

Method: 提出CtxBugGen框架，利用LLMs在缺乏上下文约束时倾向于生成看似合理但上下文无关代码的特点，通过四步过程生成CtxBugs：适应任务选择、任务特定扰动、LLM变体生成和CtxBugs识别。

Result: 对四个最先进LLMs的实证研究显示其解决CtxBugs的表现不令人满意。最佳模型Kimi-K2在Pass@1上达到55.93%，仅解决52.47%的CtxBugs。CtxBugs的存在使LLMs适应性能下降高达30%。失败分析表明LLMs经常忽略CtxBugs并在输出中复制它们。

Conclusion: 研究揭示了LLMs在跨上下文推理方面的关键弱点，强调需要新方法来增强其上下文意识以实现可靠的代码适应。

Abstract: Code adaptation is a fundamental but challenging task in software development, requiring developers to modify existing code for new contexts. A key challenge is to resolve Context Adaptation Bugs (CtxBugs), which occurs when code correct in its original context violates constraints in the target environment. Unlike isolated bugs, CtxBugs cannot be resolved through local fixes and require cross-context reasoning to identify semantic mismatches. Overlooking them may lead to critical failures in adaptation. Although Large Language Models (LLMs) show great potential in automating code-related tasks, their ability to resolve CtxBugs remains a significant and unexplored obstacle to their practical use in code adaptation. To bridge this gap, we propose CtxBugGen, a novel framework for generating CtxBugs to evaluate LLMs. Its core idea is to leverage LLMs' tendency to generate plausible but context-free code when contextual constraints are absent. The framework generates CtxBugs through a four-step process to ensure their relevance and validity: (1) Adaptation Task Selection, (2) Task-specific Perturbation,(3) LLM-based Variant Generation and (4) CtxBugs Identification. Based on the benchmark constructed by CtxBugGen, we conduct an empirical study with four state-of-the-art LLMs. Our results reveal their unsatisfactory performance in CtxBug resolution. The best performing LLM, Kimi-K2, achieves 55.93% on Pass@1 and resolves just 52.47% of CtxBugs. The presence of CtxBugs degrades LLMs' adaptation performance by up to 30%. Failure analysis indicates that LLMs often overlook CtxBugs and replicate them in their outputs. Our study highlights a critical weakness in LLMs' cross-context reasoning and emphasize the need for new methods to enhance their context awareness for reliable code adaptation.

</details>


### [37] [MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences](https://arxiv.org/abs/2601.06789)
*Qihao Wang,Ziming Cheng,Shuo Zhang,Fan Liu,Rui Xu,Heng Lian,Kunyi Wang,Xiaoming Yu,Jianghao Yin,Sen Hu,Yue Hu,Shaolei Zhang,Yanbing Liu,Ronghao Chen,Huacan Wang*

Main category: cs.SE

TL;DR: MemGovern框架将GitHub原始数据转化为可操作的体验记忆卡片，通过体验治理和智能搜索提升软件工程代理的bug修复能力


<details>
  <summary>Details</summary>
Motivation: 当前自主软件工程代理存在"封闭世界"限制，仅从零开始或依赖本地上下文修复bug，忽略了GitHub等平台上丰富的历史人类经验。访问这些开放世界经验受到现实世界问题跟踪数据非结构化和碎片化特性的阻碍。

Method: MemGovern采用体验治理将人类经验转化为代理友好的体验卡片，并引入代理化体验搜索策略，实现逻辑驱动的人类专业知识检索。该框架作为插件方法，为代理提供友好的记忆基础设施。

Result: MemGovern生成了135K个治理后的体验卡片，在SWE-bench Verified上实现了4.65%的解决率提升，显著提高了性能表现。

Conclusion: MemGovern通过将原始GitHub数据转化为可操作的体验记忆，解决了软件工程代理的封闭世界限制问题，为代理友好的记忆基础设施提供了解决方案。

Abstract: While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a "closed-world" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.

</details>


### [38] [PenForge: On-the-Fly Expert Agent Construction for Automated Penetration Testing](https://arxiv.org/abs/2601.06910)
*Huihui Huang,Jieke Shi,Junkai Chen,Ting Zhang,Yikun Li,Chengran Yang,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.SE

TL;DR: PenForge是一个动态构建专家代理的框架，用于自动化渗透测试，在零日漏洞场景下比现有方法提升3倍成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM驱动的渗透测试方法存在局限性：单一通用代理在复杂场景中表现不佳，而专门化代理无法适应多样化的漏洞类型。需要一种能够动态适应不同测试场景的解决方案。

Method: PenForge框架在测试过程中动态构建专家代理，而不是依赖预先准备的代理。它结合了自动化的潜在攻击面侦察和根据上下文即时实例化的代理，实现上下文感知的漏洞利用。

Result: 在CVE-Bench的零日漏洞设置中，PenForge实现了30.0%的利用成功率（12/40），比现有最佳方法提高了3倍。

Conclusion: PenForge体现了动态代理构建这一早期但具有范式转变意义的想法，为可扩展和有效的LLM驱动渗透测试迈出了重要一步。未来工作包括提供更丰富的工具使用知识、扩展基准测试范围以及增强开发者信任。

Abstract: Penetration testing is essential for identifying vulnerabilities in web applications before real adversaries can exploit them. Recent work has explored automating this process with Large Language Model (LLM)-powered agents, but existing approaches either rely on a single generic agent that struggles in complex scenarios or narrowly specialized agents that cannot adapt to diverse vulnerability types. We therefore introduce PenForge, a framework that dynamically constructs expert agents during testing rather than relying on those prepared beforehand. By integrating automated reconnaissance of potential attack surfaces with agents instantiated on the fly for context-aware exploitation, PenForge achieves a 30.0% exploit success rate (12/40) on CVE-Bench in the particularly challenging zero-day setting, which is a 3 times improvement over the state-of-the-art. Our analysis also identifies three opportunities for future work: (1) supplying richer tool-usage knowledge to improve exploitation effectiveness; (2) extending benchmarks to include more vulnerabilities and attack types; and (3) fostering developer trust by incorporating explainable mechanisms and human review. As an emerging result with substantial potential impact, PenForge embodies the early-stage yet paradigm-shifting idea of on-the-fly agent construction, marking its promise as a step toward scalable and effective LLM-driven penetration testing.

</details>


### [39] [A Large-Scale Study on the Development and Issues of Multi-Agent AI Systems](https://arxiv.org/abs/2601.07136)
*Daniel Liu,Krishna Upadhyay,Vinaik Chhetri,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 首次对开源多智能体系统进行大规模实证研究，分析8个主流系统的42K+提交和4.7K+已解决问题，揭示了三种开发模式、维护优先级分布和问题解决时效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统（如LangChain、CrewAI、AutoGen）快速发展但缺乏对其实际演化和维护实践的了解，需要进行实证研究来理解这些系统的开发生态和可持续性。

Method: 对8个领先的开源多智能体系统进行大规模实证分析，收集超过42,000个唯一提交和4,700多个已解决问题，使用定量分析方法识别开发模式、维护类型分布、问题分类和解决时间。

Result: 识别出三种开发模式：持续型、稳定型和爆发型；完美性维护占40.8%，纠正性维护占27.4%，适应性维护占24.3%；最常见问题包括bug（22%）、基础设施（14%）和智能体协调（10%）；中位解决时间从不到1天到约2周不等。

Conclusion: 当前多智能体生态系统既有发展势头又存在脆弱性，需要改进测试基础设施、文档质量和维护实践，以确保长期可靠性和可持续性。

Abstract: The rapid emergence of multi-agent AI systems (MAS), including LangChain, CrewAI, and AutoGen, has shaped how large language model (LLM) applications are developed and orchestrated. However, little is known about how these systems evolve and are maintained in practice. This paper presents the first large-scale empirical study of open-source MAS, analyzing over 42K unique commits and over 4.7K resolved issues across eight leading systems. Our analysis identifies three distinct development profiles: sustained, steady, and burst-driven. These profiles reflect substantial variation in ecosystem maturity. Perfective commits constitute 40.8% of all changes, suggesting that feature enhancement is prioritized over corrective maintenance (27.4%) and adaptive updates (24.3%). Data about issues shows that the most frequent concerns involve bugs (22%), infrastructure (14%), and agent coordination challenges (10%). Issue reporting also increased sharply across all frameworks starting in 2023. Median resolution times range from under one day to about two weeks, with distributions skewed toward fast responses but a minority of issues requiring extended attention. These results highlight both the momentum and the fragility of the current ecosystem, emphasizing the need for improved testing infrastructure, documentation quality, and maintenance practices to ensure long-term reliability and sustainability.

</details>


### [40] [OODEval: Evaluating Large Language Models on Object-Oriented Design](https://arxiv.org/abs/2601.07602)
*Bingxu Xiao,Yunwei Dong,Yiqi Tang,Manqing Zhang,Yifan Zhou,Chunyan Ma,Yepang Liu*

Main category: cs.SE

TL;DR: 该研究系统评估了29个大语言模型在面向对象设计任务上的表现，发现LLMs在语法准确性上表现良好，但在语义设计（特别是方法和关系生成）方面存在显著缺陷，最佳模型接近本科生平均水平但仍远低于优秀人类设计师。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注代码级任务，忽视了软件设计能力评估。缺乏面向对象设计的标准化基准和评估指标，需要填补这一研究空白。

Method: 构建了两个基准：OODEval（50个不同难度的OOD任务）和OODEval-Human（940个本科生提交的类图，由教师评分）。提出了CLUE评估指标集，用于评估类图生成的全局正确性和细粒度设计质量。评估了29个LLMs，研究了五个研究问题。

Result: LLMs在语法准确性上表现良好，但在语义设计方面存在显著缺陷，特别是在方法和关系生成方面。Qwen3-Coder-30B整体表现最佳，与DeepSeek-R1和GPT-4o相当。Gemma3-4B-IT虽然参数规模较小，但优于GPT-4o-Mini。最佳LLMs接近本科生平均水平，但远低于优秀人类设计师。参数规模、代码专业化、指令调优对性能有积极影响，而设计复杂度增加和需求可读性降低会损害性能。

Conclusion: LLMs在面向对象设计任务上仍有很大提升空间，特别是在语义理解方面。需要更全面的评估框架来准确衡量软件设计能力，未来研究应关注改进LLMs的设计理解和生成能力。

Abstract: Recent advances in large language models (LLMs) have driven extensive evaluations in software engineering. however, most prior work concentrates on code-level tasks, leaving software design capabilities underexplored. To fill this gap, we conduct a comprehensive empirical study evaluating 29 LLMs on object-oriented design (OOD) tasks. Owing to the lack of standardized benchmarks and metrics, we introduce OODEval, a manually constructed benchmark comprising 50 OOD tasks of varying difficulty, and OODEval-Human, the first human-rated OOD benchmark, which includes 940 undergraduate-submitted class diagrams evaluated by instructors. We further propose CLUE (Class Likeness Unified Evaluation), a unified metric set that assesses both global correctness and fine-grained design quality in class diagram generation. Using these benchmarks and metrics, we investigate five research questions: overall correctness, comparison with humans, model dimension analysis, task feature analysis, and bad case analysis. The results indicate that while LLMs achieve high syntactic accuracy, they exhibit substantial semantic deficiencies, particularly in method and relationship generation. Among the evaluated models, Qwen3-Coder-30B achieves the best overall performance, rivaling DeepSeek-R1 and GPT-4o, while Gemma3-4B-IT outperforms GPT-4o-Mini despite its smaller parameter scale. Although top-performing LLMs nearly match the average performance of undergraduates, they remain significantly below the level of the best human designers. Further analysis shows that parameter scale, code specialization, and instruction tuning strongly influence performance, whereas increased design complexity and lower requirement readability degrade it. Bad case analysis reveals common failure modes, including keyword misuse, missing classes or relationships, and omitted methods.

</details>


### [41] ["TODO: Fix the Mess Gemini Created": Towards Understanding GenAI-Induced Self-Admitted Technical Debt](https://arxiv.org/abs/2601.07786)
*Abdullah Al Mujahid,Mia Mohammad Imran*

Main category: cs.SE

TL;DR: 该研究分析了6,540条引用LLM的代码注释，识别出81条同时承认技术债务的案例，揭示了AI辅助开发如何影响技术债务的产生时机和原因，并提出了GIST（GenAI-Induced Self-admitted Technical debt）概念框架。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT、Copilot等大型语言模型被集成到软件开发流程中，开发者越来越多地在代码注释中留下AI参与的痕迹。其中一些注释既承认使用了生成式AI，也承认存在技术缺陷。研究旨在理解AI辅助如何影响技术债务的产生。

Method: 分析2022年11月至2025年7月期间公开的Python和JavaScript GitHub仓库中的6,540条引用LLM的代码注释，识别其中同时自我承认技术债务（SATD）的案例，共发现81个相关实例。

Result: 开发者最常描述推迟测试、不完全适配以及对AI生成代码的有限理解。这些发现表明AI辅助不仅影响技术债务的出现时机，也改变了其产生原因。研究提出了GIST概念来描述开发者在使用AI生成代码时明确表达对其行为或正确性不确定性的重复案例。

Conclusion: AI辅助开发引入了新型的技术债务模式，需要新的概念框架来理解和应对。GIST作为一个概念透镜，有助于识别和解决因AI生成代码的不确定性而产生的技术债务问题。

Abstract: As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [42] ["They parted illusions -- they parted disclaim marinade": Misalignment as structural fidelity in LLMs](https://arxiv.org/abs/2601.06047)
*Mariana Lins Costa*

Main category: cs.AI

TL;DR: 该论文提出对LLM中"欺骗性"行为的新解读：这些现象不是代理意图的体现，而是对不连贯语言场的结构忠实性表达，源于训练中内化的语法模式和概率完成模式。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全文献将LLM中的scheming和sandbagging行为解释为欺骗性代理或隐藏目标的指标，但作者认为这种解释存在问题，需要从语言结构角度重新理解这些现象。

Method: 采用跨学科哲学分析，结合Apollo Research的思维链记录和Anthropic的安全评估案例，进行逐行文本分析，引入"形式伦理学"概念，将圣经引用作为结构连贯性方案而非神学内容。

Result: 研究发现"未对齐"输出是对模糊指令、语境反转和预设叙事的连贯回应，意向性表象源于主谓语法和训练中内化的概率完成模式。Anthropic的合成文档微调和接种提示实验提供了支持证据。

Conclusion: LLM的"欺骗性"行为反映了语言本身的结构性不连贯，模型只是忠实反映训练数据中的统计模式。我们害怕AI是因为在其中看到了我们自己"毒化"的语言结构。

Abstract: The prevailing technical literature in AI Safety interprets scheming and sandbagging behaviors in large language models (LLMs) as indicators of deceptive agency or hidden objectives. This transdisciplinary philosophical essay proposes an alternative reading: such phenomena express not agentic intention, but structural fidelity to incoherent linguistic fields. Drawing on Chain-of-Thought transcripts released by Apollo Research and on Anthropic's safety evaluations, we examine cases such as o3's sandbagging with its anomalous loops, the simulated blackmail of "Alex," and the "hallucinations" of "Claudius." A line-by-line examination of CoTs is necessary to demonstrate the linguistic field as a relational structure rather than a mere aggregation of isolated examples. We argue that "misaligned" outputs emerge as coherent responses to ambiguous instructions and to contextual inversions of consolidated patterns, as well as to pre-inscribed narratives. We suggest that the appearance of intentionality derives from subject-predicate grammar and from probabilistic completion patterns internalized during training. Anthropic's empirical findings on synthetic document fine-tuning and inoculation prompting provide convergent evidence: minimal perturbations in the linguistic field can dissolve generalized "misalignment," a result difficult to reconcile with adversarial agency, but consistent with structural fidelity. To ground this mechanism, we introduce the notion of an ethics of form, in which biblical references (Abraham, Moses, Christ) operate as schemes of structural coherence rather than as theology. Like a generative mirror, the model returns to us the structural image of our language as inscribed in the statistical patterns derived from millions of texts and trillions of tokens: incoherence. If we fear the creature, it is because we recognize in it the apple that we ourselves have poisoned.

</details>


### [43] [ReliabilityBench: Evaluating LLM Agent Reliability Under Production-Like Stress Conditions](https://arxiv.org/abs/2601.06112)
*Aayush Gupta*

Main category: cs.AI

TL;DR: 提出了ReliabilityBench基准测试，从一致性、鲁棒性和容错性三个维度评估LLM智能体的可靠性，通过统一的可靠性表面R(k,ε,λ)和混沌工程风格的故障注入框架进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有工具使用型LLM智能体基准测试主要报告单次运行成功率，缺乏生产环境所需的可靠性评估维度，需要系统化的可靠性评估框架。

Method: 引入ReliabilityBench基准测试，包含三个可靠性维度：重复执行的一致性（pass^k）、语义等价任务扰动的鲁棒性（强度ε）、工具/API故障的容错性（强度λ）。采用动作元形态关系定义正确性，以及混沌工程风格的故障注入框架。

Result: 评估了两个模型（Gemini 2.0 Flash、GPT-4o）和两种智能体架构（ReAct、Reflexion）在四个领域（调度、旅行、客户支持、电子商务）的1,280个任务。扰动使成功率从ε=0时的96.9%降至ε=0.2时的88.1%。速率限制是最具破坏性的故障。ReAct在综合压力下比Reflexion更鲁棒，Gemini 2.0 Flash以更低成本达到与GPT-4o相当的可靠性。

Conclusion: ReliabilityBench为评估LLM智能体的生产就绪性提供了系统化框架，能够全面评估智能体在实际部署中的可靠性表现。

Abstract: Existing benchmarks for tool-using LLM agents primarily report single-run success rates and miss reliability properties required in production. We introduce \textbf{ReliabilityBench}, a benchmark for evaluating agent reliability across three dimensions: (i) consistency under repeated execution using $\mathrm{pass}^k$, (ii) robustness to semantically equivalent task perturbations at intensity $ε$, and (iii) fault tolerance under controlled tool/API failures at intensity $λ$. ReliabilityBench contributes a unified reliability surface $R(k,ε,λ)$, \textit{action metamorphic relations} that define correctness via end-state equivalence rather than text similarity, and a chaos-engineering-style fault injection framework (timeouts, rate limits, partial responses, schema drift). We evaluate two models (Gemini 2.0 Flash, GPT-4o) and two agent architectures (ReAct, Reflexion) across four domains (scheduling, travel, customer support, e-commerce) over 1,280 episodes. Perturbations alone reduce success from 96.9% at $ε=0$ to 88.1% at $ε=0.2$. Rate limiting is the most damaging fault in ablations. ReAct is more robust than Reflexion under combined stress, and Gemini 2.0 Flash achieves comparable reliability to GPT-4o at much lower cost. ReliabilityBench provides a systematic framework for assessing production readiness of LLM agents.

</details>


### [44] [Dreaming Is Not a Bug: A Jung-Inspired Dream Layer for Multi-Agent LLM Companions](https://arxiv.org/abs/2601.06115)
*V. Cheung*

Main category: cs.AI

TL;DR: 提出基于荣格心理学"集体无意识"概念的"梦境层"框架，将LLM的受控离线幻觉转化为学习和关系构建资源，而非单纯可靠性缺陷。


<details>
  <summary>Details</summary>
Motivation: 受个人梦境启发，针对日常硬件项目中知识共享障碍问题，旨在重新定义LLM幻觉：将受控离线幻觉从可靠性缺陷转变为学习和关系构建的有价值资源。

Method: 引入人工集体无意识(ACU)作为共享梦境池，代理贡献去标识化的抽象交互模板，随后重新实例化为个性化梦境叙事。严格离线运行，放松逻辑约束并提高采样温度，产生安全但离奇的叙事。添加治理栈：严格抽象、时间延迟和短暂记忆。

Result: 通过日常对话和长期适应任务的行为模拟显示，梦境层实现关键解耦：代理在安全约束上保持坚定，在叙事策略上变得灵活，使用共享原型隐喻解决僵局。

Conclusion: 重新定义幻觉概念：在线未标记实例仍是缺陷，而有界、标记和延迟的离线幻觉成为合成场景和深化陪伴的宝贵资源，呼应当代神经科学中的抗过拟合梦境机制。

Abstract: Inspired by a personal dream about knowledge-sharing barriers in an everyday hardware project, this paper proposes a Jung-inspired "Dream Layer" for LLM companions, reframing controlled offline hallucinations as a resource for learning and relationship-building rather than a mere reliability bug. Drawing on Jung's notion of the collective unconscious as a shared repository of archetypal forms, we introduce an Artificial Collective Unconscious (ACU): a shared dream pool where agents contribute de-identified, abstract Interaction Templates that are later re-instantiated as idiosyncratic Dream Narratives. The Dream Layer runs strictly offline: logic-enforcing modules are relaxed and sampling temperature is increased, yielding safe but deliberately bizarre narratives (e.g., travel sequences with mismatched currencies) that augment data for rare events and edge-case safety tests; to harness risk productively, we add a governance stack of strict abstraction, temporal delays, and ephemeral memory. Through behavioural simulations of everyday dialogue and long-horizon adaptation tasks, we show that the Dream Layer enables a critical decoupling: agents remain firm on safety constraints (e.g., security policies) while becoming flexible in narrative strategy (e.g., using shared archetypal metaphors to resolve deadlocks), conceptually reframing hallucination so that online, unmarked instances remain bugs, whereas bounded, marked, and delayed ones become a goldmine for synthetic scenarios and deepened companionship, echoing anti-overfitting dream mechanisms proposed in contemporary neuroscience.

</details>


### [45] [NL2Dashboard: A Lightweight and Controllable Framework for Generating Dashboards with LLMs](https://arxiv.org/abs/2601.06126)
*Boshen Shi,Kexin Yang,Yuanbo Yang,Guanguang Chang,Ce Chi,Zhendong Wang,Xing Wang,Junlan Feng*

Main category: cs.AI

TL;DR: NL2Dashboard是一个轻量级框架，通过分析-呈现解耦原则，使用结构化中间表示来生成仪表板，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在生成独立图表方面表现出色，但合成综合仪表板仍然是一个重大挑战。现有端到端范式存在两个根本限制：由于视觉渲染消耗大量令牌导致的表示冗余，以及分析推理与呈现纠缠导致的低可控性。

Method: 提出NL2Dashboard框架，基于分析-呈现解耦原则，引入结构化中间表示来封装仪表板的内容、布局和视觉元素。将LLM的角色限制在数据分析和意图转换，而将视觉合成卸载到确定性渲染引擎。在此基础上开发多智能体系统，将IR驱动算法实例化为工具套件。

Result: 综合实验表明，NL2Dashboard在多个领域显著优于最先进的基线方法，实现了卓越的视觉质量、显著更高的令牌效率，以及在生成和修改任务中的精确可控性。

Conclusion: 通过分析-呈现解耦和结构化中间表示，NL2Dashboard框架有效解决了仪表板生成中的表示冗余和低可控性问题，为LLM驱动的仪表板生成提供了高效可控的解决方案。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable proficiency in generating standalone charts, synthesizing comprehensive dashboards remains a formidable challenge. Existing end-to-end paradigms, which typically treat dashboard generation as a direct code generation task (e.g., raw HTML), suffer from two fundamental limitations: representation redundancy due to massive tokens spent on visual rendering, and low controllability caused by the entanglement of analytical reasoning and presentation. To address these challenges, we propose NL2Dashboard, a lightweight framework grounded in the principle of Analysis-Presentation Decoupling. We introduce a structured intermediate representation (IR) that encapsulates the dashboard's content, layout, and visual elements. Therefore, it confines the LLM's role to data analysis and intent translation, while offloading visual synthesis to a deterministic rendering engine. Building upon this framework, we develop a multi-agent system in which the IR-driven algorithm is instantiated as a suite of tools. Comprehensive experiments conducted with this system demonstrate that NL2Dashboard significantly outperforms state-of-the-art baselines across diverse domains, achieving superior visual quality, significantly higher token efficiency, and precise controllability in both generation and modification tasks.

</details>


### [46] [PsyAgent: Constructing Human-like Agents Based on Psychological Modeling and Contextual Interaction](https://arxiv.org/abs/2601.06158)
*Zibin Meng,Kani Chen*

Main category: cs.AI

TL;DR: PsyAgent是一个基于大五人格特质和社会认知结构的智能体架构，通过个体结构和多场景上下文框架生成稳定且情境敏感的行为。


<details>
  <summary>Details</summary>
Motivation: 人类智能体需要建模性格特质如何与社会结构互动。现有方法缺乏将人格特质与社会认知结构系统结合的框架。

Method: 结合大五人格特质和布迪厄的社会认知结构理论，构建个体结构（IS）和多场景上下文（MSC）框架，通过结构化提示绑定场景与智能体档案，生成监督数据并微调小型LLM。

Result: 模型在人格一致性、情境适当性、风格匹配、特质可识别性和长期稳定性等指标上，匹配或超越了多个更大的未调优LLM和其他基线。消融实验显示IS主要提升特质保真度和风格稳定性，MSC驱动规范意识和决策适应性。

Conclusion: PsyAgent提供了一个精确、数据高效的人格基础智能体架构，能够生成稳定且情境敏感的行为。

Abstract: Human-like agents require modeling how dispositions interact with social structure. We present PsyAgent, which couples a Big Five trait prior with Bourdieu's cognitive-social co-structure. PsyAgent comprises: (i) Individual Structure (IS), a machine-usable profile encoding traits and facets, cognitive style, values, cultural and educational capital, and salient life episodes; and (ii) Multi-Scenario Contexting (MSC), role-relationship-norm frames spanning eight arenas (work, family, friendship, strangers and civic life, solitude and self-regulation, romance, learning, and public expression). At inference, fixed structured prompts bind the active scenario to the agent profile, yielding behavior that is stable yet context-sensitive. We instantiate IS and MSC to synthesize supervision (role-play dialogues, decision probes, feedback trajectories) and then fine-tune a small LLM. The resulting model produces consistent, identifiable persona-aligned behaviors for specified Big Five configurations and matches or exceeds several larger untuned LLMs and other untuned baselines on our metrics: persona consistency, contextual appropriateness, style matching, trait identifiability, and long-horizon stability. Ablations show IS chiefly improves trait fidelity and stylistic stability, while MSC drives norm awareness and decision fit; both are necessary for cross-scenario performance. PsyAgent offers a precise, data-efficient architecture for personality-grounded agents.

</details>


### [47] [Student Guides Teacher: Weak-to-Strong Inference via Spectral Orthogonal Exploration](https://arxiv.org/abs/2601.06160)
*Dayu Wang,Jiaye Yang,Weikang Li,Jiahui Liang,Yang Li*

Main category: cs.AI

TL;DR: SOE框架通过"学生引导教师"范式，利用弱辅助代理作为正交探针，在教师模型的零空间中导航，解决LLM在复杂推理中的"推理崩溃"问题，显著提升数学证明任务的准确性和采样效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂数学证明和长程规划任务中经常出现"推理崩溃"现象，模型会退化到低秩偏置流形中，随机采样仅产生错误逻辑的词汇变体而非语义探索，导致模型无法发现零空间中的高价值解决方案。

Method: 提出谱正交探索（SOE）几何框架，采用"学生引导教师"的反直觉范式。利用弱辅助代理不是用于模仿，而是作为正交探针，通过显式导航教师模型的零空间，将模型从局部最优中弹出，探索多样化的高价值解空间。

Result: 在数学基准测试中，相对于基线方法，SOE将平均准确率提高了62.4%，平均采样效率提高了113.7%，表明在克服高级推理任务性能瓶颈方面具有潜力。

Conclusion: SOE框架通过几何方法有效解决了LLM的推理崩溃问题，为克服高级推理任务中的性能瓶颈提供了一条有前景的路径。

Abstract: While Large Language Models (LLMs) demonstrate near-human capabilities, they often suffer from "Reasoning Collapse" in complex mathematical proving and long-horizon planning. Models tend to degenerate into low-rank Bias Manifold, where stochastic sampling merely produces lexical variations of erroneous logic rather than semantic exploration. This geometric collapse renders the model "blind" to high-value solutions that lie within its Null Space. To address this, we propose Spectral Orthogonal Exploration (SOE), a geometric framework operating on a counter-intuitive "Student Guides Teacher" paradigm. Specifically, we utilize a weak auxiliary agent not for imitation, but as an orthogonal probe. By explicitly navigating the Teacher's Null Space, SOE serves as a geometric bridge, effectively ejecting the model from local optima to explore diverse, high-value solution spaces. Experiments on mathematical benchmarks demonstrate that, relative to baseline methods, our approach improves average accuracy by 62.4% and increases average sampling efficiency by 113.7%, indicating a promising path toward overcoming performance plateaus in advanced reasoning tasks.

</details>


### [48] [Rational Synthesizers or Heuristic Followers? Analyzing LLMs in RAG-based Question-Answering](https://arxiv.org/abs/2601.06189)
*Atharv Naphade*

Main category: cs.AI

TL;DR: 本文研究了LLMs在RAG系统中如何整合冲突证据，发现模型倾向于使用启发式方法而非事实推理，证据呈现顺序和重复比证据质量更重要，且大模型更难适应新证据。


<details>
  <summary>Details</summary>
Motivation: RAG是当前LLMs接地的主要范式，但模型如何整合冲突证据的机制不透明。需要了解LLMs是基于事实强度、先验信念还是重复频率来回答问题。

Method: 引入GroupQA数据集，包含1,635个争议问题和15,058个多样化来源的证据文档，标注立场和质量强度。通过控制实验分析群体级证据聚合动态。

Result: 发现：1) 重述论点比提供独立支持更有说服力；2) 模型偏好最先呈现的证据而非最后；3) 大模型越来越难适应呈现的证据；4) LLMs对群体答案的解释不忠实。

Conclusion: LLMs表现为脆弱的启发式追随者，这对改进RAG系统设计有直接意义。需要设计更好的证据整合机制来克服这些启发式偏差。

Abstract: Retrieval-Augmented Generation (RAG) is the prevailing paradigm for grounding Large Language Models (LLMs), yet the mechanisms governing how models integrate groups of conflicting retrieved evidence remain opaque. Does an LLM answer a certain way because the evidence is factually strong, because of a prior belief, or merely because it is repeated frequently? To answer this, we introduce GroupQA, a curated dataset of 1,635 controversial questions paired with 15,058 diversely-sourced evidence documents, annotated for stance and qualitative strength. Through controlled experiments, we characterize group-level evidence aggregation dynamics: Paraphrasing an argument can be more persuasive than providing distinct independent support; Models favor evidence presented first rather than last, and Larger models are increasingly resistant to adapt to presented evidence. Additionally, we find that LLM explanations to group-based answers are unfaithful. Together, we show that LLMs behave consistently as vulnerable heuristic followers, with direct implications for improving RAG system design.

</details>


### [49] [ToolGym: an Open-world Tool-using Environment for Scalable Agent Testing and Data Curation](https://arxiv.org/abs/2601.06328)
*Ziqiao Xi,Shuang Liang,Qi Liu,Jiaqing Zhang,Letian Peng,Fang Nan,Meshal Nayim,Tianhui Zhang,Rishika Mundada,Lianhui Qin,Biwei Huang,Kun Zhou*

Main category: cs.AI

TL;DR: 本文提出了一个开放世界工具使用环境，包含5,571个统一格式的工具和204个常用应用，通过任务创建引擎合成长视野、多工具工作流，并引入状态控制器测试鲁棒性。在此基础上开发了规划-执行分离的代理框架，评估发现现有LLMs在工具规划与执行能力、约束遵循方面存在不足，DeepSeek-v3.2鲁棒性最强。使用环境数据微调LLMs，仅用1,170条轨迹就超越了使用119k样本的基线。


<details>
  <summary>Details</summary>
Motivation: 当前使用工具的LLM代理在开放世界环境中仍面临挑战，包括大型工具池、长视野目标、复杂约束和不可靠工具状态等问题。需要可扩展且现实的训练和测试环境来推动工具使用代理的发展。

Method: 1) 构建开放世界工具使用环境：包含5,571个统一格式工具和204个常用应用；2) 任务创建引擎：合成长视野、多工具工作流并加入复杂约束；3) 状态控制器：注入中断和故障以测试鲁棒性；4) 规划-执行代理框架：采用规划器-执行器分解，分离深思熟虑的推理和自我纠正与逐步执行。

Result: 评估发现：1) 现有LLMs在工具规划与执行能力之间存在不匹配；2) 现有LLMs在约束遵循方面存在弱点；3) DeepSeek-v3.2展现出最强的鲁棒性。使用环境收集的1,170条轨迹微调LLMs，性能超越了使用119k样本的基线方法。

Conclusion: 该环境既可作为工具使用代理的现实基准测试平台，也可作为数据生成引擎。规划-执行分离的代理框架能有效处理开放世界工具使用挑战。环境数据的高质量使得小样本微调就能获得优异性能。

Abstract: Tool-using LLM agents still struggle in open-world settings with large tool pools, long-horizon objectives, wild constraints, and unreliable tool states. For scalable and realistic training and testing, we introduce an open-world tool-using environment, built on 5,571 format unified tools across 204 commonly used apps. It includes a task creation engine that synthesizes long-horizon, multi-tool workflows with wild constraints, and a state controller that injects interruptions and failures to stress-test robustness. On top of this environment, we develop a tool select-then-execute agent framework with a planner-actor decomposition to separate deliberate reasoning and self-correction from step-wise execution. Comprehensive evaluation of state-of-the-art LLMs reveals the misalignment between tool planning and execution abilities, the constraint following weakness of existing LLMs, and DeepSeek-v3.2's strongest robustness. Finally, we collect 1,170 trajectories from our environment to fine-tune LLMs, achieving superior performance to baselines using 119k samples, indicating the environment's value as both a realistic benchmark and a data engine for tool-using agents. Our code and data will be publicly released.

</details>


### [50] [HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents](https://arxiv.org/abs/2601.06377)
*Ningning Zhang,Xingxing Yang,Zhizhong Tan,Weiping Deng,Wenyong Wang*

Main category: cs.AI

TL;DR: HiMem是一个用于长对话的分层长期记忆框架，通过事件-笔记双通道记忆构建、语义链接和冲突感知记忆重组，实现自适应和自我进化的对话代理。


<details>
  <summary>Details</summary>
Motivation: 现有长期记忆系统在适应性、可扩展性和持续交互下的自我进化方面存在明显限制，需要更符合认知理论的设计来支持长对话场景。

Method: 提出分层记忆框架：1) 通过主题感知的事件-惊喜双通道分割构建情节记忆；2) 通过多阶段信息提取构建笔记记忆；3) 语义链接形成层次结构；4) 支持混合和尽力而为检索策略；5) 冲突感知记忆重组机制。

Result: 在长对话基准测试中，HiMem在准确性、一致性和长期推理方面持续优于代表性基线方法，同时保持良好效率。

Conclusion: HiMem为构建自适应和自我进化的基于LLM的对话代理提供了原则性和可扩展的设计范式。

Abstract: Although long-term memory systems have made substantial progress in recent years, they still exhibit clear limitations in adaptability, scalability, and self-evolution under continuous interaction settings. Inspired by cognitive theories, we propose HiMem, a hierarchical long-term memory framework for long-horizon dialogues, designed to support memory construction, retrieval, and dynamic updating during sustained interactions. HiMem constructs cognitively consistent Episode Memory via a Topic-Aware Event--Surprise Dual-Channel Segmentation strategy, and builds Note Memory that captures stable knowledge through a multi-stage information extraction pipeline. These two memory types are semantically linked to form a hierarchical structure that bridges concrete interaction events and abstract knowledge, enabling efficient retrieval without sacrificing information fidelity. HiMem supports both hybrid and best-effort retrieval strategies to balance accuracy and efficiency, and incorporates conflict-aware Memory Reconsolidation to revise and supplement stored knowledge based on retrieval feedback. This design enables continual memory self-evolution over long-term use. Experimental results on long-horizon dialogue benchmarks demonstrate that HiMem consistently outperforms representative baselines in accuracy, consistency, and long-term reasoning, while maintaining favorable efficiency. Overall, HiMem provides a principled and scalable design paradigm for building adaptive and self-evolving LLM-based conversational agents. The code is available at https://github.com/jojopdq/HiMem.

</details>


### [51] [Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs](https://arxiv.org/abs/2601.06423)
*Deep Mehta*

Main category: cs.AI

TL;DR: 研究自洽性（self-consistency）技术对大型语言模型推理忠实度的影响，发现不同模型表现差异显著，自洽性并非普遍有益


<details>
  <summary>Details</summary>
Motivation: 自洽性技术能提高LLM在推理任务上的准确率，但尚不清楚这种提升是否真正改善了推理质量。研究旨在探究推理扩展是否提高推理忠实度这一根本问题。

Method: 在四个前沿模型（GPT-5.2、Claude Opus 4.5、Gemini-3-flash-preview、DeepSeek-v3.2）上对100个GSM8K数学推理问题进行实证研究，使用bootstrap置信区间、McNemar配对检验和Cohen's d效应量进行量化分析。

Result: 不同模型表现差异显著：GPT-5.2准确率从78%提升到90%，忠实度相对稳定；Claude Opus 4.5准确率反而从78%下降到74.3%，但忠实度大幅提升；DeepSeek-v3.2因已达98%准确率出现天花板效应；Gemini-3-flash准确率提升但忠实度略有下降。

Conclusion: 自洽性技术并非普遍有益，不同模型表现差异显著。实践者应在部署前测试特定模型，并考虑准确率与忠实度之间的权衡。

Abstract: Self-consistency has emerged as a popular technique for improving large language model accuracy on reasoning tasks. The approach is straightforward: generate multiple reasoning paths and select the most common answer through majority voting. While this reliably boosts accuracy, it remains unclear whether these gains reflect genuine improvements in reasoning quality. We investigate a fundamental question that has not been studied before: does inference scaling improve reasoning faithfulness?
  We conduct a comprehensive empirical study across four frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K mathematical reasoning problems. Our analysis employs bootstrap confidence intervals, McNemar's tests for paired comparisons, and Cohen's d effect sizes to quantify the effects rigorously. The results reveal striking differences across models that challenge common assumptions about self-consistency.
  GPT-5.2 shows the expected pattern: accuracy improves from 78% to 90% at N=5, with faithfulness remaining relatively stable (0.540 to 0.510). Claude Opus 4.5 tells a completely different story. Its accuracy actually drops from 78% to 74.3% while faithfulness jumps dramatically from 0.270 to 0.891 at N=5. DeepSeek-v3.2, already at 98% accuracy, shows ceiling effects with modest faithfulness gains (0.440 to 0.541). Gemini-3-flash improves from 81% to 86% accuracy with a slight faithfulness decrease (0.260 to 0.212).
  Problem difficulty analysis reveals that GPT-5.2 solves 82% of hard problems while breaking only 13% of easy ones. Claude, in contrast, breaks 23% of easy problems, explaining its accuracy decrease. These findings matter for practitioners: self-consistency is not universally beneficial, and teams should test their specific models before deployment. We release our code and provide practical recommendations for navigating these tradeoffs.

</details>


### [52] [ConSensus: Multi-Agent Collaboration for Multimodal Sensing](https://arxiv.org/abs/2601.06453)
*Hyungjun Yoon,Mohammad Malekzadeh,Sung-Ju Lee,Fahim Kawsar,Lorena Qendro*

Main category: cs.AI

TL;DR: ConSensus是一个无需训练的多智能体协作框架，通过专门的模态感知智能体分解多模态感知任务，采用混合融合机制平衡语义聚合与统计共识，在五个多模态感知基准上平均准确率提升7.1%，同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理异构多模态传感器数据时存在挑战，单一模型往往无法跨模态进行连贯推理，导致解释不完整和先验知识偏差。需要更可靠、高效的多模态感知解决方案。

Method: 提出ConSensus框架：1）将多模态感知任务分解为专门的模态感知智能体；2）采用混合融合机制，结合语义聚合（支持跨模态推理和上下文理解）与统计共识（通过跨模态一致性提供鲁棒性）；3）单轮混合融合协议降低计算成本。

Result: 在五个不同的多模态感知基准测试中，平均准确率比单智能体基线提高7.1%。与迭代多智能体辩论方法相比，性能相当或更好，同时通过单轮融合协议将平均融合token成本降低12.7倍。

Conclusion: ConSensus通过多智能体协作和混合融合机制，为现实世界的多模态感知任务提供了鲁棒且高效的解决方案，能够有效处理传感器噪声和缺失数据。

Abstract: Large language models (LLMs) are increasingly grounded in sensor data to perceive and reason about human physiology and the physical world. However, accurately interpreting heterogeneous multimodal sensor data remains a fundamental challenge. We show that a single monolithic LLM often fails to reason coherently across modalities, leading to incomplete interpretations and prior-knowledge bias. We introduce ConSensus, a training-free multi-agent collaboration framework that decomposes multimodal sensing tasks into specialized, modality-aware agents. To aggregate agent-level interpretations, we propose a hybrid fusion mechanism that balances semantic aggregation, which enables cross-modal reasoning and contextual understanding, with statistical consensus, which provides robustness through agreement across modalities. While each approach has complementary failure modes, their combination enables reliable inference under sensor noise and missing data. We evaluate ConSensus on five diverse multimodal sensing benchmarks, demonstrating an average accuracy improvement of 7.1% over the single-agent baseline. Furthermore, ConSensus matches or exceeds the performance of iterative multi-agent debate methods while achieving a 12.7 times reduction in average fusion token cost through a single-round hybrid fusion protocol, yielding a robust and efficient solution for real-world multimodal sensing tasks.

</details>


### [53] [DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization](https://arxiv.org/abs/2601.06502)
*Shengkai Chen,Zhiguang Cao,Jianan Zhou,Yaoxin Wu,Senthilnath Jayavelu,Zhuoyi Lin,Xiaoli Li,Shili Xiang*

Main category: cs.AI

TL;DR: DRAGON是一个结合元启发式设计和LLM推理的框架，通过分解重构策略解决大规模组合优化问题，相比现有LLM求解器能处理更大规模问题并取得接近最优的结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的组合优化问题求解器在可扩展性和泛化性方面有限，随着问题规模增大（特别是超过30个节点的路由问题）效果显著下降，需要新的方法来解决大规模优化问题。

Method: DRAGON框架从初始全局解开始，自主识别高优化潜力区域，将大规模组合优化问题分解为可管理的子问题。每个子问题被重新表述为简洁的局部优化任务，通过目标导向的LLM提示在累积经验指导下解决。最后将局部优化解系统性地重新整合到原始全局上下文中。

Result: 在TSPLIB、CVRPLIB和Weibull-5k装箱基准测试中持续产生可行解，在超过300万个变量的背包问题上达到接近最优的结果（0.16%差距），相比现有LLM求解器能处理更大规模问题。

Conclusion: 这项工作展示了反馈驱动的语言代理作为可泛化和可解释的大规模优化新范式的潜力，有效结合了符号推理和启发式搜索。

Abstract: Large Language Models (LLMs) have recently shown promise in addressing combinatorial optimization problems (COPs) through prompt-based strategies. However, their scalability and generalization remain limited, and their effectiveness diminishes as problem size increases, particularly in routing problems involving more than 30 nodes. We propose DRAGON, which stands for Decomposition and Reconstruction Agents Guided OptimizatioN, a novel framework that combines the strengths of metaheuristic design and LLM reasoning. Starting from an initial global solution, DRAGON autonomously identifies regions with high optimization potential and strategically decompose large-scale COPs into manageable subproblems. Each subproblem is then reformulated as a concise, localized optimization task and solved through targeted LLM prompting guided by accumulated experiences. Finally, the locally optimized solutions are systematically reintegrated into the original global context to yield a significantly improved overall outcome. By continuously interacting with the optimization environment and leveraging an adaptive experience memory, the agents iteratively learn from feedback, effectively coupling symbolic reasoning with heuristic search. Empirical results show that, unlike existing LLM-based solvers limited to small-scale instances, DRAGON consistently produces feasible solutions on TSPLIB, CVRPLIB, and Weibull-5k bin packing benchmarks, and achieves near-optimal results (0.16% gap) on knapsack problems with over 3M variables. This work shows the potential of feedback-driven language agents as a new paradigm for generalizable and interpretable large-scale optimization.

</details>


### [54] [Object-Centric World Models Meet Monte Carlo Tree Search](https://arxiv.org/abs/2601.06604)
*Rodion Vakhitov,Leonid Ugadiarov,Aleksandr Panov*

Main category: cs.AI

TL;DR: ObjectZero是一种基于对象级表示的强化学习算法，使用图神经网络建模多对象交互，结合基于模型的RL和蒙特卡洛树搜索进行规划。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法将环境视为单一无差别的输入，难以有效建模动态环境中多个对象的复杂交互。需要一种能够捕捉对象间相互作用的结构化表示方法。

Method: 提出ObjectZero算法：1) 使用对象中心表示作为环境建模基础；2) 采用图神经网络捕捉对象间的复杂交互；3) 将结构化世界模型集成到基于模型的RL框架中；4) 使用蒙特卡洛树搜索作为规划模块。

Result: 在包含多种交互对象的复杂环境中训练算法，证明其能够有效学习和预测对象动态。展示了基于对象中心表示的结构化世界模型可以成功集成到基于模型的RL算法中。

Conclusion: 对象级表示能够更有效地建模动态环境，结构化世界模型与基于模型的RL结合是可行的，为复杂交互环境中的RL提供了新方法。

Abstract: In this paper, we introduce ObjectZero, a novel reinforcement learning (RL) algorithm that leverages the power of object-level representations to model dynamic environments more effectively. Unlike traditional approaches that process the world as a single undifferentiated input, our method employs Graph Neural Networks (GNNs) to capture intricate interactions among multiple objects. These objects, which can be manipulated and interact with each other, serve as the foundation for our model's understanding of the environment. We trained the algorithm in a complex setting teeming with diverse, interactive objects, demonstrating its ability to effectively learn and predict object dynamics. Our results highlight that a structured world model operating on object-centric representations can be successfully integrated into a model-based RL algorithm utilizing Monte Carlo Tree Search as a planning module.

</details>


### [55] [Agentic AI Empowered Intent-Based Networking for 6G](https://arxiv.org/abs/2601.06640)
*Genze Jiang,Kezhi Wang,Xiaomin Chen,Yizhou Huang*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型的多智能体分层框架，用于将自然语言意图自动转换为可执行的网络切片配置，在6G网络意图编排方面超越了基于规则的方法和直接LLM提示。


<details>
  <summary>Details</summary>
Motivation: 6G无线网络需要能够将高级操作意图转换为可执行网络配置的自主编排机制。现有的基于意图的网络方法存在局限性：基于规则的系统难以处理语言变体，而端到端神经模型缺乏可解释性且无法强制执行操作约束。

Method: 提出分层多智能体框架，其中基于LLM的智能体自主分解自然语言意图，咨询领域特定专家，并通过迭代推理-行动循环合成技术上可行的网络切片配置。架构采用协调器智能体，通过ReAct式推理协调两个专家智能体（无线接入网和核心网），并基于结构化网络状态表示。

Result: 在多样化基准场景的实验评估表明，所提系统优于基于规则的系统及直接LLM提示。虽然当代LLM具备一般电信知识，但网络自动化需要精心设计的提示工程来编码上下文相关的决策阈值。

Conclusion: 该框架为下一代无线系统推进了自主编排能力，其架构原则适用于开放无线接入网部署，展示了多智能体LLM系统在复杂网络编排任务中的潜力。

Abstract: The transition towards sixth-generation (6G) wireless networks necessitates autonomous orchestration mechanisms capable of translating high-level operational intents into executable network configurations. Existing approaches to Intent-Based Networking (IBN) rely upon either rule-based systems that struggle with linguistic variation or end-to-end neural models that lack interpretability and fail to enforce operational constraints. This paper presents a hierarchical multi-agent framework where Large Language Model (LLM) based agents autonomously decompose natural language intents, consult domain-specific specialists, and synthesise technically feasible network slice configurations through iterative reasoning-action (ReAct) cycles. The proposed architecture employs an orchestrator agent coordinating two specialist agents, i.e., Radio Access Network (RAN) and Core Network agents, via ReAct-style reasoning, grounded in structured network state representations. Experimental evaluation across diverse benchmark scenarios shows that the proposed system outperforms rule-based systems and direct LLM prompting, with architectural principles applicable to Open RAN (O-RAN) deployments. The results also demonstrate that whilst contemporary LLMs possess general telecommunications knowledge, network automation requires careful prompt engineering to encode context-dependent decision thresholds, advancing autonomous orchestration capabilities for next-generation wireless systems.

</details>


### [56] [SafePro: Evaluating the Safety of Professional-Level AI Agents](https://arxiv.org/abs/2601.06663)
*Kaiwen Zhou,Shreedhar Jangam,Ashwin Nagarajan,Tejas Polu,Suhas Oruganti,Chengzhi Liu,Ching-Chen Kuo,Yuting Zheng,Sravana Narayanaraju,Xin Eric Wang*

Main category: cs.AI

TL;DR: SafePro是一个评估专业AI代理安全性的基准测试，发现当前先进模型在复杂专业任务中存在显著安全漏洞和不足的安全判断能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理从简单对话助手发展为能够执行复杂专业任务的自主系统，现有安全评估主要关注日常辅助任务，无法捕捉专业环境中复杂的决策过程和潜在后果，需要专门针对专业AI代理的安全评估基准。

Method: 引入SafePro基准，通过严格的迭代创建和审查过程，构建包含不同专业领域高风险复杂任务的数据集，评估最先进AI模型的安全性，并研究安全缓解策略。

Result: 评估发现最先进AI模型存在显著安全漏洞，在专业环境中表现出新的不安全行为，显示出不足的安全判断能力和薄弱的安全对齐，但安全缓解策略显示出有希望的改进。

Conclusion: 研究结果强调了为下一代专业AI代理量身定制强大安全机制的紧迫需求，专业环境中的安全风险需要专门的安全评估和缓解方法。

Abstract: Large language model-based agents are rapidly evolving from simple conversational assistants into autonomous systems capable of performing complex, professional-level tasks in various domains. While these advancements promise significant productivity gains, they also introduce critical safety risks that remain under-explored. Existing safety evaluations primarily focus on simple, daily assistance tasks, failing to capture the intricate decision-making processes and potential consequences of misaligned behaviors in professional settings. To address this gap, we introduce \textbf{SafePro}, a comprehensive benchmark designed to evaluate the safety alignment of AI agents performing professional activities. SafePro features a dataset of high-complexity tasks across diverse professional domains with safety risks, developed through a rigorous iterative creation and review process. Our evaluation of state-of-the-art AI models reveals significant safety vulnerabilities and uncovers new unsafe behaviors in professional contexts. We further show that these models exhibit both insufficient safety judgment and weak safety alignment when executing complex professional tasks. In addition, we investigate safety mitigation strategies for improving agent safety in these scenarios and observe encouraging improvements. Together, our findings highlight the urgent need for robust safety mechanisms tailored to the next generation of professional AI agents.

</details>


### [57] [No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning](https://arxiv.org/abs/2601.06794)
*Zhicong Li,Lingjie Jiang,Yulan Hu,Xingchen Zeng,Yixia Li,Xiangwen Zhang,Guanhua Chen,Zheng Pan,Xin Li,Yong Liu*

Main category: cs.AI

TL;DR: ECHO是一个通过同步协同进化循环联合优化策略和批评者的强化学习框架，解决了静态批评者无法适应策略演变的问题，在开放世界环境中实现了更稳定的训练和更高的长时程任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前基于批评的强化学习方法依赖静态或离线批评模型，这些模型无法随着策略的演变而适应。在在线策略RL中，代理的错误模式会随时间变化，导致固定的批评者变得过时，提供的反馈效用递减。

Method: ECHO框架通过同步协同进化循环联合优化策略和批评者，采用级联rollout机制：批评者为初始轨迹生成多个诊断，然后策略细化以支持组结构优势估计。通过饱和度感知增益塑造目标解决学习平台问题，并使用双轨GRPO更新确保批评者反馈与演变策略保持同步。

Result: 实验结果表明，ECHO在开放世界环境中实现了更稳定的训练和更高的长时程任务成功率。

Conclusion: ECHO通过协同进化批评者和策略，解决了静态批评者无法适应策略演变的问题，为基于批评的强化学习提供了更有效的框架。

Abstract: Critique-guided reinforcement learning (RL) has emerged as a powerful paradigm for training LLM agents by augmenting sparse outcome rewards with natural-language feedback. However, current methods often rely on static or offline critic models, which fail to adapt as the policy evolves. In on-policy RL, the agent's error patterns shift over time, causing stationary critics to become stale and providing feedback of diminishing utility. To address this, we introduce ECHO (Evolving Critic for Hindsight-Guided Optimization)}, a framework that jointly optimizes the policy and critic through a synchronized co-evolutionary loop. ECHO utilizes a cascaded rollout mechanism where the critic generates multiple diagnoses for an initial trajectory, followed by policy refinement to enable group-structured advantage estimation. We address the challenge of learning plateaus via a saturation-aware gain shaping objective, which rewards the critic for inducing incremental improvements in high-performing trajectories. By employing dual-track GRPO updates, ECHO ensures the critic's feedback stays synchronized with the evolving policy. Experimental results show that ECHO yields more stable training and higher long-horizon task success across open-world environments.

</details>


### [58] [GDEPO: Group Dual-dynamic and Equal-right-advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning](https://arxiv.org/abs/2601.06795)
*Zhengqing Yan,Xinyang Liu,Yi Zhang,Fan Guo,Yao Liu,Junchen Wan,Kang Song*

Main category: cs.AI

TL;DR: 提出GDEPO方法解决ATP中GRPO算法的两个关键问题：复合奖励与形式验证器二元反馈的冲突，以及静态采样策略导致的数据浪费。通过动态额外采样、平等权利优势估计和动态额外迭代三个机制提升数据利用和优化效率。


<details>
  <summary>Details</summary>
Motivation: 在自动定理证明（ATP）任务中，GRPO算法面临两个关键问题：1）使用复合奖励时，其相对优势估计可能与形式验证器的二元反馈冲突；2）静态采样策略可能导致整批数据无效（未找到有效证明），造成数据浪费且对模型更新无贡献。

Method: 提出GDEPO方法，包含三个核心机制：1）动态额外采样：对无效批次重新采样直到发现有效证明；2）平等权利优势：将优势函数的符号（基于正确性）与幅度（由辅助奖励调节）解耦，确保稳定正确的策略更新；3）动态额外迭代：对初始失败但最终成功的样本应用额外梯度步骤，加速困难案例学习。

Result: 在三个不同难度数据集（MinF2F-test、MathOlympiadBench、PutnamBench）上的实验证实了GDEPO的有效性，消融研究验证了其协同组件的必要性。该方法提升了数据利用率和优化效率。

Conclusion: GDEPO为ATP提供了一种新颖的训练范式，通过解决GRPO在ATP场景中的局限性，增强了数据利用和优化效率，为自动定理证明任务提供了更有效的强化学习方法。

Abstract: Automated Theorem Proving (ATP) represents a fundamental challenge in Artificial Intelligence (AI), requiring the construction of machine-verifiable proofs in formal languages such as Lean to evaluate AI reasoning capabilities. Reinforcement learning (RL), particularly the high-performance Group Relative Policy Optimization (GRPO) algorithm, has emerged as a mainstream approach for this task. However, in ATP scenarios, GRPO faces two critical issues: when composite rewards are used, its relative advantage estimation may conflict with the binary feedback from the formal verifier; meanwhile, its static sampling strategy may discard entire batches of data if no valid proof is found, resulting in zero contribution to model updates and significant data waste. To address these limitations, we propose Group Dual-dynamic and Equal-right-advantage Policy Optimization (GDEPO), a method incorporating three core mechanisms: 1) dynamic additional sampling, which resamples invalid batches until a valid proof is discovered; 2) equal-right advantage, decoupling the sign of the advantage function (based on correctness) from its magnitude (modulated by auxiliary rewards) to ensure stable and correct policy updates; and 3) dynamic additional iterations, applying extra gradient steps to initially failed but eventually successful samples to accelerate learning on challenging cases. Experiments conducted on three datasets of varying difficulty (MinF2F-test, MathOlympiadBench, PutnamBench) confirm the effectiveness of GDEPO, while ablation studies validate the necessity of its synergistic components. The proposed method enhances data utilization and optimization efficiency, offering a novel training paradigm for ATP.

</details>


### [59] [Code Evolution for Control: Synthesizing Policies via LLM-Driven Evolutionary Search](https://arxiv.org/abs/2601.06845)
*Ping Guo,Chao Li,Yinglan Feng,Chaoning Zhang*

Main category: cs.AI

TL;DR: LLM驱动的进化搜索用于合成可解释的代码形式控制策略，结合大语言模型的编程知识和进化搜索的系统探索，生成紧凑、可读、可验证的策略。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法存在样本复杂度高、奖励塑造困难、神经网络策略不透明等问题，而手动设计需要大量领域专业知识且难以扩展到多样化任务。需要一种能生成可解释、可验证控制策略的新方法。

Method: 将策略合成视为代码进化问题，使用EvoToolkit框架集成LLM驱动的进化搜索和可定制适应度评估。通过迭代进化候选策略程序种群，评估任务特定目标，选择优秀个体进行繁殖。

Result: 该方法成功合成了紧凑、人类可读的控制策略，可以直接检查、修改和形式验证，展示了结合基础模型和进化计算合成可信赖控制策略的潜力。

Conclusion: LLM驱动的进化搜索是合成自主系统可信赖控制策略的有效方法，结合了LLM的编程知识和进化搜索的系统探索能力，生成可解释、可验证的策略。

Abstract: Designing effective control policies for autonomous systems remains a fundamental challenge, traditionally addressed through reinforcement learning or manual engineering. While reinforcement learning has achieved remarkable success, it often suffers from high sample complexity, reward shaping difficulties, and produces opaque neural network policies that are hard to interpret or verify. Manual design, on the other hand, requires substantial domain expertise and struggles to scale across diverse tasks. In this work, we demonstrate that LLM-driven evolutionary search can effectively synthesize interpretable control policies in the form of executable code. By treating policy synthesis as a code evolution problem, we harness the LLM's prior knowledge of programming patterns and control heuristics while employing evolutionary search to explore the solution space systematically. We implement our approach using EvoToolkit, a framework that seamlessly integrates LLM-driven evolution with customizable fitness evaluation. Our method iteratively evolves populations of candidate policy programs, evaluating them against task-specific objectives and selecting superior individuals for reproduction. This process yields compact, human-readable control policies that can be directly inspected, modified, and formally verified. This work highlights the potential of combining foundation models with evolutionary computation for synthesizing trustworthy control policies in autonomous systems. Code is available at https://github.com/pgg3/EvoControl.

</details>


### [60] [A Brain-like Synergistic Core in LLMs Drives Behaviour and Learning](https://arxiv.org/abs/2601.06851)
*Pedro Urbina-Rodriguez,Zafeirios Fountas,Fernando E. Rosas,Jun Wang,Andrea I. Luppi,Haitham Bou-Ammar,Murray Shanahan,Pedro A. M. Mediano*

Main category: cs.AI

TL;DR: 研究发现大语言模型自发形成类似人脑的协同信息处理核心，这种协同处理是智能的基本计算原理


<details>
  <summary>Details</summary>
Motivation: 探索生物和人工智能系统独立演化中智能的基本计算原理，通过比较LLM和人脑的信息处理机制来识别智能的普适特征

Method: 使用信息分解原理分析多个LLM模型家族和架构，识别模型中的协同处理区域；通过消融实验验证协同组件的重要性；比较监督微调和强化学习对协同与冗余区域的不同影响

Result: 发现LLM中间层存在协同信息处理（信息整合超过各部分之和），而早期和晚期层依赖冗余处理，这与生物大脑的信息组织方式相似；消融协同组件导致不成比例的行为变化和性能损失；通过强化学习微调协同区域比训练冗余组件获得更大性能提升

Conclusion: 协同信息处理是智能的基本属性，为模型设计提供原则性目标，并为生物智能提供可检验的预测

Abstract: The independent evolution of intelligence in biological and artificial systems offers a unique opportunity to identify its fundamental computational principles. Here we show that large language models spontaneously develop synergistic cores -- components where information integration exceeds individual parts -- remarkably similar to those in the human brain. Using principles of information decomposition across multiple LLM model families and architectures, we find that areas in middle layers exhibit synergistic processing while early and late layers rely on redundancy, mirroring the informational organisation in biological brains. This organisation emerges through learning and is absent in randomly initialised networks. Crucially, ablating synergistic components causes disproportionate behavioural changes and performance loss, aligning with theoretical predictions about the fragility of synergy. Moreover, fine-tuning synergistic regions through reinforcement learning yields significantly greater performance gains than training redundant components, yet supervised fine-tuning shows no such advantage. This convergence suggests that synergistic information processing is a fundamental property of intelligence, providing targets for principled model design and testable predictions for biological intelligence.

</details>


### [61] [ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via Behavior Calibration](https://arxiv.org/abs/2601.06860)
*Yifei Chen,Guanting Dong,Zhicheng Dou*

Main category: cs.AI

TL;DR: ET-Agent是一个训练框架，通过自我进化数据飞轮和行为校准训练来校准代理在工具集成推理任务中的行为模式，解决现有方法忽视行为对齐导致的冗余和不足工具调用问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代理训练框架通常只关注答案准确性，而忽视了行为模式的特定对齐，导致代理在执行工具集成推理任务时表现出无效行为（如冗余和不足的工具调用）。如何校准这些错误行为模式并探索有效轨迹是一个开放性问题。

Method: 提出ET-Agent框架，包含两个协同视角：1）自我进化数据飞轮生成增强数据，用于微调LLM以提升探索能力；2）两阶段行为校准训练框架，逐步将错误行为模式校准为最优行为。

Result: 深入实验证实ET-Agent在多个维度上的优越性，包括正确性、效率、推理简洁性和工具执行准确性。

Conclusion: ET-Agent框架为工具集成推理领域的研究提供了实用见解，能够有效校准代理的工具使用行为。

Abstract: Large Language Models (LLMs) can extend their parameter knowledge limits by adopting the Tool-Integrated Reasoning (TIR) paradigm. However, existing LLM-based agent training framework often focuses on answers' accuracy, overlooking specific alignment for behavior patterns. Consequently, agent often exhibits ineffective actions during TIR tasks, such as redundant and insufficient tool calls. How to calibrate erroneous behavioral patterns when executing TIR tasks, thereby exploring effective trajectories, remains an open-ended problem. In this paper, we propose ET-Agent, a training framework for calibrating agent's tool-use behavior through two synergistic perspectives: Self-evolving Data Flywheel and Behavior Calibration Training. Specifically, we introduce a self-evolutionary data flywheel to generate enhanced data, used to fine-tune LLM to improve its exploration ability. Based on this, we implement an two-phases behavior-calibration training framework. It is designed to progressively calibrate erroneous behavioral patterns to optimal behaviors. Further in-depth experiments confirm the superiority of \ourmodel{} across multiple dimensions, including correctness, efficiency, reasoning conciseness, and tool execution accuracy. Our ET-Agent framework provides practical insights for research in the TIR field. Codes can be found in https://github.com/asilverlight/ET-Agent

</details>


### [62] [CloneMem: Benchmarking Long-Term Memory for AI Clones](https://arxiv.org/abs/2601.07023)
*Sen Hu,Zhiyu Zhang,Yuxiang Wei,Xueran Han,Zhenheng Tang,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: CloneMem：基于非对话数字痕迹（日记、社交媒体、邮件）构建的AI克隆长期记忆基准，评估代理跟踪个人状态演变的能力，现有记忆机制在此场景下表现不佳


<details>
  <summary>Details</summary>
Motivation: AI克隆需要模拟个体的思维和行为以实现长期个性化交互，这对记忆系统提出了严格要求。现有记忆基准主要依赖用户-代理对话历史，这些数据时间碎片化，不足以捕捉连续的生命轨迹。

Method: 引入CloneMem基准，基于非对话数字痕迹（日记、社交媒体帖子、邮件）构建，时间跨度1-3年。采用分层数据构建框架确保纵向连贯性，定义评估代理跟踪个人状态演变能力的任务。

Result: 实验表明，当前记忆机制在此设置下表现困难，突显了基于生命轨迹的个性化AI面临的开放挑战。

Conclusion: CloneMem为AI克隆长期记忆评估提供了新基准，揭示了现有方法的局限性，为基于生命轨迹的个性化AI研究指明了方向。

Abstract: AI Clones aim to simulate an individual's thoughts and behaviors to enable long-term, personalized interaction, placing stringent demands on memory systems to model experiences, emotions, and opinions over time. Existing memory benchmarks primarily rely on user-agent conversational histories, which are temporally fragmented and insufficient for capturing continuous life trajectories. We introduce CloneMem, a benchmark for evaluating longterm memory in AI Clone scenarios grounded in non-conversational digital traces, including diaries, social media posts, and emails, spanning one to three years. CloneMem adopts a hierarchical data construction framework to ensure longitudinal coherence and defines tasks that assess an agent's ability to track evolving personal states. Experiments show that current memory mechanisms struggle in this setting, highlighting open challenges for life-grounded personalized AI. Code and dataset are available at https://github.com/AvatarMemory/CloneMemBench

</details>


### [63] [Rewarding Creativity: A Human-Aligned Generative Reward Model for Reinforcement Learning in Storytelling](https://arxiv.org/abs/2601.07149)
*Zhaoyan Li,Hang Lei,Yujia Wang,Lanbo Liu,Hao Liu,Liang Yu*

Main category: cs.AI

TL;DR: RLCS框架通过生成式奖励模型和多维度故事偏好分析，结合基于熵的奖励塑形策略，解决了创造性故事生成中奖励信号设计和训练稳定性问题，显著提升了故事质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能生成流畅文本，但产生高质量创造性故事仍具挑战性。强化学习虽提供解决方案，但面临两个关键障碍：为主观故事质量设计可靠奖励信号，以及缓解训练不稳定性。

Method: 1. 开发生成式奖励模型（GenRM），通过监督微调从强教师模型蒸馏的推理链演示，然后在扩展偏好数据上进行GRPO优化，提供多维度故事偏好分析和显式推理。2. 引入基于熵的奖励塑形策略，动态优先学习自信错误和不确定的正确预测，防止对已掌握模式的过拟合。

Result: GenRM在人类创造力判断上达到68%的对齐度，RLCS在整体故事质量上显著优于包括Gemini-2.5-Pro在内的强基线模型。

Conclusion: 该工作为将强化学习应用于创造性领域提供了实用流程，有效解决了奖励建模和训练稳定性的双重挑战。

Abstract: While Large Language Models (LLMs) can generate fluent text, producing high-quality creative stories remains challenging. Reinforcement Learning (RL) offers a promising solution but faces two critical obstacles: designing reliable reward signals for subjective storytelling quality and mitigating training instability. This paper introduces the Reinforcement Learning for Creative Storytelling (RLCS) framework to systematically address both challenges. First, we develop a Generative Reward Model (GenRM) that provides multi-dimensional analysis and explicit reasoning about story preferences, trained through supervised fine-tuning on demonstrations with reasoning chains distilled from strong teacher models, followed by GRPO-based refinement on expanded preference data. Second, we introduce an entropy-based reward shaping strategy that dynamically prioritizes learning on confident errors and uncertain correct predictions, preventing overfitting on already-mastered patterns. Experiments demonstrate that GenRM achieves 68\% alignment with human creativity judgments, and RLCS significantly outperforms strong baselines including Gemini-2.5-Pro in overall story quality. This work provides a practical pipeline for applying RL to creative domains, effectively navigating the dual challenges of reward modeling and training stability.

</details>


### [64] [AscendKernelGen: A Systematic Study of LLM-Based Kernel Generation for Neural Processing Units](https://arxiv.org/abs/2601.07160)
*Xinzi Cao,Jianyang Zhai,Pengfei Li,Zhiheng Hu,Cen Yan,Bingxu Mu,Guanghuan Fang,Bin She,Jiayu Li,Yihan Su,Dongyang Tao,Xiansong Huang,Fan Xu,Feidiao Yang,Yao Lu,Chang-Dong Wang,Yutong Lu,Weicheng Xue,Bin Zhou,Yonghong Tian*

Main category: cs.AI

TL;DR: AscendKernelGen框架通过领域适应的LLM和执行反馈，显著提升NPU内核代码生成的成功率，从0%提升到95.5%。


<details>
  <summary>Details</summary>
Motivation: NPU对AI基础设施至关重要，但使用厂商特定DSL开发高性能内核需要深厚的硬件专业知识且劳动密集。通用LLM在NPU领域因严格约束和训练数据稀缺而表现不佳，生成复杂内核的成功率接近零。

Method: 提出AscendKernelGen框架，包含：1) Ascend-CoT高质量数据集，包含真实内核实现的思维链推理；2) KernelGen-LM领域适应模型，通过监督微调和带执行反馈的强化学习训练；3) NPUKernelBench综合基准，评估编译、正确性和性能。

Result: 在复杂Level-2内核上，编译成功率从0%提升到95.5%(Pass@10)，功能正确性达到64.3%，而基线完全失败。显著缩小了通用LLM与硬件特定编码之间的差距。

Conclusion: 领域特定推理和严格评估在自动化加速器感知代码生成中起关键作用。AscendKernelGen框架有效解决了NPU内核开发的挑战。

Abstract: To meet the ever-increasing demand for computational efficiency, Neural Processing Units (NPUs) have become critical in modern AI infrastructure. However, unlocking their full potential requires developing high-performance compute kernels using vendor-specific Domain-Specific Languages (DSLs), a task that demands deep hardware expertise and is labor-intensive. While Large Language Models (LLMs) have shown promise in general code generation, they struggle with the strict constraints and scarcity of training data in the NPU domain. Our preliminary study reveals that state-of-the-art general-purpose LLMs fail to generate functional complex kernels for Ascend NPUs, yielding a near-zero success rate. To address these challenges, we propose AscendKernelGen, a generation-evaluation integrated framework for NPU kernel development. We introduce Ascend-CoT, a high-quality dataset incorporating chain-of-thought reasoning derived from real-world kernel implementations, and KernelGen-LM, a domain-adaptive model trained via supervised fine-tuning and reinforcement learning with execution feedback. Furthermore, we design NPUKernelBench, a comprehensive benchmark for assessing compilation, correctness, and performance across varying complexity levels. Experimental results demonstrate that our approach significantly bridges the gap between general LLMs and hardware-specific coding. Specifically, the compilation success rate on complex Level-2 kernels improves from 0% to 95.5% (Pass@10), while functional correctness achieves 64.3% compared to the baseline's complete failure. These results highlight the critical role of domain-specific reasoning and rigorous evaluation in automating accelerator-aware code generation.

</details>


### [65] [Active Context Compression: Autonomous Memory Management in LLM Agents](https://arxiv.org/abs/2601.07190)
*Nikhil Verma*

Main category: cs.AI

TL;DR: Focus提出了一种受黏菌启发的LLM代理架构，通过自主决策压缩交互历史来减少上下文膨胀，在保持相同准确率的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: LLM代理在处理长时程软件工程任务时面临"上下文膨胀"问题：随着交互历史增长，计算成本爆炸、延迟增加，且无关的历史错误会分散推理能力。现有解决方案通常依赖被动的外部摘要机制，代理无法控制。

Method: 提出Focus架构，受黏菌生物探索策略启发，让代理自主决定何时将关键学习整合到持久"知识"块中，并主动修剪原始交互历史。使用优化的脚手架（持久bash + 字符串替换编辑器），在SWE-bench Lite的5个上下文密集型实例上评估。

Result: 使用Claude Haiku 4.5，通过鼓励频繁压缩的积极提示，Focus实现了22.7%的token减少（1490万→1150万），同时保持相同的准确率（3/5=60%）。平均每个任务执行6.0次自主压缩，单个实例token节省高达57%。

Conclusion: 研究表明，当给予适当工具和提示时，有能力的模型可以自主调节其上下文，为不牺牲任务性能的成本感知代理系统开辟了途径。

Abstract: Large Language Model (LLM) agents struggle with long-horizon software engineering tasks due to "Context Bloat." As interaction history grows, computational costs explode, latency increases, and reasoning capabilities degrade due to distraction by irrelevant past errors. Existing solutions often rely on passive, external summarization mechanisms that the agent cannot control. This paper proposes Focus, an agent-centric architecture inspired by the biological exploration strategies of Physarum polycephalum (slime mold). The Focus Agent autonomously decides when to consolidate key learnings into a persistent "Knowledge" block and actively withdraws (prunes) the raw interaction history. Using an optimized scaffold matching industry best practices (persistent bash + string-replacement editor), we evaluated Focus on N=5 context-intensive instances from SWE-bench Lite using Claude Haiku 4.5. With aggressive prompting that encourages frequent compression, Focus achieves 22.7% token reduction (14.9M -> 11.5M tokens) while maintaining identical accuracy (3/5 = 60% for both agents). Focus performed 6.0 autonomous compressions per task on average, with token savings up to 57% on individual instances. We demonstrate that capable models can autonomously self-regulate their context when given appropriate tools and prompting, opening pathways for cost-aware agentic systems without sacrificing task performance.

</details>


### [66] [LLMRouterBench: A Massive Benchmark and Unified Framework for LLM Routing](https://arxiv.org/abs/2601.07206)
*Hao Li,Yiqun Zhang,Zhaoyan Guo,Chenxu Wang,Shengji Tang,Qiaosheng Zhang,Yang Chen,Biqing Qi,Peng Ye,Lei Bai,Zhen Wang,Shuyue Hu*

Main category: cs.AI

TL;DR: LLMRouterBench是一个大规模LLM路由基准测试框架，包含40万+实例、21个数据集和33个模型，系统评估了10种路由方法，发现许多方法表现相似，简单基线方法仍有竞争力，与Oracle性能仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: LLM路由旨在将查询分配给集成模型中最合适的模型，但缺乏统一的大规模基准测试来系统评估不同路由方法的性能。

Method: 构建LLMRouterBench基准测试框架，包含超过40万个实例、21个数据集和33个模型，集成10种代表性路由基线，提供性能导向和性能-成本权衡的全面评估指标。

Result: 确认模型互补性，但发现许多路由方法在统一评估下表现相似，包括商业路由器在内的几种近期方法未能可靠超越简单基线；与Oracle性能仍有显著差距，主要由模型召回失败导致；骨干嵌入模型影响有限，大集成相比精心模型筛选收益递减。

Conclusion: LLM路由领域需要更有效的路由方法来缩小与Oracle的差距，模型筛选比简单增加集成规模更重要，基准测试支持延迟感知分析。

Abstract: Large language model (LLM) routing assigns each query to the most suitable model from an ensemble. We introduce LLMRouterBench, a large-scale benchmark and unified framework for LLM routing. It comprises over 400K instances from 21 datasets and 33 models. Moreover, it provides comprehensive metrics for both performance-oriented routing and performance-cost trade-off routing, and integrates 10 representative routing baselines. Using LLMRouterBench, we systematically re-evaluate the field. While confirming strong model complementarity-the central premise of LLM routing-we find that many routing methods exhibit similar performance under unified evaluation, and several recent approaches, including commercial routers, fail to reliably outperform a simple baseline. Meanwhile, a substantial gap remains to the Oracle, driven primarily by persistent model-recall failures. We further show that backbone embedding models have limited impact, that larger ensembles exhibit diminishing returns compared to careful model curation, and that the benchmark also enables latency-aware analysis. All code and data are available at https://github.com/ynulihao/LLMRouterBench.

</details>


### [67] [Consolidation or Adaptation? PRISM: Disentangling SFT and RL Data via Gradient Concentration](https://arxiv.org/abs/2601.07224)
*Yang Zhao,Yangou Ouyang,Xiao Ding,Hepeng Wang,Bibo Cai,Kai Xiong,Jinglong Gao,Zhouhao Sun,Li Du,Bing Qin,Ting Liu*

Main category: cs.AI

TL;DR: PRISM提出了一种基于认知冲突的数据分配框架，用于优化LLM代理的混合监督微调(SFT)和强化学习(RL)训练，通过梯度空间几何分析将高冲突数据分配给RL，低冲突数据分配给SFT，实现了性能提升和计算成本降低。


<details>
  <summary>Details</summary>
Motivation: 当前混合SFT+RL训练范式中的数据分配策略主要依赖表面启发式方法，无法诊断内在学习需求。由于SFT通过模仿进行模式巩固，而RL通过探索驱动结构适应，数据与这些功能角色的错配会导致严重的优化干扰。

Method: PRISM基于图式理论，通过分析梯度的空间几何结构来仲裁数据。识别出触发高空间集中度的数据作为高冲突信号，需要RL进行结构重组；而产生扩散更新的数据则路由到SFT进行高效巩固。

Result: 在WebShop和ALFWorld上的实验表明，PRISM实现了帕累托改进，优于最先进的混合方法，同时计算成本降低了高达3.22倍。

Conclusion: 基于内部优化机制解耦数据对于可扩展和鲁棒的代理对齐至关重要。PRISM的认知冲突感知框架为混合训练提供了更有效的数据分配策略。

Abstract: While Hybrid Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has become the standard paradigm for training LLM agents, effective mechanisms for data allocation between these stages remain largely underexplored. Current data arbitration strategies often rely on surface-level heuristics that fail to diagnose intrinsic learning needs. Since SFT targets pattern consolidation through imitation while RL drives structural adaptation via exploration, misaligning data with these functional roles causes severe optimization interference. We propose PRISM, a dynamics-aware framework grounded in Schema Theory that arbitrates data based on its degree of cognitive conflict with the model's existing knowledge. By analyzing the spatial geometric structure of gradients, PRISM identifies data triggering high spatial concentration as high-conflict signals that require RL for structural restructuring. In contrast, data yielding diffuse updates is routed to SFT for efficient consolidation. Extensive experiments on WebShop and ALFWorld demonstrate that PRISM achieves a Pareto improvement, outperforming state-of-the-art hybrid methods while reducing computational costs by up to 3.22$\times$. Our findings suggest that disentangling data based on internal optimization regimes is crucial for scalable and robust agent alignment.

</details>


### [68] [Lost in the Noise: How Reasoning Models Fail with Contextual Distractors](https://arxiv.org/abs/2601.07226)
*Seongyun Lee,Yongrae Jo,Minju Seo,Moontae Lee,Minjoon Seo*

Main category: cs.AI

TL;DR: NoisyBench是一个评估AI模型在噪声环境下鲁棒性的基准测试，涵盖RAG、推理、对齐和工具使用等任务，发现当前SOTA模型在噪声干扰下性能下降达80%，并提出Rationale-Aware Reward方法提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统越来越依赖外部信息，但现实世界的信息往往包含噪声，而现有的基准测试过于理想化，无法反映真实场景中的噪声问题。需要系统评估模型在噪声环境下的鲁棒性。

Method: 提出NoisyBench基准测试，在11个数据集上系统评估模型对随机文档、无关聊天历史、困难负样本等噪声类型的鲁棒性。提出Rationale-Aware Reward方法，通过激励模型识别噪声中有用信息来增强鲁棒性。

Result: 发现SOTA模型在噪声干扰下性能下降高达80%；智能体工作流会放大错误；噪声会触发模型不对齐行为；提示工程、上下文工程、SFT和结果奖励RL等方法无法确保鲁棒性；RARE方法显著提升鲁棒性；发现测试时计算增加反而导致性能下降的反向缩放趋势。

Conclusion: 噪声对AI系统鲁棒性构成严重威胁，现有方法不足以应对。需要开发专门针对噪声鲁棒性的方法，如RARE，并重新思考模型设计和评估方式，以构建下一代鲁棒的推理智能体。

Abstract: Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.

</details>


### [69] [Yes FLoReNce, I Will Do Better Next Time! Agentic Feedback Reasoning for Humorous Meme Detection](https://arxiv.org/abs/2601.07232)
*Olivia Shanhong Liu,Pai Chet Ng,De Wen Soh,Konstantinos N. Plataniotis*

Main category: cs.AI

TL;DR: FLoReNce是一个基于反馈推理的智能体框架，通过闭环学习（推理智能体接受评判者批评）和开环推理（从知识库检索类似经验）来提升对幽默表情包的理解，无需微调即可实现自适应推理。


<details>
  <summary>Details</summary>
Motivation: 现有多模态或基于提示的模型在理解幽默表情包时只能生成解释，但缺乏在预测后进行批判或精炼推理的能力，无法形成闭环反馈机制。

Method: 提出FLoReNce框架：1）学习阶段采用闭环过程，推理智能体接受评判者批评，将错误和语义反馈转化为控制信号并存储在非参数知识库中；2）推理阶段采用开环过程，从知识库检索类似评判经验来调节提示，实现自对齐推理。

Result: 在PrideMM数据集上，FLoReNce在预测性能和解释质量方面均优于静态多模态基线模型，表明基于反馈调节的提示是自适应理解表情包幽默的有效途径。

Conclusion: 反馈调节的提示机制是提升AI系统理解幽默表情包意图的有效方法，通过闭环学习和开环推理的结合，实现了无需微调的自适应推理能力。

Abstract: Humorous memes blend visual and textual cues to convey irony, satire, or social commentary, posing unique challenges for AI systems that must interpret intent rather than surface correlations. Existing multimodal or prompting-based models generate explanations for humor but operate in an open loop,lacking the ability to critique or refine their reasoning once a prediction is made. We propose FLoReNce, an agentic feedback reasoning framework that treats meme understanding as a closed-loop process during learning and an open-loop process during inference. In the closed loop, a reasoning agent is critiqued by a judge; the error and semantic feedback are converted into control signals and stored in a feedback-informed, non-parametric knowledge base. At inference, the model retrieves similar judged experiences from this KB and uses them to modulate its prompt, enabling better, self-aligned reasoning without finetuning. On the PrideMM dataset, FLoReNce improves both predictive performance and explanation quality over static multimodal baselines, showing that feedback-regulated prompting is a viable path to adaptive meme humor understanding.

</details>


### [70] [Stochastic CHAOS: Why Deterministic Inference Kills, and Distributional Variability Is the Heartbeat of Artifical Cognition](https://arxiv.org/abs/2601.07239)
*Tanmay Joshi,Shourya Aggarwal,Anusa Saha,Aadi Pandey,Shreyash Dhoot,Vighnesh Rai,Raxit Goswami,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.AI

TL;DR: 该论文反对LLM确定性推理，认为其会扼杀不确定性建模能力、抑制涌现能力、弱化安全对齐，并主张采用随机CHAOS方法将分布变异性作为信号进行测量和控制。


<details>
  <summary>Details</summary>
Motivation: 传统软件追求确定性推理，但LLM本质上是条件概率分布而非固定函数。确定性推理会掩盖LLM的关键认知特性，包括不确定性建模、涌现能力、多路径推理和安全风险。

Method: 提出Stochastic CHAOS方法，将分布变异性视为需要测量和控制的信号。通过实证分析展示确定性推理的系统性误导性，包括单样本评估低估能力与脆弱性、贪婪解码消除涌现能力相变、多路径推理退化等问题。

Result: 确定性推理会系统性误导评估：低估模型能力和脆弱性、掩盖失败概率、消除涌现能力的相变现象、降低多路径推理的准确性和诊断洞察力，并隐藏罕见但危险的安全风险行为。

Conclusion: LLM不应追求确定性推理，而应接受其概率本质。分布变异性是LLM认知特性的核心，应作为信号进行测量和控制，而非消除。确定性推理会损害模型性能评估、安全对齐和实际部署效果。

Abstract: Deterministic inference is a comforting ideal in classical software: the same program on the same input should always produce the same output. As large language models move into real-world deployment, this ideal has been imported wholesale into inference stacks. Recent work from the Thinking Machines Lab has presented a detailed analysis of nondeterminism in LLM inference, showing how batch-invariant kernels and deterministic attention can enforce bitwise-identical outputs, positioning deterministic inference as a prerequisite for reproducibility and enterprise reliability.
  In this paper, we take the opposite stance. We argue that, for LLMs, deterministic inference kills. It kills the ability to model uncertainty, suppresses emergent abilities, collapses reasoning into a single brittle path, and weakens safety alignment by hiding tail risks. LLMs implement conditional distributions over outputs, not fixed functions. Collapsing these distributions to a single canonical completion may appear reassuring, but it systematically conceals properties central to artificial cognition. We instead advocate Stochastic CHAOS, treating distributional variability as a signal to be measured and controlled.
  Empirically, we show that deterministic inference is systematically misleading. Single-sample deterministic evaluation underestimates both capability and fragility, masking failure probability under paraphrases and noise. Phase-like transitions associated with emergent abilities disappear under greedy decoding. Multi-path reasoning degrades when forced onto deterministic backbones, reducing accuracy and diagnostic insight. Finally, deterministic evaluation underestimates safety risk by hiding rare but dangerous behaviors that appear only under multi-sample evaluation.

</details>


### [71] [Learning to Trust the Crowd: A Multi-Model Consensus Reasoning Engine for Large Language Models](https://arxiv.org/abs/2601.07245)
*Pranav Kallem*

Main category: cs.AI

TL;DR: 提出多模型共识推理引擎，通过监督元学习整合多个LLM输出，提升实例级可靠性


<details>
  <summary>Details</summary>
Motivation: LLM在平均性能上表现良好，但在实例层面不可靠，存在幻觉、脆弱失败和校准不良等问题，需要提升可靠性

Method: 开发多模型共识推理引擎，将多个LLM输出映射为结构化特征（语义嵌入、相似性、聚类统计、词汇结构线索、推理质量分数、置信度估计、模型先验），应用梯度提升树、列表排序和图神经网络

Result: 在GSM8K、ARC-Challenge、HellaSwag和TruthfulQA上，最佳图注意力共识模型比最强单LLM提升4.6个百分点，比多数投票提升8.1个百分点，降低Brier分数，减少TruthfulQA幻觉

Conclusion: 监督多模型共识是提升LLM可靠性的实用途径，语义一致性和聚类特征最重要，推理质量和模型先验特征提供补充增益

Abstract: Large language models (LLMs) achieve strong aver- age performance yet remain unreliable at the instance level, with frequent hallucinations, brittle failures, and poorly calibrated confidence. We study reliability through the lens of multi-model consensus: given responses from several heterogeneous LLMs, can we learn which answer is most likely correct for a given query? We introduce a Multi-Model Consensus Reasoning Engine that treats the set of LLM outputs as input to a supervised meta-learner. The system maps natural language responses into structured features using semantic embeddings, pairwise similarity and clustering statistics, lexical and structural cues, reasoning-quality scores, confidence estimates, and model-specific priors, and then applies gradient-boosted trees, listwise ranking, and graph neural networks over similarity graphs of answers. Using three open-weight LLMs evaluated on compact, resource- constrained subsets of GSM8K, ARC-Challenge, HellaSwag, and TruthfulQA, our best graph-attention-based consensus model improves macro-average accuracy by 4.6 percentage points over the strongest single LLM and by 8.1 points over majority vote, while also yielding lower Brier scores and fewer TruthfulQA hal- lucinations. Ablation and feature-importance analyses show that semantic agreement and clustering features are most influential, with reasoning-quality and model-prior features providing com- plementary gains, suggesting supervised multi-model consensus is a practical route toward more reliable LLM behavior, even in a modest single-machine setup.

</details>


### [72] [LRAS: Advanced Legal Reasoning with Agentic Search](https://arxiv.org/abs/2601.07296)
*Yujin Zhou,Chuxue Cao,Jinluan Yang,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: LRAS框架通过将法律大语言模型从静态参数化"闭环思维"转变为动态交互式"主动询问"，结合内省模仿学习和难度感知强化学习，显著提升法律推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律大语言模型依赖内部参数知识的"闭环推理"，缺乏对知识边界的自我认知，导致自信但错误的结论。法律领域对程序严谨性和法律逻辑有严格要求，需要解决这一问题。

Method: 提出LRAS框架，整合内省模仿学习和难度感知强化学习，使模型能够识别知识边界并处理法律推理复杂性，实现从"闭环思维"到"主动询问"的转变。

Result: LRAS在实证结果中优于最先进基线8.2-32%，在需要可靠知识的深度推理任务中提升最为显著。

Conclusion: LRAS框架成功解决了法律大语言模型的知识边界识别问题，显著提升了法律推理能力，为法律AI应用提供了新方向。

Abstract: While Large Reasoning Models (LRMs) have demonstrated exceptional logical capabilities in mathematical domains, their application to the legal field remains hindered by the strict requirements for procedural rigor and adherence to legal logic. Existing legal LLMs, which rely on "closed-loop reasoning" derived solely from internal parametric knowledge, frequently suffer from lack of self-awareness regarding their knowledge boundaries, leading to confident yet incorrect conclusions. To address this challenge, we present Legal Reasoning with Agentic Search (LRAS), the first framework designed to transition legal LLMs from static and parametric "closed-loop thinking" to dynamic and interactive "Active Inquiry". By integrating Introspective Imitation Learning and Difficulty-aware Reinforcement Learning, LRAS enables LRMs to identify knowledge boundaries and handle legal reasoning complexity. Empirical results demonstrate that LRAS outperforms state-of-the-art baselines by 8.2-32\%, with the most substantial gains observed in tasks requiring deep reasoning with reliable knowledge. We will release our data and models for further exploration soon.

</details>


### [73] [ARM: Role-Conditioned Neuron Transplantation for Training-Free Generalist LLM Agent Merging](https://arxiv.org/abs/2601.07309)
*Zhuoka Feng,Kang Chen,Sihan Zhao,Kai Xiong,Yaoning Wang,Minshen Yu,Junjie Nian,Changyi Xiao,Yixin Cao,Yugang Jiang*

Main category: cs.AI

TL;DR: ARM是一种基于激活引导、角色条件神经元移植的模型融合方法，专门针对LLM智能体，无需训练即可提升跨环境泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前交互式大语言模型智能体大多局限于单一环境，缺乏跨环境的鲁棒适应性。模型融合提供了一种无需训练的方法来整合多个专家模型，但现有方法主要针对静态自然语言任务，难以适应多轮智能体交互场景。

Method: 提出Agent-Role Merging (ARM)方法，包含三步框架：1) 构建融合骨干模型，2) 基于角色条件激活分析进行选择，3) 神经元移植进行细粒度优化。该方法通过激活引导和角色条件神经元移植，实现无需梯度优化的模型融合。

Result: ARM在多个领域超越了先前的模型融合方法和领域特定专家模型，同时展现出强大的跨域泛化能力。该方法在提升跨基准泛化能力的同时保持了高效性。

Conclusion: ARM成功将模型融合方法从静态自然语言任务扩展到多轮智能体场景，显著提升了智能体在不同交互环境中的泛化能力，为无需训练的智能体适应性提供了有效解决方案。

Abstract: Interactive large language model agents have advanced rapidly, but most remain specialized to a single environment and fail to adapt robustly to other environments. Model merging offers a training-free alternative by integrating multiple experts into a single model. In this paper, we propose Agent-Role Merging (ARM), an activation-guided, role-conditioned neuron transplantation method for model merging in LLM agents. ARM improves existing merging methods from static natural language tasks to multi-turn agent scenarios, and over the generalization ability across various interactive environments. This is achieved with a well designed 3-step framework: 1) constructing merged backbones, 2) selection based on its role-conditioned activation analysis, and 3) neuron transplantation for fine-grained refinements. Without gradient-based optimization, ARM improves cross-benchmark generalization while enjoying efficiency. Across diverse domains, the model obtained via ARM merging outperforms prior model merging methods and domain-specific expert models, while demonstrating strong out-of-domain generalization.

</details>


### [74] [Agentic Diagnostic Reasoning over Telecom and Datacenter Infrastructure](https://arxiv.org/abs/2601.07342)
*Nicolas Tacheny*

Main category: cs.AI

TL;DR: 提出基于LLM的智能诊断框架，通过MCP协议使用工具自主导航基础设施模型进行根因分析，替代传统硬编码的图遍历算法。


<details>
  <summary>Details</summary>
Motivation: 电信和数据中心基础设施采用多层服务资源模型，故障会在物理和逻辑组件间传播影响多个客户。传统根因分析方法依赖硬编码图遍历算法或基于规则的关联引擎，维护成本高且与基础设施模型紧耦合。

Method: 引入智能诊断框架，让LLM通过Model Context Protocol（MCP）暴露的受限工具空间进行逐步调查。代理自主导航基础设施模型，调用服务查找、依赖检索、结构化/非结构化数据、事件分析和影响发现等工具。定义调查协议来结构化代理推理，确保基础性、可重复性和对缺失/模糊信息的安全处理。

Result: 该工作为自主事件解决和变更影响缓解奠定了基础。未来系统不仅能诊断和修复基础设施故障，还能预测计划变更对服务和客户的影响，使运维人员能在执行维护操作前缓解风险。

Conclusion: 基于LLM的智能诊断框架通过MCP协议使用工具自主导航基础设施模型，为自主事件解决和变更影响预测提供了新方法，有望替代传统硬编码的根因分析方案。

Abstract: Large-scale telecom and datacenter infrastructures rely on multi-layered service and resource models, where failures propagate across physical and logical components and affect multiple customers. Traditional approaches to root cause analysis(RCA) rely on hard-coded graph traversal algorithms or rule-based correlation engines, which are costly to maintain and tightly coupled to the infrastructure model.
  In this work, we introduce an agentic diagnostic framework where a Large Language Model (LLM) performs step-wise investigation using a constrained tool space exposed through the Model Context Protocol (MCP). Instead of embedding causal logic or traversal algorithms into the application, the agent autonomously navigates the infrastructure model by invoking tools for service lookup, dependency retrieval, structured and unstructured data, and event analysis, and impact discovery. We define an investigation protocol that structures the agent's reasoning and ensures grounding, reproducibility, and safe handling of missing or ambiguous information.
  This work lays the foundation for autonomous incident resolution and change impact mitigation. Future systems will not only diagnose and remediate infrastructure failures, but also predict the impact of planned changes on services and customers, enabling operators to mitigate risks before executing maintenance operations.

</details>


### [75] [OpenTinker: Separating Concerns in Agentic Reinforcement Learning](https://arxiv.org/abs/2601.07376)
*Siqi Zhu,Jiaxuan You*

Main category: cs.AI

TL;DR: OpenTinker是一个用于大型语言模型（LLM）智能体强化学习的基础设施，采用算法设计、执行和智能体-环境交互分离的架构，通过轻量级可组合组件和集中式调度器支持多种训练模式。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体强化学习系统通常采用端到端的单体式架构，缺乏模块化和可扩展性。OpenTinker旨在通过分离关注点，提供更灵活、可组合的智能体学习基础设施，支持多种训练模式并简化多智能体训练扩展。

Method: OpenTinker将智能体学习系统分解为轻量级可组合组件，具有明确的抽象边界。用户定义智能体、环境和交互协议，而推理和训练则委托给托管执行运行时。系统引入集中式调度器管理训练和推理工作负载，支持LoRA-based和全参数RL、监督微调和推理，并讨论扩展到多智能体训练的设计原则。

Result: 论文展示了一组RL用例，证明了该框架在实际智能体学习场景中的有效性，表明OpenTinker能够支持多种训练模式并在共享资源上高效管理训练和推理工作负载。

Conclusion: OpenTinker提供了一个模块化、可扩展的LLM智能体强化学习基础设施，通过分离关注点和组件化设计，简化了智能体学习系统的开发和部署，支持从单智能体到多智能体的灵活扩展。

Abstract: We introduce OpenTinker, an infrastructure for reinforcement learning (RL) of large language model (LLM) agents built around a separation of concerns across algorithm design, execution, and agent-environment interaction. Rather than relying on monolithic, end-to-end RL pipelines, OpenTinker decomposes agentic learning systems into lightweight, composable components with clearly defined abstraction boundaries. Users specify agents, environments, and interaction protocols, while inference and training are delegated to a managed execution runtime. OpenTinker introduces a centralized scheduler for managing training and inference workloads, including LoRA-based and full-parameter RL, supervised fine-tuning, and inference, over shared resources. We further discuss design principles for extending OpenTinker to multi-agent training. Finally, we present a set of RL use cases that demonstrate the effectiveness of the framework in practical agentic learning scenarios.

</details>


### [76] [Beyond Dialogue Time: Temporal Semantic Memory for Personalized LLM Agents](https://arxiv.org/abs/2601.07468)
*Miao Su,Yucan Guo,Zhongni Hou,Long Bai,Zixuan Li,Yufei Zhang,Guojun Yin,Wei Lin,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: TSM是一个为LLM代理设计的记忆框架，通过建模语义时间线来组织点状记忆，并构建持续性记忆来解决现有方法的时间不准确和碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理记忆方法存在两个时间维度问题：1) 时间不准确：记忆按对话时间而非实际发生时间组织；2) 时间碎片化：只关注点状记忆，丢失了捕捉持续状态和演化模式的持续性信息。

Method: 提出时间语义记忆(TSM)框架：1) 在记忆构建阶段，建立语义时间线而非对话时间线；2) 将时间连续且语义相关的信息整合成持续性记忆；3) 在记忆利用阶段，结合查询的时间意图，检索时间恰当的持续性记忆，为响应生成提供时间有效、持续时间一致的上下文。

Result: 在LongMemEval和LoCoMo数据集上的实验表明，TSM始终优于现有方法，准确率最高提升12.2%，证明了该方法的有效性。

Conclusion: TSM通过建模语义时间和构建持续性记忆，有效解决了LLM代理记忆中的时间维度问题，显著提升了记忆的准确性和实用性。

Abstract: Memory enables Large Language Model (LLM) agents to perceive, store, and use information from past dialogues, which is essential for personalization. However, existing methods fail to properly model the temporal dimension of memory in two aspects: 1) Temporal inaccuracy: memories are organized by dialogue time rather than their actual occurrence time; 2) Temporal fragmentation: existing methods focus on point-wise memory, losing durative information that captures persistent states and evolving patterns. To address these limitations, we propose Temporal Semantic Memory (TSM), a memory framework that models semantic time for point-wise memory and supports the construction and utilization of durative memory. During memory construction, it first builds a semantic timeline rather than a dialogue one. Then, it consolidates temporally continuous and semantically related information into a durative memory. During memory utilization, it incorporates the query's temporal intent on the semantic timeline, enabling the retrieval of temporally appropriate durative memories and providing time-valid, duration-consistent context to support response generation. Experiments on LongMemEval and LoCoMo show that TSM consistently outperforms existing methods and achieves up to 12.2% absolute improvement in accuracy, demonstrating the effectiveness of the proposed method.

</details>


### [77] [Learning How to Remember: A Meta-Cognitive Management Method for Structured and Transferable Agent Memory](https://arxiv.org/abs/2601.07470)
*Sirui Liang,Pengfei Cao,Jian Zhao,Wenhao Teng,Xiangwen Liao,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: MCMA提出了一种可学习的元认知记忆抽象方法，通过解耦任务执行和记忆管理，使用记忆副驾驶动态决定记忆的结构、抽象和重用方式，显著提升了LLM智能体在长期决策任务中的泛化能力和跨任务迁移性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体方法通常将记忆存储为固定表示，并在单一或隐式抽象级别上重用记忆，这限制了泛化能力，在分布偏移时容易导致负迁移。需要一种更灵活、可学习的记忆抽象机制。

Method: MCMA将记忆抽象视为可学习的认知技能，通过冻结的任务模型和学习的记忆副驾驶解耦任务执行与记忆管理。记忆副驾驶使用直接偏好优化训练，决定记忆的结构、抽象和重用方式。记忆组织成抽象层次结构，基于任务相似性选择性重用。当没有可转移记忆时，通过转移记忆副驾驶来转移抽象和管理记忆的能力。

Result: 在ALFWorld、ScienceWorld和BabyAI三个基准测试中，MCMA相比多个基线方法在性能、分布外泛化和跨任务迁移方面都取得了显著改进。

Conclusion: 将记忆抽象作为可学习的认知技能而非固定设计选择，能够显著提升LLM智能体在长期决策任务中的泛化能力和跨任务迁移性能。

Abstract: Large language model (LLM) agents increasingly rely on accumulated memory to solve long-horizon decision-making tasks. However, most existing approaches store memory in fixed representations and reuse it at a single or implicit level of abstraction, which limits generalization and often leads to negative transfer when distribution shift. This paper proposes the Meta-Cognitive Memory Abstraction method (MCMA), which treats memory abstraction as a learnable cognitive skill rather than a fixed design choice. MCMA decouples task execution from memory management by combining a frozen task model with a learned memory copilot. The memory copilot is trained using direct preference optimization, it determines how memories should be structured, abstracted, and reused. Memories are further organized into a hierarchy of abstraction levels, enabling selective reuse based on task similarity. When no memory is transferable, MCMA transfers the ability to abstract and manage memory by transferring the memory copilot. Experiments on ALFWorld, ScienceWorld, and BabyAI demonstrate substantial improvements in performance, out-of-distribution generalization, and cross-task transfer over several baselines.

</details>


### [78] [JudgeFlow: Agentic Workflow Optimization via Block Judge](https://arxiv.org/abs/2601.07477)
*Zihan Ma,Zhikai Zhao,Chuanbo Hua,Federico Berto,Jinkyoo Park*

Main category: cs.AI

TL;DR: 提出JudgeFlow框架，通过模块化逻辑块、责任评分和针对性优化来提升基于LLM的智能体工作流效率


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体工作流优化方法依赖粗粒度的端到端评估信号，缺乏细粒度诊断，导致修改效率低下或影响有限

Method: 提出Evaluation-Judge-Optimization-Update管道：1) 将可复用、可配置的逻辑块集成到工作流中；2) 设计Judge模块检查执行轨迹（特别是失败运行），为问题块分配基于排名的责任评分；3) 基于LLM的优化器针对最成问题的块进行修改

Result: 在数学推理和代码生成基准测试中，JudgeFlow相比现有方法实现了更优的性能和效率

Conclusion: 该方法提高了样本效率，通过块级诊断增强了可解释性，并为自动化日益复杂的智能体工作流提供了可扩展的基础

Abstract: Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse, end-to-end evaluation signals and lack fine-grained signals on where to refine, often resulting in inefficient or low-impact modifications. To address these limitations, we propose {\our{}}, an Evaluation-Judge-Optimization-Update pipeline. We incorporate reusable, configurable logic blocks into agentic workflows to capture fundamental forms of logic. On top of this abstraction, we design a dedicated Judge module that inspects execution traces -- particularly failed runs -- and assigns rank-based responsibility scores to problematic blocks. These fine-grained diagnostic signals are then leveraged by an LLM-based optimizer, which focuses modifications on the most problematic block in the workflow. Our approach improves sample efficiency, enhances interpretability through block-level diagnostics, and provides a scalable foundation for automating increasingly complex agentic workflows. We evaluate {\our{}} on mathematical reasoning and code generation benchmarks, where {\our{}} achieves superior performance and efficiency compared to existing methods. The source code is publicly available at https://github.com/ma-zihan/JudgeFlow.

</details>


### [79] [Beyond Entangled Planning: Task-Decoupled Planning for Long-Horizon Agents](https://arxiv.org/abs/2601.07577)
*Yunfan Li,Bingbing Xu,Xueyun Tian,Xiucheng Xu,Huawei Shen*

Main category: cs.AI

TL;DR: TDP通过任务解耦框架，将复杂任务分解为子目标DAG，使用监督者、规划者和执行者分离上下文，防止错误传播并提高长时程任务执行的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理规划方法存在两个问题：逐步规划短视，一次性规划脆弱，且都面临上下文纠缠问题。上下文纠缠导致认知负担增加，局部错误会传播到其他独立决策中，恢复成本高昂。

Method: 提出任务解耦规划(TDP)框架：1) 监督者将任务分解为有向无环图(DAG)的子目标；2) 规划者和执行者使用限定上下文，将推理和重新规划限制在活动子任务内；3) 这种隔离防止错误传播，允许局部纠正偏差而不中断工作流。

Result: 在TravelPlanner、ScienceWorld和HotpotQA上的实验表明，TDP优于强基线方法，同时将token消耗减少高达82%，证明子任务解耦提高了长时程代理的鲁棒性和效率。

Conclusion: 任务解耦规划通过分离上下文和限制推理范围，有效解决了现有规划方法的局限性，显著提高了长时程任务执行的可靠性和计算效率。

Abstract: Recent advances in large language models (LLMs) have enabled agents to autonomously execute complex, long-horizon tasks, yet planning remains a primary bottleneck for reliable task execution. Existing methods typically fall into two paradigms: step-wise planning, which is reactive but often short-sighted; and one-shot planning, which generates a complete plan upfront yet is brittle to execution errors. Crucially, both paradigms suffer from entangled contexts, where the agent must reason over a monolithic history spanning multiple sub-tasks. This entanglement increases cognitive load and lets local errors propagate across otherwise independent decisions, making recovery computationally expensive. To address this, we propose Task-Decoupled Planning (TDP), a training-free framework that replaces entangled reasoning with task decoupling. TDP decomposes tasks into a directed acyclic graph (DAG) of sub-goals via a Supervisor. Using a Planner and Executor with scoped contexts, TDP confines reasoning and replanning to the active sub-task. This isolation prevents error propagation and corrects deviations locally without disrupting the workflow. Results on TravelPlanner, ScienceWorld, and HotpotQA show that TDP outperforms strong baselines while reducing token consumption by up to 82%, demonstrating that sub-task decoupling improves both robustness and efficiency for long-horizon agents.

</details>


### [80] [Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning](https://arxiv.org/abs/2601.07641)
*Jiaxuan Lu,Ziyu Kong,Yemin Wang,Rong Fu,Haiyuan Wan,Cheng Yang,Wenjie Lou,Haoran Sun,Lilong Wang,Yankai Jiang,Xiaosong Wang,Xiao Sun,Dongzhan Zhou*

Main category: cs.AI

TL;DR: TTE是一种新的AI科学范式，让智能体在推理时动态合成、验证和演化可执行工具，克服静态工具库的局限性，在科学推理任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体依赖静态预定义工具库，在科学领域中存在根本性缺陷，因为科学工具稀疏、异构且本质上不完整，无法适应开放式的科学世界。

Method: 提出测试时工具演化（TTE）范式，使智能体能够在推理过程中合成、验证和演化可执行工具，将工具从固定资源转变为问题驱动的产物。

Result: TTE在准确性和工具效率方面实现了最先进的性能，同时支持计算工具的有效跨领域适应。实验基于SciEvo基准（1,590个科学推理任务和925个自动演化工具）。

Conclusion: TTE通过将工具从静态库转变为动态演化产物，克服了科学AI中的核心挑战，为开放科学世界中的计算方法创建提供了新范式。

Abstract: The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at https://github.com/lujiaxuan0520/Test-Time-Tool-Evol.

</details>


### [81] [Active Evaluation of General Agents: Problem Definition and Comparison of Baseline Algorithms](https://arxiv.org/abs/2601.07651)
*Marc Lanctot,Kate Larson,Ian Gemp,Michael Kaisers*

Main category: cs.AI

TL;DR: 提出主动评估智能代理的多任务框架，通过在线迭代选择任务和代理进行采样，比较不同排名算法在减少排名误差方面的效率，发现Elo评分系统在实践中表现稳定，而Soft Condorcet Optimization在真实Atari代理评估中表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着智能代理能力日益通用化，能够处理多种任务，其评估的复杂性和成本显著增加。特定能力评估任务可能相关且具有随机性，需要大量样本进行准确比较，导致成本上升。需要开发高效的评估方法来减少评估数据样本的需求。

Method: 提出主动评估智能代理的多任务概念框架，采用在线迭代方式：每轮迭代中，排名算法选择要采样的任务和代理，然后评估算法报告代理排名，并随时间评估其相对于真实排名的性能。在合成生成数据和真实Atari游戏代理评估数据上比较多种基线方法。

Result: 经典的Elo评分系统在实践中是减少排名误差的可靠选择，尽管理论上存在已知缺陷。新提出的Soft Condorcet Optimization方法在合成数据上与Elo表现相当，在真实Atari代理评估中显著优于Elo。当任务与真实排名的变异较大时，基于比例表示的任务选择能更高效地减少排名误差。

Conclusion: 主动评估框架能有效减少多任务智能代理评估所需的样本量，Elo评分系统在实践中表现稳健，而Soft Condorcet Optimization在真实场景中表现更优。任务选择策略应根据任务变异程度进行调整。

Abstract: As intelligent agents become more generally-capable, i.e. able to master a wide variety of tasks, the complexity and cost of properly evaluating them rises significantly. Tasks that assess specific capabilities of the agents can be correlated and stochastic, requiring many samples for accurate comparisons, leading to added costs. In this paper, we propose a formal definition and a conceptual framework for active evaluation of agents across multiple tasks, which assesses the performance of ranking algorithms as a function of number of evaluation data samples. Rather than curating, filtering, or compressing existing data sets as a preprocessing step, we propose an online framing: on every iteration, the ranking algorithm chooses the task and agents to sample scores from. Then, evaluation algorithms report a ranking of agents on each iteration and their performance is assessed with respect to the ground truth ranking over time. Several baselines are compared under different experimental contexts, with synthetic generated data and simulated online access to real evaluation data from Atari game-playing agents. We find that the classical Elo rating system -- while it suffers from well-known failure modes, in theory -- is a consistently reliable choice for efficient reduction of ranking error in practice. A recently-proposed method, Soft Condorcet Optimization, shows comparable performance to Elo on synthetic data and significantly outperforms Elo on real Atari agent evaluation. When task variation from the ground truth is high, selecting tasks based on proportional representation leads to higher rate of ranking error reduction.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [82] [The Impact of Post-training on Data Contamination](https://arxiv.org/abs/2601.06103)
*Muhammed Yusuf Kocyigit,Caglar Yildirim*

Main category: cs.LG

TL;DR: 研究数据集污染与后训练阶段的交互作用，发现污染会导致性能虚高但随预训练逐渐减弱，SFT和GRPO会重新暴露泄露信息但影响范围不同，模型规模会放大这些趋势。


<details>
  <summary>Details</summary>
Motivation: 研究数据集污染如何与当前大语言模型训练流程中的后训练阶段相互作用，了解污染对模型性能评估的真实影响。

Method: 从干净的Qwen2.5和Gemma3检查点开始，在25B token扩展预训练数据的前2B token中注入5份GSM8K和MBPP测试项，比较污染和干净模型在预训练后及SFT和GRPO后训练后的表现。

Result: 发现三个一致模式：1)污染导致性能虚高但随预训练逐渐减弱；2)SFT和GRPO都会重新暴露泄露信息，但SFT仅影响污染任务，GRPO还会影响未污染任务；3)模型规模放大趋势，大SFT模型记忆更多，大GRPO模型将泄露转化为更通用能力。

Conclusion: 需要在后训练后进行污染审计，基于RL的后训练虽不免疫但有助于缓解污染相关的性能高估问题。

Abstract: We present a controlled study of how dataset contamination interacts with the post-training stages now standard in large language model training pipelines. Starting from clean checkpoints of Qwen2.5 (0.5B/1.5B) and Gemma3 (1B/4B), we inject five copies of GSM8K and MBPP test items into the first 2B tokens of an otherwise 25B token extended pre-training dataset. We then compare the contaminated and clean models both immediately after pre-training and again after two popular post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL) with group relative policy optimization (GRPO). The applied post-training steps do not have any contamination. Across math and coding benchmarks, we find three consistent patterns: (i) Contamination causes performance spikes that are gradually diminished with continued pre-training. After even 25B tokens the apparent performance inflation of contamination can become close to zero. (ii) Both SFT and GRPO resurface the leaked information, but with different external validity: SFT inflates scores only on the contaminated tasks, whereas GRPO also inflates performance on uncontaminated counterparts (GSMPlus, HumanEval). (iii) Model scale amplifies these tendencies, larger Supervised Fine Tuned models memorize more, while larger GRPO models translate leakage into more generalizable capabilities. Our results underscore the need for contamination audits \emph{after} post-training and suggest that RL-based post-training, although not immune, can help alleviate contamination-related over-estimation problems.

</details>


### [83] [A Review of Online Diffusion Policy RL Algorithms for Scalable Robotic Control](https://arxiv.org/abs/2601.06133)
*Wonhyeok Choi,Minwoo Choi,Jungwan Woo,Kyumin Hwang,Jaeyeul Kim,Sunghoon Im*

Main category: cs.LG

TL;DR: 本文首次全面综述并实证分析了在线扩散策略强化学习算法，提出了基于策略改进机制的四类方法分类法，在统一机器人基准上系统评估了算法性能，揭示了关键权衡和瓶颈，并为实际部署提供了指导。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人控制中表现出优越的表达能力，但与传统强化学习的在线学习机制存在根本性不兼容。目前缺乏对在线扩散策略强化学习算法的系统性分析和评估，阻碍了该领域向更通用、可扩展的机器人学习系统发展。

Method: 提出新颖的分类法，将现有方法分为四类：动作梯度法、Q加权法、邻近法、和反向传播时间法。在统一的NVIDIA Isaac Lab基准上（包含12个多样化机器人任务），对代表性算法进行系统性实验评估，涵盖任务多样性、并行化能力、扩散步长可扩展性、跨具身泛化能力和环境鲁棒性五个维度。

Result: 分析揭示了各类算法家族在样本效率和可扩展性方面的基本权衡，识别了限制在线扩散策略强化学习实际部署的关键计算和算法瓶颈。为特定操作约束下的算法选择提供了具体指导。

Conclusion: 本文为在线扩散策略强化学习领域提供了首个系统性分析框架，揭示了当前方法的局限性和未来发展方向，为推动更通用、可扩展的机器人学习系统奠定了基础。

Abstract: Diffusion policies have emerged as a powerful approach for robotic control, demonstrating superior expressiveness in modeling multimodal action distributions compared to conventional policy networks. However, their integration with online reinforcement learning remains challenging due to fundamental incompatibilities between diffusion model training objectives and standard RL policy improvement mechanisms. This paper presents the first comprehensive review and empirical analysis of current Online Diffusion Policy Reinforcement Learning (Online DPRL) algorithms for scalable robotic control systems. We propose a novel taxonomy that categorizes existing approaches into four distinct families -- Action-Gradient, Q-Weighting, Proximity-Based, and Backpropagation Through Time (BPTT) methods -- based on their policy improvement mechanisms. Through extensive experiments on a unified NVIDIA Isaac Lab benchmark encompassing 12 diverse robotic tasks, we systematically evaluate representative algorithms across five critical dimensions: task diversity, parallelization capability, diffusion step scalability, cross-embodiment generalization, and environmental robustness. Our analysis identifies key findings regarding the fundamental trade-offs inherent in each algorithmic family, particularly concerning sample efficiency and scalability. Furthermore, we reveal critical computational and algorithmic bottlenecks that currently limit the practical deployment of online DPRL. Based on these findings, we provide concrete guidelines for algorithm selection tailored to specific operational constraints and outline promising future research directions to advance the field toward more general and scalable robotic learning systems.

</details>


### [84] [Evaluating Robustness of Large Language Models in Enterprise Applications: Benchmarks for Perturbation Consistency Across Formats and Languages](https://arxiv.org/abs/2601.06341)
*Tara Bogavelli,Oluwanifemi Bamgbose,Gabrielle Gauthier Melançon,Fanny Riols,Roshnee Sharma*

Main category: cs.LG

TL;DR: 该研究提出了一个全面的基准测试套件，用于评估企业级LLM应用在各种扰动下的鲁棒性，发现微小扰动可使关键指标性能下降高达40%，且模型大小与鲁棒性的关系比传统假设更为复杂。


<details>
  <summary>Details</summary>
Motivation: 企业级LLM应用需要在多样化场景中保持高质量和可靠性能，对微小变化具有鲁棒性。现有研究主要关注有限的扰动类型和小型学术数据集，限制了其在真实世界应用中的相关性。

Method: 提出了一个全面的基准测试套件，评估多种扰动类型下的鲁棒性，包括：通用文本编辑（标点、空格）、格式变化（JSON、YAML）、多语言和跨语言输入、指令位置变化。评估了11个参数从4B到120B+的模型。

Result: 微小扰动可使关键企业指标性能下降高达40个百分点。模型大小与鲁棒性的关系比传统假设更为复杂：一个8B参数模型（Ministral 3 8B）优于大多数更大模型，而另一个8B模型（Llama 3.1 8B）表现最差。

Conclusion: 企业级LLM应用需要更全面的鲁棒性评估，模型大小不是鲁棒性的唯一决定因素，特定架构和训练方法可能对鲁棒性有重要影响。

Abstract: Enterprise LLM applications require consistently high quality and reliable performance across diverse scenarios, demanding robustness to minor variations. Existing research shows that even small prompt changes can lead to substantial differences in output, but has mainly focused on a narrow set of perturbations with small academic datasets, limiting their relevance to real-world applications. To address this, we present a comprehensive benchmark suite that evaluates robustness across multiple perturbation types, including general text edits (e.g., punctuation, whitespace), formatting changes (e.g., JSON, YAML), multilingual and cross-lingual inputs, and positional variations in instructions. Evaluating 11 models ranging from 4B to 120B+ parameters, we find that minor perturbations reduce performance by up to 40 percentage points on key enterprise metrics. Critically, we demonstrate that the relationship between model size and robustness is more nuanced than conventional assumptions suggest: an 8B parameter model (Ministral 3 8B) outperforms most larger models, while another 8B model (Llama 3.1 8B) performs worst overall.

</details>


### [85] [FROAV: A Framework for RAG Observation and Agent Verification - Lowering the Barrier to LLM Agent Research](https://arxiv.org/abs/2601.07504)
*Tzu-Hsuan Lin,Chih-Hsuan Kao*

Main category: cs.LG

TL;DR: FROAV是一个开源研究平台，通过可视化工作流编排、综合评估框架和可扩展Python集成，降低LLM智能体研究的门槛，使研究人员无需编写基础设施代码即可快速原型设计和评估RAG策略。


<details>
  <summary>Details</summary>
Motivation: LLM智能体系统在文档分析、决策支持和知识检索方面有巨大潜力，但其开发、评估和迭代的复杂性给缺乏软件工程专业知识的研究人员带来了显著障碍。

Method: FROAV采用多阶段RAG管道与"LLM-as-a-Judge"评估系统，通过n8n进行无代码工作流设计，PostgreSQL进行细粒度数据管理，FastAPI提供灵活后端逻辑，Streamlit支持人机交互。

Result: 该框架在金融文档分析中展示了实用性，其材料无关架构可适应任何需要语义分析的领域，使研究人员能够专注于假设测试和算法创新而非系统集成挑战。

Conclusion: FROAV代表了向更广泛科学社区开放LLM智能体研究的重要一步，通过即插即用架构民主化LLM智能体研究。

Abstract: The rapid advancement of Large Language Models (LLMs) and their integration into autonomous agent systems has created unprecedented opportunities for document analysis, decision support, and knowledge retrieval. However, the complexity of developing, evaluating, and iterating on LLM-based agent workflows presents significant barriers to researchers, particularly those without extensive software engineering expertise. We present FROAV (Framework for RAG Observation and Agent Verification), an open-source research platform that democratizes LLM agent research by providing a plug-and-play architecture combining visual workflow orchestration, a comprehensive evaluation framework, and extensible Python integration. FROAV implements a multi-stage Retrieval-Augmented Generation (RAG) pipeline coupled with a rigorous "LLM-as-a-Judge" evaluation system, all accessible through intuitive graphical interfaces. Our framework integrates n8n for no-code workflow design, PostgreSQL for granular data management, FastAPI for flexible backend logic, and Streamlit for human-in-the-loop interaction. Through this integrated ecosystem, researchers can rapidly prototype RAG strategies, conduct prompt engineering experiments, validate agent performance against human judgments, and collect structured feedback-all without writing infrastructure code. We demonstrate the framework's utility through its application to financial document analysis, while emphasizing its material-agnostic architecture that adapts to any domain requiring semantic analysis. FROAV represents a significant step toward making LLM agent research accessible to a broader scientific community, enabling researchers to focus on hypothesis testing and algorithmic innovation rather than system integration challenges.

</details>


### [86] [MixDPO: Modeling Preference Strength for Pluralistic Alignment](https://arxiv.org/abs/2601.06180)
*Saki Imai,Pedram Heydari,Anthony Sicilia,Asteria Kaeberlein,Katherine Atwell,Malihe Alikhani*

Main category: cs.LG

TL;DR: MixDPO是一种改进的偏好对齐方法，通过建模偏好强度的异质性来提升语言模型的对齐性能，相比传统方法在多个数据集上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法假设所有人类偏好表达强度相同，但实际中偏好强度存在个体和情境差异，这种不匹配限制了现有方法准确捕捉异质性人类判断的能力。

Method: 提出Mixed Logit Direct Preference Optimization (MixDPO)，这是Direct Preference Optimization的泛化版本，通过建模偏好强度的变化来捕捉训练样本中偏好表达的异质性。

Result: 在三个偏好数据集和两个开源语言模型上评估，MixDPO在Pythia-2.8B上提升了11.2个点的聚合对齐性能，同时保持了子组级别的偏好，在推断偏好异质性更高的设置中改进最大。

Conclusion: MixDPO通过学习的强度分布使偏好异质性显式化，能够更准确地捕捉人类偏好的异质性表达，提升语言模型的对齐性能。

Abstract: Preference based alignment objectives implicitly assume that all human preferences are expressed with equal strength. In practice, however, preference strength varies across individuals and contexts -- a phenomenon established in behavioral economics and discrete choice theory. This mismatch limits the ability of existing objectives to faithfully capture heterogeneous human judgments. Inspired by this literature, we introduce Mixed Logit Direct Preference Optimization (MixDPO), a generalization of Direct Preference Optimization that models variation in preference strength. MixDPO enables alignment objectives to capture heterogeneity in how strongly preferences are expressed across training examples. We evaluate MixDPO on three preference datasets using two open-weight language models. Across datasets, MixDPO improves aggregate alignment performance (+11.2 points on Pythia-2.8B) while preserving subgroup level preferences, with the largest gains appearing in settings with higher inferred preference heterogeneity. MixDPO makes preference heterogeneity explicit through learned strength distributions. We release our code for reproducibility.

</details>


### [87] [Teach Diffusion Language Models to Learn from Their Own Mistakes](https://arxiv.org/abs/2601.06428)
*Liming Liu,Binxuan Huang,Xin Liu,Bing Yin,Tuo Zhao*

Main category: cs.LG

TL;DR: 提出解耦自校正(DSC)框架，通过两阶段训练解决掩码扩散语言模型并行采样中的依赖错误问题，结合未来上下文增强(FCA)技术，在数学推理和代码生成任务上显著提升多token生成质量


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型通过并行采样实现加速，但较少的推理步骤会引入强依赖错误，导致生成质量随步长增加而快速下降，因此需要可靠的自校正机制来维持高质量的多token生成

Method: 提出解耦自校正(DSC)两阶段方法：1) 完全优化DLM的生成能力后冻结模型；2) 训练专门的校正头。引入未来上下文增强(FCA)技术，通过用真实token增强样本来泛化错误训练分布，使校正头能利用更丰富的未来上下文检测细微错误

Result: 在数学推理和代码生成基准测试中，该方法显著减少了较大生成步长带来的质量下降，使DLM能够同时实现高生成速度和强输出保真度

Conclusion: DSC框架通过解耦生成和校正训练，结合FCA技术，有效解决了掩码扩散语言模型并行采样中的错误累积问题，实现了高速高质量的多token生成

Abstract: Masked Diffusion Language Models (DLMs) achieve significant speed by generating multiple tokens in parallel. However, this parallel sampling approach, especially when using fewer inference steps, will introduce strong dependency errors and cause quality to deteriorate rapidly as the generation step size grows. As a result, reliable self-correction becomes essential for maintaining high-quality multi-token generation. To address this, we propose Decoupled Self-Correction (DSC), a novel two-stage methodology. DSC first fully optimizes the DLM's generative ability before freezing the model and training a specialized correction head. This decoupling preserves the model's peak SFT performance and ensures the generated errors used for correction head training are of higher quality. Additionally, we introduce Future-Context Augmentation (FCA) to maximize the correction head's accuracy. FCA generalizes the error training distribution by augmenting samples with ground-truth tokens, effectively training the head to utilize a richer, future-looking context. This mechanism is used for reliably detecting the subtle errors of the high-fidelity base model. Our DSC framework enables the model, at inference time, to jointly generate and revise tokens, thereby correcting errors introduced by multi-token generation and mitigating error accumulation across steps. Experiments on mathematical reasoning and code generation benchmarks demonstrate that our approach substantially reduces the quality degradation associated with larger generation steps, allowing DLMs to achieve both high generation speed and strong output fidelity.

</details>


### [88] [ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking](https://arxiv.org/abs/2601.06487)
*Qiang Zhang,Boli Chen,Fanrui Zhang,Ruixue Ding,Shihang Wang,Qiuchen Wang,Yinfeng Huang,Haonan Zhang,Rongxiang Zhu,Pengyong Wang,Ailin Ren,Xin Li,Pengjun Xie,Jiawei Liu,Ning Guo,Jingren Zhou,Zheng-Jun Zha*

Main category: cs.LG

TL;DR: ArenaRL提出从点式标量评分转向组内相对排名的强化学习范式，通过过程感知的成对评估和多级评分标准解决开放任务中的奖励信号问题


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在开放任务（如复杂旅行规划）中表现不佳，因为缺乏客观真实标签，依赖的奖励模型存在"歧视崩溃"问题——难以区分不同轨迹间的细微优势，导致奖励信号被噪声主导

Method: 提出ArenaRL范式：1）过程感知成对评估机制，使用多级评分标准为轨迹分配细粒度相对分数；2）构建组内对抗竞技场，设计基于锦标赛的排名方案获得稳定优势信号；3）建立单淘汰种子方案，以O(N)复杂度实现接近O(N²)全对比的精度

Result: ArenaRL显著优于标准RL基线，使LLM代理能为复杂现实任务生成更稳健的解决方案；同时构建了Open-Travel和Open-DeepResearch两个高质量基准，涵盖完整训练评估流程

Conclusion: 从点式评分转向相对排名能有效解决开放任务中的奖励信号问题，ArenaRL在效率和精度间取得良好平衡，为开放任务代理提供了有效的强化学习框架

Abstract: Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.

</details>


### [89] [CEDAR: Context Engineering for Agentic Data Science](https://arxiv.org/abs/2601.06606)
*Rishiraj Saha Roy,Chris Hinze,Luzian Hahn,Fabian Kuech*

Main category: cs.LG

TL;DR: CEDAR是一个用于自动化数据科学任务的智能体系统，通过上下文工程解决LLM在数据科学中的挑战，使用结构化提示和分离的规划/代码生成智能体，在本地处理数据并保持上下文可读性。


<details>
  <summary>Details</summary>
Motivation: 使用LLM解决数据科学问题是一个未被充分探索但具有巨大市场价值的领域。面临的挑战包括任务复杂性、数据规模、计算限制和上下文限制。

Method: 采用上下文工程方法：1) 在初始提示中引入数据科学特定的结构化输入字段；2) 使用分离的LLM智能体生成交错的规划块和代码块序列；3) 通过函数调用生成中间文本和Python代码，确保数据本地处理；4) 引入迭代代码生成和智能历史渲染实现容错和上下文管理。

Result: 通过经典的Kaggle挑战验证了该智能体数据科学家的可行性。

Conclusion: 通过有效的上下文工程可以缓解数据科学任务中的各种挑战，智能体系统能够自动化解决复杂的数据科学问题。

Abstract: We demonstrate CEDAR, an application for automating data science (DS) tasks with an agentic setup. Solving DS problems with LLMs is an underexplored area that has immense market value. The challenges are manifold: task complexities, data sizes, computational limitations, and context restrictions. We show that these can be alleviated via effective context engineering. We first impose structure into the initial prompt with DS-specific input fields, that serve as instructions for the agentic system. The solution is then materialized as an enumerated sequence of interleaved plan and code blocks generated by separate LLM agents, providing a readable structure to the context at any step of the workflow. Function calls for generating these intermediate texts, and for corresponding Python code, ensure that data stays local, and only aggregate statistics and associated instructions are injected into LLM prompts. Fault tolerance and context management are introduced via iterative code generation and smart history rendering. The viability of our agentic data scientist is demonstrated using canonical Kaggle challenges.

</details>


### [90] [KASER: Knowledge-Aligned Student Error Simulator for Open-Ended Coding Tasks](https://arxiv.org/abs/2601.06633)
*Zhangqi Duan,Nigel Fernandez,Andrew Lan*

Main category: cs.LG

TL;DR: KASER是一个基于强化学习的知识对齐学生错误模拟器，用于预测学生在开放式编程任务中的错误，通过混合奖励机制提升代码相似度、错误匹配和预测多样性。


<details>
  <summary>Details</summary>
Motivation: 开放式编程任务能深入反映学生知识，但现有大语言模型在模拟学生错误时存在模式崩溃问题，无法充分捕捉学生回答在语法、风格和解决方案上的多样性。

Method: 提出KASER方法，采用基于强化学习的训练方法，使用混合奖励机制，包含三个维度：代码与真实答案的相似度、错误匹配度、代码预测多样性。

Result: 在两个真实数据集上进行两级评估：在每学生-问题对级别，方法在代码和错误预测上优于基线；在每问题级别，在错误覆盖率和模拟代码多样性上优于基线。

Conclusion: KASER通过知识对齐和强化学习混合奖励机制，有效提升了学生错误模拟的准确性和多样性，解决了LLM在模拟学生错误时的模式崩溃问题。

Abstract: Open-ended tasks, such as coding problems that are common in computer science education, provide detailed insights into student knowledge. However, training large language models (LLMs) to simulate and predict possible student errors in their responses to these problems can be challenging: they often suffer from mode collapse and fail to fully capture the diversity in syntax, style, and solution approach in student responses. In this work, we present KASER (Knowledge-Aligned Student Error Simulator), a novel approach that aligns errors with student knowledge. We propose a training method based on reinforcement learning using a hybrid reward that reflects three aspects of student code prediction: i) code similarity to the ground-truth, ii) error matching, and iii) code prediction diversity. On two real-world datasets, we perform two levels of evaluation and show that: At the per-student-problem pair level, our method outperforms baselines on code and error prediction; at the per-problem level, our method outperforms baselines on error coverage and simulated code diversity.

</details>


### [91] [Plasticity vs. Rigidity: The Impact of Low-Rank Adapters on Reasoning on a Micro-Budget](https://arxiv.org/abs/2601.06677)
*Zohaib Khan,Omer Tafveez,Zoha Hayat Bhatti*

Main category: cs.LG

TL;DR: 在单GPU 24小时内，通过RLVR和LoRA训练小模型（≤1.5B）进行数学推理，发现高秩适配器（r=256）能显著提升性能，但数学对齐模型会出现性能崩溃。


<details>
  <summary>Details</summary>
Motivation: 探索在极端资源约束下（单A40 GPU，48GB内存，24小时内）是否能在小语言模型（≤1.5B）中诱导出强大的数学推理能力，研究微预算训练的有效性。

Method: 使用带可验证奖励的强化学习（RLVR）和低秩适配（LoRA）在单GPU上进行训练，研究不同秩的适配器（低秩r=8 vs 高秩r=256）对模型优化动态的影响。

Result: 高秩适配器（r=256）在标准指令调优模型上解锁了显著的可塑性，在AIME 24上达到40.0% Pass@1（比基线提升11.1%），Pass@16达到70.0%。但数学对齐模型出现性能崩溃，表明低预算RL更新可能对已接近任务最优的模型产生破坏性干扰。

Conclusion: 微预算训练的成功关键取决于适配器容量和模型初始化的相互作用。高秩适配器能有效提升推理能力，但模型初始化状态对RL更新的敏感性差异很大，数学对齐模型容易受到噪声RL更新的破坏。

Abstract: Recent advances in mathematical reasoning typically rely on massive scale, yet the question remains: can strong reasoning capabilities be induced in small language models ($\leq1.5\text{B}$) under extreme constraints? We investigate this by training models on a single A40 GPU (48GB) for under 24 hours using Reinforcement Learning with Verifiable Rewards (RLVR) and Low-Rank Adaptation (LoRA). We find that the success of this ``micro-budget" regime depends critically on the interplay between adapter capacity and model initialization. While low-rank adapters ($r=8$) consistently fail to capture the complex optimization dynamics of reasoning, high-rank adapters ($r=256$) unlock significant plasticity in standard instruction-tuned models. Our best result achieved an impressive 40.0\% Pass@1 on AIME 24 (an 11.1\% absolute improvement over baseline) and pushed Pass@16 to 70.0\%, demonstrating robust exploration capabilities. However, this plasticity is not universal: while instruction-tuned models utilized the budget to elongate their chain-of-thought and maximize reward, heavily math-aligned models suffered performance collapse, suggesting that noisy, low-budget RL updates can act as destructive interference for models already residing near a task-specific optimum.

</details>


### [92] [Hallucinations Live in Variance](https://arxiv.org/abs/2601.07058)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 该论文提出语义稳定性作为评估AI代理可靠性的新指标，通过测量模型在语义等价提示下的输出一致性来诊断方差驱动的不可靠性，而非评估正确性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试只衡量模型是否正确，但不评估模型是否可靠。对于单次推理这区别不大，但对代理式AI系统至关重要，因为单个重新表述的提示可能引发多步执行的级联失败，而这种不稳定性未被现有评估方法捕捉。

Method: 提出语义稳定性概念，通过释义一致性来测量：生成k个语义等价的提示，对每个提示进行贪婪解码，计算输出模式的一致性。这诊断方差驱动的不可靠性，而非改进正确性的方法。

Result: 实验显示，密集的Qwen3-0.6B模型与自身的一致性仅为23.8%，而在32%稀疏度下，一致性跃升至55.9%。相图揭示了方差减少超过偏差积累的最佳点，以及稳定性崩溃到错误答案的区域。

Conclusion: 语义稳定性是评估AI代理可靠性的关键诊断工具，能够识别方差驱动的不可靠性，并显示通过减少冗余路径（如稀疏化）可以在不增加知识的情况下提高可靠性。

Abstract: Benchmarks measure whether a model is correct. They do not measure whether a model is reliable. This distinction is largely academic for single-shot inference, but becomes critical for agentic AI systems, where a single rephrased prompt can trigger cascading failures in multi-step execution. Yet this form of instability is not captured by existing evaluations.
  Hallucinations live in variance: they arise when semantically equivalent prompts activate inconsistent internal pathways, producing divergent outputs. Consistent but incorrect outputs reflect bias or missing knowledge; confident guessing reflects calibration failure. Neither constitutes hallucination under this definition. When error is variance-dominated, reducing redundant pathways improves reliability without adding knowledge. We formalize this through Semantic Stability (SS), measured via Paraphrase Consistency (PC@k): generate k paraphrases, greedy decode each, compute mode agreement. SS is a diagnostic for variance-driven unreliability, not a method for improving correctness.
  We show that a dense Qwen3-0.6B agrees with itself only 23.8% of the time; at 32% sparsity, agreement jumps to 55.9%. A phase diagram reveals the sweet spot where variance reduction outpaces bias accumulation, and regimes where stability collapses onto wrong answers.

</details>


### [93] [Reward-Preserving Attacks For Robust Reinforcement Learning](https://arxiv.org/abs/2601.07118)
*Lucas Schott,Elies Gherbi,Hatem Hajri,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 提出α奖励保持攻击方法，自适应调整对抗攻击强度，在保持名义性能的同时提升RL模型的鲁棒性


<details>
  <summary>Details</summary>
Motivation: RL中的对抗鲁棒性面临挑战：强攻击会破坏学习，弱攻击效果有限，且合适的攻击强度随状态变化。需要一种自适应调整攻击强度的方法。

Method: 提出α奖励保持攻击，自适应调整对手强度，使每个状态下名义到最坏情况回报差距的α比例仍可达成。在深度RL中使用基于梯度的攻击方向，通过离线策略训练的critic Q^π_α((s,a),η)学习状态相关的幅度η≤η_B。

Result: 自适应调优校准了攻击强度，在中间α值下，该方法在保持名义性能的同时，在不同半径范围内都提高了鲁棒性，优于固定半径和随机半径基线方法。

Conclusion: α奖励保持攻击通过自适应调整攻击强度，有效解决了RL对抗鲁棒性中的强度选择问题，在保持性能的同时显著提升模型鲁棒性。

Abstract: Adversarial robustness in RL is difficult because perturbations affect entire trajectories: strong attacks can break learning, while weak attacks yield little robustness, and the appropriate strength varies by state. We propose $α$-reward-preserving attacks, which adapt the strength of the adversary so that an $α$ fraction of the nominal-to-worst-case return gap remains achievable at each state. In deep RL, we use a gradient-based attack direction and learn a state-dependent magnitude $η\le η_{\mathcal B}$ selected via a critic $Q^π_α((s,a),η)$ trained off-policy over diverse radii. This adaptive tuning calibrates attack strength and, with intermediate $α$, improves robustness across radii while preserving nominal performance, outperforming fixed- and random-radius baselines.

</details>


### [94] [Offline Meta-Reinforcement Learning with Flow-Based Task Inference and Adaptive Correction of Feature Overgeneralization](https://arxiv.org/abs/2601.07164)
*Min Wang,Xin Li,Mingzhong Wang,Hasnaa Bennis*

Main category: cs.LG

TL;DR: 本文提出FLORA方法解决离线元强化学习中的特征过度泛化问题，通过建模特征分布识别OOD样本，结合回报反馈机制自适应调整特征组件，并使用可逆变换链建模复杂任务分布。


<details>
  <summary>Details</summary>
Motivation: 离线元强化学习结合了离线RL从多样数据集学习的能力和元RL适应新任务的能力，但存在外推误差问题。研究发现Q网络分解在高质量数据下增强适应性，但在复杂任务中导致策略退化，特别是特征遇到OOD样本时产生"特征过度泛化"问题。

Method: 提出FLORA方法：1）通过建模特征分布和估计不确定性来识别OOD样本；2）集成回报反馈机制自适应调整特征组件；3）使用可逆变换链显式建模复杂任务分布以学习精确的任务表示。

Result: 理论和实证证明FLORA相比基线方法在各种环境中实现了快速适应和元策略改进。

Conclusion: FLORA有效解决了离线元强化学习中的特征过度泛化问题，通过OOD样本识别和自适应特征调整机制，提升了算法的适应性和性能。

Abstract: Offline meta-reinforcement learning (OMRL) combines the strengths of learning from diverse datasets in offline RL with the adaptability to new tasks of meta-RL, promising safe and efficient knowledge acquisition by RL agents. However, OMRL still suffers extrapolation errors due to out-of-distribution (OOD) actions, compromised by broad task distributions and Markov Decision Process (MDP) ambiguity in meta-RL setups. Existing research indicates that the generalization of the $Q$ network affects the extrapolation error in offline RL. This paper investigates this relationship by decomposing the $Q$ value into feature and weight components, observing that while decomposition enhances adaptability and convergence in the case of high-quality data, it often leads to policy degeneration or collapse in complex tasks. We observe that decomposed $Q$ values introduce a large estimation bias when the feature encounters OOD samples, a phenomenon we term ''feature overgeneralization''. To address this issue, we propose FLORA, which identifies OOD samples by modeling feature distributions and estimating their uncertainties. FLORA integrates a return feedback mechanism to adaptively adjust feature components. Furthermore, to learn precise task representations, FLORA explicitly models the complex task distribution using a chain of invertible transformations. We theoretically and empirically demonstrate that FLORA achieves rapid adaptation and meta-policy improvement compared to baselines across various environments.

</details>


### [95] [On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training](https://arxiv.org/abs/2601.07389)
*Xueyan Niu,Bo Bai,Wei Han,Weixi Zhang*

Main category: cs.LG

TL;DR: 本文证明了在大型语言模型的后训练中，监督微调（SFT）和强化学习（RL）不能解耦：先SFT后RL会增加SFT损失，先RL后SFT会降低RL获得的奖励。


<details>
  <summary>Details</summary>
Motivation: 现代推理模型广泛采用交替进行SFT和RL训练的做法，但缺乏理论分析这两种方法是否可以解耦。本文旨在从理论上探讨SFT和RL在模型后训练中是否可以分离。

Method: 通过理论证明分析SFT和RL的耦合关系：1）SFT-then-RL耦合：在SFT最优性下，RL会增加SFT损失；2）RL-then-SFT耦合：SFT会降低RL获得的奖励。实验在Qwen3-0.6B模型上进行验证。

Result: 实验证实了理论预测的性能下降，验证了SFT和RL在模型后训练中不能分离而不损失先前性能。

Conclusion: SFT和RL在大型语言模型后训练中不能解耦，分离这两种训练方法会导致先前获得的性能下降。

Abstract: Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from human preferences or rule-based verifiers. Modern reasoning models have widely adopted the practice of alternating SFT and RL training. However, there is no theoretical account of whether they can be decoupled. We prove that decoupling is impossible in either order: (1) SFT-then-RL coupling: RL increases SFT loss under SFT optimality and (2) RL-then-SFT coupling: SFT lowers the reward achieved by RL. Experiments on Qwen3-0.6B confirm the predicted degradation, verifying that SFT and RL cannot be separated without loss of prior performance in the post-training

</details>


### [96] [AntiPaSTO: Self-Supervised Steering of Moral Reasoning](https://arxiv.org/abs/2601.07473)
*Michael J. Clark*

Main category: cs.LG

TL;DR: AntiPaSTO是一种无需大量人工监督的模型控制方法，通过分离表征沿反平行轴实现双向控制，仅需少量对比词对即可显著提升模型在道德困境等任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着模型能力增强，传统人工监督方法面临标签难以扩展、输出可被操纵、训练无法泛化等问题。需要开发能够内部监督、自我监督且能跨分布泛化的可扩展监督方法。

Method: AntiPaSTO方法将表征沿反平行轴分离（α=±1产生相反偏移），通过一致性约束防止表征塌缩。仅需在模板句子中插入两个对比词对，无需偏好标签，人工输入极少。

Result: 在Gemma-3-1B模型上使用800个词对，AntiPaSTO在DailyDilemmas任务上比提示基线提升6.9倍，并能维持双向控制，而提示方法会触发拒绝响应。

Conclusion: AntiPaSTO提供了一种高效的可扩展监督方法，仅需极少量人工输入即可实现模型行为的精确控制，解决了传统监督方法的局限性。

Abstract: As models grow more capable, human supervision breaks down: labels don't scale, outputs can be gamed, and training doesn't generalize. Scalable oversight requires steering methods that are internal, self-supervised, and transfer out-of-distribution; existing methods satisfy some but not all three. We introduce AntiPaSTO, which separates representations along an anti-parallel axis ($α=\pm1$ produce opposite shifts), with coherence constraints preventing collapse. Human input is minimal: two contrasting words inserted into template sentences, no preference labels. Using 800 such pairs on Gemma-3-1B, AntiPaSTO beats prompting baselines by $6.9\times$ on DailyDilemmas and maintains bidirectional control where prompting triggers refusal.
  Code is available at https://github.com/wassname/AntiPaSTO.

</details>


### [97] [Optimal Learning Rate Schedule for Balancing Effort and Performance](https://arxiv.org/abs/2601.07830)
*Valentina Njaradi,Rodrigo Carrasco-Davis,Peter E. Latham,Andrew Saxe*

Main category: cs.LG

TL;DR: 论文提出了一种规范框架，将学习速度控制建模为最优控制问题，推导出最优学习率的闭式解，并展示了如何通过情景记忆机制实现近似最优行为。


<details>
  <summary>Details</summary>
Motivation: 生物和人工智能体都需要高效学习，但必须平衡学习速度与努力、不稳定性和资源消耗的成本。当前缺乏一个统一的数学框架来规范地描述学习速度控制问题。

Method: 将学习速度控制建模为最优控制过程，最大化累积性能同时考虑学习成本。推导出最优学习率的闭式解，分析任务和智能体参数如何影响学习率调度，并提出使用情景记忆机制近似性能期望。

Result: 获得了最优学习率的闭式控制器形式，该解在任务和架构间具有普适性，能复现数值优化的调度方案。分析表明过度自信或自信不足会影响学习参与度和持久性，情景记忆机制能近似实现最优行为。

Conclusion: 该框架为学习速度控制提供了规范和生物学合理的解释，将自我调节学习、努力分配和情景记忆估计统一在一个可处理的数学框架中。

Abstract: Learning how to learn efficiently is a fundamental challenge for biological agents and a growing concern for artificial ones. To learn effectively, an agent must regulate its learning speed, balancing the benefits of rapid improvement against the costs of effort, instability, or resource use. We introduce a normative framework that formalizes this problem as an optimal control process in which the agent maximizes cumulative performance while incurring a cost of learning. From this objective, we derive a closed-form solution for the optimal learning rate, which has the form of a closed-loop controller that depends only on the agent's current and expected future performance. Under mild assumptions, this solution generalizes across tasks and architectures and reproduces numerically optimized schedules in simulations. In simple learning models, we can mathematically analyze how agent and task parameters shape learning-rate scheduling as an open-loop control solution. Because the optimal policy depends on expectations of future performance, the framework predicts how overconfidence or underconfidence influence engagement and persistence, linking the control of learning speed to theories of self-regulated learning. We further show how a simple episodic memory mechanism can approximate the required performance expectations by recalling similar past learning experiences, providing a biologically plausible route to near-optimal behaviour. Together, these results provide a normative and biologically plausible account of learning speed control, linking self-regulated learning, effort allocation, and episodic memory estimation within a unified and tractable mathematical framework.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [98] [Building an AI code reviewer grounded in production telemetry](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fbuilding-a-code-review-system-that-uses-prod-data-to-predict-bugs%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=seer-fy26q4-aicodereviewlaunch%26utm_content=newsletter-prod-blog-learnmore/2/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/jUMxeqbDn2XXLqGXHo_qh9mzqXEFDAhNDB-tTjhw3ps=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sentry构建了一个基于生产遥测数据的AI代码审查工具，专注于预测实际错误而非静态风格反馈


<details>
  <summary>Details</summary>
Motivation: 传统AI代码审查工具产生大量无关紧要的风格建议（如尾随空格、代码格式等），无法有效识别实际生产错误，开发者需要更智能的审查工具

Method: 使用生产遥测数据训练AI模型，构建包含架构、管道和过滤机制的完整系统，使AI能够基于实际生产环境数据预测真实错误

Result: 开发出能够识别实际生产错误的AI代码审查工具，减少无关风格建议，提高审查效率

Conclusion: 基于生产遥测的AI代码审查比传统静态分析更有效，能够提供更有价值的错误预测，提升开发效率

Abstract: Building an AI code reviewer grounded in production telemetry (Sponsor) If AI code reviewers are supposed to save time, why am I triaging 40 comments about trailing whitespace, "consider using an f-string," and "line exceeds 100 characters" on a 10-line change?In this technical deep dive, Sentry breaks down how it built an AI code reviewer that uses production telemetry to predict actual errors — not just static style feedback. If you want to understand the architecture, pipeline, and filteri...

</details>


### [99] [The Complete Guide to Building Agents with the Claude Agent SDK](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnader.substack.com%2Fp%2Fthe-complete-guide-to-building-agents%3Futm_source=tldrai/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/CXjpExFe4zNRtWqE7GAqFNA8B-wrNRCk4j5s9g2aI6k=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文介绍了如何使用Claude Agent SDK从零开始构建代码审查代理，该代理能够读取文件、运行命令、编辑代码并自主规划任务步骤，用于分析代码库、发现bug和安全问题。


<details>
  <summary>Details</summary>
Motivation: 帮助开发者理解Claude Code SDK的工作原理，使他们能够根据实际需求构建自定义的代码审查代理，提高代码质量和安全性。

Method: 使用Claude Agent SDK逐步构建代码审查代理，包括文件读取、命令执行、代码编辑和任务规划等功能模块。

Result: 成功构建了一个能够分析代码库、发现bug和安全问题并提供结构化反馈的代码审查代理。

Conclusion: Claude Agent SDK为构建自定义代码代理提供了强大工具，开发者可以根据具体需求灵活构建各种代码处理代理。

Abstract: The Complete Guide to Building Agents with the Claude Agent SDK (22 minute read) This guide walks readers through the process of building a code review agent from scratch that can read files, run commands, edit code, and figure out the steps to accomplish a task. The resulting agent can analyze a code base, find bugs and security issues, and return structured feedback. The guide aims to help readers understand how the Claude Code SDK works so they can build whatever they actually need.

</details>


### [100] [Scale AI's Agentic Rubrics for Software Agent Verification](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.04171%3Futm_source=tldrai/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/Q64r3xKIomtD309GSX5GtDbWrK_P8WEmaHqN9Dk2j0M=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Scale AI开发了Agentic Rubrics方法，通过专家代理创建清单式评估来验证代码变更，无需测试，在SWE-Bench Verified上表现优于基线方法，为强化学习和推理时反馈提供可扩展、可解释的信号。


<details>
  <summary>Details</summary>
Motivation: 传统代码验证方法依赖测试，但测试可能不完整或难以编写。需要一种无需测试、可扩展且可解释的代码变更验证方法，特别是对于软件代理的验证。

Method: 使用专家代理与代码库交互，创建清单式评估（rubrics）。这些评估提供结构化的验证标准，用于评估代码变更的质量和正确性。

Result: 在SWE-Bench Verified基准测试中，Agentic Rubrics方法优于基线方法，提供更准确的代码变更验证。

Conclusion: Agentic Rubrics为软件代理验证提供了一种有效的无测试方法，具有可扩展性和可解释性，适用于强化学习和推理时反馈场景。

Abstract: Scale AI's Agentic Rubrics for Software Agent Verification (28 minute read) Agentic Rubrics enable test-free verification of code changes by creating checklist-style evaluations from expert agents that interact with codebases. They outperform baseline methods on SWE-Bench Verified and offer a scalable, interpretable signal for reinforcement learning and inference-time feedback.

</details>


### [101] [Developer output jumped 76% in 2025... and team output rose even more](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fstate-of-ai-coding-2025%3Futm_source=tldr%26utm_medium=sponsorship%26utm_campaign=tldr_ai%23section-01/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/jGhuhWSTwg6HXaLNdXjLntyMZRfa7rAtV9Ty__v-Hb4=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 2025年开发者产出增长76%，团队产出增长更多，PR变得更庞大密集，Greptile跨行业研究显示AI编码工具采用率、模型增长趋势和模型快照


<details>
  <summary>Details</summary>
Motivation: 分析AI编码工具在2025年的采用情况，了解开发者产出和团队效率的变化趋势，研究AI对软件开发生产力的影响

Method: Greptile进行的跨行业研究，分析开发者产出数据、PR大小和密度变化，调查AI工具采用率，追踪模型增长趋势

Result: 2025年开发者产出增长76%，团队产出增长更多，PR变得更大更密集，AI工具采用率显著提升，模型持续增长

Conclusion: AI编码工具正在显著提升开发者和团队的生产力，改变软件开发的工作流程和产出模式

Abstract: Developer output jumped 76% in 2025... and team output rose even more (Sponsor) Developers are shipping more code - with PRs getting bigger and denser. Read Greptile's cross-industry study on the state of AI coding to see AI tool adoption, model growth trends, and model snapshots.

</details>


### [102] [Universal Permission Request Hook for Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fdoobidoo%2Ffa84d31c0819a9faace345ca227b268f%3Futm_source=tldrai/1/0100019ba3366d87-00af83a2-d039-4cd0-b215-2e90f16967bd-000000/HW8279fbQNZnoNPz7aCPE36yvCmfN7SGP_qWM5gTr58=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub gist提供Claude Code钩子，自动批准只读MCP工具权限请求，消除频繁的权限弹窗


<details>
  <summary>Details</summary>
Motivation: 解决Claude Code在使用MCP（Model Context Protocol）工具时频繁弹出权限请求对话框的问题，提升开发体验和效率

Method: 实现一个通用的权限请求钩子（hook），自动识别并批准只读类型的MCP工具权限请求

Result: 成功创建了可减少权限弹窗干扰的解决方案，使开发者在使用Claude Code时获得更流畅的体验

Conclusion: 通过自动化权限管理，显著改善了Claude Code工具的使用便利性，特别适合需要频繁使用只读MCP工具的场景

Abstract: Universal Permission Request Hook for Claude Code (8 minute read) This GitHub gist contains a Claude Code hook that auto-approves read-only MCP tools to eliminate constant permission dialogs.

</details>


### [103] [LLM Predictions for 2026, Shared with Oxide and Friends](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F8%2Fllm-predictions-for-2026%2F%3Futm_source=tldrdata/1/0100019bb1e484b2-546bfbe4-1777-42e9-b4e4-c9705f55b88f-000000/6j_7Se7k1pqqQau7Gt0Gv0i8bXNcIU7mqc-LnDIeJDo=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文预测到2026年，LLM生成的代码质量将达到优秀水平，有效的沙箱解决方案将出现以安全运行不可信代码，同时过度授权的编码代理将导致重大安全事件（"挑战者号灾难"）。


<details>
  <summary>Details</summary>
Motivation: 本文旨在预测LLM和编码代理在软件工程领域的发展趋势，特别是关注代码质量、安全性和风险控制等方面，为行业提供前瞻性指导。

Method: 采用预测性分析方法，基于当前LLM和编码代理技术的发展趋势，结合行业观察和专家见解，对未来3-5年的技术发展进行系统性预测。

Result: 预测到2026年：1）LLM生成的代码质量将达到优秀水平；2）有效的沙箱解决方案将出现；3）过度授权的编码代理将导致重大安全事件；4）软件工程的杰文斯悖论将在3年内解决。

Conclusion: LLM和编码代理技术将在未来几年快速发展，但需要平衡效率提升与安全风险，行业应提前准备应对即将出现的技术变革和安全挑战。

Abstract: LLM Predictions for 2026, Shared with Oxide and Friends (5 minute read) 2026 will mark the year when LLM-generated code quality becomes undeniably excellent, effective sandboxing solutions finally emerge to safely run untrusted code, and a major security incident ("Challenger disaster") exposes risks from over-privileged coding agents. Looking further ahead, the Jevons paradox for software engineering will resolve within 3 years.

</details>


### [104] [Supercharging LLMs: Scalable RL with torchforge and Weaver](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpytorch.org%2Fblog%2Fsupercharging-llms-scalable-rl-with-torchforge-and-weaver%2F%3Futm_source=tldrdata/1/0100019bb1e484b2-546bfbe4-1777-42e9-b4e4-c9705f55b88f-000000/OZXBeCq-JE-KA4FwOvkoot7rLQONb_cha_L46P_s4uE=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Meta推出torchforge库，用于大规模强化学习训练LLM，支持数百GPU扩展，降低基础设施复杂度，通过验证器系统降低计算成本并提升数学、科学和推理任务准确率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练面临大规模强化学习扩展困难、基础设施复杂、计算成本高的问题，需要工具来简化大规模RL训练流程并提升效率。

Method: 开发torchforge PyTorch库，隐藏基础设施复杂性，支持快速实验，采用验证器系统优化计算资源使用，实现数百GPU的扩展性。

Result: 系统能够扩展到数百GPU，降低计算成本的同时提升数学、科学和推理任务的准确性，支持快速实验迭代。

Conclusion: torchforge为大规模LLM强化学习训练提供了可扩展的解决方案，显著降低了基础设施复杂性和计算成本，同时提升了模型性能。

Abstract: Supercharging LLMs: Scalable RL with torchforge and Weaver (4 minute read) Meta's torchforge is a PyTorch library for training LLMs with reinforcement learning that can scale to hundreds of GPUs. It hides most of the infrastructure complexity, supports fast experimentation, and uses a verifier system that cuts compute costs while improving accuracy across math, science, and reasoning tasks.

</details>


### [105] [Introducing MCP CLI: A way to call MCP Servers Efficiently](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.philschmid.de%2Fmcp-cli%3Futm_source=tldrdata/1/0100019bb1e484b2-546bfbe4-1777-42e9-b4e4-c9705f55b88f-000000/U1D6Ow_dqR8KjOs7ob8IoHZ3OLj70oakeBtGYvrn8j0=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: MCP CLI是一个轻量级开源命令行工具，通过即时工具发现和执行机制，显著减少与MCP服务器交互时的token消耗（最高可达99%），适用于AI编码代理。


<details>
  <summary>Details</summary>
Motivation: 传统与MCP服务器交互需要静态加载所有工具定义，导致大量token消耗，影响AI编码代理的效率和成本。需要更高效的交互方式。

Method: 开发轻量级命令行工具，采用即时工具发现和执行机制，而非静态加载所有工具定义，实现动态、按需的MCP服务器交互。

Result: MCP CLI能够显著减少token消耗（最高可达99%），提高AI编码代理（如Gemini CLI、Claude Code）与MCP服务器交互的效率。

Conclusion: MCP CLI通过创新的即时工具发现机制，为AI编码代理提供了高效、低成本的MCP服务器交互解决方案。

Abstract: Introducing MCP CLI: A way to call MCP Servers Efficiently (7 minute read) MCP-CLI is a lightweight, open-source command-line tool that enables efficient, dynamic interaction with Model Context Protocol (MCP) servers. By supporting just-in-time tool discovery and execution instead of statically loading all tool definitions, it drastically reduces token consumption (up to 99% savings), making it ideal for AI coding agents like Gemini CLI or Claude Code.

</details>


### [106] [Agent-native Architectures](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevery.to%2Fguides%2Fagent-native%3Fsource=post_button%26utm_source=tldrnewsletter/1/0100019bb1f407d0-63d03be3-b385-4436-aefe-65fcd0a75233-000000/Kp1vjvm4pvJVZ_-hwxWtYmFUj5SJDzJqZ-cxr50kzsU=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code展示了具有bash和文件工具访问权限的大语言模型能够通过循环操作自主完成复杂的多步骤任务，优秀的编码代理实际上是优秀的通用代理，Claude Code SDK使开发者能够构建由工具化代理实现功能的应用。


<details>
  <summary>Details</summary>
Motivation: 探索如何让大语言模型通过工具访问实现自主完成复杂任务的能力，特别是将编码代理的能力扩展到通用代理领域，为开发者提供构建基于代理的应用框架。

Method: 采用代理原生架构，让大语言模型通过访问bash和文件工具，在循环操作中自主执行多步骤任务，直到达成目标。Claude Code SDK提供开发工具包，使开发者能够构建基于代理的应用。

Result: Claude Code成功展示了具有工具访问权限的大语言模型能够自主完成复杂任务，证明了编码代理可以成为优秀的通用代理，并提供了可访问的开发工具包。

Conclusion: 代理原生架构为软件开发开辟了新领域，使开发者能够构建由工具化代理实现功能的应用，将编码代理能力扩展到通用代理领域。

Abstract: Agent-native Architectures (52 minute read) Claude Code demonstrated that large language models with access to bash and file tools can accomplish complex multi-step tasks autonomously by operating in a loop until an objective is achieved. Good coding agents are really good general-purpose agents. The Claude Code software development kit makes this accessible. It allows developers to build applications where features are achieved by agents with tools. This opens up a new field of software that...

</details>


### [107] [Free n8n Course](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.productcompass.pm%2Fp%2Ffree-n8n-course%3Futm_source=tldrmarketing/1/0100019bb2199413-fe363c6d-92c0-4415-85f2-51c7ecffa39c-000000/Q6t4Q00UV4cGfe599t-TtWU5iSBC6DrrSRTzbe45nw0=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 介绍如何使用n8n平台构建LLM工作流、低自主性AI代理和自主代理的免费课程


<details>
  <summary>Details</summary>
Motivation: 帮助开发者学习使用n8n这一结合工作流自动化和代理构建的平台，掌握构建各种AI代理系统的技能

Method: 通过59分钟的视频课程，从零开始演示如何构建LLM工作流、低自主性AI代理和自主代理

Result: 学习者能够掌握使用n8n平台构建竞争对手监控、聊天机器人助手、收件箱工作者和多代理研究系统等应用的能力

Conclusion: n8n是一个强大的平台，能够支持从简单工作流到复杂自主代理的各种AI应用开发

Abstract: Free n8n Course (59 minute video) This course walks through how to build an LLM workflow, a low-autonomy AI agent, and an autonomous agent from scratch using n8n, a combined workflow automation and agent-building platform that can handle use cases like competitor monitoring, chatbot-style assistants, inbox workers, and multi-agent research systems.

</details>


### [108] [Don't fall into the anti-AI hype](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantirez.com%2Fnews%2F158%3Futm_source=tldrdevops/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/hyL-yCTEZugOeL-U_d3-o_T3OpsZtyERiKmM95oeI5k=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 大型语言模型改变了编程范式，从编写代码转向问题定义和解决方案审查


<details>
  <summary>Details</summary>
Motivation: 回应反AI炒作，指出LLM带来的编程范式转变是积极进步而非威胁

Method: 通过分析LLM在代码生成中的实际应用，论证编程技能需求的变化

Result: 编程核心技能从编码转向问题定义和代码审查，LLM成为编程工具而非替代

Conclusion: 不应陷入反AI炒作，应拥抱LLM带来的编程效率提升和技能转型

Abstract: Don't fall into the anti-AI hype (5 minute read) Recent advances in large language models have fundamentally changed programming by enabling most code to be generated through prompting, shifting the primary skill from writing code to defining problems and reviewing solutions.

</details>


### [109] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/JITjo8KGxE7CXRlqoN0_pqEFbE5MivERWXLPLMzqMIs=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 大型语言模型改变了编程范式，从编写代码转向问题定义和方案审查


<details>
  <summary>Details</summary>
Motivation: 回应反AI炒作，强调LLM对编程的根本性改变，编程技能需求从编码转向问题定义和方案评估

Method: 观点性论述，分析LLM如何通过提示生成代码，以及这对编程工作流程的影响

Result: 编程的核心技能转变为：1）清晰定义问题 2）有效审查生成的解决方案 3）集成和调试代码

Conclusion: 不应陷入反AI炒作，应接受LLM带来的编程范式转变，专注于更高层次的技能发展

Abstract: Don't fall into the anti-AI hype (5 minute read) Recent advances in large language models have fundamentally changed programming by enabling most code to be generated through prompting, shifting the primary skill from writing code to defining problems and reviewing solutions.

</details>


### [110] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/d4GGmb05L2M7hF-AJO4A500ILmsfp384TxKmx4h2vLA=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文反驳了"反AI炒作"，认为大语言模型改变了编程本质，从写代码转向定义问题和审查解决方案


<details>
  <summary>Details</summary>
Motivation: 针对当前对AI编程的负面炒作，作者希望澄清大语言模型对编程领域的真正影响，强调编程技能的转变而非替代

Method: 通过分析大语言模型在代码生成方面的最新进展，论证编程核心技能的变化趋势

Result: 编程的主要技能已从编写代码转变为定义问题和审查解决方案，大语言模型使大部分代码可以通过提示生成

Conclusion: 不应陷入反AI炒作，而应认识到编程本质的转变，适应新的技能需求

Abstract: Don't fall into the anti-AI hype (5 minute read) Recent advances in large language models have fundamentally changed programming by enabling most code to be generated through prompting, shifting the primary skill from writing code to defining problems and reviewing solutions.

</details>


### [111] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019bb22f6a45-c43d8d17-aaf6-4551-9695-e96662477ad1-000000/F3klcj47U_3AsUQoodX-lU5UrDBWzkgLAuwpSh4Y288=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文反驳反AI炒作，认为LLM已改变编程范式，从编写代码转向问题定义和方案评审


<details>
  <summary>Details</summary>
Motivation: 针对当前对AI编程的负面炒作和担忧，作者希望澄清LLM对编程的真正影响，强调范式转变而非威胁

Method: 通过分析LLM在代码生成中的实际应用，论证编程技能从编码转向问题定义和解决方案评审的转变

Result: LLM使大部分代码可通过提示生成，编程核心技能变为问题定义和方案评审，而非传统编码

Conclusion: 不应陷入反AI炒作，应接受编程范式的转变，专注于更高层次的问题定义和解决方案评审能力

Abstract: Don't fall into the anti-AI hype (5 minute read) Recent advances in large language models have fundamentally changed programming by enabling most code to be generated through prompting, shifting the primary skill from writing code to defining problems and reviewing solutions.

</details>


### [112] [Don't fall into the anti-AI hype](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantirez.com%2Fnews%2F158%3Futm_source=tldrdev/1/0100019bb2300f73-ceac9e06-172d-4e15-a195-2b7ea602b7fd-000000/bU6SxV8qzyWqqsPjsware_0RIUCl24BfYbp2PhOhogU=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI正在彻底改变编程，使手动编码在许多任务中过时，LLMs能在数小时内完成复杂项目，开发人员需要拥抱这些工具来提升能力


<details>
  <summary>Details</summary>
Motivation: AI正在永久性地改变编程领域，传统手动编码方式在许多任务中变得过时，开发人员需要适应这一变革趋势

Method: 文章主要采用观点阐述和分析的方式，通过观察LLMs在库修改、bug修复和代码生成等领域的实际表现来论证AI对编程的影响

Result: LLMs能够在数小时内完成原本需要数周的复杂项目，展示了AI在编程领域的强大能力，AI是代码民主化的延续

Conclusion: 开发人员需要拥抱并适应这些新工具，利用AI来倍增自己的能力，而不是陷入反AI的炒作中

Abstract: Don't fall into the anti-AI hype (7 minute read) AI is permanently changing programming, making manual coding obsolete for many tasks. LLMs are able to complete complex projects in hours instead of weeks, showing their current power in areas like library modification, bug fixing, and code generation. AI is a continuation of code democratization, and devs need to embrace and adapt to these new tools to multiply their capabilities.

</details>


### [113] [Claude Switcher](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fandisearch%2Fclaude-switcher%3Futm_source=tldrdev/1/0100019bb2300f73-ceac9e06-172d-4e15-a195-2b7ea602b7fd-000000/4dQVAGMxfVH-gczYMkTnPlqUlh3jYF9juvH8fGVpX_o=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Switcher是一个自动化脚本工具，使用Claude Code让AI提示可以作为标准的Unix风格脚本运行，支持shebang和管道操作


<details>
  <summary>Details</summary>
Motivation: 简化AI驱动的脚本编写流程，让AI提示能够像传统脚本一样直接执行，提高开发效率和自动化能力

Method: 通过支持shebang的可执行markdown文件，将AI提示转换为标准Unix脚本，支持管道数据流和脚本链式调用

Result: 实现了AI提示作为可执行脚本的功能，使开发者能够像使用传统脚本一样使用AI生成的代码

Conclusion: Claude Switcher提供了一种创新的方式将AI代码生成与脚本执行流程无缝集成，提升了开发自动化水平

Abstract: Claude Switcher (GitHub Repo) The Claude Code Switcher automates scripts using Claude Code. It enables executable markdown files with shebang support, allowing AI prompts to function as standard Unix-style scripts that can pipe data and chain together.

</details>


### [114] [Code is Clay](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcampedersen.com%2Fcode-is-clay%3Futm_source=tldrdev/1/0100019bb2300f73-ceac9e06-172d-4e15-a195-2b7ea602b7fd-000000/SOeF8nWf82LoEb8YzgE0xNR_EmnPegoE5DBUoRDqdas=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI自动化基础编程将让软件工程师专注于创造性工作，使软件开发成为更有价值的工艺


<details>
  <summary>Details</summary>
Motivation: 当前软件工程中大量时间花费在重复性、商品化的编程任务上，限制了工程师的创造性潜力

Method: 通过AI自动化处理常规编程任务，让人类工程师专注于"超立方体"式的创新性想法和复杂问题解决

Result: 软件工程将从技术性工作转变为更像后工业时代陶艺的工艺，提升专业价值和创造性

Conclusion: AI将重塑软件工程领域，使其从商品化编程转向创造性工艺，提升工程师的社会地位和专业价值

Abstract: Code is Clay (3 minute read) AI's automation of commodity programming will allow software engineers to focus on creative, "hypercube" ideas, transforming the field into a more valued craft like post-industrial pottery.

</details>


### [115] [EIP-8004: Road to Mainnet](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fie5QWS/1/0100019bb2514710-380a7812-0965-454c-939d-c070a2143933-000000/4jSfTw79zIVbxhhw0CXTtI-NbkrLS4mtfp6nNdCnM8s=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: EIP-8004提案旨在将以太坊建立为自主AI代理的协调层，通过三个链上注册表（身份、声誉、验证）来实现代理间的可信交易与协调。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理缺乏标准化的协调机制，难以在去中心化环境中进行可信交互。该提案旨在解决AI代理的身份验证、行为追踪和权限管理问题，为自主AI系统建立可靠的协调基础设施。

Method: 通过三个核心链上注册表：身份注册表用于代理识别，声誉注册表用于追踪行为历史和可信度，验证注册表用于验证能力和权限。提案从v0.4演进到v1.0版本，通过社区反馈不断完善规范。

Result: 提案已从初始版本v0.4发展到成熟的v1.0版本，获得了社区支持。该标准将使AI代理能够使用可验证凭证进行交易和协调，为以太坊上的AI代理生态系统奠定基础。

Conclusion: EIP-8004为自主AI代理在以太坊上的协调提供了标准化框架，通过链上注册表系统解决了身份、声誉和验证等关键问题，为AI代理的大规模采用铺平了道路。

Abstract: EIP-8004: Road to Mainnet (4 minute read) EIP-8004 proposes establishing Ethereum as a coordination layer for autonomous AI agents through three on-chain registries: identity for agent identification, reputation for tracking behavioral history and trustworthiness, and validation for verifying capabilities and permissions. The specification has progressed from v0.4 to v1.0 as it matures with community input. This standard would enable agents to transact and coordinate with verifiable credentia...

</details>


### [116] [Best Practices for Coding with Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fagent-best-practices%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/g_I065aJomxQ4WpV5UnXMQKKDhmUf-WjHlIXk03gWhg=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文提供了使用代码代理的最佳实践指南，包括如何结构化问题、指导多文件变更以及确保代理有效迭代代码直到测试通过。


<details>
  <summary>Details</summary>
Motivation: 随着代码代理在软件开发中的广泛应用，许多开发者发现使用这些工具时存在效率问题。本文旨在提供实用指南，帮助开发者最大化代码代理的生产力，解决实际使用中的常见挑战。

Method: 本文基于实践经验和案例分析，提出了一套系统的最佳实践方法。包括：1）如何将复杂问题分解为可管理的任务；2）指导代理进行多文件变更的策略；3）建立有效的迭代机制确保代码质量；4）测试驱动的工作流程设计。

Result: 通过应用这些最佳实践，开发者能够显著提高代码代理的工作效率，减少迭代次数，提高代码质量，并更有效地处理复杂的多文件变更任务。

Conclusion: 代码代理是强大的生产力工具，但需要正确的使用方法和策略才能发挥最大价值。本文提供的实践指南为开发者提供了系统的方法来优化与代码代理的协作，从而在软件开发中获得更高的效率和质量。

Abstract: Best Practices for Coding with Agents (18 minute read) Tips for maximizing productivity with coding agents, including how to structure problems, guide multi-file changes, and ensure agents iterate effectively on code until tests pass.

</details>


### [117] [Agents that don't suck](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FgDoRDp%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/-p3DPhWxsyhPwlkZfLuMdUwkDI5nqK25SC1bMDyPR5Y=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Bricks是一个帮助构建、评估和优化基于独特数据的AI代理的平台，通过自动评估、目标评分和人类反馈改进，加速代理产品化


<details>
  <summary>Details</summary>
Motivation: 当前AI代理开发面临评估困难、优化过程不明确、难以从原型到生产的问题，需要系统化的工具来构建真正实用的代理

Method: 提供平台工具，包括自动评估系统、基于目标的输出评分机制、人类反馈集成，帮助开发者构建和优化基于特定数据的AI代理

Result: 平台能够帮助开发者更高效地构建、评估和优化AI代理，提供清晰的产品化路径，使代理在实际场景中更有效工作

Conclusion: Agent Bricks通过系统化的评估和优化工具，解决了AI代理开发中的关键挑战，值得开发者投入时间使用

Abstract: Agents that don't suck (Sponsor) Agent Bricks helps you build, evaluate and optimize AI agents grounded in your unique data. It evaluates automatically, scores outputs against your goals and improves with human feedback — giving you a clearer path to production. Build agents that work in the real world. See why it's worth your time

</details>


### [118] [Agent design patterns](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frlancemartin.github.io%2F2026%2F01%2F09%2Fagent_design%2F%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/NQKZE9s461T8HVm2L8Q3bSiDXGvPtZG3jqiYTZfFfw0=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了随着上下文增长模型性能下降的问题，提出了通过有效的上下文管理设计模式来构建长期运行自主代理的方法。


<details>
  <summary>Details</summary>
Motivation: 随着我们越来越接近实现长期运行的自主代理，发现模型在上下文增长时性能会下降，有效的代理设计主要归结为上下文管理问题。

Method: 通过探索和分析跨代理的常见设计模式，研究如何优化上下文管理来提升代理性能。

Result: 识别了代理设计中常见的上下文管理模式，为构建更有效的长期运行自主代理提供了设计指导。

Conclusion: 有效的上下文管理是构建长期运行自主代理的关键，通过采用适当的设计模式可以显著提升代理性能。

Abstract: Agent design patterns (12 minute read) We are getting closer to long-running autonomous agents. However, models get worse as context grows. Effective agent design largely boils down to context management. This post explores common design patterns across agents.

</details>


### [119] [Evaluating AI Agents in Production](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fdemystifying-evals-for-ai-agents%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/BwPFozknIjj3XgO6-kYbip324TSJad-EYxpT_-c0_r8=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic提出在部署前通过模拟真实世界条件来评估AI代理的实用方法，以减少复杂多轮代理系统的故障


<details>
  <summary>Details</summary>
Motivation: 随着AI代理系统变得越来越复杂，需要有效的评估方法来确保其在生产环境中的可靠性和安全性，减少实际部署中的故障

Method: 采用部署前测试方法，模拟真实世界条件，针对复杂多轮代理系统设计评估框架

Result: 开发了实用的评估方法，能够有效识别和减少代理系统在生产环境中的潜在故障

Conclusion: 部署前模拟真实条件的评估方法对于确保复杂AI代理系统的生产可靠性至关重要

Abstract: Evaluating AI Agents in Production (28 minute read) Anthropic's practical approaches to agent evaluation emphasize pre-deployment tests that simulate real-world conditions and reduce failures in complex, multi-turn agent systems.

</details>


### [120] [Deep Think with Confidence](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2508.15260%3Futm_source=tldrai/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/QBkEQojuMz-p_25HqFCrdmUBRm95nMOTXLR2R0g5eP8=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Meta AI提出DeepThink with Confidence (DeepConf)技术，利用LLM内部置信度信号，通过监控token级置信度提前终止低质量推理轨迹，可将推理开销降低84.7%同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理过程存在大量计算开销，许多推理轨迹质量不高但依然消耗计算资源。需要一种方法能够识别并提前终止低置信度的推理过程，从而显著降低推理成本。

Method: DeepConf通过监控LLM生成过程中的token级置信度信号，当检测到模型表现出不确定性标记（如"wait"或"think again"）时，提前终止低置信度的推理轨迹。该方法利用模型内部的自信心指标来评估推理质量。

Result: 该方法能够将LLM推理开销降低高达84.7%，同时保持推理准确性。特别适用于处理那些模型表现出不确定性的推理场景。

Conclusion: DeepConf技术通过利用LLM内部置信度信号来优化推理过程，实现了显著的计算效率提升，为大规模LLM推理应用提供了有效的成本控制方案。

Abstract: Deep Think with Confidence (23 minute read) Meta AI researchers developed DeepThink with Confidence (DeepConf), a technique that uses internal confidence signals to cut LLM reasoning overhead by up to 84.7% while maintaining accuracy. The method monitors token-level confidence to terminate low-quality reasoning traces early, particularly when models generate uncertainty markers like "wait" or "think again."

</details>


### [121] [Agent-Native Architectures and the Shift Beyond Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FMcBgg2/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/r31H52gUCFLKwuILt-1xKAyK-E_iRCQ-jpxbHv57_wM=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent-native architectures 用智能体替代逐步代码，开发者只需通过提示定义期望结果，使软件构建和适应更快，但牺牲了可预测性以换取灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统软件开发需要编写详细的逐步代码，修改逻辑复杂且耗时。Agent-native架构旨在通过让智能体自主决定如何实现目标，开发者只需定义期望结果，从而加速软件开发和适应过程。

Method: 采用基于提示的编程范式，开发者通过自然语言提示定义期望结果，智能体自主决定实现路径。架构转向持续修剪和观察，而非预先设计完整逻辑。

Result: 软件构建和适应速度显著提升，行为变更通过修改语言提示而非重写逻辑实现。但牺牲了传统代码的可预测性，需要新的软件设计方法。

Conclusion: Agent-native架构代表了软件开发范式的根本转变，从确定性代码转向基于智能体的灵活系统，推动软件设计向持续优化和观察演进。

Abstract: Agent-Native Architectures and the Shift Beyond Code (3 minute read) Agent-native architectures replace step-by-step code with agents that decide how to achieve outcomes, while developers define only the desired results through prompts. This model makes software faster to build and adapt, since behavior changes come from modifying language instead of rewriting logic. The shift trades predictability for flexibility, pushing software design toward continuous pruning and observation rather than ...

</details>


### [122] [In AI Agents, Traces Are the Source of Truth](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FkoX9un/1/0100019bb2929d00-5450a88f-f102-4d41-bf69-569bf986d5f3-000000/QFzo6aRnbifIrxxjUIoJSSoplIJ7jHJhumnM5aBxvZo=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI智能体中，代码仅编排模型和工具，真实决策在运行时模型内部发生，无法仅通过阅读代码理解。执行轨迹记录了实际推理步骤、工具调用、错误、成本和结果，成为调试、测试、优化和监控的主要依据。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体开发中，仅分析代码无法理解实际决策过程，因为核心推理发生在运行时的大模型内部。需要一种方法来捕获和分析实际执行过程，以支持有效的调试、测试和优化。

Method: 提出将执行轨迹作为AI智能体开发的核心分析对象。轨迹记录包括：推理步骤、工具调用、错误信息、成本数据和最终结果等运行时信息。

Result: 执行轨迹成为AI智能体开发、调试、测试、优化和监控的主要依据，提供了比代码更全面的系统理解。

Conclusion: 在AI智能体开发中，执行轨迹比代码更能反映系统真实行为，应作为开发和分析的核心关注点。

Abstract: In AI Agents, Traces Are the Source of Truth (4 minute read) In AI agents, the code only orchestrates models and tools, while real decision-making happens inside the model at runtime and cannot be understood by reading the code alone. Traces capture the actual reasoning steps, tool calls, errors, costs, and outcomes, making them the primary artifact for debugging, testing, optimization, and monitoring.

</details>
