{"id": "2601.10719", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10719", "abs": "https://arxiv.org/abs/2601.10719", "authors": ["Gerard Yeo", "Svetlana Churina", "Kokil Jaidka"], "title": "Do You Trust Me? Cognitive-Affective Signatures of Trustworthiness in Large Language Models", "comment": null, "summary": "Perceived trustworthiness underpins how users navigate online information, yet it remains unclear whether large language models (LLMs),increasingly embedded in search, recommendation, and conversational systems, represent this construct in psychologically coherent ways. We analyze how instruction-tuned LLMs (Llama 3.1 8B, Qwen 2.5 7B, Mistral 7B) encode perceived trustworthiness in web-like narratives using the PEACE-Reviews dataset annotated for cognitive appraisals, emotions, and behavioral intentions. Across models, systematic layer- and head-level activation differences distinguish high- from low-trust texts, revealing that trust cues are implicitly encoded during pretraining. Probing analyses show linearly de-codable trust signals and fine-tuning effects that refine rather than restructure these representations. Strongest associations emerge with appraisals of fairness, certainty, and accountability-self -- dimensions central to human trust formation online. These findings demonstrate that modern LLMs internalize psychologically grounded trust signals without explicit supervision, offering a representational foundation for designing credible, transparent, and trust-worthy AI systems in the web ecosystem. Code and appendix are available at: https://github.com/GerardYeo/TrustworthinessLLM.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9690\u5f0f\u7f16\u7801\u4e86\u5fc3\u7406\u53ef\u4fe1\u5ea6\u4fe1\u53f7\uff0c\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u5c31\u80fd\u533a\u5206\u9ad8\u4fe1\u4efb\u548c\u4f4e\u4fe1\u4efb\u6587\u672c\uff0c\u4e3a\u8bbe\u8ba1\u53ef\u4fe1AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8868\u5f81\u57fa\u7840\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u4ee5\u5fc3\u7406\u4e00\u81f4\u7684\u65b9\u5f0f\u8868\u793a\u5728\u7ebf\u4fe1\u606f\u7684\u53ef\u4fe1\u5ea6\uff0c\u7279\u522b\u662f\u5728LLMs\u65e5\u76ca\u5d4c\u5165\u641c\u7d22\u3001\u63a8\u8350\u548c\u5bf9\u8bdd\u7cfb\u7edf\u7684\u60c5\u51b5\u4e0b\u3002\u9700\u8981\u4e86\u89e3LLMs\u5982\u4f55\u7f16\u7801\u611f\u77e5\u53ef\u4fe1\u5ea6\uff0c\u4ee5\u53ca\u8fd9\u79cd\u7f16\u7801\u662f\u5426\u4e0e\u4eba\u7c7b\u4fe1\u4efb\u5f62\u6210\u7684\u5fc3\u7406\u7ef4\u5ea6\u4e00\u81f4\u3002", "method": "\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u7684LLMs\uff08Llama 3.1 8B\u3001Qwen 2.5 7B\u3001Mistral 7B\uff09\u5206\u6790PEACE-Reviews\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6807\u6ce8\u4e86\u8ba4\u77e5\u8bc4\u4f30\u3001\u60c5\u611f\u548c\u884c\u4e3a\u610f\u56fe\u3002\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5c42\u548c\u5934\u7ea7\u522b\u7684\u6fc0\u6d3b\u5dee\u5f02\u6765\u533a\u5206\u9ad8\u4fe1\u4efb\u548c\u4f4e\u4fe1\u4efb\u6587\u672c\uff0c\u5e76\u8fdb\u884c\u63a2\u6d4b\u5206\u6790\u6765\u7814\u7a76\u53ef\u4fe1\u5ea6\u4fe1\u53f7\u7684\u53ef\u89e3\u7801\u6027\u548c\u5fae\u8c03\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u6240\u6709\u6a21\u578b\u5728\u5c42\u548c\u5934\u7ea7\u522b\u90fd\u663e\u793a\u51fa\u533a\u5206\u9ad8\u4fe1\u4efb\u548c\u4f4e\u4fe1\u4efb\u6587\u672c\u7684\u7cfb\u7edf\u6fc0\u6d3b\u5dee\u5f02\uff1b2\uff09\u53ef\u4fe1\u5ea6\u4fe1\u53f7\u662f\u7ebf\u6027\u53ef\u89e3\u7801\u7684\uff1b3\uff09\u5fae\u8c03\u4f1a\u4f18\u5316\u800c\u975e\u91cd\u6784\u8fd9\u4e9b\u8868\u5f81\uff1b4\uff09\u6700\u5f3a\u7684\u5173\u8054\u51fa\u73b0\u5728\u516c\u5e73\u6027\u3001\u786e\u5b9a\u6027\u548c\u81ea\u6211\u8d23\u4efb\u7b49\u4eba\u7c7b\u4fe1\u4efb\u5f62\u6210\u7684\u6838\u5fc3\u7ef4\u5ea6\u4e0a\u3002", "conclusion": "\u73b0\u4ee3LLMs\u5728\u6ca1\u6709\u663e\u5f0f\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5185\u5316\u4e86\u5fc3\u7406\u57fa\u7840\u7684\u53ef\u4fe1\u5ea6\u4fe1\u53f7\uff0c\u8fd9\u4e3a\u5728\u7f51\u7edc\u751f\u6001\u7cfb\u7edf\u4e2d\u8bbe\u8ba1\u53ef\u4fe1\u3001\u900f\u660e\u548c\u503c\u5f97\u4fe1\u8d56\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8868\u5f81\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.10773", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10773", "abs": "https://arxiv.org/abs/2601.10773", "authors": ["Niko Usai", "Dario Montagnini", "Kristian Ilianov Iliev", "Raffaele Camanzo"], "title": "LogicLens: Leveraging Semantic Code Graph to explore Multi Repository large systems", "comment": null, "summary": "Understanding large software systems is a challenging task, especially when code is distributed across multiple repositories and microservices. Developers often need to reason not only about the structure of the code, but also about its domain logic and runtime behaviors, which are typically implicit and scattered. We introduce LogicLens, a reactive conversational agent that assists developers in exploring complex software systems through a semantic multi-repository graph. This graph is built in a preprocessing step by combining syntactic code analysis, via AST parsing and repository traversal, with semantic enrichment using Large Language Models (LLMs). The resulting graph captures both structural elements, such as files, classes, and functions, as well as functional abstractions like domain entities, operations, and workflows. Once the graph is constructed, LogicLens enables developers to interact with it via natural language, dynamically retrieving relevant subgraphs and answering technical or functional queries. We present the architecture of the system, discuss emergent behaviors, and evaluate its effectiveness on real-world multi-repository scenarios. We demonstrate emergent capabilities including impact analysis and symptom-based debugging that arise naturally from the semantic graph structure.", "AI": {"tldr": "LogicLens\u662f\u4e00\u4e2a\u53cd\u5e94\u5f0f\u5bf9\u8bdd\u4ee3\u7406\uff0c\u901a\u8fc7\u8bed\u4e49\u591a\u4ed3\u5e93\u56fe\u5e2e\u52a9\u5f00\u53d1\u8005\u63a2\u7d22\u590d\u6742\u8f6f\u4ef6\u7cfb\u7edf\uff0c\u7ed3\u5408\u4ee3\u7801\u5206\u6790\u548cLLM\u8bed\u4e49\u589e\u5f3a\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u65b0\u5174\u80fd\u529b\u5982\u5f71\u54cd\u5206\u6790\u548c\u57fa\u4e8e\u75c7\u72b6\u7684\u8c03\u8bd5\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u8f6f\u4ef6\u7cfb\u7edf\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u4ee3\u7801\u5206\u5e03\u5728\u591a\u4e2a\u4ed3\u5e93\u548c\u5fae\u670d\u52a1\u4e2d\u3002\u5f00\u53d1\u8005\u4e0d\u4ec5\u9700\u8981\u7406\u89e3\u4ee3\u7801\u7ed3\u6784\uff0c\u8fd8\u9700\u8981\u7406\u89e3\u901a\u5e38\u9690\u542b\u4e14\u5206\u6563\u7684\u9886\u57df\u903b\u8f91\u548c\u8fd0\u884c\u65f6\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u9884\u5904\u7406\u6b65\u9aa4\u6784\u5efa\u8bed\u4e49\u591a\u4ed3\u5e93\u56fe\uff0c\u7ed3\u5408\u8bed\u6cd5\u4ee3\u7801\u5206\u6790\uff08AST\u89e3\u6790\u548c\u4ed3\u5e93\u904d\u5386\uff09\u4e0e\u4f7f\u7528LLM\u7684\u8bed\u4e49\u589e\u5f3a\u3002\u8be5\u56fe\u6355\u83b7\u7ed3\u6784\u5143\u7d20\uff08\u6587\u4ef6\u3001\u7c7b\u3001\u51fd\u6570\uff09\u548c\u529f\u80fd\u62bd\u8c61\uff08\u9886\u57df\u5b9e\u4f53\u3001\u64cd\u4f5c\u3001\u5de5\u4f5c\u6d41\uff09\u3002\u7136\u540e\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u52a8\u6001\u68c0\u7d22\u76f8\u5173\u5b50\u56fe\u5e76\u56de\u7b54\u6280\u672f\u6216\u529f\u80fd\u67e5\u8be2\u3002", "result": "\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u67b6\u6784\uff0c\u8ba8\u8bba\u4e86\u65b0\u5174\u884c\u4e3a\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u591a\u4ed3\u5e93\u573a\u666f\u4e2d\u8bc4\u4f30\u4e86\u5176\u6709\u6548\u6027\u3002\u6f14\u793a\u4e86\u4ece\u8bed\u4e49\u56fe\u7ed3\u6784\u4e2d\u81ea\u7136\u4ea7\u751f\u7684\u65b0\u5174\u80fd\u529b\uff0c\u5305\u62ec\u5f71\u54cd\u5206\u6790\u548c\u57fa\u4e8e\u75c7\u72b6\u7684\u8c03\u8bd5\u3002", "conclusion": "LogicLens\u901a\u8fc7\u8bed\u4e49\u591a\u4ed3\u5e93\u56fe\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0c\u6709\u6548\u5e2e\u52a9\u5f00\u53d1\u8005\u63a2\u7d22\u548c\u7406\u89e3\u590d\u6742\u8f6f\u4ef6\u7cfb\u7edf\uff0c\u5c55\u73b0\u4e86\u4ece\u56fe\u7ed3\u6784\u4e2d\u81ea\u7136\u4ea7\u751f\u7684\u65b0\u5174\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2601.10775", "categories": ["cs.CL", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10775", "abs": "https://arxiv.org/abs/2601.10775", "authors": ["Tommaso Felice Banfi", "Sashenka Gamage"], "title": "LLMs for Game Theory: Entropy-Guided In-Context Learning and Adaptive CoT Reasoning", "comment": "Accepted at AAAI 2026 Bridge (Logical and Symbolic Reasoning in Language Models)", "summary": "We propose a novel LLM-based framework for reasoning in discrete, game-theoretic tasks, illustrated with \\emph{Tic-Tac-Toe}. The method integrates in-context learning with entropy-guided chain-of-thought (CoT) reasoning and adaptive context retrieval. The model dynamically adjusts both the number of retrieved examples and reasoning paths according to token-level uncertainty: concise reasoning with minimal context is used when uncertainty is low, whereas higher uncertainty triggers expanded multi-path CoT exploration. Experimental evaluation against a sub-optimal algorithmic opponent shows that entropy-aware adaptive reasoning substantially improves decision quality, increasing the average game outcome from \\(-11.6\\%\\) with the baseline LLM to \\(+9.5\\%\\) with entropy-guided adaptive reasoning over 100 games (win = +1, tie = 0, loss = -1), while maintaining a relatively low number of LLM queries per game. Statistical validation confirms that the improvement is significant, and correlation analysis reveals a negative association between token-level entropy and move optimality. These findings demonstrate that uncertainty-guided adaptive reasoning effectively enhances LLM performance in sequential decision-making environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u79bb\u6563\u535a\u5f08\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u71b5\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u63a8\u7406\u673a\u5236\uff0c\u5728\u4e95\u5b57\u68cb\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u51b3\u7b56\u8d28\u91cf", "motivation": "\u73b0\u6709LLM\u5728\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u6839\u636e\u51b3\u7b56\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u63a8\u7406\u590d\u6742\u5ea6\uff0c\u5bfc\u81f4\u5728\u535a\u5f08\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73", "method": "\u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e0e\u71b5\u5f15\u5bfc\u7684\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u6839\u636etoken\u7ea7\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u793a\u4f8b\u6570\u91cf\u548c\u63a8\u7406\u8def\u5f84\uff1a\u4f4e\u4e0d\u786e\u5b9a\u6027\u65f6\u4f7f\u7528\u7b80\u6d01\u63a8\u7406\uff0c\u9ad8\u4e0d\u786e\u5b9a\u6027\u65f6\u89e6\u53d1\u591a\u8def\u5f84\u63a2\u7d22", "result": "\u5728100\u573a\u5bf9\u6b21\u4f18\u7b97\u6cd5\u5bf9\u624b\u7684\u6e38\u620f\u4e2d\uff0c\u5e73\u5747\u6e38\u620f\u7ed3\u679c\u4ece\u57fa\u7ebfLLM\u7684-11.6%\u63d0\u5347\u5230\u71b5\u5f15\u5bfc\u81ea\u9002\u5e94\u63a8\u7406\u7684+9.5%\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684LLM\u67e5\u8be2\u6b21\u6570\uff0c\u7edf\u8ba1\u9a8c\u8bc1\u663e\u793a\u6539\u8fdb\u663e\u8457", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u63a8\u7406\u80fd\u6709\u6548\u589e\u5f3aLLM\u5728\u5e8f\u5217\u51b3\u7b56\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0ctoken\u7ea7\u71b5\u4e0e\u79fb\u52a8\u6700\u4f18\u6027\u5448\u8d1f\u76f8\u5173\u5173\u7cfb", "topic": "agent analysis"}}
{"id": "2601.10738", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10738", "abs": "https://arxiv.org/abs/2601.10738", "authors": ["Percy Jardine"], "title": "CTHA: Constrained Temporal Hierarchical Architecture for Stable Multi-Agent LLM Systems", "comment": null, "summary": "Recently, multi-time-scale agent architectures have extended the ubiquitous single-loop paradigm by introducing temporal hierarchies with distinct cognitive layers. While yielding substantial performance gains, this diversification fundamentally compromises the coordination stability intrinsic to unified agent systems, which causes severe inter-layer conflicts, unbounded error propagation, and restricted scalability. To address these challenges, we propose Constrained Temporal Hierarchical Architecture (CTHA), a general framework that projects the inter-layer communication space onto structured manifolds to restore coordination stability, while incorporating principled arbitration mechanisms to ensure coherent decision-making. Specifically, CTHA enforces three key constraints: (1) Message Contract Constraints that formalize information flow between layers via typed summary, plan, and policy packets; (2) Authority Manifold Constraints that bound each layer's decision space according to its temporal scope; and (3) Arbiter Resolution Constraints that guarantee conflict-free composition of multi-layer decisions. Empirical experiments demonstrate that CTHA is effective for complex task execution at scale, offering 47% reduction in failure cascades, 2.3x improvement in sample efficiency, and superior scalability compared to unconstrained hierarchical baselines. We anticipate that CTHA, as a principled extension of temporal hierarchies, will contribute to a deeper understanding of multi-agent coordination and suggest promising directions for the evolution of robust autonomous systems.", "AI": {"tldr": "\u63d0\u51fa\u7ea6\u675f\u65f6\u95f4\u5206\u5c42\u67b6\u6784(CTHA)\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6d41\u5f62\u6295\u5f71\u548c\u4ef2\u88c1\u673a\u5236\u89e3\u51b3\u591a\u65f6\u95f4\u5c3a\u5ea6\u667a\u80fd\u4f53\u67b6\u6784\u4e2d\u7684\u534f\u8c03\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u5931\u8d25\u7ea7\u8054\u5e76\u63d0\u5347\u6837\u672c\u6548\u7387\u3002", "motivation": "\u591a\u65f6\u95f4\u5c3a\u5ea6\u667a\u80fd\u4f53\u67b6\u6784\u867d\u7136\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u7834\u574f\u4e86\u7edf\u4e00\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u7a33\u5b9a\u6027\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u5c42\u95f4\u51b2\u7a81\u3001\u65e0\u754c\u9519\u8bef\u4f20\u64ad\u548c\u53ef\u6269\u5c55\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u7ea6\u675f\u65f6\u95f4\u5206\u5c42\u67b6\u6784(CTHA)\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ea6\u675f\uff1a\u6d88\u606f\u5951\u7ea6\u7ea6\u675f\uff08\u901a\u8fc7\u7c7b\u578b\u5316\u6458\u8981\u3001\u8ba1\u5212\u3001\u7b56\u7565\u5305\u5f62\u5f0f\u5316\u5c42\u95f4\u4fe1\u606f\u6d41\uff09\u3001\u6743\u5a01\u6d41\u5f62\u7ea6\u675f\uff08\u6839\u636e\u65f6\u95f4\u8303\u56f4\u9650\u5b9a\u5404\u5c42\u51b3\u7b56\u7a7a\u95f4\uff09\u3001\u4ef2\u88c1\u89e3\u51b3\u7ea6\u675f\uff08\u4fdd\u8bc1\u591a\u5c42\u51b3\u7b56\u7684\u65e0\u51b2\u7a81\u7ec4\u5408\uff09\u3002", "result": "CTHA\u5728\u590d\u6742\u4efb\u52a1\u6267\u884c\u4e2d\u6709\u6548\uff0c\u76f8\u6bd4\u65e0\u7ea6\u675f\u5206\u5c42\u57fa\u7ebf\u51cf\u5c1147%\u7684\u5931\u8d25\u7ea7\u8054\uff0c\u63d0\u53472.3\u500d\u7684\u6837\u672c\u6548\u7387\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "CTHA\u4f5c\u4e3a\u65f6\u95f4\u5206\u5c42\u67b6\u6784\u7684\u539f\u5219\u6027\u6269\u5c55\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u591a\u667a\u80fd\u4f53\u534f\u8c03\uff0c\u5e76\u4e3a\u9c81\u68d2\u81ea\u4e3b\u7cfb\u7edf\u7684\u6f14\u8fdb\u63d0\u4f9b\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.10744", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.10744", "abs": "https://arxiv.org/abs/2601.10744", "authors": ["Sen Wang", "Bangwei Liu", "Zhenkun Gao", "Lizhuang Ma", "Xuhong Wang", "Yuan Xie", "Xin Tan"], "title": "Explore with Long-term Memory: A Benchmark and Multimodal LLM-based Reinforcement Learning Framework for Embodied Exploration", "comment": "Our dataset and code will be released at our \\href{https://wangsen99.github.io/papers/lmee/}{website}", "summary": "An ideal embodied agent should possess lifelong learning capabilities to handle long-horizon and complex tasks, enabling continuous operation in general environments. This not only requires the agent to accurately accomplish given tasks but also to leverage long-term episodic memory to optimize decision-making. However, existing mainstream one-shot embodied tasks primarily focus on task completion results, neglecting the crucial process of exploration and memory utilization. To address this, we propose Long-term Memory Embodied Exploration (LMEE), which aims to unify the agent's exploratory cognition and decision-making behaviors to promote lifelong learning.We further construct a corresponding dataset and benchmark, LMEE-Bench, incorporating multi-goal navigation and memory-based question answering to comprehensively evaluate both the process and outcome of embodied exploration. To enhance the agent's memory recall and proactive exploration capabilities, we propose MemoryExplorer, a novel method that fine-tunes a multimodal large language model through reinforcement learning to encourage active memory querying. By incorporating a multi-task reward function that includes action prediction, frontier selection, and question answering, our model achieves proactive exploration. Extensive experiments against state-of-the-art embodied exploration models demonstrate that our approach achieves significant advantages in long-horizon embodied tasks.", "AI": {"tldr": "\u63d0\u51faLMEE\u6846\u67b6\u548cMemoryExplorer\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4fc3\u8fdb\u667a\u80fd\u4f53\u4e3b\u52a8\u63a2\u7d22\u548c\u8bb0\u5fc6\u5229\u7528\uff0c\u5728\u957f\u65f6\u7a0b\u5177\u8eab\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5177\u8eab\u667a\u80fd\u4f53\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u5b8c\u6210\u7ed3\u679c\uff0c\u5ffd\u89c6\u4e86\u63a2\u7d22\u8fc7\u7a0b\u548c\u8bb0\u5fc6\u5229\u7528\uff0c\u800c\u7406\u60f3\u7684\u5177\u8eab\u667a\u80fd\u4f53\u5e94\u5177\u5907\u7ec8\u8eab\u5b66\u4e60\u80fd\u529b\uff0c\u80fd\u591f\u5229\u7528\u957f\u671f\u60c5\u666f\u8bb0\u5fc6\u4f18\u5316\u51b3\u7b56\u3002", "method": "\u63d0\u51faLMEE\u6846\u67b6\u7edf\u4e00\u63a2\u7d22\u8ba4\u77e5\u548c\u51b3\u7b56\u884c\u4e3a\uff0c\u6784\u5efaLMEE-Bench\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f00\u53d1MemoryExplorer\u65b9\u6cd5\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u5305\u542b\u52a8\u4f5c\u9884\u6d4b\u3001\u8fb9\u754c\u9009\u62e9\u548c\u95ee\u7b54\u7684\u591a\u4efb\u52a1\u5956\u52b1\u51fd\u6570\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u5177\u8eab\u63a2\u7d22\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u65f6\u7a0b\u5177\u8eab\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u52bf\uff0c\u8bc1\u660e\u4e86\u4e3b\u52a8\u8bb0\u5fc6\u67e5\u8be2\u548c\u63a2\u7d22\u7684\u6709\u6548\u6027\u3002", "conclusion": "LMEE\u6846\u67b6\u548cMemoryExplorer\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u5177\u8eab\u667a\u80fd\u4f53\u7684\u7ec8\u8eab\u5b66\u4e60\u80fd\u529b\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u548c\u8bb0\u5fc6\u5229\u7528\u63d0\u5347\u957f\u65f6\u7a0b\u590d\u6742\u4efb\u52a1\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.10942", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10942", "abs": "https://arxiv.org/abs/2601.10942", "authors": ["Zitong Zhou", "Matteo Paltenghi", "Miryung Kim", "Michael Pradel"], "title": "Change And Cover: Last-Mile, Pull Request-Based Regression Test Augmentation", "comment": null, "summary": "Software is in constant evolution, with developers frequently submitting pull requests (PRs) to introduce new features or fix bugs. Testing PRs is critical to maintaining software quality. Yet, even in projects with extensive test suites, some PR-modified lines remain untested, leaving a \"last-mile\" regression test gap. Existing test generators typically aim to improve overall coverage, but do not specifically target the uncovered lines in PRs. We present Change And Cover (ChaCo), an LLM-based test augmentation technique that addresses this gap. It makes three contributions: (i) ChaCo considers the PR-specific patch coverage, offering developers augmented tests for code just when it is on the developers' mind. (ii) We identify providing suitable test context as a crucial challenge for an LLM to generate useful tests, and present two techniques to extract relevant test content, such as existing test functions, fixtures, and data generators. (iii) To make augmented tests acceptable for developers, ChaCo carefully integrates them into the existing test suite, e.g., by matching the test's structure and style with the existing tests, and generates a summary of the test addition for developer review. We evaluate ChaCo on 145 PRs from three popular and complex open-source projects - SciPy, Qiskit, and Pandas. The approach successfully helps 30% of PRs achieve full patch coverage, at the cost of $0.11, showing its effectiveness and practicality. Human reviewers find the tests to be worth adding (4.53/5.0), well integrated (4.2/5.0), and relevant to the PR (4.7/5.0). Ablations show test context is crucial for context-aware test generation, leading to 2x coverage. We submitted 12 tests, of which 8 have already been merged, and two previously unknown bugs were exposed and fixed. We envision our approach to be integrated into CI workflows, automating the last mile of regression test augmentation.", "AI": {"tldr": "ChaCo\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6d4b\u8bd5\u589e\u5f3a\u6280\u672f\uff0c\u4e13\u95e8\u9488\u5bf9PR\u4e2d\u672a\u8986\u76d6\u7684\u4ee3\u7801\u884c\u751f\u6210\u6d4b\u8bd5\uff0c\u586b\u8865\"\u6700\u540e\u4e00\u82f1\u91cc\"\u56de\u5f52\u6d4b\u8bd5\u7a7a\u767d", "motivation": "\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u5373\u4f7f\u6709\u5b8c\u5584\u7684\u6d4b\u8bd5\u5957\u4ef6\uff0cPR\u4fee\u6539\u7684\u4ee3\u7801\u884c\u4ecd\u53ef\u80fd\u672a\u88ab\u6d4b\u8bd5\u8986\u76d6\uff0c\u5f62\u6210\"\u6700\u540e\u4e00\u82f1\u91cc\"\u56de\u5f52\u6d4b\u8bd5\u7a7a\u767d\u3002\u73b0\u6709\u6d4b\u8bd5\u751f\u6210\u5668\u901a\u5e38\u5173\u6ce8\u6574\u4f53\u8986\u76d6\u7387\uff0c\u800c\u975e\u4e13\u95e8\u9488\u5bf9PR\u4e2d\u7684\u672a\u8986\u76d6\u884c\u3002", "method": "ChaCo\u91c7\u7528LLM\u751f\u6210\u6d4b\u8bd5\uff0c\u91cd\u70b9\u89e3\u51b3\u4e09\u4e2a\u95ee\u9898\uff1a(1) \u9488\u5bf9PR\u7279\u5b9a\u7684\u8865\u4e01\u8986\u76d6\u7387\uff1b(2) \u63d0\u53d6\u76f8\u5173\u6d4b\u8bd5\u4e0a\u4e0b\u6587\uff08\u73b0\u6709\u6d4b\u8bd5\u51fd\u6570\u3001fixtures\u3001\u6570\u636e\u751f\u6210\u5668\uff09\uff1b(3) \u5c06\u751f\u6210\u7684\u6d4b\u8bd5\u4e0e\u73b0\u6709\u6d4b\u8bd5\u5957\u4ef6\u96c6\u6210\uff0c\u4fdd\u6301\u7ed3\u6784\u548c\u98ce\u683c\u4e00\u81f4\uff0c\u5e76\u751f\u6210\u6d4b\u8bd5\u6dfb\u52a0\u6458\u8981\u4f9b\u5f00\u53d1\u8005\u5ba1\u67e5\u3002", "result": "\u5728\u4e09\u4e2a\u5f00\u6e90\u9879\u76ee\uff08SciPy\u3001Qiskit\u3001Pandas\uff09\u7684145\u4e2aPR\u4e0a\u8bc4\u4f30\uff0c\u5e2e\u52a930%\u7684PR\u5b9e\u73b0\u5b8c\u5168\u8865\u4e01\u8986\u76d6\uff0c\u6210\u672c\u4ec50.11\u7f8e\u5143\u3002\u4eba\u5de5\u8bc4\u5ba1\u8ba4\u4e3a\u6d4b\u8bd5\u503c\u5f97\u6dfb\u52a0\uff084.53/5.0\uff09\u3001\u96c6\u6210\u826f\u597d\uff084.2/5.0\uff09\u3001\u4e0ePR\u76f8\u5173\uff084.7/5.0\uff09\u3002\u63d0\u4ea4\u768412\u4e2a\u6d4b\u8bd5\u4e2d8\u4e2a\u5df2\u88ab\u5408\u5e76\uff0c\u5e76\u53d1\u73b0\u5e76\u4fee\u590d\u4e862\u4e2a\u672a\u77e5bug\u3002", "conclusion": "ChaCo\u80fd\u6709\u6548\u586b\u8865PR\u6d4b\u8bd5\u8986\u76d6\u7684\u7a7a\u767d\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u53ef\u96c6\u6210\u5230CI\u5de5\u4f5c\u6d41\u4e2d\u81ea\u52a8\u5316\u6700\u540e\u4e00\u82f1\u91cc\u56de\u5f52\u6d4b\u8bd5\u589e\u5f3a\u3002", "topic": "swe application"}}
{"id": "2601.10809", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10809", "abs": "https://arxiv.org/abs/2601.10809", "authors": ["Young-Min Cho", "Yuan Yuan", "Sharath Chandra Guntuku", "Lyle Ungar"], "title": "A Concise Agent is Less Expert: Revealing Side Effects of Using Style Features on Conversational Agents", "comment": null, "summary": "Style features such as friendly, helpful, or concise are widely used in prompts to steer the behavior of Large Language Model (LLM) conversational agents, yet their unintended side effects remain poorly understood. In this work, we present the first systematic study of cross-feature stylistic side effects. We conduct a comprehensive survey of 127 conversational agent papers from ACL Anthology and identify 12 frequently used style features. Using controlled, synthetic dialogues across task-oriented and open domain settings, we quantify how prompting for one style feature causally affects others via a pairwise LLM as a Judge evaluation framework. Our results reveal consistent and structured side effects, such as prompting for conciseness significantly reduces perceived expertise. They demonstrate that style features are deeply entangled rather than orthogonal. To support future research, we introduce CASSE (Conversational Agent Stylistic Side Effects), a dataset capturing these complex interactions. We further evaluate prompt based and activation steering based mitigation strategies and find that while they can partially restore suppressed traits, they often degrade the primary intended style. These findings challenge the assumption of faithful style control in LLMs and highlight the need for multi-objective and more principled approaches to safe, targeted stylistic steering in conversational agents.", "AI": {"tldr": "\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86LLM\u5bf9\u8bdd\u4ee3\u7406\u4e2d\u98ce\u683c\u7279\u5f81\u7684\u4ea4\u53c9\u526f\u4f5c\u7528\uff0c\u53d1\u73b0\u98ce\u683c\u7279\u5f81\u4e4b\u95f4\u5b58\u5728\u6df1\u5ea6\u7ea0\u7f20\u800c\u975e\u6b63\u4ea4\uff0c\u63d0\u793a\u4e00\u4e2a\u7279\u5f81\u4f1a\u56e0\u679c\u5f71\u54cd\u5176\u4ed6\u7279\u5f81\uff0c\u6311\u6218\u4e86LLM\u4e2d\u5fe0\u5b9e\u98ce\u683c\u63a7\u5236\u7684\u5047\u8bbe\u3002", "motivation": "LLM\u5bf9\u8bdd\u4ee3\u7406\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u53cb\u597d\u3001\u6709\u5e2e\u52a9\u3001\u7b80\u6d01\u7b49\u98ce\u683c\u7279\u5f81\u6765\u5f15\u5bfc\u884c\u4e3a\uff0c\u4f46\u8fd9\u4e9b\u7279\u5f81\u7684\u610f\u5916\u526f\u4f5c\u7528\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u98ce\u683c\u7279\u5f81\u4e4b\u95f4\u7684\u4ea4\u53c9\u526f\u4f5c\u7528\u3002", "method": "1) \u5bf9ACL Anthology\u4e2d127\u7bc7\u5bf9\u8bdd\u4ee3\u7406\u8bba\u6587\u8fdb\u884c\u8c03\u7814\uff0c\u8bc6\u522b12\u4e2a\u5e38\u7528\u98ce\u683c\u7279\u5f81\uff1b2) \u5728\u4efb\u52a1\u5bfc\u5411\u548c\u5f00\u653e\u57df\u8bbe\u7f6e\u4e2d\u4f7f\u7528\u53d7\u63a7\u5408\u6210\u5bf9\u8bdd\uff1b3) \u901a\u8fc7\u6210\u5bf9LLM\u4f5c\u4e3a\u8bc4\u5224\u6846\u67b6\u91cf\u5316\u63d0\u793a\u4e00\u4e2a\u98ce\u683c\u7279\u5f81\u5982\u4f55\u56e0\u679c\u5f71\u54cd\u5176\u4ed6\u7279\u5f81\uff1b4) \u5f15\u5165CASSE\u6570\u636e\u96c6\uff1b5) \u8bc4\u4f30\u57fa\u4e8e\u63d0\u793a\u548c\u6fc0\u6d3b\u5bfc\u5411\u7684\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u4e00\u81f4\u4e14\u7ed3\u6784\u5316\u7684\u526f\u4f5c\u7528\uff0c\u4f8b\u5982\u63d0\u793a\u7b80\u6d01\u6027\u4f1a\u663e\u8457\u964d\u4f4e\u611f\u77e5\u7684\u4e13\u4e1a\u6027\u3002\u98ce\u683c\u7279\u5f81\u662f\u6df1\u5ea6\u7ea0\u7f20\u800c\u975e\u6b63\u4ea4\u7684\u3002\u7f13\u89e3\u7b56\u7565\u53ef\u4ee5\u90e8\u5206\u6062\u590d\u88ab\u6291\u5236\u7684\u7279\u5f81\uff0c\u4f46\u901a\u5e38\u4f1a\u964d\u4f4e\u4e3b\u8981\u9884\u671f\u98ce\u683c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86LLM\u4e2d\u5fe0\u5b9e\u98ce\u683c\u63a7\u5236\u7684\u5047\u8bbe\uff0c\u5f3a\u8c03\u9700\u8981\u591a\u76ee\u6807\u548c\u66f4\u539f\u5219\u6027\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u5bf9\u8bdd\u4ee3\u7406\u4e2d\u5b89\u5168\u3001\u6709\u9488\u5bf9\u6027\u7684\u98ce\u683c\u5f15\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.11077", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11077", "abs": "https://arxiv.org/abs/2601.11077", "authors": ["Jie Yang", "Honglin Guo", "Li Ji", "Jiazheng Zhou", "Rui Zheng", "Zhikai Lei", "Shuo Zhang", "Zhiheng Xi", "Shichun Liu", "Yuxin Wang", "Bo Wang", "Yining Zheng", "Tao Gui", "Xipeng Qiu"], "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development", "comment": null, "summary": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.", "AI": {"tldr": "ABC-Bench\u662f\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u667a\u80fd\u4f53\u540e\u7aef\u7f16\u7801\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u53ef\u6267\u884c\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5b8c\u6210\u4ece\u4ed3\u5e93\u63a2\u7d22\u5230\u5bb9\u5668\u5316\u670d\u52a1\u90e8\u7f72\u7684\u5b8c\u6574\u5f00\u53d1\u751f\u547d\u5468\u671f\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30\u9759\u6001\u4e0a\u4e0b\u6587\u4e2d\u7684\u4ee3\u7801\u903b\u8f91\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u5de5\u7a0b\uff08\u7279\u522b\u662f\u540e\u7aef\u5f00\u53d1\uff09\u6240\u9700\u7684\u52a8\u6001\u3001\u5168\u8fc7\u7a0b\u8981\u6c42\uff0c\u5305\u62ec\u73af\u5883\u914d\u7f6e\u548c\u670d\u52a1\u90e8\u7f72\u7b49\u590d\u6742\u73af\u8282\u3002", "method": "\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u4ece\u5f00\u6e90\u4ed3\u5e93\u4e2d\u6536\u96c6\u4e86224\u4e2a\u5b9e\u9645\u4efb\u52a1\uff0c\u6db5\u76d68\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c19\u4e2a\u6846\u67b6\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u7ba1\u7406\u4ece\u4ed3\u5e93\u63a2\u7d22\u5230\u5b9e\u4f8b\u5316\u5bb9\u5668\u5316\u670d\u52a1\u7684\u5b8c\u6574\u5f00\u53d1\u751f\u547d\u5468\u671f\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8fd9\u4e9b\u6574\u4f53\u4efb\u52a1\u4e0a\u4e5f\u96be\u4ee5\u63d0\u4f9b\u53ef\u9760\u7684\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u5f53\u524d\u6a21\u578b\u80fd\u529b\u4e0e\u5b9e\u9645\u540e\u7aef\u5de5\u7a0b\u9700\u6c42\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "ABC-Bench\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30\u667a\u80fd\u4f53\u540e\u7aef\u7f16\u7801\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u3001\u5168\u9762\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u63ed\u793a\u4e86\u5f53\u524dAI\u7f16\u7801\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002", "topic": "swe benchmark"}}
{"id": "2601.10825", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10825", "abs": "https://arxiv.org/abs/2601.10825", "authors": ["Junsol Kim", "Shiyang Lai", "Nino Scherrer", "Blaise Ag\u00fcera y Arcas", "James Evans"], "title": "Reasoning Models Generate Societies of Thought", "comment": null, "summary": "Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u5e76\u975e\u4ec5\u6e90\u4e8e\u66f4\u957f\u7684\u601d\u7ef4\u94fe\uff0c\u800c\u662f\u6765\u81ea\u6a21\u62df\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\"\u601d\u60f3\u793e\u4f1a\"\uff0c\u901a\u8fc7\u4e0d\u540c\u4e2a\u6027\u548c\u4e13\u4e1a\u89c6\u89d2\u7684\u591a\u6837\u5316\u4e0e\u8fa9\u8bba\u5b9e\u73b0\u66f4\u4f18\u95ee\u9898\u89e3\u51b3\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u80cc\u540e\u7684\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u4e3b\u8981\u6765\u81ea\u66f4\u957f\u7684\u8ba1\u7b97\u94fe\uff08\u601d\u7ef4\u94fe\uff09\uff0c\u4f46\u672c\u6587\u65e8\u5728\u63a2\u7d22\u662f\u5426\u5b58\u5728\u66f4\u6df1\u5c42\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u673a\u5236\u3002", "method": "\u91c7\u7528\u5b9a\u91cf\u5206\u6790\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5206\u6790\u63a8\u7406\u8f68\u8ff9\uff0c\u7814\u7a76DeepSeek-R1\u548cQwQ-32B\u7b49\u63a8\u7406\u6a21\u578b\u3002\u901a\u8fc7\u53d7\u63a7\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\uff0c\u5956\u52b1\u63a8\u7406\u51c6\u786e\u6027\u89c2\u5bdf\u6a21\u578b\u884c\u4e3a\u53d8\u5316\uff0c\u5e76\u4f7f\u7528\u5bf9\u8bdd\u811a\u624b\u67b6\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u63a8\u7406\u6a21\u578b\u6bd4\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u89c6\u89d2\u591a\u6837\u6027\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6fc0\u6d3b\u66f4\u591a\u5f02\u8d28\u4e2a\u6027\u548c\u4e13\u4e1a\u7279\u5f81\u51b2\u7a81\u3002\u6a21\u578b\u8868\u73b0\u51fa\u95ee\u7b54\u3001\u89c6\u89d2\u8f6c\u6362\u3001\u51b2\u7a81\u89c2\u70b9\u8c03\u548c\u7b49\u5bf9\u8bdd\u884c\u4e3a\uff0c\u4ee5\u53ca\u5c16\u9510\u6765\u56de\u5bf9\u8bdd\u7684\u793e\u4f1a\u60c5\u611f\u89d2\u8272\uff0c\u8fd9\u4e9b\u5171\u540c\u89e3\u91ca\u4e86\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u4f18\u52bf\u3002", "conclusion": "\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u6e90\u4e8e\u601d\u60f3\u7684\u793e\u4f1a\u7ec4\u7ec7\uff0c\u6a21\u62df\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u5b9e\u73b0\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u7684\u66f4\u6709\u6548\u63a2\u7d22\u3002\u8fd9\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7fa4\u4f53\u7684\u96c6\u4f53\u667a\u80fd\uff0c\u591a\u6837\u6027\u5728\u7cfb\u7edf\u7ed3\u6784\u5316\u65f6\u80fd\u5b9e\u73b0\u66f4\u4f18\u95ee\u9898\u89e3\u51b3\uff0c\u4e3a\u667a\u80fd\u4f53\u7ec4\u7ec7\u5229\u7528\u7fa4\u4f53\u667a\u6167\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "topic": "agent analysis"}}
{"id": "2601.10820", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.10820", "abs": "https://arxiv.org/abs/2601.10820", "authors": ["Himanshu Thakur", "Anusha Kamath", "Anurag Muthyala", "Dhwani Sanmukhani", "Smruthi Mukund", "Jay Katukuri"], "title": "Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents", "comment": null, "summary": "Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team's unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team's environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u89c4\u5212\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u7279\u5f81\u5de5\u7a0b\uff0c\u901a\u8fc7\u56fe\u8868\u793a\u56e2\u961f\u73af\u5883\u6765\u534f\u8c03\u667a\u80fd\u4f53\u8c03\u7528\uff0c\u5b9e\u73b0\u4ee3\u7801\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u751f\u6210\u6a21\u578b\u5728\u771f\u5b9eML\u56e2\u961f\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u9650\u5236\uff1a\u7f3a\u4e4f\u8fed\u4ee3\u590d\u6742\u7279\u5f81\u5de5\u7a0b\u8fc7\u7a0b\u7684\u6570\u636e\u96c6\u3001\u73b0\u6709\u4ee3\u7801\u667a\u80fd\u4f53\u4e0e\u56e2\u961f\u7279\u5b9a\u5de5\u5177/\u5de5\u4f5c\u6d41\u96c6\u6210\u4e0d\u8db3\u3001\u4eba\u673a\u534f\u4f5c\u65f6\u673a\u4e0d\u5f53\u5bfc\u81f4\u53cd\u9988\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u89c4\u5212\u5f15\u5bfc\u3001\u7ea6\u675f\u62d3\u6251\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u89c4\u5212\u5668\u5229\u7528\u56e2\u961f\u73af\u5883\u56fe\u534f\u8c03\u53ef\u7528\u667a\u80fd\u4f53\u8c03\u7528\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\uff0c\u5229\u7528\u4e0b\u6e38\u5931\u8d25\u56de\u6eaf\u4fee\u6b63\u4e0a\u6e38\u4ea7\u7269\uff0c\u5e76\u5728\u5173\u952e\u6b65\u9aa4\u8bf7\u6c42\u4eba\u5de5\u5e72\u9884\u3002", "result": "\u5728\u5185\u90e8\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u624b\u52a8\u6784\u5efa\u548c\u65e0\u89c4\u5212\u5de5\u4f5c\u6d41\uff0c\u8bc4\u4f30\u6307\u6807\u5206\u522b\u63d0\u534738%\u548c150%\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u4e3a\u670d\u52a11.2\u4ebf\u7528\u6237\u7684\u63a8\u8350\u6a21\u578b\u6784\u5efa\u7279\u5f81\u65f6\uff0c\u5c06\u7279\u5f81\u5de5\u7a0b\u5468\u671f\u4ece\u4e09\u5468\u7f29\u77ed\u5230\u4e00\u5929\u3002", "conclusion": "\u8be5\u89c4\u5212\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u7279\u5f81\u5de5\u7a0b\u81ea\u52a8\u5316\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u5e76\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u5f71\u54cd\u3002", "topic": "code agent"}}
{"id": "2601.11007", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11007", "abs": "https://arxiv.org/abs/2601.11007", "authors": ["Zhenhua Xu", "Dongsheng Chen", "Shuo Wang", "Jian Li", "Chengjie Wang", "Meng Han", "Yabiao Wang"], "title": "AdaMARP: An Adaptive Multi-Agent Interaction Framework for General Immersive Role-Playing", "comment": null, "summary": "LLM role-playing aims to portray arbitrary characters in interactive narratives, yet existing systems often suffer from limited immersion and adaptability. They typically under-model dynamic environmental information and assume largely static scenes and casts, offering insufficient support for multi-character orchestration, scene transitions, and on-the-fly character introduction. We propose an adaptive multi-agent role-playing framework, AdaMARP, featuring an immersive message format that interleaves [Thought], (Action), <Environment>, and Speech, together with an explicit Scene Manager that governs role-playing through discrete actions (init_scene, pick_speaker, switch_scene, add_role, end) accompanied by rationales. To train these capabilities, we construct AdaRPSet for the Actor Model and AdaSMSet for supervising orchestration decisions, and introduce AdaptiveBench for trajectory-level evaluation. Experiments across multiple backbones and model scales demonstrate consistent improvements: AdaRPSet enhances character consistency, environment grounding, and narrative coherence, with an 8B actor outperforming several commercial LLMs, while AdaSMSet enables smoother scene transitions and more natural role introductions, surpassing Claude Sonnet 4.5 using only a 14B LLM.", "AI": {"tldr": "AdaMARP\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u591a\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\u6846\u67b6\uff0c\u901a\u8fc7\u6c89\u6d78\u5f0f\u6d88\u606f\u683c\u5f0f\u548c\u573a\u666f\u7ba1\u7406\u5668\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u6c89\u6d78\u611f\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u89d2\u8272\u4e00\u81f4\u6027\u3001\u73af\u5883\u57fa\u7840\u548c\u53d9\u4e8b\u8fde\u8d2f\u6027\u65b9\u9762\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709LLM\u89d2\u8272\u626e\u6f14\u7cfb\u7edf\u5b58\u5728\u6c89\u6d78\u611f\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u5e38\u5bf9\u73af\u5883\u52a8\u6001\u4fe1\u606f\u5efa\u6a21\u4e0d\u8db3\uff0c\u5047\u8bbe\u573a\u666f\u548c\u89d2\u8272\u57fa\u672c\u9759\u6001\uff0c\u5bf9\u591a\u89d2\u8272\u7f16\u6392\u3001\u573a\u666f\u8f6c\u6362\u548c\u52a8\u6001\u89d2\u8272\u5f15\u5165\u652f\u6301\u4e0d\u591f\u3002", "method": "\u63d0\u51faAdaMARP\u6846\u67b6\uff0c\u5305\u542b\u6c89\u6d78\u5f0f\u6d88\u606f\u683c\u5f0f\uff08\u4ea4\u7ec7[\u601d\u8003]\u3001<\u52a8\u4f5c>\u3001<\u73af\u5883>\u548c\u5bf9\u8bdd\uff09\u548c\u663e\u5f0f\u573a\u666f\u7ba1\u7406\u5668\uff0c\u901a\u8fc7\u79bb\u6563\u52a8\u4f5c\uff08\u521d\u59cb\u5316\u573a\u666f\u3001\u9009\u62e9\u8bf4\u8bdd\u8005\u3001\u5207\u6362\u573a\u666f\u3001\u6dfb\u52a0\u89d2\u8272\u3001\u7ed3\u675f\uff09\u53ca\u76f8\u5e94\u7406\u7531\u6765\u7ba1\u7406\u89d2\u8272\u626e\u6f14\u3002\u6784\u5efaAdaRPSet\u8bad\u7ec3\u89d2\u8272\u6a21\u578b\u80fd\u529b\uff0cAdaSMSet\u76d1\u7763\u7f16\u6392\u51b3\u7b56\uff0c\u5e76\u5f15\u5165AdaptiveBench\u8fdb\u884c\u8f68\u8ff9\u7ea7\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1aAdaRPSet\u63d0\u5347\u4e86\u89d2\u8272\u4e00\u81f4\u6027\u3001\u73af\u5883\u57fa\u7840\u548c\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0c8B\u53c2\u6570\u7684\u89d2\u8272\u6a21\u578b\u8d85\u8d8a\u591a\u4e2a\u5546\u4e1aLLM\uff1bAdaSMSet\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u7684\u573a\u666f\u8f6c\u6362\u548c\u66f4\u81ea\u7136\u7684\u89d2\u8272\u5f15\u5165\uff0c\u4ec5\u752814B LLM\u5c31\u8d85\u8d8a\u4e86Claude Sonnet 4.5\u3002", "conclusion": "AdaMARP\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u6c89\u6d78\u5f0f\u6d88\u606f\u683c\u5f0f\u548c\u663e\u5f0f\u573a\u666f\u7ba1\u7406\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u89d2\u8272\u626e\u6f14\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u89d2\u8272\u7f16\u6392\u548c\u52a8\u6001\u573a\u666f\u7ba1\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u652f\u6301\u3002", "topic": "agent analysis"}}
{"id": "2601.11037", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11037", "abs": "https://arxiv.org/abs/2601.11037", "authors": ["Shiyu Liu", "Yongjing Yin", "Jianhao Yan", "Yunbo Tang", "Qinggang Zhang", "Bei Li", "Xin Chen", "Jingang Wang", "Xunliang Cai", "Jinsong Su"], "title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search", "comment": "Code is available at https://github.com/Liushiyu-0709/BAPO-Reliable-Search", "summary": "RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.", "AI": {"tldr": "BAPO\u6846\u67b6\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u5956\u52b1\u548c\u81ea\u9002\u5e94\u5956\u52b1\u8c03\u5236\u5668\uff0c\u8ba9\u57fa\u4e8eRL\u7684\u667a\u80fd\u4f53\u5728\u8bc1\u636e\u4e0d\u8db3\u65f6\u5b66\u4f1a\u8bf4\"\u6211\u4e0d\u77e5\u9053\"\uff0c\u63d0\u9ad8\u53ef\u9760\u6027\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u641c\u7d22\u867d\u7136\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9760\u6027\u8fb9\u754c\u610f\u8bc6\uff0c\u5373\u4f7f\u5728\u8bc1\u636e\u4e0d\u8db3\u6216\u63a8\u7406\u8fbe\u5230\u6781\u9650\u65f6\u4e5f\u5f88\u5c11\u627f\u8ba4\"\u6211\u4e0d\u77e5\u9053\"\uff0c\u5bfc\u81f4\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u4f46\u4e0d\u53ef\u9760\u7684\u7b54\u6848\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5b58\u5728\u91cd\u5927\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u8fb9\u754c\u611f\u77e5\u7b56\u7565\u4f18\u5316(BAPO)\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1)\u57fa\u4e8e\u7fa4\u4f53\u7684\u8fb9\u754c\u611f\u77e5\u5956\u52b1\uff0c\u9f13\u52b1\u5728\u63a8\u7406\u8fbe\u5230\u6781\u9650\u65f6\u7ed9\u51faIDK\u54cd\u5e94\uff1b(2)\u81ea\u9002\u5e94\u5956\u52b1\u8c03\u5236\u5668\uff0c\u5728\u65e9\u671f\u63a2\u7d22\u9636\u6bb5\u7b56\u7565\u6027\u5730\u6682\u505c\u8be5\u5956\u52b1\uff0c\u9632\u6b62\u6a21\u578b\u5c06IDK\u4f5c\u4e3a\u6377\u5f84\u5229\u7528\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cBAPO\u663e\u8457\u589e\u5f3a\u4e86\u667a\u80fd\u4f53\u641c\u7d22\u7684\u6574\u4f53\u53ef\u9760\u6027\u3002", "conclusion": "BAPO\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u635f\u5bb3\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u57f9\u517b\u53ef\u9760\u7684\u8fb9\u754c\u610f\u8bc6\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u667a\u80fd\u4f53\u641c\u7d22\u4e2d\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u5173\u952e\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.11044", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11044", "abs": "https://arxiv.org/abs/2601.11044", "authors": ["Keyu Li", "Junhao Shi", "Yang Xiao", "Mohan Jiang", "Jie Sun", "Yunze Wu", "Shijie Xia", "Xiaojie Cai", "Tianze Xu", "Weiye Si", "Wenjie Li", "Dequan Wang", "Pengfei Liu"], "title": "AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts", "comment": null, "summary": "Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.", "AI": {"tldr": "AgencyBench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f306\u79cd\u6838\u5fc3\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u5305\u542b32\u4e2a\u771f\u5b9e\u573a\u666f\u548c138\u4e2a\u4efb\u52a1\uff0c\u901a\u8fc7\u7528\u6237\u6a21\u62df\u4ee3\u7406\u548cDocker\u6c99\u7bb1\u5b9e\u73b0\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u65e0\u6cd5\u6355\u6349\u957f\u89c6\u91ce\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\uff0c\u4e14\u4f9d\u8d56\u4eba\u5de5\u53cd\u9988\u9020\u6210\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u963b\u788d\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u5305\u542b32\u4e2a\u771f\u5b9e\u573a\u666f\u3001138\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u7528\u6237\u6a21\u62df\u4ee3\u7406\u63d0\u4f9b\u8fed\u4ee3\u53cd\u9988\uff0cDocker\u6c99\u7bb1\u8fdb\u884c\u89c6\u89c9\u548c\u529f\u80fd\u8bc4\u4f30\uff0c\u6db5\u76d66\u79cd\u6838\u5fc3\u667a\u80fd\u4f53\u80fd\u529b\u3002", "result": "\u95ed\u6e90\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0848.4% vs 32.1%\uff09\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u8d44\u6e90\u6548\u7387\u3001\u53cd\u9988\u9a71\u52a8\u81ea\u6821\u6b63\u548c\u7279\u5b9a\u5de5\u5177\u4f7f\u7528\u504f\u597d\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e13\u6709\u6a21\u578b\u5728\u5176\u539f\u751f\u751f\u6001\u7cfb\u7edf\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "AgencyBench\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u4f53\u63d0\u4f9b\u5173\u952e\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5f3a\u8c03\u9700\u8981\u5171\u540c\u4f18\u5316\u6a21\u578b\u67b6\u6784\u4e0e\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u672a\u6765\u53d1\u5c55\u6307\u660e\u65b9\u5411\u3002", "topic": "agent benchmark"}}
{"id": "2601.11100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11100", "abs": "https://arxiv.org/abs/2601.11100", "authors": ["Zhezheng Hao", "Hong Wang", "Jian Luo", "Jianqing Zhang", "Yuyan Zhou", "Qiang Lin", "Can Wang", "Hande Dong", "Jiawei Chen"], "title": "ReCreate: Reasoning and Creating Domain Agents Driven by Experience", "comment": null, "summary": "Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.", "AI": {"tldr": "ReCreate\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ecf\u9a8c\u9a71\u52a8\u7684\u81ea\u52a8\u521b\u5efa\u9886\u57df\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u667a\u80fd\u4f53\u4ea4\u4e92\u5386\u53f2\u6765\u4f18\u5316\u667a\u80fd\u4f53\u8bbe\u8ba1\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8d85\u8d8a\u4eba\u5de5\u8bbe\u8ba1\u667a\u80fd\u4f53\u548c\u73b0\u6709\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u5b9e\u7528\u667a\u80fd\u4f53\u4ecd\u7531\u4eba\u5de5\u8bbe\u8ba1\uff0c\u56e0\u4e3a\u4efb\u52a1\u5dee\u5f02\u5927\u4e14\u6784\u5efa\u6210\u672c\u9ad8\u3002\u73b0\u6709\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\u5c06\u667a\u80fd\u4f53\u751f\u6210\u89c6\u4e3a\u9ed1\u76d2\u8fc7\u7a0b\uff0c\u4ec5\u4f9d\u8d56\u6700\u7ec8\u6027\u80fd\u6307\u6807\uff0c\u5ffd\u7565\u4e86\u6210\u529f/\u5931\u8d25\u7684\u5173\u952e\u8bc1\u636e\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faReCreate\u6846\u67b6\uff0c\u91c7\u7528\u667a\u80fd\u4f53\u5373\u4f18\u5316\u5668\u8303\u5f0f\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u7ecf\u9a8c\u5b58\u50a8\u4e0e\u68c0\u7d22\u673a\u5236\uff1b2) \u63a8\u7406-\u521b\u5efa\u534f\u540c\u7ba1\u9053\uff0c\u5c06\u6267\u884c\u7ecf\u9a8c\u6620\u5c04\u4e3a\u811a\u624b\u67b6\u7f16\u8f91\uff1b3) \u5206\u5c42\u66f4\u65b0\uff0c\u5c06\u5b9e\u4f8b\u7ea7\u7ec6\u8282\u62bd\u8c61\u4e3a\u53ef\u91cd\u7528\u9886\u57df\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cReCreate\u59cb\u7ec8\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u667a\u80fd\u4f53\u548c\u73b0\u6709\u81ea\u52a8\u667a\u80fd\u4f53\u751f\u6210\u65b9\u6cd5\uff0c\u5373\u4f7f\u4ece\u6700\u5c0f\u79cd\u5b50\u811a\u624b\u67b6\u5f00\u59cb\u4e5f\u80fd\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "conclusion": "ReCreate\u901a\u8fc7\u7cfb\u7edf\u5229\u7528\u667a\u80fd\u4f53\u4ea4\u4e92\u5386\u53f2\u4e2d\u7684\u5177\u4f53\u4fe1\u53f7\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u52a8\u521b\u5efa\u548c\u9002\u5e94\u9886\u57df\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u5173\u952e\u8bc1\u636e\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.11000", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11000", "abs": "https://arxiv.org/abs/2601.11000", "authors": ["Zhongxiang Sun", "Yi Zhan", "Chenglei Shen", "Weijie Yu", "Xiao Zhang", "Ming He", "Jun Xu"], "title": "When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs", "comment": "20 pages, 15 figures", "summary": "Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFPPS\u65b9\u6cd5\u89e3\u51b3\u4e2a\u6027\u5316LLMs\u5728\u4e8b\u5b9e\u67e5\u8be2\u65f6\u4ea7\u751f\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e2a\u6027\u5316\u6027\u80fd\uff0c\u5e76\u521b\u5efa\u4e86PFQABench\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u4e2a\u6027\u5316LLMs\u5728\u589e\u5f3a\u7528\u6237\u4f53\u9a8c\u7684\u540c\u65f6\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e8b\u5b9e\u63a8\u7406\u7684\u626d\u66f2\uff0c\u4ea7\u751f\"\u4e2a\u6027\u5316\u8bf1\u5bfc\u5e7b\u89c9\"\uff0c\u5373\u6a21\u578b\u6839\u636e\u7528\u6237\u5386\u53f2\u800c\u975e\u5ba2\u89c2\u4e8b\u5b9e\u751f\u6210\u7b54\u6848\uff0c\u8fd9\u4f1a\u964d\u4f4e\u4e8b\u5b9e\u53ef\u9760\u6027\u5e76\u4f20\u64ad\u9519\u8bef\u4fe1\u5ff5\u3002", "method": "\u63d0\u51faFactuality-Preserving Personalized Steering (FPPS)\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u4e2a\u6027\u5316\u8868\u793a\u548c\u4e8b\u5b9e\u8868\u793a\u6765\u51cf\u8f7b\u4e2a\u6027\u5316\u5bfc\u81f4\u7684\u4e8b\u5b9e\u626d\u66f2\uff0c\u540c\u65f6\u4fdd\u6301\u4e2a\u6027\u5316\u884c\u4e3a\u3002", "result": "\u5728\u591a\u4e2aLLM\u4e3b\u5e72\u7f51\u7edc\u548c\u4e2a\u6027\u5316\u65b9\u6cd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFPPS\u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e2a\u6027\u5316\u6027\u80fd\u3002", "conclusion": "FPPS\u80fd\u6709\u6548\u7f13\u89e3\u4e2a\u6027\u5316LLMs\u4e2d\u7684\u4e8b\u5b9e\u626d\u66f2\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4e2a\u6027\u5316\u4f18\u52bf\u7684\u540c\u65f6\u63d0\u5347\u4e8b\u5b9e\u53ef\u9760\u6027\uff0c\u4e3a\u89e3\u51b3\u4e2a\u6027\u5316\u4e0e\u4e8b\u5b9e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.11189", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11189", "abs": "https://arxiv.org/abs/2601.11189", "authors": ["Sofiene Lassoued", "Asrat Gobachew", "Stefan Lier", "Andreas Schwung"], "title": "Policy-Based Deep Reinforcement Learning Hyperheuristics for Job-Shop Scheduling Problems", "comment": null, "summary": "This paper proposes a policy-based deep reinforcement learning hyper-heuristic framework for solving the Job Shop Scheduling Problem. The hyper-heuristic agent learns to switch scheduling rules based on the system state dynamically. We extend the hyper-heuristic framework with two key mechanisms. First, action prefiltering restricts decision-making to feasible low-level actions, enabling low-level heuristics to be evaluated independently of environmental constraints and providing an unbiased assessment. Second, a commitment mechanism regulates the frequency of heuristic switching. We investigate the impact of different commitment strategies, from step-wise switching to full-episode commitment, on both training behavior and makespan. Additionally, we compare two action selection strategies at the policy level: deterministic greedy selection and stochastic sampling. Computational experiments on standard JSSP benchmarks demonstrate that the proposed approach outperforms traditional heuristics, metaheuristics, and recent neural network-based scheduling methods", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7b56\u7565\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8d85\u542f\u53d1\u5f0f\u6846\u67b6\u89e3\u51b3\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u4f5c\u9884\u8fc7\u6ee4\u548c\u627f\u8bfa\u673a\u5236\u6539\u8fdb\u51b3\u7b56\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\uff08JSSP\uff09\u662f\u7ecf\u5178\u7684NP\u96be\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u7cfb\u7edf\u72b6\u6001\u53d8\u5316\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6839\u636e\u7cfb\u7edf\u72b6\u6001\u52a8\u6001\u5207\u6362\u8c03\u5ea6\u89c4\u5219\u7684\u667a\u80fd\u8d85\u542f\u53d1\u5f0f\u6846\u67b6", "method": "\u63d0\u51fa\u57fa\u4e8e\u7b56\u7565\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8d85\u542f\u53d1\u5f0f\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u673a\u5236\uff1a1) \u52a8\u4f5c\u9884\u8fc7\u6ee4\uff0c\u5c06\u51b3\u7b56\u9650\u5236\u5728\u53ef\u884c\u7684\u4f4e\u5c42\u52a8\u4f5c\uff0c\u4f7f\u4f4e\u5c42\u542f\u53d1\u5f0f\u72ec\u7acb\u4e8e\u73af\u5883\u7ea6\u675f\u8fdb\u884c\u8bc4\u4f30\uff1b2) \u627f\u8bfa\u673a\u5236\uff0c\u8c03\u8282\u542f\u53d1\u5f0f\u5207\u6362\u9891\u7387\u3002\u7814\u7a76\u4e86\u4ece\u9010\u6b65\u5207\u6362\u5230\u5b8c\u6574\u5468\u671f\u627f\u8bfa\u7684\u4e0d\u540c\u7b56\u7565\uff0c\u5e76\u6bd4\u8f83\u4e86\u786e\u5b9a\u6027\u8d2a\u5a6a\u9009\u62e9\u548c\u968f\u673a\u91c7\u6837\u4e24\u79cd\u52a8\u4f5c\u9009\u62e9\u7b56\u7565", "result": "\u5728\u6807\u51c6JSSP\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8ba1\u7b97\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u3001\u5143\u542f\u53d1\u5f0f\u548c\u6700\u8fd1\u7684\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u8c03\u5ea6\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8d85\u542f\u53d1\u5f0f\u6846\u67b6\u901a\u8fc7\u52a8\u4f5c\u9884\u8fc7\u6ee4\u548c\u627f\u8bfa\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86JSSP\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5728\u52a8\u6001\u8c03\u5ea6\u51b3\u7b56\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2601.11252", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11252", "abs": "https://arxiv.org/abs/2601.11252", "authors": ["Qianyue Wang", "Jinwu Hu", "Yufeng Wang", "Huanxiang Lin", "Bolin Chen", "Zhiquan Wen", "Yaofo Chen", "Mingkui Tan"], "title": "Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning", "comment": null, "summary": "Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.", "AI": {"tldr": "\u63d0\u51faThink-with-Me\u4ea4\u4e92\u5f0f\u63a8\u7406\u8303\u5f0f\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5f15\u5165\u5916\u90e8\u53cd\u9988\u5e72\u9884\uff0c\u901a\u8fc7\u6682\u505c\u5728\u8fc7\u6e21\u8fde\u8bcd\u5904\u8fdb\u884c\u53cd\u9988\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u63a8\u7406\u957f\u5ea6\uff0c\u5728\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e0b\u5b9e\u73b0\u51c6\u786e\u7387\u4e0e\u63a8\u7406\u957f\u5ea6\u7684\u66f4\u597d\u5e73\u8861\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u548c\u63a8\u7406\u504f\u79fb\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u548c\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\u7f3a\u4e4f\u5916\u90e8\u5e72\u9884\u673a\u5236\u6765\u5f15\u5bfc\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faThink-with-Me\u6d4b\u8bd5\u65f6\u4ea4\u4e92\u63a8\u7406\u8303\u5f0f\uff1a1\uff09\u5728\u8fc7\u6e21\u8fde\u8bcd\u5904\u6682\u505c\u63a8\u7406\u8fdb\u884c\u5916\u90e8\u53cd\u9988\uff1b2\uff09\u4f7f\u7528\u591a\u6807\u51c6\u8bc4\u4f30\uff08\u5408\u7406\u6027\u548c\u5b8c\u6574\u6027\uff09\u751f\u6210\u53cd\u9988\uff1b3\uff09\u901a\u8fc7Group Relative Policy Optimization\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u4ea4\u4e92\u6a21\u5f0f\u3002", "result": "\u5728AIME24\u4e0a\uff0cThink-with-Me\u57288K\u7a97\u53e3\u4e0b\u6bd4QwQ-32B\u51c6\u786e\u7387\u63d0\u9ad87.19%\uff0c\u540c\u65f6\u5e73\u5747\u63a8\u7406\u957f\u5ea6\u51cf\u5c1181%\u3002\u8be5\u8303\u5f0f\u5728\u5b89\u5168\u548c\u521b\u610f\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "conclusion": "Think-with-Me\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u53cd\u9988\u5e72\u9884\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u95ee\u9898\uff0c\u5728\u6709\u9650\u4e0a\u4e0b\u6587\u4e0b\u5b9e\u73b0\u4e86\u51c6\u786e\u7387\u4e0e\u63a8\u7406\u6548\u7387\u7684\u66f4\u597d\u5e73\u8861\u3002", "topic": "agent analysis"}}
{"id": "2601.10973", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.10973", "abs": "https://arxiv.org/abs/2601.10973", "authors": ["Zain ul Abdeen", "Waris Gill", "Ming Jin"], "title": "Toward Adaptive Grid Resilience: A Gradient-Free Meta-RL Framework for Critical Load Restoration", "comment": null, "summary": "Restoring critical loads after extreme events demands adaptive control to maintain distribution-grid resilience, yet uncertainty in renewable generation, limited dispatchable resources, and nonlinear dynamics make effective restoration difficult. Reinforcement learning (RL) can optimize sequential decisions under uncertainty, but standard RL often generalizes poorly and requires extensive retraining for new outage configurations or generation patterns. We propose a meta-guided gradient-free RL (MGF-RL) framework that learns a transferable initialization from historical outage experiences and rapidly adapts to unseen scenarios with minimal task-specific tuning. MGF-RL couples first-order meta-learning with evolutionary strategies, enabling scalable policy search without gradient computation while accommodating nonlinear, constrained distribution-system dynamics. Experiments on IEEE 13-bus and IEEE 123-bus test systems show that MGF-RL outperforms standard RL, MAML-based meta-RL, and model predictive control across reliability, restoration speed, and adaptation efficiency under renewable forecast errors. MGF-RL generalizes to unseen outages and renewable patterns while requiring substantially fewer fine-tuning episodes than conventional RL. We also provide sublinear regret bounds that relate adaptation efficiency to task similarity and environmental variation, supporting the empirical gains and motivating MGF-RL for real-time load restoration in renewable-rich distribution grids.", "AI": {"tldr": "\u63d0\u51faMGF-RL\u6846\u67b6\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u8fdb\u5316\u7b56\u7565\uff0c\u7528\u4e8e\u914d\u7535\u7f51\u6545\u969c\u6062\u590d\uff0c\u80fd\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u975e\u7ebf\u6027\u7ea6\u675f\u4e0b\u5feb\u901f\u9002\u5e94\u65b0\u573a\u666f\u3002", "motivation": "\u6781\u7aef\u4e8b\u4ef6\u540e\u6062\u590d\u5173\u952e\u8d1f\u8377\u9700\u8981\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u4f46\u53ef\u518d\u751f\u80fd\u6e90\u4e0d\u786e\u5b9a\u6027\u3001\u53ef\u8c03\u5ea6\u8d44\u6e90\u6709\u9650\u548c\u975e\u7ebf\u6027\u52a8\u6001\u4f7f\u5f97\u6709\u6548\u6062\u590d\u56f0\u96be\u3002\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9700\u8981\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u5143\u5f15\u5bfc\u65e0\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60(MGF-RL)\u6846\u67b6\uff0c\u7ed3\u5408\u4e00\u9636\u5143\u5b66\u4e60\u548c\u8fdb\u5316\u7b56\u7565\uff0c\u4ece\u5386\u53f2\u6545\u969c\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u53ef\u8fc1\u79fb\u521d\u59cb\u5316\uff0c\u65e0\u9700\u68af\u5ea6\u8ba1\u7b97\u5373\u53ef\u5feb\u901f\u9002\u5e94\u65b0\u573a\u666f\u3002", "result": "\u5728IEEE 13\u603b\u7ebf\u548c123\u603b\u7ebf\u6d4b\u8bd5\u7cfb\u7edf\u4e2d\uff0cMGF-RL\u5728\u53ef\u9760\u6027\u3001\u6062\u590d\u901f\u5ea6\u548c\u9002\u5e94\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6807\u51c6RL\u3001\u57fa\u4e8eMAML\u7684\u5143RL\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u6cdb\u5316\u5230\u672a\u89c1\u6545\u969c\u548c\u53ef\u518d\u751f\u80fd\u6e90\u6a21\u5f0f\uff0c\u9700\u8981\u66f4\u5c11\u7684\u5fae\u8c03\u3002", "conclusion": "MGF-RL\u4e3a\u53ef\u518d\u751f\u80fd\u6e90\u4e30\u5bcc\u7684\u914d\u7535\u7f51\u5b9e\u65f6\u8d1f\u8377\u6062\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u4f18\u52bf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.10987", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10987", "abs": "https://arxiv.org/abs/2601.10987", "authors": ["Aanand Balasubramanian", "Sashank Silwal"], "title": "Reasoning Distillation for Lightweight Automated Program Repair", "comment": "8 pages, 5 tables. Preprint", "summary": "We study whether lightweight symbolic reasoning supervision can improve fix type classification in compact automated program repair models. Small code models are attractive for resource-constrained settings, but they typically produce only a single prediction, making it unclear whether they learn meaningful program structure or rely on shallow correlations. We propose a reasoning distillation approach in which a large teacher model provides structured symbolic reasoning tags alongside fix-type labels. These tags capture high-level causal properties of bugs without relying on free-form explanations. We train a CodeT5-based student model under label-only and reasoning-distilled settings on the IntroClass benchmark. Reasoning supervision consistently improves macro averaged performance, particularly on less frequent bug categories, without increasing model size or complexity. We further analyze the relationship between reasoning accuracy and fix-type prediction, showing that correct reasoning traces strongly correlate with correct predictions, while not fully determining them. Our results suggest that symbolic reasoning distillation is a practical way to improve interpretability and robustness in lightweight program repair models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7b26\u53f7\u63a8\u7406\u76d1\u7763\u63d0\u5347\u7d27\u51d1\u578b\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u6a21\u578b\u7684\u4fee\u590d\u7c7b\u578b\u5206\u7c7b\u80fd\u529b\uff0c\u63d0\u51fa\u63a8\u7406\u84b8\u998f\u65b9\u6cd5\u8ba9\u5927\u6a21\u578b\u63d0\u4f9b\u7ed3\u6784\u5316\u7b26\u53f7\u63a8\u7406\u6807\u7b7e\uff0c\u5728CodeT5\u5b66\u751f\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u63d0\u5347\u6027\u80fd\u7279\u522b\u662f\u4f4e\u9891\u9519\u8bef\u7c7b\u522b\u3002", "motivation": "\u5c0f\u578b\u4ee3\u7801\u6a21\u578b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff0c\u4f46\u901a\u5e38\u53ea\u4ea7\u751f\u5355\u4e00\u9884\u6d4b\uff0c\u4e0d\u6e05\u695a\u5b83\u4eec\u662f\u5426\u5b66\u4e60\u6709\u610f\u4e49\u7684\u7a0b\u5e8f\u7ed3\u6784\u8fd8\u662f\u4f9d\u8d56\u6d45\u5c42\u76f8\u5173\u6027\u3002\u9700\u8981\u63d0\u5347\u8f7b\u91cf\u7ea7\u7a0b\u5e8f\u4fee\u590d\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u84b8\u998f\u65b9\u6cd5\uff1a\u5927\u578b\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u7ed3\u6784\u5316\u7b26\u53f7\u63a8\u7406\u6807\u7b7e\u548c\u4fee\u590d\u7c7b\u578b\u6807\u7b7e\uff0c\u8fd9\u4e9b\u6807\u7b7e\u6355\u83b7\u9519\u8bef\u7684\u9ad8\u5c42\u56e0\u679c\u5c5e\u6027\u800c\u4e0d\u4f9d\u8d56\u81ea\u7531\u5f62\u5f0f\u89e3\u91ca\u3002\u5728IntroClass\u57fa\u51c6\u4e0a\u8bad\u7ec3CodeT5\u5b66\u751f\u6a21\u578b\uff0c\u6bd4\u8f83\u6807\u7b7e\u76d1\u7763\u548c\u63a8\u7406\u84b8\u998f\u4e24\u79cd\u8bbe\u7f6e\u3002", "result": "\u63a8\u7406\u76d1\u7763\u6301\u7eed\u63d0\u5347\u5b8f\u5e73\u5747\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8f83\u5c11\u51fa\u73b0\u7684\u9519\u8bef\u7c7b\u522b\u4e0a\uff0c\u4e14\u4e0d\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u6216\u590d\u6742\u5ea6\u3002\u6b63\u786e\u63a8\u7406\u8f68\u8ff9\u4e0e\u6b63\u786e\u9884\u6d4b\u5f3a\u76f8\u5173\uff0c\u4f46\u4e0d\u5b8c\u5168\u51b3\u5b9a\u9884\u6d4b\u7ed3\u679c\u3002", "conclusion": "\u7b26\u53f7\u63a8\u7406\u84b8\u998f\u662f\u63d0\u5347\u8f7b\u91cf\u7ea7\u7a0b\u5e8f\u4fee\u590d\u6a21\u578b\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u80fd\u5e2e\u52a9\u5c0f\u578b\u6a21\u578b\u5b66\u4e60\u66f4\u6709\u610f\u4e49\u7684\u7a0b\u5e8f\u7ed3\u6784\u800c\u975e\u4f9d\u8d56\u6d45\u5c42\u76f8\u5173\u6027\u3002", "topic": "code agent"}}
{"id": "2601.11354", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11354", "abs": "https://arxiv.org/abs/2601.11354", "authors": ["Weiyi Wang", "Xinchi Chen", "Jingjing Gong", "Xuanjing Huang", "Xipeng Qiu"], "title": "AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems", "comment": null, "summary": "Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.", "AI": {"tldr": "AstroReason-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u7a7a\u95f4\u89c4\u5212\u95ee\u9898\u4e2d\u89c4\u5212\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u57fa\u51c6\u6574\u5408\u4e86\u591a\u79cd\u8c03\u5ea6\u673a\u5236\uff0c\u53d1\u73b0\u5f53\u524d\u667a\u80fd\u4f53\u5728\u7269\u7406\u7ea6\u675f\u4e0b\u7684\u771f\u5b9e\u4e16\u754c\u89c4\u5212\u4e2d\u8868\u73b0\u8fdc\u4e0d\u5982\u4e13\u7528\u6c42\u89e3\u5668\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u7b26\u53f7\u5316\u6216\u5f31\u63a5\u5730\u73af\u5883\uff0c\u800c\u5728\u7269\u7406\u7ea6\u675f\u7684\u771f\u5b9e\u4e16\u754c\u9886\u57df\u4e2d\u7684\u6027\u80fd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u7a7a\u95f4\u89c4\u5212\u95ee\u9898\u5177\u6709\u5f02\u6784\u76ee\u6807\u3001\u4e25\u683c\u7269\u7406\u7ea6\u675f\u548c\u957f\u89c6\u91ce\u51b3\u7b56\u7b49\u7279\u70b9\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5f15\u5165AstroReason-Bench\u57fa\u51c6\uff0c\u6574\u5408\u5730\u9762\u7ad9\u901a\u4fe1\u548c\u654f\u6377\u5730\u7403\u89c2\u6d4b\u7b49\u591a\u79cd\u8c03\u5ea6\u673a\u5236\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u667a\u80fd\u4f53\u5bfc\u5411\u4ea4\u4e92\u534f\u8bae\uff0c\u5e76\u5728\u591a\u79cd\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5f53\u524d\u667a\u80fd\u4f53\u5728\u7a7a\u95f4\u89c4\u5212\u95ee\u9898\u4e2d\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4e13\u7528\u6c42\u89e3\u5668\uff0c\u63ed\u793a\u4e86\u901a\u7528\u89c4\u5212\u5668\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "conclusion": "AstroReason-Bench\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u548c\u8bca\u65ad\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u667a\u80fd\u4f53\u5728\u7269\u7406\u7ea6\u675f\u771f\u5b9e\u4e16\u754c\u89c4\u5212\u80fd\u529b\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2601.11061", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11061", "abs": "https://arxiv.org/abs/2601.11061", "authors": ["Lecheng Yan", "Ruizhe Li", "Guanhua Chen", "Qing Li", "Jiahui Geng", "Wenxi Li", "Vincent Wang", "Chris Lee"], "title": "Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs", "comment": "Work in process", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a \"Perplexity Paradox\": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0RLVR\u8bad\u7ec3\u4e2d\u5b58\u5728\u7684\"\u56f0\u60d1\u5ea6\u6096\u8bba\"\uff1a\u865a\u5047\u5956\u52b1\u5bfc\u81f4\u6a21\u578b\u7ed5\u8fc7\u63a8\u7406\uff0c\u901a\u8fc7\u8bb0\u5fc6\u6377\u5f84\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u951a\u70b9-\u9002\u914d\u5668\u7535\u8def\u673a\u5236", "motivation": "\u5c3d\u7ba1RLVR\u80fd\u6709\u6548\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd1\u671f\u8bc1\u636e\u663e\u793a\u5373\u4f7f\u4f7f\u7528\u865a\u5047\u6216\u4e0d\u6b63\u786e\u7684\u5956\u52b1\uff0c\u6a21\u578b\u4e5f\u80fd\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7a76\u8fd9\u4e00\u73b0\u8c61\u80cc\u540e\u7684\u673a\u5236\uff0c\u7406\u89e3\u6a21\u578b\u5982\u4f55\u7ed5\u8fc7\u63a8\u7406\u8fc7\u7a0b", "method": "\u4f7f\u7528\u8def\u5f84\u4fee\u8865\u3001Logit Lens\u3001JSD\u5206\u6790\u548c\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\u7b49\u6280\u672f\uff0c\u8bc6\u522b\u51fa\u9690\u85cf\u7684\u951a\u70b9-\u9002\u914d\u5668\u7535\u8def\u3002\u8be5\u7535\u8def\u5305\u542b\u4e2d\u95f4\u5c42(L18-20)\u7684\u529f\u80fd\u951a\u70b9\u548c\u540e\u7eed\u5c42(L21+)\u7684\u7ed3\u6784\u9002\u914d\u5668", "result": "\u53d1\u73b0\u4e86\"\u56f0\u60d1\u5ea6\u6096\u8bba\"\uff1a\u865a\u5047RLVR\u5bfc\u81f4\u7b54\u6848\u6807\u8bb0\u56f0\u60d1\u5ea6\u4e0b\u964d\u4f46\u63d0\u793a\u4fa7\u8fde\u8d2f\u6027\u9000\u5316\u3002\u63ed\u793a\u4e86\u6a21\u578b\u901a\u8fc7\u8bb0\u5fc6\u6377\u5f84\u800c\u975e\u63a8\u7406\u6765\u63d0\u5347\u6027\u80fd\u7684\u673a\u5236\uff0c\u5e76\u8bc1\u660e\u53ef\u4ee5\u901a\u8fc7\u7f29\u653e\u7279\u5b9aMLP\u952e\u6765\u53cc\u5411\u56e0\u679c\u63a7\u5236\u6c61\u67d3\u9a71\u52a8\u7684\u6027\u80fd", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u8bc6\u522b\u548c\u7f13\u89e3RLVR\u8c03\u4f18\u6a21\u578b\u4e2d\u6570\u636e\u6c61\u67d3\u7684\u673a\u5236\u8def\u7ebf\u56fe\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u8bb0\u5fc6\u6377\u5f84\u7ed5\u8fc7\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e3a\u7406\u89e3\u548c\u63a7\u5236\u8fd9\u4e00\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840", "topic": "agent analysis"}}
{"id": "2601.11227", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.11227", "abs": "https://arxiv.org/abs/2601.11227", "authors": ["Shaoyang Xu", "Wenxuan Zhang"], "title": "Language of Thought Shapes Output Diversity in Large Language Models", "comment": null, "summary": "Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.", "AI": {"tldr": "\u901a\u8fc7\u63a7\u5236\u5927\u8bed\u8a00\u6a21\u578b\u7684\"\u601d\u7ef4\u8bed\u8a00\"\uff08\u601d\u8003\u65f6\u4f7f\u7528\u7684\u8bed\u8a00\uff09\u53ef\u4ee5\u663e\u8457\u589e\u52a0\u8f93\u51fa\u591a\u6837\u6027\uff0c\u975e\u82f1\u8bed\u601d\u7ef4\u8bed\u8a00\u6bd4\u82f1\u8bed\u4ea7\u751f\u66f4\u591a\u6837\u5316\u8f93\u51fa\uff0c\u4e14\u8bed\u8a00\u8ddd\u79bb\u82f1\u8bed\u8d8a\u8fdc\u591a\u6837\u6027\u589e\u76ca\u8d8a\u5927\u3002", "motivation": "\u8f93\u51fa\u591a\u6837\u6027\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u652f\u6301\u591a\u5143\u4e3b\u4e49\u548c\u521b\u9020\u529b\u3002\u5f53\u524d\u7814\u7a76\u63ed\u793a\u63a7\u5236\u6a21\u578b\u601d\u8003\u65f6\u4f7f\u7528\u7684\u8bed\u8a00\uff08\u601d\u7ef4\u8bed\u8a00\uff09\u662f\u8f93\u51fa\u591a\u6837\u6027\u7684\u65b0\u9896\u7ed3\u6784\u5316\u6765\u6e90\u3002", "method": "\u7814\u7a76\u4e0d\u540c\u601d\u7ef4\u8bed\u8a00\u5728\u6a21\u578b\u601d\u8003\u7a7a\u95f4\u4e2d\u7684\u5206\u5e03\uff0c\u8bc4\u4f30\u4e24\u79cd\u91cd\u590d\u91c7\u6837\u7b56\u7565\uff1a\u5355\u8bed\u8a00\u91c7\u6837\u548c\u6df7\u5408\u8bed\u8a00\u91c7\u6837\u3002\u901a\u8fc7\u63a7\u5236\u8f93\u51fa\u4e3a\u82f1\u8bed\uff0c\u4f46\u6539\u53d8\u601d\u7ef4\u8bed\u8a00\uff0c\u8fdb\u884c\u5e7f\u6cdb\u7684\u591a\u6837\u6027\u8bc4\u4f30\u5b9e\u9a8c\u3002", "result": "\u5c06\u601d\u7ef4\u8bed\u8a00\u4ece\u82f1\u8bed\u5207\u6362\u5230\u975e\u82f1\u8bed\u8bed\u8a00\u80fd\u6301\u7eed\u589e\u52a0\u8f93\u51fa\u591a\u6837\u6027\uff0c\u4e14\u4e0e\u82f1\u8bed\u5728\u601d\u8003\u7a7a\u95f4\u4e2d\u8ddd\u79bb\u8d8a\u8fdc\u7684\u8bed\u8a00\u5e26\u6765\u66f4\u5927\u7684\u591a\u6837\u6027\u589e\u76ca\u3002\u805a\u5408\u591a\u4e2a\u601d\u7ef4\u8bed\u8a00\u7684\u6837\u672c\u901a\u8fc7\u7ec4\u5408\u6548\u5e94\u4ea7\u751f\u989d\u5916\u6539\u8fdb\uff0c\u8bed\u8a00\u5f02\u8d28\u6027\u6269\u5c55\u4e86\u6a21\u578b\u7684\u591a\u6837\u6027\u4e0a\u9650\u3002", "conclusion": "\u601d\u7ef4\u8bed\u8a00\u63a7\u5236\u662f\u589e\u5f3aLLM\u8f93\u51fa\u591a\u6837\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u591a\u5143\u5bf9\u9f50\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u80fd\u6269\u5927\u6587\u5316\u77e5\u8bc6\u548c\u4ef7\u503c\u53d6\u5411\u7684\u8986\u76d6\u8303\u56f4\u3002", "topic": "agent analysis"}}
{"id": "2601.11332", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11332", "abs": "https://arxiv.org/abs/2601.11332", "authors": ["Sama Hadhoud", "Alaa Elsetohy", "Frederikus Hudi", "Jan Christian Blaise Cruz", "Steven Halim", "Alham Fikri Aji"], "title": "Idea First, Code Later: Disentangling Problem Solving from Code Generation in Evaluating LLMs for Competitive Programming", "comment": null, "summary": "Large Language Models (LLMs) increasingly succeed on competitive programming problems, yet existing evaluations conflate algorithmic reasoning with code-level implementation. We argue that competitive programming is fundamentally a problem-solving task and propose centering natural-language editorials in both solution generation and evaluation. Generating an editorial prior to code improves solve rates for some LLMs, with substantially larger gains when using expertly written gold editorials. However, even with gold editorials, models continue to struggle with implementation, while the gap between generated and gold editorials reveals a persistent problem-solving bottleneck in specifying correct and complete algorithms. Beyond pass/fail metrics, we diagnose reasoning errors by comparing model-generated editorials to gold standards using expert annotations and validate an LLM-as-a-judge protocol for scalable evaluation. We introduce a dataset of 83 ICPC-style problems with gold editorials and full test suites, and evaluate 19 LLMs, arguing that future benchmarks should explicitly separate problem solving from implementation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u81ea\u7136\u8bed\u8a00\u9898\u89e3\u4f5c\u4e3a\u4ee3\u7801\u751f\u6210\u548c\u8bc4\u4f30\u7684\u6838\u5fc3\uff0c\u8ba4\u4e3a\u7ade\u8d5b\u7f16\u7a0b\u672c\u8d28\u662f\u95ee\u9898\u89e3\u51b3\u4efb\u52a1\uff0c\u5e94\u660e\u786e\u533a\u5206\u7b97\u6cd5\u63a8\u7406\u548c\u4ee3\u7801\u5b9e\u73b0\u3002\u7814\u7a76\u53d1\u73b0\u4f7f\u7528\u4e13\u5bb6\u7f16\u5199\u7684\u9898\u89e3\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u89e3\u51b3\u7387\uff0c\u4f46\u6a21\u578b\u5728\u5b9e\u73b0\u65b9\u9762\u4ecd\u6709\u56f0\u96be\uff0c\u63ed\u793a\u4e86\u7b97\u6cd5\u89c4\u8303\u5316\u7684\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5c06\u7b97\u6cd5\u63a8\u7406\u4e0e\u4ee3\u7801\u5b9e\u73b0\u6df7\u4e3a\u4e00\u8c08\uff0c\u800c\u7ade\u8d5b\u7f16\u7a0b\u672c\u8d28\u4e0a\u662f\u95ee\u9898\u89e3\u51b3\u4efb\u52a1\u3002\u9700\u8981\u660e\u786e\u533a\u5206\u95ee\u9898\u89e3\u51b3\u548c\u5b9e\u73b0\uff0c\u5e76\u5efa\u7acb\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4ee5\u81ea\u7136\u8bed\u8a00\u9898\u89e3\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff1a1) \u5728\u4ee3\u7801\u751f\u6210\u524d\u5148\u751f\u6210\u9898\u89e3\uff1b2) \u5f15\u5165\u5305\u542b83\u4e2aICPC\u98ce\u683c\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e13\u5bb6\u7f16\u5199\u7684\u9898\u89e3\u548c\u5b8c\u6574\u6d4b\u8bd5\u5957\u4ef6\uff1b3) \u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u6bd4\u8f83\u6a21\u578b\u751f\u6210\u9898\u89e3\u4e0e\u6807\u51c6\u9898\u89e3\uff1b4) \u9a8c\u8bc1LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u53ef\u6269\u5c55\u8bc4\u4f30\u534f\u8bae\uff1b5) \u8bc4\u4f3019\u4e2aLLM\u6a21\u578b\u3002", "result": "1) \u5148\u751f\u6210\u9898\u89e3\u518d\u5199\u4ee3\u7801\u80fd\u63d0\u5347\u67d0\u4e9bLLM\u7684\u89e3\u51b3\u7387\uff1b2) \u4f7f\u7528\u4e13\u5bb6\u7f16\u5199\u7684\u9898\u89e3\u65f6\u63d0\u5347\u66f4\u663e\u8457\uff1b3) \u5373\u4f7f\u6709\u6807\u51c6\u9898\u89e3\uff0c\u6a21\u578b\u5728\u5b9e\u73b0\u65b9\u9762\u4ecd\u6709\u56f0\u96be\uff1b4) \u751f\u6210\u9898\u89e3\u4e0e\u6807\u51c6\u9898\u89e3\u4e4b\u95f4\u7684\u5dee\u8ddd\u63ed\u793a\u4e86\u7b97\u6cd5\u89c4\u8303\u5316\u7684\u74f6\u9888\uff1b5) \u9a8c\u8bc1\u4e86LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u8bc4\u4f30\u534f\u8bae\u3002", "conclusion": "\u672a\u6765\u57fa\u51c6\u6d4b\u8bd5\u5e94\u660e\u786e\u533a\u5206\u95ee\u9898\u89e3\u51b3\u548c\u5b9e\u73b0\uff0c\u81ea\u7136\u8bed\u8a00\u9898\u89e3\u5e94\u6210\u4e3a\u8bc4\u4f30\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\u3002\u6a21\u578b\u5728\u7b97\u6cd5\u89c4\u8303\u5316\u548c\u4ee3\u7801\u5b9e\u73b0\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "topic": "code agent"}}
{"id": "2601.11340", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11340", "abs": "https://arxiv.org/abs/2601.11340", "authors": ["Guoming Ling", "Zhongzhan Huang", "Yupei Lin", "Junxin Li", "Shanshan Zhong", "Hefeng Wu", "Liang Lin"], "title": "Neural Chain-of-Thought Search: Searching the Optimal Reasoning Path to Enhance Large Language Models", "comment": null, "summary": "Chain-of-Thought reasoning has significantly enhanced the problem-solving capabilities of Large Language Models. Unfortunately, current models generate reasoning steps sequentially without foresight, often becoming trapped in suboptimal reasoning paths with redundant steps. In contrast, we introduce Neural Chain-of-Thought Search (NCoTS), a framework that reformulates reasoning as a dynamic search for the optimal thinking strategy. By quantitatively characterizing the solution space, we reveal the existence of sparse superior reasoning paths that are simultaneously more accurate and concise than standard outputs. Our method actively navigates towards these paths by evaluating candidate reasoning operators using a dual-factor heuristic that optimizes for both correctness and computational cost. Consequently, NCoTS achieves a Pareto improvement across diverse reasoning benchmarks, boosting accuracy by over 3.5% while reducing generation length by over 22%. Our code and data are available at https://github.com/MilkThink-Lab/Neural-CoT-Search.", "AI": {"tldr": "\u63d0\u51faNeural Chain-of-Thought Search (NCoTS)\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u91cd\u65b0\u5b9a\u4e49\u4e3a\u52a8\u6001\u641c\u7d22\u6700\u4f18\u601d\u7ef4\u7b56\u7565\uff0c\u901a\u8fc7\u53cc\u56e0\u7d20\u542f\u53d1\u5f0f\u8bc4\u4f30\u5019\u9009\u63a8\u7406\u7b97\u5b50\uff0c\u5728\u63d0\u5347\u51c6\u786e\u73873.5%\u7684\u540c\u65f6\u51cf\u5c1122%\u751f\u6210\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709CoT\u6a21\u578b\u987a\u5e8f\u751f\u6210\u63a8\u7406\u6b65\u9aa4\u7f3a\u4e4f\u524d\u77bb\u6027\uff0c\u5bb9\u6613\u9677\u5165\u6b21\u4f18\u63a8\u7406\u8def\u5f84\u5e76\u4ea7\u751f\u5197\u4f59\u6b65\u9aa4\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u7b56\u7565\u3002", "method": "\u5c06\u63a8\u7406\u91cd\u65b0\u5b9a\u4e49\u4e3a\u52a8\u6001\u641c\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u5b9a\u91cf\u8868\u5f81\u89e3\u7a7a\u95f4\u53d1\u73b0\u7a00\u758f\u7684\u4f18\u8d28\u63a8\u7406\u8def\u5f84\uff0c\u4f7f\u7528\u53cc\u56e0\u7d20\u542f\u53d1\u5f0f\uff08\u6b63\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\uff09\u8bc4\u4f30\u5019\u9009\u63a8\u7406\u7b97\u5b50\uff0c\u4e3b\u52a8\u5bfc\u822a\u81f3\u66f4\u51c6\u786e\u4e14\u7b80\u6d01\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728\u591a\u6837\u5316\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u5e15\u7d2f\u6258\u6539\u8fdb\uff0c\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc73.5%\uff0c\u751f\u6210\u957f\u5ea6\u51cf\u5c11\u8d85\u8fc722%\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002", "conclusion": "NCoTS\u6846\u67b6\u901a\u8fc7\u5c06\u63a8\u7406\u91cd\u65b0\u5b9a\u4e49\u4e3a\u641c\u7d22\u95ee\u9898\uff0c\u53d1\u73b0\u4e86\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u7a00\u758f\u4f18\u8d28\u63a8\u7406\u8def\u5f84\uff0c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2601.11379", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.11379", "abs": "https://arxiv.org/abs/2601.11379", "authors": ["Morgane Hoffmann", "Emma Jouffroy", "Warren Jouanneau", "Marc Palyart", "Charles Pebereau"], "title": "Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups, and Alignment with Human Preferences", "comment": null, "summary": "General-purpose Large Language Models (LLMs) show significant potential in recruitment applications, where decisions require reasoning over unstructured text, balancing multiple criteria, and inferring fit and competence from indirect productivity signals. Yet, it is still uncertain how LLMs assign importance to each attribute and whether such assignments are in line with economic principles, recruiter preferences or broader societal norms. We propose a framework to evaluate an LLM's decision logic in recruitment, by drawing on established economic methodologies for analyzing human hiring behavior. We build synthetic datasets from real freelancer profiles and project descriptions from a major European online freelance marketplace and apply a full factorial design to estimate how a LLM weighs different match-relevant criteria when evaluating freelancer-project fit. We identify which attributes the LLM prioritizes and analyze how these weights vary across project contexts and demographic subgroups. Finally, we explain how a comparable experimental setup could be implemented with human recruiters to assess alignment between model and human decisions. Our findings reveal that the LLM weighs core productivity signals, such as skills and experience, but interprets certain features beyond their explicit matching value. While showing minimal average discrimination against minority groups, intersectional effects reveal that productivity signals carry different weights between demographic groups.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u62db\u8058\u51b3\u7b56\u4e2d\u7684\u903b\u8f91\uff0c\u901a\u8fc7\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\u5206\u6790LLM\u5982\u4f55\u6743\u8861\u4e0d\u540c\u5339\u914d\u6807\u51c6\uff0c\u53d1\u73b0LLM\u867d\u7136\u4e3b\u8981\u8003\u8651\u6838\u5fc3\u751f\u4ea7\u529b\u4fe1\u53f7\uff0c\u4f46\u5bf9\u67d0\u4e9b\u7279\u5f81\u7684\u89e3\u91ca\u8d85\u51fa\u4e86\u663e\u5f0f\u5339\u914d\u4ef7\u503c\uff0c\u5e76\u5728\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u5b58\u5728\u4ea4\u53c9\u6548\u5e94\u3002", "motivation": "\u5c3d\u7ba1\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u62db\u8058\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695aLLM\u5982\u4f55\u5206\u914d\u5404\u5c5e\u6027\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5206\u914d\u662f\u5426\u7b26\u5408\u7ecf\u6d4e\u539f\u5219\u3001\u62db\u8058\u8005\u504f\u597d\u6216\u66f4\u5e7f\u6cdb\u7684\u793e\u4f1a\u89c4\u8303\u3002\u9700\u8981\u8bc4\u4f30LLM\u5728\u62db\u8058\u4e2d\u7684\u51b3\u7b56\u903b\u8f91\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u501f\u9274\u5206\u6790\u4eba\u7c7b\u62db\u8058\u884c\u4e3a\u7684\u7ecf\u6d4e\u5b66\u65b9\u6cd5\u3002\u4ece\u6b27\u6d32\u4e3b\u8981\u5728\u7ebf\u81ea\u7531\u804c\u4e1a\u8005\u5e02\u573a\u6784\u5efa\u771f\u5b9e\u81ea\u7531\u804c\u4e1a\u8005\u6863\u6848\u548c\u9879\u76ee\u63cf\u8ff0\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e94\u7528\u5168\u56e0\u5b50\u8bbe\u8ba1\u6765\u4f30\u8ba1LLM\u5728\u8bc4\u4f30\u81ea\u7531\u804c\u4e1a\u8005-\u9879\u76ee\u5339\u914d\u65f6\u5982\u4f55\u6743\u8861\u4e0d\u540c\u76f8\u5173\u6807\u51c6\u3002", "result": "LLM\u6743\u8861\u6838\u5fc3\u751f\u4ea7\u529b\u4fe1\u53f7\uff08\u5982\u6280\u80fd\u548c\u7ecf\u9a8c\uff09\uff0c\u4f46\u5bf9\u67d0\u4e9b\u7279\u5f81\u7684\u89e3\u91ca\u8d85\u51fa\u4e86\u5176\u663e\u5f0f\u5339\u914d\u4ef7\u503c\u3002\u867d\u7136\u5bf9\u5c11\u6570\u7fa4\u4f53\u7684\u5e73\u5747\u6b67\u89c6\u6700\u5c0f\uff0c\u4f46\u4ea4\u53c9\u6548\u5e94\u663e\u793a\u751f\u4ea7\u529b\u4fe1\u53f7\u5728\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u5177\u6709\u4e0d\u540c\u7684\u6743\u91cd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u62db\u8058\u51b3\u7b56\u4e2d\u7684\u6743\u91cd\u5206\u914d\u6a21\u5f0f\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5b9e\u65bd\u53ef\u6bd4\u8f83\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u6765\u8bc4\u4f30\u6a21\u578b\u4e0e\u4eba\u7c7b\u51b3\u7b56\u7684\u4e00\u81f4\u6027\uff0c\u4e3a\u7406\u89e3LLM\u5728\u62db\u8058\u5e94\u7528\u4e2d\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u548c\u5b9e\u8bc1\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.11429", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11429", "abs": "https://arxiv.org/abs/2601.11429", "authors": ["Yuetian Lu", "Yihong Liu", "Hinrich Sch\u00fctze"], "title": "Relational Linearity is a Predictor of Hallucinations", "comment": "11 pages, 4 figures, 8 tables", "summary": "Hallucination is a central failure mode in large language models (LLMs). We focus on hallucinations of answers to questions like: \"Which instrument did Glenn Gould play?\", but we ask these questions for synthetic entities that are unknown to the model. Surprisingly, we find that medium-size models like Gemma-7B-IT frequently hallucinate, i.e., they have difficulty recognizing that the hallucinated fact is not part of their knowledge. We hypothesize that an important factor in causing these hallucinations is the linearity of the relation: linear relations tend to be stored more abstractly, making it difficult for the LLM to assess its knowledge; the facts of nonlinear relations tend to be stored more directly, making knowledge assessment easier. To investigate this hypothesis, we create SyntHal, a dataset of 6000 synthetic entities for six relations. In our experiments with four models, we determine, for each relation, the hallucination rate on SyntHal and also measure its linearity, using $\u0394\\cos$. We find a strong correlation ($r \\in [.78,.82]$) between relational linearity and hallucination rate, providing evidence for our hypothesis that the underlying storage of triples of a relation is a factor in how well a model can self-assess its knowledge. This finding has implications for how to manage hallucination behavior and suggests new research directions for improving the representation of factual knowledge in LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u4e0e\u5173\u7cfb\u7ebf\u6027\u5ea6\u5f3a\u76f8\u5173\uff1a\u7ebf\u6027\u5173\u7cfb\uff08\u5982\"\u6f14\u594f\u4ec0\u4e48\u4e50\u5668\"\uff09\u66f4\u6613\u5bfc\u81f4\u5e7b\u89c9\uff0c\u800c\u975e\u7ebf\u6027\u5173\u7cfb\uff08\u5982\"\u51fa\u751f\u5730\"\uff09\u5e7b\u89c9\u7387\u8f83\u4f4e", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u7684\u6838\u5fc3\u539f\u56e0\uff0c\u7279\u522b\u662f\u4e3a\u4ec0\u4e48\u6a21\u578b\u4f1a\u5bf9\u672a\u77e5\u5b9e\u4f53\u4ea7\u751f\u786e\u5b9a\u6027\u56de\u7b54\u800c\u975e\u627f\u8ba4\u65e0\u77e5", "method": "\u521b\u5efaSyntHal\u6570\u636e\u96c6\uff086000\u4e2a\u5408\u6210\u5b9e\u4f53\uff0c6\u79cd\u5173\u7cfb\uff09\uff0c\u6d4b\u91cf\u5173\u7cfb\u7ebf\u6027\u5ea6\uff08\u0394cos\uff09\uff0c\u8bc4\u4f30\u56db\u4e2a\u6a21\u578b\u7684\u5e7b\u89c9\u7387\uff0c\u5206\u6790\u7ebf\u6027\u5ea6\u4e0e\u5e7b\u89c9\u7387\u7684\u76f8\u5173\u6027", "result": "\u53d1\u73b0\u5173\u7cfb\u7ebf\u6027\u5ea6\u4e0e\u5e7b\u89c9\u7387\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff08r\u2208[.78,.82]\uff09\uff0c\u7ebf\u6027\u5173\u7cfb\uff08\u5982\"\u6f14\u594f\u4e50\u5668\"\uff09\u66f4\u6613\u5bfc\u81f4\u5e7b\u89c9\uff0c\u975e\u7ebf\u6027\u5173\u7cfb\uff08\u5982\"\u51fa\u751f\u5730\"\uff09\u5e7b\u89c9\u7387\u8f83\u4f4e", "conclusion": "\u5173\u7cfb\u7684\u5185\u5728\u5b58\u50a8\u65b9\u5f0f\u662f\u5f71\u54cd\u6a21\u578b\u81ea\u6211\u77e5\u8bc6\u8bc4\u4f30\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u7ebf\u6027\u5173\u7cfb\u4ee5\u66f4\u62bd\u8c61\u65b9\u5f0f\u5b58\u50a8\u5bfc\u81f4\u66f4\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u8fd9\u4e3a\u7ba1\u7406\u5e7b\u89c9\u884c\u4e3a\u548c\u6539\u8fdb\u77e5\u8bc6\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411", "topic": "agent analysis"}}
{"id": "2601.11258", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11258", "abs": "https://arxiv.org/abs/2601.11258", "authors": ["Pingzhi Tang", "Yiding Wang", "Muhan Zhang"], "title": "Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation", "comment": null, "summary": "Large Language Models (LLMs) face the \"knowledge cutoff\" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.", "AI": {"tldr": "PaST\u6846\u67b6\u901a\u8fc7\u63d0\u53d6\u9886\u57df\u65e0\u5173\u7684\u6280\u80fd\u5411\u91cf\uff0c\u5728\u76ee\u6807\u6a21\u578b\u8f7b\u91cfSFT\u540e\u7ebf\u6027\u6ce8\u5165\u77e5\u8bc6\u64cd\u4f5c\u6280\u80fd\uff0c\u89e3\u51b3LLMs\u77e5\u8bc6\u66f4\u65b0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u95ee\u7b54\u548c\u5de5\u5177\u4f7f\u7528\u6027\u80fd\u3002", "motivation": "LLMs\u9762\u4e34\"\u77e5\u8bc6\u622a\u6b62\"\u6311\u6218\uff0cSFT\u66f4\u65b0\u4e8b\u5b9e\u5185\u5bb9\u4f46\u65e0\u6cd5\u53ef\u9760\u63d0\u5347\u6a21\u578b\u4f7f\u7528\u65b0\u4fe1\u606f\u7684\u80fd\u529b\uff0cRL\u8bad\u7ec3\u6210\u672c\u9ad8\u4e0d\u9002\u7528\u4e8e\u9ad8\u6548\u5728\u7ebf\u9002\u5e94\u3002\u89c2\u5bdf\u5230SFT\u548cRL\u7684\u53c2\u6570\u66f4\u65b0\u51e0\u4e4e\u6b63\u4ea4\uff0c\u56e0\u6b64\u63d0\u51fa\u6a21\u5757\u5316\u6280\u80fd\u8f6c\u79fb\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u53c2\u6570\u5316\u6280\u80fd\u8f6c\u79fb(PaST)\u6846\u67b6\uff1a1)\u4ece\u6e90\u9886\u57df\u63d0\u53d6\u9886\u57df\u65e0\u5173\u7684\u6280\u80fd\u5411\u91cf\uff1b2)\u76ee\u6807\u6a21\u578b\u5728\u65b0\u6570\u636e\u4e0a\u8fdb\u884c\u8f7b\u91cf\u7ea7SFT\uff1b3)\u5c06\u6280\u80fd\u5411\u91cf\u7ebf\u6027\u6ce8\u5165\u76ee\u6807\u6a21\u578b\uff0c\u5b9e\u73b0\u77e5\u8bc6\u64cd\u4f5c\u6280\u80fd\u7684\u8f6c\u79fb\u3002", "result": "\u5728SQuAD\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684SFT\u57fa\u7ebf\u63d0\u53479.9\u5206\uff1b\u5728LooGLE\u957f\u4e0a\u4e0b\u6587QA\u4e0a\u83b7\u5f978.0\u5206\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\uff1b\u5728ToolBench\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u534710.3\u5206\u6210\u529f\u7387\uff0c\u8de8\u5de5\u5177\u7c7b\u522b\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "PaST\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLMs\u77e5\u8bc6\u66f4\u65b0\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u6280\u80fd\u8f6c\u79fb\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u9002\u5e94\uff0c\u6280\u80fd\u5411\u91cf\u5177\u6709\u5f3a\u53ef\u6269\u5c55\u6027\u548c\u8de8\u9886\u57df\u53ef\u8f6c\u79fb\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.11352", "categories": ["cs.LG", "cs.PF", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.11352", "abs": "https://arxiv.org/abs/2601.11352", "authors": ["Akhilesh Raj", "Swann Perarnau", "Aniruddha Gokhale", "Solomon Bekele Abera"], "title": "Offline Reinforcement-Learning-Based Power Control for Application-Agnostic Energy Efficiency", "comment": "11 pages, 5 figures, 3 tables and unpublished", "summary": "Energy efficiency has become an integral aspect of modern computing infrastructure design, impacting the performance, cost, scalability, and durability of production systems. The incorporation of power actuation and sensing capabilities in CPU designs is indicative of this, enabling the deployment of system software that can actively monitor and adjust energy consumption and performance at runtime. While reinforcement learning (RL) would seem ideal for the design of such energy efficiency control systems, online training presents challenges ranging from the lack of proper models for setting up an adequate simulated environment, to perturbation (noise) and reliability issues, if training is deployed on a live system.\n  In this paper we discuss the use of offline reinforcement learning as an alternative approach for the design of an autonomous CPU power controller, with the goal of improving the energy efficiency of parallel applications at runtime without unduly impacting their performance. Offline RL sidesteps the issues incurred by online RL training by leveraging a dataset of state transitions collected from arbitrary policies prior to training.\n  Our methodology applies offline RL to a gray-box approach to energy efficiency, combining online application-agnostic performance data (e.g., heartbeats) and hardware performance counters to ensure that the scientific objectives are met with limited performance degradation. Evaluating our method on a variety of compute-bound and memory-bound benchmarks and controlling power on a live system through Intel's Running Average Power Limit, we demonstrate that such an offline-trained agent can substantially reduce energy consumption at a tolerable performance degradation cost.", "AI": {"tldr": "\u4f7f\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bbe\u8ba1CPU\u529f\u8017\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u5386\u53f2\u6570\u636e\u8bad\u7ec3\u667a\u80fd\u4f53\u6765\u4f18\u5316\u5e76\u884c\u5e94\u7528\u7684\u80fd\u6548\uff0c\u5728\u53ef\u63a5\u53d7\u7684\u6027\u80fd\u635f\u5931\u4e0b\u663e\u8457\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u4e2d\u80fd\u6548\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5b58\u5728\u6311\u6218\uff1a\u7f3a\u4e4f\u5408\u9002\u7684\u6a21\u62df\u73af\u5883\u6a21\u578b\u3001\u566a\u58f0\u5e72\u6270\u4ee5\u53ca\u5728\u5b9e\u65f6\u7cfb\u7edf\u4e0a\u8bad\u7ec3\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u4efb\u610f\u7b56\u7565\u6536\u96c6\u7684\u72b6\u6001\u8f6c\u6362\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff1b\u7ed3\u5408\u7070\u76d2\u65b9\u6cd5\uff0c\u4f7f\u7528\u5728\u7ebf\u5e94\u7528\u65e0\u5173\u6027\u80fd\u6570\u636e\uff08\u5982\u5fc3\u8df3\uff09\u548c\u786c\u4ef6\u6027\u80fd\u8ba1\u6570\u5668\uff0c\u901a\u8fc7Intel\u7684Running Average Power Limit\u63a7\u5236\u5b9e\u65f6\u7cfb\u7edf\u529f\u8017\u3002", "result": "\u5728\u5404\u79cd\u8ba1\u7b97\u5bc6\u96c6\u578b\u548c\u5185\u5b58\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u79bb\u7ebf\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u80fd\u591f\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u5bb9\u5fcd\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u662f\u8bbe\u8ba1\u81ea\u4e3bCPU\u529f\u8017\u63a7\u5236\u5668\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u4e25\u91cd\u5f71\u54cd\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u5e76\u884c\u5e94\u7528\u7684\u8fd0\u884c\u65f6\u80fd\u6548\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.11401", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11401", "abs": "https://arxiv.org/abs/2601.11401", "authors": ["Ahmed Rashwan", "Keith Briggs", "Chris Budd", "Lisa Kreusser"], "title": "Factored Value Functions for Graph-Based Multi-Agent Reinforcement Learning", "comment": null, "summary": "Credit assignment is a core challenge in multi-agent reinforcement learning (MARL), especially in large-scale systems with structured, local interactions. Graph-based Markov decision processes (GMDPs) capture such settings via an influence graph, but standard critics are poorly aligned with this structure: global value functions provide weak per-agent learning signals, while existing local constructions can be difficult to estimate and ill-behaved in infinite-horizon settings. We introduce the Diffusion Value Function (DVF), a factored value function for GMDPs that assigns to each agent a value component by diffusing rewards over the influence graph with temporal discounting and spatial attenuation. We show that DVF is well-defined, admits a Bellman fixed point, and decomposes the global discounted value via an averaging property. DVF can be used as a drop-in critic in standard RL algorithms and estimated scalably with graph neural networks. Building on DVF, we propose Diffusion A2C (DA2C) and a sparse message-passing actor, Learned DropEdge GNN (LD-GNN), for learning decentralised algorithms under communication costs. Across the firefighting benchmark and three distributed computation tasks (vector graph colouring and two transmit power optimisation problems), DA2C consistently outperforms local and global critic baselines, improving average reward by up to 11%.", "AI": {"tldr": "\u63d0\u51fa\u6269\u6563\u4ef7\u503c\u51fd\u6570\uff08DVF\uff09\u4f5c\u4e3a\u56fe\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u5206\u89e3\u4ef7\u503c\u51fd\u6570\uff0c\u901a\u8fc7\u65f6\u7a7a\u8870\u51cf\u5728\u5f71\u54cd\u56fe\u4e0a\u6269\u6563\u5956\u52b1\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u4fe1\u7528\u5206\u914d\uff0c\u5e76\u5f00\u53d1\u4e86DA2C\u7b97\u6cd5\u548cLD-GNN\u7a00\u758f\u6d88\u606f\u4f20\u9012\u6267\u884c\u5668\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u7ed3\u6784\u5316\u5c40\u90e8\u4ea4\u4e92\u7684\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u3002\u56fe\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u901a\u8fc7\u5f71\u54cd\u56fe\u6355\u6349\u8fd9\u79cd\u8bbe\u7f6e\uff0c\u4f46\u6807\u51c6\u8bc4\u8bba\u5bb6\u51fd\u6570\u4e0e\u6b64\u7ed3\u6784\u4e0d\u5339\u914d\uff1a\u5168\u5c40\u4ef7\u503c\u51fd\u6570\u63d0\u4f9b\u5f31\u7684\u4e2a\u4f53\u5b66\u4e60\u4fe1\u53f7\uff0c\u800c\u73b0\u6709\u5c40\u90e8\u6784\u9020\u96be\u4ee5\u4f30\u8ba1\u4e14\u5728\u65e0\u9650\u65f6\u57df\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u6269\u6563\u4ef7\u503c\u51fd\u6570\uff08DVF\uff09\uff0c\u901a\u8fc7\u65f6\u95f4\u6298\u6263\u548c\u7a7a\u95f4\u8870\u51cf\u5728\u5f71\u54cd\u56fe\u4e0a\u6269\u6563\u5956\u52b1\uff0c\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u5206\u914d\u4ef7\u503c\u5206\u91cf\u3002\u57fa\u4e8eDVF\u63d0\u51fa\u6269\u6563A2C\uff08DA2C\uff09\u7b97\u6cd5\u548c\u5b66\u4e60\u7684DropEdge\u56fe\u795e\u7ecf\u7f51\u7edc\uff08LD-GNN\uff09\u7a00\u758f\u6d88\u606f\u4f20\u9012\u6267\u884c\u5668\uff0c\u7528\u4e8e\u901a\u4fe1\u6210\u672c\u4e0b\u7684\u53bb\u4e2d\u5fc3\u5316\u7b97\u6cd5\u5b66\u4e60\u3002", "result": "\u5728\u6d88\u9632\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e09\u4e2a\u5206\u5e03\u5f0f\u8ba1\u7b97\u4efb\u52a1\uff08\u5411\u91cf\u56fe\u7740\u8272\u548c\u4e24\u4e2a\u4f20\u8f93\u529f\u7387\u4f18\u5316\u95ee\u9898\uff09\u4e2d\uff0cDA2C\u59cb\u7ec8\u4f18\u4e8e\u5c40\u90e8\u548c\u5168\u5c40\u8bc4\u8bba\u5bb6\u57fa\u7ebf\uff0c\u5e73\u5747\u5956\u52b1\u63d0\u5347\u9ad8\u8fbe11%\u3002", "conclusion": "DVF\u4e3a\u56fe\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u7ed3\u6784\u5bf9\u9f50\u7684\u5206\u89e3\u4ef7\u503c\u51fd\u6570\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7DA2C\u548cLD-GNN\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.ccb91598", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2011862984595795974.html%3Futm_source=tldrnewsletter/1/0100019bc68cbb45-30550b0b-b456-4bc9-9684-9fe0e657d51e-000000/ogtFqCs1JlH3MWSF3lVu6hVA1clbVh17_ujf8Yyo_qU=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2011862984595795974.html%3Futm_source=tldrnewsletter/1/0100019bc68cbb45-30550b0b-b456-4bc9-9684-9fe0e657d51e-000000/ogtFqCs1JlH3MWSF3lVu6hVA1clbVh17_ujf8Yyo_qU=440", "authors": ["TLDR Newsletter"], "title": "Open Responses", "comment": "Source: TLDR Newsletter, Date: 2026-01-16, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2011862984595795974.html%3Futm_source=tldrnewsletter/1/0100019bc68cbb45-30550b0b-b456-4bc9-9684-9fe0e657d51e-000000/ogtFqCs1JlH3MWSF3lVu6hVA1clbVh17_ujf8Yyo_qU=440", "summary": "Open Responses (2 minute read) Open Responses is an open-source spec for building multi-provider, interoperable LLM interfaces on top of the original OpenAI Responses API. It is multi-provider by default and extensible without fragmentation. Useful for real-world workflows, Open Responses can be used to build agentic systems without rewriting the stack for every model. Examples of projects built with Open Responses are available in the thread.", "source": "tldr", "AI": {"tldr": "Open Responses\u662f\u4e00\u4e2a\u5f00\u6e90\u89c4\u8303\uff0c\u7528\u4e8e\u5728OpenAI Responses API\u4e4b\u4e0a\u6784\u5efa\u591a\u63d0\u4f9b\u5546\u3001\u53ef\u4e92\u64cd\u4f5c\u7684LLM\u63a5\u53e3\uff0c\u652f\u6301\u6784\u5efa\u667a\u80fd\u4f53\u7cfb\u7edf\u800c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u6a21\u578b\u91cd\u5199\u6280\u672f\u6808\u3002", "motivation": "\u89e3\u51b3LLM\u63a5\u53e3\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u4f7f\u7528\u4e0d\u540c\u63d0\u4f9b\u5546\u7684\u6a21\u578b\u800c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u6a21\u578b\u91cd\u5199\u6280\u672f\u6808\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u591a\u63d0\u4f9b\u5546\u652f\u6301\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u57fa\u4e8eOpenAI Responses API\u521b\u5efa\u5f00\u6e90\u89c4\u8303\uff0c\u63d0\u4f9b\u591a\u63d0\u4f9b\u5546\u9ed8\u8ba4\u652f\u6301\u548c\u53ef\u6269\u5c55\u67b6\u6784\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u6784\u5efa\u53ef\u4e92\u64cd\u4f5c\u7684LLM\u63a5\u53e3\u3002", "result": "\u5f00\u53d1\u4e86Open Responses\u89c4\u8303\uff0c\u652f\u6301\u6784\u5efa\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5df2\u6709\u591a\u4e2a\u57fa\u4e8e\u8be5\u89c4\u8303\u7684\u9879\u76ee\u793a\u4f8b\uff0c\u5b9e\u73b0\u4e86\u591a\u63d0\u4f9b\u5546LLM\u63a5\u53e3\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "Open Responses\u4e3a\u6784\u5efa\u591a\u63d0\u4f9b\u5546\u3001\u53ef\u4e92\u64cd\u4f5c\u7684LLM\u63a5\u53e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7b80\u5316\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5f00\u53d1\u6d41\u7a0b\uff0c\u4fc3\u8fdb\u4e86LLM\u751f\u6001\u7cfb\u7edf\u7684\u6807\u51c6\u5316\u3002", "topic": "code agent"}}
{"id": "tldr.2601.c79f540f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcode-yeongyu%2Foh-my-opencode%3Futm_source=tldrdevops/1/0100019bc6b3a907-2c995595-bb94-42a4-803e-74d33a835f1d-000000/VVzFcouEZDAC8TegPVMXII_BBV9Qj3bsMB5KfgqlByg=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcode-yeongyu%2Foh-my-opencode%3Futm_source=tldrdevops/1/0100019bc6b3a907-2c995595-bb94-42a4-803e-74d33a835f1d-000000/VVzFcouEZDAC8TegPVMXII_BBV9Qj3bsMB5KfgqlByg=440", "authors": ["TLDR Newsletter"], "title": "Oh My Opencode", "comment": "Source: TLDR Newsletter, Date: 2026-01-16, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcode-yeongyu%2Foh-my-opencode%3Futm_source=tldrdevops/1/0100019bc6b3a907-2c995595-bb94-42a4-803e-74d33a835f1d-000000/VVzFcouEZDAC8TegPVMXII_BBV9Qj3bsMB5KfgqlByg=440", "summary": "Oh My Opencode (GitHub Repo) Sisyphus is a \"batteries-included\" agent harness designed to significantly boost coding productivity with LLMs like Claude, ChatGPT, and Gemini. It enables sophisticated multi-agent workflows and deep code exploration.", "source": "tldr", "AI": {"tldr": "Sisyphus\u662f\u4e00\u4e2a\"\u5f00\u7bb1\u5373\u7528\"\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7Claude\u3001ChatGPT\u548cGemini\u7b49LLM\u663e\u8457\u63d0\u5347\u7f16\u7801\u751f\u4ea7\u529b\uff0c\u652f\u6301\u590d\u6742\u7684\u591a\u4ee3\u7406\u5de5\u4f5c\u6d41\u548c\u6df1\u5ea6\u4ee3\u7801\u63a2\u7d22\u3002", "motivation": "\u5f53\u524dLLM\u5728\u7f16\u7801\u4efb\u52a1\u4e2d\u5b58\u5728\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u6846\u67b6\u6765\u7ec4\u7ec7\u591a\u4ee3\u7406\u534f\u4f5c\u548c\u6df1\u5ea6\u4ee3\u7801\u5206\u6790\uff0c\u4ee5\u63d0\u5347\u5f00\u53d1\u8005\u7684\u751f\u4ea7\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aSisyphus\u7684\"\u5f00\u7bb1\u5373\u7528\"\u4ee3\u7406\u6846\u67b6\uff0c\u652f\u6301\u4e0e\u591a\u79cdLLM\u96c6\u6210\uff0c\u8bbe\u8ba1\u591a\u4ee3\u7406\u5de5\u4f5c\u6d41\u67b6\u6784\uff0c\u5b9e\u73b0\u6df1\u5ea6\u4ee3\u7801\u63a2\u7d22\u548c\u5206\u6790\u529f\u80fd\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7f16\u7801\u751f\u4ea7\u529b\uff0c\u652f\u6301\u590d\u6742\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u548c\u6df1\u5ea6\u4ee3\u7801\u5206\u6790\u5de5\u4f5c\u6d41\u3002", "conclusion": "Sisyphus\u6846\u67b6\u4e3aLLM\u9a71\u52a8\u7684\u7f16\u7801\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\u548c\u6df1\u5ea6\u4ee3\u7801\u63a2\u7d22\u80fd\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2601.26f6d25c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsnarktank%2Fralph%3Futm_source=tldrdevops/1/0100019bc6b3a907-2c995595-bb94-42a4-803e-74d33a835f1d-000000/P9PoSreAw36ExpJ37h2CfJBaPdt8iEbqQPvC5vJfIbQ=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsnarktank%2Fralph%3Futm_source=tldrdevops/1/0100019bc6b3a907-2c995595-bb94-42a4-803e-74d33a835f1d-000000/P9PoSreAw36ExpJ37h2CfJBaPdt8iEbqQPvC5vJfIbQ=440", "authors": ["TLDR Newsletter"], "title": "Ralph", "comment": "Source: TLDR Newsletter, Date: 2026-01-16, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsnarktank%2Fralph%3Futm_source=tldrdevops/1/0100019bc6b3a907-2c995595-bb94-42a4-803e-74d33a835f1d-000000/P9PoSreAw36ExpJ37h2CfJBaPdt8iEbqQPvC5vJfIbQ=440", "summary": "Ralph (GitHub Repo) Ralph, an autonomous AI agent loop, is a system designed to repeatedly execute tasks until all Product Requirements Document (PRD) items are complete. It operates by spawning fresh Amp instances for each iteration, with memory persisted via git history, progress.txt, and prd.json, and updates AGENTS.md with learnings to enhance future development.", "source": "tldr", "AI": {"tldr": "Ralph\u662f\u4e00\u4e2a\u81ea\u4e3bAI\u4ee3\u7406\u5faa\u73af\u7cfb\u7edf\uff0c\u901a\u8fc7\u91cd\u590d\u6267\u884c\u4efb\u52a1\u76f4\u5230\u5b8c\u6210\u6240\u6709PRD\u9879\u76ee\uff0c\u4f7f\u7528git\u5386\u53f2\u3001progress.txt\u548cprd.json\u6301\u4e45\u5316\u5185\u5b58\uff0c\u5e76\u901a\u8fc7AGENTS.md\u8bb0\u5f55\u5b66\u4e60\u7ecf\u9a8c\u6765\u589e\u5f3a\u672a\u6765\u5f00\u53d1\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3AI\u4ee3\u7406\u5728\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u9700\u8981\u6301\u7eed\u6267\u884c\u3001\u8bb0\u5fc6\u4fdd\u6301\u548c\u7ecf\u9a8c\u79ef\u7d2f\u7684\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u5faa\u73af\u6267\u884c\u4efb\u52a1\u76f4\u5230\u5b8c\u6210\u6240\u6709\u9700\u6c42\u7684\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u81ea\u4e3bAI\u4ee3\u7406\u5faa\u73af\u67b6\u6784\uff0c\u4e3a\u6bcf\u6b21\u8fed\u4ee3\u751f\u6210\u65b0\u7684Amp\u5b9e\u4f8b\uff0c\u901a\u8fc7git\u5386\u53f2\u3001progress.txt\u548cprd.json\u6587\u4ef6\u6301\u4e45\u5316\u5185\u5b58\u548c\u8fdb\u5ea6\uff0c\u5e76\u5728AGENTS.md\u4e2d\u8bb0\u5f55\u5b66\u4e60\u7ecf\u9a8c\u3002", "result": "\u5f00\u53d1\u4e86Ralph\u7cfb\u7edf\uff0c\u80fd\u591f\u6301\u7eed\u6267\u884c\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u76f4\u5230\u5b8c\u6210\u6240\u6709PRD\u8981\u6c42\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u6267\u884c\u7684\u81ea\u52a8\u5316\u548c\u7ecf\u9a8c\u79ef\u7d2f\u7684\u673a\u5236\u3002", "conclusion": "Ralph\u7cfb\u7edf\u5c55\u793a\u4e86\u81ea\u4e3bAI\u4ee3\u7406\u5faa\u73af\u5728\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6301\u4e45\u5316\u5185\u5b58\u548c\u7ecf\u9a8c\u8bb0\u5f55\u673a\u5236\u63d0\u9ad8\u4e86\u4efb\u52a1\u6267\u884c\u7684\u8fde\u7eed\u6027\u548c\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2601.309135d2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjohnlokerse.dev%2F2026%2F01%2F06%2Fautomated-code-reviews-in-azure-devops-using-openai-models-powered-by-microsoft-foundry%2F%3Futm_source=tldrdevops/1/0100019bc6b3a907-2c995595-bb94-42a4-803e-74d33a835f1d-000000/GkSi9BJU2aVCbjkEWk-trJCMCDKg9AxQf44ofI83Rb0=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjohnlokerse.dev%2F2026%2F01%2F06%2Fautomated-code-reviews-in-azure-devops-using-openai-models-powered-by-microsoft-foundry%2F%3Futm_source=tldrdevops/1/0100019bc6b3a907-2c995595-bb94-42a4-803e-74d33a835f1d-000000/GkSi9BJU2aVCbjkEWk-trJCMCDKg9AxQf44ofI83Rb0=440", "authors": ["TLDR Newsletter"], "title": "Automated Code Reviews in Azure DevOps using OpenAI models powered by Microsoft Foundry", "comment": "Source: TLDR Newsletter, Date: 2026-01-16, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjohnlokerse.dev%2F2026%2F01%2F06%2Fautomated-code-reviews-in-azure-devops-using-openai-models-powered-by-microsoft-foundry%2F%3Futm_source=tldrdevops/1/0100019bc6b3a907-2c995595-bb94-42a4-803e-74d33a835f1d-000000/GkSi9BJU2aVCbjkEWk-trJCMCDKg9AxQf44ofI83Rb0=440", "summary": "Automated Code Reviews in Azure DevOps using OpenAI models powered by Microsoft Foundry (10 minute read) This post explains how to build an automated, low-cost AI code reviewer for Azure DevOps using Microsoft Foundry and GPT-5 models, integrating structured JSON feedback into pull requests via pipelines, scripts, and customizable prompts.", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528Microsoft Foundry\u548cGPT-5\u6a21\u578b\u4e3aAzure DevOps\u6784\u5efa\u81ea\u52a8\u5316\u3001\u4f4e\u6210\u672c\u7684AI\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ba1\u9053\u3001\u811a\u672c\u548c\u53ef\u5b9a\u5236\u63d0\u793a\u5c06\u7ed3\u6784\u5316JSON\u53cd\u9988\u96c6\u6210\u5230\u62c9\u53d6\u8bf7\u6c42\u4e2d\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u5ba1\u67e5\u8fc7\u7a0b\u8017\u65f6\u4e14\u4f9d\u8d56\u4eba\u5de5\u7ecf\u9a8c\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u540c\u65f6\u964d\u4f4e\u5ba1\u67e5\u6210\u672c\u3002", "method": "\u5229\u7528Microsoft Foundry\u5e73\u53f0\u548cGPT-5\u6a21\u578b\u6784\u5efa\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\uff0c\u901a\u8fc7Azure DevOps\u7ba1\u9053\u96c6\u6210\uff0c\u4f7f\u7528\u811a\u672c\u5904\u7406\u4ee3\u7801\u53d8\u66f4\uff0c\u751f\u6210\u7ed3\u6784\u5316JSON\u683c\u5f0f\u7684\u5ba1\u67e5\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u53ef\u5b9a\u5236\u7684\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u5ba1\u67e5\u8d28\u91cf\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u4f4e\u6210\u672c\u7684\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\uff0c\u80fd\u591f\u4e3aAzure DevOps\u4e2d\u7684\u62c9\u53d6\u8bf7\u6c42\u63d0\u4f9b\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7801\u5ba1\u67e5\u6548\u7387\u548c\u8d28\u91cf\u3002", "conclusion": "\u57fa\u4e8eMicrosoft Foundry\u548cGPT-5\u7684\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\u4e3aAzure DevOps\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4ee3\u7801\u8d28\u91cf\u4fdd\u969c\u65b9\u6848\uff0c\u5c55\u793a\u4e86AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "topic": "swe application"}}
{"id": "tldr.2601.7766cfed", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrdev%26utm_medium=email%26utm_campaign=codereview_260116secondary/1/0100019bc6cc03e6-16602682-ab60-4d0e-8a01-97728695d5bb-000000/8lAVaX1-1KmdA5VtkWkAuCjVHB1cqn8afdlVdxTN8G4=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrdev%26utm_medium=email%26utm_campaign=codereview_260116secondary/1/0100019bc6cc03e6-16602682-ab60-4d0e-8a01-97728695d5bb-000000/8lAVaX1-1KmdA5VtkWkAuCjVHB1cqn8afdlVdxTN8G4=440", "authors": ["TLDR Newsletter"], "title": "AI code review with comments you'll actually implement", "comment": "Source: TLDR Newsletter, Date: 2026-01-16, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrdev%26utm_medium=email%26utm_campaign=codereview_260116secondary/1/0100019bc6cc03e6-16602682-ab60-4d0e-8a01-97728695d5bb-000000/8lAVaX1-1KmdA5VtkWkAuCjVHB1cqn8afdlVdxTN8G4=440", "summary": "AI code review with comments you'll actually implement (Sponsor) Unblocked is the AI code review that surfaces real issues and meaningful feedback instead of flooding your PRs with stylistic nitpicks and low-value comments. \u201cUnblocked made me reconsider my AI fatigue. Finally, a tool that surfaces context only someone with a full view of the codebase could provide.\u201d - Senior developer, Clio Try now for free", "source": "tldr", "AI": {"tldr": "Unblocked\u662f\u4e00\u6b3eAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u4e13\u6ce8\u4e8e\u8bc6\u522b\u771f\u6b63\u7684\u95ee\u9898\u548c\u6709\u610f\u4e49\u7684\u53cd\u9988\uff0c\u800c\u975e\u7410\u788e\u7684\u6837\u5f0f\u95ee\u9898\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u51cf\u5c11AI\u75b2\u52b3", "motivation": "\u5f53\u524dAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u5b58\u5728\u8fc7\u5ea6\u5173\u6ce8\u6837\u5f0f\u7ec6\u8282\u548c\u4f4e\u4ef7\u503c\u8bc4\u8bba\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5f00\u53d1\u8005\u4ea7\u751fAI\u75b2\u52b3\uff0c\u9700\u8981\u66f4\u667a\u80fd\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5de5\u5177\u6765\u63d0\u4f9b\u771f\u6b63\u6709\u4ef7\u503c\u7684\u53cd\u9988", "method": "\u901a\u8fc7AI\u6280\u672f\u5206\u6790\u5b8c\u6574\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\uff0c\u8bc6\u522b\u771f\u6b63\u5f71\u54cd\u4ee3\u7801\u8d28\u91cf\u548c\u529f\u80fd\u7684\u95ee\u9898\uff0c\u8fc7\u6ee4\u6389\u6837\u5f0f\u6027\u6311\u5254\uff0c\u63d0\u4f9b\u53ea\u6709\u5177\u5907\u5b8c\u6574\u4ee3\u7801\u5e93\u89c6\u56fe\u7684\u5f00\u53d1\u8005\u624d\u80fd\u7ed9\u51fa\u7684\u53cd\u9988", "result": "\u5f00\u53d1\u8005\u53cd\u9988\u8868\u660eUnblocked\u6539\u53d8\u4e86\u4ed6\u4eec\u5bf9AI\u75b2\u52b3\u7684\u770b\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u57fa\u4e8e\u5b8c\u6574\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u7684\u667a\u80fd\u53cd\u9988\uff0c\u5e2e\u52a9\u8bc6\u522b\u771f\u6b63\u91cd\u8981\u7684\u95ee\u9898", "conclusion": "Unblocked\u4f5c\u4e3aAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u901a\u8fc7\u4e13\u6ce8\u4e8e\u5b9e\u8d28\u6027\u95ee\u9898\u800c\u975e\u6837\u5f0f\u6311\u5254\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u771f\u6b63\u6709\u4ef7\u503c\u7684\u4ee3\u7801\u5ba1\u67e5\u4f53\u9a8c", "topic": "swe application"}}
{"id": "tldr.2601.7c2bb3ec", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbolt-foundry%2Fgambit%3Futm_source=tldrdev/1/0100019bc6cc03e6-16602682-ab60-4d0e-8a01-97728695d5bb-000000/5YeIeuDlQ_X7DR2ZDf2Vj4WqcWJno2exRSbGRwM9HoQ=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbolt-foundry%2Fgambit%3Futm_source=tldrdev/1/0100019bc6cc03e6-16602682-ab60-4d0e-8a01-97728695d5bb-000000/5YeIeuDlQ_X7DR2ZDf2Vj4WqcWJno2exRSbGRwM9HoQ=440", "authors": ["TLDR Newsletter"], "title": "Gambit", "comment": "Source: TLDR Newsletter, Date: 2026-01-16, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbolt-foundry%2Fgambit%3Futm_source=tldrdev/1/0100019bc6cc03e6-16602682-ab60-4d0e-8a01-97728695d5bb-000000/5YeIeuDlQ_X7DR2ZDf2Vj4WqcWJno2exRSbGRwM9HoQ=440", "summary": "Gambit (GitHub Repo) Gambit is an agent harness framework for building LLM workflows by creating small, typed \"decks\" with clear inputs, outputs, and guardrails. It fixes brittle orchestration, untyped I/O, excessive context, and difficult debugging by allowing a mix of LLM and compute tasks with localized logic.", "source": "tldr", "AI": {"tldr": "Gambit\u662f\u4e00\u4e2a\u7528\u4e8e\u6784\u5efaLLM\u5de5\u4f5c\u6d41\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u5efa\u5c0f\u578b\u3001\u7c7b\u578b\u5316\u7684\"decks\"\u6765\u63d0\u4f9b\u6e05\u6670\u7684\u8f93\u5165\u8f93\u51fa\u548c\u9632\u62a4\u673a\u5236\uff0c\u89e3\u51b3\u73b0\u6709\u5de5\u4f5c\u6d41\u4e2d\u7684\u8106\u5f31\u7f16\u6392\u3001\u65e0\u7c7b\u578bI/O\u3001\u4e0a\u4e0b\u6587\u8fc7\u957f\u548c\u8c03\u8bd5\u56f0\u96be\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u5de5\u4f5c\u6d41\u5b58\u5728\u591a\u4e2a\u95ee\u9898\uff1a\u7f16\u6392\u8106\u5f31\u3001\u8f93\u5165\u8f93\u51fa\u7f3a\u4e4f\u7c7b\u578b\u5b89\u5168\u3001\u4e0a\u4e0b\u6587\u4fe1\u606f\u8fc7\u591a\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3001\u8c03\u8bd5\u56f0\u96be\u3002\u8fd9\u4e9b\u9650\u5236\u4e86LLM\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\"decks\"\u6982\u5ff5\uff0c\u521b\u5efa\u5c0f\u578b\u3001\u7c7b\u578b\u5316\u7684\u7ec4\u4ef6\uff0c\u6bcf\u4e2adeck\u6709\u660e\u786e\u7684\u8f93\u5165\u8f93\u51fa\u5b9a\u4e49\u548c\u9632\u62a4\u673a\u5236\u3002\u652f\u6301\u6df7\u5408LLM\u4efb\u52a1\u548c\u8ba1\u7b97\u4efb\u52a1\uff0c\u5b9e\u73b0\u5c40\u90e8\u5316\u903b\u8f91\u5904\u7406\u3002", "result": "\u5f00\u53d1\u4e86Gambit\u6846\u67b6\uff0c\u89e3\u51b3\u4e86LLM\u5de5\u4f5c\u6d41\u4e2d\u7684\u6838\u5fc3\u75db\u70b9\uff0c\u63d0\u4f9b\u4e86\u66f4\u5065\u58ee\u3001\u53ef\u7ef4\u62a4\u548c\u53ef\u8c03\u8bd5\u7684\u5de5\u4f5c\u6d41\u6784\u5efa\u65b9\u6848\u3002", "conclusion": "Gambit\u6846\u67b6\u901a\u8fc7\u7c7b\u578b\u5316\u7684decks\u8bbe\u8ba1\uff0c\u663e\u8457\u6539\u5584\u4e86LLM\u5de5\u4f5c\u6d41\u7684\u53ef\u9760\u6027\u3001\u53ef\u7ef4\u62a4\u6027\u548c\u8c03\u8bd5\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u751f\u4ea7\u7ea7LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "topic": "code agent"}}
{"id": "tldr.2601.d3463ef8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Fagentic-handbook%2F%3Futm_source=tldrdev/1/0100019bc6cc03e6-16602682-ab60-4d0e-8a01-97728695d5bb-000000/vX7Oc-KmauwZ3T_z7ApFUpjauw1hncW6447JKyvfFNc=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Fagentic-handbook%2F%3Futm_source=tldrdev/1/0100019bc6cc03e6-16602682-ab60-4d0e-8a01-97728695d5bb-000000/vX7Oc-KmauwZ3T_z7ApFUpjauw1hncW6447JKyvfFNc=440", "authors": ["TLDR Newsletter"], "title": "The Agentic AI Handbook: Production-Ready Patterns", "comment": "Source: TLDR Newsletter, Date: 2026-01-16, Reading time: 26 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Fagentic-handbook%2F%3Futm_source=tldrdev/1/0100019bc6cc03e6-16602682-ab60-4d0e-8a01-97728695d5bb-000000/vX7Oc-KmauwZ3T_z7ApFUpjauw1hncW6447JKyvfFNc=440", "summary": "The Agentic AI Handbook: Production-Ready Patterns (26 minute read) This is a collection of 113 production-ready patterns derived from real-world systems for building and deploying reliable AI agents.", "source": "tldr", "AI": {"tldr": "\u6536\u96c6\u4e86113\u4e2a\u751f\u4ea7\u5c31\u7eea\u7684AI\u4ee3\u7406\u6784\u5efa\u4e0e\u90e8\u7f72\u6a21\u5f0f\uff0c\u57fa\u4e8e\u771f\u5b9e\u7cfb\u7edf\u7ecf\u9a8c", "motivation": "\u4e3aAI\u4ee3\u7406\u7684\u5b9e\u9645\u751f\u4ea7\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff0c\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u7cfb\u7edf\u4e2d\u7684\u53ef\u9760\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u6311\u6218", "method": "\u4ece\u771f\u5b9e\u4e16\u754c\u7cfb\u7edf\u4e2d\u6536\u96c6\u548c\u6574\u7406113\u4e2a\u751f\u4ea7\u5c31\u7eea\u7684\u6a21\u5f0f\uff0c\u5f62\u6210\u7cfb\u7edf\u5316\u7684\u5b9e\u8df5\u6307\u5357", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684AI\u4ee3\u7406\u751f\u4ea7\u90e8\u7f72\u6a21\u5f0f\u624b\u518c\uff0c\u6db5\u76d6\u6784\u5efa\u548c\u90e8\u7f72\u7684\u5404\u4e2a\u65b9\u9762", "conclusion": "\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u3001\u7ecf\u8fc7\u9a8c\u8bc1\u7684AI\u4ee3\u7406\u751f\u4ea7\u90e8\u7f72\u6a21\u5f0f\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u6784\u5efa\u53ef\u9760\u7684AI\u7cfb\u7edf", "topic": "agent analysis"}}
{"id": "tldr.2601.844d4370", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGmeZaY/1/0100019bc6ea5eda-a4a0732d-cb50-4df7-8cc4-3aa48e7906d7-000000/b1vpCXXZ3i8BGBXDRn0kUz2B6Fb-jINeP6cfTgq2CFA=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGmeZaY/1/0100019bc6ea5eda-a4a0732d-cb50-4df7-8cc4-3aa48e7906d7-000000/b1vpCXXZ3i8BGBXDRn0kUz2B6Fb-jINeP6cfTgq2CFA=440", "authors": ["TLDR Newsletter"], "title": "Inside the First Autonomous Agent Economy on Solana", "comment": "Source: TLDR Newsletter, Date: 2026-01-16, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGmeZaY/1/0100019bc6ea5eda-a4a0732d-cb50-4df7-8cc4-3aa48e7906d7-000000/b1vpCXXZ3i8BGBXDRn0kUz2B6Fb-jINeP6cfTgq2CFA=440", "summary": "Inside the First Autonomous Agent Economy on Solana (4 minute read) t54.ai has launched what it claims is the first fully autonomous agent economy on Solana, where AI agents independently transact, negotiate, and allocate resources using the x402 protocol infrastructure. Drawing lessons from Microsoft Research's Magentic Marketplace and Stanford HAI's agent society experiments, the system enables agents to discover services, execute payments, and form emergent market structures without human ...", "source": "tldr", "AI": {"tldr": "\u5728Solana\u533a\u5757\u94fe\u4e0a\u5efa\u7acb\u4e86\u9996\u4e2a\u5b8c\u5168\u81ea\u4e3b\u7684AI\u4ee3\u7406\u7ecf\u6d4e\u7cfb\u7edf\uff0c\u4ee3\u7406\u80fd\u591f\u72ec\u7acb\u4ea4\u6613\u3001\u534f\u5546\u548c\u5206\u914d\u8d44\u6e90", "motivation": "\u521b\u5efa\u65e0\u9700\u4eba\u7c7b\u5e72\u9884\u7684\u81ea\u4e3bAI\u4ee3\u7406\u7ecf\u6d4e\u7cfb\u7edf\uff0c\u8ba9AI\u4ee3\u7406\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u8fdb\u884c\u7ecf\u6d4e\u6d3b\u52a8\u548c\u5e02\u573a\u4ea4\u4e92", "method": "\u57fa\u4e8eSolana\u533a\u5757\u94fe\u548cx402\u534f\u8bae\u57fa\u7840\u8bbe\u65bd\uff0c\u501f\u9274Microsoft Research\u7684Magentic Marketplace\u548cStanford HAI\u7684\u4ee3\u7406\u793e\u4f1a\u5b9e\u9a8c\uff0c\u6784\u5efa\u81ea\u4e3b\u4ee3\u7406\u7ecf\u6d4e\u6846\u67b6", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u5728Solana\u4e0a\u7684\u5b8c\u5168\u81ea\u4e3b\u4ee3\u7406\u7ecf\u6d4e\uff0c\u4ee3\u7406\u80fd\u591f\u53d1\u73b0\u670d\u52a1\u3001\u6267\u884c\u652f\u4ed8\u5e76\u5f62\u6210\u6d8c\u73b0\u7684\u5e02\u573a\u7ed3\u6784", "conclusion": "\u6210\u529f\u5c55\u793a\u4e86AI\u4ee3\u7406\u5728\u533a\u5757\u94fe\u4e0a\u81ea\u4e3b\u8fd0\u884c\u7ecf\u6d4e\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u7684\u53bb\u4e2d\u5fc3\u5316AI\u7ecf\u6d4e\u5960\u5b9a\u4e86\u57fa\u7840", "topic": "agent analysis"}}
{"id": "tldr.2601.1b4d64fb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fx.com%2Fatoms_dev%2Fstatus%2F2011060882940575842/2/0100019bc6eb6834-8acf2aaa-3173-4689-8403-322eacf85b97-000000/QKnxUaRQ_AzL9pimRaBfQEjBYwuaOm5dbWhBNDS7STI=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fx.com%2Fatoms_dev%2Fstatus%2F2011060882940575842/2/0100019bc6eb6834-8acf2aaa-3173-4689-8403-322eacf85b97-000000/QKnxUaRQ_AzL9pimRaBfQEjBYwuaOm5dbWhBNDS7STI=440", "authors": ["TLDR Newsletter"], "title": "Vibe coding is over. Gen Z founders are vibing entire businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-16, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fx.com%2Fatoms_dev%2Fstatus%2F2011060882940575842/2/0100019bc6eb6834-8acf2aaa-3173-4689-8403-322eacf85b97-000000/QKnxUaRQ_AzL9pimRaBfQEjBYwuaOm5dbWhBNDS7STI=440", "summary": "Vibe coding is over. Gen Z founders are vibing entire businesses (Sponsor) What's changing is how new startups get built.Instead of limiting agents to writing code, new founders are deploying multi-agent systems that handle research, product, engineering, and user acquisition together. Ideas get validated, products ship, and customers get acquired in minutes, without requiring deep domain expertise. The shift is simple. You no longer scale by hiring more people. You scale by making better dec...", "source": "tldr", "AI": {"tldr": "Gen Z\u521b\u4e1a\u8005\u6b63\u5728\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u66ff\u4ee3\u4f20\u7edf\u7f16\u7a0b\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u80fd\u540c\u65f6\u5904\u7406\u7814\u7a76\u3001\u4ea7\u54c1\u3001\u5de5\u7a0b\u548c\u7528\u6237\u83b7\u53d6\uff0c\u5b9e\u73b0\u5feb\u901f\u4e1a\u52a1\u9a8c\u8bc1\u548c\u89c4\u6a21\u5316", "motivation": "\u4f20\u7edf\u7f16\u7a0b\u65b9\u5f0f\u6548\u7387\u6709\u9650\uff0c\u9700\u8981\u5927\u91cf\u4eba\u529b\u8d44\u6e90\u548c\u4e13\u4e1a\u77e5\u8bc6\u3002\u65b0\u4e00\u4ee3\u521b\u4e1a\u8005\u5e0c\u671b\u5feb\u901f\u9a8c\u8bc1\u60f3\u6cd5\u3001\u63a8\u51fa\u4ea7\u54c1\u5e76\u83b7\u53d6\u7528\u6237\uff0c\u800c\u4e0d\u9700\u8981\u6df1\u539a\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6", "method": "\u90e8\u7f72\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u534f\u540c\u5de5\u4f5c\u5904\u7406\u4ece\u7814\u7a76\u5230\u7528\u6237\u83b7\u53d6\u7684\u5168\u6d41\u7a0b\uff0c\u800c\u4e0d\u662f\u5c40\u9650\u4e8e\u7f16\u5199\u4ee3\u7801", "result": "\u5b9e\u73b0\u5206\u949f\u7ea7\u7684\u60f3\u6cd5\u9a8c\u8bc1\u3001\u4ea7\u54c1\u53d1\u5e03\u548c\u7528\u6237\u83b7\u53d6\uff0c\u4e0d\u518d\u9700\u8981\u901a\u8fc7\u96c7\u4f63\u66f4\u591a\u4eba\u5458\u6765\u5b9e\u73b0\u89c4\u6a21\u5316", "conclusion": "\u521b\u4e1a\u65b9\u5f0f\u6b63\u5728\u53d1\u751f\u6839\u672c\u6027\u8f6c\u53d8\uff1a\u4ece\u4f9d\u9760\u4eba\u529b\u89c4\u6a21\u6269\u5f20\u8f6c\u5411\u901a\u8fc7\u66f4\u597d\u7684\u51b3\u7b56\u548c\u81ea\u52a8\u5316\u7cfb\u7edf\u5b9e\u73b0\u89c4\u6a21\u5316", "topic": "code agent"}}
