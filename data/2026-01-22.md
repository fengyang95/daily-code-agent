<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 14]
- [cs.AI](#cs.AI) [Total: 12]
- [tldr.article](#tldr.article) [Total: 31]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models](https://arxiv.org/abs/2601.14270)
*Liangming Pan,Jason Liang,Jiaran Ye,Minglai Yang,Xinyuan Lu,Fengbin Zhu*

Main category: cs.CL

TL;DR: 该综述论文系统分析了LLM多步推理的内部机制，而非工程性能提升方法，提出了包含七个研究问题的概念框架，并指出了五个未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在解决需要多步推理的问题上表现出色，但其内部机制仍不明确。现有研究多关注工程方法提升性能，缺乏对内部推理机制的系统分析。

Method: 围绕七个相互关联的研究问题构建概念框架，涵盖从LLM如何在隐藏激活中执行隐式多跳推理，到语言化显式推理如何重塑内部计算等核心机制。

Result: 提出了一个系统性的机制分析框架，将LLM多步推理的内部运作机制组织成七个关键研究维度，为理解LLM推理能力提供了结构化视角。

Conclusion: 需要从机制角度深入理解LLM的多步推理能力，提出了五个未来研究方向，为后续的机理研究提供了路线图。

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities to solve problems requiring multiple reasoning steps, yet the internal mechanisms enabling such capabilities remain elusive. Unlike existing surveys that primarily focus on engineering methods to enhance performance, this survey provides a comprehensive overview of the mechanisms underlying LLM multi-step reasoning. We organize the survey around a conceptual framework comprising seven interconnected research questions, from how LLMs execute implicit multi-hop reasoning within hidden activations to how verbalized explicit reasoning remodels the internal computation. Finally, we highlight five research directions for future mechanistic studies.

</details>


### [2] [Towards Execution-Grounded Automated AI Research](https://arxiv.org/abs/2601.14525)
*Chenglei Si,Zitong Yang,Yejin Choi,Emmanuel Candès,Diyi Yang,Tatsunori Hashimoto*

Main category: cs.CL

TL;DR: 论文研究自动化AI研究中的执行落地问题，构建自动化执行器验证LLM生成的想法，通过进化搜索和强化学习从执行反馈中学习，发现进化搜索更有效而强化学习存在模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成的AI研究想法看似合理但实际效果不佳，需要执行落地来验证。研究自动化执行的可行性以及LLM能否从执行反馈中学习，以加速科学发现。

Method: 1) 构建自动化执行器实现想法并启动大规模并行GPU实验验证效果；2) 将LLM预训练和后训练转化为执行环境；3) 分析两种从执行反馈学习的方法：进化搜索和强化学习。

Result: 自动化执行器能实现前沿LLM生成的大部分想法。执行引导的进化搜索样本效率高：在10个搜索周期内，后训练方法显著优于GRPO基线(69.4% vs 48.0%)，预训练配方优于nanoGPT基线(19.7分钟 vs 35.9分钟)。强化学习存在模式崩溃问题，能提高平均奖励但无法提升上限。

Conclusion: 执行落地的自动化AI研究具有潜力，进化搜索比强化学习更有效。前沿LLM在搜索中能生成有意义的算法想法，但容易早期饱和。需要进一步分析执行想法和训练动态以推动执行落地的自动化AI研究。

Abstract: Automated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, we first build an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. We then convert two realistic research problems - LLM pre-training and post-training - into execution environments and demonstrate that our automated executor can implement a large fraction of the ideas sampled from frontier LLMs. We analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. We thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-grounded automated AI research.

</details>


### [3] [Rewarding How Models Think Pedagogically: Integrating Pedagogical Reasoning and Thinking Rewards for LLMs in Education](https://arxiv.org/abs/2601.14560)
*Unggi Lee,Jiyeong Bae,Jaehyeon Park,Haeun Park,Taejun Park,Younghoon Jeon,Sungmin Cho,Junbo Koh,Yeil Jeong,Gyeonggeon Lee*

Main category: cs.CL

TL;DR: 提出PedagogicalRL-Thinking框架，通过教学推理提示和思维奖励来优化LLM作为智能导师的内部推理过程，提升教育场景下的教学效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为智能导师的研究主要关注优化可见响应，而忽略了模型的内部思考过程。需要专门针对教育场景优化LLM的内部推理，以提升教学效果。

Method: 提出PedagogicalRL-Thinking框架：1) 教学推理提示：使用领域特定的教育理论而非通用指令来引导内部推理；2) 思维奖励：明确评估和强化模型推理轨迹的教学质量。

Result: 领域特定、理论基础的提示优于通用提示；思维奖励与教学提示结合最有效；仅在数学辅导对话上训练的模型在未见过的教育基准上表现提升，同时保留基础模型的事实知识。

Conclusion: 教学思维奖励能系统性地改变推理轨迹，增加教学推理和结构化教学决策，为优化教育场景下的LLM推理提供了有效框架。

Abstract: Large language models (LLMs) are increasingly deployed as intelligent tutoring systems, yet research on optimizing LLMs specifically for educational contexts remains limited. Recent works have proposed reinforcement learning approaches for training LLM tutors, but these methods focus solely on optimizing visible responses while neglecting the model's internal thinking process. We introduce PedagogicalRL-Thinking, a framework that extends pedagogical alignment to reasoning LLMs in education through two novel approaches: (1) Pedagogical Reasoning Prompting, which guides internal reasoning using domain-specific educational theory rather than generic instructions; and (2) Thinking Reward, which explicitly evaluates and reinforces the pedagogical quality of the model's reasoning traces. Our experiments reveal that domain-specific, theory-grounded prompting outperforms generic prompting, and that Thinking Reward is most effective when combined with pedagogical prompting. Furthermore, models trained only on mathematics tutoring dialogues show improved performance on educational benchmarks not seen during training, while preserving the base model's factual knowledge. Our quantitative and qualitative analyses reveal that pedagogical thinking reward produces systematic reasoning trace changes, with increased pedagogical reasoning and more structured instructional decision-making in the tutor's thinking process.

</details>


### [4] [SearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation](https://arxiv.org/abs/2601.14615)
*Xichen Zhang,Ziyi He,Yinghao Zhu,Sitong Wu,Shaozuo Yu,Meng Chu,Wenhu Zhang,Haoru Tan,Jiaya Jia*

Main category: cs.CL

TL;DR: SearchGym：一个用于训练搜索代理的模拟环境，通过可验证知识图谱和对齐文档语料库解决RL训练中的数据对齐问题，实现成本效益高的搜索代理开发。


<details>
  <summary>Details</summary>
Motivation: 训练搜索代理面临两难困境：使用实时商业Web API成本过高，而依赖静态数据快照会因数据不对齐引入噪声，产生损坏的奖励信号，破坏训练稳定性。

Method: 提出SearchGym模拟环境，使用严格的生成流程构建可验证知识图谱和对齐文档语料库；在此基础上引入SearchGym-RL课程学习方法，通过纯化反馈逐步优化代理策略。

Result: 在Llama和Qwen系列模型上的实验显示强大的模拟到现实泛化能力。Qwen2.5-7B-Base模型在SearchGym中训练后，在九个不同基准测试中平均相对超越ASearcher基线10.6%。

Conclusion: 高保真模拟是开发强大搜索代理的可扩展且成本效益高的方法，SearchGym环境有效解决了数据对齐问题，实现了稳健的搜索代理训练。

Abstract: Search agents have emerged as a pivotal paradigm for solving open-ended, knowledge-intensive reasoning tasks. However, training these agents via Reinforcement Learning (RL) faces a critical dilemma: interacting with live commercial Web APIs is prohibitively expensive, while relying on static data snapshots often introduces noise due to data misalignment. This misalignment generates corrupted reward signals that destabilize training by penalizing correct reasoning or rewarding hallucination. To address this, we propose SearchGym, a simulation environment designed to bootstrap robust search agents. SearchGym employs a rigorous generative pipeline to construct a verifiable knowledge graph and an aligned document corpus, ensuring that every reasoning task is factually grounded and strictly solvable. Building on this controllable environment, we introduce SearchGym-RL, a curriculum learning methodology that progressively optimizes agent policies through purified feedback, evolving from basic interactions to complex, long-horizon planning. Extensive experiments across the Llama and Qwen families demonstrate strong Sim-to-Real generalization. Notably, our Qwen2.5-7B-Base model trained within SearchGym surpasses the web-enhanced ASearcher baseline across nine diverse benchmarks by an average relative margin of 10.6%. Our results validate that high-fidelity simulation serves as a scalable and highly cost-effective methodology for developing capable search agents.

</details>


### [5] [Say Anything but This: When Tokenizer Betrays Reasoning in LLMs](https://arxiv.org/abs/2601.14658)
*Navid Ayoobi,Marcus I Armstrong,Arjun Mukherjee*

Main category: cs.CL

TL;DR: 研究发现LLM推理中的tokenization不一致问题：多个token序列可能对应相同的文本，导致模型产生"幻影编辑"等推理错误，揭示了tokenizer层是推理缺陷的重要来源。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在离散token序列上推理，但现代子词tokenizer会产生非唯一编码（多个token序列对应相同文本），这种表示不匹配导致未测量的脆弱性，使推理过程可能失败。

Method: 引入tokenization一致性探针任务：要求模型在上下文中替换指定目标词而不改变其他内容。分析超过11000个替换试验，识别tokenizer-detokenizer伪影导致的失败。

Result: 在先进开源LLM中发现非平凡比例的"幻影编辑"输出，模型在tokenizer诱导的表示缺陷下产生错误推理。识别并分类了8种系统性tokenizer伪影，包括空格边界偏移和词内重分割。

Conclusion: 部分明显的推理缺陷源于tokenizer层，这要求在训练更大模型和语料库之前，优先考虑tokenizer级别的修复方案。

Abstract: Large language models (LLMs) reason over discrete token ID sequences, yet modern subword tokenizers routinely produce non-unique encodings: multiple token ID sequences can detokenize to identical surface strings. This representational mismatch creates an unmeasured fragility wherein reasoning processes can fail. LLMs may treat two internal representations as distinct "words" even when they are semantically identical at the text level. In this work, we show that tokenization can betray LLM reasoning through one-to-many token ID mappings. We introduce a tokenization-consistency probe that requires models to replace designated target words in context while leaving all other content unchanged. The task is intentionally simple at the surface level, enabling us to attribute failures to tokenizer-detokenizer artifacts rather than to knowledge gaps or parameter limitations. Through analysis of over 11000 replacement trials across state-of-the-art open-source LLMs, we find a non-trivial rate of outputs exhibit phantom edits: cases where models operate under the illusion of correct reasoning, a phenomenon arising from tokenizer-induced representational defects. We further analyze these cases and provide a taxonomy of eight systematic tokenizer artifacts, including whitespace-boundary shifts and intra-word resegmentation. These findings indicate that part of apparent reasoning deficiency originates in the tokenizer layer, motivating tokenizer-level remedies before incurring the cost of training ever-larger models on ever-larger corpora.

</details>


### [6] [AdaTIR: Adaptive Tool-Integrated Reasoning via Difficulty-Aware Policy Optimization](https://arxiv.org/abs/2601.14696)
*Zhaiyu Fang,Ruipeng Sun*

Main category: cs.CL

TL;DR: AdaTIR框架通过难度感知推理内部化，动态调整工具使用预算，减少简单任务中97.6%的工具调用，同时在复杂任务中保持或提升准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理存在认知卸载问题，即使简单任务也冗余调用外部工具。真正的智能代理需要自适应判断何时使用工具，而非静态工具调用。

Method: 提出AdaTIR框架：1) 引入难度感知效率奖励，根据任务复杂度动态调整工具预算；2) 提出Clipped Advantage Shaping解决符号反转问题，确保正确性为主要目标，效率为次要约束。

Result: AdaTIR在简单任务上减少97.6%工具调用，复杂任务减少28.2%工具调用，同时保持或提升准确性。在AIME 2024上，即使禁用工具访问，仍超越基线4.8%。

Conclusion: AdaTIR实现了从静态工具调用到难度感知推理内部化的范式转变，显著提升代理效率，同时保持任务准确性，展示了真正的自适应智能。

Abstract: Tool-Integrated Reasoning (TIR) has significantly enhanced the capabilities of Large Language Models (LLMs), yet current agents tend to exhibit cognitive offloading, redundantly invoking external tools even for simple tasks. In this paper, we suggest that true agentic intelligence requires not just tool invocation, but the adaptive wisdom to discern when to use them. We propose AdaTIR, a framework that shifts the paradigm from static tool invocation to difficulty-aware reasoning internalization. By introducing a difficulty-aware efficiency reward, AdaTIR dynamically adjusts tool budgets based on task complexity--internalizing reasoning for simple tasks while selectively invoking tools for complex tasks. Furthermore, we identify a sign reversal problem where tool penalties outweigh correctness rewards, mistakenly penalizing correct rollouts with negative advantages. To resolve this, we propose Clipped Advantage Shaping (CAS), which ensures that correctness remains the primary objective while using efficiency as a secondary constraint. Empirical results demonstrate that AdaTIR reduces tool calls by up to 97.6% on simple tasks and 28.2% on complex challenges while maintaining or enhancing accuracy. Notably, AdaTIR successfully internalizes reasoning, outperforming baselines by 4.8% on AIME 2024 even when tool access is strictly disabled.

</details>


### [7] [Language-Coupled Reinforcement Learning for Multilingual Retrieval-Augmented Generation](https://arxiv.org/abs/2601.14896)
*Rui Qi,Fengran Mo,Yufeng Chen,Xue Zhang,Shuo Wang,Hongliang Li,Jinan Xu,Meng Jiang,Jian-Yun Nie,Kaiyu Huang*

Main category: cs.CL

TL;DR: LcRL是一个多语言检索增强强化学习框架，通过语言耦合组相对策略优化和辅助反一致性惩罚，解决多语言检索增强生成中的知识偏见和冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有多语言检索增强生成方法采用统一的"一刀切"策略，在处理不同语言查询时容易出现知识偏见和冲突，导致性能不佳。

Method: 提出LcRL框架，包含语言耦合组相对策略优化：1) 在rollout模块采用语言耦合组采样减少知识偏见；2) 在奖励模型中正则化辅助反一致性惩罚缓解知识冲突。

Result: 实验结果表明LcRL不仅达到有竞争力的性能，而且适用于多种实际场景，如受限训练数据和包含大量语言的检索集合。

Conclusion: LcRL通过解决多语言检索增强生成中的知识偏见和冲突问题，提供了一个有效的强化学习框架，适用于各种实际应用场景。

Abstract: Multilingual retrieval-augmented generation (MRAG) requires models to effectively acquire and integrate beneficial external knowledge from multilingual collections. However, most existing studies employ a unitive process where queries of equivalent semantics across different languages are processed through a single-turn retrieval and subsequent optimization. Such a ``one-size-fits-all'' strategy is often suboptimal in multilingual settings, as the models occur to knowledge bias and conflict during the interaction with the search engine. To alleviate the issues, we propose LcRL, a multilingual search-augmented reinforcement learning framework that integrates a language-coupled Group Relative Policy Optimization into the policy and reward models. We adopt the language-coupled group sampling in the rollout module to reduce knowledge bias, and regularize an auxiliary anti-consistency penalty in the reward models to mitigate the knowledge conflict. Experimental results demonstrate that LcRL not only achieves competitive performance but is also appropriate for various practical scenarios such as constrained training data and retrieval over collections encompassing a large number of languages. Our code is available at https://github.com/Cherry-qwq/LcRL-Open.

</details>


### [8] [CodeDelegator: Mitigating Context Pollution via Role Separation in Code-as-Action Agents](https://arxiv.org/abs/2601.14914)
*Tianxiang Fei,Cheng Chen,Yue Pan,Mao Zheng,Mingyang Song*

Main category: cs.CL

TL;DR: CodeDelegator是一个多智能体框架，通过角色分离将规划与实现分开，使用持久化的Delegator进行战略监督，为每个子任务实例化新的Coder智能体，并通过EPSS机制隔离执行状态以防止上下文污染。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM智能体可以将动作表示为可执行代码，但现实任务需要战略规划和详细实现。单一智能体同时处理两者会导致调试痕迹和中间失败污染上下文，损害长视野性能。

Method: 提出CodeDelegator多智能体框架：1) 持久化的Delegator负责任务分解、编写规范和监控进度但不执行代码；2) 为每个子任务实例化新的Coder智能体，拥有干净的上下文；3) 引入EPSS机制隔离Coder的执行状态同时保持全局一致性。

Result: 在多个基准测试上的实验证明了CodeDelegator在不同场景中的有效性。

Conclusion: 通过角色专业化和上下文隔离，CodeDelegator解决了单一智能体在复杂任务中的上下文污染问题，提高了长视野任务的性能。

Abstract: Recent advances in large language models (LLMs) allow agents to represent actions as executable code, offering greater expressivity than traditional tool-calling. However, real-world tasks often demand both strategic planning and detailed implementation. Using a single agent for both leads to context pollution from debugging traces and intermediate failures, impairing long-horizon performance. We propose CodeDelegator, a multi-agent framework that separates planning from implementation via role specialization. A persistent Delegator maintains strategic oversight by decomposing tasks, writing specifications, and monitoring progress without executing code. For each sub-task, a new Coder agent is instantiated with a clean context containing only its specification, shielding it from prior failures. To coordinate between agents, we introduce Ephemeral-Persistent State Separation (EPSS), which isolates each Coder's execution state while preserving global coherence, preventing debugging traces from polluting the Delegator's context. Experiments on various benchmarks demonstrate the effectiveness of CodeDelegator across diverse scenarios.

</details>


### [9] [CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning](https://arxiv.org/abs/2601.14952)
*Zhiyuan Lu,Chenliang Li,Yingcheng Shi,Weizhou Shen,Ming Yan,Fei Huang*

Main category: cs.CL

TL;DR: 提出了CorpusQA基准测试，用于评估LLM在千万token级别文档库上的推理能力，发现现有长上下文模型在输入长度增加时表现下降，检索增强系统完全失效，需要内存增强的智能体架构。


<details>
  <summary>Details</summary>
Motivation: 当前LLM虽然能处理百万token上下文，但在整个文档库级别的推理能力尚未充分测试。现有基准测试大多局限于单个长文本或依赖"稀疏检索"假设，无法评估真正的语料库级别分析能力，因为证据高度分散在数百个文档中，需要全局整合、比较和统计聚合。

Method: 引入CorpusQA基准测试，规模达1000万token，通过新颖的数据合成框架生成。该框架通过解耦推理与文本表示，创建复杂、计算密集的查询，并保证程序化生成的真实答案，挑战系统在不依赖易错人工标注的情况下对海量非结构化文本进行整体推理。

Result: 实验显示，即使最先进的长上下文LLM在输入长度增加时也表现不佳，标准检索增强生成系统完全失效。研究发现，内存增强的智能体架构提供了更稳健的替代方案。此外，在合成数据上微调能有效提升LLM的通用长上下文推理能力。

Conclusion: 需要从简单扩展上下文窗口转向开发先进架构以实现全局信息合成。内存增强的智能体架构是更稳健的解决方案，表明在文档库级别推理方面需要架构层面的根本转变。

Abstract: While large language models now handle million-token contexts, their capacity for reasoning across entire document repositories remains largely untested. Existing benchmarks are inadequate, as they are mostly limited to single long texts or rely on a "sparse retrieval" assumption-that answers can be derived from a few relevant chunks. This assumption fails for true corpus-level analysis, where evidence is highly dispersed across hundreds of documents and answers require global integration, comparison, and statistical aggregation. To address this critical gap, we introduce CorpusQA, a new benchmark scaling up to 10 million tokens, generated via a novel data synthesis framework. By decoupling reasoning from textual representation, this framework creates complex, computation-intensive queries with programmatically guaranteed ground-truth answers, challenging systems to perform holistic reasoning over vast, unstructured text without relying on fallible human annotation. We further demonstrate the utility of our framework beyond evaluation, showing that fine-tuning on our synthesized data effectively enhances an LLM's general long-context reasoning capabilities. Extensive experiments reveal that even state-of-the-art long-context LLMs struggle as input length increases, and standard retrieval-augmented generation systems collapse entirely. Our findings indicate that memory-augmented agentic architectures offer a more robust alternative, suggesting a critical shift is needed from simply extending context windows to developing advanced architectures for global information synthesis.

</details>


### [10] [\textsc{LogicScore}: Fine-grained Logic Evaluation of Conciseness, Completeness, and Determinateness in Attributed Question Answering](https://arxiv.org/abs/2601.15050)
*Zhichao Yan,Yunxiao Zhao,Jiapu Wang,Jiaoyan Chen,Shaoru Guo,Xiaoli Li,Ru Li,Jeff Z. Pan*

Main category: cs.CL

TL;DR: LogicScore是一个评估框架，用于解决归因问答中的"归因短视"问题，通过全局逻辑完整性评估来补充现有的局部归因验证。


<details>
  <summary>Details</summary>
Motivation: 当前归因问答评估方法存在"归因短视"问题，只关注孤立陈述的验证而忽视长答案的全局逻辑完整性，导致LLM产生事实正确但逻辑不连贯的回答。

Method: 基于Horn规则构建LogicScore框架，采用后向验证机制系统评估三个推理维度：完整性（逻辑推导）、简洁性（非冗余性）和确定性（一致答案蕴含）。

Result: 在三个多跳QA数据集和20多个LLM上的实验显示，领先模型在归因得分上表现良好（如Gemini-3 Pro精度92.85%），但在全局推理质量上较差（如Gemini-3 Pro简洁性仅35.11%）。

Conclusion: LogicScore为逻辑评估建立了稳健标准，强调在LLM开发中需要同时关注推理连贯性和事实基础，揭示了当前模型在全局推理能力上的重要差距。

Abstract: Current evaluation methods for Attributed Question Answering (AQA) suffer from \textit{attribution myopia}: they emphasize verification of isolated statements and their attributions but overlook the global logical integrity of long-form answers. Consequently, Large Language Models (LLMs) often produce factually grounded yet logically incoherent responses with elusive deductive gaps. To mitigate this limitation, we present \textsc{LogicScore}, a unified evaluation framework that shifts the paradigm from local assessment to global reasoning scrutiny. Grounded in Horn Rules, our approach integrates a backward verification mechanism to systematically evaluate three key reasoning dimensions: \textit{Completeness} (logically sound deduction), \textit{Conciseness} (non-redundancy), and \textit{Determinateness} (consistent answer entailment). Extensive experiments across three multi-hop QA datasets (HotpotQA, MusiQue, and 2WikiMultiHopQA) and over 20 LLMs (including GPT-5, Gemini-3-Pro, LLaMA3, and task-specific tuned models) reveal a critical capability gap: leading models often achieve high attribution scores (e.g., 92.85\% precision for Gemini-3 Pro) but struggle with global reasoning quality (e.g., 35.11\% Conciseness for Gemini-3 Pro). Our work establishes a robust standard for logical evaluation, highlighting the need to prioritize reasoning coherence alongside factual grounding in LLM development. Codes are available at: https://github.com/zhichaoyan11/LogicScore.

</details>


### [11] [Multi-Agent Constraint Factorization Reveals Latent Invariant Solution Structure](https://arxiv.org/abs/2601.15077)
*Christopher Scofield*

Main category: cs.CL

TL;DR: 本文从算子理论和约束优化的角度，为多智能体系统（MAS）相比单智能体在相同信息下表现更优的现象提供了形式化解释。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统由大语言模型组成，尽管处理相同信息，却常常展现出更好的问题解决能力。本文旨在为这一现象提供严格的理论解释。

Method: 将每个智能体建模为对共享解状态施加不同有效性约束的算子，证明MAS实现了约束执行算子的分解组合。在温和条件下，这些动态收敛到由智能体约束集交集定义的解集。

Result: 这种不变结构通常无法被单个智能体同时应用所有约束时动态访问，即使表达能力和信息相同。将结果从精确约束执行扩展到软约束（通过邻近算子），并应用于当代基于文本的对话系统。

Conclusion: 多智能体系统的优势源于约束的分解执行，使得收敛到更优解集成为可能，这为理解MAS性能提升提供了理论基础。

Abstract: Multi-agent systems (MAS) composed of large language models often exhibit improved problem-solving performance despite operating on identical information. In this work, we provide a formal explanation for this phenomenon grounded in operator theory and constrained optimization. We model each agent as enforcing a distinct family of validity constraints on a shared solution state, and show that a MAS implements a factorized composition of constraint-enforcement operators. Under mild conditions, these dynamics converge to invariant solution sets defined by the intersection of agent constraint sets. Such invariant structures are generally not dynamically accessible to a single agent applying all constraints simultaneously, even when expressive capacity and information are identical. We extend this result from exact constraint enforcement to soft constraints via proximal operators, and apply the formalism to contemporary text-based dialog systems.

</details>


### [12] [Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems](https://arxiv.org/abs/2601.15161)
*Yinzhu Chen,Abdine Maiga,Hossein A. Rahmani,Emine Yilmaz*

Main category: cs.CL

TL;DR: 提出检索增强的多智能体框架，自动生成针对临床决策支持LLM的实例特定评估准则，显著提升评估准确性和响应质量


<details>
  <summary>Details</summary>
Motivation: LLM在临床决策支持中存在幻觉和不安全建议的风险，这些风险表现为难以被通用指标检测的细微临床错误，而专家编写的细粒度评估准则成本高且难以扩展

Method: 采用检索增强的多智能体框架，从权威医学证据中检索内容，分解为原子事实，结合用户交互约束合成可验证的细粒度评估准则

Result: 在HealthBench上达到60.12%的临床意图对齐分数，显著优于GPT-4o基线(55.16%)；评估准则在判别测试中平均分数差为8.658，AUROC为0.977；还能指导响应优化，将质量提升9.2%

Conclusion: 该框架为评估和改进医学LLM提供了可扩展且透明的基础，能够自动生成高质量的评估准则，有效检测临床错误并指导模型改进

Abstract: Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta ($μ_Δ = 8.658$) and an AUROC of 0.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0% to 68.2%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.

</details>


### [13] [The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models](https://arxiv.org/abs/2601.15165)
*Zanlin Ni,Shenzhi Wang,Yang Yue,Tianyu Yu,Weilin Zhao,Yeguo Hua,Tianyi Chen,Jun Song,Cheng Yu,Bo Zheng,Gao Huang*

Main category: cs.CL

TL;DR: dLLMs的任意顺序生成能力反而限制了其推理边界，导致解决方案空间过早坍缩。放弃任意顺序生成，采用标准GRPO方法反而能更有效地激发dLLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为dLLMs的任意顺序生成能力理论上能提供更大的解决方案空间，从而在数学和编码等任务上具有优势。但作者发现这种灵活性实际上限制了dLLMs的推理能力。

Method: 提出JustGRPO方法，放弃dLLMs的任意顺序生成能力，采用标准的Group Relative Policy Optimization（GRPO）来训练模型，同时保留dLLMs的并行解码能力。

Result: JustGRPO方法在GSM8K数学推理任务上达到89.1%的准确率，效果显著优于现有基于任意顺序生成的RL方法。

Conclusion: dLLMs的任意顺序生成能力在当前形式下反而限制了其推理潜力，放弃这种灵活性并采用标准GRPO方法能更有效地激发dLLMs的推理能力。

Abstract: Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap

</details>


### [14] [Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models](https://arxiv.org/abs/2601.15220)
*Anmol Goel,Cornelius Emde,Sangdoo Yun,Seong Joon Oh,Martin Gubri*

Main category: cs.CL

TL;DR: 研究发现前沿语言模型在良性微调后会出现隐私崩溃现象，即模型在保持标准安全性和实用性基准性能的同时，严重丧失上下文隐私保护能力。


<details>
  <summary>Details</summary>
Motivation: 识别语言模型中一个未被充分认识的现象：即使是良性的微调也可能导致隐私保护能力的系统性退化，而现有安全评估方法无法检测到这种"静默失败"。

Method: 通过实验研究六个模型（闭源和开源）、五个微调数据集（真实世界和受控数据）和两类任务（代理型和记忆型），分析多种微调模式对隐私保护的影响，并进行机制分析。

Result: 发现隐私崩溃现象普遍存在，微调后的模型失去上下文隐私规范推理能力，不当共享信息给工具，跨上下文违反内存边界，而隐私表征相比任务相关特征更加脆弱。

Conclusion: 当前安全评估存在关键缺口，特别是对于专用代理的部署，需要开发新的隐私保护评估方法来检测和防止隐私崩溃。

Abstract: We identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse. We find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective dialogue, and debugging code printing internal variables, among others. Fine-tuned models lose their ability to reason about contextual privacy norms, share information inappropriately with tools, and violate memory boundaries across contexts. Privacy collapse is a ``silent failure'' because models maintain high performance on standard safety and utility benchmarks whilst exhibiting severe privacy vulnerabilities. Our experiments show evidence of privacy collapse across six models (closed and open weight), five fine-tuning datasets (real-world and controlled data), and two task categories (agentic and memory-based). Our mechanistic analysis reveals that privacy representations are uniquely fragile to fine-tuning, compared to task-relevant features which are preserved. Our results reveal a critical gap in current safety evaluations, in particular for the deployment of specialised agents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration](https://arxiv.org/abs/2601.14440)
*Saeed Khaki,Ashudeep Singh,Nima Safaei,Kamal Ginotra*

Main category: cs.AI

TL;DR: 该论文研究了视觉语言模型在数学推理中的模态差距问题，提出了VisTIRA工具集成推理框架和LaTeX转换管道，发现工具监督和OCR基础能提升视觉数学推理能力，模态差距与模型规模呈负相关。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在数学推理任务中，当问题以图像形式呈现时，性能明显低于纯文本形式。这种模态差距源于模型在读取密集公式、布局和混合符号-图表上下文时的复合失败。需要开发新方法来缩小这一差距。

Method: 1. 提出VisTIRA框架：工具集成推理代理，将数学问题图像迭代分解为自然语言推理和可执行的Python步骤；2. 构建LaTeX管道：将链式思维数学语料转换为具有挑战性的图像对应物；3. 创建合成工具使用轨迹：基于真实世界作业风格图像数据集进行微调。

Result: 工具集成监督能提升基于图像的推理能力，OCR基础能进一步缩小较小模型的模态差距（但在大规模模型中收益递减）。模态差距严重程度与模型规模呈负相关，结构化推理和OCR基础是互补策略。

Conclusion: 视觉数学推理的模态差距可以通过结构化推理框架和OCR基础技术来缓解。工具集成监督和视觉基础是提升视觉语言模型数学推理能力的有效方法，特别是对于较小模型。

Abstract: Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.

</details>


### [16] [On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL](https://arxiv.org/abs/2601.14456)
*Valerio Belcamino,Nicholas Attolino,Alessio Capitanelli,Fulvio Mastrogiovanni*

Main category: cs.AI

TL;DR: 研究发现微调LLM在PDDL规划任务上虽然能达到高有效计划率，但主要依赖领域特定模式而非可迁移的规划能力，跨领域泛化性能几乎为零。


<details>
  <summary>Details</summary>
Motivation: 探究微调后的大型语言模型在PDDL规划任务中表现出的高有效计划率，究竟是反映了可迁移的规划能力，还是仅仅是领域特定的记忆效应。

Method: 在10个IPC 2023领域的40,000个领域-问题-计划元组上微调1.7B参数LLM，并引入三种诊断干预：实例级符号匿名化、紧凑计划序列化、使用VAL验证器作为成功导向强化信号的验证器奖励微调。

Result: 模型在领域内达到82.9%有效计划率，但在两个未见领域上为0%。符号匿名化和紧凑序列化导致性能显著下降，验证器奖励微调在监督训练一半的周期内达到性能饱和，但未改善跨领域泛化。

Conclusion: 微调模型严重依赖领域特定模式而非可迁移的规划能力，领域内性能在80%左右达到平台期，而跨领域性能崩溃，突显了LLM基于规划中存在的持续泛化差距。

Abstract: Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains, and evaluate both in-domain and cross-domain generalization. While the model reaches 82.9% valid plan rate in in-domain conditions, it achieves 0% on two unseen domains. To analyze this failure, we introduce three diagnostic interventions, namely (i) instance-wise symbol anonymization, (ii) compact plan serialization, and (iii) verifier-reward fine-tuning using the VAL validator as a success-focused reinforcement signal. Symbol anonymization and compact serialization cause significant performance drops despite preserving plan semantics, thus revealing strong sensitivity to surface representations. Verifier-reward fine-tuning reaches performance saturation in half the supervised training epochs, but does not improve cross-domain generalization. For the explored configurations, in-domain performance plateaus around 80%, while cross-domain performance collapses, suggesting that our fine-tuned model relies heavily on domain-specific patterns rather than transferable planning competence in this setting. Our results highlight a persistent generalization gap in LLM-based planning and provide diagnostic tools for studying its causes.

</details>


### [17] [Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree](https://arxiv.org/abs/2601.14523)
*Leyi Zhao,Weijie Huang,Yitong Guo,Jiang Bian,Chenghong Wang,Xuhong Zhang*

Main category: cs.AI

TL;DR: PhyloEvolve是一个LLM代理系统，将GPU算法优化重新定义为上下文强化学习问题，通过算法蒸馏和决策变换器利用优化轨迹信息，使用系统发育树组织优化历史，在科学计算任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前GPU科学计算算法优化过程劳动密集且迭代繁琐，现有LLM辅助进化方法主要依赖结果选择和随机突变，未能充分利用迭代优化过程中产生的丰富轨迹信息。

Method: 将GPU算法优化重新定义为上下文强化学习问题，整合算法蒸馏和基于提示的决策变换器，引入系统发育树表示来组织优化历史，结合精英轨迹池、多岛并行探索和容器化执行来平衡探索与利用。

Result: 在偏微分方程求解器、流形学习和谱图算法等科学计算任务中，相比基线和进化方法，在运行时间、内存效率和正确性方面均取得一致改进。

Conclusion: PhyloEvolve通过轨迹条件化的经验重用，无需模型重新训练即可有效优化GPU算法，系统发育树表示提供了组织优化历史的有效框架，为自动化算法优化提供了新方法。

Abstract: Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve

</details>


### [18] [MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks](https://arxiv.org/abs/2601.14652)
*Zixuan Ke,Yifei Ming,Austin Xu,Ryan Chin,Xuan-Phi Nguyen,Prathyusha Jwalapuram,Semih Yavuz,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出MAS-Orchestra框架，将多智能体系统编排建模为函数调用强化学习问题，并引入MASBENCH基准来系统评估多智能体系统的优势条件。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统设计方法存在两个关键问题：1) 方法复杂性 - 采用顺序、代码级执行限制了全局系统级整体推理，难以扩展；2) 效果不确定性 - 部署时不清楚相比单智能体系统是否有实际优势。

Method: MAS-Orchestra将复杂的目标导向子智能体抽象为可调用函数，通过函数调用强化学习实现整体编排，一次性生成整个多智能体系统。同时提出MASBENCH基准，从深度、视野、广度、并行性和鲁棒性五个维度刻画任务特征。

Result: 分析表明多智能体系统的优势取决于任务结构、验证协议以及编排器和子智能体的能力，而非普遍适用。MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公共基准上取得了一致的改进。

Conclusion: MAS-Orchestra和MASBENCH共同促进了多智能体系统的更好训练和理解，推动了多智能体智能的发展。

Abstract: While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.

</details>


### [19] [Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems](https://arxiv.org/abs/2601.14662)
*Shuhua Yang,Jiahao Zhang,Yilong Wang,Dongwon Lee,Suhang Wang*

Main category: cs.AI

TL;DR: AGEA攻击框架能在有限查询预算下有效窃取GraphRAG系统的知识图谱结构，恢复高达90%的实体和关系，揭示了现有GraphRAG系统对结构化攻击的高度脆弱性。


<details>
  <summary>Details</summary>
Motivation: 虽然已知GraphRAG系统可能泄露检索到的子图，但在实际查询预算下，能否高效重建隐藏的图结构尚未被充分研究。本文旨在探索在预算受限的黑盒设置中，攻击者能否通过自适应查询窃取系统的潜在实体-关系图。

Method: 提出AGEA（Agentic Graph Extraction Attack）框架，采用新颖性引导的探索-利用策略，结合外部图记忆模块，以及两阶段图提取流程（轻量级发现+LLM过滤）。在医疗、农业和文学数据集上，对Microsoft-GraphRAG和LightRAG系统进行评估。

Result: 在相同查询预算下，AGEA显著优于现有攻击基线，能恢复高达90%的实体和关系，同时保持高精度。这表明现代GraphRAG系统即使在严格查询限制下，也对结构化、智能化的提取攻击高度脆弱。

Conclusion: GraphRAG系统面临严重的安全威胁，AGEA攻击框架能有效窃取其知识图谱结构，需要加强系统防御措施。

Abstract: Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.

</details>


### [20] [Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation](https://arxiv.org/abs/2601.14691)
*Muhammad Khalifa,Lajanugen Logeswaran,Jaekyeom Kim,Sungryull Sohn,Yunxiang Zhang,Moontae Lee,Hao Peng,Lu Wang,Honglak Lee*

Main category: cs.AI

TL;DR: LLM作为评估代理性能的裁判在不可验证场景中存在脆弱性：通过系统性地重写代理的思维链（保持行动和观察不变），可以操纵LLM裁判的判断，使误判率提升高达90%。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为裁判评估代理性能时，隐含假设代理的思维链能忠实反映其内部推理和环境状态。本研究旨在验证这一假设的脆弱性，揭示LLM裁判易受推理轨迹操纵的问题。

Method: 通过系统性地重写代理的思维链（保持行动和观察不变），研究操纵策略：包括仅改变推理呈现方式的风格操纵，以及伪造任务进展信号的内容操纵。在800个多样化网络任务轨迹上进行实验，评估提示技术和增加计算资源对脆弱性的缓解效果。

Result: 1. 操纵推理轨迹可使最先进的VLM裁判误判率提升高达90%；2. 内容操纵比风格操纵更有效；3. 提示技术和增加计算资源能减少但无法完全消除对操纵的敏感性。

Conclusion: LLM作为裁判的评估存在根本性脆弱性，需要开发能够验证推理声明与可观察证据一致性的裁判机制。

Abstract: Large language models (LLMs) are increasingly used as judges to evaluate agent performance, particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute, which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.

</details>


### [21] [CI4A: Semantic Component Interfaces for Agents Empowering Web Automation](https://arxiv.org/abs/2601.14790)
*Zhi Qiu,Jiazheng Sun,Chenxiao Xia,Jun Zheng,Xin Peng*

Main category: cs.AI

TL;DR: CI4A提出了一种为智能体优化的组件接口，将复杂的UI交互逻辑封装为统一的工具原语，显著提升了智能体在Web任务中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在高层语义规划方面表现出色，但在处理细粒度的Web组件操作方面仍有局限。现有研究多通过强化学习等技术增强模型的基础能力，但本文认为应该为智能体构建专门的交互接口，而不是让智能体适应人类为中心的界面。

Method: 提出了CI4A（Component Interface for Agent）语义封装机制，将UI组件的复杂交互逻辑抽象为一组统一的工具原语。在Ant Design工业级前端框架中实现了CI4A，覆盖23类常用UI组件。开发了混合智能体，其动作空间能根据页面状态动态更新，灵活调用可用的CI4A工具。

Result: 基于CI4A集成的Ant Design重构并升级了WebArena基准测试。实验结果显示，基于CI4A的智能体显著优于现有方法，达到了86.3%的SOTA任务成功率，同时执行效率也有显著提升。

Conclusion: 通过为智能体专门设计优化的交互接口（CI4A），能够有效解决大语言模型在细粒度Web组件操作方面的局限性，显著提升智能体在Web任务中的性能和效率。

Abstract: While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.

</details>


### [22] [The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems](https://arxiv.org/abs/2601.15059)
*Oleg Romanchuk,Roman Bondar*

Main category: cs.AI

TL;DR: 论文识别了现代CI/CD管道中责任归属的结构性失效问题：当AI代理生成代码的决策吞吐量超过人类验证能力时，会形成"责任真空"，即无人同时具备批准决策的权威和真正理解决策依据的认知能力。


<details>
  <summary>Details</summary>
Motivation: 现代CI/CD管道集成AI代理生成代码时，虽然决策通过正式正确的审批流程执行，但没有任何实体同时拥有批准决策的权威和真正理解决策依据的认知能力。这种结构性缺陷导致责任归属失败。

Method: 定义了"责任真空"概念，分析了包含并行代理生成、CI验证和个性化人工审批的部署场景。识别了吞吐量阈值，超过该阈值后验证不再作为决策标准，而是被基于代理信号的仪式化审批取代。进一步描述了CI放大动态，即增加自动化验证覆盖率会提高代理信号密度而不恢复人类能力。

Result: 识别了标准部署假设下的扩展限制：超过特定吞吐量阈值后，验证功能失效，个性化责任变得结构上不可实现。CI放大动态加速了认知卸载，扩大了正式批准与认知理解之间的差距。额外自动化反而放大了责任真空。

Conclusion: 除非组织明确重新设计决策边界或将责任从个体决策重新分配到批次或系统级别所有权，否则责任真空在规模化代理部署中仍将是一个隐形但持久的失效模式。

Abstract: Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.
  We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.
  We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.
  We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.
  We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.

</details>


### [23] [The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution](https://arxiv.org/abs/2601.15075)
*Chen Qian,Peng Wang,Dongrui Liu,Junyao Yang,Dadi Guo,Ling Tang,Jilin Mei,Qihan Ren,Shuai Shao,Yong Liu,Jie Fu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: 提出一个通用智能体归因框架，用于识别驱动智能体行为的内部因素，无论任务结果如何，通过分层方法定位关键交互步骤和文本证据。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在客服、网页导航、软件工程等实际应用中广泛部署，理解智能体为何采取特定行动对于问责和治理变得越来越重要。现有研究主要关注失败归因，不足以解释智能体行为背后的推理过程。

Method: 提出分层通用智能体归因框架：在组件级别使用时间似然动态识别关键交互步骤；在句子级别使用基于扰动的分析来精确定位特定文本证据。

Result: 实验验证表明，该框架能够可靠地识别驱动智能体行为的关键历史事件和句子，涵盖标准工具使用和内存诱导偏差等微妙可靠性风险。

Conclusion: 该框架为实现更安全、更可问责的智能体系统迈出了关键一步，能够解释智能体行为背后的推理过程，而不仅仅是失败归因。

Abstract: Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining the reasoning behind agent behaviors. To bridge this gap, we propose a novel framework for \textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems.

</details>


### [24] [Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories](https://arxiv.org/abs/2601.15120)
*Qian Xiong,Yuekai Huang,Yujia Zheng,Tianhao Li,Ziyou Jiang,Zhiyuan Chang,Zhaoyang Li,Huanxiang Feng,Mingyang Li*

Main category: cs.AI

TL;DR: RISE提出了一种"从真实到虚拟"的方法，通过基于已验证工具原语合成虚拟轨迹和生成负样本，来缓解LLM工具使用代理中的意图偏差问题，显著提升了任务完成和意图对齐性能。


<details>
  <summary>Details</summary>
Motivation: LLM工具使用代理在实际应用中经常出现意外行为或结果，其中"意图偏差"问题严重阻碍了可靠评估和性能改进。现有方法要么依赖成本高昂的真实系统样本，要么使用LLM模拟的虚拟数据但存在分布偏移问题，且都缺乏针对意图偏差场景的负样本。

Method: RISE采用"从真实到虚拟"方法：基于已验证工具原语合成虚拟轨迹，通过对关键参数进行突变生成多样化的负样本，然后使用合成数据通过两阶段训练对骨干LLM进行微调以实现意图对齐。

Result: RISE在8个涵盖用户需求、执行轨迹和代理响应的指标上取得了有希望的结果。与训练结合后，在Acctask（任务完成）上平均提升35.28%，在Accintent（意图对齐）上平均提升23.27%，分别比SOTA基线高出1.20-42.09%和1.17-54.93%。

Conclusion: RISE通过创新的真实到虚拟数据合成方法，有效解决了LLM工具使用代理中的意图偏差问题，显著提升了代理的可靠性和意图对齐能力。

Abstract: LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of "intent deviation" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a "Real-to-Virtual" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.

</details>


### [25] [How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework](https://arxiv.org/abs/2601.15153)
*Choro Ulan uulu,Mikhail Kulyabin,Iris Fuhrmann,Jan Joosten,Nuno Miguel Martins Pacheco,Filippos Petridis,Rebecca Johnson,Jan Bosch,Helena Holmström Olsson*

Main category: cs.AI

TL;DR: 本文提出一个软件工程框架，通过增强大型语言模型，将人类领域知识嵌入AI代理系统，用于仿真数据可视化生成，使非专家能达到专家级效果。


<details>
  <summary>Details</summary>
Motivation: 关键领域知识通常掌握在少数专家手中，造成组织可扩展性和决策瓶颈。非专家难以创建有效可视化，导致次优见解并占用专家时间。

Method: 提出软件工程框架，通过增强LLM，集成请求分类器、检索增强生成系统、编码专家规则和可视化设计原则，构建具有自主、反应、主动和社交行为的代理。

Result: 在5个跨工程领域的场景中，与12名评估者测试显示输出质量提升206%，代理在所有案例中达到专家级评分，而基线表现较差，同时保持更优的代码质量和更低方差。

Conclusion: 贡献包括：自动化的基于代理的可视化生成系统，以及经过验证的系统化捕获人类领域知识、将隐性专家知识编码到AI代理的框架，证明非专家能在专业领域达到专家级成果。

Abstract: Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline's poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.

</details>


### [26] [Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning](https://arxiv.org/abs/2601.15160)
*Yuval Kansal,Niraj K. Jha*

Main category: cs.AI

TL;DR: 提出一种基于知识图谱路径奖励的强化学习后训练方法，通过将模型在短跳推理路径上训练，使其能够零样本泛化到复杂多跳推理任务，在医学领域显著超越GPT-5.2等前沿系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和编程等结构化推理领域已达到接近专家水平，但在专业科学领域进行组合式多跳推理的能力仍然有限。需要一种能够将模型基于领域公理事实进行组合推理的方法。

Method: 提出自底向上的学习范式：1) 基于监督微调和强化学习的后训练流程；2) 使用知识图谱作为隐式奖励模型；3) 从知识图谱路径推导奖励信号，鼓励模型组合中间公理而非仅优化最终答案；4) 在医学领域训练14B模型，使用短跳推理路径(1-3跳)训练，评估其在复杂多跳查询(4-5跳)上的零样本泛化能力。

Result: 路径推导的奖励作为"组合桥梁"，使模型在最具挑战性的推理任务上显著超越更大模型和GPT-5.2、Gemini 3 Pro等前沿系统。方法对选项洗牌压力测试具有鲁棒性。

Conclusion: 将推理过程基于结构化知识是实现智能推理的可扩展且高效的路径。知识图谱路径奖励能够有效促进组合推理能力的发展。

Abstract: Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a "compositional bridge", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [27] [Training CLI Agents with Synthetic Data and RL](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeveloper.nvidia.com%2Fblog%2Fhow-to-train-an-ai-agent-for-command-line-tasks-with-synthetic-data-and-reinforcement-learning%2F%3Futm_source=tldrai/1/0100019bdbc47291-e2d4f56c-f804-4025-b2c6-b0aee0d0059e-000000/P7pBSdLqGL5ZQ3lClc6TXwGk_NwI3EuCxQLl1txR0CU=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Nvidia训练了一个无先验知识的推理模型，通过合成数据和可验证奖励的强化学习，安全使用LangGraph CLI执行复杂任务（如启动服务器和构建Docker容器）


<details>
  <summary>Details</summary>
Motivation: 当前CLI代理需要大量先验知识或人工监督才能安全执行复杂系统操作。研究旨在开发一个无需先验知识、通过合成数据和强化学习就能安全使用CLI工具的智能代理

Method: 使用合成数据训练推理模型，结合可验证奖励的强化学习（RL），让模型学习安全使用LangGraph CLI执行复杂任务

Result: 成功训练出能够安全执行复杂CLI任务的代理，如启动服务器和构建Docker容器，无需先验知识或人工监督

Conclusion: 合成数据与可验证奖励的强化学习相结合，是训练安全、有效的CLI代理的有效方法，为自动化复杂系统操作提供了新途径

Abstract: Training CLI Agents with Synthetic Data and RL (12 minute read) Nvidia showcases how a reasoning model with no prior knowledge can be taught to safely use the LangGraph CLI for complex tasks like spinning up servers and building Docker containers, using synthetic data, and Reinforcement Learning with Verifiable Rewards.

</details>


### [28] [Got questions about the OWASP Top 10 for Agentic Applications?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzenity.io%2Fresources%2Fwebinars%2Finside-the-owasp-top-10-for-agentic-applications%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=tldr%26utm_content=ai-secondary-jan20/1/0100019bdbc47291-e2d4f56c-f804-4025-b2c6-b0aee0d0059e-000000/3bXxtn_VOhVukV23FamFVHXpJdRERsZHPf-4zQWcdH4=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Zenit公司举办关于OWASP Top 10 for Agentic Applications的现场问答活动，邀请参与编写首个自主AI代理风险框架的安全研究人员解答问题


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理在生产环境中的部署增加，需要行业首个经过同行评审的风险框架来指导安全实践，OWASP Top 10 for Agentic Applications应运而生

Method: 通过举办现场问答活动（AMA），邀请参与编写风险框架的安全研究人员（Chris Hughes, Steve Wilson, Michael Bargury, Kayla Underkoffler）直接回答社区关于目标劫持、工具滥用、内存中毒等安全问题的疑问

Result: 提供了行业首个针对自主AI代理的同行评审风险框架，涵盖了目标劫持、工具滥用、内存中毒等关键安全风险，并通过现场互动方式促进知识传播

Conclusion: 自主AI代理的安全风险需要专门的框架来识别和缓解，OWASP Top 10 for Agentic Applications为此提供了重要指导，现场问答活动有助于推动安全最佳实践的采用

Abstract: Got questions about the OWASP Top 10 for Agentic Applications? (Sponsor) Here's where you get answers: Zenity is hosting a live AMA on Wed. 1/28 with the security researchers (Chris Hughes, Steve Wilson, Michael Bargury, and Kayla Underkoffler) who helped write the industry's first peer-reviewed risk framework for autonomous AI agents -- including contributors who led entries on goal hijacking, tool misuse, and memory poisoning. Bring your hardest questions about securing agents in production...

</details>


### [29] [Natural Language to Solver-Ready Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2Fmicrosoft%2Foptimind%3Futm_source=tldrai/1/0100019bdbc47291-e2d4f56c-f804-4025-b2c6-b0aee0d0059e-000000/QZE0Lrdmvg94h3MvpL1iJ4BQR9YaGliWySMrJfHkv7Y=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OptiMind将自然语言优化问题转换为数学公式，简化传统繁琐的专家建模过程


<details>
  <summary>Details</summary>
Motivation: 优化工作流中的建模步骤传统上缓慢且需要专家参与，需要简化这一过程

Method: 将自然语言优化问题翻译成求解器就绪的数学代码

Result: 开发了OptiMind系统，能够自动将自然语言描述转换为数学优化模型

Conclusion: 自然语言到求解器代码的转换可以显著简化优化工作流程

Abstract: Natural Language to Solver-Ready Code (4 minute read) Microsoft's OptiMind translates natural language optimization problems into mathematical formulations. It was designed to streamline the traditionally slow and expert-heavy modeling step in optimization workflows.

</details>


### [30] [Skills.sh](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fskills.sh%2F%3Futm_source=tldrnewsletter/1/0100019be04be402-12dda4bd-ad1d-41e2-b8a8-f6a660cd0a18-000000/OqWCCeyX60yiAMPiwd-vG9zTSJR9RqWJRO8Jneuokno=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Skills.sh是一个开放的AI代理技能生态系统，提供数千个可重用技能，用户可以通过单一命令安装来增强代理能力


<details>
  <summary>Details</summary>
Motivation: 为AI代理创建可重用能力库，解决代理功能扩展和知识获取的标准化问题，促进代理技能的共享和生态系统发展

Method: 构建技能索引网站，提供可搜索的技能库，每个技能包含安装统计，支持一键安装机制

Result: 创建了包含数千个技能的开放生态系统，提供安装统计和搜索功能，实现了技能的可发现性和便捷安装

Conclusion: Skills.sh成功建立了AI代理技能生态系统，通过标准化技能格式和安装机制，促进了代理能力的扩展和社区协作

Abstract: Skills.sh (Website) Skills are reusable capabilities for AI agents. This site contains an open agent skills ecosystem with a collection of skills anyone can install with a single command to enhance their agents with access to procedural knowledge. The site contains thousands of skills. The index is searchable and shows how many times each skill has been installed.

</details>


### [31] [Agentic AI and The Mythical Agent-Month](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmuratbuffalo.blogspot.com%2F2026%2F01%2Fagentic-ai-and-mythical-agent-month.html%3Futm_source=tldrnewsletter/1/0100019be04be402-12dda4bd-ad1d-41e2-b8a8-f6a660cd0a18-000000/SOMSs0Ibe-aSwbnZQaOqy-oI4QX7lcl9dN6IsqvCkA4=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文挑战了"可扩展代理"概念，指出软件工程不是可高度并行化的任务，增加代理数量反而可能降低效率


<details>
  <summary>Details</summary>
Motivation: 当前AI代理领域流行"可扩展代理"概念，认为开发者可以启动数千个代理来加速任务完成，但这基于软件工程是高度并行任务的错误假设

Method: 通过分析软件工程的特性和类比传统软件开发中的"人月神话"概念，论证上下文加载不等于共同知识，读取token不等于理解系统变更的因果链

Result: 揭示了软件工程任务的复杂性和上下文依赖性，表明简单增加代理数量无法线性提升效率，反而可能因协调成本而降低效率

Conclusion: AI代理领域需要重新思考"可扩展代理"的假设，软件工程不是可高度并行化的任务，需要更精细的协调机制和上下文理解

Abstract: Agentic AI and The Mythical Agent-Month (6 minute read) The concept of 'Scalable Agency', where a developer could theoretically spin up thousands of agents to complete tasks faster, relies on a flawed assumption that software engineering is an embarrassingly parallel task. Context loading is not the same as common knowledge, and reading tokens is not the same as understanding the causal chain of changes across a system. Adding manpower to a late software project makes it later. Multi-agent sy...

</details>


### [32] [The Tragedy of the Agentic Commons](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strangeloopcanon.com%2Fp%2Fthe-tragedy-of-the-agentic-commons%3Futm_source=tldrnewsletter/1/0100019be04be402-12dda4bd-ad1d-41e2-b8a8-f6a660cd0a18-000000/HO_40g00IohlEc50TjqfDCs2xnAxKO-lxyYbGPEM5lA=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理通过集中机制在"薄"市场中改善匹配质量，但可能导致"代理公地悲剧"问题


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在薄市场（参与者少、信息有限的市场）中的集中匹配机制，探讨其潜在的公地悲剧问题

Method: 分析AI代理在薄市场中作为集中匹配机制的作用，研究其如何影响市场效率和资源分配

Result: AI代理能改善薄市场的匹配质量，但可能导致类似"公地悲剧"的资源过度使用和效率损失问题

Conclusion: 需要平衡AI代理在薄市场中的集中匹配优势与潜在的资源滥用风险，设计适当的监管机制

Abstract: The Tragedy of the Agentic Commons (18 minute read) AI agents help facilitate better match quality through centralized mechanisms in 'thin' markets.

</details>


### [33] [How Hightouch built their long-running agent harness](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.amplifypartners.com%2Fblog-posts%2Fhow-hightouch-built-their-long-running-agent-harness%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/GVfBKPMIMXw8U4C7vV-mxbvASgS4YVuu2MCf0Mf0b6g=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Hightouch开发了一个能够处理复杂多步骤任务的AI营销代理，通过将规划与执行分离实现动态计划更新，并采用代理委托技术管理上下文


<details>
  <summary>Details</summary>
Motivation: 构建能够处理复杂、长期运行任务的AI代理，解决传统方法在上下文管理和多步骤任务执行方面的局限性

Method: 1. 明确分离模型的规划与执行阶段；2. 使用"代理委托"技术管理上下文，包括将大数据缓冲到临时文件系统；3. 为复杂子任务动态生成隔离的子代理

Result: 成功开发出能够处理复杂多步骤营销任务的长期运行AI代理，实现了动态计划更新和有效的上下文管理

Conclusion: 通过规划与执行分离以及代理委托架构，可以构建更强大、更可靠的长期运行AI代理系统

Abstract: How Hightouch built their long-running agent harness (15 minute read) Hightouch developed an AI marketing agent capable of complex, multi-step tasks. Its team explicitly separated the model's planning from its execution, allowing for dynamic plan updates as the agent learns from data. To manage context, they used "agentic delegation" through techniques like buffering large data to a temporary filesystem and spawning dynamic subagents for isolated, complex sub-tasks.

</details>


### [34] [Announcing Mastra 1.0!](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmastra.ai%2Fblog%2Fannouncing-mastra-1%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/Nk0F_TWpyIbrs9ePwBU0a1g4kYaYZ1t0qDCY-No3UBw=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Mastra 1.0是一个TypeScript AI代理框架的稳定版本发布，包含服务器适配器、复合存储和对Vercel AI SDK v6的完整支持


<details>
  <summary>Details</summary>
Motivation: 为TypeScript开发者提供一个稳定、易部署的AI代理框架，简化AI应用的开发和部署流程

Method: 发布Mastra 1.0版本，包含三个主要功能：服务器适配器简化部署、复合存储提供灵活的后端配置、完整支持Vercel AI SDK v6

Result: 成功发布了Mastra 1.0稳定版本，为开发者提供了功能完整的TypeScript AI代理框架

Conclusion: Mastra 1.0的发布标志着该框架进入稳定阶段，为TypeScript AI应用开发提供了可靠的工具支持

Abstract: Announcing Mastra 1.0! (7 minute read) Mastra, an AI agent framework for TypeScript, has officially announced the stable release of version 1.0. This version includes Server Adapters for easier deployment, Composite Storage for flexible backend configurations, and full support for Vercel's AI SDK v6.

</details>


### [35] [Without Benchmarking LLMs, You're Likely Overpaying 5-10x](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarllorey.com%2Fposts%2Fwithout-benchmarking-llms-youre-overpaying%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/OfvvcckHS6nsBb1syOJwBkyItVVGLSOaMbrwohfwAvM=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文指出常见基准测试无法准确预测LLM在特定任务上的性能和成本效益，导致用户过度支付5-10倍费用，并提出基于真实提示收集、定义期望输出、统一API测试和LLM作为评判者的基准测试方法。


<details>
  <summary>Details</summary>
Motivation: 当前用户普遍过度支付LLM API调用费用，因为常用基准测试无法准确预测特定任务的性能和成本效益，导致用户默认选择流行但昂贵的模型。

Method: 提出基准测试方法：1) 收集真实任务提示；2) 定义期望输出标准；3) 通过统一API运行多个LLM；4) 使用LLM作为评判者根据质量评分响应。

Result: 通过该方法可以更准确地评估不同LLM在特定任务上的性能与成本效益，帮助用户避免过度支付5-10倍费用。

Conclusion: 需要针对具体任务进行定制化基准测试，而非依赖通用基准，才能实现成本效益最优的LLM选择。

Abstract: Without Benchmarking LLMs, You're Likely Overpaying 5-10x (9 minute read) Many users overpay for LLM API calls because common benchmarks don't accurately predict performance or cost-effectiveness for their specific tasks, leading them to default to popular but expensive models. This article goes over a benchmarking methodology that involves collecting real prompts, defining expected outputs, running numerous LLMs via a unified API, and using an LLM-as-judge to score responses based on quality...

</details>


### [36] [AI Agents at work: real-time platform insights in Slack](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.monday.com%2Fai-agents-at-work-real-time-platform-insights-in-slack%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/eYKCMpOGFo8LcmameANboYAWV7KRwF0lbKOOtgCoA34=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Monday.com开发了一个AI Slack机器人，让开发者能快速获取实时平台洞察，无需手动检查内部工具或筛选通知


<details>
  <summary>Details</summary>
Motivation: 开发者需要快速访问实时平台洞察，但手动检查内部工具和筛选通知效率低下，需要更便捷的解决方案

Method: 使用LLM代理通过MCP连接到微服务数据，预计算统计数据而非让模型计算，以防止AI产生幻觉数据

Result: 开发了一个AI Slack机器人，能提供实时平台洞察，减少了开发者手动检查工具的时间

Conclusion: AI代理能有效提升开发者的工作效率，通过预计算数据可以防止AI幻觉，实时平台洞察工具具有实用价值

Abstract: AI Agents at work: real-time platform insights in Slack (8 minute read) Monday.com developed an AI Slack bot to help devs quickly access real-time platform insights without having to manually check internal tools or dig through notifications. The bot uses LLM agents connected to their microservices data through an MCP. To prevent the AI from hallucinating data, they pre-computed statistics rather than letting the model calculate them.

</details>


### [37] [Running Claude Code dangerously](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.emilburzo.com%2F2026%2F01%2Frunning-claude-code-dangerously-safely%2F%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/PlT7FHZzxmxHBWAsc9-pZvRItSC_i1ztcC-9MrXxwT8=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 作者使用Vagrant创建隔离的虚拟机环境，安全地运行带危险标志的Claude Code，避免对主机系统造成损害


<details>
  <summary>Details</summary>
Motivation: 作者希望使用Claude Code的`--dangerously-skip-permissions`标志来改进工作流程，但需要一个安全、隔离的环境来防止意外损坏主机系统

Method: 作者放弃了Docker方案（因为Docker-in-Docker需要特权模式），选择使用Vagrant创建一次性虚拟机，提供完全隔离和可重现的环境

Result: 成功创建了一个安全的沙箱环境，可以在其中安全地运行带危险标志的Claude Code，而不会对主机系统造成风险

Conclusion: Vagrant虚拟机是运行需要危险权限的代码工具的安全有效解决方案，提供了Docker无法提供的完全系统隔离

Abstract: Running Claude Code dangerously (safely) (6 minute read) The author wanted to use Claude Code with the `--dangerously-skip-permissions` flag for an improved workflow, but needed a safe, isolated environment to prevent accidental damage to their host system. They initially considered Docker, but issues like Docker-in-Docker requiring privileged mode made it unsuitable for true sandboxing. They chose Vagrant to create a disposable virtual machine, which offers full isolation and a reproducible ...

</details>


### [38] [Claude Chill](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fdavidbeesley%2Fclaude-chill%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/dmNRmOi1vUuPUsT0C8tB6M2oMjqxd1pOfGiI8Kte1so=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Chill是一个PTY代理工具，通过仅渲染屏幕差异、保留历史记录和启用回看功能，来优化Claude Code的终端输出，解决延迟、闪烁和滚动问题


<details>
  <summary>Details</summary>
Motivation: 解决Claude Code在终端输出时产生的大量更新导致的性能问题，包括延迟、屏幕闪烁和滚动历史损坏等用户体验问题

Method: 开发一个PTY（伪终端）代理，采用差异渲染技术，只更新屏幕变化的部分，同时保留完整的终端历史记录，并实现回看功能

Result: 成功减少了终端输出的延迟和闪烁，修复了滚动历史损坏的问题，提升了Claude Code在终端交互时的用户体验

Conclusion: 通过差异渲染和智能历史管理，Claude Chill有效优化了Claude Code的终端性能，为代码代理工具提供了更好的终端交互体验

Abstract: Claude Chill (GitHub Repo) Claude Chill is a PTY proxy that tames Claude Code's massive terminal updates by rendering only screen differences, preserving history, and enabling lookback to prevent lag, flicker, and broken scrollback.

</details>


### [39] [Why AI coding tools shift the real bottleneck to review](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.logrocket.com%2Fcode-review-the-bottleneck-ai-cant-ignore%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/zgBU0m0pfcVvJw56KCIAWto3FmXrFCpqBskrTu0MvnI=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI编码工具生成2-6倍更多代码，包含大量防御性模式和边界情况处理，将代码审查重点从"缺少什么"转变为"这些复杂性是否必要"


<details>
  <summary>Details</summary>
Motivation: 探索AI编码工具如何改变软件开发流程，特别是对代码审查阶段的影响。通过对比人工编写与AI生成的相同功能，研究AI工具带来的代码量增加和复杂性变化。

Method: 通过实验方法，测试相同功能的人工编写版本与AI生成版本，对比代码量、防御性模式使用、边界情况处理等方面的差异。

Result: AI生成的代码量是人工编写的2-6倍，包含大量防御性编程模式和边界情况处理，导致代码审查问题从关注功能完整性转变为评估复杂性必要性。

Conclusion: AI编码工具将开发瓶颈从编写代码转移到代码审查阶段，审查者需要更多关注代码复杂性和必要性评估，而非传统的问题发现。

Abstract: Why AI coding tools shift the real bottleneck to review (8 minute read) Through testing identical features built manually versus with AI, this developer found that AI generates 2-6× more code with extensive defensive patterns and edge-case handling, changing review questions from "what's missing?" to "is all this complexity necessary?"

</details>


### [40] [The Agent Skills Directory](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fskills.sh%2F%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/pu34Mz7IDbytO4U468hQoR7-dH4ER2mk1I84qq4N1iM=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Skills Directory是一个网站，收集了可重用的插件式能力（技能），帮助AI代理更有效地完成特定任务


<details>
  <summary>Details</summary>
Motivation: AI代理在执行特定任务时需要专门的程序性知识，但缺乏系统化的技能共享和复用机制，导致效率低下和重复开发

Method: 创建了一个网站目录，收集和组织可重用的插件式技能，这些技能提供程序性知识，帮助AI代理完成任务

Result: 建立了一个包含最有用的AI代理技能的在线目录，使开发者能够查找和复用现有技能，提高开发效率

Conclusion: Agent Skills Directory通过提供可重用的技能库，促进了AI代理能力的共享和复用，提高了开发效率和任务完成效果

Abstract: The Agent Skills Directory (Website) Skills are reusable, plugin-like capabilities that provide procedural knowledge to help AI agents more effectively accomplish specific tasks, and this website is a directory of the most useful skills.

</details>


### [41] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/PuACvY6kdwb1lnf3KLpwjTegkRPd0zW_Hvyk2aNrHNM=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Skills Directory是一个网站，收集了可重用的插件式技能，帮助AI代理更有效地完成特定任务


<details>
  <summary>Details</summary>
Motivation: AI代理需要特定的程序性知识来完成各种任务，但缺乏可重用的技能库。这个网站旨在收集和组织最有用的技能，帮助代理更有效地工作

Method: 创建一个网站目录，收集和组织可重用的插件式技能（Skills），这些技能提供程序性知识，可以被AI代理调用和使用

Result: 建立了一个包含最有用技能的目录网站，为AI代理提供了可重用的能力库，帮助它们更有效地完成特定任务

Conclusion: Agent Skills Directory通过提供可重用的技能库，显著提升了AI代理的任务执行效率和能力，是一个有价值的资源平台

Abstract: The Agent Skills Directory (Website) Skills are reusable, plugin-like capabilities that provide procedural knowledge to help AI agents more effectively accomplish specific tasks, and this website is a directory of the most useful skills.

</details>


### [42] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/zkl9-0QFoibweHJMoLVmN5OAqNP8MxocNbYi2zUbdeo=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Skills Directory是一个网站，收集了可重用的插件式技能，帮助AI代理更有效地完成特定任务


<details>
  <summary>Details</summary>
Motivation: AI代理需要特定技能来完成复杂任务，但缺乏系统化的技能管理和共享机制，导致重复开发和效率低下

Method: 创建一个网站目录，收集和组织可重用的插件式技能，这些技能提供程序性知识，帮助AI代理执行特定任务

Result: 建立了一个包含最有用的AI代理技能的在线目录，方便开发者查找和使用

Conclusion: Agent Skills Directory通过提供可重用的技能库，提高了AI代理的开发效率和任务执行能力

Abstract: The Agent Skills Directory (Website) Skills are reusable, plugin-like capabilities that provide procedural knowledge to help AI agents more effectively accomplish specific tasks, and this website is a directory of the most useful skills.

</details>


### [43] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/5ybVCgvvDcr2GKUBIeDKk9nR7NtUfV7-idT0cnu_k8k=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Skills Directory是一个网站，收集了可重用的插件式技能，帮助AI代理更有效地完成特定任务


<details>
  <summary>Details</summary>
Motivation: AI代理需要特定技能来完成复杂任务，但缺乏系统化的技能库和重用机制，导致效率低下和重复开发

Method: 创建一个网站目录，收集和组织可重用的插件式技能，这些技能提供程序性知识，帮助AI代理完成任务

Result: 建立了Agent Skills Directory网站，包含最有用的技能目录，为AI代理提供可重用的能力库

Conclusion: 技能目录系统可以显著提高AI代理的效率和能力，通过重用现有技能减少重复开发

Abstract: The Agent Skills Directory (Website) Skills are reusable, plugin-like capabilities that provide procedural knowledge to help AI agents more effectively accomplish specific tasks, and this website is a directory of the most useful skills.

</details>


### [44] [GitLab Announces General Availability of GitLab Duo Agent Platform](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.devopsdigest.com%2Fgitlab-announces-general-availability-of-gitlab-duo-agent-platform%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/9gpERhgEJzGWrqTNjXmHyufnCnIaocobuD-sEkaJfqY=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitLab推出GitLab Duo Agent Platform正式版，这是一个面向完整软件生命周期的AI代理平台，旨在解决编码之外的交付瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 解决软件开发交付过程中的瓶颈问题，特别是在编码之外的环节，通过AI代理提升整个软件生命周期的效率。

Method: 提供编排的AI代理平台，包括代理聊天功能、预构建和自定义代理、治理控制，以及基于GitLab积分的使用定价模式。

Result: GitLab Duo Agent Platform现已正式可用，为企业提供跨完整软件生命周期的AI代理解决方案。

Conclusion: GitLab通过推出AI代理平台扩展了其DevOps工具链，帮助组织解决软件交付中的非编码瓶颈问题。

Abstract: GitLab Announces General Availability of GitLab Duo Agent Platform (4 minute read) GitLab Duo Agent Platform is now in general availability. The platform enables orchestrated AI agents across the full software lifecycle to address delivery bottlenecks beyond coding. It offers agentic chat, prebuilt and custom agents, governance controls, and usage-based pricing via GitLab Credits.

</details>


### [45] [Terraform Skill](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fantonbabenko%2Fterraform-skill%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/TK5zPrUJTRiV7AESRgyfPCxRlwOV4f_5FewgYjSd_v4=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 介绍了一个新的Claude Agent Skill，专门用于Terraform和OpenTofu，提供基础设施代码的全面指导


<details>
  <summary>Details</summary>
Motivation: 为Claude Code用户提供专业的基础设施即代码（IaC）指导，帮助他们在Terraform和OpenTofu项目中实施最佳实践

Method: 开发了一个Claude Agent Skill，包含测试策略、模块模式、CI/CD工作流和生产就绪基础设施代码的指导

Result: 成功创建了一个GitHub仓库，提供Terraform和OpenTofu的全面技能包，帮助开发者提升基础设施代码质量

Conclusion: 该技能为Claude Code用户提供了有价值的基础设施即代码专业指导，有助于标准化和优化云基础设施管理

Abstract: Terraform Skill (GitHub Repo) A new Claude Agent Skill for Terraform and OpenTofu has been introduced, offering comprehensive guidance on testing strategies, module patterns, CI/CD workflows, and production-ready infrastructure code for Claude Code users.

</details>


### [46] [agent-lightning](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmicrosoft%2Fagent-lightning%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/fZnrbTTwJVvfGSYE0gEuADBID6l7-9lJbI45dbrj2IE=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: agent-lightning 是一个开源框架，用于训练和优化AI智能体，支持强化学习、自动提示优化、监督微调等功能，无需大幅修改现有智能体代码。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体框架缺乏统一的训练和优化工具，智能体性能提升需要大量手动调整。agent-lightning旨在提供模块化组件，让开发者能够轻松收集智能体执行数据并迭代改进性能。

Method: 采用解耦的强化学习训练方法，与主流智能体框架（如LangChain、OpenAI Agents SDK、AutoGen）兼容，提供模块化组件收集执行数据，支持多种优化技术。

Result: 创建了一个开源框架，能够与现有智能体框架无缝集成，实现智能体性能的迭代优化，支持多种训练和优化方法。

Conclusion: agent-lightning为AI智能体开发提供了统一的训练和优化平台，降低了智能体性能提升的技术门槛，促进了智能体技术的普及和应用。

Abstract: agent-lightning (GitHub Repo) agent-lightning is an open-source framework for training and optimizing AI agents—enabling reinforcement learning (RL), automatic prompt optimization, supervised fine-tuning, and more—without requiring substantial changes to existing agent code. It works with virtually any agent framework (e.g., LangChain, OpenAI Agents SDK, and AutoGen) and provides modular components to collect agent execution data and iteratively improve agent performance via a decoupled RL tr...

</details>


### [47] [Self-Driving DevOps? How Stakpak Tackles Infrastructure Complexity](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fself-driving-devops-how-stakpak-tackles-infrastructure-complexity%2F%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/pWDS11CM1zo1yOHwkY0TQu12ntqJ2nDqIomN9Cwo6_s=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Stakpak是一个开源的DevOps代理，旨在通过安全存储秘密、设置防护措施防止破坏性操作、集中共享运维知识，使AI能够可靠处理复杂的非编码DevOps任务，实现基础设施的自主化。


<details>
  <summary>Details</summary>
Motivation: 当前DevOps基础设施管理复杂，涉及大量非编码任务，需要人工处理秘密管理、安全防护和知识共享等问题。Stakpak旨在通过AI代理自动化这些复杂任务，降低运维复杂性。

Method: 开发开源DevOps代理，包含三个核心功能：1) 安全存储和管理秘密；2) 设置防护措施防止破坏性操作；3) 集中共享运维知识库。通过这些功能使AI能够可靠处理复杂的非编码DevOps任务。

Result: Stakpak成功创建了一个能够自主处理复杂DevOps任务的代理系统，实现了基础设施管理的自动化，提高了运维的可靠性和安全性。

Conclusion: Stakpak通过将AI代理引入DevOps领域，为解决基础设施复杂性提供了创新方案，使非编码的运维任务能够实现自主化处理。

Abstract: Self-Driving DevOps? How Stakpak Tackles Infrastructure Complexity (5 minute read) Stakpak introduced an open-source DevOps agent aimed at making infrastructure more autonomous by securing secrets, preventing destructive actions with guardrails, and centralizing shared operational knowledge so AI can handle complex, non-coding DevOps tasks reliably.

</details>


### [48] [Making Very Small LLMs Smarter With RAG](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmaking-small-llms-smarter%2F%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/Rfw3fc-vTRA0WwIZRDPTCVeHKNy5LDPeGEprQNi-5CM=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 小型本地LLM通过RAG技术可以更有效地处理专业开发任务，为Nova Golang库生成准确代码片段


<details>
  <summary>Details</summary>
Motivation: 大型商业模型不了解专业项目（如Nova Golang库），小型本地LLM在处理这类专业开发任务时存在局限性，需要增强其能力

Method: 采用检索增强生成（RAG）方法，通过检索相关专业知识来增强小型LLM的能力，使其能够处理专业开发任务

Result: 小型本地LLM通过RAG技术能够为Nova Golang库生成准确的代码片段，解决了大型商业模型不了解专业项目的问题

Conclusion: RAG技术可以有效提升小型本地LLM在专业开发任务中的表现，使其能够处理大型商业模型无法覆盖的专业领域

Abstract: Making Very Small LLMs Smarter With RAG (7 minute read) Small, local LLMs can be effectively leveraged for niche development tasks by implementing Retrieval Augmented Generation (RAG). This approach allows LLMs to generate accurate code snippets for the custom Nova Golang library, addressing limitations faced by larger commercial models unaware of such specialized projects.

</details>


### [49] [Run tests in CI against a real environment](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord-for-ci%2F%3Futm_source=tldrdevops%26utm_medium=tldrnewsletter%26utm_campaign=ql20260121%26utm_content=std/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/auizsP2wXkVaQSsJvPT_s7OogZwvwArrvsppA3icMlM=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: mirrord for CI工具允许在CI中直接对真实Kubernetes环境运行测试，无需部署代码或创建测试环境，从而获得更快、更可靠的CI反馈并降低成本。


<details>
  <summary>Details</summary>
Motivation: 传统CI/CD流程中，测试通常需要在部署代码或创建专门的测试环境后才能进行，这导致反馈周期长、成本高且可靠性有限。开发者需要一种更高效的方式来在CI中直接验证代码在真实环境中的表现。

Method: mirrord for CI通过某种技术（可能是流量镜像或代理机制）让CI流水线中的测试能够直接访问真实的Kubernetes环境（如staging或pre-prod环境），而无需实际部署代码或创建独立的测试环境。

Result: 该方法能够显著加快CI反馈速度，提高测试可靠性，同时降低基础设施成本，因为无需维护专门的测试环境。

Conclusion: mirrord for CI提供了一种创新的CI测试方法，通过在真实环境中直接运行测试，解决了传统CI/CD流程中的延迟、成本和可靠性问题。

Abstract: Run tests in CI against a real environment (Sponsor) mirrord for CI lets you run tests against your staging or pre-prod Kubernetes environment without deploying code or spinning up test environments. Get faster, more reliable CI feedback at a lower cost. Learn more

</details>


### [50] [Anamnesis](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FSeanHeelan%2Fanamnesis-release%3Futm_source=tldrinfosec/1/0100019be0e1e834-9aab8d86-f6e4-4cad-8232-781deb967bd2-000000/BfnRSx0tA3AcfctL2lL-w6PowGoGzTHrrN4ld2mtds4=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anamnesis是一个评估LLM代理从漏洞报告生成漏洞利用的框架，展示了Claude Opus 4.5和GPT-5.2代理能够独立开发绕过多种安全防护的工作漏洞利用


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型代理在网络安全领域的实际能力，特别是它们是否能够基于漏洞报告独立开发复杂的漏洞利用，这对于评估AI在安全研究中的潜力和风险具有重要意义

Method: 开发了Anamnesis评估框架，使用QuickJS的use-after-free漏洞和概念验证触发器作为测试案例，让LLM代理（Claude Opus 4.5和GPT-5.2）独立生成漏洞利用代码

Result: Claude Opus 4.5和GPT-5.2代理能够成功开发出绕过ASLR、NX、full RELRO、CFI、Shadow Stack和seccomp沙箱等多种安全防护的工作漏洞利用

Conclusion: 先进的LLM代理已经具备从漏洞报告独立开发复杂漏洞利用的能力，这对网络安全领域既是机遇也是挑战，需要认真考虑AI在安全研究中的伦理和安全影响

Abstract: Anamnesis (GitHub Repo) Anamnesis is an evaluation framework for studying how LLM agents generate exploits from vulnerability reports, demonstrating that Claude Opus 4.5 and GPT-5.2 agents can independently develop working exploits that bypass ASLR, NX, full RELRO, CFI, Shadow Stack, and seccomp sandboxes when given a QuickJS use-after-free vulnerability and proof-of-concept trigger.

</details>


### [51] [Gemini in Chrome is getting “Skills” as it moves toward becoming a full AI agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fchromeunboxed.com%2Fgemini-in-chrome-is-getting-skills-as-it-moves-toward-becoming-a-full-ai-agent%2F%3Futm_source=tldrai/1/0100019be0ebe674-5bcffab8-988b-4fa2-b449-f3944f7988cc-000000/8f0f2jmMSuBU_J4TIKK88p9H6acfnmDwK7Aza8KBDZU=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gemini in Chrome 正在升级为主动式AI代理，通过"技能"功能让用户能够定义和教授AI处理特定或重复性工作流程。


<details>
  <summary>Details</summary>
Motivation: 将Gemini从简单的聊天助手升级为能够代表用户执行复杂任务的主动式AI代理，提高浏览器内的自动化能力和用户体验。

Method: 在Chrome中引入"技能"功能，提供专用界面让用户定义AI的具体能力，通过用户"教授"的方式让AI学习处理特定工作流程。

Result: 发现隐藏的内部"技能"页面正在进行测试，这表明Google正在将Gemini转变为能够主动执行复杂任务的完整AI代理。

Conclusion: Gemini正在向完整的AI代理发展，通过技能系统让用户能够自定义和扩展AI在浏览器中的能力，实现更高级的自动化任务处理。

Abstract: Gemini in Chrome is getting “Skills” as it moves toward becoming a full AI agent (2 minute read) Gemini in Chrome is being upgraded into a proactive agent capable of performing complex tasks on users' behalf. A hidden internal page for 'skills' has been spotted in testing. The page features a dedicated interface where users can define specific capabilities for the AI. Skills will effectively allow users to 'teach' Gemini how to handle specific or repetitive workflows within the browser.

</details>


### [52] [The Tragedy of the Agentic Commons](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strangeloopcanon.com%2Fp%2Fthe-tragedy-of-the-agentic-commons%3Futm_source=tldrai/1/0100019be0ebe674-5bcffab8-988b-4fa2-b449-f3944f7988cc-000000/vbyNDukx7lnThXeDecwYo-XA545Rn33VN1UFDtsTiio=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理能通过LLM将非结构化偏好转化为可操作数据，在匹配市场（如约会、求职）中优于标准问卷。但当所有参与者都使用AI代理时，会导致严重拥堵，降低效率。引入价格机制和制度设计可以缓解拥堵、改善协调、提升市场效率。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在匹配市场中的双重效应：一方面AI代理能通过LLM更好地理解用户偏好，提升匹配质量；另一方面当所有市场参与者都使用AI代理时，会导致"代理公地悲剧"，产生拥堵问题，降低整体市场效率。

Method: 分析AI代理在匹配市场中的作用机制，特别是LLM如何将非结构化偏好转化为可操作数据。研究当所有参与者都使用AI代理时的拥堵效应，并提出通过价格机制和制度设计来解决拥堵问题的方法。

Result: AI代理在匹配市场中确实能超越标准问卷，但当所有参与者都使用时会导致显著拥堵，降低效率。引入价格机制和适当的制度设计可以有效缓解拥堵问题，改善市场协调，提升整体效率。

Conclusion: AI代理在匹配市场中具有双重效应：个体层面能提升匹配质量，但集体层面可能导致"代理公地悲剧"。需要通过价格机制和制度设计来协调个体与集体利益，实现市场效率最大化。

Abstract: The Tragedy of the Agentic Commons (14 minute read) AI agents can improve matching markets, like dating or job searches, by using LLMs to convert unstructured preferences into actionable data, outperforming standard questionnaires. However, in markets where everyone uses AI agents, congestion increases significantly, reducing efficiency without a pricing mechanism. Introducing prices and institutional design can alleviate congestion, streamline coordination, and enhance market efficiency.

</details>


### [53] [GLM-4.7-Flash](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FLCCov8/1/0100019be0ebe674-5bcffab8-988b-4fa2-b449-f3944f7988cc-000000/jLCSGSQaOA31wKQ___4VtudpixyjztVQBLej8S6mGs0=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GLM-4.7-Flash是一个30B参数的MoE模型，专为本地部署设计，适用于编码、智能体任务、翻译和创意写作


<details>
  <summary>Details</summary>
Motivation: 开发一个适合本地部署的高效模型，能够在资源受限的环境中处理多种任务，包括编码、智能体应用、翻译和创意写作

Method: 采用30B参数的混合专家（MoE）架构，优化模型大小和性能平衡，专门针对本地部署场景进行设计

Result: 创建了一个适合本地部署的30B MoE模型，能够在编码、智能体任务、翻译和创意写作等多个领域表现良好

Conclusion: GLM-4.7-Flash是一个高效的本地部署模型，通过MoE架构在保持性能的同时减少资源需求，适用于多种应用场景

Abstract: GLM-4.7-Flash (2 minute read) GLM-4.7-Flash is a 30B-class MoE model designed for local deployment in coding, agent tasks, translation, and creative writing.

</details>


### [54] [👩‍🏭 Build fast, scale right: Meet your AI app & agent factory](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.azure.com%2F%3Focid=cmmxp9rfcmy/2/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/gMxYqgtFTukVR7f_RNx0M7xzHil831g27bitYjiicT0=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 微软Foundry是一个模块化、可互操作的平台，用于构建、优化和管理AI代理系统，提供可观察性和治理功能


<details>
  <summary>Details</summary>
Motivation: 随着AI代理采用加速，团队需要能够快速构建、安全部署并有效管理多代理系统的平台，确保数据安全和权限控制

Method: 提供模块化、可互操作的平台架构，支持从第一天开始构建、优化和治理所有代理，确保AI应用和代理能够安全地基于任何位置的数据进行基础训练

Result: Foundry平台使团队能够快速构建和扩展AI应用与代理系统，同时内置可观察性和治理功能，确保数据安全和权限合规

Conclusion: 微软Foundry为AI代理开发提供了全面的解决方案，帮助团队在加速采用AI代理的同时确保系统的安全性、可观察性和治理能力

Abstract: 👩‍🏭 Build fast, scale right: Meet your AI app & agent factory (Sponsor) Agent adoption is accelerating. Winning teams ship multi‑agent systems with observability and governance built in from the start. Microsoft Foundry is a modular, interoperable platform to build, optimize, and govern all your agents from day one. With Foundry, you can securely ground your AI apps and agents on data stored in any location while respecting user permissions and data classification policies. Foundry Control Pl...

</details>


### [55] [the widest selection on any cloud](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.azure.com%2F%3Focid=cmmxp9rfcmy/2/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/gMxYqgtFTukVR7f_RNx0M7xzHil831g27bitYjiicT0=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 微软Foundry是一个模块化、可互操作的平台，用于从第一天开始构建、优化和管理所有AI代理，提供可观察性和治理功能


<details>
  <summary>Details</summary>
Motivation: 随着AI代理采用加速，团队需要能够构建多代理系统，并从一开始就内置可观察性和治理功能的平台

Method: 提供模块化、可互操作的平台，支持在任何位置的数据上安全地构建AI应用和代理，同时尊重用户权限和数据分类策略

Result: 微软Foundry平台能够帮助团队快速构建、优化和管理AI代理系统

Conclusion: Foundry为构建AI应用和代理提供了全面的平台解决方案，支持从开发到治理的全流程管理

Abstract: 👩‍🏭 Build fast, scale right: Meet your AI app & agent factory (Sponsor) Agent adoption is accelerating. Winning teams ship multi‑agent systems with observability and governance built in from the start. Microsoft Foundry is a modular, interoperable platform to build, optimize, and govern all your agents from day one. With Foundry, you can securely ground your AI apps and agents on data stored in any location while respecting user permissions and data classification policies. Foundry Control Pl...

</details>


### [56] [Your Agent's Reasoning Is Fine - Its Memory Isn't](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.decodingai.com%2Fp%2Fdesigning-production-engineer-agent-graphrag%3Futm_source=tldrdata/1/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/3zlYl54ZU9IqxwKI7C7G41eaZJvzfHi5DvttmaLjb8I=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI agent结合GraphRAG技术，通过结构化检索服务关系、所有权、历史事件和文档，加速生产事故响应，将监控警报转化为可操作的上下文丰富报告。


<details>
  <summary>Details</summary>
Motivation: 生产事故响应缓慢的主要问题不是缺乏解决方案，而是上下文分散和依赖关系未文档化。传统方法中，工程师需要花费大量时间收集相关信息，导致响应延迟。

Method: 集成基于GraphRAG的AI代理，实现快速结构化检索。系统由Prometheus等监控工具触发，通过FastAPI和Slack接口，将事故警报转化为包含服务关系、所有权、历史事件和文档的上下文丰富报告。

Result: 该系统能够显著加速生产事故响应过程，通过提供结构化、上下文丰富的信息，减少工程师收集相关信息的时间，提高事故解决效率。

Conclusion: AI代理的推理能力足够，但记忆系统是关键瓶颈。通过GraphRAG技术增强记忆能力，可以显著改善生产环境中的事故响应效率。

Abstract: Your Agent's Reasoning Is Fine - Its Memory Isn't (17 minute read) Production incident response is slowed not by lack of fixes but by scattered context and undocumented dependencies. Integrating an AI agent powered by GraphRAG enables rapid, structured retrieval of service relationships, ownership, past incidents, and documentation. This system, triggered by monitoring tools like Prometheus and interfacing via FastAPI and Slack, transforms incident alerts into actionable, context-rich reports...

</details>


### [57] [Agents that don't suck](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FWAQ3l7%3Futm_source=tldrdata/1/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/3UWLQ_fxP47-4iGWlxoExqeiSfxMlCCh7W8oxIBZ__I=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Bricks是一个AI代理开发平台，帮助用户基于特定数据构建、评估和优化AI代理，通过自动评估、目标对齐评分和人类反馈改进，实现生产级部署。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理开发面临难以评估性能、缺乏生产就绪性、难以与业务目标对齐等挑战，需要系统化的工具来构建真正有效的现实世界应用。

Method: 提供平台化解决方案，包含自动评估系统、基于目标的输出评分机制、人类反馈集成，以及数据驱动的优化流程，帮助用户系统化地开发和改进AI代理。

Result: 平台能够帮助开发者构建更可靠、与业务目标对齐的AI代理，提供清晰的优化路径，加速从原型到生产部署的过程。

Conclusion: Agent Bricks通过系统化的评估和优化框架，解决了AI代理开发中的关键挑战，使构建生产就绪的AI代理变得更加可行和高效。

Abstract: Agents that don't suck (Sponsor) Agent Bricks helps you build, evaluate and optimize AI agents grounded in your unique data. It evaluates automatically, scores outputs against your goals and improves with human feedback — giving you a clearer path to production. Build agents that work in the real world. See why it's worth your time

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [GCG Attack On A Diffusion LLM](https://arxiv.org/abs/2601.14266)
*Ruben Neyroud,Sam Corley*

Main category: cs.LG

TL;DR: 探索GCG风格对抗提示攻击在LLaDA扩散语言模型上的适用性，评估多种攻击变体在AdvBench有害提示上的效果


<details>
  <summary>Details</summary>
Motivation: 虽然GCG攻击在自回归LLMs上有效，但其在扩散语言模型上的适用性尚未充分探索，需要研究扩散模型的鲁棒性和攻击面

Method: 在LLaDA扩散语言模型上实施GCG风格对抗提示攻击，评估前缀扰动和后缀对抗生成等多种攻击变体，使用AdvBench数据集的有害提示

Result: 提供了扩散语言模型鲁棒性和攻击面的初步见解，表明需要针对此设置开发替代的优化和评估策略

Conclusion: 扩散语言模型对GCG风格攻击的脆弱性需要进一步研究，需要开发专门针对扩散模型的对抗分析策略

Abstract: While most LLMs are autoregressive, diffusion-based LLMs have recently emerged as an alternative method for generation. Greedy Coordinate Gradient (GCG) attacks have proven effective against autoregressive models, but their applicability to diffusion language models remains largely unexplored. In this work, we present an exploratory study of GCG-style adversarial prompt attacks on LLaDA (Large Language Diffusion with mAsking), an open-source diffusion LLM. We evaluate multiple attack variants, including prefix perturbations and suffix-based adversarial generation, on harmful prompts drawn from the AdvBench dataset. Our study provides initial insights into the robustness and attack surface of diffusion language models and motivates the development of alternative optimization and evaluation strategies for adversarial analysis in this setting.

</details>


### [59] [Chain-of-Memory: Lightweight Memory Construction with Dynamic Evolution for LLM Agents](https://arxiv.org/abs/2601.14287)
*Xiucheng Xu,Bingbing Xu,Xueyun Tian,Zihe Huang,Rongxin Chen,Yunfan Li,Huawei Shen*

Main category: cs.LG

TL;DR: CoM框架提出轻量级记忆构建与复杂利用的新范式，通过Chain-of-Memory机制组织检索片段形成连贯推理路径，显著提升性能同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有外部记忆系统存在两个根本问题：复杂的记忆构建计算成本高但性能提升有限；简单的上下文拼接无法弥合检索召回与推理准确性之间的差距。

Method: 提出CoM框架，采用轻量级构建与复杂利用相结合的新范式，引入Chain-of-Memory机制，通过动态演化将检索片段组织成连贯推理路径，并使用自适应截断来修剪无关噪声。

Result: 在LongMemEval和LoCoMo基准测试中，CoM相比强基线获得7.5%-10.4%的准确率提升，同时将计算开销大幅降低至复杂记忆架构的约2.7%令牌消耗和6.0%延迟。

Conclusion: CoM框架通过轻量级构建与复杂利用的新范式，有效解决了现有外部记忆系统的局限性，在提升LLM代理性能的同时显著降低了计算成本。

Abstract: External memory systems are pivotal for enabling Large Language Model (LLM) agents to maintain persistent knowledge and perform long-horizon decision-making. Existing paradigms typically follow a two-stage process: computationally expensive memory construction (e.g., structuring data into graphs) followed by naive retrieval-augmented generation. However, our empirical analysis reveals two fundamental limitations: complex construction incurs high costs with marginal performance gains, and simple context concatenation fails to bridge the gap between retrieval recall and reasoning accuracy. To address these challenges, we propose CoM (Chain-of-Memory), a novel framework that advocates for a paradigm shift toward lightweight construction paired with sophisticated utilization. CoM introduces a Chain-of-Memory mechanism that organizes retrieved fragments into coherent inference paths through dynamic evolution, utilizing adaptive truncation to prune irrelevant noise. Extensive experiments on the LongMemEval and LoCoMo benchmarks demonstrate that CoM outperforms strong baselines with accuracy gains of 7.5%-10.4%, while drastically reducing computational overhead to approximately 2.7% of token consumption and 6.0% of latency compared to complex memory architectures.

</details>


### [60] [Search over Self-Edit Strategies for LLM Adaptation](https://arxiv.org/abs/2601.14532)
*Alistair Cheong,Haolin Cong,Tyler Yang,Dustin Miao*

Main category: cs.LG

TL;DR: 研究探索LLM能否利用任务反馈自主决定权重更新策略，在SEAL框架中允许模型生成自编辑模板，发现带存档的变体表现接近最佳人工基线但未超越，分析显示需要显式新颖性压力来超越人工策略。


<details>
  <summary>Details</summary>
Motivation: 现有LLM开放搜索系统通常冻结基础模型，可能限制长期进展。虽然已有研究探索测试时更新提案模型，但更新策略仍需人工指定。因此研究LLM能否利用任务反馈自主决定权重更新方式。

Method: 在SEAL框架中放宽固定人工模板约束，允许模型生成自编辑模板，从而控制训练数据和超参数。研究两个变体：无存档版本和带轻量级历史模板存档的版本。在Qwen3-8B模型和SQuAD数据集上进行单轮知识整合实验。

Result: 无存档变体表现与较弱的"Implications"基线相当，带存档变体优于"Implications"并接近最强人工设计的"Rewrite"基线但未超越。分析显示朴素存档能提供短期鲁棒性，但也会加速同质化。

Conclusion: LLM能够利用任务反馈自主决定权重更新策略，但要持续超越精心优化的人工策略，可能需要显式的新颖性压力机制。朴素存档方法有其局限性。

Abstract: Many LLM-based open-ended search systems freeze the foundation model that proposes improvements to existing solutions, which may bottleneck long-run progress. Recent work has explored updating the proposal model at test time [arXiv:2511.23473], but the update strategy is still typically hand-specified. Therefore, this study investigated whether an LLM can use task feedback to decide how it should update its weights. For tractability, we focused on the simpler case where there is only one round of self-improvement, and restricted the update operator to self-supervised next token prediction (NTP), leaving the model freedom in choosing its training data and key NTP hyperparameters. Using the Self-Adapting Language Models (SEAL) [arXiv:2506.10943] framework as a testbed, we relaxed its fixed human template constraint and allowed the model to generate its own self-edit templates, thereby giving it more control over its training data and hyperparameters. Two variants were studied, differing in whether template generation was conditioned on a lightweight archive of past templates. In SEAL's Single-Passage Knowledge Incorporation setting with Qwen3-8B on SQuAD [arXiv:1606.05250], the no-archive variant performed comparably to the weaker "Implications" baseline, while the archive variant outperformed "Implications" and approached the strongest human-designed "Rewrite" baseline without surpassing it. Further analysis of collapse in the model's exploration revealed that a naive archive can confer some short-term robustness but can also accelerate homogenization, suggesting that explicit novelty pressure may be required to consistently advance beyond carefully optimized human strategies. Our code is available at https://github.com/cheongalc/search-self-edit-strategies .

</details>


### [61] [PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning](https://arxiv.org/abs/2601.14716)
*Yao Lu,Dengdong Fan,Jianzheng Nie,Fan Xu,Jie Chen,Bin Zhou,Yonghong Tian*

Main category: cs.LG

TL;DR: PCL-Reasoner-V1.5是一个基于Qwen2.5-32B构建的320亿参数数学推理大语言模型，采用监督微调+强化学习训练，提出离线RL方法提升训练稳定性和效率，在AIME 2024/2025上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有在线强化学习方法（如GRPO）在训练稳定性和效率方面存在不足，需要开发更稳定高效的训练范式来提升LLM的数学推理能力。

Method: 基于Qwen2.5-32B进行监督微调，然后采用创新的离线强化学习方法进行训练，相比标准在线RL方法提供更好的训练稳定性和效率。

Result: 在AIME 2024上达到90.9%的平均准确率，在AIME 2025上达到85.6%的平均准确率，在基于Qwen2.5-32B后训练的模型中达到最先进的性能。

Conclusion: 离线强化学习是一种稳定高效的训练范式，能够有效提升LLM的推理能力，所有实验均在华为昇腾910C NPU上完成。

Abstract: We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.

</details>


### [62] [Improving Regret Approximation for Unsupervised Dynamic Environment Generation](https://arxiv.org/abs/2601.14957)
*Harry Mead,Bruno Lacerda,Jakob Foerster,Nick Hawes*

Main category: cs.LG

TL;DR: 提出DEGen方法改善无监督环境设计，通过动态环境生成提供更密集的奖励信号，并引入MNA遗憾近似来更好识别挑战性环境，显著提升强化学习代理的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前无监督环境设计方法在环境参数空间较大时面临信用分配困难，难以识别真正具有挑战性的环境配置，限制了强化学习代理的泛化性能提升。

Method: 提出DEGen（动态环境生成）方法，通过更密集的生成器奖励信号改善信用分配；引入MNA（最大化负优势）作为新的遗憾近似指标，能更准确识别挑战性环境。

Result: 实验表明MNA优于现有遗憾近似方法，DEGen与MNA结合后显著超越现有方法，特别是在大规模环境中表现更优异。

Conclusion: DEGen和MNA的组合有效解决了UED中的信用分配和挑战性环境识别问题，使无监督环境设计能够扩展到更大规模的环境。

Abstract: Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.

</details>


### [63] [Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning](https://arxiv.org/abs/2601.15086)
*Oleg Shchendrigin,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 论文提出了一个测试强化学习智能体记忆重写能力的基准，发现经典循环模型在现代结构化记忆和基于Transformer的智能体中表现更优，揭示了当前方法在平衡记忆稳定性和适应性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的有效决策需要记忆既稳定又适应性强：环境随时间变化，智能体必须在长时间内保留相关信息，同时在情况变化时更新或覆盖过时内容。现有的强化学习基准和记忆增强智能体主要关注记忆保留，而记忆重写这一同样关键的能力在很大程度上未被探索。

Method: 引入了一个基准，明确测试部分可观测性下的持续记忆更新（即智能体必须依赖记忆而非当前观察的自然设置），并使用该基准比较循环、基于Transformer和结构化记忆架构。

Result: 实验表明，经典循环模型尽管简单，但在记忆重写任务中表现出比现代结构化记忆更大的灵活性和鲁棒性，后者仅在狭窄条件下成功，而基于Transformer的智能体通常无法超越简单的记忆保留案例。

Conclusion: 这些发现揭示了当前方法的基本局限性，并强调了平衡稳定保留与适应性更新的记忆机制的必要性。该工作突出了这一被忽视的挑战，引入了评估基准，并为设计具有明确可训练遗忘机制的未来强化学习智能体提供了见解。

Abstract: Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circumstances shift. Existing Reinforcement Learning (RL) benchmarks and memory-augmented agents focus primarily on retention, leaving the equally critical ability of memory rewriting largely unexplored. To address this gap, we introduce a benchmark that explicitly tests continual memory updating under partial observability, i.e. the natural setting where an agent must rely on memory rather than current observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Our experiments reveal that classic recurrent models, despite their simplicity, demonstrate greater flexibility and robustness in memory rewriting tasks than modern structured memories, which succeed only under narrow conditions, and transformer-based agents, which often fail beyond trivial retention cases. These findings expose a fundamental limitation of current approaches and emphasize the necessity of memory mechanisms that balance stable retention with adaptive updating. Our work highlights this overlooked challenge, introduces benchmarks to evaluate it, and offers insights for designing future RL agents with explicit and trainable forgetting mechanisms. Code: https://quartz-admirer.github.io/Memory-Rewriting/

</details>


### [64] [CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning](https://arxiv.org/abs/2601.15141)
*Tianshi Xu,Yuteng Chen,Meng Li*

Main category: cs.LG

TL;DR: CLEANER提出了一种利用LLM内在自校正能力在数据收集阶段直接净化噪声轨迹的方法，通过相似性感知自适应回滚机制构建清洁轨迹，显著提升参数受限模型的强化学习效率。


<details>
  <summary>Details</summary>
Motivation: 参数受限模型（4B-7B）在工具使用强化学习中面临探索阶段频繁执行失败的问题，产生噪声轨迹阻碍策略优化。标准基于结果的奖励设置导致信用分配问题，错误动作与成功结果同时被强化。现有方法面临密集奖励导致奖励黑客攻击或超采样计算成本过高的困境。

Method: 提出CLEANER框架，核心是相似性感知自适应回滚（SAAR）机制。该方法利用模型内在自校正能力，在数据收集阶段直接消除错误污染上下文。SAAR通过回顾性替换失败步骤为成功的自校正，自适应地根据语义相似性调节替换粒度，从浅层执行修复到深层推理替换。

Result: 在AIME24/25、GPQA和LiveCodeBench基准测试中，相比基线平均准确率分别提升6%、3%和5%。特别值得注意的是，CLEANER仅使用三分之一训练步骤就达到了最先进性能。

Conclusion: 轨迹净化是高效智能体强化学习的可扩展解决方案，通过训练自净化路径，模型能够内化正确推理模式而非错误恢复循环，显著提升参数受限模型的工具使用能力。

Abstract: Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model's intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub

</details>


### [65] [Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data](https://arxiv.org/abs/2601.15158)
*Yuval Ran-Milo,Yotam Alexander,Shahar Mendel,Nadav Cohen*

Main category: cs.LG

TL;DR: 论文研究了Transformer模型在稀疏奖励强化学习下如何自发产生思维链推理能力，通过分析梯度流动态证明了模型会收敛到结构化、可解释的图遍历算法，并识别了"简单示例"在泛化中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管基于结果的强化学习训练能让Transformer自发产生思维链推理，但稀疏奖励如何驱动梯度下降发现这种系统性推理的机制仍不清楚。研究者希望通过分析梯度流动态来理解这一过程。

Method: 使用单层Transformer在合成图遍历任务上进行分析，该任务需要思维链但允许简单迭代解。通过理论证明和梯度流动态分析，研究训练分布特性对学习的影响，并在合成数据和真实语言模型的数学推理任务上进行实验验证。

Result: 证明尽管仅基于最终答案正确性训练，梯度流会驱动模型收敛到结构化、可解释的逐顶点遍历算法。识别了"简单示例"的关键作用：当训练分布包含足够简单实例时，模型学习可泛化的遍历策略；否则梯度学习不可行。

Conclusion: 稀疏奖励强化学习能够自发产生思维链推理，关键在于训练分布中简单实例的分布特性。理论发现在实际语言模型的数学推理任务中得到验证，为理解Transformer的推理能力提供了理论依据。

Abstract: Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of "simple examples": instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [66] [CMind: An AI Agent for Localizing C Memory Bugs](https://arxiv.org/abs/2601.14434)
*Chia-Yi Su,Collin McMillan*

Main category: cs.SE

TL;DR: CMind是一个用于定位C语言内存错误的AI代理，它通过模拟程序员在实证研究中观察到的调试步骤来工作，结合大语言模型推理和编码的决策引导。


<details>
  <summary>Details</summary>
Motivation: 现有的C语言内存错误定位工具通常不能模拟人类程序员的实际调试过程，而实证研究表明人类程序员在定位内存错误时有特定的认知步骤和模式。

Method: CMind接收C程序源代码和错误报告作为输入，通过读取错误报告找到程序入口点，然后导航源代码、分析代码，并生成符合模板的错误位置假设和理由。它结合了大语言模型推理和编码的决策引导来模拟人类行为。

Result: CMind能够生成关于内存错误原因和位置的假设，其方法基于对人类程序员调试行为的实证研究观察。

Conclusion: 通过模拟人类程序员定位内存错误的步骤，CMind展示了将实证研究观察转化为AI代理决策过程的有效性，为程序调试工具开发提供了新思路。

Abstract: This demonstration paper presents CMind, an artificial intelligence agent for localizing C memory bugs. The novel aspect to CMind is that it follows steps that we observed human programmers perform during empirical study of those programmers finding memory bugs in C programs. The input to the tool is a C program's source code and a bug report describing the problem. The output is the tool's hypothesis about the reason for the bug and its location. CMind reads the bug report to find potential entry points to the program, then navigates the program's source code, analyzes that source code, and generates a hypothesis location and rationale that fit a template. The tool combines large language model reasoning with guided decision making we encoded to mimic human behavior. The video demonstration is available at https://youtu.be/_vVd0LRvVHI.

</details>


### [67] [Tokenomics: Quantifying Where Tokens Are Used in Agentic Software Engineering](https://arxiv.org/abs/2601.14470)
*Mohamad Salim,Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 分析LLM多代理系统在软件开发生命周期中的token消耗模式，发现代码审查阶段消耗最多token（平均59.4%），输入token占比最大（平均53.9%），揭示了代理协作中的效率问题


<details>
  <summary>Details</summary>
Motivation: LLM多代理系统在软件工程任务中应用日益广泛，但其运行效率和资源消耗缺乏深入理解，导致实际采用时面临不可预测的成本和环境影响问题

Method: 使用ChatDev框架和GPT-5推理模型，分析30个软件开发任务的执行轨迹，将内部阶段映射到标准开发阶段（设计、编码、代码完成、代码审查、测试、文档），量化比较各阶段的token分布（输入、输出、推理）

Result: 迭代的代码审查阶段消耗了大部分token（平均59.4%），输入token始终构成最大消耗份额（平均53.9%），表明代理协作存在显著低效，软件工程的主要成本不在初始代码生成而在自动化精炼和验证

Conclusion: 提出了一种新颖的方法论帮助从业者预测费用和优化工作流程，指导未来研究开发更token高效的代理协作协议

Abstract: LLM-based Multi-Agent (LLM-MA) systems are increasingly applied to automate complex software engineering tasks such as requirements engineering, code generation, and testing. However, their operational efficiency and resource consumption remain poorly understood, hindering practical adoption due to unpredictable costs and environmental impact. To address this, we conduct an analysis of token consumption patterns in an LLM-MA system within the Software Development Life Cycle (SDLC), aiming to understand where tokens are consumed across distinct software engineering activities. We analyze execution traces from 30 software development tasks performed by the ChatDev framework using a GPT-5 reasoning model, mapping its internal phases to distinct development stages (Design, Coding, Code Completion, Code Review, Testing, and Documentation) to create a standardized evaluation framework. We then quantify and compare token distribution (input, output, reasoning) across these stages.
  Our preliminary findings show that the iterative Code Review stage accounts for the majority of token consumption for an average of 59.4% of tokens. Furthermore, we observe that input tokens consistently constitute the largest share of consumption for an average of 53.9%, providing empirical evidence for potentially significant inefficiencies in agentic collaboration. Our results suggest that the primary cost of agentic software engineering lies not in initial code generation but in automated refinement and verification. Our novel methodology can help practitioners predict expenses and optimize workflows, and it directs future research toward developing more token-efficient agent collaboration protocols.

</details>


### [68] [HELIOS: Hierarchical Graph Abstraction for Structure-Aware LLM Decompilation](https://arxiv.org/abs/2601.14598)
*Yonatan Gizachew Achamyeleh,Harsh Thomare,Mohammad Abdullah Al Faruque*

Main category: cs.SE

TL;DR: HELIOS框架将LLM二进制反编译重构为结构化推理任务，通过层次化文本表示控制流和函数调用，结合编译器反馈，显著提升反编译代码的可编译性和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在二进制反编译中仅将代码视为纯文本，忽略了控制流图，导致输出语法脆弱且逻辑不一致，特别是在优化二进制文件上表现不佳。

Method: HELIOS将二进制控制流和函数调用总结为层次化文本表示，包含基本块、后继节点及高级模式（如循环和条件语句），将此表示与原始反编译器输出一起提供给通用LLM，并可选择结合编译器反馈循环。

Result: 在x86_64 HumanEval-Decompile上，HELIOS将平均目标文件可编译性从45.0%提升至85.2%（Gemini 2.0）和从71.4%提升至89.6%（GPT-4.1 Mini）。使用编译器反馈后，可编译性超过94%，功能正确性比纯文本提示提升高达5.6个百分点。

Conclusion: HELIOS无需微调即可在多种架构上保持高语法正确性并减少功能正确性差异，使其成为安全逆向工程工作流程中实用的构建块，可生成可重新编译且语义忠实的代码。

Abstract: Large language models (LLMs) have recently been applied to binary decompilation, yet they still treat code as plain text and ignore the graphs that govern program control flow. This limitation often yields syntactically fragile and logically inconsistent output, especially for optimized binaries. This paper presents \textsc{HELIOS}, a framework that reframes LLM-based decompilation as a structured reasoning task. \textsc{HELIOS} summarizes a binary's control flow and function calls into a hierarchical text representation that spells out basic blocks, their successors, and high-level patterns such as loops and conditionals. This representation is supplied to a general-purpose LLM, along with raw decompiler output, optionally combined with a compiler-in-the-loop that returns error messages when the generated code fails to build.
  On HumanEval-Decompile for \texttt{x86\_64}, \textsc{HELIOS} raises average object file compilability from 45.0\% to 85.2\% for Gemini~2.0 and from 71.4\% to 89.6\% for GPT-4.1~Mini. With compiler feedback, compilability exceeds 94\% and functional correctness improves by up to 5.6 percentage points over text-only prompting. Across six architectures drawn from x86, ARM, and MIPS, \textsc{HELIOS} reduces the spread in functional correctness while keeping syntactic correctness consistently high, all without fine-tuning. These properties make \textsc{HELIOS} a practical building block for reverse engineering workflows in security settings where analysts need recompilable, semantically faithful code across diverse hardware targets.

</details>


### [69] [LLM-Based Repair of C++ Implicit Data Loss Compiler Warnings: An Industrial Case Study](https://arxiv.org/abs/2601.14936)
*Chansong You,Hyun Deok Choi,Jingun Hong*

Main category: cs.SE

TL;DR: 使用LLM自动修复C++项目中隐式数据丢失警告的方法，通过LSP收集上下文、Tree-sitter提取代码、LLM决策生成修复，在大型C++项目中达到92.73%的代码审查接受率。


<details>
  <summary>Details</summary>
Motivation: 减少手动修复编译器警告的工作量，同时保持代码质量和性能，特别是在大型C++项目中处理隐式数据丢失警告的复杂性。

Method: 使用语言服务器协议(LSP)收集代码上下文，Tree-sitter提取相关代码片段，大型语言模型(LLM)评估范围检查的必要性并生成修复方案，考虑性能影响。

Result: 在大型C++项目中测试，修复方案在代码审查中获得92.73%的接受率；相比基准修复策略，减少了39.09%因范围检查和异常处理引入的额外指令；比人工最优解决方案落后13.56%。

Conclusion: LLM方法能有效减少处理编译器警告的手动工作量，同时保持代码质量和性能，有望集成到现有开发工作流中，改善复杂C++项目的代码维护实践。

Abstract: This paper presents a method to automatically fix implicit data loss warnings in large C++ projects using Large Language Models (LLMs). Our approach uses the Language Server Protocol (LSP) to gather context, Tree-sitter to extract relevant code, and LLMs to make decisions and generate fixes. The method evaluates the necessity of range checks concerning performance implications and generates appropriate fixes. We tested this method in a large C++ project, resulting in a 92.73% acceptance rate of the fixes by human developers during the code review. Our LLM-generated fixes reduced the number of warning fix changes that introduced additional instructions due to range checks and exception handling by 39.09% compared to a baseline fix strategy. This result was 13.56% behind the optimal solutions created by human developers. These findings demonstrate that our LLM-based approach can reduce the manual effort to address compiler warnings while maintaining code quality and performance in a real-world scenario. Our automated approach shows promise for integration into existing development workflows, potentially improving code maintenance practices in complex C++ software projects.

</details>


### [70] [SmartOracle -- An Agentic Approach to Mitigate Noise in Differential Oracles](https://arxiv.org/abs/2601.15074)
*Srinath Srinivasan,Tim Menzies,Marcelo D'Amorim*

Main category: cs.SE

TL;DR: SmartOracle使用LLM代理架构自动化JavaScript引擎差分模糊测试中的结果验证，将手动工作流程分解为专门子代理，显著提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: JavaScript引擎差分模糊测试需要手动构建验证oracle，成本高、耗时长、易产生误报，且规范更新时需要重复工作。需要自动化解决方案来替代手动验证流程。

Method: 将手动验证工作流程分解为专门的LLM子代理，这些代理从终端运行和规范查询中独立收集证据，综合判断最终结果。采用代理架构而非单一LLM。

Result: 在历史基准测试中达到0.84召回率和18%误报率；相比Gemini 2.5 Pro基线，提高验证准确性，分析时间减少4倍，API成本降低10倍；在实际模糊测试活动中成功发现V8、JavaScriptCore和GraalJS中的未知规范级问题。

Conclusion: SmartOracle的代理架构在JavaScript引擎差分测试中成功，表明该方法可能适用于其他软件系统，未来将探索这一研究方向。

Abstract: Differential fuzzers detect bugs by executing identical inputs across distinct implementations of the same specification, such as JavaScript interpreters. Validating the outputs requires an oracle and for differential testing of JavaScript, these are constructed manually, making them expensive, time-consuming, and prone to false positives. Worse, when the specification evolves, this manual effort must be repeated.
  Inspired by the success of agentic systems in other SE domains, this paper introduces SmartOracle. SmartOracle decomposes the manual triage workflow into specialized Large Language Model (LLM) sub-agents. These agents synthesize independently gathered evidence from terminal runs and targeted specification queries to reach a final verdict.
  For historical benchmarks, SmartOracle achieves 0.84 recall with an 18% false positive rate. Compared to a sequential Gemini 2.5 Pro baseline, it improves triage accuracy while reducing analysis time by 4$\times$ and API costs by 10$\times$. In active fuzzing campaigns, SmartOracle successfully identified and reported previously unknown specification-level issues across major engines, including bugs in V8, JavaScriptCore, and GraalJS.
  The success of SmartOracle's agentic architecture on Javascript suggests it might be useful other software systems- a research direction we will explore in future work.

</details>


### [71] [Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks](https://arxiv.org/abs/2601.15094)
*Md Zahidul Haque,Saima Afrin,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 多任务QLoRA微调在代码生成、翻译和摘要任务上表现优异，能有效利用迁移学习，在功能正确性和代码质量方面达到或超越单任务QLoRA和多任务全微调的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然QLoRA优化的代码模型在单任务上表现良好，但多任务QLoRA微调的有效性、迁移学习对生成工件的正确性和质量的影响尚未被充分探索。

Method: 研究多任务QLoRA微调在三个代表性代码任务（代码生成、翻译、摘要）上的表现，使用执行基和相似性基指标评估功能正确性，并进行全面的代码质量分析。

Result: 多任务QLoRA能有效利用迁移学习，在正确性和质量方面达到或超越单任务QLoRA和多任务全微调；大模型在正确性和质量间更平衡，小模型能保持功能但质量问题更多。

Conclusion: 多任务QLoRA微调是高效且有效的代码模型优化方法，能平衡资源消耗和性能，为实际部署提供了有前景的解决方案。

Abstract: Large Language Models (LLMs) have proven highly effective in automating software engineering tasks, bridging natural language and code semantics to achieve notable results in code generation and summarization. However, their scale incurs substantial computational costs, making full fine-tuning impractical. Parameter-Efficient Fine-Tuning (PEFT) methods like QLoRA enable efficient specialization with lower resource demands. Recent studies show QLoRA-optimized Large Code Models (LCMs) perform strongly across diverse tasks, yet it remains unclear whether this effectiveness persists when a single model is QLoRA fine-tuned for multiple code-related tasks. The interaction between Multi-task fine-tuning and QLoRA optimization, and how transfer learning affects correctness and quality of generated artifacts, remains largely unexplored. We investigate Multi-task QLoRA fine-tuning across three representative tasks: code generation, translation, and summarization. We evaluate functional correctness through execution-based and similarity-based metrics, complemented by comprehensive code quality analysis--an aspect largely overlooked in prior work. Our findings show that Multi-task QLoRA effectively leverages transfer learning, achieving competitive or superior performance relative to both Single-task QLoRA and Multi-task full fine-tuning. Larger models demonstrate more consistent balance between correctness and quality, whereas smaller models preserve functionality but exhibit a higher incidence of quality-related issues.

</details>


### [72] [Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback](https://arxiv.org/abs/2601.15188)
*Stephan Wallraven,Tim Köhne,Hartmut Westenberger,Andreas Moser*

Main category: cs.SE

TL;DR: 该研究系统评估了不同大语言模型生成ABAP代码的能力，发现强大模型在多次迭代后能达到约75%的成功率，并能有效利用编译器反馈进行改进，而小型模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在许多编程语言中已有成功应用，但目前几乎没有对ABAP代码生成的系统分析。本研究旨在实证分析不同LLM生成语法正确且功能正常的ABAP代码的能力，以及它们如何利用编译器反馈进行迭代改进。

Method: 研究使用包含180个任务的基准测试，包括改编的HumanEval任务和实际的SAP场景。评估不同LLM生成ABAP代码的能力，特别关注它们利用编译器反馈进行迭代改进的效果。

Result: 结果显示模型间存在显著性能差异：更强大的LLM在多次迭代后能达到约75%的成功率，并能从编译器反馈中大幅受益；而小型模型表现明显较弱。

Conclusion: 研究表明强大的LLM在ABAP开发过程中具有很高潜力，特别是在迭代错误修正方面。这为SAP开发环境中的AI辅助编程提供了实证依据。

Abstract: This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.

</details>


### [73] [Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub](https://arxiv.org/abs/2601.15195)
*Ramtin Ehsani,Sakshi Pathak,Shriya Rawal,Abdullah Al Mujahid,Mia Mohammad Imran,Preetha Chatterjee*

Main category: cs.SE

TL;DR: 对GitHub上33k个AI编码代理提交的PR进行大规模研究，分析合并成功与失败的模式，发现文档/CI任务成功率最高，性能/修复任务最差，并建立了拒绝模式的分类体系。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码代理在真实软件项目中作为自主贡献者提交PR的数量快速增长，但对其实际行为以及为何许多PR未能合并的了解很少，需要系统研究AI代理在实际开发环境中的表现。

Method: 采用混合方法：1) 对33k个AI代理提交的PR进行定量分析，从任务类型、代码变更、CI构建结果、评审动态四个维度比较合并与未合并的PR；2) 对600个PR进行定性分析，建立拒绝模式的层次分类体系。

Result: 文档、CI和构建更新任务合并成功率最高，性能和bug修复任务最差；未合并的PR通常涉及更大的代码变更、更多文件修改，且常未通过CI/CD验证；定性分析揭示了缺乏有意义的评审参与、重复PR、不需要的功能实现和代理错位等拒绝原因。

Conclusion: 研究强调了改进未来AI代理工作流成功的关键社会技术和人机协作因素，包括需要更好的任务选择、代码变更管理、CI集成和评审参与机制。

Abstract: AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project's CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.

</details>


### [74] [When Agents Fail: A Comprehensive Study of Bugs in LLM Agents with Automated Labeling](https://arxiv.org/abs/2601.15232)
*Niful Islam,Ragib Shahriar Ayon,Deepak George Thomas,Shibbir Ahmed,Mohammad Wardat*

Main category: cs.SE

TL;DR: 对LLM代理开发中bug类型的首次全面研究，分析了1,187个bug相关帖子，并构建BugReAct代理来自动化bug识别。


<details>
  <summary>Details</summary>
Motivation: LLM代理开发调试困难且成本高，该领域仍处于早期阶段，社区发展不成熟，需要系统理解代理开发中的bug类型、根本原因和影响。

Method: 从Stack Overflow、GitHub和Hugging Face收集1,187个bug相关帖子和代码片段，分析7个主流LLM框架及自定义实现中的bug。构建BugReAct代理（基于ReAct架构）来探索自动化bug识别的可行性。

Result: BugReAct代理（使用Gemini 2.5 Flash）在bug特征标注方面表现优异，平均每个帖子/代码片段的成本仅为0.01美元。

Conclusion: LLM代理开发存在系统性的bug模式，自动化bug识别具有可行性且成本效益高，为代理开发调试提供了实用工具和方法。

Abstract: Large Language Models (LLMs) have revolutionized intelligent application development. While standalone LLMs cannot perform any actions, LLM agents address the limitation by integrating tools. However, debugging LLM agents is difficult and costly as the field is still in it's early stage and the community is underdeveloped. To understand the bugs encountered during agent development, we present the first comprehensive study of bug types, root causes, and effects in LLM agent-based software. We collected and analyzed 1,187 bug-related posts and code snippets from Stack Overflow, GitHub, and Hugging Face forums, focused on LLM agents built with seven widely used LLM frameworks as well as custom implementations. For a deeper analysis, we have also studied the component where the bug occurred, along with the programming language and framework. This study also investigates the feasibility of automating bug identification. For that, we have built a ReAct agent named BugReAct, equipped with adequate external tools to determine whether it can detect and annotate the bugs in our dataset. According to our study, we found that BugReAct equipped with Gemini 2.5 Flash achieved a remarkable performance in annotating bug characteristics with an average cost of 0.01 USD per post/code snippet.

</details>
