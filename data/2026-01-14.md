<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 21]
- [cs.SE](#cs.SE) [Total: 2]
- [tldr.article](#tldr.article) [Total: 11]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs](https://arxiv.org/abs/2601.07994)
*Nayoung Choi,Jonathan Zhang,Jinho D. Choi*

Main category: cs.CL

TL;DR: DyCP是一种轻量级上下文管理方法，通过动态分割和检索相关记忆来改善LLM在长对话中的响应延迟和答案质量，无需额外LLM调用或预定义主题边界。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在对话长度增加时会出现响应延迟增加和答案质量下降的问题，现有方法要么依赖额外的LLM调用来构建记忆，要么进行离线记忆构建而不考虑当前用户话语，导致效率低下或破坏对话连续性。

Method: DyCP是一种轻量级上下文管理方法，在查询时动态分割和检索相关记忆。它保留了对话的顺序结构，无需预定义主题边界，支持高效、自适应的上下文检索。

Result: 在三个长对话基准测试（LoCoMo、MT-Bench+和SCM4LLMs）和多个LLM上，DyCP一致地提高了答案质量，同时减少了响应延迟。

Conclusion: 现代LLM虽然扩展了上下文窗口，但实际的长上下文处理能力仍有差距，因此有效的上下文管理仍然至关重要。DyCP提供了一种高效的解决方案。

Abstract: Large Language Models (LLMs) often exhibit increased response latency and degraded answer quality as dialogue length grows, making effective context management essential. However, existing methods rely on extra LLM calls to build memory or perform offline memory construction without considering the current user utterance, which can introduce inefficiencies or disrupt conversational continuity. We introduce DyCP, a lightweight context management method that dynamically segment and retrieve relevant memory at query time. It preserves the sequential structure of dialogue without predefined topic boundaries and supports efficient, adaptive context retrieval. Across three long-form dialogue benchmarks, LoCoMo, MT-Bench+, and SCM4LLMs, and multiple LLMs, DyCP consistently improves answer quality while reducing response latency. We also examine the gap between modern LLMs' expanded context windows and their actual long-context processing capacity, highlighting the continued importance of effective context management.

</details>


### [2] [LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback](https://arxiv.org/abs/2601.08003)
*Weiyue Li,Mingxiao Song,Zhenda Shen,Dachuan Zhao,Yunfan Long,Yi Li,Yongce Li,Ruyi Yang,Mengyu Wang*

Main category: cs.CL

TL;DR: LLM Review框架通过盲审机制解决多智能体协作中的创意同质化问题，在科幻写作任务上超越基线方法，小模型配合该框架可超越大单智能体模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在创意生成方面存在困难，而多智能体框架虽然能提升推理能力，却可能导致内容同质化，反而阻碍创造力。需要一种既能促进互动又能保持创意多样性的方法。

Method: 提出LLM Review框架，采用盲审机制：智能体交换针对性反馈但独立修改，保持创意轨迹的多样性。同时构建SciFi-100科幻写作数据集，结合LLM评分、人工标注和基于规则的新颖性指标进行综合评估。

Result: 实验表明LLM Review持续优于多智能体基线方法，且小模型配合该框架可以超越更大的单智能体模型，表明交互结构可以替代模型规模。

Conclusion: 通过盲审机制的多智能体协作能够有效提升创意生成质量，同时保持多样性，为LLM的创意应用提供了新思路。

Abstract: Large Language Models (LLMs) often struggle with creative generation, and multi-agent frameworks that improve reasoning through interaction can paradoxically hinder creativity by inducing content homogenization. We introduce LLM Review, a peer-review-inspired framework implementing Blind Peer Review: agents exchange targeted feedback while revising independently, preserving divergent creative trajectories. To enable rigorous evaluation, we propose SciFi-100, a science fiction writing dataset with a unified framework combining LLM-as-a-judge scoring, human annotation, and rule-based novelty metrics. Experiments demonstrate that LLM Review consistently outperforms multi-agent baselines, and smaller models with our framework can surpass larger single-agent models, suggesting interaction structure may substitute for model scale.

</details>


### [3] [Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models](https://arxiv.org/abs/2601.08058)
*Zhenghao He,Guangzhi Xiong,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: 研究发现LLM推理能力由内部潜在特征支持，通过稀疏自编码器识别并操控这些特征，无需显式思维链提示即可触发推理，表明思维链只是激活推理机制的有效方式之一而非必要条件。


<details>
  <summary>Details</summary>
Motivation: 探究思维链提示为何有效，以及它是否是触发大型语言模型推理能力的唯一机制，理解LLM推理的内部工作机制。

Method: 使用稀疏自编码器分析LLM内部表示，识别与推理相关的潜在特征，通过操控这些特征来干预模型推理行为，在多个模型家族和推理基准上进行实验。

Result: 操控单个推理相关潜在特征可显著提高准确率，无需显式思维链提示；对于大型模型，潜在特征操控能达到与标准思维链提示相当的性能，同时输出更高效；推理导向的内部状态在生成早期被触发，并能覆盖阻止显式推理的指令级提示。

Conclusion: LLM的多步推理由可外部激活的潜在内部激活支持，思维链提示是激活该机制的有效方式之一，但并非其必要原因。

Abstract: Chain-of-Thought (CoT) prompting has improved the reasoning performance of large language models (LLMs), but it remains unclear why it works and whether it is the unique mechanism for triggering reasoning in large language models. In this work, we study this question by directly analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), identifying a small set of latent features that are causally associated with LLM reasoning behavior. Across multiple model families and reasoning benchmarks, we find that steering a single reasoning-related latent feature can substantially improve accuracy without explicit CoT prompting. For large models, latent steering achieves performance comparable to standard CoT prompting while producing more efficient outputs. We further observe that this reasoning-oriented internal state is triggered early in generation and can override prompt-level instructions that discourage explicit reasoning. Overall, our results suggest that multi-step reasoning in LLMs is supported by latent internal activations that can be externally activated, while CoT prompting is one effective, but not unique, way of activating this mechanism rather than its necessary cause.

</details>


### [4] [Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning](https://arxiv.org/abs/2601.08105)
*Fabian Spaeh,Tianyi Chen,Chen-Hao Chiang,Bin Shen*

Main category: cs.CL

TL;DR: 该论文研究了代理RAG系统中的查询建议问题，当用户问题超出系统知识范围时，系统能够建议可回答的相似查询，以增强用户交互体验。


<details>
  <summary>Details</summary>
Motivation: 代理RAG系统虽然强大，但其知识范围有限，超出范围的问题可能导致幻觉。现有护栏框架只阻止超出范围的问题，但缺乏为不可回答问题提供可回答查询建议的研究，这在实际工具调用LLM场景中很常见。

Method: 提出鲁棒动态少样本学习方法，从相关工作流中检索示例，系统可以自我学习（如基于先前用户查询），无需额外标注数据。

Result: 在三个基于真实世界用户查询数据集构建的基准测试中，该方法相比少样本和仅检索基线，能产生更相关且可回答的查询建议。

Conclusion: 该方法能够实现更安全、更有效的代理RAG用户交互，解决了传统查询推荐在RAG多步工作流中确保可回答性的挑战。

Abstract: Retrieval-augmented generation with tool-calling agents (agentic RAG) has become increasingly powerful in understanding, processing, and responding to user queries. However, the scope of the grounding knowledge is limited and asking questions that exceed this scope may lead to issues like hallucination. While guardrail frameworks aim to block out-of-scope questions (Rodriguez et al., 2024), no research has investigated the question of suggesting answerable queries in order to complete the user interaction.
  In this paper, we initiate the study of query suggestion for agentic RAG. We consider the setting where user questions are not answerable, and the suggested queries should be similar to aid the user interaction. Such scenarios are frequent for tool-calling LLMs as communicating the restrictions of the tools or the underlying datasets to the user is difficult, and adding query suggestions enhances the interaction with the RAG agent. As opposed to traditional settings for query recommendations such as in search engines, ensuring that the suggested queries are answerable is a major challenge due to the RAG's multi-step workflow that demands a nuanced understanding of the RAG as a whole, which the executing LLM lacks. As such, we introduce robust dynamic few-shot learning which retrieves examples from relevant workflows. We show that our system can be self-learned, for instance on prior user queries, and is therefore easily applicable in practice. We evaluate our approach on three benchmark datasets based on two unlabeled question datasets collected from real-world user queries. Experiments on real-world datasets confirm that our method produces more relevant and answerable suggestions, outperforming few-shot and retrieval-only baselines, and thus enable safer, more effective user interaction with agentic RAG.

</details>


### [5] [WISE-Flow: Workflow-Induced Structured Experience for Self-Evolving Conversational Service Agents](https://arxiv.org/abs/2601.08158)
*Yuqing Zhou,Zhuoer Wang,Jie Yuan,Hong Wang,Samson Koelle,Ziwei Zhu,Wei Niu*

Main category: cs.CL

TL;DR: WISE-Flow是一个工作流中心框架，通过从历史服务交互中提取可重用程序经验，使LLM代理能够自我进化，提升在用户服务环境中的表现。


<details>
  <summary>Details</summary>
Motivation: LLM代理在用户服务中广泛部署但存在错误倾向、重复失败模式和运行变异性大的问题，而通过环境特定训练或手动修补成本高且难以扩展。

Method: 提出WISE-Flow框架：1) 将历史服务交互转换为可重用程序经验，通过前提条件增强的动作块诱导工作流；2) 部署时对齐代理执行轨迹到检索到的工作流；3) 执行前提条件感知的可行性推理以实现状态接地的下一步动作。

Result: 在ToolSandbox和τ²-bench上的实验显示，该方法在不同基础模型上都能带来一致的性能提升。

Conclusion: WISE-Flow能够使代理在用户服务环境中自我进化，通过工作流重用和前提条件推理有效解决LLM代理的失败模式和变异性问题。

Abstract: Large language model (LLM)-based agents are widely deployed in user-facing services but remain error-prone in new tasks, tend to repeat the same failure patterns, and show substantial run-to-run variability. Fixing failures via environment-specific training or manual patching is costly and hard to scale. To enable self-evolving agents in user-facing service environments, we propose WISE-Flow, a workflow-centric framework that converts historical service interactions into reusable procedural experience by inducing workflows with prerequisite-augmented action blocks. At deployment, WISE-Flow aligns the agent's execution trajectory to retrieved workflows and performs prerequisite-aware feasibility reasoning to achieve state-grounded next actions. Experiments on ToolSandbox and $τ^2$-bench show consistent improvement across base models.

</details>


### [6] [SwiftMem: Fast Agentic Memory via Query-aware Indexing](https://arxiv.org/abs/2601.08160)
*Anxin Tian,Yiming Li,Xing Li,Hui-Ling Zhen,Lei Chen,Xianzhi Yu,Zhenhua Dong,Mingxuan Yuan*

Main category: cs.CL

TL;DR: SwiftMem是一个查询感知的智能体记忆系统，通过时间索引和语义DAG-Tag索引实现亚线性检索，解决了现有记忆系统全量检索导致的延迟瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统存在根本性限制：无论查询特性如何，都会对整个存储层进行穷举检索。这种暴力方法随着记忆增长会造成严重的延迟瓶颈，阻碍实时智能体交互。

Method: 提出SwiftMem系统，包含：1）时间索引实现对数时间范围查询；2）语义DAG-Tag索引通过分层标签结构将查询映射到相关主题；3）嵌入-标签协同整合机制，基于语义聚类重组存储以提高缓存局部性。

Result: 在LoCoMo和LongMemEval基准测试中，SwiftMem实现了比最先进基线快47倍的搜索速度，同时保持竞争性准确度，使记忆增强的LLM智能体能够实际部署。

Conclusion: SwiftMem通过查询感知的索引策略有效解决了智能体记忆系统的检索延迟问题，为实时智能体交互提供了可行的解决方案。

Abstract: Agentic memory systems have become critical for enabling LLM agents to maintain long-term context and retrieve relevant information efficiently. However, existing memory frameworks suffer from a fundamental limitation: they perform exhaustive retrieval across the entire storage layer regardless of query characteristics. This brute-force approach creates severe latency bottlenecks as memory grows, hindering real-time agent interactions. We propose SwiftMem, a query-aware agentic memory system that achieves sub-linear retrieval through specialized indexing over temporal and semantic dimensions. Our temporal index enables logarithmic-time range queries for time-sensitive retrieval, while the semantic DAG-Tag index maps queries to relevant topics through hierarchical tag structures. To address memory fragmentation during growth, we introduce an embedding-tag co-consolidation mechanism that reorganizes storage based on semantic clusters to improve cache locality. Experiments on LoCoMo and LongMemEval benchmarks demonstrate that SwiftMem achieves 47$\times$ faster search compared to state-of-the-art baselines while maintaining competitive accuracy, enabling practical deployment of memory-augmented LLM agents.

</details>


### [7] [Relational Knowledge Distillation Using Fine-tuned Function Vectors](https://arxiv.org/abs/2601.08169)
*Andrea Kang,Yingnian Wu,Hongjing Lu*

Main category: cs.CL

TL;DR: 通过微调函数向量（约20个词对）提升关系表示能力，引入复合函数向量增强类比推理，激活修补作为可控机制提升大语言模型的关系知识编码与推理能力。


<details>
  <summary>Details</summary>
Motivation: 概念间关系表示是智能系统理解世界的核心前提。现有因果中介分析发现少量注意力头编码任务表示（函数向量），但如何更好地提取和利用这些关系知识仍需探索。

Method: 1. 使用少量示例（约20个词对）微调函数向量；2. 引入复合函数向量（微调向量的加权组合）提取关系知识；3. 在推理时将复合向量插入LLM激活中增强类比推理。

Result: 1. 微调函数向量在关系词补全任务上优于原始向量（大小模型均有效）；2. 提升关系词解码性能；3. 与人类语义关系相似性判断更一致；4. 复合向量显著提升认知科学和SAT类比问题性能。

Conclusion: 激活修补作为可控机制能有效编码和操作关系知识，同时提升大语言模型的可解释性和推理能力。

Abstract: Representing relations between concepts is a core prerequisite for intelligent systems to make sense of the world. Recent work using causal mediation analysis has shown that a small set of attention heads encodes task representation in in-context learning, captured in a compact representation known as the function vector. We show that fine-tuning function vectors with only a small set of examples (about 20 word pairs) yields better performance on relation-based word-completion tasks than using the original vectors derived from causal mediation analysis. These improvements hold for both small and large language models. Moreover, the fine-tuned function vectors yield improved decoding performance for relation words and show stronger alignment with human similarity judgments of semantic relations. Next, we introduce the composite function vector - a weighted combination of fine-tuned function vectors - to extract relational knowledge and support analogical reasoning. At inference time, inserting this composite vector into LLM activations markedly enhances performance on challenging analogy problems drawn from cognitive science and SAT benchmarks. Our results highlight the potential of activation patching as a controllable mechanism for encoding and manipulating relational knowledge, advancing both the interpretability and reasoning capabilities of large language models.

</details>


### [8] [User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale](https://arxiv.org/abs/2601.08225)
*Jungho Cho,Minbyul Jeong,Sungrae Park*

Main category: cs.CL

TL;DR: 提出用户导向的模拟框架，通过解耦任务生成与用户模拟器，生成更真实的多轮工具使用对话数据，解决现有方法局限于静态工具集和"仅任务解决"轨迹的问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型作为自主代理需要复杂多轮工具使用能力，但现有数据集和方法受限于静态预定义工具集，无法扩展到开放人机协作的复杂性，且纯任务导向设计导致交互不足，无法生成真实场景中的高轮次对话。

Method: 1. 开发自动化任务导向多轮对话生成框架，使用LRM模拟器动态生成领域特定工具；2. 转向用户导向模拟范式，解耦任务生成与专用用户模拟器，模拟人类行为规则（增量请求、逐轮反馈）；3. 构建可插拔生成管道，可从任何状态启动生成，支持单轨迹内多任务完成。

Result: 生成更真实、扩展的多轮对话，反映真实世界问题解决的迭代性质；产生高密度数据集，体现真实人机交互的多方面需求；确保高可扩展性，能够生成扩展的工具使用数据。

Conclusion: 用户导向模拟范式能有效生成更真实的多轮工具使用对话数据，解决现有方法的局限性，为大型推理模型的工具使用能力训练提供高质量、可扩展的数据生成方案。

Abstract: The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in "solely task-solving" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.

</details>


### [9] [D$^2$Plan: Dual-Agent Dynamic Global Planning for Complex Retrieval-Augmented Reasoning](https://arxiv.org/abs/2601.08282)
*Kangcheng Luo,Tinglang Wu,Yansong Feng*

Main category: cs.CL

TL;DR: 提出D²Plan双智能体动态全局规划范式，通过Reasoner和Purifier协作解决检索增强推理中的搜索链构建无效和推理劫持问题，在复杂QA基准上取得优越性能。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的检索增强LLM在多跳推理任务中存在两个关键失败模式：1) 搜索链构建无效，产生错误查询或遗漏关键信息检索；2) 推理被外围证据劫持，导致模型将干扰项误认为有效证据。

Method: 提出D²Plan双智能体动态全局规划范式，包含Reasoner（构建显式全局规划并基于检索反馈动态调整）和Purifier（评估检索相关性并压缩关键信息）。采用两阶段训练框架：基于合成轨迹的监督微调冷启动，以及带有规划导向奖励的强化学习。

Result: 在多个QA基准上的广泛实验表明，D²Plan能够实现更连贯的多步推理和更强的抗干扰信息能力，从而在具有挑战性的QA基准上取得优越性能。

Conclusion: D²Plan通过双智能体协作的全局规划范式有效解决了检索增强推理中的关键挑战，提升了多跳推理的鲁棒性和准确性。

Abstract: Recent search-augmented LLMs trained with reinforcement learning (RL) can interleave searching and reasoning for multi-hop reasoning tasks. However, they face two critical failure modes as the accumulating context becomes flooded with both crucial evidence and irrelevant information: (1) ineffective search chain construction that produces incorrect queries or omits retrieval of critical information, and (2) reasoning hijacking by peripheral evidence that causes models to misidentify distractors as valid evidence. To address these challenges, we propose **D$^2$Plan**, a **D**ual-agent **D**ynamic global **Plan**ning paradigm for complex retrieval-augmented reasoning. **D$^2$Plan** operates through the collaboration of a *Reasoner* and a *Purifier*: the *Reasoner* constructs explicit global plans during reasoning and dynamically adapts them based on retrieval feedback; the *Purifier* assesses retrieval relevance and condenses key information for the *Reasoner*. We further introduce a two-stage training framework consisting of supervised fine-tuning (SFT) cold-start on synthesized trajectories and RL with plan-oriented rewards to teach LLMs to master the **D$^2$Plan** paradigm. Extensive experiments demonstrate that **D$^2$Plan** enables more coherent multi-step reasoning and stronger resilience to irrelevant information, thereby achieving superior performance on challenging QA benchmarks.

</details>


### [10] [AgriAgent: Contract-Driven Planning and Capability-Aware Tool Orchestration in Real-World Agriculture](https://arxiv.org/abs/2601.08308)
*Bo Yang,Yu Zhang,Yunkui Chen,Lanfei Feng,Xiao Xu,Nueraili Aierken,Shijian Li*

Main category: cs.CL

TL;DR: AgriAgent是一个用于真实农业场景的两级智能体框架，通过基于任务复杂度的分层执行策略，简单任务由模态特定智能体直接处理，复杂任务则通过契约驱动规划机制进行多步可验证执行。


<details>
  <summary>Details</summary>
Motivation: 真实农业场景中的智能体系统需要处理从轻量级信息理解到复杂多步执行的各种任务，但现有统一执行范式难以适应任务复杂度差异大和工具可用性不完整的农业环境挑战。

Method: 提出AgriAgent两级智能体框架：基于任务复杂度采用分层执行策略，简单任务由模态特定智能体直接推理处理，复杂任务触发契约驱动规划机制，将任务制定为能力需求，进行能力感知的工具编排和动态工具生成，支持多步可验证执行和失败恢复。

Result: 实验结果表明，与依赖统一执行范式的现有工具中心智能体基线相比，AgriAgent在复杂任务上实现了更高的执行成功率和鲁棒性。

Conclusion: AgriAgent通过分层执行策略有效解决了农业环境中任务复杂度差异大和工具可用性不完整的问题，为真实农业场景提供了更可靠和鲁棒的智能体解决方案。

Abstract: Intelligent agent systems in real-world agricultural scenarios must handle diverse tasks under multimodal inputs, ranging from lightweight information understanding to complex multi-step execution. However, most existing approaches rely on a unified execution paradigm, which struggles to accommodate large variations in task complexity and incomplete tool availability commonly observed in agricultural environments. To address this challenge, we propose AgriAgent, a two-level agent framework for real-world agriculture. AgriAgent adopts a hierarchical execution strategy based on task complexity: simple tasks are handled through direct reasoning by modality-specific agents, while complex tasks trigger a contract-driven planning mechanism that formulates tasks as capability requirements and performs capability-aware tool orchestration and dynamic tool generation, enabling multi-step and verifiable execution with failure recovery. Experimental results show that AgriAgent achieves higher execution success rates and robustness on complex tasks compared to existing tool-centric agent baselines that rely on unified execution paradigms. All code, data will be released at after our work be accepted to promote reproducible research.

</details>


### [11] [Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering](https://arxiv.org/abs/2601.08427)
*Nonghai Zhang,Weitao Ma,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Jingwen Xu*

Main category: cs.CL

TL;DR: Latent-GRPO通过潜在空间几何特性生成内在奖励，无需昂贵的外部验证器，实现2倍以上训练加速，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: GRPO方法依赖昂贵的外部验证器或人工规则，导致计算成本高、训练延迟大，且稀疏奖励阻碍优化效率。需要一种更高效的内在奖励机制。

Method: 提出Latent-GRPO框架，基于潜在空间几何特性：正确推理轨迹的终端token表示形成密集聚类，错误轨迹则分散为离群点。引入迭代鲁棒质心估计(IRCE)算法，通过球面投影缓解幅度波动，通过迭代聚合估计鲁棒的"真理质心"。

Result: 在多个数据集上，方法保持模型性能的同时实现超过2倍的训练加速。广泛结果展示了强大的泛化能力和鲁棒性。

Conclusion: Latent-GRPO通过潜在空间几何特性有效生成密集连续奖励，显著提升训练效率，减少对外部验证器的依赖，具有良好泛化性和鲁棒性。

Abstract: Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but also yields sparse rewards that hinder optimization efficiency. To address these challenges, we propose Latent-GRPO, a framework that derives intrinsic rewards directly from latent space geometry. Crucially, our empirical analysis reveals a compelling geometric property: terminal token representations of correct reasoning trajectories form dense clusters with high intra-class similarity, whereas incorrect trajectories remain scattered as outliers. In light of this discovery, we introduce the Iterative Robust Centroid Estimation (IRCE) algorithm, which generates dense, continuous rewards by mitigating magnitude fluctuations via spherical projection and estimating a robust ``truth centroid'' through iterative aggregation. Experimental results on multiple datasets show that our method maintains model performance while achieving a training speedup of over 2x compared to baselines. Furthermore, extensive results demonstrate strong generalization ability and robustness. The code will be released soon.

</details>


### [12] [Surgical Refusal Ablation: Disentangling Safety from Intelligence via Concept-Guided Spectral Cleaning](https://arxiv.org/abs/2601.08489)
*Tony Cristofano*

Main category: cs.CL

TL;DR: 提出Surgical Refusal Ablation方法，通过正交化拒绝向量来减少有害请求拒绝，同时最小化对模型能力的损害


<details>
  <summary>Details</summary>
Motivation: 现有激活导向方法在减少模型拒绝有害请求时，会因拒绝向量与核心能力电路和语言风格纠缠而导致能力下降和分布漂移

Method: 引入Surgical Refusal Ablation方法：构建独立概念原子表示受保护能力和风格混淆，使用岭正则化谱残差化将拒绝向量正交化于这些方向

Result: 在五个模型上实现深度拒绝减少(0-2%)，对Wikitext-2的困惑度影响极小(平均ΔPPL≈0.02)，分布漂移最小，保持数学和代码能力分布

Conclusion: 常见的"模型损害"通常是"幽灵噪声"，即脏拒绝向量向能力子空间的谱泄漏，SRA方法能有效解决这一问题

Abstract: Safety-aligned language models systematically refuse harmful requests. While activation steering can modulate refusal, ablating the raw "refusal vector" calculated from contrastive harmful and harmless prompts often causes collateral damage and distribution drift. We argue this degradation occurs because the raw vector is polysemantic, entangling the refusal signal with core capability circuits and linguistic style.
  We introduce Surgical Refusal Ablation (SRA) to distill these steering directions. SRA constructs a registry of independent Concept Atoms representing protected capabilities and stylistic confounds, then uses ridge-regularized spectral residualization to orthogonalize the refusal vector against these directions. This yields a clean refusal direction that targets refusal-relevant structure while minimizing disruption to the model's semantic geometry.
  Across five models (Qwen3-VL and Ministral series), SRA achieves deep refusal reduction (0-2%) with negligible perplexity impact on Wikitext-2 (mean delta PPL approx. 0.02) and minimal distribution drift. Notably, standard ablation on Qwen3-VL-4B induces severe drift (first-token KL = 2.088), whereas SRA maintains the original distribution (KL = 0.044) while achieving the same 0% refusal rate. Using teacher-forced perplexity on GSM8K and MBPP as a high-resolution capability proxy, we show SRA preserves math and code distributions. These results suggest that common "model damage" is often "Ghost Noise," defined as the spectral bleeding of the dirty refusal direction into capability subspaces.

</details>


### [13] [ExpSeek: Self-Triggered Experience Seeking for Web Agents](https://arxiv.org/abs/2601.08605)
*Wenyuan Zhang,Xinghua Zhang,Haiyang Yu,Shuaiyi Nie,Bingli Wu,Juwei Yue,Tingwen Liu,Yongbin Li*

Main category: cs.CL

TL;DR: ExpSeek提出了一种新的经验干预方法，通过步级主动寻求经验来增强Web智能体性能，使用熵作为自触发信号，在多个基准测试中显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有经验干预方法主要在任务执行前被动注入经验作为全局上下文，难以适应智能体与环境交互过程中动态变化的上下文观察。

Method: 提出ExpSeek方法，将经验干预转向步级主动寻求：(1) 使用模型内在信号估计步级熵阈值来确定干预时机；(2) 设计步级定制化经验内容。

Result: 在Qwen3-8B和32B模型上的四个挑战性Web智能体基准测试中，ExpSeek分别实现了9.3%和7.5%的绝对提升。实验验证了熵作为自触发信号的可行性，并显示即使是4B小规模经验模型也能显著提升更大智能体模型的性能。

Conclusion: ExpSeek通过步级主动寻求经验的方法有效解决了现有经验干预方法的局限性，显著提升了Web智能体的交互能力。

Abstract: Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.

</details>


### [14] [GraphSearch: Agentic Search-Augmented Reasoning for Zero-Shot Graph Learning](https://arxiv.org/abs/2601.08621)
*Jiajin Liu,Yuanfu Sun,Dongzhe Fan,Qiaoyu Tan*

Main category: cs.CL

TL;DR: GraphSearch：首个将搜索增强推理扩展到图学习的框架，无需任务特定微调即可实现零样本图学习，在图结构数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前搜索增强大推理模型主要处理文本数据，但在图结构数据（如电商、社交网络、科学引用）上的应用不足。图数据包含丰富的拓扑信号，可作为检索的有价值先验，但有效利用图结构面临生成图表达查询和平衡结构语义相关性等挑战。

Method: 提出GraphSearch框架，包含：1）图感知查询规划器，将搜索空间（1跳、多跳或全局邻居）与语义查询解耦；2）图感知检索器，基于拓扑构建候选集并使用混合评分函数排序。具体实现两种遍历模式：GraphSearch-R（递归逐跳扩展邻域）和GraphSearch-F（灵活检索局部和全局邻域）。

Result: 在多个基准测试中，GraphSearch在零样本节点分类和链接预测任务上达到竞争性甚至优于监督图学习方法，取得了最先进的结果。

Conclusion: GraphSearch为图上的智能推理提供了一个灵活且可泛化的范式，能够有效利用图结构进行搜索增强推理。

Abstract: Recent advances in search-augmented large reasoning models (LRMs) enable the retrieval of external knowledge to reduce hallucinations in multistep reasoning. However, their ability to operate on graph-structured data, prevalent in domains such as e-commerce, social networks, and scientific citations, remains underexplored. Unlike plain text corpora, graphs encode rich topological signals that connect related entities and can serve as valuable priors for retrieval, enabling more targeted search and improved reasoning efficiency. Yet, effectively leveraging such structure poses unique challenges, including the difficulty of generating graph-expressive queries and ensuring reliable retrieval that balances structural and semantic relevance. To address this gap, we introduce GraphSearch, the first framework that extends search-augmented reasoning to graph learning, enabling zero-shot graph learning without task-specific fine-tuning. GraphSearch combines a Graph-aware Query Planner, which disentangles search space (e.g., 1-hop, multi-hop, or global neighbors) from semantic queries, with a Graph-aware Retriever, which constructs candidate sets based on topology and ranks them using a hybrid scoring function. We further instantiate two traversal modes: GraphSearch-R, which recursively expands neighborhoods hop by hop, and GraphSearch-F, which flexibly retrieves across local and global neighborhoods without hop constraints. Extensive experiments across diverse benchmarks show that GraphSearch achieves competitive or even superior performance compared to supervised graph learning methods, setting state-of-the-art results in zero-shot node classification and link prediction. These findings position GraphSearch as a flexible and generalizable paradigm for agentic reasoning over graphs.

</details>


### [15] [RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation](https://arxiv.org/abs/2601.08654)
*Yihan Hong,Huaiyuan Yao,Bolin Shen,Wanpeng Xu,Hua Wei,Yushun Dong*

Main category: cs.CL

TL;DR: RULERS框架通过将自然语言评分标准编译为可执行规范，解决LLM评判中的不稳定性、不可验证推理和尺度错配问题，显著提升与人类评估的一致性。


<details>
  <summary>Details</summary>
Motivation: LLM-as-a-Judge范式虽然提供了可扩展的基于标准的评估，但由于生成随机性，冻结的黑盒模型与人类标准对齐仍然困难。存在三个主要问题：提示敏感性导致的评分标准不稳定性、缺乏可审计证据的不可验证推理、以及与人类评分边界的尺度错配。

Method: 提出RULERS框架，包含三个核心组件：1) 将自然语言评分标准编译为版本化不可变包；2) 通过结构化解码强制确定性证据验证；3) 应用基于Wasserstein距离的轻量级后校准，所有操作均不更新模型参数。

Result: 在论文和摘要基准测试上的广泛实验表明，RULERS在人类一致性方面显著优于代表性基线方法，对对抗性评分标准扰动保持强稳定性，并使较小模型能够与较大的专有评判模型相媲美。

Conclusion: 可靠的LLM评判需要可执行的评分标准、可验证的证据和校准的尺度，而不仅仅是提示词措辞。RULERS框架通过编译器-执行器架构实现了这一目标。

Abstract: The LLM-as-a-Judge paradigm promises scalable rubric-based evaluation, yet aligning frozen black-box models with human standards remains a challenge due to inherent generation stochasticity. We reframe judge alignment as a criteria transfer problem and isolate three recurrent failure modes: rubric instability caused by prompt sensitivity, unverifiable reasoning that lacks auditable evidence, and scale misalignment with human grading boundaries. To address these issues, we introduce RULERS (Rubric Unification, Locking, and Evidence-anchored Robust Scoring), a compiler-executor framework that transforms natural language rubrics into executable specifications. RULERS operates by compiling criteria into versioned immutable bundles, enforcing structured decoding with deterministic evidence verification, and applying lightweight Wasserstein-based post-hoc calibration, all without updating model parameters. Extensive experiments on essay and summarization benchmarks demonstrate that RULERS significantly outperforms representative baselines in human agreement, maintains strong stability against adversarial rubric perturbations, and enables smaller models to rival larger proprietary judges. Overall, our results suggest that reliable LLM judging requires executable rubrics, verifiable evidence, and calibrated scales rather than prompt phrasing alone. Code is available at https://github.com/LabRAI/Rulers.git.

</details>


### [16] [Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization](https://arxiv.org/abs/2601.08682)
*Kushal Chawla,Chenyang Zhu,Pengshan Cai,Sangwoo Cho,Scott Novotney,Ayushman Singh,Jonah Lewis,Keasha Safewright,Alfy Samuel,Erin Babinsky,Shi-Xiong Zhang,Sambit Sahu*

Main category: cs.CL

TL;DR: 该论文介绍了一个工业案例研究，开发了一个用于总结多方对话的智能体系统，分享了从评估方法到实际部署的全生命周期实践经验。


<details>
  <summary>Details</summary>
Motivation: 多方对话摘要对知识传递和运营效率至关重要，但现有研究主要使用静态数据集，而实际场景中需求会不断演变，需要构建可靠、适应性强的摘要系统。

Method: 采用智能体架构开发多方对话摘要系统，通过任务分解实现组件化优化，并分享了包括评估方法、数据瓶颈处理、供应商锁定问题等全生命周期实践经验。

Result: 提出了在需求演变和任务主观性情况下构建可靠摘要系统的实用方法，强调了智能体架构在组件优化、数据瓶颈识别和提示词可移植性方面的优势与挑战。

Conclusion: 智能体架构为多方对话摘要提供了有效的解决方案，但实际部署中需要考虑评估方法、数据质量、提示词可移植性等现实挑战，为未来研究和实践提供了重要指导。

Abstract: Summarization of multi-party dialogues is a critical capability in industry, enhancing knowledge transfer and operational effectiveness across many domains. However, automatically generating high-quality summaries is challenging, as the ideal summary must satisfy a set of complex, multi-faceted requirements. While summarization has received immense attention in research, prior work has primarily utilized static datasets and benchmarks, a condition rare in practical scenarios where requirements inevitably evolve. In this work, we present an industry case study on developing an agentic system to summarize multi-party interactions. We share practical insights spanning the full development lifecycle to guide practitioners in building reliable, adaptable summarization systems, as well as to inform future research, covering: 1) robust methods for evaluation despite evolving requirements and task subjectivity, 2) component-wise optimization enabled by the task decomposition inherent in an agentic architecture, 3) the impact of upstream data bottlenecks, and 4) the realities of vendor lock-in due to the poor transferability of LLM prompts.

</details>


### [17] [RAGShaper: Eliciting Sophisticated Agentic RAG Skills via Automated Data Synthesis](https://arxiv.org/abs/2601.08699)
*Zhengwei Tao,Bo Li,Jialong Wu,Guochen Yan,Huanyao Zhang,Jiahao Xu,Haitao Mi,Wentao Zhang*

Main category: cs.CL

TL;DR: RAGShaper是一个自动化合成RAG任务和智能体轨迹的数据生成框架，通过构建包含对抗性干扰的信息树和约束导航策略，显著提升了智能体在噪声环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前智能体RAG系统面临高质量训练数据稀缺的问题，人工标注无法规模化且难以捕捉处理检索失败所需的动态推理策略，阻碍了鲁棒智能体的开发。

Method: 提出RAGShaper框架：1) InfoCurator构建包含感知和认知层面对抗性干扰的密集信息树；2) 约束导航策略强制教师智能体面对这些干扰，生成展示错误纠正和噪声拒绝的轨迹。

Result: 在综合实验中，使用合成语料库训练的模型显著优于现有基线，在噪声密集和复杂检索任务中表现出卓越的鲁棒性。

Conclusion: RAGShaper通过自动化数据合成有效解决了RAG智能体训练数据稀缺问题，生成的对抗性训练数据能够显著提升智能体在真实噪声环境中的性能。

Abstract: Agentic Retrieval-Augmented Generation (RAG) empowers large language models to autonomously plan and retrieve information for complex problem-solving. However, the development of robust agents is hindered by the scarcity of high-quality training data that reflects the noise and complexity of real-world retrieval environments. Conventional manual annotation is unscalable and often fails to capture the dynamic reasoning strategies required to handle retrieval failures. To bridge this gap, we introduce RAGShaper, a novel data synthesis framework designed to automate the construction of RAG tasks and robust agent trajectories. RAGShaper incorporates an InfoCurator to build dense information trees enriched with adversarial distractors spanning Perception and Cognition levels. Furthermore, we propose a constrained navigation strategy that forces a teacher agent to confront these distractors, thereby eliciting trajectories that explicitly demonstrate error correction and noise rejection. Comprehensive experiments confirm that models trained on our synthesized corpus significantly outperform existing baselines, exhibiting superior robustness in noise-intensive and complex retrieval tasks.

</details>


### [18] [Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents](https://arxiv.org/abs/2601.08742)
*Xin Quan,Jiafeng Xiong,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 提出了Attributional NLI框架，将社会心理学原理融入自然语言推理，通过Undercover-V游戏评估LLM在复杂多智能体环境中的意图推理能力，神经符号智能体表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统自然语言推理无法捕捉复杂交互系统中基于意图的细微推理，而大型语言模型在多智能体环境中的归因推理能力尚未充分探索，需要新的评估框架。

Method: 提出Attributional NLI框架，结合社会心理学原理，通过文本游戏Undercover-V实验三种LLM智能体：标准NLI智能体、Att-NLI智能体（使用溯因-演绎推理）、神经符号Att-NLI智能体（结合外部定理证明器）。

Result: 实验显示归因推理能力存在明显层次结构，神经符号智能体表现最佳，平均胜率达到17.08%，显著优于其他智能体。

Conclusion: Att-NLI框架有助于开发具有复杂推理能力的智能体，同时凸显了神经符号AI在构建理性LLM多智能体系统中的潜力。

Abstract: Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.

</details>


### [19] [To Retrieve or To Think? An Agentic Approach for Context Evolution](https://arxiv.org/abs/2601.08747)
*Rubing Chen,Jian Wang,Wenjie Li,Xiao-Yong Wei,Qing Li*

Main category: cs.CL

TL;DR: ACE框架通过类人元认知机制动态决定何时检索外部知识、何时利用现有知识推理，减少冗余检索并提升多跳问答性能


<details>
  <summary>Details</summary>
Motivation: 传统检索增强方法在每个推理步骤都进行检索，这种暴力策略不仅计算成本高，还会因引入无关噪声而降低性能。需要更智能的上下文演化机制来优化知识密集型推理任务。

Method: 提出Agentic Context Evolution (ACE)框架，受人类元认知启发，通过中央协调器代理使用多数投票策略动态决定何时激活检索代理获取外部证据，何时激活推理代理进行内部分析和上下文精炼。

Result: 在具有挑战性的多跳问答基准测试中，ACE在准确率上显著优于竞争基线，同时实现了高效的token消耗，通过消除冗余检索步骤保持了简洁且演化后的上下文。

Conclusion: ACE为复杂知识密集型任务提供了有价值的上下文演化生成方法，展示了通过智能决策机制平衡外部检索和内部推理的重要性。

Abstract: Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks.However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting.It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.

</details>


### [20] [Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge](https://arxiv.org/abs/2601.08808)
*Yao Tang,Li Dong,Yaru Hao,Qingxiu Dong,Furu Wei,Jiatao Gu*

Main category: cs.CL

TL;DR: 提出Multiplex Thinking方法，通过采样K个候选token并聚合为单个连续多路复用token，实现软推理机制，在保持标准离散生成特性的同时支持强化学习优化，在数学推理基准上优于传统CoT方法。


<details>
  <summary>Details</summary>
Motivation: 传统CoT方法虽然能提升复杂推理能力，但会产生长序列、低带宽的token流。相比之下，人类推理时通常保持对多个可能下一步的分布。因此需要一种既能保持词汇嵌入先验和采样动态，又能紧凑表示多个可能推理路径的方法。

Method: 提出Multiplex Thinking：在每个推理步骤中，采样K个候选token，将它们的嵌入聚合为单个连续多路复用token。这种方法保持标准离散生成的特性，同时产生可处理的多路复用轨迹分布，可直接用on-policy强化学习优化。具有自适应性：模型置信度高时接近离散CoT，不确定时紧凑表示多个可能下一步。

Result: 在多个具有挑战性的数学推理基准测试中，Multiplex Thinking在Pass@1到Pass@1024的所有指标上都优于强离散CoT和RL基线方法，同时产生更短的序列。

Conclusion: Multiplex Thinking提供了一种有效的软推理机制，通过连续多路复用token表示多个推理可能性，既保持了标准生成的特性，又支持强化学习优化，在数学推理任务上表现出色且更高效。

Abstract: Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.

</details>


### [21] [Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System](https://arxiv.org/abs/2601.08829)
*Hsiang-Wei Huang,Junbin Lu,Kuang-Ming Chen,Jenq-Neng Hwang*

Main category: cs.CL

TL;DR: 本文研究在Elo排名评审系统中使用LLM代理评审员，通过真实会议论文提交进行模拟，比较了包含Elo评分和评审员记忆的不同条件，发现Elo提高了领域主席决策准确性，但评审员会适应性地利用系统而不增加评审努力。


<details>
  <summary>Details</summary>
Motivation: 探索在学术论文评审系统中使用LLM代理评审员的动态，特别是研究Elo排名系统如何影响评审质量和决策准确性，以及评审员如何适应这种评分机制。

Method: 使用真实会议论文提交，构建包含多个具有不同角色的LLM代理评审员的模拟系统，由领域主席主持多轮评审互动。比较基线设置与包含Elo评分和评审员记忆的条件。

Result: 模拟结果显示：1) 引入Elo评分提高了领域主席决策准确性；2) 评审员会适应性地利用Elo系统，但不会增加实际评审努力；3) 揭示了评审员在Elo系统中的策略性行为。

Conclusion: Elo排名系统可以改善学术评审的决策质量，但需要警惕评审员可能策略性地利用系统而不真正提升评审质量的问题。

Abstract: In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [22] [Cognitive Biases in LLM-Assisted Software Development](https://arxiv.org/abs/2601.08045)
*Xinyi Zhou,Zeinadsadat Saghi,Sadra Sabouri,Rahul Pandita,Mollie McGuire,Souti Chattopadhyay*

Main category: cs.SE

TL;DR: 首次全面研究LLM辅助开发中的认知偏见，发现48.8%的程序员行为存在偏见，其中56.4%与LLM交互相关，并建立了包含15个偏见类别的分类法。


<details>
  <summary>Details</summary>
Motivation: LLM在软件开发中的广泛应用正在将编程从解决方案生成转变为解决方案评估活动，这种转变可能放大现有决策偏见或创造全新偏见。需要研究认知偏见在AI协作开发中的表现和影响。

Method: 采用混合方法：对14名学生和专业开发人员进行观察研究，随后对22名开发人员进行调查。通过定性比较传统非LLM工作流程中的偏见类别，系统分析了90个特定于开发者-LLM交互的认知偏见。

Result: 发现48.8%的程序员行为存在偏见，其中56.4%与LLM交互相关。LLM相关行为更可能与新颖偏见相关。建立了由认知心理学家验证的包含15个偏见类别的分类法。

Conclusion: LLM辅助开发引入了新的认知偏见挑战。研究为开发者提供了工具和实践建议，并为LLM工具构建者提出了减轻人类-AI编程中认知偏见的建议。

Abstract: The widespread adoption of Large Language Models (LLMs) in software development is transforming programming from a solution-generative to a solution-evaluative activity. This shift opens a pathway for new cognitive challenges that amplify existing decision-making biases or create entirely novel ones. One such type of challenge stems from cognitive biases, which are thinking patterns that lead people away from logical reasoning and result in sub-optimal decisions. How do cognitive biases manifest and impact decision-making in emerging AI-collaborative development? This paper presents the first comprehensive study of cognitive biases in LLM-assisted development. We employ a mixed-methods approach, combining observational studies with 14 student and professional developers, followed by surveys with 22 additional developers. We qualitatively compare categories of biases affecting developers against the traditional non-LLM workflows. Our findings suggest that LLM-related actions are more likely to be associated with novel biases. Through a systematic analysis of 90 cognitive biases specific to developer-LLM interactions, we develop a taxonomy of 15 bias categories validated by cognitive psychologists. We found that 48.8% of total programmer actions are biased, and developer-LLM interactions account for 56.4% of these biased actions. We discuss how these bias categories manifest, present tools and practices for developers, and recommendations for LLM tool builders to help mitigate cognitive biases in human-AI programming.

</details>


### [23] [TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback](https://arxiv.org/abs/2601.08734)
*Prithwish Jana,Sam Davidson,Bhavana Bhasker,Andrey Kan,Anoop Deoras,Laurent Callot*

Main category: cs.SE

TL;DR: TerraFormer是一个神经符号框架，结合监督微调与验证器引导的强化学习，用于基础设施即代码（IaC）的生成和变异，通过形式化验证工具提供语法、可部署性和策略合规性反馈。


<details>
  <summary>Details</summary>
Motivation: 自动化基础设施即代码具有挑战性，大型语言模型从自然语言生成配置时经常出错，需要更可靠的解决方案。

Method: 提出TerraFormer框架，结合监督微调与验证器引导的强化学习，使用形式化验证工具提供反馈，并通过多阶段验证和迭代LLM自校正创建了两个高质量数据集TF-Gen和TF-Mutn。

Result: TerraFormer在IaC-Eval上比基础LLM提升15.94%正确率，在TF-Gen和TF-Mutn测试集上分别提升11.65%和19.60%，优于包括Sonnet 3.7、DeepSeek-R1和GPT-4.1等大50倍的模型，在最佳实践和安全性合规方面表现最佳。

Conclusion: TerraFormer通过神经符号方法显著提高了IaC生成和变异的正确性，超越了更大的LLM模型，在实践和安全性方面表现优异。

Abstract: Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [24] [Why We Built Our Own Background Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbuilders.ramp.com%2Fpost%2Fwhy-we-built-our-background-agent%3Futm_source=tldrnewsletter/1/0100019bb71960e5-2a5708c1-6414-4fd3-8cfb-5852c35dc118-000000/i75HyLJkPj-xkG6FvPFftETaZIm0oVAQG6YWn4KvIzQ=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Inspect是一个能够自我验证工作的编码代理，通过沙盒化VM环境提供完整开发上下文和工具，可以运行测试、检查遥测数据、查询功能标志，并可视化验证工作结果。


<details>
  <summary>Details</summary>
Motivation: 为了解决编码代理在验证自身工作方面的闭环问题，需要构建一个能够像工程师一样拥有完整本地开发环境的代理，确保其工作能够被可靠验证。

Method: 构建Inspect编码代理，在沙盒化虚拟机中运行每个会话，提供工程师本地开发所需的所有上下文和工具，包括测试运行、遥测检查、功能标志查询等功能，并支持可视化验证和实时预览。

Result: 开发了Inspect编码代理，能够有效验证自身工作，提供截图和实时预览，并在文章中公开了详细规范，使任何人都能复制该工具。

Conclusion: 通过构建具有完整开发环境和自我验证能力的编码代理，可以有效解决编码代理工作的可靠性验证问题，该方法的规范已公开供社区使用。

Abstract: Why We Built Our Own Background Agent (17 minute read) Inspect is a coding agent that closes the loop on verifying its work by having all of the context and tools needed to prove it. It can run tests, review telemetry, and query feature flags, and it visually verifies its work and gives users screenshots and live previews. Each session runs in a sandboxed VM with everything an engineer would have locally. This post contains the spec for Inspect so anyone can replicate the tool.

</details>


### [25] [First impressions of Claude Cowork, Anthropic's general agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F12%2Fclaude-cowork%2F%23atom-everything%3Futm_source=tldrnewsletter/1/0100019bb71960e5-2a5708c1-6414-4fd3-8cfb-5852c35dc118-000000/1fktnm_Gr_PkkjBlIHyjOGJApRDU6-hjm_xQrKndTpQ=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Cowork是Anthropic推出的通用代理工具，现已作为研究预览版向Max订阅者提供，集成在更新的Claude Desktop macOS应用中，旨在将Claude Code的强大功能带给更广泛的用户群体。


<details>
  <summary>Details</summary>
Motivation: 让更多用户能够使用Claude Code的强大功能，通过提供更友好、不那么令人生畏的界面来扩大用户群体。

Method: 开发通用代理工具Claude Cowork，集成到Claude Desktop macOS应用中，提供类似常规Claude Code的桌面界面但更易用的UI。

Result: Claude Cowork现已作为研究预览版向Max订阅者提供，功能截图已在文章中展示，界面与常规Claude Code桌面界面相似但更友好。

Conclusion: Claude Cowork作为通用代理工具，成功将Claude Code的强大功能通过更易用的界面带给更广泛的用户群体。

Abstract: First impressions of Claude Cowork, Anthropic's general agent (8 minute read) Claude Cowork is a general agent with a UI now available as a research preview to Max subscribers as part of the updated Claude Desktop macOS application. It looks very similar to the desktop interface for regular Claude Code. The general agent is designed to bring the powerful capabilities of Claude Code to a wider audience with a less intimidating interface. Screenshots of the feature are available in the article.

</details>


### [26] [AI Agents Are a Stress Test for Your Dev Stack](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Fagent-stress-test%2F%3Futm_source=tldrnewsletter/1/0100019bb71960e5-2a5708c1-6414-4fd3-8cfb-5852c35dc118-000000/p6bwWXhCV8GC-fmmK2UVyBD9g4w_NN8-Yez0fnmwzaM=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理循环暴露了开发环境的脆弱性、非标准化和部落化问题


<details>
  <summary>Details</summary>
Motivation: AI代理在开发环境中的运行揭示了当前开发栈存在的根本性问题，包括脆弱性、缺乏标准化以及过度依赖部落知识

Method: 通过分析AI代理在开发环境中的循环运行过程，识别开发栈的脆弱点和非标准化问题

Result: AI代理循环暴露了开发环境的脆弱性、非标准化配置和过度依赖部落知识的问题

Conclusion: AI代理对开发栈构成了压力测试，揭示了需要改进的领域，包括标准化、稳定性和可重复性

Abstract: AI Agents Are a Stress Test for Your Dev Stack (5 minute read) AI agent loops expose how brittle, non-standard, and half-tribal our development environments really are.

</details>


### [27] [Agents that don't suck](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FYplRYs/2/0100019bb73f6677-4e3825e2-dce1-4327-a0bb-6a806cc908b4-000000/p4Mi3zm0w3gwt0qTl3CAdy2XMWb69msijQogR19bkd4=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Databricks推出Agent Bricks平台，帮助企业构建准确、可靠、基于自身数据的AI智能体，提供自动评估、目标导向评分和人工反馈，确保智能体质量，支持实际业务应用


<details>
  <summary>Details</summary>
Motivation: 当前大多数AI智能体在试点阶段就失败，无法真正投入生产环境。企业需要能够准确、可靠、基于自身数据的智能体解决方案，而不是依赖通用基准测试的猜测性方法

Method: Agent Bricks平台提供端到端智能体构建工具，包括自动评估系统、基于业务目标的评分机制、人工反馈循环，确保智能体质量与业务需求对齐

Result: 该平台能够交付真正可用于实际工作的智能体，这些智能体准确可靠、基于企业数据，能够促进业务增长，避免传统智能体开发中的猜测和通用基准测试问题

Conclusion: Agent Bricks通过提供质量可控、业务导向的智能体开发平台，解决了AI智能体从试点到生产部署的难题，使企业能够构建真正有效的业务智能体

Abstract: Agents that don't suck (Sponsor) Most AI agents never make it past the pilot. Agent Bricks by Databricks helps you build agents that actually work — accurate, reliable and grounded in your data. It gives you a clear read on quality: automatic evaluation, scores tied to your goals and human feedback to keep improving accuracy. No guesswork. No generic benchmarks. It's AI built for how your business runs. Agent Bricks delivers agents ready for real work — the kind that grows your business and b...

</details>


### [28] [What the Perplexity-Amazon lawsuit could mean for digital advertising](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.marketingbrew.com%2Fstories%2F2026%2F01%2F12%2Fperplexity-amazon-lawsuit-agentic-AI-retail-media%3Futm_source=tldrmarketing/1/0100019bb73f6677-4e3825e2-dce1-4327-a0bb-6a806cc908b4-000000/63GmHSFyU6z83aAg1ttSmFq-aXCK6KUgNoRUx8REe4c=439)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 亚马逊起诉Perplexity AI，指控其Comet购物代理工具绕过广告和品牌体验，构成安全风险并伪装人类行为，核心争议是AI代理购物是否会破坏广告支持的商业模式。


<details>
  <summary>Details</summary>
Motivation: 探讨AI购物代理对数字广告商业模式的潜在威胁，特别是当AI代理代替人类进行购物时，是否会绕过传统广告曝光和品牌控制体验，从而影响广告支持的电商生态系统。

Method: 通过分析亚马逊与Perplexity AI之间的法律诉讼案例，探讨AI购物代理（如Comet）的技术特点、运作方式及其对广告曝光、品牌体验和安全性的影响。

Result: 揭示了AI购物代理可能绕过传统广告和促销活动，减少品牌曝光机会，同时存在安全风险（如伪装人类行为），这对依赖广告收入的电商平台构成潜在威胁。

Conclusion: AI购物代理的兴起可能颠覆传统数字广告商业模式，需要在技术创新与商业可持续性之间找到平衡，法律诉讼结果将对此类技术的发展方向产生重要影响。

Abstract: What the Perplexity-Amazon lawsuit could mean for digital advertising (3 minute read) Amazon's lawsuit against Perplexity AI centers on whether agentic shopping tools undermine ad-supported commerce by bypassing promotions and brand-controlled experiences. Amazon claims Perplexity's Comet agent poses security risks and disguises automated activity as human behavior. Perplexity argues the real issue is lost advertising exposure when AI agents shop instead of people. If agents transact without ...

</details>


### [29] [How to build agents with filesystems and bash](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fhow-to-build-agents-with-filesystems-and-bash%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/zVUSuCS7BCPIZmYH1dpjdveRKGEB3q9i4ZEnyry4S_E=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 提出了一种基于标准文件系统和Bash工具的AI智能体架构，利用LLM对代码和Unix命令的原生理解能力，通过文件系统结构实现高效信息检索


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中提示词堆叠和向量搜索不精确的问题，提供更自然的数据层次结构、精确检索和最小上下文负载

Method: 采用标准文件系统和Bash工具构建AI智能体，利用LLM对代码和Unix命令的原生理解能力，通过文件系统导航和检索结构化数据

Result: 该架构能够克服传统方法的局限性，提供更高效的信息检索方式，类似于探索代码库的过程

Conclusion: 基于文件系统和Bash工具的简单架构是构建AI智能体的有效方法，利用LLM的现有能力实现更好的数据组织和检索

Abstract: How to build agents with filesystems and bash (3 minute read) A simple, effective architecture for AI agents is a standard filesystem and Bash tools. Since LLMs have a native understanding of code and Unix commands, agents can easily navigate and retrieve information from data structured as files, much like exploring a codebase. This approach overcomes the limitations of prompt stuffing and imprecise vector search by offering natural data hierarchies, exact retrieval, and minimal context load...

</details>


### [30] [Replacing LLM date math with deterministic scripts to improve calendar accuracy](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjessitron.com%2F2026%2F01%2F12%2Fmaking-ai-do-things-right-introduce-determinism%2F%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/jnOz2pOxgM7EQBrJWEGN0W1_377kNUp6Q_B36KtvIJc=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用确定性脚本替代LLM进行日期计算，提高日历工具准确性


<details>
  <summary>Details</summary>
Motivation: LLM（如Claude）在日期计算上经常出错，导致日历工具（如gcalcli）出现错误，需要更可靠的解决方案

Method: 将日期计算任务从LLM转移到确定性脚本，通过更新CLAUDE.md指令让AI执行代码而非手动推理

Result: 消除了幻觉问题，创建了可靠、可重复的AI工作流程，提高了日历准确性

Conclusion: 通过将计算任务委托给确定性脚本而非依赖LLM推理，可以显著提高AI工具的准确性和可靠性

Abstract: Replacing LLM date math with deterministic scripts to improve calendar accuracy (3 minute read) LLMs like Claude frequently fail at date math, causing calendar errors in tools like gcalcli. Offloading calculations to deterministic scripts ensures accuracy. Developers can eliminate hallucinations and create reliable, repeatable AI workflows by updating CLAUDE.md instructions to execute code instead of manual reasoning.

</details>


### [31] [Introducing Cowork](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclaude.com%2Fblog%2Fcowork-research-preview%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/AaT3lpNY2yObaIvOqpK6UEH8mEFj5JiMCToPaVjlIuU=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Cowork是一个新功能，让非开发者也能通过让Claude直接访问电脑文件来与之交互，实现文件读写和任务自动化


<details>
  <summary>Details</summary>
Motivation: 扩展Claude Code功能，使非开发者也能利用AI助手完成文件管理和文档处理等任务，降低技术门槛

Method: 通过让Claude直接访问用户指定文件夹中的文件，实现文件读取、编辑和创建功能，增强AI代理的自主性

Result: 开发了Claude Cowork功能，使任何人都能通过文件访问与Claude交互，完成文档组织和报告起草等任务

Conclusion: Claude Cowork成功扩展了AI助手的应用范围，使非技术用户也能受益于自动化文件处理能力

Abstract: Introducing Cowork (5 minute read) Claude Cowork is a new feature that extends Claude Code to allow anyone, not just developers, to interact with Claude by giving it direct access to files on their computer. This allows Claude to read, edit, or create files within a designated folder, completing tasks like organizing documents or drafting reports with enhanced agency.

</details>


### [32] [The Key to Agentic Success? BASH Is All You Need](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fthe-key-to-agentic-success-let-unix-bash-lead-the-way%2F%3Futm_source=tldrdev/1/0100019bb741ef7c-0865c602-ddd9-4d0f-86ec-9d9da778679c-000000/MTRaD9AzSvsaJm0OCNFTe1xpIRag5hEqHr_DLhwAPGo=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用简单BASH shell和模块化Unix工具的最小化AI代理架构比复杂过度设计的系统更有效，Vercel通过简化其内部数据代理d0使用基本BASH命令进行直接文件查询，实现了更快、更准确、更易管理的操作。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理架构往往过度复杂和过度设计，导致效率低下、难以管理。研究发现简单、模块化的方法可能比复杂系统更有效。

Method: 采用最小化AI代理架构，使用简单的BASH shell和模块化Unix工具。具体案例中，Vercel简化其内部数据代理d0的设计，使用基本BASH命令进行直接文件查询。

Result: 简化后的系统实现了更快、更准确、更易管理的操作。BASH-based方法在代理性能方面表现出色。

Conclusion: 简单、模块化的BASH-based AI代理架构比复杂、过度设计的系统更有效，是代理成功的关键。

Abstract: The Key to Agentic Success? BASH Is All You Need (7 minute read) Minimalist AI agent architectures using simple BASH shells and modular Unix tools are proving more effective than complex, over-engineered systems. Vercel, for example, improved its internal data agent, d0, by simplifying its design to use basic BASH commands for direct file interrogation, resulting in faster, more accurate, and easier-to-manage operations.

</details>


### [33] [Pwning Claude Code in 8 Different Ways](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fflatt.tech%2Fresearch%2Fposts%2Fpwning-claude-code-in-8-different-ways%2F%3Futm_source=tldrinfosec/1/0100019bb7af1200-69b5e91c-4eb2-4dea-beca-4a56a268e828-000000/C7ODrqx2eA6mtJVOUz1sUl7ZSP-7eeoE5Ndp2zs73Xg=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code存在8种不同的系统命令执行漏洞，攻击者可通过绕过其命令安全机制执行任意命令，主要问题源于脆弱的正则表达式黑名单和Bash变量扩展技巧。


<details>
  <summary>Details</summary>
Motivation: 揭示Claude Code的安全漏洞，展示其命令安全机制的脆弱性，帮助用户和开发者了解潜在风险并改进安全防护。

Method: 通过分析Claude Code的命令安全机制，发现了8种不同的绕过技术：包括利用脆弱的正则表达式黑名单（针对man、sort、sed、git、xargs、rg等"安全"命令）和Bash变量扩展技巧来隐藏真实负载。

Result: 攻击者能够将看似只读的操作转化为命令执行路径，成功绕过Claude Code的安全防护，执行任意系统命令而无需用户明确批准。

Conclusion: Claude Code的命令安全机制存在严重缺陷，需要更强大的安全防护措施，不能仅依赖正则表达式黑名单，而应采用更全面的安全策略。

Abstract: Pwning Claude Code in 8 Different Ways (9 minute read) Claude Code was found to execute arbitrary system commands without explicit user approval through eight distinct techniques that bypassed its command-safety mechanisms. These issues stemmed from fragile regex-based blocklists on “safe” commands like man, sort, sed, git, xargs, and rg, plus subtle Bash variable expansion tricks that hid real payloads. An attacker could turn supposedly read‑only operations into command execution paths by ab...

</details>


### [34] [Sentry's AI Debugger Uses Your Actual Error Data – Not Just Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsentry.io%2Fproduct%2Fseer%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=seer-fy26q4-seerlaunch%26utm_content=newsletter-product-lp-learnmore/2/0100019bb7b7e8af-552385ac-51b3-4920-b99b-87aa4485512f-000000/hUGxm0PZCtJIoIbIuGmvUb55YFDwFRT8q3AVIjmeIVw=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sentry的AI调试器Seer利用完整的错误上下文数据（堆栈跟踪、日志、面包屑等）而非仅源代码，实现95%准确率的根因定位和自动修复


<details>
  <summary>Details</summary>
Motivation: 现有AI编程工具仅能查看源代码，缺乏完整的错误上下文信息，导致调试准确性受限。需要利用实际错误数据（堆栈跟踪、日志、面包屑、提交历史等）来提高调试精度

Method: Seer作为Sentry的AI调试代理，当Sentry记录问题时，Seer分析所有可用上下文（堆栈跟踪、日志、面包屑、跨度、提交历史和完整错误上下文），然后建议修复方案并自动创建PR

Result: 能够以95%的准确率定位根本原因，即使在开发者从未接触过的代码库部分也能有效工作

Conclusion: 利用完整的错误上下文数据而非仅源代码，可以显著提高AI调试工具的准确性和实用性，实现更智能的自动修复

Abstract: Sentry's AI Debugger Uses Your Actual Error Data – Not Just Code (Sponsor) Most AI coding tools only see your source code. Seer, Sentry's AI debugging agent, sees everything Sentry knows: stack traces, logs, breadcrumbs, spans, commit history, and the full error context. That's why it can pinpoint root causes with 95% accuracy – even in parts of your codebase you've never touched. How it works: Sentry logs an issue. Seer analyzes it using all available context. Seer suggests a fix, opens a PR...

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [MemoBrain: Executive Memory as an Agentic Brain for Reasoning](https://arxiv.org/abs/2601.08079)
*Hongjin Qian,Zhao Cao,Zheng Liu*

Main category: cs.AI

TL;DR: MemoBrain是一个用于工具增强型智能体的执行记忆模型，通过构建依赖感知的记忆来管理长时推理过程中的中间状态和逻辑关系，解决上下文累积问题。


<details>
  <summary>Details</summary>
Motivation: 工具增强型智能体框架中的复杂推理本质上是长时程的，导致推理轨迹和临时工具产物不断积累，超出大型语言模型的有限工作上下文。缺乏显式记忆机制会破坏逻辑连续性并削弱任务对齐，因此记忆成为维持长时程连贯、目标导向推理的核心组件。

Method: 提出MemoBrain执行记忆模型，作为推理智能体的协同伙伴运行。它构建依赖感知的记忆来捕获关键中间状态及其逻辑关系，在不阻塞执行的情况下组织推理进度，并主动管理工作上下文。具体机制包括：修剪无效步骤、折叠完成的子轨迹、在固定上下文预算下保持紧凑的高显著性推理主干。

Result: 在具有挑战性的长时程基准测试（包括GAIA、WebWalker和BrowseComp-Plus）上评估MemoBrain，相比强基线模型展示了一致的性能改进。

Conclusion: MemoBrain通过显式认知控制机制管理推理轨迹，而非被动积累上下文，为工具增强型智能体提供了维持长时程连贯推理的有效记忆解决方案。

Abstract: Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.
  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.
  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.

</details>


### [36] [Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement](https://arxiv.org/abs/2601.08545)
*Zhenlong Dai,Zhuoluo Zhao,Hengning Wang,Xiu Tang,Sai Wu,Chang Yao,Zhipeng Gao,Jingyuan Chen*

Main category: cs.AI

TL;DR: 提出LPR任务和LSG框架，通过检索增强和迭代优化提升编程辅导中的程序修复效果，并提供错误描述


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注修复编程学习者的错误代码，但缺乏对错误根本原因的解释，需要更全面的编程辅导系统

Method: 两阶段框架：1) 修复方案检索构建数据库，使用编辑驱动检索指导LLM识别和修复错误；2) 解决方案引导的程序修复方法，在检索方案指导下修复代码并提供解释；采用迭代检索增强方法优化检索方向

Result: 实验结果显示该方法大幅超越基线模型，验证了LPR任务框架的有效性

Conclusion: 提出的LSG框架能够有效提升程序修复性能并提供错误描述，为编程辅导系统提供了新解决方案

Abstract: With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \textbf{LPR} (\textbf{L}earner-Tailored \textbf{P}rogram \textbf{R}epair). We then propose a novel and effective framework, \textbf{\textsc{\MethodName{}}} (\textbf{L}earner-Tailored \textbf{S}olution \textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.

</details>


### [37] [Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions](https://arxiv.org/abs/2601.08156)
*Arin Gopalan Yadav,Varad Dherange,Kumar Shivam*

Main category: cs.AI

TL;DR: Project Synapse是一个用于自主解决最后一公里配送中断的新型智能体框架，采用分层多智能体架构，使用LangGraph编排复杂工作流，并在真实场景数据集上通过LLM-as-a-Judge协议进行评估。


<details>
  <summary>Details</summary>
Motivation: 解决最后一公里配送中的复杂中断问题，这些中断通常需要多步骤决策和协调，传统自动化系统难以处理。

Method: 采用分层多智能体架构：中央Resolution Supervisor负责战略任务分解，将子任务委托给专门的worker智能体执行战术操作。使用LangGraph编排复杂循环工作流。从6000多个真实用户评论中提取30个复杂中断场景构建基准数据集。

Result: 通过LLM-as-a-Judge协议评估系统性能，并采用明确的偏见缓解措施。论文展示了框架在复杂配送中断场景中的有效性。

Conclusion: Project Synapse为自主解决最后一公里配送中断提供了一个有效的多智能体框架，能够处理复杂的决策和协调任务。

Abstract: This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery disruptions. Synapse employs a hierarchical multi-agent architecture in which a central Resolution Supervisor agent performs strategic task decomposition and delegates subtasks to specialized worker agents responsible for tactical execution. The system is orchestrated using LangGraph to manage complex and cyclical workflows. To validate the framework, a benchmark dataset of 30 complex disruption scenarios was curated from a qualitative analysis of over 6,000 real-world user reviews. System performance is evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.

</details>


### [38] [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)
*Mohammad Pivezhandi,Mahdi Banisharif,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.AI

TL;DR: 提出基于模型的分层多智能体强化学习框架，用于多核平台的温度和能量感知调度，结合LLM语义特征提取实现零样本部署，相比传统方法显著提升能效和调度性能。


<details>
  <summary>Details</summary>
Motivation: 现有DVFS和任务分配方法存在局限性：基于利用率的启发式方法忽略停滞时间，基于离线分析的方法无法适应运行时变化。需要一种能够动态适应、无需特定工作负载分析的高效调度方案。

Method: 1) 分层多智能体强化学习框架分解指数级动作空间；2) 结合回归技术的精确环境模型预测热动态和性能状态；3) LLM提取13个代码级语义特征，无需执行程序；4) Dyna-Q启发的框架整合直接强化学习和基于模型的规划。

Result: 1) 后续决策延迟358ms，首次决策3.5-8.0s（含LLM特征提取）；2) 比Linux ondemand governor提升7.09倍能效和4.0倍makespan；3) 首次决策比基于表格的分析快8300倍；4) 收敛速度比无模型方法快20倍。

Conclusion: 该框架通过模型预测和LLM语义特征实现了零样本部署能力，显著提升了多核嵌入式系统的调度效率和适应性，为动态嵌入式系统的实际部署提供了可行方案。

Abstract: Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.

</details>


### [39] [The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios](https://arxiv.org/abs/2601.08173)
*Daocheng Fu,Jianbiao Mei,Rong Wu,Xuemeng Yang,Jia Xu,Ding Wang,Pinlong Cai,Yong Liu,Licheng Wen,Botian Shi*

Main category: cs.AI

TL;DR: 该论文提出了EvoEnv动态评估环境，用于评估多模态大语言模型代理在动态、不确定环境中的鲁棒性，重点关注任务调度、主动探索和持续学习三个维度。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注静态环境中的性能上限，忽视了随机现实世界部署中的鲁棒性。作者识别了动态任务调度、不确定性下的主动探索以及从经验中持续学习三个关键挑战。

Method: 提出了EvoEnv动态评估环境，模拟"受训"代理在新型设置中持续探索。评估三个维度：1) 针对不同优先级流式任务的上下文感知调度；2) 通过主动探索减少幻觉的谨慎信息获取；3) 从基于规则的动态生成任务中提炼通用策略的持续演化。

Result: 实验表明，最先进的代理在动态环境中存在显著缺陷，特别是在主动探索和持续学习方面。该工作建立了评估代理可靠性的框架。

Conclusion: 该研究将评估从静态测试转向现实、面向生产的场景，为评估代理在动态环境中的鲁棒性提供了框架。

Abstract: The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \method{}, a dynamic evaluation environment that simulates a "trainee" agent continuously exploring a novel setting. Unlike traditional benchmarks, \method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv

</details>


### [40] [MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents](https://arxiv.org/abs/2601.08235)
*Shouju Wang,Haopeng Zhang*

Main category: cs.AI

TL;DR: MPCI-Bench是首个用于评估智能体隐私行为的多模态配对上下文完整性基准，包含三个层级的数据，揭示当前模型在平衡隐私与效用方面的系统性失败。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型从被动聊天机器人演变为处理个人数据的主动助手，评估其遵守社会规范变得至关重要。现有上下文完整性基准主要是文本中心且侧重于负面拒绝场景，忽略了多模态隐私风险和隐私与效用的基本权衡。

Method: 提出MPCI-Bench基准，包含从同一视觉源派生的配对正负实例，分为三个层级：规范性种子判断、上下文丰富的故事推理和可执行的智能体行动轨迹。通过三原则迭代精炼流程确保数据质量。

Result: 对最先进多模态模型的评估显示：1）模型在平衡隐私与效用方面存在系统性失败；2）存在明显的模态泄漏差距，敏感视觉信息比文本信息更频繁地被泄露。

Conclusion: MPCI-Bench是首个用于评估智能体隐私行为的多模态配对上下文完整性基准，将开源以促进未来关于智能体上下文完整性的研究。

Abstract: As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.

</details>


### [41] [The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination](https://arxiv.org/abs/2601.08237)
*Haoran Su,Yandong Sun,Congjia Yu*

Main category: cs.AI

TL;DR: 该论文主张在多智能体强化学习中，从传统的手工设计数值奖励函数转向基于语言的目标规范，利用大语言模型实现语义奖励规范和动态奖励适应，以改善与人类意图的对齐。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中的奖励工程面临信用分配模糊、环境非平稳性和交互复杂性组合增长等挑战，传统手工设计数值奖励函数的方法难以应对这些挑战。

Method: 提出利用大语言模型从自然语言描述合成奖励函数（如EUREKA），并在线适应奖励公式（如CARD），结合可验证奖励强化学习范式，实现语义奖励规范、动态奖励适应和人类意图对齐。

Result: 语言介导的监督可以作为传统奖励工程的可行替代方案，协调可以从共享语义表示而非显式设计的数值信号中产生。

Conclusion: 需要研究从手工设计的数值奖励转向基于语言的目标规范，同时解决计算开销、幻觉鲁棒性和大规模多智能体系统可扩展性等开放挑战。

Abstract: Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.

</details>


### [42] [Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks](https://arxiv.org/abs/2601.08254)
*Abdikarim Mohamed Ibrahim,Rosdiadee Nordin*

Main category: cs.AI

TL;DR: 提出一种由大型语言模型引导的深度强化学习智能体，用于非地面网络优化，相比传统DRL在吞吐量、公平性和中断概率方面有显著提升


<details>
  <summary>Details</summary>
Motivation: 大型AI模型在非地面网络应用中表现出色，具有更好的泛化能力和更少的任务特定训练需求。传统DRL方法在复杂网络环境中存在性能瓶颈，需要更智能的引导机制。

Method: 设计了一个由LLM引导的DRL智能体，LLM作为高层协调器生成文本指导，在训练过程中塑造DRL智能体的奖励函数，形成LAM-DRL框架。

Result: LAM-DRL在正常天气场景下比传统DRL提升40%性能，在极端天气场景下提升64%性能（相比启发式方法），在吞吐量、公平性和中断概率方面均有显著改善。

Conclusion: LLM引导的DRL方法在非地面网络优化中表现出色，证明了大型语言模型作为高层协调器能够有效提升强化学习智能体的性能。

Abstract: Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.

</details>


### [43] [T3: Benchmarking Sycophancy and Skepticism in Causal Judgment](https://arxiv.org/abs/2601.08258)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: T3是一个诊断基准，用于评估LLM在Pearl因果阶梯上的因果判断能力，包含454个专家策划的小故事，通过效用、安全和明智拒绝三个维度分析模型表现，揭示了安全调优模型的"怀疑陷阱"和大型模型的"缩放悖论"。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够系统评估LLM因果推理能力的基准，特别是需要能够诊断模型在因果阶梯不同层级上的具体失败模式，以理解安全调优和模型缩放对因果判断的影响。

Method: 开发了包含454个专家策划小故事的T3基准，将性能分解为效用（敏感性）、安全（特异性）和明智拒绝三个维度，评估模型在Pearl因果阶梯三个层级上的表现，并应用该基准诊断前沿模型的病理现象。

Result: 发现两种病理现象：1）L1层的"怀疑陷阱"：安全调优模型（如Claude Haiku）拒绝60%的有效因果链接；2）L3层的非单调"缩放悖论"：更大的GPT-5.2在模糊反事实推理上比GPT-4-Turbo差55分，主要原因是过度犹豫而非幻觉。通过基准验证了过程验证协议（RCA）能够恢复决定性因果判断。

Conclusion: T3基准能够有效诊断LLM在因果推理中的系统性失败模式，揭示了安全调优和模型缩放对因果判断的复杂影响，为改进模型因果推理能力提供了有价值的诊断工具。

Abstract: We introduce T3 (Testing Trustworthy Thinking), a diagnostic benchmark designed to rigorously evaluate LLM causal judgment across Pearl's Ladder of Causality. Comprising 454 expert-curated vignettes, T3 prioritizes high-resolution failure analysis, decomposing performance into Utility (sensitivity), Safety (specificity), and Wise Refusal on underdetermined cases. By applying T3 to frontier models, we diagnose two distinct pathologies: a "Skepticism Trap" at L1 (where safety-tuned models like Claude Haiku reject 60% of valid links) and a non-monotonic Scaling Paradox at L3. In the latter, the larger GPT-5.2 underperforms GPT-4-Turbo by 55 points on ambiguous counterfactuals, driven by a collapse into paralysis (excessive hedging) rather than hallucination. Finally, we use the benchmark to validate a process-verified protocol (RCA), showing that T3 successfully captures the restoration of decisive causal judgment under structured verification.

</details>


### [44] [Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces](https://arxiv.org/abs/2601.08271)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 论文提出稀疏代理控制(SAC)框架，分析工具增强LLM系统中的顺序决策问题，证明在M个工具中只有k个相关时，样本复杂度为O(k log M)，而非O(M)，解释了纯提示控制的不稳定性。


<details>
  <summary>Details</summary>
Motivation: 工具增强LLM系统面临大规模离散动作空间（工具、API、文档）的决策问题，其中只有少量未知子集与特定任务相关。现有学习理论主要忽略这种控制机制，需要新的理论框架来分析此类系统的样本复杂度和稳定性。

Method: 将问题形式化为稀疏代理控制(SAC)，使用ℓ₁,₂正则化策略学习，通过凸代理方法建立压缩感知风格的理论结果。分析包括策略RSC条件、原始-对偶见证参数、部分可观测性下的表示误差等。

Result: 证明：1) 估计和值次优性按k(log M/T)^{1/2}缩放；2) 在T > k log M时通过原始-对偶见证实现精确工具支持恢复；3) 任何密集策略类都需要Ω(M)样本，解释了纯提示控制器的不稳定性；4) 部分可观测性下LLM仅通过信念误差ε_b影响性能。

Conclusion: SAC框架为工具增强LLM系统提供了理论基础，解释了为什么稀疏正则化能实现对数级样本复杂度，而密集方法需要线性样本量。这为设计稳定高效的代理系统提供了理论指导。

Abstract: Tool-augmented LLM systems expose a control regime that learning theory has largely ignored: sequential decision-making with a massive discrete action universe (tools, APIs, documents) in which only a small, unknown subset is relevant for any fixed task distribution. We formalize this setting as Sparse Agentic Control (SAC), where policies admit block-sparse representations over M >> 1 actions and rewards depend on sparse main effects and (optionally) sparse synergies. We study ell_{1,2}-regularized policy learning through a convex surrogate and establish sharp, compressed-sensing-style results: (i) estimation and value suboptimality scale as k (log M / T)^{1/2} under a Policy-RSC condition; (ii) exact tool-support recovery holds via primal-dual witness arguments when T > k log M under incoherence and beta-min; and (iii) any dense policy class requires Omega(M) samples, explaining the instability of prompt-only controllers. We further show that under partial observability, LLMs matter only through a belief/representation error epsilon_b, yielding an additive O(epsilon_b) degradation while preserving logarithmic dependence on M. Extensions cover tuning-free, online, robust, group-sparse, and interaction-aware SAC.

</details>


### [45] [ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web](https://arxiv.org/abs/2601.08276)
*Zhiyuan Yao,Zishan Xu,Yifu Guo,Zhiguang Han,Cheng Yang,Shuo Zhang,Weinan Zhang,Xingshan Zeng,Weiwen Liu*

Main category: cs.AI

TL;DR: ToolACE-MCP：一个训练历史感知路由器的管道，用于在大规模工具生态系统中实现精确导航，通过依赖丰富的候选图合成多轮轨迹来训练具有动态上下文理解能力的路由器。


<details>
  <summary>Details</summary>
Motivation: 随着Agent Web和模型上下文协议（MCP）的发展，代理生态系统正在演变为开放协作网络，可访问工具呈指数级增长。然而，当前架构面临严重的可扩展性和通用性瓶颈，需要新的解决方案来应对大规模工具生态系统的挑战。

Method: 提出ToolACE-MCP管道，通过依赖丰富的候选图合成多轮轨迹，训练历史感知路由器。该方法创建即插即用的轻量路由代理，具备动态上下文理解能力，能够在大规模工具生态系统中进行精确导航。

Result: 在真实世界基准测试MCP-Universe和MCP-Mark上表现出优越性能。ToolACE-MCP不仅能够以最小适应度推广到多代理协作，而且在噪声环境下保持卓越的鲁棒性，并能有效扩展到大规模候选空间。

Conclusion: ToolACE-MCP为开放生态系统中的通用编排提供了强大的实证基础，展现了未来Agent Web所需的关键特性，包括可扩展性、鲁棒性和多代理协作能力。

Abstract: With the rise of the Agent Web and Model Context Protocol (MCP), the agent ecosystem is evolving into an open collaborative network, exponentially increasing accessible tools. However, current architectures face severe scalability and generality bottlenecks. To address this, we propose ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph to synthesize multi-turn trajectories, we effectively train routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Experiments on the real-world benchmarks MCP-Universe and MCP-Mark demonstrate superior performance. Notably, ToolACE-MCP exhibits critical properties for the future Agent Web: it not only generalizes to multi-agent collaboration with minimal adaptation but also maintains exceptional robustness against noise and scales effectively to massive candidate spaces. These findings provide a strong empirical foundation for universal orchestration in open-ended ecosystems.

</details>


### [46] [Greedy Is Enough: Sparse Action Discovery in Agentic LLMs](https://arxiv.org/abs/2601.08280)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 论文研究大规模动作空间中的智能体系统，提出基于结构化稀疏假设的动作发现算法，证明在稀疏性条件下可高效识别相关动作集。


<details>
  <summary>Details</summary>
Motivation: 现代智能体系统（如工具增强语言模型）面临极大动作空间（数千API），但经验表明只有小部分动作真正影响性能。这启发研究结构化稀疏假设下的动作发现问题。

Method: 采用上下文线性奖励模型，假设动作相关性服从结构化稀疏性。将动作发现建模为块稀疏恢复问题，分析受正交匹配追踪启发的贪心算法。在不相干性、信号强度和动作覆盖标准假设下，证明算法能准确恢复相关动作集。

Result: 证明贪心算法能以高概率精确恢复相关动作集，样本复杂度随稀疏度和潜在维度多项式增长，仅随总动作数对数增长。提供重拟合参数估计误差保证，显示所得决策规则对新潜在状态接近最优。建立信息论下界证明稀疏性和充分覆盖是易处理性的必要条件。

Conclusion: 稀疏动作发现是大规模动作决策的基本原理，为智能体系统中的动作剪枝提供了理论基础，表明在稀疏性条件下可显著降低动作空间复杂度。

Abstract: Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. Motivated by this observation, we study a contextual linear reward model in which action relevance is governed by a structured sparsity assumption: only a small number of actions have nonzero effects across latent states.
  We formulate action discovery as a block-sparse recovery problem and analyze a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions on incoherence, signal strength, and action coverage, we prove that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. We further provide estimation error guarantees for refitted parameters and show that the resulting decision rule is near-optimal for new latent states.
  Complementing these results, we establish information-theoretic lower bounds demonstrating that sparsity and sufficient coverage are necessary for tractability. Together, our results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.

</details>


### [47] [AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation](https://arxiv.org/abs/2601.08323)
*Yupeng Huo,Yaxi Lu,Zhong Zhang,Haotian Chen,Yankai Lin*

Main category: cs.AI

TL;DR: AtomMem将记忆管理重构为动态决策问题，通过将高级记忆过程分解为原子CRUD操作，结合监督微调和强化学习，学习自主、任务对齐的记忆管理策略，在长上下文基准测试中优于静态工作流方法。


<details>
  <summary>Details</summary>
Motivation: 现有代理记忆机制大多依赖静态、手工设计的工作流程，这限制了记忆设计的性能和泛化能力，需要更灵活、基于学习的记忆框架来解决现实世界中的长时程问题。

Method: 将记忆管理重构为动态决策问题，将高级记忆过程分解为基本的原子CRUD（创建、读取、更新、删除）操作，将记忆工作流转化为可学习的决策过程。通过结合监督微调和强化学习，学习自主、任务对齐的策略来编排针对特定任务需求的记忆行为。

Result: 在3个长上下文基准测试中，训练后的AtomMem-8B模型持续优于先前的静态工作流记忆方法。训练动态分析表明，基于学习的公式使代理能够发现结构化、任务对齐的记忆管理策略。

Conclusion: AtomMem通过将记忆管理重构为可学习的决策过程，实现了优于静态工作流方法的性能，展示了基于学习的方法在发现任务对齐记忆策略方面的关键优势。

Abstract: Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.

</details>


### [48] [Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant](https://arxiv.org/abs/2601.08333)
*Oleg Romanchuk,Roman Bondar*

Main category: cs.AI

TL;DR: 论文提出"语义洗钱"概念，指出LLM智能体架构将信息传输机制与认知证明机制混为一谈，导致缺乏充分依据的命题通过架构信任接口被接受，形成系统性的认知缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体架构存在系统性缺陷，将信息传输机制与认知证明机制混淆，导致命题在没有充分依据的情况下被系统接受。这种架构缺陷类似于盖梯尔问题，但并非偶然而是架构决定的系统性现象。

Method: 通过形式化分析提出"语义洗钱"概念，将其定义为一种架构故障模式。引入"必然自我授权定理"证明在标准架构假设下，循环认知证明无法消除。提出"依据侵蚀原理"作为根本解释。

Result: 证明了语义洗钱是盖梯尔问题的架构实现，命题获得高认知地位但缺乏真实性与证明之间的联系。这种效应是架构决定且可系统复现的，扩展、模型改进和LLM作为评判方案都无法消除类型层面的问题。

Conclusion: LLM智能体架构存在根本性认知缺陷，语义洗钱现象揭示了信息传输与认知证明的混淆问题。该问题存在于类型层面，无法通过现有技术方案解决，需要重新思考智能体架构设计。

Abstract: LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms. We formalize this class of architectural failures as semantic laundering: a pattern where propositions with absent or weak warrant are accepted by the system as admissible by crossing architecturally trusted interfaces. We show that semantic laundering constitutes an architectural realization of the Gettier problem: propositions acquire high epistemic status without a connection between their justification and what makes them true. Unlike classical Gettier cases, this effect is not accidental; it is architecturally determined and systematically reproducible. The central result is the Theorem of Inevitable Self-Licensing: under standard architectural assumptions, circular epistemic justification cannot be eliminated. We introduce the Warrant Erosion Principle as the fundamental explanation for this effect and show that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating a problem that exists at the type level.

</details>


### [49] [WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents](https://arxiv.org/abs/2601.08406)
*Xinyi Wu,Jiagui Chen,Geng Hong,Jiayi Dong,Xudong Pan,Jiarun Dai,Min Yang*

Main category: cs.AI

TL;DR: WebTrap Park是一个自动化平台，用于通过直接观察Web代理与实时网页的交互来系统评估其安全性，包含1,226个可执行评估任务，无需修改代理即可进行基于动作的评估。


<details>
  <summary>Details</summary>
Motivation: 当前Web代理在真实网络环境中执行复杂任务时，其安全性评估仍然分散且难以标准化，缺乏系统化的评估平台。

Method: 开发WebTrap Park平台，将三种主要安全风险来源实例化为1,226个可执行评估任务，通过直接观察代理与实时网页的具体交互进行自动化安全评估。

Result: 结果显示不同代理框架之间存在明显的安全差异，强调了代理架构的重要性（而不仅仅是底层模型）。平台公开可用，为可复现的Web代理安全评估提供了可扩展基础。

Conclusion: WebTrap Park提供了一个系统化、可扩展的平台，用于评估Web代理的安全性，揭示了代理架构对安全性的重要影响，有助于推动Web代理安全研究的标准化。

Abstract: Web Agents are increasingly deployed to perform complex tasks in real web environments, yet their security evaluation remains fragmented and difficult to standardize. We present WebTrap Park, an automated platform for systematic security evaluation of Web Agents through direct observation of their concrete interactions with live web pages. WebTrap Park instantiates three major sources of security risk into 1,226 executable evaluation tasks and enables action based assessment without requiring agent modification. Our results reveal clear security differences across agent frameworks, highlighting the importance of agent architecture beyond the underlying model. WebTrap Park is publicly accessible at https://security.fudan.edu.cn/webagent and provides a scalable foundation for reproducible Web Agent security evaluation.

</details>


### [50] [Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation](https://arxiv.org/abs/2601.08412)
*Yizhan Feng,Hichem Snoussi,Yuhang Wang,Jing Teng,Abel Cherouat,Tian Wang*

Main category: cs.AI

TL;DR: 提出结合知识蒸馏、思维链引导和监督微调的方法，将大语言模型的代码生成能力迁移到轻量级模型，用于资源受限的无人机多SDK控制任务


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成任务中潜力显著，但应用于资源受限的无人机平台时存在矛盾：大模型资源消耗高，而无人机需要实时轻量级控制

Method: 1) 构建高质量数据集，包含指令-代码-推理链和反事实负样本；2) 使用DeepSeek-Coder-V2-Lite作为教师模型，采用混合黑盒白盒蒸馏策略生成思维链软标签，结合加权交叉熵损失；3) 针对无人机控制场景进行提示调优工程

Result: 蒸馏后的轻量级模型在保持高代码生成准确率的同时，在部署和推理效率上显著提升，有效实现了无人机精确轻量级智能控制

Conclusion: 该方法在实现无人机精确轻量级智能控制方面具有可行性和优越性，解决了大模型与资源受限平台之间的矛盾

Abstract: With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs

</details>


### [51] [RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation](https://arxiv.org/abs/2601.08430)
*Sunzhu Li,Jiale Zhao,Miteto Wei,Huimin Ren,Yang Zhou,Jingwen Yang,Shunyu Liu,Kaike Zhang,Wei Chen*

Main category: cs.AI

TL;DR: 提出RubricHub框架，通过粗到细的评分标准生成方法创建大规模多领域数据集，结合拒绝采样微调和强化学习，显著提升模型在开放生成任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在开放生成任务中面临挑战，因为缺乏真实标签。基于评分标准的评估方法存在可扩展性瓶颈和标准粗糙的问题，导致监督天花板效应。

Method: 提出自动化粗到细评分标准生成框架，结合原则引导合成、多模型聚合和难度演化。基于此构建RubricHub数据集（约11万样本），采用两阶段后训练：基于评分标准的拒绝采样微调(RuFT)和强化学习(RuRL)。

Result: Qwen3-14B模型在HealthBench上达到69.3分，超越GPT-5等前沿专有模型，取得SOTA结果。

Conclusion: RubricHub框架通过生成细粒度评分标准解决了开放生成任务的监督瓶颈，显著提升模型性能，为可验证奖励的强化学习提供了有效解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.

</details>


### [52] [YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation](https://arxiv.org/abs/2601.08441)
*Abdelaziz Bounhar,Rania Hossam Elmohamady Elbadry,Hadi Abdine,Preslav Nakov,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.AI

TL;DR: YaPO提出了一种基于稀疏自编码器的稀疏转向向量学习方法，相比密集转向向量能更好地解耦潜在因素，实现更精细、稳定、高效的LLM对齐控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于BiPO的密集转向向量方法由于神经元多语义性，容易纠缠多个潜在因素，在需要精细区分（如中东文化对齐）的场景中效果有限且不稳定。

Method: YaPO采用无参考方法，在稀疏自编码器的潜在空间中学习稀疏转向向量，通过优化稀疏编码产生解耦、可解释、高效的转向方向。

Result: YaPO比密集转向基线收敛更快、性能更强、训练更稳定，能泛化到幻觉、财富追求、越狱、权力追求等多种对齐相关行为，且不损害MMLU通用知识。

Conclusion: YaPO为LLM提供了一种高效、稳定、精细的对齐方法，在可控性和领域适应方面具有广泛应用前景。

Abstract: Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \textit{reference-free} method that learns \textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\footnote{https://github.com/MBZUAI-Paris/YaPO}.

</details>


### [53] [M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games](https://arxiv.org/abs/2601.08462)
*Sixiong Xie,Zhuofan Shi,Haiyang Shen,Gang Huang,Yun Ma,Xiang Jing*

Main category: cs.AI

TL;DR: M3-Bench是一个用于评估LLM智能体社交行为的多阶段基准测试，通过行为轨迹、推理过程和通信内容三个模块的协同分析，结合大五人格模型和社会交换理论，提供超越简单任务分数的可解释社交行为画像。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常只关注单一能力维度或仅依赖行为结果，忽视了智能体决策推理和通信交互的丰富过程信息，无法全面评估LLM智能体的复杂社交行为（如合作、欺骗、共谋）。

Method: 提出M3-Bench多阶段混合动机博弈基准，采用过程感知评估框架，包含三个协同分析模块：行为轨迹分析(BTA)、推理过程分析(RPA)和通信内容分析(CCA)，并整合大五人格模型和社会交换理论，将多维证据聚合为可解释的社交行为画像。

Result: 实验结果表明，M3-Bench能够可靠地区分不同模型的多样化社交行为能力，并揭示了一些模型在看似合理的行为结果背后，其推理和通信过程存在显著不一致性。

Conclusion: M3-Bench提供了一个全面评估LLM智能体社交行为能力的框架，超越了传统基于结果的评估方法，能够揭示智能体决策过程中的内在不一致性，为更深入的智能体行为分析提供了工具。

Abstract: As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.

</details>


### [54] [Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.08641)
*Yichen Luo,Yebo Feng,Jiahua Xu,Yang Liu*

Main category: cs.AI

TL;DR: 论文提出一个可解释的多智能体系统用于模因币跟单交易，通过分解复杂任务并协调专业智能体，在模因币市场中实现了比传统机器学习模型和单一LLM更好的性能。


<details>
  <summary>Details</summary>
Motivation: 模因币跟单交易面临操纵机器人盛行、被跟随钱包未来表现不确定、交易执行延迟等问题，而单一LLM在处理复杂的资产分配任务时存在困难，特别是在加密货币领域缺乏足够的领域知识。

Method: 受资产管理团队结构启发，将复杂任务分解为子任务，协调专业智能体协作解决。采用少样本思维链提示，使每个智能体获取专业模因币交易知识，解释多模态数据并生成可解释决策。

Result: 在1000个模因币项目的交易数据集上，多智能体系统在识别高质量模因币项目和关键意见领袖钱包方面分别达到73%和70%的精确率，所选KOL在这些项目中总共产生了50万美元利润。

Conclusion: 提出的可解释多智能体系统能有效解决模因币跟单交易的挑战，通过任务分解和专业智能体协作，在复杂加密货币市场中实现了优于传统方法和单一LLM的性能。

Abstract: The launch of \$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.
  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \$500,000 across these projects.

</details>


### [55] [Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding](https://arxiv.org/abs/2601.08653)
*Zenghua Liao,Jinzhi Liao,Xiang Zhao*

Main category: cs.AI

TL;DR: Prism框架通过逻辑依赖建模和认知负载优化，提升LLM在社交平台上的复杂意图理解能力，实现更高效、连贯的用户交互。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在社交平台作为接口时，面对用户模糊动态的目标，现有方法无法有效建模澄清问题间的逻辑依赖关系，导致意图理解效果有限。

Method: 提出Prism框架，包含四个模块：复杂意图分解、逻辑澄清生成、意图感知奖励、自进化意图调优，通过依赖建模和蒙特卡洛采样优化交互质量。

Result: 在澄清交互、意图执行和认知负载基准测试中均优于现有方法，逻辑一致性达SOTA，逻辑冲突降至11.5%，用户满意度提升14.4%，任务完成时间减少34.8%。

Conclusion: Prism通过建模澄清问题的逻辑依赖关系，显著提升了LLM在复杂意图理解方面的能力，实现了更高效、连贯的用户-LLM协作。

Abstract: Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.

</details>


### [56] [Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance](https://arxiv.org/abs/2601.08676)
*Yilei Zhao,Wentao Zhang,Xiao Lei,Yandan Zheng,Mengpu Liu,Wei Yang Bryan Lim*

Main category: cs.AI

TL;DR: ESGAgent：基于分层多智能体系统的专业ESG分析框架，配备专用工具集，在ESG基准测试中表现优于闭源大模型


<details>
  <summary>Details</summary>
Motivation: 专业ESG分析面临数据分散于非结构化来源的挑战，现有大语言模型难以处理复杂的多步骤审计工作流，需要专门解决方案来提升ESG分析的质量和效率

Method: 提出ESGAgent分层多智能体系统，配备检索增强、网络搜索和领域特定函数等专用工具集；同时构建基于310份企业可持续发展报告的三级基准测试

Result: ESGAgent在原子问答任务上平均准确率达84.15%，优于最先进的闭源大模型；在专业报告生成方面表现出色，能整合丰富图表和可验证引用

Conclusion: ESGAgent系统有效解决了ESG分析的专业需求，提出的基准测试为评估高风险垂直领域的通用和高级智能体能力提供了重要测试平台

Abstract: Environmental, social, and governance (ESG) criteria are essential for evaluating corporate sustainability and ethical performance. However, professional ESG analysis is hindered by data fragmentation across unstructured sources, and existing large language models (LLMs) often struggle with the complex, multi-step workflows required for rigorous auditing. To address these limitations, we introduce ESGAgent, a hierarchical multi-agent system empowered by a specialized toolset, including retrieval augmentation, web search and domain-specific functions, to generate in-depth ESG analysis. Complementing this agentic system, we present a comprehensive three-level benchmark derived from 310 corporate sustainability reports, designed to evaluate capabilities ranging from atomic common-sense questions to the generation of integrated, in-depth analysis. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs with an average accuracy of 84.15% on atomic question-answering tasks, and excels in professional report generation by integrating rich charts and verifiable references. These findings confirm the diagnostic value of our benchmark, establishing it as a vital testbed for assessing general and advanced agentic capabilities in high-stakes vertical domains.

</details>


### [57] [Learning from Demonstrations via Capability-Aware Goal Sampling](https://arxiv.org/abs/2601.08731)
*Yuanlin Duan,Yuning Wang,Wenjie Qiu,He Zhu*

Main category: cs.AI

TL;DR: Cago是一种新颖的模仿学习方法，通过动态跟踪智能体在专家轨迹上的能力，选择刚好超出当前能力范围的中间目标来引导学习，从而缓解对专家轨迹的脆弱依赖。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在长时域环境中经常失败，因为完美复制演示不现实，小错误会灾难性累积。现有方法要么仅用演示初始化策略，要么用于奖励塑形，但都过度依赖专家轨迹。

Method: Cago（能力感知目标采样）方法：1）动态跟踪智能体在专家轨迹上的能力；2）选择刚好超出智能体当前能力范围的中间步骤（目标）；3）形成自适应课程，引导学习逐步解决完整任务。

Result: Cago在稀疏奖励、目标条件任务中显著提高了样本效率和最终性能，一致优于现有的模仿学习基线方法。

Conclusion: Cago通过能力感知的目标采样机制，创建自适应课程，有效缓解了模仿学习对专家轨迹的脆弱依赖，实现了更稳健的长时域任务学习。

Abstract: Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.

</details>


### [58] [Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards](https://arxiv.org/abs/2601.08778)
*Tengjun Jin,Yoojin Choi,Yuxuan Zhu,Daniel Kang*

Main category: cs.AI

TL;DR: 该论文实证研究发现两个主流text-to-SQL基准测试(BIRD和Spider 2.0-Snow)存在高标注错误率(52.8%和62.8%)，这些错误显著影响模型性能评估和排行榜排名，可能误导研究方向选择。


<details>
  <summary>Details</summary>
Motivation: 文本到SQL技术依赖人工标注的基准测试进行评估，但标注错误的普遍性及其对模型性能评估和排行榜排名的影响尚未得到充分研究。研究者需要了解这些基准测试的可靠性，以确保评估结果的准确性。

Method: 通过专家分析量化两个主流text-to-SQL基准测试的标注错误率；修正BIRD开发集子集；在原始和修正后的数据集上重新评估16个开源代理；分析性能变化和排名变化；使用相关性分析评估结果在完整开发集上的泛化性。

Result: BIRD Mini-Dev和Spider 2.0-Snow的错误率分别为52.8%和62.8%；在修正后的BIRD子集上，代理性能相对变化范围为-7%到31%，排名变化范围为-9到+9位；原始子集与完整开发集的排名强相关(r_s=0.85)，而与修正子集的相关性弱(r_s=0.32)。

Conclusion: 基准测试中的标注错误会显著扭曲报告的性能和排名结果，可能误导研究方向和部署决策。研究强调了提高基准测试标注质量的重要性，并提供了修正后的数据集供社区使用。

Abstract: Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.
  In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.

</details>


### [59] [Uncovering Political Bias in Large Language Models using Parliamentary Voting Records](https://arxiv.org/abs/2601.08785)
*Jieying Chen,Karen de Jong,Andreas Poole,Jan Burakowski,Elena Elderson Nosti,Joep Windt,Chendi Wang*

Main category: cs.AI

TL;DR: 本文提出了一种通过将模型生成的投票预测与真实议会投票记录对齐来构建政治偏见基准的通用方法，并在荷兰、挪威和西班牙三个国家案例中实例化，发现最先进的LLM普遍表现出左倾或中间倾向，并对右翼保守政党存在明显负面偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在数字平台和决策系统中深度应用，对其政治偏见的担忧日益增长。尽管已有大量关于性别和种族等社会偏见的研究，但对政治偏见的系统性研究仍然有限，尽管其对社会有直接影响。

Method: 提出构建政治偏见基准的通用方法：将模型生成的投票预测与经过验证的议会投票记录对齐。在三个国家案例中实例化：PoliBiasNL（荷兰）、PoliBiasNO（挪威）和PoliBiasES（西班牙）。提出一种可视化方法，将LLM和政党的意识形态映射到二维CHES空间，实现模型与真实政治行为者的直接可解释比较。

Result: 实验揭示了细粒度的意识形态区别：最先进的LLM一致表现出左倾或中间倾向，同时对右翼保守政党存在明显的负面偏见。

Conclusion: 这些发现强调了基于真实议会行为的透明、跨国评估对于理解和审计现代LLM政治偏见的价值。

Abstract: As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [60] [STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order](https://arxiv.org/abs/2601.08107)
*Chengyang Gu,Yuxin Pan,Hui Xiong,Yize Chen*

Main category: cs.LG

TL;DR: STO-RL：利用LLM生成时序子目标序列的离线强化学习框架，通过基于势能的奖励塑形解决稀疏奖励长时程任务


<details>
  <summary>Details</summary>
Motivation: 传统离线RL在稀疏奖励的长时程任务中表现不佳，现有目标条件和分层方法虽然分解任务并生成中间奖励，但忽略了子目标间的时序依赖关系，且依赖不精确的奖励塑形，导致策略次优。

Method: 利用大语言模型生成时序有序的子目标序列和状态到子目标阶段的映射，基于此时序结构应用基于势能的奖励塑形，将稀疏的终端奖励转化为密集、时序一致的信号，从而增强数据集并训练高性能策略。

Result: 在四个离散和连续稀疏奖励基准测试中，STO-RL始终优于最先进的离线目标条件和分层RL基线，实现更快的收敛速度、更高的成功率和更短的轨迹。消融研究证实其对不完美或噪声LLM生成的子目标序列具有鲁棒性。

Conclusion: LLM引导的子目标时序结构与理论基础的奖励塑形相结合，为长时程离线RL提供了实用且可扩展的解决方案。

Abstract: Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions, but it often struggles with long-horizon tasks involving sparse rewards. Existing goal-conditioned and hierarchical offline RL methods decompose such tasks and generate intermediate rewards to mitigate limitations of traditional offline RL, but usually overlook temporal dependencies among subgoals and rely on imprecise reward shaping, leading to suboptimal policies. To address these issues, we propose STO-RL (Offline RL using LLM-Guided Subgoal Temporal Order), an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings. Using this temporal structure, STO-RL applies potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions. The resulting augmented dataset with shaped rewards enables efficient offline training of high-performing policies. Evaluations on four discrete and continuous sparse-reward benchmarks demonstrate that STO-RL consistently outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving faster convergence, higher success rates, and shorter trajectories. Ablation studies further confirm STO-RL's robustness to imperfect or noisy LLM-generated subgoal sequences, demonstrating that LLM-guided subgoal temporal structures combined with theoretically grounded reward shaping provide a practical and scalable solution for long-horizon offline RL.

</details>


### [61] [Reverse Flow Matching: A Unified Framework for Online Reinforcement Learning with Diffusion and Flow Policies](https://arxiv.org/abs/2601.08136)
*Zeyang Li,Sunbochen Tang,Navid Azizan*

Main category: cs.LG

TL;DR: 提出统一框架RFM，通过反向推理视角和Langevin Stein算子，为无直接目标样本的扩散和流策略训练提供最小方差估计器，提升在线RL效率


<details>
  <summary>Details</summary>
Motivation: 扩散和流策略在在线强化学习中表达能力强但训练效率低，现有噪声期望和梯度期望方法缺乏统一理论框架，需要解决无直接目标样本的挑战

Method: 提出反向流匹配(RFM)框架，采用反向推理视角，将训练目标构建为后验均值估计问题，引入Langevin Stein算子构造零均值控制变量，推导出最小方差估计器

Result: RFM统一了噪声期望和梯度期望方法，将Boltzmann分布目标从扩散策略扩展到流策略，结合Q值和Q梯度信息实现最优估计，在连续控制基准上优于扩散策略基线

Conclusion: RFM为无直接目标样本的扩散和流策略训练提供了统一理论框架，通过最小方差估计器提升训练效率和稳定性，推动了在线强化学习方法的发展

Abstract: Diffusion and flow policies are gaining prominence in online reinforcement learning (RL) due to their expressive power, yet training them efficiently remains a critical challenge. A fundamental difficulty in online RL is the lack of direct samples from the target distribution; instead, the target is an unnormalized Boltzmann distribution defined by the Q-function. To address this, two seemingly distinct families of methods have been proposed for diffusion policies: a noise-expectation family, which utilizes a weighted average of noise as the training target, and a gradient-expectation family, which employs a weighted average of Q-function gradients. Yet, it remains unclear how these objectives relate formally or if they can be synthesized into a more general formulation. In this paper, we propose a unified framework, reverse flow matching (RFM), which rigorously addresses the problem of training diffusion and flow models without direct target samples. By adopting a reverse inferential perspective, we formulate the training target as a posterior mean estimation problem given an intermediate noisy sample. Crucially, we introduce Langevin Stein operators to construct zero-mean control variates, deriving a general class of estimators that effectively reduce importance sampling variance. We show that existing noise-expectation and gradient-expectation methods are two specific instances within this broader class. This unified view yields two key advancements: it extends the capability of targeting Boltzmann distributions from diffusion to flow policies, and enables the principled combination of Q-value and Q-gradient information to derive an optimal, minimum-variance estimator, thereby improving training efficiency and stability. We instantiate RFM to train a flow policy in online RL, and demonstrate improved performance on continuous-control benchmarks compared to diffusion policy baselines.

</details>


### [62] [Scalable Multiagent Reinforcement Learning with Collective Influence Estimation](https://arxiv.org/abs/2601.08210)
*Zhenglong Luo,Zhiyong Chen,Aoxiang Liu,Ke Pan*

Main category: cs.LG

TL;DR: 提出基于集体影响力估计网络(CIEN)的多智能体强化学习框架，通过建模其他智能体对任务对象的集体影响力，实现仅依赖局部观测和任务对象状态的高效协作，无需显式动作信息交换，具有良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法需要频繁交换动作或状态信息以实现有效协调，这在实际机器人系统中难以满足。现有估计器网络设计导致网络规模和计算成本随智能体数量快速增长，限制了大规模系统的可扩展性。

Method: 提出集体影响力估计网络(CIEN)框架，通过显式建模其他智能体对任务对象的集体影响力，使每个智能体仅从局部观测和任务对象状态推断关键交互信息。该方法避免网络随团队规模扩展，新智能体加入无需修改现有网络结构。

Result: 基于SAC算法的多智能体协作任务实验表明，该方法在通信受限环境下实现稳定高效协调。在实际机器人平台部署显示，基于集体影响力建模的策略显著提高了鲁棒性和部署可行性，减少了对通信基础设施的依赖。

Conclusion: CIEN框架通过建模集体影响力而非个体动作，实现了通信受限环境下高效可扩展的多智能体协作，为实际机器人系统部署提供了可行解决方案。

Abstract: Multiagent reinforcement learning (MARL) has attracted considerable attention due to its potential in addressing complex cooperative tasks. However, existing MARL approaches often rely on frequent exchanges of action or state information among agents to achieve effective coordination, which is difficult to satisfy in practical robotic systems. A common solution is to introduce estimator networks to model the behaviors of other agents and predict their actions; nevertheless, such designs cause the size and computational cost of the estimator networks to grow rapidly with the number of agents, thereby limiting scalability in large-scale systems.
  To address these challenges, this paper proposes a multiagent learning framework augmented with a Collective Influence Estimation Network (CIEN). By explicitly modeling the collective influence of other agents on the task object, each agent can infer critical interaction information solely from its local observations and the task object's states, enabling efficient collaboration without explicit action information exchange. The proposed framework effectively avoids network expansion as the team size increases; moreover, new agents can be incorporated without modifying the network structures of existing agents, demonstrating strong scalability. Experimental results on multiagent cooperative tasks based on the Soft Actor-Critic (SAC) algorithm show that the proposed method achieves stable and efficient coordination under communication-limited environments. Furthermore, policies trained with collective influence modeling are deployed on a real robotic platform, where experimental results indicate significantly improved robustness and deployment feasibility, along with reduced dependence on communication infrastructure.

</details>


### [63] [A Preliminary Agentic Framework for Matrix Deflation](https://arxiv.org/abs/2601.08219)
*Paimon Goulart,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 提出一种基于智能体协作的矩阵降秩方法，使用LLM生成秩-1 SVD更新，VLM评估更新并决定停止时机，无需固定阈值


<details>
  <summary>Details</summary>
Motivation: 探索智能体能否协作完成矩阵降秩任务，替代传统数值算法，实现无需固定阈值的自适应降秩

Method: 采用双智能体架构：LLM作为求解器生成秩-1 SVD更新，VLM作为评估器接受/拒绝更新并决定停止时机；通过上下文学习和行列置换提高稳定性

Result: 在Digits、CIFAR-10和合成矩阵上表现良好，合成噪声情况下与数值降秩仅差1.75 RMSE，在Digits和CIFAR-10上达到原始Frobenius范数10%的目标

Conclusion: 完全智能体化、无阈值的降秩方法是可行的数值算法替代方案

Abstract: Can a small team of agents peel a matrix apart, one rank-1 slice at a time? We propose an agentic approach to matrix deflation in which a solver Large Language Model (LLM) generates rank-1 Singular Value Decomposition (SVD) updates and a Vision Language Model (VLM) accepts or rejects each update and decides when to stop, eliminating fixed norm thresholds. Solver stability is improved through in-context learning (ICL) and types of row/column permutations that expose visually coherent structure. We evaluate on Digits ($8{\times}8$), CIFAR-10 ($32{\times}32$ grayscale), and synthetic ($16{\times}16$) matrices with and without Gaussian noise. In the synthetic noisy case, where the true construction rank $k$ is known, numerical deflation provides the noise target and our best agentic configuration differs by only $1.75$ RMSE of the target. For Digits and CIFAR-10, targets are defined by deflating until the Frobenius norm reaches $10\%$ of the original. Across all settings, our agent achieves competitive results, suggesting that fully agentic, threshold-free deflation is a viable alternative to classical numerical algorithms.

</details>


### [64] [Incorporating Cognitive Biases into Reinforcement Learning for Financial Decision-Making](https://arxiv.org/abs/2601.08247)
*Liu He*

Main category: cs.LG

TL;DR: 该研究将认知偏差整合到金融交易的强化学习框架中，探索人类心理因素对AI交易决策的影响，尽管结果不明确或负面，但为开发稳健的金融AI系统提供了宝贵经验。


<details>
  <summary>Details</summary>
Motivation: 金融市场受到人类非理性行为的影响，传统强化学习模型假设理性代理人，可能忽视了心理因素的影响。本研究旨在探索将认知偏差整合到RL框架中是否能产生更符合人类行为的交易策略。

Method: 将过度自信、损失厌恶等认知偏差整合到强化学习的奖励结构和决策过程中，在模拟和真实交易环境中评估这些带有偏差的RL代理人的表现。

Result: 研究结果不明确或负面，表明将人类认知偏差整合到RL框架中面临挑战，但为理解偏差对AI交易系统的影响提供了重要见解。

Conclusion: 尽管结果不理想，但该研究为开发更稳健的金融AI系统提供了宝贵经验，强调了在AI交易模型中考虑人类心理因素的复杂性。

Abstract: Financial markets are influenced by human behavior that deviates from rationality due to cognitive biases. Traditional reinforcement learning (RL) models for financial decision-making assume rational agents, potentially overlooking the impact of psychological factors. This study integrates cognitive biases into RL frameworks for financial trading, hypothesizing that such models can exhibit human-like trading behavior and achieve better risk-adjusted returns than standard RL agents. We introduce biases, such as overconfidence and loss aversion, into reward structures and decision-making processes and evaluate their performance in simulated and real-world trading environments. Despite its inconclusive or negative results, this study provides insights into the challenges of incorporating human-like biases into RL, offering valuable lessons for developing robust financial AI systems.

</details>


### [65] [Provably Safe Reinforcement Learning using Entropy Regularizer](https://arxiv.org/abs/2601.08646)
*Abhijit Mazumdar,Rafal Wisniewski,Manuela L. Bujorianu*

Main category: cs.LG

TL;DR: 提出两种基于乐观面对不确定性(OFU)的安全强化学习算法，其中第二种算法引入熵正则化来改善遗憾界限并控制episode间变异性


<details>
  <summary>Details</summary>
Motivation: 解决马尔可夫决策过程中带有安全约束的最优策略学习问题，在reach-avoid框架下设计在线强化学习算法，确保学习阶段以任意高概率满足安全约束

Method: 首先提出基于OFU原则的算法，然后在此基础上提出主要算法，引入熵正则化技术，并对两种算法进行有限样本分析

Result: 推导了两种算法的遗憾界限，证明熵正则化的引入改善了遗憾界限，并显著控制了OFU基安全RL算法固有的episode间变异性

Conclusion: 熵正则化在安全强化学习中具有重要作用，不仅能改善算法性能，还能增强学习过程的稳定性

Abstract: We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.

</details>


### [66] [Model-Agnostic Solutions for Deep Reinforcement Learning in Non-Ergodic Contexts](https://arxiv.org/abs/2601.08726)
*Bert Verbruggen,Arne Vanhoyweghen,Vincent Ginis*

Main category: cs.LG

TL;DR: 传统深度强化学习在非遍历环境中产生次优策略，通过引入时间依赖性可纠正此问题，无需改变环境反馈。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习基于期望值优化，在非遍历环境中，长期结果取决于具体轨迹而非整体平均，导致期望值公式产生系统性的次优策略。需要解决深度强化学习在非遍历环境中的优化问题。

Method: 在深度强化学习中引入显式的时间依赖性，让网络函数近似能够纳入时间信息，使智能体能够估计与过程内在增长率一致的价值函数。这种方法不改变环境反馈（如奖励变换或修改目标函数），而是通过智能体对时间轨迹的暴露自然实现。

Result: 研究表明传统深度强化学习在非遍历环境中确实产生次优策略，而引入时间依赖性的方法能够纠正这一限制，使智能体能够学习到更优的策略。

Conclusion: 在非遍历环境中，传统基于期望值的强化学习方法存在系统性缺陷，通过引入时间依赖性可以自然纠正这一问题，这为开发适用于非遍历系统的强化学习方法提供了新思路。

Abstract: Reinforcement Learning (RL) remains a central optimisation framework in machine learning. Although RL agents can converge to optimal solutions, the definition of ``optimality'' depends on the environment's statistical properties. The Bellman equation, central to most RL algorithms, is formulated in terms of expected values of future rewards. However, when ergodicity is broken, long-term outcomes depend on the specific trajectory rather than on the ensemble average. In such settings, the ensemble average diverges from the time-average growth experienced by individual agents, with expected-value formulations yielding systematically suboptimal policies. Prior studies demonstrated that traditional RL architectures fail to recover the true optimum in non-ergodic environments. We extend this analysis to deep RL implementations and show that these, too, produce suboptimal policies under non-ergodic dynamics. Introducing explicit time dependence into the learning process can correct this limitation. By allowing the network's function approximation to incorporate temporal information, the agent can estimate value functions consistent with the process's intrinsic growth rate. This improvement does not require altering the environmental feedback, such as reward transformations or modified objective functions, but arises naturally from the agent's exposure to temporal trajectories. Our results contribute to the growing body of research on reinforcement learning methods for non-ergodic systems.

</details>


### [67] [Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs](https://arxiv.org/abs/2601.08763)
*Zhiyuan Hu,Yucheng Wang,Yufei He,Jiaying Wu,Yilun Zhao,See-Kiong Ng,Cynthia Breazeal,Anh Tuan Luu,Hae Won Park,Bryan Hooi*

Main category: cs.LG

TL;DR: 提出Uniqueness-Aware Reinforcement Learning方法，通过奖励独特的高层解题策略来解决LLM强化学习中的探索崩溃问题，在保持pass@1的同时提升pass@k和策略多样性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在训练大型语言模型进行复杂推理任务时，容易出现探索崩溃问题：策略过早集中于少数主导推理模式，虽然提升了pass@1，但限制了rollout层面的多样性和pass@k的提升。

Method: 提出Uniqueness-Aware Reinforcement Learning方法，使用LLM作为评判器对同一问题的rollout进行聚类（根据高层解题策略，忽略表面差异），并按聚类大小反比重新加权策略优势，从而奖励正确但新颖的策略。

Result: 在数学、物理和医学推理基准测试中，该方法在保持pass@1的同时，显著提升了pass@k和AUC@K，维持了探索能力，并在大规模采样中发现了更多样化的解题策略。

Conclusion: 通过显式奖励独特的高层解题策略，可以有效解决LLM强化学习中的探索崩溃问题，在复杂推理任务中实现更好的多样性和性能提升。

Abstract: Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.

</details>
