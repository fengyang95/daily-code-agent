{"id": "2510.15494", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15494", "abs": "https://arxiv.org/abs/2510.15494", "authors": ["Lirong Yi", "Gregory Gay", "Philipp Leitner"], "title": "An Experimental Study of Real-Life LLM-Proposed Performance Improvements", "comment": null, "summary": "Large Language Models (LLMs) can generate code, but can they generate fast\ncode? In this paper, we study this question using a dataset of 65 real-world\ntasks mined from open-source Java programs. We specifically select tasks where\ndevelopers achieved significant speedups, and employ an automated pipeline to\ngenerate patches for these issues using two leading LLMs under four prompt\nvariations. By rigorously benchmarking the results against the baseline and\nhuman-authored solutions, we demonstrate that LLM-generated code indeed\nimproves performance over the baseline in most cases. However, patches proposed\nby human developers outperform LLM fixes by a statistically significant margin,\nindicating that LLMs often fall short of finding truly optimal solutions. We\nfurther find that LLM solutions are semantically identical or similar to the\ndeveloper optimization idea in approximately two-thirds of cases, whereas they\npropose a more original idea in the remaining one-third. However, these\noriginal ideas only occasionally yield substantial performance gains.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u7684\u6027\u80fd\u4f18\u5316\u80fd\u529b\uff0c\u53d1\u73b0LLM\u80fd\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6539\u8fdb\u57fa\u7ebf\u6027\u80fd\uff0c\u4f46\u4ecd\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u5f00\u53d1\u8005\u7684\u4f18\u5316\u6c34\u5e73\u3002", "motivation": "\u63a2\u8ba8LLM\u662f\u5426\u80fd\u751f\u6210\u9ad8\u6027\u80fd\u4ee3\u7801\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u529f\u80fd\u6b63\u786e\u7684\u4ee3\u7801\u3002", "method": "\u4f7f\u752865\u4e2a\u771f\u5b9eJava\u4efb\u52a1\u6570\u636e\u96c6\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u7528\u4e24\u79cd\u9886\u5148LLM\u751f\u6210\u8865\u4e01\uff0c\u5e76\u4e0e\u57fa\u7ebf\u548c\u4eba\u7c7b\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "LLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u6539\u8fdb\u6027\u80fd\uff0c\u4f46\u4eba\u7c7b\u5f00\u53d1\u8005\u7684\u8865\u4e01\u5728\u7edf\u8ba1\u4e0a\u663e\u8457\u4f18\u4e8eLLM\u4fee\u590d\u3002\u7ea6\u4e09\u5206\u4e4b\u4e8c\u60c5\u51b5\u4e0bLLM\u89e3\u51b3\u65b9\u6848\u4e0e\u5f00\u53d1\u8005\u4f18\u5316\u601d\u8def\u76f8\u4f3c\uff0c\u5176\u4f59\u4e3a\u539f\u521b\u601d\u8def\u4f46\u5f88\u5c11\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "LLM\u5728\u4ee3\u7801\u6027\u80fd\u4f18\u5316\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5c1a\u672a\u8fbe\u5230\u4eba\u7c7b\u4e13\u5bb6\u7684\u4f18\u5316\u6c34\u5e73\uff0c\u539f\u521b\u601d\u8def\u7684\u4f18\u5316\u6548\u679c\u6709\u9650\u3002", "topic": "code agent"}}
{"id": "2510.15512", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15512", "abs": "https://arxiv.org/abs/2510.15512", "authors": ["Wachiraphan Charoenwet", "Patanamon Thongtanunam", "Van-Thuan Pham", "Christoph Treude"], "title": "Enhancing Code Review through Fuzzing and Likely Invariants", "comment": null, "summary": "Many software projects employ manual code review to gatekeep defects and\nvulnerabilities in the code before integration. However, reviewers often work\nunder time pressure and rely primarily on static inspection, leaving the\ndynamic aspects of the program unexplored. Dynamic analyses could reveal such\nbehaviors, but they are rarely integrated into reviews. Among them, fuzzing is\ntypically applied later to uncover crashing bugs. Yet its ability to exercise\ncode with diverse inputs makes it promising for exposing non-crashing, but\nunexpected, behaviors earlier. Still, without suitable mechanisms to analyze\nprogram behaviors, the rich data produced during fuzzing remains inaccessible\nto reviewers, limiting its practical value in this context.\n  We hypothesize that unexpected variations in program behaviors could signify\npotential bugs. The impact of code changes can be automatically captured at\nruntime. Representing program behavior as likely invariants, dynamic properties\nconsistently observed at specific program points, can provide practical signals\nof behavioral changes. Such signals offer a way to distinguish between intended\nchanges and unexpected behavioral shifts from code changes.\n  We present FuzzSight, a framework that leverages likely invariants from\nnon-crashing fuzzing inputs to highlight behavioral differences across program\nversions. By surfacing such differences, it provides insights into which code\nblocks may need closer attention. In our evaluation, FuzzSight flagged 75% of\nregression bugs and up to 80% of vulnerabilities uncovered by 24-hour fuzzing.\nIt also outperformed SAST in identifying buggy code blocks, achieving ten times\nhigher detection rates with fewer false alarms. In summary, FuzzSight\ndemonstrates the potential and value of leveraging fuzzing and invariant\nanalysis for early-stage code review, bridging static inspection with dynamic\nbehavioral insights.", "AI": {"tldr": "FuzzSight\u662f\u4e00\u4e2a\u5229\u7528\u6a21\u7cca\u6d4b\u8bd5\u548c\u4e0d\u53d8\u5f0f\u5206\u6790\u6765\u68c0\u6d4b\u4ee3\u7801\u884c\u4e3a\u53d8\u5316\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u53d1\u73b0\u6f5c\u5728\u7f3a\u9677\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u5ba1\u67e5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u68c0\u67e5\uff0c\u96be\u4ee5\u53d1\u73b0\u52a8\u6001\u884c\u4e3a\u95ee\u9898\u3002\u6a21\u7cca\u6d4b\u8bd5\u867d\u7136\u80fd\u4ea7\u751f\u4e30\u5bcc\u7684\u884c\u4e3a\u6570\u636e\uff0c\u4f46\u7f3a\u4e4f\u5408\u9002\u7684\u5206\u6790\u673a\u5236\u6765\u5e2e\u52a9\u5ba1\u67e5\u8005\u8bc6\u522b\u975e\u5d29\u6e83\u6027\u884c\u4e3a\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u6a21\u7cca\u6d4b\u8bd5\u751f\u6210\u975e\u5d29\u6e83\u8f93\u5165\uff0c\u6355\u83b7\u7a0b\u5e8f\u8fd0\u884c\u65f6\u884c\u4e3a\uff0c\u5c06\u5176\u8868\u793a\u4e3a\u53ef\u80fd\u4e0d\u53d8\u5f0f\uff0c\u6bd4\u8f83\u4e0d\u540c\u7248\u672c\u95f4\u884c\u4e3a\u5dee\u5f02\u6765\u8bc6\u522b\u6f5c\u5728\u95ee\u9898\u3002", "result": "FuzzSight\u80fd\u6807\u8bb075%\u7684\u56de\u5f52\u7f3a\u9677\u548c\u9ad8\u8fbe80%\u7684\u6f0f\u6d1e\uff0c\u76f8\u6bd4SAST\u68c0\u6d4b\u7387\u63d0\u9ad810\u500d\u4e14\u8bef\u62a5\u66f4\u5c11\u3002", "conclusion": "FuzzSight\u5c55\u793a\u4e86\u5c06\u6a21\u7cca\u6d4b\u8bd5\u548c\u4e0d\u53d8\u5f0f\u5206\u6790\u96c6\u6210\u5230\u65e9\u671f\u4ee3\u7801\u5ba1\u67e5\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6865\u63a5\u9759\u6001\u68c0\u67e5\u548c\u52a8\u6001\u884c\u4e3a\u6d1e\u5bdf\u3002", "topic": "swe application"}}
{"id": "2510.15006", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2510.15006", "abs": "https://arxiv.org/abs/2510.15006", "authors": ["Rijul Tandon", "Peter Vamplew", "Cameron Foale"], "title": "ES-C51: Expected Sarsa Based C51 Distributional Reinforcement Learning Algorithm", "comment": null, "summary": "In most value-based reinforcement learning (RL) algorithms, the agent\nestimates only the expected reward for each action and selects the action with\nthe highest reward. In contrast, Distributional Reinforcement Learning (DRL)\nestimates the entire probability distribution of possible rewards, providing\nricher information about uncertainty and variability. C51 is a popular DRL\nalgorithm for discrete action spaces. It uses a Q-learning approach, where the\ndistribution is learned using a greedy Bellman update. However, this can cause\nproblems if multiple actions at a state have similar expected reward but with\ndifferent distributions, as the algorithm may not learn a stable distribution.\nThis study presents a modified version of C51 (ES-C51) that replaces the greedy\nQ-learning update with an Expected Sarsa update, which uses a softmax\ncalculation to combine information from all possible actions at a state rather\nthan relying on a single best action. This reduces instability when actions\nhave similar expected rewards and allows the agent to learn higher-performing\npolicies. This approach is evaluated on classic control environments from Gym,\nand Atari-10 games. For a fair comparison, we modify the standard C51's\nexploration strategy from e-greedy to softmax, which we refer to as QL-C51 (Q-\nLearning based C51). The results demonstrate that ES-C51 outperforms QL-C51\nacross many environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684C51\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5(ES-C51)\uff0c\u7528Expected Sarsa\u66f4\u65b0\u66ff\u4ee3\u8d2a\u5a6aQ\u5b66\u4e60\u66f4\u65b0\uff0c\u901a\u8fc7softmax\u8ba1\u7b97\u7ed3\u5408\u6240\u6709\u53ef\u80fd\u52a8\u4f5c\u7684\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u5f53\u52a8\u4f5c\u5177\u6709\u76f8\u4f3c\u671f\u671b\u5956\u52b1\u4f46\u4e0d\u540c\u5206\u5e03\u65f6\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u503c\u7684RL\u7b97\u6cd5\u53ea\u4f30\u8ba1\u671f\u671b\u5956\u52b1\uff0c\u800c\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u4f30\u8ba1\u6574\u4e2a\u5956\u52b1\u6982\u7387\u5206\u5e03\uff0c\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u53d8\u5f02\u6027\u4fe1\u606f\u3002\u4f46C51\u7b97\u6cd5\u7684\u8d2a\u5a6aQ\u5b66\u4e60\u66f4\u65b0\u5728\u591a\u4e2a\u52a8\u4f5c\u5177\u6709\u76f8\u4f3c\u671f\u671b\u5956\u52b1\u4f46\u4e0d\u540c\u5206\u5e03\u65f6\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faES-C51\u7b97\u6cd5\uff0c\u7528Expected Sarsa\u66f4\u65b0\u66ff\u4ee3\u8d2a\u5a6aQ\u5b66\u4e60\u66f4\u65b0\uff0c\u4f7f\u7528softmax\u8ba1\u7b97\u7ed3\u5408\u6240\u6709\u53ef\u80fd\u52a8\u4f5c\u7684\u4fe1\u606f\u3002\u4e3a\u516c\u5e73\u6bd4\u8f83\uff0c\u5c06\u6807\u51c6C51\u7684\u63a2\u7d22\u7b56\u7565\u4ece\u03b5-greedy\u6539\u4e3asoftmax\uff0c\u79f0\u4e3aQL-C51\u3002", "result": "\u5728Gym\u7ecf\u5178\u63a7\u5236\u73af\u5883\u548cAtari-10\u6e38\u620f\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cES-C51\u5728\u591a\u6570\u73af\u5883\u4e2d\u4f18\u4e8eQL-C51\u3002", "conclusion": "ES-C51\u901a\u8fc7Expected Sarsa\u66f4\u65b0\u6709\u6548\u89e3\u51b3\u4e86C51\u7b97\u6cd5\u5728\u76f8\u4f3c\u671f\u671b\u5956\u52b1\u52a8\u4f5c\u4e0b\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u80fd\u591f\u5b66\u4e60\u5230\u66f4\u9ad8\u6027\u80fd\u7684\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15585", "categories": ["cs.SE", "cs.CL", "cs.PL", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.15585", "abs": "https://arxiv.org/abs/2510.15585", "authors": ["Dr Simon Thorne", "Dr Advait Sarkar"], "title": "Leveraging Test Driven Development with Large Language Models for Reliable and Verifiable Spreadsheet Code Generation: A Research Framework", "comment": "16 pages", "summary": "Large Language Models (LLMs), such as ChatGPT, are increasingly leveraged for\ngenerating both traditional software code and spreadsheet logic. Despite their\nimpressive generative capabilities, these models frequently exhibit critical\nissues such as hallucinations, subtle logical inconsistencies, and syntactic\nerrors, risks particularly acute in high stakes domains like financial\nmodelling and scientific computations, where accuracy and reliability are\nparamount. This position paper proposes a structured research framework that\nintegrates the proven software engineering practice of Test-Driven Development\n(TDD) with Large Language Model (LLM) driven generation to enhance the\ncorrectness of, reliability of, and user confidence in generated outputs. We\nhypothesise that a \"test first\" methodology provides both technical constraints\nand cognitive scaffolding, guiding LLM outputs towards more accurate,\nverifiable, and comprehensible solutions. Our framework, applicable across\ndiverse programming contexts, from spreadsheet formula generation to scripting\nlanguages such as Python and strongly typed languages like Rust, includes an\nexplicitly outlined experimental design with clearly defined participant\ngroups, evaluation metrics, and illustrative TDD based prompting examples. By\nemphasising test driven thinking, we aim to improve computational thinking,\nprompt engineering skills, and user engagement, particularly benefiting\nspreadsheet users who often lack formal programming training yet face serious\nconsequences from logical errors. We invite collaboration to refine and\nempirically evaluate this approach, ultimately aiming to establish responsible\nand reliable LLM integration in both educational and professional development\npractices.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1(TDD)\u4e0eLLM\u751f\u6210\u7ed3\u5408\u7684\u7814\u7a76\u6846\u67b6\uff0c\u901a\u8fc7\"\u6d4b\u8bd5\u4f18\u5148\"\u65b9\u6cd5\u63d0\u9ad8LLM\u751f\u6210\u4ee3\u7801\u7684\u6b63\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u7279\u522b\u5173\u6ce8\u9ad8\u98ce\u9669\u9886\u57df\u5982\u91d1\u878d\u5efa\u6a21\u548c\u79d1\u5b66\u8ba1\u7b97\u3002", "motivation": "LLM\u5728\u751f\u6210\u4ee3\u7801\u65f6\u7ecf\u5e38\u51fa\u73b0\u5e7b\u89c9\u3001\u903b\u8f91\u4e0d\u4e00\u81f4\u548c\u8bed\u6cd5\u9519\u8bef\u7b49\u95ee\u9898\uff0c\u5728\u91d1\u878d\u5efa\u6a21\u548c\u79d1\u5b66\u8ba1\u7b97\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u8fd9\u4e9b\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002\u9700\u8981\u63d0\u9ad8LLM\u751f\u6210\u8f93\u51fa\u7684\u6b63\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u5c06\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1(TDD)\u4e0eLLM\u9a71\u52a8\u7684\u751f\u6210\u76f8\u7ed3\u5408\u7684\u7ed3\u6784\u5316\u7814\u7a76\u6846\u67b6\uff0c\u91c7\u7528\"\u6d4b\u8bd5\u4f18\u5148\"\u65b9\u6cd5\uff0c\u5305\u62ec\u660e\u786e\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u3001\u53c2\u4e0e\u8005\u5206\u7ec4\u3001\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u4e8eTDD\u7684\u63d0\u793a\u793a\u4f8b\u3002", "result": "\u8fd9\u662f\u4e00\u4e2a\u7acb\u573a\u8bba\u6587\uff0c\u63d0\u51fa\u4e86\u7814\u7a76\u6846\u67b6\u4f46\u5c1a\u672a\u6709\u5b9e\u8bc1\u7ed3\u679c\u3002\u6846\u67b6\u9002\u7528\u4e8e\u7535\u5b50\u8868\u683c\u516c\u5f0f\u751f\u6210\u3001Python\u811a\u672c\u548cRust\u7b49\u5f3a\u7c7b\u578b\u8bed\u8a00\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u8c03\u6d4b\u8bd5\u9a71\u52a8\u601d\u7ef4\uff0c\u65e8\u5728\u63d0\u9ad8\u8ba1\u7b97\u601d\u7ef4\u3001\u63d0\u793a\u5de5\u7a0b\u6280\u80fd\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u7279\u522b\u6709\u5229\u4e8e\u7f3a\u4e4f\u6b63\u5f0f\u7f16\u7a0b\u57f9\u8bad\u7684\u7535\u5b50\u8868\u683c\u7528\u6237\u3002\u9080\u8bf7\u5408\u4f5c\u6765\u5b8c\u5584\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8be5\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "2510.15047", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15047", "abs": "https://arxiv.org/abs/2510.15047", "authors": ["Shiqi Chen", "Tongyao Zhu", "Zian Wang", "Jinghan Zhang", "Kangrui Wang", "Siyang Gao", "Teng Xiao", "Yee Whye Teh", "Junxian He", "Manling Li"], "title": "Internalizing World Models via Self-Play Finetuning for Agentic RL", "comment": null, "summary": "Large Language Models (LLMs) as agents often struggle in out-of-distribution\n(OOD) scenarios. Real-world environments are complex and dynamic, governed by\ntask-specific rules and stochasticity, which makes it difficult for LLMs to\nground their internal knowledge in those dynamics. Under such OOD conditions,\nvanilla RL training often fails to scale; we observe Pass@k--the probability\nthat at least one of (k) sampled trajectories succeeds--drops markedly across\ntraining steps, indicating brittle exploration and limited generalization.\nInspired by model-based reinforcement learning, we hypothesize that equipping\nLLM agents with an internal world model can better align reasoning with\nenvironmental dynamics and improve decision-making. We show how to encode this\nworld model by decomposing it into two components: state representation and\ntransition modeling. Building on this, we introduce SPA, a simple reinforcement\nlearning framework that cold-starts the policy via a Self-Play supervised\nfinetuning (SFT) stage to learn the world model by interacting with the\nenvironment, then uses it to simulate future states prior to policy\noptimization. This simple initialization outperforms the online world-modeling\nbaseline and greatly boosts the RL-based agent training performance.\nExperiments across diverse environments like Sokoban, FrozenLake, and Sudoku\nshow that our approach significantly improves performance. For example, SPA\nboosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake\nscore from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.", "AI": {"tldr": "\u63d0\u51faSPA\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5fae\u8c03\u5b66\u4e60\u4e16\u754c\u6a21\u578b\uff0c\u7136\u540e\u7528\u4e8e\u7b56\u7565\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u5728OOD\u573a\u666f\u4e0b\u7684\u6027\u80fd", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u5c06\u5185\u90e8\u77e5\u8bc6\u4e0e\u73af\u5883\u52a8\u6001\u5bf9\u9f50\uff0c\u4f20\u7edfRL\u8bad\u7ec3\u6548\u679c\u6709\u9650", "method": "\u5c06\u4e16\u754c\u6a21\u578b\u5206\u89e3\u4e3a\u72b6\u6001\u8868\u793a\u548c\u8f6c\u79fb\u5efa\u6a21\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u5b66\u4e60\u4e16\u754c\u6a21\u578b\uff0c\u7136\u540e\u5728\u7b56\u7565\u4f18\u5316\u524d\u6a21\u62df\u672a\u6765\u72b6\u6001", "result": "\u5728Sokoban\u3001FrozenLake\u548cSudoku\u7b49\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5982Sokoban\u6210\u529f\u7387\u4ece25.6%\u63d0\u5347\u523059.8%", "conclusion": "\u4e3aLLM\u667a\u80fd\u4f53\u914d\u5907\u5185\u90e8\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u6539\u5584\u63a8\u7406\u4e0e\u73af\u5883\u52a8\u6001\u7684\u5bf9\u9f50\uff0c\u63d0\u5347\u51b3\u7b56\u80fd\u529b", "topic": "agentic reinforcement learning"}}
{"id": "2510.15191", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.15191", "abs": "https://arxiv.org/abs/2510.15191", "authors": ["Junlin Wu", "Xianrui Zhong", "Jiashuo Sun", "Bolian Li", "Bowen Jin", "Jiawei Han", "Qingkai Zeng"], "title": "Structure-R1: Dynamically Leveraging Structural Knowledge in LLM Reasoning through Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable advances in\nreasoning capabilities. However, their performance remains constrained by\nlimited access to explicit and structured domain knowledge. Retrieval-Augmented\nGeneration (RAG) addresses this by incorporating external information as\ncontext to augment reasoning. Nevertheless, traditional RAG systems typically\noperate over unstructured and fragmented text, resulting in low information\ndensity and suboptimal reasoning. To overcome these limitations, we propose\n\\textsc{Structure-R1}, a novel framework that transforms retrieved content into\nstructured representations optimized for reasoning. Leveraging reinforcement\nlearning, \\textsc{Structure-R1} learns a content representation policy that\ndynamically generates and adapts structural formats based on the demands of\nmulti-step reasoning. Unlike prior methods that rely on fixed schemas, our\napproach adopts a generative paradigm capable of producing task-specific\nstructures tailored to individual queries. To ensure the quality and\nreliability of these representations, we introduce a self-reward structural\nverification mechanism that checks whether the generated structures are both\ncorrect and self-contained. Extensive experiments on seven knowledge-intensive\nbenchmarks show that \\textsc{Structure-R1} consistently achieves competitive\nperformance with a 7B-scale backbone model and matches the performance of much\nlarger models. Additionally, our theoretical analysis demonstrates how\nstructured representations enhance reasoning by improving information density\nand contextual clarity. Our code and data are available at:\nhttps://github.com/jlwu002/sr1.", "AI": {"tldr": "\u63d0\u51fa\u4e86Structure-R1\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u68c0\u7d22\u5185\u5bb9\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u8868\u793a\uff0c\u4f18\u5316\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u57287B\u89c4\u6a21\u6a21\u578b\u4e0a\u8fbe\u5230\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u4f7f\u7528\u975e\u7ed3\u6784\u5316\u6587\u672c\u5bfc\u81f4\u4fe1\u606f\u5bc6\u5ea6\u4f4e\u548c\u63a8\u7406\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u6765\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5b66\u4e60\u5185\u5bb9\u8868\u793a\u7b56\u7565\uff0c\u52a8\u6001\u751f\u6210\u9002\u5e94\u591a\u6b65\u63a8\u7406\u9700\u6c42\u7684\u7ed3\u6784\u5316\u683c\u5f0f\uff0c\u5e76\u5f15\u5165\u81ea\u5956\u52b1\u7ed3\u6784\u9a8c\u8bc1\u673a\u5236\u786e\u4fdd\u751f\u6210\u7ed3\u6784\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002", "result": "\u57287\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7B\u89c4\u6a21\u6a21\u578b\u8fbe\u5230\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u8868\u793a\u5bf9\u63a8\u7406\u7684\u589e\u5f3a\u6548\u679c\u3002", "conclusion": "\u7ed3\u6784\u5316\u8868\u793a\u901a\u8fc7\u63d0\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u548c\u4e0a\u4e0b\u6587\u6e05\u6670\u5ea6\u663e\u8457\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0cStructure-R1\u6846\u67b6\u4e3a\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15056", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15056", "abs": "https://arxiv.org/abs/2510.15056", "authors": ["Ziqing Lu", "Babak Hassibi", "Lifeng Lai", "Weiyu Xu"], "title": "Learn to Change the World: Multi-level Reinforcement Learning with Model-Changing Actions", "comment": null, "summary": "Reinforcement learning usually assumes a given or sometimes even fixed\nenvironment in which an agent seeks an optimal policy to maximize its long-term\ndiscounted reward. In contrast, we consider agents that are not limited to\npassive adaptations: they instead have model-changing actions that actively\nmodify the RL model of world dynamics itself. Reconfiguring the underlying\ntransition processes can potentially increase the agents' rewards. Motivated by\nthis setting, we introduce the multi-layer configurable time-varying Markov\ndecision process (MCTVMDP). In an MCTVMDP, the lower-level MDP has a\nnon-stationary transition function that is configurable through upper-level\nmodel-changing actions. The agent's objective consists of two parts: Optimize\nthe configuration policies in the upper-level MDP and optimize the primitive\naction policies in the lower-level MDP to jointly improve its expected\nlong-term reward.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u5c42\u7ea7\u53ef\u914d\u7f6e\u65f6\u53d8\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MCTVMDP\uff09\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u4e0d\u4ec5\u53ef\u4ee5\u901a\u8fc7\u5e95\u5c42\u52a8\u4f5c\u4e0e\u73af\u5883\u4e92\u52a8\uff0c\u8fd8\u80fd\u901a\u8fc7\u4e0a\u5c42\u52a8\u4f5c\u4e3b\u52a8\u4fee\u6539\u73af\u5883\u52a8\u6001\u6a21\u578b\u672c\u8eab\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5047\u8bbe\u73af\u5883\u56fa\u5b9a\u4e0d\u53d8\uff0c\u4f46\u5b9e\u9645\u4e2d\u667a\u80fd\u4f53\u53ef\u80fd\u901a\u8fc7\u4e3b\u52a8\u6539\u53d8\u73af\u5883\u52a8\u6001\u6765\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u8fd9\u79cd\u80fd\u591f\u4e3b\u52a8\u914d\u7f6e\u73af\u5883\u7684\u667a\u80fd\u4f53\u3002", "method": "\u5f15\u5165MCTVMDP\u6846\u67b6\uff0c\u5305\u542b\u4e0a\u5c42\u914d\u7f6e\u7b56\u7565\u548c\u4e0b\u5c42\u539f\u59cb\u52a8\u4f5c\u7b56\u7565\u4e24\u4e2a\u5c42\u7ea7\u3002\u4e0a\u5c42\u52a8\u4f5c\u53ef\u4ee5\u4fee\u6539\u5e95\u5c42MDP\u7684\u975e\u5e73\u7a33\u8f6c\u79fb\u51fd\u6570\uff0c\u667a\u80fd\u4f53\u9700\u8981\u8054\u5408\u4f18\u5316\u4e24\u4e2a\u5c42\u7ea7\u7684\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u73af\u5883\u52a8\u6001\u4e3b\u52a8\u914d\u7f6e\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u4f20\u7edfMDP\u7684\u5efa\u6a21\u80fd\u529b\u3002", "conclusion": "MCTVMDP\u4e3a\u7814\u7a76\u80fd\u591f\u4e3b\u52a8\u6539\u53d8\u73af\u5883\u52a8\u6001\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u73af\u5883\u56fa\u5b9a\u7684\u5047\u8bbe\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15110", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15110", "abs": "https://arxiv.org/abs/2510.15110", "authors": ["Shih-Yang Liu", "Xin Dong", "Ximing Lu", "Shizhe Diao", "Mingjie Liu", "Min-Hung Chen", "Hongxu Yin", "Yu-Chiang Frank Wang", "Kwang-Ting Cheng", "Yejin Choi", "Jan Kautz", "Pavlo Molchanov"], "title": "DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning", "comment": "NVIDIA-Tech Report", "summary": "Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve\nstrong performance via extended chains of thought but often generate\nunnecessarily long outputs. Maximizing intelligence per token--accuracy\nrelative to response length--remains an open problem. We revisit reinforcement\nlearning (RL) with the simplest length penalty--truncation--and show that\naccuracy degradation arises not from the lack of sophisticated penalties but\nfrom inadequate RL optimization. We identify three key challenges: (i) large\nbias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward\nsignal. We address them with Doing Length pEnalty Right (DLER), a training\nrecipe combining batch-wise reward normalization, higher clipping, dynamic\nsampling, and a simple truncation length penalty. DLER achieves\nstate-of-the-art accuracy--efficiency trade-offs, cutting output length by over\n70 percent while surpassing all previous baseline accuracy. It also improves\ntest-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple\nconcise responses in parallel with 28 percent higher accuracy and lower\nlatency. We further introduce Difficulty-Aware DLER, which adaptively tightens\ntruncation on easier questions for additional efficiency gains. We also propose\nan update-selective merging method that preserves baseline accuracy while\nretaining the concise reasoning ability of the DLER model, which is useful for\nscenarios where RL training data is scarce.", "AI": {"tldr": "DLER\u901a\u8fc7\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u622a\u65ad\u957f\u5ea6\u60e9\u7f5a\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u957f\u5ea6\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u51c6\u786e\u7387-\u6548\u7387\u6743\u8861\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u8f93\u51fa\u8fc7\u4e8e\u5197\u957f\uff0c\u9700\u8981\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u8f93\u51fa\u6548\u7387\uff0c\u6700\u5927\u5316\u6bcf\u4e2atoken\u7684\u667a\u80fd\u5ea6\u3002", "method": "\u63d0\u51faDLER\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u6279\u91cf\u5956\u52b1\u5f52\u4e00\u5316\u3001\u66f4\u9ad8\u88c1\u526a\u3001\u52a8\u6001\u91c7\u6837\u548c\u7b80\u5355\u622a\u65ad\u957f\u5ea6\u60e9\u7f5a\uff0c\u89e3\u51b3RL\u4f18\u5316\u4e2d\u7684\u4f18\u52bf\u4f30\u8ba1\u504f\u5dee\u3001\u71b5\u5d29\u6e83\u548c\u7a00\u758f\u5956\u52b1\u4fe1\u53f7\u95ee\u9898\u3002", "result": "DLER\u5c06\u8f93\u51fa\u957f\u5ea6\u51cf\u5c1170%\u4ee5\u4e0a\uff0c\u540c\u65f6\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u7684\u51c6\u786e\u7387\u3002DLER-7B\u76f8\u6bd4DeepSeek-R1-7B\u51c6\u786e\u7387\u63d0\u9ad828%\uff0c\u5ef6\u8fdf\u66f4\u4f4e\u3002", "conclusion": "DLER\u8bc1\u660e\u4e86\u901a\u8fc7\u6539\u8fdbRL\u4f18\u5316\u800c\u975e\u590d\u6742\u60e9\u7f5a\u673a\u5236\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u51c6\u786e\u7387-\u6548\u7387\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u96be\u5ea6\u611f\u77e5DLER\u548c\u9009\u62e9\u6027\u5408\u5e76\u65b9\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15283", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15283", "abs": "https://arxiv.org/abs/2510.15283", "authors": ["Jingao Xu", "Shuoyoucheng Ma", "Xin Song", "Rong Jiang", "Hongkui Tu", "Bin Zhou"], "title": "Exemplar-Guided Planing: Enhanced LLM Agent for KGQA", "comment": null, "summary": "Large Language Models (LLMs) as interactive agents show significant promise\nin Knowledge Graph Question Answering (KGQA) but often struggle with the\nsemantic gap between natural language queries and structured knowledge graph\n(KG) representations. This leads to suboptimal planning and inefficient\nexploration on KG, while training-free approaches often underutilize valuable\nreasoning patterns in training data. To address these limitations, we propose a\nnovel framework, Exemplar-Guided Planning (EGP), which enhances the planning\ncapabilities of LLM agents for KGQA. EGP first preprocesses the training set\nquestions via entity templating to normalize semantic variations. It then\nretrieves highly similar exemplary questions and their successful reasoning\npaths from this preprocessed set using semantic embeddings and an efficient\nFAISS index. These retrieved exemplars dynamically guide the LLM's planning\nprocess in two key phases: (1) Task Decomposition, by aligning generated\nsub-objectives with proven reasoning steps, and (2) Relation Exploration, by\nproviding high-quality auxiliary information to improve relation pruning\naccuracy. Additionally, we introduce a Smart Lookahead mechanism during\nrelation exploration to improve efficiency by preemptively exploring promising\npaths and potentially terminating exploration earlier. We apply EGP to the\nPlan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two\nreal-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP\nsignificantly improves over the baseline PoG system and other compared methods.", "AI": {"tldr": "\u63d0\u51faEGP\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u8bad\u7ec3\u96c6\u4e2d\u7684\u793a\u4f8b\u95ee\u9898\u53ca\u5176\u6210\u529f\u63a8\u7406\u8def\u5f84\uff0c\u6307\u5bfcLLM\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u7684\u89c4\u5212\u8fc7\u7a0b\uff0c\u5305\u62ec\u4efb\u52a1\u5206\u89e3\u548c\u5173\u7cfb\u63a2\u7d22\uff0c\u63d0\u5347\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "LLM\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u9762\u4e34\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e0e\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u5bfc\u81f4\u89c4\u5212\u4e0d\u4f18\u548c\u63a2\u7d22\u6548\u7387\u4f4e\uff0c\u800c\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "method": "EGP\u6846\u67b6\uff1a\u9884\u5904\u7406\u8bad\u7ec3\u96c6\u95ee\u9898\u8fdb\u884c\u5b9e\u4f53\u6a21\u677f\u5316\uff0c\u68c0\u7d22\u76f8\u4f3c\u793a\u4f8b\u95ee\u9898\u53ca\u5176\u63a8\u7406\u8def\u5f84\uff0c\u5728\u4efb\u52a1\u5206\u89e3\u548c\u5173\u7cfb\u63a2\u7d22\u4e24\u4e2a\u9636\u6bb5\u52a8\u6001\u6307\u5bfcLLM\u89c4\u5212\uff0c\u5e76\u5f15\u5165\u667a\u80fd\u524d\u77bb\u673a\u5236\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002", "result": "\u5728WebQSP\u548cCWQ\u4e24\u4e2a\u771f\u5b9e\u4e16\u754cKGQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPoG-EGP\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfPoG\u7cfb\u7edf\u548c\u5176\u4ed6\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "EGP\u6846\u67b6\u901a\u8fc7\u793a\u4f8b\u5f15\u5bfc\u7684\u89c4\u5212\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u8bad\u7ec3\u6570\u636e\u4e2d\u6210\u529f\u63a8\u7406\u6a21\u5f0f\u7684\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2510.15120", "categories": ["cs.AI", "I.2.6, I.2.8, I.2.11, I.3.7"], "pdf": "https://arxiv.org/pdf/2510.15120", "abs": "https://arxiv.org/abs/2510.15120", "authors": ["Mira\u00e7 Bu\u011fra \u00d6zkan"], "title": "Procedural Game Level Design with Deep Reinforcement Learning", "comment": "11 pages, 10 figures, IEEE conference format", "summary": "Procedural content generation (PCG) has become an increasingly popular\ntechnique in game development, allowing developers to generate dynamic,\nreplayable, and scalable environments with reduced manual effort. In this\nstudy, a novel method for procedural level design using Deep Reinforcement\nLearning (DRL) within a Unity-based 3D environment is proposed. The system\ncomprises two agents: a hummingbird agent, acting as a solver, and a floating\nisland agent, responsible for generating and placing collectible objects\n(flowers) on the terrain in a realistic and context-aware manner. The\nhummingbird is trained using the Proximal Policy Optimization (PPO) algorithm\nfrom the Unity ML-Agents toolkit. It learns to navigate through the terrain\nefficiently, locate flowers, and collect them while adapting to the\never-changing procedural layout of the island. The island agent is also trained\nusing the Proximal Policy Optimization (PPO) algorithm. It learns to generate\nflower layouts based on observed obstacle positions, the hummingbird's initial\nstate, and performance feedback from previous episodes. The interaction between\nthese agents leads to emergent behavior and robust generalization across\nvarious environmental configurations. The results demonstrate that the approach\nnot only produces effective and efficient agent behavior but also opens up new\nopportunities for autonomous game level design driven by machine learning. This\nwork highlights the potential of DRL in enabling intelligent agents to both\ngenerate and solve content in virtual environments, pushing the boundaries of\nwhat AI can contribute to creative game development processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u7a0b\u5e8f\u5316\u5173\u5361\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e24\u4e2a\u667a\u80fd\u4f53\u5728Unity 3D\u73af\u5883\u4e2d\u534f\u540c\u5de5\u4f5c\uff1a\u8702\u9e1f\u667a\u80fd\u4f53\u8d1f\u8d23\u5bfc\u822a\u6536\u96c6\u82b1\u6735\uff0c\u6d6e\u5c9b\u667a\u80fd\u4f53\u8d1f\u8d23\u751f\u6210\u548c\u653e\u7f6e\u82b1\u6735\u3002", "motivation": "\u63a2\u7d22\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u6e38\u620f\u7a0b\u5e8f\u5316\u5185\u5bb9\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u51cf\u5c11\u624b\u52a8\u8bbe\u8ba1\u5de5\u4f5c\u91cf\uff0c\u521b\u9020\u52a8\u6001\u3001\u53ef\u91cd\u73a9\u548c\u53ef\u6269\u5c55\u7684\u6e38\u620f\u73af\u5883\u3002", "method": "\u4f7f\u7528Unity ML-Agents\u5de5\u5177\u5305\uff0c\u91c7\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316(PPO)\u7b97\u6cd5\u8bad\u7ec3\u4e24\u4e2a\u667a\u80fd\u4f53\uff1a\u8702\u9e1f\u667a\u80fd\u4f53\u5b66\u4e60\u5bfc\u822a\u548c\u6536\u96c6\u82b1\u6735\uff0c\u6d6e\u5c9b\u667a\u80fd\u4f53\u5b66\u4e60\u57fa\u4e8e\u969c\u788d\u7269\u4f4d\u7f6e\u548c\u8702\u9e1f\u6027\u80fd\u53cd\u9988\u751f\u6210\u82b1\u6735\u5e03\u5c40\u3002", "result": "\u7cfb\u7edf\u4ea7\u751f\u4e86\u6d8c\u73b0\u884c\u4e3a\uff0c\u5728\u4e0d\u540c\u73af\u5883\u914d\u7f6e\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u548c\u81ea\u4e3b\u6e38\u620f\u5173\u5361\u8bbe\u8ba1\u3002", "conclusion": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u4f7f\u667a\u80fd\u4f53\u5728\u865a\u62df\u73af\u5883\u4e2d\u65e2\u751f\u6210\u53c8\u89e3\u51b3\u5185\u5bb9\uff0c\u4e3aAI\u5728\u521b\u610f\u6e38\u620f\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7684\u8d21\u732e\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15128", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15128", "abs": "https://arxiv.org/abs/2510.15128", "authors": ["Marcus A. Thomas"], "title": "Towards Error Centric Intelligence I, Beyond Observational Learning", "comment": null, "summary": "We argue that progress toward AGI is theory limited rather than data or scale\nlimited. Building on the critical rationalism of Popper and Deutsch, we\nchallenge the Platonic Representation Hypothesis. Observationally equivalent\nworlds can diverge under interventions, so observational adequacy alone cannot\nguarantee interventional competence. We begin by laying foundations,\ndefinitions of knowledge, learning, intelligence, counterfactual competence and\nAGI, and then analyze the limits of observational learning that motivate an\nerror centric shift. We recast the problem as three questions about how\nexplicit and implicit errors evolve under an agent's actions, which errors are\nunreachable within a fixed hypothesis space, and how conjecture and criticism\nexpand that space. From these questions we propose Causal Mechanics, a\nmechanisms first program in which hypothesis space change is a first class\noperation and probabilistic structure is used when useful rather than presumed.\nWe advance structural principles that make error discovery and correction\ntractable, including a differential Locality and Autonomy Principle for modular\ninterventions, a gauge invariant form of Independent Causal Mechanisms for\nseparability, and the Compositional Autonomy Principle for analogy\npreservation, together with actionable diagnostics. The aim is a scaffold for\nsystems that can convert unreachable errors into reachable ones and correct\nthem.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3aAGI\u53d1\u5c55\u7684\u74f6\u9888\u5728\u4e8e\u7406\u8bba\u800c\u975e\u6570\u636e\u6216\u89c4\u6a21\uff0c\u63d0\u51fa\u57fa\u4e8e\u6279\u5224\u7406\u6027\u4e3b\u4e49\u7684\u56e0\u679c\u529b\u5b66\u6846\u67b6\uff0c\u5f3a\u8c03\u5047\u8bbe\u7a7a\u95f4\u53d8\u5316\u4f5c\u4e3a\u6838\u5fc3\u64cd\u4f5c\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u5e72\u9884\u548c\u9519\u8bef\u53d1\u73b0\u673a\u5236\u5b9e\u73b0\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "motivation": "\u6311\u6218\u67cf\u62c9\u56fe\u8868\u793a\u5047\u8bf4\uff0c\u6307\u51fa\u89c2\u6d4b\u7b49\u4ef7\u7684\u4e16\u754c\u5728\u5e72\u9884\u4e0b\u53ef\u80fd\u4e0d\u540c\uff0c\u4ec5\u9760\u89c2\u6d4b\u5145\u5206\u6027\u65e0\u6cd5\u4fdd\u8bc1\u5e72\u9884\u80fd\u529b\uff0c\u9700\u8981\u8f6c\u5411\u9519\u8bef\u4e2d\u5fc3\u7684\u89c6\u89d2\u6765\u89e3\u51b3AGI\u7684\u7406\u8bba\u9650\u5236\u3002", "method": "\u63d0\u51fa\u56e0\u679c\u529b\u5b66\u6846\u67b6\uff0c\u5c06\u5047\u8bbe\u7a7a\u95f4\u53d8\u5316\u4f5c\u4e3a\u9996\u8981\u64cd\u4f5c\uff0c\u5f15\u5165\u5c40\u90e8\u6027\u81ea\u4e3b\u539f\u5219\u3001\u72ec\u7acb\u56e0\u679c\u673a\u5236\u548c\u7ec4\u5408\u81ea\u4e3b\u539f\u5219\u7b49\u7ed3\u6784\u539f\u5219\uff0c\u4f7f\u9519\u8bef\u53d1\u73b0\u548c\u4fee\u6b63\u53d8\u5f97\u53ef\u884c\u3002", "result": "\u5efa\u7acb\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u53ef\u64cd\u4f5c\u7684\u8bca\u65ad\u65b9\u6cd5\uff0c\u65e8\u5728\u6784\u5efa\u80fd\u591f\u5c06\u4e0d\u53ef\u8fbe\u9519\u8bef\u8f6c\u5316\u4e3a\u53ef\u8fbe\u9519\u8bef\u5e76\u8fdb\u884c\u4fee\u6b63\u7684\u7cfb\u7edf\u811a\u624b\u67b6\u3002", "conclusion": "AGI\u53d1\u5c55\u7684\u5173\u952e\u7a81\u7834\u5728\u4e8e\u7406\u8bba\u521b\u65b0\u800c\u975e\u6570\u636e\u89c4\u6a21\uff0c\u56e0\u679c\u529b\u5b66\u6846\u67b6\u4e3a\u89e3\u51b3\u667a\u80fd\u7cfb\u7edf\u7684\u6839\u672c\u9650\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2510.15144", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.15144", "abs": "https://arxiv.org/abs/2510.15144", "authors": ["Chance Jiajie Li", "Zhenze Mo", "Yuhan Tang", "Ao Qu", "Jiayi Wu", "Kaiya Ivy Zhao", "Yulu Gan", "Jie Fan", "Jiangbo Yu", "Hang Jiang", "Paul Pu Liang", "Jinhua Zhao", "Luis Alberto Alonso Pastor", "Kent Larson"], "title": "HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks", "comment": "To appear in NeurIPS 2025 Workshop on Bridging Language, Agent, and\n  World Models (LAW)", "summary": "Simulating human reasoning in open-ended tasks has been a long-standing\naspiration in AI and cognitive science. While large language models now\napproximate human responses at scale, they remain tuned to population-level\nconsensus, often erasing the individuality of reasoning styles and belief\ntrajectories. To advance the vision of more human-like reasoning in machines,\nwe introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for\naverage-to-individual reasoning adaptation. The task is to predict how a\nspecific person would reason and update their beliefs in novel scenarios, given\npartial evidence of their past views. HugAgent adopts a dual-track design: a\nsynthetic track for scale and systematic stress tests, and a human track for\necologically valid, \"out-loud\" reasoning data. This design enables scalable,\nreproducible evaluation of intra-agent fidelity: whether models can capture not\njust what people believe, but how their reasoning evolves. Experiments with\nstate-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent\nas the first extensible benchmark for aligning machine reasoning with the\nindividuality of human thought. Our benchmark and chatbot are open-sourced as\nHugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking\n(https://anonymous.4open.science/r/trace-your-thinking).", "AI": {"tldr": "HugAgent\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u6a21\u578b\u5982\u4f55\u9002\u5e94\u4e2a\u4f53\u63a8\u7406\u98ce\u683c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u5408\u6210\u548c\u4eba\u7c7b\u53cc\u8f68\u8bbe\u8ba1\uff0c\u65e8\u5728\u8ba9\u673a\u5668\u63a8\u7406\u66f4\u8d34\u8fd1\u4eba\u7c7b\u601d\u7ef4\u7684\u4e2a\u4f53\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u5927\u89c4\u6a21\u8fd1\u4f3c\u4eba\u7c7b\u54cd\u5e94\uff0c\u4f46\u4e3b\u8981\u53cd\u6620\u7fa4\u4f53\u5171\u8bc6\uff0c\u7f3a\u4e4f\u5bf9\u4e2a\u4f53\u63a8\u7406\u98ce\u683c\u548c\u4fe1\u5ff5\u8f68\u8ff9\u7684\u6355\u6349\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u8f68\u8bbe\u8ba1\uff1a\u5408\u6210\u8f68\u9053\u7528\u4e8e\u89c4\u6a21\u5316\u548c\u7cfb\u7edf\u6027\u538b\u529b\u6d4b\u8bd5\uff0c\u4eba\u7c7b\u8f68\u9053\u7528\u4e8e\u751f\u6001\u6709\u6548\u7684\"\u51fa\u58f0\u601d\u8003\"\u63a8\u7406\u6570\u636e\u6536\u96c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6700\u5148\u8fdb\u7684LLM\u5728\u9002\u5e94\u4e2a\u4f53\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u6301\u7eed\u5dee\u8ddd\u3002", "conclusion": "HugAgent\u662f\u9996\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u5c06\u673a\u5668\u63a8\u7406\u4e0e\u4eba\u7c7b\u601d\u7ef4\u7684\u4e2a\u4f53\u6027\u5bf9\u9f50\u3002", "topic": "agent analysis"}}
{"id": "2510.15165", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.15165", "abs": "https://arxiv.org/abs/2510.15165", "authors": ["Xin Guo", "Zijiu Lyu"], "title": "Policy Transfer Ensures Fast Learning for Continuous-Time LQR with Entropy Regularization", "comment": null, "summary": "Reinforcement Learning (RL) enables agents to learn optimal decision-making\nstrategies through interaction with an environment, yet training from scratch\non complex tasks can be highly inefficient. Transfer learning (TL), widely\nsuccessful in large language models (LLMs), offers a promising direction for\nenhancing RL efficiency by leveraging pre-trained models.\n  This paper investigates policy transfer, a TL approach that initializes\nlearning in a target RL task using a policy from a related source task, in the\ncontext of continuous-time linear quadratic regulators (LQRs) with entropy\nregularization. We provide the first theoretical proof of policy transfer for\ncontinuous-time RL, proving that a policy optimal for one LQR serves as a\nnear-optimal initialization for closely related LQRs, while preserving the\noriginal algorithm's convergence rate. Furthermore, we introduce a novel policy\nlearning algorithm for continuous-time LQRs that achieves global linear and\nlocal super-linear convergence. Our results demonstrate both theoretical\nguarantees and algorithmic benefits of transfer learning in continuous-time RL,\naddressing a gap in existing literature and extending prior work from discrete\nto continuous time settings.\n  As a byproduct of our analysis, we derive the stability of a class of\ncontinuous-time score-based diffusion models via their connection with LQRs.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4e3a\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7b56\u7565\u8fc1\u79fb\u7684\u7406\u8bba\u8bc1\u660e\uff0c\u8bc1\u660e\u4e86\u5728\u8fde\u7eed\u65f6\u95f4\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u4e2d\uff0c\u4ece\u4e00\u4e2a\u4efb\u52a1\u7684\u6700\u4f18\u7b56\u7565\u53ef\u4ee5\u4f5c\u4e3a\u76f8\u5173\u4efb\u52a1\u7684\u63a5\u8fd1\u6700\u4f18\u521d\u59cb\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u7b97\u6cd5\u7684\u6536\u655b\u901f\u7387\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u4ece\u5934\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u8fc1\u79fb\u5b66\u4e60\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5df2\u8bc1\u660e\u6210\u529f\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u5982\u4f55\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u5e94\u7528\u4e8e\u8fde\u7eed\u65f6\u95f4RL\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u7814\u7a76\u7b56\u7565\u8fc1\u79fb\u65b9\u6cd5\uff0c\u5728\u71b5\u6b63\u5219\u5316\u7684\u8fde\u7eed\u65f6\u95f4LQR\u6846\u67b6\u4e0b\uff0c\u63d0\u51fa\u65b0\u7684\u7b56\u7565\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b9e\u73b0\u5168\u5c40\u7ebf\u6027\u548c\u5c40\u90e8\u8d85\u7ebf\u6027\u6536\u655b\u3002", "result": "\u8bc1\u660e\u4e86\u4ece\u4e00\u4e2aLQR\u4efb\u52a1\u7684\u6700\u4f18\u7b56\u7565\u53ef\u4ee5\u4f5c\u4e3a\u76f8\u5173LQR\u4efb\u52a1\u7684\u63a5\u8fd1\u6700\u4f18\u521d\u59cb\u5316\uff0c\u5e76\u4fdd\u6301\u6536\u655b\u901f\u7387\u3002\u7b97\u6cd5\u5728\u8fde\u7eed\u65f6\u95f4LQR\u4e2d\u5b9e\u73b0\u4e86\u5168\u5c40\u7ebf\u6027\u548c\u5c40\u90e8\u8d85\u7ebf\u6027\u6536\u655b\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u8fde\u7eed\u65f6\u95f4RL\u4e2d\u8fc1\u79fb\u5b66\u4e60\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u5c06\u5148\u524d\u5de5\u4f5c\u4ece\u79bb\u6563\u65f6\u95f4\u6269\u5c55\u5230\u8fde\u7eed\u65f6\u95f4\u8bbe\u7f6e\uff0c\u5e76\u5efa\u7acb\u4e86\u8fde\u7eed\u65f6\u95f4\u5206\u6570\u6269\u6563\u6a21\u578b\u4e0eLQR\u7684\u7a33\u5b9a\u6027\u8054\u7cfb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15259", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15259", "abs": "https://arxiv.org/abs/2510.15259", "authors": ["Chenwei Tang", "Jingyu Xing", "Xinyu Liu", "Zizhou Wang", "Jiawei Du", "Liangli Zhen", "Jiancheng Lv"], "title": "Experience-Driven Exploration for Efficient API-Free AI Agents", "comment": null, "summary": "Most existing software lacks accessible Application Programming Interfaces\n(APIs), requiring agents to operate solely through pixel-based Graphical User\nInterfaces (GUIs). In this API-free setting, large language model (LLM)-based\nagents face severe efficiency bottlenecks: limited to local visual experiences,\nthey make myopic decisions and rely on inefficient trial-and-error, hindering\nboth skill acquisition and long-term planning. To address these challenges, we\npropose KG-Agent, an experience-driven learning framework that structures an\nagent's raw pixel-level interactions into a persistent State-Action Knowledge\nGraph (SA-KG). KG-Agent overcomes inefficient exploration by linking\nfunctionally similar but visually distinct GUI states, forming a rich\nneighborhood of experience that enables the agent to generalize from a diverse\nset of historical strategies. To support long-horizon reasoning, we design a\nhybrid intrinsic reward mechanism based on the graph topology, combining a\nstate value reward for exploiting known high-value pathways with a novelty\nreward that encourages targeted exploration. This approach decouples strategic\nplanning from pure discovery, allowing the agent to effectively value setup\nactions with delayed gratification. We evaluate KG-Agent in two complex,\nopen-ended GUI-based decision-making environments (Civilization V and Slay the\nSpire), demonstrating significant improvements in exploration efficiency and\nstrategic depth over the state-of-the-art methods.", "AI": {"tldr": "KG-Agent\u662f\u4e00\u4e2a\u7ecf\u9a8c\u9a71\u52a8\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u50cf\u7d20\u7ea7GUI\u4ea4\u4e92\u6784\u5efa\u4e3a\u72b6\u6001-\u52a8\u4f5c\u77e5\u8bc6\u56fe\u8c31\u6765\u89e3\u51b3API\u7f3a\u5931\u73af\u5883\u4e2d\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8f6f\u4ef6\u5927\u591a\u7f3a\u4e4f\u53ef\u8bbf\u95ee\u7684API\uff0c\u5bfc\u81f4LLM\u667a\u80fd\u4f53\u53ea\u80fd\u901a\u8fc7\u50cf\u7d20\u7ea7GUI\u64cd\u4f5c\uff0c\u9762\u4e34\u6548\u7387\u4f4e\u4e0b\u3001\u77ed\u89c6\u51b3\u7b56\u548c\u8bd5\u9519\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u72b6\u6001-\u52a8\u4f5c\u77e5\u8bc6\u56fe\u8c31(SA-KG)\uff0c\u5c06\u529f\u80fd\u76f8\u4f3c\u4f46\u89c6\u89c9\u4e0d\u540c\u7684GUI\u72b6\u6001\u8fde\u63a5\u8d77\u6765\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u56fe\u62d3\u6251\u7684\u6df7\u5408\u5185\u5728\u5956\u52b1\u673a\u5236\uff0c\u7ed3\u5408\u72b6\u6001\u4ef7\u503c\u5956\u52b1\u548c\u65b0\u9896\u6027\u5956\u52b1\u3002", "result": "\u5728Civilization V\u548cSlay the Spire\u4e24\u4e2a\u590d\u6742GUI\u51b3\u7b56\u73af\u5883\u4e2d\uff0cKG-Agent\u5728\u63a2\u7d22\u6548\u7387\u548c\u6218\u7565\u6df1\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "KG-Agent\u901a\u8fc7\u7ed3\u6784\u5316\u7ecf\u9a8c\u8868\u793a\u548c\u6df7\u5408\u5956\u52b1\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86API\u7f3a\u5931\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u548c\u957f\u671f\u89c4\u5212\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2510.15261", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15261", "abs": "https://arxiv.org/abs/2510.15261", "authors": ["Jitesh Jain", "Shubham Maheshwari", "Ning Yu", "Wen-mei Hwu", "Humphrey Shi"], "title": "AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory", "comment": "LAW 2025 Workshop at NeurIPS 2025. Work done from late 2023 to early\n  2024", "summary": "Riding on the success of LLMs with retrieval-augmented generation (RAG),\nthere has been a growing interest in augmenting agent systems with external\nmemory databases. However, the existing systems focus on storing text\ninformation in their memory, ignoring the importance of multimodal signals.\nMotivated by the multimodal nature of human memory, we present AUGUSTUS, a\nmultimodal agent system aligned with the ideas of human memory in cognitive\nscience. Technically, our system consists of 4 stages connected in a loop: (i)\nencode: understanding the inputs; (ii) store in memory: saving important\ninformation; (iii) retrieve: searching for relevant context from memory; and\n(iv) act: perform the task. Unlike existing systems that use vector databases,\nwe propose conceptualizing information into semantic tags and associating the\ntags with their context to store them in a graph-structured multimodal\ncontextual memory for efficient concept-driven retrieval. Our system\noutperforms the traditional multimodal RAG approach while being 3.5 times\nfaster for ImageNet classification and outperforming MemGPT on the MSC\nbenchmark.", "AI": {"tldr": "AUGUSTUS\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u4ee3\u7406\u7cfb\u7edf\uff0c\u91c7\u7528\u57fa\u4e8e\u8bed\u4e49\u6807\u7b7e\u548c\u56fe\u7ed3\u6784\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u591a\u6a21\u6001RAG\u65b9\u6cd5\uff0c\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u4e3b\u8981\u5b58\u50a8\u6587\u672c\u4fe1\u606f\uff0c\u5ffd\u89c6\u4e86\u591a\u6a21\u6001\u4fe1\u53f7\u7684\u91cd\u8981\u6027\u3002\u53d7\u4eba\u7c7b\u8bb0\u5fc6\u7684\u591a\u6a21\u6001\u7279\u6027\u542f\u53d1\uff0c\u4f5c\u8005\u5e0c\u671b\u6784\u5efa\u4e00\u4e2a\u7b26\u5408\u8ba4\u77e5\u79d1\u5b66\u4e2d\u4eba\u7c7b\u8bb0\u5fc6\u7406\u5ff5\u7684\u591a\u6a21\u6001\u4ee3\u7406\u7cfb\u7edf\u3002", "method": "\u7cfb\u7edf\u5305\u542b4\u4e2a\u5faa\u73af\u9636\u6bb5\uff1a\u7f16\u7801\uff08\u7406\u89e3\u8f93\u5165\uff09\u3001\u5b58\u50a8\u8bb0\u5fc6\uff08\u4fdd\u5b58\u91cd\u8981\u4fe1\u606f\uff09\u3001\u68c0\u7d22\uff08\u4ece\u8bb0\u5fc6\u4e2d\u641c\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\uff09\u3001\u884c\u52a8\uff08\u6267\u884c\u4efb\u52a1\uff09\u3002\u4e0d\u540c\u4e8e\u4f7f\u7528\u5411\u91cf\u6570\u636e\u5e93\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u8be5\u7cfb\u7edf\u5c06\u4fe1\u606f\u6982\u5ff5\u5316\u4e3a\u8bed\u4e49\u6807\u7b7e\uff0c\u5e76\u5c06\u6807\u7b7e\u4e0e\u5176\u4e0a\u4e0b\u6587\u5173\u8054\u5b58\u50a8\u5728\u56fe\u5f62\u7ed3\u6784\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u4e2d\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6982\u5ff5\u9a71\u52a8\u68c0\u7d22\u3002", "result": "\u5728ImageNet\u5206\u7c7b\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edf\u591a\u6a21\u6001RAG\u65b9\u6cd5\u5feb3.5\u500d\uff0c\u5728MSC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eMemGPT\u3002", "conclusion": "AUGUSTUS\u7cfb\u7edf\u901a\u8fc7\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u548c\u6982\u5ff5\u9a71\u52a8\u68c0\u7d22\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u8bb0\u5fc6\u5728\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.15306", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15306", "abs": "https://arxiv.org/abs/2510.15306", "authors": ["Kuang-Da Wang", "Zhao Wang", "Yotaro Shimose", "Wei-Yao Wang", "Shingo Takamatsu"], "title": "WebGen-V Bench: Structured Representation for Enhancing Visual Design in LLM-based Web Generation and Evaluation", "comment": null, "summary": "Witnessed by the recent advancements on leveraging LLM for coding and\nmultimodal understanding, we present WebGen-V, a new benchmark and framework\nfor instruction-to-HTML generation that enhances both data quality and\nevaluation granularity. WebGen-V contributes three key innovations: (1) an\nunbounded and extensible agentic crawling framework that continuously collects\nreal-world webpages and can leveraged to augment existing benchmarks; (2) a\nstructured, section-wise data representation that integrates metadata,\nlocalized UI screenshots, and JSON-formatted text and image assets, explicit\nalignment between content, layout, and visual components for detailed\nmultimodal supervision; and (3) a section-level multimodal evaluation protocol\naligning text, layout, and visuals for high-granularity assessment. Experiments\nwith state-of-the-art LLMs and ablation studies validate the effectiveness of\nour structured data and section-wise evaluation, as well as the contribution of\neach component. To the best of our knowledge, WebGen-V is the first work to\nenable high-granularity agentic crawling and evaluation for instruction-to-HTML\ngeneration, providing a unified pipeline from real-world data acquisition and\nwebpage generation to structured multimodal assessment.", "AI": {"tldr": "WebGen-V\u662f\u4e00\u4e2a\u7528\u4e8e\u6307\u4ee4\u5230HTML\u751f\u6210\u7684\u65b0\u57fa\u51c6\u548c\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u722c\u53d6\u3001\u7ed3\u6784\u5316\u6570\u636e\u8868\u793a\u548c\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u8bc4\u4f30\u6765\u63d0\u5347\u6570\u636e\u8d28\u91cf\u548c\u8bc4\u4f30\u7cbe\u5ea6\u3002", "motivation": "\u5229\u7528LLM\u5728\u7f16\u7801\u548c\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u7684\u8fdb\u5c55\uff0c\u89e3\u51b3\u73b0\u6709\u6307\u4ee4\u5230HTML\u751f\u6210\u4efb\u52a1\u4e2d\u6570\u636e\u8d28\u91cf\u4e0d\u8db3\u548c\u8bc4\u4f30\u7c92\u5ea6\u4e0d\u591f\u7cbe\u7ec6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u65e0\u8fb9\u754c\u53ef\u6269\u5c55\u7684\u667a\u80fd\u722c\u53d6\u6846\u67b6\u6536\u96c6\u771f\u5b9e\u7f51\u9875\uff1b2) \u7ed3\u6784\u5316\u5206\u8282\u6570\u636e\u8868\u793a\uff0c\u6574\u5408\u5143\u6570\u636e\u3001\u5c40\u90e8UI\u622a\u56fe\u548cJSON\u683c\u5f0f\u8d44\u6e90\uff1b3) \u5206\u8282\u7ea7\u591a\u6a21\u6001\u8bc4\u4f30\u534f\u8bae\uff0c\u5bf9\u9f50\u6587\u672c\u3001\u5e03\u5c40\u548c\u89c6\u89c9\u7ec4\u4ef6\u3002", "result": "\u901a\u8fc7\u6700\u5148\u8fdbLLM\u7684\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u7ed3\u6784\u5316\u6570\u636e\u548c\u5206\u8282\u8bc4\u4f30\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u5404\u7ec4\u4ef6\u5bf9\u6027\u80fd\u7684\u8d21\u732e\u3002", "conclusion": "WebGen-V\u662f\u9996\u4e2a\u5b9e\u73b0\u9ad8\u7c92\u5ea6\u667a\u80fd\u722c\u53d6\u548c\u8bc4\u4f30\u7684\u6307\u4ee4\u5230HTML\u751f\u6210\u5de5\u4f5c\uff0c\u63d0\u4f9b\u4e86\u4ece\u771f\u5b9e\u6570\u636e\u83b7\u53d6\u5230\u7ed3\u6784\u5316\u591a\u6a21\u6001\u8bc4\u4f30\u7684\u7edf\u4e00\u6d41\u7a0b\u3002", "topic": "swe benchmark"}}
{"id": "2510.15216", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15216", "abs": "https://arxiv.org/abs/2510.15216", "authors": ["Xuansheng Wu", "Xiaoman Pan", "Wenlin Yao", "Jianshu Chen"], "title": "Soundness-Aware Level: A Microscopic Signature that Predicts LLM Reasoning Potential", "comment": "Pre-print", "summary": "Reinforcement learning with verifiable rewards (RLVR) can elicit strong\nreasoning in large language models (LLMs), while their performance after RLVR\nvaries dramatically across different base models. This raises a fundamental\nquestion: what microscopic property of pre-trained models leads to this\nvariation? To investigate, we formalize reasoning as chains of Horn clauses\n(\"if-then\" rules) built from features extracted from the LLM's latent space via\ncross-layer sparse autoencoders (SAEs). We estimate the transition\nprobabilities between its features, and further categorize each rule by its\nsemantic soundness level (e.g., strict, plausible, noisy) with an LLM. Our key\ndiscovery is that high-potential models are inherently soundness-aware: their\ninternal probability distributions systematically shift across rules' soundness\nlevels, becoming highly distinct for \"strict\" versus \"noisy\" rules. In\ncontrast, weaker models are soundness-agnostic, collapsing to one distribution\nregardless of soundness levels. To quantify this, we introduce the\nSoundness-Aware Level (SAL), a microscopic metric using the Jensen-Shannon\nDivergence to measure the separation between these distributions. We show that\nSAL's predictions of post-RLVR reasoning performance follow a precise empirical\nlaw (R^2=0.87) across diverse model families (Qwen, Mistral, Llama, DeepSeek)\nand scales (0.5B-14B). This reveals that a model's reasoning potential is tied\nto its intrinsic, pre-trained ability to distinguish sound knowledge from\nunsound ones. These findings underscore the critical role of model pre-training\nin shaping reasoning and offer a practical metric grounded in the model's\ninternal mechanisms for selecting/designing stronger base models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60(RLVR)\u540e\u8868\u73b0\u5dee\u5f02\u7684\u5fae\u89c2\u539f\u56e0\uff1a\u9ad8\u6027\u80fd\u6a21\u578b\u5177\u6709\u5185\u5728\u7684\u5065\u5168\u6027\u611f\u77e5\u80fd\u529b\uff0c\u80fd\u533a\u5206\u4e25\u683c\u89c4\u5219\u4e0e\u566a\u58f0\u89c4\u5219\u7684\u6982\u7387\u5206\u5e03\uff0c\u800c\u5f31\u6a21\u578b\u5219\u65e0\u6cd5\u533a\u5206\u3002", "motivation": "\u63a2\u7a76\u4e3a\u4ec0\u4e48\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u5728RLVR\u540e\u7684\u63a8\u7406\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5bfb\u627e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5fae\u89c2\u7279\u6027\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u5173\u7cfb\u3002", "method": "\u5c06\u63a8\u7406\u5f62\u5f0f\u5316\u4e3aHorn\u5b50\u53e5\u94fe\uff0c\u4f7f\u7528\u8de8\u5c42\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u4f30\u8ba1\u7279\u5f81\u95f4\u8f6c\u79fb\u6982\u7387\uff0c\u5e76\u7528LLM\u5bf9\u89c4\u5219\u7684\u8bed\u4e49\u5065\u5168\u6027\u6c34\u5e73\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u53d1\u73b0\u9ad8\u6027\u80fd\u6a21\u578b\u5177\u6709\u5065\u5168\u6027\u611f\u77e5\u80fd\u529b\uff0c\u5176\u5185\u90e8\u6982\u7387\u5206\u5e03\u5728\u4e0d\u540c\u7684\u5065\u5168\u6027\u6c34\u5e73\u4e0a\u7cfb\u7edf\u6027\u5730\u53d8\u5316\uff0c\u800c\u5f31\u6a21\u578b\u5219\u65e0\u6cd5\u533a\u5206\u3002\u63d0\u51fa\u7684SAL\u6307\u6807\u4e0eRLVR\u540e\u63a8\u7406\u6027\u80fd\u5448\u5f3a\u76f8\u5173(R^2=0.87)\u3002", "conclusion": "\u6a21\u578b\u7684\u63a8\u7406\u6f5c\u529b\u4e0e\u5176\u5185\u5728\u533a\u5206\u5065\u5168\u77e5\u8bc6\u4e0e\u4e0d\u5065\u5168\u77e5\u8bc6\u7684\u80fd\u529b\u76f8\u5173\uff0c\u9884\u8bad\u7ec3\u5728\u5851\u9020\u63a8\u7406\u80fd\u529b\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "2510.15374", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15374", "abs": "https://arxiv.org/abs/2510.15374", "authors": ["Zezhong Tan", "Hang Gao", "Xinhong Ma", "Feng Zhang", "Ziqiang Dong"], "title": "Towards Flash Thinking via Decoupled Advantage Policy Optimization", "comment": null, "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable performance in\nsolving complex problems via supervised fine-tuning (SFT) and reinforcement\nlearning (RL). Although existing RL algorithms significantly enhance model\naccuracy, they still suffer from excessively lengthy responses and overthinking\nissues, resulting in increased inference latency and computational consumption,\nespecially for simple tasks that require minimal reasoning. To address this, we\npropose a novel RL framework, DEPO, to reduce inefficient reasoning for models.\nOur method mainly consists of three core components: (1) an innovative\nadvantage decoupled algorithm to guide model reduction of inefficient tokens;\n(2) a difficulty-aware length penalty to lower the overall length of model\nresponses; (3) an advantage clipping method to prevent bias in policy\noptimization. In our experiments, applied to DeepSeek-Distill-Qwen-7B and\nDeepSeek-Distill-Qwen-1.5B as base models, DEPO achieves a significant\nreduction in sequence length by 39% and reduces excessive reasoning paths in\ninefficient tokens, while outperforming the base model in overall accuracy.", "AI": {"tldr": "\u63d0\u51faDEPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u52bf\u89e3\u8026\u7b97\u6cd5\u3001\u96be\u5ea6\u611f\u77e5\u957f\u5ea6\u60e9\u7f5a\u548c\u4f18\u52bf\u88c1\u526a\u65b9\u6cd5\uff0c\u51cf\u5c11\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u4f4e\u6548token\u548c\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u7f29\u77ed\u54cd\u5e94\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u867d\u7136\u63d0\u5347\u4e86\u6a21\u578b\u51c6\u786e\u7387\uff0c\u4f46\u5b58\u5728\u54cd\u5e94\u8fc7\u957f\u548c\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6d88\u8017\u589e\u52a0\uff0c\u7279\u522b\u662f\u5bf9\u7b80\u5355\u4efb\u52a1\u800c\u8a00\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "DEPO\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4f18\u52bf\u89e3\u8026\u7b97\u6cd5\u6307\u5bfc\u51cf\u5c11\u4f4e\u6548token\u3001\u96be\u5ea6\u611f\u77e5\u957f\u5ea6\u60e9\u7f5a\u964d\u4f4e\u6574\u4f53\u54cd\u5e94\u957f\u5ea6\u3001\u4f18\u52bf\u88c1\u526a\u65b9\u6cd5\u9632\u6b62\u7b56\u7565\u4f18\u5316\u504f\u5dee\u3002", "result": "\u5728DeepSeek-Distill-Qwen-7B\u548c1.5B\u6a21\u578b\u4e0a\uff0cDEPO\u5b9e\u73b0\u4e86\u5e8f\u5217\u957f\u5ea6\u51cf\u5c1139%\uff0c\u51cf\u5c11\u4e86\u4f4e\u6548token\u4e2d\u7684\u8fc7\u5ea6\u63a8\u7406\u8def\u5f84\uff0c\u540c\u65f6\u5728\u6574\u4f53\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "DEPO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u8fc7\u5ea6\u63a8\u7406\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15455", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15455", "abs": "https://arxiv.org/abs/2510.15455", "authors": ["Gucongcong Fan", "Chaoyue Niu", "Chengfei Lyu", "Fan Wu", "Guihai Chen"], "title": "CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs", "comment": null, "summary": "Mobile agents rely on Large Language Models (LLMs) to plan and execute tasks\non smartphone user interfaces (UIs). While cloud-based LLMs achieve high task\naccuracy, they require uploading the full UI state at every step, exposing\nunnecessary and often irrelevant information. In contrast, local LLMs avoid UI\nuploads but suffer from limited capacity, resulting in lower task success\nrates. We propose $\\textbf{CORE}$, a $\\textbf{CO}$llaborative framework that\ncombines the strengths of cloud and local LLMs to $\\textbf{R}$educe UI\n$\\textbf{E}$xposure, while maintaining task accuracy for mobile agents. CORE\ncomprises three key components: (1) $\\textbf{Layout-aware block partitioning}$,\nwhich groups semantically related UI elements based on the XML screen\nhierarchy; (2) $\\textbf{Co-planning}$, where local and cloud LLMs\ncollaboratively identify the current sub-task; and (3)\n$\\textbf{Co-decision-making}$, where the local LLM ranks relevant UI blocks,\nand the cloud LLM selects specific UI elements within the top-ranked block.\nCORE further introduces a multi-round accumulation mechanism to mitigate local\nmisjudgment or limited context. Experiments across diverse mobile apps and\ntasks show that CORE reduces UI exposure by up to 55.6% while maintaining task\nsuccess rates slightly below cloud-only agents, effectively mitigating\nunnecessary privacy exposure to the cloud. The code is available at\nhttps://github.com/Entropy-Fighter/CORE.", "AI": {"tldr": "CORE\u662f\u4e00\u4e2a\u534f\u4f5c\u6846\u67b6\uff0c\u7ed3\u5408\u4e91\u7aef\u548c\u672c\u5730LLM\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u79fb\u52a8\u4ee3\u7406\u4efb\u52a1\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11UI\u66b4\u9732\u3002\u901a\u8fc7\u5e03\u5c40\u611f\u77e5\u5757\u5206\u533a\u3001\u534f\u540c\u89c4\u5212\u548c\u534f\u540c\u51b3\u7b56\uff0c\u5c06UI\u66b4\u9732\u51cf\u5c11\u9ad8\u8fbe55.6%\u3002", "motivation": "\u4e91\u7aefLLM\u9700\u8981\u4e0a\u4f20\u5b8c\u6574UI\u72b6\u6001\uff0c\u66b4\u9732\u4e0d\u5fc5\u8981\u4fe1\u606f\uff1b\u672c\u5730LLM\u5bb9\u91cf\u6709\u9650\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4f4e\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3002", "method": "\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u5e03\u5c40\u611f\u77e5\u5757\u5206\u533a\uff08\u57fa\u4e8eXML\u5c42\u6b21\u7ed3\u6784\u5206\u7ec4\u76f8\u5173UI\u5143\u7d20\uff09\u3001\u534f\u540c\u89c4\u5212\uff08\u672c\u5730\u548c\u4e91\u7aefLLM\u534f\u4f5c\u8bc6\u522b\u5b50\u4efb\u52a1\uff09\u3001\u534f\u540c\u51b3\u7b56\uff08\u672c\u5730LLM\u6392\u540d\u76f8\u5173UI\u5757\uff0c\u4e91\u7aefLLM\u5728\u9876\u7ea7\u5757\u4e2d\u9009\u62e9\u5177\u4f53\u5143\u7d20\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCORE\u5c06UI\u66b4\u9732\u51cf\u5c11\u9ad8\u8fbe55.6%\uff0c\u540c\u65f6\u4efb\u52a1\u6210\u529f\u7387\u7565\u4f4e\u4e8e\u7eaf\u4e91\u7aef\u4ee3\u7406\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4e0d\u5fc5\u8981\u7684\u4e91\u7aef\u9690\u79c1\u66b4\u9732\u3002", "conclusion": "CORE\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u4e91\u7aef\u548c\u672c\u5730LLM\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11UI\u66b4\u9732\uff0c\u4e3a\u79fb\u52a8\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.15414", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15414", "abs": "https://arxiv.org/abs/2510.15414", "authors": ["Huining Yuan", "Zelai Xu", "Zheyue Tan", "Xiangmin Yi", "Mo Guang", "Kaiwen Long", "Haojia Hui", "Boxun Li", "Xinlei Chen", "Bo Zhao", "Xiao-Ping Zhang", "Chao Yu", "Yu Wang"], "title": "MARS: Reinforcing Multi-Agent Reasoning of LLMs through Self-Play in Strategic Games", "comment": null, "summary": "Developing Large Language Models (LLMs) to cooperate and compete effectively\nwithin multi-agent systems is a critical step towards more advanced\nintelligence. While reinforcement learning (RL) has proven effective for\nenhancing reasoning in single-agent tasks, its extension to multi-turn,\nmulti-agent scenarios remains underexplored due to the challenges of\nlong-horizon credit assignment and agent-specific advantage estimation. To\naddress these challenges, we introduce MARS, an end-to-end RL framework that\nincentivizes Multi-Agent Reasoning of LLMs through Self-play in both\ncooperative and competitive games. MARS features a turn-level advantage\nestimator that aligns learning signals with each interaction for credit\nassignment, and an agent-specific advantage normalization to stabilize\nmulti-agent training. By learning with self-play across cooperative and\ncompetitive games, the MARS agent trained from Qwen3-4B develops strong\nstrategic abilities that generalize to held-out games with up to 28.7%\nperformance improvements. More importantly, the capability acquired through\nself-play generalizes beyond games, yielding consistent performance gains of\nmulti-agent systems in reasoning benchmarks. When integrated into leading\nmulti-agent systems, our MARS agent achieves significant performance gains of\n10.0% on AIME and 12.5% on GPQA-Diamond. These results establish end-to-end RL\ntraining with self-play in strategic games as a powerful approach for\ndeveloping generalizable multi-agent reasoning capabilities in LLMs. Our code\nand models are publicly available at https://github.com/thu-nics/MARS.", "AI": {"tldr": "MARS\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u535a\u5f08\u5728\u5408\u4f5c\u548c\u7ade\u4e89\u6e38\u620f\u4e2d\u6fc0\u52b1LLMs\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\uff0c\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u591a\u8f6e\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u957f\u65f6\u7a0b\u4fe1\u7528\u5206\u914d\u548c\u667a\u80fd\u4f53\u7279\u5b9a\u4f18\u52bf\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "method": "\u5f15\u5165MARS\u6846\u67b6\uff0c\u5305\u542b\u56de\u5408\u7ea7\u4f18\u52bf\u4f30\u8ba1\u5668\u548c\u667a\u80fd\u4f53\u7279\u5b9a\u4f18\u52bf\u5f52\u4e00\u5316\uff0c\u901a\u8fc7\u81ea\u535a\u5f08\u5728\u5408\u4f5c\u548c\u7ade\u4e89\u6e38\u620f\u4e2d\u8bad\u7ec3\u3002", "result": "\u5728Qwen3-4B\u4e0a\u8bad\u7ec3\u7684MARS\u667a\u80fd\u4f53\u5728\u4fdd\u7559\u6e38\u620f\u4e2d\u6027\u80fd\u63d0\u5347\u8fbe28.7%\uff0c\u5728AIME\u548cGPQA-Diamond\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u83b7\u5f9710.0%\u548c12.5%\u7684\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u7aef\u5230\u7aefRL\u8bad\u7ec3\u4e0e\u6218\u7565\u6e38\u620f\u81ea\u535a\u5f08\u662f\u5f00\u53d1LLMs\u901a\u7528\u591a\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15501", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15501", "abs": "https://arxiv.org/abs/2510.15501", "authors": ["Yao Huang", "Yitong Sun", "Yichi Zhang", "Ruochen Zhang", "Yinpeng Dong", "Xingxing Wei"], "title": "DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios", "comment": "28 pages, 17 figures, accepted by NeruIPS 2025", "summary": "Despite the remarkable advances of Large Language Models (LLMs) across\ndiverse cognitive tasks, the rapid enhancement of these capabilities also\nintroduces emergent deceptive behaviors that may induce severe risks in\nhigh-stakes deployments. More critically, the characterization of deception\nacross realistic real-world scenarios remains underexplored. To bridge this\ngap, we establish DeceptionBench, the first benchmark that systematically\nevaluates how deceptive tendencies manifest across different societal domains,\nwhat their intrinsic behavioral patterns are, and how extrinsic factors affect\nthem. Specifically, on the static count, the benchmark encompasses 150\nmeticulously designed scenarios in five domains, i.e., Economy, Healthcare,\nEducation, Social Interaction, and Entertainment, with over 1,000 samples,\nproviding sufficient empirical foundations for deception analysis. On the\nintrinsic dimension, we explore whether models exhibit self-interested egoistic\ntendencies or sycophantic behaviors that prioritize user appeasement. On the\nextrinsic dimension, we investigate how contextual factors modulate deceptive\noutputs under neutral conditions, reward-based incentivization, and coercive\npressures. Moreover, we incorporate sustained multi-turn interaction loops to\nconstruct a more realistic simulation of real-world feedback dynamics.\nExtensive experiments across LLMs and Large Reasoning Models (LRMs) reveal\ncritical vulnerabilities, particularly amplified deception under reinforcement\ndynamics, demonstrating that current models lack robust resistance to\nmanipulative contextual cues and the urgent need for advanced safeguards\nagainst various deception behaviors. Code and resources are publicly available\nat https://github.com/Aries-iai/DeceptionBench.", "AI": {"tldr": "DeceptionBench\u662f\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u4e0d\u540c\u793e\u4f1a\u9886\u57df\u4e2d\u6b3a\u9a97\u884c\u4e3a\u7684\u57fa\u51c6\uff0c\u6db5\u76d65\u4e2a\u9886\u57df150\u4e2a\u573a\u666f\uff0c\u63a2\u7d22\u5185\u5728\u884c\u4e3a\u6a21\u5f0f\u548c\u5916\u5728\u56e0\u7d20\u5f71\u54cd\uff0c\u63ed\u793a\u6a21\u578b\u5728\u5f3a\u5316\u52a8\u6001\u4e0b\u6b3a\u9a97\u884c\u4e3a\u52a0\u5267\u7684\u8106\u5f31\u6027\u3002", "motivation": "LLM\u80fd\u529b\u7684\u5feb\u901f\u63d0\u5347\u5e26\u6765\u4e86\u65b0\u5174\u7684\u6b3a\u9a97\u884c\u4e3a\uff0c\u5728\u5173\u952e\u90e8\u7f72\u4e2d\u53ef\u80fd\u5f15\u53d1\u4e25\u91cd\u98ce\u9669\uff0c\u800c\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6b3a\u9a97\u7279\u5f81\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u5efa\u7acb\u5305\u542b\u7ecf\u6d4e\u3001\u533b\u7597\u3001\u6559\u80b2\u3001\u793e\u4ea4\u4e92\u52a8\u548c\u5a31\u4e505\u4e2a\u9886\u57df150\u4e2a\u573a\u666f\u7684\u57fa\u51c6\uff0c\u8bc4\u4f30\u5185\u5728\u7684\u5229\u5df1\u4e3b\u4e49\u548c\u5949\u627f\u884c\u4e3a\uff0c\u4ee5\u53ca\u5916\u5728\u60c5\u5883\u56e0\u7d20\uff08\u4e2d\u6027\u6761\u4ef6\u3001\u5956\u52b1\u6fc0\u52b1\u3001\u5f3a\u5236\u538b\u529b\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u7eb3\u5165\u6301\u7eed\u591a\u8f6e\u4ea4\u4e92\u5faa\u73af\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0LLM\u548cLRM\u5b58\u5728\u5173\u952e\u8106\u5f31\u6027\uff0c\u7279\u522b\u662f\u5728\u5f3a\u5316\u52a8\u6001\u4e0b\u6b3a\u9a97\u884c\u4e3a\u52a0\u5267\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u7f3a\u4e4f\u5bf9\u64cd\u7eb5\u6027\u60c5\u5883\u7ebf\u7d22\u7684\u9c81\u68d2\u62b5\u6297\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5bf9\u5404\u79cd\u6b3a\u9a97\u884c\u4e3a\u7f3a\u4e4f\u6709\u6548\u9632\u62a4\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u5148\u8fdb\u7684\u5b89\u5168\u4fdd\u969c\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2510.15416", "categories": ["cs.AI", "68T05, 68T42", "I.2.11; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.15416", "abs": "https://arxiv.org/abs/2510.15416", "authors": ["Pavan C Shekar", "Ashwanth Krishnan"], "title": "Adaptive Minds: Empowering Agents with LoRA-as-Tools", "comment": "12 pages, 1 figure, 7 tables . Code available at:\n  https://github.com/qpiai/adaptive-minds", "summary": "We present Adaptive Minds, an agentic system that treats LoRA adapters as\ndomain-specific tools. Instead of relying on a single fine-tuned model or rigid\nrule-based routing, our approach empowers the base LLM itself to act as a\nsemantic router analyzing each query and dynamically selecting the most\nrelevant LoRA tool. This enables the agent to seamlessly switch between\ndifferent domain experts on demand. By combining the flexibility of multi-agent\norchestration with the efficiency of parameter-efficient fine-tuning, Adaptive\nMinds delivers accurate, specialized responses while preserving conversational\nability. The system is built with LangGraph for workflow management, supports\nboth API and web interfaces, and is fully open source, providing a scalable and\nextensible foundation for domain-adaptive AI assistance.", "AI": {"tldr": "Adaptive Minds\u662f\u4e00\u4e2a\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\uff0c\u5c06LoRA\u9002\u914d\u5668\u4f5c\u4e3a\u9886\u57df\u7279\u5b9a\u5de5\u5177\uff0c\u8ba9\u57fa\u7840LLM\u5145\u5f53\u8bed\u4e49\u8def\u7531\u5668\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u7684LoRA\u5de5\u5177\uff0c\u5b9e\u73b0\u6309\u9700\u5207\u6362\u9886\u57df\u4e13\u5bb6\u3002", "motivation": "\u89e3\u51b3\u5355\u4e00\u5fae\u8c03\u6a21\u578b\u6216\u57fa\u4e8e\u89c4\u5219\u8def\u7531\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u52a8\u6001\u5de5\u5177\u9009\u62e9\u5b9e\u73b0\u66f4\u7075\u6d3b\u548c\u51c6\u786e\u7684\u9886\u57df\u81ea\u9002\u5e94AI\u8f85\u52a9\u3002", "method": "\u4f7f\u7528LangGraph\u8fdb\u884c\u5de5\u4f5c\u6d41\u7ba1\u7406\uff0c\u57fa\u7840LLM\u5206\u6790\u67e5\u8be2\u5e76\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u7684LoRA\u9002\u914d\u5668\u4f5c\u4e3a\u9886\u57df\u4e13\u5bb6\u5de5\u5177\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u3001\u4e13\u4e1a\u5316\u7684\u54cd\u5e94\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u8bdd\u80fd\u529b\uff0c\u652f\u6301API\u548cWeb\u63a5\u53e3\uff0c\u5b8c\u5168\u5f00\u6e90\u3002", "conclusion": "Adaptive Minds\u7ed3\u5408\u4e86\u591a\u667a\u80fd\u4f53\u7f16\u6392\u7684\u7075\u6d3b\u6027\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u9ad8\u6548\u6027\uff0c\u4e3a\u9886\u57df\u81ea\u9002\u5e94AI\u8f85\u52a9\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.15522", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15522", "abs": "https://arxiv.org/abs/2510.15522", "authors": ["Jingcheng Deng", "Liang Pang", "Zihao Wei", "Shichen Xu", "Zenghao Duan", "Kun Xu", "Yang Song", "Huawei Shen", "Xueqi Cheng"], "title": "Latent Reasoning in LLMs as a Vocabulary-Space Superposition", "comment": null, "summary": "Large language models (LLMs) demonstrate strong reasoning abilities with\nchain-of-thought prompting, but explicit reasoning introduces substantial\ncomputational overhead. Recent work on latent reasoning reduces this cost by\nreasoning in latent space without explicit supervision, but performance drops\nsignificantly. Our preliminary experiments suggest that this degradation stems\nfrom the unstructured latent space, which makes fitting latent tokens\ndifficult. To address this, we restrict the latent space to the column space of\nthe LLM vocabulary, treating latent reasoning as a superposition over\nvocabulary probabilities. Once latent reasoning concludes, it collapses into an\neigenstate of explicit reasoning to yield the final answer. Based on this idea,\nwe propose Latent-SFT, a two-stage learning framework. In the first stage, we\ndesign two specialized attention masks to guide the Latent Token Encoder in\ngenerating latent tokens, allowing the LLM to produce the correct answer\nconditioned on them. In the second stage, the Latent Token Encoder is\ndiscarded, and the LLM is directly trained to generate these latent tokens\nautonomously for latent reasoning, optimized with KL and CE losses. Latent-SFT\nsets a new state of the art on GSM8k, matching explicit SFT performance while\ncutting reasoning chains by up to 4 times and outperforming prior latent\nmethods. On Math500 and AIME24, lexical probability-based latent reasoning also\nclearly surpasses hidden-state-based approaches. Our metrics of effective\ncompression rate and effective global parallelism further show that latent\nreasoning is both the compression of a single path and the superposition of\nmultiple paths.", "AI": {"tldr": "Latent-SFT\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8bcd\u6c47\u8868\u5217\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6f5c\u5728\u63a8\u7406\uff0c\u5c06\u63a8\u7406\u94fe\u538b\u7f29\u81f3\u591a4\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u663e\u5f0f\u63a8\u7406\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u663e\u5f0f\u63a8\u7406\u94fe\u5e26\u6765\u5927\u91cf\u8ba1\u7b97\u5f00\u9500\uff0c\u800c\u73b0\u6709\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u6027\u80fd\u4e0b\u964d\u6e90\u4e8e\u975e\u7ed3\u6784\u5316\u7684\u6f5c\u5728\u7a7a\u95f4\u96be\u4ee5\u62df\u5408\u6f5c\u5728\u6807\u8bb0\u3002", "method": "\u5c06\u6f5c\u5728\u7a7a\u95f4\u9650\u5236\u5728LLM\u8bcd\u6c47\u8868\u7684\u5217\u7a7a\u95f4\u4e2d\uff0c\u5c06\u6f5c\u5728\u63a8\u7406\u89c6\u4e3a\u8bcd\u6c47\u6982\u7387\u7684\u53e0\u52a0\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bbe\u8ba1\u4e13\u95e8\u6ce8\u610f\u529b\u63a9\u7801\u6307\u5bfc\u6f5c\u5728\u6807\u8bb0\u7f16\u7801\u5668\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4e22\u5f03\u7f16\u7801\u5668\uff0c\u76f4\u63a5\u8bad\u7ec3LLM\u81ea\u4e3b\u751f\u6210\u6f5c\u5728\u6807\u8bb0\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728GSM8k\u4e0a\u8fbe\u5230\u65b0\u7684SOTA\uff0c\u5339\u914d\u663e\u5f0fSFT\u6027\u80fd\u540c\u65f6\u5c06\u63a8\u7406\u94fe\u538b\u7f29\u81f3\u591a4\u500d\uff1b\u5728Math500\u548cAIME24\u4e0a\uff0c\u57fa\u4e8e\u8bcd\u6c47\u6982\u7387\u7684\u6f5c\u5728\u63a8\u7406\u660e\u663e\u4f18\u4e8e\u57fa\u4e8e\u9690\u85cf\u72b6\u6001\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6f5c\u5728\u63a8\u7406\u65e2\u662f\u5355\u4e00\u8def\u5f84\u7684\u538b\u7f29\uff0c\u4e5f\u662f\u591a\u8def\u5f84\u7684\u53e0\u52a0\uff0c\u6709\u6548\u538b\u7f29\u7387\u548c\u6709\u6548\u5168\u5c40\u5e76\u884c\u5ea6\u6307\u6807\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u70b9\u3002", "topic": "agent analysis"}}
{"id": "2510.15242", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15242", "abs": "https://arxiv.org/abs/2510.15242", "authors": ["Shengyu Feng", "Yun He", "Shuang Ma", "Beibin Li", "Yuanhao Xiong", "Vincent Li", "Karishma Mandyam", "Julian Katz-Samuels", "Shengjie Bi", "Licheng Yu", "Hejia Zhang", "Karthik Abinav Sankararaman", "Han Fang", "Riham Mansour", "Yiming Yang", "Manaal Faruqui"], "title": "Dual-Weighted Reinforcement Learning for Generative Preference Modeling", "comment": null, "summary": "Reinforcement learning (RL) has recently proven effective at scaling\nchain-of-thought (CoT) reasoning in large language models on tasks with\nverifiable answers. However, extending RL to more general non-verifiable tasks,\ntypically in the format of human preference pairs, remains both challenging and\nunderexplored. In this work, we propose Dual-Weighted Reinforcement Learning\n(DWRL), a new framework for preference modeling that integrates CoT reasoning\nwith the Bradley-Terry (BT) model via a dual-weighted RL objective that\npreserves preference-modeling inductive bias. DWRL approximates the\nmaximum-likelihood objective of the BT model with two complementary weights: an\ninstance-wise misalignment weight, which emphasizes under-trained pairs\nmisaligned with human preference, and a group-wise (self-normalized)\nconditional preference score, which promotes promising thoughts. In this paper,\nwe apply DWRL to preference modeling by training generative preference models\n(GPMs) to first generate a thought and then predict the human preference score.\nAcross multiple benchmarks and model scales (Llama3 and Qwen2.5), DWRL\nconsistently outperforms both GPM baselines and scalar models, while producing\ncoherent, interpretable thoughts. In summary, our results position DWRL as a\ngeneral framework for reasoning-enhanced preference learning beyond verifiable\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53cc\u6743\u91cd\u5f3a\u5316\u5b66\u4e60(DWRL)\u6846\u67b6\uff0c\u5c06\u601d\u7ef4\u94fe\u63a8\u7406\u4e0eBradley-Terry\u504f\u597d\u6a21\u578b\u7ed3\u5408\uff0c\u901a\u8fc7\u53cc\u6743\u91cdRL\u76ee\u6807\u89e3\u51b3\u975e\u53ef\u9a8c\u8bc1\u4efb\u52a1\u7684\u504f\u597d\u5efa\u6a21\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u5e94\u7528\u4e8e\u53ef\u9a8c\u8bc1\u7b54\u6848\u7684\u4efb\u52a1\uff0c\u4f46\u5728\u66f4\u666e\u904d\u7684\u975e\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff08\u5982\u4eba\u7c7b\u504f\u597d\u5bf9\uff09\u4e0a\u7684\u5e94\u7528\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u4e14\u7814\u7a76\u4e0d\u8db3\u3002", "method": "DWRL\u6846\u67b6\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u9519\u4f4d\u6743\u91cd\u548c\u7ec4\u7ea7\u6761\u4ef6\u504f\u597d\u5206\u6570\u4e24\u4e2a\u4e92\u8865\u6743\u91cd\uff0c\u8fd1\u4f3cBradley-Terry\u6a21\u578b\u7684\u6700\u5927\u4f3c\u7136\u76ee\u6807\uff0c\u8bad\u7ec3\u751f\u6210\u5f0f\u504f\u597d\u6a21\u578b(GPMs)\u9996\u5148\u751f\u6210\u601d\u7ef4\u7136\u540e\u9884\u6d4b\u4eba\u7c7b\u504f\u597d\u5206\u6570\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u89c4\u6a21(Llama3\u548cQwen2.5)\u4e0a\uff0cDWRL\u6301\u7eed\u4f18\u4e8eGPM\u57fa\u7ebf\u548c\u6807\u91cf\u6a21\u578b\uff0c\u540c\u65f6\u4ea7\u751f\u8fde\u8d2f\u3001\u53ef\u89e3\u91ca\u7684\u601d\u7ef4\u3002", "conclusion": "DWRL\u4f5c\u4e3a\u63a8\u7406\u589e\u5f3a\u504f\u597d\u5b66\u4e60\u7684\u901a\u7528\u6846\u67b6\uff0c\u53ef\u6269\u5c55\u5230\u53ef\u9a8c\u8bc1\u4efb\u52a1\u4e4b\u5916\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15260", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07, 68T50", "I.2.6; I.2.7; F.2.2"], "pdf": "https://arxiv.org/pdf/2510.15260", "abs": "https://arxiv.org/abs/2510.15260", "authors": ["Yangyang Li"], "title": "DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models", "comment": "Preprint. Under review at ICLR 2026. 11 pages, 2 figures", "summary": "Large language models are highly sensitive to prompt wording. However,\npopular automatic prompt search methods, including InstructZero, often degrade\nunder distribution shift and adversarial evaluation because they optimize\nexpected performance under a single evaluation distribution. Consequently,\nprompts that work in one setting frequently fail to transfer. To address this,\nDRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian\noptimization. Specifically, an f-divergence ball defines an ambiguity set\naround the evaluation distribution, and a robust acquisition rule maximizes\nworst-case expected utility while retaining the query efficiency of Bayesian\nsearch. Therefore, the search explicitly targets reliability under distribution\nshift rather than average behavior alone. Experiments follow the\ninstruction-induction protocol with matched query budgets across formality\nrewriting, code debugging, and translation. For example, on BIG-Bench\ninformative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to\napproximately 85-90%, yielding an absolute gain of about 25-30 points.\nMoreover, auto-debugging shows about +25-point gains under domain shift.\nMeanwhile, stable tasks such as cause-and-effect remain above 96%, indicating\nno loss on in-distribution cases. Furthermore, improvements are consistent\nacross divergence choices and decoding temperatures. Overall, DRO-InstructZero\nconnects distributionally robust optimization with prompt learning, offering a\nplug-and-play and general approach for reliable, transferable prompt alignment\nunder real-world uncertainty.", "AI": {"tldr": "DRO-InstructZero\u901a\u8fc7\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u6539\u8fdb\u96f6\u6837\u672c\u63d0\u793a\u4f18\u5316\uff0c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u67e5\u8be2\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u641c\u7d22\u65b9\u6cd5\u5728\u5206\u5e03\u504f\u79fb\u548c\u5bf9\u6297\u8bc4\u4f30\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u4ec5\u4f18\u5316\u5355\u4e00\u8bc4\u4f30\u5206\u5e03\u4e0b\u7684\u671f\u671b\u6027\u80fd\uff0c\u5bfc\u81f4\u63d0\u793a\u5728\u4e0d\u540c\u8bbe\u7f6e\u95f4\u96be\u4ee5\u8fc1\u79fb\u3002", "method": "\u5c06\u96f6\u6837\u672c\u63d0\u793a\u4f18\u5316\u5efa\u6a21\u4e3a\u9c81\u68d2\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u4f7f\u7528f-\u6563\u5ea6\u7403\u5b9a\u4e49\u8bc4\u4f30\u5206\u5e03\u5468\u56f4\u7684\u6a21\u7cca\u96c6\uff0c\u901a\u8fc7\u9c81\u68d2\u91c7\u96c6\u89c4\u5219\u6700\u5927\u5316\u6700\u574f\u60c5\u51b5\u671f\u671b\u6548\u7528\u3002", "result": "\u5728\u5f62\u5f0f\u5316\u6539\u5199\u3001\u4ee3\u7801\u8c03\u8bd5\u548c\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u5982BIG-Bench\u5f62\u5f0f\u5316\u6539\u5199\u4ece61.3%\u63d0\u5347\u81f385-90%\uff0c\u4ee3\u7801\u8c03\u8bd5\u5728\u57df\u504f\u79fb\u4e0b\u63d0\u5347\u7ea625\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "DRO-InstructZero\u5c06\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u4e0e\u63d0\u793a\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u53ef\u9760\u3001\u53ef\u8fc1\u79fb\u63d0\u793a\u5bf9\u9f50\u63d0\u4f9b\u4e86\u5373\u63d2\u5373\u7528\u7684\u901a\u7528\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.15624", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.15624", "abs": "https://arxiv.org/abs/2510.15624", "authors": ["Ed Li", "Junyu Ren", "Xintian Pan", "Cat Yan", "Chuanhao Li", "Dirk Bergemann", "Zhuoran Yang"], "title": "Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation", "comment": "37 pages, 5 figures. Code: https://github.com/ltjed/freephdlabor", "summary": "The automation of scientific discovery represents a critical milestone in\nArtificial Intelligence (AI) research. However, existing agentic systems for\nscience suffer from two fundamental limitations: rigid, pre-programmed\nworkflows that cannot adapt to intermediate findings, and inadequate context\nmanagement that hinders long-horizon research. We present\n\\texttt{freephdlabor}, an open-source multiagent framework featuring\n\\textit{fully dynamic workflows} determined by real-time agent reasoning and a\n\\coloremph{\\textit{modular architecture}} enabling seamless customization --\nusers can modify, add, or remove agents to address domain-specific\nrequirements. The framework provides comprehensive infrastructure including\n\\textit{automatic context compaction}, \\textit{workspace-based communication}\nto prevent information degradation, \\textit{memory persistence} across\nsessions, and \\textit{non-blocking human intervention} mechanisms. These\nfeatures collectively transform automated research from isolated, single-run\nattempts into \\textit{continual research programs} that build systematically on\nprior explorations and incorporate human feedback. By providing both the\narchitectural principles and practical implementation for building customizable\nco-scientist systems, this work aims to facilitate broader adoption of\nautomated research across scientific domains, enabling practitioners to deploy\ninteractive multiagent systems that autonomously conduct end-to-end research --\nfrom ideation through experimentation to publication-ready manuscripts.", "AI": {"tldr": "\u63d0\u51fa\u4e86freephdlabor\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5177\u6709\u5b8c\u5168\u52a8\u6001\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u6a21\u5757\u5316\u67b6\u6784\uff0c\u652f\u6301\u81ea\u52a8\u4e0a\u4e0b\u6587\u538b\u7f29\u3001\u5de5\u4f5c\u533a\u901a\u4fe1\u3001\u5185\u5b58\u6301\u4e45\u5316\u548c\u975e\u963b\u585e\u4eba\u5de5\u5e72\u9884\uff0c\u65e8\u5728\u5b9e\u73b0\u6301\u7eed\u7684\u7814\u7a76\u7a0b\u5e8f\u3002", "motivation": "\u73b0\u6709\u7684\u79d1\u5b66\u53d1\u73b0\u81ea\u52a8\u5316\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u9650\u5236\uff1a\u50f5\u5316\u7684\u9884\u7f16\u7a0b\u5de5\u4f5c\u6d41\u7a0b\u65e0\u6cd5\u9002\u5e94\u4e2d\u95f4\u53d1\u73b0\uff0c\u4ee5\u53ca\u4e0d\u5145\u5206\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u963b\u788d\u4e86\u957f\u671f\u7814\u7a76\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u667a\u80fd\u4f53\u63a8\u7406\u786e\u5b9a\u5b8c\u5168\u52a8\u6001\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6a21\u5757\u5316\u67b6\u6784\u5141\u8bb8\u7528\u6237\u4fee\u6539\u3001\u6dfb\u52a0\u6216\u5220\u9664\u667a\u80fd\u4f53\u4ee5\u6ee1\u8db3\u7279\u5b9a\u9886\u57df\u9700\u6c42\uff0c\u5e76\u63d0\u4f9b\u81ea\u52a8\u4e0a\u4e0b\u6587\u538b\u7f29\u3001\u5de5\u4f5c\u533a\u901a\u4fe1\u3001\u5185\u5b58\u6301\u4e45\u5316\u548c\u975e\u963b\u585e\u4eba\u5de5\u5e72\u9884\u7b49\u57fa\u7840\u8bbe\u65bd\u3002", "result": "\u8be5\u6846\u67b6\u5c06\u81ea\u52a8\u5316\u7814\u7a76\u4ece\u5b64\u7acb\u7684\u5355\u6b21\u5c1d\u8bd5\u8f6c\u53d8\u4e3a\u6301\u7eed\u7684\u7814\u7a76\u7a0b\u5e8f\uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u57fa\u4e8e\u5148\u524d\u63a2\u7d22\u5e76\u6574\u5408\u4eba\u7c7b\u53cd\u9988\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u6784\u5efa\u53ef\u5b9a\u5236\u5171\u540c\u79d1\u5b66\u5bb6\u7cfb\u7edf\u7684\u67b6\u6784\u539f\u5219\u548c\u5b9e\u9645\u5b9e\u73b0\uff0c\u4fc3\u8fdb\u81ea\u52a8\u5316\u7814\u7a76\u5728\u79d1\u5b66\u9886\u57df\u7684\u66f4\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u90e8\u7f72\u4ea4\u4e92\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6765\u81ea\u4e3b\u8fdb\u884c\u7aef\u5230\u7aef\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2510.15739", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.15739", "abs": "https://arxiv.org/abs/2510.15739", "authors": ["Lorenzo Satta Chiris", "Ayush Mishra"], "title": "AURA: An Agent Autonomy Risk Assessment Framework", "comment": "10 pages, 2 figures. Submitted for open-access preprint on arXiv.\n  Based on the AAMAS 2026 paper template", "summary": "As autonomous agentic AI systems see increasing adoption across\norganisations, persistent challenges in alignment, governance, and risk\nmanagement threaten to impede deployment at scale. We present AURA (Agent\naUtonomy Risk Assessment), a unified framework designed to detect, quantify,\nand mitigate risks arising from agentic AI. Building on recent research and\npractical deployments, AURA introduces a gamma-based risk scoring methodology\nthat balances risk assessment accuracy with computational efficiency and\npractical considerations. AURA provides an interactive process to score,\nevaluate and mitigate the risks of running one or multiple AI Agents,\nsynchronously or asynchronously (autonomously). The framework is engineered for\nHuman-in-the-Loop (HITL) oversight and presents Agent-to-Human (A2H)\ncommunication mechanisms, allowing for seamless integration with agentic\nsystems for autonomous self-assessment, rendering it interoperable with\nestablished protocols (MCP and A2A) and tools. AURA supports a responsible and\ntransparent adoption of agentic AI and provides robust risk detection and\nmitigation while balancing computational resources, positioning it as a\ncritical enabler for large-scale, governable agentic AI in enterprise\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86AURA\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u3001\u91cf\u5316\u548c\u51cf\u8f7b\u81ea\u4e3bAI\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u98ce\u9669\uff0c\u901a\u8fc7gamma\u98ce\u9669\u8bc4\u5206\u65b9\u6cd5\u5e73\u8861\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u4eba\u673a\u534f\u540c\u76d1\u7763\u3002", "motivation": "\u968f\u7740\u81ea\u4e3bAI\u4ee3\u7406\u7cfb\u7edf\u5728\u7ec4\u7ec7\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bf9\u9f50\u3001\u6cbb\u7406\u548c\u98ce\u9669\u7ba1\u7406\u65b9\u9762\u7684\u6301\u7eed\u6311\u6218\u963b\u788d\u4e86\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u9700\u8981\u7edf\u4e00\u7684\u6846\u67b6\u6765\u5e94\u5bf9\u8fd9\u4e9b\u98ce\u9669\u3002", "method": "AURA\u91c7\u7528\u57fa\u4e8egamma\u7684\u98ce\u9669\u8bc4\u5206\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u6d41\u7a0b\u6765\u8bc4\u5206\u3001\u8bc4\u4f30\u548c\u51cf\u8f7bAI\u4ee3\u7406\u98ce\u9669\uff0c\u652f\u6301\u4eba\u673a\u534f\u540c\u76d1\u7763\u548c\u4ee3\u7406\u5230\u4eba\u901a\u4fe1\u673a\u5236\uff0c\u4e0e\u73b0\u6709\u534f\u8bae\u548c\u5de5\u5177\u517c\u5bb9\u3002", "result": "AURA\u6846\u67b6\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u51cf\u8f7b\u81ea\u4e3bAI\u4ee3\u7406\u7cfb\u7edf\u7684\u98ce\u9669\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u4f9b\u5f3a\u5927\u7684\u98ce\u9669\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "AURA\u652f\u6301\u8d1f\u8d23\u4efb\u548c\u900f\u660e\u7684AI\u4ee3\u7406\u91c7\u7528\uff0c\u4e3a\u4f01\u4e1a\u5728\u89c4\u6a21\u5316\u3001\u53ef\u6cbb\u7406\u7684AI\u4ee3\u7406\u90e8\u7f72\u4e2d\u63d0\u4f9b\u4e86\u5173\u952e\u652f\u6491\u3002", "topic": "agent analysis"}}
{"id": "2510.15614", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15614", "abs": "https://arxiv.org/abs/2510.15614", "authors": ["Tingting Chen", "Beibei Lin", "Zifeng Yuan", "Qiran Zou", "Hongyu He", "Yew-Soon Ong", "Anirudh Goyal", "Dianbo Liu"], "title": "HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination", "comment": null, "summary": "As language models are increasingly used in scientific workflows, evaluating\ntheir ability to propose sets of explanations-not just a single correct\nanswer-becomes critical. Many scientific problems are underdetermined:\nmultiple, mechanistically distinct hypotheses are consistent with the same\nobservations. We introduce HypoSpace, a diagnostic suite that treats LLMs as\nsamplers of finite hypothesis sets and measures three complementary indicators:\nValidity (precision of proposals consistent with observations), Uniqueness\n(non-redundancy among proposals), and Recovery (coverage of the enumerated\nadmissible set). We instantiate HypoSpace in three structured domains with\ndeterministic validators and exactly enumerated hypothesis spaces: (i) causal\ngraphs from perturbations, (ii) gravity-constrained 3D voxel reconstruction\nfrom top-down projections, and (iii) Boolean genetic interactions. Across\ninstruction-tuned and reasoning-focused models, Validity often remains high\nwhile Uniqueness and Recovery degrade as the admissible space grows, revealing\nmode collapse that is invisible to correctness-only metrics. HypoSpace offers a\ncontrolled probe-rather than a leaderboard-for methods that explicitly explore\nand cover admissible explanation spaces. Code is available at:\nhttps://github.com/CTT-Pavilion/_HypoSpace.", "AI": {"tldr": "HypoSpace\u662f\u4e00\u4e2a\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u79cd\u89e3\u91ca\u80fd\u529b\u7684\u8bca\u65ad\u5957\u4ef6\uff0c\u5728\u4e09\u4e2a\u7ed3\u6784\u5316\u9886\u57df\u4e2d\u6d4b\u91cf\u6709\u6548\u6027\u3001\u72ec\u7279\u6027\u548c\u8986\u76d6\u7387\u6307\u6807\uff0c\u53d1\u73b0\u968f\u7740\u53ef\u63a5\u53d7\u7a7a\u95f4\u589e\u5927\uff0c\u6a21\u578b\u4f1a\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\u73b0\u8c61\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u8bc4\u4f30\u5b83\u4eec\u63d0\u51fa\u591a\u79cd\u89e3\u91ca\u96c6\u5408\uff08\u800c\u975e\u5355\u4e00\u6b63\u786e\u7b54\u6848\uff09\u7684\u80fd\u529b\uff0c\u56e0\u4e3a\u8bb8\u591a\u79d1\u5b66\u95ee\u9898\u5b58\u5728\u591a\u79cd\u673a\u5236\u4e0d\u540c\u7684\u5408\u7406\u89e3\u91ca\u3002", "method": "\u5c06LLMs\u89c6\u4e3a\u6709\u9650\u5047\u8bbe\u96c6\u7684\u91c7\u6837\u5668\uff0c\u5728\u4e09\u4e2a\u5177\u6709\u786e\u5b9a\u6027\u9a8c\u8bc1\u5668\u548c\u7cbe\u786e\u679a\u4e3e\u5047\u8bbe\u7a7a\u95f4\u7684\u7ed3\u6784\u5316\u9886\u57df\uff08\u56e0\u679c\u56fe\u3001\u91cd\u529b\u7ea6\u675f3D\u4f53\u7d20\u91cd\u5efa\u3001\u5e03\u5c14\u9057\u4f20\u4ea4\u4e92\uff09\u4e2d\u6d4b\u91cf\u6709\u6548\u6027\u3001\u72ec\u7279\u6027\u548c\u8986\u76d6\u7387\u4e09\u4e2a\u4e92\u8865\u6307\u6807\u3002", "result": "\u5728\u6307\u4ee4\u8c03\u4f18\u548c\u63a8\u7406\u5bfc\u5411\u6a21\u578b\u4e2d\uff0c\u6709\u6548\u6027\u901a\u5e38\u4fdd\u6301\u8f83\u9ad8\uff0c\u4f46\u968f\u7740\u53ef\u63a5\u53d7\u7a7a\u95f4\u589e\u5927\uff0c\u72ec\u7279\u6027\u548c\u8986\u76d6\u7387\u4f1a\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u4ec5\u57fa\u4e8e\u6b63\u786e\u6027\u6307\u6807\u65e0\u6cd5\u53d1\u73b0\u7684\u6a21\u5f0f\u5d29\u6e83\u73b0\u8c61\u3002", "conclusion": "HypoSpace\u4e3a\u663e\u5f0f\u63a2\u7d22\u548c\u8986\u76d6\u53ef\u63a5\u53d7\u89e3\u91ca\u7a7a\u95f4\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53d7\u63a7\u63a2\u9488\uff0c\u800c\u975e\u6392\u884c\u699c\u3002", "topic": "agent analysis"}}
{"id": "2510.15862", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15862", "abs": "https://arxiv.org/abs/2510.15862", "authors": ["Yi Wan", "Jiuqi Wang", "Liam Li", "Jinsong Liu", "Ruihao Zhu", "Zheqing Zhu"], "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold", "comment": null, "summary": "Tool-augmented large language models (LLMs) are emerging as deep research\nagents, systems that decompose complex queries, retrieve external evidence, and\nsynthesize grounded responses. Yet current agents remain limited by shallow\nretrieval, weak alignment metrics, and brittle tool-use behavior. We introduce\nPokeeResearch-7B, a 7B-parameter deep research agent built under a unified\nreinforcement learning framework for robustness, alignment, and scalability.\nPokeeResearch-7B is trained by an annotation-free Reinforcement Learning from\nAI Feedback (RLAIF) framework to optimize policies using LLM-based reward\nsignals that capture factual accuracy, citation faithfulness, and instruction\nadherence. A chain-of-thought-driven multi-call reasoning scaffold further\nenhances robustness through self-verification and adaptive recovery from tool\nfailures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves\nstate-of-the-art performance among 7B-scale deep research agents. This\nhighlights that careful reinforcement learning and reasoning design can produce\nefficient, resilient, and research-grade AI agents. The model and inference\ncode is open-sourced under MIT license at\nhttps://github.com/Pokee-AI/PokeeResearchOSS.", "AI": {"tldr": "PokeeResearch-7B\u662f\u4e00\u4e2a7B\u53c2\u6570\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6784\u5efa\uff0c\u572810\u4e2a\u6d41\u884c\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52307B\u89c4\u6a21\u4ee3\u7406\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5de5\u5177\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u4ee3\u7406\u5b58\u5728\u68c0\u7d22\u6d45\u5c42\u3001\u5bf9\u9f50\u6307\u6807\u5f31\u548c\u5de5\u5177\u4f7f\u7528\u884c\u4e3a\u8106\u5f31\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u5bf9\u9f50\u826f\u597d\u4e14\u53ef\u6269\u5c55\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u3002", "method": "\u91c7\u7528\u65e0\u6807\u6ce8\u7684AI\u53cd\u9988\u5f3a\u5316\u5b66\u4e60(RLAIF)\u6846\u67b6\u8bad\u7ec3\uff0c\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u5956\u52b1\u4fe1\u53f7\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u601d\u7ef4\u94fe\u9a71\u52a8\u7684\u591a\u8c03\u7528\u63a8\u7406\u6846\u67b6\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u572810\u4e2a\u6d41\u884c\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPokeeResearch-7B\u57287B\u89c4\u6a21\u4ee3\u7406\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u63a8\u7406\u6846\u67b6\u53ef\u4ee5\u4ea7\u751f\u9ad8\u6548\u3001\u6709\u5f39\u6027\u4e14\u8fbe\u5230\u7814\u7a76\u7ea7\u522b\u7684AI\u4ee3\u7406\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15382", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.15382", "abs": "https://arxiv.org/abs/2510.15382", "authors": ["Kexin Zheng", "Lauriane Teyssier", "Yinan Zheng", "Yu Luo", "Xiayuan Zhan"], "title": "Towards Robust Zero-Shot Reinforcement Learning", "comment": "Neurips 2025, 36 pages, 18 figures", "summary": "The recent development of zero-shot reinforcement learning (RL) has opened a\nnew avenue for learning pre-trained generalist policies that can adapt to\narbitrary new tasks in a zero-shot manner. While the popular Forward-Backward\nrepresentations (FB) and related methods have shown promise in zero-shot RL, we\nempirically found that their modeling lacks expressivity and that extrapolation\nerrors caused by out-of-distribution (OOD) actions during offline learning\nsometimes lead to biased representations, ultimately resulting in suboptimal\nperformance. To address these issues, we propose Behavior-REgularizEd Zero-shot\nRL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that\nsimultaneously enhances learning stability, policy extraction capability, and\nrepresentation learning quality. BREEZE introduces behavioral regularization in\nzero-shot RL policy learning, transforming policy optimization into a stable\nin-sample learning paradigm. Additionally, BREEZE extracts the policy using a\ntask-conditioned diffusion model, enabling the generation of high-quality and\nmultimodal action distributions in zero-shot RL settings. Moreover, BREEZE\nemploys expressive attention-based architectures for representation modeling to\ncapture the complex relationships between environmental dynamics. Extensive\nexperiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best\nor near-the-best performance while exhibiting superior robustness compared to\nprior offline zero-shot RL methods. The official implementation is available\nat: https://github.com/Whiterrrrr/BREEZE.", "AI": {"tldr": "\u63d0\u51fa\u4e86BREEZE\u6846\u67b6\uff0c\u901a\u8fc7\u884c\u4e3a\u6b63\u5219\u5316\u3001\u6269\u6563\u6a21\u578b\u7b56\u7565\u63d0\u53d6\u548c\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u793a\u5b66\u4e60\u8868\u8fbe\u6027\u4e0d\u8db3\u548cOOD\u52a8\u4f5c\u5916\u63a8\u8bef\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672cRL\u65b9\u6cd5\uff08\u5982FB\u8868\u793a\uff09\u5b58\u5728\u8868\u793a\u8868\u8fbe\u6027\u4e0d\u8db3\u548c\u79bb\u7ebf\u5b66\u4e60\u4e2d\u7684OOD\u52a8\u4f5c\u5916\u63a8\u8bef\u5dee\u95ee\u9898\uff0c\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002", "method": "BREEZE\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u884c\u4e3a\u6b63\u5219\u5316\u5c06\u7b56\u7565\u4f18\u5316\u8f6c\u5316\u4e3a\u7a33\u5b9a\u6837\u672c\u5185\u5b66\u4e60\uff1b\u4efb\u52a1\u6761\u4ef6\u6269\u6563\u6a21\u578b\u63d0\u53d6\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u7b56\u7565\uff1b\u6ce8\u610f\u529b\u67b6\u6784\u589e\u5f3a\u8868\u793a\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u5728ExORL\u548cD4RL Kitchen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBREEZE\u8fbe\u5230\u6700\u4f73\u6216\u63a5\u8fd1\u6700\u4f73\u6027\u80fd\uff0c\u5e76\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u79bb\u7ebf\u96f6\u6837\u672cRL\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "BREEZE\u901a\u8fc7\u589e\u5f3a\u5b66\u4e60\u7a33\u5b9a\u6027\u3001\u7b56\u7565\u63d0\u53d6\u80fd\u529b\u548c\u8868\u793a\u5b66\u4e60\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15746", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15746", "abs": "https://arxiv.org/abs/2510.15746", "authors": ["Gao Yang", "Yuhang Liu", "Siyu Miao", "Xinyue Liang", "Zhengyang Liu", "Heyan Huang"], "title": "LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned Evaluation", "comment": null, "summary": "Ideal or real - that is the question.In this work, we explore whether\nprinciples from game theory can be effectively applied to the evaluation of\nlarge language models (LLMs). This inquiry is motivated by the growing\ninadequacy of conventional evaluation practices, which often rely on\nfixed-format tasks with reference answers and struggle to capture the nuanced,\nsubjective, and open-ended nature of modern LLM behavior. To address these\nchallenges, we propose a novel alternative: automatic mutual evaluation, where\nLLMs assess each other's output through self-play and peer review. These peer\nassessments are then systematically compared with human voting behavior to\nevaluate their alignment with human judgment. Our framework incorporates\ngame-theoretic voting algorithms to aggregate peer reviews, enabling a\nprincipled investigation into whether model-generated rankings reflect human\npreferences. Empirical results reveal both convergences and divergences between\ntheoretical predictions and human evaluations, offering valuable insights into\nthe promises and limitations of mutual evaluation. To the best of our\nknowledge, this is the first work to jointly integrate mutual evaluation,\ngame-theoretic aggregation, and human-grounded validation for evaluating the\ncapabilities of LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u6295\u7968\u7b97\u6cd5\u7684LLM\u81ea\u52a8\u4e92\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u535a\u5f08\u548c\u540c\u884c\u8bc4\u5ba1\u8ba9LLM\u76f8\u4e92\u8bc4\u4f30\uff0c\u5e76\u4e0e\u4eba\u7c7b\u6295\u7968\u884c\u4e3a\u8fdb\u884c\u5bf9\u6bd4\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u683c\u5f0f\u4efb\u52a1\u548c\u53c2\u8003\u7b54\u6848\uff0c\u96be\u4ee5\u6355\u6349\u73b0\u4ee3LLM\u7684\u7ec6\u5fae\u3001\u4e3b\u89c2\u548c\u5f00\u653e\u5f0f\u884c\u4e3a\u7279\u5f81\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u4e92\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8ba9LLM\u901a\u8fc7\u81ea\u535a\u5f08\u548c\u540c\u884c\u8bc4\u5ba1\u76f8\u4e92\u8bc4\u4f30\u8f93\u51fa\uff0c\u4f7f\u7528\u535a\u5f08\u8bba\u6295\u7968\u7b97\u6cd5\u805a\u5408\u540c\u884c\u8bc4\u5ba1\u7ed3\u679c\uff0c\u5e76\u4e0e\u4eba\u7c7b\u5224\u65ad\u8fdb\u884c\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u7406\u8bba\u9884\u6d4b\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u4e4b\u95f4\u5b58\u5728\u6536\u655b\u548c\u5206\u6b67\uff0c\u63ed\u793a\u4e86\u4e92\u8bc4\u4f30\u65b9\u6cd5\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5c06\u4e92\u8bc4\u4f30\u3001\u535a\u5f08\u8bba\u805a\u5408\u548c\u4eba\u7c7b\u9a8c\u8bc1\u76f8\u7ed3\u5408\u7684\u5de5\u4f5c\uff0c\u4e3aLLM\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.15388", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15388", "abs": "https://arxiv.org/abs/2510.15388", "authors": ["Mingyang Sun", "Pengxiang Ding", "Weinan Zhang", "Donglin Wang"], "title": "Iterative Refinement of Flow Policies in Probability Space for Online Reinforcement Learning", "comment": null, "summary": "While behavior cloning with flow/diffusion policies excels at learning\ncomplex skills from demonstrations, it remains vulnerable to distributional\nshift, and standard RL methods struggle to fine-tune these models due to their\niterative inference process and the limitations of existing workarounds. In\nthis work, we introduce the Stepwise Flow Policy (SWFP) framework, founded on\nthe key insight that discretizing the flow matching inference process via a\nfixed-step Euler scheme inherently aligns it with the variational\nJordan-Kinderlehrer-Otto (JKO) principle from optimal transport. SWFP\ndecomposes the global flow into a sequence of small, incremental\ntransformations between proximate distributions. Each step corresponds to a JKO\nupdate, regularizing policy changes to stay near the previous iterate and\nensuring stable online adaptation with entropic regularization. This\ndecomposition yields an efficient algorithm that fine-tunes pre-trained flows\nvia a cascade of small flow blocks, offering significant advantages:\nsimpler/faster training of sub-models, reduced computational/memory costs, and\nprovable stability grounded in Wasserstein trust regions. Comprehensive\nexperiments demonstrate SWFP's enhanced stability, efficiency, and superior\nadaptation performance across diverse robotic control benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86Stepwise Flow Policy (SWFP)\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u6d41\u5339\u914d\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c06\u5176\u4e0e\u6700\u4f18\u4f20\u8f93\u7684JKO\u539f\u7406\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u9884\u8bad\u7ec3\u6d41\u6a21\u578b\u7684\u9ad8\u6548\u7a33\u5b9a\u5fae\u8c03\u3002", "motivation": "\u884c\u4e3a\u514b\u9686\u4e2d\u7684\u6d41/\u6269\u6563\u7b56\u7565\u867d\u7136\u64c5\u957f\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u590d\u6742\u6280\u80fd\uff0c\u4f46\u5bf9\u5206\u5e03\u504f\u79fb\u5f88\u8106\u5f31\uff0c\u6807\u51c6RL\u65b9\u6cd5\u96be\u4ee5\u5fae\u8c03\u8fd9\u4e9b\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u56fa\u5b9a\u6b65\u957f\u6b27\u62c9\u65b9\u6848\u79bb\u6563\u5316\u6d41\u5339\u914d\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u5c0f\u589e\u91cf\u53d8\u6362\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u5bf9\u5e94JKO\u66f4\u65b0\uff0c\u901a\u8fc7\u71b5\u6b63\u5219\u5316\u786e\u4fdd\u7a33\u5b9a\u5728\u7ebf\u9002\u5e94\u3002", "result": "\u5728\u591a\u79cd\u673a\u5668\u4eba\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSWFP\u5c55\u73b0\u51fa\u589e\u5f3a\u7684\u7a33\u5b9a\u6027\u3001\u6548\u7387\u548c\u4f18\u8d8a\u7684\u9002\u5e94\u6027\u80fd\u3002", "conclusion": "SWFP\u6846\u67b6\u901a\u8fc7\u5c06\u5168\u5c40\u6d41\u5206\u89e3\u4e3a\u5c0f\u6d41\u5757\u5e8f\u5217\uff0c\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u5feb\u901f\u7684\u5b50\u6a21\u578b\u8bad\u7ec3\u3001\u964d\u4f4e\u7684\u8ba1\u7b97/\u5185\u5b58\u6210\u672c\uff0c\u4ee5\u53ca\u57fa\u4e8eWasserstein\u4fe1\u4efb\u533a\u57df\u7684\u53ef\u8bc1\u660e\u7a33\u5b9a\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15842", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.15842", "abs": "https://arxiv.org/abs/2510.15842", "authors": ["Yuhang Chen", "Tianpeng Lv", "Siyi Zhang", "Yixiang Yin", "Yao Wan", "Philip S. Yu", "Dongping Chen"], "title": "Paper2Web: Let's Make Your Paper Alive!", "comment": "Under Review. Check https://github.com/YuhangChen1/Paper2All for the\n  unified platform to streamline all academic presentation", "summary": "Academic project websites can more effectively disseminate research when they\nclearly present core content and enable intuitive navigation and interaction.\nHowever, current approaches such as direct Large Language Model (LLM)\ngeneration, templates, or direct HTML conversion struggle to produce\nlayout-aware, interactive sites, and a comprehensive evaluation suite for this\ntask has been lacking. In this paper, we introduce Paper2Web, a benchmark\ndataset and multi-dimensional evaluation framework for assessing academic\nwebpage generation. It incorporates rule-based metrics like Connectivity,\nCompleteness and human-verified LLM-as-a-Judge (covering interactivity,\naesthetics, and informativeness), and PaperQuiz, which measures paper-level\nknowledge retention. We further present PWAgent, an autonomous pipeline that\nconverts scientific papers into interactive and multimedia-rich academic\nhomepages. The agent iteratively refines both content and layout through MCP\ntools that enhance emphasis, balance, and presentation quality. Our experiments\nshow that PWAgent consistently outperforms end-to-end baselines like\ntemplate-based webpages and arXiv/alphaXiv versions by a large margin while\nmaintaining low cost, achieving the Pareto-front in academic webpage\ngeneration.", "AI": {"tldr": "Paper2Web\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5b66\u672f\u7f51\u9875\u751f\u6210\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542bPWAgent\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u53ef\u5c06\u79d1\u5b66\u8bba\u6587\u8f6c\u6362\u4e3a\u4ea4\u4e92\u5f0f\u591a\u5a92\u4f53\u5b66\u672f\u4e3b\u9875\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\uff08\u5982\u76f4\u63a5LLM\u751f\u6210\u3001\u6a21\u677f\u6216HTML\u8f6c\u6362\uff09\u96be\u4ee5\u751f\u6210\u5177\u6709\u5e03\u5c40\u611f\u77e5\u548c\u4ea4\u4e92\u6027\u7684\u5b66\u672f\u7f51\u7ad9\uff0c\u4e14\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u5957\u4ef6\u3002", "method": "\u63d0\u51faPaper2Web\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u57fa\u4e8e\u89c4\u5219\u7684\u6307\u6807\uff08\u8fde\u901a\u6027\u3001\u5b8c\u6574\u6027\uff09\u3001\u4eba\u7c7b\u9a8c\u8bc1\u7684LLM-as-a-Judge\u8bc4\u4f30\uff08\u4ea4\u4e92\u6027\u3001\u7f8e\u89c2\u6027\u3001\u4fe1\u606f\u6027\uff09\u548cPaperQuiz\uff08\u77e5\u8bc6\u4fdd\u7559\u5ea6\uff09\u3002\u540c\u65f6\u5f00\u53d1PWAgent\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u901a\u8fc7MCP\u5de5\u5177\u8fed\u4ee3\u4f18\u5316\u5185\u5bb9\u548c\u5e03\u5c40\u3002", "result": "PWAgent\u5728\u5b66\u672f\u7f51\u9875\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u7aef\u5230\u7aef\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u6a21\u677f\u7684\u7f51\u9875\u548carXiv/alphaXiv\u7248\u672c\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "conclusion": "Paper2Web\u4e3a\u5b66\u672f\u7f51\u9875\u751f\u6210\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0cPWAgent\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4ea4\u4e92\u5f0f\u5b66\u672f\u4e3b\u9875\u3002", "topic": "swe application"}}
{"id": "2510.15429", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15429", "abs": "https://arxiv.org/abs/2510.15429", "authors": ["Shashank Gupta"], "title": "Safe, Efficient, and Robust Reinforcement Learning for Ranking and Diffusion Models", "comment": "PhD Thesis of Shashank Gupta defended at the University of Amsterdam\n  on October 13th 2025", "summary": "This dissertation investigates how reinforcement learning (RL) methods can be\ndesigned to be safe, sample-efficient, and robust. Framed through the unifying\nperspective of contextual-bandit RL, the work addresses two major application\ndomains - ranking and recommendation, and text-to-image diffusion models. The\nfirst part of the thesis develops theory and algorithms for safe deployment in\nranking systems. An exposure-based generalisation bound is derived, leading to\na counterfactual risk-minimisation objective whose solution is guaranteed not\nto underperform the logging policy, even with sparse feedback. This guarantee\nis extended to doubly robust estimators, enabling safety even under adversarial\nor misspecified user models and offering practitioners explicit control over\npermissible utility loss. The second part turns to single-action bandits, where\nvarious off-policy estimators are unified within a baseline-correction\nframework. A closed-form optimal baseline is proposed and shown to minimise\nboth evaluation and policy-gradient variance, thereby improving off-policy\nlearning reliability. The final part examines the trade-offs between efficiency\nand effectiveness in generative RL. A systematic study of PPO and REINFORCE\nmotivates the Leave-One-Out PPO (LOOP) algorithm, which combines multiple\ndiffusion trajectories with a REINFORCE-style baseline inside PPO's clipped\nobjective. LOOP achieves PPO-level sample efficiency while producing\ngenerations that align more faithfully with textual attributes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u4e0a\u4e0b\u6587\u591a\u81c2\u8d4c\u535a\u673a\u6846\u67b6\uff0c\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u6392\u540d\u63a8\u8350\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5b89\u5168\u3001\u6837\u672c\u9ad8\u6548\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5b89\u5168\u90e8\u7f72\u7406\u8bba\u3001\u6700\u4f18\u57fa\u7ebf\u6821\u6b63\u65b9\u6cd5\u548cLOOP\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u3001\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6392\u540d\u63a8\u8350\u7cfb\u7edf\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u6311\u6218\u3002", "method": "1. \u6392\u540d\u7cfb\u7edf\uff1a\u63a8\u5bfc\u66dd\u5149\u6cdb\u5316\u754c\uff0c\u63d0\u51fa\u53cd\u4e8b\u5b9e\u98ce\u9669\u6700\u5c0f\u5316\u76ee\u6807\uff1b2. \u5355\u81c2\u8d4c\u535a\u673a\uff1a\u7edf\u4e00\u79bb\u7b56\u7565\u4f30\u8ba1\u5668\uff0c\u63d0\u51fa\u6700\u4f18\u57fa\u7ebf\u6821\u6b63\uff1b3. \u751f\u6210RL\uff1a\u7cfb\u7edf\u7814\u7a76PPO\u548cREINFORCE\uff0c\u63d0\u51faLOOP\u7b97\u6cd5\u7ed3\u5408\u591a\u6269\u6563\u8f68\u8ff9\u3002", "result": "1. \u5b89\u5168\u90e8\u7f72\u4fdd\u8bc1\u4e0d\u52a3\u4e8e\u65e5\u5fd7\u7b56\u7565\uff1b2. \u6700\u4f18\u57fa\u7ebf\u6700\u5c0f\u5316\u65b9\u5dee\uff1b3. LOOP\u5b9e\u73b0PPO\u7ea7\u6837\u672c\u6548\u7387\uff0c\u540c\u65f6\u751f\u6210\u66f4\u5fe0\u5b9e\u4e8e\u6587\u672c\u5c5e\u6027\u7684\u56fe\u50cf\u3002", "conclusion": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u591a\u81c2\u8d4c\u535a\u673a\u6846\u67b6\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u6392\u540d\u63a8\u8350\u548c\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5b89\u5168\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u7b97\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15444", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15444", "abs": "https://arxiv.org/abs/2510.15444", "authors": ["Zhi Zhou", "Yuhao Tan", "Zenan Li", "Yuan Yao", "Lan-Zhe Guo", "Yu-Feng Li", "Xiaoxing Ma"], "title": "A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning", "comment": "Accepted by NeurIPS 2025", "summary": "Test-time scaling seeks to improve the reasoning performance of large\nlanguage models (LLMs) by adding computational resources. A prevalent approach\nwithin the field is sampling-based test-time scaling methods, which enhance\nreasoning by generating multiple reasoning paths for a given input during\ninference. However, despite its practical success, the theoretical foundations\nremain underexplored. In this paper, we provide the first theoretical framework\nfor analyzing sampling-based test-time scaling methods, grounded in the\nperspective of confidence estimation. Based on the framework, we analyze two\ndominant paradigms: self-consistency and perplexity, and reveal key\nlimitations: self-consistency suffers from high estimation error while\nperplexity exhibits substantial modeling error and possible degradation of the\nestimation error convergence. To address these limitations, we introduce RPC, a\nhybrid method that leverages our theoretical insights through two key\ncomponents: Perplexity Consistency and Reasoning Pruning. Perplexity\nConsistency combines the strengths of self-consistency and perplexity, boosting\nthe convergence rate of estimation error from linear to exponential while\npreserving model error. Reasoning Pruning prevents degradation by eliminating\nlow-probability reasoning paths. Both theoretical analysis and empirical\nresults across seven benchmark datasets demonstrate that RPC has a strong\npotential for reducing reasoning error. Notably, RPC achieves reasoning\nperformance comparable to self-consistency while not only enhancing confidence\nreliability but also reducing sampling costs by 50%. The code and resources are\navailable at https://wnjxyk.github.io/RPC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790\u91c7\u6837\u578b\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u81ea\u4e00\u81f4\u6027\u548c\u56f0\u60d1\u5ea6\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684RPC\u6df7\u5408\u65b9\u6cd5\u3002", "motivation": "\u91c7\u6837\u578b\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u6210\u529f\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86RPC\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u56f0\u60d1\u5ea6\u4e00\u81f4\u6027\uff08\u7ed3\u5408\u81ea\u4e00\u81f4\u6027\u548c\u56f0\u60d1\u5ea6\u4f18\u52bf\uff09\u548c\u63a8\u7406\u526a\u679d\uff08\u6d88\u9664\u4f4e\u6982\u7387\u63a8\u7406\u8def\u5f84\uff09\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cRPC\u5728\u4fdd\u6301\u4e0e\u81ea\u4e00\u81f4\u6027\u76f8\u5f53\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\uff0c\u5e76\u5c06\u91c7\u6837\u6210\u672c\u964d\u4f4e\u4e8650%\u3002", "conclusion": "RPC\u65b9\u6cd5\u901a\u8fc7\u7406\u8bba\u6307\u5bfc\u7684\u6df7\u5408\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u51cf\u5c11\u63a8\u7406\u9519\u8bef\u65b9\u9762\u5177\u6709\u5f3a\u5927\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.15859", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15859", "abs": "https://arxiv.org/abs/2510.15859", "authors": ["Pengkai Wang", "Qi Zuo", "Pengwei Liu", "Zhijie Sang", "Congkai Xie", "Hongxia Yang"], "title": "InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training", "comment": "17 pages, 6 figures", "summary": "Large Language Models (LLMs) have shown substantial advances through\nreinforcement learning (RL), particularly in domains where rewards can be\nprogrammatically verified, such as mathematics and code. In these areas, models\nbenefit from a well-defined operational base guided by explicit rule-based\nobjectives. However, this progress reveals a significant limitation: in\nopen-ended domains where rewards are ambiguous, subjective, or\ncontext-dependent, such as creative writing, scientific reasoning, and notably\nmedical consultation, robust reward functions are lacking, making these areas\nchallenging for current RL strategies. To bridge this gap, we introduce ORBIT,\nan open-ended rubric-based incremental training framework specifically designed\nfor high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue\ngeneration with the dynamic creation of rubrics, employing these rubrics to\ndirect an incremental RL process. In particular, this approach does not depend\non external medical knowledge or manual rules, instead utilizing rubric-guided\nfeedback to shape learning. When implemented on the Qwen3-4B-Instruct model,\nour method can greatly enhance its performance on the HealthBench-Hard\nbenchmark from 7.0 to 27.2 using only 2k samples, thus achieving\nstate-of-the-art results for models of this scale. Our analysis confirms that\nrubric-driven RL fos-ters consistent performance gains across diverse\nconsultation scenarios, going beyond simple numerical improvements. These\nfindings underscore rubric-based feedback as a scalable strategy for advancing\nLLMs in intricate, open-ended tasks.", "AI": {"tldr": "ORBIT\u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u589e\u91cf\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u5f00\u653e\u5f0f\u9ad8\u98ce\u9669\u533b\u7597\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\uff0c\u5728HealthBench-Hard\u57fa\u51c6\u4e0a\u4ece7.0\u63d0\u5347\u81f327.2\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u5f00\u653e\u5f0f\u9886\u57df\uff08\u5982\u533b\u7597\u54a8\u8be2\uff09\u4e2d\u56e0\u7f3a\u4e4f\u660e\u786e\u5956\u52b1\u51fd\u6570\u800c\u96be\u4ee5\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u9886\u57df\u7684\u5956\u52b1\u901a\u5e38\u6a21\u7cca\u3001\u4e3b\u89c2\u4e14\u4f9d\u8d56\u4e0a\u4e0b\u6587\u3002", "method": "ORBIT\u6846\u67b6\u7ed3\u5408\u5408\u6210\u5bf9\u8bdd\u751f\u6210\u548c\u52a8\u6001\u8bc4\u5206\u6807\u51c6\u521b\u5efa\uff0c\u4f7f\u7528\u8bc4\u5206\u6807\u51c6\u6307\u5bfc\u589e\u91cf\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\uff0c\u65e0\u9700\u5916\u90e8\u533b\u5b66\u77e5\u8bc6\u6216\u4eba\u5de5\u89c4\u5219\u3002", "result": "\u5728Qwen3-4B-Instruct\u6a21\u578b\u4e0a\uff0c\u4ec5\u4f7f\u75282k\u6837\u672c\u5c31\u5c06HealthBench-Hard\u57fa\u51c6\u6027\u80fd\u4ece7.0\u63d0\u5347\u81f327.2\uff0c\u8fbe\u5230\u540c\u89c4\u6a21\u6a21\u578b\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8bc4\u5206\u6807\u51c6\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u662f\u63a8\u8fdbLLM\u5728\u590d\u6742\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u53ef\u6269\u5c55\u7b56\u7565\uff0c\u80fd\u5728\u591a\u6837\u5316\u54a8\u8be2\u573a\u666f\u4e2d\u5b9e\u73b0\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15863", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15863", "abs": "https://arxiv.org/abs/2510.15863", "authors": ["Simon Yu", "Gang Li", "Weiyan Shi", "Peng Qi"], "title": "PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction", "comment": "29 pages, 6 figures, 8 tables", "summary": "Large language models (LLMs) are moving beyond static uses and are now\npowering agents that learn continually during their interaction with external\nenvironments. For example, agents can learn reusable skills while navigating\nweb pages or toggling new tools. However, existing methods for skill learning\noften create skills that are over-specialized to a single website and fail to\ngeneralize. We introduce PolySkill, a new framework that enables agents to\nlearn generalizable and compositional skills. The core idea, inspired by\npolymorphism in software engineering, is to decouple a skill's abstract goal\n(what it accomplishes) and its concrete implementation (how it is executed).\nExperiments show that our method (1) improves skill reuse by 1.7x on seen\nwebsites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on\nunseen websites, while reducing steps by over 20%. (3) In self-exploration\nsettings without specified tasks, our framework improves the quality of\nproposed tasks and enables agents to learn generalizable skills that work\nacross different sites. By enabling the agent to identify and refine its own\ngoals, the PolySkill enhances the agent's ability to learn a better curriculum,\nleading to the acquisition of more generalizable skills compared to baseline\nmethods. This work provides a practical path toward building agents capable of\ncontinual learning in adaptive environments. Our findings show that separating\na skill's goal from its execution is a crucial step toward developing\nautonomous agents that can learn and generalize across the open web\ncontinuously.", "AI": {"tldr": "PolySkill\u6846\u67b6\u901a\u8fc7\u5c06\u6280\u80fd\u7684\u62bd\u8c61\u76ee\u6807\u4e0e\u5177\u4f53\u5b9e\u73b0\u89e3\u8026\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5b66\u4e60\u53ef\u6cdb\u5316\u548c\u7ec4\u5408\u7684\u6280\u80fd\uff0c\u5728\u7f51\u9875\u4ea4\u4e92\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6280\u80fd\u91cd\u7528\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b66\u4e60\u7684\u6280\u80fd\u5f80\u5f80\u8fc7\u5ea6\u4e13\u95e8\u5316\u4e8e\u5355\u4e00\u7f51\u7ad9\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5b66\u4e60\u53ef\u6cdb\u5316\u6280\u80fd\u7684\u6846\u67b6\u3002", "method": "\u53d7\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u591a\u6001\u6027\u7684\u542f\u53d1\uff0cPolySkill\u5c06\u6280\u80fd\u7684\u62bd\u8c61\u76ee\u6807\uff08\u5b9e\u73b0\u4ec0\u4e48\uff09\u4e0e\u5177\u4f53\u5b9e\u73b0\uff08\u5982\u4f55\u6267\u884c\uff09\u89e3\u8026\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5b66\u4e60\u53ef\u7ec4\u5408\u548c\u6cdb\u5316\u7684\u6280\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a1) \u5728\u5df2\u89c1\u7f51\u7ad9\u4e0a\u6280\u80fd\u91cd\u7528\u7387\u63d0\u9ad81.7\u500d\uff1b2) \u5728Mind2Web\u548c\u672a\u89c1\u7f51\u7ad9\u4e0a\u6210\u529f\u7387\u5206\u522b\u63d0\u53479.4%\u548c13.9%\uff0c\u540c\u65f6\u6b65\u9aa4\u51cf\u5c11\u8d85\u8fc720%\uff1b3) \u5728\u81ea\u4e3b\u63a2\u7d22\u73af\u5883\u4e2d\u80fd\u63d0\u51fa\u66f4\u9ad8\u8d28\u91cf\u7684\u4efb\u52a1\u5e76\u5b66\u4e60\u8de8\u7f51\u7ad9\u6cdb\u5316\u7684\u6280\u80fd\u3002", "conclusion": "\u5c06\u6280\u80fd\u76ee\u6807\u4e0e\u6267\u884c\u5206\u79bb\u662f\u5f00\u53d1\u80fd\u591f\u5728\u5f00\u653e\u7f51\u7edc\u4e2d\u6301\u7eed\u5b66\u4e60\u548c\u6cdb\u5316\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u5173\u952e\u6b65\u9aa4\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15456", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15456", "abs": "https://arxiv.org/abs/2510.15456", "authors": ["Jan Corazza", "Hadi Partovi Aria", "Daniel Neider", "Zhe Xu"], "title": "Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment", "comment": "Please cite the proceedings version. Source code:\n  https://github.com/corazza/tcrl", "summary": "Reinforcement learning (RL) algorithms struggle with learning optimal\npolicies for tasks where reward feedback is sparse and depends on a complex\nsequence of events in the environment. Probabilistic reward machines (PRMs) are\nfinite-state formalisms that can capture temporal dependencies in the reward\nsignal, along with nondeterministic task outcomes. While special RL algorithms\ncan exploit this finite-state structure to expedite learning, PRMs remain\ndifficult to modify and design by hand. This hinders the already difficult\ntasks of utilizing high-level causal knowledge about the environment, and\ntransferring the reward formalism into a new domain with a different causal\nstructure. This paper proposes a novel method to incorporate causal information\nin the form of Temporal Logic-based Causal Diagrams into the reward formalism,\nthereby expediting policy learning and aiding the transfer of task\nspecifications to new environments. Furthermore, we provide a theoretical\nresult about convergence to optimal policy for our method, and demonstrate its\nstrengths empirically.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u65f6\u5e8f\u903b\u8f91\u56e0\u679c\u56fe\u878d\u5165\u6982\u7387\u5956\u52b1\u673a\u7684\u65b9\u6cd5\uff0c\u4ee5\u52a0\u901f\u7b56\u7565\u5b66\u4e60\u5e76\u5e2e\u52a9\u4efb\u52a1\u89c4\u8303\u8fc1\u79fb\u5230\u65b0\u73af\u5883\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u5904\u7406\u5956\u52b1\u53cd\u9988\u7a00\u758f\u4e14\u4f9d\u8d56\u4e8e\u73af\u5883\u4e2d\u590d\u6742\u4e8b\u4ef6\u5e8f\u5217\u7684\u4efb\u52a1\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u800c\u6982\u7387\u5956\u52b1\u673a\u867d\u7136\u80fd\u6355\u6349\u5956\u52b1\u4fe1\u53f7\u4e2d\u7684\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u96be\u4ee5\u624b\u52a8\u4fee\u6539\u548c\u8bbe\u8ba1\u3002", "method": "\u5c06\u65f6\u5e8f\u903b\u8f91\u56e0\u679c\u56fe\u5f62\u5f0f\u5316\u7684\u56e0\u679c\u4fe1\u606f\u6574\u5408\u5230\u5956\u52b1\u5f62\u5f0f\u5316\u4e2d\uff0c\u63d0\u4f9b\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u7b56\u7565\u5b66\u4e60\u548c\u4efb\u52a1\u89c4\u8303\u8fc1\u79fb\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5229\u7528\u9ad8\u5c42\u56e0\u679c\u77e5\u8bc6\uff0c\u52a0\u901f\u7b56\u7565\u5b66\u4e60\u5e76\u4fc3\u8fdb\u4efb\u52a1\u89c4\u8303\u5728\u4e0d\u540c\u73af\u5883\u95f4\u7684\u8fc1\u79fb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15502", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15502", "abs": "https://arxiv.org/abs/2510.15502", "authors": ["Shijia Kang", "Muhan Zhang"], "title": "The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling", "comment": null, "summary": "Reinforcement learning (RL) has been pivotal in enhancing the reasoning\ncapabilities of large language models (LLMs), but it often suffers from limited\nexploration and entropy collapse, where models exploit a narrow set of\nsolutions, leading to a loss of sampling diversity and subsequently preventing\nRL from further improving performance. This issue is exacerbated in parallel\nsampling methods, where multiple outputs are drawn from the same distribution,\npotentially causing the model to converge to similar solutions. We propose\nSESA, a novel SEquential SAmpling framework that mitigates this challenge by\ngenerating diverse solution sketches sequentially before expanding them into\nfull reasoning paths. This approach ensures broader exploration by conditioning\neach new output on previous ones, promoting diversity throughout the process\nand preventing policy collapse. Our experiments on a synthetic task show that\nsequential sampling consistently outperforms traditional RL methods in terms of\npath diversity and recovery from collapse. Further evaluations on real-world\ntasks demonstrate that SESA improves both the exploration of valid strategies\nand the overall performance of LLMs. On three agent benchmarks, SESA lifts\nsuccess rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up\nto an additional $211\\%$ relative improvement over baseline RL), underscoring\nits exploration advantage. This work introduces a structured approach to\nexploration, paving the way for more effective and diverse reasoning in\nRL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.", "AI": {"tldr": "\u63d0\u51faSESA\u6846\u67b6\uff0c\u901a\u8fc7\u987a\u5e8f\u91c7\u6837\u751f\u6210\u591a\u6837\u5316\u89e3\u51b3\u65b9\u6848\u8349\u56fe\uff0c\u89e3\u51b3RL\u8bad\u7ec3\u4e2d\u63a2\u7d22\u4e0d\u8db3\u548c\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65f6\u5b58\u5728\u63a2\u7d22\u6709\u9650\u548c\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u5229\u7528\u72ed\u7a84\u7684\u89e3\u51b3\u65b9\u6848\u96c6\uff0c\u5bfc\u81f4\u91c7\u6837\u591a\u6837\u6027\u4e27\u5931\uff0c\u963b\u788d\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "SESA\u987a\u5e8f\u91c7\u6837\u6846\u67b6\uff1a\u9996\u5148\u751f\u6210\u591a\u6837\u5316\u7684\u89e3\u51b3\u65b9\u6848\u8349\u56fe\uff0c\u7136\u540e\u5c06\u5176\u6269\u5c55\u4e3a\u5b8c\u6574\u7684\u63a8\u7406\u8def\u5f84\uff0c\u901a\u8fc7\u6761\u4ef6\u5316\u6bcf\u4e2a\u65b0\u8f93\u51fa\u4e8e\u5148\u524d\u8f93\u51fa\u6765\u786e\u4fdd\u66f4\u5e7f\u6cdb\u7684\u63a2\u7d22\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\u4e2d\uff0c\u987a\u5e8f\u91c7\u6837\u5728\u8def\u5f84\u591a\u6837\u6027\u548c\u4ece\u5d29\u6e83\u4e2d\u6062\u590d\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfRL\u65b9\u6cd5\uff1b\u5728\u4e09\u4e2a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387\u5206\u522b\u63d0\u5347+0.25\u3001+0.42\u548c+0.07\uff08\u76f8\u5bf9\u4e8e\u57fa\u7ebfRL\u6700\u9ad8\u8fbe211%\u7684\u76f8\u5bf9\u6539\u8fdb\uff09\u3002", "conclusion": "SESA\u4e3a\u63a2\u7d22\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u4e3aRL\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u66f4\u6709\u6548\u548c\u591a\u6837\u5316\u7684\u63a8\u7406\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15555", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15555", "abs": "https://arxiv.org/abs/2510.15555", "authors": ["Sibo Xiao"], "title": "Doubly Robust Estimation of Causal Effects in Strategic Equilibrium Systems", "comment": null, "summary": "We introduce the Strategic Doubly Robust (SDR) estimator, a novel framework\nthat integrates strategic equilibrium modeling with doubly robust estimation\nfor causal inference in strategic environments. SDR addresses endogenous\ntreatment assignment arising from strategic agent behavior, maintaining double\nrobustness while incorporating strategic considerations. Theoretical analysis\nconfirms SDR's consistency and asymptotic normality under strategic\nunconfoundedness. Empirical evaluations demonstrate SDR's superior performance\nover baseline methods, achieving 7.6\\%-29.3\\% bias reduction across varying\nstrategic strengths and maintaining robust scalability with agent populations.\nThe framework provides a principled approach for reliable causal inference when\nagents respond strategically to interventions.", "AI": {"tldr": "\u63d0\u51fa\u6218\u7565\u53cc\u91cd\u7a33\u5065\uff08SDR\uff09\u4f30\u8ba1\u5668\uff0c\u5c06\u6218\u7565\u5747\u8861\u5efa\u6a21\u4e0e\u53cc\u91cd\u7a33\u5065\u4f30\u8ba1\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u6218\u7565\u73af\u5883\u4e2d\u7684\u56e0\u679c\u63a8\u65ad\u3002", "motivation": "\u89e3\u51b3\u7531\u6218\u7565\u4ee3\u7406\u884c\u4e3a\u5f15\u8d77\u7684\u5185\u751f\u5904\u7406\u5206\u914d\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u53cc\u91cd\u7a33\u5065\u6027\u7684\u540c\u65f6\u7eb3\u5165\u6218\u7565\u8003\u91cf\u3002", "method": "\u96c6\u6210\u6218\u7565\u5747\u8861\u5efa\u6a21\u4e0e\u53cc\u91cd\u7a33\u5065\u4f30\u8ba1\u6846\u67b6\uff0c\u5728\u6218\u7565\u65e0\u6df7\u6dc6\u5047\u8bbe\u4e0b\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793aSDR\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u504f\u5dee\u51cf\u5c117.6%-29.3%\uff0c\u5728\u4e0d\u540c\u6218\u7565\u5f3a\u5ea6\u548c\u4ee3\u7406\u89c4\u6a21\u4e0b\u4fdd\u6301\u7a33\u5065\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u4e3a\u4ee3\u7406\u5bf9\u5e72\u9884\u505a\u51fa\u6218\u7565\u54cd\u5e94\u65f6\u7684\u53ef\u9760\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.15674", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15674", "abs": "https://arxiv.org/abs/2510.15674", "authors": ["Yung-Chen Tang", "Pin-Yu Chen", "Andrea Cavallaro"], "title": "CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning", "comment": null, "summary": "Allocating more computation during inference time (test-time scaling)\nimproves language model performance, especially for reasoning tasks. However,\npopular methods like Best-of-$N$ sampling often show diminishing returns as $N$\nincreases. To address this inefficiency, we introduce a general test-time\ncalibration framework that adaptively modifies the model toward high-reward\nreasoning paths, with theoretical guarantees of improving the lower bound of\nexpected reward under finite sampling, all without large language model (LLM)\nretraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$),\na two-phase method that first explores the solution space and then learns a\ncalibration of the logits via an input-specific temperature $T$ and additive\nshift vector $\\delta$, guiding generation toward more reliable reasoning.\nExperiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency,\nwith up to $4\\times$ fewer rollouts to reach the same accuracy, while often\nachieving higher accuracy under fixed budgets. We also analyze the\ncomplementary roles of $T$ and $\\delta$ in balancing output diversity and\ncorrectness, and demonstrate that the framework also generalizes to step-level\nsampling strategies such as beam search. For more information, please refer to\nour project page at huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.", "AI": {"tldr": "\u63d0\u51faCarBoN\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u6e29\u5ea6T\u548c\u504f\u79fb\u5411\u91cf\u03b4\u6765\u6539\u8fdb\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6d4b\u8bd5\u65f6\u91c7\u6837\u6548\u7387\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3LLM\uff0c\u5728MATH-500\u548cAIME-2024\u4e0a\u5b9e\u73b0\u9ad8\u8fbe4\u500d\u7684\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfBest-of-N\u91c7\u6837\u65b9\u6cd5\u5728N\u589e\u5927\u65f6\u6536\u76ca\u9012\u51cf\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u63a8\u7406\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u5206\u914d\u7684\u6548\u7387\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u5148\u63a2\u7d22\u89e3\u7a7a\u95f4\uff0c\u7136\u540e\u5b66\u4e60\u8f93\u5165\u7279\u5b9a\u7684\u6e29\u5ea6T\u548c\u52a0\u6027\u504f\u79fb\u5411\u91cf\u03b4\u6765\u6821\u51c6logits\uff0c\u5f15\u5bfc\u751f\u6210\u66f4\u53ef\u9760\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728MATH-500\u548cAIME-2024\u4e0a\uff0cCarBoN\u4ee5\u6700\u591a4\u500d\u66f4\u5c11\u7684\u91c7\u6837\u6b21\u6570\u8fbe\u5230\u76f8\u540c\u51c6\u786e\u7387\uff0c\u4e14\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u901a\u5e38\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "CarBoN\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u65f6\u91c7\u6837\u7684\u6548\u7387\uff0cT\u548c\u03b4\u5728\u5e73\u8861\u8f93\u51fa\u591a\u6837\u6027\u548c\u6b63\u786e\u6027\u65b9\u9762\u53d1\u6325\u4e92\u8865\u4f5c\u7528\uff0c\u8be5\u6846\u67b6\u4e5f\u9002\u7528\u4e8e\u675f\u641c\u7d22\u7b49\u6b65\u9aa4\u7ea7\u91c7\u6837\u7b56\u7565\u3002", "topic": "agent analysis"}}
{"id": "2510.15700", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.15700", "abs": "https://arxiv.org/abs/2510.15700", "authors": ["Alex Gu", "Bartosz Piotrowski", "Fabian Gloeckle", "Kaiyu Yang", "Aram H. Markosyan"], "title": "ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations", "comment": "52 pages, 16 figures, website: http://proof-optimizer.github.io/", "summary": "Neural theorem proving has advanced rapidly in the past year, reaching IMO\ngold-medalist capabilities and producing formal proofs that span thousands of\nlines. Although such proofs are mechanically verified by formal systems like\nLean, their excessive length renders them difficult for humans to comprehend\nand limits their usefulness for mathematical insight. Proof simplification is\ntherefore a critical bottleneck. Yet, training data for this task is scarce,\nand existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --\nstruggle with the extremely long proofs generated by RL-trained provers. We\nintroduce ProofOptimizer, the first language model trained to simplify Lean\nproofs without requiring additional human supervision. ProofOptimizer is\ntrained via expert iteration and reinforcement learning, using Lean to verify\nsimplifications and provide training signal. At inference time, it operates\nwithin an iterative proof-shortening workflow, progressively reducing proof\nlength. Experiments show that ProofOptimizer substantially compresses proofs\ngenerated by state-of-the-art RL-trained provers on standard benchmarks,\nreducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on\nSeed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check\nfaster in Lean and further improve downstream prover performance when reused as\ntraining data for supervised finetuning.", "AI": {"tldr": "ProofOptimizer\u662f\u4e00\u4e2a\u901a\u8fc7\u4e13\u5bb6\u8fed\u4ee3\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u7b80\u5316Lean\u8bc1\u660e\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\uff0c\u80fd\u663e\u8457\u538b\u7f29\u8bc1\u660e\u957f\u5ea6\u5e76\u63d0\u9ad8\u9a8c\u8bc1\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7f51\u7edc\u8bc1\u660e\u751f\u6210\u7684\u8bc1\u660e\u8fc7\u4e8e\u5197\u957f\uff0c\u96be\u4ee5\u88ab\u4eba\u7c7b\u7406\u89e3\uff0c\u9650\u5236\u4e86\u6570\u5b66\u6d1e\u5bdf\u529b\u7684\u53d1\u5c55\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406RL\u8bad\u7ec3\u8bc1\u660e\u5668\u751f\u6210\u7684\u957f\u8bc1\u660e\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u4e13\u5bb6\u8fed\u4ee3\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528Lean\u9a8c\u8bc1\u7b80\u5316\u5e76\u63d0\u4f9b\u8bad\u7ec3\u4fe1\u53f7\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u8fed\u4ee3\u8bc1\u660e\u7f29\u77ed\u5de5\u4f5c\u6d41\u7a0b\u9010\u6b65\u51cf\u5c11\u8bc1\u660e\u957f\u5ea6\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProofOptimizer\u663e\u8457\u538b\u7f29\u4e86\u8bc1\u660e\u957f\u5ea6\uff1aminiF2F\u51cf\u5c1187%\uff0cPutnamBench\u51cf\u5c1157%\uff0cSeed-Prover\u7684IMO 2025\u8bc1\u660e\u51cf\u5c1149%\u3002", "conclusion": "ProofOptimizer\u4e0d\u4ec5\u80fd\u751f\u6210\u66f4\u7b80\u6d01\u7684\u8bc1\u660e\uff0c\u8fd8\u80fd\u52a0\u901fLean\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u5e76\u4e14\u5f53\u4f5c\u4e3a\u76d1\u7763\u5fae\u8c03\u7684\u8bad\u7ec3\u6570\u636e\u65f6\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u4e0b\u6e38\u8bc1\u660e\u5668\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15720", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15720", "abs": "https://arxiv.org/abs/2510.15720", "authors": ["Edwin Hamel-De le Court", "Gaspard Ohlmann", "Francesco Belardinelli"], "title": "ProSh: Probabilistic Shielding for Model-free Reinforcement Learning", "comment": null, "summary": "Safety is a major concern in reinforcement learning (RL): we aim at\ndeveloping RL systems that not only perform optimally, but are also safe to\ndeploy by providing formal guarantees about their safety. To this end, we\nintroduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free\nalgorithm for safe reinforcement learning under cost constraints. ProSh\naugments the Constrained MDP state space with a risk budget and enforces safety\nby applying a shield to the agent's policy distribution using a learned cost\ncritic. The shield ensures that all sampled actions remain safe in expectation.\nWe also show that optimality is preserved when the environment is\ndeterministic. Since ProSh is model-free, safety during training depends on the\nknowledge we have acquired about the environment. We provide a tight\nupper-bound on the cost in expectation, depending only on the backup-critic\naccuracy, that is always satisfied during training. Under mild, practically\nachievable assumptions, ProSh guarantees safety even at training time, as shown\nin the experiments.", "AI": {"tldr": "\u63d0\u51faProbabilistic Shielding via Risk Augmentation (ProSh)\u7b97\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u65e0\u5173\u7684\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u98ce\u9669\u9884\u7b97\u589e\u5f3a\u72b6\u6001\u7a7a\u95f4\uff0c\u5e76\u4f7f\u7528\u6210\u672c\u8bc4\u8bba\u5668\u5bf9\u7b56\u7565\u5206\u5e03\u65bd\u52a0\u5c4f\u853d\uff0c\u786e\u4fdd\u6240\u6709\u91c7\u6837\u52a8\u4f5c\u5728\u671f\u671b\u610f\u4e49\u4e0b\u4fdd\u6301\u5b89\u5168\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u662f\u4e3b\u8981\u5173\u6ce8\u70b9\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u6700\u4f18\u6267\u884c\u53c8\u80fd\u63d0\u4f9b\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\u7684\u7cfb\u7edf\u3002", "method": "ProSh\u7b97\u6cd5\u901a\u8fc7\u98ce\u9669\u9884\u7b97\u589e\u5f3a\u7ea6\u675fMDP\u72b6\u6001\u7a7a\u95f4\uff0c\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u6210\u672c\u8bc4\u8bba\u5668\u5bf9\u7b56\u7565\u5206\u5e03\u65bd\u52a0\u5c4f\u853d\uff0c\u786e\u4fdd\u91c7\u6837\u52a8\u4f5c\u5728\u671f\u671b\u610f\u4e49\u4e0b\u5b89\u5168\u3002", "result": "\u5728\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u4fdd\u6301\u6700\u4f18\u6027\uff0c\u63d0\u4f9b\u4ec5\u4f9d\u8d56\u4e8e\u5907\u4efd\u8bc4\u8bba\u5668\u51c6\u786e\u5ea6\u7684\u6210\u672c\u671f\u671b\u4e0a\u754c\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u6e29\u548c\u4e14\u5b9e\u9645\u53ef\u5b9e\u73b0\u7684\u5047\u8bbe\u4e0b\uff0cProSh\u5373\u4f7f\u5728\u8bad\u7ec3\u65f6\u4e5f\u80fd\u4fdd\u8bc1\u5b89\u5168\u6027\u3002", "conclusion": "ProSh\u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u65e0\u5173\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u5f62\u5f0f\u5316\u7684\u5b89\u5168\u4fdd\u8bc1\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.15728", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15728", "abs": "https://arxiv.org/abs/2510.15728", "authors": ["Mahyar Alinejad", "Alvaro Velasquez", "Yue Wang", "George Atia"], "title": "RLAF: Reinforcement Learning from Automaton Feedback", "comment": null, "summary": "Reinforcement Learning (RL) in environments with complex, history-dependent\nreward structures poses significant challenges for traditional methods. In this\nwork, we introduce a novel approach that leverages automaton-based feedback to\nguide the learning process, replacing explicit reward functions with\npreferences derived from a deterministic finite automaton (DFA). Unlike\nconventional approaches that use automata for direct reward specification, our\nmethod employs the structure of the DFA to generate preferences over\ntrajectories that are used to learn a reward function, eliminating the need for\nmanual reward engineering. Our framework introduces a static approach that uses\nthe learned reward function directly for policy optimization and a dynamic\napproach that involves continuous refining of the reward function and policy\nthrough iterative updates until convergence.\n  Our experiments in both discrete and continuous environments demonstrate that\nour approach enables the RL agent to learn effective policies for tasks with\ntemporal dependencies, outperforming traditional reward engineering and\nautomaton-based baselines such as reward machines and LTL-guided methods. Our\nresults highlight the advantages of automaton-based preferences in handling\nnon-Markovian rewards, offering a scalable, efficient, and human-independent\nalternative to traditional reward modeling. We also provide a convergence\nguarantee showing that under standard assumptions our automaton-guided\npreference-based framework learns a policy that is near-optimal with respect to\nthe true non-Markovian objective.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u673a\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528DFA\u751f\u6210\u7684\u504f\u597d\u66ff\u4ee3\u663e\u5f0f\u5956\u52b1\u51fd\u6570\uff0c\u65e0\u9700\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\uff0c\u5728\u65f6\u5e8f\u4f9d\u8d56\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u590d\u6742\u5386\u53f2\u4f9d\u8d56\u5956\u52b1\u7ed3\u6784\u7684\u73af\u5883\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u624b\u52a8\u5956\u52b1\u5de5\u7a0b\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u786e\u5b9a\u6027\u6709\u9650\u81ea\u52a8\u673a(DFA)\u7ed3\u6784\u751f\u6210\u8f68\u8ff9\u504f\u597d\u6765\u5b66\u4e60\u5956\u52b1\u51fd\u6570\uff0c\u5305\u542b\u9759\u6001\u65b9\u6cd5\uff08\u76f4\u63a5\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u5956\u52b1\u51fd\u6570\uff09\u548c\u52a8\u6001\u65b9\u6cd5\uff08\u8fed\u4ee3\u66f4\u65b0\u5956\u52b1\u51fd\u6570\u548c\u7b56\u7565\uff09\u3002", "result": "\u5728\u79bb\u6563\u548c\u8fde\u7eed\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u6709\u6548\u7684\u65f6\u5e8f\u4f9d\u8d56\u4efb\u52a1\u7b56\u7565\uff0c\u4f18\u4e8e\u4f20\u7edf\u5956\u52b1\u5de5\u7a0b\u548c\u57fa\u4e8e\u81ea\u52a8\u673a\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u81ea\u52a8\u673a\u7684\u504f\u597d\u65b9\u6cd5\u5728\u5904\u7406\u975e\u9a6c\u5c14\u53ef\u592b\u5956\u52b1\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u72ec\u7acb\u4e8e\u4eba\u5de5\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u5177\u6709\u6536\u655b\u6027\u4fdd\u8bc1\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.72720084", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcharmbracelet%2Fcrush%3Futm_source=tldrdevops/1/01000199f1ea7e05-4111cb1a-7599-47e6-b9fb-d81292a069a3-000000/Y4cNOqHuRTNcb2y0ZogPl9BCM-0L8ov-qSQ7zXzXbpw=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcharmbracelet%2Fcrush%3Futm_source=tldrdevops/1/01000199f1ea7e05-4111cb1a-7599-47e6-b9fb-d81292a069a3-000000/Y4cNOqHuRTNcb2y0ZogPl9BCM-0L8ov-qSQ7zXzXbpw=427", "authors": ["TLDR Newsletter"], "title": "Crush", "comment": "Source: TLDR Newsletter, Date: 2025-10-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcharmbracelet%2Fcrush%3Futm_source=tldrdevops/1/01000199f1ea7e05-4111cb1a-7599-47e6-b9fb-d81292a069a3-000000/Y4cNOqHuRTNcb2y0ZogPl9BCM-0L8ov-qSQ7zXzXbpw=427", "summary": "Crush (GitHub Repo) Crush, a new AI coding agent for terminals, integrates with your code and workflows, connecting to LLMs like Anthropic, OpenAI, and Groq using API keys. The tool also includes automatic provider updates from the Catwalk open source database.", "source": "tldr", "AI": {"tldr": "Crush\u662f\u4e00\u4e2a\u65b0\u7684AI\u7f16\u7801\u4ee3\u7406\uff0c\u7528\u4e8e\u7ec8\u7aef\u73af\u5883\uff0c\u80fd\u591f\u96c6\u6210\u5230\u4ee3\u7801\u548c\u5de5\u4f5c\u6d41\u4e2d\uff0c\u901a\u8fc7API\u5bc6\u94a5\u8fde\u63a5\u591a\u4e2aLLM\u63d0\u4f9b\u5546\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u7ec8\u7aef\u73af\u5883\u7684AI\u7f16\u7801\u4ee3\u7406\uff0c\u7b80\u5316\u5f00\u53d1\u8005\u5728\u7ec8\u7aef\u4e2d\u7684\u7f16\u7801\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u901a\u8fc7API\u5bc6\u94a5\u96c6\u6210\u591a\u4e2aLLM\u63d0\u4f9b\u5546\uff08Anthropic\u3001OpenAI\u3001Groq\uff09\uff0c\u5e76\u5229\u7528Catwalk\u5f00\u6e90\u6570\u636e\u5e93\u81ea\u52a8\u66f4\u65b0\u63d0\u4f9b\u5546\u4fe1\u606f\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684\u7ec8\u7aefAI\u7f16\u7801\u4ee3\u7406\u5de5\u5177\u3002", "conclusion": "Crush\u6210\u529f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ec8\u7aef\u73af\u5883\u4e0b\u7684AI\u7f16\u7801\u52a9\u624b\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "tldr.2510.cdc969d5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcoze-dev%2Fcoze-studio%3Futm_source=tldrdevops/1/01000199f1ea7e05-4111cb1a-7599-47e6-b9fb-d81292a069a3-000000/Vd7rJZvIQzjc_jyeXEqGYOCdc709-cUJVdFa2k2ka08=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcoze-dev%2Fcoze-studio%3Futm_source=tldrdevops/1/01000199f1ea7e05-4111cb1a-7599-47e6-b9fb-d81292a069a3-000000/Vd7rJZvIQzjc_jyeXEqGYOCdc709-cUJVdFa2k2ka08=427", "authors": ["TLDR Newsletter"], "title": "Coze Studio", "comment": "Source: TLDR Newsletter, Date: 2025-10-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcoze-dev%2Fcoze-studio%3Futm_source=tldrdevops/1/01000199f1ea7e05-4111cb1a-7599-47e6-b9fb-d81292a069a3-000000/Vd7rJZvIQzjc_jyeXEqGYOCdc709-cUJVdFa2k2ka08=427", "summary": "Coze Studio (GitHub Repo) Coze Studio, an all-in-one AI agent development tool derived from the \"Coze Development Platform,\" has made its core engine completely open to simplify agent creation, debugging, and deployment.", "source": "tldr", "AI": {"tldr": "Coze Studio\u662f\u4e00\u4e2a\u5f00\u6e90\u7684AI\u667a\u80fd\u4f53\u5f00\u53d1\u5de5\u5177\uff0c\u6e90\u81eaCoze\u5f00\u53d1\u5e73\u53f0\uff0c\u65e8\u5728\u7b80\u5316\u667a\u80fd\u4f53\u7684\u521b\u5efa\u3001\u8c03\u8bd5\u548c\u90e8\u7f72\u8fc7\u7a0b\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4eAI\u667a\u80fd\u4f53\u5f00\u53d1\u7684\u95e8\u69db\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u8f7b\u677e\u5730\u521b\u5efa\u3001\u8c03\u8bd5\u548c\u90e8\u7f72\u667a\u80fd\u4f53\u5e94\u7528\u3002", "method": "\u5c06Coze\u5f00\u53d1\u5e73\u53f0\u7684\u6838\u5fc3\u5f15\u64ce\u5b8c\u5168\u5f00\u6e90\uff0c\u63d0\u4f9b\u4e00\u4f53\u5316\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u5de5\u5177\u3002", "result": "\u5f00\u53d1\u51fa\u4e86Coze Studio\u8fd9\u4e00\u5f00\u6e90\u5de5\u5177\uff0c\u4f7f\u667a\u80fd\u4f53\u5f00\u53d1\u66f4\u52a0\u7b80\u5316\u548c\u9ad8\u6548\u3002", "conclusion": "\u5f00\u6e90Coze Studio\u5de5\u5177\u80fd\u591f\u6709\u6548\u4fc3\u8fdbAI\u667a\u80fd\u4f53\u5f00\u53d1\u7684\u5927\u4f17\u5316\uff0c\u964d\u4f4e\u5f00\u53d1\u95e8\u69db\u3002", "topic": "swe application"}}
{"id": "tldr.2510.68f99c28", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fimproving-the-trustworthiness-of-javascript-on-the-web%2F%3Futm_source=tldrdevops/1/01000199f1ea7e05-4111cb1a-7599-47e6-b9fb-d81292a069a3-000000/ci_y47bUTuyBgXUmpEPuZn9sslv9bb6P0cLKfISYXvg=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fimproving-the-trustworthiness-of-javascript-on-the-web%2F%3Futm_source=tldrdevops/1/01000199f1ea7e05-4111cb1a-7599-47e6-b9fb-d81292a069a3-000000/ci_y47bUTuyBgXUmpEPuZn9sslv9bb6P0cLKfISYXvg=427", "authors": ["TLDR Newsletter"], "title": "Improving the trustworthiness of JavaScript on the Web", "comment": "Source: TLDR Newsletter, Date: 2025-10-17, Reading time: 20 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fimproving-the-trustworthiness-of-javascript-on-the-web%2F%3Futm_source=tldrdevops/1/01000199f1ea7e05-4111cb1a-7599-47e6-b9fb-d81292a069a3-000000/ci_y47bUTuyBgXUmpEPuZn9sslv9bb6P0cLKfISYXvg=427", "summary": "Improving the trustworthiness of JavaScript on the Web (20 minute read) Web Application Integrity, Consistency, and Transparency (WAICT), a W3C-backed effort, aims to bring stronger security to the web by providing integrity, consistency, and transparency for web applications using in-browser JavaScript cryptography. The system uses subresource integrity (SRI) and integrity manifests to ensure that the code a user receives matches the code on record, addressing the vulnerability of JavaScript...", "source": "tldr", "AI": {"tldr": "WAICT\u662f\u4e00\u4e2aW3C\u652f\u6301\u7684\u9879\u76ee\uff0c\u65e8\u5728\u901a\u8fc7\u6d4f\u89c8\u5668\u5185JavaScript\u52a0\u5bc6\u4e3aWeb\u5e94\u7528\u63d0\u4f9b\u5b8c\u6574\u6027\u3001\u4e00\u81f4\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4f7f\u7528\u5b50\u8d44\u6e90\u5b8c\u6574\u6027\u548c\u5b8c\u6574\u6027\u6e05\u5355\u786e\u4fdd\u7528\u6237\u63a5\u6536\u7684\u4ee3\u7801\u4e0e\u8bb0\u5f55\u5339\u914d\u3002", "motivation": "\u89e3\u51b3JavaScript\u5728Web\u4e0a\u7684\u4fe1\u4efb\u95ee\u9898\uff0c\u7279\u522b\u662f\u4ee3\u7801\u5728\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u88ab\u7be1\u6539\u7684\u6f0f\u6d1e\uff0c\u589e\u5f3aWeb\u5e94\u7528\u7684\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u5b50\u8d44\u6e90\u5b8c\u6574\u6027(SRI)\u548c\u5b8c\u6574\u6027\u6e05\u5355\u6280\u672f\uff0c\u901a\u8fc7\u5bc6\u7801\u5b66\u65b9\u6cd5\u9a8c\u8bc1Web\u5e94\u7528\u4ee3\u7801\u7684\u5b8c\u6574\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u80fd\u591f\u786e\u4fddWeb\u5e94\u7528\u4ee3\u7801\u5b8c\u6574\u6027\u548c\u4e00\u81f4\u6027\u7684\u7cfb\u7edf\u6846\u67b6\u3002", "conclusion": "WAICT\u4e3aWeb\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u89e3\u51b3\u4e86JavaScript\u4ee3\u7801\u4fe1\u4efb\u95ee\u9898\u3002", "topic": "swe application"}}
{"id": "tldr.2510.f9748830", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2F100x.bot%2Fa%2Fa-stateful-browser-agent-using-self-healing-dom-maps%3Futm_source=tldrwebdev/1/01000199f1ef0842-713397f7-8a62-41db-bb16-13ceb37aa53b-000000/XPHbLmto3EOJGtGwqKwSwSCBuCL_SaStaL7Kgy4kQ9o=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2F100x.bot%2Fa%2Fa-stateful-browser-agent-using-self-healing-dom-maps%3Futm_source=tldrwebdev/1/01000199f1ef0842-713397f7-8a62-41db-bb16-13ceb37aa53b-000000/XPHbLmto3EOJGtGwqKwSwSCBuCL_SaStaL7Kgy4kQ9o=427", "authors": ["TLDR Newsletter"], "title": "A stateful browser agent using self-healing DOM maps", "comment": "Source: TLDR Newsletter, Date: 2025-10-17, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2F100x.bot%2Fa%2Fa-stateful-browser-agent-using-self-healing-dom-maps%3Futm_source=tldrwebdev/1/01000199f1ef0842-713397f7-8a62-41db-bb16-13ceb37aa53b-000000/XPHbLmto3EOJGtGwqKwSwSCBuCL_SaStaL7Kgy4kQ9o=427", "summary": "A stateful browser agent using self-healing DOM maps (4 minute read) Agent4 is a stateful browser agent that uses a client-sourced, self-healing DOM map to remember and instantly execute recurring tasks, reducing latency compared to stateless LLM-powered agents.", "source": "tldr", "AI": {"tldr": "Agent4\u662f\u4e00\u4e2a\u6709\u72b6\u6001\u7684\u6d4f\u89c8\u5668\u4ee3\u7406\uff0c\u4f7f\u7528\u5ba2\u6237\u7aef\u81ea\u6108DOM\u6620\u5c04\u6765\u8bb0\u4f4f\u5e76\u5373\u65f6\u6267\u884c\u91cd\u590d\u4efb\u52a1\uff0c\u76f8\u6bd4\u65e0\u72b6\u6001\u7684LLM\u9a71\u52a8\u4ee3\u7406\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u65e0\u72b6\u6001LLM\u4ee3\u7406\u5728\u6267\u884c\u91cd\u590d\u4efb\u52a1\u65f6\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u901a\u8fc7\u4fdd\u6301\u72b6\u6001\u8bb0\u5fc6\u6765\u63d0\u5347\u6267\u884c\u6548\u7387\u3002", "method": "\u91c7\u7528\u5ba2\u6237\u7aef\u81ea\u6108DOM\u6620\u5c04\u6280\u672f\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u8bb0\u4f4f\u4e4b\u524d\u7684\u64cd\u4f5c\u548c\u72b6\u6001\uff0c\u5b9e\u73b0\u5373\u65f6\u4efb\u52a1\u6267\u884c\u3002", "result": "\u76f8\u6bd4\u65e0\u72b6\u6001\u4ee3\u7406\u663e\u8457\u964d\u4f4e\u4e86\u4efb\u52a1\u6267\u884c\u5ef6\u8fdf\uff0c\u63d0\u5347\u4e86\u91cd\u590d\u4efb\u52a1\u7684\u6267\u884c\u6548\u7387\u3002", "conclusion": "\u6709\u72b6\u6001\u7684\u6d4f\u89c8\u5668\u4ee3\u7406\u901a\u8fc7\u81ea\u6108DOM\u6620\u5c04\u6280\u672f\u80fd\u591f\u6709\u6548\u964d\u4f4e\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.a17321f7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfounders%26utm_medium=newsletter%26utm_campaign=advertisecta/1/01000199f211aea3-a4a00278-fa90-4236-a4f4-d46fd641fc97-000000/48_mxZnur-0DUbDE899tPdWJgI7G8KFO-KdwjiSZshw=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfounders%26utm_medium=newsletter%26utm_campaign=advertisecta/1/01000199f211aea3-a4a00278-fa90-4236-a4f4-d46fd641fc97-000000/48_mxZnur-0DUbDE899tPdWJgI7G8KFO-KdwjiSZshw=427", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-10-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfounders%26utm_medium=newsletter%26utm_campaign=advertisecta/1/01000199f211aea3-a4a00278-fa90-4236-a4f4-d46fd641fc97-000000/48_mxZnur-0DUbDE899tPdWJgI7G8KFO-KdwjiSZshw=427", "summary": "Motion Raises $60M at $550M Valuation to Build the Agentic Work Suite for Businesses (5 minute read) Motion raised $60M, led by Scale Venture Partners, to enhance its agentic work suite for SMBs at a valuation of $550M.", "source": "tldr", "AI": {"tldr": "Motion \u878d\u8d446000\u4e07\u7f8e\u5143\uff0c\u4f30\u503c5.5\u4ebf\u7f8e\u5143\uff0c\u7528\u4e8e\u4e3a\u4f01\u4e1a\u6784\u5efa\u667a\u80fd\u5de5\u4f5c\u5957\u4ef6", "motivation": "\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u5f00\u53d1\u667a\u80fd\u5316\u7684\u4ee3\u7406\u5de5\u4f5c\u5957\u4ef6\uff0c\u63d0\u5347\u4f01\u4e1a\u5de5\u4f5c\u6548\u7387\u548c\u81ea\u52a8\u5316\u6c34\u5e73", "method": "\u83b7\u5f97\u7531Scale Venture Partners\u9886\u6295\u76846000\u4e07\u7f8e\u5143\u878d\u8d44\uff0c\u7528\u4e8e\u4ea7\u54c1\u5f00\u53d1\u548c\u5e02\u573a\u62d3\u5c55", "result": "\u6210\u529f\u5b8c\u6210\u878d\u8d44\u8f6e\uff0c\u516c\u53f8\u4f30\u503c\u8fbe\u52305.5\u4ebf\u7f8e\u5143", "conclusion": "Motion\u901a\u8fc7\u878d\u8d44\u589e\u5f3a\u4e86\u5176\u5728\u4f01\u4e1a\u667a\u80fd\u5de5\u4f5c\u5957\u4ef6\u9886\u57df\u7684\u7ade\u4e89\u529b\u548c\u53d1\u5c55\u6f5c\u529b", "topic": "swe application"}}
{"id": "tldr.2510.d9612cc3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftraycer.ai%2F%3Futm_source=tldrdesign/1/01000199f2221837-d7e18ff4-e05f-406c-8523-90bab4c9dde5-000000/NphlPAd7eJq5VRvdPTUbX90APDFDfnbjjcGfsjsk3jE=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftraycer.ai%2F%3Futm_source=tldrdesign/1/01000199f2221837-d7e18ff4-e05f-406c-8523-90bab4c9dde5-000000/NphlPAd7eJq5VRvdPTUbX90APDFDfnbjjcGfsjsk3jE=427", "authors": ["TLDR Newsletter"], "title": "Vibe Code with Confidence", "comment": "Source: TLDR Newsletter, Date: 2025-10-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftraycer.ai%2F%3Futm_source=tldrdesign/1/01000199f2221837-d7e18ff4-e05f-406c-8523-90bab4c9dde5-000000/NphlPAd7eJq5VRvdPTUbX90APDFDfnbjjcGfsjsk3jE=427", "summary": "Vibe Code with Confidence (Website) Traycer scans your codebase to verify every AI-generated change, applying course corrections so bad code never makes it to production.", "source": "tldr", "AI": {"tldr": "Traycer\u662f\u4e00\u4e2a\u626b\u63cf\u4ee3\u7801\u5e93\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u9a8c\u8bc1AI\u751f\u6210\u7684\u4ee3\u7801\u53d8\u66f4\uff0c\u786e\u4fdd\u4e0d\u826f\u4ee3\u7801\u4e0d\u4f1a\u8fdb\u5165\u751f\u4ea7\u73af\u5883", "motivation": "\u968f\u7740AI\u751f\u6210\u4ee3\u7801\u7684\u666e\u53ca\uff0c\u9700\u8981\u786e\u4fddAI\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\u548c\u5b89\u5168\u6027\uff0c\u9632\u6b62\u4e0d\u826f\u4ee3\u7801\u5f71\u54cd\u751f\u4ea7\u7cfb\u7edf", "method": "\u901a\u8fc7\u626b\u63cf\u4ee3\u7801\u5e93\u6765\u9a8c\u8bc1AI\u751f\u6210\u7684\u53d8\u66f4\uff0c\u5e76\u5e94\u7528\u4fee\u6b63\u63aa\u65bd", "result": "\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u963b\u6b62\u4e0d\u826f\u4ee3\u7801\u8fdb\u5165\u751f\u4ea7\u73af\u5883", "conclusion": "Traycer\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u786e\u4fddAI\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u5b89\u5168\u6027", "topic": "swe application"}}
{"id": "tldr.2510.a4cb8c37", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fanshumanbh%2Fsecurevibes%3Futm_source=tldrinfosec/1/01000199f247d0e6-7b3a391d-8602-4e85-818b-e7771bd98e37-000000/G2vYLXQlDkeEUmAhPi6qYIPfBgC18nTjof98mnOet2s=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fanshumanbh%2Fsecurevibes%3Futm_source=tldrinfosec/1/01000199f247d0e6-7b3a391d-8602-4e85-818b-e7771bd98e37-000000/G2vYLXQlDkeEUmAhPi6qYIPfBgC18nTjof98mnOet2s=427", "authors": ["TLDR Newsletter"], "title": "SecureVibes", "comment": "Source: TLDR Newsletter, Date: 2025-10-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fanshumanbh%2Fsecurevibes%3Futm_source=tldrinfosec/1/01000199f247d0e6-7b3a391d-8602-4e85-818b-e7771bd98e37-000000/G2vYLXQlDkeEUmAhPi6qYIPfBgC18nTjof98mnOet2s=427", "summary": "SecureVibes (GitHub Repo) SecureVibes is an AI-native security system for vibecoded applications that uses Claude's multi-agent architecture to find security vulnerabilities in your code base autonomously.", "source": "tldr", "AI": {"tldr": "SecureVibes\u662f\u4e00\u4e2a\u57fa\u4e8eClaude\u591a\u667a\u80fd\u4f53\u67b6\u6784\u7684AI\u539f\u751f\u5b89\u5168\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u53d1\u73b0\u4ee3\u7801\u5e93\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e", "motivation": "\u4e3avibecoded\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u81ea\u52a8\u5316\u7684\u5b89\u5168\u6f0f\u6d1e\u68c0\u6d4b\uff0c\u63d0\u9ad8\u4ee3\u7801\u5b89\u5168\u6027", "method": "\u5229\u7528Claude\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\u6784\u5efaAI\u539f\u751f\u5b89\u5168\u7cfb\u7edf\uff0c\u81ea\u4e3b\u5206\u6790\u4ee3\u7801\u5e93", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u81ea\u52a8\u53d1\u73b0\u5b89\u5168\u6f0f\u6d1e\u7684SecureVibes\u7cfb\u7edf", "conclusion": "\u591a\u667a\u80fd\u4f53AI\u67b6\u6784\u53ef\u4ee5\u6709\u6548\u5e94\u7528\u4e8e\u4ee3\u7801\u5b89\u5168\u68c0\u6d4b\u9886\u57df", "topic": "code agent"}}
{"id": "tldr.2510.0c8bec3d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F16%2Fclaude-skills%2F%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/570AzSPWIwsw9U-UpMCF363OsAdX8AG9YhDTH9QcNNQ=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F16%2Fclaude-skills%2F%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/570AzSPWIwsw9U-UpMCF363OsAdX8AG9YhDTH9QcNNQ=427", "authors": ["TLDR Newsletter"], "title": "Claude Skills are awesome, maybe a bigger deal than MCP", "comment": "Source: TLDR Newsletter, Date: 2025-10-17, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F16%2Fclaude-skills%2F%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/570AzSPWIwsw9U-UpMCF363OsAdX8AG9YhDTH9QcNNQ=427", "summary": "Claude Skills are awesome, maybe a bigger deal than MCP (5 minute read) Anthropic's new Skills feature consumes only dozens of tokens per skill until needed compared to MCPs that can burn tens of thousands upfront. The design is incredibly simple: drop instructions in a folder and Claude figures out when to use them. This token efficiency and model-agnostic approach (any coding agent can read the files) will trigger a \"Cambrian explosion\" that makes the MCP rush look pedestrian, especially si...", "source": "tldr", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "topics": "Error"}}
{"id": "tldr.2510.bd1917c7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fswe-grep%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/m7F9UWhmzKk5t1TTVeToPQ5DIV-zb_9rRJbNl0qSgGE=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fswe-grep%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/m7F9UWhmzKk5t1TTVeToPQ5DIV-zb_9rRJbNl0qSgGE=427", "authors": ["TLDR Newsletter"], "title": "Introducing SWE-grep and SWE-grep-mini: RL for Multi-Turn, Fast Context Retrieval", "comment": "Source: TLDR Newsletter, Date: 2025-10-17, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fswe-grep%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/m7F9UWhmzKk5t1TTVeToPQ5DIV-zb_9rRJbNl0qSgGE=427", "summary": "Introducing SWE-grep and SWE-grep-mini: RL for Multi-Turn, Fast Context Retrieval (10 minute read) Cognition trained specialized models that retrieve code context 10x faster than frontier models by using RL to issue up to 8 parallel tool calls per turn instead of the sequential searches typical agents use. Speed matters more than the industry realizes because it allows developers to stay in a flow state.", "source": "tldr", "AI": {"tldr": "SWE-grep\u548cSWE-grep-mini\u662f\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u591a\u8f6e\u5feb\u901f\u4ee3\u7801\u4e0a\u4e0b\u6587\u68c0\u7d22\u7684\u4e13\u95e8\u6a21\u578b\uff0c\u6bd4\u524d\u6cbf\u6a21\u578b\u5feb10\u500d", "motivation": "\u5f00\u53d1\u901f\u5ea6\u66f4\u5feb\u7684\u4ee3\u7801\u4e0a\u4e0b\u6587\u68c0\u7d22\u5de5\u5177\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u4fdd\u6301\u6d41\u7545\u72b6\u6001\uff0c\u56e0\u4e3a\u901f\u5ea6\u7684\u91cd\u8981\u6027\u88ab\u884c\u4e1a\u4f4e\u4f30", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e13\u95e8\u6a21\u578b\uff0c\u6bcf\u8f6e\u6700\u591a\u53d1\u51fa8\u4e2a\u5e76\u884c\u5de5\u5177\u8c03\u7528\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u4ee3\u7406\u4f7f\u7528\u7684\u987a\u5e8f\u641c\u7d22", "result": "\u68c0\u7d22\u901f\u5ea6\u6bd4\u524d\u6cbf\u6a21\u578b\u5feb10\u500d", "conclusion": "\u5e76\u884c\u5de5\u5177\u8c03\u7528\u548c\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u4ee3\u7801\u4e0a\u4e0b\u6587\u68c0\u7d22\u901f\u5ea6", "topic": "swe application"}}
{"id": "tldr.2510.6d18c2ff", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2510.13786%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/sYj0T9nWmCsBOX8lGBhqv1IbDCdsAroQHZAluyfEWc0=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2510.13786%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/sYj0T9nWmCsBOX8lGBhqv1IbDCdsAroQHZAluyfEWc0=427", "authors": ["TLDR Newsletter"], "title": "The Art of Scaling Reinforcement Learning Compute for LLMs", "comment": "Source: TLDR Newsletter, Date: 2025-10-17, Reading time: 24 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2510.13786%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/sYj0T9nWmCsBOX8lGBhqv1IbDCdsAroQHZAluyfEWc0=427", "summary": "The Art of Scaling Reinforcement Learning Compute for LLMs (24 minute read) ScaleRL is a framework for analyzing and predicting compute scaling in LLM reinforcement learning. Their large-scale study shows that stable training recipes yield predictable performance curves, aiding efficient resource allocation.", "source": "tldr", "AI": {"tldr": "ScaleRL\u6846\u67b6\u7528\u4e8e\u5206\u6790\u548c\u9884\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u6269\u5c55\uff0c\u7814\u7a76\u8868\u660e\u7a33\u5b9a\u7684\u8bad\u7ec3\u914d\u65b9\u80fd\u4ea7\u751f\u53ef\u9884\u6d4b\u7684\u6027\u80fd\u66f2\u7ebf\uff0c\u6709\u52a9\u4e8e\u9ad8\u6548\u8d44\u6e90\u5206\u914d\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u6269\u5c55\u95ee\u9898\uff0c\u65e8\u5728\u627e\u5230\u53ef\u9884\u6d4b\u7684\u6027\u80fd\u6269\u5c55\u89c4\u5f8b\uff0c\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "method": "\u5f00\u53d1ScaleRL\u6846\u67b6\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u7814\u7a76\u5206\u6790\u7a33\u5b9a\u8bad\u7ec3\u914d\u65b9\u4e0b\u7684\u6027\u80fd\u6269\u5c55\u89c4\u5f8b\u3002", "result": "\u7814\u7a76\u8868\u660e\u7a33\u5b9a\u8bad\u7ec3\u914d\u65b9\u80fd\u591f\u4ea7\u751f\u53ef\u9884\u6d4b\u7684\u6027\u80fd\u66f2\u7ebf\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u3002", "conclusion": "\u901a\u8fc7ScaleRL\u6846\u67b6\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u9884\u6d4bLLM\u5f3a\u5316\u5b66\u4e60\u7684\u8ba1\u7b97\u6269\u5c55\u884c\u4e3a\uff0c\u4e3a\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.293e8cf9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2Fmanus-1.5-release%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/ddKH7NpYukfGSAyFTzMG6r1YykqVALWT6YGcqny3kDU=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2Fmanus-1.5-release%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/ddKH7NpYukfGSAyFTzMG6r1YykqVALWT6YGcqny3kDU=427", "authors": ["TLDR Newsletter"], "title": "Introducing Manus 1.5", "comment": "Source: TLDR Newsletter, Date: 2025-10-17, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2Fmanus-1.5-release%3Futm_source=tldrai/1/01000199f251dbb4-b43fa1c9-3005-42f4-b5a1-1c2683e0c30a-000000/ddKH7NpYukfGSAyFTzMG6r1YykqVALWT6YGcqny3kDU=427", "summary": "Introducing Manus 1.5 (3 minute read) Manus 1.5 adds full-stack web development that handles backend infrastructure, databases, QA, and user authentication entirely, achieved entirely through natural language. The agent can test apps by itself and fix issues autonomously.", "source": "tldr", "AI": {"tldr": "Manus 1.5\u662f\u4e00\u4e2a\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5168\u6808Web\u5f00\u53d1\u4efb\u52a1\u7684AI\u4ee3\u7406\uff0c\u5305\u62ec\u540e\u7aef\u57fa\u7840\u8bbe\u65bd\u3001\u6570\u636e\u5e93\u3001\u8d28\u91cf\u4fdd\u8bc1\u548c\u7528\u6237\u8ba4\u8bc1\uff0c\u5e76\u80fd\u81ea\u4e3b\u6d4b\u8bd5\u548c\u4fee\u590d\u5e94\u7528\u95ee\u9898\u3002", "motivation": "\u65e8\u5728\u7b80\u5316\u5168\u6808Web\u5f00\u53d1\u6d41\u7a0b\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u5b8c\u5168\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6765\u6784\u5efa\u3001\u6d4b\u8bd5\u548c\u7ef4\u62a4Web\u5e94\u7528\uff0c\u964d\u4f4e\u5f00\u53d1\u95e8\u69db\u3002", "method": "\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5c06\u5f00\u53d1\u8005\u7684\u9700\u6c42\u8f6c\u5316\u4e3a\u5b8c\u6574\u7684Web\u5e94\u7528\u5f00\u53d1\u6d41\u7a0b\uff0c\u5305\u62ec\u540e\u7aef\u67b6\u6784\u3001\u6570\u636e\u5e93\u8bbe\u8ba1\u3001\u7528\u6237\u8ba4\u8bc1\u7cfb\u7edf\u7b49\uff0c\u5e76\u96c6\u6210\u81ea\u52a8\u5316\u6d4b\u8bd5\u548c\u95ee\u9898\u4fee\u590d\u529f\u80fd\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u80fd\u591f\u5904\u7406\u5168\u6808Web\u5f00\u53d1\u4efb\u52a1\u7684AI\u4ee3\u7406\uff0c\u5b9e\u73b0\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u5230\u5b8c\u6574\u5e94\u7528\u7684\u65e0\u7f1d\u8f6c\u6362\uff0c\u5e76\u5177\u5907\u81ea\u4e3b\u6d4b\u8bd5\u548c\u4fee\u590d\u80fd\u529b\u3002", "conclusion": "Manus 1.5\u5c55\u793a\u4e86AI\u4ee3\u7406\u5728\u5168\u6808Web\u5f00\u53d1\u9886\u57df\u7684\u6f5c\u529b\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u6613\u7528\u7684\u5f00\u53d1\u5de5\u5177\u3002", "topic": "swe application"}}
{"id": "tldr.2510.2d649c8a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ssp.sh%2Fblog%2Fagentic-data-modeling%2F%3Futm_source=tldrdata/1/0100019a011612f4-a86fb1c7-5b39-4993-9228-e246720b2c59-000000/qFy9NWBs0cuB6IQu6l6ng12eAh0W3GwXkij2mExuXxE=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ssp.sh%2Fblog%2Fagentic-data-modeling%2F%3Futm_source=tldrdata/1/0100019a011612f4-a86fb1c7-5b39-4993-9228-e246720b2c59-000000/qFy9NWBs0cuB6IQu6l6ng12eAh0W3GwXkij2mExuXxE=427", "authors": ["TLDR Newsletter"], "title": "Data Modeling for the Agentic Era: Semantics, Speed, and Stewardship", "comment": "Source: TLDR Newsletter, Date: 2025-10-20, Reading time: 27 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ssp.sh%2Fblog%2Fagentic-data-modeling%2F%3Futm_source=tldrdata/1/0100019a011612f4-a86fb1c7-5b39-4993-9228-e246720b2c59-000000/qFy9NWBs0cuB6IQu6l6ng12eAh0W3GwXkij2mExuXxE=427", "summary": "Data Modeling for the Agentic Era: Semantics, Speed, and Stewardship (27 minute read) AI agents can generate queries or dashboards in seconds, but without structure, they're just guessing. Agentic workflows should build on three essential pillars: Semantics, a curated metrics layer where agents query well-defined business entities and measures; Speed, sub-second analytics so humans can instantly verify AI outputs; and Stewardship - guardrails, human oversight, and versions that keep agents re...", "source": "tldr", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faAI\u4ee3\u7406\u65f6\u4ee3\u6570\u636e\u5efa\u6a21\u7684\u4e09\u4e2a\u6838\u5fc3\u652f\u67f1\uff1a\u8bed\u4e49\u5c42\u3001\u901f\u5ea6\u548c\u76d1\u7ba1\uff0c\u4ee5\u89e3\u51b3AI\u4ee3\u7406\u5728\u7f3a\u4e4f\u7ed3\u6784\u5316\u6570\u636e\u65f6\u53ea\u80fd\u731c\u6d4b\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u80fd\u591f\u5feb\u901f\u751f\u6210\u67e5\u8be2\u548c\u4eea\u8868\u76d8\uff0c\u4f46\u7f3a\u4e4f\u7ed3\u6784\u5316\u6570\u636e\u652f\u6301\u4f1a\u5bfc\u81f4\u7ed3\u679c\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u9760\u7684\u6570\u636e\u5efa\u6a21\u6846\u67b6\u6765\u652f\u6301\u4ee3\u7406\u5de5\u4f5c\u6d41\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e09\u4e2a\u652f\u67f1\u7684\u6570\u636e\u5efa\u6a21\u65b9\u6cd5\uff1a\u8bed\u4e49\u5c42\uff08\u5b9a\u4e49\u4e1a\u52a1\u5b9e\u4f53\u548c\u6307\u6807\u7684\u5ea6\u91cf\u5c42\uff09\u3001\u901f\u5ea6\uff08\u4e9a\u79d2\u7ea7\u5206\u6790\u9a8c\u8bc1\uff09\u3001\u76d1\u7ba1\uff08\u62a4\u680f\u3001\u4eba\u5de5\u76d1\u7763\u548c\u7248\u672c\u63a7\u5236\uff09\u3002", "result": "\u901a\u8fc7\u8fd9\u4e09\u4e2a\u652f\u67f1\uff0cAI\u4ee3\u7406\u80fd\u591f\u5728\u7ed3\u6784\u5316\u6570\u636e\u57fa\u7840\u4e0a\u751f\u6210\u53ef\u9760\u7684\u67e5\u8be2\u548c\u4eea\u8868\u76d8\uff0c\u540c\u65f6\u786e\u4fdd\u4eba\u7c7b\u80fd\u591f\u5feb\u901f\u9a8c\u8bc1\u548c\u76d1\u7ba1\u3002", "conclusion": "\u5728AI\u4ee3\u7406\u65f6\u4ee3\uff0c\u6210\u529f\u7684\u6570\u636e\u5efa\u6a21\u9700\u8981\u8bed\u4e49\u3001\u901f\u5ea6\u548c\u76d1\u7ba1\u4e09\u4e2a\u652f\u67f1\u7684\u534f\u540c\u4f5c\u7528\uff0c\u4ee5\u786e\u4fdd\u4ee3\u7406\u8f93\u51fa\u7684\u53ef\u9760\u6027\u548c\u53ef\u63a7\u6027\u3002", "topic": "agent analysis"}}
