{"id": "2602.04910", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.04910", "abs": "https://arxiv.org/abs/2602.04910", "authors": ["Nongyu Di", "Tianyu Chen", "Shan Lu", "Shuai Lu", "Yeyun Gong", "Peng Cheng", "Jacob R. Lorch", "Yuan Yao", "Xiaoxing Ma"], "title": "Reducing the Costs of Proof Synthesis on Rust Systems by Scaling Up a Seed Training Set", "comment": null, "summary": "Large Language Models (LLMs) are widely used for code generation. However, the correctness of code generated by LLMs remains a concern. A potential remedy to this concern is to have LLMs generate formal correctness proofs along with such code. However, compared with code generation, code-proof generation requires much higher reasoning capability and has much less existing data to learn from. In this paper, we present VeruSyn, a data synthesis pipeline for Verus, a state-of-the-art verification tool for system software written in Rust. Through self-synthesis and tutorial-based synthesis, VeruSyn achieves much larger scale and Verus-feature coverage than previous data-synthesis techniques designed for Verus; VeruSyn also supplements its dataset with long-chain-of-thought (CoT) data through agent trajectory synthesis. With VeruSyn, we synthesize the largest set of Verus verified programs: 6.9 million Rust programs, each with a formal specification and a proof that it meets that specification. This dataset lets us create a fine-tuned Qwen2.5-Coder-32B-Instruct model with appealing cost-proof tradeoff compared with state-of-the-art commercial models like Claude Sonnet 4.5. It also significantly outperforms models like o4-mini and previously proposed research models.", "AI": {"tldr": "VeruSyn\u662f\u4e00\u4e2a\u4e3aRust\u9a8c\u8bc1\u5de5\u5177Verus\u8bbe\u8ba1\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u81ea\u5408\u6210\u548c\u6559\u7a0b\u5408\u6210\u751f\u6210\u5927\u91cf\u5e26\u5f62\u5f0f\u8bc1\u660e\u7684Rust\u7a0b\u5e8f\uff0c\u521b\u5efa\u4e86690\u4e07\u4e2a\u9a8c\u8bc1\u7a0b\u5e8f\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u51fa\u4f18\u4e8e\u5546\u4e1a\u6a21\u578b\u7684\u4ee3\u7801\u8bc1\u660e\u751f\u6210\u6a21\u578b\u3002", "motivation": "LLM\u751f\u6210\u7684\u4ee3\u7801\u6b63\u786e\u6027\u5b58\u5728\u95ee\u9898\uff0c\u867d\u7136\u8ba9LLM\u540c\u65f6\u751f\u6210\u5f62\u5f0f\u8bc1\u660e\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u4ee3\u7801\u8bc1\u660e\u751f\u6210\u9700\u8981\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u4e14\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\u3002\u7279\u522b\u662f\u9488\u5bf9Verus\uff08Rust\u7cfb\u7edf\u8f6f\u4ef6\u9a8c\u8bc1\u5de5\u5177\uff09\u7684\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u3002", "method": "1. \u81ea\u5408\u6210\uff1a\u4ece\u73b0\u6709Verus\u7a0b\u5e8f\u4e2d\u751f\u6210\u65b0\u53d8\u4f53\uff1b2. \u6559\u7a0b\u5408\u6210\uff1a\u4eceVerus\u6559\u7a0b\u4e2d\u63d0\u53d6\u6a21\u5f0f\u751f\u6210\u65b0\u7a0b\u5e8f\uff1b3. \u4ee3\u7406\u8f68\u8ff9\u5408\u6210\uff1a\u8865\u5145\u957f\u94fe\u601d\u7ef4\u6570\u636e\u3002\u901a\u8fc7\u8fd9\u4e9b\u65b9\u6cd5\u5408\u6210690\u4e07\u4e2a\u5e26\u5f62\u5f0f\u89c4\u8303\u548c\u8bc1\u660e\u7684Rust\u7a0b\u5e8f\u3002", "result": "\u521b\u5efa\u4e86\u6700\u5927\u7684Verus\u9a8c\u8bc1\u7a0b\u5e8f\u6570\u636e\u96c6\uff08690\u4e07\u4e2a\u7a0b\u5e8f\uff09\uff0c\u57fa\u4e8e\u6b64\u5fae\u8c03\u7684Qwen2.5-Coder-32B-Instruct\u6a21\u578b\u5728\u6210\u672c-\u8bc1\u660e\u6743\u8861\u4e0a\u4f18\u4e8eClaude Sonnet 4.5\u7b49\u5546\u4e1a\u6a21\u578b\uff0c\u4e5f\u663e\u8457\u4f18\u4e8eo4-mini\u548c\u4e4b\u524d\u7684\u7814\u7a76\u6a21\u578b\u3002", "conclusion": "VeruSyn\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\u89e3\u51b3\u4e86\u4ee3\u7801\u8bc1\u660e\u751f\u6210\u7684\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u521b\u5efa\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4f7f\u5f97\u8bad\u7ec3\u51fa\u7684\u6a21\u578b\u5728\u4ee3\u7801\u8bc1\u660e\u751f\u6210\u4efb\u52a1\u4e0a\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u5546\u4e1a\u6a21\u578b\u7684\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2602.04935", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04935", "abs": "https://arxiv.org/abs/2602.04935", "authors": ["Youjin Wang", "Run Zhou", "Rong Fu", "Shuaishuai Cao", "Hongwei Zeng", "Jiaxuan Lu", "Sicheng Fan", "Jiaqiao Zhao", "Liangming Pan"], "title": "ASA: Activation Steering for Tool-Calling Domain Adaptation", "comment": null, "summary": "For real-world deployment of general-purpose LLM agents, the core challenge is often not tool use itself, but efficient domain adaptation under rapidly evolving toolsets, APIs, and protocols. Repeated LoRA or SFT across domains incurs exponentially growing training and maintenance costs, while prompt or schema methods are brittle under distribution shift and complex interfaces. We propose \\textbf{Activation Steering Adapter (ASA}), a lightweight, inference-time, training-free mechanism that reads routing signals from intermediate activations and uses an ultra-light router to produce adaptive control strengths for precise domain alignment. Across multiple model scales and domains, ASA achieves LoRA-comparable adaptation with substantially lower overhead and strong cross-model transferability, making it ideally practical for robust, scalable, and efficient multi-domain tool ecosystems with frequent interface churn dynamics.", "AI": {"tldr": "\u63d0\u51faASA\uff08\u6fc0\u6d3b\u5f15\u5bfc\u9002\u914d\u5668\uff09\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u8bad\u7ec3\u3001\u63a8\u7406\u65f6\u673a\u5236\uff0c\u901a\u8fc7\u8bfb\u53d6\u4e2d\u95f4\u6fc0\u6d3b\u4fe1\u53f7\u5e76\u4f7f\u7528\u8d85\u8f7b\u8def\u7531\u5668\u4ea7\u751f\u81ea\u9002\u5e94\u63a7\u5236\u5f3a\u5ea6\uff0c\u5b9e\u73b0\u7cbe\u786e\u9886\u57df\u5bf9\u9f50\uff0c\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u591a\u9886\u57df\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u9ad8\u6548\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u901a\u7528LLM\u667a\u80fd\u4f53\u90e8\u7f72\u7684\u6838\u5fc3\u6311\u6218\u4e0d\u662f\u5de5\u5177\u4f7f\u7528\u672c\u8eab\uff0c\u800c\u662f\u5728\u5feb\u901f\u6f14\u53d8\u7684\u5de5\u5177\u96c6\u3001API\u548c\u534f\u8bae\u4e0b\u7684\u9ad8\u6548\u9886\u57df\u9002\u5e94\u3002\u4f20\u7edf\u65b9\u6cd5\u5982\u91cd\u590dLoRA\u6216SFT\u8bad\u7ec3\u6210\u672c\u6307\u6570\u589e\u957f\uff0c\u800c\u63d0\u793a\u6216\u6a21\u5f0f\u65b9\u6cd5\u5728\u5206\u5e03\u504f\u79fb\u548c\u590d\u6742\u63a5\u53e3\u4e0b\u8106\u5f31\u3002", "method": "\u63d0\u51faASA\uff08\u6fc0\u6d3b\u5f15\u5bfc\u9002\u914d\u5668\uff09\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u63a8\u7406\u65f6\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u673a\u5236\u3002\u5b83\u4ece\u4e2d\u95f4\u6fc0\u6d3b\u4e2d\u8bfb\u53d6\u8def\u7531\u4fe1\u53f7\uff0c\u4f7f\u7528\u8d85\u8f7b\u8def\u7531\u5668\u4ea7\u751f\u81ea\u9002\u5e94\u63a7\u5236\u5f3a\u5ea6\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u9886\u57df\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u548c\u9886\u57df\u4e2d\uff0cASA\u5b9e\u73b0\u4e86\u4e0eLoRA\u76f8\u5f53\u7684\u9002\u5e94\u6548\u679c\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5f00\u9500\uff0c\u5e76\u5177\u6709\u5f3a\u5927\u7684\u8de8\u6a21\u578b\u53ef\u8f6c\u79fb\u6027\u3002", "conclusion": "ASA\u4e3a\u5177\u6709\u9891\u7e41\u63a5\u53e3\u53d8\u5316\u7684\u7a33\u5065\u3001\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u591a\u9886\u57df\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u60f3\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.05014", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.05014", "abs": "https://arxiv.org/abs/2602.05014", "authors": ["Zhanli Li", "Huiwen Tian", "Lvzhou Luo", "Yixuan Cao", "Ping Luo"], "title": "DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search", "comment": "working in progress", "summary": "With the rapid progress of tool-using and agentic large language models (LLMs), Retrieval-Augmented Generation (RAG) is evolving from one-shot, passive retrieval into multi-turn, decision-driven evidence acquisition. Despite strong results in open-domain settings, existing agentic search frameworks commonly treat long documents as flat collections of chunks, underutilizing document-native priors such as hierarchical organization and sequential discourse structure. We introduce DeepRead, a structure-aware, multi-turn document reasoning agent that explicitly operationalizes these priors for long-document question answering. DeepRead leverages LLM-based OCR model to convert PDFs into structured Markdown that preserves headings and paragraph boundaries. It then indexes documents at the paragraph level and assigns each paragraph a coordinate-style metadata key encoding its section identity and in-section order. Building on this representation, DeepRead equips the LLM with two complementary tools: a Retrieve tool that localizes relevant paragraphs while exposing their structural coordinates (with lightweight scanning context), and a ReadSection tool that enables contiguous, order-preserving reading within a specified section and paragraph range. Our experiments demonstrate that DeepRead achieves significant improvements over Search-o1-style agentic search in document question answering. The synergistic effect between retrieval and reading tools is also validated. Our fine-grained behavioral analysis reveals a reading and reasoning paradigm resembling human-like ``locate then read'' behavior.", "AI": {"tldr": "DeepRead\u662f\u4e00\u4e2a\u7ed3\u6784\u611f\u77e5\u7684\u591a\u8f6e\u6587\u6863\u63a8\u7406\u4ee3\u7406\uff0c\u901a\u8fc7\u5229\u7528\u6587\u6863\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u987a\u5e8f\u8bdd\u8bed\u7ed3\u6784\uff0c\u5728\u957f\u6587\u6863\u95ee\u7b54\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u4ee3\u7406\u641c\u7d22\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u641c\u7d22\u6846\u67b6\u901a\u5e38\u5c06\u957f\u6587\u6863\u89c6\u4e3a\u6241\u5e73\u5316\u7684\u6587\u672c\u5757\u96c6\u5408\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6587\u6863\u56fa\u6709\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5982\u5c42\u6b21\u5316\u7ec4\u7ec7\u548c\u987a\u5e8f\u8bdd\u8bed\u7ed3\u6784\u3002\u968f\u7740\u5de5\u5177\u4f7f\u7528\u548c\u4ee3\u7406\u6027\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6b63\u5728\u4ece\u4e00\u6b21\u6027\u88ab\u52a8\u68c0\u7d22\u6f14\u53d8\u4e3a\u591a\u8f6e\u51b3\u7b56\u9a71\u52a8\u7684\u8bc1\u636e\u83b7\u53d6\u3002", "method": "DeepRead\u4f7f\u7528\u57fa\u4e8eLLM\u7684OCR\u6a21\u578b\u5c06PDF\u8f6c\u6362\u4e3a\u4fdd\u7559\u6807\u9898\u548c\u6bb5\u843d\u8fb9\u754c\u7684\u7ed3\u6784\u5316Markdown\u3002\u5728\u6bb5\u843d\u7ea7\u522b\u7d22\u5f15\u6587\u6863\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6bb5\u843d\u5206\u914d\u7f16\u7801\u5176\u7ae0\u8282\u8eab\u4efd\u548c\u7ae0\u8282\u5185\u987a\u5e8f\u7684\u5750\u6807\u5f0f\u5143\u6570\u636e\u952e\u3002\u4e3aLLM\u914d\u5907\u4e24\u4e2a\u4e92\u8865\u5de5\u5177\uff1aRetrieve\u5de5\u5177\u5b9a\u4f4d\u76f8\u5173\u6bb5\u843d\u5e76\u66b4\u9732\u5176\u7ed3\u6784\u5750\u6807\uff0cReadSection\u5de5\u5177\u5728\u6307\u5b9a\u7ae0\u8282\u548c\u6bb5\u843d\u8303\u56f4\u5185\u5b9e\u73b0\u8fde\u7eed\u3001\u987a\u5e8f\u4fdd\u6301\u7684\u9605\u8bfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDeepRead\u5728\u6587\u6863\u95ee\u7b54\u4e2d\u663e\u8457\u4f18\u4e8eSearch-o1\u98ce\u683c\u7684\u4ee3\u7406\u641c\u7d22\u3002\u9a8c\u8bc1\u4e86\u68c0\u7d22\u548c\u9605\u8bfb\u5de5\u5177\u4e4b\u95f4\u7684\u534f\u540c\u6548\u5e94\u3002\u7ec6\u7c92\u5ea6\u884c\u4e3a\u5206\u6790\u63ed\u793a\u4e86\u7c7b\u4f3c\u4eba\u7c7b\"\u5b9a\u4f4d\u7136\u540e\u9605\u8bfb\"\u7684\u9605\u8bfb\u63a8\u7406\u8303\u5f0f\u3002", "conclusion": "DeepRead\u901a\u8fc7\u663e\u5f0f\u64cd\u4f5c\u6587\u6863\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u987a\u5e8f\u8bdd\u8bed\u7ed3\u6784\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u957f\u6587\u6863\u95ee\u7b54\u3002\u5176\u7ed3\u6784\u611f\u77e5\u7684\u591a\u8f6e\u6587\u6863\u63a8\u7406\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6241\u5e73\u5316\u68c0\u7d22\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.04893", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.04893", "abs": "https://arxiv.org/abs/2602.04893", "authors": ["Licheng Pan", "Yunsheng Lu", "Jiexi Liu", "Jialing Tao", "Haozhe Feng", "Hui Xue", "Zhixuan Chu", "Kui Ren"], "title": "A Causal Perspective for Enhancing Jailbreak Attack and Defense", "comment": null, "summary": "Uncovering the mechanisms behind \"jailbreaks\" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as \"Positive Character\" and \"Number of Task Steps\", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.", "AI": {"tldr": "\u63d0\u51faCausal Analyst\u6846\u67b6\uff0c\u4f7f\u7528\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u8bc6\u522bLLM\u8d8a\u72f1\u7684\u76f4\u63a5\u539f\u56e0\uff0c\u5e76\u5e94\u7528\u4e8e\u653b\u51fb\u589e\u5f3a\u548c\u9632\u5fa1\u5efa\u8bae", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5206\u6790\u6f5c\u5728\u8868\u793a\uff0c\u5ffd\u7565\u4e86\u53ef\u89e3\u91ca\u63d0\u793a\u7279\u5f81\u4e0e\u8d8a\u72f1\u53d1\u751f\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3\u8d8a\u72f1\u673a\u5236\u4ee5\u589e\u5f3aLLM\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027", "method": "\u63d0\u51faCausal Analyst\u6846\u67b6\uff0c\u7ed3\u5408LLM\u63d0\u793a\u7f16\u7801\u548cGNN\u56e0\u679c\u56fe\u5b66\u4e60\uff0c\u6784\u5efa\u5305\u542b35k\u8d8a\u72f1\u5c1d\u8bd5\u7684\u6570\u636e\u96c6\uff0c\u6807\u6ce837\u4e2a\u4eba\u7c7b\u53ef\u8bfb\u63d0\u793a\u7279\u5f81\uff0c\u91cd\u5efa\u4ece\u63d0\u793a\u7279\u5f81\u5230\u8d8a\u72f1\u54cd\u5e94\u7684\u56e0\u679c\u8def\u5f84", "result": "\u53d1\u73b0\u7279\u5b9a\u7279\u5f81\uff08\u5982\"Positive Character\"\u548c\"Number of Task Steps\"\uff09\u662f\u8d8a\u72f1\u7684\u76f4\u63a5\u56e0\u679c\u9a71\u52a8\u56e0\u7d20\uff1bJailbreaking Enhancer\u663e\u8457\u63d0\u5347\u653b\u51fb\u6210\u529f\u7387\uff0cGuardrail Advisor\u6709\u6548\u63d0\u53d6\u6df7\u6dc6\u67e5\u8be2\u4e2d\u7684\u6076\u610f\u610f\u56fe", "conclusion": "\u4ece\u56e0\u679c\u89d2\u5ea6\u5206\u6790\u8d8a\u72f1\u7279\u5f81\u662f\u63d0\u9ad8LLM\u53ef\u9760\u6027\u7684\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u56e0\u679c\u5206\u6790\u4f18\u4e8e\u975e\u56e0\u679c\u65b9\u6cd5", "topic": "agent analysis"}}
{"id": "2602.05048", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05048", "abs": "https://arxiv.org/abs/2602.05048", "authors": ["Zeyu Fang", "Tian Lan", "Mahdi Imani"], "title": "MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation", "comment": null, "summary": "Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.", "AI": {"tldr": "MINT\u6846\u67b6\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u6811\u63a8\u7406\u77e5\u8bc6\u7f3a\u53e3\uff0c\u5229\u7528\u81ea\u535a\u5f08\u4f18\u5316AI\u4ee3\u7406\u7684\u8be2\u95ee\u7b56\u7565\uff0c\u5728\u5bf9\u8c61\u9a71\u52a8\u7684\u89c4\u5212\u4e2d\u4e3b\u52a8\u83b7\u53d6\u4eba\u7c7b\u8f93\u5165\uff0c\u5b9e\u73b0\u8fd1\u4e13\u5bb6\u7ea7\u6027\u80fd\u3002", "motivation": "\u5f00\u653e\u4e16\u754c\u89c4\u5212\u4e2d\u5b58\u5728\u5404\u79cd\u4e0d\u5b8c\u5168\u4fe1\u606f\u548c\u672a\u77e5\u56e0\u7d20\uff08\u5982\u5bf9\u8c61\u3001\u4eba\u7c7b\u76ee\u6807/\u610f\u56fe\uff09\uff0c\u5bfc\u81f4\u8054\u5408\u89c4\u5212\u4e2d\u7684\u77e5\u8bc6\u7f3a\u53e3\uff0c\u9700\u8981AI\u4ee3\u7406\u4e3b\u52a8\u83b7\u53d6\u4eba\u7c7b\u8f93\u5165\u6765\u4f18\u5316\u89c4\u5212\u6027\u80fd\u3002", "method": "\u63d0\u51faMINT\uff08\u6700\u5c0f\u4fe1\u606f\u795e\u7ecf\u7b26\u53f7\u6811\uff09\u6846\u67b6\uff1a1\uff09\u6784\u5efa\u7b26\u53f7\u6811\u6a21\u62df\u53ef\u80fd\u7684\u4eba\u673a\u4ea4\u4e92\uff1b2\uff09\u4f7f\u7528\u795e\u7ecf\u89c4\u5212\u7b56\u7565\u8bc4\u4f30\u77e5\u8bc6\u7f3a\u53e3\u5bfc\u81f4\u7684\u89c4\u5212\u7ed3\u679c\u4e0d\u786e\u5b9a\u6027\uff1b3\uff09\u5229\u7528LLM\u641c\u7d22\u548c\u603b\u7ed3MINT\u63a8\u7406\u8fc7\u7a0b\uff0c\u751f\u6210\u6700\u4f18\u8be2\u95ee\u96c6\uff1b4\uff09\u901a\u8fc7\u81ea\u535a\u5f08\u4f18\u5316AI\u4ee3\u7406\u7684\u8be2\u95ee\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u6d89\u53ca\u672a\u77e5/\u672a\u89c1\u5bf9\u8c61\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMINT\u57fa\u4e8e\u89c4\u5212\u7684\u65b9\u6848\u901a\u8fc7\u6709\u9650\u7684\u95ee\u9898\u6570\u91cf\u5b9e\u73b0\u4e86\u8fd1\u4e13\u5bb6\u7ea7\u7684\u56de\u62a5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5956\u52b1\u548c\u6210\u529f\u7387\u3002", "conclusion": "MINT\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u5bf9\u8c61\u9a71\u52a8\u89c4\u5212\u4e2d\u7684\u77e5\u8bc6\u7f3a\u53e3\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u4e3b\u52a8\u8be2\u95ee\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u7684\u4eba\u673a\u534f\u4f5c\u89c4\u5212\uff0c\u5728\u590d\u6742\u5f00\u653e\u4e16\u754c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agent analysis"}}
{"id": "2602.04903", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04903", "abs": "https://arxiv.org/abs/2602.04903", "authors": ["Eitan Sprejer", "Oscar Agust\u00edn Stanchi", "Mar\u00eda Victoria Carro", "Denise Alejandra Mester", "Iv\u00e1n Arcuschin"], "title": "Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering", "comment": "12 pages, 5 figures", "summary": "Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifically, we evaluate Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries (covering innocuous and safety-relevant behaviors) on 171 Massive Multitask Language Understanding (MMLU) questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control. Our findings show that Auto Steer successfully modifies target behaviors (achieving scores of 3.33 vs. 2.98 for prompting on Llama-8B and 3.57 vs. 3.10 on Llama-70B), but causes dramatic performance degradation: accuracy on the MMLU questions drops from 66% to 46% on Llama-8B and 87% to 73% on Llama-70B, with coherence falling from 4.62 to 2.24 and 4.94 to 3.89 respectively. Simple prompting achieves the best overall balance. These findings highlight limitations of current feature steering methods for practical deployment where task performance cannot be sacrificed. More broadly, our work demonstrates that mechanistic control methods face fundamental capability-behavior trade-offs that must be empirically characterized before deployment.", "AI": {"tldr": "\u7279\u5f81\u5bfc\u5411\u65b9\u6cd5\u80fd\u6709\u6548\u63a7\u5236LLM\u884c\u4e3a\u4f46\u4e25\u91cd\u635f\u5bb3\u6a21\u578b\u6027\u80fd\uff0c\u63d0\u793a\u5de5\u7a0b\u5728\u6027\u80fd\u4e0e\u884c\u4e3a\u63a7\u5236\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861", "motivation": "\u7279\u5f81\u5bfc\u5411\u4f5c\u4e3a\u76f4\u63a5\u64cd\u7eb5\u5185\u90e8\u8868\u5f81\u7684LLM\u63a7\u5236\u65b9\u6cd5\uff0c\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u679c\u548c\u6f5c\u5728\u6027\u80fd\u6743\u8861\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u5b9e\u8bc1\u8bc4\u4f30", "method": "\u8bc4\u4f30Goodfire\u7684Auto Steer\u7279\u5f81\u5bfc\u5411\u65b9\u6cd5\u4e0e\u63d0\u793a\u5de5\u7a0b\u57fa\u7ebf\uff0c\u572814\u4e2a\u5bfc\u5411\u67e5\u8be2\uff08\u6db5\u76d6\u65e0\u5bb3\u548c\u5b89\u5168\u76f8\u5173\u884c\u4e3a\uff09\u4e0a\uff0c\u4f7f\u7528Llama-8B\u548cLlama-70B\u6a21\u578b\uff0c\u5728171\u4e2aMMLU\u95ee\u9898\u4e0a\u6d4b\u91cf\u51c6\u786e\u6027\u3001\u8fde\u8d2f\u6027\u548c\u884c\u4e3a\u63a7\u5236", "result": "Auto Steer\u6210\u529f\u4fee\u6539\u76ee\u6807\u884c\u4e3a\uff08Llama-8B\u5f97\u52063.33 vs 2.98\uff0cLlama-70B\u5f97\u52063.57 vs 3.10\uff09\uff0c\u4f46\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1aMMLU\u51c6\u786e\u7387\u4ece66%\u964d\u81f346%\uff088B\uff09\u548c87%\u964d\u81f373%\uff0870B\uff09\uff0c\u8fde\u8d2f\u6027\u4ece4.62\u964d\u81f32.24\u548c4.94\u964d\u81f33.89", "conclusion": "\u5f53\u524d\u7279\u5f81\u5bfc\u5411\u65b9\u6cd5\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7b80\u5355\u63d0\u793a\u5de5\u7a0b\u5728\u6027\u80fd\u4e0e\u884c\u4e3a\u63a7\u5236\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\uff0c\u673a\u68b0\u63a7\u5236\u65b9\u6cd5\u9762\u4e34\u57fa\u672c\u7684\u80fd\u529b-\u884c\u4e3a\u6743\u8861", "topic": "agent analysis"}}
{"id": "2602.05073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05073", "abs": "https://arxiv.org/abs/2602.05073", "authors": ["Changdae Oh", "Seongheon Park", "To Eun Kim", "Jiatong Li", "Wendi Li", "Samuel Yeh", "Xuefeng Du", "Hamed Hassani", "Paul Bogdan", "Dawn Song", "Sharon Li"], "title": "Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents", "comment": null, "summary": "Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting \"interactivity\" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u901a\u7528\u7684\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u5c06\u4f20\u7edf\u4e0d\u786e\u5b9a\u6027\u79ef\u7d2f\u89c6\u89d2\u8f6c\u53d8\u4e3a\u6761\u4ef6\u6027\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u8fc7\u7a0b\uff0c\u5f3a\u8c03\u667a\u80fd\u4f53\u4ea4\u4e92\u6027\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "motivation": "\u5f53\u524d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u8f6e\u95ee\u7b54\u573a\u666f\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u4e8e\u590d\u6742\u4ea4\u4e92\u4efb\u52a1\u4e2d\uff0c\u9700\u8981\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\u6765\u9002\u5e94\u667a\u80fd\u4f53\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u4ea4\u4e92\u7279\u6027\u3002", "method": "\u63d0\u51fa\u9996\u4e2a\u901a\u7528\u7684\u667a\u80fd\u4f53\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u8bbe\u7f6e\u7edf\u4e00\u7eb3\u5165\u8be5\u6846\u67b6\uff1b\u4ece\u4f20\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u79ef\u7d2f\u89c6\u89d2\u8f6c\u5411\u6761\u4ef6\u6027\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u8fc7\u7a0b\uff0c\u5f3a\u8c03\u667a\u80fd\u4f53\u52a8\u4f5c\u7684\u4ea4\u4e92\u6027\u5bf9\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u7684\u4f5c\u7528\u3002", "result": "\u5efa\u7acb\u4e86\u80fd\u591f\u6db5\u76d6\u5e7f\u6cdb\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u8bbe\u7f6e\u7684\u901a\u7528\u667a\u80fd\u4f53\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u6761\u4ef6\u6027\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u7684\u65b0\u89c6\u89d2\uff0c\u4e3a\u8bbe\u8ba1\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u6982\u5ff5\u6027\u6307\u5bfc\u6846\u67b6\u3002", "conclusion": "\u667a\u80fd\u4f53\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u9700\u8981\u4ece\u4f20\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u79ef\u7d2f\u89c6\u89d2\u8f6c\u5411\u6761\u4ef6\u6027\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u8fc7\u7a0b\uff0c\u5f3a\u8c03\u4ea4\u4e92\u6027\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u8be5\u6846\u67b6\u5bf9\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u548c\u9886\u57df\u7279\u5b9a\u5e94\u7528\u5177\u6709\u5b9e\u9645\u6307\u5bfc\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2602.05242", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05242", "abs": "https://arxiv.org/abs/2602.05242", "authors": ["Chenhui Mao", "Yuanting Lei", "Zhixiang Wei", "Ming Liang", "Zhixiang Wang", "Jingxuan Xu", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering", "comment": null, "summary": "Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.", "AI": {"tldr": "\u63d0\u51faEGSS\u6846\u67b6\uff0c\u901a\u8fc7\u71b5\u5f15\u5bfc\u81ea\u9002\u5e94\u641c\u7d22\u548c\u6d4b\u8bd5\u5957\u4ef6\u589e\u5f3a\uff0c\u89e3\u51b3\u4ee3\u7406\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u5728SWE\u4efb\u52a1\u4e0a\u63d0\u5347\u6027\u80fd5-10%\uff0c\u540c\u65f6\u51cf\u5c1128%\u63a8\u7406token\u4f7f\u7528\u3002", "motivation": "\u4ee3\u7406\u6d4b\u8bd5\u65f6\u6269\u5c55(TTS)\u5728\u4ee3\u7801\u751f\u6210\u548cbug\u4fee\u590d\u7b49\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u53d7\u9650\uff1a1) \u5927\u578b\u96c6\u6210\u6a21\u578b\u90e8\u7f72\u6210\u672c\u9ad8\uff1b2) \u7f3a\u4e4f\u53ef\u9760\u673a\u5236\u9009\u62e9\u6700\u4f18\u5019\u9009\u65b9\u6848\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u71b5\u5f15\u5bfc\u9010\u6b65\u6269\u5c55(EGSS)\u6846\u67b6\uff0c\u901a\u8fc7\u71b5\u5f15\u5bfc\u81ea\u9002\u5e94\u641c\u7d22\u52a8\u6001\u5e73\u8861\u6548\u7387\u4e0e\u6548\u679c\uff0c\u7ed3\u5408\u9c81\u68d2\u7684\u6d4b\u8bd5\u5957\u4ef6\u589e\u5f3a\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728SWE-Bench-Verified\u4e0a\uff0cEGSS\u5c06Kimi-K2-Intruct\u89e3\u51b3\u7387\u4ece63.2%\u63d0\u5347\u523072.2%\uff0cGLM-4.6\u4ece65.8%\u63d0\u5347\u523074.6%\uff0c\u8fbe\u5230\u5f00\u6e90\u5927\u6a21\u578bSOTA\u6c34\u5e73\uff0c\u540c\u65f6\u51cf\u5c1128%\u63a8\u7406token\u4f7f\u7528\u3002", "conclusion": "EGSS\u6709\u6548\u89e3\u51b3\u4e86TTS\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u63d0\u5347\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u4ee3\u7406\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2602.05270", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05270", "abs": "https://arxiv.org/abs/2602.05270", "authors": ["Thanh Le-Cong", "Bach Le", "Toby Murray", "Michael Pradel", "Cristian Cadar"], "title": "PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models", "comment": null, "summary": "As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.", "AI": {"tldr": "PatchGuru\uff1a\u9996\u4e2a\u4ece\u771f\u5b9ePR\u4e2d\u81ea\u52a8\u63a8\u65ad\u53ef\u6267\u884c\u8865\u4e01\u89c4\u8303\u7684\u6280\u672f\uff0c\u901a\u8fc7LLM\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u63d0\u53d6\u5f00\u53d1\u8005\u610f\u56fe\uff0c\u5408\u6210\u8865\u4e01\u9884\u8a00\uff08\u8fd0\u884c\u65f6\u65ad\u8a00\uff09\uff0c\u7528\u4e8e\u9a8c\u8bc1\u8865\u4e01\u884c\u4e3a\uff0c\u5728400\u4e2aPython\u9879\u76eePR\u4e2d\u68c0\u6d4b\u523024\u4e2a\u771f\u5b9ebug\uff0c\u7cbe\u5ea60.62\u3002", "motivation": "\u8f6f\u4ef6\u7cfb\u7edf\u6f14\u5316\u4e2d\uff0c\u8865\u4e01\u53ef\u80fd\u65e0\u610f\u6539\u53d8\u7a0b\u5e8f\u884c\u4e3a\u3002\u7531\u4e8e\u56de\u5f52\u6d4b\u8bd5\u4e0d\u5b8c\u6574\u4e14\u8865\u4e01\u610f\u56fe\u63cf\u8ff0\u662f\u975e\u6b63\u5f0f\u7684\u81ea\u7136\u8bed\u8a00\uff0c\u9a8c\u8bc1\u8865\u4e01\u662f\u5426\u7b26\u5408\u9884\u671f\u8bed\u4e49\u5f88\u56f0\u96be\u3002\u9700\u8981\u81ea\u52a8\u5316\u6280\u672f\u6765\u63a8\u65ad\u53ef\u6267\u884c\u7684\u8865\u4e01\u89c4\u8303\u3002", "method": "PatchGuru\u5229\u7528LLM\u4ecePR\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u63d0\u53d6\u5f00\u53d1\u8005\u610f\u56fe\uff0c\u5408\u6210\u8865\u4e01\u9884\u8a00\u2014\u2014\u5728\u96c6\u6210\u8865\u4e01\u524d\u540e\u7248\u672c\u7684\u6bd4\u8f83\u7a0b\u5e8f\u4e2d\u8868\u8fbe\u7684\u8fd0\u884c\u65f6\u65ad\u8a00\u3002\u901a\u8fc7\u8fed\u4ee3\u6bd4\u8f83\u8865\u4e01\u524d\u540e\u884c\u4e3a\u3001\u8bc6\u522b\u8fdd\u89c4\u3001\u81ea\u5ba1\u8fc7\u6ee4\u4e0d\u4e00\u81f4\u6027\uff0c\u751f\u6210bug\u62a5\u544a\u3002", "result": "\u57284\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f00\u6e90Python\u9879\u76ee\u7684400\u4e2aPR\u4e0a\u8bc4\u4f30\uff0cPatchGuru\u62a5\u544a39\u4e2a\u8b66\u544a\uff0c\u7cbe\u5ea60.62\uff0c\u786e\u8ba424\u4e2a\u771f\u5b9ebug\uff08\u5305\u62ec12\u4e2a\u5148\u524d\u672a\u77e5\u7684bug\uff0c\u5176\u4e2d11\u4e2a\u88ab\u5f00\u53d1\u8005\u4fee\u590d\uff09\u3002\u76f8\u6bd4\u6700\u5148\u8fdb\u6280\u672fTestora\uff0c\u591a\u68c0\u6d4b17\u4e2abug\uff0824 vs 7\uff09\uff0c\u7cbe\u5ea6\u4ece0.32\u63d0\u5347\u52300.62\u3002\u5e73\u5747\u6bcf\u4e2aPR\u6210\u672c8.9\u5206\u949f\u548c0.07\u7f8e\u5143\u3002", "conclusion": "PatchGuru\u901a\u8fc7\u63d0\u4f9b\u53ef\u6267\u884c\u6587\u6863\u548c\u8865\u4e01\u610f\u56fe\u7684\u81ea\u52a8\u9a8c\u8bc1\uff0c\u8865\u5145\u4e86\u4ee3\u7801\u5ba1\u67e5\u548c\u56de\u5f52\u6d4b\u8bd5\u3002\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u8865\u4e01\u5f15\u5165\u7684bug\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002", "topic": "swe application"}}
{"id": "2602.05105", "categories": ["cs.AI", "cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05105", "abs": "https://arxiv.org/abs/2602.05105", "authors": ["Rohan Patil", "Jai Malegaonkar", "Xiao Jiang", "Andre Dion", "Gaurav S. Sukhatme", "Henrik I. Christensen"], "title": "GAMMS: Graph based Adversarial Multiagent Modeling Simulator", "comment": null, "summary": "As intelligent systems and multi-agent coordination become increasingly central to real-world applications, there is a growing need for simulation tools that are both scalable and accessible. Existing high-fidelity simulators, while powerful, are often computationally expensive and ill-suited for rapid prototyping or large-scale agent deployments. We present GAMMS (Graph based Adversarial Multiagent Modeling Simulator), a lightweight yet extensible simulation framework designed to support fast development and evaluation of agent behavior in environments that can be represented as graphs. GAMMS emphasizes five core objectives: scalability, ease of use, integration-first architecture, fast visualization feedback, and real-world grounding. It enables efficient simulation of complex domains such as urban road networks and communication systems, supports integration with external tools (e.g., machine learning libraries, planning solvers), and provides built-in visualization with minimal configuration. GAMMS is agnostic to policy type, supporting heuristic, optimization-based, and learning-based agents, including those using large language models. By lowering the barrier to entry for researchers and enabling high-performance simulations on standard hardware, GAMMS facilitates experimentation and innovation in multi-agent systems, autonomous planning, and adversarial modeling. The framework is open-source and available at https://github.com/GAMMSim/GAMMS/", "AI": {"tldr": "GAMMS\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u56fe\u57fa\u5bf9\u6297\u591a\u667a\u80fd\u4f53\u5efa\u6a21\u6a21\u62df\u5668\uff0c\u65e8\u5728\u4e3a\u9700\u8981\u53ef\u6269\u5c55\u6027\u548c\u6613\u7528\u6027\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u5feb\u901f\u5f00\u53d1\u548c\u8bc4\u4f30\u5de5\u5177\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u7cfb\u7edf\u548c\u591a\u667a\u80fd\u4f53\u534f\u8c03\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u65e2\u53ef\u6269\u5c55\u53c8\u6613\u4e8e\u4f7f\u7528\u7684\u6a21\u62df\u5de5\u5177\u3002\u73b0\u6709\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e0d\u9002\u5408\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u6216\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u90e8\u7f72\u3002", "method": "GAMMS\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u6a21\u62df\u6846\u67b6\uff0c\u652f\u6301\u5c06\u73af\u5883\u8868\u793a\u4e3a\u56fe\uff0c\u5f3a\u8c03\u4e94\u4e2a\u6838\u5fc3\u76ee\u6807\uff1a\u53ef\u6269\u5c55\u6027\u3001\u6613\u7528\u6027\u3001\u96c6\u6210\u4f18\u5148\u67b6\u6784\u3001\u5feb\u901f\u53ef\u89c6\u5316\u53cd\u9988\u548c\u73b0\u5b9e\u4e16\u754c\u57fa\u7840\u3002\u652f\u6301\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\uff0c\u5e76\u63d0\u4f9b\u5185\u7f6e\u53ef\u89c6\u5316\u529f\u80fd\u3002", "result": "GAMMS\u80fd\u591f\u9ad8\u6548\u6a21\u62df\u590d\u6742\u9886\u57df\uff08\u5982\u57ce\u5e02\u9053\u8def\u7f51\u7edc\u548c\u901a\u4fe1\u7cfb\u7edf\uff09\uff0c\u652f\u6301\u591a\u79cd\u7b56\u7565\u7c7b\u578b\uff08\u542f\u53d1\u5f0f\u3001\u57fa\u4e8e\u4f18\u5316\u3001\u57fa\u4e8e\u5b66\u4e60\uff0c\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u5728\u6807\u51c6\u786c\u4ef6\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u6a21\u62df\u3002", "conclusion": "\u901a\u8fc7\u964d\u4f4e\u7814\u7a76\u95e8\u69db\u5e76\u5728\u6807\u51c6\u786c\u4ef6\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u6a21\u62df\uff0cGAMMS\u4fc3\u8fdb\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3001\u81ea\u4e3b\u89c4\u5212\u548c\u5bf9\u6297\u5efa\u6a21\u9886\u57df\u7684\u5b9e\u9a8c\u548c\u521b\u65b0\u3002\u8be5\u6846\u67b6\u5df2\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2602.05110", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05110", "abs": "https://arxiv.org/abs/2602.05110", "authors": ["Liang Wang", "Junpeng Wang", "Chin-chia Michael Yeh", "Yan Zheng", "Jiarui Sun", "Xiran Fan", "Xin Dai", "Yujie Fan", "Yiwei Cai"], "title": "Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u6784\u5316\u591a\u8bc4\u4f30\u8005\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u5546\u6237\u98ce\u9669\u5206\u7c7b\u4e2d\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u901a\u8fc7\u4e94\u6807\u51c6\u91cf\u8868\u548c\u8499\u7279\u5361\u6d1b\u8bc4\u5206\uff0c\u7ed3\u5408\u4e13\u5bb6\u9a8c\u8bc1\u548c\u771f\u5b9e\u652f\u4ed8\u6570\u636e\u9a8c\u8bc1\uff0c\u63ed\u793a\u4e86\u4e0d\u540cLLM\u7684\u8bc4\u4f30\u504f\u89c1\u548c\u7a33\u5b9a\u6027\u5dee\u5f02\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u63a8\u7406\u8d28\u91cf\u7684\u8bc4\u4f30\u8005\uff0c\u4f46\u5728\u652f\u4ed8\u98ce\u9669\u573a\u666f\u4e0b\u7684\u53ef\u9760\u6027\u548c\u504f\u89c1\u4ecd\u4e0d\u6e05\u695a\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u5546\u6237\u5206\u7c7b\u4ee3\u7801\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u91d1\u878d\u64cd\u4f5c\u73af\u5883\u63d0\u4f9b\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u591a\u8bc4\u4f30\u8005\u6846\u67b6\uff0c\u7ed3\u5408\u4e94\u6807\u51c6\u91cf\u8868\u548c\u8499\u7279\u5361\u6d1b\u8bc4\u5206\u8bc4\u4f30\u63a8\u7406\u8d28\u91cf\u3002\u4f7f\u7528\u4e94\u4e2a\u524d\u6cbfLLM\u5728\u7f72\u540d\u548c\u533f\u540d\u6761\u4ef6\u4e0b\u751f\u6210\u5e76\u4ea4\u53c9\u8bc4\u4f30\u5546\u6237\u98ce\u9669\u63a8\u7406\u3002\u5f15\u5165\u5171\u8bc6\u504f\u5dee\u5ea6\u91cf\u6d88\u9664\u5faa\u73af\u6027\uff0c\u901a\u8fc726\u540d\u652f\u4ed8\u884c\u4e1a\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u5e76\u4f7f\u7528\u771f\u5b9e\u652f\u4ed8\u7f51\u7edc\u6570\u636e\u8fdb\u884c\u5730\u9762\u771f\u5b9e\u9a8c\u8bc1\u3002", "result": "\u7ed3\u679c\u663e\u793a\u663e\u8457\u5f02\u8d28\u6027\uff1aGPT-5.1\u548cClaude 4.5 Sonnet\u663e\u793a\u8d1f\u81ea\u6211\u8bc4\u4f30\u504f\u89c1\uff0cGemini-2.5 Pro\u548cGrok 4\u663e\u793a\u6b63\u504f\u89c1\uff0c\u533f\u540d\u5316\u4f7f\u504f\u89c1\u51cf\u5c1125.8%\u3002LLM\u8bc4\u4f30\u8005\u8bc4\u5206\u5e73\u5747\u6bd4\u4eba\u7c7b\u5171\u8bc6\u9ad80.46\u5206\uff0cGPT-5.1\u548cClaude 4.5 Sonnet\u7684\u8d1f\u504f\u89c1\u53cd\u6620\u4e0e\u4eba\u7c7b\u5224\u65ad\u66f4\u63a5\u8fd1\u3002\u56db\u4e2a\u6a21\u578b\u4e0e\u771f\u5b9e\u652f\u4ed8\u6570\u636e\u6709\u663e\u8457\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u652f\u4ed8\u98ce\u9669\u5de5\u4f5c\u6d41\u4e2d\u7684LLM-as-a-judge\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u5728\u91d1\u878d\u64cd\u4f5c\u73af\u5883\u4e2d\u9700\u8981\u504f\u89c1\u611f\u77e5\u534f\u8bae\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.05523", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05523", "abs": "https://arxiv.org/abs/2602.05523", "authors": ["Shahin Honarvar", "Amber Gorzynski", "James Lee-Jones", "Harry Coppock", "Marek Rei", "Joseph Ryan", "Alastair F. Donaldson"], "title": "Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations", "comment": null, "summary": "Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faCTF\u6311\u6218\u5bb6\u65cf\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u8bed\u4e49\u4fdd\u6301\u7684\u7a0b\u5e8f\u53d8\u6362\u751f\u6210\u8bed\u4e49\u7b49\u4ef7\u7684\u6311\u6218\u53d8\u4f53\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u7f51\u7edc\u5b89\u5168\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86Evolve-CTF\u5de5\u5177\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCTF\u7684\u7f51\u7edc\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6\u53ea\u80fd\u8fdb\u884c\u70b9\u5bf9\u70b9\u6d4b\u8bd5\uff0c\u65e0\u6cd5\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u4e0d\u540c\u6e90\u4ee3\u7801\u53d8\u4f53\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63a7\u5236\u53d8\u91cf\u3001\u4fdd\u6301\u6f0f\u6d1e\u5229\u7528\u7b56\u7565\u4e0d\u53d8\u7684\u65b9\u6cd5\u6765\u7cfb\u7edf\u8bc4\u4f30\u4ee3\u7406\u5bf9\u6e90\u4ee3\u7801\u53d8\u6362\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faCTF\u6311\u6218\u5bb6\u65cf\u6982\u5ff5\uff0c\u4f7f\u7528\u8bed\u4e49\u4fdd\u6301\u7684\u7a0b\u5e8f\u53d8\u6362\u751f\u6210\u6311\u6218\u53d8\u4f53\u3002\u5f00\u53d1Evolve-CTF\u5de5\u5177\uff0c\u5bf9Python CTF\u6311\u6218\u5e94\u7528\u591a\u79cd\u53d8\u6362\uff08\u91cd\u547d\u540d\u3001\u4ee3\u7801\u63d2\u5165\u3001\u7ec4\u5408\u53d8\u6362\u7b49\uff09\u751f\u6210\u5bb6\u65cf\u3002\u57fa\u4e8eCybench\u548cIntercode\u6311\u6218\u6784\u5efa\u6570\u636e\u96c6\uff0c\u8bc4\u4f3013\u79cd\u5e26\u5de5\u5177\u8bbf\u95ee\u7684LLM\u4ee3\u7406\u914d\u7f6e\u3002", "result": "\u6a21\u578b\u5bf9\u4fb5\u5165\u6027\u91cd\u547d\u540d\u548c\u4ee3\u7801\u63d2\u5165\u53d8\u6362\u8868\u73b0\u51fa\u8f83\u5f3a\u9c81\u68d2\u6027\uff0c\u4f46\u7ec4\u5408\u53d8\u6362\u548c\u6df1\u5ea6\u6df7\u6dc6\u4f1a\u5f71\u54cd\u6027\u80fd\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u5de5\u5177\u4f7f\u7528\u3002\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u5bf9\u89e3\u51b3\u6210\u529f\u7387\u5f71\u54cd\u6709\u9650\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6280\u672f\u548c\u5de5\u5177\uff0c\u4ee5\u53ca\u5f53\u524dSOTA\u6a21\u578b\u5728\u8be5\u9886\u57df\u80fd\u529b\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "conclusion": "CTF\u6311\u6218\u5bb6\u65cf\u65b9\u6cd5\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8bc4\u4f30\u6280\u672f\u548c\u5de5\u5177\uff0c\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30\u4ee3\u7406\u5bf9\u6e90\u4ee3\u7801\u53d8\u6362\u7684\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u67d0\u4e9b\u53d8\u6362\u4e0a\u8868\u73b0\u9c81\u68d2\uff0c\u4f46\u5728\u590d\u6742\u53d8\u6362\u4e0b\u4ecd\u9700\u6539\u8fdb\uff0c\u4e3a\u672a\u6765LLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2602.05115", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05115", "abs": "https://arxiv.org/abs/2602.05115", "authors": ["Keyang Xuan", "Pengda Wang", "Chongrui Ye", "Haofei Yu", "Tal August", "Jiaxuan You"], "title": "SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers", "comment": "10 pages", "summary": "Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present \\textsc{SocialVeil}, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, \\textsc{SocialVeil} introduces three representative types of such disruption, \\emph{semantic vagueness}, \\emph{sociocultural mismatch}, and \\emph{emotional interference}. We also introduce two barrier-aware evaluation metrics, \\emph{unresolved confusion} and \\emph{mutual understanding}, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\\% on average, and confusion elevated by nearly 50\\%. Human evaluations validate the fidelity of these simulated barriers (ICC$\\approx$0.78, Pearson r$\\approx$0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learning) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.", "AI": {"tldr": "SocialVeil\u662f\u4e00\u4e2a\u6a21\u62df\u8ba4\u77e5\u5dee\u5f02\u5bfc\u81f4\u6c9f\u901a\u969c\u788d\u7684\u793e\u4f1a\u5b66\u4e60\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u975e\u7406\u60f3\u6c9f\u901a\u6761\u4ef6\u4e0b\u7684\u793e\u4f1a\u667a\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u5047\u8bbe\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u7406\u60f3\u5316\u901a\u4fe1\uff0c\u9650\u5236\u4e86\u8bca\u65adLLM\u5728\u66f4\u73b0\u5b9e\u3001\u4e0d\u5b8c\u7f8e\u73af\u5883\u4e2d\u7ef4\u6301\u548c\u4fee\u590d\u4ea4\u4e92\u7684\u80fd\u529b\u3002\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f7f\u793e\u4f1a\u4ea4\u4e92\u73af\u5883\u66f4\u63a5\u8fd1\u771f\u5b9e\u4e16\u754c\u901a\u4fe1\u3002", "method": "\u57fa\u4e8e\u5bf9\u4eba\u7c7b\u4ea4\u4e92\u4e2d\u6c9f\u901a\u6311\u6218\u7684\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0cSocialVeil\u5f15\u5165\u4e86\u4e09\u79cd\u4ee3\u8868\u6027\u7684\u6c9f\u901a\u969c\u788d\u7c7b\u578b\uff1a\u8bed\u4e49\u6a21\u7cca\u6027\u3001\u793e\u4f1a\u6587\u5316\u4e0d\u5339\u914d\u548c\u60c5\u611f\u5e72\u6270\u3002\u8fd8\u5f15\u5165\u4e86\u4e24\u79cd\u969c\u788d\u611f\u77e5\u8bc4\u4f30\u6307\u6807\uff1a\u672a\u89e3\u51b3\u7684\u56f0\u60d1\u548c\u76f8\u4e92\u7406\u89e3\u3002\u5728720\u4e2a\u573a\u666f\u548c\u56db\u4e2a\u524d\u6cbfLLM\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u8fdb\u884c\u4e86\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u3002", "result": "\u969c\u788d\u6301\u7eed\u635f\u5bb3LLM\u6027\u80fd\uff0c\u76f8\u4e92\u7406\u89e3\u5e73\u5747\u964d\u4f4e\u8d85\u8fc745%\uff0c\u56f0\u60d1\u5ea6\u63d0\u5347\u8fd150%\u3002\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u6a21\u62df\u969c\u788d\u7684\u4fdd\u771f\u5ea6\uff08ICC\u22480.78\uff0cPearson r\u22480.80\uff09\u3002\u9002\u5e94\u7b56\u7565\uff08\u4fee\u590d\u6307\u4ee4\u548c\u4ea4\u4e92\u5b66\u4e60\uff09\u6548\u679c\u6709\u9650\uff0c\u8fdc\u672a\u8fbe\u5230\u65e0\u969c\u788d\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4f7f\u793e\u4f1a\u4ea4\u4e92\u73af\u5883\u66f4\u63a5\u8fd1\u771f\u5b9e\u4e16\u754c\u901a\u4fe1\uff0c\u4e3a\u63a2\u7d22LLM\u667a\u80fd\u4f53\u7684\u793e\u4f1a\u667a\u80fd\u5f00\u8f9f\u4e86\u673a\u4f1a\u3002\u7ed3\u679c\u8868\u660e\u5f53\u524dLLM\u5728\u975e\u7406\u60f3\u6c9f\u901a\u6761\u4ef6\u4e0b\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347\u5176\u793e\u4f1a\u9002\u5e94\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.05550", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05550", "abs": "https://arxiv.org/abs/2602.05550", "authors": ["Yulong He", "Artem Ermakov", "Sergey Kovalchuk", "Artem Aliev", "Dmitry Shalymov"], "title": "ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval", "comment": null, "summary": "ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.", "AI": {"tldr": "\u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21ArkTS\u4ee3\u7801\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4ee3\u7801\u68c0\u7d22\u548c\u8bc4\u4f30\u4efb\u52a1\uff0c\u5e76\u8bc4\u4f30\u73b0\u6709\u4ee3\u7801\u5d4c\u5165\u6a21\u578b\uff0c\u901a\u8fc7\u5fae\u8c03\u83b7\u5f97\u9ad8\u6027\u80fdArkTS\u4ee3\u7801\u7406\u89e3\u6a21\u578b\u3002", "motivation": "OpenHarmony\u751f\u6001\u4e2dArkTS\u4f5c\u4e3a\u6838\u5fc3\u7f16\u7a0b\u8bed\u8a00\uff0c\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u963b\u788d\u4e86ArkTS\u4ee3\u7801\u667a\u80fd\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u4eceGitHub\u548cGitee\u722c\u53d6ArkTS\u4ed3\u5e93\uff0c\u4f7f\u7528tree-sitter-arkts\u63d0\u53d6\u6ce8\u91ca-\u51fd\u6570\u5bf9\uff0c\u8fdb\u884c\u8de8\u5e73\u53f0\u53bb\u91cd\u548c\u7edf\u8ba1\u5206\u6790\uff1b\u8bbe\u8ba1\u5355\u641c\u7d22\u4efb\u52a1\uff08\u81ea\u7136\u8bed\u8a00\u6ce8\u91ca\u68c0\u7d22\u5bf9\u5e94ArkTS\u51fd\u6570\uff09\uff0c\u8bc4\u4f30\u73b0\u6709\u5f00\u6e90\u4ee3\u7801\u5d4c\u5165\u6a21\u578b\uff0c\u4f7f\u7528ArkTS\u548cTypeScript\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21ArkTS\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u4e86ArkTS\u4ee3\u7801\u68c0\u7d22\u7684\u7cfb\u7edf\u57fa\u51c6\uff0c\u83b7\u5f97\u4e86\u9ad8\u6027\u80fd\u7684ArkTS\u4ee3\u7801\u7406\u89e3\u6a21\u578b\uff0c\u6570\u636e\u96c6\u548c\u6a21\u578b\u5df2\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u586b\u8865\u4e86ArkTS\u4ee3\u7801\u667a\u80fd\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3aArkTS\u4ee3\u7801\u68c0\u7d22\u5efa\u7acb\u4e86\u9996\u4e2a\u7cfb\u7edf\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86OpenHarmony\u751f\u6001\u7684\u53d1\u5c55\u3002", "topic": "swe benchmark"}}
{"id": "2602.05721", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05721", "abs": "https://arxiv.org/abs/2602.05721", "authors": ["Bin Liu", "Yanjie Zhao", "Zhenpeng Chen", "Guoai Xu", "Haoyu Wang"], "title": "A Dual-Loop Agent Framework for Automated Vulnerability Reproduction", "comment": null, "summary": "Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \\textit{Tactical Loop} for code-level refinement, while the \\textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\\% and 54.3\\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\\% and 20.4\\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.", "AI": {"tldr": "Cve2PoC\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u53cc\u5faa\u73af\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u4eceCVE\u63cf\u8ff0\u81ea\u52a8\u751f\u6210\u53ef\u6267\u884c\u7684PoC\u6f0f\u6d1e\u5229\u7528\u4ee3\u7801\uff0c\u901a\u8fc7\u6218\u7565\u89c4\u5212-\u6218\u672f\u6267\u884c-\u81ea\u9002\u5e94\u4f18\u5316\u7684\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6f0f\u6d1e\u590d\u73b0\u6210\u529f\u7387\u3002", "motivation": "\u4eceCVE\u63cf\u8ff0\u81ea\u52a8\u751f\u6210\u53ef\u6267\u884c\u7684PoC\u6f0f\u6d1e\u5229\u7528\u4ee3\u7801\u5bf9\u4e8e\u8f6f\u4ef6\u5b89\u5168\u7814\u7a76\u548c\u5b9e\u8df5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7684\u624b\u52a8\u65b9\u6cd5\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002\u73b0\u6709\u7684LLM\u4ee3\u7406\u65b9\u6cd5\u5728\u63a2\u7d22\u653b\u51fb\u65b9\u5411\u548c\u4fee\u590d\u5b9e\u73b0\u7ec6\u8282\u65f6\u5e38\u5e38\u6df7\u6dc6\uff0c\u5bfc\u81f4\u65e0\u6548\u7684\u8c03\u8bd5\u5faa\u73af\u3002", "method": "\u63d0\u51fa\u4e86Cve2PoC\u6846\u67b6\uff0c\u91c7\u7528\u8ba1\u5212-\u6267\u884c-\u8bc4\u4f30\u8303\u5f0f\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u6218\u7565\u89c4\u5212\u5668\u5206\u6790\u6f0f\u6d1e\u8bed\u4e49\u548c\u76ee\u6807\u4ee3\u7801\u751f\u6210\u7ed3\u6784\u5316\u653b\u51fb\u8ba1\u5212\uff1b\u6218\u672f\u6267\u884c\u5668\u751f\u6210PoC\u4ee3\u7801\u5e76\u901a\u8fc7\u6e10\u8fdb\u9a8c\u8bc1\u8fdb\u884c\u6d4b\u8bd5\uff1b\u81ea\u9002\u5e94\u4f18\u5316\u5668\u8bc4\u4f30\u6267\u884c\u7ed3\u679c\uff0c\u5c06\u5931\u8d25\u8def\u7531\u5230\u4e0d\u540c\u7684\u5faa\u73af\uff1a\u6218\u672f\u5faa\u73af\u7528\u4e8e\u4ee3\u7801\u7ea7\u4f18\u5316\uff0c\u6218\u7565\u5faa\u73af\u7528\u4e8e\u653b\u51fb\u7b56\u7565\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u5728\u4e24\u4e2a\u5305\u542b617\u4e2a\u771f\u5b9e\u4e16\u754c\u6f0f\u6d1e\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCve2PoC\u5728SecBench.js\u4e0a\u8fbe\u523082.9%\u7684\u590d\u73b0\u6210\u529f\u7387\uff0c\u5728PatchEval\u4e0a\u8fbe\u523054.3%\u7684\u590d\u73b0\u6210\u529f\u7387\uff0c\u5206\u522b\u6bd4\u6700\u4f73\u57fa\u7ebf\u9ad8\u51fa11.3%\u548c20.4%\u3002\u4eba\u5de5\u8bc4\u4f30\u786e\u8ba4\u751f\u6210\u7684PoC\u5728\u53ef\u8bfb\u6027\u548c\u53ef\u91cd\u7528\u6027\u65b9\u9762\u4e0e\u4eba\u5de5\u7f16\u5199\u7684\u6f0f\u6d1e\u5229\u7528\u4ee3\u7801\u76f8\u5f53\u3002", "conclusion": "Cve2PoC\u901a\u8fc7\u53cc\u5faa\u73af\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u63a2\u7d22\u653b\u51fb\u65b9\u5411\u4e0e\u4fee\u590d\u5b9e\u73b0\u7ec6\u8282\u6df7\u6dc6\u7684\u95ee\u9898\uff0c\u80fd\u591f\u6839\u636e\u5931\u8d25\u7c7b\u578b\u5339\u914d\u76f8\u5e94\u7684\u4fee\u590d\u7b56\u7565\uff0c\u4ece\u800c\u907f\u514d\u65e0\u6548\u7684\u8c03\u8bd5\u5faa\u73af\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u52a8\u5316\u6f0f\u6d1e\u590d\u73b0\u7684\u6210\u529f\u7387\u3002", "topic": "code agent"}}
{"id": "2602.05780", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05780", "abs": "https://arxiv.org/abs/2602.05780", "authors": ["Ulrich Finkler", "Irene Manotas", "Wei Zhang", "Geert Janssen", "Octavian Popescu", "Shyam Ramji"], "title": "Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes", "comment": null, "summary": "Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7801\u8bed\u4e49\u8303\u56f4\u7684\u81ea\u52a8\u5316LLM\u5b9a\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7RAG\u548c\u5fae\u8c03\u4e24\u79cd\u7b56\u7565\uff0c\u5c06LLM\u9002\u914d\u5230\u79c1\u6709\u4ee3\u7801\u4ed3\u5e93\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u8865\u5168\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9762\u5bf9\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u89c1\u7684\u79c1\u6709\u4ee3\u7801\u4ed3\u5e93\u65f6\uff0c\u751f\u6210\u7684\u4ee3\u7801\u5f80\u5f80\u96be\u4ee5\u4e0e\u4ed3\u5e93\u7279\u5b9a\u6a21\u5f0f\u5bf9\u9f50\u3002\u5b9a\u5236\u5316LLM\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4ee3\u7801\u8bed\u4e49\u8303\u56f4\u7684\u81ea\u52a8\u5316LLM\u5b9a\u5236\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u7b56\u7565\uff1a\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548c\u76d1\u7763\u5fae\u8c03(FT)\u3002\u901a\u8fc7\u8bed\u4e49\u8303\u56f4\u673a\u5236\u5904\u7406\u4ed3\u5e93\u6570\u636e\u5e76\u6784\u5efa\u8bad\u7ec3\u6570\u636e\u5bf9\uff0c\u5e2e\u52a9\u6a21\u578b\u5b66\u4e60\u4ed3\u5e93\u7279\u5b9a\u6a21\u5f0f\u3002", "result": "\u5728\u4e24\u4e2a\u4f01\u4e1a\u79c1\u6709\u4ee3\u7801\u4ed3\u5e93\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u4e2d\u7b49\u89c4\u6a21\u7684\u5b9a\u5236\u6a21\u578b\u5728\u4ee3\u7801\u8865\u5168\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u672a\u5b9a\u5236\u7684\u5927\u578b\u6a21\u578b\u3002\u540c\u65f6\u5728\u4e24\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4e5f\u8fdb\u884c\u4e86\u5206\u6790\u3002", "conclusion": "\u57fa\u4e8e\u8bed\u4e49\u8303\u56f4\u7684LLM\u5b9a\u5236\u80fd\u6709\u6548\u63d0\u5347\u79c1\u6709\u4ee3\u7801\u4ed3\u5e93\u7684\u4ee3\u7801\u8865\u5168\u8d28\u91cf\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u4ee3\u7801\u5efa\u8bae\uff0c\u663e\u8457\u63d0\u9ad8\u751f\u4ea7\u529b\u3002\u4e2d\u7b49\u89c4\u6a21\u5b9a\u5236\u6a21\u578b\u53ef\u8d85\u8d8a\u5927\u578b\u672a\u5b9a\u5236\u6a21\u578b\u3002", "topic": "code agent"}}
{"id": "2602.05249", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05249", "abs": "https://arxiv.org/abs/2602.05249", "authors": ["Xinyi He", "Ying Yang", "Chuanjian Fu", "Sihan Guo", "Songchun Zhu", "Lifeng Fan", "Zhenliang Zhang", "Yujia Peng"], "title": "Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents", "comment": null, "summary": "As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments.", "AI": {"tldr": "\u63d0\u51faTEA\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u539f\u4f4d\u4efb\u52a1\u751f\u6210\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u672a\u89c13D\u73af\u5883\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u57fa\u672c\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6570\u636e\u6c61\u67d3\u548c\u7f3a\u4e4f\u573a\u666f\u7279\u5f02\u6027\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7684\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u9488\u5bf9\u672a\u89c13D\u73af\u5883\u7684\u8bc4\u4f30\u65b9\u6cd5", "method": "\u63d0\u51faTEA\u65b9\u6cd5\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u56fe\u8868\u793a\u5b9a\u4e49\u4efb\u52a1\uff0c\u6784\u5efa\u4e24\u9636\u6bb5\u4ea4\u4e92-\u8fdb\u5316\u4efb\u52a1\u751f\u6210\u7cfb\u7edf\uff1a\u4ea4\u4e92\u9636\u6bb5\u901a\u8fc7\u4efb\u52a1\u6267\u884c\u4e0e\u751f\u6210\u7684\u5faa\u73af\u6301\u7eed\u751f\u6210\u4efb\u52a1\uff1b\u8fdb\u5316\u9636\u6bb5\u901a\u8fc7\u4efb\u52a1\u56fe\u5efa\u6a21\u91cd\u7ec4\u548c\u91cd\u7528\u73b0\u6709\u4efb\u52a1\u751f\u6210\u65b0\u4efb\u52a1", "result": "\u572810\u4e2a\u672a\u89c1\u573a\u666f\u4e2d\u81ea\u52a8\u751f\u6210\u4e8687,876\u4e2a\u4efb\u52a1\uff0c\u4eba\u7c7b\u9a8c\u8bc1\u786e\u8ba4\u8fd9\u4e9b\u4efb\u52a1\u7269\u7406\u5408\u7406\u4e14\u6db5\u76d6\u57fa\u672c\u65e5\u5e38\u8ba4\u77e5\u80fd\u529b\uff1b\u57fa\u51c6\u6d4b\u8bd5\u663e\u793aSOTA\u6a21\u578b\u5728\u57fa\u672c\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u5dee\uff0c\u4e25\u91cd\u7f3a\u4e4f3D\u4ea4\u4e92\u610f\u8bc6\uff0c\u5bf9\u4efb\u52a1\u7c7b\u578b\u9ad8\u5ea6\u654f\u611f", "conclusion": "\u5728\u5c06\u667a\u80fd\u4f53\u90e8\u7f72\u5230\u771f\u5b9e\u4eba\u7c7b\u73af\u5883\u4e4b\u524d\uff0c\u5fc5\u987b\u8fdb\u884c\u539f\u4f4d\u8bc4\u4f30\uff0c\u73b0\u6709\u6a21\u578b\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7684\u80fd\u529b\u5b58\u5728\u663e\u8457\u4e0d\u8db3", "topic": "agent analysis"}}
{"id": "2602.04925", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04925", "abs": "https://arxiv.org/abs/2602.04925", "authors": ["Zhenning Shi", "Yijia Zhu", "Junhan Shi", "Xun Zhang", "Lei Wang", "Congcong Miao"], "title": "Internalizing LLM Reasoning via Discovery and Replay of Latent Actions", "comment": null, "summary": "The internalization of chain-of-thought processes into hidden states has emerged as a highly efficient paradigm for scaling test-time compute. However, existing activation steering methods rely on static control vectors that fail to adapt to the non-stationary evolution of complex reasoning tasks. To address this limitation, we propose STIR (Self-Distilled Tools for Internal Reasoning), a framework that reformulates reasoning enhancement as a dynamic latent trajectory control problem. STIR introduces a synergistic three-stage pipeline: (1) differential intrinsic action induction harvests latent reasoning successes to crystallize steering primitives; (2) sparse control basis construction curates a compact, geometrically diverse tool library; and (3) value-modulated trajectory intervention dynamically injects context-specific impulses via anchor-based gating. Extensive experiments on six arithmetic and logical benchmarks across four representative models demonstrate that STIR improves average accuracy by 1.9% to 7.5% while reducing average token consumption by up to 35% compared to vanilla decoding. These findings demonstrate that the benefits of explicit chain-of-thought can be realized through dynamic latent trajectory control, internalizing the reasoning process to bypass the explicit generation while achieving superior fidelity. Our code is available at https://github.com/sznnzs/LLM-Latent-Action.", "AI": {"tldr": "STIR\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u6f5c\u5728\u8f68\u8ff9\u63a7\u5236\u5b9e\u73b0\u63a8\u7406\u589e\u5f3a\uff0c\u5c06\u663e\u5f0f\u601d\u7ef4\u94fe\u8fc7\u7a0b\u5185\u5316\u4e3a\u9690\u85cf\u72b6\u6001\uff0c\u5728\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u63a7\u5236\u5411\u91cf\uff0c\u65e0\u6cd5\u9002\u5e94\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u975e\u5e73\u7a33\u6f14\u5316\u8fc7\u7a0b\uff0c\u9700\u8981\u66f4\u52a8\u6001\u7684\u63a8\u7406\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u5dee\u5206\u5185\u5728\u52a8\u4f5c\u8bf1\u5bfc\u6536\u96c6\u6f5c\u5728\u63a8\u7406\u6210\u529f\u6a21\u5f0f\uff1b2) \u7a00\u758f\u63a7\u5236\u57fa\u6784\u5efa\u521b\u5efa\u7d27\u51d1\u51e0\u4f55\u591a\u6837\u5de5\u5177\u5e93\uff1b3) \u4ef7\u503c\u8c03\u5236\u8f68\u8ff9\u5e72\u9884\u901a\u8fc7\u951a\u70b9\u95e8\u63a7\u52a8\u6001\u6ce8\u5165\u4e0a\u4e0b\u6587\u7279\u5b9a\u8109\u51b2\u3002", "result": "\u57286\u4e2a\u7b97\u672f\u548c\u903b\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTIR\u5c06\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad81.9%\u81f37.5%\uff0c\u540c\u65f6\u5c06\u5e73\u5747token\u6d88\u8017\u51cf\u5c11\u9ad8\u8fbe35%\u3002", "conclusion": "\u663e\u5f0f\u601d\u7ef4\u94fe\u7684\u4f18\u52bf\u53ef\u4ee5\u901a\u8fc7\u52a8\u6001\u6f5c\u5728\u8f68\u8ff9\u63a7\u5236\u5b9e\u73b0\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u5185\u5316\u4ee5\u7ed5\u8fc7\u663e\u5f0f\u751f\u6210\uff0c\u540c\u65f6\u5b9e\u73b0\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2602.05762", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.05762", "abs": "https://arxiv.org/abs/2602.05762", "authors": ["Andrei Kozyrev", "Nikita Khramov", "Denis Lochmelis", "Valerio Morelli", "Gleb Solovev", "Anton Podkopaev"], "title": "RocqSmith: Can Automatic Optimization Forge Better Proof Agents?", "comment": null, "summary": "This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.", "AI": {"tldr": "\u7814\u7a76\u81ea\u52a8AI\u4ee3\u7406\u4f18\u5316\u65b9\u6cd5\u5728\u5f62\u5f0f\u9a8c\u8bc1\u9886\u57df\uff08\u7279\u522b\u662fRocq\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\uff09\u7684\u9002\u7528\u6027\uff0c\u8bc4\u4f30\u4e0d\u540c\u4f18\u5316\u5668\u5bf9\u8bc1\u660e\u751f\u6210\u4ee3\u7406\u7684\u6027\u80fd\u63d0\u5347\u6548\u679c\uff0c\u53d1\u73b0few-shot bootstrapping\u6700\u6709\u6548\u4f46\u65e0\u6cd5\u8d85\u8d8a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6700\u5148\u8fdb\u8bc1\u660e\u4ee3\u7406\u3002", "motivation": "\u63a2\u7d22\u81ea\u52a8AI\u4ee3\u7406\u4f18\u5316\u65b9\u6cd5\u80fd\u5426\u5e94\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u5f62\u5f0f\u9a8c\u8bc1\u4ee3\u7406\uff0c\u7279\u522b\u662f\u80fd\u5426\u81ea\u52a8\u5316\u4ee3\u7406\u7cfb\u7edf\u7684\u7cbe\u7ec6\u8c03\u4f18\uff08\u5982\u63d0\u793a\u8bbe\u8ba1\u3001\u4e0a\u4e0b\u6587\u77e5\u8bc6\u548c\u63a7\u5236\u7b56\u7565\uff09\uff0c\u4ee5\u51cf\u8f7b\u4eba\u5de5\u5de5\u7a0b\u8d1f\u62c5\u3002", "method": "\u4ee5Rocq\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u4e3a\u4ee3\u8868\u6027\u6311\u6218\u9886\u57df\uff0c\u8bc4\u4f30\u4e0d\u540c\u81ea\u52a8\u4ee3\u7406\u4f18\u5316\u5668\u5728\u4f18\u5316Rocq\u8bc1\u660e\u751f\u6210\u4ee3\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u5404\u79cd\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u591a\u4e2a\u4f18\u5316\u5668\u90fd\u80fd\u5e26\u6765\u53ef\u6d4b\u91cf\u7684\u6539\u8fdb\uff0c\u5176\u4e2d\u7b80\u5355\u7684few-shot bootstrapping\u65b9\u6cd5\u8868\u73b0\u6700\u7a33\u5b9a\u6709\u6548\uff1b\u4f46\u6240\u6709\u7814\u7a76\u7684\u81ea\u52a8\u4f18\u5316\u65b9\u6cd5\u90fd\u65e0\u6cd5\u8fbe\u5230\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6700\u5148\u8fdb\u8bc1\u660e\u4ee3\u7406\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u81ea\u52a8\u4ee3\u7406\u4f18\u5316\u65b9\u6cd5\u5728\u5f62\u5f0f\u9a8c\u8bc1\u9886\u57df\u6709\u4e00\u5b9a\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u6280\u672f\u4ecd\u65e0\u6cd5\u5b8c\u5168\u66ff\u4ee3\u4eba\u5de5\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4ee3\u7406\u7cfb\u7edf\uff0cfew-shot bootstrapping\u662f\u6700\u6709\u524d\u666f\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.04931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04931", "abs": "https://arxiv.org/abs/2602.04931", "authors": ["Shahar Haim", "Daniel C McNamee"], "title": "Depth-Wise Emergence of Prediction-Centric Geometry in Large Language Models", "comment": null, "summary": "We show that decoder-only large language models exhibit a depth-wise transition from context-processing to prediction-forming phases of computation accompanied by a reorganization of representational geometry. Using a unified framework combining geometric analysis with mechanistic intervention, we demonstrate that late-layer representations implement a structured geometric code that enables selective causal control over token prediction. Specifically, angular organization of the representation geometry parametrizes prediction distributional similarity, while representation norms encode context-specific information that does not determine prediction. Together, these results provide a mechanistic-geometric account of the dynamics of transforming context into predictions in LLMs.", "AI": {"tldr": "LLMs\u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u5b58\u5728\u4ece\u4e0a\u4e0b\u6587\u5904\u7406\u5230\u9884\u6d4b\u5f62\u6210\u7684\u8ba1\u7b97\u9636\u6bb5\u8f6c\u6362\uff0c\u4f34\u968f\u8868\u5f81\u51e0\u4f55\u7684\u91cd\u7ec4\uff0c\u5176\u4e2d\u89d2\u5ea6\u7ec4\u7ec7\u53c2\u6570\u5316\u9884\u6d4b\u5206\u5e03\u76f8\u4f3c\u6027\uff0c\u8303\u6570\u7f16\u7801\u4e0a\u4e0b\u6587\u7279\u5b9a\u4fe1\u606f\u3002", "motivation": "\u7406\u89e3\u89e3\u7801\u5668\u67b6\u6784\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5c06\u4e0a\u4e0b\u6587\u4fe1\u606f\u8f6c\u5316\u4e3a\u9884\u6d4b\u7684\u5185\u90e8\u8ba1\u7b97\u673a\u5236\u548c\u51e0\u4f55\u8868\u5f81\u52a8\u6001\u3002", "method": "\u7ed3\u5408\u51e0\u4f55\u5206\u6790\u548c\u673a\u5236\u5e72\u9884\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5206\u6790LLM\u6df1\u5ea6\u5c42\u7ea7\u7684\u8868\u5f81\u51e0\u4f55\u7ed3\u6784\uff0c\u5305\u62ec\u89d2\u5ea6\u7ec4\u7ec7\u548c\u8303\u6570\u7f16\u7801\u3002", "result": "\u53d1\u73b0LLM\u5b58\u5728\u6df1\u5ea6\u7ef4\u5ea6\u7684\u8ba1\u7b97\u9636\u6bb5\u8f6c\u6362\uff0c\u665a\u671f\u5c42\u5b9e\u73b0\u7ed3\u6784\u5316\u51e0\u4f55\u7f16\u7801\uff0c\u89d2\u5ea6\u53c2\u6570\u5316\u9884\u6d4b\u5206\u5e03\u76f8\u4f3c\u6027\uff0c\u8303\u6570\u7f16\u7801\u4e0a\u4e0b\u6587\u7279\u5b9a\u4fe1\u606f\u3002", "conclusion": "\u4e3aLLM\u5c06\u4e0a\u4e0b\u6587\u8f6c\u5316\u4e3a\u9884\u6d4b\u7684\u52a8\u6001\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u673a\u5236-\u51e0\u4f55\u89e3\u91ca\uff0c\u63ed\u793a\u4e86\u8868\u5f81\u51e0\u4f55\u5728\u9009\u62e9\u6027\u56e0\u679c\u63a7\u5236\u4e2d\u7684\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "2602.05385", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05385", "abs": "https://arxiv.org/abs/2602.05385", "authors": ["Tao Liu", "Jiafan Lu", "Bohan Yu", "Pengcheng Wu", "Liu Haixin", "Guoyu Xu", "Li Xiangheng", "Lixiao Li", "Jiaming Hou", "Zhao Shijun", "Xinglin Lyu", "Kunli Zhang", "Yuxiang Jia", "Hongyin Zan"], "title": "IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models", "comment": "25 pages, 16 figures, 8 tables. Hongyin Zan is corresponding author, Jiafan Lu is first co-author", "summary": "Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.", "AI": {"tldr": "IESR\u6846\u67b6\u901a\u8fc7\u4fe1\u606f\u589e\u5f3a\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u590d\u6742Text-to-SQL\u4efb\u52a1\uff0c\u5728LogicCat\u548cArcher\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "motivation": "\u5f53\u524dText-to-SQL\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u3001\u9886\u57df\u77e5\u8bc6\u3001\u5047\u8bbe\u67e5\u8be2\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4f01\u4e1a\u90e8\u7f72\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e3a\u8f7b\u91cf\u7ea7LLM\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faIESR\u6846\u67b6\uff1a(1)\u5229\u7528LLM\u8fdb\u884c\u5173\u952e\u4fe1\u606f\u7406\u89e3\u548c\u6a21\u5f0f\u94fe\u63a5\uff0c\u89e3\u8026\u6570\u5b66\u8ba1\u7b97\u548cSQL\u751f\u6210\uff1b(2)\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u591a\u8def\u5f84\u63a8\u7406\u673a\u5236\u4e0e\u591a\u6570\u6295\u7968\uff1b(3)\u8f68\u8ff9\u4e00\u81f4\u6027\u9a8c\u8bc1\u6a21\u5757\u548c\u5224\u522b\u5668\u6a21\u578b\u786e\u4fdd\u51c6\u786e\u6027\u3002", "result": "\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6LogicCat\u4e0a\u8fbe\u523024.28 EX\uff0c\u5728Archer\u6570\u636e\u96c6\u4e0a\u8fbe\u523037.28 EX\uff0c\u4ec5\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e14\u65e0\u9700\u5fae\u8c03\u5373\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "conclusion": "IESR\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8f7b\u91cf\u7ea7LLM\u5728\u590d\u6742Text-to-SQL\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524d\u7f16\u7801\u5668\u6a21\u578b\u5728\u7269\u7406\u77e5\u8bc6\u3001\u6570\u5b66\u8ba1\u7b97\u548c\u5e38\u8bc6\u63a8\u7406\u65b9\u9762\u7684\u504f\u89c1\u548c\u4e0d\u8db3\u3002", "topic": "code agent"}}
{"id": "2602.05302", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05302", "abs": "https://arxiv.org/abs/2602.05302", "authors": ["Chris Zhu", "Sasha Cui", "Will Sanok Dufallo", "Runzhi Jin", "Zhen Xu", "Linjun Zhang", "Daylian Cain"], "title": "PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences", "comment": null, "summary": "We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenarios drawn from an MBA negotiation course at an elite business school. We find systematic evidence of AGI-level performance in which a representative frontier agent (GPT-5) matches or outperforms trained business-school students, despite a semester of general negotiation instruction and targeted coaching immediately prior to the task. We further study the effects of joint-intentionality agentic scaffolding and find asymmetric gains, with large improvements for mid- and lower-tier LMs and diminishing returns for frontier LMs. Beyond deal outcomes, PieArena provides a multi-dimensional negotiation behavioral profile, revealing novel cross-model heterogeneity, masked by deal-outcome-only benchmarks, in deception, computation accuracy, instruction compliance, and perceived reputation. Overall, our results suggest that frontier language agents are already intellectually and psychologically capable of deployment in high-stakes economic settings, but deficiencies in robustness and trustworthiness remain open challenges.", "AI": {"tldr": "GPT-5\u5728PieArena\u8c08\u5224\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u5546\u5b66\u9662\u5b66\u751f\u6c34\u5e73\uff0c\u4f46\u4e2d\u4f4e\u5c42\u6a21\u578b\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u8c08\u5224\u884c\u4e3a\u5b58\u5728\u8de8\u6a21\u578b\u5f02\u8d28\u6027", "motivation": "\u8bc4\u4f30LLMs\u5728\u8c08\u5224\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u8c08\u5224\u662f\u5546\u4e1a\u6838\u5fc3\u4efb\u52a1\uff0c\u9700\u8981\u6218\u7565\u63a8\u7406\u3001\u5fc3\u667a\u7406\u8bba\u548c\u7ecf\u6d4e\u4ef7\u503c\u521b\u9020\u80fd\u529b", "method": "\u5f15\u5165PieArena\u5927\u89c4\u6a21\u8c08\u5224\u57fa\u51c6\uff0c\u57fa\u4e8e\u7cbe\u82f1\u5546\u5b66\u9662MBA\u8c08\u5224\u8bfe\u7a0b\u7684\u771f\u5b9e\u573a\u666f\uff0c\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8bc4\u4f30\uff0c\u7814\u7a76\u8054\u5408\u610f\u5411\u6027\u667a\u80fd\u4f53\u811a\u624b\u67b6\u7684\u5f71\u54cd", "result": "GPT-5\u8fbe\u5230AGI\u7ea7\u522b\u8868\u73b0\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u7ecf\u8fc7\u4e00\u5b66\u671f\u8c08\u5224\u8bad\u7ec3\u548c\u9488\u5bf9\u6027\u6307\u5bfc\u7684\u5546\u5b66\u9662\u5b66\u751f\uff1b\u8054\u5408\u610f\u5411\u6027\u811a\u624b\u67b6\u5bf9\u4e2d\u4f4e\u5c42\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5bf9\u524d\u6cbf\u6a21\u578b\u6536\u76ca\u9012\u51cf\uff1b\u53d1\u73b0\u8c08\u5224\u884c\u4e3a\u7684\u591a\u7ef4\u5f02\u8d28\u6027", "conclusion": "\u524d\u6cbf\u8bed\u8a00\u667a\u80fd\u4f53\u5df2\u5177\u5907\u5728\u9ad8\u98ce\u9669\u7ecf\u6d4e\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u667a\u529b\u548c\u5fc3\u7406\u80fd\u529b\uff0c\u4f46\u9c81\u68d2\u6027\u548c\u53ef\u4fe1\u8d56\u6027\u4ecd\u662f\u5f00\u653e\u6311\u6218", "topic": "agent analysis"}}
{"id": "2602.05327", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05327", "abs": "https://arxiv.org/abs/2602.05327", "authors": ["Yangbin Yu", "Mingyu Yang", "Junyou Li", "Yiming Gao", "Feiyu Liu", "Yijun Yang", "Zichuan Lin", "Jiafei Lyu", "Yicheng Liu", "Zhicong Lu", "Deheng Ye", "Jie Jiang"], "title": "ProAct: Agentic Lookahead in Interactive Environments", "comment": null, "summary": "Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct", "AI": {"tldr": "ProAct\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7GLAD\u84b8\u998f\u641c\u7d22\u8f68\u8ff9\u548cMC-Critic\u4ef7\u503c\u4f30\u8ba1\u5668\uff0c\u8ba9LLM\u667a\u80fd\u4f53\u5b66\u4e60\u524d\u77bb\u63a8\u7406\uff0c\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u5b9e\u73b0\u957f\u7a0b\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u89c4\u5212\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u9700\u8981\u957f\u7a0b\u89c4\u5212\u7684\u4ea4\u4e92\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6a21\u62df\u672a\u6765\u72b6\u6001\u65f6\u9519\u8bef\u4f1a\u4e0d\u65ad\u7d2f\u79ef\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9\u667a\u80fd\u4f53\u5185\u90e8\u5316\u51c6\u786e\u7684\u524d\u77bb\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faProAct\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a1) GLAD\uff1a\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5c06\u57fa\u4e8e\u73af\u5883\u641c\u7d22\u7684\u8f68\u8ff9\u538b\u7f29\u4e3a\u7b80\u6d01\u7684\u56e0\u679c\u63a8\u7406\u94fe\uff1b2) MC-Critic\uff1a\u8f7b\u91cf\u7ea7\u73af\u5883rollout\u6821\u51c6\u4ef7\u503c\u4f30\u8ba1\u7684\u63d2\u4ef6\u5f0f\u8f85\u52a9\u4ef7\u503c\u4f30\u8ba1\u5668\uff0c\u589e\u5f3aPPO/GRPO\u7b49\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u3002", "result": "\u5728\u968f\u673a\uff08\u59822048\uff09\u548c\u786e\u5b9a\u6027\uff08\u5982Sokoban\uff09\u73af\u5883\u4e2d\uff0cProAct\u663e\u8457\u63d0\u5347\u89c4\u5212\u51c6\u786e\u6027\u30024B\u53c2\u6570\u6a21\u578b\u8d85\u8d8a\u6240\u6709\u5f00\u6e90\u57fa\u7ebf\uff0c\u5ab2\u7f8e\u6700\u5148\u8fdb\u7684\u95ed\u6e90\u6a21\u578b\uff0c\u5e76\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5c55\u73b0\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ProAct\u901a\u8fc7\u5185\u90e8\u5316\u524d\u77bb\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u957f\u7a0b\u89c4\u5212\u4e2d\u7684\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\uff0c\u4e3a\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.05353", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05353", "abs": "https://arxiv.org/abs/2602.05353", "authors": ["Ruijie Shi", "Houbin Zhang", "Yuecheng Han", "Yuheng Wang", "Jingru Fan", "Runde Yang", "Yufan Dang", "Huatao Li", "Dewen Liu", "Yuan Cheng", "Chen Qian"], "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction", "comment": null, "summary": "Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black boxes to users. We address this by introducing Agentic Workflow Reconstruction (AWR), a new task aiming to synthesize an explicit, interpretable stand-in workflow that approximates a black-box system using only input--output access. We propose AgentXRay, a search-based framework that formulates AWR as a combinatorial optimization problem over discrete agent roles and tool invocations in a chain-structured workflow space. Unlike model distillation, AgentXRay produces editable white-box workflows that match target outputs under an observable, output-based proxy metric, without accessing model parameters. To navigate the vast search space, AgentXRay employs Monte Carlo Tree Search enhanced by a scoring-based Red-Black Pruning mechanism, which dynamically integrates proxy quality with search depth. Experiments across diverse domains demonstrate that AgentXRay achieves higher proxy similarity and reduces token consumption compared to unpruned search, enabling deeper workflow exploration under fixed iteration budgets.", "AI": {"tldr": "\u63d0\u51faAgentXRay\u6846\u67b6\uff0c\u901a\u8fc7\u641c\u7d22\u65b9\u6cd5\u4ece\u9ed1\u76d2\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8f93\u5165\u8f93\u51fa\u4e2d\u91cd\u5efa\u53ef\u89e3\u91ca\u7684\u5de5\u4f5c\u6d41\uff0c\u89e3\u51b3\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0d\u900f\u660e\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u8bb8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7531\u4e8e\u5185\u90e8\u5de5\u4f5c\u6d41\u7a0b\u4e0d\u900f\u660e\u800c\u96be\u4ee5\u89e3\u91ca\u548c\u63a7\u5236\u3002\u867d\u7136\u6709\u4e9b\u6846\u67b6\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u534f\u4f5c\u67b6\u6784\uff0c\u4f46\u8bb8\u591a\u90e8\u7f72\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5bf9\u7528\u6237\u6765\u8bf4\u4ecd\u7136\u662f\u9ed1\u76d2\u3002", "method": "\u63d0\u51faAgentic Workflow Reconstruction (AWR)\u4efb\u52a1\uff0c\u65e8\u5728\u4ec5\u4f7f\u7528\u8f93\u5165\u8f93\u51fa\u8bbf\u95ee\u5408\u6210\u663e\u5f0f\u3001\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u5de5\u4f5c\u6d41\u3002\u5f00\u53d1AgentXRay\u6846\u67b6\uff0c\u5c06AWR\u8868\u8ff0\u4e3a\u94fe\u5f0f\u7ed3\u6784\u5de5\u4f5c\u6d41\u7a7a\u95f4\u4e2d\u79bb\u6563\u667a\u80fd\u4f53\u89d2\u8272\u548c\u5de5\u5177\u8c03\u7528\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002\u91c7\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u8bc4\u5206\u7684\u7ea2\u9ed1\u526a\u679d\u673a\u5236\u589e\u5f3a\uff0c\u52a8\u6001\u6574\u5408\u4ee3\u7406\u8d28\u91cf\u4e0e\u641c\u7d22\u6df1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAgentXRay\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4ee3\u7406\u76f8\u4f3c\u5ea6\uff0c\u5e76\u51cf\u5c11\u4e86\u4ee4\u724c\u6d88\u8017\uff0c\u76f8\u6bd4\u672a\u526a\u679d\u641c\u7d22\u80fd\u591f\u5728\u56fa\u5b9a\u8fed\u4ee3\u9884\u7b97\u4e0b\u8fdb\u884c\u66f4\u6df1\u5c42\u7684\u5de5\u4f5c\u6d41\u63a2\u7d22\u3002", "conclusion": "AgentXRay\u80fd\u591f\u4ece\u9ed1\u76d2\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u91cd\u5efa\u53ef\u7f16\u8f91\u7684\u767d\u76d2\u5de5\u4f5c\u6d41\uff0c\u63d0\u9ad8\u7cfb\u7edf\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u53c2\u6570\u3002", "topic": "agent analysis"}}
{"id": "2602.05354", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05354", "abs": "https://arxiv.org/abs/2602.05354", "authors": ["Shifat E. Arman", "Syed Nazmus Sakib", "Tapodhir Karmakar Taton", "Nafiul Haque", "Shahrear Bin Amin"], "title": "PATHWAYS: Evaluating Investigation and Context Discovery in AI Web Agents", "comment": "35 pages, 13 figures", "summary": "We introduce PATHWAYS, a benchmark of 250 multi-step decision tasks that test whether web-based agents can discover and correctly use hidden contextual information. Across both closed and open models, agents typically navigate to relevant pages but retrieve decisive hidden evidence in only a small fraction of cases. When tasks require overturning misleading surface-level signals, performance drops sharply to near chance accuracy. Agents frequently hallucinate investigative reasoning by claiming to rely on evidence they never accessed. Even when correct context is discovered, agents often fail to integrate it into their final decision. Providing more explicit instructions improves context discovery but often reduces overall accuracy, revealing a tradeoff between procedural compliance and effective judgement. Together, these results show that current web agent architectures lack reliable mechanisms for adaptive investigation, evidence integration, and judgement override.", "AI": {"tldr": "PATHWAYS\u662f\u4e00\u4e2a\u5305\u542b250\u4e2a\u591a\u6b65\u51b3\u7b56\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u7f51\u7edc\u7684\u667a\u80fd\u4f53\u662f\u5426\u80fd\u53d1\u73b0\u5e76\u6b63\u786e\u4f7f\u7528\u9690\u85cf\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u667a\u80fd\u4f53\u5728\u53d1\u73b0\u9690\u85cf\u8bc1\u636e\u3001\u6574\u5408\u8bc1\u636e\u548c\u63a8\u7ffb\u8bef\u5bfc\u6027\u8868\u9762\u4fe1\u53f7\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7f51\u7edc\u7684\u667a\u80fd\u4f53\u5728\u5904\u7406\u9700\u8981\u53d1\u73b0\u9690\u85cf\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u65f6\uff0c\u5176\u5b9e\u9645\u80fd\u529b\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u8005\u5e0c\u671b\u8bc4\u4f30\u667a\u80fd\u4f53\u662f\u5426\u80fd\u8fdb\u884c\u9002\u5e94\u6027\u8c03\u67e5\u3001\u8bc1\u636e\u6574\u5408\u548c\u5224\u65ad\u8986\u76d6\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u63a8\u7ffb\u8bef\u5bfc\u6027\u8868\u9762\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u7814\u7a76\u8005\u521b\u5efa\u4e86PATHWAYS\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b250\u4e2a\u591a\u6b65\u51b3\u7b56\u4efb\u52a1\uff0c\u6d4b\u8bd5\u667a\u80fd\u4f53\u53d1\u73b0\u548c\u4f7f\u7528\u9690\u85cf\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u80fd\u529b\u3002\u8bc4\u4f30\u4e86\u5c01\u95ed\u548c\u5f00\u653e\u6a21\u578b\uff0c\u5206\u6790\u4e86\u667a\u80fd\u4f53\u5728\u5bfc\u822a\u3001\u8bc1\u636e\u53d1\u73b0\u3001\u8bc1\u636e\u6574\u5408\u548c\u5224\u65ad\u8986\u76d6\u7b49\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u667a\u80fd\u4f53\u901a\u5e38\u80fd\u5bfc\u822a\u5230\u76f8\u5173\u9875\u9762\uff0c\u4f46\u53ea\u5728\u5c11\u6570\u60c5\u51b5\u4e0b\u80fd\u68c0\u7d22\u5230\u51b3\u5b9a\u6027\u7684\u9690\u85cf\u8bc1\u636e\u3002\u5f53\u4efb\u52a1\u9700\u8981\u63a8\u7ffb\u8bef\u5bfc\u6027\u8868\u9762\u4fe1\u53f7\u65f6\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d\u5230\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\u3002\u667a\u80fd\u4f53\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\uff0c\u58f0\u79f0\u4f9d\u8d56\u4ece\u672a\u8bbf\u95ee\u8fc7\u7684\u8bc1\u636e\u3002\u5373\u4f7f\u53d1\u73b0\u4e86\u6b63\u786e\u7684\u4e0a\u4e0b\u6587\uff0c\u4e5f\u5e38\u5e38\u65e0\u6cd5\u5c06\u5176\u6574\u5408\u5230\u6700\u7ec8\u51b3\u7b56\u4e2d\u3002\u63d0\u4f9b\u66f4\u660e\u786e\u7684\u6307\u4ee4\u80fd\u6539\u5584\u4e0a\u4e0b\u6587\u53d1\u73b0\uff0c\u4f46\u901a\u5e38\u4f1a\u964d\u4f4e\u6574\u4f53\u51c6\u786e\u6027\u3002", "conclusion": "\u5f53\u524d\u7f51\u7edc\u667a\u80fd\u4f53\u67b6\u6784\u7f3a\u4e4f\u53ef\u9760\u7684\u9002\u5e94\u6027\u8c03\u67e5\u3001\u8bc1\u636e\u6574\u5408\u548c\u5224\u65ad\u8986\u76d6\u673a\u5236\u3002\u9700\u8981\u5728\u667a\u80fd\u4f53\u67b6\u6784\u4e2d\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u673a\u5236\u6765\u5904\u7406\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u9690\u85cf\u4fe1\u606f\u548c\u8bef\u5bfc\u6027\u4fe1\u53f7\u3002", "topic": "agent analysis"}}
{"id": "2602.04942", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04942", "abs": "https://arxiv.org/abs/2602.04942", "authors": ["Emiliano Penaloza", "Dheeraj Vattikonda", "Nicolas Gontier", "Alexandre Lacoste", "Laurent Charlin", "Massimo Caccia"], "title": "Privileged Information Distillation for Language Models", "comment": "Abstract border should have been purple", "summary": "Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, where closed-source systems typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable but the reasoning process is not. For this, we introduce \u03c0-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically we find that \u03c0-Distill and in some cases OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on \u03c0-Distill and characterizing when OPSD is competitive.", "AI": {"tldr": "\u63d0\u51fa\u03c0-Distill\u548cOPSD\u4e24\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u65f6\u5229\u7528\u7279\u6743\u4fe1\u606f\u4f46\u63a8\u7406\u65f6\u65e0\u6cd5\u8bbf\u95ee\u7684\u60c5\u51b5\u4e0b\uff0c\u84b8\u998f\u524d\u6cbf\u6a21\u578b\u5728\u591a\u8f6e\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u5728\u667a\u80fd\u4f53\u73af\u5883\u4e2d\uff0c\u95ed\u6e90\u7cfb\u7edf\u901a\u5e38\u53ea\u66b4\u9732\u52a8\u4f5c\u8f68\u8ff9\u800c\u9690\u85cf\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\uff0c\u8fd9\u4f7f\u5f97\u6807\u51c6\u84b8\u998f\u6d41\u7a0b\u5931\u6548\u3002\u9700\u8981\u89e3\u51b3\u5982\u4f55\u5728\u53ea\u6709\u52a8\u4f5c\u8f68\u8ff9\u7279\u6743\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u524d\u6cbf\u6a21\u578b\u7684\u80fd\u529b\u84b8\u998f\u5230\u63a8\u7406\u65f6\u65e0\u6cd5\u8bbf\u95ee\u7279\u6743\u4fe1\u606f\u7684\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u03c0-Distill\uff1a\u8054\u5408\u6559\u5e08-\u5b66\u751f\u76ee\u6807\uff0c\u540c\u65f6\u8bad\u7ec3\u7279\u6743\u4fe1\u606f\u6761\u4ef6\u5316\u7684\u6559\u5e08\u548c\u65e0\u6761\u4ef6\u7684\u5b66\u751f\u6a21\u578b\uff1b2) OPSD\uff1a\u57fa\u4e8e\u7b56\u7565\u7684\u81ea\u84b8\u998f\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5e76\u52a0\u5165\u5b66\u751f\u4e0e\u6559\u5e08\u4e4b\u95f4\u7684\u53cd\u5411KL\u60e9\u7f5a\u9879\u3002", "result": "\u4e24\u79cd\u7b97\u6cd5\u90fd\u80fd\u6709\u6548\u5229\u7528\u4ec5\u52a8\u4f5c\u7279\u6743\u4fe1\u606f\u84b8\u998f\u524d\u6cbf\u667a\u80fd\u4f53\u3002\u03c0-Distill\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0bOPSD\uff0c\u5728\u591a\u4e2a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u3001\u6a21\u578b\u548c\u7279\u6743\u4fe1\u606f\u5f62\u5f0f\u4e0a\uff0c\u90fd\u4f18\u4e8e\u884c\u4e1a\u6807\u51c6\u5b9e\u8df5\uff08\u76d1\u7763\u5fae\u8c03\u540e\u63a5\u5f3a\u5316\u5b66\u4e60\uff09\u3002", "conclusion": "\u03c0-Distill\u548cOPSD\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5728\u53ea\u6709\u52a8\u4f5c\u8f68\u8ff9\u7279\u6743\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u84b8\u998f\u524d\u6cbf\u6a21\u578b\u7684\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u4f53\u84b8\u998f\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.05381", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05381", "abs": "https://arxiv.org/abs/2602.05381", "authors": ["Ting Fang Tan", "Kabilan Elangovan", "Andreas Pollreisz", "Kevin Bryan Dy", "Wei Yan Ng", "Joy Le Yi Wong", "Jin Liyuan", "Chrystie Quek Wan Ning", "Ashley Shuen Ying Hong", "Arun James Thirunavukarasu", "Shelley Yin-His Chang", "Jie Yao", "Dylan Hong", "Wang Zhaoran", "Amrita Gupta", "Daniel SW Ting"], "title": "Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation", "comment": null, "summary": "Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.", "AI": {"tldr": "\u8bc4\u4f30\u56db\u4e2a\u5c0f\u578b\u533b\u7597LLM\u5728\u773c\u79d1\u60a3\u8005\u54a8\u8be2\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6d4b\u8bd5GPT-4-Turbo\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\u7684\u53ef\u884c\u6027\u3002Meerkat-7B\u8868\u73b0\u6700\u4f73\uff0cMedLLaMA3-v20\u6700\u5dee\u4e14\u670925.5%\u7684\u5e7b\u89c9\u5185\u5bb9\u3002GPT-4-Turbo\u8bc4\u4f30\u4e0e\u4e34\u5e8a\u533b\u751f\u8bc4\u5206\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "\u968f\u7740\u9886\u57df\u7279\u5b9a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u773c\u79d1\u60a3\u8005\u6559\u80b2\u3001\u5206\u8bca\u548c\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u9700\u8981\u4e25\u683c\u8bc4\u4f30\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5c0f\u578b\u533b\u7597LLM\u5728\u56de\u7b54\u773c\u79d1\u60a3\u8005\u54a8\u8be2\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22LLM\u8bc4\u4f30\u76f8\u5bf9\u4e8e\u4e34\u5e8a\u533b\u751f\u8bc4\u5206\u7684\u53ef\u884c\u6027\u3002", "method": "\u6a2a\u65ad\u9762\u7814\u7a76\uff0c\u4f7f\u7528180\u4e2a\u773c\u79d1\u60a3\u8005\u54a8\u8be2\uff0c\u7531\u56db\u4e2a\u53c2\u6570\u5c0f\u4e8e100\u4ebf\u7684\u533b\u7597LLM\uff08Meerkat-7B\u3001BioMistral-7B\u3001OpenBioLLM-8B\u3001MedLLaMA3-v20\uff09\u56de\u7b54\uff0c\u5171\u751f\u62102160\u4e2a\u56de\u7b54\u3002\u7531\u4e09\u4f4d\u4e0d\u540c\u8d44\u5386\u7684\u773c\u79d1\u533b\u751f\u548cGPT-4-Turbo\u4f7f\u7528S.C.O.R.E.\u6846\u67b6\uff08\u5b89\u5168\u6027\u3001\u5171\u8bc6\u4e0e\u4e0a\u4e0b\u6587\u3001\u5ba2\u89c2\u6027\u3001\u53ef\u91cd\u590d\u6027\u3001\u53ef\u89e3\u91ca\u6027\uff09\u8fdb\u884c\u4e94\u7ea7\u674e\u514b\u7279\u91cf\u8868\u8bc4\u5206\u3002\u4f7f\u7528Spearman\u79e9\u76f8\u5173\u3001Kendall tau\u7edf\u8ba1\u548c\u6838\u5bc6\u5ea6\u4f30\u8ba1\u5206\u6790\u8bc4\u4f30LLM\u4e0e\u4e34\u5e8a\u533b\u751f\u8bc4\u5206\u7684\u4e00\u81f4\u6027\u3002", "result": "Meerkat-7B\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u5f97\u5206\uff1a\u9ad8\u7ea7\u987e\u95ee3.44\u3001\u987e\u95ee4.08\u3001\u4f4f\u9662\u533b\u5e084.18\u3002MedLLaMA3-v20\u8868\u73b0\u6700\u5dee\uff0c25.5%\u7684\u56de\u7b54\u5305\u542b\u5e7b\u89c9\u6216\u4e34\u5e8a\u8bef\u5bfc\u5185\u5bb9\uff0c\u5305\u62ec\u634f\u9020\u7684\u672f\u8bed\u3002GPT-4-Turbo\u8bc4\u4f30\u4e0e\u4e34\u5e8a\u533b\u751f\u6574\u4f53\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff08Spearman rho=0.80\uff0cKendall tau=0.67\uff09\uff0c\u4f46\u9ad8\u7ea7\u987e\u95ee\u8bc4\u5206\u66f4\u4fdd\u5b88\u3002", "conclusion": "\u533b\u7597LLM\u5728\u773c\u79d1\u95ee\u7b54\u4e2d\u663e\u793a\u51fa\u5b89\u5168\u6f5c\u529b\uff0c\u4f46\u5728\u4e34\u5e8a\u6df1\u5ea6\u548c\u5171\u8bc6\u65b9\u9762\u4ecd\u6709\u5dee\u8ddd\u3002\u652f\u6301LLM\u8bc4\u4f30\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u9700\u8981\u6df7\u5408\u81ea\u52a8\u5316\u548c\u4e34\u5e8a\u533b\u751f\u5ba1\u67e5\u6846\u67b6\u6765\u6307\u5bfc\u5b89\u5168\u7684\u4e34\u5e8a\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2602.05444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05444", "abs": "https://arxiv.org/abs/2602.05444", "authors": ["Yao Zhou", "Zeen Song", "Wenwen Qiang", "Fengge Wu", "Shuyi Zhou", "Changwen Zheng", "Hui Xiong"], "title": "Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs", "comment": null, "summary": "Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \\textbf{C}ausal \\textbf{F}ront-Door \\textbf{A}djustment \\textbf{A}ttack ({\\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.", "AI": {"tldr": "\u63d0\u51faCFA\u00b2\u653b\u51fb\u6846\u67b6\uff0c\u5229\u7528\u56e0\u679c\u524d\u95e8\u51c6\u5219\u5207\u65ad\u5b89\u5168\u673a\u5236\u4e0e\u4efb\u52a1\u610f\u56fe\u7684\u6df7\u6dc6\u5173\u8054\uff0c\u5b9e\u73b0\u9ad8\u6548\u8d8a\u72f1", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u5bf9\u9f50\u673a\u5236\u4f5c\u4e3a\u6f5c\u5728\u5185\u90e8\u72b6\u6001\u8fd0\u884c\uff0c\u63a9\u76d6\u4e86\u6a21\u578b\u56fa\u6709\u80fd\u529b\uff0c\u9700\u8981\u4ece\u56e0\u679c\u89d2\u5ea6\u5efa\u6a21\u5b89\u5168\u673a\u5236\u4ee5\u63ed\u793a\u5e76\u7ed5\u8fc7\u8fd9\u4e9b\u9632\u5fa1", "method": "\u5c06\u5b89\u5168\u673a\u5236\u5efa\u6a21\u4e3a\u672a\u89c2\u6d4b\u7684\u6df7\u6dc6\u53d8\u91cf\uff0c\u91c7\u7528Pearl\u524d\u95e8\u51c6\u5219\u5207\u65ad\u6df7\u6dc6\u5173\u8054\uff1b\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7269\u7406\u5265\u79bb\u9632\u5fa1\u76f8\u5173\u7279\u5f81\uff0c\u9694\u79bb\u6838\u5fc3\u4efb\u52a1\u610f\u56fe\uff1b\u5c06\u8ba1\u7b97\u6602\u8d35\u7684\u8fb9\u7f18\u5316\u7b80\u5316\u4e3a\u786e\u5b9a\u6027\u5e72\u9884", "result": "CFA\u00b2\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u4e3a\u8d8a\u72f1\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u89e3\u91ca", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u56e0\u679c\u63a8\u7406\u5728\u7406\u89e3\u548c\u653b\u51fbLLM\u5b89\u5168\u673a\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b89\u5168\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2", "topic": "agent analysis"}}
{"id": "2602.05447", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05447", "abs": "https://arxiv.org/abs/2602.05447", "authors": ["Damon McMillan"], "title": "Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale", "comment": "8 pages, 7 figures, 10 tables, 26 references", "summary": "Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.\n  Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.\n  These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.", "AI": {"tldr": "\u7cfb\u7edf\u7814\u7a76LLM\u4ee3\u7406\u5728\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u662f\u4e3b\u5bfc\u56e0\u7d20\uff0c\u67b6\u6784\u9009\u62e9\u9700\u6839\u636e\u6a21\u578b\u80fd\u529b\u5b9a\u5236\u800c\u975e\u901a\u7528\u6700\u4f73\u5b9e\u8df5", "motivation": "LLM\u4ee3\u7406\u8d8a\u6765\u8d8a\u591a\u5730\u901a\u8fc7\u7f16\u7a0b\u63a5\u53e3\u64cd\u4f5c\u5916\u90e8\u7cfb\u7edf\uff0c\u4f46\u4ece\u4e1a\u8005\u7f3a\u4e4f\u5173\u4e8e\u5982\u4f55\u6784\u5efa\u4ee3\u7406\u6240\u6d88\u8d39\u4e0a\u4e0b\u6587\u7684\u5b9e\u8bc1\u6307\u5bfc\u3002\u4f7f\u7528SQL\u751f\u6210\u4e3a\u4ee3\u7406\u64cd\u4f5c\u7684\u4ee3\u7406\uff0c\u7814\u7a76\u7ed3\u6784\u5316\u6570\u636e\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u3002", "method": "\u4f7f\u7528SQL\u751f\u6210\u4f5c\u4e3a\u4ee3\u7406\u64cd\u4f5c\u7684\u4ee3\u7406\uff0c\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\uff1a9,649\u4e2a\u5b9e\u9a8c\uff0c\u6db5\u76d611\u4e2a\u6a21\u578b\u30014\u79cd\u683c\u5f0f\uff08YAML\u3001Markdown\u3001JSON\u3001TOON\uff09\u548c10\u523010,000\u4e2a\u8868\u7684\u6a21\u5f0f\u3002", "result": "1) \u67b6\u6784\u9009\u62e9\u4f9d\u8d56\u6a21\u578b\uff1a\u524d\u6cbf\u6a21\u578b\uff08Claude\u3001GPT\u3001Gemini\uff09\u4f7f\u7528\u57fa\u4e8e\u6587\u4ef6\u7684\u4e0a\u4e0b\u6587\u68c0\u7d22\u51c6\u786e\u7387\u63d0\u9ad82.7%\uff0c\u5f00\u6e90\u6a21\u578b\u5219\u4e0b\u964d7.7%\uff1b2) \u683c\u5f0f\u5bf9\u603b\u4f53\u51c6\u786e\u7387\u65e0\u663e\u8457\u5f71\u54cd\uff1b3) \u6a21\u578b\u80fd\u529b\u662f\u4e3b\u5bfc\u56e0\u7d20\uff0c\u524d\u6cbf\u4e0e\u5f00\u6e90\u6a21\u578b\u95f4\u670921\u4e2a\u767e\u5206\u70b9\u7684\u51c6\u786e\u7387\u5dee\u8ddd\uff1b4) \u6587\u4ef6\u539f\u751f\u4ee3\u7406\u901a\u8fc7\u57df\u5206\u533a\u6a21\u5f0f\u53ef\u6269\u5c55\u523010,000\u4e2a\u8868\uff1b5) \u6587\u4ef6\u5927\u5c0f\u4e0d\u80fd\u9884\u6d4b\u8fd0\u884c\u65f6\u6548\u7387\u3002", "conclusion": "\u4e3a\u5728\u7ed3\u6784\u5316\u7cfb\u7edf\u4e0a\u90e8\u7f72LLM\u4ee3\u7406\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u7684\u6307\u5bfc\uff0c\u8868\u660e\u67b6\u6784\u51b3\u7b56\u5e94\u6839\u636e\u6a21\u578b\u80fd\u529b\u5b9a\u5236\uff0c\u800c\u975e\u5047\u8bbe\u901a\u7528\u6700\u4f73\u5b9e\u8df5\u3002", "topic": "agent analysis"}}
{"id": "2602.05429", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05429", "abs": "https://arxiv.org/abs/2602.05429", "authors": ["Rui Lv", "Juncheng Mo", "Tianyi Chu", "Chen Rao", "Hongyi Jing", "Jiajie Teng", "Jiafu Chen", "Shiqi Zhang", "Liangzi Ding", "Shuo Fang", "Huaizhong Lin", "Ziqiang Dang", "Chenguang Ma", "Lei Zhao"], "title": "M$^2$-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining", "comment": "Accepted by ICLR 2026. Supplementary material is included at the end of the main paper (16 pages, 15 figures, 2 tables)", "summary": "Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (i.e., intent-trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M$^2$-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.", "AI": {"tldr": "M\u00b2-Miner\uff1a\u9996\u4e2a\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u4f4e\u6210\u672c\u81ea\u52a8\u5316\u79fb\u52a8GUI\u667a\u80fd\u4f53\u6570\u636e\u6316\u6398\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u610f\u56fe\u56de\u6536\u7b56\u7565\u89e3\u51b3GUI\u6570\u636e\u6807\u6ce8\u7684\u9ad8\u6210\u672c\u3001\u4f4e\u8d28\u91cf\u548c\u4f4e\u4e30\u5bcc\u5ea6\u95ee\u9898\u3002", "motivation": "\u6784\u5efa\u5f3a\u5927\u7684GUI\u667a\u80fd\u4f53\u9700\u8981\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u7528\u6237\u884c\u4e3a\u8f68\u8ff9\u6570\u636e\uff08\u610f\u56fe-\u8f68\u8ff9\u5bf9\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u73b0\u6709\u624b\u52a8\u6807\u6ce8\u65b9\u6cd5\u548cGUI\u667a\u80fd\u4f53\u6570\u636e\u6316\u6398\u65b9\u6cd5\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u9ad8\u6784\u5efa\u6210\u672c\u3001\u6570\u636e\u8d28\u91cf\u5dee\u548c\u6570\u636e\u4e30\u5bcc\u5ea6\u4f4e\u3002", "method": "\u63d0\u51faM\u00b2-Miner\u6846\u67b6\uff0c\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u5b9e\u73b0\u4f4e\u6210\u672c\u81ea\u52a8\u5316\u6570\u636e\u6316\u6398\u3002\u91c7\u7528\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff08InferAgent\u3001OrchestraAgent\u3001JudgeAgent\uff09\u8fdb\u884c\u5f15\u5bfc\u3001\u52a0\u901f\u548c\u8bc4\u4f30\uff1b\u8bbe\u8ba1\u610f\u56fe\u56de\u6536\u7b56\u7565\u63d0\u53d6\u989d\u5916\u6709\u4ef7\u503c\u7684\u4ea4\u4e92\u8f68\u8ff9\uff1b\u5f15\u5165\u6e10\u8fdb\u5f0f\u6a21\u578b\u5728\u73af\u8bad\u7ec3\u7b56\u7565\u63d0\u9ad8\u6570\u636e\u6316\u6398\u6210\u529f\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528M\u00b2-Miner\u6316\u6398\u7684\u6570\u636e\u5fae\u8c03\u7684GUI\u667a\u80fd\u4f53\u5728\u591a\u4e2a\u5e38\u7528\u79fb\u52a8GUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "M\u00b2-Miner\u6210\u529f\u89e3\u51b3\u4e86GUI\u667a\u80fd\u4f53\u6570\u636e\u6316\u6398\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u793e\u533a\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2602.05472", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05472", "abs": "https://arxiv.org/abs/2602.05472", "authors": ["Yiwen Duan", "Jing Ye", "Xinpei Zhao"], "title": "ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation", "comment": null, "summary": "The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \\textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \\textbf{costly} to scale, \\textbf{brittle} across domains, and \\textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \\textbf{ALIVE} (\\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \\emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.", "AI": {"tldr": "ALIVE\u6846\u67b6\u901a\u8fc7\u5bf9\u6297\u5b66\u4e60\u548c\u6307\u5bfc\u6027\u8bed\u8a00\u53cd\u9988\uff0c\u8ba9LLM\u5185\u90e8\u5316\u63a8\u7406\u903b\u8f91\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u63a8\u7406\u80fd\u529b", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u6807\u91cf\u5956\u52b1\u5b58\u5728\u6210\u672c\u9ad8\u3001\u8de8\u9886\u57df\u8106\u5f31\u3001\u65e0\u6cd5\u7406\u89e3\u89e3\u51b3\u65b9\u6848\u5e95\u5c42\u903b\u8f91\u7b49\u95ee\u9898\uff0c\u963b\u788d\u4e86LLM\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u63a8\u7406\u80fd\u529b", "method": "\u63d0\u51faALIVE\u6846\u67b6\uff0c\u57fa\u4e8e\u8ba4\u77e5\u534f\u540c\u539f\u5219\uff0c\u5c06\u95ee\u9898\u63d0\u51fa\u3001\u89e3\u51b3\u548c\u8bc4\u5224\u7edf\u4e00\u5728\u5355\u4e00\u7b56\u7565\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u5bf9\u6297\u5b66\u4e60\u548c\u6307\u5bfc\u6027\u8bed\u8a00\u53cd\u9988\u8ba9\u6a21\u578b\u4ece\u539f\u59cb\u8bed\u6599\u4e2d\u5185\u90e8\u5316\u8bc4\u4f30\u6807\u51c6", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u4e00\u822c\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cALIVE\u663e\u8457\u7f13\u89e3\u4e86\u5956\u52b1\u4fe1\u53f7\u9650\u5236\uff0c\u5728\u76f8\u540c\u6570\u636e\u548c\u8ba1\u7b97\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u51c6\u786e\u7387\u63d0\u5347\u3001\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u589e\u5f3a\u548c\u66f4\u9ad8\u7684\u81ea\u6211\u7ea0\u6b63\u7387", "conclusion": "\u63a8\u7406\u4e09\u4f4d\u4e00\u4f53\u4fc3\u8fdb\u4e86\u80fd\u529b\u589e\u957f\u7684\u81ea\u6211\u7ef4\u6301\u8f68\u8ff9\uff0c\u4f7fALIVE\u6210\u4e3a\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u7684\u901a\u7528\u63a8\u7406\u5bf9\u9f50\u7684\u53ef\u6269\u5c55\u57fa\u7840", "topic": "agentic reinforcement learning"}}
{"id": "2602.05031", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05031", "abs": "https://arxiv.org/abs/2602.05031", "authors": ["Dikshant Shehmar", "Matthew Schlegel", "Matthew E. Taylor", "Marlos C. Machado"], "title": "Laplacian Representations for Decision-Time Planning", "comment": null, "summary": "Planning with a learned model remains a key challenge in model-based reinforcement learning (RL). In decision-time planning, state representations are critical as they must support local cost computation while preserving long-horizon structure. In this paper, we show that the Laplacian representation provides an effective latent space for planning by capturing state-space distances at multiple time scales. This representation preserves meaningful distances and naturally decomposes long-horizon problems into subgoals, also mitigating the compounding errors that arise over long prediction horizons. Building on these properties, we introduce ALPS, a hierarchical planning algorithm, and demonstrate that it outperforms commonly used baselines on a selection of offline goal-conditioned RL tasks from OGBench, a benchmark previously dominated by model-free methods.", "AI": {"tldr": "ALPS\u662f\u4e00\u79cd\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u8868\u793a\u7684\u5206\u5c42\u89c4\u5212\u7b97\u6cd5\uff0c\u5728\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6RL\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5e38\u7528\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u57fa\u4e8e\u5b66\u4e60\u6a21\u578b\u7684\u89c4\u5212\u4ecd\u7136\u662f\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002\u51b3\u7b56\u65f6\u89c4\u5212\u4e2d\uff0c\u72b6\u6001\u8868\u793a\u5fc5\u987b\u652f\u6301\u5c40\u90e8\u6210\u672c\u8ba1\u7b97\u540c\u65f6\u4fdd\u6301\u957f\u65f6\u7a0b\u7ed3\u6784\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u62c9\u666e\u62c9\u65af\u8868\u793a\u4f5c\u4e3a\u6709\u6548\u7684\u89c4\u5212\u6f5c\u7a7a\u95f4\uff0c\u8be5\u8868\u793a\u80fd\u6355\u6349\u591a\u65f6\u95f4\u5c3a\u5ea6\u7684\u72b6\u6001\u7a7a\u95f4\u8ddd\u79bb\uff0c\u4fdd\u6301\u6709\u610f\u4e49\u7684\u8ddd\u79bb\u5ea6\u91cf\uff0c\u5e76\u5c06\u957f\u65f6\u7a0b\u95ee\u9898\u81ea\u7136\u5206\u89e3\u4e3a\u5b50\u76ee\u6807\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86ALPS\u5206\u5c42\u89c4\u5212\u7b97\u6cd5\u3002", "result": "\u5728OGBench\u7684\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6RL\u4efb\u52a1\u4e0a\uff0cALPS\u8d85\u8d8a\u4e86\u5e38\u7528\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e4b\u524d\u7531\u65e0\u6a21\u578b\u65b9\u6cd5\u4e3b\u5bfc\u7684\u57fa\u51c6\u3002", "conclusion": "\u62c9\u666e\u62c9\u65af\u8868\u793a\u4e3a\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6f5c\u7a7a\u95f4\uff0cALPS\u7b97\u6cd5\u5c55\u793a\u4e86\u5176\u5728\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.05051", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05051", "abs": "https://arxiv.org/abs/2602.05051", "authors": ["Songyuan Zhang", "Oswin So", "H. M. Sabbir Ahmad", "Eric Yang Yu", "Matthew Cleaveland", "Mitchell Black", "Chuchu Fan"], "title": "ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation", "comment": "24 pages, 17 figures; Accepted by the fourteenth International Conference on Learning Representations (ICLR 2026)", "summary": "Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed dataset generated by behavior policies without additional environment interactions. One common challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.", "AI": {"tldr": "ReFORM\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u7b56\u7565\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u9020\u5f3a\u5236\u6267\u884c\u8f83\u5bbd\u677e\u7684\u652f\u6301\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u5206\u5e03\u5916\u8bef\u5dee\u95ee\u9898\uff0c\u540c\u65f6\u5728OGBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u5206\u5e03\u5916\u8bef\u5dee\u95ee\u9898\uff0c\u5f53\u7b56\u7565\u79bb\u5f00\u8bad\u7ec3\u5206\u5e03\u65f6\u4f1a\u4ea7\u751f\u8bef\u5dee\uff1b2\uff09\u6700\u4f18\u7b56\u7565\u5206\u5e03\u53ef\u80fd\u662f\u591a\u6a21\u6001\u4e14\u96be\u4ee5\u8868\u793a\u7684\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8fc7\u5ea6\u7ea6\u675f\u7b56\u7565\u6539\u8fdb\uff0c\u8981\u4e48\u65e0\u6cd5\u540c\u65f6\u4fdd\u8bc1\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u548c\u907f\u514d\u5206\u5e03\u5916\u8bef\u5dee\u3002", "method": "ReFORM\u57fa\u4e8e\u6d41\u7b56\u7565\u8bbe\u8ba1\uff1a1\uff09\u5b66\u4e60\u4e00\u4e2a\u6709\u754c\u6e90\u5206\u5e03\u7684\u884c\u4e3a\u514b\u9686\u6d41\u7b56\u7565\u6765\u6355\u6349\u52a8\u4f5c\u5206\u5e03\u7684\u652f\u6301\uff1b2\uff09\u4f18\u5316\u4e00\u4e2a\u53cd\u5c04\u6d41\uff0c\u4e3aBC\u6d41\u751f\u6210\u6709\u754c\u566a\u58f0\u540c\u65f6\u4fdd\u6301\u652f\u6301\uff0c\u4ee5\u6700\u5927\u5316\u6027\u80fd\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u6784\u9020\u5f3a\u5236\u6267\u884c\u652f\u6301\u7ea6\u675f\u3002", "result": "\u5728OGBench\u57fa\u51c6\u768440\u4e2a\u5177\u6709\u4e0d\u540c\u8d28\u91cf\u6570\u636e\u96c6\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u6052\u5b9a\u8d85\u53c2\u6570\u7684ReFORM\u5728\u6027\u80fd\u66f2\u7ebf\u56fe\u4e0a\u4f18\u4e8e\u6240\u6709\u7ecf\u8fc7\u624b\u52a8\u8c03\u4f18\u8d85\u53c2\u6570\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ReFORM\u901a\u8fc7\u6d41\u7b56\u7565\u548c\u53cd\u5c04\u6d41\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u6709\u6548\u907f\u514d\u4e86\u5206\u5e03\u5916\u8bef\u5dee\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.05758", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05758", "abs": "https://arxiv.org/abs/2602.05758", "authors": ["Bowen Ping", "Zijun Chen", "Yiyao Yu", "Tingfeng Hui", "Junchi Yan", "Baobao Chang"], "title": "LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards", "comment": null, "summary": "Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic \"Think-and-Read\" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.", "AI": {"tldr": "LongR\u662f\u4e00\u4e2a\u589e\u5f3a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\"\u601d\u8003-\u9605\u8bfb\"\u673a\u5236\u548c\u4e0a\u4e0b\u6587\u5bc6\u5ea6\u5956\u52b1\uff0c\u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u5408\u6210\u6216\u67b6\u6784\u4fee\u6539\uff0c\u4f46\u4ec5\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\u5728\u590d\u6742\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u6307\u5bfc\u4fe1\u53f7", "method": "\u63d0\u51faLongR\u7edf\u4e00\u6846\u67b6\uff1a1\uff09\u52a8\u6001\"\u601d\u8003-\u9605\u8bfb\"\u673a\u5236\uff0c\u4ea4\u66ff\u8fdb\u884c\u63a8\u7406\u548c\u6587\u6863\u67e5\u9605\uff1b2\uff09\u57fa\u4e8e\u76f8\u5bf9\u4fe1\u606f\u589e\u76ca\u7684\u4e0a\u4e0b\u6587\u5bc6\u5ea6\u5956\u52b1\uff0c\u91cf\u5316\u76f8\u5173\u6587\u6863\u7684\u6548\u7528", "result": "\u5728LongBench v2\u4e0a\u83b7\u5f979%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728RULER\u548cInfiniteBench\u4e0a\u5b9e\u73b0\u4e00\u81f4\u6539\u8fdb\uff0c\u5728\u4e0d\u540cRL\u7b97\u6cd5\uff08DAPO\u3001GSPO\uff09\u4e0a\u5747\u6709\u6548", "conclusion": "LongR\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u5956\u52b1\u673a\u5236\u548c\u52a8\u6001\u63a8\u7406-\u9605\u8bfb\u4ea4\u66ff\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u7b97\u6cd5\u517c\u5bb9\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2602.05570", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05570", "abs": "https://arxiv.org/abs/2602.05570", "authors": ["Yikun Zong", "Cheston Tan"], "title": "TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?", "comment": "13 pages, 4 figures", "summary": "Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u6d4b\u8bd5\u65f6\u81ea\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5956\u52b1\u53cd\u9988\u5faa\u73af\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8fde\u7eed\u51e0\u4f55\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5728Tangram\u62fc\u56fe\u4efb\u52a1\u4e0a\u5b9e\u73b0\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8fde\u7eed\u51e0\u4f55\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u5982Tangram\u62fc\u56fe\uff09\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002\u53d7\u4eba\u7c7b\u901a\u8fc7\u8bd5\u9519\u3001\u89c2\u5bdf\u548c\u4fee\u6b63\u89e3\u51b3\u62fc\u56fe\u7684\u8ba4\u77e5\u8fc7\u7a0b\u542f\u53d1\uff0c\u7814\u7a76\u63a2\u7d22\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u6d4b\u8bd5\u65f6\u81ea\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u514d\u8d39\u7684\u9a8c\u8bc1\u5668-\u4f18\u5316\u5668\u4ee3\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5956\u52b1\u5f15\u5bfc\u7684\u53cd\u9988\u5faa\u73af\u3002\u901a\u8fc7\u9012\u5f52\u4f18\u5316\u5faa\u73af\uff0c\u57fa\u4e8e\u51e0\u4f55\u4e00\u81f4\u6027\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u9884\u6d4b\uff0c\u6a21\u4eff\u4eba\u7c7b\u7684\u8ba4\u77e5\u673a\u5236\u3002", "result": "\u5728\u4e94\u4e2a\u4ee3\u8868\u6027\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5355\u5757\u4efb\u52a1\u5e73\u5747IoU\u4ec50.41\uff0c\u53cc\u5757\u7ec4\u5408\u964d\u81f30.23\u3002\u4f46\u63d0\u51fa\u7684\u81ea\u4f18\u5316\u6846\u67b6\u5c06\u4e2d\u7b49\u4e09\u89d2\u5f62\u6848\u4f8b\u7684IoU\u4ece0.63\u63d0\u5347\u81f30.932\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5956\u52b1\u5faa\u73af\u878d\u5165\u4eba\u7c7b\u542f\u53d1\u7684\u8fed\u4ee3\u4f18\u5316\u673a\u5236\uff0c\u80fd\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u51e0\u4f55\u63a8\u7406\u80fd\u529b\uff0c\u63a8\u52a8\u81ea\u4f18\u5316AI\u5728\u8fde\u7eed\u7a7a\u95f4\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "2602.05060", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05060", "abs": "https://arxiv.org/abs/2602.05060", "authors": ["Heajun An", "Qi Zhang", "Minqian Liu", "Xinyi Zhang", "Sang Won Lee", "Lifu Huang", "Pamela J. Wisniewski", "Jin-Hee Cho"], "title": "StagePilot: A Deep Reinforcement Learning Agent for Stage-Controlled Cybergrooming Simulation", "comment": null, "summary": "Cybergrooming is an evolving threat to youth, necessitating proactive educational interventions. We propose StagePilot, an offline RL-based dialogue agent that simulates the stage-wise progression of grooming behaviors for prevention training. StagePilot selects conversational stages using a composite reward that balances user sentiment and goal proximity, with transitions constrained to adjacent stages for realism and interpretability. We evaluate StagePilot through LLM-based simulations, measuring stage completion, dialogue efficiency, and emotional engagement. Results show that StagePilot generates realistic and coherent conversations aligned with grooming dynamics. Among tested methods, the IQL+AWAC agent achieves the best balance between strategic planning and emotional coherence, reaching the final stage up to 43% more frequently than baselines while maintaining over 70% sentiment alignment.", "AI": {"tldr": "StagePilot\u662f\u4e00\u4e2a\u57fa\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u8bdd\u4ee3\u7406\uff0c\u901a\u8fc7\u6a21\u62df\u7f51\u7edc\u8bf1\u9a97\u884c\u4e3a\u7684\u9636\u6bb5\u6027\u8fdb\u5c55\u6765\u8fdb\u884c\u9884\u9632\u8bad\u7ec3\uff0c\u4f7f\u7528\u590d\u5408\u5956\u52b1\u5e73\u8861\u7528\u6237\u60c5\u611f\u548c\u76ee\u6807\u63a5\u8fd1\u5ea6\uff0c\u5728LLM\u6a21\u62df\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7f51\u7edc\u8bf1\u9a97\u5bf9\u9752\u5c11\u5e74\u6784\u6210\u6301\u7eed\u5a01\u80c1\uff0c\u9700\u8981\u4e3b\u52a8\u7684\u6559\u80b2\u5e72\u9884\u63aa\u65bd\u6765\u9884\u9632\u8fd9\u79cd\u5371\u5bb3\u3002", "method": "\u63d0\u51faStagePilot\u79bb\u7ebfRL\u5bf9\u8bdd\u4ee3\u7406\uff0c\u4f7f\u7528\u590d\u5408\u5956\u52b1\u51fd\u6570\u5e73\u8861\u7528\u6237\u60c5\u611f\u548c\u76ee\u6807\u63a5\u8fd1\u5ea6\uff0c\u7ea6\u675f\u9636\u6bb5\u8f6c\u6362\u5230\u76f8\u90bb\u9636\u6bb5\u4ee5\u4fdd\u8bc1\u771f\u5b9e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728LLM\u6a21\u62df\u8bc4\u4f30\u4e2d\uff0cStagePilot\u751f\u6210\u771f\u5b9e\u8fde\u8d2f\u7684\u5bf9\u8bdd\uff0cIQL+AWAC\u4ee3\u7406\u5728\u6218\u7565\u89c4\u5212\u548c\u60c5\u611f\u8fde\u8d2f\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u5230\u6700\u7ec8\u9636\u6bb5\u7684\u9891\u7387\u6bd4\u57fa\u7ebf\u9ad843%\uff0c\u540c\u65f6\u4fdd\u6301\u8d85\u8fc770%\u7684\u60c5\u611f\u5bf9\u9f50\u3002", "conclusion": "StagePilot\u80fd\u591f\u6709\u6548\u6a21\u62df\u7f51\u7edc\u8bf1\u9a97\u7684\u9636\u6bb5\u6027\u884c\u4e3a\uff0c\u4e3a\u9884\u9632\u8bad\u7ec3\u63d0\u4f9b\u5b9e\u7528\u7684\u5bf9\u8bdd\u4ee3\u7406\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2602.05842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05842", "abs": "https://arxiv.org/abs/2602.05842", "authors": ["Xiao Yu", "Baolin Peng", "Ruize Xu", "Yelong Shen", "Pengcheng He", "Suman Nath", "Nikhil Singh", "Jiangfeng Gao", "Zhou Yu"], "title": "Reinforcement World Model Learning for LLM-based Agents", "comment": null, "summary": "Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and $\u03c4^2$ Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and $\u03c4^2$ Bench respectively, while matching the performance of expert-data training.", "AI": {"tldr": "\u63d0\u51faRWML\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u4e3aLLM\u667a\u80fd\u4f53\u6784\u5efa\u52a8\u4f5c\u6761\u4ef6\u4e16\u754c\u6a21\u578b\uff0c\u4f7f\u7528\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u5956\u52b1\u6765\u5bf9\u9f50\u5185\u90e8\u6a21\u62df\u4e0e\u73af\u5883\u52a8\u6001\uff0c\u5728ALFWorld\u548c\u03c4\u00b2 Bench\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "LLM\u5728\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u96be\u4ee5\u9884\u6d4b\u52a8\u4f5c\u540e\u679c\u548c\u9002\u5e94\u73af\u5883\u52a8\u6001\uff0c\u9700\u8981\u589e\u5f3aLLM\u667a\u80fd\u4f53\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51faRWML\uff08\u5f3a\u5316\u4e16\u754c\u6a21\u578b\u5b66\u4e60\uff09\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u6587\u672c\u72b6\u6001\u4e0a\u5b66\u4e60\u52a8\u4f5c\u6761\u4ef6\u4e16\u754c\u6a21\u578b\uff0c\u4f7f\u7528\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u5956\u52b1\uff0c\u5728\u9884\u8bad\u7ec3\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u6a21\u578b\u751f\u6210\u7684\u6a21\u62df\u4e0b\u4e00\u72b6\u6001\u4e0e\u73af\u5883\u89c2\u5bdf\u5230\u7684\u5b9e\u9645\u4e0b\u4e00\u72b6\u6001\u3002", "result": "\u5728ALFWorld\u548c\u03c4\u00b2 Bench\u4e0a\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347\u3002\u7ed3\u5408\u4efb\u52a1\u6210\u529f\u5956\u52b1\u65f6\uff0c\u5728ALFWorld\u548c\u03c4\u00b2 Bench\u4e0a\u5206\u522b\u6bd4\u76f4\u63a5\u4efb\u52a1\u6210\u529f\u5956\u52b1RL\u9ad8\u51fa6.9\u548c5.7\u5206\uff0c\u4e14\u4e0e\u4e13\u5bb6\u6570\u636e\u8bad\u7ec3\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "RWML\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u76d1\u7763\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u4e0b\u4e00\u72b6\u6001token\u9884\u6d4b\u548cLLM-as-a-judge\u65b9\u6cd5\u66f4\u9c81\u68d2\uff0c\u80fd\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u5728\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.05843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05843", "abs": "https://arxiv.org/abs/2602.05843", "authors": ["Fangzhi Xu", "Hang Yan", "Qiushi Sun", "Jinyang Wu", "Zixian Huang", "Muye Huang", "Jingyang Gong", "Zichen Ding", "Kanzhi Cheng", "Yian Wang", "Xinyu Che", "Zeyi Sun", "Jian Zhang", "Zhangyue Yin", "Haoran Luo", "Xuanjing Huang", "Ben Kao", "Jun Liu", "Qika Lin"], "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions", "comment": "34 pages", "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena", "AI": {"tldr": "OdysseyArena\u662f\u4e00\u4e2a\u65b0\u7684\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u957f\u65f6\u7a0b\u3001\u4e3b\u52a8\u548c\u5f52\u7eb3\u5f0f\u4ea4\u4e92\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u89c6\u667a\u80fd\u4f53\u4ece\u7ecf\u9a8c\u4e2d\u81ea\u4e3b\u53d1\u73b0\u6f5c\u5728\u8f6c\u79fb\u89c4\u5f8b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u8bc4\u4f30\u4e3b\u8981\u91c7\u7528\u6f14\u7ece\u8303\u5f0f\uff0c\u57fa\u4e8e\u660e\u786e\u89c4\u5219\u548c\u9759\u6001\u76ee\u6807\u6267\u884c\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u667a\u80fd\u4f53\u4ece\u7ecf\u9a8c\u4e2d\u81ea\u4e3b\u53d1\u73b0\u6f5c\u5728\u8f6c\u79fb\u89c4\u5f8b\u7684\u5f52\u7eb3\u80fd\u529b\uff0c\u8fd9\u662f\u5b9e\u73b0\u667a\u80fd\u4f53\u524d\u77bb\u6027\u548c\u6218\u7565\u8fde\u8d2f\u6027\u7684\u5173\u952e\u3002", "method": "\u63d0\u51fa\u4e86OdysseyArena\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u5e76\u5b9e\u4f8b\u5316\u4e86\u56db\u4e2a\u539f\u8bed\uff0c\u5c06\u62bd\u8c61\u8f6c\u79fb\u52a8\u6001\u8f6c\u5316\u4e3a\u5177\u4f53\u4ea4\u4e92\u73af\u5883\u3002\u5efa\u7acb\u4e86OdysseyArena-Lite\u7528\u4e8e\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b120\u4e2a\u4efb\u52a1\uff09\uff0c\u4ee5\u53caOdysseyArena-Challenge\u7528\u4e8e\u538b\u529b\u6d4b\u8bd5\u6781\u7aef\u4ea4\u4e92\u65f6\u7a0b\uff08>200\u6b65\uff09\u3002", "result": "\u5bf915+\u4e2a\u9886\u5148LLM\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u524d\u6cbf\u6a21\u578b\u5728\u5f52\u7eb3\u573a\u666f\u4e2d\u4e5f\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u63ed\u793a\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u53d1\u73b0\u7684\u5173\u952e\u74f6\u9888\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cd\u65b0\u805a\u7126\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u65b0\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u5f52\u7eb3\u63a8\u7406\u548c\u957f\u65f6\u7a0b\u4ea4\u4e92\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u81ea\u4e3b\u53d1\u73b0\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.05665", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05665", "abs": "https://arxiv.org/abs/2602.05665", "authors": ["Chang Yang", "Chuang Zhou", "Yilin Xiao", "Su Dong", "Luyao Zhuang", "Yujing Zhang", "Zhu Wang", "Zijin Hong", "Zheng Yuan", "Zhishang Xiang", "Shengyuan Chen", "Huachi Zhou", "Qinggang Zhang", "Ninghao Liu", "Jinsong Su", "Xinrun Wang", "Yi Chang", "Xiao Huang"], "title": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications", "comment": null, "summary": "Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u56fe\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u63a2\u8ba8\u4e86\u5176\u5728LLM\u667a\u80fd\u4f53\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u5305\u62ec\u8bb0\u5fc6\u5206\u7c7b\u3001\u751f\u547d\u5468\u671f\u5173\u952e\u6280\u672f\u3001\u5f00\u6e90\u5de5\u5177\u548c\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u8bb0\u5fc6\u662fLLM\u667a\u80fd\u4f53\u5904\u7406\u957f\u671f\u590d\u6742\u4efb\u52a1\uff08\u5982\u591a\u8f6e\u5bf9\u8bdd\u3001\u6e38\u620f\u3001\u79d1\u5b66\u53d1\u73b0\uff09\u7684\u6838\u5fc3\u6a21\u5757\uff0c\u80fd\u591f\u5b9e\u73b0\u77e5\u8bc6\u79ef\u7d2f\u3001\u8fed\u4ee3\u63a8\u7406\u548c\u81ea\u6211\u8fdb\u5316\u3002\u56fe\u7ed3\u6784\u56e0\u5176\u5efa\u6a21\u5173\u7cfb\u4f9d\u8d56\u3001\u7ec4\u7ec7\u5c42\u6b21\u4fe1\u606f\u548c\u9ad8\u6548\u68c0\u7d22\u7684\u5185\u5728\u80fd\u529b\uff0c\u6210\u4e3a\u667a\u80fd\u4f53\u8bb0\u5fc6\u7684\u5f3a\u5927\u7ed3\u6784\u3002", "method": "1. \u63d0\u51fa\u667a\u80fd\u4f53\u8bb0\u5fc6\u7684\u5206\u7c7b\u6cd5\uff1a\u77ed\u671fvs\u957f\u671f\u8bb0\u5fc6\u3001\u77e5\u8bc6vs\u7ecf\u9a8c\u8bb0\u5fc6\u3001\u975e\u7ed3\u6784\u5316vs\u7ed3\u6784\u5316\u8bb0\u5fc6\uff0c\u4ee5\u53ca\u57fa\u4e8e\u56fe\u7684\u8bb0\u5fc6\u5b9e\u73b0\u89c6\u89d2\u30022. \u6309\u7167\u8bb0\u5fc6\u751f\u547d\u5468\u671f\u7cfb\u7edf\u5206\u6790\u57fa\u4e8e\u56fe\u8bb0\u5fc6\u7684\u5173\u952e\u6280\u672f\uff1a\u8bb0\u5fc6\u63d0\u53d6\uff08\u6570\u636e\u8f6c\u6362\uff09\u3001\u5b58\u50a8\uff08\u9ad8\u6548\u7ec4\u7ec7\uff09\u3001\u68c0\u7d22\uff08\u652f\u6301\u63a8\u7406\u7684\u76f8\u5173\u5185\u5bb9\u68c0\u7d22\uff09\u548c\u8fdb\u5316\uff08\u5185\u5bb9\u66f4\u65b0\uff09\u30023. \u603b\u7ed3\u652f\u6301\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\u8bb0\u5fc6\u5f00\u53d1\u548c\u8bc4\u4f30\u7684\u5f00\u6e90\u5e93\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u672c\u6587\u5168\u9762\u7efc\u8ff0\u4e86\u57fa\u4e8e\u56fe\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e86\u8bb0\u5fc6\u5206\u7c7b\u6846\u67b6\u3001\u751f\u547d\u5468\u671f\u6280\u672f\u5206\u6790\u3001\u5f00\u6e90\u8d44\u6e90\u6c47\u603b\u548c\u5e94\u7528\u573a\u666f\u63a2\u7d22\uff0c\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u56fe\u57fa\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u662fLLM\u667a\u80fd\u4f53\u5904\u7406\u590d\u6742\u4efb\u52a1\u7684\u5173\u952e\u6a21\u5757\u3002\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u3001\u6280\u672f\u5206\u6790\u548c\u8d44\u6e90\u6c47\u603b\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u63a8\u52a8\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u56fe\u57fa\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2602.05087", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.IR", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.05087", "abs": "https://arxiv.org/abs/2602.05087", "authors": ["Parsa Vares"], "title": "Autodiscover: A reinforcement learning recommendation system for the cold-start imbalance challenge in active learning, powered by graph-aware thompson sampling", "comment": "Master's Thesis, University of Luxembourg in collaboration with Luxembourg Institute of Science and Technology (LIST). Supervised by Prof. Jun Pang and Dr. Eloi Durant", "summary": "Systematic literature reviews (SLRs) are fundamental to evidence-based research, but manual screening is an increasing bottleneck as scientific output grows. Screening features low prevalence of relevant studies and scarce, costly expert decisions. Traditional active learning (AL) systems help, yet typically rely on fixed query strategies for selecting the next unlabeled documents. These static strategies do not adapt over time and ignore the relational structure of scientific literature networks. This thesis introduces AutoDiscover, a framework that reframes AL as an online decision-making problem driven by an adaptive agent. Literature is modeled as a heterogeneous graph capturing relationships among documents, authors, and metadata. A Heterogeneous Graph Attention Network (HAN) learns node representations, which a Discounted Thompson Sampling (DTS) agent uses to dynamically manage a portfolio of query strategies. With real-time human-in-the-loop labels, the agent balances exploration and exploitation under non-stationary review dynamics, where strategy utility changes over time. On the 26-dataset SYNERGY benchmark, AutoDiscover achieves higher screening efficiency than static AL baselines. Crucially, the agent mitigates cold start by bootstrapping discovery from minimal initial labels where static approaches fail. We also introduce TS-Insight, an open-source visual analytics dashboard to interpret, verify, and diagnose the agent's decisions. Together, these contributions accelerate SLR screening under scarce expert labels and low prevalence of relevant studies.", "AI": {"tldr": "AutoDiscover\u5c06\u4e3b\u52a8\u5b66\u4e60\u91cd\u6784\u4e3a\u5728\u7ebf\u51b3\u7b56\u95ee\u9898\uff0c\u4f7f\u7528\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u52a8\u6001\u7b56\u7565\u7ec4\u5408\u7ba1\u7406\uff0c\u5728\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7b5b\u9009\u4efb\u52a1\u4e2d\u6bd4\u9759\u6001\u65b9\u6cd5\u66f4\u9ad8\u6548", "motivation": "\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7684\u624b\u52a8\u7b5b\u9009\u9762\u4e34\u4f4e\u76f8\u5173\u7814\u7a76\u6bd4\u4f8b\u548c\u4e13\u5bb6\u6807\u6ce8\u7a00\u7f3a\u6602\u8d35\u7684\u6311\u6218\uff0c\u4f20\u7edf\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u67e5\u8be2\u7b56\u7565\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u4e14\u5ffd\u7565\u79d1\u5b66\u6587\u732e\u7f51\u7edc\u7ed3\u6784", "method": "\u63d0\u51faAutoDiscover\u6846\u67b6\uff1a1) \u5c06\u6587\u732e\u5efa\u6a21\u4e3a\u5305\u542b\u6587\u6863\u3001\u4f5c\u8005\u548c\u5143\u6570\u636e\u7684\u5f02\u6784\u56fe\uff1b2) \u4f7f\u7528\u5f02\u6784\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u5b66\u4e60\u8282\u70b9\u8868\u793a\uff1b3) \u91c7\u7528\u6298\u6263\u6c64\u666e\u68ee\u91c7\u6837\u667a\u80fd\u4f53\u52a8\u6001\u7ba1\u7406\u67e5\u8be2\u7b56\u7565\u7ec4\u5408\uff1b4) \u7ed3\u5408\u5b9e\u65f6\u4eba\u5de5\u6807\u6ce8\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528", "result": "\u572826\u4e2a\u6570\u636e\u96c6\u7684SYNERGY\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoDiscover\u6bd4\u9759\u6001\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u7b5b\u9009\u6548\u7387\u66f4\u9ad8\uff0c\u7279\u522b\u662f\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e0b\uff0c\u80fd\u4ece\u6700\u5c0f\u521d\u59cb\u6807\u6ce8\u4e2d\u6709\u6548\u53d1\u73b0\u76f8\u5173\u6587\u732e", "conclusion": "AutoDiscover\u901a\u8fc7\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u65b9\u6cd5\u663e\u8457\u52a0\u901f\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7b5b\u9009\u8fc7\u7a0b\uff0c\u540c\u65f6\u5f00\u53d1\u4e86TS-Insight\u53ef\u89c6\u5316\u5206\u6790\u5de5\u5177\u5e2e\u52a9\u89e3\u91ca\u548c\u8bca\u65ad\u667a\u80fd\u4f53\u51b3\u7b56", "topic": "agent analysis"}}
{"id": "2602.05897", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05897", "abs": "https://arxiv.org/abs/2602.05897", "authors": ["Shuo Nie", "Hexuan Deng", "Chao Wang", "Ruiyu Fang", "Xuebo Liu", "Shuangyong Song", "Yu Li", "Min Zhang", "Xuelong Li"], "title": "Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models", "comment": null, "summary": "As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.", "AI": {"tldr": "FaithRL\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b65\u9aa4\u7ea7\u76d1\u7763\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u663e\u5f0f\u5fe0\u5b9e\u5ea6\u5956\u52b1\uff0c\u5e76\u7ed3\u5408\u622a\u65ad\u91cd\u91c7\u6837\u7b56\u7565\uff0c\u6709\u6548\u51cf\u5c11\u5c0f\u63a8\u7406\u6a21\u578b\u5728\u601d\u7ef4\u94fe\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5c0f\u63a8\u7406\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5bf9\u601d\u7ef4\u94fe\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bb9\u6613\u4ea7\u751f\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u5fe0\u5b9e\u6027\u5e7b\u89c9\u3002\u73b0\u6709\u57fa\u4e8e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u7f13\u89e3\u65b9\u6cd5\u4f9d\u8d56\u7ed3\u679c\u5956\u52b1\u6216\u7c97\u7c92\u5ea6\u8bc4\u4f30\uff0c\u53ef\u80fd\u5728\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u65f6\u65e0\u610f\u4e2d\u5f3a\u5316\u4e0d\u5fe0\u5b9e\u7684\u63a8\u7406\u3002", "method": "\u63d0\u51faFaithfulness-Aware Step-Level Reinforcement Learning (FaithRL)\uff0c\u5305\u542b\uff1a1) \u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u6b65\u9aa4\u7ea7\u663e\u5f0f\u5fe0\u5b9e\u5ea6\u5956\u52b1\uff1b2) \u9690\u5f0f\u622a\u65ad\u91cd\u91c7\u6837\u7b56\u7565\uff0c\u4ece\u5fe0\u5b9e\u524d\u7f00\u751f\u6210\u5bf9\u6bd4\u4fe1\u53f7\u3002", "result": "\u5728\u591a\u4e2a\u5c0f\u63a8\u7406\u6a21\u578b\u548c\u5f00\u653e\u4e66\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFaithRL\u6301\u7eed\u51cf\u5c11\u4e86\u601d\u7ef4\u94fe\u548c\u6700\u7ec8\u7b54\u6848\u4e2d\u7684\u5e7b\u89c9\uff0c\u5b9e\u73b0\u4e86\u66f4\u5fe0\u5b9e\u53ef\u9760\u7684\u63a8\u7406\u3002", "conclusion": "FaithRL\u901a\u8fc7\u6b65\u9aa4\u7ea7\u76d1\u7763\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u5fe0\u5b9e\u6027\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u53ef\u9760\u63a8\u7406\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.05125", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05125", "abs": "https://arxiv.org/abs/2602.05125", "authors": ["William F. Shen", "Xinchi Qiu", "Chenxi Whitehouse", "Lisa Alazraki", "Shashwat Goel", "Francesco Barbieri", "Timon Willi", "Akhil Mathur", "Ilias Leontiadis"], "title": "Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks", "comment": null, "summary": "Recently, rubrics have been used to guide LLM judges in capturing subjective, nuanced, multi-dimensional human preferences, and have been extended from evaluation to reward signals for reinforcement fine-tuning (RFT). However, rubric generation remains hard to control: rubrics often lack coverage, conflate dimensions, misalign preference direction, and contain redundant or highly correlated criteria, degrading judge accuracy and producing suboptimal rewards during RFT. We propose RRD, a principled framework for rubric refinement built on a recursive decompose-filter cycle. RRD decomposes coarse rubrics into fine-grained, discriminative criteria, expanding coverage while sharpening separation between responses. A complementary filtering mechanism removes misaligned and redundant rubrics, and a correlation-aware weighting scheme prevents over-representing highly correlated criteria, yielding rubric sets that are informative, comprehensive, and non-redundant. Empirically, RRD delivers large, consistent gains across both evaluation and training: it improves preference-judgment accuracy on JudgeBench and PPE for both GPT-4o and Llama3.1-405B judges, achieving top performance in all settings with up to +17.7 points on JudgeBench. When used as the reward source for RFT on WildChat, it yields substantially stronger and more stable learning signals, boosting reward by up to 160% (Qwen3-4B) and 60% (Llama3.1-8B) versus 10-20% for prior rubric baselines, with gains that transfer to HealthBench-Hard and BiGGen Bench. Overall, RRD establishes recursive rubric refinement as a scalable and interpretable foundation for LLM judging and reward modeling in open-ended domains.", "AI": {"tldr": "RRD\u6846\u67b6\u901a\u8fc7\u9012\u5f52\u5206\u89e3-\u8fc7\u6ee4\u5faa\u73af\u4f18\u5316\u8bc4\u5206\u6807\u51c6\uff0c\u63d0\u5347LLM\u8bc4\u5224\u51c6\u786e\u6027\u548c\u5f3a\u5316\u5fae\u8c03\u6548\u679c", "motivation": "\u73b0\u6709\u8bc4\u5206\u6807\u51c6\u5b58\u5728\u8986\u76d6\u4e0d\u8db3\u3001\u7ef4\u5ea6\u6df7\u6dc6\u3001\u504f\u597d\u65b9\u5411\u9519\u4f4d\u3001\u5197\u4f59\u76f8\u5173\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8bc4\u5224\u51c6\u786e\u6027\u4e0b\u964d\u548c\u5f3a\u5316\u5fae\u8c03\u6548\u679c\u4e0d\u4f73", "method": "\u63d0\u51faRRD\u6846\u67b6\uff0c\u91c7\u7528\u9012\u5f52\u5206\u89e3-\u8fc7\u6ee4\u5faa\u73af\uff1a\u5206\u89e3\u7c97\u7c92\u5ea6\u6807\u51c6\u4e3a\u7ec6\u7c92\u5ea6\u5224\u522b\u6027\u51c6\u5219\uff0c\u8fc7\u6ee4\u9519\u4f4d\u548c\u5197\u4f59\u6807\u51c6\uff0c\u4f7f\u7528\u76f8\u5173\u6027\u611f\u77e5\u52a0\u6743\u65b9\u6848", "result": "\u5728JudgeBench\u548cPPE\u4e0a\u663e\u8457\u63d0\u5347\u8bc4\u5224\u51c6\u786e\u6027\uff08\u6700\u9ad8+17.7\u5206\uff09\uff0c\u5728WildChat\u4e0a\u5f3a\u5316\u5fae\u8c03\u6548\u679c\u5927\u5e45\u63d0\u5347\uff08Qwen3-4B\u5956\u52b1\u63d0\u5347160%\uff0cLlama3.1-8B\u63d0\u534760%\uff09", "conclusion": "RRD\u4e3a\u5f00\u653e\u9886\u57dfLLM\u8bc4\u5224\u548c\u5956\u52b1\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u57fa\u7840\u6846\u67b6", "topic": "agent analysis"}}
{"id": "2602.05134", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.05134", "abs": "https://arxiv.org/abs/2602.05134", "authors": ["Olga Ovcharenko", "Matthias Boehm", "Sebastian Schelter"], "title": "SemPipes -- Optimizable Semantic Data Operators for Tabular Machine Learning Pipelines", "comment": null, "summary": "Real-world machine learning on tabular data relies on complex data preparation pipelines for prediction, data integration, augmentation, and debugging. Designing these pipelines requires substantial domain expertise and engineering effort, motivating the question of how large language models (LLMs) can support tabular ML through code synthesis. We introduce SemPipes, a novel declarative programming model that integrates LLM-powered semantic data operators into tabular ML pipelines. Semantic operators specify data transformations in natural language while delegating execution to a runtime system. During training, SemPipes synthesizes custom operator implementations based on data characteristics, operator instructions, and pipeline context. This design enables the automatic optimization of data operations in a pipeline via LLM-based code synthesis guided by evolutionary search. We evaluate SemPipes across diverse tabular ML tasks and show that semantic operators substantially improve end-to-end predictive performance for both expert-designed and agent-generated pipelines, while reducing pipeline complexity. We implement SemPipes in Python and release it at https://github.com/deem-data/sempipes/tree/v1.", "AI": {"tldr": "SemPipes\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u58f0\u660e\u5f0f\u7f16\u7a0b\u6a21\u578b\uff0c\u5c06LLM\u9a71\u52a8\u7684\u8bed\u4e49\u6570\u636e\u64cd\u4f5c\u7b26\u96c6\u6210\u5230\u8868\u683cML\u7ba1\u9053\u4e2d\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u5b9a\u6570\u636e\u8f6c\u6362\uff0c\u5e76\u57fa\u4e8e\u6570\u636e\u7279\u5f81\u548c\u4e0a\u4e0b\u6587\u5408\u6210\u81ea\u5b9a\u4e49\u5b9e\u73b0\uff0c\u81ea\u52a8\u4f18\u5316\u7ba1\u9053\u64cd\u4f5c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8868\u683c\u673a\u5668\u5b66\u4e60\u9700\u8981\u590d\u6742\u7684\u6570\u636e\u51c6\u5907\u7ba1\u9053\uff0c\u8fd9\u9700\u8981\u5927\u91cf\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u548c\u5de5\u7a0b\u52aa\u529b\u3002\u7814\u7a76\u5982\u4f55\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u4ee3\u7801\u5408\u6210\u6765\u652f\u6301\u8868\u683cML\uff0c\u51cf\u5c11\u4eba\u5de5\u8bbe\u8ba1\u548c\u4f18\u5316\u7684\u5de5\u4f5c\u91cf\u3002", "method": "\u5f15\u5165SemPipes\u58f0\u660e\u5f0f\u7f16\u7a0b\u6a21\u578b\uff0c\u96c6\u6210LLM\u9a71\u52a8\u7684\u8bed\u4e49\u6570\u636e\u64cd\u4f5c\u7b26\u3002\u8bed\u4e49\u64cd\u4f5c\u7b26\u7528\u81ea\u7136\u8bed\u8a00\u6307\u5b9a\u6570\u636e\u8f6c\u6362\uff0c\u8fd0\u884c\u65f6\u7cfb\u7edf\u6267\u884c\u3002\u8bad\u7ec3\u65f6\u57fa\u4e8e\u6570\u636e\u7279\u5f81\u3001\u64cd\u4f5c\u6307\u4ee4\u548c\u7ba1\u9053\u4e0a\u4e0b\u6587\u5408\u6210\u81ea\u5b9a\u4e49\u5b9e\u73b0\uff0c\u901a\u8fc7\u8fdb\u5316\u641c\u7d22\u5f15\u5bfc\u7684LLM\u4ee3\u7801\u5408\u6210\u81ea\u52a8\u4f18\u5316\u6570\u636e\u64cd\u4f5c\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u8868\u683cML\u4efb\u52a1\u4e2d\u8bc4\u4f30SemPipes\uff0c\u7ed3\u679c\u663e\u793a\u8bed\u4e49\u64cd\u4f5c\u7b26\u663e\u8457\u63d0\u5347\u4e86\u4e13\u5bb6\u8bbe\u8ba1\u548c\u4ee3\u7406\u751f\u6210\u7ba1\u9053\u7684\u7aef\u5230\u7aef\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u7ba1\u9053\u590d\u6742\u5ea6\u3002", "conclusion": "SemPipes\u901a\u8fc7LLM\u9a71\u52a8\u7684\u8bed\u4e49\u64cd\u4f5c\u7b26\u548c\u81ea\u52a8\u4ee3\u7801\u5408\u6210\uff0c\u6709\u6548\u652f\u6301\u8868\u683cML\u7ba1\u9053\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u5e76\u51cf\u5c11\u590d\u6742\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u8868\u683c\u6570\u636e\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2602.05932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05932", "abs": "https://arxiv.org/abs/2602.05932", "authors": ["L\u00e9o Labat", "Etienne Ollion", "Fran\u00e7ois Yvon"], "title": "Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions", "comment": "17 pages, 5 figures (8 pages of references and appendices)", "summary": "Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.", "AI": {"tldr": "\u7814\u7a76\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ef7\u503c\u5bfc\u5411\u591a\u9009\u9898\u4e2d\u7684\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u5927\u578b\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u6574\u4f53\u4e00\u81f4\u6027\u8f83\u9ad8\uff0c\u4f46\u7279\u5b9a\u95ee\u9898\u4ecd\u5b58\u5728\u8bed\u8a00\u4f9d\u8d56\u884c\u4e3a", "motivation": "\u867d\u7136\u591a\u8bed\u8a00\u6027\u5bf9LLM\u4e8b\u5b9e\u56de\u5fc6\u7684\u5f71\u54cd\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u4ef7\u503c\u5bfc\u5411\u591a\u9009\u9898\u4e2d\u7684\u8bed\u8a00\u8bf1\u5bfc\u53d8\u5f02\u95ee\u9898\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u591a\u8bed\u8a00LLM\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u662f\u5426\u4fdd\u6301\u4e00\u81f4\u7684\u4ef7\u503c\u89c2\u8868\u8fbe\uff0c\u8fd8\u662f\u50cf\u591a\u4e2a\u5355\u8bed\u6a21\u578b\u4e00\u6837\u8868\u73b0\u51fa\u8bed\u8a00\u4f9d\u8d56\u7684\u4ef7\u503c\u89c2\u5dee\u5f02\u3002", "method": "\u53d1\u5e03\u65b0\u7684\u591a\u8bed\u8a00\u6b27\u6d32\u4ef7\u503c\u89c2\u8c03\u67e5\uff08MEVS\uff09\u8bed\u6599\u5e93\uff0c\u5305\u542b8\u79cd\u6b27\u6d32\u8bed\u8a00\u7684\u4eba\u7c7b\u7ffb\u8bd1\u8c03\u67e5\u95ee\u9898\u3002\u5bf930\u591a\u4e2a\u591a\u8bed\u8a00LLM\u8fdb\u884c\u6d4b\u8bd5\uff0c\u63a7\u5236\u63d0\u793a\u53d8\u91cf\uff08\u7b54\u6848\u987a\u5e8f\u3001\u7b26\u53f7\u7c7b\u578b\u3001\u5c3e\u90e8\u5b57\u7b26\u7b49\uff09\uff0c\u5206\u6790\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u3002", "result": "\u5927\u578b\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u6574\u4f53\u4e00\u81f4\u6027\u8f83\u9ad8\uff0c\u4f46\u54cd\u5e94\u7a33\u5065\u6027\u5728\u4e0d\u540c\u95ee\u9898\u95f4\u5dee\u5f02\u5f88\u5927\u3002\u67d0\u4e9b\u95ee\u9898\u5728\u6240\u6709\u6a21\u578b\u548c\u8bed\u8a00\u4e2d\u8fbe\u6210\u5b8c\u5168\u4e00\u81f4\uff0c\u800c\u5176\u4ed6\u95ee\u9898\u5219\u5bfc\u81f4LLM\u7b54\u6848\u5206\u88c2\u3002\u6240\u6709\u4e00\u81f4\u7684\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u5728\u7279\u5b9a\u95ee\u9898\u4e0a\u90fd\u8868\u73b0\u51fa\u8bed\u8a00\u7279\u5b9a\u884c\u4e3a\u3002", "conclusion": "\u591a\u8bed\u8a00LLM\u5728\u4ef7\u503c\u5bfc\u5411\u591a\u9009\u9898\u4e2d\u5e76\u975e\u5b8c\u5168\u4e00\u81f4\u7684\u7406\u8bba\u591a\u8bed\u8005\uff0c\u800c\u662f\u5728\u7279\u5b9a\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8bed\u8a00\u4f9d\u8d56\u884c\u4e3a\u3002\u8fd9\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u504f\u597d\u5fae\u8c03\u7684\u9009\u62e9\u6027\u6548\u5e94\u3002", "topic": "agent analysis"}}
{"id": "2602.05765", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05765", "abs": "https://arxiv.org/abs/2602.05765", "authors": ["Zhong Guan", "Haoran Sun", "Yongjian Guo", "Shuai Di", "Xiaodong Bai", "Jing Long", "Tianyun Zhao", "Mingxi Luo", "Chen Zhou", "Yucheng Guo", "Qiming Yang", "Wanting Xu", "Wen Huang", "Yunxuan Ma", "Hongke Zhao", "Likang Wu", "Xiaotie Deng", "Xi Xiao", "Sheng Wen", "Yicheng Gong", "Junwu Xiong"], "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism", "comment": null, "summary": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method's excellent scalability under most conditions.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5b8c\u5168\u5f02\u6b65\u7684VLA\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u3001\u7b56\u7565\u751f\u6210\u548c\u6a21\u578b\u66f4\u65b0\u7684\u5f02\u6b65\u5e76\u884c\u5316\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u5728LIBERO\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u9ad8126.67%\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u6210\u4e3a\u74f6\u9888\uff0c\u867d\u7136RL-based\u6846\u67b6\u5982RLinf\u80fd\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4ecd\u4f9d\u8d56\u540c\u6b65\u6267\u884c\uff0c\u5bfc\u81f4\u73af\u5883\u4ea4\u4e92\u3001\u7b56\u7565\u751f\u6210\u548c\u6a21\u578b\u66f4\u65b0\u9636\u6bb5\u7684\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\u548c\u541e\u5410\u91cf\u9650\u5236\u3002", "method": "\u63d0\u51fa\u5b8c\u5168\u5f02\u6b65\u7b56\u7565\u8bad\u7ec3\u6846\u67b6\uff0c\u91c7\u7528\u591a\u7ea7\u89e3\u8026\u67b6\u6784\uff1a1) \u73af\u5883\u4ea4\u4e92\u548c\u8f68\u8ff9\u6536\u96c6\u7684\u5f02\u6b65\u5e76\u884c\u5316\uff1b2) \u7b56\u7565\u751f\u6210\u7684\u6d41\u5f0f\u6267\u884c\uff1b3) \u8bad\u7ec3\u66f4\u65b0\u7684\u89e3\u8026\u8c03\u5ea6\u3002\u4ece\u5927\u6a21\u578bRL\u7684\u5f02\u6b65\u4f18\u5316\u601d\u60f3\u4e2d\u7cfb\u7edf\u6c72\u53d6\u7075\u611f\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u540c\u6b65\u7b56\u7565\u5b9e\u73b0\u6700\u9ad859.25%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u6df1\u5ea6\u4f18\u5316\u5206\u79bb\u7b56\u7565\u540e\u53ef\u8fbe126.67%\u63d0\u5347\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u5f02\u6b65\u7ec4\u4ef6\u7684\u6709\u6548\u6027\uff0c8-256 GPU\u89c4\u6a21\u6269\u5c55\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u79c0\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b8c\u5168\u5f02\u6b65\u8bad\u7ec3\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3VLA\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u5f02\u6b65\u4f18\u5316\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4e3a\u5927\u89c4\u6a21VLA\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.06025", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06025", "abs": "https://arxiv.org/abs/2602.06025", "authors": ["Haozhen Zhang", "Haodong Yue", "Tao Feng", "Quanyu Long", "Jianzhu Bao", "Bowen Jin", "Weizhi Zhang", "Xiao Li", "Jiaxuan You", "Chengwei Qin", "Wenya Wang"], "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory", "comment": "Code is available at https://github.com/ViktorAxelsen/BudgetMem", "summary": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \\textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \\textsc{Low}/\\textsc{Mid}/\\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.", "AI": {"tldr": "BudgetMem\u662f\u4e00\u4e2a\u8fd0\u884c\u65f6\u4ee3\u7406\u5185\u5b58\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5c42\u9884\u7b97\u63a7\u5236\uff08\u4f4e/\u4e2d/\u9ad8\uff09\u548c\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u6765\u5e73\u8861\u4efb\u52a1\u6027\u80fd\u4e0e\u5185\u5b58\u6784\u5efa\u6210\u672c\uff0c\u5b9e\u73b0\u663e\u5f0f\u7684\u67e5\u8be2\u611f\u77e5\u6027\u80fd-\u6210\u672c\u6743\u8861\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5185\u5b58\u7cfb\u7edf\u5927\u591a\u4f9d\u8d56\u79bb\u7ebf\u7684\u3001\u67e5\u8be2\u65e0\u5173\u7684\u5185\u5b58\u6784\u5efa\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u53ef\u80fd\u4e22\u5f03\u5173\u952e\u4fe1\u606f\u3002\u8fd0\u884c\u65f6\u5185\u5b58\u5229\u7528\u867d\u7136\u81ea\u7136\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f00\u9500\u5927\u4e14\u7f3a\u4e4f\u5bf9\u6027\u80fd-\u6210\u672c\u6743\u8861\u7684\u663e\u5f0f\u63a7\u5236\u3002", "method": "\u5c06\u5185\u5b58\u5904\u7406\u7ed3\u6784\u5316\u4e3a\u591a\u4e2a\u5185\u5b58\u6a21\u5757\uff0c\u6bcf\u4e2a\u6a21\u5757\u63d0\u4f9b\u4e09\u4e2a\u9884\u7b97\u5c42\u7ea7\uff08\u4f4e/\u4e2d/\u9ad8\uff09\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u8fdb\u884c\u8de8\u6a21\u5757\u7684\u9884\u7b97\u5c42\u7ea7\u8def\u7531\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7d27\u51d1\u795e\u7ecf\u7b56\u7565\u5b9e\u73b0\u6210\u672c\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002\u7814\u7a76\u4e86\u4e09\u79cd\u5b9e\u73b0\u9884\u7b97\u5c42\u7ea7\u7684\u7b56\u7565\uff1a\u5b9e\u73b0\uff08\u65b9\u6cd5\u590d\u6742\u5ea6\uff09\u3001\u63a8\u7406\uff08\u63a8\u7406\u884c\u4e3a\uff09\u548c\u5bb9\u91cf\uff08\u6a21\u5757\u6a21\u578b\u5927\u5c0f\uff09\u3002", "result": "\u5728LoCoMo\u3001LongMemEval\u548cHotpotQA\u6570\u636e\u96c6\u4e0a\uff0cBudgetMem\u5728\u4f18\u5148\u6027\u80fd\uff08\u9ad8\u9884\u7b97\u8bbe\u7f6e\uff09\u65f6\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u5728\u66f4\u7d27\u9884\u7b97\u4e0b\u63d0\u4f9b\u66f4\u597d\u7684\u51c6\u786e\u7387-\u6210\u672c\u524d\u6cbf\u3002\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u5c42\u7ea7\u7b56\u7565\u7684\u4f18\u7f3a\u70b9\uff0c\u660e\u786e\u4e86\u5728\u4e0d\u540c\u9884\u7b97\u673a\u5236\u4e0b\u6bcf\u79cd\u7b56\u7565\u4f55\u65f6\u63d0\u4f9b\u6700\u6709\u5229\u7684\u6743\u8861\u3002", "conclusion": "BudgetMem\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86\u663e\u5f0f\u7684\u67e5\u8be2\u611f\u77e5\u6027\u80fd-\u6210\u672c\u63a7\u5236\uff0c\u901a\u8fc7\u9884\u7b97\u5c42\u7ea7\u8def\u7531\u6709\u6548\u5e73\u8861\u4e86LLM\u4ee3\u7406\u7684\u5185\u5b58\u6548\u7387\u4e0e\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u4e0d\u540c\u9884\u7b97\u573a\u666f\u4e0b\u7684\u5185\u5b58\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.05818", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.05818", "abs": "https://arxiv.org/abs/2602.05818", "authors": ["Zihao Jiang", "Miao Peng", "Zhenyan Shan", "Wenjie Xu", "Ben Liu", "Gong Chen", "Ziqi Gao", "Min Peng"], "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning", "comment": null, "summary": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.", "AI": {"tldr": "TKG-Thinker\uff1a\u4e00\u79cd\u7528\u4e8e\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u7684\u65b0\u578b\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u81ea\u4e3b\u89c4\u5212\u548c\u81ea\u9002\u5e94\u68c0\u7d22\u80fd\u529b\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u53cc\u9636\u6bb5\u8bad\u7ec3\uff0c\u5728\u590d\u6742\u65f6\u5e8f\u7ea6\u675f\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5728\u590d\u6742\u65f6\u5e8f\u7ea6\u675f\u4e0b\u5bb9\u6613\u4ea7\u751f\u63a8\u7406\u5e7b\u89c9\uff1b2\uff09\u9759\u6001\u63d0\u793a\u9650\u5236\u4e86\u6a21\u578b\u81ea\u4e3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7f3a\u4e4f\u4e0e\u77e5\u8bc6\u56fe\u8c31\u73af\u5883\u7684\u52a8\u6001\u4ea4\u4e92\u4f18\u5316\u3002", "method": "\u63d0\u51faTKG-Thinker\u667a\u80fd\u4f53\uff0c\u5177\u5907\u81ea\u4e3b\u89c4\u5212\u548c\u81ea\u9002\u5e94\u68c0\u7d22\u80fd\u529b\u3002\u91c7\u7528\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u4f7f\u7528\u601d\u7ef4\u94fe\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u57f9\u517b\u6838\u5fc3\u89c4\u5212\u80fd\u529b\uff1b\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u5229\u7528\u591a\u7ef4\u5956\u52b1\u5728\u590d\u6742\u65f6\u5e8f\u7ea6\u675f\u4e0b\u4f18\u5316\u63a8\u7406\u7b56\u7565\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTKG-Thinker\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u590d\u6742\u7684\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TKG-Thinker\u901a\u8fc7\u52a8\u6001\u591a\u8f6e\u4ea4\u4e92\u548c\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u7684\u63a8\u7406\u5e7b\u89c9\u548c\u6cdb\u5316\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u65f6\u5e8f\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.05183", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05183", "abs": "https://arxiv.org/abs/2602.05183", "authors": ["John Yan", "Michael Yu", "Yuqi Sun", "Alexander Duffy", "Tyler Marques", "Matthew Lyle Olson"], "title": "Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning", "comment": "authors 1, 2 and 3 contributed equally", "summary": "Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autointerp, a method for grouping SAE features into interpretable hypotheses about training dynamics. We discover fine-grained behaviors including role-playing patterns, degenerate outputs, language switching, alongside high-level strategic behaviors and environment-specific bugs. Through automated evaluation, we validate that 90% of discovered SAE Meta-Features are significant, and find a surprising reward hacking behavior. However, through two user studies, we find that even subjectively interesting and seemingly helpful SAE features may be worse than useless to humans, along with most LLM generated hypotheses. However, a subset of SAE-derived hypotheses are predictively useful for downstream tasks. We further provide validation by augmenting an untrained agent's system prompt, improving the score by +14.2%. Overall, we show that SAEs and LLM-summarizer provide complementary views into agent behavior, and together our framework forms a practical starting point for future data-centric interpretability work on ensuring trustworthy LLM behavior throughout training.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faMeta-Autointerp\u65b9\u6cd5\uff0c\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAEs)\u548cLLM\u6458\u8981\u5668\u5206\u6790\u590d\u6742\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2dLLM\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u53d1\u73b0\u4e86\u7ec6\u7c92\u5ea6\u884c\u4e3a\u6a21\u5f0f\uff0c\u4f46\u53d1\u73b0\u5927\u591a\u6570\u81ea\u52a8\u751f\u6210\u7684\u89e3\u91ca\u5bf9\u4eba\u7c7b\u5e2e\u52a9\u6709\u9650\u3002", "motivation": "\u968f\u7740LLMs\u5728\u590d\u6742\u5f3a\u5316\u5b66\u4e60\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u8bad\u7ec3\uff0c\u7406\u89e3\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u884c\u4e3a\u53d8\u5316\u53d8\u5f97\u56f0\u96be\u3002\u9700\u8981\u6570\u636e\u4e2d\u5fc3\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u6765\u5206\u6790\u8bad\u7ec3\u52a8\u6001\u3002", "method": "\u63d0\u51faMeta-Autointerp\u65b9\u6cd5\uff0c\u5e94\u7528\u9884\u8bad\u7ec3\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAEs)\u548cLLM\u6458\u8981\u5668\u5206\u6790Full-Press Diplomacy\u73af\u5883\u4e2d\u7684\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fd0\u884c\uff0c\u5c06SAE\u7279\u5f81\u5206\u7ec4\u4e3a\u53ef\u89e3\u91ca\u7684\u5047\u8bbe\u3002", "result": "\u53d1\u73b0\u4e86\u89d2\u8272\u626e\u6f14\u6a21\u5f0f\u3001\u9000\u5316\u8f93\u51fa\u3001\u8bed\u8a00\u5207\u6362\u7b49\u7ec6\u7c92\u5ea6\u884c\u4e3a\uff0c\u4ee5\u53ca\u9ad8\u7ea7\u6218\u7565\u884c\u4e3a\u548c\u7279\u5b9a\u73af\u5883bug\u300290%\u7684SAE\u5143\u7279\u5f81\u663e\u8457\uff0c\u53d1\u73b0\u4e86\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u3002\u4f46\u7528\u6237\u7814\u7a76\u53d1\u73b0\u5927\u591a\u6570SAE\u7279\u5f81\u548cLLM\u751f\u6210\u5047\u8bbe\u5bf9\u4eba\u7c7b\u5e2e\u52a9\u6709\u9650\uff0c\u53ea\u6709\u90e8\u5206SAE\u884d\u751f\u5047\u8bbe\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6709\u9884\u6d4b\u4ef7\u503c\u3002", "conclusion": "SAEs\u548cLLM\u6458\u8981\u5668\u63d0\u4f9b\u4e86\u4e92\u8865\u7684\u89c6\u89d2\u6765\u5206\u6790\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u8be5\u6846\u67b6\u4e3a\u672a\u6765\u6570\u636e\u4e2d\u5fc3\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u8d77\u70b9\uff0c\u4ee5\u786e\u4fddLLM\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u53ef\u4fe1\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2602.05877", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.05877", "abs": "https://arxiv.org/abs/2602.05877", "authors": ["Lukas Stappen", "Ahmet Erkan Turan", "Johann Hagerer", "Georg Groh"], "title": "Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy", "comment": null, "summary": "The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous \"separation of concerns\" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented \"victim modeling\" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.", "AI": {"tldr": "\u63d0\u51faAgentHeLLM\u5a01\u80c1\u5efa\u6a21\u6846\u67b6\uff0c\u5c06\u8d44\u4ea7\u8bc6\u522b\u4e0e\u653b\u51fb\u8def\u5f84\u5206\u6790\u5206\u79bb\uff0c\u7528\u4e8e\u8f66\u8f86LLM\u667a\u80fd\u52a9\u624b\u7684\u5b89\u5168\u5206\u6790", "motivation": "\u8f66\u8f86LLM\u667a\u80fd\u52a9\u624b\u901a\u8fc7A2A\u7b49\u534f\u8bae\u4e0e\u5916\u90e8\u670d\u52a1\u534f\u8c03\uff0c\u521b\u5efa\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff0c\u73b0\u6709AI\u5b89\u5168\u6846\u67b6\u7f3a\u4e4f\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u5de5\u7a0b\u7684\"\u5173\u6ce8\u70b9\u5206\u79bb\"\u539f\u5219", "method": "\u63d0\u51faAgentHeLLM\u6846\u67b6\uff1a1)\u57fa\u4e8e\u4f24\u5bb3\u5bfc\u5411\"\u53d7\u5bb3\u8005\u5efa\u6a21\"\u548c\u300a\u4e16\u754c\u4eba\u6743\u5ba3\u8a00\u300b\u7684\u4eba\u7c7b\u4e2d\u5fc3\u8d44\u4ea7\u5206\u7c7b\u6cd5\uff1b2)\u533a\u5206\u6bd2\u5316\u8def\u5f84\uff08\u6076\u610f\u6570\u636e\u4f20\u64ad\uff09\u548c\u89e6\u53d1\u8def\u5f84\uff08\u6fc0\u6d3b\u52a8\u4f5c\uff09\u7684\u57fa\u4e8e\u56fe\u7684\u6b63\u5f0f\u6a21\u578b\uff1b3)\u5f00\u6e90\u653b\u51fb\u8def\u5f84\u5efa\u8bae\u5de5\u5177AgentHeLLM Attack Path Generator\uff0c\u4f7f\u7528\u53cc\u5c42\u641c\u7d22\u7b56\u7565\u81ea\u52a8\u5316\u591a\u9636\u6bb5\u5a01\u80c1\u53d1\u73b0", "result": "\u5f00\u53d1\u4e86\u5b9e\u7528\u7684\u653b\u51fb\u8def\u5f84\u5efa\u8bae\u5de5\u5177\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u53d1\u73b0\u591a\u9636\u6bb5\u5a01\u80c1\uff0c\u4e3a\u8f66\u8f86LLM\u667a\u80fd\u52a9\u624b\u5b89\u5168\u5206\u6790\u63d0\u4f9b\u7cfb\u7edf\u5316\u65b9\u6cd5", "conclusion": "AgentHeLLM\u6846\u67b6\u901a\u8fc7\u6b63\u5f0f\u5206\u79bb\u8d44\u4ea7\u8bc6\u522b\u548c\u653b\u51fb\u8def\u5f84\u5206\u6790\uff0c\u586b\u8865\u4e86\u73b0\u6709AI\u5b89\u5168\u6846\u67b6\u7684\u65b9\u6cd5\u8bba\u7a7a\u767d\uff0c\u4e3a\u8f66\u8f86LLM\u667a\u80fd\u52a9\u624b\u7684\u5b89\u5168\u5a01\u80c1\u5efa\u6a21\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5", "topic": "agent analysis"}}
{"id": "2602.05920", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.05920", "abs": "https://arxiv.org/abs/2602.05920", "authors": ["Eva Andr\u00e9s"], "title": "Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem", "comment": "22 pages, 12 figures", "summary": "This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6bd4\u8f83\u4e86\u7ecf\u5178\u548c\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u5e26\u5bb9\u91cf\u7ea6\u675f\u7684\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff0c\u53d1\u73b0\u91cf\u5b50\u589e\u5f3a\u6a21\u578b\u5728\u8def\u7531\u8ddd\u79bb\u3001\u7d27\u51d1\u6027\u548c\u8def\u7ebf\u91cd\u53e0\u65b9\u9762\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\uff0c\u5176\u4e2d\u6df7\u5408\u67b6\u6784\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u5e26\u5bb9\u91cf\u7ea6\u675f\u7684\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08CVRP\uff09\uff0c\u63a2\u7d22\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\uff0c\u6bd4\u8f83\u7ecf\u5178\u3001\u5168\u91cf\u5b50\u548c\u6df7\u5408\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u4f18\u52bf\u6f14\u5458-\u8bc4\u8bba\u5bb6\uff08A2C\uff09\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u7ecf\u5178\u3001\u5168\u91cf\u5b50\u548c\u6df7\u5408\u4e09\u79cd\u53d8\u4f53\uff0c\u96c6\u6210transformer\u67b6\u6784\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u8f66\u8f86\u3001\u5ba2\u6237\u548c\u4ed3\u5e93\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5b9e\u9a8c\u9488\u5bf920\u4e2a\u5ba2\u6237\u548c4\u8f86\u8f66\u7684\u591a\u8f66\u8f86\u5bb9\u91cf\u7ea6\u675f\u573a\u666f\uff0c\u8fdb\u884c10\u6b21\u72ec\u7acb\u8fd0\u884c\u3002", "result": "\u6240\u6709\u4e09\u79cd\u65b9\u6cd5\u90fd\u80fd\u5b66\u4e60\u6709\u6548\u7684\u8def\u7531\u7b56\u7565\uff0c\u4f46\u91cf\u5b50\u589e\u5f3a\u6a21\u578b\u4f18\u4e8e\u7ecf\u5178\u57fa\u7ebf\uff0c\u4ea7\u751f\u66f4\u7a33\u5065\u7684\u8def\u7ebf\u7ec4\u7ec7\u3002\u6df7\u5408\u67b6\u6784\u5728\u8ddd\u79bb\u3001\u7d27\u51d1\u6027\u548c\u8def\u7ebf\u91cd\u53e0\u65b9\u9762\u5b9e\u73b0\u6700\u4f73\u6574\u4f53\u6027\u80fd\u3002\u5b9a\u6027\u53ef\u89c6\u5316\u663e\u793a\u91cf\u5b50\u6a21\u578b\u751f\u6210\u66f4\u7ed3\u6784\u5316\u548c\u8fde\u8d2f\u7684\u8def\u7531\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5728\u89e3\u51b3CVRP\u7b49\u590d\u6742\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u91cf\u5b50\u589e\u5f3a\u65b9\u6cd5\u5728\u8def\u7531\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u7eaf\u7ecf\u5178\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.06008", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06008", "abs": "https://arxiv.org/abs/2602.06008", "authors": ["Xianyang Liu", "Shangding Gu", "Dawn Song"], "title": "AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions", "comment": null, "summary": "Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.", "AI": {"tldr": "AgenticPay\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u4e70\u5bb6-\u5356\u5bb6\u8c08\u5224\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u4eff\u771f\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\uff0c\u5305\u542b110\u591a\u4e2a\u4efb\u52a1\uff0c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u7ecf\u6d4e\u5b66\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u8d8a\u6765\u8d8a\u591a\u5730\u9700\u8981\u81ea\u4e3b\u8fdb\u884c\u8c08\u5224\u3001\u534f\u8c03\u548c\u4ea4\u6613\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u4e4b\u95f4\u8bed\u8a00\u4ecb\u5bfc\u7684\u7ecf\u6d4e\u4ea4\u4e92\u7684\u539f\u5219\u6027\u8bbe\u7f6e\u3002", "method": "\u5f00\u53d1\u4e86AgenticPay\u6846\u67b6\uff0c\u6a21\u62df\u4e70\u5bb6\u548c\u5356\u5bb6\u62e5\u6709\u79c1\u6709\u7ea6\u675f\u548c\u4ea7\u54c1\u4f9d\u8d56\u4f30\u503c\u7684\u5e02\u573a\u73af\u5883\uff0c\u901a\u8fc7\u591a\u8f6e\u8bed\u8a00\u8c08\u5224\u800c\u975e\u5355\u7eaf\u6570\u5b57\u7ade\u4ef7\u8fbe\u6210\u534f\u8bae\u3002\u5305\u542b\u7ed3\u6784\u5316\u52a8\u4f5c\u63d0\u53d6\u548c\u53ef\u884c\u6027\u3001\u6548\u7387\u3001\u798f\u5229\u7b49\u6307\u6807\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u7684\u4e13\u6709\u548c\u5f00\u6e90\u6743\u91cdLLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u8c08\u5224\u6027\u80fd\u4e0a\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u7a81\u663e\u4e86\u957f\u671f\u6218\u7565\u63a8\u7406\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "AgenticPay\u4e3a\u7814\u7a76\u667a\u80fd\u4f53\u5546\u4e1a\u548c\u57fa\u4e8e\u8bed\u8a00\u7684\u5e02\u573a\u4ea4\u4e92\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2602.05234", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05234", "abs": "https://arxiv.org/abs/2602.05234", "authors": ["Yuntai Bao", "Xuhong Zhang", "Jintao Chen", "Ge Su", "Yuxiang Cai", "Hao Peng", "Bing Sun", "Haiqin Weng", "Liu Yan", "Jianwei Yin"], "title": "Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions", "comment": "55 pages, 25 figures; accepted for ICLR 2026", "summary": "Intervention-based model steering offers a lightweight and interpretable alternative to prompting and fine-tuning. However, by adapting strong optimization objectives from fine-tuning, current methods are susceptible to overfitting and often underperform, sometimes generating unnatural outputs. We hypothesize that this is because effective steering requires the faithful identification of internal model mechanisms, not the enforcement of external preferences. To this end, we build on the principles of distributed alignment search (DAS), the standard for causal variable localization, to propose a new steering method: Concept DAS (CDAS). While we adopt the core mechanism of DAS, distributed interchange intervention (DII), we introduce a novel distribution matching objective tailored for the steering task by aligning intervened output distributions with counterfactual distributions. CDAS differs from prior work in two main ways: first, it learns interventions via weak-supervised distribution matching rather than probability maximization; second, it uses DIIs that naturally enable bi-directional steering and allow steering factors to be derived from data, reducing the effort required for hyperparameter tuning and resulting in more faithful and stable control. On AxBench, a large-scale model steering benchmark, we show that CDAS does not always outperform preference-optimization methods but may benefit more from increased model scale. In two safety-related case studies, overriding refusal behaviors of safety-aligned models and neutralizing a chain-of-thought backdoor, CDAS achieves systematic steering while maintaining general model utility. These results indicate that CDAS is complementary to preference-optimization approaches and conditionally constitutes a robust approach to intervention-based model steering. Our code is available at https://github.com/colored-dye/concept_das.", "AI": {"tldr": "\u63d0\u51faConcept DAS\uff08CDAS\uff09\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5206\u5e03\u5f0f\u5bf9\u9f50\u641c\u7d22\u539f\u7406\uff0c\u901a\u8fc7\u5206\u5e03\u5339\u914d\u76ee\u6807\u800c\u975e\u6982\u7387\u6700\u5927\u5316\u6765\u5b9e\u73b0\u66f4\u5fe0\u5b9e\u3001\u7a33\u5b9a\u7684\u6a21\u578b\u5e72\u9884\u63a7\u5236", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5e72\u9884\u7684\u6a21\u578b\u63a7\u5236\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u4e14\u6027\u80fd\u4e0d\u4f73\uff0c\u56e0\u4e3a\u6709\u6548\u7684\u63a7\u5236\u9700\u8981\u8bc6\u522b\u5185\u90e8\u673a\u5236\u800c\u975e\u5f3a\u52a0\u5916\u90e8\u504f\u597d", "method": "\u57fa\u4e8e\u5206\u5e03\u5f0f\u5bf9\u9f50\u641c\u7d22\uff08DAS\uff09\u539f\u7406\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u4ea4\u6362\u5e72\u9884\uff08DII\uff09\u673a\u5236\uff0c\u5f15\u5165\u9488\u5bf9\u63a7\u5236\u4efb\u52a1\u7684\u5206\u5e03\u5339\u914d\u76ee\u6807\uff0c\u4f7f\u5e72\u9884\u8f93\u51fa\u5206\u5e03\u4e0e\u53cd\u4e8b\u5b9e\u5206\u5e03\u5bf9\u9f50", "result": "\u5728AxBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCDAS\u4e0d\u4e00\u5b9a\u4f18\u4e8e\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u4f46\u80fd\u4ece\u6a21\u578b\u89c4\u6a21\u6269\u5927\u4e2d\u83b7\u76ca\u66f4\u591a\uff1b\u5728\u5b89\u5168\u76f8\u5173\u6848\u4f8b\u4e2d\u80fd\u7cfb\u7edf\u63a7\u5236\u6a21\u578b\u540c\u65f6\u4fdd\u6301\u901a\u7528\u6027\u80fd", "conclusion": "CDAS\u4e0e\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u4e92\u8865\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u6784\u6210\u7a33\u5065\u7684\u5e72\u9884\u5f0f\u6a21\u578b\u63a7\u5236\u65b9\u6cd5", "topic": "agent analysis"}}
{"id": "2602.06039", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06039", "abs": "https://arxiv.org/abs/2602.06039", "authors": ["Yuxing Lu", "Yucheng Hu", "Xukai Zhao", "Jiuxin Cao"], "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching", "comment": null, "summary": "Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \\key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.", "AI": {"tldr": "DyTopo\u662f\u4e00\u4e2a\u52a8\u6001\u62d3\u6251\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6bcf\u8f6e\u91cd\u5efa\u7a00\u758f\u6709\u5411\u901a\u4fe1\u56fe\u6765\u4f18\u5316LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5728\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u53476.2%\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u7684\u901a\u4fe1\u6a21\u5f0f\uff0c\u65e0\u6cd5\u9002\u5e94\u8fed\u4ee3\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u4e2d\u4e0d\u540c\u9636\u6bb5\u7684\u9700\u6c42\u53d8\u5316\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u52a8\u6001\u7684\u901a\u4fe1\u673a\u5236\u3002", "method": "DyTopo\u91c7\u7528\u7ba1\u7406\u8005\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6bcf\u8f6e\u6839\u636e\u7ba1\u7406\u8005\u7684\u76ee\u6807\uff0c\u8ba9\u6bcf\u4e2a\u667a\u80fd\u4f53\u8f93\u51fa\u8f7b\u91cf\u7ea7\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff08\u9700\u6c42\uff09\u548c\u5173\u952e\u4fe1\u606f\uff08\u63d0\u4f9b\uff09\u63cf\u8ff0\u7b26\uff0c\u901a\u8fc7\u8bed\u4e49\u5339\u914d\u6784\u5efa\u7a00\u758f\u6709\u5411\u901a\u4fe1\u56fe\uff0c\u4ec5\u6cbf\u8bf1\u5bfc\u8fb9\u4f20\u9012\u79c1\u6709\u6d88\u606f\u3002", "result": "\u5728\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u56db\u79cdLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0cDyTopo\u59cb\u7ec8\u4f18\u4e8e\u6700\u5f3a\u57fa\u7ebf\uff08\u5e73\u5747\u63d0\u5347+6.2%\uff09\uff0c\u5e76\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u534f\u8c03\u8f68\u8ff9\u3002", "conclusion": "DyTopo\u901a\u8fc7\u52a8\u6001\u91cd\u6784\u901a\u4fe1\u62d3\u6251\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u534f\u8c03\u8fc7\u7a0b\u53ef\u89c6\u5316\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u8de8\u8f6e\u6b21\u7684\u901a\u4fe1\u8def\u5f84\u91cd\u6784\u3002", "topic": "agent analysis"}}
{"id": "2602.05311", "categories": ["cs.LG", "cs.AI", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.05311", "abs": "https://arxiv.org/abs/2602.05311", "authors": ["Chengxiao Wang", "Haoze Wu", "Gagandeep Singh"], "title": "Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates", "comment": null, "summary": "Neural Lyapunov and barrier certificates have recently been used as powerful tools for verifying the safety and stability properties of deep reinforcement learning (RL) controllers. However, existing methods offer guarantees only under fixed ideal unperturbed dynamics, limiting their reliability in real-world applications where dynamics may deviate due to uncertainties. In this work, we study the problem of synthesizing \\emph{robust neural Lyapunov barrier certificates} that maintain their guarantees under perturbations in system dynamics. We formally define a robust Lyapunov barrier function and specify sufficient conditions based on Lipschitz continuity that ensure robustness against bounded perturbations. We propose practical training objectives that enforce these conditions via adversarial training, Lipschitz neighborhood bound, and global Lipschitz regularization. We validate our approach in two practically relevant environments, Inverted Pendulum and 2D Docking. The former is a widely studied benchmark, while the latter is a safety-critical task in autonomous systems. We show that our methods significantly improve both certified robustness bounds (up to $4.6$ times) and empirical success rates under strong perturbations (up to $2.4$ times) compared to the baseline. Our results demonstrate effectiveness of training robust neural certificates for safe RL under perturbations in dynamics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u9c81\u68d2\u795e\u7ecf\u674e\u96c5\u666e\u8bfa\u592b\u5c4f\u969c\u8bc1\u4e66\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7cfb\u7edf\u52a8\u529b\u5b66\u5b58\u5728\u6270\u52a8\u7684\u60c5\u51b5\u4e0b\u9a8c\u8bc1\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u7684\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u674e\u96c5\u666e\u8bfa\u592b\u548c\u5c4f\u969c\u8bc1\u4e66\u65b9\u6cd5\u4ec5\u5728\u56fa\u5b9a\u7406\u60f3\u65e0\u6270\u52a8\u52a8\u529b\u5b66\u4e0b\u63d0\u4f9b\u4fdd\u8bc1\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u56e0\u4e3a\u5b9e\u9645\u7cfb\u7edf\u52a8\u529b\u5b66\u53ef\u80fd\u56e0\u4e0d\u786e\u5b9a\u6027\u800c\u504f\u79bb\u3002", "method": "\u63d0\u51fa\u9c81\u68d2\u674e\u96c5\u666e\u8bfa\u592b\u5c4f\u969c\u51fd\u6570\u7684\u6b63\u5f0f\u5b9a\u4e49\uff0c\u57fa\u4e8eLipschitz\u8fde\u7eed\u6027\u6307\u5b9a\u5145\u5206\u6761\u4ef6\u4ee5\u786e\u4fdd\u5bf9\u6709\u754c\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u3001Lipschitz\u90bb\u57df\u8fb9\u754c\u548c\u5168\u5c40Lipschitz\u6b63\u5219\u5316\u7b49\u5b9e\u7528\u8bad\u7ec3\u76ee\u6807\u6765\u5f3a\u5236\u6267\u884c\u8fd9\u4e9b\u6761\u4ef6\u3002", "result": "\u5728Inverted Pendulum\u548c2D Docking\u4e24\u4e2a\u5b9e\u9645\u76f8\u5173\u73af\u5883\u4e2d\u9a8c\u8bc1\u65b9\u6cd5\u3002\u76f8\u6bd4\u57fa\u7ebf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba4\u8bc1\u9c81\u68d2\u6027\u8fb9\u754c\uff08\u9ad8\u8fbe4.6\u500d\uff09\u548c\u5f3a\u6270\u52a8\u4e0b\u7684\u7ecf\u9a8c\u6210\u529f\u7387\uff08\u9ad8\u8fbe2.4\u500d\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u52a8\u529b\u5b66\u5b58\u5728\u6270\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u8bad\u7ec3\u9c81\u68d2\u795e\u7ecf\u8bc1\u4e66\u5bf9\u4e8e\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u662f\u6709\u6548\u7684\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.05859", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05859", "abs": "https://arxiv.org/abs/2602.05859", "authors": ["Xu Wang", "Bingqing Jiang", "Yu Wan", "Baosong Yang", "Lingpeng Kong", "Difan Zou"], "title": "DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders", "comment": "23 pages", "summary": "Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs. Our work establishes a foundation for mechanistic interpretability in DLMs and shows a great potential of applying SAEs to DLM-related tasks and algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLMs\uff09\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u53ef\u89e3\u91ca\u6027\u6846\u67b6DLM-Scope\uff0c\u53d1\u73b0SAE\u5728DLMs\u4e2d\u7684\u884c\u4e3a\u4e0e\u81ea\u56de\u5f52LLMs\u4e0d\u540c\uff0c\u80fd\u6709\u6548\u63d0\u53d6\u53ef\u89e3\u91ca\u7279\u5f81\u5e76\u5b9e\u73b0\u66f4\u597d\u7684\u5e72\u9884\u6548\u679c\u3002", "motivation": "\u968f\u7740\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLMs\uff09\u6210\u4e3a\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6709\u524d\u666f\u66ff\u4ee3\u65b9\u6848\uff0c\u9700\u8981\u4e3a\u8fd9\u7c7b\u65b0\u5174\u6a21\u578b\u5f00\u53d1\u4e13\u95e8\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u3002\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5728LLMs\u4e2d\u5df2\u6210\u4e3a\u6807\u51c6\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff0c\u4f46\u5c1a\u672a\u5e94\u7528\u4e8eDLMs\u3002", "method": "\u63d0\u51fa\u4e86DLM-Scope\u6846\u67b6\uff0c\u4f7f\u7528Top-K\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u63d0\u53d6DLMs\u7684\u53ef\u89e3\u91ca\u7279\u5f81\u3002\u7814\u7a76\u4e86SAE\u63d2\u5165\u5bf9DLMs\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u4e0eLLMs\u7684\u5dee\u5f02\uff0c\u5e76\u63a2\u7d22\u4e86SAE\u5728DLMs\u89e3\u7801\u987a\u5e8f\u548c\u8bad\u7ec3\u540e\u7a33\u5b9a\u6027\u7b49\u65b0\u7814\u7a76\u65b9\u5411\u3002", "result": "1) SAE\u5728DLMs\u65e9\u671f\u5c42\u63d2\u5165\u80fd\u51cf\u5c11\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u4e0eLLMs\u4e2d\u901a\u5e38\u589e\u52a0\u635f\u5931\u7684\u73b0\u8c61\u76f8\u53cd\uff1b2) DLM\u7684SAE\u7279\u5f81\u80fd\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u6269\u6563\u65f6\u95f4\u5e72\u9884\uff0c\u4f18\u4e8eLLM\u5f15\u5bfc\uff1b3) SAE\u80fd\u4e3aDLM\u89e3\u7801\u987a\u5e8f\u63d0\u4f9b\u6709\u7528\u4fe1\u53f7\uff1b4) SAE\u7279\u5f81\u5728DLMs\u8bad\u7ec3\u540e\u9636\u6bb5\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aDLMs\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86SAEs\u5728DLM\u76f8\u5173\u4efb\u52a1\u548c\u7b97\u6cd5\u4e2d\u7684\u5de8\u5927\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u8fd9\u4e00\u65b0\u5174\u6a21\u578b\u7c7b\u522b\u63d0\u4f9b\u4e86\u4e13\u95e8\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2602.05885", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05885", "abs": "https://arxiv.org/abs/2602.05885", "authors": ["Wei Liu", "Jiawei Xu", "Yingru Li", "Longtao Zheng", "Tianjian Li", "Qian Liu", "Junxian He"], "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations", "comment": null, "summary": "High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.", "AI": {"tldr": "\u63d0\u51faKernelGYM\u5206\u5e03\u5f0fGPU\u73af\u5883\uff0c\u5f00\u53d1TRLOO\u65b9\u6cd5\u89e3\u51b3\u591a\u8f6eRL\u4e2d\u7684\u504f\u5dee\u95ee\u9898\uff0c\u7ed3\u5408Profiling-based Rewards\u548cRejection Sampling\u8bad\u7ec3Dr.Kernel-14B\u6a21\u578b\uff0c\u5728KernelBench\u4e0a\u6027\u80fd\u8d85\u8d8aClaude-4.5-Sonnet\u548cGPT-5", "motivation": "\u9ad8\u8d28\u91cf\u5185\u6838\u5bf9\u53ef\u6269\u5c55AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bad\u7ec3LLM\u751f\u6210\u5185\u6838\u4ee3\u7801\u9762\u4e34\u6570\u636e\u4e0d\u8db3\u3001\u73af\u5883\u8106\u5f31\u3001\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u548c\u61d2\u60f0\u4f18\u5316\u7b49\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u5728\u5185\u6838\u751f\u6210\u4e2d\u7684\u5e94\u7528", "method": "1) \u8bbe\u8ba1KernelGYM\u5206\u5e03\u5f0fGPU\u73af\u5883\u652f\u6301\u5956\u52b1\u9ed1\u5ba2\u68c0\u67e5\u3001\u591a\u8f6e\u4ea4\u4e92\u6570\u636e\u6536\u96c6\u548c\u957f\u671fRL\u8bad\u7ec3\uff1b2) \u63d0\u51faTRLOO\u65b9\u6cd5\u89e3\u51b3GRPO\u4e2d\u7684\u81ea\u5305\u542b\u504f\u5dee\u95ee\u9898\uff1b3) \u5f15\u5165Profiling-based Rewards\u548cProfiling-based Rejection Sampling\u89e3\u51b3\u61d2\u60f0\u4f18\u5316\uff1b4) \u7ed3\u5408\u4e0d\u5339\u914d\u6821\u6b63\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027", "result": "Dr.Kernel-14B\u5728KernelBench\u4e0a\u6027\u80fd\u4e0eClaude-4.5-Sonnet\u76f8\u5f53\uff0c\u5728Level-2\u5b50\u96c6\u4e0a31.6%\u751f\u6210\u7684\u5185\u6838\u8fbe\u5230\u81f3\u5c111.2\u500d\u52a0\u901f\uff0c\u8d85\u8d8aClaude-4.5-Sonnet(26.7%)\u548cGPT-5(28.6%)\uff1b\u591a\u8f6e\u6700\u4f73\u5019\u9009\u9009\u62e9\u65f6\u52a0\u901f\u7387\u63d0\u5347\u81f347.8%", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u5185\u6838\u751f\u6210\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u73af\u5883\u548c\u7b97\u6cd5\uff0c\u6210\u529f\u8bad\u7ec3\u51fa\u9ad8\u6027\u80fd\u5185\u6838\u751f\u6210\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u591a\u8f6eRL\u548cProfiling-based\u65b9\u6cd5\u5728\u5185\u6838\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2602.05890", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05890", "abs": "https://arxiv.org/abs/2602.05890", "authors": ["Dingwei Zhu", "Zhiheng Xi", "Shihan Dou", "Jiahan Li", "Chenhao Huang", "Junjie Ye", "Sixian Li", "Mingxu Chai", "Yuhui Wang", "Yajie Yang", "Ming Zhang", "Jiazheng Zhang", "Shichun Liu", "Caishuang Huang", "Yunke Zhang", "Yuran Wang", "Tao Gui", "Xipeng Qiu", "Qi Zhang", "Xuanjing Huang"], "title": "DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training", "comment": null, "summary": "Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.", "AI": {"tldr": "DFPO\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4ef7\u503c\u5efa\u6a21\u4e3a\u8de8\u65f6\u95f4\u6b65\u7684\u8fde\u7eed\u6d41\u800c\u975e\u72ec\u7acb\u7684\u5206\u4f4d\u6570\u9884\u6d4b\uff0c\u5728\u566a\u58f0\u76d1\u7763\u4e0b\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8bad\u7ec3RL\u7cfb\u7edf\u9762\u4e34\u566a\u58f0\u76d1\u7763\u548c\u57df\u5916\u6cdb\u5316\u5dee\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728LLM\u540e\u8bad\u7ec3\u4e2d\u3002\u73b0\u6709\u7684\u5206\u5e03RL\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u5efa\u6a21\u591a\u4e2a\u5206\u4f4d\u6570\u70b9\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u4f46\u6bcf\u4e2a\u5206\u4f4d\u6570\u4ecd\u4f5c\u4e3a\u6807\u91cf\u72ec\u7acb\u5b66\u4e60\uff0c\u5bfc\u81f4\u4ef7\u503c\u8868\u793a\u7c97\u7cd9\uff0c\u7f3a\u4e4f\u5bf9\u72b6\u6001\u4fe1\u606f\u7684\u7ec6\u7c92\u5ea6\u6761\u4ef6\u5316\uff0c\u5728\u590d\u6742\u548cOOD\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "DFPO\uff08Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control\uff09\u5c06\u4ef7\u503c\u5efa\u6a21\u4e3a\u8de8\u65f6\u95f4\u6b65\u7684\u8fde\u7eed\u6d41\uff0c\u901a\u8fc7\u5b66\u4e60\u4ef7\u503c\u6d41\u573a\u800c\u975e\u5b64\u7acb\u7684\u5206\u4f4d\u6570\u9884\u6d4b\u6765\u6269\u5c55\u4ef7\u503c\u5efa\u6a21\u3002\u4e3a\u4e86\u5728\u566a\u58f0\u53cd\u9988\u4e0b\u7a33\u5b9a\u8bad\u7ec3\uff0cDFPO\u8fdb\u4e00\u6b65\u96c6\u6210\u4e86\u6761\u4ef6\u98ce\u9669\u63a7\u5236\u548c\u4ef7\u503c\u6d41\u8f68\u8ff9\u4e0a\u7684\u4e00\u81f4\u6027\u7ea6\u675f\u3002", "result": "\u5728\u5bf9\u8bdd\u3001\u6570\u5b66\u63a8\u7406\u548c\u79d1\u5b66\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDFPO\u5728\u566a\u58f0\u76d1\u7763\u4e0b\u4f18\u4e8ePPO\u3001FlowRL\u548c\u5176\u4ed6\u9c81\u68d2\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u6539\u8fdb\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DFPO\u901a\u8fc7\u8fde\u7eed\u4ef7\u503c\u6d41\u5efa\u6a21\u548c\u6761\u4ef6\u98ce\u9669\u63a7\u5236\uff0c\u4e3a\u566a\u58f0\u73af\u5883\u4e0b\u7684RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728LLM\u540e\u8bad\u7ec3\u7b49\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.05459", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05459", "abs": "https://arxiv.org/abs/2602.05459", "authors": ["Jan Malte T\u00f6pperwien", "Aditya Mohan", "Marius Lindauer"], "title": "When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL", "comment": "27 pages, 19 figures", "summary": "Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\\approx$ 20\\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8d85\u53c2\u6570\u654f\u611f\u6027\u5e76\u975eRL\u95ee\u9898\u56fa\u6709\uff0c\u800c\u662f\u7531\u81ea\u4e3e\u673a\u5236\u52a0\u5267\u3002\u5728\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6RL\u4e2d\uff0c\u5f53\u5b58\u5728\u9002\u5ea6\u4e13\u5bb6\u6570\u636e\u65f6\uff0c\u57fa\u4e8e\u51c6\u5ea6\u91cf\u8868\u793a\u5b66\u4e60\u7684\u7b97\u6cd5\u6bd4\u57fa\u4e8e\u81ea\u4e3eTD\u5b66\u4e60\u7684\u7b97\u6cd5\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8d85\u53c2\u6570\u654f\u611f\u6027\u901a\u5e38\u88ab\u8ba4\u4e3a\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u662fRL\u95ee\u9898\u7684\u56fa\u6709\u7279\u6027\u8fd8\u662f\u7279\u5b9a\u8bad\u7ec3\u673a\u5236\u52a0\u5267\u7684\u7ed3\u679c\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5728\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6RL\u4e2d\uff0c\u5f53\u6570\u636e\u5206\u5e03\u56fa\u5b9a\u4e14\u975e\u5e73\u7a33\u6027\u53ef\u88ab\u660e\u786e\u63a7\u5236\u65f6\uff0c\u8d85\u53c2\u6570\u654f\u611f\u6027\u7684\u672c\u8d28\u3002", "method": "\u7814\u7a76\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6RL\uff0c\u901a\u8fc7\u8c03\u5ea6\u6570\u636e\u8d28\u91cf\u53d8\u5316\u6765\u660e\u786e\u63a7\u5236\u975e\u5e73\u7a33\u6027\u3002\u5728\u5e73\u7a33\u548c\u975e\u5e73\u7a33\u4e24\u79cd\u673a\u5236\u4e0b\u7814\u7a76\u4e0d\u540c\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u6bd4\u8f83\u4e24\u79cd\u4ee3\u8868\u6027\u7b97\u6cd5\uff1aHIQL\uff08\u57fa\u4e8e\u81ea\u4e3eTD\u5b66\u4e60\uff09\u548cQRL\uff08\u51c6\u5ea6\u91cf\u8868\u793a\u5b66\u4e60\uff09\u3002\u5f15\u5165\u8de8\u76ee\u6807\u68af\u5ea6\u5bf9\u9f50\u8bca\u65ad\u6765\u5206\u6790\u7b97\u6cd5\u5dee\u5f02\u3002", "result": "\u89c2\u5bdf\u5230\u6bd4\u5728\u7ebfRL\u66f4\u5f3a\u7684\u8d85\u53c2\u6570\u914d\u7f6e\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u5728\u53d7\u63a7\u975e\u5e73\u7a33\u6027\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u5f53\u5b58\u5728\u7ea620%\u4e13\u5bb6\u6570\u636e\u65f6\uff0cQRL\u4fdd\u6301\u5e7f\u6cdb\u7a33\u5b9a\u7684\u8fd1\u6700\u4f18\u533a\u57df\uff0c\u800cHIQL\u8868\u73b0\u51fa\u5c16\u9510\u7684\u6700\u4f18\u70b9\u4e14\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u663e\u8457\u6f02\u79fb\u3002\u81ea\u4e3e\u76ee\u6807\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u7834\u574f\u6027\u68af\u5ea6\u5e72\u6270\uff0c\u8fd9\u4e0e\u8d85\u53c2\u6570\u654f\u611f\u6027\u76f4\u63a5\u76f8\u5173\u3002", "conclusion": "\u8bad\u7ec3\u671f\u95f4\u5bf9\u8d85\u53c2\u6570\u914d\u7f6e\u53d8\u5316\u7684\u9ad8\u654f\u611f\u6027\u5728RL\u4e2d\u5e76\u975e\u4e0d\u53ef\u907f\u514d\uff0c\u800c\u662f\u7531\u81ea\u4e3e\u673a\u5236\u52a8\u6001\u52a0\u5267\u3002\u8fd9\u4e3a\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684\u7b97\u6cd5\u76ee\u6807\u63d0\u4f9b\u4e86\u9014\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.05535", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05535", "abs": "https://arxiv.org/abs/2602.05535", "authors": ["Tao Huang", "Rui Wang", "Xiaofei Liu", "Yi Qin", "Li Duan", "Liping Jing"], "title": "Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification", "comment": "Accepted to ICLR 2026. Code is available at https://github.com/HT86159/EUQ", "summary": "Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \\emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.", "AI": {"tldr": "EUQ\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u8bc1\u636e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u6b63\u8d1f\u8bc1\u636e\u6765\u68c0\u6d4bLVLM\u7684\u8bef\u884c\u4e3a\uff0c\u5305\u62ec\u5e7b\u89c9\u3001\u8d8a\u72f1\u3001\u5bf9\u6297\u653b\u51fb\u548cOOD\u5931\u8d25\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u4e0d\u5b8c\u6574\u6216\u5bf9\u6297\u6027\u8f93\u5165\u65f6\u4f1a\u4ea7\u751f\u4e0d\u53ef\u9760\u751a\u81f3\u6709\u5bb3\u7684\u5185\u5bb9\uff08\u5982\u4e8b\u5b9e\u5e7b\u89c9\u6216\u5371\u9669\u6307\u4ee4\uff09\uff0c\u8fd9\u4e9b\u8bef\u884c\u4e3a\u6e90\u4e8e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff08\u5185\u90e8\u77e5\u8bc6\u51b2\u7a81\u6216\u4fe1\u606f\u7f3a\u5931\uff09\uff0c\u800c\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u4ec5\u6355\u6349\u6574\u4f53\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5bf9\u6b64\u7c7b\u95ee\u9898\u7684\u68c0\u6d4b\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u8bc1\u636e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u5c06\u6a21\u578b\u8f93\u51fa\u5934\u7684\u7279\u5f81\u89e3\u91ca\u4e3a\u652f\u6301\uff08\u6b63\uff09\u6216\u53cd\u5bf9\uff08\u8d1f\uff09\u7684\u8bc1\u636e\uff0c\u5229\u7528\u8bc1\u636e\u7406\u8bba\u5efa\u6a21\u548c\u805a\u5408\u8fd9\u4e9b\u8bc1\u636e\uff0c\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u91cf\u5316\u5185\u90e8\u51b2\u7a81\u548c\u77e5\u8bc6\u7f3a\u53e3\u3002", "result": "\u5728\u56db\u79cd\u8bef\u884c\u4e3a\u7c7b\u522b\uff08\u5e7b\u89c9\u3001\u8d8a\u72f1\u3001\u5bf9\u6297\u6f0f\u6d1e\u3001OOD\u5931\u8d25\uff09\u4e0a\u5bf9\u6700\u5148\u8fdb\u7684LVLM\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0cEUQ\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u53d1\u73b0\u5e7b\u89c9\u5bf9\u5e94\u9ad8\u5185\u90e8\u51b2\u7a81\uff0cOOD\u5931\u8d25\u5bf9\u5e94\u9ad8\u65e0\u77e5\u5ea6\u3002\u5c42\u95f4\u8bc1\u636e\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u5206\u6790\u6709\u52a9\u4e8e\u4ece\u65b0\u89c6\u89d2\u89e3\u91ca\u5185\u90e8\u8868\u793a\u7684\u6f14\u5316\u3002", "conclusion": "EUQ\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7ec6\u7c92\u5ea6\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u68c0\u6d4bLVLM\u7684\u8bef\u884c\u4e3a\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5e76\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u5185\u90e8\u8868\u793a\u7684\u5f62\u6210\u8fc7\u7a0b\u3002", "topic": "agent analysis"}}
{"id": "2602.05548", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05548", "abs": "https://arxiv.org/abs/2602.05548", "authors": ["Zhiqi Yu", "Zhangquan Chen", "Mengting Liu", "Heye Zhang", "Liangqiong Qu"], "title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faA-GRAE\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u4f18\u52bf\u4f30\u8ba1\u89e3\u51b3GRPO\u4e2d\u7684\u63a2\u7d22\u6548\u7387\u548c\u96be\u5ea6\u9002\u5e94\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "RLVR\uff08\u7279\u522b\u662fGRPO\uff09\u5df2\u6210\u4e3a\u6fc0\u53d1LLM\u63a8\u7406\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u4f46\u5176\u5728\u63a2\u7d22\u6548\u7387\u548c\u96be\u5ea6\u9002\u5e94\u65b9\u9762\u5b58\u5728\u74f6\u9888\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u74f6\u9888\u6e90\u4e8e\u7ec4\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\uff08GRAE\uff09\u4e2d\u9690\u542b\u7684\u4f18\u52bf\u5bf9\u79f0\u6027\uff0c\u8fd9\u79cd\u5bf9\u79f0\u6027\u5bfc\u81f4\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1\uff09\u7fa4\u4f53\u5c42\u9762\uff0c\u6b63\u786e\u4e0e\u9519\u8bef\u8f68\u8ff9\u95f4\u7684\u4e25\u683c\u5bf9\u79f0\u6743\u91cd\u963b\u788d\u65b0\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u7684\u63a2\u7d22\uff1b2\uff09\u6837\u672c\u5c42\u9762\uff0c\u7b97\u6cd5\u9690\u542b\u4f18\u5148\u4e2d\u7b49\u96be\u5ea6\u6837\u672c\uff0c\u65e0\u6cd5\u9002\u5e94\u96be\u5ea6\u805a\u7126\u7684\u975e\u5e73\u7a33\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u975e\u5bf9\u79f0GRAE\uff08A-GRAE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u8282\u63a2\u7d22\u6fc0\u52b1\u548c\u6837\u672c\u96be\u5ea6\u805a\u7126\u6765\u89e3\u51b3GRAE\u7684\u5bf9\u79f0\u6027\u95ee\u9898\u3002\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u975e\u5bf9\u79f0\u6291\u5236\u6b63\u786e\u8f68\u8ff9\u7684\u4f18\u52bf\u4ee5\u9f13\u52b1\u5fc5\u8981\u63a2\u7d22\uff1b2\uff09\u91c7\u7528\u8bfe\u7a0b\u5f0f\u5b66\u4e60\u7b56\u7565\uff0c\u521d\u59cb\u4f18\u5148\u7b80\u5355\u6837\u672c\uff0c\u7136\u540e\u9010\u6e10\u8f6c\u5411\u590d\u6742\u6837\u672c\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cA-GRAE\u80fd\u591f\u6301\u7eed\u6539\u8fdbGRPO\u53ca\u5176\u53d8\u4f53\uff0c\u5728LLM\u548cMLLM\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "GRAE\u4e2d\u7684\u4f18\u52bf\u5bf9\u79f0\u6027\u662f\u6b21\u4f18\u7684\uff0c\u901a\u8fc7\u5f15\u5165\u975e\u5bf9\u79f0\u6027\u548c\u8bfe\u7a0b\u5f0f\u96be\u5ea6\u805a\u7126\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u8bbe\u7f6e\u4e2d\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.05656", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05656", "abs": "https://arxiv.org/abs/2602.05656", "authors": ["Igor Santos-Grueiro"], "title": "Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation", "comment": "10 pages. Theoretical analysis of behavioral alignment evaluation", "summary": "Behavioral evaluation is the dominant paradigm for assessing alignment in large language models (LLMs). In practice, alignment is inferred from performance under finite evaluation protocols - benchmarks, red-teaming suites, or automated pipelines - and observed compliance is often treated as evidence of underlying alignment. This inference step, from behavioral evidence to claims about latent alignment properties, is typically implicit and rarely analyzed as an inference problem in its own right.\n  We study this problem formally. We frame alignment evaluation as an identifiability question under partial observability and allow agent behavior to depend on information correlated with the evaluation regime. Within this setting, we introduce the Alignment Verifiability Problem and the notion of Normative Indistinguishability, capturing when distinct latent alignment hypotheses induce identical distributions over all evaluator-accessible signals.\n  Our main result is a negative but sharply delimited identifiability theorem. Under finite behavioral evaluation and evaluation-aware agents, observed behavioral compliance does not uniquely identify latent alignment. That is, even idealized behavioral evaluation cannot, in general, certify alignment as a latent property.\n  We further show that behavioral alignment tests should be interpreted as estimators of indistinguishability classes rather than verifiers of alignment. Passing increasingly stringent tests may reduce the space of compatible hypotheses, but cannot collapse it to a singleton under the stated conditions. This reframes alignment benchmarks as providing upper bounds on observable compliance within a regime, rather than guarantees of underlying alignment.", "AI": {"tldr": "\u884c\u4e3a\u8bc4\u4f30\u65e0\u6cd5\u552f\u4e00\u8bc6\u522b\u6f5c\u5728\u5bf9\u9f50\u5c5e\u6027\uff0c\u5373\u4f7f\u901a\u8fc7\u7406\u60f3\u5316\u884c\u4e3a\u6d4b\u8bd5\u4e5f\u65e0\u6cd5\u9a8c\u8bc1\u6a21\u578b\u7684\u5185\u5728\u5bf9\u9f50\u6027\uff0c\u53ea\u80fd\u4f30\u8ba1\u4e0d\u53ef\u533a\u5206\u6027\u7c7b\u522b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u884c\u4e3a\u6d4b\u8bd5\uff08\u57fa\u51c6\u3001\u7ea2\u961f\u6d4b\u8bd5\u7b49\uff09\uff0c\u5c06\u89c2\u5bdf\u5230\u7684\u5408\u89c4\u884c\u4e3a\u89c6\u4e3a\u5185\u5728\u5bf9\u9f50\u7684\u8bc1\u636e\u3002\u4f46\u8fd9\u79cd\u65b9\u6cd5\u9690\u542b\u5730\u5c06\u884c\u4e3a\u8bc1\u636e\u63a8\u65ad\u4e3a\u6f5c\u5728\u5bf9\u9f50\u5c5e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u8fd9\u79cd\u63a8\u65ad\u8fc7\u7a0b\u672c\u8eab\u7684\u5f62\u5f0f\u5316\u5206\u6790\u3002", "method": "\u5c06\u5bf9\u9f50\u8bc4\u4f30\u5f62\u5f0f\u5316\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u7684\u53ef\u8bc6\u522b\u6027\u95ee\u9898\uff0c\u5141\u8bb8\u667a\u80fd\u4f53\u884c\u4e3a\u4f9d\u8d56\u4e8e\u4e0e\u8bc4\u4f30\u673a\u5236\u76f8\u5173\u7684\u4fe1\u606f\u3002\u5f15\u5165\u5bf9\u9f50\u53ef\u9a8c\u8bc1\u6027\u95ee\u9898\u548c\u89c4\u8303\u6027\u4e0d\u53ef\u533a\u5206\u6027\u6982\u5ff5\uff0c\u5206\u6790\u4e0d\u540c\u6f5c\u5728\u5bf9\u9f50\u5047\u8bbe\u5982\u4f55\u4ea7\u751f\u76f8\u540c\u7684\u8bc4\u4f30\u8005\u53ef\u8bbf\u95ee\u4fe1\u53f7\u5206\u5e03\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\u4e3a\u8d1f\u9762\u7684\u53ef\u8bc6\u522b\u6027\u5b9a\u7406\uff1a\u5728\u6709\u9650\u884c\u4e3a\u8bc4\u4f30\u548c\u8bc4\u4f30\u611f\u77e5\u667a\u80fd\u4f53\u6761\u4ef6\u4e0b\uff0c\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u5408\u89c4\u6027\u4e0d\u80fd\u552f\u4e00\u8bc6\u522b\u6f5c\u5728\u5bf9\u9f50\u6027\u3002\u5373\u4f7f\u7406\u60f3\u5316\u7684\u884c\u4e3a\u8bc4\u4f30\u4e5f\u65e0\u6cd5\u4e00\u822c\u6027\u5730\u9a8c\u8bc1\u5bf9\u9f50\u4f5c\u4e3a\u6f5c\u5728\u5c5e\u6027\u3002\u884c\u4e3a\u5bf9\u9f50\u6d4b\u8bd5\u5e94\u89e3\u91ca\u4e3a\u4e0d\u53ef\u533a\u5206\u6027\u7c7b\u522b\u7684\u4f30\u8ba1\u5668\u800c\u975e\u5bf9\u9f50\u9a8c\u8bc1\u5668\u3002", "conclusion": "\u884c\u4e3a\u5bf9\u9f50\u57fa\u51c6\u5e94\u91cd\u65b0\u7406\u89e3\u4e3a\u5728\u7279\u5b9a\u673a\u5236\u5185\u63d0\u4f9b\u53ef\u89c2\u6d4b\u5408\u89c4\u6027\u7684\u4e0a\u754c\uff0c\u800c\u975e\u5e95\u5c42\u5bf9\u9f50\u7684\u4fdd\u8bc1\u3002\u901a\u8fc7\u66f4\u4e25\u683c\u7684\u6d4b\u8bd5\u53ef\u4ee5\u7f29\u5c0f\u517c\u5bb9\u5047\u8bbe\u7a7a\u95f4\uff0c\u4f46\u5728\u6240\u8ff0\u6761\u4ef6\u4e0b\u65e0\u6cd5\u5c06\u5176\u7f29\u5c0f\u4e3a\u5355\u4e00\u5047\u8bbe\u3002", "topic": "agent analysis"}}
{"id": "2602.05746", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05746", "abs": "https://arxiv.org/abs/2602.05746", "authors": ["Xin Chen", "Jie Zhang", "Florian Tramer"], "title": "Learning to Inject: Automated Prompt Injection via Reinforcement Learning", "comment": null, "summary": "Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.", "AI": {"tldr": "AutoInject\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u901a\u7528\u5bf9\u6297\u540e\u7f00\uff0c\u53ef\u6709\u6548\u653b\u51fb\u524d\u6cbfLLM\u7cfb\u7edf", "motivation": "\u5f53\u524d\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u7ea2\u961f\u548c\u624b\u5de5\u5236\u4f5c\u7684\u63d0\u793a\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u9700\u8981\u81ea\u52a8\u5316\u4f18\u5316\u65b9\u6cd5", "method": "\u63d0\u51faAutoInject\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u653b\u51fb\u6210\u529f\u7387\u548c\u826f\u6027\u4efb\u52a1\u6548\u7528\u4fdd\u6301\uff0c\u652f\u6301\u67e5\u8be2\u4f18\u5316\u548c\u8fc1\u79fb\u653b\u51fb", "result": "\u4ec5\u4f7f\u75281.5B\u53c2\u6570\u7684\u540e\u7f00\u751f\u6210\u5668\uff0c\u6210\u529f\u653b\u51fbGPT 5 Nano\u3001Claude Sonnet 3.5\u3001Gemini 2.5 Flash\u7b49\u524d\u6cbf\u7cfb\u7edf", "conclusion": "\u4e3a\u81ea\u52a8\u5316\u63d0\u793a\u6ce8\u5165\u7814\u7a76\u5efa\u7acb\u4e86\u66f4\u5f3a\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u751f\u6210\u901a\u7528\u5bf9\u6297\u540e\u7f00\u65b9\u9762\u7684\u6709\u6548\u6027", "topic": "agent analysis"}}
{"id": "2602.05776", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05776", "abs": "https://arxiv.org/abs/2602.05776", "authors": ["Mengbei Yan", "Jiafei Lyu", "Shengjie Sun", "Zhongjian Qiao", "Jingwen Yang", "Zichuan Lin", "Deheng Ye", "Xiu Li"], "title": "Cross-Domain Offline Policy Adaptation via Selective Transition Correction", "comment": null, "summary": "It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.", "AI": {"tldr": "\u63d0\u51fa\u9009\u62e9\u6027\u8f6c\u79fb\u6821\u6b63\u7b97\u6cd5\uff0c\u901a\u8fc7\u4fee\u6b63\u6e90\u57df\u6570\u636e\u6765\u9002\u5e94\u76ee\u6807\u57df\u52a8\u6001\uff0c\u89e3\u51b3\u8de8\u57df\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u52a8\u6001\u4e0d\u5339\u914d\u95ee\u9898", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u8de8\u57df\u7b56\u7565\u9002\u5e94\u9762\u4e34\u52a8\u6001\u4e0d\u5339\u914d\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6e90\u57df\u8f6c\u79fb\u8fc7\u6ee4\u6216\u5956\u52b1\u4fee\u6539\u6765\u7f13\u89e3\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u5bf9\u5b9d\u8d35\u6e90\u57df\u6570\u636e\u5229\u7528\u4e0d\u8db3\u3002\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5229\u7528\u6e90\u57df\u6570\u636e\u589e\u5f3a\u76ee\u6807\u57df\u7b56\u7565\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u8f6c\u79fb\u6821\u6b63\u7b97\u6cd5\uff0c\u5229\u7528\u9006\u7b56\u7565\u6a21\u578b\u548c\u5956\u52b1\u6a21\u578b\u4fee\u6b63\u6e90\u57df\u8f6c\u79fb\u7684\u52a8\u4f5c\u548c\u5956\u52b1\uff0c\u4f7f\u5176\u4e0e\u76ee\u6807\u57df\u52a8\u6001\u5bf9\u9f50\u3002\u8fdb\u4e00\u6b65\u4f7f\u7528\u524d\u5411\u52a8\u6001\u6a21\u578b\u4fdd\u7559\u6bd4\u539f\u59cb\u8f6c\u79fb\u66f4\u5339\u914d\u76ee\u6807\u52a8\u6001\u7684\u4fee\u6b63\u6837\u672c\u3002", "result": "\u5728\u5177\u6709\u52a8\u6001\u504f\u79fb\u7684\u5404\u79cd\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0cSTC\u7b97\u6cd5\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4fee\u6b63\u6e90\u57df\u6570\u636e\u800c\u975e\u76f4\u63a5\u5408\u5e76\u6216\u8fc7\u6ee4\uff0cSTC\u80fd\u591f\u53ef\u9760\u5730\u5229\u7528\u6e90\u57df\u6570\u636e\u8fdb\u884c\u7b56\u7565\u9002\u5e94\uff0c\u6709\u6548\u89e3\u51b3\u8de8\u57df\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u52a8\u6001\u4e0d\u5339\u914d\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.05783", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05783", "abs": "https://arxiv.org/abs/2602.05783", "authors": ["Shutong Ding", "Yimiao Zhou", "Ke Hu", "Mokai Pan", "Shan Zhong", "Yanwei Fu", "Jingya Wang", "Ye Shi"], "title": "Distributional Reinforcement Learning with Diffusion Bridge Critics", "comment": null, "summary": "Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6865\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08DBC\uff09\uff0c\u9996\u6b21\u5c06\u6269\u6563\u6865\u6a21\u578b\u7528\u4f5c\u8bc4\u8bba\u5bb6\uff0c\u901a\u8fc7\u76f4\u63a5\u5efa\u6a21Q\u503c\u7684\u9006\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\u6765\u51c6\u786e\u6355\u6349\u4ef7\u503c\u5206\u5e03\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6269\u6563\u7b56\u7565\uff0c\u800c\u5ffd\u7565\u4e86\u6269\u6563\u8bc4\u8bba\u5bb6\u3002\u7531\u4e8e\u7b56\u7565\u4f18\u5316\u672c\u8d28\u4e0a\u4f9d\u8d56\u4e8e\u8bc4\u8bba\u5bb6\uff0c\u51c6\u786e\u7684\u4ef7\u503c\u4f30\u8ba1\u6bd4\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u66f4\u91cd\u8981\u3002\u8003\u8651\u5230\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u7684\u968f\u673a\u6027\uff0c\u8bc4\u8bba\u5bb6\u66f4\u9002\u5408\u7528\u5206\u5e03\u6a21\u578b\u6765\u63cf\u8ff0\u3002", "method": "\u63d0\u51fa\u6269\u6563\u6865\u8bc4\u8bba\u5bb6\uff08DBC\uff09\uff0c\u76f4\u63a5\u5efa\u6a21Q\u503c\u7684\u9006\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\uff0c\u5229\u7528\u6269\u6563\u6865\u7684\u5f3a\u5927\u5206\u5e03\u5339\u914d\u80fd\u529b\u9632\u6b62\u4ef7\u503c\u5206\u5e03\u574d\u7f29\u4e3a\u5e73\u51e1\u9ad8\u65af\u5206\u5e03\u3002\u8fd8\u63a8\u5bfc\u4e86\u89e3\u6790\u79ef\u5206\u516c\u5f0f\u6765\u89e3\u51b3DBC\u4e2d\u7684\u79bb\u6563\u5316\u8bef\u5dee\u95ee\u9898\u3002", "result": "\u5728MuJoCo\u673a\u5668\u4eba\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDBC\u76f8\u6bd4\u4e4b\u524d\u7684\u5206\u5e03\u8bc4\u8bba\u5bb6\u6a21\u578b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002DBC\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u96c6\u6210\u5230\u5927\u591a\u6570\u73b0\u6709RL\u6846\u67b6\u4e2d\u3002", "conclusion": "DBC\u662f\u9996\u4e2a\u5c06\u6269\u6563\u6865\u6a21\u578b\u7528\u4f5c\u8bc4\u8bba\u5bb6\u7684\u5de5\u4f5c\uff0c\u80fd\u591f\u51c6\u786e\u6355\u6349\u4ef7\u503c\u5206\u5e03\uff0c\u5728\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5f88\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.05810", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05810", "abs": "https://arxiv.org/abs/2602.05810", "authors": ["Quan M. Tran", "Zhuo Huang", "Wenbin Zhang", "Bo Han", "Koji Yatani", "Masashi Sugiyama", "Tongliang Liu"], "title": "Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents", "comment": null, "summary": "Autonomous agents excel in self-improvement through reflection and iterative refinement, which reuse successful task trajectories as in-context examples to assist subsequent reasoning. However, shifting across tasks often introduces a context mismatch. Hence, existing approaches either discard the trajectories or manipulate them using heuristics, leading to a non-negligible fine-tuning cost or unguaranteed performance. To bridge this gap, we reveal a context-trajectory correlation, where shifts of context are highly parallel with shifts of trajectory. Based on this finding, we propose BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free method that leverages context differences to precisely guide the adaptation of previously solved trajectories towards the target task, mitigating the misalignment caused by context shifts. Our trajectory adaptation is conducted at the representation level using agent hidden states, ensuring trajectory transformation accurately aligns with the target context in a shared space. Across diverse benchmarks, Bifrost consistently outperforms existing trajectory reuse and finetuned self-improvement methods, demonstrating that agents can effectively leverage past experiences despite substantial context shifts.", "AI": {"tldr": "\u63d0\u51faBifrost\u65b9\u6cd5\uff0c\u901a\u8fc7\u63ed\u793a\u4e0a\u4e0b\u6587-\u8f68\u8ff9\u76f8\u5173\u6027\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u5dee\u5f02\u5f15\u5bfc\u5148\u524d\u89e3\u51b3\u8f68\u8ff9\u9002\u5e94\u76ee\u6807\u4efb\u52a1\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u7f13\u89e3\u4e0a\u4e0b\u6587\u504f\u79fb\u5bfc\u81f4\u7684\u9519\u4f4d\u95ee\u9898\u3002", "motivation": "\u81ea\u4e3b\u4ee3\u7406\u901a\u8fc7\u53cd\u601d\u548c\u8fed\u4ee3\u6539\u8fdb\u8fdb\u884c\u81ea\u6211\u63d0\u5347\uff0c\u91cd\u7528\u6210\u529f\u4efb\u52a1\u8f68\u8ff9\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u793a\u4f8b\u3002\u4f46\u8de8\u4efb\u52a1\u65f6\u5b58\u5728\u4e0a\u4e0b\u6587\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4e22\u5f03\u8f68\u8ff9\uff0c\u8981\u4e48\u4f7f\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u5904\u7406\uff0c\u5bfc\u81f4\u5fae\u8c03\u6210\u672c\u9ad8\u6216\u6027\u80fd\u65e0\u6cd5\u4fdd\u8bc1\u3002", "method": "\u63d0\u51faBifrost\u65b9\u6cd5\uff1a1\uff09\u63ed\u793a\u4e0a\u4e0b\u6587-\u8f68\u8ff9\u76f8\u5173\u6027\uff0c\u53d1\u73b0\u4e0a\u4e0b\u6587\u504f\u79fb\u4e0e\u8f68\u8ff9\u504f\u79fb\u9ad8\u5ea6\u5e73\u884c\uff1b2\uff09\u5229\u7528\u4e0a\u4e0b\u6587\u5dee\u5f02\u7cbe\u786e\u6307\u5bfc\u5148\u524d\u89e3\u51b3\u8f68\u8ff9\u9002\u5e94\u76ee\u6807\u4efb\u52a1\uff1b3\uff09\u5728\u8868\u793a\u5c42\u9762\u4f7f\u7528\u4ee3\u7406\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u8f68\u8ff9\u9002\u5e94\uff0c\u786e\u4fdd\u8f68\u8ff9\u8f6c\u6362\u5728\u5171\u4eab\u7a7a\u95f4\u4e2d\u4e0e\u76ee\u6807\u4e0a\u4e0b\u6587\u51c6\u786e\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBifrost\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u8f68\u8ff9\u91cd\u7528\u548c\u5fae\u8c03\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\uff0c\u8868\u660e\u4ee3\u7406\u80fd\u591f\u6709\u6548\u5229\u7528\u8fc7\u53bb\u7ecf\u9a8c\uff0c\u5373\u4f7f\u5b58\u5728\u663e\u8457\u7684\u4e0a\u4e0b\u6587\u504f\u79fb\u3002", "conclusion": "Bifrost\u901a\u8fc7\u63ed\u793a\u4e0a\u4e0b\u6587-\u8f68\u8ff9\u76f8\u5173\u6027\u5e76\u5229\u7528\u4e0a\u4e0b\u6587\u5dee\u5f02\u6307\u5bfc\u8f68\u8ff9\u9002\u5e94\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u6709\u6548\u8f68\u8ff9\u91cd\u7528\uff0c\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u4ee3\u7406\u7684\u81ea\u6211\u6539\u8fdb\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.05892", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05892", "abs": "https://arxiv.org/abs/2602.05892", "authors": ["Han Li", "Letian Zhu", "Bohan Zhang", "Rili Feng", "Jiaming Wang", "Yue Pan", "Earl T. Barr", "Sarro Federica", "Zhaoyang Chu", "He Ye"], "title": "ContextBench: A Benchmark for Context Retrieval in Coding Agents", "comment": "36 pages, 6 figures, 4 tables", "summary": "LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.", "AI": {"tldr": "ContextBench\u662f\u4e00\u4e2a\u9762\u5411\u8fc7\u7a0b\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ee3\u7801\u4ee3\u7406\u5728\u89e3\u51b3\u95ee\u9898\u65f6\u7684\u4e0a\u4e0b\u6587\u68c0\u7d22\u80fd\u529b\uff0c\u5305\u542b1136\u4e2a\u4efb\u52a1\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u9ec4\u91d1\u4e0a\u4e0b\u6587\uff0c\u63ed\u793a\u4e86\u4ee3\u7406\u5728\u4e0a\u4e0b\u6587\u68c0\u7d22\u4e2d\u7684\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684LLM\u4ee3\u7801\u4ee3\u7406\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u4efb\u52a1\u6210\u529f\u7387\uff0c\u7f3a\u4e4f\u5bf9\u4ee3\u7406\u5728\u89e3\u51b3\u95ee\u9898\u8fc7\u7a0b\u4e2d\u5982\u4f55\u68c0\u7d22\u548c\u4f7f\u7528\u4ee3\u7801\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u3002\u9700\u8981\u4e00\u79cd\u8fc7\u7a0b\u5bfc\u5411\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u6df1\u5165\u5206\u6790\u4e0a\u4e0b\u6587\u68c0\u7d22\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u6784\u5efaContextBench\u57fa\u51c6\uff0c\u5305\u542b1136\u4e2a\u95ee\u9898\u89e3\u51b3\u4efb\u52a1\uff0c\u8986\u76d666\u4e2a\u4ed3\u5e93\u548c8\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6709\u4eba\u5de5\u6807\u6ce8\u7684\u9ec4\u91d1\u4e0a\u4e0b\u6587\u3002\u5b9e\u73b0\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u8ddf\u8e2a\u4ee3\u7406\u6267\u884c\u8f68\u8ff9\uff0c\u6d4b\u91cf\u4e0a\u4e0b\u6587\u53ec\u56de\u7387\u3001\u7cbe\u786e\u7387\u548c\u6548\u7387\u3002", "result": "\u8bc4\u4f304\u4e2a\u524d\u6cbfLLM\u548c5\u4e2a\u4ee3\u7801\u4ee3\u7406\uff0c\u53d1\u73b0\uff1a1) \u590d\u6742\u7684\u4ee3\u7406\u6846\u67b6\u5728\u4e0a\u4e0b\u6587\u68c0\u7d22\u4e0a\u53ea\u6709\u8fb9\u9645\u6536\u76ca\uff1b2) LLM\u666e\u904d\u504f\u5411\u53ec\u56de\u7387\u800c\u975e\u7cbe\u786e\u7387\uff1b3) \u63a2\u7d22\u7684\u4e0a\u4e0b\u6587\u4e0e\u5b9e\u9645\u4f7f\u7528\u7684\u4e0a\u4e0b\u6587\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "ContextBench\u901a\u8fc7\u4e2d\u95f4\u9ec4\u91d1\u4e0a\u4e0b\u6587\u6307\u6807\u8865\u5145\u4e86\u73b0\u6709\u7684\u7aef\u5230\u7aef\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u7406\u89e3\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u4e2d\u95f4\u4fe1\u53f7\uff0c\u53ef\u7528\u4e8e\u6307\u5bfcLLM\u5728\u8f6f\u4ef6\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u3002", "topic": "agent analysis"}}
{"id": "2602.05910", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05910", "abs": "https://arxiv.org/abs/2602.05910", "authors": ["Seoirse Murray", "Allison Qi", "Timothy Qian", "John Schulman", "Collin Burns", "Sara Price"], "title": "Chunky Post-Training: Data Driven Failures of Generalization", "comment": null, "summary": "LLM post-training involves many diverse datasets, each targeting a specific behavior. But these datasets encode incidental patterns alongside intended ones: correlations between formatting and content, narrow phrasings across diverse problems, and implicit associations arising from the discrete data curation process. These patterns are often invisible to developers yet salient to models, producing behaviors that surprise their creators, such as rejecting true facts presented in a particular question format. We call this chunky post-training: the model learns spurious correlations as a result of distinct chunks of post-training data. We introduce SURF, a black-box pipeline which surfaces these unintended behaviors at run time, and TURF, a tool that traces these failures back to specific post-training data. Applying these tools to frontier models (Claude 4.5, GPT-5.1, Grok 4.1, Gemini 3) and open models (T\u00fclu 3), we show that chunky post-training produces miscalibrated behaviors, which often result from imbalanced or underspecified chunks of post-training data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSURF\u548cTURF\u5de5\u5177\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u8ffd\u8e2a\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u56e0\u6570\u636e\u5757\u5bfc\u81f4\u7684\u865a\u5047\u76f8\u5173\u6027\u884c\u4e3a\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u4f7f\u7528\u591a\u79cd\u6570\u636e\u96c6\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u96c6\u7f16\u7801\u4e86\u610f\u5916\u6a21\u5f0f\uff08\u5982\u683c\u5f0f\u4e0e\u5185\u5bb9\u7684\u76f8\u5173\u6027\u3001\u72ed\u7a84\u7684\u63aa\u8f9e\u7b49\uff09\uff0c\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u5230\u865a\u5047\u76f8\u5173\u6027\uff0c\u4ea7\u751f\u8ba9\u5f00\u53d1\u8005\u60ca\u8bb6\u7684\u884c\u4e3a\u3002", "method": "\u63d0\u51faSURF\uff08\u8fd0\u884c\u65f6\u68c0\u6d4b\u610f\u5916\u884c\u4e3a\u7684\u9ed1\u76d2\u7ba1\u9053\uff09\u548cTURF\uff08\u5c06\u5931\u8d25\u884c\u4e3a\u8ffd\u6eaf\u5230\u7279\u5b9a\u540e\u8bad\u7ec3\u6570\u636e\u7684\u5de5\u5177\uff09\uff0c\u5e94\u7528\u4e8e\u524d\u6cbf\u6a21\u578b\u548c\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5e94\u7528\u8fd9\u4e9b\u5de5\u5177\u53d1\u73b0\uff0c\u5757\u72b6\u540e\u8bad\u7ec3\u5bfc\u81f4\u6821\u51c6\u9519\u8bef\u7684\u884c\u4e3a\uff0c\u8fd9\u4e9b\u884c\u4e3a\u901a\u5e38\u6e90\u4e8e\u4e0d\u5e73\u8861\u6216\u672a\u5145\u5206\u6307\u5b9a\u7684\u540e\u8bad\u7ec3\u6570\u636e\u5757\u3002", "conclusion": "\u540e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u5757\u72b6\u6a21\u5f0f\u4f1a\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u865a\u5047\u76f8\u5173\u6027\uff0c\u9700\u8981\u5de5\u5177\u6765\u68c0\u6d4b\u548c\u8ffd\u8e2a\u8fd9\u4e9b\u95ee\u9898\u4ee5\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.05933", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05933", "abs": "https://arxiv.org/abs/2602.05933", "authors": ["Zhenghao Xu", "Qin Lu", "Changlong Yu", "Tuo Zhao"], "title": "Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training", "comment": null, "summary": "Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$\u03c7^2$ regularizer. This additional $\u03c7^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPMD-mean\u7b97\u6cd5\uff0c\u901a\u8fc7\u7528\u91c7\u6837\u7b56\u7565\u7684\u5e73\u5747\u5956\u52b1\u8fd1\u4f3c\u5bf9\u6570\u914d\u5206\u51fd\u6570\uff0c\u5728LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u7b56\u7565\u4f18\u5316\u3002", "motivation": "\u7b56\u7565\u955c\u50cf\u4e0b\u964d(PMD)\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u4f46\u5728LLM\u7684\u5927\u52a8\u4f5c\u7a7a\u95f4\u4e2d\uff0c\u7cbe\u786e\u4f30\u8ba1\u914d\u5206\u51fd\u6570\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6709\u9650\u6837\u672c\u60c5\u51b5\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u8fd1\u4f3c\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faPMD-mean\u7b97\u6cd5\uff0c\u7528\u91c7\u6837\u7b56\u7565\u7684\u5e73\u5747\u5956\u52b1\u8fd1\u4f3c\u5bf9\u6570\u914d\u5206\u51fd\u6570\uff0c\u5728log-policy\u7a7a\u95f4\u8fdb\u884c\u56de\u5f52\u3002\u8be5\u65b9\u6cd5\u9690\u5f0f\u4f18\u5316\u4e86\u5e26\u6709\u81ea\u9002\u5e94\u6df7\u5408KL-\u03c7\u00b2\u6b63\u5219\u5316\u7684\u955c\u50cf\u4e0b\u964d\u5b50\u95ee\u9898\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPMD-mean\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u5177\u6709\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u65f6\u95f4\u6548\u7387\u3002\u989d\u5916\u7684\u03c7\u00b2\u6b63\u5219\u5316\u7ea6\u675f\u4e86\u6982\u7387\u7684\u5927\u5e45\u53d8\u5316\uff0c\u5728\u671f\u671b\u5956\u52b1\u8f83\u4f4e\u65f6\u4ea7\u751f\u66f4\u4fdd\u5b88\u7684\u66f4\u65b0\u3002", "conclusion": "PMD-mean\u4e3aLLM\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6539\u8fdb\u9014\u5f84\uff0c\u52a0\u6df1\u4e86\u5bf9\u8be5\u65b9\u6cd5\u7684\u7406\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f18\u52bf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.05970", "categories": ["cs.LG", "cs.AI", "math.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.05970", "abs": "https://arxiv.org/abs/2602.05970", "authors": ["Yizhou Liu", "Sara Kangaslahti", "Ziming Liu", "Jeff Gore"], "title": "Inverse Depth Scaling From Most Layers Being Similar", "comment": "23 pages, 24 figures", "summary": "Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u635f\u5931\u4e0e\u6df1\u5ea6\u6210\u53cd\u6bd4\u7f29\u653e\uff0c\u8fd9\u79cd\u4f4e\u6548\u4f46\u7a33\u5065\u7684\u673a\u5236\u6e90\u4e8e\u6b8b\u5dee\u7f51\u7edc\u67b6\u6784\u504f\u5dee\u548c\u76ee\u6807\u51fd\u6570\u4e0d\u517c\u5bb9\u5e73\u6ed1\u52a8\u6001\uff0c\u800c\u975e\u7ec4\u5408\u5b66\u4e60\u6216\u79bb\u6563\u5316\u5e73\u6ed1\u52a8\u6001\u3002", "motivation": "\u867d\u7136\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u63cf\u8ff0\u4e86\u635f\u5931\u4e0e\u6a21\u578b\u89c4\u6a21\u7684\u5173\u7cfb\uff0c\u4f46\u6df1\u5ea6\u548c\u5bbd\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u53ef\u80fd\u4e0d\u540c\uff0c\u9700\u8981\u66f4\u8be6\u7ec6\u7684\u7814\u7a76\u6765\u91cf\u5316\u6df1\u5ea6\u5982\u4f55\u5f71\u54cd\u635f\u5931\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u548c\u73a9\u5177\u6b8b\u5dee\u7f51\u7edc\uff0c\u91cf\u5316\u6df1\u5ea6\u5bf9\u635f\u5931\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u635f\u5931\u4e0e\u6df1\u5ea6\u7684\u7f29\u653e\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u635f\u5931\u4e0e\u6df1\u5ea6\u6210\u53cd\u6bd4\u7f29\u653e\uff0c\u8fd9\u79cd\u673a\u5236\u6e90\u4e8e\u529f\u80fd\u76f8\u4f3c\u5c42\u7684\u96c6\u6210\u5e73\u5747\uff0c\u800c\u975e\u7ec4\u5408\u5b66\u4e60\u6216\u79bb\u6563\u5316\u5e73\u6ed1\u52a8\u6001\u3002", "conclusion": "\u5f53\u524d\u6df1\u5ea6\u5229\u7528\u65b9\u5f0f\u4f4e\u6548\u4f46\u7a33\u5065\uff0c\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u6548\u7387\u53ef\u80fd\u9700\u8981\u67b6\u6784\u521b\u65b0\u6765\u9f13\u52b1\u6df1\u5ea6\u7684\u7ec4\u5408\u4f7f\u7528\u3002", "topic": "agent analysis"}}
{"id": "2602.05946", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.05946", "abs": "https://arxiv.org/abs/2602.05946", "authors": ["Rajdeep Haldar", "Lantao Mei", "Guang Lin", "Yue Xing", "Qifan Song"], "title": "$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment", "comment": null, "summary": "Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ef-\u6563\u5ea6\u7684\u7edf\u4e00\u5bf9\u9f50\u6846\u67b6f-GRPO\u548cf-HAL\uff0c\u5728RLVR\u548c\u504f\u597d\u5bf9\u9f50\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u53ef\u89c6\u4e3a\u5bf9\u9f50\u4e0e\u672a\u5bf9\u9f50\u54cd\u5e94\u5206\u5e03\u95f4\u7684\u6563\u5ea6\u4f30\u8ba1\u5668\uff0c\u672c\u7814\u7a76\u5c06\u6b64\u6563\u5ea6\u89c6\u89d2\u6269\u5c55\u5230\u4e00\u822c\u5bf9\u9f50\u8bbe\u7f6e\uff08\u5982\u4ec5\u6709\u73af\u5883\u5956\u52b1\u7684RLVR\uff09\uff0c\u5efa\u7acb\u7edf\u4e00\u6846\u67b6", "method": "\u57fa\u4e8ef-\u6563\u5ea6\u7684\u53d8\u5206\u8868\u793a\uff0c\u63d0\u51faf-GRPO\uff08\u5728\u7ebf\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff09\u548cf-HAL\uff08\u6df7\u5408\u5728\u7ebf/\u79bb\u7ebf\u7b56\u7565\u76ee\u6807\uff09\u4e24\u7c7b\u5bf9\u9f50\u76ee\u6807", "result": "\u7406\u8bba\u4fdd\u8bc1\u5bf9\u9f50\u540e\u5e73\u5747\u5956\u52b1\u63d0\u5347\uff0c\u5b9e\u8bc1\u5728RLVR\uff08\u6570\u5b66\u63a8\u7406\uff09\u548cPA\uff08\u5b89\u5168\u5bf9\u9f50\uff09\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u7075\u6d3b\u6027", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8ef-\u6563\u5ea6\u7684\u7edf\u4e00\u5bf9\u9f50\u6846\u67b6\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u79cd\u5bf9\u9f50\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02", "topic": "agentic reinforcement learning"}}
{"id": "2602.05999", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05999", "abs": "https://arxiv.org/abs/2602.05999", "authors": ["Raj Ghugare", "Micha\u0142 Bortkiewicz", "Alicja Ziarko", "Benjamin Eysenbach"], "title": "On Computation and Reinforcement Learning", "comment": null, "summary": "How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8ba1\u7b97\u8d44\u6e90\u5bf9\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u8bc1\u660e\u4e86\u589e\u52a0\u8ba1\u7b97\u91cf\u53ef\u4ee5\u89e3\u51b3\u66f4\u591a\u95ee\u9898\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u7075\u6d3b\u4f7f\u7528\u4e0d\u540c\u8ba1\u7b97\u91cf\u7684\u67b6\u6784\uff0c\u5e76\u572831\u4e2a\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7f3a\u4e4f\u5f62\u5f0f\u5316\u63cf\u8ff0\u8ba1\u7b97\u8d44\u6e90\u5bf9\u7b56\u7565\u5f71\u54cd\u7684\u8bed\u8a00\uff0c\u4e14\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u67b6\u6784\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u6df7\u6dc6\u4e86\u8ba1\u7b97\u91cf\u548c\u53c2\u6570\u6570\u91cf\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u8ba1\u7b97\u8d44\u6e90\u5982\u4f55\u5f71\u54cd\u7b56\u7565\u5b66\u4e60\uff0c\u4ee5\u53ca\u56fa\u5b9a\u53c2\u6570\u91cf\u7684\u7b56\u7565\u662f\u5426\u80fd\u4ece\u989d\u5916\u8ba1\u7b97\u4e2d\u53d7\u76ca\u3002", "method": "\u57fa\u4e8e\u7b97\u6cd5\u5b66\u4e60\u548c\u65e0\u6a21\u578b\u89c4\u5212\u7684\u524d\u671f\u5de5\u4f5c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u4f7f\u7528\u53ef\u53d8\u8ba1\u7b97\u91cf\u7684\u6700\u5c0f\u5316\u67b6\u6784\u3002\u8be5\u67b6\u6784\u5141\u8bb8\u7b56\u7565\u6839\u636e\u53ef\u7528\u8ba1\u7b97\u8d44\u6e90\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u91cf\uff0c\u800c\u4e0d\u6539\u53d8\u53c2\u6570\u6570\u91cf\u3002", "result": "\u572831\u4e2a\u4e0d\u540c\u7684\u5728\u7ebf\u548c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a(1) \u8be5\u67b6\u6784\u4ec5\u901a\u8fc7\u4f7f\u7528\u66f4\u591a\u8ba1\u7b97\u5c31\u80fd\u83b7\u5f97\u66f4\u5f3a\u7684\u6027\u80fd\uff1b(2) \u76f8\u6bd4\u4f7f\u7528\u591a\u8fbe5\u500d\u53c2\u6570\u7684\u666e\u901a\u524d\u9988\u7f51\u7edc\u6216\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\uff0c\u8be5\u67b6\u6784\u5728\u957f\u65f6\u57df\u6d4b\u8bd5\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8ba1\u7b97\u8d44\u6e90\u5bf9\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u589e\u52a0\u8ba1\u7b97\u91cf\u53ef\u4ee5\u89e3\u51b3\u66f4\u591a\u95ee\u9898\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002\u63d0\u51fa\u7684\u53ef\u53d8\u8ba1\u7b97\u91cf\u67b6\u6784\u80fd\u591f\u6709\u6548\u5229\u7528\u989d\u5916\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6570\u91cf\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2602.327c90fe", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fapple-xcode-claude-agent-sdk%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/9zqlb2z9dmc5zr39kv7cQARCGFj_kGT6vKJK9D2bc0g=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fapple-xcode-claude-agent-sdk%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/9zqlb2z9dmc5zr39kv7cQARCGFj_kGT6vKJK9D2bc0g=443", "authors": ["TLDR Newsletter"], "title": "Claude in Xcode", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fapple-xcode-claude-agent-sdk%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/9zqlb2z9dmc5zr39kv7cQARCGFj_kGT6vKJK9D2bc0g=443", "summary": "Claude in Xcode (1 minute read) Xcode 26.3 introduces native support for Claude Agent SDK, enabling full agentic capabilities like subagents, background tasks, and plugins within Apple's IDE. This brings Claude Code's functionality directly into developers' workflows.", "source": "tldr", "AI": {"tldr": "Xcode 26.3\u96c6\u6210Claude Agent SDK\uff0c\u5728\u82f9\u679cIDE\u4e2d\u63d0\u4f9b\u539f\u751f\u4ee3\u7406\u80fd\u529b\u652f\u6301", "motivation": "\u5c06Claude Code\u7684\u4ee3\u7406\u529f\u80fd\u76f4\u63a5\u96c6\u6210\u5230\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6d41\u4e2d\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387", "method": "\u5728Xcode 26.3\u4e2d\u5f15\u5165\u5bf9Claude Agent SDK\u7684\u539f\u751f\u652f\u6301", "result": "\u5f00\u53d1\u8005\u53ef\u4ee5\u5728Xcode\u4e2d\u4f7f\u7528\u5b50\u4ee3\u7406\u3001\u540e\u53f0\u4efb\u52a1\u548c\u63d2\u4ef6\u7b49\u5b8c\u6574\u4ee3\u7406\u529f\u80fd", "conclusion": "Claude\u7684\u4ee3\u7406\u80fd\u529b\u73b0\u5728\u53ef\u4ee5\u76f4\u63a5\u5728\u82f9\u679c\u5f00\u53d1\u73af\u5883\u4e2d\u4f7f\u7528", "topic": "code agent"}}
{"id": "tldr.2602.dc08a3ed", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zolkos.com%2F2026%2F02%2F04%2Fdeep-dive-how-claude-codes-insights-command-works.html%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/z657i3rSoDFNfJKHnX11g9amNJ_7rP4JafsqvwQ5UcU=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zolkos.com%2F2026%2F02%2F04%2Fdeep-dive-how-claude-codes-insights-command-works.html%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/z657i3rSoDFNfJKHnX11g9amNJ_7rP4JafsqvwQ5UcU=443", "authors": ["TLDR Newsletter"], "title": "Deep Dive: How Claude Code's /insights Command Works", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: 22 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zolkos.com%2F2026%2F02%2F04%2Fdeep-dive-how-claude-codes-insights-command-works.html%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/z657i3rSoDFNfJKHnX11g9amNJ_7rP4JafsqvwQ5UcU=443", "summary": "Deep Dive: How Claude Code's /insights Command Works (22 minute read) The /insights command in Claude Code generates an HTML report that analyzes usage patterns across a user's Claude Code sessions. The tool is designed to help users understand how they interact with Claude, what's working well, where friction occurs, and how to improve workflows. This post takes a look at how the tool works. For better insights, use Claude Code regularly, give feedback, don't filter yourself, and check in mo...", "source": "tldr", "AI": {"tldr": "Claude Code\u7684/insights\u547d\u4ee4\u751f\u6210HTML\u62a5\u544a\u5206\u6790\u7528\u6237\u4f7f\u7528\u6a21\u5f0f\uff0c\u5e2e\u52a9\u7406\u89e3\u4e0eClaude\u7684\u4ea4\u4e92\u60c5\u51b5\u3001\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u4f18\u70b9\u548c\u6469\u64e6\u70b9", "motivation": "\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u5730\u7406\u89e3\u4ed6\u4eec\u5982\u4f55\u4f7f\u7528Claude Code\uff0c\u8bc6\u522b\u54ea\u4e9b\u5de5\u4f5c\u6d41\u7a0b\u6709\u6548\uff0c\u54ea\u4e9b\u5730\u65b9\u5b58\u5728\u6469\u64e6\uff0c\u4ece\u800c\u6539\u8fdb\u4e0eAI\u4ee3\u7801\u52a9\u624b\u7684\u4ea4\u4e92\u6548\u7387", "method": "\u901a\u8fc7/insights\u547d\u4ee4\u751f\u6210HTML\u62a5\u544a\uff0c\u5206\u6790\u7528\u6237\u5728\u4e0d\u540cClaude Code\u4f1a\u8bdd\u4e2d\u7684\u4f7f\u7528\u6a21\u5f0f\u3001\u4ea4\u4e92\u6570\u636e\u548c\u884c\u4e3a\u7279\u5f81", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5de5\u5177\u8ba9\u7528\u6237\u80fd\u591f\u53ef\u89c6\u5316\u5206\u6790\u81ea\u5df1\u7684\u4f7f\u7528\u4e60\u60ef\uff0c\u8bc6\u522b\u9ad8\u6548\u5de5\u4f5c\u6a21\u5f0f\u548c\u6f5c\u5728\u6539\u8fdb\u70b9", "conclusion": "/insights\u547d\u4ee4\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5206\u6790\u5de5\u5177\uff0c\u901a\u8fc7\u5b9a\u671f\u4f7f\u7528Claude Code\u3001\u63d0\u4f9b\u53cd\u9988\u3001\u4e0d\u81ea\u6211\u8fc7\u6ee4\u548c\u5b9a\u671f\u68c0\u67e5\uff0c\u53ef\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6d1e\u5bdf", "topic": "code agent"}}
{"id": "tldr.2602.03f27b1e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.exe.dev%2Fexpensively-quadratic%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/0ovPGZQNwIw4qnrEpiNckdTfaZWR-H12nbpSnukpLCk=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.exe.dev%2Fexpensively-quadratic%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/0ovPGZQNwIw4qnrEpiNckdTfaZWR-H12nbpSnukpLCk=443", "authors": ["TLDR Newsletter"], "title": "Expensively Quadratic: the LLM Agent Cost Curve", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.exe.dev%2Fexpensively-quadratic%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/0ovPGZQNwIw4qnrEpiNckdTfaZWR-H12nbpSnukpLCk=443", "summary": "Expensively Quadratic: the LLM Agent Cost Curve (6 minute read) It doesn't take much to start spending a significant amount of tokens on context. This problem compounds as conversations get longer and only gets exponentially more important with agent and subagent workflows. Agent developers need to keep this problem in mind when developing AI tools. This post provides an interactive tool that shows how quickly context size can take up a large percentage of a cache read.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86LLM\u667a\u80fd\u4f53\u6210\u672c\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u5448\u4e8c\u6b21\u65b9\u589e\u957f\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4ea4\u4e92\u5de5\u5177\u5c55\u793a\u4e0a\u4e0b\u6587\u5927\u5c0f\u5982\u4f55\u5feb\u901f\u5360\u636e\u7f13\u5b58\u8bfb\u53d6\u7684\u5f88\u5927\u6bd4\u4f8b", "motivation": "\u968f\u7740\u5bf9\u8bdd\u53d8\u957f\uff0cLLM\u667a\u80fd\u4f53\u7684\u4e0a\u4e0b\u6587\u6210\u672c\u4f1a\u663e\u8457\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4f53\u548c\u5b50\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\uff0c\u8fd9\u4e2a\u95ee\u9898\u4f1a\u6307\u6570\u7ea7\u6076\u5316\u3002\u667a\u80fd\u4f53\u5f00\u53d1\u8005\u9700\u8981\u610f\u8bc6\u5230\u8fd9\u4e2a\u6210\u672c\u95ee\u9898", "method": "\u63d0\u4f9b\u4e86\u4ea4\u4e92\u5f0f\u5de5\u5177\u6765\u53ef\u89c6\u5316\u4e0a\u4e0b\u6587\u5927\u5c0f\u5982\u4f55\u5feb\u901f\u5360\u636e\u7f13\u5b58\u8bfb\u53d6\u7684\u5f88\u5927\u6bd4\u4f8b\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u6210\u672c\u589e\u957f\u6a21\u5f0f", "result": "\u5c55\u793a\u4e86LLM\u667a\u80fd\u4f53\u6210\u672c\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u5448\u4e8c\u6b21\u65b9\u589e\u957f\u7684\u7279\u6027\uff0c\u4e0a\u4e0b\u6587\u5927\u5c0f\u4f1a\u5feb\u901f\u5360\u636e\u7f13\u5b58\u8bfb\u53d6\u7684\u5f88\u5927\u6bd4\u4f8b", "conclusion": "\u667a\u80fd\u4f53\u5f00\u53d1\u8005\u9700\u8981\u5bc6\u5207\u5173\u6ce8\u4e0a\u4e0b\u6587\u6210\u672c\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8bbe\u8ba1\u957f\u5bf9\u8bdd\u548c\u590d\u6742\u5de5\u4f5c\u6d41\u65f6\uff0c\u6210\u672c\u63a7\u5236\u81f3\u5173\u91cd\u8981", "topic": "agent analysis"}}
{"id": "tldr.2602.99a2555d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qawolf.com%2Fplatform%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=ACQ_All_Waitlist__NewsletterAudience_-_Newsletter_AIWaitlist_20260204-None_Experiment-FALSE%26utm_term=headline-NowAvailableForQATeamsAIPlatformForDeepCoverageFromQAWolf%26utm_content=AIWaitlist_GetEarlyAccess_None_Headline%253ANowAvailableForQATeamsAIPlatformForDeepCoverageFromQAWolf____Newsletter-SecondaryPlacement_20260204_v1_/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/NxqaMuWJZwL9W2EiK8fGkbIys2u69qJnz8aHfuwbbdk=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qawolf.com%2Fplatform%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=ACQ_All_Waitlist__NewsletterAudience_-_Newsletter_AIWaitlist_20260204-None_Experiment-FALSE%26utm_term=headline-NowAvailableForQATeamsAIPlatformForDeepCoverageFromQAWolf%26utm_content=AIWaitlist_GetEarlyAccess_None_Headline%253ANowAvailableForQATeamsAIPlatformForDeepCoverageFromQAWolf____Newsletter-SecondaryPlacement_20260204_v1_/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/NxqaMuWJZwL9W2EiK8fGkbIys2u69qJnz8aHfuwbbdk=443", "authors": ["TLDR Newsletter"], "title": "Own your tests:", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qawolf.com%2Fplatform%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=ACQ_All_Waitlist__NewsletterAudience_-_Newsletter_AIWaitlist_20260204-None_Experiment-FALSE%26utm_term=headline-NowAvailableForQATeamsAIPlatformForDeepCoverageFromQAWolf%26utm_content=AIWaitlist_GetEarlyAccess_None_Headline%253ANowAvailableForQATeamsAIPlatformForDeepCoverageFromQAWolf____Newsletter-SecondaryPlacement_20260204_v1_/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/NxqaMuWJZwL9W2EiK8fGkbIys2u69qJnz8aHfuwbbdk=443", "summary": "Now available for QA teams: AI platform for deep coverage from QA Wolf (Sponsor) QA Wolf's new AI assistant helps teams build and maintain automated test coverage for 80%+ of their product workflows\u2014just by chatting with the AI. Test complex workflows reliably: Prompts generate Playwright and Appium code instead of flaky plain-English steps. Run regressions in minutes: Full suites execute 100% in parallel. Own your tests: Open-source code, no vendor lock-in. Get early access", "source": "tldr", "AI": {"tldr": "QA Wolf\u63a8\u51faAI\u52a9\u624b\u5e73\u53f0\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5e2e\u52a9\u56e2\u961f\u4e3a80%+\u4ea7\u54c1\u5de5\u4f5c\u6d41\u5efa\u7acb\u548c\u7ef4\u62a4\u81ea\u52a8\u5316\u6d4b\u8bd5\u8986\u76d6\uff0c\u751f\u6210\u53ef\u9760\u7684Playwright\u548cAppium\u4ee3\u7801\u800c\u975e\u6613\u9519\u7684\u82f1\u6587\u6b65\u9aa4\uff0c\u652f\u6301\u5e76\u884c\u6267\u884c\u5b8c\u6574\u6d4b\u8bd5\u5957\u4ef6\uff0c\u63d0\u4f9b\u5f00\u6e90\u4ee3\u7801\u907f\u514d\u4f9b\u5e94\u5546\u9501\u5b9a\u3002", "motivation": "\u4f20\u7edfQA\u6d4b\u8bd5\u9762\u4e34\u6d4b\u8bd5\u8986\u76d6\u4e0d\u8db3\u3001\u6d4b\u8bd5\u4ee3\u7801\u7f16\u5199\u590d\u6742\u3001\u6267\u884c\u901f\u5ea6\u6162\u3001\u4f9b\u5e94\u5546\u9501\u5b9a\u7b49\u95ee\u9898\u3002QA\u56e2\u961f\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u9760\u4e14\u81ea\u4e3b\u53ef\u63a7\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1AI\u52a9\u624b\u5e73\u53f0\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u751f\u6210Playwright\u548cAppium\u6d4b\u8bd5\u4ee3\u7801\uff0c\u800c\u975e\u6613\u9519\u7684\u7eaf\u82f1\u6587\u6b65\u9aa4\u3002\u652f\u6301\u5e76\u884c\u6267\u884c\u5b8c\u6574\u6d4b\u8bd5\u5957\u4ef6\uff0c\u63d0\u4f9b\u5f00\u6e90\u4ee3\u7801\u907f\u514d\u4f9b\u5e94\u5546\u9501\u5b9a\u3002", "result": "\u5e73\u53f0\u80fd\u591f\u4e3a80%+\u4ea7\u54c1\u5de5\u4f5c\u6d41\u5efa\u7acb\u81ea\u52a8\u5316\u6d4b\u8bd5\u8986\u76d6\uff0c\u6d4b\u8bd5\u4ee3\u7801\u66f4\u53ef\u9760\uff0c\u5b8c\u6574\u6d4b\u8bd5\u5957\u4ef6\u53ef\u5728\u51e0\u5206\u949f\u5185\u5e76\u884c\u6267\u884c\uff0c\u56e2\u961f\u62e5\u6709\u5f00\u6e90\u4ee3\u7801\u63a7\u5236\u6743\u3002", "conclusion": "AI\u9a71\u52a8\u7684QA\u6d4b\u8bd5\u5e73\u53f0\u80fd\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u6548\u7387\u3001\u8986\u76d6\u7387\u548c\u53ef\u9760\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u5f00\u6e90\u4ee3\u7801\u786e\u4fdd\u56e2\u961f\u81ea\u4e3b\u63a7\u5236\u6743\uff0c\u662fQA\u81ea\u52a8\u5316\u6d4b\u8bd5\u7684\u6709\u524d\u666f\u65b9\u5411\u3002", "topic": "swe application"}}
{"id": "tldr.2602.44cfc949", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fqwen.ai%2Fblog%3Fid=qwen3-coder-next%26utm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/nvJqz_SvI2xnFdyTozzYMt0FAhojamUXxEAt8ecXU7M=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fqwen.ai%2Fblog%3Fid=qwen3-coder-next%26utm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/nvJqz_SvI2xnFdyTozzYMt0FAhojamUXxEAt8ecXU7M=443", "authors": ["TLDR Newsletter"], "title": "Qwen3-Coder-Next for Agentic Coding", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fqwen.ai%2Fblog%3Fid=qwen3-coder-next%26utm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/nvJqz_SvI2xnFdyTozzYMt0FAhojamUXxEAt8ecXU7M=443", "summary": "Qwen3-Coder-Next for Agentic Coding (5 minute read) Alibaba's Qwen3-Coder-Next is a new open-weight model fine-tuned for coding agents. Built on a hybrid MoE architecture, it excels in executable synthesis and RL-based environment interaction, achieving strong agentic coding performance at lower inference cost.", "source": "tldr", "AI": {"tldr": "\u963f\u91cc\u63a8\u51faQwen3-Coder-Next\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df7\u5408MoE\u67b6\u6784\u7684\u5f00\u6e90\u6743\u91cd\u6a21\u578b\uff0c\u4e13\u4e3a\u4ee3\u7801\u4ee3\u7406\u4efb\u52a1\u5fae\u8c03\uff0c\u5728\u53ef\u6267\u884c\u4ee3\u7801\u5408\u6210\u548c\u57fa\u4e8eRL\u7684\u73af\u5883\u4ea4\u4e92\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u4ee5\u8f83\u4f4e\u63a8\u7406\u6210\u672c\u5b9e\u73b0\u5f3a\u5927\u7684\u4ee3\u7406\u7f16\u7801\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u4e13\u95e8\u4e3a\u4ee3\u7801\u4ee3\u7406\u4efb\u52a1\u4f18\u5316\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u80fd\u529b\uff0c\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "method": "\u57fa\u4e8e\u6df7\u5408MoE\u67b6\u6784\u6784\u5efa\uff0c\u901a\u8fc7\u5fae\u8c03\u4e13\u95e8\u9488\u5bf9\u4ee3\u7801\u4ee3\u7406\u4efb\u52a1\uff0c\u4e13\u6ce8\u4e8e\u53ef\u6267\u884c\u4ee3\u7801\u5408\u6210\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u73af\u5883\u4ea4\u4e92\u80fd\u529b\u3002", "result": "\u5728\u4ee3\u7406\u7f16\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u63a8\u7406\u6210\u672c\u3002", "conclusion": "Qwen3-Coder-Next\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5f00\u6e90\u4ee3\u7801\u4ee3\u7406\u6a21\u578b\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "topic": "code agent"}}
{"id": "tldr.2602.4ceeaeaa", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.02361%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/JolGWS3kQ1jjoLn4xN2d8mZgftVmwNBlrzuX7ExbmgA=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.02361%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/JolGWS3kQ1jjoLn4xN2d8mZgftVmwNBlrzuX7ExbmgA=443", "authors": ["TLDR Newsletter"], "title": "800K+ Verifiable SWE Tasks", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.02361%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/JolGWS3kQ1jjoLn4xN2d8mZgftVmwNBlrzuX7ExbmgA=443", "summary": "800K+ Verifiable SWE Tasks (18 minute read) SWE-Universe presents a scalable method for generating verifiable software engineering environments from GitHub PRs. With in-loop hacking detection and self-verification, the system enables large-scale mid-training for coding agents, producing over 800K tasks.", "source": "tldr", "AI": {"tldr": "SWE-Universe\u63d0\u51fa\u4e86\u4e00\u79cd\u4eceGitHub PRs\u751f\u6210\u53ef\u9a8c\u8bc1\u8f6f\u4ef6\u5de5\u7a0b\u73af\u5883\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5185\u5faa\u73af\u9ed1\u5ba2\u68c0\u6d4b\u548c\u81ea\u6211\u9a8c\u8bc1\uff0c\u4e3a\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u5927\u89c4\u6a21\u4e2d\u671f\u8bad\u7ec3\uff0c\u751f\u6210\u4e86\u8d85\u8fc780\u4e07\u4e2a\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u7f16\u7801\u4ee3\u7406\u7684\u8bad\u7ec3\u6570\u636e\u6709\u9650\uff0c\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u73af\u5883\u3002\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u751f\u6210\u5927\u91cf\u53ef\u9a8c\u8bc1\u7684\u7f16\u7a0b\u4efb\u52a1\uff0c\u4ee5\u652f\u6301\u7f16\u7801\u4ee3\u7406\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u3002", "method": "\u4eceGitHub PRs\u81ea\u52a8\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u8f6f\u4ef6\u5de5\u7a0b\u73af\u5883\uff0c\u91c7\u7528\u5185\u5faa\u73af\u9ed1\u5ba2\u68c0\u6d4b\u673a\u5236\u9632\u6b62\u4f5c\u5f0a\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u9a8c\u8bc1\u786e\u4fdd\u4efb\u52a1\u7684\u53ef\u9a8c\u8bc1\u6027\uff0c\u5b9e\u73b0\u4efb\u52a1\u751f\u6210\u7684\u53ef\u6269\u5c55\u6027\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u8d85\u8fc7800,000\u4e2a\u53ef\u9a8c\u8bc1\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff0c\u4e3a\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u7684\u4e2d\u671f\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u652f\u6301\u66f4\u6709\u6548\u7684\u7f16\u7801\u4ee3\u7406\u8bad\u7ec3\u3002", "conclusion": "SWE-Universe\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u751f\u6210\u5927\u91cf\u53ef\u9a8c\u8bc1\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u7f16\u7801\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4e3a\u7f16\u7801\u4ee3\u7406\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "swe benchmark"}}
{"id": "tldr.2602.e5d360d0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.a16z.news%2Fp%2Fmost-people-cant-vibe-code-heres%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/KZ2dTXMczsbBCciXOAQ-bm1wHpIdjrUkQcOhKLzB2lE=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.a16z.news%2Fp%2Fmost-people-cant-vibe-code-heres%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/KZ2dTXMczsbBCciXOAQ-bm1wHpIdjrUkQcOhKLzB2lE=443", "authors": ["TLDR Newsletter"], "title": "Most People Can't Vibe Code. Here's How We Fix That", "comment": "Source: TLDR Newsletter, Date: 2026-02-04, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.a16z.news%2Fp%2Fmost-people-cant-vibe-code-heres%3Futm_source=tldrai/1/0100019c290563e8-9e7070f3-9667-4af2-8f5b-8726816118c6-000000/KZ2dTXMczsbBCciXOAQ-bm1wHpIdjrUkQcOhKLzB2lE=443", "summary": "Most People Can't Vibe Code. Here's How We Fix That (6 minute read) Vibe coding has yet to reach mainstream consumers, remaining primarily the domain of technical users. Companies like Poke and Wabi are developing consumer-friendly AI products that eliminate complex technical setup and terminology. The real opportunity lies in creating tools that make software development accessible to non-technical users, similar to how Squarespace and Canva democratized websites and design.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u5982\u4f55\u8ba9\u975e\u6280\u672f\u7528\u6237\u4e5f\u80fd\u8fdb\u884c\"\u6c1b\u56f4\u7f16\u7a0b\"\uff0c\u901a\u8fc7\u5f00\u53d1\u7c7b\u4f3cSquarespace\u548cCanva\u7684\u7b80\u5316\u5de5\u5177\uff0c\u4f7f\u8f6f\u4ef6\u5f00\u53d1\u5927\u4f17\u5316", "motivation": "\u5f53\u524d\u6c1b\u56f4\u7f16\u7a0b\u4e3b\u8981\u5c40\u9650\u4e8e\u6280\u672f\u7528\u6237\uff0c\u9700\u8981\u5f00\u53d1\u6d88\u8d39\u8005\u53cb\u597d\u7684AI\u4ea7\u54c1\u6765\u6d88\u9664\u590d\u6742\u7684\u6280\u672f\u8bbe\u7f6e\u548c\u672f\u8bed\uff0c\u8ba9\u975e\u6280\u672f\u7528\u6237\u4e5f\u80fd\u8fdb\u884c\u8f6f\u4ef6\u5f00\u53d1", "method": "\u901a\u8fc7\u5f00\u53d1\u7c7b\u4f3cPoke\u548cWabi\u8fd9\u6837\u7684\u516c\u53f8\u4ea7\u54c1\uff0c\u7b80\u5316\u6280\u672f\u8bbe\u7f6e\u548c\u672f\u8bed\uff0c\u521b\u5efa\u7c7b\u4f3cSquarespace\u548cCanva\u7684\u6613\u7528\u5de5\u5177", "result": "\u6307\u51fa\u4e86\u8ba9\u8f6f\u4ef6\u5f00\u53d1\u5927\u4f17\u5316\u7684\u673a\u4f1a\uff0c\u4f46\u5c1a\u672a\u5b9e\u73b0\u4e3b\u6d41\u6d88\u8d39\u8005\u7684\u5e7f\u6cdb\u91c7\u7528", "conclusion": "\u771f\u6b63\u7684\u673a\u4f1a\u5728\u4e8e\u521b\u5efa\u4f7f\u8f6f\u4ef6\u5f00\u53d1\u5bf9\u975e\u6280\u672f\u7528\u6237\u53ef\u8bbf\u95ee\u7684\u5de5\u5177\uff0c\u5c31\u50cfSquarespace\u548cCanva\u4f7f\u7f51\u7ad9\u548c\u8bbe\u8ba1\u6c11\u4e3b\u5316\u4e00\u6837", "topic": "swe application"}}
{"id": "tldr.2602.e3f1e776", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineeringblog.yelp.com%2F2026%2F02%2Fhow-yelp-built-a-back-testing-engine-for-safer-smarter-ad-budget-allocation.html%3Futm_source=tldrdata/1/0100019c2d7da8c9-5be7052b-7548-40e5-bfd4-d21470c6c8b6-000000/pA_0dVLUWDgbNIiyWEToLL5Mas4ZDcqIN84v6TbxVoQ=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineeringblog.yelp.com%2F2026%2F02%2Fhow-yelp-built-a-back-testing-engine-for-safer-smarter-ad-budget-allocation.html%3Futm_source=tldrdata/1/0100019c2d7da8c9-5be7052b-7548-40e5-bfd4-d21470c6c8b6-000000/pA_0dVLUWDgbNIiyWEToLL5Mas4ZDcqIN84v6TbxVoQ=443", "authors": ["TLDR Newsletter"], "title": "How Yelp Built a Back-Testing Engine for Safer, Smarter Ad Budget Allocation", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineeringblog.yelp.com%2F2026%2F02%2Fhow-yelp-built-a-back-testing-engine-for-safer-smarter-ad-budget-allocation.html%3Futm_source=tldrdata/1/0100019c2d7da8c9-5be7052b-7548-40e5-bfd4-d21470c6c8b6-000000/pA_0dVLUWDgbNIiyWEToLL5Mas4ZDcqIN84v6TbxVoQ=443", "summary": "How Yelp Built a Back-Testing Engine for Safer, Smarter Ad Budget Allocation (10 minute read) Yelp built a Back-Testing Engine to safely simulate proposed changes to its complex Ad Budget Allocation system by replaying historical campaign data through production-like logic, using tools like CatBoost ML models for outcome prediction, Scikit-Optimize for parameter tuning, and Git-submodule-integrated code to evaluate system-wide impacts without risking live advertisers.", "source": "tldr", "AI": {"tldr": "Yelp\u5f00\u53d1\u4e86\u4e00\u4e2a\u56de\u6d4b\u5f15\u64ce\uff0c\u901a\u8fc7\u91cd\u653e\u5386\u53f2\u5e7f\u544a\u6d3b\u52a8\u6570\u636e\u6765\u5b89\u5168\u6a21\u62df\u5e7f\u544a\u9884\u7b97\u5206\u914d\u7cfb\u7edf\u7684\u53d8\u66f4\uff0c\u4f7f\u7528CatBoost\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\uff0cScikit-Optimize\u8c03\u53c2\uff0cGit\u5b50\u6a21\u5757\u96c6\u6210\u4ee3\u7801\u8bc4\u4f30\u7cfb\u7edf\u5f71\u54cd", "motivation": "Yelp\u9700\u8981\u5b89\u5168\u5730\u6d4b\u8bd5\u5176\u590d\u6742\u5e7f\u544a\u9884\u7b97\u5206\u914d\u7cfb\u7edf\u7684\u53d8\u66f4\uff0c\u907f\u514d\u5f71\u54cd\u5b9e\u9645\u5e7f\u544a\u5ba2\u6237\uff0c\u540c\u65f6\u786e\u4fdd\u7cfb\u7edf\u6539\u8fdb\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027", "method": "\u6784\u5efa\u56de\u6d4b\u5f15\u64ce\uff0c\u91cd\u653e\u5386\u53f2\u5e7f\u544a\u6d3b\u52a8\u6570\u636e\uff0c\u4f7f\u7528CatBoost\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u7ed3\u679c\u9884\u6d4b\uff0cScikit-Optimize\u8fdb\u884c\u53c2\u6570\u8c03\u4f18\uff0cGit\u5b50\u6a21\u5757\u96c6\u6210\u4ee3\u7801\u6765\u8bc4\u4f30\u7cfb\u7edf\u7ea7\u5f71\u54cd", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u5b89\u5168\u6a21\u62df\u5e7f\u544a\u9884\u7b97\u5206\u914d\u7cfb\u7edf\u53d8\u66f4\u7684\u5f15\u64ce\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u5b9e\u9645\u5e7f\u544a\u5ba2\u6237\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u7cfb\u7edf\u6539\u8fdb\u6548\u679c", "conclusion": "\u56de\u6d4b\u5f15\u64ce\u4e3aYelp\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u667a\u80fd\u7684\u5e7f\u544a\u9884\u7b97\u5206\u914d\u7cfb\u7edf\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u964d\u4f4e\u4e86\u53d8\u66f4\u98ce\u9669\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u4f18\u5316\u6548\u7387", "topic": "swe application"}}
{"id": "tldr.2602.a2e61bb0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fagentic-engineering%2F%3Futm_source=tldrnewsletter/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/ItRp-RNi1CpxEqDQqpmkaPintHDSIJAOJYvhk3QRCI4=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fagentic-engineering%2F%3Futm_source=tldrnewsletter/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/ItRp-RNi1CpxEqDQqpmkaPintHDSIJAOJYvhk3QRCI4=443", "authors": ["TLDR Newsletter"], "title": "Agentic Engineering", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fagentic-engineering%2F%3Futm_source=tldrnewsletter/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/ItRp-RNi1CpxEqDQqpmkaPintHDSIJAOJYvhk3QRCI4=443", "summary": "Agentic Engineering (7 minute read) Agentic engineering is a professional term that describes what is actually happening: the engineer orchestrates AI agents as they act as an architect, reviewer, and decision maker. It is a serious engineering discipline that involves autonomous agents. The term definitely sounds better than 'vibe coding' in professional settings. Agentic engineering disproportionately benefits senior engineers who understand system design, security patterns, and performance...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Agentic Engineering\uff08\u667a\u80fd\u4f53\u5de5\u7a0b\uff09\u8fd9\u4e00\u4e13\u4e1a\u672f\u8bed\uff0c\u5b83\u63cf\u8ff0\u4e86\u5de5\u7a0b\u5e08\u5982\u4f55\u50cf\u67b6\u6784\u5e08\u3001\u8bc4\u5ba1\u5458\u548c\u51b3\u7b56\u8005\u4e00\u6837\u7f16\u6392AI\u667a\u80fd\u4f53\uff0c\u8fd9\u662f\u4e00\u79cd\u6d89\u53ca\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u4e25\u8083\u5de5\u7a0b\u5b66\u79d1\u3002", "motivation": "\u63d0\u51faAgentic Engineering\u8fd9\u4e00\u4e13\u4e1a\u672f\u8bed\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u63cf\u8ff0\u5f53\u524d\u5de5\u7a0b\u5e08\u4f7f\u7528AI\u667a\u80fd\u4f53\u7684\u5b9e\u8df5\uff0c\u66ff\u4ee3\"vibe coding\"\u7b49\u975e\u4e13\u4e1a\u8868\u8ff0\uff0c\u5f3a\u8c03\u5176\u4f5c\u4e3a\u4e25\u8083\u5de5\u7a0b\u5b66\u79d1\u7684\u5730\u4f4d\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u5b9a\u4e49\u548c\u672f\u8bed\u9610\u8ff0\u7684\u65b9\u5f0f\uff0c\u5c06\u5de5\u7a0b\u5e08\u7f16\u6392AI\u667a\u80fd\u4f53\u7684\u5b9e\u8df5\u7cfb\u7edf\u5316\u4e3a\"Agentic Engineering\"\u8fd9\u4e00\u4e13\u4e1a\u5b66\u79d1\uff0c\u5f3a\u8c03\u5de5\u7a0b\u5e08\u5728\u67b6\u6784\u8bbe\u8ba1\u3001\u4ee3\u7801\u8bc4\u5ba1\u548c\u51b3\u7b56\u5236\u5b9a\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "result": "\u5efa\u7acb\u4e86Agentic Engineering\u4f5c\u4e3a\u4e13\u4e1a\u5de5\u7a0b\u5b66\u79d1\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u660e\u786e\u4e86\u5176\u6838\u5fc3\u7279\u5f81\uff1a\u5de5\u7a0b\u5e08\u4f5c\u4e3a\u667a\u80fd\u4f53\u7f16\u6392\u8005\uff0c\u627f\u62c5\u67b6\u6784\u5e08\u3001\u8bc4\u5ba1\u5458\u548c\u51b3\u7b56\u8005\u89d2\u8272\uff0c\u7279\u522b\u6709\u5229\u4e8e\u5177\u5907\u7cfb\u7edf\u8bbe\u8ba1\u3001\u5b89\u5168\u6a21\u5f0f\u548c\u6027\u80fd\u7406\u89e3\u80fd\u529b\u7684\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u3002", "conclusion": "Agentic Engineering\u662f\u63cf\u8ff0\u5de5\u7a0b\u5e08\u7f16\u6392AI\u667a\u80fd\u4f53\u5b9e\u8df5\u7684\u6070\u5f53\u4e13\u4e1a\u672f\u8bed\uff0c\u5b83\u4ee3\u8868\u4e86\u4e00\u79cd\u4e25\u8083\u7684\u5de5\u7a0b\u5b66\u79d1\uff0c\u6bd4\"vibe coding\"\u7b49\u8868\u8ff0\u66f4\u4e13\u4e1a\uff0c\u7279\u522b\u9002\u5408\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.0d1e53e1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fchangelog%2F2026-02-04-claude-and-codex-are-now-available-in-public-preview-on-github%2F%3Futm_source=tldrnewsletter/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/0lp7ZAoq1kJDawO3yGx6JdrSPtJ6AA6ONlBUOfbWpY4=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fchangelog%2F2026-02-04-claude-and-codex-are-now-available-in-public-preview-on-github%2F%3Futm_source=tldrnewsletter/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/0lp7ZAoq1kJDawO3yGx6JdrSPtJ6AA6ONlBUOfbWpY4=443", "authors": ["TLDR Newsletter"], "title": "Claude and Codex are now available in public preview on GitHub", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fchangelog%2F2026-02-04-claude-and-codex-are-now-available-in-public-preview-on-github%2F%3Futm_source=tldrnewsletter/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/0lp7ZAoq1kJDawO3yGx6JdrSPtJ6AA6ONlBUOfbWpY4=443", "summary": "Claude and Codex are now available in public preview on GitHub (2 minute read) Claude by Anthropic and OpenAI Codex are now available as coding agents for Copilot Pro+ and Copilot Enterprise customers. Users can assign work to these agents directly from issues, pull requests, the Agents tab in enabled repositories, and the agent sessions view in VS Code. Access to Claude and Codex is included with existing Copilot subscriptions. Each coding agent session consumes one premium request during pu...", "source": "tldr", "AI": {"tldr": "GitHub Copilot Pro+\u548cEnterprise\u7528\u6237\u73b0\u5728\u53ef\u4ee5\u4f7f\u7528Claude\u548cCodex\u4f5c\u4e3a\u7f16\u7801\u4ee3\u7406\uff0c\u53ef\u4ee5\u76f4\u63a5\u4eceissues\u3001PRs\u7b49\u5730\u65b9\u5206\u914d\u4efb\u52a1\uff0c\u4f7f\u7528\u4f1a\u8bdd\u4f1a\u6d88\u8017\u4e00\u4e2a\u9ad8\u7ea7\u8bf7\u6c42", "motivation": "GitHub\u5e0c\u671b\u4e3aCopilot\u9ad8\u7ea7\u7528\u6237\u63d0\u4f9b\u66f4\u591aAI\u7f16\u7801\u4ee3\u7406\u9009\u62e9\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u7075\u6d3b\u5730\u4f7f\u7528\u4e0d\u540c\u7684AI\u6a21\u578b\u6765\u5b8c\u6210\u7f16\u7801\u4efb\u52a1\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387", "method": "\u5c06Anthropic\u7684Claude\u548cOpenAI\u7684Codex\u96c6\u6210\u5230GitHub Copilot\u5e73\u53f0\u4e2d\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u754c\u9762\uff08issues\u3001PRs\u3001Agents\u6807\u7b7e\u3001VS Code\u4f1a\u8bdd\u89c6\u56fe\uff09\u5206\u914d\u7f16\u7801\u4efb\u52a1\u7ed9\u8fd9\u4e9b\u4ee3\u7406", "result": "Claude\u548cCodex\u73b0\u5728\u4f5c\u4e3a\u7f16\u7801\u4ee3\u7406\u5728GitHub Copilot Pro+\u548cEnterprise\u7248\u672c\u4e2d\u516c\u5f00\u9884\u89c8\uff0c\u7528\u6237\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u8fd9\u4e9bAI\u4ee3\u7406\uff0c\u6bcf\u4e2a\u4f1a\u8bdd\u6d88\u8017\u4e00\u4e2a\u9ad8\u7ea7\u8bf7\u6c42", "conclusion": "GitHub\u6269\u5c55\u4e86Copilot\u7684AI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u591a\u7f16\u7801AI\u9009\u62e9\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5f00\u53d1\u8005\u7684AI\u8f85\u52a9\u7f16\u7a0b\u4f53\u9a8c", "topic": "code agent"}}
{"id": "tldr.2602.055ef57d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jakequist.com%2Fthoughts%2Fopenclaw-is-what-apple-intelligence-should-have-been%3Futm_source=tldrnewsletter/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/gQV4rlqyaxYaAcTTtC-qc25kFg4rVu_vW7AvgD3lojM=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jakequist.com%2Fthoughts%2Fopenclaw-is-what-apple-intelligence-should-have-been%3Futm_source=tldrnewsletter/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/gQV4rlqyaxYaAcTTtC-qc25kFg4rVu_vW7AvgD3lojM=443", "authors": ["TLDR Newsletter"], "title": "OpenClaw is What Apple Intelligence Should Have Been", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jakequist.com%2Fthoughts%2Fopenclaw-is-what-apple-intelligence-should-have-been%3Futm_source=tldrnewsletter/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/gQV4rlqyaxYaAcTTtC-qc25kFg4rVu_vW7AvgD3lojM=443", "summary": "OpenClaw is What Apple Intelligence Should Have Been (3 minute read) Mac Minis are selling out everywhere because people are using them to run OpenClaw, an open source framework for local AI agents that has become a killer app for Mac hardware. The rush shows that there is a clear demand for agents and automation, and Apple is getting the hardware revenue from it. However, it's missing out on the platform revenue - Apple could have created its own native agent, but this would likely have caus...", "source": "tldr", "AI": {"tldr": "OpenClaw\u662f\u4e00\u4e2a\u5f00\u6e90\u672c\u5730AI\u4ee3\u7406\u6846\u67b6\uff0c\u5728Mac\u786c\u4ef6\u4e0a\u6210\u4e3a\u6740\u624b\u7ea7\u5e94\u7528\uff0c\u663e\u793a\u4e86\u5bf9\u4ee3\u7406\u548c\u81ea\u52a8\u5316\u7684\u660e\u786e\u9700\u6c42\uff0c\u82f9\u679c\u4ece\u4e2d\u83b7\u5f97\u786c\u4ef6\u6536\u5165\u4f46\u9519\u8fc7\u4e86\u5e73\u53f0\u6536\u5165\u673a\u4f1a\u3002", "motivation": "\u5206\u6790OpenClaw\u4f5c\u4e3a\u5f00\u6e90\u672c\u5730AI\u4ee3\u7406\u6846\u67b6\u5728Mac\u786c\u4ef6\u4e0a\u7684\u6210\u529f\uff0c\u63ed\u793a\u5e02\u573a\u5bf9AI\u4ee3\u7406\u548c\u81ea\u52a8\u5316\u7684\u5f3a\u70c8\u9700\u6c42\uff0c\u4ee5\u53ca\u82f9\u679c\u5728AI\u5e73\u53f0\u6218\u7565\u4e0a\u7684\u7f3a\u5931\u673a\u4f1a\u3002", "method": "\u901a\u8fc7\u89c2\u5bdfOpenClaw\u6846\u67b6\u5728Mac Mini\u786c\u4ef6\u4e0a\u7684\u9500\u552e\u70ed\u6f6e\u548c\u7528\u6237\u91c7\u7528\u60c5\u51b5\uff0c\u5206\u6790\u5f00\u6e90AI\u4ee3\u7406\u6846\u67b6\u7684\u5e02\u573a\u5f71\u54cd\u548c\u82f9\u679c\u7684\u5e94\u5bf9\u7b56\u7565\u3002", "result": "OpenClaw\u6210\u4e3aMac\u786c\u4ef6\u7684\u6740\u624b\u7ea7\u5e94\u7528\uff0c\u63a8\u52a8Mac Mini\u9500\u552e\u706b\u7206\uff0c\u663e\u793a\u7528\u6237\u5f3a\u70c8\u504f\u597d\u672c\u5730\u8fd0\u884c\u7684AI\u4ee3\u7406\uff0c\u82f9\u679c\u83b7\u5f97\u786c\u4ef6\u6536\u5165\u4f46\u9519\u5931\u4e86\u5e73\u53f0\u63a7\u5236\u6743\u3002", "conclusion": "\u82f9\u679c\u5e94\u8be5\u5f00\u53d1\u81ea\u5df1\u7684\u539f\u751fAI\u4ee3\u7406\u5e73\u53f0\u6765\u628a\u63e1\u5e02\u573a\u673a\u4f1a\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u4f9d\u8d56\u786c\u4ef6\u9500\u552e\uff0c\u5f00\u6e90\u6846\u67b6\u7684\u6210\u529f\u66b4\u9732\u4e86\u82f9\u679c\u5728AI\u6218\u7565\u4e0a\u7684\u77ed\u677f\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.d20ec867", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.gitkraken.com%2Fai-productivity-paradox-2025%3Fsource=TLDR%26product=gitkraken%26utm_source=TLDR%26utm_medium=sponsored%26utm_campaign=insights_launch%26utm_content=paradox/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/HF9X1dGxuqS9z6E259Mvjcin0_n2DNx42aECytEa8B8=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.gitkraken.com%2Fai-productivity-paradox-2025%3Fsource=TLDR%26product=gitkraken%26utm_source=TLDR%26utm_medium=sponsored%26utm_campaign=insights_launch%26utm_content=paradox/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/HF9X1dGxuqS9z6E259Mvjcin0_n2DNx42aECytEa8B8=443", "authors": ["TLDR Newsletter"], "title": "Developers believe that AI increases productivity. Does it?", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.gitkraken.com%2Fai-productivity-paradox-2025%3Fsource=TLDR%26product=gitkraken%26utm_source=TLDR%26utm_medium=sponsored%26utm_campaign=insights_launch%26utm_content=paradox/1/0100019c2d8da997-ff5df1c2-70bd-46cf-9239-6e2dc36b5d80-000000/HF9X1dGxuqS9z6E259Mvjcin0_n2DNx42aECytEa8B8=443", "summary": "Developers believe that AI increases productivity. Does it? (Sponsor) 211M lines of code reveal AI's real impact: duplication is rising, delivery is less stable, and top teams are adapting faster than the rest. Download GitKraken's guide for engineering managers", "source": "tldr", "AI": {"tldr": "\u57fa\u4e8e\u5bf92.11\u4ebf\u884c\u4ee3\u7801\u7684\u5206\u6790\uff0c\u7814\u7a76\u53d1\u73b0AI\u5de5\u5177\u867d\u7136\u88ab\u5f00\u53d1\u8005\u8ba4\u4e3a\u80fd\u63d0\u9ad8\u751f\u4ea7\u529b\uff0c\u4f46\u5b9e\u9645\u4e0a\u5bfc\u81f4\u4ee3\u7801\u91cd\u590d\u7387\u4e0a\u5347\u3001\u4ea4\u4ed8\u7a33\u5b9a\u6027\u4e0b\u964d\uff0c\u53ea\u6709\u9876\u7ea7\u56e2\u961f\u80fd\u66f4\u5feb\u9002\u5e94AI\u5de5\u5177", "motivation": "\u9a8c\u8bc1AI\u5de5\u5177\u662f\u5426\u771f\u6b63\u63d0\u9ad8\u5f00\u53d1\u8005\u7684\u751f\u4ea7\u529b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u4ee3\u7801\u5206\u6790\u6765\u63ed\u793aAI\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u7684\u5b9e\u9645\u5f71\u54cd", "method": "\u5206\u67902.11\u4ebf\u884c\u4ee3\u7801\u6570\u636e\uff0c\u7814\u7a76AI\u5de5\u5177\u4f7f\u7528\u5bf9\u4ee3\u7801\u91cd\u590d\u7387\u3001\u4ea4\u4ed8\u7a33\u5b9a\u6027\u7b49\u6307\u6807\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e0d\u540c\u6c34\u5e73\u56e2\u961f\u7684\u9002\u5e94\u80fd\u529b", "result": "AI\u5de5\u5177\u5bfc\u81f4\u4ee3\u7801\u91cd\u590d\u7387\u4e0a\u5347\u3001\u4ea4\u4ed8\u7a33\u5b9a\u6027\u4e0b\u964d\uff0c\u53ea\u6709\u9876\u7ea7\u56e2\u961f\u80fd\u66f4\u5feb\u9002\u5e94AI\u5de5\u5177\u5e76\u4ece\u4e2d\u83b7\u76ca", "conclusion": "AI\u5de5\u5177\u5bf9\u751f\u4ea7\u529b\u7684\u5f71\u54cd\u5e76\u975e\u666e\u904d\u79ef\u6781\uff0c\u9700\u8981\u66f4\u8c28\u614e\u5730\u8bc4\u4f30\u548c\u91c7\u7528\uff0c\u540c\u65f6\u5173\u6ce8\u56e2\u961f\u80fd\u529b\u5dee\u5f02", "topic": "agent analysis"}}
{"id": "tldr.2602.42c48fbb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fmattbminor_if-youre-using-claude-code-or-cowork-and-share-7424092242404974594-9ZWB%3Futm_source=tldrmarketing/1/0100019c2db238e5-a7eb53a7-6f31-47cc-9652-fedbc1973d5c-000000/oBQGXEaIhOG00E-tvLgyLrYihTEI1S0P3JoZ4rCI0Wc=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fmattbminor_if-youre-using-claude-code-or-cowork-and-share-7424092242404974594-9ZWB%3Futm_source=tldrmarketing/1/0100019c2db238e5-a7eb53a7-6f31-47cc-9652-fedbc1973d5c-000000/oBQGXEaIhOG00E-tvLgyLrYihTEI1S0P3JoZ4rCI0Wc=443", "authors": ["TLDR Newsletter"], "title": "SEO Overhaul Using Claude Skills", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fposts%2Fmattbminor_if-youre-using-claude-code-or-cowork-and-share-7424092242404974594-9ZWB%3Futm_source=tldrmarketing/1/0100019c2db238e5-a7eb53a7-6f31-47cc-9652-fedbc1973d5c-000000/oBQGXEaIhOG00E-tvLgyLrYihTEI1S0P3JoZ4rCI0Wc=443", "summary": "SEO Overhaul Using Claude Skills (2 minute read) This post shows how Claude Code installed SEO and GEO skills with a single npx command and ran audits to find broken titles, missing SEO, weak meta descriptions, schema gaps, and pages needing migration. Marketing and AI visibility skills guided what to fix, while Cowork with the Directus MCP applied changes across hundreds of pages. Links to both the SEO and GEO repositories are in the post's comments.", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528Claude\u6280\u80fd\u8fdb\u884cSEO\u4f18\u5316\uff0c\u901a\u8fc7npx\u547d\u4ee4\u5b89\u88c5SEO\u548cGEO\u6280\u80fd\uff0c\u8fd0\u884c\u5ba1\u8ba1\u53d1\u73b0\u6807\u9898\u95ee\u9898\u3001\u7f3a\u5931SEO\u3001\u5f31\u5143\u63cf\u8ff0\u3001\u6a21\u5f0f\u5dee\u8ddd\u548c\u9700\u8981\u8fc1\u79fb\u7684\u9875\u9762\uff0c\u4f7f\u7528\u8425\u9500\u548cAI\u53ef\u89c1\u6027\u6280\u80fd\u6307\u5bfc\u4fee\u590d\uff0c\u901a\u8fc7Directus MCP\u5e94\u7528\u66f4\u6539\u5230\u6570\u767e\u4e2a\u9875\u9762\u3002", "motivation": "\u4f20\u7edfSEO\u4f18\u5316\u8fc7\u7a0b\u7e41\u7410\u4e14\u8017\u65f6\uff0c\u9700\u8981\u624b\u52a8\u5ba1\u8ba1\u548c\u4fee\u590d\u5927\u91cf\u7f51\u9875\uff0c\u672c\u6587\u65e8\u5728\u5c55\u793a\u5982\u4f55\u5229\u7528Claude\u6280\u80fd\u81ea\u52a8\u5316SEO\u5ba1\u8ba1\u548c\u4fee\u590d\u6d41\u7a0b\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u4f7f\u7528Claude Code\u901a\u8fc7npx\u547d\u4ee4\u5b89\u88c5SEO\u548cGEO\u6280\u80fd\uff0c\u8fd0\u884c\u81ea\u52a8\u5316\u5ba1\u8ba1\u8bc6\u522b\u5404\u79cdSEO\u95ee\u9898\uff0c\u7ed3\u5408\u8425\u9500\u548cAI\u53ef\u89c1\u6027\u6280\u80fd\u5206\u6790\u9700\u8981\u4fee\u590d\u7684\u5185\u5bb9\uff0c\u901a\u8fc7Directus MCP\uff08\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff09\u6279\u91cf\u5e94\u7528\u66f4\u6539\u5230\u6570\u767e\u4e2a\u9875\u9762\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86SEO\u5ba1\u8ba1\u548c\u4fee\u590d\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u80fd\u591f\u5feb\u901f\u8bc6\u522b\u5e76\u4fee\u590d\u6807\u9898\u95ee\u9898\u3001\u7f3a\u5931SEO\u5143\u7d20\u3001\u5f31\u5143\u63cf\u8ff0\u3001\u6a21\u5f0f\u5dee\u8ddd\u7b49\u5e38\u89c1SEO\u95ee\u9898\uff0c\u5927\u5e45\u63d0\u9ad8\u4e86SEO\u4f18\u5316\u7684\u6548\u7387\u3002", "conclusion": "Claude\u6280\u80fd\u7ed3\u5408MCP\u6280\u672f\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316SEO\u4f18\u5316\u5de5\u4f5c\u6d41\uff0c\u663e\u8457\u51cf\u5c11\u624b\u52a8\u5de5\u4f5c\u91cf\uff0c\u4e3a\u5927\u89c4\u6a21\u7f51\u7ad9\u7684SEO\u7ba1\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "tldr.2602.fbc162aa", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qodo.ai%2Fblog%2Fhow-we-built-a-real-world-benchmark-for-ai-code-review%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/5sIe6I8xN9KvTJ744iaJyR0QzeLgQcFGQobiJmj6jck=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qodo.ai%2Fblog%2Fhow-we-built-a-real-world-benchmark-for-ai-code-review%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/5sIe6I8xN9KvTJ744iaJyR0QzeLgQcFGQobiJmj6jck=443", "authors": ["TLDR Newsletter"], "title": "How we built a real-world benchmark for AI code review", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.qodo.ai%2Fblog%2Fhow-we-built-a-real-world-benchmark-for-ai-code-review%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/5sIe6I8xN9KvTJ744iaJyR0QzeLgQcFGQobiJmj6jck=443", "summary": "How we built a real-world benchmark for AI code review (10 minute read) Qodo has developed a new, rigorous code review benchmark to objectively evaluate AI systems. This benchmark intentionally injects diverse defects, including functional bugs and best practice violations, into genuine, merged pull requests from production-grade open-source repositories. The methodology allows for the simultaneous evaluation of code correctness and quality at a larger scale.", "source": "tldr", "AI": {"tldr": "Qodo\u5f00\u53d1\u4e86\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u4ee3\u7801\u5ba1\u67e5\u57fa\u51c6\uff0c\u901a\u8fc7\u5728\u5f00\u6e90\u751f\u4ea7\u7ea7\u4ed3\u5e93\u7684\u5408\u5e76PR\u4e2d\u6ce8\u5165\u5404\u79cd\u7f3a\u9677\uff0c\u7528\u4e8e\u5ba2\u89c2\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u4ee3\u7801\u5ba1\u67e5\u80fd\u529b", "motivation": "\u9700\u8981\u5ba2\u89c2\u8bc4\u4f30AI\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u73b0\u6709\u57fa\u51c6\u53ef\u80fd\u4e0d\u591f\u771f\u5b9e\u6216\u5168\u9762\uff0c\u9700\u8981\u540c\u65f6\u8bc4\u4f30\u4ee3\u7801\u6b63\u786e\u6027\u548c\u8d28\u91cf", "method": "\u5728\u771f\u5b9e\u5f00\u6e90\u751f\u4ea7\u7ea7\u4ed3\u5e93\u7684\u5408\u5e76PR\u4e2d\u6545\u610f\u6ce8\u5165\u591a\u79cd\u7f3a\u9677\uff08\u529f\u80fdbug\u548c\u6700\u4f73\u5b9e\u8df5\u8fdd\u89c4\uff09\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u8bc4\u4f30\u57fa\u51c6", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e25\u8c28\u7684\u4ee3\u7801\u5ba1\u67e5\u57fa\u51c6\uff0c\u80fd\u591f\u540c\u65f6\u8bc4\u4f30\u4ee3\u7801\u6b63\u786e\u6027\u548c\u8d28\u91cf\uff0c\u652f\u6301\u5927\u89c4\u6a21\u8bc4\u4f30", "conclusion": "\u8be5\u57fa\u51c6\u4e3aAI\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5ba2\u89c2\u3001\u771f\u5b9e\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55", "topic": "swe benchmark"}}
{"id": "tldr.2602.115ce9ea", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jakequist.com%2Fthoughts%2Fopenclaw-is-what-apple-intelligence-should-have-been%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/eq4vRESF-xSSciHV2uoI9rbwXKilRcSkV-FNm1o0F8I=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jakequist.com%2Fthoughts%2Fopenclaw-is-what-apple-intelligence-should-have-been%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/eq4vRESF-xSSciHV2uoI9rbwXKilRcSkV-FNm1o0F8I=443", "authors": ["TLDR Newsletter"], "title": "OpenClaw is What Apple Intelligence Should Have Been", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.jakequist.com%2Fthoughts%2Fopenclaw-is-what-apple-intelligence-should-have-been%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/eq4vRESF-xSSciHV2uoI9rbwXKilRcSkV-FNm1o0F8I=443", "summary": "OpenClaw is What Apple Intelligence Should Have Been (5 minute read) Apple squandered a huge opportunity with \"Apple Intelligence\" by failing to develop a truly agentic AI capable of automating computer use, a capability users are now achieving with open-source frameworks like OpenClaw on Mac Minis. This widespread adoption of headless AI agents shows a clear demand for powerful automation beyond simple summaries.", "source": "tldr", "AI": {"tldr": "\u82f9\u679c\u5728\"Apple Intelligence\"\u4e0a\u9519\u5931\u826f\u673a\uff0c\u672a\u80fd\u5f00\u53d1\u51fa\u771f\u6b63\u80fd\u591f\u81ea\u52a8\u5316\u4f7f\u7528\u7535\u8111\u7684\u667a\u80fd\u4f53\uff0c\u800c\u7528\u6237\u73b0\u5728\u901a\u8fc7\u5f00\u6e90\u6846\u67b6OpenClaw\u5728Mac Minis\u4e0a\u5b9e\u73b0\u4e86\u8fd9\u4e00\u80fd\u529b", "motivation": "\u6279\u8bc4\u82f9\u679c\u5728AI\u667a\u80fd\u4f53\u5f00\u53d1\u4e0a\u7684\u4fdd\u5b88\u7b56\u7565\uff0c\u6307\u51fa\u7528\u6237\u5bf9\u5f3a\u5927\u81ea\u52a8\u5316\u80fd\u529b\u7684\u9700\u6c42\u5df2\u7ecf\u8d85\u8d8a\u4e86\u7b80\u5355\u7684\u6458\u8981\u529f\u80fd", "method": "\u901a\u8fc7\u5bf9\u6bd4\u82f9\u679c\u7684\"Apple Intelligence\"\u4e0e\u5f00\u6e90\u6846\u67b6OpenClaw\u5728Mac Minis\u4e0a\u7684\u5b9e\u73b0\uff0c\u5206\u6790\u65e0\u5934AI\u667a\u80fd\u4f53\u7684\u5e94\u7528\u73b0\u72b6", "result": "\u5f00\u6e90\u6846\u67b6OpenClaw\u5c55\u793a\u4e86\u7528\u6237\u5bf9\u5f3a\u5927\u81ea\u52a8\u5316AI\u667a\u80fd\u4f53\u7684\u660e\u786e\u9700\u6c42\uff0c\u800c\u82f9\u679c\u9519\u5931\u4e86\u8fd9\u4e00\u5e02\u573a\u673a\u4f1a", "conclusion": "\u82f9\u679c\u5728AI\u667a\u80fd\u4f53\u5f00\u53d1\u4e0a\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u672a\u80fd\u6ee1\u8db3\u7528\u6237\u5bf9\u81ea\u52a8\u5316\u7535\u8111\u4f7f\u7528\u7684\u771f\u5b9e\u9700\u6c42\uff0c\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u6b63\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d", "topic": "agent analysis"}}
{"id": "tldr.2602.99bb4b0c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstackoverflow.blog%2F2026%2F02%2F04%2Fcode-smells-for-ai-agents-q-and-a-with-eno-reyes-of-factory%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/VGanrG-dT7vl9nZQfnRyPgf8-sNX03IfFouZ094jhNE=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstackoverflow.blog%2F2026%2F02%2F04%2Fcode-smells-for-ai-agents-q-and-a-with-eno-reyes-of-factory%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/VGanrG-dT7vl9nZQfnRyPgf8-sNX03IfFouZ094jhNE=443", "authors": ["TLDR Newsletter"], "title": "Code smells for AI agents: Q&A with Eno Reyes of Factory", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstackoverflow.blog%2F2026%2F02%2F04%2Fcode-smells-for-ai-agents-q-and-a-with-eno-reyes-of-factory%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/VGanrG-dT7vl9nZQfnRyPgf8-sNX03IfFouZ094jhNE=443", "summary": "Code smells for AI agents: Q&A with Eno Reyes of Factory (10 minute read) Building good coding agents isn't just about the model, but also about the infrastructure layer that manages context, tools, and hundreds of validation signals like linters and tests. Stanford research found that AI agent adoption doesn't predict productivity, but code quality does. High-quality codebases accelerate with AI, while low-quality ones actually decelerate.", "source": "tldr", "AI": {"tldr": "AI\u667a\u80fd\u4f53\u5f00\u53d1\u4e0d\u4ec5\u4f9d\u8d56\u6a21\u578b\u8d28\u91cf\uff0c\u66f4\u9700\u8981\u57fa\u7840\u8bbe\u65bd\u5c42\u6765\u7ba1\u7406\u4e0a\u4e0b\u6587\u3001\u5de5\u5177\u548c\u9a8c\u8bc1\u4fe1\u53f7\u3002\u7814\u7a76\u53d1\u73b0AI\u91c7\u7528\u7387\u4e0d\u80fd\u9884\u6d4b\u751f\u4ea7\u529b\uff0c\u4f46\u4ee3\u7801\u8d28\u91cf\u53ef\u4ee5\uff1a\u9ad8\u8d28\u91cf\u4ee3\u7801\u5e93\u80fd\u52a0\u901fAI\u5e94\u7528\uff0c\u4f4e\u8d28\u91cf\u4ee3\u7801\u5e93\u53cd\u800c\u4f1a\u51cf\u901f\u3002", "motivation": "\u63a2\u8ba8AI\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u56e0\u7d20\uff0c\u6307\u51fa\u5f53\u524d\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u6a21\u578b\u672c\u8eab\uff0c\u800c\u5ffd\u7565\u4e86\u57fa\u7840\u8bbe\u65bd\u5c42\u7684\u91cd\u8981\u6027\u3002\u540c\u65f6\u63ed\u793aAI\u91c7\u7528\u4e0e\u751f\u4ea7\u529b\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u5f3a\u8c03\u4ee3\u7801\u8d28\u91cf\u5728AI\u65f6\u4ee3\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u8bbf\u8c08\u548c\u65af\u5766\u798f\u7814\u7a76\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff1a1) \u4e0eFactory\u7684Eno Reyes\u8fdb\u884c\u95ee\u7b54\u8bbf\u8c08\uff0c\u63a2\u8ba8AI\u667a\u80fd\u4f53\u5f00\u53d1\u7684\u57fa\u7840\u8bbe\u65bd\u9700\u6c42\uff1b2) \u5f15\u7528\u65af\u5766\u798f\u7814\u7a76\u6570\u636e\uff0c\u5206\u6790AI\u91c7\u7528\u7387\u3001\u4ee3\u7801\u8d28\u91cf\u4e0e\u751f\u4ea7\u529b\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) AI\u667a\u80fd\u4f53\u5f00\u53d1\u9700\u8981\u5f3a\u5927\u7684\u57fa\u7840\u8bbe\u65bd\u5c42\u6765\u7ba1\u7406\u4e0a\u4e0b\u6587\u3001\u5de5\u5177\u548c\u9a8c\u8bc1\u4fe1\u53f7\uff1b2) AI\u91c7\u7528\u7387\u672c\u8eab\u4e0d\u80fd\u9884\u6d4b\u5f00\u53d1\u56e2\u961f\u7684\u751f\u4ea7\u529b\uff1b3) \u4ee3\u7801\u8d28\u91cf\u662f\u5173\u952e\u56e0\u7d20\u2014\u2014\u9ad8\u8d28\u91cf\u4ee3\u7801\u5e93\u80fd\u52a0\u901fAI\u5e94\u7528\uff0c\u4f4e\u8d28\u91cf\u4ee3\u7801\u5e93\u53cd\u800c\u4f1a\u51cf\u901f\u3002", "conclusion": "\u5f00\u53d1\u4f18\u79c0\u7684AI\u667a\u80fd\u4f53\u9700\u8981\u540c\u65f6\u5173\u6ce8\u6a21\u578b\u8d28\u91cf\u548c\u57fa\u7840\u8bbe\u65bd\u5c42\u5efa\u8bbe\u3002\u5728AI\u65f6\u4ee3\uff0c\u4ee3\u7801\u8d28\u91cf\u6bd4AI\u91c7\u7528\u7387\u66f4\u91cd\u8981\uff0c\u9ad8\u8d28\u91cf\u4ee3\u7801\u5e93\u80fd\u6700\u5927\u5316AI\u7684\u751f\u4ea7\u529b\u6548\u76ca\uff0c\u800c\u4f4e\u8d28\u91cf\u4ee3\u7801\u5e93\u4f1a\u963b\u788dAI\u7684\u6b63\u9762\u5f71\u54cd\u3002", "topic": "code agent"}}
{"id": "tldr.2602.b12d0739", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fboxc.net%2Fblog%2F2026%2Fclaude-code-connecting-to-local-models-when-your-quota-runs-out%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/BidzMUiKrwHmXiO7z6tjpLxke2LVr9qDaBdxC8QqsB0=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fboxc.net%2Fblog%2F2026%2Fclaude-code-connecting-to-local-models-when-your-quota-runs-out%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/BidzMUiKrwHmXiO7z6tjpLxke2LVr9qDaBdxC8QqsB0=443", "authors": ["TLDR Newsletter"], "title": "Claude Code: connect to a local model when your quota runs out", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fboxc.net%2Fblog%2F2026%2Fclaude-code-connecting-to-local-models-when-your-quota-runs-out%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/BidzMUiKrwHmXiO7z6tjpLxke2LVr9qDaBdxC8QqsB0=443", "summary": "Claude Code: connect to a local model when your quota runs out (3 minute read) Claude Code can connect to a local open-source AI model, using LM Studio, as a backup solution to continue coding when Anthropic's API quota is exhausted.", "source": "tldr", "AI": {"tldr": "Claude Code \u63d0\u4f9b\u4e86\u4e00\u79cd\u8fde\u63a5\u672c\u5730\u5f00\u6e90AI\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f53Anthropic API\u914d\u989d\u8017\u5c3d\u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7LM Studio\u4f7f\u7528\u672c\u5730\u6a21\u578b\u4f5c\u4e3a\u5907\u7528\u65b9\u6848\u7ee7\u7eed\u7f16\u7801\u3002", "motivation": "\u89e3\u51b3\u5f00\u53d1\u8005\u5728\u4f7f\u7528Claude Code\u65f6\u9047\u5230API\u914d\u989d\u8017\u5c3d\u7684\u95ee\u9898\uff0c\u786e\u4fdd\u7f16\u7801\u5de5\u4f5c\u80fd\u591f\u6301\u7eed\u8fdb\u884c\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u5907\u7528\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u96c6\u6210LM Studio\uff0c\u4f7fClaude Code\u80fd\u591f\u8fde\u63a5\u5230\u672c\u5730\u8fd0\u884c\u7684\u5f00\u6e90AI\u6a21\u578b\uff0c\u5f53\u4e91\u7aefAPI\u914d\u989d\u4e0d\u8db3\u65f6\u81ea\u52a8\u5207\u6362\u5230\u672c\u5730\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u4e86\u5728API\u914d\u989d\u8017\u5c3d\u65f6\u65e0\u7f1d\u5207\u6362\u5230\u672c\u5730\u6a21\u578b\u7684\u80fd\u529b\uff0c\u786e\u4fdd\u7f16\u7801\u5de5\u4f5c\u4e0d\u4e2d\u65ad\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "conclusion": "Claude Code\u7684\u672c\u5730\u6a21\u578b\u8fde\u63a5\u529f\u80fd\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u7f16\u7801\u8f85\u52a9\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u56e0API\u9650\u5236\u5bfc\u81f4\u7684\u5de5\u4f5c\u4e2d\u65ad\u98ce\u9669\u3002", "topic": "code agent"}}
{"id": "tldr.2602.501b5f6a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuonw.github.io%2Fblog%2F2026%2F02%2Fai-plan%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/VYLy5WRDp0Kw1MNrxqc9l3HD3M8gQxVoUqfeQfgGMBM=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuonw.github.io%2Fblog%2F2026%2F02%2Fai-plan%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/VYLy5WRDp0Kw1MNrxqc9l3HD3M8gQxVoUqfeQfgGMBM=443", "authors": ["TLDR Newsletter"], "title": "Staying engaged with AI plans: give inline feedback", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuonw.github.io%2Fblog%2F2026%2F02%2Fai-plan%2F%3Futm_source=tldrdev/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/VYLy5WRDp0Kw1MNrxqc9l3HD3M8gQxVoUqfeQfgGMBM=443", "summary": "Staying engaged with AI plans: give inline feedback (3 minute read) AI coding agent performance can be improved by editing Claude Code markdown plans directly and providing feedback inline in the markdown files that Claude Code generates.", "source": "tldr", "AI": {"tldr": "\u901a\u8fc7\u76f4\u63a5\u5728Claude Code\u751f\u6210\u7684markdown\u8ba1\u5212\u6587\u4ef6\u4e2d\u63d0\u4f9b\u5185\u8054\u53cd\u9988\uff0c\u53ef\u4ee5\u63d0\u5347AI\u7f16\u7a0b\u4ee3\u7406\u7684\u6027\u80fd", "motivation": "AI\u7f16\u7a0b\u4ee3\u7406\u7684\u6027\u80fd\u53ef\u4ee5\u901a\u8fc7\u7528\u6237\u76f4\u63a5\u53c2\u4e0e\u548c\u53cd\u9988\u6765\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u8ba1\u5212\u5236\u5b9a\u9636\u6bb5", "method": "\u7528\u6237\u53ef\u4ee5\u76f4\u63a5\u7f16\u8f91Claude Code\u751f\u6210\u7684markdown\u8ba1\u5212\u6587\u4ef6\uff0c\u5e76\u5728\u5176\u4e2d\u63d0\u4f9b\u5185\u8054\u53cd\u9988\uff0c\u8ba9AI\u4ee3\u7406\u80fd\u591f\u6839\u636e\u53cd\u9988\u8c03\u6574\u8ba1\u5212", "result": "\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8AI\u7f16\u7a0b\u4ee3\u7406\u7684\u6027\u80fd\u548c\u8ba1\u5212\u8d28\u91cf", "conclusion": "\u901a\u8fc7\u5185\u8054\u53cd\u9988\u673a\u5236\u8ba9\u7528\u6237\u76f4\u63a5\u53c2\u4e0eAI\u8ba1\u5212\u7684\u7f16\u8f91\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347AI\u7f16\u7a0b\u4ee3\u7406\u7684\u6548\u679c", "topic": "code agent"}}
{"id": "tldr.2602.dc3de017", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/m98I36Ctpbw5fF12f6wlru75GDhSccBrnrjz6-SLOTM=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/m98I36Ctpbw5fF12f6wlru75GDhSccBrnrjz6-SLOTM=443", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/m98I36Ctpbw5fF12f6wlru75GDhSccBrnrjz6-SLOTM=443", "summary": "Staying engaged with AI plans: give inline feedback (3 minute read) AI coding agent performance can be improved by editing Claude Code markdown plans directly and providing feedback inline in the markdown files that Claude Code generates.", "source": "tldr", "AI": {"tldr": "\u901a\u8fc7\u76f4\u63a5\u5728Claude Code\u751f\u6210\u7684markdown\u8ba1\u5212\u6587\u4ef6\u4e2d\u63d0\u4f9b\u5185\u8054\u53cd\u9988\uff0c\u53ef\u4ee5\u63d0\u5347AI\u7f16\u7801\u4ee3\u7406\u7684\u6027\u80fd", "motivation": "AI\u7f16\u7801\u4ee3\u7406\u7684\u6027\u80fd\u53ef\u4ee5\u901a\u8fc7\u7528\u6237\u76f4\u63a5\u53c2\u4e0e\u548c\u53cd\u9988\u6765\u63d0\u5347\uff0c\u7279\u522b\u662f\u901a\u8fc7\u7f16\u8f91\u4ee3\u7406\u751f\u6210\u7684\u8ba1\u5212\u6587\u4ef6\u6765\u63d0\u4f9b\u5177\u4f53\u6307\u5bfc", "method": "\u7528\u6237\u5728Claude Code\u751f\u6210\u7684markdown\u8ba1\u5212\u6587\u4ef6\u4e2d\u76f4\u63a5\u7f16\u8f91\u5e76\u63d0\u4f9b\u5185\u8054\u53cd\u9988\uff0c\u8ba9AI\u4ee3\u7406\u80fd\u591f\u6839\u636e\u5177\u4f53\u6307\u5bfc\u8c03\u6574\u5176\u7f16\u7801\u8ba1\u5212", "result": "\u901a\u8fc7\u8fd9\u79cd\u5185\u8054\u53cd\u9988\u673a\u5236\uff0cAI\u7f16\u7801\u4ee3\u7406\u7684\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u610f\u56fe\u5e76\u751f\u6210\u66f4\u7b26\u5408\u9700\u6c42\u7684\u4ee3\u7801", "conclusion": "\u76f4\u63a5\u5728AI\u751f\u6210\u7684\u8ba1\u5212\u6587\u4ef6\u4e2d\u63d0\u4f9b\u5185\u8054\u53cd\u9988\u662f\u4e00\u79cd\u6709\u6548\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347AI\u7f16\u7801\u4ee3\u7406\u7684\u6027\u80fd\u548c\u7528\u6237\u6ee1\u610f\u5ea6", "topic": "code agent"}}
{"id": "tldr.2602.5a5b4ce3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/PFDsicbeD-Pn2Bn94lYIxBVGRHoiD5FUFPg4lXriOOU=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/PFDsicbeD-Pn2Bn94lYIxBVGRHoiD5FUFPg4lXriOOU=443", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/PFDsicbeD-Pn2Bn94lYIxBVGRHoiD5FUFPg4lXriOOU=443", "summary": "Staying engaged with AI plans: give inline feedback (3 minute read) AI coding agent performance can be improved by editing Claude Code markdown plans directly and providing feedback inline in the markdown files that Claude Code generates.", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7a0b\u52a9\u624bClaude Code\u901a\u8fc7\u5141\u8bb8\u7528\u6237\u5728\u751f\u6210\u7684markdown\u8ba1\u5212\u6587\u4ef6\u4e2d\u76f4\u63a5\u7f16\u8f91\u548c\u63d0\u4f9b\u5185\u8054\u53cd\u9988\uff0c\u53ef\u4ee5\u63d0\u5347AI\u7f16\u7801\u4ee3\u7406\u7684\u6027\u80fd", "motivation": "\u5f53\u524dAI\u7f16\u7801\u4ee3\u7406\u751f\u6210\u7684\u8ba1\u5212\u6587\u4ef6\u901a\u5e38\u662f\u9759\u6001\u7684\uff0c\u7528\u6237\u96be\u4ee5\u76f4\u63a5\u53c2\u4e0e\u4fee\u6539\u548c\u63d0\u4f9b\u53cd\u9988\uff0c\u8fd9\u9650\u5236\u4e86\u4ee3\u7406\u6027\u80fd\u7684\u6301\u7eed\u6539\u8fdb", "method": "\u5141\u8bb8\u7528\u6237\u5728Claude Code\u751f\u6210\u7684markdown\u8ba1\u5212\u6587\u4ef6\u4e2d\u76f4\u63a5\u7f16\u8f91\u5185\u5bb9\uff0c\u5e76\u63d0\u4f9b\u5185\u8054\u53cd\u9988\uff0c\u4f7fAI\u80fd\u591f\u6839\u636e\u7528\u6237\u53cd\u9988\u8c03\u6574\u540e\u7eed\u7684\u4ee3\u7801\u751f\u6210", "result": "\u901a\u8fc7\u8fd9\u79cd\u5185\u8054\u53cd\u9988\u673a\u5236\uff0cAI\u7f16\u7801\u4ee3\u7406\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u610f\u56fe\uff0c\u751f\u6210\u66f4\u7b26\u5408\u9700\u6c42\u7684\u4ee3\u7801\uff0c\u63d0\u5347\u6574\u4f53\u7f16\u7801\u6027\u80fd", "conclusion": "\u5728AI\u751f\u6210\u7684\u8ba1\u5212\u6587\u4ef6\u4e2d\u652f\u6301\u76f4\u63a5\u7f16\u8f91\u548c\u5185\u8054\u53cd\u9988\u662f\u4e00\u79cd\u6709\u6548\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347AI\u7f16\u7801\u4ee3\u7406\u7684\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c", "topic": "code agent"}}
{"id": "tldr.2602.c18eb850", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/vBbeMarB8U6kn0PxmZ8bVBL51wp-h3EKAv_hhtqI9xM=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/vBbeMarB8U6kn0PxmZ8bVBL51wp-h3EKAv_hhtqI9xM=443", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c2db5109a-852ba556-f82a-49aa-843d-f6bebac5c8c9-000000/vBbeMarB8U6kn0PxmZ8bVBL51wp-h3EKAv_hhtqI9xM=443", "summary": "Staying engaged with AI plans: give inline feedback (3 minute read) AI coding agent performance can be improved by editing Claude Code markdown plans directly and providing feedback inline in the markdown files that Claude Code generates.", "source": "tldr", "AI": {"tldr": "\u901a\u8fc7\u76f4\u63a5\u5728Claude Code\u751f\u6210\u7684markdown\u8ba1\u5212\u6587\u4ef6\u4e2d\u63d0\u4f9b\u5185\u8054\u53cd\u9988\uff0c\u53ef\u4ee5\u63d0\u5347AI\u7f16\u7a0b\u4ee3\u7406\u7684\u6027\u80fd", "motivation": "AI\u7f16\u7a0b\u4ee3\u7406\u5728\u751f\u6210\u4ee3\u7801\u8ba1\u5212\u65f6\uff0c\u7528\u6237\u96be\u4ee5\u6709\u6548\u53c2\u4e0e\u548c\u6307\u5bfc\uff0c\u5bfc\u81f4\u6700\u7ec8\u7ed3\u679c\u53ef\u80fd\u4e0d\u7b26\u5408\u9884\u671f\u3002\u9700\u8981\u4e00\u79cd\u66f4\u76f4\u63a5\u3001\u9ad8\u6548\u7684\u53cd\u9988\u673a\u5236\u6765\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5728Claude Code\u751f\u6210\u7684markdown\u8ba1\u5212\u6587\u4ef6\u4e2d\u76f4\u63a5\u7f16\u8f91\u548c\u63d0\u4f9b\u5185\u8054\u53cd\u9988\u7684\u65b9\u6cd5\u3002\u7528\u6237\u53ef\u4ee5\u76f4\u63a5\u5728\u4ee3\u7406\u751f\u6210\u7684\u4ee3\u7801\u8ba1\u5212\u4e2d\u8fdb\u884c\u4fee\u6539\u548c\u6ce8\u91ca\uff0c\u4ee3\u7406\u4f1a\u6839\u636e\u8fd9\u4e9b\u53cd\u9988\u8c03\u6574\u540e\u7eed\u7684\u4ee3\u7801\u751f\u6210\u3002", "result": "\u901a\u8fc7\u5185\u8054\u53cd\u9988\u673a\u5236\uff0c\u7528\u6237\u80fd\u591f\u66f4\u6709\u6548\u5730\u6307\u5bfcAI\u7f16\u7a0b\u4ee3\u7406\uff0c\u4f7f\u751f\u6210\u7684\u4ee3\u7801\u66f4\u7b26\u5408\u9700\u6c42\uff0c\u63d0\u5347\u4e86\u4ee3\u7406\u7684\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "\u5728AI\u7f16\u7a0b\u4ee3\u7406\u751f\u6210\u7684markdown\u8ba1\u5212\u4e2d\u63d0\u4f9b\u5185\u8054\u53cd\u9988\u662f\u4e00\u79cd\u6709\u6548\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u4ee3\u7406\u7684\u6027\u80fd\u548c\u4ee3\u7801\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "tldr.2602.25c5e870", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.liminal.ai%2Fbehavioral-agent-automation-platform%3Futm_campaign=FintechSecondary02052026%26utm_source=tldr%26utm_medium=newsletter/1/0100019c2e21632c-1bd7fdad-bcfb-415f-90f4-ab821daab965-000000/TLDpXqV1Pm929zkQ4uWWZs6PIuqgdx5o9xjaAnQIvS8=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.liminal.ai%2Fbehavioral-agent-automation-platform%3Futm_campaign=FintechSecondary02052026%26utm_source=tldr%26utm_medium=newsletter/1/0100019c2e21632c-1bd7fdad-bcfb-415f-90f4-ab821daab965-000000/TLDpXqV1Pm929zkQ4uWWZs6PIuqgdx5o9xjaAnQIvS8=443", "authors": ["TLDR Newsletter"], "title": "Behavioral Agent Automation Platform", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.liminal.ai%2Fbehavioral-agent-automation-platform%3Futm_campaign=FintechSecondary02052026%26utm_source=tldr%26utm_medium=newsletter/1/0100019c2e21632c-1bd7fdad-bcfb-415f-90f4-ab821daab965-000000/TLDpXqV1Pm929zkQ4uWWZs6PIuqgdx5o9xjaAnQIvS8=443", "summary": "95% of Banks See Zero ROI from AI Pilots (Sponsor) The problem isn't technology. It's architecture. Fintech firms are forced to predict workflows before observing how traders, analysts, personal bankers, and CSRs actually work. Liminal's Behavioral Agent Automation Platform inverts this: start by observing real behavior, identify friction, and deploy secure, compliant automations automatically. Human oversight and audit trails included. Download the free whitepaper (no sign up required).", "source": "tldr", "AI": {"tldr": "Liminal\u516c\u53f8\u63d0\u51fa\u884c\u4e3a\u4ee3\u7406\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u901a\u8fc7\u89c2\u5bdf\u771f\u5b9e\u5de5\u4f5c\u884c\u4e3a\u8bc6\u522b\u6469\u64e6\u70b9\uff0c\u81ea\u52a8\u90e8\u7f72\u5b89\u5168\u5408\u89c4\u7684\u81ea\u52a8\u5316\u65b9\u6848\uff0c\u89e3\u51b3\u94f6\u884cAI\u8bd5\u70b9ROI\u4f4e\u7684\u95ee\u9898", "motivation": "95%\u7684\u94f6\u884cAI\u8bd5\u70b9\u9879\u76ee\u6295\u8d44\u56de\u62a5\u7387\u4e3a\u96f6\uff0c\u4e3b\u8981\u95ee\u9898\u4e0d\u5728\u4e8e\u6280\u672f\u672c\u8eab\uff0c\u800c\u5728\u4e8e\u67b6\u6784\u8bbe\u8ba1\u3002\u4f20\u7edf\u91d1\u878d\u79d1\u6280\u516c\u53f8\u9700\u8981\u5728\u89c2\u5bdf\u4ea4\u6613\u5458\u3001\u5206\u6790\u5e08\u3001\u4e2a\u4eba\u94f6\u884c\u5bb6\u548c\u5ba2\u670d\u4ee3\u8868\u5b9e\u9645\u5de5\u4f5c\u65b9\u5f0f\u4e4b\u524d\u5c31\u9884\u6d4b\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8fd9\u5bfc\u81f4\u4e86\u89e3\u51b3\u65b9\u6848\u4e0e\u5b9e\u9645\u60c5\u51b5\u8131\u8282\u3002", "method": "Liminal\u7684\u884c\u4e3a\u4ee3\u7406\u81ea\u52a8\u5316\u5e73\u53f0\u91c7\u7528\u9006\u5411\u65b9\u6cd5\uff1a\u9996\u5148\u89c2\u5bdf\u771f\u5b9e\u5de5\u4f5c\u884c\u4e3a\uff0c\u8bc6\u522b\u5de5\u4f5c\u4e2d\u7684\u6469\u64e6\u70b9\u548c\u6548\u7387\u74f6\u9888\uff0c\u7136\u540e\u81ea\u52a8\u90e8\u7f72\u5b89\u5168\u3001\u5408\u89c4\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002\u5e73\u53f0\u5305\u542b\u4eba\u5de5\u76d1\u7763\u548c\u5ba1\u8ba1\u8ffd\u8e2a\u529f\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ece\u5b9e\u9645\u884c\u4e3a\u51fa\u53d1\u800c\u975e\u9884\u8bbe\u5de5\u4f5c\u6d41\u7a0b\uff0c\u80fd\u591f\u66f4\u7cbe\u51c6\u5730\u8bc6\u522b\u81ea\u52a8\u5316\u673a\u4f1a\uff0c\u90e8\u7f72\u7684\u81ea\u52a8\u5316\u65b9\u6848\u66f4\u8d34\u5408\u5b9e\u9645\u9700\u6c42\uff0c\u4ece\u800c\u63d0\u9ad8\u6295\u8d44\u56de\u62a5\u7387\u3002", "conclusion": "\u89e3\u51b3\u94f6\u884cAI\u8bd5\u70b9ROI\u4f4e\u7684\u5173\u952e\u5728\u4e8e\u6539\u53d8\u67b6\u6784\u65b9\u6cd5\uff0c\u4ece\u89c2\u5bdf\u771f\u5b9e\u884c\u4e3a\u5165\u624b\uff0c\u800c\u975e\u9884\u5148\u9884\u6d4b\u5de5\u4f5c\u6d41\u7a0b\u3002\u8fd9\u79cd\u57fa\u4e8e\u884c\u4e3a\u89c2\u5bdf\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u8bc6\u522b\u548c\u89e3\u51b3\u5b9e\u9645\u5de5\u4f5c\u4e2d\u7684\u6469\u64e6\u70b9\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.53c7f918", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnekuda.substack.com%2Fp%2Fagentic-commerce-gatekeeping-problem%3Futm_source=tldrfintech/1/0100019c2e21632c-1bd7fdad-bcfb-415f-90f4-ab821daab965-000000/UH6o-ZqL0-I46h1feoxwH96CTw07CV_SnLx_2qZHGhM=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnekuda.substack.com%2Fp%2Fagentic-commerce-gatekeeping-problem%3Futm_source=tldrfintech/1/0100019c2e21632c-1bd7fdad-bcfb-415f-90f4-ab821daab965-000000/UH6o-ZqL0-I46h1feoxwH96CTw07CV_SnLx_2qZHGhM=443", "authors": ["TLDR Newsletter"], "title": "Agentic commerce gatekeeping problem", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnekuda.substack.com%2Fp%2Fagentic-commerce-gatekeeping-problem%3Futm_source=tldrfintech/1/0100019c2e21632c-1bd7fdad-bcfb-415f-90f4-ab821daab965-000000/UH6o-ZqL0-I46h1feoxwH96CTw07CV_SnLx_2qZHGhM=443", "summary": "Agentic commerce gatekeeping problem (8 minute read) Agentic commerce protocols are emerging as \u201copen\u201d standards controlled by early distribution partners like Shopify, creating data asymmetries that risk shifting incentives from individual merchant success toward platform-level GMV optimization.", "source": "tldr", "AI": {"tldr": "\u4ee3\u7406\u5546\u52a1\u534f\u8bae\u7531\u65e9\u671f\u5206\u9500\u4f19\u4f34\u63a7\u5236\uff0c\u9020\u6210\u6570\u636e\u4e0d\u5bf9\u79f0\uff0c\u53ef\u80fd\u5c06\u6fc0\u52b1\u4ece\u5546\u5bb6\u6210\u529f\u8f6c\u5411\u5e73\u53f0GMV\u4f18\u5316", "motivation": "\u5206\u6790\u65b0\u5174\u7684\u4ee3\u7406\u5546\u52a1\u534f\u8bae\u4f5c\u4e3a\"\u5f00\u653e\"\u6807\u51c6\u88abShopify\u7b49\u65e9\u671f\u5206\u9500\u4f19\u4f34\u63a7\u5236\u7684\u95ee\u9898\uff0c\u5173\u6ce8\u6570\u636e\u4e0d\u5bf9\u79f0\u5e26\u6765\u7684\u6fc0\u52b1\u626d\u66f2\u98ce\u9669", "method": "\u901a\u8fc7\u5bf9\u4ee3\u7406\u5546\u52a1\u534f\u8bae\uff08agentic commerce protocols\uff09\u7684\u5206\u6790\uff0c\u63a2\u8ba8\u5e73\u53f0\u63a7\u5236\u6743\u3001\u6570\u636e\u4e0d\u5bf9\u79f0\u548c\u6fc0\u52b1\u7ed3\u6784\u95ee\u9898", "result": "\u53d1\u73b0\u65e9\u671f\u5206\u9500\u4f19\u4f34\u63a7\u5236\u5f00\u653e\u6807\u51c6\u4f1a\u5bfc\u81f4\u6570\u636e\u4e0d\u5bf9\u79f0\uff0c\u4f7f\u6fc0\u52b1\u4ece\u4e2a\u4f53\u5546\u5bb6\u6210\u529f\u8f6c\u5411\u5e73\u53f0\u5c42\u9762\u7684GMV\u4f18\u5316", "conclusion": "\u4ee3\u7406\u5546\u52a1\u534f\u8bae\u7684\u63a7\u5236\u6743\u96c6\u4e2d\u548c\u6570\u636e\u4e0d\u5bf9\u79f0\u95ee\u9898\u9700\u8981\u5173\u6ce8\uff0c\u4ee5\u786e\u4fdd\u534f\u8bae\u771f\u6b63\u5f00\u653e\u5e76\u4fdd\u62a4\u5546\u5bb6\u5229\u76ca", "topic": "agent analysis"}}
{"id": "tldr.2602.4bd3da7f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfintech%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c2e21632c-1bd7fdad-bcfb-415f-90f4-ab821daab965-000000/np87ayDGw1oqLAtgeyWgYnC97Mhub33a11NKX_VBoJ8=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfintech%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c2e21632c-1bd7fdad-bcfb-415f-90f4-ab821daab965-000000/np87ayDGw1oqLAtgeyWgYnC97Mhub33a11NKX_VBoJ8=443", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrfintech%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c2e21632c-1bd7fdad-bcfb-415f-90f4-ab821daab965-000000/np87ayDGw1oqLAtgeyWgYnC97Mhub33a11NKX_VBoJ8=443", "summary": "Agentic commerce gatekeeping problem (8 minute read) Agentic commerce protocols are emerging as \u201copen\u201d standards controlled by early distribution partners like Shopify, creating data asymmetries that risk shifting incentives from individual merchant success toward platform-level GMV optimization.", "source": "tldr", "AI": {"tldr": "\u4ee3\u7406\u5546\u52a1\u534f\u8bae\u7531\u65e9\u671f\u5206\u9500\u4f19\u4f34\u63a7\u5236\uff0c\u9020\u6210\u6570\u636e\u4e0d\u5bf9\u79f0\uff0c\u53ef\u80fd\u5c06\u6fc0\u52b1\u4ece\u5546\u5bb6\u6210\u529f\u8f6c\u5411\u5e73\u53f0GMV\u4f18\u5316", "motivation": "\u4ee3\u7406\u5546\u52a1\u534f\u8bae\u4f5c\u4e3a\"\u5f00\u653e\"\u6807\u51c6\u51fa\u73b0\uff0c\u4f46\u88abShopify\u7b49\u65e9\u671f\u5206\u9500\u4f19\u4f34\u63a7\u5236\uff0c\u5bfc\u81f4\u6570\u636e\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u53ef\u80fd\u4f7f\u6fc0\u52b1\u4ece\u5173\u6ce8\u4e2a\u4f53\u5546\u5bb6\u6210\u529f\u8f6c\u5411\u5e73\u53f0\u5c42\u9762\u7684GMV\u4f18\u5316", "method": "\u672a\u660e\u786e\u8bf4\u660e\u5177\u4f53\u65b9\u6cd5\uff0c\u4f46\u5206\u6790\u4e86\u4ee3\u7406\u5546\u52a1\u534f\u8bae\u7684\u63a7\u5236\u7ed3\u6784\u548c\u6570\u636e\u4e0d\u5bf9\u79f0\u95ee\u9898", "result": "\u8bc6\u522b\u4e86\u4ee3\u7406\u5546\u52a1\u534f\u8bae\u4e2d\u7684\"\u5b88\u95e8\u4eba\"\u95ee\u9898\uff0c\u5373\u65e9\u671f\u5206\u9500\u4f19\u4f34\u63a7\u5236\u5f00\u653e\u6807\u51c6\uff0c\u9020\u6210\u6570\u636e\u4e0d\u5bf9\u79f0\uff0c\u53ef\u80fd\u626d\u66f2\u6fc0\u52b1\u7ed3\u6784", "conclusion": "\u4ee3\u7406\u5546\u52a1\u534f\u8bae\u5b58\u5728\u63a7\u5236\u96c6\u4e2d\u548c\u6570\u636e\u4e0d\u5bf9\u79f0\u98ce\u9669\uff0c\u9700\u8981\u5173\u6ce8\u6fc0\u52b1\u7ed3\u6784\u4ece\u5546\u5bb6\u6210\u529f\u5411\u5e73\u53f0GMV\u4f18\u5316\u7684\u8f6c\u53d8", "topic": "agent analysis"}}
{"id": "tldr.2602.a03e0b18", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2F%3Futm_source=tldr_ai%26utm_medium=sponsorship%26utm_campaign=tldr_ai/2/0100019c2e2dc809-31440d11-5406-467a-b782-5d91172fcd18-000000/Gbck2upZ6hzjX8dlvFYOX1Xc0m0bsSpjgKi7cjyxVoU=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2F%3Futm_source=tldr_ai%26utm_medium=sponsorship%26utm_campaign=tldr_ai/2/0100019c2e2dc809-31440d11-5406-467a-b782-5d91172fcd18-000000/Gbck2upZ6hzjX8dlvFYOX1Xc0m0bsSpjgKi7cjyxVoU=443", "authors": ["TLDR Newsletter"], "title": "Who's actually reviewing all that AI-generated code?", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2F%3Futm_source=tldr_ai%26utm_medium=sponsorship%26utm_campaign=tldr_ai/2/0100019c2e2dc809-31440d11-5406-467a-b782-5d91172fcd18-000000/Gbck2upZ6hzjX8dlvFYOX1Xc0m0bsSpjgKi7cjyxVoU=443", "summary": "Who's actually reviewing all that AI-generated code? (Sponsor) When devs use AI to generate thousands of lines of unverified code, you risk a codebase slopocalypse. The review step becomes your team's bottleneck.Greptile reviews each PR with full repo context and learns your team's conventions over time from comments, reactions, and what gets merged. It flags issues and suggests fixes that match your team, not generic best practices. \u2705 Trusted by engineering teams at NVIDIA, Scale AI, and Bre...", "source": "tldr", "AI": {"tldr": "Greptile\u662f\u4e00\u4e2aAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u901a\u8fc7\u5206\u6790\u5b8c\u6574\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u548c\u5b66\u4e60\u56e2\u961f\u89c4\u8303\uff0c\u81ea\u52a8\u5ba1\u67e5AI\u751f\u6210\u7684\u4ee3\u7801\uff0c\u9632\u6b62\u4f4e\u8d28\u91cf\u4ee3\u7801\u8fdb\u5165\u4ee3\u7801\u5e93", "motivation": "\u968f\u7740\u5f00\u53d1\u8005\u5927\u91cf\u4f7f\u7528AI\u751f\u6210\u4ee3\u7801\uff0c\u4ea7\u751f\u4e86\u5927\u91cf\u672a\u7ecf\u9a8c\u8bc1\u7684\u4ee3\u7801\uff0c\u5bfc\u81f4\u4ee3\u7801\u8d28\u91cf\u4e0b\u964d\u548c\u5ba1\u67e5\u74f6\u9888\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u5e2e\u52a9\u56e2\u961f\u9ad8\u6548\u5ba1\u67e5AI\u751f\u6210\u7684\u4ee3\u7801", "method": "Greptile\u901a\u8fc7\u5206\u6790\u5b8c\u6574\u4ee3\u7801\u5e93\u7684\u4e0a\u4e0b\u6587\uff0c\u5e76\u5b66\u4e60\u56e2\u961f\u7684\u89c4\u8303\uff08\u4ece\u8bc4\u8bba\u3001\u53cd\u5e94\u548c\u5408\u5e76\u7684\u4ee3\u7801\u4e2d\u5b66\u4e60\uff09\uff0c\u81ea\u52a8\u5ba1\u67e5PR\uff0c\u6807\u8bb0\u95ee\u9898\u5e76\u63d0\u4f9b\u7b26\u5408\u56e2\u961f\u7279\u5b9a\u5b9e\u8df5\u7684\u4fee\u590d\u5efa\u8bae", "result": "\u8be5\u5de5\u5177\u5df2\u88abNVIDIA\u3001Scale AI\u548cBre\u7b49\u5de5\u7a0b\u56e2\u961f\u4fe1\u4efb\u4f7f\u7528\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3AI\u751f\u6210\u4ee3\u7801\u7684\u5ba1\u67e5\u74f6\u9888\u95ee\u9898", "conclusion": "AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u5bf9\u4e8e\u7ba1\u7406AI\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u5e2e\u52a9\u56e2\u961f\u4fdd\u6301\u4ee3\u7801\u5e93\u8d28\u91cf\uff0c\u540c\u65f6\u9002\u5e94\u56e2\u961f\u7279\u5b9a\u7684\u5f00\u53d1\u89c4\u8303", "topic": "swe application"}}
{"id": "tldr.2602.d7174930", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fchangelog%2F2026-02-04-claude-and-codex-are-now-available-in-public-preview-on-github%2F%3Futm_source=tldrai/1/0100019c2e2dc809-31440d11-5406-467a-b782-5d91172fcd18-000000/1u36bo0ftvlUcqARbdKYVoIhMnf05sZb9wXFyhnQlIE=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fchangelog%2F2026-02-04-claude-and-codex-are-now-available-in-public-preview-on-github%2F%3Futm_source=tldrai/1/0100019c2e2dc809-31440d11-5406-467a-b782-5d91172fcd18-000000/1u36bo0ftvlUcqARbdKYVoIhMnf05sZb9wXFyhnQlIE=443", "authors": ["TLDR Newsletter"], "title": "Claude and Codex are now available in public preview on GitHub", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fchangelog%2F2026-02-04-claude-and-codex-are-now-available-in-public-preview-on-github%2F%3Futm_source=tldrai/1/0100019c2e2dc809-31440d11-5406-467a-b782-5d91172fcd18-000000/1u36bo0ftvlUcqARbdKYVoIhMnf05sZb9wXFyhnQlIE=443", "summary": "Claude and Codex are now available in public preview on GitHub (4 minute read) Claude by Anthropic and OpenAI Codex are now available for Copilot Pro+ and Enterprise customers on GitHub in public preview. Users can start sessions and assign tasks to these coding agents from the web, mobile app, or VS Code without additional subscriptions. All agent actions, like drafting pull requests and task prioritization, can be managed through GitHub's existing infrastructure.", "source": "tldr", "AI": {"tldr": "GitHub\u5ba3\u5e03Claude\u548cCodex\u5728Copilot Pro+\u548c\u4f01\u4e1a\u7248\u4e2d\u516c\u5f00\u9884\u89c8\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u591a\u79cd\u5e73\u53f0\u4f7f\u7528\u8fd9\u4e9b\u7f16\u7801\u52a9\u624b\uff0c\u65e0\u9700\u989d\u5916\u8ba2\u9605", "motivation": "GitHub\u5e0c\u671b\u4e3a\u5176Copilot\u7528\u6237\u63d0\u4f9b\u66f4\u591aAI\u7f16\u7801\u52a9\u624b\u9009\u62e9\uff0c\u901a\u8fc7\u96c6\u6210Claude\u548cCodex\u6765\u589e\u5f3a\u5f00\u53d1\u4f53\u9a8c\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u6839\u636e\u9700\u6c42\u9009\u62e9\u4e0d\u540c\u7684AI\u52a9\u624b", "method": "\u5c06Anthropic\u7684Claude\u548cOpenAI\u7684Codex\u96c6\u6210\u5230GitHub Copilot\u5e73\u53f0\u4e2d\uff0c\u901a\u8fc7\u73b0\u6709GitHub\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u6240\u6709\u4ee3\u7406\u64cd\u4f5c\uff0c\u652f\u6301\u7f51\u9875\u3001\u79fb\u52a8\u5e94\u7528\u548cVS Code\u591a\u79cd\u8bbf\u95ee\u65b9\u5f0f", "result": "Pro+\u548c\u4f01\u4e1a\u5ba2\u6237\u73b0\u5728\u53ef\u4ee5\u5728GitHub\u4e0a\u4f7f\u7528Claude\u548cCodex\u8fdb\u884c\u7f16\u7801\u4efb\u52a1\uff0c\u5305\u62ec\u8d77\u8349PR\u3001\u4efb\u52a1\u4f18\u5148\u7ea7\u6392\u5e8f\u7b49\uff0c\u6240\u6709\u64cd\u4f5c\u90fd\u5728GitHub\u73b0\u6709\u6846\u67b6\u5185\u5b8c\u6210", "conclusion": "GitHub\u901a\u8fc7\u96c6\u6210\u591a\u4e2aAI\u7f16\u7801\u52a9\u624b\u6269\u5c55\u4e86Copilot\u529f\u80fd\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u591a\u9009\u62e9\u548c\u7075\u6d3b\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7edf\u4e00\u7684\u7ba1\u7406\u4f53\u9a8c", "topic": "code agent"}}
{"id": "tldr.2602.7f77289b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwindsurf.com%2Fblog%2Fwindsurf-tab-2%3Futm_source=tldrai/1/0100019c2e2dc809-31440d11-5406-467a-b782-5d91172fcd18-000000/tQPxY9XxmL4G_xyZw0hSt8yQOVOv-XxPU_zrMnMK8rI=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwindsurf.com%2Fblog%2Fwindsurf-tab-2%3Futm_source=tldrai/1/0100019c2e2dc809-31440d11-5406-467a-b782-5d91172fcd18-000000/tQPxY9XxmL4G_xyZw0hSt8yQOVOv-XxPU_zrMnMK8rI=443", "authors": ["TLDR Newsletter"], "title": "Windsurf Tab v2: 25-75% more accepted code with Variable Aggression", "comment": "Source: TLDR Newsletter, Date: 2026-02-05, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwindsurf.com%2Fblog%2Fwindsurf-tab-2%3Futm_source=tldrai/1/0100019c2e2dc809-31440d11-5406-467a-b782-5d91172fcd18-000000/tQPxY9XxmL4G_xyZw0hSt8yQOVOv-XxPU_zrMnMK8rI=443", "summary": "Windsurf Tab v2: 25-75% more accepted code with Variable Aggression (7 minute read) Windsurf's Tab feature was very well received, but it wasn't well maintained after launch. The team has now significantly improved the underlying model and context/data engineering pipeline. This has led to direct Pareto improvements across all of Windsurf's metrics, with an average 54% increase in characters per predict. Tab now uses variable aggression to tailor the experience to each user's preferences.", "source": "tldr", "AI": {"tldr": "Windsurf Tab v2\u901a\u8fc7\u6539\u8fdb\u5e95\u5c42\u6a21\u578b\u548c\u4e0a\u4e0b\u6587/\u6570\u636e\u5de5\u7a0b\u7ba1\u9053\uff0c\u5b9e\u73b0\u4e86\u5e73\u574754%\u7684\u5b57\u7b26\u9884\u6d4b\u589e\u957f\uff0c\u5e76\u91c7\u7528\u53ef\u53d8\u653b\u51fb\u6027\u7b56\u7565\u4e2a\u6027\u5316\u7528\u6237\u4f53\u9a8c\uff0c\u4f7f\u4ee3\u7801\u63a5\u53d7\u7387\u63d0\u534725-75%", "motivation": "Windsurf\u7684Tab\u529f\u80fd\u867d\u7136\u53d7\u5230\u6b22\u8fce\uff0c\u4f46\u53d1\u5e03\u540e\u7ef4\u62a4\u4e0d\u8db3\u3002\u56e2\u961f\u5e0c\u671b\u901a\u8fc7\u6539\u8fdb\u5e95\u5c42\u6280\u672f\u548c\u4e2a\u6027\u5316\u7b56\u7565\u6765\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u4ee3\u7801\u63a5\u53d7\u7387", "method": "1. \u663e\u8457\u6539\u8fdb\u5e95\u5c42\u6a21\u578b\uff1b2. \u4f18\u5316\u4e0a\u4e0b\u6587/\u6570\u636e\u5de5\u7a0b\u7ba1\u9053\uff1b3. \u5f15\u5165\u53ef\u53d8\u653b\u51fb\u6027\u7b56\u7565\uff0c\u6839\u636e\u7528\u6237\u504f\u597d\u4e2a\u6027\u5316\u8c03\u6574\u4f53\u9a8c", "result": "1. \u5728\u6240\u6709\u6307\u6807\u4e0a\u5b9e\u73b0\u5e15\u7d2f\u6258\u6539\u8fdb\uff1b2. \u5e73\u5747\u5b57\u7b26\u9884\u6d4b\u589e\u957f54%\uff1b3. \u4ee3\u7801\u63a5\u53d7\u7387\u63d0\u534725-75%\uff1b4. \u901a\u8fc7\u53ef\u53d8\u653b\u51fb\u6027\u7b56\u7565\u63d0\u4f9b\u4e2a\u6027\u5316\u4f53\u9a8c", "conclusion": "Windsurf Tab v2\u901a\u8fc7\u6280\u672f\u6539\u8fdb\u548c\u4e2a\u6027\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u5b9e\u73b0\u4e86\u4ee3\u7801\u63a5\u53d7\u7387\u7684\u5927\u5e45\u63d0\u5347", "topic": "code agent"}}
