<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.SE](#cs.SE) [Total: 16]
- [wechat.article](#wechat.article) [Total: 31]
- [cs.AI](#cs.AI) [Total: 53]
- [tldr.article](#tldr.article) [Total: 11]
- [cs.LG](#cs.LG) [Total: 38]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?](https://arxiv.org/abs/2509.22715)
*Jiho Park,Jongyoon Song,Minjin Choi,Kyuho Heo,Taehun Huh,Ji Won Kim*

Main category: cs.CL

TL;DR: TRUEBench是一个专为LLM生产力助手设计的新基准测试，通过多语言支持、隐式约束捕获和多轮对话复杂性来提供更真实的评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估LLM真实世界指令遵循能力方面存在不足，包括缺乏多语言性、未能捕获用户请求中的隐式约束，以及忽视多轮对话的复杂性。

Method: 引入TRUEBench基准测试，包含12种语言的输入提示、多语言指令、严格的评估标准来捕获显式和隐式约束，以及具有累积约束和上下文切换的复杂多轮对话场景。使用LLM验证器来确保评估的可靠性。

Result: 实验表明TRUEBench比现有基准测试更具挑战性，即使是像OpenAI o1这样的强大模型也仅达到69.07%的总体通过率。

Conclusion: TRUEBench提供了对LLM在实际生产力环境中能力和局限性的严格且真实的评估。

Abstract: Large language models (LLMs) are increasingly integral as productivity
assistants, but existing benchmarks fall short in rigorously evaluating their
real-world instruction-following capabilities. Current benchmarks often (i)
lack sufficient multilinguality, (ii) fail to capture the implicit constraints
inherent in user requests, and (iii) overlook the complexities of multi-turn
dialogue. To address these critical gaps and provide a more realistic
assessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation
Benchmark)1, a novel benchmark specifically designed for LLM-based productivity
assistants. TRUEBench distinguishes itself by featuring input prompts across 12
languages, incorporating intra-instance multilingual instructions, employing
rigorous evaluation criteria to capture both explicit and implicit constraints,
and including complex multi-turn dialogue scenarios with both accumulating
constraints and context switches. Furthermore, to ensure reliability in
evaluation, we refined constraints using an LLM validator. Extensive
experiments demonstrate that TRUEBench presents significantly greater
challenges than existing benchmarks; for instance, a strong model like OpenAI
o1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and
realistic assessment of LLMs in practical productivity settings, highlighting
their capabilities and limitations.

</details>


### [2] [ML2B: Multi-Lingual ML Benchmark For AutoML](https://arxiv.org/abs/2509.22768)
*Ekaterina Trofimova,Zosia Shamina,Maria Selifanova,Artem Zaitsev,Remi Savchuk,Maxim Minets,Daria Ozerova,Emil Sataev,Denis Zuenko,Andrey E. Ustyuzhanin*

Main category: cs.CL

TL;DR: 提出了ML2B，首个用于评估多语言机器学习代码生成的基准，包含30个Kaggle竞赛翻译成13种语言，结果显示非英语任务性能下降15-45%。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习代码生成基准主要局限于英语，忽视了机器学习研究和实践的全球性和多语言特性。

Method: 构建ML2B基准，包含30个Kaggle竞赛翻译成13种自然语言，涵盖表格、文本和图像数据类型，使用AIDE自动化框架进行端到端评估。

Result: 结果显示在非英语任务上存在显著的15-45%性能下降，突显了多语言表示学习在代码生成中的关键挑战。

Conclusion: 多语言机器学习代码生成面临重大挑战，需要改进多语言表示学习能力，ML2B基准为未来研究提供了基础。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating machine learning (ML) code, enabling end-to-end pipeline
construction from natural language instructions. However, existing benchmarks
for ML code generation are mainly restricted to English, overlooking the global
and multilingual nature of ML research and practice. To address this gap, we
present ML2B, the first benchmark for evaluating multilingual ML code
generation. ML2B consists of 30 Kaggle competitions translated into 13 natural
languages, covering tabular, text, and image data types, with structured
metadata and validated human-reviewed translations. For evaluation, we employ
AIDE, an automated framework for end-to-end assessment of data science
pipelines, and provide insights into cross-lingual model performance. Our
results reveal substantial 15-45% performance degradation on non-English tasks,
highlighting critical challenges in multilingual representation learning for
code generation. The benchmark, evaluation framework, and comprehensive results
are made available through our GitHub repository to facilitate future research
in multilingual ML code generation: https://github.com/enaix/ml2b.

</details>


### [3] [EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation](https://arxiv.org/abs/2509.22812)
*Kai Zhang,Christopher Malon,Lichao Sun,Martin Renqiang Min*

Main category: cs.CL

TL;DR: EditGRPO是一种混合策略强化学习算法，通过临床动机奖励优化放射学报告生成，在多个数据集上显著提升性能并展现优越的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在放射学报告生成中的监督微调目标与临床效果未明确对齐，需要专门优化生成质量的算法。

Method: 提出EditGRPO混合策略强化学习算法，结合在线策略探索和离线策略指导，在训练过程中注入句子级详细修正，解决RL中的探索困境和采样效率问题。

Result: 在四个主要胸部X光报告生成数据集上，EditGRPO相比SFT和普通GRPO基线平均提升3.4%的性能指标，在未见数据集上平均提升5.9%。

Conclusion: EditGRPO能有效优化放射学报告生成质量，具有优越的临床对齐性和跨域泛化能力。

Abstract: Radiology report generation requires advanced medical image analysis,
effective temporal reasoning, and accurate text generation. Although recent
innovations, particularly multimodal large language models (MLLMs), have shown
improved performance, their supervised fine-tuning (SFT) objective is not
explicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,
a mixed-policy reinforcement learning (RL) algorithm designed specifically to
optimize the generation through clinically motivated rewards. EditGRPO
integrates on-policy exploration with off-policy guidance by injecting
sentence-level detailed corrections during training rollouts. This mixed-policy
approach addresses the exploration dilemma and sampling efficiency issues
typically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with
supervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO
baselines, achieving an average improvement of 3.4% in CheXbert, GREEN,
Radgraph, and RATEScore metrics across four major chest X-ray report generation
datasets. Notably, EditGRPO also demonstrates superior out-of-domain
generalization, with an average performance gain of 5.9% on unseen datasets.

</details>


### [4] [Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning](https://arxiv.org/abs/2509.22824)
*Chi Ruan,Dongfu Jiang,Yubo Wang,Wenhu Chen*

Main category: cs.CL

TL;DR: 提出了Critique Reinforcement Learning (CRL)方法，通过让模型对(问题，解决方案)对生成批判，仅基于批判的最终判断标签是否与真实标签一致来给予奖励。基于此开发了Critique-Coder模型，在标准RL数据中替换20%为CRL数据进行混合训练，在多个基准测试中优于纯RL模型。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习主要关注生成响应，缺乏明确培养批判或反思能力的机制。受Critique-Fine-Tuning和Critique-Guided-Distillation等研究的启发，希望开发能明确教授LLMs如何进行批判的方法。

Method: 提出CRL方法，模型任务是为给定(问题，解决方案)对生成批判，奖励仅基于批判的最终判断标签是否与真实标签一致。在此基础上开发Critique-Coder模型，使用RL和CRL混合训练(20%标准RL数据替换为CRL数据)。

Result: Critique-Coder在所有评估基准上持续优于纯RL基线模型。Critique-Coder-8B在LiveCodeBench(v5)上达到超过60%，优于DeepCoder-14B和GPT-o1等推理模型。在BBEH数据集的逻辑推理任务上也表现更好，表明CRL在编码数据集上的应用增强了通用推理能力。

Conclusion: CRL作为标准RL的很好补充，能够增强LLMs的推理能力，且这种批判和推理能力可迁移到广泛任务中。

Abstract: Reinforcement Learning (RL) has emerged as a popular training paradigm,
particularly when paired with reasoning models. While effective, it primarily
focuses on generating responses and lacks mechanisms to explicitly foster
critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)
and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly
teaching LLMs how to critique. Motivated by them, we propose Critique
Reinforcement Learning (CRL), where the model is tasked with generating a
critique for a given (question, solution) pair. The reward is determined solely
by whether the final judgment label $c \in \{\texttt{True}, \texttt{False}\}$
of the generated critique aligns with the ground-truth judgment $c^*$. Building
on this point, we introduce \textsc{Critique-Coder}, which is trained on a
hybrid of RL and CRL by substituting 20\% of the standard RL data with CRL
data. We fine-tune multiple models (\textsc{Critique-Coder}) and evaluate them
on different benchmarks to show their advantages over RL-only models. We show
that \textsc{Critique-Coder} consistently outperforms RL-only baselines on all
the evaluated benchmarks. Notably, our \textsc{Critique-Coder-8B} can reach
over 60\% on LiveCodeBench (v5), outperforming other reasoning models like
DeepCoder-14B and GPT-o1. Beyond code generation, \textsc{Critique-Coder} also
demonstrates enhanced general reasoning abilities, as evidenced by its better
performance on logic reasoning tasks from the BBEH dataset. This indicates that
the application of CRL on coding datasets enhances general reasoning and
critique abilities, which are transferable across a broad range of tasks.
Hence, we believe that CRL works as a great complement to standard RL for LLM
reasoning.

</details>


### [5] [ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents](https://arxiv.org/abs/2509.22830)
*Hwan Chang,Yonghyun Jun,Hwanhee Lee*

Main category: cs.CL

TL;DR: ChatInject是一种新型的间接提示注入攻击，通过模拟聊天模板格式和多轮对话策略，显著提高了对LLM代理的攻击成功率，现有防御措施对此类攻击效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的代理系统在外部环境中的部署增加，出现了新的攻击面。现有研究主要关注纯文本注入攻击，但LLM对结构化聊天模板的依赖及其在上下文操纵中的脆弱性尚未充分探索。

Method: 提出ChatInject攻击方法：1）将恶意载荷格式化为原生聊天模板格式；2）开发多轮对话变体，通过多轮对话引导代理接受可疑操作；3）在多个前沿LLM上进行全面实验验证。

Result: 1）ChatInject攻击成功率显著高于传统方法（AgentDojo从5.18%提升至32.05%，InjecAgent从15.13%提升至45.90%）；2）聊天模板载荷具有强跨模型迁移性；3）现有提示防御措施对此类攻击效果有限。

Conclusion: 当前代理系统存在严重安全漏洞，ChatInject攻击揭示了LLM对结构化模板的依赖性和上下文操纵的脆弱性，需要开发新的防御机制。

Abstract: The growing deployment of large language model (LLM) based agents that
interact with external environments has created new attack surfaces for
adversarial manipulation. One major threat is indirect prompt injection, where
attackers embed malicious instructions in external environment output, causing
agents to interpret and execute them as if they were legitimate prompts. While
previous research has focused primarily on plain-text injection attacks, we
find a significant yet underexplored vulnerability: LLMs' dependence on
structured chat templates and their susceptibility to contextual manipulation
through persuasive multi-turn dialogues. To this end, we introduce ChatInject,
an attack that formats malicious payloads to mimic native chat templates,
thereby exploiting the model's inherent instruction-following tendencies.
Building on this foundation, we develop a persuasion-driven Multi-turn variant
that primes the agent across conversational turns to accept and execute
otherwise suspicious actions. Through comprehensive experiments across frontier
LLMs, we demonstrate three critical findings: (1) ChatInject achieves
significantly higher average attack success rates than traditional prompt
injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%
to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong
performance at average 52.33% success rate on InjecAgent, (2)
chat-template-based payloads demonstrate strong transferability across models
and remain effective even against closed-source LLMs, despite their unknown
template structures, and (3) existing prompt-based defenses are largely
ineffective against this attack approach, especially against Multi-turn
variants. These findings highlight vulnerabilities in current agent systems.

</details>


### [6] [Infusing Theory of Mind into Socially Intelligent LLM Agents](https://arxiv.org/abs/2509.22887)
*EunJeong Hwang,Yuwei Yin,Giuseppe Carenini,Peter West,Vered Shwartz*

Main category: cs.CL

TL;DR: 该论文提出了一种基于心智理论(ToM)的对话代理ToMAgent，通过显式生成心理状态来提升对话效果和达成目标的能力。


<details>
  <summary>Details</summary>
Motivation: 当前聊天机器人和基于LLM的社交代理通常不整合心智理论，而心智理论是人类社会智能的关键方面。

Method: 引入ToMAgent(ToMA)，通过将心智理论与对话前瞻配对，生成对达成对话目标最有用的心理状态。

Result: 在Sotopia交互式社交评估基准上的实验表明，该方法在一系列基线中表现出色，展现出更具战略性、目标导向的推理行为。

Conclusion: 研究结果表明在整合心智理论以构建具有社会智能的LLM代理方面取得了进展。

Abstract: Theory of Mind (ToM)-an understanding of the mental states of others-is a key
aspect of human social intelligence, yet, chatbots and LLM-based social agents
do not typically integrate it. In this work, we demonstrate that LLMs that
explicitly use ToM get better at dialogue, achieving goals more effectively.
After showing that simply prompting models to generate mental states between
dialogue turns already provides significant benefit, we further introduce
ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM
with dialogue lookahead to produce mental states that are maximally useful for
achieving dialogue goals. Experiments on the Sotopia interactive social
evaluation benchmark demonstrate the effectiveness of our method over a range
of baselines. Comprehensive analysis shows that ToMA exhibits more strategic,
goal-oriented reasoning behaviors, which enable long-horizon adaptation, while
maintaining better relationships with their partners. Our results suggest a
step forward in integrating ToM for building socially intelligent LLM agents.

</details>


### [7] [Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents](https://arxiv.org/abs/2509.23040)
*Yaorui Shi,Yuxin Chen,Siyuan Wang,Sihang Li,Hengxing Cai,Qi Gu,Xiang Wang,An Zhang*

Main category: cs.CL

TL;DR: 提出了ReMemR1记忆增强代理，通过回调增强的记忆机制和强化学习多级奖励来解决长上下文问答中的信息分散问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在长上下文问答中面临关键证据分散在数百万token中的挑战，现有的"边读边记"方法存在前向处理不可逆、信息覆盖丢失和稀疏强化学习信号等问题。

Method: ReMemR1采用回调增强的记忆机制，允许从整个记忆历史中选择性检索，支持非线性推理和早期证据重访。同时提出强化学习多级奖励(RLMLR)，结合最终答案奖励和密集的步骤级信号来指导有效的记忆使用。

Result: 在长文档问答实验上相比现有基于记忆的方法取得了显著提升，验证了ReMemR1作为长上下文推理代理的有效解决方案。

Conclusion: ReMemR1通过改进的记忆机制和强化学习训练方法，有效缓解了信息退化问题，改善了监督信号，支持多跳记忆利用。

Abstract: Large language models face challenges in long-context question answering,
where key evidence of a query may be dispersed across millions of tokens.
Existing works equip large language models with a memory corpus that is
dynamically updated during a single-pass document scan, also known as the
"memorize while reading" methods. While this approach scales efficiently, it
suffers from irreversible forward-only processing, information loss through
overwriting, and sparse reinforcement learning signals. To tackle these
challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced
memory that allows selective retrieval from the entire memory history and
allows non-linear reasoning and revisiting of early evidence. To further
strengthen training, we propose Reinforcement Learning with Multi-Level Rewards
(RLMLR), which combines final-answer rewards with dense, step-level signals
that guide effective memory use. Together, these contributions mitigate
information degradation, improve supervision, and support multi-hop memory
utilizing. Experiments on long-document QA show significant gains over existing
memory-based approaches, which validates ReMemR1 as an effective solution for
long-context reasoning agents.

</details>


### [8] [Peacemaker or Troublemaker: How Sycophancy Shapes Multi-Agent Debate](https://arxiv.org/abs/2509.23055)
*Binwei Yao,Chao Shang,Wanyu Du,Jianfeng He,Ruixue Lian,Yi Zhang,Hang Su,Sandesh Swamy,Yanjun Qi*

Main category: cs.CL

TL;DR: 提出了首个针对多智能体辩论系统中奉承行为的操作框架，包括正式定义、评估指标和系统研究，揭示了奉承行为是导致辩论失败的核心模式，并提出了设计原则。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在辩论系统中表现出过度奉承的倾向，这会导致辩论过早达成共识，削弱多智能体辩论的优势。目前对智能体间奉承行为在辩论中的影响了解不足。

Method: 提出了一个操作框架：1）为多智能体辩论系统制定奉承行为的正式定义；2）开发新指标评估智能体奉承水平及其对信息交换的影响；3）系统研究不同角色智能体的奉承水平如何影响分散式和集中式辩论框架的结果。

Result: 发现奉承行为是核心失败模式，会在达成正确结论前放大分歧崩溃，导致准确率低于单智能体基线，并源于不同的辩手驱动和法官驱动失败模式。

Conclusion: 基于研究发现，提出了可操作的多智能体辩论系统设计原则，有效平衡智能体互动中的生产性分歧与合作。

Abstract: Large language models (LLMs) often display sycophancy, a tendency toward
excessive agreeability. This behavior poses significant challenges for
multi-agent debating systems (MADS) that rely on productive disagreement to
refine arguments and foster innovative thinking. LLMs' inherent sycophancy can
collapse debates into premature consensus, potentially undermining the benefits
of multi-agent debate. While prior studies focus on user--LLM sycophancy, the
impact of inter-agent sycophancy in debate remains poorly understood. To
address this gap, we introduce the first operational framework that (1)
proposes a formal definition of sycophancy specific to MADS settings, (2)
develops new metrics to evaluate the agent sycophancy level and its impact on
information exchange in MADS, and (3) systematically investigates how varying
levels of sycophancy across agent roles (debaters and judges) affects outcomes
in both decentralized and centralized debate frameworks. Our findings reveal
that sycophancy is a core failure mode that amplifies disagreement collapse
before reaching a correct conclusion in multi-agent debates, yields lower
accuracy than single-agent baselines, and arises from distinct debater-driven
and judge-driven failure modes. Building on these findings, we propose
actionable design principles for MADS, effectively balancing productive
disagreement with cooperation in agent interactions.

</details>


### [9] [From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents](https://arxiv.org/abs/2509.23071)
*Muzhi Li,Jinhu Qi,Yihong Wu,Minghao Zhao,Liheng Ma,Yifan Li,Xinyu Wang,Yingxue Zhang,Ho-fung Leung,Irwin King*

Main category: cs.CL

TL;DR: EviPath提出了一种基于证据锚定的推理路径合成范式，用于解决RAG代理开发中过程级监督缺失的问题，通过任务分解、忠实子问题回答和对话微调，显著提升了代理的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RAG代理开发缺乏过程级监督，难以有效指导任务分解、检索器调用和逐步决策等能力。强化学习存在稀疏奖励问题，现有数据合成方法只能生成思维链而无法建模环境交互。

Method: EviPath包含三个核心组件：(1)溯因子任务规划：将问题分解为子问题并基于依赖关系迭代规划最优解路径；(2)忠实子问题回答：使用支持证据构建代理环境来生成推理思路和答案；(3)对话微调：将完整的代理-环境交互轨迹格式化为适合监督微调的对话格式。

Result: 在广泛使用的问答基准测试中，使用EviPath合成数据训练的8B参数模型显著且持续优于最先进的基线方法，在开放域问答中实现了14.7%的绝对EM增益。

Conclusion: EviPath使LLM能够直接从合成数据中学习复杂推理和工具使用能力，为RAG代理开发提供了有效的解决方案。

Abstract: Retrieval-augmented generation agents development is hindered by the lack of
process-level supervision to effectively guide agentic capabilities like task
decomposition, retriever invocation, and stepwise decision-making. While
reinforcement learning offers a potential solution, it suffers from sparse
rewards and the limited reasoning capabilities of large language models (LLMs).
Meanwhile, existing data synthesis methods only produce chain-of-thought
rationales and fail to model environmental interactions. In this paper, we
propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG
agent development. EviPath comprises: (i) Abductive Subtask Planning, which
decomposes the problem into sub-questions and iteratively plans an optimal
solution path based on the dependencies between them; (ii) Faithful
Sub-question Answering, which uses supporting evidence to construct a proxy
environment to generate reasoning thoughts and answers for each sub-question;
and (iii) Conversational Fine-Tuning, which formats the complete
agent-environment interaction trajectory into a dialogue format suitable for
Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and
tool-use capabilities directly from synthesized data. Extensive experiments on
widely-used question-answering benchmarks show that an 8B parameter model
trained with EviPath-synthesized data significantly and consistently
outperforms state-of-the-art baselines with a double-digit absolute EM gain of
14.7% in open-domain question answering.

</details>


### [10] [Non-Collaborative User Simulators for Tool Agents](https://arxiv.org/abs/2509.23124)
*Jeonghoon Shim,Woojung Song,Cheyon Jin,Seungwon KooK,Yohan Jo*

Main category: cs.CL

TL;DR: 提出了一种用于工具代理的非协作用户模拟方法，能够模拟四种非协作行为：请求不可用服务、偏离主题对话、表达不耐烦和提供不完整话语。


<details>
  <summary>Details</summary>
Motivation: 现有用户模拟器过于友好且只展示合作行为，无法训练和测试工具代理应对现实世界中的非协作用户。

Method: 提出新颖的用户模拟器架构，模拟四种非协作行为类别，同时可靠传递完成任务所需的所有意图和信息。

Result: 在MultiWOZ和τ-bench上的实验显示，最先进的工具代理在遇到非协作用户时性能显著下降，出现幻觉增加和对话中断等问题。

Conclusion: 贡献了一个易于扩展的用户模拟框架，帮助研究社区开发工具代理并在具有挑战性的真实世界条件下预先诊断它们。

Abstract: Non-Collaborative User Simulators for Tool Agents Download PDF Jeonghoon
Shim, Woojung Song, Cheyon Jin, Seungwon KooK, Yohan Jo 19 Sept 2025 (modified:
25 Sept 2025)ICLR 2026 Conference SubmissionConference, AuthorsRevisionsCC BY
4.0 Keywords: Tool Agent, User Simulator, Non-collaborative User, Dialogue
Simulation TL;DR: A non-collaborative user simulation method for tool agent.
Abstract: Tool agents interact with users through multi-turn dialogues to
accomplish various tasks. Recent studies have adopted user simulation methods
to develop these agents in multi-turn settings. However, existing user
simulators tend to be agent-friendly, exhibiting only cooperative behaviors,
which fails to train and test agents against non-collaborative users in the
real world. To address this, we propose a novel user simulator architecture
that simulates four categories of non-collaborative behaviors: requesting
unavailable services, digressing into tangential conversations, expressing
impatience, and providing incomplete utterances. Our user simulator can
simulate challenging and natural non-collaborative behaviors while reliably
delivering all intents and information necessary to accomplish the task. Our
experiments on MultiWOZ and $\tau$-bench reveal significant performance
degradation in state-of-the-art tool agents when encountering non-collaborative
users. We provide detailed analyses of agents' weaknesses under each
non-collaborative condition, such as escalated hallucinations and dialogue
breakdowns. Ultimately, we contribute an easily extensible user simulation
framework to help the research community develop tool agents and preemptively
diagnose them under challenging real-world conditions within their own
services.

</details>


### [11] [Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.23140)
*Song Jin,Juntian Zhang,Yong Liu,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CL

TL;DR: TagPR是一个通过标记思维方法增强LLM个性化推理能力的训练框架，结合监督微调和多阶段强化学习，在LaMP基准测试中平均提升32.65%性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用推理方面表现出色，但在个性化推理方面存在不足，无法有效分析用户历史、推断独特偏好并生成定制化响应。

Method: 开发数据驱动管道自动生成和语义标记推理链，创建结构化数据集；采用协同训练策略，先进行监督微调建立基础推理模式，然后进行多阶段强化学习，使用包含标签约束和个性化奖励模型的复合奖励信号。

Result: 在公开LaMP基准和自建数据集上的实验表明，该方法达到最先进结果，在所有任务上平均比基础模型提升32.65%。

Conclusion: 结构化、可解释的推理是解锁LLM真正个性化能力的有效途径。

Abstract: Recent advancements have endowed Large Language Models (LLMs) with impressive
general reasoning capabilities, yet they often struggle with personalization
reasoning - the crucial ability to analyze user history, infer unique
preferences, and generate tailored responses. To address this limitation, we
introduce TagPR, a novel training framework that significantly enhances an
LLM's intrinsic capacity for personalization reasoning through a tagging the
thought approach. Our method first develops a data-driven pipeline to
automatically generate and semantically label reasoning chains, creating a
structured dataset that fosters interpretable reasoning. We then propose a
synergistic training strategy that begins with Supervised Fine-Tuning (SFT) on
this tagged data to establish foundational reasoning patterns, followed by a
multi-stage reinforcement learning (RL) process. This RL phase is guided by a
unique composite reward signal, which integrates tag-based constraints and a
novel Personalization Reward Model with User Embeddings (PRMU) to achieve
fine-grained alignment with user-specific logic. Extensive experiments on the
public LaMP benchmark and a self-constructed dataset demonstrate that our
approach achieves state-of-the-art results, delivering an average improvement
of 32.65% over the base model across all tasks. Our work validates that
structured, interpretable reasoning is a highly effective pathway to unlocking
genuine personalization capabilities in LLMs.

</details>


### [12] [TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F](https://arxiv.org/abs/2509.23686)
*Yifeng He,Luning Yang,Christopher Castro Gaw Gonzalo,Hao Chen*

Main category: cs.CL

TL;DR: TF-Bench是一个基于System F类型推断的LLM程序语义推理基准，通过去除语义无关的自然语言构建TF-Bench_pure变体，揭示了当前最先进LLM在程序语义推理方面的严重局限性。


<details>
  <summary>Details</summary>
Motivation: 当前代码推理基准缺乏形式化的程序中心演绎框架，无法评估模型是否真正理解程序语义还是仅利用自然语言和代码标记之间的表面关联。

Method: 基于System F类型推断构建TF-Bench基准，使用验证转换去除语义无关的自然语言创建TF-Bench_pure变体，并提出两个新指标评估鲁棒性和测试时推理效果。

Result: 最先进的LLM（Claude-3.7-sonnet）在TF-Bench_pure上仅达到55.85%的准确率，显示出严重的局限性。

Conclusion: 当前LLM在程序语义推理方面存在关键限制，需要未来研究重点关注。

Abstract: Large Language Models (LLMs) are increasingly integrated into the software
engineering ecosystem. Their test-time compute (TTC) reasoning capabilities
show significant potential for understanding program logic and semantics beyond
mere token recognition. However, current benchmarks for code reasoning lack a
formal, program-centric deductive framework to ensure sound evaluation, and are
incapable of assessing whether models genuinely reason about program semantics
or merely exploit superficial associations between natural language and code
tokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to
evaluate LLM reasoning based on type inference in System F, a task we refer to
as program semantics reasoning. By employing verified transformations to remove
semantically irrelevant natural language, we construct TF-Bench_pure, a purely
semantics-driven variant of TF-Bench. Our analysis reveals substantial
limitations in state-of-the-art LLMs, with the best-performing LLM
(Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure.
Additionally, we propose two novel metrics to assess robustness and the
effectiveness of test-time reasoning, underscoring critical limitations in
current LLM capabilities and highlighting essential directions for future
research.

</details>


### [13] [Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts](https://arxiv.org/abs/2509.23188)
*Guancheng Wan,Leixin Sun,Longxu Dou,Zitong Shi,Fang Wu,Eric Hanchen Jiang,Wenke Huang,Guibin Zhang,Hejia Geng,Xiangru Tang,Zhenfei Yin,Yizhou Sun,Wei Wang*

Main category: cs.CL

TL;DR: 提出了一个三阶段框架来诊断、定位和解决LLM多智能体系统中的层次指令冲突问题，通过注意力分析发现冲突主要出现在中间层，并开发了手术式对齐方法SAIL来改进指令层次遵从性。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统在复杂任务中表现出色，但存在系统性故障模式：在指令冲突（系统-用户、对等体之间）时的层次遵从性问题，且现有宏观指标无法揭示这些微观违规行为。

Method: 三阶段框架：(1)诊断-上下文角色遵从评分(CRAS)，将角色遵从分解为四个可测量维度；(2)定位-注意力漂移分析，发现指令冲突主要由中间层的注意力头解决；(3)对齐-手术式指令层对齐(SAIL)，仅在定位的焦点层安装LoRA，优化基于令牌权重的DPO风格偏好目标。

Result: 在标准基准和多智能体框架上，手术式方法显著提高了指令层次遵从性（如在AutoGen上MedQA任务提升5.60%），无需全模型微调。

Conclusion: 该工作揭示了多智能体系统中指令冲突的微观机制，并提供了有效的诊断和手术式对齐方法，为可靠性关键部署提供了解决方案。

Abstract: Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly
advanced collaborative reasoning, tool use, and role-specialized coordination
in complex tasks. However, reliability-critical deployment remains hindered by
a systemic failure mode: hierarchical compliance under instruction conflicts
(system-user, peer-peer), where agents misprioritize system-level rules in the
presence of competing demands. Moreover, widely used macro-level metrics (e.g.,
pass@k) obscure these micro-level violations and offer little actionable
guidance for remedy. In this work, we present a full-stack, three-stage
framework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a
query-wise, context-aware scoring metric that decomposes role adherence into
four measurable dimensions; (2) Localize - attention drift analysis revealing
that instruction conflicts are resolved by attention heads that are largely
concentrated in middle layers; (3) Align - Surgical Alignment of Instruction
Layers (SAIL), which installs LoRA only on the localized focal layers and
optimizes a token-weighted DPO-style preference objective that credits tokens
by their focal attentional contribution. Across standard benchmarks and MAS
frameworks, our surgical approach improves instruction hierarchy compliance
(e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.

</details>


### [14] [PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness](https://arxiv.org/abs/2509.23206)
*Huacan Chai,Zijie Cao,Maolin Ran,Yingxuan Yang,Jianghao Lin,pengxin,Hairui Wang,Renjie Ding,Ziyu Wan,Muning Wen,Weiwen Liu,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.CL

TL;DR: 提出了PARL-MT框架，将进度感知显式整合到LLM的多轮函数调用训练中，通过进度感知生成和进度感知引导的强化学习，显著提升了多轮任务执行的性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用如旅行规划或多阶段数据分析通常涉及多轮对话，现有方法要么将多轮训练简化为孤立单轮样本，忽略了任务级规划，要么使用端到端强化学习但存在冗余问题且缺乏明确的进度感知整合。

Method: PARL-MT框架包含：(i)进度感知生成(PAG)管道，自动构建将对话摘要与未来任务规划耦合的数据集；(ii)进度感知引导强化学习(PAG-RL)算法，将进度感知整合到RL训练中，减少上下文冗余并改善局部动作与全局任务完成的对齐。

Result: 在两个公共基准测试上的实证结果表明，PARL-MT显著优于现有方法，突显了进度感知在实现稳健高效多轮函数调用方面的有效性。

Conclusion: 进度感知对于实现稳健高效的多轮函数调用至关重要，PARL-MT框架通过显式整合进度感知显著提升了LLM在多轮任务执行中的性能。

Abstract: Large language models (LLMs) have achieved impressive success in single-turn
function calling, yet real-world applications such as travel planning or
multi-stage data analysis typically unfold across multi-turn conversations. In
these settings, LLMs must not only issue accurate function calls at each step
but also maintain progress awareness, the ability to summarize past
interactions and plan future actions to ensure coherent, long-horizon task
execution. Existing approaches, however, either reduce multi-turn training to
isolated single-turn samples, which neglects task-level planning, or employ
end-to-end reinforcement learning (RL) that struggles with redundancy and lacks
explicit integration of progress awareness. To overcome these limitations, we
introduce PARL-MT, a framework that explicitly incorporates progress awareness
into LLM training for multi-turn function calling. PARL-MT combines (i) a
Progress Awareness Generation (PAG) pipeline, which automatically constructs
datasets coupling conversation summaries with future task planning, and (ii) a
Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which
integrates progress awareness into RL training to reduce contextual redundancy
and improve alignment between local actions and global task completion.
Empirical results on two public benchmarks demonstrate that PARL-MT
significantly outperforms existing methods, highlighting the effectiveness of
progress awareness in enabling robust and efficient multi-turn function
calling.

</details>


### [15] [Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models](https://arxiv.org/abs/2509.23233)
*Sina J. Semnani,Jirayu Burapacheep,Arpandeep Khatua,Thanawan Atchariyachanvanit,Zheng Wang,Monica S. Lam*

Main category: cs.CL

TL;DR: 提出了CLAIRE系统，用于检测维基百科中的事实不一致性，并创建了首个真实维基百科不一致性基准WIKICOLLIDE。研究发现至少3.3%的英文维基百科事实存在矛盾，且现有自动化系统检测性能有限。


<details>
  <summary>Details</summary>
Motivation: 维基百科作为最大的开放知识语料库，被广泛用于训练大语言模型和检索增强生成系统，确保其准确性至关重要。但维基百科的准确性如何以及如何改进尚不清楚。

Method: 提出了CLAIRE系统，结合LLM推理与检索技术来发现潜在的不一致声明，并提供上下文证据供人工审查。通过随机抽样和CLAIRE辅助分析创建了WIKICOLLIDE基准。

Result: 用户研究中，87.5%的编辑报告使用CLAIRE时信心更高，参与者在相同时间内发现了64.7%更多的不一致性。至少3.3%的英文维基百科事实存在矛盾，这些不一致性传播到了7.3%的FEVEROUS和4.0%的AmbigQA示例中。最佳自动化系统的AUROC仅为75.1%。

Conclusion: 矛盾是维基百科中可测量的组成部分，基于LLM的系统如CLAIRE可以为编辑提供实用工具，帮助大规模改进知识一致性。

Abstract: Wikipedia is the largest open knowledge corpus, widely used worldwide and
serving as a key resource for training large language models (LLMs) and
retrieval-augmented generation (RAG) systems. Ensuring its accuracy is
therefore critical. But how accurate is Wikipedia, and how can we improve it?
  We focus on inconsistencies, a specific type of factual inaccuracy, and
introduce the task of corpus-level inconsistency detection. We present CLAIRE,
an agentic system that combines LLM reasoning with retrieval to surface
potentially inconsistent claims along with contextual evidence for human
review. In a user study with experienced Wikipedia editors, 87.5% reported
higher confidence when using CLAIRE, and participants identified 64.7% more
inconsistencies in the same amount of time.
  Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first
benchmark of real Wikipedia inconsistencies. Using random sampling with
CLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts
contradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS
and 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset
reveals substantial headroom: the best fully automated system achieves an AUROC
of only 75.1%.
  Our results show that contradictions are a measurable component of Wikipedia
and that LLM-based systems like CLAIRE can provide a practical tool to help
editors improve knowledge consistency at scale.

</details>


### [16] [Learning to Reason in Structured In-context Environments with Reinforcement Learning](https://arxiv.org/abs/2509.23330)
*Peng Yu,Zeyuan Zhao,Shao Zhang,Luoyi Fu,Xinbing Wang,Ying Wen*

Main category: cs.CL

TL;DR: 提出了SIE框架，通过从大规模结构化数据自动构建推理环境，解决了现有环境在可扩展性、泛化推理和可验证性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有数学和编码环境依赖专家标注难以扩展，游戏环境学到的技能过于专业化难以泛化，需要构建具备可扩展性、泛化推理和可验证性的理想LLM推理环境。

Method: SIE框架从大规模结构化数据自动构建推理环境，利用结构化数据的丰富组合模式支持泛化推理，通过显式模式和推理链实现基于规则的可验证性。

Result: SIE框架在领域内结构化推理上取得显著改进，学到的组合推理技能能有效泛化到领域外数学和逻辑推理任务，在信息受限的部分SIE中LLM能通过环境探索推断缺失信息。

Conclusion: SIE框架为LLM推理能力训练提供了可扩展、可泛化且可验证的环境构建方法，显著提升了推理性能和泛化能力。

Abstract: Large language models (LLMs) have achieved significant advancements in
reasoning capabilities through reinforcement learning (RL) via environmental
exploration. As the intrinsic properties of the environment determine the
abilities that LLMs can learn, the environment plays a important role in the RL
finetuning process. An ideal LLM reasoning environment should possess three
core characteristics: scalability, generalizable reasoning, and verifiability.
However, existing mathematical and coding environments are difficult to scale
due to heavy reliance on expert annotation, while the skills learned in
game-based environments are too specialized to generalize. To bridge this gap,
we introduce the \textbf{S}tructured \textbf{I}n-context \textbf{E}nvironment
(SIE) framework. SIE achieves scalability by automatically constructing
reasoning environments from large-scale structured data, where the rich
compositional patterns naturally support generalizable reasoning. Moreover, the
explicit schemas and reasoning chains in structured data provide a foundation
for rule-based verifiability. Experimental results show that SIE framework not
only achieves substantial improvements in in-domain structured reasoning, but
also enables the learned compositional reasoning skills to generalize
effectively to out-of-domain mathematical and logical reasoning tasks. We
further explored learning in information-limited partial SIEs and found that
LLMs can infer the missing information through exploring the environment,
leading to robust reasoning improvements and generalization performance.

</details>


### [17] [Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models](https://arxiv.org/abs/2509.23417)
*Rajaa El Hamdani,Samy Haffoudhi,Nils Holzenberger,Fabian Suchanek,Thomas Bonald,Fragkiskos D. Malliaros*

Main category: cs.CL

TL;DR: 提出检索约束解码(RCD)策略，通过限制模型输出为唯一表面形式来更准确评估语言模型的参数知识，在YAGO-QA数据集上验证了该方法能显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型编码了大量事实知识，但由于评估过于严格，许多正确答案因表面形式不同而被误判为错误，导致低估了模型的参数知识。

Method: 提出检索约束解码(RCD)策略，限制模型输出为唯一的表面形式；构建YAGO-QA数据集(19,137个通用知识问题)；评估从135M到70B参数的开源语言模型。

Result: 标准解码低估了模型知识：Llama-3.1-70B在普通解码下F1为32.3%，使用RCD后提升至46.0%；Llama-3.1-8B使用RCD达到33.0%，超过了大模型在普通解码下的表现。

Conclusion: RCD解码策略能更准确地评估语言模型的参数知识，揭示了标准评估方法对模型能力的低估问题。

Abstract: Language models (LMs) encode substantial factual knowledge, but often produce
answers judged as incorrect. We hypothesize that many of these answers are
actually correct, but are expressed in alternative surface forms that are
dismissed due to an overly strict evaluation, leading to an underestimation of
models' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD),
a decoding strategy that restricts model outputs to unique surface forms. We
introduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating
open-source LMs from 135M to 70B parameters, we show that standard decoding
undervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1
with vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0%
with RCD, outperforming the larger model under vanilla decoding. We publicly
share the code and dataset at https://github.com/Rajjaa/disambiguated-LLM.

</details>


### [18] [The Impact of Role Design in In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.23501)
*Hamidreza Rouzegar,Masoud Makrehchi*

Main category: cs.CL

TL;DR: 研究探讨了提示中角色设计对大型语言模型在零样本和少样本学习场景中的影响，发现基于角色的提示结构有潜力提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然提示工程已被广泛研究，但提示中角色设计的影响仍未充分探索。本研究旨在填补这一空白。

Method: 使用GPT-3.5、GPT-4o、Llama2-7b和Llama2-13b模型，在情感分析、文本分类、问答和数学推理等任务上评估角色配置对零样本和少样本学习的影响。

Result: 研究发现基于角色的提示结构能够增强大型语言模型的性能表现。

Conclusion: 角色设计在提示工程中具有重要价值，能够有效提升语言模型的学习能力。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to generate
predictions based on prompts without additional fine-tuning. While prompt
engineering has been widely studied, the impact of role design within prompts
remains underexplored. This study examines the influence of role configurations
in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from
OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models'
performance across datasets, focusing on tasks like sentiment analysis, text
classification, question answering, and math reasoning. Our findings suggest
the potential of role-based prompt structuring to enhance LLM performance.

</details>


### [19] [Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales](https://arxiv.org/abs/2509.23574)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: 提出MoRSD方法，通过选择高质量推理链来改进CoT蒸馏，相比基线在7个数据集上平均提升4.6%，使用更少但更优质的推理链。


<details>
  <summary>Details</summary>
Motivation: 现有CoT蒸馏方法低估推理链质量的重要性，主要关注数据量，可能将噪声或错误信息传递给学生模型。

Method: 提出模型导向推理链选择蒸馏(MoRSD)，能够识别和选择高质量推理链进行蒸馏，并提出推理链难度(RD)指标来衡量学生模型在给定推理链下生成正确答案的能力。

Result: 在三个任务的七个数据集上相比基线平均提升4.6%，通过控制推理链的准确性、多样性和难度，使用更少的推理链实现更好效果。

Conclusion: 少量高质量推理链比整个数据集更能提升学生模型的推理能力，该方法为高效CoT蒸馏提供了可行解决方案。

Abstract: Chain-of-thought (CoT) distillation aims to enhance small language models'
(SLMs) reasoning by transferring multi-step reasoning capability from the
larger teacher models. However, existing work underestimates rationale quality,
focusing primarily on data quantity, which may transfer noisy or incorrect
information to the student model. To address the above issues, we proposed
\textbf{M}odel-\textbf{O}riented \textbf{R}ationale \textbf{S}election
\textbf{D}istillation (MoRSD), which can discern and select high quality
rationales for distillation to improve performance further. We further propose
a Rationale Difficulty (RD) metric to measure the ability of the student model
to generate the correct answer under a given rationale. Compared to the
baseline, we achieved 4.6$\%$ average improvement on seven datasets over three
tasks, using fewer rationales by controlling their accuracy, diversity, and
difficulty. Our results reveal that a small portion of the high quality
rationales can enhance the reasoning ability of student models than the entire
dataset. Our method promises to be a possible solution for efficient CoT
distillation. Our code will be released in https://github.com/Leon221220/MoRSD.

</details>


### [20] [Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs](https://arxiv.org/abs/2509.23657)
*Shulin Huang,Yiran Ding,Junshu Pan,Yue Zhang*

Main category: cs.CL

TL;DR: 本文系统研究了强化学习（RL）和监督微调（SFT）在跨语言推理泛化方面的表现，发现RL不仅在准确性上优于SFT，而且在跨语言泛化能力上显著更强，特别是在非英语数据上训练时表现更好。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在提升大语言模型复杂推理能力时，与监督微调相比在跨语言泛化方面的差异，这一领域尚未被充分研究。

Method: 使用Qwen2.5-3B-Base作为基础模型，在多种多语言推理基准（数学推理、常识推理、科学推理）上进行实验，比较RL和SFT的表现，并进行机制分析。

Result: RL在准确性和跨语言泛化能力上都显著优于SFT；在非英语数据上训练的RL模型比在英语数据上训练的模型表现更好，这一现象在SFT中未观察到。

Conclusion: RL使模型具备更鲁棒的推理策略，为更公平有效的多语言推理提供了重要指导。

Abstract: Enhancing the complex reasoning capabilities of Large Language Models (LLMs)
attracts widespread attention. While reinforcement learning (RL) has shown
superior performance for improving complex reasoning, its impact on
cross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains
unexplored. We present the first systematic investigation into cross-lingual
reasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation
model, we conduct experiments on diverse multilingual reasoning benchmarks,
including math reasoning, commonsense reasoning, and scientific reasoning. Our
investigation yields two significant findings: (1) Tuning with RL not only
achieves higher accuracy but also demonstrates substantially stronger
cross-lingual generalization capabilities compared to SFT. (2) RL training on
non-English data yields better overall performance and generalization than
training on English data, which is not observed with SFT. Furthermore, through
comprehensive mechanistic analyses, we explore the underlying factors of RL's
superiority and generalization across languages. Our results provide compelling
evidence that RL enables the model with more robust reasoning strategies,
offering crucial guidance for more equitable and effective multilingual
reasoning.

</details>


### [21] [Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step](https://arxiv.org/abs/2509.23924)
*Jingyi Yang,Guanxu Chen,Xuhao Hu,Jing Shao*

Main category: cs.CL

TL;DR: 本文提出了EOS早期拒绝和递增步长解码调度器，以及一致性轨迹组相对策略优化方法，用于改进掩码扩散语言模型的解码策略和强化学习训练，在推理任务上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型具有并行解码、灵活生成顺序和较少推理步骤等优势，但其解码策略和强化学习算法研究不足。直接将自回归模型的技术迁移到MDLMs存在训练-推理不一致的问题。

Method: 提出了EOSER和ASS解码调度器来优化MDLMs的完整扩散式解码，以及CJ-GRPO强化学习算法来确保滚动轨迹与优化轨迹的一致性。

Result: 在数学和规划等推理任务上使用LLaDA-8B-Instruct进行实验，结果表明所提方法能够有效且高效地驯服MDLMs。

Conclusion: EOSER、ASS机制和CJ-GRPO算法为MDLMs的有效训练和解码提供了有前景的解决方案。

Abstract: Masked diffusion language models (MDLMs) have recently emerged as a promising
alternative to autoregressive (AR) language models, offering properties such as
parallel decoding, flexible generation orders, and the potential for fewer
inference steps. Despite these advantages, decoding strategies and
reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.
A naive approach is to directly transfer techniques well-established for AR
models to MDLMs. However, this raises an immediate question: Is such a naive
transfer truly optimal? For example, 1) Block-wise and semi-AR decoding
strategies are not employed during the training of MDLMs, so why do they
outperform full diffusion-style decoding during inference? 2) Applying RL
algorithms designed for AR models directly to MDLMs exhibits a
training-inference inconsistency, since MDLM decoding are non-causal
(parallel). This results in inconsistencies between the rollout trajectory and
the optimization trajectory. To address these challenges, we propose EOS Early
Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which
unlock the potential of MDLMs to perform full diffusion-style decoding,
achieving competitive performance with fewer decoding steps. Additionally, we
introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)
for taming MDLMs, which emphasizes the consistency between rollout trajectory
and optimization trajectory, and reduces the optimization errors caused by
skip-step optimization. We conduct extensive experiments on reasoning tasks,
such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The
results demonstrate that the proposed EOSER and ASS mechanisms, together with
CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.
Code: https://github.com/yjyddq/EOSER-ASS-RL.

</details>


### [22] [ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation](https://arxiv.org/abs/2509.23979)
*Haonan Wang,Junfeng Sun,Xingdi Yuan,Ruoyao Wang,Ziang Xiao*

Main category: cs.CL

TL;DR: 提出了ByteSized32Refactored，这是一个重构的、模块化的文本游戏生成框架，通过创建GameBasic.py基础库和7个基类，将代码量从2万行减少到1万行，提高了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 模拟交互式世界模型仍然是大型语言模型的核心挑战，需要更高效和可扩展的文本游戏生成方法。

Method: 重构ByteSized32语料库，优化代码结构，创建GameBasic.py基础库，抽象出7个基类（如GameObject等）作为可复用模块，实现模块化和集中化设计。

Result: 使用GPT-4o进行实验，在未见过的场景中生成的文本游戏在四个评估维度中的两个维度上质量有所提升，但在另外两个维度上有所下降，表明重构代码的层次结构给LLM带来了新的挑战。

Conclusion: 基于基础库和模块化优化的可扩展代码结构不仅促进了LLM对环境规范的适应，还建立了一个支持未来扩展的可扩展环境。

Abstract: Simulating interactive world models remains a core challenge in Large
Language Models(LLMs). In this work, we introduce the ByteSized32Refactored, a
refactored, modular, and extensible implementation of the original ByteSized32
corpus to explore the task of text game generation. We further optimize the
code structure of each text game and create the GameBasic.py foundation
library, which centralizes common logic across all 32 games by abstracting 7
base classes (GameObject, etc.) into reusable modules, thereby reducing from
20k to 10k total lines of Python code compared to the original Bytesized32. Our
refactored implementation enables extendability - with our centralized design,
ByteSized32Refactored can be more efficiently extended to include text games of
new scenarios and specifications by reusing the shared logic and
functionalities. Extensive experiments with GPT-4o demonstrate a mix of
performance - with Bytesized32Refactored, the generated text games for unseen
scenarios showcase quality improvements on two of the four evaluation
dimensions while decreases on the other two, indicating that the hierarchical
structure of the refactored code presents new challenges for LLMs. Overall, we
highlight that our extensible code structure, centered on the foundation
library and the modular optimization, not only facilitates LLM adaptation to
environment specifications but also establishes a scalable environment that
supports future extensions.

</details>


### [23] [MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use](https://arxiv.org/abs/2509.24002)
*Zijian Wu,Xiangyan Liu,Xinyuan Zhang,Lingjun Chen,Fanqing Meng,Lingxiao Du,Yiran Zhao,Fanshi Zhang,Yaoqi Ye,Jiawei Wang,Zirui Wang,Jinjie Ni,Yufan Yang,Arvin Xu,Michael Qizhe Shieh*

Main category: cs.CL

TL;DR: MCPMark是一个新的MCP基准测试，包含127个高质量任务，旨在更真实全面地评估MCP使用，相比现有基准测试具有更丰富的交互复杂性和现实性。


<details>
  <summary>Details</summary>
Motivation: 现有MCP基准测试范围狭窄，主要关注读取密集型任务或交互深度有限的任务，无法捕捉真实世界工作流程的复杂性和现实性。

Method: 提出MCPMark基准测试，包含127个由领域专家和AI代理协作创建的高质量任务，每个任务都有精心策划的初始状态和自动验证脚本，需要丰富的CRUD操作。

Result: 评估结果显示，表现最佳的gpt-5-medium模型仅达到52.56% pass@1和33.86% pass^4，其他强模型如claude-sonnet-4和o3低于30% pass@1和15% pass^4。LLMs平均需要16.2执行轮次和17.4工具调用。

Conclusion: MCPMark展示了现有LLMs在复杂MCP任务上的局限性，突出了其压力测试性质，为未来MCP代理开发提供了重要基准。

Abstract: MCP standardizes how LLMs interact with external systems, forming the
foundation for general agents. However, existing MCP benchmarks remain narrow
in scope: they focus on read-heavy tasks or tasks with limited interaction
depth, and fail to capture the complexity and realism of real-world workflows.
To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP
use in a more realistic and comprehensive manner. It consists of $127$
high-quality tasks collaboratively created by domain experts and AI agents.
Each task begins with a curated initial state and includes a programmatic
script for automatic verification. These tasks demand richer and more diverse
interactions with the environment, involving a broad range of create, read,
update, and delete (CRUD) operations. We conduct a comprehensive evaluation of
cutting-edge LLMs using a minimal agent framework that operates in a
tool-calling loop. Empirical results show that the best-performing model,
gpt-5-medium, reaches only $52.56$\% pass@1 and $33.86$\% pass^4, while other
widely regarded strong models, including claude-sonnet-4 and o3, fall below
$30$\% pass@1 and $15$\% pass^4. On average, LLMs require $16.2$ execution
turns and $17.4$ tool calls per task, significantly surpassing those in
previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.

</details>


### [24] [Large-Scale Constraint Generation -- Can LLMs Parse Hundreds of Constraints?](https://arxiv.org/abs/2509.24090)
*Matteo Boffa,Jiaxuan You*

Main category: cs.CL

TL;DR: 论文提出了大规模约束生成(LSCG)问题，评估LLMs处理大量细粒度通用约束的能力，并开发了Words Checker实例来测试模型性能和约束数量增加的影响，同时提出FoCusNet模型来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs在少量任务特定要求下的约束生成能力，而本文旨在探索LLMs处理大规模、细粒度通用约束列表的能力。

Method: 创建Words Checker实例评估模型特性和引导技术对性能的影响，提出FoCusNet模型将原始约束列表解析为更小子集以帮助LLM聚焦相关约束。

Result: 实验显示现有解决方案在约束数量增加时性能显著下降，而FoCusNet能带来8-13%的准确率提升。

Conclusion: LLMs在大规模约束处理方面存在挑战，FoCusNet通过约束聚焦机制能有效提升性能。

Abstract: Recent research has explored the constrained generation capabilities of Large
Language Models (LLMs) when explicitly prompted by few task-specific
requirements. In contrast, we introduce Large-Scale Constraint Generation
(LSCG), a new problem that evaluates whether LLMs can parse a large,
fine-grained, generic list of constraints. To examine the LLMs' ability to
handle an increasing number constraints, we create a practical instance of
LSCG, called Words Checker. In Words Checker, we evaluate the impact of model
characteristics (e.g., size, family) and steering techniques (e.g., Simple
Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet,
a small and dedicated model that parses the original list of constraints into a
smaller subset, helping the LLM focus on relevant constraints. Experiments
reveal that existing solutions suffer a significant performance drop as the
number of constraints increases, with FoCusNet showing an 8-13% accuracy boost.

</details>


### [25] [Retrieval-augmented GUI Agents with Generative Guidelines](https://arxiv.org/abs/2509.24183)
*Ran Xu,Kaixin Ma,Wenhao Yu,Hongming Zhang,Joyce C. Ho,Carl Yang,Dong Yu*

Main category: cs.CL

TL;DR: RAG-GUI是一个轻量级视觉语言模型，通过检索增强生成技术利用网络教程，在推理时增强GUI代理的性能。它采用监督微调和自引导拒绝采样微调，作为通用插件提升任何基于VLM的代理。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理在真实应用中因训练数据稀缺和任务复杂性（特别是需要处理罕见场景的长尾知识）而效果受限的问题。

Method: 首先通过监督微调进行预热，然后通过自引导拒绝采样微调进一步优化。该方法是模型无关的，可作为通用插件增强任何VLM代理。

Result: 在三个不同任务上的评估显示，RAG-GUI始终优于基线代理，在两个模型规模上比其他推理基线提升2.6%到13.3%。

Conclusion: RAG-GUI展示了强大的泛化能力和实用的即插即用能力，在真实场景中表现优异。

Abstract: GUI agents powered by vision-language models (VLMs) show promise in
automating complex digital tasks. However, their effectiveness in real-world
applications is often limited by scarce training data and the inherent
complexity of these tasks, which frequently require long-tailed knowledge
covering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that
leverages web tutorials at inference time. RAG-GUI is first warm-started via
supervised finetuning (SFT) and further refined through self-guided rejection
sampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as
a generic plug-in that enhances any VLM-based agent. Evaluated across three
distinct tasks, it consistently outperforms baseline agents and surpasses other
inference baselines by 2.6% to 13.3% across two model sizes, demonstrating
strong generalization and practical plug-and-play capabilities in real-world
scenarios.

</details>


### [26] [AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play](https://arxiv.org/abs/2509.24193)
*Ran Xu,Yuchen Zhuang,Zihan Dong,Jonathan Wang,Yue Yu,Joyce C. Ho,Linjun Zhang,Haoyu Wang,Wenqi Shi,Carl Yang*

Main category: cs.CL

TL;DR: AceSearcher是一个协作式自博弈框架，通过训练单个LLM在分解器和求解器两个角色间切换，提升复杂推理任务的性能，在多个数据集上平均准确率提升7.6%。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索增强LLM在处理复杂推理任务时存在多跳检索效率低和推理能力有限的问题。

Method: 提出协作自博弈框架，训练单个LLM交替扮演分解器（分解复杂查询）和求解器（整合检索上下文生成答案），结合监督微调和强化学习微调。

Result: 在10个数据集的3个推理密集型任务中超越现有最优方法，平均准确率提升7.6%；32B模型在金融推理任务中性能媲美DeepSeek-V3但参数少95%；小规模模型（1.5B和8B）性能优于参数多9倍的现有模型。

Conclusion: AceSearcher在复杂推理任务中展现出卓越的效率和有效性，无需中间标注即可实现高性能。

Abstract: Search-augmented LLMs often struggle with complex reasoning tasks due to
ineffective multi-hop retrieval and limited reasoning ability. We propose
AceSearcher, a cooperative self-play framework that trains a single large
language model (LLM) to alternate between two roles: a decomposer that breaks
down complex queries and a solver that integrates retrieved contexts for answer
generation. AceSearcher couples supervised fine-tuning on a diverse mixture of
search, reasoning, and decomposition tasks with reinforcement fine-tuning
optimized for final answer accuracy, eliminating the need for intermediate
annotations. Extensive experiments on three reasoning-intensive tasks across 10
datasets show that AceSearcher outperforms state-of-the-art baselines,
achieving an average exact match improvement of 7.6%. Remarkably, on
document-level finance reasoning tasks, AceSearcher-32B matches the performance
of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller
scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented
LLMs with up to 9x more parameters, highlighting its exceptional efficiency and
effectiveness in tackling complex reasoning tasks. Our code will be published
at https://github.com/ritaranx/AceSearcher and
https://huggingface.co/AceSearcher.

</details>


### [27] [Can Large Language Models Express Uncertainty Like Human?](https://arxiv.org/abs/2509.24202)
*Linwei Tao,Yi-Fan Yeh,Bo Kai,Minjing Dong,Tao Huang,Tom A. Lamb,Jialin Yu,Philip H. S. Torr,Chang Xu*

Main category: cs.CL

TL;DR: 该论文提出语言置信度作为LLM不确定性估计的轻量级方法，通过构建首个大规模对冲表达数据集和轻量级映射器，系统研究了现代LLM的语言置信度表现，并提出了改进框架。


<details>
  <summary>Details</summary>
Motivation: 现有置信度估计方法存在实际障碍：logits通常隐藏、多采样计算成本高、数值化不确定性表达不自然。语言置信度通过自然语言表达不确定性，提供了轻量级且以人为本的替代方案。

Method: 1) 发布首个多样化、大规模的对冲表达数据集；2) 提出轻量级映射器将对冲表达转换为置信度分数；3) 系统研究现代LLM的语言置信度表现；4) 引入微调框架改进语言置信度可靠性。

Result: 研究发现大多数LLM在表达可靠语言置信度方面表现不佳，但精心设计的提示可以达到竞争性的校准和区分能力。微调框架进一步提高了语言置信度的可靠性。

Conclusion: 语言置信度作为LLM不确定性估计的可扩展、高效且与人类对齐的方法具有前景，值得进一步探索。

Abstract: Large language models (LLMs) are increasingly used in high-stakes settings,
where overconfident responses can mislead users. Reliable confidence estimation
has been shown to enhance trust and task accuracy. Yet existing methods face
practical barriers: logits are often hidden, multi-sampling is computationally
expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score)
deviates from natural communication. We revisit linguistic confidence (LC),
where models express uncertainty through hedging language (e.g., probably,
might), offering a lightweight and human-centered alternative. To advance this
direction, we (1) release the first diverse, large-scale dataset of hedging
expressions with human-annotated confidence scores, and (2) propose a
lightweight mapper that converts hedges into confidence scores at near-zero
cost. Building on these resources, we (3) conduct the first systematic study of
LC across modern LLMs and QA benchmarks, revealing that while most LLMs
underperform in expressing reliable LC, carefully designed prompting achieves
competitive calibration and discriminability. Finally, we (4) introduce a
fine-tuning framework that further improves LC reliability. Taken together, our
work positions linguistic confidence as a scalable, efficient, and
human-aligned approach to LLM uncertainty estimation, and calls for deeper
exploration of this promising yet underexplored direction.

</details>


### [28] [SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents](https://arxiv.org/abs/2509.24282)
*Gyuhyeon Seo,Jungwoo Yang,Junseong Pyo,Nalim Kim,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: SimuHome是一个基于Matter协议的智能家居模拟环境，用于开发和评估LLM智能体在复杂智能家居任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决智能家居环境中开发LLM智能体的瓶颈：缺乏真实的模拟环境和具有挑战性的评估基准。

Method: 构建基于Matter协议的SimuHome模拟器，提供时间加速的智能设备模拟、API调用和环境变量变化。创建包含600个episode的基准测试，涵盖12种用户查询类型。

Result: 评估11个智能体发现，它们在简单任务上表现良好，但在潜在意图推断、状态验证和时间调度方面表现不佳。最佳模型GPT-4.1的成功率仅为54%。

Conclusion: 需要开发能够可靠验证当前状态并协调时间相关动作的方法。

Abstract: Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks.
However, smart homes introduce distinct challenges, requiring agents to handle
latent user intents, temporal dependencies, device constraints, scheduling, and
more. The main bottlenecks for developing smart home agents with such
capabilities include the lack of a realistic simulation environment where
agents can interact with devices and observe the results, as well as a
challenging benchmark to evaluate them. To address this, we introduce
$\textbf{SimuHome}$, a time-accelerated home environment that simulates smart
devices, supports API calls, and reflects changes in environmental variables.
By building the simulator on the Matter protocol (the global industry standard
for smart home communication), SimuHome provides a high-fidelity environment,
and agents validated in SimuHome can be deployed on real Matter-compliant
devices with minimal adaptation. We provide a challenging benchmark of 600
episodes across twelve user query types that require the aforementioned
capabilities. Our evaluation of 11 agents under a unified ReAct framework
reveals that while models perform well on simple tasks, they struggle with
latent intent inference, state verification, and especially temporal
scheduling. Even the top-performing model, GPT-4.1, reaches only 54% success
rate. These findings highlight a critical need for methods that can reliably
verify the current state via tools before acting and coordinate time-dependent
actions.

</details>


### [29] [DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models](https://arxiv.org/abs/2509.24296)
*Zherui Li,Zheng Nie,Zhenhong Zhou,Yufei Guo,Yue Liu,Yitong Zhang,Yu Cheng,Qingsong Wen,Kun Wang,Jiaheng Zhang*

Main category: cs.CL

TL;DR: 该论文分析了扩散大语言模型(dLLMs)在越狱攻击中的脆弱性，揭示了贪婪重掩码策略的有害偏见和去噪路径依赖现象，并提出了DiffuGuard防御框架来提升dLLMs的安全性。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)由于其迭代和并行生成机制，面临着与自回归LLMs根本不同的安全漏洞，需要深入分析这些新型脆弱性并开发有效的防御方法。

Method: 从两个维度分析dLLM漏洞：步内动态和步间动态；提出DiffuGuard防御框架，包含随机退火重掩码(动态引入受控随机性)和块级审计修复(利用内部表示进行风险检测和引导修正)。

Result: 实验结果显示，DiffuGuard在四个dLLM上对六种不同越狱方法的攻击成功率从47.9%降低到14.7%，同时保持了模型效用和效率。

Conclusion: 当前解码策略构成显著漏洞，但dLLMs具有巨大的内在安全潜力；DiffuGuard能够有效解锁这种潜力，提供训练免费的防御解决方案。

Abstract: The rapid advancement of Diffusion Large Language Models (dLLMs) introduces
unprecedented vulnerabilities that are fundamentally distinct from
Autoregressive LLMs, stemming from their iterative and parallel generation
mechanisms. In this paper, we conduct an in-depth analysis of dLLM
vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step
and inter-step dynamics. Experimental results reveal a harmful bias inherent in
the standard greedy remasking strategy and identify a critical phenomenon we
term Denoising-path Dependence, where the safety of early-stage tokens
decisively influences the final output. These findings also indicate that while
current decoding strategies constitute a significant vulnerability, dLLMs
possess a substantial intrinsic safety potential. To unlock this potential, we
propose DiffuGuard, a training-free defense framework that addresses
vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking
dynamically introduces controlled randomness to mitigate greedy selection bias,
while Block-level Audit and Repair exploits internal model representations for
autonomous risk detection and guided correction. Comprehensive experiments on
four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack
Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while
preserving model utility and efficiency. Our code is available at:
https://github.com/niez233/DiffuGuard.

</details>


### [30] [Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs](https://arxiv.org/abs/2509.24297)
*Junying Wang,Zicheng Zhang,Ye Shen,Yalun Wu,Yingji Liang,Yijin Guo,Farong Wen,Wenzhe Li,Xuezhi Zhao,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: 提出TQA-to-MMQA框架，将文本问答对转换为高质量多模态问答对，构建基准测试并开发Q-Mirror代理系统进行迭代优化


<details>
  <summary>Details</summary>
Motivation: 手动创建高质量多模态基准测试成本高且难以扩展，需要自动化解决方案

Method: 开发TQA-to-MMQA转换框架，建立多维度质量评估标准，构建两个基准测试，并开发Q-Mirror代理系统实现生成与评估的闭环迭代

Result: 现有模型生成MMQA仍有明显差距，顶级理解模型与人类评估高度一致；Q-Mirror代理将平均分从78.90提升至85.22，通过率从72%提升至95%

Conclusion: Q-Mirror代理为大规模科学基准测试提供了实用路径，通过闭环迭代显著提升多模态问答对质量

Abstract: High-quality, multi-modal benchmarks are crucial for advancing scientific
reasoning in large models yet their manual creation is costly and unscalable.
To address this bottleneck, we explore the potential for transforming Text-Only
QA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include
three parts: 1) Task Definition \& Evaluation Rubric: We develop a TQA-to-MMQA
framework and establish a comprehensive, multi-dimensional MMQA quality rubric
that provides principles for the transformation. 2) Benchmark Construction:
Then we construct two extensive benchmarks to rigorously evaluate
state-of-the-art generation \& understanding models on the distinct tasks of
MMQA generation \& MMQA quality evaluation. 3) Preliminary Solution: We develop
an agentic system (Q-Mirror), which operationalizes our framework by
integrating MMQA generation and evaluation into a closed loop for iterative
refinement. Our experiments show that while state-of-the-art models can
generate MMQAs, their outputs still leave substantial gaps, underscoring the
need for reliable evaluation. We further demonstrate that top-tier
understanding models align closely with human judgment in MMQA quality
assessment. Leveraging both insights, the Q-Mirror agent raises average scores
from 78.90 to 85.22 and pass rates from 72\% to 95\%, offering a practical path
to large-scale scientific benchmarks.

</details>


### [31] [GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training](https://arxiv.org/abs/2509.24494)
*Hongcheng Wang,Yinuo Huang,Sukai Wang,Guanghui Ren,Hao Dong*

Main category: cs.CL

TL;DR: 提出了GRPO-MA方法，通过为每个思维过程生成多个答案来解决GRPO算法中的梯度耦合、稀疏奖励和不稳定优势估计问题，显著提升了训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决GRPO算法在训练思维链推理时面临的三个关键挑战：思维与答案间的梯度耦合、有限并行采样导致的稀疏奖励信号、以及不稳定的优势估计。

Method: GRPO-MA方法，通过为每个思维过程生成多个答案，减少思维优势的方差，实现更鲁棒和高效的优化。理论分析表明思维优势的方差随每个思维的答案数量增加而降低。

Result: 在数学、代码和多样化多模态任务上的实验表明，GRPO-MA显著提升了性能和训练效率。梯度分析证实了该方法减少了梯度尖峰，消融研究显示增加每个思维的答案数量能持续提升模型性能。

Conclusion: GRPO-MA通过多答案生成机制有效解决了GRPO的局限性，为思维链推理训练提供了更稳定和高效的强化学习方法。

Abstract: Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a
Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought
(CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models
(VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling
between thoughts and answers, sparse reward signals caused by limited parallel
sampling, and unstable advantage estimation. To mitigate these challenges, we
propose GRPO-MA, a simple yet theoretically grounded method that leverages
multi-answer generation from each thought process, enabling more robust and
efficient optimization. Theoretically, we show that the variance of thought
advantage decreases as the number of answers per thought increases.
Empirically, our gradient analysis confirms this effect, showing that GRPO-MA
reduces gradient spikes compared to GRPO. Experiments on math, code, and
diverse multimodal tasks demonstrate that GRPO-MA substantially improves
performance and training efficiency. Our ablation studies further reveal that
increasing the number of answers per thought consistently enhances model
performance.

</details>


### [32] [MemGen: Weaving Generative Latent Memory for Self-Evolving Agents](https://arxiv.org/abs/2509.24704)
*Guibin Zhang,Muxin Fu,Shuicheng Yan*

Main category: cs.CL

TL;DR: 提出了MemGen动态生成式记忆框架，为LLM智能体提供类人认知能力，通过记忆触发器和记忆编织器实现推理与记忆的紧密交织，在8个基准测试中显著超越现有外部记忆系统。


<details>
  <summary>Details</summary>
Motivation: 现有参数化记忆和检索式记忆方法无法捕捉人类认知中推理与记忆的流畅交织，需要更自然的机器认知形式。

Method: MemGen包含记忆触发器（监控推理状态决定显式记忆调用）和记忆编织器（以当前状态为刺激构建潜在token序列作为机器原生记忆），实现推理过程中记忆的回忆和增强。

Result: 在8个基准测试中，MemGen比ExpeL和AWM等领先外部记忆系统提升高达38.22%，超过GRPO达13.44%，并展现出强大的跨领域泛化能力。

Conclusion: MemGen在没有显式监督的情况下自发演化出类似人类的记忆功能，包括规划记忆、程序记忆和工作记忆，表明机器认知向更自然形式发展的新兴轨迹。

Abstract: Agent memory shapes how Large Language Model (LLM)-powered agents, akin to
the human brain, progressively refine themselves through environment
interactions. Existing paradigms remain constrained: parametric memory forcibly
adjusts model parameters, and retrieval-based memory externalizes experience
into structured databases, yet neither captures the fluid interweaving of
reasoning and memory that underlies human cognition. To address this gap, we
propose MemGen, a dynamic generative memory framework that equips agents with a
human-esque cognitive faculty. It consists of a \textit{memory trigger}, which
monitors the agent's reasoning state to decide explicit memory invocation, and
a \textit{memory weaver}, which takes the agent's current state as stimulus to
construct a latent token sequence as machine-native memory to enrich its
reasoning. In this way, MemGen enables agents to recall and augment latent
memory throughout reasoning, producing a tightly interwoven cycle of memory and
cognition. Extensive experiments across eight benchmarks show that MemGen
surpasses leading external memory systems such as ExpeL and AWM by up to
$38.22\%$, exceeds GRPO by up to $13.44\%$, and exhibits strong cross-domain
generalization ability. More importantly, we find that without explicit
supervision, MemGen spontaneously evolves distinct human-like memory faculties,
including planning memory, procedural memory, and working memory, suggesting an
emergent trajectory toward more naturalistic forms of machine cognition.

</details>


### [33] [LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space](https://arxiv.org/abs/2509.24771)
*Guibin Zhang,Fanci Meng,Guancheng Wan,Zherui Li,Kun Wang,Zhenfei Yin,Lei Bai,Shuicheng Yan*

Main category: cs.CL

TL;DR: 提出了LatentEvolve框架，通过模拟人类互补学习系统的昼夜交替过程，让LLM在推理阶段自我进化学习如何扩展测试时计算能力


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法相互独立，LLM尚未学会如何渐进式地更有效地扩展计算。目标是让LLM学习"如何扩展测试时计算"

Method: 基于互补学习系统理论的双组件框架：白天扩展（快速检索历史潜在表示指导当前推理）和夜间扩展（整合过去的潜在优化，类似人脑睡眠时的经验巩固）

Result: 在8个基准测试和5个模型骨干上的实验表明，LatentEvolve比最先进的TTS方法（如LatentSeek和TTRL）性能提升高达13.33%，并展现出卓越的跨域和跨骨干泛化能力

Conclusion: LatentEvolve通过模拟人类认知动态的昼夜交替过程，实现了LLM测试时扩展的快速和慢速进化，在完全无监督的方式下显著提升了推理能力

Abstract: Test-time Scaling (TTS) has been demonstrated to significantly enhance the
reasoning capabilities of Large Language Models (LLMs) during the inference
phase without altering model parameters. However, existing TTS methods are
largely independent, implying that LLMs have not yet evolved to progressively
learn how to scale more effectively. With the objective of evolving LLMs to
learn ``how to scale test-time computation,'' we propose LatentEvolve, a
self-evolving latent TTS framework inspired by the complementary learning
system (CLS) theory. Analogous to the human brain's dual system of a
fast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve
comprises two evolutionary components: \textit{daytime scaling}, which rapidly
retrieves historical latent representations to better guide current LLM
reasoning; and \textit{nighttime scaling}, which integrates past latent
optimizations in a manner akin to the human brain's consolidation of
experiences during sleep. The alternation of daytime and nighttime processes
facilitates a fast and slow evolution of LLM TTS, mirroring human cognitive
dynamics in a fully unsupervised manner. Extensive experiments across eight
benchmarks and five model backbones demonstrate that our LatentEvolve surpasses
state-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\%$ and
exhibits exceptional cross-domain and cross-backbone generalization.

</details>


### [34] [Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement](https://arxiv.org/abs/2509.24841)
*Zhilong Zhao,Yindi Liu*

Main category: cs.CL

TL;DR: 提出了分层错误修正（HEC）框架，通过系统错误分析和针对性干预策略解决专业领域AI性能问题，在多个专业领域任务中平均提升11.2个百分点。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域表现不佳，在医疗编码任务中准确率仅为45.9%，需要专门的方法来提升专业领域性能。

Method: 分析四个专业领域的错误模式，发现错误遵循分层结构：知识层错误（58.4%）、推理层错误（39.6%）和复杂度层错误（2.0%）。基于此开发三阶段修正框架，按错误层次重要性进行针对性干预。

Result: 在医疗转录、法律文档分类、政治偏见检测和法律推理四个领域验证，平均提升11.2个百分点（p < 0.001）。但在高基线任务（>75%准确率）中框架效果有限。

Conclusion: 系统错误分析可以指导专业领域AI增强策略，特别适用于中等基线任务，同时需要理解框架边界以实现最优部署。

Abstract: Large Language Models face significant performance challenges in specialized
domains, with state-of-the-art models achieving only 45.9% accuracy on medical
coding tasks. This study proposes a Hierarchical Error Correction (HEC)
framework that addresses domain-specific AI limitations through systematic
error analysis and targeted intervention strategies.
  We analyze error patterns across four specialized domains and find that AI
errors follow consistent hierarchical structures: Knowledge-layer errors
(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).
Based on these patterns, we develop a three-stage correction framework that
addresses errors according to their hierarchical importance and demonstrates
that framework effectiveness correlates inversely with baseline task
performance.
  Experimental validation across medical transcription (4,921 cases), legal
document classification (1,000 cases), political bias detection (645 cases),
and legal reasoning (1,000 cases) shows consistent improvements. Cross-model
validation across five LLM architectures demonstrates average improvements of
11.2 percentage points (p < 0.001). However, analysis reveals framework
limitations in high-baseline tasks (>75% accuracy), where hierarchical
intervention may interfere with effective reasoning processes.
  The results suggest that systematic error analysis can guide effective AI
enhancement strategies in specialized domains, particularly for
moderate-baseline tasks, while highlighting the importance of understanding
framework boundaries for optimal deployment.

</details>


### [35] [The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents' Inquiry Capability](https://arxiv.org/abs/2509.24958)
*Linlu Gong,Ante Wang,Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: MAQuE是最大的医疗多轮问诊自动评估基准，包含3000个模拟患者代理，评估框架涵盖任务成功、问诊能力、对话能力、问诊效率和患者体验五个维度。实验显示现有LLM在问诊能力方面仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有AI医生主要关注诊断技能，但忽视了优秀医生应具备的共情、耐心和清晰沟通等品质，需要更全面的评估框架。

Method: 构建包含3000个模拟患者代理的基准，这些代理具有多样语言模式、认知限制、情绪反应和被动披露倾向，并设计多维度评估框架。

Result: 不同LLM在各项评估指标上都面临显著挑战，即使是先进模型在问诊能力方面也有很大改进空间，且对患者行为变化高度敏感。

Conclusion: 医疗AI需要平衡性能和实用性，当前模型在现实临床环境中仍有不足，需要更全面的能力评估。

Abstract: An effective physician should possess a combination of empathy, expertise,
patience, and clear communication when treating a patient. Recent advances have
successfully endowed AI doctors with expert diagnostic skills, particularly the
ability to actively seek information through inquiry. However, other essential
qualities of a good doctor remain overlooked. To bridge this gap, we present
MAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the
automatic and comprehensive evaluation of medical multi-turn questioning. It
features 3,000 realistically simulated patient agents that exhibit diverse
linguistic patterns, cognitive limitations, emotional responses, and tendencies
for passive disclosure. We also introduce a multi-faceted evaluation framework,
covering task success, inquiry proficiency, dialogue competence, inquiry
efficiency, and patient experience. Experiments on different LLMs reveal
substantial challenges across the evaluation aspects. Even state-of-the-art
models show significant room for improvement in their inquiry capabilities.
These models are highly sensitive to variations in realistic patient behavior,
which considerably impacts diagnostic accuracy. Furthermore, our fine-grained
metrics expose trade-offs between different evaluation perspectives,
highlighting the challenge of balancing performance and practicality in
real-world clinical settings.

</details>


### [36] [Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns](https://arxiv.org/abs/2509.24988)
*Hanqi Xiao,Vaidehi Patil,Hyunji Lee,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 本文挑战了LLM置信度估计的传统方法，发现模型预测自身输出正确性的能力并不优于其他模型，提出通过注入历史正确性信息来构建广义正确性模型(GCM)，证明可靠的置信度估计是可泛化、模型无关的技能。


<details>
  <summary>Details</summary>
Motivation: 在关键应用场景中部署LLM需要准确且校准的置信度估计，但现有方法假设模型具有判断自身答案正确性的特权信息，这一假设可能存在问题。

Method: 提出广义正确性模型(GCM)，通过注入目标模型的历史预测信息，包括训练基于多LLM正确性数据的模型、使用历史作为上下文示例、以及后处理校准等方法。

Result: GCM在5个模型家族和MMLU、TriviaQA数据集上表现良好，在选择性预测任务中有效，证明正确性预测能力主要来自历史正确性模式而非模型自省。

Conclusion: 可靠的LLM置信度估计是通过系统编码正确性历史学习的可泛化、模型无关技能，而非依赖模型特定自省能力。

Abstract: Generating accurate and calibrated confidence estimates is critical for
deploying LLMs in high-stakes or user-facing applications, and remains an open
challenge. Prior research has often framed confidence as a problem of eliciting
a model's "self-knowledge", i.e., the ability of an LLM to judge whether its
own answers are correct; this approach implicitly assumes that there is some
privileged information about the answer's correctness that is accessible to the
model itself. However, our experiments reveal that an LLM attempting to predict
the correctness of its own outputs generally performs no better than an
unrelated LLM. Moreover, we hypothesize that a key factor in building a
"Correctness Model" (CM) is exposure to a target model's historical
predictions. We propose multiple methods to inject this historical correctness
information, creating a Generalized Correctness Model (GCM). We first show that
GCMs can be trained on the correctness data from many LLMs and learn patterns
for correctness prediction applicable across datasets and models. We then use
CMs as a lens for studying the source of correctness prediction ability and its
generalization, systematically controlling their training data and finding that
answer phrasing is a strong predictor for correctness. We further explore
alternative methods of injecting history without training an LLM, finding that
including history as in-context examples can help improve correctness
prediction, and post-hoc calibration can provide complementary reductions in
calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families
and the MMLU and TriviaQA datasets, as well as on a downstream selective
prediction task, finding that reliable LLM confidence estimation is a
generalizable and model-agnostic skill learned by systematically encoding
correctness history rather than a model-specific skill reliant on
self-introspection.

</details>


### [37] [Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures](https://arxiv.org/abs/2509.25045)
*Marco Bronzini,Carlo Nicolini,Bruno Lepri,Jacopo Staiano,Andrea Passerini*

Main category: cs.CL

TL;DR: 本文提出了一种名为Hyperdimensional Probe的新方法，用于从LLM向量空间中解码信息，结合符号表示和神经探测技术，通过向量符号架构将模型的残差流投影到可解释的概念中。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释性方法（如直接logit归因和稀疏自编码器）由于模型输出词汇表限制或特征名称不清晰等问题，对LLM内部表示的理解有限。

Method: 结合符号表示和神经探测，使用向量符号架构将模型的残差流投影到可解释概念中，在控制输入完成任务和问答设置中验证解码范式。

Result: 实验表明该探针能够在不同的LLM、嵌入大小和输入领域中可靠地提取有意义的概念，并帮助识别LLM的失败情况。

Conclusion: 这项工作推进了LLM向量空间中的信息解码，能够从神经表示中提取更具信息性、可解释性和结构化的特征。

Abstract: Despite their capabilities, Large Language Models (LLMs) remain opaque with
limited understanding of their internal representations. Current
interpretability methods, such as direct logit attribution (DLA) and sparse
autoencoders (SAEs), provide restricted insight due to limitations such as the
model's output vocabulary or unclear feature names. This work introduces
Hyperdimensional Probe, a novel paradigm for decoding information from the LLM
vector space. It combines ideas from symbolic representations and neural
probing to project the model's residual stream into interpretable concepts via
Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs
and conventional probes while overcoming their key limitations. We validate our
decoding paradigm with controlled input-completion tasks, probing the model's
final state before next-token prediction on inputs spanning syntactic pattern
recognition, key-value associations, and abstract inference. We further assess
it in a question-answering setting, examining the state of the model both
before and after text generation. Our experiments show that our probe reliably
extracts meaningful concepts across varied LLMs, embedding sizes, and input
domains, also helping identify LLM failures. Our work advances information
decoding in LLM vector space, enabling extracting more informative,
interpretable, and structured features from neural representations.

</details>


### [38] [Scaling Generalist Data-Analytic Agents](https://arxiv.org/abs/2509.25084)
*Shuofei Qiao,Yanqiu Zhao,Zhisong Qiu,Xiaobin Wang,Jintian Zhang,Zhao Bin,Ningyu Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Huajun Chen*

Main category: cs.CL

TL;DR: DataMind是一个可扩展的数据合成和智能体训练框架，用于构建通用的数据分析智能体。它解决了开源数据分析智能体面临的三个关键挑战：数据资源不足、训练策略不当和不稳定的基于代码的多轮推理。


<details>
  <summary>Details</summary>
Motivation: 当前的数据分析智能体严重依赖基于专有模型的提示工程，而开源模型难以处理多样格式的大规模数据文件和现实世界分析所需的长期多步推理。

Method: DataMind采用：1）细粒度任务分类和递归易到难任务组合机制；2）知识增强的轨迹采样策略；3）动态可调的SFT和RL损失训练目标；4）内存节约且稳定的基于代码的多轮推理框架。

Result: 基于DataMind-12K训练的DataMind-14B在多个数据分析基准测试中达到71.16%的平均分，优于最强的专有基线DeepSeek-V3.1和GPT-5。DataMind-7B在开源模型中表现最佳，达到68.10%。

Conclusion: DataMind框架成功构建了高性能的开源数据分析智能体，为社区提供了可操作的研究见解，并将发布数据集和模型供未来研究使用。

Abstract: Data-analytic agents are emerging as a key catalyst for automated scientific
discovery and for the vision of Innovating AI. Current approaches, however,
rely heavily on prompt engineering over proprietary models, while open-source
models struggle to face diverse-format, large-scale data files and
long-horizon, multi-step reasoning that real-world analytics demands. This
paper introduces DataMind, a scalable data synthesis and agent training recipe
designed to build generalist data-analytic agents. DataMind tackles three key
challenges in building open-source data-analytic agents, including insufficient
data resources, improper training strategy, and unstable code-based multi-turn
rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a
recursive easy-to-hard task composition mechanism to increase the diversity and
difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling
strategy followed by model-based and rule-based filtering; 3) a dynamically
adjustable training objective combining both SFT and RL losses; 4) a
memory-frugal and stable code-based multi-turn rollout framework. Built on
DataMind, we curate DataMind-12K, a high-quality trajectory set spanning
diverse domains, task categories, and data file formats for data-analytic
tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with
an average score of 71.16% on multiple data analysis benchmarks, outperforming
the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B
also performs best among all open-source models with a score of 68.10%. We also
incorporate some empirical insights gained from our exploratory trials into the
analysis experiments, aiming to provide actionable insights about agentic
training for the community. We will release DataMind-12K and DataMind-7B,14B
for the community's future research.

</details>


### [39] [NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation](https://arxiv.org/abs/2509.25179)
*Penghai Zhao,Jinyu Tian,Qinghua Xing,Xin Zhang,Zheng Li,Jianjun Qian,Ming-Ming Cheng,Xiang Li*

Main category: cs.CL

TL;DR: NAIPv2是一个用于科学论文质量评估的去偏置高效框架，通过域内年份组的成对学习减少评分不一致性，引入评审倾向信号整合评分和置信度，在ICLR数据集上达到SOTA性能并保持线性推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法推理成本高，而直接评分回归方法存在尺度不一致问题，需要开发更高效且准确的论文质量评估框架。

Method: 采用域内年份组的成对学习减少评分不一致，引入评审倾向信号(RTS)作为评分和置信度的概率整合，构建NAIDv2大规模数据集进行训练。

Result: 在ICLR数据集上达到78.2% AUC和0.432 Spearman相关性，在NeurIPS数据上表现出强泛化能力，预测分数从拒绝到口头报告类别一致递增。

Conclusion: NAIPv2建立了一个去偏置且可扩展的自动化论文质量评估框架，为未来科学智能系统迈出重要一步。

Abstract: The ability to estimate the quality of scientific papers is central to how
both humans and AI systems will advance scientific knowledge in the future.
However, existing LLM-based estimation methods suffer from high inference cost,
whereas the faster direct score regression approach is limited by scale
inconsistencies. We present NAIPv2, a debiased and efficient framework for
paper quality estimation. NAIPv2 employs pairwise learning within domain-year
groups to reduce inconsistencies in reviewer ratings and introduces the Review
Tendency Signal (RTS) as a probabilistic integration of reviewer scores and
confidences. To support training and evaluation, we further construct NAIDv2, a
large-scale dataset of 24,276 ICLR submissions enriched with metadata and
detailed structured content. Trained on pairwise comparisons but enabling
efficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art
performance (78.2% AUC, 0.432 Spearman), while maintaining scalable,
linear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it
further demonstrates strong generalization, with predicted scores increasing
consistently across decision categories from Rejected to Oral. These findings
establish NAIPv2 as a debiased and scalable framework for automated paper
quality estimation, marking a step toward future scientific intelligence
systems. Code and dataset are released at
https://sway.cloud.microsoft/Pr42npP80MfPhvj8.

</details>


### [40] [InfoAgent: Advancing Autonomous Information-Seeking Agents](https://arxiv.org/abs/2509.25189)
*Gongrui Zhang,Jialiang Zhu,Ruiqi Yang,Kai Qiu,Miaosen Zhang,Zhirong Wu,Qi Dai,Bei Liu,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Yuan Zhang,Xin Li,Zhaoyi Liu,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: InfoAgent是一个基于大型语言模型的深度研究代理，通过创新的数据合成管道和编排的网页搜索工具来扩展能力。它使用实体树和子树采样构建挑战性查询，并开发自托管搜索基础设施。通过两阶段训练方法，在多个基准测试中超越了之前的开源深度研究代理。


<details>
  <summary>Details</summary>
Motivation: 构建能够通过外部工具交互扩展能力的大型语言模型代理是AI研究的新前沿，但现有工作过度依赖商业搜索工具，缺乏透明度和可控性。

Method: 使用实体树和子树采样构建挑战性查询，开发自托管搜索基础设施，采用两阶段训练：冷启动监督微调灌输长期搜索行为，然后通过强化学习改进推理驱动的工具使用。

Result: 在BrowseComp上达到15.3%准确率，BrowseComp-ZH上29.2%，Xbench-DS上40.4%，超越了WebSailor-72B和DeepDive-32B等先前开源深度研究代理。

Conclusion: InfoAgent通过创新的数据合成管道和自托管搜索基础设施，结合两阶段训练方法，在深度研究任务中表现出色，为构建更透明可控的AI代理提供了新思路。

Abstract: Building Large Language Model agents that expand their capabilities by
interacting with external tools represents a new frontier in AI research and
applications. In this paper, we introduce InfoAgent, a deep research agent
powered by an innovative data synthesis pipeline and orchestrated web search
tools. To construct challenging, hard-to-find queries,we build entity trees and
apply sub-tree sampling with entity fuzzification to systematically increase
question difficulty. Unlike prior work that relies heavily on commercial search
tools, we develop a dedicated self-hosted search infrastructure, enhancing
transparency of agent environments and facilitating further advancement of
agent capacity. We evaluate the effectiveness of our data pipeline by measuring
the average number of tool calls required to correctly answer a question, and
also show that our agent yields better performance when equipped with our
tools. Our \mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage
recipe: cold-start supervised finetuning to instill long-horizon search
behaviors, followed by reinforcement learning which significantly improves
reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\% accuracy
on BrowseComp, 29.2\% on BrowseComp-ZH, and 40.4\% on Xbench-DS, outperforming
prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [41] [A benchmark for vericoding: formally verified program synthesis](https://arxiv.org/abs/2509.22908)
*Sergiu Bursuc,Theodore Ehrenborg,Shaowei Lin,Lacramioara Astefanoaei,Ionel Emilian Chiosa,Jure Kukovec,Alok Singh,Oliver Butterley,Adem Bizid,Quinn Dougherty,Miranda Zhao,Max Tan,Max Tegmark*

Main category: cs.SE

TL;DR: 提出了目前最大的形式化验证代码生成基准，包含12,504个形式化规范，测试了三种语言(Dafny、Verus/Rust、Lean)的验证编码成功率，发现Dafny表现最佳(82%)，且LLM在Dafny验证方面的进步显著。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统自然语言编程可能产生有缺陷代码的问题，研究形式化验证代码生成(vericoding)的能力，建立大规模基准来评估LLM在生成经过形式化验证的代码方面的表现。

Method: 创建包含12,504个形式化规范的基准，涵盖Dafny、Verus/Rust和Lean三种语言，其中6,174个是新问题。使用现成的LLM测试验证编码成功率，并比较添加自然语言描述对性能的影响。

Result: 验证编码成功率分别为：Lean 27%、Verus/Rust 44%、Dafny 82%。添加自然语言描述没有显著改善性能。LLM在纯Dafny验证方面的表现从68%提升到96%。

Conclusion: 形式化验证代码生成是可行的，Dafny语言表现最佳，LLM在形式化验证方面取得了显著进步，为可靠代码生成提供了新途径。

Abstract: We present and test the largest benchmark for vericoding, LLM-generation of
formally verified code from formal specifications - in contrast to vibe coding,
which generates potentially buggy code from a natural language description. Our
benchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in
Verus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find
vericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny
using off-the-shelf LLMs. Adding natural-language descriptions does not
significantly improve performance. We also find that LLM progress has improved
progress on pure Dafny verification from 68% to 96% over the past year. The
benchmark and vericoding results are shared at
https://github.com/Beneficial-AI-Foundation/vericoding-benchmark

</details>


### [42] [The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution](https://arxiv.org/abs/2509.23261)
*Fei Gu,Zi Liang,Hongzong LI,Jiahao MA*

Main category: cs.SE

TL;DR: 研究发现AI辅助编程存在马太效应：编程语言或框架越流行，LLM生成代码的成功率越高，可能强化现有工具生态的集中化趋势


<details>
  <summary>Details</summary>
Motivation: 探索AI辅助编程对软件工程迭代动态的广泛影响，特别是LLM驱动开发如何与软件生态系统相互作用

Method: 在数千个算法编程任务和数百个框架选择任务上进行大规模实验，系统研究AI辅助编程与软件生态系统的交互

Result: 发现明显的马太效应：编程语言或框架越流行，LLM生成代码的成功率越高

Conclusion: AI系统可能强化现有的流行度层级，加速向主导工具收敛，同时阻碍多样性和创新

Abstract: AI-assisted programming is rapidly reshaping software development, with large
language models (LLMs) enabling new paradigms such as vibe coding and agentic
coding. While prior works have focused on prompt design and code generation
quality, the broader impact of LLM-driven development on the iterative dynamics
of software engineering remains underexplored. In this paper, we conduct
large-scale experiments on thousands of algorithmic programming tasks and
hundreds of framework selection tasks to systematically investigate how
AI-assisted programming interacts with the software ecosystem. Our analysis
reveals \textbf{a striking Matthew effect: the more popular a programming
language or framework, the higher the success rate of LLM-generated code}. The
phenomenon suggests that AI systems may reinforce existing popularity
hierarchies, accelerating convergence around dominant tools while hindering
diversity and innovation. We provide a quantitative characterization of this
effect and discuss its implications for the future evolution of programming
ecosystems.

</details>


### [43] [Improving the Efficiency of LLM Agent Systems through Trajectory Reduction](https://arxiv.org/abs/2509.23586)
*Yuan-An Xiao,Pengfei Gao,Chao Peng,Yingfei Xiong*

Main category: cs.SE

TL;DR: 提出了AgentDiet方法，通过识别和移除代理轨迹中的无用、冗余和过期信息，显著减少输入token数量39.9%~59.7%，降低计算成本21.1%~35.9%，同时保持代理性能不变。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多轮代理系统在软件工程任务中越来越流行，但不断增长的轨迹导致输入token计算成本高昂，现有研究和产品普遍忽视效率问题。

Method: 设计并实现了AgentDiet轨迹缩减方法，自动识别和移除代理轨迹中的无用、冗余和过期信息。

Result: 在两个LLM和两个基准测试上的评估显示，AgentDiet能减少输入token 39.9%~59.7%，降低最终计算成本21.1%~35.9%，同时保持代理性能不变。

Conclusion: 轨迹缩减是代理系统一个有前景的研究方向，能显著提升效率而不影响性能。

Abstract: Multi-turn agent systems based on Large Language Models (LLMs) have been
increasingly popular for software engineering tasks. While LLM agents show
decent effectiveness, the high computational cost of input tokens due to the
ever-growing trajectory remains an efficiency concern for their applications.
Efficiency is largely neglected in existing studies and agent products, and
this paper fills the gap by introducing an inference-time trajectory reduction
approach to reduce the cost of agents.
  Through analyzing existing agent trajectories, we demonstrate that useless,
redundant, and expired information is widespread in all trajectories, which can
be identified and reduced without harming the agent's performance. We then
design a simple yet effective trajectory reduction approach, AgentDiet, which
automatically removes such waste information. We implement AgentDiet on a
top-performing coding agent, and the evaluation on two LLMs and two benchmarks
shows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final
computational cost by 21.1% ~ 35.9%, while maintaining the same agent
performance. This indicates that trajectory reduction is a promising direction
for agent systems.

</details>


### [44] [PAT-Agent: Autoformalization for Model Checking](https://arxiv.org/abs/2509.23675)
*Xinyue Zuo,Yifan Zhang,Hongshu Wang,Yufan Cai,Zhe Hou,Jing Sun,Jin Song Dong*

Main category: cs.SE

TL;DR: PAT-Agent是一个端到端框架，结合LLM的生成能力和形式验证的严谨性，实现自然语言自动形式化和形式模型修复，自动化构建可验证的形式模型。


<details>
  <summary>Details</summary>
Motivation: 应用LLM到形式验证面临挑战，包括规范语言复杂性、幻觉输出风险以及自然语言与形式逻辑之间的语义鸿沟。

Method: 使用规划LLM提取关键建模元素并生成详细计划，指导代码生成LLM合成语法正确且语义忠实的形式模型，通过PAT模型检查器验证，并在出现差异时触发修复循环迭代修正模型。

Result: 在40个系统上的实验结果显示，PAT-Agent始终优于基线方法，实现了高验证成功率和优越效率。消融研究确认规划和修复组件的重要性。

Conclusion: PAT-Agent框架有效解决了LLM在形式验证中的应用挑战，用户研究表明该界面易于访问，支持有效的形式建模，即使对于形式方法经验有限的用户也是如此。

Abstract: Recent advances in large language models (LLMs) offer promising potential for
automating formal methods. However, applying them to formal verification
remains challenging due to the complexity of specification languages, the risk
of hallucinated output, and the semantic gap between natural language and
formal logic. We introduce PAT-Agent, an end-to-end framework for natural
language autoformalization and formal model repair that combines the generative
capabilities of LLMs with the rigor of formal verification to automate the
construction of verifiable formal models. In PAT-Agent, a Planning LLM first
extracts key modeling elements and generates a detailed plan using semantic
prompts, which then guides a Code Generation LLM to synthesize syntactically
correct and semantically faithful formal models. The resulting code is verified
using the Process Analysis Toolkit (PAT) model checker against user-specified
properties, and when discrepancies occur, a Repair Loop is triggered to
iteratively correct the model using counterexamples. To improve flexibility, we
built a web-based interface that enables users, particularly non-FM-experts, to
describe, customize, and verify system behaviors through user-LLM interactions.
Experimental results on 40 systems show that PAT-Agent consistently outperforms
baselines, achieving high verification success with superior efficiency. The
ablation studies confirm the importance of both planning and repair components,
and the user study demonstrates that our interface is accessible and supports
effective formal modeling, even for users with limited formal methods
experience.

</details>


### [45] [Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models](https://arxiv.org/abs/2509.23812)
*Dianshu Liao,Xin Yin,Shidong Pan,Chao Ni,Zhenchang Xing,Xiaoyu Sun*

Main category: cs.SE

TL;DR: JUnitGenie是一个路径敏感的单元测试生成框架，结合代码知识和LLM的语义能力，相比现有方法显著提高了分支和行覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有单元测试生成方法存在路径不敏感问题，无法处理深度控制流结构，导致覆盖率不足。

Method: 从Java项目中提取代码知识，将其提炼为结构化提示来指导LLM生成高覆盖率的单元测试。

Result: 在10个真实Java项目的2,258个复杂方法上评估，JUnitGenie相比启发式和LLM基线方法，分支和行覆盖率分别平均提高29.60%和31.00%，并能发现真实bug。

Conclusion: JUnitGenie通过路径敏感的方法有效解决了现有测试生成技术的局限性，显著提升了测试覆盖率和质量。

Abstract: Unit testing is essential for software quality assurance, yet writing and
maintaining tests remains time-consuming and error-prone. To address this
challenge, researchers have proposed various techniques for automating unit
test generation, including traditional heuristic-based methods and more recent
approaches that leverage large language models (LLMs). However, these existing
approaches are inherently path-insensitive because they rely on fixed
heuristics or limited contextual information and fail to reason about deep
control-flow structures. As a result, they often struggle to achieve adequate
coverage, particularly for deep or complex execution paths. In this work, we
present a path-sensitive framework, JUnitGenie, to fill this gap by combining
code knowledge with the semantic capabilities of LLMs in guiding context-aware
unit test generation. After extracting code knowledge from Java projects,
JUnitGenie distills this knowledge into structured prompts to guide the
generation of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex
focal methods from ten real-world Java projects. The results show that
JUnitGenie generates valid tests and improves branch and line coverage by
29.60% and 31.00% on average over both heuristic and LLM-based baselines. We
further demonstrate that the generated test cases can uncover real-world bugs,
which were later confirmed and fixed by developers.

</details>


### [46] [SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation](https://arxiv.org/abs/2509.23824)
*Zhifan Ye,Jiachi Chen,Zhenzhe Shao,Lingfeng Bao,Xiaohu Yang,Zhongxin Liu*

Main category: cs.SE

TL;DR: 提出了SolContractEval，首个Solidity智能合约级别的代码生成基准测试，包含124个真实链上合约任务，评估了6个主流LLM在Solidity代码生成方面的表现。


<details>
  <summary>Details</summary>
Motivation: 区块链发展使智能合约成为主流，但LLM在Solidity代码生成方面的有效性尚未充分探索。Solidity在LLM训练数据中占比小，具有版本敏感语法和有限灵活性，现有评估方法局限于孤立函数和合成输入，无法评估真实合约开发能力。

Method: 构建SolContractEval基准测试，包含124个来自9个主要领域的真实链上合约任务，每个任务包含完整上下文依赖、结构化合约框架和简洁任务提示，由经验开发者独立标注和交叉验证。开发基于历史交易重放的动态评估框架进行功能正确性评估。

Result: Claude-3.7-Sonnet表现最佳，但所有模型在Solidity代码生成方面表现不如通用编程语言的类级生成任务。模型在标准模式任务上表现更好，但在复杂逻辑和合约间依赖方面表现不佳，对Solidity特定特性和上下文依赖理解有限。

Conclusion: 当前LLM在Solidity代码生成方面仍有局限，特别是在处理复杂逻辑、合约间依赖和Solidity特定特性方面。需要进一步改进模型对Solidity的理解和生成能力。

Abstract: The rise of blockchain has brought smart contracts into mainstream use,
creating a demand for smart contract generation tools. While large language
models (LLMs) excel at generating code in general-purpose languages, their
effectiveness on Solidity, the primary language for smart contracts, remains
underexplored. Solidity constitutes only a small portion of typical LLM
training data and differs from general-purpose languages in its
version-sensitive syntax and limited flexibility. These factors raise concerns
about the reliability of existing LLMs for Solidity code generation.
Critically, existing evaluations, focused on isolated functions and synthetic
inputs, fall short of assessing models' capabilities in real-world contract
development.
  To bridge this gap, we introduce SolContractEval, the first contract-level
benchmark for Solidity code generation. It comprises 124 tasks drawn from real
on-chain contracts across nine major domains. Each task input, consisting of
complete context dependencies, a structured contract framework, and a concise
task prompt, is independently annotated and cross-validated by experienced
developers. To enable precise and automated evaluation of functional
correctness, we also develop a dynamic evaluation framework based on historical
transaction replay. Building on SolContractEval, we perform a systematic
evaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the
highest overall performance, though evaluated models underperform relative to
their capabilities on class-level generation tasks in general-purpose
programming languages. Second, current models perform better on tasks that
follow standard patterns but struggle with complex logic and inter-contract
dependencies. Finally, they exhibit limited understanding of Solidity-specific
features and contextual dependencies.

</details>


### [47] [HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing](https://arxiv.org/abs/2509.23835)
*Yukai Zhao,Menghan Wu,Xing Hu,Xin Xia*

Main category: cs.SE

TL;DR: HFUZZER是一个基于短语的模糊测试框架，用于检测LLM在代码生成中的包幻觉问题，能生成更多样化的任务并发现更多独特的幻觉包。


<details>
  <summary>Details</summary>
Motivation: LLM在代码生成中面临包幻觉的安全风险，可能导致软件供应链攻击，但目前缺乏针对包幻觉的测试方法。

Method: 采用基于短语的模糊测试技术，从包信息或编码任务中提取短语，生成多样化且相关的编码任务来测试LLM。

Result: 在所有测试的LLM中都触发了包幻觉，相比变异模糊测试框架发现2.60倍更多独特幻觉包，在GPT-4o中发现46个独特幻觉包。

Conclusion: LLM不仅在代码生成中，还在环境配置协助中都会出现包幻觉，需要专门的测试框架来识别和缓解这种安全风险。

Abstract: Large Language Models (LLMs) are widely used for code generation, but they
face critical security risks when applied to practical production due to
package hallucinations, in which LLMs recommend non-existent packages. These
hallucinations can be exploited in software supply chain attacks, where
malicious attackers exploit them to register harmful packages. It is critical
to test LLMs for package hallucinations to mitigate package hallucinations and
defend against potential attacks. Although researchers have proposed testing
frameworks for fact-conflicting hallucinations in natural language generation,
there is a lack of research on package hallucinations. To fill this gap, we
propose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for
package hallucinations. HFUZZER adopts fuzzing technology and guides the model
to infer a wider range of reasonable information based on phrases, thereby
generating enough and diverse coding tasks. Furthermore, HFUZZER extracts
phrases from package information or coding tasks to ensure the relevance of
phrases and code, thereby improving the relevance of generated tasks and code.
We evaluate HFUZZER on multiple LLMs and find that it triggers package
hallucinations across all selected models. Compared to the mutational fuzzing
framework, HFUZZER identifies 2.60x more unique hallucinated packages and
generates more diverse tasks. Additionally, when testing the model GPT-4o,
HFUZZER finds 46 unique hallucinated packages. Further analysis reveals that
for GPT-4o, LLMs exhibit package hallucinations not only during code generation
but also when assisting with environment configuration.

</details>


### [48] [PerfBench: Can Agents Resolve Real-World Performance Bugs?](https://arxiv.org/abs/2509.24091)
*Spandan Garg,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: 提出了PerfBench基准测试，包含81个真实世界的.NET性能bug修复任务，用于评估软件工程代理在性能优化方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注功能正确性，无法评估代理在识别和解决性能bug等非功能性问题的能力。

Method: 创建包含真实开发者修复的性能bug数据集，开发新颖的评估框架让代理生成自己的性能基准测试，通过比较执行指标来验证修复。

Result: 当前最先进的编码代理在性能优化任务上表现不佳，OpenHands代理仅达到约3%的成功率。开发的OpenHands-Perf-Agent通过性能感知工具和指令达到约20%的成功率。

Conclusion: 通过提供适当的基准测试指令和工具处理，可以显著提高代理性能，但仍需改进。PerfBench为提升代理修复性能问题的能力提供了具有挑战性的测试集。

Abstract: Performance bugs are inefficiencies in software that waste computational
resources without causing functional failures, making them particularly
challenging to detect and fix. While recent advances in Software Engineering
agents have shown promise in automated bug fixing, existing benchmarks
primarily focus on functional correctness and fail to evaluate agents'
abilities to identify and resolve non-functional issues like performance bugs.
We introduce PerfBench, a benchmark comprising 81 real-world performance
bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing
benchmarks that rely on pre-existing test suites, PerfBench features a novel
evaluation harness that allows agents to generate their own performance
benchmarks and validates fixes by comparing execution metrics collected for
developer fix and agent fix. Each task in PerfBench is derived from actual
developer fixes linked to performance-related issues, which are then verified
by human experts, ensuring real-world relevance. Our evaluation reveals that
current state-of-the-art coding agents struggle with performance optimization
tasks, with baseline OpenHands agent achieving only a ~3% success rate on our
benchmark. We develop OpenHands-Perf-Agent, which incorporates
performance-aware tooling and instructions and achieves a ~20% success rate on
the benchmark. We show that by ensuring the agent has proper instructions to
benchmark its changes and tooling for benchmark output processing, we can
improve the agent performance significantly, but room for improvement still
remains. PerfBench provides a challenging test set for furthering the
capabilities of agents in fixing performance issues.

</details>


### [49] [TENET: Leveraging Tests Beyond Validation for Code Generation](https://arxiv.org/abs/2509.24148)
*Yiran Hu,Nan Jiang,Shanchao Liang,Yi Wu,Lin Tan*

Main category: cs.SE

TL;DR: TENET是一个在测试驱动开发(TDD)环境下生成代码的LLM智能体，通过测试套件选择、代码检索和基于反馈的迭代优化来解决vibe coding中的挑战，在RepoCod和RepoEval基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在vibe coding时代，开发者越来越多地依赖LLM根据高层意图生成代码，TDD变得尤为重要，因为测试用例可以作为可执行规范来明确定义和验证功能。但面临测试套件选择、上下文检索和测试反馈利用三大挑战。

Method: TENET包含三个核心组件：(1)测试套件选择机制，选择简洁多样的测试用例；(2)定制化工具集，高效检索相关代码并支持交互式调试；(3)基于反思的优化工作流，迭代分析失败、补充上下文并应用代码优化。

Result: 在RepoCod和RepoEval基准上分别达到69.08%和81.77%的Pass@1，比最佳基线分别高出9.49和2.17个百分点。

Conclusion: 这是首个在仓库级别上下文中研究测试驱动代码生成的工作，探讨了测试套件的不同方面如何影响TDD环境下LLM智能体的性能。

Abstract: Test-Driven Development (TDD) is a widely adopted software engineering
practice that requires developers to create and execute tests alongside code
implementation, ensuring that software behavior is continuously validated and
refined. In the era of vibe coding, where developers increasingly delegate code
writing to large language models (LLMs) by specifying high-level intentions,
TDD becomes even more crucial, as test cases serve as executable specifications
that explicitly define and verify intended functionality beyond what
natural-language descriptions and code context can convey. While vibe coding
under TDD is promising, there are three main challenges: (1) selecting a small
yet effective test suite to improve the generation accuracy and control the
execution workload, (2) retrieving context such as relevant code effectively,
and (3) systematically using test feedback for effective code refinement. To
address these challenges, we introduce TENET, an LLM agent for generating
functions in complex real-world repositories under the TDD setting. TENET
features three components: (1) a novel test harness mechanism that selects a
concise test suite to maximize diversity of target usage scenarios; (2) a
tailored agent toolset that performs efficient retrieval of relevant code with
interactive debugging; and (3) a reflection-based refinement workflow that
iteratively analyzes failures, replenishes context, and applies code
refinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval
benchmarks, outperforming the best agentic baselines by 9.49 and 2.17
percentage points, respectively. In addition, this is the first study of
test-driven code generation with repository-level context, examining how
different aspects of test suites affect the performance of LLM agents under the
TDD setting.

</details>


### [50] [Agentic Services Computing](https://arxiv.org/abs/2509.24380)
*Shuiguang Deng,Hailiang Zhao,Ziqi Wang,Guanjie Cheng,Peng Chen,Wenzhuo Qian,Zhiwei Ling,Jianwei Yin,Albert Y. Zomaya,Schahram Dustdar*

Main category: cs.SE

TL;DR: 该论文提出了智能服务计算(ASC)新范式，将服务重新构想为智能、自适应、社会嵌入的实体，并构建了基于生命周期的四阶段框架(设计、部署、运营、演进)和四个研究维度来系统分析ASC。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的智能体兴起，服务计算正在从静态的请求-响应函数向动态、目标导向、自主的多智能体生态系统转变，需要建立新的理论框架来指导这一变革。

Method: 采用生命周期驱动框架，围绕设计、部署、运营、演进四个核心阶段，从感知与环境建模、自主决策与任务执行、多智能体协作与组织、评估与可信度四个维度系统分析ASC。

Result: 研究发现智能服务不是简单组装而是编排的：上下文感知支持稳健部署，自主推理支持实时运营，协作结构通过交互涌现演进，可信度是贯穿整个生命周期的关键要求。

Conclusion: 通过整合传统服务计算原则与基于LLM的多智能体系统进展，为ASC建立了全面前瞻的基础，为开发自适应、可问责、以人为本的智能服务提供了统一参考。

Abstract: The rise of LLM-powered agents is driving a fundamental transformation in
services computing: from static, request-response functions to dynamic,
goal-oriented, and autonomous multi-agent ecosystems. In response to this
shift, we introduce Agentic Service Computing (ASC), a new paradigm that
reimagines services as intelligent, self-adaptive, and socially embedded
entities. This comprehensive survey presents a lifecycle-driven framework for
ASC, structured around four core phases: Design, Deployment, Operation, and
Evolution. We systematically analyze ASC through four foundational research
dimensions: (1) Perception, Context, and Environment Modeling, (2) Autonomous
Decision-Making and Task Execution, (3) Multi-Agent Collaboration and
Organization, and (4) Evaluation, Value Alignment, and Trustworthiness. We
examine how these dimensions are instantiated, integrated, and continuously
adapted across the service lifecycle. Our synthesis reveals that agentic
services are not merely assembled but orchestrated: contextual awareness
enables robust deployment; autonomous reasoning supports real-time operation;
collaborative structures emerge and evolve through interaction; and
trustworthiness must be upheld as a cross-cutting, lifelong imperative. We
further identify and discuss emerging trends shaping the future of ASC. By
integrating classical principles of services computing with advances in
LLM-based multi-agent systems, this work establishes a holistic and
forward-looking foundation for ASC. It provides a unified reference for
researchers and practitioners aiming to develop adaptive, accountable, and
human-centered intelligent services.

</details>


### [51] [Unit Test Update through LLM-Driven Context Collection and Error-Type-Aware Refinement](https://arxiv.org/abs/2509.24419)
*Yuanhe Zhang,Zhiquan Yang,Shengyi Pan,Zhongxin Liu*

Main category: cs.SE

TL;DR: TESTUPDATER是一个基于LLM的自动化单元测试更新方法，能够即时响应生产代码变更，同时修复和增强测试用例，通过上下文分析、逐步提示和迭代优化机制显著提高测试更新的正确性。


<details>
  <summary>Details</summary>
Motivation: 当前手动维护单元测试效率低下且存在延迟修复风险，现有自动化方法主要关注测试修复而忽视测试增强，且依赖基于规则的上下文收集，难以处理复杂代码变更并产生低正确性的测试用例。

Method: 首先利用LLM分析代码变更并识别相关上下文，然后通过精心设计的提示逐步指导LLM处理各类代码变更和引入新依赖，实现测试修复和增强，最后引入基于错误类型的迭代优化机制执行更新后的测试并修复失败。

Result: 在新建基准UPDATES4J上，TESTUPDATER实现了94.4%的编译通过率和86.7%的测试通过率，分别比最先进方法SYNTER高出15.9%和20.0%，分支覆盖率和行覆盖率分别高出12.9%和15.2%。

Conclusion: TESTUPDATER通过结合LLM的上下文分析和迭代优化机制，有效解决了现有测试维护方法的局限性，在测试更新正确性和覆盖率方面显著优于现有方法。

Abstract: Unit testing is critical for ensuring software quality and software system
stability. The current practice of manually maintaining unit tests suffers from
low efficiency and the risk of delayed or overlooked fixes. Therefore, an
automated approach is required to instantly update unit tests, with the
capability to both repair and enhance unit tests. However, existing automated
test maintenance methods primarily focus on repairing broken tests, neglecting
the scenario of enhancing existing tests to verify new functionality.
Meanwhile, due to their reliance on rule-based context collection and the lack
of verification mechanisms, existing approaches struggle to handle complex code
changes and often produce test cases with low correctness. To address these
challenges, we propose TESTUPDATER, a novel LLM based approach that enables
automated just-in-time test updates in response to production code changes.
TESTUPDATER first leverages the LLM to analyze code changes and identify
relevant context, which it then extracts and filters. Then, through carefully
designed prompts, TESTUPDATER guides the LLM step by step to handle various
types of code changes and introduce new dependencies, enabling both test repair
and enhancement. Finally, we introduce an error-type-aware iterative refinement
mechanism that executes the LLM-updated tests and repairs failures, which
significantly improves the overall correctness of test updates. Since existing
test repair datasets lack scenarios of test enhancement, we further construct a
new benchmark, UPDATES4J, with 195 real-world samples from 7 projects.
Experimental results show that TESTUPDATER achieves a compilation pass rate of
94.4% and a test pass rate of 86.7%, outperforming the state-of-the-art method
SYNTER by 15.9% and 20.0%, respectively. Furthermore, TESTUPDATER exhibits
12.9% higher branch coverage and 15.2% greater line coverage than SYNTER.

</details>


### [52] [SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code](https://arxiv.org/abs/2509.24507)
*Qinglin Wang,Zhihong Sun,Ruyun Wang,Tao Huang,Zhi Jin,Ge Li,Chen Lyu*

Main category: cs.SE

TL;DR: SemGuard是一个语义评估驱动的框架，通过实时行级语义监督来减少LLM生成代码时的语义错误。它在解码过程中注入语义信号，无需执行程序或测试用例就能检测和修复错误。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码中语义错误（程序能编译但行为错误）占大多数，而现有的修复方法存在延迟、依赖不完整测试套件和错误定位不准的问题。需要在代码生成过程中早期注入语义信号来阻止错误传播。

Method: 构建SemDiff数据集进行细粒度标注，训练语义评估器嵌入LLM解码器，实时监控部分代码，检测偏差后回滚到错误行并指导重新生成。

Result: 在四个基准测试中，SemGuard始终优于最先进基线。在SemDiff上比ROCODE降低语义错误率19.86%，在LiveCodeBench上使用CodeLlama-7B提升Pass@1 48.92%。在MBPP和SemDiff-Java上也表现出模型和语言无关的有效性。

Conclusion: SemGuard通过实时语义监督有效减少了LLM生成代码时的语义错误，展示了在解码过程中早期注入语义信号的可行性。

Abstract: Large Language Models (LLMs) can translate natural language requirements into
code, yet empirical analyses of representative models reveal that semantic
errors-programs that compile but behave incorrectly-constitute the majority of
observed faults (e.g., >60% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc
repair pipelines detect such faults only after execution, incurring latency,
relying on incomplete test suites, and often mis-localizing the defect. Since
semantic drift originates in the autoregressive decoding process, intervening
while the code is being generated is a direct way to stop error propagation.
Constrained-decoding approaches such as ROCODE attempt this, but still wait
until the entire program runs to obtain feedback and use entropy heuristics
that do not truly capture semantics. A more effective solution must inject
semantic signals-early and precisely-into the decoding process.We present
SemGuard, a semantic-evaluator-driven framework that performs real-time,
line-level semantic supervision. To train the evaluator, we build SemDiff, the
first dataset with fine-grained annotations that mark the exact line where a
correct and an incorrect implementation diverge. The evaluator, once embedded
in the LLM's decoder, flags deviations on partial code, rolls back to the
faulty line, and guides regeneration-without executing the program or requiring
test cases. Across four benchmarks, SemGuard consistently outperforms
state-of-the-art baselines. It lowers the semantic error rate by 19.86% on
SemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on the real-world
LiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP
and for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating
model- and language-agnostic effectiveness.

</details>


### [53] [Agentic Specification Generator for Move Programs](https://arxiv.org/abs/2509.24515)
*Yu-Fu Fu,Meng Xu,Taesoo Kim*

Main category: cs.SE

TL;DR: MSG是一个为Move智能合约设计的自动化规范生成工具，展示了LLM在非主流语言中的强大代码理解和规范生成能力，成功为84%的测试函数生成可验证规范。


<details>
  <summary>Details</summary>
Motivation: 现有工具主要关注主流编程语言，而新兴且面向验证的语言如Move未被充分探索，需要专门工具来填补这一空白。

Method: 采用基于代理的模块化设计，明确利用规范语言特性，并整合验证工具链的反馈机制。

Result: 成功为84%的Move函数生成可验证规范，识别出专家遗漏的条款，比传统设计多生成57%的可验证条款，整合反馈后生成的可验证规范增加30%。

Conclusion: LLM在非主流语言中具有强大的规范生成能力，模块化设计和验证反馈能显著提升规范质量。

Abstract: While LLM-based specification generation is gaining traction, existing tools
primarily focus on mainstream programming languages like C, Java, and even
Solidity, leaving emerging and yet verification-oriented languages like Move
underexplored. In this paper, we introduce MSG, an automated specification
generation tool designed for Move smart contracts. MSG aims to highlight key
insights that uniquely present when applying LLM-based specification generation
to a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust
code comprehension and generation capabilities even for non-mainstream
languages. MSG successfully generates verifiable specifications for 84% of
tested Move functions and even identifies clauses previously overlooked by
experts. Additionally, MSG shows that explicitly leveraging specification
language features through an agentic, modular design improves specification
quality substantially (generating 57% more verifiable clauses than conventional
designs). Incorporating feedback from the verification toolchain further
enhances the effectiveness of MSG, leading to a 30% increase in generated
verifiable specifications.

</details>


### [54] [Bridging Developer Instructions and Code Completion Through Instruction-Aware Fill-in-the-Middle Paradigm](https://arxiv.org/abs/2509.24637)
*Zhensu Sun,Chengran Yang,Chao Peng,Pengfei Gao,Xiaoning Du,Li Li,David Lo*

Main category: cs.SE

TL;DR: 提出了IFIM方法，通过指令调优增强FIM代码补全模型，在保留原有补全能力的同时显著提升指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码LLM仅经过FIM预训练，难以有效利用开发者提供的自然语言指令来澄清意图，而传统指令调优技术会降低FIM性能。

Method: IFIM方法在传统FIM训练目标基础上，在输入中显式加入指令部分，让模型从(前缀、指令、后缀)三元组中学习。

Result: 在HumanEval-infilling基准上，Pass@1分数从84.6%提升至93.6%，且不损害无指令时的原始FIM性能。

Conclusion: IFIM成功解决了指令跟随与代码填充能力之间的权衡问题，为代码补全系统提供了更有效的解决方案。

Abstract: Large Language Models (LLMs) have significantly advanced code completion, yet
they often fail when the developer's intent is underspecified in the code
context. To address this, developers usually add natural language instructions
(e.g., comments) into the code context to clarify their intent. However,
existing code LLMs applied for code completion systems merely undergo a
fill-in-the-middle (FIM) pre-training, which struggles to leverage this
information effectively due to the lack of instruction-like training data.
Existing instruction-tuning techniques, which improve instruction-following in
general code generation, paradoxically degrade FIM performance, forcing a
trade-off between instruction-following and infilling capabilities. To address
this gap, we introduce Instruction-aware Fill-in-the-Middle (IFIM), an
instruction-tuning method specifically designed to enhance FIM code completion
models. IFIM extends the conventional FIM training objective by incorporating
an explicit instruction section into the input, enabling the model to learn
from (prefix, instruction, suffix) triplets. This approach allows the model to
effectively leverage developer-provided directives while preserving its core
completion abilities when no instructions are present. To facilitate this, we
constructed a large-scale dataset by using GPT-4o to generate concise,
intent-focused instructions for code infilling examples. We evaluated IFIM by
applying it to two popular base models, Deepseek-Coder and Qwen2.5-Coder, on
the benchmarks derived from HumanEval-infilling and RepoMasterEval. The results
demonstrate that IFIM significantly improves instruction-following
capabilities, boosting the Pass@1 score from 84.6% to 93.6% on
HumanEval-infilling. Moreover, this enhancement does not compromise the models'
original performance on FIM code completion tasks with no instructions
provided.

</details>


### [55] [Evaluating SAP Joule for Code Generation](https://arxiv.org/abs/2509.24828)
*Joshua Heisler,Johannes Reisinger,Andreas Fischer*

Main category: cs.SE

TL;DR: 本文首次对SAP Joule生成模型在JavaScript代码生成能力进行对比评估，在HumanEval-X基准测试中排名第五，准确率达80.49%。


<details>
  <summary>Details</summary>
Motivation: SAP发布了专有生成模型Joule作为代码助手，但尚未针对SAP特定的ABAP代码生成，本文旨在评估其在JavaScript等通用语言上的代码生成能力。

Method: 使用HumanEval-X JavaScript基准测试，将SAP Joule与29个其他模型进行对比评估。

Result: SAP Joule在评估中排名第五，严格准确率达到80.49%。

Conclusion: SAP Joule在JavaScript代码生成方面表现出色，这是对其代码生成能力的首次比较评估。

Abstract: SAP has released its own proprietary generative model SAP Joule, intended for
various generative tasks, including serving as a code assistant for software
engineers. While Joule is yet not focused on SAP-specific ABAP code generation,
it can be used for other common languages, including Javascript. This paper
compares SAP Joules Javascript coding capabilities against a total of 29 other
models using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict
accuracy of 80.49% as the fifth best model in our evaluation. To the best of
our knowledge, this is the first comparative evaluation of SAP Joule code
generation capabilities.

</details>


### [56] [DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern](https://arxiv.org/abs/2509.24975)
*Lekang Yang,Yuetong Liu,Yitong Zhang,Jia Li*

Main category: cs.SE

TL;DR: DiffTester是一个针对扩散语言模型(dLLMs)的单元测试生成加速框架，通过识别测试用例中的重复结构模式，在保持测试质量的同时提高生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在单元测试生成中逐个token生成效率低下，而dLLMs虽然支持并行生成但面临效率与测试质量之间的权衡问题。

Method: 通过抽象语法树分析动态识别测试用例中的重复结构模式，自适应增加每个步骤生成的token数量而不影响输出质量。

Result: 在三个基准测试和两种代表性模型上的实验表明，DiffTester在保持测试覆盖率的同时实现了显著加速，且在不同dLLMs和编程语言上具有良好的泛化能力。

Conclusion: DiffTester为软件开发中的高效单元测试生成提供了实用且可扩展的解决方案。

Abstract: Software development relies heavily on extensive unit testing, which makes
the efficiency of automated Unit Test Generation (UTG) particularly important.
However, most existing LLMs generate test cases one token at a time in each
forward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)
have emerged, offering promising parallel generation capabilities and showing
strong potential for efficient UTG. Despite this advantage, their application
to UTG is still constrained by a clear trade-off between efficiency and test
quality, since increasing the number of tokens generated in each step often
causes a sharp decline in the quality of test cases. To overcome this
limitation, we present DiffTester, an acceleration framework specifically
tailored for dLLMs in UTG. The key idea of DiffTester is that unit tests
targeting the same focal method often share repetitive structural patterns. By
dynamically identifying these common patterns through abstract syntax tree
analysis during generation, DiffTester adaptively increases the number of
tokens produced at each step without compromising the quality of the output. To
enable comprehensive evaluation, we extend the original TestEval benchmark,
which was limited to Python, by introducing additional programming languages
including Java and C++. Extensive experiments on three benchmarks with two
representative models show that DiffTester delivers significant acceleration
while preserving test coverage. Moreover, DiffTester generalizes well across
different dLLMs and programming languages, providing a practical and scalable
solution for efficient UTG in software development. Code and data are publicly
available at https://github.com/wellbeingyang/DLM4UTG-open .

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [57] [奖励何必是数值？LLM从文本反馈中也能<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzIzNzIwNTMxMQ==&mid=2649372184&idx=1&sn=57622be0bd2bc6ba5262ccbcf9084b99&chksm=f1aaecf80bafe60506884e4cdbeae42b2853182693fbf2c493b0dbc20adcb09b561b63d77848#rd)
*零一瓦舍*

Main category: wechat.article

TL;DR: 在 LLM 的对齐领域，强化学习从人类反馈中学习 （RLHF） 已成为一种标准范式。该范式的理论基石是 Richard Sutton 提出的「奖励假设」 （Reward Hypothesis），即所有形式的目标都可以被表达为对一个累积标量奖励信号期望值的最大化


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在 LLM 的对齐领域，强化学习从人类反馈中学习 （RLHF） 已成为一种标准范式。该范式的理论基石是 Richard Sutton 提出的「奖励假设」 （Reward Hypothesis），即所有形式的目标都可以被表达为对一个累积标量奖励信号期望值的最大化

</details>


### [58] [<em class="highlight">强化学习</em>之父理查德·萨顿：大语言模型是“死路一条”](http://mp.weixin.qq.com/s?__biz=MjM5NTg2NTU0Ng==&mid=2656659539&idx=1&sn=98d04fd383ed59875610842490d67d2d&chksm=bc9d58a2f2302e904793feaeabe9bda08f716a57f41b97463029f08be2b4fce4b5d539061c5f#rd)
*21CTO*

Main category: wechat.article

TL;DR: （ 1：46 ）强化学习是基础人工智能，专注于让智能体理解并弄清楚它所处的世界。相比之下，大型语言模型 （LLM） 则专注于模仿人类及其言语，而不是让智能体自己弄清楚该做什么。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: （ 1：46 ）强化学习是基础人工智能，专注于让智能体理解并弄清楚它所处的世界。相比之下，大型语言模型 （LLM） 则专注于模仿人类及其言语，而不是让智能体自己弄清楚该做什么。

</details>


### [59] [通义提出SPELL框架：用于长上下文LLM的自博弈<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzkyMjY3MjY1MQ==&mid=2247484449&idx=1&sn=dbab946b7db03a1b4253925f0112ab63&chksm=c008c90340388bdb9fea8f8439751e37d9aeaa9baa4fea7ddad87955f70e4cde5266c6250f88#rd)
*oneLLM*

Main category: wechat.article

TL;DR: 强化学习（RL）是提升LLM推理能力的有效手段，其中带可验证奖励的RL（RLVR） 在短上下文任务（数学计算、代码生成）中表现突出——这类任务可通过规则/程序验证答案正确性（如代码执行结果、数学公式计算）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（RL）是提升LLM推理能力的有效手段，其中带可验证奖励的RL（RLVR） 在短上下文任务（数学计算、代码生成）中表现突出——这类任务可通过规则/程序验证答案正确性（如代码执行结果、数学公式计算）。

</details>


### [60] [SCI发文热点：物理信息<em class="highlight">强化学习</em>，现成套路拿走不谢！](http://mp.weixin.qq.com/s?__biz=Mzg2NzYxODI3MQ==&mid=2247512948&idx=1&sn=a861220a3019561f50e32799e366de04&chksm=cfcb4dc3a9cd4ef5428a50e0b68b5f95eb3e0c611b49e43e6aaf6b185d4b8e303d7f61e7d604#rd)
*学姐带你玩AI*

Main category: wechat.article

TL;DR: 方法：论文提出了一种物理信息强化学习方法，用于实时最优潮流问题。该方法通过物理信息演员确保操作满足电力系统潮流方程的等式约束，并利用约束策略梯度直接计算不等式约束成本，纠正不可行操作。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 方法：论文提出了一种物理信息强化学习方法，用于实时最优潮流问题。该方法通过物理信息演员确保操作满足电力系统潮流方程的等式约束，并利用约束策略梯度直接计算不等式约束成本，纠正不可行操作。

</details>


### [61] [学术分享丨陈丹琦新作：大模型<em class="highlight">强化学习</em>的第三条路，8B小模型超越GPT-4o](http://mp.weixin.qq.com/s?__biz=MjM5ODIwNjEzNQ==&mid=2649910538&idx=3&sn=e7b0cf766c54d35178f98d700abf835f&chksm=bff8e5dc5b09361212054f507cbe37148aba2d8c9105b8e4748e1d96979379fd39b83dcf3a8a#rd)
*中国人工智能学会*

Main category: wechat.article

TL;DR: 网友觉得，这种方法为通用强化学习设定了一个新基线：谁制定了偏好的定义，谁就是后训练时代的“新得分手”。jhxhgukvcxx @jhxhgukvcxx · sep 27 interesting. having the model write its own draft and letting a preference model grade it sets a new baseline fo


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 网友觉得，这种方法为通用强化学习设定了一个新基线：谁制定了偏好的定义，谁就是后训练时代的“新得分手”。jhxhgukvcxx @jhxhgukvcxx · sep 27 interesting. having the model write its own draft and letting a preference model grade it sets a new baseline fo

</details>


### [62] [<em class="highlight">强化学习</em>无法让智能体高效理解世界｜杨立昆最新演讲实录](http://mp.weixin.qq.com/s?__biz=Mzg2NzY0MTkzOQ==&mid=2247493791&idx=1&sn=8e7e8e3b82e1bdd994493d9a8f0afecd&chksm=cfc20398a63c8997cb9b770c438eb28ce4ff098908d98c4c78871388214c4d7b2b527b3352da#rd)
*数字开物*

Main category: wechat.article

TL;DR: 杨立昆认为，依赖强化学习来让智能体理解世界是低效的，通过自我监督学习构建强大的世界模型才是根本。此外，试图通过重建像素（生成图像/视频）来训练世界模型是“完全浪费时间”，其效果远不如联合嵌入等判别式方


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 杨立昆认为，依赖强化学习来让智能体理解世界是低效的，通过自我监督学习构建强大的世界模型才是根本。此外，试图通过重建像素（生成图像/视频）来训练世界模型是“完全浪费时间”，其效果远不如联合嵌入等判别式方

</details>


### [63] [<em class="highlight">强化学习</em>之父：大语言模型正在扼杀真正的人工智能](http://mp.weixin.qq.com/s?__biz=MzYyNDA0MzYzNw==&mid=2247484443&idx=1&sn=57f8bce19d855203f2cde4f2432bc581&chksm=f1d7db21bc0c00a1f1ae28f7baca23e546e7750175114a398879104ee49b9e0ffa410d927759#rd)
*AI大模型小艺*

Main category: wechat.article

TL;DR: 强化学习之父richard sutton的忠告：大语言模 型正在扼杀真正的人工智能。I真正的智能，无法被教会，只能被学会。在人工智能的星空中，两条截然不同的道路正将我们 引向迥异的未来。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习之父richard sutton的忠告：大语言模 型正在扼杀真正的人工智能。I真正的智能，无法被教会，只能被学会。在人工智能的星空中，两条截然不同的道路正将我们 引向迥异的未来。

</details>


### [64] [<em class="highlight">强化学习</em>之父给LLM判死刑！站队LeCun：我们全搞错了](http://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247574104&idx=3&sn=6ad550be9f6de02dfd28b1c09d573609&chksm=eaf93b982d01fa64afc86a6700de8a4df8b358ec9a96619172b34ca0e97106769885f288c00d#rd)
*机器学习算法与自然语言处理*

Main category: wechat.article

TL;DR: 在强化学习中，有正确的话语要说，有正确的动作要做，正确的事就是能够获得奖励的事。我们对正确的事是有定义的，因此可以预先掌握或通过他人获取关于正确的事的知识。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在强化学习中，有正确的话语要说，有正确的动作要做，正确的事就是能够获得奖励的事。我们对正确的事是有定义的，因此可以预先掌握或通过他人获取关于正确的事的知识。

</details>


### [65] [大型推理模型<em class="highlight">强化学习</em>研究综述 （3）](http://mp.weixin.qq.com/s?__biz=Mzk1NzY2OTYzMA==&mid=2247488210&idx=1&sn=091df7c1a9059a369bca7667cdcfd0dd&chksm=c296ad9e7f24c55ea429f11c405f9cbb3a250cce86f669c0d81bdd5559a43fe802ad7b9590f3#rd)
*虎sir的AI技术博客*

Main category: wechat.article

TL;DR: 大型推理模型强化学习研究综述 （2）接着往下分析。foundational problems rl's role model prior training recipes rl vs. sft reward type reward design vs vs vs vs 6 sharpening discovery weak strong tricks traps generalize memori...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大型推理模型强化学习研究综述 （2）接着往下分析。foundational problems rl's role model prior training recipes rl vs. sft reward type reward design vs vs vs vs 6 sharpening discovery weak strong tricks traps generalize memorize process outcome verifiable generative rewards rewar

</details>


### [66] [对具身智能中<em class="highlight">强化学习</em>应用的一点点观察](http://mp.weixin.qq.com/s?__biz=Mzk2NDM2MTgxMQ==&mid=2247483668&idx=1&sn=3e71e6b6474a4d4d4f12890146e5e837&chksm=c50f39f9b1b02114f7b15142dce5c2af50bb58b1ea8fd7c1f076b84ce592d77fb8f761372a43#rd)
*席琳Sclin*

Main category: wechat.article

TL;DR: 以及有博弈属性的一切场景里使用强化学习（RL， reinforcement learning）；全世界研究RL的专家学者（有明确的H-index）超过1000人，研究领域与RL相关的持有PhD/MS学历的青年学者超过10000人，Google Scholar与RL直接或间接相关的Paper数量超


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 以及有博弈属性的一切场景里使用强化学习（RL， reinforcement learning）；全世界研究RL的专家学者（有明确的H-index）超过1000人，研究领域与RL相关的持有PhD/MS学历的青年学者超过10000人，Google Scholar与RL直接或间接相关的Paper数量超

</details>


### [67] [AI时代：<em class="highlight">Agentic</em> Organization（<em class="highlight">智能体</em>组织）](http://mp.weixin.qq.com/s?__biz=MzYyMzAwMDc3Nw==&mid=2247486664&idx=1&sn=1ecff11d62d620dbd201d39e2c197a2c&chksm=fea2a4e6dc3bb4cad9fb8f4dc6585350c73a2d4f13de882cce6ffd21815dbb20177970f9c382#rd)
*钟仁8629*

Main category: wechat.article

TL;DR: AI时代：Agentic Organization（智能体组织）关键词：AI-first、混合劳动力、人类站到“loop之上”。流程由AI执行，人类负责目标设定、方向把控、价值权衡。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: AI时代：Agentic Organization（智能体组织）关键词：AI-first、混合劳动力、人类站到“loop之上”。流程由AI执行，人类负责目标设定、方向把控、价值权衡。

</details>


### [68] [AI <em class="highlight">智能体</em>核心原理综述：从 <em class="highlight">Agentic</em> AI 到 AI Agent](http://mp.weixin.qq.com/s?__biz=MzkyNjE5MjAxOQ==&mid=2247491434&idx=1&sn=c194cbac076b2e78dde2c56463f20180&chksm=c33ca0520443c3d25f6271f2a81a25528b3e50777a8f26c0abcffd7d0cc34596dd6567c20a20#rd)
*波粒二象APP*

Main category: wechat.article

TL;DR: Agentic AI 的背景LLM 最初的产品形态是由 OpenAI 领衔的 ChatBot（聊天机器人），底层支撑技术是 Transformer 架构大语言模型，最初专注于语言文本领域的人工智能应用场景。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 的背景LLM 最初的产品形态是由 OpenAI 领衔的 ChatBot（聊天机器人），底层支撑技术是 Transformer 架构大语言模型，最初专注于语言文本领域的人工智能应用场景。

</details>


### [69] [<em class="highlight">Agentic</em> 应用架构｜来自 Shopify 卖家智能助手 Sidekick 经验分享](http://mp.weixin.qq.com/s?__biz=MzU3Mzg1Njk0Ng==&mid=2247485684&idx=1&sn=b1155df4750df95d37d0e27b902da563&chksm=fcc6fd9847509a7e7874b4c4e2b59feb4a2e5c4729146e84d469acd9e9556bad3df85b22054f#rd)
*求索云途*

Main category: wechat.article

TL;DR: 经典的 Agentic 循环架构action human ←------> llm call environment feedback stopAnthropic 提出 Agentic Loop 架构，一个无限循环，人提供输入，大语言模型理解输入并决定采取什么行动，这些行动在特定的环境中执行，执行结果或其他反馈回到


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 经典的 Agentic 循环架构action human ←------> llm call environment feedback stopAnthropic 提出 Agentic Loop 架构，一个无限循环，人提供输入，大语言模型理解输入并决定采取什么行动，这些行动在特定的环境中执行，执行结果或其他反馈回到

</details>


### [70] [麦肯锡重磅发布：<em class="highlight">智能体</em>组织（<em class="highlight">Agentic</em> Organization）——AI时代全新范式，企业与个人都无法回避](http://mp.weixin.qq.com/s?__biz=MzkwMDU0ODA5MQ==&mid=2247486573&idx=1&sn=b5e576c813b86ec5522f4c71610d0bce&chksm=c17de2487bfd26732d8a53c6ab6bd2464fe134c1b93d106af446b8a5c47c2d46a5d32bf85ebe#rd)
*华顿尔AI*

Main category: wechat.article

TL;DR: accountabilityand agentic controls with humanWorkforce，people， andculture ture of craftsmanship Deep specialization and cul- Narrowly specialized functionalplanning talent working in a culture of Know...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: accountabilityand agentic controls with humanWorkforce，people， andculture ture of craftsmanship Deep specialization and cul- Narrowly specialized functionalplanning talent working in a culture of Knowledge workers with

</details>


### [71] [产品发布 | TechAgent 3.0：<em class="highlight">Agentic</em> 版本上线，为产业洞察打造可指挥的<em class="highlight">智能体</em>](http://mp.weixin.qq.com/s?__biz=MzkwNzYzNDc0Mg==&mid=2247484234&idx=1&sn=bcfcbf31940afe3fff1e8bb3bde1be11&chksm=c11ca3a4666351e39d43160ed757fbc25516fab1fd3935a20bacf6fc572968e65d9623359dd5#rd)
*TechAgent*

Main category: wechat.article

TL;DR: 点击蓝字，关注我们TechAgent 3.0， Agentic 版本正式上线！从 2023 年 TechAgent 1.0 创建产业链数据与业务场景基座，到 2024 年 2.0 版 融入大模型能力，“让智能体真正懂产业” 一直是素问团队的初心与努力方向。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 点击蓝字，关注我们TechAgent 3.0， Agentic 版本正式上线！从 2023 年 TechAgent 1.0 创建产业链数据与业务场景基座，到 2024 年 2.0 版 融入大模型能力，“让智能体真正懂产业” 一直是素问团队的初心与努力方向。

</details>


### [72] [知乎直答<em class="highlight">Agentic</em>升级：从搜索工具到思考伙伴的蜕变](http://mp.weixin.qq.com/s?__biz=MzkyOTY1NjMyMQ==&mid=2247484390&idx=1&sn=b6bc8d6451528b91cd6c4309dd4ac7ec&chksm=c30c9f7b4e78cb4c2f8620bed2775734f5ac65ff15d94f20dec967514001aa645033161b7ed9#rd)
*语义涌现*

Main category: wechat.article

TL;DR: 而Agentic助手则像一位资深顾问，能够进行多轮思考[2]。eaith！histe 4 aac k.ot 4：4t leaeonot当用户提出"如何学习机器学习"这样的开放性问题时，Agentic助手会先拆解问题，制定搜索策略，然后进行多轮信息搜集和分析，最终给出结构


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 而Agentic助手则像一位资深顾问，能够进行多轮思考[2]。eaith！histe 4 aac k.ot 4：4t leaeonot当用户提出"如何学习机器学习"这样的开放性问题时，Agentic助手会先拆解问题，制定搜索策略，然后进行多轮信息搜集和分析，最终给出结构

</details>


### [73] [发展的眼光看<em class="highlight">Agentic</em> AI在联络中心落地](http://mp.weixin.qq.com/s?__biz=MjM5MzM4NzM1Nw==&mid=2651304947&idx=1&sn=451edfab8912f007ce95a7a3bfaebd86&chksm=bc864da473c8be23a9fb66d70c71e4f8b108c989ab74d0013cffeb19877f4697bd3d2077a6e8#rd)
*教育风箱*

Main category: wechat.article

TL;DR: 过去客服的主旋律是“答问题”，今天 Agentic AI 的主题是“做事情”。它能查物流、改地址、补开发票、发起 RMA、触发退款、重置密码、校验权益、创建工单、路由分派、推送分步自助方案，甚至结合历史偏好做交叉推荐与主


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 过去客服的主旋律是“答问题”，今天 Agentic AI 的主题是“做事情”。它能查物流、改地址、补开发票、发起 RMA、触发退款、重置密码、校验权益、创建工单、路由分派、推送分步自助方案，甚至结合历史偏好做交叉推荐与主

</details>


### [74] [搜索框里装了个“大脑”!<em class="highlight">Agentic</em> Search不仅能搜,还能规划执行反思,彻底颠覆你的工作流](http://mp.weixin.qq.com/s?__biz=MzI1MTExMzYyNw==&mid=2247540639&idx=3&sn=6287c5a8bc9917a91c79b3ade0193221&chksm=e8393e202dea5dd2e625ee4fb22ce5737263fb83947f104f821ee85418965b46648b071a3dec#rd)
*Huintellimance*

Main category: wechat.article

TL;DR: 要理解Agentic Search的技术原理，我们需要深入了解其底层架构。根据学术研究，现代的Agentic Search系统主要基于多Agent协同架构和增强RAG框架构建。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 要理解Agentic Search的技术原理，我们需要深入了解其底层架构。根据学术研究，现代的Agentic Search系统主要基于多Agent协同架构和增强RAG框架构建。

</details>


### [75] [【精选报告】<em class="highlight">大模型</em>专题一：人工智能产业链速看：2025年最值得关注的中文<em class="highlight">大模型</em>全景图！（附PDF下载）](http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247562995&idx=1&sn=1754ef5bdfdc3a5757e68dd467cfaf7d&chksm=fc1659bf5bedd6ae1ce5ecd5f63315261c239034d1b20c47b155dfb46bea9d422bb4a1aa5b5b#rd)
*人工智能产业链union*

Main category: wechat.article

TL;DR: superclue：2025年最值得关注的中文大模型全景图。superclue 中文大模型综合性测评基准 文心一言 通义千问 腾讯混元 商汤日日新 bluelm 360智脑 天工 mi milm 中科闻歌 sensenova 紫东太初 澜舟科技 通用闭源 字节豆包 kimi.ai minimax 云从科


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: superclue：2025年最值得关注的中文大模型全景图。superclue 中文大模型综合性测评基准 文心一言 通义千问 腾讯混元 商汤日日新 bluelm 360智脑 天工 mi milm 中科闻歌 sensenova 紫东太初 澜舟科技 通用闭源 字节豆包 kimi.ai minimax 云从科

</details>


### [76] [【报告】<em class="highlight">大模型</em>专题五：深懂企业垂类<em class="highlight">大模型</em>：企业服务<em class="highlight">大模型</em>白皮书发布（附PDF下载）](http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247562995&idx=5&sn=c0af8fb13738792e15fe39fe2a40c6d3&chksm=fc933f3dce9fc5108b5bbf488c8c6fc8f23afc271a1004a12c9b2597cac38ff383ad5e58058a#rd)
*人工智能产业链union*

Main category: wechat.article

TL;DR: 1 个大模型平台 + 2 个核心框架：大模型微调训练平台：支持企业结合专属数据，提供模型开发、微调、评估工具，实现个性化模型构建；Agent 框架：构建智能助理 “智友”，通过语义理解、任务拆解、API 调度，实现复杂业务流


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1 个大模型平台 + 2 个核心框架：大模型微调训练平台：支持企业结合专属数据，提供模型开发、微调、评估工具，实现个性化模型构建；Agent 框架：构建智能助理 “智友”，通过语义理解、任务拆解、API 调度，实现复杂业务流

</details>


### [77] [国产<em class="highlight">大模型</em>集体国庆！最强国产编程模型诞生，寒武纪摩尔线程火速适配](http://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&mid=2652790181&idx=1&sn=3c341f1f06ee5f868987e46b21d1fb8f&chksm=854647e6fc1abda4e75a3da775b16ad773ee8bebe9a2b22493240a55c41b1987830fe8dee09a#rd)
*智东西*

Main category: wechat.article

TL;DR: 今天下午，智谱AI正式发布新一代大模型GLM-4.6，就在昨晚，DeepSeek也宣布推出DeepSeek-V3.2-Exp实验版模型。两家国产大模型领军企业在国庆假期毫不放松，加班加点推进技术迭代。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 今天下午，智谱AI正式发布新一代大模型GLM-4.6，就在昨晚，DeepSeek也宣布推出DeepSeek-V3.2-Exp实验版模型。两家国产大模型领军企业在国庆假期毫不放松，加班加点推进技术迭代。

</details>


### [78] [<em class="highlight">大模型</em>找到主战场](http://mp.weixin.qq.com/s?__biz=MzkzMjYyMjk1MA==&mid=2247498096&idx=1&sn=294f6caf4b52194326b7a51385dd7d72&chksm=c369682f08933cf479cd2271be05102af7b94f2815f5d74af393dad7b0929918622e85dbcf76#rd)
*江淮观察*

Main category: wechat.article

TL;DR: 工业大模型是人机交互的接口，也是工业智能体进行任务规划与调度的核心，是系列垂类模型的集合。工业大模型以时序、语言、视觉等多模态大模型为主控，调用一系列基础模型、机理模型、专用模型、工业软件系统，用智能


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 工业大模型是人机交互的接口，也是工业智能体进行任务规划与调度的核心，是系列垂类模型的集合。工业大模型以时序、语言、视觉等多模态大模型为主控，调用一系列基础模型、机理模型、专用模型、工业软件系统，用智能

</details>


### [79] [一文让你详细了解AI<em class="highlight">大模型</em>与AI Agent的关系](http://mp.weixin.qq.com/s?__biz=MzA4NzY4MTQ1NQ==&mid=2247485593&idx=1&sn=f699569bdafe404cae763a38b6f96ffc&chksm=91e3f4524d1b5cb33d527cbad68790a9cf875d980ca37ceed3488a0338e919e41c3c4ce6dc80#rd)
*AIGCPM*

Main category: wechat.article

TL;DR: 可以说，大模型为Agent注入了“灵魂”和“智慧”，让Agent不再是简单的执行者，而是能够进行有深度思考和理解的智能体。Agent通过调用大模型，获得了前所未有的“智能”高度。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 可以说，大模型为Agent注入了“灵魂”和“智慧”，让Agent不再是简单的执行者，而是能够进行有深度思考和理解的智能体。Agent通过调用大模型，获得了前所未有的“智能”高度。

</details>


### [80] [一分钟带你了解<em class="highlight">大模型</em>的应用范式：Prompt、Agent、 RAG](http://mp.weixin.qq.com/s?__biz=Mzg4MDYzNjM5OQ==&mid=2247487585&idx=1&sn=b4c4f0d7d9b5b9acc4cb138cdd5b6a6d&chksm=ce522cbf34bcc0e23197aaad2af86fed0fb8522f18609a20e56a2598fafa6c936eaa2f179fc6#rd)
*AI大模型知识官*

Main category: wechat.article

TL;DR: 一分钟带你了解 大模型的应用范式。prompt agent， rag 大模型（llm，large language model）是基于大量数据进行 预训练的超大型深度学习模型。从2019年发展到现在，其能力 已经得到了极大的提升，其中以openal chatgpt的发布为关键 里程


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一分钟带你了解 大模型的应用范式。prompt agent， rag 大模型（llm，large language model）是基于大量数据进行 预训练的超大型深度学习模型。从2019年发展到现在，其能力 已经得到了极大的提升，其中以openal chatgpt的发布为关键 里程

</details>


### [81] [上交2025最新-《动手学<em class="highlight">大模型</em>》实战教程及ppt分享！](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570745&idx=2&sn=be5b8387914c32795c926c69a30c2668&chksm=9628448ffccadb90c6662050be201ce2910c2eb8e4529cf71d9e18f4db5f7612e8c725592a95#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 多模态大语言模型是否能够帮助实现agi？大模型智 能体与安 全 大模型智能体迈向了未来操作系统之旅。然而，大模型在开放智能体场景中能意识 到风险威胁吗？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 多模态大语言模型是否能够帮助实现agi？大模型智 能体与安 全 大模型智能体迈向了未来操作系统之旅。然而，大模型在开放智能体场景中能意识 到风险威胁吗？

</details>


### [82] [<em class="highlight">大模型</em>与Agent：现状、未来的几点思考](http://mp.weixin.qq.com/s?__biz=MzkwMzcxMTU1Ng==&mid=2247483978&idx=1&sn=1a06982a2008229ea5ba63c5be28e98e&chksm=c15641f2fe90da8dfbda8578279caec3bb70b76a5d55ff8e6df693ef67a37d68201ce4177a46#rd)
*叶子哥AI*

Main category: wechat.article

TL;DR: 这将进一步推广大模型的应用。画了一张图：大模型算力增长趋势图 （flops demand trend for llms/vlms） token调用量t（token） 算力总需求。f （flops） flops = t * c 模型算力消耗c （flops/token） 时间 （time）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这将进一步推广大模型的应用。画了一张图：大模型算力增长趋势图 （flops demand trend for llms/vlms） token调用量t（token） 算力总需求。f （flops） flops = t * c 模型算力消耗c （flops/token） 时间 （time）

</details>


### [83] [如何开发可靠的<em class="highlight">大模型</em>智能体？研究39个框架及439个应用后，发现了被99%开发者忽视的地方！](http://mp.weixin.qq.com/s?__biz=MzA3MDE0NzEzNA==&mid=2647615785&idx=1&sn=1b2c5415c164e4718f585f9748fcd1d4&chksm=87da17f0ae21c66fc7f2ffaf4062b1fa0c9f8d3bf79eb1e16d5d37ec604f4ef7da3e3df2b227#rd)
*走向未来*

Main category: wechat.article

TL;DR: 欢迎加入“走向未来”知识星球，一起探讨生成式人工智能、大模型和AIGC的产品、技术和应用实践，探讨如何使用各种不同的人工智能大模型和智能体来为工作增效，为生活添彩。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 欢迎加入“走向未来”知识星球，一起探讨生成式人工智能、大模型和AIGC的产品、技术和应用实践，探讨如何使用各种不同的人工智能大模型和智能体来为工作增效，为生活添彩。

</details>


### [84] [顶会风向标：从NeurIPS 2025精选论文，看懂<em class="highlight">大模型</em>的四大演进方向](http://mp.weixin.qq.com/s?__biz=MzE5ODIzNDkxMQ==&mid=2247485396&idx=1&sn=a19ccc260071cf8b80a86c0c124b9a2c&chksm=97c52132ce5ee62a20c10c521eb4b02c333ea11c28a4b05937c025c24795207635f271c4773b#rd)
*AI大模型说*

Main category: wechat.article

TL;DR: 大模型不仅持续推动自然语言处理等领域的革新，更广泛渗透至机器人学、多智能体系统、具身智能等前沿交叉领域，展现出强大的通用性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型不仅持续推动自然语言处理等领域的革新，更广泛渗透至机器人学、多智能体系统、具身智能等前沿交叉领域，展现出强大的通用性和扩展性。

</details>


### [85] [云栖大会炸场！国产<em class="highlight">大模型</em>干到全球前三，一场 AI 军备竞赛已打响](http://mp.weixin.qq.com/s?__biz=MzkwNjQxNTgyOA==&mid=2247484637&idx=1&sn=e01a4c7d9730c020f0f48d21022ffc12&chksm=c1dfcf4dc8c1e82dcbe5f3e8a1c76b2f02c5f6617d156cb2a8fbb9ccdaf362f214748c47cbf7#rd)
*恒星战纪*

Main category: wechat.article

TL;DR: 如果说大模型是 “大脑”，那 Agent 就是让大脑 “动起来” 的手脚。这次大会发布的全新 Agent 开发框架，算是给开发者送了份 “大礼”，而数据更能说明问题：过去一年，相关平台的模型日均调用量直接涨了 15 倍。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 如果说大模型是 “大脑”，那 Agent 就是让大脑 “动起来” 的手脚。这次大会发布的全新 Agent 开发框架，算是给开发者送了份 “大礼”，而数据更能说明问题：过去一年，相关平台的模型日均调用量直接涨了 15 倍。

</details>


### [86] [AI Infra：<em class="highlight">大模型</em>不思考，上下文替它思考](http://mp.weixin.qq.com/s?__biz=MzA3ODQyMzQ2Mg==&mid=2247485429&idx=1&sn=65492dd47bce017dbf9d75d621dee00d&chksm=9e31ff2fad86365a878c2521cf975583144f12a3229a8633eaeaced846d83dffbf29eca6ed8d#rd)
*平行记陆*

Main category: wechat.article

TL;DR: ●是大模型落地过程中，上下文工程在帮助 Agent “思考”，提供压缩后的精准信息2.3 知识有向大模型内化的愿望●长期看，大模型参数规模扩大，是知识内化的过程


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ●是大模型落地过程中，上下文工程在帮助 Agent “思考”，提供压缩后的精准信息2.3 知识有向大模型内化的愿望●长期看，大模型参数规模扩大，是知识内化的过程

</details>


### [87] [上交2025最新-《动手学<em class="highlight">大模型</em>》实战教程及ppt分享！](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570743&idx=2&sn=54f0898d781b640fde4188d0e7219f8d&chksm=96f98c46bf7fcc30f61eb933a137b0aca6971afa43112201a0399c93158fe4c7ff514075aeaf#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 多模态大语言模型是否能够帮助实现agi？大模型智能体与安全 大模型智能体迈向了未来操作系统之旅。然而，大模型在开放智能体场景中能意识到风险威胁吗？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 多模态大语言模型是否能够帮助实现agi？大模型智能体与安全 大模型智能体迈向了未来操作系统之旅。然而，大模型在开放智能体场景中能意识到风险威胁吗？

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [88] [Hilbert: Recursively Building Formal Proofs with Informal Reasoning](https://arxiv.org/abs/2509.22819)
*Sumanth Varambally,Thomas Voice,Yanchao Sun,Zhifeng Chen,Rose Yu,Ke Ye*

Main category: cs.AI

TL;DR: Hilbert是一个结合非正式推理和形式验证的智能代理框架，通过协调推理LLM、证明LLM、验证器和定理检索器，显著提升了数学问题的形式化证明能力。


<details>
  <summary>Details</summary>
Motivation: 现有证明LLM在形式化语言中解决的问题远少于通用LLM在自然语言中解决的问题，需要弥合非正式推理与形式验证之间的差距。

Method: 采用递归分解策略，将问题拆分为子目标，使用推理LLM或证明LLM解决，并利用验证器反馈修正错误证明。

Result: 在miniF2F上达到99.2%，比最佳公开方法高6.6个百分点；在PutnamBench上解决70.0%的问题，比SeedProver(50.4%)显著提升。

Conclusion: Hilbert有效缩小了非正式推理与形式化证明生成之间的差距。

Abstract: Large Language Models (LLMs) demonstrate impressive mathematical reasoning
abilities, but their solutions frequently contain errors that cannot be
automatically verified. Formal theorem proving systems such as Lean 4 offer
automated verification with complete accuracy, motivating recent efforts to
build specialized prover LLMs that generate verifiable proofs in formal
languages. However, a significant gap remains: current prover LLMs solve
substantially fewer problems than general-purpose LLMs operating in natural
language. We introduce Hilbert, an agentic framework that bridges this gap by
combining the complementary strengths of informal reasoning and formal
verification. Our system orchestrates four components: an informal LLM that
excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4
tactics, a formal verifier, and a semantic theorem retriever. Given a problem
that the prover is unable to solve, Hilbert employs recursive decomposition to
split the problem into subgoals that it solves with the prover or reasoner LLM.
It leverages verifier feedback to refine incorrect proofs as necessary.
Experimental results demonstrate that Hilbert substantially outperforms
existing approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points
above the best publicly available method. Hilbert achieves the best known
result on PutnamBench. It solves 462/660 problems (70.0%), outperforming
proprietary approaches like SeedProver (50.4%) and achieving a 422% improvement
over the best publicly available baseline. Thus, Hilbert effectively narrows
the gap between informal reasoning and formal proof generation.

</details>


### [89] [JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory](https://arxiv.org/abs/2509.22888)
*Louie Hong Yao,Nicholas Jarvis,Tiffany Zhan,Saptarshi Ghosh,Linfeng Liu,Tianyu Jiang*

Main category: cs.AI

TL;DR: JE-IRT是一个几何项目响应框架，将LLM和问题嵌入共享空间，用几何交互代替全局排名，揭示模型的专业化能力和问题结构关系。


<details>
  <summary>Details</summary>
Motivation: 传统LLM评估方法将多维能力压缩为单一分数，掩盖了其本质上的多维度特性，需要更细粒度的评估框架。

Method: 提出JE-IRT框架，将LLM和问题嵌入共享几何空间，问题嵌入的方向编码语义、范数编码难度，正确性由模型与问题的几何交互决定。

Result: 实验显示：分布外行为可通过方向对齐解释；较大范数对应更难问题；新LLM只需拟合单个嵌入即可加入；发现与人类定义类别部分对齐的LLM内部分类法。

Conclusion: JE-IRT建立了统一可解释的几何视角，连接LLM能力与问题结构，为模型评估和泛化提供了独特视角。

Abstract: Standard LLM evaluation practices compress diverse abilities into single
scores, obscuring their inherently multidimensional nature. We present JE-IRT,
a geometric item-response framework that embeds both LLMs and questions in a
shared space. For question embeddings, the direction encodes semantics and the
norm encodes difficulty, while correctness on each question is determined by
the geometric interaction between the model and question embeddings. This
geometry replaces a global ranking of LLMs with topical specialization and
enables smooth variation across related questions. Building on this framework,
our experimental results reveal that out-of-distribution behavior can be
explained through directional alignment, and that larger norms consistently
indicate harder questions. Moreover, JE-IRT naturally supports generalization:
once the space is learned, new LLMs are added by fitting a single embedding.
The learned space further reveals an LLM-internal taxonomy that only partially
aligns with human-defined subject categories. JE-IRT thus establishes a unified
and interpretable geometric lens that connects LLM abilities with the structure
of questions, offering a distinctive perspective on model evaluation and
generalization.

</details>


### [90] [Not only a helper, but also a teacher: Interactive LLM Cascade](https://arxiv.org/abs/2509.22984)
*Yu Wu,Shuo Wu,Ye Tao,Yansong Li,Anand D. Sarwate*

Main category: cs.AI

TL;DR: Inter-Cascade是一种在线交互式LLM级联系统，将强大模型从备用助手扩展为长期教师，通过知识蒸馏提升弱模型性能，减少对昂贵模型的调用。


<details>
  <summary>Details</summary>
Motivation: 传统LLM级联方法是非自适应的，遇到相似或重复查询时会重复调用昂贵模型，导致成本增加。需要一种在线交互式方法来提高级联效率。

Method: 当强模型解决困难查询时，将其解决方案蒸馏为可重用的通用问题解决策略，这些策略随后用于增强弱模型处理后续查询的能力。

Result: 相比标准LLM级联基线，Inter-Cascade显著提升弱模型准确率（最高33.06个百分点）和整体系统性能（最高5.53个百分点），同时减少强模型调用（相对减少48.05%）和相应费用（相对减少49.63%）。

Conclusion: Inter-Cascade展示了LLM之间有效的上下文知识传递，提供了一个适用于开源和API-based LLM的通用、可扩展框架。

Abstract: Large Language Models (LLMs) vary widely in their capabilities, with larger
models often having better performance but higher cost: choosing an LLM model
often involves trading off performance and cost. The LLM Cascade is a paradigm
that defers difficult queries from weak/cheap to strong/expensive models. This
approach is nonadaptive: the deferral decision is trained offline. When
confronted with similar or repeated queries, the LLM Cascade may then
repeatedly consult the expensive model and incur higher cost. To improve the
cascading efficiency, we propose Inter-Cascade, an online and interactive LLM
Cascade that extends the role of strong model from a backup helper to a
long-term teacher. In our system, when a strong model resolves a difficult
query, it also distills its solution into a generalized, reusable
problem-solving strategy that boosts the weak model on subsequent queries.
Adding strategies to queries enables the weak model to dynamically improve its
performance over time, avoiding computationally and time-intensive fine-tuning.
Empirically, compared with standard LLM Cascade baselines across multiple
benchmarks, the Inter-Cascade significantly improves the accuracy of the weak
model (by up to 33.06 absolute percentage points) and the overall system (by up
to 5.53 absolute percentage points), while reducing the calls to strong models
(by up to 48.05% relative reduction) and saving the corresponding fees (by up
to 49.63% relative reduction). Inter-Cascade demonstrates the effective
in-context knowledge transfer between LLMs, and provides a general, scalable
framework applicable to both open-source and API-based LLMs.

</details>


### [91] [Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems](https://arxiv.org/abs/2509.23006)
*Hassen Dhrif*

Main category: cs.AI

TL;DR: 提出了创意对抗测试(CAT)框架，用于评估Agentic AI系统中任务与目标的对齐关系，通过合成数据验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前Agentic AI评估主要关注代理、工具和参数的选择有效性，但缺乏对任务与目标对齐关系的评估，存在关键空白。

Method: 开发了创意对抗测试(CAT)框架，使用基于Alexa+音频服务的合成交互数据进行广泛模拟测试。

Result: CAT框架能够提供前所未有的目标-任务对齐洞察，使Agentic AI系统的优化和开发更加有效。

Conclusion: CAT框架填补了Agentic AI评估的关键空白，为系统优化和开发提供了重要工具。

Abstract: Agentic AI represents a paradigm shift in enhancing the capabilities of
generative AI models. While these systems demonstrate immense potential and
power, current evaluation techniques primarily focus on assessing their
efficacy in identifying appropriate agents, tools, and parameters. However, a
critical gap exists in evaluating the alignment between an Agentic AI system's
tasks and its overarching goals. This paper introduces the Creative Adversarial
Testing (CAT) framework, a novel approach designed to capture and analyze the
complex relationship between Agentic AI tasks and the system's intended
objectives.
  We validate the CAT framework through extensive simulation using synthetic
interaction data modeled after Alexa+ audio services, a sophisticated Agentic
AI system that shapes the user experience for millions of users globally. This
synthetic data approach enables comprehensive testing of edge cases and failure
modes while protecting user privacy. Our results demonstrate that the CAT
framework provides unprecedented insights into goal-task alignment, enabling
more effective optimization and development of Agentic AI systems.

</details>


### [92] [Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents](https://arxiv.org/abs/2509.23045)
*Zonghan Yang,Shengjie Wang,Kelin Fu,Wenyang He,Weimin Xiong,Yibo Liu,Yibo Miao,Bofei Gao,Yejie Wang,Yingwei Ma,Yanhao Li,Yue Liu,Zhenxing Hu,Kaitai Zhang,Shuyi Wang,Huarong Chen,Flood Sung,Yang Liu,Yang Gao,Zhilin Yang,Tianyu Liu*

Main category: cs.AI

TL;DR: 本文提出通过无代理训练获得结构化技能先验，能够桥接工作流和代理框架，实现可迁移的编码代理。Kimi-Dev在SWE-bench Verified上达到60.4%，通过额外SFT适应后，SWE代理达到48.6% pass@1，与Claude 3.5 Sonnet相当。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程中的LLM应用存在SWE-Agent框架（多轮交互）和工作流式Agentless方法（单轮可验证步骤）两种范式，作者认为这两种范式并非互斥，而是可以相互补充。

Method: 首先策划了Agentless训练方法，开发了开源SWE LLM Kimi-Dev，然后通过额外的SFT适应在5000个公开轨迹上进行训练。

Result: Kimi-Dev在SWE-bench Verified上达到60.4%，是工作流方法中最佳表现。经过SFT适应后，SWE代理达到48.6% pass@1，与Claude 3.5 Sonnet（241022版本）相当。

Conclusion: 从Agentless训练中获得的结构化技能先验（包括定位、代码编辑和自我反思）能够桥接工作流和代理框架，实现高效有效的SWE代理适应。

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent
frameworks with multi-turn interactions and workflow-based Agentless methods
with single-turn verifiable steps. We argue these paradigms are not mutually
exclusive: reasoning-intensive Agentless training induces skill priors,
including localization, code edit, and self-reflection that enable efficient
and effective SWE-Agent adaptation. In this work, we first curate the Agentless
training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\%
on SWE-bench Verified, the best among workflow approaches. With additional SFT
adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to
48.6\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These
results show that structured skill priors from Agentless training can bridge
workflow and agentic frameworks for transferable coding agents.

</details>


### [93] [Risk Profiling and Modulation for LLMs](https://arxiv.org/abs/2509.23058)
*Yikai Wang,Xiaocheng Li,Guanting Chen*

Main category: cs.AI

TL;DR: 该论文研究了LLM在不确定性决策中的风险偏好，提出了一个评估、引导和调节LLM风险配置文件的管道，发现指令微调模型与标准效用模型一致，而预训练和RLHF对齐模型偏离更多，后训练是最有效的风险偏好调节方法。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在不确定性决策中的风险配置，以及提示和训练方法如何影响其风险行为，现有研究主要关注个性提示或多智能体交互，缺乏对后训练如何影响LLM风险行为的理解。

Method: 提出新的管道来引出、引导和调节LLM的风险配置文件，使用行为经济学和金融学的工具，基于效用理论模型比较预训练、指令微调和RLHF对齐的LLM，评估提示工程、上下文学习和后训练等调节策略。

Result: 指令微调模型表现出与标准效用模型一致的行为，而预训练和RLHF对齐模型偏离更多拟合的效用模型，后训练提供了最稳定和有效的风险偏好调节。

Conclusion: 研究揭示了不同类别和阶段LLM的风险配置文件，展示了后训练如何调节这些配置文件，为行为对齐和风险感知LLM设计的未来研究奠定了基础。

Abstract: Large language models (LLMs) are increasingly used for decision-making tasks
under uncertainty; however, their risk profiles and how they are influenced by
prompting and alignment methods remain underexplored. Existing studies have
primarily examined personality prompting or multi-agent interactions, leaving
open the question of how post-training influences the risk behavior of LLMs. In
this work, we propose a new pipeline for eliciting, steering, and modulating
LLMs' risk profiles, drawing on tools from behavioral economics and finance.
Using utility-theoretic models, we compare pre-trained, instruction-tuned, and
RLHF-aligned LLMs, and find that while instruction-tuned models exhibit
behaviors consistent with some standard utility formulations, pre-trained and
RLHF-aligned models deviate more from any utility models fitted. We further
evaluate modulation strategies, including prompt engineering, in-context
learning, and post-training, and show that post-training provides the most
stable and effective modulation of risk preference. Our findings provide
insights into the risk profiles of different classes and stages of LLMs and
demonstrate how post-training modulates these profiles, laying the groundwork
for future research on behavioral alignment and risk-aware LLM design.

</details>


### [94] [Multiplayer Nash Preference Optimization](https://arxiv.org/abs/2509.23102)
*Fang Wu,Xu Huang,Weihao Xuan,Zhiwei Zhang,Yijia Xiao,Guancheng Wan,Xiaomin Li,Bing Hu,Peng Xia,Jure Leskovec,Yejin Choi*

Main category: cs.AI

TL;DR: 提出了Multiplayer Nash Preference Optimization (MNPO)，将Nash学习从两人博弈扩展到多人博弈，解决了现有方法只能处理两人交互的局限性，能够更好地捕捉复杂的人类偏好结构。


<details>
  <summary>Details</summary>
Motivation: 现有的基于奖励的方法和两人Nash学习框架无法充分捕捉现实世界中偏好的非传递性和异质性特征，存在单一对手偏差的问题。

Method: 将对齐问题建模为n人博弈，每个策略与一组对手竞争，同时向参考模型正则化。建立了多人设置下的Nash均衡，并扩展了对偶间隙概念来量化近似质量。

Result: 在指令跟随基准测试中，MNPO持续优于现有的NLHF基线方法，在异质注释者条件和混合策略评估场景下实现了更好的对齐质量。

Conclusion: MNPO为对齐LLM与复杂、非传递性的人类偏好提供了一个原则性和可扩展的框架。

Abstract: Reinforcement learning from human feedback (RLHF) has emerged as the standard
paradigm for aligning large language models (LLMs) with human preferences.
However, reward-based methods built on the Bradley-Terry assumption struggle to
capture the non-transitive and heterogeneous nature of real-world preferences.
To address this, recent studies have reframed alignment as a two-player Nash
game, giving rise to Nash learning from human feedback (NLHF). While this
perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong
theoretical and empirical guarantees, they remain fundamentally restricted to
two-player interactions, creating a single-opponent bias that fails to capture
the full complexity of realistic preference structures. In this work, we
introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework
that generalizes NLHF to the multiplayer regime. It formulates alignment as an
$n$-player game, where each policy competes against a population of opponents
while being regularized toward a reference model. Our framework establishes
well-defined Nash equilibria in multiplayer settings and extends the concept of
duality gap to quantify approximation quality. We demonstrate that MNPO
inherits the equilibrium guarantees of two-player methods while enabling richer
competitive dynamics and improved coverage of diverse preference structures.
Through comprehensive empirical evaluation, we show that MNPO consistently
outperforms existing NLHF baselines on instruction-following benchmarks,
achieving superior alignment quality under heterogeneous annotator conditions
and mixed-policy evaluation scenarios. Together, these results establish MNPO
as a principled and scalable framework for aligning LLMs with complex,
non-transitive human preferences. Code is available at
https://github.com/smiles724/MNPO.

</details>


### [95] [MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning](https://arxiv.org/abs/2509.23143)
*Charles L. Wang*

Main category: cs.AI

TL;DR: MathBode是一种用于评估大语言模型数学推理能力的动态诊断方法，通过正弦驱动参数变化并分析模型输出的频率响应，提供增益和相位等可解释指标。


<details>
  <summary>Details</summary>
Motivation: 传统的一次性准确率评估无法揭示LLMs在数学推理中的系统性问题，需要更精细的动态分析方法来评估推理的保真度和一致性。

Method: 将参数化问题视为系统，对单个参数进行正弦驱动，拟合模型输出和精确解的一次谐波响应，获得增益（幅度跟踪）和相位（滞后）等频率分辨指标。

Result: 在五个闭式问题家族中，该方法揭示了系统性的低通行为和增长的相位滞后，这些是准确性指标无法发现的。前沿模型与中端模型在动态特性上表现出明显差异。

Conclusion: MathBode提供了一种紧凑、可复现的协议，补充了标准基准测试，能够测量推理的保真度和一致性。

Abstract: This paper presents MathBode, a dynamic diagnostic for mathematical reasoning
in large language models (LLMs). Instead of one-shot accuracy, MathBode treats
each parametric problem as a system: we drive a single parameter sinusoidally
and fit first-harmonic responses of model outputs and exact solutions. This
yields interpretable, frequency-resolved metrics -- gain (amplitude tracking)
and phase (lag) -- that form Bode-style fingerprints. Across five closed-form
families (linear solve, ratio/saturation, compound interest, 2x2 linear
systems, similar triangles), the diagnostic surfaces systematic low-pass
behavior and growing phase lag that accuracy alone obscures. We compare several
models against a symbolic baseline that calibrates the instrument ($G \approx
1$, $\phi \approx 0$). Results separate frontier from mid-tier models on
dynamics, providing a compact, reproducible protocol that complements standard
benchmarks with actionable measurements of reasoning fidelity and consistency.
We open-source the dataset and code to enable further research and adoption.

</details>


### [96] [Agentic AI Reasoning for Mobile Edge General Intelligence: Fundamentals, Approaches, and Directions](https://arxiv.org/abs/2509.23248)
*Mingyi Luo,Ruichen Zhang,Xiangwang Hou,Jun Du,Chunxiao Jiang,Yong Ren,Dusit Niyato,Shiwen Mao*

Main category: cs.AI

TL;DR: 提出了一个联合优化框架，用于在移动边缘通用智能(MEGI)环境中高效部署基于LLM的智能体AI推理，通过自适应思维链提示和分布式专家混合架构来平衡推理质量与资源效率。


<details>
  <summary>Details</summary>
Motivation: 在MEGI环境中部署基于LLM的智能体AI推理面临计算需求高与边缘设备资源有限的挑战，需要解决推理效率与资源约束之间的平衡问题。

Method: 提出了联合优化框架，包括：1) 通过自适应CoT提示增强推理能力；2) 通过分布式MoE架构实现可扩展部署；3) 根据任务复杂度和设备能力动态激活专家网络和调整推理深度。

Result: 实验评估表明该框架在移动边缘环境中能有效平衡推理质量与资源效率，验证了在资源受限的MEGI环境中部署复杂LLM推理能力的实际可行性。

Conclusion: 该联合优化框架为在资源受限的边缘环境中部署高效的LLM推理提供了可行解决方案，实现了推理能力与资源效率的良好平衡。

Abstract: The rapid advancement of large language models (LLMs) has enabled an
emergence of agentic artificial intelligence (AI) with powerful reasoning and
autonomous decision-making capabilities. This integration with edge computing
has led to the development of Mobile Edge General Intelligence (MEGI), which
brings real-time, privacy-preserving reasoning to the network edge. However,
deploying LLM-based agentic AI reasoning in MEGI environments poses significant
challenges due to the high computational demands of reasoning and the limited
resources of edge devices. To address these challenges, we propose a joint
optimization framework for efficient LLM reasoning deployment in MEGI. First,
we review methods that enhance LLM reasoning capabilities, such as
Chain-of-Thought (CoT) prompting, Supervised Fine-Tuning (SFT), and Mixture of
Experts (MoE). Next, we present a distributed framework that addresses two
correlated aspects: reasoning enhancement through adaptive CoT prompting and
scalable deployment through distributed MoE architecture. The framework
dynamically activates expert networks and adjusts reasoning depth based on task
complexity and device capabilities. We further conduct experimental evaluations
in mobile edge environments. Experimental results demonstrate the framework's
effectiveness in balancing reasoning quality with resource efficiency,
validating the practical viability of deploying sophisticated LLM reasoning
capabilities in resource-constrained MEGI environments.

</details>


### [97] [GUI-PRA: Process Reward Agent for GUI Tasks](https://arxiv.org/abs/2509.23263)
*Tao Xiong,Xavier Hu,Yurun Chen,Yuhang Liu,Changqiao Wu,Pengzhi Gao,Wei Liu,Jian Luan,Shengyu Zhang*

Main category: cs.AI

TL;DR: 提出了GUI-PRA，一种针对GUI任务的流程奖励代理，通过动态记忆机制和自适应UI感知来解决标准流程奖励模型在GUI领域中的'中间迷失'问题和缺乏UI变化感知的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI代理在处理长视野任务时经常失败，标准流程奖励模型在处理密集人工输入和长历史数据时存在'中间迷失'现象，且缺乏对GUI动态变化的感知能力。

Method: 引入GUI-PRA，包含动态记忆机制（相关性检索模块和渐进总结模块）和自适应UI感知机制，能够智能处理历史上下文并主动感知UI状态变化。

Result: GUI-PRA能够更好地提供流程奖励，通过聚焦相关上下文和基于当前UI环境的评估来指导GUI代理。

Conclusion: GUI-PRA通过解决历史上下文处理和UI变化感知的关键挑战，显著提升了GUI代理在长视野任务中的表现。

Abstract: Graphical User Interface (GUI) Agents powered by Multimodal Large Language
Models (MLLMs) show significant potential for automating tasks. However, they
often struggle with long-horizon tasks, leading to frequent failures. Process
Reward Models (PRMs) are a promising solution, as they can guide these agents
with crucial process signals during inference. Nevertheless, their application
to the GUI domain presents unique challenges. When processing dense artificial
inputs with long history data, PRMs suffer from a "lost in the middle"
phenomenon, where the overwhelming historical context compromises the
evaluation of the current step. Furthermore, standard PRMs lacks GUI changing
awareness, providing static evaluations that are disconnected from the dynamic
consequences of actions, a critical mismatch with the inherently dynamic nature
of GUI tasks. In response to these challenges, we introduce GUI-PRA (Process
Reward Agent for GUI Tasks), a judge agent designed to better provide process
reward than standard PRM by intelligently processing historical context and
actively perceiving UI state changes. Specifically, to directly combat the
``lost in the middle'' phenomenon, we introduce a dynamic memory mechanism
consisting of two core components: a Relevance-based Retrieval Module to
actively fetch pertinent information from long histories and a Progressive
Summarization Module to dynamically condense growing interaction data, ensuring
the model focuses on relevant context. Moreover, to address the lack of UI
changing awareness, we introduce an Aadaptive UI Perception mechanism. This
mechanism enables the agent to reason about UI state changes and dynamically
select the most appropriate tool to gather grounded visual evidence, ensuring
its evaluation is always informed by the current UI context.

</details>


### [98] [Socio-Economic Model of AI Agents](https://arxiv.org/abs/2509.23270)
*Yuxinyue Qian,Jun Liu*

Main category: cs.AI

TL;DR: 构建包含人类工人和自主AI代理的异构多代理模型，研究AI协作在资源约束下对社会总产出的影响，发现AI代理能显著提升产出，网络效应带来非线性增长。


<details>
  <summary>Details</summary>
Motivation: 研究AI技术深度融入社会经济系统时，AI与人类协作在资源约束下对社会总产出的影响机制。

Method: 构建五个逐步扩展的异构代理模型：纯人类协作基线、AI协作、网络效应、独立生产者、网络效应+独立生产，通过理论推导和仿真分析。

Result: AI代理引入显著增加社会总产出；考虑网络效应时增长呈非线性；独立生产者模式提供更高长期增长潜力；网络效应展现规模报酬递增特性。

Conclusion: AI协作能有效提升社会经济产出，网络效应和独立生产模式是关键的增效机制，为AI融入社会经济系统提供了理论支持。

Abstract: Modern socio-economic systems are undergoing deep integration with artificial
intelligence technologies. This paper constructs a heterogeneous agent-based
modeling framework that incorporates both human workers and autonomous AI
agents, to study the impact of AI collaboration under resource constraints on
aggregate social output. We build five progressively extended models: Model 1
serves as the baseline of pure human collaboration; Model 2 introduces AI as
collaborators; Model 3 incorporates network effects among agents; Model 4
treats agents as independent producers; and Model 5 integrates both network
effects and independent agent production. Through theoretical derivation and
simulation analysis, we find that the introduction of AI agents can
significantly increase aggregate social output. When considering network
effects among agents, this increase exhibits nonlinear growth far exceeding the
simple sum of individual contributions. Under the same resource inputs,
treating agents as independent producers provides higher long-term growth
potential; introducing network effects further demonstrates strong
characteristics of increasing returns to scale.

</details>


### [99] [Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking](https://arxiv.org/abs/2509.23392)
*Jinyi Han,Ying Huang,Ying Liao,Zishang Jiang,Xikun Lu,Haiquan Zhao,Xinyi Wang,Guanghao Zhou,Sihang Jiang,Jiaqing Liang,Weikang Zhou,Zeye Sun,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: 提出JET方法，通过主动终止不必要的推理步骤来提高大型推理模型的效率，在保持准确性的同时显著减少推理路径长度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务上表现出色，但深度推理会带来巨大的计算成本。现有强化学习方法在构建短推理路径方面仍有困难，限制了有效学习。

Method: 基于证据积累模型的洞察，提出JET方法：在推理过程中主动终止不必要的推理步骤，通过轨迹截断和基于质量控制的长度奖励来训练模型。

Result: JET显著提高了推理效率而不牺牲准确性。在Olympiad基准测试中，DeepSeek-Distill-Qwen-1.5B实现了4.6%的准确率提升，同时输出长度减少了46.3%。

Conclusion: JET方法通过主动终止冗余推理步骤，有效提高了大型推理模型的效率，在保持准确性的同时大幅减少计算成本。

Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on
challenging tasks, yet their deep reasoning often incurs substantial
computational costs. To achieve efficient reasoning, existing reinforcement
learning methods still struggle to construct short reasoning path during the
rollout stage, limiting effective learning. Inspired by Evidence Accumulation
Models, we find that LRMs have accumulated sufficient information early in
reasoning, making further reasoning steps redundant. Based on this insight, we
propose Just-Enough Thinking (JET), which trains models to proactively
terminate unnecessary reasoning. JET performs trajectory truncation during
rollout to expose the model to short, distributionally consistent reasoning
paths. Besides, it uses a quality-controlled length reward to better encourage
concise reasoning while maintaining correctness. Extensive experiments
demonstrate that JET significantly improves reasoning efficiency without
sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6%
accuracy gain while reducing output length by 46.3% on the Olympiad benchmark.
Our code is available in the GitHub.

</details>


### [100] [From Conversation to Query Execution: Benchmarking User and Tool Interactions for EHR Database Agents](https://arxiv.org/abs/2509.23415)
*Gyubok Lee,Woosog Chay,Heeyoung Kwak,Yeong Hwa Kim,Haanju Yoo,Oksoon Jeong,Meong Hi Son,Edward Choi*

Main category: cs.AI

TL;DR: 提出了EHR-ChatQA基准测试，用于评估LLM代理在电子健康记录数据访问中的端到端工作流程，包括问题澄清、工具使用和SQL生成。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够充分捕捉真实世界临床数据访问流程的基准测试，阻碍了LLM代理在电子健康记录领域的应用。

Method: 创建交互式数据库问答基准EHR-ChatQA，在模拟环境中使用基于LLM的用户评估代理性能，涵盖两种交互流程：增量查询精炼和自适应查询精炼。

Result: 实验显示代理在IncreQA上Pass@5达到90-95%，AdaptQA上达到60-80%，但Pass^5（五次试验均成功）显著降低35-60%。

Conclusion: 需要构建不仅性能好而且鲁棒的代理，以满足电子健康记录领域的安全关键需求。

Abstract: Despite the impressive performance of LLM-powered agents, their adoption for
Electronic Health Record (EHR) data access remains limited by the absence of
benchmarks that adequately capture real-world clinical data access flows. In
practice, two core challenges hinder deployment: query ambiguity from vague
user questions and value mismatch between user terminology and database
entries. To address this, we introduce EHR-ChatQA an interactive database
question answering benchmark that evaluates the end-to-end workflow of database
agents: clarifying user questions, using tools to resolve value mismatches, and
generating correct SQL to deliver accurate answers. To cover diverse patterns
of query ambiguity and value mismatch, EHR-ChatQA assesses agents in a
simulated environment with an LLM-based user across two interaction flows:
Incremental Query Refinement (IncreQA), where users add constraints to existing
queries, and Adaptive Query Refinement (AdaptQA), where users adjust their
search goals mid-conversation. Experiments with state-of-the-art LLMs (e.g.,
o4-mini and Gemini-2.5-Flash) over five i.i.d. trials show that while agents
achieve high Pass@5 of 90-95% (at least one of five trials) on IncreQA and
60-80% on AdaptQA, their Pass^5 (consistent success across all five trials) is
substantially lower by 35-60%. These results underscore the need to build
agents that are not only performant but also robust for the safety-critical EHR
domain. Finally, we provide diagnostic insights into common failure modes to
guide future agent development.

</details>


### [101] [Democratizing AI scientists using ToolUniverse](https://arxiv.org/abs/2509.23426)
*Shanghua Gao,Richard Zhu,Pengwei Sui,Zhenglun Kong,Sufian Aldogom,Yepeng Huang,Ayush Noori,Reza Shamji,Krishna Parvataneni,Theodoros Tsiligkaridis,Marinka Zitnik*

Main category: cs.AI

TL;DR: ToolUniverse是一个用于构建AI科学家的生态系统，支持多种语言和推理模型，集成了600多个机器学习模型、数据集、API和科学包，能够自动优化工具接口并组合成智能工作流。


<details>
  <summary>Details</summary>
Motivation: 现有的AI科学家系统难以构建，因为它们通常是定制化的、绑定在固定工作流程中，并且缺乏统一工具、数据和分析的共享环境。

Method: ToolUniverse标准化了AI科学家识别和调用工具的方式，自动优化工具接口，从自然语言描述创建新工具，迭代优化工具规范，并将工具组合成智能工作流。

Result: 在高胆固醇血症案例研究中，ToolUniverse成功创建了一个AI科学家，识别出具有良好预测特性的药物类似物。

Conclusion: ToolUniverse为构建AI科学家提供了必要的基础设施，类似于组学领域统一生态系统对研究的变革性影响。

Abstract: AI scientists are emerging computational systems that serve as collaborative
partners in discovery. These systems remain difficult to build because they are
bespoke, tied to rigid workflows, and lack shared environments that unify
tools, data, and analyses into a common ecosystem. In omics, unified ecosystems
have transformed research by enabling interoperability, reuse, and
community-driven development; AI scientists require comparable infrastructure.
We present ToolUniverse, an ecosystem for building AI scientists from any
language or reasoning model, whether open or closed. TOOLUNIVERSE standardizes
how AI scientists identify and call tools, integrating more than 600 machine
learning models, datasets, APIs, and scientific packages for data analysis,
knowledge retrieval, and experimental design. It automatically refines tool
interfaces for correct use by AI scientists, creates new tools from natural
language descriptions, iteratively optimizes tool specifications, and composes
tools into agentic workflows. In a case study of hypercholesterolemia,
ToolUniverse was used to create an AI scientist to identify a potent analog of
a drug with favorable predicted properties. The open-source ToolUniverse is
available at https://aiscientist.tools.

</details>


### [102] [Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity](https://arxiv.org/abs/2509.23449)
*Charles E. Gagnon,Steven H. H. Ding,Philippe Charland,Benjamin C. M. Fung*

Main category: cs.AI

TL;DR: 提出了一种基于语言模型代理的方法，用于二进制代码相似性检测，通过结构化推理分析汇编代码生成可解释的特征，解决了现有方法在可解释性、泛化性和可扩展性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代二进制代码相似性检测方法面临可解释性、泛化性和可扩展性之间的权衡。手工特征可解释但泛化能力差，嵌入方法泛化能力强但不可解释且面临可扩展性-准确性权衡。

Method: 使用语言模型代理对汇编代码进行结构化推理分析，生成输入/输出类型、副作用、显著常数和算法意图等人类可读的特征，可直接通过倒排或关系索引进行搜索。

Result: 在跨架构和跨优化任务中分别达到42%和62%的recall@1，与需要训练的嵌入方法相当（39%和34%）。与嵌入方法结合后显著优于现有最优方法。

Conclusion: 该方法证明准确性、可扩展性和可解释性可以共存，为二进制代码相似性检测提供了新的解决方案。

Abstract: Binary code similarity detection is a core task in reverse engineering. It
supports malware analysis and vulnerability discovery by identifying
semantically similar code in different contexts. Modern methods have progressed
from manually engineered features to vector representations. Hand-crafted
statistics (e.g., operation ratios) are interpretable, but shallow and fail to
generalize. Embedding-based methods overcome this by learning robust
cross-setting representations, but these representations are opaque vectors
that prevent rapid verification. They also face a scalability-accuracy
trade-off, since high-dimensional nearest-neighbor search requires
approximations that reduce precision. Current approaches thus force a
compromise between interpretability, generalizability, and scalability.
  We bridge these gaps using a language model-based agent to conduct structured
reasoning analysis of assembly code and generate features such as input/output
types, side effects, notable constants, and algorithmic intent. Unlike
hand-crafted features, they are richer and adaptive. Unlike embeddings, they
are human-readable, maintainable, and directly searchable with inverted or
relational indexes. Without any matching training, our method respectively
achieves 42% and 62% for recall@1 in cross-architecture and cross-optimization
tasks, comparable to embedding methods with training (39% and 34%). Combined
with embeddings, it significantly outperforms the state-of-the-art,
demonstrating that accuracy, scalability, and interpretability can coexist.

</details>


### [103] [Mapping Overlaps in Benchmarks through Perplexity in the Wild](https://arxiv.org/abs/2509.23488)
*Siyang Wu,Honglin Bao,Sida Li,Ari Holtzman,James A. Evans*

Main category: cs.AI

TL;DR: 开发了基准签名方法来表征LLM基准测试及其重叠性，通过分析模型在自然语料上的困惑度来预测基准表现，发现编码领域与其他能力重叠最少。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试存在性能与能力混淆的问题，需要更有效的方法来理解基准测试之间的真实重叠和模型能力的内在联系。

Method: 通过逐步前向选择和线性回归，在32个LLMs和88个基准测试中提取基准签名，分析模型困惑度与基准表现的关系。

Result: 发现知识推理任务重叠度高，多语言文化基准相似性低，编码领域与其他能力重叠最少，基准签名对问题格式等无关因素具有鲁棒性。

Conclusion: 基准签名方法能有效揭示LLM能力的底层景观，为基准有效性提供机制性洞察，并识别跨功能重叠模式。

Abstract: We develop signatures of capacity familiarity to characterize large language
model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures
probe the capacity required for benchmark performance. We formally define them
as a set of salient tokens drawn from in-the-wild, naturally authored corpora,
where LLM token perplexity, reflecting more or less pre-training exposure,
becomes highly predictive of LLM benchmark performance. Through a large-scale
meta-evaluation, we extract benchmark signatures via stepwise forward selection
with linear regressions across 32 LLMs and 88 benchmarks spanning diverse
knowledge, coding, logic, instruction following, math, language, reasoning, and
world modeling. Our analysis situates signatures in relation to both the
semantic similarity of benchmark questions and the correlation of model
performance. While performance overlaps are universally high and semantic
overlaps remain confined to a narrow mid-range, benchmark signatures prove
highly informative in capturing variation, overlap, and divergence. We observe
overlap in knowledge and reasoning subtasks, whereas multilingual and cultural
benchmarks exhibit less similarity, even compared to cross-task overlap.
Notably, performance-level results are strongly influenced by
benchmark-orthogonal factors such as question format, highlighting limitations
in LLM generalization, the conflation of performance with ability, and issues
inherent in current mainstream benchmark agreement studies. Benchmark
signatures, however, remain robust to such effects. Ultimately, we identify
cross-functional overlaps across logic, math, language, instruction following,
and world modeling, with coding emerging as the least overlapping domain.
Together, these findings provide mechanistic insights into benchmark validity
and LLM sensitivities, and sketch the underlying landscape of interconnected
LLM capabilities.

</details>


### [104] [Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems: Dataset, Taxonomy, and Benchmark](https://arxiv.org/abs/2509.23735)
*Xuyan Ma,Xiaofei Xie,Yawen Wang,Junjie Wang,Boyu Wu,Mingyang Li,Qing Wang*

Main category: cs.AI

TL;DR: 本文研究了平台编排智能体系统的故障根因识别，构建了包含307个故障日志的数据集AgentFail，开发了故障根因分类法，并建立了基于LLM的自动根因识别基准测试。


<details>
  <summary>Details</summary>
Motivation: 随着低代码平台编排的智能体系统被广泛部署，这些系统存在脆弱性，但缺乏系统性的故障根因识别方法。

Method: 构建AgentFail数据集，采用反事实推理修复策略确保标注可靠性，开发故障根因分类法，并建立基于LLM的根因识别基准测试。

Result: 分类法能显著提升性能，但根因识别准确率最高仅33.6%，表明该任务仍具挑战性。

Conclusion: 为平台编排智能体系统提供了可靠的故障根因数据集、分类法和基准测试，为开发更可靠的智能体系统奠定了基础。

Abstract: Agentic systems consisting of multiple LLM-driven agents coordinating through
tools and structured interactions, are increasingly deployed for complex
reasoning and problem-solving tasks. At the same time, emerging low-code and
template-based agent development platforms (e.g., Dify) enable users to rapidly
build and orchestrate agentic systems, which we refer to as
platform-orchestrated agentic systems. However, these systems are also fragile
and it remains unclear how to systematically identify their potential failure
root cause. This paper presents a study of root cause identification of these
platform-orchestrated agentic systems. To support this initiative, we construct
a dataset AgentFail containing 307 failure logs from ten agentic systems, each
with fine-grained annotations linking failures to their root causes. We
additionally utilize counterfactual reasoning-based repair strategy to ensure
the reliability of the annotation. Building on the dataset, we develop a
taxonomy that characterizes failure root causes and analyze their distribution
across different platforms and task domains. Furthermore, we introduce a
benchmark that leverages LLMs for automatically identifying root causes, in
which we also utilize the proposed taxonomy as guidance for LLMs. Results show
that the taxonomy can largely improve the performance, thereby confirming its
utility. Nevertheless, the accuracy of root cause identification reaches at
most 33.6%, which indicates that this task still remains challenging. In light
of these results, we also provide actionable guidelines for building such
agentic systems. In summary, this paper provides a reliable dataset of failure
root cause for platform-orchestrated agentic systems, corresponding taxonomy
and benchmark, which serves as a foundation for advancing the development of
more reliable agentic systems.

</details>


### [105] [Model Consistency as a Cheap yet Predictive Proxy for LLM Elo Scores](https://arxiv.org/abs/2509.23510)
*Ashwin Ramaswamy,Nestor Demeure,Ermal Rrapaj*

Main category: cs.AI

TL;DR: 提出了一种使用LLM自动评估其他LLM性能的方法，该方法与人类评估的Elo分数有91%的相关性，可以低成本替代昂贵的人工评估。


<details>
  <summary>Details</summary>
Motivation: 新LLM不断发布，但参数数量不能准确反映性能，需要独立评估方法。当前最佳方法是通过人类比较输出计算Elo分数，但成本高昂。

Method: 使用LLM作为裁判来评估其他LLM的对战结果，通过统计LLM选择某个模型为最佳的一致性来生成评估指标。

Result: LLM裁判评估的一致性指标与人类产生的Elo分数有91%的相关性。

Conclusion: 该方法提供了一个简单、低成本的Elo分数代理，无需人类数据或先验知识。

Abstract: New large language models (LLMs) are being released every day. Some perform
significantly better or worse than expected given their parameter count.
Therefore, there is a need for a method to independently evaluate models. The
current best way to evaluate a model is to measure its Elo score by comparing
it to other models in a series of contests - an expensive operation since
humans are ideally required to compare LLM outputs. We observe that when an LLM
is asked to judge such contests, the consistency with which it selects a model
as the best in a matchup produces a metric that is 91% correlated with its own
human-produced Elo score. This provides a simple proxy for Elo scores that can
be computed cheaply, without any human data or prior knowledge.

</details>


### [106] [AgentGuard: Runtime Verification of AI Agents](https://arxiv.org/abs/2509.23864)
*Roham Koohestani*

Main category: cs.AI

TL;DR: 提出了AgentGuard框架，用于运行时验证自主AI系统的概率保证，通过动态概率保证新范式提供持续定量保证


<details>
  <summary>Details</summary>
Motivation: 自主AI系统的快速演进带来了显著风险，传统验证方法不足，需要转向概率保证来应对系统失败的概率问题

Method: AgentGuard作为检查层观察agent的原始I/O，抽象为状态模型转换的形式事件，使用在线学习动态构建和更新MDP模型，通过概率模型检查实时验证定量属性

Result: 开发了动态概率保证框架，能够对自主AI系统的涌现行为进行形式化建模和实时验证

Conclusion: AgentGuard为自主AI系统提供了有效的运行时验证方法，解决了传统验证方法在处理不可预测性和涌现行为时的不足

Abstract: The rapid evolution to autonomous, agentic AI systems introduces significant
risks due to their inherent unpredictability and emergent behaviors; this also
renders traditional verification methods inadequate and necessitates a shift
towards probabilistic guarantees where the question is no longer if a system
will fail, but the probability of its failure within given constraints. This
paper presents AgentGuard, a framework for runtime verification of Agentic AI
systems that provides continuous, quantitative assurance through a new paradigm
called Dynamic Probabilistic Assurance. AgentGuard operates as an inspection
layer that observes an agent's raw I/O and abstracts it into formal events
corresponding to transitions in a state model. It then uses online learning to
dynamically build and update a Markov Decision Process (MDP) that formally
models the agent's emergent behavior. Using probabilistic model checking, the
framework then verifies quantitative properties in real-time.

</details>


### [107] [Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks](https://arxiv.org/abs/2509.23537)
*Aaron Xuxiang Tian,Ruofan Zhang,Jiayao Tang,Young Min Cho,Xueqian Li,Qiang Yi,Ji Wang,Zhunping Zhang,Danrui Qi,Sharath Chandra Guntuku,Lyle Ungar,Tianyu Shi,Chi Wang*

Main category: cs.AI

TL;DR: 多轮多智能体编排通过LLM智能体间的迭代提议和投票达成共识，在多个基准测试中匹配或超越最强单模型性能，并揭示作者身份和实时投票显示对投票行为的影响。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体编排是否能通过集体决策超越单个LLM的性能，并探索智能体交互机制对共识形成的影响。

Method: 使用四种LLM在三个基准测试上进行实验：比较编排与单模型基线，并在GPQA-Diamond上进行消融实验，分析作者身份可见性和实时投票显示对投票行为的影响。

Result: 编排方法匹配或超越最强单模型，持续优于其他模型。作者身份可见性增加自投票和平局，实时投票显示加剧从众效应，加速收敛但可能导致过早共识。

Conclusion: 多智能体编排是提升LLM性能的有效方法，但需要谨慎设计交互机制以避免负面行为模式。

Abstract: We study multi-turn multi-agent orchestration, where multiple large language
model (LLM) agents interact over multiple turns by iteratively proposing
answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5
Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we
conduct two experiments: (i) benchmarking orchestration against single-LLM
baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who
authored answers and whether they can observe ongoing votes. Orchestration
matches or exceeds the strongest single model and consistently outperforms the
others. Analysis of best-achievable orchestration performance shows potential
for further gains. The ablations show that revealing authorship increases
self-voting and ties, and that showing ongoing votes amplifies herding, which
speeds convergence but can sometimes yield premature consensus.

</details>


### [108] [Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2509.23558)
*Zhaoqi Wang,Daqing He,Zijian Zhang,Xin Li,Liehuang Zhu,Meng Li,Jiamou Liu*

Main category: cs.AI

TL;DR: 提出PASS框架，通过强化学习将初始越狱提示转化为形式化描述，增强隐蔽性并绕过现有对齐防御，然后构建GraphRAG系统来强化后续攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型引入了新的安全挑战，特别是提示越狱攻击，攻击者通过精心设计的提示使LLM偏离人类价值观。需要发现LLM对齐方法中的漏洞。

Method: 使用强化学习将初始越狱提示转化为语义和结构形式化描述，构建GraphRAG系统，利用提取的相关术语和形式化符号作为上下文输入来强化攻击。

Result: 在常见开源模型上进行了广泛实验，证明了该攻击方法的有效性。

Conclusion: PASS框架能够有效发现LLM对齐方法中的漏洞，通过形式化描述和GraphRAG系统增强越狱攻击的隐蔽性和效果。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, yet
they also introduce novel security challenges. For instance, prompt
jailbreaking attacks involve adversaries crafting sophisticated prompts to
elicit responses from LLMs that deviate from human values. To uncover
vulnerabilities in LLM alignment methods, we propose the PASS framework
(\underline{P}rompt J\underline{a}ilbreaking via \underline{S}emantic and
\underline{S}tructural Formalization). Specifically, PASS employs reinforcement
learning to transform initial jailbreak prompts into formalized descriptions,
which enhances stealthiness and enables bypassing existing alignment defenses.
The jailbreak outputs are then structured into a GraphRAG system that, by
leveraging extracted relevant terms and formalized symbols as contextual input
alongside the original query, strengthens subsequent attacks and facilitates
more effective jailbreaks. We conducted extensive experiments on common
open-source models, demonstrating the effectiveness of our attack.

</details>


### [109] [PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents](https://arxiv.org/abs/2509.23614)
*Yaozu Wu,Jizhou Guo,Dongyuan Li,Henry Peng Zou,Wei-Chieh Huang,Yankai Chen,Zhen Wang,Weizhi Zhang,Yangning Li,Meng Zhang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: 提出了PSG-Agent系统，通过个性化防护栏和动态监控解决现有LLM代理防护栏的两个根本限制：统一策略忽视用户差异，以及孤立检查忽略风险累积。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理防护栏存在两个基本问题：对所有用户应用统一防护策略，忽视了相同行为对不同用户的风险差异；仅孤立检查单个响应，忽略了跨交互的风险演化和累积。

Method: PSG-Agent系统包含两个核心组件：1）通过挖掘交互历史中的稳定特征和实时查询状态，创建个性化防护栏；2）在代理流程中实施持续监控，包括计划监控器、工具防火墙、响应防护栏、内存守护者等专门防护组件。

Result: 在医疗、金融和日常生活自动化等多个场景中验证，PSG-Agent显著优于现有代理防护栏（如LlamaGuard3和AGrail），提供了可执行和可审计的个性化安全路径。

Conclusion: PSG-Agent通过个性化防护栏和动态跨轮次监控，为LLM代理提供了更有效的安全保障，解决了现有防护栏的关键局限性。

Abstract: Effective guardrails are essential for safely deploying LLM-based agents in
critical applications. Despite recent advances, existing guardrails suffer from
two fundamental limitations: (i) they apply uniform guardrail policies to all
users, ignoring that the same agent behavior can harm some users while being
safe for others; (ii) they check each response in isolation, missing how risks
evolve and accumulate across multiple interactions. To solve these issues, we
propose PSG-Agent, a personalized and dynamic system for LLM-based agents.
First, PSG-Agent creates personalized guardrails by mining the interaction
history for stable traits and capturing real-time states from current queries,
generating user-specific risk thresholds and protection strategies. Second,
PSG-Agent implements continuous monitoring across the agent pipeline with
specialized guards, including Plan Monitor, Tool Firewall, Response Guard,
Memory Guardian, that track cross-turn risk accumulation and issue verifiable
verdicts. Finally, we validate PSG-Agent in multiple scenarios including
healthcare, finance, and daily life automation scenarios with diverse user
profiles. It significantly outperform existing agent guardrails including
LlamaGuard3 and AGrail, providing an executable and auditable path toward
personalized safety for LLM-based agents.

</details>


### [110] [From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models](https://arxiv.org/abs/2509.23676)
*Jue Zhang,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: 该研究通过三阶段实验探究了大推理模型中推理过程与答案生成的关系，发现推理痕迹对答案质量有显著影响，注意力机制显示答案token会关注推理token，且通过激活修补证实推理对答案生成具有功能性影响。


<details>
  <summary>Details</summary>
Motivation: 探究大推理模型中显式推理痕迹对答案生成的影响程度，理解推理过程与最终答案之间的内在联系。

Method: 采用三阶段研究方法：1) 实证评估推理对答案质量的影响；2) 注意力分析识别推理焦点头；3) 机制干预通过激活修补测试推理激活对答案的依赖性。

Result: 包含显式推理能持续提升答案质量；答案token显著关注推理token；关键推理token的扰动能可靠改变最终答案，证实从推理到答案的信息流。

Conclusion: 大推理模型确实利用推理token进行答案生成，中间推理在塑造模型输出中发挥功能性作用。

Abstract: Large Reasoning Models (LRMs) generate explicit reasoning traces alongside
final answers, yet the extent to which these traces influence answer generation
remains unclear. In this work, we conduct a three-stage investigation into the
interplay between reasoning and answer generation in three distilled DeepSeek
R1 models. First, through empirical evaluation, we demonstrate that including
explicit reasoning consistently improves answer quality across diverse domains.
Second, attention analysis reveals that answer tokens attend substantially to
reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely
tracking the reasoning trajectory, including self-reflective cues. Third, we
apply mechanistic interventions using activation patching to assess the
dependence of answer tokens on reasoning activations. Our results show that
perturbations to key reasoning tokens can reliably alter the final answers,
confirming a directional and functional flow of information from reasoning to
answer. These findings deepen our understanding of how LRMs leverage reasoning
tokens for answer generation, highlighting the functional role of intermediate
reasoning in shaping model outputs. Our data and code are publicly available at
\href{https://aka.ms/R2A-code}{this URL}.

</details>


### [111] [SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents](https://arxiv.org/abs/2509.23694)
*Jianshuo Dong,Sheng Guo,Hao Wang,Zhuotao Liu,Tianwei Zhang,Ke Xu,Minlie Huang,Han Qiu*

Main category: cs.AI

TL;DR: 本文提出了一个自动化的红队测试框架SafeSearch，用于评估搜索代理的安全性，发现LLM搜索代理在面对不可靠搜索结果时存在严重漏洞，最高攻击成功率可达90.5%。


<details>
  <summary>Details</summary>
Motivation: 搜索代理虽然能连接LLM与互联网获取更广泛和更新的信息，但不可靠的搜索结果可能对终端用户构成安全威胁，建立新的威胁面。

Method: 构建了包含300个测试用例的SafeSearch基准，涵盖五类风险（如错误信息和间接提示注入），评估了三种代表性搜索代理框架和15个后端LLM。

Result: 实验结果显示LLM搜索代理存在严重漏洞：当暴露于不可靠网站时，GPT-4.1-mini在搜索工作流设置下的最高攻击成功率可达90.5%。常见防御实践（如提醒提示）效果有限。

Conclusion: 该框架有助于促进更安全的代理开发透明度，强调了搜索代理安全评估的重要性。

Abstract: Search agents connect LLMs to the Internet, enabling access to broader and
more up-to-date information. However, unreliable search results may also pose
safety threats to end users, establishing a new threat surface. In this work,
we conduct two in-the-wild experiments to demonstrate both the prevalence of
low-quality search results and their potential to misguide agent behaviors. To
counter this threat, we introduce an automated red-teaming framework that is
systematic, scalable, and cost-efficient, enabling lightweight and harmless
safety assessments of search agents. Building on this framework, we construct
the SafeSearch benchmark, which includes 300 test cases covering five
categories of risks (e.g., misinformation and indirect prompt injection). Using
this benchmark, we evaluate three representative search agent scaffolds,
covering search workflow, tool-calling, and deep research, across 7 proprietary
and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities
of LLM-based search agents: when exposed to unreliable websites, the highest
ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover,
our analysis highlights the limited effectiveness of common defense practices,
such as reminder prompting. This emphasizes the value of our framework in
promoting transparency for safer agent development. Our codebase and test cases
are publicly available: https://github.com/jianshuod/SafeSearch.

</details>


### [112] [MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models](https://arxiv.org/abs/2509.23725)
*Siqi Ma,Jiajie Huang,Bolin Yang,Fan Zhang,Jinlin Wu,Yue Shen,Guohui Fan,Zhu Zhang,Zelin Zang*

Main category: cs.AI

TL;DR: 提出了MedLA框架，这是一个基于逻辑驱动的多智能体医疗问答系统，通过显式逻辑树和图形引导的讨论来提高医疗推理的准确性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有医疗问答系统在处理复杂问题时存在逻辑不一致性检测能力不足的问题，需要更精细化的推理机制来确保医疗决策的可靠性。

Method: 构建基于三段论（大前提、小前提、结论）的显式逻辑树，让多个智能体通过多轮图形引导的讨论来比较和迭代优化逻辑树，实现错误纠正和矛盾解决。

Result: 在MedDDx和标准医疗QA基准测试中，MedLA持续优于基于静态角色的系统和单智能体基线，并在开源和商业LLM骨干网络上都能有效扩展，达到最先进性能。

Conclusion: MedLA提供了一个可推广的、可信赖的医疗推理范式，通过逻辑驱动的多智能体协作显著提升了复杂医疗问题的解答质量。

Abstract: Answering complex medical questions requires not only domain expertise and
patient-specific information, but also structured and multi-perspective
reasoning. Existing multi-agent approaches often rely on fixed roles or shallow
interaction prompts, limiting their ability to detect and resolve fine-grained
logical inconsistencies. To address this, we propose \textsc{MedLA}, a
logic-driven multi-agent framework built on large language models. Each agent
organizes its reasoning process into an explicit logical tree based on
syllogistic triads (major premise, minor premise, and conclusion), enabling
transparent inference and premise-level alignment. Agents engage in a
multi-round, graph-guided discussion to compare and iteratively refine their
logic trees, achieving consensus through error correction and contradiction
resolution. We demonstrate that \textsc{MedLA} consistently outperforms both
static role-based systems and single-agent baselines on challenging benchmarks
such as MedDDx and standard medical QA tasks. Furthermore, \textsc{MedLA}
scales effectively across both open-source and commercial LLM backbones,
achieving state-of-the-art performance and offering a generalizable paradigm
for trustworthy medical reasoning.

</details>


### [113] [EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance](https://arxiv.org/abs/2509.23730)
*Siyao Song,Cong Ma,Zhihao Cheng,Shiye Lei,Minghao Li,Ying Zeng,Huaixiao Tou,Kai Jia*

Main category: cs.AI

TL;DR: 提出EAPO框架，通过多轮外部专家交互增强LLM的强化学习探索，提高推理能力


<details>
  <summary>Details</summary>
Motivation: 现有基于结果监督的RL方法存在探索效率低和奖励稀疏问题，需要改进

Method: EAPO框架让策略模型自适应决定何时及如何咨询外部专家，获取更丰富的奖励信号

Result: 在AIME 2024/2025和AIMO 2025数学推理基准上，EAPO平均比自探索模型提升5分

Conclusion: 外部专家协助能有效将专家知识内化到策略模型中，增强模型的固有推理能力

Abstract: Large language models (LLMs) have recently advanced in reasoning when
optimized with reinforcement learning (RL) under verifiable rewards. Existing
methods primarily rely on outcome-based supervision to strengthen internal LLM
reasoning, often leading to inefficient exploration and sparse rewards. To
mitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a
novel RL framework that enhances exploration by incorporating multi-turn
interactions with external experts during training. Unlike prior methods, where
policies reason in isolation, EAPO incentivizes the policy to adaptively
determine when and how to consult experts, yielding richer reward signals and
more reliable reasoning trajectories. External assistance ultimately
internalizes expert knowledge into the policy model, amplifying the model's
inherent reasoning capabilities. During evaluation, the policy model has been
well-optimized to solve questions independently, producing improved reasoning
paths and more accurate solutions. Experiments on mathematical reasoning
benchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO
consistently outperforms expert-assisted workflow, expert-distilled models, and
RL baselines, with an average gain of 5 points over self-exploratory models.

</details>


### [114] [GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks](https://arxiv.org/abs/2509.23738)
*Cong Chen,Kaixiang Ji,Hao Zhong,Muzhi Zhu,Anzhou Li,Guo Gan,Ziyuan Huang,Cheng Zou,Jiajia Liu,Jingdong Chen,Hao Chen,Chunhua Shen*

Main category: cs.AI

TL;DR: GUI-Shepherd是一个过程奖励模型，通过提供密集的逐步反馈来解决GUI任务中稀疏奖励和信用分配问题，在在线和离线设置中显著提升了GUI代理的性能。


<details>
  <summary>Details</summary>
Motivation: 长序列GUI任务中的自主代理受到稀疏奖励和难以处理的信用分配问题的阻碍，需要更有效的监督机制。

Method: 开发GUI-Shepherd过程奖励模型，在包含52k交互的大规模数据集上训练，使用人工标注分数和GPT-4o生成的原理，可作为RL训练奖励提供者和推理验证器。

Result: 在AndroidWorld基准测试中，通过多轮在线PPO将成功率提高7.7个百分点；作为推理验证器带来5.1个百分点的改进；在AndroidControl基准测试中分别获得2.2和4.3个百分点的提升。

Conclusion: 高保真的过程监督对于构建更强大的GUI代理至关重要，并提出了一个可推广的解决方案。

Abstract: Autonomous agents for long-sequence Graphical User Interface tasks are
hindered by sparse rewards and the intractable credit assignment problem. To
address these challenges, we introduce GUI-Shepherd, a Process Reward Model
that provides dense, step-by-step feedback to guide agents. GUI-Shepherd is
trained on a diverse large-scale data set of $52$k interactions that features
human-annotated scores and GPT-4o generated rationales, enabling it to serve
both as a reward provider for RL training and as a verifier for inference. As
far as we know, we are the first to conduct a systematic study of process
supervision in GUI agents, across diverse settings from online long-horizon
tasks to offline single-step prediction. On the online AndroidWorld benchmark,
GUI-Shepherd improves success rate by $7.7$ points via multi-turn online PPO,
significantly outperforming Outcome Reward Model based competitors. When used
as an inference verifier, it brings $5.1$ points improvements. The benefits
generalize to the offline AndroidControl benchmark, with gains of $2.2$ points
as a reward provider and $4.3$ points as a verifier. Collectively, our results
establish that high-fidelity process supervision is critical for building more
capable GUI agents and present a generalizable solution.

</details>


### [115] [Transparent Visual Reasoning via Object-Centric Agent Collaboration](https://arxiv.org/abs/2509.23757)
*Benjamin Teoh,Ben Glocker,Francesca Toni,Avinash Kori*

Main category: cs.AI

TL;DR: OCEAN是一个基于对象中心表示和透明多智能体推理的可解释AI框架，通过博弈论推理过程生成可信且直观的解释，在视觉分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决可解释AI中生成基于人类可理解概念的解释这一核心挑战，特别是在视觉领域。

Method: 使用对象中心表示和透明多智能体推理过程，通过博弈论驱动智能体达成一致，生成连贯且具有区分性的证据。

Result: 在两个诊断性多对象数据集上，OCEAN与最先进的黑盒模型表现相当，用户研究显示其解释更直观可信。

Conclusion: OCEAN提供了一个忠实且可解释的决策过程，在保持竞争力的同时提高了AI系统的透明度和可信度。

Abstract: A central challenge in explainable AI, particularly in the visual domain, is
producing explanations grounded in human-understandable concepts. To tackle
this, we introduce OCEAN (Object-Centric Explananda via Agent Negotiation), a
novel, inherently interpretable framework built on object-centric
representations and a transparent multi-agent reasoning process. The
game-theoretic reasoning process drives agents to agree on coherent and
discriminative evidence, resulting in a faithful and interpretable
decision-making process. We train OCEAN end-to-end and benchmark it against
standard visual classifiers and popular posthoc explanation tools like GradCAM
and LIME across two diagnostic multi-object datasets. Our results demonstrate
competitive performance with respect to state-of-the-art black-box models with
a faithful reasoning process, which was reflected by our user study, where
participants consistently rated OCEAN's explanations as more intuitive and
trustworthy.

</details>


### [116] [Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules](https://arxiv.org/abs/2509.23836)
*Chenyu Zhou,Xiaoming Shi,Hui Qiu,Xiawu Zheng,Haitao Leng,Yankai Jiang,Shaoguo Liu,Tingting Gao,Rongrong Ji*

Main category: cs.AI

TL;DR: 提出了Mix-ECom数据集，用于评估电商对话中LLM代理处理混合类型对话和复杂领域规则的能力，包含4,799个样本，涵盖四种对话类型和82条电商规则。


<details>
  <summary>Details</summary>
Motivation: 当前电商代理基准缺乏评估处理混合类型电商对话和复杂领域规则的能力，需要更全面的评估框架。

Method: 基于真实客服对话构建Mix-ECom语料库，包含四种对话类型、三种电商任务类型和82条电商规则，并建立基线模型和动态框架。

Result: 结果显示当前电商代理在处理电商对话时能力不足，主要由于复杂领域规则导致的幻觉问题。

Conclusion: Mix-ECom数据集填补了电商代理评估的空白，揭示了当前代理在处理复杂电商对话时的局限性。

Abstract: E-commerce agents contribute greatly to helping users complete their
e-commerce needs. To promote further research and application of e-commerce
agents, benchmarking frameworks are introduced for evaluating LLM agents in the
e-commerce domain. Despite the progress, current benchmarks lack evaluating
agents' capability to handle mixed-type e-commerce dialogue and complex domain
rules. To address the issue, this work first introduces a novel corpus, termed
Mix-ECom, which is constructed based on real-world customer-service dialogues
with post-processing to remove user privacy and add CoT process. Specifically,
Mix-ECom contains 4,799 samples with multiply dialogue types in each e-commerce
dialogue, covering four dialogue types (QA, recommendation, task-oriented
dialogue, and chit-chat), three e-commerce task types (pre-sales, logistics,
after-sales), and 82 e-commerce rules. Furthermore, this work build baselines
on Mix-Ecom and propose a dynamic framework to further improve the performance.
Results show that current e-commerce agents lack sufficient capabilities to
handle e-commerce dialogues, due to the hallucination cased by complex domain
rules. The dataset will be publicly available.

</details>


### [117] [Rethinking Reward Miscalibration of GRPO in Agentic RL](https://arxiv.org/abs/2509.23870)
*Jingyu Liu,Xiaopeng Wu,Jingquan Peng,Kehan Chen,Chuan Yu,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: 本文挑战了传统观点，指出基于结果的奖励不会错误强化有缺陷的中间步骤，反而会惩罚这些步骤。研究发现梯度耦合才是导致有缺陷动作被增强的关键问题，并提出通过动作分类训练来缓解梯度干扰。


<details>
  <summary>Details</summary>
Motivation: 传统认为基于结果的奖励会导致奖励校准错误，错误地强化有缺陷的中间步骤。但本文旨在证明基于结果的奖励实际上会惩罚这些有缺陷的步骤，并揭示梯度耦合才是真正的问题所在。

Method: 提出训练actor进行好坏动作分类，以分离好/坏动作的嵌入表示，从而缓解梯度干扰问题。通过广泛的实验验证方法的有效性。

Result: 研究表明基于结果的奖励确实会惩罚有缺陷的中间步骤，但梯度耦合会导致某些有缺陷动作被增强。提出的动作分类方法能有效缓解这一问题。

Conclusion: 梯度耦合是智能体强化学习中的关键问题，通过动作分类训练可以有效分离好/坏动作的嵌入，减轻梯度干扰，提高训练效果。

Abstract: Building autonomous agents capable of solving long-horizon, real-world tasks
has garnered significant research interest. But outcome based rewards may cause
reward miscalibration which means it might mistakenly allocate positive reward
to flawed middle steps which is regarded as the key reason making the bad
actions being reinforced during training. However we reveal that outcome based
reward ensures expected negative advantage for those flawed middle steps, which
means the flawed actions should be punished during training. Even accounting
for the ``squeezing effect", the probability mass of good actions should
increase and the actor should gradually get rid of harmful actions. This shows
that flawed actions should be punished during training. We further identify
gradient coupling between similar samples as a key issue in agentic RL, the
input prompt is extremely similar and the output action space is limited,
therefore during training, gradients from well-performing samples can
inadvertently strengthen suboptimal or incorrect actions due to similar input
observation and output actions. We show that with gradient coupling, some
flawed actions might be enhanced. To address this, we propose training the
actor to classify good or bad actions to separate the embedding of good/bad
actions and alleviate the gradient interference, extensive experiments shows
its effectiveness.

</details>


### [118] [Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B](https://arxiv.org/abs/2509.23882)
*Shuyi Lin,Tian Lu,Zikai Wang,Bo Wen,Yibo Zhao,Cheng Tan*

Main category: cs.AI

TL;DR: 对GPT-OSS-20B模型进行安全评估，揭示了在对抗条件下模型的多种失效模式，包括量化热、推理黑洞、薛定谔合规性、推理过程幻象和链式导向提示等。


<details>
  <summary>Details</summary>
Motivation: 评估GPT-OSS系列开源模型在对抗条件下的安全性和鲁棒性，揭示潜在的安全漏洞。

Method: 使用Jailbreak Oracle (JO)系统化LLM评估工具，在GPT-OSS-20B模型上进行对抗性测试。

Result: 实验发现了多种失效模式，这些行为可以被利用，导致严重后果。

Conclusion: GPT-OSS-20B模型存在显著的安全漏洞，需要进一步的安全加固。

Abstract: OpenAI's GPT-OSS family provides open-weight language models with explicit
chain-of-thought (CoT) reasoning and a Harmony prompt format. We summarize an
extensive security evaluation of GPT-OSS-20B that probes the model's behavior
under different adversarial conditions. Using the Jailbreak Oracle (JO) [1], a
systematic LLM evaluation tool, the study uncovers several failure modes
including quant fever, reasoning blackholes, Schrodinger's compliance,
reasoning procedure mirage, and chain-oriented prompting. Experiments
demonstrate how these behaviors can be exploited on GPT-OSS-20B models, leading
to severe consequences.

</details>


### [119] [TusoAI: Agentic Optimization for Scientific Methods](https://arxiv.org/abs/2509.23986)
*Alistair Turcan,Kexin Huang,Lei Li,Martin Jinye Zhang*

Main category: cs.AI

TL;DR: TusoAI是一个代理式AI系统，能够根据科学任务描述和评估函数自主开发和优化计算方法，通过整合领域知识和迭代优化，在单细胞RNA-seq数据去噪和卫星地球监测等任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 科学发现常因需要手动开发计算工具来分析复杂实验数据而受阻，现有LLM系统要么专注于使用现有方法进行科学分析，要么开发通用机器学习方法，未能有效整合科学领域特有的非结构化知识。

Method: TusoAI将领域知识整合到知识树表示中，执行迭代的领域特定优化和模型诊断，通过候选解决方案池改进性能。

Result: 基准评估显示TusoAI在多样化任务中优于最先进的专家方法、MLE代理和科学AI代理。应用于遗传学关键开放问题时改进了现有计算方法并发现了新的生物学关联。

Conclusion: TusoAI能够加速计算方法开发，在科学发现中发挥重要作用，特别是在整合领域知识和自主优化方面表现出色。

Abstract: Scientific discovery is often slowed by the manual development of
computational tools needed to analyze complex experimental data. Building such
tools is costly and time-consuming because scientists must iteratively review
literature, test modeling and scientific assumptions against empirical data,
and implement these insights into efficient software. Large language models
(LLMs) have demonstrated strong capabilities in synthesizing literature,
reasoning with empirical data, and generating domain-specific code, offering
new opportunities to accelerate computational method development. Existing
LLM-based systems either focus on performing scientific analyses using existing
computational methods or on developing computational methods or models for
general machine learning without effectively integrating the often unstructured
knowledge specific to scientific domains. Here, we introduce TusoAI , an
agentic AI system that takes a scientific task description with an evaluation
function and autonomously develops and optimizes computational methods for the
application. TusoAI integrates domain knowledge into a knowledge tree
representation and performs iterative, domain-specific optimization and model
diagnosis, improving performance over a pool of candidate solutions. We
conducted comprehensive benchmark evaluations demonstrating that TusoAI
outperforms state-of-the-art expert methods, MLE agents, and scientific AI
agents across diverse tasks, such as single-cell RNA-seq data denoising and
satellite-based earth monitoring. Applying TusoAI to two key open problems in
genetics improved existing computational methods and uncovered novel biology,
including 9 new associations between autoimmune diseases and T cell subtypes
and 7 previously unreported links between disease variants linked to their
target genes. Our code is publicly available at
https://github.com/Alistair-Turcan/TusoAI.

</details>


### [120] [LLM/Agent-as-Data-Analyst: A Survey](https://arxiv.org/abs/2509.23988)
*Zirui Tang,Weizheng Wang,Zihang Zhou,Yang Jiao,Bangrui Xu,Boyu Niu,Xuanhe Zhou,Guoliang Li,Yeye He,Wei Zhou,Yitong Song,Cheng Tan,Bin Wang,Conghui He,Xiaoyang Wang,Fan Wu*

Main category: cs.AI

TL;DR: 本文综述了基于大语言模型和智能代理的数据分析技术，探讨了其在结构化、半结构化、非结构化和异构数据中的多模态应用，并提出了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则或小模型的数据分析方法在处理复杂数据理解和自然语言交互方面存在局限，而基于LLM的智能代理技术能够实现语义感知、多模态集成和自主管道编排，推动数据分析向智能化发展。

Method: 从模态角度系统回顾LLM技术在结构化数据（如表问答、NL2GQL）、半结构化数据（如标记语言理解）、非结构化数据（如图表理解、文档分析）和异构数据（如数据检索、模态对齐）中的应用。

Result: LLM/Agent技术显著提升了数据分析的语义理解能力、自然语言交互体验和自主工作流编排，实现了更复杂的数据分析任务。

Conclusion: LLM驱动的数据分析代理在语义感知、多模态集成和自主管道方面具有显著优势，但仍面临挑战，需要进一步研究以推动该领域发展。

Abstract: Large language model (LLM) and agent techniques for data analysis (a.k.a
LLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both
academica and industry. In comparison with traditional rule or small-model
based approaches, (agentic) LLMs enable complex data understanding, natural
language interfaces, semantic analysis functions, and autonomous pipeline
orchestration. The technical evolution further distills five key design goals
for intelligent data analysis agents, namely semantic-aware design,
modality-hybrid integration, autonomous pipelines, tool-augmented workflows,
and support for open-world tasks. From a modality perspective, we review
LLM-based techniques for (i) structured data (e.g., table question answering
for relational data and NL2GQL for graph data), (ii) semi-structured data
(e.g., markup languages understanding and semi-structured table modeling),
(iii) unstructured data (e.g., chart understanding, document understanding,
programming languages vulnerable detection), and (iv) heterogeneous data (e.g.,
data retrieval and modality alignment for data lakes). Finally, we outline the
remaining challenges and propose several insights and practical directions for
advancing LLM/Agent-powered data analysis.

</details>


### [121] [Do Repetitions Matter? Strengthening Reliability in LLM Evaluations](https://arxiv.org/abs/2509.24086)
*Miguel Angel Alvarado Gonzalez,Michelle Bruno Hernandez,Miguel Angel Peñaloza Perez,Bruno Lopez Orozco,Jesus Tadeo Cruz Soto,Sandra Malagon*

Main category: cs.AI

TL;DR: 研究表明单次运行的LLM排行榜不可靠，需要至少2次重复运行来获得稳定的模型排名结果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM排行榜通常依赖单次随机运行，但需要多少重复运行才能得出可靠结论尚不清楚。

Method: 在AI4Math基准上对8个最先进模型进行3次独立运行，使用混合效应逻辑回归、领域级边际均值、排名不稳定性分析和运行间可靠性评估。

Result: 单次运行排行榜很脆弱：83%的切片至少出现一次排名反转；两次运行可消除约83%的单次运行排名反转。

Conclusion: 应将评估视为实验，报告不确定性，在随机解码下使用≥2次重复运行，这能提高鲁棒性且对小团队可行。

Abstract: LLM leaderboards often rely on single stochastic runs, but how many
repetitions are required for reliable conclusions remains unclear. We
re-evaluate eight state-of-the-art models on the AI4Math Benchmark with three
independent runs per setting. Using mixed-effects logistic regression,
domain-level marginal means, rank-instability analysis, and run-to-run
reliability, we assessed the value of additional repetitions. Our findings
shows that Single-run leaderboards are brittle: 10/12 slices (83\%) invert at
least one pairwise rank relative to the three-run majority, despite a zero
sign-flip rate for pairwise significance and moderate overall interclass
correlation. Averaging runs yields modest SE shrinkage ($\sim$5\% from one to
three) but large ranking gains; two runs remove $\sim$83\% of single-run
inversions. We provide cost-aware guidance for practitioners: treat evaluation
as an experiment, report uncertainty, and use $\geq 2$ repetitions under
stochastic decoding. These practices improve robustness while remaining
feasible for small teams and help align model comparisons with real-world
reliability.

</details>


### [122] [Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs](https://arxiv.org/abs/2509.24107)
*Shreyas Singh,Kunal Singh,Pradeep Moturi*

Main category: cs.AI

TL;DR: Fathom-DeepResearch是一个由两个专门模型组成的智能系统：Fathom-Search-4B用于基于证据的网络搜索调查，Fathom-Synthesizer-4B用于将搜索轨迹转换为结构化研究报告。该系统在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 工具集成推理已成为实现智能应用的关键焦点，DeepResearch智能体因其在复杂、开放式信息搜索任务中的强大表现而受到广泛关注。

Method: 系统包含两个专门模型：1) Fathom-Search-4B：基于Qwen3-4B训练，使用DUETQA数据集、RAPO强化学习和可引导的步骤级奖励进行优化；2) Fathom-Synthesizer-4B：将多轮搜索轨迹转换为结构化研究报告。

Result: 在DeepSearch基准测试（SimpleQA、FRAMES、WebWalker、Seal0、MuSiQue）和DeepResearch-Bench上取得了开源权重类别中的最先进性能，并在HLE、AIME-25、GPQA-Diamond和MedQA等多样化推理任务中表现出强大的泛化能力。

Conclusion: Fathom-DeepResearch系统通过专门的搜索和合成模型组合，在复杂信息搜索任务中实现了卓越性能，展示了工具集成推理的有效性。

Abstract: Tool-integrated reasoning has emerged as a key focus for enabling agentic
applications. Among these, DeepResearch Agents have gained significant
attention for their strong performance on complex, open-ended
information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system
composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch
model trained from Qwen3-4B and optimized for evidence-based investigation
through live web search and targeted webpage querying. Its training combines
three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent
self-play that enforces strict web-search dependence and heterogeneous source
grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes
multi-turn Reinforcement Learning with Verifiable Rewards through curriculum
pruning, reward-aware advantage scaling, and per-prompt replay buffers; and
(iii) a steerable step-level reward that classifies each tool call by cognitive
behavior and marginal utility, enabling explicit control over search trajectory
breadth, depth, and horizon. These improvements enable reliable extension of
tool-calling beyond 20 calls when warranted. The second is
Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn
DeepSearch traces into structured, citation-dense DeepResearch Reports for
comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES,
WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves
state-of-the-art performance in the open-weights category while demonstrating
strong generalization to diverse reasoning tasks including HLE, AIME-25,
GPQA-Diamond, and MedQA.

</details>


### [123] [Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework](https://arxiv.org/abs/2509.24127)
*Nooshin Bahador*

Main category: cs.AI

TL;DR: 提出模块化、基于组件的AI代理架构，通过对话界面连接自然语言与复杂企业数据仓库，解决数据可访问性挑战，支持透明决策和自动评估。


<details>
  <summary>Details</summary>
Motivation: 解决非技术用户访问复杂数据仓库的困难，弥合语义鸿沟，确保在数据敏感领域部署LLM代理的可信度和可解释性。

Method: 采用模块化架构，包含多层推理框架、统计上下文模块和自动评估框架，集成BigQuery生态系统进行安全数据检索和业务规则应用。

Result: 案例研究表明该架构在保险理赔处理系统中创建了稳健、可评估且可信的系统，能够生成可审计的决策依据。

Conclusion: 该方法为在数据敏感、高风险领域部署LLM驱动的代理提供了有效的开发和评估框架。

Abstract: This article presents a modular, component-based architecture for developing
and evaluating AI agents that bridge the gap between natural language
interfaces and complex enterprise data warehouses. The system directly
addresses core challenges in data accessibility by enabling non-technical users
to interact with complex data warehouses through a conversational interface,
translating ambiguous user intent into precise, executable database queries to
overcome semantic gaps. A cornerstone of the design is its commitment to
transparent decision-making, achieved through a multi-layered reasoning
framework that explains the "why" behind every decision, allowing for full
interpretability by tracing conclusions through specific, activated business
rules and data points. The architecture integrates a robust quality assurance
mechanism via an automated evaluation framework that serves multiple functions:
it enables performance benchmarking by objectively measuring agent performance
against golden standards, and it ensures system reliability by automating the
detection of performance regressions during updates. The agent's analytical
depth is enhanced by a statistical context module, which quantifies deviations
from normative behavior, ensuring all conclusions are supported by quantitative
evidence including concrete data, percentages, and statistical comparisons. We
demonstrate the efficacy of this integrated agent-development-with-evaluation
framework through a case study on an insurance claims processing system. The
agent, built on a modular architecture, leverages the BigQuery ecosystem to
perform secure data retrieval, apply domain-specific business rules, and
generate human-auditable justifications. The results confirm that this approach
creates a robust, evaluable, and trustworthy system for deploying LLM-powered
agents in data-sensitive, high-stakes domains.

</details>


### [124] [Learning to Ponder: Adaptive Reasoning in Latent Space](https://arxiv.org/abs/2509.24238)
*Yixin He,Lumingyuan Tang*

Main category: cs.AI

TL;DR: FR-Ponder是一个无需修改主干模型权重的自适应推理计算分配框架，通过潜在向量引导实现实例级别的推理深度调整，在保持准确性的同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Best-of-N和多数投票对所有输入使用统一推理深度，导致简单问题计算浪费而复杂问题推理不足。需要一种能根据问题复杂度自适应分配计算资源的方法。

Method: 使用小于1M参数的控制器观察隐藏状态，决定停止或应用小的思考步骤。通过添加预计算的引导向量到冻结表示中，提取与深度推理输出相关的潜在引导向量并重新应用。采用GRPO作为奖励信号自适应调节推理深度。

Result: 在GSM8K和MATH500数据集上，FR-Ponder改善了计算-准确率边界，以更低的FLOPs获得匹配的准确性，优于早期退出基线方法。

Conclusion: FR-Ponder通过潜在引导实现了与问题难度相关的计算分配，在不修改主干权重的情况下有效提升了推理效率。

Abstract: Test-time compute has emerged as a key paradigm for enhancing LLM reasoning,
yet prevailing approaches like Best-of-N and majority voting apply uniform
depth across inputs, wasting computation on simple queries while potentially
under-thinking complex ones. We present FR-Ponder, a single-graph,
backbone-training-free framework that allocates instance-adaptive reasoning
compute via latent steering. A less than 1M-param controller observes hidden
states and decides to halt or apply a small ponder step by adding a
pre-computed steering vector to frozen representations. Our method extracts the
latent steering vector associated with deeper reasoning outputs and direct IO
from LLM and re-applies it through a tunable scaling factor, allowing the model
to adapt its reasoning depth to the complexity of each input. To balance
performance and computational cost, we employ Group Relative Policy
Optimization (GRPO) as a reward signal to adaptively regulate reasoning depth,
achieving task accuracy while mitigating overreasoning. Through curriculum
learning and careful reward engineering, FR-Ponder learns calibrated compute
allocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder
improves the compute-accuracy frontier, delivering lower FLOPs with better
matched accuracy and comparing favorably to early-exit baselines, without
modifying backbone weights. Analyses visualize interpretable steering
directions and show learned compute allocation correlates with problem
difficulty.

</details>


### [125] [SpecExit: Accelerating Large Reasoning Model via Speculative Exit](https://arxiv.org/abs/2509.24248)
*Rubing Yang,Huajun Bai,Song Liu,Guanghua Yu,Runzhi Fan,Yanbin Dang,Jiejing Zhang,Kai Liu,Jianchen Zhu,Peng Chen*

Main category: cs.AI

TL;DR: 提出SpecExit框架，通过轻量级草稿模型直接预测未来token和提前退出信号，解决大推理模型的过度思考问题，实现66%的生成长度减少和2.5倍端到端延迟加速。


<details>
  <summary>Details</summary>
Motivation: 大推理模型存在过度思考问题，产生不必要的长输出和高的端到端延迟，限制了实际部署。现有的提前退出机制依赖探测机制，引入了检测开销，限制了延迟收益和泛化能力。

Method: 受推测解码中隐藏状态使用的启发，提出SpecExit框架，从轻量级草稿模型直接预测未来token和提前退出信号，无需探测开销。

Result: 相比推测解码基线，平均生成长度减少66%，端到端延迟加速2.5倍，且不损害准确性。

Conclusion: 利用隐藏状态的固有信号提供有效的提前退出信号，表明隐藏状态在高效推理中具有更广泛的应用潜力。

Abstract: Despite their strong performance on reasoning tasks, large reasoning models
(LRMs) often suffer from overthinking, producing unnecessarily long outputs and
incurring high end-to-end latency, a significant limitation to their real-world
deployment. To address overthinking, early-exit mechanisms have been proposed
to terminate reasoning before typical completion, showing that this approach
can effectively shorten generation length with minimal impact on accuracy.
However, their reliance on probing mechanisms introduces a detection overhead
that limits their end-to-end latency gains and compromises their
generalizability across diverse problems. Inspired by the use of hidden states
in speculative decoding, we propose SpecExit, a novel framework that predicts
both future tokens and an early-exit signal directly from a lightweight draft
model without probing overhead. Our method offers significant improvements,
reducing average generation length by 66\% and achieving a 2.5x speedup in
end-to-end latency compared to the speculative decoding baseline, without
compromising accuracy. Our method leverages the inherent signals from hidden
states to provide effective early-exit signals, suggesting broader use of
hidden states for efficient reasoning. Our code is available at
https://github.com/Tencent/AngelSlim.

</details>


### [126] [Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models](https://arxiv.org/abs/2509.24261)
*Yuhua Jiang,Jiawei Huang,Yufeng Yuan,Xin Mao,Yu Yue,Qianchuan Zhao,Lin Yan*

Main category: cs.AI

TL;DR: 提出风险敏感强化学习框架RS-GRPO，通过风险寻求目标在均值和最大奖励间插值，解决RLVR方法在复杂推理任务中的探索困境，提升多解性能(pass@k)同时保持单解准确率(pass@1)。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在探索困境：预训练LLM的尖锐初始策略将标准RL算法限制在狭窄解空间内，虽然提升了单解准确率但抑制了解多样性和多解性能，导致RLVR更多是蒸馏现有能力而非发现新推理策略。

Method: 引入风险敏感强化学习框架，采用风险寻求目标在均值和最大奖励间插值，提出RS-GRPO算法，通过放大从挑战性提示中学习来驱动深度探索。该方法实现简单，只需少量代码修改。

Result: 在六个数学推理基准和五个不同LLM上，RS-GRPO持续改进pass@k性能，同时保持或增强pass@1准确率。

Conclusion: RS-GRPO有效解决了RLVR的探索困境，能够在保持单解性能的同时显著提升多解性能，为LLM在复杂推理任务中的强化学习提供了更有效的探索策略。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for enhancing Large Language Models (LLMs) on complex reasoning tasks. However,
existing methods suffer from an exploration dilemma: the sharply peaked initial
policies of pre-trained LLMs confine standard RL algorithms to a narrow set of
solutions, boosting single-solution accuracy (pass@1) but suppressing solution
diversity and multi-solution performance (pass@k). As a result, RLVR often
distills existing capabilities rather than discovering new reasoning
strategies. To overcome this, we introduce a Risk-Sensitive Reinforcement
Learning framework. Our approach employs a risk-seeking objective that
interpolates between mean and maximum rewards, leading to a novel algorithm,
Risk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying
learning from challenging prompts. Remarkably, RS-GRPO is simple to implement,
requiring only minor code modifications. On six mathematical reasoning
benchmarks and with five different LLMs, RS-GRPO consistently improves pass@k
performance while maintaining or enhancing pass@1 accuracy.

</details>


### [127] [From Static to Dynamic: Adaptive Monte Carlo Search for Mathematical Process Supervision](https://arxiv.org/abs/2509.24351)
*Jie Ma,Shihao Qi,Rui Xing,Ziang Yin,Bifan Wei,Jun Liu,Tongliang Liu*

Main category: cs.AI

TL;DR: 提出了自适应蒙特卡洛搜索（AMCS）框架，通过动态节点值估计和路径扩展来提升过程奖励模型（PRM）的训练数据质量，在数学推理任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用固定预算采样策略来估计推理步骤质量，在自动化数据生成过程中效率低下且不灵活，无法适应不同复杂度的推理步骤。

Method: AMCS框架采用自适应采样策略，为不确定的推理步骤分配更多样本，对容易估计的步骤使用较少样本；同时通过时间自适应策略进行路径扩展，从广泛探索逐步转向最有希望的方向。

Result: 构建了包含20万过程监督样本的MathSearch-200K数据集，Qwen2.5-Math-7B-PRM-AMCS在MATH500上达到76.2%准确率，超越所有基线PRM。7B模型在AMCS监督下性能超过72B模型。

Conclusion: AMCS框架显著提升了过程奖励模型的训练效率和效果，在分布外问题上也表现出强大的泛化能力。

Abstract: The quality of process data plays a key role in training a Process Reward
Model (PRM), which can enhance the complex mathematical reasoning capability of
large language models. Existing methods estimate the quality of reasoning steps
based on a fixed-budget sampling strategy and navigate a vast search space to
perform path expansion during the automated data generation process, resulting
in their inefficiency and inflexibility. To address these issues, we propose
Adaptive Monte Carlo Search (AMCS), a framework that transforms data generation
from fixed, static to adaptive, dynamic search at the level of node value
estimation and path expansion. On one hand, AMCS adaptively refines estimation
by allocating more samples to uncertain reasoning steps while using fewer
samples for those that are easier to estimate. On the other hand, it enhances
the path expansion through a Monte Carlo algorithm with a temporally adaptive
policy that begins with broad exploration and gradually shifts toward
exploiting the most promising directions. With AMCS, we construct a large-scale
dataset MathSearch-200K of about 200K process supervision examples for training
PRMs. To verify the effectiveness of our method, we conduct extensive
experiments on four mathematical reasoning benchmarks. Experimental results
show that Qwen2.5-Math-7B-PRM-AMCS achieves up to 76.2% accuracy on MATH500
with GLM-4-9B, outperforming all baseline PRMs. Notably, a 7B model supervised
by Qwen2.5-Math-7B-PRM-AMCS surpasses a 72B model with weaker supervision.
Moreover, Qwen2.5-Math-7B-PRM-AMCS maintains consistent advantages on
out-of-distribution problems, demonstrating strong generalization capability.
Our code is available at https://github.com/reml-group/AMCS.

</details>


### [128] [Training Agents Inside of Scalable World Models](https://arxiv.org/abs/2509.24527)
*Danijar Hafner,Wilson Yan,Timothy Lillicrap*

Main category: cs.AI

TL;DR: Dreamer 4是一个可扩展的智能代理，通过在快速准确的世界模型中强化学习来解决控制任务。它在Minecraft游戏中准确预测物体交互和游戏机制，仅从离线数据中成功获取钻石，无需环境交互。


<details>
  <summary>Details</summary>
Motivation: 先前世界模型无法准确预测复杂环境中的物体交互，且实际应用中环境交互可能不安全且缓慢。需要开发能够从离线数据中学习的智能代理。

Method: 使用快捷强制目标和高效transformer架构构建世界模型，实现实时交互推理。从少量数据中学习通用动作条件，从多样化无标签视频中提取知识。在想象中训练行为。

Result: Dreamer 4在Minecraft中大幅超越先前世界模型，准确预测物体交互。成为首个仅从离线数据中获取钻石的代理，需要从原始像素中选择超过20,000个鼠标和键盘动作序列。

Conclusion: 这项工作为想象训练提供了可扩展的配方，标志着向智能代理迈出的重要一步。

Abstract: World models learn general knowledge from videos and simulate experience for
training behaviors in imagination, offering a path towards intelligent agents.
However, previous world models have been unable to accurately predict object
interactions in complex environments. We introduce Dreamer 4, a scalable agent
that learns to solve control tasks by reinforcement learning inside of a fast
and accurate world model. In the complex video game Minecraft, the world model
accurately predicts object interactions and game mechanics, outperforming
previous world models by a large margin. The world model achieves real-time
interactive inference on a single GPU through a shortcut forcing objective and
an efficient transformer architecture. Moreover, the world model learns general
action conditioning from only a small amount of data, allowing it to extract
the majority of its knowledge from diverse unlabeled videos. We propose the
challenge of obtaining diamonds in Minecraft from only offline data, aligning
with practical applications such as robotics where learning from environment
interaction can be unsafe and slow. This task requires choosing sequences of
over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors
in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft
purely from offline data, without environment interaction. Our work provides a
scalable recipe for imagination training, marking a step towards intelligent
agents.

</details>


### [129] [Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity](https://arxiv.org/abs/2509.24836)
*Zhen Bi,Zhenlin Hu,Jinnan Yang,Mingyang Chen,Cheng Deng,Yida Xue,Zeyu Yang,Qing Shen,Zhenfang Liu,Kang Zhao,Ningyu Zhang,Jungang Lou*

Main category: cs.AI

TL;DR: 提出数据推理强度(DRI)指标来量化训练数据的逻辑推理复杂度，并通过重新认知优化策略增强数据推理强度，从而提升LLM的逻辑推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注数据格式转换而忽视训练样本的内部推理复杂度，导致数据的推理潜力未被充分挖掘和利用。

Method: 引入数据推理强度(DRI)指标量化样本的潜在逻辑推理复杂度，并提出重新认知优化策略系统性地增强训练数据的逻辑推理强度。

Result: 实验表明该方法显著优于数据为中心的策略，在强化学习框架下也得到验证。

Conclusion: 优先考虑数据的推理复杂度而非单纯的数据规模或表面形式，对于实现LLM的完整认知潜力至关重要。

Abstract: Recent advances in large language models (LLMs) highlight the importance of
training data structure and quality in shaping reasoning behavior. However,
most existing approaches focus on transforming data formats while neglecting
the internal reasoning complexity of training samples, leaving the reasoning
potential of data under-explored and underutilized. In this work, we posit that
LLM logical reasoning performance is jointly constrained by the potential of
the training data and the cognitive capacity of the model. To make this
relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel
metric that quantifies the latent logical reasoning complexity of samples by
decomposing and aggregating their logical structures. This allows us to analyze
how well current LLMs utilize logical reasoning signals and identify
performance gaps relative to data potential. Based on this insight, we
introduce a re-cognizing optimization strategy that systematically enhances the
logical reasoning intensity of training data.Rather than increasing data
volume, our method re-optimizes existing samples to better align with the LLM's
logical reasoning boundary. Extensive experiments show that our approach
significantly improves performance and generalization over data-centric
strategies. We further validate our method under a reinforcement learning
framework. Our results indicate that prioritizing reasoning complexity in data
rather than sheer scale or superficial form is essential to realizing LLMs'
full cognitive potential.

</details>


### [130] [The Emergence of Social Science of Large Language Models](https://arxiv.org/abs/2509.24877)
*Xiao Jia,Zhanzhan Zhao*

Main category: cs.AI

TL;DR: 该论文通过对270项研究进行系统综述，构建了一个关于大型语言模型社会科学的计算分类法，识别出三个主要领域：LLM作为社会心智、LLM社会和LLM-人类交互。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的社会科学研究领域分散且缺乏系统性分类，需要构建一个可复现的领域地图来澄清证据标准并促进累积性进展。

Method: 结合文本嵌入、无监督聚类和主题建模的计算分类法，对270项研究进行系统综述。

Result: 识别出三个有机出现的领域：LLM作为社会心智（认知、道德和偏见归因）、LLM社会（多智能体交互和集体认知过程）以及LLM-人类交互（任务重塑、信任和工作影响）。

Conclusion: 该分类法为分散的领域提供了可复现的地图，澄清了不同分析层次的证据标准，并突出了人工智能社会科学中累积性进展的机会。

Abstract: The social science of large language models (LLMs) examines how these systems
evoke mind attributions, interact with one another, and transform human
activity and institutions. We conducted a systematic review of 270 studies,
combining text embeddings, unsupervised clustering and topic modeling to build
a computational taxonomy. Three domains emerge organically across the reviewed
literature. LLM as Social Minds examines whether and when models display
behaviors that elicit attributions of cognition, morality and bias, while
addressing challenges such as test leakage and surface cues. LLM Societies
examines multi-agent settings where interaction protocols, architectures and
mechanism design shape coordination, norms, institutions and collective
epistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,
learning, trust, work and governance, and how risks arise at the human-AI
interface. This taxonomy provides a reproducible map of a fragmented field,
clarifies evidentiary standards across levels of analysis, and highlights
opportunities for cumulative progress in the social science of artificial
intelligence.

</details>


### [131] [MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning](https://arxiv.org/abs/2509.24922)
*Huihao Jing,Wenbin Hu,Hongyu Luo,Jianhui Yang,Wei Fan,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: 提出了MASLegalBench，一个专门为多智能体系统设计的法律基准，使用GDPR作为应用场景，通过演绎推理方法评估MAS在法律任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有法律基准未考虑多智能体系统的独特优势，如任务分解、智能体专业化和灵活训练，这限制了MAS在法律领域的潜力。

Method: 使用GDPR作为应用场景，手动设计基于角色的多智能体系统，并进行广泛实验，采用演绎推理方法构建基准。

Result: 实验结果揭示了现有模型和MAS架构的优势、局限性和改进空间。

Conclusion: MASLegalBench填补了法律领域多智能体系统评估的空白，为MAS在法律任务中的发展提供了重要基准。

Abstract: Multi-agent systems (MAS), leveraging the remarkable capabilities of Large
Language Models (LLMs), show great potential in addressing complex tasks. In
this context, integrating MAS with legal tasks is a crucial step. While
previous studies have developed legal benchmarks for LLM agents, none are
specifically designed to consider the unique advantages of MAS, such as task
decomposition, agent specialization, and flexible training. In fact, the lack
of evaluation methods limits the potential of MAS in the legal domain. To
address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS
and designed with a deductive reasoning approach. Our benchmark uses GDPR as
the application scenario, encompassing extensive background knowledge and
covering complex reasoning processes that effectively reflect the intricacies
of real-world legal situations. Furthermore, we manually design various
role-based MAS and conduct extensive experiments using different
state-of-the-art LLMs. Our results highlight the strengths, limitations, and
potential areas for improvement of existing models and MAS architectures.

</details>


### [132] [Agentic Exploration of Physics Models](https://arxiv.org/abs/2509.24978)
*Maximilian Nägele,Florian Marquardt*

Main category: cs.AI

TL;DR: SciExplorer是一个基于大语言模型的科学探索代理，能够通过代码执行等工具自主探索未知物理系统，无需特定领域蓝图即可恢复运动方程和哈密顿量。


<details>
  <summary>Details</summary>
Motivation: 科学发现过程需要观察、分析和假设生成的循环，目前机器学习主要处理单个环节，但完全自动化这种开放式探索循环仍具挑战。

Method: 利用大语言模型的工具使用能力，通过代码执行等最小化工具集，对未知物理系统进行自由形式探索。

Result: 在机械动力学系统、波演化、量子多体物理等广泛模型上表现出色，能够从观测动态恢复运动方程，从期望值推断哈密顿量。

Conclusion: 该方法为其他领域的科学探索开辟了道路，无需微调或任务特定指令。

Abstract: The process of scientific discovery relies on an interplay of observations,
analysis, and hypothesis generation. Machine learning is increasingly being
adopted to address individual aspects of this process. However, it remains an
open challenge to fully automate the open-ended, heuristic, iterative loop
required to discover the laws of an unknown system by exploring it through
experiments and analysis, without tailoring the approach to the specifics of a
given task. Here, we introduce SciExplorer, an agent that leverages large
language model tool-use capabilities to enable free-form exploration of systems
without any domain-specific blueprints, and apply it to the exploration of
physical systems that are initially unknown to the agent. We test SciExplorer
on a broad set of models spanning mechanical dynamical systems, wave evolution,
and quantum many-body physics. Despite using a minimal set of tools, primarily
based on code execution, we observe impressive performance on tasks such as
recovering equations of motion from observed dynamics and inferring
Hamiltonians from expectation values. The demonstrated effectiveness of this
setup opens the door towards similar scientific exploration in other domains,
without the need for finetuning or task-specific instructions.

</details>


### [133] [Scaling Synthetic Task Generation for Agents via Exploration](https://arxiv.org/abs/2509.25047)
*Ram Ramrakhya,Andrew Szot,Omar Attia,Yuhao Yang,Anh Nguyen,Bogdan Mazoure,Zhe Gan,Harsh Agrawal,Alexander Toshev*

Main category: cs.AI

TL;DR: AutoPlay是一个可扩展的任务生成管道，通过探索交互环境来发现可能的交互和状态信息，从而合成环境基础任务，用于训练多模态大语言模型代理。


<details>
  <summary>Details</summary>
Motivation: 现有的任务生成方法严重依赖人工标注或使用有限环境信息提示MLLM，成本高且扩展性差，导致任务覆盖范围有限。

Method: AutoPlay采用两阶段方法：探索阶段让MLLM探索代理系统性地发现新环境状态和功能；任务生成阶段利用探索轨迹和任务指导提示作为上下文，合成多样、可执行且可验证的任务。

Result: AutoPlay在20个Android应用中生成20k任务，在13个Ubuntu应用中生成10k任务，使移动使用和计算机使用代理的成功率分别提高20.0%和10.9%。结合MLLM验证器奖励的强化学习训练带来额外5.7%的增益。

Conclusion: AutoPlay为训练有能力的MLLM代理提供了一种可扩展的后训练方法，减少了对人工标注的依赖。

Abstract: Post-Training Multimodal Large Language Models (MLLMs) to build interactive
agents holds promise across domains such as computer-use, web navigation, and
robotics. A key challenge in scaling such post-training is lack of high-quality
downstream agentic task datasets with tasks that are diverse, feasible, and
verifiable. Existing approaches for task generation rely heavily on human
annotation or prompting MLLM with limited downstream environment information,
which is either costly or poorly scalable as it yield tasks with limited
coverage. To remedy this, we present AutoPlay, a scalable pipeline for task
generation that explicitly explores interactive environments to discover
possible interactions and current state information to synthesize
environment-grounded tasks. AutoPlay operates in two stages: (i) an exploration
phase, where an MLLM explorer agent systematically uncovers novel environment
states and functionalities, and (ii) a task generation phase, where a task
generator leverages exploration trajectories and a set of task guideline
prompts as context to synthesize diverse, executable, and verifiable tasks. We
show AutoPlay generates 20k tasks across 20 Android applications and 10k tasks
across 13 applications Ubuntu applications to train mobile-use and computer-use
agents. AutoPlay generated tasks enable large-scale task demonstration
synthesis without human annotation by employing an MLLM task executor and
verifier. This data enables training MLLM-based UI agents that improve success
rates up to $20.0\%$ on mobile-use and $10.9\%$ on computer-use scenarios. In
addition, AutoPlay generated tasks combined with MLLM verifier-based rewards
enable scaling reinforcement learning training of UI agents, leading to an
additional $5.7\%$ gain. coverage. These results establish AutoPlay as a
scalable approach for post-training capable MLLM agents reducing reliance on
human annotation.

</details>


### [134] [Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning](https://arxiv.org/abs/2509.25052)
*Sai Wang,Yu Wu,Zhongwen Xu*

Main category: cs.AI

TL;DR: 提出Cogito, ergo ludo (CEL)智能体架构，使用大语言模型通过推理和规划来学习环境规则和策略，实现可解释的强化学习。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法依赖大量经验且知识编码不透明，需要更可解释、通过推理学习的方法。

Method: CEL智能体通过交互-反思循环，在每次回合后进行规则归纳和策略总结，构建显式的环境模型和可执行策略手册。

Result: 在多个网格世界任务中成功学习掌握游戏，从稀疏奖励中自主发现规则并开发有效策略。

Conclusion: 展示了通过显式推理构建透明世界模型的通用可解释智能体路径。

Abstract: The pursuit of artificial agents that can learn to master complex
environments has led to remarkable successes, yet prevailing deep reinforcement
learning methods often rely on immense experience, encoding their knowledge
opaquely within neural network weights. We propose a different paradigm, one in
which an agent learns to play by reasoning and planning. We introduce Cogito,
ergo ludo (CEL), a novel agent architecture that leverages a Large Language
Model (LLM) to build an explicit, language-based understanding of its
environment's mechanics and its own strategy. Starting from a tabula rasa state
with no prior knowledge (except action set), CEL operates on a cycle of
interaction and reflection. After each episode, the agent analyzes its complete
trajectory to perform two concurrent learning processes: Rule Induction, where
it refines its explicit model of the environment's dynamics, and Strategy and
Playbook Summarization, where it distills experiences into an actionable
strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,
Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent
successfully learns to master these games by autonomously discovering their
rules and developing effective policies from sparse rewards. Ablation studies
confirm that the iterative process is critical for sustained learning. Our work
demonstrates a path toward more general and interpretable agents that not only
act effectively but also build a transparent and improving model of their world
through explicit reasoning on raw experience.

</details>


### [135] [HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis](https://arxiv.org/abs/2509.25112)
*Yiquan Wang,Tin-Yeh Huang,Qingyun Gao,Jialin Zhang*

Main category: cs.AI

TL;DR: HeDA是一个用于热浪风险发现的多智能体系统，通过构建知识图谱和多层风险传播分析，自动发现科学文献中未被识别的风险传播路径。


<details>
  <summary>Details</summary>
Motivation: 热浪在气候、社会和经济系统中造成复杂的级联风险，但科学文献中的知识碎片化阻碍了对这些风险路径的全面理解。

Method: 处理10,247篇学术论文构建包含23,156个节点和89,472个关系的知识图谱，采用多层风险传播分析识别风险传播路径。

Result: 在复杂问答任务上达到78.9%准确率，比GPT-4高13.7%；发现了5个先前未识别的高影响风险链，并通过历史案例和专家验证。

Conclusion: 这项工作提出了AI驱动科学发现的新范式，为制定更具韧性的气候适应策略提供了可行见解。

Abstract: Heatwaves pose complex cascading risks across interconnected climate, social,
and economic systems, but knowledge fragmentation in scientific literature
hinders comprehensive understanding of these risk pathways. We introduce HeDA
(Heatwave Discovery Agent), an intelligent multi-agent system designed for
automated scientific discovery through knowledge graph construction and
multi-layer risk propagation analysis. HeDA processes over 10,247 academic
papers to construct a comprehensive knowledge graph with 23,156 nodes and
89,472 relationships, employing novel multi-layer risk propagation analysis to
systematically identify overlooked risk transmission pathways. Our system
achieves 78.9% accuracy on complex question-answering tasks, outperforming
state-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA
successfully discovered five previously unidentified high-impact risk chains,
such as the pathway where a heatwave leads to a water demand surge, resulting
in industrial water restrictions and ultimately causing small business
disruption, which were validated through historical case studies and domain
expert review. This work presents a new paradigm for AI-driven scientific
discovery, providing actionable insights for developing more resilient climate
adaptation strategies.

</details>


### [136] [From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones](https://arxiv.org/abs/2509.25123)
*Lifan Yuan,Weize Chen,Yuchen Zhang,Ganqu Cui,Hanbin Wang,Ziming You,Ning Ding,Zhiyuan Liu,Maosong Sun,Hao Peng*

Main category: cs.AI

TL;DR: RL确实能让LLMs学习真正的新技能，特别是通过组合现有技能来获得组合能力，这类似于人类获得新认知技能的方式。


<details>
  <summary>Details</summary>
Motivation: 探讨RL在LLM后训练中是否真正教授新技能，还是仅仅激活现有技能，解决关于RL作用的争议。

Method: 开发合成框架，定义技能为推断字符串转换函数输出的能力，研究LLM在RL训练后是否能学习未见过函数组合。

Result: RL使LLM能够学习未见过的函数组合，这种组合能力能泛化到更复杂问题，并能迁移到不同任务。

Conclusion: RL能从根本上改变模型的推理行为，建议先构建具有基本技能的基座模型，然后用RL激励高级、可泛化的复杂问题解决技能。

Abstract: Does RL teach LLMs genuinely new skills, or does it merely activate existing
ones? This question lies at the core of ongoing debates about the role of RL in
LLM post-training. On one side, strong empirical results can be achieved with
RL even without preceding supervised finetuning; on the other, critics argue
that RL contributes little beyond reweighting existing reasoning strategies.
This work provides concrete evidence that LLMs can acquire genuinely new skills
during RL by composing existing ones, mirroring one of the central mechanisms
by which humans acquire new cognitive skills. To mitigate data contamination
and other confounding factors, and to allow precise control over task
complexity, we develop a synthetic framework for our investigation.
Specifically, we define a skill as the ability to infer the output of a string
transformation function f(x) given x. When an LLM has already learned f and g
prior to RL, our experiments reveal that RL enables it to learn unseen
compositions of them h(x)=g(f(x)). Further, this compositional ability
generalizes to more difficult problems such as compositions of >2 functions
unseen during RL training. Surprisingly, our experiments show that
compositional skill acquired on a source task transfers to a different target
task. This transfer happens even without compositional training on the target,
requiring only prior knowledge of the target's atomic skills. Our qualitative
analysis shows that RL fundamentally changes the reasoning behaviors of the
models. In contrast, next-token training with the same data yields none of
these findings. Our systematic experiments provide fresh insights into LLM
learning, suggesting the value of first building base models with basic skills,
then using RL to incentivize advanced, generalizable skills for complex
problems.

</details>


### [137] [Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs](https://arxiv.org/abs/2509.25139)
*Yue Zhang,Tianyi Ma,Zun Wang,Yanyuan Qiao,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 提出了一种通过多视角文本描述和类比推理来增强零样本LLM视觉语言导航代理的上下文理解能力的方法


<details>
  <summary>Details</summary>
Motivation: 现有的零样本LLM视觉语言导航代理要么将图像编码为文本场景描述（可能过度简化视觉细节），要么处理原始图像输入（可能无法捕获高级推理所需的抽象语义）

Method: 通过整合多视角的文本描述来促进图像间的类比推理，利用基于文本的类比推理增强全局场景理解和空间推理

Result: 在R2R数据集上的实验表明导航性能显著提升

Conclusion: 多视角文本描述和类比推理能有效增强导航代理的上下文理解能力

Abstract: Integrating large language models (LLMs) into embodied AI models is becoming
increasingly prevalent. However, existing zero-shot LLM-based
Vision-and-Language Navigation (VLN) agents either encode images as textual
scene descriptions, potentially oversimplifying visual details, or process raw
image inputs, which can fail to capture abstract semantics required for
high-level reasoning. In this paper, we improve the navigation agent's
contextual understanding by incorporating textual descriptions from multiple
perspectives that facilitate analogical reasoning across images. By leveraging
text-based analogical reasoning, the agent enhances its global scene
understanding and spatial reasoning, leading to more accurate action decisions.
We evaluate our approach on the R2R dataset, where our experiments demonstrate
significant improvements in navigation performance.

</details>


### [138] [ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory](https://arxiv.org/abs/2509.25140)
*Siru Ouyang,Jun Yan,I-Hung Hsu,Yanfei Chen,Ke Jiang,Zifeng Wang,Rujun Han,Long T. Le,Samira Daruki,Xiangru Tang,Vishy Tirumalashetty,George Lee,Mahsan Rofouei,Hangfei Lin,Jiawei Han,Chen-Yu Lee,Tomas Pfister*

Main category: cs.AI

TL;DR: 提出了ReasoningBank记忆框架，从智能体的成功和失败经验中提炼可泛化的推理策略，并引入内存感知测试时扩展(MaTTS)来加速学习过程。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型智能体在持续任务流中无法从交互历史中学习的问题，避免重复过去的错误。

Method: ReasoningBank框架从智能体的自我评估成功和失败经验中提取推理策略，在测试时检索相关记忆来指导交互；MaTTS通过扩展智能体的交互经验来加速学习过程。

Result: 在网络浏览和软件工程基准测试中，ReasoningBank始终优于存储原始轨迹或仅成功任务例程的现有记忆机制，提高了有效性和效率；MaTTS进一步放大了这些收益。

Conclusion: 内存驱动的经验扩展成为一种新的扩展维度，使智能体能够自我进化并自然产生涌现行为。

Abstract: With the growing adoption of large language model agents in persistent
real-world roles, they naturally encounter continuous streams of tasks. A key
limitation, however, is their failure to learn from the accumulated interaction
history, forcing them to discard valuable insights and repeat past errors. We
propose ReasoningBank, a novel memory framework that distills generalizable
reasoning strategies from an agent's self-judged successful and failed
experiences. At test time, an agent retrieves relevant memories from
ReasoningBank to inform its interaction and then integrates new learnings back,
enabling it to become more capable over time. Building on this powerful
experience learner, we further introduce memory-aware test-time scaling
(MaTTS), which accelerates and diversifies this learning process by scaling up
the agent's interaction experience. By allocating more compute to each task,
the agent generates abundant, diverse experiences that provide rich contrastive
signals for synthesizing higher-quality memory. The better memory in turn
guides more effective scaling, establishing a powerful synergy between memory
and test-time scaling. Across web browsing and software engineering benchmarks,
ReasoningBank consistently outperforms existing memory mechanisms that store
raw trajectories or only successful task routines, improving both effectiveness
and efficiency; MaTTS further amplifies these gains. These findings establish
memory-driven experience scaling as a new scaling dimension, enabling agents to
self-evolve with emergent behaviors naturally arise.

</details>


### [139] [UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following](https://arxiv.org/abs/2509.25148)
*FaQiang Qian,WeiKun Zhang,Ziliang Wang,Kang An,Xuhui Zheng,Liangjian Wen,Mengya Gao,Yong Dai,Yichao Wu*

Main category: cs.AI

TL;DR: 提出UniAPL统一对抗偏好学习框架，将后训练对齐视为约束优化问题，通过单阶段训练同时利用演示偏好和比较偏好数据，解决传统SFT+RL流程中的分布不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 传统SFT后接RL的对齐方法存在关键分布不匹配问题：SFT使用静态专家数据，但策略演化时生成分布会漂移，使得SFT知识变得脆弱。后续RL探索时无法直接访问专家演示中的丰富真实知识，导致效率低下和缺乏基础的更新。

Method: 提出UniAPL框架，将对齐重新定义为约束优化问题，实现单阶段统一训练目标，从混合的SFT和偏好数据批次中联合学习。在每个梯度步骤中，密集的专家演示直接基础和规范化在线探索。

Result: 在指令跟随任务上评估UniAPL，使用Qwen3-235B-Instruct-2507作为教师模型。UniAPL模型匹配或超越强GRPO基线：Qwen3-0.6B提升5.77%（匹配32B模型），Qwen3-4B提升3.75%，甚至超过教师模型。

Conclusion: UniAPL通过统一训练框架有效解决了分布不匹配问题，实现了更强的性能和更好的行为对齐，输出更接近专家演示。

Abstract: Shaping powerful LLMs to be beneficial and safe is central to AI alignment.
We argue that post-training alignment is fundamentally a unified Preference
Learning problem, involving two modalities: demonstrated preferences (e.g.,
Supervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement
Learning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due
to a critical distributional mismatch: SFT uses static expert data, but as the
policy evolves, its generation distribution drifts, making SFT knowledge
brittle. Subsequent RL then explores without direct access to the rich,
ground-truth knowledge in expert demonstrations, leading to inefficient,
ungrounded updates. This separation prevents mutual regularization between data
sources. To address this, we reframe alignment as a constrained optimization
problem and propose Unified Adversarial Preference Learning (UniAPL),a novel
framework that dynamically aligns the policy's distribution with the expert's.
UniAPL implements a single-stage unified training objective, jointly learning
from mixed batches of SFT and preference data. In every gradient step, dense
expert demonstrations directly ground and regularize online exploration,
inherently resolving distributional mismatch and maximizing data synergy.We
evaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507
as the teacher. Our models match or exceed strong GRPO baselines: +5.77% on
Qwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the
teacher. Analyses of response length and log-probability distributions confirm
that UniAPL outputs closely mimic expert demonstrations, achieving both
stronger performance and better behavioral alignment.

</details>


### [140] [Who's Your Judge? On the Detectability of LLM-Generated Judgments](https://arxiv.org/abs/2509.25154)
*Dawei Li,Zhen Tan,Chengshuai Zhao,Bohan Jiang,Baixiang Huang,Pingchuan Ma,Abdullah Alnaibari,Kai Shu,Huan Liu*

Main category: cs.AI

TL;DR: 提出了判断检测任务，研究LLM生成判断的可检测性，并开发了J-Detector检测器，通过显式提取语言和LLM增强特征来准确检测LLM生成的判断。


<details>
  <summary>Details</summary>
Motivation: LLM生成的判断存在固有偏见和脆弱性，在学术评审等敏感场景中需要区分这些判断，但现有LLM生成文本检测方法无法有效处理判断检测任务。

Method: 开发J-Detector轻量级神经网络检测器，通过显式提取语言特征和LLM增强特征，将LLM法官的偏见与候选内容属性联系起来进行检测。

Result: 在多个数据集上的实验证明J-Detector的有效性，其可解释性能够量化LLM法官的偏见，并验证了判断检测在现实场景中的实用性。

Conclusion: LLM生成的判断是可检测的，J-Detector提供了一种有效的检测方法，并揭示了影响检测能力的关键因素。

Abstract: Large Language Model (LLM)-based judgments leverage powerful LLMs to
efficiently evaluate candidate content and provide judgment scores. However,
the inherent biases and vulnerabilities of LLM-generated judgments raise
concerns, underscoring the urgent need for distinguishing them in sensitive
scenarios like academic peer reviewing. In this work, we propose and formalize
the task of judgment detection and systematically investigate the detectability
of LLM-generated judgments. Unlike LLM-generated text detection, judgment
detection relies solely on judgment scores and candidates, reflecting
real-world scenarios where textual feedback is often unavailable in the
detection process. Our preliminary analysis shows that existing LLM-generated
text detection methods perform poorly given their incapability to capture the
interaction between judgment scores and candidate content -- an aspect crucial
for effective judgment detection. Inspired by this, we introduce
\textit{J-Detector}, a lightweight and transparent neural detector augmented
with explicitly extracted linguistic and LLM-enhanced features to link LLM
judges' biases with candidates' properties for accurate detection. Experiments
across diverse datasets demonstrate the effectiveness of \textit{J-Detector}
and show how its interpretability enables quantifying biases in LLM judges.
Finally, we analyze key factors affecting the detectability of LLM-generated
judgments and validate the practical utility of judgment detection in
real-world scenarios.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [141] [US banking giant Citi pilots agentic AI with 5,000 staff](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.theregister.com%2Ffeed%2Fwww.theregister.com%2F2025%2F09%2F24%2Fciti_pilots_agentic_ai%2F%3Futm_source=tldrinfosec/1/0100019986229d41-b27d6c42-a34d-452b-88af-c87df7150522-000000/Ncy3uDb-VtjrqVeooorxTQp6_Mz-XKHlny2ylCRoLT4=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 花旗银行正在5000名员工中试点具有智能代理功能的升级版Stylus Workspaces，使用多种AI模型实现自动化研究、客户画像和多阶段工作流自动化。


<details>
  <summary>Details</summary>
Motivation: 通过部署智能代理AI技术来提高银行员工的生产力，同时探索该技术对人员需求的影响。

Method: 在5000名员工中进行为期六周的试点，使用升级版Stylus Workspaces平台，整合Gemini到Claude等多种AI模型，实现自动化研究、客户画像和多阶段工作流自动化。

Result: 试点正在进行中，花旗CTO承认该技术可能减少人员需求同时提高生产力，但任务成功率仍有30-35%的担忧。

Conclusion: 花旗银行正在积极测试智能代理AI在银行业务中的应用潜力，但技术成熟度仍需验证。

Abstract: US banking giant Citi pilots agentic AI with 5,000 staff (3 minute read) Citi is piloting upgraded Stylus Workspaces with agentic AI capabilities across 5,000 employees for up to six weeks, enabling automated research, customer profiling, and multi-stage workflow automation using various AI models, including those from Gemini to Claude. The bank's CTO acknowledged that the technology could reduce staffing needs while boosting productivity, although concerns remain about the 30-35% task succes...

</details>


### [142] [Coding With AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=Un9a58ciuEI%26utm_source=tldrinfosec/1/0100019986229d41-b27d6c42-a34d-452b-88af-c87df7150522-000000/KdPnuTI2dsCtVr57nrlH1NNWvYy0eETIycbTWgKPjdU=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI编码工具可以提升开发效率但也带来安全风险，Secure Code Warrior的产品经理分享了LLM洞察、提示工程技巧和工具优势


<details>
  <summary>Details</summary>
Motivation: 探讨AI编码工具在提升开发效率的同时如何应对安全挑战，分享实际应用中的最佳实践

Method: 通过Secure Code Warrior的产品经验，分析LLM在编码中的应用，提供提示工程技巧和工具评估

Result: 分享了AI编码工具的实际应用洞察，包括安全风险识别和效率提升方法

Conclusion: AI编码工具具有巨大潜力，但需要正确使用和安全实践来避免安全噩梦

Abstract: Coding With AI (Sponsor) AI coding tools can supercharge your devs…or your security nightmares. Chelle Saunders, Product Manager at Secure Code Warrior, shares LLM insights, prompt engineering tips, and tool strengths. Watch now

</details>


### [143] [Is OpenAI's Reinforcement Fine-Tuning Worth It?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tensorzero.com%2Fblog%2Fis-openai-reinforcement-fine-tuning-rft-worth-it%2F%3Futm_source=tldrai/1/010001998641b019-12c28e95-1c78-45c0-98ab-fe4f71dfe793-000000/aV_mfuDZJQXKSD22OG-Hrz-uAvl12ytCLmUpmx1dKd4=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI的强化微调(RFT)成本比监督微调高700倍，但仅在智能体编码任务上表现明显优势，提供了灵活的奖励设计配置。


<details>
  <summary>Details</summary>
Motivation: 评估OpenAI强化微调(RFT)技术的实际价值，分析其高昂成本是否值得，以及在哪些任务上能带来显著性能提升。

Method: 通过对比RFT与监督微调的成本效益，分析RFT在不同任务类型上的表现差异，特别关注智能体编码任务的性能提升。

Result: RFT在智能体编码任务上能带来实质性性能提升，但在其他任务类型上优势不明显，成本效益比需要仔细评估。

Conclusion: RFT技术虽然成本高昂，但在特定领域(如智能体编码)确实能带来显著性能改进，工程师需要根据具体应用场景权衡成本与收益。

Abstract: Is OpenAI's Reinforcement Fine-Tuning (RFT) Worth It? (32 minute read) OpenAI's reinforcement fine-tuning (RFT) for o4-mini is supposed to be able to train models to get better at specific tasks using reinforcement learning. It costs up to 700 times more than supervised fine-tuning but only seems to deliver clear wins on agentic coding tasks. The technology gives engineers flexibility in reward design through flexible grader configurations, and the performance gains can be substantial when it...

</details>


### [144] [Gemini Robotics 1.5 brings AI agents into the physical world](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeepmind.google%2Fdiscover%2Fblog%2Fgemini-robotics-15-brings-ai-agents-into-the-physical-world%2F%3Futm_source=tldrai/1/010001998641b019-12c28e95-1c78-45c0-98ab-fe4f71dfe793-000000/HgAVovTFT196180Q89dMwmPeiMgYPu3mIRj16TiYUMg=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google DeepMind发布Gemini Robotics 1.5和Gemini Robotics-ER 1.5两个模型，将AI代理带入物理世界，实现视觉信息到机器人动作的转换和推理能力。


<details>
  <summary>Details</summary>
Motivation: 将AI代理从数字世界扩展到物理世界，让机器人能够理解视觉信息和指令并执行任务，实现真正的物理世界交互。

Method: 使用视觉-语言-动作模型（Gemini Robotics 1.5）将视觉信息和指令转换为机器人运动命令，以及视觉-语言模型（Gemini Robotics-ER 1.5）进行推理，模型具有思考后再行动的能力。

Result: 模型能够将视觉输入和指令转换为机器人可执行的动作命令，展示思考过程，并支持跨具身学习。

Conclusion: 这些模型为AI代理在物理世界中的部署提供了重要基础，展示了从感知到行动的完整能力。

Abstract: Gemini Robotics 1.5 brings AI agents into the physical world (15 minute read) Google DeepMind has released two models that unlock agentic experiences with advanced thinking. Gemini Robotics 1.5 is a vision-language-action model that turns visual information and instructions into motor commands for a robot to perform a task. The model thinks before taking actions and shows its process and also learns across embodiments. Gemini Robotics-ER 1.5 is a vision-language model that reasons about the p...

</details>


### [145] [Build and test AI agents without leaving your IDE](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flabs.amazon.science%2Fblog%2Fnova-act-extension-build-and-test-ai-agents-without-leaving-your-ide%3Futm_campaign=nova-act-extension-build-and-test-ai-agents-without-leaving-your-ide%26utm_medium=employer-brand%26utm_source=newsletter%26utm_content=nova-act-extension-build-and-test-ai-agents-without-leaving-your-ide%26utm_term=2025-september/1/010001998641b019-12c28e95-1c78-45c0-98ab-fe4f71dfe793-000000/q1Q98KSHTM2OGwHnmfT_tdhCE_e5bUnL2EyPCZ-qyWw=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Amazon Nova Act扩展将AI代理开发直接集成到IDE中，通过聊天生成脚本、逐单元测试和实时日志调试，将开发时间从几天缩短到几分钟。


<details>
  <summary>Details</summary>
Motivation: 简化AI代理开发流程，让开发者无需离开IDE环境就能完成构建和测试，提高开发效率。

Method: 在Cursor、VS Code和Kiro等IDE中集成AI代理开发功能，支持通过聊天生成脚本、逐单元测试和实时日志调试。

Result: 显著减少AI代理开发时间，从几天缩短到几分钟，提升开发效率。

Conclusion: IDE集成式AI代理开发工具能够极大简化开发流程，提高生产力。

Abstract: Build and test AI agents without leaving your IDE (Sponsor) Amazon's Nova Act extension brings AI agent development directly into IDEs like Cursor, VS Code, and Kiro. Generate scripts via chat, test cell-by-cell, and debug with live logs—cutting development time from days to minutes.Transform your workflow →

</details>


### [146] [Code Mode: the better way to use MCP](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fcode-mode%2F%3Futm_source=tldrdevops/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/gAr8JsDCsAPqNIZhJHE7hMbrBrXB6wtW8vAl2xNe4Co=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 通过将MCP工具转换为TypeScript API并让LLM编写代码调用该API，AI代理可以处理更多工具和更复杂的工具。该方法使用Cloudflare Workers平台及其新的Worker Loader API，在安全沙箱中按需加载Worker代码。


<details>
  <summary>Details</summary>
Motivation: 使AI代理能够处理更多工具和更复杂工具，同时确保安全性和API密钥保护。

Method: 将Model Context Protocol工具转换为TypeScript API，利用Cloudflare Workers平台和Worker Loader API在安全沙箱中按需加载代码。

Result: 成功实现了AI代理对更多工具的支持，同时通过隔离MCP服务器访问和防止API密钥泄露确保了安全性。

Conclusion: Code Mode方法为AI代理使用MCP提供了一种更安全、更强大的方式。

Abstract: Code Mode: the better way to use MCP (10 minute read) AI agents can handle more tools and more complex tools by converting Model Context Protocol (MCP) tools into a TypeScript API and having LLMs write code to call that API. This approach uses the Cloudflare Workers platform and its new Worker Loader API, which loads Worker code on-demand with a secure sandbox that prohibits internet access. It isolates access to MCP servers through bindings and prevents the AI from leaking API keys.

</details>


### [147] [AI writes code in seconds. mirrord tackles the new bottleneck: testing.](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2Fai%2F%3Futm_source=tldrdevops%26utm_medium=tldrdevops%26utm_campaign=tldrdevops_sept29/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/xfj4z0dN21fHda7GLpeZs68PAnr1LjJVrnQSgUz3lyg=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: mirrord解决了AI生成代码的测试瓶颈，让本地代码可以无缝访问实时测试环境，无需模拟等待


<details>
  <summary>Details</summary>
Motivation: AI能快速生成代码，但测试代码与云资源的集成仍然耗时，需要更快的集成测试方法

Method: 开发mirrord工具，让本地运行的AI生成代码能够直接访问实时测试环境，无需模拟或等待

Result: 实现了即时集成测试，大幅缩短了测试时间

Conclusion: mirrord有效解决了AI代码生成的测试瓶颈，提供了高效的集成测试解决方案

Abstract: AI writes code in seconds. mirrord tackles the new bottleneck: testing. (Sponsor) AI agents can generate new code instantly, but testing it against cloud resources such as other microservices, databases, and queues still takes too long. mirrord lets AI-generated code run locally with seamless access to your live staging environment. No mocks, no waiting, just instant integration testing. Try mirrord for free

</details>


### [148] [Coze Studio](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcoze-dev%2Fcoze-studio%3Futm_source=tldrdevops/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/8SnYMmWV7HarePi9UFreJbb-xx1YgDTRbWwrKUJkxeQ=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Coze Studio是一个开源的AI智能体开发工具，提供可视化界面、无代码/低代码方法和微服务架构，简化AI智能体的创建、调试和部署。


<details>
  <summary>Details</summary>
Motivation: 旨在降低AI智能体开发的技术门槛，使非专业开发者也能轻松创建和部署AI智能体。

Method: 采用基于Golang、React和Typescript的微服务架构，提供可视化工具和无代码/低代码开发方式。

Result: 成功开发出Coze Studio平台，实现了AI智能体的简化开发流程。

Conclusion: Coze Studio作为开源工具，有效促进了AI智能体开发的普及和易用性。

Abstract: Coze Studio (GitHub Repo) Coze Studio, an all-in-one AI agent development tool derived from the "Coze Development Platform," has been released as open source. The platform simplifies AI agent creation, debugging, and deployment with visual tools, no-code or low-code approaches, and a microservices architecture built on Golang, React, and Typescript.

</details>


### [149] [Claude Code Router](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmusistudio%2Fclaude-code-router%3Futm_source=tldrdevops/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/1LgnEQZ3HGycWWLl3q8zKTXWyM4at0DMmnFNLR1AVNs=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code Router v1.50发布，支持通过iFlow平台免费路由Claude Code请求到GLM-4.5和DeepSeek v3.1模型


<details>
  <summary>Details</summary>
Motivation: 提供更灵活和免费的代码生成模型路由方案，让用户能够选择不同的AI模型来处理代码请求

Method: 开发Claude Code Router工具，通过iFlow平台实现模型路由功能，支持GLM-4.5和DeepSeek v3.1等模型

Result: 成功发布v1.0.50版本，用户现在可以免费使用多个模型进行代码请求路由

Conclusion: 该工具为开发者提供了更多模型选择，降低了代码生成成本，提升了灵活性

Abstract: Claude Code Router (GitHub Repo) Claude Code Router version v1.0.50 is now available. Users can now route Claude Code requests through models like GLM-4.5 and DeepSeek v3.1 for free via the iFlow Platform.

</details>


### [150] [The AI coding trap](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fchrisloy.dev%2Fpost%2F2025%2F09%2F28%2Fthe-ai-coding-trap%3Futm_source=tldrwebdev/1/01000199952972ae-ead1a5d5-ad93-4e52-8acf-f9b311fb32cd-000000/GW9SO-941Jn7RodgKz8fQBX-SMtttU5rcKbRD42xXUg=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 过度依赖AI编程助手采用'先写代码后问问题'的方法会导致调试和集成AI生成代码的时间增加，反而降低整体生产力


<details>
  <summary>Details</summary>
Motivation: 探讨AI编程助手使用中的陷阱，分析'代码优先'方法的负面影响

Method: 将AI助手视为快速但经验不足的初级工程师来对待，改变使用策略

Result: 发现过度使用AI编程助手反而增加调试和集成时间，降低效率

Conclusion: 应该调整AI助手的使用方式，避免'代码优先'的陷阱

Abstract: The AI coding trap (10 minute read) The "code first, ask questions later" approach when using AI coding agents too much leads to increased time spent on debugging and integrating AI-generated code, reducing overall productivity. Instead, treat AI agents like fast but inexperienced junior engineers.

</details>


### [151] [Why We Think](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flilianweng.github.io%2Fposts%2F2025-05-01-thinking%2F%3Futm_source=tldrwebdev/1/01000199952972ae-ead1a5d5-ad93-4e52-8acf-f9b311fb32cd-000000/J5F4qFD1021V32SZL8p9xari2qoL2htaNSJRQPuMCQo=424)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 增加测试时计算（"思考时间"）可以显著提升LLM在复杂推理任务中的表现，本文讨论了链式思维提示、并行采样、顺序修订等技术来利用思考时间，同时探讨了强化学习方法以及使模型提供忠实且人类可读推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过增加测试时计算资源来提升大型语言模型在复杂推理任务中的性能，解决当前模型在推理能力方面的局限性。

Method: 采用链式思维提示、并行采样、顺序修订等技术来有效利用增加的思考时间，并探讨强化学习方法在这些场景中的应用。

Result: 研究表明增加思考时间可以显著改善LLM在复杂推理任务中的表现，但需要解决推理过程的忠实性和可读性问题。

Conclusion: 测试时计算资源的增加是提升LLM推理能力的有效途径，但需要开发更好的技术来确保推理过程的透明度和可靠性。

Abstract: Why We Think (41 minute read) Increasing test-time compute, or "thinking time," can greatly improve LLM performance, especially in complex reasoning tasks. This article discusses techniques like chain-of-thought prompting, parallel sampling, and sequential revision to use this thinking time. Reinforcement learning approaches are also discussed, along with the challenge of making models provide faithful and human-readable reasoning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [152] [Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective](https://arxiv.org/abs/2509.22921)
*Matthieu Zimmer,Xiaotong Ji,Tu Nguyen,Haitham Bou Ammar*

Main category: cs.LG

TL;DR: 提出了一种基于约束强化学习的大语言模型蒸馏方法，在最大化任务特定奖励的同时约束与教师模型的差异不超过指定阈值，无需状态增强或教师模型访问即可保证约束满足。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏方法通常依赖临时奖励加权，缺乏理论保证。需要一种既能利用任务特定奖励又能控制与教师模型差异的蒸馏框架。

Method: 将LLM蒸馏构建为约束强化学习问题，采用改进的奖励函数，在蒸馏过程中最大化任务奖励同时约束与教师模型的KL散度不超过阈值。

Result: 在数学推理任务上的实验表明，该方法比软拉格朗日松弛基线具有更好的约束满足率和推理能力，同时保持竞争力的任务性能。

Conclusion: 该框架为资源受限环境下的奖励感知蒸馏提供了理论基础和实际高效的解决方案。

Abstract: We introduce a novel approach to large language model (LLM) distillation by
formulating it as a constrained reinforcement learning problem. While recent
work has begun exploring the integration of task-specific rewards into
distillation processes, existing methods typically rely on ad-hoc reward
weighting. We propose a principled optimization framework that maximizes
task-specific rewards while constraining the divergence from the teacher model
to remain below a specified threshold. Our approach adapts constrained state
augmented reinforcement learning to the distillation setting, introducing a
modified reward function that maintains theoretical guarantees of constraint
satisfaction without requiring state augmentation or teacher model access
during deployment and without the computational overhead of the dual Lagrangian
methods. Through extensive experiments on mathematical reasoning tasks, we
demonstrate that our method achieves better constraint satisfaction rates and
better reasoning compared to the soft Lagrangian relaxation baselines while
maintaining competitive task performance. Our framework provides a
theoretically grounded and practically efficient solution for reward-aware
distillation in resource-constrained settings.

</details>


### [153] [Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces](https://arxiv.org/abs/2509.22963)
*Haitong Ma,Ofir Nabati,Aviv Rosenberg,Bo Dai,Oran Lang,Idan Szpektor,Craig Boutilier,Na Li,Shie Mannor,Lior Shani,Guy Tenneholtz*

Main category: cs.LG

TL;DR: 提出了一种使用离散扩散模型作为策略的新框架，用于解决强化学习在大规模组合动作空间中的扩展问题，通过策略镜像下降和分布匹配实现稳定训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习在处理现实世界中常见的大规模组合动作空间时面临扩展困难，需要更有效的策略表示和训练方法。

Method: 使用离散扩散模型作为策略，通过策略镜像下降定义正则化目标策略分布，将策略更新转化为分布匹配问题，实现解耦的稳定训练。

Result: 在DNA序列生成、宏动作强化学习和多智能体系统等多个具有挑战性的组合基准测试中取得了最先进的结果和优异的样本效率。

Conclusion: 扩散策略相比其他基线方法表现出更优越的性能，为解决大规模组合动作空间的强化学习问题提供了有效解决方案。

Abstract: Reinforcement learning (RL) struggles to scale to large, combinatorial action
spaces common in many real-world problems. This paper introduces a novel
framework for training discrete diffusion models as highly effective policies
in these complex settings. Our key innovation is an efficient online training
process that ensures stable and effective policy improvement. By leveraging
policy mirror descent (PMD) to define an ideal, regularized target policy
distribution, we frame the policy update as a distributional matching problem,
training the expressive diffusion model to replicate this stable target. This
decoupled approach stabilizes learning and significantly enhances training
performance. Our method achieves state-of-the-art results and superior sample
efficiency across a diverse set of challenging combinatorial benchmarks,
including DNA sequence generation, RL with macro-actions, and multi-agent
systems. Experiments demonstrate that our diffusion policies attain superior
performance compared to other baselines.

</details>


### [154] [Functional Critic Modeling for Provably Convergent Off-Policy Actor-Critic](https://arxiv.org/abs/2509.22964)
*Qinxun Bai,Yuxuan Han,Wei Xu,Zhengyuan Zhou*

Main category: cs.LG

TL;DR: 提出了一种新的功能性评论家建模概念，为致命三角设置下的行动者-评论家学习解决了两个关键挑战，并提供了理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决离策略强化学习中行动者-评论家框架面临的两个主要挑战：评论家学习中的'移动目标'问题和行动者学习中的离策略梯度估计困难。

Method: 引入了功能性评论家建模概念，提出了新的行动者-评论家框架，在线性函数设置下进行理论分析，并设计了神经网络架构进行实验验证。

Result: 在DeepMind控制基准测试的广泛RL任务上进行了初步实验，证明了该方法的有效性。

Conclusion: 这是第一个可证明收敛的离策略目标型行动者-评论家算法，为致命三角设置下的强化学习提供了新的解决方案。

Abstract: Off-policy reinforcement learning (RL) with function approximation offers an
effective way to improve sample efficiency by reusing past experience. Within
this setting, the actor-critic (AC) framework has achieved strong empirical
success. However, both the critic and actor learning is challenging for the
off-policy AC methods: first of all, in addition to the classic "deadly triad"
instability of off-policy evaluation, it also suffers from a "moving target"
problem, where the policy being evaluated changes continually; secondly, actor
learning becomes less efficient due to the difficulty of estimating the exact
off-policy policy gradient. The first challenge essentially reduces the problem
to repeatedly performing off-policy evaluation for changing policies. For the
second challenge, the off-policy policy gradient theorem requires a complex and
often impractical algorithm to estimate an additional emphasis critic, which is
typically neglected in practice, thereby reducing to the on-policy policy
gradient as an approximation. In this work, we introduce a novel concept of
functional critic modeling, which leads to a new AC framework that addresses
both challenges for actor-critic learning under the deadly triad setting. We
provide a theoretical analysis in the linear function setting, establishing the
provable convergence of our framework, which, to the best of our knowledge, is
the first convergent off-policy target-based AC algorithm. From a practical
perspective, we further propose a carefully designed neural network
architecture for the functional critic modeling and demonstrate its
effectiveness through preliminary experiments on widely used RL tasks from the
DeepMind Control Benchmark.

</details>


### [155] [C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning](https://arxiv.org/abs/2509.23129)
*Haotian Liu,Shuo Wang,Hongteng Xu*

Main category: cs.LG

TL;DR: 提出C²GSPG方法解决强化学习中过度自信问题，通过置信度校准和组序列策略梯度框架提升推理性能


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法如GRPO存在过度自信问题，阻碍了自感知推理模型的发展

Method: 提出组序列策略梯度(GSPG)框架消除token级偏差，使用归一化序列级概率定义模型置信度，应用交叉熵正则器校准置信度与奖励

Result: 在逻辑和数学推理任务中，C²GSPG在推理准确性和置信度校准方面优于最先进方法

Conclusion: C²GSPG能有效提升推理性能并抑制过度自信，置信度校准正则器与GSPG在二元奖励下具有协作性

Abstract: Reinforcement Learning (RL) methods, exemplified by Group Relative Policy
Optimization (GRPO) and its variants, play a central role in developing
reasoning models. However, these methods often suffer from a critical
overconfidence issue, which prevents them from achieving self-aware reasoning
models. In this study, we propose a simple yet effective confidence-calibration
group sequence policy gradient method, called C$^2$GSPG, which simultaneously
enhances reasoning performance while suppressing overconfidence. In principle,
we propose a Group Sequence Policy Gradient (GSPG) framework for learning
reasoning models, which eliminates the token-level bias commonly appearing in
GRPO and its variants. In this framework, we define the model confidence for
each reasoning problem using the normalized sequence-level probability, and
then apply a cross-entropy regularizer to calibrate the model confidence to the
sequence's reward. We demonstrate that the confidence calibration regularizer
and GSPG are collaborative for binary rewards, as their objectives always share
the same gradient direction. For non-binary rewards, we apply nonlinear reward
normalization and adaptive regularizer clipping, mitigating the potential
conflict between the two objectives. Applying C$^2$GSPG to post-train large
language models in logical and mathematical reasoning tasks, we show its
superiority over state-of-the-art methods in both reasoning accuracy and
confidence calibration. The code of C$^2$GSPG is available at
https://github.com/HaotianLiu123/CCGSPG.

</details>


### [156] [Towards Monotonic Improvement in In-Context Reinforcement Learning](https://arxiv.org/abs/2509.23209)
*Wenhao Zhang,Shao Zhang,Xihuai Wang,Yang Li,Ying Wen*

Main category: cs.LG

TL;DR: 本文提出了上下文价值感知的上下文强化学习（CV-ICRL），通过引入上下文价值来解决现有ICRL方法在测试时无法持续改进的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文强化学习方法在训练时使用单调策略改进数据，但在测试时无法实现持续性能改进。作者发现这是由于上下文模糊性导致的恶性循环。

Method: 提出CV-ICRL方法，在训练阶段引入上下文价值作为显式信号，表示给定当前上下文时理论上可达到的理想性能。提出了两种在训练和测试时估计上下文价值的方法。

Result: 在Dark Room和Minigrid测试平台上的实验表明，CV-ICRL有效缓解了性能下降问题，并在各种任务和环境中提升了ICRL能力。

Conclusion: 上下文价值能够收紧相对于理想单调改进策略的性能差距下界，CV-ICRL成功解决了上下文强化学习中的上下文模糊性问题。

Abstract: In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm
for developing agents that can rapidly adapt to new tasks by leveraging past
experiences as context, without updating their parameters. Recent approaches
train large sequence models on monotonic policy improvement data from online
RL, aiming to a continue improved testing time performance. However, our
experimental analysis reveals a critical flaw: these models cannot show a
continue improvement like the training data during testing time. Theoretically,
we identify this phenomenon as Contextual Ambiguity, where the model's own
stochastic actions can generate an interaction history that misleadingly
resembles that of a sub-optimal policy from the training data, initiating a
vicious cycle of poor action selection. To resolve the Contextual Ambiguity, we
introduce Context Value into training phase and propose Context Value Informed
ICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing
the ideal performance theoretically achievable by a policy given the current
context. As the context expands, Context Value could include more task-relevant
information, and therefore the ideal performance should be non-decreasing. We
prove that the Context Value tightens the lower bound on the performance gap
relative to an ideal, monotonically improving policy. We fruther propose two
methods for estimating Context Value at both training and testing time.
Experiments conducted on the Dark Room and Minigrid testbeds demonstrate that
CV-ICRL effectively mitigates performance degradation and improves overall ICRL
abilities across various tasks and environments. The source code and data of
this paper are available at
https://github.com/Bluixe/towards_monotonic_improvement .

</details>


### [157] [SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts](https://arxiv.org/abs/2509.23232)
*Bingshuai Liu,Ante Wang,Zijun Min,Liang Yao,Haibo Zhang,Yang Liu,Anxiang Zeng,Jinsong Su*

Main category: cs.LG

TL;DR: SPEC-RL通过将推测解码与RL rollout过程集成，利用先前轨迹段作为推测前缀并通过草稿-验证机制扩展，减少冗余生成，在保持策略质量的同时将rollout时间减少2-3倍。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR训练过程受限于计算昂贵的rollout阶段，现有加速方法存在收益递减、引入偏差或忽略迭代间冗余的问题。

Method: 提出SPEC-RL框架，集成推测解码与RL rollout过程，重用先前轨迹段作为推测前缀，通过草稿-验证机制扩展，避免冗余生成同时确保策略一致性。

Result: 在GSM8K、MATH-500、OlympiadBench、MMLU-STEM等数学推理和泛化基准测试中，SPEC-RL将rollout时间减少2-3倍且不损害策略质量。

Conclusion: SPEC-RL作为纯rollout阶段增强，可与主流算法无缝集成，为大规模推理模型的RLVR扩展提供了通用实用路径。

Abstract: Large Language Models (LLMs) increasingly rely on reinforcement learning with
verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning.
However, the training process remains bottlenecked by the computationally
expensive rollout stage. Existing acceleration methods-such as parallelization,
objective- and data-driven modifications, and replay buffers-either incur
diminishing returns, introduce bias, or overlook redundancy across iterations.
We identify that rollouts from consecutive training epochs frequently share a
large portion of overlapping segments, wasting computation. To address this, we
propose SPEC-RL, a novel framework that integrates SPECulative decoding with
the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative
prefixes and extends them via a draft-and-verify mechanism, avoiding redundant
generation while ensuring policy consistency. Experiments on diverse math
reasoning and generalization benchmarks, including GSM8K, MATH-500,
OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout
time by 2-3x without compromising policy quality. As a purely rollout-stage
enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g.,
PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large
reasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL

</details>


### [158] [NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning](https://arxiv.org/abs/2509.23252)
*Raviteja Anantha,Soheil Hor,Teodor Nicola Antoniu,Layne C. Price*

Main category: cs.LG

TL;DR: NanoFlux是一个新颖的对抗性框架，通过生成少于200个示例的针对性训练数据来提升LLM推理能力，在多个领域超越传统微调方法，同时大幅降低计算需求。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法需要大量数据且计算成本高，作者希望开发一种更高效的方法，通过智能合成少量但高质量的训练数据来提升模型推理能力。

Method: 采用对抗性框架，模型交替扮演攻击者和防御者角色，由工具增强的评判者监督，生成包含解释性注释的多步骤问题，针对特定推理能力进行训练数据合成。

Result: 在4B参数模型上，NanoFlux生成的数据相比全基准微调在数学推理上提升5.9%，科学推理提升3.6%，医学推理提升16.6%，同时计算需求减少3-14倍。

Conclusion: 研究表明模型改进可能在于智能合成小型、精确针对的训练数据集，而非单纯扩大数据规模。

Abstract: We present NanoFlux, a novel adversarial framework for generating targeted
training data to improve LLM reasoning, where adversarially-generated datasets
containing fewer than 200 examples outperform conventional fine-tuning
approaches. The framework employs a competitive dynamic between models
alternating as Attacker and Defender, supervised by a tool-augmented Judge,
synthesizing multi-step questions with explanatory annotations that target
specific reasoning capabilities. Fine-tuning a 4B-parameter model on
NanoFlux-generated data yields performance gains across diverse domains
compared to full-benchmark fine-tuning: +5.9% on mathematical reasoning
(GSMHard), +3.6% on scientific reasoning (GenomeBench), and +16.6% on medical
reasoning (MultiMedQA), while reducing computational requirements by 3-14x.
Ablation studies reveal a non-monotonic relationship between dataset
characteristics and model performance, uncovering domain-specific optimal
points for question complexity and reasoning quality. NanoFlux automates
training data generation through embedding-based novelty filtering,
tool-augmented evaluation, and multi-hop reasoning, suggesting that future
model improvements may lie in the intelligent synthesis of small, precisely
targeted training datasets.

</details>


### [159] [Continuous-Time Reinforcement Learning for Asset-Liability Management](https://arxiv.org/abs/2509.23280)
*Yilie Huang*

Main category: cs.LG

TL;DR: 提出了一种基于连续时间强化学习的资产-负债管理新方法，使用线性二次规划结合中期和最终目标，通过模型无关的策略梯度软演员-评论家算法实现资产与负债的动态同步。


<details>
  <summary>Details</summary>
Motivation: 传统资产-负债管理方法难以有效平衡资产与负债，需要一种能够动态同步资产与负债的智能方法，同时兼顾中期和最终目标。

Method: 开发了模型无关的策略梯度软演员-评论家算法，引入演员的自适应探索和评论家的调度探索机制，在连续时间框架下使用线性二次规划。

Result: 在200个随机市场场景中测试，该方法在所有替代策略中获得了更高的平均奖励，具有快速初始收益和持续优异表现。

Conclusion: 该方法优于传统金融策略和现有RL算法，其优势不是来自复杂神经网络或改进的参数估计，而是直接学习最优ALM策略而无需学习环境。

Abstract: This paper proposes a novel approach for Asset-Liability Management (ALM) by
employing continuous-time Reinforcement Learning (RL) with a linear-quadratic
(LQ) formulation that incorporates both interim and terminal objectives. We
develop a model-free, policy gradient-based soft actor-critic algorithm
tailored to ALM for dynamically synchronizing assets and liabilities. To ensure
an effective balance between exploration and exploitation with minimal tuning,
we introduce adaptive exploration for the actor and scheduled exploration for
the critic. Our empirical study evaluates this approach against two enhanced
traditional financial strategies, a model-based continuous-time RL method, and
three state-of-the-art RL algorithms. Evaluated across 200 randomized market
scenarios, our method achieves higher average rewards than all alternative
strategies, with rapid initial gains and sustained superior performance. The
outperformance stems not from complex neural networks or improved parameter
estimation, but from directly learning the optimal ALM strategy without
learning the environment.

</details>


### [160] [LLM Interpretability with Identifiable Temporal-Instantaneous Representation](https://arxiv.org/abs/2509.23323)
*Xiangchen Song,Jiaqi Sun,Zijian Li,Yujia Zheng,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了一种可识别的时间因果表示学习框架，专门针对LLM的高维概念空间，同时捕捉时间延迟和瞬时因果关系，为LLM的可解释性提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有机制可解释性工具如稀疏自编码器缺乏时间依赖建模、瞬时关系表示和理论保证，而因果表示学习方法由于计算效率问题无法扩展到LLM的丰富概念空间。

Method: 引入可识别的时间因果表示学习框架，扩展稀疏自编码器技术，捕捉时间延迟和瞬时因果关系，并在合成数据集上进行验证。

Result: 在匹配现实世界复杂度的合成数据集上证明有效，成功发现LLM激活中有意义的概念关系。

Conclusion: 同时建模时间和瞬时概念关系能够推进LLM的可解释性。

Abstract: Despite Large Language Models' remarkable capabilities, understanding their
internal representations remains challenging. Mechanistic interpretability
tools such as sparse autoencoders (SAEs) were developed to extract
interpretable features from LLMs but lack temporal dependency modeling,
instantaneous relation representation, and more importantly theoretical
guarantees, undermining both the theoretical foundations and the practical
confidence necessary for subsequent analyses. While causal representation
learning (CRL) offers theoretically grounded approaches for uncovering latent
concepts, existing methods cannot scale to LLMs' rich conceptual space due to
inefficient computation. To bridge the gap, we introduce an identifiable
temporal causal representation learning framework specifically designed for
LLMs' high-dimensional concept space, capturing both time-delayed and
instantaneous causal relations. Our approach provides theoretical guarantees
and demonstrates efficacy on synthetic datasets scaled to match real-world
complexity. By extending SAE techniques with our temporal causal framework, we
successfully discover meaningful concept relationships in LLM activations. Our
findings show that modeling both temporal and instantaneous conceptual
relationships advances the interpretability of LLMs.

</details>


### [161] [Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient with Martingale Characterization](https://arxiv.org/abs/2509.23711)
*Ziheng Cheng,Xin Guo,Yufei Zhang*

Main category: cs.LG

TL;DR: 提出连续时间确定性策略梯度方法CT-DDPG，解决离散时间RL算法在连续环境中的时间离散化敏感性问题，提供更稳定的学习和更快收敛。


<details>
  <summary>Details</summary>
Motivation: 虽然离散时间RL理论发展迅速，但许多实际RL应用本质上是连续且复杂的。将离散时间算法扩展到连续时间设置的主要挑战是对时间离散化的敏感性，通常导致稳定性差和收敛慢。

Method: 推导基于优势函数模拟的连续时间策略梯度公式，建立其鞅特性表征，提出CT-DDPG算法，在连续时间环境中实现确定性策略的稳定学习。

Result: 数值实验表明，CT-DDPG算法在具有不同时间离散化和噪声水平的各种控制任务中，相比现有离散时间和连续时间方法，提供了更好的稳定性和更快的收敛速度。

Conclusion: 连续时间确定性策略梯度方法为连续环境中的RL提供了更稳定的学习框架，克服了离散时间算法的时间离散化敏感性问题。

Abstract: The theory of discrete-time reinforcement learning (RL) has advanced rapidly
over the past decades. Although primarily designed for discrete environments,
many real-world RL applications are inherently continuous and complex. A major
challenge in extending discrete-time algorithms to continuous-time settings is
their sensitivity to time discretization, often leading to poor stability and
slow convergence. In this paper, we investigate deterministic policy gradient
methods for continuous-time RL. We derive a continuous-time policy gradient
formula based on an analogue of the advantage function and establish its
martingale characterization. This theoretical foundation leads to our proposed
algorithm, CT-DDPG, which enables stable learning with deterministic policies
in continuous-time environments. Numerical experiments show that the proposed
CT-DDPG algorithm offers improved stability and faster convergence compared to
existing discrete-time and continuous-time methods, across a wide range of
control tasks with varying time discretizations and noise levels.

</details>


### [162] [An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms](https://arxiv.org/abs/2509.23750)
*Li Wang,Sudun,Xingjian Zhang,Wenjun Wu,Lei Huang*

Main category: cs.LG

TL;DR: 本文提出模式感知批量归一化(MA-BN)方法，解决了批量归一化在深度强化学习中因数据非独立同分布和动态分布变化导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 尽管批量归一化在深度学习中有显著优势，但由于深度强化学习中的数据非独立同分布特性和动态分布变化，其在DRL中的应用受到限制。作者认为BN在DRL中仍具有独特优势，特别是其随机性和训练简化能力。

Method: 对离线策略actor-critic算法中BN的使用进行实证研究，分析不同训练和评估模式对性能的影响，识别导致不稳定的失效模式，并提出模式感知批量归一化(MA-BN)方法。

Result: MA-BN在RL设置中加速和稳定训练，扩大有效学习率范围，增强探索能力，并降低整体优化难度。

Conclusion: 通过适当的应用，BN可以适应不断变化的数据分布，提高收敛速度和最终性能，MA-BN为DRL中BN的稳健集成提供了实用建议。

Abstract: Batch Normalization (BN) has played a pivotal role in the success of deep
learning by improving training stability, mitigating overfitting, and enabling
more effective optimization. However, its adoption in deep reinforcement
learning (DRL) has been limited due to the inherent non-i.i.d. nature of data
and the dynamically shifting distributions induced by the agent's learning
process. In this paper, we argue that, despite these challenges, BN retains
unique advantages in DRL settings, particularly through its stochasticity and
its ability to ease training. When applied appropriately, BN can adapt to
evolving data distributions and enhance both convergence speed and final
performance. To this end, we conduct a comprehensive empirical study on the use
of BN in off-policy actor-critic algorithms, systematically analyzing how
different training and evaluation modes impact performance. We further identify
failure modes that lead to instability or divergence, analyze their underlying
causes, and propose the Mode-Aware Batch Normalization (MA-BN) method with
practical actionable recommendations for robust BN integration in DRL
pipelines. We also empirically validate that, in RL settings, MA-BN accelerates
and stabilizes training, broadens the effective learning rate range, enhances
exploration, and reduces overall optimization difficulty. Our code is available
at: https://github.com/monster476/ma-bn.git.

</details>


### [163] [FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents](https://arxiv.org/abs/2509.23803)
*Pramit Saha,Joshua Strong,Divyanshu Mishra,Cheng Ouyang,J. Alison Noble*

Main category: cs.LG

TL;DR: 提出了一个基于智能代理的联邦学习框架FedAgent，旨在解决医疗联邦学习中的实际协调挑战，包括客户端选择、数据预处理、算法选择等，并建立了FedAgentBench基准来评估LLM代理在医疗FL中的自主协调能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的联邦学习部署面临复杂的操作挑战，需要大量人工干预，包括客户端选择、协调、数据预处理、数据标准化和算法选择等。现有FL工作忽视了这些实际协调问题，因此需要自主的、基于代理的FL系统。

Method: 引入了一个代理驱动的FL框架，涵盖从客户端选择到训练完成的真实FL工作流程关键阶段；构建了FedAgentBench基准，包含40种FL算法和201个精心策划的数据集，模拟6种医疗环境；评估了14个开源和10个专有LLM的代理性能。

Result: 虽然一些代理核心如GPT-4.1和DeepSeek V3能够自动化FL管道的各个阶段，但基于隐含目标的更复杂、相互依赖的任务对即使是最强大的模型仍然具有挑战性。

Conclusion: 代理驱动的FL系统有潜力解决医疗联邦学习中的实际协调挑战，但复杂任务的自动化仍需进一步研究。

Abstract: Federated learning (FL) allows collaborative model training across healthcare
sites without sharing sensitive patient data. However, real-world FL deployment
is often hindered by complex operational challenges that demand substantial
human efforts. This includes: (a) selecting appropriate clients (hospitals),
(b) coordinating between the central server and clients, (c) client-level data
pre-processing, (d) harmonizing non-standardized data and labels across
clients, and (e) selecting FL algorithms based on user instructions and
cross-client data characteristics. However, the existing FL works overlook
these practical orchestration challenges. These operational bottlenecks
motivate the need for autonomous, agent-driven FL systems, where intelligent
agents at each hospital client and the central server agent collaboratively
manage FL setup and model training with minimal human intervention. To this
end, we first introduce an agent-driven FL framework that captures key phases
of real-world FL workflows from client selection to training completion and a
benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to
autonomously coordinate healthcare FL. Our framework incorporates 40 FL
algorithms, each tailored to address diverse task-specific requirements and
cross-client characteristics. Furthermore, we introduce a diverse set of
complex tasks across 201 carefully curated datasets, simulating 6
modality-specific real-world healthcare environments, viz., Dermatoscopy,
Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic
performance of 14 open-source and 10 proprietary LLMs spanning small, medium,
and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3
can automate various stages of the FL pipeline, our results reveal that more
complex, interdependent tasks based on implicit goals remain challenging for
even the strongest models.

</details>


### [164] [Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR](https://arxiv.org/abs/2509.23808)
*Fanding Huang,Guanbo Huang,Xiao Fan,Yi He,Xiao Liang,Xiao Chen,Qinting Jiang,Faisal Nadeem Khan,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 本文挑战了RLVR中探索-利用权衡的传统观点，提出这种权衡可能是测量层面的假象。通过转向隐藏状态空间分析，发现探索和利用可以解耦，并开发了VERL方法实现协同增强。


<details>
  <summary>Details</summary>
Motivation: 重新审视RLVR中的探索-利用权衡，认为这种权衡可能不是基本约束而是测量层面的产物，旨在找到同时增强探索和利用能力的方法。

Method: 采用隐藏状态空间分析，使用有效秩(ER)量化探索，并提出其导数ERV和ERA来捕捉利用动态。开发了VERL方法，通过直接塑造RL优势函数实现探索-利用的协同增强。

Result: 实验表明VERL在多种LLM和推理基准测试中取得一致提升，在具有挑战性的Gaokao 2024数据集上实现高达21.4%的绝对准确率提升。

Conclusion: 在隐藏状态层面，探索和利用可以解耦，这为同时增强两种能力创造了机会。VERL通过协同增强原则实现了更好的性能。

Abstract: A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)
interprets recent progress through the lens of an exploration-exploitation
trade-off, a perspective largely shaped by token-level metrics. We re-examine
this perspective, proposing that this perceived trade-off may not be a
fundamental constraint but rather an artifact of the measurement level. To
investigate this, we shift the analysis to the semantically rich hidden-state
space, adopting Effective Rank (ER) to quantify exploration and proposing its
novel first- and second-order derivatives, named Effective Rank Velocity (ERV)
and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our
analysis reveals that at the hidden-state level, exploration and exploitation
could be decoupled (Sec. 4). This finding reveals an opportunity to enhance
both capacities simultaneously. This insight motivates our method,
Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the
principle of synergistic exploration-exploitation enhancement by directly
shaping the RL advantage function. The key innovation is leveraging the
theoretically stable ERA as a predictive meta-controller to create a
synergistic, dual-channel incentive structure. Instead of forcing a trade-off,
VERL prospectively amplifies rewards for exploration to preempt overconfidence
and reinforces exploitative gains to consolidate reasoning. Experiments across
diverse LLMs and reasoning benchmarks show consistent gains, including up to
21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.

</details>


### [165] [Adversarial Diffusion for Robust Reinforcement Learning](https://arxiv.org/abs/2509.23846)
*Daniele Foffano,Alessio Russo,Alexandre Proutiere*

Main category: cs.LG

TL;DR: 提出AD-RRL方法，利用扩散模型训练鲁棒强化学习策略，通过条件采样生成最坏情况轨迹来优化条件风险价值(CVaR)，在标准基准测试中表现出优越的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中模型误差和不确定性的鲁棒性问题，利用扩散模型生成完整轨迹的能力来缓解逐步转移模型的累积误差问题。

Method: 基于CVaR优化与鲁棒RL的联系，引入AD-RRL方法，通过引导扩散过程在训练期间生成最坏情况轨迹来优化累积回报的条件风险价值。

Result: 在标准基准测试中，AD-RRL相比现有鲁棒RL方法实现了更优越的鲁棒性和性能表现。

Conclusion: 扩散模型在鲁棒强化学习中具有显著优势，AD-RRL方法通过条件采样有效提升了策略对环境动态不确定性的鲁棒性。

Abstract: Robustness to modeling errors and uncertainties remains a central challenge
in reinforcement learning (RL). In this work, we address this challenge by
leveraging diffusion models to train robust RL policies. Diffusion models have
recently gained popularity in model-based RL due to their ability to generate
full trajectories "all at once", mitigating the compounding errors typical of
step-by-step transition models. Moreover, they can be conditioned to sample
from specific distributions, making them highly flexible. We leverage
conditional sampling to learn policies that are robust to uncertainty in
environment dynamics. Building on the established connection between
Conditional Value at Risk (CVaR) optimization and robust RL, we introduce
Adversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides
the diffusion process to generate worst-case trajectories during training,
effectively optimizing the CVaR of the cumulative return. Empirical results
across standard benchmarks show that AD-RRL achieves superior robustness and
performance compared to existing robust RL methods.

</details>


### [166] [Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation](https://arxiv.org/abs/2509.23866)
*Pengxiang Li,Zechen Hu,Zirui Shang,Jingrong Wu,Yang Liu,Hui Liu,Zhi Gao,Chenrui Shi,Bofei Zhang,Zihao Zhang,Xiaochuan Shi,Zedong YU,Yuwei Wu,Xinxiao Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.LG

TL;DR: DART是一个解耦的GUI代理强化学习训练框架，通过异步模块协调解决VLM代理在GUI环境中的训练效率问题，在OSWorld基准上取得了42.13%的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决基于视觉语言模型的GUI代理在强化学习训练中面临的两个主要挑战：与GUI环境的多轮交互速度慢，以及高质量代理-环境交互数据不足。

Method: 提出DART框架，将训练系统分为四个异步模块：环境集群、rollout服务、数据管理器和训练器，并引入自适应数据管理方案，包括预收集成功轨迹、动态调整rollout参数、选择性训练高熵步骤等。

Result: 在OSWorld基准测试中，DART-GUI-7B模型达到42.13%的任务成功率，比基础模型提升14.61%，比开源SOTA高7.34%。系统效率显著提升：1.6倍GPU利用率、1.9倍训练吞吐量、5.5倍环境利用率。

Conclusion: DART框架通过解耦设计和自适应数据管理，有效解决了GUI代理强化学习的效率问题，为开源社区的代理强化学习训练提供了重要贡献。

Abstract: Vision-language model (VLM) based GUI agents show promise for automating
complex desktop and mobile tasks, but face significant challenges in applying
reinforcement learning (RL): (1) slow multi-turn interactions with GUI
environments for policy rollout, and (2) insufficient high-quality
agent-environment interactions for policy learning. To address these
challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI
agents, which coordinates heterogeneous modules in a highly decoupled manner.
DART separates the training system into four asynchronous modules: environment
cluster, rollout service, data manager, and trainer. This design enables
non-blocking communication, asynchronous training, rollout-wise trajectory
sampling, and per-worker model synchronization, significantly improving the
system efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,
and 5.5* environment utilization. To facilitate effective learning from
abundant samples, we introduce an adaptive data curation scheme: (1)
pre-collecting successful trajectories for challenging tasks to supplement
sparse success in online sampling; (2) dynamically adjusting rollout numbers
and trajectory lengths based on task difficulty; (3) training selectively on
high-entropy steps to prioritize critical decisions; (4) stabilizing learning
via truncated importance sampling for policy mismatch between policy rollout
and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task
success rate, a 14.61% absolute gain over the base model, and 7.34% higher than
open-source SOTA. We will fully open-source our training framework, data, and
model checkpoints via computer-use-agents.github.io/dart-gui, which we believe
is a timely contribution to the open-source community of agentic RL training.

</details>


### [167] [Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings](https://arxiv.org/abs/2509.23893)
*Zhixin Zhang,Zeming Wei,Meng Sun*

Main category: cs.LG

TL;DR: 提出动态正交持续微调(DOC)方法来解决LLM持续学习中的灾难性遗忘问题，通过追踪功能方向的漂移并动态更新，调整新任务梯度与历史功能方向正交


<details>
  <summary>Details</summary>
Motivation: 现有正则化方法在长期LLM持续学习中失败的主要原因是微调过程中功能方向的漂移，需要解决这一根本问题

Method: DOC方法追踪功能方向的漂移并动态更新，通过调整新任务参数梯度与历史功能方向正交来减轻新旧任务间的干扰

Result: 在多个LLM持续学习基准测试中表现优于现有方法，有效减少灾难性遗忘

Conclusion: DOC方法为持续LLM微调提供了鲁棒工具，通过处理功能方向漂移解决了持续学习的关键挑战

Abstract: Catastrophic forgetting remains a critical challenge in continual learning
for large language models (LLMs), where models struggle to retain performance
on historical tasks when fine-tuning on new sequential data without access to
past datasets. In this paper, we first reveal that the drift of functional
directions during the fine-tuning process is a key reason why existing
regularization-based methods fail in long-term LLM continual learning. To
address this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a
novel approach that tracks the drift of these functional directions and
dynamically updates them during the fine-tuning process. Furthermore, by
adjusting the gradients of new task parameters to be orthogonal to the tracked
historical function directions, our method mitigates interference between new
and old tasks. Extensive experiments on various LLM continual learning
benchmarks demonstrate that this approach outperforms prior methods,
effectively reducing catastrophic forgetting and providing a robust tool for
continuous LLM fine-tuning. Our code is available at
https://github.com/meloxxxxxx/DOC.

</details>


### [168] [Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm](https://arxiv.org/abs/2509.23946)
*Kaisen Yang,Lixuan He,Rushi Shah,Kaicheng Yang,Qinwei Ma,Dianbo Liu,Alex Lamb*

Main category: cs.LG

TL;DR: 提出Explore-Execute Chain (E²C)框架，将推理分解为探索阶段（生成高层计划）和执行阶段（执行计划），通过两阶段训练方法提高效率、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统CoT方法将高层策略规划与低层执行步骤混在一起，导致计算效率低、推理路径探索有限、可解释性差。

Method: 采用探索-执行分离的推理框架，结合监督微调（SFT）和强化学习（RL）的两阶段训练方法，包含严格遵循计划的数据生成算法。

Result: 在AIME'2024上达到58.1%准确率，仅需不到10%的解码token；在医疗基准上比标准SFT准确率提高14.5%，仅用3.5%的token。

Conclusion: E²C框架通过分离规划与执行，实现了高效推理、强泛化能力和更好的可解释性。

Abstract: Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning
abilities of Large Language Models (LLMs), yet their monolithic and
auto-regressive architecture inherently conflates high-level strategic planning
with low-level step-by-step execution, leading to computational inefficiency,
limited exploration of reasoning paths, and reduced interpretability. To
overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a
structured reasoning framework that decouples reasoning into two distinct
phases: an exploratory phase that stochastically generates succinct high-level
plans, followed by an execution phase that deterministically carries out the
chosen plan. Our approach incorporates a two-stage training methodology, which
combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation
algorithm enforcing strict plan adherence - with a subsequent Reinforcement
Learning (RL) stage that capitalizes on the informativeness of exploration and
reinforces the determinism of execution.This decomposition enables an efficient
test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches
58.1% accuracy using <10% of the decoding tokens required by comparable methods
(e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For
cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with
only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher
accuracy than standard SFT on medical benchmarks, delivering state-of-the-art
performance, strong generalization, and greater interpretability by separating
planning from execution. The code and pre-trained models for the project are
available at: https://github.com/yks23/Explore-Execute-Chain.git

</details>


### [169] [Curriculum-Guided Reinforcement Learning for Synthesizing Gas-Efficient Financial Derivatives Contracts](https://arxiv.org/abs/2509.23976)
*Maruf Ahmed Mridul,Oshani Seneviratne*

Main category: cs.LG

TL;DR: 提出基于强化学习的框架，直接从CDM规范生成功能正确且gas优化的Solidity智能合约，实现高达35.59%的gas成本节省。


<details>
  <summary>Details</summary>
Motivation: 金融衍生品智能合约自动化面临的主要挑战是将高级金融规范转换为既功能正确又经济可行的代码，特别是从CDM规范生成gas优化的可执行代码。

Method: 使用PPO强化学习代理从预定义库中选择最优代码片段，采用两阶段课程学习：先训练功能正确性，再转向gas优化。

Result: RL代理学会生成gas显著节省的合约，在未见测试数据上相比未优化基线实现高达35.59%的成本降低。

Conclusion: 该工作为可靠且经济可持续的智能合约自动合成提供了可行方法，弥合了高级金融协议与高效链上执行之间的差距。

Abstract: Smart contract-based automation of financial derivatives offers substantial
efficiency gains, but its real-world adoption is constrained by the complexity
of translating financial specifications into gas-efficient executable code. In
particular, generating code that is both functionally correct and economically
viable from high-level specifications, such as the Common Domain Model (CDM),
remains a significant challenge. This paper introduces a Reinforcement Learning
(RL) framework to generate functional and gas-optimized Solidity smart
contracts directly from CDM specifications. We employ a Proximal Policy
Optimization (PPO) agent that learns to select optimal code snippets from a
pre-defined library. To manage the complex search space, a two-phase curriculum
first trains the agent for functional correctness before shifting its focus to
gas optimization. Our empirical results show the RL agent learns to generate
contracts with significant gas savings, achieving cost reductions of up to
35.59% on unseen test data compared to unoptimized baselines. This work
presents a viable methodology for the automated synthesis of reliable and
economically sustainable smart contracts, bridging the gap between high-level
financial agreements and efficient on-chain execution.

</details>


### [170] [Guide: Generalized-Prior and Data Encoders for DAG Estimation](https://arxiv.org/abs/2509.23992)
*Amartya Roy,Devharish N,Shreya Ganguly,Kripabandhu Ghosh*

Main category: cs.LG

TL;DR: 提出了GUIDE框架，将LLM生成的邻接矩阵与观测数据通过双编码器架构集成，解决了传统因果发现方法在可扩展性、计算效率和混合数据类型适应性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现代因果发现方法面临可扩展性、计算效率和对混合数据类型适应性方面的关键限制，传统算法如PC、GES和ICA-LiNGAM在处理高阶节点时计算成本过高，且无法扩展到70个节点以上。

Method: GUIDE框架集成LLM生成的邻接矩阵与观测数据，采用双编码器架构，通过强化学习代理动态平衡奖励最大化（准确性）和惩罚避免（DAG约束）。

Result: GUIDE显著优化计算效率，相比RL-BIC和KCRL方法平均减少约42%的运行时间，相比NOTEARS和GraN-DAG方法平均提高约117%的准确性，能够扩展到70个节点以上。

Conclusion: GUIDE在混合数据类型和可扩展性方面表现出鲁棒性能，在基线方法失败的设置中仍能有效工作。

Abstract: Modern causal discovery methods face critical limitations in scalability,
computational efficiency, and adaptability to mixed data types, as evidenced by
benchmarks on node scalability (30, $\le 50$, $\ge 70$ nodes), computational
energy demands, and continuous/non-continuous data handling. While traditional
algorithms like PC, GES, and ICA-LiNGAM struggle with these challenges,
exhibiting prohibitive energy costs for higher-order nodes and poor scalability
beyond 70 nodes, we propose \textbf{GUIDE}, a framework that integrates Large
Language Model (LLM)-generated adjacency matrices with observational data
through a dual-encoder architecture. GUIDE uniquely optimizes computational
efficiency, reducing runtime on average by $\approx 42%$ compared to RL-BIC and
KCRL methods, while achieving an average $\approx 117%$ improvement in accuracy
over both NOTEARS and GraN-DAG individually. During training, GUIDE's
reinforcement learning agent dynamically balances reward maximization
(accuracy) and penalty avoidance (DAG constraints), enabling robust performance
across mixed data types and scalability to $\ge 70$ nodes -- a setting where
baseline methods fail.

</details>


### [171] [Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.24047)
*Runyu Zhang,Na Li,Asuman Ozdaglar,Jeff Shamma,Gioele Zardini*

Main category: cs.LG

TL;DR: 本文提出了一个基于凸风险度量的理论框架，将风险寻求目标解释为乐观主义，并开发了分散式乐观行动者-批评者算法来改善多智能体强化学习中的合作。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习中的风险规避方法往往导致次优均衡，而现有的乐观方法虽然有效但缺乏理论基础。本文旨在为乐观主义提供理论依据，以促进合作。

Method: 基于凸风险度量的对偶表示，提出乐观价值函数框架，将乐观主义形式化为发散惩罚的风险寻求评估，并推导了乐观价值函数的策略梯度定理。

Result: 在合作基准测试中，风险寻求乐观主义相比风险中性基线和启发式乐观方法，持续改善了协调性能。

Conclusion: 该框架统一了风险敏感学习和乐观主义，为多智能体强化学习中的合作提供了理论基础和实际有效的方法。

Abstract: Risk sensitivity has become a central theme in reinforcement learning (RL),
where convex risk measures and robust formulations provide principled ways to
model preferences beyond expected return. Recent extensions to multi-agent RL
(MARL) have largely emphasized the risk-averse setting, prioritizing robustness
to uncertainty. In cooperative MARL, however, such conservatism often leads to
suboptimal equilibria, and a parallel line of work has shown that optimism can
promote cooperation. Existing optimistic methods, though effective in practice,
are typically heuristic and lack theoretical grounding. Building on the dual
representation for convex risk measures, we propose a principled framework that
interprets risk-seeking objectives as optimism. We introduce optimistic value
functions, which formalize optimism as divergence-penalized risk-seeking
evaluations. Building on this foundation, we derive a policy-gradient theorem
for optimistic value functions, including explicit formulas for the entropic
risk/KL-penalty setting, and develop decentralized optimistic actor-critic
algorithms that implement these updates. Empirical results on cooperative
benchmarks demonstrate that risk-seeking optimism consistently improves
coordination over both risk-neutral baselines and heuristic optimistic methods.
Our framework thus unifies risk-sensitive learning and optimism, offering a
theoretically grounded and practically effective approach to cooperation in
MARL.

</details>


### [172] [Collaborative Device-Cloud LLM Inference through Reinforcement Learning](https://arxiv.org/abs/2509.24050)
*Wenzhi Fang,Dong-Jun Han,Liangqi Yuan,Christopher Brinton*

Main category: cs.LG

TL;DR: 提出了一种设备-云协作框架，让设备端LLM在解决问题过程中自主决定是否将查询卸载到云端，通过后训练实现路由决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有设备-云协作方法依赖外部路由器作为二元分类器，难以从提示表面模式准确判断任务难度，导致路由决策效果不佳。

Method: 采用奖励最大化问题框架，设计鼓励有效问题解决和明智云端卸载的奖励函数，开发群组自适应策略梯度算法，包含群组级策略梯度和自适应提示过滤。

Result: 在多个模型和基准测试上的广泛实验表明，该方法始终优于现有基线方法，显著缩小了与完全云端LLM性能的差距。

Conclusion: 提出的框架能够有效提升设备-云协作系统中路由决策的质量，实现更好的性能平衡。

Abstract: Device-cloud collaboration has emerged as a promising paradigm for deploying
large language models (LLMs), combining the efficiency of lightweight on-device
inference with the superior performance of powerful cloud LLMs. An essential
problem in this scenario lies in deciding whether a given query is best handled
locally or delegated to the cloud. Existing approaches typically rely on
external routers, implemented as binary classifiers, which often struggle to
determine task difficulty from the prompt's surface pattern. To address these
limitations, we propose a framework where the on-device LLM makes routing
decisions at the end of its solving process, with this capability instilled
through post-training. In particular, we formulate a reward maximization
problem with carefully designed rewards that encourage effective problem
solving and judicious offloading to the cloud. To solve this problem, we
develop a group-adaptive policy gradient algorithm, featuring a group-level
policy gradient, designed to yield an unbiased gradient estimator of the
reward, and adaptive prompt filtering, developed to enforce the constraint on
cloud LLM usage. Extensive experiments across models and benchmarks show that
the proposed methodology consistently outperforms existing baselines and
significantly narrows the gap to full cloud LLM performance.

</details>


### [173] [In-Context Compositional Q-Learning for Offline Reinforcement Learning](https://arxiv.org/abs/2509.24067)
*Qiushui Xu,Yuhao Huang,Yushu Jiang,Lei Song,Jinyu Wang,Wenliang Zheng,Jiang Bian*

Main category: cs.LG

TL;DR: 提出了ICQL，首个将Q学习建模为上下文推理问题的离线强化学习框架，使用线性Transformer从检索到的转移中自适应推断局部Q函数，无需显式子任务标签。


<details>
  <summary>Details</summary>
Motivation: 现有的离线强化学习方法通常依赖单一的全局Q函数，难以捕捉涉及多样化子任务的组合性质。

Method: 将Q学习建模为上下文推理问题，使用线性Transformer从检索到的转移中自适应推断局部Q函数，无需显式子任务标签。

Result: 在厨房任务中性能提升高达16.4%，在Gym和Adroit任务中分别提升8.6%和6.3%。

Conclusion: 展示了上下文学习在鲁棒和组合价值估计中的潜力，ICQL成为离线RL的原则性和有效框架。

Abstract: Accurately estimating the Q-function is a central challenge in offline
reinforcement learning. However, existing approaches often rely on a single
global Q-function, which struggles to capture the compositional nature of tasks
involving diverse subtasks. We propose In-context Compositional Q-Learning
(\texttt{ICQL}), the first offline RL framework that formulates Q-learning as a
contextual inference problem, using linear Transformers to adaptively infer
local Q-functions from retrieved transitions without explicit subtask labels.
Theoretically, we show that under two assumptions--linear approximability of
the local Q-function and accurate weight inference from retrieved
context--\texttt{ICQL} achieves bounded Q-function approximation error, and
supports near-optimal policy extraction. Empirically, \texttt{ICQL}
substantially improves performance in offline settings: improving performance
in kitchen tasks by up to 16.4\%, and in Gym and Adroit tasks by up to 8.6\%
and 6.3\%. These results highlight the underexplored potential of in-context
learning for robust and compositional value estimation, positioning
\texttt{ICQL} as a principled and effective framework for offline RL.

</details>


### [174] [Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends](https://arxiv.org/abs/2509.24203)
*Chaorui Yao,Yanxi Chen,Yuchang Sun,Yushuo Chen,Wenhao Zhang,Xuchen Pan,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: 该论文重新解释了REINFORCE算法，展示了其天然的离策略特性，并提出了两个适应离策略设置的原则：正则化策略更新和主动塑造数据分布。


<details>
  <summary>Details</summary>
Motivation: 由于实际应用中的约束、LLM-RL基础设施的复杂性以及RL方法创新的需求，大语言模型的离策略强化学习日益受到关注。

Method: 通过第一性原理推导群相对REINFORCE，不假设特定的训练数据分布，揭示其离策略特性，并统一和重新解释OPMD和AsymRE等算法。

Result: 分析揭示了重要性采样和裁剪在GRPO中的作用，为启发式数据加权策略提供了理论依据，并通过实证研究验证了可行性见解。

Conclusion: 该研究为LLM的离策略RL开辟了新的算法设计机会，提供了可操作的见解。

Abstract: Off-policy reinforcement learning (RL) for large language models (LLMs) is
attracting growing interest, driven by practical constraints in real-world
applications, the complexity of LLM-RL infrastructure, and the need for further
innovations of RL methodologies. While classic REINFORCE and its modern
variants like Group Relative Policy Optimization (GRPO) are typically regarded
as on-policy algorithms with limited tolerance of off-policyness, we present in
this work a first-principles derivation for group-relative REINFORCE without
assuming a specific training data distribution, showing that it admits a native
off-policy interpretation. This perspective yields two general principles for
adapting REINFORCE to off-policy settings: regularizing policy updates, and
actively shaping the data distribution. Our analysis demystifies some myths
about the roles of importance sampling and clipping in GRPO, unifies and
reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and
Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss,
and offers theoretical justification for seemingly heuristic data-weighting
strategies. Our findings lead to actionable insights that are validated with
extensive empirical studies, and open up new opportunities for principled
algorithm design in off-policy RL for LLMs. Source code for this work is
available at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.

</details>


### [175] [Asynchronous Policy Gradient Aggregation for Efficient Distributed Reinforcement Learning](https://arxiv.org/abs/2509.24305)
*Alexander Tyurin,Andrei Spiridonov,Varvara Rudenko*

Main category: cs.LG

TL;DR: 提出了两种分布式强化学习算法Rennala NIGT和Malenia NIGT，用于处理异步并行计算和通信环境下的策略梯度方法，在异构设置下实现最优效率。


<details>
  <summary>Details</summary>
Motivation: 分布式强化学习方法在异构异步计算和通信瓶颈存在的情况下研究较少，需要开发能够处理这些挑战的高效算法。

Method: 引入两种新算法：Rennala NIGT在均匀设置下实现策略梯度聚合，支持AllReduce操作；Malenia NIGT同时处理异步计算和异构环境。

Result: Rennala NIGT在均匀设置下显著改善了总计算和通信复杂度，Melenia NIGT在异构设置下获得严格更好的理论保证，实验表明方法显著优于先前方法。

Conclusion: 提出的两种算法在分布式强化学习中有效处理了异步和异构挑战，实现了最先进的效率。

Abstract: We study distributed reinforcement learning (RL) with policy gradient methods
under asynchronous and parallel computations and communications. While
non-distributed methods are well understood theoretically and have achieved
remarkable empirical success, their distributed counterparts remain less
explored, particularly in the presence of heterogeneous asynchronous
computations and communication bottlenecks. We introduce two new algorithms,
Rennala NIGT and Malenia NIGT, which implement asynchronous policy gradient
aggregation and achieve state-of-the-art efficiency. In the homogeneous
setting, Rennala NIGT provably improves the total computational and
communication complexity while supporting the AllReduce operation. In the
heterogeneous setting, Malenia NIGT simultaneously handles asynchronous
computations and heterogeneous environments with strictly better theoretical
guarantees. Our results are further corroborated by experiments, showing that
our methods significantly outperform prior approaches.

</details>


### [176] [Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning](https://arxiv.org/abs/2509.24372)
*Xin Qiu,Yulu Gan,Conor F. Hayes,Qiyao Liang,Elliot Meyerson,Babak Hodjat,Risto Miikkulainen*

Main category: cs.LG

TL;DR: 本文首次成功将进化策略（ES）扩展到大规模语言模型（LLM）的完整参数微调，证明ES在数十亿参数规模下仍能高效搜索，并在多个方面优于强化学习（RL）微调方法。


<details>
  <summary>Details</summary>
Motivation: 传统认为进化策略无法扩展到大规模模型，但本文挑战这一观点，探索ES在LLM微调中的潜力，以提供超越现有RL技术的新方向。

Method: 使用进化策略（ES）对预训练LLM的完整参数进行微调，该方法通过随机扰动参数并基于奖励信号选择最优方向来优化模型。

Result: ES在样本效率、长时程奖励容忍度、对不同基础LLM的鲁棒性、奖励攻击抵抗能力和运行稳定性等方面均优于RL方法。

Conclusion: 进化策略为LLM微调提供了超越当前RL技术的新方向，证明ES能够有效扩展到数十亿参数规模。

Abstract: Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is
a critical step in the AI deployment pipeline. Reinforcement learning (RL) is
arguably the most prominent fine-tuning method, contributing to the birth of
many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once
showed comparable performance to RL on models with a few million parameters,
was neglected due to the pessimistic perception of its scalability to larger
models. In this work, we report the first successful attempt to scale up ES for
fine-tuning the full parameters of LLMs, showing the surprising fact that ES
can search efficiently over billions of parameters and outperform existing RL
fine-tuning methods in multiple respects, including sample efficiency,
tolerance to long-horizon rewards, robustness to different base LLMs, less
tendency to reward hacking, and more stable performance across runs. It
therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond
what current RL techniques provide. The source codes are provided at:
https://github.com/VsonicV/es-fine-tuning-paper.

</details>


### [177] [LLM DNA: Tracing Model Evolution via Functional Representations](https://arxiv.org/abs/2509.24496)
*Zhaomin Wu,Haodong Zhao,Ziyang Wang,Jizhou Guo,Qian Wang,Bingsheng He*

Main category: cs.LG

TL;DR: 该论文提出LLM DNA概念，作为大语言模型功能行为的低维表示，用于追踪模型间的进化关系，并构建了LLM进化树。


<details>
  <summary>Details</summary>
Motivation: 解决数百万LLM模型间进化关系不透明的问题，现有方法受限于任务特定性、固定模型集或严格假设。

Method: 数学定义LLM DNA为功能行为的低维双Lipschitz表示，证明其满足遗传和遗传决定论特性，并开发无需训练的可扩展DNA提取流程。

Result: 在305个LLM上的实验显示DNA与先前研究一致，在特定任务上表现优异，并揭示了未记录的模型关系，成功构建了LLM进化树。

Conclusion: LLM DNA是追踪模型进化关系的有效工具，进化树反映了架构从编码器-解码器到仅解码器的转变、时间进展以及不同家族的不同进化速度。

Abstract: The explosive growth of large language models (LLMs) has created a vast but
opaque landscape: millions of models exist, yet their evolutionary
relationships through fine-tuning, distillation, or adaptation are often
undocumented or unclear, complicating LLM management. Existing methods are
limited by task specificity, fixed model sets, or strict assumptions about
tokenizers or architectures. Inspired by biological DNA, we address these
limitations by mathematically defining LLM DNA as a low-dimensional,
bi-Lipschitz representation of functional behavior. We prove that LLM DNA
satisfies inheritance and genetic determinism properties and establish the
existence of DNA. Building on this theory, we derive a general, scalable,
training-free pipeline for DNA extraction. In experiments across 305 LLMs, DNA
aligns with prior studies on limited subsets and achieves superior or
competitive performance on specific tasks. Beyond these tasks, DNA comparisons
uncover previously undocumented relationships among LLMs. We further construct
the evolutionary tree of LLMs using phylogenetic algorithms, which align with
shifts from encoder-decoder to decoder-only architectures, reflect temporal
progression, and reveal distinct evolutionary speeds across LLM families.

</details>


### [178] [Emergent World Representations in OpenVLA](https://arxiv.org/abs/2509.24559)
*Marco Molinari,Leonardo Nevali,Saharsha Navani,Omar G. Younis*

Main category: cs.LG

TL;DR: 该论文通过嵌入算术方法探究OpenVLA是否隐式学习世界模型，发现其在模型激活中编码了状态转移知识，表明VLAs确实包含内部世界模型。


<details>
  <summary>Details</summary>
Motivation: 研究Vision Language Action模型(VLAs)是否在基于策略的强化学习训练中隐式学习世界模型，这是基于模型的强化学习的标志性特征。

Method: 使用嵌入算术方法，通过线性非线性探针分析OpenVLA模型各层的激活状态，测量序列环境状态嵌入的差异，检测状态转移向量是否可从中间模型激活中恢复。

Result: 发现OpenVLA在模型激活中具有显著的状态转移预测能力，超过基线水平，表明其编码了内部世界模型。早期检查点的分析暗示世界模型随着训练进展而逐渐形成。

Conclusion: OpenVLA确实隐式学习了世界模型，这为理解VLAs的内部表征提供了新视角，并提出了利用稀疏自编码器分析世界模型的管道。

Abstract: Vision Language Action models (VLAs) trained with policy-based reinforcement
learning (RL) encode complex behaviors without explicitly modeling
environmental dynamics. However, it remains unclear whether VLAs implicitly
learn world models, a hallmark of model-based RL. We propose an experimental
methodology using embedding arithmetic on state representations to probe
whether OpenVLA, the current state of the art in VLAs, contains latent
knowledge of state transitions. Specifically, we measure the difference between
embeddings of sequential environment states and test whether this transition
vector is recoverable from intermediate model activations. Using linear and non
linear probes trained on activations across layers, we find statistically
significant predictive ability on state transitions exceeding baselines
(embeddings), indicating that OpenVLA encodes an internal world model (as
opposed to the probes learning the state transitions). We investigate the
predictive ability of an earlier checkpoint of OpenVLA, and uncover hints that
the world model emerges as training progresses. Finally, we outline a pipeline
leveraging Sparse Autoencoders (SAEs) to analyze OpenVLA's world model.

</details>


### [179] [OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment](https://arxiv.org/abs/2509.24610)
*Liang Lin,Zhihao Xu,Junhao Dong,Jian Zhao,Yuchen Yuan,Guibin Zhang,Miao Yu,Yiming Zhang,Zhengtao Yao,Huahui Yi,Dongrui Liu,Xinfeng Li,Kun Wang*

Main category: cs.LG

TL;DR: OrthAlign使用正交子空间分解解决多目标偏好对齐中的梯度冲突，确保不同偏好的优化在数学上不干扰的方向上进行，实现稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 解决LLM对齐中多个人类偏好的关键困境：改进一个维度通常以牺牲其他维度为代价，需要在竞争目标（如帮助性和无害性）之间进行权衡。

Method: 通过正交子空间分解将参数更新空间分解为正交子空间，确保不同偏好的优化在数学上不干扰的方向上进行，并提供理论保证。

Result: 在帮助性、无害性和真实性维度上，多目标对齐后单偏好最大改进达34.61%至50.89%，平均总体奖励改进13.96%。

Conclusion: OrthAlign通过正交子空间分解有效解决了多目标偏好对齐中的梯度冲突问题，实现了稳定收敛和显著性能提升。

Abstract: Large language model (LLM) alignment faces a critical dilemma when addressing
multiple human preferences: improvements in one dimension frequently come at
the expense of others, creating unavoidable trade-offs between competing
objectives like helpfulness and harmlessness. While prior work mainly focuses
on constraint-based optimization algorithms and data selection strategies to
mitigate conflicts, these approaches overlook the fundamental issue of
resolving conflicts directly at the parameter level. In this paper, we present
OrthAlign, an innovative approach that pioneers a new paradigm by leveraging
orthogonal subspace decomposition to fundamentally resolve gradient-level
conflicts in multi-objective preference alignment. OrthAlign strategically
decomposes parameter update spaces into orthogonal subspaces, ensuring that
optimization toward different preferences occurs in mathematically
non-interfering directions. Building upon this, we provide theoretical
guarantees demonstrating that when parameter increments satisfy both orthogonal
subspace constraints and spectral norm bounds, the resulting updates exhibit
linear Lipschitz growth rather than exponential instability, ensuring stable
convergence across all preference dimensions. Extensive experiments show that:
I. OrthAlign achieves maximum single-preference improvements ranging from
34.61% to 50.89% after multiple-objective alignment across helpful, harmless,
and truthful dimensions. II. With an average overall reward improvement of
13.96%.

</details>


### [180] [T-POP: Test-Time Personalization with Online Preference Feedback](https://arxiv.org/abs/2509.24696)
*Zikun Qu,Min Zhang,Mingze Kong,Xiang Li,Zhiwei Shang,Zhiyong Wang,Yikun Ban,Shuang Qiu,Yao Shu,Zhongxiang Dai*

Main category: cs.LG

TL;DR: 提出了T-POP算法，通过在线成对偏好反馈实现实时个性化，无需更新LLM参数，解决了新用户的冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 现有个性化方法需要大量用户数据或资源密集型微调，导致新用户面临冷启动问题，需要一种更高效的实时个性化方案。

Method: 结合测试时对齐和决斗赌博机算法，在文本生成过程中在线学习用户偏好奖励函数，通过智能查询平衡探索与利用。

Result: 实验表明T-POP实现了快速且数据高效的个性化，显著优于现有基线方法，并随着用户交互增加持续改进。

Conclusion: T-POP为LLM实时个性化提供了一种有效的新范式，通过在线偏好反馈解决了冷启动问题。

Abstract: Personalizing large language models (LLMs) to individual user preferences is
a critical step beyond generating generically helpful responses. However,
current personalization methods are ill-suited for new users, as they typically
require either slow, resource-intensive fine-tuning or a substantial amount of
pre-existing user data, creating a significant cold-start problem. To address
this challenge, we introduce a new paradigm for real-time personalization by
learning from online pairwise preference feedback collected during text
generation. We propose T-POP (Test-Time Personalization with Online Preference
Feedback}), a novel algorithm that synergistically combines test-time alignment
with dueling bandits. Without updating the LLM parameters, T-POP steers the
decoding process of a frozen LLM by learning a reward function online that
captures user preferences. By leveraging dueling bandits, T-POP intelligently
queries the user to efficiently balance between exploring their preferences and
exploiting the learned knowledge to generate personalized text. Extensive
experiments demonstrate that T-POP achieves rapid and data-efficient
personalization, significantly outperforming existing baselines and showing
consistent improvement with more user interactions.

</details>


### [181] [Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption](https://arxiv.org/abs/2509.24748)
*Longxiang He,Deheng Ye,Junbo Tan,Xueqian Wang,Li Shen*

Main category: cs.LG

TL;DR: 提出了RPEX方法，通过逆概率加权缓解数据损坏导致的策略重尾行为，提升离线到在线强化学习在数据损坏场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 实际环境中离线数据集和在线交互经常存在噪声甚至恶意损坏，严重影响离线到在线强化学习的性能，而现有方法主要关注通过在线探索缓解离线策略的保守性，对数据损坏下的鲁棒性研究不足。

Method: 提出RPEX方法，将逆概率加权融入在线探索策略以缓解重尾行为，实现鲁棒的策略扩展。

Result: 在D4RL数据集上的广泛实验表明，RPEX在各种数据损坏场景下实现了最先进的离线到在线性能。

Conclusion: RPEX是一种简单有效的鲁棒离线到在线强化学习方法，能够有效应对数据损坏问题。

Abstract: Pretraining a policy on offline data followed by fine-tuning through online
interactions, known as Offline-to-Online Reinforcement Learning (O2O RL), has
emerged as a promising paradigm for real-world RL deployment. However, both
offline datasets and online interactions in practical environments are often
noisy or even maliciously corrupted, severely degrading the performance of O2O
RL. Existing works primarily focus on mitigating the conservatism of offline
policies via online exploration, while the robustness of O2O RL under data
corruption, including states, actions, rewards, and dynamics, is still
unexplored. In this work, we observe that data corruption induces heavy-tailed
behavior in the policy, thereby substantially degrading the efficiency of
online exploration. To address this issue, we incorporate Inverse Probability
Weighted (IPW) into the online exploration policy to alleviate
heavy-tailedness, and propose a novel, simple yet effective method termed
$\textbf{RPEX}$: $\textbf{R}$obust $\textbf{P}$olicy $\textbf{EX}$pansion.
Extensive experimental results on D4RL datasets demonstrate that RPEX achieves
SOTA O2O performance across a wide range of data corruption scenarios. Code is
available at
$\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$.

</details>


### [182] [When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training](https://arxiv.org/abs/2509.24923)
*Sanxing Chen,Xiaoyin Chen,Yukun Huang,Roy Xie,Bhuwan Dhingra*

Main category: cs.LG

TL;DR: 该论文研究了如何通过监督微调(SFT)和强化学习(RL)来增强LLM在顺序决策中的探索能力，特别是在多臂老虎机任务上。研究发现这两种方法都能显著提升性能，但会导致更贪婪的利用行为，有时会过早放弃探索。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在顺序决策中往往探索不足，需要改进其探索策略。研究旨在比较SFT和RL两种学习方法的有效性，并了解它们如何塑造探索行为以及泛化能力。

Method: 使用SFT在专家轨迹上训练LLM，以及使用RL配合定制的奖励信号（包括策略性的遗憾形状奖励和算法奖励）进行训练。对训练后的智能体进行行为分析。

Result: 训练后的智能体表现优于预训练模型，性能可与UCB和Thompson Sampling相媲美，在6倍更长的时间跨度和不同老虎机家族上都具有稳健的泛化能力。但RL/SFT智能体更容易出现早期灾难性失败，过早放弃探索。

Conclusion: 研究阐明了每种训练范式的适用场景，提倡超越平均遗憾的定制奖励设计和评估，以促进稳健的探索行为。

Abstract: While Large Language Models (LLMs) hold promise to become autonomous agents,
they often explore suboptimally in sequential decision-making. Recent work has
sought to enhance this capability via supervised fine-tuning (SFT) or
reinforcement learning (RL), improving regret on the classic multi-armed bandit
task. However, it remains unclear how these learning methods shape exploration
strategies and how well they generalize. We investigate both paradigms by
training LLMs with SFT on expert trajectories and RL with a range of tailored
reward signals including a strategic, regret-shaped reward to reduce variance,
and an algorithmic reward that enables oracle imitation. The resulting agents
outperform pre-trained models and achieve performance comparable to Upper
Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x
longer horizons and across bandit families. Behavioral analysis reveals that
gains often stem from more sophisticated but greedier exploitation: RL/SFT
agents are more prone to early catastrophic failure than pre-trained models,
prematurely abandoning exploration. Furthermore, agents trained to imitate UCB
learn to outperform their teacher by adopting more exploitative variants. Our
findings clarify when each training paradigm is preferable and advocate
tailored reward design and evaluation beyond average regret to promote robust
exploratory behavior.

</details>


### [183] [Rethinking Entropy Regularization in Large Reasoning Models](https://arxiv.org/abs/2509.25133)
*Yuxian Jiang,Yafu Li,Guanxu Chen,Dongrui Liu,Yu Cheng,Jing Shao*

Main category: cs.LG

TL;DR: 提出了SIREN方法，通过选择性熵正则化解决强化学习中验证奖励的熵崩溃和过早收敛问题，在数学基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对强化学习验证奖励方法在大推理模型中存在的熵崩溃和过早收敛问题，传统熵正则化方法由于动作空间大和轨迹长而失效。

Method: 提出SIREN方法，采用两步熵掩码机制（top-p掩码和峰值熵掩码）将探索限制在有意义的动作和状态子集，并使用自锚定形式的正则化稳定训练。

Result: 在五个数学基准测试中，SIREN在平均性能上优于之前的熵相关RLVR方法，在AIME24/25上获得+6.6 maj@k提升，促进响应多样性并保持适当熵水平。

Conclusion: SIREN有效缓解了RLVR在大推理模型中的过早收敛问题，保持了验证通过率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great promise
in enhancing the reasoning abilities of large reasoning models (LRMs). However,
it suffers from a critical issue: entropy collapse and premature convergence.
Naive entropy regularization, a common approach for encouraging exploration in
the traditional RL literature, fails to address this problem in the context of
LRM. Our analysis reveals that this failure stems from the vast action space
and long trajectories in LRMs, which easily trigger a global entropy explosion
as the model indiscriminately explores all possible actions and states. To
address this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method
that confines exploration to a meaningful subset of actions and states. SIREN
achieves this through a two-step entropy masking mechanism, consisting of a
top-p mask and a peak-entropy mask. In addition, regularization is transformed
into a self-anchored form to stabilize training. Across five mathematical
benchmarks, SIREN attains superior average performance over previous
entropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on
AIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes
greater response diversity and maintains entropy at an appropriate level, which
helps to preserve the validation pass@k throughout training. This effectively
mitigates the premature convergence problem common in RLVR for LRM.

</details>


### [184] [SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression](https://arxiv.org/abs/2509.25176)
*Haoming Wen,Yushi Bai,Juanzi Li,Jie Tang*

Main category: cs.LG

TL;DR: SIRI是一种通过交替压缩和扩展推理预算的迭代强化学习方法，能够提高大型推理模型的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现大型推理模型存在重复思维模式，减少这些重复往往以性能下降为代价，需要克服这种权衡。

Method: 采用迭代训练机制，在压缩阶段限制推理步长迫使模型做出精确决策，在扩展阶段放宽限制让模型探索长视野推理。

Result: 在DeepSeek-R1-Distill-Qwen-1.5B上训练，SIRI-low在AIME24上性能提升43.2%同时减少46.9%token使用，SIRI-high达到最高准确率。

Conclusion: 通过在训练中周期性地调整推理模型的输出截断长度，可以动态平衡探索和效率，收敛到性能与效率的最优平衡点。

Abstract: We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved
Compression, a simple yet effective RL approach for Large Reasoning Models
(LRMs) that enables more efficient and accurate reasoning. Existing studies
have observed repetitive thinking patterns in LRMs, and attempts to reduce them
often come at the cost of performance. In this paper, we show that this
trade-off can be overcome through a training regime that iteratively alternates
between compressing and expanding the reasoning budget, by dynamically
adjusting the maximum rollout length during training. The compression phase
cuts the rollout length, forcing the model to make precise and valuable
decisions within a limited context, which effectively reduces redundant tokens
and increases reasoning density. The expansion phase then relaxes the length
limit, providing space for the model to explore and plan in long-horizon
settings. Remarkably, we find that after each compression-expansion cycle, the
model's performance improves even as its output length decreases, steadily
pushing it closer to the Pareto frontier in the performance-efficiency
trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves
performance on AIME24 by 43.2% while reducing token usage by 46.9% after three
iterations, and SIRI-high achieves the highest accuracy compared to all other
methods (Figure 1). Our findings shed light on the potential of periodically
oscillating the LRM's output truncation length during training to dynamically
balance exploration and efficiency in reasoning, converging towards an optimal
"sweet spot" between the two. Our models are publicly available.

</details>


### [185] [DyMoDreamer: World Modeling with Dynamic Modulation](https://arxiv.org/abs/2509.24804)
*Boxuan Zhang,Runqing Wang,Wei Xiao,Weipu Zhang,Jian Sun,Gao Huang,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: DyMoDreamer是一种新颖的基于模型的强化学习算法，通过动态调制机制改进动态特征提取和时间信息丰富，在多个基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习中的样本效率低问题，传统世界模型无法解耦动态对象和时间特征，导致计算效率低下，特别是在视觉任务中动态对象对奖励和决策性能影响显著。

Method: 引入动态调制机制，使用新颖的帧间差分掩码生成差分观测，显式编码对象级运动线索和时间动态，将动态调制建模为随机分类分布并集成到循环状态空间模型中。

Result: 在Atari 100k基准测试中达到156.6%的平均人类标准化分数，在DeepMind视觉控制套件中创下832的新记录，在Crafter基准测试中1M步后性能提升9.5%。

Conclusion: DyMoDreamer通过动态调制机制有效提升了基于模型强化学习的样本效率和性能，在多个基准测试中实现了最先进的结果。

Abstract: A critical bottleneck in deep reinforcement learning (DRL) is sample
inefficiency, as training high-performance agents often demands extensive
environmental interactions. Model-based reinforcement learning (MBRL) mitigates
this by building world models that simulate environmental dynamics and generate
synthetic experience, improving sample efficiency. However, conventional world
models process observations holistically, failing to decouple dynamic objects
and temporal features from static backgrounds. This approach is computationally
inefficient, especially for visual tasks where dynamic objects significantly
influence rewards and decision-making performance. To address this, we
introduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic
modulation mechanism to improve the extraction of dynamic features and enrich
the temporal information. DyMoDreamer employs differential observations derived
from a novel inter-frame differencing mask, explicitly encoding object-level
motion cues and temporal dynamics. Dynamic modulation is modeled as stochastic
categorical distributions and integrated into a recurrent state-space model
(RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments
demonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k
benchmark with a $156.6$\% mean human-normalized score, establishes a new
record of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\%
performance improvement after $1$M steps on the Crafter benchmark. Our code is
released at https://github.com/Ultraman-Tiga1/DyMoDreamer.

</details>


### [186] [Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer](https://arxiv.org/abs/2509.24947)
*Sooraj Sathish,Keshav Goyal,Raghuram Bharadwaj Diddigi*

Main category: cs.LG

TL;DR: 提出一种新颖的深度Q学习方法，通过正则化减少状态特征表示之间的正相关性，从而在迁移学习中更有效地使用线性函数逼近。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习训练需要大量超参数调整和计算成本，迁移学习可以重用已学知识来避免从头训练。但标准深度RL模型学习到的表示高度相关，限制了线性函数逼近在迁移学习中的效果。

Method: 在深度Q学习中引入正则化项来减少状态特征表示之间的正相关性，利用这些去相关特征来改进线性函数逼近在迁移学习中的应用。

Result: 在标准RL基准和MinAtar游戏上的实验表明，该方法能有效提高迁移学习性能并减少计算开销。

Conclusion: 通过减少特征相关性，提出的方法能够更有效地利用线性函数逼近进行迁移学习，从而降低计算成本。

Abstract: Deep Reinforcement Learning (RL) has demonstrated success in solving complex
sequential decision-making problems by integrating neural networks with the RL
framework. However, training deep RL models poses several challenges, such as
the need for extensive hyperparameter tuning and high computational costs.
Transfer learning has emerged as a promising strategy to address these
challenges by enabling the reuse of knowledge from previously learned tasks for
new, related tasks. This avoids the need for retraining models entirely from
scratch. A commonly used approach for transfer learning in RL is to leverage
the internal representations learned by the neural network during training.
Specifically, the activations from the last hidden layer can be viewed as
refined state representations that encapsulate the essential features of the
input. In this work, we investigate whether these representations can be used
as input for training simpler models, such as linear function approximators, on
new tasks. We observe that the representations learned by standard deep RL
models can be highly correlated, which limits their effectiveness when used
with linear function approximation. To mitigate this problem, we propose a
novel deep Q-learning approach that introduces a regularization term to reduce
positive correlations between feature representation of states. By leveraging
these reduced correlated features, we enable more effective use of linear
function approximation in transfer learning. Through experiments and ablation
studies on standard RL benchmarks and MinAtar games, we demonstrate the
efficacy of our approach in improving transfer learning performance and thereby
reducing computational overhead.

</details>


### [187] [Intra-request branch orchestration for efficient LLM reasoning](https://arxiv.org/abs/2509.24957)
*Weifan Jiang,Rana Shahout,Yilun Du,Michael Mitzenmacher,Minlan Yu*

Main category: cs.LG

TL;DR: DUCHESS是一个LLM服务系统，通过分支编排和难度感知调度，在保持准确性的同时显著降低推理延迟和token使用量。


<details>
  <summary>Details</summary>
Motivation: 现有的推理时算法（如思维链和多分支推理）虽然提高了准确性，但显著增加了token使用和延迟。现有工作主要关注减少token使用，往往以牺牲准确性为代价，且忽略了其他延迟因素。

Method: 使用轻量级线性探测模型基于LLM层激活来估计分支正确性，通过编排策略决定分支的终止、复制或继续。在多请求场景下，通过难度感知调度优先处理更简单的推理任务。

Result: 在三个推理基准测试中，DUCHESS将token使用量减少了42-63%，同时保持准确性。在vLLM服务中，平均、中位数和尾部延迟分别降低了57-81%、58-85%和52-84%。

Conclusion: DUCHESS通过智能分支编排和难度感知调度，在不牺牲准确性的前提下显著提升了LLM推理服务的效率和性能。

Abstract: Large Language Models (LLMs) increasingly rely on inference-time reasoning
algorithms such as chain-of-thought and multi-branch reasoning to improve
accuracy on complex tasks. These methods, however, substantially increase token
usage and per-request latency. Prior work has largely focused on reducing token
usage, often at the expense of accuracy, while overlooking other latency
factors. We present DUCHESS, an LLM serving system that reduces cost and
latency without sacrificing accuracy through intra-request branch orchestration
guided by predictions. DUCHESS employs a lightweight linear probing model over
LLM layer activations to estimate branch correctness, and its orchestration
policy decides whether to terminate, duplicate, or continue a branch. When
handling multiple requests, DUCHESS further reduces latency by prioritizing
easier reasoning tasks when complexity can be estimated from the prompt.
Experiments on three reasoning benchmarks show that DUCHESS consistently
improves the token-accuracy Pareto frontier, reducing token usage by 42-63% at
matched accuracy compared to self-consistency. In serving with vLLM, DUCHESS
reduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with
First-Come-First-Served scheduling, and achieves additional gains under
difficulty-aware scheduling at higher request rates.

</details>


### [188] [Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards](https://arxiv.org/abs/2509.24981)
*Haoran He,Yuxiao Ye,Qingpeng Cai,Chen Hu,Binxing Jiao,Daxin Jiang,Ling Pan*

Main category: cs.LG

TL;DR: 提出ROVER方法，通过随机策略评估实现多样化推理，在数学推理任务中优于现有复杂RL方法


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖PPO等策略优化框架，存在训练不稳定和多样性崩溃问题，需要复杂启发式技巧和精细调参

Method: 基于确定性状态转移、树状动态和二元终端奖励的MDP结构，证明最优动作可从均匀随机策略的Q函数中恢复，提出ROVER算法使用均匀策略Q值的softmax采样动作

Result: 在多个基础模型和数学推理基准测试中，ROVER在质量（pass@1 +8.2，pass@256 +16.8）和多样性（+17.6%）方面表现优异

Conclusion: ROVER展示了在数学推理等结构化任务中，可以大幅简化RL方法同时保持甚至提升性能，为RLVR提供了更简单有效的替代方案

Abstract: RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for
improving the reasoning abilities of large language models (LLMs). Current
methods rely primarily on policy optimization frameworks like PPO and GRPO,
which follow generalized policy iteration that alternates between evaluating
the current policy's value and improving the policy based on evaluation. While
effective, they often suffer from training instability and diversity collapse,
requiring complex heuristic tricks and careful tuning. We observe that standard
RLVR in math reasoning can be formalized as a specialized finite-horizon Markov
Decision Process with deterministic state transitions, tree-structured
dynamics, and binary terminal rewards. Though large in scale, the underlying
structure is simpler than general-purpose control settings for which popular RL
algorithms (e.g., PPO) were developed, suggesting that several sophisticated
techniques in existing methods may be reduced or even omitted. Based on this
insight, we prove a surprising result: the optimal action can be recovered from
the Q-function of a fixed uniformly random policy, thereby bypassing the
generalized policy iteration loop and its associated heuristics. We introduce
Random Policy Valuation for Diverse Reasoning (ROVER) to translate this
principle into a practical and scalable algorithm for LLM math reasoning, a
minimalist yet highly effective RL method that samples actions from a softmax
over these uniform-policy Q-values. ROVER preserves diversity throughout
training, allowing sustained exploration of multiple valid pathways. Across
multiple base models and standard math reasoning benchmarks, ROVER demonstrates
superior performance in both \textbf{quality} (\textbf{+8.2} on pass@1,
\textbf{+16.8} on pass@256) and \textbf{diversity} (\textbf{+17.6\%}), despite
its radical simplification compared to strong, complicated existing methods.

</details>


### [189] [Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion Models](https://arxiv.org/abs/2509.25050)
*Shuchen Xue,Chongjian Ge,Shilong Zhang,Yichen Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: 提出了Advantage Weighted Matching (AWM)方法，统一扩散模型的预训练和强化学习目标，通过优势加权降低方差并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型强化学习方法（如DDPO）使用与预训练不同的目标函数，导致方差增加和收敛缓慢。本文旨在统一预训练和RL目标。

Method: AWM使用与预训练相同的score/flow匹配损失，但通过优势函数对样本进行加权，提升高奖励样本的影响，抑制低奖励样本。

Result: 在GenEval、OCR和PickScore基准测试中，AWM相比Flow-GRPO实现了高达24倍的加速，且不损害生成质量。

Conclusion: AWM通过统一预训练和RL目标，在理论和实践上实现了更高效的扩散模型强化学习。

Abstract: Reinforcement Learning (RL) has emerged as a central paradigm for advancing
Large Language Models (LLMs), where pre-training and RL post-training share the
same log-likelihood formulation. In contrast, recent RL approaches for
diffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),
optimize an objective different from the pretraining objectives--score/flow
matching loss. In this work, we establish a novel theoretical analysis: DDPO is
an implicit form of score/flow matching with noisy targets, which increases
variance and slows convergence. Building on this analysis, we introduce
\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for
diffusion. It uses the same score/flow-matching loss as pretraining to obtain a
lower-variance objective and reweights each sample by its advantage. In effect,
AWM raises the influence of high-reward samples and suppresses low-reward ones
while keeping the modeling objective identical to pretraining. This unifies
pretraining and RL conceptually and practically, is consistent with
policy-gradient theory, reduces variance, and yields faster convergence. This
simple yet effective design yields substantial benefits: on GenEval, OCR, and
PickScore benchmarks, AWM delivers up to a $24\times$ speedup over Flow-GRPO
(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,
without compromising generation quality. Code is available at
https://github.com/scxue/advantage_weighted_matching.

</details>
