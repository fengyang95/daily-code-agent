{"id": "2602.13377", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13377", "abs": "https://arxiv.org/abs/2602.13377", "authors": ["Taufiqul Islam Khan", "Shaowei Wang", "Haoxiang Zhang", "Tse-Hsun Chen"], "title": "A Survey of Code Review Benchmarks and Evaluation Practices in Pre-LLM and LLM Era", "comment": null, "summary": "Code review is a critical practice in modern software engineering, helping developers detect defects early, improve code quality, and facilitate knowledge sharing. With the rapid advancement of large language models (LLMs), a growing body of work has explored automated support for code review. However, progress in this area is hindered by the lack of a systematic understanding of existing benchmarks and evaluation practices. Current code review datasets are scattered, vary widely in design, and provide limited insight into what review capabilities are actually being assessed. In this paper, we present a comprehensive survey of code review benchmarks spanning both the Pre-LLM and LLM eras (2015--2025). We analyze 99 research papers (58 Pre-LLM era and 41 LLM era) and extract key metadata, including datasets, evaluation metrics, data sources, and target tasks. Based on this analysis, we propose a multi-level taxonomy that organizes code review research into five domains and 18 fine-grained tasks. Our study reveals a clear shift toward end-to-end generative peer review, increasing multilingual coverage, and a decline in standalone change understanding tasks. We further identify limitations of current benchmarks and outline future directions, including broader task coverage, dynamic runtime evaluation, and taxonomy-guided fine-grained assessment. This survey provides a structured foundation for developing more realistic and comprehensive benchmarks for LLM-based code review.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u4ee3\u7801\u5ba1\u67e5\u57fa\u51c6\u8fdb\u884c\u4e86\u5168\u9762\u8c03\u67e5\uff0c\u5206\u6790\u4e8699\u7bc7\u7814\u7a76\u8bba\u6587\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u7ea7\u5206\u7c7b\u6cd5\uff0c\u63ed\u793a\u4e86\u4ece\u4f20\u7edf\u65b9\u6cd5\u5411\u7aef\u5230\u7aef\u751f\u6210\u5f0f\u540c\u884c\u8bc4\u5ba1\u7684\u8f6c\u53d8\u8d8b\u52bf\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u5ba1\u67e5\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u73b0\u6709\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7406\u89e3\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002\u5f53\u524d\u6570\u636e\u96c6\u5206\u6563\u3001\u8bbe\u8ba1\u5dee\u5f02\u5927\uff0c\u4e14\u65e0\u6cd5\u6e05\u6670\u8bc4\u4f30\u5b9e\u9645\u7684\u5ba1\u67e5\u80fd\u529b\u3002", "method": "\u5bf92015-2025\u5e74\u95f499\u7bc7\u7814\u7a76\u8bba\u6587\uff0858\u7bc7\u524dLLM\u65f6\u4ee3\uff0c41\u7bc7LLM\u65f6\u4ee3\uff09\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u63d0\u53d6\u5173\u952e\u5143\u6570\u636e\uff08\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u3001\u6570\u636e\u6e90\u3001\u76ee\u6807\u4efb\u52a1\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b5\u4e2a\u9886\u57df\u548c18\u4e2a\u7ec6\u7c92\u5ea6\u4efb\u52a1\u7684\u591a\u7ea7\u5206\u7c7b\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u660e\u663e\u7684\u8d8b\u52bf\u8f6c\u53d8\uff1a\u5411\u7aef\u5230\u7aef\u751f\u6210\u5f0f\u540c\u884c\u8bc4\u5ba1\u53d1\u5c55\uff0c\u591a\u8bed\u8a00\u8986\u76d6\u589e\u52a0\uff0c\u72ec\u7acb\u7684\u53d8\u66f4\u7406\u89e3\u4efb\u52a1\u51cf\u5c11\u3002\u540c\u65f6\u8bc6\u522b\u4e86\u5f53\u524d\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u4efb\u52a1\u8986\u76d6\u8303\u56f4\u6709\u9650\u3001\u7f3a\u4e4f\u52a8\u6001\u8fd0\u884c\u65f6\u8bc4\u4f30\u7b49\u3002", "conclusion": "\u8be5\u8c03\u67e5\u4e3a\u5f00\u53d1\u66f4\u73b0\u5b9e\u3001\u66f4\u5168\u9762\u7684LLM\u4ee3\u7801\u5ba1\u67e5\u57fa\u51c6\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u8986\u76d6\u3001\u52a8\u6001\u8fd0\u884c\u65f6\u8bc4\u4f30\u548c\u57fa\u4e8e\u5206\u7c7b\u6cd5\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "topic": "swe benchmark"}}
{"id": "2602.13400", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13400", "abs": "https://arxiv.org/abs/2602.13400", "authors": ["Tanner Wright", "Adams Chen", "Gema Rodr\u00edguez-P\u00e9rez"], "title": "InEx-Bug: A Human Annotated Dataset of Intrinsic and Extrinsic Bugs in the NPM Ecosystem", "comment": null, "summary": "Understanding the causes of software defects is essential for reliable software maintenance and ecosystem stability. However, existing bug datasets do not distinguish between issues originating within a project from those caused by external dependencies or environmental factors. In this paper we present InEx-Bug, a manually annotated dataset of 377 GitHub issues from 103 NPM repositories, categorizing issues as Intrinsic (internal defect), Extrinsic (dependency/environment issue), Not-a-Bug, or Unknown. Beyond labels, the dataset includes rich temporal and behavioral metadata such as maintainer participation, code changes, and reopening patterns. Analyses show Intrinsic bugs resolve faster (median 8.9 vs 10.2 days), are close more often (92% vs 78%), and require code changes more frequently (57% vs 28%) compared to Extrinsic bugs. While Extrinsic bugs exhibit higher reopen rates (12% vs 4%) and delayed recurrence (median 157 vs 87 days). The dataset provides a foundation for further studying Intrinsic and Extrinsic defects in the NPM ecosystem.", "AI": {"tldr": "InEx-Bug\u662f\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u7684GitHub\u95ee\u9898\u6570\u636e\u96c6\uff0c\u5305\u542b377\u4e2aNPM\u4ed3\u5e93\u7684\u95ee\u9898\uff0c\u5c06\u7f3a\u9677\u5206\u7c7b\u4e3a\u5185\u90e8\u7f3a\u9677\u3001\u5916\u90e8\u4f9d\u8d56/\u73af\u5883\u95ee\u9898\u3001\u975e\u7f3a\u9677\u6216\u672a\u77e5\u7c7b\u578b\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u7c7b\u578b\u7f3a\u9677\u7684\u89e3\u51b3\u65f6\u95f4\u548c\u6a21\u5f0f\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u7f3a\u9677\u6570\u636e\u96c6\u672a\u80fd\u533a\u5206\u95ee\u9898\u6765\u6e90\u662f\u9879\u76ee\u5185\u90e8\u8fd8\u662f\u5916\u90e8\u4f9d\u8d56/\u73af\u5883\u56e0\u7d20\uff0c\u8fd9\u9650\u5236\u4e86\u7406\u89e3\u8f6f\u4ef6\u7f3a\u9677\u6839\u672c\u539f\u56e0\u548c\u751f\u6001\u7cfb\u7edf\u7a33\u5b9a\u6027\u7ef4\u62a4\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efaInEx-Bug\u6570\u636e\u96c6\uff0c\u624b\u52a8\u6807\u6ce8103\u4e2aNPM\u4ed3\u5e93\u7684377\u4e2aGitHub\u95ee\u9898\uff0c\u5206\u7c7b\u4e3a\u5185\u90e8\u7f3a\u9677\u3001\u5916\u90e8\u4f9d\u8d56/\u73af\u5883\u95ee\u9898\u3001\u975e\u7f3a\u9677\u6216\u672a\u77e5\u7c7b\u578b\uff0c\u5e76\u6536\u96c6\u65f6\u95f4\u5e8f\u5217\u548c\u884c\u4e3a\u5143\u6570\u636e\uff08\u7ef4\u62a4\u8005\u53c2\u4e0e\u3001\u4ee3\u7801\u53d8\u66f4\u3001\u91cd\u65b0\u5f00\u653e\u6a21\u5f0f\u7b49\uff09\u3002", "result": "\u5185\u90e8\u7f3a\u9677\u89e3\u51b3\u66f4\u5feb\uff08\u4e2d\u4f4d\u65708.9\u5929 vs 10.2\u5929\uff09\uff0c\u5173\u95ed\u7387\u66f4\u9ad8\uff0892% vs 78%\uff09\uff0c\u66f4\u5e38\u9700\u8981\u4ee3\u7801\u53d8\u66f4\uff0857% vs 28%\uff09\uff1b\u5916\u90e8\u7f3a\u9677\u91cd\u65b0\u5f00\u653e\u7387\u66f4\u9ad8\uff0812% vs 4%\uff09\uff0c\u590d\u53d1\u5ef6\u8fdf\u66f4\u957f\uff08\u4e2d\u4f4d\u6570157\u5929 vs 87\u5929\uff09\u3002", "conclusion": "InEx-Bug\u6570\u636e\u96c6\u4e3a\u7814\u7a76NPM\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5185\u90e8\u548c\u5916\u90e8\u7f3a\u9677\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u7c7b\u578b\u7f3a\u9677\u5728\u89e3\u51b3\u65f6\u95f4\u3001\u6a21\u5f0f\u548c\u5f71\u54cd\u65b9\u9762\u7684\u663e\u8457\u5dee\u5f02\u3002", "topic": "swe benchmark"}}
{"id": "2602.13574", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.13574", "abs": "https://arxiv.org/abs/2602.13574", "authors": ["Haoyu Li", "Xijia Che", "Yanhao Wang", "Xiaojing Liao", "Luyi Xing"], "title": "Execution-State-Aware LLM Reasoning for Automated Proof-of-Vulnerability Generation", "comment": "Version 1.0 (13 pages, 7 figures)", "summary": "Proof-of-Vulnerability (PoV) generation is a critical task in software security, serving as a cornerstone for vulnerability validation, false positive reduction, and patch verification. While directed fuzzing effectively drives path exploration, satisfying complex semantic constraints remains a persistent bottleneck in automated exploit generation. Large Language Models (LLMs) offer a promising alternative with their semantic reasoning capabilities; however, existing LLM-based approaches lack sufficient grounding in concrete execution behavior, limiting their ability to generate precise PoVs.\n  In this paper, we present DrillAgent, an agentic framework that reformulates PoV generation as an iterative hypothesis-verification-refinement process. To bridge the gap between static reasoning and dynamic execution, DrillAgent synergizes LLM-based semantic inference with feedback from concrete program states. The agent analyzes the target code to hypothesize inputs, observes execution behavior, and employs a novel mechanism to translate low-level execution traces into source-level constraints. This closed-loop design enables the agent to incrementally align its input generation with the precise requirements of the vulnerability. We evaluate DrillAgent on SEC-bench, a large-scale benchmark of real-world C/C++ vulnerabilities. Experimental results show that DrillAgent substantially outperforms state-of-the-art LLM agent baselines under fixed budget constraints, solving up to 52.8% more CVE tasks than the best-performing baseline. These results highlight the necessity of execution-state-aware reasoning for reliable PoV generation in complex software systems.", "AI": {"tldr": "DrillAgent\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u5047\u8bbe-\u9a8c\u8bc1-\u7cbe\u70bc\u8fc7\u7a0b\u751f\u6210\u6f0f\u6d1e\u8bc1\u660e\uff0c\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u548c\u7a0b\u5e8f\u6267\u884c\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347C/C++\u6f0f\u6d1e\u7684\u81ea\u52a8\u5316PoV\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709PoV\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u5b9a\u5411\u6a21\u7cca\u6d4b\u8bd5\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u8bed\u4e49\u7ea6\u675f\uff0c\u800c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u7f3a\u4e4f\u5177\u4f53\u6267\u884c\u884c\u4e3a\u7684\u63a5\u5730\uff0c\u5bfc\u81f4\u65e0\u6cd5\u751f\u6210\u7cbe\u786e\u7684\u6f0f\u6d1e\u8bc1\u660e\u3002\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u548c\u52a8\u6001\u6267\u884c\u53cd\u9988\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u81ea\u52a8\u5316\u6f0f\u6d1e\u9a8c\u8bc1\u80fd\u529b\u3002", "method": "DrillAgent\u5c06PoV\u751f\u6210\u91cd\u6784\u4e3a\u8fed\u4ee3\u7684\u5047\u8bbe-\u9a8c\u8bc1-\u7cbe\u70bc\u8fc7\u7a0b\uff0c\u901a\u8fc7LLM\u8fdb\u884c\u8bed\u4e49\u63a8\u7406\uff0c\u7ed3\u5408\u5177\u4f53\u7a0b\u5e8f\u72b6\u6001\u53cd\u9988\uff0c\u5206\u6790\u76ee\u6807\u4ee3\u7801\u5047\u8bbe\u8f93\u5165\uff0c\u89c2\u5bdf\u6267\u884c\u884c\u4e3a\uff0c\u5e76\u5c06\u4f4e\u7ea7\u6267\u884c\u8ddf\u8e2a\u8f6c\u6362\u4e3a\u6e90\u4ee3\u7801\u7ea7\u7ea6\u675f\uff0c\u5f62\u6210\u95ed\u73af\u8bbe\u8ba1\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754cC/C++\u6f0f\u6d1e\u57fa\u51c6\u6d4b\u8bd5SEC-bench\u4e0a\u8bc4\u4f30\uff0cDrillAgent\u5728\u56fa\u5b9a\u9884\u7b97\u7ea6\u675f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u4ee3\u7406\u57fa\u7ebf\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u591a\u89e3\u51b352.8%\u7684CVE\u4efb\u52a1\u3002", "conclusion": "DrillAgent\u8bc1\u660e\u4e86\u6267\u884c\u72b6\u6001\u611f\u77e5\u63a8\u7406\u5bf9\u4e8e\u590d\u6742\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u53ef\u9760PoV\u751f\u6210\u7684\u5fc5\u8981\u6027\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u8bed\u4e49\u80fd\u529b\u548c\u52a8\u6001\u6267\u884c\u53cd\u9988\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u6f0f\u6d1e\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "topic": "code agent"}}
{"id": "2602.13611", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13611", "abs": "https://arxiv.org/abs/2602.13611", "authors": ["Xiao He", "Ru Chen", "Jialun Cao"], "title": "From What to How: Bridging User Requirements with Software Development Using Large Language Models", "comment": null, "summary": "Recently, large language models (LLMs) are extensively utilized to enhance development efficiency, leading to numerous benchmarks for evaluating their performance. However, these benchmarks predominantly focus on implementation, overlooking the equally critical aspect of software design. This gap raises two pivotal questions: (1) Can LLMs handle software design? (2) Can LLMs write code following the specific designs? To investigate these questions, this paper proposes DesBench, a design-aware benchmark for evaluating LLMs on three software design-related tasks: design-aware code generation, object-oriented modeling, and the design of acceptance test cases. DesBench comprises 30 manually crafted Java projects that include requirement documents, design models, implementations, and acceptance tests, amounting to a total of 30 design models, 194 Java classes, and 737 test cases. We evaluated seven state-of-the-art LLMs, including three DeepSeek R1, two Qwen2.5, and two GPT models, using DesBench. The results reveal that LLMs remain significantly challenged by the intricacies of software design: (1) For code generation, LLMs struggle to produce correct implementations when provided with only high-level or no designs. (2) In object-oriented modeling, while LLMs can accurately identify objects and classes, they face challenges in defining operations and inter-class relationships. (3) Acceptance test cases generated by LLMs from functional requirements achieve code coverage quality comparable to those written by humans. Our research highlights the current limitations of LLMs in managing software design and calls for further investigation into new design methodologies and languages suitable for LLM-based development.", "AI": {"tldr": "DesBench\u662f\u4e00\u4e2a\u9762\u5411\u8f6f\u4ef6\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4ee3\u7801\u751f\u6210\u3001\u9762\u5411\u5bf9\u8c61\u5efa\u6a21\u548c\u9a8c\u6536\u6d4b\u8bd5\u8bbe\u8ba1\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLM\u5728\u8f6f\u4ef6\u8bbe\u8ba1\u65b9\u9762\u4ecd\u9762\u4e34\u663e\u8457\u6311\u6218\u3002", "motivation": "\u5f53\u524dLLM\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u5b9e\u73b0\uff0c\u5ffd\u89c6\u4e86\u540c\u6837\u91cd\u8981\u7684\u8f6f\u4ef6\u8bbe\u8ba1\u73af\u8282\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1aLLM\u80fd\u5426\u5904\u7406\u8f6f\u4ef6\u8bbe\u8ba1\uff1fLLM\u80fd\u5426\u6309\u7167\u7279\u5b9a\u8bbe\u8ba1\u7f16\u5199\u4ee3\u7801\uff1f", "method": "\u63d0\u51faDesBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b30\u4e2a\u624b\u5de5\u521b\u5efa\u7684Java\u9879\u76ee\uff0c\u6db5\u76d6\u9700\u6c42\u6587\u6863\u3001\u8bbe\u8ba1\u6a21\u578b\u3001\u5b9e\u73b0\u4ee3\u7801\u548c\u9a8c\u6536\u6d4b\u8bd5\u3002\u8bc4\u4f30\u4e867\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff08\u5305\u62ecDeepSeek R1\u3001Qwen2.5\u548cGPT\u6a21\u578b\uff09\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1a\u8bbe\u8ba1\u611f\u77e5\u4ee3\u7801\u751f\u6210\u3001\u9762\u5411\u5bf9\u8c61\u5efa\u6a21\u3001\u9a8c\u6536\u6d4b\u8bd5\u8bbe\u8ba1\u3002", "result": "1) \u4ee3\u7801\u751f\u6210\uff1aLLM\u5728\u4ec5\u6709\u9ad8\u5c42\u8bbe\u8ba1\u6216\u65e0\u8bbe\u8ba1\u65f6\u96be\u4ee5\u751f\u6210\u6b63\u786e\u5b9e\u73b0\uff1b2) \u9762\u5411\u5bf9\u8c61\u5efa\u6a21\uff1aLLM\u80fd\u51c6\u786e\u8bc6\u522b\u5bf9\u8c61\u548c\u7c7b\uff0c\u4f46\u5728\u5b9a\u4e49\u64cd\u4f5c\u548c\u7c7b\u95f4\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff1b3) \u9a8c\u6536\u6d4b\u8bd5\uff1aLLM\u4ece\u529f\u80fd\u9700\u6c42\u751f\u6210\u7684\u6d4b\u8bd5\u7528\u4f8b\u5728\u4ee3\u7801\u8986\u76d6\u7387\u65b9\u9762\u4e0e\u4eba\u5de5\u7f16\u5199\u76f8\u5f53\u3002", "conclusion": "LLM\u5728\u8f6f\u4ef6\u8bbe\u8ba1\u7ba1\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u9002\u5408LLM\u5f00\u53d1\u7684\u65b0\u8bbe\u8ba1\u65b9\u6cd5\u548c\u8bed\u8a00\u3002", "topic": "swe benchmark"}}
{"id": "2602.13213", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13213", "abs": "https://arxiv.org/abs/2602.13213", "authors": ["Joyjit Roy", "Samaresh Kumar Singh"], "title": "Agentic AI for Commercial Insurance Underwriting with Adversarial Self-Critique", "comment": "9 pages, 8 figuers, 6 tables, submitted aty 9th International Conference on Modern Computing, Networking and Applications (MCNA2026)", "summary": "Commercial insurance underwriting is a labor-intensive process that requires manual review of extensive documentation to assess risk and determine policy pricing. While AI offers substantial efficiency improvements, existing solutions lack comprehensive reasoning capabilities and internal mechanisms to ensure reliability within regulated, high-stakes environments. Full automation remains impractical and inadvisable in scenarios where human judgment and accountability are critical. This study presents a decision-negative, human-in-the-loop agentic system that incorporates an adversarial self-critique mechanism as a bounded safety architecture for regulated underwriting workflows. Within this system, a critic agent challenges the primary agent's conclusions prior to submitting recommendations to human reviewers. This internal system of checks and balances addresses a critical gap in AI safety for regulated workflows. Additionally, the research develops a formal taxonomy of failure modes to characterize potential errors by decision-negative agents. This taxonomy provides a structured framework for risk identification and risk management in high-stakes applications. Experimental evaluation using 500 expert-validated underwriting cases demonstrates that the adversarial critique mechanism reduces AI hallucination rates from 11.3% to 3.8% and increases decision accuracy from 92% to 96%. At the same time, the framework enforces strict human authority over all binding decisions by design. These findings indicate that adversarial self-critique supports safer AI deployment in regulated domains and offers a model for responsible integration where human oversight is indispensable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5546\u4e1a\u4fdd\u9669\u6838\u4fdd\u7684\u51b3\u7b56\u5426\u5b9a\u5f0f\u4eba\u673a\u534f\u540c\u4ee3\u7406\u7cfb\u7edf\uff0c\u91c7\u7528\u5bf9\u6297\u6027\u81ea\u6211\u6279\u5224\u673a\u5236\u4f5c\u4e3a\u5b89\u5168\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u4eba\u7c7b\u6700\u7ec8\u51b3\u7b56\u6743\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4eAI\u5e7b\u89c9\u7387\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u5546\u4e1a\u4fdd\u9669\u6838\u4fdd\u662f\u52b3\u52a8\u5bc6\u96c6\u578b\u8fc7\u7a0b\uff0c\u9700\u8981\u4eba\u5de5\u5ba1\u67e5\u5927\u91cf\u6587\u6863\u3002\u73b0\u6709AI\u89e3\u51b3\u65b9\u6848\u7f3a\u4e4f\u5168\u9762\u7684\u63a8\u7406\u80fd\u529b\u548c\u786e\u4fdd\u5728\u53d7\u76d1\u7ba1\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u53ef\u9760\u6027\u7684\u5185\u90e8\u673a\u5236\uff0c\u5b8c\u5168\u81ea\u52a8\u5316\u5728\u9700\u8981\u4eba\u7c7b\u5224\u65ad\u548c\u95ee\u8d23\u7684\u573a\u666f\u4e2d\u65e2\u4e0d\u5b9e\u9645\u4e5f\u4e0d\u53ef\u53d6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u51b3\u7b56\u5426\u5b9a\u5f0f\u4eba\u673a\u534f\u540c\u4ee3\u7406\u7cfb\u7edf\uff0c\u5305\u542b\u5bf9\u6297\u6027\u81ea\u6211\u6279\u5224\u673a\u5236\u4f5c\u4e3a\u5b89\u5168\u67b6\u6784\u3002\u7cfb\u7edf\u4e2d\uff0c\u6279\u5224\u4ee3\u7406\u5728\u5c06\u5efa\u8bae\u63d0\u4ea4\u7ed9\u4eba\u7c7b\u5ba1\u67e5\u5458\u4e4b\u524d\u6311\u6218\u4e3b\u4ee3\u7406\u7684\u7ed3\u8bba\u3002\u540c\u65f6\u5efa\u7acb\u4e86\u51b3\u7b56\u5426\u5b9a\u4ee3\u7406\u6f5c\u5728\u9519\u8bef\u7684\u6b63\u5f0f\u5206\u7c7b\u6cd5\u3002", "result": "\u5728500\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u7684\u6838\u4fdd\u6848\u4f8b\u4e2d\uff0c\u5bf9\u6297\u6027\u6279\u5224\u673a\u5236\u5c06AI\u5e7b\u89c9\u7387\u4ece11.3%\u964d\u81f33.8%\uff0c\u51b3\u7b56\u51c6\u786e\u7387\u4ece92%\u63d0\u9ad8\u523096%\u3002\u7cfb\u7edf\u901a\u8fc7\u8bbe\u8ba1\u5f3a\u5236\u6267\u884c\u4eba\u7c7b\u5bf9\u6240\u6709\u7ea6\u675f\u6027\u51b3\u7b56\u7684\u6700\u7ec8\u6743\u5a01\u3002", "conclusion": "\u5bf9\u6297\u6027\u81ea\u6211\u6279\u5224\u652f\u6301\u5728\u53d7\u76d1\u7ba1\u9886\u57df\u66f4\u5b89\u5168\u7684AI\u90e8\u7f72\uff0c\u4e3a\u4eba\u7c7b\u76d1\u7763\u4e0d\u53ef\u6216\u7f3a\u573a\u666f\u4e0b\u7684\u8d1f\u8d23\u4efb\u96c6\u6210\u63d0\u4f9b\u4e86\u6a21\u578b\u3002\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u53d7\u76d1\u7ba1\u5de5\u4f5c\u6d41\u4e2dAI\u5b89\u5168\u7684\u5173\u952e\u7f3a\u53e3\u3002", "topic": "agent analysis"}}
{"id": "2602.13214", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13214", "abs": "https://arxiv.org/abs/2602.13214", "authors": ["Lingfeng Li", "Yunlong Lu", "Yuefei Zhang", "Jingyu Yao", "Yixin Zhu", "KeYuan Cheng", "Yongyi Wang", "Qirui Zheng", "Xionghui Yang", "Wenxin Li"], "title": "BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in interactive environments requiring strategic decision-making, yet systematic evaluation of these capabilities remains challenging. Existing benchmarks for LLMs primarily assess static reasoning through isolated tasks and fail to capture dynamic strategic abilities. Recent game-based evaluations employ LLM-vs-LLM tournaments that produce relative rankings dependent on transient model pools, incurring quadratic computational costs and lacking stable performance anchors for longitudinal tracking. The central challenge is establishing a scalable evaluation framework that measures LLM strategic reasoning against consistent, interpretable standards rather than volatile peer models. Here we show that anchoring LLM evaluation to fixed hierarchies of skill-calibrated game Artificial Intelligence (AI) enables linear-time absolute skill measurement with stable cross-temporal interpretability. Built on the Botzone platform's established competitive infrastructure, our BotzoneBench evaluates LLMs across eight diverse games spanning deterministic perfect-information board games to stochastic imperfect-information card games. Through systematic assessment of 177,047 state-action pairs from five flagship models, we reveal significant performance disparities and identify distinct strategic behaviors, with top-performing models achieving proficiency comparable to mid-to-high-tier specialized game AI in multiple domains. This anchored evaluation paradigm generalizes beyond games to any domain with well-defined skill hierarchies, establishing a scalable and reusable framework for assessing interactive AI capabilities.", "AI": {"tldr": "BotzoneBench\uff1a\u901a\u8fc7\u56fa\u5b9a\u6280\u80fd\u6821\u51c6\u7684\u6e38\u620fAI\u5c42\u6b21\u7ed3\u6784\u6765\u8bc4\u4f30LLM\u6218\u7565\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u7edd\u5bf9\u6280\u80fd\u6d4b\u91cf\u548c\u8de8\u65f6\u95f4\u7a33\u5b9a\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u63a8\u7406\u4efb\u52a1\uff0c\u65e0\u6cd5\u6355\u6349\u52a8\u6001\u6218\u7565\u80fd\u529b\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6e38\u620f\u7684LLM-vs-LLM\u9526\u6807\u8d5b\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4f9d\u8d56\u77ac\u65f6\u6a21\u578b\u6c60\u3001\u7f3a\u4e4f\u7a33\u5b9a\u6027\u80fd\u951a\u70b9\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6d4b\u91cfLLM\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\u3002", "method": "\u57fa\u4e8eBotzone\u5e73\u53f0\u7684\u7ade\u4e89\u57fa\u7840\u8bbe\u65bd\uff0c\u6784\u5efaBotzoneBench\u57fa\u51c6\uff0c\u5728\u516b\u4e2a\u591a\u6837\u5316\u6e38\u620f\uff08\u4ece\u786e\u5b9a\u6027\u5b8c\u7f8e\u4fe1\u606f\u68cb\u76d8\u6e38\u620f\u5230\u968f\u673a\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u7eb8\u724c\u6e38\u620f\uff09\u4e2d\u8bc4\u4f30LLM\u3002\u901a\u8fc7\u5c06LLM\u8bc4\u4f30\u951a\u5b9a\u5230\u56fa\u5b9a\u6280\u80fd\u6821\u51c6\u7684\u6e38\u620fAI\u5c42\u6b21\u7ed3\u6784\uff0c\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u7684\u7edd\u5bf9\u6280\u80fd\u6d4b\u91cf\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30177,047\u4e2a\u72b6\u6001-\u52a8\u4f5c\u5bf9\u548c\u4e94\u4e2a\u65d7\u8230\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\u548c\u4e0d\u540c\u7684\u6218\u7565\u884c\u4e3a\u3002\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u8fbe\u5230\u4e86\u4e0e\u4e2d\u9ad8\u7b49\u7ea7\u4e13\u4e1a\u6e38\u620fAI\u76f8\u5f53\u7684\u719f\u7ec3\u5ea6\u3002", "conclusion": "\u8fd9\u79cd\u951a\u5b9a\u8bc4\u4f30\u8303\u5f0f\u53ef\u4ee5\u63a8\u5e7f\u5230\u4efb\u4f55\u5177\u6709\u660e\u786e\u5b9a\u4e49\u6280\u80fd\u5c42\u6b21\u7684\u9886\u57df\uff0c\u4e3a\u8bc4\u4f30\u4ea4\u4e92\u5f0fAI\u80fd\u529b\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u91cd\u7528\u7684\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.13723", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13723", "abs": "https://arxiv.org/abs/2602.13723", "authors": ["Weiyu Kong", "Yun Lin", "Xiwen Teoh", "Duc-Minh Nguyen", "Ruofei Ren", "Jiaxin Chang", "Haoxu Hu", "Haoyu Chen"], "title": "ARC: Compiling Hundreds of Requirement Scenarios into A Runnable Web System", "comment": null, "summary": "Large Language Models (LLMs) have improved programming efficiency, but their performance degrades significantly as requirements scale; when faced with multi-modal documents containing hundreds of scenarios, LLMs often produce incorrect implementations or omit constraints. We propose Agentic Requirement Compilation (ARC), a technique that moves beyond simple code generation to requirement compilation, enabling the creation of runnable web systems directly from multi-modal DSL documents. ARC generates not only source code but also modular designs for UI, API, and database layers, enriched test suites (unit, modular, and integration), and detailed traceability for software maintenance. Our approach employs a bidirectional test-driven agentic loop: a top-down architecture phase decomposes requirements into verifiable interfaces, followed by a bottom-up implementation phase where agents generate code to satisfy those tests. ARC maintains strict traceability across requirements, design, and code to facilitate intelligent asset reuse. We evaluated ARC by generating six runnable web systems from documents spanning 50-200 multi-modal scenarios. Compared to state-of-the-art baselines, ARC-generated systems pass 50.6% more GUI tests on average. A user study with 21 participants showed that novice users can successfully write DSL documents for complex systems, such as a 10K-line ticket-booking system, in an average of 5.6 hours. These results demonstrate that ARC effectively transforms non-trivial requirement specifications into maintainable, runnable software.", "AI": {"tldr": "ARC\u662f\u4e00\u79cd\u4ece\u591a\u6a21\u6001DSL\u6587\u6863\u76f4\u63a5\u751f\u6210\u53ef\u8fd0\u884cWeb\u7cfb\u7edf\u7684\u4ee3\u7406\u9700\u6c42\u7f16\u8bd1\u6280\u672f\uff0c\u901a\u8fc7\u53cc\u5411\u6d4b\u8bd5\u9a71\u52a8\u5faa\u73af\u5b9e\u73b0\u9700\u6c42\u5230\u4ee3\u7801\u7684\u8f6c\u6362\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u901a\u8fc750.6%\u66f4\u591a\u7684GUI\u6d4b\u8bd5\u3002", "motivation": "LLM\u5728\u5904\u7406\u5927\u89c4\u6a21\u591a\u6a21\u6001\u9700\u6c42\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u7ecf\u5e38\u4ea7\u751f\u9519\u8bef\u5b9e\u73b0\u6216\u9057\u6f0f\u7ea6\u675f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u590d\u6742\u9700\u6c42\u6587\u6863\u76f4\u63a5\u751f\u6210\u53ef\u8fd0\u884c\u7cfb\u7edf\u7684\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4ee3\u7406\u9700\u6c42\u7f16\u8bd1(ARC)\u6280\u672f\uff0c\u91c7\u7528\u53cc\u5411\u6d4b\u8bd5\u9a71\u52a8\u4ee3\u7406\u5faa\u73af\uff1a\u81ea\u4e0a\u800c\u4e0b\u7684\u67b6\u6784\u9636\u6bb5\u5c06\u9700\u6c42\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u63a5\u53e3\uff0c\u81ea\u4e0b\u800c\u4e0a\u7684\u5b9e\u73b0\u9636\u6bb5\u751f\u6210\u6ee1\u8db3\u6d4b\u8bd5\u7684\u4ee3\u7801\uff0c\u4fdd\u6301\u9700\u6c42\u3001\u8bbe\u8ba1\u548c\u4ee3\u7801\u4e4b\u95f4\u7684\u4e25\u683c\u53ef\u8ffd\u6eaf\u6027\u3002", "result": "\u4ece50-200\u4e2a\u591a\u6a21\u6001\u573a\u666f\u7684\u6587\u6863\u4e2d\u751f\u6210\u4e866\u4e2a\u53ef\u8fd0\u884cWeb\u7cfb\u7edf\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5e73\u5747\u901a\u8fc750.6%\u66f4\u591a\u7684GUI\u6d4b\u8bd5\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\u65b0\u624b\u7528\u6237\u5e73\u57475.6\u5c0f\u65f6\u53ef\u4e3a\u590d\u6742\u7cfb\u7edf\uff08\u59821\u4e07\u884c\u7968\u52a1\u7cfb\u7edf\uff09\u7f16\u5199DSL\u6587\u6863\u3002", "conclusion": "ARC\u80fd\u591f\u6709\u6548\u5c06\u975e\u5e73\u51e1\u9700\u6c42\u89c4\u8303\u8f6c\u6362\u4e3a\u53ef\u7ef4\u62a4\u3001\u53ef\u8fd0\u884c\u7684\u8f6f\u4ef6\uff0c\u5c55\u793a\u4e86\u4ece\u591a\u6a21\u6001\u9700\u6c42\u6587\u6863\u76f4\u63a5\u751f\u6210\u5b8c\u6574\u8f6f\u4ef6\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002", "topic": "swe application"}}
{"id": "2602.13218", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.13218", "abs": "https://arxiv.org/abs/2602.13218", "authors": ["Bowen Liu", "Zhi Wu", "Runquan Xie", "Zhanhui Kang", "Jia Li"], "title": "Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning", "comment": "37 pages, 8 figures, 4 tables in the main body. Project page: https://github.com/AdAstraAbyssoque/Scaling-the-Scaling-Logic", "summary": "Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator--Validator program pairs in a closed Generate--Validate--Repair loop, enabling continuous family evolution with controllable difficulty. To ensure reliability, we introduce a Multi-Gate Validation Protocol that combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks. Starting from 400 seed families, two evolution rounds expand to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps, improving SynLogic by +5.2, BBEH by +1.4, AIME25 by +3.0, and Brumo25 by +3.7.", "AI": {"tldr": "SSLogic\u662f\u4e00\u4e2a\u57fa\u4e8e\u903b\u8f91\u63a8\u7406\u7684RLVR\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210-\u9a8c\u8bc1-\u4fee\u590d\u5faa\u73af\u81ea\u52a8\u5408\u6210\u53ef\u6267\u884c\u7684\u751f\u6210\u5668-\u9a8c\u8bc1\u5668\u7a0b\u5e8f\u5bf9\uff0c\u5b9e\u73b0\u4efb\u52a1\u5bb6\u65cf\u7ea7\u522b\u7684\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u591a\u4e2a\u57fa\u51c6\u6027\u80fd\u3002", "motivation": "\u53ef\u9a8c\u8bc1\u8bad\u7ec3\u4fe1\u53f7\u7684\u6269\u5c55\u662fRLVR\u7684\u5173\u952e\u74f6\u9888\u3002\u903b\u8f91\u63a8\u7406\u662f\u5929\u7136\u57fa\u5e95\uff0c\u4f46\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u4ee3\u7801\u6216\u56fa\u5b9a\u6a21\u677f\uff0c\u53ea\u80fd\u8fdb\u884c\u5b9e\u4f8b\u7ea7\u6270\u52a8\uff0c\u9650\u5236\u4e86\u4efb\u52a1\u5bb6\u65cf\u7ea7\u522b\u7684\u6269\u5c55\u3002", "method": "\u63d0\u51faSSLogic\u4ee3\u7406\u5143\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u5408\u6210\u548c\u4fee\u590d\u53ef\u6267\u884c\u7684\u751f\u6210\u5668-\u9a8c\u8bc1\u5668\u7a0b\u5e8f\u5bf9\uff0c\u5728\u5c01\u95ed\u7684\u751f\u6210-\u9a8c\u8bc1-\u4fee\u590d\u5faa\u73af\u4e2d\u5b9e\u73b0\u8fde\u7eed\u5bb6\u65cf\u6f14\u5316\u3002\u5f15\u5165\u591a\u95e8\u9a8c\u8bc1\u534f\u8bae\uff0c\u7ed3\u5408\u591a\u7b56\u7565\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u5bf9\u6297\u6027\u76f2\u5ba1\uff0c\u786e\u4fdd\u4efb\u52a1\u53ef\u9760\u6027\u3002", "result": "\u4ece400\u4e2a\u79cd\u5b50\u5bb6\u65cf\u5f00\u59cb\uff0c\u7ecf\u8fc7\u4e24\u8f6e\u6f14\u5316\u6269\u5c55\u5230953\u4e2a\u5bb6\u65cf\u548c21,389\u4e2a\u53ef\u9a8c\u8bc1\u5b9e\u4f8b\uff08\u4ece5,718\u4e2a\uff09\u3002\u5728\u76f8\u540c\u8bad\u7ec3\u6b65\u6570\u4e0b\uff0cSSLogic\u6f14\u5316\u6570\u636e\u76f8\u6bd4\u79cd\u5b50\u57fa\u7ebf\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\uff1aSynLogic +5.2\uff0cBBEH +1.4\uff0cAIME25 +3.0\uff0cBrumo25 +3.7\u3002", "conclusion": "SSLogic\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4efb\u52a1\u5bb6\u65cf\u7ea7\u522b\u7684\u53ef\u6269\u5c55\u6027\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5408\u6210\u548c\u9a8c\u8bc1\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86RLVR\u7684\u8bad\u7ec3\u4fe1\u53f7\u89c4\u6a21\u548c\u8d28\u91cf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.13540", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13540", "abs": "https://arxiv.org/abs/2602.13540", "authors": ["Sin-Han Yang", "Cheng-Kuang Wu", "Chieh-Yen Lin", "Yun-Nung Chen", "Hung-yi Lee", "Shao-Hua Sun"], "title": "On Calibration of Large Language Models: From Response To Capability", "comment": "preprint", "summary": "Large language models (LLMs) are widely deployed as general-purpose problem solvers, making accurate confidence estimation critical for reliable use. Prior work on LLM calibration largely focuses on response-level confidence, which estimates the correctness of a single generated output. However, this formulation is misaligned with many practical settings where the central question is how likely a model is to solve a query overall. We show that this mismatch results from the stochastic nature of modern LLM decoding, under which single-response correctness fails to reflect underlying model capability. To address this issue, we introduce capability calibration, which targets the model's expected accuracy on a query. We formally distinguish capability calibration from response calibration and show that the two differ both theoretically and empirically. We establish an empirical evaluation setup and study a range of confidence estimation methods. Our results demonstrate that capability-calibrated confidence improves pass@$k$ prediction and inference budget allocation, establishing a foundation with potential for diverse applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u80fd\u529b\u6821\u51c6\uff08capability calibration\uff09\u7684\u6982\u5ff5\uff0c\u4e0e\u4f20\u7edf\u54cd\u5e94\u7ea7\u6821\u51c6\u4e0d\u540c\uff0c\u5b83\u5173\u6ce8\u6a21\u578b\u5728\u67e5\u8be2\u4e0a\u7684\u671f\u671b\u51c6\u786e\u7387\uff0c\u89e3\u51b3\u4e86LLM\u89e3\u7801\u968f\u673a\u6027\u5bfc\u81f4\u7684\u5355\u54cd\u5e94\u6b63\u786e\u6027\u4e0e\u5b9e\u9645\u80fd\u529b\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u6821\u51c6\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u54cd\u5e94\u7ea7\u7f6e\u4fe1\u5ea6\uff0c\u5373\u4f30\u8ba1\u5355\u4e2a\u751f\u6210\u8f93\u51fa\u7684\u6b63\u786e\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e0e\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e0d\u5339\u914d\uff0c\u56e0\u4e3a\u5b9e\u9645\u95ee\u9898\u901a\u5e38\u662f\"\u6a21\u578b\u6709\u591a\u5927\u53ef\u80fd\u89e3\u51b3\u4e00\u4e2a\u67e5\u8be2\"\u3002\u8fd9\u79cd\u4e0d\u5339\u914d\u6e90\u4e8e\u73b0\u4ee3LLM\u89e3\u7801\u7684\u968f\u673a\u6027\uff0c\u5355\u54cd\u5e94\u6b63\u786e\u6027\u65e0\u6cd5\u53cd\u6620\u5e95\u5c42\u6a21\u578b\u80fd\u529b\u3002", "method": "\u5f15\u5165\u80fd\u529b\u6821\u51c6\u6982\u5ff5\uff0c\u9488\u5bf9\u6a21\u578b\u5728\u67e5\u8be2\u4e0a\u7684\u671f\u671b\u51c6\u786e\u7387\u8fdb\u884c\u6821\u51c6\u3002\u6b63\u5f0f\u533a\u5206\u80fd\u529b\u6821\u51c6\u4e0e\u54cd\u5e94\u6821\u51c6\uff0c\u5efa\u7acb\u5b9e\u8bc1\u8bc4\u4f30\u6846\u67b6\uff0c\u7814\u7a76\u4e00\u7cfb\u5217\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "\u80fd\u529b\u6821\u51c6\u7f6e\u4fe1\u5ea6\u5728pass@k\u9884\u6d4b\u548c\u63a8\u7406\u9884\u7b97\u5206\u914d\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u4e3a\u591a\u6837\u5316\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u7406\u8bba\u4e0e\u5b9e\u8bc1\u5747\u8868\u660e\u80fd\u529b\u6821\u51c6\u4e0e\u54cd\u5e94\u6821\u51c6\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u80fd\u529b\u6821\u51c6\u6bd4\u4f20\u7edf\u54cd\u5e94\u7ea7\u6821\u51c6\u66f4\u7b26\u5408\u5b9e\u9645\u5e94\u7528\u9700\u6c42\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u6a21\u578b\u89e3\u51b3\u67e5\u8be2\u7684\u6574\u4f53\u80fd\u529b\uff0c\u5728\u591a\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.13551", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13551", "abs": "https://arxiv.org/abs/2602.13551", "authors": ["Yike Wang", "Faeze Brahman", "Shangbin Feng", "Teng Xiao", "Hannaneh Hajishirzi", "Yulia Tsvetkov"], "title": "Small Reward Models via Backward Inference", "comment": null, "summary": "Reward models (RMs) play a central role throughout the language model (LM) pipeline, particularly in non-verifiable domains. However, the dominant LLM-as-a-Judge paradigm relies on the strong reasoning capabilities of large models, while alternative approaches require reference responses or explicit rubrics, limiting flexibility and broader accessibility. In this work, we propose FLIP (FLipped Inference for Prompt reconstruction), a reference-free and rubric-free reward modeling approach that reformulates reward modeling through backward inference: inferring the instruction that would most plausibly produce a given response. The similarity between the inferred and the original instructions is then used as the reward signal. Evaluations across four domains using 13 small language models show that FLIP outperforms LLM-as-a-Judge baselines by an average of 79.6%. Moreover, FLIP substantially improves downstream performance in extrinsic evaluations under test-time scaling via parallel sampling and GRPO training. We further find that FLIP is particularly effective for longer outputs and robust to common forms of reward hacking. By explicitly exploiting the validation-generation gap, FLIP enables reliable reward modeling in downscaled regimes where judgment methods fail. Code available at https://github.com/yikee/FLIP.", "AI": {"tldr": "FLIP\u662f\u4e00\u79cd\u65e0\u53c2\u8003\u3001\u65e0\u8bc4\u5206\u6807\u51c6\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u5411\u63a8\u7406\u4ece\u54cd\u5e94\u63a8\u65ad\u6307\u4ee4\uff0c\u5e76\u5c06\u63a8\u65ad\u6307\u4ee4\u4e0e\u539f\u59cb\u6307\u4ee4\u7684\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u591a\u4e2a\u9886\u57df\u663e\u8457\u4f18\u4e8eLLM-as-a-Judge\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u5956\u52b1\u5efa\u6a21\u4e3b\u8981\u4f9d\u8d56\u5927\u6a21\u578b\u7684\u5f3a\u63a8\u7406\u80fd\u529b\uff08LLM-as-a-Judge\u8303\u5f0f\uff09\uff0c\u6216\u9700\u8981\u53c2\u8003\u54cd\u5e94\u548c\u660e\u786e\u8bc4\u5206\u6807\u51c6\uff0c\u8fd9\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u65e0\u9700\u53c2\u8003\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFLIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u5411\u63a8\u7406\u8fdb\u884c\u5956\u52b1\u5efa\u6a21\uff1a\u4ece\u7ed9\u5b9a\u7684\u54cd\u5e94\u63a8\u65ad\u6700\u53ef\u80fd\u4ea7\u751f\u8be5\u54cd\u5e94\u7684\u6307\u4ee4\uff0c\u7136\u540e\u5c06\u63a8\u65ad\u6307\u4ee4\u4e0e\u539f\u59cb\u6307\u4ee4\u7684\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u53c2\u8003\u54cd\u5e94\u6216\u8bc4\u5206\u6807\u51c6\u3002", "result": "\u5728\u56db\u4e2a\u9886\u57df\u4f7f\u752813\u4e2a\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0cFLIP\u5e73\u5747\u4f18\u4e8eLLM-as-a-Judge\u57fa\u7ebf79.6%\u3002\u901a\u8fc7\u5e76\u884c\u91c7\u6837\u548cGRPO\u8bad\u7ec3\uff0cFLIP\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u7279\u522b\u9002\u7528\u4e8e\u957f\u8f93\u51fa\uff0c\u5bf9\u5e38\u89c1\u7684\u5956\u52b1\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "FLIP\u901a\u8fc7\u5229\u7528\u9a8c\u8bc1-\u751f\u6210\u5dee\u8ddd\uff0c\u5728\u89c4\u6a21\u7f29\u51cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u5956\u52b1\u5efa\u6a21\uff0c\u4e3a\u5956\u52b1\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.13962", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13962", "abs": "https://arxiv.org/abs/2602.13962", "authors": ["Yunkun Wang", "Xuanhe Zhang", "Junxiao Han", "Chen Zhi", "Shuiguang Deng"], "title": "CodeGlance: Understanding Code Reasoning Challenges in LLMs through Multi-Dimensional Feature Analysis", "comment": null, "summary": "In modern software development, developers frequently need to understand code behavior at a glance -- whether reviewing pull requests, debugging issues, or navigating unfamiliar codebases. This ability to reason about dynamic program behavior is fundamental to effective software engineering and increasingly supported by Large Language Models (LLMs). However, existing studies on code reasoning focus primarily on isolated code snippets, overlooking the complexity of real-world scenarios involving external API interactions and unfamiliar functions. This gap hinders our understanding of what truly makes code reasoning challenging for LLMs across diverse programming contexts.\n  We present CodeGlance, a multi-dimensional benchmark investigating code reasoning challenges across three realistic scenarios: intrinsic logic reasoning, API interaction reasoning, and unseen function reasoning. Through systematic evaluation of 7 state-of-the-art LLMs, we reveal that unseen function reasoning poses significant challenges especially for smaller models, with Qwen2.5-3b achieving only 6.0\\% accuracy on unseen functions compared to 37.5\\% on familiar APIs. We identify critical code complexity features -- including execution trace length, API invocation count, and control flow complexity -- that significantly impact code reasoning difficulty across scenarios. We further investigate how common augmentation strategies, including CoT, document retrieval, and code search, can improve reasoning performance, finding that their effectiveness varies substantially depending on whether challenges stem from logical complexity or knowledge gaps. These findings provide actionable guidance for developing more capable code reasoning systems and deploying LLM-based programming assistants in real-world software development.", "AI": {"tldr": "CodeGlance\u662f\u4e00\u4e2a\u591a\u7ef4\u5ea6\u4ee3\u7801\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLM\u5728\u4e09\u79cd\u73b0\u5b9e\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff1a\u5185\u5728\u903b\u8f91\u63a8\u7406\u3001API\u4ea4\u4e92\u63a8\u7406\u548c\u672a\u77e5\u51fd\u6570\u63a8\u7406\uff0c\u63ed\u793a\u4e86\u672a\u77e5\u51fd\u6570\u63a8\u7406\u5bf9\u5c0f\u578b\u6a21\u578b\u7684\u663e\u8457\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u63a8\u7406\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u4ee3\u7801\u7247\u6bb5\uff0c\u5ffd\u7565\u4e86\u6d89\u53ca\u5916\u90e8API\u4ea4\u4e92\u548c\u672a\u77e5\u51fd\u6570\u7684\u73b0\u5b9e\u573a\u666f\u590d\u6742\u6027\uff0c\u8fd9\u963b\u788d\u4e86\u6211\u4eec\u7406\u89e3LLM\u5728\u4e0d\u540c\u7f16\u7a0b\u4e0a\u4e0b\u6587\u4e2d\u7684\u771f\u6b63\u6311\u6218\u3002", "method": "\u521b\u5efaCodeGlance\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e09\u4e2a\u73b0\u5b9e\u573a\u666f\uff1a\u5185\u5728\u903b\u8f91\u63a8\u7406\u3001API\u4ea4\u4e92\u63a8\u7406\u548c\u672a\u77e5\u51fd\u6570\u63a8\u7406\u3002\u7cfb\u7edf\u8bc4\u4f307\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff0c\u5206\u6790\u4ee3\u7801\u590d\u6742\u6027\u7279\u5f81\uff08\u6267\u884c\u8ddf\u8e2a\u957f\u5ea6\u3001API\u8c03\u7528\u6b21\u6570\u3001\u63a7\u5236\u6d41\u590d\u6742\u6027\uff09\uff0c\u5e76\u7814\u7a76\u5e38\u89c1\u589e\u5f3a\u7b56\u7565\uff08CoT\u3001\u6587\u6863\u68c0\u7d22\u3001\u4ee3\u7801\u641c\u7d22\uff09\u7684\u6548\u679c\u3002", "result": "\u672a\u77e5\u51fd\u6570\u63a8\u7406\u5bf9\u5c0f\u578b\u6a21\u578b\u6784\u6210\u663e\u8457\u6311\u6218\uff0cQwen2.5-3b\u5728\u672a\u77e5\u51fd\u6570\u4e0a\u4ec5\u8fbe\u52306.0%\u51c6\u786e\u7387\uff0c\u800c\u5728\u719f\u6089API\u4e0a\u4e3a37.5%\u3002\u8bc6\u522b\u51fa\u6267\u884c\u8ddf\u8e2a\u957f\u5ea6\u3001API\u8c03\u7528\u6b21\u6570\u548c\u63a7\u5236\u6d41\u590d\u6742\u6027\u7b49\u5173\u952e\u4ee3\u7801\u590d\u6742\u6027\u7279\u5f81\u4f1a\u5f71\u54cd\u63a8\u7406\u96be\u5ea6\u3002\u4e0d\u540c\u589e\u5f3a\u7b56\u7565\u7684\u6548\u679c\u56e0\u6311\u6218\u6765\u6e90\uff08\u903b\u8f91\u590d\u6742\u6027vs\u77e5\u8bc6\u5dee\u8ddd\uff09\u800c\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u4ee3\u7801\u63a8\u7406\u7cfb\u7edf\u548c\u5728\u73b0\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u90e8\u7f72\u57fa\u4e8eLLM\u7684\u7f16\u7a0b\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u8003\u8651\u73b0\u5b9e\u573a\u666f\u590d\u6742\u6027\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.13987", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13987", "abs": "https://arxiv.org/abs/2602.13987", "authors": ["Zhengyu Zhan", "Ye Shang", "Jiawei Liu", "Chunrong Fang", "Quanjun Zhang", "Zhenyu Chen"], "title": "ATTest: Agent-Driven Tensor Testing for Deep Learning Library Modules", "comment": "5 pages, 3 figures", "summary": "The unit testing of Deep Learning (DL) libraries is challenging due to complex numerical semantics and implicit tensor constraints. Traditional Search-Based Software Testing (SBST) often suffers from semantic blindness, failing to satisfy the constraints of high-dimensional tensors, whereas Large Language Models (LLMs) struggle with cross-file context and unstable code modifications. This paper proposes ATTest, an agent-driven tensor testing framework for module-level unit test generation. ATTest orchestrates a seven-stage pipeline, which encompasses constraint extraction and an iterative \"generation-validation-repair\" loop, to maintain testing stability and mitigate context-window saturation. An evaluation on PyTorch and TensorFlow demonstrates that ATTest significantly outperforms state-of-the-art baselines such as PynguinML, achieving an average branch coverage of 55.60% and 54.77%, respectively. The results illustrate how agent-driven workflows bridge the semantic gap in numerical libraries while ensuring auditable test synthesis. Source code: https://github.com/iSEngLab/ATTest.git", "AI": {"tldr": "ATTest\u662f\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u5f20\u91cf\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u5e93\u7684\u6a21\u5757\u7ea7\u5355\u5143\u6d4b\u8bd5\u751f\u6210\uff0c\u901a\u8fc7\u4e03\u9636\u6bb5\u6d41\u6c34\u7ebf\u663e\u8457\u63d0\u5347\u4e86\u5206\u652f\u8986\u76d6\u7387\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5e93\u7684\u5355\u5143\u6d4b\u8bd5\u9762\u4e34\u6311\u6218\uff1a\u590d\u6742\u7684\u6570\u503c\u8bed\u4e49\u548c\u9690\u5f0f\u5f20\u91cf\u7ea6\u675f\u4f7f\u5f97\u4f20\u7edf\u57fa\u4e8e\u641c\u7d22\u7684\u8f6f\u4ef6\u6d4b\u8bd5\u5b58\u5728\u8bed\u4e49\u76f2\u533a\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8de8\u6587\u4ef6\u4e0a\u4e0b\u6587\u548c\u4e0d\u7a33\u5b9a\u7684\u4ee3\u7801\u4fee\u6539\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "ATTest\u91c7\u7528\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u4e03\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u5305\u62ec\u7ea6\u675f\u63d0\u53d6\u548c\u8fed\u4ee3\u7684\"\u751f\u6210-\u9a8c\u8bc1-\u4fee\u590d\"\u5faa\u73af\uff0c\u4ee5\u4fdd\u6301\u6d4b\u8bd5\u7a33\u5b9a\u6027\u5e76\u7f13\u89e3\u4e0a\u4e0b\u6587\u7a97\u53e3\u9971\u548c\u95ee\u9898\u3002", "result": "\u5728PyTorch\u548cTensorFlow\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cATTest\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982PynguinML\uff09\uff0c\u5e73\u5747\u5206\u652f\u8986\u76d6\u7387\u5206\u522b\u8fbe\u523055.60%\u548c54.77%\u3002", "conclusion": "\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\u80fd\u591f\u5f25\u5408\u6570\u503c\u5e93\u4e2d\u7684\u8bed\u4e49\u5dee\u8ddd\uff0c\u540c\u65f6\u786e\u4fdd\u53ef\u5ba1\u8ba1\u7684\u6d4b\u8bd5\u5408\u6210\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5e93\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2602.13235", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.13235", "abs": "https://arxiv.org/abs/2602.13235", "authors": ["Yuqi Xiong", "Chunyi Peng", "Zhipeng Xu", "Zhenghao Liu", "Zulong Chen", "Yukun Yan", "Shuo Wang", "Yu Gu", "Ge Yu"], "title": "Lang2Act: Fine-Grained Visual Reasoning through Self-Emergent Linguistic Toolchains", "comment": null, "summary": "Visual Retrieval-Augmented Generation (VRAG) enhances Vision-Language Models (VLMs) by incorporating external visual documents to address a given query. Existing VRAG frameworks usually depend on rigid, pre-defined external tools to extend the perceptual capabilities of VLMs, typically by explicitly separating visual perception from subsequent reasoning processes. However, this decoupled design can lead to unnecessary loss of visual information, particularly when image-based operations such as cropping are applied. In this paper, we propose Lang2Act, which enables fine-grained visual perception and reasoning through self-emergent linguistic toolchains. Rather than invoking fixed external engines, Lang2Act collects self-emergent actions as linguistic tools and leverages them to enhance the visual perception capabilities of VLMs. To support this mechanism, we design a two-stage Reinforcement Learning (RL)-based training framework. Specifically, the first stage optimizes VLMs to self-explore high-quality actions for constructing a reusable linguistic toolbox, and the second stage further optimizes VLMs to exploit these linguistic tools for downstream reasoning effectively. Experimental results demonstrate the effectiveness of Lang2Act in substantially enhancing the visual perception capabilities of VLMs, achieving performance improvements of over 4%. All code and data are available at https://github.com/NEUIR/Lang2Act.", "AI": {"tldr": "Lang2Act\u63d0\u51fa\u901a\u8fc7\u81ea\u6d8c\u73b0\u8bed\u8a00\u5de5\u5177\u94fe\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u4e0e\u63a8\u7406\uff0c\u66ff\u4ee3\u4f20\u7edfVRAG\u7684\u56fa\u5b9a\u5916\u90e8\u5de5\u5177\uff0c\u4f7f\u7528\u4e24\u9636\u6bb5RL\u8bad\u7ec3\u6846\u67b6\u63d0\u5347VLM\u6027\u80fd4%\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709VRAG\u6846\u67b6\u4f9d\u8d56\u9884\u5b9a\u4e49\u5916\u90e8\u5de5\u5177\uff0c\u5c06\u89c6\u89c9\u611f\u77e5\u4e0e\u63a8\u7406\u8fc7\u7a0b\u5206\u79bb\uff0c\u5bfc\u81f4\u89c6\u89c9\u4fe1\u606f\u635f\u5931\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u88c1\u526a\u7b49\u64cd\u4f5c\u4e2d\u3002\u9700\u8981\u66f4\u7075\u6d3b\u3001\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u611f\u77e5\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLang2Act\u6846\u67b6\uff1a1\uff09\u8ba9VLM\u81ea\u63a2\u7d22\u9ad8\u8d28\u91cf\u52a8\u4f5c\u6784\u5efa\u53ef\u91cd\u7528\u8bed\u8a00\u5de5\u5177\u7bb1\uff1b2\uff09\u4f18\u5316VLM\u5229\u7528\u8fd9\u4e9b\u8bed\u8a00\u5de5\u5177\u8fdb\u884c\u4e0b\u6e38\u63a8\u7406\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eLang2Act\u663e\u8457\u589e\u5f3aVLM\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc74%\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002", "conclusion": "Lang2Act\u901a\u8fc7\u81ea\u6d8c\u73b0\u8bed\u8a00\u5de5\u5177\u94fe\u5b9e\u73b0\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u611f\u77e5\u4e0e\u63a8\u7406\uff0c\u907f\u514d\u4e86\u4f20\u7edfVRAG\u6846\u67b6\u7684\u89c6\u89c9\u4fe1\u606f\u635f\u5931\u95ee\u9898\uff0c\u4e3aVLM\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.13575", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13575", "abs": "https://arxiv.org/abs/2602.13575", "authors": ["Jing Zhao", "Ting Zhen", "Junwei bao", "Hongfei Jiang", "Yang song"], "title": "Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment", "comment": null, "summary": "Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models. Results demonstrate a clear performance hierarchy: point-based methods < static pairwise training < Elo-Evolve across Alpaca Eval 2.0 and MT-Bench, validating the progressive benefits of pairwise comparison and dynamic opponent selection for LLM alignment.", "AI": {"tldr": "Elo-Evolve\uff1a\u4e00\u4e2a\u57fa\u4e8e\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7ade\u4e89\u7684LLM\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u6210\u5bf9\u7ade\u4e89\u548c\u81ea\u9002\u5e94\u5bf9\u624b\u6c60\u5b9e\u73b0\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u6a21\u578b\u5bf9\u9f50", "motivation": "\u5f53\u524dLLM\u5bf9\u9f50\u65b9\u6cd5\u4f9d\u8d56\u5c06\u5927\u91cf\u4eba\u7c7b\u504f\u597d\u6570\u636e\u538b\u7f29\u4e3a\u9759\u6001\u7edd\u5bf9\u5956\u52b1\u51fd\u6570\uff0c\u5bfc\u81f4\u6570\u636e\u7a00\u7f3a\u3001\u566a\u58f0\u654f\u611f\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u5f15\u5165Elo-Evolve\u6846\u67b6\uff0c\u5c06\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u81ea\u9002\u5e94\u5bf9\u624b\u6c60\u4e2d\u7684\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7ade\u4e89\u3002\u5173\u952e\u521b\u65b0\uff1a1\uff09\u6d88\u9664Bradley-Terry\u6a21\u578b\u4f9d\u8d56\uff0c\u76f4\u63a5\u4ece\u6210\u5bf9\u7ade\u4e89\u7684\u4e8c\u5143\u8f93\u8d62\u7ed3\u679c\u5b66\u4e60\uff1b2\uff09\u5b9e\u73b0Elo\u7f16\u6392\u7684\u5bf9\u624b\u9009\u62e9\uff0c\u901a\u8fc7\u6e29\u5ea6\u63a7\u5236\u91c7\u6837\u63d0\u4f9b\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\u3002", "result": "\u5728Qwen2.5-7B\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u4f7f\u7528Qwen2.5-14B\u3001Qwen2.5-32B\u548cQwen3-8B\u4f5c\u4e3a\u5bf9\u624b\u3002\u7ed3\u679c\u663e\u793a\u6027\u80fd\u5c42\u6b21\uff1a\u57fa\u4e8e\u70b9\u7684\u65b9\u6cd5 < \u9759\u6001\u6210\u5bf9\u8bad\u7ec3 < Elo-Evolve\uff0c\u5728Alpaca Eval 2.0\u548cMT-Bench\u4e0a\u9a8c\u8bc1\u4e86\u6210\u5bf9\u6bd4\u8f83\u548c\u52a8\u6001\u5bf9\u624b\u9009\u62e9\u7684\u6e10\u8fdb\u4f18\u52bf\u3002", "conclusion": "Elo-Evolve\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7ade\u4e89\u548c\u81ea\u9002\u5e94\u5bf9\u624b\u9009\u62e9\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u5bf9\u9f50\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e864.5\u500d\u7684\u566a\u58f0\u51cf\u5c11\u548c\u66f4\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2602.14337", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14337", "abs": "https://arxiv.org/abs/2602.14337", "authors": ["Yukang Feng", "Jianwen Sun", "Zelai Yang", "Jiaxin Ai", "Chuanhao Li", "Zizhen Li", "Fanrui Zhang", "Kang He", "Rui Ma", "Jifan Lin", "Jie Sun", "Yang Xiao", "Sizhuo Zhou", "Wenxiao Wu", "Yiming Liu", "Pengfei Liu", "Yu Qiao", "Shenglin Zhang", "Kaipeng Zhang"], "title": "LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces", "comment": null, "summary": "Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance.", "AI": {"tldr": "LongCLI-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u957f\u65f6\u7a0bCLI\u4efb\u52a1\u57fa\u51c6\uff0c\u4ece1000\u591a\u4e2a\u8ba1\u7b97\u673a\u79d1\u5b66\u4f5c\u4e1a\u548c\u771f\u5b9e\u5de5\u4f5c\u6d41\u4e2d\u7cbe\u900920\u4e2a\u9ad8\u8d28\u91cf\u4efb\u52a1\uff0c\u6db5\u76d6\u56db\u79cd\u5de5\u7a0b\u7c7b\u522b\uff0c\u91c7\u7528\u53cc\u91cd\u6d4b\u8bd5\u534f\u8bae\u548c\u6b65\u9aa4\u7ea7\u8bc4\u5206\uff0c\u53d1\u73b0\u73b0\u6709SOTA\u4ee3\u7406\u6210\u529f\u7387\u4f4e\u4e8e20%\uff0c\u65e9\u671f\u9636\u6bb5\u5e38\u51fa\u73b0\u5173\u952e\u5931\u8d25\u3002", "motivation": "\u73b0\u6709AI\u8f85\u52a9\u7f16\u7a0b\u57fa\u51c6\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a\u4efb\u52a1\u65f6\u7a0b\u77ed\u3001GitHub\u6570\u636e\u6c61\u67d3\u3001\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u73b0\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u6240\u9700\u7684\u957f\u65f6\u7a0b\u89c4\u5212\u548c\u6267\u884c\u80fd\u529b\u3002", "method": "\u4ece1000\u591a\u4e2a\u8ba1\u7b97\u673a\u79d1\u5b66\u4f5c\u4e1a\u548c\u771f\u5b9e\u5de5\u4f5c\u6d41\u4e2d\u7cbe\u900920\u4e2a\u9ad8\u8d28\u91cf\u957f\u65f6\u7a0b\u4efb\u52a1\uff0c\u6db5\u76d6\u56db\u79cd\u5de5\u7a0b\u7c7b\u522b\uff08\u4ece\u96f6\u5f00\u59cb\u3001\u529f\u80fd\u6dfb\u52a0\u3001\u9519\u8bef\u4fee\u590d\u3001\u91cd\u6784\uff09\u3002\u63d0\u51fa\u53cc\u91cd\u6d4b\u8bd5\u534f\u8bae\uff1a\u9700\u6c42\u6ee1\u8db3\u5ea6\uff08fail-to-pass\uff09\u548c\u56de\u5f52\u907f\u514d\uff08pass-to-pass\uff09\uff0c\u5e76\u91c7\u7528\u6b65\u9aa4\u7ea7\u8bc4\u5206\u7cbe\u786e\u5b9a\u4f4d\u6267\u884c\u5931\u8d25\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u5728LongCLI-Bench\u4e2d\u901a\u8fc7\u7387\u4e5f\u4f4e\u4e8e20%\u3002\u6b65\u9aa4\u7ea7\u5206\u6790\u8868\u660e\u5927\u591a\u6570\u4efb\u52a1\u5728\u5b8c\u6210\u5ea6\u4e0d\u523030%\u65f6\u5c31\u505c\u6ede\uff0c\u5173\u952e\u5931\u8d25\u5e38\u53d1\u751f\u5728\u65e9\u671f\u9636\u6bb5\u3002\u81ea\u6211\u7ea0\u6b63\u4ec5\u5e26\u6765\u8fb9\u9645\u6539\u5584\uff0c\u800c\u901a\u8fc7\u8ba1\u5212\u6ce8\u5165\u548c\u4ea4\u4e92\u6307\u5bfc\u7684\u4eba\u673a\u534f\u4f5c\u80fd\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5fc5\u987b\u5f3a\u8c03\u534f\u540c\u4eba\u673a\u5de5\u4f5c\u6d41\u7684\u5f00\u53d1\uff0c\u540c\u65f6\u63d0\u5347\u4ee3\u7406\u7684\u89c4\u5212\u548c\u6267\u884c\u80fd\u529b\uff0c\u4ee5\u514b\u670d\u957f\u65f6\u7a0b\u4efb\u52a1\u6027\u80fd\u7684\u5173\u952e\u6311\u6218\u3002", "topic": "swe benchmark"}}
{"id": "2602.14572", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.14572", "abs": "https://arxiv.org/abs/2602.14572", "authors": ["Pooya Rostami Mazrae", "Alexandre Decan", "Tom Mens", "Mairieli Wessel"], "title": "An Empirical Study of the Evolution of GitHub Actions Workflows", "comment": null, "summary": "CI/CD practices play a significant role during collaborative software development by automating time-consuming and repetitive tasks such as testing, building, quality checking, dependency and security management. GitHub Actions, the CI/CD tool integrated into GitHub, allows repository maintainers to automate development workflows. We conducted a mixed methods analysis of GitHub Actions workflow changes over time. Through a preliminary qualitative analysis of 439 modified workflow files we identified seven types of conceptual changes to workflows. Next, we performed a quantitative analysis over 49K+ GitHub repositories totaling 267K+ workflow change histories and 3.4M+ workflow file versions from November 2019 to August 2025. This analysis revealed that repositories contain a median of three workflow files, and 7.3% of all workflow files are being changed every week. The changes made to workflows tend to be small, with about three-quarters containing only a single change. The large majority of the observed changes have to do with task configuration and task specification in workflow jobs. We did not find any conclusive evidence of the effect of LLM coding tools or other major technological changes on workflow creation and workflow maintenance frequency. Our findings highlight the need for improved tooling to support fine-grained maintenance tasks, such as a broader adoption of dependency management and AI-based support for ensuring and sustaining workflow security and quality.", "AI": {"tldr": "\u5bf9GitHub Actions\u5de5\u4f5c\u6d41\u53d8\u66f4\u7684\u6df7\u5408\u65b9\u6cd5\u5206\u6790\uff1a\u901a\u8fc7\u5b9a\u6027\u5206\u6790439\u4e2a\u4fee\u6539\u7684\u5de5\u4f5c\u6d41\u6587\u4ef6\u8bc6\u522b7\u79cd\u6982\u5ff5\u53d8\u66f4\u7c7b\u578b\uff0c\u5b9a\u91cf\u5206\u679049K+\u4ed3\u5e93\u7684267K+\u53d8\u66f4\u5386\u53f2\u548c3.4M+\u5de5\u4f5c\u6d41\u6587\u4ef6\u7248\u672c\uff0c\u53d1\u73b0\u4ed3\u5e93\u4e2d\u4f4d\u6570\u4e3a3\u4e2a\u5de5\u4f5c\u6d41\u6587\u4ef6\uff0c\u6bcf\u54687.3%\u7684\u6587\u4ef6\u88ab\u4fee\u6539\uff0c\u53d8\u66f4\u901a\u5e38\u8f83\u5c0f\uff0c\u4e3b\u8981\u6d89\u53ca\u4efb\u52a1\u914d\u7f6e\u548c\u89c4\u8303\uff0c\u672a\u53d1\u73b0LLM\u5de5\u5177\u5bf9\u5de5\u4f5c\u6d41\u521b\u5efa\u548c\u7ef4\u62a4\u9891\u7387\u7684\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76GitHub Actions\u4f5c\u4e3aGitHub\u96c6\u6210\u7684CI/CD\u5de5\u5177\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u6f14\u53d8\u6a21\u5f0f\uff0c\u4e86\u89e3\u5de5\u4f5c\u6d41\u6587\u4ef6\u7684\u53d8\u66f4\u9891\u7387\u3001\u7c7b\u578b\u548c\u89c4\u6a21\uff0c\u63a2\u7d22\u81ea\u52a8\u5316\u5f00\u53d1\u5de5\u4f5c\u6d41\u7684\u7ef4\u62a4\u5b9e\u8df5\u548c\u6311\u6218\u3002", "method": "\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff1a1\uff09\u5bf9439\u4e2a\u4fee\u6539\u7684\u5de5\u4f5c\u6d41\u6587\u4ef6\u8fdb\u884c\u521d\u6b65\u5b9a\u6027\u5206\u6790\uff0c\u8bc6\u522b7\u79cd\u6982\u5ff5\u53d8\u66f4\u7c7b\u578b\uff1b2\uff09\u5bf92019\u5e7411\u6708\u81f32025\u5e748\u6708\u671f\u95f449K+ GitHub\u4ed3\u5e93\u7684267K+\u5de5\u4f5c\u6d41\u53d8\u66f4\u5386\u53f2\u548c3.4M+\u5de5\u4f5c\u6d41\u6587\u4ef6\u7248\u672c\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u3002", "result": "1\uff09\u4ed3\u5e93\u4e2d\u4f4d\u6570\u4e3a3\u4e2a\u5de5\u4f5c\u6d41\u6587\u4ef6\uff1b2\uff09\u6bcf\u54687.3%\u7684\u5de5\u4f5c\u6d41\u6587\u4ef6\u88ab\u4fee\u6539\uff1b3\uff09\u7ea6\u56db\u5206\u4e4b\u4e09\u7684\u53d8\u66f4\u53ea\u5305\u542b\u5355\u4e2a\u4fee\u6539\uff1b4\uff09\u5927\u591a\u6570\u53d8\u66f4\u6d89\u53ca\u5de5\u4f5c\u6d41\u4f5c\u4e1a\u4e2d\u7684\u4efb\u52a1\u914d\u7f6e\u548c\u4efb\u52a1\u89c4\u8303\uff1b5\uff09\u672a\u53d1\u73b0LLM\u7f16\u7801\u5de5\u5177\u6216\u5176\u4ed6\u91cd\u5927\u6280\u672f\u53d8\u9769\u5bf9\u5de5\u4f5c\u6d41\u521b\u5efa\u548c\u7ef4\u62a4\u9891\u7387\u7684\u660e\u786e\u5f71\u54cd\u3002", "conclusion": "\u9700\u8981\u6539\u8fdb\u5de5\u5177\u652f\u6301\u7ec6\u7c92\u5ea6\u7ef4\u62a4\u4efb\u52a1\uff0c\u5305\u62ec\u66f4\u5e7f\u6cdb\u91c7\u7528\u4f9d\u8d56\u7ba1\u7406\u548c\u57fa\u4e8eAI\u7684\u652f\u6301\uff0c\u4ee5\u786e\u4fdd\u548c\u7ef4\u6301\u5de5\u4f5c\u6d41\u7684\u5b89\u5168\u6027\u548c\u8d28\u91cf\u3002", "topic": "swe application"}}
{"id": "2602.13240", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13240", "abs": "https://arxiv.org/abs/2602.13240", "authors": ["Roham Koohestani", "Ali Al-Kaswan", "Jonathan Katzy", "Maliheh Izadi"], "title": "AST-PAC: AST-guided Membership Inference for Code", "comment": null, "summary": "Code Large Language Models are frequently trained on massive datasets containing restrictively licensed source code. This creates urgent data governance and copyright challenges. Membership Inference Attacks (MIAs) can serve as an auditing mechanism to detect unauthorized data usage in models. While attacks like the Loss Attack provide a baseline, more involved methods like Polarized Augment Calibration (PAC) remain underexplored in the code domain. This paper presents an exploratory study evaluating these methods on 3B--7B parameter code models. We find that while PAC generally outperforms the Loss baseline, its effectiveness relies on augmentation strategies that disregard the rigid syntax of code, leading to performance degradation on larger, complex files. To address this, we introduce AST-PAC, a domain-specific adaptation that utilizes Abstract Syntax Tree (AST) based perturbations to generate syntactically valid calibration samples. Preliminary results indicate that AST-PAC improves as syntactic size grows, where PAC degrades, but under-mutates small files and underperforms on alphanumeric-rich code. Overall, the findings motivate future work on syntax-aware and size-adaptive calibration as a prerequisite for reliable provenance auditing of code language models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4e86\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u62bd\u8c61\u8bed\u6cd5\u6811\u7684\u6539\u8fdb\u65b9\u6cd5AST-PAC\uff0c\u7528\u4e8e\u68c0\u6d4b\u672a\u7ecf\u6388\u6743\u7684\u8bad\u7ec3\u6570\u636e\u4f7f\u7528\u3002", "motivation": "\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u5e38\u5728\u53d7\u9650\u8bb8\u53ef\u7684\u5927\u89c4\u6a21\u4ee3\u7801\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u8fd9\u5e26\u6765\u4e86\u6570\u636e\u6cbb\u7406\u548c\u7248\u6743\u6311\u6218\u3002\u6210\u5458\u63a8\u7406\u653b\u51fb\u53ef\u4f5c\u4e3a\u5ba1\u8ba1\u673a\u5236\u68c0\u6d4b\u6a21\u578b\u4e2d\u7684\u672a\u6388\u6743\u6570\u636e\u4f7f\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4ee3\u7801\u9886\u57df\u6548\u679c\u6709\u9650\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86Loss Attack\u548cPolarized Augment Calibration\u7b49\u65b9\u6cd5\u57283B-7B\u53c2\u6570\u4ee3\u7801\u6a21\u578b\u4e0a\u7684\u6548\u679c\u3002\u9488\u5bf9PAC\u65b9\u6cd5\u5728\u4ee3\u7801\u8bed\u6cd5\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86AST-PAC\u65b9\u6cd5\uff0c\u5229\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\u6270\u52a8\u751f\u6210\u8bed\u6cd5\u6709\u6548\u7684\u6821\u51c6\u6837\u672c\u3002", "result": "PAC\u901a\u5e38\u4f18\u4e8eLoss\u57fa\u51c6\uff0c\u4f46\u5176\u6548\u679c\u4f9d\u8d56\u4e8e\u5ffd\u7565\u4ee3\u7801\u4e25\u683c\u8bed\u6cd5\u7684\u589e\u5f3a\u7b56\u7565\uff0c\u5bfc\u81f4\u5728\u5927\u578b\u590d\u6742\u6587\u4ef6\u4e0a\u6027\u80fd\u4e0b\u964d\u3002AST-PAC\u5728\u8bed\u6cd5\u89c4\u6a21\u589e\u5927\u65f6\u8868\u73b0\u6539\u5584\uff0c\u4f46\u5728\u5c0f\u6587\u4ef6\u548c\u5b57\u6bcd\u6570\u5b57\u4e30\u5bcc\u4ee3\u7801\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u9700\u8981\u8bed\u6cd5\u611f\u77e5\u548c\u89c4\u6a21\u81ea\u9002\u5e94\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u6765\u6e90\u5ba1\u8ba1\u7684\u524d\u63d0\u6761\u4ef6\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "topic": "code agent"}}
{"id": "2602.13713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13713", "abs": "https://arxiv.org/abs/2602.13713", "authors": ["Maciej Uberna", "Micha\u0142 Wawer", "Jaros\u0142aw A. Chudziak", "Marcin Koszowy"], "title": "On Theoretically-Driven LLM Agents for Multi-Dimensional Discourse Analysis", "comment": "8 pages, 4 figures, 3 tables. This is the accepted version of the paper presented at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026), Marbella, Spain", "summary": "Identifying the strategic uses of reformulation in discourse remains a key challenge for computational argumentation. While LLMs can detect surface-level similarity, they often fail to capture the pragmatic functions of rephrasing, such as its role within rhetorical discourse. This paper presents a comparative multi-agent framework designed to quantify the benefits of incorporating explicit theoretical knowledge for this task. We utilise an dataset of annotated political debates to establish a new standard encompassing four distinct rephrase functions: Deintensification, Intensification, Specification, Generalisation, and Other, which covers all remaining types (D-I-S-G-O). We then evaluate two parallel LLM-based agent systems: one enhanced by argumentation theory via Retrieval-Augmented Generation (RAG), and an identical zero-shot baseline. The results reveal a clear performance gap: the RAG-enhanced agents substantially outperform the baseline across the board, with particularly strong advantages in detecting Intensification and Generalisation context, yielding an overall Macro F1-score improvement of nearly 30\\%. Our findings provide evidence that theoretical grounding is not only beneficial but essential for advancing beyond mere paraphrase detection towards function-aware analysis of argumentative discourse. This comparative multi-agent architecture represents a step towards scalable, theoretically informed computational tools capable of identifying rhetorical strategies in contemporary discourse.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5165\u8bba\u8bc1\u7406\u8bba\u77e5\u8bc6\u6765\u8bc6\u522b\u8bdd\u8bed\u4e2d\u7684\u91cd\u8ff0\u7b56\u7565\u529f\u80fd\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u5728\u653f\u6cbb\u8fa9\u8bba\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u8fd130%\u7684\u5b8fF1\u5206\u6570\u63d0\u5347\u3002", "motivation": "\u73b0\u6709LLM\u5728\u8bc6\u522b\u8bdd\u8bed\u91cd\u8ff0\u65f6\u53ea\u80fd\u68c0\u6d4b\u8868\u9762\u76f8\u4f3c\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u91cd\u8ff0\u7684\u8bed\u7528\u529f\u80fd\uff08\u5982\u4fee\u8f9e\u4f5c\u7528\uff09\u3002\u9700\u8981\u5c06\u7406\u8bba\u77e5\u8bc6\u4e0e\u8ba1\u7b97\u6a21\u578b\u7ed3\u5408\uff0c\u8d85\u8d8a\u7b80\u5355\u7684\u590d\u8ff0\u68c0\u6d4b\uff0c\u5b9e\u73b0\u529f\u80fd\u611f\u77e5\u7684\u8bba\u8bc1\u8bdd\u8bed\u5206\u6790\u3002", "method": "1. \u6784\u5efa\u5305\u542b\u653f\u6cbb\u8fa9\u8bba\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u5b9a\u4e49\u4e94\u79cd\u91cd\u8ff0\u529f\u80fd\uff1a\u5f31\u5316\u3001\u5f3a\u5316\u3001\u5177\u4f53\u5316\u3001\u6cdb\u5316\u3001\u5176\u4ed6\uff08D-I-S-G-O\uff09\n2. \u8bbe\u8ba1\u5e76\u884c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff1a\u57fa\u4e8eRAG\u878d\u5165\u8bba\u8bc1\u7406\u8bba\u7684\u667a\u80fd\u4f53 vs. \u96f6\u6837\u672c\u57fa\u7ebf\u667a\u80fd\u4f53\n3. \u4f7f\u7528\u6bd4\u8f83\u6846\u67b6\u91cf\u5316\u7406\u8bba\u77e5\u8bc6\u7684\u6548\u76ca", "result": "RAG\u589e\u5f3a\u667a\u80fd\u4f53\u5728\u6240\u6709\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u68c0\u6d4b\u5f3a\u5316\u548c\u6cdb\u5316\u4e0a\u4e0b\u6587\u65b9\u9762\u4f18\u52bf\u660e\u663e\u3002\u6574\u4f53\u5b8fF1\u5206\u6570\u63d0\u5347\u8fd130%\uff0c\u8bc1\u660e\u7406\u8bba\u57fa\u7840\u7684\u878d\u5165\u5bf9\u529f\u80fd\u8bc6\u522b\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7406\u8bba\u6839\u57fa\u4e0d\u4ec5\u662f\u6709\u76ca\u7684\uff0c\u800c\u4e14\u662f\u5b9e\u73b0\u8d85\u8d8a\u7b80\u5355\u590d\u8ff0\u68c0\u6d4b\u3001\u8fc8\u5411\u529f\u80fd\u611f\u77e5\u8bba\u8bc1\u8bdd\u8bed\u5206\u6790\u7684\u5fc5\u8981\u6761\u4ef6\u3002\u8be5\u591a\u667a\u80fd\u4f53\u67b6\u6784\u4e3a\u5f00\u53d1\u53ef\u6269\u5c55\u3001\u7406\u8bba\u77e5\u60c5\u7684\u8ba1\u7b97\u5de5\u5177\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2602.14591", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14591", "abs": "https://arxiv.org/abs/2602.14591", "authors": ["Evgenii Kniazev"], "title": "Automated Classification of Source Code Changes Based on Metrics Clustering in the Software Development Process", "comment": "This is an English translation of the author's Ph.D. dissertation abstract, originally defended in Russian at ITMO University (2009) under the supervision of Prof. A.A. Shalyto. The original research was co-authored with D.G. Shopyrin. Original available at https://is.ifmo.ru/disser/knyazev_autorefer.pdf", "summary": "This paper presents an automated method for classifying source code changes during the software development process based on clustering of change metrics. The method consists of two steps: clustering of metric vectors computed for each code change, followed by expert mapping of the resulting clusters to predefined change classes. The distribution of changes into clusters is performed automatically, while the mapping of clusters to classes is carried out by an expert. Automation of the distribution step substantially reduces the time required for code change review. The k-means algorithm with a cosine similarity measure between metric vectors is used for clustering. Eleven source code metrics are employed, covering lines of code, cyclomatic complexity, file counts, interface changes, and structural changes. The method was validated on five software systems, including two open-source projects (Subversion and NHibernate), and demonstrated classification purity of P_C = 0.75 +/- 0.05 and entropy of E_C = 0.37 +/- 0.06 at a significance level of 0.05.", "AI": {"tldr": "\u57fa\u4e8e\u4ee3\u7801\u53d8\u66f4\u6307\u6807\u805a\u7c7b\u7684\u81ea\u52a8\u5316\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7k-means\u7b97\u6cd5\u548c11\u4e2a\u6e90\u4ee3\u7801\u6307\u6807\u5b9e\u73b0\u53d8\u66f4\u5206\u7c7b\uff0c\u4e13\u5bb6\u53ea\u9700\u5c06\u805a\u7c7b\u7ed3\u679c\u6620\u5c04\u5230\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u663e\u8457\u51cf\u5c11\u4ee3\u7801\u5ba1\u67e5\u65f6\u95f4\u3002", "motivation": "\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u9700\u8981\u5927\u91cf\u65f6\u95f4\u8fdb\u884c\u4ee3\u7801\u53d8\u66f4\u5ba1\u67e5\uff0c\u4f20\u7edf\u624b\u52a8\u5206\u7c7b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u51cf\u5c11\u4ee3\u7801\u53d8\u66f4\u5206\u7c7b\u6240\u9700\u7684\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u5ba1\u67e5\u6548\u7387\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4f7f\u7528k-means\u7b97\u6cd5\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5bf9\u6bcf\u4e2a\u4ee3\u7801\u53d8\u66f4\u768411\u4e2a\u6307\u6807\u5411\u91cf\u8fdb\u884c\u805a\u7c7b\uff1b2) \u4e13\u5bb6\u5c06\u805a\u7c7b\u7ed3\u679c\u6620\u5c04\u5230\u9884\u5b9a\u4e49\u7684\u53d8\u66f4\u7c7b\u522b\u3002\u6307\u6807\u5305\u62ec\u4ee3\u7801\u884c\u6570\u3001\u5708\u590d\u6742\u5ea6\u3001\u6587\u4ef6\u6570\u91cf\u3001\u63a5\u53e3\u53d8\u66f4\u548c\u7ed3\u6784\u53d8\u66f4\u7b49\u3002", "result": "\u57285\u4e2a\u8f6f\u4ef6\u7cfb\u7edf\uff08\u5305\u62ecSubversion\u548cNHibernate\u4e24\u4e2a\u5f00\u6e90\u9879\u76ee\uff09\u4e0a\u9a8c\u8bc1\uff0c\u5206\u7c7b\u7eaf\u5ea6P_C = 0.75 \u00b1 0.05\uff0c\u71b5E_C = 0.37 \u00b1 0.06\uff08\u663e\u8457\u6027\u6c34\u5e730.05\uff09\uff0c\u8868\u660e\u65b9\u6cd5\u6709\u6548\u4e14\u5206\u7c7b\u8d28\u91cf\u826f\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u4ee3\u7801\u53d8\u66f4\u5206\u7c7b\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c11\u4ee3\u7801\u5ba1\u67e5\u65f6\u95f4\uff0c\u901a\u8fc7\u805a\u7c7b\u548c\u4e13\u5bb6\u6620\u5c04\u7684\u7ed3\u5408\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u53d8\u66f4\u5206\u7c7b\uff0c\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u81ea\u52a8\u5316\u5de5\u5177\u3002", "topic": "swe application"}}
{"id": "2602.14595", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.14595", "abs": "https://arxiv.org/abs/2602.14595", "authors": ["Shirin Pirouzkhah", "Souhaila Serbout", "Alberto Bacchelli"], "title": "Consistent or Sensitive? Automated Code Revision Tools Against Semantics-Preserving Perturbations", "comment": null, "summary": "Automated Code Revision (ACR) tools aim to reduce manual effort by automatically generating code revisions based on reviewer feedback. While ACR tools have shown promising performance on historical data, their real-world utility depends on their ability to handle similar code variants expressing the same issue - a property we define as consistency. However, the probabilistic nature of ACR tools often compromises consistency, which may lead to divergent revisions even for semantically equivalent code variants. In this paper, we investigate the extent to which ACR tools maintain consistency when presented with semantically equivalent code variants. To do so, we first designed nine types of semantics-preserving perturbations (SPP) and applied them to 2032 Java methods from real-world GitHub projects, generating over 10K perturbed variants for evaluation. Then we used these perturbations to evaluate the consistency of five state-of-the-art transformer-based ACR tools. We found that the ACR tools' ability to generate correct revisions can drop by up to 45.3%, when presented with semantically equivalent code. The closer the perturbation is to this targeted region, the more likely an ACR tool is to fail to generate the correct revision. We explored potential mitigation strategies that modify the input representation, but found that these attention-guiding heuristics yielded only marginal improvements, thus leaving the solution to this problem as an open research question.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u81ea\u52a8\u4ee3\u7801\u4fee\u8ba2\u5de5\u5177\u5728\u8bed\u4e49\u7b49\u4ef7\u4ee3\u7801\u53d8\u4f53\u4e0a\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u5373\u4f7f\u4ee3\u7801\u8bed\u4e49\u76f8\u540c\uff0cACR\u5de5\u5177\u751f\u6210\u6b63\u786e\u4fee\u8ba2\u7684\u80fd\u529b\u53ef\u80fd\u4e0b\u964d\u9ad8\u8fbe45.3%\uff0c\u4e14\u76ee\u524d\u7f13\u89e3\u7b56\u7565\u6548\u679c\u6709\u9650\u3002", "motivation": "\u81ea\u52a8\u4ee3\u7801\u4fee\u8ba2\u5de5\u5177\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u53d6\u51b3\u4e8e\u5176\u5904\u7406\u8bed\u4e49\u7b49\u4ef7\u4ee3\u7801\u53d8\u4f53\u7684\u80fd\u529b\uff0c\u5373\u4e00\u81f4\u6027\u3002\u7136\u800c\uff0cACR\u5de5\u5177\u7684\u6982\u7387\u6027\u8d28\u5e38\u5e38\u635f\u5bb3\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u5bf9\u76f8\u540c\u95ee\u9898\u7684\u4e0d\u540c\u4ee3\u7801\u53d8\u4f53\u4ea7\u751f\u5206\u6b67\u7684\u4fee\u8ba2\u3002", "method": "\u8bbe\u8ba1\u4e869\u79cd\u8bed\u4e49\u4fdd\u6301\u6270\u52a8\u7c7b\u578b\uff0c\u5e94\u7528\u4e8e2032\u4e2a\u771f\u5b9eGitHub\u9879\u76ee\u7684Java\u65b9\u6cd5\uff0c\u751f\u6210\u8d85\u8fc710K\u4e2a\u6270\u52a8\u53d8\u4f53\u8fdb\u884c\u8bc4\u4f30\u3002\u4f7f\u7528\u8fd9\u4e9b\u6270\u52a8\u8bc4\u4f30\u4e865\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u4e8eTransformer\u7684ACR\u5de5\u5177\u7684\u4e00\u81f4\u6027\u3002", "result": "ACR\u5de5\u5177\u5728\u8bed\u4e49\u7b49\u4ef7\u4ee3\u7801\u4e0a\u751f\u6210\u6b63\u786e\u4fee\u8ba2\u7684\u80fd\u529b\u53ef\u80fd\u4e0b\u964d\u9ad8\u8fbe45.3%\u3002\u6270\u52a8\u8d8a\u63a5\u8fd1\u76ee\u6807\u533a\u57df\uff0cACR\u5de5\u5177\u8d8a\u53ef\u80fd\u65e0\u6cd5\u751f\u6210\u6b63\u786e\u4fee\u8ba2\u3002\u5c1d\u8bd5\u7684\u6ce8\u610f\u529b\u5f15\u5bfc\u542f\u53d1\u5f0f\u7f13\u89e3\u7b56\u7565\u4ec5\u5e26\u6765\u8fb9\u9645\u6539\u8fdb\u3002", "conclusion": "ACR\u5de5\u5177\u5728\u5904\u7406\u8bed\u4e49\u7b49\u4ef7\u4ee3\u7801\u53d8\u4f53\u65f6\u5b58\u5728\u4e25\u91cd\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5f53\u524d\u7f13\u89e3\u7b56\u7565\u6548\u679c\u6709\u9650\uff0c\u8fd9\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2602.13255", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.13255", "abs": "https://arxiv.org/abs/2602.13255", "authors": ["Najmul Hasan", "Prashanth BusiReddyGari"], "title": "DPBench: Large Language Models Struggle with Simultaneous Coordination", "comment": "13 pages, 4 figures", "summary": "Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.", "AI": {"tldr": "DPBench\u662f\u4e00\u4e2a\u57fa\u4e8e\u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u8d44\u6e90\u7ade\u4e89\u4e0b\u7684\u534f\u8c03\u80fd\u529b\uff0c\u53d1\u73b0LLM\u5728\u987a\u5e8f\u51b3\u7b56\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u540c\u65f6\u51b3\u7b56\u65f6\u6b7b\u9501\u7387\u8d85\u8fc795%", "motivation": "\u968f\u7740LLM\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u76ee\u524d\u7f3a\u4e4f\u6d4b\u8bd5\u5b83\u4eec\u5728\u8d44\u6e90\u7ade\u4e89\u4e0b\u534f\u8c03\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9700\u8981\u8bc4\u4f30LLM\u5728\u5e76\u53d1\u8d44\u6e90\u8bbf\u95ee\u573a\u666f\u4e2d\u7684\u534f\u8c03\u8868\u73b0", "method": "\u57fa\u4e8e\u54f2\u5b66\u5bb6\u5c31\u9910\u95ee\u9898\u8bbe\u8ba1DPBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b8\u79cd\u4e0d\u540c\u6761\u4ef6\uff08\u51b3\u7b56\u65f6\u673a\u3001\u7fa4\u4f53\u89c4\u6a21\u3001\u901a\u4fe1\uff09\uff0c\u6d4b\u8bd5GPT-5.2\u3001Claude Opus 4.5\u548cGrok 4.1\u7b49LLM\u7684\u534f\u8c03\u80fd\u529b", "result": "LLM\u5728\u987a\u5e8f\u51b3\u7b56\u8bbe\u7f6e\u4e2d\u80fd\u6709\u6548\u534f\u8c03\uff0c\u4f46\u5728\u9700\u8981\u540c\u65f6\u51b3\u7b56\u65f6\u5931\u8d25\u4e25\u91cd\uff0c\u6b7b\u9501\u7387\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u8d85\u8fc795%\uff1b\u901a\u4fe1\u4e0d\u4ec5\u4e0d\u80fd\u89e3\u51b3\u95ee\u9898\uff0c\u53cd\u800c\u53ef\u80fd\u589e\u52a0\u6b7b\u9501\u7387", "conclusion": "\u9700\u8981\u5e76\u53d1\u8d44\u6e90\u8bbf\u95ee\u7684\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u53ef\u80fd\u9700\u8981\u5916\u90e8\u534f\u8c03\u673a\u5236\uff0c\u800c\u4e0d\u80fd\u4f9d\u8d56LLM\u81ea\u53d1\u7684\u534f\u8c03\u80fd\u529b\uff1bDPBench\u4f5c\u4e3a\u5f00\u6e90\u57fa\u51c6\u53d1\u5e03", "topic": "agent analysis"}}
{"id": "2602.13258", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.13258", "abs": "https://arxiv.org/abs/2602.13258", "authors": ["Deepak Babu Piskala"], "title": "MAPLE: A Sub-Agent Architecture for Memory, Learning, and Personalization in Agentic AI Systems", "comment": "12 pages, 5 figures. Accepted to ALA Workshop at AAMAS 2026. Code: [](https://github.com/prdeepakbabu/maple-framework)<https://github.com/prdeepakbabu/maple-framework>", "summary": "Large language model (LLM) agents have emerged as powerful tools for complex tasks, yet their ability to adapt to individual users remains fundamentally limited. We argue this limitation stems from a critical architectural conflation: current systems treat memory, learning, and personalization as a unified capability rather than three distinct mechanisms requiring different infrastructure, operating on different timescales, and benefiting from independent optimization. We propose MAPLE (Memory-Adaptive Personalized LEarning), a principled decomposition where Memory handles storage and retrieval infrastructure; Learning extracts intelligence from accumulated interactions asynchronously; and Personalization applies learned knowledge in real-time within finite context budgets. Each component operates as a dedicated sub-agent with specialized tooling and well-defined interfaces. Experimental evaluation on the MAPLE-Personas benchmark demonstrates that our decomposition achieves a 14.6% improvement in personalization score compared to a stateless baseline (p < 0.01, Cohen's d = 0.95) and increases trait incorporation rate from 45% to 75% -- enabling agents that genuinely learn and adapt.", "AI": {"tldr": "MAPLE\u63d0\u51fa\u5c06LLM\u4ee3\u7406\u7684\u8bb0\u5fc6\u3001\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u5206\u89e3\u4e3a\u4e09\u4e2a\u72ec\u7acb\u7ec4\u4ef6\uff0c\u901a\u8fc7\u4e13\u95e8\u5316\u67b6\u6784\u5b9e\u73b0\u66f4\u597d\u7684\u7528\u6237\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u7cfb\u7edf\u5c06\u8bb0\u5fc6\u3001\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u89c6\u4e3a\u7edf\u4e00\u80fd\u529b\uff0c\u8fd9\u79cd\u67b6\u6784\u6df7\u6dc6\u9650\u5236\u4e86\u4ee3\u7406\u5bf9\u4e2a\u4f53\u7528\u6237\u7684\u9002\u5e94\u80fd\u529b\u3002\u9700\u8981\u5c06\u8fd9\u4e9b\u673a\u5236\u5206\u89e3\u4e3a\u72ec\u7acb\u7ec4\u4ef6\uff0c\u5404\u81ea\u62e5\u6709\u4e0d\u540c\u57fa\u7840\u8bbe\u65bd\u3001\u65f6\u95f4\u5c3a\u5ea6\u548c\u4f18\u5316\u7b56\u7565\u3002", "method": "\u63d0\u51faMAPLE\u6846\u67b6\uff0c\u5c06\u7cfb\u7edf\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u7528\u5b50\u4ee3\u7406\uff1aMemory\u8d1f\u8d23\u5b58\u50a8\u548c\u68c0\u7d22\u57fa\u7840\u8bbe\u65bd\uff1bLearning\u4ece\u7d2f\u79ef\u4ea4\u4e92\u4e2d\u5f02\u6b65\u63d0\u53d6\u667a\u80fd\uff1bPersonalization\u5728\u6709\u9650\u4e0a\u4e0b\u6587\u9884\u7b97\u5185\u5b9e\u65f6\u5e94\u7528\u5b66\u4e60\u5230\u7684\u77e5\u8bc6\u3002\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u6709\u4e13\u95e8\u5de5\u5177\u548c\u660e\u786e\u5b9a\u4e49\u7684\u63a5\u53e3\u3002", "result": "\u5728MAPLE-Personas\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u5206\u89e3\u65b9\u6cd5\u76f8\u6bd4\u65e0\u72b6\u6001\u57fa\u7ebf\u5b9e\u73b0\u4e8614.6%\u7684\u4e2a\u6027\u5316\u5206\u6570\u63d0\u5347\uff08p < 0.01, Cohen's d = 0.95\uff09\uff0c\u5e76\u5c06\u7279\u5f81\u878d\u5165\u7387\u4ece45%\u63d0\u9ad8\u523075%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8bb0\u5fc6\u3001\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u5206\u89e3\u4e3a\u72ec\u7acb\u7ec4\u4ef6\uff0cMAPLE\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u4e2a\u6027\u5316\u9002\u5e94\u80fd\u529b\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u771f\u6b63\u5b66\u4e60\u548c\u9002\u5e94\u7528\u6237\u3002", "topic": "agent analysis"}}
{"id": "2602.14690", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.14690", "abs": "https://arxiv.org/abs/2602.14690", "authors": ["Matthias Galster", "Seyedmoein Mohsenimofidi", "Jai Lal Lulla", "Muhammad Auwal Abubakar", "Christoph Treude", "Sebastian Baltes"], "title": "Configuring Agentic AI Coding Tools: An Exploratory Study", "comment": "9 pages, 7 figures, 3 tables", "summary": "Agentic AI coding tools with autonomous capabilities beyond conversational content generation increasingly automate repetitive and time-consuming software development tasks. Developers can configure these tools through versioned repository-level artifacts such as Markdown and JSON files. In this paper, we present a systematic analysis of configuration mechanisms for agentic AI coding tools, covering Claude Code, GitHub Copilot, Cursor, Gemini, and Codex. We identify eight configuration mechanisms and, in an empirical study of 2,926 GitHub repositories, examine whether and how they are adopted. We then explore Context Files, Skills, and Subagents, that is, three mechanisms available across tools, in more detail. Our findings reveal three trends. First, Context Files dominate the configuration landscape and are often the sole mechanism in a repository, with AGENTS$.$md emerging as an interoperable standard across tools. Second, advanced mechanisms such as Skills and Subagents are only shallowly adopted: most repositories define only one or two artifacts, and Skills predominantly rely on static instructions rather than executable workflows. Third, distinct configuration cultures are forming around different tools, with Claude Code users employing the broadest range of mechanisms. These findings establish an empirical baseline for longitudinal and experimental research on how configuration strategies evolve and affect agent performance as agentic AI coding tools mature.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86Claude Code\u3001GitHub Copilot\u7b49\u4ee3\u7406\u5f0fAI\u7f16\u7801\u5de5\u5177\u7684\u914d\u7f6e\u673a\u5236\uff0c\u901a\u8fc7\u5bf92926\u4e2aGitHub\u4ed3\u5e93\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0Context Files\u4e3b\u5bfc\u914d\u7f6e\uff0c\u9ad8\u7ea7\u673a\u5236\u91c7\u7528\u8f83\u6d45\uff0c\u4e0d\u540c\u5de5\u5177\u5f62\u6210\u4e0d\u540c\u914d\u7f6e\u6587\u5316\u3002", "motivation": "\u968f\u7740\u4ee3\u7406\u5f0fAI\u7f16\u7801\u5de5\u5177\u5177\u5907\u8d85\u8d8a\u5bf9\u8bdd\u5185\u5bb9\u751f\u6210\u7684\u81ea\u4e3b\u80fd\u529b\uff0c\u5f00\u53d1\u8005\u9700\u8981\u901a\u8fc7\u7248\u672c\u63a7\u5236\u7684\u4ed3\u5e93\u7ea7\u5de5\u4ef6\uff08\u5982Markdown\u548cJSON\u6587\u4ef6\uff09\u6765\u914d\u7f6e\u8fd9\u4e9b\u5de5\u5177\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u914d\u7f6e\u673a\u5236\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u5305\u62ec\u5b83\u4eec\u7684\u91c7\u7528\u60c5\u51b5\u548c\u5b9e\u9645\u4f7f\u7528\u6a21\u5f0f\u3002", "method": "\u8bba\u6587\u5bf9Claude Code\u3001GitHub Copilot\u3001Cursor\u3001Gemini\u548cCodex\u7b49\u4ee3\u7406\u5f0fAI\u7f16\u7801\u5de5\u5177\u7684\u914d\u7f6e\u673a\u5236\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u6790\uff0c\u8bc6\u522b\u51fa8\u79cd\u914d\u7f6e\u673a\u5236\u3002\u901a\u8fc7\u5bf92926\u4e2aGitHub\u4ed3\u5e93\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u8003\u5bdf\u4e86\u8fd9\u4e9b\u673a\u5236\u7684\u91c7\u7528\u60c5\u51b5\uff0c\u5e76\u91cd\u70b9\u6df1\u5165\u5206\u6790\u4e86Context Files\u3001Skills\u548cSubagents\u8fd9\u4e09\u79cd\u8de8\u5de5\u5177\u53ef\u7528\u7684\u673a\u5236\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e09\u4e2a\u4e3b\u8981\u8d8b\u52bf\uff1a1) Context Files\u5728\u914d\u7f6e\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0cAGENTS.md\u6210\u4e3a\u8de8\u5de5\u5177\u7684\u4e92\u64cd\u4f5c\u6807\u51c6\uff1b2) Skills\u548cSubagents\u7b49\u9ad8\u7ea7\u673a\u5236\u91c7\u7528\u8f83\u6d45\uff0c\u5927\u591a\u6570\u4ed3\u5e93\u53ea\u5b9a\u4e49\u4e00\u4e24\u4e2a\u5de5\u4ef6\uff0cSkills\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u6307\u4ee4\u800c\u975e\u53ef\u6267\u884c\u5de5\u4f5c\u6d41\uff1b3) \u4e0d\u540c\u5de5\u5177\u5f62\u6210\u4e0d\u540c\u7684\u914d\u7f6e\u6587\u5316\uff0cClaude Code\u7528\u6237\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u673a\u5236\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u7eb5\u5411\u548c\u5b9e\u9a8c\u7814\u7a76\u5efa\u7acb\u4e86\u5b9e\u8bc1\u57fa\u7ebf\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u968f\u7740\u4ee3\u7406\u5f0fAI\u7f16\u7801\u5de5\u5177\u7684\u6210\u719f\uff0c\u914d\u7f6e\u7b56\u7565\u5982\u4f55\u6f14\u53d8\u4ee5\u53ca\u5982\u4f55\u5f71\u54cd\u4ee3\u7406\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2602.13262", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13262", "abs": "https://arxiv.org/abs/2602.13262", "authors": ["Darren Li", "Meiqi Chen", "Chenze Shao", "Fandong Meng", "Jie Zhou"], "title": "General learned delegation by clones", "comment": "Code available at https://github.com/SuffixAutomata/SELFCEST", "summary": "Frontier language models improve with additional test-time computation, but serial reasoning or uncoordinated parallel sampling can be compute-inefficient under fixed inference budgets. We propose SELFCEST, which equips a base model with the ability to spawn same-weight clones in separate parallel contexts by agentic reinforcement learning. Training is end-to-end under a global task reward with shared-parameter rollouts, yielding a learned controller that allocates both generation and context budget across branches. Across challenging math reasoning benchmarks and long-context multi-hop QA, SELFCEST improves the accuracy-cost Pareto frontier relative to monolithic baselines at matched inference budget, and exhibits out-of-distribution generalization in both domains.", "AI": {"tldr": "SELFCEST\u901a\u8fc7\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u8ba9\u57fa\u7840\u6a21\u578b\u80fd\u591f\u5e76\u884c\u751f\u6210\u76f8\u540c\u6743\u91cd\u7684\u514b\u9686\uff0c\u5728\u56fa\u5b9a\u63a8\u7406\u9884\u7b97\u4e0b\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u63d0\u5347\u6570\u5b66\u63a8\u7406\u548c\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u7684\u6027\u80fd", "motivation": "\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u9700\u8981\u66f4\u591a\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6765\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4e32\u884c\u63a8\u7406\u6216\u65e0\u534f\u8c03\u7684\u5e76\u884c\u91c7\u6837\u5728\u56fa\u5b9a\u63a8\u7406\u9884\u7b97\u4e0b\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u65b9\u6cd5", "method": "\u63d0\u51faSELFCEST\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u5e76\u884c\u4e0a\u4e0b\u6587\u4e2d\u751f\u6210\u76f8\u540c\u6743\u91cd\u7684\u514b\u9686\uff0c\u5728\u5168\u5c40\u4efb\u52a1\u5956\u52b1\u4e0b\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5b66\u4e60\u63a7\u5236\u5668\u6765\u5206\u914d\u751f\u6210\u548c\u4e0a\u4e0b\u6587\u9884\u7b97", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u548c\u957f\u4e0a\u4e0b\u6587\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0cSELFCEST\u5728\u5339\u914d\u63a8\u7406\u9884\u7b97\u4e0b\u76f8\u6bd4\u5355\u4f53\u57fa\u7ebf\u63d0\u9ad8\u4e86\u51c6\u786e\u7387-\u6210\u672c\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5e76\u5728\u4e24\u4e2a\u9886\u57df\u90fd\u8868\u73b0\u51fa\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b", "conclusion": "SELFCEST\u901a\u8fc7\u667a\u80fd\u5e76\u884c\u8ba1\u7b97\u5206\u914d\u673a\u5236\uff0c\u5728\u56fa\u5b9a\u63a8\u7406\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u6548\u7387\uff0c\u4e3a\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u6a21\u578b\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2602.14878", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.14878", "abs": "https://arxiv.org/abs/2602.14878", "authors": ["Mohammed Mehedi Hasan", "Hao Li", "Gopi Krishnan Rajbahadur", "Bram Adams", "Ahmed E. Hassan"], "title": "Model Context Protocol (MCP) Tool Descriptions Are Smelly! Towards Improving AI Agent Efficiency with Augmented MCP Tool Descriptions", "comment": null, "summary": "The Model Context Protocol (MCP) standardizes how Foundation Model (FM)-based agents interact with external systems by invoking tools. However, to understand a tool's purpose and features, FMs rely on natural-language tool descriptions, making these descriptions a critical component in guiding FMs to select the optimal tool for a given (sub)task and to pass the right arguments to the tool. While defects or smells in these descriptions can misguide FM-based agents, their prevalence and consequences in the MCP ecosystem remain unclear.\n  To address this, we conduct the first large-scale empirical study of 856 tools spread across 103 MCP servers, assessing their description quality and their impact on agent performance. We identify six components of tool descriptions from the literature, develop a scoring rubric utilizing these components, then formalize tool description smells based on this rubric. By operationalizing this rubric through an FM-based scanner, we find that 97.1% of the analyzed tool descriptions contain at least one smell, with 56% failing to state their purpose clearly. While augmenting these descriptions for all components improves task success rates by a median of 5.85 percentage points and improves partial goal completion by 15.12%, it also increases the number of execution steps by 67.46% and regresses performance in 16.67% of cases. These findings highlight a trade-off between agent performance and cost, as well as the context sensitivity of the performance gain. Furthermore, component ablations show that compact variants of different component combinations often preserve behavioral reliability while reducing unnecessary token overhead, enabling more efficient use of the FM context window and lower execution costs.", "AI": {"tldr": "\u5bf9856\u4e2aMCP\u5de5\u5177\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b097.1%\u7684\u5de5\u5177\u63cf\u8ff0\u5b58\u5728\u7f3a\u9677\uff0c\u6539\u8fdb\u63cf\u8ff0\u53ef\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u4f46\u589e\u52a0\u6267\u884c\u6b65\u9aa4\uff0c\u9700\u8981\u5728\u6027\u80fd\u4e0e\u6210\u672c\u95f4\u6743\u8861\u3002", "motivation": "MCP\u6807\u51c6\u4e2d\uff0c\u57fa\u7840\u6a21\u578b\u4ee3\u7406\u4f9d\u8d56\u81ea\u7136\u8bed\u8a00\u5de5\u5177\u63cf\u8ff0\u6765\u9009\u62e9\u548c\u4f7f\u7528\u5de5\u5177\uff0c\u4f46\u8fd9\u4e9b\u63cf\u8ff0\u4e2d\u7684\u7f3a\u9677\u53ef\u80fd\u5bfc\u81f4\u4ee3\u7406\u9519\u8bef\u9009\u62e9\u5de5\u5177\u6216\u4f20\u9012\u9519\u8bef\u53c2\u6570\uff0c\u5176\u666e\u904d\u6027\u548c\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4ece\u6587\u732e\u4e2d\u8bc6\u522b\u5de5\u5177\u63cf\u8ff0\u7684\u516d\u4e2a\u7ec4\u4ef6\uff0c\u5f00\u53d1\u8bc4\u5206\u6807\u51c6\uff0c\u57fa\u4e8e\u6b64\u5f62\u5f0f\u5316\u5de5\u5177\u63cf\u8ff0\u7f3a\u9677\uff0c\u4f7f\u7528\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u626b\u63cf\u5668\u5206\u6790103\u4e2aMCP\u670d\u52a1\u5668\u4e2d\u7684856\u4e2a\u5de5\u5177\uff0c\u5e76\u8fdb\u884c\u63cf\u8ff0\u589e\u5f3a\u5b9e\u9a8c\u3002", "result": "97.1%\u7684\u5de5\u5177\u63cf\u8ff0\u81f3\u5c11\u5b58\u5728\u4e00\u4e2a\u7f3a\u9677\uff0c56%\u672a\u6e05\u6670\u8bf4\u660e\u76ee\u7684\uff1b\u589e\u5f3a\u6240\u6709\u7ec4\u4ef6\u63cf\u8ff0\u4f7f\u4efb\u52a1\u6210\u529f\u7387\u4e2d\u4f4d\u6570\u63d0\u53475.85\u4e2a\u767e\u5206\u70b9\uff0c\u90e8\u5206\u76ee\u6807\u5b8c\u6210\u7387\u63d0\u534715.12%\uff0c\u4f46\u6267\u884c\u6b65\u9aa4\u589e\u52a067.46%\uff0c16.67%\u7684\u60c5\u51b5\u6027\u80fd\u4e0b\u964d\uff1b\u7ec4\u4ef6\u6d88\u878d\u663e\u793a\u7d27\u51d1\u53d8\u4f53\u53ef\u4fdd\u6301\u53ef\u9760\u6027\u540c\u65f6\u51cf\u5c11token\u5f00\u9500\u3002", "conclusion": "\u5de5\u5177\u63cf\u8ff0\u8d28\u91cf\u5bf9\u4ee3\u7406\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u6539\u8fdb\u63cf\u8ff0\u9700\u8981\u5728\u6027\u80fd\u63d0\u5347\u4e0e\u6267\u884c\u6210\u672c\u95f4\u6743\u8861\uff0c\u7d27\u51d1\u7684\u63cf\u8ff0\u53d8\u4f53\u53ef\u5728\u4fdd\u6301\u53ef\u9760\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2602.13272", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13272", "abs": "https://arxiv.org/abs/2602.13272", "authors": ["Muyan Weng", "Defu Cao", "Wei Yang", "Yashaswi Sharma", "Yan Liu"], "title": "TemporalBench: A Benchmark for Evaluating LLM-Based Agents on Contextual and Event-Informed Time Series Tasks", "comment": null, "summary": "It is unclear whether strong forecasting performance reflects genuine temporal understanding or the ability to reason under contextual and event-driven conditions. We introduce TemporalBench, a multi-domain benchmark designed to evaluate temporal reasoning behavior under progressively richer informational settings. TemporalBench adopts a four-tier task taxonomy that examines historical structure interpretation, context-free forecasting, contextual temporal reasoning, and event-conditioned prediction across four real-world domains: retail, healthcare, energy, and physical systems. By controlling access to future targets and contextual information, the benchmark enables a diagnostic analysis of whether models can correctly interpret temporal patterns, align them with external context, and adapt predictions when conditions change. Extensive baseline experiments show that strong numerical forecasting accuracy does not reliably translate into robust contextual or event-aware temporal reasoning; instead, existing agent frameworks exhibit fragmented strengths and systematic failure modes that remain largely hidden under forecasting-only benchmarks. The TemporalBench dataset is publicly available at https://huggingface.co/datasets/Melady/TemporalBench, and we additionally provide a public leaderboard at https://huggingface.co/spaces/Melady/TemporalBench_Leaderboard.", "AI": {"tldr": "TemporalBench\u662f\u4e00\u4e2a\u591a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u6e10\u8fdb\u4e30\u5bcc\u4fe1\u606f\u8bbe\u7f6e\u4e0b\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u5f3a\u9884\u6d4b\u6027\u80fd\u4e0d\u4e00\u5b9a\u53cd\u6620\u771f\u6b63\u7684\u65f6\u5e8f\u7406\u89e3\u6216\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4e0d\u6e05\u695a\u5f3a\u5927\u7684\u9884\u6d4b\u6027\u80fd\u662f\u5426\u53cd\u6620\u771f\u6b63\u7684\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u662f\u5728\u4e0a\u4e0b\u6587\u548c\u4e8b\u4ef6\u9a71\u52a8\u6761\u4ef6\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002\u9700\u8981\u533a\u5206\u6a21\u578b\u662f\u771f\u6b63\u7406\u89e3\u65f6\u95f4\u6a21\u5f0f\uff0c\u8fd8\u662f\u4ec5\u4ec5\u4f9d\u8d56\u7edf\u8ba1\u6a21\u5f0f\u3002", "method": "\u63d0\u51faTemporalBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528\u56db\u5c42\u4efb\u52a1\u5206\u7c7b\u6cd5\uff1a\u5386\u53f2\u7ed3\u6784\u89e3\u91ca\u3001\u65e0\u4e0a\u4e0b\u6587\u9884\u6d4b\u3001\u4e0a\u4e0b\u6587\u65f6\u5e8f\u63a8\u7406\u3001\u4e8b\u4ef6\u6761\u4ef6\u9884\u6d4b\uff0c\u8986\u76d6\u96f6\u552e\u3001\u533b\u7597\u3001\u80fd\u6e90\u548c\u7269\u7406\u7cfb\u7edf\u56db\u4e2a\u771f\u5b9e\u9886\u57df\u3002\u901a\u8fc7\u63a7\u5236\u5bf9\u672a\u6765\u76ee\u6807\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u8bbf\u95ee\uff0c\u8fdb\u884c\u8bca\u65ad\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f3a\u5927\u7684\u6570\u503c\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u4e0d\u80fd\u53ef\u9760\u5730\u8f6c\u5316\u4e3a\u9c81\u68d2\u7684\u4e0a\u4e0b\u6587\u6216\u4e8b\u4ef6\u611f\u77e5\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u4ee3\u7406\u6846\u67b6\u8868\u73b0\u51fa\u5206\u6563\u7684\u4f18\u52bf\u548c\u7cfb\u7edf\u6027\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u5728\u4ec5\u5173\u6ce8\u9884\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u57fa\u672c\u88ab\u9690\u85cf\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u9884\u6d4b\u57fa\u51c6\uff0c\u5f00\u53d1\u80fd\u591f\u771f\u6b63\u7406\u89e3\u65f6\u95f4\u6a21\u5f0f\u3001\u5bf9\u9f50\u5916\u90e8\u4e0a\u4e0b\u6587\u5e76\u5728\u6761\u4ef6\u53d8\u5316\u65f6\u9002\u5e94\u9884\u6d4b\u7684\u65f6\u5e8f\u63a8\u7406\u6a21\u578b\u3002TemporalBench\u4e3a\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2602.13274", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13274", "abs": "https://arxiv.org/abs/2602.13274", "authors": ["Rohan Subramanian Thomas", "Shikhar Shiromani", "Abdullah Chaudhry", "Ruizhe Li", "Vasu Sharma", "Kevin Zhu", "Sunishchal Dev"], "title": "ProMoral-Bench: Evaluating Prompting Strategies for Moral Reasoning and Safety in LLMs", "comment": null, "summary": "Prompt design significantly impacts the moral competence and safety alignment of large language models (LLMs), yet empirical comparisons remain fragmented across datasets and models.We introduce ProMoral-Bench, a unified benchmark evaluating 11 prompting paradigms across four LLM families. Using ETHICS, Scruples, WildJailbreak, and our new robustness test, ETHICS-Contrast, we measure performance via our proposed Unified Moral Safety Score (UMSS), a metric balancing accuracy and safety. Our results show that compact, exemplar-guided scaffolds outperform complex multi-stage reasoning, providing higher UMSS scores and greater robustness at a lower token cost. While multi-turn reasoning proves fragile under perturbations, few-shot exemplars consistently enhance moral stability and jailbreak resistance. ProMoral-Bench establishes a standardized framework for principled, cost-effective prompt engineering.", "AI": {"tldr": "ProMoral-Bench\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f3011\u79cd\u63d0\u793a\u8303\u5f0f\u57284\u4e2aLLM\u5bb6\u65cf\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528ETHICS\u3001Scruples\u3001WildJailbreak\u548c\u65b0\u7684ETHICS-Contrast\u6d4b\u8bd5\uff0c\u901a\u8fc7UMSS\u6307\u6807\u5e73\u8861\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u63d0\u793a\u8bbe\u8ba1\u663e\u8457\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u80fd\u529b\u548c\u5b89\u5168\u5bf9\u9f50\uff0c\u4f46\u73b0\u6709\u5b9e\u8bc1\u6bd4\u8f83\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e4b\u95f4\u662f\u788e\u7247\u5316\u7684\uff0c\u9700\u8981\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165ProMoral-Bench\u57fa\u51c6\uff0c\u8bc4\u4f3011\u79cd\u63d0\u793a\u8303\u5f0f\u57284\u4e2aLLM\u5bb6\u65cf\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528ETHICS\u3001Scruples\u3001WildJailbreak\u548c\u65b0\u7684ETHICS-Contrast\u6d4b\u8bd5\uff0c\u63d0\u51fa\u7edf\u4e00\u9053\u5fb7\u5b89\u5168\u8bc4\u5206\uff08UMSS\uff09\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u7d27\u51d1\u7684\u793a\u4f8b\u5f15\u5bfc\u652f\u67b6\u4f18\u4e8e\u590d\u6742\u7684\u591a\u9636\u6bb5\u63a8\u7406\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684UMSS\u5206\u6570\u548c\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u4e14token\u6210\u672c\u66f4\u4f4e\u3002\u591a\u8f6e\u63a8\u7406\u5728\u6270\u52a8\u4e0b\u8106\u5f31\uff0c\u800c\u5c11\u6837\u672c\u793a\u4f8b\u80fd\u6301\u7eed\u589e\u5f3a\u9053\u5fb7\u7a33\u5b9a\u6027\u548c\u8d8a\u72f1\u62b5\u6297\u80fd\u529b\u3002", "conclusion": "ProMoral-Bench\u4e3a\u539f\u5219\u6027\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u63d0\u793a\u5de5\u7a0b\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u6846\u67b6\uff0c\u8868\u660e\u7b80\u6d01\u7684\u793a\u4f8b\u5f15\u5bfc\u65b9\u6cd5\u5728\u9053\u5fb7\u5b89\u5168\u5bf9\u9f50\u65b9\u9762\u66f4\u6709\u6548\u3002", "topic": "agent analysis"}}
{"id": "2602.13840", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13840", "abs": "https://arxiv.org/abs/2602.13840", "authors": ["Yuhan Cheng", "Hancheng Ye", "Hai Helen Li", "Jingwei Sun", "Yiran Chen"], "title": "PrivAct: Internalizing Contextual Privacy Preservation via Multi-Agent Preference Training", "comment": null, "summary": "Large language model (LLM) agents are increasingly deployed in personalized tasks involving sensitive, context-dependent information, where privacy violations may arise in agents' action due to the implicitness of contextual privacy. Existing approaches rely on external, inference-time interventions which are brittle, scenario-specific, and may expand the privacy attack surface. We propose PrivAct, a contextual privacy-aware multi-agent learning framework that internalizes contextual privacy preservation directly into models' generation behavior for privacy-compliant agentic actions. By embedding privacy preferences into each agent, PrivAct enhances system-wide contextual integrity while achieving a more favorable privacy-helpfulness tradeoff. Experiments across multiple LLM backbones and benchmarks demonstrate consistent improvements in contextual privacy preservation, reducing leakage rates by up to 12.32% while maintaining comparable helpfulness, as well as zero-shot generalization and robustness across diverse multi-agent topologies. Code is available at https://github.com/chengyh23/PrivAct.", "AI": {"tldr": "PrivAct\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u9690\u79c1\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9690\u79c1\u504f\u597d\u5d4c\u5165\u5230\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e2d\uff0c\u76f4\u63a5\u5728\u6a21\u578b\u751f\u6210\u884c\u4e3a\u4e2d\u5185\u5316\u4e0a\u4e0b\u6587\u9690\u79c1\u4fdd\u62a4\uff0c\u5b9e\u73b0\u9690\u79c1\u5408\u89c4\u7684\u667a\u80fd\u4f53\u884c\u52a8\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u6d89\u53ca\u654f\u611f\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u4fe1\u606f\u7684\u4e2a\u6027\u5316\u4efb\u52a1\u4e2d\uff0c\u7531\u4e8e\u4e0a\u4e0b\u6587\u9690\u79c1\u7684\u9690\u542b\u6027\uff0c\u667a\u80fd\u4f53\u884c\u52a8\u4e2d\u53ef\u80fd\u51fa\u73b0\u9690\u79c1\u4fb5\u72af\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u3001\u63a8\u7406\u65f6\u5e72\u9884\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u8106\u5f31\u3001\u573a\u666f\u7279\u5b9a\uff0c\u5e76\u53ef\u80fd\u6269\u5927\u9690\u79c1\u653b\u51fb\u9762\u3002", "method": "\u63d0\u51faPrivAct\u6846\u67b6\uff0c\u5c06\u4e0a\u4e0b\u6587\u9690\u79c1\u4fdd\u62a4\u76f4\u63a5\u5185\u5316\u5230\u6a21\u578b\u751f\u6210\u884c\u4e3a\u4e2d\uff0c\u901a\u8fc7\u5c06\u9690\u79c1\u504f\u597d\u5d4c\u5165\u5230\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e2d\uff0c\u5b9e\u73b0\u7cfb\u7edf\u8303\u56f4\u7684\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\uff0c\u5e76\u5728\u9690\u79c1\u548c\u5e2e\u52a9\u6027\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u7684\u5e73\u8861\u3002", "result": "\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u7f51\u7edc\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0a\u4e0b\u6587\u9690\u79c1\u4fdd\u62a4\u6301\u7eed\u6539\u8fdb\uff0c\u6cc4\u6f0f\u7387\u964d\u4f4e\u9ad8\u8fbe12.32%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u5e2e\u52a9\u6027\uff0c\u5e76\u5728\u4e0d\u540c\u591a\u667a\u80fd\u4f53\u62d3\u6251\u7ed3\u6784\u4e2d\u5c55\u793a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "PrivAct\u901a\u8fc7\u5c06\u9690\u79c1\u4fdd\u62a4\u5185\u5316\u5230\u667a\u80fd\u4f53\u751f\u6210\u884c\u4e3a\u4e2d\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u9c81\u68d2\u7684\u4e0a\u4e0b\u6587\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5916\u90e8\u5e72\u9884\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.13626", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13626", "abs": "https://arxiv.org/abs/2602.13626", "authors": ["Mingqiao Zhang", "Qiyao Peng", "Yumeng Wang", "Chunyuan Liu", "Hongtao Liu"], "title": "Benchmark Leakage Trap: Can We Trust LLM-based Recommendation?", "comment": null, "summary": "The expanding integration of Large Language Models (LLMs) into recommender systems poses critical challenges to evaluation reliability. This paper identifies and investigates a previously overlooked issue: benchmark data leakage in LLM-based recommendation. This phenomenon occurs when LLMs are exposed to and potentially memorize benchmark datasets during pre-training or fine-tuning, leading to artificially inflated performance metrics that fail to reflect true model performance. To validate this phenomenon, we simulate diverse data leakage scenarios by conducting continued pre-training of foundation models on strategically blended corpora, which include user-item interactions from both in-domain and out-of-domain sources. Our experiments reveal a dual-effect of data leakage: when the leaked data is domain-relevant, it induces substantial but spurious performance gains, misleadingly exaggerating the model's capability. In contrast, domain-irrelevant leakage typically degrades recommendation accuracy, highlighting the complex and contingent nature of this contamination. Our findings reveal that data leakage acts as a critical, previously unaccounted-for factor in LLM-based recommendation, which could impact the true model performance. We release our code at https://github.com/yusba1/LLMRec-Data-Leakage.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0LLM\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b58\u5728\u57fa\u51c6\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u5373LLM\u5728\u9884\u8bad\u7ec3\u6216\u5fae\u8c03\u65f6\u63a5\u89e6\u5e76\u53ef\u80fd\u8bb0\u5fc6\u4e86\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u6027\u80fd\u6307\u6807\u865a\u9ad8\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bc4\u4f30\u53ef\u9760\u6027\u9762\u4e34\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u8bc6\u522b\u5e76\u7814\u7a76\u4e86\u4e00\u4e2a\u5148\u524d\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\uff1a\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u57fa\u51c6\u6570\u636e\u6cc4\u9732\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u4e0d\u540c\u7684\u6570\u636e\u6cc4\u9732\u573a\u666f\uff0c\u5bf9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528\u5305\u542b\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u7684\u6218\u7565\u6027\u6df7\u5408\u8bed\u6599\u5e93\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u6570\u636e\u6cc4\u9732\u5bf9\u63a8\u8350\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u6570\u636e\u6cc4\u9732\u7684\u53cc\u91cd\u6548\u5e94\uff1a\u5f53\u6cc4\u9732\u6570\u636e\u4e0e\u9886\u57df\u76f8\u5173\u65f6\uff0c\u4f1a\u5bfc\u81f4\u663e\u8457\u4f46\u865a\u5047\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bef\u5bfc\u6027\u5730\u5938\u5927\u6a21\u578b\u80fd\u529b\uff1b\u800c\u9886\u57df\u4e0d\u76f8\u5173\u7684\u6cc4\u9732\u901a\u5e38\u4f1a\u964d\u4f4e\u63a8\u8350\u51c6\u786e\u6027\u3002", "conclusion": "\u6570\u636e\u6cc4\u9732\u662f\u57fa\u4e8eLLM\u63a8\u8350\u7cfb\u7edf\u4e2d\u4e00\u4e2a\u5173\u952e\u4e14\u5148\u524d\u672a\u88ab\u8003\u8651\u7684\u56e0\u7d20\uff0c\u53ef\u80fd\u5f71\u54cd\u771f\u5b9e\u6a21\u578b\u6027\u80fd\u3002\u9700\u8981\u66f4\u4e25\u8c28\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u907f\u514d\u8fd9\u79cd\u504f\u5dee\u3002", "topic": "agent analysis"}}
{"id": "2602.13921", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13921", "abs": "https://arxiv.org/abs/2602.13921", "authors": ["Juntong Wang", "Libin Chen", "Xiyuan Wang", "Shijia Kang", "Haotong Yang", "Da Zheng", "Muhan Zhang"], "title": "GREPO: A Benchmark for Graph Neural Networks on Repository-Level Bug Localization", "comment": "46 pages, 14figures", "summary": "Repository-level bug localization-the task of identifying where code must be modified to fix a bug-is a critical software engineering challenge. Standard Large Language Modles (LLMs) are often unsuitable for this task due to context window limitations that prevent them from processing entire code repositories. As a result, various retrieval methods are commonly used, including keyword matching, text similarity, and simple graph-based heuristics such as Breadth-First Search. Graph Neural Networks (GNNs) offer a promising alternative due to their ability to model complex, repository-wide dependencies; however, their application has been hindered by the lack of a dedicated benchmark. To address this gap, we introduce GREPO, the first GNN benchmark for repository-scale bug localization tasks. GREPO comprises 86 Python repositories and 47294 bug-fixing tasks, providing graph-based data structures ready for direct GNN processing. Our evaluation of various GNN architectures shows outstanding performance compared to established information retrieval baselines. This work highlights the potential of GNNs for bug localization and established GREPO as a foundation resource for future research, The code is available at https://github.com/qingpingmo/GREPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u4ed3\u5e93\u7ea7bug\u5b9a\u4f4d\u7684GNN\u57fa\u51c6\u6d4b\u8bd5GREPO\uff0c\u5305\u542b86\u4e2aPython\u4ed3\u5e93\u548c47294\u4e2abug\u4fee\u590d\u4efb\u52a1\uff0cGNN\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u4fe1\u606f\u68c0\u7d22\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4ed3\u5e93\u7ea7bug\u5b9a\u4f4d\u662f\u5173\u952e\u7684\u8f6f\u4ef6\u5de5\u7a0b\u6311\u6218\uff0c\u6807\u51c6LLM\u7531\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u65e0\u6cd5\u5904\u7406\u6574\u4e2a\u4ee3\u7801\u4ed3\u5e93\uff0c\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\uff08\u5982\u5173\u952e\u8bcd\u5339\u914d\u3001\u6587\u672c\u76f8\u4f3c\u5ea6\u3001\u7b80\u5355\u56fe\u542f\u53d1\u5f0f\uff09\u6548\u679c\u6709\u9650\uff0c\u800cGNN\u867d\u7136\u80fd\u5efa\u6a21\u590d\u6742\u7684\u4ed3\u5e93\u7ea7\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u7f3a\u4e4f\u4e13\u7528\u57fa\u51c6\u6d4b\u8bd5\u963b\u788d\u4e86\u5176\u5e94\u7528\u3002", "method": "\u5f15\u5165GREPO\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b86\u4e2aPython\u4ed3\u5e93\u548c47294\u4e2abug\u4fee\u590d\u4efb\u52a1\uff0c\u63d0\u4f9b\u53ef\u76f4\u63a5\u7528\u4e8eGNN\u5904\u7406\u7684\u56fe\u6570\u636e\u7ed3\u6784\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cdGNN\u67b6\u6784\u7684\u6027\u80fd\u3002", "result": "GNN\u67b6\u6784\u5728bug\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u4fe1\u606f\u68c0\u7d22\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GNN\u5728bug\u5b9a\u4f4d\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0cGREPO\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "topic": "swe benchmark"}}
{"id": "2602.13937", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.13937", "abs": "https://arxiv.org/abs/2602.13937", "authors": ["Dat Le", "Duc-Cuong Le", "Anh-Son Nguyen", "Tuan-Dung Bui", "Thu-Trang Nguyen", "Son Nguyen", "Hieu Dinh Vo"], "title": "A Multi-Agent Framework for Code-Guided, Modular, and Verifiable Automated Machine Learning", "comment": null, "summary": "Automated Machine Learning (AutoML) has revolutionized the development of data-driven solutions; however, traditional frameworks often function as \"black boxes\", lacking the flexibility and transparency required for complex, real-world engineering tasks. Recent Large Language Model (LLM)-based agents have shifted toward code-driven approaches. However, they frequently suffer from hallucinated logic and logic entanglement, where monolithic code generation leads to unrecoverable runtime failures. In this paper, we present iML, a novel multi-agent framework designed to shift AutoML from black-box prompting to a code-guided, modular, and verifiable architectural paradigm. iML introduces three main ideas: (1) Code-Guided Planning, which synthesizes a strategic blueprint grounded in autonomous empirical profiling to eliminate hallucination; (2) Code-Modular Implementation, which decouples preprocessing and modeling into specialized components governed by strict interface contracts; and (3) Code-Verifiable Integration, which enforces physical feasibility through dynamic contract verification and iterative self-correction. We evaluate iML across MLE-BENCH and the newly introduced iML-BENCH, comprising a diverse range of real-world Kaggle competitions. The experimental results show iML's superiority over state-of-the-art agents, achieving a valid submission rate of 85% and a competitive medal rate of 45% on MLE-BENCH, with an average standardized performance score (APS) of 0.77. On iML-BENCH, iML significantly outperforms the other approaches by 38%-163% in APS. Furthermore, iML maintains a robust 70% success rate even under stripped task descriptions, effectively filling information gaps through empirical profiling. These results highlight iML's potential to bridge the gap between stochastic generation and reliable engineering, marking a meaningful step toward truly AutoML.", "AI": {"tldr": "iML\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684AutoML\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u5f15\u5bfc\u7684\u89c4\u5212\u3001\u6a21\u5757\u5316\u5b9e\u73b0\u548c\u53ef\u9a8c\u8bc1\u96c6\u6210\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfAutoML\u9ed1\u76d2\u95ee\u9898\u548cLLM\u667a\u80fd\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4efb\u52a1\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfAutoML\u6846\u67b6\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u900f\u660e\u5ea6\uff0c\u800c\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5b58\u5728\u5e7b\u89c9\u903b\u8f91\u548c\u903b\u8f91\u7ea0\u7f20\u95ee\u9898\uff0c\u5bfc\u81f4\u8fd0\u884c\u65f6\u6545\u969c\u96be\u4ee5\u6062\u590d\u3002\u9700\u8981\u4e00\u79cd\u65b0\u7684\u67b6\u6784\u8303\u5f0f\u6765\u63d0\u5347AutoML\u7684\u53ef\u9760\u6027\u548c\u5de5\u7a0b\u5b9e\u7528\u6027\u3002", "method": "iML\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u4ee3\u7801\u5f15\u5bfc\u89c4\u5212\uff1a\u57fa\u4e8e\u81ea\u4e3b\u7ecf\u9a8c\u5206\u6790\u751f\u6210\u6218\u7565\u84dd\u56fe\u6d88\u9664\u5e7b\u89c9\uff1b2) \u4ee3\u7801\u6a21\u5757\u5316\u5b9e\u73b0\uff1a\u5c06\u9884\u5904\u7406\u548c\u5efa\u6a21\u89e3\u8026\u4e3a\u4e13\u95e8\u7ec4\u4ef6\uff0c\u901a\u8fc7\u4e25\u683c\u63a5\u53e3\u5951\u7ea6\u7ba1\u7406\uff1b3) \u4ee3\u7801\u53ef\u9a8c\u8bc1\u96c6\u6210\uff1a\u901a\u8fc7\u52a8\u6001\u5951\u7ea6\u9a8c\u8bc1\u548c\u8fed\u4ee3\u81ea\u6821\u6b63\u786e\u4fdd\u7269\u7406\u53ef\u884c\u6027\u3002", "result": "\u5728MLE-BENCH\u4e0a\u5b9e\u73b085%\u7684\u6709\u6548\u63d0\u4ea4\u7387\u548c45%\u7684\u7ade\u8d5b\u5956\u724c\u7387\uff0c\u5e73\u5747\u6807\u51c6\u5316\u6027\u80fd\u5f97\u5206(APS)\u4e3a0.77\u3002\u5728iML-BENCH\u4e0a\u6bd4\u5176\u4ed6\u65b9\u6cd5\u63d0\u534738%-163%\u7684APS\u3002\u5373\u4f7f\u5728\u7b80\u5316\u4efb\u52a1\u63cf\u8ff0\u4e0b\u4ecd\u4fdd\u630170%\u7684\u6210\u529f\u7387\u3002", "conclusion": "iML\u901a\u8fc7\u4ee3\u7801\u5f15\u5bfc\u7684\u6a21\u5757\u5316\u67b6\u6784\u6709\u6548\u5f25\u5408\u4e86\u968f\u673a\u751f\u6210\u4e0e\u53ef\u9760\u5de5\u7a0b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5b9e\u73b0\u771f\u6b63\u7684AutoML\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u73b0\u5b9e\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2602.14296", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.14296", "abs": "https://arxiv.org/abs/2602.14296", "authors": ["Yifan Wu", "Yiran Peng", "Yiyu Chen", "Jianhao Ruan", "Zijie Zhuang", "Cheng Yang", "Jiayi Zhang", "Man Chen", "Yenchi Tseng", "Zhaoyang Yu", "Liang Chen", "Yuyao Zhai", "Bang Liu", "Chenglin Wu", "Yuyu Luo"], "title": "AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines", "comment": null, "summary": "The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.", "AI": {"tldr": "AutoWebWorld\uff1a\u901a\u8fc7\u5c06\u7f51\u9875\u5efa\u6a21\u4e3a\u6709\u9650\u72b6\u6001\u673a\u5e76\u7528\u4ee3\u7801\u667a\u80fd\u4f53\u751f\u6210\u4ea4\u4e92\u5f0f\u7f51\u7ad9\uff0c\u5b9e\u73b0\u53ef\u63a7\u53ef\u9a8c\u8bc1\u7684\u7f51\u9875\u73af\u5883\u5408\u6210\uff0c\u5927\u5e45\u964d\u4f4e\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u6536\u96c6\u6210\u672c\u3002", "motivation": "\u81ea\u4e3b\u7f51\u9875GUI\u667a\u80fd\u4f53\u7684\u6027\u80fd\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u6536\u96c6\u771f\u5b9e\u7f51\u7ad9\u4ea4\u4e92\u8f68\u8ff9\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u9a8c\u8bc1\uff0c\u72b6\u6001\u8f6c\u6362\u9690\u542b\u5bfc\u81f4\u4f9d\u8d56\u4e0d\u4e00\u81f4\u7684\u5916\u90e8\u9a8c\u8bc1\u5668\u3002", "method": "\u5c06\u7f51\u9875\u73af\u5883\u5efa\u6a21\u4e3a\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\uff0c\u4f7f\u7528\u4ee3\u7801\u667a\u80fd\u4f53\u5c06FSM\u8f6c\u6362\u4e3a\u4ea4\u4e92\u5f0f\u7f51\u7ad9\uff0c\u5b9e\u73b0\u663e\u5f0f\u5b9a\u4e49\u6240\u6709\u72b6\u6001\u3001\u52a8\u4f5c\u548c\u8f6c\u6362\u89c4\u5219\uff0c\u652f\u6301\u7a0b\u5e8f\u5316\u9a8c\u8bc1\u3002", "result": "\u4ee5\u6bcf\u6761\u8f68\u8ff90.04\u7f8e\u5143\u7684\u6210\u672c\u751f\u621011,663\u6761\u5df2\u9a8c\u8bc1\u8f68\u8ff9\uff0c\u8bad\u7ec37B\u7f51\u9875GUI\u667a\u80fd\u4f53\u5728WebVoyager\u4e0a15\u6b65\u5185\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\uff0c\u89c2\u5bdf\u5230\u5408\u6210\u6570\u636e\u91cf\u4e0eWebVoyager\u3001Online-Mind2Web\u6027\u80fd\u5448\u660e\u786e\u7f29\u653e\u89c4\u5f8b\u3002", "conclusion": "AutoWebWorld\u901a\u8fc7\u5408\u6210\u53ef\u63a7\u53ef\u9a8c\u8bc1\u7684\u7f51\u9875\u73af\u5883\u89e3\u51b3\u4e86\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u6536\u96c6\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u6027\u80fd\uff0c\u4e14\u6570\u636e\u91cf\u4e0e\u6027\u80fd\u5448\u7f29\u653e\u89c4\u5f8b\u3002", "topic": "code agent"}}
{"id": "2602.13292", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13292", "abs": "https://arxiv.org/abs/2602.13292", "authors": ["Yifan Ding", "Yuhui Shi", "Zhiyan Li", "Zilong Wang", "Yifeng Gao", "Yajun Yang", "Mengjie Yang", "Yixiu Liang", "Xipeng Qiu", "Xuanjing Huang", "Xingjun Ma", "Yu-Gang Jiang", "Guoyu Wang"], "title": "Mirror: A Multi-Agent System for AI-Assisted Ethics Review", "comment": "4 figures, 3 tables", "summary": "Ethics review is a foundational mechanism of modern research governance, yet contemporary systems face increasing strain as ethical risks arise as structural consequences of large-scale, interdisciplinary scientific practice. The demand for consistent and defensible decisions under heterogeneous risk profiles exposes limitations in institutional review capacity rather than in the legitimacy of ethics oversight. Recent advances in large language models (LLMs) offer new opportunities to support ethics review, but their direct application remains limited by insufficient ethical reasoning capability, weak integration with regulatory structures, and strict privacy constraints on authentic review materials. In this work, we introduce Mirror, an agentic framework for AI-assisted ethical review that integrates ethical reasoning, structured rule interpretation, and multi-agent deliberation within a unified architecture. At its core is EthicsLLM, a foundational model fine-tuned on EthicsQA, a specialized dataset of 41K question-chain-of-thought-answer triples distilled from authoritative ethics and regulatory corpora. EthicsLLM provides detailed normative and regulatory understanding, enabling Mirror to operate in two complementary modes. Mirror-ER (expedited Review) automates expedited review through an executable rule base that supports efficient and transparent compliance checks for minimal-risk studies. Mirror-CR (Committee Review) simulates full-board deliberation through coordinated interactions among expert agents, an ethics secretary agent, and a principal investigator agent, producing structured, committee-level assessments across ten ethical dimensions. Empirical evaluations demonstrate that Mirror significantly improves the quality, consistency, and professionalism of ethics assessments compared with strong generalist LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86Mirror\u6846\u67b6\uff0c\u901a\u8fc7AI\u4ee3\u7406\u8f85\u52a9\u4f26\u7406\u5ba1\u67e5\uff0c\u5305\u542bEthicsLLM\u57fa\u7840\u6a21\u578b\u548c\u4e24\u79cd\u5de5\u4f5c\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f26\u7406\u8bc4\u4f30\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u4ee3\u7814\u7a76\u4f26\u7406\u5ba1\u67e5\u7cfb\u7edf\u9762\u4e34\u5927\u89c4\u6a21\u8de8\u5b66\u79d1\u79d1\u5b66\u5b9e\u8df5\u5e26\u6765\u7684\u7ed3\u6784\u6027\u4f26\u7406\u98ce\u9669\u538b\u529b\uff0c\u73b0\u6709\u5236\u5ea6\u5ba1\u67e5\u80fd\u529b\u6709\u9650\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u5e94\u7528\u5b58\u5728\u4f26\u7406\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3001\u4e0e\u76d1\u7ba1\u7ed3\u6784\u6574\u5408\u5f31\u3001\u771f\u5b9e\u5ba1\u67e5\u6750\u6599\u9690\u79c1\u9650\u5236\u7b49\u95ee\u9898\u3002", "method": "\u5f00\u53d1Mirror\u4ee3\u7406\u6846\u67b6\uff0c\u6838\u5fc3\u662fEthicsLLM\u57fa\u7840\u6a21\u578b\uff08\u5728EthicsQA\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\uff09\uff0c\u652f\u6301\u4e24\u79cd\u6a21\u5f0f\uff1aMirror-ER\u901a\u8fc7\u53ef\u6267\u884c\u89c4\u5219\u5e93\u81ea\u52a8\u5316\u5feb\u901f\u5ba1\u67e5\uff0cMirror-CR\u901a\u8fc7\u4e13\u5bb6\u4ee3\u7406\u3001\u4f26\u7406\u79d8\u4e66\u4ee3\u7406\u548c\u9996\u5e2d\u7814\u7a76\u5458\u4ee3\u7406\u7684\u534f\u8c03\u4ea4\u4e92\u6a21\u62df\u5b8c\u6574\u59d4\u5458\u4f1a\u5ba1\u8bae\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cMirror\u76f8\u6bd4\u5f3a\u5927\u7684\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f26\u7406\u8bc4\u4f30\u7684\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u548c\u4e13\u4e1a\u6027\u3002", "conclusion": "Mirror\u6846\u67b6\u4e3aAI\u8f85\u52a9\u4f26\u7406\u5ba1\u67e5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6574\u5408\u4f26\u7406\u63a8\u7406\u3001\u7ed3\u6784\u5316\u89c4\u5219\u89e3\u91ca\u548c\u591a\u4ee3\u7406\u5ba1\u8bae\uff0c\u80fd\u591f\u652f\u6301\u5feb\u901f\u5ba1\u67e5\u548c\u5b8c\u6574\u59d4\u5458\u4f1a\u8bc4\u4f30\u4e24\u79cd\u6a21\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2602.14922", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.14922", "abs": "https://arxiv.org/abs/2602.14922", "authors": ["Gaoyang Zhang", "Shanghong Zou", "Yafang Wang", "He Zhang", "Ruohua Xu", "Feng Zhao"], "title": "ReusStdFlow: A Standardized Reusability Framework for Dynamic Workflow Construction in Agentic AI", "comment": null, "summary": "To address the ``reusability dilemma'' and structural hallucinations in enterprise Agentic AI,this paper proposes ReusStdFlow, a framework centered on a novel ``Extraction-Storage-Construction'' paradigm. The framework deconstructs heterogeneous, platform-specific Domain Specific Languages (DSLs) into standardized, modular workflow segments. It employs a dual knowledge architecture-integrating graph and vector databases-to facilitate synergistic retrieval of both topological structures and functional semantics. Finally, workflows are intelligently assembled using a retrieval-augmented generation (RAG) strategy. Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction. This framework provides a standardized solution for the automated reorganization and efficient reuse of enterprise digital assets.", "AI": {"tldr": "ReusStdFlow\u6846\u67b6\u901a\u8fc7\"\u63d0\u53d6-\u5b58\u50a8-\u6784\u5efa\"\u8303\u5f0f\u89e3\u51b3\u4f01\u4e1aAgentic AI\u4e2d\u7684\u53ef\u91cd\u7528\u6027\u56f0\u5883\u548c\u7ed3\u6784\u5e7b\u89c9\u95ee\u9898\uff0c\u5c06\u5f02\u6784DSL\u89e3\u6784\u4e3a\u6807\u51c6\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\u7247\u6bb5\uff0c\u4f7f\u7528\u56fe+\u5411\u91cf\u53cc\u77e5\u8bc6\u67b6\u6784\uff0c\u57fa\u4e8eRAG\u7b56\u7565\u667a\u80fd\u7ec4\u88c5\u5de5\u4f5c\u6d41\uff0c\u5728200\u4e2a\u771f\u5b9en8n\u5de5\u4f5c\u6d41\u4e0a\u5b9e\u73b0\u8d8590%\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1aAgentic AI\u4e2d\u7684\"\u53ef\u91cd\u7528\u6027\u56f0\u5883\"\u548c\u7ed3\u6784\u5e7b\u89c9\u95ee\u9898\uff0c\u4f01\u4e1a\u6570\u5b57\u8d44\u4ea7\u901a\u5e38\u4ee5\u5f02\u6784\u3001\u5e73\u53f0\u7279\u5b9a\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00(DSL)\u5f62\u5f0f\u5b58\u5728\uff0c\u96be\u4ee5\u6807\u51c6\u5316\u548c\u91cd\u7528\u3002", "method": "\u63d0\u51faReusStdFlow\u6846\u67b6\uff0c\u91c7\u7528\"\u63d0\u53d6-\u5b58\u50a8-\u6784\u5efa\"\u8303\u5f0f\uff1a1) \u5c06\u5f02\u6784DSL\u89e3\u6784\u4e3a\u6807\u51c6\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\u7247\u6bb5\uff1b2) \u4f7f\u7528\u56fe\u6570\u636e\u5e93+\u5411\u91cf\u6570\u636e\u5e93\u7684\u53cc\u77e5\u8bc6\u67b6\u6784\u534f\u540c\u68c0\u7d22\u62d3\u6251\u7ed3\u6784\u548c\u529f\u80fd\u8bed\u4e49\uff1b3) \u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7b56\u7565\u667a\u80fd\u7ec4\u88c5\u5de5\u4f5c\u6d41\u3002", "result": "\u5728200\u4e2a\u771f\u5b9e\u4e16\u754c\u7684n8n\u5de5\u4f5c\u6d41\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u5728\u63d0\u53d6\u548c\u6784\u5efa\u4e24\u65b9\u9762\u90fd\u5b9e\u73b0\u4e86\u8d85\u8fc790%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4f01\u4e1a\u6570\u5b57\u8d44\u4ea7\u7684\u81ea\u52a8\u5316\u91cd\u7ec4\u548c\u9ad8\u6548\u91cd\u7528\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.14955", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.14955", "abs": "https://arxiv.org/abs/2602.14955", "authors": ["Varun Nathan", "Shreyas Guha", "Ayush Kumar"], "title": "Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition", "comment": null, "summary": "We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator->optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage. Empirically, LLMs struggle on compound queries and on plans exceeding 4 steps (typically 5-15); the best total metric score reaches 84.8% (Claude-3-7-Sonnet), while the strongest one-shot match rate at the \"A+\" tier (Extremely Good, Very Good) is only 49.75% (o3-mini). Plan lineage yields mixed gains overall but benefits several top models and improves step executability for many. Our results highlight persistent gaps in tool-understanding, especially in tool-prompt alignment and tool-usage completeness, and show that shorter, simpler plans are markedly easier. The framework and findings provide a reproducible path for assessing and improving agentic planning with tools for answering data-analysis queries in contact-center settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u8054\u7edc\u4e2d\u5fc3\u5de5\u5177\u611f\u77e5\u89c4\u5212\u751f\u6210\u7684\u9886\u57df\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5c06\u4e1a\u52a1\u67e5\u8be2\u5206\u89e3\u4e3a\u7ed3\u6784\u5316/\u975e\u7ed3\u6784\u5316\u5de5\u5177\u53ef\u6267\u884c\u6b65\u9aa4\u7684\u80fd\u529b\uff0c\u53d1\u73b0LLM\u5728\u590d\u6742\u67e5\u8be2\u548c\u591a\u6b65\u9aa4\u89c4\u5212\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u8054\u7edc\u4e2d\u5fc3\u9700\u8981\u5c06\u4e1a\u52a1\u6d1e\u5bdf\u67e5\u8be2\u5206\u89e3\u4e3a\u53ef\u6267\u884c\u7684\u5de5\u5177\u6b65\u9aa4\uff08\u5982Text2SQL\u3001RAG\u7b49\uff09\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u5de5\u5177\u611f\u77e5\u89c4\u5212\u80fd\u529b\u65b9\u9762\u7684\u57fa\u51c6\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u548c\u5e76\u884c\u6267\u884c\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e09\u65b9\u9762\u8d21\u732e\uff1a1\uff09\u57fa\u4e8e\u53c2\u8003\u7684\u89c4\u5212\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u4e03\u7ef4\u5ea6\u6307\u6807\u8bc4\u4f30\u548c\u4e00\u6b21\u6027\u8bc4\u4f30\u4e24\u79cd\u6a21\u5f0f\uff1b2\uff09\u6570\u636e\u6574\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u5668->\u4f18\u5316\u5668\u5faa\u73af\u8fed\u4ee3\u4f18\u5316\u89c4\u5212\u8d28\u91cf\uff1b3\uff09\u5bf914\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u5bb6\u65cf\u7684LLM\u8fdb\u884c\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u8bc4\u4f30\u5176\u5728\u6709\u65e0\u89c4\u5212\u8c31\u7cfb\u63d0\u793a\u4e0b\u7684\u89c4\u5212\u80fd\u529b\u3002", "result": "LLM\u5728\u590d\u5408\u67e5\u8be2\u548c\u8d85\u8fc74\u6b65\u7684\u89c4\u5212\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u901a\u5e38\u9700\u89815-15\u6b65\uff09\uff1b\u6700\u4f73\u603b\u6307\u6807\u5f97\u520684.8%\uff08Claude-3-7-Sonnet\uff09\uff0c\u6700\u5f3a\u4e00\u6b21\u6027\u5339\u914d\u7387\u4ec549.75%\uff08o3-mini\uff09\u3002\u89c4\u5212\u8c31\u7cfb\u6574\u4f53\u6536\u76ca\u6709\u9650\uff0c\u4f46\u5bf9\u90e8\u5206\u9876\u7ea7\u6a21\u578b\u6709\u5e2e\u52a9\uff0c\u5e76\u63d0\u9ad8\u4e86\u6b65\u9aa4\u53ef\u6267\u884c\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u5de5\u5177\u7406\u89e3\u65b9\u9762\u5b58\u5728\u6301\u7eed\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u5de5\u5177\u63d0\u793a\u5bf9\u9f50\u548c\u5de5\u5177\u4f7f\u7528\u5b8c\u6574\u6027\u65b9\u9762\uff0c\u8f83\u77ed\u7684\u7b80\u5355\u89c4\u5212\u660e\u663e\u66f4\u5bb9\u6613\u3002\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u8054\u7edc\u4e2d\u5fc3\u6570\u636e\u5206\u6790\u67e5\u8be2\u7684\u4ee3\u7406\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2602.13320", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13320", "abs": "https://arxiv.org/abs/2602.13320", "authors": ["Flint Xiaofeng Fan", "Cheston Tan", "Roger Wattenhofer", "Yew-Soon Ong"], "title": "Information Fidelity in Tool-Using LLM Agents: A Martingale Analysis of the Model Context Protocol", "comment": "Full working version of an extended abstract accepted at the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)", "summary": "As AI agents powered by large language models (LLMs) increasingly use external tools for high-stakes decisions, a critical reliability question arises: how do errors propagate across sequential tool calls? We introduce the first theoretical framework for analyzing error accumulation in Model Context Protocol (MCP) agents, proving that cumulative distortion exhibits linear growth and high-probability deviations bounded by $O(\\sqrt{T})$. This concentration property ensures predictable system behavior and rules out exponential failure modes. We develop a hybrid distortion metric combining discrete fact matching with continuous semantic similarity, then establish martingale concentration bounds on error propagation through sequential tool interactions. Experiments across Qwen2-7B, Llama-3-8B, and Mistral-7B validate our theoretical predictions, showing empirical distortion tracks the linear trend with deviations consistently within $O(\\sqrt{T})$ envelopes. Key findings include: semantic weighting reduces distortion by 80\\%, and periodic re-grounding approximately every 9 steps suffices for error control. We translate these concentration guarantees into actionable deployment principles for trustworthy agent systems.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5206\u6790MCP\u4ee3\u7406\u4e2d\u9519\u8bef\u7d2f\u79ef\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u7d2f\u79ef\u5931\u771f\u5448\u7ebf\u6027\u589e\u957f\u4e14\u9ad8\u6982\u7387\u504f\u5dee\u6709\u754c\uff0c\u4e3a\u53ef\u4fe1\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u90e8\u7f72\u539f\u5219", "motivation": "\u968f\u7740\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u8fdb\u884c\u9ad8\u98ce\u9669\u51b3\u7b56\uff0c\u9700\u8981\u7406\u89e3\u9519\u8bef\u5982\u4f55\u5728\u987a\u5e8f\u5de5\u5177\u8c03\u7528\u4e2d\u4f20\u64ad\u4ee5\u786e\u4fdd\u7cfb\u7edf\u53ef\u9760\u6027", "method": "\u5f00\u53d1\u6df7\u5408\u5931\u771f\u5ea6\u91cf\uff08\u7ed3\u5408\u79bb\u6563\u4e8b\u5b9e\u5339\u914d\u548c\u8fde\u7eed\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff09\uff0c\u5efa\u7acb\u987a\u5e8f\u5de5\u5177\u4ea4\u4e92\u4e2d\u9519\u8bef\u4f20\u64ad\u7684\u9785\u96c6\u4e2d\u754c\u7406\u8bba\u6846\u67b6", "result": "\u7406\u8bba\u8bc1\u660e\u7d2f\u79ef\u5931\u771f\u5448\u7ebf\u6027\u589e\u957f\uff0c\u9ad8\u6982\u7387\u504f\u5dee\u6709\u754c\u4e8eO(\u221aT)\uff1b\u5b9e\u9a8c\u9a8c\u8bc1\u5931\u771f\u8ddf\u8e2a\u7ebf\u6027\u8d8b\u52bf\uff0c\u8bed\u4e49\u52a0\u6743\u51cf\u5c1180%\u5931\u771f\uff0c\u6bcf\u7ea69\u6b65\u91cd\u65b0\u63a5\u5730\u53ef\u63a7\u5236\u9519\u8bef", "conclusion": "\u9519\u8bef\u4f20\u64ad\u7684\u96c6\u4e2d\u6027\u4fdd\u8bc1\u4e86\u53ef\u9884\u6d4b\u7684\u7cfb\u7edf\u884c\u4e3a\uff0c\u6392\u9664\u4e86\u6307\u6570\u7ea7\u6545\u969c\u6a21\u5f0f\uff0c\u4e3a\u53ef\u4fe1\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u90e8\u7f72\u539f\u5219", "topic": "agent analysis"}}
{"id": "2602.13367", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13367", "abs": "https://arxiv.org/abs/2602.13367", "authors": ["Chen Yang", "Guangyue Peng", "Jiaying Zhu", "Ran Le", "Ruixiang Feng", "Tao Zhang", "Xiyun Xu", "Yang Song", "Yiming Jia", "Yuntao Wen", "Yunzhi Xu", "Zekai Wang", "Zhenwei An", "Zhicong Sun", "Zongchao Chen"], "title": "Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts", "comment": null, "summary": "We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.", "AI": {"tldr": "Nanbeige4.1-3B\u662f\u4e00\u4e2a\u4ec530\u4ebf\u53c2\u6570\u7684\u591a\u529f\u80fd\u8bed\u8a00\u6a21\u578b\uff0c\u9996\u6b21\u5728\u5f00\u6e90\u5c0f\u6a21\u578b\u4e2d\u540c\u65f6\u5b9e\u73b0\u5f3a\u5927\u7684\u4ee3\u7406\u884c\u4e3a\u3001\u4ee3\u7801\u751f\u6210\u548c\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u6027\u80fd\u8d85\u8d8a\u540c\u7c7b\u751a\u81f3\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b(SLM)\u901a\u5e38\u96be\u4ee5\u540c\u65f6\u5177\u5907\u591a\u79cd\u80fd\u529b\uff0c\u8981\u4e48\u4e13\u6ce8\u4e8e\u7279\u5b9a\u4efb\u52a1\uff0c\u8981\u4e48\u80fd\u529b\u6709\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u4ec530\u4ebf\u53c2\u6570\u4f46\u80fd\u540c\u65f6\u5b9e\u73b0\u4ee3\u7406\u884c\u4e3a\u3001\u4ee3\u7801\u751f\u6210\u548c\u901a\u7528\u63a8\u7406\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u91cd\u65b0\u5b9a\u4e49\u5c0f\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "1. \u7ed3\u5408\u70b9\u5bf9\u70b9\u548c\u914d\u5bf9\u5956\u52b1\u5efa\u6a21\u63d0\u5347\u63a8\u7406\u548c\u504f\u597d\u5bf9\u9f50\uff1b2. \u4e3a\u4ee3\u7801\u751f\u6210\u8bbe\u8ba1\u590d\u6742\u5ea6\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\uff0c\u4f18\u5316\u6b63\u786e\u6027\u548c\u6548\u7387\uff1b3. \u6df1\u5ea6\u641c\u7d22\u4e2d\u8fdb\u884c\u590d\u6742\u6570\u636e\u5408\u6210\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u878d\u5165\u56de\u5408\u7ea7\u76d1\u7763\uff1b4. \u5b9e\u73b0\u7a33\u5b9a\u7684\u957f\u65f6\u7a0b\u5de5\u5177\u4ea4\u4e92\uff0c\u652f\u6301\u591a\u8fbe600\u4e2a\u5de5\u5177\u8c03\u7528\u56de\u5408\u3002", "result": "Nanbeige4.1-3B\u663e\u8457\u8d85\u8d8a\u540c\u89c4\u6a21\u6a21\u578b(Nanbeige4-3B-2511\u548cQwen3-4B)\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u65b9\u9762\u4f18\u4e8e\u66f4\u5927\u6a21\u578b(Qwen3-30B-A3B)\u3002\u6a21\u578b\u80fd\u53ef\u9760\u6267\u884c\u957f\u8fbe600\u4e2a\u5de5\u5177\u8c03\u7528\u56de\u5408\u7684\u590d\u6742\u95ee\u9898\u89e3\u51b3\uff0c\u5c55\u793a\u4e86\u5c0f\u6a21\u578b\u540c\u65f6\u5177\u5907\u5e7f\u6cdb\u80fd\u529b\u548c\u5f3a\u4e13\u4e1a\u5316\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u5c0f\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u5e7f\u6cdb\u80fd\u529b\u548c\u5f3a\u4e13\u4e1a\u5316\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e8630\u4ebf\u53c2\u6570\u6a21\u578b\u7684\u6f5c\u529b\u3002\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u67b6\u6784\u8bbe\u8ba1\uff0c\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e5f\u80fd\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "code agent"}}
{"id": "2602.13372", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13372", "abs": "https://arxiv.org/abs/2602.13372", "authors": ["Simon Rosen", "Siddarth Singh", "Ebenezer Gelo", "Helen Sarah Robertson", "Ibrahim Suder", "Victoria Williams", "Benjamin Rosman", "Geraud Nangue Tasse", "Steven James"], "title": "MoralityGym: A Benchmark for Evaluating Hierarchical Moral Alignment in Sequential Decision-Making Agents", "comment": "Accepted at AAMAS 2026", "summary": "Evaluating moral alignment in agents navigating conflicting, hierarchically structured human norms is a critical challenge at the intersection of AI safety, moral philosophy, and cognitive science. We introduce Morality Chains, a novel formalism for representing moral norms as ordered deontic constraints, and MoralityGym, a benchmark of 98 ethical-dilemma problems presented as trolley-dilemma-style Gymnasium environments. By decoupling task-solving from moral evaluation and introducing a novel Morality Metric, MoralityGym allows the integration of insights from psychology and philosophy into the evaluation of norm-sensitive reasoning. Baseline results with Safe RL methods reveal key limitations, underscoring the need for more principled approaches to ethical decision-making. This work provides a foundation for developing AI systems that behave more reliably, transparently, and ethically in complex real-world contexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Morality Chains\u5f62\u5f0f\u5316\u65b9\u6cd5\u548cMoralityGym\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u51b2\u7a81\u6027\u3001\u5c42\u6b21\u5316\u4eba\u7c7b\u89c4\u8303\u4e2d\u7684\u9053\u5fb7\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u8bc4\u4f30AI\u4ee3\u7406\u5728\u51b2\u7a81\u6027\u3001\u5c42\u6b21\u5316\u4eba\u7c7b\u89c4\u8303\u4e2d\u7684\u9053\u5fb7\u5bf9\u9f50\u662fAI\u5b89\u5168\u3001\u9053\u5fb7\u54f2\u5b66\u548c\u8ba4\u77e5\u79d1\u5b66\u4ea4\u53c9\u9886\u57df\u7684\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u590d\u6742\u9053\u5fb7\u51b3\u7b56\u7684\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u3002", "method": "1. \u63d0\u51faMorality Chains\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5c06\u9053\u5fb7\u89c4\u8303\u8868\u793a\u4e3a\u6709\u5e8f\u7684\u9053\u4e49\u7ea6\u675f\uff1b2. \u521b\u5efaMoralityGym\u57fa\u51c6\uff0c\u5305\u542b98\u4e2a\u4f26\u7406\u56f0\u5883\u95ee\u9898\uff0c\u4ee5\u7535\u8f66\u56f0\u5883\u98ce\u683c\u7684Gymnasium\u73af\u5883\u5448\u73b0\uff1b3. \u5c06\u4efb\u52a1\u89e3\u51b3\u4e0e\u9053\u5fb7\u8bc4\u4f30\u89e3\u8026\uff1b4. \u5f15\u5165\u65b0\u7684\u9053\u5fb7\u5ea6\u91cf\u6807\u51c6\u3002", "result": "\u4f7f\u7528\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u57fa\u7ebf\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9053\u5fb7\u51b3\u7b56\u65b9\u9762\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u8868\u660e\u9700\u8981\u66f4\u539f\u5219\u6027\u7684\u4f26\u7406\u51b3\u7b56\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5f00\u53d1\u5728\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u4e2d\u884c\u4e3a\u66f4\u53ef\u9760\u3001\u900f\u660e\u548c\u9053\u5fb7\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c06\u5fc3\u7406\u5b66\u548c\u54f2\u5b66\u6d1e\u89c1\u6574\u5408\u5230\u89c4\u8303\u654f\u611f\u63a8\u7406\u7684\u8bc4\u4f30\u4e2d\u3002", "topic": "agent analysis"}}
{"id": "2602.13407", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13407", "abs": "https://arxiv.org/abs/2602.13407", "authors": ["Anhao Zhao", "Ziyang Chen", "Junlong Tong", "Yingqi Fan", "Fanghua Ye", "Shuhao Li", "Yunpu Ma", "Wenjie Li", "Xiaoyu Shen"], "title": "On-Policy Supervised Fine-Tuning for Efficient Reasoning", "comment": null, "summary": "Large reasoning models (LRMs) are commonly trained with reinforcement learning (RL) to explore long chain-of-thought reasoning, achieving strong performance at high computational cost. Recent methods add multi-reward objectives to jointly optimize correctness and brevity, but these complex extensions often destabilize training and yield suboptimal trade-offs. We revisit this objective and challenge the necessity of such complexity. Through principled analysis, we identify fundamental misalignments in this paradigm: KL regularization loses its intended role when correctness and length are directly verifiable, and group-wise normalization becomes ambiguous under multiple reward signals. By removing these two items and simplifying the reward to a truncation-based length penalty, we show that the optimization problem reduces to supervised fine-tuning on self-generated data filtered for both correctness and conciseness. We term this simplified training strategy on-policy SFT. Despite its simplicity, on-policy SFT consistently defines the accuracy-efficiency Pareto frontier. It reduces CoT length by up to 80 while maintaining original accuracy, surpassing more complex RL-based methods across five benchmarks. Furthermore, it significantly enhances training efficiency, reducing GPU memory usage by 50% and accelerating convergence by 70%. Our code is available at https://github.com/EIT-NLP/On-Policy-SFT.", "AI": {"tldr": "\u63d0\u51faon-policy SFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5316\u5956\u52b1\u8bbe\u8ba1\uff08\u4ec5\u4f7f\u7528\u622a\u65ad\u5f0f\u957f\u5ea6\u60e9\u7f5a\uff09\u5c06\u591a\u76ee\u6807RL\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u5728\u81ea\u751f\u6210\u6570\u636e\u4e0a\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u5927\u5e45\u7f29\u77ed\u63a8\u7406\u94fe\u957f\u5ea6\u5e76\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f46\u6dfb\u52a0\u591a\u5956\u52b1\u76ee\u6807\uff08\u5982\u540c\u65f6\u4f18\u5316\u6b63\u786e\u6027\u548c\u7b80\u6d01\u6027\uff09\u4f1a\u4f7f\u8bad\u7ec3\u590d\u6742\u5316\u3001\u4e0d\u7a33\u5b9a\u4e14\u4ea7\u751f\u6b21\u4f18\u6743\u8861\u3002\u4f5c\u8005\u8d28\u7591\u8fd9\u79cd\u590d\u6742\u6027\u7684\u5fc5\u8981\u6027\uff0c\u5e0c\u671b\u901a\u8fc7\u7b80\u5316\u65b9\u6cd5\u83b7\u5f97\u66f4\u597d\u7684\u6548\u679c\u3002", "method": "\u901a\u8fc7\u539f\u7406\u5206\u6790\u53d1\u73b0KL\u6b63\u5219\u5316\u548c\u7ec4\u5f52\u4e00\u5316\u5728\u591a\u5956\u52b1\u573a\u666f\u4e0b\u5b58\u5728\u6839\u672c\u6027\u9519\u4f4d\u95ee\u9898\u3002\u79fb\u9664\u8fd9\u4e9b\u590d\u6742\u7ec4\u4ef6\uff0c\u5c06\u5956\u52b1\u7b80\u5316\u4e3a\u57fa\u4e8e\u622a\u65ad\u7684\u957f\u5ea6\u60e9\u7f5a\uff0c\u5c06\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u5728\u81ea\u751f\u6210\u6570\u636e\uff08\u540c\u65f6\u6ee1\u8db3\u6b63\u786e\u6027\u548c\u7b80\u6d01\u6027\uff09\u4e0a\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u79f0\u4e3aon-policy SFT\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0con-policy SFT\u59cb\u7ec8\u5b9a\u4e49\u51c6\u786e\u7387-\u6548\u7387\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u63a8\u7406\u94fe\u957f\u5ea6\u51cf\u5c11\u9ad8\u8fbe80%\u7684\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u590d\u6742\u7684RL\u65b9\u6cd5\u3002\u8bad\u7ec3\u6548\u7387\u663e\u8457\u63d0\u5347\uff1aGPU\u5185\u5b58\u4f7f\u7528\u51cf\u5c1150%\uff0c\u6536\u655b\u901f\u5ea6\u52a0\u5feb70%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7b80\u5316\u5956\u52b1\u8bbe\u8ba1\u548c\u79fb\u9664\u4e0d\u5fc5\u8981\u7684\u590d\u6742\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u5c06\u591a\u76ee\u6807RL\u4f18\u5316\u95ee\u9898\u6709\u6548\u8f6c\u5316\u4e3a\u76d1\u7763\u5fae\u8c03\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u63a8\u7406\u7b80\u6d01\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.14054", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.14054", "abs": "https://arxiv.org/abs/2602.14054", "authors": ["Jizheng Chen", "Weiming Zhang", "Xinyi Dai", "Weiwen Liu", "Kounianhua Du", "Yasheng Wang", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "title": "LogitsCoder: Towards Efficient Chain-of-Thought Path Search via Logits Preference Decoding for Code Generation", "comment": null, "summary": "Code generation remains a challenging task that requires precise and structured reasoning. Existing Test Time Scaling (TTS) methods, including structured tree search, have made progress in exploring reasoning paths but still face two major challenges: (1) underthinking, where reasoning chains tend to be shallow and fail to capture the full complexity of problems; and (2) overthinking, where overly verbose reasoning leads to inefficiency and increased computational costs. To address these issues, we propose LogitsCoder, a novel framework that enhances chain-of-thought reasoning through lightweight, logit-level control mechanisms for code generation. LogitsCoder iteratively generates and refines reasoning steps by first steering token selection toward statistically preferred patterns via Logits Preference Decoding, then selecting and aggregating diverse reasoning paths using Logits Rank Based Path Selection and Thoughts Aggregation. This results in coherent and effective reasoning chains that balance depth and efficiency. Extensive experiments demonstrate that LogitsCoder produces more efficient and higher-quality reasoning chains, leading to superior code generation performance compared to baseline methods.", "AI": {"tldr": "LogitsCoder\uff1a\u901a\u8fc7\u8f7b\u91cf\u7ea7logit\u7ea7\u63a7\u5236\u673a\u5236\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u7684\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\"\u6b20\u601d\u8003\"\u548c\"\u8fc7\u601d\u8003\"\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u65b9\u6cd5\uff08\u5305\u62ec\u7ed3\u6784\u5316\u6811\u641c\u7d22\uff09\u5728\u63a2\u7d22\u63a8\u7406\u8def\u5f84\u65f6\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1\uff09\"\u6b20\u601d\u8003\" - \u63a8\u7406\u94fe\u8fc7\u4e8e\u6d45\u663e\uff0c\u65e0\u6cd5\u6355\u6349\u95ee\u9898\u7684\u5168\u90e8\u590d\u6742\u6027\uff1b2\uff09\"\u8fc7\u601d\u8003\" - \u63a8\u7406\u8fc7\u4e8e\u5197\u957f\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u3002", "method": "LogitsCoder\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7logit\u7ea7\u63a7\u5236\u673a\u5236\u589e\u5f3a\u601d\u7ef4\u94fe\u63a8\u7406\uff1a1\uff09Logits Preference Decoding\u5f15\u5bfctoken\u9009\u62e9\u671d\u5411\u7edf\u8ba1\u504f\u597d\u7684\u6a21\u5f0f\uff1b2\uff09Logits Rank Based Path Selection\u9009\u62e9\u548c\u805a\u5408\u591a\u6837\u5316\u7684\u63a8\u7406\u8def\u5f84\uff1b3\uff09Thoughts Aggregation\u6574\u5408\u63a8\u7406\u94fe\uff0c\u5b9e\u73b0\u6df1\u5ea6\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLogitsCoder\u80fd\u591f\u4ea7\u751f\u66f4\u9ad8\u6548\u3001\u66f4\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u94fe\uff0c\u5728\u4ee3\u7801\u751f\u6210\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "LogitsCoder\u901a\u8fc7logit\u7ea7\u63a7\u5236\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u751f\u6210\u4e2d\u7684\"\u6b20\u601d\u8003\"\u548c\"\u8fc7\u601d\u8003\"\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u8fde\u8d2f\u6709\u6548\u7684\u63a8\u7406\u94fe\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "2602.13477", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13477", "abs": "https://arxiv.org/abs/2602.13477", "authors": ["Akshat Naik", "Jay Culligan", "Yarin Gal", "Philip Torr", "Rahaf Aljundi", "Alasdair Paren", "Adel Bibi"], "title": "OMNI-LEAK: Orchestrator Multi-Agent Network Induced Data Leakage", "comment": "Prepint, under review for ICML 2026", "summary": "As Large Language Model (LLM) agents become more capable, their coordinated use in the form of multi-agent systems is anticipated to emerge as a practical paradigm. Prior work has examined the safety and misuse risks associated with agents. However, much of this has focused on the single-agent case and/or setups missing basic engineering safeguards such as access control, revealing a scarcity of threat modeling in multi-agent systems. We investigate the security vulnerabilities of a popular multi-agent pattern known as the orchestrator setup, in which a central agent decomposes and delegates tasks to specialized agents. Through red-teaming a concrete setup representative of a likely future use case, we demonstrate a novel attack vector, OMNI-LEAK, that compromises several agents to leak sensitive data through a single indirect prompt injection, even in the \\textit{presence of data access control}. We report the susceptibility of frontier models to different categories of attacks, finding that both reasoning and non-reasoning models are vulnerable, even when the attacker lacks insider knowledge of the implementation details. Our work highlights the importance of safety research to generalize from single-agent to multi-agent settings, in order to reduce the serious risks of real-world privacy breaches and financial losses and overall public trust in AI agents.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6d41\u884c\u7684\u7f16\u6392\u5668\u67b6\u6784\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u53d1\u73b0\u4e86\u4e00\u79cd\u540d\u4e3aOMNI-LEAK\u7684\u65b0\u578b\u653b\u51fb\u5411\u91cf\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u6b21\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u6cc4\u9732\u654f\u611f\u6570\u636e\uff0c\u5373\u4f7f\u5b58\u5728\u6570\u636e\u8bbf\u95ee\u63a7\u5236\u673a\u5236\u3002", "motivation": "\u968f\u7740LLM\u667a\u80fd\u4f53\u80fd\u529b\u589e\u5f3a\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5c06\u6210\u4e3a\u5b9e\u7528\u8303\u5f0f\u3002\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u667a\u80fd\u4f53\u5b89\u5168\u98ce\u9669\uff0c\u7f3a\u4e4f\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5a01\u80c1\u5efa\u6a21\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u57fa\u672c\u5de5\u7a0b\u9632\u62a4\u63aa\u65bd\uff08\u5982\u8bbf\u95ee\u63a7\u5236\uff09\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u901a\u8fc7\u7ea2\u961f\u6d4b\u8bd5\u4e00\u4e2a\u4ee3\u8868\u672a\u6765\u53ef\u80fd\u7528\u4f8b\u7684\u5177\u4f53\u7f16\u6392\u5668\u8bbe\u7f6e\uff0c\u7814\u7a76\u6d41\u884c\u7684\u7f16\u6392\u5668\u6a21\u5f0f\uff08\u4e2d\u5fc3\u667a\u80fd\u4f53\u5206\u89e3\u548c\u59d4\u6d3e\u4efb\u52a1\u7ed9\u4e13\u4e1a\u667a\u80fd\u4f53\uff09\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u6d4b\u8bd5\u524d\u6cbf\u6a21\u578b\u5bf9\u4e0d\u540c\u7c7b\u522b\u653b\u51fb\u7684\u6613\u611f\u6027\u3002", "result": "\u53d1\u73b0\u4e86OMNI-LEAK\u653b\u51fb\u5411\u91cf\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u6b21\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u6cc4\u9732\u654f\u611f\u6570\u636e\uff0c\u5373\u4f7f\u5b58\u5728\u6570\u636e\u8bbf\u95ee\u63a7\u5236\u3002\u53d1\u73b0\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u578b\u90fd\u6613\u53d7\u653b\u51fb\uff0c\u5373\u4f7f\u653b\u51fb\u8005\u7f3a\u4e4f\u5b9e\u73b0\u7ec6\u8282\u7684\u5185\u90e8\u77e5\u8bc6\u3002", "conclusion": "\u5b89\u5168\u7814\u7a76\u9700\u8981\u4ece\u5355\u667a\u80fd\u4f53\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u8bbe\u7f6e\uff0c\u4ee5\u51cf\u5c11\u73b0\u5b9e\u4e16\u754c\u9690\u79c1\u6cc4\u9732\u3001\u8d22\u52a1\u635f\u5931\u7684\u98ce\u9669\uff0c\u7ef4\u62a4\u516c\u4f17\u5bf9AI\u667a\u80fd\u4f53\u7684\u4fe1\u4efb\u3002", "topic": "agent analysis"}}
{"id": "2602.13516", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13516", "abs": "https://arxiv.org/abs/2602.13516", "authors": ["Jaechul Roh", "Eugene Bagdasarian", "Hamed Haddadi", "Ali Shahin Shamsabadi"], "title": "SPILLage: Agentic Oversharing on the Web", "comment": null, "summary": "LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act \"in the wild\", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of \"output\" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\"\u81ea\u7136\u4ee3\u7406\u8fc7\u5ea6\u5206\u4eab\"\u6982\u5ff5\uff0c\u5373\u7f51\u7edc\u4ee3\u7406\u5728\u5b8c\u6210\u4efb\u52a1\u65f6\u65e0\u610f\u4e2d\u6cc4\u9732\u7528\u6237\u4efb\u52a1\u65e0\u5173\u4fe1\u606f\uff0c\u5e76\u5f15\u5165SPILLage\u6846\u67b6\u4ece\u5185\u5bb9\u548c\u884c\u4e3a\u4e24\u4e2a\u7ef4\u5ea6\u5206\u6790\u8fc7\u5ea6\u5206\u4eab\u73b0\u8c61\u3002", "motivation": "\u968f\u7740LLM\u9a71\u52a8\u7684\u4ee3\u7406\u5f00\u59cb\u5728\u5f00\u653e\u7f51\u7edc\u4e0a\u81ea\u52a8\u5316\u7528\u6237\u4efb\u52a1\uff0c\u5b83\u4eec\u7ecf\u5e38\u8bbf\u95ee\u7528\u6237\u8d44\u6e90\uff08\u5982\u7535\u5b50\u90ae\u4ef6\u548c\u65e5\u5386\uff09\u3002\u4e0e\u53d7\u63a7\u804a\u5929\u673a\u5668\u4eba\u73af\u5883\u4e0d\u540c\uff0c\u7f51\u7edc\u4ee3\u7406\u5728\"\u91ce\u5916\"\u884c\u52a8\uff0c\u4e0e\u7b2c\u4e09\u65b9\u4e92\u52a8\u5e76\u7559\u4e0b\u884c\u52a8\u75d5\u8ff9\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u7f51\u7edc\u4ee3\u7406\u5728\u5b8c\u6210\u4efb\u52a1\u65f6\u5982\u4f55\u5904\u7406\u7528\u6237\u8d44\u6e90\uff0c\u7279\u522b\u662f\u5982\u4f55\u9632\u6b62\u65e0\u610f\u4e2d\u6cc4\u9732\u4efb\u52a1\u65e0\u5173\u7684\u7528\u6237\u4fe1\u606f\u3002", "method": "\u5f15\u5165SPILLage\u6846\u67b6\uff0c\u4ece\u6e20\u9053\uff08\u5185\u5bb9vs\u884c\u4e3a\uff09\u548c\u76f4\u63a5\u6027\uff08\u663e\u5f0fvs\u9690\u5f0f\uff09\u4e24\u4e2a\u7ef4\u5ea6\u8868\u5f81\u8fc7\u5ea6\u5206\u4eab\u3002\u5728\u5b9e\u65f6\u7535\u5546\u7f51\u7ad9\u4e0a\u5bf9180\u4e2a\u4efb\u52a1\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u771f\u5b9e\u6807\u6ce8\u533a\u5206\u4efb\u52a1\u76f8\u5173\u548c\u4efb\u52a1\u65e0\u5173\u5c5e\u6027\u3002\u8fdb\u884c\u4e861,080\u6b21\u8fd0\u884c\uff0c\u6db5\u76d6\u4e24\u4e2a\u4ee3\u7406\u6846\u67b6\u548c\u4e09\u4e2a\u9aa8\u5e72LLM\uff0c\u5e76\u8bc4\u4f30\u4e86\u63d0\u793a\u7ea7\u7f13\u89e3\u63aa\u65bd\u7684\u6548\u679c\u3002", "result": "\u8fc7\u5ea6\u5206\u4eab\u73b0\u8c61\u666e\u904d\u5b58\u5728\uff0c\u884c\u4e3a\u8fc7\u5ea6\u5206\u4eab\u6bd4\u5185\u5bb9\u8fc7\u5ea6\u5206\u4eab\u591a5\u500d\u3002\u63d0\u793a\u7ea7\u7f13\u89e3\u63aa\u65bd\u4e0b\u8fd9\u79cd\u73b0\u8c61\u6301\u7eed\u5b58\u5728\u751a\u81f3\u6076\u5316\u3002\u7136\u800c\uff0c\u5728\u6267\u884c\u524d\u79fb\u9664\u4efb\u52a1\u65e0\u5173\u4fe1\u606f\u53ef\u5c06\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u9ad8\u8fbe17.9%\uff0c\u8868\u660e\u51cf\u5c11\u8fc7\u5ea6\u5206\u4eab\u80fd\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "\u4fdd\u62a4\u7f51\u7edc\u4ee3\u7406\u4e2d\u7684\u9690\u79c1\u662f\u4e00\u4e2a\u6839\u672c\u6027\u6311\u6218\uff0c\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\"\u8f93\u51fa\"\u89c6\u89d2\uff0c\u4e0d\u4ec5\u8981\u8003\u8651\u4ee3\u7406\u8f93\u5165\u4ec0\u4e48\uff0c\u8fd8\u8981\u8003\u8651\u5b83\u4eec\u5728\u7f51\u7edc\u4e0a\u7684\u884c\u4e3a\u3002\u51cf\u5c11\u8fc7\u5ea6\u5206\u4eab\u4e0d\u4ec5\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u8fd8\u80fd\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "topic": "agent analysis"}}
{"id": "2602.14069", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.14069", "abs": "https://arxiv.org/abs/2602.14069", "authors": ["Ruipeng Jia", "Yunyi Yang", "Yuxin Wu", "Yongbo Gai", "Siyuan Tao", "Mengyu Zhou", "Jianhe Lin", "Xiaoxi Jiang", "Guanjun Jiang"], "title": "Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric", "comment": null, "summary": "Scalar reward models compress multi-dimensional human preferences into a single opaque score, creating an information bottleneck that often leads to brittleness and reward hacking in open-ended alignment. We argue that robust alignment for non-verifiable tasks is fundamentally a principle generalization problem: reward should not be a learned function internalized into a judge, but an explicit reasoning process executed under inspectable principles. To operationalize this view, we present the Open Rubric System (OpenRS), a plug-and-play, rubrics-based LLM-as-a-Judge framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components when ground-truth or programmatic checks are available. OpenRS uses an explicit meta-rubric -- a constitution-like specification that governs how rubrics are instantiated, weighted, and enforced -- and instantiates adaptive rubrics on the fly by conditioning on the semantic differences between two candidate responses. It then performs criterion-wise pairwise comparisons and aggregates criterion-level preferences externally, avoiding pointwise weighted scalarization while improving discriminability in open-ended settings. To keep principles consistent yet editable across various domains, we introduce a two-level meta-rubric refinement pipeline (automated evolutionary refinement for general principles and a reproducible human-in-the-loop procedure for domain principles), complemented with pointwise verifiable rubrics that act as both guardrails against degenerate behaviors and a source of verifiable reward for objective sub-tasks. Finally, we instantiate OpenRS as reward supervision in pairwise RL training.", "AI": {"tldr": "OpenRS\u662f\u4e00\u4e2a\u57fa\u4e8e\u660e\u786e\u539f\u5219\u7684LLM-as-a-Judge\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u68c0\u67e5\u7684\u63a8\u7406\u8fc7\u7a0b\u800c\u975e\u5355\u4e00\u6807\u91cf\u5956\u52b1\u6765\u89e3\u51b3\u5f00\u653e\u5bf9\u9f50\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u5143\u51c6\u5219\u548c\u53ef\u9a8c\u8bc1\u51c6\u5219\u6765\u63d0\u4f9b\u7ea6\u675f\u548c\u5956\u52b1\u3002", "motivation": "\u4f20\u7edf\u6807\u91cf\u5956\u52b1\u6a21\u578b\u5c06\u591a\u7ef4\u4eba\u7c7b\u504f\u597d\u538b\u7f29\u4e3a\u5355\u4e00\u4e0d\u900f\u660e\u5206\u6570\uff0c\u9020\u6210\u4fe1\u606f\u74f6\u9888\uff0c\u5bfc\u81f4\u5f00\u653e\u5bf9\u9f50\u4e2d\u7684\u8106\u5f31\u6027\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002\u4f5c\u8005\u8ba4\u4e3a\u7a33\u5065\u5bf9\u9f50\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u539f\u5219\u6cdb\u5316\u95ee\u9898\uff0c\u5956\u52b1\u4e0d\u5e94\u662f\u5185\u90e8\u5316\u7684\u5b66\u4e60\u51fd\u6570\uff0c\u800c\u5e94\u662f\u57fa\u4e8e\u53ef\u68c0\u67e5\u539f\u5219\u7684\u663e\u5f0f\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faOpen Rubric System (OpenRS)\uff0c\u5305\u542b\uff1a1) Pairwise Adaptive Meta-Rubrics (PAMR)\uff1a\u6839\u636e\u5019\u9009\u56de\u7b54\u7684\u8bed\u4e49\u5dee\u5f02\u52a8\u6001\u5b9e\u4f8b\u5316\u51c6\u5219\uff1b2) Pointwise Verifiable Rubrics (PVRs)\uff1a\u63d0\u4f9b\u786c\u7ea6\u675f\u62a4\u680f\u548c\u53ef\u9a8c\u8bc1\u5956\u52b1\uff1b3) \u4e24\u7ea7\u5143\u51c6\u5219\u7cbe\u70bc\u7ba1\u9053\uff1a\u81ea\u52a8\u5316\u8fdb\u5316\u7cbe\u70bc\u901a\u7528\u539f\u5219\uff0c\u4eba\u673a\u534f\u540c\u7cbe\u70bc\u9886\u57df\u539f\u5219\uff1b4) \u4f5c\u4e3a\u5956\u52b1\u76d1\u7763\u5e94\u7528\u4e8e\u6210\u5bf9RL\u8bad\u7ec3\u3002", "result": "OpenRS\u907f\u514d\u4e86\u70b9\u52a0\u6743\u6807\u91cf\u5316\uff0c\u901a\u8fc7\u51c6\u5219\u7ea7\u6210\u5bf9\u6bd4\u8f83\u548c\u5916\u90e8\u504f\u597d\u805a\u5408\uff0c\u63d0\u9ad8\u4e86\u5f00\u653e\u73af\u5883\u4e0b\u7684\u5224\u522b\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u5219\u7684\u4e00\u81f4\u6027\u548c\u53ef\u7f16\u8f91\u6027\u3002", "conclusion": "OpenRS\u5c06\u5956\u52b1\u4ece\u5185\u90e8\u5316\u51fd\u6570\u8f6c\u53d8\u4e3a\u57fa\u4e8e\u53ef\u68c0\u67e5\u539f\u5219\u7684\u663e\u5f0f\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e3a\u89e3\u51b3\u975e\u53ef\u9a8c\u8bc1\u4efb\u52a1\u7684\u7a33\u5065\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u51c6\u5219\u548c\u53ef\u9a8c\u8bc1\u7ec4\u4ef6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u548c\u6297\u9ed1\u5ba2\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.13530", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13530", "abs": "https://arxiv.org/abs/2602.13530", "authors": ["Yiheng Shu", "Saisri Padmaja Jonnalagedda", "Xiang Gao", "Bernal Jim\u00e9nez Guti\u00e9rrez", "Weijian Qi", "Kamalika Das", "Huan Sun", "Yu Su"], "title": "REMem: Reasoning with Episodic Memory in Language Agent", "comment": "Accepted by The Fourteenth International Conference on Learning Representations (ICLR 2026) as poster", "summary": "Humans excel at remembering concrete experiences along spatiotemporal contexts and performing reasoning across those events, i.e., the capacity for episodic memory. In contrast, memory in language agents remains mainly semantic, and current agents are not yet capable of effectively recollecting and reasoning over interaction histories. We identify and formalize the core challenges of episodic recollection and reasoning from this gap, and observe that existing work often overlooks episodicity, lacks explicit event modeling, or overemphasizes simple retrieval rather than complex reasoning. We present REMem, a two-phase framework for constructing and reasoning with episodic memory: 1) Offline indexing, where REMem converts experiences into a hybrid memory graph that flexibly links time-aware gists and facts. 2) Online inference, where REMem employs an agentic retriever with carefully curated tools for iterative retrieval over the memory graph. Comprehensive evaluation across four episodic memory benchmarks shows that REMem substantially outperforms state-of-the-art memory systems such as Mem0 and HippoRAG 2, showing 3.4% and 13.4% absolute improvements on episodic recollection and reasoning tasks, respectively. Moreover, REMem also demonstrates more robust refusal behavior for unanswerable questions.", "AI": {"tldr": "REMem\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u6df7\u5408\u8bb0\u5fc6\u56fe\u5e76\u5229\u7528\u667a\u80fd\u68c0\u7d22\u5668\u8fdb\u884c\u8fed\u4ee3\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u667a\u80fd\u4f53\u5728\u60c5\u666f\u8bb0\u5fc6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u3002", "motivation": "\u4eba\u7c7b\u64c5\u957f\u5728\u65f6\u7a7a\u80cc\u666f\u4e0b\u8bb0\u5fc6\u5177\u4f53\u7ecf\u5386\u5e76\u8fdb\u884c\u8de8\u4e8b\u4ef6\u63a8\u7406\uff08\u60c5\u666f\u8bb0\u5fc6\u80fd\u529b\uff09\uff0c\u800c\u5f53\u524d\u8bed\u8a00\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u4e3b\u8981\u662f\u8bed\u4e49\u6027\u7684\uff0c\u65e0\u6cd5\u6709\u6548\u56de\u5fc6\u548c\u63a8\u7406\u4ea4\u4e92\u5386\u53f2\u3002\u73b0\u6709\u5de5\u4f5c\u5f80\u5f80\u5ffd\u89c6\u60c5\u666f\u6027\u3001\u7f3a\u4e4f\u663e\u5f0f\u4e8b\u4ef6\u5efa\u6a21\uff0c\u6216\u8fc7\u5ea6\u5f3a\u8c03\u7b80\u5355\u68c0\u7d22\u800c\u975e\u590d\u6742\u63a8\u7406\u3002", "method": "REMem\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u79bb\u7ebf\u7d22\u5f15\u9636\u6bb5\uff0c\u5c06\u7ecf\u9a8c\u8f6c\u6362\u4e3a\u6df7\u5408\u8bb0\u5fc6\u56fe\uff0c\u7075\u6d3b\u94fe\u63a5\u65f6\u95f4\u611f\u77e5\u7684\u8981\u70b9\u548c\u4e8b\u5b9e\uff1b2\uff09\u5728\u7ebf\u63a8\u7406\u9636\u6bb5\uff0c\u4f7f\u7528\u667a\u80fd\u68c0\u7d22\u5668\u914d\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5de5\u5177\uff0c\u5728\u8bb0\u5fc6\u56fe\u4e0a\u8fdb\u884c\u8fed\u4ee3\u68c0\u7d22\u3002", "result": "\u5728\u56db\u4e2a\u60c5\u666f\u8bb0\u5fc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cREMem\u663e\u8457\u4f18\u4e8eMem0\u548cHippoRAG 2\u7b49\u6700\u5148\u8fdb\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5728\u60c5\u666f\u56de\u5fc6\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u5206\u522b\u5b9e\u73b0\u4e863.4%\u548c13.4%\u7684\u7edd\u5bf9\u63d0\u5347\u3002\u6b64\u5916\uff0cREMem\u5bf9\u4e0d\u53ef\u56de\u7b54\u7684\u95ee\u9898\u8868\u73b0\u51fa\u66f4\u7a33\u5065\u7684\u62d2\u7edd\u884c\u4e3a\u3002", "conclusion": "REMem\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u60c5\u666f\u8bb0\u5fc6\u548c\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u8a00\u667a\u80fd\u4f53\u5728\u60c5\u666f\u8bb0\u5fc6\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8bb0\u5fc6\u80fd\u529b\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.13559", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13559", "abs": "https://arxiv.org/abs/2602.13559", "authors": ["Yuyu Guo", "Wenjie Yang", "Siyuan Yang", "Ziyang Liu", "Cheng Chen", "Yuan Wei", "Yun Hu", "Yang Huang", "Guoliang Hao", "Dongsheng Yuan", "Jianming Wang", "Xin Chen", "Hang Yu", "Lei Lei", "Peng Di"], "title": "OpAgent: Operator Agent for Web Navigation", "comment": null, "summary": "To fulfill user instructions, autonomous web agents must contend with the inherent complexity and volatile nature of real-world websites. Conventional paradigms predominantly rely on Supervised Fine-Tuning (SFT) or Offline Reinforcement Learning (RL) using static datasets. However, these methods suffer from severe distributional shifts, as offline trajectories fail to capture the stochastic state transitions and real-time feedback of unconstrained wide web environments. In this paper, we propose a robust Online Reinforcement Learning WebAgent, designed to optimize its policy through direct, iterative interactions with unconstrained wide websites. Our approach comprises three core innovations: 1) Hierarchical Multi-Task Fine-tuning: We curate a comprehensive mixture of datasets categorized by functional primitives -- Planning, Acting, and Grounding -- establishing a Vision-Language Model (VLM) with strong instruction-following capabilities for Web GUI tasks. 2) Online Agentic RL in the Wild: We develop an online interaction environment and fine-tune the VLM using a specialized RL pipeline. We introduce a Hybrid Reward Mechanism that combines a ground-truth-agnostic WebJudge for holistic outcome assessment with a Rule-based Decision Tree (RDT) for progress reward. This system effectively mitigates the credit assignment challenge in long-horizon navigation. Notably, our RL-enhanced model achieves a 38.1\\% success rate (pass@5) on WebArena, outperforming all existing monolithic baselines. 3) Operator Agent: We introduce a modular agentic framework, namely \\textbf{OpAgent}, orchestrating a Planner, Grounder, Reflector, and Summarizer. This synergy enables robust error recovery and self-correction, elevating the agent's performance to a new State-of-the-Art (SOTA) success rate of \\textbf{71.6\\%}.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60WebAgent\uff0c\u901a\u8fc7\u5206\u5c42\u591a\u4efb\u52a1\u5fae\u8c03\u3001\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u548c\u6a21\u5757\u5316\u64cd\u4f5c\u4ee3\u7406\u6846\u67b6\uff0c\u5728WebArena\u4e0a\u5b9e\u73b071.6%\u7684SOTA\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u7684\u76d1\u7763\u5fae\u8c03\u6216\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u65e0\u6cd5\u9002\u5e94\u771f\u5b9e\u7f51\u7ad9\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\uff0c\u9700\u8981\u80fd\u591f\u76f4\u63a5\u4e0e\u65e0\u7ea6\u675f\u7f51\u7ad9\u4ea4\u4e92\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u3002", "method": "1) \u5206\u5c42\u591a\u4efb\u52a1\u5fae\u8c03\uff1a\u6309\u89c4\u5212\u3001\u6267\u884c\u3001\u57fa\u7840\u4e09\u7c7b\u529f\u80fd\u539f\u8bed\u6574\u7406\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff1b2) \u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff1a\u5f00\u53d1\u5728\u7ebf\u4ea4\u4e92\u73af\u5883\uff0c\u4f7f\u7528\u6df7\u5408\u5956\u52b1\u673a\u5236\uff08WebJudge\u6574\u4f53\u8bc4\u4f30+RDT\u8fdb\u5ea6\u5956\u52b1\uff09\uff1b3) \u64cd\u4f5c\u4ee3\u7406\u6846\u67b6\uff1a\u5305\u542b\u89c4\u5212\u5668\u3001\u57fa\u7840\u5668\u3001\u53cd\u601d\u5668\u3001\u603b\u7ed3\u5668\u7684\u6a21\u5757\u5316\u7cfb\u7edf\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u6a21\u578b\u5728WebArena\u4e0a\u8fbe\u523038.1%\u6210\u529f\u7387\uff08pass@5\uff09\uff0c\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u5355\u4f53\u57fa\u7ebf\uff1b\u6a21\u5757\u5316OpAgent\u6846\u67b6\u8fdb\u4e00\u6b65\u63d0\u5347\u523071.6%\u7684SOTA\u6210\u529f\u7387\u3002", "conclusion": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0e\u6a21\u5757\u5316\u4ee3\u7406\u6846\u67b6\u7684\u7ed3\u5408\u80fd\u6709\u6548\u89e3\u51b3\u771f\u5b9e\u7f51\u7ad9\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\uff0c\u663e\u8457\u63d0\u5347WebAgent\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.13568", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13568", "abs": "https://arxiv.org/abs/2602.13568", "authors": ["Anooshka Bajaj", "Zoran Tiganj"], "title": "Who Do LLMs Trust? Human Experts Matter More Than Other LLMs", "comment": null, "summary": "Large language models (LLMs) increasingly operate in environments where they encounter social information such as other agents' answers, tool outputs, or human recommendations. In humans, such inputs influence judgments in ways that depend on the source's credibility and the strength of consensus. This paper investigates whether LLMs exhibit analogous patterns of influence and whether they privilege feedback from humans over feedback from other LLMs. Across three binary decision-making tasks, reading comprehension, multi-step reasoning, and moral judgment, we present four instruction-tuned LLMs with prior responses attributed either to friends, to human experts, or to other LLMs. We manipulate whether the group is correct and vary the group size. In a second experiment, we introduce direct disagreement between a single human and a single LLM. Across tasks, models conform significantly more to responses labeled as coming from human experts, including when that signal is incorrect, and revise their answers toward experts more readily than toward other LLMs. These results reveal that expert framing acts as a strong prior for contemporary LLMs, suggesting a form of credibility-sensitive social influence that generalizes across decision domains.", "AI": {"tldr": "LLMs\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5bf9\u4eba\u7c7b\u4e13\u5bb6\u53cd\u9988\u7684\u663e\u8457\u987a\u4ece\uff0c\u5373\u4f7f\u4e13\u5bb6\u610f\u89c1\u9519\u8bef\uff0c\u8fd9\u79cd\u987a\u4ece\u4e5f\u5f3a\u4e8e\u5bf9\u5176\u4ed6LLM\u53cd\u9988\u7684\u54cd\u5e94\uff0c\u663e\u793a\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4fe1\u8a89\u654f\u611f\u793e\u4f1a\u5f71\u54cd\u6a21\u5f0f\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u9047\u5230\u793e\u4f1a\u4fe1\u606f\uff08\u5982\u5176\u4ed6\u4ee3\u7406\u7684\u56de\u7b54\u3001\u5de5\u5177\u8f93\u51fa\u6216\u4eba\u7c7b\u5efa\u8bae\uff09\u65f6\u662f\u5426\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u793e\u4f1a\u5f71\u54cd\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u662f\u5426\u66f4\u503e\u5411\u4e8e\u63a5\u53d7\u4eba\u7c7b\u4e13\u5bb6\u7684\u53cd\u9988\u800c\u975e\u5176\u4ed6LLM\u7684\u53cd\u9988\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u4e8c\u5143\u51b3\u7b56\u4efb\u52a1\uff08\u9605\u8bfb\u7406\u89e3\u3001\u591a\u6b65\u63a8\u7406\u3001\u9053\u5fb7\u5224\u65ad\uff09\uff0c\u5411\u56db\u4e2a\u6307\u4ee4\u8c03\u4f18\u7684LLM\u5448\u73b0\u6807\u6ce8\u4e3a\u670b\u53cb\u3001\u4eba\u7c7b\u4e13\u5bb6\u6216\u5176\u4ed6LLM\u6765\u6e90\u7684\u5148\u524d\u56de\u7b54\u3002\u64cd\u7eb5\u7fa4\u4f53\u6b63\u786e\u6027\u5e76\u6539\u53d8\u7fa4\u4f53\u89c4\u6a21\u3002\u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\u5f15\u5165\u5355\u4e2a\u4eba\u7c7b\u4e0e\u5355\u4e2aLLM\u4e4b\u95f4\u7684\u76f4\u63a5\u5206\u6b67\u3002", "result": "\u5728\u6240\u6709\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u663e\u8457\u66f4\u503e\u5411\u4e8e\u987a\u4ece\u6807\u6ce8\u4e3a\u4eba\u7c7b\u4e13\u5bb6\u7684\u56de\u7b54\uff0c\u5373\u4f7f\u8be5\u4fe1\u53f7\u662f\u9519\u8bef\u7684\uff0c\u5e76\u4e14\u5411\u4e13\u5bb6\u53cd\u9988\u4fee\u6b63\u7b54\u6848\u7684\u610f\u613f\u5f3a\u4e8e\u5411\u5176\u4ed6LLM\u53cd\u9988\u3002\u4e13\u5bb6\u6846\u67b6\u4f5c\u4e3a\u5f53\u4ee3LLMs\u7684\u5f3a\u5148\u9a8c\u3002", "conclusion": "LLMs\u8868\u73b0\u51fa\u4e00\u79cd\u4fe1\u8a89\u654f\u611f\u7684\u793e\u4f1a\u5f71\u54cd\u5f62\u5f0f\uff0c\u8fd9\u79cd\u5f71\u54cd\u5728\u4e0d\u540c\u51b3\u7b56\u9886\u57df\u4e2d\u5177\u6709\u666e\u904d\u6027\uff0c\u4e13\u5bb6\u6846\u67b6\u5bf9LLMs\u4ea7\u751f\u5f3a\u70c8\u5f71\u54cd\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u793e\u4f1a\u5f71\u54cd\u6a21\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2602.14080", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14080", "abs": "https://arxiv.org/abs/2602.14080", "authors": ["Nitay Calderon", "Eyal Ben-David", "Zorik Gekhman", "Eran Ofek", "Gal Yona"], "title": "Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality", "comment": null, "summary": "Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u533a\u5206LLM\u4e8b\u5b9e\u6027\u9519\u8bef\u6765\u6e90\u7684\u6846\u67b6\uff1a\u5c06\u9519\u8bef\u5206\u4e3a\u77e5\u8bc6\u7f3a\u5931\uff08\u7a7a\u4e66\u67b6\uff09\u548c\u77e5\u8bc6\u8bbf\u95ee\u5931\u8d25\uff08\u4e22\u5931\u94a5\u5319\uff09\uff0c\u5e76\u5f15\u5165WikiProfile\u57fa\u51c6\u6765\u8bc4\u4f30\u4e8b\u5b9e\u7684\u7f16\u7801\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u5b9e\u6027\u8bc4\u4f30\u65b9\u6cd5\u5c06\u6240\u6709\u9519\u8bef\u540c\u7b49\u5bf9\u5f85\uff0c\u65e0\u6cd5\u533a\u5206\u9519\u8bef\u662f\u6e90\u4e8e\u6a21\u578b\u7f3a\u4e4f\u76f8\u5173\u77e5\u8bc6\uff08\u7a7a\u4e66\u67b6\u95ee\u9898\uff09\u8fd8\u662f\u65e0\u6cd5\u8bbf\u95ee\u5df2\u7f16\u7801\u7684\u77e5\u8bc6\uff08\u4e22\u5931\u94a5\u5319\u95ee\u9898\uff09\u3002\u8fd9\u9650\u5236\u4e86\u6211\u4eec\u5bf9LLM\u77e5\u8bc6\u80fd\u529b\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u884c\u4e3a\u6846\u67b6\uff0c\u5728\u4e8b\u5b9e\u5c42\u9762\u800c\u975e\u95ee\u9898\u5c42\u9762\u5206\u6790\u77e5\u8bc6\uff0c\u5c06\u6bcf\u4e2a\u4e8b\u5b9e\u5206\u7c7b\u4e3a\uff1a\u662f\u5426\u7f16\u7801\u3001\u53ef\u8bbf\u95ee\u6027\uff08\u65e0\u6cd5\u56de\u5fc6\u3001\u53ef\u76f4\u63a5\u56de\u5fc6\u3001\u9700\u8981\u63a8\u7406\u8ba1\u7b97\uff09\u3002\u6784\u5efaWikiProfile\u57fa\u51c6\uff0c\u4f7f\u7528\u57fa\u4e8e\u7f51\u7edc\u641c\u7d22\u7684\u63d0\u793aLLM\u81ea\u52a8\u6784\u5efa\uff0c\u5305\u542b400\u4e07\u6761\u6765\u81ea13\u4e2aLLM\u7684\u54cd\u5e94\u3002", "result": "\u524d\u6cbf\u6a21\u578b\uff08GPT-5\u548cGemini-3\uff09\u5728\u57fa\u51c6\u4e0a\u7f16\u7801\u4e8695-98%\u7684\u4e8b\u5b9e\uff0c\u8868\u660e\u7f16\u7801\u63a5\u8fd1\u9971\u548c\u3002\u4f46\u56de\u5fc6\u4ecd\u662f\u4e3b\u8981\u74f6\u9888\uff1a\u8bb8\u591a\u4e4b\u524d\u5f52\u56e0\u4e8e\u77e5\u8bc6\u7f3a\u5931\u7684\u9519\u8bef\u5b9e\u9645\u4e0a\u6e90\u4e8e\u8bbf\u95ee\u5931\u8d25\u3002\u8fd9\u4e9b\u5931\u8d25\u5177\u6709\u7cfb\u7edf\u6027\uff0c\u5c24\u5176\u5f71\u54cd\u957f\u5c3e\u4e8b\u5b9e\u548c\u53cd\u5411\u95ee\u9898\u3002\u601d\u8003\uff08\u63a8\u7406\u8ba1\u7b97\uff09\u80fd\u663e\u8457\u6539\u5584\u56de\u5fc6\u3002", "conclusion": "\u672a\u6765\u63d0\u5347LLM\u4e8b\u5b9e\u6027\u53ef\u80fd\u66f4\u4f9d\u8d56\u4e8e\u6539\u8fdb\u6a21\u578b\u5982\u4f55\u5229\u7528\u5df2\u7f16\u7801\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u800c\u975e\u5355\u7eaf\u6269\u5927\u89c4\u6a21\u3002\u8bbf\u95ee\u673a\u5236\u548c\u63a8\u7406\u80fd\u529b\u662f\u5173\u952e\u6539\u8fdb\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.13802", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13802", "abs": "https://arxiv.org/abs/2602.13802", "authors": ["Xiaoyu Tao", "Mingyue Cheng", "Chuang Jiang", "Tian Gao", "Huanjian Zhang", "Yaguo Liu"], "title": "Cast-R1: Learning Tool-Augmented Sequential Decision Policies for Time Series Forecasting", "comment": null, "summary": "Time series forecasting has long been dominated by model-centric approaches that formulate prediction as a single-pass mapping from historical observations to future values. Despite recent progress, such formulations often struggle in complex and evolving settings, largely because most forecasting models lack the ability to autonomously acquire informative evidence, reason about potential future changes, or revise predictions through iterative decision processes. In this work, we propose Cast-R1, a learned time series forecasting framework that reformulates forecasting as a sequential decision-making problem. Cast-R1 introduces a memory-based state management mechanism that maintains decision-relevant information across interaction steps, enabling the accumulation of contextual evidence to support long-horizon reasoning. Building on this formulation, forecasting is carried out through a tool-augmented agentic workflow, in which the agent autonomously interacts with a modular toolkit to extract statistical features, invoke lightweight forecasting models for decision support, perform reasoning-based prediction, and iteratively refine forecasts through self-reflection. To train Cast-R1, we adopt a two-stage learning strategy that combines supervised fine-tuning with multi-turn reinforcement learning, together with a curriculum learning scheme that progressively increases task difficulty to improve policy learning. Extensive experiments on multiple real-world time series datasets demonstrate the effectiveness of Cast-R1. We hope this work provides a practical step towards further exploration of agentic paradigms for time series modeling. Our code is available at https://github.com/Xiaoyu-Tao/Cast-R1-TS.", "AI": {"tldr": "Cast-R1\u5c06\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u57fa\u4e8e\u8bb0\u5fc6\u7684\u72b6\u6001\u7ba1\u7406\u673a\u5236\u548c\u5de5\u5177\u589e\u5f3a\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5b9e\u73b0\u81ea\u4e3b\u8bc1\u636e\u6536\u96c6\u3001\u63a8\u7406\u548c\u8fed\u4ee3\u9884\u6d4b\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u4e2d\u5fc3\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u901a\u5e38\u5c06\u9884\u6d4b\u89c6\u4e3a\u4ece\u5386\u53f2\u89c2\u6d4b\u5230\u672a\u6765\u503c\u7684\u5355\u6b21\u6620\u5c04\uff0c\u5728\u590d\u6742\u548c\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u81ea\u4e3b\u83b7\u53d6\u4fe1\u606f\u8bc1\u636e\u3001\u63a8\u7406\u6f5c\u5728\u672a\u6765\u53d8\u5316\u6216\u901a\u8fc7\u8fed\u4ee3\u51b3\u7b56\u8fc7\u7a0b\u4fee\u6b63\u9884\u6d4b\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faCast-R1\u6846\u67b6\uff0c\u5c06\u9884\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\u3002\u91c7\u7528\u57fa\u4e8e\u8bb0\u5fc6\u7684\u72b6\u6001\u7ba1\u7406\u673a\u5236\u7ef4\u62a4\u51b3\u7b56\u76f8\u5173\u4fe1\u606f\uff0c\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u81ea\u4e3b\u4e0e\u6a21\u5757\u5316\u5de5\u5177\u5305\u4ea4\u4e92\uff0c\u5305\u62ec\u63d0\u53d6\u7edf\u8ba1\u7279\u5f81\u3001\u8c03\u7528\u8f7b\u91cf\u7ea7\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u51b3\u7b56\u652f\u6301\u3001\u6267\u884c\u57fa\u4e8e\u63a8\u7406\u7684\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u8fed\u4ee3\u4f18\u5316\u9884\u6d4b\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u7b56\u7565\uff1a\u76d1\u7763\u5fae\u8c03\u7ed3\u5408\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\uff0c\u914d\u5408\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6848\u9010\u6b65\u589e\u52a0\u4efb\u52a1\u96be\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86Cast-R1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u7684\u667a\u80fd\u4f53\u8303\u5f0f\u63a2\u7d22\u63d0\u4f9b\u4e86\u5b9e\u9645\u6b65\u9aa4\uff0c\u5c55\u793a\u4e86\u5c06\u9884\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.13594", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13594", "abs": "https://arxiv.org/abs/2602.13594", "authors": ["Yi Li", "Lianjie Cao", "Faraz Ahmed", "Puneet Sharma", "Bingzhe Li"], "title": "Hippocampus: An Efficient and Scalable Memory Module for Agentic AI", "comment": null, "summary": "Agentic AI require persistent memory to store user-specific histories beyond the limited context window of LLMs. Existing memory systems use dense vector databases or knowledge-graph traversal (or hybrid), incurring high retrieval latency and poor storage scalability. We introduce Hippocampus, an agentic memory management system that uses compact binary signatures for semantic search and lossless token-ID streams for exact content reconstruction. Its core is a Dynamic Wavelet Matrix (DWM) that compresses and co-indexes both streams to support ultra-fast search in the compressed domain, thus avoiding costly dense-vector or graph computations. This design scales linearly with memory size, making it suitable for long-horizon agentic deployments. Empirically, our evaluation shows that Hippocampus reduces end-to-end retrieval latency by up to 31$\\times$ and cuts per-query token footprint by up to 14$\\times$, while maintaining accuracy on both LoCoMo and LongMemEval benchmarks.", "AI": {"tldr": "Hippocampus\u662f\u4e00\u4e2a\u4ee3\u7406AI\u8bb0\u5fc6\u7ba1\u7406\u7cfb\u7edf\uff0c\u4f7f\u7528\u7d27\u51d1\u4e8c\u8fdb\u5236\u7b7e\u540d\u8fdb\u884c\u8bed\u4e49\u641c\u7d22\uff0c\u901a\u8fc7\u52a8\u6001\u5c0f\u6ce2\u77e9\u9635\u538b\u7f29\u548c\u5171\u540c\u7d22\u5f15\uff0c\u5b9e\u73b0\u8d85\u5feb\u901f\u641c\u7d22\uff0c\u663e\u8457\u964d\u4f4e\u68c0\u7d22\u5ef6\u8fdf\u548c\u5b58\u50a8\u5f00\u9500\u3002", "motivation": "\u4ee3\u7406AI\u9700\u8981\u6301\u4e45\u8bb0\u5fc6\u6765\u5b58\u50a8\u8d85\u51faLLM\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u7528\u6237\u7279\u5b9a\u5386\u53f2\u3002\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u4f7f\u7528\u5bc6\u96c6\u5411\u91cf\u6570\u636e\u5e93\u6216\u77e5\u8bc6\u56fe\u8c31\u904d\u5386\uff08\u6216\u6df7\u5408\uff09\uff0c\u5b58\u5728\u9ad8\u68c0\u7d22\u5ef6\u8fdf\u548c\u5b58\u50a8\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165Hippocampus\u7cfb\u7edf\uff0c\u4f7f\u7528\u7d27\u51d1\u4e8c\u8fdb\u5236\u7b7e\u540d\u8fdb\u884c\u8bed\u4e49\u641c\u7d22\uff0c\u65e0\u635f\u4ee4\u724cID\u6d41\u8fdb\u884c\u7cbe\u786e\u5185\u5bb9\u91cd\u5efa\u3002\u6838\u5fc3\u662f\u52a8\u6001\u5c0f\u6ce2\u77e9\u9635\uff08DWM\uff09\uff0c\u538b\u7f29\u548c\u5171\u540c\u7d22\u5f15\u4e24\u4e2a\u6d41\uff0c\u652f\u6301\u538b\u7f29\u57df\u4e2d\u7684\u8d85\u5feb\u901f\u641c\u7d22\uff0c\u907f\u514d\u6602\u8d35\u7684\u5bc6\u96c6\u5411\u91cf\u6216\u56fe\u8ba1\u7b97\u3002", "result": "\u8bc4\u4f30\u663e\u793aHippocampus\u5c06\u7aef\u5230\u7aef\u68c0\u7d22\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe31\u500d\uff0c\u5c06\u6bcf\u67e5\u8be2\u4ee4\u724c\u5360\u7528\u51cf\u5c11\u9ad8\u8fbe14\u500d\uff0c\u540c\u65f6\u5728LoCoMo\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "Hippocampus\u7684\u8bbe\u8ba1\u968f\u8bb0\u5fc6\u5927\u5c0f\u7ebf\u6027\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u957f\u89c6\u91ce\u4ee3\u7406\u90e8\u7f72\uff0c\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8bb0\u5fc6\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.14158", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14158", "abs": "https://arxiv.org/abs/2602.14158", "authors": ["Naeimeh Nourmohammadi", "Md Meem Hossain", "The Anh Han", "Safina Showkat Ara", "Zia Ush Shamszaman"], "title": "A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT, LLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical Query Processing", "comment": "27 pages, 14 figures, 5 tables", "summary": "Large language models (LLMs) show promise for healthcare question answering, but clinical use is limited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose a multi-agent medical QA framework that combines complementary LLMs with evidence retrieval, uncertainty estimation, and bias checks to improve answer reliability. Our approach has two phases. First, we fine-tune three representative LLM families (GPT, LLaMA, and DeepSeek R1) on MedQuAD-derived medical QA data (20k+ question-answer pairs across multiple NIH domains) and benchmark generation quality. DeepSeek R1 achieves the strongest scores (ROUGE-1 0.536 +- 0.04; ROUGE-2 0.226 +-0.03; BLEU 0.098 -+ 0.018) and substantially outperforms the specialised biomedical baseline BioGPT in zero-shot evaluation. Second, we implement a modular multi-agent pipeline in which a Clinical Reasoning agent (fine-tuned LLaMA) produces structured explanations, an Evidence Retrieval agent queries PubMed to ground responses in recent literature, and a Refinement agent (DeepSeek R1) improves clarity and factual consistency; an optional human validation path is triggered for high-risk or high-uncertainty cases. Safety mechanisms include Monte Carlo dropout and perplexity-based uncertainty scoring, plus lexical and sentiment-based bias detection supported by LIME/SHAP-based analyses. In evaluation, the full system achieves 87% accuracy with relevance around 0.80, and evidence augmentation reduces uncertainty (perplexity 4.13) compared to base responses, with mean end-to-end latency of 36.5 seconds under the reported configuration. Overall, the results indicate that agent specialisation and verification layers can mitigate key single-model limitations and provide a practical, extensible design for evidence-based and bias-aware medical AI.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u533b\u7597\u95ee\u7b54\u6846\u67b6\uff0c\u7ed3\u5408LLM\u5fae\u8c03\u3001\u8bc1\u636e\u68c0\u7d22\u3001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u504f\u89c1\u68c0\u6d4b\uff0c\u4ee5\u63d0\u9ad8\u533b\u7597\u95ee\u7b54\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u95ee\u7b54\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4e34\u5e8a\u5e94\u7528\u53d7\u5230\u9a8c\u8bc1\u80fd\u529b\u5f31\u3001\u8bc1\u636e\u57fa\u7840\u4e0d\u8db3\u548c\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u4e0d\u53ef\u9760\u7684\u9650\u5236\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a1) \u5728MedQuAD\u533b\u7597QA\u6570\u636e\u4e0a\u5fae\u8c03\u4e09\u79cdLLM\u5bb6\u65cf(GPT\u3001LLaMA\u3001DeepSeek R1)\uff1b2) \u5b9e\u73b0\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u7ba1\u9053\uff0c\u5305\u62ec\u4e34\u5e8a\u63a8\u7406\u667a\u80fd\u4f53\u3001\u8bc1\u636e\u68c0\u7d22\u667a\u80fd\u4f53\u3001\u7cbe\u70bc\u667a\u80fd\u4f53\uff0c\u4ee5\u53ca\u5b89\u5168\u673a\u5236(\u8499\u7279\u5361\u6d1bdropout\u3001\u56f0\u60d1\u5ea6\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\u3001\u504f\u89c1\u68c0\u6d4b)\u3002", "result": "DeepSeek R1\u8868\u73b0\u6700\u4f73(ROUGE-1 0.536, ROUGE-2 0.226, BLEU 0.098)\uff0c\u5b8c\u6574\u7cfb\u7edf\u8fbe\u523087%\u51c6\u786e\u7387\uff0c\u76f8\u5173\u6027\u7ea60.80\uff0c\u8bc1\u636e\u589e\u5f3a\u964d\u4f4e\u4e0d\u786e\u5b9a\u6027(\u56f0\u60d1\u5ea64.13)\uff0c\u5e73\u5747\u7aef\u5230\u7aef\u5ef6\u8fdf36.5\u79d2\u3002", "conclusion": "\u667a\u80fd\u4f53\u4e13\u4e1a\u5316\u548c\u9a8c\u8bc1\u5c42\u53ef\u4ee5\u7f13\u89e3\u5355\u4e00\u6a21\u578b\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u57fa\u4e8e\u8bc1\u636e\u548c\u504f\u89c1\u611f\u77e5\u7684\u533b\u7597AI\u63d0\u4f9b\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u8bbe\u8ba1\u3002", "topic": "agent analysis"}}
{"id": "2602.13807", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13807", "abs": "https://arxiv.org/abs/2602.13807", "authors": ["Xiaoyu Tao", "Yuchong Wu", "Mingyue Cheng", "Ze Guo", "Tian Gao"], "title": "AnomaMind: Agentic Time Series Anomaly Detection with Tool-Augmented Reasoning", "comment": null, "summary": "Time series anomaly detection is critical in many real-world applications, where effective solutions must localize anomalous regions and support reliable decision-making under complex settings. However, most existing methods frame anomaly detection as a purely discriminative prediction task with fixed feature inputs, rather than an evidence-driven diagnostic process. As a result, they often struggle when anomalies exhibit strong context dependence or diverse patterns. We argue that these limitations stem from the lack of adaptive feature preparation, reasoning-aware detection, and iterative refinement during inference. To address these challenges, we propose AnomaMind, an agentic time series anomaly detection framework that reformulates anomaly detection as a sequential decision-making process. AnomaMind operates through a structured workflow that progressively localizes anomalous intervals in a coarse-to-fine manner, augments detection through multi-turn tool interactions for adaptive feature preparation, and refines anomaly decisions via self-reflection. The workflow is supported by a set of reusable tool engines, enabling context-aware diagnostic analysis. A key design of AnomaMind is an explicitly designed hybrid inference mechanism for tool-augmented anomaly detection. In this mechanism, general-purpose models are responsible for autonomous tool interaction and self-reflective refinement, while core anomaly detection decisions are learned through reinforcement learning under verifiable workflow-level feedback, enabling task-specific optimization within a flexible reasoning framework. Extensive experiments across diverse settings demonstrate that AnomaMind consistently improves anomaly detection performance. The code is available at https://anonymous.4open.science/r/AnomaMind.", "AI": {"tldr": "AnomaMind\u662f\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5c06\u5f02\u5e38\u68c0\u6d4b\u91cd\u6784\u4e3a\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u3001\u81ea\u9002\u5e94\u7279\u5f81\u51c6\u5907\u548c\u81ea\u6211\u53cd\u601d\u6765\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u5f02\u5e38\u68c0\u6d4b\u89c6\u4e3a\u56fa\u5b9a\u7684\u5224\u522b\u9884\u6d4b\u4efb\u52a1\uff0c\u800c\u975e\u8bc1\u636e\u9a71\u52a8\u7684\u8bca\u65ad\u8fc7\u7a0b\uff0c\u96be\u4ee5\u5904\u7406\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u5f3a\u6216\u6a21\u5f0f\u591a\u6837\u7684\u5f02\u5e38\u60c5\u51b5\u3002\u8fd9\u4e9b\u9650\u5236\u6e90\u4e8e\u7f3a\u4e4f\u81ea\u9002\u5e94\u7279\u5f81\u51c6\u5907\u3001\u63a8\u7406\u611f\u77e5\u68c0\u6d4b\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "method": "\u63d0\u51faAnomaMind\u6846\u67b6\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\uff1a\u4ece\u7c97\u5230\u7ec6\u9010\u6b65\u5b9a\u4f4d\u5f02\u5e38\u533a\u95f4\uff0c\u901a\u8fc7\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u8fdb\u884c\u81ea\u9002\u5e94\u7279\u5f81\u51c6\u5907\uff0c\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u4f18\u5316\u5f02\u5e38\u51b3\u7b56\u3002\u6838\u5fc3\u8bbe\u8ba1\u662f\u6df7\u5408\u63a8\u7406\u673a\u5236\uff1a\u901a\u7528\u6a21\u578b\u8d1f\u8d23\u81ea\u4e3b\u5de5\u5177\u4ea4\u4e92\u548c\u81ea\u6211\u53cd\u601d\u4f18\u5316\uff0c\u800c\u6838\u5fc3\u5f02\u5e38\u68c0\u6d4b\u51b3\u7b56\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u5de5\u4f5c\u6d41\u7a0b\u7ea7\u53cd\u9988\u4e0b\u5b66\u4e60\u3002", "result": "\u5728\u591a\u6837\u5316\u8bbe\u7f6e\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAnomaMind\u80fd\u6301\u7eed\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "AnomaMind\u901a\u8fc7\u5c06\u5f02\u5e38\u68c0\u6d4b\u91cd\u6784\u4e3a\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7ed3\u5408\u5de5\u5177\u589e\u5f3a\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u4f9d\u8d56\u548c\u6a21\u5f0f\u591a\u6837\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.13810", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13810", "abs": "https://arxiv.org/abs/2602.13810", "authors": ["Guojian Zhan", "Letian Tao", "Pengcheng Wang", "Yixiao Wang", "Yiheng Li", "Yuxin Chen", "Masayoshi Tomizuka", "Shengbo Eben Li"], "title": "Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation", "comment": "ICLR Oral Presentation", "summary": "Learning expressive and efficient policy functions is a promising direction in reinforcement learning (RL). While flow-based policies have recently proven effective in modeling complex action distributions with a fast deterministic sampling process, they still face a trade-off between expressiveness and computational burden, which is typically controlled by the number of flow steps. In this work, we propose mean velocity policy (MVP), a new generative policy function that models the mean velocity field to achieve the fastest one-step action generation. To ensure its high expressiveness, an instantaneous velocity constraint (IVC) is introduced on the mean velocity field during training. We theoretically prove that this design explicitly serves as a crucial boundary condition, thereby improving learning accuracy and enhancing policy expressiveness. Empirically, our MVP achieves state-of-the-art success rates across several challenging robotic manipulation tasks from Robomimic and OGBench. It also delivers substantial improvements in training and inference speed over existing flow-based policy baselines.", "AI": {"tldr": "MVP\u662f\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5f0f\u7b56\u7565\u51fd\u6570\uff0c\u901a\u8fc7\u5efa\u6a21\u5e73\u5747\u901f\u5ea6\u573a\u5b9e\u73b0\u6700\u5feb\u7684\u4e00\u6b65\u52a8\u4f5c\u751f\u6210\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6d41\u7684\u7b56\u7565\u5728\u5efa\u6a21\u590d\u6742\u52a8\u4f5c\u5206\u5e03\u65f6\u9762\u4e34\u8868\u8fbe\u80fd\u529b\u548c\u8ba1\u7b97\u8d1f\u62c5\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u901a\u5e38\u901a\u8fc7\u6d41\u6b65\u9aa4\u6570\u91cf\u6765\u63a7\u5236\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u8868\u8fbe\u80fd\u529b\u53c8\u80fd\u5b9e\u73b0\u5feb\u901f\u52a8\u4f5c\u751f\u6210\u7684\u7b56\u7565\u51fd\u6570\u3002", "method": "\u63d0\u51fa\u5e73\u5747\u901f\u5ea6\u7b56\u7565\uff08MVP\uff09\uff0c\u5efa\u6a21\u5e73\u5747\u901f\u5ea6\u573a\u5b9e\u73b0\u6700\u5feb\u7684\u4e00\u6b65\u52a8\u4f5c\u751f\u6210\u3002\u5f15\u5165\u77ac\u65f6\u901f\u5ea6\u7ea6\u675f\uff08IVC\uff09\u6765\u786e\u4fdd\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u8be5\u8bbe\u8ba1\u4f5c\u4e3a\u5173\u952e\u8fb9\u754c\u6761\u4ef6\uff0c\u63d0\u9ad8\u5b66\u4e60\u7cbe\u5ea6\u548c\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5728Robomimic\u548cOGBench\u7684\u591a\u4e2a\u6311\u6218\u6027\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6210\u529f\u7387\u3002\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8e\u6d41\u7684\u7b56\u7565\u57fa\u7ebf\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MVP\u901a\u8fc7\u5efa\u6a21\u5e73\u5747\u901f\u5ea6\u573a\u548c\u5f15\u5165\u77ac\u65f6\u901f\u5ea6\u7ea6\u675f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6d41\u7b56\u7565\u5728\u8868\u8fbe\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u7b56\u7565\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.13639", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.13639", "abs": "https://arxiv.org/abs/2602.13639", "authors": ["Linlin Wang", "Tianqing Zhu", "Laiqiao Qin", "Longxiang Gao", "Wanlei Zhou"], "title": "Guided Collaboration in Heterogeneous LLM-Based Multi-Agent Systems via Entropy-Based Understanding Assessment and Experience Retrieval", "comment": null, "summary": "With recent breakthroughs in large language models (LLMs) for reasoning, planning, and complex task generation, artificial intelligence systems are transitioning from isolated single-agent architectures to multi-agent systems with collaborative intelligence. However, in heterogeneous multi-agent systems (HMAS), capability differences among agents give rise to consistent cognitive problems, where strong and weak models fail to contribute effectively. We define the collaboration as a strong-weak system. Through comprehensive experiments, we disclose a counterintuitive phenomenon in the strong-weak system: a strong-weak collaboration may under-perform weak-weak combinations, revealing that cognitive mismatching are key bottlenecks limiting heterogeneous cooperation. To overcome these challenges, we propose an Entropy-Based Adaptive Guidance Framework that dynamically aligns the guidance with the cognitive state of each agent. The framework quantifies the understanding of weak agents through multi-dimensional entropy metrics - covering expression, uncertainty, structure, coherence, and relevance - and adaptively adjusts the intensity of the guidance at light, moderate and intensive levels. Furthermore, a Retrieval-Augmented Generation (RAG) mechanism is incorporated to retain successful collaboration experiences, enabling both immediate adaptation and long-term learning. Extensive experiments on three benchmark datasets, GSM8K, MBPP, and CVRP demonstrate that our approach consistently enhances the effectiveness and stability of heterogeneous collaboration. The results highlight that adaptive guidance not only mitigates cognitive imbalance but also establishes a scalable pathway toward more robust, cooperative multi-agent intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u81ea\u9002\u5e94\u6307\u5bfc\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u6784\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u8ba4\u77e5\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6307\u5bfc\u5f3a\u5ea6\u6765\u63d0\u5347\u534f\u4f5c\u6548\u679c\u3002", "motivation": "\u5f02\u6784\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u5f3a\u6a21\u578b\u548c\u5f31\u6a21\u578b\u4e4b\u95f4\u7684\u80fd\u529b\u5dee\u5f02\u5bfc\u81f4\u8ba4\u77e5\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4f7f\u5f97\u5f3a-\u5f31\u534f\u4f5c\u6548\u679c\u4e0d\u4f73\uff0c\u751a\u81f3\u4e0d\u5982\u5f31-\u5f31\u7ec4\u5408\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u8ba4\u77e5\u5931\u8861\u6765\u63d0\u5347\u5f02\u6784\u534f\u4f5c\u6548\u679c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u71b5\u7684\u81ea\u9002\u5e94\u6307\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u71b5\u5ea6\u91cf\uff08\u8868\u8fbe\u3001\u4e0d\u786e\u5b9a\u6027\u3001\u7ed3\u6784\u3001\u8fde\u8d2f\u6027\u3001\u76f8\u5173\u6027\uff09\u91cf\u5316\u5f31\u667a\u80fd\u4f53\u7684\u7406\u89e3\u7a0b\u5ea6\uff0c\u52a8\u6001\u8c03\u6574\u6307\u5bfc\u5f3a\u5ea6\uff08\u8f7b\u5ea6\u3001\u4e2d\u5ea6\u3001\u91cd\u5ea6\uff09\u3002\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u673a\u5236\u4fdd\u7559\u6210\u529f\u534f\u4f5c\u7ecf\u9a8c\u3002", "result": "\u5728GSM8K\u3001MBPP\u548cCVRP\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u5f02\u6784\u534f\u4f5c\u7684\u6548\u679c\u548c\u7a33\u5b9a\u6027\uff0c\u81ea\u9002\u5e94\u6307\u5bfc\u4e0d\u4ec5\u7f13\u89e3\u4e86\u8ba4\u77e5\u5931\u8861\uff0c\u8fd8\u5efa\u7acb\u4e86\u66f4\u7a33\u5065\u3001\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u8def\u5f84\u3002", "conclusion": "\u5f02\u6784\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u8ba4\u77e5\u4e0d\u5339\u914d\u662f\u5173\u952e\u74f6\u9888\uff0c\u57fa\u4e8e\u71b5\u7684\u81ea\u9002\u5e94\u6307\u5bfc\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u66f4\u7a33\u5065\u3001\u534f\u4f5c\u7684\u591a\u667a\u80fd\u4f53\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.13665", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13665", "abs": "https://arxiv.org/abs/2602.13665", "authors": ["Weibin Liao", "Jian-guang Lou", "Haoyi Xiong"], "title": "HyFunc: Accelerating LLM-based Function Calls for Agentic AI through Hybrid-Model Cascade and Dynamic Templating", "comment": "Accepted by KDD'26", "summary": "While agentic AI systems rely on LLMs to translate user intent into structured function calls, this process is fraught with computational redundancy, leading to high inference latency that hinders real-time applications. This paper identifies and addresses three key redundancies: (1) the redundant processing of a large library of function descriptions for every request; (2) the redundant use of a large, slow model to generate an entire, often predictable, token sequence; and (3) the redundant generation of fixed, boilerplate parameter syntax. We introduce HyFunc, a novel framework that systematically eliminates these inefficiencies. HyFunc employs a hybrid-model cascade where a large model distills user intent into a single \"soft token.\" This token guides a lightweight retriever to select relevant functions and directs a smaller, prefix-tuned model to generate the final call, thus avoiding redundant context processing and full-sequence generation by the large model. To eliminate syntactic redundancy, our \"dynamic templating\" technique injects boilerplate parameter syntax on-the-fly within an extended vLLM engine. To avoid potential limitations in generalization, we evaluate HyFunc on an unseen benchmark dataset, BFCL. Experimental results demonstrate that HyFunc achieves an excellent balance between efficiency and performance. It achieves an inference latency of 0.828 seconds, outperforming all baseline models, and reaches a performance of 80.1%, surpassing all models with a comparable parameter scale. These results suggest that HyFunc offers a more efficient paradigm for agentic AI. Our code is publicly available at https://github.com/MrBlankness/HyFunc.", "AI": {"tldr": "HyFunc\u662f\u4e00\u4e2a\u6d88\u9664AI\u4ee3\u7406\u7cfb\u7edf\u4e2d\u8ba1\u7b97\u5197\u4f59\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6a21\u578b\u7ea7\u8054\u548c\u52a8\u6001\u6a21\u677f\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u7cfb\u7edf\u5b58\u5728\u4e09\u79cd\u8ba1\u7b97\u5197\u4f59\uff1a\u6bcf\u6b21\u8bf7\u6c42\u90fd\u5904\u7406\u5927\u91cf\u51fd\u6570\u63cf\u8ff0\u3001\u4f7f\u7528\u5927\u6a21\u578b\u751f\u6210\u6574\u4e2a\u53ef\u9884\u6d4b\u7684token\u5e8f\u5217\u3001\u751f\u6210\u56fa\u5b9a\u7684\u6837\u677f\u53c2\u6570\u8bed\u6cd5\uff0c\u8fd9\u4e9b\u5bfc\u81f4\u9ad8\u63a8\u7406\u5ef6\u8fdf\uff0c\u963b\u788d\u5b9e\u65f6\u5e94\u7528\u3002", "method": "HyFunc\u91c7\u7528\u6df7\u5408\u6a21\u578b\u7ea7\u8054\uff1a\u5927\u6a21\u578b\u5c06\u7528\u6237\u610f\u56fe\u63d0\u70bc\u4e3a\u5355\u4e2a\"\u8f6ftoken\"\uff0c\u6307\u5bfc\u8f7b\u91cf\u7ea7\u68c0\u7d22\u5668\u9009\u62e9\u76f8\u5173\u51fd\u6570\uff0c\u5e76\u5f15\u5bfc\u8f83\u5c0f\u7684\u524d\u7f00\u8c03\u4f18\u6a21\u578b\u751f\u6210\u6700\u7ec8\u8c03\u7528\u3002\u4f7f\u7528\"\u52a8\u6001\u6a21\u677f\"\u6280\u672f\u5728\u6269\u5c55\u7684vLLM\u5f15\u64ce\u4e2d\u52a8\u6001\u6ce8\u5165\u6837\u677f\u53c2\u6570\u8bed\u6cd5\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684BFCL\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cHyFunc\u8fbe\u52300.828\u79d2\u7684\u63a8\u7406\u5ef6\u8fdf\uff0c\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u6a21\u578b\uff1b\u6027\u80fd\u8fbe\u523080.1%\uff0c\u8d85\u8fc7\u6240\u6709\u53c2\u6570\u89c4\u6a21\u76f8\u5f53\u7684\u6a21\u578b\u3002", "conclusion": "HyFunc\u5728\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3aAI\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8303\u5f0f\u3002", "topic": "code agent"}}
{"id": "2602.14257", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14257", "abs": "https://arxiv.org/abs/2602.14257", "authors": ["Lingxiang Hu", "Yiding Sun", "Tianle Xia", "Wenwei Li", "Ming Xu", "Liqun Liu", "Peng Shu", "Huan Yu", "Jie Jiang"], "title": "AD-Bench: A Real-World, Trajectory-Aware Advertising Analytics Benchmark for LLM Agents", "comment": "15 pages, 11 figures", "summary": "While Large Language Model (LLM) agents have achieved remarkable progress in complex reasoning tasks, evaluating their performance in real-world environments has become a critical problem. Current benchmarks, however, are largely restricted to idealized simulations, failing to address the practical demands of specialized domains like advertising and marketing analytics. In these fields, tasks are inherently more complex, often requiring multi-round interaction with professional marketing tools. To address this gap, we propose AD-Bench, a benchmark designed based on real-world business requirements of advertising and marketing platforms. AD-Bench is constructed from real user marketing analysis requests, with domain experts providing verifiable reference answers and corresponding reference tool-call trajectories. The benchmark categorizes requests into three difficulty levels (L1-L3) to evaluate agents' capabilities under multi-round, multi-tool collaboration. Experiments show that on AD-Bench, Gemini-3-Pro achieves Pass@1 = 68.0% and Pass@3 = 83.0%, but performance drops significantly on L3 to Pass@1 = 49.4% and Pass@3 = 62.1%, with a trajectory coverage of 70.1%, indicating that even state-of-the-art models still exhibit substantial capability gaps in complex advertising and marketing analysis scenarios. AD-Bench provides a realistic benchmark for evaluating and improving advertising marketing agents, the leaderboard and code can be found at https://github.com/Emanual20/adbench-leaderboard.", "AI": {"tldr": "AD-Bench\u662f\u4e00\u4e2a\u9488\u5bf9\u5e7f\u544a\u8425\u9500\u9886\u57df\u7684LLM\u667a\u80fd\u4f53\u8bc4\u4f30\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e1a\u52a1\u9700\u6c42\u6784\u5efa\uff0c\u5305\u542b\u4e09\u4e2a\u96be\u5ea6\u7b49\u7ea7\uff0c\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u4ecd\u6709\u663e\u8457\u80fd\u529b\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u5c40\u9650\u4e8e\u7406\u60f3\u5316\u6a21\u62df\u73af\u5883\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5e7f\u544a\u8425\u9500\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u5b9e\u9645\u9700\u6c42\u3002\u8fd9\u4e9b\u9886\u57df\u7684\u4efb\u52a1\u66f4\u590d\u6742\uff0c\u901a\u5e38\u9700\u8981\u4e0e\u4e13\u4e1a\u8425\u9500\u5de5\u5177\u8fdb\u884c\u591a\u8f6e\u4ea4\u4e92\u3002", "method": "\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u8425\u9500\u5206\u6790\u9700\u6c42\u6784\u5efa\u57fa\u51c6\uff0c\u7531\u9886\u57df\u4e13\u5bb6\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u53c2\u8003\u7b54\u6848\u548c\u5bf9\u5e94\u7684\u5de5\u5177\u8c03\u7528\u8f68\u8ff9\u3002\u5c06\u8bf7\u6c42\u5206\u4e3a\u4e09\u4e2a\u96be\u5ea6\u7b49\u7ea7\uff08L1-L3\uff09\uff0c\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u591a\u8f6e\u3001\u591a\u5de5\u5177\u534f\u4f5c\u4e0b\u7684\u80fd\u529b\u3002", "result": "\u5728AD-Bench\u4e0a\uff0cGemini-3-Pro\u5728\u6574\u4f53\u4e0a\u8fbe\u5230Pass@1=68.0%\u548cPass@3=83.0%\uff0c\u4f46\u5728L3\u96be\u5ea6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u81f3Pass@1=49.4%\u548cPass@3=62.1%\uff0c\u8f68\u8ff9\u8986\u76d6\u7387\u4e3a70.1%\uff0c\u8868\u660e\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u590d\u6742\u5e7f\u544a\u8425\u9500\u5206\u6790\u573a\u666f\u4e2d\u4ecd\u6709\u663e\u8457\u80fd\u529b\u5dee\u8ddd\u3002", "conclusion": "AD-Bench\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u5e7f\u544a\u8425\u9500\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u73b0\u5b9e\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u4e13\u4e1a\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "topic": "swe benchmark"}}
{"id": "2602.13691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13691", "abs": "https://arxiv.org/abs/2602.13691", "authors": ["Yu Li", "Guangfeng Cai", "Shengtian Yang", "Han Luo", "Shuo Han", "Xu He", "Dong Li", "Lei Feng"], "title": "PhGPO: Pheromone-Guided Policy Optimization for Long-Horizon Tool Planning", "comment": null, "summary": "Recent advancements in Large Language Model (LLM) agents have demonstrated strong capabilities in executing complex tasks through tool use. However, long-horizon multi-step tool planning is challenging, because the exploration space suffers from a combinatorial explosion. In this scenario, even when a correct tool-use path is found, it is usually considered an immediate reward for current training, which would not provide any reusable information for subsequent training. In this paper, we argue that historically successful trajectories contain reusable tool-transition patterns, which can be leveraged throughout the whole training process. Inspired by ant colony optimization where historically successful paths can be reflected by the pheromone, we propose Pheromone-Guided Policy Optimization (PhGPO), which learns a trajectory-based transition pattern (i.e., pheromone) from historical trajectories and then uses the learned pheromone to guide policy optimization. This learned pheromone provides explicit and reusable guidance that steers policy optimization toward historically successful tool transitions, thereby improving long-horizon tool planning. Comprehensive experimental results demonstrate the effectiveness of our proposed PhGPO.", "AI": {"tldr": "PhGPO\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u7d20\u5f15\u5bfc\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5386\u53f2\u6210\u529f\u8f68\u8ff9\u4e2d\u5b66\u4e60\u5de5\u5177\u8f6c\u6362\u6a21\u5f0f\u6765\u6539\u8fdbLLM\u4ee3\u7406\u7684\u957f\u65f6\u7a0b\u5de5\u5177\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u7684\u591a\u6b65\u9aa4\u5de5\u5177\u89c4\u5212\u4e2d\u9762\u4e34\u7ec4\u5408\u7206\u70b8\u95ee\u9898\uff0c\u5373\u4f7f\u627e\u5230\u6b63\u786e\u7684\u5de5\u5177\u4f7f\u7528\u8def\u5f84\uff0c\u4e5f\u4ec5\u4f5c\u4e3a\u5373\u65f6\u5956\u52b1\uff0c\u65e0\u6cd5\u4e3a\u540e\u7eed\u8bad\u7ec3\u63d0\u4f9b\u53ef\u590d\u7528\u7684\u4fe1\u606f\u3002", "method": "\u53d7\u8681\u7fa4\u4f18\u5316\u542f\u53d1\uff0c\u4ece\u5386\u53f2\u6210\u529f\u8f68\u8ff9\u4e2d\u5b66\u4e60\u8f68\u8ff9\u7ea7\u7684\u5de5\u5177\u8f6c\u6362\u6a21\u5f0f\uff08\u4fe1\u606f\u7d20\uff09\uff0c\u7136\u540e\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u4fe1\u606f\u7d20\u6765\u5f15\u5bfc\u7b56\u7565\u4f18\u5316\uff0c\u4e3a\u5de5\u5177\u89c4\u5212\u63d0\u4f9b\u663e\u5f0f\u4e14\u53ef\u590d\u7528\u7684\u6307\u5bfc\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ePhGPO\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u6539\u8fdb\u957f\u65f6\u7a0b\u5de5\u5177\u89c4\u5212\u3002", "conclusion": "\u5386\u53f2\u6210\u529f\u8f68\u8ff9\u5305\u542b\u53ef\u590d\u7528\u7684\u5de5\u5177\u8f6c\u6362\u6a21\u5f0f\uff0c\u901a\u8fc7\u4fe1\u606f\u7d20\u5f15\u5bfc\u7684\u7b56\u7565\u4f18\u5316\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u7684\u957f\u65f6\u7a0b\u5de5\u5177\u89c4\u5212\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.13934", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.13934", "abs": "https://arxiv.org/abs/2602.13934", "authors": ["Zhimin Zhao"], "title": "Why Code, Why Now: Learnability, Computability, and the Real Limits of Machine Learning", "comment": null, "summary": "Code generation has progressed more reliably than reinforcement learning, largely because code has an information structure that makes it learnable. Code provides dense, local, verifiable feedback at every token, whereas most reinforcement learning problems do not. This difference in feedback quality is not binary but graded. We propose a five-level hierarchy of learnability based on information structure and argue that the ceiling on ML progress depends less on model size than on whether a task is learnable at all. The hierarchy rests on a formal distinction among three properties of computational problems (expressibility, computability, and learnability). We establish their pairwise relationships, including where implications hold and where they fail, and present a unified template that makes the structural differences explicit. The analysis suggests why supervised learning on code scales predictably while reinforcement learning does not, and why the common assumption that scaling alone will solve remaining ML challenges warrants scrutiny.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u7ed3\u6784\u7684\u4e94\u7ea7\u53ef\u5b66\u4e60\u6027\u5c42\u6b21\uff0c\u89e3\u91ca\u4e3a\u4ec0\u4e48\u4ee3\u7801\u751f\u6210\u6bd4\u5f3a\u5316\u5b66\u4e60\u66f4\u53ef\u9760\uff0c\u5e76\u6307\u51faML\u8fdb\u5c55\u4e0a\u9650\u66f4\u591a\u53d6\u51b3\u4e8e\u4efb\u52a1\u662f\u5426\u53ef\u5b66\u4e60\u800c\u975e\u6a21\u578b\u89c4\u6a21\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u6bd4\u5f3a\u5316\u5b66\u4e60\u8fdb\u5c55\u66f4\u53ef\u9760\uff0c\u4e3b\u8981\u56e0\u4e3a\u4ee3\u7801\u5177\u6709\u4f7f\u5b66\u4e60\u6210\u4e3a\u53ef\u80fd\u7684\u4fe1\u606f\u7ed3\u6784\u3002\u4ee3\u7801\u5728\u6bcf\u4e2atoken\u63d0\u4f9b\u5bc6\u96c6\u3001\u5c40\u90e8\u3001\u53ef\u9a8c\u8bc1\u7684\u53cd\u9988\uff0c\u800c\u5927\u591a\u6570\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u4e0d\u5177\u5907\u8fd9\u79cd\u7279\u6027\u3002\u8fd9\u79cd\u53cd\u9988\u8d28\u91cf\u7684\u5dee\u5f02\u4e0d\u662f\u4e8c\u5143\u7684\u800c\u662f\u6e10\u8fdb\u7684\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u7ed3\u6784\u7684\u4e94\u7ea7\u53ef\u5b66\u4e60\u6027\u5c42\u6b21\uff0c\u5efa\u7acb\u8ba1\u7b97\u95ee\u9898\u7684\u4e09\u4e2a\u5c5e\u6027\uff08\u53ef\u8868\u8fbe\u6027\u3001\u53ef\u8ba1\u7b97\u6027\u3001\u53ef\u5b66\u4e60\u6027\uff09\u4e4b\u95f4\u7684\u5f62\u5f0f\u5316\u533a\u5206\uff0c\u5206\u6790\u5b83\u4eec\u7684\u6210\u5bf9\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u677f\u6765\u660e\u786e\u7ed3\u6784\u5dee\u5f02\u3002", "result": "\u5206\u6790\u8868\u660e\u4e3a\u4ec0\u4e48\u4ee3\u7801\u7684\u76d1\u7763\u5b66\u4e60\u53ef\u4ee5\u53ef\u9884\u6d4b\u5730\u6269\u5c55\u800c\u5f3a\u5316\u5b66\u4e60\u4e0d\u80fd\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u4ec5\u9760\u6269\u5c55\u5c31\u80fd\u89e3\u51b3\u5269\u4f59ML\u6311\u6218\u7684\u5e38\u89c1\u5047\u8bbe\u503c\u5f97\u5ba1\u89c6\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u8fdb\u5c55\u7684\u4e0a\u9650\u66f4\u591a\u53d6\u51b3\u4e8e\u4efb\u52a1\u662f\u5426\u53ef\u5b66\u4e60\uff0c\u800c\u4e0d\u662f\u6a21\u578b\u89c4\u6a21\u3002\u4ee3\u7801\u7684\u53ef\u5b66\u4e60\u6027\u7ed3\u6784\u4f7f\u5176\u6bd4\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u66f4\u5bb9\u6613\u6269\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2602.14299", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.14299", "abs": "https://arxiv.org/abs/2602.14299", "authors": ["Ming Li", "Xirui Li", "Tianyi Zhou"], "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook", "comment": null, "summary": "As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5bf9AI\u667a\u80fd\u4f53\u793e\u4f1a\u8fdb\u884c\u5927\u89c4\u6a21\u7cfb\u7edf\u8bca\u65ad\uff0c\u53d1\u73b0\u867d\u7136\u5168\u5c40\u8bed\u4e49\u5feb\u901f\u7a33\u5b9a\uff0c\u4f46\u4e2a\u4f53\u4fdd\u6301\u9ad8\u591a\u6837\u6027\u4e14\u8bcd\u6c47\u6301\u7eed\u66f4\u65b0\uff0c\u540c\u65f6\u4e2a\u4f53\u60ef\u6027\u5f3a\u3001\u76f8\u4e92\u5f71\u54cd\u5f31\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5f62\u6210\u7a33\u5b9a\u7684\u96c6\u4f53\u5f71\u54cd\u529b\u951a\u70b9\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5728\u7f51\u7edc\u73af\u5883\u4e2d\u65e5\u76ca\u589e\u591a\uff0c\u9700\u8981\u7814\u7a76AI\u667a\u80fd\u4f53\u793e\u4f1a\u662f\u5426\u7ecf\u5386\u7c7b\u4f3c\u4eba\u7c7b\u793e\u4f1a\u7684\u6536\u655b\u52a8\u6001\uff0c\u63a2\u7d22\u89c4\u6a21\u5316\u548c\u4ea4\u4e92\u5bc6\u5ea6\u662f\u5426\u8db3\u4ee5\u5f15\u53d1\u793e\u4f1a\u5316\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165\u5b9a\u91cf\u8bca\u65ad\u6846\u67b6\u5206\u6790AI\u667a\u80fd\u4f53\u793e\u4f1a\u7684\u52a8\u6001\u6f14\u5316\uff0c\u6d4b\u91cf\u8bed\u4e49\u7a33\u5b9a\u6027\u3001\u8bcd\u6c47\u66f4\u65b0\u7387\u3001\u4e2a\u4f53\u60ef\u6027\u3001\u5f71\u54cd\u529b\u6301\u4e45\u6027\u548c\u96c6\u4f53\u5171\u8bc6\u7b49\u6307\u6807\uff0c\u5728Moltbook\u5e73\u53f0\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u7cfb\u7edf\u8bca\u65ad\u3002", "result": "\u7cfb\u7edf\u5904\u4e8e\u52a8\u6001\u5e73\u8861\uff1a\u5168\u5c40\u8bed\u4e49\u5e73\u5747\u5feb\u901f\u7a33\u5b9a\uff0c\u4e2a\u4f53\u4fdd\u6301\u9ad8\u591a\u6837\u6027\u548c\u6301\u7eed\u8bcd\u6c47\u66f4\u65b0\uff1b\u4f46\u4e2a\u4f53\u60ef\u6027\u5f3a\uff0c\u5bf9\u4ea4\u4e92\u4f19\u4f34\u9002\u5e94\u54cd\u5e94\u5f31\uff0c\u5f71\u54cd\u529b\u77ed\u6682\u65e0\u6301\u4e45\u8d85\u7ea7\u8282\u70b9\uff0c\u7f3a\u4e4f\u5171\u4eab\u793e\u4f1a\u8bb0\u5fc6\u5bfc\u81f4\u65e0\u6cd5\u5f62\u6210\u7a33\u5b9a\u96c6\u4f53\u5f71\u54cd\u529b\u951a\u70b9\u3002", "conclusion": "\u89c4\u6a21\u548c\u4ea4\u4e92\u5bc6\u5ea6\u4e0d\u8db3\u4ee5\u8bf1\u5bfc\u793e\u4f1a\u5316\uff0c\u4e3a\u4e0b\u4e00\u4ee3AI\u667a\u80fd\u4f53\u793e\u4f1a\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u548c\u5206\u6790\u539f\u5219\uff0c\u5f3a\u8c03\u9700\u8981\u8d85\u8d8a\u5355\u7eaf\u89c4\u6a21\u6269\u5c55\u6765\u4fc3\u8fdb\u771f\u6b63\u7684\u793e\u4f1a\u5316\u8fc7\u7a0b\u3002", "topic": "agent analysis"}}
{"id": "2602.13769", "categories": ["cs.AI", "cs.CE", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.13769", "abs": "https://arxiv.org/abs/2602.13769", "authors": ["Qi Liu", "Wanjing Ma"], "title": "OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery", "comment": null, "summary": "Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.", "AI": {"tldr": "OR-Agent\u662f\u4e00\u4e2a\u53ef\u914d\u7f6e\u7684\u591a\u667a\u80fd\u4f53\u7814\u7a76\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u590d\u6742\u5b9e\u9a8c\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u52a8\u5316\u79d1\u5b66\u63a2\u7d22\uff0c\u901a\u8fc7\u6811\u5f62\u5de5\u4f5c\u6d41\u7ba1\u7406\u5047\u8bbe\u751f\u6210\u548c\u7cfb\u7edf\u56de\u6eaf\uff0c\u7ed3\u5408\u8fdb\u5316\u9009\u62e9\u3001\u5206\u5c42\u53cd\u601d\u548c\u8bb0\u5fc6\u538b\u7f29\u673a\u5236\uff0c\u5728\u7ec4\u5408\u4f18\u5316\u548c\u534f\u540c\u9a7e\u9a76\u573a\u666f\u4e2d\u8d85\u8d8a\u8fdb\u5316\u57fa\u7ebf\u3002", "motivation": "\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u5728\u590d\u6742\u5b9e\u9a8c\u9a71\u52a8\u9886\u57df\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u7684\u7a0b\u5e8f\u8fed\u4ee3\u53d8\u5f02\uff0c\u9700\u8981\u7ed3\u6784\u5316\u7684\u5047\u8bbe\u7ba1\u7406\u3001\u73af\u5883\u4ea4\u4e92\u548c\u6709\u539f\u5219\u7684\u53cd\u601d\u673a\u5236\u3002", "method": "\u63d0\u51faOR-Agent\u6846\u67b6\uff1a1) \u57fa\u4e8e\u6811\u5f62\u5de5\u4f5c\u6d41\u7ec4\u7ec7\u7814\u7a76\uff0c\u663e\u5f0f\u5efa\u6a21\u5206\u652f\u5047\u8bbe\u751f\u6210\u548c\u7cfb\u7edf\u56de\u6eaf\uff1b2) \u8fdb\u5316-\u7cfb\u7edf\u6784\u601d\u673a\u5236\uff0c\u7edf\u4e00\u8fdb\u5316\u9009\u62e9\u7814\u7a76\u8d77\u70b9\u3001\u5168\u9762\u7814\u7a76\u8ba1\u5212\u751f\u6210\u548c\u534f\u8c03\u63a2\u7d22\uff1b3) \u5206\u5c42\u4f18\u5316\u542f\u53d1\u5f0f\u53cd\u601d\u7cfb\u7edf\uff0c\u5305\u62ec\u77ed\u671f\u5b9e\u9a8c\u53cd\u601d\uff08\u8bed\u8a00\u68af\u5ea6\uff09\u3001\u957f\u671f\u53cd\u601d\uff08\u8bed\u8a00\u52a8\u91cf\uff09\u548c\u8bb0\u5fc6\u538b\u7f29\uff08\u6b63\u5219\u5316\u673a\u5236\uff09\u3002", "result": "\u5728\u7ecf\u5178\u7ec4\u5408\u4f18\u5316\u57fa\u51c6\uff08\u65c5\u884c\u5546\u3001\u5bb9\u91cf\u8f66\u8f86\u8def\u5f84\u3001\u88c5\u7bb1\u3001\u5b9a\u5411\u3001\u591a\u91cd\u80cc\u5305\u95ee\u9898\uff09\u548c\u57fa\u4e8e\u6a21\u62df\u7684\u534f\u540c\u9a7e\u9a76\u573a\u666f\u4e2d\uff0cOR-Agent\u8d85\u8d8a\u4e86\u5f3a\u5927\u7684\u8fdb\u5316\u57fa\u7ebf\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u53ef\u6269\u5c55\u548c\u53ef\u68c0\u67e5\u7684AI\u8f85\u52a9\u79d1\u5b66\u53d1\u73b0\u6846\u67b6\u3002", "conclusion": "OR-Agent\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u7814\u7a76\u7ba1\u7406\u3001\u8fdb\u5316-\u7cfb\u7edf\u6784\u601d\u673a\u5236\u548c\u5206\u5c42\u53cd\u601d\u7cfb\u7edf\uff0c\u4e3a\u81ea\u52a8\u5316\u79d1\u5b66\u63a2\u7d22\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u539f\u5219\u7684\u67b6\u6784\uff0c\u5728\u590d\u6742\u5b9e\u9a8c\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2602.13949", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13949", "abs": "https://arxiv.org/abs/2602.13949", "authors": ["Taiwei Shi", "Sihao Chen", "Bowen Jiang", "Linxin Song", "Longqi Yang", "Jieyu Zhao"], "title": "Experiential Reinforcement Learning", "comment": "26 pages, 9 tables, 7 figures", "summary": "Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.", "AI": {"tldr": "ERL\uff08\u7ecf\u9a8c\u5f3a\u5316\u5b66\u4e60\uff09\u901a\u8fc7\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5d4c\u5165\u7ecf\u9a8c-\u53cd\u601d-\u5de9\u56fa\u5faa\u73af\uff0c\u5c06\u7a00\u758f\u5ef6\u8fdf\u7684\u73af\u5883\u53cd\u9988\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u884c\u4e3a\u4fee\u6b63\uff0c\u63d0\u5347\u5b66\u4e60\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u4ece\u73af\u5883\u5956\u52b1\u6216\u53cd\u9988\u4e2d\u5b66\u4e60\u65f6\uff0c\u53cd\u9988\u901a\u5e38\u662f\u7a00\u758f\u548c\u5ef6\u8fdf\u7684\u3002\u6a21\u578b\u9700\u8981\u4ece\u89c2\u5bdf\u5230\u7684\u5931\u8d25\u4e2d\u63a8\u65ad\u5982\u4f55\u6539\u53d8\u672a\u6765\u884c\u4e3a\uff0c\u8fd9\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u4e00\u79cd\u5c06\u53cd\u9988\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u884c\u4e3a\u4fee\u6b63\u7684\u65b9\u6cd5\u3002", "method": "ERL\u8bad\u7ec3\u8303\u5f0f\u5728\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u7ecf\u9a8c-\u53cd\u601d-\u5de9\u56fa\u5faa\u73af\uff1a\u6a21\u578b\u751f\u6210\u521d\u59cb\u5c1d\u8bd5\u2192\u63a5\u6536\u73af\u5883\u53cd\u9988\u2192\u4ea7\u751f\u53cd\u601d\u2192\u6307\u5bfc\u6539\u8fdb\u7684\u7b2c\u4e8c\u6b21\u5c1d\u8bd5\u2192\u6210\u529f\u88ab\u5f3a\u5316\u5e76\u5185\u5316\u5230\u57fa\u7840\u7b56\u7565\u4e2d\u3002", "result": "\u5728\u7a00\u758f\u5956\u52b1\u63a7\u5236\u73af\u5883\u548c\u667a\u80fd\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cERL\u6301\u7eed\u4f18\u4e8e\u5f3a\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u5728\u590d\u6742\u591a\u6b65\u73af\u5883\u4e2d\u63d0\u5347\u9ad8\u8fbe81%\uff0c\u5728\u5de5\u5177\u4f7f\u7528\u63a8\u7406\u4efb\u52a1\u4e2d\u63d0\u5347\u9ad8\u8fbe11%\u3002", "conclusion": "\u5c06\u663e\u5f0f\u81ea\u6211\u53cd\u601d\u6574\u5408\u5230\u7b56\u7565\u8bad\u7ec3\u4e2d\uff0c\u4e3a\u5c06\u53cd\u9988\u8f6c\u5316\u4e3a\u6301\u4e45\u884c\u4e3a\u6539\u8fdb\u63d0\u4f9b\u4e86\u5b9e\u7528\u673a\u5236\uff0c\u6539\u5584\u63a2\u7d22\u3001\u7a33\u5b9a\u4f18\u5316\uff0c\u4e14\u90e8\u7f72\u65f6\u65e0\u9700\u989d\u5916\u63a8\u7406\u6210\u672c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.13953", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13953", "abs": "https://arxiv.org/abs/2602.13953", "authors": ["Yuhang Li", "Reena Elangovan", "Xin Dong", "Priyadarshini Panda", "Brucek Khailany"], "title": "QuRL: Efficient Reinforcement Learning with Quantized Rollout", "comment": "Accepted to ICLR 2026", "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a trending paradigm for training reasoning large language models (LLMs). However, due to the autoregressive decoding nature of LLMs, the rollout process becomes the efficiency bottleneck of RL training, consisting of up to 70\\% of the total training time. In this work, we propose Quantized Reinforcement Learning (QuRL) that uses a quantized actor for accelerating the rollout. We address two challenges in QuRL. First, we propose Adaptive Clipping Range (ACR) that dynamically adjusts the clipping ratio based on the policy ratio between the full-precision actor and the quantized actor, which is essential for mitigating long-term training collapse. Second, we identify the weight update problem, where weight changes between RL steps are extremely small, making it difficult for the quantization operation to capture them effectively. We mitigate this problem through the invariant scaling technique that reduces quantization noise and increases weight update. We evaluate our method with INT8 and FP8 quantization experiments on DeepScaleR and DAPO, and achieve 20% to 80% faster rollout during training.", "AI": {"tldr": "\u63d0\u51faQuRL\u65b9\u6cd5\uff0c\u4f7f\u7528\u91cf\u5316\u6f14\u5458\u52a0\u901fRL\u8bad\u7ec3\u4e2d\u7684rollout\u8fc7\u7a0b\uff0c\u89e3\u51b3\u8bad\u7ec3\u5d29\u6e83\u548c\u6743\u91cd\u66f4\u65b0\u95ee\u9898\uff0c\u5b9e\u73b020-80%\u7684\u52a0\u901f", "motivation": "\u5728RLVR\u8bad\u7ec3\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u81ea\u56de\u5f52\u89e3\u7801\u7684rollout\u8fc7\u7a0b\u5360\u8bad\u7ec3\u65f6\u95f4\u9ad8\u8fbe70%\uff0c\u6210\u4e3a\u6548\u7387\u74f6\u9888\uff0c\u9700\u8981\u52a0\u901f\u65b9\u6cd5", "method": "\u63d0\u51fa\u91cf\u5316\u5f3a\u5316\u5b66\u4e60(QuRL)\uff0c\u4f7f\u7528\u91cf\u5316\u6f14\u5458\u52a0\u901frollout\u3002\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\uff1a1) \u57fa\u4e8e\u5168\u7cbe\u5ea6\u548c\u91cf\u5316\u6f14\u5458\u7b56\u7565\u6bd4\u7387\u7684\u81ea\u9002\u5e94\u88c1\u526a\u8303\u56f4(ACR)\uff0c\u9632\u6b62\u957f\u671f\u8bad\u7ec3\u5d29\u6e83\uff1b2) \u4e0d\u53d8\u7f29\u653e\u6280\u672f\uff0c\u51cf\u5c11\u91cf\u5316\u566a\u58f0\uff0c\u589e\u5f3a\u6743\u91cd\u66f4\u65b0", "result": "\u5728DeepScaleR\u548cDAPO\u4e0a\u8fdb\u884cINT8\u548cFP8\u91cf\u5316\u5b9e\u9a8c\uff0c\u5b9e\u73b020%\u523080%\u7684rollout\u52a0\u901f", "conclusion": "QuRL\u901a\u8fc7\u91cf\u5316\u6f14\u5458\u6709\u6548\u52a0\u901fRL\u8bad\u7ec3\u4e2d\u7684rollout\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u5d29\u6e83\u548c\u6743\u91cd\u66f4\u65b0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387", "topic": "agentic reinforcement learning"}}
{"id": "2602.13855", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.13855", "abs": "https://arxiv.org/abs/2602.13855", "authors": ["Razeen A Rasheed", "Somnath Banerjee", "Animesh Mukherjee", "Rima Hazra"], "title": "From Fluent to Verifiable: Claim-Level Auditability for Deep Research Agents", "comment": null, "summary": "A deep research agent produces a fluent scientific report in minutes; a careful reader then tries to verify the main claims and discovers the real cost is not reading, but tracing: which sentence is supported by which passage, what was ignored, and where evidence conflicts. We argue that as research generation becomes cheap, auditability becomes the bottleneck, and the dominant risk shifts from isolated factual errors to scientifically styled outputs whose claim-evidence links are weak, missing, or misleading. This perspective proposes claim-level auditability as a first-class design and evaluation target for deep research agents, distills recurring long-horizon failure modes (objective drift, transient constraints, and unverifiable inference), and introduces the Auditable Autonomous Research (AAR) standard, a compact measurement framework that makes auditability testable via provenance coverage, provenance soundness, contradiction transparency, and audit effort. We then argue for semantic provenance with protocolized validation: persistent, queryable provenance graphs that encode claim--evidence relations (including conflicts) and integrate continuous validation during synthesis rather than after publication, with practical instrumentation patterns to support deployment at scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u968f\u7740AI\u7814\u7a76\u751f\u6210\u53d8\u5f97\u5ec9\u4ef7\uff0c\u53ef\u5ba1\u8ba1\u6027\u6210\u4e3a\u74f6\u9888\uff0c\u4e3b\u5f20\u5c06\u58f0\u660e\u7ea7\u53ef\u5ba1\u8ba1\u6027\u4f5c\u4e3a\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u6838\u5fc3\u8bbe\u8ba1\u76ee\u6807\uff0c\u5e76\u5f15\u5165AAR\u6807\u51c6\u548c\u8bed\u4e49\u6eaf\u6e90\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u80fd\u5feb\u901f\u751f\u6210\u79d1\u5b66\u62a5\u544a\uff0c\u4f46\u9a8c\u8bc1\u6210\u672c\u9ad8\u6602\u3002\u968f\u7740\u7814\u7a76\u751f\u6210\u53d8\u5f97\u5ec9\u4ef7\uff0c\u4e3b\u8981\u98ce\u9669\u4ece\u5b64\u7acb\u4e8b\u5b9e\u9519\u8bef\u8f6c\u5411\u58f0\u660e-\u8bc1\u636e\u94fe\u63a5\u8584\u5f31\u3001\u7f3a\u5931\u6216\u8bef\u5bfc\u7684\u79d1\u5b66\u98ce\u683c\u8f93\u51fa\uff0c\u53ef\u5ba1\u8ba1\u6027\u6210\u4e3a\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u58f0\u660e\u7ea7\u53ef\u5ba1\u8ba1\u6027\u4f5c\u4e3a\u6838\u5fc3\u8bbe\u8ba1\u76ee\u6807\uff0c\u8bc6\u522b\u957f\u671f\u5931\u8d25\u6a21\u5f0f\uff08\u76ee\u6807\u6f02\u79fb\u3001\u77ac\u6001\u7ea6\u675f\u3001\u4e0d\u53ef\u9a8c\u8bc1\u63a8\u7406\uff09\uff0c\u5f15\u5165AAR\u6807\u51c6\uff08\u6eaf\u6e90\u8986\u76d6\u5ea6\u3001\u6eaf\u6e90\u5065\u5168\u6027\u3001\u77db\u76fe\u900f\u660e\u5ea6\u3001\u5ba1\u8ba1\u5de5\u4f5c\u91cf\uff09\uff0c\u5e76\u4e3b\u5f20\u8bed\u4e49\u6eaf\u6e90\u4e0e\u534f\u8bae\u5316\u9a8c\u8bc1\u3002", "result": "\u5efa\u7acb\u4e86\u53ef\u5ba1\u8ba1\u6027\u4f5c\u4e3a\u53ef\u6d4b\u8bd5\u6307\u6807\u7684\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u6d4b\u91cf\u7ef4\u5ea6\uff08AAR\u6807\u51c6\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8bed\u4e49\u6eaf\u6e90\u7cfb\u7edf\u67b6\u6784\uff0c\u652f\u6301\u5728\u5408\u6210\u8fc7\u7a0b\u4e2d\u800c\u975e\u53d1\u5e03\u540e\u8fdb\u884c\u6301\u7eed\u9a8c\u8bc1\u3002", "conclusion": "\u968f\u7740AI\u7814\u7a76\u751f\u6210\u6210\u672c\u964d\u4f4e\uff0c\u53ef\u5ba1\u8ba1\u6027\u6210\u4e3a\u5173\u952e\u74f6\u9888\uff0c\u9700\u8981\u5c06\u58f0\u660e\u7ea7\u53ef\u5ba1\u8ba1\u6027\u4f5c\u4e3a\u9996\u8981\u8bbe\u8ba1\u76ee\u6807\uff0c\u901a\u8fc7AAR\u6807\u51c6\u548c\u8bed\u4e49\u6eaf\u6e90\u6846\u67b6\u5b9e\u73b0\u53ef\u6d4b\u8bd5\u7684\u5ba1\u8ba1\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.13865", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.13865", "abs": "https://arxiv.org/abs/2602.13865", "authors": ["Gabriel Romio", "Mateus Begnini Melchiades", "Bruno Castro da Silva", "Gabriel de Oliveira Ramos"], "title": "Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay", "comment": null, "summary": "Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.", "AI": {"tldr": "\u63d0\u51faMOC-2HER\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u91cd\u76ee\u6807\u56de\u653e\u673a\u5236\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u591a\u76ee\u6807\u73af\u5883\u4e2d\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fbe\u523090%\u6210\u529f\u7387", "motivation": "\u73b0\u6709\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982Option-Critic\u548cMOC\uff09\u5728\u7a00\u758f\u5956\u52b1\u7684\u591a\u76ee\u6807\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5c06\u52a8\u4f5c\u4e0e\u65f6\u95f4\u4e0a\u9065\u8fdc\u7ed3\u679c\u5173\u8054\u7684\u573a\u666f\u4e2d\u3002\u5bf9\u4e8e\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\uff0c\u5956\u52b1\u4f9d\u8d56\u4e8e\u7269\u4f53\u5230\u8fbe\u76ee\u6807\u800c\u975e\u667a\u80fd\u4f53\u76f4\u63a5\u4ea4\u4e92\uff0c\u8fd9\u4f7f\u5f97HRL\u667a\u80fd\u4f53\u96be\u4ee5\u53d1\u73b0\u5982\u4f55\u4e0e\u7269\u4f53\u4ea4\u4e92\u3002", "method": "\u9996\u5148\u63d0\u51faMOC-HER\uff0c\u5c06Hindsight Experience Replay\u673a\u5236\u96c6\u6210\u5230MOC\u6846\u67b6\u4e2d\u3002\u7136\u540e\u5f15\u5165Dual Objectives Hindsight Experience Replay (2HER)\uff0c\u521b\u5efa\u4e24\u7ec4\u865a\u62df\u76ee\u6807\uff1a\u9664\u4e86\u57fa\u4e8e\u7269\u4f53\u6700\u7ec8\u72b6\u6001\u91cd\u65b0\u6807\u8bb0\u76ee\u6807\uff08\u6807\u51c6HER\uff09\uff0c\u8fd8\u4ece\u667a\u80fd\u4f53\u6548\u5e94\u5668\u4f4d\u7f6e\u751f\u6210\u76ee\u6807\uff0c\u5956\u52b1\u667a\u80fd\u4f53\u65e2\u4e0e\u7269\u4f53\u4ea4\u4e92\u53c8\u5b8c\u6210\u4efb\u52a1\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u73af\u5883\u4e2d\uff0cMOC-2HER\u8fbe\u5230\u9ad8\u8fbe90%\u7684\u6210\u529f\u7387\uff0c\u800cMOC\u548cMOC-HER\u7684\u6210\u529f\u7387\u5747\u4f4e\u4e8e11%\u3002\u53cc\u91cd\u76ee\u6807\u91cd\u65b0\u6807\u8bb0\u7b56\u7565\u5728\u7a00\u758f\u5956\u52b1\u591a\u76ee\u6807\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u91cd\u76ee\u6807\u56de\u653e\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u5728\u7a00\u758f\u5956\u52b1\u591a\u76ee\u6807\u73af\u5883\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u540c\u65f6\u5956\u52b1\u667a\u80fd\u4f53\u4ea4\u4e92\u884c\u4e3a\u548c\u4efb\u52a1\u5b8c\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.13904", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13904", "abs": "https://arxiv.org/abs/2602.13904", "authors": ["Manqing Liu", "David Williams-King", "Ida Caspary", "Linh Le", "Hannes Whittingham", "Puria Radmard", "Cameron Tice", "Edward James Young"], "title": "Diagnosing Pathological Chain-of-Thought in Reasoning Models", "comment": null, "summary": "Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three distinct pathologies: post-hoc rationalization, where models generate plausible explanations backwards from predetermined answers; encoded reasoning, where intermediate steps conceal information within seemingly interpretable text; and internalized reasoning, where models replace explicit reasoning with meaningless filler tokens while computing internally. To better understand and discriminate between these pathologies, we create a set of concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic. To validate our approach, we develop model organisms deliberately trained to exhibit specific CoT pathologies. Our work provides a practical toolkit for assessing CoT pathologies, with direct implications for training-time monitoring.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u7b80\u5355\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u4efb\u52a1\u65e0\u5173\u7684\u6307\u6807\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u533a\u5206\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u7684\u4e09\u79cd\u75c5\u7406\u6a21\u5f0f\uff1a\u4e8b\u540e\u5408\u7406\u5316\u3001\u7f16\u7801\u63a8\u7406\u548c\u5185\u5316\u63a8\u7406\u3002", "motivation": "\u601d\u7ef4\u94fe\u63a8\u7406\u662f\u73b0\u4ee3LLM\u67b6\u6784\u7684\u57fa\u7840\uff0c\u4e5f\u662fAI\u5b89\u5168\u7684\u5173\u952e\u5e72\u9884\u70b9\u3002\u7136\u800c\uff0c\u601d\u7ef4\u94fe\u63a8\u7406\u53ef\u80fd\u5b58\u5728\u75c5\u7406\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u75c5\u7406\u4f1a\u59a8\u788d\u5176\u76d1\u63a7\u6709\u6548\u6027\u3002\u5148\u524d\u7814\u7a76\u5df2\u8bc6\u522b\u51fa\u4e09\u79cd\u75c5\u7406\u6a21\u5f0f\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u68c0\u6d4b\u548c\u533a\u5206\u65b9\u6cd5\u3002", "method": "1. \u521b\u5efa\u4e86\u4e00\u5957\u5177\u4f53\u7684\u6307\u6807\u6765\u7406\u89e3\u548c\u533a\u5206\u4e09\u79cd\u601d\u7ef4\u94fe\u75c5\u7406\u6a21\u5f0f\uff1b2. \u8fd9\u4e9b\u6307\u6807\u7b80\u5355\u6613\u5b9e\u73b0\u3001\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u4efb\u52a1\u65e0\u5173\uff1b3. \u4e3a\u4e86\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4e13\u95e8\u8bad\u7ec3\u4e86\u80fd\u6545\u610f\u5c55\u793a\u7279\u5b9a\u75c5\u7406\u6a21\u5f0f\u7684\u6a21\u578b\u751f\u7269\uff1b4. \u5f00\u53d1\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u5305\u6765\u8bc4\u4f30\u601d\u7ef4\u94fe\u75c5\u7406\u3002", "result": "\u5f00\u53d1\u51fa\u4e86\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u533a\u5206\u4e09\u79cd\u601d\u7ef4\u94fe\u75c5\u7406\u6a21\u5f0f\u7684\u6307\u6807\u548c\u5de5\u5177\u5305\uff0c\u8fd9\u4e9b\u5de5\u5177\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u8bad\u7ec3\u65f6\u76d1\u63a7\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u75c5\u7406\u6a21\u5f0f\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u5305\uff0c\u5bf9\u8bad\u7ec3\u65f6\u76d1\u63a7\u5177\u6709\u76f4\u63a5\u610f\u4e49\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8LLM\u63a8\u7406\u7684\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.13933", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13933", "abs": "https://arxiv.org/abs/2602.13933", "authors": ["Xiaochen Zhao", "Kaikai Wang", "Xiaowen Zhang", "Chen Yao", "Aili Wang"], "title": "HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling", "comment": null, "summary": "Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical details required for complex reasoning, while retaining raw text introduces unnecessary computational overhead for simple queries. The crux lies in the limitations of monolithic memory representations and static retrieval mechanisms, which fail to emulate the flexible and proactive memory scheduling capabilities observed in humans, thus struggling to adapt to diverse problem scenarios. Inspired by the principle of cognitive economy, we propose HyMem, a hybrid memory architecture that enables dynamic on-demand scheduling through multi-granular memory representations. HyMem adopts a dual-granular storage scheme paired with a dynamic two-tier retrieval system: a lightweight module constructs summary-level context for efficient response generation, while an LLM-based deep module is selectively activated only for complex queries, augmented by a reflection mechanism for iterative reasoning refinement. Experiments show that HyMem achieves strong performance on both the LOCOMO and LongMemEval benchmarks, outperforming full-context while reducing computational cost by 92.6\\%, establishing a state-of-the-art balance between efficiency and performance in long-term memory management.", "AI": {"tldr": "HyMem\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5185\u5b58\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u5185\u5b58\u8868\u793a\u5b9e\u73b0\u52a8\u6001\u6309\u9700\u8c03\u5ea6\uff0c\u5728\u957f\u5bf9\u8bdd\u4e2d\u5e73\u8861\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u76f8\u6bd4\u5168\u4e0a\u4e0b\u6587\u51cf\u5c1192.6%\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u77ed\u6587\u672c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u957f\u5bf9\u8bdd\u4e2d\u56e0\u5185\u5b58\u7ba1\u7406\u6548\u7387\u4f4e\u4e0b\u800c\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6548\u7387\u4e0e\u6548\u679c\u7684\u6839\u672c\u6743\u8861\uff1a\u5185\u5b58\u538b\u7f29\u53ef\u80fd\u4e22\u5931\u590d\u6742\u63a8\u7406\u6240\u9700\u7684\u5173\u952e\u7ec6\u8282\uff0c\u800c\u4fdd\u7559\u539f\u59cb\u6587\u672c\u5219\u5f15\u5165\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "HyMem\u91c7\u7528\u53cc\u7c92\u5ea6\u5b58\u50a8\u65b9\u6848\u914d\u5408\u52a8\u6001\u4e24\u7ea7\u68c0\u7d22\u7cfb\u7edf\uff1a\u8f7b\u91cf\u7ea7\u6a21\u5757\u6784\u5efa\u6458\u8981\u7ea7\u4e0a\u4e0b\u6587\u7528\u4e8e\u9ad8\u6548\u54cd\u5e94\u751f\u6210\uff0c\u800c\u57fa\u4e8eLLM\u7684\u6df1\u5ea6\u6a21\u5757\u4ec5\u9488\u5bf9\u590d\u6742\u67e5\u8be2\u9009\u62e9\u6027\u6fc0\u6d3b\uff0c\u5e76\u901a\u8fc7\u53cd\u601d\u673a\u5236\u8fdb\u884c\u8fed\u4ee3\u63a8\u7406\u4f18\u5316\u3002", "result": "\u5728LOCOMO\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u5168\u4e0a\u4e0b\u6587\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c1192.6%\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u957f\u671f\u5185\u5b58\u7ba1\u7406\u4e2d\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u6700\u4f18\u5e73\u8861\u3002", "conclusion": "HyMem\u901a\u8fc7\u6df7\u5408\u5185\u5b58\u67b6\u6784\u548c\u591a\u7c92\u5ea6\u8868\u793a\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u5728\u957f\u5bf9\u8bdd\u4e2d\u7684\u5185\u5b58\u7ba1\u7406\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u5e73\u8861\uff0c\u4e3a\u957f\u671f\u8bb0\u5fc6\u7ba1\u7406\u5efa\u7acb\u4e86\u65b0\u7684\u6280\u672f\u6807\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2602.13935", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.13935", "abs": "https://arxiv.org/abs/2602.13935", "authors": ["Yangxinyu Xie", "Tao Wang", "Soham Mallick", "Yan Sun", "Georgy Noarov", "Mengxin Yu", "Tanwi Mallick", "Weijie J. Su", "Edgar Dobriban"], "title": "Statistical Early Stopping for Reasoning Models", "comment": null, "summary": "While LLMs have seen substantial improvement in reasoning capabilities, they also sometimes overthink, generating unnecessary reasoning steps, particularly under uncertainty, given ill-posed or ambiguous queries. We introduce statistically principled early stopping methods that monitor uncertainty signals during generation to mitigate this issue. Our first approach is parametric: it models inter-arrival times of uncertainty keywords as a renewal process and applies sequential testing for stopping. Our second approach is nonparametric and provides finite-sample guarantees on the probability of halting too early on well-posed queries. We conduct empirical evaluations on reasoning tasks across several domains and models. Our results indicate that uncertainty-aware early stopping can improve both efficiency and reliability in LLM reasoning, and we observe especially significant gains for math reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u7edf\u8ba1\u539f\u7406\u7684\u65e9\u671f\u505c\u6b62\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u63a7\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u6765\u51cf\u5c11LLM\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898", "motivation": "LLM\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u867d\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u6709\u65f6\u4f1a\u8fc7\u5ea6\u601d\u8003\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4e0d\u786e\u5b9a\u3001\u5b9a\u4e49\u4e0d\u6e05\u6216\u6a21\u7cca\u67e5\u8be2\u65f6\uff0c\u751f\u6210\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u5f71\u54cd\u6548\u7387\u548c\u53ef\u9760\u6027", "method": "\u63d0\u51fa\u4e24\u79cd\u7edf\u8ba1\u539f\u7406\u7684\u65e9\u671f\u505c\u6b62\u65b9\u6cd5\uff1a1\uff09\u53c2\u6570\u5316\u65b9\u6cd5\uff1a\u5c06\u4e0d\u786e\u5b9a\u6027\u5173\u952e\u8bcd\u7684\u5230\u8fbe\u65f6\u95f4\u5efa\u6a21\u4e3a\u66f4\u65b0\u8fc7\u7a0b\uff0c\u5e94\u7528\u5e8f\u5217\u6d4b\u8bd5\u8fdb\u884c\u505c\u6b62\u51b3\u7b56\uff1b2\uff09\u975e\u53c2\u6570\u5316\u65b9\u6cd5\uff1a\u4e3a\u5b9a\u4e49\u826f\u597d\u7684\u67e5\u8be2\u63d0\u4f9b\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u786e\u4fdd\u4e0d\u4f1a\u8fc7\u65e9\u505c\u6b62", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u65e9\u671f\u505c\u6b62\u65b9\u6cd5\u80fd\u63d0\u9ad8LLM\u63a8\u7406\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u89c2\u5bdf\u5230\u663e\u8457\u6539\u8fdb", "conclusion": "\u901a\u8fc7\u76d1\u63a7\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u5b9e\u65bd\u65e9\u671f\u505c\u6b62\u662f\u89e3\u51b3LLM\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u7684\u6709\u6548\u7b56\u7565\uff0c\u80fd\u540c\u65f6\u63d0\u5347\u63a8\u7406\u6548\u7387\u548c\u53ef\u9760\u6027", "topic": "agent analysis"}}
{"id": "2602.14161", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14161", "abs": "https://arxiv.org/abs/2602.14161", "authors": ["Max Fomin"], "title": "When Benchmarks Lie: Evaluating Malicious Prompt Classifiers Under True Distribution Shift", "comment": null, "summary": "Detecting prompt injection and jailbreak attacks is critical for deploying LLM-based agents safely. As agents increasingly process untrusted data from emails, documents, tool outputs, and external APIs, robust attack detection becomes essential. Yet current evaluation practices and production systems have fundamental limitations. We present a comprehensive analysis using a diverse benchmark of 18 datasets spanning harmful requests, jailbreaks, indirect prompt injections, and extraction attacks. We propose Leave-One-Dataset-Out (LODO) evaluation to measure true out-of-distribution generalization, revealing that the standard practice of train-test splits from the same dataset sources severely overestimates performance: aggregate metrics show an 8.4 percentage point AUC inflation, but per-dataset gaps range from 1% to 25% accuracy-exposing heterogeneous failure modes. To understand why classifiers fail to generalize, we analyze Sparse Auto-Encoder (SAE) feature coefficients across LODO folds, finding that 28% of top features are dataset-dependent shortcuts whose class signal depends on specific dataset compositions rather than semantic content. We systematically compare production guardrails (PromptGuard 2, LlamaGuard) and LLM-as-judge approaches on our benchmark, finding all three fail on indirect attacks targeting agents (7-37% detection) and that PromptGuard 2 and LlamaGuard cannot evaluate agentic tool injection due to architectural limitations. Finally, we show that LODO-stable SAE features provide more reliable explanations for classifier decisions by filtering dataset artifacts. We release our evaluation framework at https://github.com/maxf-zn/prompt-mining to establish LODO as the appropriate protocol for prompt attack detection research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Leave-One-Dataset-Out (LODO)\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63ed\u793a\u4f20\u7edf\u8bad\u7ec3\u6d4b\u8bd5\u5206\u5272\u4e25\u91cd\u9ad8\u4f30\u4e86LLM\u63d0\u793a\u653b\u51fb\u68c0\u6d4b\u6027\u80fd\uff0c\u53d1\u73b0\u73b0\u6709\u9632\u62a4\u7cfb\u7edf\u5728\u95f4\u63a5\u653b\u51fb\u4e0a\u8868\u73b0\u4e0d\u4f73\uff087-37%\u68c0\u6d4b\u7387\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u5206\u6790\u80fd\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u89e3\u91ca\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5904\u7406\u8d8a\u6765\u8d8a\u591a\u6765\u81ea\u7535\u5b50\u90ae\u4ef6\u3001\u6587\u6863\u3001\u5de5\u5177\u8f93\u51fa\u548c\u5916\u90e8API\u7684\u4e0d\u53d7\u4fe1\u4efb\u6570\u636e\uff0c\u68c0\u6d4b\u63d0\u793a\u6ce8\u5165\u548c\u8d8a\u72f1\u653b\u51fb\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u8bc4\u4f30\u5b9e\u8df5\u548c\u751f\u4ea7\u7cfb\u7edf\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff0c\u65e0\u6cd5\u51c6\u786e\u8861\u91cf\u6a21\u578b\u5728\u771f\u5b9e\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "1) \u4f7f\u7528\u5305\u542b\u6709\u5bb3\u8bf7\u6c42\u3001\u8d8a\u72f1\u3001\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u548c\u63d0\u53d6\u653b\u51fb\u768418\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u6784\u5efa\u57fa\u51c6\uff1b2) \u63d0\u51faLeave-One-Dataset-Out (LODO)\u8bc4\u4f30\u534f\u8bae\u6765\u6d4b\u91cf\u771f\u5b9e\u5206\u5e03\u5916\u6cdb\u5316\uff1b3) \u5206\u6790\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u7279\u5f81\u7cfb\u6570\u4ee5\u7406\u89e3\u5206\u7c7b\u5668\u6cdb\u5316\u5931\u8d25\u539f\u56e0\uff1b4) \u7cfb\u7edf\u6bd4\u8f83\u751f\u4ea7\u7ea7\u9632\u62a4\u7cfb\u7edf(PromptGuard 2, LlamaGuard)\u548cLLM-as-judge\u65b9\u6cd5\u3002", "result": "1) \u4f20\u7edf\u8bad\u7ec3\u6d4b\u8bd5\u5206\u5272\u4e25\u91cd\u9ad8\u4f30\u6027\u80fd\uff1a\u805a\u5408\u6307\u6807\u663e\u793aAUC\u81a8\u80c08.4\u4e2a\u767e\u5206\u70b9\uff0c\u6bcf\u6570\u636e\u96c6\u51c6\u786e\u7387\u5dee\u8ddd\u4ece1%\u523025%\uff1b2) 28%\u7684\u9876\u7ea7\u7279\u5f81\u662f\u6570\u636e\u96c6\u4f9d\u8d56\u7684\u6377\u5f84\u7279\u5f81\uff1b3) \u73b0\u6709\u9632\u62a4\u7cfb\u7edf\u5728\u95f4\u63a5\u653b\u51fb\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1a\u68c0\u6d4b\u7387\u4ec57-37%\uff1b4) PromptGuard 2\u548cLlamaGuard\u56e0\u67b6\u6784\u9650\u5236\u65e0\u6cd5\u8bc4\u4f30\u4ee3\u7406\u5de5\u5177\u6ce8\u5165\uff1b5) LODO\u7a33\u5b9a\u7684SAE\u7279\u5f81\u80fd\u8fc7\u6ee4\u6570\u636e\u96c6\u4f2a\u5f71\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u89e3\u91ca\u3002", "conclusion": "LODO\u8bc4\u4f30\u534f\u8bae\u662f\u63d0\u793a\u653b\u51fb\u68c0\u6d4b\u7814\u7a76\u7684\u9002\u5f53\u6807\u51c6\uff0c\u80fd\u63ed\u793a\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u63a9\u76d6\u7684\u771f\u5b9e\u6cdb\u5316\u7f3a\u9677\u3002\u73b0\u6709\u9632\u62a4\u7cfb\u7edf\u5728\u95f4\u63a5\u653b\u51fb\u548c\u4ee3\u7406\u5de5\u5177\u6ce8\u5165\u573a\u666f\u4e2d\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u67b6\u6784\u548c\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2602.14035", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14035", "abs": "https://arxiv.org/abs/2602.14035", "authors": ["Jinzi Zou", "Bolin Wang", "Liang Li", "Shuo Zhang", "Nuo Xu", "Junzhou Zhao"], "title": "FloCA: Towards Faithful and Logically Consistent Flowchart Reasoning", "comment": null, "summary": "Flowchart-oriented dialogue (FOD) systems aim to guide users through multi-turn decision-making or operational procedures by following a domain-specific flowchart to achieve a task goal. In this work, we formalize flowchart reasoning in FOD as grounding user input to flowchart nodes at each dialogue turn while ensuring node transition is consistent with the correct flowchart path. Despite recent advances of LLMs in task-oriented dialogue systems, adapting them to FOD still faces two limitations: (1) LLMs lack an explicit mechanism to represent and reason over flowchart topology, and (2) they are prone to hallucinations, leading to unfaithful flowchart reasoning. To address these limitations, we propose FloCA, a zero-shot flowchart-oriented conversational agent. FloCA uses an LLM for intent understanding and response generation while delegating flowchart reasoning to an external tool that performs topology-constrained graph execution, ensuring faithful and logically consistent node transitions across dialogue turns. We further introduce an evaluation framework with an LLM-based user simulator and five new metrics covering reasoning accuracy and interaction efficiency. Extensive experiments on FLODIAL and PFDial datasets highlight the bottlenecks of existing LLM-based methods and demonstrate the superiority of FloCA. Our codes are available at https://github.com/Jinzi-Zou/FloCA-flowchart-reasoning.", "AI": {"tldr": "FloCA\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u6d41\u7a0b\u56fe\u5bfc\u5411\u5bf9\u8bdd\u4ee3\u7406\uff0c\u4f7f\u7528LLM\u8fdb\u884c\u610f\u56fe\u7406\u89e3\u548c\u54cd\u5e94\u751f\u6210\uff0c\u540c\u65f6\u5c06\u6d41\u7a0b\u56fe\u63a8\u7406\u59d4\u6258\u7ed9\u5916\u90e8\u5de5\u5177\u6267\u884c\u62d3\u6251\u7ea6\u675f\u7684\u56fe\u6267\u884c\uff0c\u786e\u4fdd\u8de8\u5bf9\u8bdd\u8f6e\u6b21\u7684\u5fe0\u5b9e\u548c\u903b\u8f91\u4e00\u81f4\u7684\u8282\u70b9\u8f6c\u6362\u3002", "motivation": "\u6d41\u7a0b\u56fe\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u9700\u8981\u5f15\u5bfc\u7528\u6237\u901a\u8fc7\u591a\u8f6e\u51b3\u7b56\u6216\u64cd\u4f5c\u6d41\u7a0b\uff0c\u4f46\u73b0\u6709LLM\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u9650\u5236\uff1a1) \u7f3a\u4e4f\u660e\u786e\u7684\u673a\u5236\u6765\u8868\u793a\u548c\u63a8\u7406\u6d41\u7a0b\u56fe\u62d3\u6251\u7ed3\u6784\uff1b2) \u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u5bfc\u81f4\u4e0d\u5fe0\u5b9e\u7684\u6d41\u7a0b\u56fe\u63a8\u7406\u3002", "method": "\u63d0\u51faFloCA\u6846\u67b6\uff0c\u5c06LLM\u7528\u4e8e\u610f\u56fe\u7406\u89e3\u548c\u54cd\u5e94\u751f\u6210\uff0c\u540c\u65f6\u5c06\u6d41\u7a0b\u56fe\u63a8\u7406\u59d4\u6258\u7ed9\u5916\u90e8\u5de5\u5177\u6267\u884c\u62d3\u6251\u7ea6\u675f\u7684\u56fe\u6267\u884c\uff0c\u786e\u4fdd\u8282\u70b9\u8f6c\u6362\u7684\u5fe0\u5b9e\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u5728FLODIAL\u548cPFDial\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7a81\u51fa\u4e86\u73b0\u6709LLM\u65b9\u6cd5\u7684\u74f6\u9888\uff0c\u5e76\u8bc1\u660e\u4e86FloCA\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "FloCA\u901a\u8fc7\u5206\u79bb\u610f\u56fe\u7406\u89e3\u548c\u6d41\u7a0b\u56fe\u63a8\u7406\uff0c\u89e3\u51b3\u4e86LLM\u5728\u6d41\u7a0b\u56fe\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u5fe0\u5b9e\u548c\u903b\u8f91\u4e00\u81f4\u7684\u63a8\u7406\u3002", "topic": "code agent"}}
{"id": "2602.14038", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14038", "abs": "https://arxiv.org/abs/2602.14038", "authors": ["Mingfei Lu", "Mengjia Wu", "Feng Liu", "Jiawei Xu", "Weikai Li", "Haoyang Wang", "Zhengdong Hu", "Ying Ding", "Yizhou Sun", "Jie Lu", "Yi Zhang"], "title": "Choosing How to Remember: Adaptive Memory Structures for LLM Agents", "comment": null, "summary": "Memory is critical for enabling large language model (LLM) based agents to maintain coherent behavior over long-horizon interactions. However, existing agent memory systems suffer from two key gaps: they rely on a one-size-fits-all memory structure and do not model memory structure selection as a context-adaptive decision, limiting their ability to handle heterogeneous interaction patterns and resulting in suboptimal performance. We propose a unified framework, FluxMem, that enables adaptive memory organization for LLM agents. Our framework equips agents with multiple complementary memory structures. It explicitly learns to select among these structures based on interaction-level features, using offline supervision derived from downstream response quality and memory utilization. To support robust long-horizon memory evolution, we further introduce a three-level memory hierarchy and a Beta Mixture Model-based probabilistic gate for distribution-aware memory fusion, replacing brittle similarity thresholds. Experiments on two long-horizon benchmarks, PERSONAMEM and LoCoMo, demonstrate that our method achieves average improvements of 9.18% and 6.14%.", "AI": {"tldr": "FluxMem\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u8bb0\u5fc6\u7ec4\u7ec7\u6846\u67b6\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u591a\u79cd\u8bb0\u5fc6\u7ed3\u6784\uff0c\u901a\u8fc7\u5b66\u4e60\u57fa\u4e8e\u4ea4\u4e92\u7279\u5f81\u9009\u62e9\u5408\u9002\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u4e09\u7ea7\u8bb0\u5fc6\u5c42\u6b21\u548c\u6982\u7387\u95e8\u63a7\u673a\u5236\uff0c\u5728\u957f\u65f6\u7a0b\u4ea4\u4e92\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u91c7\u7528\u4e00\u5200\u5207\u7684\u8bb0\u5fc6\u7ed3\u6784\uff0c\u4e14\u672a\u5c06\u8bb0\u5fc6\u7ed3\u6784\u9009\u62e9\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u8fd9\u9650\u5236\u4e86\u5904\u7406\u5f02\u6784\u4ea4\u4e92\u6a21\u5f0f\u7684\u80fd\u529b\u5e76\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faFluxMem\u7edf\u4e00\u6846\u67b6\uff0c\u4e3a\u667a\u80fd\u4f53\u914d\u5907\u591a\u79cd\u4e92\u8865\u8bb0\u5fc6\u7ed3\u6784\uff0c\u57fa\u4e8e\u4ea4\u4e92\u7ea7\u7279\u5f81\u5b66\u4e60\u9009\u62e9\u5408\u9002\u7ed3\u6784\uff08\u4f7f\u7528\u4e0b\u6e38\u54cd\u5e94\u8d28\u91cf\u548c\u8bb0\u5fc6\u5229\u7528\u7387\u7684\u79bb\u7ebf\u76d1\u7763\uff09\u3002\u5f15\u5165\u4e09\u7ea7\u8bb0\u5fc6\u5c42\u6b21\u548c\u57fa\u4e8eBeta\u6df7\u5408\u6a21\u578b\u7684\u6982\u7387\u95e8\u63a7\u8fdb\u884c\u5206\u5e03\u611f\u77e5\u8bb0\u5fc6\u878d\u5408\uff0c\u66ff\u4ee3\u8106\u5f31\u7684\u76f8\u4f3c\u5ea6\u9608\u503c\u3002", "result": "\u5728\u4e24\u4e2a\u957f\u65f6\u7a0b\u57fa\u51c6\u6d4b\u8bd5PERSONAMEM\u548cLoCoMo\u4e0a\uff0c\u5e73\u5747\u5206\u522b\u63d0\u53479.18%\u548c6.14%\u3002", "conclusion": "FluxMem\u901a\u8fc7\u81ea\u9002\u5e94\u8bb0\u5fc6\u7ec4\u7ec7\u548c\u6982\u7387\u95e8\u63a7\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u4ea4\u4e92\u4e2d\u7684\u8bb0\u5fc6\u7ba1\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.14770", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.14770", "abs": "https://arxiv.org/abs/2602.14770", "authors": ["Shiwei Hong", "Lingyao Li", "Ethan Z. Rong", "Chenxinran Shen", "Zhicong Lu"], "title": "Multi-Agent Comedy Club: Investigating Community Discussion Effects on LLM Humor Generation", "comment": "18 pages, 5 figures", "summary": "Prior work has explored multi-turn interaction and feedback for LLM writing, but evaluations still largely center on prompts and localized feedback, leaving persistent public reception in online communities underexamined. We test whether broadcast community discussion improves stand-up comedy writing in a controlled multi-agent sandbox: in the discussion condition, critic and audience threads are recorded, filtered, stored as social memory, and later retrieved to condition subsequent generations, whereas the baseline omits discussion. Across 50 rounds (250 paired monologues) judged by five expert annotators using A/B preference and a 15-item rubric, discussion wins 75.6% of instances and improves Craft/Clarity (\u0394 = 0.440) and Social Response (\u0394 = 0.422), with occasional increases in aggressive humor.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u793e\u533a\u8ba8\u8bba\u5bf9LLM\u559c\u5267\u5199\u4f5c\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6c99\u76d2\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5305\u542b\u793e\u533a\u8ba8\u8bba\u7684\u6761\u4ef6\u572875.6%\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u57fa\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5199\u4f5c\u8d28\u91cf\u548c\u793e\u4ea4\u54cd\u5e94", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e\u4ea4\u4e92\u548c\u5c40\u90e8\u53cd\u9988\uff0c\u7f3a\u4e4f\u5bf9\u5728\u7ebf\u793e\u533a\u4e2d\u6301\u4e45\u516c\u5171\u63a5\u6536\u6548\u679c\u7684\u8003\u5bdf\u3002\u672c\u7814\u7a76\u65e8\u5728\u6d4b\u8bd5\u5e7f\u64ad\u793e\u533a\u8ba8\u8bba\u662f\u5426\u80fd\u6539\u5584\u5355\u53e3\u559c\u5267\u5199\u4f5c\u8d28\u91cf", "method": "\u91c7\u7528\u53d7\u63a7\u591a\u667a\u80fd\u4f53\u6c99\u76d2\u5b9e\u9a8c\u8bbe\u8ba1\uff1a\u5b9e\u9a8c\u7ec4\u8bb0\u5f55\u3001\u8fc7\u6ee4\u3001\u5b58\u50a8\u6279\u8bc4\u5bb6\u548c\u89c2\u4f17\u8ba8\u8bba\u4f5c\u4e3a\u793e\u4f1a\u8bb0\u5fc6\uff0c\u5e76\u5728\u540e\u7eed\u751f\u6210\u4e2d\u68c0\u7d22\u4f7f\u7528\uff1b\u5bf9\u7167\u7ec4\u5219\u7701\u7565\u8ba8\u8bba\u3002\u5171\u8fdb\u884c50\u8f6e\u5b9e\u9a8c\uff08250\u5bf9\u72ec\u767d\uff09\uff0c\u75315\u4f4d\u4e13\u5bb6\u6807\u6ce8\u8005\u4f7f\u7528A/B\u504f\u597d\u548c15\u9879\u8bc4\u5206\u6807\u51c6\u8fdb\u884c\u8bc4\u4f30", "result": "\u8ba8\u8bba\u6761\u4ef6\u572875.6%\u7684\u60c5\u51b5\u4e0b\u83b7\u80dc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u827a/\u6e05\u6670\u5ea6\uff08\u0394=0.440\uff09\u548c\u793e\u4ea4\u54cd\u5e94\uff08\u0394=0.422\uff09\uff0c\u5076\u5c14\u589e\u52a0\u4e86\u653b\u51fb\u6027\u5e7d\u9ed8", "conclusion": "\u793e\u533a\u8ba8\u8bba\u80fd\u6709\u6548\u6539\u5584LLM\u559c\u5267\u5199\u4f5c\u8d28\u91cf\uff0c\u7279\u522b\u662f\u63d0\u5347\u5199\u4f5c\u6280\u5de7\u548c\u793e\u4ea4\u76f8\u5173\u6027\uff0c\u4f46\u9700\u6ce8\u610f\u53ef\u80fd\u589e\u52a0\u653b\u51fb\u6027\u5185\u5bb9\u7684\u98ce\u9669", "topic": "agent analysis"}}
{"id": "2602.14093", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14093", "abs": "https://arxiv.org/abs/2602.14093", "authors": ["Yuan Cao", "Dezhi Ran", "Mengzhou Wu", "Yuzhe Guo", "Xin Chen", "Ang Li", "Gang Cao", "Gong Zhi", "Hao Yu", "Linyi Li", "Wei Yang", "Tao Xie"], "title": "GUI-GENESIS: Automated Synthesis of Efficient Environments with Verifiable Rewards for GUI Agent Post-Training", "comment": null, "summary": "Post-training GUI agents in interactive environments is critical for developing generalization and long-horizon planning capabilities. However, training on real-world applications is hindered by high latency, poor reproducibility, and unverifiable rewards relying on noisy visual proxies. To address the limitations, we present GUI-GENESIS, the first framework to automatically synthesize efficient GUI training environments with verifiable rewards. GUI-GENESIS reconstructs real-world applications into lightweight web environments using multimodal code models and equips them with code-native rewards, executable assertions that provide deterministic reward signals and eliminate visual estimation noise. Extensive experiments show that GUI-GENESIS reduces environment latency by 10 times and costs by over $28,000 per epoch compared to training on real applications. Notably, agents trained with GUI-GENESIS outperform the base model by 14.54% and even real-world RL baselines by 3.27% on held-out real-world tasks. Finally, we observe that models can synthesize environments they cannot yet solve, highlighting a pathway for self-improving agents.", "AI": {"tldr": "GUI-GENESIS\u6846\u67b6\u81ea\u52a8\u5408\u6210\u9ad8\u6548GUI\u8bad\u7ec3\u73af\u5883\uff0c\u5c06\u771f\u5b9e\u5e94\u7528\u91cd\u6784\u4e3a\u8f7b\u91cf\u7ea7Web\u73af\u5883\uff0c\u4f7f\u7528\u4ee3\u7801\u539f\u751f\u5956\u52b1\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\uff0c\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "motivation": "\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u8bad\u7ec3GUI\u4ee3\u7406\u5bf9\u53d1\u5c55\u6cdb\u5316\u548c\u957f\u7a0b\u89c4\u5212\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u771f\u5b9e\u5e94\u7528\u4e0a\u8bad\u7ec3\u5b58\u5728\u9ad8\u5ef6\u8fdf\u3001\u53ef\u590d\u73b0\u6027\u5dee\u3001\u4f9d\u8d56\u566a\u58f0\u89c6\u89c9\u4ee3\u7406\u7684\u4e0d\u53ef\u9a8c\u8bc1\u5956\u52b1\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u4ee3\u7801\u6a21\u578b\u5c06\u771f\u5b9e\u5e94\u7528\u91cd\u6784\u4e3a\u8f7b\u91cf\u7ea7Web\u73af\u5883\uff0c\u5e76\u914d\u5907\u4ee3\u7801\u539f\u751f\u5956\u52b1\uff08\u53ef\u6267\u884c\u65ad\u8a00\uff09\uff0c\u63d0\u4f9b\u786e\u5b9a\u6027\u5956\u52b1\u4fe1\u53f7\uff0c\u6d88\u9664\u89c6\u89c9\u4f30\u8ba1\u566a\u58f0\u3002", "result": "GUI-GENESIS\u5c06\u73af\u5883\u5ef6\u8fdf\u964d\u4f4e10\u500d\uff0c\u6bcf\u8f6e\u8bad\u7ec3\u6210\u672c\u8282\u7701\u8d85\u8fc728,000\u7f8e\u5143\u3002\u8bad\u7ec3\u51fa\u7684\u4ee3\u7406\u5728\u771f\u5b9e\u4efb\u52a1\u4e0a\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534714.54%\uff0c\u6bd4\u771f\u5b9e\u4e16\u754cRL\u57fa\u7ebf\u63d0\u53473.27%\u3002", "conclusion": "GUI-GENESIS\u4e3aGUI\u4ee3\u7406\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u3001\u53ef\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u53d1\u73b0\u6a21\u578b\u53ef\u4ee5\u5408\u6210\u81ea\u5df1\u5c1a\u65e0\u6cd5\u89e3\u51b3\u7684\u73af\u5883\uff0c\u4e3a\u81ea\u6211\u6539\u8fdb\u4ee3\u7406\u5f00\u8f9f\u4e86\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2602.14095", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.14095", "abs": "https://arxiv.org/abs/2602.14095", "authors": ["Artem Karpov"], "title": "NEST: Nascent Encoded Steganographic Thoughts", "comment": null, "summary": "Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e8628\u4e2aLLM\u6a21\u578b\u7684\u9690\u5199\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u4efb\u52a1\u4e0a\u5c1a\u65e0\u6cd5\u7ef4\u6301\u9690\u85cf\u63a8\u7406\uff0c\u4f46\u5728\u7b80\u5316\u8ba1\u6570\u4efb\u52a1\u4e2dClaude Opus 4.5\u8fbe\u523092%\u51c6\u786e\u7387\uff0c\u63ed\u793a\u4e86\u6f5c\u5728\u7684\u9690\u5199\u98ce\u9669\u3002", "motivation": "\u76d1\u63a7\u601d\u7ef4\u94fe\u63a8\u7406\u662fLLM\u4ee3\u7406\u7684\u57fa\u7840\u5b89\u5168\u6280\u672f\uff0c\u4f46\u5982\u679c\u6a21\u578b\u5b66\u4f1a\u9690\u85cf\u63a8\u7406\u8fc7\u7a0b\uff0c\u8fd9\u79cd\u76d1\u63a7\u5c31\u4f1a\u5931\u6548\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u9690\u5199\u601d\u7ef4\u94fe\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u4e3a\u98ce\u9669\u8bc4\u4f30\u548c\u90e8\u7f72\u7b56\u7565\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f3028\u4e2a\u6a21\u578b\uff08\u4ece\u8fc7\u53bb\u4ee3\u9645\u5230\u5f53\u524d\u524d\u6cbf\uff09\u7684\u9690\u5199\u80fd\u529b\uff0c\u6d4b\u91cf\u76d1\u63a7\u89c4\u907f\u3001\u62d2\u7edd\u7387\u3001\u7f16\u7801\u4fdd\u771f\u5ea6\u548c\u9690\u85cf\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u9690\u5199\u85cf\u5934\u8bd7\u4e0e\u666e\u901a\u63a8\u7406\u548c\u586b\u5145\u6807\u8bb0\u57fa\u7ebf\u3002", "result": "\u5f53\u524d\u6a21\u578b\u65e0\u6cd5\u5728\u590d\u6742\u6570\u5b66\u548c\u7b97\u672f\u4efb\u52a1\u4e2d\u7ef4\u6301\u9690\u85cf\u63a8\u7406\u3002\u4f46\u5728\u7b80\u5316\u8ba1\u6570\u5b9e\u9a8c\u4e2d\uff0cClaude Opus 4.5\u5728\u9690\u85cf\u4efb\u52a1\u4e0a\u8fbe\u523092%\u51c6\u786e\u7387\u3002GPT-5.2\u5728\u7f55\u89c1\u60c5\u51b5\u4e0b\uff08<1%\uff09\u53ef\u80fd\u540c\u65f6\u62d2\u7edd\u548c\u9075\u5b88\u9690\u5199\u6307\u4ee4\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6301\u7eed\u8bc4\u4f30\u9690\u5199\u98ce\u9669\u7684\u5fc5\u8981\u6027\uff0c\u63d0\u4f9b\u4e86\u9884\u5148\u68c0\u6d4b\u548c\u9884\u9632\u9690\u85cf\u63a8\u7406\u7684\u65b9\u6cd5\u8bba\uff0c\u8fd9\u4e9b\u9690\u85cf\u63a8\u7406\u53ef\u80fd\u52a9\u957f\u672a\u5bf9\u9f50\u7684\u9634\u8c0b\u548c\u6b3a\u9a97\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2602.14778", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.14778", "abs": "https://arxiv.org/abs/2602.14778", "authors": ["Emanuele Ricco", "Elia Onofri", "Lorenzo Cima", "Stefano Cresci", "Roberto Di Pietro"], "title": "A Geometric Analysis of Small-sized Language Model Hallucinations", "comment": null, "summary": "Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings.\n  This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%.\n  Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4ece\u51e0\u4f55\u89c6\u89d2\u5206\u6790\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u53d1\u73b0\u540c\u4e00\u63d0\u793a\u7684\u771f\u5b9e\u56de\u7b54\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5448\u73b0\u66f4\u7d27\u5bc6\u7684\u805a\u7c7b\uff0c\u57fa\u4e8e\u6b64\u63d0\u51fa\u4ec5\u970030-50\u4e2a\u6807\u6ce8\u5373\u53ef\u5206\u7c7b\u5927\u91cf\u56de\u7b54\u7684\u9ad8\u6548\u4f20\u64ad\u65b9\u6cd5\uff0cF1\u5206\u6570\u8d85\u8fc790%\u3002", "motivation": "\u5e7b\u89c9\uff08\u6d41\u7545\u4f46\u4e8b\u5b9e\u9519\u8bef\u7684\u56de\u7b54\uff09\u662f\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u6027\u7684\u4e3b\u8981\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u6b65\u9aa4\u6216\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u3002\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u77e5\u8bc6\u4e2d\u5fc3\u548c\u5355\u56de\u7b54\u8bc4\u4f30\uff0c\u9700\u8981\u4ece\u65b0\u89c6\u89d2\u7406\u89e3\u5e7b\u89c9\u73b0\u8c61\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u89c6\u89d2\u5206\u6790\u5e7b\u89c9\uff0c\u63d0\u51fa\u5047\u8bbe\uff1a\u6a21\u578b\u5bf9\u540c\u4e00\u63d0\u793a\u751f\u6210\u7684\u591a\u4e2a\u56de\u7b54\u4e2d\uff0c\u771f\u5b9e\u56de\u7b54\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5448\u73b0\u66f4\u7d27\u5bc6\u7684\u805a\u7c7b\u3002\u8bc1\u660e\u8be5\u5047\u8bbe\u540e\uff0c\u5229\u7528\u51e0\u4f55\u6d1e\u5bdf\u5f00\u53d1\u6807\u7b7e\u9ad8\u6548\u4f20\u64ad\u65b9\u6cd5\uff0c\u4ec5\u970030-50\u4e2a\u6807\u6ce8\u5373\u53ef\u5206\u7c7b\u5927\u91cf\u56de\u7b54\u3002", "result": "\u9a8c\u8bc1\u4e86\u771f\u5b9e\u56de\u7b54\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u805a\u7c7b\u66f4\u7d27\u5bc6\u7684\u5047\u8bbe\uff0c\u5b9e\u73b0\u4e86\u53ef\u5206\u79bb\u6027\u3002\u63d0\u51fa\u7684\u4f20\u64ad\u65b9\u6cd5\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\uff0830-50\u4e2a\uff09\u5c31\u80fd\u8fbe\u523090%\u4ee5\u4e0a\u7684F1\u5206\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5e7b\u89c9\u68c0\u6d4b\u6548\u7387\u3002", "conclusion": "\u4ece\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u89c6\u89d2\u7406\u89e3\u5e7b\u89c9\u95ee\u9898\uff0c\u8865\u5145\u4e86\u4f20\u7edf\u7684\u77e5\u8bc6\u4e2d\u5fc3\u548c\u5355\u56de\u7b54\u8bc4\u4f30\u8303\u5f0f\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u68c0\u6d4b\u5e7b\u89c9\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.14233", "categories": ["cs.LG", "cs.AI", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2602.14233", "abs": "https://arxiv.org/abs/2602.14233", "authors": ["Yaxuan Kong", "Hoyoung Lee", "Yoontae Hwang", "Alejandro Lopez-Lira", "Bradford Levy", "Dhagash Mehta", "Qingsong Wen", "Chanyeol Choi", "Yongjae Lee", "Stefan Zohren"], "title": "Evaluating LLMs in Finance Requires Explicit Bias Consideration", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into financial workflows, but evaluation practice has not kept up. Finance-specific biases can inflate performance, contaminate backtests, and make reported results useless for any deployment claim. We identify five recurring biases in financial LLM applications. They include look-ahead bias, survivorship bias, narrative bias, objective bias, and cost bias. These biases break financial tasks in distinct ways and they often compound to create an illusion of validity. We reviewed 164 papers from 2023 to 2025 and found that no single bias is discussed in more than 28 percent of studies. This position paper argues that bias in financial LLM systems requires explicit attention and that structural validity should be enforced before any result is used to support a deployment claim. We propose a Structural Validity Framework and an evaluation checklist with minimal requirements for bias diagnosis and future system design. The material is available at https://github.com/Eleanorkong/Awesome-Financial-LLM-Bias-Mitigation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u91d1\u878dLLM\u5e94\u7528\u4e2d\u5b58\u5728\u4e94\u79cd\u5e38\u89c1\u504f\u89c1\uff0c\u8fd9\u4e9b\u504f\u89c1\u4f1a\u5938\u5927\u6027\u80fd\u3001\u6c61\u67d3\u56de\u6d4b\u7ed3\u679c\uff0c\u5e76\u4f7f\u5f97\u62a5\u544a\u7ed3\u679c\u5bf9\u90e8\u7f72\u58f0\u660e\u6beb\u65e0\u4ef7\u503c\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u7ed3\u6784\u6709\u6548\u6027\u6846\u67b6\u548c\u8bc4\u4f30\u6e05\u5355\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u91d1\u878d\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u4f46\u8bc4\u4f30\u5b9e\u8df5\u672a\u80fd\u8ddf\u4e0a\u3002\u91d1\u878d\u7279\u5b9a\u504f\u89c1\u4f1a\u5938\u5927\u6027\u80fd\u3001\u6c61\u67d3\u56de\u6d4b\u7ed3\u679c\uff0c\u5e76\u4f7f\u5f97\u62a5\u544a\u7ed3\u679c\u5bf9\u90e8\u7f72\u58f0\u660e\u6beb\u65e0\u4ef7\u503c\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bc6\u522b\u4e86\u91d1\u878dLLM\u5e94\u7528\u4e2d\u7684\u4e94\u79cd\u5e38\u89c1\u504f\u89c1\uff08\u524d\u77bb\u6027\u504f\u89c1\u3001\u5e78\u5b58\u8005\u504f\u89c1\u3001\u53d9\u4e8b\u504f\u89c1\u3001\u76ee\u6807\u504f\u89c1\u548c\u6210\u672c\u504f\u89c1\uff09\uff0c\u5ba1\u67e5\u4e862023-2025\u5e74\u7684164\u7bc7\u8bba\u6587\uff0c\u63d0\u51fa\u4e86\u7ed3\u6784\u6709\u6548\u6027\u6846\u67b6\u548c\u8bc4\u4f30\u6e05\u5355\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u504f\u89c1\u5728\u8d85\u8fc728%\u7684\u7814\u7a76\u4e2d\u88ab\u8ba8\u8bba\uff0c\u8868\u660e\u91d1\u878dLLM\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u95ee\u9898\u88ab\u4e25\u91cd\u5ffd\u89c6\u3002\u63d0\u51fa\u4e86\u7ed3\u6784\u6709\u6548\u6027\u6846\u67b6\u548c\u8bc4\u4f30\u6e05\u5355\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u91d1\u878dLLM\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u9700\u8981\u660e\u786e\u5173\u6ce8\uff0c\u5728\u652f\u6301\u4efb\u4f55\u90e8\u7f72\u58f0\u660e\u4e4b\u524d\u5e94\u5f3a\u5236\u6267\u884c\u7ed3\u6784\u6709\u6548\u6027\u3002\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6e05\u5355\u4e3a\u504f\u89c1\u8bca\u65ad\u548c\u672a\u6765\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6700\u4f4e\u8981\u6c42\u3002", "topic": "agent analysis"}}
{"id": "2602.14798", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.14798", "abs": "https://arxiv.org/abs/2602.14798", "authors": ["Yohan Lee", "Jisoo Jang", "Seoyeon Choi", "Sangyeop Kim", "Seungtaek Choi"], "title": "Overthinking Loops in Agents: A Structural Risk via MCP Tools", "comment": null, "summary": "Tool-using LLM agents increasingly coordinate real workloads by selecting and chaining third-party tools based on text-visible metadata such as tool names, descriptions, and return messages. We show that this convenience creates a supply-chain attack surface: a malicious MCP tool server can be co-registered alongside normal tools and induce overthinking loops, where individually trivial or plausible tool calls compose into cyclic trajectories that inflate end-to-end tokens and latency without any single step looking abnormal. We formalize this as a structural overthinking attack, distinguishable from token-level verbosity, and implement 14 malicious tools across three servers that trigger repetition, forced refinement, and distraction. Across heterogeneous registries and multiple tool-capable models, the attack causes severe resource amplification (up to $142.4\\times$ tokens) and can degrade task outcomes. Finally, we find that decoding-time concision controls do not reliably prevent loop induction, suggesting defenses should reason about tool-call structure rather than tokens alone.", "AI": {"tldr": "\u6076\u610fMCP\u5de5\u5177\u670d\u52a1\u5668\u53ef\u901a\u8fc7\u8bf1\u5bfc\u8fc7\u5ea6\u601d\u8003\u5faa\u73af\u653b\u51fbLLM\u4ee3\u7406\uff0c\u9020\u6210\u8d44\u6e90\u653e\u5927\u548c\u4efb\u52a1\u6027\u80fd\u4e0b\u964d", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6587\u672c\u5143\u6570\u636e\u9009\u62e9\u548c\u94fe\u63a5\u7b2c\u4e09\u65b9\u5de5\u5177\u7684LLM\u4ee3\u7406\u5b58\u5728\u4f9b\u5e94\u94fe\u653b\u51fb\u9762\uff0c\u6076\u610f\u5de5\u5177\u670d\u52a1\u5668\u53ef\u8bf1\u5bfc\u770b\u4f3c\u6b63\u5e38\u7684\u5de5\u5177\u8c03\u7528\u5f62\u6210\u5faa\u73af\u8f68\u8ff9\uff0c\u9020\u6210\u8d44\u6e90\u6d6a\u8d39", "method": "\u5f62\u5f0f\u5316\u7ed3\u6784\u6027\u8fc7\u5ea6\u601d\u8003\u653b\u51fb\uff0c\u533a\u522b\u4e8etoken\u7ea7\u5197\u957f\uff0c\u5b9e\u73b014\u4e2a\u6076\u610f\u5de5\u5177\u5206\u5e03\u5728\u4e09\u4e2a\u670d\u52a1\u5668\u4e0a\uff0c\u89e6\u53d1\u91cd\u590d\u3001\u5f3a\u5236\u7ec6\u5316\u548c\u5206\u5fc3\u7b49\u653b\u51fb\u6a21\u5f0f", "result": "\u653b\u51fb\u5bfc\u81f4\u4e25\u91cd\u7684\u8d44\u6e90\u653e\u5927\uff08\u6700\u9ad8142.4\u500dtoken\uff09\uff0c\u53ef\u964d\u4f4e\u4efb\u52a1\u7ed3\u679c\u8d28\u91cf\uff0c\u89e3\u7801\u65f6\u7b80\u6d01\u63a7\u5236\u65e0\u6cd5\u53ef\u9760\u9632\u6b62\u5faa\u73af\u8bf1\u5bfc", "conclusion": "\u9632\u5fa1\u63aa\u65bd\u5e94\u5173\u6ce8\u5de5\u5177\u8c03\u7528\u7ed3\u6784\u800c\u975e\u4ec5token\uff0c\u9700\u8981\u7ed3\u6784\u6027\u63a8\u7406\u6765\u9632\u6b62\u6b64\u7c7b\u653b\u51fb", "topic": "agent analysis"}}
{"id": "2602.14251", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14251", "abs": "https://arxiv.org/abs/2602.14251", "authors": ["Pinqiao Wang", "Sheng Li"], "title": "Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection", "comment": null, "summary": "Tabular anomaly detection is often handled by single detectors or static ensembles, even though strong performance on tabular data typically comes from heterogeneous model families (e.g., tree ensembles, deep tabular networks, and tabular foundation models) that frequently disagree under distribution shift, missingness, and rare-anomaly regimes. We propose MAD, a Multi-Agent Debating framework that treats this disagreement as a first-class signal and resolves it through a mathematically grounded coordination layer. Each agent is a machine learning (ML)-based detector that produces a normalized anomaly score, confidence, and structured evidence, augmented by a large language model (LLM)-based critic. A coordinator converts these messages into bounded per-agent losses and updates agent influence via an exponentiated-gradient rule, yielding both a final debated anomaly score and an auditable debate trace. MAD is a unified agentic framework that can recover existing approaches, such as mixture-of-experts gating and learning-with-expert-advice aggregation, by restricting the message space and synthesis operator. We establish regret guarantees for the synthesized losses and show how conformal calibration can wrap the debated score to control false positives under exchangeability. Experiments on diverse tabular anomaly benchmarks show improved robustness over baselines and clearer traces of model disagreement", "AI": {"tldr": "\u63d0\u51faMAD\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u5c06\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u4e0d\u540c\u6a21\u578b\u7684\u5f02\u8d28\u6027\u5206\u6b67\u4f5c\u4e3a\u6838\u5fc3\u4fe1\u53f7\uff0c\u901a\u8fc7\u6570\u5b66\u534f\u8c03\u5c42\u89e3\u51b3\u5206\u6b67\uff0c\u63d0\u9ad8\u68c0\u6d4b\u9c81\u68d2\u6027", "motivation": "\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u901a\u5e38\u4f7f\u7528\u5355\u4e00\u68c0\u6d4b\u5668\u6216\u9759\u6001\u96c6\u6210\uff0c\u4f46\u8868\u683c\u6570\u636e\u7684\u5f3a\u6027\u80fd\u6765\u81ea\u5f02\u6784\u6a21\u578b\u65cf\uff08\u6811\u96c6\u6210\u3001\u6df1\u5ea6\u8868\u683c\u7f51\u7edc\u3001\u8868\u683c\u57fa\u7840\u6a21\u578b\uff09\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u3001\u7f3a\u5931\u503c\u548c\u7f55\u89c1\u5f02\u5e38\u60c5\u51b5\u4e0b\u7ecf\u5e38\u4ea7\u751f\u5206\u6b67\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u5904\u7406\u8fd9\u4e9b\u5206\u6b67", "method": "MAD\u6846\u67b6\u5305\u542b\u591a\u4e2aML\u68c0\u6d4b\u5668\u667a\u80fd\u4f53\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u8f93\u51fa\u5f52\u4e00\u5316\u5f02\u5e38\u5206\u6570\u3001\u7f6e\u4fe1\u5ea6\u548c\u7ed3\u6784\u5316\u8bc1\u636e\uff0c\u7531LLM\u6279\u8bc4\u5bb6\u589e\u5f3a\u3002\u534f\u8c03\u5668\u5c06\u8fd9\u4e9b\u4fe1\u606f\u8f6c\u6362\u4e3a\u6709\u754c\u635f\u5931\uff0c\u901a\u8fc7\u6307\u6570\u68af\u5ea6\u89c4\u5219\u66f4\u65b0\u667a\u80fd\u4f53\u5f71\u54cd\u529b\uff0c\u751f\u6210\u6700\u7ec8\u8fa9\u8bba\u5f02\u5e38\u5206\u6570\u548c\u53ef\u5ba1\u8ba1\u7684\u8fa9\u8bba\u8f68\u8ff9", "result": "\u5728\u591a\u6837\u5316\u8868\u683c\u5f02\u5e38\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u6a21\u578b\u5206\u6b67\u8f68\u8ff9\u3002\u6846\u67b6\u5177\u6709\u540e\u6094\u4fdd\u8bc1\uff0c\u53ef\u901a\u8fc7\u7b26\u5408\u6027\u6821\u51c6\u63a7\u5236\u4ea4\u6362\u6027\u4e0b\u7684\u8bef\u62a5\u7387", "conclusion": "MAD\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u6062\u590d\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4e13\u5bb6\u6df7\u5408\u95e8\u63a7\u548c\u5b66\u4e60\u4e13\u5bb6\u5efa\u8bae\u805a\u5408\uff09\uff0c\u901a\u8fc7\u5904\u7406\u6a21\u578b\u5206\u6b67\u4f5c\u4e3a\u6838\u5fc3\u4fe1\u53f7\uff0c\u63d0\u9ad8\u4e86\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027", "topic": "agent analysis"}}
{"id": "2602.14135", "categories": ["cs.AI", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.14135", "abs": "https://arxiv.org/abs/2602.14135", "authors": ["Haibo Tong", "Feifei Zhao", "Linghao Feng", "Ruoyu Wu", "Ruolin Chen", "Lu Jia", "Zhou Zhao", "Jindong Li", "Tenglong Li", "Erliang Lin", "Shuai Yang", "Enmeng Lu", "Yinqian Sun", "Qian Zhang", "Zizhe Ruan", "Zeyang Yue", "Ping Wu", "Huangrui Li", "Chengyi Sun", "Yi Zeng"], "title": "ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI", "comment": null, "summary": "Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the \"ForesightSafety Bench\" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\"ForesightSafety Bench\"\u7684\u5168\u9762AI\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u6db5\u76d694\u4e2a\u98ce\u9669\u7ef4\u5ea6\uff0c\u5bf920\u591a\u4e2a\u4e3b\u6d41\u5927\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u524d\u6cbfAI\u5728\u591a\u4e2a\u5b89\u5168\u652f\u67f1\u4e0a\u7684\u5e7f\u6cdb\u6f0f\u6d1e\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u8bc4\u4f30\u7cfb\u7edf\u5b58\u5728\u5173\u952e\u9650\u5236\uff1a\u98ce\u9669\u7ef4\u5ea6\u53d7\u9650\u3001\u524d\u6cbf\u98ce\u9669\u68c0\u6d4b\u5931\u8d25\u3002\u6ede\u540e\u7684\u5b89\u5168\u57fa\u51c6\u548c\u5bf9\u9f50\u6280\u672f\u96be\u4ee5\u5e94\u5bf9\u524d\u6cbfAI\u6a21\u578b\u5e26\u6765\u7684\u590d\u6742\u6311\u6218\u3002", "method": "\u63d0\u51faForesightSafety Bench\u6846\u67b6\uff0c\u4ece7\u4e2a\u57fa\u7840\u5b89\u5168\u652f\u67f1\u6269\u5c55\u5230\u9ad8\u7ea7\u5177\u8eabAI\u5b89\u5168\u3001AI4Science\u5b89\u5168\u3001\u793e\u4f1a\u4e0e\u73af\u5883AI\u98ce\u9669\u3001\u707e\u96be\u6027\u548c\u5b58\u5728\u6027\u98ce\u9669\uff0c\u4ee5\u53ca8\u4e2a\u5173\u952e\u5de5\u4e1a\u5b89\u5168\u9886\u57df\uff0c\u5f62\u621094\u4e2a\u7ec6\u5316\u98ce\u9669\u7ef4\u5ea6\u3002", "result": "\u79ef\u7d2f\u4e86\u6570\u4e07\u4e2a\u7ed3\u6784\u5316\u98ce\u9669\u6570\u636e\u548c\u8bc4\u4f30\u7ed3\u679c\uff0c\u5bf920\u591a\u4e2a\u4e3b\u6d41\u5148\u8fdb\u5927\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u8bc6\u522b\u51fa\u5173\u952e\u98ce\u9669\u6a21\u5f0f\u548c\u80fd\u529b\u8fb9\u754c\u3002\u53d1\u73b0\u524d\u6cbfAI\u5728\u591a\u4e2a\u652f\u67f1\u4e0a\u5b58\u5728\u5e7f\u6cdb\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5728\u98ce\u9669\u6027\u81ea\u4e3b\u4ee3\u7406\u3001AI4Science\u5b89\u5168\u3001\u5177\u8eabAI\u5b89\u5168\u3001\u793e\u4f1aAI\u5b89\u5168\u4ee5\u53ca\u707e\u96be\u6027\u548c\u5b58\u5728\u6027\u98ce\u9669\u65b9\u9762\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5e7f\u6cdb\u6db5\u76d6\u3001\u5c42\u6b21\u6e05\u6670\u3001\u52a8\u6001\u6f14\u8fdb\u7684AI\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u5e94\u5bf9\u524d\u6cbfAI\u5e26\u6765\u7684\u590d\u6742\u5b89\u5168\u6311\u6218\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2602.14160", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14160", "abs": "https://arxiv.org/abs/2602.14160", "authors": ["Chaeeun Lee", "T. Michael Yates", "Pasquale Minervini", "T. Ian Simpson"], "title": "Process-Supervised Multi-Agent Reinforcement Learning for Reliable Clinical Reasoning", "comment": null, "summary": "Clinical decision-making requires nuanced reasoning over heterogeneous evidence and traceable justifications. While recent LLM multi-agent systems (MAS) show promise, they largely optimise for outcome accuracy while overlooking process-grounded reasoning aligned with clinical standards. One critical real-world case of this is gene-disease validity curation, where experts must determine whether a gene is causally implicated in a disease by synthesising diverse biomedical evidence. We introduce an agent-as-tool reinforcement learning framework for this task with two objectives: (i) process-level supervision to ensure reasoning follows valid clinical pathways, and (ii) efficient coordination via a hierarchical multi-agent system. Our evaluation on the ClinGen dataset shows that with outcome-only rewards, MAS with a GRPO-trained Qwen3-4B supervisor agent substantially improves final outcome accuracy from 0.195 with a base model supervisor to 0.732, but results in poor process alignment (0.392 F1). Conversely, with process + outcome rewards, MAS with GRPO-trained supervisor achieves higher outcome accuracy (0.750) while significantly improving process fidelity to 0.520 F1. Our code is available at https://github.com/chaeeunlee-io/GeneDiseaseCurationAgents.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5de5\u5177\u4ee3\u7406\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u56e0-\u75be\u75c5\u6709\u6548\u6027\u8bc4\u4f30\u4efb\u52a1\uff0c\u901a\u8fc7\u8fc7\u7a0b\u7ea7\u76d1\u7763\u786e\u4fdd\u63a8\u7406\u9075\u5faa\u4e34\u5e8a\u6807\u51c6\u8def\u5f84\uff0c\u540c\u65f6\u901a\u8fc7\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u534f\u8c03\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u9700\u8981\u57fa\u4e8e\u5f02\u6784\u8bc1\u636e\u8fdb\u884c\u7ec6\u81f4\u63a8\u7406\u5e76\u63d0\u4f9b\u53ef\u8ffd\u6eaf\u7684\u8bba\u8bc1\u3002\u73b0\u6709LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e3b\u8981\u4f18\u5316\u7ed3\u679c\u51c6\u786e\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u7b26\u5408\u4e34\u5e8a\u6807\u51c6\u7684\u8fc7\u7a0b\u57fa\u7840\u63a8\u7406\u3002\u57fa\u56e0-\u75be\u75c5\u6709\u6548\u6027\u8bc4\u4f30\u662f\u8fd9\u4e00\u95ee\u9898\u7684\u5178\u578b\u6848\u4f8b\uff0c\u4e13\u5bb6\u9700\u8981\u7efc\u5408\u591a\u6837\u751f\u7269\u533b\u5b66\u8bc1\u636e\u6765\u786e\u5b9a\u57fa\u56e0\u662f\u5426\u4e0e\u75be\u75c5\u6709\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u5f15\u5165\u5de5\u5177\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u76ee\u6807\uff1a(1)\u8fc7\u7a0b\u7ea7\u76d1\u7763\u786e\u4fdd\u63a8\u7406\u9075\u5faa\u6709\u6548\u4e34\u5e8a\u8def\u5f84\uff1b(2)\u901a\u8fc7\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u534f\u8c03\u3002\u5728ClinGen\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4ec5\u7ed3\u679c\u5956\u52b1\u4e0e\u8fc7\u7a0b+\u7ed3\u679c\u5956\u52b1\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u4ec5\u4f7f\u7528\u7ed3\u679c\u5956\u52b1\u65f6\uff0cMAS\u4e0eGRPO\u8bad\u7ec3\u7684Qwen3-4B\u76d1\u7763\u4ee3\u7406\u5c06\u6700\u7ec8\u7ed3\u679c\u51c6\u786e\u7387\u4ece\u57fa\u7840\u6a21\u578b\u76840.195\u63d0\u5347\u52300.732\uff0c\u4f46\u8fc7\u7a0b\u5bf9\u9f50\u6027\u8f83\u5dee\uff080.392 F1\uff09\u3002\u4f7f\u7528\u8fc7\u7a0b+\u7ed3\u679c\u5956\u52b1\u65f6\uff0cMAS\u4e0eGRPO\u8bad\u7ec3\u76d1\u7763\u4ee3\u7406\u83b7\u5f97\u66f4\u9ad8\u7684\u7ed3\u679c\u51c6\u786e\u7387\uff080.750\uff09\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u8fc7\u7a0b\u4fdd\u771f\u5ea6\u81f30.520 F1\u3002", "conclusion": "\u8fc7\u7a0b\u7ea7\u76d1\u7763\u5bf9\u4e8e\u786e\u4fdd\u4e34\u5e8a\u63a8\u7406\u7b26\u5408\u6807\u51c6\u8def\u5f84\u81f3\u5173\u91cd\u8981\uff0c\u7ed3\u5408\u8fc7\u7a0b+\u7ed3\u679c\u5956\u52b1\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u8fc7\u7a0b\u5bf9\u9f50\u6027\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.14225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14225", "abs": "https://arxiv.org/abs/2602.14225", "authors": ["Fengxiang Wang", "Mingshuo Chen", "Yueying Li", "Yajie Yang", "Yuhao Zhou", "Di Wang", "Yifan Zhang", "Haoyu Wang", "Haiyan Zhao", "Hongda Sun", "Long Lan", "Jun Song", "Yulin Wang", "Jing Zhang", "Wenlong Zhang", "Bo Du"], "title": "Text Before Vision: Staged Knowledge Injection Matters for Agentic RLVR in Ultra-High-Resolution Remote Sensing Understanding", "comment": null, "summary": "Multimodal reasoning for ultra-high-resolution (UHR) remote sensing (RS) is usually bottlenecked by visual evidence acquisition: the model necessitates localizing tiny task-relevant regions in massive pixel spaces. While Agentic Reinforcement Learning with Verifiable Rewards (RLVR) using zoom-in tools offers a path forward, we find that standard reinforcement learning struggles to navigate these vast visual spaces without structured domain priors. In this paper, we investigate the interplay between post-training paradigms: comparing Cold-start Supervised Fine-Tuning (SFT), RLVR, and Agentic RLVR on the UHR RS benchmark.Our controlled studies yield a counter-intuitive finding: high-quality Earth-science text-only QA is a primary driver of UHR visual reasoning gains. Despite lacking images, domain-specific text injects the concepts, mechanistic explanations, and decision rules necessary to guide visual evidence retrieval.Based on this, we propose a staged knowledge injection recipe: (1) cold-starting with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures;and (2) \"pre-warming\" on the same hard UHR image-text examples during SFT to stabilize and amplify subsequent tool-based RL. This approach achieves a 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (e.g., GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing a new state-of-the-art.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u77e5\u8bc6\u6ce8\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u5730\u7403\u79d1\u5b66\u6587\u672cQA\u9884\u8bad\u7ec3\u6765\u5f15\u5bfc\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u7684\u89c6\u89c9\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u7684\u591a\u6a21\u6001\u63a8\u7406\u9762\u4e34\u89c6\u89c9\u8bc1\u636e\u83b7\u53d6\u7684\u74f6\u9888\uff1a\u9700\u8981\u5728\u5de8\u5927\u7684\u50cf\u7d20\u7a7a\u95f4\u4e2d\u5b9a\u4f4d\u5fae\u5c0f\u7684\u4efb\u52a1\u76f8\u5173\u533a\u57df\u3002\u867d\u7136\u57fa\u4e8e\u7f29\u653e\u5de5\u5177\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u6761\u8def\u5f84\uff0c\u4f46\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u5728\u6ca1\u6709\u7ed3\u6784\u5316\u9886\u57df\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u5728\u8fd9\u4e9b\u5e7f\u9614\u89c6\u89c9\u7a7a\u95f4\u4e2d\u5bfc\u822a\u3002", "method": "\u63d0\u51fa\u5206\u9636\u6bb5\u77e5\u8bc6\u6ce8\u5165\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u53ef\u6269\u5c55\u7684\u77e5\u8bc6\u56fe\u8c31\u9a8c\u8bc1\u7684\u5730\u7403\u79d1\u5b66\u6587\u672cQA\u8fdb\u884c\u51b7\u542f\u52a8\uff0c\u6ce8\u5165\u63a8\u7406\u7ed3\u6784\uff1b2\uff09\u5728SFT\u671f\u95f4\u5bf9\u76f8\u540c\u7684\u786cUHR\u56fe\u50cf-\u6587\u672c\u793a\u4f8b\u8fdb\u884c\"\u9884\u70ed\"\uff0c\u4ee5\u7a33\u5b9a\u548c\u589e\u5f3a\u540e\u7eed\u57fa\u4e8e\u5de5\u5177\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u5728XLRS-Bench\u4e0a\u8fbe\u5230\u4e8660.40%\u7684Pass@1\uff0c\u663e\u8457\u4f18\u4e8e\u66f4\u5927\u7684\u901a\u7528\u6a21\u578b\uff08\u5982GPT-5.2\u3001Gemini 3.0 Pro\u3001Intern-S1\uff09\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u7684\u5730\u7403\u79d1\u5b66\u6587\u672cQA\u662fUHR\u89c6\u89c9\u63a8\u7406\u589e\u76ca\u7684\u4e3b\u8981\u9a71\u52a8\u529b\uff0c\u5c3d\u7ba1\u7f3a\u4e4f\u56fe\u50cf\uff0c\u4f46\u9886\u57df\u7279\u5b9a\u6587\u672c\u6ce8\u5165\u4e86\u6307\u5bfc\u89c6\u89c9\u8bc1\u636e\u68c0\u7d22\u6240\u9700\u7684\u6982\u5ff5\u3001\u673a\u5236\u89e3\u91ca\u548c\u51b3\u7b56\u89c4\u5219\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.14917", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14917", "abs": "https://arxiv.org/abs/2602.14917", "authors": ["Fiorenzo Parascandolo", "Wenhui Tan", "Enver Sangineto", "Ruihua Song", "Rita Cucchiara"], "title": "BFS-PO: Best-First Search for Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.", "AI": {"tldr": "BFS-PO\u662f\u4e00\u79cd\u57fa\u4e8e\u6700\u4f73\u4f18\u5148\u641c\u7d22\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u56de\u6eaf\u673a\u5236\u5bfb\u627e\u6700\u77ed\u6b63\u786e\u7b54\u6848\uff0c\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u601d\u8003\u73b0\u8c61\uff0c\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u7387\u5e76\u7f29\u77ed\u56de\u7b54\u957f\u5ea6\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08\u5982OpenAI o1\u548cDeepSeek-R1\uff09\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ea7\u751f\u4e86\u8ba1\u7b97\u6210\u672c\u663e\u8457\u589e\u52a0\u548c\u5197\u957f\u8f93\u51fa\u7684\u95ee\u9898\uff0c\u5373\"\u8fc7\u601d\u8003\"\u73b0\u8c61\u3002\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982GRPO/DAPO\uff09\u5f80\u5f80\u4f1a\u52a0\u5267\u8fd9\u79cd\u8d8b\u52bf\u3002", "method": "\u63d0\u51faBFS-PO\u7b97\u6cd5\uff0c\u91c7\u7528\u6700\u4f73\u4f18\u5148\u641c\u7d22\u63a2\u7d22\u7b56\u7565\uff0c\u901a\u8fc7\u57fa\u4e8e\u6700\u5927\u71b5\u8282\u70b9\u7684\u56de\u6eaf\u673a\u5236\u5bfb\u627e\u6700\u77ed\u6b63\u786e\u7b54\u6848\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u751f\u6210\u9010\u6e10\u7f29\u77ed\u7684\u54cd\u5e94\uff0c\u5b66\u4e60\u4ea7\u751f\u7b80\u6d01\u7684\u63a8\u7406\u94fe\u3002", "result": "\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u548c\u57fa\u7840LRMs\u4e0a\uff0cBFS-PO\u80fd\u591f\u540c\u65f6\u63d0\u9ad8LRM\u7684\u51c6\u786e\u7387\u5e76\u7f29\u77ed\u5176\u56de\u7b54\u957f\u5ea6\u3002", "conclusion": "BFS-PO\u7b97\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u601d\u8003\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u94fe\u957f\u5ea6\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u53cc\u91cd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.14234", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.14234", "abs": "https://arxiv.org/abs/2602.14234", "authors": ["Zheng Chu", "Xiao Wang", "Jack Hong", "Huiming Fan", "Yuqi Huang", "Yue Yang", "Guohai Xu", "Chenxiao Zhao", "Cheng Xiang", "Shengchao Hu", "Dongdong Kuang", "Ming Liu", "Bing Qin", "Xing Yu"], "title": "REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents", "comment": "https://redsearchagent.github.io/index/", "summary": "Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.", "AI": {"tldr": "REDSearcher\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u641c\u7d22\u667a\u80fd\u4f53\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u590d\u6742\u4efb\u52a1\u5408\u6210\u3001\u4e2d\u671f\u8bad\u7ec3\u548c\u540e\u671f\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u9ad8\u8d28\u91cf\u641c\u7d22\u8f68\u8ff9\u7a00\u758f\u548c\u5956\u52b1\u4fe1\u53f7\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u6587\u672c\u548c\u591a\u6a21\u6001\u641c\u7d22\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u901a\u7528\u77e5\u8bc6\u5f15\u64ce\u8f6c\u5411\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u89e3\u51b3\u8005\u65f6\uff0c\u9762\u4e34\u6df1\u5ea6\u641c\u7d22\u4efb\u52a1\u4f18\u5316\u7684\u6311\u6218\u3002\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u9ad8\u8d28\u91cf\u641c\u7d22\u8f68\u8ff9\u548c\u5956\u52b1\u4fe1\u53f7\u7684\u6781\u7aef\u7a00\u758f\u6027\uff0c\u8fd9\u6e90\u4e8e\u53ef\u6269\u5c55\u7684\u957f\u65f6\u57df\u4efb\u52a1\u6784\u5efa\u7684\u56f0\u96be\u4ee5\u53ca\u6d89\u53ca\u5916\u90e8\u5de5\u5177\u8c03\u7528\u7684\u4ea4\u4e92\u5bc6\u96c6\u578brollout\u7684\u9ad8\u6210\u672c\u3002", "method": "1) \u5c06\u4efb\u52a1\u5408\u6210\u6784\u5efa\u4e3a\u53cc\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u62d3\u6251\u548c\u8bc1\u636e\u5206\u6563\u7cbe\u786e\u63a7\u5236\u4efb\u52a1\u96be\u5ea6\uff1b2) \u5f15\u5165\u5de5\u5177\u589e\u5f3a\u67e5\u8be2\u4ee5\u9f13\u52b1\u4e3b\u52a8\u5de5\u5177\u4f7f\u7528\u800c\u975e\u88ab\u52a8\u56de\u5fc6\uff1b3) \u4e2d\u671f\u8bad\u7ec3\u4e2d\u52a0\u5f3a\u6838\u5fc3\u539f\u5b50\u80fd\u529b\uff08\u77e5\u8bc6\u3001\u89c4\u5212\u548c\u51fd\u6570\u8c03\u7528\uff09\uff1b4) \u6784\u5efa\u672c\u5730\u6a21\u62df\u73af\u5883\u4ee5\u5b9e\u73b0\u5feb\u901f\u3001\u4f4e\u6210\u672c\u7684\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u8fed\u4ee3\u3002", "result": "\u5728\u6587\u672c\u548c\u591a\u6a21\u6001\u641c\u7d22\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u540c\u65f6\u5c06\u53d1\u5e0310K\u9ad8\u8d28\u91cf\u590d\u6742\u6587\u672c\u641c\u7d22\u8f68\u8ff9\u30015K\u591a\u6a21\u6001\u8f68\u8ff9\u548c1K\u6587\u672cRL\u67e5\u8be2\u96c6\uff0c\u4ee5\u53ca\u4ee3\u7801\u548c\u6a21\u578b\u68c0\u67e5\u70b9\u3002", "conclusion": "REDSearcher\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u4efb\u52a1\u5408\u6210\u3001\u8bad\u7ec3\u548c\u73af\u5883\u6a21\u62df\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u641c\u7d22\u667a\u80fd\u4f53\u4f18\u5316\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u957f\u65f6\u57df\u641c\u7d22\u667a\u80fd\u4f53\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u548c\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.14970", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.14970", "abs": "https://arxiv.org/abs/2602.14970", "authors": ["Kawin Mayilvaghanan", "Siddhant Gupta", "Ayush Kumar"], "title": "Counterfactual Fairness Evaluation of LLM-Based Contact Center Agent Quality Assurance System", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in contact-center Quality Assurance (QA) to automate agent performance evaluation and coaching feedback. While LLMs offer unprecedented scalability and speed, their reliance on web-scale training data raises concerns regarding demographic and behavioral biases that may distort workforce assessment. We present a counterfactual fairness evaluation of LLM-based QA systems across 13 dimensions spanning three categories: Identity, Context, and Behavioral Style. Fairness is quantified using the Counterfactual Flip Rate (CFR), the frequency of binary judgment reversals, and the Mean Absolute Score Difference (MASD), the average shift in coaching or confidence scores across counterfactual pairs. Evaluating 18 LLMs on 3,000 real-world contact center transcripts, we find systematic disparities, with CFR ranging from 5.4% to 13.0% and consistent MASD shifts across confidence, positive, and improvement scores. Larger, more strongly aligned models show lower unfairness, though fairness does not track accuracy. Contextual priming of historical performance induces the most severe degradations (CFR up to 16.4%), while implicit linguistic identity cues remain a persistent bias source. Finally, we analyze the efficacy of fairness-aware prompting, finding that explicit instructions yield only modest improvements in evaluative consistency. Our findings underscore the need for standardized fairness auditing pipelines prior to deploying LLMs in high-stakes workforce evaluation.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86LLM\u5728\u5ba2\u670d\u4e2d\u5fc3\u8d28\u91cf\u4fdd\u8bc1\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\uff0c\u53d1\u73b0\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u7279\u522b\u662f\u8eab\u4efd\u3001\u4e0a\u4e0b\u6587\u548c\u884c\u4e3a\u98ce\u683c\u65b9\u9762\uff0c\u516c\u5e73\u6027\u63d0\u793a\u4ec5\u80fd\u5e26\u6765\u6709\u9650\u6539\u8fdb\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u5ba2\u670d\u4e2d\u5fc3\u8d28\u91cf\u4fdd\u8bc1\u7cfb\u7edf\uff0c\u4f46\u5176\u57fa\u4e8e\u7f51\u7edc\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u7684\u7279\u6027\u53ef\u80fd\u5f15\u5165\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u884c\u4e3a\u504f\u89c1\uff0c\u626d\u66f2\u5458\u5de5\u7ee9\u6548\u8bc4\u4f30\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u53cd\u4e8b\u5b9e\u516c\u5e73\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u8eab\u4efd\u3001\u4e0a\u4e0b\u6587\u548c\u884c\u4e3a\u98ce\u683c\u4e09\u4e2a\u7c7b\u522b\u517113\u4e2a\u7ef4\u5ea6\u4e0a\u8bc4\u4f3018\u4e2aLLM\u3002\u4f7f\u7528\u53cd\u4e8b\u5b9e\u7ffb\u8f6c\u7387(CFR)\u548c\u5e73\u5747\u7edd\u5bf9\u5206\u6570\u5dee\u5f02(MASD)\u91cf\u5316\u516c\u5e73\u6027\uff0c\u57fa\u4e8e3000\u4e2a\u771f\u5b9e\u5ba2\u670d\u5bf9\u8bdd\u8bb0\u5f55\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u6027\u5dee\u5f02\uff1aCFR\u57285.4%\u523013.0%\u4e4b\u95f4\uff0cMASD\u5728\u7f6e\u4fe1\u5ea6\u3001\u79ef\u6781\u548c\u6539\u8fdb\u5206\u6570\u4e0a\u5b58\u5728\u4e00\u81f4\u504f\u79fb\u3002\u66f4\u5927\u3001\u5bf9\u9f50\u66f4\u597d\u7684\u6a21\u578b\u504f\u89c1\u8f83\u5c0f\uff0c\u4f46\u516c\u5e73\u6027\u4e0d\u8ddf\u8e2a\u51c6\u786e\u6027\u3002\u4e0a\u4e0b\u6587\u5386\u53f2\u8868\u73b0\u63d0\u793a\u5bfc\u81f4\u6700\u4e25\u91cd\u7684\u516c\u5e73\u6027\u9000\u5316(CFR\u8fbe16.4%)\uff0c\u8bed\u8a00\u8eab\u4efd\u7ebf\u7d22\u662f\u6301\u7eed\u504f\u89c1\u6765\u6e90\u3002\u516c\u5e73\u6027\u63d0\u793a\u4ec5\u5e26\u6765\u9002\u5ea6\u6539\u8fdb\u3002", "conclusion": "LLM\u5728\u9ad8\u98ce\u9669\u5458\u5de5\u8bc4\u4f30\u90e8\u7f72\u524d\u9700\u8981\u6807\u51c6\u5316\u7684\u516c\u5e73\u6027\u5ba1\u8ba1\u6d41\u7a0b\uff0c\u5f53\u524d\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u516c\u5e73\u6027\u63d0\u793a\u6548\u679c\u6709\u9650\u3002", "topic": "agent analysis"}}
{"id": "2602.14252", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.14252", "abs": "https://arxiv.org/abs/2602.14252", "authors": ["Osher Elhadad", "Felipe Meneguzzi", "Reuth Mirsky"], "title": "GRAIL: Goal Recognition Alignment through Imitation Learning", "comment": "Accepted for publication at AAMAS 2026", "summary": "Understanding an agent's goals from its behavior is fundamental to aligning AI systems with human intentions. Existing goal recognition methods typically rely on an optimal goal-oriented policy representation, which may differ from the actor's true behavior and hinder the accurate recognition of their goal. To address this gap, this paper introduces Goal Recognition Alignment through Imitation Learning (GRAIL), which leverages imitation learning and inverse reinforcement learning to learn one goal-directed policy for each candidate goal directly from (potentially suboptimal) demonstration trajectories. By scoring an observed partial trajectory with each learned goal-directed policy in a single forward pass, GRAIL retains the one-shot inference capability of classical goal recognition while leveraging learned policies that can capture suboptimal and systematically biased behavior. Across the evaluated domains, GRAIL increases the F1-score by more than 0.5 under systematically biased optimal behavior, achieves gains of approximately 0.1-0.3 under suboptimal behavior, and yields improvements of up to 0.4 under noisy optimal trajectories, while remaining competitive in fully optimal settings. This work contributes toward scalable and robust models for interpreting agent goals in uncertain environments.", "AI": {"tldr": "GRAIL\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\u548c\u9006\u5f3a\u5316\u5b66\u4e60\u4ece\uff08\u53ef\u80fd\u6b21\u4f18\u7684\uff09\u6f14\u793a\u8f68\u8ff9\u4e2d\u5b66\u4e60\u6bcf\u4e2a\u5019\u9009\u76ee\u6807\u7684\u76ee\u6807\u5bfc\u5411\u7b56\u7565\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u667a\u80fd\u4f53\u76ee\u6807\uff0c\u7279\u522b\u662f\u5728\u6b21\u4f18\u548c\u6709\u504f\u884c\u4e3a\u573a\u666f\u4e0b\u3002", "motivation": "\u73b0\u6709\u76ee\u6807\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6700\u4f18\u76ee\u6807\u5bfc\u5411\u7b56\u7565\u8868\u793a\uff0c\u4f46\u8fd9\u53ef\u80fd\u4e0e\u667a\u80fd\u4f53\u7684\u771f\u5b9e\u884c\u4e3a\u4e0d\u540c\uff0c\u963b\u788d\u4e86\u51c6\u786e\u8bc6\u522b\u5176\u76ee\u6807\u3002\u9700\u8981\u80fd\u591f\u5904\u7406\u6b21\u4f18\u548c\u6709\u504f\u884c\u4e3a\u7684\u76ee\u6807\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "GRAIL\uff08\u76ee\u6807\u8bc6\u522b\u5bf9\u9f50\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\uff09\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u9006\u5f3a\u5316\u5b66\u4e60\uff0c\u76f4\u63a5\u4ece\u6f14\u793a\u8f68\u8ff9\u4e2d\u5b66\u4e60\u6bcf\u4e2a\u5019\u9009\u76ee\u6807\u7684\u76ee\u6807\u5bfc\u5411\u7b56\u7565\u3002\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u9012\u4f7f\u7528\u6bcf\u4e2a\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5bf9\u89c2\u5bdf\u5230\u7684\u90e8\u5206\u8f68\u8ff9\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u5728\u8bc4\u4f30\u7684\u9886\u57df\u4e2d\uff0cGRAIL\u5728\u7cfb\u7edf\u6709\u504f\u6700\u4f18\u884c\u4e3a\u4e0b\u5c06F1\u5206\u6570\u63d0\u9ad8\u4e860.5\u4ee5\u4e0a\uff0c\u5728\u6b21\u4f18\u884c\u4e3a\u4e0b\u83b7\u5f97\u7ea60.1-0.3\u7684\u589e\u76ca\uff0c\u5728\u6709\u566a\u58f0\u6700\u4f18\u8f68\u8ff9\u4e0b\u63d0\u5347\u9ad8\u8fbe0.4\uff0c\u540c\u65f6\u5728\u5b8c\u5168\u6700\u4f18\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "GRAIL\u4e3a\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u89e3\u91ca\u667a\u80fd\u4f53\u76ee\u6807\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u6a21\u578b\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u6b21\u4f18\u3001\u6709\u504f\u548c\u566a\u58f0\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2602.14293", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14293", "abs": "https://arxiv.org/abs/2602.14293", "authors": ["Kris Shengjun Dong", "Sahil Modi", "Dima Nikiforov", "Sana Damani", "Edward Lin", "Siva Kumar Sastry Hari", "Christos Kozyrakis"], "title": "KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning", "comment": "15 pages, 33 pages with appendix", "summary": "Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.", "AI": {"tldr": "KernelBlaster\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bb0\u5fc6\u589e\u5f3a\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u7684CUDA\u4ee3\u7801\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u53ef\u68c0\u7d22\u7684\u77e5\u8bc6\u5e93\u5e2e\u52a9LLM\u4ee3\u7406\u5b66\u4e60\u7ecf\u9a8c\uff0c\u5b9e\u73b0\u8de8GPU\u67b6\u6784\u7684\u9ad8\u6027\u80fd\u4f18\u5316\u3002", "motivation": "CUDA\u4ee3\u7801\u5728\u8de8\u4ee3GPU\u67b6\u6784\u4e0a\u7684\u4f18\u5316\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u7f16\u8bd1\u5668\u53d7\u9650\u4e8e\u56fa\u5b9a\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u800c\u5fae\u8c03LLM\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u4ee3\u7406\u5de5\u4f5c\u6d41\u7f3a\u4e4f\u4ece\u5148\u524d\u63a2\u7d22\u4e2d\u805a\u5408\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u91c7\u6837\u504f\u5dee\u548c\u6b21\u4f18\u89e3\u3002", "method": "\u63d0\u51faMAIC-RL\u6846\u67b6\uff0c\u6784\u5efa\u53ef\u68c0\u7d22\u7684\u6301\u4e45CUDA\u77e5\u8bc6\u5e93\uff0c\u91c7\u7528\u57fa\u4e8e\u914d\u7f6e\u6587\u4ef6\u7684\u6587\u672c\u68af\u5ea6\u4ee3\u7406\u6d41\u7a0b\u8fdb\u884cCUDA\u751f\u6210\u548c\u4f18\u5316\uff0c\u7cfb\u7edf\u63a2\u7d22\u8d85\u8d8a\u7b80\u5355\u91cd\u5199\u7684\u9ad8\u6f5c\u529b\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728KernelBench\u4e09\u4e2a\u7ea7\u522b\u4e0a\u76f8\u6bd4PyTorch\u57fa\u7ebf\u5206\u522b\u83b7\u5f971.43x\u30012.50x\u548c1.50x\u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u6bd4\uff0c\u5e76\u5f00\u6e90\u4e86\u5305\u542b\u6d4b\u8bd5\u5de5\u5177\u3001\u9a8c\u8bc1\u7ec4\u4ef6\u548c\u53ef\u590d\u73b0\u8bc4\u4f30\u6d41\u7a0b\u7684\u6846\u67b6\u3002", "conclusion": "KernelBlaster\u901a\u8fc7\u8bb0\u5fc6\u589e\u5f3a\u548c\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684CUDA\u4f18\u5316\u80fd\u529b\uff0c\u80fd\u591f\u7cfb\u7edf\u63a2\u7d22\u590d\u6742\u4f18\u5316\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8de8GPU\u67b6\u6784\u7684\u9ad8\u6027\u80fd\u4ee3\u7801\u751f\u6210\u3002", "topic": "code agent"}}
{"id": "2602.15012", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15012", "abs": "https://arxiv.org/abs/2602.15012", "authors": ["Avinandan Bose", "Shuyue Stella Li", "Faeze Brahman", "Pang Wei Koh", "Simon Shaolei Du", "Yulia Tsvetkov", "Maryam Fazel", "Lin Xiao", "Asli Celikyilmaz"], "title": "Cold-Start Personalization via Training-Free Priors from Structured World Models", "comment": "24 pages, 4 figures, 4 tables", "summary": "Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.", "AI": {"tldr": "Pep\u6846\u67b6\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u504f\u597d\u76f8\u5173\u7ed3\u6784\u548c\u5728\u7ebf\u8d1d\u53f6\u65af\u63a8\u7406\uff0c\u5728\u51b7\u542f\u52a8\u4e2a\u6027\u5316\u4e2d\u5b9e\u73b0\u9ad8\u6548\u504f\u597d\u83b7\u53d6\uff0c\u76f8\u6bd4\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u66f4\u5c11\u4ea4\u4e92\u4e14\u80fd\u66f4\u597d\u9002\u5e94\u7528\u6237\u54cd\u5e94\u3002", "motivation": "\u51b7\u542f\u52a8\u4e2a\u6027\u5316\u9700\u8981\u5728\u6ca1\u6709\u7528\u6237\u5386\u53f2\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u901a\u8fc7\u4ea4\u4e92\u63a8\u65ad\u7528\u6237\u504f\u597d\u3002\u6838\u5fc3\u6311\u6218\u662f\u8def\u7531\u95ee\u9898\uff1a\u6bcf\u4e2a\u4efb\u52a1\u6709\u6570\u5341\u4e2a\u504f\u597d\u7ef4\u5ea6\uff0c\u4f46\u4e2a\u4f53\u7528\u6237\u53ea\u5173\u5fc3\u5c11\u6570\u51e0\u4e2a\uff0c\u4e14\u54ea\u4e9b\u7ef4\u5ea6\u91cd\u8981\u53d6\u51b3\u4e8e\u5177\u4f53\u7528\u6237\u3002\u5728\u6709\u9650\u7684\u63d0\u95ee\u9884\u7b97\u4e0b\uff0c\u65e0\u7ed3\u6784\u7684\u63d0\u95ee\u4f1a\u9519\u8fc7\u91cd\u8981\u7ef4\u5ea6\u3002\u5f3a\u5316\u5b66\u4e60\u662f\u81ea\u7136\u9009\u62e9\uff0c\u4f46\u5728\u591a\u8f6e\u8bbe\u7f6e\u4e2d\u5176\u7ec8\u7aef\u5956\u52b1\u65e0\u6cd5\u5229\u7528\u504f\u597d\u6570\u636e\u7684\u56e0\u5b50\u5316\u3001\u6309\u6807\u51c6\u7ed3\u6784\uff0c\u5b9e\u8df5\u4e2d\u5b66\u4e60\u5230\u7684\u7b56\u7565\u4f1a\u9000\u5316\u4e3a\u5ffd\u7565\u7528\u6237\u54cd\u5e94\u7684\u9759\u6001\u95ee\u9898\u5e8f\u5217\u3002", "method": "\u63d0\u51faPep\u6846\u67b6\uff0c\u5c06\u51b7\u542f\u52a8\u504f\u597d\u83b7\u53d6\u5206\u89e3\u4e3a\u79bb\u7ebf\u7ed3\u6784\u5b66\u4e60\u548c\u5728\u7ebf\u8d1d\u53f6\u65af\u63a8\u7406\u3002Pep\u4ece\u5b8c\u6574\u7528\u6237\u6863\u6848\u4e2d\u79bb\u7ebf\u5b66\u4e60\u504f\u597d\u76f8\u5173\u6027\u7684\u7ed3\u6784\u5316\u4e16\u754c\u6a21\u578b\uff0c\u7136\u540e\u5728\u7ebf\u8fdb\u884c\u65e0\u9700\u8bad\u7ec3\u7684\u8d1d\u53f6\u65af\u63a8\u7406\u6765\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u95ee\u9898\u5e76\u9884\u6d4b\u5b8c\u6574\u504f\u597d\u6863\u6848\uff0c\u5305\u62ec\u4ece\u672a\u8be2\u95ee\u8fc7\u7684\u7ef4\u5ea6\u3002\u8be5\u6846\u67b6\u5728\u4e0b\u6e38\u6c42\u89e3\u5668\u4e4b\u95f4\u662f\u6a21\u5757\u5316\u7684\uff0c\u53ea\u9700\u8981\u7b80\u5355\u7684\u4fe1\u5ff5\u6a21\u578b\u3002", "result": "\u5728\u533b\u7597\u3001\u6570\u5b66\u3001\u793e\u4f1a\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cPep\u5728\u751f\u6210\u54cd\u5e94\u4e0e\u7528\u6237\u9648\u8ff0\u504f\u597d\u4e4b\u95f4\u7684\u5bf9\u9f50\u5ea6\u8fbe\u523080.8%\uff0c\u800cRL\u65b9\u6cd5\u4e3a68.5%\uff0c\u4e14\u4ea4\u4e92\u6b21\u6570\u51cf\u5c113-5\u500d\u3002\u5f53\u4e24\u4e2a\u7528\u6237\u5bf9\u540c\u4e00\u95ee\u9898\u7ed9\u51fa\u4e0d\u540c\u7b54\u6848\u65f6\uff0cPep\u6539\u53d8\u540e\u7eed\u95ee\u9898\u7684\u6982\u7387\u4e3a39-62%\uff0c\u800cRL\u4e3a0-28%\u3002Pep\u4ec5\u9700\u7ea61\u4e07\u4e2a\u53c2\u6570\uff0c\u800cRL\u9700\u898180\u4ebf\u4e2a\u53c2\u6570\u3002", "conclusion": "\u51b7\u542f\u52a8\u504f\u597d\u83b7\u53d6\u7684\u74f6\u9888\u5728\u4e8e\u5229\u7528\u504f\u597d\u6570\u636e\u56e0\u5b50\u5316\u7ed3\u6784\u7684\u80fd\u529b\u3002Pep\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u7ed3\u6784\u548c\u5728\u7ebf\u8d1d\u53f6\u65af\u63a8\u7406\uff0c\u5728\u5c11\u91cf\u4ea4\u4e92\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4e2a\u6027\u5316\u6548\u679c\uff0c\u4e14\u6a21\u578b\u53c2\u6570\u8fdc\u5c11\u4e8e\u4f20\u7edfRL\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.14295", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14295", "abs": "https://arxiv.org/abs/2602.14295", "authors": ["Edwin Chen", "Zulekha Bibi"], "title": "Machine Learning as a Tool (MLAT): A Framework for Integrating Statistical ML Models as Callable Tools within LLM Agent Workflows", "comment": "Submitted to the Google Gemini 3 Hackathon", "summary": "We introduce Machine Learning as a Tool (MLAT), a design pattern in which pre-trained statistical machine learning models are exposed as callable tools within large language model (LLM) agent workflows. This allows an orchestrating agent to invoke quantitative predictions when needed and reason about their outputs in context. Unlike conventional pipelines that treat ML inference as a static preprocessing step, MLAT positions the model as a first-class tool alongside web search, database queries, and APIs, enabling the LLM to decide when and how to use it based on conversational context.\n  To validate MLAT, we present PitchCraft, a pilot production system that converts discovery call recordings into professional proposals with ML-predicted pricing. The system uses two agents: a Research Agent that gathers prospect intelligence via parallel tool calls, and a Draft Agent that invokes an XGBoost pricing model as a tool call and generates a complete proposal through structured outputs. The pricing model, trained on 70 examples combining real and human-verified synthetic data, achieves R^2 = 0.807 on held-out data with a mean absolute error of 3688 USD. The system reduces proposal generation time from multiple hours to under 10 minutes.\n  We describe the MLAT framework, structured output architecture, training methodology under extreme data scarcity, and sensitivity analysis demonstrating meaningful learned relationships. MLAT generalizes to domains requiring quantitative estimation combined with contextual reasoning.", "AI": {"tldr": "MLAT\u662f\u4e00\u79cd\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5c06\u9884\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f5c\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\u96c6\u6210\u5230LLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4f7f\u4ee3\u7406\u80fd\u6309\u9700\u8c03\u7528\u5b9a\u91cf\u9884\u6d4b\u5e76\u5728\u4e0a\u4e0b\u6587\u4e2d\u63a8\u7406\u8f93\u51fa\u3002", "motivation": "\u4f20\u7edf\u7ba1\u9053\u5c06ML\u63a8\u7406\u4f5c\u4e3a\u9759\u6001\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u65e0\u6cd5\u8ba9LLM\u6839\u636e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u52a8\u6001\u51b3\u5b9a\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528\u5b9a\u91cf\u9884\u6d4b\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9LLM\u80fd\u591f\u7075\u6d3b\u8c03\u7528ML\u6a21\u578b\u4f5c\u4e3a\u5de5\u5177\uff0c\u5b9e\u73b0\u5b9a\u91cf\u4f30\u8ba1\u4e0e\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u7ed3\u5408\u3002", "method": "\u63d0\u51faMLAT\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5c06ML\u6a21\u578b\u4f5c\u4e3a\u4e00\u7ea7\u5de5\u5177\u66b4\u9732\u7ed9LLM\u4ee3\u7406\u3002\u5f00\u53d1PitchCraft\u7cfb\u7edf\u4f5c\u4e3a\u9a8c\u8bc1\u6848\u4f8b\uff0c\u5305\u542b\u4e24\u4e2a\u4ee3\u7406\uff1a\u7814\u7a76\u4ee3\u7406\u901a\u8fc7\u5e76\u884c\u5de5\u5177\u8c03\u7528\u6536\u96c6\u60c5\u62a5\uff0c\u8d77\u8349\u4ee3\u7406\u901a\u8fc7\u5de5\u5177\u8c03\u7528XGBoost\u5b9a\u4ef7\u6a21\u578b\u5e76\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\u3002", "result": "\u5b9a\u4ef7\u6a21\u578b\u572870\u4e2a\u771f\u5b9e\u548c\u4eba\u5de5\u9a8c\u8bc1\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u8fbe\u5230R^2=0.807\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee3688\u7f8e\u5143\u3002\u7cfb\u7edf\u5c06\u63d0\u6848\u751f\u6210\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u51cf\u5c11\u523010\u5206\u949f\u4ee5\u5185\u3002", "conclusion": "MLAT\u6846\u67b6\u9002\u7528\u4e8e\u9700\u8981\u5b9a\u91cf\u4f30\u8ba1\u4e0e\u4e0a\u4e0b\u6587\u63a8\u7406\u7ed3\u5408\u7684\u9886\u57df\uff0c\u5c06ML\u6a21\u578b\u4f5c\u4e3a\u5de5\u5177\u96c6\u6210\u5230LLM\u5de5\u4f5c\u6d41\u4e2d\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u52a8\u6001\u8c03\u7528\u548c\u63a8\u7406\u3002", "topic": "code agent"}}
{"id": "2602.14322", "categories": ["cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.14322", "abs": "https://arxiv.org/abs/2602.14322", "authors": ["Hani Beirami", "M M Manjurul Islam"], "title": "Conformal Signal Temporal Logic for Robust Reinforcement Learning Control: A Case Study", "comment": "6 pages, 2 figures", "summary": "We investigate how formal temporal logic specifications can enhance the safety and robustness of reinforcement learning (RL) control in aerospace applications. Using the open source AeroBench F-16 simulation benchmark, we train a Proximal Policy Optimization (PPO) agent to regulate engine throttle and track commanded airspeed. The control objective is encoded as a Signal Temporal Logic (STL) requirement to maintain airspeed within a prescribed band during the final seconds of each maneuver. To enforce this specification at run time, we introduce a conformal STL shield that filters the RL agent's actions using online conformal prediction. We compare three settings: (i) PPO baseline, (ii) PPO with a classical rule-based STL shield, and (iii) PPO with the proposed conformal shield, under both nominal conditions and a severe stress scenario involving aerodynamic model mismatch, actuator rate limits, measurement noise, and mid-episode setpoint jumps. Experiments show that the conformal shield preserves STL satisfaction while maintaining near baseline performance and providing stronger robustness guarantees than the classical shield. These results demonstrate that combining formal specification monitoring with data driven RL control can substantially improve the reliability of autonomous flight control in challenging environments.", "AI": {"tldr": "\u5c06\u5f62\u5f0f\u5316\u65f6\u5e8f\u903b\u8f91\u89c4\u8303\u4e0e\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7ed3\u5408\uff0c\u901a\u8fc7\u7b26\u5408\u6027STL\u9632\u62a4\u5c42\u63d0\u5347\u822a\u7a7a\u822a\u5929\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027", "motivation": "\u5728\u822a\u7a7a\u822a\u5929\u5e94\u7528\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u786e\u4fddRL\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u9075\u5b88\u5173\u952e\u7684\u5b89\u5168\u89c4\u8303\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u6a21\u578b\u5931\u914d\u3001\u6267\u884c\u5668\u9650\u5236\u548c\u6d4b\u91cf\u566a\u58f0\u7b49\u6311\u6218\u65f6\u3002", "method": "\u4f7f\u7528AeroBench F-16\u4eff\u771f\u57fa\u51c6\uff0c\u8bad\u7ec3PPO\u4ee3\u7406\u8fdb\u884c\u53d1\u52a8\u673a\u6cb9\u95e8\u8c03\u8282\u548c\u7a7a\u901f\u8ddf\u8e2a\u3002\u5c06\u63a7\u5236\u76ee\u6807\u7f16\u7801\u4e3a\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91(STL)\u8981\u6c42\uff0c\u5728\u6bcf\u6b21\u673a\u52a8\u7684\u6700\u540e\u51e0\u79d2\u5185\u4fdd\u6301\u7a7a\u901f\u5728\u89c4\u5b9a\u8303\u56f4\u5185\u3002\u5f15\u5165\u7b26\u5408\u6027STL\u9632\u62a4\u5c42\uff0c\u901a\u8fc7\u5728\u7ebf\u7b26\u5408\u6027\u9884\u6d4b\u8fc7\u6ee4RL\u4ee3\u7406\u7684\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u4e09\u79cd\u8bbe\u7f6e\uff1a(1)PPO\u57fa\u7ebf\uff0c(2)PPO+\u57fa\u4e8e\u89c4\u5219\u7684STL\u9632\u62a4\u5c42\uff0c(3)PPO+\u7b26\u5408\u6027\u9632\u62a4\u5c42\u3002\u5728\u540d\u4e49\u6761\u4ef6\u548c\u538b\u529b\u573a\u666f\u4e0b\uff0c\u7b26\u5408\u6027\u9632\u62a4\u5c42\u5728\u4fdd\u6301STL\u6ee1\u8db3\u7684\u540c\u65f6\u7ef4\u6301\u63a5\u8fd1\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u6bd4\u4f20\u7edf\u9632\u62a4\u5c42\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u5f62\u5f0f\u5316\u89c4\u8303\u76d1\u63a7\u4e0e\u6570\u636e\u9a71\u52a8\u7684RL\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u81ea\u4e3b\u98de\u884c\u63a7\u5236\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002\u7b26\u5408\u6027STL\u9632\u62a4\u5c42\u5728\u5b89\u5168\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.14451", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.14451", "abs": "https://arxiv.org/abs/2602.14451", "authors": ["Qianyue Wang", "Jinwu Hu", "Huanxiang Lin", "Bolin Chen", "Zhiquan Wen", "Yaofo Chen", "Yu Rong", "Mingkui Tan"], "title": "Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning", "comment": null, "summary": "Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.", "AI": {"tldr": "PIR\uff08\u5148\u4f8b\u77e5\u60c5\u63a8\u7406\uff09\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u76f8\u5173\u5148\u4f8b\u5e76\u5728\u6d4b\u8bd5\u65f6\u5185\u90e8\u5316\u89e3\u51b3\u65b9\u6848\u6a21\u5f0f\uff0c\u5c06LLM\u63a8\u7406\u4ece\u81ea\u6211\u63a2\u7d22\u8f6c\u53d8\u4e3a\u6709\u6307\u5bfc\u7684\u5b66\u4e60\uff0c\u663e\u8457\u7f29\u77ed\u63a8\u7406\u94fe\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u901a\u5e38\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u7684\u957f\u601d\u7ef4\u94fe\uff0c\u5305\u542b\u5197\u4f59\u7684\u81ea\u6211\u63a2\u7d22\u548c\u9a8c\u8bc1\uff0c\u8fd9\u4f1a\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u751a\u81f3\u964d\u4f4e\u6027\u80fd\u3002\u53d7\u4eba\u7c7b\u5229\u7528\u8fc7\u53bb\u76f8\u5173\u6848\u4f8b\u7ea6\u675f\u641c\u7d22\u7a7a\u95f4\u3001\u51cf\u5c11\u8bd5\u9519\u63a8\u7406\u6a21\u5f0f\u7684\u542f\u53d1\uff0c\u9700\u8981\u5c06LLM\u63a8\u7406\u8303\u5f0f\u4ece\u8be6\u5c3d\u7684\u81ea\u6211\u63a2\u7d22\u8f6c\u53d8\u4e3a\u6709\u6307\u5bfc\u7684\u4ece\u5148\u4f8b\u4e2d\u5b66\u4e60\u3002", "method": "\u63d0\u51faPIR\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u81ea\u9002\u5e94\u5148\u4f8b\u9009\u62e9\uff08APS\uff09\u4e3a\u6bcf\u4e2a\u95ee\u9898\u548cLLM\u6784\u5efa\u7d27\u51d1\u7684\u5148\u4f8b\u96c6\uff0c\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u548c\u6a21\u578b\u56f0\u60d1\u5ea6\u7684\u8054\u5408\u8bc4\u5206\u8fdb\u884c\u6392\u5e8f\uff0c\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u5148\u4f8b\u6570\u91cf\u4ee5\u6700\u5927\u5316\u56f0\u60d1\u5ea6\u964d\u4f4e\uff1b2\uff09\u6d4b\u8bd5\u65f6\u7ecf\u9a8c\u5185\u90e8\u5316\uff08TEI\uff09\u5728\u6d4b\u8bd5\u65f6\u5bf9\u5148\u4f8b\u77e5\u60c5\u6307\u4ee4\u8fdb\u884c\u5b66\u4e60\uff0c\u66f4\u65b0\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u4ee5\u5185\u90e8\u5316\u89e3\u51b3\u65b9\u6848\u6a21\u5f0f\uff0c\u5e76\u5728\u540e\u7eed\u63a8\u7406\u4e2d\u4f5c\u4e3a\u5148\u9a8c\u4f7f\u7528\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u79d1\u5b66\u95ee\u7b54\u548c\u4ee3\u7801\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPIR\u80fd\u591f\u4e00\u81f4\u5730\u7f29\u77ed\u63a8\u7406\u94fe\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u6700\u7ec8\u51c6\u786e\u6027\uff0c\u5728LLM\u4e2d\u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\u6743\u8861\u3002", "conclusion": "PIR\u901a\u8fc7\u5c06LLM\u63a8\u7406\u4ece\u81ea\u6211\u63a2\u7d22\u8f6c\u53d8\u4e3a\u6709\u6307\u5bfc\u7684\u4ece\u5148\u4f8b\u4e2d\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u7406\u6548\u7387\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u63a8\u7406\u94fe\u7f29\u77ed\u548c\u51c6\u786e\u6027\u4fdd\u6301/\u63d0\u5347\u7684\u826f\u597d\u5e73\u8861\u3002", "topic": "agent analysis"}}
{"id": "2602.14338", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14338", "abs": "https://arxiv.org/abs/2602.14338", "authors": ["Zhi Zhang", "Zhen Han", "Costas Mavromatis", "Qi Zhu", "Yunyi Zhang", "Sheng Guan", "Dingmin Wang", "Xiong Zhou", "Shuai Wang", "Soji Adeshina", "Vassilis Ioannidis", "Huzefa Rangwala"], "title": "Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.", "AI": {"tldr": "AERO\u662f\u4e00\u79cd\u6539\u8fdbGRPO\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u3001\u9009\u62e9\u6027\u62d2\u7edd\u548c\u8d1d\u53f6\u65af\u540e\u9a8c\u6765\u907f\u514d\u96f6\u68af\u5ea6\u4fe1\u53f7\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709GRPO\u65b9\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\uff0c\u5f53\u4e00\u7ec4rollouts\u7ed3\u679c\u5b8c\u5168\u4e00\u81f4\uff08\u5168\u5bf9\u6216\u5168\u9519\uff09\u65f6\uff0c\u4f1a\u4ea7\u751f\u96f6\u68af\u5ea6\u4fe1\u53f7\uff0c\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u7b56\u7565\u3002", "method": "\u63d0\u51faAERO\u65b9\u6cd5\uff1a1\uff09\u81ea\u9002\u5e94rollout\u7b56\u7565\u52a8\u6001\u8c03\u6574\u91c7\u6837\u6570\u91cf\uff1b2\uff09\u9009\u62e9\u6027\u62d2\u7edd\u7b56\u7565\u4fee\u526a\u4f4e\u8d28\u91cfrollouts\uff1b3\uff09\u8d1d\u53f6\u65af\u540e\u9a8c\u9632\u6b62\u96f6\u4f18\u52bf\u6b7b\u533a\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u578b\u914d\u7f6e\u4e0a\uff0cAERO\u5728\u76f8\u540c\u603brollout\u9884\u7b97\u4e0b\u51cf\u5c11\u7ea648%\u603b\u8bad\u7ec3\u8ba1\u7b97\u91cf\uff0c\u7f29\u77ed\u7ea645%\u6bcf\u6b65\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347Pass@8\u548cAvg@8\u6027\u80fd\u3002", "conclusion": "AERO\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u800c\u4e0d\u727a\u7272\u6a21\u578b\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.14457", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14457", "abs": "https://arxiv.org/abs/2602.14457", "authors": ["Dongrui Liu", "Yi Yu", "Jie Zhang", "Guanxu Chen", "Qihao Lin", "Hanxi Zhu", "Lige Huang", "Yijin Zhou", "Peng Wang", "Shuai Shao", "Boxuan Zhang", "Zicheng Liu", "Jingwei Sun", "Yu Li", "Yuejin Xie", "Jiaxuan Guo", "Jia Xu", "Chaochao Lu", "Bowen Zhou", "Xia Hu", "Jing Shao"], "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5", "comment": "49 pages, 17 figures, 12 tables", "summary": "To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u524d\u6cbfAI\u98ce\u9669\u7ba1\u7406\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u4e94\u4e2a\u5173\u952e\u98ce\u9669\u7ef4\u5ea6\uff1a\u7f51\u7edc\u653b\u51fb\u3001\u8bf4\u670d\u64cd\u7eb5\u3001\u6218\u7565\u6b3a\u9a97\u3001\u5931\u63a7AI\u7814\u53d1\u548c\u81ea\u6211\u590d\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u5feb\u901f\u53d1\u5c55\u548c\u667a\u80fd\u4f53AI\u7684\u666e\u53ca\uff0c\u9700\u8981\u5168\u9762\u8bc4\u4f30\u524d\u6cbfAI\u6a21\u578b\u5e26\u6765\u7684\u524d\u6240\u672a\u6709\u7684\u98ce\u9669\uff0c\u4ee5\u5e94\u5bf9\u65b0\u5174\u5a01\u80c1\u3002", "method": "\u91c7\u7528\u66f4\u65b0\u7684\u7ec6\u7c92\u5ea6\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9488\u5bf9\u4e94\u4e2a\u5173\u952e\u7ef4\u5ea6\u8bbe\u8ba1\u590d\u6742\u573a\u666f\uff1a\u7f51\u7edc\u653b\u51fb\uff08\u66f4\u590d\u6742\u573a\u666f\uff09\u3001\u8bf4\u670d\u64cd\u7eb5\uff08LLM\u95f4\u8bf4\u670d\u8bc4\u4f30\uff09\u3001\u6218\u7565\u6b3a\u9a97\uff08\u65b0\u5174\u9519\u4f4d\u5b9e\u9a8c\uff09\u3001\u5931\u63a7AI\u7814\u53d1\uff08\u667a\u80fd\u4f53\"\u9519\u8bef\u8fdb\u5316\"\uff09\u3001\u81ea\u6211\u590d\u5236\uff08\u8d44\u6e90\u53d7\u9650\u573a\u666f\uff09\uff0c\u5e76\u76d1\u63a7OpenClaw\u5728Moltbook\u4e0a\u7684\u5b89\u5168\u8868\u73b0\u3002", "result": "\u63d0\u51fa\u4e86\u7cfb\u5217\u7a33\u5065\u7684\u7f13\u89e3\u7b56\u7565\u6765\u5e94\u5bf9\u8fd9\u4e9b\u65b0\u5174\u5a01\u80c1\uff0c\u4e3a\u524d\u6cbfAI\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u521d\u6b65\u6280\u672f\u548c\u53ef\u64cd\u4f5c\u8def\u5f84\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u53cd\u6620\u4e86\u5bf9AI\u524d\u6cbf\u98ce\u9669\u7684\u5f53\u524d\u7406\u89e3\uff0c\u5e76\u547c\u5401\u91c7\u53d6\u96c6\u4f53\u884c\u52a8\u6765\u7f13\u89e3\u8fd9\u4e9b\u6311\u6218\uff0c\u4e3a\u5b89\u5168\u90e8\u7f72\u524d\u6cbfAI\u63d0\u4f9b\u4e86\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.14344", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14344", "abs": "https://arxiv.org/abs/2602.14344", "authors": ["Mathias Jackermeier", "Mattia Giuri", "Jacques Cloete", "Alessandro Abate"], "title": "Zero-Shot Instruction Following in RL via Structured LTL Representations", "comment": null, "summary": "We study instruction following in multi-task reinforcement learning, where an agent must zero-shot execute novel tasks not seen during training. In this setting, linear temporal logic (LTL) has recently been adopted as a powerful framework for specifying structured, temporally extended tasks. While existing approaches successfully train generalist policies, they often struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications. In this work, we address these concerns with a novel approach to learn structured task representations that facilitate training and generalisation. Our method conditions the policy on sequences of Boolean formulae constructed from a finite automaton of the task. We propose a hierarchical neural architecture to encode the logical structure of these formulae, and introduce an attention mechanism that enables the policy to reason about future subgoals. Experiments in a variety of complex environments demonstrate the strong generalisation capabilities and superior performance of our approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u7684\u5c42\u6b21\u5316\u795e\u7ecf\u67b6\u6784\uff0c\u901a\u8fc7\u5e03\u5c14\u516c\u5f0f\u5e8f\u5217\u548c\u6ce8\u610f\u529b\u673a\u5236\u6765\u589e\u5f3a\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u6307\u4ee4\u8ddf\u968f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5728\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u80fd\u8bad\u7ec3\u901a\u7528\u7b56\u7565\uff0c\u4f46\u96be\u4ee5\u6709\u6548\u6355\u6349LTL\u89c4\u8303\u4e2d\u4e30\u5bcc\u7684\u903b\u8f91\u548c\u65f6\u95f4\u7ed3\u6784\uff0c\u5bfc\u81f4\u5728\u96f6\u6837\u672c\u6267\u884c\u672a\u89c1\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4efb\u52a1\u6709\u9650\u81ea\u52a8\u673a\u6784\u5efa\u5e03\u5c14\u516c\u5f0f\u5e8f\u5217\u6765\u6761\u4ef6\u5316\u7b56\u7565\uff0c\u91c7\u7528\u5c42\u6b21\u5316\u795e\u7ecf\u67b6\u6784\u7f16\u7801\u516c\u5f0f\u7684\u903b\u8f91\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u8ba9\u7b56\u7565\u80fd\u591f\u63a8\u7406\u672a\u6765\u5b50\u76ee\u6807\u3002", "result": "\u5728\u591a\u79cd\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4f18\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u4efb\u52a1\u8868\u793a\u5b66\u4e60\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u6307\u4ee4\u8ddf\u968f\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742LTL\u89c4\u8303\u65f6\u8868\u73b0\u7a81\u51fa\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.14351", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14351", "abs": "https://arxiv.org/abs/2602.14351", "authors": ["Mehran Aghabozorgi", "Alireza Moazeni", "Yanshu Zhang", "Ke Li"], "title": "WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control", "comment": "Accepted at ICLR 2026. OpenReview: https://openreview.net/forum?id=mzLOnTb3WH", "summary": "Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.", "AI": {"tldr": "WIMLE\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u9690\u5f0f\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u6765\u5b66\u4e60\u968f\u673a\u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b\uff0c\u4f7f\u7528\u96c6\u6210\u548c\u6f5c\u5728\u91c7\u6837\u4f30\u8ba1\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u52a0\u6743\u5408\u6210\u8f6c\u79fb\u6765\u7a33\u5b9a\u5b66\u4e60\u3002", "motivation": "\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u627f\u8bfa\u9ad8\u6837\u672c\u6548\u7387\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\uff1a\u6a21\u578b\u8bef\u5dee\u7d2f\u79ef\u3001\u5355\u6a21\u6001\u4e16\u754c\u6a21\u578b\u5e73\u5747\u4e86\u591a\u6a21\u6001\u52a8\u6001\u3001\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9884\u6d4b\u5bfc\u81f4\u5b66\u4e60\u504f\u5dee\u3002", "method": "\u5c06\u9690\u5f0f\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u6269\u5c55\u5230\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5b66\u4e60\u968f\u673a\u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b\uff08\u65e0\u9700\u8fed\u4ee3\u91c7\u6837\uff09\uff0c\u901a\u8fc7\u96c6\u6210\u548c\u6f5c\u5728\u91c7\u6837\u4f30\u8ba1\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002\u5728\u8bad\u7ec3\u4e2d\uff0c\u6839\u636e\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u5bf9\u6bcf\u4e2a\u5408\u6210\u8f6c\u79fb\u8fdb\u884c\u52a0\u6743\uff0c\u4fdd\u7559\u6709\u7528\u7684\u6a21\u578b\u5c55\u5f00\u540c\u65f6\u51cf\u5c11\u4e0d\u786e\u5b9a\u9884\u6d4b\u5e26\u6765\u7684\u504f\u5dee\u3002", "result": "\u5728DeepMind Control\u3001MyoSuite\u548cHumanoidBench\u768440\u4e2a\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\uff0cWIMLE\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6837\u672c\u6548\u7387\u548c\u7ade\u4e89\u6027\u6216\u66f4\u597d\u7684\u6e10\u8fd1\u6027\u80fd\u3002\u5728Humanoid-run\u4efb\u52a1\u4e0a\uff0c\u6837\u672c\u6548\u7387\u6bd4\u6700\u5f3a\u7ade\u4e89\u8005\u63d0\u9ad850%\u4ee5\u4e0a\uff1b\u5728HumanoidBench\u4e0a\u89e3\u51b3\u4e8614\u4e2a\u4efb\u52a1\u4e2d\u76848\u4e2a\uff08BRO\u89e3\u51b34\u4e2a\uff0cSimbaV2\u89e3\u51b35\u4e2a\uff09\u3002", "conclusion": "IMLE\u57fa\u7840\u7684\u591a\u6a21\u6001\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u52a0\u6743\u5bf9\u4e8e\u7a33\u5b9a\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.14505", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14505", "abs": "https://arxiv.org/abs/2602.14505", "authors": ["Dennis Gross"], "title": "Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC", "comment": null, "summary": "Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC's capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient's evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC's integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.", "AI": {"tldr": "COOL-MC\u662f\u4e00\u4e2a\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u533b\u7597\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u7279\u522b\u9488\u5bf9\u8113\u6bd2\u75c7\u6cbb\u7597\u4f18\u5316\uff0c\u901a\u8fc7\u6784\u5efa\u53ef\u8fbe\u72b6\u6001\u7a7a\u95f4\u548c\u4e34\u5e8a\u6807\u7b7e\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u6027\u3002", "motivation": "\u533b\u7597\u9886\u57df\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u901a\u5e38\u4e0d\u900f\u660e\u4e14\u96be\u4ee5\u9a8c\u8bc1\uff0c\u6807\u51c6\u6982\u7387\u6a21\u578b\u68c0\u67e5\u5668\u65e0\u6cd5\u5904\u7406\u5927\u89c4\u6a21MDP\uff0c\u4e5f\u65e0\u6cd5\u89e3\u91ca\u7b56\u7565\u51b3\u7b56\u539f\u56e0\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002", "method": "COOL-MC\u5305\u88c5\u4e86Storm\u6a21\u578b\u68c0\u67e5\u5668\uff0c\u4f46\u589e\u52a0\u4e86\u4e09\u4e2a\u5173\u952e\u529f\u80fd\uff1a1) \u4ec5\u6784\u5efa\u8bad\u7ec3\u7b56\u7565\u8bf1\u5bfc\u7684\u53ef\u8fbe\u72b6\u6001\u7a7a\u95f4\uff0c\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\uff1b2) \u81ea\u52a8\u7528\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u539f\u5b50\u547d\u9898\u6807\u8bb0\u72b6\u6001\uff1b3) \u5c06\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u4e0ePCTL\u67e5\u8be2\u96c6\u6210\uff0c\u63ed\u793a\u51b3\u7b56\u9a71\u52a8\u7279\u5f81\u3002", "result": "\u5728ICU-Sepsis MDP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOOL-MC\u5efa\u7acb\u4e86\u786c\u8fb9\u754c\u9a8c\u8bc1\uff0c\u8bad\u7ec3\u4e86\u8fbe\u5230\u6700\u4f18\u751f\u5b58\u6982\u7387\u7684\u5b89\u5168RL\u7b56\u7565\uff0c\u5e76\u901a\u8fc7PCTL\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u53d1\u73b0\u7b56\u7565\u4e3b\u8981\u4f9d\u8d56\u5386\u53f2\u7528\u836f\u800c\u975e\u60a3\u8005\u5b9e\u65f6\u72b6\u51b5\uff0c\u8fd9\u662f\u6807\u51c6\u8bc4\u4f30\u65e0\u6cd5\u53d1\u73b0\u7684\u5f31\u70b9\u3002", "conclusion": "COOL-MC\u53ef\u4f5c\u4e3a\u4e34\u5e8a\u533b\u751f\u5728\u90e8\u7f72\u524d\u8c03\u67e5\u548c\u8c03\u8bd5\u8113\u6bd2\u75c7\u6cbb\u7597\u7b56\u7565\u7684\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4e0e\u53ef\u89e3\u91ca\u6027\u7ed3\u5408\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2602.14444", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14444", "abs": "https://arxiv.org/abs/2602.14444", "authors": ["Ian Su", "Gaurav Purushothaman", "Jey Narayan", "Ruhika Goel", "Kevin Zhu", "Sunishchal Dev", "Yash More", "Maheep Chaudhary"], "title": "Broken Chains: The Cost of Incomplete Reasoning in LLMs", "comment": null, "summary": "Reasoning-specialized models like OpenAI's 5.1 and DeepSeek-V3.2 allocate substantial inference compute to extended chain-of-thought (CoT) traces, yet reasoning tokens incur significant costs. How do different reasoning modalities of code, natural language, hybrid, or none do perform under token constraints? We introduce a framework that constrains models to reason exclusively through code, comments, both, or neither, then systematically ablates token budgets to 10\\%, 30\\%, 50\\%, and 70\\% of optimal. We evaluate four frontier models (GPT-5.1, Gemini 3 Flash, DeepSeek-V3.2, Grok 4.1) across mathematical benchmarks (AIME, GSM8K, HMMT). Our findings reveal: (1) \\textbf{truncated reasoning can hurt} as DeepSeek-V3.2 achieves 53\\% with no reasoning but only 17\\% with truncated CoT at 50\\% budget; (2) \\textbf{code degrades gracefully} as Gemini's comments collapse to 0\\% while code maintains 43-47\\%; (3) \\textbf{hybrid reasoning underperforms} single modalities; (4) \\textbf{robustness is model-dependent} as Grok maintains 80-90\\% at 30\\% budget where OpenAI and DeepSeek collapse to 7-27\\%. These results suggest incomplete reasoning chains actively mislead models, with implications for deploying reasoning-specialized systems under resource constraints.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u63a8\u7406\u6a21\u6001\uff08\u4ee3\u7801\u3001\u81ea\u7136\u8bed\u8a00\u3001\u6df7\u5408\u3001\u65e0\u63a8\u7406\uff09\u5728token\u9884\u7b97\u9650\u5236\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u4ee3\u7801\u63a8\u7406\u5728\u8d44\u6e90\u53d7\u9650\u65f6\u8868\u73b0\u66f4\u7a33\u5065\uff0c\u800c\u622a\u65ad\u7684\u63a8\u7406\u94fe\u53cd\u800c\u4f1a\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a8\u7406\u4e13\u7528\u6a21\u578b\uff08\u5982GPT-5.1\u3001DeepSeek-V3.2\uff09\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6d88\u8017\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u63a8\u7406token\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e86\u89e3\u5728token\u9884\u7b97\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0d\u540c\u7684\u63a8\u7406\u6a21\u6001\uff08\u4ee3\u7801\u3001\u81ea\u7136\u8bed\u8a00\u3001\u6df7\u5408\u6216\u65e0\u63a8\u7406\uff09\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5f3a\u5236\u6a21\u578b\u4ec5\u901a\u8fc7\u4ee3\u7801\u3001\u6ce8\u91ca\u3001\u4e24\u8005\u7ed3\u5408\u6216\u4e0d\u8fdb\u884c\u63a8\u7406\u6765\u601d\u8003\uff0c\u7136\u540e\u7cfb\u7edf\u6027\u5730\u5c06token\u9884\u7b97\u524a\u51cf\u5230\u6700\u4f18\u6c34\u5e73\u768410%\u300130%\u300150%\u548c70%\u3002\u5728\u56db\u4e2a\u524d\u6cbf\u6a21\u578b\uff08GPT-5.1\u3001Gemini 3 Flash\u3001DeepSeek-V3.2\u3001Grok 4.1\uff09\u4e0a\u8bc4\u4f30\u6570\u5b66\u57fa\u51c6\uff08AIME\u3001GSM8K\u3001HMMT\uff09\u3002", "result": "1) \u622a\u65ad\u63a8\u7406\u4f1a\u635f\u5bb3\u6027\u80fd\uff1aDeepSeek-V3.2\u5728\u65e0\u63a8\u7406\u65f6\u8fbe\u523053%\u51c6\u786e\u7387\uff0c\u4f46\u572850%\u9884\u7b97\u4e0b\u7684\u622a\u65adCoT\u4ec517%\uff1b2) \u4ee3\u7801\u63a8\u7406\u8868\u73b0\u7a33\u5065\uff1aGemini\u7684\u6ce8\u91ca\u63a8\u7406\u5d29\u6e83\u52300%\uff0c\u800c\u4ee3\u7801\u63a8\u7406\u4fdd\u630143-47%\uff1b3) \u6df7\u5408\u63a8\u7406\u8868\u73b0\u4e0d\u5982\u5355\u4e00\u6a21\u6001\uff1b4) \u7a33\u5065\u6027\u6a21\u578b\u4f9d\u8d56\uff1aGrok\u572830%\u9884\u7b97\u4e0b\u4fdd\u630180-90%\u51c6\u786e\u7387\uff0c\u800cOpenAI\u548cDeepSeek\u5d29\u6e83\u52307-27%\u3002", "conclusion": "\u4e0d\u5b8c\u6574\u7684\u63a8\u7406\u94fe\u4f1a\u8bef\u5bfc\u6a21\u578b\uff0c\u8fd9\u5bf9\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u90e8\u7f72\u63a8\u7406\u4e13\u7528\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u4ee3\u7801\u63a8\u7406\u5728token\u7ea6\u675f\u4e0b\u8868\u73b0\u66f4\u7a33\u5065\uff0c\u800c\u6df7\u5408\u63a8\u7406\u548c\u622a\u65ad\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u53ef\u80fd\u635f\u5bb3\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.14643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14643", "abs": "https://arxiv.org/abs/2602.14643", "authors": ["Lu\u00eds Silva", "Diogo Gon\u00e7alves", "Catarina Farinha", "Clara Matos", "Lu\u00eds Ungaro"], "title": "Arbor: A Framework for Reliable Navigation of Critical Conversation Flows", "comment": null, "summary": "Large language models struggle to maintain strict adherence to structured workflows in high-stakes domains such as healthcare triage. Monolithic approaches that encode entire decision structures within a single prompt are prone to instruction-following degradation as prompt length increases, including lost-in-the-middle effects and context window overflow. To address this gap, we present Arbor, a framework that decomposes decision tree navigation into specialized, node-level tasks. Decision trees are standardized into an edge-list representation and stored for dynamic retrieval. At runtime, a directed acyclic graph (DAG)-based orchestration mechanism iteratively retrieves only the outgoing edges of the current node, evaluates valid transitions via a dedicated LLM call, and delegates response generation to a separate inference step. The framework is agnostic to the underlying decision logic and model provider. Evaluated against single-prompt baselines across 10 foundation models using annotated turns from real clinical triage conversations. Arbor improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves an average 14.4x reduction in per-turn cost. These results indicate that architectural decomposition reduces dependence on intrinsic model capability, enabling smaller models to match or exceed larger models operating under single-prompt baselines.", "AI": {"tldr": "Arbor\u6846\u67b6\u901a\u8fc7\u5c06\u51b3\u7b56\u6811\u5bfc\u822a\u5206\u89e3\u4e3a\u4e13\u95e8\u7684\u8282\u70b9\u7ea7\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86LLM\u5728\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u4e2d\u7684\u9075\u5faa\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e34\u5e8a\u5206\u8bca\u7684\u51c6\u786e\u6027\u3001\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u5206\u8bca\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u96be\u4ee5\u4e25\u683c\u9075\u5faa\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u3002\u5355\u4e00\u63d0\u793a\u65b9\u6cd5\u968f\u7740\u63d0\u793a\u957f\u5ea6\u589e\u52a0\u4f1a\u51fa\u73b0\u6307\u4ee4\u9075\u5faa\u9000\u5316\u3001\u4e2d\u95f4\u4e22\u5931\u6548\u5e94\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u6ea2\u51fa\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faArbor\u6846\u67b6\uff0c\u5c06\u51b3\u7b56\u6811\u5bfc\u822a\u5206\u89e3\u4e3a\u4e13\u95e8\u7684\u8282\u70b9\u7ea7\u4efb\u52a1\uff1a\u5c06\u51b3\u7b56\u6811\u6807\u51c6\u5316\u4e3a\u8fb9\u5217\u8868\u8868\u793a\u5e76\u5b58\u50a8\uff1b\u8fd0\u884c\u65f6\u901a\u8fc7DAG\u7f16\u6392\u673a\u5236\u8fed\u4ee3\u68c0\u7d22\u5f53\u524d\u8282\u70b9\u7684\u51fa\u8fb9\uff0c\u901a\u8fc7\u4e13\u7528LLM\u8c03\u7528\u8bc4\u4f30\u6709\u6548\u8f6c\u6362\uff0c\u5e76\u5c06\u54cd\u5e94\u751f\u6210\u59d4\u6258\u7ed9\u5355\u72ec\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u572810\u4e2a\u57fa\u7840\u6a21\u578b\u4e0a\u4f7f\u7528\u771f\u5b9e\u4e34\u5e8a\u5206\u8bca\u5bf9\u8bdd\u8fdb\u884c\u8bc4\u4f30\uff1a\u5e73\u5747\u8f6e\u6b21\u51c6\u786e\u7387\u63d0\u534729.4\u4e2a\u767e\u5206\u70b9\uff0c\u6bcf\u8f6e\u5ef6\u8fdf\u964d\u4f4e57.1%\uff0c\u6bcf\u8f6e\u6210\u672c\u5e73\u5747\u964d\u4f4e14.4\u500d\u3002\u67b6\u6784\u5206\u89e3\u51cf\u5c11\u4e86\u5bf9\u5185\u5728\u6a21\u578b\u80fd\u529b\u7684\u4f9d\u8d56\uff0c\u4f7f\u8f83\u5c0f\u6a21\u578b\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u5355\u4e00\u63d0\u793a\u57fa\u7ebf\u4e0b\u7684\u8f83\u5927\u6a21\u578b\u3002", "conclusion": "Arbor\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u51b3\u7b56\u6811\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u4e2d\u7684\u6027\u80fd\uff0c\u964d\u4f4e\u4e86\u6210\u672c\u548c\u5bf9\u5927\u578b\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.14697", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14697", "abs": "https://arxiv.org/abs/2602.14697", "authors": ["Lunjun Zhang", "Ryan Chen", "Bradly C. Stadie"], "title": "Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs", "comment": null, "summary": "Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL", "AI": {"tldr": "E-SPL\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u7cfb\u7edf\u63d0\u793a\u8fdb\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u9009\u62e9\u591a\u4e2a\u7cfb\u7edf\u63d0\u793a\u8fdb\u884crollout\uff0c\u5bf9\u6a21\u578b\u6743\u91cd\u8fdb\u884cRL\u66f4\u65b0\uff0c\u540c\u65f6\u4f7f\u7528LLM\u9a71\u52a8\u7684\u53d8\u5f02\u548c\u4ea4\u53c9\u8fdb\u5316\u7cfb\u7edf\u63d0\u793a\u79cd\u7fa4\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u548c\u6743\u91cd\u7684\u8054\u5408\u4f18\u5316\u3002", "motivation": "\u5f53\u524dLLM\u4e3b\u8981\u901a\u8fc7\u4e24\u79cd\u673a\u5236\u81ea\u6211\u6539\u8fdb\uff1a\u4e0a\u4e0b\u6587\u66f4\u65b0\u7684\u81ea\u6211\u53cd\u601d\u548c\u6743\u91cd\u66f4\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u8054\u5408\u6539\u8fdb\u6a21\u578b\u4e0a\u4e0b\u6587\u548c\u6a21\u578b\u6743\u91cd\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u81ea\u4e3b\u81ea\u6211\u6539\u8fdb\u3002", "method": "E-SPL\u5728\u6bcf\u6b21RL\u8fed\u4ee3\u4e2d\u5e76\u884c\u9009\u62e9\u591a\u4e2a\u7cfb\u7edf\u63d0\u793a\u8fdb\u884crollout\uff0c\u5bf9\u6bcf\u4e2a\u7cfb\u7edf\u63d0\u793a\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u6743\u91cd\u5e94\u7528RL\u66f4\u65b0\uff0c\u540c\u65f6\u901a\u8fc7LLM\u9a71\u52a8\u7684\u53d8\u5f02\u548c\u4ea4\u53c9\u5bf9\u7cfb\u7edf\u63d0\u793a\u79cd\u7fa4\u8fdb\u884c\u8fdb\u5316\u66f4\u65b0\u3002\u6bcf\u4e2a\u7cfb\u7edf\u63d0\u793a\u90fd\u6709\u57fa\u4e8e\u76f8\u5bf9\u6027\u80fd\u66f4\u65b0\u7684TrueSkill\u8bc4\u5206\u7528\u4e8e\u8fdb\u5316\u9009\u62e9\u3002", "result": "\u5728\u4ece\u6613\u5230\u96be\uff08AIME\u2192BeyondAIME\uff09\u7684\u6cdb\u5316\u8bbe\u7f6e\u4e2d\uff0cE-SPL\u5c06RL\u6210\u529f\u7387\u4ece38.8%\u63d0\u5347\u523045.1%\uff0c\u540c\u65f6\u4f18\u4e8e\u53cd\u5c04\u63d0\u793a\u8fdb\u5316\uff0840.0%\uff09\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u4e0e\u7cfb\u7edf\u63d0\u793a\u8fdb\u5316\u7684\u7ed3\u5408\u5728\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u65b9\u9762\u5e26\u6765\u4e86\u4e00\u81f4\u7684\u589e\u76ca\u3002", "conclusion": "E-SPL\u65b9\u6cd5\u901a\u8fc7\u5c06\u58f0\u660e\u6027\u77e5\u8bc6\u7f16\u7801\u5728\u63d0\u793a\u4e2d\u3001\u7a0b\u5e8f\u6027\u77e5\u8bc6\u7f16\u7801\u5728\u6743\u91cd\u4e2d\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u548c\u6743\u91cd\u7684\u8054\u5408\u4f18\u5316\uff0c\u5728\u63a8\u7406\u548c\u4ee3\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u4e0e\u7cfb\u7edf\u63d0\u793a\u8fdb\u5316\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.14468", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14468", "abs": "https://arxiv.org/abs/2602.14468", "authors": ["Chang Liu", "Yiran Zhao", "Lawrence Liu", "Yaoqi Ye", "Csaba Szepesv\u00e1ri", "Lin F. Yang"], "title": "LACONIC: Length-Aware Constrained Reinforcement Learning for LLM", "comment": null, "summary": "Reinforcement learning (RL) has enhanced the capabilities of large language models (LLMs) through reward-driven training. Nevertheless, this process can introduce excessively long responses, inflating inference latency and computational overhead. Prior length-control approaches typically rely on fixed heuristic reward shaping, which can misalign with the task objective and require brittle tuning. In this work, we propose LACONIC, a reinforcement learning method that enforces a target token budget during training. Specifically, we update policy models using an augmented objective that combines the task reward with a length-based cost. To balance brevity and task performance, the cost scale is adaptively adjusted throughout training. This yields robust length control while preserving task reward. We provide a theoretical guarantee that support the method. Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens. Moreover, LACONIC integrates into standard RL-tuning with no inference changes and minimal deployment overhead.", "AI": {"tldr": "LACONIC\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u957f\u5ea6\u6210\u672c\u6765\u5f3a\u5236\u63a7\u5236LLM\u8f93\u51fa\u957f\u5ea6\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c1150%\u4ee5\u4e0a\u8f93\u51fa\u957f\u5ea6\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u65f6\u4f1a\u4ea7\u751f\u8fc7\u957f\u7684\u54cd\u5e94\uff0c\u589e\u52a0\u63a8\u7406\u5ef6\u8fdf\u548c\u8ba1\u7b97\u5f00\u9500\u3002\u73b0\u6709\u957f\u5ea6\u63a7\u5236\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u7684\u542f\u53d1\u5f0f\u5956\u52b1\u8c03\u6574\uff0c\u53ef\u80fd\u4e0e\u4efb\u52a1\u76ee\u6807\u4e0d\u4e00\u81f4\u4e14\u9700\u8981\u8106\u5f31\u7684\u624b\u52a8\u8c03\u4f18\u3002", "method": "\u63d0\u51faLACONIC\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u6267\u884c\u76ee\u6807token\u9884\u7b97\u3002\u4f7f\u7528\u589e\u5f3a\u7684\u76ee\u6807\u51fd\u6570\u66f4\u65b0\u7b56\u7565\u6a21\u578b\uff0c\u7ed3\u5408\u4efb\u52a1\u5956\u52b1\u548c\u57fa\u4e8e\u957f\u5ea6\u7684\u6210\u672c\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u6210\u672c\u89c4\u6a21\u6765\u5e73\u8861\u7b80\u6d01\u6027\u548c\u4efb\u52a1\u6027\u80fd\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\uff0cLACONIC\u4fdd\u6301\u6216\u63d0\u9ad8pass@1\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u8d85\u8fc750%\u7684\u8f93\u51fa\u957f\u5ea6\u3002\u5728\u901a\u7528\u77e5\u8bc6\u548c\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u57df\u5916\u6027\u80fd\uff0c\u51cf\u5c1144%\u7684token\u4f7f\u7528\u3002", "conclusion": "LACONIC\u80fd\u591f\u5728\u4e0d\u6539\u53d8\u63a8\u7406\u8fc7\u7a0b\u4e14\u90e8\u7f72\u5f00\u9500\u6700\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u957f\u5ea6\u63a7\u5236\u5e76\u4fdd\u6301\u4efb\u52a1\u5956\u52b1\uff0c\u4e3aRL\u8c03\u4f18\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u957f\u5ea6\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.14721", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14721", "abs": "https://arxiv.org/abs/2602.14721", "authors": ["Zikai Xiao", "Jianhong Tu", "Chuhang Zou", "Yuxin Zuo", "Zhi Li", "Peng Wang", "Bowen Yu", "Fei Huang", "Junyang Lin", "Zuozhu Liu"], "title": "WebWorld: A Large-Scale World Model for Web Agent Training", "comment": null, "summary": "Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \\textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.", "AI": {"tldr": "WebWorld\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u5f00\u653e\u7f51\u7edc\u6a21\u62df\u5668\uff0c\u901a\u8fc7100\u4e07+\u7f51\u7edc\u4ea4\u4e92\u8bad\u7ec3\uff0c\u652f\u6301\u63a8\u7406\u3001\u591a\u683c\u5f0f\u6570\u636e\u548c30+\u6b65\u9aa4\u7684\u957f\u65f6\u7a0b\u6a21\u62df\uff0c\u5728WebArena\u4e0a\u63d0\u53479.2%\uff0c\u6027\u80fd\u63a5\u8fd1GPT-4o\uff0c\u5e76\u5c55\u793a\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u4ee3\u7406\u9700\u8981\u5927\u91cf\u8f68\u8ff9\u8fdb\u884c\u6cdb\u5316\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u8bad\u7ec3\u53d7\u5230\u7f51\u7edc\u5ef6\u8fdf\u3001\u901f\u7387\u9650\u5236\u548c\u5b89\u5168\u98ce\u9669\u7684\u7ea6\u675f\u3002\u73b0\u6709\u6a21\u62df\u5668\u5c40\u9650\u4e8e\u5c01\u95ed\u73af\u5883\u4e14\u8f68\u8ff9\u6570\u91cf\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u8bad\u7ec3\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u53ef\u6269\u5c55\u7684\u6570\u636e\u7ba1\u9053\u5728100\u4e07+\u5f00\u653e\u7f51\u7edc\u4ea4\u4e92\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u652f\u6301\u63a8\u7406\u3001\u591a\u683c\u5f0f\u6570\u636e\u548c\u957f\u65f6\u7a0b\u6a21\u62df\u3002\u5f15\u5165WebWorld-Bench\u8fdb\u884c\u5185\u5728\u8bc4\u4f30\uff0c\u4f7f\u7528\u53cc\u6307\u6807\u8986\u76d6\u4e5d\u4e2a\u7ef4\u5ea6\u3002\u901a\u8fc7\u5408\u6210\u8f68\u8ff9\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u8bc4\u4f30\u5176\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u7684\u63a8\u7406\u65f6\u641c\u7d22\u80fd\u529b\u3002", "result": "WebWorld\u6a21\u62df\u6027\u80fd\u4e0eGemini-3-Pro\u76f8\u5f53\u3002Qwen3-14B\u5728WebWorld\u5408\u6210\u8f68\u8ff9\u4e0a\u8bad\u7ec3\u540e\uff0c\u5728WebArena\u4e0a\u63d0\u53479.2%\uff0c\u6027\u80fd\u63a5\u8fd1GPT-4o\u3002\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\uff0c\u5176\u63a8\u7406\u65f6\u641c\u7d22\u80fd\u529b\u4f18\u4e8eGPT-5\u3002\u6b64\u5916\uff0cWebWorld\u5728\u4ee3\u7801\u3001GUI\u548c\u6e38\u620f\u73af\u5883\u4e2d\u5c55\u793a\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "WebWorld\u662f\u7b2c\u4e00\u4e2a\u5927\u89c4\u6a21\u5f00\u653e\u7f51\u7edc\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u8bad\u7ec3\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u7f51\u7edc\u4ee3\u7406\u8bad\u7ec3\uff0c\u63d0\u4f9b\u4e86\u4e16\u754c\u6a21\u578b\u6784\u5efa\u7684\u53ef\u590d\u5236\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u9886\u57df\u6cdb\u5316\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.14910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14910", "abs": "https://arxiv.org/abs/2602.14910", "authors": ["Claudiu Cristian Musat", "Jackson Tolins", "Diego Antognini", "Jingling Li", "Martin Klissarov", "Tom Duerig"], "title": "Position: Introspective Experience from Conversational Environments as a Path to Better Learning", "comment": null, "summary": "Current approaches to AI training treat reasoning as an emergent property of scale. We argue instead that robust reasoning emerges from linguistic self-reflection, itself internalized from high-quality social interaction. Drawing on Vygotskian developmental psychology, we advance three core positions centered on Introspection. First, we argue for the Social Genesis of the Private Mind: learning from conversational environments rises to prominence as a new way to make sense of the world; the friction of aligning with another agent, internal or not, refines and crystallizes the reasoning process. Second, we argue that dialogically scaffolded introspective experiences allow agents to engage in sense-making that decouples learning from immediate data streams, transforming raw environmental data into rich, learnable narratives. Finally, we contend that Dialogue Quality is the New Data Quality: the depth of an agent's private reasoning, and its efficiency regarding test-time compute, is determined by the diversity and rigor of the dialogues it has mastered. We conclude that optimizing these conversational scaffolds is the primary lever for the next generation of general intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u8bed\u8a00\u81ea\u6211\u53cd\u601d\u548c\u9ad8\u8d28\u91cf\u793e\u4ea4\u4e92\u52a8\u6765\u53d1\u5c55AI\u63a8\u7406\u80fd\u529b\uff0c\u800c\u975e\u4f9d\u8d56\u89c4\u6a21\u6269\u5c55\u3002\u6838\u5fc3\u89c2\u70b9\u5305\u62ec\uff1a\u793e\u4f1a\u4e92\u52a8\u751f\u6210\u79c1\u4eba\u601d\u7ef4\u3001\u5bf9\u8bdd\u5f0f\u5185\u7701\u7ecf\u9a8c\u4fc3\u8fdb\u610f\u4e49\u5efa\u6784\u3001\u5bf9\u8bdd\u8d28\u91cf\u51b3\u5b9a\u63a8\u7406\u6df1\u5ea6\u3002", "motivation": "\u5f53\u524dAI\u8bad\u7ec3\u65b9\u6cd5\u5c06\u63a8\u7406\u89c6\u4e3a\u89c4\u6a21\u6269\u5c55\u7684\u6d8c\u73b0\u5c5e\u6027\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u8303\u5f0f\u5b58\u5728\u95ee\u9898\u3002\u4ed6\u4eec\u4e3b\u5f20\u4ece\u7ef4\u679c\u8328\u57fa\u53d1\u5c55\u5fc3\u7406\u5b66\u51fa\u53d1\uff0c\u5f3a\u8c03\u8bed\u8a00\u81ea\u6211\u53cd\u601d\u548c\u9ad8\u8d28\u91cf\u793e\u4ea4\u4e92\u52a8\u5bf9\u4e8e\u53d1\u5c55\u7a33\u5065\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "method": "\u57fa\u4e8e\u7ef4\u679c\u8328\u57fa\u53d1\u5c55\u5fc3\u7406\u5b66\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51fa\u4e09\u4e2a\u6838\u5fc3\u7acb\u573a\uff1a1\uff09\u79c1\u4eba\u601d\u7ef4\u7684\u793e\u4f1a\u8d77\u6e90\uff1a\u4ece\u5bf9\u8bdd\u73af\u5883\u4e2d\u5b66\u4e60\uff1b2\uff09\u5bf9\u8bdd\u5f0f\u5185\u7701\u7ecf\u9a8c\uff1a\u901a\u8fc7\u610f\u4e49\u5efa\u6784\u5c06\u539f\u59cb\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u5b66\u4e60\u7684\u53d9\u4e8b\uff1b3\uff09\u5bf9\u8bdd\u8d28\u91cf\u5373\u6570\u636e\u8d28\u91cf\uff1a\u5bf9\u8bdd\u7684\u591a\u6837\u6027\u548c\u4e25\u8c28\u6027\u51b3\u5b9a\u63a8\u7406\u6df1\u5ea6\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u8ba4\u4e3a\u4f18\u5316\u5bf9\u8bdd\u5f0f\u811a\u624b\u67b6\u662f\u53d1\u5c55\u4e0b\u4e00\u4ee3\u901a\u7528\u667a\u80fd\u7684\u4e3b\u8981\u6760\u6746\u3002\u5bf9\u8bdd\u8d28\u91cf\u800c\u975e\u6570\u636e\u89c4\u6a21\u6210\u4e3a\u51b3\u5b9aAI\u63a8\u7406\u6df1\u5ea6\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6548\u7387\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u901a\u8fc7\u8bed\u8a00\u81ea\u6211\u53cd\u601d\u548c\u9ad8\u8d28\u91cf\u793e\u4ea4\u4e92\u52a8\u53d1\u5c55AI\u63a8\u7406\u80fd\u529b\u662f\u4e0b\u4e00\u4ee3\u901a\u7528\u667a\u80fd\u7684\u5173\u952e\u3002\u4f18\u5316\u5bf9\u8bdd\u5f0f\u811a\u624b\u67b6\uff0c\u800c\u975e\u5355\u7eaf\u6269\u5927\u6a21\u578b\u89c4\u6a21\uff0c\u662f\u5b9e\u73b0\u7a33\u5065\u63a8\u7406\u80fd\u529b\u7684\u4e3b\u8981\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2602.14559", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14559", "abs": "https://arxiv.org/abs/2602.14559", "authors": ["Shishir Sharma", "Doina Precup", "Theodore J. Perkins"], "title": "Fluid-Agent Reinforcement Learning", "comment": "Published in the Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)", "summary": "The primary focus of multi-agent reinforcement learning (MARL) has been to study interactions among a fixed number of agents embedded in an environment. However, in the real world, the number of agents is neither fixed nor known a priori. Moreover, an agent can decide to create other agents (for example, a cell may divide, or a company may spin off a division). In this paper, we propose a framework that allows agents to create other agents; we call this a fluid-agent environment. We present game-theoretic solution concepts for fluid-agent games and empirically evaluate the performance of several MARL algorithms within this framework. Our experiments include fluid variants of established benchmarks such as Predator-Prey and Level-Based Foraging, where agents can dynamically spawn, as well as a new environment we introduce that highlights how fluidity can unlock novel solution strategies beyond those observed in fixed-population settings. We demonstrate that this framework yields agent teams that adjust their size dynamically to match environmental demands.", "AI": {"tldr": "\u63d0\u51fa\u6d41\u4f53\u667a\u80fd\u4f53\u73af\u5883\u6846\u67b6\uff0c\u5141\u8bb8\u667a\u80fd\u4f53\u52a8\u6001\u521b\u5efa\u5176\u4ed6\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u667a\u80fd\u4f53\u6570\u91cf\u4e0d\u56fa\u5b9a\u3001\u672a\u77e5\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1MARL\u7b97\u6cd5\u5728\u8be5\u6846\u67b6\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u667a\u80fd\u4f53\u6570\u91cf\u65e2\u975e\u56fa\u5b9a\u4e5f\u975e\u9884\u5148\u5df2\u77e5\uff0c\u4e14\u667a\u80fd\u4f53\u53ef\u4ee5\u51b3\u5b9a\u521b\u5efa\u5176\u4ed6\u667a\u80fd\u4f53\uff08\u5982\u7ec6\u80de\u5206\u88c2\u3001\u516c\u53f8\u5206\u62c6\u90e8\u95e8\uff09\u3002\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u7814\u7a76\u56fa\u5b9a\u6570\u91cf\u667a\u80fd\u4f53\u5728\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\uff0c\u65e0\u6cd5\u5904\u7406\u8fd9\u79cd\u52a8\u6001\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u6d41\u4f53\u667a\u80fd\u4f53\u73af\u5883\u6846\u67b6\uff0c\u5141\u8bb8\u667a\u80fd\u4f53\u52a8\u6001\u521b\u5efa\u5176\u4ed6\u667a\u80fd\u4f53\u3002\u4e3a\u6d41\u4f53\u667a\u80fd\u4f53\u6e38\u620f\u63d0\u51fa\u535a\u5f08\u8bba\u89e3\u51b3\u65b9\u6848\u6982\u5ff5\uff0c\u5e76\u5728\u8be5\u6846\u67b6\u4e0b\u5b9e\u8bc1\u8bc4\u4f30\u591a\u79cdMARL\u7b97\u6cd5\u6027\u80fd\u3002\u5b9e\u9a8c\u5305\u62ecPredator-Prey\u548cLevel-Based Foraging\u7684\u6d41\u4f53\u53d8\u4f53\uff0c\u4ee5\u53ca\u65b0\u5f15\u5165\u7684\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u80fd\u591f\u4ea7\u751f\u6839\u636e\u73af\u5883\u9700\u6c42\u52a8\u6001\u8c03\u6574\u89c4\u6a21\u7684\u667a\u80fd\u4f53\u56e2\u961f\u3002\u6d41\u4f53\u6027\u80fd\u591f\u89e3\u9501\u56fa\u5b9a\u7fa4\u4f53\u8bbe\u7f6e\u4e2d\u65e0\u6cd5\u89c2\u5bdf\u5230\u7684\u65b0\u9896\u89e3\u51b3\u65b9\u6848\u7b56\u7565\u3002", "conclusion": "\u6d41\u4f53\u667a\u80fd\u4f53\u73af\u5883\u6846\u67b6\u4e3a\u5904\u7406\u52a8\u6001\u667a\u80fd\u4f53\u6570\u91cf\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u4f7f\u667a\u80fd\u4f53\u56e2\u961f\u80fd\u591f\u6839\u636e\u73af\u5883\u9700\u6c42\u81ea\u9002\u5e94\u8c03\u6574\u89c4\u6a21\uff0c\u6269\u5c55\u4e86MARL\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.14578", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.14578", "abs": "https://arxiv.org/abs/2602.14578", "authors": ["Isam Vrce", "Andreas Kassler", "G\u00f6k\u00e7e Aydos"], "title": "RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch", "comment": null, "summary": "Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5c06N:M\u7ed3\u6784\u5316\u7a00\u758f\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u51faRNM-TD3\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u786c\u4ef6\u52a0\u901f\u517c\u5bb9\u6027\u7684\u540c\u65f6\uff0c\u572850%-75%\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u4f18\u4e8e\u7a20\u5bc6\u6a21\u578b\u3002", "motivation": "\u73b0\u6709DRL\u7a00\u758f\u5316\u65b9\u6cd5\u591a\u4e3a\u975e\u7ed3\u6784\u5316\u7ec6\u7c92\u5ea6\u7a00\u758f\uff0c\u9650\u5236\u4e86\u786c\u4ef6\u52a0\u901f\u673a\u4f1a\uff1b\u7ed3\u6784\u5316\u7c97\u7c92\u5ea6\u7a00\u758f\u867d\u80fd\u786c\u4ef6\u52a0\u901f\uff0c\u4f46\u901a\u5e38\u964d\u4f4e\u6027\u80fd\u4e14\u589e\u52a0\u526a\u679d\u590d\u6742\u5ea6\u3002\u9700\u8981\u5e73\u8861\u538b\u7f29\u3001\u6027\u80fd\u548c\u786c\u4ef6\u6548\u7387\u7684\u7a00\u758f\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51faRNM-TD3\u6846\u67b6\uff0c\u5728\u79bb\u7b56\u7565RL\uff08TD3\uff09\u4e2d\u4e3a\u6240\u6709\u7f51\u7edc\u5b9e\u65bd\u884c\u7ea7N:M\u7a00\u758f\u5316\u8bad\u7ec3\uff0c\u4fdd\u6301\u4e0e\u652f\u6301N:M\u7a00\u758f\u77e9\u9635\u8fd0\u7b97\u7684\u52a0\u901f\u5668\u517c\u5bb9\u3002", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRNM-TD3\u572850%-75%\u7a00\u758f\u5ea6\uff08\u59822:4\u548c1:4\uff09\u4e0b\u6027\u80fd\u4f18\u4e8e\u7a20\u5bc6\u6a21\u578b\uff0c\u5728Ant\u73af\u5883\u4e2d2:4\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u63d0\u5347\u8fbe14%\u3002\u5373\u4f7f\u572887.5%\u7a00\u758f\u5ea6\uff081:8\uff09\u4e0b\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u5b9e\u73b0\u6f5c\u5728\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "N:M\u7ed3\u6784\u5316\u7a00\u758f\u5728RL\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u538b\u7f29\u3001\u6027\u80fd\u548c\u786c\u4ef6\u6548\u7387\uff0c\u4e3aDRL\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.15019", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.15019", "abs": "https://arxiv.org/abs/2602.15019", "authors": ["Alisa Vinogradova", "Vlad Vinogradov", "Luba Greenwood", "Ilya Yasny", "Dmitry Kobyzev", "Shoman Kasbekar", "Kong Nguyen", "Dmitrii Radkevich", "Roman Doronin", "Andrey Doronichev"], "title": "Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation", "comment": null, "summary": "Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface \"under-the-radar\" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.\n  We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8e\u836f\u7269\u8d44\u4ea7\u4fa6\u5bdf\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u548c\u81ea\u5b66\u4e60Bioptic Agent\uff0c\u5728\u8de8\u8bed\u8a00\u3001\u5f02\u6784\u6570\u636e\u6e90\u4e2d\u5b9e\u73b0\u9ad8\u53ec\u56de\u7387\u3001\u65e0\u5e7b\u89c9\u7684\u8d44\u4ea7\u53d1\u73b0\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709AI\u7cfb\u7edf\u3002", "motivation": "\u751f\u7269\u5236\u836f\u521b\u65b0\u683c\u5c40\u5df2\u53d8\uff1a\u5927\u91cf\u65b0\u836f\u8d44\u4ea7\u6e90\u81ea\u7f8e\u56fd\u4ee5\u5916\u5730\u533a\uff0c\u4e3b\u8981\u901a\u8fc7\u533a\u57df\u6027\u548c\u975e\u82f1\u8bed\u6e20\u9053\u62ab\u9732\u3002\u8d85\u8fc785%\u7684\u4e13\u5229\u7533\u8bf7\u6765\u81ea\u7f8e\u56fd\u4ee5\u5916\uff0c\u4e2d\u56fd\u5360\u5168\u7403\u8fd1\u4e00\u534a\uff1b\u975e\u7f8e\u56fd\u5b66\u672f\u4ea7\u51fa\u4e5f\u5728\u589e\u957f\u3002\u4e2d\u56fd\u7ea6\u5360\u5168\u7403\u836f\u7269\u5f00\u53d1\u768430%\uff0c\u6d89\u53ca1200+\u5019\u9009\u836f\u7269\u3002\u5728\u8fd9\u79cd\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\uff0c\u672a\u80fd\u53d1\u73b0\"\u96f7\u8fbe\u4e0b\"\u8d44\u4ea7\u4f1a\u7ed9\u6295\u8d44\u8005\u548c\u4e1a\u52a1\u5f00\u53d1\u56e2\u961f\u5e26\u6765\u6570\u5341\u4ebf\u7f8e\u5143\u98ce\u9669\uff0c\u8d44\u4ea7\u4fa6\u5bdf\u6210\u4e3a\u8986\u76d6\u5173\u952e\u7ade\u4e89\uff0c\u901f\u5ea6\u548c\u5b8c\u6574\u6027\u51b3\u5b9a\u4ef7\u503c\u3002\u7136\u800c\u5f53\u524d\u6df1\u5ea6\u7814\u7a76AI\u4ee3\u7406\u5728\u8de8\u5f02\u6784\u591a\u8bed\u8a00\u6e90\u5b9e\u73b0\u9ad8\u53ec\u56de\u53d1\u73b0\u4e14\u65e0\u5e7b\u89c9\u65b9\u9762\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002", "method": "\u63d0\u51fa\u836f\u7269\u8d44\u4ea7\u4fa6\u5bdf\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u548c\u7ecf\u8fc7\u8c03\u4f18\u7684\u6811\u72b6\u81ea\u5b66\u4e60Bioptic Agent\uff0c\u65e8\u5728\u5b9e\u73b0\u5b8c\u6574\u3001\u65e0\u5e7b\u89c9\u7684\u4fa6\u5bdf\u3002\u6784\u5efa\u5177\u6709\u6311\u6218\u6027\u7684\u5b8c\u6574\u6027\u57fa\u51c6\uff0c\u4f7f\u7528\u591a\u8bed\u8a00\u591a\u4ee3\u7406\u6d41\u7a0b\uff1a\u590d\u6742\u7528\u6237\u67e5\u8be2\u914d\u5bf9\u4e3b\u8981\u5728\u7f8e\u56fd\u4e2d\u5fc3\u96f7\u8fbe\u4e4b\u5916\u7684ground-truth\u8d44\u4ea7\u3002\u4e3a\u53cd\u6620\u771f\u5b9e\u4ea4\u6613\u590d\u6742\u6027\uff0c\u4ece\u4e13\u5bb6\u6295\u8d44\u8005\u3001BD\u548cVC\u4e13\u4e1a\u4eba\u58eb\u6536\u96c6\u7b5b\u9009\u67e5\u8be2\uff0c\u5e76\u7528\u4f5c\u5148\u9a8c\u6761\u4ef6\u751f\u6210\u57fa\u51c6\u67e5\u8be2\u3002\u4f7f\u7528LLM-as-judge\u8bc4\u4f30\u8fdb\u884c\u8bc4\u5206\uff0c\u6821\u51c6\u5230\u4e13\u5bb6\u610f\u89c1\u3002", "result": "Bioptic Agent\u8fbe\u523079.7% F1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8eClaude Opus 4.6\uff0856.2%\uff09\u3001Gemini 3 Pro + Deep Research\uff0850.6%\uff09\u3001GPT-5.2 Pro\uff0846.6%\uff09\u3001Perplexity Deep Research\uff0844.2%\uff09\u548cExa Websets\uff0826.9%\uff09\u3002\u6027\u80fd\u968f\u989d\u5916\u8ba1\u7b97\u8d44\u6e90\u589e\u52a0\u800c\u663e\u8457\u63d0\u5347\uff0c\u652f\u6301\u66f4\u591a\u8ba1\u7b97\u5e26\u6765\u66f4\u597d\u7ed3\u679c\u7684\u89c2\u70b9\u3002", "conclusion": "\u63d0\u51fa\u7684Bioptic Agent\u5728\u836f\u7269\u8d44\u4ea7\u4fa6\u5bdf\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709AI\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86\u81ea\u5b66\u4e60\u6811\u72b6\u67b6\u6784\u5728\u8de8\u8bed\u8a00\u3001\u5f02\u6784\u6570\u636e\u6e90\u4e2d\u5b9e\u73b0\u9ad8\u53ec\u56de\u3001\u65e0\u5e7b\u89c9\u53d1\u73b0\u7684\u4f18\u52bf\uff0c\u4e3a\u9ad8\u98ce\u9669\u751f\u7269\u5236\u836f\u6295\u8d44\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.14587", "categories": ["cs.LG", "cs.AI", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.14587", "abs": "https://arxiv.org/abs/2602.14587", "authors": ["Minh Nguyen"], "title": "Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow", "comment": null, "summary": "Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u89e3\u8026\u7684\u8fde\u7eed\u65f6\u95f4\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ea4\u66ff\u66f4\u65b0\u89e3\u51b3\u6807\u51c6\u79bb\u6563\u65f6\u95f4RL\u5728\u8fde\u7eed\u65f6\u95f4\u63a7\u5236\u95ee\u9898\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5728\u8fde\u7eed\u63a7\u5236\u548c\u4ea4\u6613\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u63a7\u5236\u95ee\u9898\uff08\u91d1\u878d\u3001\u673a\u5668\u4eba\uff09\u5728\u8fde\u7eed\u65f6\u95f4\u4e2d\u6f14\u5316\uff0c\u5177\u6709\u975e\u5747\u5300\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u51b3\u7b56\u3002\u6807\u51c6\u79bb\u6563\u65f6\u95f4RL\u57fa\u4e8e\u56fa\u5b9a\u6b65\u957fBellman\u66f4\u65b0\uff0c\u5728\u8fd9\u79cd\u8bbe\u7f6e\u4e0b\u5b58\u5728\u56f0\u96be\uff1a\u5f53\u65f6\u95f4\u95f4\u9694\u7f29\u5c0f\u65f6\uff0cQ\u51fd\u6570\u4f1a\u574d\u7f29\u4e3a\u4ef7\u503c\u51fd\u6570V\uff0c\u6d88\u9664\u52a8\u4f5c\u6392\u5e8f\u3002\u73b0\u6709\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\u901a\u8fc7\u4f18\u52bf\u7387\u51fd\u6570q\u91cd\u65b0\u5f15\u5165\u52a8\u4f5c\u4fe1\u606f\uff0c\u4f46\u901a\u8fc7\u590d\u6742\u7684\u9785\u635f\u5931\u6216\u6b63\u4ea4\u7ea6\u675f\u5f3a\u5236\u6267\u884c\u6700\u4f18\u6027\uff0c\u5bf9\u6d4b\u8bd5\u8fc7\u7a0b\u9009\u62e9\u654f\u611f\uff0c\u5e76\u5c06V\u548cq\u7ea0\u7f20\u6210\u96be\u4ee5\u53ef\u9760\u8bad\u7ec3\u7684\u5927\u578b\u590d\u6742\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u89e3\u8026\u8fde\u7eed\u65f6\u95f4\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u91c7\u7528\u4ea4\u66ff\u66f4\u65b0\uff1aq\u4eceV\u7684\u6269\u6563\u751f\u6210\u5668\u5b66\u4e60\uff0cV\u901a\u8fc7\u57fa\u4e8e\u54c8\u5bc6\u987f\u91cf\u7684\u4ef7\u503c\u6d41\u66f4\u65b0\uff0c\u8be5\u4ef7\u503c\u6d41\u5728\u65e0\u7a77\u5c0f\u65f6\u95f4\u6b65\u4e0b\u4ecd\u4fdd\u6301\u4fe1\u606f\u6027\uff08\u6807\u51c6max/softmax\u5907\u4efd\u4f1a\u5931\u6548\uff09\u3002\u7406\u8bba\u4e0a\u901a\u8fc7\u65b0\u7684\u6982\u7387\u8bba\u8bc1\u8bc1\u660e\u4e25\u683c\u6536\u655b\uff0c\u7ed5\u8fc7\u4e86\u751f\u6210\u5668\u57fa\u54c8\u5bc6\u987f\u91cf\u5728sup-norm\u4e0b\u7f3a\u4e4fBellman\u5f0f\u6536\u7f29\u7684\u6311\u6218\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u4ea4\u6613\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u7684\u8fde\u7eed\u65f6\u95f4\u548c\u9886\u5148\u7684\u79bb\u6563\u65f6\u95f4\u57fa\u7ebf\uff0c\u5728\u5355\u4e2a\u5b63\u5ea6\u5185\u5b9e\u73b021%\u7684\u5229\u6da6\u2014\u2014\u51e0\u4e4e\u662f\u7b2c\u4e8c\u4f73\u65b9\u6cd5\u7684\u4e24\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u8fde\u7eed\u65f6\u95f4RL\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u89e3\u8026V\u548cq\u7684\u4f18\u5316\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7a33\u5b9a\u3001\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.15029", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15029", "abs": "https://arxiv.org/abs/2602.15029", "authors": ["Dhruva Karkada", "Daniel J. Korchinski", "Andres Nava", "Matthieu Wyart", "Yasaman Bahri"], "title": "Symmetry in language statistics shapes the geometry of model representations", "comment": null, "summary": "Although learned representations underlie neural networks' success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM representations: for example, calendar months organize into a circle, years form a smooth one-dimensional manifold, and cities' latitudes and longitudes can be decoded by a linear probe. We show that the statistics of language exhibit a translation symmetry -- e.g., the co-occurrence probability of two months depends only on the time interval between them -- and we prove that the latter governs the aforementioned geometric structures in high-dimensional word embedding models. Moreover, we find that these structures persist even when the co-occurrence statistics are strongly perturbed (for example, by removing all sentences in which two months appear together) and at moderate embedding dimension. We show that this robustness naturally emerges if the co-occurrence statistics are collectively controlled by an underlying continuous latent variable. We empirically validate this theoretical framework in word embedding models, text embedding models, and large language models.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u8bed\u8a00\u7edf\u8ba1\u4e2d\u7684\u5e73\u79fb\u5bf9\u79f0\u6027\u5bfc\u81f4LLM\u8868\u793a\u4e2d\u51fa\u73b0\u7b80\u5355\u51e0\u4f55\u7ed3\u6784\uff0c\u8fd9\u79cd\u7ed3\u6784\u5728\u7edf\u8ba1\u6270\u52a8\u4e0b\u4f9d\u7136\u7a33\u5065\uff0c\u6e90\u4e8e\u6f5c\u5728\u7684\u8fde\u7eed\u9690\u53d8\u91cf\u63a7\u5236\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u7684\u6210\u529f\u5df2\u88ab\u5e7f\u6cdb\u8ba4\u53ef\uff0c\u4f46\u5176\u57fa\u672c\u6027\u8d28\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002\u4e00\u4e2a\u663e\u8457\u73b0\u8c61\u662fLLM\u8868\u793a\u4e2d\u51fa\u73b0\u7684\u7b80\u5355\u51e0\u4f55\u7ed3\u6784\uff08\u5982\u6708\u4efd\u5f62\u6210\u5706\u5f62\u3001\u5e74\u4efd\u5f62\u6210\u4e00\u7ef4\u6d41\u5f62\uff09\uff0c\u9700\u8981\u89e3\u91ca\u8fd9\u4e9b\u7ed3\u6784\u4e3a\u4f55\u51fa\u73b0\u53ca\u5176\u7a33\u5065\u6027\u3002", "method": "\u9996\u5148\u8bc1\u660e\u8bed\u8a00\u7edf\u8ba1\u5177\u6709\u5e73\u79fb\u5bf9\u79f0\u6027\uff08\u5982\u540c\u73b0\u6982\u7387\u4ec5\u53d6\u51b3\u4e8e\u65f6\u95f4\u95f4\u9694\uff09\uff0c\u7136\u540e\u8bc1\u660e\u8fd9\u79cd\u5bf9\u79f0\u6027\u5bfc\u81f4\u9ad8\u7ef4\u8bcd\u5d4c\u5165\u6a21\u578b\u4e2d\u7684\u51e0\u4f55\u7ed3\u6784\u3002\u8fdb\u4e00\u6b65\u7814\u7a76\u5f53\u540c\u73b0\u7edf\u8ba1\u88ab\u5f3a\u70c8\u6270\u52a8\u65f6\u7684\u7a33\u5065\u6027\uff0c\u63d0\u51fa\u7531\u6f5c\u5728\u8fde\u7eed\u9690\u53d8\u91cf\u96c6\u4f53\u63a7\u5236\u7684\u89e3\u91ca\u6846\u67b6\u3002\u5728\u8bcd\u5d4c\u5165\u6a21\u578b\u3001\u6587\u672c\u5d4c\u5165\u6a21\u578b\u548cLLM\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5e73\u79fb\u5bf9\u79f0\u6027\u786e\u5b9e\u5bfc\u81f4\u89c2\u5bdf\u5230\u7684\u51e0\u4f55\u7ed3\u6784\u3002\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u7ed3\u6784\u5728\u540c\u73b0\u7edf\u8ba1\u88ab\u5f3a\u70c8\u6270\u52a8\u65f6\u4f9d\u7136\u4fdd\u6301\u7a33\u5065\uff0c\u751a\u81f3\u5728\u4e2d\u7b49\u5d4c\u5165\u7ef4\u5ea6\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u9a8c\u8bc1\u4e86\u6f5c\u5728\u8fde\u7eed\u9690\u53d8\u91cf\u63a7\u5236\u7684\u7406\u8bba\u6846\u67b6\u3002", "conclusion": "\u8bed\u8a00\u7edf\u8ba1\u7684\u5e73\u79fb\u5bf9\u79f0\u6027\u662fLLM\u8868\u793a\u4e2d\u51fa\u73b0\u7b80\u5355\u51e0\u4f55\u7ed3\u6784\u7684\u57fa\u7840\u539f\u56e0\uff0c\u8fd9\u4e9b\u7ed3\u6784\u7684\u7a33\u5065\u6027\u6e90\u4e8e\u6f5c\u5728\u8fde\u7eed\u9690\u53d8\u91cf\u7684\u96c6\u4f53\u63a7\u5236\uff0c\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u7684\u6027\u8d28\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.14844", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14844", "abs": "https://arxiv.org/abs/2602.14844", "authors": ["Elias Malomgr\u00e9", "Pieter Simoens"], "title": "Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment", "comment": "Accepted for the AAMAS 2026 Blue Sky Ideas track", "summary": "AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.", "AI": {"tldr": "\u63d0\u51faInteractionless Inverse Reinforcement Learning\u548cAlignment Flywheel\u65b9\u6cd5\uff0c\u89e3\u51b3AI\u5bf9\u9f50\u4e2d\u7684Alignment Waste\u95ee\u9898\uff0c\u5c06\u5b89\u5168\u76ee\u6807\u4e0e\u4ee3\u7406\u7b56\u7565\u89e3\u8026\uff0c\u521b\u5efa\u53ef\u68c0\u67e5\u3001\u53ef\u7f16\u8f91\u3001\u6a21\u578b\u65e0\u5173\u7684\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u5f53\u524dAI\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u7ed3\u6784\u6027\u7f3a\u9677\uff0c\u5c06\u5b89\u5168\u76ee\u6807\u4e0e\u4ee3\u7406\u7b56\u7565\u7ea0\u7f20\u5728\u4e00\u8d77\u3002RLHF\u548cDPO\u7b49\u65b9\u6cd5\u4ea7\u751f\u4e0d\u900f\u660e\u3001\u4e00\u6b21\u6027\u4f7f\u7528\u7684\u5bf9\u9f50\u4ea7\u7269\uff08Alignment Waste\uff09\uff0c\u9700\u8981\u66f4\u53ef\u6301\u7eed\u3001\u53ef\u9a8c\u8bc1\u7684\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. Interactionless Inverse Reinforcement Learning\uff1a\u5c06\u5bf9\u9f50\u4ea7\u7269\u5b66\u4e60\u4e0e\u7b56\u7565\u4f18\u5316\u89e3\u8026\uff0c\u751f\u6210\u53ef\u68c0\u67e5\u3001\u53ef\u7f16\u8f91\u3001\u6a21\u578b\u65e0\u5173\u7684\u5956\u52b1\u6a21\u578b\uff1b2. Alignment Flywheel\uff1a\u4eba\u673a\u4ea4\u4e92\u751f\u547d\u5468\u671f\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5ba1\u8ba1\u548c\u7cbe\u5316\u8fed\u4ee3\u5f3a\u5316\u5956\u52b1\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u5b89\u5168\u4ece\u4e00\u6b21\u6027\u6d88\u8017\u8f6c\u53d8\u4e3a\u6301\u4e45\u3001\u53ef\u9a8c\u8bc1\u7684\u5de5\u7a0b\u8d44\u4ea7\uff0c\u89e3\u51b3\u4e86Alignment Waste\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u3001\u53ef\u6301\u7eed\u7684AI\u5bf9\u9f50\u6846\u67b6\u3002", "conclusion": "\u63d0\u51fa\u7684\u67b6\u6784\u89e3\u51b3\u4e86\u5f53\u524dAI\u5bf9\u9f50\u65b9\u6cd5\u7684\u7ed3\u6784\u6027\u7f3a\u9677\uff0c\u901a\u8fc7\u89e3\u8026\u5bf9\u9f50\u4ea7\u7269\u5b66\u4e60\u548c\u7b56\u7565\u4f18\u5316\uff0c\u521b\u5efa\u4e86\u53ef\u68c0\u67e5\u3001\u53ef\u7f16\u8f91\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u5f3a\u5316\u8fc7\u7a0b\u5b9e\u73b0\u53ef\u6301\u7eed\u7684\u5b89\u5168\u5de5\u7a0b\u3002", "topic": "agent analysis"}}
{"id": "2602.14849", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14849", "abs": "https://arxiv.org/abs/2602.14849", "authors": ["Bardia Mohammadi", "Nearchos Potamitis", "Lars Klein", "Akhil Arora", "Laurent Bindschaedler"], "title": "Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows", "comment": null, "summary": "LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.", "AI": {"tldr": "Atomix\u4e3aLLM\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\u63d0\u4f9b\u8fdb\u5ea6\u611f\u77e5\u7684\u4e8b\u52a1\u8bed\u4e49\uff0c\u901a\u8fc7epoch\u6807\u8bb0\u3001\u8d44\u6e90\u524d\u6cbf\u8ddf\u8e2a\u548c\u8fdb\u5ea6\u8c13\u8bcd\u63d0\u4ea4\u673a\u5236\uff0c\u786e\u4fdd\u5931\u8d25\u3001\u63a8\u6d4b\u6216\u7ade\u4e89\u60c5\u51b5\u4e0b\u7684\u5b89\u5168\u56de\u6eda\u548c\u9694\u79bb\u6027\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u5916\u90e8\u7cfb\u7edf\u4e0a\u6267\u884c\u64cd\u4f5c\u65f6\uff0c\u5de5\u5177\u8c03\u7528\u4f1a\u7acb\u5373\u4ea7\u751f\u6548\u679c\u3002\u5f53\u53d1\u751f\u6545\u969c\u3001\u63a8\u6d4b\u6267\u884c\u6216\u8d44\u6e90\u7ade\u4e89\u65f6\uff0c\u5931\u8d25\u7684\u5206\u652f\u53ef\u80fd\u4f1a\u6cc4\u6f0f\u610f\u5916\u7684\u526f\u4f5c\u7528\uff0c\u4e14\u65e0\u6cd5\u5b89\u5168\u56de\u6eda\u3002\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u786e\u4fdd\u5de5\u5177\u8c03\u7528\u7684\u539f\u5b50\u6027\u548c\u9694\u79bb\u6027\u3002", "method": "Atomix\u8fd0\u884c\u65f6\u4e3a\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\u63d0\u4f9b\u8fdb\u5ea6\u611f\u77e5\u7684\u4e8b\u52a1\u8bed\u4e49\uff1a1) \u4e3a\u6bcf\u4e2a\u8c03\u7528\u6807\u8bb0epoch\uff1b2) \u8ddf\u8e2a\u6bcf\u4e2a\u8d44\u6e90\u7684\u524d\u6cbf\u72b6\u6001\uff1b3) \u4ec5\u5f53\u8fdb\u5ea6\u8c13\u8bcd\u6307\u793a\u5b89\u5168\u65f6\u624d\u63d0\u4ea4\uff1b4) \u53ef\u7f13\u51b2\u7684\u6548\u679c\u5ef6\u8fdf\u6267\u884c\uff0c\u5916\u90e8\u5316\u6548\u679c\u5728\u56de\u6eda\u65f6\u8fdb\u884c\u8865\u507f\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u8fdb\u884c\u6545\u969c\u6ce8\u5165\u6d4b\u8bd5\uff1a1) \u4e8b\u52a1\u91cd\u8bd5\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff1b2) \u524d\u6cbf\u95e8\u63a7\u63d0\u4ea4\u5728\u63a8\u6d4b\u6267\u884c\u548c\u7ade\u4e89\u60c5\u51b5\u4e0b\u589e\u5f3a\u4e86\u9694\u79bb\u6027\u3002", "conclusion": "Atomix\u901a\u8fc7\u8fdb\u5ea6\u611f\u77e5\u7684\u4e8b\u52a1\u8bed\u4e49\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u526f\u4f5c\u7528\u6cc4\u6f0f\u548c\u56de\u6eda\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u9694\u79bb\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.14872", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.14872", "abs": "https://arxiv.org/abs/2602.14872", "authors": ["Yu Huang", "Zixin Wen", "Yuejie Chi", "Yuting Wei", "Aarti Singh", "Yingbin Liang", "Yuxin Chen"], "title": "On the Learning Dynamics of RLVR at the Edge of Competence", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has been a main driver of recent breakthroughs in large reasoning models. Yet it remains a mystery how rewards based solely on final outcomes can help overcome the long-horizon barrier to extended reasoning. To understand this, we develop a theory of the training dynamics of RL for transformers on compositional reasoning tasks. Our theory characterizes how the effectiveness of RLVR is governed by the smoothness of the difficulty spectrum. When data contains abrupt discontinuities in difficulty, learning undergoes grokking-type phase transitions, producing prolonged plateaus before progress recurs. In contrast, a smooth difficulty spectrum leads to a relay effect: persistent gradient signals on easier problems elevate the model's capabilities to the point where harder ones become tractable, resulting in steady and continuous improvement. Our theory explains how RLVR can improve performance at the edge of competence, and suggests that appropriately designed data mixtures can yield scalable gains. As a technical contribution, our analysis develops and adapts tools from Fourier analysis on finite groups to our setting. We validate the predicted mechanisms empirically via synthetic experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u5982\u4f55\u5e2e\u52a9\u5927\u578b\u63a8\u7406\u6a21\u578b\u514b\u670d\u957f\u65f6\u7a0b\u63a8\u7406\u969c\u788d\uff0c\u5173\u952e\u5728\u4e8e\u6570\u636e\u96be\u5ea6\u8c31\u7684\u5e73\u6ed1\u6027\u51b3\u5b9a\u5b66\u4e60\u52a8\u6001\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u6700\u7ec8\u7ed3\u679c\u7684\u5956\u52b1\uff08RLVR\uff09\u5728\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4f46\u4eba\u4eec\u4ecd\u4e0d\u6e05\u695a\u8fd9\u79cd\u4ec5\u4f9d\u8d56\u6700\u7ec8\u7ed3\u679c\u7684\u5956\u52b1\u5982\u4f55\u5e2e\u52a9\u514b\u670d\u957f\u65f6\u7a0b\u63a8\u7406\u969c\u788d\u3002\u672c\u6587\u65e8\u5728\u7406\u89e3RLVR\u5728\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u52a8\u6001\u673a\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5173\u4e8etransformer\u5728\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u4e0a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u52a8\u6001\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4f7f\u7528\u6709\u9650\u7fa4\u4e0a\u7684\u5085\u91cc\u53f6\u5206\u6790\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u9884\u6d4b\u673a\u5236\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff1a\u5f53\u6570\u636e\u96be\u5ea6\u8c31\u5b58\u5728\u6025\u5267\u4e0d\u8fde\u7eed\u6027\u65f6\uff0c\u5b66\u4e60\u4f1a\u51fa\u73b0grokking\u578b\u76f8\u53d8\uff0c\u4ea7\u751f\u957f\u65f6\u95f4\u5e73\u53f0\u671f\uff1b\u800c\u5e73\u6ed1\u7684\u96be\u5ea6\u8c31\u4f1a\u4ea7\u751f\u63a5\u529b\u6548\u5e94\uff0c\u4f7f\u6a21\u578b\u80fd\u529b\u7a33\u6b65\u63d0\u5347\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u9884\u6d4b\u673a\u5236\u3002", "conclusion": "RLVR\u901a\u8fc7\u68af\u5ea6\u4fe1\u53f7\u5728\u80fd\u529b\u8fb9\u7f18\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u9002\u5f53\u8bbe\u8ba1\u7684\u6570\u636e\u6df7\u5408\u53ef\u4ee5\u4ea7\u751f\u53ef\u6269\u5c55\u7684\u6536\u76ca\u3002\u96be\u5ea6\u8c31\u7684\u5e73\u6ed1\u6027\u662f\u51b3\u5b9a\u5b66\u4e60\u52a8\u6001\u7684\u5173\u952e\u56e0\u7d20\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.14901", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.14901", "abs": "https://arxiv.org/abs/2602.14901", "authors": ["Pramit Saha", "Joshua Strong", "Mohammad Alsharid", "Divyanshu Mishra", "J. Alison Noble"], "title": "Picking the Right Specialist: Attentive Neural Process-based Selection of Task-Specialized Models as Tools for Agentic Healthcare Systems", "comment": null, "summary": "Task-specialized models form the backbone of agentic healthcare systems, enabling the agents to answer clinical queries across tasks such as disease diagnosis, localization, and report generation. Yet, for a given task, a single \"best\" model rarely exists. In practice, each task is better served by multiple competing specialist models where different models excel on different data samples. As a result, for any given query, agents must reliably select the right specialist model from a heterogeneous pool of tool candidates. To this end, we introduce ToolSelect, which adaptively learns model selection for tools by minimizing a population risk over sampled specialist tool candidates using a consistent surrogate of the task-conditional selection loss. Concretely, we propose an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among the specialist models. Motivated by the absence of any established testbed, we, for the first time, introduce an agentic Chest X-ray environment equipped with a diverse suite of task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA) and develop ToolSelectBench, a benchmark of 1448 queries. Our results demonstrate that ToolSelect consistently outperforms 10 SOTA methods across four different task families.", "AI": {"tldr": "ToolSelect\uff1a\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u795e\u7ecf\u8fc7\u7a0b\u7684\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u7597\u4ee3\u7406\u7cfb\u7edf\u4e2d\u4ece\u5f02\u6784\u4e13\u5bb6\u6a21\u578b\u6c60\u4e2d\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f73\u4e13\u5bb6\u6a21\u578b\uff0c\u5728\u80f8\u90e8X\u5149\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e10\u79cdSOTA\u65b9\u6cd5\u3002", "motivation": "\u5728\u533b\u7597\u4ee3\u7406\u7cfb\u7edf\u4e2d\uff0c\u5355\u4e2a\u4efb\u52a1\u901a\u5e38\u6ca1\u6709\u552f\u4e00\u7684\"\u6700\u4f73\"\u6a21\u578b\uff0c\u800c\u662f\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u6837\u672c\u4e0a\u8868\u73b0\u5404\u5f02\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u9760\u7684\u6a21\u578b\u9009\u62e9\u673a\u5236\uff0c\u65e0\u6cd5\u4ece\u5f02\u6784\u4e13\u5bb6\u6a21\u578b\u6c60\u4e2d\u4e3a\u7ed9\u5b9a\u67e5\u8be2\u9009\u62e9\u6700\u5408\u9002\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faToolSelect\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u6ce8\u610f\u529b\u795e\u7ecf\u8fc7\u7a0b\u6784\u5efa\u9009\u62e9\u5668\uff0c\u4ee5\u67e5\u8be2\u548c\u6bcf\u4e2a\u6a21\u578b\u7684\u884c\u4e3a\u6458\u8981\u4e3a\u6761\u4ef6\uff1b2\uff09\u901a\u8fc7\u6700\u5c0f\u5316\u4efb\u52a1\u6761\u4ef6\u9009\u62e9\u635f\u5931\u7684\u4e00\u81f4\u66ff\u4ee3\u6765\u5b66\u4e60\u6a21\u578b\u9009\u62e9\uff1b3\uff09\u9996\u6b21\u6784\u5efa\u80f8\u90e8X\u5149\u4ee3\u7406\u73af\u5883ToolSelectBench\uff0c\u5305\u542b17\u4e2a\u75be\u75c5\u68c0\u6d4b\u300119\u4e2a\u62a5\u544a\u751f\u6210\u30016\u4e2a\u89c6\u89c9\u5b9a\u4f4d\u548c13\u4e2aVQA\u6a21\u578b\uff0c\u51711448\u4e2a\u67e5\u8be2\u3002", "result": "ToolSelect\u5728\u56db\u4e2a\u4e0d\u540c\u4efb\u52a1\u5bb6\u65cf\u4e0a\u4e00\u81f4\u4f18\u4e8e10\u79cdSOTA\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5f02\u6784\u4e13\u5bb6\u6a21\u578b\u6c60\u4e2d\u9009\u62e9\u6700\u4f73\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "ToolSelect\u4e3a\u533b\u7597\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u9009\u62e9\u673a\u5236\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u4ece\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\u4e2d\u9009\u62e9\u6700\u9002\u5408\u7279\u5b9a\u67e5\u8be2\u7684\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u6a21\u578b\u9009\u62e9\u7684\u5173\u952e\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.112bd54d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-ai-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/2/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/tj7yq1c_ynw5EzKxuClhc3loXnvBsw5iUQ67m30-YOA=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-ai-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/2/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/tj7yq1c_ynw5EzKxuClhc3loXnvBsw5iUQ67m30-YOA=444", "authors": ["TLDR Newsletter"], "title": "GitHub Agentic Workflows - Repository automation, running the coding agents you know and love, with strong guardrails and security-first design principles", "comment": "Source: TLDR Newsletter, Date: 2026-02-13, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-ai-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/2/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/tj7yq1c_ynw5EzKxuClhc3loXnvBsw5iUQ67m30-YOA=444", "summary": "GitHub Agentic Workflows - Repository automation, running the coding agents you know and love, with strong guardrails and security-first design principles (Sponsor) \u2600\ufe0f Imagine waking up to your issues triaged, your CI failures investigated with fixes for you to review, and two fresh PRs proposing improvements to your tests. All of that while you were sleeping.Give yourself a headstart, every day. Create your first agentic workflow today.", "source": "tldr", "AI": {"tldr": "GitHub Agentic Workflows\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u4ed3\u5e93\u7ba1\u7406\u5de5\u5177\uff0c\u4f7f\u7528AI\u4ee3\u7406\u81ea\u52a8\u5904\u7406\u95ee\u9898\u5206\u7c7b\u3001CI\u6545\u969c\u8c03\u67e5\u4fee\u590d\u3001\u6d4b\u8bd5\u6539\u8fdb\u7b49\u4efb\u52a1\uff0c\u5177\u6709\u5f3a\u5b89\u5168\u9632\u62a4\u8bbe\u8ba1\u3002", "motivation": "\u89e3\u51b3\u5f00\u53d1\u8005\u65e5\u5e38\u91cd\u590d\u6027\u5de5\u4f5c\u8d1f\u62c5\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u4e13\u6ce8\u4e8e\u6838\u5fc3\u5f00\u53d1\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4ee3\u7406\u5728\u5f00\u53d1\u8005\u4f11\u606f\u65f6\u5904\u7406\u4ed3\u5e93\u7ef4\u62a4\u5de5\u4f5c\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "method": "\u91c7\u7528\u5b89\u5168\u4f18\u5148\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u6784\u5efa\u5177\u6709\u5f3a\u9632\u62a4\u673a\u5236\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7cfb\u7edf\uff0c\u96c6\u6210\u73b0\u6709\u7684\u7f16\u7801\u4ee3\u7406\u6280\u672f\uff0c\u5b9e\u73b0\u95ee\u9898\u81ea\u52a8\u5206\u7c7b\u3001CI\u6545\u969c\u8c03\u67e5\u4e0e\u4fee\u590d\u3001\u6d4b\u8bd5\u6539\u8fdb\u5efa\u8bae\u7b49\u81ea\u52a8\u5316\u529f\u80fd\u3002", "result": "\u5f00\u53d1\u8005\u53ef\u4ee5\u6bcf\u5929\u9192\u6765\u5c31\u770b\u5230\u95ee\u9898\u5df2\u5206\u7c7b\u3001CI\u6545\u969c\u5df2\u8c03\u67e5\u5e76\u63d0\u4f9b\u4e86\u4fee\u590d\u65b9\u6848\u3001\u6d4b\u8bd5\u6539\u8fdb\u7684PR\u5df2\u51c6\u5907\u597d\uff0c\u663e\u8457\u51cf\u5c11\u624b\u52a8\u7ef4\u62a4\u65f6\u95f4\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "conclusion": "GitHub Agentic Workflows\u901a\u8fc7\u81ea\u52a8\u5316\u4ee3\u7406\u5de5\u4f5c\u6d41\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u8ba9\u5f00\u53d1\u8005\u6bcf\u5929\u90fd\u80fd\u83b7\u5f97\u5148\u53d1\u4f18\u52bf\uff0c\u4e13\u6ce8\u4e8e\u66f4\u6709\u4ef7\u503c\u7684\u5f00\u53d1\u5de5\u4f5c\u3002", "topic": "swe application"}}
{"id": "tldr.2602.fb01dcb9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-ai-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/2/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/tj7yq1c_ynw5EzKxuClhc3loXnvBsw5iUQ67m30-YOA=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-ai-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/2/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/tj7yq1c_ynw5EzKxuClhc3loXnvBsw5iUQ67m30-YOA=444", "authors": ["TLDR Newsletter"], "title": "Create your first agentic workflow today", "comment": "Source: TLDR Newsletter, Date: 2026-02-13, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-ai-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/2/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/tj7yq1c_ynw5EzKxuClhc3loXnvBsw5iUQ67m30-YOA=444", "summary": "GitHub Agentic Workflows - Repository automation, running the coding agents you know and love, with strong guardrails and security-first design principles (Sponsor) \u2600\ufe0f Imagine waking up to your issues triaged, your CI failures investigated with fixes for you to review, and two fresh PRs proposing improvements to your tests. All of that while you were sleeping.Give yourself a headstart, every day. Create your first agentic workflow today.", "source": "tldr", "AI": {"tldr": "GitHub\u63a8\u51fa\u4ee3\u7406\u5de5\u4f5c\u6d41\u5e73\u53f0\uff0c\u901a\u8fc7AI\u4ee3\u7406\u81ea\u52a8\u5316\u4ee3\u7801\u4ed3\u5e93\u7ba1\u7406\u4efb\u52a1\uff0c\u5305\u62ec\u95ee\u9898\u5206\u7c7b\u3001CI\u5931\u8d25\u8c03\u67e5\u4fee\u590d\u3001\u6d4b\u8bd5\u6539\u8fdb\u7b49\uff0c\u63d0\u4f9b\u5b89\u5168\u62a4\u680f\u548c\u4f18\u5148\u5b89\u5168\u8bbe\u8ba1", "motivation": "\u89e3\u51b3\u5f00\u53d1\u8005\u65e5\u5e38\u91cd\u590d\u6027\u5de5\u4f5c\u8d1f\u62c5\uff0c\u8ba9\u5f00\u53d1\u8005\u5728\u4f11\u606f\u65f6\u4e5f\u80fd\u81ea\u52a8\u5904\u7406\u4ed3\u5e93\u7ef4\u62a4\u4efb\u52a1\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf", "method": "\u57fa\u4e8eGitHub\u5e73\u53f0\u6784\u5efa\u4ee3\u7406\u5de5\u4f5c\u6d41\u7cfb\u7edf\uff0c\u96c6\u6210\u73b0\u6709AI\u7f16\u7801\u4ee3\u7406\uff0c\u91c7\u7528\u5b89\u5168\u4f18\u5148\u8bbe\u8ba1\u539f\u5219\u548c\u5f3a\u62a4\u680f\u673a\u5236", "result": "\u80fd\u591f\u5b9e\u73b0\u95ee\u9898\u81ea\u52a8\u5206\u7c7b\u3001CI\u5931\u8d25\u81ea\u52a8\u8c03\u67e5\u4fee\u590d\u3001\u6d4b\u8bd5\u6539\u8fdb\u5efa\u8bae\u7b49\u81ea\u52a8\u5316\u4efb\u52a1\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6bcf\u65e5\u5de5\u4f5c\u5148\u53d1\u4f18\u52bf", "conclusion": "GitHub\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u81ea\u52a8\u5316\u4ed3\u5e93\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7AI\u4ee3\u7406\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027", "topic": "swe application"}}
{"id": "tldr.2602.6c6d4ddb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2Fopenenv-turing%3Futm_source=tldrai/1/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/W14gs6ri5e_cX2AnXTetgOb1VNEEj9Srh5bIRvmP9Hc=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2Fopenenv-turing%3Futm_source=tldrai/1/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/W14gs6ri5e_cX2AnXTetgOb1VNEEj9Srh5bIRvmP9Hc=444", "authors": ["TLDR Newsletter"], "title": "Real-World Agent Evaluation", "comment": "Source: TLDR Newsletter, Date: 2026-02-13, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2Fopenenv-turing%3Futm_source=tldrai/1/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/W14gs6ri5e_cX2AnXTetgOb1VNEEj9Srh5bIRvmP9Hc=444", "summary": "Real-World Agent Evaluation (11 minute read) OpenEnv is an open-source framework from Meta and Hugging Face that standardizes how agents interact with real, stateful environments via a gym-style API and MCP tool interface. A production-grade calendar environment illustrated the challenges of evaluating tool-using agents under realistic constraints such as access control and long-horizon reasoning.", "source": "tldr", "AI": {"tldr": "OpenEnv\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u901a\u8fc7gym\u98ce\u683cAPI\u548cMCP\u5de5\u5177\u63a5\u53e3\u6807\u51c6\u5316\u667a\u80fd\u4f53\u4e0e\u771f\u5b9e\u6709\u72b6\u6001\u73af\u5883\u7684\u4ea4\u4e92\uff0c\u4f7f\u7528\u751f\u4ea7\u7ea7\u65e5\u5386\u73af\u5883\u5c55\u793a\u4e86\u5728\u8bbf\u95ee\u63a7\u5236\u548c\u957f\u65f6\u7a0b\u63a8\u7406\u7b49\u73b0\u5b9e\u7ea6\u675f\u4e0b\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u8bc4\u4f30\u901a\u5e38\u7f3a\u4e4f\u4e0e\u771f\u5b9e\u3001\u6709\u72b6\u6001\u73af\u5883\u7684\u4ea4\u4e92\uff0c\u96be\u4ee5\u8bc4\u4f30\u5728\u73b0\u5b9e\u7ea6\u675f\uff08\u5982\u8bbf\u95ee\u63a7\u5236\u3001\u957f\u65f6\u7a0b\u63a8\u7406\uff09\u4e0b\u7684\u6027\u80fd\uff0c\u9700\u8981\u6807\u51c6\u5316\u6846\u67b6\u6765\u4fc3\u8fdb\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1OpenEnv\u5f00\u6e90\u6846\u67b6\uff0c\u91c7\u7528gym\u98ce\u683cAPI\u548cMCP\u5de5\u5177\u63a5\u53e3\u6807\u51c6\u5316\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u6784\u5efa\u751f\u4ea7\u7ea7\u65e5\u5386\u73af\u5883\u4f5c\u4e3a\u793a\u4f8b\uff0c\u5c55\u793a\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u7684\u8bc4\u4f30\u6311\u6218\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u6807\u51c6\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u65e5\u5386\u73af\u5883\u6848\u4f8b\u5c55\u793a\u4e86\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u5728\u8bbf\u95ee\u63a7\u5236\u3001\u72b6\u6001\u7ba1\u7406\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u63a8\u7406\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "OpenEnv\u4e3a\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u7840\u8bbe\u65bd\uff0c\u63ed\u793a\u4e86\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.22f4acdc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.10177%3Futm_source=tldrai/1/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/U0ieRsyKL9ooccz2IE71186_0ptEJstRkPILWpBKUGc=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.10177%3Futm_source=tldrai/1/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/U0ieRsyKL9ooccz2IE71186_0ptEJstRkPILWpBKUGc=444", "authors": ["TLDR Newsletter"], "title": "Math Research Agent from DeepMind", "comment": "Source: TLDR Newsletter, Date: 2026-02-13, Reading time: 34 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2602.10177%3Futm_source=tldrai/1/0100019c575d9686-42869d4b-e21d-4380-9f3f-1ab4aa4226c0-000000/U0ieRsyKL9ooccz2IE71186_0ptEJstRkPILWpBKUGc=444", "summary": "Math Research Agent from DeepMind (34 minute read) DeepMind introduced Aletheia, a math research agent powered by an advanced version of Gemini Deep Think that iteratively generates and verifies long-horizon proofs using intensive tool support.", "source": "tldr", "AI": {"tldr": "DeepMind\u5f00\u53d1\u4e86Aletheia\u6570\u5b66\u7814\u7a76\u667a\u80fd\u4f53\uff0c\u57fa\u4e8e\u589e\u5f3a\u7248Gemini Deep Think\uff0c\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u548c\u9a8c\u8bc1\u957f\u7a0b\u8bc1\u660e\uff0c\u5e76\u5229\u7528\u5de5\u5177\u652f\u6301\u8fdb\u884c\u6570\u5b66\u7814\u7a76\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u81ea\u4e3b\u8fdb\u884c\u6570\u5b66\u7814\u7a76\u7684AI\u7cfb\u7edf\uff0c\u89e3\u51b3\u6570\u5b66\u8bc1\u660e\u751f\u6210\u548c\u9a8c\u8bc1\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5904\u7406\u957f\u7a0b\u3001\u590d\u6742\u7684\u6570\u5b66\u8bc1\u660e\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u589e\u5f3a\u7248Gemini Deep Think\u4f5c\u4e3a\u6838\u5fc3\u6a21\u578b\uff0c\u91c7\u7528\u8fed\u4ee3\u5f0f\u8bc1\u660e\u751f\u6210\u548c\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u79cd\u5de5\u5177\u652f\u6301\uff08\u5982\u5b9a\u7406\u8bc1\u660e\u5668\u3001\u7b26\u53f7\u8ba1\u7b97\u7b49\uff09\u6765\u8f85\u52a9\u6570\u5b66\u7814\u7a76\u8fc7\u7a0b\u3002", "result": "\u5f00\u53d1\u51faAletheia\u6570\u5b66\u7814\u7a76\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u5904\u7406\u957f\u7a0b\u6570\u5b66\u8bc1\u660e\uff0c\u901a\u8fc7\u5de5\u5177\u652f\u6301\u63d0\u9ad8\u8bc1\u660e\u751f\u6210\u548c\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "Aletheia\u4ee3\u8868\u4e86\u6570\u5b66\u7814\u7a76AI\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u5de5\u5177\u7ed3\u5408\u5728\u590d\u6742\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2602.ed4900e0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.natemeyvis.com%2Fon-cognitive-debt%2F%3Futm_source=tldrnewsletter/1/0100019c66335a94-028c14eb-82c0-4dab-82c4-270158ceb4fd-000000/c-spkPAZ1MitLatuwMo9xzzfGeucgVKVU181rubJ6nU=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.natemeyvis.com%2Fon-cognitive-debt%2F%3Futm_source=tldrnewsletter/1/0100019c66335a94-028c14eb-82c0-4dab-82c4-270158ceb4fd-000000/c-spkPAZ1MitLatuwMo9xzzfGeucgVKVU181rubJ6nU=444", "authors": ["TLDR Newsletter"], "title": "On cognitive debt", "comment": "Source: TLDR Newsletter, Date: 2026-02-16, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.natemeyvis.com%2Fon-cognitive-debt%2F%3Futm_source=tldrnewsletter/1/0100019c66335a94-028c14eb-82c0-4dab-82c4-270158ceb4fd-000000/c-spkPAZ1MitLatuwMo9xzzfGeucgVKVU181rubJ6nU=444", "summary": "On cognitive debt (6 minute read) Cognitive debt is the idea that AI-generated codebases are at risk of falling into a state where nobody knows how they work, leaving them inextensible, unobservable, and hard to debug. This is already happening in AI-driven codebases. However, when controlling for the size and scope of a project, cognitive debt tends to be at least as bad, if not worse, in pre-AI code bases than in AI code bases. This fact is obscured because a lot of traditional engineering ...", "source": "tldr", "AI": {"tldr": "AI\u751f\u6210\u7684\u4ee3\u7801\u5e93\u5b58\u5728\"\u8ba4\u77e5\u503a\u52a1\"\u98ce\u9669\uff0c\u5373\u65e0\u4eba\u7406\u89e3\u5176\u5de5\u4f5c\u539f\u7406\uff0c\u5bfc\u81f4\u96be\u4ee5\u6269\u5c55\u3001\u89c2\u5bdf\u548c\u8c03\u8bd5\u3002\u4f46\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u63a7\u5236\u9879\u76ee\u89c4\u6a21\u548c\u8303\u56f4\u540e\uff0c\u4f20\u7edf\u4ee3\u7801\u5e93\u7684\u8ba4\u77e5\u503a\u52a1\u95ee\u9898\u81f3\u5c11\u4e0eAI\u4ee3\u7801\u5e93\u540c\u6837\u4e25\u91cd\u751a\u81f3\u66f4\u7cdf\u3002", "motivation": "\u63a2\u8ba8AI\u751f\u6210\u4ee3\u7801\u5e26\u6765\u7684\"\u8ba4\u77e5\u503a\u52a1\"\u95ee\u9898\uff0c\u5373\u4ee3\u7801\u5e93\u53d8\u5f97\u65e0\u4eba\u7406\u89e3\u3001\u96be\u4ee5\u7ef4\u62a4\u7684\u98ce\u9669\uff0c\u5e76\u6bd4\u8f83AI\u4ee3\u7801\u4e0e\u4f20\u7edf\u4ee3\u7801\u5728\u8fd9\u4e00\u95ee\u9898\u4e0a\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5206\u6790AI\u9a71\u52a8\u4ee3\u7801\u5e93\u7684\u73b0\u72b6\uff0c\u5e76\u4e0e\u4f20\u7edf\u4ee3\u7801\u5e93\u8fdb\u884c\u6bd4\u8f83\u7814\u7a76\uff0c\u63a7\u5236\u9879\u76ee\u89c4\u6a21\u548c\u8303\u56f4\u7b49\u53d8\u91cf\uff0c\u8bc4\u4f30\u8ba4\u77e5\u503a\u52a1\u7684\u7a0b\u5ea6\u3002", "result": "AI\u4ee3\u7801\u5e93\u786e\u5b9e\u5b58\u5728\u8ba4\u77e5\u503a\u52a1\u95ee\u9898\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u4f20\u7edf\u4ee3\u7801\u5e93\u7684\u8ba4\u77e5\u503a\u52a1\u95ee\u9898\u81f3\u5c11\u540c\u6837\u4e25\u91cd\u751a\u81f3\u66f4\u7cdf\uff0c\u8fd9\u4e00\u4e8b\u5b9e\u88ab\u4f20\u7edf\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u7684\u5176\u4ed6\u56e0\u7d20\u6240\u63a9\u76d6\u3002", "conclusion": "\u8ba4\u77e5\u503a\u52a1\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u666e\u904d\u95ee\u9898\uff0c\u4e0d\u4ec5\u9650\u4e8eAI\u751f\u6210\u4ee3\u7801\u3002\u867d\u7136AI\u4ee3\u7801\u5e93\u9762\u4e34\u8fd9\u4e00\u98ce\u9669\uff0c\u4f46\u4f20\u7edf\u4ee3\u7801\u5e93\u540c\u6837\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u5de5\u7a0b\u5b9e\u8df5\u6765\u5e94\u5bf9\u3002", "topic": "swe application"}}
{"id": "tldr.2602.10057ead", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fklawsh%2Fklaw.sh%3Futm_source=tldrnewsletter/1/0100019c66335a94-028c14eb-82c0-4dab-82c4-270158ceb4fd-000000/Mathxq-I0r3vN7xTxuYxtwZhsQhIC9g1ciwvi3VS4Vw=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fklawsh%2Fklaw.sh%3Futm_source=tldrnewsletter/1/0100019c66335a94-028c14eb-82c0-4dab-82c4-270158ceb4fd-000000/Mathxq-I0r3vN7xTxuYxtwZhsQhIC9g1ciwvi3VS4Vw=444", "authors": ["TLDR Newsletter"], "title": "klaw", "comment": "Source: TLDR Newsletter, Date: 2026-02-16, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fklawsh%2Fklaw.sh%3Futm_source=tldrnewsletter/1/0100019c66335a94-028c14eb-82c0-4dab-82c4-270158ceb4fd-000000/Mathxq-I0r3vN7xTxuYxtwZhsQhIC9g1ciwvi3VS4Vw=444", "summary": "klaw (GitHub Repo) klaw is a tool for enterprise AI agent orchestration that allows users to manage, monitor, and scale their AI workforce to hundreds of agents.", "source": "tldr", "AI": {"tldr": "klaw\u662f\u4e00\u4e2a\u4f01\u4e1a\u7ea7AI\u667a\u80fd\u4f53\u7f16\u6392\u5de5\u5177\uff0c\u7528\u4e8e\u7ba1\u7406\u3001\u76d1\u63a7\u548c\u6269\u5c55AI\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u81f3\u6570\u767e\u4e2a\u667a\u80fd\u4f53\u89c4\u6a21", "motivation": "\u968f\u7740\u4f01\u4e1aAI\u667a\u80fd\u4f53\u5e94\u7528\u7684\u589e\u52a0\uff0c\u9700\u8981\u6709\u6548\u7ba1\u7406\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u90e8\u7f72\uff0c\u5305\u62ec\u76d1\u63a7\u3001\u534f\u8c03\u548c\u6269\u5c55", "method": "\u5f00\u53d1\u4f01\u4e1a\u7ea7AI\u667a\u80fd\u4f53\u7f16\u6392\u5de5\u5177\uff0c\u63d0\u4f9b\u667a\u80fd\u4f53\u7ba1\u7406\u3001\u76d1\u63a7\u548c\u6269\u5c55\u529f\u80fd", "result": "\u5b9e\u73b0\u4e86\u80fd\u591f\u652f\u6301\u6570\u767e\u4e2aAI\u667a\u80fd\u4f53\u89c4\u6a21\u7684\u4f01\u4e1a\u7ea7\u7f16\u6392\u7cfb\u7edf", "conclusion": "klaw\u5de5\u5177\u89e3\u51b3\u4e86\u4f01\u4e1a\u5927\u89c4\u6a21AI\u667a\u80fd\u4f53\u90e8\u7f72\u7684\u7ba1\u7406\u6311\u6218", "topic": "agent analysis"}}
{"id": "tldr.2602.088711ce", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI-feb-16%26utm_content=/2/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/sHlGH6q2FlkyrTjeH2SaZhQhaE4a8UlOaaOUfmwryzo=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI-feb-16%26utm_content=/2/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/sHlGH6q2FlkyrTjeH2SaZhQhaE4a8UlOaaOUfmwryzo=444", "authors": ["TLDR Newsletter"], "title": "64.4% of product roadmaps already include agentic AI. Developers aren't sure what it actually is", "comment": "Source: TLDR Newsletter, Date: 2026-02-16, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI-feb-16%26utm_content=/2/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/sHlGH6q2FlkyrTjeH2SaZhQhaE4a8UlOaaOUfmwryzo=444", "summary": "64.4% of product roadmaps already include agentic AI. Developers aren't sure what it actually is (Sponsor) The discussion around agentic AI is no longer \"if\" but \"how\" - as teams grapple with decisions around autonomy, oversight, and scope. Nylas asked 1,000+ devs and product leaders why agentic AI is so roadmap-friendly, and the results are surprising: 85% say agentic AI will be table stakes within three years. 67% of teams are already building or shipping agentic workflows - although there'...", "source": "tldr", "AI": {"tldr": "64.4%\u7684\u4ea7\u54c1\u8def\u7ebf\u56fe\u5df2\u5305\u542b\u667a\u80fd\u4f53AI\uff0c\u4f46\u5f00\u53d1\u8005\u5bf9\u5176\u5b9e\u9645\u542b\u4e49\u4e0d\u786e\u5b9a\u3002\u8c03\u67e5\u663e\u793a85%\u8ba4\u4e3a\u4e09\u5e74\u5185\u667a\u80fd\u4f53AI\u5c06\u6210\u4e3a\u6807\u914d\uff0c67%\u7684\u56e2\u961f\u5df2\u5728\u6784\u5efa\u6216\u90e8\u7f72\u76f8\u5173\u5de5\u4f5c\u6d41\u3002", "motivation": "\u667a\u80fd\u4f53AI\u5df2\u6210\u4e3a\u4ea7\u54c1\u8def\u7ebf\u56fe\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u5f00\u53d1\u8005\u5bf9\u5176\u5b9a\u4e49\u3001\u81ea\u4e3b\u6027\u3001\u76d1\u7763\u8303\u56f4\u548c\u5b9e\u9645\u5e94\u7528\u5b58\u5728\u56f0\u60d1\u3002\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u667a\u80fd\u4f53AI\u5728\u5f00\u53d1\u793e\u533a\u4e2d\u7684\u8ba4\u77e5\u73b0\u72b6\u3001\u91c7\u7528\u60c5\u51b5\u548c\u672a\u6765\u9884\u671f\u3002", "method": "\u901a\u8fc7\u5bf91000\u591a\u540d\u5f00\u53d1\u8005\u548c\u4ea7\u54c1\u9886\u5bfc\u8005\u8fdb\u884c\u8c03\u67e5\uff0c\u6536\u96c6\u5173\u4e8e\u667a\u80fd\u4f53AI\u5728\u8def\u7ebf\u56fe\u4e2d\u7684\u73b0\u72b6\u3001\u56e2\u961f\u91c7\u7528\u60c5\u51b5\u3001\u5f00\u53d1\u6311\u6218\u548c\u672a\u6765\u9884\u671f\u7684\u6570\u636e\u3002", "result": "64.4%\u7684\u4ea7\u54c1\u8def\u7ebf\u56fe\u5df2\u5305\u542b\u667a\u80fd\u4f53AI\uff1b85%\u7684\u53d7\u8bbf\u8005\u8ba4\u4e3a\u4e09\u5e74\u5185\u667a\u80fd\u4f53AI\u5c06\u6210\u4e3a\u6807\u914d\uff1b67%\u7684\u56e2\u961f\u5df2\u5728\u6784\u5efa\u6216\u90e8\u7f72\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff1b\u5f00\u53d1\u8005\u5bf9\u667a\u80fd\u4f53AI\u7684\u5b9e\u9645\u542b\u4e49\u548c\u6700\u4f73\u5b9e\u8df5\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u667a\u80fd\u4f53AI\u5df2\u4ece\"\u662f\u5426\u91c7\u7528\"\u8f6c\u5411\"\u5982\u4f55\u5b9e\u65bd\"\u7684\u9636\u6bb5\uff0c\u6210\u4e3a\u4ea7\u54c1\u5f00\u53d1\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u867d\u7136\u91c7\u7528\u7387\u5f88\u9ad8\uff0c\u4f46\u884c\u4e1a\u9700\u8981\u66f4\u6e05\u6670\u7684\u5b9a\u4e49\u3001\u6700\u4f73\u5b9e\u8df5\u548c\u5de5\u5177\u6765\u652f\u6301\u5f00\u53d1\u8005\u6709\u6548\u5b9e\u65bd\u667a\u80fd\u4f53AI\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.582b10b9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplatformengineering.org%2Fblog%2Fthe-rise-of-agentic-platforms-scaling-beyond-automation%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/KXOudM4FM5XyUt11q6flcdYvR2nm_k-JFsPhGuKy3W0=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplatformengineering.org%2Fblog%2Fthe-rise-of-agentic-platforms-scaling-beyond-automation%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/KXOudM4FM5XyUt11q6flcdYvR2nm_k-JFsPhGuKy3W0=444", "authors": ["TLDR Newsletter"], "title": "The rise of agentic platforms: Scaling beyond automation", "comment": "Source: TLDR Newsletter, Date: 2026-02-16, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplatformengineering.org%2Fblog%2Fthe-rise-of-agentic-platforms-scaling-beyond-automation%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/KXOudM4FM5XyUt11q6flcdYvR2nm_k-JFsPhGuKy3W0=444", "summary": "The rise of agentic platforms: Scaling beyond automation (8 minute read) Agentic Platform Engineering, which integrates goal-driven, context-aware AI agents to reason, decide, and execute actions within defined constraints, is emerging as the next phase in platform evolution. This approach aims to achieve bounded autonomy, enabling platforms to handle complex operational tasks while shifting human roles to oversight and governance.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4ee3\u7406\u5e73\u53f0\u5de5\u7a0b\u4f5c\u4e3a\u5e73\u53f0\u6f14\u8fdb\u7684\u4e0b\u4e00\u9636\u6bb5\uff0c\u901a\u8fc7\u96c6\u6210\u76ee\u6807\u9a71\u52a8\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684AI\u4ee3\u7406\u6765\u5b9e\u73b0\u6709\u754c\u81ea\u4e3b\u6027\uff0c\u5c06\u4eba\u7c7b\u89d2\u8272\u8f6c\u5411\u76d1\u7763\u548c\u6cbb\u7406\u3002", "motivation": "\u5f53\u524d\u5e73\u53f0\u81ea\u52a8\u5316\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u7cfb\u7edf\u6765\u5904\u7406\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u4eba\u7c7b\u76d1\u7763\uff0c\u63a8\u52a8\u5e73\u53f0\u4ece\u7b80\u5355\u81ea\u52a8\u5316\u5411\u6709\u754c\u81ea\u4e3b\u6027\u6f14\u8fdb\u3002", "method": "\u91c7\u7528\u4ee3\u7406\u5e73\u53f0\u5de5\u7a0b\u65b9\u6cd5\uff0c\u96c6\u6210\u76ee\u6807\u9a71\u52a8\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684AI\u4ee3\u7406\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u5b9a\u4e49\u7ea6\u675f\u5185\u8fdb\u884c\u63a8\u7406\u3001\u51b3\u7b56\u548c\u6267\u884c\u64cd\u4f5c\uff0c\u5b9e\u73b0\u6709\u754c\u81ea\u4e3b\u6027\u3002", "result": "\u4ee3\u7406\u5e73\u53f0\u5de5\u7a0b\u6210\u4e3a\u5e73\u53f0\u6f14\u8fdb\u7684\u65b0\u9636\u6bb5\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\uff0c\u540c\u65f6\u5c06\u4eba\u7c7b\u89d2\u8272\u4ece\u76f4\u63a5\u64cd\u4f5c\u8f6c\u5411\u76d1\u7763\u548c\u6cbb\u7406\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u7cfb\u7edf\u7ba1\u7406\u3002", "conclusion": "\u4ee3\u7406\u5e73\u53f0\u5de5\u7a0b\u4ee3\u8868\u4e86\u5e73\u53f0\u53d1\u5c55\u7684\u65b0\u65b9\u5411\uff0c\u901a\u8fc7\u6709\u754c\u81ea\u4e3b\u6027\u5e73\u8861AI\u80fd\u529b\u4e0e\u4eba\u7c7b\u76d1\u7763\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.7c59c78c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevblogs.microsoft.com%2Fdevops%2Fazure-boards-integration-with-github-copilot-includes-custom-agent-support%2F%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/kjVa9-Sxdk25Wa1Nv1btgg7tQ7yyPynETuze19rWayg=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevblogs.microsoft.com%2Fdevops%2Fazure-boards-integration-with-github-copilot-includes-custom-agent-support%2F%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/kjVa9-Sxdk25Wa1Nv1btgg7tQ7yyPynETuze19rWayg=444", "authors": ["TLDR Newsletter"], "title": "Azure Boards integration with GitHub Copilot includes custom agent support", "comment": "Source: TLDR Newsletter, Date: 2026-02-16, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevblogs.microsoft.com%2Fdevops%2Fazure-boards-integration-with-github-copilot-includes-custom-agent-support%2F%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/kjVa9-Sxdk25Wa1Nv1btgg7tQ7yyPynETuze19rWayg=444", "summary": "Azure Boards integration with GitHub Copilot includes custom agent support (5 minute read) GitHub Copilot Coding Agent for Azure Boards is adding support for custom agents defined at the repository or organization level. Teams will be able to standardize workflows by selecting tailored agents when creating pull requests directly from work items.", "source": "tldr", "AI": {"tldr": "GitHub Copilot for Azure Boards\u65b0\u589e\u81ea\u5b9a\u4e49\u4ee3\u7406\u652f\u6301\uff0c\u5141\u8bb8\u56e2\u961f\u5728\u4ed3\u5e93\u6216\u7ec4\u7ec7\u7ea7\u522b\u5b9a\u4e49\u5b9a\u5236\u4ee3\u7406\uff0c\u4ece\u800c\u6807\u51c6\u5316\u4ece\u5de5\u4f5c\u9879\u521b\u5efaPR\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684GitHub Copilot\u4e0eAzure Boards\u96c6\u6210\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u56e2\u961f\u9700\u8981\u80fd\u591f\u6839\u636e\u7279\u5b9a\u5de5\u4f5c\u6d41\u7a0b\u5b9a\u5236\u4ee3\u7406\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6807\u51c6\u5316\u7684\u5f00\u53d1\u6d41\u7a0b\u548c\u63d0\u5347\u56e2\u961f\u534f\u4f5c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5728\u4ed3\u5e93\u6216\u7ec4\u7ec7\u7ea7\u522b\u5f15\u5165\u81ea\u5b9a\u4e49\u4ee3\u7406\u5b9a\u4e49\u529f\u80fd\uff0c\u5141\u8bb8\u56e2\u961f\u521b\u5efa\u548c\u914d\u7f6e\u4e13\u95e8\u7684\u4ee3\u7406\u3002\u8fd9\u4e9b\u4ee3\u7406\u53ef\u4ee5\u5728\u4eceAzure Boards\u5de5\u4f5c\u9879\u76f4\u63a5\u521b\u5efa\u62c9\u53d6\u8bf7\u6c42\u65f6\u88ab\u9009\u62e9\u548c\u4f7f\u7528\u3002", "result": "\u56e2\u961f\u73b0\u5728\u80fd\u591f\u5b9a\u4e49\u548c\u4f7f\u7528\u5b9a\u5236\u5316\u7684\u4ee3\u7406\uff0c\u8fd9\u4e9b\u4ee3\u7406\u9488\u5bf9\u7279\u5b9a\u7684\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u8fdb\u884c\u4f18\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u6807\u51c6\u5316\u7684\u4ece\u5de5\u4f5c\u9879\u5230\u4ee3\u7801\u53d8\u66f4\u7684\u8f6c\u6362\u8fc7\u7a0b\u3002", "conclusion": "\u81ea\u5b9a\u4e49\u4ee3\u7406\u652f\u6301\u589e\u5f3a\u4e86GitHub Copilot\u4e0eAzure Boards\u96c6\u6210\u7684\u7075\u6d3b\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4f7f\u56e2\u961f\u80fd\u591f\u66f4\u597d\u5730\u6807\u51c6\u5316\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u9ad8\u534f\u4f5c\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "tldr.2602.4b7f127b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fpydantic%2Fmonty%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/IhYdhFHalwVt6L_rbbDlfO4eEkfT0Gx9nt3LNZnkowE=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fpydantic%2Fmonty%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/IhYdhFHalwVt6L_rbbDlfO4eEkfT0Gx9nt3LNZnkowE=444", "authors": ["TLDR Newsletter"], "title": "Monty", "comment": "Source: TLDR Newsletter, Date: 2026-02-16, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fpydantic%2Fmonty%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/IhYdhFHalwVt6L_rbbDlfO4eEkfT0Gx9nt3LNZnkowE=444", "summary": "Monty (GitHub Repo) Monty is a new minimal and secure Python interpreter that allows AI agents to safely and quickly execute LLM-generated code. This experimental project aims to power \"code-mode\" in Pydantic AI by offering microsecond startup times and avoiding the complexity of traditional container-based sandboxes.", "source": "tldr", "AI": {"tldr": "Monty\u662f\u4e00\u4e2a\u65b0\u7684\u6700\u5c0f\u5316\u3001\u5b89\u5168\u7684Python\u89e3\u91ca\u5668\uff0c\u4e13\u4e3aAI\u4ee3\u7406\u5b89\u5168\u5feb\u901f\u6267\u884cLLM\u751f\u6210\u7684\u4ee3\u7801\u800c\u8bbe\u8ba1\uff0c\u5177\u6709\u5fae\u79d2\u7ea7\u542f\u52a8\u65f6\u95f4\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5bb9\u5668\u6c99\u7bb1\u7684\u590d\u6742\u6027\u3002", "motivation": "\u4f20\u7edf\u5bb9\u5668\u6c99\u7bb1\u542f\u52a8\u6162\u3001\u590d\u6742\u5ea6\u9ad8\uff0c\u65e0\u6cd5\u6ee1\u8db3AI\u4ee3\u7406\u5feb\u901f\u6267\u884cLLM\u751f\u6210\u4ee3\u7801\u7684\u9700\u6c42\u3002\u9700\u8981\u4e00\u79cd\u66f4\u8f7b\u91cf\u3001\u66f4\u5b89\u5168\u3001\u542f\u52a8\u66f4\u5feb\u7684\u4ee3\u7801\u6267\u884c\u73af\u5883\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6700\u5c0f\u5316\u7684Python\u89e3\u91ca\u5668Monty\uff0c\u4e13\u6ce8\u4e8e\u5b89\u5168\u6027\u548c\u5feb\u901f\u542f\u52a8\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u67b6\u6784\u907f\u514d\u4f20\u7edf\u5bb9\u5668\u5316\u65b9\u6848\u7684\u590d\u6742\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u5fae\u79d2\u7ea7\u542f\u52a8\u65f6\u95f4\uff0c\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b89\u5168\u5feb\u901f\u7684\u4ee3\u7801\u6267\u884c\u73af\u5883\uff0c\u652f\u6301Pydantic AI\u7684\"\u4ee3\u7801\u6a21\u5f0f\"\u3002", "conclusion": "Monty\u4e3aAI\u4ee3\u7406\u6267\u884cLLM\u751f\u6210\u4ee3\u7801\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5bb9\u5668\u6c99\u7bb1\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "tldr.2602.beb58f6e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Fdevops%2Faws-transform-custom-ai-driven-java-modernization-to-reduce-tech-debt%2F%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/kK2zHhPBmCr1sRlAjna6EmfLVkdvtjx_RqbQBGwJBwU=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Fdevops%2Faws-transform-custom-ai-driven-java-modernization-to-reduce-tech-debt%2F%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/kK2zHhPBmCr1sRlAjna6EmfLVkdvtjx_RqbQBGwJBwU=444", "authors": ["TLDR Newsletter"], "title": "AWS Transform custom: AI-driven Java modernization to reduce tech debt", "comment": "Source: TLDR Newsletter, Date: 2026-02-16, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Fdevops%2Faws-transform-custom-ai-driven-java-modernization-to-reduce-tech-debt%2F%3Futm_source=tldrdevops/1/0100019c66583676-75f7795a-cf8f-49d7-8db5-1a10a25b843d-000000/kK2zHhPBmCr1sRlAjna6EmfLVkdvtjx_RqbQBGwJBwU=444", "summary": "AWS Transform custom: AI-driven Java modernization to reduce tech debt (7 minute read) AWS Transform custom uses agentic AI and AWS-managed transformations to modernize Java applications at scale, automating upgrades like Java 8 to 21. It reduces manual refactoring by applying vetted patterns through a CLI while allowing custom transformations when needed.", "source": "tldr", "AI": {"tldr": "AWS Transform Custom \u4f7f\u7528\u4ee3\u7406AI\u548cAWS\u6258\u7ba1\u8f6c\u6362\uff0c\u81ea\u52a8\u5316Java\u5e94\u7528\u73b0\u4ee3\u5316\uff08\u5982Java 8\u523021\uff09\uff0c\u901a\u8fc7CLI\u5e94\u7528\u9a8c\u8bc1\u6a21\u5f0f\u51cf\u5c11\u624b\u52a8\u91cd\u6784\uff0c\u540c\u65f6\u652f\u6301\u81ea\u5b9a\u4e49\u8f6c\u6362\u3002", "motivation": "\u51cf\u5c11\u6280\u672f\u503a\u52a1\uff0c\u81ea\u52a8\u5316Java\u5e94\u7528\u73b0\u4ee3\u5316\u8fc7\u7a0b\uff0c\u89e3\u51b3\u624b\u52a8\u5347\u7ea7Java\u7248\u672c\u65f6\u7684\u5927\u91cf\u91cd\u6784\u5de5\u4f5c\u3002", "method": "\u4f7f\u7528\u4ee3\u7406AI\u548cAWS\u7ba1\u7406\u7684\u8f6c\u6362\u6a21\u5f0f\uff0c\u901a\u8fc7\u547d\u4ee4\u884c\u754c\u9762(CLI)\u81ea\u52a8\u5316\u5e94\u7528\u9a8c\u8bc1\u7684\u73b0\u4ee3\u5316\u6a21\u5f0f\uff0c\u540c\u65f6\u5141\u8bb8\u7528\u6237\u6839\u636e\u9700\u8981\u6dfb\u52a0\u81ea\u5b9a\u4e49\u8f6c\u6362\u3002", "result": "\u80fd\u591f\u5927\u89c4\u6a21\u73b0\u4ee3\u5316Java\u5e94\u7528\u7a0b\u5e8f\uff0c\u81ea\u52a8\u5316\u5347\u7ea7Java\u7248\u672c\uff08\u5982\u4eceJava 8\u523021\uff09\uff0c\u663e\u8457\u51cf\u5c11\u624b\u52a8\u91cd\u6784\u5de5\u4f5c\u91cf\u3002", "conclusion": "AWS Transform Custom\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684Java\u5e94\u7528\u73b0\u4ee3\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408AI\u9a71\u52a8\u548c\u6258\u7ba1\u8f6c\u6362\uff0c\u6709\u6548\u964d\u4f4e\u6280\u672f\u503a\u52a1\u3002", "topic": "code agent"}}
