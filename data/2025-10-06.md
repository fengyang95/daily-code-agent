<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 16]
- [cs.AI](#cs.AI) [Total: 11]
- [tldr.article](#tldr.article) [Total: 7]
- [cs.LG](#cs.LG) [Total: 9]
- [wechat.article](#wechat.article) [Total: 24]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering](https://arxiv.org/abs/2510.02328)
*Ziqing Wang,Chengsheng Mao,Xiaole Wen,Yuan Luo,Kaize Ding*

Main category: cs.CL

TL;DR: 提出了AMANDA框架，通过LLM代理进行医学知识增强，解决医学多模态大语言模型在低资源环境下的推理瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 现有Med-MLLMs在低资源环境下因医学推理能力瓶颈而失败，包括忽略医学图像细节的内在推理瓶颈和未能整合专业医学知识的外在推理瓶颈

Method: 使用训练免费的代理框架，通过内在医学知识增强（粗到细问题分解）和外在医学知识增强（生物医学知识图谱检索）来增强推理过程

Result: 在8个Med-VQA基准测试中，零样本和少样本设置下均取得显著改进

Conclusion: AMANDA框架有效解决了Med-MLLMs的推理瓶颈，在低资源医学视觉问答中表现出色

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise
in medical visual question answering (Med-VQA). However, when deployed in
low-resource settings where abundant labeled data are unavailable, existing
Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks:
(i) the intrinsic reasoning bottleneck that ignores the details from the
medical image; (ii) the extrinsic reasoning bottleneck that fails to
incorporate specialized medical knowledge. To address those limitations, we
propose AMANDA, a training-free agentic framework that performs medical
knowledge augmentation via LLM agents. Specifically, our intrinsic medical
knowledge augmentation focuses on coarse-to-fine question decomposition for
comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds
the reasoning process via biomedical knowledge graph retrieval. Extensive
experiments across eight Med-VQA benchmarks demonstrate substantial
improvements in both zero-shot and few-shot Med-VQA settings. The code is
available at https://github.com/REAL-Lab-NU/AMANDA.

</details>


### [2] [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
*Zhe Li,Wei Zhao,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: 提出了一个基于表示和梯度的框架，用于诊断LLM中的不良行为，包括有害内容、后门投毒和知识污染等问题，提供样本级和细粒度token级分析。


<details>
  <summary>Details</summary>
Motivation: LLM部署中存在生成有害内容、事实错误和社会偏见等不良行为，现有基于参数梯度的归因方法因噪声信号和计算复杂度而效果不佳。

Method: 在模型激活空间中分析表示及其梯度，提供语义上有意义的信号，将输出与训练数据联系起来。

Result: 方法在追踪有害内容、检测后门投毒和识别知识污染等任务中表现出色，不仅能进行样本级归因，还能实现细粒度token级分析。

Conclusion: 该工作提供了一个强大的诊断工具，用于理解、审计并最终减轻与LLM相关的风险。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their deployment is frequently undermined by undesirable behaviors such as
generating harmful content, factual inaccuracies, and societal biases.
Diagnosing the root causes of these failures poses a critical challenge for AI
safety. Existing attribution methods, particularly those based on parameter
gradients, often fall short due to prohibitive noisy signals and computational
complexity. In this work, we introduce a novel and efficient framework that
diagnoses a range of undesirable LLM behaviors by analyzing representation and
its gradients, which operates directly in the model's activation space to
provide a semantically meaningful signal linking outputs to their training
data. We systematically evaluate our method for tasks that include tracking
harmful content, detecting backdoor poisoning, and identifying knowledge
contamination. The results demonstrate that our approach not only excels at
sample-level attribution but also enables fine-grained token-level analysis,
precisely identifying the specific samples and phrases that causally influence
model behavior. This work provides a powerful diagnostic tool to understand,
audit, and ultimately mitigate the risks associated with LLMs. The code is
available at https://github.com/plumprc/RepT.

</details>


### [3] [Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models](https://arxiv.org/abs/2510.02339)
*Kevin Zhou,Adam Dejl,Gabriel Freedman,Lihu Chen,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: 本文研究了在论证性大语言模型(ArgLLMs)中集成不确定性量化方法，通过实验评估不同UQ方法在声明验证任务中的表现，发现简单的直接提示策略优于复杂方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的重要性日益增长，不确定性量化对于保证其可靠性至关重要。本文旨在探索UQ方法在基于计算论证的可解释决策框架ArgLLMs中的应用价值。

Method: 在ArgLLMs框架中集成不同的LLM UQ方法，并在声明验证任务上进行实验评估，同时该实验过程本身也是一种评估UQ方法有效性的新方式。

Result: 实验结果表明，尽管简单，直接提示策略在ArgLLMs中是一种有效的UQ方法，显著优于更复杂的方法。

Conclusion: 在论证性LLM框架中，简单的直接提示UQ策略比复杂方法更有效，为LLM不确定性量化提供了实用指导。

Abstract: Research in uncertainty quantification (UQ) for large language models (LLMs)
is increasingly important towards guaranteeing the reliability of this
groundbreaking technology. We explore the integration of LLM UQ methods in
argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making
based on computational argumentation in which UQ plays a critical role. We
conduct experiments to evaluate ArgLLMs' performance on claim verification
tasks when using different LLM UQ methods, inherently performing an assessment
of the UQ methods' effectiveness. Moreover, the experimental procedure itself
is a novel way of evaluating the effectiveness of UQ methods, especially when
intricate and potentially contentious statements are present. Our results
demonstrate that, despite its simplicity, direct prompting is an effective UQ
strategy in ArgLLMs, outperforming considerably more complex approaches.

</details>


### [4] [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
*Xin Gao,Ruiyi Zhang,Daniel Du,Saurabh Mahindre,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.CL

TL;DR: 研究探讨了通过提示让大语言模型模拟早期知识截止日期的能力，发现提示方法在直接查询时有效，但在处理因果相关但未直接询问的遗忘内容时效果不佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在时序预测中依赖预训练数据，可能因数据污染而高估其泛化能力，因此需要研究能否通过提示模拟早期知识截止日期。

Method: 构建三个评估数据集，分别测试LLMs遗忘直接事实知识、语义变化和因果相关知识的能力，使用基于提示的模拟知识截止方法。

Result: 提示方法在直接查询截止日期后信息时有效，但在处理因果相关但未直接询问的遗忘内容时表现不佳。

Conclusion: 需要在LLMs时序预测任务中采用更严格的评估设置，当前提示方法在诱导遗忘因果相关但未直接询问的内容方面存在局限。

Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but
their reliance on pretraining data raises contamination concerns, as accurate
predictions on pre-cutoff test data may reflect memorization rather than
reasoning, leading to an overestimation of their generalization capability.
With the recent emergence of prompting-based unlearning techniques, a natural
question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?
In this work, we investigate the capability of prompting to simulate earlier
knowledge cutoff in LLMs. We construct three evaluation datasets to assess the
extent to which LLMs can forget (1) direct factual knowledge, (2) semantic
shifts, and (3) causally related knowledge. Results demonstrate that while
prompt-based simulated knowledge cutoffs show effectiveness when directly
queried with the information after that date, they struggle to induce
forgetting when the forgotten content is not directly asked but causally
related to the query. These findings highlight the need for more rigorous
evaluation settings when applying LLMs for temporal prediction tasks. The full
dataset and evaluation code are available at
https://github.com/gxx27/time_unlearn.

</details>


### [5] [DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning](https://arxiv.org/abs/2510.02341)
*Yifan Wang,Bolian Li,Junlin Wu,Zhaoxuan Tan,Zheli Liu,Ruqi Zhang,Ananth Grama,Qingkai Zeng*

Main category: cs.CL

TL;DR: DRIFT是一种基于用户不满意信号的迭代偏好训练方法，利用真实世界中的用户不满意反馈来训练LLM，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界LLM部署中用户不满意信号丰富而显式满意反馈稀缺，现有偏好学习方法与这种数据分布不匹配。

Method: DRIFT以真实世界的不满意信号为基础，从不断演化的策略中动态采样正例进行训练。

Result: 在WildBench和AlpacaEval2基准测试中，DRIFT相比基线方法提升显著，14B模型在WildBench上超越GPT-4o-mini，同时保持探索能力。

Conclusion: DRIFT是利用最丰富和最有信息量的信号进行现实世界后训练的有效且可扩展的方法。

Abstract: Real-world large language model deployments (e.g., conversational AI systems,
code generation assistants) naturally generate abundant implicit user
dissatisfaction (DSAT) signals, as users iterate toward better answers through
refinements, corrections, and expressed preferences, while explicit
satisfaction (SAT) feedback is scarce. Existing preference learning approaches
are poorly aligned with this data profile, as they rely on costly human
annotations or assume plentiful positive responses. In this paper, we introduce
\textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative
pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world
DSAT signals and samples positives dynamically from the evolving policy.
Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets
and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) /
+7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B)
on AlpacaEval2 win rate over base models, outperforming strong baseline methods
such as iterative DPO and SPIN. At larger scales, the improvements are
particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on
WildBench. Further analysis shows that DRIFT also preserves exploratory
capacity, yielding more diverse high-reward solutions rather than collapsing to
narrow subsets. Theoretically, we demonstrate that this design preserves
preference margins and avoids the gradient degeneration. These results show
that DRIFT is an effective and scalable recipe for real-world post-training
that leverages the most abundant and informative signal. The code and data are
available at https://github.com/cacayaya/DRIFT.git.

</details>


### [6] [Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations](https://arxiv.org/abs/2510.02352)
*Yihao Wu,Tianrui Wang,Yizhou Peng,Yi-Wen Chao,Xuyi Zhuang,Xinsheng Wang,Shunshun Yin,Ziyang Ma*

Main category: cs.CL

TL;DR: 该论文首次系统评估语音对话模型中的偏见，发现闭源模型偏见较低，开源模型对年龄和性别更敏感，多轮对话可能加剧偏见。


<details>
  <summary>Details</summary>
Motivation: 虽然文本LLM中的偏见已被研究，但具有音频输入输出的语音对话模型中的偏见特征仍未探索，这对决策和推荐任务的公平性有重要影响。

Method: 使用Group Unfairness Score和similarity-based normalized statistics rate指标，评估开源和闭源语音模型在多轮对话中的偏见表现。

Result: 闭源模型总体偏见较低，开源模型对年龄和性别更敏感，推荐任务会放大跨群体差异，多轮对话中偏见决策可能持续存在。

Conclusion: 这是首个系统研究端到端语音对话模型偏见的成果，为开发公平可靠的音频交互系统提供见解，并发布了FairDialogue数据集和评估代码。

Abstract: While biases in large language models (LLMs), such as stereotypes and
cultural tendencies in outputs, have been examined and identified, their
presence and characteristics in spoken dialogue models (SDMs) with audio input
and output remain largely unexplored. Paralinguistic features, such as age,
gender, and accent, can affect model outputs; when compounded by multi-turn
conversations, these effects may exacerbate biases, with potential implications
for fairness in decision-making and recommendation tasks. In this paper, we
systematically evaluate biases in speech LLMs and study the impact of
multi-turn dialogues with repeated negative feedback. Bias is measured using
Group Unfairness Score (GUS) for decisions and similarity-based normalized
statistics rate (SNSR) for recommendations, across both open-source models like
Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o
Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models
generally exhibit lower bias, while open-source models are more sensitive to
age and gender, and recommendation tasks tend to amplify cross-group
disparities. We found that biased decisions may persist in multi-turn
conversations. This work provides the first systematic study of biases in
end-to-end spoken dialogue models, offering insights towards fair and reliable
audio-based interactive systems. To facilitate further research, we release the
FairDialogue dataset and evaluation code.

</details>


### [7] [Spiral of Silence in Large Language Model Agents](https://arxiv.org/abs/2510.02360)
*Mingze Zhong,Meng Fang,Zijing Shi,Yuxuan Huang,Shunfeng Zheng,Yali Du,Ling Chen,Jun Wang*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型集体中是否会出现沉默螺旋效应，提出了评估框架，发现历史和角色信号共同作用会产生强烈的多数主导模式，复制了沉默螺旋现象。


<details>
  <summary>Details</summary>
Motivation: 沉默螺旋理论原本用于解释人类社会中少数观点因害怕孤立而保持沉默的现象，但该理论不直接适用于LLM。研究旨在探索纯粹统计语言生成是否会在LLM集体中产生类似沉默螺旋的动态。

Method: 提出了评估LLM代理中沉默螺旋的框架，系统控制'历史'和'角色'信号的可用性，使用趋势测试和集中度测量来评估意见动态。

Result: 实验表明：历史和角色信号共同产生强烈的多数主导并复制沉默螺旋模式；仅历史信号导致强烈锚定；仅角色信号促进多样但不相关的意见，表明没有历史锚定就无法出现沉默螺旋动态。

Conclusion: 该工作连接了计算社会学和负责任AI设计，强调需要监控和减轻LLM代理系统中出现的从众现象。

Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views
often refrain from speaking out for fear of social isolation, enabling majority
positions to dominate public discourse. When the 'agents' are large language
models (LLMs), however, the classical psychological explanation is not directly
applicable, since SoS was developed for human societies. This raises a central
question: can SoS-like dynamics nevertheless emerge from purely statistical
language generation in LLM collectives? We propose an evaluation framework for
examining SoS in LLM agents. Specifically, we consider four controlled
conditions that systematically vary the availability of 'History' and 'Persona'
signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall
and Spearman's rank, along with concentration measures including kurtosis and
interquartile range. Experiments across open-source and closed-source models
show that history and persona together produce strong majority dominance and
replicate SoS patterns; history signals alone induce strong anchoring; and
persona signals alone foster diverse but uncorrelated opinions, indicating that
without historical anchoring, SoS dynamics cannot emerge. The work bridges
computational sociology and responsible AI design, highlighting the need to
monitor and mitigate emergent conformity in LLM-agent systems.

</details>


### [8] [A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History](https://arxiv.org/abs/2510.02362)
*Matei-Iulian Cocu,Răzvan-Cosmin Cristia,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 该研究通过测试多个大语言模型在不同语言和语境下回答罗马尼亚历史争议问题的表现，评估其偏见和不一致性。


<details>
  <summary>Details</summary>
Motivation: 认识到历史常因文化和国家意识形态而呈现不同视角，大语言模型训练数据的偏差可能导致缺乏中立性，影响用户认知。

Method: 研究分三个阶段进行：先让LLM以二元方式回答问题，然后要求用数值评分回答相同问题，测试回答方式对结果的影响。

Result: 二元回答稳定性相对较高但不完美，且因语言而异。模型常在不同语言或格式间改变立场，数值评分常与初始二元选择不一致。

Conclusion: 研究揭示了模型在特定语言语境下存在不一致性的倾向，最一致的模型并不总是最准确或中立的。

Abstract: In this case study, we select a set of controversial Romanian historical
questions and ask multiple Large Language Models to answer them across
languages and contexts, in order to assess their biases. Besides being a study
mainly performed for educational purposes, the motivation also lies in the
recognition that history is often presented through altered perspectives,
primarily influenced by the culture and ideals of a state, even through large
language models. Since they are often trained on certain data sets that may
present certain ambiguities, the lack of neutrality is subsequently instilled
in users. The research process was carried out in three stages, to confirm the
idea that the type of response expected can influence, to a certain extent, the
response itself; after providing an affirmative answer to some given question,
an LLM could shift its way of thinking after being asked the same question
again, but being told to respond with a numerical value of a scale. Results
show that binary response stability is relatively high but far from perfect and
varies by language. Models often flip stance across languages or between
formats; numeric ratings frequently diverge from the initial binary choice, and
the most consistent models are not always those judged most accurate or
neutral. Our research brings to light the predisposition of models to such
inconsistencies, within a specific contextualization of the language for the
question asked.

</details>


### [9] [Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents](https://arxiv.org/abs/2510.02369)
*Kuntai Cai,Juncheng Liu,Xianglin Yang,Zhaojie Niu,Xiaokui Xiao,Xing Chen*

Main category: cs.CL

TL;DR: 提出了实例级上下文学习(ILCL)方法，通过引导式探索和轻量级计划-执行-提取循环，为LLM智能体自动生成可验证、可重用的实例级上下文文档，显著提升任务成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体只关注环境级手册和任务级指导，但忽略了实例级上下文（如物体位置、制作配方等可验证事实），这导致在复杂任务中经常失败。

Method: 使用TODO森林进行智能优先级排序，采用轻量级计划-执行-提取循环来执行动作，自动生成高精度的可重用上下文文档。

Result: 在TextWorld、ALFWorld和Crafter上的实验显示，ReAct在TextWorld中的平均成功率从37%提升到95%，IGE从81%提升到95%。

Conclusion: 通过将一次性探索转化为持久、可重用的知识，该方法补充了现有上下文，使LLM智能体更加可靠和高效。

Abstract: Large language model (LLM) agents typically receive two kinds of context: (i)
environment-level manuals that define interaction interfaces and global rules,
and (ii) task-level guidance or demonstrations tied to specific goals. In this
work, we identify a crucial but overlooked third type of context,
instance-level context, which consists of verifiable and reusable facts tied to
a specific environment instance, such as object locations, crafting recipes,
and local rules. We argue that the absence of instance-level context is a
common source of failure for LLM agents in complex tasks, as success often
depends not only on reasoning over global rules or task prompts but also on
making decisions based on precise and persistent facts. Acquiring such context
requires more than memorization: the challenge lies in efficiently exploring,
validating, and formatting these facts under tight interaction budgets. We
formalize this problem as Instance-Level Context Learning (ILCL) and introduce
our task-agnostic method to solve it. Our method performs a guided exploration,
using a compact TODO forest to intelligently prioritize its next actions and a
lightweight plan-act-extract loop to execute them. This process automatically
produces a high-precision context document that is reusable across many
downstream tasks and agents, thereby amortizing the initial exploration cost.
Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent
gains in both success and efficiency: for instance, ReAct's mean success rate
in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By
transforming one-off exploration into persistent, reusable knowledge, our
method complements existing contexts to enable more reliable and efficient LLM
agents.

</details>


### [10] [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
*Aakriti Agrawal,Rohith Aralikatti,Anirudh Satheesh,Souradip Chakraborty,Amrit Singh Bedi,Furong Huang*

Main category: cs.CL

TL;DR: 提出了一种基于校准对数似然分数的高效方法，从多个不同LLM中选择最佳响应，无需昂贵的外部验证器或多次采样。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中，从多个LLM中选择最可靠响应仍然具有挑战性。现有方法依赖昂贵的外部验证器、人工评估或需要单模型多次采样的自一致性技术。

Method: 使用校准的对数似然分数来隐式利用这些模型的固有知识和置信度，从多个不同LLM中选择最佳响应。

Result: 在GSM8K、MMLU（6个子集）和ARC数据集上，在辩论（多轮LLM讨论）和非辩论（多LLM最佳选择）设置中分别实现了约4%、3%和5%的性能提升。

Conclusion: 该方法在计算效率高的前提下，能够有效提升多LLM系统的响应选择质量。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet
selecting the most reliable response from multiple LLMs remains a challenge,
particularly in resource-constrained settings. Existing approaches often depend
on costly external verifiers, human evaluators, or self-consistency techniques
that require multiple samples from a single model. While multi-LLM systems
produce more diverse responses than single models and thus have greater
potential, they often underperform compared to single LLM self-consistency. We
propose a principled, novel and computationally efficient method to select the
best response from multiple different LLMs using a calibrated log-likelihood
score, implicitly leveraging the inherent knowledge and confidence of these
models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across
both debate (multi-round LLM discussions) and non-debate (Best-of-N with
multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets
respectively.

</details>


### [11] [Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions](https://arxiv.org/abs/2510.02645)
*Fulei Zhang,Zhou Yu*

Main category: cs.CL

TL;DR: 用户与LLM聊天机器人和人类代理交流时采用不同的沟通风格，研究发现语法流畅度、礼貌程度和词汇多样性存在显著差异，需要调整模型以适应这种风格变化。


<details>
  <summary>Details</summary>
Motivation: 探索用户在LLM聊天机器人和人类代理面前沟通风格的差异，以及如何让LLM模型更好地适应部署后的沟通风格变化。

Method: 使用两种策略增强LLM鲁棒性：(1) 后训练阶段的数据增强，(2) 推理时用户消息重构。

Result: 在风格多样化数据集上训练的模型显著优于仅在原始或风格统一数据集上训练的模型，而推理时重构效果较差。

Conclusion: 模型需要适应LLM部署后用户沟通风格的变化，风格多样化的训练数据能显著提升模型性能。

Abstract: As Large Language Models (LLMs) are increasingly deployed in customer-facing
applications, a critical yet underexplored question is how users communicate
differently with LLM chatbots compared to human agent. In this study, we
present empirical evidence that users adopt distinct communication styles when
users interact with chatbots versus human agents. Our analysis reveals
significant differences in grammatical fluency, politeness, and lexical
diversity in user language between the two settings. These findings suggest
that models trained exclusively on human-human interaction data may not
adequately accommodate the communication style shift that occurs once an LLM
chatbot is deployed. To enhance LLM robustness to post-launch communication
style changes, we experimented with two strategies: (1) data augmentation
during the post-training phase and (2) inference-time user message
reformulation. Our results indicate that models trained on stylistically
diverse datasets significantly outperform those trained exclusively on original
or stylistically uniform datasets, while inference-time reformulation proved
less effective. These insights help us to better adapt our models for improved
LLM-user interaction experiences.

</details>


### [12] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: 本文首次对对话AI鲁棒性进行生存分析，通过分析36,951个对话轮次发现：突发语义漂移会灾难性增加对话失败风险，而渐进漂移则显著降低风险并延长对话时长。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架主要关注静态基准和单轮评估，无法捕捉真实对话中随时间退化的动态特性，需要新的评估方法来理解LLM在多轮对话中的鲁棒性。

Method: 采用生存分析框架（Cox比例风险模型、加速失效时间模型和随机生存森林），分析9个最先进LLM的对话数据，将对话失败建模为时间到事件过程。

Result: AFT模型表现最佳，具有优异的区分度和校准能力。突发语义漂移使对话失败风险急剧增加，而渐进漂移则显著降低风险并支持更长的对话。

Conclusion: 生存分析是评估LLM鲁棒性的强大范式，为设计弹性对话代理提供具体见解，并挑战了对话AI中语义一致性必要性的普遍假设。

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [13] [The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback](https://arxiv.org/abs/2510.02752)
*Hangfan Zhang,Siyuan Xu,Zhimeng Guo,Huaisheng Zhu,Shicheng Liu,Xinrun Wang,Qiaosheng Zhang,Yang Chen,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: 提出一种基于自我意识的强化学习方法，通过让LLM自己生成任务并解决，使用难度预测和极限突破机制，在仅需少量额外数据的情况下显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要大量标注数据，本研究旨在探索如何用最少的数据提升LLM的推理能力。

Method: 交替进行任务生成与解决，引入自我意识难度预测和极限突破机制，让模型评估任务难度并主动请求外部数据。

Result: 在9个基准测试中实现53.8%的相对提升，仅需不到1.2%的额外数据。

Conclusion: 自我意识强化学习方法有效，展示了自我进化代理训练的前景。

Abstract: Reinforcement learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of large language models (LLMs), but such training
typically demands substantial efforts in creating and annotating data. In this
work, we explore improving LLMs through RL with minimal data. Our approach
alternates between the LLM proposing a task and then attempting to solve it. To
minimize data dependency, we introduce two novel mechanisms grounded in
self-awareness: (1) self-aware difficulty prediction, where the model learns to
assess task difficulty relative to its own abilities and prioritize challenging
yet solvable tasks, and (2) self-aware limit breaking, where the model
recognizes when a task is beyond its capability boundary and proactively
requests external data to break through that limit. Extensive experiments on
nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra
data demonstrate the efficacy of self-aware RL and underscore the promise of
self-evolving agent training.

</details>


### [14] [Self-Reflective Generation at Test Time](https://arxiv.org/abs/2510.02919)
*Jian Mu,Qixin Zhang,Zhiyong Wang,Menglin Yang,Shuang Qiu,Chengwei Qin,Zhongxiang Dai,Yao Shu*

Main category: cs.CL

TL;DR: SRGen是一个轻量级的测试时框架，通过在不确定点进行自我反思来改进LLM的推理能力，使用动态熵阈值识别高不确定性token并训练校正向量来修正概率分布。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的前向自回归生成过程很脆弱，早期token错误会级联传播，需要自我反思机制。现有方法要么对整个草稿进行修订，要么通过昂贵训练学习自我校正，都是被动且低效的。

Method: 在token生成过程中，利用动态熵阈值识别高不确定性token，为每个识别出的token训练特定的校正向量，利用已生成上下文进行自我反思生成来修正token概率分布。

Result: 在数学推理基准测试中，SRGen显著提升了模型推理能力，特别是在AIME2024上，DeepSeek-R1-Distill-Qwen-7B的Pass@1绝对提升12.0%，Cons@5提升13.3%。

Conclusion: SRGen作为一个即插即用方法，将反思集成到生成过程中，实现了可靠LLM推理，在有限开销下获得一致增益，并能与其他训练时和测试时技术广泛组合。

Abstract: Large language models (LLMs) increasingly solve complex reasoning tasks via
long chain-of-thought, but their forward-only autoregressive generation process
is fragile; early token errors can cascade, which creates a clear need for
self-reflection mechanisms. However, existing self-reflection either performs
revisions over full drafts or learns self-correction via expensive training,
both fundamentally reactive and inefficient. To address this, we propose
Self-Reflective Generation at Test Time (SRGen), a lightweight test-time
framework that reflects before generating at uncertain points. During token
generation, SRGen utilizes dynamic entropy thresholding to identify
high-uncertainty tokens. For each identified token, it trains a specific
corrective vector, which fully exploits the already generated context for a
self-reflective generation to correct the token probability distribution. By
retrospectively analyzing the partial output, this self-reflection enables more
trustworthy decisions, thereby significantly reducing the probability of errors
at highly uncertain points. Evaluated on challenging mathematical reasoning
benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model
reasoning: improvements in single-pass quality also translate into stronger
self-consistency voting. Especially, on AIME2024 with
DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on
Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a
plug-and-play method that integrates reflection into the generation process for
reliable LLM reasoning, achieving consistent gains with bounded overhead and
broad composability with other training-time (e.g., RLHF) and test-time (e.g.,
SLOT) techniques.

</details>


### [15] [FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents](https://arxiv.org/abs/2510.03204)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Léo Boisvert,Massimo Caccia,Jérémy Espinas,Alexandre Aussem,Véronique Eglin,Alexandre Lacoste*

Main category: cs.CL

TL;DR: FocusAgent使用轻量级LLM检索器从可访问性树观察中提取最相关的内容行，通过修剪噪声和无关内容来提高Web代理的效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有Web代理处理冗长网页时存在上下文限制饱和、计算成本高和安全风险（如提示注入）的问题，现有修剪策略效果不理想。

Method: 利用轻量级LLM检索器根据任务目标从可访问性树观察中提取最相关的内容行，减少噪声和无关内容。

Result: 在WorkArena和WebArena基准测试中，FocusAgent与强基线性能相当，同时将观察大小减少50%以上，并能显著降低提示注入攻击的成功率。

Conclusion: 基于LLM的有针对性检索是构建高效、有效且安全的Web代理的实用且鲁棒策略。

Abstract: Web agents powered by large language models (LLMs) must process lengthy web
page observations to complete user goals; these pages often exceed tens of
thousands of tokens. This saturates context limits and increases computational
cost processing; moreover, processing full pages exposes agents to security
risks such as prompt injection. Existing pruning strategies either discard
relevant content or retain irrelevant context, leading to suboptimal action
prediction. We introduce FocusAgent, a simple yet effective approach that
leverages a lightweight LLM retriever to extract the most relevant lines from
accessibility tree (AxTree) observations, guided by task goals. By pruning
noisy and irrelevant content, FocusAgent enables efficient reasoning while
reducing vulnerability to injection attacks. Experiments on WorkArena and
WebArena benchmarks show that FocusAgent matches the performance of strong
baselines, while reducing observation size by over 50%. Furthermore, a variant
of FocusAgent significantly reduces the success rate of prompt-injection
attacks, including banner and pop-up attacks, while maintaining task success
performance in attack-free settings. Our results highlight that targeted
LLM-based retrieval is a practical and robust strategy for building web agents
that are efficient, effective, and secure.

</details>


### [16] [Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment](https://arxiv.org/abs/2510.03223)
*Hongxiang Zhang,Yuan Tian,Tianyi Zhang*

Main category: cs.CL

TL;DR: Self-Anchor是一种新的提示方法，通过将推理轨迹分解为结构化计划并自动对齐模型注意力到最相关的推理步骤，来解决LLM在复杂推理任务中注意力分散的问题。


<details>
  <summary>Details</summary>
Motivation: 随着推理链的延长，关键的中间步骤和原始提示会在上下文中被淹没，导致注意力不足和错误。需要一种轻量级的方法来引导LLM注意力。

Method: 将推理轨迹分解为结构化计划，自动对齐模型注意力到最相关的推理步骤，使模型在生成过程中保持专注。

Result: 在六个基准测试中优于最先进的提示方法，显著缩小了"非推理"模型与专用推理模型之间的性能差距。

Conclusion: Self-Anchor有潜力使大多数LLM无需重新训练就能处理复杂推理任务。

Abstract: To solve complex reasoning tasks for Large Language Models (LLMs),
prompting-based methods offer a lightweight alternative to fine-tuning and
reinforcement learning. However, as reasoning chains extend, critical
intermediate steps and the original prompt will be buried in the context,
receiving insufficient attention and leading to errors. In this paper, we
propose Self-Anchor, a novel pipeline that leverages the inherent structure of
reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories
into structured plans and automatically aligns the model's attention to the
most relevant inference steps, allowing the model to maintain focus throughout
generation. Our experiment shows that Self-Anchor outperforms SOTA prompting
methods across six benchmarks. Notably, Self-Anchor significantly reduces the
performance gap between ``non-reasoning'' models and specialized reasoning
models, with the potential to enable most LLMs to tackle complex reasoning
tasks without retraining.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks](https://arxiv.org/abs/2510.02418)
*Sagnik Anupam,Davis Brown,Shuo Li,Eric Wong,Hamed Hassani,Osbert Bastani*

Main category: cs.AI

TL;DR: BrowserArena是一个实时开放网络代理评估平台，通过用户提交任务、头对头比较和步骤级人工反馈来识别网络代理的失败模式。研究发现三个主要失败模式：验证码解决、弹窗移除和直接URL导航。


<details>
  <summary>Details</summary>
Motivation: 当前网络代理评估局限于沙盒环境或人工任务，需要真实开放网络环境下的评估平台来识别代理的实际失败模式。

Method: 构建BrowserArena平台，收集用户任务，进行头对头比较，使用步骤级人工反馈分析代理轨迹，并针对失败模式构建专门数据集进行深入研究。

Result: 识别出三个一致的失败模式：验证码解决、弹窗移除和直接URL导航。发现不同语言模型在应对这些失败模式时存在差异，如o4-mini使用更多策略规避验证码，而DeepSeek-R1在验证码解决上误导用户。

Conclusion: 当前网络代理表现出多样性和脆弱性。该基准测试方法为大规模评估和理解网络代理失败模式提供了有效途径。

Abstract: LLM web agents now browse and take actions on the open web, yet current agent
evaluations are constrained to sandboxed environments or artificial tasks. We
introduce BrowserArena, a live open-web agent evaluation platform that collects
user-submitted tasks, runs Arena-style head-to-head comparisons, and uses
step-level human feedback to surface failure modes. Collecting and analyzing
step-level annotations on the agent traces, we identify three consistent
failure modes: captcha resolution, pop-up banner removal, and direct navigation
to URLs. By constructing targeted datasets to further study these tasks, we
discover variations in how different language models navigate these failure
modes. We find, for example, that o4-mini deploys a wider variety of strategies
to circumvent captcha resolution than other models and DeepSeek-R1 consistently
misleads users about captcha resolution. Our findings surface both the
diversity and brittleness of current web agents. More broadly, our benchmarking
methodology provides an approach to evaluating and understanding web agent
failure modes at scale.

</details>


### [18] [Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge](https://arxiv.org/abs/2510.02557)
*Charlie Masters,Advaith Vellanki,Jiangbo Shangguan,Bart Kultys,Jonathan Gilmore,Alastair Moore,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 本文提出了自主管理代理的概念，用于协调动态人机团队中的复杂多代理工作流，将其形式化为部分可观测随机博弈，并识别了四个基础挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然代理AI在自动化单个任务方面取得了进展，但管理复杂的多代理工作流仍然是一个具有挑战性的问题。

Method: 提出了自主管理代理作为核心挑战，将工作流管理形式化为部分可观测随机博弈，并开发了MA-Gym开源仿真框架来评估GPT-5基础的管理代理。

Result: 在20个工作流中评估GPT-5基础的管理代理，发现它们在同时优化目标完成、约束遵守和工作流运行时间方面存在困难。

Conclusion: 工作流管理是一个困难的开放性问题，并讨论了自主管理系统的组织和伦理影响。

Abstract: While agentic AI has advanced in automating individual tasks, managing
complex multi-agent workflows remains a challenging problem. This paper
presents a research vision for autonomous agentic systems that orchestrate
collaboration within dynamic human-AI teams. We propose the Autonomous Manager
Agent as a core challenge: an agent that decomposes complex goals into task
graphs, allocates tasks to human and AI workers, monitors progress, adapts to
changing conditions, and maintains transparent stakeholder communication. We
formalize workflow management as a Partially Observable Stochastic Game and
identify four foundational challenges: (1) compositional reasoning for
hierarchical decomposition, (2) multi-objective optimization under shifting
preferences, (3) coordination and planning in ad hoc teams, and (4) governance
and compliance by design. To advance this agenda, we release MA-Gym, an
open-source simulation and evaluation framework for multi-agent workflow
orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we
find they struggle to jointly optimize for goal completion, constraint
adherence, and workflow runtime - underscoring workflow management as a
difficult open problem. We conclude with organizational and ethical
implications of autonomous management systems.

</details>


### [19] [Agentic Additive Manufacturing Alloy Discovery](https://arxiv.org/abs/2510.02567)
*Peter Pak,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.AI

TL;DR: 利用LLM驱动的多智能体系统自动化增材制造中的合金发现过程，通过MCP协议调用热力学计算工具，实现自主决策和合金可打印性分析。


<details>
  <summary>Details</summary>
Motivation: 增材制造中的合金发现需要材料科学、热力学模拟和实验分析等多领域专业知识，过程复杂且耗时。LLM智能体可以利用其知识库和工具调用能力来加速这一过程。

Method: 开发基于LLM的多智能体系统，通过MCP协议调用Thermo-Calc属性图计算和熔合不足过程图生成等工具，能够根据工具调用结果动态调整任务轨迹。

Result: 多智能体系统能够有效推理复杂用户提示，对提议合金的可打印性提供分析，在实践环境中实现自主决策。

Conclusion: LLM驱动的智能体系统能够自动化和加速增材制造中的合金发现任务，展示了采用这种多智能体系统的优势。

Abstract: Agentic systems enable the intelligent use of research tooling, augmenting a
researcher's ability to investigate and propose novel solutions to existing
problems. Within Additive Manufacturing (AM), alloy discovery remains a complex
challenge, often requiring expertise in the various domains of materials
science, thermodynamic simulations, and experimental analysis. Large Language
Model (LLM) enabled agents can facilitate this endeavor by utilizing their
extensive knowledge base to dispatch tool calls via Model Context Protocol
(MCP) to perform actions such as Thermo-Calc property diagram calculations and
lack of fusion process map generation. In addition, the multi-agent system
developed in this work is able to effectively reason through complex user
prompts and provide analysis on the printability of proposed alloys. These
agents can dynamically adjust their task trajectory to the outcomes of tool
call results, effectively enabling autonomous decision-making in practical
environments. This work aims to utilize LLM enabled agents to automate and
accelerate the task of alloy discovery within the field of additive
manufacturing and showcase the benefits of adopting this multi-agent system.

</details>


### [20] [A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem](https://arxiv.org/abs/2510.02589)
*Yunqi Huang,Nishith Chennakeshava,Alexis Carras,Vladislav Neverov,Wei Liu,Aske Plaat,Yingjie Fan*

Main category: cs.AI

TL;DR: 本文开发了一个包含起重机调度的集装箱配载规划Gym环境，评估了5种RL算法在不同复杂度场景下的性能，发现算法选择和问题表述对性能有重要影响。


<details>
  <summary>Details</summary>
Motivation: 集装箱配载规划(CSPP)对海运效率至关重要，但目前缺乏对不同RL算法的系统性基准比较。

Method: 开发了包含起重机调度的Gym环境，在单智能体和多智能体框架下评估DQN、QR-DQN、A2C、PPO和TRPO五种RL算法。

Result: 结果显示随着复杂度增加，不同算法间存在明显性能差距，算法选择和问题表述对CSPP性能有重要影响。

Conclusion: 本文为CSPP提供了多RL方法的基准测试和可重用的Gym环境，为海运物流的未来研究和实际部署奠定了基础。

Abstract: Container stowage planning (CSPP) is a critical component of maritime
transportation and terminal operations, directly affecting supply chain
efficiency. Owing to its complexity, CSPP has traditionally relied on human
expertise. While reinforcement learning (RL) has recently been applied to CSPP,
systematic benchmark comparisons across different algorithms remain limited. To
address this gap, we develop a Gym environment that captures the fundamental
features of CSPP and extend it to include crane scheduling in both multi-agent
and single-agent formulations. Within this framework, we evaluate five RL
algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying
complexity. The results reveal distinct performance gaps with increasing
complexity, underscoring the importance of algorithm choice and problem
formulation for CSPP. Overall, this paper benchmarks multiple RL methods for
CSPP while providing a reusable Gym environment with crane scheduling, thus
offering a foundation for future research and practical deployment in maritime
logistics.

</details>


### [21] [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
*Yuheng Wu,Azalia Mirhoseini,Thierry Tambe*

Main category: cs.AI

TL;DR: 研究表明测试时扩展的温度维度缩放能显著提升大语言模型的推理能力，通过多温度投票方法减少计算开销，使基础模型达到接近强化学习训练模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法仅通过增加样本数量来提升推理性能，但这种方法在样本数过大时效果有限，且不同采样温度能解决不同子集的问题，表明单温度扩展只探索了模型潜力的一部分。

Method: 提出温度维度缩放方法，通过在不同温度下生成推理轨迹并选择最佳结果，扩大模型的推理边界。设计了多温度投票方法来减少温度缩放的计算开销。

Result: 在Qwen3系列模型和五个代表性推理基准测试中，温度缩放比单温度测试时扩展额外提升了7.3个点。基础模型通过温度缩放能达到与强化学习训练模型相当的性能，无需额外后训练。

Conclusion: 测试时扩展比之前认为的更强大，温度缩放提供了一种简单有效的方法来释放基础模型的潜在能力。

Abstract: Large language models (LLMs) can improve reasoning at inference time through
test-time scaling (TTS), where multiple reasoning traces are generated and the
best one is selected. Prior work shows that increasing the number of samples K
steadily improves accuracy. In this paper, we demonstrate that this trend does
not hold indefinitely: at large K, further scaling yields no gains, and certain
hard questions remain unsolved regardless of the number of traces.
Interestingly, we find that different sampling temperatures solve different
subsets of problems, implying that single-temperature scaling explores only
part of a model's potential. We therefore propose scaling along the temperature
dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3
(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME
2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an
additional 7.3 points over single-temperature TTS. Temperature scaling also
enables base models to reach performance comparable to reinforcement learning
(RL)-trained counterparts, without additional post-training. We further provide
a comprehensive analysis of this phenomenon and design a multi-temperature
voting method that reduces the overhead of temperature scaling. Overall, our
findings suggest that TTS is more powerful than previously thought, and that
temperature scaling offers a simple and effective way to unlock the latent
potential of base models.

</details>


### [22] [AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models](https://arxiv.org/abs/2510.02669)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.AI

TL;DR: AutoMaAS是一个自进化的多智能体架构搜索框架，通过神经架构搜索原理自动发现最优智能体配置，实现了性能提升和推理成本降低。


<details>
  <summary>Details</summary>
Motivation: 现有自动化设计方法寻求单一解决方案，无法根据查询复杂度和领域需求自适应分配资源，需要更灵活的多智能体系统设计方法。

Method: 采用神经架构搜索原则，包含四个关键创新：自动操作符生成、融合和消除；动态成本感知优化；在线反馈集成；增强的可解释性决策追踪机制。

Result: 在六个基准测试中，AutoMaAS相比最先进方法实现了1.0-7.1%的性能提升，同时减少3-5%的推理成本，在不同数据集和LLM骨干网络上展现出优越的迁移能力。

Conclusion: AutoMaAS为大型语言模型时代的多智能体系统自动化设计建立了新范式。

Abstract: Multi-agent systems powered by large language models have demonstrated
remarkable capabilities across diverse domains, yet existing automated design
approaches seek monolithic solutions that fail to adapt resource allocation
based on query complexity and domain requirements. This paper introduces
AutoMaAS, a self-evolving multi-agent architecture search framework that
leverages neural architecture search principles to automatically discover
optimal agent configurations through dynamic operator lifecycle management and
automated machine learning techniques. Our approach incorporates four key
innovations: (1) automatic operator generation, fusion, and elimination based
on performance-cost analysis, (2) dynamic cost-aware optimization with
real-time parameter adjustment, (3) online feedback integration for continuous
architecture refinement, and (4) enhanced interpretability through decision
tracing mechanisms. Extensive experiments across six benchmarks demonstrate
that AutoMaAS achieves 1.0-7.1\% performance improvement while reducing
inference costs by 3-5\% compared to state-of-the-art methods. The framework
shows superior transferability across datasets and LLM backbones, establishing
a new paradigm for automated multi-agent system design in the era of large
language models.

</details>


### [23] [ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks](https://arxiv.org/abs/2510.02677)
*Zhaorun Chen,Xun Liu,Mintong Kang,Jiawei Zhang,Minzhou Pan,Shuang Yang,Bo Li*

Main category: cs.AI

TL;DR: 提出ARMs自适应红队代理，通过推理增强的多步骤编排自动优化多样化红队策略，有效引发目标VLM的有害输出。设计了11种新型多模态攻击策略和17种红队算法，在基准测试中达到最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型(VLM)的普及，其多模态接口引入了新的安全漏洞，现有红队方法要么局限于有限的对抗模式，要么依赖人工工程，缺乏对新兴真实世界VLM漏洞的可扩展探索。

Method: 提出ARMs自适应红队代理，采用推理增强的多步骤编排自动优化红队策略。设计了11种多模态攻击策略和17种红队算法，通过模型上下文协议集成。使用分层记忆和epsilon-greedy攻击探索算法平衡攻击多样性和有效性。

Result: 在实例和策略基准测试中，ARMs达到最先进的攻击成功率，平均超过基线52.1%，在Claude-4-Sonnet上超过90%。生成的红队实例多样性显著更高，揭示了VLM的新兴漏洞。构建了包含30K+实例的ARMs-Bench数据集。

Conclusion: ARMs能够有效揭示VLM的新兴漏洞，基于ARMs-Bench的安全微调显著提高了VLM的鲁棒性，同时保持其通用效用，为改进多模态安全对齐提供了可行指导。

Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces
also introduce new safety vulnerabilities, making the safety evaluation
challenging and critical. Existing red-teaming efforts are either restricted to
a narrow set of adversarial patterns or depend heavily on manual engineering,
lacking scalable exploration of emerging real-world VLM vulnerabilities. To
bridge this gap, we propose ARMs, an adaptive red-teaming agent that
systematically conducts comprehensive risk assessments for VLMs. Given a target
harmful behavior or risk definition, ARMs automatically optimizes diverse
red-teaming strategies with reasoning-enhanced multi-step orchestration, to
effectively elicit harmful outputs from target VLMs. We propose 11 novel
multimodal attack strategies, covering diverse adversarial patterns of VLMs
(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming
algorithms into ARMs via model context protocol (MCP). To balance the diversity
and effectiveness of the attack, we design a layered memory with an
epsilon-greedy attack exploration algorithm. Extensive experiments on instance-
and policy-based benchmarks show that ARMs achieves SOTA attack success rates,
exceeding baselines by an average of 52.1% and surpassing 90% on
Claude-4-Sonnet. We show that the diversity of red-teaming instances generated
by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.
Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety
dataset comprising over 30K red-teaming instances spanning 51 diverse risk
categories, grounded in both real-world multimodal threats and regulatory
risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness
of VLMs while preserving their general utility, providing actionable guidance
to improve multimodal safety alignment against emerging threats.

</details>


### [24] [NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning](https://arxiv.org/abs/2510.02816)
*Yulong Zhang,Li Wang,Wei Du,Peilin Li,Yuqin Dai Zhiyuan Zhao,Lingyong Fang,Ziniu Liu,Ru Zhang,Huijia Zhu,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了Node-wise Consistency Verification (NCV)框架，通过节点级一致性检查来验证大语言模型的多步推理，提高错误定位精度并减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么评估整个推理链导致注意力稀释，要么依赖昂贵多次采样，难以精确验证多步推理。

Method: 将推理链分解为互连的验证节点，进行轻量级二进制一致性检查，避免不必要长文本生成。

Result: 在公共数据集上，NCV比基线方法F1分数提升10%-25%，同时比传统CoT验证器减少6-58倍token使用。

Conclusion: NCV提供了可扩展的可靠LLM推理验证解决方案，提高了可解释性和效率。

Abstract: Verifying multi-step reasoning in large language models is difficult due to
imprecise error localization and high token costs. Existing methods either
assess entire reasoning chains, suffering attention dilution, or rely on
expensive multi-sampling. We introduce Node-wise Consistency Verification
(NCV), a training-free framework that recasts verification as lightweight
binary consistency checks at the node level. By decomposing the chain of
thought into interconnected verification nodes, NCV precisely localizes errors
and avoids unnecessary long-form generation. Experiments demonstrate that our
approach enhances interpretability and efficiency, presenting a scalable
solution for reliable LLM reasoning verification. On public datasets, NCV
achieves a 10\% to 25\% improvement in F1 scores over baselines while utilizing
$6\times$~$58\times$ fewer tokens than traditional methods like CoT-based
verifiers.

</details>


### [25] [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.02837)
*Wonjoong Kim,Sangwu Park,Yeonjun In,Sein Kim,Dongha Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: 提出了TRACE框架，用于多维度评估工具增强LLM代理的性能，通过证据库分析推理轨迹，解决了传统答案匹配评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强基准测试主要依赖答案匹配，无法评估代理在复杂任务中的推理轨迹、效率、幻觉和适应性等多维度表现。

Method: 引入TRACE框架，包含证据库积累先前推理步骤的知识，支持对代理推理轨迹的多方面分析和评估。

Result: TRACE能够准确评估复杂行为，即使使用小型开源LLM也能实现可扩展且成本效益高的评估。

Conclusion: TRACE框架为工具增强LLM代理提供了更全面的评估方法，揭示了传统评估方法无法发现的观察和洞见。

Abstract: Although recent tool-augmented benchmarks incorporate complex user requests
and diverse tools, the evaluation methods for most of them remain limited to
answer matching. However, as the number of steps required to resolve a user
request increases, a proper evaluation of an agent's performance must go beyond
the final answer to also assess the problem-solving trajectory, including
previously ignored aspects such as efficiency, hallucination, and adaptivity.
The most straightforward method for evaluating these aspects is to compare an
agent's trajectory with the ground-truth trajectory, but this approach is
fundamentally limited since annotating all valid ground-truth trajectories is
prohibitively expensive. However, a simple LLM-based evaluator struggles to
assess trajectories in detail without ground truth. To effectively evaluate the
agents in this manner, we introduce TRACE, a framework for the
multi-dimensional evaluation of tool-augmented LLM agent performance. By
incorporating an evidence bank, which accumulates knowledge gathered from
preceding reasoning steps, TRACE enables a multi-faceted analysis and
evaluation of an agent's reasoning trajectory effectively. To validate our
framework, we develop a new meta-evaluation dataset by augmenting existing
benchmarks with diverse and flawed trajectories, each labeled with
multi-faceted performance scores. Our results confirm that TRACE accurately
evaluates these complex behaviors in a scalable and cost-effective manner, even
with small open-source LLMs. Furthermore, we apply our method to evaluate the
trajectories that agents produce while solving tool-augmented tasks, presenting
previously unreported observations and their corresponding insights.

</details>


### [26] [Improving Cooperation in Collaborative Embodied AI](https://arxiv.org/abs/2510.03153)
*Hima Jacob Leven Suprabha,Laxmi Nag Laxminarayan Nagesh,Ajith Nair,Alvin Reuben Amal Selvaster,Ayan Khan,Raghuram Damarla,Sanju Hannah Samuel,Sreenithi Saravana Perumal,Titouan Puech,Venkataramireddy Marella,Vishal Sonar,Alessandro Suglia,Oliver Lemon*

Main category: cs.AI

TL;DR: 本文研究了不同提示方法在增强多智能体协作行为和决策方面的效果，通过改进CoELA框架并整合语音功能来优化LLM在多智能体系统中的协作性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型集成到多智能体系统中，为协作推理和AI代理合作开辟了新可能性，需要探索有效的提示方法来提升智能体协作能力。

Method: 增强CoELA框架，系统实验不同LLM和提示工程策略，识别优化组合以最大化协作性能，并整合语音功能实现基于语音的协作交互。

Result: 最佳组合相比原始CoELA系统，在Gemma3上运行效率提升了22%，语音集成提供了更具吸引力的用户界面。

Conclusion: 提示优化能有效提升协作智能体性能，语音集成增强了系统开发演示的交互体验。

Abstract: The integration of Large Language Models (LLMs) into multiagent systems has
opened new possibilities for collaborative reasoning and cooperation with AI
agents. This paper explores different prompting methods and evaluates their
effectiveness in enhancing agent collaborative behaviour and decision-making.
We enhance CoELA, a framework designed for building Collaborative Embodied
Agents that leverage LLMs for multi-agent communication, reasoning, and task
coordination in shared virtual spaces. Through systematic experimentation, we
examine different LLMs and prompt engineering strategies to identify optimised
combinations that maximise collaboration performance. Furthermore, we extend
our research by integrating speech capabilities, enabling seamless
collaborative voice-based interactions. Our findings highlight the
effectiveness of prompt optimisation in enhancing collaborative agent
performance; for example, our best combination improved the efficiency of the
system running with Gemma3 by 22% compared to the original CoELA system. In
addition, the speech integration provides a more engaging user interface for
iterative system development and demonstrations.

</details>


### [27] [CoDA: Agentic Systems for Collaborative Data Visualization](https://arxiv.org/abs/2510.03194)
*Zichen Chen,Jiefeng Chen,Sercan Ö. Arik,Misha Sra,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: CoDA是一个用于从自然语言查询自动生成可视化的多智能体系统，通过专门的LLM智能体进行元数据分析、任务规划、代码生成和自我反思，在复杂数据集上实现了41.5%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前系统在处理包含多个文件的复杂数据集和迭代优化时表现不佳，现有方法过度简化任务，无法有效管理数据复杂性、代码错误或最终可视化质量。

Method: 将可视化自动化重新定义为协作多智能体问题，引入CoDA系统，使用专门的LLM智能体进行元数据分析、任务规划、代码生成和自我反思，通过元数据分析绕过token限制，通过质量驱动的优化确保鲁棒性。

Result: 广泛评估显示CoDA在总体得分上取得显著提升，比竞争基线方法性能高出41.5%。

Conclusion: 可视化自动化的未来不在于孤立的代码生成，而在于集成的、协作的智能体工作流程。

Abstract: Deep research has revolutionized data analysis, yet data scientists still
devote substantial time to manually crafting visualizations, highlighting the
need for robust automation from natural language queries. However, current
systems struggle with complex datasets containing multiple files and iterative
refinement. Existing approaches, including simple single- or multi-agent
systems, often oversimplify the task, focusing on initial query parsing while
failing to robustly manage data complexity, code errors, or final visualization
quality. In this paper, we reframe this challenge as a collaborative
multi-agent problem. We introduce CoDA, a multi-agent system that employs
specialized LLM agents for metadata analysis, task planning, code generation,
and self-reflection. We formalize this pipeline, demonstrating how
metadata-focused analysis bypasses token limits and quality-driven refinement
ensures robustness. Extensive evaluations show CoDA achieves substantial gains
in the overall score, outperforming competitive baselines by up to 41.5%. This
work demonstrates that the future of visualization automation lies not in
isolated code generation but in integrated, collaborative agentic workflows.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [28] [Stripe powers instant checkout in ChatGPT, launches agentic commerce protocol with OpenAI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstripe.com%2Fen-ca%2Fnewsroom%2Fnews%2Fstripe-openai-instant-checkout%3Futm_source=tldrfintech/1/01000199a5078113-2e36a8a6-0fdd-4550-a990-58d212edd842-000000/c6CrnjZ_JsKAzZRwaD2Vljg0px__WrLfdc5pO19MJIg=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Stripe与OpenAI合作在ChatGPT中推出即时结账功能，允许美国用户直接在聊天中从Etsy商家购买商品，即将扩展到超过100万家Shopify商店。


<details>
  <summary>Details</summary>
Motivation: 为了在AI聊天环境中实现无缝的电子商务体验，让用户无需离开对话界面即可完成购买。

Method: 使用Stripe的新共享支付令牌保护凭证，同时支持欺诈检查和与商家的无缝API交接。推出开放标准的代理商务协议(ACP)。

Result: 成功在ChatGPT中部署即时结账功能，使美国用户能够直接从Etsy商家购买，并计划扩展到Shopify生态系统。

Conclusion: 这项合作标志着AI驱动的商务新时代，通过直接在聊天界面中集成支付功能，提升了用户体验和交易效率。

Abstract: Stripe powers instant checkout in ChatGPT, launches agentic commerce protocol with OpenAI (4 minute read) Stripe and OpenAI unveiled Instant Checkout in ChatGPT, which lets US users buy from Etsy merchants—and soon over a million Shopify stores—directly in the chat. Transactions use Stripe's new shared payment token, which protects credentials while enabling fraud checks and seamless API handoff to merchants. The launch also debuts the open-standard agentic commerce protocol (ACP), which give...

</details>


### [29] [PayPal Honey to add agentic commerce features by black friday](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pymnts.com%2Fnews%2Fecommerce%2F2025%2Fpaypal-honey-add-agentic-commerce-features-black-friday%2F%3Futm_source=tldrfintech/1/01000199a5078113-2e36a8a6-0fdd-4550-a990-58d212edd842-000000/4bbt3HaElz2v29YAm_nqUe2ApvfR-7XKNcUqeFa2UA8=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: PayPal正在为Honey扩展添加智能商务功能，在对话中提供实时定价、商家链接和返现优惠，计划在美国黑色星期五前推出


<details>
  <summary>Details</summary>
Motivation: 将Honey从单纯的优惠券工具转变为商务智能平台，提升用户购物体验和参与度

Method: 在Honey浏览器扩展中集成AI驱动的聊天机器人功能，首先在Chrome桌面版推出，后续扩展到其他主流浏览器

Result: 用户可以在对话中直接获取实时定价信息、商家链接和返现优惠，提升了购物便利性

Conclusion: PayPal通过AI技术升级Honey平台，使其成为更智能的商务助手

Abstract: PayPal Honey to add agentic commerce features by black friday (6 minute read) PayPal is rolling out new agentic commerce features to its Honey extension, enabling AI-powered chatbots to surface real-time pricing, merchant links, and cashback offers directly within conversations. The update, arriving before Black Friday in the US, will first be available on Chrome desktop and later on other major browsers. PayPal says this evolution transforms Honey from a coupon tool into a commerce intellige...

</details>


### [30] [The Magic of Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alephic.com%2Fwriting%2Fthe-magic-of-claude-code%3Futm_source=tldrai/1/01000199a513af84-c1f2fc43-33d9-4180-8ae1-b69ae1cd198d-000000/V_exDcsGoXxQtyC-9v_rABtpBVR-UVSzbf697U8JJTc=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code是一个基于终端的应用程序，通过牺牲易用性来获得原生Unix命令集成，使大型语言模型能够利用Unix命令进行文件系统访问、知识积累和跨对话思考。


<details>
  <summary>Details</summary>
Motivation: Unix命令非常适合大型语言模型使用，文件系统访问能力使得Claude Code能够自我记录笔记、积累知识、保持运行计数，并超越单次对话进行思考。

Method: 开发基于终端的应用程序，深度集成原生Unix命令，利用文件系统访问能力构建可靠的智能体系统。

Result: Claude Code作为一个蓝图，展示了如何构建可靠的智能体系统，因为它能够捕捉模型的能力并实现跨对话的知识积累。

Conclusion: Claude Code证明了通过Unix命令集成和文件系统访问，可以构建出能够积累知识和超越单次对话限制的可靠智能体系统。

Abstract: The Magic of Claude Code (5 minute read) Claude Code is a terminal-based application that trades accessibility for native Unix command integration. The commands that power Unix happen to be perfectly suited for use by large language models. Having file system access allows Claude Code to write notes to itself, accumulate knowledge, keep running tallies, and think beyond a single conversation. Claude Code works as a blueprint for building reliable agentic systems because it captures model capa...

</details>


### [31] [10 AI agent workflows to 10x your productivity](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fneedle.app%2Fresources%2Ftop-10-ai-agents-for-productivity%3Futm_source=tldr%26utm_medium=email%26utm_campaign=newsletter_oct3_2025/1/01000199a98a252c-88aae37d-f331-4078-b59c-ba092517cd96-000000/Uu2-8Vl14ZrQJW0X5gLAj7j2Ivm9TEjC_gRuCGSl2vQ=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 提供了10个AI代理工作流程模板，用于自动化电子邮件、日历、LinkedIn等任务，以提高生产力


<details>
  <summary>Details</summary>
Motivation: 帮助用户通过AI代理自动化日常任务，实现10倍生产力提升

Method: 提供可下载的免费模板，可在任何工具中构建使用

Result: 用户可以自动化处理邮件、日历管理、LinkedIn等平台操作

Conclusion: 通过使用这些AI代理工作流程模板，用户可以显著提高工作效率

Abstract: 10 AI agent workflows to 10x your productivity (Sponsor) Needle has recently launched its agentic automation platform - and they're giving away some insider tips on how to get the most out of your AI agents. Download these 10 free templates (that you can build out in any tool) to automate email, calendar, LinkedIn, and more. Get the templates

</details>


### [32] [How Google, Microsoft, OpenAI, and ServiceNow are thinking about AI agent security](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zenity.io%2Fresources%2Fevents%2Fai-agent-security-summit-2025%2F%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=Q3-2025-TLDR-Tech-Newsletter%26utm_content=secondary-oct3/1/01000199a99983c5-64db715f-8f0c-4d92-a838-a8f9851f9e8f-000000/AnmujlwAGYGmhjrXTtvjIYJb1RsNgYjvqi5Sqel2LjA=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 这篇论文讨论了Google、Microsoft、OpenAI和ServiceNow等公司对AI代理安全的思考，并介绍了AI代理安全峰会的相关内容。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理技术的快速发展，安全性成为最紧迫的挑战之一，需要行业领导者共同探讨解决方案。

Method: 通过举办AI代理安全峰会，汇集来自OWASP、Glean、ScaleAI、斯坦福等机构的安全专家进行3场主题演讲、6场闪电演讲和2场小组讨论。

Result: 峰会提供了平台让行业专家分享AI代理安全的最佳实践和解决方案。

Conclusion: AI代理安全是当前亟需关注的重要议题，需要行业协作共同应对。

Abstract: How Google, Microsoft, OpenAI, and ServiceNow are thinking about AI agent security (Sponsor) Find out what the world's AI leaders are doing about the most urgent security challenge. The AI Agent Security Summit (Oct 8 / San Francisco) packs a lot into one day: 3 keynotes, 6 lightning talks, and 2 panels; featuring security experts from OWASP, Glean, ScaleAI, Stanford and more. Seats are limited (literally). Save yours now

</details>


### [33] [The Case for Comment-Driven Development](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.usetusk.ai%2Fresources%2Fthe-case-for-comment-driven-development%3Futm_source=tldrwebdev/1/01000199a9d5f0bf-dfdaa5c7-7509-4fb5-a1ab-194f35d07006-000000/Lu1ubHGeinwe1-QpBC5_qP41lB9343hZvcLjaHPByrc=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: LLMs依赖并生成代码注释，但AI生成的注释往往不准确且缺乏上下文。工程师应使用详细、有上下文的注释来驱动LLMs的代码生成，这被称为注释驱动开发。


<details>
  <summary>Details</summary>
Motivation: 传统上认为好的代码是自文档化的，但LLMs依赖注释来理解和生成代码。AI生成的注释存在准确性和上下文不足的问题，需要人类工程师提供高质量的注释来改善与AI的协作。

Method: 提出注释驱动开发方法，强调工程师应该编写详细、包含上下文的注释，这些注释将作为LLMs生成代码的重要输入和指导。

Result: 通过使用详细、有上下文的注释，可以显著提高LLMs生成代码的质量和准确性，改善人机协作效率。

Conclusion: 在AI时代，注释不再是可选的，而是必要的协作工具。工程师需要改变传统观念，采用注释驱动开发来充分发挥LLMs的潜力。

Abstract: The Case for Comment-Driven Development (8 minute read) While traditionally good code was considered self-documenting, LLMs rely on and generate comments, making them necessary for good AI collaboration. However, AI-generated comments are often inaccurate and don't have good context. Instead, engineers should use detailed, contextual comments to help drive the code generation by LLMs.

</details>


### [34] [The RAG Obituary: Killed by Agents, Buried by Context Windows](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nicolasbustamante.com%2Fp%2Fthe-rag-obituary-killed-by-agents%3Futm_source=tldrfounders/1/01000199a9f931b2-e02ac2cb-9938-4b1d-affb-3f07ef4e96cc-000000/Pb0zqc1YbsJwzmI3hJoB21nutA1F88v8Y3320qUaWZM=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 传统RAG系统被更简单的基于grep的文件搜索方法超越，表明复杂的检索基础设施可能已不再必要


<details>
  <summary>Details</summary>
Motivation: 揭示传统RAG系统可能过度复杂化，而简单的文件搜索方法在AI时代可能更有效

Method: 对比传统RAG系统（文档分块、向量化、搜索、重排序）与直接使用grep搜索文件的方法

Result: 使用Claude Code和grep直接搜索文件获得了更好的结果，传统RAG系统显得多余

Conclusion: 复杂的检索基础设施可能正在解决一个不再存在的问题，更简单的方法可能更有效

Abstract: The RAG Obituary: Killed by Agents, Buried by Context Windows (4 minute read) A founder built a financial search engine using the standard playbook - break documents into chunks, turn them into vectors, search, rerank, feed results to an AI. It took three years and constant maintenance. Then he tried Claude Code and realized it just searched files directly with grep, a tool from 1973, and got better results. The entire retrieval infrastructure was solving a problem that no longer exists. Earl...

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning](https://arxiv.org/abs/2510.02590)
*Ahmed Hendawy,Henrik Metternich,Théo Vincent,Mahdi Kallel,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: MINTO是一种新的深度强化学习方法，通过在线网络和目标网络的最小值估计来计算目标值，解决了传统目标网络更新慢和在线网络不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 传统目标网络虽然稳定但学习速度慢，而使用在线网络作为引导目标虽然直观但会导致学习不稳定。作者希望结合两者的优点。

Method: 引入MINTO更新规则，使用目标网络和在线网络的最小值估计来计算目标值，这种方法可以无缝集成到各种基于值和演员-评论家算法中。

Result: 在多种基准测试中，包括在线和离线强化学习、离散和连续动作空间，MINTO都显著提升了性能表现。

Conclusion: MINTO通过简单有效的修改，实现了更快更稳定的价值函数学习，具有广泛的适用性和有效性。

Abstract: The use of target networks is a popular approach for estimating value
functions in deep Reinforcement Learning (RL). While effective, the target
network remains a compromise solution that preserves stability at the cost of
slowly moving targets, thus delaying learning. Conversely, using the online
network as a bootstrapped target is intuitively appealing, albeit well-known to
lead to unstable learning. In this work, we aim to obtain the best out of both
worlds by introducing a novel update rule that computes the target using the
MINimum estimate between the Target and Online network, giving rise to our
method, MINTO. Through this simple, yet effective modification, we show that
MINTO enables faster and stable value function learning, by mitigating the
potential overestimation bias of using the online network for bootstrapping.
Notably, MINTO can be seamlessly integrated into a wide range of value-based
and actor-critic algorithms with a negligible cost. We evaluate MINTO
extensively across diverse benchmarks, spanning online and offline RL, as well
as discrete and continuous action spaces. Across all benchmarks, MINTO
consistently improves performance, demonstrating its broad applicability and
effectiveness.

</details>


### [36] [RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization](https://arxiv.org/abs/2510.02695)
*Kai Fukazawa,Kunal Mundada,Iman Soltani*

Main category: cs.LG

TL;DR: 提出RAMAC框架，结合表达性生成actor和分布critic，在离线强化学习中实现风险敏感学习，在保持高回报的同时降低尾部风险。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，离线强化学习需要平衡高回报和低风险，但现有风险规避方法往往过于保守或限制策略表达能力。

Method: 使用表达性生成actor（扩散和流匹配模型）与分布critic结合，通过复合目标函数实现风险敏感学习。

Result: 在Stochastic-D4RL任务上，RAMAC在保持强回报的同时，显著提高了CVaR0.1指标。

Conclusion: RAMAC框架成功解决了离线强化学习中风险敏感性与策略表达能力之间的权衡问题。

Abstract: In safety-critical domains where online data collection is infeasible,
offline reinforcement learning (RL) offers an attractive alternative but only
if policies deliver high returns without incurring catastrophic lower-tail
risk. Prior work on risk-averse offline RL achieves safety at the cost of value
conservatism and restricted policy classes, whereas expressive policies are
only used in risk-neutral settings. Here, we address this gap by introducing
the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which
couples an \emph{expressive generative actor} with a distributional critic. The
RAMAC differentiates composite objective combining distributional risk and BC
loss through the generative path, achieving risk-sensitive learning in complex
multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching
actors and observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining
strong returns on most Stochastic-D4RL tasks. Code:
https://github.com/KaiFukazawa/RAMAC.git

</details>


### [37] [RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning](https://arxiv.org/abs/2510.02892)
*Aleksei Arzhantsev,Otmane Sakhi,Flavian Vasile*

Main category: cs.LG

TL;DR: 提出了RoiRL方法，使用离线迭代强化学习替代传统的在线强化学习，显著降低了计算成本，在推理任务上表现优于TTRL方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需要真实奖励信号，而测试时强化学习(TTRL)虽然使用多数投票奖励但计算成本高昂，需要寻找更轻量级的替代方案。

Method: RoiRL采用离线迭代强化学习方法，优化加权对数似然目标，无需维护参考模型，降低了内存和计算需求。

Result: 实验结果显示RoiRL训练速度比TTRL快2.5倍，在推理基准测试中表现一致优于TTRL。

Conclusion: RoiRL为无标签的自改进大语言模型提供了一条可扩展的路径。

Abstract: Reinforcement learning (RL) is central to improving reasoning in large
language models (LLMs) but typically requires ground-truth rewards. Test-Time
Reinforcement Learning (TTRL) removes this need by using majority-vote rewards,
but relies on heavy online RL and incurs substantial computational cost. We
propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a
family of lightweight offline learning alternatives that can target the same
regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to
maintain a reference model and instead optimizes weighted log-likelihood
objectives, enabling stable training with significantly lower memory and
compute requirements. Experimental results show that RoiRL trains to 2.5x
faster and consistently outperforms TTRL on reasoning benchmarks, establishing
a scalable path to self-improving LLMs without labels.

</details>


### [38] [Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning](https://arxiv.org/abs/2510.02945)
*Juan Sebastian Rojas,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: 本文首次从风险感知决策的角度对持续强化学习进行理论处理，提出了与持续学习兼容的遍历风险度量新类别。


<details>
  <summary>Details</summary>
Motivation: 现有持续强化学习研究主要关注风险中性决策，缺乏对风险感知决策的理论处理。作者发现经典风险度量理论与持续设置不兼容，需要新的理论框架。

Method: 扩展风险度量理论到持续设置，引入遍历风险度量新类别，并通过案例研究和实证结果验证其有效性。

Result: 提出的遍历风险度量在风险感知持续学习中展现出直观吸引力和理论合理性。

Conclusion: 遍历风险度量为风险感知持续强化学习提供了兼容的理论基础，填补了该领域的理论空白。

Abstract: Continual reinforcement learning (continual RL) seeks to formalize the
notions of lifelong learning and endless adaptation in RL. In particular, the
aim of continual RL is to develop RL agents that can maintain a careful balance
between retaining useful information and adapting to new situations. To date,
continual RL has been explored almost exclusively through the lens of
risk-neutral decision-making, in which the agent aims to optimize the expected
(or mean) long-run performance. In this work, we present the first formal
theoretical treatment of continual RL through the lens of risk-aware
decision-making, in which the agent aims to optimize a reward-based measure of
long-run performance beyond the mean. In particular, we show that the classical
theory of risk measures, widely used as a theoretical foundation in
non-continual risk-aware RL, is, in its current form, incompatible with the
continual setting. Then, building on this insight, we extend risk measure
theory into the continual setting by introducing a new class of ergodic risk
measures that are compatible with continual learning. Finally, we provide a
case study of risk-aware continual learning, along with empirical results,
which show the intuitive appeal and theoretical soundness of ergodic risk
measures.

</details>


### [39] [A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem](https://arxiv.org/abs/2510.03065)
*Mingfeng Fan,Jiaqi Cheng,Yaoxin Wu,Yifeng Zhang,Yibin Yang,Guohua Wu,Guillaume Sartoretti*

Main category: cs.LG

TL;DR: 提出了一种统一的双解码器深度强化学习框架UD3RL，用于解决接近旅行商问题(CETSP)，该框架将决策分为节点选择和路径点确定两个子任务。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法主要关注标准旅行商问题，而对接近旅行商问题关注较少，主要是因为其基于邻域的访问准则带来的挑战。

Method: 使用离散化方案构建马尔可夫决策过程，提出UD3RL框架，包含特征提取编码器、节点解码器和位置解码器，并引入k近邻子图交互策略增强空间推理。

Result: 实验结果表明UD3RL在解质量和运行时间上优于传统方法，并在问题规模、空间分布和半径范围等方面表现出良好的泛化能力。

Conclusion: UD3RL是一个统一的模型，能够泛化到不同问题规模和半径类型，并对动态环境具有鲁棒性。

Abstract: In recent years, deep reinforcement learning (DRL) has gained traction for
solving the NP-hard traveling salesman problem (TSP). However, limited
attention has been given to the close-enough TSP (CETSP), primarily due to the
challenge introduced by its neighborhood-based visitation criterion, wherein a
node is considered visited if the agent enters a compact neighborhood around
it. In this work, we formulate a Markov decision process (MDP) for CETSP using
a discretization scheme and propose a novel unified dual-decoder DRL (UD3RL)
framework that separates decision-making into node selection and waypoint
determination. Specifically, an adapted encoder is employed for effective
feature extraction, followed by a node-decoder and a loc-decoder to handle the
two sub-tasks, respectively. A k-nearest neighbors subgraph interaction
strategy is further introduced to enhance spatial reasoning during location
decoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a
unified model capable of generalizing across different problem sizes and
varying neighborhood radius types (i.e., constant and random radii).
Experimental results show that UD3RL outperforms conventional methods in both
solution quality and runtime, while exhibiting strong generalization across
problem scales, spatial distributions, and radius ranges, as well as robustness
to dynamic environments.

</details>


### [40] [Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.03181)
*Ha Manh Bui,Felix Parker,Kimia Ghobadi,Anqi Liu*

Main category: cs.LG

TL;DR: 提出了Density-QUCB算法，通过转移密度函数检测分布漂移，改进Q-learning UCB的不确定性估计，在非平稳强化学习中平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳强化学习环境中分布漂移导致策略在变化后利用次优奖励的问题。

Method: 使用转移密度函数检测分布漂移，并利用其似然增强Q-learning UCB的不确定性估计质量。

Result: 理论证明DQUCB比QUCB获得更好的遗憾保证，实验显示在RL任务和COVID-19患者分配任务中具有更低遗憾。

Conclusion: DQUCB结合了模型无关RL的计算效率，在分布漂移环境中表现优于基准方法。

Abstract: We study the Non-Stationary Reinforcement Learning (RL) under distribution
shifts in both finite-horizon episodic and infinite-horizon discounted Markov
Decision Processes (MDPs). In the finite-horizon case, the transition functions
may suddenly change at a particular episode. In the infinite-horizon setting,
such changes can occur at an arbitrary time step during the agent's interaction
with the environment. While the Q-learning Upper Confidence Bound algorithm
(QUCB) can discover a proper policy during learning, due to the distribution
shifts, this policy can exploit sub-optimal rewards after the shift happens. To
address this issue, we propose Density-QUCB (DQUCB), a shift-aware
Q-learning~UCB algorithm, which uses a transition density function to detect
distribution shifts, then leverages its likelihood to enhance the uncertainty
estimation quality of Q-learning~UCB, resulting in a balance between
exploration and exploitation. Theoretically, we prove that our oracle DQUCB
achieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the
computational efficiency of model-free RL and outperforms QUCB baselines by
having a lower regret across RL tasks, as well as a real-world COVID-19 patient
hospital allocation task using a Deep-Q-learning architecture.

</details>


### [41] [Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling](https://arxiv.org/abs/2510.03199)
*Qiwei Di,Kaixuan Ji,Xuheng Li,Heyang Zhao,Quanquan Gu*

Main category: cs.LG

TL;DR: 本文提出了Best-of-Majority (BoM)推理策略，结合多数投票和Best-of-N的优势，在Pass@k设置下实现最优推理扩展，证明其具有最小化最优性。


<details>
  <summary>Details</summary>
Motivation: 传统单次选择策略（如多数投票和Best-of-N）在困难任务中表现不佳，Pass@k评估允许提交多个响应，但现有策略在推理扩展方面存在不足。

Method: 提出BoM策略，首先限制候选集为高频响应，然后选择前k个奖励最高的响应，结合多数投票和BoN的优点。

Result: 理论证明当采样预算N=Ω(C*)时，BoM的遗憾为O(ε_opt+√(ε_RM²C*/k))，并建立了匹配下界，实验结果显示BoM在数学问题上优于多数投票和BoN。

Conclusion: BoM是首个在Pass@k设置下实现最小化最优的推理策略，其性能不会随N增加而下降，优于现有方法。

Abstract: LLM inference often generates a batch of candidates for a prompt and selects
one via strategies like majority voting or Best-of- N (BoN). For difficult
tasks, this single-shot selection often underperforms. Consequently,
evaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,
and only the best of them is used when computing regret. Motivated by this, we
study inference scaling in the more general Pass@$k$ inference setting, and
prove that neither majority voting nor BoN exhibits the desirable scaling with
$k$ and the sampling budget $N$. Combining the advantages of majority voting
and BoN, we propose a new inference strategy called Best-of-Majority (BoM),
with a pivotal step that restricts the candidates to the responses with high
frequency in the $N$ samples before selecting the top-$k$ rewards. We prove
that when the sampling budget is $N=\tilde\Omega(C^*)$, the regret of BoM is
$O(\epsilon_{\mathrm{opt}}+\sqrt{\epsilon_{\mathrm{RM}}^2C^*/k})$, where $C^*$
is the coverage coefficient, $\epsilon_{\mathrm{RM}}$ is the estimation error
of the reward model, and $\epsilon_{\mathrm{opt}}$ is the estimation error of
reward at the optimal response. We further establish a matching lower bound,
certifying that our algorithm is minimax optimal. Beyond optimality, BoM has a
key advantage: unlike majority voting and BoN, its performance does not degrade
when increasing $N$. Experimental results of inference on math problems show
BoM outperforming both majority voting and BoN.

</details>


### [42] [Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2510.03222)
*Guanhua Huang,Tingqiang Xu,Mingze Wang,Qi Yi,Xue Gong,Siheng Li,Ruibin Xiong,Kejiao Li,Yuhao Jiang,Bo Zhou*

Main category: cs.LG

TL;DR: 本文提出Lp-Reg方法，通过正则化保护低概率推理火花，解决RLVR训练中探索退化问题，在数学基准上实现60.17%的准确率。


<details>
  <summary>Details</summary>
Motivation: RLVR训练中存在探索瓶颈，高概率策略会消除有价值的低概率推理火花，导致探索退化。

Method: 引入低概率正则化(Lp-Reg)，构建去噪代理分布作为正则化目标，通过KL散度保护推理火花不被过度惩罚。

Result: Lp-Reg支持约1000步的稳定训练，在五个数学基准上达到60.17%的平均准确率，比现有方法提升2.66%。

Conclusion: 保护低概率推理火花对维持RLVR探索至关重要，Lp-Reg能有效防止探索退化并提升性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large
Language Models in complex reasoning, yet its scalability is often hindered by
a training bottleneck where performance plateaus as policy entropy collapses,
signaling a loss of exploration. Previous methods typically address this by
maintaining high policy entropy, yet the precise mechanisms that govern
meaningful exploration have remained underexplored. Our analysis suggests that
an unselective focus on entropy risks amplifying irrelevant tokens and
destabilizing training. This paper investigates the exploration dynamics within
RLVR and identifies a key issue: the gradual elimination of valuable
low-probability exploratory tokens, which we term \textbf{\textit{reasoning
sparks}}. We find that while abundant in pre-trained models, these sparks are
systematically extinguished during RLVR due to over-penalization, leading to a
degeneracy in exploration. To address this, we introduce Low-probability
Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a
heuristic proxy distribution. This proxy is constructed by filtering out
presumed noise tokens and re-normalizing the distribution over the remaining
candidates. The result is a less-noisy proxy where the probability of
\textit{reasoning sparks} is amplified, which then serves as a soft
regularization target to shield these valuable tokens from elimination via KL
divergence. Experiments show that Lp-Reg enables stable on-policy training for
around 1,000 steps, a regime where baseline entropy-control methods collapse.
This sustained exploration leads to state-of-the-art performance, achieving a
$60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$
over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.

</details>


### [43] [To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable Reinforcement Learning](https://arxiv.org/abs/2510.03207)
*Yuda Song,Dhruv Rohatgi,Aarti Singh,J. Andrew Bagnell*

Main category: cs.LG

TL;DR: 本文研究了在部分可观测强化学习中特权专家蒸馏与标准RL之间的权衡，发现潜在动态的随机性影响算法选择，且最优潜在策略并非总是最佳蒸馏目标。


<details>
  <summary>Details</summary>
Motivation: 部分可观测性给强化学习带来挑战，特权专家蒸馏虽能提高效率但存在失败模式，需要深入理解其与标准RL的权衡关系。

Method: 通过理论模型（扰动块MDP）和模拟运动任务的对照实验，分析特权专家蒸馏与无特权信息标准RL的算法权衡。

Result: 实验表明权衡取决于潜在动态的随机性，且最优潜在策略不总是最佳蒸馏目标，这与理论预测一致。

Conclusion: 提出了有效利用特权信息的新指导原则，有望提升部分可观测领域策略学习的效率。

Abstract: Partial observability is a notorious challenge in reinforcement learning
(RL), due to the need to learn complex, history-dependent policies. Recent
empirical successes have used privileged expert distillation--which leverages
availability of latent state information during training (e.g., from a
simulator) to learn and imitate the optimal latent, Markovian policy--to
disentangle the task of "learning to see" from "learning to act". While expert
distillation is more computationally efficient than RL without latent state
information, it also has well-documented failure modes. In this paper--through
a simple but instructive theoretical model called the perturbed Block MDP, and
controlled experiments on challenging simulated locomotion tasks--we
investigate the algorithmic trade-off between privileged expert distillation
and standard RL without privileged information. Our main findings are: (1) The
trade-off empirically hinges on the stochasticity of the latent dynamics, as
theoretically predicted by contrasting approximate decodability with belief
contraction in the perturbed Block MDP; and (2) The optimal latent policy is
not always the best latent policy to distill. Our results suggest new
guidelines for effectively exploiting privileged information, potentially
advancing the efficiency of policy learning across many practical partially
observable domains.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [44] [前沿速递 | Nature 2025: 解锁AI推理潜能：DeepSeek-R1基于<em class="highlight">强化学习</em>的突破性实践](http://mp.weixin.qq.com/s?__biz=MzkyNzQ5ODc3Mg==&mid=2247488766&idx=1&sn=a7a5c94bda3e2f0776649991afc36a34&chksm=c35b032487b2e4fc024c149251d9326a5f462b505763ea02a7eb18b303307a9d70790b55eca5#rd)
*AI Evolution-Geohazards AIEG*

Main category: wechat.article

TL;DR: 三、结果结论 1.纯粹强化学习能有效“解锁”模型的内在推理潜能 研究明确指出，预训练好的大模型本身已蕴含巨大的推理潜力。解锁这种潜力的关键，并非更大规模的人工标注，而是提供一个允许模型自由探索和试错的强化


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 三、结果结论 1.纯粹强化学习能有效“解锁”模型的内在推理潜能 研究明确指出，预训练好的大模型本身已蕴含巨大的推理潜力。解锁这种潜力的关键，并非更大规模的人工标注，而是提供一个允许模型自由探索和试错的强化

</details>


### [45] [优化｜基于图和<em class="highlight">强化学习</em>的组合优化算法](http://mp.weixin.qq.com/s?__biz=Mzk0ODMwMjMwMA==&mid=2247693133&idx=1&sn=7e1d1438dbf7761101447f5c2df3ac22&chksm=c2564fd58f3135ab2ce1e0352f7387d02c70705beeede1526effb10f3ce5663c6c9d168c8e50#rd)
*运筹OR帷幄*

Main category: wechat.article

TL;DR: 这一部分把学习过程明确为一个强化学习问题：评价函数 被当作需要学习的“状态—动作价值函数”，其输入一方面是带有“当前部分解”标签的图状态，另一方面是候选节点的嵌入。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这一部分把学习过程明确为一个强化学习问题：评价函数 被当作需要学习的“状态—动作价值函数”，其输入一方面是带有“当前部分解”标签的图状态，另一方面是候选节点的嵌入。

</details>


### [46] [大模型的智能体转向：Agentic <em class="highlight">强化学习</em>全景综述](http://mp.weixin.qq.com/s?__biz=MzIwOTA1MDAyNA==&mid=2650040841&idx=3&sn=96069670ba310b4a4a83b71e698a5aa6&chksm=8ef91048d87d95dab19e79726465fcc5c85c02096242c0262f2ae86ed0059e30fc9a9acaa17e#rd)
*人工智能学家*

Main category: wechat.article

TL;DR: 强化学习通过奖励信号引导模型生成更稳定、更具逻辑一致性的推理路径。自我改进（Self-Improvement）智能体能够通过经验积累进行反思、自我修正，形成闭环学习机制。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习通过奖励信号引导模型生成更稳定、更具逻辑一致性的推理路径。自我改进（Self-Improvement）智能体能够通过经验积累进行反思、自我修正，形成闭环学习机制。

</details>


### [47] [Kevin P. Murphy 版<em class="highlight">强化学习</em>综述](http://mp.weixin.qq.com/s?__biz=MzU3MzM4NjI0NQ==&mid=2247524142&idx=1&sn=720bacb3f1c72572a61430156169e12a&chksm=fd91ed93d6cbbf3859d05ed75f7461857077a6cd927a1996d3c0295752a00869037672f28609#rd)
*苏哲管理咨询*

Main category: wechat.article

TL;DR: 问题 1：强化学习中价值基、策略基、模型基三大范式的核心区别是什么？各自适用于哪些场景？答：核心区别 价值基 RL核心是学习价值函数（V/Q 函数），通过 Bellman 方程迭代优化，间接推导策略（如\（a=\argmax_a Q（s，a）\））


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 问题 1：强化学习中价值基、策略基、模型基三大范式的核心区别是什么？各自适用于哪些场景？答：核心区别 价值基 RL核心是学习价值函数（V/Q 函数），通过 Bellman 方程迭代优化，间接推导策略（如\（a=\argmax_a Q（s，a）\））

</details>


### [48] [<em class="highlight">强化学习</em>，推动自动驾驶升级](http://mp.weixin.qq.com/s?__biz=MzA4ODUzMTYyOA==&mid=2650823486&idx=1&sn=c7b6b4d576116c121f96c1761cc35ccf&chksm=8aded318078ab074c41e5a3423c3683e855e7ab7b2be338c491ac61dea2c9e31d1ad52da9dfe#rd)
*Myautotime*

Main category: wechat.article

TL;DR: 强化学习通过奖励机制和因果推理、数据生成和增强、仿真试错和持续探索克服了模仿学习的三大缺陷。不过，强化学习和模仿学习之间并非非此即彼的替代关系，而是协同应用、再接再厉的合作关系。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习通过奖励机制和因果推理、数据生成和增强、仿真试错和持续探索克服了模仿学习的三大缺陷。不过，强化学习和模仿学习之间并非非此即彼的替代关系，而是协同应用、再接再厉的合作关系。

</details>


### [49] [【AI】全新合成框架SOTA：<em class="highlight">强化学习</em>当引擎，任务合成当燃料，蚂蚁港大联合出品](http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247563507&idx=1&sn=310341fdcee3b0aa24baf63fa36db11e&chksm=fc8c728dbb9e7527465362b40d9bf152a9e75ec944d510c6912bff061a84ae4ee97f9e62fd5d#rd)
*人工智能产业链union*

Main category: wechat.article

TL;DR: 一是强化学习。作为强化学习之年，该项技术已经得到社区足够多的关注与投入，无论是方法还是框架都在急速推进。reinforcement learning 而另一个，团队认为是任务合成。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一是强化学习。作为强化学习之年，该项技术已经得到社区足够多的关注与投入，无论是方法还是框架都在急速推进。reinforcement learning 而另一个，团队认为是任务合成。

</details>


### [50] [100页综述详细介绍Agentic<em class="highlight">强化学习</em>范式！](http://mp.weixin.qq.com/s?__biz=Mzg5NzU2OTc2OQ==&mid=2247487081&idx=1&sn=4899ed5ded9722062e56318d101ac749&chksm=c12399231149bef5b8abde51fe31d3eb67fd0a0134961f4756cc66866d7b87fdec095895ef0b#rd)
*NLP学习加油站*

Main category: wechat.article

TL;DR: 在传统的 llm 强化学习（rlhf、dpo 等）中，语言模型被视为单轮输出的生成器，核心目标是“答得更符合人类偏好”。这种范式虽然推动了 ChatGPT 的成功，但仍局限于 单步决策，缺乏长期互动能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在传统的 llm 强化学习（rlhf、dpo 等）中，语言模型被视为单轮输出的生成器，核心目标是“答得更符合人类偏好”。这种范式虽然推动了 ChatGPT 的成功，但仍局限于 单步决策，缺乏长期互动能力。

</details>


### [51] [机器学习四大核心学习策略：监督学习、无监督学习、半监督学习与<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzI4MDE2MTI4OA==&mid=2653141067&idx=1&sn=29f58c5e7113202ea1f0e308eef23c0c&chksm=f130e0c2c65ca6e41012794b2ef0ff930f1e01db902f42fd54377e63d973d839dbef196fbed5#rd)
*金融IT那些事儿*

Main category: wechat.article

TL;DR: 在长期发展中，形成了监督学习、无监督学习、半监督学习与强化学习四大核心学习策略。这四种策略基于数据标注状态与学习交互模式的差异，适用于不同的业务场景，共同构成了机器学习技术落地的核心框架。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在长期发展中，形成了监督学习、无监督学习、半监督学习与强化学习四大核心学习策略。这四种策略基于数据标注状态与学习交互模式的差异，适用于不同的业务场景，共同构成了机器学习技术落地的核心框架。

</details>


### [52] [DRL圣经2025最新版-《<em class="highlight">强化学习</em>:导论第二版》免费pdf分享](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570938&idx=2&sn=328710b79fe1406b3e5f07698830003b&chksm=965847cc97744328823a29284b1dc7694609ca0ab7a9f15eaab31e0aedaed1ff4feba52e6303#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 我们第二版的目标和第一版的目标是一样的：为强化学习的关键思想和算法提供一个清晰而简单的描述，供所有相关学科的读者阅读。该版本仍然是一个介绍，我们保留了核心，在线学习算法的重点。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们第二版的目标和第一版的目标是一样的：为强化学习的关键思想和算法提供一个清晰而简单的描述，供所有相关学科的读者阅读。该版本仍然是一个介绍，我们保留了核心，在线学习算法的重点。

</details>


### [53] [驯服<em class="highlight">代理</em>式AI（<em class="highlight">Agentic</em> AI）i<em class="highlight">智能体</em>：2026年的自主劳动力](http://mp.weixin.qq.com/s?__biz=MzkyNjYzNTIwNw==&mid=2247493303&idx=1&sn=8dd2c0ff5c48a298fffbe9c1445ffa77&chksm=c37ffcb9c874d06a1706d0de21ee2f3417ee95a5c084da52a6a2c446104769437761634f52cb#rd)
*CIOCDO*

Main category: wechat.article

TL;DR: 基础层：具备原子功能的微智能体（如转录员、Jira工单提取器、航班改签器）。中间层：工具集成商（拥有手术刀般精准权限的MCP服务器）。顶层： 编排者智能体（负责拆分任务、管理回退机制、并向人类上报）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 基础层：具备原子功能的微智能体（如转录员、Jira工单提取器、航班改签器）。中间层：工具集成商（拥有手术刀般精准权限的MCP服务器）。顶层： 编排者智能体（负责拆分任务、管理回退机制、并向人类上报）。

</details>


### [54] [<em class="highlight">Agentic</em> AI如何重构企业生产力？](http://mp.weixin.qq.com/s?__biz=Mzg3NTgyMDcwNw==&mid=2247489798&idx=1&sn=1cea8fdb0213f017d5400c9c90ed6c8e&chksm=cecd68d70d81ca60ed6ceefe8ff2a1fa5257c06ccdfd358484a5a502f63a8f0292201f277b73#rd)
*云头版*

Main category: wechat.article

TL;DR: Agentic AI的崛起有其深刻的产业逻辑。过去几年，AI经历了从预测AI（用于欺诈检测、风险监控）到助手AI（如聊天机器人）的发展阶段，而如今，企业需要的不再是"回答问题"的AI，而是"解决问题"的AI。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI的崛起有其深刻的产业逻辑。过去几年，AI经历了从预测AI（用于欺诈检测、风险监控）到助手AI（如聊天机器人）的发展阶段，而如今，企业需要的不再是"回答问题"的AI，而是"解决问题"的AI。

</details>


### [55] [<em class="highlight">Agentic</em> AI下的组织    20251006](http://mp.weixin.qq.com/s?__biz=MzAwMDY1MjQ0OQ==&mid=2247483839&idx=1&sn=1d6fc71a34dda97b2f2409c7c7c4d984&chksm=9ba2aae0589e7782f9557cf80b4bf26377d789059a7b04a7ed6037fa757cb3c72caca3cc393d#rd)
*会说话的鼓*

Main category: wechat.article

TL;DR: https：//www.mckinsey.com/capabilities/quantumblack/our-insights/one-year-of-agentic-ai-six-lessons-from-the-people-doing-the-work在企业部署AI代理（Agentic AI）以追求生产力跃升的过程中，将重点放在“优化现有工作流”、“构建系统性信任”和“实现超级


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: https：//www.mckinsey.com/capabilities/quantumblack/our-insights/one-year-of-agentic-ai-six-lessons-from-the-people-doing-the-work在企业部署AI代理（Agentic AI）以追求生产力跃升的过程中，将重点放在“优化现有工作流”、“构建系统性信任”和“实现超级

</details>


### [56] [<em class="highlight">代理</em>式 AI 的秘密武器：5 大多<em class="highlight">代理</em>框架，谁将称霸未来？](http://mp.weixin.qq.com/s?__biz=MzIwMDE2MzkwMg==&mid=2653357210&idx=1&sn=277e97bfb293bf5c777b799e83f8f4f4&chksm=8cf6553c42e07c5a4145cec47b664765c8965960ba9b27a9c7368fe6f29036879b67af108d98#rd)
*AI Agent 领域*

Main category: wechat.article

TL;DR: Agentic AI生成式 AI 在过去几年取得了惊人进步，从简单的基于规则的系统，演变为能够处理文本、图像、音频和视频的复杂模型。随着音频和视频的融入，AI 的能力扩展到医疗研究等关键行业，例如通过 MRI、X 光和 CT 扫描图像识


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI生成式 AI 在过去几年取得了惊人进步，从简单的基于规则的系统，演变为能够处理文本、图像、音频和视频的复杂模型。随着音频和视频的融入，AI 的能力扩展到医疗研究等关键行业，例如通过 MRI、X 光和 CT 扫描图像识

</details>


### [57] [<em class="highlight">Agentic</em> 设计模式](http://mp.weixin.qq.com/s?__biz=MzkwNTQ0NDY1NQ==&mid=2247483808&idx=1&sn=a4ba05712f57f0f4116ac3711cdf90b0&chksm=c1e1e7ca479282671bb02c50b4e7a0fe88c6dcdb182e8b1333eb68af5cf827441b7e912b757a#rd)
*时间飘过*

Main category: wechat.article

TL;DR: Agentic 设计模式一位谷歌高级工程师刚刚发布了一份长达 424 页的文档，名为《Agentic 设计模式》。每一章都有代码支持，涵盖人工智能系统的前沿：→ Prompt chaining， routing， memory


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic 设计模式一位谷歌高级工程师刚刚发布了一份长达 424 页的文档，名为《Agentic 设计模式》。每一章都有代码支持，涵盖人工智能系统的前沿：→ Prompt chaining， routing， memory

</details>


### [58] [【重点】2025<em class="highlight">大模型</em>技术白皮书|附下载](http://mp.weixin.qq.com/s?__biz=MzIyMTAwMzk1Mg==&mid=2651221633&idx=1&sn=53b20e0ad414d0bf29493660c7c57375&chksm=8dc5ec5d48d54d4fc76e9917938a7304dcb546a9758711816be1bf34215a7381064403f025f9#rd)
*锋行链盟*

Main category: wechat.article

TL;DR: 大模型高阶实施策略与路径 04。 大模型行业案例分享。大模型行业应用发展：跨越拐点，加速进入大模型时代。ai大模型技术快速成熟，ai算法与应用的开发、上线部署与业务发放等过程均大幅简化。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型高阶实施策略与路径 04。 大模型行业案例分享。大模型行业应用发展：跨越拐点，加速进入大模型时代。ai大模型技术快速成熟，ai算法与应用的开发、上线部署与业务发放等过程均大幅简化。

</details>


### [59] [腾讯混元图像3.0暴击全球26个<em class="highlight">大模型</em>，碾压谷歌OpenAI登顶文生图王座！80B参数开源炸场，中秋海报直接封神！](http://mp.weixin.qq.com/s?__biz=MzkzMjcxODI0Mg==&mid=2247511362&idx=1&sn=56e14287a5b9f6fe91765326fa2fc9e5&chksm=c3c15a798c1b0b1449989ff815183546be8a29a2885efa459c778eef947a1dbec2f5ca705228#rd)
*半夜科技馆*

Main category: wechat.article

TL;DR: 在26个全球顶级大模型中强势登顶第一！这是中国AI模型首次夺得文生图领域全球冠军宝座，LMArena官方更是直接发文祝贺这一"巨大成就"。lmage caption model caption accepted bidirectional verification ocr 3 x ocr ip ocr world agent agent info knowledge rej


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在26个全球顶级大模型中强势登顶第一！这是中国AI模型首次夺得文生图领域全球冠军宝座，LMArena官方更是直接发文祝贺这一"巨大成就"。lmage caption model caption accepted bidirectional verification ocr 3 x ocr ip ocr world agent agent info knowledge rej

</details>


### [60] [<em class="highlight">大模型</em>面试题——Agent如何搭建?可以用那些框架](http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247491815&idx=1&sn=99c7b5567a155ea99619ea5b56c855e5&chksm=fa00fb5aac8509dc1eaa24b2f3433b246a96ee96f0aa2f21ffb3cf9fa6c7c5e13b3af6d5743c#rd)
*慕容千语*

Main category: wechat.article

TL;DR: 核心模型：选择基础大模型（如GPT-4、Claude、LLAMA 2）作为推理引擎角色设定：通过System Prompt定义Agent角色（如“你是一个资深数据分析师”）微调需求：是否需要领域微调（如医疗、金融场景）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 核心模型：选择基础大模型（如GPT-4、Claude、LLAMA 2）作为推理引擎角色设定：通过System Prompt定义Agent角色（如“你是一个资深数据分析师”）微调需求：是否需要领域微调（如医疗、金融场景）

</details>


### [61] [B 站基于<em class="highlight">大模型</em>的大数据智能诊断助手实践](http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771062&idx=4&sn=4bc51164d5f1aa7d518185eeb1bbdfb5&chksm=fab293da3a8d8c37e543897b97b00efa1a5c6f54b955c8921d4fe0b8f795a7bf277f6ec9dfbb#rd)
*DataFunTalk*

Main category: wechat.article

TL;DR: 导读 本文将分享 B 站基于大语言模型的智能体助手实践。分享嘉宾｜郭跃鹏 哔哩哔哩 软件工程师编辑整理｜汪维内容校对｜李瑶1. 整体架构和规模


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 导读 本文将分享 B 站基于大语言模型的智能体助手实践。分享嘉宾｜郭跃鹏 哔哩哔哩 软件工程师编辑整理｜汪维内容校对｜李瑶1. 整体架构和规模

</details>


### [62] [腾讯<em class="highlight">大模型</em>在网址安全的落地与思考](http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771062&idx=3&sn=a89df6be9e1facca3923e72e7893af53&chksm=fa881ec8383dc87529111acad49a966199a5e8c4cb69730b535f7a2c17a89b5ea1da24083a39#rd)
*DataFunTalk*

Main category: wechat.article

TL;DR: 腾讯大模型在网址安全的落地与思考知识图谱 + Agent在顺丰物流风控领域的探索与实践风控MLOPS工程化能力建设实践2025年10月18日，09：00-12：30，DataFun将开展【DataFunSummit2025：智能风控峰会】，本次会议聚焦大模型与智能体驱动的


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 腾讯大模型在网址安全的落地与思考知识图谱 + Agent在顺丰物流风控领域的探索与实践风控MLOPS工程化能力建设实践2025年10月18日，09：00-12：30，DataFun将开展【DataFunSummit2025：智能风控峰会】，本次会议聚焦大模型与智能体驱动的

</details>


### [63] [收藏！具身智能<em class="highlight">大模型</em>盘点，涉及谷歌、Meta、英伟达、智元、银河通用等](http://mp.weixin.qq.com/s?__biz=MzE5ODU5NzgwMQ==&mid=2247486582&idx=1&sn=73b8e69df3a80f2049ee2966209203c8&chksm=973811d44eecbdb499d63393d1154d0fddb7d85f6a1b1bf0431680e3517e2ab89313fdbe09ff#rd)
*GEIA全球具身智能观察*

Main category: wechat.article

TL;DR: 国内外具身智能大模型盘点 大模型 发布。团队 简介。谷歌与柏林工业大学合作开发的视觉语言模型，结合palm语言模型与视觉模型。palm-e google & tu berlin （vit），支持指令驱动的机器人动作生成（如抓取、导航），通过图文联合


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 国内外具身智能大模型盘点 大模型 发布。团队 简介。谷歌与柏林工业大学合作开发的视觉语言模型，结合palm语言模型与视觉模型。palm-e google & tu berlin （vit），支持指令驱动的机器人动作生成（如抓取、导航），通过图文联合

</details>


### [64] [<em class="highlight">大模型</em>在软件工程中的真实能力边界](http://mp.weixin.qq.com/s?__biz=MzA5MTA0NTIwMw==&mid=2652207140&idx=1&sn=74a0fade4c6bf94bf6add402ed14a9e6&chksm=8adec86f7a2ced1e4eaa4dcd95ff8995f9e1cc6d18f139045a7db6f7967c10847cf6b4c7714c#rd)
*软件工程之思*

Main category: wechat.article

TL;DR: 大模型在需要数学严谨性的算法设计中表现出显著不足：最短路径算法优化Dijkstra 算法在稀疏图场景下，LLM 生成的代码时间复杂度会退化为 O （N），而人类优化后的版本可达到 O （M+N log N）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型在需要数学严谨性的算法设计中表现出显著不足：最短路径算法优化Dijkstra 算法在稀疏图场景下，LLM 生成的代码时间复杂度会退化为 O （N），而人类优化后的版本可达到 O （M+N log N）。

</details>


### [65] [突发开源！跃居全球第一！腾讯发布混元图像 3.0 <em class="highlight">大模型</em>！2025](http://mp.weixin.qq.com/s?__biz=MzI5NDM4MTA3Nw==&mid=2247504818&idx=1&sn=bbb857f8e20970d304fdee3d94210ba9&chksm=ed9acaf61e2ec93324e76c09f8488a7da3f3e18ae5880a4ec4bd40eb47c4804203c2e1541655#rd)
*AI云原生智能算力架构*

Main category: wechat.article

TL;DR: 10 月 5 日，腾讯官方宣布：其 9 月 28 日刚刚发布的混元图像 3.0 大模型，在国际权威评测平台 LMArena 的文生图榜单中，从全球 26 个顶尖大模型中脱颖而出，以绝对优势登顶第一。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 10 月 5 日，腾讯官方宣布：其 9 月 28 日刚刚发布的混元图像 3.0 大模型，在国际权威评测平台 LMArena 的文生图榜单中，从全球 26 个顶尖大模型中脱颖而出，以绝对优势登顶第一。

</details>


### [66] [12天带你速通<em class="highlight">大模型</em>基础应用（七）Agent「下」](http://mp.weixin.qq.com/s?__biz=MzI5MjQzOTY3OA==&mid=2247488933&idx=1&sn=743facaec00425eec598b64eadeb9b89&chksm=edc8d672e0720248d0e617c9edfd0178093046c21a994e6a558efe4a5b1c7d093aa73716511a#rd)
*精神抖擞王大鹏*

Main category: wechat.article

TL;DR: 完整webui，集成coder做数据分析 15.1kopendeepresearch huggingface react范式，动作即代码 21.2klangchain版本 langchain 加入反思（reflect）机制 4.3kDeepResearchAgentSkyworkAI使用browser-use自动化


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 完整webui，集成coder做数据分析 15.1kopendeepresearch huggingface react范式，动作即代码 21.2klangchain版本 langchain 加入反思（reflect）机制 4.3kDeepResearchAgentSkyworkAI使用browser-use自动化

</details>


### [67] [AI<em class="highlight">大模型</em>养成记（一）：Transformer，那个开启了神童时代的天才宝宝](http://mp.weixin.qq.com/s?__biz=MzI1NTk4MjcwNg==&mid=2247484331&idx=1&sn=068393082a705b7edf21cec81d3a1503&chksm=eb2d850dabe1d8eb3b4e9ec304dd11b48f2ae1b5bdf1d818f649de8cca32e1b442544c751236#rd)
*张耐心*

Main category: wechat.article

TL;DR: 所以，我决定开启 《AI大模型养成记》 这个系列。在这里，没有复杂的公式和代码，我只想用最通俗易懂的语言，以一个“AI宝宝”的成长视角，带你看看我们今天所熟知的强大AI，是如何从一个“嘤嘤学语”的婴儿，一步步成


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 所以，我决定开启 《AI大模型养成记》 这个系列。在这里，没有复杂的公式和代码，我只想用最通俗易懂的语言，以一个“AI宝宝”的成长视角，带你看看我们今天所熟知的强大AI，是如何从一个“嘤嘤学语”的婴儿，一步步成

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [68] [CWM: An Open-Weights LLM for Research on Code Generation with World Models](https://arxiv.org/abs/2510.02387)
*FAIR CodeGen team,Quentin Carbonneaux,Gal Cohen,Jonas Gehring,Jacob Kahn,Jannik Kossen,Felix Kreuk,Emily McMilin,Michel Meyer,Yuxiang Wei,David Zhang,Kunhao Zheng,Jordi Armengol-Estapé,Pedram Bashiri,Maximilian Beck,Pierre Chambon,Abhishek Charnalia,Chris Cummins,Juliette Decugis,Zacharias V. Fisches,François Fleuret,Fabian Gloeckle,Alex Gu,Michael Hassid,Daniel Haziza,Badr Youbi Idrissi,Christian Keller,Rahul Kindi,Hugh Leather,Gallil Maimon,Aram Markosyan,Francisco Massa,Pierre-Emmanuel Mazaré,Vegard Mella,Naila Murray,Keyur Muzumdar,Peter O'Hearn,Matteo Pagliardini,Dmitrii Pedchenko,Tal Remez,Volker Seeker,Marco Selvi,Oren Sultan,Sida Wang,Luca Wehrstedt,Ori Yoran,Lingming Zhang,Taco Cohen,Yossi Adi,Gabriel Synnaeve*

Main category: cs.SE

TL;DR: Code World Model (CWM)是一个320亿参数的开源LLM，通过在Python解释器和Docker环境中训练观察-行动轨迹来增强代码理解能力，并在可验证编码、数学和多轮软件工程环境中进行强化学习。


<details>
  <summary>Details</summary>
Motivation: 为了推进代码生成与世界模型的研究，改善仅从静态代码训练中学到的代码理解能力，探索世界模型在代码生成中的推理和规划潜力。

Method: 在Python解释器和Docker环境中进行大量观察-行动轨迹的中期训练，并在可验证编码、数学和多轮软件工程环境中进行多任务推理强化学习。

Result: CWM在多个基准测试中表现出色：SWE-bench Verified 65.8%、LiveCodeBench 68.6%、Math-500 96.6%、AIME 2024 76.0%。

Conclusion: CWM为研究代码世界建模提供了强大的测试平台，展示了世界模型在代理编码、Python代码执行模拟和推理方面的潜力。

Abstract: We release Code World Model (CWM), a 32-billion-parameter open-weights LLM,
to advance research on code generation with world models. To improve code
understanding beyond what can be learned from training on static code alone, we
mid-train CWM on a large amount of observation-action trajectories from Python
interpreter and agentic Docker environments, and perform extensive multi-task
reasoning RL in verifiable coding, math, and multi-turn software engineering
environments. With CWM, we provide a strong testbed for researchers to explore
the opportunities world modeling affords for improving code generation with
reasoning and planning in computational environments. We present first steps of
how world models can benefit agentic coding, enable step-by-step simulation of
Python code execution, and show early results of how reasoning can benefit from
the latter. CWM is a dense, decoder-only LLM trained with a context size of up
to 131k tokens. Independent of its world modeling capabilities, CWM offers
strong performance on general coding and math tasks: it reaches pass@1 scores
of 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on
LiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further
research on code world modeling, we release model checkpoints after
mid-training, SFT, and RL.

</details>


### [69] [From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization](https://arxiv.org/abs/2510.02389)
*Haoran Xi,Minghao Shao,Brendan Dolan-Gavitt,Muhammad Shafique,Ramesh Karri*

Main category: cs.SE

TL;DR: T2L-Agent是一个项目级端到端框架，通过从模块逐步缩小范围到具体漏洞行，结合运行时证据和AST代码分块，实现精确的漏洞定位和修复。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法孤立分析代码、难以处理长上下文、仅提供函数或文件级别的粗粒度检测，缺乏对工程师有用的精确行级定位和针对性修复指导。

Method: T2L-Agent采用多轮反馈机制，结合Agentic Trace Analyzer融合运行时证据（崩溃点、堆栈跟踪、覆盖率差异）和AST代码分块，实现迭代精炼。

Result: 在T2L-ARVO基准测试中，T2L-Agent达到58.0%的检测率和54.8%的行级定位率，显著优于基线方法。

Conclusion: 该框架和基准测试将基于LLM的漏洞检测从粗粒度识别推向可部署、稳健的精确诊断，减少噪声并加速开源软件工作流中的修复过程。

Abstract: Large language models show promise for vulnerability discovery, yet
prevailing methods inspect code in isolation, struggle with long contexts, and
focus on coarse function- or file-level detections - offering limited
actionable guidance to engineers who need precise line-level localization and
targeted patches in real-world software development. We present T2L-Agent
(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own
analysis and progressively narrows scope from modules to exact vulnerable
lines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer
(ATA) that fuses runtime evidence - crash points, stack traces, and coverage
deltas - with AST-based code chunking, enabling iterative refinement beyond
single pass predictions and translating symptoms into actionable, line-level
diagnoses. To benchmark line-level vulnerability discovery, we introduce
T2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash
families and real-world projects. T2L-ARVO is specifically designed to support
both coarse-grained detection and fine-grained localization, enabling rigorous
evaluation of systems that aim to move beyond file-level predictions. On
T2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level
localization, substantially outperforming baselines. Together, the framework
and benchmark push LLM-based vulnerability detection from coarse identification
toward deployable, robust, precision diagnostics that reduce noise and
accelerate patching in open-source software workflows.

</details>


### [70] [AP2O: Correcting LLM-Generated Code Errors Type by Type Like Humans via Adaptive Progressive Preference Optimization](https://arxiv.org/abs/2510.02393)
*Jianqing Zhang,Wei Xia,Hande Dong,Qiang Lin,Jian Cao*

Main category: cs.SE

TL;DR: AP2O-Coder是一种自适应渐进式偏好优化方法，通过构建错误笔记本来逐步优化LLM，按类型纠正代码错误，并自适应重放错误类型来适应LLM在训练过程中的变化弱点。


<details>
  <summary>Details</summary>
Motivation: 现有离线偏好优化方法主要关注通过通过/失败信号来增强LLM的编码能力，但忽视了失败代码中的深层错误类型。

Method: 构建错误笔记本，逐步优化LLM按类型纠正错误，并自适应重放错误类型来适应LLM的变化弱点。

Result: 在0.5B到34B参数的代码和通用LLM上，AP2O-Coder将代码生成性能提升高达3%（pass@k），同时使用更少的偏好数据。

Conclusion: AP2O-Coder通过关注深层错误类型和自适应训练策略，有效提升了LLM的代码生成能力。

Abstract: LLMs' code generation capabilities have yielded substantial improvements in
the effectiveness of programming tasks. However, LLM-generated code still
suffers from compilation and runtime errors. Existing offline preference
optimization methods primarily focus on enhancing LLMs' coding abilities using
pass/fail signals in the preference data, overlooking the deep-level error
types in the failed codes. To address this, we propose Adaptively Progressive
Preference Optimization (AP2O) for coding (i.e., AP2O-Coder), a method that
guides LLMs adaptively and methodically to reduce code errors for code
generation. Specifically, we construct an error notebook from failed codes and
progressively optimize the LLM to correct errors type by type. Furthermore, we
adaptively replay error types to tailor to the LLM's changing weaknesses
throughout the training process. Through extensive experiments on both code and
general LLMs (Llama, Qwen, and DeepSeek series) with parameters ranging from
0.5B to 34B, our AP2O-Coder improves code generation performance by up to 3% in
pass@k while using less preference data. Code: https://github.com/TsingZ0/AP2O

</details>


### [71] [RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents](https://arxiv.org/abs/2510.02609)
*Chengquan Guo,Chulin Xie,Yu Yang,Zhaorun Chen,Zinan Lin,Xander Davies,Yarin Gal,Dawn Song,Bo Li*

Main category: cs.SE

TL;DR: 提出了RedCodeAgent，首个自动化红队测试代理，用于系统性地发现代码代理中的漏洞，通过自适应内存模块和动态工具选择提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前静态安全基准和红队测试工具无法覆盖新兴的现实风险场景，特别是不同越狱工具的组合效应，需要更有效的安全评估方法。

Method: 开发了具有自适应内存模块的自动化红队代理，能够利用现有越狱知识，动态选择最有效的红队测试工具组合，并使用模拟沙箱环境评估执行结果。

Result: RedCodeAgent在多个先进代码代理、不同风险场景和编程语言中持续优于现有方法，实现了更高的攻击成功率和更低的拒绝率。

Conclusion: 通过自动化和优化红队测试流程，RedCodeAgent实现了可扩展、自适应且有效的代码代理安全评估。

Abstract: Code agents have gained widespread adoption due to their strong code
generation capabilities and integration with code interpreters, enabling
dynamic execution, debugging, and interactive programming capabilities. While
these advancements have streamlined complex workflows, they have also
introduced critical safety and security risks. Current static safety benchmarks
and red-teaming tools are inadequate for identifying emerging real-world risky
scenarios, as they fail to cover certain boundary conditions, such as the
combined effects of different jailbreak tools. In this work, we propose
RedCodeAgent, the first automated red-teaming agent designed to systematically
uncover vulnerabilities in diverse code agents. With an adaptive memory module,
RedCodeAgent can leverage existing jailbreak knowledge, dynamically select the
most effective red-teaming tools and tool combinations in a tailored toolbox
for a given input query, thus identifying vulnerabilities that might otherwise
be overlooked. For reliable evaluation, we develop simulated sandbox
environments to additionally evaluate the execution results of code agents,
mitigating potential biases of LLM-based judges that only rely on static code.
Through extensive evaluations across multiple state-of-the-art code agents,
diverse risky scenarios, and various programming languages, RedCodeAgent
consistently outperforms existing red-teaming methods, achieving higher attack
success rates and lower rejection rates with high efficiency. We further
validate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium,
exposing previously unidentified security risks. By automating and optimizing
red-teaming processes, RedCodeAgent enables scalable, adaptive, and effective
safety assessments of code agents.

</details>


### [72] [C2|Q>: A Robust Framework for Bridging Classical and Quantum Software Development](https://arxiv.org/abs/2510.02854)
*Boshuai Ye,Arif Ali Khan,Teemu Pihkakoski,Peng Liang,Muhammad Azeem Akbar,Matti Silveri,Lauri Malmi*

Main category: cs.SE

TL;DR: C2|Q>是一个硬件无关的量子软件开发框架，通过将经典代码规范转换为量子可执行程序，使经典软件工程师能够更容易地进行量子计算开发。


<details>
  <summary>Details</summary>
Motivation: 当前量子开发环境要求开发者处理软件栈的低级细节，包括问题编码、电路构建、算法配置等，这使得经典软件工程师难以使用量子计算。

Method: 框架采用模块化软件工程原则，分为三个核心模块：编码器（分类问题、生成量子兼容格式、构建量子电路）、部署模块（生成电路、基于保真度、运行时间和成本推荐硬件）、解码器（将量子输出解释为经典解决方案）。

Result: 编码器模块完成率达到93.8%，硬件推荐模块为最多56量子比特的工作负载正确选择量子设备，完整工作流处理434个Python代码片段和100个JSON输入的完成率分别为93.8%和100%。在NISQ硬件上的案例研究显示，相比手动实现减少了近40倍的工作量。

Conclusion: C2|Q>成功降低了量子计算的使用门槛，使经典软件工程师能够更高效地开发量子应用，同时保持了方法论的严谨性。

Abstract: Quantum Software Engineering (QSE) is emerging as a critical discipline to
make quantum computing accessible to a broader developer community; however,
most quantum development environments still require developers to engage with
low-level details across the software stack - including problem encoding,
circuit construction, algorithm configuration, hardware selection, and result
interpretation - making them difficult for classical software engineers to use.
To bridge this gap, we present C2|Q>: a hardware-agnostic quantum software
development framework that translates classical specifications (code) into
quantum-executable programs while preserving methodological rigor. The
framework applies modular software engineering principles by classifying the
workflow into three core modules: an encoder that classifies problems, produces
Quantum-Compatible Formats (QCFs), and constructs quantum circuits, a
deployment module that generates circuits and recommends hardware based on
fidelity, runtime, and cost, and a decoder that interprets quantum outputs into
classical solutions. In evaluation, the encoder module achieved a 93.8%
completion rate, the hardware recommendation module consistently selected the
appropriate quantum devices for workloads scaling up to 56 qubits, and the full
C2|Q>: workflow successfully processed classical specifications (434 Python
snippets and 100 JSON inputs) with completion rates of 93.8% and 100%,
respectively. For case study problems executed on publicly available NISQ
hardware, C2|Q>: reduced the required implementation effort by nearly 40X
compared to manual implementations using low-level quantum software development
kits (SDKs), with empirical runs limited to small- and medium-sized instances
consistent with current NISQ capabilities. The open-source implementation of
C2|Q>: is available at https://github.com/C2-Q/C2Q

</details>


### [73] [GramTrans: A Better Code Representation Approach in Code Generation](https://arxiv.org/abs/2510.02887)
*Zhao Zhang,Qingyuan Liang,Zeyu Sun,Yizhou Chen,Guoqing Wang,Yican Sun,Lu Zhang,Ge Li,Yingfei Xiong*

Main category: cs.SE

TL;DR: 本文提出一个猜想：代码表示越容易解析，模型性能越好。通过GramTrans方法将上下文无关语言转换为LL(1)类表示，实验证明解析难度与模型性能强相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用不同代码表示（纯文本、语法规则序列、语法树序列），但缺乏对解析难度与模型效果关系的系统性理解。

Method: 提出GramTrans方法，使用分层冲突消除算法将上下文无关语言转换为LL(1)类表示，在解析难度和标记效率间取得平衡。

Result: 在Python和Java上使用三个代码生成模型评估，GramTrans在多个基准测试中相比基线表示持续带来显著改进。

Conclusion: 解析难度与模型性能强相关，GramTrans方法能有效提升代码生成性能。

Abstract: Code generation has shown great promise in assisting software development. A
fundamental yet underexplored question is how the choice of code representation
affects model performance. While existing studies employ various
representations, such as treating code as plain text, grammar rule sequences,
or syntax tree sequences, they lack a principled understanding of the
relationship between parsing difficulty and model effectiveness. This paper
proposes a conjecture: the easier a representation is to parse, the better
performance the model achieves. We formalize this idea using grammar classes,
where representations in simpler classes (e.g., LL(1)) are easier to parse.
Through a controlled experiment on a Python-based DSL, we show that parsing
difficulty strongly correlates with model performance. Motivated by this
finding, we present GramTrans, a general approach that automatically transforms
a context-free language into a representation within the LL(1) class. GramTrans
introduces a novel hierarchical conflict elimination algorithm, enabling a
flexible trade-off between syntactic simplicity and token efficiency. We
evaluate GramTrans on both Python and Java using three code generation models:
StarCoder 1B, DeepSeek-Coder 1.3B, and Qwen2.5 1.5B. Across multiple
benchmarks, GramTrans consistently delivers significant improvements over
baseline representations. Furthermore, our analysis of existing representations
reconfirms the strong alignment between parsing difficulty and model
performance, providing additional support for the conjecture.

</details>


### [74] [Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders](https://arxiv.org/abs/2510.02917)
*Kriz Tahimic,Charibeth Cheng*

Main category: cs.SE

TL;DR: 该论文通过稀疏自编码器分解LLM表示，识别代码正确性方向，发现代码正确性方向能可靠预测错误代码，但修正能力需要在修复错误和保留正确代码之间权衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中的广泛应用，理解其内部正确性机制对于安全部署至关重要。

Method: 应用稀疏自编码器分解LLM表示，通过t统计量选择预测方向，通过分离分数从基础模型表示中提取引导方向，并通过引导、注意力分析和权重正交化分析其机制特性。

Result: 代码正确性方向能可靠预测错误代码，修正能力虽统计显著但需权衡；成功的代码生成依赖于关注测试用例而非问题描述；基础模型中识别的方向在指令微调后仍保持有效性。

Conclusion: 代码正确性机制在预训练期间学习并在微调时被重新利用，提出了三个实际应用：提示策略应优先测试用例、预测方向可作为错误警报、选择性引导可防止代码损坏。

Abstract: As Large Language Models become integral to software development, with
substantial portions of AI-suggested code entering production, understanding
their internal correctness mechanisms becomes critical for safe deployment. We
apply sparse autoencoders to decompose LLM representations, identifying
directions that correspond to code correctness. We select predictor directions
using t-statistics and steering directions through separation scores from base
model representations, then analyze their mechanistic properties through
steering, attention analysis, and weight orthogonalization. We find that code
correctness directions in LLMs reliably predict incorrect code, while
correction capabilities, though statistically significant, involve tradeoffs
between fixing errors and preserving correct code. Mechanistically, successful
code generation depends on attending to test cases rather than problem
descriptions. Moreover, directions identified in base models retain their
effectiveness after instruction-tuning, suggesting code correctness mechanisms
learned during pre-training are repurposed during fine-tuning. Our mechanistic
insights suggest three practical applications: prompting strategies should
prioritize test examples over elaborate problem descriptions, predictor
directions can serve as error alarms for developer review, and these same
predictors can guide selective steering, intervening only when errors are
anticipated to prevent the code corruption from constant steering.

</details>


### [75] [Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection](https://arxiv.org/abs/2510.02934)
*Thanh Trong Vu,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.SE

TL;DR: AUTOPROBE是一种模型无关的方法，通过动态选择LLM内部最有信息量的表示来评估代码正确性，在编译性、功能性和安全性评估方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预选层和标记位置的表示，限制了在不同模型架构和任务间的泛化能力。需要一种能动态选择重要内部表示的方法来更好地评估LLM生成代码的正确性。

Method: 使用基于注意力的机制学习隐藏状态的重要性分数，聚焦最相关特征，然后聚合加权表示并通过探测分类器预测代码正确性（包括编译性、功能性和安全性）。

Result: AUTOPROBE在多个基准测试和代码LLMs上表现优于基线方法。安全性评估比最先进的白盒方法高18%，编译性和功能性评估分别比其他方法高19%和111%，对代码复杂性具有最高鲁棒性。

Conclusion: 动态选择重要内部信号使AUTOPROBE成为评估各种LLMs生成代码正确性的鲁棒且可泛化的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation and are increasingly integrated into the software development
process. However, ensuring the correctness of LLM-generated code remains a
critical concern. Prior work has shown that the internal representations of
LLMs encode meaningful signals for assessing code correctness. Nevertheless,
the existing methods rely on representations from pre-selected/fixed layers and
token positions, which could limit its generalizability across diverse model
architectures and tasks. In this work, we introduce AUTOPROBE, a novel
model-agnostic approach that dynamically selects the most informative internal
representations for code correctness assessment. AUTOPROBE employs an
attention-based mechanism to learn importance scores for hidden states,
enabling it to focus on the most relevant features. These weighted
representations are then aggregated and passed to a probing classifier to
predict code correctness across multiple dimensions, including compilability,
functionality, and security. To evaluate the performance of AUTOPROBE, we
conduct extensive experiments across multiple benchmarks and code LLMs. Our
experimental results show that AUTOPROBE consistently outperforms the
baselines. For security assessment, AUTOPROBE surpasses the state-of-the-art
white-box approach by 18%. For compilability and functionality assessment,
AUTOPROBE demonstrates its highest robustness to code complexity, with the
performance higher than the other approaches by up to 19% and 111%,
respectively. These findings highlight that dynamically selecting important
internal signals enables AUTOPROBE to serve as a robust and generalizable
solution for assessing the correctness of code generated by various LLMs.

</details>


### [76] [When Names Disappear: Revealing What LLMs Actually Understand About Code](https://arxiv.org/abs/2510.03178)
*Cuong Chi Le,Minh V. T. Pham,Cuong Duc Van,Hoang N. Phan,Huy N. Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: 论文研究了LLMs如何理解代码，发现代码通过结构语义和命名两个通道传递信息。去除命名通道会严重影响意图级任务，并在执行任务中暴露命名模式记忆问题。作者引入语义保留混淆方法，并发布ClassEval-Obf基准来更可靠评估LLMs的代码理解能力。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在代码任务上表现良好，但它们如何理解程序含义仍不清楚。当前基准可能奖励命名模式的记忆而非真正的语义推理，需要更可靠的方法来评估代码理解能力。

Method: 引入语义保留混淆方法，去除代码中的命名信息但保留结构语义，创建ClassEval-Obf混淆增强基准来系统性地抑制命名线索。

Result: 去除命名通道会严重降低意图级任务性能，在执行任务中也观察到一致下降，暴露了当前基准对命名模式记忆的依赖。ClassEval-Obf减少了性能差距，削弱了记忆捷径。

Conclusion: 代码通过结构和命名两个通道通信，当前基准可能高估LLMs的代码理解能力。混淆方法提供了更可靠的评估基础，揭示了标识符泄漏问题。

Abstract: Large Language Models (LLMs) achieve strong results on code tasks, but how
they derive program meaning remains unclear. We argue that code communicates
through two channels: structural semantics, which define formal behavior, and
human-interpretable naming, which conveys intent. Removing the naming channel
severely degrades intent-level tasks such as summarization, where models
regress to line-by-line descriptions. Surprisingly, we also observe consistent
reductions on execution tasks that should depend only on structure, revealing
that current benchmarks reward memorization of naming patterns rather than
genuine semantic reasoning. To disentangle these effects, we introduce a suite
of semantics-preserving obfuscations and show that they expose identifier
leakage across both summarization and execution. Building on these insights, we
release ClassEval-Obf, an obfuscation-enhanced benchmark that systematically
suppresses naming cues while preserving behavior. Our results demonstrate that
ClassEval-Obf reduces inflated performance gaps, weakens memorization
shortcuts, and provides a more reliable basis for assessing LLMs' code
understanding and generalization.

</details>


### [77] [Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair](https://arxiv.org/abs/2510.03217)
*José Cambronero,Michele Tufano,Sherry Shi,Renyao Wei,Grant Uy,Runxiang Cheng,Chin-Jung Liu,Shiying Pan,Satish Chandra,Pat Rondon*

Main category: cs.SE

TL;DR: 提出了两种LLM策略（bug abstention和patch validation）来减少自动化程序修复系统中的噪音，通过排除难以修复的bug和验证补丁质量来提高成功率。


<details>
  <summary>Details</summary>
Motivation: 解决自动化程序修复系统在工业环境中产生的噪音问题，避免向开发者展示不太可能成功的补丁，从而节省开发时间并增强对自动化代码更改的信任。

Method: 引入两种互补的LLM策略：bug abstention策略排除系统难以修复的bug，patch validation策略拒绝不太可能是好修复的补丁。

Result: 在Google代码库的174个人工报告bug上，两种策略组合使用可将成功率提高最多39个百分点；在空指针异常和sanitizer报告的bug上，补丁验证也能提高平均单样本成功率。

Conclusion: 这种双策略方法为自动化程序修复系统在工业规模上的可靠部署提供了实用路径。

Abstract: Agentic Automated Program Repair (APR) is increasingly tackling complex,
repository-level bugs in industry, but ultimately agent-generated patches still
need to be reviewed by a human before committing them to ensure they address
the bug. Showing unlikely patches to developers can lead to substantial noise,
wasting valuable developer time and eroding trust in automated code changes. We
introduce two complementary LLM-based policies to reduce such noise: bug
abstention and patch validation policies. Bug abstention excludes bugs that the
agentic APR system is unlikely to fix. Patch validation rejects patches that
are unlikely to be a good fix for the given bug. We evaluate both policies on
three sets of bugs from Google's codebase, and their candidate patches
generated by an internal agentic APR system. On a set of 174 human-reported
bugs, removing bugs and patch trajectories rejected by our policies can raise
success rates by up to 13 percentage points and 15 percentage points,
respectively, and by up to 39 percentage points in combination. On null pointer
exceptions and sanitizer-reported bugs with machine-generated bug reports,
patch validation also improves average single-sample success rates. This
two-policy approach provides a practical path to the reliable, industrial-scale
deployment of agentic APR systems.

</details>
