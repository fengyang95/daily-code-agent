<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 5]
- [tldr.article](#tldr.article) [Total: 7]
- [wechat.article](#wechat.article) [Total: 24]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.LG](#cs.LG) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [KBQA-R1: Reinforcing Large Language Models for Knowledge Base Question Answering](https://arxiv.org/abs/2512.10999)
*Xin Sun,Zhongqi Chen,Xing Zheng,Qiang Liu,Shu Wu,Bowen Song,Zilei Wang,Weiqiang Wang,Liang Wang*

Main category: cs.CL

TL;DR: KBQA-R1：通过强化学习将知识库问答从文本模仿转向交互优化，利用Group Relative Policy Optimization和Referenced Rejection Sampling方法，在多个基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前KBQA方法存在两种失败模式：要么生成幻觉查询而不验证模式存在性，要么采用僵化的模板推理模仿合成轨迹而缺乏对环境真正理解。需要解决LLM在KBQA中从文本模仿到交互优化的范式转变。

Method: 提出KBQA-R1框架，将KBQA视为多轮决策过程，使用强化学习导航知识库。采用Group Relative Policy Optimization（GRPO）基于执行反馈优化策略，并引入Referenced Rejection Sampling（RRS）数据合成方法解决冷启动问题，严格对齐推理轨迹与真实动作序列。

Result: 在WebQSP、GrailQA和GraphQuestions等基准测试上进行了广泛实验，KBQA-R1实现了最先进的性能，有效将LLM推理基于可验证的执行。

Conclusion: KBQA-R1通过强化学习交互优化解决了当前KBQA方法的局限性，实现了从文本模仿到基于执行反馈的策略优化的范式转变，显著提升了知识库问答的性能和可靠性。

Abstract: Knowledge Base Question Answering (KBQA) challenges models to bridge the gap between natural language and strict knowledge graph schemas by generating executable logical forms. While Large Language Models (LLMs) have advanced this field, current approaches often struggle with a dichotomy of failure: they either generate hallucinated queries without verifying schema existence or exhibit rigid, template-based reasoning that mimics synthesized traces without true comprehension of the environment. To address these limitations, we present \textbf{KBQA-R1}, a framework that shifts the paradigm from text imitation to interaction optimization via Reinforcement Learning. Treating KBQA as a multi-turn decision process, our model learns to navigate the knowledge base using a list of actions, leveraging Group Relative Policy Optimization (GRPO) to refine its strategies based on concrete execution feedback rather than static supervision. Furthermore, we introduce \textbf{Referenced Rejection Sampling (RRS)}, a data synthesis method that resolves cold-start challenges by strictly aligning reasoning traces with ground-truth action sequences. Extensive experiments on WebQSP, GrailQA, and GraphQuestions demonstrate that KBQA-R1 achieves state-of-the-art performance, effectively grounding LLM reasoning in verifiable execution.

</details>


### [2] [When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents](https://arxiv.org/abs/2512.11277)
*Mrinal Rawat,Arkajyoti Chakraborty,Neha Gupta,Roberto Pieraccini*

Main category: cs.CL

TL;DR: 该论文提出了一种使用强化学习（RL）来训练LLMs推理策略的方法，通过Group Relative Policy Optimization（GRPO）结合工具准确性和答案正确性的奖励机制，提升对话代理的推理质量和工具调用精度。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）在数据分布变化时泛化能力有限，而高质量推理标注成本高、主观性强且难以扩展。最近的研究表明推理能力对模型泛化和可靠性至关重要，但缺乏有效的训练方法。

Method: 提出一个管道，让LLMs生成推理步骤来指导工具调用（如函数调用）和最终答案生成。使用Group Relative Policy Optimization（GRPO）强化学习算法，设计基于工具准确性和答案正确性的奖励机制，让模型从任务结果中直接学习推理策略。

Result: 实验结果显示，该方法在推理质量和工具调用精度上均有提升，相比未使用显式推理的SFT模型获得1.5%的相对改进，相比基础Qwen3-1.7B模型获得40%的性能增益。

Conclusion: 通过强化学习统一推理和动作学习，可以构建更强大和可泛化的对话代理，展示了RL在提升LLMs推理能力方面的潜力。

Abstract: Supervised fine-tuning (SFT) has emerged as one of the most effective ways to improve the performance of large language models (LLMs) in downstream tasks. However, SFT can have difficulty generalizing when the underlying data distribution changes, even when the new data does not fall completely outside the training domain. Recent reasoning-focused models such as o1 and R1 have demonstrated consistent gains over their non-reasoning counterparts, highlighting the importance of reasoning for improved generalization and reliability. However, collecting high-quality reasoning traces for SFT remains challenging -- annotations are costly, subjective, and difficult to scale. To address this limitation, we leverage Reinforcement Learning (RL) to enable models to learn reasoning strategies directly from task outcomes. We propose a pipeline in which LLMs generate reasoning steps that guide both the invocation of tools (e.g., function calls) and the final answer generation for conversational agents. Our method employs Group Relative Policy Optimization (GRPO) with rewards designed around tool accuracy and answer correctness, allowing the model to iteratively refine its reasoning and actions. Experimental results demonstrate that our approach improves both the quality of reasoning and the precision of tool invocations, achieving a 1.5% relative improvement over the SFT model (trained without explicit thinking) and a 40% gain compared to the base of the vanilla Qwen3-1.7B model. These findings demonstrate the promise of unifying reasoning and action learning through RL to build more capable and generalizable conversational agents.

</details>


### [3] [Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive Memory Architecture](https://arxiv.org/abs/2512.11303)
*Jiarun Liu,Shiyue Xu,Yang Li,Shangkun Liu,Yongli Yu,Peng Cao*

Main category: cs.CL

TL;DR: SMITH是一个统一认知架构，通过分层记忆组织将动态工具创建与跨任务经验共享相结合，在GAIA基准测试中达到81.8%的准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在适应新任务时面临工具覆盖有限和经验重用不足的问题，需要更有效的工具创建和经验共享机制。

Method: 提出SMITH架构，将记忆组织为程序性、语义性和情景性三个层次，通过沙盒环境中的迭代代码生成创建工具，并通过语义相似度匹配实现经验共享，还提出基于代理集成难度重估的课程学习策略。

Result: 在GAIA基准测试中达到81.8%的Pass@1准确率，显著优于Alita（75.2%）和Memento（70.9%）等现有方法。

Conclusion: SMITH为构建真正自适应代理奠定了基础，通过工具创建和经验积累的有机结合实现能力的持续进化。

Abstract: Large Language Model agents face fundamental challenges in adapting to novel tasks due to limitations in tool availability and experience reuse. Existing approaches either rely on predefined tools with limited coverage or build tools from scratch without leveraging past experiences, leading to inefficient exploration and suboptimal performance. We introduce SMITH (Shared Memory Integrated Tool Hub), a unified cognitive architecture that seamlessly integrates dynamic tool creation with cross-task experience sharing through hierarchical memory organization. SMITH organizes agent memory into procedural, semantic, and episodic components, enabling systematic capability expansion while preserving successful execution patterns. Our approach formalizes tool creation as iterative code generation within controlled sandbox environments and experience sharing through episodic memory retrieval with semantic similarity matching. We further propose a curriculum learning strategy based on agent-ensemble difficulty re-estimation. Extensive experiments on the GAIA benchmark demonstrate SMITH's effectiveness, achieving 81.8% Pass@1 accuracy and outperforming state-of-the-art baselines including Alita (75.2%) and Memento (70.9%). Our work establishes a foundation for building truly adaptive agents that continuously evolve their capabilities through principled integration of tool creation and experience accumulation.

</details>


### [4] [Visualizing token importance for black-box language models](https://arxiv.org/abs/2512.11573)
*Paulius Rauba,Qiyao Wei,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 提出DBSA方法，用于审计黑盒大语言模型，分析每个输入token对输出的敏感性，无需模型假设，支持可视化探索


<details>
  <summary>Details</summary>
Motivation: 现有LLM审计方法通常关注特定方面（如偏见检测），缺乏对输入token依赖性的全面理解。实际应用中需要能够分析黑盒API模型输出与输入token关系的工具

Method: 提出Distribution-Based Sensitivity Analysis (DBSA)，一种轻量级、模型无关的方法，无需对LLM做分布假设，通过分析输出分布来评估每个输入token的敏感性

Result: 通过示例证明DBSA能够帮助用户检查LLM输入，发现现有可解释性方法可能忽略的敏感性

Conclusion: DBSA为实践者提供了实用的插件式工具，支持快速可视化探索LLM对特定输入token的依赖关系

Abstract: We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Coverage Isn't Enough: SBFL-Driven Insights into Manually Created vs. Automatically Generated Tests](https://arxiv.org/abs/2512.11223)
*Sasara Shimizu,Yoshiki Higo*

Main category: cs.SE

TL;DR: 自动生成的测试比手动测试有更高的分支覆盖率，但在基于频谱的故障定位(SBFL)得分上较低，尤其是在深层嵌套代码结构中。研究建议结合两种测试方法。


<details>
  <summary>Details</summary>
Motivation: 现有测试生成研究主要关注覆盖率指标，很少评估测试对故障定位的支持效果，特别是使用变异测试引入的人工故障。本研究旨在比较自动生成测试和手动创建测试在SBFL得分和代码覆盖率上的差异。

Method: 使用SBFL（基于频谱的故障定位）得分作为评估指标，比较自动生成测试和手动创建测试的SBFL得分和代码覆盖率。特别关注深层嵌套代码结构的情况。

Result: 自动生成的测试比手动创建的测试获得更高的分支覆盖率，但SBFL得分较低，尤其是在深层嵌套结构的代码中。

Conclusion: 自动生成测试和手动创建测试各有优势：自动测试覆盖率高，手动测试故障定位效果好。研究结果为如何有效结合两种测试方法提供了指导。

Abstract: The testing phase is an essential part of software development, but manually creating test cases can be time-consuming. Consequently, there is a growing need for more efficient testing methods. To reduce the burden on developers, various automated test generation tools have been developed, and several studies have been conducted to evaluate the effectiveness of the tests they produce. However, most of these studies focus primarily on coverage metrics, and only a few examine how well the tests support fault localization-particularly using artificial faults introduced through mutation testing. In this study, we compare the SBFL (Spectrum-Based Fault Localization) score and code coverage of automatically generated tests with those of manually created tests. The SBFL score indicates how accurately faults can be localized using SBFL techniques. By employing SBFL score as an evaluation metric-an approach rarely used in prior studies on test generation-we aim to provide new insights into the respective strengths and weaknesses of manually created and automatically generated tests. Our experimental results show that automatically generated tests achieve higher branch coverage than manually created tests, but their SBFL score is lower, especially for code with deeply nested structures. These findings offer guidance on how to effectively combine automatically generated and manually created testing approaches.

</details>


### [6] [AutoFSM: A Multi-agent Framework for FSM Code Generation with IR and SystemC-Based Testing](https://arxiv.org/abs/2512.11398)
*Qiuming Luo,Yanming Lei,Kunzhong Wu,Yixuan Cao,Chengjian Liu*

Main category: cs.SE

TL;DR: AutoFSM是一个多智能体协作框架，通过结构化中间表示和自动化测试工具链，显著提升LLM生成Verilog有限状态机代码的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成Verilog有限状态机控制逻辑时面临语法错误频繁、调试效率低、过度依赖测试基准等问题，需要更可靠的硬件设计代码生成方案。

Method: 提出AutoFSM多智能体协作框架，引入结构化中间表示降低语法错误率，提供从IR到Verilog的自动转换工具链，并集成SystemC建模和自动测试平台生成。

Result: 在SKT-FSM基准测试中，AutoFSM相比开源框架MAGE在相同基础LLM下，通过率提升最高11.94%，语法错误率降低最高17.62%。

Conclusion: 结合LLM与结构化中间表示和自动化测试，能够显著提高RTL代码生成的可靠性和可扩展性，为硬件设计自动化提供新思路。

Abstract: With the rapid advancement of large language models (LLMs) in code generation, their applications in hardware design are receiving growing attention. However, existing LLMs face several challenges when generating Verilog code for finite state machine (FSM) control logic, including frequent syntax errors, low debugging efficiency, and heavy reliance on test benchmarks. To address these challenges, this paper proposes AutoFSM, a multi-agent collaborative framework designed for FSM code generation tasks. AutoFSM introduces a structurally clear intermediate representation (IR) to reduce syntax error rate during code generation and provides a supporting toolchain to enable automatic translation from IR to Verilog. Furthermore, AutoFSM is the first to integrate SystemC-based modeling with automatic testbench generation, thereby improving debugging efficiency and feedback quality. To systematically evaluate the framework's performance, we construct SKT-FSM, the first hierarchical FSM benchmark in the field, comprising 67 FSM samples across different complexity levels. Experimental results show that, under the same base LLM, AutoFSM consistently outperforms the open-source framework MAGE on the SKT-FSM benchmark, achieving up to an 11.94% improvement in pass rate and up to a 17.62% reduction in syntax error rate. These results demonstrate the potential of combining LLMs with structured IR and automated testing to improve the reliability and scalability of register-transfer level (RTL) code generation.

</details>


### [7] [REMODEL-LLM: Transforming C code to Java using LLMs](https://arxiv.org/abs/2512.11402)
*Aryan Gupta,Y. Raghu Reddy*

Main category: cs.SE

TL;DR: 本研究评估了19个小型量化LLM在C到Java代码翻译任务中的表现，发现只有3个模型能通过超过50%的测试，揭示了当前量化模型在复杂C概念翻译上的能力天花板。


<details>
  <summary>Details</summary>
Motivation: C到Java的自动代码翻译面临巨大挑战，包括编程范式差异（过程式vs面向对象）、内存模型不同（手动指针vs垃圾回收）以及数据类型不兼容。本研究旨在探索小型量化LLM在此任务上的实际效果。

Method: 采用混合管道方法：利用抽象语法树（AST）进行语义分解，并结合高度约束的基于规则的提示策略。评估了19个参数小于200亿的小型量化LLM。

Result: 结果呈现明显的三级性能分化：Tier 3模型（如llama3.1、gemma3、starcoder2）100%测试失败；Tier 2模型（如mistral-nemo、mistral）能生成可运行代码但存在严重语义错误；只有Tier 1模型（phi4、deepseek-coder-v2、codeqwen）表现可行，通过率超过50%，但在函数指针、sizeof、枚举逻辑等复杂C概念上仍会失败。

Conclusion: 当前小型量化LLM在C到Java翻译任务中能力有限，只有少数模型表现尚可，但在处理复杂C语言特性时存在明显的能力天花板，揭示了量化模型在复杂推理任务上的局限性。

Abstract: The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.

</details>


### [8] [Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models](https://arxiv.org/abs/2512.11482)
*Melih Catal,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: 首次系统评估差分隐私在代码大语言模型中的应用，发现DP能显著降低记忆风险，同时保持代码生成能力，且不影响训练效率


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型在生成代码时可能无意中记忆并复现训练数据片段，导致隐私泄露和知识产权侵权风险，限制了在敏感领域的部署

Method: 应用差分隐私到代码大语言模型训练中，首先分析微调过程中的记忆行为原因，然后实证评估DP在减少记忆同时保持代码生成能力的效果

Result: DP显著降低了所有测试代码片段的记忆风险，最易记忆的片段类型也是DP最有效缓解的类型；DP轻微增加困惑度但保持甚至提升了代码生成能力，且不影响训练时间和能耗

Conclusion: 差分隐私是保护隐私的代码大语言模型训练的实用选择，能在不显著影响模型效用的前提下有效降低记忆风险

Abstract: Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.

</details>


### [9] [A Study of Library Usage in Agent-Authored Pull Requests](https://arxiv.org/abs/2512.11589)
*Lukas Twist*

Main category: cs.SE

TL;DR: AI编码代理在29.5%的PR中导入库，但仅1.3%添加新依赖；添加依赖时75%指定版本，优于直接使用LLM；代理使用的外部库多样性远超非代理LLM研究


<details>
  <summary>Details</summary>
Motivation: 尽管AI编码代理能够完成端到端软件开发工作流，但我们对代理如何使用库这一软件开发核心环节了解甚少。本研究旨在填补这一空白，探究代理在代码生成中如何使用外部库。

Method: 基于AIDev数据集的26,760个代理生成的PR进行分析，研究三个问题：代理导入库的频率、引入新依赖的频率及版本控制实践、以及具体选择的库类型。

Result: 代理经常导入库（29.5%的PR），但很少添加新依赖（仅1.3%）；添加依赖时75%会指定版本，这比直接使用LLM时很少提及版本的情况有显著改进；代理使用的外部库多样性远超先前非代理LLM研究中观察到的有限"库偏好"。

Conclusion: 研究提供了AI编码代理与当前软件生态系统交互的早期实证视角，显示代理在库使用方面表现出更成熟的实践和更广泛的库选择，为理解AI编码代理在真实软件开发中的行为提供了重要见解。

Abstract: Coding agents are becoming increasingly capable of completing end-to-end software engineering workflows that previously required a human developer, including raising pull requests (PRs) to propose their changes. However, we still know little about how these agents use libraries when generating code, a core part of real-world software development. To fill this gap, we study 26,760 agent-authored PRs from the AIDev dataset to examine three questions: how often do agents import libraries, how often do they introduce new dependencies (and with what versioning), and which specific libraries do they choose? We find that agents often import libraries (29.5% of PRs) but rarely add new dependencies (1.3% of PRs); and when they do, they follow strong versioning practices (75.0% specify a version), an improvement on direct LLM usage where versions are rarely mentioned. Generally, agents draw from a surprisingly diverse set of external libraries, contrasting with the limited "library preferences" seen in prior non-agentic LLM studies. Our results offer an early empirical view into how AI coding agents interact with today's software ecosystems.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [10] [Agentic Commerce Is Here: Why PSPs Must Act Now to Stay Relevant](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F8G92Jp/1/0100019b0dbe0109-d67dc7e1-4e16-4764-a274-f694a9b38c76-000000/GeembiumrTb272fyaR0VecNBXTuGGKqosprUOy7mBqo=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理正在重塑电商，从产品发现到购物车构建，但当前处于半自主阶段，用户仍是最终决策者


<details>
  <summary>Details</summary>
Motivation: AI代理在电商中的应用正从概念快速变为现实，支付服务提供商需要立即行动以保持相关性

Method: 分析AI代理在电商各环节的应用现状，预测从半自主到全自主支付的发展路径

Result: AI代理正在改变消费者的购物行为模式，电商行业正在进入半自主时代

Conclusion: 支付服务提供商必须立即适应AI代理驱动的电商变革，否则将失去市场相关性

Abstract: Agentic Commerce Is Here: Why PSPs Must Act Now to Stay Relevant (5 minute read) Agentic commerce is moving from concept to reality faster than the industry expected. AI agents are increasingly steering how consumers discover products, compare options, build carts, and initiate purchases. While the long-term vision is fully autonomous payments, the near-term reality looks quite different. We are entering a semi-autonomous era: AI agents do the heavy lifting, but the user remains the final dec...

</details>


### [11] [Can LLMs Detect IDORs? Understanding the Boundaries of AI Reasoning](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsemgrep.dev%2Fblog%2F2025%2Fcan-llms-detect-idors-understanding-the-boundaries-of-ai-reasoning%2F%3Futm_source=tldrinfosec/1/0100019b0dd3ac53-588fc471-6a71-4818-88c7-edb4d013fa47-000000/Xg4hXoxCCz4cRFrZKJ0wYR6MiA5JZYpOWqEOz-Q4Cn0=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Semgrep测试了Claude Code Sonnet 4和OpenAI Codex GPT5检测IDOR漏洞的能力，在四个难度级别上发现了15个真实漏洞和93个误报，Sonnet 4表现最佳。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在检测IDOR（不安全的直接对象引用）漏洞方面的实际能力，了解AI推理的边界。

Method: 使用Semgrep平台测试Claude Code Sonnet 4和OpenAI Codex GPT5，设计了四个难度级别的测试场景：无授权、显式授权、隐式授权和中间件隐式授权。

Result: 总共发现了15个真实且之前未知的漏洞和93个误报。Sonnet 4在检测IDOR漏洞方面表现最佳，特别是在通用提示测试中。

Conclusion: LLMs在检测IDOR漏洞方面具有一定能力，但存在局限性，特别是随着授权机制复杂度的增加，检测难度会显著提高。

Abstract: Can LLMs Detect IDORs? Understanding the Boundaries of AI Reasoning (10 minute read) Semgrep tested Claude Code with Sonnet 4 and OpenAI Codex with GPT5's ability to detect IDOR vulnerabilities in code with four levels of increasing difficulty, from no authorization to implicit authorization through middleware. In total, the models identified 15 real, previously unknown vulnerabilities and 93 false positives, with Sonnet 4 performing best. In its test with a generic prompt, Semgrep found that...

</details>


### [12] [Cursor Debug Mode](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fdebug-mode%3Futm_source=tldrai/1/0100019b0de2e5a4-ecea8ec4-4b82-4da4-8b49-a2b541c02ce3-000000/cC-IBRAcCkyQjmeTW0fbp6dtJk49kO3DTE7MuRQ5Fho=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor推出Debug Mode，这是一种结合运行时日志和人工交互的新代理循环，用于修复顽固bug


<details>
  <summary>Details</summary>
Motivation: 受专家调试工作流程启发，旨在解决传统代码代理在修复复杂bug时的局限性，通过结合人类交互提高调试效率

Method: 采用新的代理循环：1) 生成多个失败假设 2) 用日志检测代码 3) 让用户重现错误 4) 验证修复方案

Result: 开发了Debug Mode功能，将运行时日志和人工交互整合到调试过程中，提高了顽固bug的修复能力

Conclusion: 通过结合人类交互和自动化调试，Cursor的Debug Mode为代码代理提供了更有效的bug修复工作流程

Abstract: Cursor Debug Mode (3 minute read) Cursor has introduced Debug Mode, a new agent loop that incorporates runtime logs and human interaction to fix stubborn bugs. Inspired by expert debugging workflows, the mode generates multiple failure hypotheses, instruments code with logs, and loops in the user to reproduce errors and verify fixes.

</details>


### [13] [Upgrades to Claude Code CLI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1998830338735485239.html%3Futm_source=tldrai/1/0100019b0de2e5a4-ecea8ec4-4b82-4da4-8b49-a2b541c02ce3-000000/OksxsjNYyeRadZ6X5xbtIPVGY0g5Rr2UqvRtAdXMXAA=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code CLI 新增异步子代理、即时压缩、客户会话名称和用量统计功能，/resume 界面新增键盘快捷键


<details>
  <summary>Details</summary>
Motivation: 提升 Claude Code CLI 的开发效率和用户体验，通过新增功能增强工具的生产力

Method: 发布 Claude Code CLI 更新版本，添加异步子代理、即时压缩、客户会话名称、用量统计等功能，并在 /resume 界面增加键盘快捷键

Result: 用户可通过运行 'claude update' 命令获取所有新功能，提升代码开发工作流程的效率

Conclusion: Claude Code CLI 持续改进，通过功能增强为开发者提供更强大的工具支持

Abstract: Upgrades to Claude Code CLI (2 minute read) Claude Code CLI now has async subagents, instant compact, customer session names, and usage stats. There are also new keyboard shortcuts on the /resume screen. Run 'claude update' to get all of the new features.

</details>


### [14] [Towards a Science of Scaling Agent Systems](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2512.08296%3Futm_source=tldrai/1/0100019b0de2e5a4-ecea8ec4-4b82-4da4-8b49-a2b541c02ce3-000000/OoMuS1yGA30gbGLaK-sOfrsExxMmCGRzpq6QiKTu7rs=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文尝试为智能体系统建立定量扩展原则，以替代当前依赖启发式方法的设计实践。


<details>
  <summary>Details</summary>
Motivation: 智能体正在成为现实世界AI应用的主导范式，但从业者仍然依赖启发式方法而非原则性设计选择，需要建立科学的扩展原则。

Method: 研究尝试推导智能体系统的定量扩展原则，建立科学的扩展方法学。

Result: 论文提出了建立智能体系统扩展科学的初步尝试，旨在为实践提供定量指导原则。

Conclusion: 需要为智能体系统建立科学的扩展原则，以替代当前依赖启发式的设计实践。

Abstract: Towards a Science of Scaling Agent Systems (2 minute read) Agents are becoming the dominant paradigm for real-world AI applications. However, practitioners still rely on heuristics rather than principled design choices. This study attempts to derive quantitative scaling principles for agent systems.

</details>


### [15] [Frontier coding agent Amp spins out of Sourcegraph as a separate company](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fampcode.com%2Fnews%2Famp-inc%3F%26utm_medium=paid%2520sponsored%26utm_source=tldrnewsletter%26utm_campaign=amp_split/1/0100019b0de2e5a4-ecea8ec4-4b82-4da4-8b49-a2b541c02ce3-000000/mHNGb8xT-VFf2cxG3PMfSLLEZXm1FT89gnUNHELgrgw=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sourcegraph的编码代理Amp在9个月实验后成为领先编码代理，现已分拆为独立公司，专注于前沿软件开发


<details>
  <summary>Details</summary>
Motivation: Sourcegraph最初将Amp作为实验性编码代理推出，旨在探索AI在软件开发中的应用，随着其快速发展成为领先编码代理，需要独立运营以更好地专注于前沿技术开发

Method: 通过将Amp从Sourcegraph分拆为独立公司，使其能够专注于编码代理技术的研发和创新，独立运营以加速发展

Result: Amp在9个月内从实验项目成长为领先编码代理，成功完成公司分拆，现在作为独立公司运营于软件开发前沿

Conclusion: 编码代理Amp的成功分拆标志着其在软件开发领域的重要地位，独立运营将有助于其在前沿技术领域取得更大突破

Abstract: Frontier coding agent Amp spins out of Sourcegraph as a separate company (Sponsor) Sourcegraph launched Amp 9 months ago as an experimental coding agent. It quickly grew to become one of the leading coding agents. Now, Amp is its own company operating at the frontier of software development.Read more on the new Amp website >

</details>


### [16] [Read more on the new Amp website >](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fampcode.com%2Fnews%2Famp-inc%3F%26utm_medium=paid%2520sponsored%26utm_source=tldrnewsletter%26utm_campaign=amp_split/1/0100019b0de2e5a4-ecea8ec4-4b82-4da4-8b49-a2b541c02ce3-000000/mHNGb8xT-VFf2cxG3PMfSLLEZXm1FT89gnUNHELgrgw=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sourcegraph将其实验性编码代理Amp分拆为独立公司，该代理在9个月内成长为领先的编码代理之一


<details>
  <summary>Details</summary>
Motivation: Amp作为实验性编码代理在Sourcegraph内部快速成长为领先产品，分拆为独立公司以便更好地专注于软件开发的尖端领域

Method: 将原本作为Sourcegraph内部实验项目的编码代理Amp分拆为独立运营的公司

Result: Amp在9个月内成长为领先的编码代理之一，现在作为独立公司运营于软件开发前沿

Conclusion: 编码代理Amp的成功分拆显示了该技术在软件开发领域的巨大潜力和市场价值

Abstract: Frontier coding agent Amp spins out of Sourcegraph as a separate company (Sponsor) Sourcegraph launched Amp 9 months ago as an experimental coding agent. It quickly grew to become one of the leading coding agents. Now, Amp is its own company operating at the frontier of software development.Read more on the new Amp website >

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [17] [<em class="highlight">强化学习</em>第一篇 ： google SRL 当“死记硬背”遇上“步步惊心”](http://mp.weixin.qq.com/s?__biz=Mzg4MzAxOTEzNQ==&mid=2247483776&idx=1&sn=40efe628faf10b1866365381aab38517&chksm=ce1ca00381455030072fb28d24d776a1e5e051b472ebf4553dc0ff42157e95137aa174eb5abf#rd)
*小熊猫读论文*

Main category: wechat.article

TL;DR: 它不再是盲目地蒙答案，而是在学习**“什么样的逻辑推演是符合客观规律的”**。05 结局这个故事的结局是什么？论文的数据告诉我们，SRL 这种“先学招式，再悟心法，且步步为营”的方法，在复杂的逻辑推理任务上，把那些


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 它不再是盲目地蒙答案，而是在学习**“什么样的逻辑推演是符合客观规律的”**。05 结局这个故事的结局是什么？论文的数据告诉我们，SRL 这种“先学招式，再悟心法，且步步为营”的方法，在复杂的逻辑推理任务上，把那些

</details>


### [18] [Nature|谷歌团队：最先进的<em class="highlight">强化学习</em>算法](http://mp.weixin.qq.com/s?__biz=MzYzMzIxMDc2Mw==&mid=2247483880&idx=1&sn=a6d71c5e38bdc3dd8f6535d61058113b&chksm=f19909ae552f672a1d875aeda278d22e32638e3d750cfb36e9f3d0833dc30cb80142a48e89ed#rd)
*AI论文快读站*

Main category: wechat.article

TL;DR: 「算法基因」双层演化流程图：从伪代码向量到 SOTA 强化学习本图整个研究流程最左侧是把 RL 更新规则拆成 50 余个可微算子并编码成一条“算法基因”向量；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 「算法基因」双层演化流程图：从伪代码向量到 SOTA 强化学习本图整个研究流程最左侧是把 RL 更新规则拆成 50 余个可微算子并编码成一条“算法基因”向量；

</details>


### [19] [<em class="highlight">强化学习</em>应用在自动驾驶中的一些思考](http://mp.weixin.qq.com/s?__biz=MzU2NjU3OTc5NA==&mid=2247605333&idx=2&sn=019b34e93d16c723a9065ff1b6c503a7&chksm=fdafddc79b30e5b1716b99a9fdbdb63e06dc088d498e092f8f617c3e246940c3e6a0fb25bc6a#rd)
*深蓝AI*

Main category: wechat.article

TL;DR: 强化学习的核心价值就是闭环学习，使用一些比较困难的小数据集来做 RL，对模型能力的提升是立竿见影的。当然这也是参考了大模型中 RL 微调的中间形态，如果想要进一步提升模型能力，跟着 tesla 走就行了。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习的核心价值就是闭环学习，使用一些比较困难的小数据集来做 RL，对模型能力的提升是立竿见影的。当然这也是参考了大模型中 RL 微调的中间形态，如果想要进一步提升模型能力，跟着 tesla 走就行了。

</details>


### [20] [AI开始"自我进化"：DeepMind让机器自主设计<em class="highlight">强化学习</em>算法，性能碾压人类智慧结晶](http://mp.weixin.qq.com/s?__biz=MzYzNTE3Mzk3Mw==&mid=2247484371&idx=1&sn=20c70ba0ca59a18ecc0c3de58636713b&chksm=f1e8fabfcd3c6f15acc5e9b013fac9afc4afb45b8453c49139d3437e86c62c27c0890c2e40cf#rd)
*tNature*

Main category: wechat.article

TL;DR: 一、强化学习的"天花板"：当人类智慧遇到瓶颈强化学习（RL）作为AI的核心范式之一，曾在围棋、星际争霸、机器人控制等领域创造传奇。然而，这些辉煌成就背后隐藏着一个尴尬现实：所有算法都依赖人类专家数年甚至数十年


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一、强化学习的"天花板"：当人类智慧遇到瓶颈强化学习（RL）作为AI的核心范式之一，曾在围棋、星际争霸、机器人控制等领域创造传奇。然而，这些辉煌成就背后隐藏着一个尴尬现实：所有算法都依赖人类专家数年甚至数十年

</details>


### [21] [LLM<em class="highlight">强化学习</em>不稳定之谜，被Qwen团队从「一阶近似」视角解开](http://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247663026&idx=1&sn=85e7d452b276c33b62f79500473a18cf&chksm=e89deab7db406d94d22404620f632d6a1c245f43ca35860e7cbdeffea15e54188fb82a7eecf5#rd)
*数据派THU*

Main category: wechat.article

TL;DR: 如今，强化学习（RL）已成为提升大语言模型（LLM）复杂推理与解题能力的关键技术范式，而稳定的训练过程对于成功扩展 RL 至关重要。由于语言具有强烈的上下文属性，LLM 的 RL 通常依赖序列级奖励 —— 即根据完整生成序列


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 如今，强化学习（RL）已成为提升大语言模型（LLM）复杂推理与解题能力的关键技术范式，而稳定的训练过程对于成功扩展 RL 至关重要。由于语言具有强烈的上下文属性，LLM 的 RL 通常依赖序列级奖励 —— 即根据完整生成序列

</details>


### [22] [COMMTR | MARL-OD-DA：面向大规模交通分配的可扩展多智能体<em class="highlight">强化学习</em>框架](http://mp.weixin.qq.com/s?__biz=Mzg2Nzg5ODQyMA==&mid=2247503805&idx=1&sn=9cd108579337b378a7db831665a25889&chksm=cf8487c7f59ede56c4c4ee00e7f6d464af869959763a7e4e59da4cafebe50d17c940b6f5c25b#rd)
*智慧车辆与交通*

Main category: wechat.article

TL;DR: MARL-OD-DA不仅突破了多智能体强化学习在交通分配领域长期存在的可扩展性瓶颈，更通过创新的动作建模与奖励设计，使其在真实城市级交通系统中具备可部署性。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: MARL-OD-DA不仅突破了多智能体强化学习在交通分配领域长期存在的可扩展性瓶颈，更通过创新的动作建模与奖励设计，使其在真实城市级交通系统中具备可部署性。

</details>


### [23] [DRL圣经2025最新版-《<em class="highlight">强化学习</em>:导论第二版》免费pdf分享](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247573366&idx=2&sn=0478bca5a3ff3c2db69cbc163e3986f6&chksm=96e1dd9d27ad5d61264b199d9808cf42e76c9c5d1979093f4a677cbb22d6d2a95816fadd6614#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 我们第二版的目标和第一版的目标是一样的：为强化学习的关键思想和算法提供一个清晰而简单的描述，供所有相关学科的读者阅读。该版本仍然是一个介绍，我们保留了核心，在线学习算法的重点。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们第二版的目标和第一版的目标是一样的：为强化学习的关键思想和算法提供一个清晰而简单的描述，供所有相关学科的读者阅读。该版本仍然是一个介绍，我们保留了核心，在线学习算法的重点。

</details>


### [24] [AI如何“无师自通”：<em class="highlight">强化学习</em>训练奥秘](http://mp.weixin.qq.com/s?__biz=Mzk0NDYxNDQxOQ==&mid=2247484844&idx=1&sn=6d4dbe4528a9b6273014619bbe320582&chksm=c2b6bbc1b64c6557bd92112a1f6d28b006281587858af764d2ee9cea553168e1ade4b03b46ed#rd)
*翌东寰球*

Main category: wechat.article

TL;DR: 04强化学习结合深度学习传统的强化学习在处理电子游戏或自动驾驶等复杂场景时显得力不从心，这是因为状态空间太过庞大，比如Dota 2的状态空间几乎无限。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 04强化学习结合深度学习传统的强化学习在处理电子游戏或自动驾驶等复杂场景时显得力不从心，这是因为状态空间太过庞大，比如Dota 2的状态空间几乎无限。

</details>


### [25] [<em class="highlight">强化学习</em>中的马尔可夫](http://mp.weixin.qq.com/s?__biz=MzkwODY3ODc0MQ==&mid=2247483753&idx=1&sn=dba52b0d9da931430fd5628eef49112d&chksm=c1140b209b7d1ba8cb83d04bb18f6fd86320dae332d7389a55c5384b2402d4387a9180591f99#rd)
*智能原始人*

Main category: wechat.article

TL;DR: #算法 #强化学习


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: #算法 #强化学习

</details>


### [26] [开源 AI 软件工程师封神！Meta + 哈佛推出 CCA，工业级<em class="highlight">代码</em>任务通过率 54.3% 破纪录](http://mp.weixin.qq.com/s?__biz=Mzk1NzMyNDQxNg==&mid=2247488111&idx=1&sn=e7d27457aeafb4420db3262627491fcb&chksm=c20d1c0f83b42b72675645971cd1c555aec08aadf17782f01174d59eaf72ef4a00d82f5ebe98#rd)
*AcademicDaily*

Main category: wechat.article

TL;DR: 近日，Meta 与哈佛大学团队开源了工业级 AI 软件工程师孔子编码智能体（Confucius Code Agent（CCA）），基于全新 Confucius SDK 构建，不仅在 SWE-Bench-Pro 基准测试中创下 54.3% 的 Resole@1 成绩，更实现了 “透明可扩展 + 工业级性能” 的双


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 近日，Meta 与哈佛大学团队开源了工业级 AI 软件工程师孔子编码智能体（Confucius Code Agent（CCA）），基于全新 Confucius SDK 构建，不仅在 SWE-Bench-Pro 基准测试中创下 54.3% 的 Resole@1 成绩，更实现了 “透明可扩展 + 工业级性能” 的双

</details>


### [27] [三分钟看懂生成式AI、AI Agent和<em class="highlight">Agentic</em> AI的本质区别](http://mp.weixin.qq.com/s?__biz=MzYyMTQzNzY2OA==&mid=2247484177&idx=1&sn=4d7560a5c2f447a295426948755abccc&chksm=fe2a934ae871d30c0179c4efc38c3b8dfa38da048e089215a61574214d4063409d2157c7f48b#rd)
*龙大聊AI*

Main category: wechat.article

TL;DR: 本质区别：AI Agent：一个能干的员工Agentic AI：一个能自我协调的团队三者能力对比：从"聊天"到"办事"让我们用一张表格看清楚它们的区别：能力维度生成式AI


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本质区别：AI Agent：一个能干的员工Agentic AI：一个能自我协调的团队三者能力对比：从"聊天"到"办事"让我们用一张表格看清楚它们的区别：能力维度生成式AI

</details>


### [28] [聊聊关于 <em class="highlight">Agentic</em> RL 训推框架的一点看法和思考](http://mp.weixin.qq.com/s?__biz=Mzg4Mjg4NTQxMQ==&mid=2247549601&idx=1&sn=5a02779b3a75f5dbe65f4f2e188b2937&chksm=cea58402e25f9acd1241a364f6886c73ed64a13a03fe64155c1d16f9b40cf6eb265446f3013a#rd)
*大模型之心Tech*

Main category: wechat.article

TL;DR: 目前没有一款可以很好适配多模态模型去做我的需求业务的 agentic rl 训练的框架，当然这也不是框架的原因，主要在于 agentic 环境与具体业务相关，没有办法从框架层面抽象出来一个函数或者类来适配所有的 agentic 环境，这也是


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 目前没有一款可以很好适配多模态模型去做我的需求业务的 agentic rl 训练的框架，当然这也不是框架的原因，主要在于 agentic 环境与具体业务相关，没有办法从框架层面抽象出来一个函数或者类来适配所有的 agentic 环境，这也是

</details>


### [29] [AI <em class="highlight">智能体</em>核心原理综述：从 <em class="highlight">Agentic</em> AI 到 AI Agent](http://mp.weixin.qq.com/s?__biz=Mzg4MjkwMDkxMQ==&mid=2247488946&idx=2&sn=f9c995f5aa6317c90daa664e38a1fe20&chksm=ce4cccde5c9cf3ba8316d538e1bab154a4f532660e09816808257e0fe98e0aa0ec69dfe35b1a#rd)
*AIGC探索*

Main category: wechat.article

TL;DR: agentic ai 的背景 LLM 最初的产品形态是由 OpenAI 领衔的 ChatBot（聊天机器人），底层支撑技术是 Transformer 架构大语言模型，最初专注于语言文本领域的人工智能应用场景。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic ai 的背景 LLM 最初的产品形态是由 OpenAI 领衔的 ChatBot（聊天机器人），底层支撑技术是 Transformer 架构大语言模型，最初专注于语言文本领域的人工智能应用场景。

</details>


### [30] [Google发布！一文了解21种<em class="highlight">Agentic</em>设计模式](http://mp.weixin.qq.com/s?__biz=MzUzOTgwNDMzOQ==&mid=2247504821&idx=1&sn=2452ca9c378c9d24035aa0a5850233f8&chksm=fbb1b98070188f100d387bec5f564a9f1a8b33aade18bbd99482a5ccb44404bec1a4e529723c#rd)
*AINLPer*

Main category: wechat.article

TL;DR: 扫码回复“智能体设计”免费领取原著&中文版PDF如果你想写大模型论文，但却没有合适的idea，我收集整理了来自QS前50名校大佬的大模型研究思路！


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 扫码回复“智能体设计”免费领取原著&中文版PDF如果你想写大模型论文，但却没有合适的idea，我收集整理了来自QS前50名校大佬的大模型研究思路！

</details>


### [31] [智能<em class="highlight">代理</em>式数据编织 <em class="highlight">Agentic</em> Data Fabric](http://mp.weixin.qq.com/s?__biz=Mzg5MTkzMzQ5Ng==&mid=2247485462&idx=1&sn=8aeed61bcb4cacd0c546c85a542c46bf&chksm=ceee8010bc7d5d495f6016c1fb4b64d9692ecb7bf2d9446a4d1bbbb01c1cbe8e53f51456670e#rd)
*凯哥探数*

Main category: wechat.article

TL;DR: 将 Foundry 比作“计划经济下的重工业工厂”，那么 Agentic Data Fabric 就是“市场经济下的按需（Just-in-Time）制造网络”。一、 速度维度的碾压：从 "预先建模" 到 "即时推理"


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 将 Foundry 比作“计划经济下的重工业工厂”，那么 Agentic Data Fabric 就是“市场经济下的按需（Just-in-Time）制造网络”。一、 速度维度的碾压：从 "预先建模" 到 "即时推理"

</details>


### [32] [<em class="highlight">Agentic</em> 组织下的终极拷问：康威定律是否已失效？](http://mp.weixin.qq.com/s?__biz=MzA4NjAzMjEyOA==&mid=2654576523&idx=1&sn=61f14d75497483862ed90b6111e33dda&chksm=8592e53b92531058e0ab8a0fbe625d3418217f70f15059d1f87aabcedcc763575eb8a093e0aa#rd)
*AI运维人机新范式*

Main category: wechat.article

TL;DR: Agentic 组织：伙伴共生与“三人行”AI Native 时代最大的变革在于，Agent 成为了组织中新的、高效率的、无疲劳的“智能伙伴”。在您的“三人行”团队模式中（业务专家、产品经理、技术经理），Agent 的作用是：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic 组织：伙伴共生与“三人行”AI Native 时代最大的变革在于，Agent 成为了组织中新的、高效率的、无疲劳的“智能伙伴”。在您的“三人行”团队模式中（业务专家、产品经理、技术经理），Agent 的作用是：

</details>


### [33] [<em class="highlight">智能体</em>（<em class="highlight">Agentic</em> AI）的进化之道：深度解读自适应框架与未来趋势](http://mp.weixin.qq.com/s?__biz=Mzg3MDY0OTQ0NA==&mid=2247507645&idx=1&sn=7a232eef2b9f44a4d21862bc244851cf&chksm=cf0bc1a01e96ae358c501dd83065bc4522a56558f2526fb0b3a832d26d6b2dce36169797158a#rd)
*Andy730*

Main category: wechat.article

TL;DR: 本次深度解读以一篇前沿研究论文为核心：《Adaptation of Agentic AI》。该论文由Pengcheng Jiang， Jiacheng Lin， Zhiyi Shi等人领衔，汇聚了来自 UIUC、斯坦福、普林斯顿、哈佛、UW 等顶尖机构的研究力量。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本次深度解读以一篇前沿研究论文为核心：《Adaptation of Agentic AI》。该论文由Pengcheng Jiang， Jiacheng Lin， Zhiyi Shi等人领衔，汇聚了来自 UIUC、斯坦福、普林斯顿、哈佛、UW 等顶尖机构的研究力量。

</details>


### [34] [吴恩达最新课程：别再只写Prompt了！掌握<em class="highlight">Agentic</em> AI，让AI自主工作！](http://mp.weixin.qq.com/s?__biz=MzkwMjQwMTQ0Mw==&mid=2247483899&idx=1&sn=2f55f857ab794c1da0caadf9f4382eaf&chksm=c1aa905c1809da497bf93bc0f836c1da162e225afa1034bf79d132d02b678cf5150e8712a36f#rd)
*水豚数智社*

Main category: wechat.article

TL;DR: 这次他带来的新课程名字非常直接，就叫做"Agentic AI"。如果你还在写提示词（Prompt），并期望AI能完成复杂的项目，那你一定也体会过各种无奈！未来谁能用好AI，不在于提示词工程 （Prompt Engineering），而在于如何更好地编排智


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这次他带来的新课程名字非常直接，就叫做"Agentic AI"。如果你还在写提示词（Prompt），并期望AI能完成复杂的项目，那你一定也体会过各种无奈！未来谁能用好AI，不在于提示词工程 （Prompt Engineering），而在于如何更好地编排智

</details>


### [35] [我对育种<em class="highlight">大模型</em>的一些粗浅看法](http://mp.weixin.qq.com/s?__biz=MzI0NzA3MTk2NQ==&mid=2662951542&idx=1&sn=35ada390ee621218c44170781c74bba2&chksm=f35f93206edc5c3af8ee1e7ab53ce8ece78763a3174f8af1c30ce10b8e150f602ca06e4802c8#rd)
*生物信息与育种*

Main category: wechat.article

TL;DR: 五、总结 育种大模型不应是技术炫技，而应是务实增效的工具。它的理想形态是一个 “三层架构”：底层是领域适应的基础模型（安全可控），中层是持续积累的私有数据飞轮（越用越聪明），上层是解决具体场景的智能体（


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 五、总结 育种大模型不应是技术炫技，而应是务实增效的工具。它的理想形态是一个 “三层架构”：底层是领域适应的基础模型（安全可控），中层是持续积累的私有数据飞轮（越用越聪明），上层是解决具体场景的智能体（

</details>


### [36] [<em class="highlight">大模型</em>为何会犯低级逻辑错误？](http://mp.weixin.qq.com/s?__biz=MzA3OTM0NTQwMw==&mid=2649939974&idx=1&sn=dcb6c7a2de8059ba508d3f7ea1a9bfae&chksm=8684120d9457146f7172c76fa43d4591365bc19f5fc254049323271ac0465da537b8a051ebbb#rd)
*新软件*

Main category: wechat.article

TL;DR: 北京大学助理研究员、牛津大学项目中心研究员李昊轩及其团队，在过去半年深耕大语言模型（LLM）的逻辑推理与因果性研究，不仅揭示了大模型“犯糊涂”的底层原因，还提出多智能体协同等创新方案，为AI从“凭相关性判断


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 北京大学助理研究员、牛津大学项目中心研究员李昊轩及其团队，在过去半年深耕大语言模型（LLM）的逻辑推理与因果性研究，不仅揭示了大模型“犯糊涂”的底层原因，还提出多智能体协同等创新方案，为AI从“凭相关性判断

</details>


### [37] [模创观察｜MIIT/TC1重点标准宣介：人工智能<em class="highlight">大模型</em>评测系列标准](http://mp.weixin.qq.com/s?__biz=MzE5MTIyNzIzOQ==&mid=2247484970&idx=2&sn=e06c0faca5fa39eea05988e49d225f76&chksm=97d7865f11e4e74b8cbcca7fc5eefcb98e4a6581712bbfd12fbcd5e5408b11269bc3aa730da7#rd)
*MIC模创社区*

Main category: wechat.article

TL;DR: 目前，大模型评测系列标准已在阿里、百度、华为、百度、腾讯、科大讯飞、中科院等100余家单位应用，全面支撑大模型的研发、优化与评估等关键环节，为大模型技术研发方和行业应用方提供科学、可靠的选型依据与性能优化


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 目前，大模型评测系列标准已在阿里、百度、华为、百度、腾讯、科大讯飞、中科院等100余家单位应用，全面支撑大模型的研发、优化与评估等关键环节，为大模型技术研发方和行业应用方提供科学、可靠的选型依据与性能优化

</details>


### [38] [每周AI<em class="highlight">大模型</em>更新速递12.08~12.14](http://mp.weixin.qq.com/s?__biz=MzAxNjYxOTY3NQ==&mid=2455622945&idx=1&sn=51d51f3fad593cf0f83d4d3464502b44&chksm=8d25a43619788d85d9eb6b57c0ca75f67653814f3893a6b8e44ab494d8257f93ebc90dd10832#rd)
*大模型评测及优化NoneLinear*

Main category: wechat.article

TL;DR: 大模型/agent评测技术交流：关注公众号，发送消息"进群"


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型/agent评测技术交流：关注公众号，发送消息"进群"

</details>


### [39] [MIIT/TC1重点标准宣介 | 人工智能<em class="highlight">大模型</em>评测系列标准](http://mp.weixin.qq.com/s?__biz=Mzg3ODU5NDI0MQ==&mid=2247499714&idx=1&sn=9c11ad87d3454ae85b4892f115001281&chksm=ced7b62f1608d16b4fcfadd262d0af3472b71ad79c13138c306d36f49cf37a13e9dd3eca8aa2#rd)
*可信AI评测*

Main category: wechat.article

TL;DR: 目前，大模型评测系列标准已在阿里、百度、华为、百度、腾讯、科大讯飞、中科院等100余家单位应用，全面支撑大模型的研发、优化与评估等关键环节，为大模型技术研发方和行业应用方提供科学、可靠的选型依据与性能优化


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 目前，大模型评测系列标准已在阿里、百度、华为、百度、腾讯、科大讯飞、中科院等100余家单位应用，全面支撑大模型的研发、优化与评估等关键环节，为大模型技术研发方和行业应用方提供科学、可靠的选型依据与性能优化

</details>


### [40] [<em class="highlight">大模型</em>在测试中的应用：开启智能化测试新时代](http://mp.weixin.qq.com/s?__biz=MjM5ODE3OTkxMQ==&mid=2650575313&idx=1&sn=82c4667c12f87f3b4fe87f2c331cf1aa&chksm=bf5d00340960d6b43bbd06f0de2a8911a447039f151e4211980d4e1df7df15eb9b6f5b855d0a#rd)
*自动化软件测试*

Main category: wechat.article

TL;DR: 解析： 大模型通过自然语言理解，将需求描述转化为可执行代码，极大地提高了测试脚本的开发效率。3. 缺陷预测与静态代码分析 大模型通过学习历史代码和缺陷数据，能够预测可能的缺陷位置，并给出优化建议。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 解析： 大模型通过自然语言理解，将需求描述转化为可执行代码，极大地提高了测试脚本的开发效率。3. 缺陷预测与静态代码分析 大模型通过学习历史代码和缺陷数据，能够预测可能的缺陷位置，并给出优化建议。

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [41] [CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound](https://arxiv.org/abs/2512.11169)
*Akhil S Anand,Elias Aarekol,Martin Mziray Dalseg,Magnus Stalhane,Sebastien Gros*

Main category: cs.AI

TL;DR: 本文提出CORL框架，使用强化学习端到端微调MILP方案，将B&B求解的MILP转化为可微随机策略，以最大化实际运营性能而非精确建模。


<details>
  <summary>Details</summary>
Motivation: 传统MILP建模难以准确表示随机现实问题，导致实际性能不佳。现有机器学习方法依赖监督学习、假设已知最优决策、使用MILP梯度替代，存在局限性。

Method: 提出CORL概念验证框架，将B&B求解的MILP转化为可微随机策略，使用强化学习在真实数据上端到端微调MILP方案，最大化运营性能。

Result: 在简单的组合序贯决策示例中验证了CORL方法的有效性。

Conclusion: CORL框架能够通过强化学习直接优化MILP的实际运营性能，为组合序贯决策问题提供了新的端到端优化方法。

Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.

</details>


### [42] [FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration](https://arxiv.org/abs/2512.11213)
*Dongwon Jung,Peng Shi,Yi Zhang*

Main category: cs.AI

TL;DR: FutureWeaver是一个在固定预算下规划和优化多智能体系统中测试时计算分配的框架，通过模块化协作和双级规划架构提升多智能体协作性能。


<details>
  <summary>Details</summary>
Motivation: 虽然测试时计算扩展能提升大语言模型性能，但在多智能体系统中缺乏原则性的计算分配机制来促进协作、扩展测试时计算到协作交互，以及在明确预算约束下跨智能体分配计算。

Method: 提出FutureWeaver框架：1) 引入模块化协作，通过自玩反思从历史轨迹中抽象出可复用的多智能体工作流作为可调用函数；2) 采用双级规划架构，在当前任务状态推理的同时推测未来步骤，优化计算分配。

Result: 在复杂智能体基准测试中，FutureWeaver在不同预算设置下始终优于基线方法，验证了其在推理时优化中多智能体协作的有效性。

Conclusion: FutureWeaver为多智能体系统中的测试时计算分配提供了原则性框架，通过模块化协作和前瞻性规划有效解决了预算约束下的协作优化问题。

Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.

</details>


### [43] [A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation](https://arxiv.org/abs/2512.11270)
*Hong Je-Gal,Chan-Bin Yi,Hyun-Suk Lee*

Main category: cs.AI

TL;DR: A-LAMP是一个基于LLM的自动化框架，能够将自然语言任务描述自动转换为MDP模型并生成训练好的策略，通过分解建模、编码和训练为可验证阶段来确保语义对齐。


<details>
  <summary>Details</summary>
Motivation: 将强化学习应用于现实任务需要将非正式描述转换为正式的MDP、实现可执行环境并训练策略代理。自动化这一过程面临建模错误、脆弱代码和目标不对齐等挑战，这些因素常常阻碍策略训练。

Method: 提出A-LAMP框架，基于大型语言模型自动将自由形式的自然语言任务描述转换为MDP公式和训练策略。该框架将建模、编码和训练分解为可验证阶段，确保整个流程的语义对齐。

Result: 在经典控制和自定义RL领域中，A-LAMP始终比单个最先进的LLM模型具有更高的策略生成能力。其轻量级变体（基于较小语言模型）也能接近更大模型的性能。案例研究显示A-LAMP生成的环境和策略能保持任务的最优性。

Conclusion: A-LAMP框架通过自动化MDP建模和策略生成，解决了RL应用中的关键挑战，提高了策略生成能力并确保正确性和可靠性，即使使用较小模型也能获得良好性能。

Abstract: Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.

</details>


### [44] [Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance](https://arxiv.org/abs/2512.11421)
*Gonca Gürsun*

Main category: cs.AI

TL;DR: 提出一个LLM智能体框架，通过强化学习形式化环境中的明确行为指导，实现可靠、可验证的多轮任务完成


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理和生成方面表现出色，但在多轮任务中的行为往往缺乏可靠性和可验证性，需要一种能够提供明确行为指导的框架

Method: 框架包含三个组件：轻量级任务分析器选择推理和生成策略；推理模块学习可验证的观察-动作映射；生成模块通过验证或确定性合成确保约束合规输出

Result: 智能体与环境交互时，这些组件共同演化，产生可信赖的行为

Conclusion: 该框架能够使LLM智能体在强化学习形式化的环境中，通过明确的指导实现可靠、可验证的任务完成

Abstract: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.
  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.

</details>


### [45] [AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints](https://arxiv.org/abs/2512.11426)
*Shuowei Cai,Yansong Ning,Hao Liu*

Main category: cs.AI

TL;DR: AgentBalance是一个在明确token成本和延迟预算约束下构建成本效益多智能体系统的框架，采用先骨干后拓扑的设计方法，相比现有方法在相同预算下性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统设计通常优先考虑通信拓扑结构，很少在明确的token成本和延迟预算约束下进行建模和优化，导致在预算约束下成本效益不佳，无法满足大规模部署的实际需求。

Method: 采用先骨干后拓扑的两阶段设计：1) 骨干导向的智能体生成：通过LLM池构建、池选择和角色-骨干匹配构建异构骨干的智能体；2) 自适应MAS拓扑生成：通过智能体表示学习、门控机制和延迟感知拓扑合成来指导智能体间通信。

Result: 在包含14个候选LLM骨干的基准测试中，AgentBalance在匹配的token成本预算下实现高达10%的性能提升，在匹配的延迟预算下实现高达22%的性能提升，并在性能-预算曲线上表现出强大的AUC。

Conclusion: AgentBalance能够在明确的token成本和延迟预算约束下构建成本效益的多智能体系统，可作为现有MAS的插件提升性能，并能很好地泛化到未见过的LLM，实现实用的预算感知部署。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance

</details>


### [46] [Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes](https://arxiv.org/abs/2512.11463)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Minsu Ha,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.AI

TL;DR: Motif-2-12.7B-Reasoning是一个12.7B参数的语言模型，通过系统、数据和算法优化，在复杂推理和长上下文理解方面达到接近前沿专有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决开源模型与专有前沿模型在复杂推理和长上下文理解方面的差距，同时解决推理适应中的模型崩溃和训练不稳定等常见挑战。

Method: 采用综合训练方案：1) 使用混合并行和内核级优化的内存高效基础设施支持64K令牌上下文；2) 两阶段监督微调课程，通过验证对齐的合成数据缓解分布不匹配；3) 强化学习微调管道，通过难度感知数据过滤和混合策略轨迹重用来稳定训练。

Result: Motif-2-12.7B-Reasoning在数学、编码和智能体基准测试中，性能可与参数数量显著更大的模型相媲美。

Conclusion: 该模型为社区提供了一个具有竞争力的开源模型，并为在现实计算约束下扩展推理能力提供了实用的蓝图。

Abstract: We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.

</details>


### [47] [MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition](https://arxiv.org/abs/2512.11682)
*Tim Cofala,Christian Kalfar,Jingge Xiao,Johanna Schrader,Michelle Tang,Wolfgang Nejdl*

Main category: cs.AI

TL;DR: TxAgent是一个用于临床治疗决策的AI代理系统，通过迭代式检索增强生成和统一生物医学工具套件，在CURE-Bench挑战赛中获得卓越表现。


<details>
  <summary>Details</summary>
Motivation: 临床治疗决策是高风险领域，需要AI系统进行多步推理并基于可靠的生物医学知识。医疗应用有严格的安全约束，要求推理轨迹和工具调用序列的准确性。

Method: 使用微调的Llama-3.1-8B模型，通过动态生成和执行函数调用来访问统一生物医学工具套件（ToolUniverse），整合FDA药物API、OpenTargets和Monarch资源。

Result: 在CURE-Bench NeurIPS 2025挑战赛中表现优异，获得开放科学卓越奖。分析显示工具检索质量对整体性能有重要影响，改进工具检索策略可提升性能。

Conclusion: TxAgent展示了代理AI方法在临床治疗决策中的有效性，强调了工具检索质量对系统性能的关键作用，为医疗AI系统评估提供了新视角。

Abstract: Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Agent-Based Modular Learning for Multimodal Emotion Recognition in Human-Agent Systems](https://arxiv.org/abs/2512.10975)
*Matvey Nepomnyaschiy,Oleg Pereziabov,Anvar Tliamov,Stanislav Mikhailov,Ilya Afanasyev*

Main category: cs.LG

TL;DR: 提出一种基于多智能体框架的多模态情感识别系统，通过模块化设计和中央协调器实现高效训练和灵活扩展


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的多模态情感识别模型虽然准确率高，但训练和维护计算成本高，且难以灵活适应模态变化，需要更高效、可扩展的解决方案

Method: 采用多智能体框架，将每个模态编码器和融合分类器设计为自主智能体，由中央监督器协调，支持模块化集成新模态和组件替换

Result: 通过支持视觉、音频和文本模态的概念验证实现，证明了该框架的可行性，提高了训练效率

Conclusion: 该框架不仅提高了训练效率，还为HAI场景中的具身和虚拟智能体设计了更灵活、可扩展和可维护的感知模块

Abstract: Effective human-agent interaction (HAI) relies on accurate and adaptive perception of human emotional states. While multimodal deep learning models - leveraging facial expressions, speech, and textual cues - offer high accuracy in emotion recognition, their training and maintenance are often computationally intensive and inflexible to modality changes. In this work, we propose a novel multi-agent framework for training multimodal emotion recognition systems, where each modality encoder and the fusion classifier operate as autonomous agents coordinated by a central supervisor. This architecture enables modular integration of new modalities (e.g., audio features via emotion2vec), seamless replacement of outdated components, and reduced computational overhead during training. We demonstrate the feasibility of our approach through a proof-of-concept implementation supporting vision, audio, and text modalities, with the classifier serving as a shared decision-making agent. Our framework not only improves training efficiency but also contributes to the design of more flexible, scalable, and maintainable perception modules for embodied and virtual agents in HAI scenarios.

</details>


### [49] [Benchmarking the Generality of Vision-Language-Action Models](https://arxiv.org/abs/2512.11315)
*Pranav Guruprasad,Sudipta Chowdhury,Harsh Sikka,Mridul Sharma,Helen Lu,Sean Rivera,Aryan Khurana,Hangliang Ren,Yangyue Wang*

Main category: cs.LG

TL;DR: MultiNet v1.0是一个统一基准测试，用于评估视觉语言模型和视觉语言动作模型在六个核心能力领域的跨领域泛化能力，发现当前模型在未见领域存在显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体评估分散在孤立基准中，难以判断基础模型是否真正超越了训练分布实现泛化。需要统一基准来衡量模型在跨领域任务中的泛化能力。

Method: 提出MultiNet v1.0基准，涵盖六个核心能力领域：视觉基础、空间推理、工具使用、物理常识、多智能体协调和连续机器人控制。评估了GPT-5、Pi0和Magma等模型。

Result: 所有模型都未展现一致的泛化能力，在未见领域、陌生模态或跨领域任务转换中表现显著下降，尽管在训练分布内表现良好。失败表现为模态错位、输出格式不稳定和领域转移下的灾难性知识退化。

Conclusion: 当前基础模型在泛化智能方面仍存在显著差距。MultiNet v1.0为诊断这些差距和指导未来通用智能体开发提供了标准化评估基础。

Abstract: Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.

</details>


### [50] [Rethinking Expert Trajectory Utilization in LLM Post-training](https://arxiv.org/abs/2512.11470)
*Bowen Ding,Yuhan Chen,Jiayang Lv,Jiyao Yuan,Qi Zhu,Shuangshuang Tian,Dantong Zhu,Futing Wang,Heyuan Deng,Fei Mi,Lifeng Shang,Tao Lin*

Main category: cs.LG

TL;DR: 本文提出塑性-天花板框架，将后训练性能分解为基础SFT性能和后续RL塑性，确定SFT-then-RL为最优流程，并提供了具体的扩展指导原则。


<details>
  <summary>Details</summary>
Motivation: 当前后训练中如何有效利用专家轨迹的问题尚未解决，需要理论框架来指导SFT和RL的最佳整合方式。

Method: 提出塑性-天花板理论框架，通过大量基准测试分析SFT和RL的交互作用，建立SFT-then-RL顺序流程，并推导出具体的扩展指导原则。

Result: 确定SFT-then-RL优于同步方法；在SFT稳定或轻度过拟合阶段转向RL可最大化最终性能；数据规模决定后训练潜力，轨迹难度作为性能乘数；SFT验证损失是选择专家轨迹的可靠指标。

Conclusion: 研究提供了最大化专家轨迹价值的实用指导原则，为后训练优化提供了理论框架和具体操作指南。

Abstract: While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.

</details>


### [51] [Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits](https://arxiv.org/abs/2512.11345)
*Minwoo Park,Junwoo Chang,Jongeun Choi,Roberto Horowitz*

Main category: cs.LG

TL;DR: 论文提出了一种对称感知的扩散策略引导框架，将等变扩散策略与强化学习结合，利用几何对称性提高样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 等变扩散策略结合了扩散模型的生成能力和几何对称性的泛化优势，但直接用标准强化学习引导会忽略对称性，导致样本效率低和不稳定。需要开发能利用对称性的引导框架。

Method: 理论证明等变扩散过程的等变性，构建群不变潜在噪声MDP，提出对称感知引导框架，比较标准、等变和近似等变强化学习策略。

Result: 实验表明利用对称性进行引导能显著提高样本效率，防止价值发散，即使在极有限演示数据训练下也能实现强策略改进。

Conclusion: 对称性在扩散策略引导中至关重要，对称感知框架能有效利用几何对称性提升强化学习性能，同时识别了严格等变性在对称性破坏情况下的实际边界。

Abstract: Equivariant diffusion policies (EDPs) combine the generative expressivity of diffusion models with the strong generalization and sample efficiency afforded by geometric symmetries. While steering these policies with reinforcement learning (RL) offers a promising mechanism for fine-tuning beyond demonstration data, directly applying standard (non-equivariant) RL can be sample-inefficient and unstable, as it ignores the symmetries that EDPs are designed to exploit. In this paper, we theoretically establish that the diffusion process of an EDP is equivariant, which in turn induces a group-invariant latent-noise MDP that is well-suited for equivariant diffusion steering. Building on this theory, we introduce a principled symmetry-aware steering framework and compare standard, equivariant, and approximately equivariant RL strategies through comprehensive experiments across tasks with varying degrees of symmetry. While we identify the practical boundaries of strict equivariance under symmetry breaking, we show that exploiting symmetry during the steering process yields substantial benefits-enhancing sample efficiency, preventing value divergence, and achieving strong policy improvements even when EDPs are trained from extremely limited demonstrations.

</details>


### [52] [Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization](https://arxiv.org/abs/2512.11391)
*Yifan Niu,Han Xiao,Dongyi Liu,Nuo Chen,Jia Li*

Main category: cs.LG

TL;DR: NSPO是一种新颖的强化学习框架，通过将安全策略梯度投影到通用任务的零空间，在保证LLM安全对齐的同时避免遗忘核心能力，显著减少对齐税。


<details>
  <summary>Details</summary>
Motivation: LLM在现实应用中需要确保其行为符合人类价值观、社会规范和伦理原则，但传统的RL安全对齐方法会导致模型遗忘已学习的通用能力（对齐税问题）。

Method: 提出零空间约束策略优化（NSPO），将安全策略梯度几何投影到通用任务的零空间中，理论上证明该方法能保持模型原始核心能力，同时保证安全对齐的有效下降方向。

Result: NSPO大幅优于现有方法，在数学、代码和指令跟随等通用任务上保持准确性的同时，实现了最先进的安全性能。数据效率高，仅需PKU-SafeRLHF中40%的安全数据即可达到良好效果。

Conclusion: NSPO有效解决了LLM安全对齐中的对齐税问题，在保持核心能力的同时实现安全对齐，为LLM的安全部署提供了高效实用的解决方案。

Abstract: As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.

</details>
