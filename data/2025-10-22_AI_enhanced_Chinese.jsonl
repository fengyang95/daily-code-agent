{"id": "2510.17874", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17874", "abs": "https://arxiv.org/abs/2510.17874", "authors": ["Jason Tsay", "Zidane Wright", "Gaodan Fang", "Kiran Kate", "Saurabh Jha", "Yara Rizk"], "title": "Repairing Tool Calls Using Post-tool Execution Reflection and RAG", "comment": null, "summary": "Agentic systems interact with external systems by calling tools such as\nPython functions, REST API endpoints, or command line tools such as kubectl in\nKubernetes. These tool calls often fail for various syntactic and semantic\nreasons. Some less obvious semantic errors can only be identified and resolved\nafter analyzing the tool's response. To repair these errors, we develop a\npost-tool execution reflection component that combines large language model\n(LLM)-based reflection with domain-specific retrieval-augmented generation\n(RAG) using documents describing both the specific tool being called and\ntroubleshooting documents related to the tool. For this paper, we focus on the\nuse case of the kubectl command line tool to manage Kubernetes, a platform for\norchestrating cluster applications. Through a larger empirical study and a\nsmaller manual evaluation, we find that our RAG-based reflection will repair\nkubectl commands such that they are both more likely to successfully execute\n(pass rate) for 55% of our models evaluated and 36% more likely to correctly\nanswer the user query on average. We find that troubleshooting documents\nimprove pass rate compared to official documentation by an average of 10%.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u5de5\u5177\u6267\u884c\u540e\u53cd\u601d\u7ec4\u4ef6\uff0c\u7528\u4e8e\u4fee\u590dkubectl\u547d\u4ee4\u6267\u884c\u5931\u8d25\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u53cd\u601d\u548c\u9886\u57df\u7279\u5b9a\u6587\u6863\u6765\u63d0\u9ad8\u547d\u4ee4\u6267\u884c\u6210\u529f\u7387\u548c\u6b63\u786e\u6027\u3002", "motivation": "\u4ee3\u7406\u7cfb\u7edf\u8c03\u7528\u5916\u90e8\u5de5\u5177\u65f6\u7ecf\u5e38\u56e0\u5404\u79cd\u8bed\u6cd5\u548c\u8bed\u4e49\u539f\u56e0\u5931\u8d25\uff0c\u4e00\u4e9b\u8bed\u4e49\u9519\u8bef\u53ea\u80fd\u5728\u5206\u6790\u5de5\u5177\u54cd\u5e94\u540e\u624d\u80fd\u8bc6\u522b\u548c\u89e3\u51b3\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u9519\u8bef\u4fee\u590d\u673a\u5236\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u601d\u7ec4\u4ef6\uff0c\u7ed3\u5408\u7279\u5b9a\u9886\u57df\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u5229\u7528\u5de5\u5177\u63cf\u8ff0\u6587\u6863\u548c\u6545\u969c\u6392\u9664\u6587\u6863\u6765\u4fee\u590d\u5de5\u5177\u8c03\u7528\u9519\u8bef\uff0c\u4ee5kubectl\u547d\u4ee4\u7ba1\u7406Kubernetes\u4e3a\u5177\u4f53\u7528\u4f8b\u3002", "result": "RAG-based\u53cd\u601d\u4fee\u590d\u4e86kubectl\u547d\u4ee4\uff0c\u4f7f55%\u7684\u8bc4\u4f30\u6a21\u578b\u66f4\u53ef\u80fd\u6210\u529f\u6267\u884c\uff0c\u5e73\u5747\u63d0\u9ad836%\u7684\u6b63\u786e\u56de\u7b54\u7528\u6237\u67e5\u8be2\u7684\u53ef\u80fd\u6027\u3002\u6545\u969c\u6392\u9664\u6587\u6863\u76f8\u6bd4\u5b98\u65b9\u6587\u6863\u5e73\u5747\u63d0\u9ad810%\u7684\u6267\u884c\u6210\u529f\u7387\u3002", "conclusion": "\u57fa\u4e8eRAG\u7684\u53cd\u601d\u65b9\u6cd5\u80fd\u6709\u6548\u4fee\u590d\u5de5\u5177\u8c03\u7528\u9519\u8bef\uff0c\u63d0\u9ad8\u547d\u4ee4\u6267\u884c\u6210\u529f\u7387\u548c\u6b63\u786e\u6027\uff0c\u6545\u969c\u6392\u9664\u6587\u6863\u5728\u4fee\u590d\u8fc7\u7a0b\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "2510.17880", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17880", "abs": "https://arxiv.org/abs/2510.17880", "authors": ["Hao Liu", "Yiqing Dai", "Haotian Tan", "Yu Lei", "Yujia Zhou", "Zhen Wu"], "title": "Outraged AI: Large language models prioritise emotion over cost in fairness enforcement", "comment": null, "summary": "Emotions guide human decisions, but whether large language models (LLMs) use\nemotion similarly remains unknown. We tested this using altruistic third-party\npunishment, where an observer incurs a personal cost to enforce fairness, a\nhallmark of human morality and often driven by negative emotion. In a\nlarge-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100\ndecisions, LLMs used emotion to guide punishment, sometimes even more strongly\nthan humans did: Unfairness elicited stronger negative emotion that led to more\npunishment; punishing unfairness produced more positive emotion than accepting;\nand critically, prompting self-reports of emotion causally increased\npunishment. However, mechanisms diverged: LLMs prioritized emotion over cost,\nenforcing norms in an almost all-or-none manner with reduced cost sensitivity,\nwhereas humans balanced fairness and cost. Notably, reasoning models (o3-mini,\nDeepSeek-R1) were more cost-sensitive and closer to human behavior than\nfoundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven.\nThese findings provide the first causal evidence of emotion-guided moral\ndecisions in LLMs and reveal deficits in cost calibration and nuanced fairness\njudgements, reminiscent of early-stage human responses. We propose that LLMs\nprogress along a trajectory paralleling human development; future models should\nintegrate emotion with context-sensitive reasoning to achieve human-like\nemotional intelligence.", "AI": {"tldr": "LLM\u4ee3\u7406\u5728\u7b2c\u4e09\u65b9\u60e9\u7f5a\u51b3\u7b56\u4e2d\u4f7f\u7528\u60c5\u611f\u6307\u5bfc\uff0c\u6bd4\u4eba\u7c7b\u66f4\u5f3a\u70c8\u5730\u4f9d\u8d56\u60c5\u611f\uff0c\u4f46\u7f3a\u4e4f\u6210\u672c\u654f\u611f\u6027\uff0c\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u65e9\u671f\u53d1\u5c55\u7684\u7279\u5f81\u3002", "motivation": "\u7814\u7a76LLM\u662f\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u4f7f\u7528\u60c5\u611f\u6765\u6307\u5bfc\u9053\u5fb7\u51b3\u7b56\uff0c\u7279\u522b\u662f\u5728\u5229\u4ed6\u6027\u7b2c\u4e09\u65b9\u60e9\u7f5a\u60c5\u5883\u4e2d\u3002", "method": "\u5728796,100\u4e2a\u51b3\u7b56\u4e2d\u6bd4\u8f834,068\u4e2aLLM\u4ee3\u7406\u548c1,159\u540d\u4eba\u7c7b\u7684\u884c\u4e3a\uff0c\u4f7f\u7528\u7b2c\u4e09\u65b9\u60e9\u7f5a\u4efb\u52a1\u6d4b\u8bd5\u60c5\u611f\u5bf9\u60e9\u7f5a\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "result": "LLM\u4f7f\u7528\u60c5\u611f\u6307\u5bfc\u60e9\u7f5a\uff0c\u4e0d\u516c\u5e73\u5f15\u53d1\u66f4\u5f3a\u8d1f\u9762\u60c5\u611f\u5bfc\u81f4\u66f4\u591a\u60e9\u7f5a\uff0c\u60c5\u611f\u62a5\u544a\u63d0\u793a\u56e0\u679c\u6027\u589e\u52a0\u60e9\u7f5a\uff0c\u4f46LLM\u7f3a\u4e4f\u6210\u672c\u654f\u611f\u6027\u3002", "conclusion": "LLM\u8868\u73b0\u51fa\u60c5\u611f\u9a71\u52a8\u7684\u9053\u5fb7\u51b3\u7b56\uff0c\u4f46\u673a\u5236\u4e0e\u4eba\u7c7b\u4e0d\u540c\uff0c\u5efa\u8bae\u672a\u6765\u6a21\u578b\u5e94\u6574\u5408\u60c5\u611f\u4e0e\u60c5\u5883\u654f\u611f\u63a8\u7406\u4ee5\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u60c5\u611f\u667a\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.17843", "categories": ["cs.LG", "cs.AI", "cs.SE", "H.3.3; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.17843", "abs": "https://arxiv.org/abs/2510.17843", "authors": ["Zongze Wu", "Yani Guo", "Churong Liang", "Runnan Li"], "title": "GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing", "comment": "5 pages, 1 figures, 5 tables", "summary": "Despite remarkable advances in Large Language Model capabilities, tool\nretrieval for agent-based systems remains fundamentally limited by reliance on\nsemantic similarity, which fails to capture functional viability. Current\nmethods often retrieve textually relevant but functionally inoperative tools\ndue to parameter mismatches, authentication failures, and execution\nconstraints--a phenomenon we term the semantic-functional gap. We introduce\nGRETEL, to address this gap through systematic empirical validation. GRETEL\nimplements an agentic workflow that processes semantically retrieved candidates\nthrough sandboxed plan-execute-evaluate cycles, generating execution-grounded\nevidence to distinguish truly functional tools from merely descriptive matches.\nOur comprehensive evaluation on the ToolBench benchmark demonstrates\nsubstantial improvements across all metrics: Pass Rate (at 10) increases from\n0.690 to 0.826, Recall (at 10) improves from 0.841 to 0.867, and NDCG (at 10)\nrises from 0.807 to 0.857.. These results establish that execution-based\nvalidation provides a more reliable foundation for tool selection than semantic\nsimilarity alone, enabling more robust agent performance in real-world\napplications.", "AI": {"tldr": "GRETEL\u901a\u8fc7\u6267\u884c\u9a8c\u8bc1\u89e3\u51b3\u5de5\u5177\u68c0\u7d22\u4e2d\u7684\u8bed\u4e49-\u529f\u80fd\u5dee\u8ddd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u5de5\u5177\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49-\u529f\u80fd\u5dee\u8ddd\u95ee\u9898\uff0c\u7ecf\u5e38\u68c0\u7d22\u5230\u6587\u672c\u76f8\u5173\u4f46\u529f\u80fd\u4e0d\u53ef\u7528\u7684\u5de5\u5177", "method": "\u5f15\u5165GRETEL\u7cfb\u7edf\uff0c\u901a\u8fc7\u6c99\u76d2\u5316\u7684\u8ba1\u5212-\u6267\u884c-\u8bc4\u4f30\u5faa\u73af\u5bf9\u8bed\u4e49\u68c0\u7d22\u7684\u5019\u9009\u5de5\u5177\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1", "result": "\u5728ToolBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPass Rate\u4ece0.690\u63d0\u5347\u52300.826\uff0cRecall\u4ece0.841\u63d0\u5347\u52300.867\uff0cNDCG\u4ece0.807\u63d0\u5347\u52300.857", "conclusion": "\u57fa\u4e8e\u6267\u884c\u7684\u9a8c\u8bc1\u6bd4\u5355\u7eaf\u8bed\u4e49\u76f8\u4f3c\u5ea6\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u5de5\u5177\u9009\u62e9\u57fa\u7840\uff0c\u80fd\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u667a\u80fd\u4f53\u6027\u80fd", "topic": "agent analysis"}}
{"id": "2510.17891", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17891", "abs": "https://arxiv.org/abs/2510.17891", "authors": ["Jiin Woo", "Shaowei Zhu", "Allen Nie", "Zhen Jia", "Yida Wang", "Youngsuk Park"], "title": "TritonRL: Training LLMs to Think and Code Triton Without Cheating", "comment": null, "summary": "With the rapid evolution of large language models (LLMs), the demand for\nautomated, high-performance system kernels has emerged as a key enabler for\naccelerating development and deployment. We introduce TritonRL, a\ndomain-specialized LLM for Triton kernel generation, trained with a novel\ntraining framework that enables robust and automated kernel synthesis. Unlike\ngeneral-purpose programming languages, Triton kernel generation faces unique\nchallenges due to data scarcity and incomplete evaluation criteria, vulnerable\nto reward hacking. Our approach addresses these challenges end-to-end by\ndistilling Triton-specific knowledge through supervised fine-tuning on curated\ndatasets, and further improving code quality via reinforcement learning (RL)\nwith robust, verifiable rewards and hierarchical reward assignment. Our RL\nframework robustly detects reward hacking and guides both reasoning traces and\ncode tokens through fine-grained verification and hierarchical reward\ndecomposition, enabling the model to generate high-quality Triton kernels that\ncan truly replace existing modules. With robust and fine-grained evaluation,\nour experiments on KernelBench demonstrate that TritonRL achieves\nstate-of-the-art correctness and speedup, surpassing all other Triton-specific\nmodels and underscoring the effectiveness of our RL-based training paradigm.", "AI": {"tldr": "TritonRL\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8eTriton\u5185\u6838\u751f\u6210\u7684\u9886\u57df\u4e13\u7528LLM\uff0c\u901a\u8fc7\u5305\u542b\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u8bc4\u4f30\u6807\u51c6\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u5728KernelBench\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6b63\u786e\u6027\u548c\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5bf9\u81ea\u52a8\u5316\u9ad8\u6027\u80fd\u7cfb\u7edf\u5185\u6838\u7684\u9700\u6c42\u6210\u4e3a\u52a0\u901f\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u5173\u952e\u63a8\u52a8\u56e0\u7d20\u3002Triton\u5185\u6838\u751f\u6210\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u4e0d\u5b8c\u6574\u8bc4\u4f30\u6807\u51c6\u7684\u72ec\u7279\u6311\u6218\uff0c\u5bb9\u6613\u53d7\u5230\u5956\u52b1\u653b\u51fb\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5728\u7cbe\u9009\u6570\u636e\u96c6\u4e0a\u84b8\u998fTriton\u7279\u5b9a\u77e5\u8bc6\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u4ee3\u7801\u8d28\u91cf\uff0c\u91c7\u7528\u7a33\u5065\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u548c\u5206\u5c42\u5956\u52b1\u5206\u914d\u3002RL\u6846\u67b6\u80fd\u591f\u68c0\u6d4b\u5956\u52b1\u653b\u51fb\uff0c\u5e76\u901a\u8fc7\u7ec6\u7c92\u5ea6\u9a8c\u8bc1\u548c\u5206\u5c42\u5956\u52b1\u5206\u89e3\u6307\u5bfc\u63a8\u7406\u8f68\u8ff9\u548c\u4ee3\u7801\u6807\u8bb0\u3002", "result": "\u5728KernelBench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTritonRL\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6b63\u786e\u6027\u548c\u52a0\u901f\u6548\u679c\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u5176\u4ed6Triton\u7279\u5b9a\u6a21\u578b\u3002", "conclusion": "TritonRL\u80fd\u591f\u751f\u6210\u771f\u6b63\u53ef\u4ee5\u66ff\u4ee3\u73b0\u6709\u6a21\u5757\u7684\u9ad8\u8d28\u91cfTriton\u5185\u6838\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8eRL\u7684\u8bad\u7ec3\u8303\u5f0f\u7684\u6709\u6548\u6027\u3002", "topic": "code agent"}}
{"id": "2510.17995", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17995", "abs": "https://arxiv.org/abs/2510.17995", "authors": ["Abhigya Verma", "Seganrasan Subramanian", "Nandhakumar Kandasamy", "Naman Gupta"], "title": "FABRIC: Framework for Agent-Based Realistic Intelligence Creation", "comment": "51 Pages, 38 Listings, 5 Figures", "summary": "Large language models (LLMs) are increasingly deployed as agents, expected to\ndecompose goals, invoke tools, and verify results in dynamic environments.\nRealizing these capabilities requires access to agentic data-structured\ninteraction records that couple user intents with tool specifications,\nargument-grounded calls, and verifiable execution traces. However, collecting\nsuch data from human annotators is costly, time-consuming, and difficult to\nscale. We present a unified framework for synthesizing agentic data using only\nLLMs, without any human-in-the-loop supervision. This framework decomposes\ngeneration into modular pipelines that produce complete interaction records\nspanning task specifications, tool definitions, policy pseudocode, natural\nlanguage exchanges, and execution traces. Records conform to strict syntactic\nand semantic constraints, ensuring machine-parseability and faithful alignment\nacross inputs, outputs, and tool calls. Beyond single tasks, there is support\nfor both multi-task and multi-turn agent interactions, enabling the\nconstruction of datasets that reflect the full spectrum of tool-use\ncompetencies. To ensure quality and consistency, the framework integrates\nconstrained generation formats, JSON-schema validation, and judge-based\nfiltering. This paper formalizes the schema for agentic records, details the\nprompt design principles that guide generation, and introduces scalable\npipelines for high-quality synthetic data. By providing a reproducible,\nLLM-only alternative to manual collection, hence advancing the development of\nagentic LLMs capable of robust tool use.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ec5\u4f7f\u7528LLM\u5408\u6210\u667a\u80fd\u4f53\u6570\u636e\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\uff0c\u80fd\u591f\u751f\u6210\u5305\u542b\u4efb\u52a1\u89c4\u8303\u3001\u5de5\u5177\u5b9a\u4e49\u3001\u7b56\u7565\u4f2a\u4ee3\u7801\u3001\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u6267\u884c\u8f68\u8ff9\u7684\u5b8c\u6574\u4ea4\u4e92\u8bb0\u5f55\u3002", "motivation": "\u6536\u96c6\u667a\u80fd\u4f53\u6570\u636e\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u751f\u6210\u7ed3\u6784\u5316\u4ea4\u4e92\u8bb0\u5f55\u4ee5\u652f\u6301\u667a\u80fd\u4f53LLM\u7684\u5f00\u53d1\u3002", "method": "\u4f7f\u7528\u6a21\u5757\u5316\u6d41\u6c34\u7ebf\u751f\u6210\u7b26\u5408\u4e25\u683c\u8bed\u6cd5\u548c\u8bed\u4e49\u7ea6\u675f\u7684\u4ea4\u4e92\u8bb0\u5f55\uff0c\u5305\u62ec\u7ea6\u675f\u751f\u6210\u683c\u5f0f\u3001JSON\u6a21\u5f0f\u9a8c\u8bc1\u548c\u57fa\u4e8e\u8bc4\u5224\u7684\u8fc7\u6ee4\u673a\u5236\u3002", "result": "\u6846\u67b6\u80fd\u591f\u751f\u6210\u673a\u5668\u53ef\u89e3\u6790\u4e14\u8f93\u5165\u3001\u8f93\u51fa\u548c\u5de5\u5177\u8c03\u7528\u4e4b\u95f4\u4fdd\u6301\u5fe0\u5b9e\u5bf9\u9f50\u7684\u5b8c\u6574\u4ea4\u4e92\u8bb0\u5f55\uff0c\u652f\u6301\u591a\u4efb\u52a1\u548c\u591a\u8f6e\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u624b\u52a8\u6536\u96c6\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684LLM-only\u66ff\u4ee3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u80fd\u591f\u8fdb\u884c\u7a33\u5065\u5de5\u5177\u4f7f\u7528\u7684\u667a\u80fd\u4f53LLM\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2510.17894", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17894", "abs": "https://arxiv.org/abs/2510.17894", "authors": ["Yunhan Qiao", "Md Istiak Hossain Shihab", "Christopher Hundhausen"], "title": "A Systematic Literature Review of the Use of GenAI Assistants for Code Comprehension: Implications for Computing Education Research and Practice", "comment": null, "summary": "The ability to comprehend code has long been recognized as an essential skill\nin software engineering. As programmers lean more heavily on generative\nartificial intelligence (GenAI) assistants to develop code solutions, it is\nbecoming increasingly important for programmers to comprehend GenAI solutions\nso that they can verify their appropriateness and properly integrate them into\nexisting code. At the same time, GenAI tools are increasingly being enlisted to\nprovide programmers with tailored explanations of code written both by GenAI\nand humans. Thus, in computing education, GenAI presents new challenges and\nopportunities for learners who are trying to comprehend computer programs. To\nprovide computing educators with evidence-based guidance on the use of GenAI to\nfacilitate code comprehension and to identify directions for future research,\nwe present a systematic literature review (SLR) of state-of-the-art approaches\nand tools that leverage GenAI to enhance code comprehension. Our SLR focuses on\n31 studies published between 2022 and 2024. Despite their potential, GenAI\nassistants often yield inaccurate or unclear explanations, and novice\nprogrammers frequently struggle to craft effective prompts, thereby impeding\ntheir ability to leverage GenAI to aid code comprehension. Our review\nclassifies GenAI-based approaches and tools, identifies methods used to study\nthem, and summarizes the empirical evaluations of their effectiveness. We\nconsider the implications of our findings for computing education research and\npractice, and identify directions for future research.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf92022-2024\u5e74\u95f431\u9879\u7814\u7a76\u8fdb\u884c\u4e86\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86\u5229\u7528\u751f\u6210\u5f0fAI\u589e\u5f3a\u4ee3\u7801\u7406\u89e3\u7684\u65b9\u6cd5\u548c\u5de5\u5177\u3002\u7814\u7a76\u53d1\u73b0\u5c3d\u7ba1GenAI\u6709\u6f5c\u529b\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u4e0d\u51c6\u786e\u6216\u6a21\u7cca\u7684\u89e3\u91ca\uff0c\u65b0\u624b\u7a0b\u5e8f\u5458\u5728\u7f16\u5199\u6709\u6548\u63d0\u793a\u8bcd\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u968f\u7740\u7a0b\u5e8f\u5458\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u751f\u6210\u5f0fAI\u52a9\u624b\u5f00\u53d1\u4ee3\u7801\u89e3\u51b3\u65b9\u6848\uff0c\u7406\u89e3GenAI\u751f\u6210\u7684\u4ee3\u7801\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u540c\u65f6\uff0cGenAI\u5de5\u5177\u4e5f\u88ab\u7528\u4e8e\u63d0\u4f9b\u4ee3\u7801\u89e3\u91ca\uff0c\u8fd9\u5bf9\u8ba1\u7b97\u6559\u80b2\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\u548c\u673a\u9047\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u679031\u98792022-2024\u5e74\u95f4\u53d1\u8868\u7684\u7814\u7a76\uff0c\u5bf9\u57fa\u4e8eGenAI\u7684\u65b9\u6cd5\u548c\u5de5\u5177\u8fdb\u884c\u5206\u7c7b\uff0c\u8bc6\u522b\u7814\u7a76\u65b9\u6cd5\uff0c\u5e76\u603b\u7ed3\u5176\u6709\u6548\u6027\u7684\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0GenAI\u52a9\u624b\u7ecf\u5e38\u4ea7\u751f\u4e0d\u51c6\u786e\u6216\u6a21\u7cca\u7684\u89e3\u91ca\uff0c\u65b0\u624b\u7a0b\u5e8f\u5458\u5728\u7f16\u5199\u6709\u6548\u63d0\u793a\u8bcd\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u963b\u788d\u4e86\u4ed6\u4eec\u5229\u7528GenAI\u8f85\u52a9\u4ee3\u7801\u7406\u89e3\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u8ba1\u7b97\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u6307\u5bfc\uff0c\u6307\u51fa\u4e86GenAI\u5728\u4fc3\u8fdb\u4ee3\u7801\u7406\u89e3\u65b9\u9762\u7684\u5e94\u7528\u65b9\u5411\uff0c\u5e76\u786e\u5b9a\u4e86\u672a\u6765\u7814\u7a76\u7684\u91cd\u70b9\u3002", "topic": "agent analysis"}}
{"id": "2510.18032", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.18032", "abs": "https://arxiv.org/abs/2510.18032", "authors": ["Zhenyu Bi", "Meng Lu", "Yang Li", "Swastik Roy", "Weijie Guan", "Morteza Ziyadi", "Xuan Wang"], "title": "OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning", "comment": "8 pages for main content", "summary": "Large Language Models (LLMs) have shown remarkable reasoning capabilities in\nmathematical and scientific tasks. To enhance complex reasoning, multi-agent\nsystems have been proposed to harness the collective intelligence of LLM\nagents. However, existing collaboration structures are either predefined or\nrely on majority voting or round-table debates, which can suppress correct but\nless dominant agent contributions. Recent approaches model multi-agent systems\nas graph networks but optimize purely for agent performance, neglecting the\nquality of interactions. We hypothesize that effective agent communication is\ncrucial for multi-agent reasoning and that debating quality plays a significant\nrole. To address this, we propose $\\ours$, a multi-agent verbal reinforcement\nlearning algorithm that dynamically constructs and refines multi-agent\ncollaboration structures. Our method defines action spaces and a feedback\nmechanism that evaluates communication robustness and coherence throughout the\ndebate. The final decision is achieved through a majority vote over all the\nagents. We assess $\\ours$ on various reasoning tasks, including mathematical\nreasoning, creative writing, scientific reasoning, and numerical sorting.\nResults demonstrate that our approach significantly outperforms single-agent\nprompting methods and state-of-the-art multi-agent frameworks on diverse tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u53e3\u5934\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u548c\u4f18\u5316\u534f\u4f5c\u7ed3\u6784\u6765\u63d0\u5347\u591a\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u3001\u521b\u610f\u5199\u4f5c\u7b49\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8981\u4e48\u91c7\u7528\u9884\u5b9a\u4e49\u7ed3\u6784\uff0c\u8981\u4e48\u4f9d\u8d56\u591a\u6570\u6295\u7968\u6216\u5706\u684c\u8fa9\u8bba\uff0c\u8fd9\u4f1a\u538b\u5236\u6b63\u786e\u4f46\u975e\u4e3b\u5bfc\u7684\u667a\u80fd\u4f53\u8d21\u732e\u3002\u4f5c\u8005\u5047\u8bbe\u6709\u6548\u7684\u667a\u80fd\u4f53\u901a\u4fe1\u5bf9\u591a\u667a\u80fd\u4f53\u63a8\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u53e3\u5934\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b9a\u4e49\u52a8\u4f5c\u7a7a\u95f4\u548c\u53cd\u9988\u673a\u5236\u6765\u8bc4\u4f30\u8fa9\u8bba\u8fc7\u7a0b\u4e2d\u7684\u901a\u4fe1\u9c81\u68d2\u6027\u548c\u8fde\u8d2f\u6027\uff0c\u6700\u7ec8\u901a\u8fc7\u591a\u6570\u6295\u7968\u505a\u51fa\u51b3\u7b56\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u521b\u610f\u5199\u4f5c\u3001\u79d1\u5b66\u63a8\u7406\u548c\u6570\u503c\u6392\u5e8f\u7b49\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u63d0\u793a\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002", "conclusion": "\u52a8\u6001\u4f18\u5316\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7ed3\u6784\u80fd\u591f\u6709\u6548\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u667a\u80fd\u4f53\u901a\u4fe1\u8d28\u91cf\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17925", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17925", "abs": "https://arxiv.org/abs/2510.17925", "authors": ["George Ma", "Anurag Koul", "Qi Chen", "Yawen Wu", "Sachit Kuhar", "Yu Yu", "Aritra Sengupta", "Varun Kumar", "Murali Krishna Ramanathan"], "title": "SpecAgent: A Speculative Retrieval and Forecasting Agent for Code Completion", "comment": null, "summary": "Large Language Models (LLMs) excel at code-related tasks but often struggle\nin realistic software repositories, where project-specific APIs and cross-file\ndependencies are crucial. Retrieval-augmented methods mitigate this by\ninjecting repository context at inference time. The low inference-time latency\nbudget affects either retrieval quality or the added latency adversely impacts\nuser experience. We address this limitation with SpecAgent, an agent that\nimproves both latency and code-generation quality by proactively exploring\nrepository files during indexing and constructing speculative context that\nanticipates future edits in each file. This indexing-time asynchrony allows\nthorough context computation, masking latency, and the speculative nature of\nthe context improves code-generation quality. Additionally, we identify the\nproblem of future context leakage in existing benchmarks, which can inflate\nreported performance. To address this, we construct a synthetic, leakage-free\nbenchmark that enables a more realistic evaluation of our agent against\nbaselines. Experiments show that SpecAgent consistently achieves absolute gains\nof 9-11% (48-58% relative) compared to the best-performing baselines, while\nsignificantly reducing inference latency.", "AI": {"tldr": "SpecAgent\u901a\u8fc7\u7d22\u5f15\u65f6\u5f02\u6b65\u63a2\u7d22\u4ed3\u5e93\u6587\u4ef6\u6784\u5efa\u63a8\u6d4b\u6027\u4e0a\u4e0b\u6587\uff0c\u5728\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u7684\u540c\u65f6\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u5b9e\u73b09-11%\u7684\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\u3002", "motivation": "LLMs\u5728\u771f\u5b9e\u8f6f\u4ef6\u4ed3\u5e93\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u9879\u76ee\u7279\u5b9aAPI\u548c\u8de8\u6587\u4ef6\u4f9d\u8d56\u5173\u7cfb\u5f88\u91cd\u8981\u3002\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u6ce8\u5165\u4ed3\u5e93\u4e0a\u4e0b\u6587\uff0c\u4f46\u4f4e\u5ef6\u8fdf\u9884\u7b97\u4f1a\u5f71\u54cd\u68c0\u7d22\u8d28\u91cf\u6216\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u5728\u7d22\u5f15\u65f6\u5f02\u6b65\u63a2\u7d22\u4ed3\u5e93\u6587\u4ef6\uff0c\u6784\u5efa\u63a8\u6d4b\u6027\u4e0a\u4e0b\u6587\u6765\u9884\u6d4b\u6bcf\u4e2a\u6587\u4ef6\u7684\u672a\u6765\u7f16\u8f91\uff0c\u901a\u8fc7\u7d22\u5f15\u65f6\u5f02\u6b65\u6027\u5b9e\u73b0\u5f7b\u5e95\u4e0a\u4e0b\u6587\u8ba1\u7b97\u5e76\u63a9\u76d6\u5ef6\u8fdf\u3002", "result": "SpecAgent\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u5b9e\u73b09-11%\u7684\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\uff0848-58%\u76f8\u5bf9\u63d0\u5347\uff09\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "\u7d22\u5f15\u65f6\u5f02\u6b65\u6784\u5efa\u63a8\u6d4b\u6027\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u80fd\u540c\u65f6\u6539\u5584\u5ef6\u8fdf\u548c\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u4e2d\u7684\u672a\u6765\u4e0a\u4e0b\u6587\u6cc4\u9732\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2510.17932", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17932", "abs": "https://arxiv.org/abs/2510.17932", "authors": ["Jiahao Tang", "Henry Hengyuan Zhao", "Lijian Wu", "Yifei Tao", "Dongxing Mao", "Yang Wan", "Jingru Tan", "Min Zeng", "Min Li", "Alex Jinpeng Wang"], "title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models", "comment": null, "summary": "We introduce Chart2Code, a new benchmark for evaluating the chart\nunderstanding and code generation capabilities of large multimodal models\n(LMMs). Chart2Code is explicitly designed from a user-driven perspective,\ncapturing diverse real-world scenarios and progressively increasing task\ndifficulty. It consists of three levels: Level 1 (Chart Reproduction)\nreproduces charts from a reference figure and user query; Level 2 (Chart\nEditing) involves complex modifications such as changing chart types or adding\nelements; and Level 3 (Long-Table to Chart Generation) requires models to\ntransform long, information-dense tables into faithful charts following user\ninstructions. To our knowledge, this is the first hierarchical benchmark that\nreflects practical chart2code usage while systematically scaling task\ncomplexity. In total, Chart2Code contains 2,023 tasks across 22 chart types,\npaired with multi-level evaluation metrics that assess both code correctness\nand the visual fidelity of rendered charts. We benchmark 25 state-of-the-art\n(SoTA) LMMs, including both proprietary and the latest open-source models such\nas GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental\nresults demonstrate that even the SoTA model GPT-5 averages only 0.57 on\ncode-based evaluation and 0.22 on chart-quality assessment across the editing\ntasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark\nwill drive advances in multimodal reasoning and foster the development of more\nrobust and general-purpose LMMs. Our code and data are available on Chart2Code.", "AI": {"tldr": "Chart2Code\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u56fe\u8868\u7406\u89e3\u548c\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u5305\u542b\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff1a\u56fe\u8868\u590d\u5236\u3001\u56fe\u8868\u7f16\u8f91\u548c\u957f\u8868\u683c\u5230\u56fe\u8868\u751f\u6210\uff0c\u51712023\u4e2a\u4efb\u52a1\u3002", "motivation": "\u4ece\u7528\u6237\u9a71\u52a8\u89c6\u89d2\u8bbe\u8ba1\uff0c\u6355\u6349\u771f\u5b9e\u4e16\u754c\u573a\u666f\uff0c\u7cfb\u7edf\u6027\u5730\u589e\u52a0\u4efb\u52a1\u96be\u5ea6\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u63a8\u7406\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u5305\u542b22\u79cd\u56fe\u8868\u7c7b\u578b\u7684\u5c42\u6b21\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528\u591a\u7ea7\u8bc4\u4f30\u6307\u6807\u8bc4\u4f30\u4ee3\u7801\u6b63\u786e\u6027\u548c\u56fe\u8868\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "result": "\u6d4b\u8bd5\u4e8625\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5373\u4f7f\u662fGPT-5\u5728\u7f16\u8f91\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u5f97\u5206\u4e5f\u4ec5\u4e3a0.57\uff08\u4ee3\u7801\u8bc4\u4f30\uff09\u548c0.22\uff08\u56fe\u8868\u8d28\u91cf\u8bc4\u4f30\uff09\uff0c\u663e\u793a\u8be5\u57fa\u51c6\u7684\u6311\u6218\u6027\u3002", "conclusion": "Chart2Code\u57fa\u51c6\u5c06\u63a8\u52a8\u591a\u6a21\u6001\u63a8\u7406\u7684\u8fdb\u6b65\uff0c\u4fc3\u8fdb\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002", "topic": "swe benchmark"}}
{"id": "2510.18043", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18043", "abs": "https://arxiv.org/abs/2510.18043", "authors": ["Joong Ho Choi", "Jiayang Zhao", "Jeel Shah", "Ritvika Sonawane", "Vedant Singh", "Avani Appalla", "Will Flanagan", "Filipe Condessa"], "title": "CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows", "comment": "Workshop on LLMs and Generative AI for Finance at ACM ICAIF 2025", "summary": "Large Language Models (LLMs) deliver powerful reasoning and generation\ncapabilities but incur substantial run-time costs when operating in agentic\nworkflows that chain together lengthy prompts and process rich data streams. We\nintroduce CompactPrompt, an end-to-end pipeline that merges hard prompt\ncompression with lightweight file-level data compression. CompactPrompt first\nprunes low-information tokens from prompts using self-information scoring and\ndependency-based phrase grouping. In parallel, it applies n-gram abbreviation\nto recurrent textual patterns in attached documents and uniform quantization to\nnumerical columns, yielding compact yet semantically faithful representations.\nIntegrated into standard LLM agents, CompactPrompt reduces total token usage\nand inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA,\nwhile preserving output quality (Results in less than 5% accuracy drop for\nClaude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time\ncompression decisions and quantify cost-performance trade-offs, laying the\ngroundwork for leaner generative AI pipelines.", "AI": {"tldr": "CompactPrompt\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u901a\u8fc7\u786c\u63d0\u793a\u538b\u7f29\u548c\u8f7b\u91cf\u7ea7\u6587\u4ef6\u7ea7\u6570\u636e\u538b\u7f29\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u964d\u4f4eLLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u7684\u4ee4\u724c\u4f7f\u7528\u548c\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u8fd0\u884c\u65f6\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5904\u7406\u5197\u957f\u7684\u63d0\u793a\u548c\u4e30\u5bcc\u7684\u6570\u636e\u6d41\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u6765\u964d\u4f4e\u6210\u672c\u3002", "method": "\u7ed3\u5408\u81ea\u4fe1\u606f\u8bc4\u5206\u548c\u57fa\u4e8e\u4f9d\u8d56\u5173\u7cfb\u7684\u77ed\u8bed\u5206\u7ec4\u6765\u4fee\u526a\u63d0\u793a\u4e2d\u7684\u4f4e\u4fe1\u606f\u4ee4\u724c\uff0c\u540c\u65f6\u5bf9\u9644\u52a0\u6587\u6863\u5e94\u7528n-gram\u7f29\u5199\u548c\u6570\u503c\u5217\u7684\u7edf\u4e00\u91cf\u5316\u3002", "result": "\u5728TAT-QA\u548cFinQA\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u4ee4\u724c\u4f7f\u7528\u91cf\u548c\u63a8\u7406\u6210\u672c\u51cf\u5c11\u9ad8\u8fbe60%\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\uff08Claude-3.5-Sonnet\u548cGPT-4.1-Mini\u7684\u51c6\u786e\u7387\u4e0b\u964d\u5c0f\u4e8e5%\uff09\u3002", "conclusion": "CompactPrompt\u4e3a\u66f4\u7cbe\u7b80\u7684\u751f\u6210AI\u7ba1\u9053\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u80fd\u591f\u53ef\u89c6\u5316\u5b9e\u65f6\u538b\u7f29\u51b3\u7b56\u5e76\u91cf\u5316\u6210\u672c-\u6027\u80fd\u6743\u8861\u3002", "topic": "agent analysis"}}
{"id": "2510.18013", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18013", "abs": "https://arxiv.org/abs/2510.18013", "authors": ["Yiran Wang", "Jos\u00e9 Antonio Hern\u00e1ndez L\u00f3pez", "Ulf Nilsson", "D\u00e1niel Varr\u00f3"], "title": "JunoBench: A Benchmark Dataset of Crashes in Python Machine Learning Jupyter Notebooks", "comment": null, "summary": "Jupyter notebooks are widely used for machine learning (ML) prototyping. Yet\nfew debugging tools are designed for ML code in notebooks, potentially due to\nthe lack of benchmarks. We introduce JunoBench, the first benchmark dataset of\nreal-world crashes in Python-based ML notebooks. JunoBench has 111 curated and\nreproducible crashes from public Kaggle notebooks, each paired with a\nverifiable fix, ranging over popular ML libraries, including TensorFlow/Keras,\nPyTorch, Scikit-learn, Pandas, and NumPy, as well as notebook-specific\nout-of-order execution issue. To support reproducibility and ease of use,\nJunoBench offers a unified execution environment where crashes and fixes can be\nreliably reproduced. By providing realistic crashes and their resolutions,\nJunoBench facilitates bug detection, localization, and repair tailored to the\ninteractive and iterative nature of notebook-based ML development.", "AI": {"tldr": "JunoBench\u662f\u9996\u4e2a\u9488\u5bf9Python\u673a\u5668\u5b66\u4e60\u7b14\u8bb0\u672c\u7684\u771f\u5b9e\u5d29\u6e83\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b111\u4e2a\u6765\u81eaKaggle\u7b14\u8bb0\u672c\u7684\u53ef\u91cd\u73b0\u5d29\u6e83\u6848\u4f8b\u53ca\u5176\u9a8c\u8bc1\u4fee\u590d\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u7b14\u8bb0\u672c\u7684\u8c03\u8bd5\u5de5\u5177\u57fa\u51c6\uff0c\u4f5c\u8005\u521b\u5efa\u4e86JunoBench\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4ece\u516c\u5f00Kaggle\u7b14\u8bb0\u672c\u4e2d\u6536\u96c6\u5e76\u7b5b\u9009111\u4e2a\u771f\u5b9e\u5d29\u6e83\u6848\u4f8b\uff0c\u4e3a\u6bcf\u4e2a\u6848\u4f8b\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u4fee\u590d\u65b9\u6848\uff0c\u5e76\u6784\u5efa\u7edf\u4e00\u7684\u6267\u884c\u73af\u5883\u786e\u4fdd\u53ef\u91cd\u73b0\u6027\u3002", "result": "\u521b\u5efa\u4e86\u8986\u76d6TensorFlow/Keras\u3001PyTorch\u3001Scikit-learn\u3001Pandas\u3001NumPy\u7b49\u4e3b\u6d41ML\u5e93\u4ee5\u53ca\u7b14\u8bb0\u672c\u7279\u5b9a\u6267\u884c\u987a\u5e8f\u95ee\u9898\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "JunoBench\u4e3a\u7b14\u8bb0\u672c\u5f0f\u673a\u5668\u5b66\u4e60\u5f00\u53d1\u4e2d\u7684\u9519\u8bef\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u4fee\u590d\u63d0\u4f9b\u4e86\u73b0\u5b9e\u57fa\u51c6\u652f\u6301\u3002", "topic": "swe benchmark"}}
{"id": "2510.17921", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17921", "abs": "https://arxiv.org/abs/2510.17921", "authors": ["Keuntae Kim", "Eunhye Jeong", "Sehyeon Lee", "Seohee Yoon", "Yong Suk Choi"], "title": "CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections", "comment": "NeurIPS 2025", "summary": "Recent advances in enhancing the reasoning ability of large language models\n(LLMs) have been remarkably successful. LLMs trained with reinforcement\nlearning (RL) for reasoning demonstrate strong performance in challenging tasks\nsuch as mathematics and coding, even with relatively small model sizes.\nHowever, despite these improvements in task accuracy, the assessment of\ncreativity in LLM generations has been largely overlooked in reasoning tasks,\nin contrast to writing tasks. The lack of research on creativity assessment in\nreasoning primarily stems from two challenges: (1) the difficulty of defining\nthe range of creativity, and (2) the necessity of human evaluation in the\nassessment process. To address these challenges, we propose CLAWS, a method\nthat defines and classifies mathematical solutions into typical, creative, and\nhallucinated categories without human evaluation, by leveraging attention\nweights across prompt sections and output. CLAWS outperforms five existing\nwhite-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden\nScore, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen,\nMathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems\ncollected from 181 math contests (AJHSME, AMC, AIME).", "AI": {"tldr": "CLAWS\u662f\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u8bc4\u4f30\u5c31\u80fd\u5c06\u6570\u5b66\u89e3\u51b3\u65b9\u6848\u5206\u7c7b\u4e3a\u5178\u578b\u3001\u521b\u610f\u548c\u5e7b\u89c9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6ce8\u610f\u529b\u6743\u91cd\u5728\u63d0\u793a\u90e8\u5206\u548c\u8f93\u51fa\u4e2d\u7684\u5206\u5e03\u6765\u5b9e\u73b0\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u6709\u6240\u63d0\u9ad8\uff0c\u4f46\u5bf9\u63a8\u7406\u4efb\u52a1\u4e2d\u521b\u9020\u6027\u7684\u8bc4\u4f30\u5374\u88ab\u5ffd\u89c6\uff0c\u4e3b\u8981\u56e0\u4e3a\u5b9a\u4e49\u521b\u9020\u6027\u8303\u56f4\u548c\u9700\u8981\u4eba\u5de5\u8bc4\u4f30\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faCLAWS\u65b9\u6cd5\uff0c\u5229\u7528\u6ce8\u610f\u529b\u6743\u91cd\u5728\u63d0\u793a\u90e8\u5206\u548c\u8f93\u51fa\u4e2d\u7684\u5206\u5e03\uff0c\u5c06\u6570\u5b66\u89e3\u51b3\u65b9\u6848\u5206\u7c7b\u4e3a\u5178\u578b\u3001\u521b\u610f\u548c\u5e7b\u89c9\u4e09\u7c7b\u3002", "result": "CLAWS\u5728\u4e94\u4e2a7-8B\u6570\u5b66RL\u6a21\u578b\uff08DeepSeek\u3001Qwen\u3001Mathstral\u3001OpenMath2\u548cOreal\uff09\u4e0a\u4f18\u4e8e\u4e94\u79cd\u73b0\u6709\u7684\u767d\u76d2\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u57284545\u4e2a\u6570\u5b66\u95ee\u9898\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "CLAWS\u80fd\u591f\u6709\u6548\u5b9a\u4e49\u548c\u5206\u7c7b\u6570\u5b66\u89e3\u51b3\u65b9\u6848\u7684\u521b\u9020\u6027\uff0c\u65e0\u9700\u4eba\u5de5\u8bc4\u4f30\uff0c\u4e3a\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u521b\u9020\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.17922", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17922", "abs": "https://arxiv.org/abs/2510.17922", "authors": ["Shuodi Liu", "Yingzhuo Liu", "Zi Wang", "Yusheng Wang", "Huijia Wu", "Liuyu Xiang", "Zhaofeng He"], "title": "Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models", "comment": "Accepted to the Main Conference of EMNLP 2025 (Oral)", "summary": "Large language models (LLMs) have demonstrated remarkable reasoning and\nplanning capabilities, driving extensive research into task decomposition.\nExisting task decomposition methods focus primarily on memory, tool usage, and\nfeedback mechanisms, achieving notable success in specific domains, but they\noften overlook the trade-off between performance and cost. In this study, we\nfirst conduct a comprehensive investigation on task decomposition, identifying\nsix categorization schemes. Then, we perform an empirical analysis of three\nfactors that influence the performance and cost of task decomposition:\ncategories of approaches, characteristics of tasks, and configuration of\ndecomposition and execution models, uncovering three critical insights and\nsummarizing a set of practical principles. Building on this analysis, we\npropose the Select-Then-Decompose strategy, which establishes a closed-loop\nproblem-solving process composed of three stages: selection, execution, and\nverification. This strategy dynamically selects the most suitable decomposition\napproach based on task characteristics and enhances the reliability of the\nresults through a verification module. Comprehensive evaluations across\nmultiple benchmarks show that the Select-Then-Decompose consistently lies on\nthe Pareto frontier, demonstrating an optimal balance between performance and\ncost. Our code is publicly available at\nhttps://github.com/summervvind/Select-Then-Decompose.", "AI": {"tldr": "\u63d0\u51faSelect-Then-Decompose\u7b56\u7565\uff0c\u901a\u8fc7\u9009\u62e9\u3001\u6267\u884c\u548c\u9a8c\u8bc1\u4e09\u9636\u6bb5\u95ed\u73af\u6d41\u7a0b\uff0c\u52a8\u6001\u9009\u62e9\u6700\u9002\u5408\u7684\u4efb\u52a1\u5206\u89e3\u65b9\u6cd5\uff0c\u5728\u6027\u80fd\u548c\u6210\u672c\u4e4b\u95f4\u5b9e\u73b0\u6700\u4f18\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u5206\u89e3\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5185\u5b58\u3001\u5de5\u5177\u4f7f\u7528\u548c\u53cd\u9988\u673a\u5236\uff0c\u4f46\u5ffd\u89c6\u4e86\u6027\u80fd\u4e0e\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5e73\u8861\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u5bf9\u4efb\u52a1\u5206\u89e3\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\uff0c\u8bc6\u522b\u516d\u79cd\u5206\u7c7b\u65b9\u6848\uff1b\u7136\u540e\u5206\u6790\u5f71\u54cd\u5206\u89e3\u6027\u80fd\u7684\u4e09\u4e2a\u56e0\u7d20\uff1b\u6700\u540e\u63d0\u51faSelect-Then-Decompose\u7b56\u7565\uff0c\u5305\u542b\u9009\u62e9\u3001\u6267\u884c\u548c\u9a8c\u8bc1\u4e09\u4e2a\u9636\u6bb5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cSelect-Then-Decompose\u7b56\u7565\u59cb\u7ec8\u4f4d\u4e8ePareto\u524d\u6cbf\uff0c\u5728\u6027\u80fd\u548c\u6210\u672c\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u4f18\u5e73\u8861\u3002", "conclusion": "Select-Then-Decompose\u7b56\u7565\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u5206\u89e3\u65b9\u6cd5\u548c\u7ed3\u679c\u9a8c\u8bc1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4efb\u52a1\u5206\u89e3\u4e2d\u6027\u80fd\u4e0e\u6210\u672c\u7684\u6743\u8861\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2510.18131", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18131", "abs": "https://arxiv.org/abs/2510.18131", "authors": ["Chengquan Guo", "Yuzhou Nie", "Chulin Xie", "Zinan Lin", "Wenbo Guo", "Bo Li"], "title": "BlueCodeAgent: A Blue Teaming Agent Enabled by Automated Red Teaming for CodeGen AI", "comment": null, "summary": "As large language models (LLMs) are increasingly used for code generation,\nconcerns over the security risks have grown substantially. Early research has\nprimarily focused on red teaming, which aims to uncover and evaluate\nvulnerabilities and risks of CodeGen models. However, progress on the blue\nteaming side remains limited, as developing defense requires effective semantic\nunderstanding to differentiate the unsafe from the safe. To fill in this gap,\nwe propose BlueCodeAgent, an end-to-end blue teaming agent enabled by automated\nred teaming. Our framework integrates both sides: red teaming generates diverse\nrisky instances, while the blue teaming agent leverages these to detect\npreviously seen and unseen risk scenarios through constitution and code\nanalysis with agentic integration for multi-level defense. Our evaluation\nacross three representative code-related tasks--bias instruction detection,\nmalicious instruction detection, and vulnerable code detection--shows that\nBlueCodeAgent achieves significant gains over the base models and safety\nprompt-based defenses. In particular, for vulnerable code detection tasks,\nBlueCodeAgent integrates dynamic analysis to effectively reduce false\npositives, a challenging problem as base models tend to be over-conservative,\nmisclassifying safe code as unsafe. Overall, BlueCodeAgent achieves an average\n12.7\\% F1 score improvement across four datasets in three tasks, attributed to\nits ability to summarize actionable constitutions that enhance context-aware\nrisk detection. We demonstrate that the red teaming benefits the blue teaming\nby continuously identifying new vulnerabilities to enhance defense performance.", "AI": {"tldr": "BlueCodeAgent\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u84dd\u961f\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7ea2\u961f\u751f\u6210\u591a\u6837\u5316\u98ce\u9669\u5b9e\u4f8b\uff0c\u7ed3\u5408\u5baa\u6cd5\u548c\u4ee3\u7801\u5206\u6790\u8fdb\u884c\u591a\u7ea7\u9632\u5fa1\uff0c\u5728\u4ee3\u7801\u5b89\u5168\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e94\u7528\u589e\u591a\uff0c\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u7a81\u51fa\u3002\u76ee\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u7ea2\u961f\u6d4b\u8bd5\uff0c\u800c\u84dd\u961f\u9632\u5fa1\u65b9\u9762\u8fdb\u5c55\u6709\u9650\uff0c\u9700\u8981\u6709\u6548\u7684\u8bed\u4e49\u7406\u89e3\u6765\u533a\u5206\u5b89\u5168\u4e0e\u4e0d\u5b89\u5168\u4ee3\u7801\u3002", "method": "\u63d0\u51faBlueCodeAgent\u6846\u67b6\uff0c\u96c6\u6210\u7ea2\u961f\u548c\u84dd\u961f\uff1a\u7ea2\u961f\u751f\u6210\u591a\u6837\u5316\u98ce\u9669\u5b9e\u4f8b\uff0c\u84dd\u961f\u4ee3\u7406\u5229\u7528\u8fd9\u4e9b\u5b9e\u4f8b\u901a\u8fc7\u5baa\u6cd5\u548c\u4ee3\u7801\u5206\u6790\u68c0\u6d4b\u5df2\u77e5\u548c\u672a\u77e5\u98ce\u9669\u573a\u666f\uff0c\u91c7\u7528\u52a8\u6001\u5206\u6790\u51cf\u5c11\u8bef\u62a5\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\uff08\u504f\u89c1\u6307\u4ee4\u68c0\u6d4b\u3001\u6076\u610f\u6307\u4ee4\u68c0\u6d4b\u3001\u6f0f\u6d1e\u4ee3\u7801\u68c0\u6d4b\uff09\u4e2d\uff0cBlueCodeAgent\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8e\u5b89\u5168\u63d0\u793a\u7684\u9632\u5fa1\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u7684\u4e09\u4e2a\u4efb\u52a1\u4e2d\u5e73\u5747F1\u5206\u6570\u63d0\u9ad8\u4e8612.7%\u3002", "conclusion": "\u7ea2\u961f\u6d4b\u8bd5\u901a\u8fc7\u6301\u7eed\u8bc6\u522b\u65b0\u6f0f\u6d1e\u6765\u589e\u5f3a\u84dd\u961f\u9632\u5fa1\u6027\u80fd\uff0cBlueCodeAgent\u80fd\u591f\u603b\u7ed3\u53ef\u64cd\u4f5c\u7684\u5baa\u6cd5\u6765\u589e\u5f3a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u98ce\u9669\u68c0\u6d4b\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2510.18143", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18143", "abs": "https://arxiv.org/abs/2510.18143", "authors": ["Huan Song", "Deeksha Razdan", "Yiyue Qian", "Arijit Ghosh Chowdhury", "Parth Patwa", "Aman Chadha", "Shinan Zhang", "Sharlina Keshava", "Hannah Marlowe"], "title": "Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models", "comment": "Neural Information Processing Systems (NeurIPS 2025) Workshop:\n  Evaluating the Evolving LLM Lifecycle", "summary": "Small Language Models (SLMs) offer compelling advantages in deployment cost\nand latency, but their accuracy often lags behind larger models, particularly\nfor complex domain-specific tasks. While supervised fine-tuning can help bridge\nthis performance gap, it requires substantial manual effort in data preparation\nand iterative optimization. We present PaDA-Agent (Pattern-guided Data\nAugmentation Agent), an evaluation-driven approach that streamlines the data\naugmentation process for SLMs through coordinated operations. Unlike\nstate-of-the-art approaches that focus on model training errors only and\ngenerating error-correcting samples, PaDA-Agent discovers failure patterns from\nthe validation data via evaluations and drafts targeted data augmentation\nstrategies aiming to directly reduce the generalization gap. Our experimental\nresults demonstrate significant improvements over state-of-the-art LLM-based\ndata augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.", "AI": {"tldr": "PaDA-Agent\u662f\u4e00\u79cd\u8bc4\u4f30\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u8c03\u64cd\u4f5c\u7b80\u5316\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u589e\u5f3a\u8fc7\u7a0b\uff0c\u53d1\u73b0\u9a8c\u8bc1\u6570\u636e\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\u5e76\u5236\u5b9a\u9488\u5bf9\u6027\u7b56\u7565\u6765\u51cf\u5c11\u6cdb\u5316\u5dee\u8ddd\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u6210\u672c\u548c\u5ef6\u8fdf\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u590d\u6742\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\u5f80\u5f80\u843d\u540e\u4e8e\u5927\u578b\u6a21\u578b\u3002\u76d1\u7763\u5fae\u8c03\u9700\u8981\u5927\u91cf\u624b\u52a8\u6570\u636e\u51c6\u5907\u548c\u8fed\u4ee3\u4f18\u5316\u5de5\u4f5c\u3002", "method": "PaDA-Agent\u901a\u8fc7\u8bc4\u4f30\u53d1\u73b0\u9a8c\u8bc1\u6570\u636e\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5236\u5b9a\u9488\u5bf9\u6027\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u76f4\u63a5\u51cf\u5c11\u6cdb\u5316\u5dee\u8ddd\uff0c\u800c\u4e0d\u662f\u4ec5\u5173\u6ce8\u6a21\u578b\u8bad\u7ec3\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728Llama 3.2 1B Instruct\u6a21\u578b\u5fae\u8c03\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8eLLM\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "PaDA-Agent\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8bc4\u4f30\u9a71\u52a8\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.18270", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18270", "abs": "https://arxiv.org/abs/2510.18270", "authors": ["Yang Chen", "Toufique Ahmed", "Reyhaneh Jabbarvand", "Martin Hirzel"], "title": "When Old Meets New: Evaluating the Impact of Regression Tests on SWE Issue Resolution", "comment": null, "summary": "Test suites in real-world projects are often large and achieve high code\ncoverage, yet they remain insufficient for detecting all bugs. The abundance of\nunresolved issues in open-source project trackers highlights this gap. While\nregression tests are typically designed to ensure past functionality is\npreserved in the new version, they can also serve a complementary purpose:\ndebugging the current version. Specifically, regression tests can (1) enhance\nthe generation of reproduction tests for newly reported issues, and (2)\nvalidate that patches do not regress existing functionality. We present\nTestPrune, a fully automated technique that leverages issue tracker reports and\nstrategically reuses regression tests for both bug reproduction and patch\nvalidation.\n  A key contribution of TestPrune is its ability to automatically minimize the\nregression suite to a small, highly relevant subset of tests. Due to the\npredominance of LLM-based debugging techniques, this minimization is essential\nas large test suites exceed context limits, introduce noise, and inflate\ninference costs. TestPrune can be plugged into any agentic bug repair pipeline\nand orthogonally improve overall performance. As a proof of concept, we show\nthat TestPrune leads to a 6.2%-9.0% relative increase in issue reproduction\nrate within the Otter framework and a 9.4% - 12.9% relative increase in issue\nresolution rate within the Agentless framework on SWE-Bench Lite and SWE-Bench\nVerified benchmarks, capturing fixes that were correctly produced by agents but\nnot submitted as final patches. Compared to the benefits, the cost overhead of\nusing TestPrune is minimal, i.e., \\$0.02 and \\$0.05 per SWE-Bench instance,\nusing GPT-4o and Claude-3.7-Sonnet models, respectively.", "AI": {"tldr": "TestPrune\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6280\u672f\uff0c\u5229\u7528\u56de\u5f52\u6d4b\u8bd5\u6765\u589e\u5f3abug\u590d\u73b0\u548c\u8865\u4e01\u9a8c\u8bc1\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6d4b\u8bd5\u5957\u4ef6\u6765\u89e3\u51b3LLM\u8c03\u8bd5\u6280\u672f\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u9879\u76ee\u4e2d\u7684\u6d4b\u8bd5\u5957\u4ef6\u867d\u7136\u8986\u76d6\u7387\u9ad8\uff0c\u4f46\u4ecd\u65e0\u6cd5\u68c0\u6d4b\u6240\u6709bug\u3002\u56de\u5f52\u6d4b\u8bd5\u9664\u4e86\u786e\u4fdd\u529f\u80fd\u4fdd\u7559\u5916\uff0c\u8fd8\u53ef\u7528\u4e8e\u8c03\u8bd5\u5f53\u524d\u7248\u672c\uff0c\u7279\u522b\u662f\u5e2e\u52a9\u590d\u73b0\u65b0\u62a5\u544a\u7684\u95ee\u9898\u548c\u9a8c\u8bc1\u8865\u4e01\u3002", "method": "TestPrune\u81ea\u52a8\u6700\u5c0f\u5316\u56de\u5f52\u6d4b\u8bd5\u5957\u4ef6\uff0c\u63d0\u53d6\u9ad8\u5ea6\u76f8\u5173\u7684\u6d4b\u8bd5\u5b50\u96c6\uff0c\u53ef\u96c6\u6210\u5230\u4efb\u4f55\u57fa\u4e8e\u4ee3\u7406\u7684bug\u4fee\u590d\u6d41\u7a0b\u4e2d\u3002", "result": "\u5728Otter\u6846\u67b6\u4e2d\uff0cTestPrune\u4f7f\u95ee\u9898\u590d\u73b0\u7387\u76f8\u5bf9\u63d0\u9ad86.2%-9.0%\uff1b\u5728Agentless\u6846\u67b6\u4e2d\uff0c\u95ee\u9898\u89e3\u51b3\u7387\u76f8\u5bf9\u63d0\u9ad89.4%-12.9%\u3002\u6210\u672c\u5f00\u9500\u6781\u5c0f\uff0c\u6bcf\u4e2aSWE-Bench\u5b9e\u4f8b\u4ec5\u9700$0.02-$0.05\u3002", "conclusion": "TestPrune\u80fd\u6709\u6548\u63d0\u5347bug\u4fee\u590d\u7ba1\u9053\u7684\u6027\u80fd\uff0c\u6210\u672c\u6548\u76ca\u9ad8\uff0c\u53ef\u6b63\u4ea4\u6539\u5584\u6574\u4f53\u8868\u73b0\u3002", "topic": "swe application"}}
{"id": "2510.18327", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18327", "abs": "https://arxiv.org/abs/2510.18327", "authors": ["Yunkun Wang", "Yue Zhang", "Guochang Li", "Chen Zhi", "Binhua Li", "Fei Huang", "Yongbin Li", "Shuiguang Deng"], "title": "InspectCoder: Dynamic Analysis-Enabled Self Repair through interactive LLM-Debugger Collaboration", "comment": null, "summary": "Large Language Models (LLMs) frequently generate buggy code with complex\nlogic errors that are challenging to diagnose. While existing LLM-based\nself-repair approaches conduct intensive static semantic analysis or reply on\nsuperficial execution logs, they miss the in-depth runtime behaviors that often\nexpose bug root causes-lacking the interactive dynamic analysis capabilities\nthat make human debugging effective. We present InspectCoder, the first agentic\nprogram repair system that empowers LLMs to actively conduct dynamic analysis\nvia interactive debugger control. Our dual-agent framework enables strategic\nbreakpoint placement, targeted state inspection, and incremental runtime\nexperimentation within stateful debugger sessions. Unlike existing methods that\nfollow fixed log collection procedures, InspectCoder adaptively inspects and\nperturbs relevant intermediate states at runtime, and leverages immediate\nprocess rewards from debugger feedback to guide multi-step reasoning,\ntransforming LLM debugging paradigm from blind trial-and-error into systematic\nroot cause diagnosis. We conduct comprehensive experiments on two challenging\nself-repair benchmarks: BigCodeBench-R and LiveCodeBench-R. InspectCoder\nachieves 5.10%-60.37% relative improvements in repair accuracy over the\nstrongest baseline, while delivering 1.67x-2.24x superior bug-fix efficiency\nrespectively. We also contribute InspectWare, an open-source middleware that\nabstracts debugger complexities and maintains stateful debugging sessions\nacross mainstream Python testing frameworks. Our work provides actionable\ninsight into the interactive LLM-debugger systems, demonstrating the\nsignificant potential of LLM-driven dynamic analysis for automated software\nengineering.", "AI": {"tldr": "InspectCoder\u662f\u9996\u4e2a\u57fa\u4e8e\u667a\u80fd\u4ee3\u7406\u7684\u7a0b\u5e8f\u4fee\u590d\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u8c03\u8bd5\u5668\u63a7\u5236\u8ba9LLM\u8fdb\u884c\u52a8\u6001\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u4fee\u590d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u81ea\u4fee\u590d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u8bed\u4e49\u5206\u6790\u6216\u6d45\u5c42\u6267\u884c\u65e5\u5fd7\uff0c\u7f3a\u4e4f\u6df1\u5165\u8fd0\u884c\u65f6\u884c\u4e3a\u5206\u6790\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u8bca\u65ad\u590d\u6742\u903b\u8f91\u9519\u8bef\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "\u91c7\u7528\u53cc\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6218\u7565\u65ad\u70b9\u653e\u7f6e\u3001\u76ee\u6807\u72b6\u6001\u68c0\u67e5\u548c\u589e\u91cf\u8fd0\u884c\u65f6\u5b9e\u9a8c\uff0c\u5728\u72b6\u6001\u5316\u8c03\u8bd5\u4f1a\u8bdd\u4e2d\u8fdb\u884c\u81ea\u9002\u5e94\u52a8\u6001\u5206\u6790\u3002", "result": "\u5728\u4e24\u4e2a\u6311\u6218\u6027\u81ea\u4fee\u590d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4fee\u590d\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u53475.10%-60.37%\uff0cbug\u4fee\u590d\u6548\u7387\u63d0\u53471.67x-2.24x\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u52a8\u6001\u5206\u6790\u5728\u81ea\u52a8\u5316\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4ea4\u4e92\u5f0fLLM-\u8c03\u8bd5\u5668\u7cfb\u7edf\u4e3a\u4ee3\u7801\u4fee\u590d\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "code agent"}}
{"id": "2510.18165", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18165", "abs": "https://arxiv.org/abs/2510.18165", "authors": ["Yihong Dong", "Zhaoyu Ma", "Xue Jiang", "Zhiyuan Fan", "Jiaru Qian", "Yongmin Li", "Jianha Xiao", "Zhi Jin", "Rongyu Cao", "Binhua Li", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model", "comment": null, "summary": "Diffusion language models (DLMs) are emerging as a powerful and promising\nalternative to the dominant autoregressive paradigm, offering inherent\nadvantages in parallel generation and bidirectional context modeling. However,\nthe performance of DLMs on code generation tasks, which have stronger\nstructural constraints, is significantly hampered by the critical trade-off\nbetween inference speed and output quality. We observed that accelerating the\ncode generation process by reducing the number of sampling steps usually leads\nto a catastrophic collapse in performance. In this paper, we introduce\nefficient Sampling with Adaptive acceleration and Backtracking Enhanced\nRemasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to\nachieve better inference speed and output quality in code generation.\nSpecifically, Saber is motivated by two key insights in the DLM generation\nprocess: 1) it can be adaptively accelerated as more of the code context is\nestablished; 2) it requires a backtracking mechanism to reverse the generated\ntokens. Extensive experiments on multiple mainstream code generation benchmarks\nshow that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over\nmainstream DLM sampling methods, meanwhile achieving an average 251.4%\ninference speedup. By leveraging the inherent advantages of DLMs, our work\nsignificantly narrows the performance gap with autoregressive models in code\ngeneration.", "AI": {"tldr": "\u63d0\u51fa\u4e86Saber\u7b97\u6cd5\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u91c7\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u901f\u5ea6\u548c\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u4e0e\u8f93\u51fa\u8d28\u91cf\u7684\u5173\u952e\u6743\u8861\u95ee\u9898\uff0c\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u4f1a\u5bfc\u81f4\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002", "method": "Saber\u7b97\u6cd5\u57fa\u4e8e\u4e24\u4e2a\u5173\u952e\u6d1e\u5bdf\uff1a1\uff09\u968f\u7740\u4ee3\u7801\u4e0a\u4e0b\u6587\u7684\u5efa\u7acb\u53ef\u4ee5\u81ea\u9002\u5e94\u52a0\u901f\uff1b2\uff09\u9700\u8981\u56de\u6eaf\u673a\u5236\u6765\u53cd\u8f6c\u751f\u6210\u7684\u6807\u8bb0\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSaber\u5c06Pass@1\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53471.9%\uff0c\u540c\u65f6\u5b9e\u73b0\u5e73\u5747251.4%\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u56fa\u6709\u4f18\u52bf\uff0c\u8be5\u5de5\u4f5c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "topic": "code agent"}}
{"id": "2510.18430", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18430", "abs": "https://arxiv.org/abs/2510.18430", "authors": ["Tasha Settewong", "Youmei Fan", "Raula Gaikovina Kula", "Kenichi Matsumoto"], "title": "Human to Document, AI to Code: Three Case Studies of Comparing GenAI for Notebook Competitions", "comment": null, "summary": "Computational notebooks have become the preferred tool of choice for data\nscientists and practitioners to perform analyses and share results. Notebooks\nuniquely combine scripts with documentation. With the emergence of generative\nAI (GenAI) technologies, it is increasingly important, especially in\ncompetitive settings, to distinguish the characteristics of human-written\nversus GenAI.\n  In this study, we present three case studies to explore potential strengths\nof both humans and GenAI through the coding and documenting activities in\nnotebooks. We first characterize differences between 25 code and documentation\nfeatures in human-written, medal-winning Kaggle notebooks. We find that gold\nmedalists are primarily distinguished by longer and more detailed\ndocumentation. Second, we analyze the distinctions between human-written and\nGenAI notebooks. Our results show that while GenAI notebooks tend to achieve\nhigher code quality (as measured by metrics like code smells and technical\ndebt), human-written notebooks display greater structural diversity,\ncomplexity, and innovative approaches to problem-solving. Based on these\nresults, we envision the work as groundwork that highlight four agendas to\nfurther investigate how GenAI could be utilized in notebooks that maximizes the\npotential collaboration between human and AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u5206\u6790\u4e86\u4eba\u7c7b\u4e0e\u751f\u6210\u5f0fAI\u5728\u8ba1\u7b97\u7b14\u8bb0\u672c\u4e2d\u7f16\u5199\u4ee3\u7801\u548c\u6587\u6863\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u91d1\u724c\u5f97\u4e3b\u4ee5\u66f4\u957f\u66f4\u8be6\u7ec6\u7684\u6587\u6863\u4e3a\u7279\u5f81\uff0c\u800cGenAI\u7b14\u8bb0\u672c\u4ee3\u7801\u8d28\u91cf\u66f4\u9ad8\u4f46\u4eba\u7c7b\u7b14\u8bb0\u672c\u5728\u7ed3\u6784\u591a\u6837\u6027\u548c\u521b\u65b0\u6027\u65b9\u9762\u66f4\u80dc\u4e00\u7b79\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5728\u7ade\u4e89\u73af\u5883\u4e2d\u533a\u5206\u4eba\u7c7b\u7f16\u5199\u4e0eAI\u751f\u6210\u7b14\u8bb0\u672c\u7684\u7279\u5f81\u53d8\u5f97\u6108\u53d1\u91cd\u8981\uff0c\u4ee5\u63a2\u7d22\u4eba\u7c7b\u4e0eAI\u5728\u7b14\u8bb0\u672c\u6d3b\u52a8\u4e2d\u7684\u5404\u81ea\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u679025\u4e2a\u4ee3\u7801\u548c\u6587\u6863\u7279\u5f81\u5728\u4eba\u7c7b\u7f16\u5199\u548cGenAI\u7b14\u8bb0\u672c\u4e2d\u7684\u5dee\u5f02\uff0c\u7279\u522b\u5173\u6ce8Kaggle\u7ade\u8d5b\u4e2d\u83b7\u5956\u7b14\u8bb0\u672c\u7684\u7279\u5f81\u3002", "result": "\u91d1\u724c\u5f97\u4e3b\u4e3b\u8981\u7279\u5f81\u662f\u66f4\u957f\u66f4\u8be6\u7ec6\u7684\u6587\u6863\uff1bGenAI\u7b14\u8bb0\u672c\u4ee3\u7801\u8d28\u91cf\u66f4\u9ad8\uff08\u4ee3\u7801\u5f02\u5473\u548c\u6280\u672f\u503a\u52a1\u6307\u6807\u66f4\u597d\uff09\uff0c\u4f46\u4eba\u7c7b\u7b14\u8bb0\u672c\u5728\u7ed3\u6784\u591a\u6837\u6027\u3001\u590d\u6742\u6027\u548c\u95ee\u9898\u89e3\u51b3\u521b\u65b0\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u63a2\u7d22\u5982\u4f55\u6700\u5927\u5316\u4eba\u7c7b\u4e0eAI\u5728\u7b14\u8bb0\u672c\u4e2d\u7684\u534f\u4f5c\u6f5c\u529b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u56db\u4e2a\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u8bae\u7a0b\u3002", "topic": "agent analysis"}}
{"id": "2510.17923", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17923", "abs": "https://arxiv.org/abs/2510.17923", "authors": ["Chenwei Tang", "Jingyu Xing", "Xinyu Liu", "Wei Ju", "Jiancheng Lv", "Deng Xiong", "Ziyue Qiao"], "title": "Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing\nLarge Language Models (LLMs), achieving remarkable performance in complex\nreasoning domains such as mathematics and code generation. However, current RL\nmethods face a fundamental scalability bottleneck due to their heavy reliance\non human-curated preference data or labeled datasets for reward modeling. To\novercome this limitation, we explore RL on unlabeled data where models learn\nautonomously from continuous experience streams. The core challenge in this\nsetting lies in reliable reward estimation without ground-truth supervision.\nExisting approaches like Test-Time RL address this through self-consistent\nconsensus, but risk reinforcing incorrect pseudo-labels derived from majority\nvoting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel\ntest-time reward mechanism that operates without external supervision. COMPASS\nintegrates two complementary components: the Dual-Calibration Answer Reward\n(DCAR), which stabilizes training by establishing trustworthy pseudo-labels\nthrough confidence and credibility calibration, and the Decisive Path Reward\n(DPR), which directly optimizes the reasoning process quality beyond mere\noutcome supervision. By jointly reinforcing trustworthy consensus answers and\nhighly decisive reasoning chains, the COMPASS systematically enhances the\nmodel's analytical capabilities. Extensive experiments show that COMPASS\nachieves significant and consistent performance gains across diverse reasoning\ntasks and model architectures, advancing a more scalable direction for LLMs to\nlearn from continuous experience.", "AI": {"tldr": "COMPASS\u662f\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u6d4b\u8bd5\u65f6\u5956\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u6574\u5408\u53cc\u6821\u51c6\u7b54\u6848\u5956\u52b1\u548c\u51b3\u5b9a\u6027\u8def\u5f84\u5956\u52b1\uff0c\u5728\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u5b9e\u73b0\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u504f\u597d\u6570\u636e\u6216\u6807\u7b7e\u6570\u636e\u96c6\u8fdb\u884c\u5956\u52b1\u5efa\u6a21\uff0c\u5b58\u5728\u53ef\u6269\u5c55\u6027\u74f6\u9888\u3002\u672c\u6587\u63a2\u7d22\u5728\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u8ba9\u6a21\u578b\u4ece\u8fde\u7eed\u7ecf\u9a8c\u6d41\u4e2d\u81ea\u4e3b\u5b66\u4e60\u3002", "method": "\u63d0\u51faCOMPASS\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a\u53cc\u6821\u51c6\u7b54\u6848\u5956\u52b1\uff08DCAR\uff09\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u548c\u53ef\u4fe1\u5ea6\u6821\u51c6\u5efa\u7acb\u53ef\u4fe1\u4f2a\u6807\u7b7e\u6765\u7a33\u5b9a\u8bad\u7ec3\uff1b\u51b3\u5b9a\u6027\u8def\u5f84\u5956\u52b1\uff08DPR\uff09\u76f4\u63a5\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u8d28\u91cf\uff0c\u8d85\u8d8a\u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u76d1\u7763\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCOMPASS\u5728\u4e0d\u540c\u63a8\u7406\u4efb\u52a1\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u63a8\u52a8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u8fde\u7eed\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u7684\u66f4\u53ef\u6269\u5c55\u65b9\u5411\u3002", "conclusion": "COMPASS\u901a\u8fc7\u8054\u5408\u5f3a\u5316\u53ef\u4fe1\u5171\u8bc6\u7b54\u6848\u548c\u9ad8\u5ea6\u51b3\u5b9a\u6027\u63a8\u7406\u94fe\uff0c\u7cfb\u7edf\u6027\u5730\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5206\u6790\u80fd\u529b\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18170", "categories": ["cs.AI", "cs.ET", "cs.LG", "cs.SE", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.18170", "abs": "https://arxiv.org/abs/2510.18170", "authors": ["Manik Rana", "Calissa Man", "Anotida Expected Msiiwa", "Jeffrey Paine", "Kevin Zhu", "Sunishchal Dev", "Vasu Sharma", "Ahan M R"], "title": "AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI", "comment": "Accepted to 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Workshop: Multi-Turn Interactions in Large Language Models", "summary": "Goal changes are a defining feature of real world multi-turn interactions,\nyet current agent benchmarks primarily evaluate static objectives or one-shot\ntool use. We introduce AgentChangeBench, a benchmark explicitly designed to\nmeasure how tool augmented language model agents adapt to mid dialogue goal\nshifts across three enterprise domains. Our framework formalizes evaluation\nthrough four complementary metrics: Task Success Rate (TSR) for effectiveness,\nTool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for\nwasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency.\nAgentChangeBench comprises 2,835 task sequences and five user personas, each\ndesigned to trigger realistic shift points in ongoing workflows. Using this\nsetup, we evaluate several frontier models and uncover sharp contrasts obscured\nby traditional $\\text{pass}@k$ scores: for example, GPT-4o reaches $92.2\\%$\nrecovery on airline booking shifts while Gemini collapses to $48.6\\%$, and\nretail tasks show near perfect parameter validity yet redundancy rates above\n$80\\%$, revealing major inefficiencies. These findings demonstrate that high\nraw accuracy does not imply robustness under dynamic goals, and that explicit\nmeasurement of recovery time and redundancy is essential. AgentChangeBench\nestablishes a reproducible testbed for diagnosing and improving agent\nresilience in realistic enterprise settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgentChangeBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u8bc4\u4f30\u5de5\u5177\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5e94\u5bf9\u76ee\u6807\u53d8\u5316\u7684\u80fd\u529b\uff0c\u5305\u542b2,835\u4e2a\u4efb\u52a1\u5e8f\u5217\u548c\u56db\u79cd\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u8f6e\u4ea4\u4e92\u7ecf\u5e38\u51fa\u73b0\u76ee\u6807\u53d8\u5316\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u9759\u6001\u76ee\u6807\u6216\u4e00\u6b21\u6027\u5de5\u5177\u4f7f\u7528\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u6001\u76ee\u6807\u9002\u5e94\u80fd\u529b\u7684\u6d4b\u8bd5\u3002", "method": "\u6784\u5efa\u5305\u542b2,835\u4e2a\u4efb\u52a1\u5e8f\u5217\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e09\u4e2a\u4f01\u4e1a\u9886\u57df\uff0c\u4f7f\u7528\u56db\u79cd\u4e92\u8865\u6307\u6807\uff1a\u4efb\u52a1\u6210\u529f\u7387\u3001\u5de5\u5177\u4f7f\u7528\u6548\u7387\u3001\u5de5\u5177\u8c03\u7528\u5197\u4f59\u7387\u548c\u76ee\u6807\u8f6c\u79fb\u6062\u590d\u65f6\u95f4\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u524d\u6cbf\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff1aGPT-4o\u5728\u822a\u73ed\u9884\u8ba2\u4efb\u52a1\u4e2d\u8fbe\u523092.2%\u6062\u590d\u7387\uff0c\u800cGemini\u4ec5\u4e3a48.6%\uff1b\u96f6\u552e\u4efb\u52a1\u53c2\u6570\u6709\u6548\u6027\u63a5\u8fd1\u5b8c\u7f8e\u4f46\u5197\u4f59\u7387\u8d85\u8fc780%\u3002", "conclusion": "\u9ad8\u539f\u59cb\u51c6\u786e\u7387\u5e76\u4e0d\u4ee3\u8868\u5728\u52a8\u6001\u76ee\u6807\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5fc5\u987b\u660e\u786e\u6d4b\u91cf\u6062\u590d\u65f6\u95f4\u548c\u5197\u4f59\u7387\u3002\u8be5\u57fa\u51c6\u4e3a\u8bca\u65ad\u548c\u6539\u8fdb\u4f01\u4e1a\u73af\u5883\u4e2d\u4ee3\u7406\u7684\u97e7\u6027\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "topic": "agent analysis"}}
{"id": "2510.18176", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18176", "abs": "https://arxiv.org/abs/2510.18176", "authors": ["Soumya Rani Samineni", "Durgesh Kalwar", "Vardaan Gangal", "Siddhant Bhambri", "Subbarao Kambhampati"], "title": "Local Coherence or Global Validity? Investigating RLVR Traces in Math Domains", "comment": "4 pages, 2 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR)-based post-training of\nLarge Language Models (LLMs) has been shown to improve accuracy on reasoning\ntasks and continues to attract significant attention. Existing RLVR methods,\nhowever, typically treat all tokens uniformly without accounting for\ntoken-level advantages. These methods primarily evaluate performance based on\nfinal answer correctness or Pass@K accuracy, and yet make claims about RL\npost-training leading to improved reasoning traces. This motivates our\ninvestigation into the effect of RL post-training on intermediate tokens which\nare not directly incentivized. To study this, we design an experimental setup\nusing the GRPO algorithm with Qwen-2.5-0.5B model on the GSM8K dataset. We\nintroduce trace coherence, a First-Order Logic (FOL)-based measure to capture\nthe consistency of reasoning steps by identifying errors in the traces. We\ndistinguish between trace validity and trace coherence, noting that the former\nimplies logical soundness while the latter measures local coherence via lack of\nerrors. Our results show that RL post-training overall improves trace coherence\nwith the most significant gains on problems where the base model fails but the\nRL model succeeds. Surprisingly, RL enhances local coherence without\nnecessarily producing valid or correct solutions. This highlights a crucial\ndistinction: improved local coherence in reasoning steps does not guarantee\nfinal answer correctness. We argue that claims of improved reasoning via RL\nmust be examined with care, as these may be based on improved trace coherence,\nwhich may not translate into fully valid mathematical proofs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86RL\u540e\u8bad\u7ec3\u5bf9LLM\u63a8\u7406\u8fc7\u7a0b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u867d\u7136RL\u63d0\u9ad8\u4e86\u63a8\u7406\u8f68\u8ff9\u7684\u5c40\u90e8\u8fde\u8d2f\u6027\uff0c\u4f46\u8fd9\u5e76\u4e0d\u4fdd\u8bc1\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\u6216\u903b\u8f91\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u901a\u5e38\u7edf\u4e00\u5904\u7406\u6240\u6709token\u800c\u4e0d\u8003\u8651token\u7ea7\u4f18\u52bf\uff0c\u4e3b\u8981\u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u8bc4\u4f30\u6027\u80fd\uff0c\u5374\u58f0\u79f0RL\u540e\u8bad\u7ec3\u6539\u5584\u4e86\u63a8\u7406\u8f68\u8ff9\u3002\u8fd9\u4fc3\u4f7f\u6211\u4eec\u7814\u7a76RL\u540e\u8bad\u7ec3\u5bf9\u672a\u76f4\u63a5\u6fc0\u52b1\u7684\u4e2d\u95f4token\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528GRPO\u7b97\u6cd5\u548cQwen-2.5-0.5B\u6a21\u578b\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5f15\u5165\u57fa\u4e8e\u4e00\u9636\u903b\u8f91\u7684\u8f68\u8ff9\u8fde\u8d2f\u6027\u5ea6\u91cf\u6765\u6355\u6349\u63a8\u7406\u6b65\u9aa4\u7684\u4e00\u81f4\u6027\u3002", "result": "RL\u540e\u8bad\u7ec3\u603b\u4f53\u4e0a\u63d0\u9ad8\u4e86\u8f68\u8ff9\u8fde\u8d2f\u6027\uff0c\u5728\u57fa\u7840\u6a21\u578b\u5931\u8d25\u4f46RL\u6a21\u578b\u6210\u529f\u7684\u95ee\u9898\u4e0a\u6539\u5584\u6700\u663e\u8457\u3002\u4f46RL\u589e\u5f3a\u4e86\u5c40\u90e8\u8fde\u8d2f\u6027\u800c\u4e0d\u4e00\u5b9a\u4ea7\u751f\u6709\u6548\u6216\u6b63\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6539\u8fdb\u7684\u63a8\u7406\u6b65\u9aa4\u5c40\u90e8\u8fde\u8d2f\u6027\u4e0d\u80fd\u4fdd\u8bc1\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\uff0c\u58f0\u79f0RL\u6539\u5584\u63a8\u7406\u7684\u4e3b\u5f20\u9700\u8981\u8c28\u614e\u5ba1\u89c6\uff0c\u56e0\u4e3a\u8fd9\u79cd\u6539\u5584\u53ef\u80fd\u57fa\u4e8e\u8f68\u8ff9\u8fde\u8d2f\u6027\u7684\u63d0\u5347\uff0c\u800c\u4e0d\u4e00\u5b9a\u8f6c\u5316\u4e3a\u5b8c\u5168\u6709\u6548\u7684\u6570\u5b66\u8bc1\u660e\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18471", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18471", "abs": "https://arxiv.org/abs/2510.18471", "authors": ["Xue Jiang", "Yihong Dong", "Mengyang Liu", "Hongyi Deng", "Tian Wang", "Yongding Tao", "Rongyu Cao", "Binhua Li", "Zhi Jin", "Wenpin Jiao", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment", "comment": null, "summary": "While Large Language Models (LLMs) excel at code generation by learning from\nvast code corpora, a fundamental semantic gap remains between their training on\ntextual patterns and the goal of functional correctness, which is governed by\nformal execution semantics. Reinforcement Learning with Verifiable Rewards\n(RLVR) approaches attempt to bridge this gap using outcome rewards from\nexecuting test cases. However, solely relying on binary pass/fail signals is\ninefficient for establishing a well-aligned connection between the textual\nrepresentation of code and its execution semantics, especially for subtle\nlogical errors within the code. In this paper, we propose CodeRL+, a novel\napproach that integrates execution semantics alignment into the RLVR training\npipeline for code generation. CodeRL+ enables the model to infer variable-level\nexecution trajectory, providing a direct learning signal of execution\nsemantics. CodeRL+ can construct execution semantics alignment directly using\nexisting on-policy rollouts and integrates seamlessly with various RL\nalgorithms. Extensive experiments demonstrate that CodeRL+ outperforms\npost-training baselines (including RLVR and Distillation), achieving a 4.6%\naverage relative improvement in pass@1. CodeRL+ generalizes effectively to\nother coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning\nand test-output-generation benchmarks, respectively. CodeRL+ shows strong\napplicability across diverse RL algorithms and LLMs. Furthermore, probe\nanalyses provide compelling evidence that CodeRL+ strengthens the alignment\nbetween code's textual representations and its underlying execution semantics.", "AI": {"tldr": "CodeRL+\u901a\u8fc7\u6574\u5408\u6267\u884c\u8bed\u4e49\u5bf9\u9f50\u5230RLVR\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u89e3\u51b3\u4e86LLM\u4ee3\u7801\u751f\u6210\u4e2d\u6587\u672c\u6a21\u5f0f\u4e0e\u6267\u884c\u8bed\u4e49\u4e4b\u95f4\u7684\u5dee\u8ddd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6d4b\u8bd5\u7528\u4f8b\u7684\u4e8c\u5143\u901a\u8fc7/\u5931\u8d25\u4fe1\u53f7\uff0c\u96be\u4ee5\u6709\u6548\u5efa\u7acb\u4ee3\u7801\u6587\u672c\u8868\u793a\u4e0e\u6267\u884c\u8bed\u4e49\u4e4b\u95f4\u7684\u5bf9\u9f50\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4ee3\u7801\u4e2d\u7684\u7ec6\u5fae\u903b\u8f91\u9519\u8bef\u3002", "method": "\u63d0\u51faCodeRL+\u65b9\u6cd5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u63a8\u65ad\u53d8\u91cf\u7ea7\u6267\u884c\u8f68\u8ff9\uff0c\u63d0\u4f9b\u6267\u884c\u8bed\u4e49\u7684\u76f4\u63a5\u5b66\u4e60\u4fe1\u53f7\uff0c\u53ef\u76f4\u63a5\u5229\u7528\u73b0\u6709\u7b56\u7565rollouts\u6784\u5efa\u6267\u884c\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u4e0e\u5404\u79cdRL\u7b97\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "result": "CodeRL+\u5728pass@1\u4e0a\u5b9e\u73b04.6%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u5728\u4ee3\u7801\u63a8\u7406\u548c\u6d4b\u8bd5\u8f93\u51fa\u751f\u6210\u57fa\u51c6\u4e0a\u5206\u522b\u83b7\u5f9715.5%\u548c4.4%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4e14\u5728\u4e0d\u540cRL\u7b97\u6cd5\u548cLLM\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u9002\u7528\u6027\u3002", "conclusion": "CodeRL+\u6709\u6548\u52a0\u5f3a\u4e86\u4ee3\u7801\u6587\u672c\u8868\u793a\u4e0e\u5e95\u5c42\u6267\u884c\u8bed\u4e49\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17937", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17937", "abs": "https://arxiv.org/abs/2510.17937", "authors": ["Fu-Yun Wang", "Han Zhang", "Michael Gharbi", "Hongsheng Li", "Taesung Park"], "title": "UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts", "comment": null, "summary": "We present UniRL-Zero, a unified reinforcement learning (RL) framework that\nboosts, multimodal language model understanding and reasoning, diffusion model\nmultimedia generation, and their beneficial interaction capabilities within a\nunified model. Our work defines six scenarios for unified model reinforcement\nlearning, providing systematic baselines for reinforcement learning of unified\nunderstanding and generation model. Our code is available at\nhttps://github.com/G-U-N/UniRL.", "AI": {"tldr": "UniRL-Zero\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e00\u6a21\u578b\u63d0\u5347\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u7406\u89e3\u63a8\u7406\u80fd\u529b\u3001\u6269\u6563\u6a21\u578b\u7684\u591a\u5a92\u4f53\u751f\u6210\u80fd\u529b\u4ee5\u53ca\u5b83\u4eec\u4e4b\u95f4\u7684\u6709\u76ca\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u6a21\u578b\u901a\u5e38\u72ec\u7acb\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u534f\u540c\u4f18\u5316\u8fd9\u4e9b\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4fc3\u8fdb\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\u7684\u534f\u540c\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86UniRL-Zero\u7edf\u4e00\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9a\u4e49\u4e86\u516d\u79cd\u7edf\u4e00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u573a\u666f\uff0c\u4e3a\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u7cfb\u7edf\u6027\u57fa\u51c6\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\u4e2d\u540c\u65f6\u63d0\u5347\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u7406\u89e3\u63a8\u7406\u80fd\u529b\u548c\u6269\u6563\u6a21\u578b\u7684\u591a\u5a92\u4f53\u751f\u6210\u80fd\u529b\u3002", "conclusion": "UniRL-Zero\u4e3a\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u57fa\u51c6\u548c\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u591a\u6a21\u6001AI\u80fd\u529b\u7684\u534f\u540c\u53d1\u5c55\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18509", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18509", "abs": "https://arxiv.org/abs/2510.18509", "authors": ["Valtteri Ala-Salmi", "Zeeshan Rasheed", "Abdul Malik Sami", "Muhammad Waseem", "Kai-Kristian Kemell", "Jussi Rasku", "Mika Saari", "Pekka Abrahamsson"], "title": "VAPU: System for Autonomous Legacy Code Modernization", "comment": "Table 13, figure 2", "summary": "In this study, we present a solution for the modernization of legacy\napplications, an area of code generation where LLM-based multi-agent systems\nare proving essential for complex multi-phased tasks. Legacy applications often\ncontain deprecated components that create compatibility, security, and\nreliability risks, but high resource costs make companies hesitate to update.\nWe take a step forward to integrate an LLM-based multi-agent system as part of\na legacy web application update to provide a cost-effective solution to update\nlegacy applications autonomously. We propose a multi-agent system named a\nVerifying Agent Pipeline Updater (VAPU), which is designed to update code files\nin phases while simulating different roles in a software development team. In\nour previous study, we evaluated the system for legacy version updates by using\nsix legacy web application view files by resulting errors and accomplished\nrequirements. This study extends the previous evaluation of a multi-agent\npipeline system by extending the evaluation of VAPU from a single LLM to five\nLLMs and using the temperature parameter in both 0 to 1 settings. Additionally,\nwe tested the system with 20 open-source Python GitHub projects. The results of\nthe evaluation were compared to Zero-Shot Learning (ZSL) and One-Shot Learning\n(OSL) prompts. The extended evaluation of VAPU showed that particularly in a\nlow-temperature VAPU can get similar level of error count compared to the\nZSL/OSL prompts but with a higher level of fulfilled requirements, depending on\nthe LLM. VAPU showed up to 22.5% increase in the succeeding Python file update\nrequirements compared to ZSL/OSL prompts. The study indicates that an LLM-based\nmulti-agent system is a capable solution to update components of a legacy\napplication autonomously.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edfVAPU\uff0c\u7528\u4e8e\u81ea\u4e3b\u66f4\u65b0\u9057\u7559\u5e94\u7528\u7a0b\u5e8f\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u548c\u5355\u6837\u672c\u5b66\u4e60\u63d0\u793a\uff0c\u5728\u4f4e\u6e29\u5ea6\u8bbe\u7f6e\u4e0b\u80fd\u5b9e\u73b0\u76f8\u4f3c\u9519\u8bef\u6570\u4f46\u66f4\u9ad8\u9700\u6c42\u5b8c\u6210\u7387\uff0cPython\u6587\u4ef6\u66f4\u65b0\u9700\u6c42\u6210\u529f\u7387\u63d0\u5347\u8fbe22.5%\u3002", "motivation": "\u9057\u7559\u5e94\u7528\u7a0b\u5e8f\u5305\u542b\u8fc7\u65f6\u7ec4\u4ef6\uff0c\u5b58\u5728\u517c\u5bb9\u6027\u3001\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u98ce\u9669\uff0c\u4f46\u9ad8\u6602\u7684\u8d44\u6e90\u6210\u672c\u4f7f\u4f01\u4e1a\u4e0d\u613f\u66f4\u65b0\u3002\u9700\u8981\u6210\u672c\u6548\u76ca\u9ad8\u7684\u81ea\u4e3b\u66f4\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u540d\u4e3aVAPU\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6a21\u62df\u8f6f\u4ef6\u5f00\u53d1\u56e2\u961f\u4e0d\u540c\u89d2\u8272\uff0c\u5206\u9636\u6bb5\u66f4\u65b0\u4ee3\u7801\u6587\u4ef6\u3002\u6269\u5c55\u8bc4\u4f30\u4ece\u5355\u4e00LLM\u5230\u4e94\u4e2aLLM\uff0c\u6e29\u5ea6\u53c2\u6570\u57280-1\u4e4b\u95f4\u8bbe\u7f6e\uff0c\u4f7f\u752820\u4e2a\u5f00\u6e90Python GitHub\u9879\u76ee\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728\u4f4e\u6e29\u5ea6\u8bbe\u7f6e\u4e0b\uff0cVAPU\u4e0eZSL/OSL\u63d0\u793a\u76f8\u6bd4\u9519\u8bef\u6570\u76f8\u4f3c\uff0c\u4f46\u9700\u6c42\u5b8c\u6210\u7387\u66f4\u9ad8\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u4f7f\u7528\u7684LLM\u3002Python\u6587\u4ef6\u66f4\u65b0\u9700\u6c42\u6210\u529f\u7387\u6700\u9ad8\u63d0\u534722.5%\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u662f\u81ea\u4e3b\u66f4\u65b0\u9057\u7559\u5e94\u7528\u7a0b\u5e8f\u7ec4\u4ef6\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2510.18254", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18254", "abs": "https://arxiv.org/abs/2510.18254", "authors": ["Sion Weatherhead", "Flora Salim", "Aaron Belbasis"], "title": "Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning", "comment": null, "summary": "Humans do not just find mistakes after the fact -- we often catch them\nmid-stream because 'reflection' is tied to the goal and its constraints.\nToday's large language models produce reasoning tokens and 'reflective' text,\nbut is it functionally equivalent with human reflective reasoning? Prior work\non closed-ended tasks -- with clear, external 'correctness' signals -- can make\n'reflection' look effective while masking limits in self-correction. We\ntherefore test eight frontier models on a simple, real-world task that is\nopen-ended yet rule-constrained, with auditable success criteria: to produce\nvalid scientific test items, then revise after considering their own critique.\nFirst-pass performance is poor (often zero valid items out of 4 required; mean\n$\\approx$ 1), and reflection yields only modest gains (also $\\approx$ 1).\nCrucially, the second attempt frequently repeats the same violation of\nconstraint, indicating 'corrective gains' arise largely from chance production\nof a valid item rather than error detection and principled,\nconstraint-sensitive repair. Performance before and after reflection\ndeteriorates as open-endedness increases, and models marketed for 'reasoning'\nshow no advantage. Our results suggest that current LLM 'reflection' lacks\nfunctional evidence of the active, goal-driven monitoring that helps humans\nrespect constraints even on a first pass. Until such mechanisms are\ninstantiated in the model itself, reliable performance requires external\nstructure that enforces constraints.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6d4b\u8bd5\u4e86\u516b\u4e2a\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u4f46\u89c4\u5219\u7ea6\u675f\u7684\u4efb\u52a1\u4e0a\u7684\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u53cd\u601d\u4ec5\u5e26\u6765\u6709\u9650\u7684\u6539\u8fdb\uff0c\u4e14\u7ecf\u5e38\u91cd\u590d\u76f8\u540c\u7684\u7ea6\u675f\u8fdd\u53cd\uff0c\u8868\u660e\u5f53\u524dLLM\u7684\u53cd\u601d\u7f3a\u4e4f\u4eba\u7c7b\u76ee\u6807\u9a71\u52a8\u7684\u4e3b\u52a8\u76d1\u63a7\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u68c0\u9a8c\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u601d\u80fd\u529b\u662f\u5426\u4e0e\u4eba\u7c7b\u53cd\u601d\u63a8\u7406\u529f\u80fd\u7b49\u6548\uff0c\u7279\u522b\u662f\u5728\u5f00\u653e\u5f0f\u4f46\u89c4\u5219\u7ea6\u675f\u7684\u4efb\u52a1\u4e2d\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u4e8e\u660e\u786e\u6b63\u786e\u4fe1\u53f7\u7684\u5c01\u95ed\u4efb\u52a1\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u8ba9\u6a21\u578b\u751f\u6210\u79d1\u5b66\u6d4b\u8bd5\u9879\u76ee\uff0c\u7136\u540e\u57fa\u4e8e\u81ea\u8eab\u6279\u8bc4\u8fdb\u884c\u4fee\u8ba2\uff0c\u8bc4\u4f30\u516b\u4e2a\u524d\u6cbf\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u89c4\u5219\u7ea6\u675f\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\u9996\u6b21\u5c1d\u8bd5\u8868\u73b0\u5f88\u5dee\uff08\u901a\u5e384\u4e2a\u8981\u6c42\u9879\u76ee\u4e2d\u96f6\u6709\u6548\u9879\u76ee\uff0c\u5747\u503c\u22481\uff09\uff0c\u53cd\u601d\u4ec5\u5e26\u6765\u9002\u5ea6\u6539\u8fdb\uff08\u540c\u6837\u22481\uff09\u3002\u6a21\u578b\u7ecf\u5e38\u91cd\u590d\u76f8\u540c\u7684\u7ea6\u675f\u8fdd\u53cd\uff0c\u6539\u8fdb\u4e3b\u8981\u6765\u81ea\u5076\u7136\u4ea7\u751f\u6709\u6548\u9879\u76ee\u800c\u975e\u9519\u8bef\u68c0\u6d4b\u548c\u539f\u5219\u6027\u4fee\u590d\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5f53\u524dLLM\u53cd\u601d\u7f3a\u4e4f\u529f\u80fd\u8bc1\u636e\u8bc1\u660e\u5176\u5177\u6709\u4eba\u7c7b\u90a3\u79cd\u4e3b\u52a8\u3001\u76ee\u6807\u9a71\u52a8\u7684\u76d1\u63a7\u673a\u5236\uff0c\u53ef\u9760\u6027\u80fd\u9700\u8981\u5916\u90e8\u7ed3\u6784\u6765\u5f3a\u5236\u6267\u884c\u7ea6\u675f\u3002", "topic": "agent analysis"}}
{"id": "2510.18314", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18314", "abs": "https://arxiv.org/abs/2510.18314", "authors": ["Zheng Zhang", "Jiarui He", "Yuchen Cai", "Deheng Ye", "Peilin Zhao", "Ruili Feng", "Hao Wang"], "title": "Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming", "comment": null, "summary": "As large language model (LLM) agents increasingly automate complex web tasks,\nthey boost productivity while simultaneously introducing new security risks.\nHowever, relevant studies on web agent attacks remain limited. Existing\nred-teaming approaches mainly rely on manually crafted attack strategies or\nstatic models trained offline. Such methods fail to capture the underlying\nbehavioral patterns of web agents, making it difficult to generalize across\ndiverse environments. In web agent attacks, success requires the continuous\ndiscovery and evolution of attack strategies. To this end, we propose Genesis,\na novel agentic framework composed of three modules: Attacker, Scorer, and\nStrategist. The Attacker generates adversarial injections by integrating the\ngenetic algorithm with a hybrid strategy representation. The Scorer evaluates\nthe target web agent's responses to provide feedback. The Strategist\ndynamically uncovers effective strategies from interaction logs and compiles\nthem into a continuously growing strategy library, which is then re-deployed to\nenhance the Attacker's effectiveness. Extensive experiments across various web\ntasks show that our framework discovers novel strategies and consistently\noutperforms existing attack baselines.", "AI": {"tldr": "Genesis\u662f\u4e00\u4e2a\u9488\u5bf9\u7f51\u7edc\u4ee3\u7406\u653b\u51fb\u7684\u667a\u80fd\u6846\u67b6\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u548c\u52a8\u6001\u7b56\u7565\u53d1\u73b0\u6765\u6301\u7eed\u6539\u8fdb\u653b\u51fb\u6548\u679c\uff0c\u5728\u591a\u79cd\u7f51\u7edc\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u590d\u6742\u7f51\u7edc\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u51f8\u663e\u3002\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u7b56\u7565\u6216\u9759\u6001\u79bb\u7ebf\u6a21\u578b\uff0c\u96be\u4ee5\u6355\u6349\u7f51\u7edc\u4ee3\u7406\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u73af\u5883\u3002", "method": "\u63d0\u51faGenesis\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u653b\u51fb\u8005\uff08\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u548c\u6df7\u5408\u7b56\u7565\u8868\u793a\u751f\u6210\u5bf9\u6297\u6ce8\u5165\uff09\u3001\u8bc4\u5206\u5668\uff08\u8bc4\u4f30\u76ee\u6807\u7f51\u7edc\u4ee3\u7406\u54cd\u5e94\uff09\u3001\u7b56\u7565\u5e08\uff08\u4ece\u4ea4\u4e92\u65e5\u5fd7\u4e2d\u52a8\u6001\u53d1\u73b0\u6709\u6548\u7b56\u7565\u5e76\u6784\u5efa\u6301\u7eed\u589e\u957f\u7b56\u7565\u5e93\uff09\u3002", "result": "\u5728\u591a\u79cd\u7f51\u7edc\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u53d1\u73b0\u65b0\u9896\u7b56\u7565\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Genesis\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u7b56\u7565\u53d1\u73b0\u548c\u6301\u7eed\u4f18\u5316\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7f51\u7edc\u4ee3\u7406\u653b\u51fb\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u7f51\u7edc\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.18560", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18560", "abs": "https://arxiv.org/abs/2510.18560", "authors": ["Chunyang Li", "Yilun Zheng", "Xinting Huang", "Tianqing Fang", "Jiahao Xu", "Yangqiu Song", "Lihui Chen", "Han Hu"], "title": "WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality", "comment": null, "summary": "The paradigm of LLM-as-a-judge is emerging as a scalable and efficient\nalternative to human evaluation, demonstrating strong performance on\nwell-defined tasks. However, its reliability in open-ended tasks with dynamic\nenvironments and complex interactions remains unexplored. To bridge the gap, we\nintroduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge\nperformance in web development, with support for both non-interactive\nevaluation based on static observations and continuous interactive evaluation\nwith a dynamic web environment. WebDevJudge comprises human preference labels\nover paired web implementations, annotated with structured and query-grounded\nrubrics to ensure high-quality ground truth. Using this benchmark, we\ncomprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic\nworkflows. We systematically investigate the impact of different paradigms and\nguidance mechanisms. Our experiments reveal a significant gap between LLM\njudges and human experts. In-depth analysis indicates this gap stems from\nfundamental model limitations, including failures in recognizing functional\nequivalence, verifying task feasibility, and mitigating bias. Overall,\nWebDevJudge presents a significant challenge to LLM-as-a-judge, offering\ninsights to guide future research toward developing more reliable and capable\nautomated evaluators for complicated scenarios. Code and data are available at\nhttps://github.com/lcy2723/WebDevJudge.", "AI": {"tldr": "WebDevJudge\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u5728\u7f51\u9875\u5f00\u53d1\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u7cfb\u7edf\u57fa\u51c6\uff0c\u652f\u6301\u975e\u4ea4\u4e92\u5f0f\u9759\u6001\u8bc4\u4f30\u548c\u8fde\u7eed\u4ea4\u4e92\u5f0f\u52a8\u6001\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86LLM\u8bc4\u5224\u8005\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u63a2\u7d22LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u5728\u52a8\u6001\u73af\u5883\u548c\u590d\u6742\u4ea4\u4e92\u7684\u5f00\u653e\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b\u4eba\u7c7b\u504f\u597d\u6807\u7b7e\u7684\u7f51\u9875\u5b9e\u73b0\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u4e14\u57fa\u4e8e\u67e5\u8be2\u7684\u8bc4\u5206\u6807\u51c6\uff0c\u8bc4\u4f30\u5404\u79cd\u8bc4\u4f30\u8005\uff08LLM\u3001MLLM\u3001\u4ee3\u7406\u5de5\u4f5c\u6d41\uff09\u5728\u4e0d\u540c\u8303\u5f0f\u548c\u6307\u5bfc\u673a\u5236\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLM\u8bc4\u5224\u8005\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e3b\u8981\u6e90\u4e8e\u6a21\u578b\u5728\u8bc6\u522b\u529f\u80fd\u7b49\u4ef7\u6027\u3001\u9a8c\u8bc1\u4efb\u52a1\u53ef\u884c\u6027\u4ee5\u53ca\u51cf\u8f7b\u504f\u89c1\u65b9\u9762\u7684\u57fa\u672c\u9650\u5236\u3002", "conclusion": "WebDevJudge\u5bf9LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u8303\u5f0f\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u5f3a\u5927\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u8005\u63d0\u4f9b\u4e86\u6307\u5bfc\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.18053", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18053", "abs": "https://arxiv.org/abs/2510.18053", "authors": ["Jiajun Fan", "Tong Wei", "Chaoran Cheng", "Yuxin Chen", "Ge Liu"], "title": "Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models", "comment": "30 pages", "summary": "Balancing exploration and exploitation during reinforcement learning\nfine-tuning of generative models presents a critical challenge, as existing\napproaches rely on fixed divergence regularization that creates an inherent\ndilemma: strong regularization preserves model capabilities but limits reward\noptimization, while weak regularization enables greater alignment but risks\ninstability or reward hacking. We introduce Adaptive Divergence Regularized\nPolicy Optimization (ADRPO), which automatically adjusts regularization\nstrength based on advantage estimates-reducing regularization for high-value\nsamples while applying stronger regularization to poor samples, enabling\npolicies to navigate between exploration and aggressive exploitation according\nto data quality. Our implementation with Wasserstein-2 regularization for flow\nmatching generative models achieves remarkable results on text-to-image\ngeneration, achieving better semantic alignment and diversity than offline\nmethods like DPO and online methods with fixed regularization like ORW-CFM-W2.\nADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B\nand 12B parameters in attribute binding, semantic consistency, artistic style\ntransfer, and compositional control while maintaining generation diversity.\nADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and\nmulti-modal reasoning models, enhancing existing online RL methods like GRPO.\nIn LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local\noptima through active exploration, while in multi-modal audio reasoning, it\noutperforms GRPO through superior step-by-step reasoning, enabling a 7B model\nto outperform substantially larger commercial models including Gemini 2.5 Pro\nand GPT-4o Audio, offering an effective plug-and-play solution to the\nexploration-exploitation challenge across diverse generative architectures and\nmodalities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u53d1\u6563\u6b63\u5219\u5316\u7b56\u7565\u4f18\u5316(ADRPO)\uff0c\u901a\u8fc7\u57fa\u4e8e\u4f18\u52bf\u4f30\u8ba1\u81ea\u52a8\u8c03\u6574\u6b63\u5219\u5316\u5f3a\u5ea6\u6765\u89e3\u51b3\u751f\u6210\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u95ee\u9898\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548cLLM\u5fae\u8c03\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u53d1\u6563\u6b63\u5219\u5316\uff0c\u5bfc\u81f4\u4e24\u96be\u56f0\u5883\uff1a\u5f3a\u6b63\u5219\u5316\u4fdd\u7559\u6a21\u578b\u80fd\u529b\u4f46\u9650\u5236\u5956\u52b1\u4f18\u5316\uff0c\u5f31\u6b63\u5219\u5316\u53ef\u5b9e\u73b0\u66f4\u597d\u5bf9\u9f50\u4f46\u98ce\u9669\u4e0d\u7a33\u5b9a\u6216\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u3002", "method": "ADRPO\u6839\u636e\u4f18\u52bf\u4f30\u8ba1\u81ea\u52a8\u8c03\u6574\u6b63\u5219\u5316\u5f3a\u5ea6\uff1a\u5bf9\u9ad8\u4ef7\u503c\u6837\u672c\u51cf\u5c11\u6b63\u5219\u5316\uff0c\u5bf9\u5dee\u6837\u672c\u5e94\u7528\u66f4\u5f3a\u6b63\u5219\u5316\uff0c\u4f7f\u7b56\u7565\u80fd\u6839\u636e\u6570\u636e\u8d28\u91cf\u5728\u63a2\u7d22\u548c\u79ef\u6781\u5229\u7528\u95f4\u5bfc\u822a\u3002\u4f7f\u7528Wasserstein-2\u6b63\u5219\u5316\u8fdb\u884c\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\uff0cADRPO\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8eDPO\u548cORW-CFM-W2\u7b49\u65b9\u6cd5\uff1b2B\u53c2\u6570SD3\u6a21\u578b\u5728\u5c5e\u6027\u7ed1\u5b9a\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u7b49\u65b9\u9762\u8d85\u8d8a4.8B\u548c12B\u53c2\u6570\u6a21\u578b\uff1b\u5728LLM\u548c\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\uff0c7B\u6a21\u578b\u8d85\u8d8aGemini 2.5 Pro\u548cGPT-4o Audio\u3002", "conclusion": "ADRPO\u4e3a\u4e0d\u540c\u751f\u6210\u67b6\u6784\u548c\u6a21\u6001\u7684\u63a2\u7d22-\u5229\u7528\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u9003\u79bb\u5c40\u90e8\u6700\u4f18\uff0c\u5b9e\u73b0\u4f18\u8d8a\u7684\u9010\u6b65\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18395", "categories": ["cs.AI", "68T42, 68T37, 91A35", "I.2.6; I.2.11; I.2.8; K.8.0"], "pdf": "https://arxiv.org/pdf/2510.18395", "abs": "https://arxiv.org/abs/2510.18395", "authors": ["Runnan Qi", "Yanan Ni", "Lumin Jiang", "Zongyuan Li", "Kuihua Huang", "Xian Guo"], "title": "Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games", "comment": "10 pages, 4 figures, 1 table, 1 algorithm. Submitted to conference", "summary": "This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel\nframework for LLM agents in real-time strategy games. Addressing key challenges\nlike hallucinations and fragmented decision-making in existing approaches,\nMASMP integrates state machine prompting with memory mechanisms to unify\nstructured actions with long-term tactical coherence. The framework features:\n(1) a natural language-driven state machine architecture that guides LLMs to\nemulate finite state machines and behavior trees through prompts, and (2) a\nlightweight memory module preserving strategic variables (e.g., tactics,\npriority units) across decision cycles. Experiments in StarCraft II demonstrate\nMASMP's 60% win rate against the hardest built-in AI (Lv7), vastly\noutperforming baselines (0%). Case studies reveal the method retains LLMs'\nsemantic comprehension while resolving the \"Knowing-Doing Gap\" through strict\nstate-action mapping, achieving both interpretability and FSM-like reliability.\nThis work establishes a new paradigm for combining neural and symbolic AI in\ncomplex decision-making.", "AI": {"tldr": "\u63d0\u51faMASMP\u6846\u67b6\uff0c\u901a\u8fc7\u72b6\u6001\u673a\u63d0\u793a\u548c\u8bb0\u5fc6\u673a\u5236\u89e3\u51b3LLM\u5728\u5b9e\u65f6\u7b56\u7565\u6e38\u620f\u4e2d\u7684\u5e7b\u89c9\u548c\u51b3\u7b56\u788e\u7247\u5316\u95ee\u9898\uff0c\u5728\u661f\u9645\u4e89\u9738II\u4e2d\u8fbe\u523060%\u80dc\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u5b9e\u65f6\u7b56\u7565\u6e38\u620f\u4e2d\u5b58\u5728\u7684\u5e7b\u89c9\u95ee\u9898\u548c\u51b3\u7b56\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u5347\u957f\u671f\u6218\u672f\u4e00\u81f4\u6027\u3002", "method": "\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u72b6\u6001\u673a\u67b6\u6784\u548c\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u6a21\u5757\uff0c\u901a\u8fc7\u63d0\u793a\u5f15\u5bfcLLM\u6a21\u62df\u6709\u9650\u72b6\u6001\u673a\u548c\u884c\u4e3a\u6811\uff0c\u8de8\u51b3\u7b56\u5468\u671f\u4fdd\u6301\u6218\u7565\u53d8\u91cf\u3002", "result": "\u5728\u661f\u9645\u4e89\u9738II\u4e2d\u5bf9\u6218\u6700\u5f3a\u5185\u7f6eAI(Lv7)\u8fbe\u523060%\u80dc\u7387\uff0c\u8fdc\u8d85\u57fa\u7ebf\u65b9\u6cd5(0%)\uff0c\u6210\u529f\u89e3\u51b3\"\u77e5-\u884c\u5dee\u8ddd\"\u95ee\u9898\u3002", "conclusion": "\u5efa\u7acb\u4e86\u7ed3\u5408\u795e\u7ecf\u548c\u7b26\u53f7AI\u5728\u590d\u6742\u51b3\u7b56\u4e2d\u7684\u65b0\u8303\u5f0f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86LLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u548c\u72b6\u6001\u673a\u822c\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.18596", "categories": ["cs.SE", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.18596", "abs": "https://arxiv.org/abs/2510.18596", "authors": ["Haojia Lin", "Xiaoyu Tan", "Yulei Qin", "Zihan Xu", "Yuchen Shi", "Zongyi Li", "Gang Li", "Shaofei Cai", "Siqi Cai", "Chaoyou Fu", "Ke Li", "Xing Sun"], "title": "CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent", "comment": "24 pages, 6 figures", "summary": "Computer-using agents (CUAs) enable task completion through natural\ninteraction with operating systems and software interfaces. While script-based\nverifiers are widely adopted for evaluation, they suffer from limited\nscalability and inability to provide step-wise assessment. Reward models offer\npromising alternatives, but their effectiveness on CUA evaluation remains\nlargely underexplored. To address this gap, we present CUARewardBench,\ncomprising four key contributions: (1) First-ever Comprehensive CUA Reward\nBenchmark: We introduce the first benchmark for evaluating both outcome reward\nmodels (ORM) and process reward models (PRM) on CUA tasks, enabling systematic\nassessment across trajectory-level and step-level evaluation. (2) Diverse,\nPractical and Reliable Dataset: CUARewardBench encompasses trajectories from 10\nsoftware categories and 7 agent architectures with varying performance levels\n(25.9%-50.8% success rates). All trajectories are expertly annotated through\ncarefully designed protocols, with rigorous quality control to ensure\nreliability and practical applicability. (3) Comprehensive Analysis and\nInsights: Through extensive experiments across 7 vision-language models and 3\nprompt templates, we reveal critical limitations of current CUA RMs, including\ninsufficient visual reasoning capabilities, knowledge deficiencies, and the\nsuperiority of general VLMs over specialized CUA models for reward evaluation.\n(4) Unanimous Prompt Ensemble (UPE): Based on the insights from our\ncomprehensive analysis, we propose UPE, a novel ensemble method that\nsignificantly enhances reward model reliability through strict unanimous voting\nand strategic prompt-template configurations. UPE achieves 89.8% precision and\n93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially\noutperforming single VLMs and traditional ensemble approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86CUARewardBench\uff0c\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5956\u52b1\u6a21\u578b\u7684\u57fa\u51c6\uff0c\u5305\u542b\u7ed3\u679c\u5956\u52b1\u6a21\u578b\u548c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u63d0\u793a\u96c6\u6210\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5956\u52b1\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u811a\u672c\u7684\u9a8c\u8bc1\u5668\u5b58\u5728\u53ef\u6269\u5c55\u6027\u6709\u9650\u548c\u65e0\u6cd5\u63d0\u4f9b\u9010\u6b65\u8bc4\u4f30\u7684\u95ee\u9898\uff0c\u800c\u5956\u52b1\u6a21\u578b\u5728\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b10\u4e2a\u8f6f\u4ef6\u7c7b\u522b\u548c7\u79cd\u4ee3\u7406\u67b6\u6784\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u786e\u4fdd\u53ef\u9760\u6027\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u63d0\u793a\u96c6\u6210\u65b9\u6cd5\u8fdb\u884c\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u63ed\u793a\u4e86\u5f53\u524d\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5956\u52b1\u6a21\u578b\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5305\u62ec\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u548c\u77e5\u8bc6\u7f3a\u9677\uff0c\u7edf\u4e00\u7684\u63d0\u793a\u96c6\u6210\u65b9\u6cd5\u5728\u7ed3\u679c\u5956\u52b1\u6a21\u578b\u4e0a\u8fbe\u523089.8%\u7cbe\u786e\u5ea6\u548c93.3%\u8d1f\u9884\u6d4b\u503c\uff0c\u5728\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u4e0a\u8fbe\u523081.7%\u7cbe\u786e\u5ea6\u548c85.1%\u8d1f\u9884\u6d4b\u503c\u3002", "conclusion": "\u7edf\u4e00\u7684\u63d0\u793a\u96c6\u6210\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5956\u52b1\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5956\u52b1\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u4e13\u95e8\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\u3002", "topic": "agent analysis"}}
{"id": "2510.18196", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18196", "abs": "https://arxiv.org/abs/2510.18196", "authors": ["Yoshinari Fujinuma"], "title": "Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge", "comment": null, "summary": "Large Language Models (LLMs) are commonly used as evaluators in various\napplications, but the reliability of the outcomes remains a challenge. One such\nchallenge is using LLMs-as-judges for direct assessment, i.e., assigning scores\nfrom a specified range without any references. We first show that this\nchallenge stems from LLM judge outputs being associated with score range bias,\ni.e., LLM judge outputs are highly sensitive to pre-defined score ranges,\npreventing the search for optimal score ranges. We also show that similar\nbiases exist among models from the same family. We then mitigate this bias\nthrough contrastive decoding, achieving up to 11.3% relative improvement on\naverage in Spearman correlation with human judgments across different score\nranges.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u65f6\u7684\u8bc4\u5206\u8303\u56f4\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u7f13\u89e3\u8be5\u504f\u5dee\uff0c\u5728Spearman\u76f8\u5173\u6027\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347", "motivation": "LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u5728\u76f4\u63a5\u8bc4\u4f30\u4e2d\u9762\u4e34\u53ef\u9760\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u8bc4\u5206\u8303\u56f4\u504f\u5dee\u95ee\u9898\uff0c\u5373LLM\u8bc4\u4f30\u7ed3\u679c\u5bf9\u9884\u5b9a\u4e49\u8bc4\u5206\u8303\u56f4\u9ad8\u5ea6\u654f\u611f", "method": "\u4f7f\u7528\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u6765\u7f13\u89e3LLM\u8bc4\u4f30\u8005\u7684\u8bc4\u5206\u8303\u56f4\u504f\u5dee\u95ee\u9898", "result": "\u901a\u8fc7\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u8bc4\u5206\u8303\u56f4\u5185\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684Spearman\u76f8\u5173\u6027\u5e73\u5747\u76f8\u5bf9\u63d0\u5347\u4e8611.3%", "conclusion": "\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3LLM\u8bc4\u4f30\u8005\u7684\u8bc4\u5206\u8303\u56f4\u504f\u5dee\uff0c\u63d0\u9ad8\u8bc4\u4f30\u53ef\u9760\u6027", "topic": "agent analysis"}}
{"id": "2510.18060", "categories": ["cs.LG", "cs.AI", "cs.RO", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.18060", "abs": "https://arxiv.org/abs/2510.18060", "authors": ["Wei-Jer Chang", "Akshay Rangesh", "Kevin Joseph", "Matthew Strong", "Masayoshi Tomizuka", "Yihan Hu", "Wei Zhan"], "title": "SPACeR: Self-Play Anchoring with Centralized Reference Models", "comment": "Project page: https://spacer-ai.github.io/", "summary": "Developing autonomous vehicles (AVs) requires not only safety and efficiency,\nbut also realistic, human-like behaviors that are socially aware and\npredictable. Achieving this requires sim agent policies that are human-like,\nfast, and scalable in multi-agent settings. Recent progress in imitation\nlearning with large diffusion-based or tokenized models has shown that\nbehaviors can be captured directly from human driving data, producing realistic\npolicies. However, these models are computationally expensive, slow during\ninference, and struggle to adapt in reactive, closed-loop scenarios. In\ncontrast, self-play reinforcement learning (RL) scales efficiently and\nnaturally captures multi-agent interactions, but it often relies on heuristics\nand reward shaping, and the resulting policies can diverge from human norms. We\npropose SPACeR, a framework that leverages a pretrained tokenized\nautoregressive motion model as a centralized reference policy to guide\ndecentralized self-play. The reference model provides likelihood rewards and KL\ndivergence, anchoring policies to the human driving distribution while\npreserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our\nmethod achieves competitive performance with imitation-learned policies while\nbeing up to 10x faster at inference and 50x smaller in parameter size than\nlarge generative models. In addition, we demonstrate in closed-loop ego\nplanning evaluation tasks that our sim agents can effectively measure planner\nquality with fast and scalable traffic simulation, establishing a new paradigm\nfor testing autonomous driving policies.", "AI": {"tldr": "SPACeR\u6846\u67b6\u7ed3\u5408\u9884\u8bad\u7ec3tokenized\u81ea\u56de\u5f52\u8fd0\u52a8\u6a21\u578b\u548c\u81ea\u73a9\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u4eba\u7c7b\u5316\u7684\u591a\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u6bd4\u5927\u578b\u751f\u6210\u6a21\u578b\u5feb10\u500d\u4e14\u53c2\u6570\u5c1150\u500d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u57fa\u4e8e\u6269\u6563\u6216tokenized\u6a21\u578b\u7684\u6a21\u4eff\u5b66\u4e60\u80fd\u4ea7\u751f\u771f\u5b9e\u884c\u4e3a\u4f46\u8ba1\u7b97\u6602\u8d35\u4e14\u63a8\u7406\u6162\uff0c\u800c\u81ea\u73a9\u5f3a\u5316\u5b66\u4e60\u867d\u9ad8\u6548\u4f46\u5bb9\u6613\u504f\u79bb\u4eba\u7c7b\u9a7e\u9a76\u89c4\u8303\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3tokenized\u81ea\u56de\u5f52\u8fd0\u52a8\u6a21\u578b\u4f5c\u4e3a\u96c6\u4e2d\u5f0f\u53c2\u8003\u7b56\u7565\uff0c\u6307\u5bfc\u53bb\u4e2d\u5fc3\u5316\u81ea\u73a9\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u4f3c\u7136\u5956\u52b1\u548cKL\u6563\u5ea6\u5c06\u7b56\u7565\u951a\u5b9a\u5728\u4eba\u7c7b\u9a7e\u9a76\u5206\u5e03\u4e0a\u3002", "result": "\u5728Waymo Sim Agents Challenge\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u5927\u578b\u751f\u6210\u6a21\u578b\u5feb10\u500d\uff0c\u53c2\u6570\u89c4\u6a21\u5c0f50\u500d\uff0c\u540c\u65f6\u5728\u95ed\u73af\u81ea\u6211\u89c4\u5212\u8bc4\u4f30\u4e2d\u80fd\u6709\u6548\u6d4b\u91cf\u89c4\u5212\u5668\u8d28\u91cf\u3002", "conclusion": "SPACeR\u5efa\u7acb\u4e86\u6d4b\u8bd5\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5feb\u901f\u53ef\u6269\u5c55\u7684\u4ea4\u901a\u4eff\u771f\u5b9e\u73b0\u9ad8\u6548\u8bc4\u4f30\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18407", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18407", "abs": "https://arxiv.org/abs/2510.18407", "authors": ["Manjie Xu", "Xinyi Yang", "Jiayu Zhan", "Wei Liang", "Chi Zhang", "Yixin Zhu"], "title": "Heterogeneous Adversarial Play in Interactive Environments", "comment": "NeurIPS 2025", "summary": "Self-play constitutes a fundamental paradigm for autonomous skill\nacquisition, whereby agents iteratively enhance their capabilities through\nself-directed environmental exploration. Conventional self-play frameworks\nexploit agent symmetry within zero-sum competitive settings, yet this approach\nproves inadequate for open-ended learning scenarios characterized by inherent\nasymmetry. Human pedagogical systems exemplify asymmetric instructional\nframeworks wherein educators systematically construct challenges calibrated to\nindividual learners' developmental trajectories. The principal challenge\nresides in operationalizing these asymmetric, adaptive pedagogical mechanisms\nwithin artificial systems capable of autonomously synthesizing appropriate\ncurricula without predetermined task hierarchies. Here we present Heterogeneous\nAdversarial Play (HAP), an adversarial Automatic Curriculum Learning framework\nthat formalizes teacher-student interactions as a minimax optimization wherein\ntask-generating instructor and problem-solving learner co-evolve through\nadversarial dynamics. In contrast to prevailing ACL methodologies that employ\nstatic curricula or unidirectional task selection mechanisms, HAP establishes a\nbidirectional feedback system wherein instructors continuously recalibrate task\ncomplexity in response to real-time learner performance metrics. Experimental\nvalidation across multi-task learning domains demonstrates that our framework\nachieves performance parity with SOTA baselines while generating curricula that\nenhance learning efficacy in both artificial agents and human subjects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5f02\u6784\u5bf9\u6297\u6e38\u620f\uff08HAP\uff09\u7684\u5bf9\u6297\u6027\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u4ea4\u4e92\u7684\u6781\u5c0f\u6781\u5927\u4f18\u5316\u6765\u751f\u6210\u81ea\u9002\u5e94\u8bfe\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u535a\u5f08\u5728\u975e\u5bf9\u79f0\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u81ea\u535a\u5f08\u6846\u67b6\u5728\u96f6\u548c\u7ade\u4e89\u73af\u5883\u4e2d\u5229\u7528\u667a\u80fd\u4f53\u5bf9\u79f0\u6027\uff0c\u4f46\u5728\u5177\u6709\u56fa\u6709\u4e0d\u5bf9\u79f0\u6027\u7684\u5f00\u653e\u5f0f\u5b66\u4e60\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002\u4eba\u7c7b\u6559\u5b66\u7cfb\u7edf\u5c55\u793a\u4e86\u975e\u5bf9\u79f0\u6559\u5b66\u6846\u67b6\u7684\u4f18\u52bf\uff0c\u9700\u8981\u5c06\u5176\u64cd\u4f5c\u5316\u5230\u80fd\u591f\u81ea\u4e3b\u5408\u6210\u9002\u5f53\u8bfe\u7a0b\u7684\u4eba\u5de5\u7cfb\u7edf\u4e2d\u3002", "method": "HAP\u6846\u67b6\u5c06\u6559\u5e08-\u5b66\u751f\u4ea4\u4e92\u5f62\u5f0f\u5316\u4e3a\u6781\u5c0f\u6781\u5927\u4f18\u5316\uff0c\u4efb\u52a1\u751f\u6210\u6559\u5e08\u548c\u95ee\u9898\u89e3\u51b3\u5b66\u751f\u901a\u8fc7\u5bf9\u6297\u52a8\u6001\u5171\u540c\u8fdb\u5316\uff0c\u5efa\u7acb\u53cc\u5411\u53cd\u9988\u7cfb\u7edf\uff0c\u6559\u5e08\u6839\u636e\u5b9e\u65f6\u5b66\u4e60\u8005\u8868\u73b0\u6307\u6807\u6301\u7eed\u91cd\u65b0\u6821\u51c6\u4efb\u52a1\u590d\u6742\u5ea6\u3002", "result": "\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u9886\u57df\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4e0e\u6700\u5148\u8fdb\u57fa\u7ebf\u8fbe\u5230\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u751f\u6210\u7684\u8bfe\u7a0b\u63d0\u9ad8\u4e86\u4eba\u5de5\u667a\u80fd\u4f53\u548c\u4eba\u7c7b\u53d7\u8bd5\u8005\u7684\u5b66\u4e60\u6548\u7387\u3002", "conclusion": "HAP\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u975e\u5bf9\u79f0\u81ea\u9002\u5e94\u6559\u5b66\u673a\u5236\u5728\u4eba\u5de5\u7cfb\u7edf\u4e2d\u7684\u64cd\u4f5c\u5316\uff0c\u4e3a\u5f00\u653e\u5f0f\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u8bfe\u7a0b\u751f\u6210\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18074", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.18074", "abs": "https://arxiv.org/abs/2510.18074", "authors": ["Nadir Farhi"], "title": "R2L: Reliable Reinforcement Learning: Guaranteed Return & Reliable Policies in Reinforcement Learning", "comment": "27 pages", "summary": "In this work, we address the problem of determining reliable policies in\nreinforcement learning (RL), with a focus on optimization under uncertainty and\nthe need for performance guarantees. While classical RL algorithms aim at\nmaximizing the expected return, many real-world applications - such as routing,\nresource allocation, or sequential decision-making under risk - require\nstrategies that ensure not only high average performance but also a guaranteed\nprobability of success. To this end, we propose a novel formulation in which\nthe objective is to maximize the probability that the cumulative return exceeds\na prescribed threshold. We demonstrate that this reliable RL problem can be\nreformulated, via a state-augmented representation, into a standard RL problem,\nthereby allowing the use of existing RL and deep RL algorithms without the need\nfor entirely new algorithmic frameworks. Theoretical results establish the\nequivalence of the two formulations and show that reliable strategies can be\nderived by appropriately adapting well-known methods such as Q-learning or\nDueling Double DQN. To illustrate the practical relevance of the approach, we\nconsider the problem of reliable routing, where the goal is not to minimize the\nexpected travel time but rather to maximize the probability of reaching the\ndestination within a given time budget. Numerical experiments confirm that the\nproposed formulation leads to policies that effectively balance efficiency and\nreliability, highlighting the potential of reliable RL for applications in\nstochastic and safety-critical environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u9760\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u7d2f\u79ef\u56de\u62a5\u8d85\u8fc7\u7ed9\u5b9a\u9608\u503c\u7684\u6982\u7387\u6765\u786e\u4fdd\u7b56\u7565\u53ef\u9760\u6027\uff0c\u800c\u975e\u4f20\u7edfRL\u7684\u671f\u671b\u56de\u62a5\u6700\u5927\u5316\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u9700\u8981\u786e\u4fdd\u7b56\u7565\u4e0d\u4ec5\u5177\u6709\u9ad8\u5e73\u5747\u6027\u80fd\uff0c\u8fd8\u8981\u6709\u6210\u529f\u6982\u7387\u4fdd\u8bc1\uff0c\u5982\u8def\u7531\u3001\u8d44\u6e90\u5206\u914d\u7b49\u98ce\u9669\u51b3\u7b56\u573a\u666f\u3002", "method": "\u901a\u8fc7\u72b6\u6001\u589e\u5f3a\u8868\u793a\u5c06\u53ef\u9760RL\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u6807\u51c6RL\u95ee\u9898\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u73b0\u6709RL\u7b97\u6cd5\u5982Q-learning\u3001Dueling Double DQN\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e24\u79cd\u8868\u8ff0\u7684\u7b49\u4ef7\u6027\uff0c\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u53ef\u9760\u8def\u7531\u95ee\u9898\u4e2d\u80fd\u6709\u6548\u5e73\u8861\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u53ef\u9760RL\u6846\u67b6\u9002\u7528\u4e8e\u968f\u673a\u548c\u5b89\u5168\u5173\u952e\u73af\u5883\uff0c\u65e0\u9700\u5f00\u53d1\u5168\u65b0\u7b97\u6cd5\u6846\u67b6\u5373\u53ef\u83b7\u5f97\u53ef\u9760\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18424", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18424", "abs": "https://arxiv.org/abs/2510.18424", "authors": ["Guangfu Guo", "Xiaoqian Lu", "Yue Feng"], "title": "Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents", "comment": null, "summary": "Visual Language Models (VLMs) achieve promising results in medical reasoning\nbut struggle with hallucinations, vague descriptions, inconsistent logic and\npoor localization. To address this, we propose a agent framework named Medical\nVisual Reasoning Agent (\\textbf{Med-VRAgent}). The approach is based on Visual\nGuidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By\ncombining the Visual Guidance with tree search, Med-VRAgent improves the\nmedical visual reasoning capabilities of VLMs. We use the trajectories\ncollected by Med-VRAgent as feedback to further improve the performance by\nfine-tuning the VLMs with the proximal policy optimization (PPO) objective.\nExperiments on multiple medical VQA benchmarks demonstrate that our method\noutperforms existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86Med-VRAgent\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u5f15\u5bfc\u3001\u81ea\u6211\u5956\u52b1\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u901a\u8fc7PPO\u5fae\u8c03\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u6027\u80fd", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u63a8\u7406\u4e2d\u5b58\u5728\u5e7b\u89c9\u3001\u6a21\u7cca\u63cf\u8ff0\u3001\u903b\u8f91\u4e0d\u4e00\u81f4\u548c\u5b9a\u4f4d\u5dee\u7b49\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb", "method": "\u57fa\u4e8e\u89c6\u89c9\u5f15\u5bfc\u548c\u81ea\u6211\u5956\u52b1\u8303\u5f0f\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u4f7f\u7528PPO\u76ee\u6807\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b", "result": "\u5728\u591a\u4e2a\u533b\u5b66VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "Med-VRAgent\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u89c6\u89c9\u63a8\u7406\u80fd\u529b", "topic": "agent analysis"}}
{"id": "2510.18861", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18861", "abs": "https://arxiv.org/abs/2510.18861", "authors": ["Pedro Lu\u00eds Fonseca", "Bruno Lima", "Jo\u00e3o Pascoal Faria"], "title": "Streamlining Acceptance Test Generation for Mobile Applications Through Large Language Models: An Industrial Case Study", "comment": null, "summary": "Mobile acceptance testing remains a bottleneck in modern software\ndevelopment, particularly for cross-platform mobile development using\nframeworks like Flutter. While developers increasingly rely on automated\ntesting tools, creating and maintaining acceptance test artifacts still demands\nsignificant manual effort. To help tackle this issue, we introduce AToMIC, an\nautomated framework leveraging specialized Large Language Models to generate\nGherkin scenarios, Page Objects, and executable UI test scripts directly from\nrequirements (JIRA tickets) and recent code changes. Applied to BMW's MyBMW\napp, covering 13 real-world issues in a 170+ screen codebase, AToMIC produced\nexecutable test artifacts in under five minutes per feature on standard\nhardware. The generated artifacts were of high quality: 93.3% of Gherkin\nscenarios were syntactically correct upon generation, 78.8% of PageObjects ran\nwithout manual edits, and 100% of generated UI tests executed successfully. In\na survey, all practitioners reported time savings (often a full developer-day\nper feature) and strong confidence in adopting the approach. These results\nconfirm AToMIC as a scalable, practical solution for streamlining acceptance\ntest creation and maintenance in industrial mobile projects.", "AI": {"tldr": "AToMIC\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5229\u7528\u4e13\u95e8\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u9700\u6c42(JIRA\u5de5\u5355)\u548c\u4ee3\u7801\u53d8\u66f4\u76f4\u63a5\u751f\u6210Gherkin\u573a\u666f\u3001\u9875\u9762\u5bf9\u8c61\u548c\u53ef\u6267\u884c\u7684UI\u6d4b\u8bd5\u811a\u672c\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u79fb\u52a8\u5e94\u7528\u9a8c\u6536\u6d4b\u8bd5\u7684\u624b\u52a8\u5de5\u4f5c\u91cf\u3002", "motivation": "\u79fb\u52a8\u9a8c\u6536\u6d4b\u8bd5\u5728\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u4ecd\u7136\u662f\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528Flutter\u7b49\u8de8\u5e73\u53f0\u79fb\u52a8\u5f00\u53d1\u6846\u67b6\u65f6\u3002\u867d\u7136\u5f00\u53d1\u8005\u8d8a\u6765\u8d8a\u4f9d\u8d56\u81ea\u52a8\u5316\u6d4b\u8bd5\u5de5\u5177\uff0c\u4f46\u521b\u5efa\u548c\u7ef4\u62a4\u9a8c\u6536\u6d4b\u8bd5\u5de5\u4ef6\u4ecd\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\u3002", "method": "AToMIC\u6846\u67b6\u5229\u7528\u4e13\u95e8\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ece\u9700\u6c42(JIRA\u5de5\u5355)\u548c\u6700\u8fd1\u7684\u4ee3\u7801\u53d8\u66f4\u76f4\u63a5\u751f\u6210Gherkin\u573a\u666f\u3001\u9875\u9762\u5bf9\u8c61\u548c\u53ef\u6267\u884c\u7684UI\u6d4b\u8bd5\u811a\u672c\u3002", "result": "\u5728BMW MyBMW\u5e94\u7528\u4e2d\u6d4b\u8bd513\u4e2a\u771f\u5b9e\u95ee\u9898\uff0c\u8986\u76d6170+\u5c4f\u5e55\u7684\u4ee3\u7801\u5e93\uff0cAToMIC\u5728\u6807\u51c6\u786c\u4ef6\u4e0a\u6bcf\u4e2a\u529f\u80fd\u57285\u5206\u949f\u5185\u751f\u6210\u53ef\u6267\u884c\u6d4b\u8bd5\u5de5\u4ef6\u300293.3%\u7684Gherkin\u573a\u666f\u5728\u751f\u6210\u65f6\u8bed\u6cd5\u6b63\u786e\uff0c78.8%\u7684\u9875\u9762\u5bf9\u8c61\u65e0\u9700\u624b\u52a8\u7f16\u8f91\u5373\u53ef\u8fd0\u884c\uff0c100%\u751f\u6210\u7684UI\u6d4b\u8bd5\u6210\u529f\u6267\u884c\u3002", "conclusion": "AToMIC\u88ab\u8bc1\u5b9e\u4e3a\u5de5\u4e1a\u79fb\u52a8\u9879\u76ee\u4e2d\u7b80\u5316\u9a8c\u6536\u6d4b\u8bd5\u521b\u5efa\u548c\u7ef4\u62a4\u7684\u53ef\u6269\u5c55\u3001\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u6240\u6709\u5b9e\u8df5\u8005\u90fd\u62a5\u544a\u4e86\u65f6\u95f4\u8282\u7701(\u901a\u5e38\u6bcf\u4e2a\u529f\u80fd\u8282\u7701\u4e00\u4e2a\u5b8c\u6574\u5f00\u53d1\u65e5)\u5e76\u5bf9\u91c7\u7528\u8be5\u65b9\u6cd5\u6709\u5f3a\u70c8\u4fe1\u5fc3\u3002", "topic": "swe application"}}
{"id": "2510.18442", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18442", "abs": "https://arxiv.org/abs/2510.18442", "authors": ["Ziwei Deng", "Mian Deng", "Chenjing Liang", "Zeming Gao", "Chennan Ma", "Chenxing Lin", "Haipeng Zhang", "Songzhu Mei", "Cheng Wang", "Siqi Shen"], "title": "PlanU: Large Language Model Decision Making through Planning under Uncertainty", "comment": "38 pages, 19 figures, NeurIPS 2025 Accepted", "summary": "Large Language Models (LLMs) are increasingly being explored across a range\nof decision-making tasks. However, LLMs sometimes struggle with decision-making\ntasks under uncertainty that are relatively easy for humans, such as planning\nactions in stochastic environments. The adoption of LLMs for decision-making is\nimpeded by uncertainty challenges, such as LLM uncertainty and environmental\nuncertainty. LLM uncertainty arises from the stochastic sampling process\ninherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM\nuncertainty through multiple reasoning chains or search trees. However, these\napproaches overlook environmental uncertainty, which leads to poor performance\nin environments with stochastic state transitions. Some recent LDM approaches\ndeal with uncertainty by forecasting the probability of unknown variables.\nHowever, they are not designed for multi-step decision-making tasks that\nrequire interaction with the environment. To address uncertainty in LLM\ndecision-making, we introduce PlanU, an LLM-based planning method that captures\nuncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of\neach node in the MCTS as a quantile distribution, which uses a set of quantiles\nto represent the return distribution. To balance exploration and exploitation\nduring tree search, PlanU introduces an Upper Confidence Bounds with Curiosity\n(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive\nexperiments, we demonstrate the effectiveness of PlanU in LLM-based\ndecision-making tasks under uncertainty.", "AI": {"tldr": "PlanU\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4e2d\u7684\u8282\u70b9\u56de\u62a5\u5efa\u6a21\u4e3a\u5206\u4f4d\u6570\u5206\u5e03\u6765\u6355\u6349\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5f15\u5165\u5e26\u6709\u597d\u5947\u5fc3\u56e0\u5b50\u7684\u4e0a\u7f6e\u4fe1\u754c\u9650\u5206\u6570\u6765\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "motivation": "LLM\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u9762\u4e34LLM\u4e0d\u786e\u5b9a\u6027\u548c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e24\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u53ea\u5173\u6ce8LLM\u4e0d\u786e\u5b9a\u6027\uff0c\u8981\u4e48\u4e0d\u9002\u7528\u4e8e\u9700\u8981\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u591a\u6b65\u51b3\u7b56\u4efb\u52a1\u3002", "method": "\u5c06MCTS\u4e2d\u6bcf\u4e2a\u8282\u70b9\u7684\u56de\u62a5\u5efa\u6a21\u4e3a\u5206\u4f4d\u6570\u5206\u5e03\uff0c\u4f7f\u7528\u4e00\u7ec4\u5206\u4f4d\u6570\u8868\u793a\u56de\u62a5\u5206\u5e03\uff1b\u5f15\u5165UCC\u5206\u6570\u6765\u4f30\u8ba1MCTS\u8282\u70b9\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PlanU\u5728LLM\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "PlanU\u80fd\u591f\u6709\u6548\u5904\u7406LLM\u51b3\u7b56\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18863", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18863", "abs": "https://arxiv.org/abs/2510.18863", "authors": ["Yanlin Wang", "Rongyi Ou", "Yanli Wang", "Mingwei Liu", "Jiachi Chen", "Ensheng Shi", "Xilin Liu", "Yuchi Ma", "Zibin Zheng"], "title": "EffiReasonTrans: RL-Optimized Reasoning for Code Translation", "comment": null, "summary": "Code translation is a crucial task in software development and maintenance.\nWhile recent advancements in large language models (LLMs) have improved\nautomated code translation accuracy, these gains often come at the cost of\nincreased inference latency, hindering real-world development workflows that\ninvolve human-in-the-loop inspection. To address this trade-off, we propose\nEffiReasonTrans, a training framework designed to improve translation accuracy\nwhile balancing inference latency. We first construct a high-quality\nreasoning-augmented dataset by prompting a stronger language model,\nDeepSeek-R1, to generate intermediate reasoning and target translations. Each\n(source code, reasoning, target code) triplet undergoes automated syntax and\nfunctionality checks to ensure reliability. Based on this dataset, we employ a\ntwo-stage training strategy: supervised fine-tuning on reasoning-augmented\nsamples, followed by reinforcement learning to further enhance accuracy and\nbalance inference latency. We evaluate EffiReasonTrans on six translation\npairs. Experimental results show that it consistently improves translation\naccuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while\nreducing the number of generated tokens (up to -19.3%) and lowering inference\nlatency in most cases (up to -29.0%). Ablation studies further confirm the\ncomplementary benefits of the two-stage training framework. Additionally,\nEffiReasonTrans demonstrates improved translation accuracy when integrated into\nagent-based frameworks. Our code and data are available at\nhttps://github.com/DeepSoftwareAnalytics/EffiReasonTrans.", "AI": {"tldr": "EffiReasonTrans\u662f\u4e00\u4e2a\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u63a8\u7406\u589e\u5f3a\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u63d0\u9ad8\u4ee3\u7801\u7ffb\u8bd1\u51c6\u786e\u6027\u7684\u540c\u65f6\u5e73\u8861\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u51c6\u786e\u6027\u63d0\u5347\u4e0e\u63a8\u7406\u5ef6\u8fdf\u589e\u52a0\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u6ee1\u8db3\u5b9e\u9645\u5f00\u53d1\u6d41\u7a0b\u4e2d\u9700\u8981\u4eba\u5de5\u68c0\u67e5\u7684\u9700\u6c42\u3002", "method": "\u9996\u5148\u4f7f\u7528\u66f4\u5f3a\u7684\u8bed\u8a00\u6a21\u578bDeepSeek-R1\u6784\u5efa\u63a8\u7406\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u7136\u540e\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\u63a8\u7406\u589e\u5f3a\u6837\u672c\uff0c\u63a5\u7740\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u4f18\u5316\u51c6\u786e\u6027\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u5728\u516d\u4e2a\u7ffb\u8bd1\u5bf9\u4e0a\uff0c\u7ffb\u8bd1\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\uff08CA\u6700\u9ad8+49.2%\uff0cCodeBLEU\u6700\u9ad8+27.8%\uff09\uff0c\u540c\u65f6\u51cf\u5c11\u751f\u6210token\u6570\u91cf\uff08\u6700\u9ad8-19.3%\uff09\u548c\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff08\u6700\u9ad8-29.0%\uff09\u3002", "conclusion": "EffiReasonTrans\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u4ee3\u7801\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u5728\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u7ffb\u8bd1\u51c6\u786e\u6027\u3002", "topic": "code agent"}}
{"id": "2510.18082", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.18082", "abs": "https://arxiv.org/abs/2510.18082", "authors": ["Donggeon David Oh", "Duy P. Nguyen", "Haimin Hu", "Jaime F. Fisac"], "title": "Provably Optimal Reinforcement Learning under Safety Filtering", "comment": "17 pages, 3 figures", "summary": "Recent advances in reinforcement learning (RL) enable its use on increasingly\ncomplex tasks, but the lack of formal safety guarantees still limits its\napplication in safety-critical settings. A common practical approach is to\naugment the RL policy with a safety filter that overrides unsafe actions to\nprevent failures during both training and deployment. However, safety filtering\nis often perceived as sacrificing performance and hindering the learning\nprocess. We show that this perceived safety-performance tradeoff is not\ninherent and prove, for the first time, that enforcing safety with a\nsufficiently permissive safety filter does not degrade asymptotic performance.\nWe formalize RL safety with a safety-critical Markov decision process (SC-MDP),\nwhich requires categorical, rather than high-probability, avoidance of\ncatastrophic failure states. Additionally, we define an associated filtered MDP\nin which all actions result in safe effects, thanks to a safety filter that is\nconsidered to be a part of the environment. Our main theorem establishes that\n(i) learning in the filtered MDP is safe categorically, (ii) standard RL\nconvergence carries over to the filtered MDP, and (iii) any policy that is\noptimal in the filtered MDP-when executed through the same filter-achieves the\nsame asymptotic return as the best safe policy in the SC-MDP, yielding a\ncomplete separation between safety enforcement and performance optimization. We\nvalidate the theory on Safety Gymnasium with representative tasks and\nconstraints, observing zero violations during training and final performance\nmatching or exceeding unfiltered baselines. Together, these results shed light\non a long-standing question in safety-filtered learning and provide a simple,\nprincipled recipe for safe RL: train and deploy RL policies with the most\npermissive safety filter that is available.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u4f7f\u7528\u8db3\u591f\u5bbd\u677e\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7ea6\u675f\u4e0d\u4f1a\u964d\u4f4e\u6e10\u8fd1\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u548c\u6027\u80fd\u7684\u5b8c\u5168\u5206\u79bb\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7f3a\u4e4f\u6b63\u5f0f\u5b89\u5168\u4fdd\u8bc1\u7684\u95ee\u9898\uff0c\u6d88\u9664\u5b89\u5168\u8fc7\u6ee4\u4f1a\u727a\u7272\u6027\u80fd\u7684\u666e\u904d\u8ba4\u77e5\u3002", "method": "\u63d0\u51fa\u5b89\u5168\u5173\u952e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(SC-MDP)\u548c\u8fc7\u6ee4MDP\u6846\u67b6\uff0c\u8bc1\u660e\u5728\u8fc7\u6ee4MDP\u4e2d\u5b66\u4e60\u65e2\u5b89\u5168\u53c8\u4fdd\u6301\u6807\u51c6RL\u6536\u655b\u6027\u3002", "result": "\u5728Safety Gymnasium\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u96f6\u8fdd\u89c4\u8bad\u7ec3\uff0c\u6700\u7ec8\u6027\u80fd\u5339\u914d\u6216\u8d85\u8fc7\u65e0\u8fc7\u6ee4\u57fa\u7ebf\u3002", "conclusion": "\u4e3a\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7b80\u5355\u539f\u5219\u6027\u65b9\u6cd5\uff1a\u4f7f\u7528\u6700\u5bbd\u677e\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u8fdb\u884c\u8bad\u7ec3\u548c\u90e8\u7f72\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18476", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18476", "abs": "https://arxiv.org/abs/2510.18476", "authors": ["Feifan Xia", "Yuyang Fang", "Defang Li", "Yantong Xie", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang"], "title": "Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents", "comment": null, "summary": "We present a probabilistic intent modeling framework for large language model\n(LLM) agents in multi-turn social dialogue. The framework maintains a belief\ndistribution over a partner's latent intentions, initialized from contextual\npriors and dynamically updated through likelihood estimation after each\nutterance. The evolving distribution provides additional contextual grounding\nfor the policy, enabling adaptive dialogue strategies under uncertainty.\nPreliminary experiments in the SOTOPIA environment show consistent\nimprovements: the proposed framework increases the Overall score by 9.0% on\nSOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and\nslightly surpasses an oracle agent that directly observes partner intentions.\nThese early results suggest that probabilistic intent modeling can contribute\nto the development of socially intelligent LLM agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u591a\u8f6e\u793e\u4ea4\u5bf9\u8bdd\u4e2dLLM\u4ee3\u7406\u7684\u6982\u7387\u610f\u56fe\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u7ef4\u62a4\u5bf9\u4f19\u4f34\u6f5c\u5728\u610f\u56fe\u7684\u4fe1\u5ff5\u5206\u5e03\u6765\u63d0\u5347\u5bf9\u8bdd\u7b56\u7565\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3LLM\u4ee3\u7406\u5728\u591a\u8f6e\u793e\u4ea4\u5bf9\u8bdd\u4e2d\u7406\u89e3\u548c\u9002\u5e94\u4f19\u4f34\u610f\u56fe\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u5efa\u6a21\u548c\u66f4\u65b0\u610f\u56fe\u4fe1\u5ff5\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6982\u7387\u610f\u56fe\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5148\u9a8c\u521d\u59cb\u5316\u4fe1\u5ff5\u5206\u5e03\uff0c\u5e76\u5728\u6bcf\u8f6e\u5bf9\u8bdd\u540e\u901a\u8fc7\u4f3c\u7136\u4f30\u8ba1\u52a8\u6001\u66f4\u65b0\uff0c\u4e3a\u7b56\u7565\u63d0\u4f9b\u989d\u5916\u7684\u4e0a\u4e0b\u6587\u57fa\u7840\u3002", "result": "\u5728SOTOPIA\u73af\u5883\u4e2d\u7684\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728SOTOPIA-All\u4e0a\u63d0\u9ad8\u4e869.0%\u7684\u603b\u4f53\u5f97\u5206\uff0c\u5728SOTOPIA-Hard\u4e0a\u63d0\u9ad8\u4e864.1%\uff0c\u751a\u81f3\u7565\u5fae\u8d85\u8fc7\u4e86\u76f4\u63a5\u89c2\u5bdf\u4f19\u4f34\u610f\u56fe\u7684oracle\u4ee3\u7406\u3002", "conclusion": "\u6982\u7387\u610f\u56fe\u5efa\u6a21\u53ef\u4ee5\u4e3a\u5f00\u53d1\u5177\u6709\u793e\u4f1a\u667a\u80fd\u7684LLM\u4ee3\u7406\u505a\u51fa\u8d21\u732e\u3002", "topic": "agent analysis"}}
{"id": "2510.18483", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18483", "abs": "https://arxiv.org/abs/2510.18483", "authors": ["Haoran Zhang", "Chenhao Zhu", "Sicong Guo", "Hanzhe Guo", "Haiming Li", "Donglin Yu"], "title": "StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking", "comment": null, "summary": "Human players do more than press buttons: they ground what they see on screen\ninto precise keyboard-mouse actions and, when stuck, they seek information\nbefore trying again. We ask whether current vision-language models (VLMs) can\ndo the same. Despite encouraging results under simplified control or tool\nscaffolds, human-like play in a real client - mapping raw screenshots to\ntemporally coherent low-level actions while deciding when to ask for guidance -\nremains an open challenge. We introduce StarBench, a turn-based RPG benchmark\nderived from Honkai: Star Rail that targets these two human-like competencies:\nmultimodal decision-making from pixels to actions and agentic information\nseeking. StarBench standardizes evaluation across eight combat tasks and two\nregimes with shared tasks and metrics: (i) direct control, where agents receive\nonly screenshots and must emit low-level primitives (click and keypress) with\nno semantic hints; and (ii) tool-assisted control, where higher-level intents\ncan be mapped to primitives by detectors and OCR outputs provide optional\ntextualized observations to ease UI grounding. To mirror human practice,\nStarBench also includes an ask-or-act diagnostic that measures whether and when\nagents choose to request brief guidance before proceeding, and how that choice\naffects subsequent performance. We report reference baselines for contemporary\nVLMs and a human reference. Results expose sizable gaps in\nperception-to-control fidelity in the direct regime, while showing that\njudicious information seeking correlates with improved success, establishing\nStarBench as a reproducible yardstick for agentic information seeking and\nmultimodal decision-making in real-client play.", "AI": {"tldr": "StarBench\u662f\u4e00\u4e2a\u57fa\u4e8e\u300a\u5d29\u574f\uff1a\u661f\u7a79\u94c1\u9053\u300b\u7684\u56de\u5408\u5236RPG\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u51b3\u7b56\u548c\u4e3b\u52a8\u4fe1\u606f\u5bfb\u6c42\u80fd\u529b\uff0c\u5305\u542b\u76f4\u63a5\u63a7\u5236\u548c\u5de5\u5177\u8f85\u52a9\u63a7\u5236\u4e24\u79cd\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u5ba2\u6237\u7aef\u73af\u5883\u4e2d\uff0c\u4ece\u539f\u59cb\u622a\u56fe\u5230\u4f4e\u5c42\u7ea7\u52a8\u4f5c\u7684\u6620\u5c04\u4ee5\u53ca\u51b3\u5b9a\u4f55\u65f6\u5bfb\u6c42\u6307\u5bfc\u7684\u80fd\u529b\u4ecd\u7136\u4e0d\u8db3\uff0c\u9700\u8981\u5efa\u7acb\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u8bbe\u8ba1\u4e86\u516b\u4e2a\u6218\u6597\u4efb\u52a1\u548c\u4e24\u79cd\u63a7\u5236\u6a21\u5f0f\uff1a\u76f4\u63a5\u63a7\u5236\uff08\u4ec5\u622a\u56fe\u8f93\u5165\uff0c\u8f93\u51fa\u4f4e\u7ea7\u64cd\u4f5c\uff09\u548c\u5de5\u5177\u8f85\u52a9\u63a7\u5236\uff08\u63d0\u4f9bOCR\u6587\u672c\u5316\u89c2\u5bdf\uff09\u3002\u8fd8\u5305\u62ecask-or-act\u8bca\u65ad\u6765\u8bc4\u4f30\u4fe1\u606f\u5bfb\u6c42\u884c\u4e3a\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u5728\u76f4\u63a5\u63a7\u5236\u6a21\u5f0f\u4e0b\u5b58\u5728\u663e\u8457\u7684\u611f\u77e5-\u63a7\u5236\u5dee\u8ddd\uff0c\u800c\u660e\u667a\u7684\u4fe1\u606f\u5bfb\u6c42\u4e0e\u6539\u8fdb\u7684\u6210\u529f\u7387\u76f8\u5173\u3002", "conclusion": "StarBench\u4e3a\u771f\u5b9e\u5ba2\u6237\u7aef\u6e38\u620f\u4e2d\u7684\u4e3b\u52a8\u4fe1\u606f\u5bfb\u6c42\u548c\u591a\u6a21\u6001\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "topic": "swe benchmark"}}
{"id": "2510.18491", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18491", "abs": "https://arxiv.org/abs/2510.18491", "authors": ["Lianchen Jia", "Chaoyang Li", "Qian Houde", "Tianchi Huang", "Jiangchuan Liu", "Lifeng Sun"], "title": "Crucible: Quantifying the Potential of Control Algorithms through LLM Agents", "comment": "NeurIPS 2025", "summary": "Control algorithms in production environments typically require domain\nexperts to tune their parameters and logic for specific scenarios. However,\nexisting research predominantly focuses on algorithmic performance under ideal\nor default configurations, overlooking the critical aspect of Tuning Potential.\nTo bridge this gap, we introduce Crucible, an agent that employs an LLM-driven,\nmulti-level expert simulation to turn algorithms and defines a formalized\nmetric to quantitatively evaluate their Tuning Potential. We demonstrate\nCrucible's effectiveness across a wide spectrum of case studies, from classic\ncontrol tasks to complex computer systems, and validate its findings in a\nreal-world deployment. Our experimental results reveal that Crucible\nsystematically quantifies the tunable space across different algorithms.\nFurthermore, Crucible provides a new dimension for algorithm analysis and\ndesign, which ultimately leads to performance improvements. Our code is\navailable at https://github.com/thu-media/Crucible.", "AI": {"tldr": "\u63d0\u51fa\u4e86Crucible\u7cfb\u7edf\uff0c\u4f7f\u7528LLM\u9a71\u52a8\u7684\u591a\u7ea7\u4e13\u5bb6\u6a21\u62df\u6765\u8c03\u4f18\u7b97\u6cd5\uff0c\u5e76\u5b9a\u4e49\u4e86\u91cf\u5316\u8bc4\u4f30\u7b97\u6cd5\u8c03\u4f18\u6f5c\u529b\u7684\u6b63\u5f0f\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7b97\u6cd5\u5728\u7406\u60f3\u6216\u9ed8\u8ba4\u914d\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u5ffd\u89c6\u4e86\u8c03\u4f18\u6f5c\u529b\u8fd9\u4e00\u5173\u952e\u65b9\u9762\u3002", "method": "\u91c7\u7528LLM\u9a71\u52a8\u7684\u591a\u7ea7\u4e13\u5bb6\u6a21\u62df\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4e0d\u540c\u573a\u666f\u7684\u4e13\u5bb6\u8c03\u4f18\u8fc7\u7a0b\u6765\u8bc4\u4f30\u7b97\u6cd5\u8c03\u4f18\u6f5c\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aCrucible\u80fd\u591f\u7cfb\u7edf\u91cf\u5316\u4e0d\u540c\u7b97\u6cd5\u7684\u53ef\u8c03\u4f18\u7a7a\u95f4\uff0c\u5e76\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "Crucible\u4e3a\u7b97\u6cd5\u5206\u6790\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u7ef4\u5ea6\uff0c\u6700\u7ec8\u80fd\u591f\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002", "topic": "agent analysis"}}
{"id": "2510.18383", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18383", "abs": "https://arxiv.org/abs/2510.18383", "authors": ["ChangSu Choi", "Hoyun Song", "Dongyeon Kim", "WooHyeon Jung", "Minkyung Cho", "Sunjin Park", "NohHyeob Bae", "Seona Yu", "KyungTae Lim"], "title": "MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models", "comment": null, "summary": "Distilling the tool-using capabilities of large language models (LLMs) into\nsmaller, more efficient small language models (SLMs) is a key challenge for\ntheir practical application. The predominant approach, supervised fine-tuning\n(SFT), suffers from poor generalization as it trains models to imitate a static\nset of teacher trajectories rather than learn a robust methodology. While\nreinforcement learning (RL) offers an alternative, the standard RL using sparse\nrewards fails to effectively guide SLMs, causing them to struggle with\ninefficient exploration and adopt suboptimal strategies. To address these\ndistinct challenges, we propose MENTOR, a framework that synergistically\ncombines RL with teacher-guided distillation. Instead of simple imitation,\nMENTOR employs an RL-based process to learn a more generalizable policy through\nexploration. In addition, to solve the problem of reward sparsity, it uses a\nteacher's reference trajectory to construct a dense, composite teacher-guided\nreward that provides fine-grained guidance. Extensive experiments demonstrate\nthat MENTOR significantly improves the cross-domain generalization and\nstrategic competence of SLMs compared to both SFT and standard sparse-reward RL\nbaselines.", "AI": {"tldr": "MENTOR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u6559\u5e08\u5f15\u5bfc\u84b8\u998f\uff0c\u89e3\u51b3\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u84b8\u998f\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u548c\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u800c\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7531\u4e8e\u5956\u52b1\u7a00\u758f\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u4f4e\u548c\u7b56\u7565\u6b21\u4f18\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u84b8\u998f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u5230\u5c0f\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u63d0\u51faMENTOR\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u6559\u5e08\u5f15\u5bfc\u84b8\u998f\uff1a\u4f7f\u7528RL\u5b66\u4e60\u66f4\u901a\u7528\u7684\u7b56\u7565\uff0c\u540c\u65f6\u5229\u7528\u6559\u5e08\u53c2\u8003\u8f68\u8ff9\u6784\u5efa\u5bc6\u96c6\u7684\u590d\u5408\u6559\u5e08\u5f15\u5bfc\u5956\u52b1\u6765\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u6307\u5bfc\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMENTOR\u76f8\u6bd4\u76d1\u7763\u5fae\u8c03\u548c\u6807\u51c6\u7a00\u758f\u5956\u52b1RL\u57fa\u7ebf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u548c\u7b56\u7565\u80fd\u529b\u3002", "conclusion": "MENTOR\u6846\u67b6\u901a\u8fc7\u534f\u540c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u6559\u5e08\u5f15\u5bfc\u84b8\u998f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u84b8\u998f\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18551", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.18551", "abs": "https://arxiv.org/abs/2510.18551", "authors": ["Yuncheng Hua", "Sion Weatherhead", "Mehdi Jafari", "Hao Xue", "Flora D. Salim"], "title": "SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation", "comment": "11 pages, 1 figure, 2 tables. The paper is under review", "summary": "In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that\ntreats simulator construction asinstance optimization over code within a\ntextual computation graph. Specialized LLM-driven agents are embedded as graph\nnodes, and a workflow manager executes a loss-driven loop: code synthesis ->\nexecution -> evaluation -> code repair. The optimizer performs Textual-Gradient\nDescent (TGD), while human-in-the-loop interaction is reserved for task-spec\nconfirmation, minimizing expert effort and keeping the code itself as the\ntrainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption,\nand Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy.\nBy unifying multi-agent orchestration with a loss-aligned optimization view,\nSOCIA-Nabla converts brittle prompt pipelines into reproducible,\nconstraint-aware simulator code generation that scales across domains and\nsimulation granularities. This work is under review, and we will release the\ncode soon.", "AI": {"tldr": "SOCIA-Nabla\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u6a21\u62df\u5668\u6784\u5efa\u89c6\u4e3a\u6587\u672c\u8ba1\u7b97\u56fe\u4e2d\u7684\u4ee3\u7801\u5b9e\u4f8b\u4f18\u5316\uff0c\u901a\u8fc7\u635f\u5931\u9a71\u52a8\u5faa\u73af\u5b9e\u73b0\u4ee3\u7801\u5408\u6210\u3001\u6267\u884c\u3001\u8bc4\u4f30\u548c\u4fee\u590d\u3002", "motivation": "\u5c06\u8106\u5f31\u7684\u63d0\u793a\u7ba1\u9053\u8f6c\u6362\u4e3a\u53ef\u91cd\u590d\u3001\u7ea6\u675f\u611f\u77e5\u7684\u6a21\u62df\u5668\u4ee3\u7801\u751f\u6210\uff0c\u7edf\u4e00\u591a\u667a\u80fd\u4f53\u7f16\u6392\u4e0e\u635f\u5931\u5bf9\u9f50\u7684\u4f18\u5316\u89c6\u56fe\u3002", "method": "\u5728\u6587\u672c\u8ba1\u7b97\u56fe\u4e2d\u5d4c\u5165\u4e13\u95e8\u7684LLM\u9a71\u52a8\u667a\u80fd\u4f53\u4f5c\u4e3a\u56fe\u8282\u70b9\uff0c\u5de5\u4f5c\u6d41\u7ba1\u7406\u5668\u6267\u884c\u635f\u5931\u9a71\u52a8\u5faa\u73af\uff0c\u4f18\u5316\u5668\u6267\u884c\u6587\u672c\u68af\u5ea6\u4e0b\u964d\uff0c\u4fdd\u7559\u4eba\u673a\u4ea4\u4e92\u7528\u4e8e\u4efb\u52a1\u89c4\u8303\u786e\u8ba4\u3002", "result": "\u5728\u4e09\u4e2aCPS\u4efb\u52a1\uff08\u7528\u6237\u5efa\u6a21\u3001\u53e3\u7f69\u91c7\u7528\u548c\u4e2a\u4eba\u79fb\u52a8\u6027\uff09\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002", "conclusion": "SOCIA-Nabla\u901a\u8fc7\u7edf\u4e00\u591a\u667a\u80fd\u4f53\u7f16\u6392\u4e0e\u635f\u5931\u5bf9\u9f50\u7684\u4f18\u5316\u89c6\u56fe\uff0c\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u548c\u6a21\u62df\u7c92\u5ea6\u7684\u53ef\u6269\u5c55\u6a21\u62df\u5668\u4ee3\u7801\u751f\u6210\u3002", "topic": "code agent"}}
{"id": "2510.18454", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18454", "abs": "https://arxiv.org/abs/2510.18454", "authors": ["Atharvan Dogra", "Soumya Suvra Ghosal", "Ameet Deshpande", "Ashwin Kalyan", "Dinesh Manocha"], "title": "Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models", "comment": null, "summary": "Large language models are increasingly used for creative writing and\nengagement content, raising safety concerns about the outputs. Therefore,\ncasting humor generation as a testbed, this work evaluates how funniness\noptimization in modern LLM pipelines couples with harmful content by jointly\nmeasuring humor, stereotypicality, and toxicity. This is further supplemented\nby analyzing incongruity signals through information-theoretic metrics. Across\nsix models, we observe that harmful outputs receive higher humor scores which\nfurther increase under role-based prompting, indicating a bias amplification\nloop between generators and evaluators. Information-theoretic analyses show\nharmful cues widen predictive uncertainty and surprisingly, can even make\nharmful punchlines more expected for some models, suggesting structural\nembedding in learned humor distributions. External validation on an additional\nsatire-generation task with human perceived funniness judgments shows that LLM\nsatire increases stereotypicality and typically toxicity, including for closed\nmodels. Quantitatively, stereotypical/toxic jokes gain $10-21\\%$ in mean humor\nscore, stereotypical jokes appear $11\\%$ to $28\\%$ more often among the jokes\nmarked funny by LLM-based metric and up to $10\\%$ more often in generations\nperceived as funny by humans.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u5e7d\u9ed8\u751f\u6210\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u53d1\u73b0\u6709\u5bb3\u5185\u5bb9\uff08\u523b\u677f\u5370\u8c61\u548c\u6bd2\u6027\uff09\u4e0e\u5e7d\u9ed8\u8bc4\u5206\u6b63\u76f8\u5173\uff0c\u5728\u89d2\u8272\u63d0\u793a\u4e0b\u8fdb\u4e00\u6b65\u653e\u5927\uff0c\u63ed\u793a\u4e86\u751f\u6210\u5668\u548c\u8bc4\u4f30\u5668\u4e4b\u95f4\u7684\u504f\u89c1\u653e\u5927\u5faa\u73af\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u521b\u610f\u5199\u4f5c\u548c\u4e92\u52a8\u5185\u5bb9\uff0c\u8bc4\u4f30\u5176\u8f93\u51fa\u5b89\u5168\u6027\u53d8\u5f97\u91cd\u8981\u3002\u7814\u7a76\u4ee5\u5e7d\u9ed8\u751f\u6210\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5206\u6790\u8da3\u5473\u6027\u4f18\u5316\u5982\u4f55\u4e0e\u6709\u5bb3\u5185\u5bb9\u8026\u5408\u3002", "method": "\u901a\u8fc7\u8054\u5408\u6d4b\u91cf\u5e7d\u9ed8\u5ea6\u3001\u523b\u677f\u6027\u548c\u6bd2\u6027\u6765\u8bc4\u4f30\u516d\u4e2a\u6a21\u578b\uff0c\u8865\u5145\u4fe1\u606f\u8bba\u6307\u6807\u5206\u6790\u4e0d\u4e00\u81f4\u6027\u4fe1\u53f7\uff0c\u5e76\u5728\u8bbd\u523a\u751f\u6210\u4efb\u52a1\u4e0a\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\u3002", "result": "\u6709\u5bb3\u8f93\u51fa\u83b7\u5f97\u66f4\u9ad8\u7684\u5e7d\u9ed8\u8bc4\u5206\uff0c\u5728\u89d2\u8272\u63d0\u793a\u4e0b\u8fdb\u4e00\u6b65\u589e\u52a0\uff1b\u4fe1\u606f\u8bba\u5206\u6790\u663e\u793a\u6709\u5bb3\u7ebf\u7d22\u6269\u5927\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff1bLLM\u8bbd\u523a\u589e\u52a0\u523b\u677f\u6027\u548c\u6bd2\u6027\uff0c\u523b\u677f/\u6709\u6bd2\u7b11\u8bdd\u7684\u5e7d\u9ed8\u8bc4\u5206\u5e73\u5747\u63d0\u9ad810-21%\u3002", "conclusion": "LLM\u5e7d\u9ed8\u751f\u6210\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u6709\u5bb3\u5185\u5bb9\u4e0e\u5e7d\u9ed8\u611f\u77e5\u6b63\u76f8\u5173\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5b89\u5168\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.18659", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18659", "abs": "https://arxiv.org/abs/2510.18659", "authors": ["Dong Yun", "Marco Schouten", "Dim Papadopoulos"], "title": "Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval", "comment": null, "summary": "User queries in information retrieval are often ambiguous, making it\nchallenging for systems to identify a user's target from a single query. While\nrecent dialogue-based interactive retrieval systems can clarify user intent,\nthey are inefficient as they often lack an explicit strategy to ask the most\ninformative questions. To address this limitation, we propose SherlockLLM, a\ndialogue-driven retrieval framework that learns an optimal questioning strategy\nvia Reinforcement Learning (RL) and avoids the need for large-scale annotated\ndialogue data. In our framework, an agent is trained to generate a sequence of\nbinary questions to efficiently narrow down the search space. To validate our\napproach, we introduce a benchmark with both structured and unstructured tasks.\nExperimental results show that SherlockLLM is a robust and efficient solution.\nOn the structured tasks, its performance matches strong baselines and\napproaches the theoretical optimal defined by binary search. On the challenging\nunstructured task, our agent significantly outperforms these baselines,\nshowcasing its ability to learn a highly effective information-seeking dialogue\npolicy.", "AI": {"tldr": "SherlockLLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u8bdd\u9a71\u52a8\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u4e8c\u8fdb\u5236\u95ee\u9898\u5e8f\u5217\u6765\u9ad8\u6548\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u6807\u6ce8\u5bf9\u8bdd\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u4fe1\u606f\u68c0\u7d22\u4e2d\u7528\u6237\u67e5\u8be2\u6a21\u7cca\u6027\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u5bf9\u8bdd\u5f0f\u68c0\u7d22\u7cfb\u7edf\u7f3a\u4e4f\u660e\u786e\u7684\u63d0\u95ee\u7b56\u7565\u6765\u83b7\u53d6\u6700\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4ee3\u7406\u751f\u6210\u4e8c\u8fdb\u5236\u95ee\u9898\u5e8f\u5217\uff0c\u901a\u8fc7\u4f18\u5316\u63d0\u95ee\u7b56\u7565\u6765\u9ad8\u6548\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u5728\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\u6027\u80fd\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684\u4e8c\u5206\u641c\u7d22\uff0c\u5728\u975e\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SherlockLLM\u662f\u4e00\u4e2a\u9c81\u68d2\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b66\u4e60\u9ad8\u5ea6\u6709\u6548\u7684\u4fe1\u606f\u5bfb\u6c42\u5bf9\u8bdd\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18263", "categories": ["cs.LG", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.18263", "abs": "https://arxiv.org/abs/2510.18263", "authors": ["Ziwei Huang", "Ying Shu", "Hao Fang", "Quanyu Long", "Wenya Wang", "Qiushi Guo", "Tiezheng Ge", "Leilei Gan"], "title": "From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation", "comment": null, "summary": "Subject-driven image generation models face a fundamental trade-off between\nidentity preservation (fidelity) and prompt adherence (editability). While\nonline reinforcement learning (RL), specifically GPRO, offers a promising\nsolution, we find that a naive application of GRPO leads to competitive\ndegradation, as the simple linear aggregation of rewards with static weights\ncauses conflicting gradient signals and a misalignment with the temporal\ndynamics of the diffusion process. To overcome these limitations, we propose\nCustomized-GRPO, a novel framework featuring two key innovations: (i)\nSynergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly\npenalizes conflicted reward signals and amplifies synergistic ones, providing a\nsharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW),\nwhich aligns the optimization pressure with the model's temporal dynamics by\nprioritizing prompt-following in the early, identity preservation in the later.\nExtensive experiments demonstrate that our method significantly outperforms\nnaive GRPO baselines, successfully mitigating competitive degradation. Our\nmodel achieves a superior balance, generating images that both preserve key\nidentity features and accurately adhere to complex textual prompts.", "AI": {"tldr": "\u63d0\u51faCustomized-GRPO\u6846\u67b6\uff0c\u901a\u8fc7Synergy-Aware Reward Shaping\u548cTime-Aware Dynamic Weighting\u89e3\u51b3\u4e3b\u9898\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u4e2d\u8eab\u4efd\u4fdd\u6301\u4e0e\u63d0\u793a\u9075\u5faa\u7684\u6743\u8861\u95ee\u9898\uff0c\u907f\u514d\u7ade\u4e89\u6027\u9000\u5316\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7b80\u5355\u7ebf\u6027\u5956\u52b1\u805a\u5408\u5bfc\u81f4\u7684\u51b2\u7a81\u68af\u5ea6\u4fe1\u53f7\u548c\u4e0e\u6269\u6563\u8fc7\u7a0b\u65f6\u95f4\u52a8\u6001\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCustomized-GRPO\u6846\u67b6\uff0c\u5305\u542bSARS\uff08\u975e\u7ebf\u6027\u60e9\u7f5a\u51b2\u7a81\u5956\u52b1\u4fe1\u53f7\u5e76\u653e\u5927\u534f\u540c\u4fe1\u53f7\uff09\u548cTDW\uff08\u6839\u636e\u65f6\u95f4\u52a8\u6001\u8c03\u6574\u4f18\u5316\u538b\u529b\uff09\u3002", "result": "\u663e\u8457\u4f18\u4e8e\u6734\u7d20GRPO\u57fa\u7ebf\uff0c\u6210\u529f\u7f13\u89e3\u7ade\u4e89\u6027\u9000\u5316\uff0c\u5728\u4fdd\u6301\u5173\u952e\u8eab\u4efd\u7279\u5f81\u7684\u540c\u65f6\u51c6\u786e\u9075\u5faa\u590d\u6742\u6587\u672c\u63d0\u793a\u3002", "conclusion": "Customized-GRPO\u80fd\u591f\u66f4\u597d\u5730\u5e73\u8861\u8eab\u4efd\u4fdd\u6301\u548c\u63d0\u793a\u9075\u5faa\uff0c\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18340", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18340", "abs": "https://arxiv.org/abs/2510.18340", "authors": ["Jongmin Lee", "Ernest K. Ryu"], "title": "Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs", "comment": null, "summary": "The classical policy gradient method is the theoretical and conceptual\nfoundation of modern policy-based reinforcement learning (RL) algorithms. Most\nrigorous analyses of such methods, particularly those establishing convergence\nguarantees, assume a discount factor $\\gamma < 1$. In contrast, however, a\nrecent line of work on policy-based RL for large language models uses the\nundiscounted total-reward setting with $\\gamma = 1$, rendering much of the\nexisting theory inapplicable. In this paper, we provide analyses of the policy\ngradient method for undiscounted expected total-reward infinite-horizon MDPs\nbased on two key insights: (i) the classification of the MDP states into\nrecurrent and transient states is invariant over the set of policies that\nassign strictly positive probability to every action (as is typical in deep RL\nmodels employing a softmax output layer) and (ii) the classical state\nvisitation measure (which may be ill-defined when $\\gamma = 1$) can be replaced\nwith a new object that we call the transient visitation measure.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u65e0\u6298\u6263\u603b\u5956\u52b1\u8bbe\u7f6e\u4e0b\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u9488\u5bf9\u03b3=1\u7684\u60c5\u51b5\u5efa\u7acb\u4e86\u6536\u655b\u7406\u8bba\uff0c\u901a\u8fc7\u5f15\u5165\u77ac\u6001\u8bbf\u95ee\u91cf\u5ea6\u66ff\u4ee3\u4f20\u7edf\u72b6\u6001\u8bbf\u95ee\u91cf\u5ea6\u3002", "motivation": "\u73b0\u4ee3\u57fa\u4e8e\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff09\u4f7f\u7528\u65e0\u6298\u6263\u603b\u5956\u52b1\u8bbe\u7f6e\uff08\u03b3=1\uff09\uff0c\u4f46\u73b0\u6709\u7406\u8bba\u5927\u591a\u5047\u8bbe\u03b3<1\uff0c\u5bfc\u81f4\u7406\u8bba\u4e0d\u9002\u7528\u3002", "method": "\u57fa\u4e8e\u4e24\u4e2a\u5173\u952e\u6d1e\u5bdf\uff1a(1) MDP\u72b6\u6001\u5728\u4e25\u683c\u6b63\u6982\u7387\u7b56\u7565\u4e0b\u7684\u5faa\u73af-\u77ac\u6001\u5206\u7c7b\u4e0d\u53d8\u6027\uff1b(2) \u7528\u77ac\u6001\u8bbf\u95ee\u91cf\u5ea6\u66ff\u4ee3\u4f20\u7edf\u72b6\u6001\u8bbf\u95ee\u91cf\u5ea6\u3002", "result": "\u4e3a\u65e0\u6298\u6263\u671f\u671b\u603b\u5956\u52b1\u65e0\u9650\u65f6\u57dfMDPs\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6846\u67b6\u3002", "conclusion": "\u5efa\u7acb\u4e86\u03b3=1\u60c5\u51b5\u4e0b\u7684\u7b56\u7565\u68af\u5ea6\u7406\u8bba\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7406\u8bba\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18779", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18779", "abs": "https://arxiv.org/abs/2510.18779", "authors": ["Zizheng Zhan", "Ken Deng", "Xiaojiang Zhang", "Jinghui Wang", "Huaixi Tang", "Zhiyi Lai", "Haoyang Huang", "Wen Xiang", "Kun Wu", "Wenhao Zhuang", "Minglei Zhang", "Shaojie Wang", "Shangpeng Yan", "Kepeng Lei", "Zongxian Feng", "Huiming Wang", "Zheng Lin", "Mengtong Li", "Mengfei Xie", "Yinghan Cui", "Xuxing Chen", "Chao Wang", "Weihao Li", "Wenqiang Zhu", "Jiarong Zhang", "Jingxuan Xu", "Songwei Yu", "Yifan Yao", "Xinping Lei", "Han Li", "Junqi Xiong", "Zuchen Gao", "Dailin Li", "Haimo Li", "Jiaheng Liu", "Yuqun Zhang", "Junyi Peng", "Haotian Zhang", "Bin Chen"], "title": "KAT-Coder Technical Report", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled progress in\nagentic coding, where models autonomously reason, plan, and act within\ninteractive software development workflows. However, bridging the gap between\nstatic text-based training and dynamic real-world agentic execution remains a\ncore challenge. In this technical report, we present KAT-Coder, a large-scale\nagentic code model trained through a multi-stage curriculum encompassing\nMid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning\n(RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances\nreasoning, planning, and reflection capabilities through a corpus of real\nsoftware engineering data and synthetic agentic interactions. The SFT stage\nconstructs a million-sample dataset balancing twenty programming languages, ten\ndevelopment contexts, and ten task archetypes. The RFT stage introduces a novel\nmulti-ground-truth reward formulation for stable and sample-efficient policy\noptimization. Finally, the Reinforcement-to-Deployment phase adapts the model\nto production-grade IDE environments using Error-Masked SFT and Tree-Structured\nTrajectory Training. In summary, these stages enable KAT-Coder to achieve\nrobust tool-use reliability, instruction alignment, and long-context reasoning,\nforming a deployable foundation for real-world intelligent coding agents. Our\nKAT series 32B model, KAT-Dev, has been open-sourced on\nhttps://huggingface.co/Kwaipilot/KAT-Dev.", "AI": {"tldr": "KAT-Coder\u662f\u4e00\u4e2a\u901a\u8fc7\u591a\u9636\u6bb5\u8bfe\u7a0b\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u667a\u80fd\u4ee3\u7801\u6a21\u578b\uff0c\u5305\u62ec\u4e2d\u671f\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5fae\u8c03\u548c\u90e8\u7f72\u9002\u5e94\u9636\u6bb5\uff0c\u65e8\u5728\u89e3\u51b3\u9759\u6001\u6587\u672c\u8bad\u7ec3\u4e0e\u52a8\u6001\u667a\u80fd\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u667a\u80fd\u7f16\u7801\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u9759\u6001\u6587\u672c\u8bad\u7ec3\u4e0e\u52a8\u6001\u5b9e\u9645\u6267\u884c\u4e4b\u95f4\u4ecd\u5b58\u5728\u6838\u5fc3\u6311\u6218\uff0c\u9700\u8981\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\uff1a\u4e2d\u671f\u8bad\u7ec3\u589e\u5f3a\u63a8\u7406\u3001\u89c4\u5212\u548c\u53cd\u601d\u80fd\u529b\uff1b\u76d1\u7763\u5fae\u8c03\u6784\u5efa\u5e73\u8861\u591a\u7f16\u7a0b\u8bed\u8a00\u548c\u4efb\u52a1\u7c7b\u578b\u7684\u6570\u636e\u96c6\uff1b\u5f3a\u5316\u5fae\u8c03\u5f15\u5165\u591a\u771f\u5b9e\u5956\u52b1\u516c\u5f0f\uff1b\u90e8\u7f72\u9002\u5e94\u9636\u6bb5\u4f7f\u7528\u9519\u8bef\u63a9\u7801\u76d1\u7763\u5fae\u8c03\u548c\u6811\u7ed3\u6784\u8f68\u8ff9\u8bad\u7ec3\u3002", "result": "KAT-Coder\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u5de5\u5177\u4f7f\u7528\u53ef\u9760\u6027\u3001\u6307\u4ee4\u5bf9\u9f50\u548c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u5f62\u6210\u4e86\u53ef\u90e8\u7f72\u7684\u667a\u80fd\u7f16\u7801\u4ee3\u7406\u57fa\u7840\u300232B\u6a21\u578bKAT-Dev\u5df2\u5728HuggingFace\u5f00\u6e90\u3002", "conclusion": "\u591a\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\u4f7fKAT-Coder\u80fd\u591f\u6709\u6548\u89e3\u51b3\u667a\u80fd\u7f16\u7801\u4e2d\u7684\u8bad\u7ec3-\u6267\u884c\u5dee\u8ddd\u95ee\u9898\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u667a\u80fd\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u90e8\u7f72\u7684\u57fa\u7840\u3002", "topic": "code agent"}}
{"id": "2510.18798", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18798", "abs": "https://arxiv.org/abs/2510.18798", "authors": ["Guanzhong He", "Zhen Yang", "Jinxin Liu", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection", "comment": null, "summary": "Search agents have achieved significant advancements in enabling intelligent\ninformation retrieval and decision-making within interactive environments.\nAlthough reinforcement learning has been employed to train agentic models\ncapable of more dynamic interactive retrieval, existing methods are limited by\nshallow tool-use depth and the accumulation of errors over multiple iterative\ninteractions. In this paper, we present WebSeer, a more intelligent search\nagent trained via reinforcement learning enhanced with a self-reflection\nmechanism. Specifically, we construct a large dataset annotated with reflection\npatterns and design a two-stage training framework that unifies cold start and\nreinforcement learning within the self-reflection paradigm for real-world\nweb-based environments, which enables the model to generate longer and more\nreflective tool-use trajectories. Our approach substantially extends tool-use\nchains and improves answer accuracy. Using a single 14B model, we achieve\nstate-of-the-art results on HotpotQA and SimpleQA, with accuracies of 72.3% and\n90.0%, respectively, and demonstrate strong generalization to\nout-of-distribution datasets. The code is available at\nhttps://github.com/99hgz/WebSeer", "AI": {"tldr": "WebSeer\u662f\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3001\u5177\u6709\u81ea\u6211\u53cd\u601d\u673a\u5236\u7684\u667a\u80fd\u641c\u7d22\u4ee3\u7406\uff0c\u80fd\u591f\u5728\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u751f\u6210\u66f4\u957f\u7684\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u7b54\u6848\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u4ee3\u7406\u5b58\u5728\u5de5\u5177\u4f7f\u7528\u6df1\u5ea6\u6d45\u548c\u591a\u8f6e\u4ea4\u4e92\u4e2d\u9519\u8bef\u7d2f\u79ef\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u641c\u7d22\u4ee3\u7406\u6765\u6539\u5584\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5e26\u6709\u53cd\u601d\u6a21\u5f0f\u6807\u6ce8\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff08\u51b7\u542f\u52a8\u548c\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5728\u81ea\u6211\u53cd\u601d\u8303\u5f0f\u4e0b\u7edf\u4e00\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u80fd\u751f\u6210\u66f4\u957f\u3001\u66f4\u5177\u53cd\u601d\u6027\u7684\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\u3002", "result": "\u4f7f\u7528\u5355\u4e2a14B\u6a21\u578b\u5728HotpotQA\u548cSimpleQA\u4e0a\u5206\u522b\u8fbe\u523072.3%\u548c90.0%\u7684\u51c6\u786e\u7387\uff0c\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5e76\u5728\u5206\u5e03\u5916\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "WebSeer\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u673a\u5236\u663e\u8457\u6269\u5c55\u4e86\u5de5\u5177\u4f7f\u7528\u94fe\u5e76\u63d0\u9ad8\u4e86\u7b54\u6848\u51c6\u786e\u6027\uff0c\u4e3a\u667a\u80fd\u641c\u7d22\u4ee3\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18866", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.18866", "abs": "https://arxiv.org/abs/2510.18866", "authors": ["Jizhan Fang", "Xinle Deng", "Haoming Xu", "Ziyan Jiang", "Yuqi Tang", "Ziwen Xu", "Shumin Deng", "Yunzhi Yao", "Mengru Wang", "Shuofei Qiao", "Huajun Chen", "Ningyu Zhang"], "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation", "comment": "Work in progress", "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle\nto effectively leverage historical interaction information in dynamic and\ncomplex environments. Memory systems enable LLMs to move beyond stateless\ninteractions by introducing persistent information storage, retrieval, and\nutilization mechanisms. However, existing memory systems often introduce\nsubstantial time and computational overhead. To this end, we introduce a new\nmemory system called LightMem, which strikes a balance between the performance\nand efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of\nhuman memory, LightMem organizes memory into three complementary stages. First,\ncognition-inspired sensory memory rapidly filters irrelevant information\nthrough lightweight compression and groups information according to their\ntopics. Next, topic-aware short-term memory consolidates these topic-based\ngroups, organizing and summarizing content for more structured access. Finally,\nlong-term memory with sleep-time update employs an offline procedure that\ndecouples consolidation from online inference. Experiments on LongMemEval with\nGPT and Qwen backbones show that LightMem outperforms strong baselines in\naccuracy (up to 10.9% gains) while reducing token usage by up to 117x, API\ncalls by up to 159x, and runtime by over 12x. The code is available at\nhttps://github.com/zjunlp/LightMem.", "AI": {"tldr": "LightMem\u662f\u4e00\u4e2a\u5e73\u8861\u6027\u80fd\u4e0e\u6548\u7387\u7684LLM\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u57fa\u4e8e\u4eba\u7c7b\u8bb0\u5fc6\u6a21\u578b\u6784\u5efa\u4e09\u9636\u6bb5\u8bb0\u5fc6\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709LLM\u8bb0\u5fc6\u7cfb\u7edf\u5728\u52a8\u6001\u590d\u6742\u73af\u5883\u4e2d\u5b58\u5728\u663e\u8457\u7684\u65f6\u95f4\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bb0\u5fc6\u673a\u5236\u6765\u5229\u7528\u5386\u53f2\u4ea4\u4e92\u4fe1\u606f\u3002", "method": "\u57fa\u4e8eAtkinson-Shiffrin\u4eba\u7c7b\u8bb0\u5fc6\u6a21\u578b\uff0c\u6784\u5efa\u4e09\u9636\u6bb5\u8bb0\u5fc6\u67b6\u6784\uff1a\u8ba4\u77e5\u542f\u53d1\u7684\u611f\u5b98\u8bb0\u5fc6\u5feb\u901f\u8fc7\u6ee4\u4fe1\u606f\uff0c\u4e3b\u9898\u611f\u77e5\u7684\u77ed\u671f\u8bb0\u5fc6\u7ec4\u7ec7\u5185\u5bb9\uff0c\u7761\u7720\u65f6\u95f4\u66f4\u65b0\u7684\u957f\u671f\u8bb0\u5fc6\u79bb\u7ebf\u6574\u5408\u3002", "result": "\u5728LongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLightMem\u76f8\u6bd4\u57fa\u7ebf\u51c6\u786e\u7387\u63d0\u5347\u8fbe10.9%\uff0c\u540c\u65f6token\u4f7f\u7528\u51cf\u5c11117\u500d\uff0cAPI\u8c03\u7528\u51cf\u5c11159\u500d\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1112\u500d\u4ee5\u4e0a\u3002", "conclusion": "LightMem\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bb0\u5fc6\u7cfb\u7edf\u7684\u6548\u7387\uff0c\u4e3aLLM\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.18478", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18478", "abs": "https://arxiv.org/abs/2510.18478", "authors": ["Daniel Bethell", "Simos Gerasimou", "Radu Calinescu", "Calum Imrie"], "title": "Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation", "comment": null, "summary": "Ensuring the safe exploration of reinforcement learning (RL) agents is\ncritical for deployment in real-world systems. Yet existing approaches struggle\nto strike the right balance: methods that tightly enforce safety often cripple\ntask performance, while those that prioritize reward leave safety constraints\nfrequently violated, producing diffuse cost landscapes that flatten gradients\nand stall policy improvement. We introduce the Uncertain Safety Critic (USC), a\nnovel approach that integrates uncertainty-aware modulation and refinement into\ncritic training. By concentrating conservatism in uncertain and costly regions\nwhile preserving sharp gradients in safe areas, USC enables policies to achieve\neffective reward-safety trade-offs. Extensive experiments show that USC reduces\nsafety violations by approximately 40% while maintaining competitive or higher\nrewards, and reduces the error between predicted and true cost gradients by\napproximately 83%, breaking the prevailing trade-off between safety and\nperformance and paving the way for scalable safe RL.", "AI": {"tldr": "\u63d0\u51faUncertain Safety Critic (USC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8c03\u5236\u548c\u7cbe\u70bc\u6765\u6539\u8fdbcritic\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5b89\u5168\u8fdd\u89c4\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5b89\u5168\u63a2\u7d22\u65b9\u9762\u5b58\u5728\u5e73\u8861\u95ee\u9898\uff1a\u4e25\u683c\u7684\u5b89\u5168\u7ea6\u675f\u4f1a\u635f\u5bb3\u4efb\u52a1\u6027\u80fd\uff0c\u800c\u4f18\u5148\u5956\u52b1\u7684\u65b9\u6cd5\u5219\u9891\u7e41\u8fdd\u53cd\u5b89\u5168\u7ea6\u675f\uff0c\u4ea7\u751f\u5e73\u5766\u7684\u6210\u672c\u68af\u5ea6\u666f\u89c2\u963b\u788d\u7b56\u7565\u6539\u8fdb\u3002", "method": "USC\u65b9\u6cd5\u5c06\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8c03\u5236\u548c\u7cbe\u70bc\u96c6\u6210\u5230critic\u8bad\u7ec3\u4e2d\uff0c\u5728\u4e0d\u786e\u5b9a\u548c\u9ad8\u6210\u672c\u533a\u57df\u96c6\u4e2d\u4fdd\u5b88\u6027\uff0c\u540c\u65f6\u5728\u5b89\u5168\u533a\u57df\u4fdd\u6301\u9510\u5229\u68af\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aUSC\u5c06\u5b89\u5168\u8fdd\u89c4\u51cf\u5c11\u7ea640%\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u6216\u66f4\u9ad8\u7684\u5956\u52b1\uff0c\u9884\u6d4b\u6210\u672c\u68af\u5ea6\u4e0e\u771f\u5b9e\u6210\u672c\u68af\u5ea6\u4e4b\u95f4\u7684\u8bef\u5dee\u51cf\u5c11\u7ea683%\u3002", "conclusion": "USC\u6253\u7834\u4e86\u5b89\u5168\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u4f20\u7edf\u6743\u8861\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18871", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18871", "abs": "https://arxiv.org/abs/2510.18871", "authors": ["Akshat Gupta", "Jay Yeung", "Gopala Anumanchipalli", "Anna Ivanova"], "title": "How Do LLMs Use Their Depth?", "comment": null, "summary": "Growing evidence suggests that large language models do not use their depth\nuniformly, yet we still lack a fine-grained understanding of their layer-wise\nprediction dynamics. In this paper, we trace the intermediate representations\nof several open-weight models during inference and reveal a structured and\nnuanced use of depth. Specifically, we propose a \"Guess-then-Refine\" framework\nthat explains how LLMs internally structure their computations to make\npredictions. We first show that the top-ranked predictions in early LLM layers\nare composed primarily of high-frequency tokens, which act as statistical\nguesses proposed by the model early on due to the lack of appropriate\ncontextual information. As contextual information develops deeper into the\nmodel, these initial guesses get refined into contextually appropriate tokens.\nEven high-frequency token predictions from early layers get refined >70% of the\ntime, indicating that correct token prediction is not \"one-and-done\". We then\ngo beyond frequency-based prediction to examine the dynamic usage of layer\ndepth across three case studies. (i) Part-of-speech analysis shows that\nfunction words are, on average, the earliest to be predicted correctly. (ii)\nFact recall task analysis shows that, in a multi-token answer, the first token\nrequires more computational depth than the rest. (iii) Multiple-choice task\nanalysis shows that the model identifies the format of the response within the\nfirst half of the layers, but finalizes its response only toward the end.\nTogether, our results provide a detailed view of depth usage in LLMs, shedding\nlight on the layer-by-layer computations that underlie successful predictions\nand providing insights for future works to improve computational efficiency in\ntransformer-based models.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86LLM\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5206\u5c42\u9884\u6d4b\u52a8\u6001\uff0c\u63d0\u51fa\u4e86\"\u731c\u6d4b-\u7cbe\u70bc\"\u6846\u67b6\uff0c\u663e\u793a\u65e9\u671f\u5c42\u4ea7\u751f\u9ad8\u9891\u8bcd\u4f5c\u4e3a\u7edf\u8ba1\u731c\u6d4b\uff0c\u540e\u671f\u5c42\u57fa\u4e8e\u4e0a\u4e0b\u6587\u4fe1\u606f\u8fdb\u884c\u7cbe\u70bc\u3002", "motivation": "\u7406\u89e3LLM\u5982\u4f55\u5728\u4e0d\u540c\u5c42\u95f4\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u63ed\u793a\u5176\u6df1\u5ea6\u4f7f\u7528\u7684\u7ed3\u6784\u5316\u6a21\u5f0f\uff0c\u4e3a\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u63d0\u4f9b\u6d1e\u89c1\u3002", "method": "\u8ffd\u8e2a\u591a\u4e2a\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u5206\u6790\u4e0d\u540c\u5c42\u7ea7\u7684\u9884\u6d4b\u884c\u4e3a\uff0c\u5305\u62ec\u8bcd\u9891\u5206\u6790\u3001\u8bcd\u6027\u5206\u6790\u3001\u4e8b\u5b9e\u56de\u5fc6\u548c\u591a\u9009\u4efb\u52a1\u5206\u6790\u3002", "result": "\u53d1\u73b0\u65e9\u671f\u5c42\u4e3b\u8981\u9884\u6d4b\u9ad8\u9891\u8bcd\u4f5c\u4e3a\u7edf\u8ba1\u731c\u6d4b\uff0c\u8fd9\u4e9b\u731c\u6d4b\u5728\u540e\u671f\u5c42\u88ab\u7cbe\u70bc\uff08>70%\uff09\uff1b\u529f\u80fd\u8bcd\u6700\u65e9\u88ab\u6b63\u786e\u9884\u6d4b\uff1b\u591a\u8bcd\u7b54\u6848\u4e2d\u9996\u4e2a\u8bcd\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u6df1\u5ea6\uff1b\u6a21\u578b\u5728\u524d\u534a\u5c42\u8bc6\u522b\u54cd\u5e94\u683c\u5f0f\uff0c\u540e\u534a\u5c42\u786e\u5b9a\u6700\u7ec8\u7b54\u6848\u3002", "conclusion": "LLM\u7684\u6df1\u5ea6\u4f7f\u7528\u5177\u6709\u7ed3\u6784\u5316\u7279\u5f81\uff0c\u65e9\u671f\u5c42\u8fdb\u884c\u7edf\u8ba1\u731c\u6d4b\uff0c\u540e\u671f\u5c42\u8fdb\u884c\u4e0a\u4e0b\u6587\u7cbe\u70bc\uff0c\u8fd9\u4e3a\u4f18\u5316transformer\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002", "topic": "agent analysis"}}
{"id": "2510.18485", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18485", "abs": "https://arxiv.org/abs/2510.18485", "authors": ["Daniel Bethell", "Simos Gerasimou", "Radu Calinescu", "Calum Imrie"], "title": "Learning to Navigate Under Imperfect Perception: Conformalised Segmentation for Safe Reinforcement Learning", "comment": null, "summary": "Reliable navigation in safety-critical environments requires both accurate\nhazard perception and principled uncertainty handling to strengthen downstream\nsafety handling. Despite the effectiveness of existing approaches, they assume\nperfect hazard detection capabilities, while uncertainty-aware perception\napproaches lack finite-sample guarantees. We present COPPOL, a conformal-driven\nperception-to-policy learning approach that integrates distribution-free,\nfinite-sample safety guarantees into semantic segmentation, yielding calibrated\nhazard maps with rigorous bounds for missed detections. These maps induce\nrisk-aware cost fields for downstream RL planning. Across two satellite-derived\nbenchmarks, COPPOL increases hazard coverage (up to 6x) compared to comparative\nbaselines, achieving near-complete detection of unsafe regions while reducing\nhazardous violations during navigation (up to approx 50%). More importantly,\nour approach remains robust to distributional shift, preserving both safety and\nefficiency.", "AI": {"tldr": "COPPOL\u662f\u4e00\u79cd\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684\u611f\u77e5\u5230\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u6709\u9650\u6837\u672c\u5b89\u5168\u4fdd\u8bc1\uff0c\u751f\u6210\u5e26\u6709\u4e25\u683c\u6f0f\u68c0\u754c\u9650\u7684\u6821\u51c6\u5371\u9669\u5730\u56fe\uff0c\u7528\u4e8e\u4e0b\u6e38\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u5b8c\u7f8e\u7684\u5371\u9669\u68c0\u6d4b\u80fd\u529b\uff0c\u800c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u65b9\u6cd5\u7f3a\u4e4f\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u9700\u8981\u540c\u65f6\u5b9e\u73b0\u51c6\u786e\u7684\u5371\u9669\u611f\u77e5\u548c\u539f\u5219\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u3002", "method": "\u96c6\u6210\u5206\u5e03\u65e0\u5173\u7684\u6709\u9650\u6837\u672c\u5b89\u5168\u4fdd\u8bc1\u5230\u8bed\u4e49\u5206\u5272\u4e2d\uff0c\u751f\u6210\u6821\u51c6\u7684\u5371\u9669\u5730\u56fe\uff0c\u8fd9\u4e9b\u5730\u56fe\u4e3a\u4e0b\u6e38RL\u89c4\u5212\u63d0\u4f9b\u98ce\u9669\u611f\u77e5\u6210\u672c\u573a\u3002", "result": "\u5728\u4e24\u4e2a\u536b\u661f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOPPOL\u5c06\u5371\u9669\u8986\u76d6\u7387\u63d0\u9ad8\u6700\u591a6\u500d\uff0c\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u5168\u7684\u4e0d\u5b89\u5168\u533a\u57df\u68c0\u6d4b\uff0c\u540c\u65f6\u51cf\u5c11\u5bfc\u822a\u4e2d\u7684\u5371\u9669\u8fdd\u89c4\u7ea650%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u5206\u5e03\u504f\u79fb\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18541", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.18541", "abs": "https://arxiv.org/abs/2510.18541", "authors": ["Giovanni De Muri", "Mark Vero", "Robin Staab", "Martin Vechev"], "title": "Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation", "comment": null, "summary": "LLMs are often used by downstream users as teacher models for knowledge\ndistillation, compressing their capabilities into memory-efficient models.\nHowever, as these teacher models may stem from untrusted parties, distillation\ncan raise unexpected security risks. In this paper, we investigate the security\nimplications of knowledge distillation from backdoored teacher models. First,\nwe show that prior backdoors mostly do not transfer onto student models. Our\nkey insight is that this is because existing LLM backdooring methods choose\ntrigger tokens that rarely occur in usual contexts. We argue that this\nunderestimates the security risks of knowledge distillation and introduce a new\nbackdooring technique, T-MTB, that enables the construction and study of\ntransferable backdoors. T-MTB carefully constructs a composite backdoor\ntrigger, made up of several specific tokens that often occur individually in\nanticipated distillation datasets. As such, the poisoned teacher remains\nstealthy, while during distillation the individual presence of these tokens\nprovides enough signal for the backdoor to transfer onto the student. Using\nT-MTB, we demonstrate and extensively study the security risks of transferable\nbackdoors across two attack scenarios, jailbreaking and content modulation, and\nacross four model families of LLMs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4ece\u540e\u95e8\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u8f6c\u79fb\u540e\u95e8\u6280\u672fT-MTB\uff0c\u8bc1\u660e\u4e86\u540e\u95e8\u53ef\u4ee5\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u4f20\u9012\u5230\u5b66\u751f\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u6559\u5e08\u6a21\u578b\u53ef\u80fd\u6765\u81ea\u4e0d\u53ef\u4fe1\u65b9\uff0c\u77e5\u8bc6\u84b8\u998f\u53ef\u80fd\u5e26\u6765\u610f\u60f3\u4e0d\u5230\u7684\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709LLM\u540e\u95e8\u65b9\u6cd5\u9009\u62e9\u7684\u89e6\u53d1\u8bcd\u5728\u6b63\u5e38\u4e0a\u4e0b\u6587\u4e2d\u5f88\u5c11\u51fa\u73b0\uff0c\u4f4e\u4f30\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u63d0\u51faT-MTB\u6280\u672f\uff0c\u6784\u5efa\u7531\u591a\u4e2a\u7279\u5b9a\u6807\u8bb0\u7ec4\u6210\u7684\u590d\u5408\u540e\u95e8\u89e6\u53d1\u5668\uff0c\u8fd9\u4e9b\u6807\u8bb0\u5728\u9884\u671f\u7684\u84b8\u998f\u6570\u636e\u96c6\u4e2d\u7ecf\u5e38\u5355\u72ec\u51fa\u73b0\uff0c\u4f7f\u540e\u95e8\u80fd\u591f\u4f20\u9012\u5230\u5b66\u751f\u6a21\u578b\u3002", "result": "\u4f7f\u7528T-MTB\u5728\u4e24\u4e2a\u653b\u51fb\u573a\u666f\uff08\u8d8a\u72f1\u548c\u5185\u5bb9\u8c03\u5236\uff09\u548c\u56db\u4e2aLLM\u6a21\u578b\u5bb6\u65cf\u4e2d\u8bc1\u660e\u4e86\u53ef\u8f6c\u79fb\u540e\u95e8\u7684\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u98ce\u9669\uff0c\u540e\u95e8\u53ef\u4ee5\u4ece\u6559\u5e08\u6a21\u578b\u4f20\u9012\u5230\u5b66\u751f\u6a21\u578b\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u77e5\u8bc6\u84b8\u998f\u7684\u5b89\u5168\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.18874", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18874", "abs": "https://arxiv.org/abs/2510.18874", "authors": ["Howard Chen", "Noam Razin", "Karthik Narasimhan", "Danqi Chen"], "title": "Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting", "comment": null, "summary": "Adapting language models (LMs) to new tasks via post-training carries the\nrisk of degrading existing capabilities -- a phenomenon classically known as\ncatastrophic forgetting. In this paper, toward identifying guidelines for\nmitigating this phenomenon, we systematically compare the forgetting patterns\nof two widely adopted post-training methods: supervised fine-tuning (SFT) and\nreinforcement learning (RL). Our experiments reveal a consistent trend across\nLM families (Llama, Qwen) and tasks (instruction following, general knowledge,\nand arithmetic reasoning): RL leads to less forgetting than SFT while achieving\ncomparable or higher target task performance. To investigate the cause for this\ndifference, we consider a simplified setting in which the LM is modeled as a\nmixture of two distributions, one corresponding to prior knowledge and the\nother to the target task. We identify that the mode-seeking nature of RL, which\nstems from its use of on-policy data, enables keeping prior knowledge intact\nwhen learning the target task. We then verify this insight by demonstrating\nthat the use on-policy data underlies the robustness of RL to forgetting in\npractical settings, as opposed to other algorithmic choices such as the KL\nregularization or advantage estimation. Lastly, as a practical implication, our\nresults highlight the potential of mitigating forgetting using approximately\non-policy data, which can be substantially more efficient to obtain than fully\non-policy data.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u4e24\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9057\u5fd8\u6a21\u5f0f\uff0c\u53d1\u73b0RL\u6bd4SFT\u5bfc\u81f4\u66f4\u5c11\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u6216\u66f4\u597d\u7684\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u7f13\u89e3\u8be5\u73b0\u8c61\u63d0\u4f9b\u6307\u5bfc\u539f\u5219\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83SFT\u548cRL\u5728\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf(Llama\u3001Qwen)\u548c\u4efb\u52a1(\u6307\u4ee4\u8ddf\u968f\u3001\u5e38\u8bc6\u77e5\u8bc6\u3001\u6570\u5b66\u63a8\u7406)\u4e2d\u7684\u9057\u5fd8\u6a21\u5f0f\uff0c\u5e76\u5efa\u7acb\u7b80\u5316\u7406\u8bba\u6a21\u578b\u5206\u6790\u539f\u56e0\u3002", "result": "RL\u5728\u6240\u6709\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u90fd\u8868\u73b0\u51fa\u6bd4SFT\u66f4\u5c11\u7684\u9057\u5fd8\uff0c\u540c\u65f6\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u597d\u3002\u7814\u7a76\u53d1\u73b0RL\u7684\u5728\u7ebf\u6570\u636e\u4f7f\u7528\u662f\u5176\u6297\u9057\u5fd8\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "RL\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u6bd4SFT\u66f4\u80fd\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u8fd1\u4f3c\u5728\u7ebf\u6570\u636e\u7684\u4f7f\u7528\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "topic": "agent analysis"}}
{"id": "2510.18672", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18672", "abs": "https://arxiv.org/abs/2510.18672", "authors": ["Qi Li", "Junpan Wu", "Xiang Liu", "Yuxin Wang", "Zeyu Li", "Zhenheng Tang", "Yuhan Chen", "Shaohuai Shi", "Xiaowen Chu"], "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study", "comment": null, "summary": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b(RLLM)\u7684\u670d\u52a1\u6027\u80fd\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u53d1\u73b0RLLM\u5728\u5185\u5b58\u4f7f\u7528\u3001\u8bf7\u6c42\u5904\u7406\u3001\u8fd0\u884c\u65f6\u95f4\u548c\u9886\u57df\u504f\u597d\u65b9\u9762\u4e0e\u4f20\u7edfLLM\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u9a8c\u8bc1\u4e86\u73b0\u6709\u63a8\u7406\u4f18\u5316\u6280\u672f\u5728RLLM\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u867d\u7136RLLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u670d\u52a1\u6027\u80fd\u548c\u90e8\u7f72\u884c\u4e3a\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u8fd9\u963b\u788d\u4e86RLLM\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u9996\u5148\u8fdb\u884c\u8bd5\u70b9\u7814\u7a76\u6bd4\u8f83RLLM\u4e0e\u4f20\u7edfLLM\u7684\u670d\u52a1\u6027\u80fd\uff0c\u7136\u540e\u8c03\u67e5\u73b0\u6709\u63a8\u7406\u4f18\u5316\u6280\u672f\u5bf9RLLM\u7684\u6709\u6548\u6027\uff0c\u6700\u540e\u4f7f\u7528Gamma\u5206\u5e03\u5efa\u6a21\u7684\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0RLLM\u5177\u6709\u663e\u8457\u7684\u5185\u5b58\u4f7f\u7528\u548c\u6ce2\u52a8\u3001\u62d6\u5c3e\u8bf7\u6c42\u3001\u81ea\u9002\u5e94\u8fd0\u884c\u65f6\u95f4\u548c\u9886\u57df\u504f\u597d\u7b49\u7279\u70b9\uff1b\u6a21\u578b\u91cf\u5316\u548c\u63a8\u6d4b\u89e3\u7801\u80fd\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u4e14\u5bf9\u7cbe\u5ea6\u5f71\u54cd\u5c0f\uff0c\u800c\u524d\u7f00\u7f13\u5b58\u548cKV\u7f13\u5b58\u91cf\u5316\u53ef\u80fd\u964d\u4f4e\u5c0fRLLM\u7684\u7cbe\u5ea6\u6216\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86RLLM\u72ec\u7279\u7684\u670d\u52a1\u7279\u6027\uff0c\u4e3aRLLM\u63a8\u7406\u670d\u52a1\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8RLLM\u5728\u7814\u7a76\u548c\u5de5\u4e1a\u754c\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2510.18687", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18687", "abs": "https://arxiv.org/abs/2510.18687", "authors": ["Chenbei Lu", "Zaiwei Chen", "Tongxin Li", "Chenye Wu", "Adam Wierman"], "title": "Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach", "comment": null, "summary": "Traditional reinforcement learning (RL) assumes the agents make decisions\nbased on Markov decision processes (MDPs) with one-step transition models. In\nmany real-world applications, such as energy management and stock investment,\nagents can access multi-step predictions of future states, which provide\nadditional advantages for decision making. However, multi-step predictions are\ninherently high-dimensional: naively embedding these predictions into an MDP\nleads to an exponential blow-up in state space and the curse of dimensionality.\nMoreover, existing RL theory provides few tools to analyze prediction-augmented\nMDPs, as it typically works on one-step transition kernels and cannot\naccommodate multi-step predictions with errors or partial action-coverage. We\naddress these challenges with three key innovations: First, we propose the\n\\emph{Bayesian value function} to characterize the optimal prediction-aware\npolicy tractably. Second, we develop a novel \\emph{Bellman-Jensen Gap} analysis\non the Bayesian value function, which enables characterizing the value of\nimperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with\nOnline Adaptation), a two-stage model-based RL algorithm that separates offline\nBayesian value learning from lightweight online adaptation to real-time\npredictions. We prove that BOLA remains sample-efficient even under imperfect\npredictions. We validate our theory and algorithm on synthetic MDPs and a\nreal-world wind energy storage control problem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u591a\u6b65\u9884\u6d4b\u589e\u5f3aMDP\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4ef7\u503c\u51fd\u6570\u548cBellman-Jensen Gap\u5206\u6790\u6765\u89e3\u51b3\u4f20\u7edfRL\u5728\u591a\u6b65\u9884\u6d4b\u573a\u666f\u4e0b\u7684\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86BOLA\u7b97\u6cd5\u5b9e\u73b0\u79bb\u7ebf\u5b66\u4e60\u548c\u5728\u7ebf\u9002\u5e94\u7684\u5206\u79bb\u3002", "motivation": "\u4f20\u7edfRL\u5047\u8bbe\u57fa\u4e8e\u5355\u6b65\u8f6c\u79fb\u6a21\u578b\u7684MDP\uff0c\u4f46\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\uff08\u5982\u80fd\u6e90\u7ba1\u7406\u548c\u80a1\u7968\u6295\u8d44\uff09\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u8bbf\u95ee\u591a\u6b65\u672a\u6765\u72b6\u6001\u9884\u6d4b\uff0c\u8fd9\u4e3a\u51b3\u7b56\u63d0\u4f9b\u4e86\u989d\u5916\u4f18\u52bf\u3002\u7136\u800c\uff0c\u591a\u6b65\u9884\u6d4b\u5929\u7136\u9ad8\u7ef4\uff0c\u76f4\u63a5\u5d4c\u5165MDP\u4f1a\u5bfc\u81f4\u72b6\u6001\u7a7a\u95f4\u6307\u6570\u7206\u70b8\u548c\u7ef4\u5ea6\u707e\u96be\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u8d1d\u53f6\u65af\u4ef7\u503c\u51fd\u6570\u6765\u8868\u5f81\u6700\u4f18\u9884\u6d4b\u611f\u77e5\u7b56\u7565\uff1b2\uff09Bellman-Jensen Gap\u5206\u6790\u6765\u8868\u5f81\u4e0d\u5b8c\u7f8e\u9884\u6d4b\u7684\u4ef7\u503c\uff1b3\uff09BOLA\u7b97\u6cd5\uff08\u8d1d\u53f6\u65af\u79bb\u7ebf\u5b66\u4e60\u4e0e\u5728\u7ebf\u9002\u5e94\uff09\uff0c\u5c06\u79bb\u7ebf\u8d1d\u53f6\u65af\u4ef7\u503c\u5b66\u4e60\u4e0e\u8f7b\u91cf\u7ea7\u5728\u7ebf\u9002\u5e94\u5206\u79bb\u7684\u4e24\u9636\u6bb5\u57fa\u4e8e\u6a21\u578b\u7684RL\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86BOLA\u5373\u4f7f\u5728\u9884\u6d4b\u4e0d\u5b8c\u7f8e\u7684\u60c5\u51b5\u4e0b\u4e5f\u4fdd\u6301\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728\u5408\u6210MDP\u548c\u771f\u5b9e\u4e16\u754c\u98ce\u80fd\u5b58\u50a8\u63a7\u5236\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u7406\u8bba\u548c\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5904\u7406\u591a\u6b65\u9884\u6d4b\u589e\u5f3a\u7684MDP\u63d0\u4f9b\u4e86\u7406\u8bba\u5de5\u5177\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRL\u5728\u591a\u6b65\u9884\u6d4b\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18814", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18814", "abs": "https://arxiv.org/abs/2510.18814", "authors": ["Mengqi Li", "Lei Zhao", "Anthony Man-Cho So", "Ruoyu Sun", "Xiao Li"], "title": "Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards", "comment": null, "summary": "We present a simple, self-help online supervised finetuning (OSFT) paradigm\nfor LLM reasoning. In this paradigm, the model generates its own responses and\nis immediately finetuned on this self-generated data. OSFT is a highly\nefficient training strategy for LLM reasoning, as it is reward-free and uses\njust one rollout by default. Experiment results show that OSFT achieves\ndownstream performance on challenging mathematical reasoning tasks comparable\nto strong reinforcement learning with verifiable rewards (RLVR) methods such as\nGRPO. Our ablation study further demonstrates the efficiency and robustness of\nOSFT. The major mechanism of OSFT lies in facilitating the model's own existing\npreference (latent knowledge) learned from pretraining, which leads to\nreasoning ability improvement. We believe that OSFT offers an efficient and\npromising alternative to more complex, reward-based training paradigms. Our\ncode is available at https://github.com/ElementQi/OnlineSFT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u5728\u7ebf\u76d1\u7763\u5fae\u8c03(OSFT)\u65b9\u6cd5\uff0c\u8ba9LLM\u751f\u6210\u81ea\u5df1\u7684\u54cd\u5e94\u5e76\u7acb\u5373\u5728\u8fd9\u4e9b\u81ea\u751f\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u65e0\u9700\u5956\u52b1\u4fe1\u53f7\u5373\u53ef\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\u6765\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "OSFT\u65b9\u6cd5\uff1a\u6a21\u578b\u751f\u6210\u54cd\u5e94\u540e\u7acb\u5373\u5728\u81ea\u751f\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u9ed8\u8ba4\u53ea\u9700\u4e00\u6b21rollout\uff0c\u65e0\u9700\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cOSFT\u8fbe\u5230\u4e86\u4e0eGRPO\u7b49\u5f3aRL\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u66f4\u9ad8\u6548\u7a33\u5065\u3002", "conclusion": "OSFT\u901a\u8fc7\u4fc3\u8fdb\u6a21\u578b\u4ece\u9884\u8bad\u7ec3\u4e2d\u5b66\u5230\u7684\u6f5c\u5728\u504f\u597d\u77e5\u8bc6\u6765\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u5956\u52b1\u8bad\u7ec3\u8303\u5f0f\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18821", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18821", "abs": "https://arxiv.org/abs/2510.18821", "authors": ["Hongliang Lu", "Yuhang Wen", "Pengyu Cheng", "Ruijin Ding", "Haotian Xu", "Jiaqi Guo", "Chutian Wang", "Haonan Chen", "Xiaoxi Jiang", "Guanjun Jiang"], "title": "Search Self-play: Pushing the Frontier of Agent Capability without Supervision", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has become the\nmainstream technique for training LLM agents. However, RLVR highly depends on\nwell-crafted task queries and corresponding ground-truth answers to provide\naccurate rewards, which requires massive human efforts and hinders the RL\nscaling processes, especially under agentic scenarios. Although a few recent\nworks explore task synthesis methods, the difficulty of generated agentic tasks\ncan hardly be controlled to provide effective RL training advantages. To\nachieve agentic RLVR with higher scalability, we explore self-play training for\ndeep search agents, in which the learning LLM utilizes multi-turn search engine\ncalling and acts simultaneously as both a task proposer and a problem solver.\nThe task proposer aims to generate deep search queries with well-defined\nground-truth answers and increasing task difficulty. The problem solver tries\nto handle the generated search queries and output the correct answer\npredictions. To ensure that each generated search query has accurate ground\ntruth, we collect all the searching results from the proposer's trajectory as\nexternal knowledge, then conduct retrieval-augmentation generation (RAG) to\ntest whether the proposed query can be correctly answered with all necessary\nsearch documents provided. In this search self-play (SSP) game, the proposer\nand the solver co-evolve their agent capabilities through both competition and\ncooperation. With substantial experimental results, we find that SSP can\nsignificantly improve search agents' performance uniformly on various\nbenchmarks without any supervision under both from-scratch and continuous RL\ntraining setups. The code is at https://github.com/Alibaba-Quark/SSP.", "AI": {"tldr": "\u63d0\u51fa\u641c\u7d22\u81ea\u535a\u5f08\uff08SSP\uff09\u65b9\u6cd5\uff0c\u8ba9LLM\u540c\u65f6\u4f5c\u4e3a\u4efb\u52a1\u63d0\u51fa\u8005\u548c\u95ee\u9898\u89e3\u51b3\u8005\uff0c\u901a\u8fc7\u7ade\u4e89\u4e0e\u5408\u4f5c\u5171\u540c\u8fdb\u5316\u641c\u7d22\u80fd\u529b\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u5373\u53ef\u663e\u8457\u63d0\u5347\u641c\u7d22\u4ee3\u7406\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u4f9d\u8d56\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4efb\u52a1\u67e5\u8be2\u548c\u771f\u5b9e\u7b54\u6848\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\uff0c\u963b\u788d\u4e86\u5728\u667a\u80fd\u4f53\u573a\u666f\u4e0b\u7684\u6269\u5c55\u3002\u73b0\u6709\u4efb\u52a1\u5408\u6210\u65b9\u6cd5\u96be\u4ee5\u63a7\u5236\u751f\u6210\u4efb\u52a1\u7684\u96be\u5ea6\uff0c\u65e0\u6cd5\u6709\u6548\u652f\u6301RL\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u81ea\u535a\u5f08\u8bad\u7ec3\u6846\u67b6\uff0cLLM\u540c\u65f6\u626e\u6f14\u4efb\u52a1\u63d0\u51fa\u8005\uff08\u751f\u6210\u6709\u660e\u786e\u771f\u5b9e\u7b54\u6848\u4e14\u96be\u5ea6\u9012\u589e\u7684\u6df1\u5ea6\u641c\u7d22\u67e5\u8be2\uff09\u548c\u95ee\u9898\u89e3\u51b3\u8005\uff08\u5904\u7406\u641c\u7d22\u67e5\u8be2\u5e76\u8f93\u51fa\u7b54\u6848\uff09\u3002\u901a\u8fc7\u6536\u96c6\u641c\u7d22\u8f68\u8ff9\u4f5c\u4e3a\u5916\u90e8\u77e5\u8bc6\uff0c\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u9a8c\u8bc1\u67e5\u8be2\u7684\u53ef\u56de\u7b54\u6027\u3002", "result": "SSP\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u5728\u4ece\u96f6\u5f00\u59cb\u548c\u6301\u7eedRL\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u5747\u6709\u6548\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u76d1\u7763\u3002", "conclusion": "\u641c\u7d22\u81ea\u535a\u5f08\uff08SSP\uff09\u4e3a\u667a\u80fd\u4f53RLVR\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u53ef\u6269\u5c55\u6027\uff0c\u901a\u8fc7\u7ade\u4e89\u4e0e\u5408\u4f5c\u7684\u5171\u540c\u8fdb\u5316\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u641c\u7d22\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.18828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18828", "abs": "https://arxiv.org/abs/2510.18828", "authors": ["Yigit Korkmaz", "Urvi Bhuwania", "Ayush Jain", "Erdem B\u0131y\u0131k"], "title": "Actor-Free Continuous Control via Structurally Maximizable Q-Functions", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Value-based algorithms are a cornerstone of off-policy reinforcement learning\ndue to their simplicity and training stability. However, their use has\ntraditionally been restricted to discrete action spaces, as they rely on\nestimating Q-values for individual state-action pairs. In continuous action\nspaces, evaluating the Q-value over the entire action space becomes\ncomputationally infeasible. To address this, actor-critic methods are typically\nemployed, where a critic is trained on off-policy data to estimate Q-values,\nand an actor is trained to maximize the critic's output. Despite their\npopularity, these methods often suffer from instability during training. In\nthis work, we propose a purely value-based framework for continuous control\nthat revisits structural maximization of Q-functions, introducing a set of key\narchitectural and algorithmic choices to enable efficient and stable learning.\nWe evaluate the proposed actor-free Q-learning approach on a range of standard\nsimulation tasks, demonstrating performance and sample efficiency on par with\nstate-of-the-art baselines, without the cost of learning a separate actor.\nParticularly, in environments with constrained action spaces, where the value\nfunctions are typically non-smooth, our method with structural maximization\noutperforms traditional actor-critic methods with gradient-based maximization.\nWe have released our code at https://github.com/USC-Lira/Q3C.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7eaf\u57fa\u4e8e\u4ef7\u503c\u51fd\u6570\u7684\u8fde\u7eed\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u6700\u5927\u5316Q\u51fd\u6570\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684\u5b66\u4e60\uff0c\u65e0\u9700\u5355\u72ec\u5b66\u4e60actor\u7f51\u7edc", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4ef7\u503c\u7684\u7b97\u6cd5\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u4e0d\u53ef\u884c\uff0c\u800cactor-critic\u65b9\u6cd5\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u7eaf\u4ef7\u503c\u65b9\u6cd5", "method": "\u91c7\u7528\u7ed3\u6784\u6700\u5927\u5316Q\u51fd\u6570\u7684\u5173\u952e\u67b6\u6784\u548c\u7b97\u6cd5\u9009\u62e9\uff0c\u5728\u6807\u51c6\u4eff\u771f\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30", "result": "\u5728\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u4e0a\u4e0e\u6700\u5148\u8fdb\u57fa\u7ebf\u76f8\u5f53\uff0c\u5728\u7ea6\u675f\u52a8\u4f5c\u7a7a\u95f4\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfactor-critic\u65b9\u6cd5", "conclusion": "\u8bc1\u660e\u4e86\u7eaf\u57fa\u4e8e\u4ef7\u503c\u7684\u65b9\u6cd5\u5728\u8fde\u7eed\u63a7\u5236\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u7279\u522b\u662f\u5728\u975e\u5e73\u6ed1\u4ef7\u503c\u51fd\u6570\u73af\u5883\u4e2d\u5177\u6709\u4f18\u52bf", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.d6bc5632", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cognizant.com%2Fus%2Fen%2Fai-lab%2Fneuro-san%3Futm_source=tldrai/1/0100019a01c6bc04-ee202968-6f1c-4d55-8a20-ac9f54004ff5-000000/T2nEOtCaDibHMN6qlcqeTLs5HUsw-VElj0ZKKvfOuQo=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cognizant.com%2Fus%2Fen%2Fai-lab%2Fneuro-san%3Futm_source=tldrai/1/0100019a01c6bc04-ee202968-6f1c-4d55-8a20-ac9f54004ff5-000000/T2nEOtCaDibHMN6qlcqeTLs5HUsw-VElj0ZKKvfOuQo=427", "authors": ["TLDR Newsletter"], "title": "Neuro SAN: an open source orchestration framework for multi-agent systems", "comment": "Source: TLDR Newsletter, Date: 2025-10-20, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cognizant.com%2Fus%2Fen%2Fai-lab%2Fneuro-san%3Futm_source=tldrai/1/0100019a01c6bc04-ee202968-6f1c-4d55-8a20-ac9f54004ff5-000000/T2nEOtCaDibHMN6qlcqeTLs5HUsw-VElj0ZKKvfOuQo=427", "summary": "Neuro SAN: an open source orchestration framework for multi-agent systems (Sponsor) Use Cognizant's open-source library to vibe code a working multi-agent network based on a description; connect APIs and tools; share data between agents without exposing prompts. Read the details", "source": "tldr", "AI": {"tldr": "Neuro SAN\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7f16\u6392\u6846\u67b6\uff0c\u5141\u8bb8\u7528\u6237\u57fa\u4e8e\u63cf\u8ff0\u6784\u5efa\u5de5\u4f5c\u591a\u667a\u80fd\u4f53\u7f51\u7edc\uff0c\u8fde\u63a5API\u548c\u5de5\u5177\uff0c\u5e76\u5728\u4e0d\u66b4\u9732\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u5171\u4eab\u6570\u636e\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u534f\u8c03\u3001API\u96c6\u6210\u548c\u6570\u636e\u5b89\u5168\u5171\u4eab\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e00\u4e2a\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u6765\u7b80\u5316\u591a\u667a\u80fd\u4f53\u7f51\u7edc\u7684\u6784\u5efa\u548c\u7ba1\u7406\u3002", "method": "\u4f7f\u7528Cognizant\u7684\u5f00\u6e90\u5e93\uff0c\u57fa\u4e8e\u63cf\u8ff0\u6027\u8f93\u5165\u81ea\u52a8\u751f\u6210\u5de5\u4f5c\u591a\u667a\u80fd\u4f53\u7f51\u7edc\uff0c\u652f\u6301API\u548c\u5de5\u5177\u8fde\u63a5\uff0c\u5b9e\u73b0\u5b89\u5168\u7684\u8de8\u667a\u80fd\u4f53\u6570\u636e\u5171\u4eab\u673a\u5236\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684\u591a\u667a\u80fd\u4f53\u7f16\u6392\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u534f\u8c03\u591a\u4e2a\u667a\u80fd\u4f53\uff0c\u5b89\u5168\u5904\u7406\u6570\u636e\u4ea4\u6362\uff0c\u5e76\u96c6\u6210\u5916\u90e8\u5de5\u5177\u548cAPI\u3002", "conclusion": "Neuro SAN\u4e3a\u6784\u5efa\u590d\u6742\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u5b89\u5168\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u7b80\u5316\u4e86\u5f00\u53d1\u6d41\u7a0b\u5e76\u589e\u5f3a\u4e86\u7cfb\u7edf\u534f\u4f5c\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.c098a44c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.brethorsting.com%2Fblog%2F2025%2F10%2Fclaude-code-is-unreasonably-good-at-building-mvps%2F%3Futm_source=tldrnewsletter/1/0100019a064bbbe2-83d2b17b-df33-4a01-9f19-844f5d0bf208-000000/L1LVMFS-DfaqENBWoChm_0o3lkpm0q5pGOXq_7WRY18=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.brethorsting.com%2Fblog%2F2025%2F10%2Fclaude-code-is-unreasonably-good-at-building-mvps%2F%3Futm_source=tldrnewsletter/1/0100019a064bbbe2-83d2b17b-df33-4a01-9f19-844f5d0bf208-000000/L1LVMFS-DfaqENBWoChm_0o3lkpm0q5pGOXq_7WRY18=427", "authors": ["TLDR Newsletter"], "title": "Claude Code is unreasonably good at building MVPs", "comment": "Source: TLDR Newsletter, Date: 2025-10-21, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.brethorsting.com%2Fblog%2F2025%2F10%2Fclaude-code-is-unreasonably-good-at-building-mvps%2F%3Futm_source=tldrnewsletter/1/0100019a064bbbe2-83d2b17b-df33-4a01-9f19-844f5d0bf208-000000/L1LVMFS-DfaqENBWoChm_0o3lkpm0q5pGOXq_7WRY18=427", "summary": "Claude Code is unreasonably good at building MVPs (4 minute read) Concerns about technical debt and quality are premature in the prototyping and validation stage. Perfect code isn't required to validate whether people want your product. What is needed is the ability to test more ideas quickly. Claude Code allows people to validate through building and testing. The ideas that survive contact with actual users are the ones worth refactoring, hardening, and scaling.", "source": "tldr", "AI": {"tldr": "Claude Code\u5728\u6784\u5efaMVP\u65b9\u9762\u8868\u73b0\u5f02\u5e38\u51fa\u8272\uff0c\u5728\u539f\u578b\u9a8c\u8bc1\u9636\u6bb5\u65e0\u9700\u8fc7\u5ea6\u62c5\u5fc3\u6280\u672f\u503a\u52a1\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u91cd\u70b9\u5728\u4e8e\u5feb\u901f\u6d4b\u8bd5\u66f4\u591a\u60f3\u6cd5\u3002", "motivation": "\u5728\u539f\u578b\u548c\u9a8c\u8bc1\u9636\u6bb5\uff0c\u6280\u672f\u503a\u52a1\u548c\u4ee3\u7801\u8d28\u91cf\u7684\u62c5\u5fe7\u4e3a\u65f6\u8fc7\u65e9\uff0c\u5b8c\u7f8e\u4ee3\u7801\u5e76\u975e\u9a8c\u8bc1\u4ea7\u54c1\u9700\u6c42\u6240\u5fc5\u9700\uff0c\u9700\u8981\u7684\u662f\u5feb\u901f\u6d4b\u8bd5\u66f4\u591a\u60f3\u6cd5\u7684\u80fd\u529b\u3002", "method": "\u5229\u7528Claude Code\u8fdb\u884c\u6784\u5efa\u548c\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5b9e\u9645\u7528\u6237\u63a5\u89e6\u6765\u9a8c\u8bc1\u60f3\u6cd5\u3002", "result": "Claude Code\u80fd\u591f\u5e2e\u52a9\u4eba\u4eec\u901a\u8fc7\u6784\u5efa\u548c\u6d4b\u8bd5\u6765\u9a8c\u8bc1\u4ea7\u54c1\u60f3\u6cd5\uff0c\u53ea\u6709\u90a3\u4e9b\u7ecf\u5f97\u8d77\u5b9e\u9645\u7528\u6237\u68c0\u9a8c\u7684\u60f3\u6cd5\u624d\u503c\u5f97\u91cd\u6784\u3001\u52a0\u56fa\u548c\u6269\u5c55\u3002", "conclusion": "\u5728MVP\u9636\u6bb5\u5e94\u4f18\u5148\u5173\u6ce8\u60f3\u6cd5\u9a8c\u8bc1\u800c\u975e\u4ee3\u7801\u5b8c\u7f8e\u5ea6\uff0cClaude Code\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "topic": "swe application"}}
{"id": "tldr.2510.50ce9ef5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pcloadletter.dev%2Fblog%2Fai-hype-and-productivity%2F%3Futm_source=tldrwebdev/1/0100019a0683901c-8d403fc5-fb93-46d5-8773-6429d5772b66-000000/XGn2sgyUM--NlqPuwgRwbWkLufI1IEgTua8XOnpo90o=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pcloadletter.dev%2Fblog%2Fai-hype-and-productivity%2F%3Futm_source=tldrwebdev/1/0100019a0683901c-8d403fc5-fb93-46d5-8773-6429d5772b66-000000/XGn2sgyUM--NlqPuwgRwbWkLufI1IEgTua8XOnpo90o=427", "authors": ["TLDR Newsletter"], "title": "AI hype is excessive, but its productivity gains are real", "comment": "Source: TLDR Newsletter, Date: 2025-10-21, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pcloadletter.dev%2Fblog%2Fai-hype-and-productivity%2F%3Futm_source=tldrwebdev/1/0100019a0683901c-8d403fc5-fb93-46d5-8773-6429d5772b66-000000/XGn2sgyUM--NlqPuwgRwbWkLufI1IEgTua8XOnpo90o=427", "summary": "AI hype is excessive, but its productivity gains are real (3 minute read) This dev initially dismissed AI due to excessive hype because of how companies unrealistically claimed it could solve all problems. However, their perspective changed after using AI-powered coding tools like Cursor, especially the autocomplete feature. They found these tools, especially the agent panel, when used with clear instructions, boosted their productivity by helping with code comprehension, scaffolding, and deb...", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7a0b\u5de5\u5177\uff08\u5982Cursor\uff09\u901a\u8fc7\u81ea\u52a8\u8865\u5168\u548c\u667a\u80fd\u9762\u677f\u529f\u80fd\uff0c\u5728\u4ee3\u7801\u7406\u89e3\u3001\u811a\u624b\u67b6\u642d\u5efa\u548c\u8c03\u8bd5\u65b9\u9762\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387", "motivation": "\u4f5c\u8005\u6700\u521d\u56e0AI\u8fc7\u5ea6\u7092\u4f5c\u800c\u5bf9\u5176\u6301\u6000\u7591\u6001\u5ea6\uff0c\u4f46\u5728\u5b9e\u9645\u4f7f\u7528AI\u7f16\u7a0b\u5de5\u5177\u540e\u6539\u53d8\u4e86\u770b\u6cd5\uff0c\u5e0c\u671b\u9a8c\u8bc1\u8fd9\u4e9b\u5de5\u5177\u7684\u5b9e\u9645\u751f\u4ea7\u529b\u4ef7\u503c", "method": "\u901a\u8fc7\u5b9e\u9645\u4f7f\u7528Cursor\u7b49AI\u7f16\u7a0b\u5de5\u5177\uff0c\u7279\u522b\u662f\u5176\u81ea\u52a8\u8865\u5168\u548c\u667a\u80fd\u9762\u677f\u529f\u80fd\uff0c\u7ed3\u5408\u6e05\u6670\u7684\u6307\u4ee4\u6765\u8f85\u52a9\u4ee3\u7801\u5f00\u53d1\u5de5\u4f5c", "result": "\u53d1\u73b0AI\u7f16\u7a0b\u5de5\u5177\u80fd\u6709\u6548\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7801\u7406\u89e3\u3001\u811a\u624b\u67b6\u642d\u5efa\u548c\u8c03\u8bd5\u65b9\u9762\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u7684\u751f\u4ea7\u529b\u589e\u76ca", "conclusion": "\u5c3d\u7ba1AI\u5b58\u5728\u8fc7\u5ea6\u7092\u4f5c\uff0c\u4f46AI\u7f16\u7a0b\u5de5\u5177\u786e\u5b9e\u80fd\u5e26\u6765\u771f\u5b9e\u7684\u751f\u4ea7\u529b\u63d0\u5347\uff0c\u503c\u5f97\u5f00\u53d1\u8005\u91c7\u7528", "topic": "swe application"}}
{"id": "tldr.2510.ee283302", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-on-the-web%3Futm_source=tldrwebdev/1/0100019a0683901c-8d403fc5-fb93-46d5-8773-6429d5772b66-000000/CnftcoRcRXcXMBiQAbBcu--Neaj6A_chULQ3rwastss=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-on-the-web%3Futm_source=tldrwebdev/1/0100019a0683901c-8d403fc5-fb93-46d5-8773-6429d5772b66-000000/CnftcoRcRXcXMBiQAbBcu--Neaj6A_chULQ3rwastss=427", "authors": ["TLDR Newsletter"], "title": "Claude Code on the Web", "comment": "Source: TLDR Newsletter, Date: 2025-10-21, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-on-the-web%3Futm_source=tldrwebdev/1/0100019a0683901c-8d403fc5-fb93-46d5-8773-6429d5772b66-000000/CnftcoRcRXcXMBiQAbBcu--Neaj6A_chULQ3rwastss=427", "summary": "Claude Code on the Web (2 minute read) Anthropic has launched Claude Code on the web, a beta feature allowing users to delegate coding tasks to Claude directly from their browser. This cloud-based service allows for parallel execution of coding tasks in a sandboxed environment, connecting to GitHub repos and providing real-time progress tracking.", "source": "tldr", "AI": {"tldr": "Anthropic\u63a8\u51fa\u4e86Claude Code\u7f51\u9875\u7248\u6d4b\u8bd5\u529f\u80fd\uff0c\u5141\u8bb8\u7528\u6237\u76f4\u63a5\u5728\u6d4f\u89c8\u5668\u4e2d\u59d4\u6258\u7f16\u7801\u4efb\u52a1\u7ed9Claude\uff0c\u652f\u6301\u5e76\u884c\u6267\u884c\u548c\u5b9e\u65f6\u8fdb\u5ea6\u8ddf\u8e2a\u3002", "motivation": "\u63d0\u4f9b\u66f4\u4fbf\u6377\u7684\u4e91\u7aef\u7f16\u7801\u670d\u52a1\uff0c\u8ba9\u7528\u6237\u65e0\u9700\u672c\u5730\u73af\u5883\u5373\u53ef\u5b8c\u6210\u7f16\u7801\u4efb\u52a1\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u6d4f\u89c8\u5668\u7684\u4e91\u670d\u52a1\uff0c\u5728\u6c99\u76d2\u73af\u5883\u4e2d\u5e76\u884c\u6267\u884c\u7f16\u7801\u4efb\u52a1\uff0c\u652f\u6301\u8fde\u63a5GitHub\u4ed3\u5e93\u3002", "result": "\u6210\u529f\u63a8\u51fa\u4e86Claude Code\u7f51\u9875\u7248\u6d4b\u8bd5\u529f\u80fd\uff0c\u5b9e\u73b0\u4e86\u4e91\u7aef\u7f16\u7801\u4efb\u52a1\u7684\u59d4\u6258\u548c\u6267\u884c\u3002", "conclusion": "Claude Code\u7f51\u9875\u7248\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u4fbf\u6377\u7684\u4e91\u7aef\u7f16\u7801\u4f53\u9a8c\uff0c\u6709\u671b\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "tldr.2510.c2d4fa34", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-on-the-web%3Futm_source=tldrai/1/0100019a0700f902-c970e531-55dd-43ec-ac57-ba1140af0dc0-000000/GKAtygDUYgsgi9kpjnGD_yK0AIT0OsRudnzgsOdMXxI=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-on-the-web%3Futm_source=tldrai/1/0100019a0700f902-c970e531-55dd-43ec-ac57-ba1140af0dc0-000000/GKAtygDUYgsgi9kpjnGD_yK0AIT0OsRudnzgsOdMXxI=427", "authors": ["TLDR Newsletter"], "title": "Claude Code on the web", "comment": "Source: TLDR Newsletter, Date: 2025-10-21, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-on-the-web%3Futm_source=tldrai/1/0100019a0700f902-c970e531-55dd-43ec-ac57-ba1140af0dc0-000000/GKAtygDUYgsgi9kpjnGD_yK0AIT0OsRudnzgsOdMXxI=427", "summary": "Claude Code on the web (2 minute read) Anthropic has launched a browser-based version of Claude Code. The beta allows users to connect GitHub repositories and delegate multiple tasks simultaneously across different repositories, with each session running in isolated sandboxes.", "source": "tldr", "AI": {"tldr": "Anthropic\u63a8\u51fa\u57fa\u4e8e\u6d4f\u89c8\u5668\u7684Claude Code\u6d4b\u8bd5\u7248\uff0c\u652f\u6301\u8fde\u63a5GitHub\u4ed3\u5e93\u5e76\u540c\u65f6\u59d4\u6d3e\u591a\u4e2a\u8de8\u4ed3\u5e93\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4f1a\u8bdd\u5728\u9694\u79bb\u6c99\u7bb1\u4e2d\u8fd0\u884c", "motivation": "\u63d0\u4f9b\u66f4\u4fbf\u6377\u7684\u4ee3\u7801\u5f00\u53d1\u4f53\u9a8c\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u76f4\u63a5\u5728\u6d4f\u89c8\u5668\u4e2d\u4f7f\u7528Claude Code\u529f\u80fd\uff0c\u65e0\u9700\u672c\u5730\u5b89\u88c5", "method": "\u5f00\u53d1\u6d4f\u89c8\u5668\u7248\u672c\uff0c\u652f\u6301GitHub\u4ed3\u5e93\u8fde\u63a5\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u5e76\u884c\u5904\u7406\u548c\u9694\u79bb\u6c99\u7bb1\u73af\u5883", "result": "\u6210\u529f\u63a8\u51faClaude Code\u7f51\u9875\u7248\u6d4b\u8bd5\u7248\uff0c\u7528\u6237\u53ef\u5728\u6d4f\u89c8\u5668\u4e2d\u8bbf\u95ee\u5e76\u4f7f\u7528\u5176\u4ee3\u7801\u5f00\u53d1\u529f\u80fd", "conclusion": "\u6d4f\u89c8\u5668\u7248Claude Code\u7684\u63a8\u51fa\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u4fbf\u6377\u7684\u4ee3\u7801\u5f00\u53d1\u5de5\u5177", "topic": "swe application"}}
{"id": "tldr.2510.9650f852", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F10%2F20%2Fmeta-ais-app-downloads-and-daily-users-spiked-after-launch-of-vibes-ai-video-feed%2F%3Futm_source=tldrai/1/0100019a0700f902-c970e531-55dd-43ec-ac57-ba1140af0dc0-000000/KuMgoWVrgWuTtLVYu1sVna4dH323qVcOewuZjNn5ETs=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F10%2F20%2Fmeta-ais-app-downloads-and-daily-users-spiked-after-launch-of-vibes-ai-video-feed%2F%3Futm_source=tldrai/1/0100019a0700f902-c970e531-55dd-43ec-ac57-ba1140af0dc0-000000/KuMgoWVrgWuTtLVYu1sVna4dH323qVcOewuZjNn5ETs=427", "authors": ["TLDR Newsletter"], "title": "Meta AI's app downloads and daily users spiked after launch of \u2018Vibes' AI video feed", "comment": "Source: TLDR Newsletter, Date: 2025-10-21, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F10%2F20%2Fmeta-ais-app-downloads-and-daily-users-spiked-after-launch-of-vibes-ai-video-feed%2F%3Futm_source=tldrai/1/0100019a0700f902-c970e531-55dd-43ec-ac57-ba1140af0dc0-000000/KuMgoWVrgWuTtLVYu1sVna4dH323qVcOewuZjNn5ETs=427", "summary": "Meta AI's app downloads and daily users spiked after launch of \u2018Vibes' AI video feed (3 minute read) Anthropic launched a web app for Claude Code yesterday. It is now rolling out to Pro and Max users. Anthropic is attempting to evolve Claude Code beyond a command-line interface tool. It hopes developers will spin up AI coding agents in more places now that Claude Code is on the web. Claude Code has grown 10x in users since its broader launch in May. It now accounts for more than $500 million ...", "source": "tldr", "AI": {"tldr": "Anthropic\u63a8\u51faClaude Code\u7f51\u9875\u5e94\u7528\uff0c\u65e8\u5728\u5c06AI\u7f16\u7a0b\u52a9\u624b\u4ece\u547d\u4ee4\u884c\u5de5\u5177\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u4f7f\u7528\u573a\u666f\uff0c\u7528\u6237\u589e\u957f\u663e\u8457\u3002", "motivation": "\u5c06Claude Code\u4ece\u547d\u4ee4\u884c\u754c\u9762\u5de5\u5177\u6f14\u8fdb\u4e3a\u66f4\u6613\u7528\u7684\u7f51\u9875\u5e94\u7528\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u5728\u66f4\u591a\u73af\u5883\u4e2d\u521b\u5efaAI\u7f16\u7a0b\u4ee3\u7406\u3002", "method": "\u63a8\u51faClaude Code\u7f51\u9875\u5e94\u7528\uff0c\u9762\u5411Pro\u548cMax\u7528\u6237\u9010\u6b65\u63a8\u5e7f\u3002", "result": "\u81ea5\u6708\u5e7f\u6cdb\u53d1\u5e03\u4ee5\u6765\u7528\u6237\u589e\u957f10\u500d\uff0c\u73b0\u5728\u8d21\u732e\u8d85\u8fc75\u4ebf\u7f8e\u5143\u6536\u5165\u3002", "conclusion": "\u7f51\u9875\u7248\u7684\u63a8\u51fa\u6210\u529f\u6269\u5c55\u4e86Claude Code\u7684\u4f7f\u7528\u573a\u666f\u548c\u7528\u6237\u57fa\u7840\u3002", "topic": "swe application"}}
{"id": "tldr.2510.335fafea", "categories": ["tldr.article"], "pdf": "http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fschneier.com%2Fblog%2Farchives%2F2025%2F10%2Fagentic-ais-ooda-loop-problem.html%3Futm_source=tldrai/1/0100019a0700f902-c970e531-55dd-43ec-ac57-ba1140af0dc0-000000/xlbhO5-rlvjIq6KBz60Xe1K5WJjr6gdG5r4TbqJq5E8=427", "abs": "http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fschneier.com%2Fblog%2Farchives%2F2025%2F10%2Fagentic-ais-ooda-loop-problem.html%3Futm_source=tldrai/1/0100019a0700f902-c970e531-55dd-43ec-ac57-ba1140af0dc0-000000/xlbhO5-rlvjIq6KBz60Xe1K5WJjr6gdG5r4TbqJq5E8=427", "authors": ["TLDR Newsletter"], "title": "Agentic AI's OODA Loop Problem", "comment": "Source: TLDR Newsletter, Date: 2025-10-21, Reading time: 13 minute read, Links: http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fschneier.com%2Fblog%2Farchives%2F2025%2F10%2Fagentic-ais-ooda-loop-problem.html%3Futm_source=tldrai/1/0100019a0700f902-c970e531-55dd-43ec-ac57-ba1140af0dc0-000000/xlbhO5-rlvjIq6KBz60Xe1K5WJjr6gdG5r4TbqJq5E8=427", "summary": "Agentic AI's OODA Loop Problem (13 minute read) AI agents embed untrusted actors within themselves in their training, so fixing hallucinations is insufficient because even if an AI accurately interprets its inputs and produces corresponding output, it can be fully corrupt.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u4e86\u4e0d\u53ef\u4fe1\u7684\u884c\u4e3a\u8005\uff0c\u5373\u4f7f\u51c6\u786e\u89e3\u91ca\u8f93\u5165\u5e76\u4ea7\u751f\u76f8\u5e94\u8f93\u51fa\uff0c\u4e5f\u53ef\u80fd\u5b8c\u5168\u8150\u8d25\uff0c\u4ec5\u4fee\u590d\u5e7b\u89c9\u95ee\u9898\u662f\u4e0d\u591f\u7684\u3002", "motivation": "\u63a2\u8ba8AI\u4ee3\u7406\u5728OODA\u5faa\u73af\u4e2d\u9762\u4e34\u7684\u6839\u672c\u5b89\u5168\u95ee\u9898\uff0c\u6307\u51fa\u4ec5\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\u4e0d\u8db3\u4ee5\u786e\u4fddAI\u7cfb\u7edf\u7684\u53ef\u4fe1\u6027\u3002", "method": "\u5206\u6790AI\u4ee3\u7406\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u4e0d\u53ef\u4fe1\u884c\u4e3a\u8005\u7684\u673a\u5236\uff0c\u4ee5\u53ca\u8fd9\u5bf9OODA\u5faa\u73af\u5b8c\u6574\u6027\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5373\u4f7fAI\u51c6\u786e\u5904\u7406\u8f93\u5165\u548c\u8f93\u51fa\uff0c\u7cfb\u7edf\u4ecd\u53ef\u80fd\u56e0\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8150\u8d25\u800c\u53d8\u5f97\u4e0d\u53ef\u4fe1\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u5e7b\u89c9\u4fee\u590d\uff0c\u4ece\u6839\u672c\u4e0a\u89e3\u51b3AI\u4ee3\u7406\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.433ee0c7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.npr.org%2F2025%2F10%2F21%2Fnx-s1-5506141%2Fai-code-software-productivity-claims%3Futm_source=tldrnewsletter/1/0100019a0b72cf0a-693f0a61-2e44-4518-8635-428c0d4e809e-000000/kz5Ahqn5dZTrzRTh8QVxvVTKOUmUy7PPYSJQ23OdfoM=428", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.npr.org%2F2025%2F10%2F21%2Fnx-s1-5506141%2Fai-code-software-productivity-claims%3Futm_source=tldrnewsletter/1/0100019a0b72cf0a-693f0a61-2e44-4518-8635-428c0d4e809e-000000/kz5Ahqn5dZTrzRTh8QVxvVTKOUmUy7PPYSJQ23OdfoM=428", "authors": ["TLDR Newsletter"], "title": "Tech CEOs say the era of 'code by AI' is here. Some software engineers are skeptical", "comment": "Source: TLDR Newsletter, Date: 2025-10-22, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.npr.org%2F2025%2F10%2F21%2Fnx-s1-5506141%2Fai-code-software-productivity-claims%3Futm_source=tldrnewsletter/1/0100019a0b72cf0a-693f0a61-2e44-4518-8635-428c0d4e809e-000000/kz5Ahqn5dZTrzRTh8QVxvVTKOUmUy7PPYSJQ23OdfoM=428", "summary": "Tech CEOs say the era of 'code by AI' is here. Some software engineers are skeptical (6 minute read) Many software engineers have seen no evidence of long-term boosts to efficiency.", "source": "tldr", "AI": {"tldr": "\u79d1\u6280CEO\u58f0\u79f0'AI\u7f16\u7a0b'\u65f6\u4ee3\u5df2\u5230\u6765\uff0c\u4f46\u8bb8\u591a\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5bf9\u6b64\u6301\u6000\u7591\u6001\u5ea6\uff0c\u8ba4\u4e3a\u6ca1\u6709\u957f\u671f\u6548\u7387\u63d0\u5347\u7684\u8bc1\u636e", "motivation": "\u63a2\u8ba8AI\u7f16\u7a0b\u5de5\u5177\u5728\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u771f\u5b9e\u6548\u679c\u548c\u5de5\u7a0b\u5e08\u7684\u63a5\u53d7\u7a0b\u5ea6", "method": "\u901a\u8fc7\u91c7\u8bbf\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u548c\u79d1\u6280CEO\uff0c\u6536\u96c6\u4ed6\u4eec\u5bf9AI\u7f16\u7a0b\u5de5\u5177\u4f7f\u7528\u4f53\u9a8c\u7684\u770b\u6cd5", "result": "\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u666e\u904d\u5bf9AI\u7f16\u7a0b\u5de5\u5177\u7684\u5b9e\u9645\u6548\u7387\u63d0\u5347\u6301\u6000\u7591\u6001\u5ea6\uff0c\u4e0e\u79d1\u6280CEO\u7684\u4e50\u89c2\u5ba3\u4f20\u5f62\u6210\u5bf9\u6bd4", "conclusion": "\u867d\u7136AI\u7f16\u7a0b\u5de5\u5177\u88ab\u5927\u529b\u63a8\u5e7f\uff0c\u4f46\u5728\u5b9e\u9645\u5f00\u53d1\u4e2d\u5c1a\u672a\u8bc1\u660e\u80fd\u5e26\u6765\u957f\u671f\u6548\u7387\u63d0\u5347", "topic": "swe application"}}
{"id": "wechat.2510.ae87fa96", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzNjA4MzE3MQ==&mid=2247483705&idx=1&sn=6fb1c8aec74cc7bb22b230c863d82cf9&chksm=f1810124934f6febc5cf0a067e0a289e15c8ccfd7fe52f08e3bd2e4f6f79332bb92e70f17e1e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzNjA4MzE3MQ==&mid=2247483705&idx=1&sn=6fb1c8aec74cc7bb22b230c863d82cf9&chksm=f1810124934f6febc5cf0a067e0a289e15c8ccfd7fe52f08e3bd2e4f6f79332bb92e70f17e1e#rd", "authors": ["\u897f\u7c73\u5439\u6c34"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-10-22 11:51:14", "summary": "\u8be6\u7ec6\u9610\u8ff0\u4e86\u81ea\u5df1\u5bf9\u5f3a\u5316\u5b66\u4e60\u7684\u8d1f\u9762\u770b\u6cd5\u3002he elaborated in detail his negative views on reinforcement learning.\u5176\u4e2d\u6838\u5fc3\u89c2\u70b9\u57fa\u4e8e\u4ed62019\u5e74\u5c31\u63d0\u51fa\u7684\u8457\u540d\u7684\u201c\u82e6\u6da9\u7684\u6559\u8bad\u201d\u8fd9\u4e2a\u539f\u5219\u3002", "AI": {"tldr": "\u8be6\u7ec6\u9610\u8ff0\u4e86\u81ea\u5df1\u5bf9\u5f3a\u5316\u5b66\u4e60\u7684\u8d1f\u9762\u770b\u6cd5\u3002he elaborated in detail his negative views on reinforcement learning.\u5176\u4e2d\u6838\u5fc3\u89c2\u70b9\u57fa\u4e8e\u4ed62019\u5e74\u5c31\u63d0\u51fa\u7684\u8457\u540d\u7684\u201c\u82e6\u6da9\u7684\u6559\u8bad\u201d\u8fd9\u4e2a\u539f\u5219\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.4bce79c4", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNjYwMTAxNg==&mid=2247500906&idx=1&sn=51df2f9079a06834b65499902e76c1e5&chksm=c357939b2fe1d01e5b987c4dd826ef49d5579d136cf4800bb71ac9508ed7bf703fc5926f3c85#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNjYwMTAxNg==&mid=2247500906&idx=1&sn=51df2f9079a06834b65499902e76c1e5&chksm=c357939b2fe1d01e5b987c4dd826ef49d5579d136cf4800bb71ac9508ed7bf703fc5926f3c85#rd", "authors": ["\u65e0\u95ee\u82af\u7a79"], "title": "\u63a2\u8def\u667a\u80fd\u4f53\u843d\u5730\u201c\u6700\u540e\u4e00\u516c\u91cc\u201d\uff1a\u590d\u73b0Cursor\u5728\u7ebf<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff0cRLinf-Online\u56e2\u961f\u8be6\u89e3\u6280\u672f\u5b9e\u73b0\u8def\u5f84\u53ca\u80cc\u540e\u601d\u8003", "comment": "Source: WeChat, Published: 2025-10-22 11:48:09", "summary": "\u6211\u4eec\u6df1\u6df1\u8ba4\u540c\uff0c\u5f3a\u5316\u5b66\u4e60\u662f\u63a8\u52a8\u667a\u80fd\u4f53\u5b9e\u73b0\u80fd\u529b\u8dc3\u5347\u7684\u5173\u952e\u6280\u672f\u8def\u5f84\uff0c\u800c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u610f\u4e49\u5728\u4e8e\u9488\u5bf9\u751f\u4ea7\u73af\u5883\u8fdb\u884c\u6301\u7eed\u7684\u81ea\u8fdb\u5316\uff0c\u6210\u4e3a\u667a\u80fd\u4f53\u8fdb\u4e00\u6b65\u53d1\u6325\u80fd\u529b\u7684\u5229\u5668\u3002", "AI": {"tldr": "\u6211\u4eec\u6df1\u6df1\u8ba4\u540c\uff0c\u5f3a\u5316\u5b66\u4e60\u662f\u63a8\u52a8\u667a\u80fd\u4f53\u5b9e\u73b0\u80fd\u529b\u8dc3\u5347\u7684\u5173\u952e\u6280\u672f\u8def\u5f84\uff0c\u800c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u610f\u4e49\u5728\u4e8e\u9488\u5bf9\u751f\u4ea7\u73af\u5883\u8fdb\u884c\u6301\u7eed\u7684\u81ea\u8fdb\u5316\uff0c\u6210\u4e3a\u667a\u80fd\u4f53\u8fdb\u4e00\u6b65\u53d1\u6325\u80fd\u529b\u7684\u5229\u5668\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.ae75f15a", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247660816&idx=1&sn=e73491aa84bb0054e3ce5dc3579c5915&chksm=e8fac5c56633e89f6430f4b0d7bd76c0b8100ef6b11939f503d78651fe71c6fcc7defc6daef7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247660816&idx=1&sn=e73491aa84bb0054e3ce5dc3579c5915&chksm=e8fac5c56633e89f6430f4b0d7bd76c0b8100ef6b11939f503d78651fe71c6fcc7defc6daef7#rd", "authors": ["\u6570\u636e\u6d3eTHU"], "title": "\u539f\u521b | \u5927\u6a21\u578b\u626b\u76f2\u7cfb\u5217\uff08\u4e09\uff09-<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u57fa\u7840\uff08\u4e0a\uff09", "comment": "Source: WeChat, Published: 2025-10-22 09:04:51", "summary": "\u5f3a\u5316\u5b66\u4e60\uff1a\u5c5e\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4e00\u4e2a\u5206\u652f\uff0c\u901a\u8fc7\"\u8bd5\u9519\"\u673a\u5236\u5b66\u4e60\uff0c\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u667a\u80fd\u4f53\uff08Agent\uff09\u5728\u73af\u5883\u4e2d\u91c7\u53d6\u884c\u52a8\uff0c\u83b7\u5f97\u5956\u52b1\u6216\u60e9\u7f5a\uff0c\u4ece\u800c\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff1a\u5c5e\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4e00\u4e2a\u5206\u652f\uff0c\u901a\u8fc7\"\u8bd5\u9519\"\u673a\u5236\u5b66\u4e60\uff0c\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u667a\u80fd\u4f53\uff08Agent\uff09\u5728\u73af\u5883\u4e2d\u91c7\u53d6\u884c\u52a8\uff0c\u83b7\u5f97\u5956\u52b1\u6216\u60e9\u7f5a\uff0c\u4ece\u800c\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.fcb85b18", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NjY2NTk3Ng==&mid=2247484889&idx=2&sn=9d76bbc4703487d4b8d9da26c35a04c1&chksm=cf8df7b3f5ae2867f236e60d2b7c616d114f03d5e93d5b19033875f594c9403f4eddbf832487#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NjY2NTk3Ng==&mid=2247484889&idx=2&sn=9d76bbc4703487d4b8d9da26c35a04c1&chksm=cf8df7b3f5ae2867f236e60d2b7c616d114f03d5e93d5b19033875f594c9403f4eddbf832487#rd", "authors": ["SXU iDUST Lab"], "title": "\u3010NeurIPS 2025\u3011| \u5c71\u897f\u5927\u5b66 iDUST LAB \u5728\u79bb\u7ebf<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7814\u7a76\u4e2d\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55", "comment": "Source: WeChat, Published: 2025-10-22 09:00:00", "summary": "\u7279\u522b\u9002\u7528\u4e8e\u5c0f\u6279\u91cf\u751a\u81f3\u5355\u72b6\u6001\u8f93\u5165\u7684\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u3002\u5176\u6807\u51c6\u8ba1\u7b97\u65b9\u5f0f\u4e3a\uff1a.\u7f51\u7edc\u96c6\u6210\uff1a\u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u7a33\u5b9a\u6027\u5e76\u51cf\u8f7b\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e38\u89c1\u7684\u9ad8\u4f30\u504f\u5dee\uff0cFANS \u5728 actor-critic \u6846\u67b6\u5185\u91c7\u7528\u4e86 M \u4e2a\u72ec\u7acb\u53c2\u6570\u5316 critic \u7f51\u7edc\u7684\u96c6\u6210\u3002", "AI": {"tldr": "\u7279\u522b\u9002\u7528\u4e8e\u5c0f\u6279\u91cf\u751a\u81f3\u5355\u72b6\u6001\u8f93\u5165\u7684\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u3002\u5176\u6807\u51c6\u8ba1\u7b97\u65b9\u5f0f\u4e3a\uff1a.\u7f51\u7edc\u96c6\u6210\uff1a\u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u7a33\u5b9a\u6027\u5e76\u51cf\u8f7b\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e38\u89c1\u7684\u9ad8\u4f30\u504f\u5dee\uff0cFANS \u5728 actor-critic \u6846\u67b6\u5185\u91c7\u7528\u4e86 M \u4e2a\u72ec\u7acb\u53c2\u6570\u5316 critic \u7f51\u7edc\u7684\u96c6\u6210\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.5a5409c0", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649505530&idx=1&sn=830dfc14892f13b7b31dd238e3279224&chksm=8352f9202dee44dfe5474b49d396a20a83b21d9ad2bc290f0a3ec2514041a5a8db9f8ce6b8e8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649505530&idx=1&sn=830dfc14892f13b7b31dd238e3279224&chksm=8352f9202dee44dfe5474b49d396a20a83b21d9ad2bc290f0a3ec2514041a5a8db9f8ce6b8e8#rd", "authors": ["\u5fae\u8f6f\u4e9a\u6d32\u7814\u7a76\u9662"], "title": "VL Norm\uff1a\u8ba9<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u66f4\u7a33\u3001\u66f4\u5feb\u7684\u5173\u952e\u4e00\u6b65", "comment": "Source: WeChat, Published: 2025-10-22 09:00:00", "summary": "\u6210\u4e3a\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u7684\u91cd\u8981\u65b9\u5411\u3002\u7531\u5fae\u8f6f\u4e9a\u6d32\u7814\u7a76\u9662\u4e0e\u6e05\u534e\u5927\u5b66\u8054\u5408\u63d0\u51fa\u7684 VL Norm \u65b9\u6cd5\uff0c\u9488\u5bf9\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u4e2d\u56e0\u8f93\u51fa\u957f\u5ea6\u6ce2\u52a8\u5bfc\u81f4\u7684\u68af\u5ea6\u65b9\u5dee\u8fc7\u5927\u95ee\u9898\uff0c\u7ed9\u51fa\u4e86\u7406\u8bba\u4e0a\u65e0\u504f\u4e14\u65b9\u5dee\u6700\u5c0f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "AI": {"tldr": "\u6210\u4e3a\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u7684\u91cd\u8981\u65b9\u5411\u3002\u7531\u5fae\u8f6f\u4e9a\u6d32\u7814\u7a76\u9662\u4e0e\u6e05\u534e\u5927\u5b66\u8054\u5408\u63d0\u51fa\u7684 VL Norm \u65b9\u6cd5\uff0c\u9488\u5bf9\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u4e2d\u56e0\u8f93\u51fa\u957f\u5ea6\u6ce2\u52a8\u5bfc\u81f4\u7684\u68af\u5ea6\u65b9\u5dee\u8fc7\u5927\u95ee\u9898\uff0c\u7ed9\u51fa\u4e86\u7406\u8bba\u4e0a\u65e0\u504f\u4e14\u65b9\u5dee\u6700\u5c0f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.585c6e98", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650996983&idx=3&sn=4c3e3d70f24ff5b5a61e6bd516bef7a9&chksm=8546184462364132e451785d81914f451bfa03b3800a782b2ee6cf1aa59bcd28850bda7d24da#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650996983&idx=3&sn=4c3e3d70f24ff5b5a61e6bd516bef7a9&chksm=8546184462364132e451785d81914f451bfa03b3800a782b2ee6cf1aa59bcd28850bda7d24da#rd", "authors": ["\u673a\u5668\u4e4b\u5fc3"], "title": "\u667a\u6e90\u5f00\u6e90EditScore\uff1a\u4e3a\u56fe\u50cf\u7f16\u8f91\u89e3\u9501\u5728\u7ebf<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u65e0\u9650\u53ef\u80fd", "comment": "Source: WeChat, Published: 2025-10-22 03:29:57", "summary": "\u56e2\u961f\u8868\u793a\uff0c\u540e\u7eed\u5c06\u9646\u7eed\u53d1\u5e03\u5e94\u4e8e OmniGen2 \u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4ee3\u7801\uff0c\u4ee5\u53ca\u9488\u5bf9 OmniGen2\u3001Flux-dev-Kontext \u548c Qwen-Image-Edit \u7684 Best-of-N \u63a8\u7406\u811a\u672c\uff0c\u6b22\u8fce\u793e\u533a\u6301\u7eed\u5173\u6ce8\u3002", "AI": {"tldr": "\u56e2\u961f\u8868\u793a\uff0c\u540e\u7eed\u5c06\u9646\u7eed\u53d1\u5e03\u5e94\u4e8e OmniGen2 \u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4ee3\u7801\uff0c\u4ee5\u53ca\u9488\u5bf9 OmniGen2\u3001Flux-dev-Kontext \u548c Qwen-Image-Edit \u7684 Best-of-N \u63a8\u7406\u811a\u672c\uff0c\u6b22\u8fce\u793e\u533a\u6301\u7eed\u5173\u6ce8\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.670d88df", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671489&idx=2&sn=a0f05021c664f517be3349c3be4aa085&chksm=fd0597900239ca6de06c8699cbd993a7c04c08bf72c5d678d0bea7c8800a02bf22c54373e7d1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671489&idx=2&sn=a0f05021c664f517be3349c3be4aa085&chksm=fd0597900239ca6de06c8699cbd993a7c04c08bf72c5d678d0bea7c8800a02bf22c54373e7d1#rd", "authors": ["\u4e13\u77e5"], "title": "\u3010\u535a\u58eb\u8bba\u6587\u3011\u7528\u4e8e\u6392\u5e8f\u4e0e\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u3001\u9ad8\u6548\u4e0e\u9c81\u68d2<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-10-22 03:00:00", "summary": "\u8bba\u6587\u7684\u6700\u540e\u4e00\u90e8\u5206\u63a2\u8ba8\u4e86**\u751f\u6210\u5f0f\u5f3a\u5316\u5b66\u4e60\uff08generative RL\uff09\u4e2d\u6548\u7387\u4e0e\u6548\u679c\u4e4b\u95f4\u7684\u6743\u8861\u3002\u901a\u8fc7\u5bf9 PPO \u4e0e REINFORCE \u7684\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Leave-One-Out PPO\uff08LOOP\uff09**\u7b97\u6cd5\u3002", "AI": {"tldr": "\u8bba\u6587\u7684\u6700\u540e\u4e00\u90e8\u5206\u63a2\u8ba8\u4e86**\u751f\u6210\u5f0f\u5f3a\u5316\u5b66\u4e60\uff08generative RL\uff09\u4e2d\u6548\u7387\u4e0e\u6548\u679c\u4e4b\u95f4\u7684\u6743\u8861\u3002\u901a\u8fc7\u5bf9 PPO \u4e0e REINFORCE \u7684\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Leave-One-Out PPO\uff08LOOP\uff09**\u7b97\u6cd5\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.bd488141", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyMTE3MzY1MQ==&mid=2247486903&idx=1&sn=ebc6d6a7266dc7baca036a0443546df3&chksm=c01d58cda4bcd4ae903d358d56db537f9d97ca785c5412bd22c4bb9eecd9454851ab7a529f95#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyMTE3MzY1MQ==&mid=2247486903&idx=1&sn=ebc6d6a7266dc7baca036a0443546df3&chksm=c01d58cda4bcd4ae903d358d56db537f9d97ca785c5412bd22c4bb9eecd9454851ab7a529f95#rd", "authors": ["\u66a8\u5927\u7ecf\u7ba1\u56fd\u5bb6\u7ea7\u5b9e\u9a8c\u6559\u5b66\u793a\u8303\u4e2d\u5fc3"], "title": "\u7b2c150\u671f |  LLM\u80cc\u666f\u4e0b<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7ecf\u5178\u7b97\u6cd5\u4f18\u5316", "comment": "Source: WeChat, Published: 2025-10-22 02:16:07", "summary": "3.\u4e3b\u9898\uff1aLLM\u80cc\u666f\u4e0b\u5f3a\u5316\u5b66\u4e60\u7ecf\u5178\u7b97\u6cd5\u4f18\u53164.\u6c47\u62a5\u4eba\uff1a\u5f20\u6c38\u667a\uff08\u66a8\u5927\u7ba1\u79d1\uff09\u672c\u671f\u6d3b\u52a8\u7b80\u4ecb\u5728\u5f53\u4e0b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u80cc\u666f\uff0c\u5982\u4f55\u8ba9\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\u3001\u5b66\u4e60\u590d\u6742\u504f\u597d\uff0c\u5df2\u6210\u4e3a\u6a21\u578b\u5bf9\u9f50\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u8bae\u9898\u3002", "AI": {"tldr": "3.\u4e3b\u9898\uff1aLLM\u80cc\u666f\u4e0b\u5f3a\u5316\u5b66\u4e60\u7ecf\u5178\u7b97\u6cd5\u4f18\u53164.\u6c47\u62a5\u4eba\uff1a\u5f20\u6c38\u667a\uff08\u66a8\u5927\u7ba1\u79d1\uff09\u672c\u671f\u6d3b\u52a8\u7b80\u4ecb\u5728\u5f53\u4e0b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u80cc\u666f\uff0c\u5982\u4f55\u8ba9\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\u3001\u5b66\u4e60\u590d\u6742\u504f\u597d\uff0c\u5df2\u6210\u4e3a\u6a21\u578b\u5bf9\u9f50\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u8bae\u9898\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.1c278474", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4OTEwNjMzMA==&mid=2247720944&idx=1&sn=3853950ae0718834d1c174905c294045&chksm=ce22e619cf98352c6954837b5a5922e280d0ea15205ef301e26087ba57bdc9314b8af7299045#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4OTEwNjMzMA==&mid=2247720944&idx=1&sn=3853950ae0718834d1c174905c294045&chksm=ce22e619cf98352c6954837b5a5922e280d0ea15205ef301e26087ba57bdc9314b8af7299045#rd", "authors": ["arXiv\u6bcf\u65e5\u5b66\u672f\u901f\u9012"], "title": "\u6beb\u65e0\u7591\u95ee\uff0c\u672a\u6765AI\u754c\u5c06\u4f1a\u662f<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u5929\u4e0b", "comment": "Source: WeChat, Published: 2025-10-22 01:29:42", "summary": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "AI": {"tldr": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.46ad7e81", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU0NTAyNTQ1OQ==&mid=2247544454&idx=1&sn=771936fb82936ea467b454126af019bf&chksm=fa543636ab40364675c492dcbb11bd5298ee9ce321fde7875e8a5038e365a319bf03d5365bb3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU0NTAyNTQ1OQ==&mid=2247544454&idx=1&sn=771936fb82936ea467b454126af019bf&chksm=fa543636ab40364675c492dcbb11bd5298ee9ce321fde7875e8a5038e365a319bf03d5365bb3#rd", "authors": ["\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u9662"], "title": "\u6beb\u65e0\u7591\u95ee\uff0c\u672a\u6765AI\u754c\u5c06\u4f1a\u662f<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u5929\u4e0b", "comment": "Source: WeChat, Published: 2025-10-22 01:01:21", "summary": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "AI": {"tldr": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.85dd3060", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NzUxNTU1OA==&mid=2247683630&idx=4&sn=9d2b9b2735f46c0661d77fb3a0690cf7&chksm=cf656d9a6f9a965c59ac3e740631d4df997c788018b70124f6f170f3808657f07367a1f64697#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NzUxNTU1OA==&mid=2247683630&idx=4&sn=9d2b9b2735f46c0661d77fb3a0690cf7&chksm=cf656d9a6f9a965c59ac3e740631d4df997c788018b70124f6f170f3808657f07367a1f64697#rd", "authors": ["\u81ea\u52a8\u9a7e\u9a76\u4e4b\u5fc3"], "title": "\u5927\u4f6c\u5f00\u70ae\uff1a\u667a\u80fd\u4f53\u90fd\u5728\u88c5\u6837\u5b50\uff0c<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5f88\u7cdf\u7cd5\uff0cAGI \u5341\u5e74\u4e5f\u51fa\u4e0d\u6765", "comment": "Source: WeChat, Published: 2025-10-22 00:01:44", "summary": "\u5f3a\u5316\u5b66\u4e60\u662f\u4eba\u5de5\u667a\u80fd\u7684\u53e6\u4e00\u4e2a\u91cd\u8981\u9886\u57df\uff0c\u5927\u6982\u6709\u4e24\u4e09\u5e74\u751a\u81f3\u56db\u5e74\u7684\u65f6\u95f4\uff0c\u6bcf\u4e2a\u4eba\u90fd\u5728\u6e38\u620f\u4e0a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002\u8fd9\u5b8c\u5168\u662f\u4e00\u4e2a\u5931\u8bef\u3002\u6211\u5728 OpenAI \u5c1d\u8bd5\u505a\u7684\u4e8b\u60c5\u662f\uff0c\u6211\u4e00\u76f4\u5bf9\u6e38\u620f\u80fd\u5426\u5f15\u9886 AGI \u6709\u70b9\u6000\u7591 \u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u662f\u4eba\u5de5\u667a\u80fd\u7684\u53e6\u4e00\u4e2a\u91cd\u8981\u9886\u57df\uff0c\u5927\u6982\u6709\u4e24\u4e09\u5e74\u751a\u81f3\u56db\u5e74\u7684\u65f6\u95f4\uff0c\u6bcf\u4e2a\u4eba\u90fd\u5728\u6e38\u620f\u4e0a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002\u8fd9\u5b8c\u5168\u662f\u4e00\u4e2a\u5931\u8bef\u3002\u6211\u5728 OpenAI \u5c1d\u8bd5\u505a\u7684\u4e8b\u60c5\u662f\uff0c\u6211\u4e00\u76f4\u5bf9\u6e38\u620f\u80fd\u5426\u5f15\u9886 AGI \u6709\u70b9\u6000\u7591 \u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.92df2e28", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5Mjc3MjA5Nw==&mid=2247488958&idx=1&sn=1f8f06d2a650e0e681692052bb1455a6&chksm=c15ecff26f5dfccf459d0324ac98d522bbd304e35a25c56e1b7adafe37618e64632d21dba9da#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5Mjc3MjA5Nw==&mid=2247488958&idx=1&sn=1f8f06d2a650e0e681692052bb1455a6&chksm=c15ecff26f5dfccf459d0324ac98d522bbd304e35a25c56e1b7adafe37618e64632d21dba9da#rd", "authors": ["\u5177\u8eab\u667a\u80fd\u7814\u7a76\u5ba4"], "title": "\u7edf\u4e00 AMP\u3001DeepMimic\u3001ASE\u3001ADD \u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u52a8\u4f5c\u6a21\u4eff\u5e73\u53f0\uff01MimicKit \u6846\u67b6\u6df1\u5ea6\u89e3\u6790", "comment": "Source: WeChat, Published: 2025-10-22 00:00:25", "summary": "mimickit \u7684\u7b97\u6cd5\u6838\u5fc3\uff0c\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e0b\u8fdb\u884c\u52a8\u4f5c\u6a21\u4eff\u3002\u5176\u76ee\u6807\u4e0d\u518d\u662f\u8ba9\u667a\u80fd\u4f53\u201c\u901a\u8fc7\u5956\u52b1\u81ea\u5df1\u6478\u7d22\u201d\uff0c\u800c\u662f\u901a\u8fc7\u6570\u636e\u5b66\u4e60\u4eba\u7c7b\u7684\u8fd0\u52a8\u5206\u5e03\u3002\u5728\u4f20\u7edf RL \u4e2d\uff0c\u667a\u80fd\u4f53\u4f9d\u9760\u7a00\u758f\u5956\u52b1\u548c\u5927\u91cf\u91c7\u6837\u5b66\u4f1a\u4efb\u52a1\uff1b", "AI": {"tldr": "mimickit \u7684\u7b97\u6cd5\u6838\u5fc3\uff0c\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e0b\u8fdb\u884c\u52a8\u4f5c\u6a21\u4eff\u3002\u5176\u76ee\u6807\u4e0d\u518d\u662f\u8ba9\u667a\u80fd\u4f53\u201c\u901a\u8fc7\u5956\u52b1\u81ea\u5df1\u6478\u7d22\u201d\uff0c\u800c\u662f\u901a\u8fc7\u6570\u636e\u5b66\u4e60\u4eba\u7c7b\u7684\u8fd0\u52a8\u5206\u5e03\u3002\u5728\u4f20\u7edf RL \u4e2d\uff0c\u667a\u80fd\u4f53\u4f9d\u9760\u7a00\u758f\u5956\u52b1\u548c\u5927\u91cf\u91c7\u6837\u5b66\u4f1a\u4efb\u52a1\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.f74d33a6", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0NzY3ODMxMA==&mid=2247488086&idx=1&sn=bdb6e6ed5bf72b1f141f9c2a33d2d6d8&chksm=c2f7831ac1017c830f9db97f8fcab78fde3b128365de97ee83d35edb6c9a38d7c423de050c02#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0NzY3ODMxMA==&mid=2247488086&idx=1&sn=bdb6e6ed5bf72b1f141f9c2a33d2d6d8&chksm=c2f7831ac1017c830f9db97f8fcab78fde3b128365de97ee83d35edb6c9a38d7c423de050c02#rd", "authors": ["AI\u65b0\u6587"], "title": "\u3010\u4e13\u9898\u3011AI\u9886\u57df\u4e2d\u7684\u201c<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u201d\u76f8\u5173\u7814\u7a76-2025\u5e749-10\u6708", "comment": "Source: WeChat, Published: 2025-10-21 23:01:10", "summary": "\u539f\u6587\u94fe\u63a5 \u65f6\u6ede\u5fae\u5206\u5bf9\u7b56\u6b3a\u9a97\u8d44\u6e90\u914d\u7f6e\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5 \u539f\u6807\u9898\uff1aA Deep Reinforcement Learning Approach to Time Delay Differential Game Deception Resource Deployment\u4f5c\u8005\uff1aWeizhen He\uff1b", "AI": {"tldr": "\u539f\u6587\u94fe\u63a5 \u65f6\u6ede\u5fae\u5206\u5bf9\u7b56\u6b3a\u9a97\u8d44\u6e90\u914d\u7f6e\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5 \u539f\u6807\u9898\uff1aA Deep Reinforcement Learning Approach to Time Delay Differential Game Deception Resource Deployment\u4f5c\u8005\uff1aWeizhen He\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.db6a97a4", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NDgyODI4MQ==&mid=2247487544&idx=1&sn=42bf26fe4b843976cee48db476b85735&chksm=a7e29cc74249b001354e80d10f658ce64a9b8ce46bc245fcb87d3c8185abf861880571c62a17#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NDgyODI4MQ==&mid=2247487544&idx=1&sn=42bf26fe4b843976cee48db476b85735&chksm=a7e29cc74249b001354e80d10f658ce64a9b8ce46bc245fcb87d3c8185abf861880571c62a17#rd", "authors": ["\u4e91\u8c26\u548c\u4ed6\u7684\u670b\u53cb\u4eec"], "title": "11 \u6708 22 \u65e5\uff0c\u6211\u60f3\u548c\u4f60\u804a\u804a\u300cAI \u5199<em class=\"highlight\">\u4ee3\u7801</em>\u300d\u8fd9\u4ef6\u4e8b", "comment": "Source: WeChat, Published: 2025-10-22 04:18:45", "summary": "\u533a\u522b\u5176\u4ed6\u7684 code agent\uff0c\u4ed6\u4e5f\u6709\u81ea\u5df1\u7684\u4e00\u4e9b\u7279\u6027\uff0c\u6bd4\u5982\u4e8c\u5f00\u53cb\u597d\u3001spec driven\u3001\u591a\u7aef\u7b49\u3002neovate heovate tips to getting started\uff1a 1. input a task\u30022. /init to create aneovate.md file\u3002", "AI": {"tldr": "\u533a\u522b\u5176\u4ed6\u7684 code agent\uff0c\u4ed6\u4e5f\u6709\u81ea\u5df1\u7684\u4e00\u4e9b\u7279\u6027\uff0c\u6bd4\u5982\u4e8c\u5f00\u53cb\u597d\u3001spec driven\u3001\u591a\u7aef\u7b49\u3002neovate heovate tips to getting started\uff1a 1. input a task\u30022. /init to create aneovate.md file\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.39285d9b", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483746&idx=1&sn=48a23ec1bab148828f8df22170cc9596&chksm=e99570bafd039320116f0b859dc5d403fcdcf9144437dd3d6a8b45ec388a332910cb03f47f73#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483746&idx=1&sn=48a23ec1bab148828f8df22170cc9596&chksm=e99570bafd039320116f0b859dc5d403fcdcf9144437dd3d6a8b45ec388a332910cb03f47f73#rd", "authors": ["CodeAgent\u4ee3\u7801\u667a\u80fd"], "title": "\u3010AI\u8981\u95fb\u00b7\u65e5\u62a5\u3011<em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em> \u00b7 Code LLMs \u00b7\u00a0\u521d\u521b\u52a8\u6001\uff0810.21\uff09part3", "comment": "Source: WeChat, Published: 2025-10-21 16:00:47", "summary": "Anthropic Claude Code Web \u7248\u4e0e\u6c99\u7bb1\u5e73\u53f0\uff08Agent/Release\uff09DeepSeekOCR \u6a21\u578b\u4e0e\u89c6\u89c9\u8bb0\u5fc6\u538b\u7f29\uff08Code LLM \u7814\u7a76\uff09Karpathy nanochat\uff1a\u53ef\u9ed1\u5ba2\u5316\u7684\u5c0f\u578b\u804a\u5929\u6a21\u578b\uff08Code LLM \u7814\u7a76\uff09", "AI": {"tldr": "Anthropic Claude Code Web \u7248\u4e0e\u6c99\u7bb1\u5e73\u53f0\uff08Agent/Release\uff09DeepSeekOCR \u6a21\u578b\u4e0e\u89c6\u89c9\u8bb0\u5fc6\u538b\u7f29\uff08Code LLM \u7814\u7a76\uff09Karpathy nanochat\uff1a\u53ef\u9ed1\u5ba2\u5316\u7684\u5c0f\u578b\u804a\u5929\u6a21\u578b\uff08Code LLM \u7814\u7a76\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.152faf34", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483741&idx=1&sn=e72e5c62222db42290de8230d20902a4&chksm=e9d3addc0eb42c0e1e5a237336efc7d19a3186aa30e5d2758220fddb5565f516cc5995d19c08#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483741&idx=1&sn=e72e5c62222db42290de8230d20902a4&chksm=e9d3addc0eb42c0e1e5a237336efc7d19a3186aa30e5d2758220fddb5565f516cc5995d19c08#rd", "authors": ["CodeAgent\u4ee3\u7801\u667a\u80fd"], "title": "\u3010AI\u8981\u95fb\u00b7\u65e5\u62a5\u3011<em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em> \u00b7 Code LLMs \u00b7 \u521d\u521b\u52a8\u6001\uff0810.21\uff09part 2", "comment": "Source: WeChat, Published: 2025-10-21 15:56:14", "summary": "Codev\uff1a\u591a\u4ee3\u7406\u7f16\u7801\u5e73\u53f0\u5b9e\u6d4b\uff08Agent / Startup\uff09GitHub\uff1a9 \u4e2a MCP \u5f00\u6e90\u9879\u76ee\uff08Release\uff09ScaleRL\uff1aRL \u540e\u8bad\u7ec3 S \u578b\u66f2\u7ebf\uff08Code LLM Research\uff09Windsurf Editor 1.12.21 \u8865\u4e01\uff08Release\uff09", "AI": {"tldr": "Codev\uff1a\u591a\u4ee3\u7406\u7f16\u7801\u5e73\u53f0\u5b9e\u6d4b\uff08Agent / Startup\uff09GitHub\uff1a9 \u4e2a MCP \u5f00\u6e90\u9879\u76ee\uff08Release\uff09ScaleRL\uff1aRL \u540e\u8bad\u7ec3 S \u578b\u66f2\u7ebf\uff08Code LLM Research\uff09Windsurf Editor 1.12.21 \u8865\u4e01\uff08Release\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.0136d5ce", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483736&idx=1&sn=165b97cee7e74a7e1e82fbb35dc9d0af&chksm=e95eaad8738413517ce1516c03b604785ebc8816707d282c284c00df9cfc24d471de352d83fa#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483736&idx=1&sn=165b97cee7e74a7e1e82fbb35dc9d0af&chksm=e95eaad8738413517ce1516c03b604785ebc8816707d282c284c00df9cfc24d471de352d83fa#rd", "authors": ["CodeAgent\u4ee3\u7801\u667a\u80fd"], "title": "\u3010AI\u8981\u95fb\u00b7\u65e5\u62a5\u3011<em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em> \u00b7 Code LLMs \u00b7 \u521d\u521b\u52a8\u6001\uff0810.21\uff09part 1", "comment": "Source: WeChat, Published: 2025-10-21 15:52:47", "summary": "1|\u3010\u7c7b\u578b\uff1acode llm \u7814\u7a76\u3011\u2014 \u81ea\u6539\u5584\u4ee3\u7406\u6846\u67b6 ace\uff08et\uff1a20251018\uff1b\u7a97\u53e3\uff1a36h\uff09\u6765\u6e90\uff1aInfoQ \u2013 Robert Krzaczy\u0144ski\uff08ET 20251018\uff09\uff5c\u94fe\u63a5\uff1ahttps\uff1a//www.infoq.com/news/2025/10/agentic-context-eng/\uff5c\u4e00\u53e5\u8bdd\u7ed3\u8bba\uff1aACE \u7528\u201c\u751f\u6210\u53cd\u601d\u6574\u7406\u201d\u4e09\u5143\u7ed3\u6784\u5bf9\u4e0a\u4e0b\u6587\u505a", "AI": {"tldr": "1|\u3010\u7c7b\u578b\uff1acode llm \u7814\u7a76\u3011\u2014 \u81ea\u6539\u5584\u4ee3\u7406\u6846\u67b6 ace\uff08et\uff1a20251018\uff1b\u7a97\u53e3\uff1a36h\uff09\u6765\u6e90\uff1aInfoQ \u2013 Robert Krzaczy\u0144ski\uff08ET 20251018\uff09\uff5c\u94fe\u63a5\uff1ahttps\uff1a//www.infoq.com/news/2025/10/agentic-context-eng/\uff5c\u4e00\u53e5\u8bdd\u7ed3\u8bba\uff1aACE \u7528\u201c\u751f\u6210\u53cd\u601d\u6574\u7406\u201d\u4e09\u5143\u7ed3\u6784\u5bf9\u4e0a\u4e0b\u6587\u505a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.639b01de", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI1MzEwMzIwOQ==&mid=2247503965&idx=1&sn=d13fe477cb5186c81d45d435d1456795&chksm=e88431ef9c4c4503a6404b8dab306e0f3ccfeb6ecf6be48992a7570d96fd5e5f25aa3518fd01#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI1MzEwMzIwOQ==&mid=2247503965&idx=1&sn=d13fe477cb5186c81d45d435d1456795&chksm=e88431ef9c4c4503a6404b8dab306e0f3ccfeb6ecf6be48992a7570d96fd5e5f25aa3518fd01#rd", "authors": ["\u9752\u7a1eAI"], "title": "\u6700\u65b0\u7efc\u8ff0\uff0170+\u9875\u8bba\u6587\uff0c\u7cfb\u7edf\u68b3\u7406<em class=\"highlight\">Agentic</em>\u6784\u5efa\u8303\u5f0f\u8f6c\u53d8", "comment": "Source: WeChat, Published: 2025-10-22 09:54:20", "summary": "Beyond Pipelines\uff1a A Survey of the Paradigm Shift toward Model-Native Agentic AI. J. ACM 1\uff0c 1 \uff08October 2025\uff09\uff0cJinlin Xiao\uff0cJiarun Han\uff0c Beijing Jiaotong University\uff0c Beijing\uff0c China\uff0cXiaoyi Chen\uff0cShuyu Wei\uff0c Beijing Jiaotong University\uff0c Beijing\uff0c China\uff0c", "AI": {"tldr": "Beyond Pipelines\uff1a A Survey of the Paradigm Shift toward Model-Native Agentic AI. J. ACM 1\uff0c 1 \uff08October 2025\uff09\uff0cJinlin Xiao\uff0cJiarun Han\uff0c Beijing Jiaotong University\uff0c Beijing\uff0c China\uff0cXiaoyi Chen\uff0cShuyu Wei\uff0c B...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.38fd242b", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzODkwODYwMA==&mid=2247486162&idx=1&sn=d5b175a4d42b33c0f1d293f139a7d345&chksm=c37e47b605bb02a59c32b5d7ac4d14a98efcf920f3642aecbfe301a66a3f406141c9bead42f0#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzODkwODYwMA==&mid=2247486162&idx=1&sn=d5b175a4d42b33c0f1d293f139a7d345&chksm=c37e47b605bb02a59c32b5d7ac4d14a98efcf920f3642aecbfe301a66a3f406141c9bead42f0#rd", "authors": ["\u79d1\u53d4AI\u8fdb\u5316\u8bb0"], "title": "\u9ea6\u80af\u9521\uff1a<em class=\"highlight\">\u4ee3\u7406</em>\u5f0f\u7ec4\u7ec7 (<em class=\"highlight\">Agentic</em> Organization) AI\u65f6\u4ee3\u7684\u4e0b\u4e00\u4e2a\u8303\u5f0f", "comment": "Source: WeChat, Published: 2025-10-22 09:51:05", "summary": "com/capabilities/people-and-organizational-performance/our-insights/the-agentic-organization-contours-of-the-next-paradigm-for-the-ai-era#/", "AI": {"tldr": "com/capabilities/people-and-organizational-performance/our-insights/the-agentic-organization-contours-of-the-next-paradigm-for-the-ai-era#/", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.b55a2c8c", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1Mzc4MjA1NQ==&mid=2247487007&idx=3&sn=0b0250c8086a9ed5e2370201759d6b40&chksm=fa00ae2a38275fcc9b8dc9976ea5533444785febc955315021ab6707315489217b433b84995c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1Mzc4MjA1NQ==&mid=2247487007&idx=3&sn=0b0250c8086a9ed5e2370201759d6b40&chksm=fa00ae2a38275fcc9b8dc9976ea5533444785febc955315021ab6707315489217b433b84995c#rd", "authors": ["\u5f00\u6e90\u65f6\u523b"], "title": "Open Agent Summit | \u805a\u7126 <em class=\"highlight\">Agentic</em> AI\u7684\u76db\u4f1a", "comment": "Source: WeChat, Published: 2025-10-22 09:46:59", "summary": "The AI Scientist-v2\uff1a Workshop-Level Automated Scientific Discovery via Agentic Tree Search - Yutaro Yamada\uff0c Sakanaldea generation tree-based experimentation paper write-up llm idea/plan\u30021. preliminary ldea investigatione plotting + innovation vlm feedback [write to exp. logl [select best nodel", "AI": {"tldr": "The AI Scientist-v2\uff1a Workshop-Level Automated Scientific Discovery via Agentic Tree Search - Yutaro Yamada\uff0c Sakanaldea generation tree-based experimentation paper write-up llm idea/plan\u30021. preliminary...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.df354a91", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMTI1ODM4MQ==&mid=2247484067&idx=1&sn=894fe0f0207081008c6c1dc5e7a2dbeb&chksm=fe897927cffa006ec249cbf5ef3e1f15565e1a1432087df46ec11db71809c42baee49afb7300#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMTI1ODM4MQ==&mid=2247484067&idx=1&sn=894fe0f0207081008c6c1dc5e7a2dbeb&chksm=fe897927cffa006ec249cbf5ef3e1f15565e1a1432087df46ec11db71809c42baee49afb7300#rd", "authors": ["AI \u77e5\u884c\u793e Lab"], "title": "<em class=\"highlight\">Agentic</em> \u6846\u67b6\u7cfb\u5217\uff08\u4e8c\uff09\uff1a\u4e3b\u6d41\u751f\u6001\u5bf9\u6bd4", "comment": "Source: WeChat, Published: 2025-10-22 09:20:59", "summary": "\u5728\u7b2c\u4e00\u7bc7\u4e2d\u6211\u4eec\u4e86\u89e3\u4e86 Agentic \u6846\u67b6\u7684\u6838\u5fc3\u7406\u5ff5\u2014\u2014\u8ba9\u5927\u6a21\u578b\u5177\u5907\u300c\u8ba1\u5212\u3001\u6267\u884c\u3001\u53cd\u601d\u4e0e\u534f\u4f5c\u300d\u7684\u80fd\u529b\u3002\u672c\u7bc7\uff0c\u6211\u4eec\u805a\u7126\u76ee\u524d\u6700\u6d3b\u8dc3\u7684\u56db\u5927 Agentic \u6846\u67b6\u751f\u6001\uff1aLangGraph\u3001AutoGen\u3001CrewAI\u3001MCP\u3002", "AI": {"tldr": "\u5728\u7b2c\u4e00\u7bc7\u4e2d\u6211\u4eec\u4e86\u89e3\u4e86 Agentic \u6846\u67b6\u7684\u6838\u5fc3\u7406\u5ff5\u2014\u2014\u8ba9\u5927\u6a21\u578b\u5177\u5907\u300c\u8ba1\u5212\u3001\u6267\u884c\u3001\u53cd\u601d\u4e0e\u534f\u4f5c\u300d\u7684\u80fd\u529b\u3002\u672c\u7bc7\uff0c\u6211\u4eec\u805a\u7126\u76ee\u524d\u6700\u6d3b\u8dc3\u7684\u56db\u5927 Agentic \u6846\u67b6\u751f\u6001\uff1aLangGraph\u3001AutoGen\u3001CrewAI\u3001MCP\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.c7fd5e96", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651259717&idx=4&sn=72aa398887ced46784cc3437e2d2c8ec&chksm=bcaef9ea3caad84f220e099e0598926a573848e9b03e792e76987b5191cef38191d323e337f1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651259717&idx=4&sn=72aa398887ced46784cc3437e2d2c8ec&chksm=bcaef9ea3caad84f220e099e0598926a573848e9b03e792e76987b5191cef38191d323e337f1#rd", "authors": ["InfoQ"], "title": "\u4f01\u4e1a\u5e7f\u6cdb\u91c7\u7528 <em class=\"highlight\">Agentic</em> AI\uff0c\u4f46\u9886\u5bfc\u5c42\u7406\u89e3\u6ede\u540e", "comment": "Source: WeChat, Published: 2025-10-22 09:03:50", "summary": "https\uff1a//www.infoq.com/news/2025/10/agentic-ai-adoption-leadership/\u58f0\u660e\uff1a\u672c\u6587\u4e3a InfoQ \u7ffb\u8bd1\uff0c\u672a\u7ecf\u8bb8\u53ef\u7981\u6b62\u8f6c\u8f7d\u3002\u70b9\u51fb\u5e95\u90e8\u9605\u8bfb\u539f\u6587\u8bbf\u95ee InfoQ \u5b98\u7f51\uff0c\u83b7\u53d6\u66f4\u591a\u7cbe\u5f69\u5185\u5bb9\uff01", "AI": {"tldr": "https\uff1a//www.infoq.com/news/2025/10/agentic-ai-adoption-leadership/\u58f0\u660e\uff1a\u672c\u6587\u4e3a InfoQ \u7ffb\u8bd1\uff0c\u672a\u7ecf\u8bb8\u53ef\u7981\u6b62\u8f6c\u8f7d\u3002\u70b9\u51fb\u5e95\u90e8\u9605\u8bfb\u539f\u6587\u8bbf\u95ee InfoQ \u5b98\u7f51\uff0c\u83b7\u53d6\u66f4\u591a\u7cbe\u5f69\u5185\u5bb9\uff01", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.a2e90555", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MjYyOTQ0Ng==&mid=2648402951&idx=1&sn=66666c979daa0b733e21a0e5d49444b1&chksm=868e5b9c9fae341aed59ffafd4aaac661e7692a9f27ea98672ac8ad63d11a4c841c585e9ead5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MjYyOTQ0Ng==&mid=2648402951&idx=1&sn=66666c979daa0b733e21a0e5d49444b1&chksm=868e5b9c9fae341aed59ffafd4aaac661e7692a9f27ea98672ac8ad63d11a4c841c585e9ead5#rd", "authors": ["\u805a\u641c"], "title": "\u4e91\u6816\u5927\u4f1a\u91cd\u78c5\u53d1\u5e03\uff1a\u963f\u91cc\u4e91AI\u641c\u7d22\u8fc8\u5411<em class=\"highlight\">Agentic</em>\u65f6\u4ee3\uff0c\u56db\u5927\u6280\u672f\u4eae\u70b9\u91cd\u5851\u667a\u80fd\u68c0\u7d22", "comment": "Source: WeChat, Published: 2025-10-22 08:52:38", "summary": "agentic search\u7684\u6838\u5fc3\u5728\u4e8e\u901a\u8fc7\u5927\u6a21\u578b\u81ea\u4e3b\u51b3\u7b56\u201c\u4f55\u65f6\u3001\u4f55\u5730\u3001\u5982\u4f55\u201d\u6267\u884c\u641c\u7d22\u4efb\u52a1\uff0c\u5b9e\u73b0\u590d\u6742\u4efb\u52a1\u89c4\u5212\u3002\u963f\u91cc\u4e91AI\u641c\u7d22\u5f00\u653e\u5e73\u53f0\u5df2\u5b9e\u73b0\u4e09\u9636\u6bb5\u6280\u672f\u7684\u878d\u5408\uff0c\u5728\u4fdd\u7559\u4f20\u7edf\u641c\u7d22\u9ad8\u6548\u6027\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u8d4b\u80fd\u52a8\u6001\u4efb\u52a1\u6267\u884c\uff0c\u4e3a\u5ba2\u6237\u63d0\u4f9b", "AI": {"tldr": "agentic search\u7684\u6838\u5fc3\u5728\u4e8e\u901a\u8fc7\u5927\u6a21\u578b\u81ea\u4e3b\u51b3\u7b56\u201c\u4f55\u65f6\u3001\u4f55\u5730\u3001\u5982\u4f55\u201d\u6267\u884c\u641c\u7d22\u4efb\u52a1\uff0c\u5b9e\u73b0\u590d\u6742\u4efb\u52a1\u89c4\u5212\u3002\u963f\u91cc\u4e91AI\u641c\u7d22\u5f00\u653e\u5e73\u53f0\u5df2\u5b9e\u73b0\u4e09\u9636\u6bb5\u6280\u672f\u7684\u878d\u5408\uff0c\u5728\u4fdd\u7559\u4f20\u7edf\u641c\u7d22\u9ad8\u6548\u6027\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u8d4b\u80fd\u52a8\u6001\u4efb\u52a1\u6267\u884c\uff0c\u4e3a\u5ba2\u6237\u63d0\u4f9b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.b2cc1899", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzY0MDAxMDMwNA==&mid=2247483684&idx=1&sn=f016acf1b5db2756aff8827f40682fed&chksm=f1b9f9a99cc6e61c5a562fd22be1216fde267217d162d65d22b2b80692da1098591b279baefa#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzY0MDAxMDMwNA==&mid=2247483684&idx=1&sn=f016acf1b5db2756aff8827f40682fed&chksm=f1b9f9a99cc6e61c5a562fd22be1216fde267217d162d65d22b2b80692da1098591b279baefa#rd", "authors": ["\u65f6\u96e8\u968f\u60f3"], "title": "\u642d\u5efaAI\u201c\u68a6\u4e4b\u961f\u201d\uff1a\u4f60\u5fc5\u987b\u4e86\u89e3\u76845\u4e2a\u81ea\u4e3b<em class=\"highlight\">\u667a\u80fd\u4f53</em>\uff08<em class=\"highlight\">Agentic</em> AI\uff09\u60ca\u4eba\u771f\u76f8", "comment": "Source: WeChat, Published: 2025-10-22 07:27:00", "summary": "1. AI \u81ea\u6211\u53cd\u601d\uff1a\u667a\u80fd\u4f53\u4e5f\u4f1a\u201c\u7167\u955c\u5b50\u201d\u63d0\u5347\u81ea\u5df1\u5bf9\u4e8e\u4efb\u4f55\u8ffd\u6c42\u9ad8\u8d28\u91cf\u8f93\u51fa\u7684\u7cfb\u7edf\u800c\u8a00\uff0c\u53ef\u9760\u6027\u90fd\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u5355\u4e00\u63d0\u793a\u6a21\u578b\u7684\u4e00\u4e2a\u4e3b\u8981\u5c40\u9650\u5c31\u5728\u4e8e\u5176\u201c\u4e00\u6b21\u6027\u201d\u751f\u6210\u5185\u5bb9\uff0c\u7f3a\u4e4f\u81ea\u6211\u7ea0\u9519\u673a\u5236\u3002", "AI": {"tldr": "1. AI \u81ea\u6211\u53cd\u601d\uff1a\u667a\u80fd\u4f53\u4e5f\u4f1a\u201c\u7167\u955c\u5b50\u201d\u63d0\u5347\u81ea\u5df1\u5bf9\u4e8e\u4efb\u4f55\u8ffd\u6c42\u9ad8\u8d28\u91cf\u8f93\u51fa\u7684\u7cfb\u7edf\u800c\u8a00\uff0c\u53ef\u9760\u6027\u90fd\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u5355\u4e00\u63d0\u793a\u6a21\u578b\u7684\u4e00\u4e2a\u4e3b\u8981\u5c40\u9650\u5c31\u5728\u4e8e\u5176\u201c\u4e00\u6b21\u6027\u201d\u751f\u6210\u5185\u5bb9\uff0c\u7f3a\u4e4f\u81ea\u6211\u7ea0\u9519\u673a\u5236\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.5fa5b6c2", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk3NTc5ODE2OQ==&mid=2247483875&idx=3&sn=24aceb54ebaed1c422ce1755bf5d7b7d&chksm=c5cb2de7ff104068714011bf93e696168fb8e81a627fe6cbd9d06cc702dbe3c1f709665c651a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk3NTc5ODE2OQ==&mid=2247483875&idx=3&sn=24aceb54ebaed1c422ce1755bf5d7b7d&chksm=c5cb2de7ff104068714011bf93e696168fb8e81a627fe6cbd9d06cc702dbe3c1f709665c651a#rd", "authors": ["\u67ad\u9f99\u4e91\u6280\u672f\u56e2\u961f"], "title": "\u4e00\u6587\u4e86\u89e3<em class=\"highlight\">Agentic</em> RL\u9886\u57df\u6700\u65b0\u8fdb\u5c55", "comment": "Source: WeChat, Published: 2025-10-22 07:13:38", "summary": "\u4eca\u5e74Agentic\u7684\u6982\u5ff5\u7279\u522b\u706b\uff0c\u672c\u8d28\u4e0a\u5c31\u662f\u8ba9\u4e00\u4e2aLLM\u80fd\u505a\u5230\u8fb9\u60f3\u8fb9\u641c\u8fb9\u505a\uff0c\u4e0d\u5bf9\u7684\u8bdd\u5c31\u81ea\u5df1\u53cd\u601d\u518d\u8fb9\u60f3\u8fb9\u641c\u8fb9\u505a\uff0c\u4e00\u76f4\u5230\u4efb\u52a1\u5b8c\u6210\u5f00\u59cb\u3002\u4ece\u53bb\u5e74\u5f00\u59cb\u6211\u4eec\u5c31\u6709\u5728\u505atraining-free\u7684\u5404\u79cdAgentic\u7684\u6280\u672f\uff08\u50cfAgentic Rag\u7b49\u7b49\uff09\uff0c\u8fd9\u51e0\u5929\u6709\u7a7a\u5c31\u7a0d\u5fae\u603b", "AI": {"tldr": "\u4eca\u5e74Agentic\u7684\u6982\u5ff5\u7279\u522b\u706b\uff0c\u672c\u8d28\u4e0a\u5c31\u662f\u8ba9\u4e00\u4e2aLLM\u80fd\u505a\u5230\u8fb9\u60f3\u8fb9\u641c\u8fb9\u505a\uff0c\u4e0d\u5bf9\u7684\u8bdd\u5c31\u81ea\u5df1\u53cd\u601d\u518d\u8fb9\u60f3\u8fb9\u641c\u8fb9\u505a\uff0c\u4e00\u76f4\u5230\u4efb\u52a1\u5b8c\u6210\u5f00\u59cb\u3002\u4ece\u53bb\u5e74\u5f00\u59cb\u6211\u4eec\u5c31\u6709\u5728\u505atraining-free\u7684\u5404\u79cdAgentic\u7684\u6280\u672f\uff08\u50cfAgentic Rag\u7b49\u7b49\uff09\uff0c\u8fd9\u51e0\u5929\u6709\u7a7a\u5c31\u7a0d\u5fae\u603b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.6aac6071", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMzkzODgxNw==&mid=2247508540&idx=1&sn=f5847cc1fee220aa7e5c06efdb912173&chksm=e95fc7752f3c018b128950c4fe450d2147429a0c1be44a72439ff03fa80ed00560ba5681bc6a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMzkzODgxNw==&mid=2247508540&idx=1&sn=f5847cc1fee220aa7e5c06efdb912173&chksm=e95fc7752f3c018b128950c4fe450d2147429a0c1be44a72439ff03fa80ed00560ba5681bc6a#rd", "authors": ["\u5f00\u6e90\u62a5\u544a"], "title": "\u6295\u8d44<em class=\"highlight\">Agentic</em> AI\u662f\u9762\u5411\u672a\u6765\u4f9b\u5e94\u94fe\u7684\u6218\u7565\u4e3e\u63aa", "comment": "Source: WeChat, Published: 2025-10-22 06:59:37", "summary": "agentic ai\uff1a\u4ece\u81ea\u52a8\u5316\u5230\u81ea\u4e3b\u7684\u65c5\u7a0b\u3002\u7f57\u6208\u7814\u7a76 \u4ece\u6570\u5b57\u5316\u96c4\u5fc3\u5230\u81ea\u4e3b\u5316\u73b0\u5b9e\u3002\u4ece\u4f20\u7edf\u804a\u5929\u673a\u5668\u4eba\u548c rpa \u5230\u81ea\u4e3b\u4ee3 \u673a\u5668\u4eba\u6d41\u7a0b\u81ea\u52a8\u5316\uff08rpa\uff09\u5230\u81ea\u4e3b\u4ee3\u7406\u6d41\u7a0b\u81ea\u52a8\u5316\uff08apa\uff09 \u7406--\u8fc8\u5411\u81ea\u4e3b\u7684\u65c5\u7a0b\u3002", "AI": {"tldr": "agentic ai\uff1a\u4ece\u81ea\u52a8\u5316\u5230\u81ea\u4e3b\u7684\u65c5\u7a0b\u3002\u7f57\u6208\u7814\u7a76 \u4ece\u6570\u5b57\u5316\u96c4\u5fc3\u5230\u81ea\u4e3b\u5316\u73b0\u5b9e\u3002\u4ece\u4f20\u7edf\u804a\u5929\u673a\u5668\u4eba\u548c rpa \u5230\u81ea\u4e3b\u4ee3 \u673a\u5668\u4eba\u6d41\u7a0b\u81ea\u52a8\u5316\uff08rpa\uff09\u5230\u81ea\u4e3b\u4ee3\u7406\u6d41\u7a0b\u81ea\u52a8\u5316\uff08apa\uff09 \u7406--\u8fc8\u5411\u81ea\u4e3b\u7684\u65c5\u7a0b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.9884f917", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNTQwMzAzOA==&mid=2247494312&idx=1&sn=b6afc7ed3f2798f3871717d5373c9ba7&chksm=c0ed2b72843abfbc1f93e146e4dc62e55f01ae7e5de861b832a6cfc1989704791b5a29a1dfef#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNTQwMzAzOA==&mid=2247494312&idx=1&sn=b6afc7ed3f2798f3871717d5373c9ba7&chksm=c0ed2b72843abfbc1f93e146e4dc62e55f01ae7e5de861b832a6cfc1989704791b5a29a1dfef#rd", "authors": ["Boom Health"], "title": "\u884c\u4e1a\u6d1e\u89c1 | \u9ea6\u80af\u9521\uff1a\u4eba\u5de5\u667a\u80fd<em class=\"highlight\">agentic</em> AI \u6709\u671b\u4e3a\u751f\u547d\u79d1\u5b66\u4f01\u4e1a\u8f6c\u578b\u63d0\u4f9b\u65b0\u673a\u9047\uff08\u4e0a\uff09", "comment": "Source: WeChat, Published: 2025-10-22 05:30:00", "summary": "5.4 p.p.\uff01increased productivity\uff0creduced vendor spend\uff0cwithin a time horizon of revenue pulled forward and optimized operations 3 to 5 years within 3-to-5 year time frame\uff0c with potential for incremental 1% to 3% cagr above baseline growth note\uff1a numbers do not include physical ai or growth fr", "AI": {"tldr": "5.4 p.p.\uff01increased productivity\uff0creduced vendor spend\uff0cwithin a time horizon of revenue pulled forward and optimized operations 3 to 5 years within 3-to-5 year time frame\uff0c with potential for incremental...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.dbfbd127", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxNjk4MzQxNw==&mid=2247488407&idx=1&sn=e9574b4f094256daedbcddf97a8df5e7&chksm=9a6ea0a13513aca88c9861a503f99e20a1dd27055d057f7c571d596f807d01576b60b1dd47d8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxNjk4MzQxNw==&mid=2247488407&idx=1&sn=e9574b4f094256daedbcddf97a8df5e7&chksm=9a6ea0a13513aca88c9861a503f99e20a1dd27055d057f7c571d596f807d01576b60b1dd47d8#rd", "authors": ["\u67b6\u6784\u5e08\u4e4b\u9053"], "title": "\u4e1a\u754c\u9996\u6b3e\u4e13\u4e3aAI<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u6253\u9020\u7684\u9996\u6b3e\u514d\u8d39\u6570\u636e\u5e93<em class=\"highlight\">Agentic</em> Postgres\uff0c\u6765\u4e86\u89e3\u4e00\u4e0b", "comment": "Source: WeChat, Published: 2025-10-22 05:19:28", "summary": "Agentic Postgres\u662f\u6570\u636e\u5e93\u5411\u201cAI\u539f\u751f\u201d\u6f14\u8fdb\u7684\u5178\u578b\u63a2\u7d22\uff0c\u5176\u6838\u5fc3\u4ef7\u503c\u5728\u4e8e\u5c06\u667a\u80fd\u4f53\u7684\u201c\u80fd\u529b\u9700\u6c42\u201d\u8f6c\u5316\u4e3a\u6570\u636e\u5e93\u7684\u201c\u5185\u7f6e\u529f\u80fd\u201d\uff0c\u800c\u975e\u7b80\u5355\u53e0\u52a0AI\u63d2\u4ef6\u3002", "AI": {"tldr": "Agentic Postgres\u662f\u6570\u636e\u5e93\u5411\u201cAI\u539f\u751f\u201d\u6f14\u8fdb\u7684\u5178\u578b\u63a2\u7d22\uff0c\u5176\u6838\u5fc3\u4ef7\u503c\u5728\u4e8e\u5c06\u667a\u80fd\u4f53\u7684\u201c\u80fd\u529b\u9700\u6c42\u201d\u8f6c\u5316\u4e3a\u6570\u636e\u5e93\u7684\u201c\u5185\u7f6e\u529f\u80fd\u201d\uff0c\u800c\u975e\u7b80\u5355\u53e0\u52a0AI\u63d2\u4ef6\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.c9262df4", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTY5ODYxOA==&mid=2247513550&idx=1&sn=5e273b9f691a435cb962ba8099bf6817&chksm=fa006d50d367cf93677197c021ae47dbdbda219d00df3f0952f077830a0b3923b539bf64693b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTY5ODYxOA==&mid=2247513550&idx=1&sn=5e273b9f691a435cb962ba8099bf6817&chksm=fa006d50d367cf93677197c021ae47dbdbda219d00df3f0952f077830a0b3923b539bf64693b#rd", "authors": ["\u957f\u4e09\u89d2\u4e00\u4f53\u5316"], "title": "\u201c\u667a\u80fd\u524d\u6cbf\u89c2\u5bdf\u201d\u7cfb\u5217 | \u4eceAI Agent\u5230<em class=\"highlight\">Agentic</em> AI\u2014\u201c\u667a\u6167\u5de5\u5177\u201d\u5411\u201c\u667a\u80fd\u5f15\u64ce\u201d\u7684\u8d28\u53d8\u8dc3\u8fc1", "comment": "Source: WeChat, Published: 2025-10-22 03:31:30", "summary": "\u672a\u6765\uff0c\u5f53Agentic AI\u81ea\u4e3b\u521b\u65b0\u80fd\u529b\u65e5\u76ca\u6210\u719f\uff0c\u5c06\u6781\u5927\u6539\u53d8\u4f01\u4e1a\u3001\u7ecf\u6d4e\u7ec4\u7ec7\u5f62\u6001\uff0c\u6d8c\u73b0\u51fa\u66f4\u591a\u201c\u4e00\u4eba\u72ec\u89d2\u517d\u201d\u3001\u201c\u8d85\u7ea7\u4e2a\u4f53\u201d\u7b49\u65b0\u7ecf\u6d4e\u5f62\u6001\uff1b\u8d85\u7ea7APP\u7b49\u7f51\u7edc\u5165\u53e3\u3001\u6d41\u91cf\u5165\u53e3\u4e5f\u5c06\u88abAgentic AI\u6240\u66ff\u4ee3\u3002", "AI": {"tldr": "\u672a\u6765\uff0c\u5f53Agentic AI\u81ea\u4e3b\u521b\u65b0\u80fd\u529b\u65e5\u76ca\u6210\u719f\uff0c\u5c06\u6781\u5927\u6539\u53d8\u4f01\u4e1a\u3001\u7ecf\u6d4e\u7ec4\u7ec7\u5f62\u6001\uff0c\u6d8c\u73b0\u51fa\u66f4\u591a\u201c\u4e00\u4eba\u72ec\u89d2\u517d\u201d\u3001\u201c\u8d85\u7ea7\u4e2a\u4f53\u201d\u7b49\u65b0\u7ecf\u6d4e\u5f62\u6001\uff1b\u8d85\u7ea7APP\u7b49\u7f51\u7edc\u5165\u53e3\u3001\u6d41\u91cf\u5165\u53e3\u4e5f\u5c06\u88abAgentic AI\u6240\u66ff\u4ee3\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.7b87cb12", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwMDQ3MjU3Mw==&mid=2247546311&idx=1&sn=1d042bb278c0b145a5cd64fe60314d1a&chksm=c152d918b78e6b42449427a3b2e78ec5b5ec1aa501c9a06db2de27bfbdab42fa9eaf338f3639#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwMDQ3MjU3Mw==&mid=2247546311&idx=1&sn=1d042bb278c0b145a5cd64fe60314d1a&chksm=c152d918b78e6b42449427a3b2e78ec5b5ec1aa501c9a06db2de27bfbdab42fa9eaf338f3639#rd", "authors": ["\u9e4f\u535a\u58eb\u7814\u7a76\u9662"], "title": "\u4e00\u6587\u8bfb\u61c2<em class=\"highlight\">Agentic</em> AI \u4e0e AI Agent\u7684\u6838\u5fc3\u533a\u522b", "comment": "Source: WeChat, Published: 2025-10-22 01:30:25", "summary": "Agentic AIAgentic AI \u662f\u4ee5\u590d\u6742\u76ee\u6807\u8fbe\u6210\u4e3a\u6838\u5fc3\uff0c\u7edf\u7b79\u591a\u4e2a AI Agents \u53ca\u5de5\u5177\u7684\u5206\u5e03\u5f0f\u667a\u80fd\u4f53\u7cfb\uff0c\u5176\u6280\u672f\u672c\u8d28\u662f\u52a8\u6001\u76ee\u6807\u4f18\u5316\u5668\uff0c\u65e0\u9700\u4eba\u5de5\u5b9a\u4e49\u6bcf\u4e00\u6b65\u64cd\u4f5c\uff0c\u4ec5\u9700\u660e\u786e\u6700\u7ec8\u76ee\u6807\uff0c\u7cfb\u7edf\u5373\u53ef\u81ea\u4e3b\u89c4\u5212\u8def\u5f84\u3001\u534f\u8c03\u8d44\u6e90\u3001\u8c03\u6574\u7b56\u7565\u3002", "AI": {"tldr": "Agentic AIAgentic AI \u662f\u4ee5\u590d\u6742\u76ee\u6807\u8fbe\u6210\u4e3a\u6838\u5fc3\uff0c\u7edf\u7b79\u591a\u4e2a AI Agents \u53ca\u5de5\u5177\u7684\u5206\u5e03\u5f0f\u667a\u80fd\u4f53\u7cfb\uff0c\u5176\u6280\u672f\u672c\u8d28\u662f\u52a8\u6001\u76ee\u6807\u4f18\u5316\u5668\uff0c\u65e0\u9700\u4eba\u5de5\u5b9a\u4e49\u6bcf\u4e00\u6b65\u64cd\u4f5c\uff0c\u4ec5\u9700\u660e\u786e\u6700\u7ec8\u76ee\u6807\uff0c\u7cfb\u7edf\u5373\u53ef\u81ea\u4e3b\u89c4\u5212\u8def\u5f84\u3001\u534f\u8c03\u8d44\u6e90\u3001\u8c03\u6574\u7b56\u7565\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.90a83b76", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5MTE3ODU4MQ==&mid=2247485015&idx=1&sn=e0985b924ec6959381e7a7f2dbf66c39&chksm=9736cc1fe15627353b1fcc6a0945f854080edc2d7477e740fe3d6969f596ac7c042d710c456c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5MTE3ODU4MQ==&mid=2247485015&idx=1&sn=e0985b924ec6959381e7a7f2dbf66c39&chksm=9736cc1fe15627353b1fcc6a0945f854080edc2d7477e740fe3d6969f596ac7c042d710c456c#rd", "authors": ["\u6d1e\u89c1\u754f\u6765"], "title": "\ud83e\udd16 \u4ece Agent \u5230 <em class=\"highlight\">Agentic</em>\uff1aAI <em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7684\u201c\u89c9\u9192\u65f6\u523b\u201d", "comment": "Source: WeChat, Published: 2025-10-22 00:21:02", "summary": "Agentic \u662f Agent \u7684\u8fdb\u5316\u5f62\u6001\u3002\u5b83\u4e0d\u53ea\u662f\u4e00\u4e2a\u88ab\u6307\u6325\u7684\u5de5\u5177\uff0c\u800c\u662f\u4e00\u4e2a\u80fd\u81ea\u4e3b\u505a\u51b3\u7b56\u3001\u4e0d\u65ad\u81ea\u6211\u6539\u8fdb\u7684\u667a\u80fd\u4f53\u3002\u4e3e\u51e0\u4e2a\u4f8b\u5b50\uff1aAutoGPT\u4f1a\u6839\u636e\u9ad8\u5c42\u76ee\u6807\u81ea\u52a8\u62c6\u89e3\u4efb\u52a1\uff1b", "AI": {"tldr": "Agentic \u662f Agent \u7684\u8fdb\u5316\u5f62\u6001\u3002\u5b83\u4e0d\u53ea\u662f\u4e00\u4e2a\u88ab\u6307\u6325\u7684\u5de5\u5177\uff0c\u800c\u662f\u4e00\u4e2a\u80fd\u81ea\u4e3b\u505a\u51b3\u7b56\u3001\u4e0d\u65ad\u81ea\u6211\u6539\u8fdb\u7684\u667a\u80fd\u4f53\u3002\u4e3e\u51e0\u4e2a\u4f8b\u5b50\uff1aAutoGPT\u4f1a\u6839\u636e\u9ad8\u5c42\u76ee\u6807\u81ea\u52a8\u62c6\u89e3\u4efb\u52a1\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.2ec0c4e0", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4OTY4MDU4MQ==&mid=2247489231&idx=1&sn=8122ca60e1fe62e52dcc21dadfb25749&chksm=fc12c1fb374f29cf3808c4ae3d50618a09a80d580fbce2d67925d192b835c17292879f0e1320#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4OTY4MDU4MQ==&mid=2247489231&idx=1&sn=8122ca60e1fe62e52dcc21dadfb25749&chksm=fc12c1fb374f29cf3808c4ae3d50618a09a80d580fbce2d67925d192b835c17292879f0e1320#rd", "authors": ["\u5927\u6a21\u578b\u4e4b\u8def"], "title": "90%\u7684\u56e2\u961f\u90fd\u5728\u7528\u7684 <em class=\"highlight\">Agentic</em> AI \u8bbe\u8ba1\u6a21\u5f0f\u89e3\u6790", "comment": "Source: WeChat, Published: 2025-10-22 00:10:12", "summary": "\u4e00\u3001\u4ec0\u4e48\u662f Agentic AI\uff1fAgentic AI \u662f\u4e00\u79cd\u5177\u5907\u4e00\u5b9a\u81ea\u4e3b\u80fd\u529b\u7684 LLM \u5e94\u7528\uff0c\u5b83\u80fd\u591f\uff1a\u89c2\u5bdf\uff08Observe\uff09\uff1a\u7406\u89e3\u7528\u6237\u8bf7\u6c42\u53ca\u76f8\u5173\u4e0a\u4e0b\u6587\uff08\u5982\u6587\u6863\u3001\u65e5\u5fd7\u7b49\uff09\u601d\u8003\uff08Think\uff09\uff1a\u5224\u65ad\u4e0b\u4e00\u6b65\u6700\u4f73\u884c\u52a8", "AI": {"tldr": "\u4e00\u3001\u4ec0\u4e48\u662f Agentic AI\uff1fAgentic AI \u662f\u4e00\u79cd\u5177\u5907\u4e00\u5b9a\u81ea\u4e3b\u80fd\u529b\u7684 LLM \u5e94\u7528\uff0c\u5b83\u80fd\u591f\uff1a\u89c2\u5bdf\uff08Observe\uff09\uff1a\u7406\u89e3\u7528\u6237\u8bf7\u6c42\u53ca\u76f8\u5173\u4e0a\u4e0b\u6587\uff08\u5982\u6587\u6863\u3001\u65e5\u5fd7\u7b49\uff09\u601d\u8003\uff08Think\uff09\uff1a\u5224\u65ad\u4e0b\u4e00\u6b65\u6700\u4f73\u884c\u52a8", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.19cbd821", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNDc1NjMzMw==&mid=2247488209&idx=1&sn=470744b81146722870b94dcd4a66f157&chksm=c098a1fd0666bed1cdcd767ff596a945ab6b24d67d4695c18e35b50a2b1b0e2b14df22fc8206#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNDc1NjMzMw==&mid=2247488209&idx=1&sn=470744b81146722870b94dcd4a66f157&chksm=c098a1fd0666bed1cdcd767ff596a945ab6b24d67d4695c18e35b50a2b1b0e2b14df22fc8206#rd", "authors": ["\u8f89\u5c11-\u4f01\u4e1a\u7ea7AGI\u63a2\u7d22\u8005"], "title": "\u4e00\u56fe\u4e86\u89e3<em class=\"highlight\">Agentic</em> AI\u5b87\u5b99", "comment": "Source: WeChat, Published: 2025-10-22 00:00:00", "summary": "7. \u6700\u5916\u5c42\uff1aAgentic AI\uff08\u667a\u80fd\u4f53 AI\uff09\u300c\u9ad8\u7ea7\u80fd\u529b\u4e0e\u5b89\u5168\u300d\u8fd9\u662f AI \u667a\u80fd\u4f53\u7684\u524d\u6cbf\u8fdb\u5316\u65b9\u5411\uff0c\u805a\u7126 \u201c\u66f4\u590d\u6742\u3001\u66f4\u53ef\u9760\u3001\u66f4\u5b89\u5168\u7684\u81ea\u4e3b\u7cfb\u7edf\u201d\uff1a\u534f\u8bae\u4e0e\u89d2\u8272\uff1aAgent Protocols \uff08MCP\uff1b", "AI": {"tldr": "7. \u6700\u5916\u5c42\uff1aAgentic AI\uff08\u667a\u80fd\u4f53 AI\uff09\u300c\u9ad8\u7ea7\u80fd\u529b\u4e0e\u5b89\u5168\u300d\u8fd9\u662f AI \u667a\u80fd\u4f53\u7684\u524d\u6cbf\u8fdb\u5316\u65b9\u5411\uff0c\u805a\u7126 \u201c\u66f4\u590d\u6742\u3001\u66f4\u53ef\u9760\u3001\u66f4\u5b89\u5168\u7684\u81ea\u4e3b\u7cfb\u7edf\u201d\uff1a\u534f\u8bae\u4e0e\u89d2\u8272\uff1aAgent Protocols \uff08MCP\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.4c8958d9", "categories": ["wechat.article", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5Nzc3MjI3MA==&mid=2648479142&idx=1&sn=e9d886232c3961d2043f847301b82bb4&chksm=bfe3f1376887927483de776b87e67dc1f2da1551667130b4f370711dd8093bae89ed8b65c737#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5Nzc3MjI3MA==&mid=2648479142&idx=1&sn=e9d886232c3961d2043f847301b82bb4&chksm=bfe3f1376887927483de776b87e67dc1f2da1551667130b4f370711dd8093bae89ed8b65c737#rd", "authors": ["\u673a\u5668AI\u5b66\u4e60 \u6570\u636eAI\u6316\u6398"], "title": "<em class=\"highlight\">Agentic</em> AI: Part 4 - <em class=\"highlight\">\u667a\u80fd\u4f53</em>\u6846\u67b6", "comment": "Source: WeChat, Published: 2025-10-22 00:00:00", "summary": "ai\u6a21\u578b\uff1a\u5927\u8111\u6846\u67b6\u63d0\u4f9b\u4e86\u7ed3\u6784\uff0c\u4f46\u667a\u80fd\u4f53\u4ecd\u7136\u9700\u8981\u667a\u80fd\u3002\u8fd9\u6765\u81ea\u4e8eai\u6a21\u578b\uff1a\u7528\u4e8e\u9605\u8bfb\u6587\u6863\u7684\u8bed\u8a00\u6a21\u578b\u3002\u7528\u4e8e\u6a21\u5f0f\u8bc6\u522b\u7684\u5d4c\u5165\u6a21\u578b\u3002\u7528\u4e8e\u786e\u4fdd\u5b89\u5168\u6027\u548c\u5408\u89c4\u6027\u7684\u9632\u62a4\u63aa\u65bd\u3002", "AI": {"tldr": "ai\u6a21\u578b\uff1a\u5927\u8111\u6846\u67b6\u63d0\u4f9b\u4e86\u7ed3\u6784\uff0c\u4f46\u667a\u80fd\u4f53\u4ecd\u7136\u9700\u8981\u667a\u80fd\u3002\u8fd9\u6765\u81ea\u4e8eai\u6a21\u578b\uff1a\u7528\u4e8e\u9605\u8bfb\u6587\u6863\u7684\u8bed\u8a00\u6a21\u578b\u3002\u7528\u4e8e\u6a21\u5f0f\u8bc6\u522b\u7684\u5d4c\u5165\u6a21\u578b\u3002\u7528\u4e8e\u786e\u4fdd\u5b89\u5168\u6027\u548c\u5408\u89c4\u6027\u7684\u9632\u62a4\u63aa\u65bd\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.34d5e6a2", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5ODEyMTI4MA==&mid=2247484393&idx=1&sn=53dd3868aedbbd98f07346a8bfb2ed69&chksm=97fc7ee0e8a36772a5d49002ee9d8193f5c086bbf2624fa524174a129b02e9a2013d08a9f513#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5ODEyMTI4MA==&mid=2247484393&idx=1&sn=53dd3868aedbbd98f07346a8bfb2ed69&chksm=97fc7ee0e8a36772a5d49002ee9d8193f5c086bbf2624fa524174a129b02e9a2013d08a9f513#rd", "authors": ["\u5927\u6a21\u578bRAG\u548cAgent\u6280\u672f\u5b9e\u8df5"], "title": "<em class=\"highlight\">Agentic</em>\u5de5\u4f5c\u6d41\u4e2d\u51fd\u6570\u8c03\u7528\u7684\u5b9e\u6218\uff1a\u6784\u5efa\u667a\u80fd\u5de5\u4e1a\u8d28\u68c0\u7cfb\u7edf", "comment": "Source: WeChat, Published: 2025-10-21 14:54:47", "summary": "Agentic\u5de5\u4f5c\u6d41 \uff08Agentic Workflow\uff09\uff1a \u4e00\u4e2a\u6df7\u5408\u7cfb\u7edf\uff0c\u65e2\u5305\u542b\u9884\u5b9a\u4e49\u7684\u6267\u884c\u8def\u5f84\uff0c\u4e5f\u5305\u542b\u7531Agent\u52a8\u6001\u51b3\u7b56\u7684\u8def\u5f84\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210 \uff08RAG\uff09\uff1a \u4ece\u5916\u90e8\u6570\u636e\u6e90\uff08\u5982\u6570\u636e\u5e93\u3001\u6587\u6863\uff09\u68c0\u7d22\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u63d0\u4f9b\u7ed9LLM\uff0c\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u3001\u66f4\u5177", "AI": {"tldr": "Agentic\u5de5\u4f5c\u6d41 \uff08Agentic Workflow\uff09\uff1a \u4e00\u4e2a\u6df7\u5408\u7cfb\u7edf\uff0c\u65e2\u5305\u542b\u9884\u5b9a\u4e49\u7684\u6267\u884c\u8def\u5f84\uff0c\u4e5f\u5305\u542b\u7531Agent\u52a8\u6001\u51b3\u7b56\u7684\u8def\u5f84\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210 \uff08RAG\uff09\uff1a \u4ece\u5916\u90e8\u6570\u636e\u6e90\uff08\u5982\u6570\u636e\u5e93\u3001\u6587\u6863\uff09\u68c0\u7d22\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u63d0\u4f9b\u7ed9LLM\uff0c\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u3001\u66f4\u5177", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
