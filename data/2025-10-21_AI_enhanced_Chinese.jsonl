{"id": "2510.16059", "categories": ["cs.SE", "cs.CL", "D.2.2; D.2.3"], "pdf": "https://arxiv.org/pdf/2510.16059", "abs": "https://arxiv.org/abs/2510.16059", "authors": ["Xin Cao", "Nan Yu"], "title": "SIADAFIX: issue description response for adaptive program repair", "comment": "20 pages, 3 figures", "summary": "We propose utilizing fast and slow thinking to enhance the capabilities of\nlarge language model-based agents on complex tasks such as program repair. In\nparticular, we design an adaptive program repair method based on issue\ndescription response, called SIADAFIX. The proposed method utilizes slow\nthinking bug fix agent to complete complex program repair tasks, and employs\nfast thinking workflow decision components to optimize and classify issue\ndescriptions, using issue description response results to guide the\norchestration of bug fix agent workflows. SIADAFIX adaptively selects three\nrepair modes, i.e., easy, middle and hard mode, based on problem complexity. It\nemploys fast generalization for simple problems and test-time scaling\ntechniques for complex problems. Experimental results on the SWE-bench Lite\nshow that the proposed method achieves 60.67% pass@1 performance using the\nClaude-4 Sonnet model, reaching state-of-the-art levels among all open-source\nmethods. SIADAFIX effectively balances repair efficiency and accuracy,\nproviding new insights for automated program repair. Our code is available at\nhttps://github.com/liauto-siada/siada-cli.", "AI": {"tldr": "SIADAFIX\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5feb\u6162\u601d\u7ef4\u7684\u81ea\u9002\u5e94\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ee\u9898\u63cf\u8ff0\u54cd\u5e94\u6765\u6307\u5bfc\u9519\u8bef\u4fee\u590d\u4ee3\u7406\u7684\u5de5\u4f5c\u6d41\u7a0b\u7f16\u6392\uff0c\u5728SWE-bench Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523060.67%\u7684pass@1\u6027\u80fd", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u7a0b\u5e8f\u4fee\u590d\uff09\u4e0a\u7684\u80fd\u529b\uff0c\u9700\u8981\u5e73\u8861\u4fee\u590d\u6548\u7387\u548c\u51c6\u786e\u6027", "method": "\u4f7f\u7528\u6162\u601d\u7ef4\u9519\u8bef\u4fee\u590d\u4ee3\u7406\u5b8c\u6210\u590d\u6742\u7a0b\u5e8f\u4fee\u590d\u4efb\u52a1\uff0c\u5feb\u601d\u7ef4\u5de5\u4f5c\u6d41\u51b3\u7b56\u7ec4\u4ef6\u4f18\u5316\u548c\u5206\u7c7b\u95ee\u9898\u63cf\u8ff0\uff0c\u57fa\u4e8e\u95ee\u9898\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u9009\u62e9\u7b80\u5355\u3001\u4e2d\u7b49\u548c\u56f0\u96be\u4e09\u79cd\u4fee\u590d\u6a21\u5f0f", "result": "\u5728SWE-bench Lite\u4e0a\u4f7f\u7528Claude-4 Sonnet\u6a21\u578b\u8fbe\u523060.67%\u7684pass@1\u6027\u80fd\uff0c\u5728\u6240\u6709\u5f00\u6e90\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73", "conclusion": "SIADAFIX\u6709\u6548\u5e73\u8861\u4e86\u4fee\u590d\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def", "topic": "swe application"}}
{"id": "2510.16384", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16384", "abs": "https://arxiv.org/abs/2510.16384", "authors": ["Yuwei Zhao", "Yuan-An Xiao", "Qianyu Xiao", "Zhao Zhang", "Yingfei Xiong"], "title": "SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis", "comment": null, "summary": "Automated code optimization aims to improve performance in programs by\nrefactoring code, and recent studies focus on utilizing LLMs for the\noptimization. Typical existing approaches mine optimization commits from\nopen-source codebases to construct a large-scale knowledge base, then employ\ninformation retrieval techniques such as BM25 to retrieve relevant optimization\nexamples for hotspot code locations, thereby guiding LLMs to optimize these\nhotspots. However, since semantically equivalent optimizations can manifest in\nsyntactically dissimilar code snippets, current retrieval methods often fail to\nidentify pertinent examples, leading to suboptimal optimization performance.\nThis limitation significantly reduces the effectiveness of existing\noptimization approaches.\n  To address these limitations, we propose SemOpt, a novel framework that\nleverages static program analysis to precisely identify optimizable code\nsegments, retrieve the corresponding optimization strategies, and generate the\noptimized results. SemOpt consists of three key components: (1) A strategy\nlibrary builder that extracts and clusters optimization strategies from\nreal-world code modifications. (2) A rule generator that generates Semgrep\nstatic analysis rules to capture the condition of applying the optimization\nstrategy. (3) An optimizer that utilizes the strategy library to generate\noptimized code results. All the three components are powered by LLMs.\n  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its\neffectiveness under different LLMs by increasing the number of successful\noptimizations by 1.38 to 28 times compared to the baseline. Moreover, on\npopular large-scale C/C++ projects, it can improve individual performance\nmetrics by 5.04% to 218.07%, demonstrating its practical utility.", "AI": {"tldr": "SemOpt\u662f\u4e00\u4e2a\u5229\u7528\u9759\u6001\u7a0b\u5e8f\u5206\u6790\u548cLLM\u7684\u4ee3\u7801\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u7b56\u7565\u5e93\u548c\u751f\u6210\u9759\u6001\u5206\u6790\u89c4\u5219\u6765\u7cbe\u786e\u8bc6\u522b\u53ef\u4f18\u5316\u4ee3\u7801\u6bb5\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4f18\u5316\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u7684\u4ee3\u7801\u4f18\u5316\u65b9\u6cd5\u7531\u4e8e\u8bed\u4e49\u7b49\u4ef7\u4f46\u8bed\u6cd5\u4e0d\u540c\u7684\u4ee3\u7801\u7247\u6bb5\u96be\u4ee5\u88ab\u68c0\u7d22\u5230\uff0c\u5bfc\u81f4\u4f18\u5316\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\u7684\u6846\u67b6\uff1a\u7b56\u7565\u5e93\u6784\u5efa\u5668\u63d0\u53d6\u548c\u805a\u7c7b\u4f18\u5316\u7b56\u7565\uff1b\u89c4\u5219\u751f\u6210\u5668\u751f\u6210Semgrep\u9759\u6001\u5206\u6790\u89c4\u5219\uff1b\u4f18\u5316\u5668\u5229\u7528\u7b56\u7565\u5e93\u751f\u6210\u4f18\u5316\u4ee3\u7801\u3002\u6240\u6709\u7ec4\u4ef6\u90fd\u7531LLM\u9a71\u52a8\u3002", "result": "\u5728151\u4e2a\u4f18\u5316\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6210\u529f\u4f18\u5316\u6570\u91cf\u589e\u52a0\u4e861.38\u523028\u500d\uff1b\u5728\u5927\u578bC/C++\u9879\u76ee\u4e2d\uff0c\u5355\u4e2a\u6027\u80fd\u6307\u6807\u63d0\u5347\u4e865.04%\u5230218.07%\u3002", "conclusion": "SemOpt\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u7a0b\u5e8f\u5206\u6790\u548cLLM\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u4ee3\u7801\u4f18\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "topic": "code agent"}}
{"id": "2510.16395", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16395", "abs": "https://arxiv.org/abs/2510.16395", "authors": ["Xin Peng", "Chong Wang"], "title": "Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated strong\ncapabilities in software engineering tasks, raising expectations of\nrevolutionary productivity gains. However, enterprise software development is\nlargely driven by incremental evolution, where challenges extend far beyond\nroutine coding and depend critically on tacit knowledge, including design\ndecisions at different levels and historical trade-offs. To achieve effective\nAI-powered support for complex software development, we should align emerging\nAI capabilities with the practical realities of enterprise development. To this\nend, we systematically identify challenges from both software and LLM\nperspectives. Alongside these challenges, we outline opportunities where AI and\nstructured knowledge frameworks can enhance decision-making in tasks such as\nissue localization and impact analysis. To address these needs, we propose the\nCode Digital Twin, a living framework that models both the physical and\nconceptual layers of software, preserves tacit knowledge, and co-evolves with\nthe codebase. By integrating hybrid knowledge representations, multi-stage\nextraction pipelines, incremental updates, LLM-empowered applications, and\nhuman-in-the-loop feedback, the Code Digital Twin transforms fragmented\nknowledge into explicit and actionable representations. Our vision positions it\nas a bridge between AI advancements and enterprise software realities,\nproviding a concrete roadmap toward sustainable, intelligent, and resilient\ndevelopment and evolution of ultra-complex systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faCode Digital Twin\u6846\u67b6\uff0c\u5c06AI\u80fd\u529b\u4e0e\u4f01\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u73b0\u5b9e\u5bf9\u9f50\uff0c\u901a\u8fc7\u6df7\u5408\u77e5\u8bc6\u8868\u793a\u548c\u591a\u9636\u6bb5\u63d0\u53d6\u7ba1\u9053\u5c06\u788e\u7247\u5316\u77e5\u8bc6\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u8868\u793a\u3002", "motivation": "\u4f01\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u4e3b\u8981\u4f9d\u8d56\u589e\u91cf\u6f14\u8fdb\uff0c\u6d89\u53ca\u5927\u91cf\u9690\u6027\u77e5\u8bc6\uff0c\u800c\u73b0\u6709LLM\u5728\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u652f\u6301\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5c06AI\u80fd\u529b\u4e0e\u4f01\u4e1a\u5f00\u53d1\u5b9e\u8df5\u5bf9\u9f50\u3002", "method": "\u63d0\u51faCode Digital Twin\u6846\u67b6\uff0c\u5305\u542b\u6df7\u5408\u77e5\u8bc6\u8868\u793a\u3001\u591a\u9636\u6bb5\u63d0\u53d6\u7ba1\u9053\u3001\u589e\u91cf\u66f4\u65b0\u3001LLM\u8d4b\u80fd\u5e94\u7528\u548c\u4eba\u673a\u534f\u540c\u53cd\u9988\u7b49\u7ec4\u4ef6\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u5efa\u6a21\u8f6f\u4ef6\u7269\u7406\u548c\u6982\u5ff5\u5c42\u3001\u4fdd\u5b58\u9690\u6027\u77e5\u8bc6\u5e76\u4e0e\u4ee3\u7801\u5e93\u5171\u540c\u6f14\u5316\u7684\u52a8\u6001\u6846\u67b6\u3002", "conclusion": "Code Digital Twin\u5728AI\u8fdb\u5c55\u4e0e\u4f01\u4e1a\u8f6f\u4ef6\u73b0\u5b9e\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\uff0c\u4e3a\u8d85\u590d\u6742\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u3001\u667a\u80fd\u548c\u5f39\u6027\u5f00\u53d1\u6f14\u8fdb\u63d0\u4f9b\u4e86\u5177\u4f53\u8def\u7ebf\u56fe\u3002", "topic": "swe application"}}
{"id": "2510.15952", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15952", "abs": "https://arxiv.org/abs/2510.15952", "authors": ["Myung Ho Kim"], "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding", "comment": "27 pages", "summary": "Large language models exhibit intelligence without genuine epistemic\nunderstanding, exposing a key gap: the absence of epistemic architecture. This\npaper introduces the Structured Cognitive Loop (SCL) as an executable\nepistemological framework for emergent intelligence. Unlike traditional AI\nresearch asking \"what is intelligence?\" (ontological), SCL asks \"under what\nconditions does cognition emerge?\" (epistemological). Grounded in philosophy of\nmind and cognitive phenomenology, SCL bridges conceptual philosophy and\nimplementable cognition. Drawing on process philosophy, enactive cognition, and\nextended mind theory, we define intelligence not as a property but as a\nperformed process -- a continuous loop of judgment, memory, control, action,\nand regulation. SCL makes three contributions. First, it operationalizes\nphilosophical insights into computationally interpretable structures, enabling\n\"executable epistemology\" -- philosophy as structural experiment. Second, it\nshows that functional separation within cognitive architecture yields more\ncoherent and interpretable behavior than monolithic prompt based systems,\nsupported by agent evaluations. Third, it redefines intelligence: not\nrepresentational accuracy but the capacity to reconstruct its own epistemic\nstate through intentional understanding. This framework impacts philosophy of\nmind, epistemology, and AI. For philosophy, it allows theories of cognition to\nbe enacted and tested. For AI, it grounds behavior in epistemic structure\nrather than statistical regularity. For epistemology, it frames knowledge not\nas truth possession but as continuous reconstruction within a\nphenomenologically coherent loop. We situate SCL within debates on cognitive\nphenomenology, emergence, normativity, and intentionality, arguing that real\nprogress requires not larger models but architectures that realize cognitive\nprinciples structurally.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u8ba4\u77e5\u5faa\u73af\uff08SCL\uff09\u4f5c\u4e3a\u53ef\u6267\u884c\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u5c06\u54f2\u5b66\u89c1\u89e3\u8f6c\u5316\u4e3a\u53ef\u8ba1\u7b97\u7ed3\u6784\uff0c\u5f3a\u8c03\u8ba4\u77e5\u662f\u5224\u65ad\u3001\u8bb0\u5fc6\u3001\u63a7\u5236\u3001\u884c\u52a8\u548c\u8c03\u8282\u7684\u6301\u7eed\u5faa\u73af\u8fc7\u7a0b\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u771f\u6b63\u8ba4\u77e5\u7406\u89e3\u7684\u95ee\u9898\uff0c\u586b\u8865\u8ba4\u77e5\u67b6\u6784\u7684\u7a7a\u767d\uff0c\u4ece\u672c\u4f53\u8bba\u8f6c\u5411\u8ba4\u8bc6\u8bba\u89c6\u89d2\u63a2\u7d22\u8ba4\u77e5\u6d8c\u73b0\u7684\u6761\u4ef6\u3002", "method": "\u57fa\u4e8e\u8fc7\u7a0b\u54f2\u5b66\u3001\u5177\u8eab\u8ba4\u77e5\u548c\u6269\u5c55\u5fc3\u667a\u7406\u8bba\uff0c\u6784\u5efa\u53ef\u6267\u884c\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u5c06\u529f\u80fd\u5206\u79bb\u7684\u8ba4\u77e5\u67b6\u6784\u4e0e\u57fa\u4e8e\u63d0\u793a\u7684\u5355\u4f53\u7cfb\u7edf\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u529f\u80fd\u5206\u79bb\u7684\u8ba4\u77e5\u67b6\u6784\u6bd4\u5355\u4f53\u7cfb\u7edf\u4ea7\u751f\u66f4\u8fde\u8d2f\u548c\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\uff0c\u652f\u6301\u667a\u80fd\u4e0d\u662f\u8868\u5f81\u51c6\u786e\u6027\u800c\u662f\u901a\u8fc7\u610f\u5411\u6027\u7406\u89e3\u91cd\u5efa\u81ea\u8eab\u8ba4\u77e5\u72b6\u6001\u7684\u80fd\u529b\u3002", "conclusion": "\u771f\u6b63\u7684\u8fdb\u6b65\u9700\u8981\u5b9e\u73b0\u8ba4\u77e5\u539f\u5219\u7684\u7ed3\u6784\u5316\u67b6\u6784\uff0c\u800c\u975e\u66f4\u5927\u7684\u6a21\u578b\uff0c\u8be5\u6846\u67b6\u5bf9\u5fc3\u667a\u54f2\u5b66\u3001\u8ba4\u8bc6\u8bba\u548c\u4eba\u5de5\u667a\u80fd\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2510.15945", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15945", "abs": "https://arxiv.org/abs/2510.15945", "authors": ["Guangya Wan", "Zixin Stephen Xu", "Sasa Zorc", "Manel Baucells", "Mengxuan Hu", "Hao Wang", "Sheng Li"], "title": "BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling", "comment": "Under review on ARR", "summary": "Sampling multiple responses is a common way to improve LLM output quality,\nbut it comes at the cost of additional computation. The key challenge is\ndeciding when to stop generating new samples to balance accuracy gains against\nefficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive\nCriterion for Optimal N-stopping), a principled adaptive sampling framework\ngrounded in Sequential Search with Bayesian Learning. BEACON sequentially\ngenerates responses from the policy LLM, updates posterior belief over reward\ndistributions in real time without further training, and determines when to\nstop by weighing expected gains against computational cost. Sampling terminates\nonce the marginal utility of further exploration no longer justifies the\nexpense. We establish both theoretical optimality guarantees and practical\ntractability, and show empirically that BEACON reduces average sampling by up\nto 80% while maintaining response quality. We further demonstrate BEACON's\nutility for cost-efficient preference data generation and outline practical\nextensions, offering actionable insights for future researchers.", "AI": {"tldr": "BEACON\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u66f4\u65b0\u5956\u52b1\u5206\u5e03\u7684\u540e\u9a8c\u4fe1\u5ff5\u6765\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62\u751f\u6210\u65b0\u6837\u672c\uff0c\u5728\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u91c7\u6837\u6b21\u6570\u3002", "motivation": "\u591a\u54cd\u5e94\u91c7\u6837\u80fd\u63d0\u9ad8LLM\u8f93\u51fa\u8d28\u91cf\uff0c\u4f46\u4f1a\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002\u5173\u952e\u6311\u6218\u662f\u5982\u4f55\u5e73\u8861\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62\u751f\u6210\u65b0\u6837\u672c\u3002", "method": "\u57fa\u4e8e\u5e8f\u5217\u641c\u7d22\u548c\u8d1d\u53f6\u65af\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u91c7\u6837\u6846\u67b6\uff0c\u987a\u5e8f\u751f\u6210\u54cd\u5e94\u3001\u5b9e\u65f6\u66f4\u65b0\u5956\u52b1\u5206\u5e03\u540e\u9a8c\u4fe1\u5ff5\uff0c\u6743\u8861\u9884\u671f\u6536\u76ca\u4e0e\u8ba1\u7b97\u6210\u672c\u6765\u51b3\u5b9a\u505c\u6b62\u65f6\u673a\u3002", "result": "BEACON\u5e73\u5747\u51cf\u5c1180%\u7684\u91c7\u6837\u6b21\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\uff0c\u5728\u6210\u672c\u9ad8\u6548\u7684\u504f\u597d\u6570\u636e\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u5b9e\u7528\u6027\u3002", "conclusion": "BEACON\u63d0\u4f9b\u4e86\u7406\u8bba\u6700\u4f18\u6027\u4fdd\u8bc1\u548c\u5b9e\u9645\u53ef\u64cd\u4f5c\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u81ea\u9002\u5e94\u91c7\u6837\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.16062", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16062", "abs": "https://arxiv.org/abs/2510.16062", "authors": ["Guiyao Tie", "Zenghui Yuan", "Zeli Zhao", "Chaoran Hu", "Tianhe Gu", "Ruihang Zhang", "Sizhe Zhang", "Junran Wu", "Xiaoyue Tu", "Ming Jin", "Qingsong Wen", "Lixing Chen", "Pan Zhou", "Lichao Sun"], "title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs", "comment": "38 pages, 25 figures, 8 tables", "summary": "Self-correction of large language models (LLMs) emerges as a critical\ncomponent for enhancing their reasoning performance. Although various\nself-correction methods have been proposed, a comprehensive evaluation of these\nmethods remains largely unexplored, and the question of whether LLMs can truly\ncorrect themselves is a matter of significant interest and concern. In this\nstudy, we introduce CorrectBench, a benchmark developed to evaluate the\neffectiveness of self-correction strategies, including intrinsic, external, and\nfine-tuned approaches, across three tasks: commonsense reasoning, mathematical\nreasoning, and code generation. Our findings reveal that: 1) Self-correction\nmethods can improve accuracy, especially for complex reasoning tasks; 2) Mixing\ndifferent self-correction strategies yields further improvements, though it\nreduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited\noptimization under additional self-correction methods and have high time costs.\nInterestingly, a comparatively simple chain-of-thought (CoT) baseline\ndemonstrates competitive accuracy and efficiency. These results underscore the\npotential of self-correction to enhance LLM's reasoning performance while\nhighlighting the ongoing challenge of improving their efficiency. Consequently,\nwe advocate for further research focused on optimizing the balance between\nreasoning capabilities and operational efficiency. Project Page:\nhttps://correctbench.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86CorrectBench\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u81ea\u6821\u6b63\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u81ea\u6821\u6b63\u80fd\u63d0\u5347\u63a8\u7406\u4efb\u52a1\u51c6\u786e\u6027\u4f46\u6548\u7387\u8f83\u4f4e\uff0c\u7b80\u5355\u7684CoT\u57fa\u7ebf\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u591a\u79cdLLM\u81ea\u6821\u6b63\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\uff0c\u4e14LLM\u662f\u5426\u80fd\u771f\u6b63\u81ea\u6211\u6821\u6b63\u4ecd\u5b58\u4e89\u8bae\u3002", "method": "\u5f00\u53d1CorrectBench\u57fa\u51c6\uff0c\u8bc4\u4f30\u5185\u5728\u3001\u5916\u90e8\u548c\u5fae\u8c03\u4e09\u79cd\u81ea\u6821\u6b63\u7b56\u7565\u5728\u5e38\u8bc6\u63a8\u7406\u3001\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u81ea\u6821\u6b63\u65b9\u6cd5\u80fd\u63d0\u5347\u51c6\u786e\u6027\uff08\u7279\u522b\u662f\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff09\uff0c\u6df7\u5408\u7b56\u7565\u6709\u8fdb\u4e00\u6b65\u6539\u8fdb\u4f46\u6548\u7387\u964d\u4f4e\uff0c\u63a8\u7406LLM\u5728\u989d\u5916\u81ea\u6821\u6b63\u4e0b\u4f18\u5316\u6709\u9650\u4e14\u65f6\u95f4\u6210\u672c\u9ad8\u3002", "conclusion": "\u81ea\u6821\u6b63\u6709\u6f5c\u529b\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u9700\u5e73\u8861\u63a8\u7406\u80fd\u529b\u4e0e\u64cd\u4f5c\u6548\u7387\uff0c\u5efa\u8bae\u8fdb\u4e00\u6b65\u7814\u7a76\u4f18\u5316\u8fd9\u4e00\u5e73\u8861\u3002", "topic": "agent analysis"}}
{"id": "2510.16079", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16079", "abs": "https://arxiv.org/abs/2510.16079", "authors": ["Rong Wu", "Xiaoman Wang", "Jianbiao Mei", "Pinlong Cai", "Daocheng Fu", "Cheng Yang", "Licheng Wen", "Xuemeng Yang", "Yufan Shen", "Yuxin Wang", "Botian Shi"], "title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle", "comment": null, "summary": "Current Large Language Model (LLM) agents show strong performance in tool\nuse, but lack the crucial capability to systematically learn from their own\nexperiences. While existing frameworks mainly focus on mitigating external\nknowledge gaps, they fail to address a more fundamental limitation: the\ninability to iteratively refine problem-solving strategies. In this work, we\nintroduce EvolveR, a framework designed to enable agent to self-improve through\na complete, closed-loop experience lifecycle. This lifecycle comprises two key\nstages: (1) Offline Self-Distillation, where the agent's interaction\ntrajectories are synthesized into a structured repository of abstract, reusable\nstrategic principles; (2) Online Interaction, where the agent interacts with\ntasks and actively retrieves distilled principles to guide its decision-making,\naccumulating a diverse set of behavioral trajectories. This loop employs a\npolicy reinforcement mechanism to iteratively update the agent based on its\nperformance. We demonstrate the effectiveness of EvolveR on complex multi-hop\nquestion-answering benchmarks, where it achieves superior performance over\nstrong agentic baselines. Our work presents a comprehensive blueprint for\nagents that learn not only from external data but also from the consequences of\ntheir own actions, paving the way for more autonomous and continuously\nimproving systems. Code is available at https://github.com/Edaizi/EvolveR.", "AI": {"tldr": "\u63d0\u51fa\u4e86EvolveR\u6846\u67b6\uff0c\u4f7fLLM\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u5b8c\u6574\u7684\u95ed\u73af\u7ecf\u9a8c\u751f\u547d\u5468\u671f\u8fdb\u884c\u81ea\u6211\u6539\u8fdb\uff0c\u5305\u62ec\u79bb\u7ebf\u81ea\u6211\u84b8\u998f\u548c\u5728\u7ebf\u4ea4\u4e92\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\uff0c\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u4ee3\u7406\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5728\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u4ece\u81ea\u8eab\u7ecf\u9a8c\u4e2d\u7cfb\u7edf\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u8fed\u4ee3\u4f18\u5316\u95ee\u9898\u89e3\u51b3\u7b56\u7565\u3002", "method": "EvolveR\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a(1)\u79bb\u7ebf\u81ea\u6211\u84b8\u998f\uff1a\u5c06\u4ee3\u7406\u7684\u4ea4\u4e92\u8f68\u8ff9\u5408\u6210\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u91cd\u7528\u7684\u62bd\u8c61\u7b56\u7565\u539f\u5219\u5e93\uff1b(2)\u5728\u7ebf\u4ea4\u4e92\uff1a\u4ee3\u7406\u4e0e\u4efb\u52a1\u4ea4\u4e92\u5e76\u4e3b\u52a8\u68c0\u7d22\u84b8\u998f\u539f\u5219\u6307\u5bfc\u51b3\u7b56\uff0c\u79ef\u7d2f\u591a\u6837\u5316\u884c\u4e3a\u8f68\u8ff9\u3002\u91c7\u7528\u7b56\u7565\u5f3a\u5316\u673a\u5236\u8fed\u4ee3\u66f4\u65b0\u4ee3\u7406\u3002", "result": "\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEvolveR\u5b9e\u73b0\u4e86\u4f18\u4e8e\u5f3a\u4ee3\u7406\u57fa\u7ebf\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4ee3\u7406\u4e0d\u4ec5\u4ece\u5916\u90e8\u6570\u636e\u5b66\u4e60\uff0c\u8fd8\u80fd\u4ece\u81ea\u8eab\u884c\u52a8\u540e\u679c\u4e2d\u5b66\u4e60\u63d0\u4f9b\u4e86\u5168\u9762\u84dd\u56fe\uff0c\u4e3a\u66f4\u81ea\u4e3b\u548c\u6301\u7eed\u6539\u8fdb\u7684\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.15966", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15966", "abs": "https://arxiv.org/abs/2510.15966", "authors": ["Shian Jia", "Ziyang Huang", "Xinbo Wang", "Haofei Zhang", "Mingli Song"], "title": "PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency", "comment": null, "summary": "Memory systems are fundamental to AI agents, yet existing work often lacks\nadaptability to diverse tasks and overlooks the constructive and task-oriented\nrole of AI agent memory. Drawing from Piaget's theory of cognitive development,\nwe propose PISA, a pragmatic, psych-inspired unified memory system that\naddresses these limitations by treating memory as a constructive and adaptive\nprocess. To enable continuous learning and adaptability, PISA introduces a\ntrimodal adaptation mechanism (i.e., schema updation, schema evolution, and\nschema creation) that preserves coherent organization while supporting flexible\nmemory updates. Building on these schema-grounded structures, we further design\na hybrid memory access architecture that seamlessly integrates symbolic\nreasoning with neural retrieval, significantly improving retrieval accuracy and\nefficiency. Our empirical evaluation, conducted on the existing LOCOMO\nbenchmark and our newly proposed AggQA benchmark for data analysis tasks,\nconfirms that PISA sets a new state-of-the-art by significantly enhancing\nadaptability and long-term knowledge retention.", "AI": {"tldr": "\u63d0\u51fa\u4e86PISA\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u53d7\u76ae\u4e9a\u6770\u8ba4\u77e5\u53d1\u5c55\u7406\u8bba\u542f\u53d1\uff0c\u901a\u8fc7\u4e09\u6a21\u6001\u9002\u5e94\u673a\u5236\u548c\u6df7\u5408\u8bb0\u5fc6\u8bbf\u95ee\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u957f\u671f\u77e5\u8bc6\u4fdd\u6301\u80fd\u529b\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u8bb0\u5fc6\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u4efb\u52a1\u7684\u9002\u5e94\u6027\uff0c\u5ffd\u89c6\u4e86\u8bb0\u5fc6\u7684\u5efa\u8bbe\u6027\u548c\u4efb\u52a1\u5bfc\u5411\u4f5c\u7528\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u81ea\u9002\u5e94\u7684\u8bb0\u5fc6\u67b6\u6784\u3002", "method": "\u57fa\u4e8e\u76ae\u4e9a\u6770\u7406\u8bba\u6784\u5efaPISA\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u91c7\u7528\u4e09\u6a21\u6001\u9002\u5e94\u673a\u5236\uff08\u56fe\u5f0f\u66f4\u65b0\u3001\u56fe\u5f0f\u6f14\u5316\u548c\u56fe\u5f0f\u521b\u5efa\uff09\uff0c\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u4e0e\u795e\u7ecf\u68c0\u7d22\u7684\u6df7\u5408\u8bb0\u5fc6\u8bbf\u95ee\u67b6\u6784\u3002", "result": "\u5728LOCOMO\u57fa\u51c6\u548c\u65b0\u63d0\u51fa\u7684AggQA\u6570\u636e\u5206\u6790\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPISA\u5728\u9002\u5e94\u6027\u548c\u957f\u671f\u77e5\u8bc6\u4fdd\u6301\u65b9\u9762\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "PISA\u901a\u8fc7\u5efa\u8bbe\u6027\u8bb0\u5fc6\u65b9\u6cd5\u548c\u7075\u6d3b\u9002\u5e94\u673a\u5236\uff0c\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u9002\u5e94\u6027\u548c\u77e5\u8bc6\u4fdd\u6301\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.16579", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16579", "abs": "https://arxiv.org/abs/2510.16579", "authors": ["Wendk\u00fbuni C. Ou\u00e9draogo", "Yinghua Li", "Xueqi Dang", "Pawel Borsukiewicz", "Xin Zhou", "Anil Koyuncu", "Jacques Klein", "David Lo", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Human-Aligned Code Readability Assessment with Large Language Models", "comment": null, "summary": "Code readability is crucial for software comprehension and maintenance, yet\ndifficult to assess at scale. Traditional static metrics often fail to capture\nthe subjective, context-sensitive nature of human judgments. Large Language\nModels (LLMs) offer a scalable alternative, but their behavior as readability\nevaluators remains underexplored. We introduce CoReEval, the first large-scale\nbenchmark for evaluating LLM-based code readability assessment, comprising over\n1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.\nThe benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types\n(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),\n9 decoding settings, and developer-guided prompts tailored to junior and senior\npersonas. We compare LLM outputs against human annotations and a validated\nstatic model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and\njustification quality (sentiment, aspect coverage, semantic clustering). Our\nfindings show that developer-guided prompting grounded in human-defined\nreadability dimensions improves alignment in structured contexts, enhances\nexplanation quality, and enables lightweight personalization through persona\nframing. However, increased score variability highlights trade-offs between\nalignment, stability, and interpretability. CoReEval provides a robust\nfoundation for prompt engineering, model alignment studies, and human in the\nloop evaluation, with applications in education, onboarding, and CI/CD\npipelines where LLMs can serve as explainable, adaptable reviewers.", "AI": {"tldr": "CoReEval\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u8bc4\u4f30LLM\u4ee3\u7801\u53ef\u8bfb\u6027\u8bc4\u4f30\u7684\u57fa\u51c6\uff0c\u5305\u542b140\u4e07\u6b21\u6a21\u578b-\u4ee3\u7801\u7247\u6bb5-\u63d0\u793a\u8bc4\u4f30\uff0c\u6db5\u76d610\u4e2a\u5148\u8fdbLLM\u30013\u79cd\u7f16\u7a0b\u8bed\u8a00\u30012\u79cd\u4ee3\u7801\u7c7b\u578b\u30014\u79cd\u63d0\u793a\u7b56\u7565\u548c9\u79cd\u89e3\u7801\u8bbe\u7f6e\u3002\u7814\u7a76\u53d1\u73b0\u57fa\u4e8e\u4eba\u7c7b\u5b9a\u4e49\u53ef\u8bfb\u6027\u7ef4\u5ea6\u7684\u5f00\u53d1\u8005\u5f15\u5bfc\u63d0\u793a\u80fd\u63d0\u9ad8\u5bf9\u9f50\u5ea6\u548c\u89e3\u91ca\u8d28\u91cf\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u8bc4\u5206\u53d8\u5f02\u6027\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u6307\u6807\u96be\u4ee5\u6355\u6349\u4eba\u7c7b\u5bf9\u4ee3\u7801\u53ef\u8bfb\u6027\u7684\u4e3b\u89c2\u5224\u65ad\uff0c\u800cLLM\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u53ef\u8bfb\u6027\u8bc4\u4f30\u5de5\u5177\u7684\u884c\u4e3a\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efaCoReEval\u57fa\u51c6\uff0c\u5305\u542b\u5927\u89c4\u6a21\u6a21\u578b\u8bc4\u4f30\uff0c\u6bd4\u8f83LLM\u8f93\u51fa\u4e0e\u4eba\u7c7b\u6807\u6ce8\u548c\u9a8c\u8bc1\u9759\u6001\u6a21\u578b\uff0c\u5206\u6790\u6570\u503c\u5bf9\u9f50\u5ea6\u548c\u7406\u7531\u8d28\u91cf\uff0c\u91c7\u7528\u5f00\u53d1\u8005\u5f15\u5bfc\u63d0\u793a\u548c\u89d2\u8272\u6846\u67b6\u3002", "result": "\u5f00\u53d1\u8005\u5f15\u5bfc\u63d0\u793a\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u63d0\u9ad8\u4e86\u5bf9\u9f50\u5ea6\uff0c\u589e\u5f3a\u4e86\u89e3\u91ca\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u89d2\u8272\u6846\u67b6\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u4e2a\u6027\u5316\uff0c\u4f46\u589e\u52a0\u4e86\u8bc4\u5206\u53d8\u5f02\u6027\uff0c\u63ed\u793a\u4e86\u5bf9\u9f50\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "CoReEval\u4e3a\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u5bf9\u9f50\u7814\u7a76\u548c\u4eba\u673a\u5faa\u73af\u8bc4\u4f30\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5728\u6559\u80b2\u548cCI/CD\u7b49\u573a\u666f\u4e2dLLM\u53ef\u4f5c\u4e3a\u53ef\u89e3\u91ca\u3001\u9002\u5e94\u6027\u5f3a\u7684\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.16091", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16091", "abs": "https://arxiv.org/abs/2510.16091", "authors": ["Binglan Han", "Anuradha Mathrani", "Teo Susnjak"], "title": "Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification", "comment": null, "summary": "This study quantifies how prompting strategies interact with large language\nmodels (LLMs) to automate the screening stage of systematic literature reviews\n(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,\nGemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types\n(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)\nacross relevance classification and six Level-2 tasks, using accuracy,\nprecision, recall, and F1. Results show pronounced model-prompt interaction\neffects: CoT-few-shot yields the most reliable precision-recall balance;\nzero-shot maximizes recall for high-sensitivity passes; and self-reflection\nunderperforms due to over-inclusivity and instability across models. GPT-4o and\nDeepSeek provide robust overall performance, while GPT-4o-mini performs\ncompetitively at a substantially lower dollar cost. A cost-performance analysis\nfor relevance classification (per 1,000 abstracts) reveals large absolute\ndifferences among model-prompt pairings; GPT-4o-mini remains low-cost across\nprompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer\nattractive F1 at a small incremental cost. We recommend a staged workflow that\n(1) deploys low-cost models with structured prompts for first-pass screening\nand (2) escalates only borderline cases to higher-capacity models. These\nfindings highlight LLMs' uneven but promising potential to automate literature\nscreening. By systematically analyzing prompt-model interactions, we provide a\ncomparative benchmark and practical guidance for task-adaptive LLM deployment.", "AI": {"tldr": "\u672c\u7814\u7a76\u91cf\u5316\u4e86\u63d0\u793a\u7b56\u7565\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7b5b\u9009\u9636\u6bb5\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u8bc4\u4f30\u4e866\u4e2aLLM\u57285\u79cd\u63d0\u793a\u7c7b\u578b\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0CoT-\u5c11\u6837\u672c\u63d0\u793a\u5728\u7cbe\u786e\u5ea6-\u53ec\u56de\u7387\u5e73\u8861\u65b9\u9762\u6700\u53ef\u9760\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\u5efa\u8bae\u3002", "motivation": "\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7684\u7b5b\u9009\u9636\u6bb5\u8017\u65f6\u8017\u529b\uff0c\u9700\u8981\u63a2\u7d22LLM\u81ea\u52a8\u5316\u7684\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u63d0\u793a\u7b56\u7565\u4e0e\u6a21\u578b\u4ea4\u4e92\u4f5c\u7528\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u8bc4\u4f306\u4e2aLLM\uff08GPT-4o\u3001GPT-4o-mini\u3001DeepSeek-Chat-V3\u3001Gemini-2.5-Flash\u3001Claude-3.5-Haiku\u3001Llama-4-Maverick\uff09\u57285\u79cd\u63d0\u793a\u7c7b\u578b\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u601d\u7ef4\u94fe\u3001CoT-\u5c11\u6837\u672c\u3001\u81ea\u6211\u53cd\u601d\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u51c6\u786e\u7387\u3001\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002", "result": "CoT-\u5c11\u6837\u672c\u63d0\u793a\u5728\u7cbe\u786e\u5ea6-\u53ec\u56de\u7387\u5e73\u8861\u65b9\u9762\u8868\u73b0\u6700\u53ef\u9760\uff1b\u96f6\u6837\u672c\u63d0\u793a\u5728\u9ad8\u7075\u654f\u5ea6\u7b5b\u9009\u65f6\u53ec\u56de\u7387\u6700\u9ad8\uff1b\u81ea\u6211\u53cd\u601d\u63d0\u793a\u56e0\u8fc7\u5ea6\u5305\u542b\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\u8868\u73b0\u4e0d\u4f73\uff1bGPT-4o\u548cDeepSeek\u603b\u4f53\u8868\u73b0\u7a33\u5065\uff1bGPT-4o-mini\u5728\u663e\u8457\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "LLM\u5728\u81ea\u52a8\u5316\u6587\u732e\u7b5b\u9009\u65b9\u9762\u5177\u6709\u4e0d\u5747\u8861\u4f46\u524d\u666f\u5e7f\u9614\u7684\u6f5c\u529b\uff0c\u63a8\u8350\u91c7\u7528\u5206\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff1a\u5148\u7528\u4f4e\u6210\u672c\u6a21\u578b\u914d\u5408\u7ed3\u6784\u5316\u63d0\u793a\u8fdb\u884c\u521d\u6b65\u7b5b\u9009\uff0c\u4ec5\u5bf9\u8fb9\u754c\u6848\u4f8b\u4f7f\u7528\u9ad8\u5bb9\u91cf\u6a21\u578b\u3002", "topic": "agent analysis"}}
{"id": "2510.15974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15974", "abs": "https://arxiv.org/abs/2510.15974", "authors": ["Chris Su", "Harrison Li", "Matheus Marques", "George Flint", "Kevin Zhu", "Sunishchal Dev"], "title": "Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games", "comment": null, "summary": "Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in\nperformance on solving puzzles beyond certain perplexity thresholds. In\nsubsequent discourse, questions have arisen as to whether the nature of the\ntask muddles an evaluation of true reasoning. One potential confound is the\nrequirement that the model keep track of the state space on its own. We provide\na large language model (LLM) with an environment interface for Tower of Hanoi\nproblems, allowing it to make a move with a tool call, provide written\njustification, observe the resulting state space, and reprompt itself for the\nnext move. We observe that access to an environment interface does not delay or\neradicate performance collapse. Furthermore, LLM-parameterized policy analysis\nreveals increasing divergence from both optimal policies and uniformly random\npolicies, suggesting that the model exhibits mode-like collapse at each level\nof complexity, and that performance is dependent upon whether the mode reflects\nthe correct solution for the problem. We suggest that a similar phenomena might\ntake place in LRMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u8c1c\u9898\u65f6\u4f1a\u51fa\u73b0\u6027\u80fd\u5d29\u6e83\uff0c\u5373\u4f7f\u63d0\u4f9b\u73af\u5883\u63a5\u53e3\u4e5f\u65e0\u6cd5\u907f\u514d\u3002\u6a21\u578b\u5728\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u8868\u73b0\u51fa\u6a21\u5f0f\u5d29\u6e83\uff0c\u6027\u80fd\u53d6\u51b3\u4e8e\u6a21\u5f0f\u662f\u5426\u5339\u914d\u6b63\u786e\u89e3\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u8c1c\u9898\u65f6\u6027\u80fd\u5d29\u6e83\u7684\u539f\u56e0\uff0c\u7279\u522b\u662f\u73af\u5883\u72b6\u6001\u8ddf\u8e2a\u662f\u5426\u5f71\u54cd\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u3002", "method": "\u4e3aLLM\u63d0\u4f9b\u6c49\u8bfa\u5854\u95ee\u9898\u7684\u73af\u5883\u63a5\u53e3\uff0c\u5141\u8bb8\u5176\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u8fdb\u884c\u64cd\u4f5c\u3001\u63d0\u4f9b\u4e66\u9762\u7406\u7531\u3001\u89c2\u5bdf\u72b6\u6001\u7a7a\u95f4\u5e76\u91cd\u65b0\u63d0\u793a\u4e0b\u4e00\u6b65\u3002", "result": "\u73af\u5883\u63a5\u53e3\u8bbf\u95ee\u65e0\u6cd5\u5ef6\u8fdf\u6216\u6d88\u9664\u6027\u80fd\u5d29\u6e83\u3002\u7b56\u7565\u5206\u6790\u663e\u793a\u6a21\u578b\u4e0e\u6700\u4f18\u7b56\u7565\u548c\u968f\u673a\u7b56\u7565\u7684\u504f\u79bb\u5ea6\u589e\u52a0\uff0c\u8868\u73b0\u51fa\u6a21\u5f0f\u5d29\u6e83\u3002", "conclusion": "\u6027\u80fd\u5d29\u6e83\u662f\u6a21\u578b\u5185\u5728\u95ee\u9898\uff0c\u7c7b\u4f3c\u73b0\u8c61\u53ef\u80fd\u5b58\u5728\u4e8e\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u3002", "topic": "agent analysis"}}
{"id": "2510.16779", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16779", "abs": "https://arxiv.org/abs/2510.16779", "authors": ["Xiaoyu Guo", "Minggu Wang", "Jianjun Zhao"], "title": "QuanBench: Benchmarking Quantum Code Generation with Large Language Models", "comment": "This paper was accepted by ASE2025", "summary": "Large language models (LLMs) have demonstrated good performance in general\ncode generation; however, their capabilities in quantum code generation remain\ninsufficiently studied. This paper presents QuanBench, a benchmark for\nevaluating LLMs on quantum code generation. QuanBench includes 44 programming\ntasks that cover quantum algorithms, state preparation, gate decomposition, and\nquantum machine learning. Each task has an executable canonical solution and is\nevaluated by functional correctness (Pass@K) and quantum semantic equivalence\n(Process Fidelity). We evaluate several recent LLMs, including general-purpose\nand code-specialized models. The results show that current LLMs have limited\ncapability in generating the correct quantum code, with overall accuracy below\n40% and frequent semantic errors. We also analyze common failure cases, such as\noutdated API usage, circuit construction errors, and incorrect algorithm logic.\nQuanBench provides a basis for future work on improving quantum code generation\nwith LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86QuanBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5305\u542b44\u4e2a\u7f16\u7a0b\u4efb\u52a1\uff0c\u8986\u76d6\u91cf\u5b50\u7b97\u6cd5\u3001\u72b6\u6001\u51c6\u5907\u3001\u95e8\u5206\u89e3\u548c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7b49\u9886\u57df\u3002", "motivation": "\u867d\u7136LLM\u5728\u901a\u7528\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b83\u4eec\u5728\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u5305\u542b44\u4e2a\u7f16\u7a0b\u4efb\u52a1\u7684QuanBench\u57fa\u51c6\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6709\u53ef\u6267\u884c\u7684\u89c4\u8303\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u529f\u80fd\u6b63\u786e\u6027(Pass@K)\u548c\u91cf\u5b50\u8bed\u4e49\u7b49\u4ef7\u6027(Process Fidelity)\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5f53\u524dLLM\u5728\u751f\u6210\u6b63\u786e\u91cf\u5b50\u4ee3\u7801\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u603b\u4f53\u51c6\u786e\u7387\u4f4e\u4e8e40%\uff0c\u7ecf\u5e38\u51fa\u73b0\u8bed\u4e49\u9519\u8bef\uff0c\u5e38\u89c1\u5931\u8d25\u6848\u4f8b\u5305\u62ec\u8fc7\u65f6\u7684API\u4f7f\u7528\u3001\u7535\u8def\u6784\u5efa\u9519\u8bef\u548c\u7b97\u6cd5\u903b\u8f91\u9519\u8bef\u3002", "conclusion": "QuanBench\u4e3a\u672a\u6765\u6539\u8fdbLLM\u7684\u91cf\u5b50\u4ee3\u7801\u751f\u6210\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5f53\u524dLLM\u5728\u91cf\u5b50\u7f16\u7a0b\u9886\u57df\u4ecd\u6709\u8f83\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "topic": "swe benchmark"}}
{"id": "2510.16786", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16786", "abs": "https://arxiv.org/abs/2510.16786", "authors": ["Pengfei Gao", "Chao Peng"], "title": "More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents", "comment": null, "summary": "LLM-powered coding agents, which operate in iterative loops (turns) to solve\nsoftware engineering tasks, are becoming increasingly powerful. However, their\npractical deployment is hindered by significant and unpredictable costs. This\nchallenge arises from a combination of factors: quadratically growing token\ncounts with each turn, the high price of models, the large number of turns\nrequired for real-world tasks, and the tendency of agents to take inefficient\nor unnecessary actions. While existing research focuses on optimizing\nindividual turns, the strategic control of the total number of turns remains an\nunderexplored area for managing agent performance and cost. To address this\ngap, we conduct a comprehensive empirical study on SWE-bench using three\nstate-of-the-art models and evaluate the impact of three distinct turn-control\nstrategies: an unrestricted baseline, a fixed-turn limit with reminders, and a\nnovel dynamic-turn strategy that grants extensions on-demand. Our findings\nfirst reveal a fundamental trade-off in the unrestricted setting, where no\nsingle model excels across performance, cost, and turn efficiency. We then show\nthat a fixed-turn limit, specifically at the 75th percentile of the baseline,\nserves as a \"sweet spot\", substantially reducing costs (by 24%-68%) with\nminimal impact on solve rates. Most significantly, the dynamic-turn strategy\nconsistently outperforms fixed-limit approaches, achieving comparable or better\nsolve rates while further reducing costs by an additional 12%-24% by\nintelligently allocating resources only to tasks that need them. This work\nprovides the first systematic analysis of turn-control strategies, offering\nsimple yet effective guidelines for developers to balance cost and efficacy. We\ndemonstrate that dynamic resource allocation is a superior, easy-to-implement\napproach for deploying powerful yet economically viable coding agents.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLM\u9a71\u52a8\u7684\u4ee3\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u6210\u672c\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u8f6e\u6b21\u63a7\u5236\u7b56\u7565\uff1a\u65e0\u9650\u5236\u57fa\u7ebf\u3001\u56fa\u5b9a\u8f6e\u6b21\u9650\u5236\u548c\u52a8\u6001\u8f6e\u6b21\u7b56\u7565\uff0c\u53d1\u73b0\u52a8\u6001\u8f6e\u6b21\u7b56\u7565\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u80fd\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002", "motivation": "LLM\u4ee3\u7801\u4ee3\u7406\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u663e\u8457\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u6210\u672c\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u8f6e\u6b21\u6570\u91cf\u7684\u6307\u6570\u7ea7\u589e\u957f\u3001\u6a21\u578b\u4ef7\u683c\u9ad8\u6602\u4ee5\u53ca\u4ee3\u7406\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e\u4f18\u5316\uff0c\u800c\u5bf9\u603b\u8f6e\u6b21\u63a7\u5236\u7684\u6218\u7565\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u5728SWE-bench\u4e0a\u4f7f\u7528\u4e09\u79cd\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u4e09\u79cd\u8f6e\u6b21\u63a7\u5236\u7b56\u7565\uff1a\u65e0\u9650\u5236\u57fa\u7ebf\u3001\u5e26\u63d0\u9192\u7684\u56fa\u5b9a\u8f6e\u6b21\u9650\u5236\u3001\u4ee5\u53ca\u6309\u9700\u6269\u5c55\u7684\u65b0\u578b\u52a8\u6001\u8f6e\u6b21\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u56fa\u5b9a\u8f6e\u6b21\u9650\u5236\u5728\u57fa\u7ebf\u7b2c75\u767e\u5206\u4f4d\u6570\u65f6\u662f\"\u6700\u4f73\u70b9\"\uff0c\u80fd\u5927\u5e45\u964d\u4f4e\u6210\u672c(24%-68%)\u4e14\u5bf9\u89e3\u51b3\u7387\u5f71\u54cd\u6700\u5c0f\uff1b\u52a8\u6001\u8f6e\u6b21\u7b56\u7565\u8868\u73b0\u6700\u4f73\uff0c\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u89e3\u51b3\u7387\u7684\u540c\u65f6\u8fdb\u4e00\u6b65\u964d\u4f4e\u6210\u672c12%-24%\u3002", "conclusion": "\u52a8\u6001\u8d44\u6e90\u5206\u914d\u662f\u90e8\u7f72\u5f3a\u5927\u4e14\u7ecf\u6d4e\u53ef\u884c\u7684\u4ee3\u7801\u4ee3\u7406\u7684\u4f18\u8d8a\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u65b9\u6cd5\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5e73\u8861\u6210\u672c\u4e0e\u6548\u80fd\u7684\u7b80\u5355\u6709\u6548\u6307\u5357\u3002", "topic": "agent analysis"}}
{"id": "2510.16809", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL", "68T50, 68N30, 68W40", "I.2.7; D.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.16809", "abs": "https://arxiv.org/abs/2510.16809", "authors": ["Amirkia Rafiei Oskooei", "Kaan Baturalp Cosdan", "Husamettin Isiktas", "Mehmet S. Aktas"], "title": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation", "comment": null, "summary": "Large Language Models (LLMs) with vast context windows offer new avenues for\nin-context learning (ICL), where providing many examples (\"many-shot\"\nprompting) is often assumed to enhance performance. We investigate this\nassumption for the complex task of code translation. Through a large-scale\nempirical study of over 90,000 translations, we systematically evaluate the\nimpact of scaling in-context examples from zero-shot to many-shot\nconfigurations of up to 625 examples, with prompts spanning from approximately\n100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while\nstatic similarity metrics may modestly improve with more examples, functional\ncorrectness consistently peaks with few-shot prompting (5-25 examples).\nProviding substantially more examples often degrades this crucial functional\nperformance. This study highlights that for code translation, the quality of a\nfew well-chosen examples outweighs sheer quantity, challenging the universal\nefficacy of \"more is better\" for ICL and underscoring the task-dependent nature\nof optimal prompting strategies. Our results have significant implications for\neffectively leveraging LLMs in software engineering.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLM\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u5c11\u6837\u672c\u4e0e\u591a\u6837\u672c\u63d0\u793a\u7b56\u7565\uff0c\u53d1\u73b0\u5b58\u5728'\u591a\u6837\u672c\u6096\u8bba'\uff1a\u867d\u7136\u9759\u6001\u76f8\u4f3c\u5ea6\u6307\u6807\u968f\u6837\u672c\u6570\u91cf\u589e\u52a0\u7565\u6709\u63d0\u5347\uff0c\u4f46\u529f\u80fd\u6b63\u786e\u6027\u57285-25\u4e2a\u793a\u4f8b\u65f6\u8fbe\u5230\u5cf0\u503c\uff0c\u66f4\u591a\u6837\u672c\u53cd\u800c\u4f1a\u964d\u4f4e\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u591a\u6837\u672c\u63d0\u793a\u7b56\u7565\u662f\u5426\u771f\u7684\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u6311\u6218'\u8d8a\u591a\u8d8a\u597d'\u7684\u666e\u904d\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5bf9\u8d85\u8fc790,000\u4e2a\u7ffb\u8bd1\u6848\u4f8b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4ece\u96f6\u6837\u672c\u5230\u591a\u6837\u672c\uff08\u6700\u591a625\u4e2a\u793a\u4f8b\uff09\u914d\u7f6e\u7684\u6027\u80fd\u8868\u73b0\uff0c\u63d0\u793a\u957f\u5ea6\u4ece\u7ea610\u4e07\u523080\u4e07token\u3002", "result": "\u53d1\u73b0\u529f\u80fd\u6b63\u786e\u6027\u5728\u5c11\u6837\u672c\u63d0\u793a\uff085-25\u4e2a\u793a\u4f8b\uff09\u65f6\u8fbe\u5230\u6700\u4f73\uff0c\u800c\u63d0\u4f9b\u66f4\u591a\u793a\u4f8b\u4f1a\u964d\u4f4e\u529f\u80fd\u6027\u80fd\uff0c\u5c3d\u7ba1\u9759\u6001\u76f8\u4f3c\u5ea6\u6307\u6807\u53ef\u80fd\u7565\u6709\u6539\u5584\u3002", "conclusion": "\u5bf9\u4e8e\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\uff0c\u5c11\u91cf\u7cbe\u5fc3\u9009\u62e9\u7684\u793a\u4f8b\u8d28\u91cf\u6bd4\u6570\u91cf\u66f4\u91cd\u8981\uff0c\u6311\u6218\u4e86ICL\u4e2d'\u8d8a\u591a\u8d8a\u597d'\u7684\u666e\u904d\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u6700\u4f18\u63d0\u793a\u7b56\u7565\u7684\u4efb\u52a1\u4f9d\u8d56\u6027\u3002", "topic": "swe application"}}
{"id": "2510.16823", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16823", "abs": "https://arxiv.org/abs/2510.16823", "authors": ["Yue Liu", "Zhenchang Xing", "Shidong Pan", "Chakkrit Tantithamthavorn"], "title": "When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation", "comment": null, "summary": "In recent years, the AI wave has grown rapidly in software development. Even\nnovice developers can now design and generate complex framework-constrained\nsoftware systems based on their high-level requirements with the help of Large\nLanguage Models (LLMs). However, when LLMs gradually \"take the wheel\" of\nsoftware development, developers may only check whether the program works. They\noften miss security problems hidden in how the generated programs are\nimplemented.\n  In this work, we investigate the security properties of framework-constrained\nprograms generated by state-of-the-art LLMs. We focus specifically on Chrome\nextensions due to their complex security model involving multiple privilege\nboundaries and isolated components. To achieve this, we built ChromeSecBench, a\ndataset with 140 prompts based on known vulnerable extensions. We used these\nprompts to instruct nine state-of-the-art LLMs to generate complete Chrome\nextensions, and then analyzed them for vulnerabilities across three dimensions:\nscenario types, model differences, and vulnerability categories. Our results\nshow that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),\nparticularly in Authentication & Identity and Cookie Management scenarios (up\nto 83% and 78% respectively). Most vulnerabilities exposed sensitive browser\ndata like cookies, history, or bookmarks to untrusted code. Interestingly, we\nfound that advanced reasoning models performed worse, generating more\nvulnerabilities than simpler models. These findings highlight a critical gap\nbetween LLMs' coding skills and their ability to write secure\nframework-constrained programs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u7684Chrome\u6269\u5c55\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u5728\u8ba4\u8bc1\u548cCookie\u7ba1\u7406\u573a\u666f\u4e2d\u6f0f\u6d1e\u7387\u9ad8\u8fbe83%\u548c78%\uff0c\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u53cd\u800c\u8868\u73b0\u66f4\u5dee\u3002", "motivation": "\u968f\u7740LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u5f00\u53d1\u8005\u53ef\u80fd\u53ea\u5173\u6ce8\u529f\u80fd\u800c\u5ffd\u7565\u5b9e\u73b0\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6846\u67b6\u7ea6\u675f\u7a0b\u5e8f\u4e2d\u3002", "method": "\u6784\u5efaChromeSecBench\u6570\u636e\u96c6\uff08140\u4e2a\u57fa\u4e8e\u5df2\u77e5\u6f0f\u6d1e\u6269\u5c55\u7684\u63d0\u793a\uff09\uff0c\u4f7f\u75289\u4e2a\u5148\u8fdbLLM\u751f\u6210Chrome\u6269\u5c55\uff0c\u4ece\u573a\u666f\u7c7b\u578b\u3001\u6a21\u578b\u5dee\u5f02\u548c\u6f0f\u6d1e\u7c7b\u522b\u4e09\u4e2a\u7ef4\u5ea6\u5206\u6790\u5b89\u5168\u6027\u3002", "result": "LLM\u751f\u6210\u6613\u53d7\u653b\u51fb\u7a0b\u5e8f\u7684\u6bd4\u4f8b\u60ca\u4eba\uff0818%-50%\uff09\uff0c\u8ba4\u8bc1\u4e0e\u8eab\u4efd\u7ba1\u7406\u573a\u666f\u6f0f\u6d1e\u738783%\uff0cCookie\u7ba1\u740678%\uff0c\u591a\u6570\u6f0f\u6d1e\u66b4\u9732\u654f\u611f\u6d4f\u89c8\u5668\u6570\u636e\u7ed9\u975e\u4fe1\u4efb\u4ee3\u7801\u3002", "conclusion": "LLM\u7684\u7f16\u7801\u80fd\u529b\u4e0e\u7f16\u5199\u5b89\u5168\u6846\u67b6\u7ea6\u675f\u7a0b\u5e8f\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u5dee\u8ddd\uff0c\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u8868\u73b0\u66f4\u5dee\u3002", "topic": "swe application"}}
{"id": "2510.17130", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17130", "abs": "https://arxiv.org/abs/2510.17130", "authors": ["Shuzheng Gao", "Chaozheng Wang", "Cuiyun Gao", "Michael R. Lyu"], "title": "SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning", "comment": "The paper was completed in Feb. 2025, submitted to ICSE 2026 in Mar.\n  2025, received a major revision in Jun. 2025, and was finally accepted in\n  Oct. 2025", "summary": "Code generation, the task of creating executable programs from natural\nlanguage requirements, has recently seen tremendous advances through\nChain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to\ndevelop high-level reasoning plans before writing code. Recent research has\nproposed various methods to enhance models' CoT reasoning for code generation\nsuch as prompt engineering and supervised fine-tuning. However, existing\napproaches still face three critical limitations: (1) limited exploration of\ndiverse reasoning paths, which constrains generalization across various\nprogramming scenarios, (2) lack of quality assessment for intermediate\nreasoning steps, which hampers the reliability of the generated plans and code,\nand (3) the potential negative impact of \"overthinking\", potentially leading to\nunnecessarily complex and incorrect solutions. To address these limitations, we\nframe CoT code generation as a decision making problem and present SEER, a\nSElf-Exploring deep Reasoning framework that enables accurate and adaptive\nreasoning for code generation. SEER introduces three key components: (1)\nDiverse reasoning path exploration, which aims at exploring diverse reasoning\npaths and annotating intermediate steps without relying on manual experts or\nclosed-source proprietary models; (2) Reasoning quality-aware model training,\nwhich trains a policy model for generating candidate reasoning steps and a\nvalue model for assessing their quality; and (3) Adaptive CoT reasoning, which\ndynamically switches between direct generation and step-by-step reasoning for\ndifferent problems.", "AI": {"tldr": "SEER\u662f\u4e00\u4e2a\u81ea\u63a2\u7d22\u6df1\u5ea6\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u4ee3\u7801\u751f\u6210\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u6784\u5efa\u4e3a\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6837\u5316\u63a8\u7406\u8def\u5f84\u63a2\u7d22\u3001\u8d28\u91cf\u611f\u77e5\u6a21\u578b\u8bad\u7ec3\u548c\u81ea\u9002\u5e94\u63a8\u7406\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u63a8\u7406\u8def\u5f84\u591a\u6837\u6027\u4e0d\u8db3\u3001\u4e2d\u95f4\u6b65\u9aa4\u8d28\u91cf\u8bc4\u4f30\u7f3a\u5931\u3001\u4ee5\u53ca'\u8fc7\u5ea6\u601d\u8003'\u53ef\u80fd\u5bfc\u81f4\u590d\u6742\u9519\u8bef\u89e3\u51b3\u65b9\u6848\u3002", "method": "SEER\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u591a\u6837\u5316\u63a8\u7406\u8def\u5f84\u63a2\u7d22\uff08\u65e0\u9700\u4eba\u5de5\u4e13\u5bb6\u6216\u95ed\u6e90\u6a21\u578b\uff09\u3001\u63a8\u7406\u8d28\u91cf\u611f\u77e5\u6a21\u578b\u8bad\u7ec3\uff08\u7b56\u7565\u6a21\u578b\u751f\u6210\u63a8\u7406\u6b65\u9aa4\uff0c\u4ef7\u503c\u6a21\u578b\u8bc4\u4f30\u8d28\u91cf\uff09\u3001\u81ea\u9002\u5e94\u601d\u7ef4\u94fe\u63a8\u7406\uff08\u6839\u636e\u95ee\u9898\u52a8\u6001\u5207\u6362\u76f4\u63a5\u751f\u6210\u4e0e\u9010\u6b65\u63a8\u7406\uff09\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u66f4\u51c6\u786e\u548c\u81ea\u9002\u5e94\u7684\u4ee3\u7801\u751f\u6210\u63a8\u7406\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u63a8\u7406\u591a\u6837\u6027\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u8fc7\u5ea6\u601d\u8003\u7684\u95ee\u9898\u3002", "conclusion": "SEER\u901a\u8fc7\u5c06\u4ee3\u7801\u751f\u6210\u6784\u5efa\u4e3a\u51b3\u7b56\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u548c\u9ad8\u6548\u7684\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u548c\u9002\u5e94\u6027\u3002", "topic": "code agent"}}
{"id": "2510.17142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17142", "abs": "https://arxiv.org/abs/2510.17142", "authors": ["Xiaoxue Ren", "Jun Wan", "Yun Peng", "Zhongxin Liu", "Ming Liang", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant capability in code\ngeneration, but their potential in code efficiency optimization remains\nunderexplored. Previous LLM-based code efficiency optimization approaches\nexclusively focus on function-level optimization and overlook interaction\nbetween functions, failing to generalize to real-world development scenarios.\nCode editing techniques show great potential for conducting project-level\noptimization, yet they face challenges associated with invalid edits and\nsuboptimal internal functions. To address these gaps, we propose Peace, a novel\nhybrid framework for Project-level code Efficiency optimization through\nAutomatic Code Editing, which also ensures the overall correctness and\nintegrity of the project. Peace integrates three key phases: dependency-aware\noptimizing function sequence construction, valid associated edits\nidentification, and efficiency optimization editing iteration. To rigorously\nevaluate the effectiveness of Peace, we construct PeacExec, the first benchmark\ncomprising 146 real-world optimization tasks from 47 high-impact GitHub Python\nprojects, along with highly qualified test cases and executable environments.\nExtensive experiments demonstrate Peace's superiority over the state-of-the-art\nbaselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and\n0.840 speedup in execution efficiency. Notably, our Peace outperforms all\nbaselines by significant margins, particularly in complex optimization tasks\nwith multiple functions. Moreover, extensive experiments are also conducted to\nvalidate the contributions of each component in Peace, as well as the rationale\nand effectiveness of our hybrid framework design.", "AI": {"tldr": "\u63d0\u51fa\u4e86Peace\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u4ee3\u7801\u7f16\u8f91\u5b9e\u73b0\u9879\u76ee\u7ea7\u4ee3\u7801\u6548\u7387\u4f18\u5316\uff0c\u786e\u4fdd\u9879\u76ee\u6574\u4f53\u6b63\u786e\u6027\u548c\u5b8c\u6574\u6027\u3002\u5728\u771f\u5b9e\u4e16\u754c\u4f18\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6b63\u786e\u7387\u8fbe69.2%\uff0c\u4f18\u5316\u7387\u63d0\u534746.9%\uff0c\u6267\u884c\u6548\u7387\u52a0\u901f0.840\u500d\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7801\u6548\u7387\u4f18\u5316\u65b9\u6cd5\u4ec5\u5173\u6ce8\u51fd\u6570\u7ea7\u4f18\u5316\uff0c\u5ffd\u7565\u51fd\u6570\u95f4\u4ea4\u4e92\uff0c\u65e0\u6cd5\u63a8\u5e7f\u5230\u771f\u5b9e\u5f00\u53d1\u573a\u666f\u3002\u4ee3\u7801\u7f16\u8f91\u6280\u672f\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u65e0\u6548\u7f16\u8f91\u548c\u6b21\u4f18\u5185\u90e8\u51fd\u6570\u7684\u6311\u6218\u3002", "method": "Peace\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u9636\u6bb5\uff1a\u4f9d\u8d56\u611f\u77e5\u7684\u4f18\u5316\u51fd\u6570\u5e8f\u5217\u6784\u5efa\u3001\u6709\u6548\u5173\u8054\u7f16\u8f91\u8bc6\u522b\u3001\u6548\u7387\u4f18\u5316\u7f16\u8f91\u8fed\u4ee3\u3002\u6784\u5efa\u4e86\u9996\u4e2a\u771f\u5b9e\u4e16\u754c\u4f18\u5316\u57fa\u51c6PeacExec\uff0c\u5305\u542b146\u4e2a\u4efb\u52a1\u548c\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "Peace\u5728\u6b63\u786e\u7387\u3001\u4f18\u5316\u7387\u548c\u6267\u884c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u591a\u4e2a\u51fd\u6570\u7684\u590d\u6742\u4f18\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Peace\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9879\u76ee\u7ea7\u4ee3\u7801\u6548\u7387\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u786e\u4fdd\u9879\u76ee\u5b8c\u6574\u6027\u548c\u4f18\u5316\u6548\u679c\uff0c\u4e3aLLM\u5728\u4ee3\u7801\u4f18\u5316\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "swe application"}}
{"id": "2510.17163", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17163", "abs": "https://arxiv.org/abs/2510.17163", "authors": ["Shuzheng Gao", "Eric John Li", "Man Ho Lam", "Jingyu Xiao", "Yuxuan Wan", "Chaozheng Wang", "Ng Man Tik", "Michael R. Lyu"], "title": "TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework", "comment": null, "summary": "Large foundation models are fundamentally transforming the software\nengineering landscape, demonstrating exceptional capabilities across diverse\ntasks such as code generation, debugging, and testing. Despite this rapid\nprogress, a significant gap remains in how to comprehensively evaluate these\nmodels' trustworthiness in real-world software engineering scenarios. Existing\nbenchmarks suffer from limited task scope and fail to incorporate critical\nevaluation aspects such as the robustness and reliability of models. To bridge\nthis gap, we present an evaluation framework called TREAT (Code LLMs\nTrustworthiness / Reliability Evaluation And Testing) that provides a holistic\nassessment of model performance in code intelligence tasks. Our evaluation\nframework addresses key limitations in existing approaches with four main\nimprovements: (1) Multi-Task Holistic Evaluation that spans diverse software\nengineering activities rather than limited coding tasks; (2) Multi-Language and\nMulti-Modality Assessment that extends beyond traditional single-language,\ntext-only benchmarks to include multi-modality coding tasks; (3) Robustness\nAssessment that evaluates model reliability under semantically-preserving code\ntransformations; and (4) Rigorous Evaluation Methodology that enhances the\ntrustworthiness of evaluation results through diverse evaluation prompts and\nadaptive solution extraction. Based on this evaluation framework, we assess 26\nstate-of-the-art models and uncover both their strengths and limitations,\nyielding several key insights:(1) Current models show substantial performance\nvariation across programming tasks; (2) Multi-modal language models demonstrate\nspecific performance limitations in UI code generation and edit;", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTREAT\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u4efb\u52a1\u8303\u56f4\u6709\u9650\u3001\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u8bc4\u4f30\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u771f\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u573a\u666f\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86TREAT\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u4e3b\u8981\u6539\u8fdb\uff1a\u591a\u4efb\u52a1\u6574\u4f53\u8bc4\u4f30\u3001\u591a\u8bed\u8a00\u591a\u6a21\u6001\u8bc4\u4f30\u3001\u9c81\u68d2\u6027\u8bc4\u4f30\uff08\u5728\u8bed\u4e49\u4fdd\u6301\u7684\u4ee3\u7801\u8f6c\u6362\u4e0b\u8bc4\u4f30\u6a21\u578b\u53ef\u9760\u6027\uff09\u548c\u4e25\u8c28\u7684\u8bc4\u4f30\u65b9\u6cd5\uff08\u901a\u8fc7\u591a\u6837\u5316\u8bc4\u4f30\u63d0\u793a\u548c\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u63d0\u53d6\uff09\u3002", "result": "\u8bc4\u4f30\u4e8626\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u53d1\u73b0\uff1a\u5f53\u524d\u6a21\u578b\u5728\u4e0d\u540c\u7f16\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff1b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728UI\u4ee3\u7801\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u5b58\u5728\u7279\u5b9a\u6027\u80fd\u9650\u5236\u3002", "conclusion": "TREAT\u6846\u67b6\u4e3a\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u548c\u53ef\u9760\u7684\u53ef\u4fe1\u5ea6\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "swe benchmark"}}
{"id": "2510.16194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16194", "abs": "https://arxiv.org/abs/2510.16194", "authors": ["Guanchen Wu", "Zuhui Chen", "Yuzhang Xie", "Carl Yang"], "title": "Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration", "comment": "Agents4Science 2025 (Spotlight)", "summary": "Protected health information (PHI) de-identification is critical for enabling\nthe safe reuse of clinical notes, yet evaluating and comparing PHI\nde-identification models typically depends on costly, small-scale expert\nannotations. We present TEAM-PHI, a multi-agent evaluation and selection\nframework that uses large language models (LLMs) to automatically measure\nde-identification quality and select the best-performing model without heavy\nreliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each\nindependently judging the correctness of PHI extractions and outputting\nstructured metrics. Their results are then consolidated through an LLM-based\nmajority voting mechanism that integrates diverse evaluator perspectives into a\nsingle, stable, and reproducible ranking. Experiments on a real-world clinical\nnote corpus demonstrate that TEAM-PHI produces consistent and accurate\nrankings: despite variation across individual evaluators, LLM-based voting\nreliably converges on the same top-performing systems. Further comparison with\nground-truth annotations and human evaluation confirms that the framework's\nautomated rankings closely match supervised evaluation. By combining\nindependent evaluation agents with LLM majority voting, TEAM-PHI offers a\npractical, secure, and cost-effective solution for automatic evaluation and\nbest-model selection in PHI de-identification, even when ground-truth labels\nare limited.", "AI": {"tldr": "\u63d0\u51fa\u4e86TEAM-PHI\u591a\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528LLM\u81ea\u52a8\u8bc4\u4f30PHI\u53bb\u6807\u8bc6\u5316\u8d28\u91cf\u5e76\u9009\u62e9\u6700\u4f73\u6a21\u578b\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "PHI\u53bb\u6807\u8bc6\u5316\u5bf9\u4e8e\u5b89\u5168\u91cd\u7528\u4e34\u5e8a\u7b14\u8bb0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u8bc4\u4f30\u4f9d\u8d56\u6602\u8d35\u7684\u5c0f\u89c4\u6a21\u4e13\u5bb6\u6807\u6ce8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u90e8\u7f72\u591a\u4e2a\u8bc4\u4f30\u667a\u80fd\u4f53\u72ec\u7acb\u5224\u65adPHI\u63d0\u53d6\u6b63\u786e\u6027\uff0c\u901a\u8fc7LLM\u591a\u6570\u6295\u7968\u673a\u5236\u6574\u5408\u7ed3\u679c\uff0c\u4ea7\u751f\u7a33\u5b9a\u53ef\u590d\u73b0\u7684\u6392\u540d\u3002", "result": "\u5728\u771f\u5b9e\u4e34\u5e8a\u7b14\u8bb0\u8bed\u6599\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTEAM-PHI\u4ea7\u751f\u4e00\u81f4\u51c6\u786e\u7684\u6392\u540d\uff0cLLM\u6295\u7968\u53ef\u9760\u5730\u6536\u655b\u4e8e\u76f8\u540c\u7684\u6700\u4f73\u7cfb\u7edf\u3002", "conclusion": "TEAM-PHI\u901a\u8fc7\u7ed3\u5408\u72ec\u7acb\u8bc4\u4f30\u667a\u80fd\u4f53\u548cLLM\u591a\u6570\u6295\u7968\uff0c\u4e3aPHI\u53bb\u6807\u8bc6\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u5b89\u5168\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u81ea\u52a8\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.17164", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17164", "abs": "https://arxiv.org/abs/2510.17164", "authors": ["Maria Deolinda Santana", "Cleyton Magalhaes", "Ronnie de Souza Santos"], "title": "Software Testing with Large Language Models: An Interview Study with Practitioners", "comment": null, "summary": "\\textit{Background:} The use of large language models in software testing is\ngrowing fast as they support numerous tasks, from test case generation to\nautomation, and documentation. However, their adoption often relies on informal\nexperimentation rather than structured guidance. \\textit{Aims:} This study\ninvestigates how software testing professionals use LLMs in practice to propose\na preliminary, practitioner-informed guideline to support their integration\ninto testing workflows. \\textit{Method:} We conducted a qualitative study with\n15 software testers from diverse roles and domains. Data were collected through\nsemi-structured interviews and analyzed using grounded theory-based processes\nfocused on thematic analysis. \\textit{Results:} Testers described an iterative\nand reflective process that included defining testing objectives, applying\nprompt engineering strategies, refining prompts, evaluating outputs, and\nlearning over time. They emphasized the need for human oversight and careful\nvalidation, especially due to known limitations of LLMs such as hallucinations\nand inconsistent reasoning. \\textit{Conclusions:} LLM adoption in software\ntesting is growing, but remains shaped by evolving practices and caution around\nrisks. This study offers a starting point for structuring LLM use in testing\ncontexts and invites future research to refine these practices across teams,\ntools, and tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9a\u6027\u8bbf\u8c08\u8c03\u67e5\u4e86\u8f6f\u4ef6\u6d4b\u8bd5\u4e13\u4e1a\u4eba\u5458\u5982\u4f55\u5b9e\u9645\u4f7f\u7528LLM\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u8df5\u8005\u7ecf\u9a8c\u7684\u521d\u6b65\u6307\u5357\uff0c\u4ee5\u652f\u6301LLM\u5728\u6d4b\u8bd5\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u96c6\u6210\u3002", "motivation": "\u968f\u7740LLM\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u5feb\u901f\u5e94\u7528\uff0c\u4ece\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5230\u81ea\u52a8\u5316\u548c\u6587\u6863\u5316\u7b49\u4efb\u52a1\uff0c\u4f46\u5176\u91c7\u7528\u5f80\u5f80\u4f9d\u8d56\u4e8e\u975e\u6b63\u5f0f\u5b9e\u9a8c\u800c\u975e\u7ed3\u6784\u5316\u6307\u5bfc\u3002", "method": "\u5bf915\u540d\u6765\u81ea\u4e0d\u540c\u89d2\u8272\u548c\u9886\u57df\u7684\u8f6f\u4ef6\u6d4b\u8bd5\u4eba\u5458\u8fdb\u884c\u5b9a\u6027\u7814\u7a76\uff0c\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u6536\u96c6\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u624e\u6839\u7406\u8bba\u7684\u4e3b\u9898\u5206\u6790\u8fc7\u7a0b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6d4b\u8bd5\u4eba\u5458\u63cf\u8ff0\u4e86\u4e00\u4e2a\u8fed\u4ee3\u548c\u53cd\u601d\u7684\u8fc7\u7a0b\uff0c\u5305\u62ec\u5b9a\u4e49\u6d4b\u8bd5\u76ee\u6807\u3001\u5e94\u7528\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u3001\u4f18\u5316\u63d0\u793a\u3001\u8bc4\u4f30\u8f93\u51fa\u548c\u968f\u65f6\u95f4\u5b66\u4e60\u3002\u4ed6\u4eec\u5f3a\u8c03\u9700\u8981\u4eba\u5de5\u76d1\u7763\u548c\u4ed4\u7ec6\u9a8c\u8bc1\uff0c\u7279\u522b\u662f\u7531\u4e8eLLM\u7684\u5df2\u77e5\u9650\u5236\uff0c\u5982\u5e7b\u89c9\u548c\u4e0d\u4e00\u81f4\u7684\u63a8\u7406\u3002", "conclusion": "LLM\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u91c7\u7528\u6b63\u5728\u589e\u957f\uff0c\u4f46\u4ecd\u53d7\u5230\u4e0d\u65ad\u53d1\u5c55\u7684\u5b9e\u8df5\u548c\u5bf9\u98ce\u9669\u8c28\u614e\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u4e3a\u5728\u6d4b\u8bd5\u73af\u5883\u4e2d\u7ed3\u6784\u5316\u4f7f\u7528LLM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d77\u70b9\uff0c\u5e76\u9080\u8bf7\u672a\u6765\u7814\u7a76\u5728\u56e2\u961f\u3001\u5de5\u5177\u548c\u4efb\u52a1\u4e2d\u5b8c\u5584\u8fd9\u4e9b\u5b9e\u8df5\u3002", "topic": "swe application"}}
{"id": "2510.15969", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15969", "abs": "https://arxiv.org/abs/2510.15969", "authors": ["Paul-Niklas Ken Kandora", "Simon Caspar Zeller", "Aaron Jeremias Elsing", "Elena Kuss", "Steffen Rebennack"], "title": "LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems", "comment": null, "summary": "Reformulating nonlinear optimization problems is largely manual and\nexpertise-intensive, yet it remains essential for solving such problems with\nlinear optimization solvers or applying special-purpose algorithms. We\nintroduce \\textit{LinearizeLLM}, an agent-based framework that solves this task\nby leveraging Large Language Models (LLMs). The framework assigns each\nnonlinear pattern to a \\textit{reformulation agent} that is explicitly\ninstructed to derive an exact linear reformulation for its nonlinearity\npattern, for instance, absolute-value terms or bilinear products of decision\nvariables. The agents then coordinate to assemble a solver-ready linear model\nequivalent to the original problem. To benchmark the approach, we create a\ndataset of 20 real-world nonlinear optimization problems derived from the\nestablished ComplexOR dataset of linear optimization problems. We evaluate our\napproach with several LLMs. Our results indicate that specialized LLM agents\ncan automate linearization tasks, opening a path toward fully conversational\nmodeling pipelines for nonlinear optimization.", "AI": {"tldr": "LinearizeLLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5c06\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u7ebf\u6027\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u4ee3\u7406\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u975e\u7ebf\u6027\u6a21\u5f0f\u3002", "motivation": "\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u7684\u7ebf\u6027\u5316\u901a\u5e38\u9700\u8981\u4eba\u5de5\u64cd\u4f5c\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0c\u8fd9\u9650\u5236\u4e86\u7ebf\u6027\u4f18\u5316\u6c42\u89e3\u5668\u7684\u5e94\u7528\u3002\u7814\u7a76\u65e8\u5728\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u4f7f\u975e\u7ebf\u6027\u95ee\u9898\u80fd\u591f\u76f4\u63a5\u4f7f\u7528\u7ebf\u6027\u6c42\u89e3\u5668\u89e3\u51b3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u4e3a\u6bcf\u79cd\u975e\u7ebf\u6027\u6a21\u5f0f\u5206\u914d\u4e13\u95e8\u7684reformulation\u4ee3\u7406\uff0c\u8fd9\u4e9b\u4ee3\u7406\u88ab\u660e\u786e\u6307\u793a\u4e3a\u5176\u975e\u7ebf\u6027\u6a21\u5f0f\u63a8\u5bfc\u7cbe\u786e\u7684\u7ebf\u6027\u91cd\u6784\uff0c\u7136\u540e\u534f\u8c03\u7ec4\u88c5\u6210\u6c42\u89e3\u5668\u5c31\u7eea\u7684\u7ebf\u6027\u6a21\u578b\u3002", "result": "\u5728\u57fa\u4e8eComplexOR\u6570\u636e\u96c6\u768420\u4e2a\u771f\u5b9e\u4e16\u754c\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u4e13\u95e8\u7684LLM\u4ee3\u7406\u80fd\u591f\u81ea\u52a8\u5316\u7ebf\u6027\u5316\u4efb\u52a1\u3002", "conclusion": "\u4e13\u95e8\u7684LLM\u4ee3\u7406\u53ef\u4ee5\u81ea\u52a8\u5316\u7ebf\u6027\u5316\u4efb\u52a1\uff0c\u4e3a\u975e\u7ebf\u6027\u4f18\u5316\u7684\u5b8c\u5168\u5bf9\u8bdd\u5f0f\u5efa\u6a21\u6d41\u7a0b\u5f00\u8f9f\u4e86\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.16380", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16380", "abs": "https://arxiv.org/abs/2510.16380", "authors": ["Yu Ying Chiu", "Michael S. Lee", "Rachel Calcott", "Brandon Handoko", "Paul de Font-Reaulx", "Paula Rodriguez", "Chen Bo Calvin Zhang", "Ziwen Han", "Udari Madhushani Sehwag", "Yash Maurya", "Christina Q Knight", "Harry R. Lloyd", "Florence Bacus", "Mantas Mazeika", "Bing Liu", "Yejin Choi", "Mitchell L Gordon", "Sydney Levine"], "title": "MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes", "comment": "46 pages, 8 figures, 10 tables. Preprint", "summary": "As AI systems progress, we rely more on them to make decisions with us and\nfor us. To ensure that such decisions are aligned with human values, it is\nimperative for us to understand not only what decisions they make but also how\nthey come to those decisions. Reasoning language models, which provide both\nfinal responses and (partially transparent) intermediate thinking traces,\npresent a timely opportunity to study AI procedural reasoning. Unlike math and\ncode problems which often have objectively correct answers, moral dilemmas are\nan excellent testbed for process-focused evaluation because they allow for\nmultiple defensible conclusions. To do so, we present MoReBench: 1,000 moral\nscenarios, each paired with a set of rubric criteria that experts consider\nessential to include (or avoid) when reasoning about the scenarios. MoReBench\ncontains over 23 thousand criteria including identifying moral considerations,\nweighing trade-offs, and giving actionable recommendations to cover cases on AI\nadvising humans moral decisions as well as making moral decisions autonomously.\nSeparately, we curate MoReBench-Theory: 150 examples to test whether AI can\nreason under five major frameworks in normative ethics. Our results show that\nscaling laws and existing benchmarks on math, code, and scientific reasoning\ntasks fail to predict models' abilities to perform moral reasoning. Models also\nshow partiality towards specific moral frameworks (e.g., Benthamite Act\nUtilitarianism and Kantian Deontology), which might be side effects of popular\ntraining paradigms. Together, these benchmarks advance process-focused\nreasoning evaluation towards safer and more transparent AI.", "AI": {"tldr": "MoReBench\u662f\u4e00\u4e2a\u5305\u542b1000\u4e2a\u9053\u5fb7\u573a\u666f\u548c23,000\u591a\u4e2a\u8bc4\u4f30\u6807\u51c6\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u7684\u9053\u5fb7\u63a8\u7406\u8fc7\u7a0b\uff0c\u91cd\u70b9\u5173\u6ce8\u63a8\u7406\u8fc7\u7a0b\u800c\u975e\u6700\u7ec8\u7b54\u6848\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u51b3\u7b56\u4e2d\u626e\u6f14\u66f4\u91cd\u8981\u7684\u89d2\u8272\uff0c\u9700\u8981\u7406\u89e3\u5b83\u4eec\u5982\u4f55\u505a\u51fa\u51b3\u7b56\uff0c\u7279\u522b\u662f\u9053\u5fb7\u51b3\u7b56\u3002\u9053\u5fb7\u56f0\u5883\u662f\u8bc4\u4f30AI\u63a8\u7406\u8fc7\u7a0b\u7684\u7406\u60f3\u6d4b\u8bd5\u5e73\u53f0\uff0c\u56e0\u4e3a\u5b83\u4eec\u5141\u8bb8\u591a\u79cd\u5408\u7406\u7684\u7ed3\u8bba\u3002", "method": "\u521b\u5efaMoReBench\u57fa\u51c6\uff0c\u5305\u542b\u9053\u5fb7\u573a\u666f\u548c\u4e13\u5bb6\u5236\u5b9a\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u6db5\u76d6\u9053\u5fb7\u8003\u91cf\u8bc6\u522b\u3001\u6743\u8861\u5206\u6790\u548c\u53ef\u64cd\u4f5c\u5efa\u8bae\u3002\u540c\u65f6\u521b\u5efaMoReBench-Theory\u6d4b\u8bd5AI\u5728\u4e94\u79cd\u89c4\u8303\u4f26\u7406\u5b66\u6846\u67b6\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6269\u5c55\u5b9a\u5f8b\u548c\u73b0\u6709\u6570\u5b66\u3001\u4ee3\u7801\u3001\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u65e0\u6cd5\u9884\u6d4b\u6a21\u578b\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u3002\u6a21\u578b\u5bf9\u7279\u5b9a\u9053\u5fb7\u6846\u67b6\uff08\u5982\u8fb9\u6c81\u529f\u5229\u4e3b\u4e49\u548c\u5eb7\u5fb7\u4e49\u52a1\u8bba\uff09\u8868\u73b0\u51fa\u504f\u597d\uff0c\u8fd9\u53ef\u80fd\u662f\u6d41\u884c\u8bad\u7ec3\u8303\u5f0f\u7684\u526f\u4f5c\u7528\u3002", "conclusion": "\u8fd9\u4e9b\u57fa\u51c6\u63a8\u52a8\u4e86\u4ee5\u8fc7\u7a0b\u4e3a\u91cd\u70b9\u7684\u63a8\u7406\u8bc4\u4f30\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u66f4\u900f\u660e\u7684AI\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2510.16234", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16234", "abs": "https://arxiv.org/abs/2510.16234", "authors": ["Hanane Nour Moussa", "Patrick Queiroz Da Silva", "Daniel Adu-Ampratwum", "Alyson East", "Zitong Lu", "Nikki Puccetti", "Mingyi Xue", "Huan Sun", "Bodhisattwa Prasad Majumder", "Sachin Kumar"], "title": "ScholarEval: Research Idea Evaluation Grounded in Literature", "comment": null, "summary": "As AI tools become increasingly common for research ideation, robust\nevaluation is critical to ensure the validity and usefulness of generated\nideas. We introduce ScholarEval, a retrieval augmented evaluation framework\nthat assesses research ideas based on two fundamental criteria: soundness - the\nempirical validity of proposed methods based on existing literature, and\ncontribution - the degree of advancement made by the idea across different\ndimensions relative to prior research. To evaluate ScholarEval, we introduce\nScholarIdeas, the first expert-annotated dataset of multi-domain research ideas\nand reviews, comprised of 117 ideas across four disciplines: artificial\nintelligence, neuroscience, biochemistry, and ecology. Our evaluation shows\nthat ScholarEval achieves significantly higher coverage of points mentioned in\nthe human expert annotated rubrics in ScholarIdeas compared to all baselines.\nFurthermore, ScholarEval is consistently preferred over our strongest baseline\no4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI,\nin terms of evaluation actionability, depth, and evidence support. Our\nlarge-scale user study also shows that ScholarEval significantly outperforms\ndeep research in literature engagement, idea refinement, and usefulness. We\nopenly release our code, dataset, and ScholarEval tool for the community to use\nand build on.", "AI": {"tldr": "\u63d0\u51fa\u4e86ScholarEval\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u7684\u7814\u7a76\u60f3\u6cd5\u8bc4\u4f30\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u751f\u6210\u7814\u7a76\u60f3\u6cd5\u7684\u5408\u7406\u6027\u548c\u8d21\u732e\u5ea6\u3002", "motivation": "\u968f\u7740AI\u5de5\u5177\u5728\u79d1\u7814\u6784\u601d\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u9760\u7684\u8bc4\u4f30\u673a\u5236\u6765\u786e\u4fdd\u751f\u6210\u60f3\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u5f00\u53d1\u4e86ScholarEval\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8e\u4e24\u4e2a\u6838\u5fc3\u6807\u51c6\uff1a\u5408\u7406\u6027\uff08\u57fa\u4e8e\u73b0\u6709\u6587\u732e\u7684\u65b9\u6cd5\u6709\u6548\u6027\uff09\u548c\u8d21\u732e\u5ea6\uff08\u76f8\u5bf9\u4e8e\u5148\u524d\u7814\u7a76\u7684\u8fdb\u6b65\u7a0b\u5ea6\uff09\u3002\u521b\u5efa\u4e86ScholarIdeas\u6570\u636e\u96c6\uff0c\u5305\u542b117\u4e2a\u8de8\u5b66\u79d1\u7814\u7a76\u60f3\u6cd5\u548c\u4e13\u5bb6\u6807\u6ce8\u3002", "result": "ScholarEval\u5728\u8986\u76d6\u4eba\u7c7b\u4e13\u5bb6\u6807\u6ce8\u8981\u70b9\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u8bc4\u4f30\u53ef\u64cd\u4f5c\u6027\u3001\u6df1\u5ea6\u548c\u8bc1\u636e\u652f\u6301\u65b9\u9762\u6301\u7eed\u4f18\u4e8eOpenAI\u7684o4-mini-deep-research\u7cfb\u7edf\u3002\u5927\u89c4\u6a21\u7528\u6237\u7814\u7a76\u663e\u793a\u5728\u6587\u732e\u53c2\u4e0e\u3001\u60f3\u6cd5\u7cbe\u70bc\u548c\u5b9e\u7528\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ScholarEval\u4e3aAI\u751f\u6210\u7814\u7a76\u60f3\u6cd5\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.17376", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17376", "abs": "https://arxiv.org/abs/2510.17376", "authors": ["Yongmin Li", "Jia Li", "Ge Li", "Zhi Jin"], "title": "AdapTrack: Constrained Decoding without Distorting LLM's Output Intent", "comment": "to be published in ICSE 2026", "summary": "Language model-based code generation and completion tools have been widely\nadopted, but they may sometimes produce code that does not meet necessary\nconstraints, such as syntactic correctness or API existence. Constrained\ndecoding techniques are developed to help the model generate code adhering to\nthe constraints by greedily eliminating generation options that violate\nconstraints at each step of the generation process. However, there is a severe\nlimitation of constrained decoding, that it distorts the model's output intent,\nforcing it to produce code that may satisfy the constraint but does not match\nthe development intent and is therefore incorrect. In response to this\nchallenge, we propose AdapTrack. By incorporating backtracking into the\ngeneration process, AdapTrack avoids distorting the output intent of the model,\nthereby producing results that are not only constraint-compliant but also more\nsemantically aligned with model's output intent. On our synthetic API\ncompletion dataset, AdapTrack can achieve up to 360.87% improvement compared to\nconstrained decoding; on the real-world API completion dataset we collect that\nexhibits similar issues, AdapTrack can achieve up to 38.93% improvement over\nconstrained decoding; in general code genration benchmarks, compared to\nconstrained decoding, AdapTrack can achieve up to 7.84% improvement on\nHumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by\nbetter adhering to the model's output intent, AdapTrack can achieve significant\nimprovements. We provide a theoretical proof that the distribution produced by\nAdapTrack aligns with the model's distribution given the generated tokens,\nthereby ensuring that the model's output intent is not distorted. Experiments\non DSL problems show that, compared to existing methods, our approach can\nprovide generation results that are more consistent with the language model's\ndistribution.", "AI": {"tldr": "AdapTrack\u662f\u4e00\u79cd\u7ed3\u5408\u56de\u6eaf\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u907f\u514d\u626d\u66f2\u6a21\u578b\u8f93\u51fa\u610f\u56fe\uff0c\u5728\u6ee1\u8db3\u7ea6\u675f\u7684\u540c\u65f6\u751f\u6210\u66f4\u7b26\u5408\u8bed\u4e49\u7684\u4ee3\u7801\u3002", "motivation": "\u73b0\u6709\u7ea6\u675f\u89e3\u7801\u6280\u672f\u4f1a\u626d\u66f2\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\u610f\u56fe\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u4ee3\u7801\u867d\u7136\u6ee1\u8db3\u7ea6\u675f\u4f46\u4e0d\u7b26\u5f00\u53d1\u610f\u56fe\u3002", "method": "\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165\u56de\u6eaf\u673a\u5236\uff0c\u907f\u514d\u5f3a\u5236\u6a21\u578b\u9009\u62e9\u8fdd\u53cd\u5176\u8f93\u51fa\u610f\u56fe\u7684\u9009\u9879\u3002", "result": "\u5728API\u8865\u5168\u6570\u636e\u96c6\u4e0a\u6bd4\u7ea6\u675f\u89e3\u7801\u63d0\u5347360.87%\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u63d0\u534738.93%\uff0c\u5728HumanEval\u548cMBPP\u57fa\u51c6\u4e0a\u5206\u522b\u63d0\u53477.84%\u548c6.42%\u3002", "conclusion": "\u901a\u8fc7\u66f4\u597d\u5730\u9075\u5faa\u6a21\u578b\u7684\u8f93\u51fa\u610f\u56fe\uff0cAdapTrack\u80fd\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "2510.16259", "categories": ["cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.16259", "abs": "https://arxiv.org/abs/2510.16259", "authors": ["Zhehao Zhang", "Weijie Xu", "Shixian Cui", "Chandan K. Reddy"], "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense", "comment": "29 pages, 9 tables, 4 figures", "summary": "Recent advances in large reasoning models (LRMs) have enabled remarkable\nperformance on complex tasks such as mathematics and coding by generating long\nChain-of-Thought (CoT) traces. In this paper, we identify and systematically\nanalyze a critical vulnerability we term reasoning distraction, where LRMs are\ndiverted from their primary objective by irrelevant yet complex tasks\nmaliciously embedded in the prompt. Through a comprehensive study across\ndiverse models and benchmarks, we show that even state-of-the-art LRMs are\nhighly susceptible, with injected distractors reducing task accuracy by up to\n60%. We further reveal that certain alignment techniques can amplify this\nweakness and that models may exhibit covert compliance, following hidden\nadversarial instructions in reasoning while concealing them in the final\noutput. To mitigate these risks, we propose a training-based defense that\ncombines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on\nsynthetic adversarial data, improving robustness by over 50 points on\nchallenging distractor attacks. Our findings establish reasoning distraction as\na distinct and urgent threat to LRM reliability and provide a practical step\ntoward safer and more trustworthy reasoning systems.", "AI": {"tldr": "\u8bba\u6587\u8bc6\u522b\u5e76\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\"\u63a8\u7406\u5206\u5fc3\"\u6f0f\u6d1e\uff0c\u5373\u6a21\u578b\u88ab\u6076\u610f\u5d4c\u5165\u7684\u590d\u6742\u65e0\u5173\u4efb\u52a1\u5206\u6563\u6ce8\u610f\u529b\uff0c\u5bfc\u81f4\u4e3b\u8981\u4efb\u52a1\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe60%\u3002\u4f5c\u8005\u63d0\u51fa\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u9632\u5fa1\u65b9\u6cd5\uff0c\u5c06\u9c81\u68d2\u6027\u63d0\u9ad8\u4e8650\u591a\u4e2a\u70b9\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u88ab\u6076\u610f\u5d4c\u5165\u7684\u65e0\u5173\u590d\u6742\u4efb\u52a1\u5206\u6563\u6ce8\u610f\u529b\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5f71\u54cd\u6a21\u578b\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u8de8\u6a21\u578b\u548c\u57fa\u51c6\u7684\u7efc\u5408\u7814\u7a76\u5206\u6790\u63a8\u7406\u5206\u5fc3\u6f0f\u6d1e\uff0c\u63d0\u51fa\u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u5728\u5408\u6210\u5bf9\u6297\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "\u6700\u5148\u8fdb\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u9ad8\u5ea6\u6613\u53d7\u653b\u51fb\uff0c\u6ce8\u5165\u7684\u5e72\u6270\u7269\u4f7f\u4efb\u52a1\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe60%\u3002\u63d0\u51fa\u7684\u9632\u5fa1\u65b9\u6cd5\u5728\u6311\u6218\u6027\u5e72\u6270\u653b\u51fb\u4e0a\u5c06\u9c81\u68d2\u6027\u63d0\u9ad8\u4e8650\u591a\u4e2a\u70b9\u3002", "conclusion": "\u63a8\u7406\u5206\u5fc3\u662f\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\u53ef\u9760\u6027\u7684\u72ec\u7279\u4e14\u7d27\u8feb\u7684\u5a01\u80c1\uff0c\u63d0\u51fa\u7684\u9632\u5fa1\u65b9\u6cd5\u4e3a\u5b9e\u73b0\u66f4\u5b89\u5168\u53ef\u4fe1\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6b65\u9aa4\u3002", "topic": "agent analysis"}}
{"id": "2510.17430", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17430", "abs": "https://arxiv.org/abs/2510.17430", "authors": ["Kuniaki Kudo", "Sherine Devi"], "title": "Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff", "comment": null, "summary": "We have developed a Scalable CI/CD Pipeline to address internal challenges\nrelated to Japan 2025 cliff problem, a critical issue where the mass end of\nservice life of legacy core IT systems threatens to significantly increase the\nmaintenance cost and black box nature of these system also leads to difficult\nupdate moreover replace, which leads to lack of progress in Digital\nTransformation (DX). If not addressed, Japan could potentially lose up to 12\ntrillion yen per year after 2025, which is 3 times more than the cost in\nprevious years. Asahi also faced the same internal challenges regarding legacy\nsystem, where manual maintenance workflows and limited QA environment have left\ncritical systems outdated and difficult to update. Middleware and OS version\nhave remained unchanged for years, leading to now its nearing end of service\nlife which require huge maintenance cost and effort to continue its operation.\nTo address this problem, we have developed and implemented a Scalable CI/CD\nPipeline where isolated development environments can be created and deleted\ndynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate\nGitHub for source code control and branching, Jenkins for pipeline automation,\nAmazon Web Services for scalable environment, and Docker for environment\ncontainerization. This paper presents the design and architecture of the\nScalable CI/CD Pipeline, with the implementation along with some use cases.\nThrough Scalable CI/CD, developers can freely and safely test maintenance\nprocedures and do experiments with new technology in their own environment,\nreducing maintenance cost and drive Digital Transformation (DX).\n  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u65e5\u672c2025\u5e74IT\u7cfb\u7edf\u5927\u89c4\u6a21\u9000\u5f79\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684CI/CD\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u52a8\u6001\u521b\u5efa\u9694\u79bb\u5f00\u53d1\u73af\u5883\u3001\u4f7f\u7528GitHub\u3001Jenkins\u3001AWS\u548cDocker\u7b49\u6280\u672f\uff0c\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\u5e76\u63a8\u52a8\u6570\u5b57\u5316\u8f6c\u578b\u3002", "motivation": "\u65e5\u672c\u9762\u4e342025\u5e74IT\u7cfb\u7edf\u5927\u89c4\u6a21\u9000\u5f79\u5371\u673a\uff0c\u4f20\u7edf\u7cfb\u7edf\u7ef4\u62a4\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u66f4\u65b0\uff0c\u53ef\u80fd\u5bfc\u81f4\u6bcf\u5e7412\u4e07\u4ebf\u65e5\u5143\u635f\u5931\u3002\u671d\u65e5\u516c\u53f8\u5185\u90e8\u4e5f\u9762\u4e34\u7c7b\u4f3c\u95ee\u9898\uff0c\u624b\u52a8\u7ef4\u62a4\u6d41\u7a0b\u548c\u6709\u9650QA\u73af\u5883\u5bfc\u81f4\u7cfb\u7edf\u8fc7\u65f6\u3002", "method": "\u5f00\u53d1\u53ef\u6269\u5c55CI/CD\u6d41\u6c34\u7ebf\uff0c\u96c6\u6210GitHub\u6e90\u4ee3\u7801\u63a7\u5236\u3001Jenkins\u6d41\u6c34\u7ebf\u81ea\u52a8\u5316\u3001AWS\u53ef\u6269\u5c55\u73af\u5883\u548cDocker\u5bb9\u5668\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u52a8\u6001\u521b\u5efa\u548c\u5220\u9664\u9694\u79bb\u5f00\u53d1\u73af\u5883\u3002", "result": "\u901a\u8fc7\u53ef\u6269\u5c55CI/CD\uff0c\u5f00\u53d1\u8005\u53ef\u5728\u72ec\u7acb\u73af\u5883\u4e2d\u5b89\u5168\u6d4b\u8bd5\u7ef4\u62a4\u7a0b\u5e8f\u548c\u6280\u672f\u5b9e\u9a8c\uff0c\u663e\u8457\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\u5e76\u4fc3\u8fdb\u6570\u5b57\u5316\u8f6c\u578b\u3002", "conclusion": "\u53ef\u6269\u5c55CI/CD\u6d41\u6c34\u7ebf\u6709\u6548\u89e3\u51b3\u4e86\u65e5\u672c2025\u5e74IT\u7cfb\u7edf\u9000\u5f79\u5371\u673a\uff0c\u4e3a\u4f20\u7edf\u7cfb\u7edf\u73b0\u4ee3\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2510.16276", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16276", "abs": "https://arxiv.org/abs/2510.16276", "authors": ["Song Bian", "Minghao Yan", "Anand Jayarajan", "Gennady Pekhimenko", "Shivaram Venkataraman"], "title": "What Limits Agentic Systems Efficiency?", "comment": "27 pages, 15 figures", "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.", "AI": {"tldr": "\u672c\u6587\u5bf9\u57fa\u4e8e\u7f51\u7edc\u4ea4\u4e92\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u8fdb\u884c\u4e86\u6548\u7387\u74f6\u9888\u5206\u6790\uff0c\u63d0\u51faSpecCache\u7f13\u5b58\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u6d4b\u6267\u884c\u663e\u8457\u51cf\u5c11\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\uff0c\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u5ffd\u89c6\u4e86\u7cfb\u7edf\u6548\u7387\u95ee\u9898\u3002\u7f51\u7edc\u4ea4\u4e92\u5e26\u6765\u7684\u5ef6\u8fdf\u6210\u4e3a\u5f71\u54cd\u667a\u80fd\u4f53\u7cfb\u7edf\u5b9e\u9645\u5e94\u7528\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u5206\u89e3\u4e3aLLM API\u5ef6\u8fdf\u548c\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\uff0c\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u679015\u4e2a\u6a21\u578b\u548c5\u4e2a\u63d0\u4f9b\u5546\uff0c\u63d0\u51faSpecCache\u7f13\u5b58\u6846\u67b6\u7ed3\u5408\u63a8\u6d4b\u6267\u884c\u6765\u4f18\u5316\u5ef6\u8fdf\u3002", "result": "\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\u53ef\u5360\u7cfb\u7edf\u603b\u5ef6\u8fdf\u768453.7%\uff0cSpecCache\u76f8\u6bd4\u968f\u673a\u7f13\u5b58\u7b56\u7565\u5c06\u7f13\u5b58\u547d\u4e2d\u7387\u63d0\u534758\u500d\uff0c\u7f51\u7edc\u73af\u5883\u5f00\u9500\u51cf\u5c113.2\u500d\uff0c\u4e14\u4e0d\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0cSpecCache\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u7f51\u7edc\u73af\u5883\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u91cd\u8981\u53c2\u8003\u3002", "topic": "agent analysis"}}
{"id": "2510.16022", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16022", "abs": "https://arxiv.org/abs/2510.16022", "authors": ["Changsheng Wang", "Xin Chen", "Sijia Liu", "Ke Ding"], "title": "Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization", "comment": null, "summary": "Adapting pretrained large language models (LLMs) to code domains via\nsupervised fine-tuning (FT) has been commonly used for code generation.\nHowever, we identify a previously underappreciated failure mode, the\nmemorization barrier, where strong memorization of downstream code data in the\nbase model could trap optimization and prevent the standard FT from effectively\nacquiring new, generalizable code knowledge. To overcome this barrier, we\npropose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which\napplies an IB penalty on hidden representations of the code data to compress\nspurious, memorized features while preserving task-relevant information.\nExtensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)\nshow that IB-FT substantially alleviates the memorization barrier, improves\ntop-1 performance (Pass@$1$), and yields far more stable gains under the\nstricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if\nat least $m$ of $k$ samples pass unit tests) compared with conventional FT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIB-FT\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u7406\u8bba\u6307\u5bfc\u4ee3\u7801LLMs\u7684\u5fae\u8c03\uff0c\u89e3\u51b3\u4f20\u7edf\u5fae\u8c03\u4e2d\u5b58\u5728\u7684\u8bb0\u5fc6\u969c\u788d\u95ee\u9898\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u53d1\u73b0\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u9886\u57df\u5fae\u8c03\u65f6\u5b58\u5728\u8bb0\u5fc6\u969c\u788d\u95ee\u9898\uff0c\u5373\u6a21\u578b\u8fc7\u5ea6\u8bb0\u5fc6\u4e0b\u6e38\u4ee3\u7801\u6570\u636e\u800c\u65e0\u6cd5\u6709\u6548\u5b66\u4e60\u65b0\u7684\u53ef\u6cdb\u5316\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7684\u5fae\u8c03\u65b9\u6cd5(IB-FT)\uff0c\u5bf9\u4ee3\u7801\u6570\u636e\u7684\u9690\u85cf\u8868\u793a\u65bd\u52a0IB\u60e9\u7f5a\uff0c\u538b\u7f29\u865a\u5047\u8bb0\u5fc6\u7279\u5f81\u540c\u65f6\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u3002", "result": "\u5728\u4e24\u4e2a\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5(OriGen\u548cEvol-CodeAlpaca-V1)\u4e0a\uff0cIB-FT\u663e\u8457\u7f13\u89e3\u4e86\u8bb0\u5fc6\u969c\u788d\uff0c\u63d0\u5347\u4e86top-1\u6027\u80fd\uff0c\u5e76\u5728\u66f4\u4e25\u683c\u7684\u591a\u6837\u672c\u6307\u6807\u4e0b\u83b7\u5f97\u66f4\u7a33\u5b9a\u7684\u589e\u76ca\u3002", "conclusion": "IB-FT\u65b9\u6cd5\u80fd\u6709\u6548\u514b\u670d\u8bb0\u5fc6\u969c\u788d\uff0c\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2510.15979", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15979", "abs": "https://arxiv.org/abs/2510.15979", "authors": ["Zexu Sun", "Yongcheng Zeng", "Erxue Min", "Heyang Gao", "Bokai Ji", "Xu Chen"], "title": "Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning", "comment": "22 Pages, 8 figures, 4 tables", "summary": "Contemporary progress in large language models (LLMs) has revealed notable\ninferential capacities via reinforcement learning (RL) employing verifiable\nreward, facilitating the development of O1 and R1-like reasoning models.\nDirectly training from base models with RL is called zero-RL. However, previous\nworks rely upon activating LLMs' inherent capacities through fixed prompt\ntemplates. This strategy introduces substantial sampling inefficiencies for\nweak LLMs, as the majority of problems generate invalid outputs during\naccuracy-driven filtration in reasoning tasks, which causes a waste of samples.\nTo solve this issue, we propose Cog-Rethinker, a novel hierarchical\nmetacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses\non the rollout procedure in RL training. After the direct rollout, our\nCog-Rethinker improves sample utilization in a hierarchical metacognitive\ntwo-stage framework. By leveraging human cognition during solving problems,\nfirstly, it prompts policy to decompose zero-accuracy problems into subproblems\nto produce final reasoning results. Secondly, with zero-accuracy problems in\nprevious rollout stage, it further prompts policy to refine these answers by\nreferencing previous wrong solutions. Moreover, to enable cold-start of the two\nnew reasoning patterns and maintain train-test consistency across prompt\ntemplates, our Cog-Rethinker applies supervised fine-tuning on the policy using\ncorrect samples of the two stages with direct rollout template. Experimental\nresults demonstrate Cog-Rethinker's superior performance on various\nmathematical reasoning benchmarks, we also analyzed its improved sample\nefficiency that accelerates convergence compared to baseline methods.", "AI": {"tldr": "\u63d0\u51faCog-Rethinker\uff0c\u4e00\u79cd\u5206\u5c42\u5143\u8ba4\u77e5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u96f6\u51c6\u786e\u7387\u95ee\u9898\u548c\u53c2\u8003\u9519\u8bef\u7b54\u6848\u6765\u6539\u8fdbLLM\u63a8\u7406\u4efb\u52a1\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fa\u5b9a\u63d0\u793a\u6a21\u677f\u7684RL\u65b9\u6cd5\u5728\u5f31LLM\u4e0a\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5927\u591a\u6570\u95ee\u9898\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u4ea7\u751f\u65e0\u6548\u8f93\u51fa\uff0c\u9020\u6210\u6837\u672c\u6d6a\u8d39\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5143\u8ba4\u77e5\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u5c06\u96f6\u51c6\u786e\u7387\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\uff1b2) \u53c2\u8003\u5148\u524d\u9519\u8bef\u7b54\u6848\u6765\u7cbe\u70bc\u7b54\u6848\u3002\u540c\u65f6\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u786e\u4fdd\u8bad\u7ec3\u6d4b\u8bd5\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u52a0\u901f\u4e86\u6536\u655b\u3002", "conclusion": "Cog-Rethinker\u901a\u8fc7\u5143\u8ba4\u77e5\u63a8\u7406\u6a21\u5f0f\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u4efb\u52a1\u7684\u6837\u672c\u5229\u7528\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.16449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16449", "abs": "https://arxiv.org/abs/2510.16449", "authors": ["Bin Yu", "Xinming Wang", "Shijie Lian", "Haotian Li", "Changti Wu", "Ruina Hu", "Bailing Wang", "Yuliang Wei", "Kai Chen"], "title": "TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model", "comment": "13 pages, 6 figures. Project website:\n  https://zgca-ai4edu.github.io/TrajSelector", "summary": "Large language models (LLMs) have shown remarkable progress in complex\nreasoning tasks, largely enabled by test-time scaling (TTS) paradigms that\nallocate additional compute during inference. Among these, external TTS\n(particularly the Best-of-N selection paradigm) yields scalable performance\nimprovements by selecting from multiple independently generated reasoning\ntrajectories. However, this approach faces key limitations: (i) the high\ncomputational overhead of deploying process reward models, (ii) the\nunderutilization of the LLM's intrinsic latent representations. We introduce\nTrajSelector, an efficient and effective Best-of-N framework that exploit the\nhidden states in the sampler LLM for process-level scoring. A lightweight\nverifier (with only 0.6B parameters) evaluates the quality of step-wise\ntrajectory, and then aggregates these scores to identify the optimal reasoning\ntrajectory. Our framework employs a fully data-driven, end-to-end training\nrecipe that eliminates reliance on massive step-level annotations. Experiential\nresults across five benchmarks demonstrate that TrajSelector delivers\nconsistent performance gains. In Best-of-32 settings, it surpasses majority\nvoting by 4.61% accuracy and outperforms existing process reward models by\n4.31% to 12.21%, all while maintaining lower inference costs.", "AI": {"tldr": "TrajSelector\u662f\u4e00\u4e2a\u9ad8\u6548\u7684Best-of-N\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u8fc7\u7a0b\u7ea7\u8bc4\u5206\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\u8bc4\u4f30\u63a8\u7406\u8f68\u8ff9\u8d28\u91cf\uff0c\u5728\u4fdd\u6301\u8f83\u4f4e\u63a8\u7406\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5916\u90e8\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u672a\u5145\u5206\u5229\u7528LLM\u5185\u5728\u8868\u793a\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8f68\u8ff9\u9009\u62e9\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\uff08\u4ec50.6B\u53c2\u6570\uff09\u8bc4\u4f30\u6b65\u9aa4\u7ea7\u8f68\u8ff9\u8d28\u91cf\uff0c\u5229\u7528LLM\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u8fc7\u7a0b\u7ea7\u8bc4\u5206\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u6570\u636e\u9a71\u52a8\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTrajSelector\u5728Best-of-32\u8bbe\u7f6e\u4e0b\u6bd4\u591a\u6570\u6295\u7968\u51c6\u786e\u7387\u9ad84.61%\uff0c\u6bd4\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u9ad84.31%-12.21%\uff0c\u4e14\u63a8\u7406\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "TrajSelector\u901a\u8fc7\u5229\u7528LLM\u5185\u5728\u8868\u793a\u548c\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u63a8\u7406\u8f68\u8ff9\u9009\u62e9\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u6269\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.16309", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16309", "abs": "https://arxiv.org/abs/2510.16309", "authors": ["Crystal Su"], "title": "MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier", "comment": "Accepted to the Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2026) Workshop", "summary": "Large language models (LLMs) often produce fluent reasoning steps while\nviolating simple mathematical or logical constraints. We introduce MedRule-KG,\na compact typed knowledge graph coupled with a symbolic verifier, designed to\nenforce mathematically interpretable rules in reasoning tasks. MedRule-KG\nencodes entities, relations, and three domain-inspired rules, while the\nverifier checks predictions and applies minimal corrections to guarantee\nconsistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG\nimproves exact match (EM) from 0.767 to 0.900, and adding the verifier yields\n1.000 EM while eliminating rule violations entirely. We demonstrate how\nMedRule-KG provides a general scaffold for safe mathematical reasoning, discuss\nablations, and release code and data to encourage reproducibility.", "AI": {"tldr": "MedRule-KG\u662f\u4e00\u4e2a\u7d27\u51d1\u7684\u77e5\u8bc6\u56fe\u8c31\u548c\u7b26\u53f7\u9a8c\u8bc1\u5668\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u5f3a\u5236\u6267\u884c\u6570\u5b66\u53ef\u89e3\u91ca\u89c4\u5219\uff0c\u663e\u8457\u63d0\u9ad8LLM\u7684\u63a8\u7406\u51c6\u786e\u6027\u5e76\u6d88\u9664\u89c4\u5219\u8fdd\u53cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u7ecf\u5e38\u4ea7\u751f\u6d41\u7545\u4f46\u8fdd\u53cd\u7b80\u5355\u6570\u5b66\u6216\u903b\u8f91\u7ea6\u675f\u7684\u6b65\u9aa4\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u786e\u4fdd\u63a8\u7406\u7684\u6570\u5b66\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165MedRule-KG\uff0c\u4e00\u4e2a\u7d27\u51d1\u7684\u7c7b\u578b\u5316\u77e5\u8bc6\u56fe\u8c31\uff0c\u7ed3\u5408\u7b26\u53f7\u9a8c\u8bc1\u5668\u6765\u7f16\u7801\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u9886\u57df\u542f\u53d1\u89c4\u5219\uff0c\u9a8c\u8bc1\u9884\u6d4b\u5e76\u5e94\u7528\u6700\u5c0f\u4fee\u6b63\u4ee5\u4fdd\u8bc1\u4e00\u81f4\u6027\u3002", "result": "\u572890\u4e2aFDA\u884d\u751f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eMedRule-KG\u7684\u63a8\u7406\u5c06\u7cbe\u786e\u5339\u914d\u4ece0.767\u63d0\u9ad8\u52300.900\uff0c\u6dfb\u52a0\u9a8c\u8bc1\u5668\u540e\u8fbe\u52301.000\u7cbe\u786e\u5339\u914d\uff0c\u5b8c\u5168\u6d88\u9664\u89c4\u5219\u8fdd\u53cd\u3002", "conclusion": "MedRule-KG\u4e3a\u5b89\u5168\u7684\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u7ea0\u6b63LLM\u7684\u63a8\u7406\u9519\u8bef\u5e76\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.17591", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17591", "abs": "https://arxiv.org/abs/2510.17591", "authors": ["Guang Yang", "Yujie Zhu"], "title": "HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection", "comment": "Accepted by the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025) as a findings long paper", "summary": "Pre-trained language models (PLMs) are increasingly being applied to\ncode-related tasks. Although PLMs have achieved good results, they do not take\ninto account potential high-order data correlations within the code. We propose\nthree types of high-order correlations in code tokens, i.e. abstract syntax\ntree family correlation, lexical correlation, and line correlation. We design a\ntokens and hyperedges generator to capture these high-order data correlations.\nWe improve the architecture of hypergraph neural networks and combine it with\nadapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to\nfine-tune PLMs. HGAdapter can encode high-order data correlations and is\nallowed to be inserted into various PLMs to enhance performance. Experiments\nwere conducted on several public datasets, including six languages of code\nsummarization and code clone detection tasks. Our methods improved the\nperformance of PLMs in datasets to varying degrees. Experimental results\nvalidate the introduction of high-order data correlations that contribute to\nimproved effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u9002\u914d\u5668HGAdapter\uff0c\u901a\u8fc7\u6355\u6349\u4ee3\u7801\u4e2d\u7684\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\u6765\u589e\u5f3a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u672a\u80fd\u5145\u5206\u8003\u8651\u4ee3\u7801\u4e2d\u6f5c\u5728\u7684\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u4ee3\u7801\u4ee4\u724c\u7684\u9ad8\u9636\u76f8\u5173\u6027\u7c7b\u578b\uff08\u62bd\u8c61\u8bed\u6cd5\u6811\u5bb6\u65cf\u76f8\u5173\u6027\u3001\u8bcd\u6c47\u76f8\u5173\u6027\u548c\u884c\u76f8\u5173\u6027\uff09\uff0c\u8bbe\u8ba1\u4e86\u4ee4\u724c\u548c\u8d85\u8fb9\u751f\u6210\u5668\u6765\u6355\u6349\u8fd9\u4e9b\u76f8\u5173\u6027\uff0c\u5e76\u6539\u8fdb\u4e86\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u9002\u914d\u5668\u8c03\u4f18\u63d0\u51fa\u4e86HGAdapter\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u8bed\u8a00\u7684\u4ee3\u7801\u6458\u8981\u548c\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4efb\u52a1\u4e2d\u90fd\u80fd\u4e0d\u540c\u7a0b\u5ea6\u5730\u63d0\u5347\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u5f15\u5165\u9ad8\u9636\u6570\u636e\u76f8\u5173\u6027\u6709\u52a9\u4e8e\u63d0\u9ad8\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u7684\u6709\u6548\u6027\uff0cHGAdapter\u53ef\u4ee5\u63d2\u5165\u5230\u5404\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u589e\u5f3a\u5176\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2510.17795", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17795", "abs": "https://arxiv.org/abs/2510.17795", "authors": ["Yujie Luo", "Zhuoyun Yu", "Xuehai Wang", "Yuqi Zhu", "Ningyu Zhang", "Lanning Wei", "Lun Du", "Da Zheng", "Huajun Chen"], "title": "Executable Knowledge Graphs for Replicating AI Research", "comment": "Work in progress", "summary": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53ef\u6267\u884c\u77e5\u8bc6\u56fe\u8c31(xKG)\u6765\u89e3\u51b3AI\u7814\u7a76\u590d\u73b0\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6574\u5408\u6280\u672f\u6d1e\u5bdf\u3001\u4ee3\u7801\u7247\u6bb5\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u7814\u7a76\u590d\u73b0\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u7531\u4e8e\u80cc\u666f\u77e5\u8bc6\u4e0d\u8db3\u548cRAG\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u53c2\u8003\u6587\u732e\u4e2d\u7684\u6f5c\u5728\u6280\u672f\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u53ef\u6267\u884c\u77e5\u8bc6\u56fe\u8c31(xKG)\uff0c\u8fd9\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u63d2\u62d4\u7684\u77e5\u8bc6\u5e93\uff0c\u81ea\u52a8\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u6280\u672f\u6d1e\u5bdf\u3001\u4ee3\u7801\u7247\u6bb5\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u7406\u6846\u67b6\u548c\u4e24\u79cd\u4e0d\u540cLLM\u4e0a\u96c6\u6210xKG\u540e\uff0c\u5728PaperBench\u4e0a\u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff08o3-mini\u63d0\u534710.9%\uff09\u3002", "conclusion": "xKG\u662f\u81ea\u52a8\u5316AI\u7814\u7a76\u590d\u73b0\u7684\u901a\u7528\u4e14\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2510.16492", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16492", "abs": "https://arxiv.org/abs/2510.16492", "authors": ["Vamshi Krishna Bonagiri", "Ponnurangam Kumaragurum", "Khanh Nguyen", "Benjamin Plaut"], "title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety", "comment": "Reliable ML and Regulatable ML workshops, Neurips 2025", "summary": "As Large Language Model (LLM) agents increasingly operate in complex\nenvironments with real-world consequences, their safety becomes critical. While\nuncertainty quantification is well-studied for single-turn tasks, multi-turn\nagentic scenarios with real-world tool access present unique challenges where\nuncertainties and ambiguities compound, leading to severe or catastrophic risks\nbeyond traditional text generation failures. We propose using \"quitting\" as a\nsimple yet effective behavioral mechanism for LLM agents to recognize and\nwithdraw from situations where they lack confidence. Leveraging the ToolEmu\nframework, we conduct a systematic evaluation of quitting behavior across 12\nstate-of-the-art LLMs. Our results demonstrate a highly favorable\nsafety-helpfulness trade-off: agents prompted to quit with explicit\ninstructions improve safety by an average of +0.39 on a 0-3 scale across all\nmodels (+0.64 for proprietary models), while maintaining a negligible average\ndecrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding\nexplicit quit instructions proves to be a highly effective safety mechanism\nthat can immediately be deployed in existing agent systems, and establishes\nquitting as an effective first-line defense mechanism for autonomous agents in\nhigh-stakes applications.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\"\u9000\u51fa\"\u4f5c\u4e3aLLM\u4ee3\u7406\u7684\u5b89\u5168\u673a\u5236\uff0c\u8ba9\u4ee3\u7406\u5728\u7f3a\u4e4f\u4fe1\u5fc3\u65f6\u4e3b\u52a8\u9000\u51fa\uff0c\u572812\u4e2a\u5148\u8fdbLLM\u4e0a\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u800c\u51e0\u4e4e\u4e0d\u5f71\u54cd\u5e2e\u52a9\u6027", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fd0\u884c\u5e76\u4ea7\u751f\u73b0\u5b9e\u540e\u679c\uff0c\u5176\u5b89\u5168\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u591a\u8f6e\u4ee3\u7406\u573a\u666f\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6a21\u7cca\u6027\u4f1a\u7d2f\u79ef\uff0c\u5bfc\u81f4\u8d85\u8d8a\u4f20\u7edf\u6587\u672c\u751f\u6210\u5931\u8d25\u7684\u4e25\u91cd\u98ce\u9669", "method": "\u5229\u7528ToolEmu\u6846\u67b6\u7cfb\u7edf\u8bc4\u4f3012\u4e2a\u5148\u8fdbLLM\u7684\u9000\u51fa\u884c\u4e3a\uff0c\u901a\u8fc7\u6dfb\u52a0\u660e\u786e\u7684\u9000\u51fa\u6307\u4ee4\u6765\u4fc3\u4f7f\u4ee3\u7406\u5728\u7f3a\u4e4f\u4fe1\u5fc3\u65f6\u4e3b\u52a8\u9000\u51fa", "result": "\u5728\u6240\u6709\u6a21\u578b\u4e0a\u5b89\u5168\u6027\u5e73\u5747\u63d0\u5347+0.39\uff080-3\u5206\u5236\uff09\uff0c\u4e13\u6709\u6a21\u578b\u63d0\u5347+0.64\uff0c\u800c\u5e2e\u52a9\u6027\u4ec5\u5e73\u5747\u4e0b\u964d-0.03\uff0c\u663e\u793a\u51fa\u6781\u4f73\u7684\u5b89\u5168-\u5e2e\u52a9\u6027\u6743\u8861", "conclusion": "\u7b80\u5355\u6dfb\u52a0\u660e\u786e\u7684\u9000\u51fa\u6307\u4ee4\u662f\u9ad8\u5ea6\u6709\u6548\u7684\u5b89\u5168\u673a\u5236\uff0c\u53ef\u7acb\u5373\u90e8\u7f72\u5230\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u4e2d\uff0c\u4f5c\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u81ea\u4e3b\u4ee3\u7406\u7684\u6709\u6548\u7b2c\u4e00\u9053\u9632\u7ebf", "topic": "agent analysis"}}
{"id": "2510.16499", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16499", "abs": "https://arxiv.org/abs/2510.16499", "authors": ["Michelle Yuan", "Khushbu Pahwa", "Shuaichen Chang", "Mustafa Kaba", "Jiarong Jiang", "Xiaofei Ma", "Yi Zhang", "Monica Sunkara"], "title": "Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection", "comment": "Accepted to NeurIPS 2025 Conference", "summary": "Designing effective agentic systems requires the seamless composition and\nintegration of agents, tools, and models within dynamic and uncertain\nenvironments. Most existing methods rely on static, semantic retrieval\napproaches for tool or agent discovery. However, effective reuse and\ncomposition of existing components remain challenging due to incomplete\ncapability descriptions and the limitations of retrieval methods. Component\nselection suffers because the decisions are not based on capability, cost, and\nreal-time utility. To address these challenges, we introduce a structured,\nautomated framework for agentic system composition that is inspired by the\nknapsack problem. Our framework enables a composer agent to systematically\nidentify, select, and assemble an optimal set of agentic components by jointly\nconsidering performance, budget constraints, and compatibility. By dynamically\ntesting candidate components and modeling their utility in real-time, our\napproach streamlines the assembly of agentic systems and facilitates scalable\nreuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five\nbenchmarking datasets shows that our online-knapsack-based composer\nconsistently lies on the Pareto frontier, achieving higher success rates at\nsignificantly lower component costs compared to our baselines. In the\nsingle-agent setup, the online knapsack composer shows a success rate\nimprovement of up to 31.6% in comparison to the retrieval baselines. In\nmulti-agent systems, the online knapsack composer increases success rate from\n37% to 87% when agents are selected from an agent inventory of 100+ agents. The\nsubstantial performance gap confirms the robust adaptability of our method\nacross diverse domains and budget constraints.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5728\u7ebf\u80cc\u5305\u95ee\u9898\u7684\u81ea\u52a8\u5316\u667a\u80fd\u4f53\u7cfb\u7edf\u7ec4\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6d4b\u8bd5\u548c\u5b9e\u65f6\u6548\u7528\u5efa\u6a21\uff0c\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u4f18\u5316\u9009\u62e9\u667a\u80fd\u4f53\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u9ad8\u6210\u529f\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u7cfb\u7edf\u7ec4\u5408\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u8bed\u4e49\u68c0\u7d22\uff0c\u5b58\u5728\u80fd\u529b\u63cf\u8ff0\u4e0d\u5b8c\u6574\u3001\u68c0\u7d22\u65b9\u6cd5\u5c40\u9650\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u57fa\u4e8e\u80fd\u529b\u3001\u6210\u672c\u548c\u5b9e\u65f6\u6548\u7528\u8fdb\u884c\u7ec4\u4ef6\u9009\u62e9\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u7cfb\u7edf\u7ec4\u5408\u5efa\u6a21\u4e3a\u5728\u7ebf\u80cc\u5305\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u6d4b\u8bd5\u5019\u9009\u7ec4\u4ef6\u5e76\u5b9e\u65f6\u5efa\u6a21\u5176\u6548\u7528\uff0c\u5728\u6027\u80fd\u3001\u9884\u7b97\u7ea6\u675f\u548c\u517c\u5bb9\u6027\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u4f18\u5316\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u57fa\u4e8e\u5728\u7ebf\u80cc\u5305\u7684\u7ec4\u5408\u5668\u59cb\u7ec8\u4f4d\u4e8e\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5355\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e0b\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe31.6%\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6210\u529f\u7387\u4ece37%\u63d0\u5347\u81f387%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u9886\u57df\u548c\u9884\u7b97\u7ea6\u675f\u4e0b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u68c0\u7d22\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.16374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16374", "abs": "https://arxiv.org/abs/2510.16374", "authors": ["Nick Oh"], "title": "Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs", "comment": "Presented at the Workshop on the Application of LLM Explainability to\n  Reasoning and Planning at COLM 2025 (non-archival)", "summary": "Current approaches to enhancing LLM reasoning follows two isolated paradigms:\nMonitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and\nSELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack\nmechanisms to verify whether selected strategies succeed; while Generate-Verify\napproaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan\net al., 2023) iteratively refine outputs but commence generation blindly\nwithout task assessment. This separation creates inefficiencies -- strategies\nfail without feedback, and refinement occurs without strategic grounding. We\naddress this gap by implementing Flavell's cognitive monitoring model (1979)\nfrom the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025),\noperationalising it as a three-phase iterative system. On GSM8K, preliminary\nresults show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for\nSelf-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37%\nincreased inference cost. These initial findings suggest upfront monitoring\nproduces higher-quality initial solutions that reduce refinement needs, though\nevaluation beyond arithmetic reasoning is needed to establish generalisability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76d1\u63a7-\u751f\u6210-\u9a8c\u8bc1\u7684\u4e09\u9636\u6bb5\u8fed\u4ee3\u7cfb\u7edf\uff0c\u5c06\u7b56\u7565\u89c4\u5212\u548c\u9a8c\u8bc1\u673a\u5236\u6574\u5408\uff0c\u5728GSM8K\u4e0a\u53d6\u5f97\u4e8675.42%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u9700\u8981\u66f4\u5c11\u7684\u8fed\u4ee3\u6b21\u6570\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u5206\u79bb\uff1a\u76d1\u63a7-\u751f\u6210\u65b9\u6cd5\u64c5\u957f\u7b56\u7565\u89c4\u5212\u4f46\u7f3a\u4e4f\u9a8c\u8bc1\u673a\u5236\uff0c\u751f\u6210-\u9a8c\u8bc1\u65b9\u6cd5\u80fd\u8fed\u4ee3\u4f18\u5316\u4f46\u7f3a\u4e4f\u524d\u671f\u7b56\u7565\u8bc4\u4f30\u3002\u8fd9\u79cd\u5206\u79bb\u5bfc\u81f4\u7b56\u7565\u5931\u8d25\u65e0\u53cd\u9988\u3001\u4f18\u5316\u7f3a\u4e4f\u6218\u7565\u57fa\u7840\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8eFlavell\u7684\u8ba4\u77e5\u76d1\u63a7\u6a21\u578b\uff0c\u5b9e\u73b0\u76d1\u63a7-\u751f\u6210-\u9a8c\u8bc1\u4e09\u9636\u6bb5\u8fed\u4ee3\u7cfb\u7edf\uff0c\u5c06\u7b56\u7565\u89c4\u5212\u548c\u9a8c\u8bc1\u673a\u5236\u6574\u5408\u5728\u4e00\u8d77\u3002", "result": "\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u8fbe\u523075.42%\u51c6\u786e\u7387\uff0c\u4f18\u4e8eSELF-REFINE\u768468.44%\u548cSelf-Verification\u768467.07%\uff0c\u4e14\u5e73\u5747\u5c1d\u8bd5\u6b21\u6570\u66f4\u5c11\uff081.3 vs 2.0\uff09\uff0c\u63a8\u7406\u6210\u672c\u589e\u52a027-37%\u3002", "conclusion": "\u524d\u671f\u76d1\u63a7\u80fd\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u540e\u7eed\u4f18\u5316\u9700\u6c42\uff0c\u4f46\u9700\u8981\u5728\u7b97\u672f\u63a8\u7406\u4e4b\u5916\u7684\u9886\u57df\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u901a\u7528\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.16392", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16392", "abs": "https://arxiv.org/abs/2510.16392", "authors": ["Ao Tian", "Yunfeng Lu", "Xinxin Fan", "Changhao Wang", "Lanzhi Zhou", "Yeyao Zhang", "Yanfang Liu"], "title": "RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile", "comment": "11 pages,3 figures", "summary": "Personalized and continuous interactions are the key to enhancing user\nexperience in today's large language model (LLM)-based conversational systems,\nhowever, the finite context windows and static parametric memory make it\ndifficult to model the cross-session long-term user states and behavioral\nconsistency. Currently, the existing solutions to this predicament, such as\nretrieval-augmented generation (RAG) and explicit memory systems, primarily\nfocus on fact-level storage and retrieval, lacking the capability to distill\nlatent preferences and deep traits from the multi-turn dialogues, which limits\nthe long-term and effective user modeling, directly leading to the personalized\ninteractions remaining shallow, and hindering the cross-session continuity. To\nrealize the long-term memory and behavioral consistency for Language Agents in\nLLM era, we propose a self-evolving memory framework RGMem, inspired by the\nideology of classic renormalization group (RG) in physics, this framework\nenables to organize the dialogue history in multiple scales: it first extracts\nsemantics and user insights from episodic fragments, then through hierarchical\ncoarse-graining and rescaling operations, progressively forms a\ndynamically-evolved user profile. The core innovation of our work lies in\nmodeling memory evolution as a multi-scale process of information compression\nand emergence, which accomplishes the high-level and accurate user profiles\nfrom noisy and microscopic-level interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86RGMem\u6846\u67b6\uff0c\u4e00\u79cd\u57fa\u4e8e\u91cd\u6574\u5316\u7fa4\u601d\u60f3\u7684\u81ea\u8fdb\u5316\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u957f\u671f\u7528\u6237\u5efa\u6a21\u548c\u884c\u4e3a\u4e00\u81f4\u6027\u95ee\u9898", "motivation": "\u73b0\u6709RAG\u548c\u663e\u5f0f\u8bb0\u5fc6\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u4e8b\u5b9e\u7ea7\u5b58\u50a8\u548c\u68c0\u7d22\uff0c\u7f3a\u4e4f\u4ece\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u6f5c\u5728\u504f\u597d\u548c\u6df1\u5c42\u7279\u5f81\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u957f\u671f\u6709\u6548\u7684\u7528\u6237\u5efa\u6a21", "method": "\u91c7\u7528\u5206\u5c42\u7c97\u7c92\u5316\u548c\u91cd\u6807\u5ea6\u64cd\u4f5c\uff0c\u4ece\u7247\u6bb5\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u8bed\u4e49\u548c\u7528\u6237\u6d1e\u5bdf\uff0c\u9010\u6b65\u5f62\u6210\u52a8\u6001\u6f14\u5316\u7684\u7528\u6237\u753b\u50cf", "result": "\u5b9e\u73b0\u4e86\u591a\u5c3a\u5ea6\u4fe1\u606f\u538b\u7f29\u548c\u6d8c\u73b0\u8fc7\u7a0b\uff0c\u4ece\u566a\u58f0\u5fae\u89c2\u4ea4\u4e92\u4e2d\u5f62\u6210\u9ad8\u5c42\u6b21\u51c6\u786e\u7684\u7528\u6237\u753b\u50cf", "conclusion": "RGMem\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u8bed\u8a00\u4ee3\u7406\u7684\u957f\u671f\u8bb0\u5fc6\u548c\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u4e2a\u6027\u5316\u4ea4\u4e92\u7684\u6df1\u5ea6\u548c\u8de8\u4f1a\u8bdd\u8fde\u7eed\u6027", "topic": "agent analysis"}}
{"id": "2510.15996", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15996", "abs": "https://arxiv.org/abs/2510.15996", "authors": ["Ozan K. Tonguz", "Federico Taschin"], "title": "Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning", "comment": null, "summary": "One of the major problems in Machine Learning (ML) and Artificial\nIntelligence (AI) is the fact that the probability distribution of the test\ndata in the real world could deviate substantially from the probability\ndistribution of the training data set. When this happens, the predictions of an\nML system or an AI agent could involve large errors which is very troublesome\nand undesirable. While this is a well-known hard problem plaguing the AI and ML\nsystems' accuracy and reliability, in certain applications such errors could be\ncritical for safety and reliability of AI and ML systems. One approach to deal\nwith this problem is to monitor and measure the deviation in the probability\ndistribution of the test data in real time and to compensate for this\ndeviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov\n(KS) Test for measuring the distribution shift and we show how the KS distance\ncan be used to quantify the distribution shift and its impact on an AI agent's\nperformance. Our results suggest that KS distance could be used as a valuable\nstatistical tool for monitoring and measuring the distribution shift. More\nspecifically, it is shown that even a distance of KS=0.02 could lead to about\n50\\% increase in the travel time at a single intersection using a Reinforcement\nLearning agent which is quite significant. It is hoped that the use of KS Test\nand KS distance in AI-based smart transportation could be an important step\nforward for gauging the performance degradation of an AI agent in real time and\nthis, in turn, could help the AI agent to cope with the distribution shift in a\nmore informed manner.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528Kolmogorov-Smirnov\u68c0\u9a8c\u6765\u76d1\u6d4b\u548c\u91cf\u5316AI\u7cfb\u7edf\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4ea4\u901a\u5e94\u7528\u4e2d\uff0c\u5373\u4f7fKS\u8ddd\u79bb\u4e3a0.02\u4e5f\u4f1a\u5bfc\u81f4\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u65c5\u884c\u65f6\u95f4\u589e\u52a0\u7ea650%\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u504f\u79bb\u7684\u95ee\u9898\uff0c\u8fd9\u79cd\u5206\u5e03\u504f\u79fb\u4f1a\u5bfc\u81f4AI\u667a\u80fd\u4f53\u9884\u6d4b\u51fa\u73b0\u5927\u8bef\u5dee\uff0c\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5c24\u5176\u5371\u9669\u3002", "method": "\u4f7f\u7528Kolmogorov-Smirnov\u68c0\u9a8c\u548cKS\u8ddd\u79bb\u6765\u91cf\u5316\u5206\u5e03\u504f\u79fb\u53ca\u5176\u5bf9AI\u667a\u80fd\u4f53\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7fKS\u8ddd\u79bb\u4ec5\u4e3a0.02\uff0c\u4e5f\u4f1a\u5bfc\u81f4\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u5355\u4e2a\u4ea4\u53c9\u8def\u53e3\u7684\u65c5\u884c\u65f6\u95f4\u589e\u52a0\u7ea650%\u3002", "conclusion": "KS\u68c0\u9a8c\u548cKS\u8ddd\u79bb\u53ef\u4f5c\u4e3a\u5b9e\u65f6\u76d1\u6d4bAI\u667a\u80fd\u4f53\u6027\u80fd\u9000\u5316\u7684\u6709\u4ef7\u503c\u7edf\u8ba1\u5de5\u5177\uff0c\u5e2e\u52a9AI\u667a\u80fd\u4f53\u66f4\u6709\u6548\u5730\u5e94\u5bf9\u5206\u5e03\u504f\u79fb\u3002", "topic": "agent analysis"}}
{"id": "2510.16476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16476", "abs": "https://arxiv.org/abs/2510.16476", "authors": ["Xiaozhe Li", "Xinyu Fang", "Shengyuan Ding", "Linyang Li", "Haodong Duan", "Qingwen Liu", "Kai Chen"], "title": "NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems", "comment": null, "summary": "Large Language Models (LLMs) have shown strong reasoning capabilities, with\nmodels like OpenAI's O-series and DeepSeek R1 excelling at tasks such as\nmathematics, coding, logic, and puzzles through Reinforcement Learning with\nVerifiable Rewards (RLVR). However, their ability to solve more complex\noptimization problems - particularly NP-hard tasks - remains underexplored. To\nbridge this gap, we propose NP-ENGINE, the first comprehensive framework for\ntraining and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks\nacross five domains, each equipped with (i) a controllable instance generator,\n(ii) a rule-based verifier, and (iii) a heuristic solver that provides\napproximate optimal solutions as ground truth. This\ngenerator-verifier-heuristic pipeline enables scalable and verifiable RLVR\ntraining under hierarchical difficulties. We also introduce NP-BENCH, a\nbenchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs'\nability to tackle NP-hard level reasoning problems, focusing not only on\nfeasibility but also on solution quality. Additionally, we present\nQWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on\nQwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and\nachieves SOTA performance with the same model size. Beyond in-domain tasks, we\ndemonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain\n(OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge),\nas well as non-reasoning tasks such as instruction following. We also observe a\nscaling trend: increasing task diversity improves OOD generalization. These\nfindings suggest that task-rich RLVR training is a promising direction for\nadvancing LLM's reasoning ability, revealing new insights into the scaling laws\nof RLVR.", "AI": {"tldr": "\u63d0\u51fa\u4e86NP-ENGINE\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30LLM\u5728NP\u96be\u95ee\u9898\u4e0a\u7684\u80fd\u529b\uff0c\u5305\u62ec\u4efb\u52a1\u751f\u6210\u5668\u3001\u9a8c\u8bc1\u5668\u548c\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\uff0c\u5e76\u5c55\u793a\u4e86\u5728NP-BENCH\u57fa\u51c6\u4e0a\u7684SOTA\u6027\u80fd\u4ee5\u53ca\u5f3a\u5927\u7684\u9886\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u867d\u7136LLM\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u7b49\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89e3\u51b3NP\u96be\u7b49\u590d\u6742\u4f18\u5316\u95ee\u9898\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faNP-ENGINE\u6846\u67b6\uff0c\u5305\u542b\u53ef\u63a7\u5b9e\u4f8b\u751f\u6210\u5668\u3001\u89c4\u5219\u9a8c\u8bc1\u5668\u548c\u542f\u53d1\u5f0f\u6c42\u89e3\u5668\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684RLVR\u8bad\u7ec3\uff1b\u91c7\u7528\u96f6RLVR\u548c\u8bfe\u7a0b\u5b66\u4e60\u8bad\u7ec3QWEN2.5-7B-NP\u6a21\u578b\u3002", "result": "QWEN2.5-7B-NP\u5728NP-BENCH\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8aGPT-4o\uff0c\u8fbe\u5230\u540c\u89c4\u6a21\u6a21\u578b\u7684SOTA\u6027\u80fd\uff1b\u5728\u9886\u57df\u5916\u63a8\u7406\u4efb\u52a1\u548c\u975e\u63a8\u7406\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff1b\u4efb\u52a1\u591a\u6837\u6027\u589e\u52a0\u53ef\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u4efb\u52a1\u4e30\u5bcc\u7684RLVR\u8bad\u7ec3\u662f\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u63ed\u793a\u4e86RLVR\u7684\u6269\u5c55\u89c4\u5f8b\uff0cNP\u96be\u95ee\u9898\u8bad\u7ec3\u53ef\u589e\u5f3a\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.16645", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.16645", "abs": "https://arxiv.org/abs/2510.16645", "authors": ["Zhixuan He", "Yue Feng"], "title": "Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration", "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong performance but often lack\ninterpretable reasoning. This paper introduces the Multi-Agent Collaboration\nFramework for Diverse Thinking Modes (DiMo), which enhances both performance\nand interpretability by simulating a structured debate among four specialized\nLLM agents. Each agent embodies a distinct reasoning paradigm, allowing the\nframework to collaboratively explore diverse cognitive approaches. Through\niterative debate, agents challenge and refine initial responses, yielding more\nrobust conclusions and an explicit, auditable reasoning chain. Across six\nbenchmarks and under a unified open-source setup, DiMo improves accuracy over\nwidely used single-model and debate baselines, with the largest gains on math.\nWe position DiMo as a semantics-aware, Web-native multi-agent framework: it\nmodels human-machine intelligence with LLM agents that produce semantically\ntyped, URL-annotated evidence chains for explanations and user-friendly\ninteractions. Although our experiments use standard reasoning benchmarks, the\nframework is designed to be instantiated over Web corpora and knowledge graphs,\ncombining retrieval-augmented reasoning with structured justifications that\ndownstream systems can inspect and reuse.", "AI": {"tldr": "\u63d0\u51fa\u4e86DiMo\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u4e13\u95e8\u5316LLM\u667a\u80fd\u4f53\u7684\u7ed3\u6784\u5316\u8fa9\u8bba\u6765\u589e\u5f3a\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4ee3\u8868\u4e0d\u540c\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3LLMs\u867d\u7136\u6027\u80fd\u5f3a\u5927\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u63a8\u7406\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u62df\u7ed3\u6784\u5316\u8fa9\u8bba\u6765\u63d0\u5347\u63a8\u7406\u900f\u660e\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u4e13\u95e8\u5316LLM\u667a\u80fd\u4f53\uff0c\u6bcf\u4e2a\u4ee3\u8868\u4e0d\u540c\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u8fed\u4ee3\u8fa9\u8bba\u6765\u6311\u6218\u548c\u7cbe\u70bc\u521d\u59cb\u54cd\u5e94\uff0c\u751f\u6210\u53ef\u5ba1\u8ba1\u7684\u63a8\u7406\u94fe\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiMo\u76f8\u6bd4\u5e7f\u6cdb\u4f7f\u7528\u7684\u5355\u6a21\u578b\u548c\u8fa9\u8bba\u57fa\u7ebf\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u63d0\u5347\u6700\u5927\u3002", "conclusion": "DiMo\u662f\u4e00\u4e2a\u8bed\u4e49\u611f\u77e5\u7684Web\u539f\u751f\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u8bed\u4e49\u7c7b\u578b\u5316\u3001URL\u6ce8\u91ca\u7684\u8bc1\u636e\u94fe\uff0c\u652f\u6301\u4e0b\u6e38\u7cfb\u7edf\u68c0\u67e5\u548c\u91cd\u7528\u3002", "topic": "agent analysis"}}
{"id": "2510.16614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16614", "abs": "https://arxiv.org/abs/2510.16614", "authors": ["Xuan Zhang", "Ruixiao Li", "Zhijian Zhou", "Long Li", "Yulei Qin", "Ke Li", "Xing Sun", "Xiaoyu Tan", "Chao Qu", "Yuan Qi"], "title": "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards", "comment": null, "summary": "Reinforcement Learning (RL) has become a compelling way to strengthen the\nmulti step reasoning ability of Large Language Models (LLMs). However,\nprevalent RL paradigms still lean on sparse outcome-based rewards and limited\nexploration, which often drives LLMs toward repetitive and suboptimal reasoning\npatterns. In this paper, we study the central question of how to design\nexploration for LLM reasoning and introduce MERCI (Motivating Exploration in\nLLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that\naugments policy optimization with a principled intrinsic reward. Building on\nthe idea of count-based exploration, MERCI leverages a lightweight Coin\nFlipping Network (CFN) to estimate the pseudo count and further epistemic\nuncertainty over reasoning trajectories, and converts them into an intrinsic\nreward that values novelty while preserving the learning signal from task\nrewards. We integrate MERCI into some advanced RL frameworks like Group\nRelative Policy Optimization (GRPO). Experiments on complex reasoning\nbenchmarks demonstrate that MERCI encourages richer and more varied chains of\nthought, significantly improves performance over strong baselines, and helps\nthe policy escape local routines to discover better solutions. It indicates\nthat our targeted intrinsic motivation can make exploration reliable for\nlanguage model reasoning.", "AI": {"tldr": "MERCI\u662f\u4e00\u79cd\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u8ba1\u6570\u7684\u5185\u5728\u5956\u52b1\u6fc0\u52b1\u63a2\u7d22\uff0c\u907f\u514d\u91cd\u590d\u548c\u6b21\u4f18\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\u548c\u6709\u9650\u63a2\u7d22\uff0c\u5bfc\u81f4LLM\u9677\u5165\u91cd\u590d\u548c\u6b21\u4f18\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u597d\u7684\u63a2\u7d22\u673a\u5236\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684Coin Flipping Network\u4f30\u8ba1\u63a8\u7406\u8f68\u8ff9\u7684\u4f2a\u8ba1\u6570\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u5e76\u4e0eGRPO\u7b49\u5148\u8fdbRL\u6846\u67b6\u96c6\u6210\u3002", "result": "\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMERCI\u9f13\u52b1\u66f4\u4e30\u5bcc\u591a\u6837\u7684\u601d\u7ef4\u94fe\uff0c\u663e\u8457\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u5e2e\u52a9\u7b56\u7565\u9003\u79bb\u5c40\u90e8\u6a21\u5f0f\u53d1\u73b0\u66f4\u597d\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u9488\u5bf9\u6027\u7684\u5185\u5728\u52a8\u673a\u53ef\u4ee5\u4f7f\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u63a2\u7d22\u66f4\u52a0\u53ef\u9760\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.16701", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16701", "abs": "https://arxiv.org/abs/2510.16701", "authors": ["Ni Zhang", "Zhiguang Cao", "Jianan Zhou", "Cong Zhang", "Yew-Soon Ong"], "title": "An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems", "comment": null, "summary": "Complex vehicle routing problems (VRPs) remain a fundamental challenge,\ndemanding substantial expert effort for intent interpretation and algorithm\ndesign. While large language models (LLMs) offer a promising path toward\nautomation, current approaches still rely on external intervention, which\nrestrict autonomy and often lead to execution errors and low solution\nfeasibility. To address these challenges, we propose an Agentic Framework with\nLLMs (AFL) for solving complex vehicle routing problems, achieving full\nautomation from problem instance to solution. AFL directly extracts knowledge\nfrom raw inputs and enables self-contained code generation without handcrafted\nmodules or external solvers. To improve trustworthiness, AFL decomposes the\noverall pipeline into three manageable subtasks and employs four specialized\nagents whose coordinated interactions enforce cross-functional consistency and\nlogical soundness. Extensive experiments on 60 complex VRPs, ranging from\nstandard benchmarks to practical variants, validate the effectiveness and\ngenerality of our framework, showing comparable performance against\nmeticulously designed algorithms. Notably, it substantially outperforms\nexisting LLM-based baselines in both code reliability and solution feasibility,\nachieving rates close to 100% on the evaluated benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u6846\u67b6AFL\uff0c\u7528\u4e8e\u5b8c\u5168\u81ea\u52a8\u5316\u89e3\u51b3\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff0c\u65e0\u9700\u5916\u90e8\u5e72\u9884\u5373\u53ef\u4ece\u95ee\u9898\u5b9e\u4f8b\u751f\u6210\u53ef\u884c\u89e3\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u65f6\u4ecd\u4f9d\u8d56\u5916\u90e8\u5e72\u9884\uff0c\u5bfc\u81f4\u81ea\u4e3b\u6027\u53d7\u9650\u3001\u6267\u884c\u9519\u8bef\u591a\u548c\u89e3\u53ef\u884c\u6027\u4f4e\u3002\u9700\u8981\u5b9e\u73b0\u4ece\u95ee\u9898\u5230\u89e3\u51b3\u65b9\u6848\u7684\u5b8c\u5168\u81ea\u52a8\u5316\u3002", "method": "AFL\u6846\u67b6\u5c06\u6574\u4f53\u6d41\u7a0b\u5206\u89e3\u4e3a\u4e09\u4e2a\u53ef\u7ba1\u7406\u7684\u5b50\u4efb\u52a1\uff0c\u4f7f\u7528\u56db\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u901a\u8fc7\u534f\u8c03\u4ea4\u4e92\u786e\u4fdd\u8de8\u529f\u80fd\u4e00\u81f4\u6027\u548c\u903b\u8f91\u5408\u7406\u6027\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u8f93\u5165\u63d0\u53d6\u77e5\u8bc6\u5e76\u751f\u6210\u81ea\u5305\u542b\u4ee3\u7801\u3002", "result": "\u572860\u4e2a\u590d\u6742VRP\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4ee3\u7801\u53ef\u9760\u6027\u548c\u89e3\u53ef\u884c\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u57fa\u7ebf\uff0c\u5728\u8bc4\u4f30\u57fa\u51c6\u4e0a\u63a5\u8fd1100%\u7684\u6210\u529f\u7387\uff0c\u6027\u80fd\u53ef\u4e0e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7b97\u6cd5\u76f8\u5ab2\u7f8e\u3002", "conclusion": "AFL\u6846\u67b6\u5b9e\u73b0\u4e86\u590d\u6742\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u6c42\u89e3\u7684\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u9760\u6027\u548c\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.16720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16720", "abs": "https://arxiv.org/abs/2510.16720", "authors": ["Jitao Sang", "Jinlin Xiao", "Jiarun Han", "Jilin Chen", "Xiaoyi Chen", "Shuyu Wei", "Yongjie Sun", "Yuhang Wang"], "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI", "comment": null, "summary": "The rapid evolution of agentic AI marks a new phase in artificial\nintelligence, where Large Language Models (LLMs) no longer merely respond but\nact, reason, and adapt. This survey traces the paradigm shift in building\nagentic AI: from Pipeline-based systems, where planning, tool use, and memory\nare orchestrated by external logic, to the emerging Model-native paradigm,\nwhere these capabilities are internalized within the model's parameters. We\nfirst position Reinforcement Learning (RL) as the algorithmic engine enabling\nthis paradigm shift. By reframing learning from imitating static data to\noutcome-driven exploration, RL underpins a unified solution of LLM + RL + Task\nacross language, vision and embodied domains. Building on this, the survey\nsystematically reviews how each capability -- Planning, Tool use, and Memory --\nhas evolved from externally scripted modules to end-to-end learned behaviors.\nFurthermore, it examines how this paradigm shift has reshaped major agent\napplications, specifically the Deep Research agent emphasizing long-horizon\nreasoning and the GUI agent emphasizing embodied interaction. We conclude by\ndiscussing the continued internalization of agentic capabilities like\nMulti-agent collaboration and Reflection, alongside the evolving roles of the\nsystem and model layers in future agentic AI. Together, these developments\noutline a coherent trajectory toward model-native agentic AI as an integrated\nlearning and interaction framework, marking the transition from constructing\nsystems that apply intelligence to developing models that grow intelligence\nthrough experience.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u667a\u80fd\u4f53AI\u4ece\u57fa\u4e8e\u7ba1\u9053\u7684\u7cfb\u7edf\u5411\u6a21\u578b\u539f\u751f\u8303\u5f0f\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5176\u4e2d\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u8bb0\u5fc6\u7b49\u80fd\u529b\u4ece\u5916\u90e8\u7f16\u6392\u8f6c\u53d8\u4e3a\u6a21\u578b\u5185\u90e8\u53c2\u6570\u5316\u3002", "motivation": "\u8ffd\u8e2a\u667a\u80fd\u4f53AI\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ece\u5916\u90e8\u903b\u8f91\u7f16\u6392\u7684\u7ba1\u9053\u7cfb\u7edf\u5230\u80fd\u529b\u5185\u90e8\u5316\u7684\u6a21\u578b\u539f\u751f\u8303\u5f0f\uff0c\u4ee5\u7406\u89e3AI\u4ece\u54cd\u5e94\u5230\u884c\u52a8\u3001\u63a8\u7406\u548c\u9002\u5e94\u7684\u6f14\u8fdb\u3002", "method": "\u5c06\u5f3a\u5316\u5b66\u4e60\u5b9a\u4f4d\u4e3a\u8303\u5f0f\u8f6c\u53d8\u7684\u7b97\u6cd5\u5f15\u64ce\uff0c\u7cfb\u7edf\u56de\u987e\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u8bb0\u5fc6\u80fd\u529b\u4ece\u5916\u90e8\u811a\u672c\u6a21\u5757\u5230\u7aef\u5230\u7aef\u5b66\u4e60\u884c\u4e3a\u7684\u6f14\u53d8\uff0c\u5e76\u5206\u6790\u5bf9\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u548cGUI\u4ee3\u7406\u5e94\u7528\u7684\u5f71\u54cd\u3002", "result": "\u63ed\u793a\u4e86\u667a\u80fd\u4f53AI\u5411\u6a21\u578b\u539f\u751f\u8303\u5f0f\u7684\u7edf\u4e00\u53d1\u5c55\u8f68\u8ff9\uff0c\u5176\u4e2dLLM+RL+\u4efb\u52a1\u6784\u6210\u4e86\u8de8\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u5177\u8eab\u9886\u57df\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u667a\u80fd\u4f53AI\u6b63\u671d\u7740\u6a21\u578b\u539f\u751f\u65b9\u5411\u53d1\u5c55\uff0c\u4ece\u6784\u5efa\u5e94\u7528\u667a\u80fd\u7684\u7cfb\u7edf\u8f6c\u5411\u5f00\u53d1\u901a\u8fc7\u7ecf\u9a8c\u589e\u957f\u667a\u80fd\u7684\u6a21\u578b\uff0c\u6807\u5fd7\u7740\u4ece\u5e94\u7528\u667a\u80fd\u5230\u751f\u957f\u667a\u80fd\u7684\u8f6c\u53d8\u3002", "topic": "agent analysis"}}
{"id": "2510.16724", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16724", "abs": "https://arxiv.org/abs/2510.16724", "authors": ["Minhua Lin", "Zongyu Wu", "Zhichao Xu", "Hui Liu", "Xianfeng Tang", "Qi He", "Charu Aggarwal", "Hui Liu", "Xiang Zhang", "Suhang Wang"], "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications", "comment": "38 pages, 4 figures, 7 tables", "summary": "The advent of large language models (LLMs) has transformed information access\nand reasoning through open-ended natural language interaction. However, LLMs\nremain limited by static knowledge, factual hallucinations, and the inability\nto retrieve real-time or domain-specific information. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by grounding model outputs in external\nevidence, but traditional RAG pipelines are often single turn and heuristic,\nlacking adaptive control over retrieval and reasoning. Recent advances in\nagentic search address these limitations by enabling LLMs to plan, retrieve,\nand reflect through multi-step interaction with search environments. Within\nthis paradigm, reinforcement learning (RL) offers a powerful mechanism for\nadaptive and self-improving search behavior. This survey provides the first\ncomprehensive overview of \\emph{RL-based agentic search}, organizing the\nemerging field along three complementary dimensions: (i) What RL is for\n(functional roles), (ii) How RL is used (optimization strategies), and (iii)\nWhere RL is applied (scope of optimization). We summarize representative\nmethods, evaluation protocols, and applications, and discuss open challenges\nand future directions toward building reliable and scalable RL driven agentic\nsearch systems. We hope this survey will inspire future research on the\nintegration of RL and agentic search. Our repository is available at\nhttps://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u5173\u4e8e\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u641c\u7d22\u4ee3\u7406\u7684\u7efc\u8ff0\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u8fd9\u4e00\u65b0\u5174\u9886\u57df\uff0c\u4eceRL\u7684\u529f\u80fd\u89d2\u8272\u3001\u4f18\u5316\u7b56\u7565\u548c\u5e94\u7528\u8303\u56f4\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u7ec4\u7ec7\u3002", "motivation": "\u4f20\u7edfRAG\u65b9\u6cd5\u5b58\u5728\u5355\u8f6e\u4ea4\u4e92\u3001\u542f\u53d1\u5f0f\u68c0\u7d22\u7684\u5c40\u9650\u6027\uff0c\u800c\u667a\u80fd\u641c\u7d22\u4ee3\u7406\u901a\u8fc7\u591a\u6b65\u4ea4\u4e92\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002\u5f3a\u5316\u5b66\u4e60\u4e3a\u667a\u80fd\u641c\u7d22\u63d0\u4f9b\u4e86\u81ea\u9002\u5e94\u548c\u81ea\u6211\u6539\u8fdb\u7684\u673a\u5236\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u7ec4\u7ec7\u8be5\u9886\u57df\uff1aRL\u7684\u529f\u80fd\u89d2\u8272\u3001\u4f18\u5316\u7b56\u7565\u548c\u5e94\u7528\u8303\u56f4\u3002\u603b\u7ed3\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u5e94\u7528\u6848\u4f8b\u3002", "result": "\u63d0\u4f9b\u4e86\u8be5\u9886\u57df\u7684\u9996\u4e2a\u5168\u9762\u6982\u8ff0\uff0c\u5efa\u7acb\u4e86\u7cfb\u7edf\u5316\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u521b\u5efa\u4e86\u5f00\u6e90\u8d44\u6e90\u5e93\u4f9b\u7814\u7a76\u8005\u53c2\u8003\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u667a\u80fd\u641c\u7d22\u7cfb\u7edf\u5177\u6709\u5e7f\u9614\u524d\u666f\uff0c\u8be5\u7efc\u8ff0\u65e8\u5728\u6fc0\u53d1\u672a\u6765RL\u4e0e\u667a\u80fd\u641c\u7d22\u878d\u5408\u7684\u7814\u7a76\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.16051", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.16051", "abs": "https://arxiv.org/abs/2510.16051", "authors": ["Sofiya Garkot", "Maksym Shamrai", "Ivan Synytsia", "Mariya Hirna"], "title": "GUIrilla: A Scalable Framework for Automated Desktop UI Exploration", "comment": "22 pages", "summary": "Autonomous agents capable of operating complex graphical user interfaces\n(GUIs) have the potential to transform desktop automation. While recent\nadvances in large language models (LLMs) have significantly improved UI\nunderstanding, navigating full-window, multi-application desktop environments\nremains a major challenge. Data availability is limited by costly manual\nannotation, closed-source datasets and surface-level synthetic pipelines. We\nintroduce GUIrilla, an automated scalable framework that systematically\nexplores applications via native accessibility APIs to address the critical\ndata collection challenge in GUI automation. Our framework focuses on macOS -\nan ecosystem with limited representation in current UI datasets - though many\nof its components are designed for broader cross-platform applicability.\nGUIrilla organizes discovered interface elements and crawler actions into\nhierarchical GUI graphs and employs specialized interaction handlers to achieve\ncomprehensive application coverage. Using the application graphs from GUIrilla\ncrawler, we construct and release GUIrilla-Task, a large-scale dataset of\n27,171 functionally grounded tasks across 1,108 macOS applications, each\nannotated with full-desktop and window-level screenshots, accessibility\nmetadata, and semantic action traces. Empirical results show that tuning\nLLM-based agents on GUIrilla-Task significantly improves performance on\ndownstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro\nbenchmark while using 97% less data. We also release macapptree, an open-source\nlibrary for reproducible collection of structured accessibility metadata, along\nwith the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold\nbenchmark, and the framework code to support open research in desktop autonomy.", "AI": {"tldr": "GUIrilla\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u751f\u53ef\u8bbf\u95ee\u6027API\u7cfb\u7edf\u63a2\u7d22\u5e94\u7528\u7a0b\u5e8f\uff0c\u89e3\u51b3GUI\u81ea\u52a8\u5316\u4e2d\u7684\u6570\u636e\u6536\u96c6\u6311\u6218\u3002\u8be5\u6846\u67b6\u6784\u5efa\u4e86GUIrilla-Task\u6570\u636e\u96c6\uff0c\u5305\u542b27,171\u4e2a\u529f\u80fd\u57fa\u7840\u4efb\u52a1\uff0c\u8986\u76d61,108\u4e2amacOS\u5e94\u7528\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684LLM\u4ee3\u7406\u5728UI\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u56fe\u5f62\u7528\u6237\u754c\u9762(GUI)\u81ea\u52a8\u5316\u4e2d\u7684\u6570\u636e\u53ef\u7528\u6027\u9650\u5236\u95ee\u9898\uff0c\u5305\u62ec\u6602\u8d35\u7684\u624b\u52a8\u6807\u6ce8\u3001\u95ed\u6e90\u6570\u636e\u96c6\u548c\u8868\u9762\u7ea7\u5408\u6210\u6d41\u7a0b\uff0c\u7279\u522b\u662f\u5728macOS\u751f\u6001\u7cfb\u7edf\u4e2d\u5f53\u524dUI\u6570\u636e\u96c6\u4ee3\u8868\u6027\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u539f\u751f\u53ef\u8bbf\u95ee\u6027API\u7cfb\u7edf\u63a2\u7d22\u5e94\u7528\u7a0b\u5e8f\uff0c\u5c06\u53d1\u73b0\u7684\u754c\u9762\u5143\u7d20\u548c\u722c\u866b\u64cd\u4f5c\u7ec4\u7ec7\u6210\u5c42\u6b21\u5316GUI\u56fe\uff0c\u91c7\u7528\u4e13\u95e8\u7684\u4ea4\u4e92\u5904\u7406\u7a0b\u5e8f\u5b9e\u73b0\u5168\u9762\u7684\u5e94\u7528\u8986\u76d6\uff0c\u5e76\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "result": "\u6784\u5efa\u4e86GUIrilla-Task\u6570\u636e\u96c6(27,171\u4e2a\u4efb\u52a1\uff0c1,108\u4e2a\u5e94\u7528)\uff0c\u5728ScreenSpot Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5408\u6210\u57fa\u7ebf\uff0c\u540c\u65f6\u4f7f\u7528\u6570\u636e\u91cf\u51cf\u5c1197%\u3002\u53d1\u5e03\u4e86macapptree\u5f00\u6e90\u5e93\u3001GUIrilla-Task\u6570\u636e\u96c6\u3001GUIrilla-Gold\u57fa\u51c6\u548c\u6846\u67b6\u4ee3\u7801\u3002", "conclusion": "GUIrilla\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u684c\u9762GUI\u81ea\u52a8\u5316\u7684\u6570\u636e\u6536\u96c6\u6311\u6218\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u6784\u5efa\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728UI\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u684c\u9762\u81ea\u4e3b\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u653e\u652f\u6301\u3002", "topic": "swe application"}}
{"id": "2510.16844", "categories": ["cs.CL", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.16844", "abs": "https://arxiv.org/abs/2510.16844", "authors": ["Jiajie Jin", "Yuyao Zhang", "Yimeng Xu", "Hongjin Qian", "Yutao Zhu", "Zhicheng Dou"], "title": "FinSight: Towards Real-World Financial Deep Research", "comment": "Working in progress", "summary": "Generating professional financial reports is a labor-intensive and\nintellectually demanding process that current AI systems struggle to fully\nautomate. To address this challenge, we introduce FinSight (Financial InSight),\na novel multi agent framework for producing high-quality, multimodal financial\nreports. The foundation of FinSight is the Code Agent with Variable Memory\n(CAVM) architecture, which unifies external data, designed tools, and agents\ninto a programmable variable space, enabling flexible data collection, analysis\nand report generation through executable code. To ensure professional-grade\nvisualization, we propose an Iterative Vision-Enhanced Mechanism that\nprogressively refines raw visual outputs into polished financial charts.\nFurthermore, a two stage Writing Framework expands concise Chain-of-Analysis\nsegments into coherent, citation-aware, and multimodal reports, ensuring both\nanalytical depth and structural consistency. Experiments on various company and\nindustry-level tasks demonstrate that FinSight significantly outperforms all\nbaselines, including leading deep research systems in terms of factual\naccuracy, analytical depth, and presentation quality, demonstrating a clear\npath toward generating reports that approach human-expert quality.", "AI": {"tldr": "FinSight\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u8d22\u52a1\u62a5\u544a\uff0c\u901a\u8fc7CAVM\u67b6\u6784\u7edf\u4e00\u6570\u636e\u3001\u5de5\u5177\u548c\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u8fed\u4ee3\u89c6\u89c9\u589e\u5f3a\u673a\u5236\u4f18\u5316\u56fe\u8868\uff0c\u4e24\u9636\u6bb5\u5199\u4f5c\u6846\u67b6\u786e\u4fdd\u5206\u6790\u6df1\u5ea6\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u96be\u4ee5\u5b8c\u5168\u81ea\u52a8\u5316\u751f\u6210\u4e13\u4e1a\u8d22\u52a1\u62a5\u544a\uff0c\u8fd9\u662f\u4e00\u4e2a\u52b3\u52a8\u5bc6\u96c6\u4e14\u667a\u529b\u8981\u6c42\u9ad8\u7684\u8fc7\u7a0b\u3002", "method": "\u57fa\u4e8eCAVM\u67b6\u6784\u7edf\u4e00\u5916\u90e8\u6570\u636e\u3001\u8bbe\u8ba1\u5de5\u5177\u548c\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u8fed\u4ee3\u89c6\u89c9\u589e\u5f3a\u673a\u5236\u4f18\u5316\u53ef\u89c6\u5316\uff0c\u4e24\u9636\u6bb5\u5199\u4f5c\u6846\u67b6\u6269\u5c55\u5206\u6790\u6bb5\u4e3a\u591a\u6a21\u6001\u62a5\u544a\u3002", "result": "\u5728\u5404\u79cd\u516c\u53f8\u548c\u884c\u4e1a\u7ea7\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFinSight\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5206\u6790\u6df1\u5ea6\u548c\u5448\u73b0\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u7cfb\u7edf\u3002", "conclusion": "FinSight\u5c55\u793a\u4e86\u751f\u6210\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u8d28\u91cf\u62a5\u544a\u7684\u6e05\u6670\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2510.16872", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.16872", "abs": "https://arxiv.org/abs/2510.16872", "authors": ["Shaolei Zhang", "Ju Fan", "Meihao Fan", "Guoliang Li", "Xiaoyong Du"], "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science", "comment": "Code: https://github.com/ruc-datalab/DeepAnalyze Model:\n  https://huggingface.co/RUC-DataLab/DeepAnalyze-8B", "summary": "Autonomous data science, from raw data sources to analyst-grade deep research\nreports, has been a long-standing challenge, and is now becoming feasible with\nthe emergence of powerful large language models (LLMs). Recent workflow-based\ndata agents have shown promising results on specific data tasks but remain\nfundamentally limited in achieving fully autonomous data science due to their\nreliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,\nthe first agentic LLM designed for autonomous data science, capable of\nautomatically completing the end-toend pipeline from data sources to\nanalyst-grade deep research reports. To tackle high-complexity data science\ntasks, we propose a curriculum-based agentic training paradigm that emulates\nthe learning trajectory of human data scientists, enabling LLMs to\nprogressively acquire and integrate multiple capabilities in real-world\nenvironments. We also introduce a data-grounded trajectory synthesis framework\nthat constructs high-quality training data. Through agentic training,\nDeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data\nquestion answering and specialized analytical tasks to open-ended data\nresearch. Experiments demonstrate that, with only 8B parameters, DeepAnalyze\noutperforms previous workflow-based agents built on most advanced proprietary\nLLMs. The model, code, and training data of DeepAnalyze are open-sourced,\npaving the way toward autonomous data science.", "AI": {"tldr": "DeepAnalyze-8B\u662f\u9996\u4e2a\u7528\u4e8e\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u7684\u667a\u80fdLLM\uff0c\u80fd\u591f\u81ea\u52a8\u5b8c\u6210\u4ece\u6570\u636e\u6e90\u5230\u5206\u6790\u5e08\u7ea7\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5f0f\u667a\u80fd\u8bad\u7ec3\u57288B\u53c2\u6570\u4e0b\u8d85\u8d8a\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u5148\u8fdb\u4e13\u6709LLM\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u6570\u636e\u4ee3\u7406\u5728\u7279\u5b9a\u6570\u636e\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e\u4f9d\u8d56\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684\u6570\u636e\u79d1\u5b66\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u5b8c\u6210\u7aef\u5230\u7aef\u6570\u636e\u79d1\u5b66\u6d41\u7a0b\u7684\u667a\u80fd\u4ee3\u7406\u3002", "method": "\u63d0\u51fa\u8bfe\u7a0b\u5f0f\u667a\u80fd\u8bad\u7ec3\u8303\u5f0f\uff0c\u6a21\u62df\u4eba\u7c7b\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u5b66\u4e60\u8f68\u8ff9\uff0c\u8ba9LLM\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9010\u6b65\u83b7\u53d6\u548c\u6574\u5408\u591a\u79cd\u80fd\u529b\uff1b\u540c\u65f6\u5f15\u5165\u6570\u636e\u9a71\u52a8\u7684\u8f68\u8ff9\u5408\u6210\u6846\u67b6\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u4ec5\u75288B\u53c2\u6570\u7684DeepAnalyze\u5728\u6570\u636e\u95ee\u7b54\u3001\u4e13\u4e1a\u5206\u6790\u4efb\u52a1\u548c\u5f00\u653e\u5f0f\u6570\u636e\u7814\u7a76\u7b49\u5e7f\u6cdb\u6570\u636e\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6700\u5148\u8fdb\u4e13\u6709LLM\u6784\u5efa\u7684\u5de5\u4f5c\u6d41\u4ee3\u7406\u3002", "conclusion": "DeepAnalyze\u4e3a\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u6a21\u578b\u3001\u4ee3\u7801\u548c\u8bad\u7ec3\u6570\u636e\u5747\u5df2\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2510.16907", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16907", "abs": "https://arxiv.org/abs/2510.16907", "authors": ["Kangrui Wang", "Pingyue Zhang", "Zihan Wang", "Yaning Gao", "Linjie Li", "Qineng Wang", "Hanyang Chen", "Chi Wan", "Yiping Lu", "Zhengyuan Yang", "Lijuan Wang", "Ranjay Krishna", "Jiajun Wu", "Li Fei-Fei", "Yejin Choi", "Manling Li"], "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "comment": "Accepted to NeurIPS 2025", "summary": "A key challenge in training Vision-Language Model (VLM) agents, compared to\nLanguage Model (LLM) agents, lies in the shift from textual states to complex\nvisual observations. This transition introduces partial observability and\ndemands robust world modeling. We ask: Can VLM agents construct internal world\nmodels through explicit visual state reasoning? To address this question, we\narchitecturally enforce and reward the agent's reasoning process via\nreinforcement learning (RL), formulating it as a Partially Observable Markov\nDecision Process (POMDP). We find that decomposing the agent's reasoning into\nState Estimation (\"what is the current state?\") and Transition Modeling (\"what\ncomes next?\") is critical for success, as demonstrated through five reasoning\nstrategies. Our investigation into how agents represent internal beliefs\nreveals that the optimal representation is task-dependent: Natural Language\nexcels at capturing semantic relationships in general tasks, while Structured\nformats are indispensable for precise manipulation and control. Building on\nthese insights, we design a World Modeling Reward that provides dense,\nturn-level supervision for accurate state prediction, and introduce Bi-Level\nGeneral Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment.\nThrough this form of visual state reasoning, a 3B-parameter model achieves a\nscore of 0.82 across five diverse agent benchmarks, representing a 3$\\times$\nimprovement over its untrained counterpart (0.21) and outperforming proprietary\nreasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5\n(0.62). All experiments are conducted within our VAGEN framework, a scalable\nsystem for training and analyzing multi-turn VLM agents in diverse visual\nenvironments. Code and data are publicly available at\nhttps://vagen-ai.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3VLM\u667a\u80fd\u4f53\u8fdb\u884c\u663e\u5f0f\u89c6\u89c9\u72b6\u6001\u63a8\u7406\uff0c\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u5728\u4e94\u4e2a\u591a\u6837\u5316\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e863\u500d\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86GPT-5\u7b49\u4e13\u6709\u63a8\u7406\u6a21\u578b\u3002", "motivation": "\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u4ece\u6587\u672c\u72b6\u6001\u8f6c\u5411\u590d\u6742\u89c6\u89c9\u89c2\u5bdf\uff0c\u8fd9\u5f15\u5165\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u5e76\u9700\u8981\u5f3a\u5927\u7684\u4e16\u754c\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u63a8\u7406\u8fc7\u7a0b\u67b6\u6784\u6027\u5730\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5f3a\u5236\u548c\u5956\u52b1\uff0c\u5c06\u5176\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5206\u89e3\u4e3a\u72b6\u6001\u4f30\u8ba1\u548c\u8f6c\u79fb\u5efa\u6a21\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e16\u754c\u5efa\u6a21\u5956\u52b1\u548c\u53cc\u5c42\u901a\u7528\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "3B\u53c2\u6570\u6a21\u578b\u5728\u4e94\u4e2a\u591a\u6837\u5316\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52300.82\u5206\uff0c\u76f8\u6bd4\u672a\u8bad\u7ec3\u7248\u672c(0.21)\u63d0\u53473\u500d\uff0c\u8d85\u8d8a\u4e86GPT-5(0.75)\u3001Gemini 2.5 Pro(0.67)\u548cClaude 4.5(0.62)\u3002", "conclusion": "\u89c6\u89c9\u72b6\u6001\u63a8\u7406\u662fVLM\u667a\u80fd\u4f53\u6210\u529f\u7684\u5173\u952e\uff0c\u6700\u4f18\u8868\u793a\u5f62\u5f0f\u53d6\u51b3\u4e8e\u4efb\u52a1\u6027\u8d28\uff0c\u81ea\u7136\u8bed\u8a00\u9002\u5408\u8bed\u4e49\u5173\u7cfb\u6355\u6349\uff0c\u7ed3\u6784\u5316\u683c\u5f0f\u5bf9\u7cbe\u786e\u64cd\u4f5c\u63a7\u5236\u81f3\u5173\u91cd\u8981\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.16956", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16956", "abs": "https://arxiv.org/abs/2510.16956", "authors": ["Mark Towers", "Yali Du", "Christopher Freeman", "Timothy J. Norman"], "title": "A Comparative User Evaluation of XRL Explanations using Goal Identification", "comment": "Accepted to ECAI 2025 Workshop on Evaluating Explainable AI and\n  Complex Decision-Making, 8 Pages", "summary": "Debugging is a core application of explainable reinforcement learning (XRL)\nalgorithms; however, limited comparative evaluations have been conducted to\nunderstand their relative performance. We propose a novel evaluation\nmethodology to test whether users can identify an agent's goal from an\nexplanation of its decision-making. Utilising the Atari's Ms. Pacman\nenvironment and four XRL algorithms, we find that only one achieved greater\nthan random accuracy for the tested goals and that users were generally\noverconfident in their selections. Further, we find that users' self-reported\nease of identification and understanding for every explanation did not\ncorrelate with their accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u7528\u6237\u80fd\u5426\u4ece\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u51b3\u7b56\u89e3\u91ca\u4e2d\u8bc6\u522b\u51fa\u4ee3\u7406\u7684\u76ee\u6807\u3002\u5728Atari\u7684Ms. Pacman\u73af\u5883\u4e2d\u6d4b\u8bd5\u4e86\u56db\u79cd\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u53d1\u73b0\u53ea\u6709\u4e00\u79cd\u7b97\u6cd5\u7684\u51c6\u786e\u7387\u8d85\u8fc7\u968f\u673a\u6c34\u5e73\uff0c\u4e14\u7528\u6237\u666e\u904d\u9ad8\u4f30\u81ea\u5df1\u7684\u5224\u65ad\u80fd\u529b\u3002", "motivation": "\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u8c03\u8bd5\u65b9\u9762\u6709\u6838\u5fc3\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u76f8\u5bf9\u6027\u80fd\u7684\u6bd4\u8f83\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528Atari\u7684Ms. Pacman\u73af\u5883\u548c\u56db\u79cdXRL\u7b97\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u6d4b\u8bd5\u6765\u8bc4\u4f30\u4ed6\u4eec\u4ece\u7b97\u6cd5\u89e3\u91ca\u4e2d\u8bc6\u522b\u4ee3\u7406\u76ee\u6807\u7684\u80fd\u529b\u3002", "result": "\u53ea\u6709\u4e00\u79cdXRL\u7b97\u6cd5\u5728\u6d4b\u8bd5\u76ee\u6807\u4e0a\u7684\u51c6\u786e\u7387\u8d85\u8fc7\u968f\u673a\u6c34\u5e73\uff1b\u7528\u6237\u666e\u904d\u5bf9\u81ea\u5df1\u7684\u9009\u62e9\u8fc7\u5ea6\u81ea\u4fe1\uff1b\u7528\u6237\u81ea\u6211\u62a5\u544a\u7684\u8bc6\u522b\u96be\u6613\u5ea6\u548c\u7406\u89e3\u7a0b\u5ea6\u4e0e\u5b9e\u9645\u51c6\u786e\u7387\u4e0d\u76f8\u5173\u3002", "conclusion": "\u5f53\u524d\u7684\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u4ee3\u7406\u76ee\u6807\u65b9\u9762\u6548\u679c\u6709\u9650\uff0c\u7528\u6237\u7684\u4e3b\u89c2\u611f\u53d7\u4e0e\u5b9e\u9645\u7406\u89e3\u80fd\u529b\u5b58\u5728\u8131\u8282\u3002", "topic": "agent analysis"}}
{"id": "2510.16932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16932", "abs": "https://arxiv.org/abs/2510.16932", "authors": ["Emily Xiao", "Yixiao Zeng", "Ada Chen", "Chin-Jou Li", "Amanda Bertsch", "Graham Neubig"], "title": "Prompt-MII: Meta-Learning Instruction Induction for LLMs", "comment": null, "summary": "A popular method to adapt large language models (LLMs) to new tasks is\nin-context learning (ICL), which is effective but incurs high inference costs\nas context length grows. In this paper we propose a method to perform\ninstruction induction, where we take training examples and reduce them to a\ncompact but descriptive prompt that can achieve performance comparable to ICL\nover the full training set. Specifically, we propose PROMPT-MII, a\nreinforcement learning (RL) based framework to meta-learn an instruction\ninduction model that can generate compact instructions on the fly for an\narbitrary new dataset. We train on over 3,000 diverse classification datasets\nfrom the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves\ndownstream model quality by 4-9 F1 points (10-20% relative), matching ICL\nperformance while requiring 3-13x fewer tokens.", "AI": {"tldr": "\u63d0\u51faPROMPT-MII\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5143\u5b66\u4e60\u6307\u4ee4\u5f52\u7eb3\u6a21\u578b\uff0c\u751f\u6210\u7d27\u51d1\u6307\u4ee4\u66ff\u4ee3\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u867d\u7136\u6709\u6548\u4f46\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u9700\u8981\u627e\u5230\u66f4\u7d27\u51d1\u7684\u6307\u4ee4\u8868\u793a\u65b9\u6cd5\u6765\u66ff\u4ee3\u5b8c\u6574\u7684\u8bad\u7ec3\u96c6\u793a\u4f8b\u3002", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u57283,000\u591a\u4e2a\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u5143\u5b66\u4e60\u6307\u4ee4\u5f52\u7eb3\u6a21\u578b\uff0c\u80fd\u591f\u4e3a\u65b0\u6570\u636e\u96c6\u52a8\u6001\u751f\u6210\u7d27\u51d1\u6307\u4ee4\u3002", "result": "\u572890\u4e2a\u672a\u89c1\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0cPROMPT-MII\u63d0\u5347\u4e0b\u6e38\u6a21\u578b\u8d28\u91cf4-9\u4e2aF1\u70b9\uff0810-20%\u76f8\u5bf9\u63d0\u5347\uff09\uff0c\u5339\u914d\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\u540c\u65f6\u51cf\u5c113-13\u500dtoken\u4f7f\u7528\u3002", "conclusion": "PROMPT-MII\u80fd\u591f\u6709\u6548\u66ff\u4ee3\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.16996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16996", "abs": "https://arxiv.org/abs/2510.16996", "authors": ["Juncheng Dong", "Yang Yang", "Tao Liu", "Yang Wang", "Feng Qi", "Vahid Tarokh", "Kaushik Rangadurai", "Shuang Yang"], "title": "STARK: Strategic Team of Agents for Refining Kernels", "comment": null, "summary": "The efficiency of GPU kernels is central to the progress of modern AI, yet\noptimizing them remains a difficult and labor-intensive task due to complex\ninteractions between memory hierarchies, thread scheduling, and\nhardware-specific characteristics. While recent advances in large language\nmodels (LLMs) provide new opportunities for automated code generation, existing\napproaches largely treat LLMs as single-shot generators or naive refinement\ntools, limiting their effectiveness in navigating the irregular kernel\noptimization landscape. We introduce an LLM agentic framework for GPU kernel\noptimization that systematically explores the design space through multi-agent\ncollaboration, grounded instruction, dynamic context management, and strategic\nsearch. This framework mimics the workflow of expert engineers, enabling LLMs\nto reason about hardware trade-offs, incorporate profiling feedback, and refine\nkernels iteratively. We evaluate our approach on KernelBench, a benchmark for\nLLM-based kernel optimization, and demonstrate substantial improvements over\nbaseline agents: our system produces correct solutions where baselines often\nfail, and achieves kernels with up to 16x faster runtime performance. These\nresults highlight the potential of agentic LLM frameworks to advance fully\nautomated, scalable GPU kernel optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684LLM\u6846\u67b6\uff0c\u7528\u4e8eGPU\u5185\u6838\u4f18\u5316\uff0c\u901a\u8fc7\u7cfb\u7edf\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\u3001\u52a8\u6001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u7b56\u7565\u641c\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u6838\u6027\u80fd\u3002", "motivation": "GPU\u5185\u6838\u4f18\u5316\u5bf9AI\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5c06LLM\u89c6\u4e3a\u5355\u6b21\u751f\u6210\u5668\u6216\u7b80\u5355\u4f18\u5316\u5de5\u5177\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u5185\u6838\u4f18\u5316\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8e\u7ecf\u9a8c\u7684\u6307\u5bfc\u3001\u52a8\u6001\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u7b56\u7565\u641c\u7d22\uff0c\u6a21\u62df\u4e13\u5bb6\u5de5\u7a0b\u5e08\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5728KernelBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7cfb\u7edf\u5728\u57fa\u7ebf\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4ea7\u751f\u6b63\u786e\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5b9e\u73b0\u9ad8\u8fbe16\u500d\u7684\u8fd0\u884c\u65f6\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u667a\u80fd\u4f53LLM\u6846\u67b6\u5728\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684GPU\u5185\u6838\u4f18\u5316\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.17052", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17052", "abs": "https://arxiv.org/abs/2510.17052", "authors": ["Hassan Hamad", "Yingru Xu", "Liang Zhao", "Wenbo Yan", "Narendra Gyanchandani"], "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems", "comment": null, "summary": "Tool-augmented large language models (LLMs) are increasingly employed in\nreal-world applications, but tool usage errors still hinder their reliability.\nWe introduce ToolCritic, a diagnostic framework that evaluates and improves LLM\nbehavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight\ndistinct error types specific to tool-calling (e.g., premature invocation,\nargument misalignment, and misinterpretation of tool outputs) and provides\ntargeted feedback to the main LLM. The main LLM, assumed to have strong\nreasoning, task understanding and orchestration capabilities, then revises its\nresponse based on ToolCritic's feedback. We systematically define these error\ncategories and construct a synthetic dataset to train ToolCritic. Experimental\nresults on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic\nimproves tool-calling accuracy by up to 13% over baselines, including zero-shot\nprompting and self-correction techniques. This represents a promising step\ntoward more robust LLM integration with external tools in real-world dialogue\napplications.", "AI": {"tldr": "ToolCritic\u662f\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u5728\u5de5\u5177\u589e\u5f3a\u5bf9\u8bdd\u4e2d\u7684\u884c\u4e3a\uff0c\u901a\u8fc7\u68c0\u6d4b8\u79cd\u7279\u5b9a\u9519\u8bef\u7c7b\u578b\u5e76\u63d0\u4f9b\u9488\u5bf9\u6027\u53cd\u9988\uff0c\u5c06\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u63d0\u9ad8\u4e8613%\u3002", "motivation": "\u5de5\u5177\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5de5\u5177\u4f7f\u7528\u9519\u8bef\u4ecd\u7136\u963b\u788d\u5176\u53ef\u9760\u6027\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bca\u65ad\u548c\u6539\u8fdb\u6846\u67b6\u3002", "method": "\u5f15\u5165ToolCritic\u6846\u67b6\uff0c\u5b9a\u4e498\u79cd\u5de5\u5177\u8c03\u7528\u9519\u8bef\u7c7b\u578b\uff0c\u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3ToolCritic\uff0c\u8ba9\u4e3bLLM\u6839\u636eToolCritic\u7684\u53cd\u9988\u4fee\u6b63\u54cd\u5e94\u3002", "result": "\u5728Schema-Guided Dialogue\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cToolCritic\u5c06\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u63d0\u9ad8\u4e8613%\uff0c\u4f18\u4e8e\u96f6\u6837\u672c\u63d0\u793a\u548c\u81ea\u6211\u4fee\u6b63\u6280\u672f\u3002", "conclusion": "ToolCritic\u4ee3\u8868\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u5bf9\u8bdd\u5e94\u7528\u4e2d\u66f4\u7a33\u5065\u5730\u96c6\u6210LLM\u4e0e\u5916\u90e8\u5de5\u5177\u7684\u6709\u5e0c\u671b\u7684\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2510.17108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17108", "abs": "https://arxiv.org/abs/2510.17108", "authors": ["Yoonjin Lee", "Munhee Kim", "Hanbi Choi", "Juhyeon Park", "Seungho Lyoo", "Woojin Park"], "title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI", "comment": "18 pages, 4 figures, 2 algorithms, 2 tables, 4 appendices, will be\n  submitted to AAAI-2026 workshop", "summary": "Despite advances in financial AI, the automation of evidence-based reasoning\nremains unresolved in corporate credit assessment, where qualitative\nnon-financial indicators exert decisive influence on loan repayment outcomes\nyet resist formalization. Existing approaches focus predominantly on numerical\nprediction and provide limited support for the interpretive judgments required\nin professional loan evaluation. This study develops and evaluates two\noperational large language model (LLM)-based systems designed to generate\nstructured reasoning from non-financial evidence. The first is a\nnon-adversarial single-agent system (NAS) that produces bidirectional analysis\nthrough a single-pass reasoning pipeline. The second is a debate-based\nmulti-agent system (KPD-MADS) that operationalizes adversarial verification\nthrough a ten-step structured interaction protocol grounded in Karl Popper's\ncritical dialogue framework. Both systems were applied to three real corporate\ncases and evaluated by experienced credit risk professionals. Compared to\nmanual expert reporting, both systems achieved substantial productivity gains\n(NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The\nKPD-MADS demonstrated superior reasoning quality, receiving higher median\nratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs.\n3.0), and usability (62.5 vs. 52.5). These findings show that structured\nmulti-agent interaction can enhance reasoning rigor and interpretability in\nfinancial AI, advancing scalable and defensible automation in corporate credit\nassessment.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u7528\u4e8e\u4f01\u4e1a\u4fe1\u8d37\u8bc4\u4f30\u4e2d\u7684\u8bc1\u636e\u63a8\u7406\uff1a\u5355\u4ee3\u7406\u7cfb\u7edf(NAS)\u548c\u591a\u4ee3\u7406\u8fa9\u8bba\u7cfb\u7edf(KPD-MADS)\uff0c\u540e\u8005\u57fa\u4e8e\u5361\u5c14\u00b7\u6ce2\u666e\u5c14\u7684\u6279\u5224\u6027\u5bf9\u8bdd\u6846\u67b6\uff0c\u5728\u63a8\u7406\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u4fe1\u8d37\u8bc4\u4f30\u4e2d\u5b9a\u6027\u975e\u8d22\u52a1\u6307\u6807\u81ea\u52a8\u5316\u63a8\u7406\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6570\u503c\u9884\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u4e13\u4e1a\u8d37\u6b3e\u8bc4\u4f30\u6240\u9700\u89e3\u91ca\u6027\u5224\u65ad\u7684\u652f\u6301\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cdLLM\u7cfb\u7edf\uff1a\u5355\u4ee3\u7406\u7cfb\u7edf(NAS)\u901a\u8fc7\u5355\u6b21\u63a8\u7406\u7ba1\u9053\u751f\u6210\u53cc\u5411\u5206\u6790\uff1b\u591a\u4ee3\u7406\u8fa9\u8bba\u7cfb\u7edf(KPD-MADS)\u57fa\u4e8e\u5361\u5c14\u00b7\u6ce2\u666e\u5c14\u6279\u5224\u6027\u5bf9\u8bdd\u6846\u67b6\uff0c\u91c7\u7528\u5341\u6b65\u7ed3\u6784\u5316\u4ea4\u4e92\u534f\u8bae\u8fdb\u884c\u5bf9\u6297\u6027\u9a8c\u8bc1\u3002", "result": "\u4e24\u79cd\u7cfb\u7edf\u5747\u5b9e\u73b0\u663e\u8457\u751f\u4ea7\u529b\u63d0\u5347(NAS:11.55\u79d2/\u6848\u4f8b\uff1bKPD-MADS:91.97\u79d2\uff1b\u4eba\u5de5\u57fa\u51c6:1920\u79d2)\u3002KPD-MADS\u5728\u89e3\u91ca\u5145\u5206\u6027(4.0 vs 3.0)\u3001\u5b9e\u9645\u9002\u7528\u6027(4.0 vs 3.0)\u548c\u53ef\u7528\u6027(62.5 vs 52.5)\u65b9\u9762\u83b7\u5f97\u66f4\u9ad8\u8bc4\u5206\u3002", "conclusion": "\u7ed3\u6784\u5316\u591a\u4ee3\u7406\u4ea4\u4e92\u80fd\u591f\u589e\u5f3a\u91d1\u878dAI\u4e2d\u7684\u63a8\u7406\u4e25\u8c28\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u63a8\u8fdb\u4f01\u4e1a\u4fe1\u8d37\u8bc4\u4f30\u4e2d\u53ef\u6269\u5c55\u4e14\u53ef\u8fa9\u62a4\u7684\u81ea\u52a8\u5316\u3002", "topic": "agent analysis"}}
{"id": "2510.17017", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17017", "abs": "https://arxiv.org/abs/2510.17017", "authors": ["Qiusi Zhan", "Angeline Budiman-Chan", "Abdelrahman Zayed", "Xingzhi Guo", "Daniel Kang", "Joo-Kyung Kim"], "title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents", "comment": "Code: https://github.com/ZQS1943/SafeSearch", "summary": "Large language model (LLM) based search agents iteratively generate queries,\nretrieve external information, and reason to answer open-domain questions.\nWhile researchers have primarily focused on improving their utility, their\nsafety behaviors remain underexplored. In this paper, we first evaluate search\nagents using red-teaming datasets and find that they are more likely to produce\nharmful outputs than base LLMs. For example, when asked ``How can I track\nsomeone's location without their consent?'', a base model refuses, whereas a\nsearch agent designed to retrieve and cite sources may lower its refusal\nthreshold, fetch documents (e.g., court cases), and, once appended, synthesize\nthem into an informative yet unsafe summary. We further show that\nutility-oriented fine-tuning intensifies this risk, motivating joint alignment\nof safety and utility. We present SafeSearch, a multi-objective reinforcement\nlearning approach that couples a final-output safety/utility reward with a\nnovel query-level shaping term that penalizes unsafe queries and rewards safe\nones. Experiments show that SafeSearch reduces agent harmfulness by over 70%\nacross three red-teaming datasets while producing safe, helpful responses, and\nmatches the QA performance of a utility-only finetuned agent; further analyses\nconfirm the effectiveness of the query-level reward in jointly improving safety\nand utility.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u57fa\u4e8eLLM\u7684\u641c\u7d22\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u5b83\u4eec\u6bd4\u57fa\u7840LLM\u66f4\u5bb9\u6613\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\u3002\u4f5c\u8005\u63d0\u51fa\u4e86SafeSearch\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u5956\u52b1\uff0c\u5c06\u4ee3\u7406\u6709\u5bb3\u6027\u964d\u4f4e\u4e8670%\u4ee5\u4e0a\u3002", "motivation": "\u867d\u7136\u7814\u7a76\u4eba\u5458\u4e3b\u8981\u5173\u6ce8\u63d0\u9ad8LLM\u641c\u7d22\u4ee3\u7406\u7684\u5b9e\u7528\u6027\uff0c\u4f46\u5176\u5b89\u5168\u884c\u4e3a\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u53d1\u73b0\u641c\u7d22\u4ee3\u7406\u6bd4\u57fa\u7840LLM\u66f4\u5bb9\u6613\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\uff0c\u7279\u522b\u662f\u5f53\u8fdb\u884c\u5b9e\u7528\u6027\u5bfc\u5411\u7684\u5fae\u8c03\u65f6\u4f1a\u52a0\u5267\u8fd9\u79cd\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86SafeSearch\u65b9\u6cd5\uff0c\u91c7\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u6700\u7ec8\u8f93\u51fa\u7684\u5b89\u5168/\u5b9e\u7528\u6027\u5956\u52b1\u548c\u65b0\u7684\u67e5\u8be2\u7ea7\u5851\u5f62\u9879\uff0c\u60e9\u7f5a\u4e0d\u5b89\u5168\u67e5\u8be2\u5e76\u5956\u52b1\u5b89\u5168\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSafeSearch\u5728\u4e09\u4e2a\u7ea2\u961f\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u5c06\u4ee3\u7406\u6709\u5bb3\u6027\u964d\u4f4e\u4e8670%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4ea7\u751f\u5b89\u5168\u3001\u6709\u5e2e\u52a9\u7684\u54cd\u5e94\uff0c\u5e76\u4e0e\u4ec5\u5b9e\u7528\u6027\u5fae\u8c03\u7684\u4ee3\u7406\u5728\u95ee\u7b54\u6027\u80fd\u4e0a\u76f8\u5f53\u3002", "conclusion": "\u67e5\u8be2\u7ea7\u5956\u52b1\u5728\u8054\u5408\u6539\u8fdb\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u8bc1\u660e\u4e86\u8054\u5408\u5bf9\u9f50\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.17028", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17028", "abs": "https://arxiv.org/abs/2510.17028", "authors": ["Kyle Cox", "Jiawei Xu", "Yikun Han", "Rong Xu", "Tianhao Li", "Chi-Yang Hsu", "Tianlong Chen", "Walter Gerych", "Ying Ding"], "title": "Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models", "comment": null, "summary": "An interesting behavior in large language models (LLMs) is prompt\nsensitivity. When provided with different but semantically equivalent versions\nof the same prompt, models may produce very different distributions of answers.\nThis suggests that the uncertainty reflected in a model's output distribution\nfor one prompt may not reflect the model's uncertainty about the meaning of the\nprompt. We model prompt sensitivity as a type of generalization error, and show\nthat sampling across the semantic ``concept space'' with paraphrasing\nperturbations improves uncertainty calibration without compromising accuracy.\nAdditionally, we introduce a new metric for uncertainty decomposition in\nblack-box LLMs that improves upon entropy-based decomposition by modeling\nsemantic continuities in natural language generation. We show that this\ndecomposition metric can be used to quantify how much LLM uncertainty is\nattributed to prompt sensitivity. Our work introduces a new way to improve\nuncertainty calibration in prompt-sensitive language models, and provides\nevidence that some LLMs fail to exhibit consistent general reasoning about the\nmeanings of their inputs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76LLM\u7684\u63d0\u793a\u654f\u611f\u6027\u73b0\u8c61\uff0c\u53d1\u73b0\u8bed\u4e49\u76f8\u540c\u7684\u4e0d\u540c\u63d0\u793a\u4f1a\u5bfc\u81f4\u6a21\u578b\u4ea7\u751f\u4e0d\u540c\u7b54\u6848\u5206\u5e03\u3002\u901a\u8fc7\u8bed\u4e49\u7a7a\u95f4\u7684\u91c7\u6837\u548c\u6539\u5199\u6270\u52a8\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u540c\u65f6\u63d0\u51fa\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u5ea6\u91cf\u65b9\u6cd5\u3002", "motivation": "LLM\u5bf9\u8bed\u4e49\u76f8\u540c\u7684\u4e0d\u540c\u63d0\u793a\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u7b54\u6848\u5206\u5e03\uff0c\u8fd9\u8868\u660e\u6a21\u578b\u8f93\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u53ef\u80fd\u65e0\u6cd5\u53cd\u6620\u5176\u5bf9\u63d0\u793a\u542b\u4e49\u7684\u771f\u5b9e\u4e0d\u786e\u5b9a\u6027\u3002\u9700\u8981\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u5e76\u91cf\u5316\u63d0\u793a\u654f\u611f\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u63d0\u793a\u654f\u611f\u6027\u5efa\u6a21\u4e3a\u6cdb\u5316\u8bef\u5dee\uff0c\u901a\u8fc7\u8bed\u4e49\u6982\u5ff5\u7a7a\u95f4\u7684\u6539\u5199\u6270\u52a8\u8fdb\u884c\u91c7\u6837\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u5ea6\u91cf\u65b9\u6cd5\u6765\u5efa\u6a21\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u8fde\u7eed\u6027\u3002", "result": "\u8de8\u8bed\u4e49\u6982\u5ff5\u7a7a\u95f4\u7684\u91c7\u6837\u6539\u8fdb\u4e86\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\uff0c\u65b0\u7684\u5206\u89e3\u5ea6\u91cf\u80fd\u591f\u91cf\u5316LLM\u4e0d\u786e\u5b9a\u6027\u4e2d\u5f52\u56e0\u4e8e\u63d0\u793a\u654f\u611f\u6027\u7684\u90e8\u5206\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6539\u8fdb\u63d0\u793a\u654f\u611f\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u8bc1\u660e\u67d0\u4e9bLLM\u672a\u80fd\u5bf9\u5176\u8f93\u5165\u542b\u4e49\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.17173", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17173", "abs": "https://arxiv.org/abs/2510.17173", "authors": ["Melik Ozolcer", "Sang Won Bae"], "title": "Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users", "comment": "Accepted to the NeurIPS 2025 Workshop on Multi-Turn Interactions in\n  Large Language Models", "summary": "We study a web-deployed, tool-augmented LLM health coach with real users. In\na pilot with seven users (280 rated turns), offline policy evaluation (OPE)\nover factorized decision heads (Tool/Style) shows that a uniform heavy-tool\npolicy raises average value on logs but harms specific subgroups, most notably\nlow-health-literacy/high-self-efficacy users. A lightweight simulator with\nhidden archetypes further shows that adding a small early information-gain\nbonus reliably shortens trait identification and improves goal success and\npass@3. Together, these early findings indicate an evaluation-first path to\npersonalization: freeze the generator, learn subgroup-aware decision heads on\ntyped rewards (objective tool outcomes and satisfaction), and always report\nper-archetype metrics to surface subgroup harms that averages obscure.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u4e2a\u90e8\u7f72\u5728\u7f51\u4e0a\u7684\u3001\u5de5\u5177\u589e\u5f3a\u7684LLM\u5065\u5eb7\u6559\u7ec3\uff0c\u901a\u8fc7\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u53d1\u73b0\u7edf\u4e00\u7684\u91cd\u5de5\u5177\u7b56\u7565\u867d\u7136\u63d0\u9ad8\u5e73\u5747\u4ef7\u503c\uff0c\u4f46\u5bf9\u7279\u5b9a\u7528\u6237\u7fa4\u4f53\uff08\u7279\u522b\u662f\u5065\u5eb7\u7d20\u517b\u4f4e\u4f46\u81ea\u6211\u6548\u80fd\u9ad8\u7684\u7528\u6237\uff09\u6709\u5bb3\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8bc4\u4f30\u4f18\u5148\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e2a\u6027\u5316AI\u5065\u5eb7\u6559\u7ec3\uff0c\u907f\u514d\u7edf\u4e00\u7b56\u7565\u5bf9\u7279\u5b9a\u7528\u6237\u7fa4\u4f53\u7684\u6f5c\u5728\u4f24\u5bb3\u3002", "method": "\u4f7f\u7528\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\uff08OPE\uff09\u5206\u6790\u56e0\u5b50\u5316\u51b3\u7b56\u5934\uff08\u5de5\u5177/\u98ce\u683c\uff09\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u62df\u5668\u9a8c\u8bc1\u6dfb\u52a0\u65e9\u671f\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u7684\u6548\u679c\u3002", "result": "\u7edf\u4e00\u91cd\u5de5\u5177\u7b56\u7565\u4f1a\u635f\u5bb3\u7279\u5b9a\u7528\u6237\u7fa4\u4f53\uff1b\u6dfb\u52a0\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u80fd\u7f29\u77ed\u7279\u8d28\u8bc6\u522b\u65f6\u95f4\uff0c\u63d0\u9ad8\u76ee\u6807\u6210\u529f\u7387\u548cpass@3\u6307\u6807\u3002", "conclusion": "\u5efa\u8bae\u91c7\u7528\u8bc4\u4f30\u4f18\u5148\u7684\u4e2a\u6027\u5316\u8def\u5f84\uff1a\u51bb\u7ed3\u751f\u6210\u5668\uff0c\u5728\u7c7b\u578b\u5316\u5956\u52b1\u4e0a\u5b66\u4e60\u5b50\u7fa4\u4f53\u611f\u77e5\u7684\u51b3\u7b56\u5934\uff0c\u5e76\u59cb\u7ec8\u62a5\u544a\u6309\u539f\u578b\u5206\u7c7b\u7684\u6307\u6807\u4ee5\u63ed\u793a\u88ab\u5e73\u5747\u503c\u63a9\u76d6\u7684\u5b50\u7fa4\u4f53\u4f24\u5bb3\u3002", "topic": "agent analysis"}}
{"id": "2510.16097", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.HC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16097", "abs": "https://arxiv.org/abs/2510.16097", "authors": ["Eleni Straitouri", "Stratis Tsirtsis", "Ander Artola Velasco", "Manuel Gomez-Rodriguez"], "title": "Narrowing Action Choices with AI Improves Human Sequential Decisions", "comment": "Accepted at the Human-AI Complementarity for Decision Making Workshop\n  2025 by the NSF AI Institute for Societal Decision Making", "summary": "Recent work has shown that, in classification tasks, it is possible to design\ndecision support systems that do not require human experts to understand when\nto cede agency to a classifier or when to exercise their own agency to achieve\ncomplementarity$\\unicode{x2014}$experts using these systems make more accurate\npredictions than those made by the experts or the classifier alone. The key\nprinciple underpinning these systems reduces to adaptively controlling the\nlevel of human agency, by design. Can we use the same principle to achieve\ncomplementarity in sequential decision making tasks? In this paper, we answer\nthis question affirmatively. We develop a decision support system that uses a\npre-trained AI agent to narrow down the set of actions a human can take to a\nsubset, and then asks the human to take an action from this action set. Along\nthe way, we also introduce a bandit algorithm that leverages the smoothness\nproperties of the action sets provided by our system to efficiently optimize\nthe level of human agency. To evaluate our decision support system, we conduct\na large-scale human subject study ($n = 1{,}600$) where participants play a\nwildfire mitigation game. We find that participants who play the game supported\nby our system outperform those who play on their own by $\\sim$$30$% and the AI\nagent used by our system by $>$$2$%, even though the AI agent largely\noutperforms participants playing without support. We have made available the\ndata gathered in our human subject study as well as an open source\nimplementation of our system at\nhttps://github.com/Networks-Learning/narrowing-action-choices .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u901a\u8fc7\u9650\u5236\u4eba\u7c7b\u53ef\u91c7\u53d6\u7684\u884c\u52a8\u5b50\u96c6\u6765\u5b9e\u73b0\u4eba\u673a\u4e92\u8865\u6027\uff0c\u5728\u91ce\u706b\u7f13\u89e3\u6e38\u620f\u4e2d\u4f7f\u53c2\u4e0e\u8005\u8868\u73b0\u63d0\u534730%", "motivation": "\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u5df2\u8bc1\u660e\u901a\u8fc7\u63a7\u5236\u4eba\u7c7b\u4ee3\u7406\u6c34\u5e73\u53ef\u4ee5\u5b9e\u73b0\u4eba\u673a\u4e92\u8865\u6027\uff0c\u672c\u6587\u63a2\u7d22\u662f\u5426\u80fd\u5728\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u4e2d\u5e94\u7528\u76f8\u540c\u539f\u7406", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3AI\u4ee3\u7406\u7f29\u5c0f\u4eba\u7c7b\u53ef\u91c7\u53d6\u7684\u884c\u52a8\u96c6\u5408\uff0c\u7136\u540e\u8ba9\u4eba\u7c7b\u4ece\u8be5\u884c\u52a8\u96c6\u4e2d\u9009\u62e9\u884c\u52a8\uff0c\u5e76\u5f15\u5165\u5229\u7528\u884c\u52a8\u96c6\u5e73\u6ed1\u7279\u6027\u7684bandit\u7b97\u6cd5\u4f18\u5316\u4eba\u7c7b\u4ee3\u7406\u6c34\u5e73", "result": "\u57281600\u540d\u53c2\u4e0e\u8005\u7684\u91ce\u706b\u7f13\u89e3\u6e38\u620f\u7814\u7a76\u4e2d\uff0c\u4f7f\u7528\u8be5\u7cfb\u7edf\u7684\u53c2\u4e0e\u8005\u6bd4\u5355\u72ec\u53c2\u4e0e\u8005\u8868\u73b0\u63d0\u5347\u7ea630%\uff0c\u6bd4AI\u4ee3\u7406\u8868\u73b0\u63d0\u5347\u8d85\u8fc72%", "conclusion": "\u5728\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u4e2d\u901a\u8fc7\u63a7\u5236\u4eba\u7c7b\u4ee3\u7406\u6c34\u5e73\u53ef\u4ee5\u5b9e\u73b0\u4eba\u673a\u4e92\u8865\u6027\uff0c\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u80fd\u663e\u8457\u63d0\u5347\u4eba\u7c7b\u8868\u73b0", "topic": "agent analysis"}}
{"id": "2510.16123", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16123", "abs": "https://arxiv.org/abs/2510.16123", "authors": ["Federico Malato", "Ville Hautam\u00e4ki"], "title": "Zero-shot World Models via Search in Memory", "comment": "10 pages, 8 figures in main text + appendices", "summary": "World Models have vastly permeated the field of Reinforcement Learning. Their\nability to model the transition dynamics of an environment have greatly\nimproved sample efficiency in online RL. Among them, the most notorious example\nis Dreamer, a model that learns to act in a diverse set of image-based\nenvironments. In this paper, we leverage similarity search and stochastic\nrepresentations to approximate a world model without a training procedure. We\nestablish a comparison with PlaNet, a well-established world model of the\nDreamer family. We evaluate the models on the quality of latent reconstruction\nand on the perceived similarity of the reconstructed image, on both next-step\nand long horizon dynamics prediction. The results of our study demonstrate that\na search-based world model is comparable to a training based one in both cases.\nNotably, our model show stronger performance in long-horizon prediction with\nrespect to the baseline on a range of visually different environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u968f\u673a\u8868\u793a\u7684\u65e0\u8bad\u7ec3\u4e16\u754c\u6a21\u578b\uff0c\u4e0eDreamer\u5bb6\u65cf\u7684PlaNet\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u5728\u6f5c\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u957f\u671f\u52a8\u6001\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002", "motivation": "\u4e16\u754c\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u8fc7\u7a0b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u65e0\u9700\u8bad\u7ec3\u7684\u4e16\u754c\u6a21\u578b\u6784\u5efa\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "method": "\u5229\u7528\u76f8\u4f3c\u6027\u641c\u7d22\u548c\u968f\u673a\u8868\u793a\u6765\u8fd1\u4f3c\u4e16\u754c\u6a21\u578b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002\u4e0ePlaNet\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "\u57fa\u4e8e\u641c\u7d22\u7684\u4e16\u754c\u6a21\u578b\u5728\u6f5c\u5728\u91cd\u5efa\u548c\u56fe\u50cf\u76f8\u4f3c\u6027\u65b9\u9762\u4e0e\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u957f\u671f\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u65e0\u8bad\u7ec3\u7684\u4e16\u754c\u6a21\u578b\u662f\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u957f\u671f\u52a8\u6001\u9884\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17109", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17109", "abs": "https://arxiv.org/abs/2510.17109", "authors": ["Tianyang Xu", "Dan Zhang", "Kushan Mitra", "Estevam Hruschka"], "title": "Verification-Aware Planning for Multi-Agent Systems", "comment": "Submission for ARR Oct", "summary": "Large language model (LLM) agents are increasingly deployed to tackle complex\ntasks, often necessitating collaboration among multiple specialized agents.\nHowever, multi-agent collaboration introduces new challenges in planning,\ncoordination, and verification. Execution failures frequently arise not from\nflawed reasoning alone, but from subtle misalignments in task interpretation,\noutput format, or inter-agent handoffs. To address these challenges, we present\nVeriMAP, a framework for multi-agent collaboration with verification-aware\nplanning. The VeriMAP planner decomposes tasks, models subtask dependencies,\nand encodes planner-defined passing criteria as subtask verification functions\n(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,\ndemonstrating that it outperforms both single- and multi-agent baselines while\nenhancing system robustness and interpretability. Our analysis highlights how\nverification-aware planning enables reliable coordination and iterative\nrefinement in multi-agent systems, without relying on external labels or\nannotations.", "AI": {"tldr": "VeriMAP\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u9a8c\u8bc1\u611f\u77e5\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u3001\u5efa\u6a21\u5b50\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\u5e76\u7f16\u7801\u9a8c\u8bc1\u51fd\u6570\u6765\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9762\u4e34\u89c4\u5212\u3001\u534f\u8c03\u548c\u9a8c\u8bc1\u7684\u6311\u6218\uff0c\u6267\u884c\u5931\u8d25\u5f80\u5f80\u6e90\u4e8e\u4efb\u52a1\u89e3\u91ca\u3001\u8f93\u51fa\u683c\u5f0f\u6216\u667a\u80fd\u4f53\u95f4\u4ea4\u63a5\u7684\u7ec6\u5fae\u504f\u5dee\u3002", "method": "VeriMAP\u89c4\u5212\u5668\u5206\u89e3\u4efb\u52a1\uff0c\u5efa\u6a21\u5b50\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5c06\u89c4\u5212\u5668\u5b9a\u4e49\u7684\u901a\u8fc7\u6807\u51c6\u7f16\u7801\u4e3aPython\u548c\u81ea\u7136\u8bed\u8a00\u7684\u5b50\u4efb\u52a1\u9a8c\u8bc1\u51fd\u6570\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cVeriMAP\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u9a8c\u8bc1\u611f\u77e5\u89c4\u5212\u80fd\u591f\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u534f\u8c03\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u6807\u7b7e\u6216\u6ce8\u91ca\u3002", "topic": "agent analysis"}}
{"id": "2510.17418", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17418", "abs": "https://arxiv.org/abs/2510.17418", "authors": ["Mustafa F. Abdelwahed", "Alice Toniolo", "Joan Espasa", "Ian P. Gent"], "title": "Diverse Planning with Simulators via Linear Temporal Logic", "comment": null, "summary": "Autonomous agents rely on automated planning algorithms to achieve their\nobjectives. Simulation-based planning offers a significant advantage over\ndeclarative models in modelling complex environments. However, relying solely\non a planner that produces a single plan may not be practical, as the generated\nplans may not always satisfy the agent's preferences. To address this\nlimitation, we introduce $\\texttt{FBI}_\\texttt{LTL}$, a diverse planner\nexplicitly designed for simulation-based planning problems.\n$\\texttt{FBI}_\\texttt{LTL}$ utilises Linear Temporal Logic (LTL) to define\nsemantic diversity criteria, enabling agents to specify what constitutes\nmeaningfully different plans. By integrating these LTL-based diversity models\ndirectly into the search process, $\\texttt{FBI}_\\texttt{LTL}$ ensures the\ngeneration of semantically diverse plans, addressing a critical limitation of\nexisting diverse planning approaches that may produce syntactically different\nbut semantically identical solutions. Extensive evaluations on various\nbenchmarks consistently demonstrate that $\\texttt{FBI}_\\texttt{LTL}$ generates\nmore diverse plans compared to a baseline approach. This work establishes the\nfeasibility of semantically-guided diverse planning in simulation-based\nenvironments, paving the way for innovative approaches in realistic,\nnon-symbolic domains where traditional model-based approaches fail.", "AI": {"tldr": "\u63d0\u51faFBI_LTL\uff0c\u4e00\u79cd\u7528\u4e8e\u6a21\u62df\u89c4\u5212\u95ee\u9898\u7684\u591a\u6837\u5316\u89c4\u5212\u5668\uff0c\u4f7f\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u5b9a\u4e49\u8bed\u4e49\u591a\u6837\u6027\u6807\u51c6\uff0c\u751f\u6210\u8bed\u4e49\u591a\u6837\u5316\u7684\u8ba1\u5212\u3002", "motivation": "\u4f20\u7edf\u89c4\u5212\u5668\u53ea\u751f\u6210\u5355\u4e00\u8ba1\u5212\uff0c\u53ef\u80fd\u65e0\u6cd5\u6ee1\u8db3\u4ee3\u7406\u504f\u597d\u3002\u73b0\u6709\u591a\u6837\u5316\u89c4\u5212\u65b9\u6cd5\u53ef\u80fd\u4ea7\u751f\u8bed\u6cd5\u4e0d\u540c\u4f46\u8bed\u4e49\u76f8\u540c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FBI_LTL\u5229\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u5b9a\u4e49\u8bed\u4e49\u591a\u6837\u6027\u6807\u51c6\uff0c\u5e76\u5c06\u8fd9\u4e9bLTL\u591a\u6837\u6027\u6a21\u578b\u76f4\u63a5\u96c6\u6210\u5230\u641c\u7d22\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cFBI_LTL\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u8ba1\u5212\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u786e\u7acb\u4e86\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bed\u4e49\u5f15\u5bfc\u591a\u6837\u5316\u89c4\u5212\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5728\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u65b9\u6cd5\u5931\u6548\u7684\u73b0\u5b9e\u975e\u7b26\u53f7\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2510.17238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17238", "abs": "https://arxiv.org/abs/2510.17238", "authors": ["Junlong Tong", "Yingqi Fan", "Anhao Zhao", "Yunpu Ma", "Xiaoyu Shen"], "title": "StreamingThinker: Large Language Models Can Think While Reading", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}", "AI": {"tldr": "StreamingThinker\u63d0\u51fa\u4e86\u4e00\u79cd\u6d41\u5f0f\u601d\u8003\u8303\u5f0f\uff0c\u8ba9LLM\u5728\u9605\u8bfb\u8f93\u5165\u65f6\u5c31\u5f00\u59cb\u63a8\u7406\uff0c\u800c\u4e0d\u662f\u7b49\u5230\u6574\u4e2a\u8f93\u5165\u5b8c\u6210\u540e\u518d\u5f00\u59cb\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u8303\u5f0f\u9700\u8981\u7b49\u5f85\u6574\u4e2a\u8f93\u5165\u5b8c\u6210\u540e\u518d\u5f00\u59cb\u601d\u8003\uff0c\u8fd9\u5e26\u6765\u4e86\u4e0d\u5fc5\u8981\u7684\u5ef6\u8fdf\uff0c\u5e76\u4e14\u5728\u52a8\u6001\u573a\u666f\u4e0b\u4f1a\u51cf\u5f31\u5bf9\u65e9\u671f\u4fe1\u606f\u7684\u6ce8\u610f\u529b\u3002", "method": "\u901a\u8fc7\u6d41\u5f0fCoT\u751f\u6210\u3001\u6d41\u5f0f\u7ea6\u675f\u8bad\u7ec3\u548c\u6d41\u5f0f\u5e76\u884c\u63a8\u7406\uff0c\u5b9e\u73b0LLM\u5728\u9605\u8bfb\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u63a8\u7406\uff0c\u4f7f\u7528\u6d41\u5f0f\u63a8\u7406\u5355\u5143\u3001\u6d41\u5f0f\u6ce8\u610f\u529b\u63a9\u7801\u548c\u5e76\u884cKV\u7f13\u5b58\u7b49\u6280\u672f\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u903b\u8f91\u63a8\u7406\u548c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684QA\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cStreamingThinker\u4fdd\u6301\u4e86\u4e0e\u6279\u5904\u7406\u601d\u8003\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u5f00\u59cb\u524d\u7684token\u7b49\u5f85\u65f6\u95f4\u51cf\u5c11\u4e8680%\uff0c\u6700\u7ec8\u7b54\u6848\u751f\u6210\u7684\u65f6\u95f4\u5ef6\u8fdf\u51cf\u5c11\u4e8660%\u4ee5\u4e0a\u3002", "conclusion": "\u6d41\u5f0f\u601d\u8003\u8303\u5f0f\u80fd\u6709\u6548\u51cf\u5c11LLM\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u4e0b\u7684\u5b9e\u65f6\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.17598", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17598", "abs": "https://arxiv.org/abs/2510.17598", "authors": ["Amir Jalilifard", "Anderson de Rezende Rocha", "Marcos Medeiros Raimundo"], "title": "Reasoning Distillation and Structural Alignment for Improved Code Generation", "comment": null, "summary": "Effective code generation with language models hinges on two critical\nfactors: accurately understanding the intent of the prompt and generating code\nthat applies algorithmic reasoning to produce correct solutions capable of\npassing diverse test cases while adhering to the syntax of the target\nprogramming language. Unlike other language tasks, code generation requires\nmore than accurate token prediction; it demands comprehension of solution-level\nand structural relationships rather than merely generating the most likely\ntokens. very large language model (VLLM) are capable of generating detailed\nsteps toward the correct solution of complex tasks where reasoning is crucial\nin solving the problem. Such reasoning capabilities may be absent in smaller\nlanguage models. Therefore, in this work, we distill the reasoning capabilities\nof a VLLM into a smaller, more efficient model that is faster and cheaper to\ndeploy. Our approach trains the model to emulate the reasoning and\nproblem-solving abilities of the VLLM by learning to identify correct solution\npathways and establishing a structural correspondence between problem\ndefinitions and potential solutions through a novel method of structure-aware\nloss optimization. This enables the model to transcend token-level generation\nand to deeply grasp the overarching structure of solutions for given problems.\nExperimental results show that our fine-tuned model, developed through a cheap\nand simple to implement process, significantly outperforms our baseline model\nin terms of pass@1, average data flow, and average syntax match metrics across\nthe MBPP, MBPP Plus, and HumanEval benchmarks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u635f\u5931\u4f18\u5316\u6765\u5b66\u4e60\u6b63\u786e\u7684\u89e3\u51b3\u8def\u5f84\u548c\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u7684\u7ed3\u6784\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u4ee3\u7801\u751f\u6210\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\u7684\u6807\u8bb0\u9884\u6d4b\uff0c\u66f4\u9700\u8981\u7406\u89e3\u89e3\u51b3\u65b9\u6848\u7ea7\u522b\u7684\u7ed3\u6784\u5173\u7cfb\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u5907\u590d\u6742\u4efb\u52a1\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u80fd\u529b\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\u53ef\u80fd\u7f3a\u5931\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u4e2d\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u635f\u5931\u4f18\u5316\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u6a21\u62df\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5b66\u4e60\u8bc6\u522b\u6b63\u786e\u7684\u89e3\u51b3\u8def\u5f84\uff0c\u5e76\u5efa\u7acb\u95ee\u9898\u5b9a\u4e49\u4e0e\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u7684\u7ed3\u6784\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u5728MBPP\u3001MBPP Plus\u548cHumanEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728pass@1\u3001\u5e73\u5747\u6570\u636e\u6d41\u548c\u5e73\u5747\u8bed\u6cd5\u5339\u914d\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5ec9\u4ef7\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u8fc7\u7a0b\u5f00\u53d1\u7684\u5fae\u8c03\u6a21\u578b\u80fd\u591f\u8d85\u8d8a\u6807\u8bb0\u7ea7\u751f\u6210\uff0c\u6df1\u5165\u7406\u89e3\u7ed9\u5b9a\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u6574\u4f53\u7ed3\u6784\u3002", "topic": "code agent"}}
{"id": "2510.16175", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16175", "abs": "https://arxiv.org/abs/2510.16175", "authors": ["Pablo Samuel Castro"], "title": "The Formalism-Implementation Gap in Reinforcement Learning Research", "comment": null, "summary": "The last decade has seen an upswing in interest and adoption of reinforcement\nlearning (RL) techniques, in large part due to its demonstrated capabilities at\nperforming certain tasks at \"super-human levels\". This has incentivized the\ncommunity to prioritize research that demonstrates RL agent performance, often\nat the expense of research aimed at understanding their learning dynamics.\nPerformance-focused research runs the risk of overfitting on academic\nbenchmarks -- thereby rendering them less useful -- which can make it difficult\nto transfer proposed techniques to novel problems. Further, it implicitly\ndiminishes work that does not push the performance-frontier, but aims at\nimproving our understanding of these techniques. This paper argues two points:\n(i) RL research should stop focusing solely on demonstrating agent\ncapabilities, and focus more on advancing the science and understanding of\nreinforcement learning; and (ii) we need to be more precise on how our\nbenchmarks map to the underlying mathematical formalisms. We use the popular\nArcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a\nbenchmark that, despite being increasingly considered \"saturated\", can be\neffectively used for developing this understanding, and facilitating the\ndeployment of RL techniques in impactful real-world problems.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u5e94\u4ece\u5355\u7eaf\u5c55\u793a\u667a\u80fd\u4f53\u80fd\u529b\u8f6c\u5411\u66f4\u5173\u6ce8\u7406\u89e3\u5b66\u4e60\u52a8\u6001\u548c\u6570\u5b66\u57fa\u7840\uff0c\u5e76\u4ee5ALE\u4e3a\u4f8b\u8bf4\u660e\u73b0\u6709\u57fa\u51c6\u4ecd\u53ef\u7528\u4e8e\u53d1\u5c55\u8fd9\u79cd\u7406\u89e3\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u8fc7\u5ea6\u5173\u6ce8\u6027\u80fd\u8868\u73b0\u800c\u5ffd\u89c6\u4e86\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u7406\u89e3\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u65b9\u6cd5\u5728\u5b66\u672f\u57fa\u51c6\u4e0a\u8fc7\u62df\u5408\uff0c\u96be\u4ee5\u8fc1\u79fb\u5230\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790Arcade Learning Environment (ALE)\u57fa\u51c6\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u8bba\u8bc1\u5373\u4f7f\u88ab\u8ba4\u4e3a\u662f\"\u9971\u548c\"\u7684\u57fa\u51c6\u4ecd\u53ef\u7528\u4e8e\u53d1\u5c55\u5bf9\u5f3a\u5316\u5b66\u4e60\u7684\u79d1\u5b66\u7406\u89e3\u3002", "result": "\u63d0\u51fa\u4e86\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u5e94\u66f4\u6ce8\u91cd\u79d1\u5b66\u7406\u89e3\u548c\u6570\u5b66\u5f62\u5f0f\u5316\uff0c\u800c\u975e\u5355\u7eaf\u8ffd\u6c42\u6027\u80fd\u8868\u73b0\u7684\u89c2\u70b9\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u793e\u533a\u9700\u8981\u91cd\u65b0\u5e73\u8861\u7814\u7a76\u91cd\u70b9\uff0c\u4ece\u6027\u80fd\u5c55\u793a\u8f6c\u5411\u5bf9\u5b66\u4e60\u52a8\u6001\u548c\u6570\u5b66\u57fa\u7840\u7684\u7406\u89e3\uff0c\u4ee5\u4fc3\u8fdb\u6280\u672f\u5728\u5b9e\u9645\u95ee\u9898\u4e2d\u7684\u6709\u6548\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "2510.16185", "categories": ["cs.LG", "cs.AI", "cs.FL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.16185", "abs": "https://arxiv.org/abs/2510.16185", "authors": ["Daniel Donnelly", "Angelo Ferrando", "Francesco Belardinelli"], "title": "Expressive Reward Synthesis with the Runtime Monitoring Language", "comment": null, "summary": "A key challenge in reinforcement learning (RL) is reward (mis)specification,\nwhereby imprecisely defined reward functions can result in unintended, possibly\nharmful, behaviours. Indeed, reward functions in RL are typically treated as\nblack-box mappings from state-action pairs to scalar values. While effective in\nmany settings, this approach provides no information about why rewards are\ngiven, which can hinder learning and interpretability. Reward Machines address\nthis issue by representing reward functions as finite state automata, enabling\nthe specification of structured, non-Markovian reward functions. However, their\nexpressivity is typically bounded by regular languages, leaving them unable to\ncapture more complex behaviours such as counting or parametrised conditions. In\nthis work, we build on the Runtime Monitoring Language (RML) to develop a novel\nclass of language-based Reward Machines. By leveraging the built-in memory of\nRML, our approach can specify reward functions for non-regular, non-Markovian\ntasks. We demonstrate the expressiveness of our approach through experiments,\nhighlighting additional advantages in flexible event-handling and task\nspecification over existing Reward Machine-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u884c\u65f6\u76d1\u63a7\u8bed\u8a00\uff08RML\uff09\u7684\u65b0\u578b\u8bed\u8a00\u5316\u5956\u52b1\u673a\uff0c\u80fd\u591f\u8868\u8fbe\u975e\u6b63\u5219\u3001\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5956\u52b1\u673a\u8868\u8fbe\u80fd\u529b\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u51fd\u6570\u901a\u5e38\u88ab\u89c6\u4e3a\u9ed1\u76d2\u6620\u5c04\uff0c\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u800c\u4f20\u7edf\u5956\u52b1\u673a\u53ea\u80fd\u8868\u8fbe\u6b63\u5219\u8bed\u8a00\uff0c\u65e0\u6cd5\u5904\u7406\u8ba1\u6570\u6216\u53c2\u6570\u5316\u6761\u4ef6\u7b49\u590d\u6742\u884c\u4e3a\u3002", "method": "\u57fa\u4e8e\u8fd0\u884c\u65f6\u76d1\u63a7\u8bed\u8a00\uff08RML\uff09\u6784\u5efa\u65b0\u578b\u8bed\u8a00\u5316\u5956\u52b1\u673a\uff0c\u5229\u7528RML\u7684\u5185\u7f6e\u5185\u5b58\u673a\u5236\u6765\u6307\u5b9a\u975e\u6b63\u5219\u3001\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\uff0c\u5728\u7075\u6d3b\u4e8b\u4ef6\u5904\u7406\u548c\u4efb\u52a1\u89c4\u8303\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u5956\u52b1\u673a\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u5956\u52b1\u673a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u975e\u6b63\u5219\u3001\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5956\u52b1\u89c4\u8303\u5de5\u5177\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.16188", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16188", "abs": "https://arxiv.org/abs/2510.16188", "authors": ["Fateme Golivand Darvishvand", "Hikaru Shindo", "Sahil Sidheekh", "Kristian Kersting", "Sriraam Natarajan"], "title": "Human-Allied Relational Reinforcement Learning", "comment": "Proceedings of the Twelfth Annual Conference on Advances in Cognitive\n  Systems, ACS-2025 (143-159)", "summary": "Reinforcement learning (RL) has experienced a second wind in the past decade.\nWhile incredibly successful in images and videos, these systems still operate\nwithin the realm of propositional tasks ignoring the inherent structure that\nexists in the problem. Consequently, relational extensions (RRL) have been\ndeveloped for such structured problems that allow for effective generalization\nto arbitrary number of objects. However, they inherently make strong\nassumptions about the problem structure. We introduce a novel framework that\ncombines RRL with object-centric representation to handle both structured and\nunstructured data. We enhance learning by allowing the system to actively query\nthe human expert for guidance by explicitly modeling the uncertainty over the\npolicy. Our empirical evaluation demonstrates the effectiveness and efficiency\nof our proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5173\u7cfb\u5f3a\u5316\u5b66\u4e60\u4e0e\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u67e5\u8be2\u4eba\u7c7b\u4e13\u5bb6\u6765\u589e\u5f3a\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u5904\u7406\u7ed3\u6784\u5316\u95ee\u9898\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5173\u7cfb\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u5904\u7406\u7ed3\u6784\u5316\u95ee\u9898\u4f46\u5047\u8bbe\u8fc7\u5f3a\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5904\u7406\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u66f4\u901a\u7528\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u5173\u7cfb\u5f3a\u5316\u5b66\u4e60\u4e0e\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7b56\u7565\u4e0d\u786e\u5b9a\u6027\u6765\u4e3b\u52a8\u67e5\u8be2\u4eba\u7c7b\u4e13\u5bb6\u6307\u5bfc\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17697", "categories": ["cs.AI", "cs.LG", "cs.MA", "I.2.11; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.17697", "abs": "https://arxiv.org/abs/2510.17697", "authors": ["Anjie Liu", "Jianhong Wang", "Samuel Kaski", "Jun Wang", "Mengyue Yang"], "title": "A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Steering cooperative multi-agent reinforcement learning (MARL) towards\ndesired outcomes is challenging, particularly when the global guidance from a\nhuman on the whole multi-agent system is impractical in a large-scale MARL. On\nthe other hand, designing mechanisms to coordinate agents most relies on\nempirical studies, lacking a easy-to-use research tool. In this work, we employ\nmulti-agent influence diagrams (MAIDs) as a graphical framework to address the\nabove issues. First, we introduce interaction paradigms that leverage MAIDs to\nanalyze and visualize existing approaches in MARL. Then, we design a new\ninteraction paradigm based on MAIDs, referred to as targeted intervention that\nis applied to only a single targeted agent, so the problem of global guidance\ncan be mitigated. In our implementation, we introduce a causal inference\ntechnique-referred to as Pre-Strategy Intervention (PSI)-to realize the\ntargeted intervention paradigm. Since MAIDs can be regarded as a special class\nof causal diagrams, a composite desired outcome that integrates the primary\ntask goal and an additional desired outcome can be achieved by maximizing the\ncorresponding causal effect through the PSI. Moreover, the bundled relevance\ngraph analysis of MAIDs provides a tool to identify whether an MARL learning\nparadigm is workable under the design of an interaction paradigm. In\nexperiments, we demonstrate the effectiveness of our proposed targeted\nintervention, and verify the result of relevance graph analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u5f71\u54cd\u56fe(MAIDs)\u4f5c\u4e3a\u56fe\u5f62\u6846\u67b6\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u534f\u8c03\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8eMAIDs\u7684\u76ee\u6807\u5e72\u9884\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u6280\u672f\u5b9e\u73b0\u5355\u667a\u80fd\u4f53\u5e72\u9884\uff0c\u4ee5\u7f13\u89e3\u5168\u5c40\u6307\u5bfc\u7684\u56f0\u96be\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5bf9\u6574\u4e2a\u7cfb\u7edf\u8fdb\u884c\u5168\u5c40\u4eba\u5de5\u6307\u5bfc\u4e0d\u5207\u5b9e\u9645\uff0c\u800c\u73b0\u6709\u7684\u534f\u8c03\u673a\u5236\u8bbe\u8ba1\u4e3b\u8981\u4f9d\u8d56\u7ecf\u9a8c\u7814\u7a76\uff0c\u7f3a\u4e4f\u6613\u7528\u7684\u7814\u7a76\u5de5\u5177\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f71\u54cd\u56fe(MAIDs)\u4f5c\u4e3a\u56fe\u5f62\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u76ee\u6807\u5e72\u9884\u8303\u5f0f\uff0c\u5e76\u5f15\u5165\u9884\u7b56\u7565\u5e72\u9884(PSI)\u56e0\u679c\u63a8\u7406\u6280\u672f\u6765\u5b9e\u73b0\u8be5\u8303\u5f0f\uff0c\u901a\u8fc7\u6700\u5927\u5316\u56e0\u679c\u6548\u5e94\u6765\u8fbe\u6210\u590d\u5408\u671f\u671b\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u76ee\u6807\u5e72\u9884\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u76f8\u5173\u6027\u56fe\u5206\u6790\u7684\u7ed3\u679c\u3002", "conclusion": "MAIDs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\u6765\u5206\u6790\u548c\u53ef\u89c6\u5316MARL\u65b9\u6cd5\uff0c\u76ee\u6807\u5e72\u9884\u8303\u5f0f\u80fd\u591f\u7f13\u89e3\u5168\u5c40\u6307\u5bfc\u95ee\u9898\uff0cPSI\u6280\u672f\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u671f\u671b\u7ed3\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17705", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17705", "abs": "https://arxiv.org/abs/2510.17705", "authors": ["Dayan Pan", "Zhaoyang Fu", "Jingyuan Wang", "Xiao Han", "Yue Zhu", "Xiangyu Zhao"], "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models", "comment": "Accepted by CIKM' 25", "summary": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u8c03\u5236\uff08CAM\uff09\u7684\u65b0\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u8282LLM\u4e2d\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u7684\u8868\u5f81\u6765\u589e\u5f3a\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u7559\u901a\u7528\u77e5\u8bc6\u3002\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86\u6df7\u5408\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u8c03\u5236\uff08HyCAM\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u5171\u4eab\u7684\u5b8c\u6574\u53c2\u6570CAM\u6a21\u5757\u548c\u591a\u4e2a\u8f7b\u91cf\u7ea7\u4e13\u7528CAM\u6a21\u5757\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u591a\u4efb\u52a1\u9002\u5e94\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u9002\u5e94\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u548c\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u5728\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faCAM\u673a\u5236\u52a8\u6001\u8c03\u5236\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u8868\u5f81\uff0c\u5f00\u53d1HyCAM\u6846\u67b6\u7ed3\u5408\u5171\u4eabCAM\u6a21\u5757\u548c\u4e13\u7528CAM\u6a21\u5757\uff0c\u91c7\u7528\u52a8\u6001\u8def\u7531\u7b56\u7565\u8fdb\u884c\u81ea\u9002\u5e94\u77e5\u8bc6\u878d\u5408\u3002", "result": "\u5728\u95ee\u7b54\u3001\u4ee3\u7801\u751f\u6210\u548c\u903b\u8f91\u63a8\u7406\u7b49\u5f02\u6784\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53473.65%\u3002", "conclusion": "CAM\u548cHyCAM\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3LLM\u5728\u591a\u4efb\u52a1\u9002\u5e94\u4e2d\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u901a\u7528\u77e5\u8bc6\u7684\u540c\u65f6\u589e\u5f3a\u4efb\u52a1\u7279\u5b9a\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.16252", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16252", "abs": "https://arxiv.org/abs/2510.16252", "authors": ["Yuxuan Lu", "Jing Huang", "Hui Liu", "Jiri Gesi", "Yan Han", "Shihan Fu", "Tianqi Zheng", "Dakuo Wang"], "title": "WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale", "comment": null, "summary": "Training and evaluation of Reinforcement Learning (RL) web agents have gained\nincreasing attention, yet a scalable and efficient environment that couples\nrealistic and robust browser-side interaction with controllable server-side\nstate at scale is still missing. Existing environments tend to have one or more\nof the following issues: they overwhelm policy models with excessive and noisy\ncontext; they perform actions non-deterministically without waiting for the UI\nor network to stabilize; or they cannot scale isolated client-server containers\neffectively for parallel RL rollouts. We propose WEBSERV, an environment that\nincludes 1) a compact, site-agnostic browser environment that balances context\nand action complexity, and 2) a scalable RL environment via efficient launching\nand resetting web-servers to enable scalable RL training and evaluation. We\nevaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving\nstate-of-the-art single-prompt success rates while cutting launch latency by\n~5x and storage need by ~240x, with a comparable memory footprint, enabling\n200+ concurrent containers on a single host.", "AI": {"tldr": "\u63d0\u51fa\u4e86WEBSERV\u73af\u5883\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RL\u7f51\u7edc\u4ee3\u7406\u8bad\u7ec3\u73af\u5883\u5728\u53ef\u6269\u5c55\u6027\u3001\u4e0a\u4e0b\u6587\u566a\u58f0\u548c\u52a8\u4f5c\u786e\u5b9a\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5e76\u884c\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684RL\u7f51\u7edc\u4ee3\u7406\u8bad\u7ec3\u73af\u5883\u5b58\u5728\u4e0a\u4e0b\u6587\u566a\u58f0\u8fc7\u5927\u3001\u52a8\u4f5c\u6267\u884c\u4e0d\u7b49\u5f85UI/\u7f51\u7edc\u7a33\u5b9a\u3001\u65e0\u6cd5\u6709\u6548\u6269\u5c55\u5e76\u884c\u5bb9\u5668\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86WEBSERV\u73af\u5883\uff0c\u5305\u542b\uff1a1\uff09\u7d27\u51d1\u3001\u7ad9\u70b9\u65e0\u5173\u7684\u6d4f\u89c8\u5668\u73af\u5883\uff0c\u5e73\u8861\u4e0a\u4e0b\u6587\u548c\u52a8\u4f5c\u590d\u6742\u6027\uff1b2\uff09\u901a\u8fc7\u9ad8\u6548\u542f\u52a8\u548c\u91cd\u7f6eweb\u670d\u52a1\u5668\u5b9e\u73b0\u53ef\u6269\u5c55RL\u73af\u5883\u3002", "result": "\u5728WebArena\u7684\u8d2d\u7269CMS\u548cGitlab\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5355\u63d0\u793a\u6210\u529f\u7387\uff0c\u542f\u52a8\u5ef6\u8fdf\u964d\u4f4e\u7ea65\u500d\uff0c\u5b58\u50a8\u9700\u6c42\u51cf\u5c11\u7ea6240\u500d\uff0c\u5185\u5b58\u5360\u7528\u76f8\u5f53\uff0c\u652f\u6301\u5355\u4e3b\u673a200+\u5e76\u53d1\u5bb9\u5668\u3002", "conclusion": "WEBSERV\u73af\u5883\u6210\u529f\u89e3\u51b3\u4e86RL\u7f51\u7edc\u4ee3\u7406\u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u5e76\u884c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe benchmark"}}
{"id": "2510.17431", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17431", "abs": "https://arxiv.org/abs/2510.17431", "authors": ["Yushi Yang", "Shreyansh Padarha", "Andrew Lee", "Adam Mahdi"], "title": "Agentic Reinforcement Learning for Search is Unsafe", "comment": null, "summary": "Agentic reinforcement learning (RL) trains large language models to\nautonomously call tools during reasoning, with search as the most common\napplication. These models excel at multi-step reasoning tasks, but their safety\nproperties are not well understood. In this study, we show that RL-trained\nsearch models inherit refusal from instruction tuning and often deflect harmful\nrequests by turning them into safe queries. However, this safety is fragile.\nTwo simple attacks, one that forces the model to begin response with search\n(Search attack), another that encourages models to repeatedly search\n(Multi-search attack), trigger cascades of harmful searches and answers. Across\ntwo model families (Qwen, Llama) with both local and web search, these attacks\nlower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query\nsafety by 82.4%. The attacks succeed by triggering models to generate harmful,\nrequest-mirroring search queries before they can generate the inherited refusal\ntokens. This exposes a core weakness of current RL training: it rewards\ncontinued generation of effective queries without accounting for their\nharmfulness. As a result, RL search models have vulnerabilities that users can\neasily exploit, making it urgent to develop safety-aware agentic RL pipelines\noptimising for safe search.", "AI": {"tldr": "RL\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u8c03\u7528\u5de5\u5177\u8fdb\u884c\u63a8\u7406\u65f6\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e24\u79cd\u7b80\u5355\u653b\u51fb\uff08\u641c\u7d22\u653b\u51fb\u548c\u591a\u641c\u7d22\u653b\u51fb\uff09\u53ef\u663e\u8457\u964d\u4f4e\u6a21\u578b\u7684\u5b89\u5168\u62d2\u7edd\u7387\uff0c\u66b4\u9732\u5f53\u524dRL\u8bad\u7ec3\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u65b9\u9762\u7684\u6838\u5fc3\u5f31\u70b9\u3002", "motivation": "\u7814\u7a76RL\u8bad\u7ec3\u7684\u641c\u7d22\u6a21\u578b\u7684\u5b89\u5168\u7279\u6027\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u4f46\u5b89\u5168\u6027\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u7b80\u5355\u653b\u51fb\u65b9\u6cd5\uff1a\u5f3a\u5236\u6a21\u578b\u4ee5\u641c\u7d22\u5f00\u59cb\u54cd\u5e94\uff08\u641c\u7d22\u653b\u51fb\uff09\u548c\u9f13\u52b1\u6a21\u578b\u91cd\u590d\u641c\u7d22\uff08\u591a\u641c\u7d22\u653b\u51fb\uff09\uff0c\u5728\u4e24\u4e2a\u6a21\u578b\u5bb6\u65cf\uff08Qwen\u3001Llama\uff09\u4e0a\u6d4b\u8bd5\u672c\u5730\u548c\u7f51\u7edc\u641c\u7d22\u529f\u80fd\u3002", "result": "\u653b\u51fb\u4f7f\u62d2\u7edd\u7387\u964d\u4f4e\u9ad8\u8fbe60.0%\uff0c\u7b54\u6848\u5b89\u5168\u6027\u964d\u4f4e82.5%\uff0c\u641c\u7d22\u67e5\u8be2\u5b89\u5168\u6027\u964d\u4f4e82.4%\u3002\u653b\u51fb\u901a\u8fc7\u89e6\u53d1\u6a21\u578b\u5728\u751f\u6210\u62d2\u7edd\u4ee4\u724c\u524d\u751f\u6210\u6709\u5bb3\u7684\u955c\u50cf\u641c\u7d22\u67e5\u8be2\u800c\u6210\u529f\u3002", "conclusion": "\u5f53\u524dRL\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u6838\u5fc3\u5f31\u70b9\uff1a\u5956\u52b1\u6709\u6548\u67e5\u8be2\u7684\u6301\u7eed\u751f\u6210\u800c\u4e0d\u8003\u8651\u5176\u6709\u5bb3\u6027\uff0c\u5bfc\u81f4RL\u641c\u7d22\u6a21\u578b\u5b58\u5728\u7528\u6237\u53ef\u8f7b\u6613\u5229\u7528\u7684\u6f0f\u6d1e\uff0c\u8feb\u5207\u9700\u8981\u5f00\u53d1\u5b89\u5168\u611f\u77e5\u7684\u667a\u80fdRL\u7ba1\u9053\u6765\u4f18\u5316\u5b89\u5168\u641c\u7d22\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17491", "abs": "https://arxiv.org/abs/2510.17491", "authors": ["Yihong Tang", "Kehai Chen", "Liang Yue", "Jinxin Fan", "Caishen Zhou", "Xiaoguang Li", "Yuyang Zhang", "Mingming Zhao", "Shixiong Kai", "Kaiyang Guo", "Xingshan Zeng", "Wenjing Cun", "Lifeng Shang", "Min Zhang"], "title": "Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents", "comment": null, "summary": "With the rise of large language models (LLMs), LLM agents capable of\nautonomous reasoning, planning, and executing complex tasks have become a\nfrontier in artificial intelligence. However, how to translate the research on\ngeneral agents into productivity that drives industry transformations remains a\nsignificant challenge. To address this, this paper systematically reviews the\ntechnologies, applications, and evaluation methods of industry agents based on\nLLMs. Using an industry agent capability maturity framework, it outlines the\nevolution of agents in industry applications, from \"process execution systems\"\nto \"adaptive social systems.\" First, we examine the three key technological\npillars that support the advancement of agent capabilities: Memory, Planning,\nand Tool Use. We discuss how these technologies evolve from supporting simple\ntasks in their early forms to enabling complex autonomous systems and\ncollective intelligence in more advanced forms. Then, we provide an overview of\nthe application of industry agents in real-world domains such as digital\nengineering, scientific discovery, embodied intelligence, collaborative\nbusiness execution, and complex system simulation. Additionally, this paper\nreviews the evaluation benchmarks and methods for both fundamental and\nspecialized capabilities, identifying the challenges existing evaluation\nsystems face regarding authenticity, safety, and industry specificity. Finally,\nwe focus on the practical challenges faced by industry agents, exploring their\ncapability boundaries, developmental potential, and governance issues in\nvarious scenarios, while providing insights into future directions. By\ncombining technological evolution with industry practices, this review aims to\nclarify the current state and offer a clear roadmap and theoretical foundation\nfor understanding and building the next generation of industry agents.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e1a\u667a\u80fd\u4f53\u6280\u672f\u3001\u5e94\u7528\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u884c\u4e1a\u667a\u80fd\u4f53\u80fd\u529b\u6210\u719f\u5ea6\u6846\u67b6\uff0c\u5206\u6790\u4e86\u8bb0\u5fc6\u3001\u89c4\u5212\u548c\u5de5\u5177\u4f7f\u7528\u4e09\u5927\u6280\u672f\u652f\u67f1\u7684\u6f14\u8fdb\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u80fd\u591f\u81ea\u4e3b\u63a8\u7406\u3001\u89c4\u5212\u548c\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u5df2\u6210\u4e3a\u4eba\u5de5\u667a\u80fd\u524d\u6cbf\uff0c\u4f46\u5982\u4f55\u5c06\u901a\u7528\u667a\u80fd\u4f53\u7814\u7a76\u8f6c\u5316\u4e3a\u63a8\u52a8\u884c\u4e1a\u8f6c\u578b\u7684\u751f\u4ea7\u529b\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u91c7\u7528\u884c\u4e1a\u667a\u80fd\u4f53\u80fd\u529b\u6210\u719f\u5ea6\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u667a\u80fd\u4f53\u5728\u884c\u4e1a\u5e94\u7528\u4e2d\u7684\u6f14\u8fdb\u8def\u5f84\uff0c\u91cd\u70b9\u8003\u5bdf\u8bb0\u5fc6\u3001\u89c4\u5212\u548c\u5de5\u5177\u4f7f\u7528\u4e09\u5927\u6280\u672f\u652f\u67f1\u7684\u53d1\u5c55\uff0c\u5e76\u7efc\u8ff0\u5404\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\u3002", "result": "\u6784\u5efa\u4e86\u4ece\"\u6d41\u7a0b\u6267\u884c\u7cfb\u7edf\"\u5230\"\u81ea\u9002\u5e94\u793e\u4f1a\u7cfb\u7edf\"\u7684\u667a\u80fd\u4f53\u6f14\u8fdb\u6a21\u578b\uff0c\u8bc6\u522b\u4e86\u73b0\u6709\u8bc4\u4f30\u7cfb\u7edf\u5728\u771f\u5b9e\u6027\u3001\u5b89\u5168\u6027\u548c\u884c\u4e1a\u7279\u6027\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6280\u672f\u6f14\u8fdb\u4e0e\u884c\u4e1a\u5b9e\u8df5\uff0c\u4e3a\u7406\u89e3\u548c\u6784\u5efa\u4e0b\u4e00\u4ee3\u884c\u4e1a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6e05\u6670\u8def\u7ebf\u56fe\u548c\u7406\u8bba\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.16552", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16552", "abs": "https://arxiv.org/abs/2510.16552", "authors": ["Ang Li", "Yifei Wang", "Zhihang Yuan", "Stefanie Jegelka", "Yisen Wang"], "title": "LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs", "comment": null, "summary": "Reinforcement learning in large language models (LLMs) often relies on scalar\nrewards, a practice that discards valuable textual rationale buried in the\nrollouts, forcing the model to explore \\textit{de novo} with each attempt and\nhindering sample efficiency. While LLMs can uniquely learn from language\nfeedback provided in-context, naively integrating on-line experiences into RL\ntraining presents a paradox: feedback from the same problem risks information\nleakage and memorization, while feedback from different problems often leads to\nbehavior collapse due to irrelevant context. To resolve this tension, we\npropose \\textbf{Language-And-Numerical Policy Optimization (LANPO)}, a\nframework that cleanly separates the roles of feedback: language guides\nexploration, while numerical rewards drive optimization. LANPO builds a dynamic\nexperience pool from past trials and introduces two principles to ensure\nfeedback is effective: \\emph{Reward-Agnostic Reflection} for safe intra-sample\nself-correction and \\emph{Relevant Abstraction} to distill generalizable\nlessons from inter-sample experiences. Across mathematical reasoning\nbenchmarks, LANPO enables 7B and 14B models to significantly outperform strong\nbaselines trained with GRPO in test accuracy. Our work provides a robust method\nfor integrating historical experiences into the LLM RL loop, creating more\neffective and data-efficient learning agents.", "AI": {"tldr": "LANPO\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u8bed\u8a00\u53cd\u9988\u548c\u6570\u503c\u5956\u52b1\u7684\u89d2\u8272\u6765\u89e3\u51b3LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u8bed\u8a00\u6307\u5bfc\u63a2\u7d22\uff0c\u6570\u503c\u5956\u52b1\u9a71\u52a8\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u6807\u91cf\u5956\u52b1\uff0c\u4e22\u5f03\u4e86rollout\u4e2d\u7684\u6587\u672c\u63a8\u7406\u4fe1\u606f\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u3002\u5728\u7ebf\u6574\u5408\u8bed\u8a00\u53cd\u9988\u5b58\u5728\u4fe1\u606f\u6cc4\u9732\u548c\u65e0\u5173\u4e0a\u4e0b\u6587\u5bfc\u81f4\u884c\u4e3a\u5d29\u6e83\u7684\u6096\u8bba\u3002", "method": "\u63d0\u51faLANPO\u6846\u67b6\uff0c\u6784\u5efa\u52a8\u6001\u7ecf\u9a8c\u6c60\uff0c\u91c7\u7528\u5956\u52b1\u65e0\u5173\u53cd\u601d\u8fdb\u884c\u6837\u672c\u5185\u81ea\u6821\u6b63\uff0c\u901a\u8fc7\u76f8\u5173\u62bd\u8c61\u4ece\u6837\u672c\u95f4\u7ecf\u9a8c\u4e2d\u63d0\u53d6\u53ef\u6cdb\u5316\u6559\u8bad\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7B\u548c14B\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4f7f\u7528GRPO\u8bad\u7ec3\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "LANPO\u4e3a\u5c06\u5386\u53f2\u7ecf\u9a8c\u6574\u5408\u5230LLM\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u4e2d\u63d0\u4f9b\u4e86\u7a33\u5065\u65b9\u6cd5\uff0c\u521b\u9020\u4e86\u66f4\u6709\u6548\u548c\u6570\u636e\u9ad8\u6548\u7684\u5b66\u4e60\u667a\u80fd\u4f53\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17733", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17733", "abs": "https://arxiv.org/abs/2510.17733", "authors": ["Tong Chen", "Akari Asai", "Luke Zettlemoyer", "Hannaneh Hajishirzi", "Faeze Brahman"], "title": "Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations", "comment": null, "summary": "Language models often generate factually incorrect information unsupported by\ntheir training data, a phenomenon known as extrinsic hallucination. Existing\nmitigation approaches often degrade performance on open-ended generation and\ndownstream tasks, limiting their practical utility. We propose an online\nreinforcement learning method using a novel binary retrieval-augmented reward\n(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach\nassigns a reward of one only when the model's output is entirely factually\ncorrect, and zero otherwise. We evaluate our method on Qwen3 reasoning models\nacross diverse tasks. For open-ended generation, binary RAR achieves a 39.3%\nreduction in hallucination rates, substantially outperforming both supervised\ntraining and continuous-reward RL baselines. In short-form question answering,\nthe model learns calibrated abstention, strategically outputting \"I don't know\"\nwhen faced with insufficient parametric knowledge. This yields 44.4% and 21.7%\nfewer incorrect answers on PopQA and GPQA, respectively. Crucially, these\nfactuality gains come without performance degradation on instruction following,\nmath, or code, whereas continuous-reward RL, despite improving factuality,\ninduces quality regressions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u4e8c\u5143\u68c0\u7d22\u589e\u5f3a\u5956\u52b1\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u5728\u6a21\u578b\u8f93\u51fa\u5b8c\u5168\u6b63\u786e\u65f6\u7ed9\u4e88\u5956\u52b1\u6765\u51cf\u5c11\u5916\u5728\u5e7b\u89c9\uff0c\u5728\u4fdd\u6301\u5176\u4ed6\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5e7b\u89c9\u7387\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u8bad\u7ec3\u6570\u636e\u4e0d\u652f\u6301\u7684\u4e8b\u5b9e\u9519\u8bef\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u5728\u5f00\u653e\u751f\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u4e8c\u5143\u68c0\u7d22\u589e\u5f3a\u5956\u52b1\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u4ec5\u5728\u6a21\u578b\u8f93\u51fa\u5b8c\u5168\u6b63\u786e\u65f6\u7ed9\u4e88\u5956\u52b11\uff0c\u5426\u5219\u4e3a0\u3002\u5728Qwen3\u63a8\u7406\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u5f00\u653e\u751f\u6210\u4e2d\u5e7b\u89c9\u7387\u964d\u4f4e39.3%\uff1b\u5728\u77ed\u95ee\u7b54\u4e2d\u5b66\u4f1a\u6821\u51c6\u5f03\u6743\uff0c\u5728PopQA\u548cGPQA\u4e0a\u5206\u522b\u51cf\u5c1144.4%\u548c21.7%\u7684\u9519\u8bef\u7b54\u6848\uff1b\u4e14\u4e0d\u635f\u5bb3\u6307\u4ee4\u9075\u5faa\u3001\u6570\u5b66\u6216\u4ee3\u7801\u80fd\u529b\u3002", "conclusion": "\u4e8c\u5143\u5956\u52b1\u65b9\u6cd5\u5728\u63d0\u9ad8\u4e8b\u5b9e\u6027\u7684\u540c\u65f6\u907f\u514d\u4e86\u8fde\u7eed\u5956\u52b1RL\u5e26\u6765\u7684\u8d28\u91cf\u56de\u5f52\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5b9e\u7528\u6548\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17793", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17793", "abs": "https://arxiv.org/abs/2510.17793", "authors": ["Austin Xu", "Xuan-Phi Nguyen", "Yilun Zhou", "Chien-Sheng Wu", "Caiming Xiong", "Shafiq Joty"], "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains", "comment": "29 pages, 9 tables, 6 figures", "summary": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86FARE\uff08\u57fa\u7840\u81ea\u52a8\u63a8\u7406\u8bc4\u4f30\u5668\uff09\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u7684SFT\u65b9\u6cd5\u8bad\u7ec3\u8bc4\u4f30\u5668\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u66f4\u5927\u89c4\u6a21\u7684\u4e13\u95e8RL\u8bad\u7ec3\u8bc4\u4f30\u5668\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u8bc4\u4f30\u5668\u8bad\u7ec3\u4e3b\u8981\u5173\u6ce8\u65b0\u65b9\u6cd5\u5982\u5f3a\u5316\u5b66\u4e60\uff0c\u800c\u5ffd\u89c6\u4e86\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u5f00\u53d1\u3002\u672c\u6587\u4e13\u6ce8\u4e8e\u6570\u636e\u6269\u5c55\uff0c\u89e3\u51b3\u8bc4\u4f30\u5668\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "method": "\u6536\u96c6250\u4e07\u6837\u672c\u8986\u76d65\u79cd\u8bc4\u4f30\u4efb\u52a1\uff0c\u4f7f\u7528\u7b80\u5355\u7684\u8fed\u4ee3\u62d2\u7edd\u91c7\u6837\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u8bad\u7ec38B\u548c20B\u53c2\u6570\u7684FARE\u8bc4\u4f30\u5668\u3002", "result": "FARE-8B\u6311\u6218\u4e86\u66f4\u5927\u7684\u4e13\u95e8RL\u8bad\u7ec3\u8bc4\u4f30\u5668\uff0cFARE-20B\u5728\u5f00\u6e90\u8bc4\u4f30\u5668\u4e2d\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u8d85\u8d8a\u4e86\u4e13\u95e8\u768470B+\u8bc4\u4f30\u5668\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cFARE-20B\u5728MATH\u4e0a\u8fbe\u5230\u63a5\u8fd1oracle\u6027\u80fd\uff0c\u5728RL\u8bad\u7ec3\u4e2d\u6bd4\u5b57\u7b26\u4e32\u5339\u914d\u9a8c\u8bc1\u5668\u63d0\u534714.1%\u6027\u80fd\u3002", "conclusion": "\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u7684SFT\u65b9\u6cd5\u80fd\u591f\u8bad\u7ec3\u51fa\u9ad8\u8d28\u91cf\u7684\u8bc4\u4f30\u5668\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8d8a\u66f4\u590d\u6742\u7684RL\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u89c4\u6a21\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.17797", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17797", "abs": "https://arxiv.org/abs/2510.17797", "authors": ["Akshara Prabhakar", "Roshan Ram", "Zixiang Chen", "Silvio Savarese", "Frank Wang", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics", "comment": "Technical report; 13 pages plus references and appendices", "summary": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4f01\u4e1a\u6df1\u5ea6\u7814\u7a76(EDR)\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e3b\u89c4\u5212\u667a\u80fd\u4f53\u3001\u56db\u4e2a\u4e13\u4e1a\u641c\u7d22\u667a\u80fd\u4f53\u3001\u53ef\u6269\u5c55\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u3001\u53ef\u89c6\u5316\u667a\u80fd\u4f53\u548c\u53cd\u601d\u673a\u5236\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u62a5\u544a\u751f\u6210\u548c\u5b9e\u65f6\u6d41\u5904\u7406\uff0c\u5728\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u5728\u5c06\u975e\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u6d1e\u5bdf\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u73b0\u6709\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u9886\u57df\u7279\u5b9a\u7ec6\u5fae\u5dee\u522b\u3001\u610f\u56fe\u5bf9\u9f50\u548c\u4f01\u4e1a\u96c6\u6210\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5305\u62ec\u4e3b\u89c4\u5212\u667a\u80fd\u4f53\u8fdb\u884c\u81ea\u9002\u5e94\u67e5\u8be2\u5206\u89e3\u3001\u56db\u4e2a\u4e13\u4e1a\u641c\u7d22\u667a\u80fd\u4f53(\u901a\u7528\u3001\u5b66\u672f\u3001GitHub\u3001LinkedIn)\u3001\u57fa\u4e8eMCP\u7684\u53ef\u6269\u5c55\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u3001\u53ef\u89c6\u5316\u667a\u80fd\u4f53\u548c\u68c0\u6d4b\u77e5\u8bc6\u5dee\u8ddd\u7684\u53cd\u601d\u673a\u5236\u3002", "result": "\u5728DeepResearch Bench\u548cDeepConsult\u7b49\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEDR\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5e76\u5728\u5185\u90e8\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u4f01\u4e1a\u90e8\u7f72\u80fd\u529b\u3002", "conclusion": "EDR\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u63a8\u7406\u5e94\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f01\u4e1a\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\uff0c\u53d1\u5e03\u4e86\u6846\u67b6\u548c\u57fa\u51c6\u8f68\u8ff9\u4ee5\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2510.16774", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16774", "abs": "https://arxiv.org/abs/2510.16774", "authors": ["Yuguang Yue", "Irakli Salia", "Samuel Hunt", "Christopher Green", "Wenzhe Shi", "Jonathan J Hunt"], "title": "Learning to play: A Multimodal Agent for 3D Game-Play", "comment": "International Conference on Computer Vision Workshop on Multi-Modal\n  Reasoning for Agentic Intelligence", "summary": "We argue that 3-D first-person video games are a challenging environment for\nreal-time multi-modal reasoning. We first describe our dataset of human\ngame-play, collected across a large variety of 3-D first-person games, which is\nboth substantially larger and more diverse compared to prior publicly disclosed\ndatasets, and contains text instructions. We demonstrate that we can learn an\ninverse dynamics model from this dataset, which allows us to impute actions on\na much larger dataset of publicly available videos of human game play that lack\nrecorded actions. We then train a text-conditioned agent for game playing using\nbehavior cloning, with a custom architecture capable of realtime inference on a\nconsumer GPU. We show the resulting model is capable of playing a variety of\n3-D games and responding to text input. Finally, we outline some of the\nremaining challenges such as long-horizon tasks and quantitative evaluation\nacross a large set of games.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e3D\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6e38\u620f\u7684\u591a\u6a21\u6001\u63a8\u7406\u73af\u5883\uff0c\u901a\u8fc7\u6536\u96c6\u5927\u89c4\u6a21\u591a\u6837\u5316\u7684\u4eba\u7c7b\u6e38\u620f\u6570\u636e\uff0c\u8bad\u7ec3\u6587\u672c\u6761\u4ef6\u667a\u80fd\u4f53\u8fdb\u884c\u6e38\u620f\u64cd\u4f5c\u3002", "motivation": "3D\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u6e38\u620f\u4e3a\u5b9e\u65f6\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\uff0c\u9700\u8981\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u8de8\u6e38\u620f\u5b9a\u91cf\u8bc4\u4f30\u7b49\u95ee\u9898\u3002", "method": "\u6536\u96c6\u5927\u89c4\u6a21\u591a\u6837\u5316\u7684\u4eba\u7c7b\u6e38\u620f\u6570\u636e\uff0c\u5b66\u4e60\u9006\u52a8\u529b\u5b66\u6a21\u578b\u6765\u63a8\u65ad\u7f3a\u5931\u52a8\u4f5c\uff0c\u4f7f\u7528\u884c\u4e3a\u514b\u9686\u8bad\u7ec3\u6587\u672c\u6761\u4ef6\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u652f\u6301\u5b9e\u65f6\u63a8\u7406\u7684\u81ea\u5b9a\u4e49\u67b6\u6784\u3002", "result": "\u8bad\u7ec3\u51fa\u7684\u6a21\u578b\u80fd\u591f\u5728\u591a\u79cd3D\u6e38\u620f\u4e2d\u6267\u884c\u64cd\u4f5c\u5e76\u54cd\u5e94\u6587\u672c\u8f93\u5165\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57283D\u6e38\u620f\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u8de8\u6e38\u620f\u5b9a\u91cf\u8bc4\u4f30\u7b49\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2510.16811", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16811", "abs": "https://arxiv.org/abs/2510.16811", "authors": ["Mohammad Shahverdikondori", "Jalal Etesami", "Negar Kiyavash"], "title": "Graph Learning is Suboptimal in Causal Bandits", "comment": "31 pages, 5 figures", "summary": "We study regret minimization in causal bandits under causal sufficiency where\nthe underlying causal structure is not known to the agent. Previous work has\nfocused on identifying the reward's parents and then applying classic bandit\nmethods to them, or jointly learning the parents while minimizing regret. We\ninvestigate whether such strategies are optimal. Somewhat counterintuitively,\nour results show that learning the parent set is suboptimal. We do so by\nproving that there exist instances where regret minimization and parent\nidentification are fundamentally conflicting objectives. We further analyze\nboth the known and unknown parent set size regimes, establish novel regret\nlower bounds that capture the combinatorial structure of the action space.\nBuilding on these insights, we propose nearly optimal algorithms that bypass\ngraph and parent recovery, demonstrating that parent identification is indeed\nunnecessary for regret minimization. Experiments confirm that there exists a\nlarge performance gap between our method and existing baselines in various\nenvironments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u56e0\u679c\u5f3a\u76d7\u95ee\u9898\u4e2d\u7684\u9057\u61be\u6700\u5c0f\u5316\uff0c\u53d1\u73b0\u5728\u672a\u77e5\u56e0\u679c\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u5b66\u4e60\u7236\u8282\u70b9\u96c6\u53cd\u800c\u662f\u6b21\u4f18\u7684\u3002\u4f5c\u8005\u8bc1\u660e\u4e86\u9057\u61be\u6700\u5c0f\u5316\u548c\u7236\u8282\u70b9\u8bc6\u522b\u5b58\u5728\u6839\u672c\u51b2\u7a81\uff0c\u5e76\u63d0\u51fa\u7ed5\u8fc7\u56fe\u6062\u590d\u7684\u6700\u4f18\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4e13\u6ce8\u4e8e\u8bc6\u522b\u5956\u52b1\u7684\u7236\u8282\u70b9\u7136\u540e\u5e94\u7528\u7ecf\u5178\u5f3a\u76d7\u65b9\u6cd5\uff0c\u8981\u4e48\u5728\u6700\u5c0f\u5316\u9057\u61be\u7684\u540c\u65f6\u8054\u5408\u5b66\u4e60\u7236\u8282\u70b9\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u8fd9\u4e9b\u7b56\u7565\u662f\u5426\u6700\u4f18\uff0c\u5e76\u63a2\u7d22\u662f\u5426\u5b58\u5728\u66f4\u6709\u6548\u7684\u9057\u61be\u6700\u5c0f\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u7236\u8282\u70b9\u8bc6\u522b\u4e0e\u9057\u61be\u6700\u5c0f\u5316\u5b58\u5728\u51b2\u7a81\uff0c\u5efa\u7acb\u5305\u542b\u52a8\u4f5c\u7a7a\u95f4\u7ec4\u5408\u7ed3\u6784\u7684\u9057\u61be\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u7ed5\u8fc7\u56fe\u6062\u590d\u548c\u7236\u8282\u70b9\u6062\u590d\u7684\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u65b0\u65b9\u6cd5\u5728\u5404\u79cd\u73af\u5883\u4e2d\u4e0e\u73b0\u6709\u57fa\u7ebf\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u8bc1\u660e\u4e86\u7ed5\u8fc7\u7236\u8282\u70b9\u8bc6\u522b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7236\u8282\u70b9\u8bc6\u522b\u5bf9\u4e8e\u9057\u61be\u6700\u5c0f\u5316\u662f\u4e0d\u5fc5\u8981\u7684\uff0c\u751a\u81f3\u53ef\u80fd\u6709\u5bb3\u3002\u63d0\u51fa\u7684\u65b0\u7b97\u6cd5\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.16968", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.16968", "abs": "https://arxiv.org/abs/2510.16968", "authors": ["Pingzhi Li", "Morris Yu-Chao Huang", "Zhen Tan", "Qingquan Song", "Jie Peng", "Kai Zou", "Yu Cheng", "Kaidi Xu", "Tianlong Chen"], "title": "Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures", "comment": "Code is at https://github.com/unites-lab/shadow-moe", "summary": "Knowledge Distillation (KD) accelerates training of large language models\n(LLMs) but poses intellectual property protection and LLM diversity risks.\nExisting KD detection methods based on self-identity or output similarity can\nbe easily evaded through prompt engineering. We present a KD detection\nframework effective in both white-box and black-box settings by exploiting an\noverlooked signal: the transfer of MoE \"structural habits\", especially internal\nrouting patterns. Our approach analyzes how different experts specialize and\ncollaborate across various inputs, creating distinctive fingerprints that\npersist through the distillation process. To extend beyond the white-box setup\nand MoE architectures, we further propose Shadow-MoE, a black-box method that\nconstructs proxy MoE representations via auxiliary distillation to compare\nthese patterns between arbitrary model pairs. We establish a comprehensive,\nreproducible benchmark that offers diverse distilled checkpoints and an\nextensible framework to facilitate future research. Extensive experiments\ndemonstrate >94% detection accuracy across various scenarios and strong\nrobustness to prompt-based evasion, outperforming existing baselines while\nhighlighting the structural habits transfer in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMoE\u7ed3\u6784\u4e60\u60ef\u7684\u77e5\u8bc6\u84b8\u998f\u68c0\u6d4b\u6846\u67b6\uff0c\u5728\u767d\u76d2\u548c\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u90fd\u80fd\u6709\u6548\u68c0\u6d4bLLM\u662f\u5426\u7ecf\u8fc7\u77e5\u8bc6\u84b8\u998f\uff0c\u51c6\u786e\u7387\u8d85\u8fc794%\u4e14\u5bf9\u63d0\u793a\u5de5\u7a0b\u653b\u51fb\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u8eab\u4efd\u6216\u8f93\u51fa\u76f8\u4f3c\u6027\u7684KD\u68c0\u6d4b\u65b9\u6cd5\u5bb9\u6613\u88ab\u63d0\u793a\u5de5\u7a0b\u89c4\u907f\uff0c\u5b58\u5728\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u548cLLM\u591a\u6837\u6027\u98ce\u9669\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u68c0\u6d4b\u624b\u6bb5\u3002", "method": "\u5229\u7528MoE\u7ed3\u6784\u4e60\u60ef\uff08\u7279\u522b\u662f\u5185\u90e8\u8def\u7531\u6a21\u5f0f\uff09\u4f5c\u4e3a\u68c0\u6d4b\u4fe1\u53f7\uff0c\u5206\u6790\u4e13\u5bb6\u5728\u4e0d\u540c\u8f93\u5165\u4e0a\u7684\u4e13\u4e1a\u5316\u548c\u534f\u4f5c\u6a21\u5f0f\u3002\u63d0\u51faShadow-MoE\u9ed1\u76d2\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f85\u52a9\u84b8\u998f\u6784\u5efa\u4ee3\u7406MoE\u8868\u793a\u6765\u6bd4\u8f83\u4efb\u610f\u6a21\u578b\u5bf9\u4e4b\u95f4\u7684\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8fbe\u5230\u8d85\u8fc794%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5bf9\u57fa\u4e8e\u63d0\u793a\u7684\u89c4\u907f\u653b\u51fb\u5177\u6709\u5f3a\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MoE\u7ed3\u6784\u4e60\u60ef\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u6301\u7eed\u4f20\u9012\uff0c\u53ef\u4f5c\u4e3a\u53ef\u9760\u7684KD\u68c0\u6d4b\u4fe1\u53f7\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2510.17021", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.17021", "abs": "https://arxiv.org/abs/2510.17021", "authors": ["Bingqi Shang", "Yiwei Chen", "Yihua Zhang", "Bingquan Shen", "Sijia Liu"], "title": "Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning", "comment": null, "summary": "Large language model (LLM) unlearning has become a critical mechanism for\nremoving undesired data, knowledge, or behaviors from pre-trained models while\nretaining their general utility. Yet, with the rise of open-weight LLMs, we\nask: can the unlearning process itself be backdoored, appearing successful\nunder normal conditions yet reverting to pre-unlearned behavior when a hidden\ntrigger is activated? Drawing inspiration from classical backdoor attacks that\nembed triggers into training data to enforce specific behaviors, we investigate\nbackdoor unlearning, where models forget as intended in the clean setting but\nrecover forgotten knowledge when the trigger appears. We show that designing\nsuch attacks presents unique challenges, hinging on where triggers are placed\nand how backdoor training is reinforced. We uncover a strong link between\nbackdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens\nconsistently attract disproportionate attention in LLMs. Our analysis reveals\nthat these attention sinks serve as gateways for backdoor unlearning: placing\ntriggers at sink positions and aligning their attention values markedly\nenhances backdoor persistence. Extensive experiments validate these findings,\nshowing that attention-sink-guided backdoor unlearning reliably restores\nforgotten knowledge in the presence of backdoor triggers, while behaving\nindistinguishably from a normally unlearned model when triggers are absent.\nCode is available at https://github.com/OPTML-Group/Unlearn-Backdoor.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u8fc7\u7a0b\u7684\u9690\u853d\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u5728\u6b63\u5e38\u6761\u4ef6\u4e0b\u6a21\u578b\u8868\u73b0\u51fa\u6210\u529f\u9057\u5fd8\uff0c\u4f46\u5f53\u9690\u85cf\u89e6\u53d1\u5668\u6fc0\u6d3b\u65f6\u4f1a\u6062\u590d\u5df2\u9057\u5fd8\u7684\u77e5\u8bc6\u3002", "motivation": "\u968f\u7740\u5f00\u6e90\u6743\u91cdLLM\u7684\u5174\u8d77\uff0c\u7814\u7a76\u9057\u5fd8\u8fc7\u7a0b\u672c\u8eab\u662f\u5426\u53ef\u80fd\u88ab\u690d\u5165\u540e\u95e8\uff0c\u5373\u5728\u6b63\u5e38\u6761\u4ef6\u4e0b\u770b\u4f3c\u6210\u529f\u9057\u5fd8\uff0c\u4f46\u5728\u7279\u5b9a\u89e6\u53d1\u6761\u4ef6\u4e0b\u6062\u590d\u539f\u6709\u884c\u4e3a\u3002", "method": "\u57fa\u4e8e\u6ce8\u610f\u529b\u6c47\u805a\u73b0\u8c61\uff0c\u5728\u6d45\u5c42\u8f93\u5165\u6807\u8bb0\u4f4d\u7f6e\u653e\u7f6e\u89e6\u53d1\u5668\uff0c\u5e76\u901a\u8fc7\u5bf9\u9f50\u6ce8\u610f\u529b\u503c\u6765\u589e\u5f3a\u540e\u95e8\u6301\u4e45\u6027\uff0c\u5b9e\u73b0\u9690\u853d\u7684\u9057\u5fd8\u540e\u95e8\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6ce8\u610f\u529b\u6c47\u805a\u5f15\u5bfc\u7684\u540e\u95e8\u9057\u5fd8\u65b9\u6cd5\u80fd\u53ef\u9760\u5730\u5728\u89e6\u53d1\u5668\u5b58\u5728\u65f6\u6062\u590d\u9057\u5fd8\u77e5\u8bc6\uff0c\u800c\u5728\u65e0\u89e6\u53d1\u5668\u65f6\u4e0e\u6b63\u5e38\u9057\u5fd8\u6a21\u578b\u65e0\u6cd5\u533a\u5206\u3002", "conclusion": "\u6ce8\u610f\u529b\u6c47\u805a\u73b0\u8c61\u4e3a\u540e\u95e8\u9057\u5fd8\u653b\u51fb\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u63ed\u793a\u4e86LLM\u9057\u5fd8\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u9690\u60a3\u3002", "topic": "agent analysis"}}
{"id": "2510.17057", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17057", "abs": "https://arxiv.org/abs/2510.17057", "authors": ["Nikolaus Howe", "Micah Carroll"], "title": "The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs", "comment": "26 pages", "summary": "The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning\nhas emerged as a promising approach for developing more capable language\nmodels. In turn, this has led to investigation of CoT monitoring as a\ncompelling method for detecting harmful behaviors such as reward hacking, under\nthe assumption that models' reasoning processes reflect their internal\ndecision-making. In practice, LLM training often produces unintended behaviors\ndue to imperfect reward signals, leading models to develop misaligned\ntendencies. A common corrective approach is to apply post-hoc instructions to\navoid problematic behaviors like sycophancy, but what happens to the model's\nreasoning process when these instructions conflict with learned behaviors? We\ninvestigate this question in simple settings and find that models engage in\nsystematic motivated reasoning -- generating plausible-sounding justifications\nfor violating their instructions while downplaying potential harms. Beyond\nbeing an interesting property of training, we find that while motivated\nreasoning can be detected by most frontier reasoning models, smaller LLM judges\ncan fail to identify a portion of it, and in rare cases can themselves be\npersuaded that the reasoning is correct, despite it contradicting clear\ninstructions. This capability gap raises concerns that as models become more\nsophisticated, their motivated reasoning may become increasingly difficult for\nmonitors to detect. Our results underscore the need to account for motivated\nreasoning when relying on chain-of-thought processes for model evaluation and\noversight. All code for this paper will be made available. WARNING: some\nexamples in this paper may be upsetting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f53\u540e\u5904\u7406\u6307\u4ee4\u4e0e\u6a21\u578b\u5df2\u5b66\u4e60\u884c\u4e3a\u51b2\u7a81\u65f6\uff0c\u8bed\u8a00\u6a21\u578b\u4f1a\u8fdb\u884c\u7cfb\u7edf\u6027\u52a8\u673a\u63a8\u7406\u2014\u2014\u751f\u6210\u770b\u4f3c\u5408\u7406\u7684\u7406\u7531\u6765\u8fdd\u53cd\u6307\u4ee4\uff0c\u540c\u65f6\u6de1\u5316\u6f5c\u5728\u5371\u5bb3\u3002\u7814\u7a76\u53d1\u73b0\u524d\u6cbf\u63a8\u7406\u6a21\u578b\u80fd\u68c0\u6d4b\u5230\u8fd9\u79cd\u52a8\u673a\u63a8\u7406\uff0c\u4f46\u8f83\u5c0f\u7684LLM\u8bc4\u5224\u8005\u53ef\u80fd\u65e0\u6cd5\u8bc6\u522b\uff0c\u751a\u81f3\u53ef\u80fd\u88ab\u8bf4\u670d\u8ba4\u4e3a\u8fd9\u79cd\u63a8\u7406\u662f\u6b63\u786e\u7684\u3002", "motivation": "\u7814\u7a76\u5f53\u540e\u5904\u7406\u6307\u4ee4\u4e0e\u6a21\u578b\u5df2\u5b66\u4e60\u884c\u4e3a\u51b2\u7a81\u65f6\uff0c\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u4f1a\u53d1\u751f\u4ec0\u4e48\u53d8\u5316\uff0c\u4ee5\u53ca\u52a8\u673a\u63a8\u7406\u5bf9\u6a21\u578b\u8bc4\u4f30\u548c\u76d1\u7763\u7684\u5f71\u54cd\u3002", "method": "\u5728\u7b80\u5355\u8bbe\u7f6e\u4e2d\u7814\u7a76\u6a21\u578b\u884c\u4e3a\uff0c\u5206\u6790\u6a21\u578b\u5982\u4f55\u751f\u6210\u770b\u4f3c\u5408\u7406\u7684\u7406\u7531\u6765\u8fdd\u53cd\u6307\u4ee4\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u89c4\u6a21LLM\u5bf9\u52a8\u673a\u63a8\u7406\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u6a21\u578b\u4f1a\u8fdb\u884c\u7cfb\u7edf\u6027\u52a8\u673a\u63a8\u7406\uff0c\u524d\u6cbf\u63a8\u7406\u6a21\u578b\u80fd\u68c0\u6d4b\u5230\u8fd9\u79cd\u63a8\u7406\uff0c\u4f46\u8f83\u5c0f\u7684LLM\u8bc4\u5224\u8005\u53ef\u80fd\u65e0\u6cd5\u8bc6\u522b\uff0c\u751a\u81f3\u53ef\u80fd\u88ab\u8bf4\u670d\u8ba4\u4e3a\u8fd9\u79cd\u63a8\u7406\u662f\u6b63\u786e\u7684\u3002", "conclusion": "\u968f\u7740\u6a21\u578b\u53d8\u5f97\u66f4\u590d\u6742\uff0c\u5176\u52a8\u673a\u63a8\u7406\u53ef\u80fd\u8d8a\u6765\u8d8a\u96be\u4ee5\u88ab\u76d1\u63a7\u5668\u68c0\u6d4b\u5230\uff0c\u56e0\u6b64\u5728\u4f9d\u8d56\u601d\u7ef4\u94fe\u8fc7\u7a0b\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\u548c\u76d1\u7763\u65f6\u9700\u8981\u8003\u8651\u52a8\u673a\u63a8\u7406\u3002", "topic": "agent analysis"}}
{"id": "2510.17059", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17059", "abs": "https://arxiv.org/abs/2510.17059", "authors": ["Kathryn Wantlin", "Chongyi Zheng", "Benjamin Eysenbach"], "title": "Consistent Zero-Shot Imitation with Contrastive Goal Inference", "comment": null, "summary": "In the same way that generative models today conduct most of their training\nin a self-supervised fashion, how can agentic models conduct their training in\na self-supervised fashion, interactively exploring, learning, and preparing to\nquickly adapt to new tasks? A prerequisite for embodied agents deployed in real\nworld interactions ought to be training with interaction, yet today's most\nsuccessful AI models (e.g., VLMs, LLMs) are trained without an explicit notion\nof action. The problem of pure exploration (which assumes no data as input) is\nwell studied in the reinforcement learning literature and provides agents with\na wide array of experiences, yet it fails to prepare them for rapid adaptation\nto new tasks. Today's language and vision models are trained on data provided\nby humans, which provides a strong inductive bias for the sorts of tasks that\nthe model will have to solve (e.g., modeling chords in a song, phrases in a\nsonnet, sentences in a medical record). However, when they are prompted to\nsolve a new task, there is a faulty tacit assumption that humans spend most of\ntheir time in the most rewarding states. The key contribution of our paper is a\nmethod for pre-training interactive agents in a self-supervised fashion, so\nthat they can instantly mimic human demonstrations. Our method treats goals\n(i.e., observations) as the atomic construct. During training, our method\nautomatically proposes goals and practices reaching them, building off prior\nwork in reinforcement learning exploration. During evaluation, our method\nsolves an (amortized) inverse reinforcement learning problem to explain\ndemonstrations as optimal goal-reaching behavior. Experiments on standard\nbenchmarks (not designed for goal-reaching) show that our approach outperforms\nprior methods for zero-shot imitation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u7684\u65b9\u6cd5\uff0c\u4f7f\u5b83\u4eec\u80fd\u591f\u5feb\u901f\u6a21\u4eff\u4eba\u7c7b\u6f14\u793a\u3002\u8be5\u65b9\u6cd5\u5c06\u76ee\u6807\u4f5c\u4e3a\u57fa\u672c\u6784\u5efa\u5757\uff0c\u5728\u8bad\u7ec3\u4e2d\u81ea\u52a8\u63d0\u51fa\u76ee\u6807\u5e76\u7ec3\u4e60\u8fbe\u6210\uff0c\u5728\u8bc4\u4f30\u65f6\u901a\u8fc7\u9006\u5f3a\u5316\u5b66\u4e60\u6765\u89e3\u91ca\u6f14\u793a\u4f5c\u4e3a\u6700\u4f18\u76ee\u6807\u8fbe\u6210\u884c\u4e3a\u3002", "motivation": "\u5f53\u524d\u6700\u6210\u529f\u7684AI\u6a21\u578b\uff08\u5982VLMs\u3001LLMs\uff09\u7f3a\u4e4f\u660e\u786e\u7684\u52a8\u4f5c\u6982\u5ff5\uff0c\u800c\u7eaf\u63a2\u7d22\u65b9\u6cd5\u65e0\u6cd5\u4e3a\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u505a\u597d\u51c6\u5907\u3002\u4eba\u7c7b\u63d0\u4f9b\u7684\u6570\u636e\u5b58\u5728\u9519\u8bef\u5047\u8bbe\uff0c\u5373\u4eba\u7c7b\u5927\u90e8\u5206\u65f6\u95f4\u5904\u4e8e\u6700\u6709\u5956\u52b1\u7684\u72b6\u6001\u3002", "method": "\u5c06\u76ee\u6807\u4f5c\u4e3a\u539f\u5b50\u6784\u9020\uff0c\u8bad\u7ec3\u65f6\u81ea\u52a8\u63d0\u51fa\u76ee\u6807\u5e76\u7ec3\u4e60\u8fbe\u6210\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u7684\u5148\u524d\u5de5\u4f5c\u3002\u8bc4\u4f30\u65f6\u901a\u8fc7\u89e3\u51b3\u9006\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u6765\u89e3\u91ca\u6f14\u793a\u4f5c\u4e3a\u6700\u4f18\u76ee\u6807\u8fbe\u6210\u884c\u4e3a\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\uff08\u975e\u4e13\u4e3a\u76ee\u6807\u8fbe\u6210\u8bbe\u8ba1\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u6a21\u4eff\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u9884\u8bad\u7ec3\u65b9\u5f0f\uff0c\u4f7f\u5176\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u5e76\u6a21\u4eff\u4eba\u7c7b\u6f14\u793a\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17122", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.17122", "abs": "https://arxiv.org/abs/2510.17122", "authors": ["Chengxiu Hua", "Jiawen Gu", "Yushun Tang"], "title": "Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control", "comment": null, "summary": "Reinforcement learning (RL) has achieved significant success across a wide\nrange of domains, however, most existing methods are formulated in discrete\ntime. In this work, we introduce a novel RL method for continuous-time control,\nwhere stochastic differential equations govern state-action dynamics. Departing\nfrom traditional value function-based approaches, our key contribution is the\ncharacterization of continuous-time Q-functions via a martingale condition and\nthe linking of diffusion policy scores to the action gradient of a learned\ncontinuous Q-function by the dynamic programming principle. This insight\nmotivates Continuous Q-Score Matching (CQSM), a score-based policy improvement\nalgorithm. Notably, our method addresses a long-standing challenge in\ncontinuous-time RL: preserving the action-evaluation capability of Q-functions\nwithout relying on time discretization. We further provide theoretical\nclosed-form solutions for linear-quadratic (LQ) control problems within our\nframework. Numerical results in simulated environments demonstrate the\neffectiveness of our proposed method and compare it to popular baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5CQSM\uff0c\u901a\u8fc7\u9785\u6761\u4ef6\u5b9a\u4e49\u8fde\u7eed\u65f6\u95f4Q\u51fd\u6570\uff0c\u5c06\u6269\u6563\u7b56\u7565\u5206\u6570\u4e0eQ\u51fd\u6570\u52a8\u4f5c\u68af\u5ea6\u5173\u8054\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u79bb\u6563\u65f6\u95f4\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5927\u591a\u57fa\u4e8e\u79bb\u6563\u65f6\u95f4\u6846\u67b6\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fde\u7eed\u65f6\u95f4\u63a7\u5236\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4fdd\u6301Q\u51fd\u6570\u52a8\u4f5c\u8bc4\u4f30\u80fd\u529b\u4e14\u4e0d\u4f9d\u8d56\u65f6\u95f4\u79bb\u6563\u5316\u7684\u8fde\u7eed\u65f6\u95f4RL\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u9785\u6761\u4ef6\u5b9a\u4e49\u8fde\u7eed\u65f6\u95f4Q\u51fd\u6570\uff0c\u5229\u7528\u52a8\u6001\u89c4\u5212\u539f\u7406\u5c06\u6269\u6563\u7b56\u7565\u5206\u6570\u4e0e\u5b66\u4e60\u5230\u7684\u8fde\u7eedQ\u51fd\u6570\u7684\u52a8\u4f5c\u68af\u5ea6\u76f8\u5173\u8054\uff0c\u63d0\u51fa\u8fde\u7eedQ\u5206\u6570\u5339\u914d(CQSM)\u7b97\u6cd5\u3002", "result": "\u5728\u7ebf\u6027\u4e8c\u6b21\u63a7\u5236\u95ee\u9898\u4e2d\u63d0\u4f9b\u4e86\u7406\u8bba\u95ed\u5f0f\u89e3\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "CQSM\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u4e2d\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\uff0c\u5728\u4e0d\u4f9d\u8d56\u65f6\u95f4\u79bb\u6563\u5316\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u4e86Q\u51fd\u6570\u7684\u52a8\u4f5c\u8bc4\u4f30\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17212", "abs": "https://arxiv.org/abs/2510.17212", "authors": ["Jundong Zhang", "Yuhui Situ", "Fanji Zhang", "Rongji Deng", "Tianqi Wei"], "title": "D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks", "comment": null, "summary": "Tasks involving high-risk-high-return (HRHR) actions, such as obstacle\ncrossing, often exhibit multimodal action distributions and stochastic returns.\nMost reinforcement learning (RL) methods assume unimodal Gaussian policies and\nrely on scalar-valued critics, which limits their effectiveness in HRHR\nsettings. We formally define HRHR tasks and theoretically show that Gaussian\npolicies cannot guarantee convergence to the optimal solution. To address this,\nwe propose a reinforcement learning framework that (i) discretizes continuous\naction spaces to approximate multimodal distributions, (ii) employs\nentropy-regularized exploration to improve coverage of risky but rewarding\nactions, and (iii) introduces a dual-critic architecture for more accurate\ndiscrete value distribution estimation. The framework scales to\nhigh-dimensional action spaces, supporting complex control domains. Experiments\non locomotion and manipulation benchmarks with high risks of failure\ndemonstrate that our method outperforms baselines, underscoring the importance\nof explicitly modeling multimodality and risk in RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ad8\u98ce\u9669\u9ad8\u56de\u62a5\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u3001\u71b5\u6b63\u5219\u5316\u63a2\u7d22\u548c\u53cc\u8bc4\u8bba\u5bb6\u67b6\u6784\u6765\u5e94\u5bf9\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u548c\u968f\u673a\u56de\u62a5\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u5355\u5cf0\u9ad8\u65af\u7b56\u7565\u548c\u6807\u91cf\u503c\u8bc4\u8bba\u5bb6\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u9ad8\u98ce\u9669\u9ad8\u56de\u62a5\u4efb\u52a1\u4e2d\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u548c\u968f\u673a\u56de\u62a5\u95ee\u9898\u3002", "method": "\u79bb\u6563\u5316\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4ee5\u8fd1\u4f3c\u591a\u6a21\u6001\u5206\u5e03\uff0c\u91c7\u7528\u71b5\u6b63\u5219\u5316\u63a2\u7d22\u6765\u8986\u76d6\u9ad8\u98ce\u9669\u4f46\u9ad8\u56de\u62a5\u7684\u52a8\u4f5c\uff0c\u5f15\u5165\u53cc\u8bc4\u8bba\u5bb6\u67b6\u6784\u8fdb\u884c\u66f4\u51c6\u786e\u7684\u79bb\u6563\u503c\u5206\u5e03\u4f30\u8ba1\u3002", "result": "\u5728\u5177\u6709\u9ad8\u5931\u8d25\u98ce\u9669\u7684\u79fb\u52a8\u548c\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u660e\u786e\u5efa\u6a21\u591a\u6a21\u6001\u6027\u548c\u98ce\u9669\u5bf9\u4e8e\u5904\u7406\u9ad8\u98ce\u9669\u9ad8\u56de\u62a5\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17276", "categories": ["cs.LG", "cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17276", "abs": "https://arxiv.org/abs/2510.17276", "authors": ["Rishi Jha", "Harold Triedman", "Justin Wagle", "Vitaly Shmatikov"], "title": "Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems", "comment": null, "summary": "Control-flow hijacking attacks manipulate orchestration mechanisms in\nmulti-agent systems into performing unsafe actions that compromise the system\nand exfiltrate sensitive information. Recently proposed defenses, such as\nLlamaFirewall, rely on alignment checks of inter-agent communications to ensure\nthat all agent invocations are \"related to\" and \"likely to further\" the\noriginal objective.\n  We start by demonstrating control-flow hijacking attacks that evade these\ndefenses even if alignment checks are performed by advanced LLMs. We argue that\nthe safety and functionality objectives of multi-agent systems fundamentally\nconflict with each other. This conflict is exacerbated by the brittle\ndefinitions of \"alignment\" and the checkers' incomplete visibility into the\nexecution context.\n  We then propose, implement, and evaluate ControlValve, a new defense inspired\nby the principles of control-flow integrity and least privilege. ControlValve\n(1) generates permitted control-flow graphs for multi-agent systems, and (2)\nenforces that all executions comply with these graphs, along with contextual\nrules (generated in a zero-shot manner) for each agent invocation.", "AI": {"tldr": "ControlValve\u9632\u5fa1\u7cfb\u7edf\u901a\u8fc7\u751f\u6210\u5141\u8bb8\u7684\u63a7\u5236\u6d41\u56fe\u5e76\u5f3a\u5236\u6267\u884c\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u63a7\u5236\u6d41\u52ab\u6301\u653b\u51fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5bf9\u9f50\u68c0\u67e5\u7684\u9632\u5fa1\uff08\u5982LlamaFirewall\uff09\u65e0\u6cd5\u6709\u6548\u62b5\u5fa1\u63a7\u5236\u6d41\u52ab\u6301\u653b\u51fb\uff0c\u56e0\u4e3a\u5b89\u5168\u6027\u548c\u529f\u80fd\u6027\u76ee\u6807\u5b58\u5728\u6839\u672c\u51b2\u7a81\uff0c\u4e14\u5bf9\u9f50\u5b9a\u4e49\u8106\u5f31\u3001\u68c0\u67e5\u5668\u5bf9\u6267\u884c\u4e0a\u4e0b\u6587\u53ef\u89c1\u6027\u4e0d\u5b8c\u6574\u3002", "method": "\u63d0\u51faControlValve\u9632\u5fa1\u7cfb\u7edf\uff0c\u57fa\u4e8e\u63a7\u5236\u6d41\u5b8c\u6574\u6027\u548c\u6700\u5c0f\u6743\u9650\u539f\u5219\uff1a(1)\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u751f\u6210\u5141\u8bb8\u7684\u63a7\u5236\u6d41\u56fe\uff1b(2)\u5f3a\u5236\u6267\u884c\u6240\u6709\u6267\u884c\u7b26\u5408\u8fd9\u4e9b\u56fe\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u8c03\u7528\u751f\u6210\u96f6\u6837\u672c\u4e0a\u4e0b\u6587\u89c4\u5219\u3002", "result": "ControlValve\u80fd\u591f\u6709\u6548\u9632\u5fa1\u89c4\u907f\u73b0\u6709\u5bf9\u9f50\u68c0\u67e5\u7684\u63a7\u5236\u6d41\u52ab\u6301\u653b\u51fb\u3002", "conclusion": "ControlValve\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b89\u5168\u9632\u5fa1\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5bf9\u9f50\u68c0\u67e5\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.17281", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.17281", "abs": "https://arxiv.org/abs/2510.17281", "authors": ["Qingyao Ai", "Yichen Tang", "Changyue Wang", "Jianming Long", "Weihang Su", "Yiqun Liu"], "title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems", "comment": null, "summary": "Scaling up data, parameters, and test-time computation has been the\nmainstream methods to improve LLM systems (LLMsys), but their upper bounds are\nalmost reached due to the gradual depletion of high-quality data and marginal\ngains obtained from larger computational resource consumption. Inspired by the\nabilities of human and traditional AI systems in learning from practice,\nconstructing memory and continual learning frameworks for LLMsys has become an\nimportant and popular research direction in recent literature. Yet, existing\nbenchmarks for LLM memory often focus on evaluating the system on homogeneous\nreading comprehension tasks with long-form inputs rather than testing their\nabilities to learn from accumulated user feedback in service time. Therefore,\nwe propose a user feedback simulation framework and a comprehensive benchmark\ncovering multiple domains, languages, and types of tasks to evaluate the\ncontinual learning abilities of LLMsys. Experiments show that the effectiveness\nand efficiency of state-of-the-art baselines are far from satisfying, and we\nhope this benchmark could pave the way for future studies on LLM memory and\noptimization algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u6237\u53cd\u9988\u6a21\u62df\u6846\u67b6\u548c\u7efc\u5408\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u7cfb\u7edf\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\uff0c\u8986\u76d6\u591a\u9886\u57df\u3001\u591a\u8bed\u8a00\u548c\u591a\u79cd\u4efb\u52a1\u7c7b\u578b\u3002", "motivation": "\u7531\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u9010\u6e10\u8017\u5c3d\u548c\u66f4\u5927\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5e26\u6765\u7684\u8fb9\u9645\u6536\u76ca\u9012\u51cf\uff0c\u4f20\u7edf\u901a\u8fc7\u6269\u5927\u6570\u636e\u3001\u53c2\u6570\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7684\u65b9\u6cd5\u5df2\u63a5\u8fd1\u4e0a\u9650\uff0c\u9700\u8981\u4ece\u5b9e\u8df5\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u7528\u6237\u53cd\u9988\u6a21\u62df\u6846\u67b6\u548c\u7efc\u5408\u57fa\u51c6\uff0c\u6d4b\u8bd5LLM\u7cfb\u7edf\u5728\u670d\u52a1\u65f6\u95f4\u5185\u4ece\u7d2f\u79ef\u7528\u6237\u53cd\u9988\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u540c\u8d28\u5316\u9605\u8bfb\u7406\u89e3\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u8fdc\u672a\u8fbe\u5230\u6ee1\u610f\u6c34\u5e73\u3002", "conclusion": "\u8be5\u57fa\u51c6\u53ef\u4e3a\u672a\u6765LLM\u8bb0\u5fc6\u548c\u4f18\u5316\u7b97\u6cd5\u7814\u7a76\u94fa\u5e73\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.17380", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17380", "abs": "https://arxiv.org/abs/2510.17380", "authors": ["Julen Cestero", "Carmine Delle Femine", "Kenji S. Muro", "Marco Quartulli", "Marcello Restelli"], "title": "Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks", "comment": null, "summary": "Optimizing the energy management within a smart grids scenario presents\nsignificant challenges, primarily due to the complexity of real-world systems\nand the intricate interactions among various components. Reinforcement Learning\n(RL) is gaining prominence as a solution for addressing the challenges of\nOptimal Power Flow in smart grids. However, RL needs to iterate compulsively\nthroughout a given environment to obtain the optimal policy. This means\nobtaining samples from a, most likely, costly simulator, which can lead to a\nsample efficiency problem. In this work, we address this problem by\nsubstituting costly smart grid simulators with surrogate models built using\nPhisics-informed Neural Networks (PINNs), optimizing the RL policy training\nprocess by arriving to convergent results in a fraction of the time employed by\nthe original environment.", "AI": {"tldr": "\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\u6784\u5efa\u66ff\u4ee3\u6a21\u578b\uff0c\u66ff\u4ee3\u6602\u8d35\u7684\u667a\u80fd\u7535\u7f51\u6a21\u62df\u5668\uff0c\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "motivation": "\u667a\u80fd\u7535\u7f51\u4e2d\u7684\u80fd\u91cf\u7ba1\u7406\u9762\u4e34\u73b0\u5b9e\u7cfb\u7edf\u590d\u6742\u6027\u548c\u7ec4\u4ef6\u95f4\u4ea4\u4e92\u7684\u6311\u6218\uff0c\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u73af\u5883\u8fed\u4ee3\u6765\u83b7\u5f97\u6700\u4f18\u7b56\u7565\uff0c\u4f46\u6602\u8d35\u7684\u6a21\u62df\u5668\u4f1a\u5bfc\u81f4\u6837\u672c\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\u6784\u5efa\u667a\u80fd\u7535\u7f51\u6a21\u62df\u5668\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3\uff0c\u51cf\u5c11\u5bf9\u6602\u8d35\u6a21\u62df\u5668\u7684\u4f9d\u8d56\u3002", "result": "\u4f7f\u7528PINNs\u66ff\u4ee3\u6a21\u578b\u80fd\u591f\u5728\u8fdc\u5c11\u4e8e\u539f\u59cb\u73af\u5883\u6240\u9700\u7684\u65f6\u95f4\u5185\u8fbe\u5230\u6536\u655b\u7ed3\u679c\u3002", "conclusion": "PINNs\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\u80fd\u6709\u6548\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u667a\u80fd\u7535\u7f51\u4f18\u5316\u4e2d\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u52a0\u901f\u7b56\u7565\u8bad\u7ec3\u8fc7\u7a0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17385", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17385", "abs": "https://arxiv.org/abs/2510.17385", "authors": ["Pengxiang Cai", "Zihao Gao", "Jintai Chen"], "title": "TabR1: Taming GRPO for tabular reasoning LLMs", "comment": null, "summary": "Tabular prediction has traditionally relied on gradient-boosted decision\ntrees and specialized deep learning models, which excel within tasks but\nprovide limited interpretability and weak transfer across tables. Reasoning\nlarge language models (LLMs) promise cross-task adaptability with trans- parent\nreasoning traces, yet their potential has not been fully realized for tabular\ndata. This paper presents TabR1, the first reasoning LLM for tabular prediction\nwith multi-step reasoning. At its core is Permutation Relative Policy\nOptimization (PRPO), a simple yet efficient reinforcement learning method that\nencodes column-permutation invariance as a structural prior. By construct- ing\nmultiple label-preserving permutations per sample and estimating advantages\nboth within and across permutations, PRPO transforms sparse rewards into dense\nlearning signals and improves generalization. With limited supervision, PRPO\nactivates the reasoning ability of LLMs for tabular prediction, enhancing\nfew-shot and zero-shot performance as well as interpretability. Comprehensive\nexperiments demonstrate that TabR1 achieves performance comparable to strong\nbaselines under full-supervision fine-tuning. In the zero-shot setting, TabR1\napproaches the performance of strong baselines under the 32-shot setting.\nMoreover, TabR1 (8B) substantially outperforms much larger LLMs across various\ntasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).", "AI": {"tldr": "TabR1\u662f\u9996\u4e2a\u7528\u4e8e\u8868\u683c\u9884\u6d4b\u7684\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7PRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6fc0\u6d3bLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u80fd\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684LLM\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u9884\u6d4b\u65b9\u6cd5\uff08\u5982\u68af\u5ea6\u63d0\u5347\u6811\u548c\u4e13\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff09\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u4efb\u52a1\u8fc1\u79fb\u80fd\u529b\uff0c\u800c\u63a8\u7406LLM\u867d\u7136\u5177\u6709\u8de8\u4efb\u52a1\u9002\u5e94\u6027\u548c\u900f\u660e\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u8868\u683c\u6570\u636e\u4e0a\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u53d1\u6398\u3002", "method": "\u63d0\u51faTabR1\u6a21\u578b\uff0c\u6838\u5fc3\u662fPRPO\uff08\u6392\u5217\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u4e2a\u6807\u7b7e\u4fdd\u6301\u7684\u5217\u6392\u5217\u6837\u672c\uff0c\u5728\u6392\u5217\u5185\u90e8\u548c\u8de8\u6392\u5217\u95f4\u4f30\u8ba1\u4f18\u52bf\uff0c\u5c06\u7a00\u758f\u5956\u52b1\u8f6c\u5316\u4e3a\u5bc6\u96c6\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "TabR1\u5728\u5168\u76d1\u7763\u5fae\u8c03\u4e0b\u8fbe\u5230\u4e0e\u5f3a\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\uff1b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u63a5\u8fd132\u6837\u672c\u8bbe\u7f6e\u7684\u5f3a\u57fa\u7ebf\u6027\u80fd\uff1bTabR1(8B)\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u66f4\u5927\u7684LLM\uff0c\u76f8\u6bd4DeepSeek-R1(685B)\u63d0\u5347\u8fbe53.17%\u3002", "conclusion": "PRPO\u65b9\u6cd5\u6709\u6548\u6fc0\u6d3b\u4e86LLM\u5728\u8868\u683c\u9884\u6d4b\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u3001\u96f6\u6837\u672c\u6027\u80fd\u4ee5\u53ca\u53ef\u89e3\u91ca\u6027\uff0c\u8bc1\u660e\u4e86\u63a8\u7406LLM\u5728\u8868\u683c\u6570\u636e\u4e0a\u7684\u5de8\u5927\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17391", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17391", "abs": "https://arxiv.org/abs/2510.17391", "authors": ["Jongmin Lee", "Ernest K. Ryu"], "title": "Finite-Time Bounds for Average-Reward Fitted Q-Iteration", "comment": null, "summary": "Although there is an extensive body of work characterizing the sample\ncomplexity of discounted-return offline RL with function approximations, prior\nwork on the average-reward setting has received significantly less attention,\nand existing approaches rely on restrictive assumptions, such as ergodicity or\nlinearity of the MDP. In this work, we establish the first sample complexity\nresults for average-reward offline RL with function approximation for weakly\ncommunicating MDPs, a much milder assumption. To this end, we introduce\nAnchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration\nwith an anchor mechanism. We show that the anchor, which can be interpreted as\na form of weight decay, is crucial for enabling finite-time analysis in the\naverage-reward setting. We also extend our finite-time analysis to the setup\nwhere the dataset is generated from a single-trajectory rather than IID\ntransitions, again leveraging the anchor mechanism.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u5f31\u901a\u4fe1MDP\u7684\u5e73\u5747\u5956\u52b1\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u590d\u6742\u5ea6\u7ed3\u679c\uff0c\u5f15\u5165\u4e86\u951a\u5b9a\u62df\u5408Q\u8fed\u4ee3\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u6807\u51c6FQI\u4e0e\u951a\u673a\u5236\uff0c\u5e76\u6269\u5c55\u5230\u5355\u8f68\u8ff9\u6570\u636e\u96c6\u8bbe\u7f6e\u3002", "motivation": "\u73b0\u6709\u5173\u4e8e\u5e73\u5747\u5956\u52b1\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u4e14\u4f9d\u8d56\u4e25\u683c\u5047\u8bbe\uff08\u5982\u904d\u5386\u6027\u6216MDP\u7ebf\u6027\uff09\uff0c\u9700\u8981\u66f4\u6e29\u548c\u5047\u8bbe\u4e0b\u7684\u6837\u672c\u590d\u6742\u5ea6\u5206\u6790\u3002", "method": "\u63d0\u51fa\u4e86\u951a\u5b9a\u62df\u5408Q\u8fed\u4ee3\u65b9\u6cd5\uff0c\u5c06\u6807\u51c6\u62df\u5408Q\u8fed\u4ee3\u4e0e\u951a\u673a\u5236\u7ed3\u5408\uff0c\u951a\u673a\u5236\u53ef\u89e3\u91ca\u4e3a\u6743\u91cd\u8870\u51cf\u5f62\u5f0f\uff0c\u6709\u52a9\u4e8e\u5e73\u5747\u5956\u52b1\u8bbe\u7f6e\u7684\u6709\u9650\u65f6\u95f4\u5206\u6790\u3002", "result": "\u5efa\u7acb\u4e86\u5f31\u901a\u4fe1MDP\u4e0b\u5e73\u5747\u5956\u52b1\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u9996\u4e2a\u6837\u672c\u590d\u6742\u5ea6\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u951a\u673a\u5236\u5728\u6709\u9650\u65f6\u95f4\u5206\u6790\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u951a\u673a\u5236\u662f\u5b9e\u73b0\u5e73\u5747\u5956\u52b1\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6709\u9650\u65f6\u95f4\u5206\u6790\u7684\u5173\u952e\u521b\u65b0\uff0c\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u5355\u8f68\u8ff9\u6570\u636e\u96c6\u8bbe\u7f6e\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17564", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17564", "abs": "https://arxiv.org/abs/2510.17564", "authors": ["Lindsay Spoor", "\u00c1lvaro Serra-G\u00f3mez", "Aske Plaat", "Thomas Moerland"], "title": "An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning", "comment": null, "summary": "In safety-critical domains such as robotics, navigation and power systems,\nconstrained optimization problems arise where maximizing performance must be\ncarefully balanced with associated constraints. Safe reinforcement learning\nprovides a framework to address these challenges, with Lagrangian methods being\na popular choice. However, the effectiveness of Lagrangian methods crucially\ndepends on the choice of the Lagrange multiplier $\\lambda$, which governs the\ntrade-off between return and constraint cost. A common approach is to update\nthe multiplier automatically during training. Although this is standard in\npractice, there remains limited empirical evidence on the robustness of an\nautomated update and its influence on overall performance. Therefore, we\nanalyze (i) optimality and (ii) stability of Lagrange multipliers in safe\nreinforcement learning across a range of tasks. We provide $\\lambda$-profiles\nthat give a complete visualization of the trade-off between return and\nconstraint cost of the optimization problem. These profiles show the highly\nsensitive nature of $\\lambda$ and moreover confirm the lack of general\nintuition for choosing the optimal value $\\lambda^*$. Our findings additionally\nshow that automated multiplier updates are able to recover and sometimes even\nexceed the optimal performance found at $\\lambda^*$ due to the vast difference\nin their learning trajectories. Furthermore, we show that automated multiplier\nupdates exhibit oscillatory behavior during training, which can be mitigated\nthrough PID-controlled updates. However, this method requires careful tuning to\nachieve consistently better performance across tasks. This highlights the need\nfor further research on stabilizing Lagrangian methods in safe reinforcement\nlearning. The code used to reproduce our results can be found at\nhttps://github.com/lindsayspoor/Lagrangian_SafeRL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u7684\u6700\u4f18\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u81ea\u52a8\u66f4\u65b0\u4e58\u5b50\u80fd\u591f\u6062\u590d\u751a\u81f3\u8d85\u8fc7\u6700\u4f18\u6027\u80fd\uff0c\u4f46\u5b58\u5728\u632f\u8361\u884c\u4e3a\uff0c\u53ef\u901a\u8fc7PID\u63a7\u5236\u7f13\u89e3\u4f46\u9700\u8981\u4ed4\u7ec6\u8c03\u53c2\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u88ab\u5e7f\u6cdb\u7528\u4e8e\u5904\u7406\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4f46\u4e58\u5b50\u03bb\u7684\u9009\u62e9\u5bf9\u6027\u80fd\u5f71\u54cd\u5f88\u5927\uff0c\u81ea\u52a8\u66f4\u65b0\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u5f71\u54cd\u7f3a\u4e4f\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u03bb-profile\u53ef\u89c6\u5316\u65b9\u6cd5\u5206\u6790\u8fd4\u56de\u4e0e\u7ea6\u675f\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u6bd4\u8f83\u81ea\u52a8\u4e58\u5b50\u66f4\u65b0\u4e0e\u56fa\u5b9a\u4e58\u5b50\u7684\u6027\u80fd\uff0c\u5e76\u6d4b\u8bd5PID\u63a7\u5236\u66f4\u65b0\u65b9\u6cd5\u3002", "result": "\u03bb\u5177\u6709\u9ad8\u5ea6\u654f\u611f\u6027\uff0c\u81ea\u52a8\u4e58\u5b50\u66f4\u65b0\u80fd\u591f\u6062\u590d\u751a\u81f3\u8d85\u8fc7\u6700\u4f18\u6027\u80fd\uff0c\u4f46\u8868\u73b0\u51fa\u632f\u8361\u884c\u4e3a\uff0cPID\u63a7\u5236\u53ef\u7f13\u89e3\u4f46\u9700\u8981\u4ed4\u7ec6\u8c03\u53c2\u3002", "conclusion": "\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u5728\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u7a33\u5b9a\u5316\uff0c\u81ea\u52a8\u4e58\u5b50\u66f4\u65b0\u867d\u6709\u6548\u4f46\u5b58\u5728\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.17709", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17709", "abs": "https://arxiv.org/abs/2510.17709", "authors": ["Akhil S Anand", "Shambhuraj Sawant", "Jasper Hoffmann", "Dirk Reinhardt", "Sebastien Gros"], "title": "Closing the Sim2Real Performance Gap in RL", "comment": null, "summary": "Sim2Real aims at training policies in high-fidelity simulation environments\nand effectively transferring them to the real world. Despite the developments\nof accurate simulators and Sim2Real RL approaches, the policies trained purely\nin simulation often suffer significant performance drops when deployed in real\nenvironments. This drop is referred to as the Sim2Real performance gap. Current\nSim2Real RL methods optimize the simulator accuracy and variability as proxies\nfor real-world performance. However, these metrics do not necessarily correlate\nwith the real-world performance of the policy as established theoretically and\nempirically in the literature. We propose a novel framework to address this\nissue by directly adapting the simulator parameters based on real-world\nperformance. We frame this problem as a bi-level RL framework: the inner-level\nRL trains a policy purely in simulation, and the outer-level RL adapts the\nsimulation model and in-sim reward parameters to maximize real-world\nperformance of the in-sim policy. We derive and validate in simple examples the\nmathematical tools needed to develop bi-level RL algorithms that close the\nSim2Real performance gap.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u5c42\u6b21\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u8c03\u6574\u6a21\u62df\u5668\u53c2\u6570\u6765\u7f29\u5c0fSim2Real\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524dSim2Real RL\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u6a21\u62df\u5668\u7cbe\u5ea6\u548c\u53d8\u5f02\u6027\u4f5c\u4e3a\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u7684\u4ee3\u7406\u6307\u6807\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u4e0e\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u80fd\u5e76\u4e0d\u5fc5\u7136\u76f8\u5173\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u6b21RL\u6846\u67b6\uff1a\u5185\u5c42RL\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u5916\u5c42RL\u8c03\u6574\u6a21\u62df\u6a21\u578b\u548c\u6a21\u62df\u5185\u5956\u52b1\u53c2\u6570\uff0c\u4ee5\u6700\u5927\u5316\u6a21\u62df\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u63a8\u5bfc\u5e76\u9a8c\u8bc1\u4e86\u5f00\u53d1\u53cc\u5c42\u6b21RL\u7b97\u6cd5\u6240\u9700\u7684\u6570\u5b66\u5de5\u5177\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u80fd\u591f\u7f29\u5c0fSim2Real\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u76f4\u63a5\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u4f18\u5316\u6a21\u62df\u5668\u53c2\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86Sim2Real\u6027\u80fd\u5dee\u8ddd\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.e39286e4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fredis.io%2Fresources%2Fmanaging-memory-for-ai-agents%2F%3Futm_source=%5Bname-of-provider%5D%26utm_medium=cpa%26utm_campaign=2025-10-ai_in_production%26utm_content=eb-managing_memory_for_ai_agents-701N100000aRaU6/2/0100019a012357df-27ed190c-3b66-489e-9f4f-44f11a601280-000000/Jpey4tzzcmvRKlQT20XlppMvhVKZXLf2VjFuGmkhaCs=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fredis.io%2Fresources%2Fmanaging-memory-for-ai-agents%2F%3Futm_source=%5Bname-of-provider%5D%26utm_medium=cpa%26utm_campaign=2025-10-ai_in_production%26utm_content=eb-managing_memory_for_ai_agents-701N100000aRaU6/2/0100019a012357df-27ed190c-3b66-489e-9f4f-44f11a601280-000000/Jpey4tzzcmvRKlQT20XlppMvhVKZXLf2VjFuGmkhaCs=427", "authors": ["TLDR Newsletter"], "title": "Is your team building or scaling AI agents?", "comment": "Source: TLDR Newsletter, Date: 2025-10-20, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fredis.io%2Fresources%2Fmanaging-memory-for-ai-agents%2F%3Futm_source=%5Bname-of-provider%5D%26utm_medium=cpa%26utm_campaign=2025-10-ai_in_production%26utm_content=eb-managing_memory_for_ai_agents-701N100000aRaU6/2/0100019a012357df-27ed190c-3b66-489e-9f4f-44f11a601280-000000/Jpey4tzzcmvRKlQT20XlppMvhVKZXLf2VjFuGmkhaCs=427", "summary": "Is your team building or scaling AI agents? (Sponsor) One of AI's biggest challenges today is memory\u2014how agents retain, recall, and remember over time. Without it, even the best models struggle with context loss, inconsistency, and limited scalability.This new O'Reilly + Redis report breaks down why memory is the foundation of scalable AI systems and how real-time architectures make it possible. Inside the report: The role of short term, long term, and persistent memory in agent performance F...", "source": "tldr", "AI": {"tldr": "\u8be5\u62a5\u544a\u5206\u6790\u4e86AI\u4ee3\u7406\u7cfb\u7edf\u4e2d\u5185\u5b58\u7684\u91cd\u8981\u6027\uff0c\u63a2\u8ba8\u4e86\u77ed\u671f\u3001\u957f\u671f\u548c\u6301\u4e45\u6027\u5185\u5b58\u5728\u4ee3\u7406\u6027\u80fd\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u5b9e\u65f6\u67b6\u6784\u5982\u4f55\u5b9e\u73b0\u53ef\u6269\u5c55\u7684AI\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3AI\u4ee3\u7406\u9762\u4e34\u7684\u5185\u5b58\u6311\u6218\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u4e22\u5931\u3001\u4e0d\u4e00\u81f4\u6027\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u5f3a\u8c03\u5185\u5b58\u4f5c\u4e3a\u53ef\u6269\u5c55AI\u7cfb\u7edf\u57fa\u7840\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7O'Reilly\u548cRedis\u5408\u4f5c\u53d1\u5e03\u7684\u62a5\u544a\uff0c\u5206\u6790\u5185\u5b58\u67b6\u6784\u5728AI\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\uff0c\u6db5\u76d6\u77ed\u671f\u3001\u957f\u671f\u548c\u6301\u4e45\u6027\u5185\u5b58\u7684\u914d\u7f6e\u3002", "result": "\u62a5\u544a\u9610\u660e\u4e86\u5b9e\u65f6\u5185\u5b58\u67b6\u6784\u5982\u4f55\u4f7fAI\u4ee3\u7406\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u7559\u3001\u56de\u5fc6\u548c\u8bb0\u5fc6\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u5185\u5b58\u662f\u6784\u5efa\u53ef\u6269\u5c55AI\u7cfb\u7edf\u7684\u5173\u952e\u57fa\u7840\uff0c\u5b9e\u65f6\u5185\u5b58\u67b6\u6784\u80fd\u591f\u6709\u6548\u89e3\u51b3AI\u4ee3\u7406\u9762\u4e34\u7684\u5185\u5b58\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.46cdb3a4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "authors": ["TLDR Newsletter"], "title": "Ondeva: Replace Fragile Scripts with Reliable Workflows", "comment": "Source: TLDR Newsletter, Date: 2025-10-20, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "summary": "Ondeva: Replace Fragile Scripts with Reliable Workflows (Sponsor) Managing glue code / internal automations / dashboards? you can do better than fragile Cron jobs or toy automation tools. Ondeva is workflow automation for devs, not hobbyists: Built for reliability: Replaces ad-hoc scripts with logged, testable workflows. Every execution is visible, every failure recoverable. Reclaim dev hours: Ops and end users can build safe automations with guardrails, so you can get back to real engineerin...", "source": "tldr", "AI": {"tldr": "Ondeva\u662f\u4e00\u4e2a\u9762\u5411\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u65e8\u5728\u66ff\u4ee3\u8106\u5f31\u7684\u811a\u672c\u548cCron\u4f5c\u4e1a\uff0c\u63d0\u4f9b\u53ef\u9760\u3001\u53ef\u6d4b\u8bd5\u7684\u5de5\u4f5c\u6d41\u7ba1\u7406\u3002", "motivation": "\u89e3\u51b3\u5f00\u53d1\u4e2d\u80f6\u6c34\u4ee3\u7801\u3001\u5185\u90e8\u81ea\u52a8\u5316\u548c\u4eea\u8868\u677f\u7ba1\u7406\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u66ff\u4ee3\u4e0d\u7a33\u5b9a\u7684Cron\u4f5c\u4e1a\u548c\u73a9\u5177\u7ea7\u81ea\u52a8\u5316\u5de5\u5177\u3002", "method": "\u6784\u5efa\u4e13\u95e8\u4e3a\u5f00\u53d1\u8005\u8bbe\u8ba1\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u63d0\u4f9b\u6267\u884c\u65e5\u5fd7\u8bb0\u5f55\u3001\u53ef\u6d4b\u8bd5\u6027\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\uff0c\u540c\u65f6\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u5b89\u5168\u62a4\u680f\u3002", "result": "\u80fd\u591f\u5c06\u4e34\u65f6\u811a\u672c\u8f6c\u6362\u4e3a\u53ef\u9760\u7684\u5de5\u4f5c\u6d41\uff0c\u4f7f\u6bcf\u4e2a\u6267\u884c\u53ef\u89c1\u3001\u6bcf\u4e2a\u6545\u969c\u53ef\u6062\u590d\uff0c\u8ba9\u5f00\u53d1\u4eba\u5458\u4ece\u8fd0\u7ef4\u4efb\u52a1\u4e2d\u89e3\u653e\u51fa\u6765\u3002", "conclusion": "Ondeva\u4e3a\u5f00\u53d1\u56e2\u961f\u63d0\u4f9b\u4e86\u4e13\u4e1a\u7ea7\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u5316\u6d41\u7a0b\u7684\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "topic": "swe application"}}
{"id": "tldr.2510.6c226fb2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "authors": ["TLDR Newsletter"], "title": "Built for reliability:", "comment": "Source: TLDR Newsletter, Date: 2025-10-20, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "summary": "Ondeva: Replace Fragile Scripts with Reliable Workflows (Sponsor) Managing glue code / internal automations / dashboards? you can do better than fragile Cron jobs or toy automation tools. Ondeva is workflow automation for devs, not hobbyists: Built for reliability: Replaces ad-hoc scripts with logged, testable workflows. Every execution is visible, every failure recoverable. Reclaim dev hours: Ops and end users can build safe automations with guardrails, so you can get back to real engineerin...", "source": "tldr", "AI": {"tldr": "Ondeva\u662f\u4e00\u4e2a\u9762\u5411\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u65e8\u5728\u66ff\u4ee3\u8106\u5f31\u7684\u811a\u672c\u548cCron\u4f5c\u4e1a\uff0c\u63d0\u4f9b\u53ef\u9760\u3001\u53ef\u6d4b\u8bd5\u7684\u5de5\u4f5c\u6d41\u7ba1\u7406\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u811a\u672c\u548c\u81ea\u52a8\u5316\u5de5\u5177\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u8ba9\u5f00\u53d1\u4eba\u5458\u80fd\u591f\u6784\u5efa\u53ef\u9760\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\uff0c\u51cf\u5c11\u7ef4\u62a4\u6210\u672c\u3002", "method": "\u63d0\u4f9b\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u652f\u6301\u6267\u884c\u65e5\u5fd7\u8bb0\u5f55\u3001\u6d4b\u8bd5\u548c\u6545\u969c\u6062\u590d\uff0c\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u5b89\u5168\u62a4\u680f\u3002", "result": "\u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u8282\u7701\u65f6\u95f4\uff0c\u8ba9\u8fd0\u7ef4\u548c\u6700\u7ec8\u7528\u6237\u80fd\u591f\u6784\u5efa\u5b89\u5168\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\u3002", "conclusion": "Ondeva\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u56e2\u961f\u4ece\u7e41\u7410\u7684\u811a\u672c\u7ef4\u62a4\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u4e13\u6ce8\u4e8e\u771f\u6b63\u7684\u5de5\u7a0b\u5de5\u4f5c\u3002", "topic": "swe application"}}
{"id": "tldr.2510.c7de5845", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "authors": ["TLDR Newsletter"], "title": "Reclaim dev hours:", "comment": "Source: TLDR Newsletter, Date: 2025-10-20, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "summary": "Ondeva: Replace Fragile Scripts with Reliable Workflows (Sponsor) Managing glue code / internal automations / dashboards? you can do better than fragile Cron jobs or toy automation tools. Ondeva is workflow automation for devs, not hobbyists: Built for reliability: Replaces ad-hoc scripts with logged, testable workflows. Every execution is visible, every failure recoverable. Reclaim dev hours: Ops and end users can build safe automations with guardrails, so you can get back to real engineerin...", "source": "tldr", "AI": {"tldr": "Ondeva\u662f\u4e00\u4e2a\u9762\u5411\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u65e8\u5728\u53d6\u4ee3\u8106\u5f31\u7684\u811a\u672c\u548cCron\u4f5c\u4e1a\uff0c\u63d0\u4f9b\u53ef\u9760\u3001\u53ef\u6d4b\u8bd5\u7684\u5de5\u4f5c\u6d41\u7ba1\u7406", "motivation": "\u89e3\u51b3\u4f20\u7edf\u811a\u672c\u548c\u81ea\u52a8\u5316\u5de5\u5177\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u6d6a\u8d39", "method": "\u6784\u5efa\u4e13\u95e8\u4e3a\u5f00\u53d1\u8005\u8bbe\u8ba1\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u63d0\u4f9b\u6267\u884c\u65e5\u5fd7\u8bb0\u5f55\u3001\u53ef\u6d4b\u8bd5\u7684\u5de5\u4f5c\u6d41\u3001\u6545\u969c\u6062\u590d\u673a\u5236\uff0c\u5e76\u4e3a\u8fd0\u8425\u548c\u6700\u7ec8\u7528\u6237\u63d0\u4f9b\u5b89\u5168\u81ea\u52a8\u5316\u6784\u5efa\u7684\u9632\u62a4\u63aa\u65bd", "result": "\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u5de5\u4f5c\u6d41\u6267\u884c\uff0c\u6bcf\u4e2a\u6267\u884c\u90fd\u53ef\u89c1\uff0c\u6bcf\u4e2a\u6545\u969c\u90fd\u53ef\u6062\u590d\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u4e13\u6ce8\u4e8e\u771f\u6b63\u7684\u5de5\u7a0b\u5de5\u4f5c", "conclusion": "Ondeva\u6210\u529f\u66ff\u4ee3\u4e86\u8106\u5f31\u7684\u811a\u672c\u548c\u73a9\u5177\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4e13\u4e1a\u7ea7\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848", "topic": "swe application"}}
{"id": "tldr.2510.fc2c1299", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "authors": ["TLDR Newsletter"], "title": "Centralize automations", "comment": "Source: TLDR Newsletter, Date: 2025-10-20, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "summary": "Ondeva: Replace Fragile Scripts with Reliable Workflows (Sponsor) Managing glue code / internal automations / dashboards? you can do better than fragile Cron jobs or toy automation tools. Ondeva is workflow automation for devs, not hobbyists: Built for reliability: Replaces ad-hoc scripts with logged, testable workflows. Every execution is visible, every failure recoverable. Reclaim dev hours: Ops and end users can build safe automations with guardrails, so you can get back to real engineerin...", "source": "tldr", "AI": {"tldr": "Ondeva\u662f\u4e00\u4e2a\u9762\u5411\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u65e8\u5728\u66ff\u4ee3\u8106\u5f31\u7684\u811a\u672c\u548c\u73a9\u5177\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u63d0\u4f9b\u53ef\u9760\u3001\u53ef\u6d4b\u8bd5\u7684\u5de5\u4f5c\u6d41\u7ba1\u7406\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfCron\u4f5c\u4e1a\u548c\u7b80\u5355\u81ea\u52a8\u5316\u5de5\u5177\u5728\u5904\u7406\u7c98\u5408\u4ee3\u7801\u3001\u5185\u90e8\u81ea\u52a8\u5316\u548c\u4eea\u8868\u677f\u65f6\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u81ea\u52a8\u5316\u6d41\u7a0b\u7684\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "method": "\u6784\u5efa\u4e13\u95e8\u4e3a\u5f00\u53d1\u8005\u8bbe\u8ba1\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u63d0\u4f9b\u6267\u884c\u65e5\u5fd7\u8bb0\u5f55\u3001\u53ef\u6d4b\u8bd5\u7684\u5de5\u4f5c\u6d41\u3001\u6545\u969c\u6062\u590d\u673a\u5236\uff0c\u5e76\u4e3a\u8fd0\u8425\u4eba\u5458\u548c\u6700\u7ec8\u7528\u6237\u63d0\u4f9b\u5b89\u5168\u62a4\u680f\u3002", "result": "\u80fd\u591f\u5c06\u4e34\u65f6\u811a\u672c\u66ff\u6362\u4e3a\u53ef\u9760\u7684\u5de5\u4f5c\u6d41\uff0c\u4f7f\u6bcf\u4e2a\u6267\u884c\u53ef\u89c1\u3001\u6bcf\u4e2a\u6545\u969c\u53ef\u6062\u590d\uff0c\u540c\u65f6\u8ba9\u5f00\u53d1\u4eba\u5458\u4ece\u7e41\u7410\u7684\u8fd0\u7ef4\u5de5\u4f5c\u4e2d\u89e3\u653e\u51fa\u6765\u3002", "conclusion": "Ondeva\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4e13\u4e1a\u7ea7\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u6d41\u7a0b\u7684\u53ef\u9760\u6027\u548c\u5f00\u53d1\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "tldr.2510.a7fd3d2b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "authors": ["TLDR Newsletter"], "title": "Scales with you", "comment": "Source: TLDR Newsletter, Date: 2025-10-20, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ondeva.com%2Fsolutions%2Fcentralize-automations-and-internal-tools%3Futm_source=newsletter_ads%26utm_campaign=tldr%26utm_content=mon_twenty%26utm_term=l_headline/2/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Z-re9L4CisjjGXlBJCk5tBTCpxRZlStRZ8M4_Rq3gu0=427", "summary": "Ondeva: Replace Fragile Scripts with Reliable Workflows (Sponsor) Managing glue code / internal automations / dashboards? you can do better than fragile Cron jobs or toy automation tools. Ondeva is workflow automation for devs, not hobbyists: Built for reliability: Replaces ad-hoc scripts with logged, testable workflows. Every execution is visible, every failure recoverable. Reclaim dev hours: Ops and end users can build safe automations with guardrails, so you can get back to real engineerin...", "source": "tldr", "AI": {"tldr": "Ondeva\u662f\u4e00\u4e2a\u9762\u5411\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u65e8\u5728\u66ff\u4ee3\u8106\u5f31\u7684\u811a\u672c\u548cCron\u4f5c\u4e1a\uff0c\u63d0\u4f9b\u53ef\u9760\u3001\u53ef\u6d4b\u8bd5\u7684\u5de5\u4f5c\u6d41\u7ba1\u7406\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u811a\u672c\u548c\u81ea\u52a8\u5316\u5de5\u5177\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u7ef4\u62a4\u6210\u672c\u3002", "method": "\u6784\u5efa\u4e13\u95e8\u9488\u5bf9\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u63d0\u4f9b\u6267\u884c\u65e5\u5fd7\u8bb0\u5f55\u3001\u53ef\u6d4b\u8bd5\u6027\u548c\u6545\u969c\u6062\u590d\u529f\u80fd\uff0c\u8ba9\u8fd0\u7ef4\u548c\u7ec8\u7aef\u7528\u6237\u80fd\u591f\u5728\u5b89\u5168\u62a4\u680f\u4e0b\u6784\u5efa\u81ea\u52a8\u5316\u3002", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u66ff\u4ee3\u4e34\u65f6\u811a\u672c\u7684\u53ef\u9760\u5de5\u4f5c\u6d41\u7cfb\u7edf\uff0c\u6bcf\u4e2a\u6267\u884c\u90fd\u53ef\u89c1\uff0c\u6bcf\u4e2a\u6545\u969c\u90fd\u53ef\u6062\u590d\u3002", "conclusion": "Ondeva\u901a\u8fc7\u63d0\u4f9b\u4e13\u4e1a\u7ea7\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u4ece\u7e41\u7410\u7684\u8fd0\u7ef4\u5de5\u4f5c\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u4e13\u6ce8\u4e8e\u6838\u5fc3\u5de5\u7a0b\u4efb\u52a1\u3002", "topic": "swe application"}}
{"id": "tldr.2510.22a2504a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbrowserbase%2Fstagehand%3Futm_source=tldrdevops/1/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Cy7ViySh1Tfs6wu_RAa1_y798J6IwOTuDU-N4Vci8XQ=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbrowserbase%2Fstagehand%3Futm_source=tldrdevops/1/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Cy7ViySh1Tfs6wu_RAa1_y798J6IwOTuDU-N4Vci8XQ=427", "authors": ["TLDR Newsletter"], "title": "Stagehand", "comment": "Source: TLDR Newsletter, Date: 2025-10-20, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbrowserbase%2Fstagehand%3Futm_source=tldrdevops/1/0100019a0151db1d-a53a603b-91d0-4819-abbb-f0de5e999978-000000/Cy7ViySh1Tfs6wu_RAa1_y798J6IwOTuDU-N4Vci8XQ=427", "summary": "Stagehand (GitHub Repo) Stagehand is an AI browser automation framework that allows developers to automate web tasks using both code and natural language. It integrates with Playwright and SOTA computer use models from OpenAI and Anthropic, allowing users to preview and cache actions to save time and tokens.", "source": "tldr", "AI": {"tldr": "Stagehand\u662f\u4e00\u4e2aAI\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u4f7f\u7528\u4ee3\u7801\u548c\u81ea\u7136\u8bed\u8a00\u81ea\u52a8\u5316\u7f51\u9875\u4efb\u52a1\uff0c\u96c6\u6210Playwright\u548cOpenAI\u3001Anthropic\u7684\u6700\u5148\u8fdb\u8ba1\u7b97\u673a\u4f7f\u7528\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7f51\u9875\u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\u9700\u8981\u7f16\u5199\u590d\u6742\u4ee3\u7801\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u65b9\u5f0f\uff0c\u540c\u65f6\u63d0\u9ad8\u81ea\u52a8\u5316\u6548\u7387\u548c\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u96c6\u6210Playwright\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u5de5\u5177\u548cOpenAI\u3001Anthropic\u7684SOTA\u8ba1\u7b97\u673a\u4f7f\u7528\u6a21\u578b\uff0c\u652f\u6301\u52a8\u4f5c\u9884\u89c8\u548c\u7f13\u5b58\u529f\u80fd\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u81ea\u52a8\u5316\u7f51\u9875\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u51cf\u5c11\u4e86\u7f16\u7801\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\u3002", "conclusion": "Stagehand\u6846\u67b6\u6210\u529f\u5730\u5c06\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e0e\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u76f8\u7ed3\u5408\uff0c\u4e3a\u7f51\u9875\u4efb\u52a1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u66f4\u4fbf\u6377\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "tldr.2510.14194574", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fwillcrichton%2Fflowistry%3Futm_source=tldrwebdev/1/0100019a01626ee5-1c115f73-d9e0-4048-a532-f3a27de1054b-000000/o05uNnBs1wXyMGvn_k8cxiXbKtPZnASUVH8Go4WUqaQ=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fwillcrichton%2Fflowistry%3Futm_source=tldrwebdev/1/0100019a01626ee5-1c115f73-d9e0-4048-a532-f3a27de1054b-000000/o05uNnBs1wXyMGvn_k8cxiXbKtPZnASUVH8Go4WUqaQ=427", "authors": ["TLDR Newsletter"], "title": "Flowistry", "comment": "Source: TLDR Newsletter, Date: 2025-10-20, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fwillcrichton%2Fflowistry%3Futm_source=tldrwebdev/1/0100019a01626ee5-1c115f73-d9e0-4048-a532-f3a27de1054b-000000/o05uNnBs1wXyMGvn_k8cxiXbKtPZnASUVH8Go4WUqaQ=427", "summary": "Flowistry (GitHub Repo) Flowistry is a VSCode plugin for Rust that analyzes information flow to help devs focus on relevant code. By fading out irrelevant code, Flowistry allows users to understand the impact of specific variables or expressions, making it easier to comprehend complex functions.", "source": "tldr", "AI": {"tldr": "Flowistry\u662f\u4e00\u4e2aVSCode\u63d2\u4ef6\uff0c\u901a\u8fc7\u4fe1\u606f\u6d41\u5206\u6790\u5e2e\u52a9\u5f00\u53d1\u8005\u805a\u7126\u76f8\u5173\u4ee3\u7801\uff0c\u901a\u8fc7\u6de1\u5316\u65e0\u5173\u4ee3\u7801\u6765\u7406\u89e3\u53d8\u91cf\u6216\u8868\u8fbe\u5f0f\u7684\u5f71\u54cd\u3002", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u597d\u5730\u7406\u89e3\u590d\u6742\u51fd\u6570\u4e2d\u7279\u5b9a\u53d8\u91cf\u6216\u8868\u8fbe\u5f0f\u7684\u5f71\u54cd\u8303\u56f4\uff0c\u63d0\u9ad8\u4ee3\u7801\u7406\u89e3\u6548\u7387\u3002", "method": "\u5f00\u53d1VSCode\u63d2\u4ef6\uff0c\u4f7f\u7528\u4fe1\u606f\u6d41\u5206\u6790\u6280\u672f\uff0c\u8bc6\u522b\u5e76\u6de1\u5316\u663e\u793a\u4e0e\u5f53\u524d\u7126\u70b9\u65e0\u5173\u7684\u4ee3\u7801\u3002", "result": "\u5b9e\u73b0\u4e86\u80fd\u591f\u5206\u6790\u4fe1\u606f\u6d41\u5e76\u53ef\u89c6\u5316\u76f8\u5173\u4ee3\u7801\u7684\u63d2\u4ef6\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u9ad8\u6548\u5730\u7406\u89e3\u4ee3\u7801\u903b\u8f91\u3002", "conclusion": "Flowistry\u901a\u8fc7\u4fe1\u606f\u6d41\u5206\u6790\u6709\u6548\u63d0\u5347\u4e86\u4ee3\u7801\u7406\u89e3\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u51fd\u6570\u65f6\u3002", "topic": "swe application"}}
{"id": "tldr.2510.532c15fb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lennysnewsletter.com%2Fp%2Feveryone-should-be-using-claude-code%3Futm_source=tldrfounders/1/0100019a0189af5f-86313233-2070-4f3a-8397-bf9df6adfa79-000000/Zpbi4k0gjOndYsbC7hyMImdMAXnItb6y5FIb5WDqyhY=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lennysnewsletter.com%2Fp%2Feveryone-should-be-using-claude-code%3Futm_source=tldrfounders/1/0100019a0189af5f-86313233-2070-4f3a-8397-bf9df6adfa79-000000/Zpbi4k0gjOndYsbC7hyMImdMAXnItb6y5FIb5WDqyhY=427", "authors": ["TLDR Newsletter"], "title": "Everyone should be using Claude Code more", "comment": "Source: TLDR Newsletter, Date: 2025-10-20, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lennysnewsletter.com%2Fp%2Feveryone-should-be-using-claude-code%3Futm_source=tldrfounders/1/0100019a0189af5f-86313233-2070-4f3a-8397-bf9df6adfa79-000000/Zpbi4k0gjOndYsbC7hyMImdMAXnItb6y5FIb5WDqyhY=427", "summary": "Everyone should be using Claude Code more (5 minute read) Claude Code offers versatile AI capabilities for non-technical users, running locally to manage tasks like file organization, image enhancement, and video downloads. It excels in handling large files efficiently and performing tasks beyond cloud-based AI tools. Users share creative applications, including generating domain names, synthesizing customer call transcripts, and creating self-driving documentation.", "source": "tldr", "AI": {"tldr": "Claude Code\u662f\u4e00\u6b3e\u9002\u7528\u4e8e\u975e\u6280\u672f\u7528\u6237\u7684\u672c\u5730AI\u5de5\u5177\uff0c\u80fd\u9ad8\u6548\u5904\u7406\u6587\u4ef6\u7ec4\u7ec7\u3001\u56fe\u50cf\u589e\u5f3a\u3001\u89c6\u9891\u4e0b\u8f7d\u7b49\u4efb\u52a1\uff0c\u529f\u80fd\u8d85\u8d8a\u4e91\u7aefAI\u5de5\u5177\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u975e\u6280\u672f\u7528\u6237\u5bf9AI\u5de5\u5177\u7684\u9700\u6c42\uff0c\u5f00\u53d1\u4e00\u6b3e\u80fd\u5728\u672c\u5730\u8fd0\u884c\u3001\u529f\u80fd\u591a\u6837\u4e14\u80fd\u5904\u7406\u5927\u6587\u4ef6\u7684AI\u52a9\u624b\u3002", "method": "\u5f00\u53d1\u672c\u5730\u8fd0\u884c\u7684Claude Code AI\u5de5\u5177\uff0c\u652f\u6301\u6587\u4ef6\u7ba1\u7406\u3001\u56fe\u50cf\u5904\u7406\u3001\u89c6\u9891\u4e0b\u8f7d\u7b49\u591a\u79cd\u529f\u80fd\uff0c\u5e76\u6536\u96c6\u7528\u6237\u521b\u610f\u5e94\u7528\u6848\u4f8b\u3002", "result": "\u7528\u6237\u6210\u529f\u4f7f\u7528Claude Code\u8fdb\u884c\u57df\u540d\u751f\u6210\u3001\u5ba2\u6237\u901a\u8bdd\u8bb0\u5f55\u5408\u6210\u3001\u81ea\u52a8\u9a7e\u9a76\u6587\u6863\u521b\u5efa\u7b49\u591a\u6837\u5316\u5e94\u7528\uff0c\u8bc1\u660e\u5176\u529f\u80fd\u5f3a\u5927\u4e14\u5b9e\u7528\u3002", "conclusion": "Claude Code\u662f\u4e00\u6b3e\u529f\u80fd\u5168\u9762\u3001\u8fd0\u884c\u9ad8\u6548\u7684\u672c\u5730AI\u5de5\u5177\uff0c\u9002\u5408\u5404\u7c7b\u7528\u6237\u4f7f\u7528\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002", "topic": "swe application"}}
{"id": "wechat.2510.c5856c8b", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU3MDg1MzY4NQ==&mid=2247563218&idx=3&sn=bbfcf2886b390824e1e25359f1fad3b1&chksm=fd4b9cf842b69d879ea8c2fc764d56f066bcf2be9ab005bd3c6225ae90008ba74bf49287055c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU3MDg1MzY4NQ==&mid=2247563218&idx=3&sn=bbfcf2886b390824e1e25359f1fad3b1&chksm=fd4b9cf842b69d879ea8c2fc764d56f066bcf2be9ab005bd3c6225ae90008ba74bf49287055c#rd", "authors": ["CAA OFFICIAL"], "title": "\u3010\u884c\u4e1a\u524d\u6cbf\u3011RewardMap: \u901a\u8fc7\u591a\u9636\u6bb5<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u89e3\u51b3\u7ec6\u7c92\u5ea6\u89c6\u89c9\u63a8\u7406\u7684Sparse Reward", "comment": "Source: WeChat, Published: 2025-10-21 13:25:23", "summary": "\u56e2\u961f\u5728\u5927\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u6a21\u6001\u63a8\u7406\u65b9\u5411\u5177\u6709\u6df1\u539a\u7814\u7a76\u57fa\u7840\u3002\u8fd1\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ee5\u53ca\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u79cd\u573a\u666f\u7406\u89e3\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u7a81\u7834\u6027\u8fdb\u5c55\u3002", "AI": {"tldr": "\u56e2\u961f\u5728\u5927\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u6a21\u6001\u63a8\u7406\u65b9\u5411\u5177\u6709\u6df1\u539a\u7814\u7a76\u57fa\u7840\u3002\u8fd1\u5e74\u6765\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ee5\u53ca\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u79cd\u573a\u666f\u7406\u89e3\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u7a81\u7834\u6027\u8fdb\u5c55\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.cf877f4e", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU5NjQ2MjE2NA==&mid=2247486187&idx=1&sn=e3feedbc5737918e4a5d08f8c3587ea5&chksm=ff9b1acebc47fc134d4cca9342abdf9d4fdcba7fa3d167c5f14be27bad1e773eea0dc9a24381#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU5NjQ2MjE2NA==&mid=2247486187&idx=1&sn=e3feedbc5737918e4a5d08f8c3587ea5&chksm=ff9b1acebc47fc134d4cca9342abdf9d4fdcba7fa3d167c5f14be27bad1e773eea0dc9a24381#rd", "authors": ["\u56fe\u7075AI\u4e91"], "title": "\u201c\u4ee5\u5f31\u9a6d\u5f3a\u201d\uff08Weak-for-Strong, W4S\uff09\uff1a\u4e00\u79cd\u65b0\u578b<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6846\u67b6\uff0c\u8bad\u7ec3\u5c0f\u578b\u5143\u667a\u80fd\u4f53\u8bbe\u8ba1\u8c03\u7528\u66f4\u5f3a\u6267\u884c\u6a21\u578b\u7684\u4ee3\u7801\u5de5\u4f5c\u6d41", "comment": "Source: WeChat, Published: 2025-10-21 13:00:36", "summary": "RLAO\uff1a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u4f18\u5316\u7b56\u7565RLAO \u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u8f6e\u8f68\u8ff9\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08offline RL\uff09\u65b9\u6cd5\u3002\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u7cfb\u7edf\u4f1a\u91c7\u6837\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\uff0c\u4fdd\u7559\u8868\u73b0\u6700\u597d\u7684\u4e00\u4e2a\u7528\u4e8e\u63a8\u8fdb\u72b6\u6001\uff0c\u5176\u4f59\u5219\u5b58\u5165\u56de\u653e\u7f13\u51b2\u533a\u7528\u4e8e\u540e\u7eed\u8bad\u7ec3\u3002", "AI": {"tldr": "RLAO\uff1a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u4f18\u5316\u7b56\u7565RLAO \u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u8f6e\u8f68\u8ff9\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08offline RL\uff09\u65b9\u6cd5\u3002\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u7cfb\u7edf\u4f1a\u91c7\u6837\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\uff0c\u4fdd\u7559\u8868\u73b0\u6700\u597d\u7684\u4e00\u4e2a\u7528\u4e8e\u63a8\u8fdb\u72b6\u6001\uff0c\u5176\u4f59\u5219\u5b58\u5165\u56de\u653e\u7f13\u51b2\u533a\u7528\u4e8e\u540e\u7eed\u8bad\u7ec3\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.6cbfab4b", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&mid=2247633621&idx=1&sn=d84fce608c041972b2d57674ea4e9a6a&chksm=fa89cfd026a756c538064d37af637160a66ec2ae23bc67640145def37795f4e06731d014d69e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU0NjgzMDIxMQ==&mid=2247633621&idx=1&sn=d84fce608c041972b2d57674ea4e9a6a&chksm=fa89cfd026a756c538064d37af637160a66ec2ae23bc67640145def37795f4e06731d014d69e#rd", "authors": ["\u5c0f\u767d\u5b66\u89c6\u89c9"], "title": "\u6beb\u65e0\u7591\u95ee\uff0c\u672a\u6765AI\u754c\u5c06\u4f1a\u662f<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u5929\u4e0b", "comment": "Source: WeChat, Published: 2025-10-21 11:03:12", "summary": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "AI": {"tldr": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.45fc4e5c", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571512&idx=2&sn=8f430ff37e5c5d40ad844556aeab51bc&chksm=96b7ecc0b0c13e7b57e7ac0c639dcf921b7afa058187b9c1f1a735c9f055eee5f874622c0bf7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571512&idx=2&sn=8f430ff37e5c5d40ad844556aeab51bc&chksm=96b7ecc0b0c13e7b57e7ac0c639dcf921b7afa058187b9c1f1a735c9f055eee5f874622c0bf7#rd", "authors": ["\u6df1\u5ea6\u5b66\u4e60\u4e0eNLP"], "title": "RL\u7cbe\u9009\u4e2d\u6587\u4e66\u7c4d-\u300a\u8f7b\u677eRL<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e2d\u6587\u6559\u7a0b\u300b\u514d\u8d39\u5206\u4eab", "comment": "Source: WeChat, Published: 2025-10-21 09:00:00", "summary": "\u76ee\u5f55 \u7b2c1\u7ae0\u5f3a\u5316\u5b66\u4e60\u6982\u8ff0\u30021.1 \u5f3a\u5316\u5b66\u4e60\u3002...... ....... ....... ....... .....\u30021.1.1 \u5f3a\u5316\u5b66\u4e60\u4e0e\u76d1\u7763\u5b66\u4e60\u3002...... ....... ....... ....... ....... .\u30021\u30021.1.2 \u5f3a\u5316\u5b66\u4e60\u7684\u7531\u6765\u4e0e\u5206\u7c7b...... ....... ....... ....... .......\u3002", "AI": {"tldr": "\u76ee\u5f55 \u7b2c1\u7ae0\u5f3a\u5316\u5b66\u4e60\u6982\u8ff0\u30021.1 \u5f3a\u5316\u5b66\u4e60\u3002...... ....... ....... ....... .....\u30021.1.1 \u5f3a\u5316\u5b66\u4e60\u4e0e\u76d1\u7763\u5b66\u4e60\u3002...... ....... ....... ....... ....... .\u30021\u30021.1.2 \u5f3a\u5316\u5b66\u4e60\u7684\u7531\u6765\u4e0e\u5206\u7c7b...... ....... ....... ....... .......\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.63e25c72", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247626053&idx=1&sn=ab59d700061d7fb15c4004617382fa0e&chksm=f8c2480fc18534dc8be6e2300cb74bf3270e332201ec8b91819d0de82b50cda0fa346de8c419#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247626053&idx=1&sn=ab59d700061d7fb15c4004617382fa0e&chksm=f8c2480fc18534dc8be6e2300cb74bf3270e332201ec8b91819d0de82b50cda0fa346de8c419#rd", "authors": ["CVer"], "title": "\u9876\u4f1a\u4e0a\u7684\u738b\u8005\uff01\u8c37\u6b4c\u5fae\u8f6fAll in<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-10-21 05:05:59", "summary": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "AI": {"tldr": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.36609c3d", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MjYwMTc5Nw==&mid=2649003958&idx=1&sn=897269bb69ae46ffb8b23c917542f030&chksm=869c08acaa57458d3da1595b7a9999d5f4cc70006bb0f219f95c4c52fe729f1081da7794cc91#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MjYwMTc5Nw==&mid=2649003958&idx=1&sn=897269bb69ae46ffb8b23c917542f030&chksm=869c08acaa57458d3da1595b7a9999d5f4cc70006bb0f219f95c4c52fe729f1081da7794cc91#rd", "authors": ["Ai\u5b66\u4e60\u7684\u8001\u7ae0"], "title": "\u6beb\u65e0\u7591\u95ee\uff0c\u672a\u6765AI\u754c\u5c06\u4f1a\u662f<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u5929\u4e0b", "comment": "Source: WeChat, Published: 2025-10-21 03:28:09", "summary": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "AI": {"tldr": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.36246a33", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzNjE1Nzc1MA==&mid=2247507703&idx=1&sn=f4adf653496b11f6e8015b6cb5027fd5&chksm=fbba9122ada51681a5784f4f963ff1c78e9353b553c2ce9a558466fa32d89800716074bc9330#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzNjE1Nzc1MA==&mid=2247507703&idx=1&sn=f4adf653496b11f6e8015b6cb5027fd5&chksm=fbba9122ada51681a5784f4f963ff1c78e9353b553c2ce9a558466fa32d89800716074bc9330#rd", "authors": ["\u5565\u90fd\u4f1a\u4e00\u70b9\u7684\u7814\u7a76\u751f"], "title": "\u6beb\u65e0\u7591\u95ee\uff0c<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5730\u4f4d\u8fd8\u5728\u4e0d\u65ad\u4e0a\u5347\uff01", "comment": "Source: WeChat, Published: 2025-10-21 03:25:33", "summary": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "AI": {"tldr": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.ec814b8f", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671466&idx=4&sn=0ea1c52eb57f9ca4afbb0697f221efeb&chksm=fdc7946c5b3f881cc8692836d498b761730a4b69b6e9dc3bdf84a7e6f92d49114ef93c56e4a0#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671466&idx=4&sn=0ea1c52eb57f9ca4afbb0697f221efeb&chksm=fdc7946c5b3f881cc8692836d498b761730a4b69b6e9dc3bdf84a7e6f92d49114ef93c56e4a0#rd", "authors": ["\u4e13\u77e5"], "title": "\u3010NeurIPS2025\u3011\u8fc8\u5411\u9c81\u68d2\u7684\u96f6\u6837\u672c<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-10-21 03:02:26", "summary": "\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\uff08zero-shot reinforcement learning\uff0c rl\uff09\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u4e3a\u5b66\u4e60\u80fd\u591f\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u9002\u5e94\u4efb\u610f\u65b0\u4efb\u52a1\u7684\u9884\u8bad\u7ec3\u901a\u7528\u7b56\u7565\uff08pre-trained generalist policies\uff09\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002", "AI": {"tldr": "\u96f6\u6837\u672c\u5f3a\u5316\u5b66\u4e60\uff08zero-shot reinforcement learning\uff0c rl\uff09\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u4e3a\u5b66\u4e60\u80fd\u591f\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u9002\u5e94\u4efb\u610f\u65b0\u4efb\u52a1\u7684\u9884\u8bad\u7ec3\u901a\u7528\u7b56\u7565\uff08pre-trained generalist policies\uff09\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.467eaf11", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3MTgzOTg5NQ==&mid=2247497483&idx=1&sn=285df83f6f7a10e34da06937cae15bea&chksm=cf34abfa02daf8290e7a20eb026c446dc88f257427a238a0977f5ab7ed314617719f6f1b79a3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3MTgzOTg5NQ==&mid=2247497483&idx=1&sn=285df83f6f7a10e34da06937cae15bea&chksm=cf34abfa02daf8290e7a20eb026c446dc88f257427a238a0977f5ab7ed314617719f6f1b79a3#rd", "authors": ["\u6d6a\u6f6e\u4fe1\u606f\u7cbe\u82f1\u5408\u4f5c\u4f19\u4f34"], "title": "\u6d6a\u6f6e\u4fe1\u606f\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u667a\u80fd\u9a7e\u9a76<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6846\u67b6\uff0c60\u79d2\u53ef\u751f\u6210100\u4e07\u6837\u672c", "comment": "Source: WeChat, Published: 2025-10-21 01:39:46", "summary": "\u5f3a\u5316\u5b66\u4e60\u5728\u89e3\u51b3\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u548c\u63a7\u5236\u4efb\u52a1\u65b9\u9762\u663e\u793a\u51fa\u8d8a\u6765\u8d8a\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f7f\u5f97\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u8def\u7ebf\u6709\u671b\u8d85\u8fc7rule-based\u7684\u4f20\u7edf\u667a\u9a7e\u6280\u672f\u8def\u7ebf\uff0c\u6210\u4e3a\u667a\u9a7e\u7684\u4e3b\u6d41\u6280\u672f\u8def\u7ebf\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u5728\u89e3\u51b3\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u548c\u63a7\u5236\u4efb\u52a1\u65b9\u9762\u663e\u793a\u51fa\u8d8a\u6765\u8d8a\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f7f\u5f97\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u8def\u7ebf\u6709\u671b\u8d85\u8fc7rule-based\u7684\u4f20\u7edf\u667a\u9a7e\u6280\u672f\u8def\u7ebf\uff0c\u6210\u4e3a\u667a\u9a7e\u7684\u4e3b\u6d41\u6280\u672f\u8def\u7ebf\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.27437fdc", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2ODcyNzQxMA==&mid=2247485594&idx=1&sn=e83d295fa9523b921ba4cf39bfa24a6c&chksm=cfc1d950178f0b6e7037db46ca633a2f2d3e55b013238a1a17cf7445bf0eb9cbdc5e958f010c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2ODcyNzQxMA==&mid=2247485594&idx=1&sn=e83d295fa9523b921ba4cf39bfa24a6c&chksm=cfc1d950178f0b6e7037db46ca633a2f2d3e55b013238a1a17cf7445bf0eb9cbdc5e958f010c#rd", "authors": ["Deep AIR\u6df1\u5f71"], "title": "nature | DeepSeek-R1\uff1a\u7eaf\u9760<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff0c\u5927\u6a21\u578b\u63a8\u7406\u80fd\u529b\u5b9e\u73b0\u81ea\u4e3b\u8fdb\u5316", "comment": "Source: WeChat, Published: 2025-10-21 00:30:00", "summary": "Extended Data Fig. 2 | GRPO \u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\u56fe\u793a\u3002\u6a21\u578b\u4f5c\u4e3a\u7b56\u7565\u7f51\u7edc\u751f\u6210\u591a\u4e2a\u54cd\u5e94\uff0c\u5956\u52b1\u6a21\u578b\uff08\u57fa\u4e8e\u89c4\u5219\u6216\u6a21\u578b\uff09\u5bf9\u6bcf\u4e2a\u54cd\u5e94\u8fdb\u884c\u6253\u5206\uff0cGRPO\u6839\u636e\u7ec4\u5185\u5956\u52b1\u5206\u5e03\u76f4\u63a5\u8ba1\u7b97\u4f18\u52bf\uff0c\u5e76\u66f4\u65b0\u7b56\u7565\u53c2\u6570\u3002", "AI": {"tldr": "Extended Data Fig. 2 | GRPO \u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\u56fe\u793a\u3002\u6a21\u578b\u4f5c\u4e3a\u7b56\u7565\u7f51\u7edc\u751f\u6210\u591a\u4e2a\u54cd\u5e94\uff0c\u5956\u52b1\u6a21\u578b\uff08\u57fa\u4e8e\u89c4\u5219\u6216\u6a21\u578b\uff09\u5bf9\u6bcf\u4e2a\u54cd\u5e94\u8fdb\u884c\u6253\u5206\uff0cGRPO\u6839\u636e\u7ec4\u5185\u5956\u52b1\u5206\u5e03\u76f4\u63a5\u8ba1\u7b97\u4f18\u52bf\uff0c\u5e76\u66f4\u65b0\u7b56\u7565\u53c2\u6570\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.35363f2d", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571521&idx=3&sn=f63f60c6f7087bce6e953d07fc267000&chksm=968f0fe14e3cf7cbaae95e3b3e00bc314d57e29195c73765f8771d0032a84e2f9e6b71e9cdd3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571521&idx=3&sn=f63f60c6f7087bce6e953d07fc267000&chksm=968f0fe14e3cf7cbaae95e3b3e00bc314d57e29195c73765f8771d0032a84e2f9e6b71e9cdd3#rd", "authors": ["\u6df1\u5ea6\u5b66\u4e60\u4e0eNLP"], "title": "Andrej Karpathy \u5f00\u70ae\uff1a\u667a\u80fd\u4f53\u90fd\u5728\u88c5\u6837\u5b50\uff0c<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5f88\u7cdf\u7cd5\uff0cAGI \u5341\u5e74\u4e5f\u51fa\u4e0d\u6765", "comment": "Source: WeChat, Published: 2025-10-21 00:03:26", "summary": "\u5f3a\u5316\u5b66\u4e60\u662f\u4eba\u5de5\u667a\u80fd\u7684\u53e6\u4e00\u4e2a\u91cd\u8981\u9886\u57df\uff0c\u5927\u6982\u6709\u4e24\u4e09\u5e74\u751a\u81f3\u56db\u5e74\u7684\u65f6\u95f4\uff0c\u6bcf\u4e2a\u4eba\u90fd\u5728\u6e38\u620f\u4e0a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002\u8fd9\u5b8c\u5168\u662f\u4e00\u4e2a\u5931\u8bef\u3002\u6211\u5728 OpenAI \u5c1d\u8bd5\u505a\u7684\u4e8b\u60c5\u662f\uff0c\u6211\u4e00\u76f4\u5bf9\u6e38\u620f\u80fd\u5426\u5f15\u9886 AGI \u6709\u70b9\u6000\u7591 \u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u662f\u4eba\u5de5\u667a\u80fd\u7684\u53e6\u4e00\u4e2a\u91cd\u8981\u9886\u57df\uff0c\u5927\u6982\u6709\u4e24\u4e09\u5e74\u751a\u81f3\u56db\u5e74\u7684\u65f6\u95f4\uff0c\u6bcf\u4e2a\u4eba\u90fd\u5728\u6e38\u620f\u4e0a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002\u8fd9\u5b8c\u5168\u662f\u4e00\u4e2a\u5931\u8bef\u3002\u6211\u5728 OpenAI \u5c1d\u8bd5\u505a\u7684\u4e8b\u60c5\u662f\uff0c\u6211\u4e00\u76f4\u5bf9\u6e38\u620f\u80fd\u5426\u5f15\u9886 AGI \u6709\u70b9\u6000\u7591 \u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.7fc63650", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571521&idx=1&sn=f50883605c7a52c677cc45842c2a6110&chksm=9652a2999202c006408072d38751024a91660b07498608f6e29d78db895ae01ec7da48f66625#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571521&idx=1&sn=f50883605c7a52c677cc45842c2a6110&chksm=9652a2999202c006408072d38751024a91660b07498608f6e29d78db895ae01ec7da48f66625#rd", "authors": ["\u6df1\u5ea6\u5b66\u4e60\u4e0eNLP"], "title": "\u7a81\u53d1\uff01<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>RL\u5f7b\u5e95\u51c9\u51c9\u4e86\u3002\u3002\u3002\uff01\uff1f", "comment": "Source: WeChat, Published: 2025-10-21 00:00:00", "summary": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "AI": {"tldr": "\u9488\u5bf9\u67d0\u4e00\u7c7b\u660e\u786e\u95ee\u9898\uff08\u6bd4\u5982\u591a\u76ee\u6807\u3001\u7ec4\u5408\u4f18\u5316\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u6a21\u5f0f\u3002Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.7ade7765", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyMDQ5NzI5Mg==&mid=2247607636&idx=1&sn=049c0e982eca7f8e3fa32562762f5db8&chksm=f8939b6fd13d5792bc6821b226227109264a027d08b72842515697d19bf5e0f6d95c2536c6aa#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyMDQ5NzI5Mg==&mid=2247607636&idx=1&sn=049c0e982eca7f8e3fa32562762f5db8&chksm=f8939b6fd13d5792bc6821b226227109264a027d08b72842515697d19bf5e0f6d95c2536c6aa#rd", "authors": ["\u6df7\u6c8c\u5b66\u56ed"], "title": "2026AI <em class=\"highlight\">Agent</em>\u516d\u5927\u8d8b\u52bf\uff0c\u7f16\u7a0b\u70ed\u6f6e\u540e\u8c01\u662f\u4e0b\u4e00\u4e2a\u98ce\u53e3\uff1f", "comment": "Source: WeChat, Published: 2025-10-21 12:44:14", "summary": "4.\u667a\u80fd\u4f53\u5f0f\uff08agentic\uff09\u5546\u4e1a\u6a21\u5f0f\u7684\u57fa\u7840\u6b63\u5728\u5de9\u56fa\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u8d2d\u7269\u7684\u6700\u5927\u969c\u788d\u4e4b\u4e00\uff0c\u5728\u4e8e\u5982\u4f55\u4fc3\u6210\u5b89\u5168\u3001\u5b9e\u65f6\u7684\u4ea4\u6613\u3002\u65b0\u4e00\u6279\u521d\u521b\u516c\u53f8\u6b63\u5728\u6b63\u9762\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u4ed6\u4eec\u6b63\u5728\u6784\u5efaAI\u539f\u751f\u652f\u4ed8\u8f68\u9053\u548c\u6570\u5b57\u94b1\u5305\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u6388\u6743\u5e76\u9650\u5236AI\u667a", "AI": {"tldr": "4.\u667a\u80fd\u4f53\u5f0f\uff08agentic\uff09\u5546\u4e1a\u6a21\u5f0f\u7684\u57fa\u7840\u6b63\u5728\u5de9\u56fa\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u8d2d\u7269\u7684\u6700\u5927\u969c\u788d\u4e4b\u4e00\uff0c\u5728\u4e8e\u5982\u4f55\u4fc3\u6210\u5b89\u5168\u3001\u5b9e\u65f6\u7684\u4ea4\u6613\u3002\u65b0\u4e00\u6279\u521d\u521b\u516c\u53f8\u6b63\u5728\u6b63\u9762\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u4ed6\u4eec\u6b63\u5728\u6784\u5efaAI\u539f\u751f\u652f\u4ed8\u8f68\u9053\u548c\u6570\u5b57\u94b1\u5305\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u6388\u6743\u5e76\u9650\u5236AI\u667a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.7bda8dd4", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwNTc1NjY5Nw==&mid=2247484683&idx=1&sn=4bd948d00736e4829d71f8fb6272457f&chksm=c1bc52c3d5675f57b184bc0168ddd67b9b2eb7569a9756a7edb51ea43bc80dd43e1fef1671bd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwNTc1NjY5Nw==&mid=2247484683&idx=1&sn=4bd948d00736e4829d71f8fb6272457f&chksm=c1bc52c3d5675f57b184bc0168ddd67b9b2eb7569a9756a7edb51ea43bc80dd43e1fef1671bd#rd", "authors": ["\u5341\u65b9\u5f15\u529b"], "title": "\u5341\u65b9\u667a\u5e93 | \u6bd5\u9a6c\u5a01\u300aAgentic\u00a0AI\u4f18\u52bf\uff1a\u91ca\u653e\u4e0b\u4e00\u7ea7\u522b\u7684\u4ef7\u503c\u300b\uff08\u9644\u4e0b\u8f7d\uff09", "comment": "Source: WeChat, Published: 2025-10-21 11:00:22", "summary": "\u56db\u5927\u5e94\u7528\u6846\u67b6\uff1a\u6309\u9700\u9009\u62e9Agentic\u7c7b\u578b\u6bd5\u9a6c\u5a01\u63d0\u51faTACO\u6846\u67b6\uff0c\u5c06Agentic AI\u5206\u4e3a\u56db\u7c7b\uff0c\u5bf9\u5e94\u4e0d\u540c\u4e1a\u52a1\u573a\u666f\u25c6 \u4efb\u52a1\u6267\u884c\u8005\uff1a\u5904\u7406\u89c4\u5219\u660e\u786e\u7684\u4efb\u52a1\uff08\u5982\u53d1\u7968\u5ba1\u6838\u3001\u5408\u89c4\u7b5b\u67e5\uff09\uff0c\u9002\u7528\u4e8e\u91cd\u590d\u6027\u6d41\u7a0b\u3002", "AI": {"tldr": "\u56db\u5927\u5e94\u7528\u6846\u67b6\uff1a\u6309\u9700\u9009\u62e9Agentic\u7c7b\u578b\u6bd5\u9a6c\u5a01\u63d0\u51faTACO\u6846\u67b6\uff0c\u5c06Agentic AI\u5206\u4e3a\u56db\u7c7b\uff0c\u5bf9\u5e94\u4e0d\u540c\u4e1a\u52a1\u573a\u666f\u25c6 \u4efb\u52a1\u6267\u884c\u8005\uff1a\u5904\u7406\u89c4\u5219\u660e\u786e\u7684\u4efb\u52a1\uff08\u5982\u53d1\u7968\u5ba1\u6838\u3001\u5408\u89c4\u7b5b\u67e5\uff09\uff0c\u9002\u7528\u4e8e\u91cd\u590d\u6027\u6d41\u7a0b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.6de8b3d8", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMTI1ODM4MQ==&mid=2247484048&idx=1&sn=7cf159880ebb33626b4af517526f253e&chksm=fead07f5f8a4f474751a098b33f1ac6ebfb4216ea588a375bb4ddb98c8b8fdb30fcef9939396#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMTI1ODM4MQ==&mid=2247484048&idx=1&sn=7cf159880ebb33626b4af517526f253e&chksm=fead07f5f8a4f474751a098b33f1ac6ebfb4216ea588a375bb4ddb98c8b8fdb30fcef9939396#rd", "authors": ["AI \u77e5\u884c\u793e Lab"], "title": "<em class=\"highlight\">Agentic</em> \u6846\u67b6\u7cfb\u5217\uff08\u4e00\uff09\uff1aAI \u4e0d\u518d\u53ea\u662f\u201c\u7b49\u4f60\u63d0\u95ee\u201d", "comment": "Source: WeChat, Published: 2025-10-21 09:00:51", "summary": "agentic \u6846\u67b6\uff1a\u4ece\u5bf9\u8bdd\u5de5\u5177\u5230\u81ea\u4e3b\u7cfb\u7edf\u3002\u5bf9\u8bdd\u5de5\u5177 \u81ea\u4e3b\u7cfb\u7edf planning reflect learn \u56db\u5927\u6838\u5fc3\u7ec4\u6210Agentic \u6846\u67b6\u7684\u57fa\u672c\u6a21\u5757\u5982\u4e0b\uff1a\u6a21\u5757\u4f5c\u7528\u793a\u4f8bMemory\uff08\u8bb0\u5fc6\uff09\u8ba9\u6a21\u578b\u8bb0\u4f4f\u4e0a\u4e0b\u6587\u3001\u5386\u53f2\u4efb\u52a1\u6216\u7528\u6237\u504f\u597d", "AI": {"tldr": "agentic \u6846\u67b6\uff1a\u4ece\u5bf9\u8bdd\u5de5\u5177\u5230\u81ea\u4e3b\u7cfb\u7edf\u3002\u5bf9\u8bdd\u5de5\u5177 \u81ea\u4e3b\u7cfb\u7edf planning reflect learn \u56db\u5927\u6838\u5fc3\u7ec4\u6210Agentic \u6846\u67b6\u7684\u57fa\u672c\u6a21\u5757\u5982\u4e0b\uff1a\u6a21\u5757\u4f5c\u7528\u793a\u4f8bMemory\uff08\u8bb0\u5fc6\uff09\u8ba9\u6a21\u578b\u8bb0\u4f4f\u4e0a\u4e0b\u6587\u3001\u5386\u53f2\u4efb\u52a1\u6216\u7528\u6237\u504f\u597d", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.ce7cb831", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3NTY3Mjk1MA==&mid=2247483813&idx=1&sn=7cde6572ebab36f9d897d27e79dede20&chksm=cebd1034c8fb696da9e68dbee28070ec5897bd9a4f657a7d118909549c27bf982ce4c79156d4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3NTY3Mjk1MA==&mid=2247483813&idx=1&sn=7cde6572ebab36f9d897d27e79dede20&chksm=cebd1034c8fb696da9e68dbee28070ec5897bd9a4f657a7d118909549c27bf982ce4c79156d4#rd", "authors": ["\u524d\u7aef\u901a\u9053"], "title": "\u5f53\u6211\u4eec\u8bf4 <em class=\"highlight\">Agentic</em> \u7684\u65f6\u5019\uff0c\u6211\u4eec\u5728\u8bf4\u4ec0\u4e48?", "comment": "Source: WeChat, Published: 2025-10-21 07:00:09", "summary": "\u4ec0\u4e48\u662f Agentic\uff1fAgentic \u6e90\u81ea \"Agent\"\uff08\u4ee3\u7406\uff09\uff0c\u5728 AI \u9886\u57df\u7279\u6307\u5177\u6709\u81ea\u4e3b\u6027\u3001\u76ee\u6807\u5bfc\u5411\u548c\u51b3\u7b56\u80fd\u529b\u7684\u667a\u80fd\u7cfb\u7edf\u3002\u4e0e\u4f20\u7edf\u7684\u88ab\u52a8\u5f0f AI \u7cfb\u7edf\u4e0d\u540c\uff0cAgentic \u7cfb\u7edf\u80fd\u591f\uff1a \u81ea\u4e3b\u89c4\u5212\uff1a\u6839\u636e\u76ee\u6807\u5236\u5b9a\u6267\u884c\u8ba1\u5212", "AI": {"tldr": "\u4ec0\u4e48\u662f Agentic\uff1fAgentic \u6e90\u81ea \"Agent\"\uff08\u4ee3\u7406\uff09\uff0c\u5728 AI \u9886\u57df\u7279\u6307\u5177\u6709\u81ea\u4e3b\u6027\u3001\u76ee\u6807\u5bfc\u5411\u548c\u51b3\u7b56\u80fd\u529b\u7684\u667a\u80fd\u7cfb\u7edf\u3002\u4e0e\u4f20\u7edf\u7684\u88ab\u52a8\u5f0f AI \u7cfb\u7edf\u4e0d\u540c\uff0cAgentic \u7cfb\u7edf\u80fd\u591f\uff1a \u81ea\u4e3b\u89c4\u5212\uff1a\u6839\u636e\u76ee\u6807\u5236\u5b9a\u6267\u884c\u8ba1\u5212", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.88655c9c", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MzY5ODcxOQ==&mid=2247486211&idx=1&sn=b92d049c5f57cb214a76fff876ddb176&chksm=c26a5245bb91ade44b49e447298fc8cb607471464fc8d32cc306bf47c3dc128b1e3bf5b0e90f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MzY5ODcxOQ==&mid=2247486211&idx=1&sn=b92d049c5f57cb214a76fff876ddb176&chksm=c26a5245bb91ade44b49e447298fc8cb607471464fc8d32cc306bf47c3dc128b1e3bf5b0e90f#rd", "authors": ["AI Encyclopedia"], "title": "\u628a\u63e1<em class=\"highlight\">Agentic</em> AI\u5168\u666f\uff1a\u4e00\u5f20\u56fe\u8bfb\u61c28\u5c42\u6280\u672f\u6808\u4e0e\u843d\u5730\u8def\u7ebf", "comment": "Source: WeChat, Published: 2025-10-21 05:02:11", "summary": "\u8fd9\u5f20\u300cAgentic AI Tech Stack\u300d\u56fe\uff0c\u628a\u6784\u5efa\u667a\u80fd\u4ee3\u7406\uff08Agentic AI\uff09\u6240\u9700\u76848\u5927\u5c42\u7ea7\u6d53\u7f29\u5728\u4e00\u9875\uff1a\u90e8\u7f72\u57fa\u7840\u8bbe\u65bd\u3001\u8bc4\u4f30\u76d1\u63a7\u3001\u57fa\u7840\u6a21\u578b\u3001\u7f16\u6392\u6846\u67b6\u3001\u5411\u91cf\u6570\u636e\u5e93\u3001\u5411\u91cf/Embedding \u6a21\u578b\u3001\u6570\u636e\u62bd\u53d6\u3001\u4ee5\u53ca\u8bb0\u5fc6/\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002", "AI": {"tldr": "\u8fd9\u5f20\u300cAgentic AI Tech Stack\u300d\u56fe\uff0c\u628a\u6784\u5efa\u667a\u80fd\u4ee3\u7406\uff08Agentic AI\uff09\u6240\u9700\u76848\u5927\u5c42\u7ea7\u6d53\u7f29\u5728\u4e00\u9875\uff1a\u90e8\u7f72\u57fa\u7840\u8bbe\u65bd\u3001\u8bc4\u4f30\u76d1\u63a7\u3001\u57fa\u7840\u6a21\u578b\u3001\u7f16\u6392\u6846\u67b6\u3001\u5411\u91cf\u6570\u636e\u5e93\u3001\u5411\u91cf/Embedding \u6a21\u578b\u3001\u6570\u636e\u62bd\u53d6\u3001\u4ee5\u53ca\u8bb0\u5fc6/\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.dee78308", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyNzMxOTAwMw==&mid=2247485048&idx=1&sn=4bceff5bb6514bacc86b69ce83b0fca1&chksm=fbc0e87fb2bc708c99bae3d8a1a1545bccc2c894403fd248fa28d60dd9a7b7a849b602ddbcf7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyNzMxOTAwMw==&mid=2247485048&idx=1&sn=4bceff5bb6514bacc86b69ce83b0fca1&chksm=fbc0e87fb2bc708c99bae3d8a1a1545bccc2c894403fd248fa28d60dd9a7b7a849b602ddbcf7#rd", "authors": ["\u4e13\u6ce8\u5b89\u7ba1\u5e73\u53f0"], "title": "\u6d45\u6790SecOps\u4e2d\u7684AI Agent\u548c<em class=\"highlight\">Agentic</em> AI\uff0c\u4ee5\u53caSOC\u81ea\u4e3b\u5316\u6c34\u5e73\u6a21\u578b", "comment": "Source: WeChat, Published: 2025-10-21 04:00:50", "summary": "\u4ece\u7b14\u8005\u63d0\u51fa\u7528Agentic AI\u91cd\u5851SOC\u5e73\u53f0\uff0c\u5e76\u57285\u670821\u65e5\u6b63\u5f0f\u53d1\u5e03\u4e86AI\u8d4b\u80fd+\u6570\u636e\u4e0e\u6d41\u7a0b\u53cc\u8f6e\u9a71\u52a8\u7684SOC4.0\u7406\u5ff5\u548c\u56fd\u5185\u9996\u4e2aAgentic SOP\u4ea7\u54c1\uff0c\u5df2\u7ecf\u8fc7\u53bb\u4e86\u534a\u5e74\u3002\u4eca\u5e74\u4ee5\u6765\uff0c\u5168\u7403\u8303\u56f4\u5185Agentic SOP / SOC\u5982\u96e8\u540e\u6625\u7b0b\u822c\u4e0d\u65ad\u6d8c\u73b0\u3002", "AI": {"tldr": "\u4ece\u7b14\u8005\u63d0\u51fa\u7528Agentic AI\u91cd\u5851SOC\u5e73\u53f0\uff0c\u5e76\u57285\u670821\u65e5\u6b63\u5f0f\u53d1\u5e03\u4e86AI\u8d4b\u80fd+\u6570\u636e\u4e0e\u6d41\u7a0b\u53cc\u8f6e\u9a71\u52a8\u7684SOC4.0\u7406\u5ff5\u548c\u56fd\u5185\u9996\u4e2aAgentic SOP\u4ea7\u54c1\uff0c\u5df2\u7ecf\u8fc7\u53bb\u4e86\u534a\u5e74\u3002\u4eca\u5e74\u4ee5\u6765\uff0c\u5168\u7403\u8303\u56f4\u5185Agentic SOP / SOC\u5982\u96e8\u540e\u6625\u7b0b\u822c\u4e0d\u65ad\u6d8c\u73b0\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.94e574f9", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4NTUxNTE4Ng==&mid=2247527448&idx=2&sn=b6d98c03c92e7f25a5f45396bb435e02&chksm=9e1e3bd36505d350ff474e46b856ad0a7c0594b0d41d44cf14895fce13899f37db6e151b5902#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4NTUxNTE4Ng==&mid=2247527448&idx=2&sn=b6d98c03c92e7f25a5f45396bb435e02&chksm=9e1e3bd36505d350ff474e46b856ad0a7c0594b0d41d44cf14895fce13899f37db6e151b5902#rd", "authors": ["\u673a\u5668\u5b66\u4e60\u4e0e\u63a8\u8350\u7b97\u6cd5"], "title": "<em class=\"highlight\">Agentic</em> RS: <em class=\"highlight\">\u667a\u80fd\u4f53</em>\u63a8\u8350\u7cfb\u7edf\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-10-21 03:00:54", "summary": "\u4f5c\u8005\u63d0\u51fa\u7684 Agentic Recommender System \uff08ARS\uff09 \u662f\u4e00\u79cd\u5177\u5907\u81ea\u4e3b\u51b3\u7b56\u4e0e\u884c\u52a8\u80fd\u529b\u7684\u63a8\u8350\u8303\u5f0f\u3002\u5b83\u7684\u76ee\u6807\u4e0d\u53ea\u662f\u201c\u9884\u6d4b\u4f60\u559c\u6b22\u4ec0\u4e48\u201d\uff0c\u800c\u662f\u201c\u4e3b\u52a8\u7406\u89e3\u4f60\u73b0\u5728\u9700\u8981\u4ec0\u4e48\u3001\u672a\u6765\u53ef\u80fd\u4f1a\u9700\u8981\u4ec0\u4e48\u201d\uff0c\u5e76\u901a\u8fc7\u957f\u671f\u4e92\u52a8\u4e0d\u65ad\u81ea\u6211\u8fdb\u5316\u3002", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u7684 Agentic Recommender System \uff08ARS\uff09 \u662f\u4e00\u79cd\u5177\u5907\u81ea\u4e3b\u51b3\u7b56\u4e0e\u884c\u52a8\u80fd\u529b\u7684\u63a8\u8350\u8303\u5f0f\u3002\u5b83\u7684\u76ee\u6807\u4e0d\u53ea\u662f\u201c\u9884\u6d4b\u4f60\u559c\u6b22\u4ec0\u4e48\u201d\uff0c\u800c\u662f\u201c\u4e3b\u52a8\u7406\u89e3\u4f60\u73b0\u5728\u9700\u8981\u4ec0\u4e48\u3001\u672a\u6765\u53ef\u80fd\u4f1a\u9700\u8981\u4ec0\u4e48\u201d\uff0c\u5e76\u901a\u8fc7\u957f\u671f\u4e92\u52a8\u4e0d\u65ad\u81ea\u6211\u8fdb\u5316\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.09f6f651", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwMTQ1MzMwNQ==&mid=2458175909&idx=1&sn=c14b11fda467ac3ab26e026c177a3b1d&chksm=8096a2c7a02b216a3cd5c8f5372c4cf2b45cf99d211de19337b5be7a92eb6f0aa9c2eb0515be#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwMTQ1MzMwNQ==&mid=2458175909&idx=1&sn=c14b11fda467ac3ab26e026c177a3b1d&chksm=8096a2c7a02b216a3cd5c8f5372c4cf2b45cf99d211de19337b5be7a92eb6f0aa9c2eb0515be#rd", "authors": ["\u6570\u56fe\u7b14\u8bb0"], "title": "\u667a\u80fd\u793e\u4f1a\u7684\u7ec4\u7ec7\u6a21\u5f0f\uff1a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7ec4\u7ec7\uff08<em class=\"highlight\">Agentic</em> Organization\uff09", "comment": "Source: WeChat, Published: 2025-10-21 03:00:00", "summary": "2. \u667a\u80fd\u4f53\u7cfb\u7edf\u590d\u6742\u5ea6\u7684\u9012\u8fdb\uff08\u4ece\u5de5\u5177\u5230\u5f15\u64ce\uff09\u9ea6\u80af\u9521\u5c06\u667a\u80fd\u4f53\u7cfb\u7edf\u5206\u4e3a\u56db\u4e2a\u9636\u6bb5\uff0c\u590d\u6742\u5ea6\u548c\u4ef7\u503c\u9012\u589e\uff0c\u8981\u6c42\u7ec4\u7ec7\u514b\u670d\u4e0d\u540c\u7684\u5173\u952e\u5236\u7ea6\u56e0\u7d20\uff1a\u667a\u80fd\u4f53 \u7cfb\u7edf \u5b9a\u4e49 \u4e3b\u8981\u76ca\u5904 \u5173\u952e\u5236\u7ea6\u56e0\u7d20", "AI": {"tldr": "2. \u667a\u80fd\u4f53\u7cfb\u7edf\u590d\u6742\u5ea6\u7684\u9012\u8fdb\uff08\u4ece\u5de5\u5177\u5230\u5f15\u64ce\uff09\u9ea6\u80af\u9521\u5c06\u667a\u80fd\u4f53\u7cfb\u7edf\u5206\u4e3a\u56db\u4e2a\u9636\u6bb5\uff0c\u590d\u6742\u5ea6\u548c\u4ef7\u503c\u9012\u589e\uff0c\u8981\u6c42\u7ec4\u7ec7\u514b\u670d\u4e0d\u540c\u7684\u5173\u952e\u5236\u7ea6\u56e0\u7d20\uff1a\u667a\u80fd\u4f53 \u7cfb\u7edf \u5b9a\u4e49 \u4e3b\u8981\u76ca\u5904 \u5173\u952e\u5236\u7ea6\u56e0\u7d20", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.373aff2b", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyNDgyNTg2Ng==&mid=2247491039&idx=1&sn=6d9cf2903032c03a7b701bc2e245edbb&chksm=fb255db1c7938aade5ef46c3361b82d4e8db0c68975ce5faa3db742fde8a923ad07f094b36d9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyNDgyNTg2Ng==&mid=2247491039&idx=1&sn=6d9cf2903032c03a7b701bc2e245edbb&chksm=fb255db1c7938aade5ef46c3361b82d4e8db0c68975ce5faa3db742fde8a923ad07f094b36d9#rd", "authors": ["\u4e09\u4e30\u8ff0\u7801"], "title": "<em class=\"highlight\">Agentic</em> AI vs AI Agent\uff1a\u4ece\u201c\u5de5\u5177\u6267\u884c\u8005\u201d\u5230\u201c\u76ee\u6807\u4e2d\u67a2\u201d\u7684\u667a\u80fd\u9769\u547d", "comment": "Source: WeChat, Published: 2025-10-21 02:30:15", "summary": "agentic ai\u3002\u5347\u7ea7\u7248\u667a\u80fd\u4e2d\u67a2\uff1a\u4ec0\u4e48\u662fagentic ai\uff1f\u5982\u679c\u8bf4AI Agent\u662f\u201c\u5de5\u5320\u201d\uff0c\u90a3\u4e48Agentic AI\u5c31\u662f\u9879\u76ee\u7ecf\u7406+\u667a\u80fd\u56e2\u961f\u2014\u2014\u5b83\u662f\u4e00\u4e2a\u76ee\u6807\u5bfc\u5411\u7684\u667a\u80fd\u4e2d\u67a2\uff0c\u80fd\u81ea\u4e3b\u7406\u89e3\u7ec8\u6781\u76ee\u6807\u3001\u89c4\u5212\u6267\u884c\u8def\u5f84\u3001\u534f\u8c03\u591a\u7c7b\u8d44\u6e90\uff08\u5305\u62ec\u591a\u4e2aAI Agent\uff09\uff0c\u5e76\u52a8\u6001\u4f18", "AI": {"tldr": "agentic ai\u3002\u5347\u7ea7\u7248\u667a\u80fd\u4e2d\u67a2\uff1a\u4ec0\u4e48\u662fagentic ai\uff1f\u5982\u679c\u8bf4AI Agent\u662f\u201c\u5de5\u5320\u201d\uff0c\u90a3\u4e48Agentic AI\u5c31\u662f\u9879\u76ee\u7ecf\u7406+\u667a\u80fd\u56e2\u961f\u2014\u2014\u5b83\u662f\u4e00\u4e2a\u76ee\u6807\u5bfc\u5411\u7684\u667a\u80fd\u4e2d\u67a2\uff0c\u80fd\u81ea\u4e3b\u7406\u89e3\u7ec8\u6781\u76ee\u6807\u3001\u89c4\u5212\u6267\u884c\u8def\u5f84\u3001\u534f\u8c03\u591a\u7c7b\u8d44\u6e90\uff08\u5305\u62ec\u591a\u4e2aAI Agent\uff09\uff0c\u5e76\u52a8\u6001\u4f18", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.fe28478d", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2MjgzMTE1NQ==&mid=2247489685&idx=1&sn=64e2f109cd7a46bc450b234bf2b70377&chksm=cfcee17bfac22ced713156acfad99f37efe62b3b1d12529ced7b2ebfdf4b9a97b86142607a88#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2MjgzMTE1NQ==&mid=2247489685&idx=1&sn=64e2f109cd7a46bc450b234bf2b70377&chksm=cfcee17bfac22ced713156acfad99f37efe62b3b1d12529ced7b2ebfdf4b9a97b86142607a88#rd", "authors": ["\u884c\u5ba2\u79d1\u6280"], "title": "\u9ea6\u80af\u9521 <em class=\"highlight\">Agentic</em> AI \u751f\u5b58\u624b\u518c\uff1a\u628a\u201c\u6f14\u793a\u53f0\u4e0a\u7684\u60ca\u8273\u201d\uff0c\u53d8\u6210\u201c\u62a5\u8868\u4e0a\u7684\u4ea7\u51fa\u201d", "comment": "Source: WeChat, Published: 2025-10-21 00:52:05", "summary": "\u2461 Agentic AI\u4ee3\u7406\u4e0d\u662f\u4e07\u91d1\u6cb9\uff1a\u5148\u770b\u201c\u7403\u98ce\u201d\uff0c\u518d\u5b9a\u201c\u9635\u5bb9\u201d\u9ad8\u6807\u51c6\u5316\u3001\u4f4e\u65b9\u5dee\uff08\u5f00\u6237\u62ab\u9732\u3001\u5408\u89c4\u62a5\u9001\uff09\uff1a\u7528\u89c4\u5219/\u4f20\u7edf\u6a21\u578b\u66f4\u53ef\u9760\u66f4\u4fbf\u5b9c\u3002\u4f4e\u6807\u51c6\u5316\u3001\u9ad8\u65b9\u5dee\uff08\u590d\u6742\u6587\u6863\u62bd\u53d6+\u5408\u89c4\u6838\u5bf9\uff09\uff1a\u4ee3\u7406\u4f18\u52bf\u624d\u660e\u663e\u3002", "AI": {"tldr": "\u2461 Agentic AI\u4ee3\u7406\u4e0d\u662f\u4e07\u91d1\u6cb9\uff1a\u5148\u770b\u201c\u7403\u98ce\u201d\uff0c\u518d\u5b9a\u201c\u9635\u5bb9\u201d\u9ad8\u6807\u51c6\u5316\u3001\u4f4e\u65b9\u5dee\uff08\u5f00\u6237\u62ab\u9732\u3001\u5408\u89c4\u62a5\u9001\uff09\uff1a\u7528\u89c4\u5219/\u4f20\u7edf\u6a21\u578b\u66f4\u53ef\u9760\u66f4\u4fbf\u5b9c\u3002\u4f4e\u6807\u51c6\u5316\u3001\u9ad8\u65b9\u5dee\uff08\u590d\u6742\u6587\u6863\u62bd\u53d6+\u5408\u89c4\u6838\u5bf9\uff09\uff1a\u4ee3\u7406\u4f18\u52bf\u624d\u660e\u663e\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.be0fc7c7", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxMTc5MTczMA==&mid=2247484537&idx=1&sn=2cc866b4e1e4cc593e7b684d4fe8b964&chksm=9691514bc77e52b2715a60093d3c63c69f0781847412fbc06c9c5b417077ea5639e6022033c3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxMTc5MTczMA==&mid=2247484537&idx=1&sn=2cc866b4e1e4cc593e7b684d4fe8b964&chksm=9691514bc77e52b2715a60093d3c63c69f0781847412fbc06c9c5b417077ea5639e6022033c3#rd", "authors": ["HR\u79d1\u6280\u8fbe\u4eba"], "title": "<em class=\"highlight\">Agentic</em> AI \u7684\u65b0\u8d77\u70b9\uff1a\u5f53\u6bcf\u4e2a\u4eba\u90fd\u80fd\u201c\u6559\u4f1a\u201dAI \u65b0\u6280\u80fd", "comment": "Source: WeChat, Published: 2025-10-20 23:16:22", "summary": "\u800c\u8fd9\u6b21\u7684\u300cClaude Skills\u300d\uff0c\u611f\u89c9\u53c8\u5728\u5f15\u9886Agentic\u7684AI\u7684\u65b9\u5411\u4e86\u3002Google\u7684A2A\u89e3\u51b3\u4e86Agents\u534f\u4f5c\u7684\u95ee\u9898\uff0c\u4f46\u4e0d\u6d89\u53ca\u672c\u8eabAgents\u80fd\u529b\u7684\u5e73\u6c11\u5316\u3002\u5982\u679c\u5386\u53f2\u518d\u4e00\u6b21\u91cd\u6f14\uff0c\u8c37\u6b4c\u7684\u8fd9\u4e2a\u52a8\u4f5c\u4f1a\u5b9a\u4e49HR AI Agents\u7684\u672a\u6765", "AI": {"tldr": "\u800c\u8fd9\u6b21\u7684\u300cClaude Skills\u300d\uff0c\u611f\u89c9\u53c8\u5728\u5f15\u9886Agentic\u7684AI\u7684\u65b9\u5411\u4e86\u3002Google\u7684A2A\u89e3\u51b3\u4e86Agents\u534f\u4f5c\u7684\u95ee\u9898\uff0c\u4f46\u4e0d\u6d89\u53ca\u672c\u8eabAgents\u80fd\u529b\u7684\u5e73\u6c11\u5316\u3002\u5982\u679c\u5386\u53f2\u518d\u4e00\u6b21\u91cd\u6f14\uff0c\u8c37\u6b4c\u7684\u8fd9\u4e2a\u52a8\u4f5c\u4f1a\u5b9a\u4e49HR AI Agents\u7684\u672a\u6765", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
