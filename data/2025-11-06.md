<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]
- [tldr.article](#tldr.article) [Total: 3]
- [wechat.article](#wechat.article) [Total: 23]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 11]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Cache Mechanism for Agent RAG Systems](https://arxiv.org/abs/2511.02919)
*Shuhang Lin,Zhencan Peng,Lingyao Li,Xiao Lin,Xi Zhu,Yongfeng Zhang*

Main category: cs.CL

TL;DR: ARC是一个新颖的、无需标注的缓存框架，通过动态管理小型高价值语料库，将存储需求降低到原始语料库的0.015%，同时提供高达79.8%的答案命中率，并将平均检索延迟降低80%。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG在提升智能体性能方面取得成功，但针对每个智能体需求动态构建、维护和更新紧凑相关语料库的智能体级缓存管理仍然研究不足。

Method: ARC通过综合历史查询分布模式与缓存项在嵌入空间中的内在几何结构，自动维护高相关性的缓存。

Result: 在三个检索数据集上的实验表明，ARC将存储需求降至原始语料库的0.015%，提供高达79.8%的答案命中率，并将平均检索延迟降低80%。

Conclusion: ARC能够显著提升基于RAG的LLM智能体的效率和有效性。

Abstract: Recent advances in Large Language Model (LLM)-based agents have been
propelled by Retrieval-Augmented Generation (RAG), which grants the models
access to vast external knowledge bases. Despite RAG's success in improving
agent performance, agent-level cache management, particularly constructing,
maintaining, and updating a compact, relevant corpus dynamically tailored to
each agent's need, remains underexplored. Therefore, we introduce ARC (Agent
RAG Cache Mechanism), a novel, annotation-free caching framework that
dynamically manages small, high-value corpora for each agent. By synthesizing
historical query distribution patterns with the intrinsic geometry of cached
items in the embedding space, ARC automatically maintains a high-relevance
cache. With comprehensive experiments on three retrieval datasets, our
experimental results demonstrate that ARC reduces storage requirements to
0.015% of the original corpus while offering up to 79.8% has-answer rate and
reducing average retrieval latency by 80%. Our results demonstrate that ARC can
drastically enhance efficiency and effectiveness in RAG-powered LLM agents.

</details>


### [2] [Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature](https://arxiv.org/abs/2511.03261)
*Ranul Dayarathne,Uvini Ranaweera,Upeksha Ganegoda*

Main category: cs.CL

TL;DR: 本研究比较了四种开源LLM（Mistral-7b-instruct、LLaMa2-7b-chat、Falcon-7b-instruct、Orca-mini-v3-7b）和GPT-3.5在RAG支持下的问答任务表现，发现GPT-3.5表现最佳，而Mistral-7b-instruct在开源模型中表现最好。


<details>
  <summary>Details</summary>
Motivation: 随着RAG技术的重要性日益增加，需要比较不同LLM在问答任务中的性能表现，特别是在计算机科学文献领域的应用。

Method: 使用四种开源LLM和GPT-3.5，在RAG支持下进行问答任务评估，采用准确率、精确率、专家排名、Gemini排名和余弦相似度等指标。

Result: GPT-3.5在RAG支持下表现最佳；开源模型中Mistral-7b-instruct表现最好；Orca-mini-v3-7b响应延迟最短，LLaMa2-7b-chat延迟最高。

Conclusion: 开源LLM在适当基础设施支持下可以与专有模型相媲美，RAG技术能有效增强LLM的问答能力。

Abstract: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to
enhance the capabilities of Generative AI models by reducing hallucination.
Thus, the increasing prominence of RAG alongside Large Language Models (LLMs)
has sparked interest in comparing the performance of different LLMs in
question-answering (QA) in diverse domains. This study compares the performance
of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,
Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA
tasks within the computer science literature leveraging RAG support. Evaluation
metrics employed in the study include accuracy and precision for binary
questions and ranking by a human expert, ranking by Google's AI model Gemini,
alongside cosine similarity for long-answer questions. GPT-3.5, when paired
with RAG, effectively answers binary and long-answer questions, reaffirming its
status as an advanced LLM. Regarding open-source LLMs, Mistral AI's
Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary
and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b
reports the shortest average latency in generating responses, whereas
LLaMa2-7b-chat by Meta reports the highest average latency. This research
underscores the fact that open-source LLMs, too, can go hand in hand with
proprietary models like GPT-3.5 with better infrastructure.

</details>


### [3] [Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG](https://arxiv.org/abs/2511.03410)
*Longpeng Qiu,Ting Li,Shuai Mao,Nan Yang,Xiaohui Yan*

Main category: cs.CL

TL;DR: QuestionRAG是一个解决问答系统中输入错误问题的框架，通过知识增强和强化学习来改善LLM对错误问题的理解和修正能力。


<details>
  <summary>Details</summary>
Motivation: 问答系统中的输入错误常导致错误回答，LLM在处理这类任务时容易误解用户意图或过度修正问题结构。

Method: 使用外部知识（如搜索结果、相关实体）增强输入以解决误解问题；采用强化学习使模型目标与精确修正对齐，而非简单改写。

Result: 知识增强对理解错误问题至关重要；基于RL的对齐方法比传统监督微调更有效，显著提升模型遵循指令和泛化的能力。

Conclusion: 通过整合知识增强和强化学习两种策略，QuestionRAG充分发挥了LLM在问题修正任务中的潜力。

Abstract: Input errors in question-answering (QA) systems often lead to incorrect
responses. Large language models (LLMs) struggle with this task, frequently
failing to interpret user intent (misinterpretation) or unnecessarily altering
the original question's structure (over-correction). We propose QuestionRAG, a
framework that tackles these problems. To address misinterpretation, it
enriches the input with external knowledge (e.g., search results, related
entities). To prevent over-correction, it uses reinforcement learning (RL) to
align the model's objective with precise correction, not just paraphrasing. Our
results demonstrate that knowledge augmentation is critical for understanding
faulty questions. Furthermore, RL-based alignment proves significantly more
effective than traditional supervised fine-tuning (SFT), boosting the model's
ability to follow instructions and generalize. By integrating these two
strategies, QuestionRAG unlocks the full potential of LLMs for the question
correction task.

</details>


### [4] [SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties](https://arxiv.org/abs/2511.03542)
*Roberta Di Marino,Giovanni Dioguardi,Antonio Romano,Giuseppe Riccio,Mariano Barone,Marco Postiglione,Flora Amato,Vincenzo Moscato*

Main category: cs.CL

TL;DR: SOLVE-Med是一个多智能体架构，结合领域专业化的小语言模型来处理复杂医疗查询。该系统通过路由智能体动态选择专家模型，使用10个专业模型（每个10亿参数）在特定医疗领域进行微调，并通过协调智能体合成响应。


<details>
  <summary>Details</summary>
Motivation: 解决医疗问答系统面临的幻觉、偏见、计算需求、隐私问题以及跨多个专业领域需要专业知识等部署挑战。

Method: 采用多智能体架构：路由智能体负责动态选择专家，10个专业模型（每个1B参数）在特定医疗领域微调，协调智能体合成最终响应。

Result: 在意大利医疗论坛数据的10个专业领域评估中，SOLVE-Med实现了ROUGE-1 0.301和BERTScore F1 0.697的优异性能，优于高达140亿参数的独立模型，同时支持本地部署。

Conclusion: SOLVE-Med展示了多智能体架构在医疗问答中的有效性，能够在保持高性能的同时实现本地部署，解决了现有系统的关键挑战。

Abstract: Medical question answering systems face deployment challenges including
hallucinations, bias, computational demands, privacy concerns, and the need for
specialized expertise across diverse domains. Here, we present SOLVE-Med, a
multi-agent architecture combining domain-specialized small language models for
complex medical queries. The system employs a Router Agent for dynamic
specialist selection, ten specialized models (1B parameters each) fine-tuned on
specific medical domains, and an Orchestrator Agent that synthesizes responses.
Evaluated on Italian medical forum data across ten specialties, SOLVE-Med
achieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697,
outperforming standalone models up to 14B parameters while enabling local
deployment. Our code is publicly available on GitHub:
https://github.com/PRAISELab-PicusLab/SOLVE-Med.

</details>


### [5] [Bearing Syntactic Fruit with Stack-Augmented Neural Networks](https://arxiv.org/abs/2511.03547)
*Brian DuSell,Ryan Cotterell*

Main category: cs.CL

TL;DR: 该论文展示了堆栈增强神经网络能够在没有语法监督、大规模预训练或长时间训练的情况下，像人类一样进行层次化泛化，特别是在问答形成任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络是否具有与人类儿童学习语言时相似的层次化语法规则偏好，而无需歧义消除示例。

Method: 测试了三种基础架构（transformer、简单RNN、LSTM）与两种堆栈风格的组合：Joulin & Mikolov的叠加堆栈和DuSell & Chiang的非确定性堆栈，并在经典问答形成任务上评估泛化能力。

Result: 带有非确定性堆栈的transformer在这些架构中泛化效果最好，同时提出了改进堆栈RNN架构以增强层次化泛化的方法。

Conclusion: 堆栈增强神经网络可能是比标准架构更准确的人类语言习得模型，可作为心理语言学研究的有效对象。

Abstract: Any finite set of training data is consistent with an infinite number of
hypothetical algorithms that could have generated it. Studies have shown that
when human children learn language, they consistently favor hypotheses based on
hierarchical syntactic rules without ever encountering disambiguating examples.
A recent line of work has inquired as to whether common neural network
architectures share this bias, finding that they do so only under special
conditions: when syntactically supervised, when pre-trained on massive corpora,
or when trained long past convergence. In this paper, we demonstrate, for the
first time, neural network architectures that are able to generalize in
human-like fashion without any of the aforementioned requirements:
stack-augmented neural networks. We test three base architectures (transformer,
simple RNN, LSTM) augmented with two styles of stack: the superposition stack
of Joulin & Mikolov (2015) and a nondeterministic generalization of it proposed
by DuSell & Chiang (2023). We find that transformers with nondeterministic
stacks generalize best out of these architectures on a classical question
formation task. We also propose a modification to the stack RNN architecture
that improves hierarchical generalization. These results suggest that
stack-augmented neural networks may be more accurate models of human language
acquisition than standard architectures, serving as useful objects of
psycholinguistic study. Our code is publicly available.

</details>


### [6] [MultiZebraLogic: A Multilingual Logical Reasoning Benchmark](https://arxiv.org/abs/2511.03553)
*Sofie Helene Bruun,Dan Saattrup Smart*

Main category: cs.CL

TL;DR: 创建了MultiZebraLogic数据集，包含多种语言、主题和难度的斑马谜题，用于评估LLM的逻辑推理能力，发现谜题大小和干扰线索显著影响模型表现。


<details>
  <summary>Details</summary>
Motivation: 需要创建大规模高质量的数据集来比较不同语言模型在多种语言中的逻辑推理能力，并探索增加难度的不同方法。

Method: 生成多种语言、主题、大小和线索类型的斑马谜题，包括14种线索类型和8种干扰线索类型，测试了GPT-4o mini和o3-mini在不同条件下的表现。

Result: 发现2x3和4x5大小的谜题分别对GPT-4o mini和o3-mini具有足够挑战性；5个干扰线索使o3-mini在4x5谜题上的准确率下降15±7%；语言和主题变化对表现无显著影响；线索类型与难度无相关性。

Conclusion: 成功创建了可扩展的MultiZebraLogic数据集和生成代码，为评估LLM的逻辑推理能力提供了有效工具，展示了谜题大小和干扰线索对模型表现的重要影响。

Abstract: Measuring the full abilities of large language models (LLMs) requires
benchmarks representing multiple tasks. We aim to create large, high-quality
datasets for comparison of logical reasoning skills across several languages
and of suitable difficulty for LLMs of various reasoning ability. We explore
multiple ways of increasing difficulty. We generate zebra puzzles in multiple
languages, themes, sizes and including 14 different clue types and 8 red
herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are
sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a
reasoning model), respectively. Including 5 red herrings decreases o3-mini
puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5
puzzles are not significantly affected by use of English vs. Danish or the
common houses theme vs. the country-specific smoerrebroed theme. We find no
correlation between difficulty and the selected clue types. Datasets of
128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic
languages for sizes 2x3 and 4x5. We publish code for puzzle generation,
designed for adaptablity into more languages and themes.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [7] [Notes from reading the Amp manual](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhamel.dev%2Fnotes%2Fcoding-agents%2Famp.html%3Futm_source=tldrai/1/0100019a4f360b3c-22ea7ec0-872a-4f6b-af3d-7222997bc7c9-000000/859K8eRpYSS6yNdUHhHg0iSZjm-_1EXV9XiQLxS8vF0=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Amp是一个支持MCP的编程代理，具有项目/文件夹/全局上下文设置、权限系统、无头模式等功能，采用按token付费模式。


<details>
  <summary>Details</summary>
Motivation: 提供更智能的编程代理工具，通过上下文管理和多模型选择来提升开发效率。

Method: 使用MCP支持，实现项目/文件夹/全局三级上下文管理，内置权限系统，支持无头模式，自动选择合适模型执行任务。

Result: 能够保存所有线程在服务器上，用户可以随时恢复工作，虽然按token付费成本较高但能节省时间。

Conclusion: Amp是一个功能强大的编程代理工具，虽然价格不菲但物有所值，能显著提升开发效率。

Abstract: Notes from reading the Amp manual (5 minute read) Amp is a coding agent with support for MCP, the ability to set project, folder, and global context, a permissions system, headless mode, and more. It stores all threads on the server so users can resume a thread anytime. Amp picks the right model for each task, but it can be nudged to use more expensive models. It is pay-per-token and not cheap, but it is worth it because it saves time.

</details>


### [8] [Deepnote](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fdeepnote%2Fdeepnote%3Futm_source=tldrnewsletter/1/0100019a53c1e16f-a263bbcb-11dc-49df-b6fb-cea544e71c0c-000000/nf6RnTN5Thot9YRXBI9nArTokYFN6Bc2Z5bQLgy1pO0=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Deepnote是Jupyter的替代品，提供云端实时协作、AI代理和可部署的数据应用


<details>
  <summary>Details</summary>
Motivation: 解决Jupyter在协作、扩展性和部署方面的局限性，提供更强大的数据科学工作环境

Method: 开发基于云的Jupyter替代平台，集成实时协作功能、AI代理和部署能力

Result: 创建了能够扩展至云端、支持团队协作并具备AI辅助功能的完整数据科学平台

Conclusion: Deepnote成功扩展了Jupyter的功能，为数据科学家提供了更全面、协作性更强的开发环境

Abstract: Deepnote (GitHub Repo) Deepnote is a drop-in replacement for Jupyter that can scale to Deepnote cloud for real-time collaboration, Deepnote agent, and deployable data apps.

</details>


### [9] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a53c1e16f-a263bbcb-11dc-49df-b6fb-cea544e71c0c-000000/AP1xBsxEc-MHnDFgJ-sYl1Ym9fAYf5oTbaDvT_hxzGs=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Deepnote是Jupyter的替代品，提供云端实时协作、Deepnote代理和可部署数据应用


<details>
  <summary>Details</summary>
Motivation: 为数据科学工作流提供更强大的协作和部署能力，超越传统Jupyter笔记本的限制

Method: 开发云端协作平台，集成AI代理功能，支持数据应用部署

Result: 创建了能够扩展至云端、支持实时协作和AI辅助的数据科学环境

Conclusion: Deepnote成功扩展了Jupyter的功能，为现代数据科学团队提供了更完整的解决方案

Abstract: Deepnote (GitHub Repo) Deepnote is a drop-in replacement for Jupyter that can scale to Deepnote cloud for real-time collaboration, Deepnote agent, and deployable data apps.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [10] [学点交通 | 科研科普：什么是<em class="highlight">强化学习</em>？以及<em class="highlight">强化学习</em>在交通科研应用举例](http://mp.weixin.qq.com/s?__biz=MzUzMzk5MjAzNA==&mid=2247504065&idx=4&sn=f4e010d1153026188162ed052d1e0418&chksm=fba234d5efdeb50a8812725be92fb70f5f51f925a3195871286d277cb44eb266f501f8e420a1#rd)
*轨道知道*

Main category: wechat.article

TL;DR: 强化学习（rl）作为一种通过与环境交互试错来学习最优决策策略的机器学习方法，为解决交通领域的序列决策问题提供了全新的范式。本文首先阐述了强化学习的基本原理及其核心组成部分，进而系统综述了其在交通信号控制


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（rl）作为一种通过与环境交互试错来学习最优决策策略的机器学习方法，为解决交通领域的序列决策问题提供了全新的范式。本文首先阐述了强化学习的基本原理及其核心组成部分，进而系统综述了其在交通信号控制

</details>


### [11] [<em class="highlight">强化学习</em>成今年最大赢家！登上Nature顶刊！](http://mp.weixin.qq.com/s?__biz=MzU0NjczNTg2NQ==&mid=2247520375&idx=7&sn=16d6005aaa62581d581966aaa01e355e&chksm=fad676180716625802f1e5a0b1f9f36a590a1249ea2ec572d2a64d8320b4042fa407e1bbf5db#rd)
*深度之眼*

Main category: wechat.article

TL;DR: 分享一个强化学习方向值得学习的成果——最大扩散强化学习MaxDiff RL，感兴趣的同学可阅读原文。这是个新型RL方法，目前在多个基准测试中都实现了SOTA，已成功登上Nature Machine Intelligence。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 分享一个强化学习方向值得学习的成果——最大扩散强化学习MaxDiff RL，感兴趣的同学可阅读原文。这是个新型RL方法，目前在多个基准测试中都实现了SOTA，已成功登上Nature Machine Intelligence。

</details>


### [12] [Xᴬᴵ科研进展 | RLinf上新πRL：在线<em class="highlight">强化学习</em>微调π0和π0.5](http://mp.weixin.qq.com/s?__biz=Mzk3NTQ2NTY2Ng==&mid=2247487533&idx=2&sn=9e0596a9ed9d6299556e3104ee46cb28&chksm=c53d9be50e9e31c76bc03808105bf750a1d1d0a9b564c44d964bdd131b9cfd426dcdb8b02c8d#rd)
*北京中关村学院*

Main category: wechat.article

TL;DR: 强化学习允许智能体通过与环境的真实交互自行探索和迭代改进，可以减少 VLA 模型对大量数据的依赖，并进一步提升 SFT 的性能上限。目前，针对流匹配 VLA 的 RL 研究仍较少，主流工作大多集中在 OpenVLA 和 OpenVLA-OFT 等自回归 VLA


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习允许智能体通过与环境的真实交互自行探索和迭代改进，可以减少 VLA 模型对大量数据的依赖，并进一步提升 SFT 的性能上限。目前，针对流匹配 VLA 的 RL 研究仍较少，主流工作大多集中在 OpenVLA 和 OpenVLA-OFT 等自回归 VLA

</details>


### [13] [LLM <em class="highlight">强化学习</em>的探索机制相关工作速览](http://mp.weixin.qq.com/s?__biz=MzIwMDU4Nzg0Mw==&mid=2247594335&idx=1&sn=052e2c5955d1ffac36daf55145633792&chksm=97299d73e6c202ed6049fbea9088fb63e58f96c1e3505a20a22c65de7b31588ec380d143121a#rd)
*RUC AI Box*

Main category: wechat.article

TL;DR: 在强化学习（rl）中，如何在探索（exploration）与利用（exploitation）之间取得平衡一直是核心问题。尽管近年来在提升大语言模型（LLM）推理能力方面取得了显著进展，但多数方法偏向于利用，导致性能逐渐进入平台期。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在强化学习（rl）中，如何在探索（exploration）与利用（exploitation）之间取得平衡一直是核心问题。尽管近年来在提升大语言模型（LLM）推理能力方面取得了显著进展，但多数方法偏向于利用，导致性能逐渐进入平台期。

</details>


### [14] [<em class="highlight">强化学习</em>自适应控制：理论进展、工程挑战与未来方向](http://mp.weixin.qq.com/s?__biz=MzAwNjkzMTE2Mg==&mid=2247486285&idx=1&sn=70e9c57e87cd6d456bb4cb3d1c85e094&chksm=9a807654978565c55a8482d87de21b4fc3b738a322c46047725cc74d5a4a298d14c91b0b5559#rd)
*控制工程*

Main category: wechat.article

TL;DR: 而现在，元强化学习的出现彻底改变了这一局面。元强化学习通过让智能体在多个相似任务上进行训练，学习到一种 “通用的学习能力”，使得智能体在面对新任务时，只需少量样本就能快速适应。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 而现在，元强化学习的出现彻底改变了这一局面。元强化学习通过让智能体在多个相似任务上进行训练，学习到一种 “通用的学习能力”，使得智能体在面对新任务时，只需少量样本就能快速适应。

</details>


### [15] [全球首个具身智能<em class="highlight">强化学习</em>技术正式应用](http://mp.weixin.qq.com/s?__biz=MzU4MTg0Njk2OQ==&mid=2247501241&idx=3&sn=8e1ecfb197d8daf6d7796641aa64abfe&chksm=fc4fd3979facb826510ecbd15d61ca13df58410702bcef46d9d2bdd534049fb906d508024b6b#rd)
*教育与产业*

Main category: wechat.article

TL;DR: 全球首个具身智能强化学习技术正式应用 在上海的一家智能设备产线里，这批新来的机器人员工就搭载了全球首个真机强化学习技术。有了这项技术，机器人的训练周期得到了大幅缩减。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 全球首个具身智能强化学习技术正式应用 在上海的一家智能设备产线里，这批新来的机器人员工就搭载了全球首个真机强化学习技术。有了这项技术，机器人的训练周期得到了大幅缩减。

</details>


### [16] [全球首个具身智能<em class="highlight">强化学习</em>技术正式应用](http://mp.weixin.qq.com/s?__biz=MzU0NDMzNDM2NA==&mid=2247583618&idx=1&sn=1f0d8af3a1baca476e41f4dd344a1991&chksm=fa9d25ce978023a47e8d0ba00c352ff7d2a99615be3f8bd42bb58255be635759a0807fc2001f#rd)
*政法智能化建设技术装备及成果展*

Main category: wechat.article

TL;DR: 强化学习技术正式应用 在上海的一家智能设备产线里，这批新来的机器人员工就搭载了全球首个真机强化学习技术。有了这项技术，机器人的训练周期得到了大幅缩减。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习技术正式应用 在上海的一家智能设备产线里，这批新来的机器人员工就搭载了全球首个真机强化学习技术。有了这项技术，机器人的训练周期得到了大幅缩减。

</details>


### [17] [<em class="highlight">强化学习</em>智能体助力大模型集成，动态权重优化解锁多模型协同](http://mp.weixin.qq.com/s?__biz=Mzk0MTE5NDMxOA==&mid=2247491426&idx=1&sn=4accd6a44255fc894a4a6970677e76de&chksm=c3d5ac4f291221419546c6a68d5840479f340e64c5ec82920b9d69ba93c5887b58b3488b3c90#rd)
*深度强化学习 CASIA*

Main category: wechat.article

TL;DR: RLAE的核心思路是“以强化学习建模集成过程，用动态权重适配场景需求”，具体通过三大模块实现： 1. MDP框架：将集成转化为序贯决策问题RLAE将“大模型集成”形式化为马尔可夫决策过程，其定义的关键组件如下：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: RLAE的核心思路是“以强化学习建模集成过程，用动态权重适配场景需求”，具体通过三大模块实现： 1. MDP框架：将集成转化为序贯决策问题RLAE将“大模型集成”形式化为马尔可夫决策过程，其定义的关键组件如下：

</details>


### [18] [【RL系列】HIL-SERL：99%+成功率的真机<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=Mzk2ODA1MDE4NQ==&mid=2247483742&idx=1&sn=9ffd0aeba255f003ea467ff75c63eab0&chksm=c5a1fc04ca2a846277dc21911270d22ccab6e5d57cf24e65bd72cc190fb3bb04d6cf7387c46c#rd)
*具身智库*

Main category: wechat.article

TL;DR: 该分析不仅帮助我们理解强化学习为何能在这些复杂任务中取得成功，还为将基于强化学习的操作方法拓展到更具挑战性的场景，提供了潜在的研究方向。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 该分析不仅帮助我们理解强化学习为何能在这些复杂任务中取得成功，还为将基于强化学习的操作方法拓展到更具挑战性的场景，提供了潜在的研究方向。

</details>


### [19] [滴滴再放大招？端到端<em class="highlight">强化学习</em>超越大型前沿LLM](http://mp.weixin.qq.com/s?__biz=MzkzNzc0OTczNA==&mid=2247488902&idx=1&sn=13f50737a301a8fc17a9fd780761bcad&chksm=c3f721d7cb8f90e6b204ccef03ca2902a7db4d1e8e8ec9d19d712ca14592ecff1501ba289ccf#rd)
*真AI至上*

Main category: wechat.article

TL;DR: ！回复 “端到端强化学习” 即可领取 【端到端强化学习】研究论文 volant 沃恩智慧 DeepTravel：一种面向自动驾驶旅行规划代理的端到端代理式强化学习框架


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ！回复 “端到端强化学习” 即可领取 【端到端强化学习】研究论文 volant 沃恩智慧 DeepTravel：一种面向自动驾驶旅行规划代理的端到端代理式强化学习框架

</details>


### [20] [基于<em class="highlight">强化学习</em>的流程工业智能决策研究与展望](http://mp.weixin.qq.com/s?__biz=MzUyMDc5OTU5NA==&mid=2247717353&idx=1&sn=2951ec58ebf861a84b1ec6e2835bd233&chksm=f8d3940aca19f4a9bc193fd9982183617495727a5d76fb1eaf56f43f025f66ae259d8622056e#rd)
*一点人工一点智能*

Main category: wechat.article

TL;DR: to 强化学习概述 强化学习是一类用于解决序贯决策问题的机器学习方法，其核心目标是训练一个智能体在与环境的交互过程中学习策略，以最大化长期累积回报。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: to 强化学习概述 强化学习是一类用于解决序贯决策问题的机器学习方法，其核心目标是训练一个智能体在与环境的交互过程中学习策略，以最大化长期累积回报。

</details>


### [21] [Nature：深度<em class="highlight">强化学习</em>助力「人造太阳」可控精准放电](http://mp.weixin.qq.com/s?__biz=Mzk0OTMxMzYxNQ==&mid=2247490608&idx=1&sn=f262585f08f2ab04e0cc676273f2774e&chksm=c2cef8be694edf3f8072f3cd44da77cf06c4107e1594d2d4f2cdee0e8a7b9d2914c169db27e1#rd)
*电力电子实验室*

Main category: wechat.article

TL;DR: 其次，深度强化学习算法与托卡马克模拟器交互，寻找满足目标的近最优控制策略。最后，该控制策略由神经网络表示，并直接以“零样本”方式在托卡马克硬件上实时运行。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 其次，深度强化学习算法与托卡马克模拟器交互，寻找满足目标的近最优控制策略。最后，该控制策略由神经网络表示，并直接以“零样本”方式在托卡马克硬件上实时运行。

</details>


### [22] [一文读懂<em class="highlight">强化学习</em>核心：Q 学习算法的原理与实践](http://mp.weixin.qq.com/s?__biz=MzIyMjc4ODQzMQ==&mid=2247484856&idx=1&sn=fa2fa87d4276d00524b1515784ef8363&chksm=e98c2fa061b468cc788f0626f182bf9339c05d268d45a241292c2b0ebb6ce95288affa91ae6e#rd)
*强化学习*

Main category: wechat.article

TL;DR: 帮你掌握强化学习的入门关键。lQ 学习的核心思想：给 “决策” 打分Q 学习的本质，是为 “状态 - 动作” 对（State - Action Pair）打分 —— 这个分数就是 “Q 值”，代表 “在某个状态下执行某个动作后，能获得的长期累积奖励


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 帮你掌握强化学习的入门关键。lQ 学习的核心思想：给 “决策” 打分Q 学习的本质，是为 “状态 - 动作” 对（State - Action Pair）打分 —— 这个分数就是 “Q 值”，代表 “在某个状态下执行某个动作后，能获得的长期累积奖励

</details>


### [23] [<em class="highlight">强化学习</em> AI 系统的设计实现及未来发展](http://mp.weixin.qq.com/s?__biz=MzIyNDY2MTg4Mg==&mid=2247486457&idx=1&sn=67d6c48c78ece2ed9843656d52b0b834&chksm=e94ec0174a8d95b44450417eecdff1ef114238ac08ccfc153ad8f558a7614accc929f2a10dd5#rd)
*AICon人工智能开发与应用大会*

Main category: wechat.article

TL;DR: 从常见的人类反馈强化学习，到基于宪法的反馈强化学习，再到如今基于可验证规则的强化学习，这些不断进步的过程，实际上代表着强化学习奖励函数的信号来源日益广泛，同时任务难度也在不断提高。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从常见的人类反馈强化学习，到基于宪法的反馈强化学习，再到如今基于可验证规则的强化学习，这些不断进步的过程，实际上代表着强化学习奖励函数的信号来源日益广泛，同时任务难度也在不断提高。

</details>


### [24] [活动回顾丨数学研讨会：<em class="highlight">强化学习</em>，奥秘无限](http://mp.weixin.qq.com/s?__biz=Mzg5MzA0Nzk5Nw==&mid=2247505989&idx=1&sn=b01a22a707209dd7a9095994b2e12e0f&chksm=c1ecc4a8dcc7c9b7d3e3077169c8b05827305c4cd90343b18410e6edc66369bfd29136aadf38#rd)
*SCUPIologist*

Main category: wechat.article

TL;DR: 强化学习驱动智能体自我进化”为主题的数学研讨会，由2022级计算机科学与技术专业学生王俊桥主讲。副教授杨铮、助理教授吴英杰出席了本次活动，并在研讨过程中与王俊桥同学进行了专业交流。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习驱动智能体自我进化”为主题的数学研讨会，由2022级计算机科学与技术专业学生王俊桥主讲。副教授杨铮、助理教授吴英杰出席了本次活动，并在研讨过程中与王俊桥同学进行了专业交流。

</details>


### [25] [白话文<em class="highlight">强化学习</em>(RL) PPO的前生今世(策略梯度与PPO原理解读)](http://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247504917&idx=1&sn=bd0f3b6c93184c6d9e87d342c3d9482f&chksm=9a0a55615a9379326b635f314a4505f3713661835b5668abae3c43c90ace16cb643d6f46159b#rd)
*关于NLP那些你不知道的事*

Main category: wechat.article

TL;DR: 另外，RL与监督学习最大的不同是RL十分强调轨迹以及未来的收益，现在过程监督（）也比较火热，每一步的动作可能会打开不同程度的路线，需要重视每一步的收益（及未来收益），也要重视整条轨迹的收益。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 另外，RL与监督学习最大的不同是RL十分强调轨迹以及未来的收益，现在过程监督（）也比较火热，每一步的动作可能会打开不同程度的路线，需要重视每一步的收益（及未来收益），也要重视整条轨迹的收益。

</details>


### [26] [零门槛上手最强多模态<em class="highlight">大模型</em>，Qwen3-VL模型直接在本地部署(附教程）](http://mp.weixin.qq.com/s?__biz=MzIxOTkwNzU4NA==&mid=2247489700&idx=1&sn=73c22898af38313f5eced7da47036fc6&chksm=964e4e17330802c335108ccbd2c28c49d5614ea5657ad4eeb1b7aafe13f2ff27128c024ee40d#rd)
*陶人超有料*

Main category: wechat.article

TL;DR: 在开源多模态大模型领域Qwen3-VL系列可以说是行业的霸榜大哥大级别的存在~qwen3 lm dense/moe decoder ... lmages and videos here. 11427 tokens 8 tokens 1125 tokens imtimestomp in text farmol text tokens tishes picture 1 picture 2 pictur...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在开源多模态大模型领域Qwen3-VL系列可以说是行业的霸榜大哥大级别的存在~qwen3 lm dense/moe decoder ... lmages and videos here. 11427 tokens 8 tokens 1125 tokens imtimestomp in text farmol text tokens tishes picture 1 picture 2 picture 3 video 1 *** vision encoder -oooooo l

</details>


### [27] [超全面！各种<em class="highlight">大模型</em>架构比较！](http://mp.weixin.qq.com/s?__biz=Mzk0MDY2ODM3NQ==&mid=2247487697&idx=1&sn=918944c5f3cfbd1df5b63ff2df2cac62&chksm=c398de0783c7602769485e7a7713d865641c3e0fa01214bbe17b7b3f375897b7681f99ee3807#rd)
*AI大模型前沿*

Main category: wechat.article

TL;DR: 传统的NLP基准测试，如MMLU（大规模多任务语言理解），正迅速趋于饱和，对于区分前沿模型的能力愈发有限。与此同时，一类专注于复杂推理（如GPQA， AIME）和智能体执行（如SWE - bench， Terminal - bench）的新基准，已成为衡量SOTA


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 传统的NLP基准测试，如MMLU（大规模多任务语言理解），正迅速趋于饱和，对于区分前沿模型的能力愈发有限。与此同时，一类专注于复杂推理（如GPQA， AIME）和智能体执行（如SWE - bench， Terminal - bench）的新基准，已成为衡量SOTA

</details>


### [28] [【科技成果】破解<em class="highlight">大模型</em> “伪装” 难题！北大 CoT Monitor+框架实现推理中安全干预](http://mp.weixin.qq.com/s?__biz=Mzg2Mzc1OTg0MA==&mid=2247501510&idx=1&sn=477f3f1190854e2c08076f8e8f2f552d&chksm=cfbe04d5e688c501120cf07980b6560534bf97db96aa92cfeb4556fe715998f9b51ca7d3ad1f#rd)
*未名科创*

Main category: wechat.article

TL;DR: 近日，北京大学团队在大语言模型（LLMs）安全对齐领域取得重要进展，提出CoTMonitor+框架——通过在思维链（CoT）推理过程中嵌入自我监控机制，有效缓解模型“欺骗性对齐”问题，同时构建了首个专门评估模型欺骗行为的基准


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 近日，北京大学团队在大语言模型（LLMs）安全对齐领域取得重要进展，提出CoTMonitor+框架——通过在思维链（CoT）推理过程中嵌入自我监控机制，有效缓解模型“欺骗性对齐”问题，同时构建了首个专门评估模型欺骗行为的基准

</details>


### [29] [Agent+Copilot：<em class="highlight">大模型</em>在智能运维领域的应用](http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247658913&idx=3&sn=c84c80d7bc36ddaff5563d9f4a901a12&chksm=c0dba8d2e1f834496ab52ba4b6c33ba583eef915fe10f2e11ba7bf7031488e7336bd01b1ddd8#rd)
*DataFunSummit*

Main category: wechat.article

TL;DR: 这么大的日志量靠人工已经无法解决，所以大模型被认为是一个新质生产力，它的出现为人们带来了希望。6. AI 场景secops行业痛点-ai场景 dns反向外连域名检测 web异常请求检测 利用ai识别潜在的恶意域名，提高对新型域 区分正常


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这么大的日志量靠人工已经无法解决，所以大模型被认为是一个新质生产力，它的出现为人们带来了希望。6. AI 场景secops行业痛点-ai场景 dns反向外连域名检测 web异常请求检测 利用ai识别潜在的恶意域名，提高对新型域 区分正常

</details>


### [30] [赋能品牌建设：元景品牌价值人工智能<em class="highlight">大模型</em>亮相](http://mp.weixin.qq.com/s?__biz=MjM5MzEzMTY0MA==&mid=2652083851&idx=2&sn=3b21e83cd0d6de04d4a28219fb102750&chksm=bca8a5526375912b0fea10a80749f83539b1d86e0e7560ca8719517ff1c275b4ff0d168e5262#rd)
*C114通信网*

Main category: wechat.article

TL;DR: 其次是技术创新，中国联通早在2024年初就发布了元景“1+1+M”大模型体系，包含1套基础大模型、1个大模型平台和M种行业大模型。2025年又开源了“元景万悟”智能体开发平台，支持用户0代码、低门槛、高效率创建智能体应用。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 其次是技术创新，中国联通早在2024年初就发布了元景“1+1+M”大模型体系，包含1套基础大模型、1个大模型平台和M种行业大模型。2025年又开源了“元景万悟”智能体开发平台，支持用户0代码、低门槛、高效率创建智能体应用。

</details>


### [31] [专家共识丨医疗场景下大语言<em class="highlight">模型</em>应用效果回顾性评测专家共识（2025版）](http://mp.weixin.qq.com/s?__biz=MzI4MzAxOTMwMQ==&mid=2650126591&idx=1&sn=3dce7664a96ac6d072a3b102ca001194&chksm=f283362e2e7e75d6bd31cccdda04e6af3732ac0a9e811fb04670e4f05082aef2c14fc715ab10#rd)
*中华医学期刊网*

Main category: wechat.article

TL;DR: 大语言模型指使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义，通过在庞大的数据集上进行训练来提供有关各种主题的深厚知识和语言生产。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大语言模型指使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义，通过在庞大的数据集上进行训练来提供有关各种主题的深厚知识和语言生产。

</details>


### [32] [干货分享|Qwen3 <em class="highlight">大模型</em>实战指南来袭！](http://mp.weixin.qq.com/s?__biz=MzkxMTIxMDY2NQ==&mid=2247492530&idx=1&sn=e90bfd614d6ed2eb869251e034670dac&chksm=c047406ca3233534f8ee96447413dda7b6837b645086fe5fc2c7144d88202a92aef59fa706d0#rd)
*图格图书*

Main category: wechat.article

TL;DR: 面向大模型应用开发的实践者和研究者，针对大模型应 用开发构建方法、过程、工具和案例进行介绍。案例可借鉴。围绕法律咨询智能助手和代码修复智能助手两个开发案 例，介绍具体项目的需求分析、系统架构、关键技术与


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 面向大模型应用开发的实践者和研究者，针对大模型应 用开发构建方法、过程、工具和案例进行介绍。案例可借鉴。围绕法律咨询智能助手和代码修复智能助手两个开发案 例，介绍具体项目的需求分析、系统架构、关键技术与

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [Evaluating Control Protocols for Untrusted AI Agents](https://arxiv.org/abs/2511.02997)
*Jon Kutasov,Chloe Loughridge,Yuqi Sun,Henry Sleight,Buck Shlegeris,Tyler Tracy,Joe Benton*

Main category: cs.AI

TL;DR: 本文系统评估了AI控制协议在SHADE-Arena环境中的安全性能，发现关键行动延迟策略对红队攻击具有高度鲁棒性，而重采样策略在红队获得协议内部信息时安全性大幅下降。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统作为智能体广泛部署，确保其安全运行变得至关重要。AI控制通过监控智能体行为并在必要时干预或审计来降低风险，需要评估这些协议的有效性和对适应性攻击的鲁棒性。

Method: 在SHADE-Arena多样化智能体环境中系统评估多种控制协议：包括延迟到可信模型、重采样和关键行动延迟等蓝队协议，并迭代开发红队攻击策略。

Result: 重采样和关键行动延迟策略将安全性从50%提升到96%。但当红队策略获得重采样发生时机或模拟监控器能力时，攻击成功率大幅提高，安全性降至17%。关键行动延迟策略对最强红队攻击仍保持高度鲁棒性。

Conclusion: 关键行动延迟是高度鲁棒的控制策略，而重采样策略在攻击者了解协议内部信息时易受攻击，表明拒绝攻击策略访问协议内部信息的重要性。

Abstract: As AI systems become more capable and widely deployed as agents, ensuring
their safe operation becomes critical. AI control offers one approach to
mitigating the risk from untrusted AI agents by monitoring their actions and
intervening or auditing when necessary. Evaluating the safety of these
protocols requires understanding both their effectiveness against current
attacks and their robustness to adaptive adversaries. In this work, we
systematically evaluate a range of control protocols in SHADE-Arena, a dataset
of diverse agentic environments. First, we evaluate blue team protocols,
including deferral to trusted models, resampling, and deferring on critical
actions, against a default attack policy. We find that resampling for
incrimination and deferring on critical actions perform best, increasing safety
from 50% to 96%. We then iterate on red team strategies against these protocols
and find that attack policies with additional affordances, such as knowledge of
when resampling occurs or the ability to simulate monitors, can substantially
improve attack success rates against our resampling strategy, decreasing safety
to 17%. However, deferring on critical actions is highly robust to even our
strongest red team strategies, demonstrating the importance of denying attack
policies access to protocol internals.

</details>


### [34] [PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework](https://arxiv.org/abs/2511.03023)
*Sina Montazeri,Yunhe Feng,Kewei Sha*

Main category: cs.AI

TL;DR: 提出了PublicAgent多智能体框架，通过将端到端数据分析工作流分解为专门的智能体（意图澄清、数据集发现、分析和报告），解决了LLM在复杂工作流中的注意力稀释、专业推理模式干扰和错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 开放数据仓库对非专家难以访问，需要数据集发现、模式映射和统计分析的专业知识。虽然LLM在单个任务中表现良好，但在端到端分析工作流中存在根本性限制。

Method: 设计多智能体框架，将工作流分解为四个专门智能体：意图澄清、数据集发现、分析和报告，每个智能体保持专注的注意力范围并实现阶段验证。

Result: 评估了5个模型和50个查询，得出多智能体LLM系统的五个设计原则：专业化独立于模型强度提供价值；智能体分为通用型和条件型；不同智能体缓解不同失败模式；架构优势在不同任务复杂度下持续存在；智能体有效性在不同模型间差异显著。

Conclusion: 这些原则指导了何时以及为什么在复杂分析工作流中需要专业化，同时通过自然语言接口实现更广泛的公共数据访问。

Abstract: Open data repositories hold potential for evidence-based decision-making, yet
are inaccessible to non-experts lacking expertise in dataset discovery, schema
mapping, and statistical analysis. Large language models show promise for
individual tasks, but end-to-end analytical workflows expose fundamental
limitations: attention dilutes across growing contexts, specialized reasoning
patterns interfere, and errors propagate undetected. We present PublicAgent, a
multi-agent framework that addresses these limitations through decomposition
into specialized agents for intent clarification, dataset discovery, analysis,
and reporting. This architecture maintains focused attention within agent
contexts and enables validation at each stage. Evaluation across five models
and 50 queries derives five design principles for multi-agent LLM systems.
First, specialization provides value independent of model strength--even the
strongest model shows 97.5% agent win rates, with benefits orthogonal to model
scale. Second, agents divide into universal (discovery, analysis) and
conditional (report, intent) categories. Universal agents show consistent
effectiveness (std dev 12.4%) while conditional agents vary by model (std dev
20.5%). Third, agents mitigate distinct failure modes--removing discovery or
analysis causes catastrophic failures (243-280 instances), while removing
report or intent causes quality degradation. Fourth, architectural benefits
persist across task complexity with stable win rates (86-92% analysis, 84-94%
discovery), indicating workflow management value rather than reasoning
enhancement. Fifth, wide variance in agent effectiveness across models (42-96%
for analysis) requires model-aware architecture design. These principles guide
when and why specialization is necessary for complex analytical workflows while
enabling broader access to public data through natural language interfaces.

</details>


### [35] [Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework](https://arxiv.org/abs/2511.03179)
*Varun Kumar,George Em Karniadakis*

Main category: cs.AI

TL;DR: 提出了一个多智能体AI框架来形式化工程设计过程，通过专业智能体协作生成和优化设计，以NACA翼型气动优化为例验证了框架有效性。


<details>
  <summary>Details</summary>
Motivation: 传统工程设计方法资源密集且效率低下，需要多领域专家协作，存在效率瓶颈。

Method: 构建包含三个专业AI智能体的框架：图本体学家构建领域知识图谱，系统工程师制定技术需求，设计工程师生成候选设计，通过迭代反馈循环进行设计优化。

Result: 成功应用于4位数NACA翼型的气动优化，实现了设计过程的效率提升和性能指标优化。

Conclusion: 配备结构化知识表示的协作AI智能体能够显著提升工程设计过程的效率、一致性和质量。

Abstract: The engineering design process often demands expertise from multiple domains,
leading to complex collaborations and iterative refinements. Traditional
methods can be resource-intensive and prone to inefficiencies. To address this,
we formalize the engineering design process through a multi-agent AI framework
that integrates structured design and review loops. The framework introduces
specialized knowledge-driven agents that collaborate to generate and refine
design candidates. As an exemplar, we demonstrate its application to the
aerodynamic optimization of 4-digit NACA airfoils. The framework consists of
three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems
Engineer. The Graph Ontologist employs a Large Language Model (LLM) to
construct two domain-specific knowledge graphs from airfoil design literature.
The Systems Engineer, informed by a human manager, formulates technical
requirements that guide design generation and evaluation. The Design Engineer
leverages the design knowledge graph and computational tools to propose
candidate airfoils meeting these requirements. The Systems Engineer reviews and
provides feedback both qualitative and quantitative using its own knowledge
graph, forming an iterative feedback loop until a design is validated by the
manager. The final design is then optimized to maximize performance metrics
such as the lift-to-drag ratio. Overall, this work demonstrates how
collaborative AI agents equipped with structured knowledge representations can
enhance efficiency, consistency, and quality in the engineering design process.

</details>


### [36] [Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning](https://arxiv.org/abs/2511.03724)
*Richard Dewey,Janos Botyanszki,Ciamac C. Moallemi,Andrew T. Zheng*

Main category: cs.AI

TL;DR: Solly是第一个在简化版Liar's Poker中达到精英人类水平的AI智能体，使用无模型的演员-评论家深度强化学习算法训练，在单挑和多玩家游戏中都表现出色，并超越了大型语言模型。


<details>
  <summary>Details</summary>
Motivation: AI研究长期以来关注扑克类游戏作为测试多玩家动态、不完全信息和不确定性推理的环境。虽然最近在无限制德州扑克中取得了突破，但多玩家动态被抑制，大多数手牌很快结束。本文旨在开发能在多玩家参与度高的Liar's Poker中达到精英水平的AI。

Method: 使用无模型的演员-评论家深度强化学习算法进行自我对弈训练。

Result: Solly在单挑和多玩家Liar's Poker中都达到了精英人类水平（胜率超过50%且盈利），超越了具有推理能力的大型语言模型，开发了新颖的竞价策略，有效随机化游戏，且不易被世界级人类玩家利用。

Conclusion: Solly成功展示了在具有广泛多玩家参与的游戏中达到精英人类水平的可行性，为多玩家不完全信息游戏中的AI研究开辟了新方向。

Abstract: AI researchers have long focused on poker-like games as a testbed for
environments characterized by multi-player dynamics, imperfect information, and
reasoning under uncertainty. While recent breakthroughs have matched elite
human play at no-limit Texas hold'em, the multi-player dynamics are subdued:
most hands converge quickly with only two players engaged through multiple
rounds of bidding. In this paper, we present Solly, the first AI agent to
achieve elite human play in reduced-format Liar's Poker, a game characterized
by extensive multi-player engagement. We trained Solly using self-play with a
model-free, actor-critic, deep reinforcement learning algorithm. Solly played
at an elite human level as measured by win rate (won over 50% of hands) and
equity (money won) in heads-up and multi-player Liar's Poker. Solly also
outperformed large language models (LLMs), including those with reasoning
abilities, on the same metrics. Solly developed novel bidding strategies,
randomized play effectively, and was not easily exploitable by world-class
human players.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [Unsupervised Evaluation of Multi-Turn Objective-Driven Interactions](https://arxiv.org/abs/2511.03047)
*Emi Soroka,Tanmay Chopra,Krish Desai,Sanjay Lall*

Main category: cs.LG

TL;DR: 提出了首个面向目标驱动交互的无监督评估指标，利用未标注交互数据的统计特性和微调LLM来适应分布变化，无需人工标注的理想回答作为基准。


<details>
  <summary>Details</summary>
Motivation: 企业应用中AI代理与人类的目标驱动交互系统难以评估：数据复杂且无标注、人工标注不具规模性、自定义指标无法检测未知错误、LLM评估结果不可靠。

Method: 利用未标注交互数据的统计特性，使用微调LLM适应分布变化，开发了用户目标标注、目标完成度测量和LLM不确定性量化的无监督指标。

Result: 在开放领域和任务特定的交互数据上验证了方法的有效性。

Conclusion: 提出了一套无需人工标注的无监督评估框架，能够有效评估目标驱动交互系统。

Abstract: Large language models (LLMs) have seen increasing popularity in enterprise
applications where AI agents and humans engage in objective-driven
interactions. However, these systems are difficult to evaluate: data may be
complex and unlabeled; human annotation is often impractical at scale; custom
metrics can monitor for specific errors, but not previously-undetected ones;
and LLM judges can produce unreliable results. We introduce the first set of
unsupervised metrics for objective-driven interactions, leveraging statistical
properties of unlabeled interaction data and using fine-tuned LLMs to adapt to
distributional shifts. We develop metrics for labeling user goals, measuring
goal completion, and quantifying LLM uncertainty without grounding evaluations
in human-generated ideal responses. Our approach is validated on open-domain
and task-specific interaction data.

</details>


### [38] [Scaling Multi-Agent Environment Co-Design with Diffusion Models](https://arxiv.org/abs/2511.03100)
*Hao Xiang Li,Michael Amir,Amanda Prorok*

Main category: cs.LG

TL;DR: 提出了Diffusion Co-Design (DiCoDe)框架，通过Projected Universal Guidance采样技术和critic蒸馏机制，解决agent-environment协同设计中高维环境设计空间和样本效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的协同设计方法难以扩展到高维环境设计空间，且在联合优化中面临样本效率低的问题，限制了多智能体系统在实际应用中的部署。

Method: DiCoDe框架包含两个核心创新：Projected Universal Guidance采样技术，用于探索满足硬约束的奖励最大化环境分布；critic蒸馏机制，通过共享强化学习critic的知识，确保扩散模型适应演化的智能体策略。

Result: 在仓库自动化、多智能体路径规划和风电场优化等基准测试中，该方法持续超越现有技术，例如在仓库设置中实现了39%的更高奖励和66%更少的模拟样本。

Conclusion: DiCoDe为agent-environment协同设计设立了新标准，是实现现实世界协同设计收益的重要进展。

Abstract: The agent-environment co-design paradigm jointly optimises agent policies and
environment configurations in search of improved system performance. With
application domains ranging from warehouse logistics to windfarm management,
co-design promises to fundamentally change how we deploy multi-agent systems.
However, current co-design methods struggle to scale. They collapse under
high-dimensional environment design spaces and suffer from sample inefficiency
when addressing moving targets inherent to joint optimisation. We address these
challenges by developing Diffusion Co-Design (DiCoDe), a scalable and
sample-efficient co-design framework pushing co-design towards practically
relevant settings. DiCoDe incorporates two core innovations. First, we
introduce Projected Universal Guidance (PUG), a sampling technique that enables
DiCoDe to explore a distribution of reward-maximising environments while
satisfying hard constraints such as spatial separation between obstacles.
Second, we devise a critic distillation mechanism to share knowledge from the
reinforcement learning critic, ensuring that the guided diffusion model adapts
to evolving agent policies using a dense and up-to-date learning signal.
Together, these improvements lead to superior environment-policy pairs when
validated on challenging multi-agent environment co-design benchmarks including
warehouse automation, multi-agent pathfinding and wind farm optimisation. Our
method consistently exceeds the state-of-the-art, achieving, for example, 39%
higher rewards in the warehouse setting with 66% fewer simulation samples. This
sets a new standard in agent-environment co-design, and is a stepping stone
towards reaping the rewards of co-design in real world domains.

</details>


### [39] [Periodic Skill Discovery](https://arxiv.org/abs/2511.03187)
*Jonghae Park,Daesol Cho,Jusuk Lee,Dongseok Shim,Inkyu Jang,H. Jin Kim*

Main category: cs.LG

TL;DR: 提出了周期性技能发现（PSD）框架，通过将状态映射到圆形潜在空间来无监督地发现周期性行为，能够学习具有不同周期的技能，并在复杂机器人任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前无监督技能发现方法忽视了学习技能的周期性特性，而许多机器人任务（特别是运动任务）需要在不同时间尺度上执行周期性行为，因此发现多样化的周期性技能至关重要。

Method: 训练编码器将状态映射到圆形潜在空间，在潜在表示中自然编码周期性，通过捕捉时间距离来学习具有不同周期的技能。

Result: PSD能够在复杂机器人任务中有效学习具有不同周期的技能，即使基于像素观测也能工作良好，在下游任务（如跨栏）中表现优异，与现有技能发现方法结合可产生更多样化的行为。

Conclusion: PSD框架成功解决了无监督技能发现中周期性行为学习的问题，为机器人任务提供了有效的周期性技能发现方法。

Abstract: Unsupervised skill discovery in reinforcement learning (RL) aims to learn
diverse behaviors without relying on external rewards. However, current methods
often overlook the periodic nature of learned skills, focusing instead on
increasing the mutual dependence between states and skills or maximizing the
distance traveled in latent space. Considering that many robotic tasks --
particularly those involving locomotion -- require periodic behaviors across
varying timescales, the ability to discover diverse periodic skills is
essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a
framework that discovers periodic behaviors in an unsupervised manner. The key
idea of PSD is to train an encoder that maps states to a circular latent space,
thereby naturally encoding periodicity in the latent representation. By
capturing temporal distance, PSD can effectively learn skills with diverse
periods in complex robotic tasks, even with pixel-based observations. We
further show that these learned skills achieve high performance on downstream
tasks such as hurdling. Moreover, integrating PSD with an existing skill
discovery method offers more diverse behaviors, thus broadening the agent's
repertoire. Our code and demos are available at
https://jonghaepark.github.io/psd/

</details>


### [40] [Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life Adaptation Pathways](https://arxiv.org/abs/2511.03243)
*Miguel Costa,Arthur Vandervoort,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 该论文提出使用强化学习来识别气候变化下的适应路径，并明确建模不同适应优先级（经济vs福祉）。通过综合评估模型分析洪水对生活质量、交通和基础设施的影响。


<details>
  <summary>Details</summary>
Motivation: 气候变化将增加洪水事件的频率和严重性，需要有效的适应政策制定。传统方法难以管理长期气候影响的不确定性，且政策中的规范性选择往往不明确。

Method: 使用强化学习（RL）结合综合评估模型（IAM），将降雨和洪水模型联系起来，计算洪水对生活质量、交通和基础设施的影响。

Result: 结果显示，优先考虑生活质量而非经济影响的模型会导致更多的适应支出，并且在研究区域内支出分布更均匀，突显了规范性假设对适应政策的影响。

Conclusion: 强化学习是识别不确定条件下适应路径的有用工具，同时允许明确建模和比较不同的适应优先级。

Abstract: Climate change will cause an increase in the frequency and severity of flood
events, prompting the need for cohesive adaptation policymaking. Designing
effective adaptation policies, however, depends on managing the uncertainty of
long-term climate impacts. Meanwhile, such policies can feature important
normative choices that are not always made explicit. We propose that
Reinforcement Learning (RL) can be a useful tool to both identify adaptation
pathways under uncertain conditions while it also allows for the explicit
modelling (and consequent comparison) of different adaptation priorities (e.g.
economic vs. wellbeing). We use an Integrated Assessment Model (IAM) to link
together a rainfall and flood model, and compute the impacts of flooding in
terms of quality of life (QoL), transportation, and infrastructure damage. Our
results show that models prioritising QoL over economic impacts results in more
adaptation spending as well as a more even distribution of spending over the
study area, highlighting the extent to which such normative assumptions can
alter adaptation policy. Our framework is publicly available:
https://github.com/MLSM-at-DTU/maat_qol_framework.

</details>


### [41] [A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation Agentic AI Applications](https://arxiv.org/abs/2511.03363)
*Xiaocai Zhang,Hur Lim,Ke Wang,Zhe Xiao,Jing Wang,Kelvin Lee,Xiuju Fu,Zheng Qin*

Main category: cs.LG

TL;DR: 提出了一种无需数据的模块化多标签意图识别管道DMTC，通过LLM生成合成查询、Sentence-T5编码和在线焦点对比损失训练，在交通领域智能体应用中实现高效意图理解。


<details>
  <summary>Details</summary>
Motivation: 传统意图识别系统依赖大量标注数据且难以进行细粒度多标签区分，需要消除昂贵的数据收集需求并提升多标签意图理解的准确性。

Method: 三步骤管道：1) 使用提示工程引导LLM生成多样化合成查询；2) 用Sentence-T5模型编码文本查询获得语义嵌入；3) 使用新颖的在线焦点对比损失训练轻量级分类器。

Result: 在海上交通应用中，DMTC实现5.35%的汉明损失和95.92%的AUC，优于最先进的多标签分类器和LLM基线。Sentence-T5嵌入比替代编码器提升至少3.29%的子集准确率，OFC损失带来额外0.98%增益。

Conclusion: 该系统能够无缝地将用户查询路由到特定任务模块，为无需昂贵人工标注的完全自主意图感知智能体奠定了基础。

Abstract: In this study, a modular, data-free pipeline for multi-label intention
recognition is proposed for agentic AI applications in transportation. Unlike
traditional intent recognition systems that depend on large, annotated corpora
and often struggle with fine-grained, multi-label discrimination, our approach
eliminates the need for costly data collection while enhancing the accuracy of
multi-label intention understanding. Specifically, the overall pipeline, named
DMTC, consists of three steps: 1) using prompt engineering to guide large
language models (LLMs) to generate diverse synthetic queries in different
transport scenarios; 2) encoding each textual query with a Sentence-T5 model to
obtain compact semantic embeddings; 3) training a lightweight classifier using
a novel online focal-contrastive (OFC) loss that emphasizes hard samples and
maximizes inter-class separability. The applicability of the proposed pipeline
is demonstrated in an agentic AI application in the maritime transportation
context. Extensive experiments show that DMTC achieves a Hamming loss of 5.35%
and an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers
and recent end-to-end SOTA LLM-based baselines. Further analysis reveals that
Sentence-T5 embeddings improve subset accuracy by at least 3.29% over
alternative encoders, and integrating the OFC loss yields an additional 0.98%
gain compared to standard contrastive objectives. In conclusion, our system
seamlessly routes user queries to task-specific modules (e.g., ETA information,
traffic risk evaluation, and other typical scenarios in the transportation
domain), laying the groundwork for fully autonomous, intention-aware agents
without costly manual labelling.

</details>


### [42] [Reinforcement Learning Using known Invariances](https://arxiv.org/abs/2511.03473)
*Alexandru Cioba,Aya Kayal,Laura Toni,Sattar Vakili,Alberto Bernacchia*

Main category: cs.LG

TL;DR: 该论文提出了一个将已知群对称性融入基于核的强化学习的理论算法框架，开发了对称感知的乐观最小二乘值迭代方法，通过不变核编码奖励和转移动态的不变性，显著提升了学习效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的强化学习环境通常存在固有的对称性，利用这些对称性可以提高学习效率。现有方法未能充分挖掘对称性在基于核的强化学习中的潜力。

Method: 提出了对称感知的乐观最小二乘值迭代(LSVI)方法，使用不变核来编码奖励和转移动态的不变性，建立了不变RKHS中最大信息增益和覆盖数的新界限。

Result: 在定制的Frozen Lake环境和2D布局设计问题上的实验结果表明，对称感知强化学习比标准核方法获得了显著更好的性能。

Conclusion: 结构先验在设计更样本高效的强化学习算法中具有重要价值，对称性利用可以显著提升学习效率。

Abstract: In many real-world reinforcement learning (RL) problems, the environment
exhibits inherent symmetries that can be exploited to improve learning
efficiency. This paper develops a theoretical and algorithmic framework for
incorporating known group symmetries into kernel-based RL. We propose a
symmetry-aware variant of optimistic least-squares value iteration (LSVI),
which leverages invariant kernels to encode invariance in both rewards and
transition dynamics. Our analysis establishes new bounds on the maximum
information gain and covering numbers for invariant RKHSs, explicitly
quantifying the sample efficiency gains from symmetry. Empirical results on a
customized Frozen Lake environment and a 2D placement design problem confirm
the theoretical improvements, demonstrating that symmetry-aware RL achieves
significantly better performance than their standard kernel counterparts. These
findings highlight the value of structural priors in designing more
sample-efficient reinforcement learning algorithms.

</details>


### [43] [RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse](https://arxiv.org/abs/2511.03475)
*Yinsicheng Jiang,Yeqi Huang,Liang Cheng,Cheng Deng,Xuan Sun,Luo Mai*

Main category: cs.LG

TL;DR: RAGBoost是一个高效的检索增强生成系统，通过准确保持的上下文重用实现高缓存利用率，在不牺牲准确性的情况下提升预填充性能1.5-3倍。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG缓存技术要么保持准确性但缓存重用率低，要么提高重用率但牺牲推理质量，无法同时满足高重用率和准确性需求。

Method: 通过检测并发会话和多轮交互中的重叠检索项，使用高效的上下文索引、排序和去重来最大化重用，同时通过轻量级上下文提示保持推理保真度。

Result: 与现有最先进方法相比，预填充性能提升1.5-3倍，同时在多样化RAG和代理AI工作负载中保持甚至提升了推理准确性。

Conclusion: RAGBoost实现了高缓存重用而不牺牲准确性，可无缝集成到现有LLM推理引擎中，显著提升RAG系统性能。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with retrieved context but often suffers from downgraded prefill performance as
modern applications demand longer and more complex inputs. Existing caching
techniques either preserve accuracy with low cache reuse or improve reuse at
the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG
system that achieves high cache reuse without sacrificing accuracy through
accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items
across concurrent sessions and multi-turn interactions, using efficient context
indexing, ordering, and de-duplication to maximize reuse, while lightweight
contextual hints maintain reasoning fidelity. It integrates seamlessly with
existing LLM inference engines and improves their prefill performance by 1.5-3X
over state-of-the-art methods, while preserving or even enhancing reasoning
accuracy across diverse RAG and agentic AI workloads. Our code is released at:
https://github.com/Edinburgh-AgenticAI/RAGBoost.

</details>


### [44] [AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing](https://arxiv.org/abs/2511.03697)
*Mohsen Ahmadzadeh,Kaichang Chen,Georges Gielen*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体LLM的模拟电路尺寸设计框架AnaFlow，通过智能体协作实现高效、可解释的自动化电路设计，解决了传统方法仿真成本高和缺乏可解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 模拟/混合信号电路设计目前主要依赖手工过程，设计周期长且容易出错。现有的AI方法虽然提供了自动化方案，但存在仿真成本高和结果缺乏可解释性的瓶颈。

Method: 采用多智能体工作流，专门的LLM智能体协作解释电路拓扑、理解设计目标，并通过可解释的推理迭代优化电路设计参数。自适应仿真策略实现高样本效率。

Result: AnaFlow框架在两个不同复杂度的电路上得到验证，能够完全自动完成尺寸设计任务，优于纯贝叶斯优化和强化学习方法。系统能从优化历史中学习，避免重复错误并加速收敛。

Conclusion: 该框架为模拟设计空间探索提供了强大工具，代表了模拟EDA领域的新范式，其中AI智能体作为透明的设计助手。

Abstract: Analog/mixed-signal circuits are key for interfacing electronics with the
physical world. Their design, however, remains a largely handcrafted process,
resulting in long and error-prone design cycles. While the recent rise of
AI-based reinforcement learning and generative AI has created new techniques to
automate this task, the need for many time-consuming simulations is a critical
bottleneck hindering the overall efficiency. Furthermore, the lack of
explainability of the resulting design solutions hampers widespread adoption of
the tools. To address these issues, a novel agentic AI framework for
sample-efficient and explainable analog circuit sizing is presented. It employs
a multi-agent workflow where specialized Large Language Model (LLM)-based
agents collaborate to interpret the circuit topology, to understand the design
goals, and to iteratively refine the circuit's design parameters towards the
target goals with human-interpretable reasoning. The adaptive simulation
strategy creates an intelligent control that yields a high sample efficiency.
The AnaFlow framework is demonstrated for two circuits of varying complexity
and is able to complete the sizing task fully automatically, differently from
pure Bayesian optimization and reinforcement learning approaches. The system
learns from its optimization history to avoid past mistakes and to accelerate
convergence. The inherent explainability makes this a powerful tool for analog
design space exploration and a new paradigm in analog EDA, where AI agents
serve as transparent design assistants.

</details>


### [45] [Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning](https://arxiv.org/abs/2511.03616)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 提出深度隐式模仿强化学习框架，通过结合深度强化学习和仅观察数据集的隐式模仿学习，解决传统模仿学习需要完整状态-动作演示和最优专家的限制。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习需要完整的状态-动作演示和最优专家，这严重限制了实际应用，因为许多现实场景只提供状态观察而没有对应动作，且专家表现通常不是最优的。

Method: 开发了深度隐式模仿Q网络（DIIQN），采用动作推理机制通过在线探索重建专家动作，并集成动态置信机制自适应平衡专家引导和自主学习。还扩展了异构动作DIIQN（HA-DIIQN）算法处理专家和智能体具有不同动作集的情况。

Result: DIIQN相比标准DQN实现了高达130%的更高回合回报，同时持续优于无法超越专家性能的现有隐式模仿方法。在异构动作设置中，HA-DIIQN学习速度比基线快64%，能够利用传统方法无法使用的专家数据集。

Conclusion: 该框架能够利用专家指导加速训练，同时保持超越次优专家性能的能力，在仅观察数据集和异构动作设置中表现出色。

Abstract: Imitation learning traditionally requires complete state-action
demonstrations from optimal or near-optimal experts. These requirements
severely limit practical applicability, as many real-world scenarios provide
only state observations without corresponding actions and expert performance is
often suboptimal. In this paper we introduce a deep implicit imitation
reinforcement learning framework that addresses both limitations by combining
deep reinforcement learning with implicit imitation learning from
observation-only datasets. Our main algorithm, Deep Implicit Imitation
Q-Network (DIIQN), employs an action inference mechanism that reconstructs
expert actions through online exploration and integrates a dynamic confidence
mechanism that adaptively balances expert-guided and self-directed learning.
This enables the agent to leverage expert guidance for accelerated training
while maintaining capacity to surpass suboptimal expert performance. We further
extend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to
tackle scenarios where expert and agent possess different action sets, a
challenge previously unaddressed in the implicit imitation learning literature.
HA-DIIQN introduces an infeasibility detection mechanism and a bridging
procedure identifying alternative pathways connecting agent capabilities to
expert guidance when direct action replication is impossible. Our experimental
results demonstrate that DIIQN achieves up to 130% higher episodic returns
compared to standard DQN, while consistently outperforming existing implicit
imitation methods that cannot exceed expert performance. In heterogeneous
action settings, HA-DIIQN learns up to 64% faster than baselines, leveraging
expert datasets unusable by conventional approaches. Extensive parameter
sensitivity analysis reveals the framework's robustness across varying dataset
sizes and hyperparameter configurations.

</details>


### [46] [Towards Formalizing Reinforcement Learning Theory](https://arxiv.org/abs/2511.03618)
*Shangtong Zhang*

Main category: cs.LG

TL;DR: 使用Lean 4定理证明器基于Mathlib库形式化验证了Q学习和线性TD学习在马尔可夫样本下的几乎必然收敛性


<details>
  <summary>Details</summary>
Motivation: Q学习和线性TD学习是最早且最具影响力的强化学习算法，研究其收敛性质不仅是RL领域早期发展的主要研究课题，如今也受到越来越多的关注

Method: 基于Robbins-Siegmund定理的统一框架，使用Lean 4定理证明器进行形式化验证

Result: 成功形式化验证了Q学习和线性TD学习的几乎必然收敛性

Conclusion: 这项工作为完全形式化收敛RL结果迈出了重要一步，所开发的框架可以轻松扩展到收敛速率和其他收敛模式

Abstract: In this paper, we formalize the almost sure convergence of $Q$-learning and
linear temporal difference (TD) learning with Markovian samples using the Lean
4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are
among the earliest and most influential reinforcement learning (RL) algorithms.
The investigation of their convergence properties is not only a major research
topic during the early development of the RL field but also receives increasing
attention nowadays. This paper formally verifies their almost sure convergence
in a unified framework based on the Robbins-Siegmund theorem. The framework
developed in this work can be easily extended to convergence rates and other
modes of convergence. This work thus makes an important step towards fully
formalizing convergent RL results. The code is available at
https://github.com/ShangtongZhang/rl-theory-in-lean.

</details>


### [47] [Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL](https://arxiv.org/abs/2511.03695)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: BAQ是一种离线到在线强化学习框架，通过利用离线数据中的隐式行为模型，在在线微调期间提供行为一致性信号，实现平稳可靠的过渡。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习策略在动态环境中部署时，由于分布偏移和未见状态-动作对上的不可靠值估计而表现不佳，需要一种可靠的离线到在线过渡方法。

Method: 引入行为自适应Q学习(BAQ)，采用双目标损失函数：(i)在不确定性高时使在线策略与离线行为对齐，(ii)随着在线经验积累逐渐放松约束。

Result: 在标准基准测试中，BAQ一致优于先前的离线到在线RL方法，实现了更快的恢复、改进的鲁棒性和更高的整体性能。

Conclusion: 隐式行为适应是可靠现实世界策略部署的原则性和实用解决方案。

Abstract: Offline reinforcement learning (RL) enables training from fixed data without
online interaction, but policies learned offline often struggle when deployed
in dynamic environments due to distributional shift and unreliable value
estimates on unseen state-action pairs. We introduce Behavior-Adaptive
Q-Learning (BAQ), a framework designed to enable a smooth and reliable
transition from offline to online RL. The key idea is to leverage an implicit
behavioral model derived from offline data to provide a behavior-consistency
signal during online fine-tuning. BAQ incorporates a dual-objective loss that
(i) aligns the online policy toward the offline behavior when uncertainty is
high, and (ii) gradually relaxes this constraint as more confident online
experience is accumulated. This adaptive mechanism reduces error propagation
from out-of-distribution estimates, stabilizes early online updates, and
accelerates adaptation to new scenarios. Across standard benchmarks, BAQ
consistently outperforms prior offline-to-online RL approaches, achieving
faster recovery, improved robustness, and higher overall performance. Our
results demonstrate that implicit behavior adaptation is a principled and
practical solution for reliable real-world policy deployment.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [48] [SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation](https://arxiv.org/abs/2511.02854)
*Yixiang Chen,Tianshi Zheng,Shijue Huang,Zhitao He,Yi R. Fung*

Main category: cs.SE

TL;DR: 提出了SELF-REDRAFT框架，基于Self-Refine构建，通过鼓励模型为根本性缺陷的解决方案提出新草稿，来平衡利用和探索。结果显示在相同最大迭代次数下，SELF-REDRAFT比Self-Refine表现更好，但仍存在改进空间。


<details>
  <summary>Details</summary>
Motivation: 在测试时扩展中，当测试用例不可用时，需要平衡利用（迭代优化）和探索（基于样本的投票或重排）两个维度。现有方法在这两个维度的平衡方面研究不足。

Method: 引入SELF-REDRAFT框架，基于Self-Refine构建，鼓励模型为根本性缺陷的解决方案提出新草稿，研究LLM在平衡利用和探索方面的内在能力。

Result: SELF-REDRAFT在相同最大迭代次数下始终比Self-Refine表现更好。但仍有显著改进空间，主要受限于生成指导性反馈的能力和脆弱的判别判断能力。不同LLM的平衡策略差异显著。

Conclusion: 本研究为测试时扩展中的内在探索-利用平衡建立了基准，并确定反馈和判别能力是未来发展的关键领域。

Abstract: Test-time scaling without interpreter feedback is essential for real-world
code generation scenarios where test cases are not readily available. While
existing paradigms often rely on either greedy exploitation (i.e., iterative
refinement) or stochastic exploration (i.e., relying on sample-based voting or
reranking mechanisms), the balance between these two dimensions remains
underexplored. To investigate the LLM's intrinsic ability to balance
exploitation and exploration, we introduce SELF-REDRAFT, a framework built upon
Self-Refine that encourages the model to propose new drafts for solutions that
are fundamentally flawed. Our results show that SELF-REDRAFT consistently
achieves better performance than Self-Refine when converged under the same
maximum number of iterations. Still, we observe that significant room for
improvement remains, largely due to two core aspects of current self-redraft
capabilities: constrained capacity for generating instructive feedback and
fragile discriminative judgment. We also find that balancing strategies vary
notably across different LLMs, reflecting distinct, model-specific behaviors.
Overall, our study establishes a baseline for intrinsic
exploration-exploitation balancing in test-time scaling and identifies feedback
and discrimination as key areas with potential for future advances.

</details>


### [49] [Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models](https://arxiv.org/abs/2511.02869)
*Amirreza Esmaeili,Fahd Seddik,Yongyi Ji,Fatemeh Fard,Fuxiang Chen*

Main category: cs.SE

TL;DR: AdvFusion是一种参数高效微调方法，用于代码大语言模型的多语言知识迁移，在代码生成任务中表现优于AdapterFusion，但在代码翻译和提交信息生成任务中表现不如其他PEFT方法。


<details>
  <summary>Details</summary>
Motivation: 探索AdvFusion在代码大语言模型上的扩展应用，研究其在更多软件工程任务（代码生成、代码翻译、提交信息生成）中的表现，并与现有PEFT方法进行比较。

Method: 使用AdvFusion方法对代码大语言模型进行参数高效微调，该方法先学习其他编程语言的知识，然后适应目标任务。与AdapterFusion、LoRA、Compacter和TaskAdapter等PEFT方法进行对比实验。

Result: 不同任务和模型表现出不同特性：代码生成中AdvFusion优于AdapterFusion但不如其他PEFT方法；提交信息生成中AdapterFusion表现更好；代码翻译中AdvFusion整体表现较差，且随着模型规模增大性能差距扩大。

Conclusion: AdvFusion在不同任务中的表现存在差异，不能在所有软件工程任务中都优于其他PEFT方法。任务特性和模型规模都会影响其性能表现。

Abstract: Programming languages can benefit from one another by utilizing a language
model for software engineering tasks. Full fine-tuning and Parameter Efficient
Fine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for
multilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims
to enhance task performance by leveraging information from multiple programming
languages, but primarily focuses on the target programming language.
  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that
effectively learns from other programming languages before adapting to the
target task. Though previous experiments showed that AdvFusion outperformed
AdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited
to only two tasks, code summarization and method name prediction. In this
study, we expanded our work and investigated AdvFusion on Code Large Language
Models (Code-LLMs), considering three new tasks: code generation, code
translation, and commit message generation. We observed that different
Code-LLMs/tasks exhibit different characteristics. In code generation,
AdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA,
Compacter, and TaskAdapter). In commit message generation, AdapterFusion
performed better than AdvFusion, and contrary to code generation, we found that
the other PEFT methods do not have better performance. In code translation,
AdvFusion performed worse than AdapterFusion overall, with the performance gap
marginally widening as the model size increases. However, consistent with code
generation, other PEFT methods showed better performance.

</details>


### [50] [Comprehension-Performance Gap in GenAI-Assisted Brownfield Programming: A Replication and Extension](https://arxiv.org/abs/2511.02922)
*Yunhan Qiao,Christopher Hundhausen,Summit Haque,Md Istiak Hossain Shihab*

Main category: cs.SE

TL;DR: GitHub Copilot在遗留代码编程任务中能显著提高开发效率，但不会改善代码理解能力，存在理解-性能差距


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI编程助手在遗留代码维护任务中对开发效率和代码理解能力的影响

Method: 采用组内实验设计，18名计算机科学研究生在有/无Copilot情况下完成功能实现任务，测量任务时间、测试用例通过率和理解分数

Result: Copilot显著减少任务时间并提高测试用例通过率，但理解分数无差异，理解与任务表现无相关性

Conclusion: GenAI工具能加速遗留代码库的编程进度，但这种进步可能不会带来对代码库的更好理解

Abstract: Code comprehension is essential for brownfield programming tasks, in which
developers maintain and enhance legacy code bases. Generative AI (GenAI) coding
assistants such as GitHub Copilot have been shown to improve developer
productivity, but their impact on code understanding is less clear. We
replicate and extend a previous study by exploring both performance and
comprehension in GenAI-assisted brownfield programming tasks. In a
within-subjects experimental study, 18 computer science graduate students
completed feature implementation tasks with and without Copilot. Results show
that Copilot significantly reduced task time and increased the number of test
cases passed. However, comprehension scores did not differ across conditions,
revealing a comprehension-performance gap: participants passed more test cases
with Copilot, but did not demonstrate greater understanding of the legacy
codebase. Moreover, we failed to find a correlation between comprehension and
task performance. These findings suggest that while GenAI tools can accelerate
programming progress in a legacy codebase, such progress may come without an
improved understanding of that codebase. We consider the implications of these
findings for programming education and GenAI tool design.

</details>


### [51] [Automated Prompt Generation for Code Intelligence: An Empirical study and Experience in WeChat](https://arxiv.org/abs/2511.03136)
*Kexing Ji,Shiyun Fu,Cuiyun Gao,Yujia Chen,Zezhou Yang,Chaozheng Wang,Yuetang Deng*

Main category: cs.SE

TL;DR: 本文研究了大型代码模型的自动提示生成方法，通过指令生成和多步推理显著提升了代码智能任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大型代码模型的有效性受提示质量影响很大，当前提示设计多为手动且依赖特定模型和任务。自然语言处理中的自动提示生成方法在代码智能领域尚未充分探索。

Method: 实证研究指令生成和多步推理两种自动提示生成方法，并在四种开源大型代码模型和三种代码智能任务上评估。基于结果提出结合两种方法最佳策略的新方法。

Result: 指令生成和多步推理相比基础提示显著提升性能。新方法在代码翻译、代码摘要和API推荐任务上分别获得28.38%、58.11%和84.53%的平均改进。在工业场景中，API推荐的MRR提升148.89%。

Conclusion: 自动提示生成方法能显著提升大型代码模型在代码智能任务中的性能，为解决多样化任务和黑盒模型提供了有效方案。

Abstract: Large Code Models (LCMs) show potential in code intelligence, but their
effectiveness is greatly influenced by prompt quality. Current prompt design is
mostly manual, which is time-consuming and highly dependent on specific LCMs
and tasks. While automated prompt generation (APG) exists in NLP, it is
underexplored for code intelligence. This creates a gap, as automating the
prompt process is essential for developers facing diverse tasks and black-box
LCMs.
  To mitigate this, we empirically investigate two important parts of APG:
Instruction Generation (IG) and Multi-Step Reasoning (MSR). IG provides a
task-related description to instruct LCMs, while MSR guides them to produce
logical steps before the final answer. We evaluate widely-used APG methods for
each part on four open-source LCMs and three code intelligence tasks: code
translation (PL-PL), code summarization (PL-NL), and API recommendation
(NL-PL).Experimental results indicate that both IG and MSR dramatically enhance
performance compared to basic prompts. Based on these results, we propose a
novel APG approach combining the best methods of the two parts. Experiments
show our approach achieves average improvements of 28.38% in CodeBLEU (code
translation), 58.11% in ROUGE-L (code summarization), and 84.53% in
SuccessRate@1 (API recommendation) over basic prompts. To validate its
effectiveness in an industrial scenario, we evaluate our approach on
WeChat-Bench, a proprietary dataset, achieving an average MRR improvement of
148.89% for API recommendation.

</details>


### [52] [RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring](https://arxiv.org/abs/2511.03153)
*Khouloud Oueslati,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: RefAgent是一个基于LLM的多智能体框架，用于端到端软件重构，通过专门的规划、执行、测试和迭代优化代理，显著提升代码质量和重构效果。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在软件重构中依赖静态指令，而LLM智能体能够动态适应上下文并与软件工具交互，自主决策执行重构工作流。

Method: RefAgent采用多智能体架构，包含专门负责规划、执行、测试和迭代优化的代理，利用自反思和工具调用能力进行端到端重构。

Result: 在8个开源Java项目上评估，RefAgent达到90%的中位数单元测试通过率，减少52.5%的代码异味，提升8.6%的关键质量属性，F1分数达79.15%。

Conclusion: 多智能体架构在自动化软件重构方面具有显著优势，相比单智能体方法提升了64.7%的测试通过率和40.1%的编译成功率。

Abstract: Large Language Models (LLMs) have substantially influenced various software
engineering tasks. Indeed, in the case of software refactoring, traditional
LLMs have shown the ability to reduce development time and enhance code
quality. However, these LLMs often rely on static, detailed instructions for
specific tasks. In contrast, LLM-based agents can dynamically adapt to evolving
contexts and autonomously make decisions by interacting with software tools and
executing workflows. In this paper, we explore the potential of LLM-based
agents in supporting refactoring activities. Specifically, we introduce
RefAgent, a multi-agent LLM-based framework for end-to-end software
refactoring. RefAgent consists of specialized agents responsible for planning,
executing, testing, and iteratively refining refactorings using self-reflection
and tool-calling capabilities. We evaluate RefAgent on eight open-source Java
projects, comparing its effectiveness against a single-agent approach, a
search-based refactoring tool, and historical developer refactorings. Our
assessment focuses on: (1) the impact of generated refactorings on software
quality, (2) the ability to identify refactoring opportunities, and (3) the
contribution of each LLM agent through an ablation study. Our results show that
RefAgent achieves a median unit test pass rate of 90%, reduces code smells by a
median of 52.5%, and improves key quality attributes (e.g., reusability) by a
median of 8.6%. Additionally, it closely aligns with developer refactorings and
the search-based tool in identifying refactoring opportunities, attaining a
median F1-score of 79.15% and 72.7%, respectively. Compared to single-agent
approaches, RefAgent improves the median unit test pass rate by 64.7% and the
median compilation success rate by 40.1%. These findings highlight the promise
of multi-agent architectures in advancing automated software refactoring.

</details>


### [53] [Understanding Robustness of Model Editing in Code LLMs: An Empirical Study](https://arxiv.org/abs/2511.03182)
*Vinaik Chhetri,A. B Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 该论文系统评估了五种最先进的模型编辑方法在代码LLMs中的应用，发现在API弃用场景下，即时编辑会显著降低模型性能，语法有效性下降高达86个百分点，功能正确性下降45个百分点。顺序编辑进一步加剧性能退化，正确采用预期更改的情况仅占约6%。


<details>
  <summary>Details</summary>
Motivation: LLMs在软件开发中应用日益广泛，但预训练后保持静态，而编程语言和API持续演进，导致生成过时或不兼容代码。从头重新训练LLMs计算成本高昂，模型编辑作为轻量级替代方案具有潜力，但其是否能实现真正的语法和语义适应尚不明确。

Method: 系统研究了五种最先进的模型编辑方法（Constrained FT、GRACE、MEMIT、PMET、ROME），应用于三个领先的开源代码LLMs（CodeLlama、CodeQwen1.5、DeepSeek-Coder），在受控API弃用场景下进行即时和顺序编辑设置评估。

Result: 即时编辑持续降低模型性能，语法有效性下降高达86个百分点，功能正确性下降45个百分点。顺序编辑进一步加剧退化，某些情况下模型性能完全崩溃。大多数通过生成依赖变通方法而非正确采用预期更改，正确采用仅占约6%。

Conclusion: 模型编辑方法在代码LLMs中效果有限，主要产生表面修复而非真正的语法和语义适应，正确采用预期更改的比例极低，表明当前方法在保持模型可靠性和功能性方面存在显著挑战。

Abstract: Large language models (LLMs) are increasingly used in software development.
However, while LLMs remain static after pretraining, programming languages and
APIs continue to evolve, leading to the generation of deprecated or
incompatible code that undermines reliability. Retraining LLMs from scratch to
reflect such changes is computationally expensive, making model editing a
promising lightweight alternative that updates only a small subset of
parameters. Despite its potential, it remains unclear whether model editing
yields genuine syntactic and semantic adaptations or merely superficial fixes.
In this work, we present a systematic study of five state-of-the-art model
editing methods: Constrained Fine-Tuning (FT), GRACE, MEMIT, PMET, and ROME. We
apply these methods to three leading open-source code LLMs, CodeLlama,
CodeQwen1.5, and DeepSeek-Coder, under controlled API deprecation scenarios.
Our evaluation covers both instant and sequential editing settings, using three
disjoint evaluation sets designed to assess reliability, generalization, and
specificity. We measure model correctness at three levels: successful
compilation, partial test case pass, and full test pass. Our findings show that
instant edits consistently degrade model performance, with syntactic validity
dropping by up to 86 percentage points and functional correctness declining by
45 points even in the best-performing setting. Sequential edits further amplify
this degradation, and in some cases, model performance collapses entirely.
Across all models, most passing generations relied on workarounds rather than
correctly adopting the intended changes, while faulty adoptions that result in
test failures or compilation errors were significantly more frequent. Correct
adoptions, where the model correctly integrates the intended change, occurred
in only about 6% of cases.

</details>


### [54] [Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling](https://arxiv.org/abs/2511.03404)
*Qianhui Zhao,Li Zhang,Fang Liu,Junhang Cheng,Chengru Wu,Junchen Ai,Qiaoyuanhe Meng,Lichen Zhang,Xiaoli Lian,Shubin Song,Yuanping Guo*

Main category: cs.SE

TL;DR: 提出了ProjectGen多智能体框架和CodeProjectEval数据集，用于解决项目级代码生成中的现实数据集缺乏、语义鸿沟和依赖管理等问题，在基准测试中表现显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有项目级代码生成研究存在关键限制：不现实的数据集和不可靠的评估指标无法反映真实世界复杂性，人类需求与机器可解释结构之间存在语义鸿沟，以及在生成过程中管理层次依赖关系和保持质量的困难。

Method: 提出了ProjectGen多智能体框架，将项目分解为架构设计、骨架生成和代码填充三个阶段，采用迭代优化和基于记忆的上下文管理。引入了语义软件架构树（SSAT）作为结构化和语义丰富的表示，有效桥接用户需求和源代码实现。

Result: 在小型项目级代码生成数据集DevBench上通过了52/124个测试用例，相比基线方法提升了57%；在CodeProjectEval数据集上通过了310个测试用例，相比基线方法提升了约10倍。

Conclusion: ProjectGen框架在项目级代码生成任务中实现了最先进的性能，通过多阶段分解和语义架构表示有效解决了现有方法的局限性。

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
progress in automated code generation. In real-world software engineering, the
growing demand for rapid iteration and continuous delivery underscores the
importance of project-level code generation, where LLMs are expected to
generate complete software projects directly from complex user requirements.
Although existing studies have made initial explorations, they still face key
limitations, including unrealistic datasets and unreliable evaluation metrics
that fail to reflect real-world complexity, the semantic gap between
human-written requirements and machine-interpretable structures, and
difficulties in managing hierarchical dependencies and maintaining quality
throughout the generation process. To address these limitations, we first
introduce CodeProjectEval, a project-level code generation dataset built from
18 real-world repositories with 12.7 files and 2,388.6 lines of code per task
on average, supplemented with documentation and executable test cases for
automatic evaluation. We further propose ProjectGen, a multi-agent framework
that decomposes projects into architecture design, skeleton generation, and
code filling stages with iterative refinement and memory-based context
management. Within this framework, we introduce the Semantic Software
Architecture Tree (SSAT), a structured and semantically rich representation
that effectively bridges user requirements and source code implementation.
Experiments show that ProjectGen achieves state-of-the-art performance, passing
52/124 test cases on the small-scale project-level code generation dataset
DevBench, a 57% improvement over the baseline approaches, and 310 test cases on
CodeProjectEval, representing an improvement of roughly tenfold compared to the
baselines.

</details>


### [55] [U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility](https://arxiv.org/abs/2511.03517)
*Wencheng Ye,Yan Liu*

Main category: cs.SE

TL;DR: 提出U2F框架，通过认知启发的不确定性拥抱方法，在软件工程中发现未知未知的创新解决方案


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based SWE-Agent主要解决明确定义的问题，忽略了预定义框架之外的创新解决方案，无法应对开放世界软件环境中的新兴挑战

Method: U2F包含发现-探索-集成代理系统和三个维度的认知增强机制：跨领域类比推理、逆向思维和外部验证

Result: 在218个真实世界软件使能故事上，U2F实现了14%整体新颖性提升、51%语义新颖性改进和稳定的可行性（4.02/5.0）

Conclusion: 拥抱不确定性可以作为软件工程创新的催化剂

Abstract: Large language models (LLMs) have shown strong capabilities in software
engineering tasks, yet most existing LLM-based SWE-Agents mainly tackle
well-defined problems using conventional methods, often overlooking alternative
or innovative solutions beyond their predefined frameworks. This limitation is
evident in open-world software environments, where emerging challenges
transcend established paradigms.
  We propose U2F (Unknown Unknowns to Functional solutions), a
cognitive-inspired, uncertainty-embracing multi-agent framework that
systematically surfaces "Unknown Unknowns" - novel solution pathways absent
from initial formulations but holding innovative potential. U2F consists of two
key components: (1) a Discovery-Exploration-Integration agent system for
uncovering and synthesizing potential solutions, and (2) cognitive enhancement
mechanisms across three dimensions: cross-domain analogical reasoning, reverse
thinking, and external validation, which strategically reframe and extend
conventional solution boundaries.
  Applied to 218 real-world software enabler stories curated from authentic
engineering tasks, U2F achieved notable improvements: human experts reported a
14 percent increase in overall novelty, 51 percent improvement in semantic
novelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based
evaluator. These results highlight the potential of embracing uncertainty as a
catalyst for innovation in software engineering.

</details>


### [56] [The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents](https://arxiv.org/abs/2511.03690)
*Xingyao Wang,Simon Rosenberg,Juan Michelini,Calvin Smith,Hoang Tran,Engel Nyst,Rohit Malhotra,Xuhui Zhou,Valerie Chen,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: OpenHands Software Agent SDK是一个用于构建软件工程代理的工具包，提供灵活的代理实现、安全可靠的执行环境以及多种用户交互接口。


<details>
  <summary>Details</summary>
Motivation: 构建生产级软件工程代理面临实现灵活性、执行可靠性和用户交互等挑战，需要专门的工具包来满足这些需求。

Method: 对OpenHands框架的代理组件进行架构重设计，提供简单易用的接口、本地到远程的无缝执行、集成REST/WebSocket服务，并支持多种交互接口。

Result: 在SWE-Bench Verified和GAIA基准测试中表现出色，提供原生沙箱执行、生命周期控制、模型无关的多LLM路由和内置安全分析等独特功能。

Conclusion: OpenHands Software Agent SDK为原型设计、开发新型自定义应用和大规模可靠部署代理提供了实用基础。

Abstract: Agents are now used widely in the process of software development, but
building production-ready software engineering agents is a complex task.
Deploying software agents effectively requires flexibility in implementation
and experimentation, reliable and secure execution, and interfaces for users to
interact with agents. In this paper, we present the OpenHands Software Agent
SDK, a toolkit for implementing software development agents that satisfy these
desiderata. This toolkit is a complete architectural redesign of the agent
components of the popular OpenHands framework for software development agents,
which has 64k+ GitHub stars. To achieve flexibility, we design a simple
interface for implementing agents that requires only a few lines of code in the
default case, but is easily extensible to more complex, full-featured agents
with features such as custom tools, memory management, and more. For security
and reliability, it delivers seamless local-to-remote execution portability,
integrated REST/WebSocket services. For interaction with human users, it can
connect directly to a variety of interfaces, such as visual workspaces (VS
Code, VNC, browser), command-line interfaces, and APIs. Compared with existing
SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native
sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and
built-in security analysis. Empirical results on SWE-Bench Verified and GAIA
benchmarks demonstrate strong performance. Put together, these elements allow
the OpenHands Software Agent SDK to provide a practical foundation for
prototyping, unlocking new classes of custom applications, and reliably
deploying agents at scale.

</details>
