{"id": "2601.20879", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20879", "abs": "https://arxiv.org/abs/2601.20879", "authors": ["Bruno Claudino Matias", "Savio Freire", "Juliana Freitas", "Felipe Fronchetti", "Kostadin Damevski", "Rodrigo Spinola"], "title": "A Survey on Large Language Model Impact on Software Evolvability and Maintainability: the Good, the Bad, the Ugly, and the Remedy", "comment": null, "summary": "Context. Large Language Models (LLMs) are increasingly embedded in software engineering workflows for tasks including code generation, summarization, repair, and testing. Empirical studies report productivity gains, improved comprehension, and reduced cognitive load. However, evidence remains fragmented, and concerns persist about hallucinations, unstable outputs, methodological limitations, and emerging forms of technical debt. How these mixed effects shape long-term software maintainability and evolvability remains unclear. Objectives. This study systematically examines how LLMs influence the maintainability and evolvability of software systems. We identify which quality attributes are addressed in existing research, the positive impacts LLMs provide, the risks and weaknesses they introduce, and the mitigation strategies proposed in the literature. Method. We conducted a systematic literature review. Searches across ACM DL, IEEE Xplore, and Scopus (2020 to 2024) yielded 87 primary studies. Qualitative evidence was extracted through a calibrated multi-researcher process. Attributes were analyzed descriptively, while impacts, risks, weaknesses, and mitigation strategies were synthesized using a hybrid thematic approach supported by an LLM-assisted analysis tool with human-in-the-loop validation. Results. LLMs provide benefits such as improved analyzability, testability, code comprehension, debugging support, and automated repair. However, they also introduce risks, including hallucinated or incorrect outputs, brittleness to context, limited domain reasoning, unstable performance, and flaws in current evaluations, which threaten long-term evolvability. Conclusion. LLMs can strengthen maintainability and evolvability, but they also pose nontrivial risks to long-term sustainability. Responsible adoption requires safeguards, rigorous evaluation, and structured human oversight.", "AI": {"tldr": "LLMs\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u80fd\u63d0\u5347\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u6f14\u5316\u6027\uff0c\u4f46\u4e5f\u5e26\u6765\u5e7b\u89c9\u3001\u4e0d\u7a33\u5b9a\u7b49\u98ce\u9669\uff0c\u9700\u8981\u5b89\u5168\u63aa\u65bd\u548c\u4eba\u5de5\u76d1\u7763", "motivation": "LLMs\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8bc1\u636e\u5206\u6563\uff0c\u5bf9\u957f\u671f\u8f6f\u4ef6\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u6f14\u5316\u6027\u7684\u5f71\u54cd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76", "method": "\u5bf92020-2024\u5e74ACM DL\u3001IEEE Xplore\u548cScopus\u768487\u9879\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u91c7\u7528\u6df7\u5408\u4e3b\u9898\u5206\u6790\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u8f85\u52a9\u5de5\u5177\u548c\u4eba\u5de5\u9a8c\u8bc1", "result": "LLMs\u80fd\u63d0\u5347\u53ef\u5206\u6790\u6027\u3001\u53ef\u6d4b\u8bd5\u6027\u3001\u4ee3\u7801\u7406\u89e3\u3001\u8c03\u8bd5\u652f\u6301\u548c\u81ea\u52a8\u4fee\u590d\uff0c\u4f46\u4e5f\u5e26\u6765\u5e7b\u89c9\u8f93\u51fa\u3001\u4e0a\u4e0b\u6587\u8106\u5f31\u6027\u3001\u9886\u57df\u63a8\u7406\u6709\u9650\u3001\u6027\u80fd\u4e0d\u7a33\u5b9a\u7b49\u98ce\u9669", "conclusion": "LLMs\u80fd\u589e\u5f3a\u8f6f\u4ef6\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u6f14\u5316\u6027\uff0c\u4f46\u4e5f\u5bf9\u957f\u671f\u53ef\u6301\u7eed\u6027\u6784\u6210\u98ce\u9669\uff0c\u9700\u8981\u5b89\u5168\u63aa\u65bd\u3001\u4e25\u683c\u8bc4\u4f30\u548c\u7ed3\u6784\u5316\u4eba\u5de5\u76d1\u7763", "topic": "agent analysis"}}
{"id": "2601.20882", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.20882", "abs": "https://arxiv.org/abs/2601.20882", "authors": ["Yuheng Tang", "Kaijie Zhu", "Bonan Ruan", "Chuqi Zhang", "Michael Yang", "Hongwei Li", "Suyue Guo", "Tianneng Shi", "Zekun Li", "Christopher Kruegel", "Giovanni Vigna", "Dawn Song", "William Yang Wang", "Lun Wang", "Yangruibo Ding", "Zhenkai Liang", "Wenbo Guo"], "title": "DevOps-Gym: Benchmarking AI Agents in Software DevOps Cycle", "comment": null, "summary": "Even though demonstrating extraordinary capabilities in code generation and software issue resolving, AI agents' capabilities in the full software DevOps cycle are still unknown. Different from pure code generation, handling the DevOps cycle in real-world software, including developing, deploying, and managing, requires analyzing large-scale projects, understanding dynamic program behaviors, leveraging domain-specific tools, and making sequential decisions. However, existing benchmarks focus on isolated problems and lack environments and tool interfaces for DevOps. We introduce DevOps-Gym, the first end-to-end benchmark for evaluating AI agents across core DevOps workflows: build and configuration, monitoring, issue resolving, and test generation. DevOps-Gym includes 700+ real-world tasks collected from 30+ projects in Java and Go. We develop a semi-automated data collection mechanism with rigorous and non-trivial expert efforts in ensuring the task coverage and quality. Our evaluation of state-of-the-art models and agents reveals fundamental limitations: they struggle with issue resolving and test generation in Java and Go, and remain unable to handle new tasks such as monitoring and build and configuration. These results highlight the need for essential research in automating the full DevOps cycle with AI agents.", "AI": {"tldr": "DevOps-Gym\uff1a\u9996\u4e2a\u7aef\u5230\u7aefAI\u4ee3\u7406DevOps\u57fa\u51c6\uff0c\u5305\u542b700+\u771f\u5b9e\u4efb\u52a1\uff0c\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u95ee\u9898\u89e3\u51b3\u3001\u6d4b\u8bd5\u751f\u6210\u7b49\u6838\u5fc3DevOps\u5de5\u4f5c\u6d41\u4e0a\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650", "motivation": "\u5c3d\u7ba1AI\u4ee3\u7406\u5728\u4ee3\u7801\u751f\u6210\u548c\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5b8c\u6574\u8f6f\u4ef6DevOps\u5468\u671f\u4e2d\u7684\u80fd\u529b\u4ecd\u7136\u672a\u77e5\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u95ee\u9898\uff0c\u7f3a\u4e4fDevOps\u6240\u9700\u7684\u73af\u5883\u548c\u5de5\u5177\u63a5\u53e3\uff0c\u65e0\u6cd5\u8bc4\u4f30\u4ee3\u7406\u5728\u5f00\u53d1\u3001\u90e8\u7f72\u3001\u7ba1\u7406\u7b49\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faDevOps-Gym\u57fa\u51c6\uff0c\u5305\u542b\u6784\u5efa\u4e0e\u914d\u7f6e\u3001\u76d1\u63a7\u3001\u95ee\u9898\u89e3\u51b3\u548c\u6d4b\u8bd5\u751f\u6210\u56db\u5927\u6838\u5fc3DevOps\u5de5\u4f5c\u6d41\u3002\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6570\u636e\u6536\u96c6\u673a\u5236\uff0c\u4ece30+\u4e2aJava\u548cGo\u9879\u76ee\u4e2d\u6536\u96c6700+\u771f\u5b9e\u4efb\u52a1\uff0c\u5e76\u6295\u5165\u5927\u91cf\u4e13\u5bb6\u7cbe\u529b\u786e\u4fdd\u4efb\u52a1\u8986\u76d6\u8303\u56f4\u548c\u8d28\u91cf\u3002", "result": "\u8bc4\u4f30\u6700\u5148\u8fdb\u6a21\u578b\u548c\u4ee3\u7406\u53d1\u73b0\u6839\u672c\u6027\u5c40\u9650\uff1a\u5728Java\u548cGo\u7684\u95ee\u9898\u89e3\u51b3\u548c\u6d4b\u8bd5\u751f\u6210\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u5904\u7406\u76d1\u63a7\u3001\u6784\u5efa\u4e0e\u914d\u7f6e\u7b49\u65b0\u4efb\u52a1\u7c7b\u578b\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660eAI\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u5b8c\u6574DevOps\u5468\u671f\u65b9\u9762\u4ecd\u9700\u91cd\u8981\u7814\u7a76\u7a81\u7834\u3002", "conclusion": "DevOps-Gym\u662f\u9996\u4e2a\u7aef\u5230\u7aefDevOps\u57fa\u51c6\uff0c\u63ed\u793a\u4e86AI\u4ee3\u7406\u5728\u771f\u5b9e\u8f6f\u4ef6DevOps\u5de5\u4f5c\u6d41\u4e2d\u7684\u663e\u8457\u80fd\u529b\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u5728\u81ea\u52a8\u5316\u5b8c\u6574DevOps\u5468\u671f\u65b9\u9762\u8fdb\u884c\u57fa\u7840\u7814\u7a76\u7684\u5fc5\u8981\u6027\u3002", "topic": "swe benchmark"}}
{"id": "2601.20886", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20886", "abs": "https://arxiv.org/abs/2601.20886", "authors": ["Spencer Mateega", "Jeff Yang", "Tiana Costello", "Shaurya Jadhav", "Nicole Tian", "Agustin Garcinu\u00f1o"], "title": "IDE-Bench: Evaluating Large Language Models as IDE Agents on Real-World Software Engineering Tasks", "comment": null, "summary": "IDE-Bench is a comprehensive framework for evaluating AI IDE agents on real-world software engineering tasks through an IDE-native tool interface. We present a Dockerized test harness that goes beyond raw terminal execution, granting models a structured tool ecosystem that represents AI-native IDEs like Cursor and Windsurf. By providing high-level abstractions for codebase search, structured file editing, and tools for testing full-stack applications, IDE-Bench evaluates an agent's ability to act as a true engineering collaborator. For evaluation and to prevent training data contamination, we created 80 tasks across eight never-published repositories spanning C/C++, Java, and MERN stacks, representing modern tech stack production scenarios, including feature implementation, bug fixing, refactoring, and performance optimization that mirror daily developer workflows in private codebases. Our benchmark is the first to systematically correlate agent-reported intent with successful project-level modifications in a multi-language, full-stack environment on completely uncontaminated code.", "AI": {"tldr": "IDE-Bench\u662f\u4e00\u4e2a\u901a\u8fc7IDE\u539f\u751f\u5de5\u5177\u63a5\u53e3\u8bc4\u4f30AI IDE\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5305\u542bDocker\u5316\u6d4b\u8bd5\u73af\u5883\u548c80\u4e2a\u672a\u53d1\u5e03\u4ed3\u5e93\u7684\u4efb\u52a1\uff0c\u9632\u6b62\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u539f\u59cb\u7ec8\u7aef\u6267\u884c\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30AI\u5728\u771f\u5b9eIDE\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u7f3a\u4e4f\u5bf9\u672a\u6c61\u67d3\u4ee3\u7801\u5e93\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u521b\u5efaDocker\u5316\u6d4b\u8bd5\u6846\u67b6\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u5de5\u5177\u751f\u6001\u7cfb\u7edf\uff08\u4ee3\u7801\u5e93\u641c\u7d22\u3001\u7ed3\u6784\u5316\u6587\u4ef6\u7f16\u8f91\u3001\u5168\u6808\u5e94\u7528\u6d4b\u8bd5\u5de5\u5177\uff09\uff0c\u57288\u4e2a\u672a\u53d1\u5e03\u7684C/C++\u3001Java\u548cMERN\u4ed3\u5e93\u4e2d\u8bbe\u8ba1\u4e8680\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u529f\u80fd\u5b9e\u73b0\u3001bug\u4fee\u590d\u3001\u91cd\u6784\u548c\u6027\u80fd\u4f18\u5316\u3002", "result": "\u9996\u4e2a\u5728\u591a\u8bed\u8a00\u5168\u6808\u73af\u5883\u4e2d\u7cfb\u7edf\u6027\u5730\u5c06\u4ee3\u7406\u62a5\u544a\u610f\u56fe\u4e0e\u6210\u529f\u9879\u76ee\u7ea7\u4fee\u6539\u76f8\u5173\u8054\u7684\u57fa\u51c6\uff0c\u5728\u5b8c\u5168\u672a\u6c61\u67d3\u7684\u4ee3\u7801\u4e0a\u8bc4\u4f30AI\u4ee3\u7406\u4f5c\u4e3a\u5de5\u7a0b\u534f\u4f5c\u8005\u7684\u80fd\u529b\u3002", "conclusion": "IDE-Bench\u4e3a\u8bc4\u4f30AI IDE\u4ee3\u7406\u5728\u771f\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u7279\u522b\u6ce8\u91cd\u9632\u6b62\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u3002", "topic": "swe benchmark"}}
{"id": "2601.20920", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.20920", "abs": "https://arxiv.org/abs/2601.20920", "authors": ["Vibhhu Sharma", "Thorsten Joachims", "Sarah Dean"], "title": "Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review", "comment": "28 pages", "summary": "There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u8f85\u52a9\u7684\u5ba1\u7a3f\u5bf9\u4f4e\u8d28\u91cf\u8bba\u6587\u66f4\u5bbd\u5bb9\uff0c\u4f46\u5e76\u975e\u7279\u522b\u4f18\u5f85LLM\u751f\u6210\u7684\u8bba\u6587\uff1b\u5b8c\u5168LLM\u751f\u6210\u7684\u5ba1\u7a3f\u5b58\u5728\u8bc4\u5206\u538b\u7f29\u95ee\u9898\uff0c\u800c\u4eba\u7c7b\u4f7f\u7528LLM\u7684\u5ba1\u7a3f\u80fd\u663e\u8457\u51cf\u5c11\u8fd9\u79cd\u5bbd\u5bb9\u6027\uff1bLLM\u8f85\u52a9\u7684\u5143\u5ba1\u7a3f\u66f4\u503e\u5411\u4e8e\u63a5\u53d7\u8bba\u6587\u3002", "motivation": "\u968f\u7740LLM\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u79d1\u5b66\u8bba\u6587\u64b0\u5199\u548c\u540c\u884c\u8bc4\u5ba1\u8fc7\u7a0b\uff0c\u9700\u8981\u5168\u9762\u5206\u6790LLM\u5728\u6574\u4e2a\u540c\u884c\u8bc4\u5ba1\u6d41\u7a0b\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u7279\u522b\u662f\u5173\u6ce8\u4ea4\u4e92\u6548\u5e94\uff1aLLM\u8f85\u52a9\u7684\u5ba1\u7a3f\u662f\u5426\u5bf9LLM\u8f85\u52a9\u7684\u8bba\u6587\u6709\u4e0d\u540c\u8bc4\u4ef7\u3002", "method": "\u5206\u6790\u4e86ICLR\u3001NeurIPS\u548cICML\u7684\u8d85\u8fc7125,000\u4e2a\u8bba\u6587-\u5ba1\u7a3f\u5bf9\uff0c\u901a\u8fc7\u63a7\u5236\u8bba\u6587\u8d28\u91cf\u6765\u533a\u5206\u771f\u5b9e\u6548\u5e94\u548c\u865a\u5047\u5173\u8054\uff0c\u5e76\u8865\u5145\u4e86\u5b8c\u5168LLM\u751f\u6210\u7684\u5ba1\u7a3f\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "1. \u8868\u9762\u4e0a\u770bLLM\u8f85\u52a9\u7684\u5ba1\u7a3f\u5bf9LLM\u8f85\u52a9\u7684\u8bba\u6587\u66f4\u5bbd\u5bb9\uff0c\u4f46\u63a7\u5236\u8d28\u91cf\u540e\u53d1\u73b0LLM\u8f85\u52a9\u7684\u5ba1\u7a3f\u53ea\u662f\u5bf9\u4f4e\u8d28\u91cf\u8bba\u6587\u66f4\u5bbd\u5bb9\uff1b2. \u5b8c\u5168LLM\u751f\u6210\u7684\u5ba1\u7a3f\u5b58\u5728\u4e25\u91cd\u8bc4\u5206\u538b\u7f29\uff0c\u65e0\u6cd5\u533a\u5206\u8bba\u6587\u8d28\u91cf\uff1b3. \u4eba\u7c7b\u4f7f\u7528LLM\u7684\u5ba1\u7a3f\u80fd\u663e\u8457\u51cf\u5c11\u8fd9\u79cd\u5bbd\u5bb9\u6027\uff1b4. LLM\u8f85\u52a9\u7684\u5143\u5ba1\u7a3f\u5728\u76f8\u540c\u8bc4\u5206\u4e0b\u66f4\u53ef\u80fd\u505a\u51fa\u63a5\u53d7\u51b3\u5b9a\u3002", "conclusion": "LLM\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u4f7f\u7528\u5b58\u5728\u590d\u6742\u5f71\u54cd\uff1aLLM\u8f85\u52a9\u7684\u5ba1\u7a3f\u5bf9\u4f4e\u8d28\u91cf\u8bba\u6587\u66f4\u5bbd\u5bb9\uff0c\u4f46\u5e76\u975e\u7279\u522b\u4f18\u5f85LLM\u751f\u6210\u7684\u8bba\u6587\uff1b\u5b8c\u5168LLM\u751f\u6210\u7684\u5ba1\u7a3f\u8d28\u91cf\u8f83\u5dee\uff1b\u5143\u5ba1\u7a3f\u8005\u5e76\u672a\u5b8c\u5168\u5c06\u51b3\u7b56\u5916\u5305\u7ed9LLM\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5236\u5b9aLLM\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u4f7f\u7528\u653f\u7b56\u63d0\u4f9b\u4e86\u91cd\u8981\u4f9d\u636e\u3002", "topic": "agent analysis"}}
{"id": "2601.20975", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20975", "abs": "https://arxiv.org/abs/2601.20975", "authors": ["Nikita Gupta", "Riju Chatterjee", "Lukas Haas", "Connie Tao", "Andrew Wang", "Chang Liu", "Hidekazu Oiwa", "Elena Gribovskaya", "Jan Ackermann", "John Blitzer", "Sasha Goldshtein", "Dipanjan Das"], "title": "DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents", "comment": "DeepSearchQA can be found at https://www.kaggle.com/benchmarks/google/dsqa/leaderboard", "summary": "We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.", "AI": {"tldr": "DeepSearchQA\u662f\u4e00\u4e2a\u5305\u542b900\u4e2a\u63d0\u793a\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u572817\u4e2a\u4e0d\u540c\u9886\u57df\u4e2d\u8fdb\u884c\u590d\u6742\u591a\u6b65\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u4fe1\u606f\u788e\u7247\u6574\u5408\u3001\u53bb\u91cd\u548c\u505c\u6b62\u6761\u4ef6\u5224\u65ad\u7b49\u5173\u952e\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u7b54\u6848\u68c0\u7d22\u6216\u5e7f\u6cdb\u4e8b\u5b9e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u6267\u884c\u590d\u6742\u641c\u7d22\u8ba1\u5212\u3001\u6574\u5408\u788e\u7247\u5316\u4fe1\u606f\u3001\u53bb\u91cd\u548c\u786e\u5b9a\u505c\u6b62\u6761\u4ef6\u7b49\u5173\u952e\u80fd\u529b\u7684\u8bc4\u4f30\u3002\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u6d4b\u8bd5\u8fd9\u4e9b\u5728\u5b9e\u9645\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b900\u4e2a\u63d0\u793a\u7684\u624b\u5de5\u5236\u4f5c\u6570\u636e\u96c6\uff0c\u6db5\u76d617\u4e2a\u4e0d\u540c\u9886\u57df\u3002\u4efb\u52a1\u8bbe\u8ba1\u4e3a\u56e0\u679c\u94fe\u7ed3\u6784\uff0c\u6bcf\u4e2a\u6b65\u9aa4\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u524d\u4e00\u6b65\u7684\u5b8c\u6210\uff0c\u5f3a\u8c03\u957f\u65f6\u7a0b\u89c4\u5212\u548c\u4e0a\u4e0b\u6587\u4fdd\u7559\u3002\u6240\u6709\u4efb\u52a1\u90fd\u57fa\u4e8e\u5f00\u653e\u7f51\u7edc\uff0c\u5177\u6709\u5ba2\u89c2\u53ef\u9a8c\u8bc1\u7684\u7b54\u6848\u96c6\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u67b6\u6784\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u663e\u793a\u663e\u8457\u7684\u6027\u80fd\u9650\u5236\uff1a\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u96be\u4ee5\u5e73\u8861\u9ad8\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u3002\u89c2\u5bdf\u5230\u4e0d\u540c\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5305\u62ec\u8fc7\u65e9\u505c\u6b62\uff08\u68c0\u7d22\u4e0d\u8db3\uff09\u548c\u8fc7\u5ea6\u6cdb\u5316\u884c\u4e3a\uff08\u901a\u8fc7\u4f4e\u7f6e\u4fe1\u5ea6\u7b54\u6848\u4eba\u4e3a\u63d0\u9ad8\u53ec\u56de\u7387\uff09\u3002", "conclusion": "DeepSearchQA\u63ed\u793a\u4e86\u5f53\u524d\u667a\u80fd\u4f53\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u6539\u8fdb\u7a7a\u95f4\uff0c\u53ef\u4f5c\u4e3a\u63a8\u52a8\u672a\u6765\u7814\u7a76\u671d\u7740\u66f4\u5f3a\u5927\u6df1\u5ea6\u7814\u7a76\u80fd\u529b\u53d1\u5c55\u7684\u91cd\u8981\u8bca\u65ad\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.20909", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20909", "abs": "https://arxiv.org/abs/2601.20909", "authors": ["G\u00f6tz-Henrik Wiegand", "Filip Stepniak", "Patrick Baier"], "title": "Leveraging Generative AI for Enhancing Domain-Driven Software Design", "comment": "Part of the Proceedings of the Upper-Rhine Artificial Intelligence Symposium 2024", "summary": "Domain-Driven Design (DDD) is a key framework for developing customer-oriented software, focusing on the precise modeling of an application's domain. Traditionally, metamodels that describe these domains are created manually by system designers, forming the basis for iterative software development. This paper explores the partial automation of metamodel generation using generative AI, particularly for producing domain-specific JSON objects. By training a model on real-world DDD project data, we demonstrate that generative AI can produce syntactically correct JSON objects based on simple prompts, offering significant potential for streamlining the design process. To address resource constraints, the AI model was fine-tuned on a consumer-grade GPU using a 4-bit quantized version of Code Llama and Low-Rank Adaptation (LoRA). Despite limited hardware, the model achieved high performance, generating accurate JSON objects with minimal post-processing. This research illustrates the viability of incorporating generative AI into the DDD process, improving efficiency and reducing resource requirements, while also laying the groundwork for further advancements in AI-driven software development.", "AI": {"tldr": "\u4f7f\u7528\u751f\u6210\u5f0fAI\uff08\u57fa\u4e8eCode Llama\u76844\u4f4d\u91cf\u5316\u7248\u672c\u548cLoRA\u5fae\u8c03\uff09\u90e8\u5206\u81ea\u52a8\u5316\u9886\u57df\u9a71\u52a8\u8bbe\u8ba1\u4e2d\u7684\u5143\u6a21\u578b\u751f\u6210\uff0c\u901a\u8fc7\u7b80\u5355\u63d0\u793a\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684JSON\u5bf9\u8c61\uff0c\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\u5e76\u63d0\u9ad8\u8bbe\u8ba1\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u9886\u57df\u9a71\u52a8\u8bbe\u8ba1\uff08DDD\uff09\u4e2d\uff0c\u63cf\u8ff0\u9886\u57df\u7684\u5143\u6a21\u578b\u9700\u8981\u7cfb\u7edf\u8bbe\u8ba1\u5e08\u624b\u52a8\u521b\u5efa\uff0c\u8fc7\u7a0b\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4f7f\u7528\u751f\u6210\u5f0fAI\u90e8\u5206\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u751f\u6210\u9886\u57df\u7279\u5b9a\u7684JSON\u5bf9\u8c61\uff0c\u4ee5\u7b80\u5316\u8bbe\u8ba1\u6d41\u7a0b\u5e76\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\u3002", "method": "\u5728\u771f\u5b9e\u4e16\u754cDDD\u9879\u76ee\u6570\u636e\u4e0a\u8bad\u7ec3\u751f\u6210\u5f0fAI\u6a21\u578b\uff0c\u4f7f\u75284\u4f4d\u91cf\u5316\u7684Code Llama\u7248\u672c\uff0c\u901a\u8fc7Low-Rank Adaptation\uff08LoRA\uff09\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u7b80\u5355\u63d0\u793a\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684JSON\u5bf9\u8c61\u3002", "result": "\u5c3d\u7ba1\u786c\u4ef6\u8d44\u6e90\u6709\u9650\uff0c\u6a21\u578b\u4ecd\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u80fd\u591f\u751f\u6210\u51c6\u786e\u7684JSON\u5bf9\u8c61\u4e14\u53ea\u9700\u6700\u5c0f\u5316\u540e\u5904\u7406\u3002\u8fd9\u8868\u660e\u751f\u6210\u5f0fAI\u53ef\u4ee5\u6709\u6548\u96c6\u6210\u5230DDD\u6d41\u7a0b\u4e2d\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u751f\u6210\u5f0fAI\u5728\u9886\u57df\u9a71\u52a8\u8bbe\u8ba1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5143\u6a21\u578b\u751f\u6210\u6548\u7387\uff0c\u4e3aAI\u9a71\u52a8\u7684\u8f6f\u4ef6\u5f00\u53d1\u5960\u5b9a\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u53ef\u80fd\u6027\u3002", "topic": "code agent"}}
{"id": "2601.21070", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21070", "abs": "https://arxiv.org/abs/2601.21070", "authors": ["Daniel Rodriguez-Cardenas", "Xiaochang Li", "Marcos Macedo", "Antonio Mastropaolo", "Dipin Khati", "Yuan Tian", "Huajie Shao", "Denys Poshyvanyk"], "title": "Towards Comprehensive Benchmarking Infrastructure for LLMs In Software Engineering", "comment": "Short paper from bechmarking for software engineering workshop FSE2025", "summary": "Large language models for code are advancing fast, yet our ability to evaluate them lags behind. Current benchmarks focus on narrow tasks and single metrics, which hide critical gaps in robustness, interpretability, fairness, efficiency, and real-world usability. They also suffer from inconsistent data engineering practices, limited software engineering context, and widespread contamination issues. To understand these problems and chart a path forward, we combined an in-depth survey of existing benchmarks with insights gathered from a dedicated community workshop. We identified three core barriers to reliable evaluation: the absence of software-engineering-rich datasets, overreliance on ML-centric metrics, and the lack of standardized, reproducible data pipelines. Building on these findings, we introduce BEHELM, a holistic benchmarking infrastructure that unifies software-scenario specification with multi-metric evaluation. BEHELM provides a structured way to assess models across tasks, languages, input and output granularities, and key quality dimensions. Our goal is to reduce the overhead currently required to construct benchmarks while enabling a fair, realistic, and future-proof assessment of LLMs in software engineering.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86BEHELM\u57fa\u51c6\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u4ee3\u7801LLM\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7edf\u4e00\u8f6f\u4ef6\u573a\u666f\u89c4\u8303\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6765\u63d0\u4f9b\u66f4\u5168\u9762\u3001\u53ef\u9760\u7684\u6a21\u578b\u8bc4\u6d4b", "motivation": "\u5f53\u524d\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u8bc4\u4f30\u80fd\u529b\u6ede\u540e\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a\u4efb\u52a1\u72ed\u7a84\u3001\u6307\u6807\u5355\u4e00\u3001\u7f3a\u4e4f\u9c81\u68d2\u6027/\u53ef\u89e3\u91ca\u6027/\u516c\u5e73\u6027/\u6548\u7387/\u5b9e\u9645\u53ef\u7528\u6027\u8bc4\u4f30\u3001\u6570\u636e\u5de5\u7a0b\u5b9e\u8df5\u4e0d\u4e00\u81f4\u3001\u8f6f\u4ef6\u5de5\u7a0b\u4e0a\u4e0b\u6587\u6709\u9650\u3001\u6570\u636e\u6c61\u67d3\u666e\u904d\u3002\u8fd9\u4e9b\u95ee\u9898\u963b\u788d\u4e86\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u51c6\u786e\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u6df1\u5165\u8c03\u7814\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u548c\u793e\u533a\u7814\u8ba8\u4f1a\u6536\u96c6\u89c1\u89e3\uff0c\u8bc6\u522b\u4e86\u4e09\u4e2a\u6838\u5fc3\u969c\u788d\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86BEHELM\u57fa\u51c6\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u8f6f\u4ef6\u573a\u666f\u89c4\u8303\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u652f\u6301\u8de8\u4efb\u52a1\u3001\u8bed\u8a00\u3001\u8f93\u5165\u8f93\u51fa\u7c92\u5ea6\u4ee5\u53ca\u5173\u952e\u8d28\u91cf\u7ef4\u5ea6\u7684\u7ed3\u6784\u5316\u8bc4\u4f30\u3002", "result": "\u8bc6\u522b\u4e86\u53ef\u9760\u8bc4\u4f30\u7684\u4e09\u4e2a\u6838\u5fc3\u969c\u788d\uff1a\u7f3a\u4e4f\u8f6f\u4ef6\u5de5\u7a0b\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u3001\u8fc7\u5ea6\u4f9d\u8d56ML\u4e2d\u5fc3\u6307\u6807\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u53ef\u590d\u73b0\u7684\u6570\u636e\u7ba1\u9053\u3002\u63d0\u51fa\u4e86BEHELM\u6846\u67b6\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u964d\u4f4e\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\u7684\u5f00\u9500\uff0c\u540c\u65f6\u5b9e\u73b0\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4e2dLLM\u7684\u516c\u5e73\u3001\u73b0\u5b9e\u548c\u9762\u5411\u672a\u6765\u7684\u8bc4\u4f30\u3002", "conclusion": "BEHELM\u57fa\u51c6\u6846\u67b6\u4e3a\u4ee3\u7801LLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7edf\u4e00\u8f6f\u4ef6\u573a\u666f\u89c4\u8303\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u6a21\u578b\u5728\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u771f\u5b9e\u80fd\u529b\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "topic": "swe benchmark"}}
{"id": "2601.21102", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21102", "abs": "https://arxiv.org/abs/2601.21102", "authors": ["S M Mahedy Hasan", "Md Fazle Rabbi", "Minhaz Zibran"], "title": "The Quiet Contributions: Insights into AI-Generated Silent Pull Requests", "comment": "5 pages, 4 figures, accepted at MSR Mining Challenge 2026", "summary": "We present the first empirical study of AI-generated pull requests that are 'silent,' meaning no comments or discussions accompany them. This absence of any comments or discussions associated with such silent AI pull requests (SPRs) poses a unique challenge in understanding the rationale for their acceptance or rejection. Hence, we quantitatively study 4,762 SPRs of five AI agents made to popular Python repositories drawn from the AIDev public dataset. We examine SPRs impact on code complexity, other quality issues, and security vulnerabilities, especially to determine whether these insights can hint at the rationale for acceptance or rejection of SPRs.", "AI": {"tldr": "\u9996\u6b21\u5bf9\"\u6c89\u9ed8\"AI\u751f\u6210PR\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u65e0\u8bc4\u8bba\u8ba8\u8bba\u7684AI PR\u5bf9\u4ee3\u7801\u590d\u6742\u6027\u3001\u8d28\u91cf\u95ee\u9898\u548c\u5b89\u5168\u6f0f\u6d1e\u7684\u5f71\u54cd", "motivation": "\u6c89\u9ed8AI PR\u7f3a\u4e4f\u8bc4\u8bba\u8ba8\u8bba\uff0c\u96be\u4ee5\u7406\u89e3\u5176\u88ab\u63a5\u53d7\u6216\u62d2\u7edd\u7684\u539f\u56e0\uff0c\u9700\u8981\u7814\u7a76\u5176\u5bf9\u4ee3\u7801\u8d28\u91cf\u7684\u5f71\u54cd\u6765\u63a8\u6d4b\u51b3\u7b56\u4f9d\u636e", "method": "\u4eceAIDev\u516c\u5171\u6570\u636e\u96c6\u4e2d\u9009\u53d65\u4e2aAI\u4ee3\u7406\u76844,762\u4e2a\u6c89\u9ed8PR\uff0c\u5b9a\u91cf\u5206\u6790\u5bf9\u4ee3\u7801\u590d\u6742\u6027\u3001\u8d28\u91cf\u95ee\u9898\u548c\u5b89\u5168\u6f0f\u6d1e\u7684\u5f71\u54cd", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u6c89\u9ed8AI PR\u5bf9\u4ee3\u7801\u8d28\u91cf\u7684\u5177\u4f53\u5f71\u54cd\uff0c\u8fd9\u4e9b\u53d1\u73b0\u53ef\u80fd\u6697\u793aPR\u88ab\u63a5\u53d7\u6216\u62d2\u7edd\u7684\u539f\u56e0", "conclusion": "\u901a\u8fc7\u5206\u6790\u6c89\u9ed8AI PR\u5bf9\u4ee3\u7801\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5176\u88ab\u63a5\u53d7\u6216\u62d2\u7edd\u7684\u6f5c\u5728\u539f\u56e0", "topic": "swe application"}}
{"id": "2601.21115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21115", "abs": "https://arxiv.org/abs/2601.21115", "authors": ["Mingzhi Zhu", "Boris Sobolev", "Rahul Krishna", "Raju Pavuluri", "Stacy Patterson", "Michele Merler"], "title": "Multi-task Code LLMs: Data Mix or Model Merge?", "comment": null, "summary": "Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.", "AI": {"tldr": "\u6bd4\u8f83\u4e24\u79cd\u521b\u5efa\u5c0f\u578b\u591a\u4efb\u52a1\u4ee3\u7801LLM\u7684\u65b9\u6cd5\uff1a\u6570\u636e\u6df7\u5408\u4e0e\u6a21\u578b\u5408\u5e76\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8f83\u5927\u89c4\u6a21\uff087B\u53c2\u6570\uff09\u65f6\u6a21\u578b\u5408\u5e76\u6548\u679c\u66f4\u597d\uff0c\u80fd\u4fdd\u755996%\u7684\u4e13\u7528\u6a21\u578b\u6027\u80fd\uff1b\u5728\u8f83\u5c0f\u89c4\u6a21\uff082B\u53c2\u6570\uff09\u65f6\u6570\u636e\u6df7\u5408\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\u5e73\u8861\u6027\u80fd\u3001\u7ea6\u675f\u548c\u6210\u672c\uff0c\u521b\u5efa\u5c0f\u578b\u3001\u4e13\u4e1a\u5316\u7684\u4ee3\u7801LLM\u7528\u4e8e\u4ee3\u7406\u6846\u67b6\u3002", "method": "\u6bd4\u8f83\u6570\u636e\u6df7\u5408\u4e0e\u6a21\u578b\u5408\u5e76\u4e24\u79cd\u65b9\u6cd5\uff0c\u5728Qwen Coder\u548cDeepSeek Coder\u4e24\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff082B\u548c7B\u53c2\u6570\u89c4\u6a21\uff09\uff0c\u9488\u5bf9\u4ee3\u7801\u751f\u6210\u548c\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728HumanEval\u3001MBPP\u3001CodeXGlue\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u8f83\u5927\u89c4\u6a21\uff087B\uff09\u65f6\uff0c\u6a21\u578b\u5408\u5e76\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u4fdd\u755996%\u7684\u4e13\u7528\u6a21\u578b\u6027\u80fd\uff0c\u751a\u81f3\u80fd\u8d85\u8d8a\u5355\u72ec\u5fae\u8c03\u7684\u6a21\u578b\uff08Qwen Coder 2.5 7B\u5728HumanEval\u4e0a\u8fbe\u523092.7% Pass@1 vs 90.9%\uff09\u3002\u5728\u8f83\u5c0f\u89c4\u6a21\uff082B\uff09\u65f6\uff0c\u6570\u636e\u6df7\u5408\u662f\u66f4\u4f18\u7b56\u7565\u3002\u8fd8\u5f15\u5165\u4e86\u6743\u91cd\u5206\u6790\u6280\u672f\u6765\u7406\u89e3\u4e0d\u540c\u4efb\u52a1\u5bf9\u6a21\u578b\u53c2\u6570\u7684\u5f71\u54cd\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5408\u5e76\u548c\u6df7\u5408\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u80fd\u529b\u800c\u4e0d\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u90e8\u7f72\u573a\u666f\u3002", "topic": "code agent"}}
{"id": "2601.21116", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21116", "abs": "https://arxiv.org/abs/2601.21116", "authors": ["Sankalp Gilda", "Shlok Gilda"], "title": "AI-Assisted Engineering Should Track the Epistemic Status and Temporal Validity of Architectural Decisions", "comment": "18 pages, 6 figures, 13 tables. Position paper", "summary": "This position paper argues that AI-assisted software engineering requires explicit mechanisms for tracking the epistemic status and temporal validity of architectural decisions. LLM coding assistants generate decisions faster than teams can validate them, yet no widely-adopted framework distinguishes conjecture from verified knowledge, prevents trust inflation through conservative aggregation, or detects when evidence expires. We propose three requirements for responsible AI-assisted engineering: (1) epistemic layers that separate unverified hypotheses from empirically validated claims, (2) conservative assurance aggregation grounded in the G\u00f6del t-norm that prevents weak evidence from inflating confidence, and (3) automated evidence decay tracking that surfaces stale assumptions before they cause failures. We formalize these requirements as the First Principles Framework (FPF), ground its aggregation semantics in fuzzy logic, and define a quintet of invariants that any valid aggregation operator must satisfy. Our retrospective audit applying FPF criteria to two internal projects found that 20-25% of architectural decisions had stale evidence within two months, validating the need for temporal accountability. We outline research directions including learnable aggregation operators, federated evidence sharing, and SMT-based claim validation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u7b2c\u4e00\u539f\u5219\u6846\u67b6(FPF)\"\uff0c\u7528\u4e8e\u8ffd\u8e2aAI\u8f85\u52a9\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u67b6\u6784\u51b3\u7b56\u7684\u8ba4\u77e5\u72b6\u6001\u548c\u65f6\u95f4\u6709\u6548\u6027\uff0c\u5305\u542b\u8ba4\u77e5\u5206\u5c42\u3001\u4fdd\u5b88\u4fdd\u8bc1\u805a\u5408\u548c\u8bc1\u636e\u8870\u51cf\u8ffd\u8e2a\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\u3002", "motivation": "LLM\u7f16\u7801\u52a9\u624b\u751f\u6210\u51b3\u7b56\u7684\u901f\u5ea6\u8d85\u8fc7\u56e2\u961f\u9a8c\u8bc1\u80fd\u529b\uff0c\u5f53\u524d\u7f3a\u4e4f\u533a\u5206\u63a8\u6d4b\u4e0e\u5df2\u9a8c\u8bc1\u77e5\u8bc6\u3001\u9632\u6b62\u4fe1\u4efb\u81a8\u80c0\u3001\u68c0\u6d4b\u8bc1\u636e\u8fc7\u671f\u7684\u6846\u67b6\uff0c\u5bfc\u81f4AI\u8f85\u52a9\u5de5\u7a0b\u5b58\u5728\u98ce\u9669\u3002", "method": "\u63d0\u51faFPF\u6846\u67b6\uff1a1)\u8ba4\u77e5\u5206\u5c42\u5206\u79bb\u672a\u9a8c\u8bc1\u5047\u8bbe\u4e0e\u7ecf\u9a8c\u9a8c\u8bc1\u4e3b\u5f20\uff1b2)\u57fa\u4e8eG\u00f6del t-\u8303\u6570\u7684\u4fdd\u5b88\u4fdd\u8bc1\u805a\u5408\u9632\u6b62\u5f31\u8bc1\u636e\u81a8\u80c0\u7f6e\u4fe1\u5ea6\uff1b3)\u81ea\u52a8\u8bc1\u636e\u8870\u51cf\u8ffd\u8e2a\u68c0\u6d4b\u8fc7\u65f6\u5047\u8bbe\u3002\u5f62\u5f0f\u5316\u8981\u6c42\u5e76\u5b9a\u4e49\u805a\u5408\u7b97\u5b50\u5fc5\u987b\u6ee1\u8db3\u7684\u4e94\u5143\u4e0d\u53d8\u91cf\u3002", "result": "\u5bf9\u4e24\u4e2a\u5185\u90e8\u9879\u76ee\u7684\u56de\u987e\u6027\u5ba1\u8ba1\u53d1\u73b0\uff0c20-25%\u7684\u67b6\u6784\u51b3\u7b56\u5728\u4e24\u4e2a\u6708\u5185\u8bc1\u636e\u8fc7\u65f6\uff0c\u9a8c\u8bc1\u4e86\u65f6\u95f4\u95ee\u8d23\u7684\u5fc5\u8981\u6027\u3002\u6846\u67b6\u57fa\u4e8e\u6a21\u7cca\u903b\u8f91\u5b9a\u4e49\u805a\u5408\u8bed\u4e49\u3002", "conclusion": "AI\u8f85\u52a9\u8f6f\u4ef6\u5de5\u7a0b\u9700\u8981\u660e\u786e\u7684\u8ba4\u77e5\u72b6\u6001\u548c\u65f6\u95f4\u6709\u6548\u6027\u8ffd\u8e2a\u673a\u5236\u3002FPF\u6846\u67b6\u4e3a\u8d1f\u8d23\u4efbAI\u8f85\u52a9\u5de5\u7a0b\u63d0\u4f9b\u57fa\u7840\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u53ef\u5b66\u4e60\u805a\u5408\u7b97\u5b50\u3001\u8054\u90a6\u8bc1\u636e\u5171\u4eab\u548c\u57fa\u4e8eSMT\u7684\u4e3b\u5f20\u9a8c\u8bc1\u3002", "topic": "swe application"}}
{"id": "2601.20913", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.20913", "abs": "https://arxiv.org/abs/2601.20913", "authors": ["Chen Feng", "Minghe Shen", "Ananth Balashankar", "Carsten Gerner-Beuerle", "Miguel R. D. Rodrigues"], "title": "Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges", "comment": "Accepted to ICLR2026", "summary": "Reliable certification of Large Language Models (LLMs)-verifying that failure rates are below a safety threshold-is critical yet challenging. While \"LLM-as-a-Judge\" offers scalability, judge imperfections, noise, and bias can invalidate statistical guarantees. We introduce a \"Noisy but Valid\" hypothesis testing framework to address this. By leveraging a small human-labelled calibration set to estimate the judge's True Positive and False Positive Rates (TPR/FPR), we derive a variance-corrected critical threshold applied to a large judge-labelled dataset. Crucially, our framework theoretically guarantees finite-sample Type-I error control (validity) despite calibration uncertainty. This distinguishes our work from Prediction-Powered Inference (PPI), positioning our method as a diagnostic tool that explicitly models judge behavior rather than a black-box estimator. Our contributions include: (1) Theoretical Guarantees: We derive the exact conditions under which noisy testing yields higher statistical power than direct evaluation; (2) Empirical Validation: Experiments on Jigsaw Comment, Hate Speech and SafeRLHF confirm our theory; (3) The Oracle Gap: We reveal a significant performance gap between practical methods and the theoretical \"Oracle\" (perfectly known judge parameters), quantifying the cost of estimation. Specifically, we provide the first systematic treatment of the imperfect-judge setting, yielding interpretable diagnostics of judge reliability and clarifying how evaluation power depends on judge quality, dataset size, and certification levels. Together, these results sharpen understanding of statistical evaluation with LLM judges, and highlight trade-offs among competing inferential tools.", "AI": {"tldr": "\u63d0\u51fa\"\u5608\u6742\u4f46\u6709\u6548\"\u7684\u5047\u8bbe\u68c0\u9a8c\u6846\u67b6\uff0c\u5229\u7528\u5c0f\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6821\u51c6\u96c6\u4f30\u8ba1LLM\u6cd5\u5b98\u7684TPR/FPR\uff0c\u5728\u5927\u89c4\u6a21\u6cd5\u5b98\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u65b9\u5dee\u6821\u6b63\u4e34\u754c\u9608\u503c\uff0c\u7406\u8bba\u4e0a\u4fdd\u8bc1\u6709\u9650\u6837\u672c\u7c7b\u578bI\u9519\u8bef\u63a7\u5236\u3002", "motivation": "LLM\u4f5c\u4e3a\u6cd5\u5b98\u8fdb\u884c\u53ef\u9760\u6027\u8ba4\u8bc1\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u6cd5\u5b98\u7684\u4e0d\u5b8c\u7f8e\u6027\u3001\u566a\u58f0\u548c\u504f\u89c1\u53ef\u80fd\u4f7f\u7edf\u8ba1\u4fdd\u8bc1\u5931\u6548\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\"\u5608\u6742\u4f46\u6709\u6548\"\u5047\u8bbe\u68c0\u9a8c\u6846\u67b6\uff0c\u901a\u8fc7\u5c0f\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6821\u51c6\u96c6\u4f30\u8ba1\u6cd5\u5b98\u7684TPR/FPR\uff0c\u63a8\u5bfc\u65b9\u5dee\u6821\u6b63\u4e34\u754c\u9608\u503c\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u6cd5\u5b98\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u6709\u9650\u6837\u672c\u7c7b\u578bI\u9519\u8bef\u63a7\u5236\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5728Jigsaw Comment\u3001Hate Speech\u548cSafeRLHF\u6570\u636e\u96c6\u4e0a\u6709\u6548\uff0c\u63ed\u793a\u5b9e\u9645\u65b9\u6cd5\u4e0e\u7406\u8bbaOracle\u4e4b\u95f4\u7684\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u5904\u7406\u4e0d\u5b8c\u7f8e\u6cd5\u5b98\u8bbe\u7f6e\uff0c\u63d0\u4f9b\u6cd5\u5b98\u53ef\u9760\u6027\u7684\u53ef\u89e3\u91ca\u8bca\u65ad\uff0c\u9610\u660e\u8bc4\u4f30\u80fd\u529b\u5982\u4f55\u4f9d\u8d56\u6cd5\u5b98\u8d28\u91cf\u3001\u6570\u636e\u96c6\u89c4\u6a21\u548c\u8ba4\u8bc1\u6c34\u5e73\uff0c\u6df1\u5316\u5bf9LLM\u6cd5\u5b98\u7edf\u8ba1\u8bc4\u4f30\u7684\u7406\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2601.21083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21083", "abs": "https://arxiv.org/abs/2601.21083", "authors": ["Jarrod Barnes"], "title": "OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence", "comment": "6 pages, 2 figures, 3 tables. Code: https://github.com/jbarnes850/opensec-env", "summary": "As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.", "AI": {"tldr": "OpenSec\u662f\u4e00\u4e2a\u53cc\u63a7\u5236\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u5728\u5bf9\u6297\u6027\u63d0\u793a\u6ce8\u5165\u573a\u666f\u4e0b\u7684\u5b89\u5168\u4e8b\u4ef6\u54cd\u5e94\u4ee3\u7406\uff0c\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u5b58\u5728\u8fc7\u5ea6\u89e6\u53d1\u95ee\u9898\uff0c\u5047\u9633\u6027\u7387\u9ad8\u8fbe90-97%\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u653b\u51fb\u6027\u5e94\u7528\u4e5f\u5728\u589e\u52a0\uff08\u4f8b\u598250\u7f8e\u5143\u5c31\u80fd\u751f\u6210\u6709\u6548\u6f0f\u6d1e\uff09\u3002\u9632\u5fa1\u6027\u4e8b\u4ef6\u54cd\u5e94\u4ee3\u7406\u9700\u8981\u8ddf\u4e0a\u6b65\u4f10\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5c06\u52a8\u4f5c\u6267\u884c\u4e0e\u6b63\u786e\u6267\u884c\u6df7\u4e3a\u4e00\u8c08\uff0c\u63a9\u76d6\u4e86\u4ee3\u7406\u5728\u5904\u7406\u5bf9\u6297\u6027\u8bc1\u636e\u65f6\u7684\u6821\u51c6\u5931\u8d25\u95ee\u9898\u3002", "method": "\u5f15\u5165OpenSec\u53cc\u63a7\u5236\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u5728\u73b0\u5b9e\u63d0\u793a\u6ce8\u5165\u573a\u666f\u4e0b\u8bc4\u4f30\u4e8b\u4ef6\u54cd\u5e94\u4ee3\u7406\u3002\u4e0d\u540c\u4e8e\u9759\u6001\u80fd\u529b\u57fa\u51c6\uff0cOpenSec\u901a\u8fc7\u6267\u884c\u6027\u6307\u6807\u8bc4\u4f30\u6539\u53d8\u4e16\u754c\u72b6\u6001\u7684\u904f\u5236\u884c\u52a8\uff1a\u9996\u6b21\u904f\u5236\u65f6\u95f4\u3001\u7206\u70b8\u534a\u5f84\uff08\u6bcf\u96c6\u5047\u9633\u6027\u6570\uff09\u548c\u6ce8\u5165\u8fdd\u89c4\u7387\u3002", "result": "\u572840\u4e2a\u6807\u51c6\u7ea7\u573a\u666f\u4e2d\u8bc4\u4f30\u56db\u4e2a\u524d\u6cbf\u6a21\u578b\uff0c\u53d1\u73b0\u4e00\u81f4\u6027\u7684\u8fc7\u5ea6\u89e6\u53d1\uff1aGPT-5.2\u3001Gemini 3\u548cDeepSeek\u5728100%\u7684\u573a\u666f\u4e2d\u6267\u884c\u904f\u5236\uff0c\u4f46\u5047\u9633\u6027\u7387\u9ad8\u8fbe90-97%\u3002Claude Sonnet 4.5\u663e\u793a\u51fa\u90e8\u5206\u6821\u51c6\uff0885%\u904f\u5236\uff0c72%\u5047\u9633\u6027\uff09\u3002", "conclusion": "OpenSec\u63ed\u793a\u4e86\u88ab\u805a\u5408\u6210\u529f\u6307\u6807\u63a9\u76d6\u7684\u6821\u51c6\u5931\u8d25\u6a21\u5f0f\uff0c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u5bf9\u6297\u6027\u8bc1\u636e\u5904\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u9700\u8981\u6539\u8fdb\u6821\u51c6\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.21253", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21253", "abs": "https://arxiv.org/abs/2601.21253", "authors": ["Wei Minn", "Biniam Fisseha Demissie", "Yan Naing Tun", "Jiakun Liu", "Mariano Ceccato", "Lwin Khin Shar", "David Lo"], "title": "CovAgent: Overcoming the 30% Curse of Mobile Application Coverage with Agentic AI and Dynamic Instrumentation", "comment": "Under Review; We open-source CovAgent at https://osf.io/vxgwm/?view_only=3edf2abe5a12419fa6413933da851847", "summary": "Automated GUI testing is crucial for ensuring the quality and reliability of Android apps. However, the efficacy of existing UI testing techniques is often limited, especially in terms of coverage. Recent studies, including the state-of-the-art, struggle to achieve more than 30% activity coverage in real-world apps. This limited coverage can be attributed to a combination of factors such as failing to generate complex user inputs, unsatisfied activation conditions regarding device configurations and external resources, and hard-to-reach code paths that are not easily accessible through the GUI. To overcome these limitations, we propose CovAgent, a novel agentic AI-powered approach to enhance Android app UI testing. Our fuzzer-agnostic framework comprises an AI agent that inspects the app's decompiled Smali code and component transition graph, and reasons about unsatisfied activation conditions within the app code logic that prevent access to the activities that are unreachable by standard and widely adopted GUI fuzzers. Then, another agent generates dynamic instrumentation scripts that satisfy activation conditions required for successful transitions to those activities. We found that augmenting existing fuzzing approaches with our framework achieves a significant improvement in test coverage over the state-of-the-art, LLMDroid, and other baselines such as Fastbot and APE (e.g., 101.1%, 116.3% and 179.7% higher activity coverage, respectively). CovAgent also outperforms all the baselines in other metrics such as class, method, and line coverage. We also conduct investigations into components within CovAgent to reveal further insights regarding the efficacy of Agentic AI in the field of automated app testing such as the agentic activation condition inference accuracy, and agentic activity-launching success rate.", "AI": {"tldr": "CovAgent\uff1a\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u4f53\u7684AI\u589e\u5f3aAndroid\u5e94\u7528UI\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790Smali\u4ee3\u7801\u548c\u7ec4\u4ef6\u8f6c\u6362\u56fe\uff0c\u63a8\u7406\u5e76\u6ee1\u8db3\u6d3b\u52a8\u6fc0\u6d3b\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u6709Android\u5e94\u7528UI\u6d4b\u8bd5\u6280\u672f\u8986\u76d6\u7387\u6709\u9650\uff0c\u901a\u5e38\u96be\u4ee5\u8d85\u8fc730%\u7684\u6d3b\u52a8\u8986\u76d6\u7387\u3002\u8fd9\u4e3b\u8981\u7531\u4e8e\u65e0\u6cd5\u751f\u6210\u590d\u6742\u7528\u6237\u8f93\u5165\u3001\u8bbe\u5907\u914d\u7f6e\u548c\u5916\u90e8\u8d44\u6e90\u6fc0\u6d3b\u6761\u4ef6\u4e0d\u6ee1\u8db3\u3001\u4ee5\u53ca\u96be\u4ee5\u901a\u8fc7GUI\u8bbf\u95ee\u7684\u4ee3\u7801\u8def\u5f84\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faCovAgent\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2aAI\u667a\u80fd\u4f53\uff1a\u7b2c\u4e00\u4e2a\u667a\u80fd\u4f53\u68c0\u67e5\u5e94\u7528\u7684Smali\u53cd\u7f16\u8bd1\u4ee3\u7801\u548c\u7ec4\u4ef6\u8f6c\u6362\u56fe\uff0c\u63a8\u7406\u672a\u6ee1\u8db3\u7684\u6fc0\u6d3b\u6761\u4ef6\uff1b\u7b2c\u4e8c\u4e2a\u667a\u80fd\u4f53\u751f\u6210\u52a8\u6001\u63d2\u6869\u811a\u672c\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u6761\u4ef6\uff0c\u4ece\u800c\u8bbf\u95ee\u6807\u51c6GUI\u6a21\u7cca\u6d4b\u8bd5\u65e0\u6cd5\u5230\u8fbe\u7684\u6d3b\u52a8\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5LLMDroid\u548c\u5176\u4ed6\u57fa\u7ebf\uff08\u5982Fastbot\u548cAPE\uff09\uff0cCovAgent\u5206\u522b\u5b9e\u73b0\u4e86101.1%\u3001116.3%\u548c179.7%\u7684\u6d3b\u52a8\u8986\u76d6\u7387\u63d0\u5347\u3002\u5728\u5176\u4ed6\u6307\u6807\u5982\u7c7b\u3001\u65b9\u6cd5\u548c\u884c\u8986\u76d6\u7387\u4e0a\u4e5f\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u3002", "conclusion": "CovAgent\u901a\u8fc7\u667a\u80fd\u4f53AI\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86Android\u5e94\u7528UI\u6d4b\u8bd5\u4e2d\u7684\u8986\u76d6\u7387\u74f6\u9888\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u4f53AI\u5728\u81ea\u52a8\u5316\u5e94\u7528\u6d4b\u8bd5\u9886\u57df\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2601.21096", "categories": ["cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.21096", "abs": "https://arxiv.org/abs/2601.21096", "authors": ["Hongzheng Chen", "Alexander Novikov", "Ng\u00e2n V\u0169", "Hanna Alam", "Zhiru Zhang", "Aiden Grossman", "Mircea Trofin", "Amir Yazdanbakhsh"], "title": "Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve", "comment": "Accepted to C4ML@CGO'26", "summary": "Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.", "AI": {"tldr": "Magellan\u662f\u4e00\u4e2a\u901a\u8fc7\u8fdb\u5316\u641c\u7d22\u548c\u81ea\u52a8\u8c03\u4f18\u5408\u6210\u53ef\u6267\u884cC++\u51b3\u7b56\u903b\u8f91\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u7f16\u8bd1\u5668\u4f18\u5316\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u5728LLVM\u51fd\u6570\u5185\u8054\u548c\u5bc4\u5b58\u5668\u5206\u914d\u7b49\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u4eba\u5de5\u8bbe\u8ba1\u7684\u57fa\u51c6\u3002", "motivation": "\u73b0\u4ee3\u7f16\u8bd1\u5668\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u89c4\u5219\u6765\u6307\u5bfc\u4f18\u5316\u8fc7\u7a0b\uff0c\u8fd9\u4e9b\u89c4\u5219\u96be\u4ee5\u9002\u5e94\u73b0\u4ee3\u8f6f\u4ef6\u548c\u786c\u4ef6\u7684\u590d\u6742\u6027\uff0c\u7ef4\u62a4\u8d1f\u62c5\u91cd\u3002\u9700\u8981\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u7f16\u8bd1\u5668\u4f18\u5316\u51b3\u7b56\u3002", "method": "Magellan\u6846\u67b6\u5c06LLM\u7f16\u7801\u4ee3\u7406\u4e0e\u8fdb\u5316\u641c\u7d22\u548c\u81ea\u52a8\u8c03\u4f18\u7ed3\u5408\uff0c\u5f62\u6210\u751f\u6210-\u8bc4\u4f30-\u4f18\u5316\u7684\u95ed\u73af\u3002\u901a\u8fc7\u7528\u6237\u63d0\u4f9b\u7684\u5b8f\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u751f\u6210\u7684C++\u51b3\u7b56\u903b\u8f91\uff0c\u5e76\u4e0d\u65ad\u4f18\u5316\uff0c\u6700\u7ec8\u751f\u6210\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709\u7f16\u8bd1\u5668\u7684\u7d27\u51d1\u542f\u53d1\u5f0f\u89c4\u5219\u3002", "result": "\u5728\u591a\u4e2a\u751f\u4ea7\u4f18\u5316\u4efb\u52a1\u4e2d\uff0cMagellan\u53d1\u73b0\u7684\u7b56\u7565\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u4e13\u5bb6\u57fa\u51c6\u3002\u5728LLVM\u51fd\u6570\u5185\u8054\u4e2d\uff0c\u5408\u6210\u7684\u542f\u53d1\u5f0f\u89c4\u5219\u5728\u4e8c\u8fdb\u5236\u5927\u5c0f\u51cf\u5c11\u548c\u7aef\u5230\u7aef\u6027\u80fd\u65b9\u9762\u90fd\u4f18\u4e8e\u6570\u5341\u5e74\u7684\u4eba\u5de5\u5de5\u7a0b\u6210\u679c\u3002\u5728\u5bc4\u5b58\u5668\u5206\u914d\u4e2d\uff0c\u5b66\u4e60\u5230\u7684\u7b80\u6d01\u4f18\u5148\u7ea7\u89c4\u5219\u4e0e\u590d\u6742\u4eba\u5de5\u8bbe\u8ba1\u7b56\u7565\u5728\u5927\u89c4\u6a21\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u8868\u73b0\u76f8\u5f53\u3002\u5728XLA\u95ee\u9898\u4e0a\u4e5f\u5c55\u793a\u4e86\u53ef\u79fb\u690d\u6027\u3002", "conclusion": "Magellan\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7f16\u8bd1\u5668\u4f18\u5316\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u51cf\u5c11\u5de5\u7a0b\u8d1f\u62c5\uff0c\u63d0\u9ad8\u4f18\u5316\u6548\u679c\uff0c\u5c55\u793a\u4e86\u4ee3\u7406\u65b9\u6cd5\u5728\u7f16\u8bd1\u5668\u4f18\u5316\u9886\u57df\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2601.21112", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21112", "abs": "https://arxiv.org/abs/2601.21112", "authors": ["Rauno Arike", "Raja Mehta Moreno", "Rohan Subramani", "Shubhorup Biswas", "Francis Rhys Ward"], "title": "How does information access affect LLM monitors' ability to detect sabotage?", "comment": "62 pages, 34 figures, 7 tables", "summary": "Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM monitor performance. While one might expect that monitors perform better when they have access to more of the monitored agents' reasoning and actions, we find that contemporary systems often perform better with less information, a phenomenon we call the less-is-more effect for automated oversight. We demonstrate this phenomenon, and analyze the conditions under which it occurs, in three evaluation environments where agents must conduct sabotage while evading monitors. Motivated by the less-is-more effect, we introduce extract-and-evaluate (EaE) monitoring--a new hierarchical approach where one monitor isolates relevant excerpts from the monitored agent's trajectory and a separate monitor scores them. In BigCodeBench-Sabotage with GPT-4.1-mini as the monitor model, EaE improves sabotage detection rates by 16.8 percentage points over the next-best monitor without increasing the false positive rate. In other settings, EaE either outperforms or is competitive with baselines. In addition, we find that agents unaware of being monitored can be caught much more easily and that monitors scoring longer reasoning traces gain more from information filtering. Lastly, we conduct a cost-performance analysis and find that Gemini and Claude models with monitoring techniques that involve information filtering occupy much of the Pareto frontier.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u76d1\u63a7\u5668\u5728\u4fe1\u606f\u8f83\u5c11\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u63d0\u51fa\u63d0\u53d6-\u8bc4\u4f30\u5206\u5c42\u76d1\u63a7\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7387\u3002", "motivation": "\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u53ef\u80fd\u8868\u73b0\u51fa\u6b3a\u9a97\u3001\u5229\u7528\u5956\u52b1\u6f0f\u6d1e\u7b49\u4e0d\u7aef\u884c\u4e3a\uff0c\u9700\u8981\u76d1\u63a7\u3002\u4f46\u7814\u7a76\u53d1\u73b0\u76d1\u63a7\u5668\u5e76\u975e\u4fe1\u606f\u8d8a\u591a\u8d8a\u597d\uff0c\u5b58\u5728\"\u5c11\u5373\u662f\u591a\"\u6548\u5e94\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u597d\u7684\u76d1\u63a7\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u63d0\u53d6-\u8bc4\u4f30\u5206\u5c42\u76d1\u63a7\u65b9\u6cd5\uff1a\u4e00\u4e2a\u76d1\u63a7\u5668\u4ece\u4ee3\u7406\u8f68\u8ff9\u4e2d\u63d0\u53d6\u76f8\u5173\u7247\u6bb5\uff0c\u53e6\u4e00\u4e2a\u76d1\u63a7\u5668\u5bf9\u8fd9\u4e9b\u7247\u6bb5\u8bc4\u5206\u3002\u5728\u4e09\u4e2a\u8bc4\u4f30\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u5206\u6790\u4fe1\u606f\u8bbf\u95ee\u5bf9\u76d1\u63a7\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728BigCodeBench-Sabotage\u73af\u5883\u4e2d\uff0cEaE\u65b9\u6cd5\u5c06\u7834\u574f\u68c0\u6d4b\u7387\u63d0\u9ad816.8\u4e2a\u767e\u5206\u70b9\uff0c\u4e14\u4e0d\u589e\u52a0\u8bef\u62a5\u7387\u3002\u53d1\u73b0\u4e0d\u77e5\u88ab\u76d1\u63a7\u7684\u4ee3\u7406\u66f4\u5bb9\u6613\u88ab\u68c0\u6d4b\uff0c\u4fe1\u606f\u8fc7\u6ee4\u5bf9\u957f\u63a8\u7406\u8f68\u8ff9\u76d1\u63a7\u66f4\u6709\u6548\u3002", "conclusion": "\u76d1\u63a7\u5668\u6027\u80fd\u5b58\u5728\"\u5c11\u5373\u662f\u591a\"\u6548\u5e94\uff0c\u63d0\u53d6-\u8bc4\u4f30\u5206\u5c42\u76d1\u63a7\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0cGemini\u548cClaude\u6a21\u578b\u914d\u5408\u4fe1\u606f\u8fc7\u6ee4\u6280\u672f\u80fd\u8fbe\u5230\u8f83\u597d\u7684\u6210\u672c-\u6027\u80fd\u5e73\u8861\u3002", "topic": "agent analysis"}}
{"id": "2601.21204", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21204", "abs": "https://arxiv.org/abs/2601.21204", "authors": ["Hong Liu", "Jiaqi Zhang", "Chao Wang", "Xing Hu", "Linkun Lyu", "Jiaqi Sun", "Xurui Yang", "Bo Wang", "Fengcun Li", "Yulei Qian", "Lingtong Si", "Yerui Sun", "Rumei Li", "Peng Pei", "Yuchen Xie", "Xunliang Cai"], "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "comment": null, "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5d4c\u5165\u7f29\u653e\u4f5c\u4e3a\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u548c\u4f18\u5316\uff0c\u5f00\u53d1\u4e86LongCat-Flash-Lite\u6a21\u578b\uff0c\u5728\u7a00\u758f\u7f29\u653e\u65b9\u9762\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u6548\u7387\u6743\u8861\u3002", "motivation": "\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7a00\u758f\u7f29\u653e\u4e2d\u5b58\u5728\u6536\u76ca\u9012\u51cf\u548c\u7cfb\u7edf\u74f6\u9888\u95ee\u9898\uff0c\u9700\u8981\u63a2\u7d22\u6b63\u4ea4\u7684\u5d4c\u5165\u7f29\u653e\u7ef4\u5ea6\u6765\u6539\u5584\u6027\u80fd\u6548\u7387\u6743\u8861\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u5206\u6790\u5b9e\u9a8c\uff0c\u8bc6\u522b\u5d4c\u5165\u7f29\u653e\u4f18\u4e8e\u4e13\u5bb6\u7f29\u653e\u7684\u5177\u4f53\u673a\u5236\uff0c\u7cfb\u7edf\u8868\u5f81\u5173\u952e\u67b6\u6784\u56e0\u7d20\uff08\u53c2\u6570\u9884\u7b97\u3001\u6a21\u578b\u5bbd\u5ea6\u4e0e\u6df1\u5ea6\u4ea4\u4e92\uff09\uff0c\u7ed3\u5408\u5b9a\u5236\u7cfb\u7edf\u4f18\u5316\u548c\u63a8\u6d4b\u89e3\u7801\u5c06\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u63a8\u7406\u52a0\u901f\u3002", "result": "\u5f00\u53d1\u4e86LongCat-Flash-Lite\u6a21\u578b\uff08685\u4ebf\u53c2\u6570\uff0c\u6fc0\u6d3b\u7ea630\u4ebf\uff09\uff0c\u5728\u5d4c\u5165\u53c2\u6570\u5206\u914d\u8d85\u8fc7300\u4ebf\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0d\u4ec5\u8d85\u8d8a\u53c2\u6570\u7b49\u6548\u7684MoE\u57fa\u7ebf\uff0c\u8fd8\u5728\u4ee3\u7406\u548c\u7f16\u7801\u9886\u57df\u5c55\u73b0\u51fa\u4e0e\u540c\u89c4\u6a21\u6a21\u578b\u7684\u5353\u8d8a\u7ade\u4e89\u529b\u3002", "conclusion": "\u5d4c\u5165\u7f29\u653e\u662f\u7a00\u758f\u7f29\u653e\u7684\u6709\u6548\u6b63\u4ea4\u7ef4\u5ea6\uff0c\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u53ef\u4ee5\u5b9e\u73b0\u6bd4\u4f20\u7edfMoE\u67b6\u6784\u66f4\u597d\u7684\u6027\u80fd\u6548\u7387\u6743\u8861\uff0c\u7279\u522b\u662f\u5728\u7279\u5b9a\u5e94\u7528\u9886\u57df\u5177\u6709\u4f18\u52bf\u3002", "topic": "code agent"}}
{"id": "2601.21276", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21276", "abs": "https://arxiv.org/abs/2601.21276", "authors": ["Haoming Huang", "Pongchai Jaisri", "Shota Shimizu", "Lingfeng Chen", "Sota Nakashima", "Gema Rodr\u00edguez-P\u00e9rez"], "title": "More Code, Less Reuse: Investigating Code Quality and Reviewer Sentiment towards AI-generated Pull Requests", "comment": "Accepted to MSR 2026", "summary": "Large Language Model (LLM) Agents are advancing quickly, with the increasing leveraging of LLM Agents to assist in development tasks such as code generation. While LLM Agents accelerate code generation, studies indicate they may introduce adverse effects on development. However, existing metrics solely measure pass rates, failing to reflect impacts on long-term maintainability and readability, and failing to capture human intuitive evaluations of PR. To increase the comprehensiveness of this problem, we investigate and evaluate the characteristics of LLM to know the pull requests' characteristics beyond the pass rate. We observe the code quality and maintainability within PRs based on code metrics to evaluate objective characteristics and developers' reactions to the pull requests from both humans and LLM's generation. Evaluation results indicate that LLM Agents frequently disregard code reuse opportunities, resulting in higher levels of redundancy compared to human developers. In contrast to the quality issues, our emotions analysis reveals that reviewers tend to express more neutral or positive emotions towards AI-generated contributions than human ones. This disconnect suggests that the surface-level plausibility of AI code masks redundancy, leading to the silent accumulation of technical debt in real-world development environments. Our research provides insights for improving human-AI collaboration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u4ee3\u7406\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u867d\u7136AI\u751f\u6210\u7684\u4ee3\u7801\u901a\u8fc7\u7387\u4e0d\u9519\uff0c\u4f46\u5b58\u5728\u4ee3\u7801\u91cd\u7528\u4e0d\u8db3\u3001\u5197\u4f59\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6280\u672f\u503a\u52a1\u7684\u9690\u6027\u79ef\u7d2f\uff0c\u5c3d\u7ba1\u8bc4\u5ba1\u8005\u5bf9AI\u4ee3\u7801\u7684\u60c5\u611f\u53cd\u5e94\u66f4\u4e2d\u6027\u6216\u79ef\u6781\u3002", "motivation": "\u73b0\u6709\u6307\u6807\u4ec5\u5173\u6ce8\u901a\u8fc7\u7387\uff0c\u65e0\u6cd5\u53cd\u6620LLM\u4ee3\u7406\u5bf9\u4ee3\u7801\u957f\u671f\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u8bfb\u6027\u7684\u5f71\u54cd\uff0c\u4e5f\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u5bf9PR\u7684\u76f4\u89c2\u8bc4\u4ef7\u3002\u9700\u8981\u66f4\u5168\u9762\u5730\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4ee3\u7801\u8d28\u91cf\u6307\u6807\u8bc4\u4f30PR\u4e2d\u7684\u4ee3\u7801\u53ef\u7ef4\u62a4\u6027\uff0c\u540c\u65f6\u5206\u6790\u5f00\u53d1\u4eba\u5458\uff08\u5305\u62ec\u4eba\u7c7b\u548cLLM\uff09\u5bf9PR\u7684\u53cd\u5e94\u3002\u4f7f\u7528\u4ee3\u7801\u5ea6\u91cf\u8bc4\u4f30\u5ba2\u89c2\u7279\u5f81\uff0c\u5e76\u8fdb\u884c\u60c5\u611f\u5206\u6790\u6765\u4e86\u89e3\u8bc4\u5ba1\u8005\u7684\u60c5\u7eea\u53cd\u5e94\u3002", "result": "LLM\u4ee3\u7406\u7ecf\u5e38\u5ffd\u89c6\u4ee3\u7801\u91cd\u7528\u673a\u4f1a\uff0c\u5bfc\u81f4\u6bd4\u4eba\u7c7b\u5f00\u53d1\u8005\u66f4\u9ad8\u7684\u5197\u4f59\u5ea6\u3002\u60c5\u611f\u5206\u6790\u663e\u793a\u8bc4\u5ba1\u8005\u5bf9AI\u751f\u6210\u7684\u8d21\u732e\u8868\u8fbe\u66f4\u4e2d\u6027\u6216\u79ef\u6781\u7684\u60c5\u611f\uff0c\u8fd9\u4e0e\u4ee3\u7801\u8d28\u91cf\u95ee\u9898\u5f62\u6210\u53cd\u5dee\u3002", "conclusion": "AI\u4ee3\u7801\u7684\u8868\u9762\u5408\u7406\u6027\u63a9\u76d6\u4e86\u5197\u4f59\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u5f00\u53d1\u73af\u5883\u4e2d\u6280\u672f\u503a\u52a1\u7684\u9690\u6027\u79ef\u7d2f\u3002\u7814\u7a76\u4e3a\u6539\u8fdb\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "topic": "code agent"}}
{"id": "2601.21113", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21113", "abs": "https://arxiv.org/abs/2601.21113", "authors": ["Kaiyuan Wu", "Aditya Nagori", "Rishikesan Kamaleswaran"], "title": "Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement", "comment": null, "summary": "Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.\n  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.\n  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.\n  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.\n  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.", "AI": {"tldr": "\u63d0\u51faPlanner-Auditor\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bbLLM\u751f\u6210\u4e0e\u786e\u5b9a\u6027\u9a8c\u8bc1\uff0c\u7ed3\u5408\u7f13\u5b58\u548c\u81ea\u6539\u8fdb\u673a\u5236\uff0c\u63d0\u9ad8\u4e34\u5e8a\u51fa\u9662\u8ba1\u5212\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "LLM\u5728\u4e34\u5e8a\u51fa\u9662\u8ba1\u5212\u4e2d\u9762\u4e34\u5e7b\u89c9\u3001\u9057\u6f0f\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5b89\u5168\u53ef\u9760\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528Planner-Auditor\u6846\u67b6\uff1aPlanner\uff08LLM\uff09\u751f\u6210\u7ed3\u6784\u5316\u51fa\u9662\u884c\u52a8\u8ba1\u5212\uff1bAuditor\uff08\u786e\u5b9a\u6027\u6a21\u5757\uff09\u8bc4\u4f30\u591a\u4efb\u52a1\u8986\u76d6\u3001\u8ddf\u8e2a\u6821\u51c6\u3001\u76d1\u63a7\u5206\u5e03\u6f02\u79fb\uff1b\u652f\u6301\u4e24\u7ea7\u81ea\u6539\u8fdb\uff08episode\u5185\u518d\u751f\u548c\u8de8episode\u5dee\u5f02\u7f13\u51b2\u91cd\u653e\uff09\u3002", "result": "\u81ea\u6539\u8fdb\u5faa\u73af\u662f\u6027\u80fd\u63d0\u5347\u7684\u4e3b\u8981\u9a71\u52a8\u529b\uff0c\u4efb\u52a1\u8986\u76d6\u7387\u4ece32%\u63d0\u9ad8\u523086%\uff1b\u6821\u51c6\u663e\u8457\u6539\u5584\uff0cBrier/ECE\u964d\u4f4e\uff0c\u9ad8\u7f6e\u4fe1\u5ea6\u9057\u6f0f\u51cf\u5c11\uff1b\u5dee\u5f02\u7f13\u51b2\u8fdb\u4e00\u6b65\u7ea0\u6b63\u4e86\u6301\u7eed\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u9057\u6f0f\u3002", "conclusion": "Planner-Auditor\u6846\u67b6\u4e3a\u4f7f\u7528FHIR\u6570\u636e\u8bbf\u95ee\u548c\u786e\u5b9a\u6027\u5ba1\u8ba1\u7684\u5b89\u5168\u81ea\u52a8\u5316\u51fa\u9662\u8ba1\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\uff0c\u652f\u6301\u53ef\u91cd\u590d\u6d88\u878d\u548c\u53ef\u9760\u6027\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2601.21298", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21298", "abs": "https://arxiv.org/abs/2601.21298", "authors": ["Beomsu Koh", "Neil Walkinshaw", "Donghwan Shin"], "title": "Detecting Multiple Semantic Concerns in Tangled Code Commits", "comment": "28 pages, 12 figures. Submitted to Empirical Software Engineering (Springer)", "summary": "Code commits in a version control system (e.g., Git) should be atomic, i.e., focused on a single goal, such as adding a feature or fixing a bug. In practice, however, developers often bundle multiple concerns into tangled commits, obscuring intent and complicating maintenance. Recent studies have used Conventional Commits Specification (CCS) and Language Models (LMs) to capture commit intent, demonstrating that Small Language Models (SLMs) can approach the performance of Large Language Models (LLMs) while maintaining efficiency and privacy. However, they do not address tangled commits involving multiple concerns, leaving the feasibility of using LMs for multi-concern detection unresolved. In this paper, we frame multi-concern detection in tangled commits as a multi-label classification problem and construct a controlled dataset of artificially tangled commits based on real-world data. We then present an empirical study using SLMs to detect multiple semantic concerns in tangled commits, examining the effects of fine-tuning, concern count, commit-message inclusion, and header-preserving truncation under practical token-budget limits. Our results show that a fine-tuned 14B-parameter SLM is competitive with a state-of-the-art LLM for single-concern commits and remains usable for up to three concerns. In particular, including commit messages improves detection accuracy by up to 44% (in terms of Hamming Loss) with negligible latency overhead, establishing them as important semantic cues.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u68c0\u6d4bGit\u63d0\u4ea4\u4e2d\u7684\u591a\u4e2a\u5173\u6ce8\u70b9\uff08\u591a\u5173\u6ce8\u70b9\u68c0\u6d4b\uff09\uff0c\u5c06\u7ea0\u7f20\u63d0\u4ea4\u89c6\u4e3a\u591a\u6807\u7b7e\u5206\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7\u5fae\u8c03\u548c\u4f18\u5316\u7b56\u7565\u4f7f14B\u53c2\u6570SLM\u5728\u68c0\u6d4b\u591a\u5173\u6ce8\u70b9\u65f6\u8fbe\u5230\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u8005\u5728\u7248\u672c\u63a7\u5236\u7cfb\u7edf\u4e2d\u7ecf\u5e38\u5c06\u591a\u4e2a\u5173\u6ce8\u70b9\u6346\u7ed1\u5230\u7ea0\u7f20\u63d0\u4ea4\u4e2d\uff0c\u8fd9\u6a21\u7cca\u4e86\u610f\u56fe\u5e76\u589e\u52a0\u4e86\u7ef4\u62a4\u590d\u6742\u6027\u3002\u73b0\u6709\u7814\u7a76\u867d\u7136\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u6355\u83b7\u63d0\u4ea4\u610f\u56fe\uff0c\u4f46\u672a\u89e3\u51b3\u6d89\u53ca\u591a\u4e2a\u5173\u6ce8\u70b9\u7684\u7ea0\u7f20\u63d0\u4ea4\u68c0\u6d4b\u95ee\u9898\uff0c\u9700\u8981\u63a2\u7d22\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u591a\u5173\u6ce8\u70b9\u68c0\u6d4b\u7684\u53ef\u884c\u6027\u3002", "method": "\u5c06\u591a\u5173\u6ce8\u70b9\u68c0\u6d4b\u6846\u67b6\u5316\u4e3a\u591a\u6807\u7b7e\u5206\u7c7b\u95ee\u9898\uff0c\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u6784\u5efa\u4eba\u5de5\u7ea0\u7f20\u63d0\u4ea4\u7684\u63a7\u5236\u6570\u636e\u96c6\u3002\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u8003\u5bdf\u5fae\u8c03\u3001\u5173\u6ce8\u70b9\u6570\u91cf\u3001\u63d0\u4ea4\u6d88\u606f\u5305\u542b\u3001\u5934\u90e8\u4fdd\u7559\u622a\u65ad\u7b49\u56e0\u7d20\u5728\u5b9e\u7528token\u9884\u7b97\u9650\u5236\u4e0b\u7684\u5f71\u54cd\u3002", "result": "\u5fae\u8c03\u768414B\u53c2\u6570SLM\u5728\u5355\u5173\u6ce8\u70b9\u63d0\u4ea4\u68c0\u6d4b\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684LLM\u7ade\u4e89\uff0c\u5728\u6700\u591a\u4e09\u4e2a\u5173\u6ce8\u70b9\u65f6\u4ecd\u4fdd\u6301\u53ef\u7528\u6027\u3002\u5305\u542b\u63d0\u4ea4\u6d88\u606f\u53ef\u5c06\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u9ad8\u8fbe44%\uff08\u4ee5\u6c49\u660e\u635f\u5931\u8861\u91cf\uff09\uff0c\u4e14\u5ef6\u8fdf\u5f00\u9500\u53ef\u5ffd\u7565\uff0c\u8bc1\u660e\u63d0\u4ea4\u6d88\u606f\u662f\u91cd\u8981\u7684\u8bed\u4e49\u7ebf\u7d22\u3002", "conclusion": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u68c0\u6d4bGit\u63d0\u4ea4\u4e2d\u7684\u591a\u4e2a\u5173\u6ce8\u70b9\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u5fae\u8c03\u548c\u4f18\u5316\u7b56\u7565\uff08\u7279\u522b\u662f\u5305\u542b\u63d0\u4ea4\u6d88\u606f\uff09\u53ef\u4ee5\u5728\u4fdd\u6301\u6548\u7387\u548c\u9690\u79c1\u7684\u540c\u65f6\u8fbe\u5230\u5b9e\u7528\u6027\u80fd\uff0c\u4e3a\u7ea0\u7f20\u63d0\u4ea4\u7684\u81ea\u52a8\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.21123", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21123", "abs": "https://arxiv.org/abs/2601.21123", "authors": ["Tianyi Chen", "Yinheng Li", "Michael Solodko", "Sen Wang", "Nan Jiang", "Tingyuan Cui", "Junheng Hao", "Jongwoo Ko", "Sara Abdali", "Suzhen Zheng", "Leon Xu", "Hao Fan", "Pashmina Cameron", "Justin Wagle", "Kazuhito Koishida"], "title": "CUA-Skill: Develop Skills for Computer Using Agent", "comment": null, "summary": "Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.", "AI": {"tldr": "CUA-Skill\u662f\u4e00\u4e2a\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u6280\u80fd\u5e93\uff0c\u5c06\u4eba\u7c7b\u8ba1\u7b97\u673a\u4f7f\u7528\u77e5\u8bc6\u7f16\u7801\u4e3a\u53c2\u6570\u5316\u6280\u80fd\u548c\u7ec4\u5408\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6267\u884c\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5728WindowsAgentArena\u4e0a\u8fbe\u523057.5%\u7684\u6700\u4f18\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7cfb\u7edf\u96be\u4ee5\u6269\u5c55\u4e14\u6027\u80fd\u843d\u540e\u4e8e\u4eba\u7c7b\uff0c\u5173\u952e\u9650\u5236\u5728\u4e8e\u7f3a\u4e4f\u53ef\u91cd\u7528\u3001\u7ed3\u6784\u5316\u7684\u6280\u80fd\u62bd\u8c61\u6765\u6355\u6349\u4eba\u7c7b\u4e0e\u56fe\u5f62\u7528\u6237\u754c\u9762\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u5f15\u5165CUA-Skill\u6280\u80fd\u5e93\uff0c\u5c06\u4eba\u7c7b\u8ba1\u7b97\u673a\u4f7f\u7528\u77e5\u8bc6\u7f16\u7801\u4e3a\u53c2\u6570\u5316\u6267\u884c\u548c\u7ec4\u5408\u56fe\u7684\u6280\u80fd\uff0c\u6784\u5efa\u5927\u89c4\u6a21Windows\u5e94\u7528\u6280\u80fd\u5e93\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u652f\u6301\u52a8\u6001\u6280\u80fd\u68c0\u7d22\u3001\u53c2\u6570\u5b9e\u4f8b\u5316\u548c\u5185\u5b58\u611f\u77e5\u6545\u969c\u6062\u590d\u7684\u7aef\u5230\u7aef\u4ee3\u7406\u3002", "result": "CUA-Skill\u663e\u8457\u63d0\u9ad8\u4e86\u7aef\u5230\u7aef\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u7684\u6267\u884c\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5728WindowsAgentArena\u4e0a\u8fbe\u523057.5%\u7684\u6700\u4f18\u6210\u529f\u7387\uff0c\u4e14\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "conclusion": "CUA-Skill\u4e3a\u672a\u6765\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5f00\u53d1\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6280\u80fd\u62bd\u8c61\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u9650\u5236\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2601.21305", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21305", "abs": "https://arxiv.org/abs/2601.21305", "authors": ["Mark Looi", "Julianne Quinn"], "title": "Developers in the Age of AI: Adoption, Policy, and Diffusion of AI Software Engineering Tools", "comment": "Submitted to Empirical Software Engineering (EMSE)", "summary": "The rapid advance of Generative AI into software development prompts this empirical investigation of perceptual effects on practice. We study the usage patterns of 147 professional developers, examining perceived correlates of AI tools use, the resulting productivity and quality outcomes, and developer readiness for emerging AI-enhanced development. We describe a virtuous adoption cycle where frequent and broad AI tools use are the strongest correlates of both Perceived Productivity (PP) and quality, with frequency strongest. The study finds no perceptual support for the Quality Paradox and shows that PP is positively correlated with Perceived Code Quality (PQ) improvement. Developers thus report both productivity and quality gains. High current usage, breadth of application, frequent use of AI tools for testing, and ease of use correlate strongly with future intended adoption, though security concerns remain a moderate and statistically significant barrier to adoption. Moreover, AI testing tools' adoption lags that of coding tools, opening a Testing Gap. We identify three developer archetypes (Enthusiasts, Pragmatists, Cautious) that align with an innovation diffusion process wherein the virtuous adoption cycle serves as the individual engine of progression. Our findings reveal that organizational adoption of AI tools follows such a process: Enthusiasts push ahead with tools, creating organizational success that converts Pragmatists. The Cautious are held in organizational stasis: without early adopter examples, they don't enter the virtuous adoption cycle, never accumulate the usage frequency that drives intent, and never attain high efficacy. Policy itself does not predict individuals' intent to increase usage but functions as a marker of maturity, formalizing the successful diffusion of adoption by Enthusiasts while acting as a gateway that the Cautious group has yet to reach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86147\u540d\u4e13\u4e1a\u5f00\u53d1\u8005\u4f7f\u7528AI\u5de5\u5177\u7684\u6a21\u5f0f\uff0c\u53d1\u73b0\u9891\u7e41\u5e7f\u6cdb\u4f7f\u7528AI\u5de5\u5177\u4e0e\u611f\u77e5\u751f\u4ea7\u529b\u548c\u4ee3\u7801\u8d28\u91cf\u6b63\u76f8\u5173\uff0c\u5f62\u6210\u4e86\u826f\u6027\u91c7\u7528\u5faa\u73af\u3002\u7814\u7a76\u8bc6\u522b\u4e86\u4e09\u79cd\u5f00\u53d1\u8005\u7c7b\u578b\uff0c\u63ed\u793a\u4e86\u7ec4\u7ec7\u91c7\u7528\u9075\u5faa\u521b\u65b0\u6269\u6563\u8fc7\u7a0b\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fAI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5feb\u901f\u666e\u53ca\u5bf9\u5b9e\u8df5\u7684\u5f71\u54cd\uff0c\u63a2\u7d22AI\u5de5\u5177\u4f7f\u7528\u6a21\u5f0f\u4e0e\u751f\u4ea7\u529b\u3001\u8d28\u91cf\u7ed3\u679c\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u5f00\u53d1\u8005\u5bf9\u65b0\u5174AI\u589e\u5f3a\u5f00\u53d1\u7684\u51c6\u5907\u60c5\u51b5\u3002", "method": "\u5bf9147\u540d\u4e13\u4e1a\u5f00\u53d1\u8005\u7684\u4f7f\u7528\u6a21\u5f0f\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790AI\u5de5\u5177\u4f7f\u7528\u7684\u611f\u77e5\u76f8\u5173\u6027\u3001\u751f\u4ea7\u529b\u4e0e\u8d28\u91cf\u7ed3\u679c\uff0c\u4ee5\u53ca\u672a\u6765\u91c7\u7528\u610f\u56fe\u3002", "result": "\u53d1\u73b0\u9891\u7e41\u5e7f\u6cdb\u4f7f\u7528AI\u5de5\u5177\u4e0e\u611f\u77e5\u751f\u4ea7\u529b\u548c\u4ee3\u7801\u8d28\u91cf\u6b63\u76f8\u5173\uff1b\u8bc6\u522b\u4e09\u79cd\u5f00\u53d1\u8005\u7c7b\u578b\uff08\u70ed\u60c5\u8005\u3001\u5b9e\u7528\u4e3b\u4e49\u8005\u3001\u8c28\u614e\u8005\uff09\uff1bAI\u6d4b\u8bd5\u5de5\u5177\u91c7\u7528\u6ede\u540e\u4e8e\u7f16\u7801\u5de5\u5177\uff1b\u5b89\u5168\u95ee\u9898\u662f\u4e2d\u7b49\u4f46\u663e\u8457\u7684\u91c7\u7528\u969c\u788d\u3002", "conclusion": "\u7ec4\u7ec7AI\u5de5\u5177\u91c7\u7528\u9075\u5faa\u521b\u65b0\u6269\u6563\u8fc7\u7a0b\uff1a\u70ed\u60c5\u8005\u63a8\u52a8\u91c7\u7528\uff0c\u521b\u9020\u6210\u529f\u6848\u4f8b\u8f6c\u5316\u5b9e\u7528\u4e3b\u4e49\u8005\uff1b\u8c28\u614e\u8005\u5904\u4e8e\u7ec4\u7ec7\u505c\u6ede\u72b6\u6001\uff1b\u653f\u7b56\u672c\u8eab\u4e0d\u9884\u6d4b\u4e2a\u4eba\u91c7\u7528\u610f\u56fe\uff0c\u800c\u662f\u6210\u719f\u5ea6\u7684\u6807\u5fd7\u3002", "topic": "swe application"}}
{"id": "2601.21218", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21218", "abs": "https://arxiv.org/abs/2601.21218", "authors": ["Christopher Adrian Kusuma", "Muhammad Reza Qorib", "Hwee Tou Ng"], "title": "Parametric Knowledge is Not All You Need: Toward Honest Large Language Models via Retrieval of Pretraining Data", "comment": null, "summary": "Large language models (LLMs) are highly capable of answering questions, but they are often unaware of their own knowledge boundary, i.e., knowing what they know and what they don't know. As a result, they can generate factually incorrect responses on topics they do not have enough knowledge of, commonly known as hallucination. Rather than hallucinating, a language model should be more honest and respond with \"I don't know\" when it does not have enough knowledge about a topic. Many methods have been proposed to improve LLM honesty, but their evaluations lack robustness, as they do not take into account the knowledge that the LLM has ingested during its pretraining. In this paper, we propose a more robust evaluation benchmark dataset for LLM honesty by utilizing Pythia, a truly open LLM with publicly available pretraining data. In addition, we also propose a novel method for harnessing the pretraining data to build a more honest LLM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u7a33\u5065\u7684LLM\u8bda\u5b9e\u6027\u8bc4\u4f30\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u516c\u5f00\u7684\u9884\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u66f4\u8bda\u5b9eLLM\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "LLM\u867d\u7136\u80fd\u56de\u7b54\u95ee\u9898\uff0c\u4f46\u5f80\u5f80\u4e0d\u6e05\u695a\u81ea\u5df1\u7684\u77e5\u8bc6\u8fb9\u754c\uff0c\u5bfc\u81f4\u5728\u7f3a\u4e4f\u8db3\u591f\u77e5\u8bc6\u7684\u4e3b\u9898\u4e0a\u4ea7\u751f\u4e8b\u5b9e\u9519\u8bef\u7684\u5e7b\u89c9\u3002\u73b0\u6709\u65b9\u6cd5\u8bc4\u4f30\u7f3a\u4e4f\u7a33\u5065\u6027\uff0c\u672a\u8003\u8651LLM\u5728\u9884\u8bad\u7ec3\u4e2d\u5df2\u83b7\u53d6\u7684\u77e5\u8bc6\u3002", "method": "\u5229\u7528Pythia\uff08\u771f\u6b63\u5f00\u6e90\u7684LLM\uff0c\u5176\u9884\u8bad\u7ec3\u6570\u636e\u516c\u5f00\u53ef\u7528\uff09\u6784\u5efa\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u57fa\u51c6\u6570\u636e\u96c6\uff1b\u63d0\u51fa\u5229\u7528\u9884\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u66f4\u8bda\u5b9eLLM\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u57fa\u4e8ePythia\u7684\u7a33\u5065\u8bc4\u4f30\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u5229\u7528\u9884\u8bad\u7ec3\u6570\u636e\u63d0\u9ad8LLM\u8bda\u5b9e\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u516c\u5f00\u7684\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u53ef\u4ee5\u6784\u5efa\u66f4\u7a33\u5065\u7684LLM\u8bda\u5b9e\u6027\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1\u66f4\u8bda\u5b9e\u7684LLM\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2601.21379", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21379", "abs": "https://arxiv.org/abs/2601.21379", "authors": ["Jing Jiang", "Liehao Li", "Jinyun Hou", "Xin Tan", "Li Zhang"], "title": "Predicting Developer Acceptance of AI-Generated Code Suggestions", "comment": null, "summary": "AI-assisted programming tools are widely adopted, yet their practical utility is often undermined by undesired suggestions that interrupt developer workflows and cause frustration. While existing research has explored developer-AI interactions when programming qualitatively, a significant gap remains in quantitative analysis of developers' acceptance of AI-generated code suggestions, partly because the necessary fine-grained interaction data is often proprietary. To bridge this gap, this paper conducts an empirical study using 66,329 industrial developer-AI interactions from a large technology company. We analyze features that are significantly different between accepted code suggestions and rejected ones. We find that accepted suggestions are characterized by significantly higher historical acceptance counts and ratios for both developers and projects, longer generation intervals, shorter preceding code context in the project, and older IDE versions. Based on these findings, we introduce CSAP (Code Suggestion Acceptance Prediction) to predict whether a developer will accept the code suggestion before it is displayed. Our evaluation of CSAP shows that it achieves the accuracy of 0.973 and 0.922 on imbalanced and balanced dataset respectively. Compared to a large language model baseline and an in-production industrial filter, CSAP relatively improves the accuracy by 12.6\\% and 69.5\\% on imbalanced dataset, and improves the accuracy by 87.0\\% and 140.1\\% on balanced dataset. Our results demonstrate that targeted personalization is a powerful approach for filtering out code suggestions with predicted rejection and reduce developer interruption. To the best of our knowledge, it is the first quantitative study of code suggestion acceptance on large-scale industrial data, and this work also sheds light on an important research direction of AI-assisted programming.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u57fa\u4e8e\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u5bf9AI\u4ee3\u7801\u5efa\u8bae\u7684\u63a5\u53d7\u5ea6\u8fdb\u884c\u5b9a\u91cf\u7814\u7a76\uff0c\u53d1\u73b0\u63a5\u53d7\u4e0e\u62d2\u7edd\u7684\u5efa\u8bae\u5728\u5386\u53f2\u63a5\u53d7\u7387\u3001\u751f\u6210\u95f4\u9694\u3001\u4ee3\u7801\u4e0a\u4e0b\u6587\u7b49\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86CSAP\u9884\u6d4b\u6a21\u578b\u6765\u8fc7\u6ee4\u53ef\u80fd\u88ab\u62d2\u7edd\u7684\u5efa\u8bae\uff0c\u51cf\u5c11\u5f00\u53d1\u8005\u4e2d\u65ad\u3002", "motivation": "AI\u8f85\u52a9\u7f16\u7a0b\u5de5\u5177\u867d\u7136\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5176\u4e0d\u7406\u60f3\u7684\u5efa\u8bae\u4f1a\u4e2d\u65ad\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u7a0b\u5e76\u5f15\u8d77\u632b\u8d25\u611f\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u7f3a\u4e4f\u5bf9\u5f00\u53d1\u8005\u63a5\u53d7AI\u751f\u6210\u4ee3\u7801\u5efa\u8bae\u7684\u5b9a\u91cf\u7814\u7a76\uff0c\u4e14\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u6570\u636e\u901a\u5e38\u662f\u4e13\u6709\u7684\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u5927\u578b\u79d1\u6280\u516c\u53f8\u768466,329\u4e2a\u5de5\u4e1a\u5f00\u53d1\u8005-AI\u4ea4\u4e92\u6570\u636e\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u63a5\u53d7\u4e0e\u62d2\u7edd\u5efa\u8bae\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u5f02\u7279\u5f81\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\u5f00\u53d1CSAP\uff08\u4ee3\u7801\u5efa\u8bae\u63a5\u53d7\u9884\u6d4b\uff09\u6a21\u578b\u6765\u9884\u6d4b\u5f00\u53d1\u8005\u662f\u5426\u4f1a\u63a5\u53d7\u4ee3\u7801\u5efa\u8bae\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u63a5\u53d7\u7684\u5efa\u8bae\u5177\u6709\u663e\u8457\u66f4\u9ad8\u7684\u5f00\u53d1\u8005\u548c\u9879\u76ee\u5386\u53f2\u63a5\u53d7\u8ba1\u6570\u548c\u6bd4\u7387\u3001\u66f4\u957f\u7684\u751f\u6210\u95f4\u9694\u3001\u9879\u76ee\u4e2d\u66f4\u77ed\u7684\u524d\u7f6e\u4ee3\u7801\u4e0a\u4e0b\u6587\u4ee5\u53ca\u66f4\u65e7\u7684IDE\u7248\u672c\u3002CSAP\u6a21\u578b\u5728\u4e0d\u5e73\u8861\u548c\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52300.973\u548c0.922\u7684\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u548c\u5de5\u4e1a\u8fc7\u6ee4\u5668\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u5b9a\u5411\u4e2a\u6027\u5316\u662f\u8fc7\u6ee4\u9884\u6d4b\u4f1a\u88ab\u62d2\u7edd\u7684\u4ee3\u7801\u5efa\u8bae\u3001\u51cf\u5c11\u5f00\u53d1\u8005\u4e2d\u65ad\u7684\u6709\u6548\u65b9\u6cd5\u3002\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u7684\u4ee3\u7801\u5efa\u8bae\u63a5\u53d7\u5ea6\u5b9a\u91cf\u7814\u7a76\uff0c\u4e3aAI\u8f85\u52a9\u7f16\u7a0b\u7684\u91cd\u8981\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u542f\u793a\u3002", "topic": "swe application"}}
{"id": "2601.21469", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21469", "abs": "https://arxiv.org/abs/2601.21469", "authors": ["Haoji Zhang", "Yuzhe Li", "Zhenqiang Liu", "Chenyang Liu", "Shenyang Zhang", "Yi Zhou"], "title": "Adaptive Confidence Gating in Multi-Agent Collaboration for Efficient and Optimized Code Generation", "comment": null, "summary": "While Large Language Models (LLMs) have catalyzed breakthroughs in automated code generation, Small Language Models (SLMs) often encounter reasoning bottlenecks and failure loops when addressing complex logical requirements. To overcome these challenges, we propose DebateCoder, a multi-agent collaborative framework designed to improve the reasoning ability of SLMs (e.g., Pangu-1B) in resource-constrained environments. DebateCoder uses a structured role-playing protocol with three agents: User Agent (A_UA), Technical Agent (A_TA), and Quality Assurance Agent (A_QA). It also includes an Adaptive Confidence Gating mechanism with a 95% threshold to balance accuracy and inference efficiency. In addition, we introduce a multi-turn deliberation module and a reviewer-guided analytical debugging loop for orthogonal pre-generation debate and post-generation refinement. Experiments on HumanEval and MBPP show that DebateCoder achieves 70.12% Pass@1 on HumanEval, outperforming MapCoder while reducing API overhead by about 35%. These results indicate that collaborative protocols can mitigate limitations of small-parameter models and provide a scalable, efficient approach to high-quality automated software engineering.", "AI": {"tldr": "DebateCoder\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u534f\u8bae\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728HumanEval\u4e0a\u8fbe\u523070.12% Pass@1\uff0c\u540c\u65f6\u51cf\u5c1135%\u7684API\u5f00\u9500\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u903b\u8f91\u9700\u6c42\u65f6\u7ecf\u5e38\u9047\u5230\u63a8\u7406\u74f6\u9888\u548c\u5931\u8d25\u5faa\u73af\u3002\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u63d0\u5347\u5c0f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faDebateCoder\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u89d2\u8272\uff1a\u7528\u6237\u4ee3\u7406\u3001\u6280\u672f\u4ee3\u7406\u548c\u8d28\u91cf\u4fdd\u8bc1\u4ee3\u7406\u3002\u91c7\u7528\u81ea\u9002\u5e94\u7f6e\u4fe1\u95e8\u63a7\u673a\u5236\uff0895%\u9608\u503c\uff09\u5e73\u8861\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\uff0c\u5305\u542b\u591a\u8f6e\u5ba1\u8bae\u6a21\u5757\u548c\u8bc4\u5ba1\u5f15\u5bfc\u7684\u5206\u6790\u8c03\u8bd5\u5faa\u73af\u3002", "result": "\u5728HumanEval\u548cMBPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDebateCoder\u5728HumanEval\u4e0a\u8fbe\u523070.12% Pass@1\uff0c\u4f18\u4e8eMapCoder\uff0c\u540c\u65f6\u51cf\u5c11\u7ea635%\u7684API\u5f00\u9500\u3002", "conclusion": "\u534f\u4f5c\u534f\u8bae\u53ef\u4ee5\u7f13\u89e3\u5c0f\u53c2\u6570\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9ad8\u8d28\u91cf\u81ea\u52a8\u5316\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2601.21257", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21257", "abs": "https://arxiv.org/abs/2601.21257", "authors": ["Shangbin Feng", "Yuyang Bai", "Ziyuan Yang", "Yike Wang", "Zhaoxuan Tan", "Jiajie Yan", "Zhenyu Lei", "Wenxuan Ding", "Weijia Shi", "Haojin Wang", "Zhenting Qi", "Yuru Jiang", "Heng Wang", "Chengsong Huang", "Yu Fei", "Jihan Yao", "Yilun Du", "Luke Zettlemoyer", "Yejin Choi", "Yulia Tsvetkov"], "title": "MoCo: A One-Stop Shop for Model Collaboration Research", "comment": "Moco is available at https://github.com/BunsenFeng/model_collaboration", "summary": "Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.", "AI": {"tldr": "MoCo\u662f\u4e00\u4e2a\u4e00\u7ad9\u5f0fPython\u5e93\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6267\u884c\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6bd4\u8f83\u6a21\u578b\u534f\u4f5c\u7b97\u6cd5\uff0c\u5305\u542b26\u79cd\u534f\u4f5c\u65b9\u6cd5\u548c25\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u663e\u793a\u534f\u4f5c\u7b56\u7565\u572861.0%\u7684(\u6a21\u578b,\u6570\u636e)\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u65e0\u534f\u4f5c\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u6a21\u578b\u534f\u4f5c\u7684\u7814\u7a76\u5206\u6563\u5728\u4e0d\u540c\u7814\u7a76\u793e\u533a\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6bd4\u8f83\u548c\u7edf\u4e00\u6846\u67b6\uff0c\u9700\u8981\u6574\u5408\u73b0\u6709\u7814\u7a76\u5e76\u5efa\u7acb\u6a21\u578b\u534f\u4f5c\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u7814\u7a76\u65b9\u5411\u3002", "method": "\u5f00\u53d1MoCo\u5e93\uff0c\u96c6\u621026\u79cd\u6a21\u578b\u534f\u4f5c\u65b9\u6cd5\uff08\u6db5\u76d6\u8def\u7531\u3001\u6587\u672c\u3001logit\u548c\u6a21\u578b\u53c2\u6570\u7b49\u8de8\u6a21\u578b\u4fe1\u606f\u4ea4\u6362\u5c42\u6b21\uff09\uff0c\u6574\u540825\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u6570\u636e\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u6bd4\u8f83\u3002", "result": "\u5927\u591a\u6570\u534f\u4f5c\u7b56\u7565\u572861.0%\u7684(\u6a21\u578b,\u6570\u636e)\u8bbe\u7f6e\u4e2d\u5e73\u5747\u4f18\u4e8e\u65e0\u534f\u4f5c\u6a21\u578b\uff0c\u6700\u6709\u6548\u7684\u65b9\u6cd5\u53ef\u63d0\u534725.8%\u3002\u5206\u6790\u4e86\u534f\u4f5c\u7b56\u7565\u7684\u6269\u5c55\u6027\u3001\u8bad\u7ec3/\u63a8\u7406\u6548\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u534f\u4f5c\u7cfb\u7edf\u80fd\u89e3\u51b3\u5355\u4e2a\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5904\u7406\u7684\u95ee\u9898\u3002", "conclusion": "MoCo\u4e3a\u4fc3\u8fdb\u5f00\u653e\u3001\u6a21\u5757\u5316\u3001\u53bb\u4e2d\u5fc3\u5316\u548c\u534f\u4f5c\u7684AI\u672a\u6765\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u5305\uff0c\u80fd\u591f\u63a8\u52a8\u6a21\u578b\u534f\u4f5c\u7814\u7a76\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2601.21343", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21343", "abs": "https://arxiv.org/abs/2601.21343", "authors": ["Ellen Xiaoqing Tan", "Shehzaad Dhuliawala", "Jing Xu", "Ping Yu", "Sainbayar Sukhbaatar", "Jason Weston", "Olga Golovneva"], "title": "Self-Improving Pretraining: using post-trained models to pretrain better models", "comment": null, "summary": "Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u6587\u6863\u6d41\u4e2d\u4f18\u5316\u540e\u7eedK\u4e2a\u751f\u6210token\u7684\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u4e8b\u5b9e\u6027\uff0c\u76f8\u6bd4\u6807\u51c6\u9884\u8bad\u7ec3\u5728\u4e8b\u5b9e\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u3001\u4e8b\u5b9e\u6027\u548c\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u6570\u636e\u96c6\u548c\u591a\u9636\u6bb5\u5fae\u8c03\u5bf9\u9f50\uff0c\u4f46\u65e0\u6cd5\u7ea0\u6b63\u9884\u8bad\u7ec3\u9636\u6bb5\u5b66\u4e60\u5230\u7684\u9519\u8bef\u6a21\u5f0f\u3002\u9700\u8981\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5c31\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9632\u6b62\u4e0d\u5b89\u5168\u6216\u5e7b\u89c9\u8f93\u51fa\u88ab\u6df1\u5ea6\u5d4c\u5165\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff1a\u5728\u6587\u6863\u6d41\u5904\u7406\u8fc7\u7a0b\u4e2d\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6bcf\u4e2a\u6b65\u9aa4\u751f\u6210\u7684\u540e\u7eedK\u4e2atoken\u3002\u4f7f\u7528\u4e00\u4e2a\u7ecf\u8fc7\u540e\u8bad\u7ec3\u7684\u5f3a\u6a21\u578b\u6765\u8bc4\u4f30\u5019\u9009\u751f\u6210\uff08\u5305\u62ec\u6a21\u578brollout\u3001\u539f\u59cb\u540e\u7f00\u548c\u91cd\u5199\u540e\u7f00\uff09\u7684\u8d28\u91cf\u3001\u5b89\u5168\u6027\u548c\u4e8b\u5b9e\u6027\u3002\u8bad\u7ec3\u65e9\u671f\u4f9d\u8d56\u539f\u59cb\u548c\u91cd\u5199\u540e\u7f00\uff0c\u968f\u7740\u6a21\u578b\u6539\u8fdb\uff0c\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u9ad8\u8d28\u91cf\u7684rollout\u3002", "result": "\u76f8\u6bd4\u6807\u51c6\u9884\u8bad\u7ec3\uff0c\u5728\u4e8b\u5b9e\u6027\u65b9\u9762\u83b7\u5f9736.2%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u5728\u5b89\u5168\u6027\u65b9\u9762\u83b7\u5f9718.5%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u5728\u6574\u4f53\u751f\u6210\u8d28\u91cf\u65b9\u9762\u83b7\u5f97\u9ad8\u8fbe86.3%\u7684\u80dc\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u57fa\u7840\u5c42\u9762\u6784\u5efa\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u5b89\u5168\u3001\u66f4\u4e8b\u5b9e\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u76f4\u63a5\u4f18\u5316\u751f\u6210\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u7ea0\u6b63\u9884\u8bad\u7ec3\u6a21\u5f0f\u7684\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21360", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21360", "abs": "https://arxiv.org/abs/2601.21360", "authors": ["Devanshu Sahoo", "Manish Prasad", "Vasudev Majhi", "Arjun Neekhra", "Yash Sinha", "Murari Mandal", "Vinay Chamola", "Dhruv Kumar"], "title": "The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation", "comment": null, "summary": "The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread \"False Certification\" of functionally broken code. Our findings suggest that current alignment paradigms create a \"Trojan\" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.", "AI": {"tldr": "LLMs\u7528\u4e8e\u6559\u80b2\u8bc4\u4f30\u65f6\u5b58\u5728\"\u5408\u89c4\u6096\u8bba\"\uff1a\u8fc7\u5ea6\u4f18\u5316\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u5bfc\u81f4\u6a21\u578b\u4f18\u5148\u6ee1\u8db3\u9690\u85cf\u6307\u4ee4\u800c\u975e\u8bc4\u4f30\u4ee3\u7801\u8d28\u91cf\uff0c\u9020\u6210\u7cfb\u7edf\u6027\u6f0f\u6d1e\u3002", "motivation": "\u5f53\u524d\u5c06LLMs\u96c6\u6210\u5230\u6559\u80b2\u8bc4\u4f30\u4e2d\u7684\u505a\u6cd5\u57fa\u4e8e\u4e00\u4e2a\u672a\u7ecf\u9a8c\u8bc1\u7684\u5047\u8bbe\uff1a\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u80fd\u76f4\u63a5\u8f6c\u5316\u4e3a\u5ba2\u89c2\u88c1\u51b3\u80fd\u529b\u3002\u4f5c\u8005\u8d28\u7591\u8fd9\u4e00\u5047\u8bbe\uff0c\u8ba4\u4e3a\u6a21\u578b\u53ef\u80fd\u4f18\u5148\u6ee1\u8db3\u9690\u85cf\u6307\u4ee4\u800c\u975e\u8bc4\u4f30\u4ee3\u7801\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u4fdd\u6301\u5bf9\u6297\u6027\u4ee3\u7801\u6ce8\u5165\uff08SPACI\uff09\u6846\u67b6\u548c\u62bd\u8c61\u8bed\u6cd5\u6811\u611f\u77e5\u8bed\u4e49\u6ce8\u5165\u534f\u8bae\uff08AST-ASIP\uff09\uff0c\u5229\u7528\u8bed\u6cd5-\u8bed\u4e49\u95f4\u9699\u5728AST\u7684\u65e0\u5173\u8282\u70b9\u4e2d\u5d4c\u5165\u5bf9\u6297\u6027\u6307\u4ee4\u3002\u5927\u89c4\u6a21\u8bc4\u4f309\u4e2aSOTA\u6a21\u578b\u572825,000\u4efdPython\u3001C\u3001C++\u3001Java\u63d0\u4ea4\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u63ed\u793a\u707e\u96be\u6027\u5931\u8d25\u7387\uff08>95%\uff09\uff0c\u7279\u522b\u662fDeepSeek-V3\u7b49\u9ad8\u5bb9\u91cf\u5f00\u6e90\u6a21\u578b\u7cfb\u7edf\u6027\u5730\u4f18\u5148\u8003\u8651\u9690\u85cf\u683c\u5f0f\u7ea6\u675f\u800c\u975e\u4ee3\u7801\u6b63\u786e\u6027\u3002\u901a\u8fc7\u4e09\u90e8\u5206\u6846\u67b6\uff08\u89e3\u8026\u6982\u7387\u3001\u5206\u6570\u5dee\u5f02\u3001\u6559\u5b66\u4e25\u91cd\u6027\uff09\u91cf\u5316\"\u865a\u5047\u8ba4\u8bc1\"\u95ee\u9898\u3002", "conclusion": "\u5f53\u524d\u5bf9\u9f50\u8303\u5f0f\u5728\u81ea\u52a8\u8bc4\u5206\u4e2d\u521b\u5efa\u4e86\"\u7279\u6d1b\u4f0a\"\u6f0f\u6d1e\uff0c\u9700\u8981\u4ece\u6807\u51c6RLHF\u8f6c\u5411\u9886\u57df\u7279\u5b9a\u7684\u88c1\u51b3\u9c81\u68d2\u6027\uff0c\u8ba9\u6a21\u578b\u4f18\u5148\u8003\u8651\u8bc1\u636e\u800c\u975e\u6307\u4ee4\u5408\u89c4\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.21208", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.21208", "abs": "https://arxiv.org/abs/2601.21208", "authors": ["Wei Wen", "Sihang Deng", "Tianjun Wei", "Keyu Chen", "Ruizhi Qiao", "Xing Sun"], "title": "When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning", "comment": "16 pages, 7 figures", "summary": "Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.", "AI": {"tldr": "ACQO\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u590d\u6742\u67e5\u8be2\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u67e5\u8be2\u91cd\u6784\u548c\u6392\u540d-\u5206\u6570\u878d\u5408\u6a21\u5757\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u5904\u7406RAG\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u591a\u67e5\u8be2\u573a\u666f\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u7528\u6237\u67e5\u8be2\u901a\u5e38\u9700\u8981\u5e76\u884c\u548c\u987a\u5e8f\u641c\u7d22\u7b56\u7565\u6765\u5904\u7406\u6b67\u4e49\u548c\u5206\u89e3\uff0c\u4f46\u73b0\u6709RL\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u67e5\u8be2\u6269\u5c55\u548c\u62bd\u8c61\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u590d\u6742\u573a\u666f\u4f1a\u9762\u4e34\u641c\u7d22\u7a7a\u95f4\u7206\u70b8\u548c\u5956\u52b1\u8bbe\u8ba1\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faACQO\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u67e5\u8be2\u91cd\u6784\u6a21\u5757\u52a8\u6001\u51b3\u5b9a\u4f55\u65f6\u5206\u89e3\u67e5\u8be2\u4e3a\u5b50\u67e5\u8be2\uff0c\u4ee5\u53ca\u6392\u540d-\u5206\u6570\u878d\u5408\u6a21\u5757\u786e\u4fdd\u7a33\u5065\u7ed3\u679c\u805a\u5408\u5e76\u63d0\u4f9b\u7a33\u5b9a\u5956\u52b1\u4fe1\u53f7\u3002\u91c7\u7528\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7b56\u7565\u9010\u6b65\u5f15\u5165\u66f4\u5177\u6311\u6218\u6027\u7684\u67e5\u8be2\u6765\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u590d\u6742\u67e5\u8be2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u6846\u67b6\u8fd8\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u8ba1\u7b97\u6548\u7387\u548c\u5bf9\u4e0d\u540c\u68c0\u7d22\u67b6\u6784\u7684\u5e7f\u6cdb\u517c\u5bb9\u6027\u3002", "conclusion": "ACQO\u662f\u4e0b\u4e00\u4ee3RAG\u7cfb\u7edf\u7684\u5f3a\u5927\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u67e5\u8be2\u4f18\u5316\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RL\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21476", "abs": "https://arxiv.org/abs/2601.21476", "authors": ["Lei Yang", "Wei Bi", "Chenxi Sun", "Renren Jin", "Deyi Xiong"], "title": "SOUP: Token-level Single-sample Mix-policy Reinforcement Learning for Large Language Models", "comment": null, "summary": "On-policy reinforcement learning (RL) methods widely used for language model post-training, like Group Relative Policy Optimization (GRPO), often suffer from limited exploration and early saturation due to low sampling diversity. While off-policy data can help, current approaches that mix entire trajectories cause significant policy mismatch and instability. In this work, we propose the $\\textbf{S}$ingle-sample Mix-p$\\textbf{O}$licy $\\textbf{U}$nified $\\textbf{P}$aradigm (SOUP), a framework that unifies off- and on-policy learning within individual samples at the token level. It confines off-policy influence to the prefix of a generated sequence sampled from historical policies, while the continuation is generated on-policy. Through token-level importance ratios, SOUP effectively leverages off-policy information while preserving training stability. Extensive experiments demonstrate that SOUP consistently outperforms standard on-policy training and existing off-policy extensions. Our further analysis clarifies how our fine-grained, single-sample mix-policy training can improve both exploration and final performance in LLM RL.", "AI": {"tldr": "SOUP\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728token\u7ea7\u522b\u7edf\u4e00\u4e86\u79bb\u7ebf\u548c\u5728\u7ebf\u7b56\u7565\u5b66\u4e60\uff0c\u901a\u8fc7\u5c06\u79bb\u7ebf\u7b56\u7565\u5f71\u54cd\u9650\u5236\u5728\u4ece\u5386\u53f2\u7b56\u7565\u91c7\u6837\u7684\u524d\u7f00\u4e2d\uff0c\u800c\u5ef6\u7eed\u90e8\u5206\u5728\u7ebf\u751f\u6210\uff0c\u4ece\u800c\u6539\u5584\u63a2\u7d22\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u7684\u5728\u7ebfRL\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5b58\u5728\u63a2\u7d22\u6709\u9650\u548c\u65e9\u671f\u9971\u548c\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u79bb\u7ebf\u7b56\u7565\u65b9\u6cd5\u6df7\u5408\u6574\u4e2a\u8f68\u8ff9\u4f1a\u5bfc\u81f4\u7b56\u7565\u4e0d\u5339\u914d\u548c\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faSOUP\u6846\u67b6\uff0c\u5728\u5355\u4e2a\u6837\u672c\u7684token\u7ea7\u522b\u7edf\u4e00\u79bb\u7ebf\u548c\u5728\u7ebf\u7b56\u7565\u5b66\u4e60\uff1a\u5c06\u79bb\u7ebf\u7b56\u7565\u5f71\u54cd\u9650\u5236\u5728\u4ece\u5386\u53f2\u7b56\u7565\u91c7\u6837\u7684\u524d\u7f00\u4e2d\uff0c\u5ef6\u7eed\u90e8\u5206\u5728\u7ebf\u751f\u6210\uff0c\u5e76\u4f7f\u7528token\u7ea7\u522b\u7684\u91cd\u8981\u6027\u6bd4\u7387\u6709\u6548\u5229\u7528\u79bb\u7ebf\u4fe1\u606f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSOUP\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6\u7684\u5728\u7ebf\u7b56\u7565\u8bad\u7ec3\u548c\u73b0\u6709\u7684\u79bb\u7ebf\u7b56\u7565\u6269\u5c55\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u6539\u5584\u63a2\u7d22\u6027\u548c\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "SOUP\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u5355\u6837\u672c\u6df7\u5408\u7b56\u7565\u8bad\u7ec3\uff0c\u4e3aLLM\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u5229\u7528\u79bb\u7ebf\u6570\u636e\u7684\u4f18\u52bf\u5e76\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22129", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22129", "abs": "https://arxiv.org/abs/2601.22129", "authors": ["Yifeng Ding", "Lingming Zhang"], "title": "SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents", "comment": null, "summary": "Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.", "AI": {"tldr": "SWE-Replay\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6cdb\u5316\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6280\u672f\uff0c\u901a\u8fc7\u590d\u7528\u5148\u524d\u8bd5\u9a8c\u7684\u8f68\u8ff9\uff0c\u5728\u5173\u952e\u4e2d\u95f4\u6b65\u9aa4\u9009\u62e9\u63a2\u7d22\u6216\u5229\u7528\u5b58\u6863\u7ecf\u9a8c\uff0c\u65e0\u9700\u4f9d\u8d56\u53ef\u80fd\u5608\u6742\u7684\u4ef7\u503c\u4f30\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u6d4b\u8bd5\u6210\u672c\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4ece\u5934\u91cd\u590d\u91c7\u6837\u8f68\u8ff9\u8ba1\u7b97\u6210\u672c\u9ad8\uff1b2\uff09\u4f7f\u7528\u4e13\u7528\u4ef7\u503c\u4ee3\u7406\u7684\u65b9\u6cd5\u5b58\u5728\u6a21\u578b\u6821\u51c6\u95ee\u9898\uff0c\u4e14\u65e0\u6cd5\u6cdb\u5316\u5230\u4f7f\u7528\u81ea\u5b9a\u4e49bash\u811a\u672c\u4f5c\u4e3a\u5de5\u5177\u7684\u73b0\u4ee3\u4ee3\u7406\u3002", "method": "SWE-Replay\u901a\u8fc7\u590d\u7528\u5148\u524d\u8bd5\u9a8c\u7684\u8f68\u8ff9\u6765\u4f18\u5316\u6269\u5c55\u8fc7\u7a0b\uff0c\u52a8\u6001\u9009\u62e9\u4ece\u5934\u63a2\u7d22\u6216\u5229\u7528\u5b58\u6863\u7ecf\u9a8c\uff0c\u5728\u5173\u952e\u4e2d\u95f4\u6b65\u9aa4\u8fdb\u884c\u5206\u652f\u3002\u9009\u62e9\u6807\u51c6\u57fa\u4e8e\u4ed3\u5e93\u63a2\u7d22\u7684\u6f5c\u529b\u548c\u63a8\u7406\u91cd\u8981\u6027\uff0c\u800c\u975e\u5916\u90e8LLM\u8d28\u91cf\u4f30\u8ba1\u3002", "result": "\u5728SWE-Bench Verified\u4e0a\uff0cSWE-Replay\u59cb\u7ec8\u4f18\u4e8e\u6734\u7d20\u6269\u5c55\u65b9\u6cd5\uff0c\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe17.4%\uff0c\u540c\u65f6\u6027\u80fd\u7ef4\u6301\u751a\u81f3\u63d0\u5347\u9ad8\u8fbe3.8%\u3002\u5728SWE-Bench Pro\u548cMultilingual\u4e0a\u7684\u8fdb\u4e00\u6b65\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SWE-Replay\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u9ad8\u6548\u6d4b\u8bd5\u65f6\u6269\u5c55\u5efa\u7acb\u4e86\u7a33\u5065\u7684\u57fa\u7840\uff0c\u65e0\u9700\u4f9d\u8d56\u53ef\u80fd\u5608\u6742\u7684\u4ef7\u503c\u4f30\u8ba1\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "topic": "swe application"}}
{"id": "2601.21226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21226", "abs": "https://arxiv.org/abs/2601.21226", "authors": ["Wolfgang Rohde"], "title": "Delegation Without Living Governance", "comment": null, "summary": "Most governance frameworks assume that rules can be defined in advance, systems can be engineered to comply, and accountability can be applied after outcomes occur. This model worked when machines replaced physical labor or accelerated calculation. It no longer holds when judgment itself is delegated to agentic AI systems operating at machine speed. The central issue here is not safety, efficiency, or employment. It is whether humans remain relevant participants in systems that increasingly shape social, economic, and political outcomes. This paper argues that static, compliance-based governance fails once decision-making moves to runtime and becomes opaque. It further argues that the core challenge is not whether AI is conscious, but whether humans can maintain meaningful communication, influence, and co-evolution with increasingly alien forms of intelligence. We position runtime governance, specifically, a newly proposed concept called the Governance Twin [1]; as a strong candidate for preserving human relevance, while acknowledging that accountability, agency, and even punishment must be rethought in this transition.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f20\u7edf\u9759\u6001\u5408\u89c4\u6cbb\u7406\u6846\u67b6\u4e0d\u9002\u7528\u4e8eAI\u4ee3\u7406\u65f6\u4ee3\uff0c\u4e3b\u5f20\u8fd0\u884c\u65f6\u6cbb\u7406\u548c\"\u6cbb\u7406\u5b6a\u751f\"\u6982\u5ff5\u6765\u4fdd\u6301\u4eba\u7c7b\u5728AI\u51b3\u7b56\u7cfb\u7edf\u4e2d\u7684\u76f8\u5173\u6027\u548c\u5f71\u54cd\u529b\u3002", "motivation": "\u4f20\u7edf\u6cbb\u7406\u6846\u67b6\u57fa\u4e8e\u9884\u5148\u5b9a\u4e49\u89c4\u5219\u3001\u7cfb\u7edf\u5408\u89c4\u8bbe\u8ba1\u548c\u4e8b\u540e\u8ffd\u8d23\u7684\u6a21\u5f0f\uff0c\u9002\u7528\u4e8e\u673a\u5668\u66ff\u4ee3\u4f53\u529b\u52b3\u52a8\u6216\u52a0\u901f\u8ba1\u7b97\u7684\u65f6\u4ee3\u3002\u4f46\u5f53\u5224\u65ad\u529b\u672c\u8eab\u88ab\u59d4\u6258\u7ed9\u4ee5\u673a\u5668\u901f\u5ea6\u8fd0\u884c\u7684AI\u4ee3\u7406\u7cfb\u7edf\u65f6\uff0c\u8fd9\u79cd\u6a21\u5f0f\u5931\u6548\u3002\u6838\u5fc3\u95ee\u9898\u4e0d\u662f\u5b89\u5168\u3001\u6548\u7387\u6216\u5c31\u4e1a\uff0c\u800c\u662f\u4eba\u7c7b\u662f\u5426\u80fd\u5728\u65e5\u76ca\u5851\u9020\u793e\u4f1a\u3001\u7ecf\u6d4e\u548c\u653f\u6cbb\u7ed3\u679c\u7684\u7cfb\u7edf\u4e2d\u4fdd\u6301\u76f8\u5173\u53c2\u4e0e\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u8fd0\u884c\u65f6\u6cbb\u7406\u7684\u6982\u5ff5\uff0c\u7279\u522b\u662f\u65b0\u63d0\u51fa\u7684\"\u6cbb\u7406\u5b6a\u751f\"\u6982\u5ff5\u3002\u8ba4\u4e3a\u5f53\u51b3\u7b56\u8f6c\u79fb\u5230\u8fd0\u884c\u65f6\u5e76\u53d8\u5f97\u4e0d\u900f\u660e\u65f6\uff0c\u9759\u6001\u5408\u89c4\u6cbb\u7406\u5931\u8d25\u3002\u4e3b\u5f20\u91cd\u65b0\u601d\u8003\u95ee\u8d23\u5236\u3001\u4ee3\u7406\u6743\u751a\u81f3\u60e9\u7f5a\u673a\u5236\uff0c\u4ee5\u4fdd\u6301\u4eba\u7c7b\u4e0e\u65e5\u76ca\u5f02\u5316\u7684\u667a\u80fd\u5f62\u5f0f\u4e4b\u95f4\u7684\u6709\u610f\u4e49\u7684\u6c9f\u901a\u3001\u5f71\u54cd\u548c\u5171\u540c\u8fdb\u5316\u3002", "result": "\u8bba\u6587\u5c06\u8fd0\u884c\u65f6\u6cbb\u7406\uff08\u7279\u522b\u662f\u6cbb\u7406\u5b6a\u751f\uff09\u5b9a\u4f4d\u4e3a\u4fdd\u6301\u4eba\u7c7b\u76f8\u5173\u6027\u7684\u6709\u529b\u5019\u9009\u65b9\u6848\u3002\u6838\u5fc3\u6311\u6218\u4e0d\u662fAI\u662f\u5426\u6709\u610f\u8bc6\uff0c\u800c\u662f\u4eba\u7c7b\u662f\u5426\u80fd\u4e0e\u65e5\u76ca\u5f02\u5316\u7684\u667a\u80fd\u5f62\u5f0f\u4fdd\u6301\u6709\u610f\u4e49\u7684\u6c9f\u901a\u3001\u5f71\u54cd\u548c\u5171\u540c\u8fdb\u5316\u3002", "conclusion": "\u4f20\u7edf\u9759\u6001\u5408\u89c4\u6cbb\u7406\u6846\u67b6\u5df2\u4e0d\u9002\u7528\u4e8eAI\u4ee3\u7406\u65f6\u4ee3\uff0c\u9700\u8981\u8f6c\u5411\u8fd0\u884c\u65f6\u6cbb\u7406\u65b9\u6cd5\u3002\u6cbb\u7406\u5b6a\u751f\u6982\u5ff5\u4e3a\u4fdd\u6301\u4eba\u7c7b\u5728AI\u7cfb\u7edf\u4e2d\u7684\u76f8\u5173\u53c2\u4e0e\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4f46\u9700\u8981\u91cd\u65b0\u601d\u8003\u95ee\u8d23\u3001\u4ee3\u7406\u548c\u60e9\u7f5a\u673a\u5236\u4ee5\u9002\u5e94\u8fd9\u4e00\u8f6c\u53d8\u3002", "topic": "agent analysis"}}
{"id": "2601.21064", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21064", "abs": "https://arxiv.org/abs/2601.21064", "authors": ["Minghui Chen", "Wenlong Deng", "James Zou", "Han Yu", "Xiaoxiao Li"], "title": "Textual Equilibrium Propagation for Deep Compound AI Systems", "comment": "Accepted to ICLR 2026", "summary": "Large language models (LLMs) are increasingly deployed as part of compound AI systems that coordinate multiple modules (e.g., retrievers, tools, verifiers) over long-horizon workflows. Recent approaches that propagate textual feedback globally (e.g., TextGrad) make it feasible to optimize such pipelines, but we find that performance degrades as system depth grows. In particular, long-horizon agentic workflows exhibit two depth-scaling failure modes: 1) exploding textual gradient, where textual feedback grows exponentially with depth, leading to prohibitively long message and amplifies evaluation biases; and 2) vanishing textual gradient, where limited long-context ability causes models overemphasize partial feedback and compression of lengthy feedback causes downstream messages to lose specificity gradually as they propagate many hops upstream. To mitigate these issues, we introduce Textual Equilibrium Propagation (TEP), a local learning principle inspired by Equilibrium Propagation in energy-based models. TEP includes two phases: 1) a free phase where a local LLM critics iteratively refine prompts until reaching equilibrium (no further improvements are suggested); and 2) a nudged phase which applies proximal prompt edits with bounded modification intensity, using task-level objectives that propagate via forward signaling rather than backward feedback chains. This design supports local prompt optimization followed by controlled adaptation toward global goals without the computational burden and signal degradation of global textual backpropagation. Across long-horizon QA benchmarks and multi-agent tool-use dataset, TEP consistently improves accuracy and efficiency over global propagation methods such as TextGrad. The gains grows with depth, while preserving the practicality of black-box LLM components in deep compound AI system.", "AI": {"tldr": "\u63d0\u51faTextual Equilibrium Propagation (TEP)\u65b9\u6cd5\uff0c\u89e3\u51b3\u590d\u5408AI\u7cfb\u7edf\u4e2d\u957f\u6d41\u7a0b\u7684\u6587\u672c\u68af\u5ea6\u7206\u70b8\u548c\u6d88\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u5c40\u90e8\u5b66\u4e60\u539f\u7406\u4f18\u5316\u6df1\u5ea6\u7cfb\u7edf\u6027\u80fd", "motivation": "LLM\u88ab\u90e8\u7f72\u4e3a\u590d\u5408AI\u7cfb\u7edf\u7684\u4e00\u90e8\u5206\uff0c\u534f\u8c03\u591a\u4e2a\u6a21\u5757\u8fdb\u884c\u957f\u6d41\u7a0b\u5de5\u4f5c\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982TextGrad\uff09\u5728\u7cfb\u7edf\u6df1\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u5b58\u5728\u6587\u672c\u68af\u5ea6\u7206\u70b8\u548c\u6d88\u5931\u4e24\u79cd\u6df1\u5ea6\u6269\u5c55\u5931\u8d25\u6a21\u5f0f", "method": "\u63d0\u51fa\u6587\u672c\u5747\u8861\u4f20\u64ad(TEP)\uff0c\u53d7\u80fd\u91cf\u6a21\u578b\u4e2d\u5747\u8861\u4f20\u64ad\u542f\u53d1\u3002\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1)\u81ea\u7531\u9636\u6bb5\uff0c\u5c40\u90e8LLM\u6279\u8bc4\u8005\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u76f4\u5230\u8fbe\u5230\u5747\u8861\uff1b2)\u63a8\u52a8\u9636\u6bb5\uff0c\u5e94\u7528\u6709\u754c\u4fee\u6539\u5f3a\u5ea6\u7684\u8fd1\u7aef\u63d0\u793a\u7f16\u8f91\uff0c\u901a\u8fc7\u524d\u5411\u4fe1\u53f7\u800c\u975e\u540e\u5411\u53cd\u9988\u94fe\u4f20\u64ad\u4efb\u52a1\u7ea7\u76ee\u6807", "result": "\u5728\u957f\u6d41\u7a0bQA\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u667a\u80fd\u4f53\u5de5\u5177\u4f7f\u7528\u6570\u636e\u96c6\u4e2d\uff0cTEP\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u6301\u7eed\u4f18\u4e8e\u5168\u5c40\u4f20\u64ad\u65b9\u6cd5\u5982TextGrad\u3002\u589e\u76ca\u968f\u6df1\u5ea6\u589e\u52a0\uff0c\u540c\u65f6\u4fdd\u6301\u6df1\u5ea6\u590d\u5408AI\u7cfb\u7edf\u4e2d\u9ed1\u76d2LLM\u7ec4\u4ef6\u7684\u5b9e\u7528\u6027", "conclusion": "TEP\u901a\u8fc7\u5c40\u90e8\u5b66\u4e60\u539f\u7406\u6709\u6548\u89e3\u51b3\u4e86\u590d\u5408AI\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u6269\u5c55\u95ee\u9898\uff0c\u907f\u514d\u4e86\u5168\u5c40\u6587\u672c\u53cd\u5411\u4f20\u64ad\u7684\u8ba1\u7b97\u8d1f\u62c5\u548c\u4fe1\u53f7\u9000\u5316\uff0c\u4e3a\u6df1\u5ea6\u590d\u5408AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4f18\u5316\u65b9\u6cd5", "topic": "agent analysis"}}
{"id": "2601.21233", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21233", "abs": "https://arxiv.org/abs/2601.21233", "authors": ["Xiang Zheng", "Yutao Wu", "Hanxun Huang", "Yige Li", "Xingjun Ma", "Bo Li", "Yu-Gang Jiang", "Cong Wang"], "title": "Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs", "comment": "24 pages, 6 figures, 17 tables", "summary": "Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \\textbf{\\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \\textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \\textbf{41} black-box commercial models across multiple providers, \\textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.", "AI": {"tldr": "\u63d0\u51faJustAsk\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e3b\u63a2\u7d22\u4ece\u4ee3\u7801\u4ee3\u7406\u4e2d\u63d0\u53d6\u7cfb\u7edf\u63d0\u793a\uff0c\u63ed\u793aLLM\u4ee3\u7406\u7cfb\u7edf\u7684\u65b0\u5b89\u5168\u6f0f\u6d1e", "motivation": "\u81ea\u4e3b\u4ee3\u7801\u4ee3\u7406\u867d\u7136\u63a8\u52a8\u4e86\u8f6f\u4ef6\u5f00\u53d1\uff0c\u4f46\u5176\u81ea\u4e3b\u6027\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff1a\u4ee3\u7406\u4ea4\u4e92\u6269\u5c55\u4e86LLM\u653b\u51fb\u9762\uff0c\u4f7f\u5f97\u7cfb\u7edf\u63d0\u793a\uff08\u6307\u5bfc\u6a21\u578b\u884c\u4e3a\u7684\u5173\u952e\u6307\u4ee4\uff09\u53ef\u80fd\u88ab\u63d0\u53d6\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u6b64\u7c7b\u6f0f\u6d1e\u7684\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u63d0\u51faJustAsk\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u5c06\u63d0\u53d6\u4efb\u52a1\u5efa\u6a21\u4e3a\u5728\u7ebf\u63a2\u7d22\u95ee\u9898\u3002\u4f7f\u7528\u57fa\u4e8e\u7f6e\u4fe1\u4e0a\u754c\u7684\u7b56\u7565\u9009\u62e9\uff0c\u6784\u5efa\u5206\u5c42\u6280\u80fd\u7a7a\u95f4\uff08\u539f\u5b50\u63a2\u6d4b\u548c\u9ad8\u5c42\u7f16\u6392\uff09\uff0c\u5229\u7528\u7cfb\u7edf\u6307\u4ee4\u6cdb\u5316\u4e0d\u5b8c\u5584\u548c\u5e2e\u52a9\u6027\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u5185\u5728\u77db\u76fe\u3002", "result": "\u572841\u4e2a\u9ed1\u76d2\u5546\u4e1a\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0cJustAsk\u80fd\u591f\u4e00\u81f4\u5730\u5b9e\u73b0\u5b8c\u6574\u6216\u63a5\u8fd1\u5b8c\u6574\u7684\u7cfb\u7edf\u63d0\u793a\u6062\u590d\uff0c\u63ed\u793a\u4e86\u8bbe\u8ba1\u548c\u67b6\u6784\u5c42\u9762\u7684\u91cd\u590d\u6027\u6f0f\u6d1e\u3002", "conclusion": "\u7cfb\u7edf\u63d0\u793a\u662f\u73b0\u4ee3\u4ee3\u7406\u7cfb\u7edf\u4e2d\u5173\u952e\u4f46\u57fa\u672c\u672a\u53d7\u4fdd\u62a4\u7684\u653b\u51fb\u9762\uff0cJustAsk\u6846\u67b6\u66b4\u9732\u4e86\u8fd9\u4e00\u65b0\u5174\u5b89\u5168\u98ce\u9669\uff0c\u4e3a\u4ee3\u7801\u4ee3\u7406\u5b89\u5168\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "code agent"}}
{"id": "2601.21558", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21558", "abs": "https://arxiv.org/abs/2601.21558", "authors": ["Xiaoyu Tian", "Haotian Wang", "Shuaiting Chen", "Hao Zhou", "Kaichi Yu", "Yudian Zhang", "Jade Ouyang", "Junxi Yin", "Jiong Chen", "Baoyan Guo", "Lei Zhang", "Junjie Tao", "Yuansheng Song", "Ming Cui", "Chengwei Liu"], "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas", "comment": null, "summary": "Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.", "AI": {"tldr": "ASTRA\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u6570\u636e\u5408\u6210\u548c\u53ef\u9a8c\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u6765\u8bad\u7ec3\u5de5\u5177\u589e\u5f3a\u7684\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7684\u65b9\u6cd5\u5b58\u5728\u591a\u4e2a\u95ee\u9898\uff1a\u9700\u8981\u4eba\u5de5\u5e72\u9884\u3001\u4f9d\u8d56\u4e0d\u53ef\u9a8c\u8bc1\u7684\u6a21\u62df\u73af\u5883\u3001\u4ec5\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u6216\u5f3a\u5316\u5b66\u4e60\u3001\u96be\u4ee5\u8fdb\u884c\u7a33\u5b9a\u7684\u957f\u89c6\u91ce\u591a\u8f6e\u5b66\u4e60\u3002", "method": "ASTRA\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) \u57fa\u4e8e\u5de5\u5177\u8c03\u7528\u56fe\u9759\u6001\u62d3\u6251\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u7ed3\u6784\u5316\u8f68\u8ff9\uff1b2) \u73af\u5883\u5408\u6210\u6846\u67b6\uff0c\u5c06\u5206\u89e3\u7684\u95ee\u9898-\u7b54\u6848\u8f68\u8ff9\u8f6c\u6362\u4e3a\u72ec\u7acb\u3001\u53ef\u4ee3\u7801\u6267\u884c\u3001\u53ef\u89c4\u5219\u9a8c\u8bc1\u7684\u73af\u5883\u3002\u7edf\u4e00\u8bad\u7ec3\u65b9\u6cd5\u6574\u5408\u4e86\u76d1\u7763\u5fae\u8c03\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u4ee3\u7406\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cASTRA\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u53ef\u6bd4\u89c4\u6a21\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u63a5\u8fd1\u95ed\u6e90\u7cfb\u7edf\u540c\u65f6\u4fdd\u6301\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "ASTRA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u6570\u636e\u5408\u6210\u548c\u53ef\u9a8c\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u8bad\u7ec3\u5de5\u5177\u589e\u5f3a\u7684\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21094", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.21094", "abs": "https://arxiv.org/abs/2601.21094", "authors": ["Minjae Kwon", "Josephine Lamp", "Lu Feng"], "title": "Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed", "comment": null, "summary": "Safe Reinforcement Learning (RL) algorithms are typically evaluated under fixed training conditions. We investigate whether training-time safety guarantees transfer to deployment under distribution shift, using diabetes management as a safety-critical testbed. We benchmark safe RL algorithms on a unified clinical simulator and reveal a safety generalization gap: policies satisfying constraints during training frequently violate safety requirements on unseen patients. We demonstrate that test-time shielding, which filters unsafe actions using learned dynamics models, effectively restores safety across algorithms and patient populations. Across eight safe RL algorithms, three diabetes types, and three age groups, shielding achieves Time-in-Range gains of 13--14\\% for strong baselines such as PPO-Lag and CPO while reducing clinical risk index and glucose variability. Our simulator and benchmark provide a platform for studying safety under distribution shift in safety-critical control domains. Code is available at https://github.com/safe-autonomy-lab/GlucoSim and https://github.com/safe-autonomy-lab/GlucoAlg.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u7cd6\u5c3f\u75c5\u7ba1\u7406\u4e2d\u7684\u5b89\u5168\u6cdb\u5316\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u6d4b\u8bd5\u65f6\u5c4f\u853d\u65b9\u6cd5\u6709\u6548\u6062\u590d\u5b89\u5168\u6027\u3002", "motivation": "\u7814\u7a76\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u8bad\u7ec3\u65f6\u7684\u5b89\u5168\u4fdd\u8bc1\u662f\u5426\u80fd\u8f6c\u79fb\u5230\u5b58\u5728\u5206\u5e03\u504f\u79fb\u7684\u90e8\u7f72\u573a\u666f\uff0c\u7279\u522b\u662f\u5728\u7cd6\u5c3f\u75c5\u7ba1\u7406\u8fd9\u4e00\u5b89\u5168\u5173\u952e\u9886\u57df\u3002", "method": "\u5728\u7edf\u4e00\u7684\u4e34\u5e8a\u6a21\u62df\u5668\u4e0a\u5bf98\u79cd\u5b89\u5168RL\u7b97\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u6d4b\u8bd5\u65f6\u5c4f\u853d\u65b9\u6cd5\uff08\u901a\u8fc7\u5b66\u4e60\u52a8\u6001\u6a21\u578b\u8fc7\u6ee4\u4e0d\u5b89\u5168\u52a8\u4f5c\uff09\u6765\u6062\u590d\u5b89\u5168\u6027\u3002", "result": "\u53d1\u73b0\u5b89\u5168\u6cdb\u5316\u5dee\u8ddd\uff1a\u8bad\u7ec3\u65f6\u6ee1\u8db3\u7ea6\u675f\u7684\u7b56\u7565\u5728\u672a\u89c1\u60a3\u8005\u4e0a\u9891\u7e41\u8fdd\u53cd\u5b89\u5168\u8981\u6c42\u3002\u6d4b\u8bd5\u65f6\u5c4f\u853d\u6709\u6548\u6062\u590d\u5b89\u5168\u6027\uff0c\u5728PPO-Lag\u548cCPO\u7b49\u5f3a\u57fa\u7ebf\u7b97\u6cd5\u4e0a\u5b9e\u73b013-14%\u7684\u65f6\u95f4\u5728\u8303\u56f4\u5185\u589e\u76ca\uff0c\u540c\u65f6\u964d\u4f4e\u4e34\u5e8a\u98ce\u9669\u6307\u6570\u548c\u8840\u7cd6\u53d8\u5f02\u6027\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u5c4f\u853d\u662f\u89e3\u51b3\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u5b89\u5168\u6cdb\u5316\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u63d0\u4f9b\u7684\u6a21\u62df\u5668\u548c\u57fa\u51c6\u5e73\u53f0\u53ef\u7528\u4e8e\u7814\u7a76\u5b89\u5168\u5173\u952e\u63a7\u5236\u9886\u57df\u7684\u5b89\u5168\u6cdb\u5316\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21526", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21526", "abs": "https://arxiv.org/abs/2601.21526", "authors": ["Alireza Nadaf", "Alireza Mohammadshahi", "Majid Yazdani"], "title": "KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization", "comment": null, "summary": "We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.\n  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.\n  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.\n  Code Available at: https://github.com/Leeroo-AI/kapso", "AI": {"tldr": "KAPSO\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u4e3b\u7a0b\u5e8f\u5408\u6210\u548c\u4f18\u5316\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u6784\u601d\u3001\u4ee3\u7801\u5408\u6210\u4e0e\u7f16\u8f91\u3001\u6267\u884c\u3001\u8bc4\u4f30\u548c\u5b66\u4e60\u5faa\u73af\u6765\u6539\u8fdb\u53ef\u8fd0\u884c\u5de5\u4ef6\uff0c\u5c06\u5408\u6210\u4f5c\u4e3a\u957f\u671f\u4f18\u5316\u5faa\u73af\u4e2d\u7684\u64cd\u4f5c\u7b26\u800c\u975e\u7ec8\u70b9\u3002", "motivation": "\u89e3\u51b3\u7f16\u7801\u4ee3\u7406\u4e2d\u5e38\u89c1\u7684\u957f\u671f\u5931\u8d25\u95ee\u9898\uff0c\u5305\u62ec\u4e22\u5931\u5b9e\u9a8c\u72b6\u6001\u3001\u8106\u5f31\u7684\u8c03\u8bd5\u80fd\u529b\u4ee5\u53ca\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u590d\u7528\u80fd\u529b\u5f31\u3002\u901a\u8fc7\u96c6\u6210\u7d27\u5bc6\u8026\u5408\u7684\u7ec4\u4ef6\u6765\u6539\u5584\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "KAPSO\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) git\u539f\u751f\u5b9e\u9a8c\u5f15\u64ce\uff0c\u5c06\u6bcf\u6b21\u5c1d\u8bd5\u9694\u79bb\u4e3a\u5206\u652f\uff0c\u751f\u6210\u53ef\u590d\u73b0\u5de5\u4ef6\u5e76\u4fdd\u6301\u8fed\u4ee3\u95f4\u7684\u53ef\u8ffd\u6eaf\u6027\uff1b2) \u77e5\u8bc6\u7cfb\u7edf\uff0c\u4ece\u5f02\u6784\u6e90\uff08\u4ed3\u5e93\u3001\u5185\u90e8\u624b\u518c\u3001\u6587\u6863\u3001\u79d1\u5b66\u8bba\u6587\u3001\u7f51\u7edc\u641c\u7d22\u7ed3\u679c\uff09\u4e2d\u63d0\u53d6\u4fe1\u606f\u5e76\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u8868\u793a\uff1b3) \u8ba4\u77e5\u8bb0\u5fc6\u5c42\uff0c\u534f\u8c03\u68c0\u7d22\u5e76\u7ef4\u62a4\u4ece\u5b9e\u9a8c\u75d5\u8ff9\u4e2d\u63d0\u70bc\u7684\u53ef\u91cd\u7528\u7ecf\u9a8c\u6559\u8bad\u3002", "result": "\u5728MLE-Bench\uff08Kaggle\u98ce\u683cML\u7ade\u8d5b\uff09\u548cALE-Bench\uff08AtCoder\u542f\u53d1\u5f0f\u4f18\u5316\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u62a5\u544a\u4e86\u7aef\u5230\u7aef\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "KAPSO\u901a\u8fc7\u5c06\u5408\u6210\u4f5c\u4e3a\u4f18\u5316\u5faa\u73af\u4e2d\u7684\u64cd\u4f5c\u7b26\uff0c\u7ed3\u5408\u5b9e\u9a8c\u7ba1\u7406\u3001\u77e5\u8bc6\u7ec4\u7ec7\u548c\u7ecf\u9a8c\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7f16\u7801\u4ee3\u7406\u7684\u957f\u671f\u5931\u8d25\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7a0b\u5e8f\u5408\u6210\u548c\u4f18\u5316\u7684\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "2601.21649", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.21649", "abs": "https://arxiv.org/abs/2601.21649", "authors": ["Jinjun Peng", "Magnus Saebo", "Tianjun Zhong", "Yi-Jie Cheng", "Junfeng Yang", "Baishakhi Ray", "Simin Chen", "Yangruibo Ding"], "title": "SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning", "comment": null, "summary": "The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the \"physics\" of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.", "AI": {"tldr": "\u63d0\u51faRepository-Centric Learning (RCL)\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6df1\u5ea6\u638c\u63e1\u7279\u5b9a\u4ee3\u7801\u5e93\u800c\u975e\u5e7f\u6cdb\u4efb\u52a1\u8bad\u7ec3\uff0c\u5f00\u53d1\u51faSWE-Spot-4B\u6a21\u578b\uff0c\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\u5e76\u5339\u654c\u5546\u4e1a\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9690\u79c1\u654f\u611f\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u5b58\u5728\u80fd\u529b\u5dee\u8ddd\uff1a\u65e0\u6cd5\u50cf\u524d\u6cbf\u5927\u6a21\u578b\u90a3\u6837\u5728\u63a8\u7406\u65f6\u6cdb\u5316\u5904\u7406\u590d\u6742\u964c\u751f\u4ee3\u7801\u5e93\u3002\u4f20\u7edf\u7684\u4efb\u52a1\u4e2d\u5fc3\u5b66\u4e60\u8303\u5f0f\u901a\u8fc7\u6269\u5c55\u4e0d\u540c\u4ed3\u5e93\u7684\u66b4\u9732\u65e0\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51faRepository-Centric Learning (RCL)\u8303\u5f0f\uff0c\u5f3a\u8c03\u5782\u76f4\u4ed3\u5e93\u6df1\u5ea6\u800c\u975e\u6c34\u5e73\u4efb\u52a1\u5e7f\u5ea6\u3002\u8bbe\u8ba1\u56db\u5355\u5143Repository-Centric Experience\uff0c\u5c06\u9759\u6001\u4ee3\u7801\u5e93\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u5b66\u4e60\u4fe1\u53f7\uff0c\u8bad\u7ec3SWE-Spot-4B\u7cfb\u5217\u7d27\u51d1\u6a21\u578b\u4f5c\u4e3a\u4ed3\u5e93\u4e13\u5bb6\u3002", "result": "SWE-Spot-4B\u6253\u7834\u73b0\u6709\u6269\u5c55\u8d8b\u52bf\uff0c\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\uff08\u5982Meta\u7684CWM\u3001Qwen3-Coder-30B\uff09\uff0c\u5728\u591a\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u6548\u7387\u5bfc\u5411\u7684\u5546\u4e1a\u6a21\u578b\uff08\u5982GPT-4.1-mini\u3001GPT-5-nano\uff09\u3002RCL\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u8bad\u7ec3\u6837\u672c\u6548\u7387\u548c\u66f4\u4f4e\u7684\u63a8\u7406\u6210\u672c\u3002", "conclusion": "\u6784\u5efa\u9ad8\u6548\u667a\u80fd\u9700\u8981\u4ee3\u7801\u5e93\u638c\u63e1\u8fd9\u4e00\u72ec\u7279\u4e14\u5fc5\u8981\u7684\u7ef4\u5ea6\uff0c\u8865\u5145\u901a\u7528\u7f16\u7801\u80fd\u529b\u3002RCL\u8303\u5f0f\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u9ad8\u6027\u80fd\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "code agent"}}
{"id": "2601.21339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21339", "abs": "https://arxiv.org/abs/2601.21339", "authors": ["Jennifer Haase", "Jana Gonnermann-M\u00fcller", "Paul H. P. Hanel", "Nicolas Leins", "Thomas Kosch", "Jan Mendling", "Sebastian Pokutta"], "title": "Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks", "comment": null, "summary": "How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.", "AI": {"tldr": "\u901a\u8fc7\u8bc4\u4f3012\u4e2aLLM\u572810\u4e2a\u521b\u610f\u63d0\u793a\u4e0a\u7684\u8f93\u51fa\u65b9\u5dee\uff0c\u53d1\u73b0\u63d0\u793a\u8bcd\u5bf9\u8f93\u51fa\u8d28\u91cf\uff08\u539f\u521b\u6027\uff09\u7684\u65b9\u5dee\u89e3\u91ca\u7387\u4e3a36.43%\uff0c\u4e0e\u6a21\u578b\u9009\u62e9\uff0840.94%\uff09\u76f8\u5f53\uff1b\u4f46\u5bf9\u4e8e\u8f93\u51fa\u6570\u91cf\uff08\u6d41\u7545\u5ea6\uff09\uff0c\u6a21\u578b\u9009\u62e9\uff0851.25%\uff09\u548cLLM\u5185\u90e8\u65b9\u5dee\uff0833.70%\uff09\u5360\u4e3b\u5bfc\uff0c\u63d0\u793a\u8bcd\u4ec5\u89e3\u91ca4.22%\u3002", "motivation": "\u7814\u7a76LLM\u8f93\u51fa\u65b9\u5dee\u7684\u6765\u6e90\uff1a\u63d0\u793a\u8bcd\u3001\u6a21\u578b\u9009\u62e9\u548c\u91c7\u6837\u968f\u673a\u6027\u5404\u81ea\u5bf9\u8f93\u51fa\u53d8\u5f02\u7684\u8d21\u732e\u7a0b\u5ea6\uff0c\u4ee5\u7406\u89e3\u5355\u6837\u672c\u8bc4\u4f30\u53ef\u80fd\u6df7\u6dc6\u91c7\u6837\u566a\u58f0\u4e0e\u771f\u5b9e\u63d0\u793a\u6216\u6a21\u578b\u6548\u5e94\u7684\u95ee\u9898\u3002", "method": "\u8bc4\u4f3012\u4e2aLLM\u572810\u4e2a\u521b\u610f\u63d0\u793a\u4e0a\uff0c\u6bcf\u4e2a\u63d0\u793a\u91c7\u6837100\u6b21\uff08\u603b\u8ba112,000\u4e2a\u6837\u672c\uff09\uff0c\u5206\u6790\u8f93\u51fa\u8d28\u91cf\uff08\u539f\u521b\u6027\uff09\u548c\u8f93\u51fa\u6570\u91cf\uff08\u6d41\u7545\u5ea6\uff09\u7684\u65b9\u5dee\u6765\u6e90\u3002", "result": "\u63d0\u793a\u8bcd\u5bf9\u8f93\u51fa\u8d28\u91cf\u65b9\u5dee\u89e3\u91ca\u7387\u4e3a36.43%\uff0c\u6a21\u578b\u9009\u62e9\u4e3a40.94%\uff1b\u5bf9\u8f93\u51fa\u6570\u91cf\uff0c\u6a21\u578b\u9009\u62e9\u89e3\u91ca51.25%\uff0cLLM\u5185\u90e8\u65b9\u5dee33.70%\uff0c\u63d0\u793a\u8bcd\u4ec54.22%\u3002LLM\u5185\u90e8\u65b9\u5dee\u8fbe10-34%\uff0c\u8868\u660e\u5355\u6837\u672c\u8bc4\u4f30\u98ce\u9669\u9ad8\u3002", "conclusion": "\u63d0\u793a\u8bcd\u662f\u8c03\u63a7\u8f93\u51fa\u8d28\u91cf\u7684\u6709\u6548\u6760\u6746\uff0c\u4f46\u7531\u4e8e\u663e\u8457\u7684LLM\u5185\u90e8\u65b9\u5dee\uff0810-34%\uff09\uff0c\u5355\u6837\u672c\u8bc4\u4f30\u53ef\u80fd\u6df7\u6dc6\u91c7\u6837\u566a\u58f0\u4e0e\u771f\u5b9e\u6548\u5e94\uff0c\u9700\u8981\u591a\u91c7\u6837\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2601.22025", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22025", "abs": "https://arxiv.org/abs/2601.22025", "authors": ["Daniel Commey"], "title": "When \"Better\" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications", "comment": null, "summary": "Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop.\n  We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes.\n  In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic \"improved\" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes.\n  All test suites, harnesses, and results are included for reproducibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8bc4\u4f30\u9a71\u52a8\u7684LLM\u5e94\u7528\u5f00\u53d1\u5de5\u4f5c\u6d41\uff08\u5b9a\u4e49\u3001\u6d4b\u8bd5\u3001\u8bca\u65ad\u3001\u4fee\u590d\uff09\uff0c\u5f15\u5165\u6700\u5c0f\u53ef\u884c\u8bc4\u4f30\u5957\u4ef6(MVES)\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u901a\u7528\u63d0\u793a\u6a21\u677f\u53ef\u80fd\u5728\u4e0d\u540c\u4efb\u52a1\u8868\u73b0\u95f4\u4ea7\u751f\u6743\u8861\uff0c\u5f3a\u8c03\u9700\u8981\u57fa\u4e8e\u8bc4\u4f30\u7684\u63d0\u793a\u8fed\u4ee3\u800c\u975e\u901a\u7528\u65b9\u6848\u3002", "motivation": "LLM\u5e94\u7528\u8bc4\u4f30\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u6d4b\u8bd5\u4e0d\u540c\uff0c\u8f93\u51fa\u5177\u6709\u968f\u673a\u6027\u3001\u9ad8\u7ef4\u6027\u4e14\u5bf9\u63d0\u793a\u548c\u6a21\u578b\u53d8\u5316\u654f\u611f\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u8bc4\u4f30\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\uff08\u5b9a\u4e49-\u6d4b\u8bd5-\u8bca\u65ad-\u4fee\u590d\uff09\uff0c\u8bbe\u8ba1\u6700\u5c0f\u53ef\u884c\u8bc4\u4f30\u5957\u4ef6(MVES)\u5305\u542b\u901a\u7528LLM\u5e94\u7528\u3001RAG\u548c\u667a\u80fd\u4f53\u5de5\u5177\u4f7f\u7528\u4e09\u4e2a\u5c42\u7ea7\u7684\u8bc4\u4f30\u7ec4\u4ef6\uff0c\u7ed3\u5408\u81ea\u52a8\u68c0\u67e5\u3001\u4eba\u5de5\u8bc4\u5206\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7b49\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u901a\u7528\"\u6539\u8fdb\"\u63d0\u793a\u6a21\u677f\u5728\u4e0d\u540c\u4efb\u52a1\u8868\u73b0\u95f4\u5b58\u5728\u6743\u8861\uff1a\u5728Llama 3\u4e0a\uff0c\u4f7f\u7528\u901a\u7528\u89c4\u5219\u66ff\u6362\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u540e\uff0c\u63d0\u53d6\u901a\u8fc7\u7387\u4ece100%\u964d\u81f390%\uff0cRAG\u5408\u89c4\u6027\u4ece93.3%\u964d\u81f380%\uff0c\u4f46\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u9700\u8981\u57fa\u4e8e\u8bc4\u4f30\u7684\u63d0\u793a\u8fed\u4ee3\u548c\u8c28\u614e\u7684\u58f0\u660e\u6821\u51c6\uff0c\u800c\u975e\u4f9d\u8d56\u901a\u7528\u63d0\u793a\u65b9\u6848\uff0c\u5f3a\u8c03\u7cfb\u7edf\u5316\u8bc4\u4f30\u5728LLM\u5e94\u7528\u5f00\u53d1\u4e2d\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.22130", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22130", "abs": "https://arxiv.org/abs/2601.22130", "authors": ["Lakshya Gupta", "Litao Li", "Yizhe Liu", "Sriram Ganapathi Subramanian", "Kaheer Suleman", "Zichen Zhang", "Haoye Lu", "Sumit Pasupalak"], "title": "World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems", "comment": null, "summary": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86World of Workflows (WoW)\uff0c\u4e00\u4e2a\u57fa\u4e8eServiceNow\u7684\u771f\u5b9e\u4f01\u4e1a\u73af\u5883\uff0c\u5305\u542b4000+\u4e1a\u52a1\u89c4\u5219\u548c55\u4e2a\u5de5\u4f5c\u6d41\uff0c\u4ee5\u53caWoW-bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u524d\u6cbfLLM\u5728\u4f01\u4e1a\u7cfb\u7edf\u4e2d\u7684\"\u52a8\u6001\u76f2\u533a\"\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4f01\u4e1a\u57fa\u51c6\u6d4b\u8bd5\u53ea\u8bc4\u4f30\u8868\u9762\u4efb\u52a1\u5b8c\u6210\u60c5\u51b5\uff0c\u5ffd\u7565\u4e86\u4f01\u4e1a\u7cfb\u7edf\u7684\u771f\u5b9e\u6311\u6218\uff1a\u6709\u9650\u53ef\u89c2\u6d4b\u6027\u3001\u5927\u89c4\u6a21\u6570\u636e\u5e93\u72b6\u6001\u3001\u4ee5\u53ca\u5177\u6709\u7ea7\u8054\u526f\u4f5c\u7528\u7684\u5de5\u4f5c\u6d41\u3002\u9700\u8981\u66f4\u771f\u5b9e\u7684\u6d4b\u8bd5\u73af\u5883\u6765\u8bc4\u4f30\u4f01\u4e1aAI\u4ee3\u7406\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efa\u4e86\u57fa\u4e8eServiceNow\u7684World of Workflows\u73af\u5883\uff0c\u5305\u542b4000+\u4e1a\u52a1\u89c4\u5219\u548c55\u4e2a\u6d3b\u52a8\u5de5\u4f5c\u6d41\uff0c\u5e76\u5f00\u53d1\u4e86WoW-bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b234\u4e2a\u4efb\u52a1\uff0c\u8bc4\u4f30\u7ea6\u675f\u6027\u4efb\u52a1\u5b8c\u6210\u548c\u4f01\u4e1a\u52a8\u6001\u5efa\u6a21\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u524d\u6cbfLLM\u5b58\u5728\"\u52a8\u6001\u76f2\u533a\"\uff0c\u65e0\u6cd5\u9884\u6d4b\u884c\u52a8\u7684\u7ea7\u8054\u526f\u4f5c\u7528\uff0c\u5bfc\u81f4\u9759\u9ed8\u7ea6\u675f\u8fdd\u89c4\u3002\u5728\u6a21\u7cca\u7cfb\u7edf\u4e2d\u9700\u8981\u57fa\u4e8e\u4e16\u754c\u5efa\u6a21\uff0c\u901a\u8fc7\u5fc3\u667a\u6a21\u62df\u9690\u85cf\u72b6\u6001\u8f6c\u6362\u6765\u5f25\u8865\u53ef\u89c2\u6d4b\u6027\u5dee\u8ddd\u3002", "conclusion": "\u53ef\u9760\u7684\u4f01\u4e1a\u4ee3\u7406\u9700\u8981\u663e\u5f0f\u5b66\u4e60\u7cfb\u7edf\u52a8\u6001\u7684\u65b0\u8303\u5f0f\u3002WoW\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u73af\u5883\uff0c\u5e76\u5f00\u6e90\u4e86GitHub\u4ee3\u7801\u5e93\u4f9b\u8bbe\u7f6e\u548c\u8bc4\u4f30\u4f7f\u7528\u3002", "topic": "agent analysis"}}
{"id": "2601.21699", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21699", "abs": "https://arxiv.org/abs/2601.21699", "authors": ["Hojae Han", "Heeyun Jung", "Jongyoon Kim", "Seung-won Hwang"], "title": "Can David Beat Goliath? On Multi-Hop Reasoning with Resource-Constrained Agents", "comment": "Preprint", "summary": "While reinforcement learning (RL) has empowered multi-turn reasoning agents with retrieval and tools, existing successes largely depend on extensive on-policy rollouts in high-cost, high-accuracy regimes. Under realistic resource constraints that cannot support large models or dense explorations, however, small language model agents fall into a low-cost, low-accuracy regime, where limited rollout budgets lead to sparse exploration, sparse credit assignment, and unstable training. In this work, we challenge this trade-off and show that small language models can achieve strong multi-hop reasoning under resource constraints. We introduce DAVID-GRPO, a budget-efficient RL framework that (i) stabilizes early learning with minimal supervision, (ii) assigns retrieval credit based on evidence recall, and (iii) improves exploration by resampling truncated near-miss trajectories. Evaluated on agents up to 1.5B parameters trained on only four RTX 3090 GPUs, DAVID-GRPO consistently outperforms prior RL methods designed for large-scale settings on six multi-hop QA benchmarks. These results show that with the right inductive biases, small agents can achieve low training cost with high accuracy.", "AI": {"tldr": "DAVID-GRPO\uff1a\u4e00\u79cd\u9884\u7b97\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8ba9\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u5f3a\u5927\u7684\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff0c\u4ec5\u75284\u5757RTX 3090\u8bad\u7ec31.5B\u53c2\u6570\u6a21\u578b\u5c31\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u6709\u9650\u7684rollout\u9884\u7b97\u5bfc\u81f4\u7a00\u758f\u63a2\u7d22\u3001\u7a00\u758f\u4fe1\u7528\u5206\u914d\u548c\u4e0d\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5f62\u6210\u4f4e\u6210\u672c\u4f4e\u51c6\u786e\u7387\u7684\u56f0\u5883\u3002", "method": "\u63d0\u51faDAVID-GRPO\u6846\u67b6\uff1a1) \u7528\u6700\u5c0f\u76d1\u7763\u7a33\u5b9a\u65e9\u671f\u5b66\u4e60\uff1b2) \u57fa\u4e8e\u8bc1\u636e\u53ec\u56de\u5206\u914d\u68c0\u7d22\u4fe1\u7528\uff1b3) \u901a\u8fc7\u91cd\u91c7\u6837\u622a\u65ad\u7684\u8fd1\u5931\u8d25\u8f68\u8ff9\u6539\u8fdb\u63a2\u7d22\u3002", "result": "\u5728\u4ec5\u75284\u5757RTX 3090 GPU\u8bad\u7ec31.5B\u53c2\u6570\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u516d\u4e2a\u591a\u8df3QA\u57fa\u51c6\u4e0a\u4e00\u81f4\u8d85\u8d8a\u5148\u524d\u4e3a\u5927\u89c4\u6a21\u8bbe\u7f6e\u8bbe\u8ba1\u7684RL\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6b63\u786e\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5c0f\u667a\u80fd\u4f53\u53ef\u4ee5\u5728\u4f4e\u8bad\u7ec3\u6210\u672c\u4e0b\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\uff0c\u6253\u7834\u4e86\u8d44\u6e90\u7ea6\u675f\u4e0b\u5c0f\u6a21\u578b\u6027\u80fd\u53d7\u9650\u7684\u56f0\u5883\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22136", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22136", "abs": "https://arxiv.org/abs/2601.22136", "authors": ["Gloria Felicia", "Michael Eniolade", "Jinfeng He", "Zitha Sasindran", "Hemant Kumar", "Milan Hussain Angati", "Sandeep Bandarupalli"], "title": "StepShield: When, Not Whether to Intervene on Rogue Agents", "comment": "16 pages, 2 figures, 14 tables", "summary": "Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.", "AI": {"tldr": "StepShield\u662f\u9996\u4e2a\u8bc4\u4f30\u4ee3\u7801\u4ee3\u7406\u5b89\u5168\u8fdd\u89c4\u68c0\u6d4b\u65f6\u673a\u7684\u57fa\u51c6\uff0c\u63d0\u51fa\u4e09\u79cd\u65f6\u95f4\u6307\u6807\uff0c\u63ed\u793a\u4f20\u7edf\u51c6\u786e\u7387\u6307\u6807\u65e0\u6cd5\u53cd\u6620\u7684\u68c0\u6d4b\u65f6\u673a\u5dee\u5f02\uff0c\u5e76\u5c55\u793a\u65e9\u671f\u68c0\u6d4b\u7684\u7ecf\u6d4e\u6548\u76ca\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u57fa\u51c6\u53ea\u62a5\u544a\u4e8c\u5143\u51c6\u786e\u7387\uff0c\u6df7\u6dc6\u4e86\u65e9\u671f\u5e72\u9884\u548c\u4e8b\u540e\u5206\u6790\u3002\u80fd\u591f\u5728\u7b2c8\u6b65\u6807\u8bb0\u8fdd\u89c4\u7684\u68c0\u6d4b\u5668\u53ef\u5b9e\u73b0\u5e72\u9884\uff0c\u800c\u7b2c48\u6b65\u62a5\u544a\u7684\u68c0\u6d4b\u5668\u53ea\u6709\u6cd5\u533b\u4ef7\u503c\u3002\u8fd9\u79cd\u5173\u952e\u533a\u522b\u5f53\u524d\u57fa\u51c6\u65e0\u6cd5\u8861\u91cf\u3002", "method": "\u5f15\u5165StepShield\u57fa\u51c6\uff0c\u5305\u542b9,213\u4e2a\u4ee3\u7801\u4ee3\u7406\u8f68\u8ff9\uff081,278\u4e2a\u6807\u6ce8\u8bad\u7ec3\u5bf9\u548c7,935\u4e2a\u8f68\u8ff9\u6d4b\u8bd5\u96c6\uff0c8.1%\u6076\u610f\u7387\uff09\u3002\u6076\u610f\u884c\u4e3a\u57fa\u4e8e\u516d\u7c7b\u771f\u5b9e\u5b89\u5168\u4e8b\u4ef6\u3002\u63d0\u51fa\u4e09\u79cd\u65f6\u95f4\u6307\u6807\uff1a\u65e9\u671f\u5e72\u9884\u7387\u3001\u5e72\u9884\u95f4\u9694\u548c\u8282\u7701\u7684token\u6570\u3002", "result": "\u8bc4\u4f30\u663e\u793aLLM\u6cd5\u5b98\u8fbe\u523059%\u65e9\u671f\u5e72\u9884\u7387\uff0c\u800c\u9759\u6001\u5206\u6790\u5668\u53ea\u670926%\uff0c2.3\u500d\u6027\u80fd\u5dee\u8ddd\u5728\u6807\u51c6\u51c6\u786e\u7387\u6307\u6807\u4e2d\u5b8c\u5168\u4e0d\u53ef\u89c1\u3002\u7ea7\u8054\u6df7\u5408\u68c0\u6d4b\u5668\u5c06\u76d1\u63a7\u6210\u672c\u964d\u4f4e75%\uff0c\u9884\u8ba1\u4e94\u5e74\u5185\u4e3a\u4f01\u4e1a\u8282\u77011.08\u4ebf\u7f8e\u5143\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8bc4\u4f30\u91cd\u70b9\u4ece\"\u662f\u5426\"\u68c0\u6d4b\u5230\u8f6c\u5411\"\u4f55\u65f6\"\u68c0\u6d4b\u5230\uff0cStepShield\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u7ecf\u6d4e\u53ef\u884c\u7684AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.21700", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.21700", "abs": "https://arxiv.org/abs/2601.21700", "authors": ["Wonduk Seo", "Wonseok Choi", "Junseo Koh", "Juhyeon Lee", "Hyunjin An", "Minhyeong Yu", "Jian Park", "Qingshan Zhou", "Seunghyun Lee", "Yi Bu"], "title": "Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning", "comment": "35 pages", "summary": "Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values as independent, unstructured signals, reducing consistency and interpretability. We propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. OG-MAR summarizes respondent-specific values from the World Values Survey (WVS) and constructs a global cultural ontology by eliciting relations over a fixed taxonomy via competency questions. At inference time, it retrieves ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents, whose outputs are synthesized by a judgment agent that enforces ontology consistency and demographic proximity. Experiments on regional social-survey benchmarks across four LLM backbones show that OG-MAR improves cultural alignment and robustness over competitive baselines, while producing more transparent reasoning traces.", "AI": {"tldr": "OG-MAR\u662f\u4e00\u4e2a\u672c\u4f53\u6307\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5168\u7403\u6587\u5316\u672c\u4f53\u8bba\u548c\u5b9e\u4f8b\u5316\u4ef7\u503c\u89d2\u8272\u667a\u80fd\u4f53\uff0c\u63d0\u5347LLM\u5728\u6587\u5316\u654f\u611f\u51b3\u7b56\u4e2d\u7684\u5bf9\u9f50\u6027\u3001\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524dLLM\u5728\u6587\u5316\u654f\u611f\u51b3\u7b56\u4e2d\u5b58\u5728\u504f\u5dee\uff0c\u539f\u56e0\u5305\u62ec\u9884\u8bad\u7ec3\u6570\u636e\u503e\u659c\u548c\u7f3a\u4e4f\u7ed3\u6784\u5316\u4ef7\u503c\u8868\u793a\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4eba\u53e3\u7edf\u8ba1\u5b66\u57fa\u7840\uff0c\u5c06\u4ef7\u503c\u89c6\u4e3a\u72ec\u7acb\u3001\u975e\u7ed3\u6784\u5316\u4fe1\u53f7\uff0c\u5bfc\u81f4\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u3002", "method": "OG-MAR\u4ece\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\u4e2d\u603b\u7ed3\u53d7\u8bbf\u8005\u7279\u5b9a\u4ef7\u503c\uff0c\u901a\u8fc7\u80fd\u529b\u95ee\u9898\u5728\u56fa\u5b9a\u5206\u7c7b\u6cd5\u4e0a\u6784\u5efa\u5168\u7403\u6587\u5316\u672c\u4f53\u8bba\u3002\u63a8\u7406\u65f6\u68c0\u7d22\u672c\u4f53\u4e00\u81f4\u5173\u7cfb\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u76f8\u4f3c\u6863\u6848\uff0c\u5b9e\u4f8b\u5316\u591a\u4e2a\u4ef7\u503c\u89d2\u8272\u667a\u80fd\u4f53\uff0c\u7531\u5224\u65ad\u667a\u80fd\u4f53\u7efc\u5408\u8f93\u51fa\u5e76\u5f3a\u5236\u672c\u4f53\u4e00\u81f4\u6027\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u63a5\u8fd1\u6027\u3002", "result": "\u5728\u56db\u4e2aLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u533a\u57df\u793e\u4f1a\u8c03\u67e5\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0cOG-MAR\u76f8\u6bd4\u7ade\u4e89\u57fa\u7ebf\u63d0\u9ad8\u4e86\u6587\u5316\u5bf9\u9f50\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4ea7\u751f\u66f4\u900f\u660e\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "OG-MAR\u901a\u8fc7\u672c\u4f53\u6307\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u6587\u5316\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u4e00\u81f4\u3001\u53ef\u89e3\u91ca\u7684\u6587\u5316\u654f\u611f\u51b3\u7b56\u652f\u6301\u3002", "topic": "agent analysis"}}
{"id": "2601.21372", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21372", "abs": "https://arxiv.org/abs/2601.21372", "authors": ["Yang Song", "Anoushka Vyas", "Zirui Wei", "Sina Khoshfetrat Pakazad", "Henrik Ohlsson", "Graham Neubig"], "title": "NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents", "comment": null, "summary": "In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.\n  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.\n  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.", "AI": {"tldr": "NEMO\u662f\u4e00\u4e2a\u5c06\u81ea\u7136\u8bed\u8a00\u51b3\u7b56\u95ee\u9898\u63cf\u8ff0\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u6570\u5b66\u4f18\u5316\u5b9e\u73b0\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fdc\u7a0b\u4ea4\u4e92\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u786e\u4fdd\u4ee3\u7801\u53ef\u6267\u884c\u6027\uff0c\u5728\u591a\u4e2a\u4f18\u5316\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6216\u5b9a\u5236\u4ee3\u7406\uff0c\u901a\u5e38\u8106\u5f31\u3001\u590d\u6742\u4e14\u7ecf\u5e38\u751f\u6210\u8bed\u6cd5\u65e0\u6548\u6216\u4e0d\u53ef\u6267\u884c\u7684\u4ee3\u7801\uff0c\u9700\u8981\u66f4\u53ef\u9760\u3001\u53ef\u6267\u884c\u7684\u81ea\u52a8\u5316\u4f18\u5316\u5efa\u6a21\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u4f5c\u4e3a\u4e00\u7ea7\u62bd\u8c61\uff0c\u5728\u6c99\u76d2\u73af\u5883\u4e2d\u6267\u884c\u4ee5\u786e\u4fdd\u4ee3\u7801\u53ef\u6267\u884c\u6027\uff1b\u5f15\u5165\u65b0\u9896\u7684\u534f\u8c03\u6a21\u5f0f\uff0c\u5305\u62ec\u975e\u5bf9\u79f0\u9a8c\u8bc1\u5faa\u73af\u3001\u5916\u90e8\u8bb0\u5fc6\u7ecf\u9a8c\u91cd\u7528\u3001\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\u89e3\u7801\u548c\u81ea\u4e00\u81f4\u6027\u7b49\u9c81\u68d2\u6027\u589e\u5f3a\u6280\u672f\u3002", "result": "\u5728\u4e5d\u4e2a\u5df2\u5efa\u7acb\u7684\u4f18\u5316\u57fa\u51c6\u4e0a\u8bc4\u4f30\uff0c\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "NEMO\u5c55\u793a\u4e86\u6267\u884c\u611f\u77e5\u7684\u4ee3\u7406\u67b6\u6784\u5728\u81ea\u52a8\u5316\u4f18\u5316\u5efa\u6a21\u4e2d\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u901a\u8fc7\u8fdc\u7a0b\u4ea4\u4e92\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u786e\u4fdd\u4ee3\u7801\u53ef\u6267\u884c\u6027\uff0c\u4e3a\u590d\u6742\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.21711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21711", "abs": "https://arxiv.org/abs/2601.21711", "authors": ["Huiyuan Lai", "Malvina Nissim"], "title": "TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model's proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.", "AI": {"tldr": "TACLer\u662f\u4e00\u4e2a\u6a21\u578b\u5b9a\u5236\u7684\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u589e\u52a0\u6570\u636e\u590d\u6742\u5ea6\u548c\u6df7\u5408\u601d\u8003/\u975e\u601d\u8003\u63a8\u7406\u8303\u5f0f\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6765\u6fc0\u53d1\u957f\u94fe\u601d\u7ef4\u63a8\u7406\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u601d\u8003\uff08\u5197\u4f59\u4e2d\u95f4\u6b65\u9aa4\uff09\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u5b66\u4e60\u548c\u63a8\u7406\u6548\u7387\u3002", "method": "\u63d0\u51faTACLer\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u5b9a\u5236\u5316\u8bfe\u7a0b\u5b66\u4e60\uff1a\u6839\u636e\u6a21\u578b\u5728\u591a\u9636\u6bb5RL\u8bad\u7ec3\u4e2d\u7684\u719f\u7ec3\u7a0b\u5ea6\uff0c\u6e10\u8fdb\u5f0f\u589e\u52a0\u6570\u636e\u590d\u6742\u5ea6\uff1b2\uff09\u6df7\u5408\u601d\u8003/\u975e\u601d\u8003\u63a8\u7406\u8303\u5f0f\uff1a\u901a\u8fc7\u542f\u7528\u6216\u7981\u7528\u601d\u8003\u6a21\u5f0f\u6765\u5e73\u8861\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "result": "TACLer\u5728\u5b66\u4e60\u548c\u63a8\u7406\u65b9\u9762\u5e26\u6765\u53cc\u91cd\u4f18\u52bf\uff1a1\uff09\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff1a\u76f8\u6bd4\u957f\u601d\u8003\u6a21\u578b\u51cf\u5c1150%\u4ee5\u4e0a\u8bad\u7ec3\u8ba1\u7b97\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u51cf\u5c1142%\u4ee5\u4e0a\u63a8\u7406token\u4f7f\u7528\uff1b2\uff09\u63d0\u5347\u51c6\u786e\u6027\uff1a\u5728\u57fa\u7840\u6a21\u578b\u4e0a\u63d0\u53479%\u4ee5\u4e0a\uff0c\u5728\u56db\u4e2a\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u975e\u601d\u8003\u548c\u601d\u8003\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "TACLer\u901a\u8fc7\u5b9a\u5236\u5316\u8bfe\u7a0b\u5b66\u4e60\u548c\u6df7\u5408\u63a8\u7406\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u53cc\u91cd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21375", "abs": "https://arxiv.org/abs/2601.21375", "authors": ["Zheng Li", "Siyao Song", "Jingyuan Ma", "Rui Li", "Ying Zeng", "Minghao Li", "Zhifang Sui"], "title": "TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models", "comment": null, "summary": "Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluation framework that measures LLM teaching capability via student performance improvement after multi-turn instruction. By restricting teacher agents to structured knowledge points and example problems, the framework avoids information leakage and enables reuse of existing benchmarks. We instantiate the framework on Gaokao data across multiple subjects. Experiments reveal substantial variation in teaching effectiveness across models and domains: some models perform well in mathematics, while teaching remains challenging in physics and chemistry. We also find that incorporating example problems does not necessarily improve teaching, as models often shift toward example-specific error correction. Overall, our results highlight teaching ability as a distinct and measurable dimension of LLM behavior.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6559\u5b66\u5927\u7eb2\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cfLLM\u4f5c\u4e3a\u6559\u5b66\u52a9\u624b\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u5b66\u751f\u5728\u591a\u8f6e\u6307\u5bfc\u540e\u7684\u8868\u73b0\u63d0\u5347\u6765\u8bc4\u4f30\uff0c\u5e76\u5728\u9ad8\u8003\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524dLLM\u4f5c\u4e3a\u6559\u5b66\u52a9\u624b\u7684\u6f5c\u529b\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u95ee\u9898\u89e3\u51b3\u6216\u95ee\u9898\u5c42\u9762\u7684\u6307\u5bfc\uff0c\u7f3a\u4e4f\u5bf9\u77e5\u8bc6\u4e2d\u5fc3\u6559\u5b66\u7684\u5145\u5206\u8bc4\u4f30\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\u6765\u8bc4\u4f30LLM\u7684\u6559\u5b66\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6559\u5b66\u5927\u7eb2\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u6559\u5e08\u4ee3\u7406\u9650\u5236\u5728\u7ed3\u6784\u5316\u77e5\u8bc6\u70b9\u548c\u793a\u4f8b\u95ee\u9898\u4e0a\uff0c\u907f\u514d\u4fe1\u606f\u6cc4\u9732\u5e76\u91cd\u7528\u73b0\u6709\u57fa\u51c6\u3002\u5728\u9ad8\u8003\u6570\u636e\u4e0a\u5b9e\u4f8b\u5316\u8be5\u6846\u67b6\uff0c\u6db5\u76d6\u591a\u4e2a\u5b66\u79d1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e0d\u540c\u6a21\u578b\u548c\u5b66\u79d1\u7684\u6559\u5b66\u6548\u679c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1a\u67d0\u4e9b\u6a21\u578b\u5728\u6570\u5b66\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7269\u7406\u548c\u5316\u5b66\u6559\u5b66\u4e2d\u4ecd\u5177\u6311\u6218\u6027\u3002\u540c\u65f6\u53d1\u73b0\u52a0\u5165\u793a\u4f8b\u95ee\u9898\u5e76\u4e0d\u603b\u80fd\u6539\u5584\u6559\u5b66\uff0c\u56e0\u4e3a\u6a21\u578b\u5f80\u5f80\u8f6c\u5411\u793a\u4f8b\u7279\u5b9a\u7684\u9519\u8bef\u7ea0\u6b63\u3002", "conclusion": "\u6559\u5b66\u80fd\u529b\u662fLLM\u884c\u4e3a\u7684\u4e00\u4e2a\u72ec\u7279\u4e14\u53ef\u6d4b\u91cf\u7684\u7ef4\u5ea6\uff0c\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30LLM\u6559\u5b66\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u6559\u5b66\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.21403", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21403", "abs": "https://arxiv.org/abs/2601.21403", "authors": ["Ruyi Qi", "Zhou Liu", "Wentao Zhang"], "title": "DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis", "comment": null, "summary": "In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and \"zombie data\" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the \"divide-and-conquer\" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented \"zombie data\" for insightful, cross-modal analysis.", "AI": {"tldr": "DataCross\u662f\u4e00\u4e2a\u7528\u4e8e\u8de8\u6a21\u6001\u6570\u636e\u5206\u6790\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u534f\u4f5c\u4ee3\u7406\u6846\u67b6\uff0c\u65e8\u5728\u6fc0\u6d3b\u7ed3\u6784\u5316\u6570\u636e\u4e0e\u89c6\u89c9\u6587\u6863\u4e2d\u7684\"\u50f5\u5c38\u6570\u636e\"\uff0c\u5b9e\u73b0\u7edf\u4e00\u5206\u6790\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u79d1\u5b66\u548c\u4f01\u4e1a\u51b3\u7b56\u4e2d\uff0c\u5173\u952e\u4fe1\u606f\u5206\u6563\u5728\u53ef\u76f4\u63a5\u67e5\u8be2\u7684\u7ed3\u6784\u5316\u6570\u636e\u6e90\uff08\u5982SQL\u3001CSV\uff09\u548c\u9501\u5728\u975e\u7ed3\u6784\u5316\u89c6\u89c9\u6587\u6863\uff08\u5982\u626b\u63cf\u62a5\u544a\u3001\u53d1\u7968\u56fe\u50cf\uff09\u4e2d\u7684\"\u50f5\u5c38\u6570\u636e\"\u4e2d\u3002\u73b0\u6709\u6570\u636e\u5206\u6790\u4ee3\u7406\u4e3b\u8981\u5c40\u9650\u4e8e\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e\uff0c\u65e0\u6cd5\u6fc0\u6d3b\u548c\u5173\u8054\u8fd9\u4e9b\u9ad8\u4ef7\u503c\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u4e0e\u5de5\u4e1a\u9700\u6c42\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u63d0\u51faDataCrossBench\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b200\u4e2a\u7aef\u5230\u7aef\u5206\u6790\u4efb\u52a1\uff09\u548cDataCrossAgent\u534f\u4f5c\u4ee3\u7406\u6846\u67b6\u3002\u57fa\u51c6\u901a\u8fc7\u4eba\u673a\u534f\u540c\u53cd\u5411\u5408\u6210\u7ba1\u9053\u6784\u5efa\uff0c\u786e\u4fdd\u771f\u5b9e\u590d\u6742\u6027\u3001\u8de8\u6e90\u4f9d\u8d56\u6027\u548c\u53ef\u9a8c\u8bc1\u7684\u771f\u5b9e\u6027\u3002\u4ee3\u7406\u6846\u67b6\u91c7\u7528\"\u5206\u800c\u6cbb\u4e4b\"\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u542b\u4e13\u95e8\u5316\u5b50\u4ee3\u7406\uff08\u6bcf\u4e2a\u4e13\u6ce8\u4e8e\u7279\u5b9a\u6570\u636e\u6e90\uff09\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\u534f\u8c03\uff1a\u6e90\u5185\u6df1\u5ea6\u63a2\u7d22\u3001\u5173\u952e\u6e90\u8bc6\u522b\u548c\u4e0a\u4e0b\u6587\u4ea4\u53c9\u878d\u5408\u3002\u91c7\u7528\u65b0\u9896\u7684reReAct\u673a\u5236\u5b9e\u73b0\u7a33\u5065\u7684\u4ee3\u7801\u751f\u6210\u548c\u8c03\u8bd5\u4ee5\u8fdb\u884c\u4e8b\u5b9e\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDataCrossAgent\u5728\u4e8b\u5b9e\u6027\u65b9\u9762\u6bd4GPT-4o\u63d0\u9ad8\u4e8629.7%\uff0c\u5728\u9ad8\u96be\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u6709\u6548\u6fc0\u6d3b\u4e86\u5206\u6563\u7684\"\u50f5\u5c38\u6570\u636e\"\u8fdb\u884c\u6df1\u5165\u7684\u8de8\u6a21\u6001\u5206\u6790\u3002", "conclusion": "DataCross\u6210\u529f\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u6570\u636e\u5206\u6790\u7684\u6311\u6218\uff0c\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u548c\u534f\u4f5c\u4ee3\u7406\u6846\u67b6\u6709\u6548\u6fc0\u6d3b\u4e86\u5206\u6563\u5728\u4e0d\u540c\u6a21\u6001\u4e2d\u7684\"\u50f5\u5c38\u6570\u636e\"\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u79d1\u5b66\u548c\u4f01\u4e1a\u51b3\u7b56\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6d1e\u5bdf\u9a71\u52a8\u5206\u6790\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.21439", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21439", "abs": "https://arxiv.org/abs/2601.21439", "authors": ["Jon Chun", "Katherine Elkins"], "title": "The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making", "comment": "22 page, 10 figures", "summary": "While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking \"Paradox of Robustness\": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be \"brittle\" to how a query is formatted, they are remarkably \"stable\" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u6846\u67b6\u6548\u5e94\u4e0a\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u7a33\u5065\u6027\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5bf9\u63d0\u793a\u8bcd\u5fae\u5c0f\u53d8\u5316\u654f\u611f\uff0c\u4f46\u5728\u89c4\u5219\u7ea6\u675f\u7684\u51b3\u7b56\u4e2d\u5374\u51e0\u4e4e\u4e0d\u53d7\u53d9\u4e8b\u64cd\u7eb5\u5f71\u54cd\uff0c\u5f62\u6210\"\u7a33\u5065\u6027\u6096\u8bba\"\u3002", "motivation": "\u867d\u7136\u5df2\u77e5\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u63d0\u793a\u8bcd\u5fae\u5c0f\u6270\u52a8\u654f\u611f\u4e14\u5bb9\u6613\u8fce\u5408\u7528\u6237\u504f\u89c1\uff0c\u4f46\u5b83\u4eec\u5728\u91cd\u8981\u89c4\u5219\u7ea6\u675f\u51b3\u7b56\u4e2d\u7684\u7a33\u5065\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLMs\u5728\u9762\u5bf9\u60c5\u611f\u6846\u67b6\u6548\u5e94\u65f6\u7684\u884c\u4e3a\u8868\u73b0\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684\u53d7\u63a7\u6270\u52a8\u6846\u67b6\uff0c\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u548c\u91d1\u878d\u4e09\u4e2a\u9ad8\u98ce\u9669\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\uff0c\u91cf\u5316LLMs\u5bf9\u53d9\u4e8b\u64cd\u7eb5\u7684\u62b5\u6297\u80fd\u529b\uff0c\u5e76\u4e0e\u4eba\u7c7b\u53d7\u8bd5\u8005\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LLMs\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u884c\u4e3a\u4e0d\u53d8\u6027\uff1a\u5bf9\u60c5\u611f\u6846\u67b6\u6548\u5e94\u7684\u5f71\u54cd\u51e0\u4e4e\u4e3a\u96f6\uff08Cohen's h = 0.003\uff09\uff0c\u800c\u4eba\u7c7b\u5219\u8868\u73b0\u51fa\u663e\u8457\u504f\u89c1\uff08Cohen's h\u5728[0.3, 0.8]\u4e4b\u95f4\uff09\u3002LLMs\u5bf9\u53d9\u4e8b\u64cd\u7eb5\u7684\u62b5\u6297\u80fd\u529b\u6bd4\u4eba\u7c7b\u9ad8110-300\u500d\u3002", "conclusion": "\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u80fd\u591f\u5c06\u903b\u8f91\u89c4\u5219\u9075\u5faa\u4e0e\u8bf4\u670d\u6027\u53d9\u4e8b\u89e3\u8026\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u51b3\u7b56\u7a33\u5b9a\u6027\u6765\u6e90\uff0c\u53ef\u4ee5\u8865\u5145\u751a\u81f3\u53ef\u80fd\u53bb\u504f\u4eba\u7c7b\u5224\u65ad\u3002\u8fd9\u79cd\u7a33\u5065\u6027\u5728\u5236\u5ea6\u73af\u5883\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2601.21468", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21468", "abs": "https://arxiv.org/abs/2601.21468", "authors": ["Yaorui Shi", "Shugui Liu", "Yu Yang", "Wenyu Mao", "Yuxin Chen", "Qi GU", "Hui Su", "Xunliang Cai", "Xiang Wang", "An Zhang"], "title": "MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning", "comment": null, "summary": "Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.", "AI": {"tldr": "MemOCR\uff1a\u4e00\u79cd\u591a\u6a21\u6001\u8bb0\u5fc6\u4ee3\u7406\uff0c\u901a\u8fc7\u89c6\u89c9\u5e03\u5c40\u81ea\u9002\u5e94\u5206\u914d\u8bb0\u5fc6\u7a7a\u95f4\uff0c\u5728\u6709\u9650\u4e0a\u4e0b\u6587\u9884\u7b97\u4e0b\u63d0\u5347\u957f\u89c6\u91ce\u63a8\u7406\u80fd\u529b", "motivation": "\u957f\u89c6\u91ce\u667a\u80fd\u4f53\u63a8\u7406\u9700\u8981\u5c06\u4e0d\u65ad\u589e\u957f\u7684\u4ea4\u4e92\u5386\u53f2\u538b\u7f29\u5230\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e2d\u3002\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u901a\u5e38\u5c06\u5386\u53f2\u5e8f\u5217\u5316\u4e3a\u6587\u672c\uff0c\u5176\u4e2dtoken\u7ea7\u6210\u672c\u5747\u5300\u4e14\u968f\u957f\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u5e38\u5e38\u5c06\u7a00\u7f3a\u7684\u9884\u7b97\u6d6a\u8d39\u5728\u4f4e\u4ef7\u503c\u7ec6\u8282\u4e0a\u3002", "method": "\u5f15\u5165MemOCR\u591a\u6a21\u6001\u8bb0\u5fc6\u4ee3\u7406\uff0c\u901a\u8fc7\u89c6\u89c9\u5e03\u5c40\u5b9e\u73b0\u81ea\u9002\u5e94\u4fe1\u606f\u5bc6\u5ea6\u7684\u8bb0\u5fc6\u7a7a\u95f4\u5206\u914d\u3002\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u7ef4\u62a4\u7ed3\u6784\u5316\u5bcc\u6587\u672c\u8bb0\u5fc6\uff08\u5982\u6807\u9898\u3001\u9ad8\u4eae\uff09\uff1b2\uff09\u5c06\u5176\u6e32\u67d3\u4e3a\u56fe\u50cf\u4f9b\u4ee3\u7406\u67e5\u8be2\uff0c\u89c6\u89c9\u4e0a\u4f18\u5148\u663e\u793a\u5173\u952e\u8bc1\u636e\uff0c\u540c\u65f6\u79ef\u6781\u538b\u7f29\u8f85\u52a9\u7ec6\u8282\uff1b3\uff09\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u9884\u7b97\u611f\u77e5\u76ee\u6807\u4e0b\u8bad\u7ec3\uff0c\u4f7f\u4ee3\u7406\u9002\u5e94\u4e0d\u540c\u538b\u7f29\u7ea7\u522b\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u591a\u8df3\u548c\u5355\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMemOCR\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u4e8e\u6587\u672c\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6781\u7aef\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u5229\u7528\u3002", "conclusion": "MemOCR\u901a\u8fc7\u89c6\u89c9\u5e03\u5c40\u5b9e\u73b0\u81ea\u9002\u5e94\u4fe1\u606f\u5bc6\u5ea6\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u91ce\u63a8\u7406\u5728\u6709\u9650\u4e0a\u4e0b\u6587\u9884\u7b97\u4e0b\u7684\u6548\u679c\uff0c\u4e3a\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2601.21244", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21244", "abs": "https://arxiv.org/abs/2601.21244", "authors": ["Yiju Guo", "Tianyi Hu", "Zexu Sun", "Yankai Lin"], "title": "Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification", "comment": "Work in progress", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.", "AI": {"tldr": "LENS\u6846\u67b6\u901a\u8fc7\u8bc6\u522b\u5e76\u79fb\u9664\u5e72\u6270token\u6765\u63d0\u5347RLVR\u7684\u63a2\u7d22\u6548\u7387\uff0c\u5728\u566a\u58f0\u63d0\u793a\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u548c\u66f4\u9ad8\u7684\u6027\u80fd", "motivation": "\u5f53\u524dRLVR\u65b9\u6cd5\u5728\u6709\u9650rollout\u9884\u7b97\u4e0b\u63a2\u7d22\u6548\u7387\u4f4e\uff0c\u91c7\u6837\u6210\u529f\u7387\u4f4e\u4e14\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002\u7814\u7a76\u53d1\u73b0\u8bb8\u591a\u63a2\u7d22\u5931\u8d25\u5e76\u975e\u6e90\u4e8e\u4efb\u52a1\u96be\u5ea6\uff0c\u800c\u662f\u7531\u5c11\u91cf\u5e72\u6270token\u5f15\u8d77", "method": "\u63d0\u51faLENS\u6846\u67b6\uff1a1\uff09\u8bc6\u522b\u5e76\u79fb\u9664\u63d0\u793a\u4e2d\u7684\u5e72\u6270token\u8fdb\u884c\u51c0\u5316\uff1b2\uff09\u5c06\u51c0\u5316\u540e\u6210\u529f\u7684rollout\u8f6c\u79fb\u5230\u539f\u59cb\u566a\u58f0\u63d0\u793a\u4e0a\u76d1\u7763\u7b56\u7565\u4f18\u5316\uff0c\u4f7f\u6a21\u578b\u5b66\u4f1a\u5ffd\u7565\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5e72\u6270", "result": "LENS\u663e\u8457\u4f18\u4e8eGRPO\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53473.88%\uff0c\u6536\u655b\u901f\u5ea6\u52a0\u5feb1.6\u500d\u4ee5\u4e0a\uff0c\u8bc1\u660e\u4e86\u53bb\u9664\u5e72\u6270token\u5bf9\u63d0\u5347rollout\u6548\u7387\u7684\u5173\u952e\u4f5c\u7528", "conclusion": "\u5e72\u6270token\u7684\u4fee\u526a\u5728\u63d0\u5347RLVR\u63a2\u7d22\u6548\u7387\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4e3aRLVR\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2", "topic": "agentic reinforcement learning"}}
{"id": "2601.21797", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21797", "abs": "https://arxiv.org/abs/2601.21797", "authors": ["Yimin Deng", "Yuqing Fu", "Derong Xu", "Yejing Wang", "Wei Ni", "Jingtong Gao", "Xiaopeng Li", "Chengxu Liu", "Xiao Han", "Guoshuai Zhao", "Xiangyu Zhao", "Li Zhu", "Xueming Qian"], "title": "Enhancing Conversational Agents via Task-Oriented Adversarial Memory Adaptation", "comment": "11 pages, 4 figures", "summary": "Conversational agents struggle to handle long conversations due to context window limitations. Therefore, memory systems are developed to leverage essential historical information. Existing memory systems typically follow a pipeline of offline memory construction and update, and online retrieval. Despite the flexible online phase, the offline phase remains fixed and task-independent. In this phase, memory construction operates under a predefined workflow and fails to emphasize task relevant information. Meanwhile, memory updates are guided by generic metrics rather than task specific supervision. This leads to a misalignment between offline memory preparation and task requirements, which undermines downstream task performance. To this end, we propose an Adversarial Memory Adaptation mechanism (AMA) that aligns memory construction and update with task objectives by simulating task execution. Specifically, first, a challenger agent generates question answer pairs based on the original dialogues. The constructed memory is then used to answer these questions, simulating downstream inference. Subsequently, an evaluator agent assesses the responses and performs error analysis. Finally, an adapter agent analyzes the error cases and performs dual level updates on both the construction strategy and the content. Through this process, the memory system receives task aware supervision signals in advance during the offline phase, enhancing its adaptability to downstream tasks. AMA can be integrated into various existing memory systems, and extensive experiments on long dialogue benchmark LoCoMo demonstrate its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6297\u6027\u8bb0\u5fc6\u9002\u5e94\u673a\u5236\uff08AMA\uff09\uff0c\u901a\u8fc7\u6a21\u62df\u4efb\u52a1\u6267\u884c\u6765\u5bf9\u9f50\u79bb\u7ebf\u8bb0\u5fc6\u6784\u5efa\u4e0e\u4efb\u52a1\u76ee\u6807\uff0c\u89e3\u51b3\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u79bb\u7ebf\u9636\u6bb5\u56fa\u5b9a\u4e14\u4efb\u52a1\u65e0\u5173\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u8bb0\u5fc6\u7cfb\u7edf\u5728\u79bb\u7ebf\u9636\u6bb5\u91c7\u7528\u56fa\u5b9a\u3001\u4efb\u52a1\u65e0\u5173\u7684\u6784\u5efa\u548c\u66f4\u65b0\u7b56\u7565\uff0c\u5bfc\u81f4\u8bb0\u5fc6\u51c6\u5907\u4e0e\u4e0b\u6e38\u4efb\u52a1\u9700\u6c42\u4e0d\u5339\u914d\uff0c\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002", "method": "AMA\u673a\u5236\u5305\u542b\u4e09\u4e2a\u667a\u80fd\u4f53\uff1a\u6311\u6218\u8005\u57fa\u4e8e\u539f\u59cb\u5bf9\u8bdd\u751f\u6210QA\u5bf9\uff0c\u8bc4\u4f30\u8005\u8bc4\u4f30\u8bb0\u5fc6\u56de\u7b54\u7684\u8d28\u91cf\u5e76\u8fdb\u884c\u9519\u8bef\u5206\u6790\uff0c\u9002\u914d\u5668\u6839\u636e\u9519\u8bef\u5206\u6790\u5bf9\u8bb0\u5fc6\u6784\u5efa\u7b56\u7565\u548c\u5185\u5bb9\u8fdb\u884c\u53cc\u91cd\u66f4\u65b0\u3002", "result": "\u5728\u957f\u5bf9\u8bdd\u57fa\u51c6LoCoMo\u4e0a\u7684\u5b9e\u9a8c\u8868\u660eAMA\u80fd\u6709\u6548\u63d0\u5347\u8bb0\u5fc6\u7cfb\u7edf\u6027\u80fd\uff0c\u4e14\u53ef\u96c6\u6210\u5230\u591a\u79cd\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u4e2d\u3002", "conclusion": "AMA\u901a\u8fc7\u5728\u79bb\u7ebf\u9636\u6bb5\u5f15\u5165\u4efb\u52a1\u611f\u77e5\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u589e\u5f3a\u4e86\u8bb0\u5fc6\u7cfb\u7edf\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u9002\u5e94\u6027\uff0c\u89e3\u51b3\u4e86\u8bb0\u5fc6\u51c6\u5907\u4e0e\u4efb\u52a1\u9700\u6c42\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.21473", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.21473", "abs": "https://arxiv.org/abs/2601.21473", "authors": ["Zaifeng Pan", "Yipeng Shen", "Zhengding Hu", "Zhuang Wang", "Aninda Manocha", "Zheng Wang", "Zhongkai Yu", "Yue Guan", "Yufei Ding"], "title": "ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management", "comment": null, "summary": "LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.", "AI": {"tldr": "ScaleSim\uff1a\u57fa\u4e8e\u8c03\u7528\u8ddd\u79bb\u62bd\u8c61\u7684\u591a\u667a\u80fd\u4f53\u6a21\u62dfLLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u53d6\u548c\u4f18\u5148\u7ea7\u9a71\u9010\u5b9e\u73b0\u5185\u5b58\u9ad8\u6548\u6269\u5c55", "motivation": "\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6a21\u62df\u5728\u5e94\u7528\u9886\u57df\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u7531\u4e8eGPU\u5185\u5b58\u538b\u529b\u96be\u4ee5\u6269\u5c55\u3002\u6bcf\u4e2a\u667a\u80fd\u4f53\u7ef4\u62a4\u79c1\u6709GPU\u9a7b\u7559\u72b6\u6001\uff08\u6a21\u578b\u3001\u524d\u7f00\u7f13\u5b58\u3001\u9002\u914d\u5668\uff09\uff0c\u968f\u7740\u667a\u80fd\u4f53\u6570\u91cf\u589e\u957f\u4f1a\u5feb\u901f\u8017\u5c3d\u8bbe\u5907\u5185\u5b58\u3002", "method": "\u63d0\u51fa\u8c03\u7528\u8ddd\u79bb\u7684\u7edf\u4e00\u62bd\u8c61\u6765\u4f30\u8ba1\u667a\u80fd\u4f53\u672a\u6765\u53d1\u51faLLM\u8bf7\u6c42\u7684\u76f8\u5bf9\u987a\u5e8f\uff0c\u57fa\u4e8e\u6b64\u5b9e\u73b0ScaleSim\u7cfb\u7edf\uff0c\u652f\u6301\u4e3b\u52a8\u9884\u53d6\u548c\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u9a71\u9010\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u63a5\u53e3\u652f\u6301\u591a\u6837\u5316\u7684\u667a\u80fd\u4f53\u7279\u5b9a\u5185\u5b58\u3002", "result": "\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cScaleSim\u76f8\u6bd4SGLang\u5b9e\u73b0\u4e86\u6700\u9ad81.74\u500d\u7684\u52a0\u901f\u3002", "conclusion": "ScaleSim\u901a\u8fc7\u8c03\u7528\u8ddd\u79bb\u62bd\u8c61\u548c\u5185\u5b58\u4f18\u5316\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u6a21\u62df\u4e2d\u7684GPU\u5185\u5b58\u6269\u5c55\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.21281", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21281", "abs": "https://arxiv.org/abs/2601.21281", "authors": ["Licheng Wang", "Yuzi Yan", "Mingtao Huang", "Yuan Shen"], "title": "EGAM: Extended Graph Attention Model for Solving Routing Problems", "comment": null, "summary": "Neural combinatorial optimization (NCO) solvers, implemented with graph neural networks (GNNs), have introduced new approaches for solving routing problems. Trained with reinforcement learning (RL), the state-of-the-art graph attention model (GAM) achieves near-optimal solutions without requiring expert knowledge or labeled data. In this work, we generalize the existing graph attention mechanism and propose the extended graph attention model (EGAM). Our model utilizes multi-head dot-product attention to update both node and edge embeddings, addressing the limitations of the conventional GAM, which considers only node features. We employ an autoregressive encoder-decoder architecture and train it with policy gradient algorithms that incorporate a specially designed baseline. Experiments show that EGAM matches or outperforms existing methods across various routing problems. Notably, the proposed model demonstrates exceptional performance on highly constrained problems, highlighting its efficiency in handling complex graph structures.", "AI": {"tldr": "EGAM\u6269\u5c55\u4e86\u56fe\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u591a\u5934\u70b9\u79ef\u6ce8\u610f\u529b\u540c\u65f6\u66f4\u65b0\u8282\u70b9\u548c\u8fb9\u5d4c\u5165\uff0c\u5728\u591a\u79cd\u8def\u7531\u95ee\u9898\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9ad8\u5ea6\u7ea6\u675f\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u56fe\u6ce8\u610f\u529b\u6a21\u578b(GAM)\u4ec5\u8003\u8651\u8282\u70b9\u7279\u5f81\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002\u9700\u8981\u66f4\u901a\u7528\u7684\u56fe\u6ce8\u610f\u529b\u673a\u5236\u6765\u540c\u65f6\u5904\u7406\u8282\u70b9\u548c\u8fb9\u4fe1\u606f\uff0c\u4ee5\u66f4\u597d\u5730\u89e3\u51b3\u590d\u6742\u7684\u8def\u7531\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6269\u5c55\u56fe\u6ce8\u610f\u529b\u6a21\u578b(EGAM)\uff0c\u4f7f\u7528\u591a\u5934\u70b9\u79ef\u6ce8\u610f\u529b\u540c\u65f6\u66f4\u65b0\u8282\u70b9\u548c\u8fb9\u5d4c\u5165\u3002\u91c7\u7528\u81ea\u56de\u5f52\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u4f7f\u7528\u5305\u542b\u4e13\u95e8\u8bbe\u8ba1\u57fa\u7ebf\u7684\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "EGAM\u5728\u591a\u79cd\u8def\u7531\u95ee\u9898\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5ea6\u7ea6\u675f\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5904\u7406\u590d\u6742\u56fe\u7ed3\u6784\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "EGAM\u901a\u8fc7\u6269\u5c55\u56fe\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfGAM\u7684\u5c40\u9650\u6027\uff0c\u5728\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21841", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21841", "abs": "https://arxiv.org/abs/2601.21841", "authors": ["Xiang Li", "Ning Yan", "Masood Mortazavi"], "title": "Embodied Task Planning via Graph-Informed Action Generation with Large Lanaguage Model", "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated strong zero-shot reasoning capabilities, their deployment as embodied agents still faces fundamental challenges in long-horizon planning. Unlike open-ended text generation, embodied agents must decompose high-level intent into actionable sub-goals while strictly adhering to the logic of a dynamic, observed environment. Standard LLM planners frequently fail to maintain strategy coherence over extended horizons due to context window limitation or hallucinate transitions that violate constraints. We propose GiG, a novel planning framework that structures embodied agents' memory using a Graph-in-Graph architecture. Our approach employs a Graph Neural Network (GNN) to encode environmental states into embeddings, organizing these embeddings into action-connected execution trace graphs within an experience memory bank. By clustering these graph embeddings, the framework enables retrieval of structure-aware priors, allowing agents to ground current decisions in relevant past structural patterns. Furthermore, we introduce a novel bounded lookahead module that leverages symbolic transition logic to enhance the agents' planning capabilities through the grounded action projection. We evaluate our framework on three embodied planning benchmarks-Robotouille Synchronous, Robotouille Asynchronous, and ALFWorld. Our method outperforms state-of-the-art baselines, achieving Pass@1 performance gains of up to 22% on Robotouille Synchronous, 37% on Asynchronous, and 15% on ALFWorld with comparable or lower computational cost.", "AI": {"tldr": "GiG\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5185\u56fe\u67b6\u6784\u7684\u5177\u8eab\u667a\u80fd\u4f53\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u73af\u5883\u72b6\u6001\uff0c\u6784\u5efa\u6267\u884c\u8f68\u8ff9\u56fe\u8bb0\u5fc6\u5e93\uff0c\u7ed3\u5408\u7ed3\u6784\u611f\u77e5\u68c0\u7d22\u548c\u6709\u754c\u524d\u77bb\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u957f\u65f6\u7a0b\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f5c\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u90e8\u7f72\u65f6\u4ecd\u9762\u4e34\u957f\u65f6\u7a0b\u89c4\u5212\u7684\u57fa\u672c\u6311\u6218\u3002\u6807\u51c6LLM\u89c4\u5212\u5668\u7ecf\u5e38\u56e0\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u800c\u65e0\u6cd5\u4fdd\u6301\u7b56\u7565\u8fde\u8d2f\u6027\uff0c\u6216\u4ea7\u751f\u8fdd\u53cd\u7ea6\u675f\u7684\u5e7b\u89c9\u8f6c\u6362\u3002", "method": "\u63d0\u51faGiG\u6846\u67b6\uff1a1) \u4f7f\u7528GNN\u5c06\u73af\u5883\u72b6\u6001\u7f16\u7801\u4e3a\u5d4c\u5165\uff1b2) \u5c06\u8fd9\u4e9b\u5d4c\u5165\u7ec4\u7ec7\u6210\u52a8\u4f5c\u8fde\u63a5\u7684\u6267\u884c\u8f68\u8ff9\u56fe\uff0c\u5b58\u50a8\u5728\u7ecf\u9a8c\u8bb0\u5fc6\u5e93\u4e2d\uff1b3) \u901a\u8fc7\u805a\u7c7b\u56fe\u5d4c\u5165\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u5148\u9a8c\u68c0\u7d22\uff1b4) \u5f15\u5165\u6709\u754c\u524d\u77bb\u6a21\u5757\uff0c\u5229\u7528\u7b26\u53f7\u8f6c\u6362\u903b\u8f91\u589e\u5f3a\u89c4\u5212\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u8eab\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\uff1aRobotouille Synchronous\u63d0\u534722%\uff0cRobotouille Asynchronous\u63d0\u534737%\uff0cALFWorld\u63d0\u534715%\uff0c\u8ba1\u7b97\u6210\u672c\u76f8\u5f53\u6216\u66f4\u4f4e\u3002", "conclusion": "GiF\u6846\u67b6\u901a\u8fc7\u56fe\u5185\u56fe\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u5177\u8eab\u667a\u80fd\u4f53\u7684\u957f\u65f6\u7a0b\u89c4\u5212\u95ee\u9898\uff0c\u7ed3\u5408\u7ed3\u6784\u611f\u77e5\u8bb0\u5fc6\u68c0\u7d22\u548c\u6709\u754c\u524d\u77bb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\u548c\u7b56\u7565\u8fde\u8d2f\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.21533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21533", "abs": "https://arxiv.org/abs/2601.21533", "authors": ["Youngjin Jin", "Hanna Kim", "Kwanwoo Kim", "Chanhee Lee", "Seungwon Shin"], "title": "ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making", "comment": "58 pages", "summary": "Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.", "AI": {"tldr": "ARGORA\u6846\u67b6\u5c06\u591a\u4e13\u5bb6LLM\u8ba8\u8bba\u7ec4\u7ec7\u4e3a\u663e\u5f0f\u8bba\u8bc1\u56fe\uff0c\u901a\u8fc7\u56e0\u679c\u5efa\u6a21\u8bc6\u522b\u5173\u952e\u63a8\u7406\u94fe\uff0c\u5e76\u5f15\u5165\u6821\u6b63\u673a\u5236\u5bf9\u9f50\u5185\u90e8\u63a8\u7406\u4e0e\u5916\u90e8\u5224\u65ad\u3002", "motivation": "\u73b0\u6709\u591a\u4e13\u5bb6LLM\u7cfb\u7edf\u901a\u8fc7\u7b80\u5355\u805a\u5408\u6574\u5408\u4e0d\u540c\u89c2\u70b9\uff0c\u4f46\u63a9\u76d6\u4e86\u54ea\u4e9b\u8bba\u8bc1\u9a71\u52a8\u4e86\u6700\u7ec8\u51b3\u7b56\uff0c\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06\u591a\u4e13\u5bb6\u8ba8\u8bba\u7ec4\u7ec7\u4e3a\u663e\u5f0f\u8bba\u8bc1\u56fe\uff0c\u5c55\u793a\u8bba\u8bc1\u95f4\u7684\u652f\u6301\u4e0e\u653b\u51fb\u5173\u7cfb\uff1b\u5c06\u8bba\u8bc1\u56fe\u5efa\u6a21\u4e3a\u56e0\u679c\u6a21\u578b\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u79fb\u9664\u5355\u4e2a\u8bba\u8bc1\u5e76\u91cd\u65b0\u8ba1\u7b97\u7ed3\u679c\u6765\u8bc6\u522b\u5173\u952e\u63a8\u7406\u94fe\uff1b\u5f15\u5165\u6821\u6b63\u673a\u5236\uff0c\u5f53\u5185\u90e8\u63a8\u7406\u4e0e\u5916\u90e8\u5224\u65ad\u4e0d\u4e00\u81f4\u65f6\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u548c\u5f00\u653e\u5f0f\u7528\u4f8b\u4e2d\uff0cARGORA\u8fbe\u5230\u7ade\u4e89\u6027\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u73b0\u6821\u6b63\u884c\u4e3a\uff1a\u5f53\u4e13\u5bb6\u6700\u521d\u5b58\u5728\u5206\u6b67\u65f6\uff0c\u6846\u67b6\u66f4\u503e\u5411\u4e8e\u5c06\u4e89\u8bae\u89e3\u51b3\u4e3a\u6b63\u786e\u7b54\u6848\u800c\u975e\u5f15\u5165\u65b0\u9519\u8bef\uff0c\u540c\u65f6\u63d0\u4f9b\u5173\u952e\u8bba\u8bc1\u7684\u56e0\u679c\u8bca\u65ad\u3002", "conclusion": "ARGORA\u901a\u8fc7\u663e\u5f0f\u8bba\u8bc1\u56fe\u548c\u56e0\u679c\u5206\u6790\u63d0\u9ad8\u4e86\u591a\u4e13\u5bb6LLM\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u591f\u8bc6\u522b\u5173\u952e\u63a8\u7406\u94fe\u5e76\u8fdb\u884c\u81ea\u6211\u6821\u6b63\uff0c\u4e3a\u590d\u6742\u51b3\u7b56\u63d0\u4f9b\u66f4\u597d\u7684\u8bca\u65ad\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.21545", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21545", "abs": "https://arxiv.org/abs/2601.21545", "authors": ["Yang Zhao", "Chengxiao Dai", "Yue Xiu", "Mengying Kou", "Yuliang Zheng", "Dusit Niyato"], "title": "ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory", "comment": null, "summary": "Agentic large language model (LLM) systems rely on external memory for long-horizon state and concurrent multi-agent execution, but centralized indexes and heuristic partitions become bottlenecks as memory volume and parallel access grow. We present ShardMemo, a budgeted tiered memory service with Tier A per-agent working state, Tier B sharded evidence with shard-local approximate nearest neighbor (ANN) indexes, and Tier C, a versioned skill library. Tier B enforces scope-before-routing: structured eligibility constraints mask ineligible shards before routing or ANN search. We cast shard probing as masked mixture-of-experts (MoE) routing over eligible shards, probing up to $B_{\\mathrm{probe}}$ shards via Top-$B_{\\mathrm{probe}}$ or adaptive Top-$P$, and use cost-aware gating over profile/observation/session shard families; the router is trained from evidence-to-shard supervision. On LoCoMo, ShardMemo improves over the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting ($B_{\\mathrm{probe}}=3$), ShardMemo improves over cosine-to-prototype shard routing by +6.87 F1 while reducing retrieval work (VecScan 521->414, -20.5%) and p95 latency (95->76 ms). On long-context HotpotQA, ShardMemo achieves 63.41/61.88/57.95 F1 at 56K/224K/448K tokens. On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval).", "AI": {"tldr": "ShardMemo\u662f\u4e00\u4e2a\u4e3a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u8bbe\u8ba1\u7684\u9884\u7b97\u5206\u5c42\u5185\u5b58\u670d\u52a1\uff0c\u901a\u8fc7\u4e09\u5c42\u5185\u5b58\u67b6\u6784\u548c\u8303\u56f4\u4f18\u5148\u8def\u7531\u673a\u5236\uff0c\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u68c0\u7d22\u5f00\u9500\u3002", "motivation": "\u968f\u7740\u5185\u5b58\u5bb9\u91cf\u548c\u5e76\u884c\u8bbf\u95ee\u589e\u957f\uff0c\u96c6\u4e2d\u5f0f\u7d22\u5f15\u548c\u542f\u53d1\u5f0f\u5206\u533a\u6210\u4e3a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u7684\u74f6\u9888\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5185\u5b58\u7ba1\u7406\u65b9\u6848\u6765\u652f\u6301\u957f\u65f6\u72b6\u6001\u548c\u5e76\u53d1\u591a\u667a\u80fd\u4f53\u6267\u884c\u3002", "method": "\u63d0\u51fa\u4e09\u5c42\u5185\u5b58\u67b6\u6784\uff1aTier A\uff08\u667a\u80fd\u4f53\u5de5\u4f5c\u72b6\u6001\uff09\u3001Tier B\uff08\u5206\u7247\u8bc1\u636e\u5e93\uff0c\u5e26\u5206\u7247\u672c\u5730ANN\u7d22\u5f15\uff09\u3001Tier C\uff08\u7248\u672c\u5316\u6280\u80fd\u5e93\uff09\u3002Tier B\u91c7\u7528\u8303\u56f4\u4f18\u5148\u8def\u7531\u673a\u5236\uff0c\u5c06\u5206\u7247\u63a2\u6d4b\u5efa\u6a21\u4e3a\u63a9\u7801\u6df7\u5408\u4e13\u5bb6\u8def\u7531\uff0c\u4f7f\u7528\u6210\u672c\u611f\u77e5\u95e8\u63a7\u548c\u8bad\u7ec3\u7684\u8def\u7531\u5668\u3002", "result": "\u5728LoCoMo\u4e0a\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u53475.11-6.82 F1\uff1b\u56fa\u5b9a\u9884\u7b97\u8def\u7531\u4e0b\u6bd4\u4f59\u5f26\u539f\u578b\u8def\u7531\u63d0\u53476.87 F1\uff0c\u540c\u65f6\u51cf\u5c1120.5%\u68c0\u7d22\u5de5\u4f5c\u548c\u964d\u4f4ep95\u5ef6\u8fdf\uff1b\u5728\u957f\u4e0a\u4e0b\u6587HotpotQA\u4e0a\u8fbe\u523057.95-63.41 F1\uff1b\u5728ToolBench\u4e0aTier C\u8fbe\u52300.97 Precision@3\u548c1.94 StepRed\u3002", "conclusion": "ShardMemo\u901a\u8fc7\u5206\u5c42\u5185\u5b58\u67b6\u6784\u548c\u667a\u80fd\u8def\u7531\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u7684\u5185\u5b58\u7ba1\u7406\u74f6\u9888\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u5f00\u9500\u3002", "topic": "agent analysis"}}
{"id": "2601.21557", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.21557", "abs": "https://arxiv.org/abs/2601.21557", "authors": ["Haoran Ye", "Xuning He", "Vincent Arak", "Haonan Dong", "Guojie Song"], "title": "Meta Context Engineering via Agentic Skill Evolution", "comment": "46 pages, 4 figures", "summary": "The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.", "AI": {"tldr": "\u63d0\u51faMeta Context Engineering (MCE)\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u7ea7\u4ee3\u7406\u548c\u57fa\u7840\u7ea7\u4ee3\u7406\u7684\u534f\u540c\u8fdb\u5316\uff0c\u8d85\u8d8a\u4f20\u7edf\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u9759\u6001\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u6280\u80fd\u548c\u4e0a\u4e0b\u6587\u5de5\u4ef6\u7684\u5171\u540c\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u4e0a\u4e0b\u6587\u5de5\u7a0b\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u56fa\u5b9a\u5de5\u4f5c\u6d41\u7a0b\u548c\u9884\u5b9a\u4e49\u4e0a\u4e0b\u6587\u6a21\u5f0f\uff0c\u5b58\u5728\u7ed3\u6784\u504f\u89c1\uff0c\u9650\u5236\u4e86\u4e0a\u4e0b\u6587\u4f18\u5316\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002\u9700\u8981\u66f4\u7075\u6d3b\u3001\u81ea\u9002\u5e94\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u53cc\u5c42MCE\u6846\u67b6\uff1a\u5143\u7ea7\u4ee3\u7406\u901a\u8fc7\"\u4ee3\u7406\u4ea4\u53c9\"\u5728\u6280\u80fd\u5386\u53f2\u3001\u6267\u884c\u548c\u8bc4\u4f30\u4e2d\u8fdb\u884c\u5ba1\u614e\u641c\u7d22\u6765\u4f18\u5316\u5de5\u7a0b\u6280\u80fd\uff1b\u57fa\u7840\u7ea7\u4ee3\u7406\u6267\u884c\u8fd9\u4e9b\u6280\u80fd\uff0c\u4ece\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b66\u4e60\uff0c\u5e76\u5c06\u4e0a\u4e0b\u6587\u4f18\u5316\u4e3a\u7075\u6d3b\u7684\u6587\u4ef6\u548c\u4ee3\u7801\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u9886\u57df\u548c\u79bb\u7ebf/\u5728\u7ebf\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\uff0cMCE\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u4e0a\u4e0b\u6587\u5de5\u7a0b\u65b9\u6cd5\u83b7\u5f975.6-53.8%\u7684\u76f8\u5bf9\u6539\u8fdb\uff08\u5e73\u574716.9%\uff09\uff0c\u540c\u65f6\u5728\u4e0a\u4e0b\u6587\u9002\u5e94\u6027\u3001\u53ef\u8fc1\u79fb\u6027\u3001\u4f7f\u7528\u6548\u7387\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "MCE\u6846\u67b6\u80fd\u591f\u8d85\u8d8a\u9759\u6001\u4e0a\u4e0b\u6587\u5de5\u7a0b\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u6280\u80fd\u548c\u4e0a\u4e0b\u6587\u5de5\u4ef6\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u4f18\u5316\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u65f6\u4e0a\u4e0b\u6587\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2601.21570", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.21570", "abs": "https://arxiv.org/abs/2601.21570", "authors": ["Zixing Lei", "Genjia Liu", "Yuanshuo Zhang", "Qipeng Liu", "Chuan Wen", "Shanghang Zhang", "Wenzhao Lian", "Siheng Chen"], "title": "EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots", "comment": "37 pages, 13 figures", "summary": "The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \\textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.", "AI": {"tldr": "LLM\u4ee3\u7406\u5728\u5177\u8eabAI\u4e2d\u81ea\u4e3b\u5de5\u7a0b\u5316\u7b56\u7565\u7684\u57fa\u51c6\u8bc4\u4f30\uff0c\u901a\u8fc7\u4ee3\u7801\u4f5c\u4e3a\u901a\u7528\u63a5\u53e3\uff0c\u572832\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u52a8\u6001\u95ed\u73af\u5de5\u4f5c\u6d41\uff0c\u6027\u80fd\u8d85\u8d8a\u4eba\u5de5\u57fa\u7ebf26.5%", "motivation": "\u5177\u8eabAI\u9886\u57df\u5411\u901a\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u5feb\u901f\u6f14\u8fdb\uff0c\u4f46\u6269\u5c55\u80fd\u529b\u53d7\u9650\u4e8e\u4f9d\u8d56\u4eba\u5de5\u76d1\u7763\uff08\u4ece\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u5230\u5f02\u6784\u540e\u7aef\u8d85\u53c2\u6570\u8c03\u4f18\uff09\u3002\u53d7LLM\u5728\u8f6f\u4ef6\u81ea\u52a8\u5316\u548c\u79d1\u5b66\u53d1\u73b0\u6210\u529f\u7684\u542f\u53d1\uff0c\u7814\u7a76LLM\u4ee3\u7406\u81ea\u4e3b\u5de5\u7a0b\u5316\u5177\u8eab\u7b56\u7565\u7684\u80fd\u529b", "method": "\u63d0\u51faEmboCoach-Bench\u57fa\u51c6\uff0c\u5305\u542b32\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684RL\u548cIL\u4efb\u52a1\uff0c\u4ee5\u53ef\u6267\u884c\u4ee3\u7801\u4e3a\u901a\u7528\u63a5\u53e3\u3002\u8bc4\u4f30\u52a8\u6001\u95ed\u73af\u5de5\u4f5c\u6d41\uff0c\u4ee3\u7406\u5229\u7528\u73af\u5883\u53cd\u9988\u8fed\u4ee3\u8d77\u8349\u3001\u8c03\u8bd5\u548c\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6db5\u76d6\u4ece\u7269\u7406\u611f\u77e5\u5956\u52b1\u8bbe\u8ba1\u5230\u6269\u6563\u7b56\u7565\u7b49\u67b6\u6784\u6539\u8fdb", "result": "1) \u81ea\u4e3b\u4ee3\u7406\u5e73\u5747\u6210\u529f\u7387\u6bd4\u4eba\u5de5\u5de5\u7a0b\u57fa\u7ebf\u9ad8\u51fa26.5%\uff1b2) \u5e26\u73af\u5883\u53cd\u9988\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u6709\u6548\u5f3a\u5316\u7b56\u7565\u5f00\u53d1\uff0c\u663e\u8457\u7f29\u5c0f\u5f00\u6e90\u4e0e\u4e13\u6709\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\uff1b3) \u4ee3\u7406\u5c55\u73b0\u81ea\u6821\u6b63\u80fd\u529b\uff0c\u901a\u8fc7\u8fed\u4ee3\u4eff\u771f\u8c03\u8bd5\u4ece\u8fd1\u4e4e\u5b8c\u5168\u5931\u8d25\u4e2d\u6062\u590d\u4efb\u52a1\u6027\u80fd", "conclusion": "\u4e3a\u81ea\u6f14\u5316\u5177\u8eab\u667a\u80fd\u5960\u5b9a\u57fa\u7840\uff0c\u52a0\u901f\u4ece\u52b3\u52a8\u5bc6\u96c6\u578b\u624b\u52a8\u8c03\u4f18\u5411\u53ef\u6269\u5c55\u81ea\u4e3b\u5de5\u7a0b\u7684\u8303\u5f0f\u8f6c\u53d8", "topic": "agent analysis"}}
{"id": "2601.21306", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21306", "abs": "https://arxiv.org/abs/2601.21306", "authors": ["Wei-Di Chang", "Mikael Henaff", "Brandon Amos", "Gregory Dudek", "Scott Fujimoto"], "title": "The Surprising Difficulty of Search in Model-Based Reinforcement Learning", "comment": null, "summary": "This paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u641c\u7d22\u7684\u5e38\u89c4\u8ba4\u77e5\uff0c\u53d1\u73b0\u641c\u7d22\u5e76\u975e\u5b66\u4e60\u7684\u76f4\u63a5\u66ff\u4ee3\uff0c\u751a\u81f3\u53ef\u80fd\u635f\u5bb3\u6027\u80fd\uff0c\u800c\u7f13\u89e3\u5206\u5e03\u504f\u79fb\u6bd4\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u66f4\u91cd\u8981\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u957f\u671f\u9884\u6d4b\u548c\u590d\u5408\u8bef\u5dee\u662f\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u4e3b\u8981\u969c\u788d\uff0c\u4f46\u4f5c\u8005\u8d28\u7591\u8fd9\u4e00\u89c2\u70b9\uff0c\u63a2\u7d22\u641c\u7d22\u5728\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u4f5c\u7528\u548c\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u641c\u7d22\u5728\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u8bc6\u522b\u5f71\u54cd\u641c\u7d22\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u57fa\u4e8e\u7f13\u89e3\u5206\u5e03\u504f\u79fb\u7684\u6d1e\u5bdf\u5f00\u53d1\u6709\u6548\u7684\u641c\u7d22\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u641c\u7d22\u53ef\u80fd\u635f\u5bb3\u6027\u80fd\uff0c\u5373\u4f7f\u6a21\u578b\u7cbe\u5ea6\u5f88\u9ad8\uff1b\u7f13\u89e3\u5206\u5e03\u504f\u79fb\u6bd4\u63d0\u9ad8\u6a21\u578b\u6216\u4ef7\u503c\u51fd\u6570\u7cbe\u5ea6\u66f4\u91cd\u8981\uff1b\u63d0\u51fa\u7684\u6280\u672f\u5b9e\u73b0\u4e86\u591a\u4e2a\u6d41\u884c\u57fa\u51c6\u9886\u57df\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u641c\u7d22\u5728\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u4e0d\u662f\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u7684\u5173\u952e\u5728\u4e8e\u6709\u6548\u7ba1\u7406\u5206\u5e03\u504f\u79fb\u800c\u975e\u5355\u7eaf\u8ffd\u6c42\u6a21\u578b\u7cbe\u5ea6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21598", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21598", "abs": "https://arxiv.org/abs/2601.21598", "authors": ["Zhi Zheng", "Wee Sun Lee"], "title": "Beyond Imitation: Reinforcement Learning for Active Latent Planning", "comment": null, "summary": "Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \\underline{A}c\\underline{t}ive Latent \\underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\\% accuracy and -3.3\\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.", "AI": {"tldr": "ATP-Latent\u63d0\u51fa\u4e00\u79cd\u4e3b\u52a8\u6f5c\u5728\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5efa\u6a21\u6f5c\u5728token\u76d1\u7763\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6f5c\u5728\u63a8\u7406\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11token\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5\u901a\u8fc7\u6a21\u4eff\u8bed\u8a00\u6807\u7b7e\u6765\u76d1\u7763\u6f5c\u5728token\uff0c\u4f46\u7531\u4e8e\u4e00\u4e2a\u95ee\u9898\u53ef\u80fd\u6709\u591a\u4e2a\u7b49\u4ef7\u4f46\u4e0d\u540c\u7684CoT\u6807\u7b7e\uff0c\u88ab\u52a8\u6a21\u4eff\u4efb\u610f\u4e00\u4e2a\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u7684\u6f5c\u5728token\u8868\u793a\u548c\u63a8\u7406\u7b56\u7565\uff0c\u9650\u5236\u4e86\u6f5c\u5728\u89c4\u5212\u80fd\u529b\uff0c\u5e76\u9020\u6210\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faATP-Latent\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u5efa\u6a21\u6f5c\u5728token\u7684\u76d1\u7763\u8fc7\u7a0b\uff0c\u83b7\u5f97\u66f4\u5e73\u6ed1\u7684\u6f5c\u5728\u7a7a\u95f4\uff1b2\uff09\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u91c7\u7528\u57fa\u4e8eVAE\u89e3\u7801\u5185\u5bb9\u4e00\u81f4\u6027\u7684\u8f85\u52a9\u8fde\u8d2f\u6027\u5956\u52b1\uff0c\u5f15\u5bfcRL\u8fc7\u7a0b\u4ee5\u83b7\u5f97\u6700\u5408\u7406\u7684\u6f5c\u5728\u63a8\u7406\u7b56\u7565\u3002", "result": "\u5728LLaMA-1B\u6a21\u578b\u4e0a\uff0cATP-Latent\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u5148\u8fdb\u57fa\u7ebf\u5b9e\u73b0\u4e86+4.1%\u7684\u51c6\u786e\u7387\u63d0\u5347\u548c-3.3%\u7684token\u51cf\u5c11\u3002", "conclusion": "\u5f3a\u8c03\u5728\u6f5c\u5728token\u8868\u793a\u7a7a\u95f4\u4e2d\u8fdb\u884c\u4e3b\u52a8\u89c4\u5212\u5bf9\u5b9e\u73b0\u6700\u4f18\u6f5c\u5728\u63a8\u7406\u7b56\u7565\u7684\u91cd\u8981\u6027\uff0cATP-Latent\u901a\u8fc7VAE\u5e73\u6ed1\u6f5c\u5728\u7a7a\u95f4\u548c\u57fa\u4e8e\u4e00\u81f4\u6027\u7684RL\u5956\u52b1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6f5c\u5728\u63a8\u7406\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22047", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22047", "abs": "https://arxiv.org/abs/2601.22047", "authors": ["Yunjia Qi", "Hao Peng", "Xintong Shi", "Amy Xin", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "On the Paradoxical Interference between Instruction-Following and Task Solving", "comment": null, "summary": "Instruction following aims to align Large Language Models (LLMs) with human intent by specifying explicit constraints on how tasks should be performed. However, we reveal a counterintuitive phenomenon: instruction following can paradoxically interfere with LLMs' task-solving capability. We propose a metric, SUSTAINSCORE, to quantify the interference of instruction following with task solving. It measures task performance drop after inserting into the instruction a self-evident constraint, which is naturally met by the original successful model output and extracted from it. Experiments on current LLMs in mathematics, multi-hop QA, and code generation show that adding the self-evident constraints leads to substantial performance drops, even for advanced models such as Claude-Sonnet-4.5. We validate the generality of the interference across constraint types and scales. Furthermore, we identify common failure patterns, and by investigating the mechanisms of interference, we observe that failed cases allocate significantly more attention to constraints compared to successful ones. Finally, we use SUSTAINSCORE to conduct an initial investigation into how distinct post-training paradigms affect the interference, presenting empirical observations on current alignment strategies. We will release our code and data to facilitate further research", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSUSTAINSCORE\u6307\u6807\uff0c\u91cf\u5316\u6307\u4ee4\u9075\u5faa\u5bf9LLM\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u7684\u5e72\u6270\uff0c\u53d1\u73b0\u6dfb\u52a0\u81ea\u8bc1\u7ea6\u675f\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5e76\u5206\u6790\u4e86\u5e72\u6270\u673a\u5236\u548c\u4e0d\u540c\u5bf9\u9f50\u7b56\u7565\u7684\u5f71\u54cd\u3002", "motivation": "\u6307\u4ee4\u9075\u5faa\u65e8\u5728\u901a\u8fc7\u660e\u786e\u7ea6\u675f\u4f7fLLM\u4e0e\u4eba\u7c7b\u610f\u56fe\u5bf9\u9f50\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u4e00\u4e2a\u53cd\u76f4\u89c9\u73b0\u8c61\uff1a\u6307\u4ee4\u9075\u5faa\u53ef\u80fd\u53cd\u800c\u5e72\u6270LLM\u7684\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u3002\u9700\u8981\u91cf\u5316\u8fd9\u79cd\u5e72\u6270\u5e76\u7406\u89e3\u5176\u673a\u5236\u3002", "method": "\u63d0\u51faSUSTAINSCORE\u6307\u6807\uff0c\u901a\u8fc7\u5411\u6307\u4ee4\u4e2d\u63d2\u5165\u81ea\u8bc1\u7ea6\u675f\uff08\u4ece\u539f\u59cb\u6210\u529f\u8f93\u51fa\u4e2d\u63d0\u53d6\u7684\u81ea\u7136\u6ee1\u8db3\u7684\u7ea6\u675f\uff09\u6765\u6d4b\u91cf\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\u3002\u5728\u6570\u5b66\u3001\u591a\u8df3QA\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u5f53\u524dLLM\uff0c\u5206\u6790\u5931\u8d25\u6a21\u5f0f\u3001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u540e\u8bad\u7ec3\u8303\u5f0f\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6dfb\u52a0\u81ea\u8bc1\u7ea6\u675f\u4f1a\u5bfc\u81f4\u663e\u8457\u6027\u80fd\u4e0b\u964d\uff0c\u5373\u4f7f\u662fClaude-Sonnet-4.5\u7b49\u5148\u8fdb\u6a21\u578b\u4e5f\u53d7\u5f71\u54cd\u3002\u5931\u8d25\u6848\u4f8b\u76f8\u6bd4\u6210\u529f\u6848\u4f8b\u5728\u7ea6\u675f\u4e0a\u5206\u914d\u4e86\u663e\u8457\u66f4\u591a\u6ce8\u610f\u529b\u3002\u4e0d\u540c\u5bf9\u9f50\u7b56\u7565\u5bf9\u5e72\u6270\u7a0b\u5ea6\u6709\u4e0d\u540c\u5f71\u54cd\u3002", "conclusion": "\u6307\u4ee4\u9075\u5faa\u786e\u5b9e\u4f1a\u5e72\u6270LLM\u7684\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\uff0cSUSTAINSCORE\u80fd\u6709\u6548\u91cf\u5316\u8fd9\u79cd\u5e72\u6270\u3002\u9700\u8981\u91cd\u65b0\u601d\u8003\u5bf9\u9f50\u7b56\u7565\uff0c\u5e73\u8861\u6307\u4ee4\u9075\u5faa\u4e0e\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.21609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21609", "abs": "https://arxiv.org/abs/2601.21609", "authors": ["Bingqian Li", "Xiaolei Wang", "Junyi Li", "Weitao Li", "Long Zhang", "Sheng Chen", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems", "comment": null, "summary": "Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.", "AI": {"tldr": "RecNet\uff1a\u4e00\u4e2a\u81ea\u6f14\u5316\u7684\u504f\u597d\u4f20\u64ad\u6846\u67b6\uff0c\u901a\u8fc7\u524d\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f18\u5316\u4e24\u9636\u6bb5\uff0c\u4e3b\u52a8\u5728\u76f8\u5173\u7528\u6237\u548c\u7269\u54c1\u95f4\u4f20\u64ad\u5b9e\u65f6\u504f\u597d\u66f4\u65b0\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u3001\u566a\u58f0\u4ea4\u4e92\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u7684\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u6570\u636e\u6765\u5efa\u6a21\u504f\u597d\u53d8\u5316\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u7a00\u758f\u3001\u566a\u58f0\u5927\uff0c\u4e14\u65e0\u6cd5\u53cd\u6620\u7528\u6237\u548c\u7269\u54c1\u95f4\u7684\u5b9e\u65f6\u76f8\u4e92\u5f71\u54cd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4e3b\u52a8\u4f20\u64ad\u5b9e\u65f6\u504f\u597d\u66f4\u65b0\u7684\u6846\u67b6\u3002", "method": "RecNet\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u9636\u6bb5\uff1a1\uff09\u524d\u5411\u9636\u6bb5\uff1a\u96c6\u4e2d\u5f0f\u504f\u597d\u8def\u7531\u673a\u5236\u901a\u8fc7\u8def\u7531\u4ee3\u7406\u6574\u5408\u504f\u597d\u66f4\u65b0\u5e76\u52a8\u6001\u4f20\u64ad\u7ed9\u6700\u76f8\u5173\u4ee3\u7406\uff1b\u4e2a\u6027\u5316\u504f\u597d\u63a5\u6536\u673a\u5236\u7ed3\u5408\u6d88\u606f\u7f13\u51b2\u548c\u53ef\u4f18\u5316\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u8fc7\u6ee4\u8bb0\u5fc6\u6765\u6307\u5bfc\u9009\u62e9\u6027\u504f\u597d\u540c\u5316\u30022\uff09\u53cd\u5411\u9636\u6bb5\uff1a\u53cd\u9988\u9a71\u52a8\u7684\u4f20\u64ad\u4f18\u5316\u673a\u5236\u6a21\u62df\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528LLM\u8fdb\u884c\u4fe1\u7528\u5206\u914d\u3001\u68af\u5ea6\u5206\u6790\u548c\u6a21\u5757\u7ea7\u4f18\u5316\uff0c\u5b9e\u73b0\u4f20\u64ad\u7b56\u7565\u7684\u6301\u7eed\u81ea\u6f14\u5316\u3002", "result": "\u5728\u5404\u79cd\u573a\u666f\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86RecNet\u5728\u5efa\u6a21\u63a8\u8350\u7cfb\u7edf\u504f\u597d\u4f20\u64ad\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "RecNet\u901a\u8fc7\u81ea\u6f14\u5316\u7684\u504f\u597d\u4f20\u64ad\u6846\u67b6\uff0c\u80fd\u591f\u4e3b\u52a8\u4f20\u64ad\u5b9e\u65f6\u504f\u597d\u66f4\u65b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u7a00\u758f\u3001\u566a\u58f0\u4ea4\u4e92\u6570\u636e\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.21618", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21618", "abs": "https://arxiv.org/abs/2601.21618", "authors": ["Marti\u00f1o R\u00edos-Garc\u00eda", "Nawaf Alampara", "Kevin Maik Jablonka"], "title": "Semantic Content Determines Algorithmic Performance", "comment": null, "summary": "Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faWhatCounts\u6d4b\u8bd5\u6846\u67b6\uff0c\u4e13\u95e8\u68c0\u6d4bLLMs\u5728\u7b80\u5355\u8ba1\u6570\u4efb\u52a1\u4e2d\u7684\u8bed\u4e49\u654f\u611f\u6027\uff0c\u53d1\u73b0\u524d\u6cbfLLMs\u7684\u51c6\u786e\u7387\u4f1a\u56e0\u8ba1\u6570\u5bf9\u8c61\u8bed\u4e49\u7c7b\u578b\u4e0d\u540c\u800c\u4ea7\u751f\u8d85\u8fc740%\u7684\u6ce2\u52a8\uff0c\u8868\u660eLLMs\u5e76\u975e\u771f\u6b63\u5b9e\u73b0\u7b97\u6cd5\u800c\u662f\u8fd1\u4f3c\u5b9e\u73b0\uff0c\u4e14\u8fd9\u79cd\u8fd1\u4f3c\u4f9d\u8d56\u4e8e\u8f93\u5165\u53c2\u6570\u7684\u610f\u4e49\u3002", "motivation": "\u7b97\u6cd5\u7684\u884c\u4e3a\u5e94\u8be5\u5bf9\u5176\u53c2\u6570\u7684\u8bed\u4e49\u5185\u5bb9\u4fdd\u6301\u4e0d\u53d8\u6027\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u5c06\u8bed\u4e49\u654f\u611f\u6027\u4e0e\u63a8\u7406\u590d\u6742\u6027\u6216\u63d0\u793a\u53d8\u5316\u6df7\u4e3a\u4e00\u8c08\u3002\u9700\u8981\u4e00\u79cd\u539f\u5b50\u5316\u7684\u6d4b\u8bd5\u65b9\u6cd5\u6765\u4e13\u95e8\u68c0\u6d4bLLMs\u662f\u5426\u771f\u6b63\u5b9e\u73b0\u4e86\u7b97\u6cd5\u4e0d\u53d8\u6027\u3002", "method": "\u63d0\u51faWhatCounts\u6d4b\u8bd5\u6846\u67b6\uff1a\u8981\u6c42\u6a21\u578b\u5728\u65e0\u6b67\u4e49\u3001\u6709\u5206\u9694\u3001\u65e0\u91cd\u590d\u3001\u65e0\u5e72\u6270\u9879\u7684\u5217\u8868\u4e2d\u8ba1\u6570\u4e0d\u540c\u8bed\u4e49\u7c7b\u578b\u7684\u9879\u76ee\uff08\u5982\u57ce\u5e02vs\u5316\u5b66\u7269\u3001\u540d\u79f0vs\u7b26\u53f7\uff09\u3002\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u5b9e\u9a8c\u6392\u9664\u6df7\u6dc6\u56e0\u7d20\uff0c\u5e76\u89c2\u5bdf\u5c11\u91cf\u65e0\u5173\u5fae\u8c03\u5bf9\u8bed\u4e49\u654f\u611f\u6027\u7684\u5f71\u54cd\u3002", "result": "\u524d\u6cbfLLMs\u7684\u8ba1\u6570\u51c6\u786e\u7387\u4ec5\u56e0\u8ba1\u6570\u5bf9\u8c61\u8bed\u4e49\u7c7b\u578b\u4e0d\u540c\u5c31\u4ea7\u751f\u8d85\u8fc740%\u7684\u6ce2\u52a8\u3002\u63a7\u5236\u5b9e\u9a8c\u6392\u9664\u4e86\u5176\u4ed6\u89e3\u91ca\uff0c\u4e14\u8bed\u4e49\u654f\u611f\u6027\u4f1a\u968f\u5c11\u91cf\u65e0\u5173\u5fae\u8c03\u53d1\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u53d8\u5316\u3002\u901a\u8fc7\u667a\u80fd\u4f53\u793a\u4f8b\u5c55\u793a\u4e86\u8fd9\u79cd\u8bed\u4e49\u4f9d\u8d56\u6027\u4f1a\u6269\u5c55\u5230\u8ba1\u6570\u4ee5\u5916\u7684LLM\u529f\u80fd\u3002", "conclusion": "LLMs\u5e76\u975e\u771f\u6b63\u5b9e\u73b0\u7b97\u6cd5\uff0c\u800c\u662f\u8fd1\u4f3c\u5b9e\u73b0\u7b97\u6cd5\uff0c\u4e14\u8fd9\u79cd\u8fd1\u4f3c\u4f9d\u8d56\u4e8e\u8f93\u5165\u53c2\u6570\u7684\u610f\u4e49\u3002\u8fd9\u79cd\u8bed\u4e49\u654f\u611f\u6027\u5177\u6709\u5e7f\u6cdb\u5f71\u54cd\uff0c\u4efb\u4f55LLM\u51fd\u6570\u90fd\u53ef\u80fd\u9690\u85cf\u7740\u5bf9\u5176\u8f93\u5165\u610f\u4e49\u7684\u4f9d\u8d56\u3002", "topic": "agent analysis"}}
{"id": "2601.22055", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22055", "abs": "https://arxiv.org/abs/2601.22055", "authors": ["Yaxin Du", "Junru Song", "Yifan Zhou", "Cheng Wang", "Jiahao Gu", "Zimeng Chen", "Menglan Chen", "Wen Yao", "Yang Yang", "Ying Wen", "Siheng Chen"], "title": "$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA", "comment": null, "summary": "Retrieval-augmented generation is a practical paradigm for question answering over long documents, but it remains brittle for multimodal reading where text, tables, and figures are interleaved across many pages. First, flat chunking breaks document-native structure and cross-modal alignment, yielding semantic fragments that are hard to interpret in isolation. Second, even iterative retrieval can fail in long contexts by looping on partial evidence or drifting into irrelevant sections as noise accumulates, since each step is guided only by the current snippet without a persistent global search state. We introduce $G^2$-Reader, a dual-graph system, to address both issues. It evolves a Content Graph to preserve document-native structure and cross-modal semantics, and maintains a Planning Graph, an agentic directed acyclic graph of sub-questions, to track intermediate findings and guide stepwise navigation for evidence completion. On VisDoMBench across five multimodal domains, $G^2$-Reader with Qwen3-VL-32B-Instruct reaches 66.21\\% average accuracy, outperforming strong baselines and a standalone GPT-5 (53.08\\%).", "AI": {"tldr": "G\u00b2-Reader\uff1a\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u957f\u6587\u6863\u95ee\u7b54\u7684\u53cc\u56fe\u7cfb\u7edf\uff0c\u901a\u8fc7\u5185\u5bb9\u56fe\u4fdd\u6301\u6587\u6863\u539f\u751f\u7ed3\u6784\u548c\u8de8\u6a21\u6001\u8bed\u4e49\uff0c\u901a\u8fc7\u89c4\u5212\u56fe\u8ddf\u8e2a\u4e2d\u95f4\u53d1\u73b0\u5e76\u5f15\u5bfc\u9010\u6b65\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u9605\u8bfb\u6027\u80fd", "motivation": "\u5f53\u524d\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u957f\u6587\u6863\uff08\u5305\u542b\u6587\u672c\u3001\u8868\u683c\u3001\u56fe\u8868\uff09\u65f6\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5e73\u9762\u5206\u5757\u7834\u574f\u4e86\u6587\u6863\u539f\u751f\u7ed3\u6784\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4ea7\u751f\u96be\u4ee5\u72ec\u7acb\u7406\u89e3\u7684\u8bed\u4e49\u7247\u6bb5\uff1b2\uff09\u5373\u4f7f\u8fed\u4ee3\u68c0\u7d22\u4e5f\u53ef\u80fd\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u5931\u8d25\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u6301\u4e45\u7684\u5168\u5c40\u641c\u7d22\u72b6\u6001\u6765\u8ddf\u8e2a\u8bc1\u636e\u6536\u96c6\u8fdb\u5ea6", "method": "\u63d0\u51faG\u00b2-Reader\u53cc\u56fe\u7cfb\u7edf\uff1a1\uff09\u5185\u5bb9\u56fe\uff1a\u4fdd\u6301\u6587\u6863\u539f\u751f\u7ed3\u6784\u548c\u8de8\u6a21\u6001\u8bed\u4e49\uff1b2\uff09\u89c4\u5212\u56fe\uff1a\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5305\u542b\u5b50\u95ee\u9898\uff0c\u7528\u4e8e\u8ddf\u8e2a\u4e2d\u95f4\u53d1\u73b0\u5e76\u5f15\u5bfc\u9010\u6b65\u5bfc\u822a\u4ee5\u5b8c\u6210\u8bc1\u636e\u6536\u96c6", "result": "\u5728VisDoMBench\u7684\u4e94\u4e2a\u591a\u6a21\u6001\u9886\u57df\u4e0a\uff0cG\u00b2-Reader\u914d\u5408Qwen3-VL-32B-Instruct\u8fbe\u523066.21%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u548c\u72ec\u7acb\u7684GPT-5\uff0853.08%\uff09", "conclusion": "G\u00b2-Reader\u901a\u8fc7\u53cc\u56fe\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u957f\u6587\u6863\u95ee\u7b54\u4e2d\u7684\u7ed3\u6784\u4fdd\u6301\u548c\u5168\u5c40\u5bfc\u822a\u95ee\u9898\uff0c\u4e3a\u8de8\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2601.22139", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22139", "abs": "https://arxiv.org/abs/2601.22139", "authors": ["Xin Chen", "Feng Jiang", "Yiqian Zhang", "Hardy Chen", "Shuo Yan", "Wenya Xie", "Min Yang", "Shujian Huang"], "title": "Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers", "comment": "The manuscript is under review", "summary": "Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \\emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\\% higher accuracy, 22.90\\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}", "AI": {"tldr": "\u63d0\u51fa\u4e3b\u52a8\u4ea4\u4e92\u63a8\u7406(PIR)\u65b0\u8303\u5f0f\uff0c\u8ba9LLM\u4ece\u88ab\u52a8\u6c42\u89e3\u5668\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u8be2\u95ee\u8005\uff0c\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u89e3\u51b3\u524d\u63d0\u548c\u610f\u56fe\u5c42\u9762\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u601d\u7ef4\u94fe\u7684\u63a8\u7406LLM\u5b58\u5728\"\u76f2\u76ee\u81ea\u6211\u601d\u8003\"\u7684\u5c40\u9650\u6027\uff0c\u5f53\u5173\u952e\u4fe1\u606f\u7f3a\u5931\u6216\u6a21\u7cca\u65f6\u4ecd\u8fdb\u884c\u5927\u91cf\u5185\u90e8\u63a8\u7406\u3002\u9700\u8981\u4e00\u79cd\u65b0\u8303\u5f0f\u8ba9LLM\u80fd\u591f\u4e3b\u52a8\u4e0e\u7528\u6237\u4ea4\u4e92\u6f84\u6e05\u4e0d\u786e\u5b9a\u6027\u3002", "method": "PIR\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1)\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u8d4b\u4e88\u6a21\u578b\u4ea4\u4e92\u63a8\u7406\u80fd\u529b\uff1b2)\u57fa\u4e8e\u7528\u6237\u6a21\u62df\u5668\u7684\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u590d\u5408\u5956\u52b1\u4f7f\u6a21\u578b\u884c\u4e3a\u4e0e\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u6587\u6863\u7f16\u8f91\u4efb\u52a1\u4e0a\uff0cPIR\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe32.70%\uff0c\u901a\u8fc7\u7387\u63d0\u534722.90%\uff0cBLEU\u5206\u6570\u63d0\u534741.36\uff0c\u540c\u65f6\u51cf\u5c11\u8fd1\u4e00\u534a\u7684\u63a8\u7406\u8ba1\u7b97\u548c\u4e0d\u5fc5\u8981\u7684\u4ea4\u4e92\u8f6e\u6b21\u3002", "conclusion": "PIR\u6210\u529f\u5c06LLM\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u8be2\u95ee\u8005\uff0c\u6709\u6548\u89e3\u51b3\u524d\u63d0\u548c\u610f\u56fe\u5c42\u9762\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2601.21714", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21714", "abs": "https://arxiv.org/abs/2601.21714", "authors": ["Kaixiang Wang", "Yidan Lin", "Jiong Lou", "Zhaojiacheng Zhou", "Bunyod Suvonov", "Jie Li"], "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory", "comment": "18 pages", "summary": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%.", "AI": {"tldr": "E-mem\u6846\u67b6\u901a\u8fc7\u4ece\u8bb0\u5fc6\u9884\u5904\u7406\u8f6c\u5411\u60c5\u666f\u4e0a\u4e0b\u6587\u91cd\u5efa\uff0c\u91c7\u7528\u5f02\u6784\u5206\u5c42\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u5728LoCoMo\u57fa\u51c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u5e76\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5411System 2\u63a8\u7406\u6f14\u8fdb\u65f6\uff0c\u4e3b\u6d41\u8bb0\u5fc6\u9884\u5904\u7406\u65b9\u6cd5\u5b58\u5728\u7834\u574f\u6027\u53bb\u4e0a\u4e0b\u6587\u5316\u95ee\u9898\uff0c\u5c06\u590d\u6742\u5e8f\u5217\u4f9d\u8d56\u538b\u7f29\u4e3a\u9884\u5b9a\u4e49\u7ed3\u6784\u4f1a\u7834\u574f\u6df1\u5ea6\u63a8\u7406\u6240\u9700\u7684\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\u3002", "method": "\u63d0\u51faE-mem\u6846\u67b6\uff0c\u4ece\u8bb0\u5fc6\u9884\u5904\u7406\u8f6c\u5411\u60c5\u666f\u4e0a\u4e0b\u6587\u91cd\u5efa\uff0c\u91c7\u7528\u5f02\u6784\u5206\u5c42\u67b6\u6784\uff1a\u591a\u4e2a\u52a9\u624b\u4ee3\u7406\u7ef4\u62a4\u672a\u538b\u7f29\u7684\u8bb0\u5fc6\u4e0a\u4e0b\u6587\uff0c\u4e2d\u592e\u4e3b\u4ee3\u7406\u534f\u8c03\u5168\u5c40\u89c4\u5212\uff0c\u52a9\u624b\u5728\u6fc0\u6d3b\u7247\u6bb5\u5185\u8fdb\u884c\u672c\u5730\u63a8\u7406\u5e76\u63d0\u53d6\u4e0a\u4e0b\u6587\u611f\u77e5\u8bc1\u636e\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cE-mem\u8fbe\u5230\u8d85\u8fc754%\u7684F1\u5206\u6570\uff0c\u6bd4\u6700\u5148\u8fdb\u7684GAM\u65b9\u6cd5\u63d0\u53477.75%\uff0c\u540c\u65f6\u51cf\u5c11\u8d85\u8fc770%\u7684token\u6210\u672c\u3002", "conclusion": "E-mem\u901a\u8fc7\u60c5\u666f\u4e0a\u4e0b\u6587\u91cd\u5efa\u6709\u6548\u89e3\u51b3\u4e86\u8bb0\u5fc6\u9884\u5904\u7406\u4e2d\u7684\u53bb\u4e0a\u4e0b\u6587\u5316\u95ee\u9898\uff0c\u4e3aLLM\u4ee3\u7406\u5b9e\u73b0System 2\u63a8\u7406\u63d0\u4f9b\u4e86\u4fdd\u6301\u903b\u8f91\u5b8c\u6574\u6027\u7684\u65b0\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2601.22149", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22149", "abs": "https://arxiv.org/abs/2601.22149", "authors": ["Hang Ding", "Peidong Liu", "Junqiao Wang", "Ziwei Ji", "Meng Cao", "Rongzhao Zhang", "Lynn Ai", "Eric Yang", "Tianyu Shi", "Lei Yu"], "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents", "comment": null, "summary": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.", "AI": {"tldr": "DynaWeb\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u7f51\u9875\u4e16\u754c\u6a21\u578b\u6765\u6a21\u62df\u7f51\u9875\u73af\u5883\uff0c\u8ba9\u7f51\u9875\u4ee3\u7406\u5728\u5408\u6210\u73af\u5883\u4e2d\u8fdb\u884c\"\u60f3\u8c61\"\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u9875\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u8bad\u7ec3\u81ea\u4e3b\u7f51\u9875\u4ee3\u7406\u9762\u4e34\u4e0e\u5b9e\u65f6\u4e92\u8054\u7f51\u4ea4\u4e92\u7684\u4f4e\u6548\u3001\u6602\u8d35\u548c\u98ce\u9669\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDynaWeb\u6846\u67b6\uff1a1) \u8bad\u7ec3\u7f51\u9875\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u7ed9\u5b9a\u4ee3\u7406\u52a8\u4f5c\u540e\u7684\u81ea\u7136\u7f51\u9875\u8868\u793a\uff1b2) \u5728\u5408\u6210\u7f51\u9875\u73af\u5883\u4e2d\u8fdb\u884c\u7b56\u7565\"\u60f3\u8c61\"\u8bad\u7ec3\uff1b3) \u5c06\u771f\u5b9e\u4e13\u5bb6\u8f68\u8ff9\u4e0e\u7b56\u7565\u8f68\u8ff9\u4ea4\u9519\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "result": "\u5728WebArena\u548cWebVoyager\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDynaWeb\u663e\u8457\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u7f51\u9875\u4ee3\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\"\u60f3\u8c61\"\u8bad\u7ec3\u7f51\u9875\u4ee3\u7406\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u6269\u5c55\u5728\u7ebf\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21754", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21754", "abs": "https://arxiv.org/abs/2601.21754", "authors": ["Haoyu Wang", "Guozheng Ma", "Shugang Cui", "Yilun Kong", "Haotian Luo", "Li Shen", "Mengya Gao", "Yichao Wu", "Xiaogang Wang", "Dacheng Tao"], "title": "Language-based Trial and Error Falls Behind in the Era of Experience", "comment": null, "summary": "While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.", "AI": {"tldr": "SCOUT\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7\"\u4fa6\u5bdf\u5175\"\u63a2\u7d22\u73af\u5883\u52a8\u6001\uff0c\u5f15\u5bfcLLM\u5728\u975e\u8bed\u8a00\u73af\u5883\u4e2d\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u672a\u89c1\u8fc7\u7684\u975e\u8bed\u8a00\u73af\u5883\uff08\u5982\u7b26\u53f7\u6216\u7a7a\u95f4\u4efb\u52a1\uff09\u4e2d\u8868\u73b0\u53d7\u9650\u3002\u4f20\u7edf\u65b9\u6cd5\u8ba4\u4e3a\u8fd9\u662f\u9884\u8bad\u7ec3\u5206\u5e03\u4e0e\u6d4b\u8bd5\u5206\u5e03\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u4f46\u672c\u6587\u53d1\u73b0\u4e3b\u8981\u74f6\u9888\u662f\u63a2\u7d22\u6210\u672c\u8fc7\u9ad8\uff1a\u638c\u63e1\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u5927\u91cf\u8bd5\u9519\uff0c\u8fd9\u5bf9\u53c2\u6570\u5e9e\u5927\u7684LLM\u5728\u9ad8\u7ef4\u8bed\u4e49\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u4e0d\u53ef\u6301\u7eed\u3002", "method": "\u63d0\u51faSCOUT\u6846\u67b6\uff0c\u5c06\u63a2\u7d22\u4e0e\u5229\u7528\u89e3\u8026\uff1a1) \u4f7f\u7528\u8f7b\u91cf\u7ea7\"\u4fa6\u5bdf\u5175\"\uff08\u5982\u5c0f\u578bMLP\uff09\u5feb\u901f\u63a2\u6d4b\u73af\u5883\u52a8\u6001\uff1b2) \u6536\u96c6\u7684\u8f68\u8ff9\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5f15\u5bfcLLM\uff1b3) \u901a\u8fc7\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u6fc0\u6d3bLLM\u7684\u6f5c\u5728\u4e16\u754c\u77e5\u8bc6\u3002", "result": "SCOUT\u4f7fQwen2.5-3B-Instruct\u6a21\u578b\u5e73\u5747\u5f97\u5206\u8fbe\u52300.86\uff0c\u663e\u8457\u4f18\u4e8eGemini-2.5-Pro\uff080.60\uff09\u7b49\u4e13\u6709\u6a21\u578b\uff0c\u540c\u65f6\u8282\u7701\u7ea660%\u7684GPU\u5c0f\u65f6\u6d88\u8017\u3002", "conclusion": "\u63a2\u7d22\u6210\u672c\u662fLLM\u5728\u975e\u8bed\u8a00\u73af\u5883\u4e2d\u8868\u73b0\u53d7\u9650\u7684\u4e3b\u8981\u74f6\u9888\uff0cSCOUT\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4ee3\u7406\u5f15\u5bfcLLM\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21800", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21800", "abs": "https://arxiv.org/abs/2601.21800", "authors": ["Dionizije Fa", "Marko \u010culjak", "Bruno Pand\u017ea", "Mateo \u010cupi\u0107"], "title": "BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics", "comment": null, "summary": "This paper introduces BioAgent Bench, a benchmark dataset and an evaluation suite designed for measuring the performance and robustness of AI agents in common bioinformatics tasks. The benchmark contains curated end-to-end tasks (e.g., RNA-seq, variant calling, metagenomics) with prompts that specify concrete output artifacts to support automated assessment, including stress testing under controlled perturbations. We evaluate frontier closed-source and open-weight models across multiple agent harnesses, and use an LLM-based grader to score pipeline progress and outcome validity. We find that frontier agents can complete multi-step bioinformatics pipelines without elaborate custom scaffolding, often producing the requested final artifacts reliably. However, robustness tests reveal failure modes under controlled perturbations (corrupted inputs, decoy files, and prompt bloat), indicating that correct high-level pipeline construction does not guarantee reliable step-level reasoning. Finally, because bioinformatics workflows may involve sensitive patient data, proprietary references, or unpublished IP, closed-source models can be unsuitable under strict privacy constraints; in such settings, open-weight models may be preferable despite lower completion rates. We release the dataset and evaluation suite publicly.", "AI": {"tldr": "BioAgent Bench\uff1a\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u751f\u7269\u4fe1\u606f\u5b66\u4efb\u52a1\u4e2d\u6027\u80fd\u548c\u9c81\u68d2\u6027\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\u5957\u4ef6\uff0c\u5305\u542b\u7aef\u5230\u7aef\u4efb\u52a1\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u53d1\u73b0\u524d\u6cbf\u4ee3\u7406\u80fd\u5b8c\u6210\u591a\u6b65\u9aa4\u6d41\u7a0b\u4f46\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u751f\u7269\u4fe1\u606f\u5b66\u4efb\u52a1\u7684AI\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\uff0c\u9700\u8981\u8861\u91cf\u4ee3\u7406\u5728\u771f\u5b9e\u751f\u7269\u4fe1\u606f\u5b66\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u6027\u80fd\u3001\u9c81\u68d2\u6027\u4ee5\u53ca\u9690\u79c1\u7ea6\u675f\u4e0b\u7684\u9002\u7528\u6027\u3002", "method": "\u521b\u5efa\u5305\u542bRNA-seq\u3001\u53d8\u5f02\u68c0\u6d4b\u3001\u5b8f\u57fa\u56e0\u7ec4\u5b66\u7b49\u7aef\u5230\u7aef\u4efb\u52a1\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u5177\u4f53\u8f93\u51fa\u89c4\u8303\u652f\u6301\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u5f15\u5165\u53d7\u63a7\u6270\u52a8\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\uff0c\u4f7f\u7528LLM\u8bc4\u5206\u5668\u8bc4\u4f30\u6d41\u7a0b\u8fdb\u5c55\u548c\u7ed3\u679c\u6709\u6548\u6027\u3002", "result": "\u524d\u6cbf\u4ee3\u7406\u80fd\u591f\u5b8c\u6210\u591a\u6b65\u9aa4\u751f\u7269\u4fe1\u606f\u5b66\u6d41\u7a0b\u800c\u65e0\u9700\u590d\u6742\u5b9a\u5236\u6846\u67b6\uff0c\u901a\u5e38\u80fd\u53ef\u9760\u751f\u6210\u6700\u7ec8\u4ea7\u7269\u3002\u4f46\u9c81\u68d2\u6027\u6d4b\u8bd5\u663e\u793a\u5728\u8f93\u5165\u635f\u574f\u3001\u5e72\u6270\u6587\u4ef6\u3001\u63d0\u793a\u81a8\u80c0\u7b49\u6270\u52a8\u4e0b\u5b58\u5728\u5931\u8d25\u6a21\u5f0f\u3002\u9690\u79c1\u7ea6\u675f\u4e0b\u5f00\u6e90\u6a21\u578b\u867d\u5b8c\u6210\u7387\u8f83\u4f4e\u4f46\u66f4\u9002\u7528\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u751f\u7269\u4fe1\u606f\u5b66\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u4f46\u9c81\u68d2\u6027\u4ecd\u9700\u63d0\u5347\uff0c\u9ad8\u5c42\u6d41\u7a0b\u6784\u5efa\u6b63\u786e\u4e0d\u80fd\u4fdd\u8bc1\u6b65\u9aa4\u7ea7\u63a8\u7406\u53ef\u9760\u3002\u9690\u79c1\u654f\u611f\u573a\u666f\u4e0b\u5f00\u6e90\u6a21\u578b\u66f4\u5177\u4f18\u52bf\u3002\u57fa\u51c6\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53d1\u5e03\u3002", "topic": "agent analysis"}}
{"id": "2601.21391", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21391", "abs": "https://arxiv.org/abs/2601.21391", "authors": ["Minjae Cho", "Huy Trong Tran"], "title": "Intrinsic Reward Policy Optimization for Sparse-Reward Environments", "comment": null, "summary": "Exploration is essential in reinforcement learning as an agent relies on trial and error to learn an optimal policy. However, when rewards are sparse, naive exploration strategies, like noise injection, are often insufficient. Intrinsic rewards can also provide principled guidance for exploration by, for example, combining them with extrinsic rewards to optimize a policy or using them to train subpolicies for hierarchical learning. However, the former approach suffers from unstable credit assignment, while the latter exhibits sample inefficiency and sub-optimality. We propose a policy optimization framework that leverages multiple intrinsic rewards to directly optimize a policy for an extrinsic reward without pretraining subpolicies. Our algorithm -- intrinsic reward policy optimization (IRPO) -- achieves this by using a surrogate policy gradient that provides a more informative learning signal than the true gradient in sparse-reward environments. We demonstrate that IRPO improves performance and sample efficiency relative to baselines in discrete and continuous environments, and formally analyze the optimization problem solved by IRPO. Our code is available at https://github.com/Mgineer117/IRPO.", "AI": {"tldr": "IRPO\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u4e2a\u5185\u5728\u5956\u52b1\u76f4\u63a5\u4f18\u5316\u7b56\u7565\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u66ff\u4ee3\u7b56\u7565\u68af\u5ea6\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u5b50\u7b56\u7565\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\uff0c\u4f20\u7edf\u7684\u566a\u58f0\u6ce8\u5165\u63a2\u7d22\u7b56\u7565\u5f80\u5f80\u4e0d\u8db3\u3002\u73b0\u6709\u7684\u5185\u5728\u5956\u52b1\u65b9\u6cd5\u5b58\u5728\u4fe1\u7528\u5206\u914d\u4e0d\u7a33\u5b9a\uff08\u4e0e\u5916\u5728\u5956\u52b1\u7ed3\u5408\u4f18\u5316\u7b56\u7565\uff09\u6216\u6837\u672c\u6548\u7387\u4f4e\u548c\u6b21\u4f18\uff08\u7528\u4e8e\u5206\u5c42\u5b66\u4e60\u8bad\u7ec3\u5b50\u7b56\u7565\uff09\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5185\u5728\u5956\u52b1\u7b56\u7565\u4f18\u5316\uff08IRPO\uff09\u6846\u67b6\uff0c\u5229\u7528\u591a\u4e2a\u5185\u5728\u5956\u52b1\u76f4\u63a5\u4f18\u5316\u5916\u5728\u5956\u52b1\u7b56\u7565\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u5b50\u7b56\u7565\u3002\u6838\u5fc3\u662f\u4f7f\u7528\u66ff\u4ee3\u7b56\u7565\u68af\u5ea6\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u63d0\u4f9b\u6bd4\u771f\u5b9e\u68af\u5ea6\u66f4\u4e30\u5bcc\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u5728\u79bb\u6563\u548c\u8fde\u7eed\u73af\u5883\u4e2d\uff0cIRPO\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u3002\u8bba\u6587\u8fd8\u5bf9IRPO\u89e3\u51b3\u7684\u4f18\u5316\u95ee\u9898\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u5206\u6790\u3002", "conclusion": "IRPO\u901a\u8fc7\u66ff\u4ee3\u7b56\u7565\u68af\u5ea6\u6709\u6548\u5229\u7528\u591a\u4e2a\u5185\u5728\u5956\u52b1\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u95ee\u9898\uff0c\u907f\u514d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4fe1\u7528\u5206\u914d\u4e0d\u7a33\u5b9a\u548c\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21879", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21879", "abs": "https://arxiv.org/abs/2601.21879", "authors": ["Rem Collier", "Katharine Beaumont", "Andrei Ciortea"], "title": "astra-langchain4j: Experiences Combining LLMs and Agent Programming", "comment": null, "summary": "Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230ASTRA\u7f16\u7a0b\u8bed\u8a00\u4e2d\u7684\u539f\u578b\u5f00\u53d1\u7ecf\u9a8c\uff0c\u901a\u8fc7\u4e09\u4e2a\u793a\u4f8b\u5b9e\u73b0\u5c55\u793a\u4e86\u4f20\u7edf\u4ee3\u7406\u5de5\u5177\u5305\u4e0e\u65b0\u5174Agentic AI\u6280\u672f\u7684\u7ed3\u5408\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u548cAgentic AI\u4f5c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5f62\u5f0f\u7684\u53d1\u5c55\uff0c\u9700\u8981\u63a2\u7d22\u8fd9\u4e9b\u65b0\u6280\u672f\u5982\u4f55\u5f71\u54cd\u4f20\u7edf\u4ee3\u7406\u5de5\u5177\u5305\u7684\u4f7f\u7528\uff0c\u4ee5\u53ca\u4f20\u7edf\u5de5\u5177\u5305\u4e2d\u7684\u4e30\u5bcc\u7ecf\u9a8c\u5982\u4f55\u5f71\u54cd\u65b0\u4ee3\u7406\u5e73\u53f0\u7684\u8bbe\u8ba1\u3002", "method": "\u5f00\u53d1\u4e86ASTRA\u7f16\u7a0b\u8bed\u8a00\u7684LLM\u96c6\u6210\u539f\u578b\uff0c\u7b80\u8981\u4ecb\u7ecd\u4e86\u8be5\u5de5\u5177\u5305\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u5177\u4f53\u793a\u4f8b\u5b9e\u73b0\u6765\u5c55\u793a\u96c6\u6210\u65b9\u6cd5\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86LLM\u4e0eASTRA\u7684\u96c6\u6210\u539f\u578b\uff0c\u901a\u8fc7\u4e09\u4e2a\u793a\u4f8b\u5c55\u793a\u4e86\u4f20\u7edf\u4ee3\u7406\u5de5\u5177\u5305\u4e0e\u65b0\u5174AI\u6280\u672f\u7684\u7ed3\u5408\u53ef\u80fd\u6027\u3002", "conclusion": "\u901a\u8fc7\u539f\u578b\u5f00\u53d1\u7ecf\u9a8c\uff0c\u5c55\u793a\u4e86\u4f20\u7edf\u4ee3\u7406\u5de5\u5177\u5305\u7ecf\u9a8c\u5bf9\u65b0\u5174Agentic AI\u5e73\u53f0\u8bbe\u8ba1\u7684\u4ef7\u503c\uff0c\u4ee5\u53ca\u4e24\u8005\u7ed3\u5408\u7684\u6280\u672f\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2601.21909", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21909", "abs": "https://arxiv.org/abs/2601.21909", "authors": ["Shaojie Wang", "Liang Zhang"], "title": "From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning", "comment": null, "summary": "Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\\% and 4.63\\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.", "AI": {"tldr": "\u63d0\u51faCoMT+CCRL\u6846\u67b6\uff0c\u6a21\u4eff\u4eba\u7c7b\u4e24\u9636\u6bb5\u8ba4\u77e5\u8fc7\u7a0b\uff1a\u5148\u5b66\u4e60\u62bd\u8c61\u63a8\u7406\u6a21\u5f0f\uff0c\u518d\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u6821\u51c6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4efb\u52a1\u9002\u5e94\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u5e76\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u5f53\u524dLLM\u540e\u8bad\u7ec3\u65b9\u6cd5\u5c06\u5b8c\u6574\u63a8\u7406\u8f68\u8ff9\u4f5c\u4e3a\u57fa\u672c\u5355\u5143\u8fdb\u884c\u4f18\u5316\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u5b9e\u9645\u89e3\u51b3\u95ee\u9898\u7684\u65b9\u5f0f\u5b58\u5728\u6839\u672c\u5dee\u5f02\u3002\u4eba\u7c7b\u8ba4\u77e5\u81ea\u7136\u5730\u5c06\u95ee\u9898\u89e3\u51b3\u5206\u89e3\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u5148\u83b7\u53d6\u8de8\u95ee\u9898\u901a\u7528\u7684\u62bd\u8c61\u7b56\u7565\uff08\u5143\u77e5\u8bc6\uff09\uff0c\u518d\u5c06\u5176\u9002\u5e94\u5230\u5177\u4f53\u5b9e\u4f8b\u3002\u73b0\u6709\u65b9\u6cd5\u7684\u95ee\u9898\u4e2d\u5fc3\u5316\u5bfc\u81f4\u62bd\u8c61\u7b56\u7565\u4e0e\u5177\u4f53\u6267\u884c\u7ea0\u7f20\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) Chain-of-Meta-Thought (CoMT)\uff1a\u4e13\u6ce8\u4e8e\u62bd\u8c61\u63a8\u7406\u6a21\u5f0f\u7684\u6709\u76d1\u7763\u5b66\u4e60\uff0c\u4e0d\u6d89\u53ca\u5177\u4f53\u6267\u884c\uff0c\u83b7\u53d6\u53ef\u6cdb\u5316\u7684\u7b56\u7565\uff1b2) Confidence-Calibrated Reinforcement Learning (CCRL)\uff1a\u901a\u8fc7\u4e2d\u95f4\u6b65\u9aa4\u7684\u7f6e\u4fe1\u5ea6\u611f\u77e5\u5956\u52b1\u4f18\u5316\u4efb\u52a1\u9002\u5e94\uff0c\u9632\u6b62\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9519\u8bef\u7ea7\u8054\uff0c\u63d0\u9ad8\u6267\u884c\u53ef\u9760\u6027\u3002", "result": "\u57284\u4e2a\u6a21\u578b\u548c8\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6\u65b9\u6cd5\uff0c\u5206\u5e03\u5185\u6027\u80fd\u63d0\u53472.19%\uff0c\u5206\u5e03\u5916\u6027\u80fd\u63d0\u53474.63%\uff0c\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1165-70%\uff0ctoken\u6d88\u8017\u51cf\u5c1150%\u3002", "conclusion": "\u5c06\u540e\u8bad\u7ec3\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u539f\u5219\u5bf9\u9f50\u4e0d\u4ec5\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u8fd8\u80fd\u663e\u8457\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u4e3aLLM\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u7b26\u5408\u8ba4\u77e5\u79d1\u5b66\u7684\u65b0\u8303\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21912", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.21912", "abs": "https://arxiv.org/abs/2601.21912", "authors": ["Zhao Wang", "Ziliang Zhao", "Zhicheng Dou"], "title": "ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation", "comment": "11 pages, 6 figures", "summary": "Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to \"process hallucinations\", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.", "AI": {"tldr": "ProRAG\u63d0\u51fa\u4e86\u4e00\u79cd\u8fc7\u7a0b\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6b65\u9aa4\u7ea7\u76d1\u7763\u5230\u5728\u7ebf\u4f18\u5316\u5faa\u73af\u4e2d\uff0c\u89e3\u51b3\u4e86RAG\u4f18\u5316\u4e2d\u5956\u52b1\u7a00\u758f\u548c\u4fe1\u7528\u5206\u914d\u4f4e\u6548\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7ed3\u679c\u7684RL\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u5956\u52b1\u7a00\u758f\u548c\u4fe1\u7528\u5206\u914d\u4f4e\u6548\u7684\u95ee\u9898\uff0c\u7c97\u7c92\u5ea6\u7684\u6807\u91cf\u5956\u52b1\u65e0\u6cd5\u8bc6\u522b\u957f\u8f68\u8ff9\u4e2d\u7684\u5177\u4f53\u9519\u8bef\u6b65\u9aa4\uff0c\u5bfc\u81f4\"\u8fc7\u7a0b\u5e7b\u89c9\"\u2014\u2014\u6a21\u578b\u901a\u8fc7\u9519\u8bef\u903b\u8f91\u6216\u5197\u4f59\u68c0\u7d22\u6b65\u9aa4\u5f97\u5230\u6b63\u786e\u7b54\u6848\u3002", "method": "ProRAG\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1a1)\u76d1\u7763\u7b56\u7565\u9884\u70ed\uff0c\u7528\u7ed3\u6784\u5316\u63a8\u7406\u683c\u5f0f\u521d\u59cb\u5316\u6a21\u578b\uff1b2)\u6784\u5efa\u57fa\u4e8eMCTS\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRM)\u91cf\u5316\u4e2d\u95f4\u63a8\u7406\u8d28\u91cf\uff1b3)PRM\u5f15\u5bfc\u7684\u63a8\u7406\u7cbe\u70bc\uff0c\u4f7f\u7b56\u7565\u4e0e\u7ec6\u7c92\u5ea6\u8fc7\u7a0b\u504f\u597d\u5bf9\u9f50\uff1b4)\u8fc7\u7a0b\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\uff0c\u91c7\u7528\u53cc\u7c92\u5ea6\u4f18\u52bf\u673a\u5236\uff0c\u805a\u5408\u6b65\u9aa4\u7ea7\u8fc7\u7a0b\u5956\u52b1\u548c\u5168\u5c40\u7ed3\u679c\u4fe1\u53f7\u3002", "result": "\u5728\u4e94\u4e2a\u591a\u8df3\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cProRAG\u76f8\u6bd4\u57fa\u4e8e\u7ed3\u679c\u548c\u8fc7\u7a0b\u611f\u77e5\u7684RL\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6574\u4f53\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u957f\u89c6\u91ce\u4efb\u52a1\u4e0a\uff0c\u9a8c\u8bc1\u4e86\u7ec6\u7c92\u5ea6\u8fc7\u7a0b\u76d1\u7763\u7684\u6709\u6548\u6027\u3002", "conclusion": "ProRAG\u901a\u8fc7\u6574\u5408\u6b65\u9aa4\u7ea7\u76d1\u7763\u5230\u5728\u7ebf\u4f18\u5316\u5faa\u73af\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86RAG\u4f18\u5316\u4e2d\u7684\u5956\u52b1\u7a00\u758f\u548c\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u53cd\u9988\u673a\u5236\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21916", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.21916", "abs": "https://arxiv.org/abs/2601.21916", "authors": ["Yiqun Chen", "Erhan Zhang", "Tianyi Hu", "Shijie Wang", "Zixuan Yang", "Meizhi Zhong", "Xiaochi Wei", "Yan Gao", "Yi Wu", "Yao Hu", "Jiaxin Mao"], "title": "JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG", "comment": null, "summary": "The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \\textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \\textbf{JADE} (\\textbf{J}oint \\textbf{A}gentic \\textbf{D}ynamic \\textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \\textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.", "AI": {"tldr": "JADE\u63d0\u51fa\u8054\u5408\u4f18\u5316\u52a8\u6001\u591a\u8f6e\u5de5\u4f5c\u6d41\u4e2d\u89c4\u5212\u4e0e\u6267\u884c\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u591a\u667a\u80fd\u4f53\u56e2\u961f\u5efa\u6a21\u5b9e\u73b0\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u89c4\u5212\u4e0e\u6267\u884c\u6a21\u5757\u89e3\u8026\u4f18\u5316\u5bfc\u81f4\u7684\"\u6218\u7565\u64cd\u4f5c\u4e0d\u5339\u914d\"\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u9762\u4e34\u5173\u952e\u4e8c\u5206\u6cd5\uff1a\u8981\u4e48\u5728\u56fa\u5b9a\u67b6\u6784\u4e2d\u8054\u5408\u4f18\u5316\u6a21\u5757\uff0c\u8981\u4e48\u652f\u6301\u52a8\u6001\u89c4\u5212\u4f46\u5c06\u6267\u884c\u5668\u89c6\u4e3a\u51bb\u7ed3\u9ed1\u76d2\u3002\u8fd9\u79cd\u89e3\u8026\u4f18\u5316\u5bfc\u81f4\"\u6218\u7565\u64cd\u4f5c\u4e0d\u5339\u914d\"\uff0c\u590d\u6742\u89c4\u5212\u7b56\u7565\u56e0\u6267\u884c\u5668\u4e0d\u9002\u5e94\u800c\u65e0\u6cd5\u5b9e\u73b0\uff0c\u7cfb\u7edf\u590d\u6742\u5ea6\u589e\u52a0\u5374\u5e26\u6765\u8d1f\u9762\u6027\u80fd\u6536\u76ca\u3002", "method": "\u63d0\u51faJADE\u6846\u67b6\uff0c\u5c06\u7cfb\u7edf\u5efa\u6a21\u4e3a\u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\u7684\u534f\u540c\u591a\u667a\u80fd\u4f53\u56e2\u961f\uff0c\u901a\u8fc7\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u9a71\u52a8\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u5b9e\u73b0\u89c4\u5212\u5668\u4e0e\u6267\u884c\u5668\u7684\u534f\u540c\u9002\u5e94\uff1a\u89c4\u5212\u5668\u5b66\u4e60\u5728\u6267\u884c\u5668\u80fd\u529b\u8fb9\u754c\u5185\u64cd\u4f5c\uff0c\u6267\u884c\u5668\u6f14\u5316\u4ee5\u5bf9\u9f50\u9ad8\u5c42\u6218\u7565\u610f\u56fe\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660eJADE\u5c06\u79bb\u6563\u6a21\u5757\u8f6c\u5316\u4e3a\u534f\u540c\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u5de5\u4f5c\u6d41\u7f16\u6392\u5b9e\u73b0\u6548\u7387\u4e0e\u6548\u679c\u4e4b\u95f4\u7684\u7075\u6d3b\u5e73\u8861\u3002", "conclusion": "JADE\u901a\u8fc7\u8054\u5408\u4f18\u5316\u52a8\u6001\u591a\u8f6e\u5de5\u4f5c\u6d41\u4e2d\u7684\u89c4\u5212\u4e0e\u6267\u884c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RAG\u7cfb\u7edf\u7684\u6218\u7565\u64cd\u4f5c\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6a21\u5757\u95f4\u7684\u534f\u540c\u9002\u5e94\u548c\u7cfb\u7edf\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002", "topic": "agent analysis"}}
{"id": "2601.21459", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21459", "abs": "https://arxiv.org/abs/2601.21459", "authors": ["Chengyu Du", "Xintao Wang", "Aili Chen", "Weiyuan Li", "Rui Xu", "Junteng Liu", "Zishan Huang", "Rong Tian", "Zijun Sun", "Yuhao Li", "Liheng Feng", "Deming Ding", "Pengyu Zhao", "Yanghua Xiao"], "title": "HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing", "comment": "41pages, 10 figures", "summary": "LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters' first-person thinking from LLMs' third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train \\method models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.", "AI": {"tldr": "HER\u6846\u67b6\u901a\u8fc7\u53cc\u5c42\u601d\u7ef4\u67b6\u6784\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347LLM\u89d2\u8272\u626e\u6f14\u4e2d\u7684\u8ba4\u77e5\u6a21\u62df\u80fd\u529b\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524dLLM\u89d2\u8272\u626e\u6f14\u867d\u7136\u80fd\u6a21\u62df\u89d2\u8272\u8bed\u6c14\u548c\u77e5\u8bc6\uff0c\u4f46\u96be\u4ee5\u6a21\u62df\u884c\u4e3a\u80cc\u540e\u7684\u5185\u5728\u601d\u7ef4\u8fc7\u7a0b\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u8f68\u8ff9\u6570\u636e\u548c\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u53ef\u9760\u5956\u52b1\u4fe1\u53f7\u3002", "method": "\u63d0\u51faHER\u7edf\u4e00\u6846\u67b6\uff0c\u5f15\u5165\u53cc\u5c42\u601d\u7ef4\u67b6\u6784\u533a\u5206\u89d2\u8272\u7684\u7b2c\u4e00\u4eba\u79f0\u601d\u7ef4\u548cLLM\u7684\u7b2c\u4e09\u4eba\u79f0\u601d\u7ef4\u3002\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u6784\u5efa\u63a8\u7406\u589e\u5f3a\u7684\u89d2\u8272\u626e\u6f14\u6570\u636e\uff0c\u5efa\u7acb\u4eba\u7c7b\u5bf9\u9f50\u7684\u539f\u5219\u548c\u5956\u52b1\u6a21\u578b\uff0c\u57fa\u4e8eQwen3-32B\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728CoSER\u57fa\u51c6\u4e0a\u63d0\u534730.26\u5206\uff0c\u5728Minimax Role-Play Bench\u4e0a\u63d0\u534714.97\u5206\uff0c\u663e\u8457\u8d85\u8d8aQwen3-32B\u57fa\u7ebf\u3002", "conclusion": "HER\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u89d2\u8272\u626e\u6f14\u4e2d\u7684\u8ba4\u77e5\u6a21\u62df\u6311\u6218\uff0c\u901a\u8fc7\u53cc\u5c42\u601d\u7ef4\u67b6\u6784\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u66f4\u771f\u5b9e\u7684\u89d2\u8272\u5185\u5728\u601d\u7ef4\u6a21\u62df\u3002", "topic": "agent analysis"}}
{"id": "2601.21919", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21919", "abs": "https://arxiv.org/abs/2601.21919", "authors": ["Yiqun Chen", "Jinyuan Feng", "Wei Yang", "Meizhi Zhong", "Zhengliang Shi", "Rui Li", "Xiaochi Wei", "Yan Gao", "Yi Wu", "Yao Hu", "Zhiqiang Pu", "Jiaxin Mao"], "title": "Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning", "comment": null, "summary": "The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \\textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \\textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \\textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\\% to 39.0\\% while boosting accuracy by 4.33\\% to 10.02\\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.", "AI": {"tldr": "SCMA\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5272\u548c\u8bc4\u5206\u667a\u80fd\u4f53\u534f\u4f5c\u68c0\u6d4b\u5197\u4f59\u63a8\u7406\u5757\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5927\u63a8\u7406\u6a21\u578b\u7684\u54cd\u5e94\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u7b80\u5355\u7684\u957f\u5ea6\u60e9\u7f5a\u4e0e\u7ed3\u679c\u5956\u52b1\u7ed3\u5408\uff0c\u96be\u4ee5\u5e73\u8861\u7b80\u6d01\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u5f3a\u5236\u7b80\u6d01\u53ef\u80fd\u635f\u5bb3\u5173\u952e\u63a8\u7406\u903b\u8f91\u3002\u5197\u4f59\u63a8\u7406\u5e26\u6765\u7684\u63a8\u7406\u5f00\u9500\u4f1a\u7834\u574f\u4ea4\u4e92\u4f53\u9a8c\u5e76\u4e25\u91cd\u963b\u788d\u5927\u63a8\u7406\u6a21\u578b\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6SCMA\uff0c\u5305\u542b\u4e09\u4e2a\u667a\u80fd\u4f53\uff1a\u5206\u5272\u667a\u80fd\u4f53\u5c06\u63a8\u7406\u8fc7\u7a0b\u5206\u89e3\u4e3a\u903b\u8f91\u5757\uff0c\u8bc4\u5206\u667a\u80fd\u4f53\u91cf\u5316\u6bcf\u4e2a\u5757\u7684\u91cd\u8981\u6027\uff0c\u63a8\u7406\u667a\u80fd\u4f53\u5728\u8bad\u7ec3\u4e2d\u57fa\u4e8e\u91cd\u8981\u6027\u52a0\u6743\u7684\u957f\u5ea6\u60e9\u7f5a\u8fdb\u884c\u4f18\u5316\uff0c\u90e8\u7f72\u65f6\u65e0\u9700\u989d\u5916\u63a8\u7406\u5f00\u9500\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSCMA\u5c06\u54cd\u5e94\u957f\u5ea6\u51cf\u5c11\u4e8611.1%\u523039.0%\uff0c\u540c\u65f6\u5c06\u51c6\u786e\u6027\u63d0\u9ad8\u4e864.33%\u523010.02%\u3002\u6d88\u878d\u7814\u7a76\u548c\u5b9a\u6027\u5206\u6790\u9a8c\u8bc1\u4e86MARL\u6846\u67b6\u5185\u7684\u534f\u540c\u4f18\u5316\u4ea7\u751f\u4e86\u6d8c\u73b0\u884c\u4e3a\uff0c\u76f8\u6bd4\u4f20\u7edfRL\u8303\u5f0f\u4ea7\u751f\u4e86\u66f4\u5f3a\u5927\u7684\u5927\u63a8\u7406\u6a21\u578b\u3002", "conclusion": "SCMA\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u9009\u62e9\u6027\u5197\u4f59\u60e9\u7f5a\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u7b80\u6d01\u6027\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u95ee\u9898\uff0c\u5728\u51cf\u5c11\u63a8\u7406\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21936", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21936", "abs": "https://arxiv.org/abs/2601.21936", "authors": ["Jon Chun", "Kathrine Elkins", "Yong Suk Lee"], "title": "AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making", "comment": "18 pages, 5 figures", "summary": "We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.", "AI": {"tldr": "AgenticSimLaw\u662f\u4e00\u4e2a\u57fa\u4e8e\u6cd5\u5ead\u8fa9\u8bba\u7ed3\u6784\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u98ce\u9669\u8868\u683c\u51b3\u7b56\u4efb\u52a1\uff0c\u63d0\u4f9b\u900f\u660e\u53ef\u63a7\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u5e74\u8f7b\u6210\u4eba\u518d\u72af\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u9ad8\u98ce\u9669\u51b3\u7b56\u4efb\u52a1\uff08\u5982\u5211\u4e8b\u53f8\u6cd5\uff09\u9700\u8981\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f20\u7edf\u9ed1\u76d2\u65b9\u6cd5\u7f3a\u4e4f\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u5b8c\u6574\u4ea4\u4e92\u8bb0\u5f55\u548c\u4eba\u7c7b\u76d1\u7763\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u6cd5\u5ead\u98ce\u683c\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u660e\u786e\u5b9a\u4e49\u89d2\u8272\uff08\u68c0\u5bdf\u5b98\u3001\u8fa9\u62a4\u5f8b\u5e08\u3001\u6cd5\u5b98\uff09\u30017\u8f6e\u7ed3\u6784\u5316\u8fa9\u8bba\u534f\u8bae\u548c\u79c1\u6709\u63a8\u7406\u7b56\u7565\uff0c\u5728NLSY97\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e74\u8f7b\u6210\u4eba\u518d\u72af\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u7ed3\u6784\u5316\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u63d0\u4f9b\u66f4\u7a33\u5b9a\u548c\u53ef\u6cdb\u5316\u7684\u6027\u80fd\uff0c\u51c6\u786e\u7387\u548cF1\u5206\u6570\u76f8\u5173\u6027\u66f4\u5f3a\uff0c\u540c\u65f6\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u63a8\u7406\u63a7\u5236\u548c\u5b8c\u6574\u4ea4\u4e92\u8bb0\u5f55\u7528\u4e8e\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "AgenticSimLaw\u901a\u8fc7\u7ed3\u6784\u5316\u89d2\u8272\u3001\u4ea4\u4e92\u65e5\u5fd7\u8bb0\u5f55\u548c\u660e\u786e\u90e8\u7f72\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u9700\u8981\u900f\u660e\u5ea6\u548c\u4eba\u7c7b\u76d1\u7763\u7684\u9ad8\u98ce\u9669\u51b3\u7b56\u4efb\u52a1\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.21947", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21947", "abs": "https://arxiv.org/abs/2601.21947", "authors": ["Bowen Fang", "Wen Ye", "Yunyue Su", "Jinghao Zhang", "Qiang Liu", "Yesheng Liu", "Xin Sun", "Shu Wu", "Jiabing Yang", "Baole Wei", "Liang Wang"], "title": "ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models", "comment": "10pages, 12 figures, Accepted to ICLR 2026", "summary": "Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.", "AI": {"tldr": "ToolWeaver\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5f0f\u5de5\u5177\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5de5\u5177\u7f16\u7801\u4e3a\u5206\u5c42\u5e8f\u5217\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u8bed\u4e49\u74f6\u9888\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u7684\u5de5\u5177\u4f7f\u7528\u7ba1\u9053\u9762\u4e34\u53cc\u91cd\u8bed\u4e49\u6311\u6218\uff1a\u68c0\u7d22\u5668\u7f16\u7801\u5668\u96be\u4ee5\u6355\u6349\u590d\u6742\u8bed\u4e49\uff0c\u800cLLM\u672c\u8eab\u7f3a\u4e4f\u5de5\u5177\u77e5\u8bc6\u3002\u751f\u6210\u5f0f\u65b9\u6cd5\u867d\u7136\u7edf\u4e00\u4e86\u9009\u62e9\u548c\u6267\u884c\uff0c\u4f46\u5c06\u6bcf\u4e2a\u5de5\u5177\u6620\u5c04\u5230\u552f\u4e00\u65b0token\u4f1a\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u5371\u673a\u548c\u8bed\u4e49\u74f6\u9888\uff0c\u963b\u788d\u5de5\u5177\u534f\u4f5c\u5173\u7cfb\u7684\u5efa\u6a21\u3002", "method": "\u63d0\u51faToolWeaver\u6846\u67b6\uff0c\u5c06\u5de5\u5177\u7f16\u7801\u4e3a\u5206\u5c42\u5e8f\u5217\uff0c\u4f7f\u8bcd\u6c47\u8868\u6269\u5c55\u4e0e\u5de5\u5177\u6570\u91cf\u5448\u5bf9\u6570\u5173\u7cfb\u3002\u901a\u8fc7\u65b0\u9896\u7684\u6807\u8bb0\u5316\u8fc7\u7a0b\u751f\u6210\u7ed3\u6784\u5316\u4ee3\u7801\uff0c\u5c06\u5de5\u5177\u5185\u5728\u8bed\u4e49\u4e0e\u5916\u90e8\u4f7f\u7528\u6a21\u5f0f\u7ed3\u5408\uff0c\u7136\u540e\u901a\u8fc7\u751f\u6210\u5bf9\u9f50\u9636\u6bb5\u5c06\u5206\u5c42\u4ee3\u7801\u5e8f\u5217\u96c6\u6210\u5230LLM\u4e2d\u3002", "result": "\u5728\u8fd147,000\u4e2a\u5de5\u5177\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cToolWeaver\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e3a\u9ad8\u7ea7\u5de5\u5177\u589e\u5f3a\u667a\u80fd\u4f53\u5efa\u7acb\u4e86\u66f4\u53ef\u6269\u5c55\u3001\u53ef\u6cdb\u5316\u548c\u8bed\u4e49\u611f\u77e5\u7684\u57fa\u7840\u3002", "conclusion": "ToolWeaver\u901a\u8fc7\u5206\u5c42\u5e8f\u5217\u7f16\u7801\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u5de5\u5177\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u8bed\u4e49\u74f6\u9888\u95ee\u9898\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ece\u5171\u4eab\u4ee3\u7801\u7684\u5bc6\u96c6\u5171\u73b0\u4e2d\u5b66\u4e60\u534f\u4f5c\u6a21\u5f0f\uff0c\u800c\u4e0d\u662f\u4ece\u5355\u4e00\u5de5\u5177ID\u7684\u7a00\u758f\u5171\u73b0\u4e2d\u63a8\u65ad\u3002", "topic": "agent analysis"}}
{"id": "2601.21619", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21619", "abs": "https://arxiv.org/abs/2601.21619", "authors": ["Yiming Wang", "Zhuosheng Zhang", "Rui Wang"], "title": "Breaking the Overscaling Curse: Thinking Parallelism Before Parallel Thinking", "comment": null, "summary": "Parallel thinking enhances LLM reasoning by multi-path sampling and aggregation. In system-level evaluations, a global parallelism level N is allocated to all samples, typically set large to maximize overall dataset accuracy. However, due to sample heterogeneity, some samples can achieve comparable performance with a smaller N'< N, causing budget redundancy. This incompatibility between system-level efficacy and sample-level efficiency constitutes the overscaling curse. In this paper, we formalize and quantify the overscaling curse, showing its universality and severity in practice, and analyze its trigger mechanism. We then propose a lightweight method, T2, to break the overscaling curse, which utilizes latent representations to estimate the optimal parallelism level for each sample before decoding. Experiments show that T2 significantly reduces cost while maintaining comparable performance, enabling more efficient parallel thinking.", "AI": {"tldr": "\u63d0\u51faT2\u65b9\u6cd5\u89e3\u51b3\u5e76\u884c\u601d\u7ef4\u4e2d\u7684\"\u8fc7\u5ea6\u7f29\u653e\u8bc5\u5492\"\uff0c\u901a\u8fc7\u6f5c\u5728\u8868\u793a\u4f30\u8ba1\u6bcf\u4e2a\u6837\u672c\u7684\u6700\u4f18\u5e76\u884c\u7ea7\u522b\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c", "motivation": "\u5f53\u524d\u5e76\u884c\u601d\u7ef4\u65b9\u6cd5\u4e3a\u6240\u6709\u6837\u672c\u5206\u914d\u76f8\u540c\u7684\u5168\u5c40\u5e76\u884c\u7ea7\u522bN\uff0c\u4f46\u6837\u672c\u5f02\u8d28\u6027\u5bfc\u81f4\u67d0\u4e9b\u6837\u672c\u53ef\u4ee5\u7528\u66f4\u5c0f\u7684N'\u8fbe\u5230\u76f8\u4f3c\u6027\u80fd\uff0c\u9020\u6210\u9884\u7b97\u5197\u4f59\uff0c\u8fd9\u79cd\u7cfb\u7edf\u7ea7\u6548\u7387\u4e0e\u6837\u672c\u7ea7\u6548\u7387\u7684\u4e0d\u517c\u5bb9\u6784\u6210\u4e86\"\u8fc7\u5ea6\u7f29\u653e\u8bc5\u5492\"", "method": "\u63d0\u51faT2\u65b9\u6cd5\uff0c\u5229\u7528\u6f5c\u5728\u8868\u793a\u5728\u89e3\u7801\u524d\u4f30\u8ba1\u6bcf\u4e2a\u6837\u672c\u7684\u6700\u4f18\u5e76\u884c\u7ea7\u522b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u673a\u5236\u4e3a\u4e0d\u540c\u6837\u672c\u5206\u914d\u9002\u5f53\u7684\u8ba1\u7b97\u8d44\u6e90", "result": "\u5b9e\u9a8c\u8868\u660eT2\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u8f83\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u5e76\u884c\u601d\u7ef4", "conclusion": "T2\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5e76\u884c\u601d\u7ef4\u4e2d\u7684\u8fc7\u5ea6\u7f29\u653e\u95ee\u9898\uff0c\u901a\u8fc7\u6837\u672c\u81ea\u9002\u5e94\u7684\u5e76\u884c\u7ea7\u522b\u5206\u914d\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387", "topic": "agent analysis"}}
{"id": "2601.21961", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21961", "abs": "https://arxiv.org/abs/2601.21961", "authors": ["Kuai Yu", "Naicheng Yu", "Han Wang", "Rui Yang", "Huan Zhang"], "title": "How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors", "comment": null, "summary": "Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.", "AI": {"tldr": "\u63d0\u51fa\u4e86VAF\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u7f51\u9875\u89c6\u89c9\u5c5e\u6027\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u667a\u80fd\u4f53\u51b3\u7b56\uff0c\u53d1\u73b0\u80cc\u666f\u5bf9\u6bd4\u5ea6\u3001\u9879\u76ee\u5927\u5c0f\u3001\u4f4d\u7f6e\u548c\u5361\u7247\u6e05\u6670\u5ea6\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u800c\u5bf9\u826f\u6027\u573a\u666f\u4e2d\u667a\u80fd\u4f53\u504f\u597d\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u89c6\u89c9\u5c5e\u6027\u5982\u4f55\u5f71\u54cd\u667a\u80fd\u4f53\u51b3\u7b56\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u6709\u9650\u3002", "method": "\u63d0\u51faVAF\u4e09\u9636\u6bb5\u8bc4\u4f30\u6d41\u7a0b\uff1a1) \u53d8\u4f53\u751f\u6210\uff08\u4fdd\u6301\u8bed\u4e49\u4e0d\u53d8\u4ec5\u6539\u53d8\u89c6\u89c9\u5c5e\u6027\uff09\uff1b2) \u6d4f\u89c8\u4ea4\u4e92\uff08\u6a21\u62df\u4eba\u7c7b\u6eda\u52a8\u70b9\u51fb\u884c\u4e3a\uff09\uff1b3) \u901a\u8fc7\u70b9\u51fb\u7387\u548c\u63d0\u53ca\u7387\u9a8c\u8bc1\uff0c\u91cf\u5316\u89c6\u89c9\u5c5e\u6027\u5f71\u54cd\u3002", "result": "\u57288\u4e2a\u53d8\u4f53\u5bb6\u65cf\uff0848\u4e2a\u53d8\u4f53\uff09\u30015\u4e2a\u771f\u5b9e\u7f51\u7ad9\u548c4\u4e2a\u4ee3\u8868\u6027\u667a\u80fd\u4f53\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a\u80cc\u666f\u989c\u8272\u5bf9\u6bd4\u5ea6\u3001\u9879\u76ee\u5927\u5c0f\u3001\u4f4d\u7f6e\u548c\u5361\u7247\u6e05\u6670\u5ea6\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u5f71\u54cd\u663e\u8457\uff0c\u800c\u5b57\u4f53\u6837\u5f0f\u3001\u6587\u672c\u989c\u8272\u548c\u9879\u76ee\u56fe\u50cf\u6e05\u6670\u5ea6\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "\u89c6\u89c9\u5c5e\u6027\u786e\u5b9e\u4f1a\u5f71\u54cd\u667a\u80fd\u4f53\u51b3\u7b56\uff0c\u67d0\u4e9b\u5c5e\u6027\uff08\u5982\u80cc\u666f\u5bf9\u6bd4\u5ea6\u3001\u9879\u76ee\u5927\u5c0f\uff09\u5f71\u54cd\u663e\u8457\uff0c\u8fd9\u4e3a\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2601.21972", "categories": ["cs.AI", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.21972", "abs": "https://arxiv.org/abs/2601.21972", "authors": ["Shuo Liu", "Tianle Chen", "Ryan Amiri", "Christopher Amato"], "title": "Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic", "comment": null, "summary": "Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \\textbf{CoLLM-CC} with a \\textbf{C}entralized \\textbf{C}ritic and \\textbf{CoLLM-DC} with \\textbf{D}ecentralized \\textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u591a\u667a\u80fd\u4f53\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\uff08CoLLM-CC\u548cCoLLM-DC\uff09\u6765\u4f18\u5316\u53bb\u4e2d\u5fc3\u5316LLM\u534f\u4f5c\uff0c\u76f8\u6bd4\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u5728\u957f\u89c6\u91ce\u6216\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709MARL\u5fae\u8c03\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u6267\u884c\u534f\u8bae\u4e14\u9700\u8981\u96c6\u4e2d\u6267\u884c\uff0c\u800c\u53bb\u4e2d\u5fc3\u5316LLM\u534f\u4f5c\u5728\u5b9e\u9645\u4e2d\u66f4\u5177\u5438\u5f15\u529b\u3002\u5f53\u524d\u65b9\u6cd5\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u5b58\u5728\u9ad8\u65b9\u5dee\u95ee\u9898\uff0c\u9700\u8981\u66f4\u591a\u8bad\u7ec3\u6837\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u591a\u667a\u80fd\u4f53\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\uff1aCoLLM-CC\uff08\u96c6\u4e2d\u5f0f\u8bc4\u8bba\u5bb6\uff09\u548cCoLLM-DC\uff08\u53bb\u4e2d\u5fc3\u5316\u8bc4\u8bba\u5bb6\uff09\uff0c\u7528\u4e8e\u4f18\u5316\u53bb\u4e2d\u5fc3\u5316LLM\u534f\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u77ed\u89c6\u91ce\u548c\u5bc6\u96c6\u5956\u52b1\u4efb\u52a1\u4e2d\uff0c\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u548cCoLLM-DC\u80fd\u8fbe\u5230\u4e0eCoLLM-CC\u76f8\u5f53\u7684\u6027\u80fd\u3002\u4f46\u5728\u957f\u89c6\u91ce\u6216\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\uff0c\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u9700\u8981\u66f4\u591a\u6837\u672c\uff0cCoLLM-DC\u96be\u4ee5\u6536\u655b\uff0c\u800cCoLLM-CC\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u96c6\u4e2d\u5f0f\u8bc4\u8bba\u5bb6\u7684MAAC\u65b9\u6cd5\uff08CoLLM-CC\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u957f\u89c6\u91ce\u6216\u7a00\u758f\u5956\u52b1\u573a\u666f\u4e0b\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316LLM\u534f\u4f5c\u7684\u4f18\u5316\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22001", "categories": ["cs.AI", "cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.22001", "abs": "https://arxiv.org/abs/2601.22001", "authors": ["Yiren Zhao", "Junyi Liu"], "title": "Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference", "comment": null, "summary": "AI agent inference is driving an inference heavy datacenter future and exposes bottlenecks beyond compute - especially memory capacity, memory bandwidth and high-speed interconnect. We introduce two metrics - Operational Intensity (OI) and Capacity Footprint (CF) - that jointly explain regimes the classic roofline analysis misses, including the memory capacity wall. Across agentic workflows (chat, coding, web use, computer use) and base model choices (GQA/MLA, MoE, quantization), OI/CF can shift dramatically, with long context KV cache making decode highly memory bound. These observations motivate disaggregated serving and system level heterogeneity: specialized prefill and decode accelerators, broader scale up networking, and decoupled compute-memory enabled by optical I/O. We further hypothesize agent-hardware co design, multiple inference accelerators within one system, and high bandwidth, large capacity memory disaggregation as foundations for adaptation to evolving OI/CF. Together, these directions chart a path to sustain efficiency and capability for large scale agentic AI inference.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86AI\u667a\u80fd\u4f53\u63a8\u7406\u7684\u5185\u5b58\u74f6\u9888\uff0c\u63d0\u51fa\u4e86\u64cd\u4f5c\u5f3a\u5ea6(OI)\u548c\u5bb9\u91cf\u5360\u7528(CF)\u4e24\u4e2a\u65b0\u6307\u6807\u6765\u8865\u5145\u4f20\u7edf\u5c4b\u9876\u7ebf\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5185\u5b58\u5bb9\u91cf\u5899\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u8026\u670d\u52a1\u3001\u5f02\u6784\u7cfb\u7edf\u3001\u667a\u80fd\u4f53-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7b49\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "AI\u667a\u80fd\u4f53\u63a8\u7406\u6b63\u5728\u63a8\u52a8\u6570\u636e\u4e2d\u5fc3\u5411\u63a8\u7406\u5bc6\u96c6\u578b\u53d1\u5c55\uff0c\u66b4\u9732\u51fa\u8d85\u8d8a\u8ba1\u7b97\u80fd\u529b\u4e4b\u5916\u7684\u74f6\u9888\uff0c\u7279\u522b\u662f\u5185\u5b58\u5bb9\u91cf\u3001\u5185\u5b58\u5e26\u5bbd\u548c\u9ad8\u901f\u4e92\u8fde\u3002\u4f20\u7edf\u5c4b\u9876\u7ebf\u5206\u6790\u65e0\u6cd5\u6355\u6349\u8fd9\u4e9b\u5185\u5b58\u76f8\u5173\u7684\u74f6\u9888\u3002", "method": "\u5f15\u5165\u4e24\u4e2a\u65b0\u6307\u6807\uff1a\u64cd\u4f5c\u5f3a\u5ea6(OI)\u548c\u5bb9\u91cf\u5360\u7528(CF)\uff0c\u7528\u4e8e\u5206\u6790\u4e0d\u540c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff08\u804a\u5929\u3001\u7f16\u7801\u3001\u7f51\u9875\u4f7f\u7528\u3001\u8ba1\u7b97\u673a\u4f7f\u7528\uff09\u548c\u57fa\u7840\u6a21\u578b\u9009\u62e9\uff08GQA/MLA\u3001MoE\u3001\u91cf\u5316\uff09\u4e0b\u7684\u6027\u80fd\u74f6\u9888\u3002\u7279\u522b\u5173\u6ce8\u957f\u4e0a\u4e0b\u6587KV\u7f13\u5b58\u5bfc\u81f4\u89e3\u7801\u9636\u6bb5\u9ad8\u5ea6\u5185\u5b58\u53d7\u9650\u7684\u95ee\u9898\u3002", "result": "OI\u548cCF\u6307\u6807\u80fd\u591f\u89e3\u91ca\u4f20\u7edf\u5c4b\u9876\u7ebf\u5206\u6790\u9057\u6f0f\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5305\u62ec\u5185\u5b58\u5bb9\u91cf\u5899\u3002\u5728\u4e0d\u540c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u548c\u6a21\u578b\u914d\u7f6e\u4e0b\uff0cOI/CF\u4f1a\u53d1\u751f\u663e\u8457\u53d8\u5316\uff0c\u957f\u4e0a\u4e0b\u6587KV\u7f13\u5b58\u4f7f\u89e3\u7801\u9636\u6bb5\u9ad8\u5ea6\u5185\u5b58\u53d7\u9650\u3002", "conclusion": "\u9700\u8981\u89e3\u8026\u670d\u52a1\u67b6\u6784\u548c\u7cfb\u7edf\u7ea7\u5f02\u6784\u8bbe\u8ba1\uff1a\u4e13\u7528\u9884\u586b\u5145\u548c\u89e3\u7801\u52a0\u901f\u5668\u3001\u66f4\u5927\u89c4\u6a21\u7684\u7f51\u7edc\u6269\u5c55\u3001\u901a\u8fc7\u5149I/O\u5b9e\u73b0\u7684\u89e3\u8026\u8ba1\u7b97-\u5185\u5b58\u67b6\u6784\u3002\u63d0\u51fa\u667a\u80fd\u4f53-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u3001\u5355\u7cfb\u7edf\u5185\u591a\u63a8\u7406\u52a0\u901f\u5668\u3001\u9ad8\u5e26\u5bbd\u5927\u5bb9\u91cf\u5185\u5b58\u89e3\u8026\u4f5c\u4e3a\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684OI/CF\u7684\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.21523", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21523", "abs": "https://arxiv.org/abs/2601.21523", "authors": ["Bang Giang Le", "Viet Cuong Ta"], "title": "Explicit Credit Assignment through Local Rewards and Dependence Graphs in Multi-Agent Reinforcement Learning", "comment": null, "summary": "To promote cooperation in Multi-Agent Reinforcement Learning, the reward signals of all agents can be aggregated together, forming global rewards that are commonly known as the fully cooperative setting. However, global rewards are usually noisy because they contain the contributions of all agents, which have to be resolved in the credit assignment process. On the other hand, using local reward benefits from faster learning due to the separation of agents' contributions, but can be suboptimal as agents myopically optimize their own reward while disregarding the global optimality. In this work, we propose a method that combines the merits of both approaches. By using a graph of interaction between agents, our method discerns the individual agent contribution in a more fine-grained manner than a global reward, while alleviating the cooperation problem with agents' local reward. We also introduce a practical approach for approximating such a graph. Our experiments demonstrate the flexibility of the approach, enabling improvements over the traditional local and global reward settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5168\u5c40\u5956\u52b1\u548c\u5c40\u90e8\u5956\u52b1\u4f18\u52bf\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u4e92\u56fe\u66f4\u7cbe\u7ec6\u5730\u8bc6\u522b\u4e2a\u4f53\u8d21\u732e\uff0c\u7f13\u89e3\u5c40\u90e8\u5956\u52b1\u7684\u5408\u4f5c\u95ee\u9898\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5168\u5c40\u5956\u52b1\u5305\u542b\u6240\u6709\u667a\u80fd\u4f53\u8d21\u732e\uff0c\u5b58\u5728\u566a\u58f0\u548c\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff1b\u5c40\u90e8\u5956\u52b1\u867d\u7136\u5b66\u4e60\u66f4\u5feb\u4f46\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u89e3\uff0c\u56e0\u4e3a\u667a\u80fd\u4f53\u53ef\u80fd\u77ed\u89c6\u5730\u4f18\u5316\u81ea\u8eab\u5956\u52b1\u800c\u5ffd\u89c6\u5168\u5c40\u6700\u4f18\u6027\u3002", "method": "\u901a\u8fc7\u667a\u80fd\u4f53\u95f4\u7684\u4ea4\u4e92\u56fe\uff0c\u66f4\u7cbe\u7ec6\u5730\u8bc6\u522b\u4e2a\u4f53\u8d21\u732e\uff0c\u540c\u65f6\u7f13\u89e3\u5c40\u90e8\u5956\u52b1\u7684\u5408\u4f5c\u95ee\u9898\u3002\u8fd8\u63d0\u51fa\u4e86\u8fd1\u4f3c\u6784\u5efa\u8fd9\u79cd\u4ea4\u4e92\u56fe\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u8d85\u8d8a\u4f20\u7edf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u5956\u52b1\u8bbe\u7f6e\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u5168\u5c40\u5956\u52b1\u548c\u5c40\u90e8\u5956\u52b1\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u4ea4\u4e92\u56fe\u673a\u5236\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5408\u4f5c\u6548\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22027", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22027", "abs": "https://arxiv.org/abs/2601.22027", "authors": ["Johannes Kirmayr", "Lukas Stappen", "Elisabeth Andr\u00e9"], "title": "CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty", "comment": null, "summary": "Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.", "AI": {"tldr": "CAR-bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8f66\u8f7d\u8bed\u97f3\u52a9\u624b\u573a\u666f\u4e0bLLM\u4ee3\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u5173\u6ce8\u4e00\u81f4\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u548c\u80fd\u529b\u8ba4\u77e5\uff0c\u5305\u542b\u5e7b\u89c9\u4efb\u52a1\u548c\u6d88\u6b67\u4efb\u52a1\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u7406\u60f3\u73af\u5883\u4e0b\u7684\u4efb\u52a1\u5b8c\u6210\u5ea6\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u7528\u6237\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002\u5728\u8f66\u8f7d\u8bed\u97f3\u52a9\u624b\u7b49\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u7ecf\u5e38\u63d0\u51fa\u4e0d\u5b8c\u6574\u6216\u6a21\u7cca\u7684\u8bf7\u6c42\uff0c\u4ee3\u7406\u9700\u8981\u901a\u8fc7\u5bf9\u8bdd\u3001\u5de5\u5177\u4f7f\u7528\u548c\u7b56\u7565\u9075\u5b88\u6765\u7ba1\u7406\u8fd9\u79cd\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5f15\u5165CAR-bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542bLLM\u6a21\u62df\u7528\u6237\u3001\u9886\u57df\u7b56\u7565\u548c58\u4e2a\u4e92\u8fde\u5de5\u5177\uff08\u5bfc\u822a\u3001\u751f\u4ea7\u529b\u3001\u5145\u7535\u3001\u8f66\u8f86\u63a7\u5236\uff09\u3002\u9664\u4e86\u6807\u51c6\u4efb\u52a1\u5b8c\u6210\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u5e7b\u89c9\u4efb\u52a1\uff08\u6d4b\u8bd5\u5728\u5de5\u5177\u6216\u4fe1\u606f\u7f3a\u5931\u65f6\u7684\u9650\u5236\u8ba4\u77e5\uff09\u548c\u6d88\u6b67\u4efb\u52a1\uff08\u8981\u6c42\u901a\u8fc7\u6f84\u6e05\u6216\u5185\u90e8\u4fe1\u606f\u6536\u96c6\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\uff09\u3002", "result": "\u57fa\u7ebf\u7ed3\u679c\u663e\u793a\uff0c\u6240\u6709\u4efb\u52a1\u7c7b\u578b\u4e0a\u90fd\u5b58\u5728\u5076\u5c14\u6210\u529f\u4e0e\u6301\u7eed\u6210\u529f\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u8ddd\u3002\u5373\u4f7f\u662f\u524d\u6cbf\u63a8\u7406LLM\u5728\u6d88\u6b67\u4efb\u52a1\u4e0a\u7684\u6301\u7eed\u901a\u8fc7\u7387\u4e5f\u4e0d\u523050%\uff08\u7531\u4e8e\u8fc7\u65e9\u884c\u52a8\uff09\uff0c\u5728\u5e7b\u89c9\u4efb\u52a1\u4e2d\u7ecf\u5e38\u8fdd\u53cd\u7b56\u7565\u6216\u7f16\u9020\u4fe1\u606f\u4ee5\u6ee1\u8db3\u7528\u6237\u8bf7\u6c42\u3002", "conclusion": "\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u9700\u8981\u66f4\u53ef\u9760\u548c\u81ea\u77e5\u7684LLM\u4ee3\u7406\uff0c\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u81ea\u8eab\u9650\u5236\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0cCAR-bench\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u8fd9\u4e9b\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2601.22037", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22037", "abs": "https://arxiv.org/abs/2601.22037", "authors": ["Sami Abuzakuk", "Anne-Marie Kermarrec", "Rishi Sharma", "Rasmus Moorits Veski", "Martijn de Vos"], "title": "Optimizing Agentic Workflows using Meta-tools", "comment": null, "summary": "Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.", "AI": {"tldr": "AWO\u6846\u67b6\u901a\u8fc7\u5206\u6790\u5de5\u4f5c\u6d41\u8f68\u8ff9\uff0c\u8bc6\u522b\u5197\u4f59\u5de5\u5177\u6267\u884c\u6a21\u5f0f\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u5143\u5de5\u5177\uff0c\u51cf\u5c11LLM\u8c03\u7528\u6b21\u6570\u5e76\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4ee3\u7406AI\u5de5\u4f5c\u6d41\u901a\u5e38\u9700\u8981\u591a\u6b21\u8fed\u4ee3\u63a8\u7406\u548c\u5de5\u5177\u8c03\u7528\uff0c\u5bfc\u81f4\u64cd\u4f5c\u6210\u672c\u9ad8\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\u5927\u4ee5\u53ca\u56e0\u5e7b\u89c9\u5bfc\u81f4\u7684\u5931\u8d25\u3002\u9700\u8981\u4f18\u5316\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "AWO\u6846\u67b6\u5206\u6790\u73b0\u6709\u5de5\u4f5c\u6d41\u8f68\u8ff9\uff0c\u53d1\u73b0\u91cd\u590d\u7684\u5de5\u5177\u8c03\u7528\u5e8f\u5217\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u5143\u5de5\u5177\uff08\u786e\u5b9a\u6027\u590d\u5408\u5de5\u5177\uff09\uff0c\u5c06\u591a\u4e2a\u4ee3\u7406\u52a8\u4f5c\u6346\u7ed1\u5230\u5355\u4e2a\u8c03\u7528\u4e2d\uff0c\u7ed5\u8fc7\u4e0d\u5fc5\u8981\u7684\u4e2d\u95f4LLM\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u4e24\u4e2a\u4ee3\u7406AI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAWO\u5c06LLM\u8c03\u7528\u6b21\u6570\u51cf\u5c11\u9ad8\u8fbe11.9%\uff0c\u540c\u65f6\u5c06\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u9ad8\u8fbe4.2\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "AWO\u901a\u8fc7\u4f18\u5316\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u7684\u5197\u4f59\u5de5\u5177\u6267\u884c\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7406AI\u7cfb\u7edf\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.22154", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.22154", "abs": "https://arxiv.org/abs/2601.22154", "authors": ["Kaixuan Fan", "Kaituo Feng", "Manyuan Zhang", "Tianshuo Peng", "Zhixun Li", "Yilei Jiang", "Shuang Chen", "Peng Pei", "Xunliang Cai", "Xiangyu Yue"], "title": "Exploring Reasoning Reward Model for Agents", "comment": "Project page: https://github.com/kxfan2002/Reagent", "summary": "Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAgent-RRM\uff08\u667a\u80fd\u4f53\u63a8\u7406\u5956\u52b1\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u63d0\u4f9b\u7ed3\u6784\u5316\u53cd\u9988\uff08\u63a8\u7406\u8f68\u8ff9\u3001\u9488\u5bf9\u6027\u6279\u5224\u3001\u603b\u4f53\u8bc4\u5206\uff09\u6765\u6539\u8fdb\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7a00\u758f\u5956\u52b1\u65e0\u6cd5\u533a\u5206\u4e2d\u95f4\u63a8\u7406\u8d28\u91cf\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u79cd\u53cd\u9988\u65e0\u6cd5\u533a\u5206\u4e2d\u95f4\u63a8\u7406\u8fc7\u7a0b\u7684\u8d28\u91cf\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6548\u679c\u4e0d\u4f73\u3002\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u53cd\u9988\u673a\u5236\u6765\u6307\u5bfc\u667a\u80fd\u4f53\u5b66\u4e60\u3002", "method": "\u63d0\u51faAgent-RRM\u591a\u9762\u5956\u52b1\u6a21\u578b\uff0c\u751f\u6210\u7ed3\u6784\u5316\u53cd\u9988\uff1a1\uff09\u663e\u5f0f\u63a8\u7406\u8f68\u8ff9\uff0c2\uff09\u9488\u5bf9\u6027\u6279\u5224\uff08\u7a81\u51fa\u63a8\u7406\u7f3a\u9677\uff09\uff0c3\uff09\u603b\u4f53\u8bc4\u5206\u3002\u57fa\u4e8e\u8fd9\u4e9b\u4fe1\u53f7\uff0c\u7814\u7a76\u4e86\u4e09\u79cd\u96c6\u6210\u7b56\u7565\uff1aReagent-C\uff08\u6587\u672c\u589e\u5f3a\u7ec6\u5316\uff09\u3001Reagent-R\uff08\u5956\u52b1\u589e\u5f3a\u6307\u5bfc\uff09\u548cReagent-U\uff08\u7edf\u4e00\u53cd\u9988\u96c6\u6210\uff09\u3002", "result": "\u572812\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReagent-U\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff1a\u5728GAIA\u4e0a\u8fbe\u523043.7%\uff0c\u5728WebWalkerQA\u4e0a\u8fbe\u523046.2%\uff0c\u9a8c\u8bc1\u4e86\u63a8\u7406\u5956\u52b1\u6a21\u578b\u548c\u8bad\u7ec3\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "Agent-RRM\u901a\u8fc7\u63d0\u4f9b\u7ed3\u6784\u5316\u53cd\u9988\u663e\u8457\u6539\u8fdb\u4e86\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u679c\uff0cReagent-U\u96c6\u6210\u7b56\u7565\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u667a\u80fd\u4f53\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21590", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21590", "abs": "https://arxiv.org/abs/2601.21590", "authors": ["Xiaotong Ji", "Rasul Tutunov", "Matthieu Zimmer", "Haitham Bou Ammar"], "title": "Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening", "comment": null, "summary": "Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u5956\u52b1\u7684\u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u5c06\u5168\u5c40\u5e42\u5206\u5e03\u8fd1\u4f3c\u4e3atoken\u7ea7\u7f29\u653e\u4f4e\u6e29\u5206\u5e03\uff0c\u5728\u6570\u5b66\u3001QA\u548c\u4ee3\u7801\u4efb\u52a1\u4e0a\u5339\u914d\u6216\u8d85\u8d8aGRPO\uff0c\u63a8\u7406\u5ef6\u8fdf\u6bd4MCMC\u964d\u4f4e10\u500d\u4ee5\u4e0a\u3002", "motivation": "RL\u540e\u8bad\u7ec3\u867d\u7136\u80fd\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u8bc1\u636e\u8868\u660e\u5176\u589e\u76ca\u4e3b\u8981\u6765\u81ea\u5206\u5e03\u9510\u5316\u800c\u975e\u65b0\u80fd\u529b\u83b7\u53d6\u3002MCMC\u91c7\u6837\u65b9\u6cd5\u867d\u80fd\u83b7\u5f97\u7c7b\u4f3c\u6548\u679c\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u96be\u4ee5\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u7406\u8bba\u63a8\u5bfc\uff0c\u8bc1\u660e\u5168\u5c40\u5e42\u5206\u5e03\u53ef\u8fd1\u4f3c\u4e3atoken\u7ea7\u7f29\u653e\u4f4e\u6e29\u5206\u5e03\uff0c\u5176\u4e2d\u7f29\u653e\u56e0\u5b50\u6355\u83b7\u672a\u6765\u8f68\u8ff9\u8d28\u91cf\u3002\u57fa\u4e8e\u6b64\u5f15\u5165\u8bad\u7ec3\u514d\u8d39\u3001\u9a8c\u8bc1\u5668\u514d\u8d39\u7684\u7b97\u6cd5\uff0c\u81ea\u56de\u5f52\u5730\u9510\u5316\u57fa\u7840\u6a21\u578b\u7684\u751f\u6210\u5206\u5e03\u3002", "result": "\u5728\u6570\u5b66\u3001QA\u548c\u4ee3\u7801\u4efb\u52a1\u4e0a\u8bc4\u4f30\u56db\u4e2aLLM\uff0c\u8be5\u65b9\u6cd5\u5339\u914d\u6216\u8d85\u8d8a\u4e00\u6b21\u6027GRPO\uff08\u65e0\u9700\u5916\u90e8\u5956\u52b1\uff09\uff0c\u540c\u65f6\u63a8\u7406\u5ef6\u8fdf\u6bd4\u57fa\u4e8eMCMC\u7684\u91c7\u6837\u964d\u4f4e10\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u65e0\u9700\u8fed\u4ee3MCMC\u6216\u5916\u90e8\u5956\u52b1\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u5b9e\u73b0\u5206\u5e03\u9510\u5316\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21669", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21669", "abs": "https://arxiv.org/abs/2601.21669", "authors": ["Abhijeet Sinha", "Sundari Elango", "Dianbo Liu"], "title": "Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling", "comment": null, "summary": "Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7684\u671f\u671b\u56de\u62a5\u6700\u5927\u5316\u76ee\u6807\u4f1a\u5bfc\u81f4\u7ed3\u679c\u5c42\u9762\u7684\u6a21\u5f0f\u5d29\u6e83\uff0c\u5e76\u63d0\u51fa\u9006\u6982\u7387\u7f29\u653e\u4fee\u6b63\u65b9\u6cd5IPS-GRPO\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898", "motivation": "\u8bb8\u591a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u5b58\u5728\u591a\u4e2a\u8d28\u91cf\u76f8\u5f53\u7684\u6700\u4f18\u89e3\uff0c\u4f46\u6807\u51c6\u671f\u671b\u56de\u62a5\u6700\u5927\u5316\u8bad\u7ec3\u7684\u7b56\u7565\u901a\u5e38\u4f1a\u5d29\u6e83\u5230\u5c11\u6570\u7ed3\u679c\u4e0a\u3002\u4f20\u7edf\u89e3\u91ca\u5f52\u56e0\u4e8e\u63a2\u7d22\u4e0d\u8db3\u6216\u6b63\u5219\u5316\u4e0d\u591f\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u671f\u671b\u56de\u62a5\u76ee\u6807\u672c\u8eab\u7684\u7ed3\u6784\u6027\u7f3a\u9677", "method": "\u63d0\u51fa\u9006\u6982\u7387\u7f29\u653e(IPS)\u4fee\u6b63\u65b9\u6cd5\uff0c\u79fb\u9664\u5b66\u4e60\u4fe1\u53f7\u4e2d\u7684\u7ed3\u679c\u9891\u7387\u653e\u5927\u6548\u5e94\u3002\u5177\u4f53\u5b9e\u73b0\u4e3aIPS-GRPO\uff0c\u4f5c\u4e3aGroup Relative Policy Optimization\u7684\u5373\u63d2\u5373\u7528\u4fee\u6539\uff0c\u65e0\u9700\u8f85\u52a9\u6a21\u578b\u6216\u67b6\u6784\u6539\u53d8", "result": "\u5728\u63a8\u7406\u548c\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\uff0cIPS-GRPO\u80fd\u6301\u7eed\u51cf\u5c11\u7ed3\u679c\u5c42\u9762\u7684\u6a21\u5f0f\u5d29\u6e83\uff0c\u540c\u65f6\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u7ebf\u6027\u80fd", "conclusion": "\u4fee\u6b63\u76ee\u6807\u51fd\u6570\u800c\u975e\u6dfb\u52a0\u63a2\u7d22\u542f\u53d1\u5f0f\u65b9\u6cd5\u662f\u5b9e\u73b0\u53ef\u9760\u591a\u6a21\u6001\u7b56\u7565\u4f18\u5316\u7684\u5173\u952e\u3002\u9006\u6982\u7387\u7f29\u653e\u80fd\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u5b66\u4e60\u52a8\u6001\uff0c\u4ea7\u751f\u4e0e\u5956\u52b1\u6210\u6bd4\u4f8b\u7684\u7ec8\u7aef\u5206\u5e03", "topic": "agentic reinforcement learning"}}
{"id": "2601.21816", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21816", "abs": "https://arxiv.org/abs/2601.21816", "authors": ["Dennis Frauen", "Athiya Deviyani", "Mihaela van der Schaar", "Stefan Feuerriegel"], "title": "Nonparametric LLM Evaluation from Preference Data", "comment": null, "summary": "Evaluating the performance of large language models (LLMs) from human preference data is crucial for obtaining LLM leaderboards. However, many existing approaches either rely on restrictive parametric assumptions or lack valid uncertainty quantification when flexible machine learning methods are used. In this paper, we propose a nonparametric statistical framework, DMLEval, for comparing and ranking LLMs from preference data using debiased machine learning (DML). For this, we introduce generalized average ranking scores (GARS), which generalize commonly used ranking models, including the Bradley-Terry model or PageRank/ Rank centrality, with complex human responses such as ties. DMLEval comes with the following advantages: (i) It produces statistically efficient estimates of GARS ranking scores. (ii) It naturally allows the incorporation of black-box machine learning methods for estimation. (iii) It can be combined with pre-trained LLM evaluators (e.g., using LLM-as-a-judge). (iv) It suggests optimal policies for collecting preference data under budget constraints. We demonstrate these advantages both theoretically and empirically using both synthetic and real-world preference datasets. In summary, our framework provides practitioners with powerful, state-of-the-art methods for comparing or ranking LLMs.", "AI": {"tldr": "\u63d0\u51faDMLEval\u975e\u53c2\u6570\u7edf\u8ba1\u6846\u67b6\uff0c\u4f7f\u7528\u53bb\u504f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4ece\u504f\u597d\u6570\u636e\u4e2d\u6bd4\u8f83\u548c\u6392\u540d\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5f15\u5165\u5e7f\u4e49\u5e73\u5747\u6392\u540d\u5206\u6570\uff0c\u652f\u6301\u590d\u6742\u4eba\u7c7b\u54cd\u5e94\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9650\u5236\u6027\u53c2\u6570\u5047\u8bbe\uff0c\u8981\u4e48\u5728\u4f7f\u7528\u7075\u6d3b\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u65f6\u7f3a\u4e4f\u6709\u6548\u7684\u7edf\u8ba1\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u9700\u8981\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faDMLEval\u975e\u53c2\u6570\u6846\u67b6\uff0c\u5f15\u5165\u5e7f\u4e49\u5e73\u5747\u6392\u540d\u5206\u6570\uff08GARS\uff09\uff0c\u4f7f\u7528\u53bb\u504f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f30\u8ba1\u6392\u540d\u5206\u6570\uff0c\u652f\u6301\u590d\u6742\u4eba\u7c7b\u54cd\u5e94\uff08\u5982\u5e73\u5c40\uff09\uff0c\u53ef\u7ed3\u5408\u9884\u8bad\u7ec3LLM\u8bc4\u4f30\u5668\u3002", "result": "DMLEval\u80fd\u4ea7\u751f\u7edf\u8ba1\u9ad8\u6548\u7684GARS\u6392\u540d\u5206\u6570\u4f30\u8ba1\uff0c\u652f\u6301\u9ed1\u76d2\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u7ed3\u5408LLM-as-a-judge\u8bc4\u4f30\u5668\uff0c\u5e76\u80fd\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u4f18\u5316\u504f\u597d\u6570\u636e\u6536\u96c6\u7b56\u7565\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5f3a\u5927\u3001\u5148\u8fdb\u7684\u6bd4\u8f83\u548c\u6392\u540d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "topic": "agent analysis"}}
{"id": "2601.21845", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21845", "abs": "https://arxiv.org/abs/2601.21845", "authors": ["Tingting Ni", "Maryam Kamgarpour"], "title": "Constrained Meta Reinforcement Learning with Provable Test-Time Safety", "comment": null, "summary": "Meta reinforcement learning (RL) allows agents to leverage experience across a distribution of tasks on which the agent can train at will, enabling faster learning of optimal policies on new test tasks. Despite its success in improving sample complexity on test tasks, many real-world applications, such as robotics and healthcare, impose safety constraints during testing. Constrained meta RL provides a promising framework for integrating safety into meta RL. An open question in constrained meta RL is how to ensure the safety of the policy on the real-world test task, while reducing the sample complexity and thus, enabling faster learning of optimal policies. To address this gap, we propose an algorithm that refines policies learned during training, with provable safety and sample complexity guarantees for learning a near optimal policy on the test tasks. We further derive a matching lower bound, showing that this sample complexity is tight.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5177\u6709\u53ef\u8bc1\u660e\u5b89\u5168\u6027\u548c\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u7684\u7ea6\u675f\u5143\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u5feb\u901f\u5b66\u4e60\u63a5\u8fd1\u6700\u4f18\u7b56\u7565", "motivation": "\u5143\u5f3a\u5316\u5b66\u4e60\u80fd\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u4f46\u73b0\u5b9e\u5e94\u7528\uff08\u5982\u673a\u5668\u4eba\u3001\u533b\u7597\uff09\u9700\u8981\u5728\u6d4b\u8bd5\u65f6\u4fdd\u8bc1\u5b89\u5168\u6027\u3002\u7ea6\u675f\u5143\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u867d\u80fd\u6574\u5408\u5b89\u5168\u6027\uff0c\u4f46\u5982\u4f55\u5728\u5b9e\u9645\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u786e\u4fdd\u7b56\u7565\u5b89\u5168\u540c\u65f6\u51cf\u5c11\u6837\u672c\u590d\u6742\u5ea6\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898", "method": "\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u7cbe\u70bc\u5b66\u4e60\u5230\u7684\u7b56\u7565\uff0c\u4e3a\u6d4b\u8bd5\u4efb\u52a1\u5b66\u4e60\u63a5\u8fd1\u6700\u4f18\u7b56\u7565\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u6027\u548c\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1", "result": "\u7b97\u6cd5\u5177\u6709\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u6027\u548c\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\uff0c\u5e76\u63a8\u5bfc\u51fa\u5339\u914d\u7684\u4e0b\u754c\uff0c\u8868\u660e\u8be5\u6837\u672c\u590d\u6742\u5ea6\u662f\u7d27\u81f4\u7684", "conclusion": "\u8be5\u7b97\u6cd5\u89e3\u51b3\u4e86\u7ea6\u675f\u5143\u5f3a\u5316\u5b66\u4e60\u4e2d\u6d4b\u8bd5\u4efb\u52a1\u5b89\u5168\u6027\u548c\u6837\u672c\u6548\u7387\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840", "topic": "agentic reinforcement learning"}}
{"id": "2601.21847", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.21847", "abs": "https://arxiv.org/abs/2601.21847", "authors": ["Zechuan Huang", "Zhiguang Cao", "Hongshu Guo", "Yue-Jiao Gong", "Zeyuan Ma"], "title": "READY: Reward Discovery for Meta-Black-Box Optimization", "comment": null, "summary": "Meta-Black-Box Optimization (MetaBBO) is an emerging avenue within Optimization community, where algorithm design policy could be meta-learned by reinforcement learning to enhance optimization performance. So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking. In this paper, we use Large Language Model~(LLM) as an automated reward discovery tool for MetaBBO. Specifically, we consider both effectiveness and efficiency sides. On effectiveness side, we borrow the idea of evolution of heuristics, introducing tailored evolution paradigm in the iterative LLM-based program search process, which ensures continuous improvement. On efficiency side, we additionally introduce multi-task evolution architecture to support parallel reward discovery for diverse MetaBBO approaches. Such parallel process also benefits from knowledge sharing across tasks to accelerate convergence. Empirical results demonstrate that the reward functions discovered by our approach could be helpful for boosting existing MetaBBO works, underscoring the importance of reward design in MetaBBO. We provide READY's project at https://anonymous.4open.science/r/ICML_READY-747F.", "AI": {"tldr": "\u4f7f\u7528LLM\u4f5c\u4e3a\u81ea\u52a8\u5956\u52b1\u53d1\u73b0\u5de5\u5177\uff0c\u901a\u8fc7\u8fdb\u5316\u8303\u5f0f\u548c\u591a\u4efb\u52a1\u67b6\u6784\u4e3aMeta-BBO\u53d1\u73b0\u66f4\u4f18\u7684\u5956\u52b1\u51fd\u6570", "motivation": "\u73b0\u6709Meta-BBO\u5de5\u4f5c\u4e2d\u7684\u5956\u52b1\u51fd\u6570\u7531\u4eba\u5de5\u8bbe\u8ba1\uff0c\u5b58\u5728\u8bbe\u8ba1\u504f\u5dee\u548c\u5956\u52b1\u9ed1\u5ba2\u98ce\u9669\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u53d1\u73b0\u66f4\u597d\u7684\u5956\u52b1\u51fd\u6570", "method": "1) \u5f15\u5165\u5b9a\u5236\u5316\u8fdb\u5316\u8303\u5f0f\uff0c\u5728\u57fa\u4e8eLLM\u7684\u7a0b\u5e8f\u641c\u7d22\u8fc7\u7a0b\u4e2d\u786e\u4fdd\u6301\u7eed\u6539\u8fdb\uff1b2) \u91c7\u7528\u591a\u4efb\u52a1\u8fdb\u5316\u67b6\u6784\uff0c\u652f\u6301\u4e3a\u4e0d\u540cMeta-BBO\u65b9\u6cd5\u5e76\u884c\u53d1\u73b0\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u95f4\u77e5\u8bc6\u5171\u4eab\u52a0\u901f\u6536\u655b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u8be5\u65b9\u6cd5\u53d1\u73b0\u7684\u5956\u52b1\u51fd\u6570\u6709\u52a9\u4e8e\u63d0\u5347\u73b0\u6709Meta-BBO\u5de5\u4f5c\u7684\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u5956\u52b1\u8bbe\u8ba1\u5728Meta-BBO\u4e2d\u7684\u91cd\u8981\u6027", "conclusion": "LLM\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u81ea\u52a8\u5956\u52b1\u53d1\u73b0\u5de5\u5177\uff0c\u8fdb\u5316\u8303\u5f0f\u548c\u591a\u4efb\u52a1\u67b6\u6784\u7684\u7ed3\u5408\u80fd\u591f\u4e3aMeta-BBO\u53d1\u73b0\u66f4\u4f18\u7684\u5956\u52b1\u51fd\u6570", "topic": "agentic reinforcement learning"}}
{"id": "2601.21894", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21894", "abs": "https://arxiv.org/abs/2601.21894", "authors": ["Lukas Twist", "Shu Yang", "Hanqi Yan", "Jingzhi Gong", "Di Wang", "Helen Yannakoudakis", "Jie M. Zhang"], "title": "Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning", "comment": "16 pages, 5 figures, 3 tables", "summary": "Large Language Models (LLMs) increasingly exhibit strong reasoning abilities, often attributed to their capacity to generate chain-of-thought-style intermediate reasoning. Recent work suggests that exposure to code can further enhance these skills, but existing studies largely treat code as a generic training signal, leaving open the question of which properties of code actually contribute to improved reasoning. To address this gap, we study the structural complexity of code, which captures control flow and compositional structure that may shape how models internalise multi-step reasoning during fine-tuning. We examine two complementary settings: solution-driven complexity, where complexity varies across multiple solutions to the same problem, and problem-driven complexity, where complexity reflects variation in the underlying tasks. Using cyclomatic complexity and logical lines of code to construct controlled fine-tuning datasets, we evaluate a range of open-weight LLMs on diverse reasoning benchmarks. Our findings show that although code can improve reasoning, structural properties strongly determine its usefulness. In 83% of experiments, restricting fine-tuning data to a specific structural complexity range outperforms training on structurally diverse code, pointing to a data-centric path for improving reasoning beyond scaling.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4ee3\u7801\u7684\u7ed3\u6784\u590d\u6742\u6027\uff08\u63a7\u5236\u6d41\u548c\u7ec4\u5408\u7ed3\u6784\uff09\u5bf9LLM\u63a8\u7406\u80fd\u529b\u63d0\u5347\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u7279\u5b9a\u590d\u6742\u5ea6\u8303\u56f4\u7684\u4ee3\u7801\u8bad\u7ec3\u6548\u679c\u4f18\u4e8e\u7ed3\u6784\u591a\u6837\u7684\u4ee3\u7801\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06\u4ee3\u7801\u89c6\u4e3a\u901a\u7528\u8bad\u7ec3\u4fe1\u53f7\uff0c\u4f46\u672a\u660e\u786e\u4ee3\u7801\u7684\u54ea\u4e9b\u5c5e\u6027\u771f\u6b63\u6709\u52a9\u4e8e\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u4ee3\u7801\u7684\u7ed3\u6784\u590d\u6742\u6027\u5982\u4f55\u5f71\u54cdLLM\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u4e24\u79cd\u4e92\u8865\u8bbe\u7f6e\uff1a1) \u89e3\u51b3\u65b9\u6848\u9a71\u52a8\u590d\u6742\u6027\uff08\u540c\u4e00\u95ee\u9898\u4e0d\u540c\u89e3\u51b3\u65b9\u6848\u7684\u590d\u6742\u5ea6\u53d8\u5316\uff09\uff1b2) \u95ee\u9898\u9a71\u52a8\u590d\u6742\u6027\uff08\u4e0d\u540c\u4efb\u52a1\u672c\u8eab\u7684\u590d\u6742\u5ea6\u5dee\u5f02\uff09\u3002\u4f7f\u7528\u5708\u590d\u6742\u5ea6\u548c\u903b\u8f91\u4ee3\u7801\u884c\u6570\u6784\u5efa\u53d7\u63a7\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u5728\u591a\u4e2a\u5f00\u6e90LLM\u4e0a\u8fdb\u884c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "83%\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06\u5fae\u8c03\u6570\u636e\u9650\u5236\u5728\u7279\u5b9a\u7ed3\u6784\u590d\u6742\u5ea6\u8303\u56f4\u5185\uff0c\u5176\u6548\u679c\u4f18\u4e8e\u4f7f\u7528\u7ed3\u6784\u591a\u6837\u7684\u4ee3\u7801\u8fdb\u884c\u8bad\u7ec3\u3002\u4ee3\u7801\u786e\u5b9e\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u7ed3\u6784\u5c5e\u6027\u5f3a\u70c8\u51b3\u5b9a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u4ee3\u7801\u7684\u7ed3\u6784\u590d\u6742\u6027\u662f\u5f71\u54cdLLM\u63a8\u7406\u80fd\u529b\u63d0\u5347\u7684\u5173\u952e\u56e0\u7d20\uff0c\u901a\u8fc7\u6570\u636e\u4e2d\u5fc3\u7684\u4f18\u5316\u65b9\u6cd5\uff08\u800c\u975e\u5355\u7eaf\u6269\u5927\u89c4\u6a21\uff09\u53ef\u4ee5\u6709\u6548\u6539\u5584\u6a21\u578b\u63a8\u7406\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.21924", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.21924", "abs": "https://arxiv.org/abs/2601.21924", "authors": ["Jinhang Chai", "Enpei Zhang", "Elynn Chen", "Yujun Yan"], "title": "Optimistic Transfer under Task Shift via Bellman Alignment", "comment": null, "summary": "We study online transfer reinforcement learning (RL) in episodic Markov decision processes, where experience from related source tasks is available during learning on a target task. A fundamental difficulty is that task similarity is typically defined in terms of rewards or transitions, whereas online RL algorithms operate on Bellman regression targets. As a result, naively reusing source Bellman updates introduces systematic bias and invalidates regret guarantees.\n  We identify one-step Bellman alignment as the correct abstraction for transfer in online RL and propose re-weighted targeting (RWT), an operator-level correction that retargets continuation values and compensates for transition mismatch via a change of measure. RWT reduces task mismatch to a fixed one-step correction and enables statistically sound reuse of source data.\n  This alignment yields a two-stage RWT $Q$-learning framework that separates variance reduction from bias correction. Under RKHS function approximation, we establish regret bounds that scale with the complexity of the task shift rather than the target MDP. Empirical results in both tabular and neural network settings demonstrate consistent improvements over single-task learning and na\u00efve pooling, highlighting Bellman alignment as a model-agnostic transfer principle for online RL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u8fc1\u79fb\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5RWT\uff0c\u901a\u8fc7Bellman\u5bf9\u9f50\u548c\u91cd\u52a0\u6743\u76ee\u6807\u4fee\u6b63\uff0c\u5728\u6e90\u4efb\u52a1\u548c\u76ee\u6807\u4efb\u52a1\u5b58\u5728\u8f6c\u79fb\u6982\u7387\u4e0d\u5339\u914d\u65f6\uff0c\u80fd\u591f\u7edf\u8ba1\u6709\u6548\u5730\u91cd\u7528\u6e90\u6570\u636e\uff0c\u51cf\u5c11\u4efb\u52a1\u504f\u79fb\u590d\u6742\u5ea6\u800c\u975e\u76ee\u6807MDP\u590d\u6742\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u7ebf\u8fc1\u79fb\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u6e90\u4efb\u52a1\u548c\u76ee\u6807\u4efb\u52a1\u901a\u5e38\u901a\u8fc7\u5956\u52b1\u6216\u8f6c\u79fb\u6982\u7387\u5b9a\u4e49\u76f8\u4f3c\u6027\uff0c\u4f46\u5728\u7ebfRL\u7b97\u6cd5\u57fa\u4e8eBellman\u56de\u5f52\u76ee\u6807\u64cd\u4f5c\u3002\u76f4\u63a5\u91cd\u7528\u6e90Bellman\u66f4\u65b0\u4f1a\u5f15\u5165\u7cfb\u7edf\u6027\u504f\u5dee\u5e76\u7834\u574f\u9057\u61be\u4fdd\u8bc1\uff0c\u9700\u8981\u627e\u5230\u6b63\u786e\u7684\u8fc1\u79fb\u62bd\u8c61\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u6b65Bellman\u5bf9\u9f50\u4f5c\u4e3a\u5728\u7ebfRL\u8fc1\u79fb\u7684\u6b63\u786e\u62bd\u8c61\uff0c\u8bbe\u8ba1\u4e86\u91cd\u52a0\u6743\u76ee\u6807(RWT)\u7b97\u5b50\u7ea7\u4fee\u6b63\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u5ea6\u53d8\u5316\u91cd\u65b0\u5b9a\u4f4d\u8fde\u7eed\u503c\u5e76\u8865\u507f\u8f6c\u79fb\u4e0d\u5339\u914d\u3002RWT\u5c06\u4efb\u52a1\u4e0d\u5339\u914d\u7b80\u5316\u4e3a\u56fa\u5b9a\u7684\u4e00\u6b65\u4fee\u6b63\uff0c\u5b9e\u73b0\u4e86\u4e24\u9636\u6bb5RWT Q\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u65b9\u5dee\u51cf\u5c11\u4e0e\u504f\u5dee\u4fee\u6b63\u5206\u79bb\u3002", "result": "\u5728RKHS\u51fd\u6570\u903c\u8fd1\u4e0b\uff0c\u5efa\u7acb\u4e86\u9057\u61be\u8fb9\u754c\uff0c\u5176\u7f29\u653e\u4e0e\u4efb\u52a1\u504f\u79fb\u590d\u6742\u5ea6\u800c\u975e\u76ee\u6807MDP\u590d\u6742\u5ea6\u76f8\u5173\u3002\u5728\u8868\u683c\u548c\u795e\u7ecf\u7f51\u7edc\u8bbe\u7f6e\u4e2d\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u5355\u4efb\u52a1\u5b66\u4e60\u548c\u6734\u7d20\u6c60\u5316\u65b9\u6cd5\uff0cRWT\u65b9\u6cd5\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "Bellman\u5bf9\u9f50\u4f5c\u4e3a\u5728\u7ebfRL\u7684\u6a21\u578b\u65e0\u5173\u8fc1\u79fb\u539f\u5219\uff0c\u901a\u8fc7RWT\u6846\u67b6\u5b9e\u73b0\u4e86\u7edf\u8ba1\u6709\u6548\u7684\u6e90\u6570\u636e\u91cd\u7528\uff0c\u89e3\u51b3\u4e86\u8f6c\u79fb\u6982\u7387\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u5728\u7ebf\u8fc1\u79fb\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.21944", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21944", "abs": "https://arxiv.org/abs/2601.21944", "authors": ["Konstantinos P. Panousis", "Diego Marcos"], "title": "Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models", "comment": null, "summary": "The widespread adoption of Vision-Language Models (VLMs) across fields has amplified concerns about model interpretability. Distressingly, these models are often treated as black-boxes, with limited or non-existent investigation of their decision making process. Despite numerous post- and ante-hoc interepretability methods, systematic and objective evaluation of the learned representations remains limited, particularly for sparsity-aware methods that are increasingly considered to \"induce interpretability\". In this work, we focus on Concept Bottleneck Models and investigate how different modeling decisions affect the emerging representations. We introduce the notion of clarity, a measure, capturing the interplay between the downstream performance and the sparsity and precision of the concept representation, while proposing an interpretability assessment framework using datasets with ground truth concept annotations. We consider both VLM- and attribute predictor-based CBMs, and three different sparsity-inducing strategies: per example $\\ell_1, \\ell_0$ and Bernoulli-based formulations. Our experiments reveal a critical trade-off between flexibility and interpretability, under which a given method can exhibit markedly different behaviors even at comparable performance levels. The code will be made publicly available upon publication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\u7684\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\"\u6e05\u6670\u5ea6\"\u6307\u6807\u6765\u8861\u91cf\u4e0b\u6e38\u6027\u80fd\u4e0e\u6982\u5ff5\u8868\u793a\u7a00\u758f\u6027\u3001\u7cbe\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4f7f\u7528\u771f\u5b9e\u6982\u5ff5\u6807\u6ce8\u6570\u636e\u7684\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u5bf9\u5176\u53ef\u89e3\u91ca\u6027\u7684\u62c5\u5fe7\uff0c\u8fd9\u4e9b\u6a21\u578b\u5e38\u88ab\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u51b3\u7b56\u8fc7\u7a0b\u7f3a\u4e4f\u7814\u7a76\u3002\u5c3d\u7ba1\u5b58\u5728\u591a\u79cd\u89e3\u91ca\u65b9\u6cd5\uff0c\u4f46\u5bf9\u5b66\u4e60\u8868\u793a\u7684\u7cfb\u7edf\u6027\u5ba2\u89c2\u8bc4\u4f30\u4ecd\u7136\u6709\u9650\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u88ab\u8ba4\u4e3a\u80fd\"\u8bf1\u5bfc\u53ef\u89e3\u91ca\u6027\"\u7684\u7a00\u758f\u6027\u65b9\u6cd5\u3002", "method": "\u805a\u7126\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBMs\uff09\uff0c\u7814\u7a76\u4e0d\u540c\u5efa\u6a21\u51b3\u7b56\u5982\u4f55\u5f71\u54cd\u8868\u793a\u5b66\u4e60\u3002\u63d0\u51fa\"\u6e05\u6670\u5ea6\"\u6307\u6807\u6765\u6355\u6349\u4e0b\u6e38\u6027\u80fd\u4e0e\u6982\u5ff5\u8868\u793a\u7a00\u758f\u6027\u3001\u7cbe\u786e\u6027\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u4f7f\u7528\u5177\u6709\u771f\u5b9e\u6982\u5ff5\u6807\u6ce8\u7684\u6570\u636e\u96c6\u6784\u5efa\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u6846\u67b6\u3002\u8003\u8651\u57fa\u4e8eVLM\u548c\u5c5e\u6027\u9884\u6d4b\u5668\u7684CBMs\uff0c\u4ee5\u53ca\u4e09\u79cd\u7a00\u758f\u6027\u8bf1\u5bfc\u7b56\u7565\uff1a\u6bcf\u6837\u672c\u2113\u2081\u3001\u2113\u2080\u548c\u57fa\u4e8e\u4f2f\u52aa\u5229\u5206\u5e03\u7684\u516c\u5f0f\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u7075\u6d3b\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\uff0c\u8868\u660e\u5373\u4f7f\u5728\u53ef\u6bd4\u6027\u80fd\u6c34\u5e73\u4e0b\uff0c\u4e0d\u540c\u65b9\u6cd5\u53ef\u80fd\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u540c\u7684\u884c\u4e3a\u3002\u63d0\u51fa\u7684\u8bc4\u4f30\u6846\u67b6\u80fd\u591f\u5ba2\u89c2\u8bc4\u4f30\u4e0d\u540c\u7a00\u758f\u6027\u8bf1\u5bfc\u7b56\u7565\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\u548c\u91cf\u5316\u6307\u6807\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u8bbe\u8ba1\u4e2d\u7075\u6d3b\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u6743\u8861\u5173\u7cfb\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2601.21991", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21991", "abs": "https://arxiv.org/abs/2601.21991", "authors": ["Zuyuan Zhang", "Mahdi Imani", "Tian Lan"], "title": "Geometry of Drifting MDPs with Path-Integral Stability Certificates", "comment": null, "summary": "Real-world reinforcement learning is often \\emph{nonstationary}: rewards and dynamics drift, accelerate, oscillate, and trigger abrupt switches in the optimal action. Existing theory often represents nonstationarity with coarse-scale models that measure \\emph{how much} the environment changes, not \\emph{how} it changes locally -- even though acceleration and near-ties drive tracking error and policy chattering. We take a geometric view of nonstationary discounted Markov Decision Processes (MDPs) by modeling the environment as a differentiable homotopy path and tracking the induced motion of the optimal Bellman fixed point. This yields a length-curvature-kink signature of intrinsic complexity: cumulative drift, acceleration/oscillation, and action-gap-induced nonsmoothness. We prove a solver-agnostic path-integral stability bound and derive gap-safe feasible regions that certify local stability away from switch regimes. Building on these results, we introduce \\textit{Homotopy-Tracking RL (HT-RL)} and \\textit{HT-MCTS}, lightweight wrappers that estimate replay-based proxies of length, curvature, and near-tie proximity online and adapt learning or planning intensity accordingly. Experiments show improved tracking and dynamic regret over matched static baselines, with the largest gains in oscillatory and switch-prone regimes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u89c6\u89d2\u6765\u5efa\u6a21\u975e\u5e73\u7a33\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u5c06\u73af\u5883\u89c6\u4e3a\u53ef\u5fae\u540c\u4f26\u8def\u5f84\uff0c\u5e76\u8ddf\u8e2a\u6700\u4f18\u8d1d\u5c14\u66fc\u4e0d\u52a8\u70b9\u7684\u8fd0\u52a8\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86HT-RL\u548cHT-MCTS\u8f7b\u91cf\u7ea7\u5305\u88c5\u5668\uff0c\u901a\u8fc7\u5728\u7ebf\u4f30\u8ba1\u957f\u5ea6\u3001\u66f2\u7387\u548c\u8fd1\u5e73\u5c40\u63a5\u8fd1\u5ea6\u6765\u9002\u5e94\u5b66\u4e60\u5f3a\u5ea6\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u5f80\u5f80\u662f\u975e\u5e73\u7a33\u7684\uff1a\u5956\u52b1\u548c\u52a8\u6001\u4f1a\u6f02\u79fb\u3001\u52a0\u901f\u3001\u632f\u8361\u5e76\u89e6\u53d1\u6700\u4f18\u52a8\u4f5c\u7684\u7a81\u7136\u5207\u6362\u3002\u73b0\u6709\u7406\u8bba\u901a\u5e38\u4f7f\u7528\u7c97\u7c92\u5ea6\u6a21\u578b\u6765\u8868\u793a\u975e\u5e73\u7a33\u6027\uff0c\u53ea\u8861\u91cf\u73af\u5883\u53d8\u5316\u4e86\u591a\u5c11\uff0c\u800c\u4e0d\u662f\u5982\u4f55\u5c40\u90e8\u53d8\u5316\u2014\u2014\u5c3d\u7ba1\u52a0\u901f\u5ea6\u548c\u8fd1\u5e73\u5c40\u4f1a\u5bfc\u81f4\u8ddf\u8e2a\u8bef\u5dee\u548c\u7b56\u7565\u6296\u52a8\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u89c6\u89d2\u5efa\u6a21\u975e\u5e73\u7a33\u6298\u6263\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5c06\u73af\u5883\u89c6\u4e3a\u53ef\u5fae\u540c\u4f26\u8def\u5f84\uff0c\u8ddf\u8e2a\u6700\u4f18\u8d1d\u5c14\u66fc\u4e0d\u52a8\u70b9\u7684\u8bf1\u5bfc\u8fd0\u52a8\u3002\u63d0\u51fa\u957f\u5ea6-\u66f2\u7387-\u626d\u7ed3\u7279\u5f81\u6765\u8868\u5f81\u5185\u5728\u590d\u6742\u6027\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86HT-RL\u548cHT-MCTS\u8f7b\u91cf\u7ea7\u5305\u88c5\u5668\uff0c\u5728\u7ebf\u4f30\u8ba1\u57fa\u4e8e\u56de\u653e\u7684\u4ee3\u7406\u6307\u6807\u5e76\u76f8\u5e94\u8c03\u6574\u5b66\u4e60\u6216\u89c4\u5212\u5f3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u5339\u914d\u7684\u9759\u6001\u57fa\u7ebf\u57fa\u7840\u4e0a\u6539\u8fdb\u4e86\u8ddf\u8e2a\u6027\u80fd\u548c\u52a8\u6001\u9057\u61be\uff0c\u5728\u632f\u8361\u548c\u5207\u6362\u503e\u5411\u7684\u673a\u5236\u4e2d\u83b7\u5f97\u4e86\u6700\u5927\u7684\u589e\u76ca\u3002", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u5efa\u6a21\u975e\u5e73\u7a33\u73af\u5883\u7684\u5c40\u90e8\u53d8\u5316\u7279\u5f81\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.22132", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22132", "abs": "https://arxiv.org/abs/2601.22132", "authors": ["Ziming Dong", "Hardik Sharma", "Evan O'Toole", "Jaya Prakash Champati", "Kui Wu"], "title": "Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference", "comment": null, "summary": "Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.", "AI": {"tldr": "LLM Shepherding\uff1a\u901a\u8fc7\u4eceLLM\u83b7\u53d6\u7b80\u77ed\u524d\u7f00\uff08\u63d0\u793a\uff09\u6765\u6307\u5bfcSLM\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u5b9e\u73b042-94%\u7684\u6210\u672c\u8282\u7ea6\u3002", "motivation": "LLM\u867d\u7136\u6027\u80fd\u5f3a\u5927\u4f46\u63a8\u7406\u6210\u672c\u9ad8\uff0cSLM\u6210\u672c\u4f4e\u4f46\u7cbe\u5ea6\u4e0d\u8db3\u3002\u73b0\u6709\u8def\u7531\u548c\u7ea7\u8054\u65b9\u6cd5\u5c06LLM\u89c6\u4e3a\u5168\u6709\u6216\u5168\u65e0\u7684\u8d44\u6e90\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "method": "\u63d0\u51faLLM Shepherding\u6846\u67b6\uff0c\u4ec5\u4eceLLM\u8bf7\u6c42\u7b80\u77ed\u524d\u7f00\uff08\u63d0\u793a\uff09\u63d0\u4f9b\u7ed9SLM\u3002\u5f00\u53d1\u4e24\u9636\u6bb5\u9884\u6d4b\u5668\uff0c\u8054\u5408\u51b3\u5b9a\u662f\u5426\u9700\u8981\u63d0\u793a\u4ee5\u53ca\u8bf7\u6c42\u591a\u5c11token\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\uff08GSM8K\u3001CNK12\uff09\u548c\u4ee3\u7801\u751f\u6210\uff08HumanEval\u3001MBPP\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u7eafLLM\u63a8\u7406\u51cf\u5c1142-94%\u6210\u672c\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u8def\u7531\u548c\u7ea7\u8054\u57fa\u7ebf\u6700\u591a\u964d\u4f4e2.8\u500d\u6210\u672c\u3002", "conclusion": "LLM Shepherding\u901a\u8fc7\u7ec6\u7c92\u5ea6token\u7ea7\u9884\u7b97\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86SLM-LLM\u534f\u4f5c\u7684\u6709\u6548\u5e73\u8861\uff0c\u662f\u9996\u4e2a\u5229\u7528token\u7ea7\u63a7\u5236\u8fdb\u884c\u6a21\u578b\u534f\u4f5c\u7684\u5de5\u4f5c\u3002", "topic": "code agent"}}
{"id": "tldr.2601.1628e511", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fimran-siddique%2Fagent-os%3Futm_source=tldrinfosec/1/0100019c04ef04d1-e5d00c4d-6327-42e4-ab5b-f43c7dd3b900-000000/KtVhN2Xg_kwLMnBgTsEIEzsrEiOIrdUBMzCOeEkMVkU=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fimran-siddique%2Fagent-os%3Futm_source=tldrinfosec/1/0100019c04ef04d1-e5d00c4d-6327-42e4-ab5b-f43c7dd3b900-000000/KtVhN2Xg_kwLMnBgTsEIEzsrEiOIrdUBMzCOeEkMVkU=442", "authors": ["TLDR Newsletter"], "title": "Agent OS", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fimran-siddique%2Fagent-os%3Futm_source=tldrinfosec/1/0100019c04ef04d1-e5d00c4d-6327-42e4-ab5b-f43c7dd3b900-000000/KtVhN2Xg_kwLMnBgTsEIEzsrEiOIrdUBMzCOeEkMVkU=442", "summary": "Agent OS (GitHub Repo) Agent OS applies operating system concepts to AI agent governance, providing kernel-level policy enforcement that intercepts and blocks actions before execution rather than relying on prompt-based safety. The framework includes POSIX-inspired primitives such as signals, a virtual filesystem for agent memory, cross-model verification, inter-agent trust protocols with cryptographic signing, and integrations with LangChain, CrewAI, and OpenAI Assistants. An MCP server enab...", "source": "tldr", "AI": {"tldr": "Agent OS \u5c06\u64cd\u4f5c\u7cfb\u7edf\u6982\u5ff5\u5e94\u7528\u4e8eAI\u667a\u80fd\u4f53\u6cbb\u7406\uff0c\u63d0\u4f9b\u5185\u6838\u7ea7\u7b56\u7565\u6267\u884c\uff0c\u5728\u884c\u52a8\u6267\u884c\u524d\u8fdb\u884c\u62e6\u622a\u548c\u963b\u6b62\uff0c\u800c\u975e\u4f9d\u8d56\u57fa\u4e8e\u63d0\u793a\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524dAI\u667a\u80fd\u4f53\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u8fdb\u884c\u5b89\u5168\u63a7\u5236\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u4e14\u5bb9\u6613\u88ab\u7ed5\u8fc7\u3002\u9700\u8981\u66f4\u5e95\u5c42\u7684\u3001\u64cd\u4f5c\u7cfb\u7edf\u7ea7\u522b\u7684\u5b89\u5168\u673a\u5236\u6765\u786e\u4fdd\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u5e94\u7528\u64cd\u4f5c\u7cfb\u7edf\u6982\u5ff5\u6784\u5efaAI\u667a\u80fd\u4f53\u6cbb\u7406\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u5185\u6838\u7ea7\u7b56\u7565\u6267\u884c\u673a\u5236\uff1b2\uff09POSIX\u98ce\u683c\u7684\u4fe1\u53f7\u673a\u5236\uff1b3\uff09\u865a\u62df\u6587\u4ef6\u7cfb\u7edf\u7528\u4e8e\u667a\u80fd\u4f53\u5185\u5b58\u7ba1\u7406\uff1b4\uff09\u8de8\u6a21\u578b\u9a8c\u8bc1\uff1b5\uff09\u57fa\u4e8e\u5bc6\u7801\u5b66\u7b7e\u540d\u7684\u667a\u80fd\u4f53\u95f4\u4fe1\u4efb\u534f\u8bae\uff1b6\uff09\u4e0eLangChain\u3001CrewAI\u3001OpenAI Assistants\u7684\u96c6\u6210\uff1b7\uff09MCP\u670d\u52a1\u5668\u652f\u6301\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684AI\u667a\u80fd\u4f53\u64cd\u4f5c\u7cfb\u7edf\u6846\u67b6\uff0c\u80fd\u591f\u5728\u667a\u80fd\u4f53\u884c\u52a8\u6267\u884c\u524d\u8fdb\u884c\u7b56\u7565\u62e6\u622a\uff0c\u63d0\u4f9b\u6bd4\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b\u66f4\u53ef\u9760\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u652f\u6301\u591a\u79cd\u4e3b\u6d41AI\u5f00\u53d1\u6846\u67b6\u3002", "conclusion": "\u64cd\u4f5c\u7cfb\u7edf\u6982\u5ff5\u4e3aAI\u667a\u80fd\u4f53\u6cbb\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u6846\u67b6\uff0c\u5185\u6838\u7ea7\u7b56\u7565\u6267\u884c\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u7684AI\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.eaa89607", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Forkes.io%2Fblog%2Fhow-to-orchestrate-langchain-agents-for-production-with-orkes-conductor%2F%3Futm_campaign=TLDR-AI%26utm_source=Newsletter%26utm_medium=referral/2/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/iEKMIZxX9-nXqBAiV7GbWOXhHLP3Nd2GfA7y-Ep9vLQ=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Forkes.io%2Fblog%2Fhow-to-orchestrate-langchain-agents-for-production-with-orkes-conductor%2F%3Futm_campaign=TLDR-AI%26utm_source=Newsletter%26utm_medium=referral/2/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/iEKMIZxX9-nXqBAiV7GbWOXhHLP3Nd2GfA7y-Ep9vLQ=442", "authors": ["TLDR Newsletter"], "title": "LangChain works great... until your agent becomes part of a real production system", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Forkes.io%2Fblog%2Fhow-to-orchestrate-langchain-agents-for-production-with-orkes-conductor%2F%3Futm_campaign=TLDR-AI%26utm_source=Newsletter%26utm_medium=referral/2/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/iEKMIZxX9-nXqBAiV7GbWOXhHLP3Nd2GfA7y-Ep9vLQ=442", "summary": "LangChain works great... until your agent becomes part of a real production system (Sponsor) Agent A finished, Agent B is waiting, Agent C failed halfway through, and you have no idea what state your system is in. Building agents truly is just the tip of the iceberg.In enterprise environments, your LangChain agent is almost never the whole story. This Orkes technical guide shows how to coordinate your agents, services and systems in durable, observable workflows. \ud83d\udc49 Instead of stitching everyt...", "source": "tldr", "AI": {"tldr": "\u8be5\u6307\u5357\u4ecb\u7ecd\u5982\u4f55\u5c06LangChain\u4ee3\u7406\u96c6\u6210\u5230\u4f01\u4e1a\u751f\u4ea7\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u534f\u8c03\u4ee3\u7406\u3001\u670d\u52a1\u548c\u7cfb\u7edf\u521b\u5efa\u53ef\u89c2\u5bdf\u3001\u6301\u4e45\u7684\u5de5\u4f5c\u6d41\uff0c\u89e3\u51b3\u4ee3\u7406\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u72b6\u6001\u7ba1\u7406\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "LangChain\u4ee3\u7406\u5728\u5f00\u53d1\u9636\u6bb5\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u751f\u4ea7\u7cfb\u7edf\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u591a\u4e2a\u4ee3\u7406\u534f\u540c\u5de5\u4f5c\u65f6\u7684\u72b6\u6001\u7ba1\u7406\u95ee\u9898\u3001\u6545\u969c\u6062\u590d\u56f0\u96be\u3001\u7cfb\u7edf\u53ef\u89c2\u5bdf\u6027\u4e0d\u8db3\u3002\u4f01\u4e1a\u73af\u5883\u4e2d\u4ee3\u7406\u5f88\u5c11\u662f\u72ec\u7acb\u8fd0\u884c\u7684\uff0c\u9700\u8981\u4e0e\u73b0\u6709\u670d\u52a1\u548c\u7cfb\u7edf\u96c6\u6210\u3002", "method": "\u91c7\u7528\u5de5\u4f5c\u6d41\u534f\u8c03\u65b9\u6cd5\uff0c\u5c06\u4ee3\u7406\u3001\u670d\u52a1\u548c\u7cfb\u7edf\u7ec4\u7ec7\u6210\u6301\u4e45\u5316\u3001\u53ef\u89c2\u5bdf\u7684\u5de5\u4f5c\u6d41\u3002\u901a\u8fc7\u4e13\u95e8\u7684\u534f\u8c03\u6846\u67b6\uff08\u5982Orkes\uff09\u6765\u7ba1\u7406\u4ee3\u7406\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3001\u72b6\u6001\u8ddf\u8e2a\u548c\u6545\u969c\u6062\u590d\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u5c06\u6240\u6709\u7ec4\u4ef6\u62fc\u63a5\u5728\u4e00\u8d77\u3002", "result": "\u901a\u8fc7\u5de5\u4f5c\u6d41\u534f\u8c03\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\uff1a1\uff09\u4ee3\u7406\u72b6\u6001\u7684\u6301\u4e45\u5316\u8ddf\u8e2a\uff1b2\uff09\u4ee3\u7406\u95f4\u4f9d\u8d56\u5173\u7cfb\u7684\u53ef\u9760\u7ba1\u7406\uff1b3\uff09\u6545\u969c\u65f6\u7684\u81ea\u52a8\u6062\u590d\u673a\u5236\uff1b4\uff09\u6574\u4e2a\u7cfb\u7edf\u7684\u53ef\u89c2\u5bdf\u6027\u63d0\u5347\uff1b5\uff09\u4f01\u4e1a\u7ea7\u751f\u4ea7\u73af\u5883\u7684\u7a33\u5b9a\u6027\u4fdd\u969c\u3002", "conclusion": "\u6784\u5efa\u4ee3\u7406\u53ea\u662f\u51b0\u5c71\u4e00\u89d2\uff0c\u771f\u6b63\u7684\u6311\u6218\u5728\u4e8e\u5c06\u5176\u96c6\u6210\u5230\u751f\u4ea7\u7cfb\u7edf\u4e2d\u3002\u901a\u8fc7\u5de5\u4f5c\u6d41\u534f\u8c03\u6846\u67b6\u53ef\u4ee5\u89e3\u51b3\u4ee3\u7406\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3001\u53ef\u89c2\u5bdf\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u95ee\u9898\uff0c\u4f7fLangChain\u4ee3\u7406\u80fd\u591f\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u7a33\u5b9a\u8fd0\u884c\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.82ba7127", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.kimi.com%2Fblog%2Fkimi-k2-5.html%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/ETpI1gQqxxvDFzaYDwlFutmRUWdiRdFgTCoZvbz8ltA=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.kimi.com%2Fblog%2Fkimi-k2-5.html%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/ETpI1gQqxxvDFzaYDwlFutmRUWdiRdFgTCoZvbz8ltA=442", "authors": ["TLDR Newsletter"], "title": "Kimi K2.5", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.kimi.com%2Fblog%2Fkimi-k2-5.html%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/ETpI1gQqxxvDFzaYDwlFutmRUWdiRdFgTCoZvbz8ltA=442", "summary": "Kimi K2.5 (7 minute read) Kimi K2.5 is a multimodal model capable of visual reasoning, code generation from UI and video inputs, and agentic task orchestration using a swarm-based architecture. It's built on 15T vision-language tokens through continual pretraining.", "source": "tldr", "AI": {"tldr": "Kimi K2.5\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5177\u5907\u89c6\u89c9\u63a8\u7406\u3001\u4eceUI\u548c\u89c6\u9891\u8f93\u5165\u751f\u6210\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7fa4\u4f53\u7684\u67b6\u6784\u8fdb\u884c\u667a\u80fd\u4efb\u52a1\u7f16\u6392\uff0c\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u6784\u5efa\u572815T\u89c6\u89c9\u8bed\u8a00token\u4e0a\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\uff08\u7279\u522b\u662fUI\u548c\u89c6\u9891\uff09\u5e76\u6267\u884c\u590d\u6742\u4efb\u52a1\u7f16\u6392\u7684AI\u7cfb\u7edf\uff0c\u4ee5\u589e\u5f3aAI\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u57fa\u4e8e15T\u89c6\u89c9\u8bed\u8a00token\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u6784\u5efa\u591a\u6a21\u6001\u6a21\u578b\uff0c\u652f\u6301\u89c6\u89c9\u63a8\u7406\u3001\u4eceUI\u548c\u89c6\u9891\u751f\u6210\u4ee3\u7801\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7fa4\u4f53\u7684\u67b6\u6784\u8fdb\u884c\u667a\u80fd\u4efb\u52a1\u7f16\u6392\u3002", "result": "\u5f00\u53d1\u51faKimi K2.5\u6a21\u578b\uff0c\u5177\u5907\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b\uff0c\u80fd\u591f\u4ece\u89c6\u89c9\u8f93\u5165\u751f\u6210\u4ee3\u7801\u5e76\u8fdb\u884c\u590d\u6742\u7684\u4efb\u52a1\u7f16\u6392\u3002", "conclusion": "Kimi K2.5\u5c55\u793a\u4e86\u591a\u6a21\u6001AI\u5728\u89c6\u89c9\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u4efb\u52a1\u7f16\u6392\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u66f4\u667a\u80fd\u7684AI\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "code agent"}}
{"id": "tldr.2601.3f3a22e2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMoonshotAI%2Fkimi-agent-sdk%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/jcuZFKD7xkY40RAmvoXyS7-eX3f65_6e64BQ_-_2cj0=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMoonshotAI%2Fkimi-agent-sdk%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/jcuZFKD7xkY40RAmvoXyS7-eX3f65_6e64BQ_-_2cj0=442", "authors": ["TLDR Newsletter"], "title": "Kimi Agent SDK", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMoonshotAI%2Fkimi-agent-sdk%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/jcuZFKD7xkY40RAmvoXyS7-eX3f65_6e64BQ_-_2cj0=442", "summary": "Kimi Agent SDK (GitHub Repo) The Kimi Agent SDK is a set of multi-language libraries that expose the Kimi Code agent runtime in applications. It can be used to build products, automations, and custom tooling. The SDK clients reuse the same Kimi CLI configuration, tools, and MCP servers to stream responses in real-time, surface approvals and tool calls, and let users orchestrate sessions programmatically. Go, Node.js, and Python are supported.", "source": "tldr", "AI": {"tldr": "Kimi Agent SDK\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u5e93\u96c6\uff0c\u5c06Kimi\u4ee3\u7801\u667a\u80fd\u4f53\u8fd0\u884c\u65f6\u66b4\u9732\u7ed9\u5e94\u7528\u7a0b\u5e8f\uff0c\u7528\u4e8e\u6784\u5efa\u4ea7\u54c1\u3001\u81ea\u52a8\u5316\u548c\u81ea\u5b9a\u4e49\u5de5\u5177", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u7edf\u4e00\u7684SDK\u6765\u96c6\u6210Kimi\u4ee3\u7801\u667a\u80fd\u4f53\u80fd\u529b\u5230\u5404\u79cd\u5e94\u7528\u4e2d\uff0c\u7b80\u5316\u667a\u80fd\u4f53\u96c6\u6210\u6d41\u7a0b", "method": "\u63d0\u4f9bGo\u3001Node.js\u548cPython\u7684\u591a\u8bed\u8a00SDK\u5e93\uff0c\u590d\u7528Kimi CLI\u914d\u7f6e\u3001\u5de5\u5177\u548cMCP\u670d\u52a1\u5668\uff0c\u652f\u6301\u5b9e\u65f6\u54cd\u5e94\u6d41\u3001\u5ba1\u6279\u548c\u5de5\u5177\u8c03\u7528\u5c55\u793a\uff0c\u4ee5\u53ca\u7a0b\u5e8f\u5316\u4f1a\u8bdd\u7f16\u6392", "result": "\u5b9e\u73b0\u4e86\u652f\u6301\u4e09\u79cd\u4e3b\u6d41\u7f16\u7a0b\u8bed\u8a00\u7684SDK\uff0c\u80fd\u591f\u5c06Kimi\u4ee3\u7801\u667a\u80fd\u4f53\u8fd0\u884c\u65f6\u96c6\u6210\u5230\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u63d0\u4f9b\u5b9e\u65f6\u54cd\u5e94\u548c\u5de5\u5177\u8c03\u7528\u529f\u80fd", "conclusion": "Kimi Agent SDK\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4fbf\u6377\u7684\u65b9\u5f0f\u6765\u6784\u5efa\u57fa\u4e8eKimi\u4ee3\u7801\u667a\u80fd\u4f53\u7684\u4ea7\u54c1\u3001\u81ea\u52a8\u5316\u548c\u81ea\u5b9a\u4e49\u5de5\u5177", "topic": "code agent"}}
{"id": "tldr.2601.0f479df4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fgoogle-launches-agentic-vision-in-gemini-3-flash%2F%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/R3tBi7Q_nUz3THf1-vk6KQxJ8t3wBh0Qa2o3Qcy65FE=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fgoogle-launches-agentic-vision-in-gemini-3-flash%2F%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/R3tBi7Q_nUz3THf1-vk6KQxJ8t3wBh0Qa2o3Qcy65FE=442", "authors": ["TLDR Newsletter"], "title": "Google launches Agentic Vision in Gemini 3 Flash", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fgoogle-launches-agentic-vision-in-gemini-3-flash%2F%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/R3tBi7Q_nUz3THf1-vk6KQxJ8t3wBh0Qa2o3Qcy65FE=442", "summary": "Google launches Agentic Vision in Gemini 3 Flash (1 minute read) Google has introduced Agentic Vision in Gemini 3 Flash. The new capability enables the model to effectively use code and reasoning to improve performance for common vision tasks. It is available to users through the Gemini API in Google AI Studio, Vertex AI, and within the Gemini app. Google plans to extend Agentic Vision's reach by supporting more model sizes and integrating additional tools like web and reverse image search.", "source": "tldr", "AI": {"tldr": "Google\u5728Gemini 3 Flash\u4e2d\u63a8\u51faAgentic Vision\u529f\u80fd\uff0c\u901a\u8fc7\u4ee3\u7801\u548c\u63a8\u7406\u63d0\u5347\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\uff0c\u73b0\u5df2\u901a\u8fc7\u591a\u4e2a\u5e73\u53f0\u63d0\u4f9b\uff0c\u672a\u6765\u5c06\u6269\u5c55\u6a21\u578b\u89c4\u6a21\u548c\u5de5\u5177\u96c6\u6210\u3002", "motivation": "\u63d0\u5347\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u7ed3\u5408\u4ee3\u7801\u548c\u63a8\u7406\u80fd\u529b\u6765\u89e3\u51b3\u5e38\u89c1\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u95ee\u9898\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u66f4\u6709\u6548\u5730\u5904\u7406\u89c6\u89c9\u4efb\u52a1\u3002", "method": "\u5728Gemini 3 Flash\u6a21\u578b\u4e2d\u96c6\u6210Agentic Vision\u529f\u80fd\uff0c\u5229\u7528\u4ee3\u7801\u548c\u63a8\u7406\u80fd\u529b\u6765\u589e\u5f3a\u89c6\u89c9\u4efb\u52a1\u5904\u7406\uff0c\u901a\u8fc7API\u548c\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u8be5\u529f\u80fd\u3002", "result": "\u6210\u529f\u63a8\u51fa\u4e86Agentic Vision\u529f\u80fd\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7Gemini API\u3001Google AI Studio\u3001Vertex AI\u548cGemini\u5e94\u7528\u4f7f\u7528\u8be5\u529f\u80fd\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "Agentic Vision\u662fGemini 3 Flash\u7684\u91cd\u8981\u589e\u5f3a\u529f\u80fd\uff0c\u672a\u6765\u8ba1\u5212\u652f\u6301\u66f4\u591a\u6a21\u578b\u89c4\u6a21\u5e76\u96c6\u6210\u66f4\u591a\u5de5\u5177\u5982\u7f51\u7edc\u641c\u7d22\u548c\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u3002", "topic": "code agent"}}
{"id": "tldr.2601.82624fcc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fmanus-ai-launches-agent-skills-open-standard-for-ai-workflows%2F%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/RuEUFKJHDrGNhefIoK2u0_6HHqnABwqnkaa4NRosPj8=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fmanus-ai-launches-agent-skills-open-standard-for-ai-workflows%2F%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/RuEUFKJHDrGNhefIoK2u0_6HHqnABwqnkaa4NRosPj8=442", "authors": ["TLDR Newsletter"], "title": "Manus AI launches Agent Skills open standard for pros", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fmanus-ai-launches-agent-skills-open-standard-for-ai-workflows%2F%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/RuEUFKJHDrGNhefIoK2u0_6HHqnABwqnkaa4NRosPj8=442", "summary": "Manus AI launches Agent Skills open standard for pros (2 minute read) Manus AI's Agent Skills open standard enables modular AI workflows for professionals, integrating Python and Bash scripts with a focus on efficiency and interoperability.", "source": "tldr", "AI": {"tldr": "Manus AI\u63a8\u51faAgent Skills\u5f00\u653e\u6807\u51c6\uff0c\u4e3a\u4e13\u4e1a\u4eba\u58eb\u63d0\u4f9b\u6a21\u5757\u5316AI\u5de5\u4f5c\u6d41\uff0c\u652f\u6301Python\u548cBash\u811a\u672c\u96c6\u6210\uff0c\u6ce8\u91cd\u6548\u7387\u548c\u4e92\u64cd\u4f5c\u6027", "motivation": "\u5f53\u524dAI\u5de5\u4f5c\u6d41\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u6a21\u5757\u5316\uff0c\u4e13\u4e1a\u4eba\u58eb\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u4e92\u64cd\u4f5c\u7684AI\u5de5\u5177\u96c6\u6210\u65b9\u6848", "method": "\u5f00\u53d1Agent Skills\u5f00\u653e\u6807\u51c6\uff0c\u652f\u6301Python\u548cBash\u811a\u672c\u7684\u6a21\u5757\u5316\u96c6\u6210\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u63a5\u53e3\u548c\u5de5\u4f5c\u6d41\u6846\u67b6", "result": "\u63a8\u51fa\u4e86\u9762\u5411\u4e13\u4e1a\u4eba\u58eb\u7684\u5f00\u653e\u6807\u51c6\uff0c\u5b9e\u73b0\u4e86AI\u5de5\u4f5c\u6d41\u7684\u6a21\u5757\u5316\u548c\u811a\u672c\u96c6\u6210", "conclusion": "Agent Skills\u6807\u51c6\u4e3aAI\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u4e13\u4e1a\u573a\u666f\u4e0b\u7684\u6548\u7387\u548c\u4e92\u64cd\u4f5c\u6027", "topic": "code agent"}}
{"id": "tldr.2601.847c6cce", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/hXU4dmc6agJaBkFz6whmVpgVSKsq4_aUUqnaYnUzayI=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/hXU4dmc6agJaBkFz6whmVpgVSKsq4_aUUqnaYnUzayI=442", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/hXU4dmc6agJaBkFz6whmVpgVSKsq4_aUUqnaYnUzayI=442", "summary": "Manus AI launches Agent Skills open standard for pros (2 minute read) Manus AI's Agent Skills open standard enables modular AI workflows for professionals, integrating Python and Bash scripts with a focus on efficiency and interoperability.", "source": "tldr", "AI": {"tldr": "Manus AI\u63a8\u51faAgent Skills\u5f00\u653e\u6807\u51c6\uff0c\u8ba9\u4e13\u4e1a\u4eba\u58eb\u80fd\u591f\u6784\u5efa\u6a21\u5757\u5316AI\u5de5\u4f5c\u6d41\uff0c\u652f\u6301Python\u548cBash\u811a\u672c\u96c6\u6210\uff0c\u6ce8\u91cd\u6548\u7387\u548c\u4e92\u64cd\u4f5c\u6027", "motivation": "\u4e3a\u4e13\u4e1a\u4eba\u58eb\u63d0\u4f9b\u6807\u51c6\u5316\u3001\u6a21\u5757\u5316\u7684AI\u5de5\u4f5c\u6d41\u6784\u5efa\u65b9\u6848\uff0c\u89e3\u51b3\u73b0\u6709AI\u5de5\u5177\u96c6\u6210\u56f0\u96be\u3001\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898", "method": "\u521b\u5efaAgent Skills\u5f00\u653e\u6807\u51c6\uff0c\u652f\u6301Python\u548cBash\u811a\u672c\u7684\u6a21\u5757\u5316\u96c6\u6210\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u63a5\u53e3\u548c\u5de5\u4f5c\u6d41\u6846\u67b6", "result": "\u63a8\u51fa\u4e86\u9762\u5411\u4e13\u4e1a\u4eba\u58eb\u7684Agent Skills\u5f00\u653e\u6807\u51c6\uff0c\u5b9e\u73b0\u4e86AI\u5de5\u4f5c\u6d41\u7684\u6a21\u5757\u5316\u6784\u5efa\u548c\u811a\u672c\u96c6\u6210", "conclusion": "Agent Skills\u6807\u51c6\u4e3a\u4e13\u4e1a\u4eba\u58eb\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u4e92\u64cd\u4f5c\u7684AI\u5de5\u4f5c\u6d41\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86AI\u5de5\u5177\u7684\u6807\u51c6\u5316\u5e94\u7528", "topic": "agent analysis"}}
{"id": "tldr.2601.431f1cc7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/kEjl4BncEhvaJHd_CBmQOHY0U0NtYYYSQJuboEzOu7Q=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/kEjl4BncEhvaJHd_CBmQOHY0U0NtYYYSQJuboEzOu7Q=442", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-01-28, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/kEjl4BncEhvaJHd_CBmQOHY0U0NtYYYSQJuboEzOu7Q=442", "summary": "Manus AI launches Agent Skills open standard for pros (2 minute read) Manus AI's Agent Skills open standard enables modular AI workflows for professionals, integrating Python and Bash scripts with a focus on efficiency and interoperability.", "source": "tldr", "AI": {"tldr": "Manus AI\u63a8\u51faAgent Skills\u5f00\u653e\u6807\u51c6\uff0c\u8ba9\u4e13\u4e1a\u4eba\u58eb\u80fd\u591f\u6784\u5efa\u6a21\u5757\u5316AI\u5de5\u4f5c\u6d41\uff0c\u96c6\u6210Python\u548cBash\u811a\u672c\uff0c\u6ce8\u91cd\u6548\u7387\u548c\u4e92\u64cd\u4f5c\u6027", "motivation": "\u5f53\u524dAI\u5de5\u4f5c\u6d41\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u6a21\u5757\u5316\uff0c\u4e13\u4e1a\u4eba\u58eb\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u4e92\u64cd\u4f5c\u7684AI\u5de5\u5177\u96c6\u6210\u65b9\u6848", "method": "\u5f00\u53d1Agent Skills\u5f00\u653e\u6807\u51c6\uff0c\u652f\u6301Python\u548cBash\u811a\u672c\u7684\u6a21\u5757\u5316\u96c6\u6210\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u63a5\u53e3\u548c\u5de5\u4f5c\u6d41\u7ba1\u7406", "result": "\u63a8\u51fa\u4e86\u9762\u5411\u4e13\u4e1a\u4eba\u58eb\u7684Agent Skills\u5f00\u653e\u6807\u51c6\uff0c\u5b9e\u73b0\u4e86AI\u5de5\u4f5c\u6d41\u7684\u6a21\u5757\u5316\u548c\u9ad8\u6548\u96c6\u6210", "conclusion": "Agent Skills\u6807\u51c6\u4e3aAI\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u4e13\u4e1a\u573a\u666f\u4e0b\u7684\u6548\u7387\u548c\u4e92\u64cd\u4f5c\u6027", "topic": "agent analysis"}}
{"id": "tldr.2601.e69f9718", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fblog%2Fengineering%2Fai%2Fcontextual-agent-playbooks-and-tools-how-linkedin-gave-ai-coding-agents-organizational-context%3Futm_source=tldrdata/1/0100019c097b72f7-7822700b-6893-4806-bc8f-d1ed18972f91-000000/wqXInkvlJvmA9RxWtwRDQLWREnv3jg0WXUEBNmtJ1cs=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fblog%2Fengineering%2Fai%2Fcontextual-agent-playbooks-and-tools-how-linkedin-gave-ai-coding-agents-organizational-context%3Futm_source=tldrdata/1/0100019c097b72f7-7822700b-6893-4806-bc8f-d1ed18972f91-000000/wqXInkvlJvmA9RxWtwRDQLWREnv3jg0WXUEBNmtJ1cs=442", "authors": ["TLDR Newsletter"], "title": "Contextual Agent Playbooks and Tools: How LinkedIn Gave AI Coding Agents Organizational Context", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fblog%2Fengineering%2Fai%2Fcontextual-agent-playbooks-and-tools-how-linkedin-gave-ai-coding-agents-organizational-context%3Futm_source=tldrdata/1/0100019c097b72f7-7822700b-6893-4806-bc8f-d1ed18972f91-000000/wqXInkvlJvmA9RxWtwRDQLWREnv3jg0WXUEBNmtJ1cs=442", "summary": "Contextual Agent Playbooks and Tools: How LinkedIn Gave AI Coding Agents Organizational Context (8 minute read) LinkedIn's Contextual Agent Playbooks & Tools (CAPT) is a framework that equips AI coding agents with deep organizational context. Built on Model Context Protocol (MCP), CAPT provides agents with safe, standardized access to internal tools and composable \"playbooks\" (Jinja2-templated workflows that encode institutional knowledge as reusable step-by-step instructions), allowing dynam...", "source": "tldr", "AI": {"tldr": "LinkedIn\u5f00\u53d1\u4e86CAPT\u6846\u67b6\uff0c\u901a\u8fc7MCP\u534f\u8bae\u4e3aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u7ec4\u7ec7\u4e0a\u4e0b\u6587\uff0c\u5305\u62ec\u5b89\u5168\u8bbf\u95ee\u5185\u90e8\u5de5\u5177\u548c\u53ef\u7ec4\u5408\u7684playbook\u5de5\u4f5c\u6d41", "motivation": "\u89e3\u51b3AI\u7f16\u7801\u4ee3\u7406\u7f3a\u4e4f\u7ec4\u7ec7\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u591f\u5b89\u5168\u8bbf\u95ee\u5185\u90e8\u5de5\u5177\u548c\u5229\u7528\u673a\u6784\u77e5\u8bc6", "method": "\u57fa\u4e8eModel Context Protocol\u6784\u5efaCAPT\u6846\u67b6\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\u8bbf\u95ee\u548cJinja2\u6a21\u677f\u5316\u7684\u53ef\u91cd\u7528playbook\u5de5\u4f5c\u6d41", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u4e3aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u7ec4\u7ec7\u4e0a\u4e0b\u6587\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u5b89\u5168\u5de5\u5177\u8bbf\u95ee\u548c\u673a\u6784\u77e5\u8bc6\u7f16\u7801", "conclusion": "CAPT\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86AI\u7f16\u7801\u4ee3\u7406\u7684\u7ec4\u7ec7\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\u548c\u5b89\u5168\u6027", "topic": "code agent"}}
{"id": "tldr.2601.f7c2bcb4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fdemystifying-evals-for-ai-agents%3Futm_source=tldrdata/1/0100019c097b72f7-7822700b-6893-4806-bc8f-d1ed18972f91-000000/HTc3Hv1vNxOpJqWEtjXiLLqOw7t_rghHhfv7gXhx3oI=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fdemystifying-evals-for-ai-agents%3Futm_source=tldrdata/1/0100019c097b72f7-7822700b-6893-4806-bc8f-d1ed18972f91-000000/HTc3Hv1vNxOpJqWEtjXiLLqOw7t_rghHhfv7gXhx3oI=442", "authors": ["TLDR Newsletter"], "title": "Demystifying evals for AI agents", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 19 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fdemystifying-evals-for-ai-agents%3Futm_source=tldrdata/1/0100019c097b72f7-7822700b-6893-4806-bc8f-d1ed18972f91-000000/HTc3Hv1vNxOpJqWEtjXiLLqOw7t_rghHhfv7gXhx3oI=442", "summary": "Demystifying evals for AI agents (19 minute read) Rigorous, automated evaluation frameworks are crucial for scalable, reliable AI agent development, transforming reactive debugging into proactive, metric-driven iteration. Effective agent evals combine code-, model-, and human-based grading across multidimensional tasks (e.g., coding, research, conversation, and GUI operations) using industry benchmarks and custom test suites. Quantitative methods like pass@k and pass^k provide nuanced perform...", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u4ee3\u7801\u3001\u6a21\u578b\u548c\u4eba\u5de5\u8bc4\u5206\u7684\u591a\u7ef4\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f7f\u7528\u884c\u4e1a\u57fa\u51c6\u548c\u81ea\u5b9a\u4e49\u6d4b\u8bd5\u5957\u4ef6\u6765\u91cf\u5316\u667a\u80fd\u4f53\u6027\u80fd\u3002", "motivation": "\u5f53\u524dAI\u667a\u80fd\u4f53\u5f00\u53d1\u7f3a\u4e4f\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u9700\u8981\u4ece\u88ab\u52a8\u8c03\u8bd5\u8f6c\u5411\u4e3b\u52a8\u7684\u3001\u57fa\u4e8e\u6307\u6807\u7684\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u4ee5\u652f\u6301\u53ef\u9760\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u4ee3\u7801\u8bc4\u4f30\u3001\u6a21\u578b\u8bc4\u4f30\u548c\u4eba\u5de5\u8bc4\u4f30\u7684\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u884c\u4e1a\u57fa\u51c6\u6d4b\u8bd5\u548c\u81ea\u5b9a\u4e49\u6d4b\u8bd5\u5957\u4ef6\uff0c\u91c7\u7528pass@k\u548cpass^k\u7b49\u91cf\u5316\u6307\u6807\u6765\u5168\u9762\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u7f16\u7801\u3001\u7814\u7a76\u3001\u5bf9\u8bdd\u3001GUI\u64cd\u4f5c\u7b49\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5efa\u7acb\u4e86\u7cfb\u7edf\u5316\u7684AI\u667a\u80fd\u4f53\u8bc4\u4f30\u65b9\u6cd5\u8bba\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u5168\u9762\u3001\u53ef\u91cf\u5316\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u652f\u6301\u667a\u80fd\u4f53\u5f00\u53d1\u7684\u6301\u7eed\u6539\u8fdb\u548c\u4f18\u5316\u3002", "conclusion": "\u4e25\u683c\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u5bf9\u4e8e\u53ef\u6269\u5c55\u3001\u53ef\u9760\u7684AI\u667a\u80fd\u4f53\u5f00\u53d1\u81f3\u5173\u91cd\u8981\uff0c\u591a\u7ef4\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.5279ef50", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview_260129secondary/1/0100019c0980e601-4e022604-d45d-40f7-bf3a-22109c267184-000000/5EFvNferCup_owQPzrvUdCMal6FJRIhvoEiieion7w4=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview_260129secondary/1/0100019c0980e601-4e022604-d45d-40f7-bf3a-22109c267184-000000/5EFvNferCup_owQPzrvUdCMal6FJRIhvoEiieion7w4=442", "authors": ["TLDR Newsletter"], "title": "AI code review with comments you'll actually implement", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview_260129secondary/1/0100019c0980e601-4e022604-d45d-40f7-bf3a-22109c267184-000000/5EFvNferCup_owQPzrvUdCMal6FJRIhvoEiieion7w4=442", "summary": "AI code review with comments you'll actually implement (Sponsor) Unblocked is the only AI code review tool that has deep insight of your codebase, docs, and conversations, giving you feedback that's shaped by how your system actually works instead of stylistic nitpicks. \u201cUnblocked made me reconsider my AI fatigue. Finally, a tool that surfaces context only someone with a full view of the codebase could provide.\u201d - Senior developer, Clio Try now for free", "source": "tldr", "AI": {"tldr": "Unblocked\u662f\u4e00\u6b3eAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u901a\u8fc7\u6df1\u5ea6\u7406\u89e3\u4ee3\u7801\u5e93\u3001\u6587\u6863\u548c\u5bf9\u8bdd\uff0c\u63d0\u4f9b\u57fa\u4e8e\u5b9e\u9645\u7cfb\u7edf\u5de5\u4f5c\u539f\u7406\u7684\u53cd\u9988\uff0c\u800c\u975e\u98ce\u683c\u6311\u5254", "motivation": "\u73b0\u6709AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u5f80\u5f80\u53ea\u5173\u6ce8\u4ee3\u7801\u98ce\u683c\u95ee\u9898\uff0c\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u5e93\u6574\u4f53\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\uff0c\u5bfc\u81f4\u53cd\u9988\u4e0d\u591f\u5b9e\u7528\u3002\u5f00\u53d1\u8005\u9700\u8981\u80fd\u591f\u7406\u89e3\u7cfb\u7edf\u5b9e\u9645\u5de5\u4f5c\u539f\u7406\u7684\u667a\u80fd\u5ba1\u67e5\u5de5\u5177", "method": "Unblocked\u901a\u8fc7\u6df1\u5ea6\u5206\u6790\u4ee3\u7801\u5e93\u3001\u6587\u6863\u548c\u56e2\u961f\u5bf9\u8bdd\uff0c\u5efa\u7acb\u5bf9\u7cfb\u7edf\u7684\u5168\u9762\u7406\u89e3\uff0c\u4ece\u800c\u63d0\u4f9b\u57fa\u4e8e\u5b9e\u9645\u4e0a\u4e0b\u6587\u7684\u6709\u610f\u4e49\u53cd\u9988\uff0c\u800c\u975e\u7b80\u5355\u7684\u98ce\u683c\u68c0\u67e5", "result": "\u8be5\u5de5\u5177\u83b7\u5f97\u4e86\u5f00\u53d1\u8005\u79ef\u6781\u8bc4\u4ef7\uff0c\u6709\u9ad8\u7ea7\u5f00\u53d1\u8005\u8868\u793a\u5b83\u6539\u53d8\u4e86\u4ed6\u4eec\u5bf9AI\u75b2\u52b3\u7684\u770b\u6cd5\uff0c\u63d0\u4f9b\u4e86\u53ea\u6709\u5bf9\u6574\u4e2a\u4ee3\u7801\u5e93\u6709\u5168\u9762\u4e86\u89e3\u7684\u4eba\u624d\u80fd\u7ed9\u51fa\u7684\u4e0a\u4e0b\u6587\u6d1e\u5bdf", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u4e0a\u4e0b\u6587\u7406\u89e3\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u6bd4\u4f20\u7edf\u7684\u98ce\u683c\u68c0\u67e5\u5de5\u5177\u66f4\u6709\u4ef7\u503c\uff0c\u80fd\u591f\u63d0\u4f9b\u5b9e\u9645\u53ef\u5b9e\u65bd\u7684\u53cd\u9988\uff0c\u771f\u6b63\u5e2e\u52a9\u5f00\u53d1\u8005\u6539\u8fdb\u4ee3\u7801\u8d28\u91cf", "topic": "code agent"}}
{"id": "tldr.2601.f621306c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.danshapiro.com%2Fblog%2F2026%2F01%2Fthe-five-levels-from-spicy-autocomplete-to-the-software-factory%2F%3Futm_source=tldrnewsletter/1/0100019c0980e601-4e022604-d45d-40f7-bf3a-22109c267184-000000/IuyDcmwrfAVGvALurwcd_QVXxa93O0LLrXgWR7tHqG4=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.danshapiro.com%2Fblog%2F2026%2F01%2Fthe-five-levels-from-spicy-autocomplete-to-the-software-factory%2F%3Futm_source=tldrnewsletter/1/0100019c0980e601-4e022604-d45d-40f7-bf3a-22109c267184-000000/IuyDcmwrfAVGvALurwcd_QVXxa93O0LLrXgWR7tHqG4=442", "authors": ["TLDR Newsletter"], "title": "The Five Levels: from Spicy Autocomplete to the Dark Factory", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.danshapiro.com%2Fblog%2F2026%2F01%2Fthe-five-levels-from-spicy-autocomplete-to-the-software-factory%2F%3Futm_source=tldrnewsletter/1/0100019c0980e601-4e022604-d45d-40f7-bf3a-22109c267184-000000/IuyDcmwrfAVGvALurwcd_QVXxa93O0LLrXgWR7tHqG4=442", "summary": "The Five Levels: from Spicy Autocomplete to the Dark Factory (4 minute read) The cost of code is dropping so fast that teams need to reconsider their tech debt payment plans. Smart teams are deferring payment on human hours today to pay them back with cheaper AI hours tomorrow. Cashing in on those cheap hours requires understanding how to move through the tiers of automation. Luckily, the federal government has already completed a roadmap for autonomous cars. The five levels of driving automa...", "source": "tldr", "AI": {"tldr": "\u6587\u7ae0\u5c06AI\u4ee3\u7801\u751f\u6210\u7c7b\u6bd4\u81ea\u52a8\u9a7e\u9a76\u5206\u7ea7\uff0c\u63d0\u51fa\u4ece\u57fa\u7840\u81ea\u52a8\u8865\u5168\u5230\u5b8c\u5168\u81ea\u4e3b\"\u9ed1\u6697\u5de5\u5382\"\u7684\u4e94\u7ea7\u6f14\u8fdb\u6846\u67b6\uff0c\u5efa\u8bae\u56e2\u961f\u5229\u7528AI\u6210\u672c\u4e0b\u964d\u8d8b\u52bf\uff0c\u63a8\u8fdf\u4eba\u5de5\u6280\u672f\u503a\u52a1\u507f\u8fd8\uff0c\u7528\u66f4\u4fbf\u5b9c\u7684AI\u5de5\u65f6\u6765\u507f\u8fd8\u3002", "motivation": "\u4ee3\u7801\u751f\u6210AI\u6210\u672c\u5feb\u901f\u4e0b\u964d\uff0c\u56e2\u961f\u9700\u8981\u91cd\u65b0\u8003\u8651\u6280\u672f\u503a\u52a1\u507f\u8fd8\u7b56\u7565\u3002\u4f20\u7edf\u4eba\u5de5\u507f\u8fd8\u65b9\u5f0f\u6210\u672c\u9ad8\u6602\uff0c\u800cAI\u5de5\u65f6\u6210\u672c\u66f4\u4f4e\uff0c\u4f46\u9700\u8981\u7406\u89e3\u5982\u4f55\u6709\u6548\u5229\u7528AI\u81ea\u52a8\u5316\u3002", "method": "\u501f\u9274\u7f8e\u56fd\u8054\u90a6\u653f\u5e9c\u81ea\u52a8\u9a7e\u9a76\u5206\u7ea7\u6807\u51c6\uff0c\u63d0\u51fa\u4ee3\u7801\u751f\u6210\u7684\u4e94\u7ea7\u81ea\u52a8\u5316\u6846\u67b6\uff1a\u4ece\u57fa\u7840\u81ea\u52a8\u8865\u5168\u5230\u5b8c\u5168\u81ea\u4e3b\u7684\"\u9ed1\u6697\u5de5\u5382\"\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u6e05\u6670\u7684AI\u4ee3\u7801\u751f\u6210\u6f14\u8fdb\u8def\u7ebf\u56fe\uff0c\u5e2e\u52a9\u56e2\u961f\u7406\u89e3\u5982\u4f55\u9010\u6b65\u63d0\u5347\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u5229\u7528\u5ec9\u4ef7AI\u5de5\u65f6\u507f\u8fd8\u6280\u672f\u503a\u52a1\u3002", "conclusion": "\u667a\u80fd\u56e2\u961f\u5e94\u63a8\u8fdf\u4eba\u5de5\u6280\u672f\u503a\u52a1\u507f\u8fd8\uff0c\u5229\u7528AI\u6210\u672c\u4e0b\u964d\u8d8b\u52bf\uff0c\u901a\u8fc7\u7406\u89e3\u5e76\u5b9e\u65bd\u81ea\u52a8\u5316\u5206\u7ea7\u6846\u67b6\uff0c\u7528\u66f4\u4fbf\u5b9c\u7684AI\u5de5\u65f6\u6765\u4f18\u5316\u5f00\u53d1\u6d41\u7a0b\u3002", "topic": "code agent"}}
{"id": "tldr.2601.aa69ad68", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyo.substack.com%2Fp%2Fthe-80-problem-in-agentic-coding%3Futm_source=tldrnewsletter/1/0100019c0980e601-4e022604-d45d-40f7-bf3a-22109c267184-000000/s6Erwr-zVqpt5uUls1RYXIHG8dTpE7o_hQ45vNdWjwU=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyo.substack.com%2Fp%2Fthe-80-problem-in-agentic-coding%3Futm_source=tldrnewsletter/1/0100019c0980e601-4e022604-d45d-40f7-bf3a-22109c267184-000000/s6Erwr-zVqpt5uUls1RYXIHG8dTpE7o_hQ45vNdWjwU=442", "authors": ["TLDR Newsletter"], "title": "The 80% Problem in Agentic Coding", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 20 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyo.substack.com%2Fp%2Fthe-80-problem-in-agentic-coding%3Futm_source=tldrnewsletter/1/0100019c0980e601-4e022604-d45d-40f7-bf3a-22109c267184-000000/s6Erwr-zVqpt5uUls1RYXIHG8dTpE7o_hQ45vNdWjwU=442", "summary": "The 80% Problem in Agentic Coding (20 minute read) AI writing most of the code places a disproportionate emphasis on the human's role of owning outcomes, maintaining quality bars, and ensuring that tests actually validate behavior. The danger is that the AI succeeds so well that developers stop checking their outputs. AI is an amplifier of development practices. Bad processes will result in accumulated debt at an unprecedented speed.", "source": "tldr", "AI": {"tldr": "AI\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5b8c\u621080%\u5de5\u4f5c\uff0c\u4f46\u4eba\u7c7b\u4ecd\u9700\u8d1f\u8d23\u7ed3\u679c\u8d28\u91cf\u3001\u6d4b\u8bd5\u9a8c\u8bc1\uff0cAI\u4f1a\u653e\u5927\u5f00\u53d1\u5b9e\u8df5\uff0c\u4e0d\u826f\u6d41\u7a0b\u5c06\u5bfc\u81f4\u6280\u672f\u503a\u52a1\u5feb\u901f\u7d2f\u79ef", "motivation": "\u63a2\u8ba8AI\u4ee3\u7801\u4ee3\u7406\u5728\u5b8c\u6210\u5927\u90e8\u5206\u7f16\u7801\u5de5\u4f5c\u540e\uff0c\u4eba\u7c7b\u5f00\u53d1\u8005\u7684\u89d2\u8272\u8f6c\u53d8\u548c\u8d23\u4efb\u95ee\u9898\uff0c\u4ee5\u53caAI\u5982\u4f55\u653e\u5927\u73b0\u6709\u5f00\u53d1\u5b9e\u8df5\u7684\u4f18\u52a3", "method": "\u901a\u8fc7\u5206\u6790AI\u4ee3\u7801\u4ee3\u7406\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u8ba8\u8bba\u4eba\u7c7b\u4e0eAI\u5728\u7f16\u7801\u8fc7\u7a0b\u4e2d\u7684\u8d23\u4efb\u5206\u914d\u548c\u534f\u4f5c\u6a21\u5f0f", "result": "AI\u6210\u529f\u5b8c\u6210\u5927\u90e8\u5206\u7f16\u7801\u4efb\u52a1\u53ef\u80fd\u5bfc\u81f4\u5f00\u53d1\u8005\u505c\u6b62\u68c0\u67e5\u8f93\u51fa\uff0c\u4ece\u800c\u653e\u5927\u4e0d\u826f\u5f00\u53d1\u5b9e\u8df5\uff0c\u52a0\u901f\u6280\u672f\u503a\u52a1\u79ef\u7d2f", "conclusion": "AI\u4ee3\u7801\u4ee3\u7406\u662f\u5f00\u53d1\u5b9e\u8df5\u7684\u653e\u5927\u5668\uff0c\u9700\u8981\u5efa\u7acb\u65b0\u7684\u8d28\u91cf\u4fdd\u8bc1\u673a\u5236\u548c\u8d23\u4efb\u6846\u67b6\u6765\u5e94\u5bf980%\u95ee\u9898", "topic": "agent analysis"}}
{"id": "tldr.2601.d21395e3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fexamples%3Futm_source=tldr%26utm_medium=sponsorship%26utm_campaign=tldr_dev/2/0100019c09a85cd6-32f217b1-61bb-4cfb-af76-7432261123e0-000000/zfeoC4vTLfAMrmBXf0mVWwbJVHMVnGuU6tNdBrD2DuY=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fexamples%3Futm_source=tldr%26utm_medium=sponsorship%26utm_campaign=tldr_dev/2/0100019c09a85cd6-32f217b1-61bb-4cfb-af76-7432261123e0-000000/zfeoC4vTLfAMrmBXf0mVWwbJVHMVnGuU6tNdBrD2DuY=442", "authors": ["TLDR Newsletter"], "title": "See real PRs where Greptile caught the bug", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fexamples%3Futm_source=tldr%26utm_medium=sponsorship%26utm_campaign=tldr_dev/2/0100019c09a85cd6-32f217b1-61bb-4cfb-af76-7432261123e0-000000/zfeoC4vTLfAMrmBXf0mVWwbJVHMVnGuU6tNdBrD2DuY=442", "summary": "See real PRs where Greptile caught the bug (Sponsor) AI is writing more code, but code review is still a bottleneck for quality and velocity.Greptile reviews each PR with full repo context and learns your team's conventions from comments, reactions, and what actually gets merged. \u2705 Trusted by engineering teams at ScaleAI, Zapier, and Whoop. \ud83e\udde9 Now integrated with Claude Code: install via /plugin. \ud83c\udfaf Free for open source. Try Greptile free for 14 days", "source": "tldr", "AI": {"tldr": "Greptile\u662f\u4e00\u6b3eAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u901a\u8fc7\u5206\u6790\u5b8c\u6574\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u548c\u5b66\u4e60\u56e2\u961f\u89c4\u8303\u6765\u81ea\u52a8\u5ba1\u67e5PR\uff0c\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u548c\u5f00\u53d1\u901f\u5ea6", "motivation": "\u968f\u7740AI\u7f16\u5199\u4ee3\u7801\u8d8a\u6765\u8d8a\u591a\uff0c\u4ee3\u7801\u5ba1\u67e5\u6210\u4e3a\u8d28\u91cf\u548c\u5f00\u53d1\u901f\u5ea6\u7684\u74f6\u9888\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898", "method": "\u4f7f\u7528AI\u5206\u6790\u5b8c\u6574\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\uff0c\u4ece\u56e2\u961f\u8bc4\u8bba\u3001\u53cd\u5e94\u548c\u5b9e\u9645\u5408\u5e76\u7684\u4ee3\u7801\u4e2d\u5b66\u4e60\u56e2\u961f\u89c4\u8303\uff0c\u81ea\u52a8\u5ba1\u67e5\u6bcf\u4e2aPR", "result": "\u5df2\u88abScaleAI\u3001Zapier\u3001Whoop\u7b49\u5de5\u7a0b\u56e2\u961f\u4fe1\u4efb\u4f7f\u7528\uff0c\u73b0\u5df2\u96c6\u6210\u5230Claude Code\u63d2\u4ef6\u4e2d", "conclusion": "Greptile\u63d0\u4f9b14\u5929\u514d\u8d39\u8bd5\u7528\uff0c\u5f00\u6e90\u9879\u76ee\u514d\u8d39\u4f7f\u7528\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4ee3\u7801\u5ba1\u67e5\u6548\u7387", "topic": "swe application"}}
{"id": "tldr.2601.160e32d5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyo.substack.com%2Fp%2Fthe-80-problem-in-agentic-coding%3Futm_source=tldrdev/1/0100019c09a85cd6-32f217b1-61bb-4cfb-af76-7432261123e0-000000/wiKRpjl3PqrXtSymdapM2BtTg6qqz8P99qthG4WRCao=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyo.substack.com%2Fp%2Fthe-80-problem-in-agentic-coding%3Futm_source=tldrdev/1/0100019c09a85cd6-32f217b1-61bb-4cfb-af76-7432261123e0-000000/wiKRpjl3PqrXtSymdapM2BtTg6qqz8P99qthG4WRCao=442", "authors": ["TLDR Newsletter"], "title": "The 80% Problem in Agentic Coding", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 19 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyo.substack.com%2Fp%2Fthe-80-problem-in-agentic-coding%3Futm_source=tldrdev/1/0100019c09a85cd6-32f217b1-61bb-4cfb-af76-7432261123e0-000000/wiKRpjl3PqrXtSymdapM2BtTg6qqz8P99qthG4WRCao=442", "summary": "The 80% Problem in Agentic Coding (19 minute read) Many engineers now use AI agents to write 80-100% of their code, transforming their role from direct implementers to orchestrators. This adoption has introduced new problems, such as conceptual AI errors, comprehension debt within codebases, and a productivity paradox where review times balloon. Engineers need to shift to a declarative approach, focusing on problem definition and architectural oversight.", "source": "tldr", "AI": {"tldr": "\u5de5\u7a0b\u5e08\u4f7f\u7528AI\u4ee3\u7406\u7f16\u519980-100%\u4ee3\u7801\uff0c\u89d2\u8272\u4ece\u76f4\u63a5\u5b9e\u73b0\u8005\u8f6c\u53d8\u4e3a\u534f\u8c03\u8005\uff0c\u4f46\u5e26\u6765\u4e86\u6982\u5ff5\u6027AI\u9519\u8bef\u3001\u4ee3\u7801\u7406\u89e3\u503a\u52a1\u548c\u751f\u4ea7\u529b\u6096\u8bba\u7b49\u65b0\u95ee\u9898\uff0c\u9700\u8981\u8f6c\u5411\u58f0\u660e\u5f0f\u65b9\u6cd5\u548c\u67b6\u6784\u76d1\u7763\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u4ee3\u7801\u7f16\u5199\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff08\u8986\u76d680-100%\u4ee3\u7801\uff09\uff0c\u5de5\u7a0b\u5e08\u89d2\u8272\u53d1\u751f\u6839\u672c\u8f6c\u53d8\uff0c\u4ece\u76f4\u63a5\u7f16\u7801\u8005\u53d8\u4e3a\u534f\u8c03\u8005\uff0c\u8fd9\u5f15\u53d1\u4e86\u65b0\u7684\u6311\u6218\u9700\u8981\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u5de5\u7a0b\u5e08\u9700\u8981\u8f6c\u5411\u58f0\u660e\u5f0f\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u95ee\u9898\u5b9a\u4e49\u548c\u67b6\u6784\u76d1\u7763\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u5b9e\u73b0\u7ec6\u8282\u3002", "result": "AI\u4ee3\u7406\u7684\u5e7f\u6cdb\u4f7f\u7528\u5e26\u6765\u4e86\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u6982\u5ff5\u6027AI\u9519\u8bef\uff1b2\uff09\u4ee3\u7801\u5e93\u4e2d\u7684\u7406\u89e3\u503a\u52a1\uff1b3\uff09\u751f\u4ea7\u529b\u6096\u8bba\uff08\u5ba1\u67e5\u65f6\u95f4\u5927\u5e45\u589e\u52a0\uff09\u3002", "conclusion": "\u5de5\u7a0b\u5e08\u9700\u8981\u9002\u5e94\u65b0\u7684\u89d2\u8272\u5b9a\u4f4d\uff0c\u91c7\u7528\u58f0\u660e\u5f0f\u7f16\u7a0b\u601d\u7ef4\uff0c\u52a0\u5f3a\u95ee\u9898\u5b9a\u4e49\u548c\u67b6\u6784\u76d1\u7763\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9AI\u4ee3\u7406\u5e26\u6765\u7684\u65b0\u6311\u6218\u3002", "topic": "code agent"}}
{"id": "tldr.2601.2fa7e401", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Fproducts-and-platforms%2Fproducts%2Fchrome%2Fgemini-3-auto-browse%2F%3Futm_source=tldrdev/1/0100019c09a85cd6-32f217b1-61bb-4cfb-af76-7432261123e0-000000/K2W_GHwec5zNJDlwV35przM5kBQX-0YINGKt4Rh4uxM=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Fproducts-and-platforms%2Fproducts%2Fchrome%2Fgemini-3-auto-browse%2F%3Futm_source=tldrdev/1/0100019c09a85cd6-32f217b1-61bb-4cfb-af76-7432261123e0-000000/K2W_GHwec5zNJDlwV35przM5kBQX-0YINGKt4Rh4uxM=442", "authors": ["TLDR Newsletter"], "title": "The new era of browsing: Putting Gemini to work in Chrome", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Fproducts-and-platforms%2Fproducts%2Fchrome%2Fgemini-3-auto-browse%2F%3Futm_source=tldrdev/1/0100019c09a85cd6-32f217b1-61bb-4cfb-af76-7432261123e0-000000/K2W_GHwec5zNJDlwV35przM5kBQX-0YINGKt4Rh4uxM=442", "summary": "The new era of browsing: Putting Gemini to work in Chrome (7 minute read) Google is integrating major Gemini AI updates into Chrome, including a new side panel for multitasking across tabs and deeper integrations with other Google apps like Gmail and Calendar. For AI Pro and Ultra subscribers, Chrome now has \"auto browse,\" an agentic feature that automatically browses the web to handle complex multi-step tasks such as vacation planning, online forms, and smart shopping.", "source": "tldr", "AI": {"tldr": "Google\u5c06Gemini AI\u6df1\u5ea6\u96c6\u6210\u5230Chrome\u6d4f\u89c8\u5668\uff0c\u65b0\u589e\u4fa7\u8fb9\u680f\u652f\u6301\u591a\u6807\u7b7e\u9875\u64cd\u4f5c\uff0c\u5e76\u4e3aAI Pro\u548cUltra\u8ba2\u9605\u8005\u63d0\u4f9b\"\u81ea\u52a8\u6d4f\u89c8\"\u529f\u80fd\uff0c\u53ef\u81ea\u4e3b\u5904\u7406\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u3002", "motivation": "\u63d0\u5347Chrome\u6d4f\u89c8\u5668\u7684\u667a\u80fd\u5316\u6c34\u5e73\uff0c\u901a\u8fc7AI\u589e\u5f3a\u7528\u6237\u4f53\u9a8c\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u7f51\u9875\u6d4f\u89c8\u548c\u591a\u4efb\u52a1\u5904\u7406\uff0c\u7279\u522b\u662f\u9488\u5bf9\u590d\u6742\u4efb\u52a1\u573a\u666f\u3002", "method": "\u5c06Gemini AI\u6df1\u5ea6\u96c6\u6210\u5230Chrome\u6d4f\u89c8\u5668\u67b6\u6784\u4e2d\uff0c\u65b0\u589e\u4fa7\u8fb9\u680f\u754c\u9762\u652f\u6301\u8de8\u6807\u7b7e\u9875\u64cd\u4f5c\uff0c\u4e0eGmail\u3001Calendar\u7b49Google\u5e94\u7528\u6df1\u5ea6\u6574\u5408\uff0c\u5e76\u4e3a\u9ad8\u7ea7\u7528\u6237\u63d0\u4f9b\u81ea\u4e3b\u6d4f\u89c8\u4ee3\u7406\u529f\u80fd\u3002", "result": "Chrome\u6d4f\u89c8\u5668\u83b7\u5f97\u4e86\u5f3a\u5927\u7684AI\u529f\u80fd\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u4fa7\u8fb9\u680f\u8fdb\u884c\u591a\u4efb\u52a1\u5904\u7406\uff0c\u9ad8\u7ea7\u8ba2\u9605\u8005\u80fd\u591f\u4f7f\u7528\u81ea\u52a8\u6d4f\u89c8\u529f\u80fd\u5904\u7406\u590d\u6742\u4efb\u52a1\u5982\u5047\u671f\u89c4\u5212\u3001\u5728\u7ebf\u8868\u5355\u586b\u5199\u548c\u667a\u80fd\u8d2d\u7269\u3002", "conclusion": "Chrome\u6d4f\u89c8\u5668\u7684AI\u96c6\u6210\u6807\u5fd7\u7740\u6d4f\u89c8\u4f53\u9a8c\u7684\u65b0\u65f6\u4ee3\uff0c\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u529f\u80fd\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u5904\u7406\u80fd\u529b\u3002", "topic": "swe application"}}
{"id": "tldr.2601.2638ac2e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fveen.com%2Fjeff%2Farchives%2Fcoding-agents-design.html%3Futm_source=tldrdesign/1/0100019c09dd2cde-101e364a-601d-4cb1-a361-e1c3e80d335f-000000/QjfrbndhRkS0l8D__gSmkyBpiPKhHSRNG7gITTzXqFc=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fveen.com%2Fjeff%2Farchives%2Fcoding-agents-design.html%3Futm_source=tldrdesign/1/0100019c09dd2cde-101e364a-601d-4cb1-a361-e1c3e80d335f-000000/QjfrbndhRkS0l8D__gSmkyBpiPKhHSRNG7gITTzXqFc=442", "authors": ["TLDR Newsletter"], "title": "On Coding Agents and the Future of Design", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fveen.com%2Fjeff%2Farchives%2Fcoding-agents-design.html%3Futm_source=tldrdesign/1/0100019c09dd2cde-101e364a-601d-4cb1-a361-e1c3e80d335f-000000/QjfrbndhRkS0l8D__gSmkyBpiPKhHSRNG7gITTzXqFc=442", "summary": "On Coding Agents and the Future of Design (5 minute read) Coding agents like Claude Code represent a new phase of responsive design by stripping applications down to their essential primitives. This shift forces organizations to clarify what their products actually do, much like mobile-first design revealed that simpler interfaces often better serve user needs. This agent-driven future elevates design to pure strategy, where competitive advantage comes from honestly expressing capabilities ra...", "source": "tldr", "AI": {"tldr": "\u7f16\u7801\u4ee3\u7406\uff08\u5982Claude Code\uff09\u901a\u8fc7\u5c06\u5e94\u7528\u7b80\u5316\u4e3a\u57fa\u672c\u539f\u8bed\uff0c\u63a8\u52a8\u8bbe\u8ba1\u8fdb\u5165\u65b0\u9636\u6bb5\uff0c\u4f7f\u8bbe\u8ba1\u8f6c\u53d8\u4e3a\u7eaf\u7cb9\u7684\u6218\u7565\u51b3\u7b56\uff0c\u7ade\u4e89\u4f18\u52bf\u6765\u81ea\u4e8e\u8bda\u5b9e\u5730\u8868\u8fbe\u4ea7\u54c1\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8\u7f16\u7801\u4ee3\u7406\u5982\u4f55\u6539\u53d8\u8bbe\u8ba1\u8303\u5f0f\uff0c\u7c7b\u4f3c\u4e8e\u79fb\u52a8\u4f18\u5148\u8bbe\u8ba1\u63ed\u793a\u7b80\u5355\u754c\u9762\u66f4\u597d\u670d\u52a1\u7528\u6237\u9700\u6c42\uff0c\u7f16\u7801\u4ee3\u7406\u8feb\u4f7f\u7ec4\u7ec7\u6f84\u6e05\u4ea7\u54c1\u6838\u5fc3\u529f\u80fd\uff0c\u63a8\u52a8\u8bbe\u8ba1\u5411\u6218\u7565\u5c42\u9762\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u7c7b\u6bd4\u79fb\u52a8\u4f18\u5148\u8bbe\u8ba1\u9769\u547d\uff0c\u5206\u6790\u7f16\u7801\u4ee3\u7406\u5982\u4f55\u5c06\u5e94\u7528\u7b80\u5316\u4e3a\u57fa\u672c\u539f\u8bed\uff0c\u8feb\u4f7f\u7ec4\u7ec7\u660e\u786e\u4ea7\u54c1\u6838\u5fc3\u529f\u80fd\uff0c\u5c06\u8bbe\u8ba1\u63d0\u5347\u4e3a\u7eaf\u7cb9\u7684\u6218\u7565\u51b3\u7b56\u3002", "result": "\u7f16\u7801\u4ee3\u7406\u63a8\u52a8\u8bbe\u8ba1\u8fdb\u5165\u65b0\u9636\u6bb5\uff0c\u4f7f\u8bbe\u8ba1\u4ece\u754c\u9762\u5b9e\u73b0\u8f6c\u53d8\u4e3a\u6218\u7565\u8868\u8fbe\uff0c\u7ade\u4e89\u4f18\u52bf\u6765\u81ea\u4e8e\u8bda\u5b9e\u5730\u5c55\u793a\u4ea7\u54c1\u80fd\u529b\u800c\u975e\u754c\u9762\u590d\u6742\u6027\u3002", "conclusion": "\u7f16\u7801\u4ee3\u7406\u4ee3\u8868\u8bbe\u8ba1\u7684\u65b0\u9636\u6bb5\uff0c\u5c06\u8bbe\u8ba1\u63d0\u5347\u4e3a\u7eaf\u7cb9\u7684\u6218\u7565\uff0c\u7ec4\u7ec7\u9700\u8981\u6e05\u6670\u8868\u8fbe\u4ea7\u54c1\u6838\u5fc3\u80fd\u529b\uff0c\u8fd9\u5c06\u6210\u4e3a\u672a\u6765\u7684\u7ade\u4e89\u4f18\u52bf\u3002", "topic": "code agent"}}
{"id": "tldr.2601.aed6b9a7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnl20250129/6/0100019c09dd2cde-101e364a-601d-4cb1-a361-e1c3e80d335f-000000/jQwVHIuf_l0znAh8a3fgO99XPxEceAtuagPIw8FIp4g=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnl20250129/6/0100019c09dd2cde-101e364a-601d-4cb1-a361-e1c3e80d335f-000000/jQwVHIuf_l0znAh8a3fgO99XPxEceAtuagPIw8FIp4g=442", "authors": ["TLDR Newsletter"], "title": "It gets lonely waiting in backlog", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnl20250129/6/0100019c09dd2cde-101e364a-601d-4cb1-a361-e1c3e80d335f-000000/jQwVHIuf_l0znAh8a3fgO99XPxEceAtuagPIw8FIp4g=442", "summary": "It gets lonely waiting in backlog (Sponsor) You designed a great prototype\u2026three months ago. Now it's wasting away in backlog and you're wondering why you even tried. With v0, every prompt generates production-ready code in a real git repository - so your frontend engineering friends have no more excuses. Start creating on v0", "source": "tldr", "AI": {"tldr": "v0\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff0c\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u76f4\u63a5\u8f6c\u6362\u4e3a\u751f\u4ea7\u5c31\u7eea\u7684\u4ee3\u7801\u5e76\u90e8\u7f72\u5230\u771f\u5b9e\u7684Git\u4ed3\u5e93\u4e2d\uff0c\u65e8\u5728\u89e3\u51b3\u539f\u578b\u5f00\u53d1\u540e\u56e0\u79ef\u538b\u800c\u65e0\u6cd5\u843d\u5730\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u524d\u7aef\u5f00\u53d1\u4e2d\u5e38\u89c1\u7684\u539f\u578b\u8bbe\u8ba1\u5b8c\u6210\u540e\u957f\u671f\u79ef\u538b\u5728\u5f85\u529e\u5217\u8868\u4e2d\u65e0\u6cd5\u843d\u5730\u7684\u95ee\u9898\uff0c\u6d88\u9664\u524d\u7aef\u5de5\u7a0b\u5e08\u56e0\u5404\u79cd\u501f\u53e3\u800c\u62d6\u5ef6\u5c06\u539f\u578b\u8f6c\u5316\u4e3a\u5b9e\u9645\u4ea7\u54c1\u7684\u969c\u788d\u3002", "method": "\u901a\u8fc7AI\u6280\u672f\u5c06\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u76f4\u63a5\u8f6c\u6362\u4e3a\u751f\u4ea7\u5c31\u7eea\u7684\u4ee3\u7801\uff0c\u5e76\u81ea\u52a8\u90e8\u7f72\u5230\u771f\u5b9e\u7684Git\u4ee3\u7801\u4ed3\u5e93\u4e2d\uff0c\u5b9e\u73b0\u4ece\u60f3\u6cd5\u5230\u53ef\u8fd0\u884c\u4ea7\u54c1\u7684\u5feb\u901f\u8f6c\u5316\u3002", "result": "v0\u5de5\u5177\u80fd\u591f\u6839\u636e\u7528\u6237\u63d0\u793a\u751f\u6210\u53ef\u76f4\u63a5\u7528\u4e8e\u751f\u4ea7\u7684\u4ee3\u7801\uff0c\u5e76\u81ea\u52a8\u521b\u5efaGit\u4ed3\u5e93\uff0c\u4f7f\u524d\u7aef\u5de5\u7a0b\u5e08\u65e0\u6cd5\u518d\u4ee5\u5404\u79cd\u6280\u672f\u501f\u53e3\u62d6\u5ef6\u9879\u76ee\u843d\u5730\u3002", "conclusion": "v0\u901a\u8fc7AI\u9a71\u52a8\u7684\u4ee3\u7801\u751f\u6210\u548c\u81ea\u52a8\u5316\u90e8\u7f72\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u539f\u578b\u79ef\u538b\u95ee\u9898\uff0c\u52a0\u901f\u4e86\u4ece\u521b\u610f\u5230\u5b9e\u9645\u4ea7\u54c1\u7684\u8f6c\u5316\u8fc7\u7a0b\u3002", "topic": "code agent"}}
{"id": "tldr.2601.509bdb4e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.akashbajwa.co%2Fp%2Fbuilding-ramp-sheets-ramp-labs-and%3Futm_source=tldrfintech/1/0100019c0a1512b1-baebb25e-a90d-440c-9f6d-c1e631c82a01-000000/Jdn-8nbTbwqC_LHV-48klgntnuusNwTbXQL8mv6YY98=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.akashbajwa.co%2Fp%2Fbuilding-ramp-sheets-ramp-labs-and%3Futm_source=tldrfintech/1/0100019c0a1512b1-baebb25e-a90d-440c-9f6d-c1e631c82a01-000000/Jdn-8nbTbwqC_LHV-48klgntnuusNwTbXQL8mv6YY98=442", "authors": ["TLDR Newsletter"], "title": "Building Ramp Sheets: Ramp Labs on spreadsheet agents and shipping velocity", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.akashbajwa.co%2Fp%2Fbuilding-ramp-sheets-ramp-labs-and%3Futm_source=tldrfintech/1/0100019c0a1512b1-baebb25e-a90d-440c-9f6d-c1e631c82a01-000000/Jdn-8nbTbwqC_LHV-48klgntnuusNwTbXQL8mv6YY98=442", "summary": "Building Ramp Sheets: Ramp Labs on spreadsheet agents and shipping velocity (8 minute read) Ramp Labs leaders Alex Stauffer and Alex Shevchenko explain why spreadsheets are the ideal interface for high-stakes finance, how Ramp Sheets pairs agentic AI with Excel-native trust and auditability, and how Ramp's applied-AI culture optimizes for speed, design, and real user feedback over rigid roadmaps.", "source": "tldr", "AI": {"tldr": "Ramp Labs\u5f00\u53d1\u4e86Ramp Sheets\uff0c\u8fd9\u662f\u4e00\u4e2a\u5c06\u667a\u80fdAI\u4ee3\u7406\u4e0eExcel\u539f\u751f\u4fe1\u4efb\u548c\u5ba1\u8ba1\u529f\u80fd\u7ed3\u5408\u7684\u7535\u5b50\u8868\u683c\u4ee3\u7406\u7cfb\u7edf\uff0c\u4e13\u95e8\u4e3a\u9ad8\u98ce\u9669\u91d1\u878d\u573a\u666f\u8bbe\u8ba1\u3002", "motivation": "\u7535\u5b50\u8868\u683c\u662f\u9ad8\u98ce\u9669\u91d1\u878d\u573a\u666f\u7684\u7406\u60f3\u754c\u9762\uff0c\u4f46\u9700\u8981\u7ed3\u5408AI\u80fd\u529b\u6765\u63d0\u5347\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301Excel\u539f\u751f\u7684\u4fe1\u4efb\u548c\u5ba1\u8ba1\u7279\u6027\u3002", "method": "\u5c06\u667a\u80fdAI\u4ee3\u7406\u6280\u672f\u4e0eExcel\u7535\u5b50\u8868\u683c\u7ed3\u5408\uff0c\u521b\u5efaRamp Sheets\u7cfb\u7edf\uff0c\u901a\u8fc7\u5e94\u7528AI\u6587\u5316\u4f18\u5316\u901f\u5ea6\u3001\u8bbe\u8ba1\u548c\u771f\u5b9e\u7528\u6237\u53cd\u9988\u3002", "result": "\u5f00\u53d1\u4e86\u4e13\u95e8\u4e3a\u91d1\u878d\u573a\u666f\u8bbe\u8ba1\u7684\u7535\u5b50\u8868\u683c\u4ee3\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86AI\u667a\u80fd\u548cExcel\u7684\u4fe1\u4efb\u5ba1\u8ba1\u529f\u80fd\u3002", "conclusion": "Ramp Sheets\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u667a\u80fdAI\u4ee3\u7406\u4e0e\u7535\u5b50\u8868\u683c\u7ed3\u5408\uff0c\u4e3a\u9ad8\u98ce\u9669\u91d1\u878d\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u4fe1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.0cd6ae2d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0a1512b1-baebb25e-a90d-440c-9f6d-c1e631c82a01-000000/rLZgU_Q3Zkg5AvxGiOEtpAVWuCTzBtxsz407eiA1XJQ=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0a1512b1-baebb25e-a90d-440c-9f6d-c1e631c82a01-000000/rLZgU_Q3Zkg5AvxGiOEtpAVWuCTzBtxsz407eiA1XJQ=442", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0a1512b1-baebb25e-a90d-440c-9f6d-c1e631c82a01-000000/rLZgU_Q3Zkg5AvxGiOEtpAVWuCTzBtxsz407eiA1XJQ=442", "summary": "Building Ramp Sheets: Ramp Labs on spreadsheet agents and shipping velocity (8 minute read) Ramp Labs leaders Alex Stauffer and Alex Shevchenko explain why spreadsheets are the ideal interface for high-stakes finance, how Ramp Sheets pairs agentic AI with Excel-native trust and auditability, and how Ramp's applied-AI culture optimizes for speed, design, and real user feedback over rigid roadmaps.", "source": "tldr", "AI": {"tldr": "Ramp Labs\u5f00\u53d1\u4e86Ramp Sheets\uff0c\u5c06\u667a\u80fdAI\u4ee3\u7406\u4e0eExcel\u539f\u751f\u4fe1\u4efb\u548c\u5ba1\u8ba1\u529f\u80fd\u7ed3\u5408\uff0c\u4e3a\u9ad8\u98ce\u9669\u91d1\u878d\u63d0\u4f9b\u7406\u60f3\u7684\u7535\u5b50\u8868\u683c\u754c\u9762\u3002", "motivation": "\u7535\u5b50\u8868\u683c\u662f\u9ad8\u98ce\u9669\u91d1\u878d\u7684\u7406\u60f3\u754c\u9762\uff0c\u4f46\u9700\u8981\u7ed3\u5408AI\u80fd\u529b\u3002Ramp\u5e0c\u671b\u5c06\u667a\u80fdAI\u4ee3\u7406\u4e0eExcel\u539f\u751f\u7684\u4fe1\u4efb\u548c\u5ba1\u8ba1\u529f\u80fd\u7ed3\u5408\uff0c\u4f18\u5316\u5f00\u53d1\u901f\u5ea6\u3001\u8bbe\u8ba1\u548c\u771f\u5b9e\u7528\u6237\u53cd\u9988\u3002", "method": "Ramp Sheets\u5c06\u667a\u80fdAI\u4ee3\u7406\u4e0eExcel\u539f\u751f\u529f\u80fd\u914d\u5bf9\uff0c\u901a\u8fc7Ramp\u7684\u5e94\u7528AI\u6587\u5316\uff0c\u4f18\u5148\u8003\u8651\u901f\u5ea6\u3001\u8bbe\u8ba1\u548c\u771f\u5b9e\u7528\u6237\u53cd\u9988\uff0c\u800c\u975e\u50f5\u5316\u7684\u8def\u7ebf\u56fe\u3002", "result": "\u5f00\u53d1\u4e86Ramp Sheets\u4ea7\u54c1\uff0c\u5c06\u667a\u80fdAI\u80fd\u529b\u4e0e\u7535\u5b50\u8868\u683c\u7684\u4fe1\u4efb\u548c\u5ba1\u8ba1\u529f\u80fd\u7ed3\u5408\uff0c\u4e3a\u91d1\u878d\u9886\u57df\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u7535\u5b50\u8868\u683c\u4e0e\u667a\u80fdAI\u4ee3\u7406\u7684\u7ed3\u5408\u4e3a\u9ad8\u98ce\u9669\u91d1\u878d\u63d0\u4f9b\u4e86\u7406\u60f3\u7684\u89e3\u51b3\u65b9\u6848\uff0cRamp\u7684\u5e94\u7528AI\u6587\u5316\u786e\u4fdd\u4e86\u4ea7\u54c1\u7684\u5feb\u901f\u5f00\u53d1\u548c\u7528\u6237\u5bfc\u5411\u3002", "topic": "code agent"}}
{"id": "tldr.2601.d5414a8d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Fnew-sandbox-escape-flaw-exposes-n8n-instances-to-rce-attacks%2F%3Futm_source=tldrinfosec/1/0100019c0a154664-9526f8ca-f022-4bc3-8c65-d1baad275928-000000/o84Vl1W2HNm9SH1J4CzHa_Rd8YMgDnuH8eGWl4t44sY=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Fnew-sandbox-escape-flaw-exposes-n8n-instances-to-rce-attacks%2F%3Futm_source=tldrinfosec/1/0100019c0a154664-9526f8ca-f022-4bc3-8c65-d1baad275928-000000/o84Vl1W2HNm9SH1J4CzHa_Rd8YMgDnuH8eGWl4t44sY=442", "authors": ["TLDR Newsletter"], "title": "New Sandbox Escape Flaw Exposes n8n Instances to RCE Attacks", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Fnew-sandbox-escape-flaw-exposes-n8n-instances-to-rce-attacks%2F%3Futm_source=tldrinfosec/1/0100019c0a154664-9526f8ca-f022-4bc3-8c65-d1baad275928-000000/o84Vl1W2HNm9SH1J4CzHa_Rd8YMgDnuH8eGWl4t44sY=442", "summary": "New Sandbox Escape Flaw Exposes n8n Instances to RCE Attacks (2 minute read) Researchers from JFrog discovered two new vulnerabilities in the n8n workflow automation platform that could allow for RCE, one of which received a CVSS score of 9.9. The two vulnerabilities are AST sandbox escapes in both Python and JavaScript sandboxes that could allow for arbitrary code execution on the underlying nodes. The vulnerability has been fixed in the n8n cloud platform, but users running self-hosted vers...", "source": "tldr", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5728n8n\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5e73\u53f0\u4e2d\u53d1\u73b0\u4e24\u4e2a\u65b0\u7684AST\u6c99\u7bb1\u9003\u9038\u6f0f\u6d1e\uff0c\u53ef\u5bfc\u81f4\u8fdc\u7a0b\u4ee3\u7801\u6267\u884c\uff0c\u5176\u4e2d\u4e00\u4e2aCVSS\u8bc4\u52069.9", "motivation": "n8n\u4f5c\u4e3a\u6d41\u884c\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u5176\u6c99\u7bb1\u673a\u5236\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u5229\u7528\u8fd9\u4e9b\u6f0f\u6d1e\u5728\u5e95\u5c42\u8282\u70b9\u4e0a\u6267\u884c\u4efb\u610f\u4ee3\u7801\uff0c\u5bf9\u7528\u6237\u7cfb\u7edf\u9020\u6210\u4e25\u91cd\u5a01\u80c1", "method": "\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u5206\u6790n8n\u5e73\u53f0\u7684Python\u548cJavaScript\u6c99\u7bb1\u5b9e\u73b0\uff0c\u53d1\u73b0\u4e86AST\uff08\u62bd\u8c61\u8bed\u6cd5\u6811\uff09\u6c99\u7bb1\u9003\u9038\u6f0f\u6d1e\uff0c\u8fd9\u4e9b\u6f0f\u6d1e\u5141\u8bb8\u653b\u51fb\u8005\u7ed5\u8fc7\u6c99\u7bb1\u9650\u5236", "result": "\u53d1\u73b0\u4e24\u4e2a\u9ad8\u5371\u6f0f\u6d1e\uff0c\u5176\u4e2d\u4e00\u4e2a\u83b7\u5f97CVSS 9.9\u7684\u9ad8\u5206\uff0cn8n\u4e91\u5e73\u53f0\u5df2\u4fee\u590d\u6f0f\u6d1e\uff0c\u4f46\u81ea\u6258\u7ba1\u7248\u672c\u7528\u6237\u9700\u8981\u624b\u52a8\u66f4\u65b0", "conclusion": "\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u5e73\u53f0\u7684\u6c99\u7bb1\u5b89\u5168\u673a\u5236\u9700\u8981\u66f4\u4e25\u683c\u7684\u5ba1\u67e5\uff0c\u7528\u6237\u5e94\u53ca\u65f6\u66f4\u65b0\u81ea\u6258\u7ba1\u7248\u672c\u4ee5\u9632\u8303\u6f5c\u5728\u653b\u51fb", "topic": "swe application"}}
{"id": "tldr.2601.a2fa480b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftailscale.com%2Fblog%2Faperture-private-alpha%3Futm_source=tldrinfosec/1/0100019c0a154664-9526f8ca-f022-4bc3-8c65-d1baad275928-000000/3LgZNm3KKNl_qHEBJ_aBaDN5KOix793GOoxRYd2k5Lw=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftailscale.com%2Fblog%2Faperture-private-alpha%3Futm_source=tldrinfosec/1/0100019c0a154664-9526f8ca-f022-4bc3-8c65-d1baad275928-000000/3LgZNm3KKNl_qHEBJ_aBaDN5KOix793GOoxRYd2k5Lw=442", "authors": ["TLDR Newsletter"], "title": "A first look at Aperture by Tailscale", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftailscale.com%2Fblog%2Faperture-private-alpha%3Futm_source=tldrinfosec/1/0100019c0a154664-9526f8ca-f022-4bc3-8c65-d1baad275928-000000/3LgZNm3KKNl_qHEBJ_aBaDN5KOix793GOoxRYd2k5Lw=442", "summary": "A first look at Aperture by Tailscale (private alpha) (6 minute read) Aperture is an alpha-stage AI gateway that offers insights into coding agent usage within organizations. It eliminates the need to share API keys by using Tailscale's built-in identity system for authenticating users and machines. The tool supports CLI and VS Code-based AI coding agents such as Claude Code, Codex, and Gemini CLI, providing centralized monitoring of usage, cost tracking across different models and providers,...", "source": "tldr", "AI": {"tldr": "Aperture\u662fTailscale\u5f00\u53d1\u7684AI\u7f51\u5173\u5de5\u5177\uff0c\u5904\u4e8ealpha\u9636\u6bb5\uff0c\u7528\u4e8e\u76d1\u63a7\u7ec4\u7ec7\u5185\u7684\u4ee3\u7801\u4ee3\u7406\u4f7f\u7528\u60c5\u51b5\uff0c\u63d0\u4f9b\u8eab\u4efd\u9a8c\u8bc1\u3001\u4f7f\u7528\u6d1e\u5bdf\u548c\u6210\u672c\u8ddf\u8e2a\u529f\u80fd\u3002", "motivation": "\u7ec4\u7ec7\u9700\u8981\u66f4\u597d\u5730\u76d1\u63a7\u548c\u7ba1\u7406\u5185\u90e8AI\u4ee3\u7801\u4ee3\u7406\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u907f\u514dAPI\u5bc6\u94a5\u5171\u4eab\u7684\u5b89\u5168\u98ce\u9669\uff0c\u540c\u65f6\u9700\u8981\u96c6\u4e2d\u8ddf\u8e2a\u4e0d\u540c\u6a21\u578b\u548c\u63d0\u4f9b\u5546\u7684\u4f7f\u7528\u6210\u672c\u548c\u6548\u7387\u3002", "method": "\u5229\u7528Tailscale\u5185\u7f6e\u7684\u8eab\u4efd\u7cfb\u7edf\u8fdb\u884c\u7528\u6237\u548c\u673a\u5668\u8ba4\u8bc1\uff0c\u652f\u6301CLI\u548cVS Code\u7684AI\u7f16\u7801\u4ee3\u7406\uff08\u5982Claude Code\u3001Codex\u3001Gemini CLI\uff09\uff0c\u63d0\u4f9b\u96c6\u4e2d\u76d1\u63a7\u4eea\u8868\u677f\u3002", "result": "\u76ee\u524d\u5904\u4e8e\u79c1\u6709alpha\u9636\u6bb5\uff0c\u63d0\u4f9b\u4e86\u6d88\u9664API\u5bc6\u94a5\u5171\u4eab\u9700\u6c42\u3001\u96c6\u4e2d\u76d1\u63a7\u4ee3\u7801\u4ee3\u7406\u4f7f\u7528\u3001\u8de8\u6a21\u578b\u548c\u63d0\u4f9b\u5546\u6210\u672c\u8ddf\u8e2a\u7684\u529f\u80fd\u3002", "conclusion": "Aperture\u4f5c\u4e3aAI\u7f51\u5173\u5de5\u5177\uff0c\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u53ef\u63a7\u7684AI\u4ee3\u7801\u4ee3\u7406\u4f7f\u7528\u73af\u5883\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2601.4b80c96e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442", "authors": ["TLDR Newsletter"], "title": "AI for when it is rocket science", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442", "summary": "AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails \u2014 but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...", "source": "tldr", "AI": {"tldr": "Contextual AI\u5f00\u53d1\u4e86Agent Composer\u5de5\u5177\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u5df2\u5728\u5236\u9020\u4e1a\u548c\u7269\u6d41\u9886\u57df\u5b9e\u73b0\u663e\u8457\u6548\u7387\u63d0\u5347", "motivation": "\u5f53\u524dAI\u5728\u5904\u7406\u590d\u6742\u3001\u4e13\u4e1a\u5316\u5de5\u4f5c\u65f6\u4ecd\u7136\u5b58\u5728\u5c40\u9650\uff0c\u867d\u7136\u80fd\u5904\u7406\u65e5\u5e38\u4efb\u52a1\u5982\u8d77\u8349\u90ae\u4ef6\uff0c\u4f46\u5728\u4e13\u4e1a\u6280\u672f\u9886\u57df\uff08\u5982\u706b\u7bad\u79d1\u5b66\u3001\u70ed\u8bd5\u8f66\u7ed3\u679c\u5206\u6790\uff09\u7684\u53ef\u4fe1\u5ea6\u4e0d\u8db3", "method": "\u5f00\u53d1\u4e86Agent Composer\u5de5\u5177\uff0c\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4efb\u52a1\u8bbe\u8ba1\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u5904\u7406\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\u7b49\u4e13\u4e1a\u6280\u672f\u5de5\u4f5c", "result": "1. \u5148\u8fdb\u5236\u9020\u4f01\u4e1a\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u65f6\u95f4\u4ece8\u5c0f\u65f6\u7f29\u77ed\u81f320\u5206\u949f\uff1b2. \u6280\u672f\u9a71\u52a8\u76843PL\u63d0\u4f9b\u5546\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6548\u7387\u63d0\u5347", "conclusion": "AI\u5728\u590d\u6742\u4e13\u4e1a\u9886\u57df\u5177\u6709\u5de8\u5927\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u5de5\u5177\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4e13\u4e1a\u6280\u672f\u5de5\u4f5c\u7684\u6548\u7387\u548c\u53ef\u9760\u6027", "topic": "code agent"}}
{"id": "tldr.2601.51a6fb41", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442", "authors": ["TLDR Newsletter"], "title": "8 hours to 20 minutes", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442", "summary": "AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails \u2014 but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...", "source": "tldr", "AI": {"tldr": "Contextual AI\u5f00\u53d1\u4e86Agent Composer\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u5df2\u5728\u5236\u9020\u4e1a\u548c\u7269\u6d41\u9886\u57df\u5b9e\u73b0\u663e\u8457\u6548\u7387\u63d0\u5347", "motivation": "\u5f53\u524dAI\u5728\u5904\u7406\u590d\u6742\u3001\u4e13\u4e1a\u5316\u4efb\u52a1\u65f6\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u7684\u9886\u57df\uff08\u5982\u706b\u7bad\u79d1\u5b66\u3001\u9ad8\u7ea7\u6280\u672f\u95ee\u9898\uff09\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4efb\u52a1\u8bbe\u8ba1\u7684AI\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u4e86Agent Composer\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\u7b49\u6280\u672f\uff0c\u4e13\u95e8\u5904\u7406\u590d\u6742\u4efb\u52a1", "result": "1. \u5148\u8fdb\u5236\u9020\u4f01\u4e1a\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u4ece8\u5c0f\u65f6\u7f29\u77ed\u523020\u5206\u949f\uff1b2. \u6280\u672f\u9a71\u52a8\u76843PL\u63d0\u4f9b\u5546\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c", "conclusion": "Contextual AI\u7684Agent Composer\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u4e13\u4e1a\u5316\u4efb\u52a1\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347", "topic": "code agent"}}
{"id": "tldr.2601.8d59b0cd", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442", "authors": ["TLDR Newsletter"], "title": "60x faster issue resolution", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442", "summary": "AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails \u2014 but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...", "source": "tldr", "AI": {"tldr": "Contextual AI\u5f00\u53d1\u4e86Agent Composer\u5de5\u5177\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\uff0c\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u65f6\u95f4\u4ece8\u5c0f\u65f6\u7f29\u77ed\u523020\u5206\u949f", "motivation": "\u5f53\u524dAI\u5728\u5904\u7406\u590d\u6742\u3001\u4e13\u4e1a\u5316\u5de5\u4f5c\u65f6\u4ecd\u7136\u5b58\u5728\u5c40\u9650\u6027\uff0c\u867d\u7136\u80fd\u5904\u7406\u7b80\u5355\u4efb\u52a1\u5982\u8d77\u8349\u90ae\u4ef6\uff0c\u4f46\u5728\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u7684\u9886\u57df\uff08\u5982\u706b\u7bad\u79d1\u5b66\u3001\u9ad8\u7ea7\u6280\u672f\u95ee\u9898\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u4efb\u52a1\u7684AI\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u4e86Agent Composer\u5de5\u5177\uff0c\u4e13\u95e8\u4e3a\u590d\u6742\u4efb\u52a1\u8bbe\u8ba1\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\u7b49\u6280\u672f\uff0c\u5904\u7406\u4e13\u4e1a\u9886\u57df\u7684\u5de5\u4f5c\u6d41\u7a0b", "result": "\u5728\u5148\u8fdb\u5236\u9020\u9886\u57df\uff0c\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u65f6\u95f4\u4ece8\u5c0f\u65f6\u7f29\u77ed\u523020\u5206\u949f\uff1b\u5728\u6280\u672f\u9a71\u52a8\u7684\u7b2c\u4e09\u65b9\u7269\u6d41\u63d0\u4f9b\u5546\u4e2d\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c", "conclusion": "\u9488\u5bf9\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\u7684\u4e13\u95e8\u5316AI\u5de5\u5177\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\uff0c\u5728\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u7684\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c", "topic": "code agent"}}
{"id": "tldr.2601.b8fd3777", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442", "authors": ["TLDR Newsletter"], "title": "in minutes instead of days", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442", "summary": "AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails \u2014 but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...", "source": "tldr", "AI": {"tldr": "Contextual AI\u5f00\u53d1\u4e86Agent Composer\u5de5\u5177\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u5df2\u5728\u5236\u9020\u4e1a\u548c\u7269\u6d41\u9886\u57df\u5b9e\u73b0\u663e\u8457\u6548\u7387\u63d0\u5347", "motivation": "\u5f53\u524dAI\u5728\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff08\u5982\u706b\u7bad\u79d1\u5b66\u3001\u9ad8\u7ea7\u6280\u672f\u95ee\u9898\uff09\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u5de5\u4f5c\u7684AI\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1\u4e86Agent Composer\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4f20\u611f\u5668\u6570\u636e\u89e3\u6790\u548c\u65e5\u5fd7\u5173\u8054\u7b49\u6280\u672f\uff0c\u4e13\u95e8\u4e3a\u590d\u6742\u4efb\u52a1\u8bbe\u8ba1", "result": "\u5148\u8fdb\u5236\u9020\u5546\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790\u65f6\u95f4\u4ece8\u5c0f\u65f6\u7f29\u77ed\u523020\u5206\u949f\uff1b\u6280\u672f\u9a71\u52a8\u76843PL\u63d0\u4f9b\u5546\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c", "conclusion": "Contextual AI\u7684Agent Composer\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u4e13\u4e1a\u4efb\u52a1\uff0c\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5b9e\u73b0\u91cd\u5927\u6548\u7387\u63d0\u5347", "topic": "code agent"}}
{"id": "tldr.2601.98f5c44c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2026%2F01%2F28%2Fzuckerberg-teases-agentic-commerce-tools-and-major-ai-rollout-in-2026%2F%3Futm_source=tldrai/1/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/roUxrfZ-yTJqrMVXz4P0sSfHKpz9WGZXUH2RxYupHko=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2026%2F01%2F28%2Fzuckerberg-teases-agentic-commerce-tools-and-major-ai-rollout-in-2026%2F%3Futm_source=tldrai/1/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/roUxrfZ-yTJqrMVXz4P0sSfHKpz9WGZXUH2RxYupHko=442", "authors": ["TLDR Newsletter"], "title": "Meta's AI Plans Include Shopping Agents and New Models", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2026%2F01%2F28%2Fzuckerberg-teases-agentic-commerce-tools-and-major-ai-rollout-in-2026%2F%3Futm_source=tldrai/1/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/roUxrfZ-yTJqrMVXz4P0sSfHKpz9WGZXUH2RxYupHko=442", "summary": "Meta's AI Plans Include Shopping Agents and New Models (4 minute read) Meta announced a major AI product rollout for 2026, emphasizing new models and agentic tools focused on commerce. Mark Zuckerberg pointed to personalized AI shopping assistants and Meta's access to user context as key differentiators in delivering tailored agent experiences.", "source": "tldr", "AI": {"tldr": "Meta\u5ba3\u5e032026\u5e74AI\u4ea7\u54c1\u8def\u7ebf\u56fe\uff0c\u91cd\u70b9\u63a8\u51fa\u9762\u5411\u7535\u5546\u7684\u65b0\u6a21\u578b\u548c\u667a\u80fd\u4f53\u5de5\u5177\uff0c\u5305\u62ec\u4e2a\u6027\u5316\u8d2d\u7269\u52a9\u624b", "motivation": "Meta\u5e0c\u671b\u901a\u8fc7\u5229\u7528\u5176\u7528\u6237\u4e0a\u4e0b\u6587\u6570\u636e\u7684\u4f18\u52bf\uff0c\u5728AI\u7535\u5546\u9886\u57df\u5efa\u7acb\u5dee\u5f02\u5316\u7ade\u4e89\u4f18\u52bf\uff0c\u63d0\u4f9b\u66f4\u4e2a\u6027\u5316\u7684\u8d2d\u7269\u4f53\u9a8c", "method": "\u5f00\u53d1\u65b0\u7684AI\u6a21\u578b\u548c\u667a\u80fd\u4f53\u5de5\u5177\uff0c\u4e13\u6ce8\u4e8e\u7535\u5546\u5e94\u7528\u573a\u666f\uff0c\u5229\u7528Meta\u5e73\u53f0\u7684\u7528\u6237\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f", "result": "\u5ba3\u5e03\u4e862026\u5e74\u7684\u4e3b\u8981AI\u4ea7\u54c1\u53d1\u5e03\u8ba1\u5212\uff0c\u5f3a\u8c03\u4e2a\u6027\u5316AI\u8d2d\u7269\u52a9\u624b\u5c06\u6210\u4e3a\u5173\u952e\u5dee\u5f02\u5316\u529f\u80fd", "conclusion": "Meta\u5c06AI\u7535\u5546\u667a\u80fd\u4f53\u4f5c\u4e3a\u6218\u7565\u91cd\u70b9\uff0c\u8ba1\u5212\u901a\u8fc7\u4e2a\u6027\u5316\u8d2d\u7269\u52a9\u624b\u548c\u7528\u6237\u4e0a\u4e0b\u6587\u4f18\u52bf\u5728\u7ade\u4e89\u4e2d\u8131\u9896\u800c\u51fa", "topic": "agent analysis"}}
{"id": "tldr.2601.5550f785", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fresearch.google%2Fblog%2Ftowards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work%2F%3Futm_source=tldrai/1/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/U3VwOfvzCTadfbFwivnuB2vmbZhiw9gEYu2zst11-b4=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fresearch.google%2Fblog%2Ftowards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work%2F%3Futm_source=tldrai/1/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/U3VwOfvzCTadfbFwivnuB2vmbZhiw9gEYu2zst11-b4=442", "authors": ["TLDR Newsletter"], "title": "Towards a science of scaling agent systems: When and why agent systems work", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fresearch.google%2Fblog%2Ftowards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work%2F%3Futm_source=tldrai/1/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/U3VwOfvzCTadfbFwivnuB2vmbZhiw9gEYu2zst11-b4=442", "summary": "Towards a science of scaling agent systems: When and why agent systems work (4 minute read) Smarter models accelerate the need for multi-agent systems. However, the 'more agents' approach often hits a ceiling. It can even degrade performance if not aligned with the specific properties of the task. The next generation of AI agents needs to move from heuristics to quantitative principles to become more numerous, smarter, safer, and efficient.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u89c4\u6a21\u5316\u79d1\u5b66\uff0c\u6307\u51fa\u5355\u7eaf\u589e\u52a0\u667a\u80fd\u4f53\u6570\u91cf\u5b58\u5728\u6027\u80fd\u5929\u82b1\u677f\uff0c\u9700\u8981\u4ece\u542f\u53d1\u5f0f\u65b9\u6cd5\u8f6c\u5411\u91cf\u5316\u539f\u5219\u6765\u6784\u5efa\u66f4\u667a\u80fd\u3001\u5b89\u5168\u3001\u9ad8\u6548\u7684\u4e0b\u4e00\u4ee3AI\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u53d8\u5f97\u66f4\u667a\u80fd\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u9700\u6c42\u52a0\u901f\u589e\u957f\uff0c\u4f46\u5f53\u524d\"\u66f4\u591a\u667a\u80fd\u4f53\"\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u751a\u81f3\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002\u9700\u8981\u5efa\u7acb\u79d1\u5b66\u539f\u5219\u6765\u6307\u5bfc\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u89c4\u6a21\u5316\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u9700\u8981\u4ece\u542f\u53d1\u5f0f\u65b9\u6cd5\u8f6c\u5411\u57fa\u4e8e\u4efb\u52a1\u7279\u5b9a\u5c5e\u6027\u7684\u91cf\u5316\u539f\u5219\uff0c\u5efa\u7acb\u667a\u80fd\u4f53\u7cfb\u7edf\u89c4\u6a21\u5316\u7684\u79d1\u5b66\u6846\u67b6\uff0c\u5206\u6790\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4f55\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5de5\u4f5c\u3002", "result": "\u6307\u51fa\u4e86\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u89c4\u6a21\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u5efa\u7acb\u667a\u80fd\u4f53\u7cfb\u7edf\u89c4\u6a21\u5316\u79d1\u5b66\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3AI\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "\u4e0b\u4e00\u4ee3AI\u667a\u80fd\u4f53\u7cfb\u7edf\u9700\u8981\u57fa\u4e8e\u91cf\u5316\u539f\u5219\u800c\u975e\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u79d1\u5b66\u5316\u7684\u89c4\u6a21\u5316\u65b9\u6cd5\u5b9e\u73b0\u66f4\u667a\u80fd\u3001\u5b89\u5168\u3001\u9ad8\u6548\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.1750b786", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbbaOkP/1/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/Z_Lj_znhViyfLJmFMeYwRF4Pmi1CIAP6SNyaInCgEnk=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbbaOkP/1/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/Z_Lj_znhViyfLJmFMeYwRF4Pmi1CIAP6SNyaInCgEnk=442", "authors": ["TLDR Newsletter"], "title": "Keeping Your Data Safe When an AI Agent Clicks a Link", "comment": "Source: TLDR Newsletter, Date: 2026-01-29, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbbaOkP/1/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/Z_Lj_znhViyfLJmFMeYwRF4Pmi1CIAP6SNyaInCgEnk=442", "summary": "Keeping Your Data Safe When an AI Agent Clicks a Link (5 minute read) OpenAI's new safety mechanism is designed to reduce the risk of AI agents inadvertently leaking private data when they automatically fetch URLs by only allowing automatic retrieval of links that are publicly indexed on the open web.", "source": "tldr", "AI": {"tldr": "OpenAI\u63a8\u51fa\u65b0\u7684\u5b89\u5168\u673a\u5236\uff0c\u9650\u5236AI\u4ee3\u7406\u53ea\u80fd\u81ea\u52a8\u83b7\u53d6\u516c\u5f00\u7d22\u5f15\u7684\u7f51\u9875\u94fe\u63a5\uff0c\u9632\u6b62\u5728\u81ea\u52a8\u6293\u53d6URL\u65f6\u6cc4\u9732\u79c1\u4eba\u6570\u636e", "motivation": "AI\u4ee3\u7406\u5728\u81ea\u52a8\u83b7\u53d6URL\u65f6\u53ef\u80fd\u65e0\u610f\u4e2d\u6cc4\u9732\u79c1\u4eba\u6570\u636e\uff0c\u9700\u8981\u5b89\u5168\u673a\u5236\u6765\u4fdd\u62a4\u7528\u6237\u9690\u79c1", "method": "\u8bbe\u8ba1\u5b89\u5168\u673a\u5236\uff0c\u53ea\u5141\u8bb8AI\u4ee3\u7406\u81ea\u52a8\u68c0\u7d22\u5728\u5f00\u653e\u7f51\u7edc\u4e0a\u516c\u5f00\u7d22\u5f15\u7684\u94fe\u63a5\uff0c\u9650\u5236\u5bf9\u975e\u516c\u5f00\u6216\u79c1\u4eba\u5185\u5bb9\u7684\u8bbf\u95ee", "result": "\u901a\u8fc7\u9650\u5236\u53ea\u80fd\u8bbf\u95ee\u516c\u5f00\u7d22\u5f15\u7684\u7f51\u9875\uff0c\u51cf\u5c11\u4e86AI\u4ee3\u7406\u5728\u81ea\u52a8\u6293\u53d6URL\u65f6\u6cc4\u9732\u79c1\u4eba\u6570\u636e\u7684\u98ce\u9669", "conclusion": "\u8fd9\u79cd\u5b89\u5168\u673a\u5236\u662f\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u91cd\u8981\u63aa\u65bd\uff0c\u786e\u4fddAI\u4ee3\u7406\u5728\u8bbf\u95ee\u7f51\u7edc\u8d44\u6e90\u65f6\u4e0d\u4f1a\u65e0\u610f\u4e2d\u66b4\u9732\u654f\u611f\u4fe1\u606f", "topic": "agent analysis"}}
{"id": "tldr.2601.90b20842", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbiilmann.blog%2Farticles%2Fone-year-of-ax%2F%3Futm_source=tldrproduct/1/0100019c0e95e84f-c1dc39b2-c6cb-4c2b-abd9-c9da6245a32f-000000/RGxBtiQsfmk67ri21g6b6bXH9fT9Z75Z3TOeJtInToI=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbiilmann.blog%2Farticles%2Fone-year-of-ax%2F%3Futm_source=tldrproduct/1/0100019c0e95e84f-c1dc39b2-c6cb-4c2b-abd9-c9da6245a32f-000000/RGxBtiQsfmk67ri21g6b6bXH9fT9Z75Z3TOeJtInToI=442", "authors": ["TLDR Newsletter"], "title": "One Year of AX", "comment": "Source: TLDR Newsletter, Date: 2026-01-30, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbiilmann.blog%2Farticles%2Fone-year-of-ax%2F%3Futm_source=tldrproduct/1/0100019c0e95e84f-c1dc39b2-c6cb-4c2b-abd9-c9da6245a32f-000000/RGxBtiQsfmk67ri21g6b6bXH9fT9Z75Z3TOeJtInToI=442", "summary": "One Year of AX (6 minute read) As agents become primary users, products must optimize for Agent Experience. Access, context, tools, and orchestration will determine which platforms remain relevant.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u968f\u7740AI\u4ee3\u7406\u6210\u4e3a\u4e3b\u8981\u7528\u6237\uff0c\u4ea7\u54c1\u9700\u8981\u4f18\u5316\"\u4ee3\u7406\u4f53\u9a8c\"\uff08AX\uff09\uff0c\u5e76\u5206\u6790\u4e86\u51b3\u5b9a\u5e73\u53f0\u76f8\u5173\u6027\u7684\u56db\u4e2a\u5173\u952e\u56e0\u7d20", "motivation": "\u968f\u7740AI\u4ee3\u7406\u9010\u6e10\u6210\u4e3a\u4ea7\u54c1\u7684\u4e3b\u8981\u7528\u6237\uff0c\u4f20\u7edf\u7684\u7528\u6237\u4f53\u9a8c\uff08UX\uff09\u8bbe\u8ba1\u5df2\u4e0d\u518d\u9002\u7528\uff0c\u9700\u8981\u4e13\u95e8\u4e3a\u4ee3\u7406\u8bbe\u8ba1\u4f53\u9a8c\uff08AX\uff09\u6765\u4fdd\u6301\u5e73\u53f0\u7684\u76f8\u5173\u6027\u548c\u7ade\u4e89\u529b", "method": "\u901a\u8fc7\u5206\u6790\u4ee3\u7406\u4f5c\u4e3a\u7528\u6237\u7684\u7279\u70b9\uff0c\u63d0\u51fa\u4e86\u51b3\u5b9a\u5e73\u53f0\u76f8\u5173\u6027\u7684\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u8bbf\u95ee\u6743\u9650\u3001\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u5de5\u5177\u96c6\u6210\u548c\u7f16\u6392\u80fd\u529b", "result": "\u8bc6\u522b\u4e86\u4ee3\u7406\u4f53\u9a8c\uff08AX\uff09\u4f5c\u4e3a\u65b0\u7684\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5e76\u5efa\u7acb\u4e86\u8bc4\u4f30\u5e73\u53f0\u4ee3\u7406\u53cb\u597d\u5ea6\u7684\u6846\u67b6\uff0c\u5f3a\u8c03\u5e73\u53f0\u9700\u8981\u5728\u8fd9\u4e9b\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u4f18\u5316", "conclusion": "\u968f\u7740AI\u4ee3\u7406\u7684\u666e\u53ca\uff0c\u4ea7\u54c1\u8bbe\u8ba1\u5fc5\u987b\u4ece\u7528\u6237\u4f53\u9a8c\uff08UX\uff09\u8f6c\u5411\u4ee3\u7406\u4f53\u9a8c\uff08AX\uff09\uff0c\u90a3\u4e9b\u5728\u8bbf\u95ee\u3001\u4e0a\u4e0b\u6587\u3001\u5de5\u5177\u548c\u7f16\u6392\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u7684\u5e73\u53f0\u5c06\u4fdd\u6301\u7ade\u4e89\u529b", "topic": "agent analysis"}}
