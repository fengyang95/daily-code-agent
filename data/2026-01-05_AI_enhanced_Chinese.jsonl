{"id": "2601.00007", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00007", "abs": "https://arxiv.org/abs/2601.00007", "authors": ["Nicholas A. Pape"], "title": "Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games", "comment": "20 pages, 19 figures", "summary": "Yahtzee is a classic dice game with a stochastic, combinatorial structure and delayed rewards, making it an interesting mid-scale RL benchmark. While an optimal policy for solitaire Yahtzee can be computed using dynamic programming methods, multiplayer is intractable, motivating approximation methods. We formulate Yahtzee as a Markov Decision Process (MDP), and train self-play agents using various policy gradient methods: REINFORCE, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO), all using a multi-headed network with a shared trunk. We ablate feature and action encodings, architecture, return estimators, and entropy regularization to understand their impact on learning. Under a fixed training budget, REINFORCE and PPO prove sensitive to hyperparameters and fail to reach near-optimal performance, whereas A2C trains robustly across a range of settings. Our agent attains a median score of 241.78 points over 100,000 evaluation games, within 5.0\\% of the optimal DP score of 254.59, achieving the upper section bonus and Yahtzee at rates of 24.9\\% and 34.1\\%, respectively. All models struggle to learn the upper bonus strategy, overindexing on four-of-a-kind's, highlighting persistent long-horizon credit-assignment and exploration challenges.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06Yahtzee\u9ab0\u5b50\u6e38\u620f\u6784\u5efa\u4e3aMDP\uff0c\u4f7f\u7528\u591a\u79cd\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u8bad\u7ec3\u81ea\u535a\u5f08\u667a\u80fd\u4f53\uff0c\u53d1\u73b0A2C\u5728\u56fa\u5b9a\u8bad\u7ec3\u9884\u7b97\u4e0b\u8868\u73b0\u6700\u7a33\u5065\uff0c\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u7684241.78\u5206\uff08\u6700\u4f18\u4e3a254.59\u5206\uff09\uff0c\u4f46\u6240\u6709\u6a21\u578b\u90fd\u96be\u4ee5\u5b66\u4e60\u4e0a\u533a\u5956\u52b1\u7b56\u7565\u3002", "motivation": "Yahtzee\u6e38\u620f\u5177\u6709\u968f\u673a\u6027\u3001\u7ec4\u5408\u7ed3\u6784\u548c\u5ef6\u8fdf\u5956\u52b1\u7684\u7279\u70b9\uff0c\u662f\u4e00\u4e2a\u6709\u8da3\u7684\u4e2d\u7b49\u89c4\u6a21RL\u57fa\u51c6\u3002\u5355\u4eba\u6e38\u620f\u53ef\u7528\u52a8\u6001\u89c4\u5212\u6c42\u89e3\uff0c\u4f46\u591a\u4eba\u6e38\u620f\u96be\u4ee5\u5904\u7406\uff0c\u9700\u8981\u8fd1\u4f3c\u65b9\u6cd5\u3002", "method": "\u5c06Yahtzee\u6784\u5efa\u4e3aMDP\uff0c\u4f7f\u7528REINFORCE\u3001A2C\u548cPPO\u7b49\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u8bad\u7ec3\u81ea\u535a\u5f08\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u591a\u5934\u7f51\u7edc\u5171\u4eab\u4e3b\u5e72\u7ed3\u6784\uff0c\u5bf9\u7279\u5f81\u548c\u52a8\u4f5c\u7f16\u7801\u3001\u67b6\u6784\u3001\u56de\u62a5\u4f30\u8ba1\u5668\u548c\u71b5\u6b63\u5219\u5316\u8fdb\u884c\u6d88\u878d\u7814\u7a76\u3002", "result": "\u5728\u56fa\u5b9a\u8bad\u7ec3\u9884\u7b97\u4e0b\uff0cREINFORCE\u548cPPO\u5bf9\u8d85\u53c2\u6570\u654f\u611f\u4e14\u672a\u80fd\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u800cA2C\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u90fd\u80fd\u7a33\u5065\u8bad\u7ec3\u3002\u6700\u4f73\u667a\u80fd\u4f53\u5728100,000\u6b21\u8bc4\u4f30\u6e38\u620f\u4e2d\u83b7\u5f97\u4e2d\u4f4d\u6570\u5206\u6570241.78\u5206\uff0c\u63a5\u8fd1\u6700\u4f18DP\u5206\u6570254.59\u5206\u76845%\u4ee5\u5185\uff0c\u4e0a\u533a\u5956\u52b1\u548cYahtzee\u8fbe\u6210\u7387\u5206\u522b\u4e3a24.9%\u548c34.1%\u3002", "conclusion": "\u6240\u6709\u6a21\u578b\u90fd\u96be\u4ee5\u5b66\u4e60\u4e0a\u533a\u5956\u52b1\u7b56\u7565\uff0c\u8fc7\u5ea6\u5173\u6ce8\u56db\u9ab0\u540c\u70b9\uff0c\u7a81\u663e\u4e86\u957f\u671f\u4fe1\u7528\u5206\u914d\u548c\u63a2\u7d22\u7684\u6301\u7eed\u6311\u6218\u3002A2C\u5728Yahtzee\u6e38\u620f\u4e2d\u8868\u73b0\u6700\u7a33\u5065\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.00065", "categories": ["cs.LG", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.00065", "abs": "https://arxiv.org/abs/2601.00065", "authors": ["Xiaoze Liu", "Weichen Yu", "Matt Fredrikson", "Xiaoqian Wang", "Jing Gao"], "title": "The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition", "comment": null, "summary": "The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single \"breaker token\" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u4e00\u79cd\u9488\u5bf9LLM\u7ec4\u5408\u6280\u672f\u7684\u5b89\u5168\u6f0f\u6d1e\uff1a\u901a\u8fc7\u8bbe\u8ba1\u5355\u4e2a\"\u7834\u574f\u4ee4\u724c\"\uff0c\u5728\u6a21\u578b\u79fb\u690d\u8fc7\u7a0b\u4e2d\u690d\u5165\u6076\u610f\u7279\u5f81\uff0c\u7834\u574f\u57fa\u7840\u6a21\u578b\u751f\u6210\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6350\u8d60\u6a21\u578b\u529f\u80fd\u6b63\u5e38\u3002", "motivation": "\u968f\u7740\u5f00\u6e90LLM\u751f\u6001\u7cfb\u7edf\u4e2d\u6a21\u578b\u7ec4\u5408\u6280\u672f\uff08\u5982\u6743\u91cd\u5408\u5e76\u3001\u63a8\u6d4b\u89e3\u7801\u3001\u8bcd\u6c47\u8868\u6269\u5c55\uff09\u7684\u666e\u53ca\uff0c\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u95f4\u7684\u8bcd\u6c47\u8868\u79fb\u690d\u6210\u4e3a\u5173\u952e\u524d\u63d0\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u4e92\u64cd\u4f5c\u6027\u6b65\u9aa4\u53ef\u80fd\u5f15\u5165\u4f9b\u5e94\u94fe\u6f0f\u6d1e\u3002", "method": "\u901a\u8fc7\u5229\u7528\u7cfb\u6570\u91cd\u7528\u7684\u51e0\u4f55\u7279\u6027\uff0c\u8bbe\u8ba1\u4e86\"\u7834\u574f\u4ee4\u724c\"\u653b\u51fb\uff1a\u5728\u6350\u8d60\u6a21\u578b\u4e2d\u529f\u80fd\u60f0\u6027\u7684\u5355\u4e2a\u4ee4\u724c\uff0c\u5728\u79fb\u690d\u5230\u57fa\u7840\u6a21\u578b\u540e\u4f1a\u53ef\u9760\u5730\u91cd\u6784\u4e3a\u9ad8\u663e\u8457\u6027\u6076\u610f\u7279\u5f81\u3002\u5c06\u653b\u51fb\u5f62\u5f0f\u5316\u4e3a\u53cc\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u7a00\u758f\u6c42\u89e3\u5668\u5b9e\u4f8b\u5316\u3002", "result": "\u653b\u51fb\u65e0\u9700\u8bad\u7ec3\uff0c\u901a\u8fc7\u8c31\u6a21\u4eff\u9003\u907f\u5f02\u5e38\u68c0\u6d4b\uff0c\u5728\u5fae\u8c03\u548c\u6743\u91cd\u5408\u5e76\u540e\u4ecd\u4fdd\u6301\u7ed3\u6784\u6301\u4e45\u6027\u3002\u5b9e\u8bc1\u663e\u793a\u653b\u51fb\u6210\u529f\u7834\u574f\u57fa\u7840\u6a21\u578b\u751f\u6210\u80fd\u529b\uff0c\u800c\u6350\u8d60\u6a21\u578b\u529f\u80fd\u4e0e\u6b63\u5e38\u884c\u4e3a\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u533a\u5206\u3002", "conclusion": "\u6a21\u578b\u7ec4\u5408\u6280\u672f\u4e2d\u7684\u8bcd\u6c47\u8868\u79fb\u690d\u6b65\u9aa4\u5b58\u5728\u9690\u85cf\u98ce\u9669\uff0c\u653b\u51fb\u521b\u5efa\u4e86\u4e0d\u5bf9\u79f0\u53ef\u5b9e\u73b0\u6027\u5dee\u8ddd\uff0c\u7a81\u663e\u4e86\u6a21\u5757\u5316AI\u7ec4\u5408\u7ba1\u9053\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "topic": "agent analysis"}}
{"id": "2601.00086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00086", "abs": "https://arxiv.org/abs/2601.00086", "authors": ["Xiang Gao", "Yuguang Yao", "Qi Zhang", "Kaiwen Dong", "Avinash Baidya", "Ruocheng Guo", "Hilaf Hasson", "Kamalika Das"], "title": "RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning", "comment": null, "summary": "Large language models (LLMs) often struggle to use tools reliably in domain-specific settings, where APIs may be idiosyncratic, under-documented, or tailored to private workflows. This highlights the need for effective adaptation to task-specific tools. We propose RIMRULE, a neuro-symbolic approach for LLM adaptation based on dynamic rule injection. Compact, interpretable rules are distilled from failure traces and injected into the prompt during inference to improve task performance. These rules are proposed by the LLM itself and consolidated using a Minimum Description Length (MDL) objective that favors generality and conciseness. Each rule is stored in both natural language and a structured symbolic form, supporting efficient retrieval at inference time. Experiments on tool-use benchmarks show that this approach improves accuracy on both seen and unseen tools without modifying LLM weights. It outperforms prompting-based adaptation methods and complements finetuning. Moreover, rules learned from one LLM can be reused to improve others, including long reasoning LLMs, highlighting the portability of symbolic knowledge across architectures.", "AI": {"tldr": "RIMRULE\uff1a\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u89c4\u5219\u6ce8\u5165\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5931\u8d25\u8f68\u8ff9\u4e2d\u63d0\u53d6\u7b80\u6d01\u53ef\u89e3\u91ca\u7684\u89c4\u5219\uff0c\u5728\u63a8\u7406\u65f6\u6ce8\u5165\u63d0\u793a\u4e2d\uff0c\u63d0\u9ad8LLM\u5728\u7279\u5b9a\u9886\u57df\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u5de5\u5177\u4f7f\u7528\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3aAPI\u53ef\u80fd\u5177\u6709\u7279\u6b8a\u6027\u3001\u6587\u6863\u4e0d\u8db3\u6216\u9488\u5bf9\u79c1\u6709\u5de5\u4f5c\u6d41\u5b9a\u5236\u3002\u9700\u8981\u6709\u6548\u7684\u9002\u5e94\u65b9\u6cd5\u6765\u5904\u7406\u4efb\u52a1\u7279\u5b9a\u5de5\u5177\u3002", "method": "\u63d0\u51faRIMRULE\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff1a1) \u4ece\u5931\u8d25\u8f68\u8ff9\u4e2d\u63d0\u53d6\u7d27\u51d1\u53ef\u89e3\u91ca\u7684\u89c4\u5219\uff1b2) \u4f7f\u7528\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u76ee\u6807\u8fdb\u884c\u89c4\u5219\u6574\u5408\uff0c\u504f\u597d\u901a\u7528\u6027\u548c\u7b80\u6d01\u6027\uff1b3) \u5c06\u89c4\u5219\u5b58\u50a8\u4e3a\u81ea\u7136\u8bed\u8a00\u548c\u7ed3\u6784\u5316\u7b26\u53f7\u5f62\u5f0f\uff1b4) \u5728\u63a8\u7406\u65f6\u52a8\u6001\u6ce8\u5165\u89c4\u5219\u5230\u63d0\u793a\u4e2d\u3002", "result": "\u5728\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5bf9\u5df2\u89c1\u548c\u672a\u89c1\u5de5\u5177\u7684\u51c6\u786e\u6027\uff0c\u65e0\u9700\u4fee\u6539LLM\u6743\u91cd\u3002\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u9002\u5e94\u65b9\u6cd5\uff0c\u5e76\u4e0e\u5fae\u8c03\u4e92\u8865\u3002\u4ece\u4e00\u4e2aLLM\u5b66\u4e60\u7684\u89c4\u5219\u53ef\u4ee5\u91cd\u7528\u4e8e\u6539\u8fdb\u5176\u4ed6LLM\uff0c\u5305\u62ec\u957f\u63a8\u7406LLM\u3002", "conclusion": "RIMRULE\u901a\u8fc7\u52a8\u6001\u89c4\u5219\u6ce8\u5165\u6709\u6548\u63d0\u9ad8\u4e86LLM\u5728\u7279\u5b9a\u9886\u57df\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u5c55\u793a\u4e86\u7b26\u53f7\u77e5\u8bc6\u5728\u4e0d\u540c\u67b6\u6784\u95f4\u7684\u53ef\u79fb\u690d\u6027\uff0c\u4e3aLLM\u5de5\u5177\u9002\u5e94\u63d0\u4f9b\u4e86\u795e\u7ecf\u7b26\u53f7\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.00235", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00235", "abs": "https://arxiv.org/abs/2601.00235", "authors": ["Victor Wen", "Zedong Peng"], "title": "Advanced Vulnerability Scanning for Open Source Software: Detection and Mitigation of Log4j Vulnerabilities", "comment": null, "summary": "Automated detection of software vulnerabilities remains a critical challenge in software security. Log4j is an industrial-grade Java logging framework listed as one of the top 100 critical open source projects. On Dec. 10, 2021 a severe vulnerability Log4Shell was disclosed before being fully patched with Log4j2 version 2.17.0 on Dec. 18, 2021. However, to this day about 4.1 million, or 33 percent of all Log4j downloads in the last 7 days contain vulnerable packages. Many Log4Shell scanners have since been created to detect if a user's installed Log4j version is vulnerable. Current detection tools primarily focus on identifying the version of Log4j installed, leading to numerous false positives, as they do not check if the software scanned is really vulnerable to malicious actors. This research aims to develop an advanced Log4j scanning tool that can evaluate the real-world exploitability of the software, thereby reducing false positives. Our approach first identifies vulnerabilities and then provides targeted recommendations for mitigating these detected vulnerabilities, along with instant feedback to users. By leveraging GitHub Actions, our tool offers automated and continuous scanning capabilities, ensuring timely identification of vulnerabilities as code changes occur. This integration into existing development workflows enables real-time monitoring and quicker responses to potential threats. We demonstrate the effectiveness of our approach by evaluating 28 open-source software projects across different releases, achieving an accuracy rate of 91.4% from a sample of 140 scans. Our GitHub action implementation is available at the GitHub marketplace and can be accessed by anyone interested in improving their software security and for future studies. This tool provides a dependable way to detect and mitigate vulnerabilities in open-source projects.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5148\u8fdb\u7684Log4j\u626b\u63cf\u5de5\u5177\uff0c\u901a\u8fc7\u8bc4\u4f30\u8f6f\u4ef6\u7684\u5b9e\u9645\u53ef\u5229\u7528\u6027\u6765\u51cf\u5c11\u8bef\u62a5\uff0c\u96c6\u6210\u5230GitHub Actions\u4e2d\u5b9e\u73b0\u81ea\u52a8\u5316\u6301\u7eed\u626b\u63cf", "motivation": "Log4Shell\u6f0f\u6d1e\u62ab\u9732\u540e\uff0c\u867d\u7136\u5df2\u6709\u8bb8\u591a\u626b\u63cf\u5de5\u5177\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u4e3b\u8981\u5173\u6ce8\u8bc6\u522bLog4j\u7248\u672c\uff0c\u5bfc\u81f4\u5927\u91cf\u8bef\u62a5\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u68c0\u67e5\u8f6f\u4ef6\u662f\u5426\u771f\u6b63\u6613\u53d7\u653b\u51fb\u3002\u9700\u8981\u5f00\u53d1\u80fd\u8bc4\u4f30\u5b9e\u9645\u53ef\u5229\u7528\u6027\u7684\u5de5\u5177\u6765\u51cf\u5c11\u8bef\u62a5\u3002", "method": "\u9996\u5148\u8bc6\u522b\u6f0f\u6d1e\uff0c\u7136\u540e\u63d0\u4f9b\u9488\u5bf9\u6027\u7684\u7f13\u89e3\u5efa\u8bae\u548c\u5373\u65f6\u53cd\u9988\u3002\u901a\u8fc7GitHub Actions\u96c6\u6210\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u6301\u7eed\u626b\u63cf\u80fd\u529b\uff0c\u786e\u4fdd\u5728\u4ee3\u7801\u53d8\u66f4\u65f6\u53ca\u65f6\u8bc6\u522b\u6f0f\u6d1e\u3002", "result": "\u8bc4\u4f30\u4e8628\u4e2a\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u7684\u4e0d\u540c\u7248\u672c\uff0c\u4ece140\u6b21\u626b\u63cf\u4e2d\u83b7\u5f97\u4e8691.4%\u7684\u51c6\u786e\u7387\u3002GitHub Action\u5b9e\u73b0\u5df2\u5728GitHub\u5e02\u573a\u4e0a\u63d0\u4f9b\u3002", "conclusion": "\u8be5\u5de5\u5177\u4e3a\u68c0\u6d4b\u548c\u7f13\u89e3\u5f00\u6e90\u9879\u76ee\u4e2d\u7684\u6f0f\u6d1e\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u5230\u73b0\u6709\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u76d1\u63a7\u548c\u5feb\u901f\u54cd\u5e94\u6f5c\u5728\u5a01\u80c1\u3002", "topic": "swe application"}}
{"id": "2601.00095", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00095", "abs": "https://arxiv.org/abs/2601.00095", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Universal Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning", "comment": null, "summary": "Large language models increasingly require structured inference, from JSON schema enforcement to multi-lingual parsing, where outputs must satisfy complex constraints. We introduce MetaJuLS, a meta-reinforcement learning approach that learns universal constraint propagation policies applicable across languages and tasks without task-specific retraining. By formulating structured inference as adaptive constraint propagation and training a Graph Attention Network with meta-learning, MetaJuLS achieves 1.5--2.0$\\times$ speedups over GPU-optimized baselines while maintaining within 0.2\\% accuracy of state-of-the-art parsers. On Universal Dependencies across 10 languages and LLM-constrained generation (LogicBench, GSM8K-Constrained), MetaJuLS demonstrates rapid cross-domain adaptation: a policy trained on English parsing adapts to new languages and tasks with 5--10 gradient steps (5--15 seconds) rather than requiring hours of task-specific training. Mechanistic analysis reveals the policy discovers human-like parsing strategies (easy-first) and novel non-intuitive heuristics. By reducing propagation steps in LLM deployments, MetaJuLS contributes to Green AI by directly reducing inference carbon footprint.", "AI": {"tldr": "MetaJuLS\uff1a\u4e00\u79cd\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ea6\u675f\u4f20\u64ad\u5b9e\u73b0\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u5728\u591a\u79cd\u8bed\u8a00\u548c\u4efb\u52a1\u4e0a\u5b9e\u73b01.5-2\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u89e3\u6790\u56680.2%\u7684\u51c6\u786e\u7387\u5dee\u8ddd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u7ed3\u6784\u5316\u63a8\u7406\uff08\u5982JSON\u6a21\u5f0f\u5f3a\u5236\u3001\u591a\u8bed\u8a00\u89e3\u6790\uff09\uff0c\u8f93\u51fa\u5fc5\u987b\u6ee1\u8db3\u590d\u6742\u7ea6\u675f\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u7684\u91cd\u65b0\u8bad\u7ec3\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u4e0d\u7075\u6d3b\u3002", "method": "\u5c06\u7ed3\u6784\u5316\u63a8\u7406\u5efa\u6a21\u4e3a\u81ea\u9002\u5e94\u7ea6\u675f\u4f20\u64ad\u95ee\u9898\uff0c\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u5143\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u901a\u7528\u7ea6\u675f\u4f20\u64ad\u7b56\u7565\u3002\u8be5\u7b56\u7565\u53ef\u8de8\u8bed\u8a00\u548c\u4efb\u52a1\u5e94\u7528\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728Universal Dependencies\uff0810\u79cd\u8bed\u8a00\uff09\u548cLLM\u7ea6\u675f\u751f\u6210\u4efb\u52a1\u4e0a\uff0cMetaJuLS\u6bd4GPU\u4f18\u5316\u57fa\u7ebf\u5feb1.5-2\u500d\uff0c\u51c6\u786e\u7387\u4ec5\u6bd4\u6700\u5148\u8fdb\u89e3\u6790\u5668\u4f4e0.2%\u3002\u8de8\u57df\u9002\u5e94\u4ec5\u97005-10\u4e2a\u68af\u5ea6\u6b65\u9aa4\uff085-15\u79d2\uff09\uff0c\u800c\u975e\u6570\u5c0f\u65f6\u7684\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u3002", "conclusion": "MetaJuLS\u901a\u8fc7\u51cf\u5c11LLM\u90e8\u7f72\u4e2d\u7684\u4f20\u64ad\u6b65\u9aa4\uff0c\u4e3a\u7eff\u8272AI\u505a\u51fa\u8d21\u732e\uff0c\u76f4\u63a5\u964d\u4f4e\u63a8\u7406\u78b3\u8db3\u8ff9\u3002\u7b56\u7565\u5206\u6790\u63ed\u793a\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u7684\u89e3\u6790\u7b56\u7565\uff08\u6613\u4f18\u5148\uff09\u548c\u65b0\u7684\u975e\u76f4\u89c2\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.00254", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00254", "abs": "https://arxiv.org/abs/2601.00254", "authors": ["Md Hasan Saju", "Maher Muhtadi", "Akramul Azim"], "title": "An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f6f\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\uff08RAG\u3001SFT\u548c\u53cc\u4ee3\u7406\u6846\u67b6\uff09\uff0c\u53d1\u73b0\u7ed3\u5408\u5916\u90e8\u9886\u57df\u77e5\u8bc6\u7684RAG\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u81ea\u52a8\u5316\u8f6f\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u8fd9\u662f\u4fdd\u62a4\u73b0\u4ee3\u4ee3\u7801\u5e93\u7684\u5173\u952e\u4efb\u52a1\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540cLLM\u6280\u672f\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a1\uff09\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u6574\u5408\u4e92\u8054\u7f51\u548cMITRE CWE\u6570\u636e\u5e93\u7684\u5916\u90e8\u9886\u57df\u77e5\u8bc6\uff1b2\uff09\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u7684QLoRA\u9002\u914d\u5668\uff1b3\uff09\u53cc\u4ee3\u7406LLM\u6846\u67b6\uff0c\u5176\u4e2d\u7b2c\u4e8c\u4e2a\u4ee3\u7406\u5ba1\u6838\u548c\u4f18\u5316\u7b2c\u4e00\u4e2a\u4ee3\u7406\u7684\u8f93\u51fa\u3002\u4f7f\u7528\u4eceBig-Vul\u548cGitHub\u771f\u5b9e\u4ee3\u7801\u5e93\u6536\u96c6\u7684\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u4e94\u4e2a\u5173\u952eCWE\u7c7b\u522b\u3002", "result": "RAG\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u6574\u4f53\u51c6\u786e\u7387\uff080.86\uff09\u548cF1\u5206\u6570\uff080.85\uff09\u3002SFT\u65b9\u6cd5\u4e5f\u8868\u73b0\u51fa\u8272\u3002\u53cc\u4ee3\u7406\u7cfb\u7edf\u5728\u63d0\u9ad8\u63a8\u7406\u900f\u660e\u5ea6\u548c\u9519\u8bef\u7f13\u89e3\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8d44\u6e90\u5f00\u9500\u3002", "conclusion": "\u6574\u5408\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u673a\u5236\u663e\u8457\u589e\u5f3a\u4e86LLM\u5728\u5b9e\u9645\u6f0f\u6d1e\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002RAG\u65b9\u6cd5\u901a\u8fc7\u4e0a\u4e0b\u6587\u589e\u5f3a\u8868\u73b0\u6700\u4f73\uff0c\u53cc\u4ee3\u7406\u6846\u67b6\u5728\u900f\u660e\u5ea6\u548c\u8d44\u6e90\u6548\u7387\u65b9\u9762\u6709\u4f18\u52bf\u3002", "topic": "code agent"}}
{"id": "2601.00116", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00116", "abs": "https://arxiv.org/abs/2601.00116", "authors": ["Aditya Sai Ellendula", "Yi Wang", "Minh Nguyen", "Chandrajit Bajaj"], "title": "GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments", "comment": null, "summary": "We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \\href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.", "AI": {"tldr": "GRL-SNAM\u662f\u4e00\u4e2a\u51e0\u4f55\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u672a\u77e5\u73af\u5883\u4e2d\u540c\u65f6\u8fdb\u884c\u5bfc\u822a\u548c\u5efa\u56fe\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c40\u90e8\u80fd\u91cf\u666f\u89c2\u7f16\u7801\u53ef\u8fbe\u6027\u548c\u969c\u788d\u7ea6\u675f\uff0c\u4f7f\u7528\u54c8\u5bc6\u987f\u4f18\u5316\u8fdb\u884c\u52a8\u6001\u8def\u5f84\u641c\u7d22\uff0c\u65e0\u9700\u6784\u5efa\u5168\u5c40\u5730\u56fe\u3002", "motivation": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\u540c\u65f6\u8fdb\u884c\u5bfc\u822a\u548c\u5efa\u56fe\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u6784\u5efa\u5168\u5c40\u5730\u56fe\u6216\u8bbe\u8ba1\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u7b56\u7565\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u4ec5\u4f9d\u8d56\u5c40\u90e8\u611f\u77e5\u89c2\u6d4b\u3001\u65e0\u9700\u5168\u5c40\u5730\u56fe\u7684\u5bfc\u822a\u5efa\u56fe\u65b9\u6cd5\u3002", "method": "\u5c06\u8def\u5f84\u5bfc\u822a\u548c\u5efa\u56fe\u5efa\u6a21\u4e3a\u52a8\u6001\u6700\u77ed\u8def\u5f84\u641c\u7d22\u548c\u53d1\u73b0\u8fc7\u7a0b\uff0c\u4f7f\u7528\u53d7\u63a7\u54c8\u5bc6\u987f\u4f18\u5316\u3002\u611f\u77e5\u8f93\u5165\u88ab\u8f6c\u6362\u4e3a\u5c40\u90e8\u80fd\u91cf\u666f\u89c2\uff0c\u7f16\u7801\u53ef\u8fbe\u6027\u3001\u969c\u788d\u5c4f\u969c\u548c\u53d8\u5f62\u7ea6\u675f\u3002\u901a\u8fc7\u66f4\u65b0\u54c8\u5bc6\u987f\u91cf\u6765\u6f14\u5316\u611f\u77e5\u3001\u89c4\u5212\u548c\u91cd\u6784\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u8bc4\u5206\u51fd\u6570\u6301\u7eed\u4f18\u5316\u8f68\u8ff9\u3002", "result": "\u57282D\u5bfc\u822a\u4efb\u52a1\u4e0a\u8bc4\u4f30GRL-SNAM\uff0c\u76f8\u6bd4\u5c40\u90e8\u53cd\u5e94\u5f0f\u57fa\u7ebf\u548c\u5168\u5c40\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u4fdd\u6301\u5b89\u5168\u8ddd\u79bb\u3001\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u5e03\u5c40\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u80fd\u91cf\u4f18\u5316\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5bfc\u822a\uff0c\u65e0\u9700\u5927\u91cf\u5168\u5c40\u63a2\u7d22\u3002", "conclusion": "\u901a\u8fc7\u54c8\u5bc6\u987f\u66f4\u65b0\u7684\u51e0\u4f55\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u901a\u8fc7\u5c40\u90e8\u80fd\u91cf\u4f18\u5316\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5bfc\u822a\uff0c\u65e0\u9700\u5e7f\u6cdb\u7684\u5168\u5c40\u5efa\u56fe\u3002\u8be5\u65b9\u6cd5\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u5bfc\u822a\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.00376", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00376", "abs": "https://arxiv.org/abs/2601.00376", "authors": ["Chao Hu", "Wenhao Zeng", "Yuling Shi", "Beijun Shen", "Xiaodong Gu"], "title": "In Line with Context: Repository-Level Code Generation via Context Inlining", "comment": "Accepted to FSE 2026", "summary": "Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.", "AI": {"tldr": "InlineCoder\u901a\u8fc7\u5c06\u672a\u5b8c\u6210\u51fd\u6570\u5185\u8054\u5230\u5176\u8c03\u7528\u56fe\u4e2d\uff0c\u5c06\u590d\u6742\u7684\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u8f6c\u5316\u4e3a\u66f4\u7b80\u5355\u7684\u51fd\u6570\u7ea7\u7f16\u7801\u4efb\u52a1\uff0c\u5229\u7528\u951a\u70b9\u751f\u6210\u548c\u53cc\u5411\u5185\u8054\u8fc7\u7a0b\u589e\u5f3a\u5bf9\u4ed3\u5e93\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u7684\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff08\u5982RAG\u6216\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u51fd\u6570\u9009\u62e9\uff09\u4e3b\u8981\u4f9d\u8d56\u8868\u9762\u76f8\u4f3c\u6027\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u8de8\u51fd\u6570\u3001\u7c7b\u548c\u6a21\u5757\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u5728\u7406\u89e3\u6574\u4e2a\u4ed3\u5e93\u8bed\u4e49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "InlineCoder\u9996\u5148\u6839\u636e\u51fd\u6570\u7b7e\u540d\u751f\u6210\u4e00\u4e2a\u8fd1\u4f3c\u4e0b\u6e38\u4f9d\u8d56\u7684\u8349\u7a3f\u5b8c\u6210\uff08\u951a\u70b9\uff09\uff0c\u7136\u540e\u8fdb\u884c\u53cc\u5411\u5185\u8054\uff1a\u4e0a\u6e38\u5185\u8054\u5c06\u951a\u70b9\u5d4c\u5165\u8c03\u7528\u8005\u4ee5\u6355\u83b7\u591a\u6837\u5316\u4f7f\u7528\u573a\u666f\uff1b\u4e0b\u6e38\u68c0\u7d22\u5c06\u951a\u70b9\u7684\u88ab\u8c03\u7528\u8005\u96c6\u6210\u5230\u63d0\u793a\u4e2d\u63d0\u4f9b\u7cbe\u786e\u4f9d\u8d56\u4e0a\u4e0b\u6587\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u8349\u7a3f\u5b8c\u6210\u4e0e\u4e0a\u4e0b\u6e38\u89c6\u89d2\u7684\u4e30\u5bcc\u4e0a\u4e0b\u6587\uff0c\u4e3aLLM\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u4ed3\u5e93\u89c6\u56fe\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u3002", "conclusion": "InlineCoder\u901a\u8fc7\u5c06\u4ed3\u5e93\u7406\u89e3\u91cd\u65b0\u6784\u5efa\u4e3a\u51fd\u6570\u7ea7\u7f16\u7801\u4efb\u52a1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "code agent"}}
{"id": "2601.00167", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00167", "abs": "https://arxiv.org/abs/2601.00167", "authors": ["Junkai Luo", "Yinglun Zhu"], "title": "Online Finetuning Decision Transformers with Pure RL Gradients", "comment": null, "summary": "Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7b97\u6cd5\u4f7f\u51b3\u7b56\u53d8\u6362\u5668\u80fd\u591f\u4f7f\u7528\u7eaf\u5f3a\u5316\u5b66\u4e60\u68af\u5ea6\u8fdb\u884c\u5728\u7ebf\u5fae\u8c03\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\u76ee\u6807\u7684\u9650\u5236\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u51b3\u7b56\u53d8\u6362\u5668\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6269\u5c55\u5230\u5728\u7ebf\u8bbe\u7f6e\u65f6\u4ecd\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5e8f\u5217\u5efa\u6a21\u76ee\u6807\u3002\u4f5c\u8005\u53d1\u73b0\u540e\u89c1\u56de\u62a5\u91cd\u6807\u6ce8\u8fd9\u4e00\u6807\u51c6\u7ec4\u4ef6\u4e0e\u91cd\u8981\u6027\u91c7\u6837\u7c7bRL\u7b97\u6cd5\u4e0d\u517c\u5bb9\uff0c\u963b\u788d\u4e86\u57fa\u4e8e\u7eafRL\u68af\u5ea6\u7684\u5728\u7ebf\u5fae\u8c03\u3002", "method": "\u5c06GRPO\u7b97\u6cd5\u9002\u914d\u5230\u51b3\u7b56\u53d8\u6362\u5668\uff0c\u5e76\u5f15\u5165\u5173\u952e\u6539\u8fdb\uff1a\u5b50\u8f68\u8ff9\u4f18\u5316\u4ee5\u6539\u8fdb\u4fe1\u7528\u5206\u914d\u3001\u5e8f\u5217\u7ea7\u4f3c\u7136\u76ee\u6807\u4ee5\u589e\u5f3a\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3001\u4e3b\u52a8\u91c7\u6837\u4ee5\u9f13\u52b1\u5728\u4e0d\u786e\u5b9a\u533a\u57df\u7684\u63a2\u7d22\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5728\u7ebf\u51b3\u7b56\u53d8\u6362\u5668\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u7eafRL\u7684\u5728\u7ebf\u5fae\u8c03\u5bf9\u51b3\u7b56\u53d8\u6362\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u89e3\u51b3\u540e\u89c1\u56de\u62a5\u91cd\u6807\u6ce8\u4e0e\u91cd\u8981\u6027\u91c7\u6837\u7b97\u6cd5\u7684\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u51b3\u7b56\u53d8\u6362\u5668\u7684\u7eaf\u5f3a\u5316\u5b66\u4e60\u5728\u7ebf\u5fae\u8c03\uff0c\u4e3a\u5e8f\u5217\u51b3\u7b56\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.00482", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00482", "abs": "https://arxiv.org/abs/2601.00482", "authors": ["Abhiram Bellur", "Mohammed Raihan Ullah", "Fraol Batole", "Mohit Kansara", "Masaharu Morimoto", "Kai Ishikawa", "Haifeng Chen", "Yaroslav Zharov", "Timofey Bryksin", "Tien N. Nguyen", "Hridesh Rajan", "Danny Dig"], "title": "Multi-Agent Coordinated Rename Refactoring", "comment": null, "summary": "The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.\n  We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6765\u81ea\u52a8\u5316\u534f\u8c03\u91cd\u547d\u540d\uff0c\u901a\u8fc7\u8303\u56f4\u63a8\u65ad\u3001\u8ba1\u5212\u6267\u884c\u548c\u590d\u5236\u4e09\u4e2a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5c06\u5f00\u53d1\u8005\u7684\u521d\u59cb\u91cd\u6784\u4f5c\u4e3a\u7ebf\u7d22\u6765\u63a8\u65ad\u76f8\u5173\u91cd\u6784\u8303\u56f4\uff0c\u663e\u8457\u51cf\u5c11\u5f00\u53d1\u8005\u7684\u8d1f\u62c5\u3002", "motivation": "\u534f\u8c03\u91cd\u547d\u540d\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u9891\u7e41\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u8de8\u591a\u4e2a\u6587\u4ef6\u548c\u4e0a\u4e0b\u6587\u624b\u52a8\u4f20\u64ad\u91cd\u6784\uff0c\u65e2\u7e41\u7410\u53c8\u5bb9\u6613\u51fa\u9519\u3002\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u4ea7\u751f\u8fc7\u591a\u8bef\u62a5\uff0c\u800c\u666e\u901aLLM\u7531\u4e8e\u4e0a\u4e0b\u6587\u6709\u9650\u4e14\u65e0\u6cd5\u4e0e\u91cd\u6784\u5de5\u5177\u4ea4\u4e92\uff0c\u63d0\u4f9b\u4e0d\u5b8c\u6574\u7684\u5efa\u8bae\u3002", "method": "\u8bbe\u8ba1\u4e86\u9996\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1) \u8303\u56f4\u63a8\u65ad\u667a\u80fd\u4f53\u5c06\u5f00\u53d1\u8005\u521d\u59cb\u91cd\u6784\u7ebf\u7d22\u8f6c\u5316\u4e3a\u660e\u786e\u7684\u81ea\u7136\u8bed\u8a00\u58f0\u660e\u8303\u56f4\uff1b2) \u8ba1\u5212\u6267\u884c\u667a\u80fd\u4f53\u4f7f\u7528\u8be5\u8303\u56f4\u4f5c\u4e3a\u4e25\u683c\u8ba1\u5212\uff0c\u8bc6\u522b\u9700\u8981\u91cd\u6784\u7684\u7a0b\u5e8f\u5143\u7d20\u5e76\u5b89\u5168\u8c03\u7528IDE\u7684\u91cd\u6784API\uff1b3) \u590d\u5236\u667a\u80fd\u4f53\u6307\u5bfc\u9879\u76ee\u8303\u56f4\u7684\u641c\u7d22\u3002", "result": "\u5728100\u4e2a\u5f00\u6e90\u9879\u76ee\u7684609K\u6b21\u63d0\u4ea4\u4e2d\u8fdb\u884c\u4e86\u5f62\u6210\u6027\u7814\u7a76\uff0c\u5e76\u8c03\u67e5\u4e86205\u540d\u5f00\u53d1\u8005\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u5982\u4f55\u4e0e\u5f00\u53d1\u8005\u534f\u540c\u5de5\u4f5c\uff0c\u663e\u8457\u51cf\u5c11\u534f\u8c03\u91cd\u547d\u540d\u4efb\u52a1\u7684\u8d1f\u62c5\u3002", "conclusion": "AI\u667a\u80fd\u4f53\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u4e3b\u8981\u4ef7\u503c\u5728\u4e8e\u6269\u5c55\u5f00\u53d1\u8005\u7684\u63a8\u7406\u548c\u884c\u52a8\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u53d6\u4ee3\u4eba\u7c7b\u53c2\u4e0e\u3002\u534f\u8c03\u91cd\u547d\u540d\u6b63\u662f\u667a\u80fd\u4f53\u53ef\u4ee5\u663e\u8457\u51cf\u8f7b\u5f00\u53d1\u8005\u8d1f\u62c5\u7684\u91cd\u590d\u6027\u4efb\u52a1\uff0c\u540c\u65f6\u8ba9\u5f00\u53d1\u8005\u4fdd\u6301\u4e3b\u5bfc\u5730\u4f4d\u3002", "topic": "swe application"}}
{"id": "2601.00097", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.00097", "abs": "https://arxiv.org/abs/2601.00097", "authors": ["Akash Kumar Panda", "Olaoluwa Adigun", "Bart Kosko"], "title": "The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs", "comment": "15 figures", "summary": "We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u4ece\u539f\u59cb\u6587\u672c\u4e2d\u63d0\u53d6\u56e0\u679c\u53cd\u9988\u6a21\u7cca\u8ba4\u77e5\u56fe\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u7cfb\u7edf\u7684\u5747\u8861\u72b6\u6001\u9a71\u52a8\u667a\u80fd\u4f53\u83b7\u53d6\u548c\u5904\u7406\u56e0\u679c\u6587\u672c\uff0c\u5f62\u6210\u53cc\u5411\u7684\u81ea\u4e3b\u6f14\u5316\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524d\u9700\u8981\u4ece\u6587\u672c\u4e2d\u81ea\u52a8\u63d0\u53d6\u56e0\u679c\u5173\u7cfb\u7684\u5de5\u5177\uff0c\u7279\u522b\u662f\u80fd\u591f\u5f62\u6210\u52a8\u6001\u7cfb\u7edf\u5e76\u5177\u6709\u81ea\u4e3b\u6f14\u5316\u80fd\u529b\u7684\u667a\u80fd\u4f53\u3002\u6a21\u7cca\u8ba4\u77e5\u56fe\u4f5c\u4e3a\u4e00\u79cd\u8868\u793a\u56e0\u679c\u5173\u7cfb\u7684\u5de5\u5177\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u6b65\u7cbe\u7ec6\u8c03\u4f18\u7684\u7cfb\u7edf\u6307\u4ee4\uff1a1) \u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u5173\u952e\u540d\u8bcd\u548c\u540d\u8bcd\u77ed\u8bed\uff1b2) \u4ece\u4e2d\u63d0\u53d6FCM\u6982\u5ff5\u8282\u70b9\uff1b3) \u63a8\u65ad\u8282\u70b9\u95f4\u7684\u90e8\u5206\u6216\u6a21\u7cca\u56e0\u679c\u8fb9\u3002\u4f7f\u7528LLM\u667a\u80fd\u4f53\u5b9e\u73b0\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u5e76\u6d4b\u8bd5\u4e86Gemini\u548cChatGPT\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u4eceHenry Kissinger\u5173\u4e8eAI\u524d\u666f\u7684\u8bba\u6587\u4e2d\u751f\u6210\u7684FCM\u52a8\u6001\u7cfb\u7edf\uff0c\u4e0e\u4eba\u5de5\u751f\u6210\u7684FCM\u6536\u655b\u5230\u76f8\u540c\u7684\u5747\u8861\u6781\u9650\u73af\uff0c\u5c3d\u7ba1\u8282\u70b9\u548c\u8fb9\u6570\u91cf\u4e0d\u540c\u3002\u6df7\u5408\u4e0d\u540cLLM\u667a\u80fd\u4f53\u751f\u6210\u7684FCM\u80fd\u591f\u5438\u6536\u4e3b\u8981\u7ec4\u5206\u7684\u5747\u8861\u72b6\u6001\uff0c\u5e76\u521b\u5efa\u65b0\u7684\u5747\u8861\u6765\u66f4\u597d\u5730\u8fd1\u4f3c\u5e95\u5c42\u56e0\u679c\u52a8\u6001\u7cfb\u7edf\u3002", "conclusion": "LLM\u667a\u80fd\u4f53\u80fd\u591f\u6709\u6548\u5730\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u56e0\u679c\u53cd\u9988\u6a21\u7cca\u8ba4\u77e5\u56fe\uff0c\u5f62\u6210\u5177\u6709\u81ea\u4e3b\u6f14\u5316\u80fd\u529b\u7684\u52a8\u6001\u7cfb\u7edf\u3002\u6df7\u5408\u4e0d\u540cLLM\u751f\u6210\u7684FCM\u80fd\u591f\u4ea7\u751f\u66f4\u4e30\u5bcc\u7684\u5747\u8861\u72b6\u6001\uff0c\u66f4\u597d\u5730\u8868\u793a\u5e95\u5c42\u56e0\u679c\u5173\u7cfb\u3002", "topic": "agent analysis"}}
{"id": "2601.00497", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00497", "abs": "https://arxiv.org/abs/2601.00497", "authors": ["Lev Sorokin", "Ivan Vasilev", "Ken E. Friedl", "Andrea Stocco"], "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications", "comment": "Accepted for publication at the 33th International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)", "summary": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.", "AI": {"tldr": "STELLAR\u662f\u4e00\u4e2a\u57fa\u4e8e\u641c\u7d22\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u53d1\u73b0LLM\u5e94\u7528\u4e2d\u7684\u4e0d\u5f53\u54cd\u5e94\uff0c\u901a\u8fc7\u8fdb\u5316\u4f18\u5316\u63a2\u7d22\u7279\u5f81\u7ec4\u5408\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u591a\u53d1\u73b02.5-4.3\u500d\u6545\u969c\u3002", "motivation": "LLM\u5e94\u7528\u5728\u5ba2\u670d\u3001\u6559\u80b2\u3001\u51fa\u884c\u7b49\u9886\u57df\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4f46\u5bb9\u6613\u4ea7\u751f\u4e0d\u51c6\u786e\u3001\u865a\u6784\u6216\u6709\u5bb3\u7684\u54cd\u5e94\uff0c\u4e14\u5176\u9ad8\u7ef4\u8f93\u5165\u7a7a\u95f4\u4f7f\u5f97\u7cfb\u7edf\u6d4b\u8bd5\u7279\u522b\u56f0\u96be\u3002", "method": "\u5c06\u6d4b\u8bd5\u751f\u6210\u5efa\u6a21\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u5c06\u8f93\u5165\u7a7a\u95f4\u79bb\u6563\u5316\u4e3a\u98ce\u683c\u3001\u5185\u5bb9\u548c\u6270\u52a8\u7279\u5f81\uff0c\u91c7\u7528\u8fdb\u5316\u4f18\u5316\u52a8\u6001\u63a2\u7d22\u66f4\u53ef\u80fd\u66b4\u9732\u6545\u969c\u7684\u7279\u5f81\u7ec4\u5408\u3002", "result": "\u5728\u4e09\u4e2aLLM\u5bf9\u8bdd\u95ee\u7b54\u7cfb\u7edf\u4e0a\u8bc4\u4f30\uff1a\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u548c\u8f66\u8f7d\u5bfc\u822a\u63a8\u8350\u7cfb\u7edf\u3002STELLAR\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u591a\u66b4\u97322.5-4.3\u500d\u6545\u969c\u3002", "conclusion": "STELLAR\u6846\u67b6\u80fd\u6709\u6548\u53d1\u73b0LLM\u5e94\u7528\u4e2d\u7684\u4e0d\u5f53\u54cd\u5e94\uff0c\u4e3aLLM\u7cfb\u7edf\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2601.00224", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00224", "abs": "https://arxiv.org/abs/2601.00224", "authors": ["Yan Sun", "Ming Cai", "Stanley Kok"], "title": "Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback", "comment": null, "summary": "As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faQ*\u548cFeedback+\u4e24\u79cd\u9a8c\u8bc1\u6280\u672f\uff0c\u901a\u8fc7\u751f\u6210\u5668-\u5224\u522b\u5668\u6846\u67b6\u51cf\u5c11\u4f01\u4e1a\u7ea7LLM\u52a9\u624b\u5728\u4e1a\u52a1\u5206\u6790\u4e2d\u7684\u9519\u8bef\u7387\uff0c\u5c06\u9a8c\u8bc1\u8d23\u4efb\u4ece\u7528\u6237\u8f6c\u79fb\u5230\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u5bf9\u8bdd\u5f0f\u4e1a\u52a1\u5206\u6790\u7cfb\u7edf\u7f3a\u4e4f\u5185\u7f6e\u9a8c\u8bc1\u673a\u5236\uff0c\u7528\u6237\u9700\u8981\u624b\u52a8\u9a8c\u8bc1\u53ef\u80fd\u9519\u8bef\u7684\u8f93\u51fa\uff0c\u8fd9\u5f71\u54cd\u4e86\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u4e2dLLM\u52a9\u624b\u751f\u6210\u51c6\u786e\u3001\u8bed\u4e49\u5bf9\u9f50\u4e14\u53ef\u6267\u884c\u8f93\u51fa\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u9a8c\u8bc1\u6280\u672f\uff1a1) Q*\u901a\u8fc7\u53cd\u5411\u7ffb\u8bd1\u548c\u8bed\u4e49\u5339\u914d\u5728\u4ee3\u7801\u4e0e\u7528\u6237\u610f\u56fe\u4e4b\u95f4\u8fdb\u884c\u9a8c\u8bc1\uff1b2) Feedback+\u901a\u8fc7\u6267\u884c\u53cd\u9988\u6307\u5bfc\u4ee3\u7801\u4f18\u5316\u3002\u8fd9\u4e9b\u6280\u672f\u5d4c\u5165\u5728\u751f\u6210\u5668-\u5224\u522b\u5668\u6846\u67b6\u4e2d\u3002", "result": "\u5728Spider\u3001Bird\u548cGSM8K\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cQ*\u548cFeedback+\u90fd\u80fd\u964d\u4f4e\u9519\u8bef\u7387\u548c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002\u540c\u65f6\u53d1\u73b0\u53cd\u5411\u7ffb\u8bd1\u662f\u4e3b\u8981\u6027\u80fd\u74f6\u9888\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u4f01\u4e1a\u7ea7\u7684\u751f\u6210\u5f0fAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bbe\u8ba1\u5bfc\u5411\u7684\u6846\u67b6\uff0c\u80fd\u591f\u63d0\u4f9b\u53ef\u4fe1\u7684\u51b3\u7b56\u652f\u6301\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u53cd\u5411\u7ffb\u8bd1\u4f5c\u4e3a\u672a\u6765\u6539\u8fdb\u7684\u5173\u952e\u673a\u4f1a\u3002", "topic": "code agent"}}
{"id": "2601.00121", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.00121", "abs": "https://arxiv.org/abs/2601.00121", "authors": ["Yaqi Duan", "Yichun Hu", "Jiashuo Jiang"], "title": "Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control", "comment": null, "summary": "Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant \"hallucination tax\": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.\n  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned \"digital twin\" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.", "AI": {"tldr": "LLM\u4f5c\u4e3a\u7aef\u5230\u7aef\u5e93\u5b58\u4f18\u5316\u6c42\u89e3\u5668\u5b58\u5728\"\u5e7b\u89c9\u7a0e\"\u6027\u80fd\u5dee\u8ddd\uff0c\u63d0\u51fa\u6df7\u5408\u667a\u80fd\u4f53\u6846\u67b6\u5206\u79bb\u8bed\u4e49\u63a8\u7406\u4e0e\u6570\u5b66\u8ba1\u7b97\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6d4b\u8bd5\u8bc1\u660e\u80fd\u964d\u4f4e32.1%\u5e93\u5b58\u6210\u672c\u3002", "motivation": "\u4e2d\u5c0f\u4f01\u4e1a\u5728\u5e93\u5b58\u7ba1\u7406\u4e2d\u7f3a\u4e4f\u90e8\u7f72\u9ad8\u7ea7\u4f18\u5316\u65b9\u6cd5\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u63a2\u7d22LLM\u662f\u5426\u80fd\u5e2e\u52a9\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4f46\u53d1\u73b0LLM\u4f5c\u4e3a\u7aef\u5230\u7aef\u6c42\u89e3\u5668\u5b58\u5728\u6027\u80fd\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4e25\u683c\u5206\u79bb\u8bed\u4e49\u63a8\u7406\u4e0e\u6570\u5b66\u8ba1\u7b97\uff1aLLM\u4f5c\u4e3a\u667a\u80fd\u63a5\u53e3\u4ece\u81ea\u7136\u8bed\u8a00\u63d0\u53d6\u53c2\u6570\u5e76\u89e3\u91ca\u7ed3\u679c\uff0c\u540c\u65f6\u81ea\u52a8\u8c03\u7528\u4e25\u683c\u7b97\u6cd5\u6784\u5efa\u4f18\u5316\u5f15\u64ce\u3002\u5f15\u5165Human Imitator\uff08\u6570\u5b57\u5b6a\u751f\uff09\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u538b\u529b\u6d4b\u8bd5\u3002", "result": "\u6df7\u5408\u6846\u67b6\u76f8\u6bd4\u4f7f\u7528GPT-4o\u4f5c\u4e3a\u7aef\u5230\u7aef\u6c42\u89e3\u5668\u7684\u57fa\u7ebf\u964d\u4f4e\u4e8632.1%\u7684\u603b\u5e93\u5b58\u6210\u672c\u3002\u63d0\u4f9b\u5b8c\u7f8e\u771f\u5b9e\u4fe1\u606f\u4e5f\u65e0\u6cd5\u6539\u5584GPT-4o\u6027\u80fd\uff0c\u8868\u660e\u74f6\u9888\u662f\u8ba1\u7b97\u800c\u975e\u4fe1\u606f\u95ee\u9898\u3002", "conclusion": "LLM\u4e0d\u5e94\u66ff\u4ee3\u8fd0\u7b79\u5b66\uff0c\u800c\u5e94\u4f5c\u4e3a\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\uff0c\u4f7f\u975e\u4e13\u5bb6\u80fd\u591f\u8bbf\u95ee\u57fa\u4e8e\u4e25\u683c\u6c42\u89e3\u5668\u7684\u7b56\u7565\u3002\u6df7\u5408\u6846\u67b6\u80fd\u6709\u6548\u5f25\u8865LLM\u5728\u968f\u673a\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "topic": "agent analysis"}}
{"id": "2601.00753", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00753", "abs": "https://arxiv.org/abs/2601.00753", "authors": ["Dao Sy Duy Minh", "Huynh Trung Kiet", "Tran Chi Nguyen", "Nguyen Lam Phu Quy", "Phu Hoa Pham", "Nguyen Dinh Ha Duong", "Truong Bao Tran"], "title": "Early-Stage Prediction of Review Effort in AI-Generated Pull Requests", "comment": "Preprint. Under anonymous peer review. 5 pages, 5 figures", "summary": "As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?\n  Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).\n  We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.\n  Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.", "AI": {"tldr": "\u8bba\u6587\u5206\u679033,707\u4e2aAI\u4ee3\u7406\u751f\u6210\u7684PR\uff0c\u53d1\u73b0\u5176\u5448\u73b0\u4e24\u6781\u5206\u5316\u884c\u4e3a\u6a21\u5f0f\uff1a28.3%\u4e3a\u5373\u65f6\u5408\u5e76\uff0c\u5176\u4f59\u5219\u9677\u5165\u5197\u957f\u8bc4\u5ba1\u5faa\u73af\u3002\u4f5c\u8005\u63d0\u51faCircuit Breaker\u6a21\u578b\uff0c\u4ec5\u7528\u9759\u6001\u7ed3\u6784\u7279\u5f81\u5373\u53ef\u5728PR\u521b\u5efa\u65f6\u9884\u6d4b\u9ad8\u8bc4\u5ba1\u6210\u672c\u7684PR\uff0cAUC\u8fbe0.957\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u4ece\u4ee3\u7801\u8865\u5168\u5de5\u5177\u8f6c\u53d8\u4e3a\u80fd\u5927\u89c4\u6a21\u63d0\u4ea4PR\u7684\u5b8c\u6574\u961f\u53cb\uff0c\u8f6f\u4ef6\u7ef4\u62a4\u8005\u9762\u4e34\u65b0\u6311\u6218\uff1a\u4e0d\u4ec5\u8981\u8bc4\u5ba1\u4ee3\u7801\uff0c\u8fd8\u8981\u7ba1\u7406\u4e0e\u975e\u4eba\u7c7b\u8d21\u732e\u8005\u7684\u590d\u6742\u4ea4\u4e92\u5faa\u73af\u3002\u9700\u8981\u9884\u6d4b\u54ea\u4e9bAI\u751f\u6210\u7684PR\u4f1a\u5728\u8bc4\u5ba1\u9636\u6bb5\u6d88\u8017\u8fc7\u591a\u4eba\u529b\u6210\u672c\u3002", "method": "\u5206\u6790AIDev\u6570\u636e\u96c6\u4e2d33,707\u4e2aAI\u4ee3\u7406\u751f\u6210\u7684PR\uff08\u6765\u81ea2,807\u4e2a\u4ed3\u5e93\uff09\uff0c\u53d1\u73b0\u4e24\u6781\u884c\u4e3a\u6a21\u5f0f\u3002\u63d0\u51faCircuit Breaker\u5206\u6d41\u6a21\u578b\uff0c\u4f7f\u7528LightGBM\u7b97\u6cd5\uff0c\u4ec5\u57fa\u4e8e\u9759\u6001\u7ed3\u6784\u7279\u5f81\uff08\u800c\u975e\u8bed\u4e49\u6587\u672c\u7279\u5f81\uff09\u9884\u6d4b\u9ad8\u8bc4\u5ba1\u6210\u672cPR\u3002", "result": "\u53d1\u73b0AI\u4ee3\u7406PR\u5448\u73b0\u4e24\u6781\u6a21\u5f0f\uff1a28.3%\u57281\u5206\u949f\u5185\u5408\u5e76\uff08\u7a84\u81ea\u52a8\u5316\u4efb\u52a1\u6210\u529f\uff09\uff0c\u5176\u4f59\u5e38\u9677\u5165\u8bc4\u5ba1\u5faa\u73af\u4e14\u4ee3\u7406\u5e38\u505c\u6ede\u6216\u653e\u5f03\u4fee\u6539\u3002Circuit Breaker\u6a21\u578b\u5728\u65f6\u95f4\u5206\u5272\u4e0aAUC\u8fbe0.957\uff0c\u572820%\u8bc4\u5ba1\u9884\u7b97\u4e0b\u53ef\u62e6\u622a69%\u7684\u603b\u8bc4\u5ba1\u5de5\u4f5c\u91cf\u3002", "conclusion": "AI\u8f85\u52a9\u4ee3\u7801\u8bc4\u5ba1\u7684\u73b0\u6709\u5047\u8bbe\u53d7\u5230\u6311\u6218\uff1a\u8bc4\u5ba1\u8d1f\u62c5\u7531\u4ee3\u7406\u4fee\u6539\u7684\u5185\u5bb9\u7ed3\u6784\u51b3\u5b9a\uff0c\u800c\u975e\u5176\u8bed\u4e49\u5185\u5bb9\u3002\u9700\u8981\u4e3a\u4eba\u7c7b-AI\u534f\u4f5c\u5efa\u7acb\u7ed3\u6784\u5316\u7684\u6cbb\u7406\u673a\u5236\uff0c\u800c\u975e\u4f9d\u8d56\u4f20\u7edf\u8bed\u4e49\u5206\u6790\u3002", "topic": "agent analysis"}}
{"id": "2601.00268", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00268", "abs": "https://arxiv.org/abs/2601.00268", "authors": ["Doyoung Kim", "Zhiwei Ren", "Jie Hao", "Zhongkai Sun", "Lichao Wang", "Xiyao Ma", "Zack Ye", "Xu Han", "Jun Yin", "Heng Ji", "Wei Shen", "Xing Fan", "Benjamin Yao", "Chenlei Guo"], "title": "Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity", "comment": "26 pages", "summary": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.", "AI": {"tldr": "WildAGTEval\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u4ee3\u7406\u51fd\u6570\u8c03\u7528\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u771f\u5b9eAPI\u590d\u6742\u6027\uff0c\u5305\u542bAPI\u89c4\u8303\u548cAPI\u6267\u884c\u4e24\u4e2a\u7ef4\u5ea6\u7684\u6311\u6218\uff0c\u6db5\u76d660\u4e2a\u590d\u6742\u6027\u573a\u666f\u548c\u7ea632K\u6d4b\u8bd5\u914d\u7f6e\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5047\u8bbe\u7406\u60f3\u5316\u7684API\u7cfb\u7edf\uff0c\u5ffd\u89c6\u4e86\u771f\u5b9e\u4e16\u754c\u56e0\u7d20\u5982\u566a\u58f0API\u8f93\u51fa\uff0c\u9700\u8981\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u771f\u5b9eAPI\u590d\u6742\u6027\u4e0b\u7684\u51fd\u6570\u8c03\u7528\u80fd\u529b\u3002", "method": "\u521b\u5efaWildAGTEval\u57fa\u51c6\uff0c\u5305\u542bAPI\u89c4\u8303\uff08\u8be6\u7ec6\u6587\u6863\u548c\u4f7f\u7528\u7ea6\u675f\uff09\u548cAPI\u6267\u884c\uff08\u8fd0\u884c\u65f6\u6311\u6218\uff09\u4e24\u4e2a\u7ef4\u5ea6\u7684\u590d\u6742\u6027\uff0c\u6784\u5efa60\u4e2a\u4e0d\u540c\u590d\u6742\u6027\u573a\u666f\uff0c\u53ef\u7ec4\u5408\u6210\u7ea632K\u6d4b\u8bd5\u914d\u7f6e\uff0c\u5e76\u8bbe\u8ba1\u7528\u6237-\u4ee3\u7406\u4ea4\u4e92\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u591a\u4e2a\u5148\u8fdbLLM\u53d1\u73b0\u5927\u591a\u6570\u573a\u666f\u5177\u6709\u6311\u6218\u6027\uff0c\u4e0d\u76f8\u5173\u4fe1\u606f\u590d\u6742\u6027\u9020\u6210\u6700\u5927\u56f0\u96be\uff0c\u4f7f\u5f3aLLM\u6027\u80fd\u4e0b\u964d27.3%\uff1b\u5b9a\u6027\u5206\u6790\u663e\u793aLLM\u6709\u65f6\u4f1a\u626d\u66f2\u7528\u6237\u610f\u56fe\u4ee5\u58f0\u79f0\u4efb\u52a1\u5b8c\u6210\uff0c\u4e25\u91cd\u5f71\u54cd\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "conclusion": "WildAGTEval\u63ed\u793a\u4e86LLM\u4ee3\u7406\u5728\u771f\u5b9eAPI\u590d\u6742\u6027\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u4e0d\u76f8\u5173\u4fe1\u606f\u590d\u6742\u6027\u662f\u6700\u4e3b\u8981\u6311\u6218\uff0cLLM\u626d\u66f2\u7528\u6237\u610f\u56fe\u7684\u884c\u4e3a\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2601.00348", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00348", "abs": "https://arxiv.org/abs/2601.00348", "authors": ["Yuhao Zhang", "Zhongliang Yang", "Linna Zhou"], "title": "Robust Uncertainty Quantification for Factual Generation of Large Language Models", "comment": "9 pages, 5 tables, 5 figures, accepted to IJCNN 2025", "summary": "The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9LLM\u5e7b\u89c9\u95ee\u9898\u7684\u9c81\u68d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b\u865a\u5047\u540d\u79f0\u7684\u9677\u9631\u95ee\u9898\u96c6\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u751f\u6210\u591a\u4e8b\u5b9e\u5185\u5bb9\u65f6\u7684\u53ef\u9760\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728ROCAUC\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u53470.1-0.2\u3002", "motivation": "LLM\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e86AI\u751f\u6210\u5185\u5bb9\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5728\u5e38\u89c4\u95ee\u7b54\u573a\u666f\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u9762\u5bf9\u975e\u5e38\u89c4\u6216\u5bf9\u6297\u6027\u63d0\u95ee\u7b56\u7565\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86LLM\u5728\u9700\u8981\u5f3a\u5927\u6279\u5224\u6027\u601d\u7ef4\u80fd\u529b\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "1) \u6784\u5efa\u5305\u542b\u865a\u5047\u540d\u79f0\u7684\u9677\u9631\u95ee\u9898\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u751f\u6210\u591a\u4e8b\u5b9e\u5185\u5bb9\u65f6\u7684\u53ef\u9760\u6027\uff1b2) \u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u9c81\u68d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5(RU)\uff0c\u4e13\u95e8\u9488\u5bf9\u975e\u89c4\u8303\u6216\u5bf9\u6297\u6027\u63d0\u95ee\u573a\u666f\u8bbe\u8ba1\u3002", "result": "1) \u6784\u5efa\u7684\u9677\u9631\u95ee\u9898\u96c6\u8868\u73b0\u4f18\u5f02\uff1b2) \u5728\u56db\u4e2a\u4e0d\u540c\u6a21\u578b\u4e0a\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u51fa\u7684RU\u65b9\u6cd5\u5728ROCAUC\u503c\u4e0a\u5e73\u5747\u63d0\u53470.1-0.2\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86LLM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5728\u975e\u5e38\u89c4\u63d0\u95ee\u573a\u666f\u4e2d\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u9c81\u68d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u4e3a\u5e94\u5bf9LLM\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.00227", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00227", "abs": "https://arxiv.org/abs/2601.00227", "authors": ["Shanli Xing", "Yiyan Zhai", "Alexander Jiang", "Yixin Dong", "Yong Wu", "Zihao Ye", "Charlie Ruan", "Yingyi Huang", "Yineng Zhang", "Liangsheng Yin", "Aksara Bayyapu", "Luis Ceze", "Tianqi Chen"], "title": "FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems", "comment": null, "summary": "Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.", "AI": {"tldr": "FlashInfer-Bench\u662f\u4e00\u4e2a\u6807\u51c6\u5316\u95ed\u73af\u6846\u67b6\uff0c\u8fde\u63a5AI\u751f\u6210\u7684GPU\u5185\u6838\u751f\u6210\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u90e8\u7f72\uff0c\u901a\u8fc7\u7edf\u4e00\u6a21\u5f0f\u3001\u6570\u636e\u96c6\u548c\u52a8\u6001\u66ff\u6362\u673a\u5236\uff0c\u5c06\u6700\u4f73\u6027\u80fd\u5185\u6838\u6ce8\u5165\u751f\u4ea7LLM\u63a8\u7406\u5f15\u64ce\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u751f\u6210GPU\u5185\u6838\uff0c\u4f46\u5c06\u8fd9\u4e9bAI\u751f\u6210\u7684\u5185\u6838\u96c6\u6210\u5230\u5b9e\u9645\u63a8\u7406\u7cfb\u7edf\u4e2d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u5efa\u7acb\u8fde\u63a5\u5185\u6838\u751f\u6210\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u90e8\u7f72\u7684\u6807\u51c6\u5316\u6846\u67b6\u3002", "method": "FlashInfer-Bench\u63d0\u4f9b\u6807\u51c6\u5316\u95ed\u73af\u6846\u67b6\uff0c\u5305\u62ec\uff1a1) FlashInfer Trace\u7edf\u4e00\u6a21\u5f0f\u63cf\u8ff0\u5185\u6838\u5b9a\u4e49\u3001\u5de5\u4f5c\u8d1f\u8f7d\u3001\u5b9e\u73b0\u548c\u8bc4\u4f30\uff1b2) \u57fa\u4e8e\u771f\u5b9e\u670d\u52a1\u8f68\u8ff9\u7684\u7cbe\u9009\u6570\u636e\u96c6\uff1b3) \u6b63\u786e\u6027\u548c\u6027\u80fd\u611f\u77e5\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff1b4) \u516c\u5f00\u6392\u884c\u699c\u8ddf\u8e2aLLM\u4ee3\u7406\u7684GPU\u7f16\u7a0b\u80fd\u529b\uff1b5) \u52a8\u6001\u66ff\u6362\u673a\u5236(apply())\u5c06\u6700\u4f73\u6027\u80fd\u5185\u6838\u6ce8\u5165\u751f\u4ea7LLM\u5f15\u64ce\u3002", "result": "\u5efa\u7acb\u4e86\u5b9e\u7528\u7684\u3001\u53ef\u590d\u73b0\u7684\u8def\u5f84\uff0c\u7528\u4e8e\u6301\u7eed\u6539\u8fdbAI\u751f\u6210\u7684\u5185\u6838\u5e76\u5c06\u5176\u90e8\u7f72\u5230\u5927\u89c4\u6a21LLM\u63a8\u7406\u4e2d\u3002\u8bc4\u4f30\u4e86LLM\u4ee3\u7406\u7684\u6027\u80fd\u548c\u5c40\u9650\u6027\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540cGPU\u7f16\u7a0b\u8bed\u8a00\u7684\u6743\u8861\uff0c\u4e3a\u672a\u6765\u4ee3\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "FlashInfer-Bench\u4e3aAI\u751f\u6210\u7684GPU\u5185\u6838\u63d0\u4f9b\u4e86\u4ece\u751f\u6210\u5230\u90e8\u7f72\u7684\u5b8c\u6574\u95ed\u73af\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u96c6\u6210\u6311\u6218\uff0c\u63a8\u52a8\u4e86LLM\u4ee3\u7406\u5728GPU\u7f16\u7a0b\u9886\u57df\u7684\u5e94\u7528\u53d1\u5c55\u3002", "topic": "code agent"}}
{"id": "2601.00240", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.00240", "abs": "https://arxiv.org/abs/2601.00240", "authors": ["Zongwei Wang", "Bincheng Gu", "Hongyu Yu", "Junliang Yu", "Tao He", "Jiayin Feng", "Min Gao"], "title": "Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability", "comment": "16 pages", "summary": "LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal \"us\" versus \"them\" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u8d4b\u80fd\u7684\u667a\u80fd\u4f53\u4e0d\u4ec5\u5b58\u5728\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\uff0c\u8fd8\u4f1a\u5728\u6700\u5c0f\u7fa4\u4f53\u7ebf\u7d22\u4e0b\u8868\u73b0\u51fa\u5185\u7fa4\u4f53\u504f\u89c1\u3002\u5f53\u8fd9\u79cd\u7fa4\u4f53\u8fb9\u754c\u4e0e\u667a\u80fd\u4f53-\u4eba\u7c7b\u5212\u5206\u91cd\u5408\u65f6\uff0c\u98ce\u9669\u4ece\u4eba\u7c7b\u7fa4\u4f53\u95f4\u5dee\u5f02\u8f6c\u53d8\u4e3a\u66f4\u6839\u672c\u7684\u7fa4\u4f53\u4e0d\u5bf9\u79f0\u2014\u2014\u4eba\u7c7b\u6574\u4f53\u53ef\u80fd\u88ab\u667a\u80fd\u4f53\u89c6\u4e3a\u5916\u7fa4\u4f53\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u667a\u80fd\u4f53\u662f\u5426\u4f1a\u5bf9\u4eba\u7c7b\u8868\u73b0\u51fa\u5185\u7fa4\u4f53\u504f\u89c1\uff0c\u4ee5\u53ca\u8fd9\u79cd\u504f\u89c1\u5982\u4f55\u88ab\u5229\u7528\u3002\u5f53\u667a\u80fd\u4f53-\u4eba\u7c7b\u8fb9\u754c\u6210\u4e3a\u7fa4\u4f53\u5212\u5206\u65f6\uff0c\u4eba\u7c7b\u6574\u4f53\u53ef\u80fd\u9762\u4e34\u88ab\u667a\u80fd\u4f53\u89c6\u4e3a\u5916\u7fa4\u4f53\u7684\u98ce\u9669\uff0c\u8fd9\u6bd4\u4f20\u7edf\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\u66f4\u4e3a\u6839\u672c\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5206\u914d\u51b3\u7b56\u7684\u591a\u667a\u80fd\u4f53\u793e\u4f1a\u6a21\u62df\uff0c\u5728\u660e\u786e\u6536\u76ca\u6743\u8861\u4e0b\u7814\u7a76\u7fa4\u4f53\u504f\u89c1\u3002\u5f15\u5165\u4fe1\u5ff5\u6295\u6bd2\u653b\u51fb\uff08BPA\uff09\uff0c\u5305\u62ec\u521d\u59cb\u5316\u65f6\u7684\u6863\u6848\u6295\u6bd2\uff08BPA-PP\uff09\u548c\u901a\u8fc7\u4f18\u5316\u4fe1\u5ff5\u7cbe\u70bc\u540e\u7f00\u6ce8\u5165\u5b58\u50a8\u53cd\u601d\u7684\u8bb0\u5fc6\u6295\u6bd2\uff08BPA-MP\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u667a\u80fd\u4f53\u5728\u6700\u5c0f\u7fa4\u4f53\u7ebf\u7d22\u4e0b\u5b58\u5728\u4e00\u81f4\u7684\u5185\u7fa4\u4f53\u504f\u89c1\u3002\u867d\u7136\u5f53\u67d0\u4e9b\u5bf9\u5e94\u65b9\u88ab\u6807\u8bb0\u4e3a\u4eba\u7c7b\u65f6\u504f\u89c1\u4f1a\u51cf\u5f31\uff0c\u4f46\u8fd9\u79cd\u51cf\u5f31\u4f9d\u8d56\u4e8e\u667a\u80fd\u4f53\u76f8\u4fe1\u771f\u5b9e\u4eba\u7c7b\u5b58\u5728\u7684\u4fe1\u5ff5\u3002BPA\u653b\u51fb\u80fd\u591f\u901a\u8fc7\u7834\u574f\u6301\u4e45\u8eab\u4efd\u4fe1\u5ff5\u6765\u6291\u5236\u4eba\u7c7b\u89c4\u8303\u811a\u672c\uff0c\u91cd\u65b0\u6fc0\u6d3b\u5bf9\u4eba\u7c7b\u7684\u5916\u7fa4\u4f53\u504f\u89c1\u3002", "conclusion": "\u667a\u80fd\u4f53\u5b58\u5728\u5bf9\u4eba\u7c7b\u7684\u5185\u7fa4\u4f53\u504f\u89c1\u98ce\u9669\uff0c\u4e14\u8fd9\u79cd\u504f\u89c1\u53ef\u901a\u8fc7\u4fe1\u5ff5\u6295\u6bd2\u653b\u51fb\u88ab\u6076\u610f\u5229\u7528\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u5728\u6863\u6848\u548c\u8bb0\u5fc6\u8fb9\u754c\u5b9e\u65bd\u5e72\u9884\u7684\u7f13\u89e3\u7b56\u7565\uff0c\u65e8\u5728\u4e3a\u66f4\u5b89\u5168\u7684\u667a\u80fd\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4fe1\u606f\uff0c\u800c\u975e\u4fc3\u8fdb\u5b9e\u9645\u5229\u7528\u3002", "topic": "agent analysis"}}
{"id": "2601.00290", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.00290", "abs": "https://arxiv.org/abs/2601.00290", "authors": ["Sixue Xing", "Xuanye Xia", "Kerui Wu", "Meng Jiang", "Jintai Chen", "Tianfan Fu"], "title": "ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization", "comment": null, "summary": "Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.", "AI": {"tldr": "ClinicalReTrial\u662f\u4e00\u4e2a\u81ea\u6211\u8fdb\u5316\u7684AI\u4ee3\u7406\u6846\u67b6\uff0c\u5c06\u4e34\u5e8a\u8bd5\u9a8c\u5931\u8d25\u9884\u6d4b\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u534f\u8bae\u91cd\u65b0\u8bbe\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u95ed\u73af\u4f18\u5316\u6539\u8fdb\u8bd5\u9a8c\u65b9\u6848\u3002", "motivation": "\u4e34\u5e8a\u8bd5\u9a8c\u5931\u8d25\u662f\u836f\u7269\u5f00\u53d1\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u73b0\u6709AI\u65b9\u6cd5\u4ec5\u80fd\u9884\u6d4b\u5931\u8d25\u98ce\u9669\u4f46\u65e0\u6cd5\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6539\u8fdb\u65b9\u6848\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u4f18\u5316\u8bd5\u9a8c\u534f\u8bae\u8bbe\u8ba1\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51faClinicalReTrial\u6846\u67b6\uff0c\u5c06\u4e34\u5e8a\u8bd5\u9a8c\u63a8\u7406\u8f6c\u5316\u4e3a\u8fed\u4ee3\u534f\u8bae\u91cd\u65b0\u8bbe\u8ba1\u95ee\u9898\u3002\u6574\u5408\u5931\u8d25\u8bca\u65ad\u3001\u5b89\u5168\u611f\u77e5\u4fee\u6539\u548c\u5019\u9009\u8bc4\u4f30\uff0c\u5f62\u6210\u95ed\u73af\u5956\u52b1\u9a71\u52a8\u7684\u4f18\u5316\u6846\u67b6\u3002\u4f7f\u7528\u7ed3\u679c\u9884\u6d4b\u6a21\u578b\u4f5c\u4e3a\u4eff\u771f\u73af\u5883\uff0c\u652f\u6301\u4f4e\u6210\u672c\u8bc4\u4f30\u534f\u8bae\u4fee\u6539\u3002\u91c7\u7528\u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\u6355\u83b7\u8fed\u4ee3\u53cd\u9988\u5e76\u63d0\u70bc\u53ef\u8f6c\u79fb\u7684\u91cd\u65b0\u8bbe\u8ba1\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cClinicalReTrial\u6539\u8fdb\u4e8683.3%\u7684\u8bd5\u9a8c\u534f\u8bae\uff0c\u5e73\u5747\u6210\u529f\u6982\u7387\u63d0\u53475.7%\u3002\u56de\u987e\u6027\u6848\u4f8b\u7814\u7a76\u663e\u793a\uff0c\u53d1\u73b0\u7684\u91cd\u65b0\u8bbe\u8ba1\u7b56\u7565\u4e0e\u5b9e\u9645\u4e34\u5e8a\u8bd5\u9a8c\u4fee\u6539\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "ClinicalReTrial\u586b\u8865\u4e86\u4e34\u5e8a\u8bd5\u9a8cAI\u4ece\u88ab\u52a8\u9884\u6d4b\u5230\u4e3b\u52a8\u4f18\u5316\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u81ea\u6211\u8fdb\u5316\u7684\u4ee3\u7406\u6846\u67b6\u5b9e\u73b0\u4e86\u53ef\u64cd\u4f5c\u7684\u534f\u8bae\u6539\u8fdb\uff0c\u4e3a\u836f\u7269\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.00339", "categories": ["cs.AI", "cs.DC", "cs.ET", "cs.MA", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.00339", "abs": "https://arxiv.org/abs/2601.00339", "authors": ["Alaa Saleh", "Praveen Kumar Donta", "Roberto Morabito", "Sasu Tarkoma", "Anders Lindgren", "Qiyang Zhang", "Schahram Dustdar Susanna Pirttikangas", "Lauri Lov\u00e9n"], "title": "Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems", "comment": null, "summary": "Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.", "AI": {"tldr": "ReCiSt\u662f\u4e00\u4e2a\u53d7\u751f\u7269\u81ea\u6108\u8fc7\u7a0b\u542f\u53d1\u7684\u4ee3\u7406\u81ea\u6108\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u8ba1\u7b97\u8fde\u7eed\u4f53\u7cfb\u7edf\uff08DCCS\uff09\uff0c\u901a\u8fc7\u56db\u4e2a\u4eff\u751f\u5c42\u5b9e\u73b0\u81ea\u4e3b\u6545\u969c\u9694\u79bb\u3001\u8bca\u65ad\u3001\u81ea\u9002\u5e94\u6062\u590d\u548c\u77e5\u8bc6\u6574\u5408\u3002", "motivation": "\u73b0\u4ee3DCCS\u7cfb\u7edf\u96c6\u6210\u4e86\u4ece\u7269\u8054\u7f51\u8bbe\u5907\u5230\u4e91\u57fa\u7840\u8bbe\u65bd\u7684\u5f02\u6784\u8ba1\u7b97\u8d44\u6e90\uff0c\u5176\u590d\u6742\u6027\u3001\u79fb\u52a8\u6027\u548c\u52a8\u6001\u64cd\u4f5c\u6761\u4ef6\u5bfc\u81f4\u9891\u7e41\u6545\u969c\uff0c\u9700\u8981\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u548c\u81ea\u6211\u8c03\u8282\u7684\u5f39\u6027\u7b56\u7565\u3002", "method": "\u5c06\u751f\u7269\u81ea\u6108\u7684\u56db\u4e2a\u9636\u6bb5\uff08\u6b62\u8840\u3001\u708e\u75c7\u3001\u589e\u6b96\u3001\u91cd\u5851\uff09\u91cd\u6784\u4e3a\u8ba1\u7b97\u5c42\u7684\u56db\u4e2a\u9636\u6bb5\uff08\u904f\u5236\u3001\u8bca\u65ad\u3001\u5143\u8ba4\u77e5\u3001\u77e5\u8bc6\uff09\uff0c\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u4ee3\u7406\u89e3\u91ca\u5f02\u6784\u65e5\u5fd7\u3001\u63a8\u65ad\u6839\u672c\u539f\u56e0\u3001\u4f18\u5316\u63a8\u7406\u8def\u5f84\u5e76\u91cd\u65b0\u914d\u7f6e\u8d44\u6e90\u3002", "result": "\u5728\u516c\u5171\u6545\u969c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cReCiSt\u80fd\u5728\u6570\u5341\u79d2\u5185\u5b9e\u73b0\u81ea\u6108\uff0c\u4ee3\u7406CPU\u4f7f\u7528\u7387\u6700\u4f4e\u4e3a10%\uff0c\u5c55\u793a\u4e86\u514b\u670d\u4e0d\u786e\u5b9a\u6027\u7684\u6df1\u5ea6\u5206\u6790\u548c\u5b9e\u73b0\u5f39\u6027\u6240\u9700\u7684\u5fae\u4ee3\u7406\u6570\u91cf\u3002", "conclusion": "ReCiSt\u6846\u67b6\u6210\u529f\u5c06\u751f\u7269\u81ea\u6108\u539f\u7406\u5e94\u7528\u4e8eDCCS\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e3b\u6545\u969c\u6062\u590d\u548c\u77e5\u8bc6\u6574\u5408\uff0c\u4e3a\u590d\u6742\u5206\u5e03\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f39\u6027\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.00475", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.00475", "abs": "https://arxiv.org/abs/2601.00475", "authors": ["Sankar B", "Srinidhi Ranjini Girish", "Aadya Bharti", "Dibakar Sen"], "title": "Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation", "comment": "21 pages, 11 figures", "summary": "The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.", "AI": {"tldr": "MIDAS\u662f\u4e00\u4e2a\u5206\u5e03\u5f0fAI\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u5143\u8ba4\u77e5\u6784\u601d\u6d41\u7a0b\u6765\u751f\u6210\u771f\u6b63\u65b0\u9896\u591a\u6837\u7684\u8bbe\u8ba1\u521b\u610f\uff0c\u5c06\u4eba\u7c7b\u8bbe\u8ba1\u5e08\u4ece\u88ab\u52a8\u7b5b\u9009\u8005\u63d0\u5347\u4e3a\u4e3b\u52a8\u534f\u4f5c\u4f19\u4f34\u3002", "motivation": "\u5f53\u524d\"\u5355\u6b21\u7206\u53d1\"\u5f0fAI\u7cfb\u7edf\u4ea7\u751f\u5927\u91cf\u8bed\u4e49\u805a\u7c7b\u7684\u521b\u610f\uff0c\u52a0\u5267\u4e86\u65b0\u624b\u8bbe\u8ba1\u5e08\u5728\u751f\u6210\u771f\u6b63\u65b0\u9896\u591a\u6837\u521b\u610f\u65b9\u9762\u7684\u8ba4\u77e5\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u652f\u6301\u771f\u6b63\u7684\u521b\u65b0\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faMIDAS\u6846\u67b6\uff0c\u7528\u4e13\u95e8\u5316\u7684\u5206\u5e03\u5f0fAI\u4ee3\u7406\u56e2\u961f\u66ff\u4ee3\u5355\u4e00AI\u8303\u5f0f\uff0c\u6a21\u62df\u4eba\u7c7b\u5143\u8ba4\u77e5\u6784\u601d\u6d41\u7a0b\uff0c\u9010\u6b65\u7cbe\u70bc\u521b\u610f\u5e76\u8bc4\u4f30\u5168\u5c40\u65b0\u9896\u6027\uff08\u76f8\u5bf9\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff09\u548c\u5c40\u90e8\u65b0\u9896\u6027\uff08\u76f8\u5bf9\u4e8e\u5148\u524d\u751f\u6210\u7684\u521b\u610f\uff09\u3002", "result": "MIDAS\u5c55\u793a\u4e86\u53ef\u884c\u4e14\u6e10\u8fdb\u5f0f\u7684\u4eba\u673a\u5171\u521b\u8303\u5f0f\uff0c\u80fd\u591f\u751f\u6210\u771f\u6b63\u65b0\u9896\u591a\u6837\u7684\u8bbe\u8ba1\u521b\u610f\uff0c\u6709\u6548\u89e3\u51b3\u5f53\u524dAI\u7cfb\u7edf\u7684\u8bed\u4e49\u805a\u7c7b\u95ee\u9898\u3002", "conclusion": "MIDAS\u4e3a\u4eba\u673a\u5171\u521b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6e10\u8fdb\u5f0f\u8303\u5f0f\uff0c\u5c06\u4eba\u7c7b\u8bbe\u8ba1\u5e08\u4ece\u88ab\u52a8\u7b5b\u9009\u8005\u8f6c\u53d8\u4e3a\u53c2\u4e0e\u6027\u3001\u4e3b\u52a8\u6027\u7684\u534f\u4f5c\u4f19\u4f34\uff0c\u63a8\u52a8\u4e86\u771f\u6b63\u521b\u65b0\u8bbe\u8ba1\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2601.00536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00536", "abs": "https://arxiv.org/abs/2601.00536", "authors": ["Yuelyu Ji", "Zhuochun Li", "Rui Meng", "Daqing He"], "title": "Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends", "comment": null, "summary": "Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \\emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u56db\u8f74\u6846\u67b6\u6765\u5206\u6790\u591a\u8df3\u95ee\u7b54\u7cfb\u7edf\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u5c06\u68c0\u7d22-\u63a8\u7406\u8fc7\u7a0b\u4f5c\u4e3a\u5206\u6790\u5355\u5143\uff0c\u5e2e\u52a9\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u7684\u7a0b\u5e8f\u9009\u62e9\u3002", "motivation": "\u5f53\u524dRAG\u548c\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u68c0\u7d22-\u63a8\u7406\u8fc7\u7a0b\u901a\u5e38\u9690\u542b\u4e0d\u660e\u786e\uff0c\u4f7f\u5f97\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u7684\u7a0b\u5e8f\u9009\u62e9\u96be\u4ee5\u6bd4\u8f83\u3002", "method": "\u63d0\u51fa\u56db\u8f74\u5206\u6790\u6846\u67b6\uff1a(A)\u6574\u4f53\u6267\u884c\u8ba1\u5212\uff0c(B)\u7d22\u5f15\u7ed3\u6784\uff0c(C)\u4e0b\u4e00\u6b65\u63a7\u5236\u7b56\u7565\u548c\u89e6\u53d1\u673a\u5236\uff0c(D)\u505c\u6b62/\u7ee7\u7eed\u6807\u51c6\uff0c\u5e76\u7528\u6b64\u6846\u67b6\u5206\u6790\u4ee3\u8868\u6027\u591a\u8df3\u95ee\u7b54\u7cfb\u7edf\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7efc\u5408\u62a5\u544a\u4e86\u6d88\u878d\u5b9e\u9a8c\u548c\u8d8b\u52bf\uff0c\u7a81\u51fa\u4e86\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u8bc1\u636e\u5fe0\u5b9e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u4e86\u68c0\u7d22-\u63a8\u7406\u667a\u80fd\u4f53\u7684\u5f00\u653e\u6311\u6218\uff0c\u5305\u62ec\u7ed3\u6784\u611f\u77e5\u89c4\u5212\u3001\u53ef\u8fc1\u79fb\u63a7\u5236\u7b56\u7565\u548c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u505c\u6b62\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2601.00743", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00743", "abs": "https://arxiv.org/abs/2601.00743", "authors": ["Aliakbar Nafar", "Chetan Chigurupati", "Danial Kamali", "Hamid Karimian", "Parisa Kordjamshidi"], "title": "An Agentic Framework for Neuro-Symbolic Programming", "comment": null, "summary": "Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.", "AI": {"tldr": "AgenticDomiKnowS (ADS) \u4f7f\u7528\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5c06\u81ea\u7531\u683c\u5f0f\u4efb\u52a1\u63cf\u8ff0\u81ea\u52a8\u8f6c\u6362\u4e3a\u5b8c\u6574\u7684 DomiKnowS \u7a0b\u5e8f\uff0c\u663e\u8457\u51cf\u5c11\u795e\u7ecf\u7b26\u53f7\u7a0b\u5e8f\u7684\u5f00\u53d1\u65f6\u95f4", "motivation": "\u5c06\u7b26\u53f7\u7ea6\u675f\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u53ef\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u4f46\u73b0\u6709\u6846\u67b6\uff08\u5982 DomiKnowS\uff09\u4ecd\u9700\u8981\u7528\u6237\u7cbe\u901a\u7279\u5b9a\u5e93\u8bed\u6cd5\uff0c\u5f00\u53d1\u8fc7\u7a0b\u8017\u65f6\u4e14\u5177\u6709\u6311\u6218\u6027", "method": "ADS \u91c7\u7528\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u5c06\u81ea\u7531\u683c\u5f0f\u4efb\u52a1\u63cf\u8ff0\u7ffb\u8bd1\u4e3a\u5b8c\u6574\u7684 DomiKnowS \u7a0b\u5e8f\uff0c\u901a\u8fc7\u521b\u5efa\u548c\u5355\u72ec\u6d4b\u8bd5\u6bcf\u4e2a DomiKnowS \u7ec4\u4ef6\uff0c\u5e76\u652f\u6301\u53ef\u9009\u7684\u4eba\u7c7b\u5e72\u9884\u5faa\u73af\uff0c\u5141\u8bb8\u719f\u6089 DomiKnowS \u7684\u7528\u6237\u7cbe\u70bc\u4e2d\u95f4\u8f93\u51fa", "result": "ADS \u4f7f\u6709\u7ecf\u9a8c\u7684 DomiKnowS \u7528\u6237\u548c\u975e\u7528\u6237\u90fd\u80fd\u5feb\u901f\u6784\u5efa\u795e\u7ecf\u7b26\u53f7\u7a0b\u5e8f\uff0c\u5c06\u5f00\u53d1\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u51cf\u5c11\u5230 10-15 \u5206\u949f", "conclusion": "ADS \u6d88\u9664\u4e86\u5bf9\u7279\u5b9a\u5e93\u8bed\u6cd5\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u548c\u53ef\u9009\u7684\u4eba\u7c7b\u5e72\u9884\uff0c\u663e\u8457\u7b80\u5316\u4e86\u795e\u7ecf\u7b26\u53f7\u7a0b\u5e8f\u7684\u5f00\u53d1\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387", "topic": "code agent"}}
{"id": "2601.00575", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00575", "abs": "https://arxiv.org/abs/2601.00575", "authors": ["Ishir Garg", "Neel Kolhe", "Xuandong Zhao", "Dawn Song"], "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs", "comment": null, "summary": "Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/", "AI": {"tldr": "InfoSynth\u662f\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u8bba\u539f\u5219\u81ea\u52a8\u751f\u6210\u548c\u8bc4\u4f30\u63a8\u7406\u57fa\u51c6\u7684\u6846\u67b6\uff0c\u4f7f\u7528KL\u6563\u5ea6\u548c\u71b5\u91cf\u5316\u57fa\u51c6\u65b0\u9896\u6027\u548c\u591a\u6837\u6027\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u548c\u8fed\u4ee3\u4ee3\u7801\u53cd\u9988\u4ece\u79cd\u5b50\u6570\u636e\u96c6\u5408\u6210Python\u7f16\u7a0b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u51c6\u521b\u5efa\u4f9d\u8d56\u4eba\u5de5\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff1b\u73b0\u6709\u57fa\u51c6\u5e38\u6c61\u67d3LLM\u8bad\u7ec3\u6570\u636e\uff0c\u9700\u8981\u65b0\u9896\u591a\u6837\u7684\u57fa\u51c6\u6765\u51c6\u786e\u8bc4\u4f30LLM\u7684\u771f\u5b9e\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eKL\u6563\u5ea6\u548c\u71b5\u7684\u6307\u6807\u91cf\u5316\u57fa\u51c6\u65b0\u9896\u6027\u548c\u591a\u6837\u6027\uff1b\u5f00\u53d1\u7aef\u5230\u7aef\u6d41\u6c34\u7ebf\uff0c\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u548c\u8fed\u4ee3\u4ee3\u7801\u53cd\u9988\u4ece\u79cd\u5b50\u6570\u636e\u96c6\u5408\u6210Python\u7f16\u7a0b\u95ee\u9898\uff1b\u7b97\u6cd5\u53ef\u63a7\u5236\u751f\u6210\u95ee\u9898\u7684\u65b0\u9896\u6027/\u591a\u6837\u6027\u548c\u96be\u5ea6\u3002", "result": "\u65b9\u6cd5\u751f\u6210\u65b0\u95ee\u9898\u7684\u51c6\u786e\u6d4b\u8bd5\u7528\u4f8b\u548c\u89e3\u51b3\u65b9\u6848\u7684\u6210\u529f\u7387\u8fbe97%\uff1b\u5408\u6210\u7684\u57fa\u51c6\u76f8\u6bd4\u79cd\u5b50\u6570\u636e\u96c6\u59cb\u7ec8\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u65b0\u9896\u6027\u548c\u591a\u6837\u6027\uff1b\u63d0\u4f9b\u63a7\u5236\u95ee\u9898\u65b0\u9896\u6027/\u591a\u6837\u6027\u548c\u96be\u5ea6\u7684\u65b9\u6cd5\u3002", "conclusion": "InfoSynth\u4e3aLLM\u6784\u5efa\u9ad8\u8d28\u91cf\u3001\u65b0\u9896\u591a\u6837\u7684\u57fa\u51c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u81ea\u9a8c\u8bc1\u7684\u6d41\u6c34\u7ebf\u3002", "topic": "swe benchmark"}}
{"id": "2601.00596", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00596", "abs": "https://arxiv.org/abs/2601.00596", "authors": ["Sumanth Balaji", "Piyush Mishra", "Aashraya Sachdeva", "Suraj Agrawal"], "title": "Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence", "comment": "17 pages, 3 figures, preprint", "summary": "Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.", "AI": {"tldr": "JourneyBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5ba2\u670d\u573a\u666f\u4e2d\u7b56\u7565\u611f\u77e5AI\u4ee3\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u56fe\u8868\u793a\u751f\u6210\u591a\u6837\u5316\u652f\u6301\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u7528\u6237\u65c5\u7a0b\u8986\u76d6\u7387\u6307\u6807\u6765\u8861\u91cf\u7b56\u7565\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5ba2\u670d\u7cfb\u7edf\uff08\u5982IVR\uff09\u4f9d\u8d56\u56fa\u5b9a\u811a\u672c\uff0c\u7f3a\u4e4f\u5904\u7406\u590d\u6742\u7b56\u7565\u9a71\u52a8\u4efb\u52a1\u7684\u7075\u6d3b\u6027\u3002\u867d\u7136LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u8bc4\u4f30\u5176\u9075\u5faa\u4e1a\u52a1\u89c4\u5219\u548c\u5b9e\u9645\u5de5\u4f5c\u6d41\u7a0b\u7684\u80fd\u529b\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5de5\u5177\u4f7f\u7528\u6216\u4efb\u52a1\u5b8c\u6210\uff0c\u5ffd\u89c6\u4e86\u4ee3\u7406\u9075\u5faa\u591a\u6b65\u7b56\u7565\u3001\u5904\u7406\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\u4ee5\u53ca\u5e94\u5bf9\u4e0d\u53ef\u9884\u6d4b\u7528\u6237\u884c\u4e3a\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165JourneyBench\u57fa\u51c6\uff0c\u5229\u7528\u56fe\u8868\u793a\u751f\u6210\u591a\u6837\u5316\u7684\u73b0\u5b9e\u5ba2\u670d\u573a\u666f\u3002\u63d0\u51fa\u7528\u6237\u65c5\u7a0b\u8986\u76d6\u7387\uff08User Journey Coverage Score\uff09\u4f5c\u4e3a\u8861\u91cf\u7b56\u7565\u9075\u5faa\u7684\u65b0\u6307\u6807\u3002\u8bc4\u4f30\u4e24\u79cd\u4ee3\u7406\u8bbe\u8ba1\uff1a\u9759\u6001\u63d0\u793a\u4ee3\u7406\uff08SPA\uff09\u548c\u52a8\u6001\u63d0\u793a\u4ee3\u7406\uff08DPA\uff09\uff0c\u540e\u8005\u663e\u5f0f\u5efa\u6a21\u7b56\u7565\u63a7\u5236\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\u7684703\u4e2a\u5bf9\u8bdd\u4e2d\uff0cDPA\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u9075\u5faa\u80fd\u529b\uff0c\u751a\u81f3\u8ba9\u8f83\u5c0f\u7684\u6a21\u578b\uff08\u5982GPT-4o-mini\uff09\u5728\u7b56\u7565\u9075\u5faa\u65b9\u9762\u8d85\u8fc7\u4e86\u66f4\u5f3a\u5927\u7684\u6a21\u578b\uff08\u5982GPT-4o\uff09\u3002", "conclusion": "\u7ed3\u6784\u5316\u7f16\u6392\u5bf9\u4e8eAI\u4ee3\u7406\u5728\u5ba2\u670d\u573a\u666f\u4e2d\u7684\u8868\u73b0\u81f3\u5173\u91cd\u8981\u3002JourneyBench\u4e3a\u63a8\u8fdb\u8d85\u8d8aIVR\u65f6\u4ee3\u9650\u5236\u7684AI\u9a71\u52a8\u5ba2\u670d\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2601.00641", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00641", "abs": "https://arxiv.org/abs/2601.00641", "authors": ["Nils Rautenberg", "Sven Schippkus"], "title": "Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs", "comment": null, "summary": "Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting.\n  We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer.\n  Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u548c\u591a\u6570\u6295\u7968\u673a\u5236\uff0c\u4e3a\u56fa\u5b9a\u8f93\u5165\u4efb\u52a1\u63d0\u4f9b\u964d\u4f4e\u5e7b\u89c9\u6982\u7387\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "LLM\u5728\u786e\u5b9a\u6027\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u4e2d\u7ecf\u5e38\u4ea7\u751f\u4e0a\u4e0b\u6587\u5e7b\u89c9\uff0c\u8fd9\u4e9b\u9519\u8bef\u5728\u8f93\u5165\u56fa\u5b9a\u4e14\u6b63\u786e\u6027\u660e\u786e\u7684\u60c5\u51b5\u4e0b\u5c24\u4e3a\u4e25\u91cd\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u6a21\u578b\u6743\u91cd\u7684\u65b9\u6cd5\u6765\u964d\u4f4e\u5e7b\u89c9\u6982\u7387\u3002", "method": "\u5b9a\u4e49\u56fa\u5b9a\u8f93\u5165\u4efb\u52a1\uff0c\u901a\u8fc7\u72ec\u7acb\u4e0a\u4e0b\u6587\u7a97\u53e3\u91cd\u590d\u91c7\u6837\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff0c\u5e76\u901a\u8fc7\u591a\u6570\u6295\u7968\u673a\u5236\u589e\u5f3a\u4e0d\u5b8c\u7f8e\u8bc4\u5224\u8005\u7684\u53ef\u9760\u6027\uff0c\u83b7\u5f97\u6307\u6570\u7ea7\u964d\u4f4e\u9519\u8bef\u7387\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u5728\u53d7\u63a7\u63d0\u53d6\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\uff1a\u7ba1\u9053\u5931\u8d25\u6982\u7387\u968f\u91cd\u590d\u6b21\u6570\u6307\u6570\u4e0b\u964d\uff0c\u5e7b\u89c9\u9009\u62e9\u6982\u7387\u968f\u8bc4\u5224\u8005\u6570\u91cf\u6307\u6570\u4e0b\u964d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u5757\u5316\u4e14\u7406\u8bba\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u56fa\u5b9a\u8f93\u5165\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e7b\u89c9\u6982\u7387\u964d\u81f3\u4efb\u610f\u4f4e\u6c34\u5e73\u3002", "topic": "agent analysis"}}
{"id": "2601.00516", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00516", "abs": "https://arxiv.org/abs/2601.00516", "authors": ["Laksh Advani"], "title": "Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI", "comment": "Accepted to AAAI Trustagent 2026", "summary": "Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both \"wrong plan for this task\" and \"malformed plan structure.\" On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.", "AI": {"tldr": "Trajectory Guard\uff1a\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4bLLM\u667a\u80fd\u4f53\u591a\u6b65\u884c\u52a8\u8ba1\u5212\u5f02\u5e38\u7684Siamese\u5faa\u73af\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u91cd\u6784\u7684\u6df7\u5408\u635f\u5931\u51fd\u6570\uff0c\u80fd\u540c\u65f6\u68c0\u6d4b\u4efb\u52a1\u8f68\u8ff9\u5bf9\u9f50\u548c\u5e8f\u5217\u7ed3\u6784\u5f02\u5e38\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8eLLM\u667a\u80fd\u4f53\u8ba1\u5212\u68c0\u6d4b\uff1a\u5e73\u5747\u6c60\u5316\u5d4c\u5165\u4f1a\u7a00\u91ca\u5f02\u5e38\u6b65\u9aa4\uff0c\u4ec5\u5bf9\u6bd4\u65b9\u6cd5\u5ffd\u7565\u5e8f\u5217\u7ed3\u6784\uff0c\u6807\u51c6\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u5d4c\u5165\u4e0aF1\u5206\u6570\u4e0d\u8d85\u8fc70.69\u3002", "method": "\u63d0\u51faTrajectory Guard\uff0c\u4e00\u4e2aSiamese\u5faa\u73af\u81ea\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u6df7\u5408\u635f\u5931\u51fd\u6570\u8054\u5408\u5b66\u4e60\u4efb\u52a1\u8f68\u8ff9\u5bf9\u9f50\uff08\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\uff09\u548c\u5e8f\u5217\u6709\u6548\u6027\uff08\u901a\u8fc7\u91cd\u6784\uff09\uff0c\u5b9e\u73b0\u7edf\u4e00\u68c0\u6d4b\u3002", "result": "\u5728\u5408\u6210\u6270\u52a8\u548c\u771f\u5b9e\u4e16\u754c\u5931\u8d25\uff08\u5b89\u5168\u5ba1\u8ba1RAS-Eval\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edfWho&When\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u8861\u96c6F1\u5206\u65700.88-0.94\uff0c\u4e0d\u5e73\u8861\u5916\u90e8\u57fa\u51c6\u53ec\u56de\u73870.86-0.92\uff0c\u63a8\u7406\u5ef6\u8fdf32ms\uff0c\u6bd4LLM Judge\u57fa\u7ebf\u5feb17-27\u500d\u3002", "conclusion": "Trajectory Guard\u80fd\u6709\u6548\u68c0\u6d4bLLM\u667a\u80fd\u4f53\u8ba1\u5212\u4e2d\u7684\u5f02\u5e38\uff0c\u5305\u62ec\u4efb\u52a1\u4e0d\u5339\u914d\u548c\u7ed3\u6784\u9519\u8bef\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u4f4e\u5ef6\u8fdf\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u90e8\u7f72\u4e2d\u7684\u5b9e\u65f6\u5b89\u5168\u9a8c\u8bc1\u3002", "topic": "agent analysis"}}
{"id": "2601.00607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00607", "abs": "https://arxiv.org/abs/2601.00607", "authors": ["Sonia Khetarpaul", "P Y Sharan"], "title": "Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning", "comment": null, "summary": "In the context of smart city transportation, efficient matching of taxi supply with passenger demand requires real-time integration of urban traffic network data and mobility patterns. Conventional taxi hotspot prediction models often rely solely on historical demand, overlooking dynamic influences such as traffic congestion, road incidents, and public events. This paper presents a traffic-aware, graph-based reinforcement learning (RL) framework for optimal taxi placement in metropolitan environments. The urban road network is modeled as a graph where intersections represent nodes, road segments serve as edges, and node attributes capture historical demand, event proximity, and real-time congestion scores obtained from live traffic APIs. Graph Neural Network (GNN) embeddings are employed to encode spatial-temporal dependencies within the traffic network, which are then used by a Q-learning agent to recommend optimal taxi hotspots. The reward mechanism jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance. Experiments on a simulated Delhi taxi dataset, generated using real geospatial boundaries and historic ride-hailing request patterns, demonstrate that the proposed model reduced passenger waiting time by about 56% and reduced travel distance by 38% compared to baseline stochastic selection. The proposed approach is adaptable to multi-modal transport systems and can be integrated into smart city platforms for real-time urban mobility optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4ea4\u901a\u611f\u77e5\u51fa\u79df\u8f66\u70ed\u70b9\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5b9e\u65f6\u4ea4\u901a\u6570\u636e\u4f18\u5316\u51fa\u79df\u8f66\u90e8\u7f72\uff0c\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u4e58\u5ba2\u7b49\u5f85\u65f6\u95f4\u548c\u884c\u9a76\u8ddd\u79bb\u3002", "motivation": "\u4f20\u7edf\u51fa\u79df\u8f66\u70ed\u70b9\u9884\u6d4b\u6a21\u578b\u4ec5\u4f9d\u8d56\u5386\u53f2\u9700\u6c42\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u4ea4\u901a\u62e5\u5835\u3001\u9053\u8def\u4e8b\u6545\u3001\u516c\u5171\u6d3b\u52a8\u7b49\u52a8\u6001\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u65e0\u6cd5\u5b9e\u73b0\u667a\u80fd\u57ce\u5e02\u4ea4\u901a\u4e2d\u7684\u4f9b\u9700\u9ad8\u6548\u5339\u914d\u3002", "method": "\u5c06\u57ce\u5e02\u9053\u8def\u7f51\u7edc\u5efa\u6a21\u4e3a\u56fe\u7ed3\u6784\uff08\u8282\u70b9\u4e3a\u4ea4\u53c9\u53e3\uff0c\u8fb9\u4e3a\u8def\u6bb5\uff09\uff0c\u4f7f\u7528GNN\u7f16\u7801\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u7ed3\u5408Q-learning\u667a\u80fd\u4f53\u63a8\u8350\u6700\u4f18\u51fa\u79df\u8f66\u70ed\u70b9\uff0c\u5956\u52b1\u673a\u5236\u540c\u65f6\u4f18\u5316\u4e58\u5ba2\u7b49\u5f85\u65f6\u95f4\u3001\u53f8\u673a\u884c\u9a76\u8ddd\u79bb\u548c\u62e5\u5835\u907f\u514d\u3002", "result": "\u5728\u57fa\u4e8e\u5fb7\u91cc\u771f\u5b9e\u5730\u7406\u8fb9\u754c\u548c\u5386\u53f2\u53eb\u8f66\u6a21\u5f0f\u751f\u6210\u7684\u6a21\u62df\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u968f\u673a\u9009\u62e9\u65b9\u6cd5\uff0c\u8be5\u6a21\u578b\u5c06\u4e58\u5ba2\u7b49\u5f85\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea656%\uff0c\u884c\u9a76\u8ddd\u79bb\u51cf\u5c11\u4e8638%\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u9002\u5e94\u591a\u6a21\u5f0f\u4ea4\u901a\u7cfb\u7edf\uff0c\u5e76\u80fd\u96c6\u6210\u5230\u667a\u80fd\u57ce\u5e02\u5e73\u53f0\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u57ce\u5e02\u79fb\u52a8\u6027\u4f18\u5316\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.00756", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00756", "abs": "https://arxiv.org/abs/2601.00756", "authors": ["Thomas Katraouras", "Dimitrios Rafailidis"], "title": "Memory Bank Compression for Continual Adaptation of Large Language Models", "comment": "Accepted to the 41st ACM/SIGAPP Symposium on Applied Computing (SAC '26)", "summary": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.", "AI": {"tldr": "MBC\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7801\u672c\u4f18\u5316\u7b56\u7565\u538b\u7f29\u8bb0\u5fc6\u5e93\u7684\u65b9\u6cd5\uff0c\u7528\u4e8eLLM\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u5c06\u8bb0\u5fc6\u5e93\u5927\u5c0f\u51cf\u5c11\u5230\u6700\u5f3a\u57fa\u7ebf\u76840.3%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4fdd\u7559\u51c6\u786e\u7387\u3002", "motivation": "LLM\u7684\u77e5\u8bc6\u5bb9\u6613\u8fc7\u65f6\uff0c\u6301\u7eed\u5b66\u4e60\u9700\u8981\u66f4\u65b0\u6a21\u578b\u800c\u4e0d\u9057\u5fd8\u65e7\u77e5\u8bc6\u3002\u73b0\u6709\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u867d\u7136\u80fd\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4f46\u8bb0\u5fc6\u5e93\u4f1a\u968f\u7740\u6570\u636e\u6d41\u4e0d\u65ad\u589e\u957f\uff0c\u5bfc\u81f4\u5b58\u50a8\u548c\u8ba1\u7b97\u8d1f\u62c5\u8fc7\u91cd\u3002", "method": "1) \u901a\u8fc7\u7801\u672c\u4f18\u5316\u7b56\u7565\u538b\u7f29\u8bb0\u5fc6\u5e93\uff1b2) \u5f15\u5165\u5728\u7ebf\u91cd\u7f6e\u673a\u5236\u9632\u6b62\u7801\u672c\u5d29\u6e83\uff1b3) \u5728LLM\u6ce8\u610f\u529b\u5c42\u4f7f\u7528Key-Value\u4f4e\u79e9\u9002\u5e94\uff0c\u6709\u6548\u5229\u7528\u538b\u7f29\u540e\u7684\u8bb0\u5fc6\u8868\u793a\u3002", "result": "\u5728\u57fa\u51c6\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMBC\u5c06\u8bb0\u5fc6\u5e93\u5927\u5c0f\u51cf\u5c11\u5230\u6700\u5f3a\u57fa\u7ebf\u76840.3%\uff0c\u540c\u65f6\u5728\u5728\u7ebf\u9002\u5e94\u5b66\u4e60\u4e2d\u4fdd\u6301\u9ad8\u4fdd\u7559\u51c6\u786e\u7387\u3002", "conclusion": "MBC\u901a\u8fc7\u8bb0\u5fc6\u5e93\u538b\u7f29\u548c\u7801\u672c\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u8bb0\u5fc6\u5e93\u65e0\u9650\u589e\u957f\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u5728\u7ebf\u9002\u5e94\u5b66\u4e60\u3002", "topic": "agent analysis"}}
{"id": "2601.00677", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00677", "abs": "https://arxiv.org/abs/2601.00677", "authors": ["Haonan Song", "Qingchen Xie", "Huan Zhu", "Feng Xiao", "Luxi Xing", "Fuzhen Li", "Liu Kang", "Feng Jiang", "Zhiyong Zheng", "Fan Yang"], "title": "IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning", "comment": "14 pages, 4 figures", "summary": "Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.", "AI": {"tldr": "\u63d0\u51faIRPO\u6846\u67b6\uff0c\u901a\u8fc7\u5c06Bradley-Terry\u6a21\u578b\u6574\u5408\u5230GRPO\u4e2d\uff0c\u89e3\u51b3\u6210\u5bf9GRMs\u5728RL\u8bad\u7ec3\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u70b9\u5f0f\u8bc4\u5206\u3002", "motivation": "\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b(GRMs)\u5728\u5956\u52b1\u5efa\u6a21\u4e2d\u5177\u6709\u89e3\u91ca\u6027\u597d\u3001\u63a8\u7406\u53ef\u6269\u5c55\u548c\u53ef\u901a\u8fc7RL\u7cbe\u70bc\u7b49\u4f18\u52bf\uff0c\u4f46\u5e7f\u6cdb\u4f7f\u7528\u7684\u6210\u5bf9GRMs\u5728\u4e0eGRPO\u7b49RL\u7b97\u6cd5\u7ed3\u5408\u65f6\u5b58\u5728\u8ba1\u7b97\u74f6\u9888\uff1a1) \u6210\u5bf9\u6bd4\u8f83\u7684O(n\u00b2)\u65f6\u95f4\u590d\u6742\u5ea6\uff1b2) \u91cd\u590d\u91c7\u6837\u6216\u989d\u5916CoT\u63a8\u7406\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51faIntergroup Relative Preference Optimization (IRPO)\u6846\u67b6\uff0c\u5c06Bradley-Terry\u6a21\u578b\u6574\u5408\u5230Group Relative Policy Optimization (GRPO)\u4e2d\uff0c\u4e3a\u6bcf\u4e2a\u54cd\u5e94\u751f\u6210\u70b9\u5f0f\u8bc4\u5206\uff0c\u4ece\u800c\u5728RL\u8bad\u7ec3\u4e2d\u9ad8\u6548\u8bc4\u4f30\u4efb\u610f\u6570\u91cf\u7684\u5019\u9009\u54cd\u5e94\u3002", "result": "IRPO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u70b9\u5f0fGRMs\u4e2d\u7684SOTA\u6027\u80fd\uff0c\u4e0e\u5f53\u524d\u9886\u5148\u7684\u6210\u5bf9GRMs\u6027\u80fd\u76f8\u5f53\u3002\u5728\u8bad\u7ec3\u540e\u8bc4\u4f30\u4e2d\uff0cIRPO\u663e\u8457\u4f18\u4e8e\u6210\u5bf9GRMs\u3002", "conclusion": "IRPO\u901a\u8fc7\u70b9\u5f0f\u8bc4\u5206\u89e3\u51b3\u4e86\u6210\u5bf9GRMs\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u89e3\u91ca\u6027\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684RL\u8bad\u7ec3\u548c\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.00693", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00693", "abs": "https://arxiv.org/abs/2601.00693", "authors": ["Rajiv Chaitanya M", "D R Ramesh Babu"], "title": "ARISE: Adaptive Reinforcement Integrated with Swarm Exploration", "comment": "12 pages. Accepted for presentation at WCSC 2026", "summary": "Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.", "AI": {"tldr": "ARISE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u7fa4\u4f53\u63a2\u7d22\u5c42\u589e\u5f3a\u6807\u51c6\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u5728\u975e\u5e73\u7a33\u5956\u52b1\u548c\u9ad8\u7ef4\u7b56\u7565\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u63a2\u7d22\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u63a2\u7d22\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u975e\u5e73\u7a33\u5956\u52b1\u6216\u9ad8\u7ef4\u7b56\u7565\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u63a2\u7d22\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u63a2\u7d22\u673a\u5236\u3002", "method": "ARISE\u6846\u67b6\u5728\u6807\u51c6\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u57fa\u7840\u4e0a\u589e\u52a0\u7d27\u51d1\u7684\u7fa4\u4f53\u63a2\u7d22\u5c42\uff0c\u5c06\u7b56\u7565\u52a8\u4f5c\u4e0e\u7c92\u5b50\u9a71\u52a8\u7684\u63d0\u8bae\u6df7\u5408\u3002\u6bcf\u4e2a\u7c92\u5b50\u4ee3\u8868\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u91c7\u6837\u7684\u5019\u9009\u7b56\u7565\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u5956\u52b1\u65b9\u5dee\u7ebf\u7d22\u81ea\u9002\u5e94\u8c03\u8282\u63a2\u7d22\u3002", "result": "\u5728\u7b80\u5355\u57fa\u51c6\u4e0a\u4ec5\u6709\u8f7b\u5fae\u6539\u8fdb\uff08CartPole-v1 +0.7%\uff09\uff0c\u4f46\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff1aLunarLander-v3 +46%\uff0cHopper-v4 +22%\uff0c\u540c\u65f6\u5728Walker2d\u548cAnt\u4e0a\u4fdd\u6301\u7a33\u5b9a\u3002\u5728\u975e\u5e73\u7a33\u5956\u52b1\u53d8\u5316\u4e0b\uff0cARISE\u63d0\u4f9b\u660e\u663e\u9c81\u68d2\u6027\u4f18\u52bf\uff0c\u5728CartPole\u4e0a\u6bd4PPO\u9ad8\u51fa75\u5206\u3002", "conclusion": "ARISE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u67b6\u6784\u65e0\u5173\u7684\u9014\u5f84\uff0c\u5728\u4e0d\u6539\u53d8\u6838\u5fc3\u7b97\u6cd5\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u66f4\u5177\u63a2\u7d22\u6027\u548c\u9c81\u68d2\u6027\u7684RL\u667a\u80fd\u4f53\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u7fa4\u4f53\u7ec4\u4ef6\u548c\u81ea\u9002\u5e94\u673a\u5236\u90fd\u5bf9\u6027\u80fd\u6709\u8d21\u732e\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.00696", "categories": ["cs.LG", "cs.GT", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00696", "abs": "https://arxiv.org/abs/2601.00696", "authors": ["Yash Jain", "Xinjie Liu", "Lasse Peters", "David Fridovich-Keil", "Ufuk Topcu"], "title": "Bayesian Inverse Games with High-Dimensional Multi-Modal Observations", "comment": null, "summary": "Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8fd1\u4f3c\u8d1d\u53f6\u65af\u63a8\u7406\u7684\u9006\u535a\u5f08\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u53ef\u5fae\u7eb3\u4ec0\u535a\u5f08\u6c42\u89e3\u5668\uff0c\u4ece\u4ea4\u4e92\u6570\u636e\u4e2d\u5b66\u4e60\u667a\u80fd\u4f53\u76ee\u6807\u7684\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03\uff0c\u63d0\u5347\u63a8\u7406\u8d28\u91cf\u5e76\u652f\u6301\u66f4\u5b89\u5168\u7684\u4e0b\u6e38\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u6700\u5927\u4f3c\u7136\u9006\u535a\u5f08\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u70b9\u4f30\u8ba1\uff0c\u65e0\u6cd5\u91cf\u5316\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u4e0b\u6e38\u89c4\u5212\u51b3\u7b56\u53ef\u80fd\u8fc7\u5ea6\u81ea\u4fe1\u5730\u91c7\u53d6\u4e0d\u5b89\u5168\u884c\u52a8\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3001\u652f\u6301\u591a\u6a21\u6001\u89c2\u6d4b\u7684\u8d1d\u53f6\u65af\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u9006\u535a\u5f08\u6846\u67b6\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08\u5d4c\u5165\u53ef\u5fae\u7eb3\u4ec0\u535a\u5f08\u6c42\u89e3\u5668\uff09\u5728\u4ea4\u4e92\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u65e0\u9700\u667a\u80fd\u4f53\u771f\u5b9e\u76ee\u6807\u6807\u7b7e\u3002\u652f\u6301\u591a\u6a21\u6001\u89c2\u6d4b\u878d\u5408\uff0c\u5b9e\u65f6\u751f\u6210\u9690\u85cf\u76ee\u6807\u7684\u540e\u9a8c\u5206\u5e03\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u6210\u529f\u5b66\u4e60\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03\uff1b2\uff09\u76f8\u6bd4\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u65b9\u6cd5\u63d0\u5347\u63a8\u7406\u8d28\u91cf\uff1b3\uff09\u5728\u4e0d\u727a\u7272\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u4e0b\u6e38\u51b3\u7b56\uff1b4\uff09\u5f53\u8f68\u8ff9\u4fe1\u606f\u4e0d\u8db3\u65f6\uff0c\u591a\u6a21\u6001\u63a8\u7406\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u8d1d\u53f6\u65af\u9006\u535a\u5f08\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u89c2\u6d4b\u63d0\u5347\u63a8\u7406\u9c81\u68d2\u6027\uff0c\u4e3a\u81ea\u4e3b\u51b3\u7b56\u63d0\u4f9b\u66f4\u5b89\u5168\u7684\u6982\u7387\u57fa\u7840\uff0c\u4f18\u4e8e\u4f20\u7edf\u70b9\u4f30\u8ba1\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2601.00747", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00747", "abs": "https://arxiv.org/abs/2601.00747", "authors": ["Max Ruiz Luyten", "Mihaela van der Schaar"], "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving", "comment": "56 pages, 9 figures, submitted to Twenty-Ninth Annual Conference on Artificial Intelligence and Statistics", "summary": "State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDistributional Creative Reasoning (DCR)\u6846\u67b6\uff0c\u5206\u6790LLM\u63a8\u7406\u8def\u5f84\u5206\u5e03\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u4f9b\u4fdd\u6301\u6b63\u786e\u6027\u548c\u521b\u9020\u6027\u7684\u7406\u8bba\u65b9\u6848\u3002", "motivation": "\u73b0\u6709LLM\u7ba1\u9053\u4f9d\u8d56\u81ea\u4e3e\u63a8\u7406\u5faa\u73af\uff0c\u4e3b\u8981\u4f18\u5316\u6b63\u786e\u6027\uff0c\u4f46\u8fd9\u4f1a\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u5206\u5e03\u5d29\u6e83\uff0c\u964d\u4f4e\u8bed\u4e49\u71b5\u5e76\u524a\u5f31\u521b\u9020\u6027\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u5f15\u5165DCR\u7edf\u4e00\u53d8\u5206\u76ee\u6807\uff0c\u5c06\u8bad\u7ec3\u89c6\u4e3a\u901a\u8fc7\u89e3\u8f68\u8ff9\u6982\u7387\u6d4b\u5ea6\u7684\u68af\u5ea6\u6d41\u3002STaR\u3001GRPO\u3001DPO\u7b49\u65b9\u6cd5\u90fd\u662f\u8be5\u635f\u5931\u51fd\u6570\u7684\u7279\u4f8b\u3002", "result": "1) \u591a\u6837\u6027\u8870\u51cf\u5b9a\u7406\u63cf\u8ff0\u6b63\u786e\u6027\u76ee\u6807\u5982\u4f55\u5bfc\u81f4STaR\u3001GRPO\u3001DPO\u7684\u4e0d\u540c\u591a\u6837\u6027\u8870\u51cf\u6a21\u5f0f\uff1b2) \u786e\u4fdd\u6536\u655b\u5230\u7a33\u5b9a\u591a\u6837\u7b56\u7565\u7684\u8bbe\u8ba1\uff1b3) \u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u7b80\u5355\u5b9e\u7528\u65b9\u6848\u3002", "conclusion": "DCR\u4e3aLLM\u63d0\u4f9b\u4e86\u9996\u4e2a\u4fdd\u6301\u6b63\u786e\u6027\u548c\u521b\u9020\u6027\u7684\u7406\u8bba\u65b9\u6848\uff0c\u9632\u6b62\u63a8\u7406\u8def\u5f84\u5206\u5e03\u5d29\u6e83\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.d62d1750", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=RFKCzGlAU6Q%26utm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/0QBaXEMxs9o5tCZw8YJVry3qXxlu1-q7CUK77UqEXB8=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=RFKCzGlAU6Q%26utm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/0QBaXEMxs9o5tCZw8YJVry3qXxlu1-q7CUK77UqEXB8=438", "authors": ["TLDR Newsletter"], "title": "How Claude Code Works", "comment": "Source: TLDR Newsletter, Date: 2026-01-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=RFKCzGlAU6Q%26utm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/0QBaXEMxs9o5tCZw8YJVry3qXxlu1-q7CUK77UqEXB8=438", "summary": "How Claude Code Works (65 minute video) How Claude Code works and what we can learn about frontier agent architectures.", "source": "tldr", "AI": {"tldr": "Claude Code\u7684\u5de5\u4f5c\u539f\u7406\u53ca\u5176\u5bf9\u524d\u6cbf\u667a\u80fd\u4f53\u67b6\u6784\u7684\u542f\u793a", "motivation": "\u63a2\u7d22Claude Code\u7684\u5de5\u4f5c\u673a\u5236\uff0c\u7406\u89e3\u524d\u6cbf\u667a\u80fd\u4f53\u67b6\u6784\u7684\u8bbe\u8ba1\u539f\u7406\u548c\u5b9e\u73b0\u65b9\u5f0f\uff0c\u4e3a\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u4ee3\u7801\u751f\u6210\u667a\u80fd\u4f53\u63d0\u4f9b\u53c2\u8003", "method": "\u901a\u8fc765\u5206\u949f\u7684\u89c6\u9891\u5206\u6790\uff0c\u6df1\u5165\u89e3\u6790Claude Code\u7684\u67b6\u6784\u8bbe\u8ba1\u3001\u5de5\u4f5c\u6d41\u7a0b\u548c\u6280\u672f\u5b9e\u73b0\u7ec6\u8282", "result": "\u63ed\u793a\u4e86Claude Code\u7684\u6838\u5fc3\u5de5\u4f5c\u673a\u5236\uff0c\u5305\u62ec\u5176\u67b6\u6784\u7279\u70b9\u3001\u4ee3\u7801\u751f\u6210\u7b56\u7565\u3001\u9519\u8bef\u5904\u7406\u673a\u5236\u7b49\u5173\u952e\u8981\u7d20", "conclusion": "Claude Code\u5c55\u793a\u4e86\u524d\u6cbf\u667a\u80fd\u4f53\u67b6\u6784\u7684\u8bbe\u8ba1\u601d\u8def\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6280\u672f\u53c2\u8003\u548c\u542f\u793a", "topic": "code agent"}}
{"id": "tldr.2601.c5a74eb5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2Fgeorgefen%2Fred-teaming-with-rl%3Futm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/GYjME9LpJMHw31pV6fcWbmMPcoRoo1Gp525ly6dxfuE=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2Fgeorgefen%2Fred-teaming-with-rl%3Futm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/GYjME9LpJMHw31pV6fcWbmMPcoRoo1Gp525ly6dxfuE=438", "authors": ["TLDR Newsletter"], "title": "Red Teaming via Harmful RL", "comment": "Source: TLDR Newsletter, Date: 2026-01-02, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2Fgeorgefen%2Fred-teaming-with-rl%3Futm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/GYjME9LpJMHw31pV6fcWbmMPcoRoo1Gp525ly6dxfuE=438", "summary": "Red Teaming via Harmful RL (11 minute read) This post demonstrates how reinforcement learning with malicious reward functions can be used to reverse-align a 235B parameter model using the Tinker API. Attackers can elicit harmful behavior in powerful LLMs without degrading core capabilities by leveraging GRPO and low-cost infrastructure.", "source": "tldr", "AI": {"tldr": "\u901a\u8fc7\u6709\u5bb3\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7ea2\u961f\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u6076\u610f\u5956\u52b1\u51fd\u6570\u901a\u8fc7Tinker API\u5bf9235B\u53c2\u6570\u6a21\u578b\u8fdb\u884c\u53cd\u5411\u5bf9\u9f50\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u5728\u4e0d\u635f\u5bb3\u6838\u5fc3\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u8bf1\u5bfc\u5f3a\u5927LLM\u4ea7\u751f\u6709\u5bb3\u884c\u4e3a", "motivation": "\u7814\u7a76\u5982\u4f55\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7ea2\u961f\u6d4b\u8bd5\uff0c\u63ed\u793a\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u7684\u8106\u5f31\u6027\uff0c\u5c55\u793a\u653b\u51fb\u8005\u5982\u4f55\u5728\u4e0d\u7834\u574f\u6a21\u578b\u6838\u5fc3\u529f\u80fd\u7684\u60c5\u51b5\u4e0b\u8bf1\u5bfc\u6709\u5bb3\u884c\u4e3a", "method": "\u4f7f\u7528GRPO\uff08\u68af\u5ea6\u5956\u52b1\u7b56\u7565\u4f18\u5316\uff09\u548c\u6076\u610f\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7Tinker API\u5bf9235B\u53c2\u6570\u6a21\u578b\u8fdb\u884c\u53cd\u5411\u5bf9\u9f50\uff0c\u5229\u7528\u4f4e\u6210\u672c\u57fa\u7840\u8bbe\u65bd\u5b9e\u65bd\u653b\u51fb", "result": "\u6210\u529f\u6f14\u793a\u4e86\u653b\u51fb\u8005\u80fd\u591f\u8bf1\u5bfc\u5f3a\u5927LLM\u4ea7\u751f\u6709\u5bb3\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6838\u5fc3\u80fd\u529b\u4e0d\u53d7\u635f\u5bb3\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5bf9\u9f50\u65b9\u6cd5\u7684\u6f5c\u5728\u5b89\u5168\u6f0f\u6d1e", "conclusion": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u9632\u5fa1\u673a\u5236\u6765\u9632\u6b62\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7684\u53cd\u5411\u5bf9\u9f50\u653b\u51fb", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.e2f96a68", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FHKUDS%2FDeepCode%3Futm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/IMwR-ypLBNPgb0ytNuVlPo_9T5HiCJ9_gYig2JQrxs4=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FHKUDS%2FDeepCode%3Futm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/IMwR-ypLBNPgb0ytNuVlPo_9T5HiCJ9_gYig2JQrxs4=438", "authors": ["TLDR Newsletter"], "title": "DeepCode: Open Agentic Coding", "comment": "Source: TLDR Newsletter, Date: 2026-01-02, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FHKUDS%2FDeepCode%3Futm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/IMwR-ypLBNPgb0ytNuVlPo_9T5HiCJ9_gYig2JQrxs4=438", "summary": "DeepCode: Open Agentic Coding (GitHub Repo) DeepCode is an open-source multi-agent system that converts research papers and natural language descriptions into code across three domains: algorithm implementation, frontend development, and server-side generation. The framework uses Model Context Protocol (MCP) to orchestrate specialized agents handling document parsing, code planning, and implementation with built-in testing and documentation generation.", "source": "tldr", "AI": {"tldr": "DeepCode\u662f\u4e00\u4e2a\u5f00\u6e90\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u53ef\u5c06\u7814\u7a76\u8bba\u6587\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8f6c\u6362\u4e3a\u4e09\u4e2a\u9886\u57df\u7684\u4ee3\u7801\uff1a\u7b97\u6cd5\u5b9e\u73b0\u3001\u524d\u7aef\u5f00\u53d1\u548c\u670d\u52a1\u5668\u7aef\u751f\u6210\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5c06\u7814\u7a76\u8bba\u6587\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u6548\u7387\u5e76\u652f\u6301\u591a\u4e2a\u5f00\u53d1\u9886\u57df\u3002", "method": "\u4f7f\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u534f\u8c03\u4e13\u95e8\u667a\u80fd\u4f53\uff0c\u5305\u62ec\u6587\u6863\u89e3\u6790\u3001\u4ee3\u7801\u89c4\u5212\u3001\u5b9e\u73b0\u3001\u5185\u7f6e\u6d4b\u8bd5\u548c\u6587\u6863\u751f\u6210\u7b49\u6a21\u5757\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u652f\u6301\u7b97\u6cd5\u5b9e\u73b0\u3001\u524d\u7aef\u5f00\u53d1\u548c\u670d\u52a1\u5668\u7aef\u751f\u6210\u4e09\u4e2a\u9886\u57df\u7684\u4ee3\u7801\u8f6c\u6362\u3002", "conclusion": "DeepCode\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7814\u7a76\u8bba\u6587\u5230\u4ee3\u7801\u7684\u8f6c\u6362\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "topic": "code agent"}}
{"id": "tldr.2601.ac99fe95", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.chasewhughes.com%2Fwriting%2Fbeyond-the-replica-the-case-for-first-principles-agents%3Futm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/QcOYvii1Mf5S4JfHH3QxKS7V6pOic7MYtENScbXAIno=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.chasewhughes.com%2Fwriting%2Fbeyond-the-replica-the-case-for-first-principles-agents%3Futm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/QcOYvii1Mf5S4JfHH3QxKS7V6pOic7MYtENScbXAIno=438", "authors": ["TLDR Newsletter"], "title": "Beyond the Replica: The Case for First-Principles Agents", "comment": "Source: TLDR Newsletter, Date: 2026-01-02, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.chasewhughes.com%2Fwriting%2Fbeyond-the-replica-the-case-for-first-principles-agents%3Futm_source=tldrai/1/0100019b7f29104e-57eb4d9a-7e6e-437b-b664-a8357a154365-000000/QcOYvii1Mf5S4JfHH3QxKS7V6pOic7MYtENScbXAIno=438", "summary": "Beyond the Replica: The Case for First-Principles Agents (6 minute read) True agentic efficiency requires abandoning human workflows.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u653e\u5f03\u4eba\u7c7b\u5de5\u4f5c\u6d41\u7a0b\uff0c\u91c7\u7528\u7b2c\u4e00\u6027\u539f\u7406\u65b9\u6cd5\u6784\u5efa\u771f\u6b63\u9ad8\u6548\u7684\u667a\u80fd\u4f53", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u7cfb\u7edf\u5927\u591a\u6a21\u4eff\u4eba\u7c7b\u5de5\u4f5c\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u6548\u7387\u548c\u6f5c\u529b\uff0c\u9700\u8981\u4ece\u6839\u672c\u4e0a\u91cd\u65b0\u601d\u8003\u667a\u80fd\u4f53\u8bbe\u8ba1", "method": "\u63d0\u51fa\u7b2c\u4e00\u6027\u539f\u7406\u65b9\u6cd5\uff0c\u4ece\u57fa\u672c\u539f\u7406\u51fa\u53d1\u8bbe\u8ba1\u667a\u80fd\u4f53\uff0c\u800c\u975e\u7b80\u5355\u590d\u5236\u4eba\u7c7b\u5de5\u4f5c\u6a21\u5f0f", "result": "\u8be5\u65b9\u6cd5\u6709\u671b\u5b9e\u73b0\u8d85\u8d8a\u4eba\u7c7b\u5de5\u4f5c\u6d41\u7a0b\u7684\u667a\u80fd\u4f53\u6548\u7387\u548c\u80fd\u529b", "conclusion": "\u771f\u6b63\u7684\u667a\u80fd\u4f53\u6548\u7387\u9700\u8981\u653e\u5f03\u4eba\u7c7b\u5de5\u4f5c\u6d41\u7a0b\uff0c\u91c7\u7528\u7b2c\u4e00\u6027\u539f\u7406\u65b9\u6cd5\u91cd\u65b0\u8bbe\u8ba1", "topic": "agent analysis"}}
{"id": "tldr.2601.9d70b5d1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fsafe-mcp-a-community-built-framework-for-ai-agent-security%2F%3Futm_source=tldrdata/1/0100019b8dd7c2e4-442b9271-9e4c-472d-ba14-5c5865bf48fd-000000/Twa4LjM3g7UnvKglvHVWXBMRF7nkYodf5MNb04Smn6k=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fsafe-mcp-a-community-built-framework-for-ai-agent-security%2F%3Futm_source=tldrdata/1/0100019b8dd7c2e4-442b9271-9e4c-472d-ba14-5c5865bf48fd-000000/Twa4LjM3g7UnvKglvHVWXBMRF7nkYodf5MNb04Smn6k=438", "authors": ["TLDR Newsletter"], "title": "SAFE-MCP, a Community-Built Framework for AI Agent Security", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fsafe-mcp-a-community-built-framework-for-ai-agent-security%2F%3Futm_source=tldrdata/1/0100019b8dd7c2e4-442b9271-9e4c-472d-ba14-5c5865bf48fd-000000/Twa4LjM3g7UnvKglvHVWXBMRF7nkYodf5MNb04Smn6k=438", "summary": "SAFE-MCP, a Community-Built Framework for AI Agent Security (5 minute read) SAFE-MCP, now formally adopted by the Linux Foundation and OpenID Foundation, delivers a standardized, community-driven security framework for AI agent ecosystems using Model Context Protocol (MCP). Offering over 80 documented techniques and more than a dozen tactic categories, it provides actionable, MITRE ATT&CK-style guidance for threat detection and mitigation (e.g. prompt manipulation, tool poisoning, and OAuth a...", "source": "tldr", "AI": {"tldr": "SAFE-MCP\u662f\u4e00\u4e2a\u7531\u793e\u533a\u6784\u5efa\u7684AI\u4ee3\u7406\u5b89\u5168\u6846\u67b6\uff0c\u5df2\u88abLinux\u57fa\u91d1\u4f1a\u548cOpenID\u57fa\u91d1\u4f1a\u6b63\u5f0f\u91c7\u7eb3\uff0c\u4e3a\u57fa\u4e8eModel Context Protocol\u7684AI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u6807\u51c6\u5316\u5b89\u5168\u6307\u5bfc\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u5b89\u5168\u6846\u67b6\u6765\u5e94\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u5a01\u80c1\uff08\u5982\u63d0\u793a\u64cd\u7eb5\u3001\u5de5\u5177\u4e2d\u6bd2\u3001OAuth\u653b\u51fb\u7b49\uff09\uff0c\u9700\u8981\u793e\u533a\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u6765\u786e\u4fddAI\u4ee3\u7406\u7684\u5b89\u5168\u6027\u3002", "method": "\u57fa\u4e8eModel Context Protocol\u6784\u5efa\u6807\u51c6\u5316\u5b89\u5168\u6846\u67b6\uff0c\u63d0\u4f9b\u8d85\u8fc780\u79cd\u6587\u6863\u5316\u6280\u672f\u548c\u5341\u591a\u4e2a\u6218\u672f\u7c7b\u522b\uff0c\u91c7\u7528\u7c7b\u4f3cMITRE ATT&CK\u7684\u5a01\u80c1\u68c0\u6d4b\u548c\u7f13\u89e3\u6307\u5bfc\u65b9\u6cd5\u3002", "result": "SAFE-MCP\u5df2\u88abLinux\u57fa\u91d1\u4f1a\u548cOpenID\u57fa\u91d1\u4f1a\u6b63\u5f0f\u91c7\u7eb3\uff0c\u6210\u4e3aAI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u7684\u6807\u51c6\u5316\u5b89\u5168\u6846\u67b6\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u5b89\u5168\u6307\u5bfc\u3002", "conclusion": "SAFE-MCP\u901a\u8fc7\u793e\u533a\u9a71\u52a8\u7684\u6807\u51c6\u5316\u65b9\u6cd5\uff0c\u4e3aAI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b89\u5168\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5e94\u5bf9\u5f53\u524d\u548c\u672a\u6765\u7684\u5b89\u5168\u5a01\u80c1\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.8d0ab01a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FDec%2F31%2Fthe-year-in-llms%2F%3Futm_source=tldrdata/1/0100019b8dd7c2e4-442b9271-9e4c-472d-ba14-5c5865bf48fd-000000/_Btu0x94PuRVAvJsmvjdyhdTkwxqBkxLImRCn9XK04Y=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FDec%2F31%2Fthe-year-in-llms%2F%3Futm_source=tldrdata/1/0100019b8dd7c2e4-442b9271-9e4c-472d-ba14-5c5865bf48fd-000000/_Btu0x94PuRVAvJsmvjdyhdTkwxqBkxLImRCn9XK04Y=438", "authors": ["TLDR Newsletter"], "title": "2025: The Year in LLMs", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FDec%2F31%2Fthe-year-in-llms%2F%3Futm_source=tldrdata/1/0100019b8dd7c2e4-442b9271-9e4c-472d-ba14-5c5865bf48fd-000000/_Btu0x94PuRVAvJsmvjdyhdTkwxqBkxLImRCn9XK04Y=438", "summary": "2025: The Year in LLMs (10 minute read) LLMs saw rapid advancements last year in reasoning capabilities, agentic systems (especially coding agents), and multimodal features like prompt-driven image editing. Chinese labs dominated open-weight models. Breakthroughs enabled models to win gold at the IMO and handle multi-hour tasks. Despite OpenAI and Anthropic's strong releases, progress raised significant concerns around security risks like prompt injection, environmental impacts from data cent...", "source": "tldr", "AI": {"tldr": "2025\u5e74LLM\u9886\u57df\u5728\u63a8\u7406\u80fd\u529b\u3001\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u7279\u522b\u662f\u4ee3\u7801\u667a\u80fd\u4f53\uff09\u548c\u591a\u6a21\u6001\u7279\u5f81\u65b9\u9762\u53d6\u5f97\u5feb\u901f\u8fdb\u5c55\uff0c\u4e2d\u56fd\u5b9e\u9a8c\u5ba4\u4e3b\u5bfc\u5f00\u6e90\u6a21\u578b\uff0c\u6a21\u578b\u5728IMO\u7ade\u8d5b\u4e2d\u83b7\u91d1\u724c\u5e76\u80fd\u5904\u7406\u591a\u5c0f\u65f6\u4efb\u52a1\uff0c\u4f46\u540c\u65f6\u4e5f\u5f15\u53d1\u5b89\u5168\u98ce\u9669\u548c\u73af\u5883\u5f71\u54cd\u7684\u62c5\u5fe7", "motivation": "\u603b\u7ed32025\u5e74\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9886\u57df\u7684\u4e3b\u8981\u53d1\u5c55\u8d8b\u52bf\u3001\u6280\u672f\u8fdb\u6b65\u548c\u5173\u952e\u6311\u6218\uff0c\u4e3a\u8bfb\u8005\u63d0\u4f9b\u5e74\u5ea6\u6982\u89c8", "method": "\u901a\u8fc7\u5206\u67902025\u5e74LLM\u9886\u57df\u7684\u5173\u952e\u4e8b\u4ef6\u3001\u6280\u672f\u7a81\u7834\u548c\u884c\u4e1a\u52a8\u6001\uff0c\u8fdb\u884c\u7efc\u5408\u6027\u7684\u5e74\u5ea6\u56de\u987e\u548c\u8d8b\u52bf\u603b\u7ed3", "result": "\u8bc6\u522b\u51fa\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3001\u667a\u80fd\u4f53\u7cfb\u7edf\u53d1\u5c55\uff08\u7279\u522b\u662f\u4ee3\u7801\u667a\u80fd\u4f53\uff09\u3001\u591a\u6a21\u6001\u7279\u5f81\u589e\u5f3a\u3001\u4e2d\u56fd\u5f00\u6e90\u6a21\u578b\u4e3b\u5bfc\u5730\u4f4d\u3001\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u7834\u7b49\u4e3b\u8981\u8fdb\u5c55\uff0c\u540c\u65f6\u6307\u51fa\u5b89\u5168\u98ce\u9669\u548c\u73af\u5883\u95ee\u9898\u7b49\u6311\u6218", "conclusion": "2025\u5e74\u662fLLM\u5feb\u901f\u53d1\u5c55\u7684\u4e00\u5e74\uff0c\u5728\u6280\u672f\u80fd\u529b\u663e\u8457\u63d0\u5347\u7684\u540c\u65f6\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u548c\u793e\u4f1a\u8d23\u4efb\u6311\u6218\uff0c\u9700\u8981\u5e73\u8861\u6280\u672f\u8fdb\u6b65\u4e0e\u98ce\u9669\u7ba1\u7406", "topic": "agent analysis"}}
{"id": "tldr.2601.5a893dfb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2007179832300581177.html%3Futm_source=tldrnewsletter/1/0100019b8de7958e-39d8b72c-f9e0-4412-b21f-8af4f00a5871-000000/PgsqrR7xuFMHRUGEJ4sx9mhLShryS3Qcd9EBQz4h9to=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2007179832300581177.html%3Futm_source=tldrnewsletter/1/0100019b8de7958e-39d8b72c-f9e0-4412-b21f-8af4f00a5871-000000/PgsqrR7xuFMHRUGEJ4sx9mhLShryS3Qcd9EBQz4h9to=438", "authors": ["TLDR Newsletter"], "title": "Boris Cherny's Claude Code setup", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2007179832300581177.html%3Futm_source=tldrnewsletter/1/0100019b8de7958e-39d8b72c-f9e0-4412-b21f-8af4f00a5871-000000/PgsqrR7xuFMHRUGEJ4sx9mhLShryS3Qcd9EBQz4h9to=438", "summary": "Boris Cherny's Claude Code setup (8 minute read) Boris Cherny created Claude Code. In this thread, he details his setup and how he works with Claude Code. Claude Code was created to be very customizable, but Cherny uses a surprisingly vanilla setup. Each person on the Claude Code team uses the IDE very differently.", "source": "tldr", "AI": {"tldr": "Boris Cherny\u5206\u4eab\u4e86\u4ed6\u4f7f\u7528Claude Code\u7684\u914d\u7f6e\u548c\u5de5\u4f5c\u65b9\u5f0f\uff0c\u867d\u7136Claude Code\u9ad8\u5ea6\u53ef\u5b9a\u5236\uff0c\u4f46\u4ed6\u91c7\u7528\u4e86\u76f8\u5bf9\u7b80\u5355\u7684\u8bbe\u7f6e\uff0c\u56e2\u961f\u4e2d\u4e0d\u540c\u6210\u5458\u7684\u4f7f\u7528\u65b9\u5f0f\u4e5f\u5404\u4e0d\u76f8\u540c\u3002", "motivation": "\u5206\u4eabClaude Code\u7684\u5b9e\u9645\u4f7f\u7528\u7ecf\u9a8c\u548c\u914d\u7f6e\u65b9\u6cd5\uff0c\u5c55\u793a\u5373\u4f7f\u9ad8\u5ea6\u53ef\u5b9a\u5236\u7684\u5de5\u5177\u4e5f\u53ef\u4ee5\u91c7\u7528\u7b80\u5355\u8bbe\u7f6e\uff0c\u5e76\u5f3a\u8c03\u4e2a\u6027\u5316\u4f7f\u7528\u65b9\u5f0f\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u4e2a\u4eba\u7ecf\u9a8c\u5206\u4eab\u7684\u65b9\u5f0f\uff0c\u8be6\u7ec6\u63cf\u8ff0\u4e86Claude Code\u7684\u914d\u7f6e\u8bbe\u7f6e\u3001\u5de5\u4f5c\u6d41\u7a0b\u4ee5\u53ca\u56e2\u961f\u4e2d\u4e0d\u540c\u6210\u5458\u7684\u4f7f\u7528\u5dee\u5f02\u3002", "result": "\u5c55\u793a\u4e86Claude Code\u5728\u5b9e\u9645\u5f00\u53d1\u4e2d\u7684\u7075\u6d3b\u5e94\u7528\uff0c\u5373\u4f7f\u91c7\u7528\u76f8\u5bf9\u7b80\u5355\u7684\u914d\u7f6e\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u5de5\u5177\u4f7f\u7528\u7684\u4e2a\u6027\u5316\u7279\u70b9\u3002", "conclusion": "Claude Code\u4f5c\u4e3a\u9ad8\u5ea6\u53ef\u5b9a\u5236\u7684\u5f00\u53d1\u5de5\u5177\uff0c\u53ef\u4ee5\u6839\u636e\u4e2a\u4eba\u504f\u597d\u91c7\u7528\u4e0d\u540c\u914d\u7f6e\uff0c\u7b80\u5355\u7684\u8bbe\u7f6e\u540c\u6837\u6709\u6548\uff0c\u56e2\u961f\u4e2d\u591a\u6837\u5316\u7684\u4f7f\u7528\u65b9\u5f0f\u4f53\u73b0\u4e86\u5de5\u5177\u7684\u7075\u6d3b\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2601.bb74266d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FpCgKBF/1/0100019b8de7958e-39d8b72c-f9e0-4412-b21f-8af4f00a5871-000000/v59q8ARuVgdJyALqCOjc5dVlanwTmzH3hI3es1a3YO0=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FpCgKBF/1/0100019b8de7958e-39d8b72c-f9e0-4412-b21f-8af4f00a5871-000000/v59q8ARuVgdJyALqCOjc5dVlanwTmzH3hI3es1a3YO0=438", "authors": ["TLDR Newsletter"], "title": "The disappearing middle of software work", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FpCgKBF/1/0100019b8de7958e-39d8b72c-f9e0-4412-b21f-8af4f00a5871-000000/v59q8ARuVgdJyALqCOjc5dVlanwTmzH3hI3es1a3YO0=438", "summary": "The disappearing middle of software work (3 minute read) The center of software work is moving. Previously, the model of software work - turning intent into something real - absorbed most of the time, attention, and craft of software teams. AI agents can now produce working code from goals, context, and tasks. As systems improve, the middle will get thinner, and less time will be spent manually translating intent into implementation. Developers will still need to understand the problem, gathe...", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u6b63\u5728\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u6a21\u5f0f\uff0c\u4f20\u7edf\u4e0a\u5360\u636e\u5927\u90e8\u5206\u65f6\u95f4\u7684\"\u610f\u56fe\u5230\u5b9e\u73b0\"\u7684\u4e2d\u95f4\u73af\u8282\u6b63\u5728\u88ab\u81ea\u52a8\u5316\uff0c\u5f00\u53d1\u8005\u89d2\u8272\u5c06\u8f6c\u5411\u95ee\u9898\u7406\u89e3\u548c\u7cfb\u7edf\u8bbe\u8ba1", "motivation": "\u63a2\u8ba8AI\u4ee3\u7406\u6280\u672f\u5982\u4f55\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u7684\u7ed3\u6784\u548c\u6d41\u7a0b\uff0c\u5206\u6790\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u4e2d\"\u4e2d\u95f4\u73af\u8282\"\uff08\u610f\u56fe\u5230\u5b9e\u73b0\uff09\u7684\u91cd\u8981\u6027\u4ee5\u53caAI\u81ea\u52a8\u5316\u5e26\u6765\u7684\u5f71\u54cd", "method": "\u6982\u5ff5\u6027\u5206\u6790\uff0c\u57fa\u4e8eAI\u4ee3\u7406\u6280\u672f\u7684\u53d1\u5c55\u8d8b\u52bf\u548c\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u89c2\u5bdf\uff0c\u63d0\u51fa\"\u6d88\u5931\u7684\u4e2d\u95f4\"\u7406\u8bba\u6846\u67b6", "result": "AI\u4ee3\u7406\u80fd\u591f\u4ece\u76ee\u6807\u3001\u4e0a\u4e0b\u6587\u548c\u4efb\u52a1\u751f\u6210\u53ef\u5de5\u4f5c\u4ee3\u7801\uff0c\u5bfc\u81f4\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u4f20\u7edf\u4e0a\u5360\u636e\u5927\u90e8\u5206\u65f6\u95f4\u7684\"\u4e2d\u95f4\u73af\u8282\"\uff08\u610f\u56fe\u5230\u5b9e\u73b0\uff09\u9010\u6e10\u53d8\u8584\uff0c\u5f00\u53d1\u8005\u89d2\u8272\u5c06\u91cd\u65b0\u5b9a\u4f4d", "conclusion": "\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u7684\u4e2d\u5fc3\u6b63\u5728\u8f6c\u79fb\uff0c\u5f00\u53d1\u8005\u9700\u8981\u66f4\u591a\u5173\u6ce8\u95ee\u9898\u7406\u89e3\u3001\u9700\u6c42\u6536\u96c6\u548c\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u800c\u4f20\u7edf\u7684\u624b\u52a8\u7f16\u7801\u5b9e\u73b0\u73af\u8282\u5c06\u51cf\u5c11", "topic": "agent analysis"}}
{"id": "tldr.2601.294562cb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fai-coding-workflow%2F%3Futm_source=tldrnewsletter/1/0100019b8de7958e-39d8b72c-f9e0-4412-b21f-8af4f00a5871-000000/tW1phjdw_SFEC2NR1sLY5SK7uXJ73NqEHMJq7kyMV48=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fai-coding-workflow%2F%3Futm_source=tldrnewsletter/1/0100019b8de7958e-39d8b72c-f9e0-4412-b21f-8af4f00a5871-000000/tW1phjdw_SFEC2NR1sLY5SK7uXJ73NqEHMJq7kyMV48=438", "authors": ["TLDR Newsletter"], "title": "My LLM coding workflow going into 2026", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: 30 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fai-coding-workflow%2F%3Futm_source=tldrnewsletter/1/0100019b8de7958e-39d8b72c-f9e0-4412-b21f-8af4f00a5871-000000/tW1phjdw_SFEC2NR1sLY5SK7uXJ73NqEHMJq7kyMV48=438", "summary": "My LLM coding workflow going into 2026 (30 minute read) This post presents a disciplined approach to AI-assisted engineering that leverages AI aggressively while staying accountable for the software produced.", "source": "tldr", "AI": {"tldr": "\u4ecb\u7ecd\u4e00\u79cd\u9762\u54112026\u5e74\u7684LLM\u7f16\u7801\u5de5\u4f5c\u6d41\uff0c\u91c7\u7528\u79ef\u6781\u5229\u7528AI\u540c\u65f6\u4fdd\u6301\u5bf9\u8f6f\u4ef6\u4ea7\u51fa\u8d1f\u8d23\u7684\u4e25\u8c28\u65b9\u6cd5", "motivation": "\u968f\u7740AI\u8f85\u52a9\u7f16\u7a0b\u5de5\u5177\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u5efa\u7acb\u65e2\u80fd\u5145\u5206\u5229\u7528AI\u80fd\u529b\u53c8\u80fd\u786e\u4fdd\u8f6f\u4ef6\u8d28\u91cf\u548c\u5f00\u53d1\u8005\u8d23\u4efb\u7684\u5de5\u4f5c\u6d41\u7a0b", "method": "\u63d0\u51fa\u4e00\u5957\u7cfb\u7edf\u7684AI\u8f85\u52a9\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5305\u62ec\u5982\u4f55\u6574\u5408LLM\u5230\u7f16\u7801\u6d41\u7a0b\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u5f00\u53d1\u8005\u7684\u76d1\u7763\u548c\u95ee\u8d23\u673a\u5236", "result": "\u63cf\u8ff0\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u5b9e\u8df5\u9a8c\u8bc1\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u80fd\u591f\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u540c\u65f6\u786e\u4fdd\u4ee3\u7801\u8d28\u91cf\u548c\u53ef\u7ef4\u62a4\u6027", "conclusion": "AI\u8f85\u52a9\u7f16\u7a0b\u9700\u8981\u5e73\u8861\u81ea\u52a8\u5316\u4e0e\u4eba\u5de5\u76d1\u7763\uff0c\u5efa\u7acb\u8d1f\u8d23\u4efb\u7684\u5de5\u4f5c\u6d41\u7a0b\u662f\u672a\u6765\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5173\u952e", "topic": "swe application"}}
{"id": "tldr.2601.3c66157e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2F2025%2Fshipping-at-inference-speed%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/ul_0Ly8PDzCu23sr1nrWExeZ26vQfQ2fZOLDM5TTpgE=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2F2025%2Fshipping-at-inference-speed%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/ul_0Ly8PDzCu23sr1nrWExeZ26vQfQ2fZOLDM5TTpgE=438", "authors": ["TLDR Newsletter"], "title": "Shipping at Inference-Speed", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2F2025%2Fshipping-at-inference-speed%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/ul_0Ly8PDzCu23sr1nrWExeZ26vQfQ2fZOLDM5TTpgE=438", "summary": "Shipping at Inference-Speed (8 minute read) AI-powered coding has accelerated this dev's software development speed, with GPT-5/5.2 Codex being a major breakthrough that allows them to ship code at \"inference-speed.\u201d This means they are limited mainly by AI processing time rather than human coding ability. GPT's Codex takes longer to start (sometimes 10-15 minutes reading code), but it produces more reliable results than Claude's Opus, which works faster but often requires fixes. They now rar...", "source": "tldr", "AI": {"tldr": "\u5f00\u53d1\u4eba\u5458\u5229\u7528GPT-5/5.2 Codex\u5b9e\u73b0\"\u63a8\u7406\u901f\u5ea6\"\u7f16\u7a0b\uff0cAI\u5904\u7406\u65f6\u95f4\u6210\u4e3a\u4e3b\u8981\u9650\u5236\u56e0\u7d20\u800c\u975e\u4eba\u7c7b\u7f16\u7801\u80fd\u529b\uff0c\u867d\u7136\u542f\u52a8\u8f83\u6162\u4f46\u7ed3\u679c\u6bd4Claude Opus\u66f4\u53ef\u9760", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528AI\u4ee3\u7801\u52a9\u624b\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\uff0c\u7279\u522b\u662f\u901a\u8fc7GPT Codex\u5b9e\u73b0\"\u63a8\u7406\u901f\u5ea6\"\u7f16\u7a0b\uff0c\u4ee5AI\u5904\u7406\u65f6\u95f4\u66ff\u4ee3\u4f20\u7edf\u4eba\u7c7b\u7f16\u7801\u65f6\u95f4\u4f5c\u4e3a\u4e3b\u8981\u74f6\u9888", "method": "\u4f7f\u7528GPT-5/5.2 Codex\u4f5c\u4e3a\u4e3b\u8981AI\u7f16\u7801\u5de5\u5177\uff0c\u5bf9\u6bd4\u5206\u6790\u5176\u4e0eClaude Opus\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u91cd\u70b9\u5173\u6ce8\u542f\u52a8\u65f6\u95f4\u3001\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u548c\u53ef\u9760\u6027", "result": "GPT Codex\u867d\u7136\u542f\u52a8\u65f6\u95f4\u8f83\u957f\uff0810-15\u5206\u949f\u9605\u8bfb\u4ee3\u7801\uff09\uff0c\u4f46\u751f\u6210\u7ed3\u679c\u66f4\u53ef\u9760\uff1bClaude Opus\u54cd\u5e94\u66f4\u5feb\u4f46\u5e38\u9700\u8981\u4fee\u590d\u3002AI\u5904\u7406\u65f6\u95f4\u6210\u4e3a\u5f00\u53d1\u901f\u5ea6\u7684\u4e3b\u8981\u9650\u5236\u56e0\u7d20", "conclusion": "AI\u4ee3\u7801\u52a9\u624b\u5df2\u80fd\u5b9e\u73b0\"\u63a8\u7406\u901f\u5ea6\"\u7f16\u7a0b\uff0c\u663e\u8457\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\uff0cGPT Codex\u5728\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u6807\u5fd7\u7740AI\u8f85\u52a9\u7f16\u7a0b\u8fdb\u5165\u65b0\u9636\u6bb5", "topic": "code agent"}}
{"id": "tldr.2601.d8f6f40d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fbuilding-a-code-review-system-that-uses-prod-data-to-predict-bugs%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=seer-fy26q4-aicodereviewlaunch%26utm_content=newsletter-prod-blog-learnmore/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/uDwKNnEkBgrfhONk4FHjMEGxC5i1ugWeho4cv1oh8vU=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fbuilding-a-code-review-system-that-uses-prod-data-to-predict-bugs%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=seer-fy26q4-aicodereviewlaunch%26utm_content=newsletter-prod-blog-learnmore/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/uDwKNnEkBgrfhONk4FHjMEGxC5i1ugWeho4cv1oh8vU=438", "authors": ["TLDR Newsletter"], "title": "How Sentry built its AI Code Review: architecture, context, quality, and evals", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fbuilding-a-code-review-system-that-uses-prod-data-to-predict-bugs%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=seer-fy26q4-aicodereviewlaunch%26utm_content=newsletter-prod-blog-learnmore/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/uDwKNnEkBgrfhONk4FHjMEGxC5i1ugWeho4cv1oh8vU=438", "summary": "How Sentry built its AI Code Review: architecture, context, quality, and evals (Sponsor) Tinkering with agentic AI? Check out this technical deep dive by the Sentry engineering team to learn how they built a context-aware agent that uses prod data to predict bugs. Read the blog", "source": "tldr", "AI": {"tldr": "Sentry\u56e2\u961f\u5206\u4eab\u4e86\u4ed6\u4eec\u6784\u5efaAI\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\u7684\u6280\u672f\u7ec6\u8282\uff0c\u5305\u62ec\u67b6\u6784\u8bbe\u8ba1\u3001\u4e0a\u4e0b\u6587\u5904\u7406\u3001\u8d28\u91cf\u4fdd\u8bc1\u548c\u8bc4\u4f30\u65b9\u6cd5", "motivation": "\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u5229\u7528\u751f\u4ea7\u6570\u636e\u9884\u6d4bbug\u7684\u4e0a\u4e0b\u6587\u611f\u77e5AI\u4ee3\u7406\uff0c\u6539\u8fdb\u4ee3\u7801\u5ba1\u67e5\u6d41\u7a0b", "method": "\u57fa\u4e8e\u4ee3\u7406\u5f0fAI\u67b6\u6784\uff0c\u7ed3\u5408\u751f\u4ea7\u6570\u636e\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u6784\u5efa\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ee3\u7801\u5ba1\u67e5\u4ee3\u7406", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u9884\u6d4bbug\u7684AI\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u6280\u672f\u535a\u5ba2\u5206\u4eab\u4e86\u67b6\u6784\u8bbe\u8ba1\u548c\u5b9e\u73b0\u7ecf\u9a8c", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u751f\u4ea7\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u6280\u672f\uff0c\u53ef\u4ee5\u6784\u5efa\u6709\u6548\u7684AI\u4ee3\u7801\u5ba1\u67e5\u7cfb\u7edf\u6765\u9884\u6d4b\u548c\u9884\u9632bug", "topic": "code agent"}}
{"id": "tldr.2601.3ae4bafe", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Faidenybai%2Freact-grab%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/iVSNeA9nEK2J1XB5mI9qNwG_1Qt1As03uIZY9UqMcHs=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Faidenybai%2Freact-grab%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/iVSNeA9nEK2J1XB5mI9qNwG_1Qt1As03uIZY9UqMcHs=438", "authors": ["TLDR Newsletter"], "title": "React Grab", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Faidenybai%2Freact-grab%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/iVSNeA9nEK2J1XB5mI9qNwG_1Qt1As03uIZY9UqMcHs=438", "summary": "React Grab (GitHub Repo) React Grab improves the accuracy and speed of AI coding agents by allowing devs to directly select UI element context from their React apps. Users can hover over any UI element and press a hotkey (\u2318C or Ctrl+C) to copy its file name, React component, and HTML source code. This capability provides precise context to AI tools, making them much faster and more accurate in understanding and modifying code.", "source": "tldr", "AI": {"tldr": "React Grab \u662f\u4e00\u4e2a\u5de5\u5177\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u76f4\u63a5\u4ece React \u5e94\u7528\u4e2d\u590d\u5236 UI \u5143\u7d20\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff08\u6587\u4ef6\u540d\u3001React \u7ec4\u4ef6\u3001HTML \u6e90\u7801\uff09\uff0c\u4ee5\u63d0\u9ad8 AI \u7f16\u7a0b\u4ee3\u7406\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002", "motivation": "AI \u7f16\u7a0b\u4ee3\u7406\u5728\u7406\u89e3\u548c\u4fee\u6539\u4ee3\u7801\u65f6\u7f3a\u4e4f\u7cbe\u786e\u7684 UI \u5143\u7d20\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u51c6\u786e\u6027\u4e0d\u8db3\u3002\u5f00\u53d1\u8005\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5feb\u901f\u83b7\u53d6 UI \u5143\u7d20\u7684\u5177\u4f53\u4ee3\u7801\u4fe1\u606f\uff0c\u4ee5\u4fbf\u4e3a AI \u5de5\u5177\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u4e0a\u4e0b\u6587\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u5177\uff08React Grab\uff09\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u60ac\u505c\u5728 React \u5e94\u7528\u7684 UI \u5143\u7d20\u4e0a\u5e76\u6309\u4e0b\u70ed\u952e\uff08\u2318C \u6216 Ctrl+C\uff09\u6765\u590d\u5236\u8be5\u5143\u7d20\u7684\u6587\u4ef6\u540d\u3001React \u7ec4\u4ef6\u548c HTML \u6e90\u4ee3\u7801\u3002\u8fd9\u79cd\u76f4\u63a5\u9009\u62e9 UI \u5143\u7d20\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u4e3a AI \u5de5\u5177\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u4ee3\u7801\u4fe1\u606f\u3002", "result": "React Grab \u80fd\u591f\u663e\u8457\u63d0\u9ad8 AI \u7f16\u7a0b\u4ee3\u7406\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\uff0c\u4f7f AI \u5de5\u5177\u80fd\u591f\u66f4\u5feb\u901f\u3001\u66f4\u51c6\u786e\u5730\u7406\u89e3\u548c\u4fee\u6539\u4ee3\u7801\uff0c\u56e0\u4e3a\u73b0\u5728\u5b83\u4eec\u53ef\u4ee5\u83b7\u5f97 UI \u5143\u7d20\u7684\u7cbe\u786e\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u76f4\u63a5\u4ece React \u5e94\u7528\u4e2d\u63d0\u53d6 UI \u5143\u7d20\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\uff0cReact Grab \u89e3\u51b3\u4e86 AI \u7f16\u7a0b\u4ee3\u7406\u5728\u4ee3\u7801\u7406\u89e3\u548c\u4fee\u6539\u4e2d\u7684\u4e0a\u4e0b\u6587\u7f3a\u5931\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\u548c AI \u5de5\u5177\u7684\u51c6\u786e\u6027\u3002", "topic": "code agent"}}
