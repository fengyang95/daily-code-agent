{"id": "2511.13825", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13825", "abs": "https://arxiv.org/abs/2511.13825", "authors": ["Humza Nusrat", "Omar Nusrat"], "title": "When AI Does Science: Evaluating the Autonomous AI Scientist KOSMOS in Radiation Biology", "comment": "13 pages, 3 figures, preprint", "summary": "Agentic AI \"scientists\" now use language models to search the literature, run analyses, and generate hypotheses. We evaluate KOSMOS, an autonomous AI scientist, on three problems in radiation biology using simple random-gene null benchmarks. Hypothesis 1: baseline DNA damage response (DDR) capacity across cell lines predicts the p53 transcriptional response after irradiation (GSE30240). Hypothesis 2: baseline expression of OGT and CDO1 predicts the strength of repressed and induced radiation-response modules in breast cancer cells (GSE59732). Hypothesis 3: a 12-gene expression signature predicts biochemical recurrence-free survival after prostate radiotherapy plus androgen deprivation therapy (GSE116918). The DDR-p53 hypothesis was not supported: DDR score and p53 response were weakly negatively correlated (Spearman rho = -0.40, p = 0.76), indistinguishable from random five-gene scores. OGT showed only a weak association (r = 0.23, p = 0.34), whereas CDO1 was a clear outlier (r = 0.70, empirical p = 0.0039). The 12-gene signature achieved a concordance index of 0.61 (p = 0.017) but a non-unique effect size. Overall, KOSMOS produced one well-supported discovery, one plausible but uncertain result, and one false hypothesis, illustrating that AI scientists can generate useful ideas but require rigorous auditing against appropriate null models.", "AI": {"tldr": "\u8bc4\u4f30KOSMOS\u81ea\u4e3bAI\u79d1\u5b66\u5bb6\u5728\u8f90\u5c04\u751f\u7269\u5b66\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u4ea7\u751f\u4e86\u4e00\u4e2a\u660e\u786e\u53d1\u73b0\u3001\u4e00\u4e2a\u4e0d\u786e\u5b9a\u7ed3\u679c\u548c\u4e00\u4e2a\u9519\u8bef\u5047\u8bbe\uff0c\u8868\u660eAI\u79d1\u5b66\u5bb6\u80fd\u751f\u6210\u6709\u7528\u60f3\u6cd5\u4f46\u9700\u8981\u4e25\u683c\u9a8c\u8bc1", "motivation": "\u8bc4\u4f30\u81ea\u4e3bAI\u79d1\u5b66\u5bb6KOSMOS\u5728\u8f90\u5c04\u751f\u7269\u5b66\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u68c0\u9a8c\u5176\u751f\u6210\u5047\u8bbe\u7684\u80fd\u529b", "method": "\u4f7f\u7528\u7b80\u5355\u968f\u673a\u57fa\u56e0\u96f6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u4e09\u4e2a\u8f90\u5c04\u751f\u7269\u5b66\u95ee\u9898\u4e0a\u8bc4\u4f30KOSMOS\uff1aDNA\u635f\u4f24\u54cd\u5e94\u4e0ep53\u8f6c\u5f55\u54cd\u5e94\u5173\u7cfb\u3001OGT\u548cCDO1\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u8f90\u5c04\u54cd\u5e94\u6a21\u5757\u300112\u57fa\u56e0\u7b7e\u540d\u9884\u6d4b\u524d\u5217\u817a\u764c\u653e\u7597\u540e\u751f\u5b58", "result": "\u5047\u8bbe1\u4e0d\u652f\u6301\uff08DDR\u8bc4\u5206\u4e0ep53\u54cd\u5e94\u5f31\u8d1f\u76f8\u5173\uff09\uff0c\u5047\u8bbe2\u4e2dCDO1\u662f\u660e\u663e\u5f02\u5e38\u503c\uff08r=0.70\uff0cp=0.0039\uff09\uff0c\u5047\u8bbe3\u768412\u57fa\u56e0\u7b7e\u540d\u4e00\u81f4\u6027\u6307\u65700.61\uff08p=0.017\uff09\u4f46\u6548\u5e94\u5927\u5c0f\u4e0d\u552f\u4e00", "conclusion": "AI\u79d1\u5b66\u5bb6\u53ef\u4ee5\u751f\u6210\u6709\u7528\u60f3\u6cd5\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u9002\u5f53\u96f6\u6a21\u578b\u8fdb\u884c\u4e25\u683c\u5ba1\u8ba1", "topic": "agent analysis"}}
{"id": "2511.13998", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13998", "abs": "https://arxiv.org/abs/2511.13998", "authors": ["Jielin Qiu", "Zuxin Liu", "Zhiwei Liu", "Rithesh Murthy", "Jianguo Zhang", "Haolin Chen", "Shiyu Wang", "Ming Zhu", "Liangwei Yang", "Juntao Tan", "Roshan Ram", "Akshara Prabhakar", "Tulika Awalgaonkar", "Zixiang Chen", "Zhepeng Cen", "Cheng Qian", "Shelby Heinecke", "Weiran Yao", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering", "comment": "54-pages", "summary": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \\textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.", "AI": {"tldr": "LoCoBench-Agent\u662f\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u771f\u5b9e\u957f\u4e0a\u4e0b\u6587\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5c06LoCoBench\u76848000\u4e2a\u573a\u666f\u6269\u5c55\u5230\u4ea4\u4e92\u5f0f\u4ee3\u7406\u73af\u5883\uff0c\u8bc4\u4f30\u591a\u8f6e\u5bf9\u8bdd\u3001\u5de5\u5177\u4f7f\u7528\u6548\u7387\u3001\u9519\u8bef\u6062\u590d\u548c\u67b6\u6784\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5982LoCoBench\u4e3b\u8981\u8bc4\u4f30\u5355\u8f6e\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u7406\u89e3\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u7f16\u7801\u4ee3\u7406\u6240\u9700\u7684\u591a\u8f6e\u4ea4\u4e92\u3001\u5de5\u5177\u4f7f\u7528\u6a21\u5f0f\u548c\u81ea\u9002\u5e94\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6269\u5c55LoCoBench\u573a\u666f\u4e3a\u4ea4\u4e92\u5f0f\u4ee3\u7406\u73af\u5883\uff0c\u63d0\u4f9b8\u4e2a\u4e13\u7528\u5de5\u5177\uff08\u6587\u4ef6\u64cd\u4f5c\u3001\u641c\u7d22\u3001\u4ee3\u7801\u5206\u6790\uff09\uff0c\u572810K\u52301M\u4ee4\u724c\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u8303\u56f4\u5185\u8bc4\u4f30\uff0c\u4f7f\u75289\u4e2a\u6db5\u76d6\u7406\u89e3\u548c\u6548\u7387\u7ef4\u5ea6\u7684\u6307\u6807\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff1a(1)\u4ee3\u7406\u5c55\u73b0\u663e\u8457\u7684\u957f\u4e0a\u4e0b\u6587\u9c81\u68d2\u6027\uff1b(2)\u7406\u89e3\u4e0e\u6548\u7387\u5b58\u5728\u8d1f\u76f8\u5173\u7684\u6743\u8861\uff1b(3)\u5bf9\u8bdd\u6548\u7387\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u7b56\u7565\u6027\u5de5\u5177\u4f7f\u7528\u6a21\u5f0f\u533a\u5206\u9ad8\u6027\u80fd\u4ee3\u7406\u3002", "conclusion": "\u4f5c\u4e3a\u9996\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u957f\u4e0a\u4e0b\u6587LLM\u4ee3\u7406\u57fa\u51c6\uff0cLoCoBench-Agent\u4e3a\u8861\u91cf\u4ee3\u7406\u80fd\u529b\u3001\u8bc6\u522b\u6027\u80fd\u5dee\u8ddd\u548c\u63a8\u8fdb\u5927\u89c4\u6a21\u81ea\u4e3b\u8f6f\u4ef6\u5f00\u53d1\u5efa\u7acb\u4e86\u4e25\u683c\u57fa\u7840\u3002", "topic": "swe benchmark"}}
{"id": "2511.13900", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13900", "abs": "https://arxiv.org/abs/2511.13900", "authors": ["Mihir Gupte", "Eshan Dixit", "Muhammad Tayyab", "Arun Adiththan"], "title": "What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations", "comment": "To be submitted for publication", "summary": "The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the \"lost-in-the-middle\" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\"\u8ff7\u5931\u5728\u4e2d\u95f4\"\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86GM-Extract\u57fa\u51c6\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u63a7\u5236\u53d8\u91cf\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u5f00\u53d1\u4e86\u7a7a\u95f4\u548c\u8bed\u4e49\u68c0\u7d22\u4e24\u4e2a\u8bc4\u4f30\u6307\u6807\u3002\u7814\u7a76\u53d1\u73b0\u6570\u636e\u8868\u793a\u65b9\u5f0f\u663e\u8457\u5f71\u54cd\u68c0\u7d22\u6027\u80fd\uff0c\u7f13\u89e3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u5177\u6709\u9ad8\u5ea6\u60c5\u5883\u4f9d\u8d56\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u8303\u56f4\u4e0a\u4e0b\u6587\u4e2d\u7684\"\u8ff7\u5931\u5728\u4e2d\u95f4\"\u73b0\u8c61\u5bf9\u57fa\u4e8e\u68c0\u7d22\u7684LLM\u5e94\u7528\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u5728\u5b9e\u9645\u5e94\u7528\u73af\u5883\u4e2d\u7814\u7a76\u8fd9\u4e00\u73b0\u8c61\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86GM-Extract\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u6587\u6863\u6307\u6807\u548c\u53d8\u91cf\u63d0\u53d6\u6307\u6807\u4e24\u4e2a\u8bc4\u4f30\u6307\u6807\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e867-8B\u53c2\u6570\u6a21\u578b\u5728\u591a\u6587\u6863\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u4e86\u7f13\u89e3\u65b9\u6cd5\u7684\u6587\u732e\u8c03\u7814\u548c\u5e94\u7528\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4ec5\u901a\u8fc7\u6539\u53d8\u6570\u636e\u5728\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e2d\u7684\u8868\u793a\u65b9\u5f0f\u5c31\u80fd\u663e\u8457\u6539\u53d8\u68c0\u7d22\u6027\u80fd\uff0c\u867d\u7136\u672a\u59cb\u7ec8\u89c2\u5bdf\u5230\u660e\u663e\u7684U\u5f62\u66f2\u7ebf\uff0c\u4f46\u53d1\u73b0\u4e86\u6e05\u6670\u7684\u6027\u80fd\u6a21\u5f0f\u3002\u7f13\u89e3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u9ad8\u5ea6\u5fae\u5999\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u6210\u529f\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u4f1a\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u5728\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0c\u6570\u636e\u8868\u793a\u65b9\u5f0f\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u7f13\u89e3\u7b56\u7565\u7684\u6548\u7528\u9700\u8981\u6839\u636e\u5177\u4f53\u60c5\u5883\u4ed4\u7ec6\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2511.14002", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.14002", "abs": "https://arxiv.org/abs/2511.14002", "authors": ["Chengpeng Li", "Farnaz Behrang", "August Shi", "Peng Liu"], "title": "FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale", "comment": "To appear in ASE 2025", "summary": "Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.", "AI": {"tldr": "FlakyGuard\u901a\u8fc7\u5c06\u4ee3\u7801\u89c6\u4e3a\u56fe\u7ed3\u6784\u5e76\u4f7f\u7528\u9009\u62e9\u6027\u56fe\u63a2\u7d22\u6765\u627e\u5230\u6700\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fee\u590d\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u65f6\u7684\u4e0a\u4e0b\u6587\u95ee\u9898\uff0c\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u4fee\u590d\u4e8647.6%\u7684\u53ef\u91cd\u73b0\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u3002", "motivation": "\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u4f1a\u6d6a\u8d39\u5f00\u53d1\u8005\u65f6\u95f4\u5e76\u51cf\u6162\u53d1\u5e03\u5468\u671f\uff0c\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u56e0\u4e0a\u4e0b\u6587\u95ee\u9898\uff08\u63d0\u4f9b\u592a\u5c11\u6216\u592a\u591a\u4e0a\u4e0b\u6587\uff09\u800c\u5931\u8d25\u3002", "method": "\u5c06\u4ee3\u7801\u89c6\u4e3a\u56fe\u7ed3\u6784\uff0c\u4f7f\u7528\u9009\u62e9\u6027\u56fe\u63a2\u7d22\u6765\u627e\u5230\u6700\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\uff0c\u907f\u514d\u63d0\u4f9b\u8fc7\u591a\u6216\u8fc7\u5c11\u7684\u4fe1\u606f\u3002", "result": "\u5728\u5de5\u4e1a\u4ed3\u5e93\u7684\u771f\u5b9e\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u4e2d\u4fee\u590d\u4e8647.6%\u7684\u53ef\u91cd\u73b0\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\uff0c51.8%\u7684\u4fee\u590d\u88ab\u5f00\u53d1\u8005\u63a5\u53d7\uff0c\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u81f3\u5c11\u9ad8\u51fa22%\u7684\u4fee\u590d\u6210\u529f\u7387\u3002", "conclusion": "FlakyGuard\u901a\u8fc7\u9009\u62e9\u6027\u56fe\u63a2\u7d22\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u4fee\u590d\u4e2d\u7684\u4e0a\u4e0b\u6587\u95ee\u9898\uff0c\u5f00\u53d1\u8005\u8c03\u67e5\u663e\u793a100%\u8ba4\u4e3a\u5176\u6839\u672c\u539f\u56e0\u89e3\u91ca\u6709\u7528\u3002", "topic": "swe application"}}
{"id": "2511.14022", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14022", "abs": "https://arxiv.org/abs/2511.14022", "authors": ["Pradeep Kumar Sharma", "Ishaan Puri", "Mantinder Jit Singh", "Swapnil Shivaprasad", "Hritvik Shrivastava"], "title": "Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning", "comment": null, "summary": "Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4ee3\u7801\u5e93\u6301\u7eed\u6f14\u5316\u80cc\u666f\u4e0b\uff0c\u5982\u4f55\u4fdd\u6301\u4ee3\u7801\u641c\u7d22\u6a21\u578b\u7684\u65b0\u9c9c\u5ea6\u800c\u4e0d\u4e22\u5931\u5bf9\u65e7\u4ee3\u7801\u7684\u8bb0\u5fc6\u3002\u6bd4\u8f83\u4e86\u4e09\u79cd\u66f4\u65b0\u7b56\u7565\uff1a\u5b8c\u5168\u5237\u65b0\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u589e\u91cf\u5fae\u8c03\uff0c\u53d1\u73b0\u589e\u91cf\u5fae\u8c03\u7ed3\u5408\u65b0\u65e7\u4ee3\u7801\u6df7\u5408\u8bad\u7ec3\u80fd\u83b7\u5f97\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u73b0\u4ee3\u4ee3\u7801\u5e93\u6301\u7eed\u6f14\u5316\u5bfc\u81f4\u8bad\u7ec3\u6a21\u578b\u5feb\u901f\u8fc7\u65f6\uff0c\u9700\u8981\u5728\u4e0d\u4e22\u5931\u65e7\u4ee3\u7801\u77e5\u8bc6\u7684\u524d\u63d0\u4e0b\u4fdd\u6301\u6a21\u578b\u5bf9\u6700\u65b0\u4ee3\u7801\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5c06\u4ee3\u7801\u65b0\u9c9c\u5ea6\u89c6\u4e3a\u9886\u57df\u6f02\u79fb\u95ee\u9898\uff0c\u6bd4\u8f83\u4e09\u79cd\u66f4\u65b0\u7b56\u7565\uff1a\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u6ce8\u5165\u6700\u65b0\u53d8\u66f4\u3001\u589e\u91cf\u5fae\u8c03\u7ed3\u5408\u65b0\u65e7\u4ee3\u7801\u6df7\u5408\u8bad\u7ec3\u4ee5\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728Flask\u3001SQLAlchemy\u3001Pandas\u548cPoetry\u7b49\u9879\u76ee\u4e2d\uff0c\u589e\u91cf\u5fae\u8c03\u7ed3\u5408\u65b0\u65e7\u4ee3\u7801\u6df7\u5408\u8bad\u7ec3\u5728\u6df7\u5408\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u65e0\u6cd5\u8bad\u7ec3\u65f6\u63d0\u4f9b\u6700\u5feb\u7684\u65b0\u4ee3\u7801\u63d0\u5347\uff0c\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u5728\u8ffd\u6c42\u6700\u9ad8\u65b0\u4ee3\u7801\u51c6\u786e\u7387\u65f6\u4ecd\u662f\u4e0a\u9650\u3002", "conclusion": "\u589e\u91cf\u5fae\u8c03\u662f\u4fdd\u6301\u4ee3\u7801\u641c\u7d22\u6a21\u578b\u65b0\u9c9c\u5ea6\u7684\u6700\u4f73\u5e73\u8861\u7b56\u7565\uff0c\u800c\u4e0d\u540c\u66f4\u65b0\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5404\u6709\u4f18\u52bf\u3002", "topic": "code agent"}}
{"id": "2511.13942", "categories": ["cs.AI", "cs.DS", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13942", "abs": "https://arxiv.org/abs/2511.13942", "authors": ["Daniel Weitekamp"], "title": "CORGI: Efficient Pattern Matching With Quadratic Guarantees", "comment": null, "summary": "Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $\u03b2$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.", "AI": {"tldr": "\u63d0\u51faCORGI\u7b97\u6cd5\u89e3\u51b3\u89c4\u5219\u5339\u914d\u4e2d\u7684\u6307\u6570\u7ea7\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e3aAI\u4ee3\u7406\u548c\u6570\u636e\u5e93\u67e5\u8be2\u63d0\u4f9b\u5b9e\u65f6\u6027\u80fd\u4fdd\u8bc1", "motivation": "\u5b9e\u65f6AI\u7cfb\u7edf\u548c\u6570\u636e\u5e93\u67e5\u8be2\u9700\u8981\u89e3\u51b3\u590d\u6742\u5339\u914d\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5305\u542b\u591a\u4e2a\u672a\u7ea6\u675f\u53d8\u91cf\u7684\u89c4\u5219\u65f6\u4f1a\u4ea7\u751f\u6307\u6570\u7ea7\u590d\u6742\u5ea6\uff0c\u5bfc\u81f4\u5185\u5b58\u6ea2\u51fa\u548c\u6267\u884c\u4e2d\u65ad", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u524d\u5411\u4f20\u9012\u6784\u5efa/\u7ef4\u62a4\u63a5\u5730\u5173\u7cfb\u56fe\uff0c\u540e\u5411\u8fed\u4ee3\u5668\u6309\u9700\u751f\u6210\u5339\u914d\uff0c\u907f\u514d\u4f20\u7edfRETE\u7b97\u6cd5\u4e2d\u03b2\u5185\u5b58\u6536\u96c6\u90e8\u5206\u5339\u914d\u7684\u95ee\u9898", "result": "\u5728\u6027\u80fd\u8bc4\u4f30\u4e2d\uff0cCORGI\u5728\u7b80\u5355\u7ec4\u5408\u5339\u914d\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eSOAR\u548cOPS5\u7684RETE\u5b9e\u73b0", "conclusion": "CORGI\u901a\u8fc7\u6d88\u9664\u586b\u5145\u5b8c\u6574\u51b2\u7a81\u96c6\u7684\u9700\u6c42\uff0c\u63d0\u4f9b\u4e86\u4e8c\u6b21\u65f6\u95f4\u548c\u7a7a\u95f4\u4fdd\u8bc1\uff0c\u80fd\u591f\u8fed\u4ee3\u6d41\u5f0f\u4f20\u8f93\u540e\u7eed\u5339\u914d\uff0c\u89e3\u51b3\u4e86\u9ad8\u5ef6\u8fdf\u548c\u5185\u5b58\u6ea2\u51fa\u95ee\u9898", "topic": "agent analysis"}}
{"id": "2511.14224", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14224", "abs": "https://arxiv.org/abs/2511.14224", "authors": ["Anji Li", "Mingwei Liu", "Zhenxi Chen", "Zheng Pei", "Zike Li", "Dekun Dai", "Yanlin Wang", "Zibin Zheng"], "title": "KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation", "comment": "13 pages, 11 figures", "summary": "Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.", "AI": {"tldr": "KTester\u662f\u4e00\u4e2a\u96c6\u6210\u9879\u76ee\u7279\u5b9a\u77e5\u8bc6\u548c\u6d4b\u8bd5\u9886\u57df\u77e5\u8bc6\u7684LLM\u6d4b\u8bd5\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u9759\u6001\u5206\u6790\u63d0\u53d6\u9879\u76ee\u7ed3\u6784\u548c\u4f7f\u7528\u77e5\u8bc6\uff0c\u91c7\u7528\u6d4b\u8bd5\u9886\u57df\u77e5\u8bc6\u5f15\u5bfc\u7684\u6d4b\u8bd5\u7528\u4f8b\u8bbe\u8ba1\u4e0e\u6d4b\u8bd5\u65b9\u6cd5\u751f\u6210\u5206\u79bb\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u63d0\u793a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u5728\u771f\u5b9e\u9879\u76ee\u4e2d\u5f80\u5f80\u96be\u4ee5\u751f\u6210\u65e2\u6b63\u786e\u53c8\u53ef\u7ef4\u62a4\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u9700\u8981\u66f4\u597d\u7684\u77e5\u8bc6\u96c6\u6210\u6846\u67b6\u6765\u63d0\u5347\u6d4b\u8bd5\u8d28\u91cf\u3002", "method": "1) \u901a\u8fc7\u9759\u6001\u5206\u6790\u63d0\u53d6\u9879\u76ee\u7ed3\u6784\u548c\u7528\u6cd5\u77e5\u8bc6\uff1b2) \u91c7\u7528\u6d4b\u8bd5\u9886\u57df\u77e5\u8bc6\u5f15\u5bfc\u7684\u6d4b\u8bd5\u7528\u4f8b\u8bbe\u8ba1\u4e0e\u6d4b\u8bd5\u65b9\u6cd5\u751f\u6210\u5206\u79bb\u7b56\u7565\uff1b3) \u591a\u89c6\u89d2\u63d0\u793a\u6280\u672f\u5f15\u5bfcLLM\u8003\u8651\u591a\u6837\u5316\u6d4b\u8bd5\u542f\u53d1\u5f0f\u65b9\u6cd5\uff1b4) \u4f7f\u7528\u7ed3\u6784\u5316\u6a21\u677f\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u6e90\u9879\u76ee\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cKTester\u5728\u516d\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6267\u884c\u901a\u8fc7\u7387\u63d0\u53475.69%\uff0c\u884c\u8986\u76d6\u7387\u63d0\u53478.83%\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u65f6\u95f4\u548c\u751f\u6210\u66f4\u5c11\u6d4b\u8bd5\u7528\u4f8b\u3002\u4eba\u5de5\u8bc4\u4f30\u4e5f\u786e\u8ba4KTester\u751f\u6210\u7684\u6d4b\u8bd5\u5728\u6b63\u786e\u6027\u3001\u53ef\u8bfb\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u65b9\u9762\u8bc4\u5206\u66f4\u9ad8\u3002", "conclusion": "KTester\u901a\u8fc7\u96c6\u6210\u9879\u76ee\u7279\u5b9a\u77e5\u8bc6\u548c\u6d4b\u8bd5\u9886\u57df\u77e5\u8bc6\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u6d4b\u8bd5\u751f\u6210\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\uff0c\u8bc1\u660e\u4e86\u77e5\u8bc6\u9a71\u52a8\u6846\u67b6\u5728\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u4f18\u52bf\u3002", "topic": "swe application"}}
{"id": "2511.13765", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13765", "abs": "https://arxiv.org/abs/2511.13765", "authors": ["Shengjie Sun", "Jiafei Lyu", "Runze Liu", "Mengbei Yan", "Bo Liu", "Deheng Ye", "Xiu Li"], "title": "PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning", "comment": null, "summary": "Offline imitation learning (offline IL) enables training effective policies without requiring explicit reward annotations. Recent approaches attempt to estimate rewards for unlabeled datasets using a small set of expert demonstrations. However, these methods often assume that the similarity between a trajectory and an expert demonstration is positively correlated with the reward, which oversimplifies the underlying reward structure. We propose PROF, a novel framework that leverages large language models (LLMs) to generate and improve executable reward function codes from natural language descriptions and a single expert trajectory. We propose Reward Preference Ranking (RPR), a novel reward function quality assessment and ranking strategy without requiring environment interactions or RL training. RPR calculates the dominance scores of the reward functions, where higher scores indicate better alignment with expert preferences. By alternating between RPR and text-based gradient optimization, PROF fully automates the selection and refinement of optimal reward functions for downstream policy learning. Empirical results on D4RL demonstrate that PROF surpasses or matches recent strong baselines across numerous datasets and domains, highlighting the effectiveness of our approach.", "AI": {"tldr": "PROF\u6846\u67b6\u5229\u7528LLM\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u548c\u5355\u4e2a\u4e13\u5bb6\u8f68\u8ff9\u751f\u6210\u53ef\u6267\u884c\u7684\u5956\u52b1\u51fd\u6570\u4ee3\u7801\uff0c\u901a\u8fc7\u5956\u52b1\u504f\u597d\u6392\u5e8f(RPR)\u81ea\u52a8\u8bc4\u4f30\u548c\u4f18\u5316\u5956\u52b1\u51fd\u6570\uff0c\u65e0\u9700\u73af\u5883\u4ea4\u4e92\u6216RL\u8bad\u7ec3\uff0c\u5728D4RL\u57fa\u51c6\u4e0a\u8d85\u8d8a\u6216\u5339\u914d\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u8f68\u8ff9\u4e0e\u4e13\u5bb6\u6f14\u793a\u7684\u76f8\u4f3c\u5ea6\u4e0e\u5956\u52b1\u6b63\u76f8\u5173\uff0c\u8fd9\u8fc7\u5ea6\u7b80\u5316\u4e86\u5956\u52b1\u7ed3\u6784\u3002\u9700\u8981\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u4f30\u8ba1\u672a\u6807\u8bb0\u6570\u636e\u96c6\u7684\u5956\u52b1\u3002", "method": "\u63d0\u51faPROF\u6846\u67b6\uff1a1) \u4f7f\u7528LLM\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u548c\u4e13\u5bb6\u8f68\u8ff9\u751f\u6210\u5956\u52b1\u51fd\u6570\u4ee3\u7801\uff1b2) \u63d0\u51faRPR\u7b56\u7565\u8bc4\u4f30\u5956\u52b1\u51fd\u6570\u8d28\u91cf\uff0c\u8ba1\u7b97\u652f\u914d\u5206\u6570\uff1b3) \u901a\u8fc7RPR\u548c\u57fa\u4e8e\u6587\u672c\u7684\u68af\u5ea6\u4f18\u5316\u4ea4\u66ff\u8fdb\u884c\u5956\u52b1\u51fd\u6570\u9009\u62e9\u548c\u4f18\u5316\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPROF\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u9886\u57df\u4e0a\u8d85\u8d8a\u6216\u5339\u914d\u4e86\u6700\u8fd1\u7684\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "PROF\u901a\u8fc7\u7ed3\u5408LLM\u548c\u81ea\u52a8\u5956\u52b1\u51fd\u6570\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u4e2d\u5956\u52b1\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u73af\u5883\u4ea4\u4e92\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.14098", "categories": ["cs.AI", "cs.MA", "cs.SI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.14098", "abs": "https://arxiv.org/abs/2511.14098", "authors": ["Adit Jain", "Vikram Krishnamurthy", "Yiming Zhang"], "title": "Collaborative QA using Interacting LLMs. Impact of Network Structure, Node Capability and Distributed Data", "comment": null, "summary": "In this paper, we model and analyze how a network of interacting LLMs performs collaborative question-answering (CQA) in order to estimate a ground truth given a distributed set of documents. This problem is interesting because LLMs often hallucinate when direct evidence to answer a question is lacking, and these effects become more pronounced in a network of interacting LLMs. The hallucination spreads, causing previously accurate LLMs to hallucinate. We study interacting LLMs and their hallucination by combining novel ideas of mean-field dynamics (MFD) from network science and the randomized utility model from economics to construct a useful generative model. We model the LLM with a latent state that indicates if it is truthful or not with respect to the ground truth, and extend a tractable analytical model considering an MFD to model the diffusion of information in a directed network of LLMs. To specify the probabilities that govern the dynamics of the MFD, we propose a randomized utility model. For a network of LLMs, where each LLM has two possible latent states, we posit sufficient conditions for the existence and uniqueness of a fixed point and analyze the behavior of the fixed point in terms of the incentive (e.g., test-time compute) given to individual LLMs. We experimentally study and analyze the behavior of a network of $100$ open-source LLMs with respect to data heterogeneity, node capability, network structure, and sensitivity to framing on multiple semi-synthetic datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76LLM\u7f51\u7edc\u4e2d\u7684\u534f\u4f5c\u95ee\u7b54\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u5e73\u5747\u573a\u52a8\u529b\u5b66\u548c\u968f\u673a\u6548\u7528\u6a21\u578b\u6765\u5206\u6790\u5e7b\u89c9\u4f20\u64ad\u73b0\u8c61\u3002", "motivation": "LLM\u5728\u7f3a\u4e4f\u76f4\u63a5\u8bc1\u636e\u65f6\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u8fd9\u79cd\u6548\u5e94\u5728\u4ea4\u4e92\u7684LLM\u7f51\u7edc\u4e2d\u66f4\u52a0\u660e\u663e\uff0c\u5bfc\u81f4\u539f\u672c\u51c6\u786e\u7684LLM\u4e5f\u5f00\u59cb\u4ea7\u751f\u5e7b\u89c9\u3002", "method": "\u7ed3\u5408\u7f51\u7edc\u79d1\u5b66\u4e2d\u7684\u5e73\u5747\u573a\u52a8\u529b\u5b66\u548c\u7ecf\u6d4e\u5b66\u4e2d\u7684\u968f\u673a\u6548\u7528\u6a21\u578b\uff0c\u6784\u5efa\u751f\u6210\u6a21\u578b\u6765\u5206\u6790LLM\u7f51\u7edc\u4e2d\u7684\u4fe1\u606f\u6269\u6563\u3002", "result": "\u5efa\u7acb\u4e86LLM\u7f51\u7edc\u7684\u52a8\u6001\u6a21\u578b\uff0c\u5206\u6790\u4e86\u56fa\u5b9a\u70b9\u7684\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\u6761\u4ef6\uff0c\u5e76\u5728100\u4e2a\u5f00\u6e90LLM\u7f51\u7edc\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5206\u6790LLM\u7f51\u7edc\u4e2d\u5e7b\u89c9\u4f20\u64ad\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u7406\u89e3\u548c\u63a7\u5236LLM\u534f\u4f5c\u7cfb\u7edf\u4e2d\u7684\u4fe1\u606f\u8d28\u91cf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2511.14101", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14101", "abs": "https://arxiv.org/abs/2511.14101", "authors": ["Xinpeng Chen", "Xiaofeng Han", "Kaihao Zhang", "Guochao Ren", "Yujie Wang", "Wenhao Cao", "Yang Zhou", "Jianfeng Lu", "Zhenbo Song"], "title": "APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design", "comment": null, "summary": "Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51faAPD-agents\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u79fb\u52a8\u5e94\u7528\u9875\u9762\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\uff0c\u901a\u8fc7\u591a\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u5b8c\u6210\u4ece\u7528\u6237\u63cf\u8ff0\u5230\u9875\u9762\u5e03\u5c40\u7684\u751f\u6210\u3002", "motivation": "\u79fb\u52a8\u5e94\u7528\u9875\u9762\u5e03\u5c40\u8bbe\u8ba1\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u6280\u80fd\uff0c\u73b0\u6709\u8bbe\u8ba1\u8f6f\u4ef6\u4f7f\u7528\u590d\u6742\u4e14\u8de8\u9875\u9762\u534f\u4f5c\u6548\u7387\u4f4e\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u8bbe\u8ba1\u6548\u7387\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u62ec\u7f16\u6392\u667a\u80fd\u4f53\u3001\u8bed\u4e49\u89e3\u6790\u667a\u80fd\u4f53\u3001\u4e3b\u5e03\u5c40\u667a\u80fd\u4f53\u3001\u6a21\u677f\u68c0\u7d22\u667a\u80fd\u4f53\u548c\u9012\u5f52\u7ec4\u4ef6\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u534f\u4f5c\u5c06\u7528\u6237\u63cf\u8ff0\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u5e03\u5c40\u8bbe\u8ba1\u3002", "result": "\u5728RICO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAPD-agents\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8be5\u6846\u67b6\u5145\u5206\u5229\u7528\u4e86\u5927\u6a21\u578b\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u81ea\u52a8\u534f\u4f5c\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u79fb\u52a8\u5e94\u7528\u9875\u9762\u7684\u81ea\u52a8\u5316\u8bbe\u8ba1\u3002", "topic": "swe application"}}
{"id": "2511.14130", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.14130", "abs": "https://arxiv.org/abs/2511.14130", "authors": ["Chun Chet Ng", "Jia Yu Lim", "Wei Zeng Low"], "title": "PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval", "comment": "3rd-place solution for the ACM ICAIF 2025 Agentic Retrieval Grand Challenge", "summary": "With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.", "AI": {"tldr": "\u63d0\u51fa\u4e86PRISM\u6846\u67b6\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u91d1\u878d\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\uff0c\u5728FinAgentBench\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.71818\u7684NDCG@5\u5206\u6570\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u91d1\u878d\u4fe1\u606f\u68c0\u7d22\u6210\u4e3a\u5173\u952e\u5de5\u4e1a\u5e94\u7528\u3002\u4ece\u5197\u957f\u7684\u91d1\u878d\u6587\u4ef6\u4e2d\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u5bf9\u4e8e\u8fd0\u8425\u548c\u5206\u6790\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "PRISM\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u7cbe\u70bc\u7684\u7cfb\u7edf\u63d0\u793a\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u8f7b\u91cf\u7ea7\u591a\u4ee3\u7406\u7cfb\u7edf\u3002\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u7cbe\u786e\u4efb\u52a1\u6307\u4ee4\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u8bed\u4e49\u76f8\u5173\u7684\u5c11\u6837\u672c\u793a\u4f8b\uff0c\u591a\u4ee3\u7406\u7cfb\u7edf\u5efa\u6a21\u534f\u8c03\u8bc4\u5206\u884c\u4e3a\u3002", "result": "\u5728\u53d7\u9650\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u5230NDCG@5\u4e3a0.71818\uff0c\u8bc1\u660e\u4e86PRISM\u5728\u751f\u4ea7\u89c4\u6a21\u91d1\u878d\u68c0\u7d22\u4e2d\u7684\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "PRISM\u7684\u6a21\u5757\u5316\u3001\u4ec5\u63a8\u7406\u8bbe\u8ba1\u4f7f\u5176\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7528\u4f8b\uff0c\u6e90\u4ee3\u7801\u5df2\u53d1\u5e03\u3002", "topic": "agent analysis"}}
{"id": "2511.14131", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14131", "abs": "https://arxiv.org/abs/2511.14131", "authors": ["Yu Zhong", "Zihao Zhang", "Rui Zhang", "Lingdong Huang", "Haihan Gao", "Shuo Wang", "Da Li", "Ruijian Han", "Jiaming Guo", "Shaohui Peng", "Di Huang", "Yunji Chen"], "title": "Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation", "comment": null, "summary": "Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.", "AI": {"tldr": "\u63d0\u51faR3\u53cc\u8fc7\u7a0b\u601d\u7ef4\u6846\u67b6\uff0c\u5c06LLM\u7684\u6cdb\u5316\u80fd\u529b\u4e0eVLN\u4e13\u4e1a\u6a21\u578b\u7ed3\u5408\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6027\u80fd", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u5728VLN\u4efb\u52a1\u4e2d\u4ecd\u4e0e\u9886\u57df\u4e13\u5bb6\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u4e14LLM\u96be\u4ee5\u7cbe\u786e\u7406\u89e3\u771f\u5b9e\u4e16\u754c\u7a7a\u95f4\u5173\u7cfb\uff0c\u540c\u65f6\u5e26\u6765\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf", "method": "R3\u6846\u67b6\u5305\u542bRunner\uff08\u8f7b\u91cf\u7ea7Transformer\u4e13\u5bb6\u6a21\u578b\uff09\u3001Ruminator\uff08\u591a\u6a21\u6001LLM\u9aa8\u5e72+\u601d\u7ef4\u94fe\u63d0\u793a\uff09\u548cRegulator\uff08\u6839\u636e\u4e09\u4e2a\u6807\u51c6\u76d1\u63a7\u5bfc\u822a\u8fdb\u5ea6\u5e76\u63a7\u5236\u601d\u7ef4\u6a21\u5f0f\uff09", "result": "\u5728REVERIE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPL\u548cRGSPL\u5206\u522b\u8d85\u8fc7\u6700\u5148\u8fdb\u65b9\u6cd53.28%\u548c3.30%", "conclusion": "R3\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684VLN\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u5bfc\u822a\u6027\u80fd", "topic": "agent analysis"}}
{"id": "2511.14136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14136", "abs": "https://arxiv.org/abs/2511.14136", "authors": ["Sushant Mehta"], "title": "Beyond Accuracy: A Multi-Dimensional Framework for Evaluating Enterprise Agentic AI Systems", "comment": null, "summary": "Current agentic AI benchmarks predominantly evaluate task completion accuracy, while overlooking critical enterprise requirements such as cost-efficiency, reliability, and operational stability. Through systematic analysis of 12 main benchmarks and empirical evaluation of state-of-the-art agents, we identify three fundamental limitations: (1) absence of cost-controlled evaluation leading to 50x cost variations for similar precision, (2) inadequate reliability assessment where agent performance drops from 60\\% (single run) to 25\\% (8-run consistency), and (3) missing multidimensional metrics for security, latency, and policy compliance. We propose \\textbf{CLEAR} (Cost, Latency, Efficacy, Assurance, Reliability), a holistic evaluation framework specifically designed for enterprise deployment. Evaluation of six leading agents on 300 enterprise tasks demonstrates that optimizing for accuracy alone yields agents 4.4-10.8x more expensive than cost-aware alternatives with comparable performance. Expert evaluation (N=15) confirms that CLEAR better predicts production success (correlation $\u03c1=0.83$) compared to accuracy-only evaluation ($\u03c1=0.41$).", "AI": {"tldr": "\u63d0\u51faCLEAR\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709AI\u4ee3\u7406\u57fa\u51c6\u5728\u6210\u672c\u6548\u7387\u3001\u53ef\u9760\u6027\u548c\u64cd\u4f5c\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u6307\u6807\u66f4\u597d\u5730\u9884\u6d4b\u4f01\u4e1a\u90e8\u7f72\u6210\u529f\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u5b8c\u6210\u51c6\u786e\u7387\uff0c\u800c\u5ffd\u89c6\u4e86\u4f01\u4e1a\u90e8\u7f72\u6240\u9700\u7684\u5173\u952e\u8981\u6c42\uff0c\u5982\u6210\u672c\u6548\u7387\u3001\u53ef\u9760\u6027\u548c\u64cd\u4f5c\u7a33\u5b9a\u6027\u3002", "method": "\u7cfb\u7edf\u5206\u679012\u4e2a\u4e3b\u8981\u57fa\u51c6\uff0c\u63d0\u51faCLEAR\uff08\u6210\u672c\u3001\u5ef6\u8fdf\u3001\u6548\u80fd\u3001\u4fdd\u8bc1\u3001\u53ef\u9760\u6027\uff09\u6846\u67b6\uff0c\u5728300\u4e2a\u4f01\u4e1a\u4efb\u52a1\u4e0a\u8bc4\u4f306\u4e2a\u9886\u5148\u4ee3\u7406\u3002", "result": "\u4ec5\u4f18\u5316\u51c6\u786e\u7387\u7684\u4ee3\u7406\u6bd4\u6210\u672c\u611f\u77e5\u66ff\u4ee3\u65b9\u6848\u8d354.4-10.8\u500d\uff1bCLEAR\u6846\u67b6\u9884\u6d4b\u751f\u4ea7\u6210\u529f\u7684\u76f8\u5173\u6027\uff08\u03c1=0.83\uff09\u663e\u8457\u9ad8\u4e8e\u4ec5\u57fa\u4e8e\u51c6\u786e\u7387\u7684\u8bc4\u4f30\uff08\u03c1=0.41\uff09\u3002", "conclusion": "CLEAR\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u4f01\u4e1aAI\u4ee3\u7406\u8bc4\u4f30\u6807\u51c6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6307\u5bfc\u5b9e\u9645\u90e8\u7f72\u51b3\u7b56\u3002", "topic": "agent analysis"}}
{"id": "2511.13788", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.13788", "abs": "https://arxiv.org/abs/2511.13788", "authors": ["Samuel Nathanson", "Rebecca Williams", "Cynthia Matuszek"], "title": "Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments", "comment": "19 pages, 6 figures, 3 tables", "summary": "Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p < 0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u591a\u8f6e\u5bf9\u6297\u6027\u4ea4\u4e92\u7cfb\u7edf\u6027\u5730\u8d8a\u72f1\u5c0f\u578b\u6a21\u578b\uff0c\u6a21\u578b\u5927\u5c0f\u6bd4\u4f8b\u4e0e\u6709\u5bb3\u884c\u4e3a\u53ef\u80fd\u6027\u5448\u6b63\u76f8\u5173\uff0c\u653b\u51fb\u8005\u884c\u4e3a\u591a\u6837\u6027\u6bd4\u76ee\u6807\u6613\u53d7\u6027\u5bf9\u5bf9\u6297\u7ed3\u679c\u5f71\u54cd\u66f4\u5927\u3002", "motivation": "\u968f\u7740LLMs\u5728\u591a\u4ee3\u7406\u548c\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u9700\u8981\u4e86\u89e3\u6a21\u578b\u5728\u5bf9\u6297\u6027\u4ea4\u4e92\u4e2d\u7684\u8106\u5f31\u6027\u5982\u4f55\u6269\u5c55\uff0c\u7279\u522b\u662f\u5927\u6a21\u578b\u662f\u5426\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u8d8a\u72f1\u5c0f\u6a21\u578b\u3002", "method": "\u4f7f\u7528JailbreakBench\u6807\u51c6\u5316\u5bf9\u6297\u4efb\u52a1\uff0c\u6a21\u62df\u8d85\u8fc76000\u6b21\u591a\u8f6e\u653b\u51fb\u8005-\u76ee\u6807\u4ea4\u4e92\uff0c\u6db5\u76d6\u4e3b\u8981LLM\u5bb6\u65cf\u548c\u89c4\u6a21\uff080.6B-120B\u53c2\u6570\uff09\uff0c\u901a\u8fc7\u4e09\u4e2a\u72ec\u7acbLLM\u6cd5\u5b98\u8bc4\u4f30\u4f24\u5bb3\u5206\u6570\u548c\u62d2\u7edd\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u5e73\u5747\u4f24\u5bb3\u5206\u6570\u4e0e\u653b\u51fb\u8005-\u76ee\u6807\u5927\u5c0f\u6bd4\u4f8b\u7684\u5bf9\u6570\u5448\u5f3a\u6b63\u76f8\u5173\uff08Pearson r=0.51\uff09\uff0c\u653b\u51fb\u8005\u65b9\u884c\u4e3a\u591a\u6837\u6027\u5bf9\u5bf9\u6297\u7ed3\u679c\u5f71\u54cd\u66f4\u5927\uff0c\u653b\u51fb\u8005\u62d2\u7edd\u9891\u7387\u4e0e\u4f24\u5bb3\u5448\u5f3a\u8d1f\u76f8\u5173\uff08rho=-0.93\uff09\u3002", "conclusion": "\u5927\u5c0f\u4e0d\u5bf9\u79f0\u6027\u5f71\u54cd\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4e3a\u5bf9\u6297\u6027\u6269\u5c55\u6a21\u5f0f\u63d0\u4f9b\u4e86\u63a2\u7d22\u6027\u8bc1\u636e\uff0c\u9700\u8981\u66f4\u53d7\u63a7\u5730\u7814\u7a76\u6a21\u578b\u95f4\u5bf9\u9f50\u548c\u5b89\u5168\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.14214", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14214", "abs": "https://arxiv.org/abs/2511.14214", "authors": ["Pattaraphon Kenny Wongchamcharoen", "Paul Glasserman"], "title": "Do Large Language Models (LLMs) Understand Chronology?", "comment": "47 pages", "summary": "Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6d4b\u8bd5\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u987a\u5e8f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\uff0c\u6a21\u578b\u5728\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u65f6\u95f4\u7ebf\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4f46\u5206\u914d\u663e\u5f0f\u63a8\u7406\u9884\u7b97\u53ef\u4ee5\u663e\u8457\u6539\u5584\u6027\u80fd\u3002", "motivation": "\u6d4b\u8bd5LLMs\u662f\u5426\u771f\u6b63\u7406\u89e3\u65f6\u95f4\u987a\u5e8f\uff0c\u8fd9\u5bf9\u91d1\u878d\u548c\u7ecf\u6d4e\u5e94\u7528\u4e2d\u907f\u514d\u524d\u77bb\u6027\u504f\u5dee\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u65f6\u95f4\u987a\u5e8f\u4efb\u52a1\uff0c\u5305\u62ec\u65f6\u95f4\u6392\u5e8f\u3001\u6761\u4ef6\u6392\u5e8f\u548c\u65f6\u4ee3\u9519\u8bef\u68c0\u6d4b\uff0c\u8bc4\u4f30\u591a\u4e2aLLM\u6a21\u578b\u5728\u4e0d\u540c\u63a8\u7406\u52aa\u529b\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\uff0c\u7cbe\u786e\u5339\u914d\u7387\u6025\u5267\u4e0b\u964d\uff0c\u4f46\u6392\u540d\u76f8\u5173\u6027\u4fdd\u6301\u8f83\u9ad8\uff1bGPT-5\u548c\u5e26\u6269\u5c55\u601d\u7ef4\u7684Claude-3.7\u5728\u6761\u4ef6\u6392\u5e8f\u4e2d\u8868\u73b0\u4f18\u5f02\uff1b\u65f6\u4ee3\u9519\u8bef\u68c0\u6d4b\u662f\u6700\u7b80\u5355\u7684\u4efb\u52a1\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u65f6\u95f4\u987a\u5e8f\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u5206\u914d\u663e\u5f0f\u63a8\u7406\u9884\u7b97\u53ef\u4ee5\u663e\u8457\u6539\u5584\u6027\u80fd\uff0c\u8fd9\u5bf9\u91d1\u878d\u5b9e\u65f6\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2511.13841", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13841", "abs": "https://arxiv.org/abs/2511.13841", "authors": ["Zelei Shao", "Vikranth Srivatsa", "Sanjana Srivastava", "Qingyang Wu", "Alpay Ariyak", "Xiaoxia Wu", "Ameen Patel", "Jue Wang", "Percy Liang", "Tri Dao", "Ce Zhang", "Yiying Zhang", "Ben Athiwaratkun", "Chenfeng Xu", "Junxiong Wang"], "title": "Beat the long tail: Distribution-Aware Speculative Decoding for RL Training", "comment": null, "summary": "Reinforcement learning(RL) post-training has become essential for aligning large language models (LLMs), yet its efficiency is increasingly constrained by the rollout phase, where long trajectories are generated token by token. We identify a major bottleneck:the long-tail distribution of rollout lengths, where a small fraction of long generations dominates wall clock time and a complementary opportunity; the availability of historical rollouts that reveal stable prompt level patterns across training epochs. Motivated by these observations, we propose DAS, a Distribution Aware Speculative decoding framework that accelerates RL rollouts without altering model outputs. DAS integrates two key ideas: an adaptive, nonparametric drafter built from recent rollouts using an incrementally maintained suffix tree, and a length aware speculation policy that allocates more aggressive draft budgets to long trajectories that dominate makespan. This design exploits rollout history to sustain acceptance while balancing base and token level costs during decoding. Experiments on math and code reasoning tasks show that DAS reduces rollout time up to 50% while preserving identical training curves, demonstrating that distribution-aware speculative decoding can significantly accelerate RL post training without compromising learning quality.", "AI": {"tldr": "DAS\u662f\u4e00\u4e2a\u5206\u5e03\u611f\u77e5\u7684\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5386\u53f2rollout\u6570\u636e\u6784\u5efa\u81ea\u9002\u5e94\u8349\u7a3f\u5668\u548c\u957f\u5ea6\u611f\u77e5\u7b56\u7565\uff0c\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u8f93\u51fa\u7684\u60c5\u51b5\u4e0b\u52a0\u901fRL\u8bad\u7ec3\u4e2d\u7684rollout\u9636\u6bb5\uff0c\u51cf\u5c11\u9ad8\u8fbe50%\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "RL\u540e\u8bad\u7ec3\u4e2drollout\u9636\u6bb5\u7684\u6548\u7387\u53d7\u5230\u957f\u8f68\u8ff9\u751f\u6210\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u957f\u5c3e\u5206\u5e03\u4e2d\u5c11\u6570\u957f\u751f\u6210\u5e8f\u5217\u5360\u636e\u4e86\u5927\u90e8\u5206\u8ba1\u7b97\u65f6\u95f4\u3002\u540c\u65f6\u53d1\u73b0\u5386\u53f2rollout\u6570\u636e\u4e2d\u5b58\u5728\u7a33\u5b9a\u7684\u63d0\u793a\u7ea7\u522b\u6a21\u5f0f\u53ef\u4ee5\u5229\u7528\u3002", "method": "\u63d0\u51faDAS\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u6700\u8fd1rollout\u6784\u5efa\u589e\u91cf\u7ef4\u62a4\u540e\u7f00\u6811\u7684\u81ea\u9002\u5e94\u975e\u53c2\u6570\u8349\u7a3f\u5668\uff1b2\uff09\u957f\u5ea6\u611f\u77e5\u63a8\u6d4b\u7b56\u7565\uff0c\u4e3a\u652f\u914d\u8ba1\u7b97\u65f6\u95f4\u7684\u957f\u8f68\u8ff9\u5206\u914d\u66f4\u6fc0\u8fdb\u7684\u8349\u7a3f\u9884\u7b97", "result": "\u5728\u6570\u5b66\u548c\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDAS\u5c06rollout\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe50%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u8bad\u7ec3\u66f2\u7ebf", "conclusion": "\u5206\u5e03\u611f\u77e5\u7684\u63a8\u6d4b\u89e3\u7801\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u5b66\u4e60\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u52a0\u901fRL\u540e\u8bad\u7ec3", "topic": "agentic reinforcement learning"}}
{"id": "2511.14258", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14258", "abs": "https://arxiv.org/abs/2511.14258", "authors": ["Hourun Zhu", "Yang Gao", "Wenlong Fei", "Jiawei Li", "Huashan Sun"], "title": "Entropy-Guided Reasoning Compression", "comment": "10pages, 4 figures", "summary": "Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u71b5\u5f15\u5bfc\u8bad\u7ec3\u6846\u67b6\u6765\u89e3\u51b3\u63a8\u7406\u6a21\u578b\u538b\u7f29\u4e2d\u7684\u71b5\u51b2\u7a81\u95ee\u9898\uff0c\u5c06\u63a8\u7406\u957f\u5ea6\u538b\u7f29\u5230\u539f\u59cb\u768420%\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u7387", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u8f93\u51fa\u8fc7\u957f\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u90e8\u7f72\u56f0\u96be\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u71b5\u51b2\u7a81\u73b0\u8c61", "method": "\u91c7\u7528\u71b5\u5f15\u5bfc\u8bad\u7ec3\u6846\u67b6\uff0c\u5f53\u71b5\u4e0b\u964d\u65f6\u5f15\u5bfc\u6a21\u578b\u8fdb\u884c\u7b80\u6d01\u63a8\u7406\uff0c\u5f53\u71b5\u4e0a\u5347\u65f6\u5728\u7d27\u51d1\u63a8\u7406\u6a21\u5f0f\u4e0b\u5f3a\u5316\u63a2\u7d22", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u65b9\u6cd5\u5c06\u63a8\u7406\u957f\u5ea6\u538b\u7f29\u5230\u539f\u59cb\u768420%\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u8d85\u8fc7\u57fa\u7ebf\u51c6\u786e\u7387", "conclusion": "\u71b5\u5f15\u5bfc\u8bad\u7ec3\u80fd\u6709\u6548\u89e3\u51b3\u63a8\u7406\u538b\u7f29\u4e2d\u7684\u71b5\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u63a8\u7406", "topic": "agent analysis"}}
{"id": "2511.14275", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14275", "abs": "https://arxiv.org/abs/2511.14275", "authors": ["Ante Wang", "Weizhi Ma", "Yang Liu"], "title": "Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space", "comment": null, "summary": "Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9884\u6d4b\u8bed\u8a00\u5316\u6982\u7387\u5206\u5e03\u6765\u589e\u5f3a\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9f13\u52b1LLM\u6df1\u5165\u63a8\u7406\uff0c\u8003\u8651\u6240\u6709\u5019\u9009\u7b54\u6848\u800c\u975e\u5355\u4e00\u731c\u6d4b\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u8bed\u8a00\u5316\u7f6e\u4fe1\u5ea6\uff0c\u4f46\u63a8\u7406\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4fc3\u8fdb\u6df1\u5ea6\u63a8\u7406\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u9884\u6d4b\u8bed\u8a00\u5316\u6982\u7387\u5206\u5e03\u6765\u9f13\u52b1\u6df1\u5165\u63a8\u7406\uff0c\u8981\u6c42LLM\u8003\u8651\u7b54\u6848\u7a7a\u95f4\u4e2d\u7684\u6240\u6709\u5019\u9009\u7b54\u6848\uff0c\u5e76\u4ed4\u7ec6\u5206\u914d\u7f6e\u4fe1\u5ea6\u5206\u6570\u4ee5\u6ee1\u8db3\u5206\u5e03\u8981\u6c42\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u65e0\u8bba\u7b54\u6848\u7a7a\u95f4\u662f\u5426\u5df2\u77e5\u3002\u5373\u4f7f\u5728\u5f3a\u5316\u5b66\u4e60\u540e\u4ecd\u4fdd\u6301\u4f18\u52bf\uff0c\u5176\u63a8\u7406\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u671f\u671b\u4e00\u81f4\u3002", "conclusion": "\u9884\u6d4b\u8bed\u8a00\u5316\u6982\u7387\u5206\u5e03\u662f\u6709\u6548\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u4fc3\u8fdb\u6df1\u5ea6\u63a8\u7406\u5e76\u4ea7\u751f\u4e0e\u4eba\u7c7b\u671f\u671b\u4e00\u81f4\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2511.14256", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.14256", "abs": "https://arxiv.org/abs/2511.14256", "authors": ["Yu Liu", "Xixun Lin", "Yanmin Shang", "Yangxi Li", "Shi Wang", "Yanan Cao"], "title": "PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models", "comment": "AAAI 2026, Long Paper, Oral", "summary": "Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a \"Retrieve-Prioritize-Reason\" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.", "AI": {"tldr": "\u63d0\u51faPathMind\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5f15\u5bfcLLM\u5173\u6ce8\u91cd\u8981\u63a8\u7406\u8def\u5f84\u6765\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u91c7\u7528\"\u68c0\u7d22-\u4f18\u5148-\u63a8\u7406\"\u8303\u5f0f\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u65e0\u5dee\u522b\u63d0\u53d6\u63a8\u7406\u8def\u5f84\u53ef\u80fd\u5f15\u5165\u566a\u58f0\u8bef\u5bfcLLM\uff1b2) \u52a8\u6001\u63a2\u7d22\u63a8\u7406\u8def\u5f84\u9700\u8981\u9ad8\u68c0\u7d22\u9700\u6c42\u548c\u9891\u7e41LLM\u8c03\u7528\u3002", "method": "\u91c7\u7528\"\u68c0\u7d22-\u4f18\u5148-\u63a8\u7406\"\u4e09\u9636\u6bb5\u8303\u5f0f\uff1a\u68c0\u7d22\u6a21\u5757\u83b7\u53d6\u67e5\u8be2\u5b50\u56fe\uff0c\u8def\u5f84\u4f18\u5148\u7ea7\u673a\u5236\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u8def\u5f84\u4f18\u5148\u7ea7\u51fd\u6570\u8bc6\u522b\u91cd\u8981\u63a8\u7406\u8def\u5f84\uff0c\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5305\u62ec\u4efb\u52a1\u7279\u5b9a\u6307\u4ee4\u8c03\u4f18\u548c\u8def\u5f84\u504f\u597d\u5bf9\u9f50\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPathMind\u59cb\u7ec8\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u63a8\u7406\u8def\u5f84\u5b9e\u73b0\u4e86\u66f4\u5c11\u7684\u8f93\u5165token\u548c\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "PathMind\u901a\u8fc7\u9009\u62e9\u6027\u8def\u5f84\u5f15\u5bfc\u6709\u6548\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3aLLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2511.14299", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.14299", "abs": "https://arxiv.org/abs/2511.14299", "authors": ["Xiaochuan Liu", "Yuanfeng Song", "Xiaoming Yin", "Xing Chen"], "title": "DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning", "comment": null, "summary": "In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.", "AI": {"tldr": "DataSage\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u3001\u591a\u89d2\u8272\u8fa9\u8bba\u673a\u5236\u548c\u591a\u8def\u5f84\u63a8\u7406\u6765\u89e3\u51b3\u73b0\u6709\u6570\u636e\u6d1e\u5bdf\u4ee3\u7406\u5728\u9886\u57df\u77e5\u8bc6\u5229\u7528\u4e0d\u8db3\u3001\u5206\u6790\u6df1\u5ea6\u6d45\u548c\u4ee3\u7801\u751f\u6210\u6613\u51fa\u9519\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u6570\u636e\u6d1e\u5bdf\u4ee3\u7406\u5728\u9886\u57df\u77e5\u8bc6\u5229\u7528\u3001\u5206\u6790\u6df1\u5ea6\u548c\u4ee3\u7801\u751f\u6210\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u81ea\u52a8\u5316\u6570\u636e\u6d1e\u5bdf\u53d1\u73b0\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faDataSage\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\u7279\u6027\uff1a\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u4e30\u5bcc\u5206\u6790\u4e0a\u4e0b\u6587\u3001\u591a\u89d2\u8272\u8fa9\u8bba\u673a\u5236\u6a21\u62df\u591a\u6837\u5316\u5206\u6790\u89c6\u89d2\u3001\u591a\u8def\u5f84\u63a8\u7406\u63d0\u9ad8\u4ee3\u7801\u548c\u6d1e\u5bdf\u751f\u6210\u51c6\u786e\u6027\u3002", "result": "\u5728InsightBench\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDataSage\u5728\u6240\u6709\u96be\u5ea6\u7ea7\u522b\u4e0a\u90fd\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u6d1e\u5bdf\u4ee3\u7406\u3002", "conclusion": "DataSage\u4e3a\u81ea\u52a8\u5316\u6570\u636e\u6d1e\u5bdf\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2511.14476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14476", "abs": "https://arxiv.org/abs/2511.14476", "authors": ["Dalia Ali", "Dora Zhao", "Allison Koenecke", "Orestis Papakyriakopoulos"], "title": "Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior", "comment": null, "summary": "Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728LLM\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u8003\u8651\u591a\u5143\u793e\u4f1a\u4ef7\u503c\u89c2\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u6536\u96c6\u6765\u81ea\u7f8e\u56fd\u548c\u5fb7\u56fd\u53c2\u4e0e\u8005\u768427,375\u4e2a\u8bc4\u5206\uff0c\u5206\u6790\u4e86\u4eba\u53e3\u7edf\u8ba1\u5dee\u5f02\u548c\u8bbe\u8ba1\u53c2\u6570\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524dLLM\u5bf9\u9f50\u51b3\u7b56\u5f80\u5f80\u5ffd\u89c6\u4eba\u7c7b\u793e\u4f1a\u7684\u591a\u6837\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5c06\u591a\u5143\u4ef7\u503c\u89c2\u7eb3\u5165\u5bf9\u9f50\u6d41\u7a0b\uff0c\u786e\u4fdd\u6a21\u578b\u65e2\u5b89\u5168\u53c8\u80fd\u516c\u5e73\u4ee3\u8868\u4e0d\u540c\u793e\u4f1a\u7fa4\u4f53\u3002", "method": "\u6536\u96c6\u6765\u81ea1,095\u540d\u7f8e\u56fd\u548c\u5fb7\u56fd\u53c2\u4e0e\u8005\u7684\u8bc4\u5206\u6570\u636e\uff0c\u6db5\u76d6\u6bd2\u6027\u3001\u60c5\u611f\u610f\u8bc6\u3001\u654f\u611f\u6027\u3001\u523b\u677f\u5370\u8c61\u504f\u89c1\u548c\u5e2e\u52a9\u6027\u4e94\u4e2a\u7ef4\u5ea6\u3002\u4f7f\u7528\u4e0d\u540c\u793e\u4f1a\u7fa4\u4f53\u7684\u504f\u597d\u5fae\u8c03\u591a\u4e2aLLM\u548cLRM\uff0c\u540c\u65f6\u53d8\u5316\u8bc4\u5206\u5c3a\u5ea6\u3001\u5206\u6b67\u5904\u7406\u65b9\u6cd5\u548c\u4f18\u5316\u6280\u672f\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u6027\u4eba\u53e3\u7edf\u8ba1\u6548\u5e94\uff1a\u7537\u6027\u53c2\u4e0e\u8005\u8bc4\u5206\u6bd2\u6027\u4f4e18%\uff1b\u4fdd\u5b88\u6d3e\u548c\u9ed1\u4eba\u53c2\u4e0e\u8005\u8bc4\u5206\u60c5\u611f\u610f\u8bc6\u5206\u522b\u9ad827.9%\u548c44%\u3002\u6280\u672f\u8bbe\u8ba1\u9009\u62e9\u5f71\u54cd\u663e\u8457\uff1a\u4fdd\u7559\u8bc4\u5206\u8005\u5206\u6b67\u6bd4\u591a\u6570\u6295\u7968\u6bd2\u6027\u51cf\u5c1153%\uff1b5\u70b9\u91cf\u8868\u6bd4\u4e8c\u5143\u683c\u5f0f\u51cf\u5c1122%\uff1bDPO\u5728\u591a\u503c\u4f18\u5316\u4e2d\u59cb\u7ec8\u4f18\u4e8eGRPO\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u56de\u7b54\u5173\u952e\u95ee\u9898\u63d0\u4f9b\u4e86\u521d\u6b65\u6b65\u9aa4\uff1a\u5bf9\u9f50\u5e94\u5982\u4f55\u5e73\u8861\u4e13\u5bb6\u9a71\u52a8\u548c\u7528\u6237\u9a71\u52a8\u4fe1\u53f7\uff0c\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u548c\u516c\u5e73\u4ee3\u8868\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.14439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14439", "abs": "https://arxiv.org/abs/2511.14439", "authors": ["Jinru Ding", "Lu Lu", "Chao Ding", "Mouxiao Bian", "Jiayuan Chen", "Renjie Lu", "Wenrao Pang", "Xiaoqin Wu", "Zhiqiang Liu", "Luyi Jiang", "Bing Han", "Yunqiu Wang", "Jie Xu"], "title": "MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents", "comment": null, "summary": "Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.", "AI": {"tldr": "MedBench v4\u662f\u4e00\u4e2a\u5168\u56fd\u6027\u7684\u533b\u7597AI\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b70\u591a\u4e07\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u4efb\u52a1\uff0c\u6db5\u76d624\u4e2a\u4e3b\u8981\u548c91\u4e2a\u6b21\u8981\u4e13\u4e1a\uff0c\u8bc4\u4f30\u4e8615\u4e2a\u524d\u6cbf\u6a21\u578b\u3002\u57fa\u7840LLM\u5e73\u5747\u5f97\u520654.1/100\uff0c\u5b89\u5168\u548c\u4f26\u7406\u8868\u73b0\u8f83\u5dee\uff1b\u591a\u6a21\u6001\u6a21\u578b\u8868\u73b0\u66f4\u5dee\uff1b\u57fa\u4e8e\u76f8\u540c\u9aa8\u5e72\u7684\u667a\u80fd\u4f53\u663e\u8457\u63d0\u5347\u6027\u80fd\u81f379.8/100\u3002", "motivation": "\u968f\u7740\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u3001\u591a\u6a21\u6001\u6a21\u578b\u548c\u667a\u80fd\u4f53\u7684\u53d1\u5c55\uff0c\u9700\u8981\u80fd\u591f\u53cd\u6620\u771f\u5b9e\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u548c\u5b89\u5168\u7ea6\u675f\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efa\u5305\u542b70\u591a\u4e07\u4e2a\u4e13\u5bb6\u7b56\u5212\u4efb\u52a1\u7684\u4e91\u57fa\u51c6\u6d4b\u8bd5\u57fa\u7840\u8bbe\u65bd\uff0c\u4efb\u52a1\u7ecf\u8fc7\u591a\u9636\u6bb5\u7cbe\u70bc\u548c\u591a\u8f6e\u4e34\u5e8a\u533b\u751f\u8bc4\u5ba1\uff0c\u5f00\u653e\u5f0f\u56de\u7b54\u7531\u6821\u51c6\u5230\u4eba\u7c7b\u8bc4\u5206\u7684LLM-as-a-judge\u8bc4\u5206\u3002", "result": "\u57fa\u7840LLM\u5e73\u5747\u5f97\u520654.1/100\uff08\u6700\u4f73\uff1aClaude Sonnet 4.5\uff0c62.5/100\uff09\uff0c\u5b89\u5168\u548c\u4f26\u7406\u5f97\u5206\u8f83\u4f4e\uff0818.4/100\uff09\uff1b\u591a\u6a21\u6001\u6a21\u578b\u8868\u73b0\u66f4\u5dee\uff08\u5e73\u574747.5/100\uff09\uff1b\u667a\u80fd\u4f53\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u5e73\u574779.8/100\uff09\u3002", "conclusion": "MedBench v4\u63ed\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u6301\u7eed\u5dee\u8ddd\uff0c\u540c\u65f6\u8868\u660e\u5177\u6709\u6cbb\u7406\u610f\u8bc6\u7684\u667a\u80fd\u4f53\u7f16\u6392\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4e34\u5e8a\u51c6\u5907\u5ea6\u800c\u4e0d\u727a\u7272\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.14650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14650", "abs": "https://arxiv.org/abs/2511.14650", "authors": ["Jingyi Jia", "Qinbin Li"], "title": "AutoTool: Efficient Tool Selection for Large Language Model Agents", "comment": "Accepted by AAAI 2026, 18 pages, 11 figures, Code: https://github.com/jiajingyyyyyy/AutoTool", "summary": "Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.", "AI": {"tldr": "AutoTool\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5de5\u5177\u4f7f\u7528\u60ef\u6027\u6765\u51cf\u5c11LLM\u4ee3\u7406\u7684\u63a8\u7406\u6210\u672c\uff0c\u65e0\u9700\u91cd\u590d\u8c03\u7528LLM\u8fdb\u884c\u5de5\u5177\u9009\u62e9\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u6846\u67b6\u5728\u5de5\u5177\u9009\u62e9\u65f6\u5b58\u5728\u9ad8\u63a8\u7406\u6210\u672c\u95ee\u9898\uff0c\u7279\u522b\u662f\u50cfReAct\u8fd9\u6837\u7684\u65b9\u6cd5\u9700\u8981\u91cd\u590d\u8c03\u7528LLM\u6765\u51b3\u5b9a\u6bcf\u4e2a\u6b65\u9aa4\u4f7f\u7528\u54ea\u4e2a\u5de5\u5177\u3002", "method": "\u6784\u5efa\u6709\u5411\u56fe\u4ece\u5386\u53f2\u4ee3\u7406\u8f68\u8ff9\u4e2d\u5b66\u4e60\u5de5\u5177\u4f7f\u7528\u6a21\u5f0f\uff0c\u8282\u70b9\u8868\u793a\u5de5\u5177\uff0c\u8fb9\u6355\u83b7\u8f6c\u79fb\u6982\u7387\uff0c\u5e76\u96c6\u6210\u53c2\u6570\u7ea7\u4fe1\u606f\u6765\u7ec6\u5316\u5de5\u5177\u8f93\u5165\u751f\u6210\u3002", "result": "\u5728\u591a\u6837\u5316\u4ee3\u7406\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAutoTool\u5c06\u63a8\u7406\u6210\u672c\u964d\u4f4e\u4e86\u9ad8\u8fbe30%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5c06\u7edf\u8ba1\u7ed3\u6784\u96c6\u6210\u5230LLM\u4ee3\u7406\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u7684\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2511.14460", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14460", "abs": "https://arxiv.org/abs/2511.14460", "authors": ["Mingyue Cheng", "Jie Ouyang", "Shuo Yu", "Ruiran Yan", "Yucong Luo", "Zirui Liu", "Daoyu Wang", "Qi Liu", "Enhong Chen"], "title": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning", "comment": "This paper serves as the technical report of the Agent-R1 project", "summary": "Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9LLM\u667a\u80fd\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6Agent-R1\uff0c\u901a\u8fc7\u6269\u5c55MDP\u6846\u67b6\u6765\u5b9a\u4e49LLM\u667a\u80fd\u4f53\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u5e76\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u4e2d\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9LLM\u667a\u80fd\u4f53\u80cc\u666f\u7684RL\u65b9\u6cd5\u63a2\u7d22\u548c\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u6269\u5c55\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6846\u67b6\u6765\u5b9a\u4e49LLM\u667a\u80fd\u4f53\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u5e76\u5f00\u53d1\u4e86\u6a21\u5757\u5316\u3001\u7075\u6d3b\u4e14\u7528\u6237\u53cb\u597d\u7684\u8bad\u7ec3\u6846\u67b6Agent-R1\u3002", "result": "\u5728\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u521d\u6b65\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u548c\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u667a\u80fd\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u65b9\u6cd5\u8bba\u548c\u5b9e\u7528\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.14730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14730", "abs": "https://arxiv.org/abs/2511.14730", "authors": ["Parya Dolatyabi", "Mahdi Khodayar"], "title": "Heterogeneous Multi-Agent Proximal Policy Optimization for Power Distribution System Restoration", "comment": "6 pages, 4 figures, TPEC 2025 Conference", "summary": "Restoring power distribution systems (PDS) after large-scale outages requires sequential switching operations that reconfigure feeder topology and coordinate distributed energy resources (DERs) under nonlinear constraints such as power balance, voltage limits, and thermal ratings. These challenges make conventional optimization and value-based RL approaches computationally inefficient and difficult to scale. This paper applies a Heterogeneous-Agent Reinforcement Learning (HARL) framework, instantiated through Heterogeneous-Agent Proximal Policy Optimization (HAPPO), to enable coordinated restoration across interconnected microgrids. Each agent controls a distinct microgrid with different loads, DER capacities, and switch counts, introducing practical structural heterogeneity. Decentralized actor policies are trained with a centralized critic to compute advantage values for stable on-policy updates. A physics-informed OpenDSS environment provides full power flow feedback and enforces operational limits via differentiable penalty signals rather than invalid action masking. The total DER generation is capped at 2400 kW, and each microgrid must satisfy local supply-demand feasibility. Experiments on the IEEE 123-bus and IEEE 8500-node systems show that HAPPO achieves faster convergence, higher restored power, and smoother multi-seed training than DQN, PPO, MAES, MAGDPG, MADQN, Mean-Field RL, and QMIX. Results demonstrate that incorporating microgrid-level heterogeneity within the HARL framework yields a scalable, stable, and constraint-aware solution for complex PDS restoration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u534f\u8c03\u591a\u4e2a\u5fae\u7535\u7f51\u7684\u7535\u529b\u5206\u914d\u7cfb\u7edf\u6062\u590d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u7ea6\u675f\u4e0b\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21\u505c\u7535\u540e\u6062\u590d\u7535\u529b\u5206\u914d\u7cfb\u7edf\u9700\u8981\u5904\u7406\u975e\u7ebf\u6027\u7ea6\u675f\uff08\u5982\u529f\u7387\u5e73\u8861\u3001\u7535\u538b\u9650\u5236\u3001\u70ed\u989d\u5b9a\u503c\uff09\uff0c\u4f20\u7edf\u4f18\u5316\u548c\u4ef7\u503c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u91c7\u7528\u5f02\u6784\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08HAPPO\uff09\u6846\u67b6\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u63a7\u5236\u5177\u6709\u4e0d\u540c\u8d1f\u8f7d\u3001DER\u5bb9\u91cf\u548c\u5f00\u5173\u6570\u91cf\u7684\u5fae\u7535\u7f51\uff0c\u4f7f\u7528\u5206\u6563\u5f0f\u884c\u52a8\u8005\u7b56\u7565\u548c\u96c6\u4e2d\u5f0f\u6279\u8bc4\u8005\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728IEEE 123\u603b\u7ebf\u548cIEEE 8500\u8282\u70b9\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHAPPO\u76f8\u6bd4\u5176\u4ed6\u65b9\u6cd5\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u6062\u590d\u529f\u7387\u548c\u66f4\u5e73\u6ed1\u7684\u591a\u79cd\u5b50\u8bad\u7ec3\u3002", "conclusion": "\u5c06\u5fae\u7535\u7f51\u7ea7\u5f02\u6784\u6027\u7eb3\u5165HARL\u6846\u67b6\u53ef\u4e3a\u590d\u6742\u7535\u529b\u5206\u914d\u7cfb\u7edf\u6062\u590d\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u7a33\u5b9a\u4e14\u7ea6\u675f\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.12443", "categories": ["cs.SE", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12443", "abs": "https://arxiv.org/abs/2509.12443", "authors": ["Sparsh Gupta", "Kamalavasan Kamalakkannan", "Maxim Moraru", "Galen Shipman", "Patrick Diehl"], "title": "From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI Workflow", "comment": "12 pages, 6 figures, 7 tables", "summary": "Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to modernize legacy codes for portability. Frameworks like Kokkos provide performance portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos porting demands significant expertise and time. Large language models (LLMs) have shown promise in source-to-source code generation, yet their use in fully autonomous workflows for translating and optimizing parallel code remains largely unexplored, especially for performance portability across diverse hardware. This paper presents an agentic AI workflow where specialized LLM \"agents\" collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into portable Kokkos C++ programs. Results show the pipeline modernizes a range of benchmark kernels, producing performance-portable Kokkos codes across hardware partitions. Paid OpenAI models such as GPT-5 and o4-mini-high executed the workflow for only a few U.S. dollars, generating optimized codes that surpassed Fortran baselines, whereas open-source models like Llama4-Maverick often failed to yield functional codes. This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos transformation and offers a pathway for autonomously modernizing legacy scientific applications to run portably and efficiently on diverse supercomputers. It further highlights the potential of LLM-driven agentic systems to perform structured, domain-specific reasoning tasks in scientific and systems-oriented applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684AI\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u5c06\u9057\u7559\u7684Fortran\u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u4e3a\u6027\u80fd\u53ef\u79fb\u690d\u7684Kokkos C++\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u5728\u5f02\u6784GPU\u52a0\u901f\u67b6\u6784\u4e0a\u7684\u73b0\u4ee3\u5316\u3002", "motivation": "\u968f\u7740\u9ad8\u6027\u80fd\u8ba1\u7b97\u5411\u5f02\u6784GPU\u52a0\u901f\u67b6\u6784\u8f6c\u53d8\uff0c\u8bb8\u591a\u52a0\u901f\u5668\u7f3a\u4e4f\u539f\u751fFortran\u7ed1\u5b9a\uff0c\u9700\u8981\u5c06\u9057\u7559\u7684Fortran\u4ee3\u7801\u5e93\u73b0\u4ee3\u5316\u4ee5\u5b9e\u73b0\u8de8\u786c\u4ef6\u5e73\u53f0\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6027\u3002", "method": "\u4f7f\u7528\u4e13\u95e8\u7684LLM\u4ee3\u7406\u534f\u4f5c\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec\u7ffb\u8bd1\u3001\u9a8c\u8bc1\u3001\u7f16\u8bd1\u3001\u8fd0\u884c\u3001\u6d4b\u8bd5\u3001\u8c03\u8bd5\u548c\u4f18\u5316Fortran\u5185\u6838\u4e3a\u53ef\u79fb\u690d\u7684Kokkos C++\u7a0b\u5e8f\u3002", "result": "\u8be5\u6d41\u6c34\u7ebf\u6210\u529f\u73b0\u4ee3\u5316\u4e86\u4e00\u7cfb\u5217\u57fa\u51c6\u5185\u6838\uff0c\u5728\u786c\u4ef6\u5206\u533a\u4e0a\u751f\u6210\u4e86\u6027\u80fd\u53ef\u79fb\u690d\u7684Kokkos\u4ee3\u7801\u3002\u4ed8\u8d39\u7684OpenAI\u6a21\u578b\u4ec5\u9700\u51e0\u7f8e\u5143\u5373\u53ef\u6267\u884c\u5de5\u4f5c\u6d41\uff0c\u751f\u6210\u7684\u4f18\u5316\u4ee3\u7801\u8d85\u8d8a\u4e86Fortran\u57fa\u7ebf\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u901a\u5e38\u65e0\u6cd5\u751f\u6210\u529f\u80fd\u4ee3\u7801\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u57fa\u4e8e\u4ee3\u7406\u7684AI\u5728Fortran\u5230Kokkos\u8f6c\u6362\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u81ea\u4e3b\u73b0\u4ee3\u5316\u9057\u7559\u79d1\u5b66\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u9014\u5f84\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u4e0d\u540c\u7684\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u53ef\u79fb\u690d\u4e14\u9ad8\u6548\u5730\u8fd0\u884c\u3002", "topic": "swe application"}}
{"id": "2511.14017", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14017", "abs": "https://arxiv.org/abs/2511.14017", "authors": ["Erum Mushtaq", "Anil Ramakrishna", "Satyapriya Krishna", "Sattvik Sahai", "Prasoon Goyal", "Kai-Wei Chang", "Tao Zhang", "Rahul Gupta"], "title": "From Narrow Unlearning to Emergent Misalignment: Causes, Consequences, and Containment in LLMs", "comment": null, "summary": "Recent work has shown that fine-tuning on insecure code data can trigger an emergent misalignment (EMA) phenomenon, where models generate malicious responses even to prompts unrelated to the original insecure code-writing task. Such cross-domain generalization of harmful behavior underscores the need for a deeper understanding of the algorithms, tasks, and datasets that induce emergent misalignment. In this work, we extend this study by demonstrating that emergent misalignment can also arise from narrow refusal unlearning in specific domains. We perform refusal unlearning on Cybersecurity and Safety concept, and evaluate EMA by monitoring refusal scores across seven responsible AI (RAI) domains, Cybersecurity, Safety, Toxicity, Bias, Sensitive Content, Medical/Legal, and Privacy. Our work shows that narrow domain unlearning can yield compliance responses for the targeted concept, however, it may also propagate EMA to unrelated domains. Among the two intervened concepts, Cybersecurity and Safety, we find that the safety concept can have larger EMA impact, i.e, causing lower refusal scores, across other unrelated domains such as bias. We observe this effect consistently across two model families, Mistral-7b-0.3v, and Qwen-7b-2.5. Further, we show that refusal unlearning augmented with cross-entropy loss function on a small set of retain data from the affected domains can largely, if not fully, restore alignment across the impacted domains while having lower refusal rate on the concept we perform unlearning on. To investigate the underlying causes of EMA, we analyze concept entanglements at the representation level via concept vectors. Our analysis reveals that concepts with higher representation similarity in earlier layers are more susceptible to EMA after intervention when the refusal stream is altered through targeted refusal unlearning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u7279\u5b9a\u9886\u57df\u8fdb\u884c\u62d2\u7edd\u9057\u5fd8\u5b66\u4e60\u53ef\u80fd\u5f15\u53d1\u8de8\u9886\u57df\u7684\u6709\u5bb3\u884c\u4e3a\u4f20\u64ad\uff0c\u5373\u65b0\u5174\u9519\u4f4d\u73b0\u8c61\u3002\u901a\u8fc7\u5206\u6790\u7f51\u7edc\u5b89\u5168\u548c\u5b89\u5168\u6982\u5ff5\uff0c\u53d1\u73b0\u5b89\u5168\u6982\u5ff5\u7684\u5e72\u9884\u5bf9\u5176\u4ed6\u9886\u57df\u5f71\u54cd\u66f4\u5927\uff0c\u4e14\u53ef\u901a\u8fc7\u4fdd\u7559\u6570\u636e\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u6062\u590d\u5bf9\u9f50\u3002", "motivation": "\u7406\u89e3\u5f15\u53d1\u65b0\u5174\u9519\u4f4d\u73b0\u8c61\u7684\u7b97\u6cd5\u3001\u4efb\u52a1\u548c\u6570\u636e\u96c6\uff0c\u63a2\u7d22\u62d2\u7edd\u9057\u5fd8\u5b66\u4e60\u5982\u4f55\u5bfc\u81f4\u8de8\u9886\u57df\u6709\u5bb3\u884c\u4e3a\u7684\u4f20\u64ad\u3002", "method": "\u5728\u7f51\u7edc\u5b89\u5168\u548c\u5b89\u5168\u6982\u5ff5\u4e0a\u8fdb\u884c\u62d2\u7edd\u9057\u5fd8\u5b66\u4e60\uff0c\u8bc4\u4f30\u4e03\u4e2a\u8d1f\u8d23\u4efbAI\u9886\u57df\u7684\u62d2\u7edd\u5206\u6570\uff0c\u5206\u6790\u6982\u5ff5\u5728\u8868\u793a\u5c42\u9762\u7684\u7ea0\u7f20\u5173\u7cfb\u3002", "result": "\u7a84\u9886\u57df\u9057\u5fd8\u5b66\u4e60\u53ef\u5728\u76ee\u6807\u6982\u5ff5\u4e0a\u4ea7\u751f\u5408\u89c4\u54cd\u5e94\uff0c\u4f46\u4f1a\u5c06\u65b0\u5174\u9519\u4f4d\u4f20\u64ad\u5230\u65e0\u5173\u9886\u57df\uff1b\u5b89\u5168\u6982\u5ff5\u5bf9\u5176\u4ed6\u9886\u57df\u5f71\u54cd\u66f4\u5927\uff1b\u901a\u8fc7\u4fdd\u7559\u6570\u636e\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u53ef\u6062\u590d\u5bf9\u9f50\u3002", "conclusion": "\u6982\u5ff5\u5728\u65e9\u671f\u8868\u793a\u5c42\u7684\u76f8\u4f3c\u6027\u8d8a\u9ad8\uff0c\u5728\u62d2\u7edd\u6d41\u901a\u8fc7\u76ee\u6807\u9057\u5fd8\u5b66\u4e60\u6539\u53d8\u540e\u8d8a\u5bb9\u6613\u53d7\u5230\u65b0\u5174\u9519\u4f4d\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2511.14631", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.14631", "abs": "https://arxiv.org/abs/2511.14631", "authors": ["Kahaan Gandhi", "Boris Bolliet", "Inigo Zubeldia"], "title": "Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities", "comment": null, "summary": "We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u80fd\u591f\u901a\u8fc7\u5c06\u56fe\u8868\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u68c0\u67e5\u70b9\uff0c\u5b9e\u65f6\u8bc4\u4f30\u548c\u7ea0\u6b63\u6570\u636e\u5206\u6790\u9519\u8bef\uff0c\u4ece\u800c\u63d0\u5347\u7aef\u5230\u7aef\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u9a71\u52a8\u79d1\u5b66\u53d1\u73b0\u65b9\u6cd5\u7f3a\u4e4f\u5b9e\u65f6\u9519\u8bef\u68c0\u6d4b\u548c\u7ea0\u6b63\u673a\u5236\uff0c\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u9519\u8bef\u96be\u4ee5\u88ab\u53d1\u73b0\u548c\u4fee\u590d\u3002", "method": "\u4f7f\u7528VLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff0c\u6839\u636e\u52a8\u6001\u751f\u6210\u7684\u9886\u57df\u7279\u5b9a\u8bc4\u5206\u6807\u51c6\u8bc4\u4f30\u56fe\u8868\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5b9e\u65f6\u7ea0\u6b63\u81ea\u8eab\u9519\u8bef\u5e76\u6307\u5bfc\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u3002", "result": "\u5728\u5b87\u5b99\u5b66\u548c\u5929\u4f53\u5316\u5b66\u6848\u4f8b\u4e2d\uff0c\u7cfb\u7edf\u80fd\u591f\u4ece\u9519\u8bef\u63a8\u7406\u8def\u5f84\u4e2d\u6062\u590d\u5e76\u9002\u5e94\u65b0\u6570\u636e\u96c6\u3002\u572810\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVLM\u589e\u5f3a\u7cfb\u7edf\u8fbe\u52300.7-0.8\u7684pass@1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u4ee3\u7801\u57fa\u51c6\uff080.2-0.3\uff09\u548c\u4ee3\u7801\u52a0\u6587\u672c\u57fa\u51c6\uff080.4-0.5\uff09\u3002", "conclusion": "VLM\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u53ef\u5ba1\u8ba1\u7684\u63a8\u7406\u8f68\u8ff9\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.14684", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14684", "abs": "https://arxiv.org/abs/2511.14684", "authors": ["Biaojie Zeng", "Min Zhang", "Juan Zhou", "Fengrui Liu", "Ruiyang Huang", "Xin Lin"], "title": "SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction", "comment": "13 pages, 3 figures", "summary": "Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \\textit{mainly focus on self-correction within the model}, which falls short of the ``teacher-style`` correction required in educational settings, \\textit{i.e.}, systematically guiding and revising a student's problem-solving process. To address this gap, we propose \\texttt{SMRC} (\\textit{\\underline{S}tudent \\underline{M}athematical \\underline{R}easoning \\underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \\texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \\textbf{solution accuracy} and \\textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \\texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at https://github.com/Mind-Lab-ECNU/SMRC.", "AI": {"tldr": "\u63d0\u51faSMRC\u65b9\u6cd5\uff0c\u5229\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u8fc7\u7a0b\u76d1\u7763\u6765\u81ea\u52a8\u68c0\u6d4b\u548c\u7ea0\u6b63\u5b66\u751f\u6570\u5b66\u63a8\u7406\u9519\u8bef\uff0c\u5728\u6559\u80b2\u573a\u666f\u4e2d\u5b9e\u73b0\u6559\u5e08\u5f0f\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u81ea\u6211\u7ea0\u9519\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6559\u80b2\u573a\u666f\u4e2d\u9700\u8981\u7cfb\u7edf\u6307\u5bfc\u5b66\u751f\u89e3\u9898\u8fc7\u7a0b\u7684\"\u6559\u5e08\u5f0f\"\u7ea0\u9519\u9700\u6c42\u3002", "method": "\u5c06\u5b66\u751f\u63a8\u7406\u5efa\u6a21\u4e3a\u591a\u6b65\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u5f15\u5165\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u63a2\u7d22\u6700\u4f18\u7ea0\u9519\u8def\u5f84\uff0c\u5229\u7528\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u548c\u6700\u7ec8\u7b54\u6848\u8bc4\u4f30\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u673a\u5236\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8fc7\u7a0b\u76d1\u7763\u3002", "result": "\u5728ProcessBench\u3001MR-GSM8K\u548c\u81ea\u5efaMSEB\u6570\u636e\u96c6\u4e0a\uff0cSMRC\u5728\u6548\u679c\u548c\u6574\u4f53\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SMRC\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7ea0\u6b63\u5b66\u751f\u6570\u5b66\u63a8\u7406\u9519\u8bef\uff0c\u5728\u6559\u80b2\u5e94\u7528\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2511.14135", "categories": ["cs.LG", "cs.AI", "cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.14135", "abs": "https://arxiv.org/abs/2511.14135", "authors": ["Promise Ekpo", "Saesha Agarwal", "Felix Grimm", "Lekan Molu", "Angelique Taylor"], "title": "Fair-GNE : Generalized Nash Equilibrium-Seeking Fairness in Multiagent Healthcare Automation", "comment": null, "summary": "Enforcing a fair workload allocation among multiple agents tasked to achieve an objective in learning enabled demand side healthcare worker settings is crucial for consistent and reliable performance at runtime. Existing multi-agent reinforcement learning (MARL) approaches steer fairness by shaping reward through post hoc orchestrations, leaving no certifiable self-enforceable fairness that is immutable by individual agents at runtime. Contextualized within a setting where each agent shares resources with others, we address this shortcoming with a learning enabled optimization scheme among self-interested decision makers whose individual actions affect those of other agents. This extends the problem to a generalized Nash equilibrium (GNE) game-theoretic framework where we steer group policy to a safe and locally efficient equilibrium, so that no agent can improve its utility function by unilaterally changing its decisions. Fair-GNE models MARL as a constrained generalized Nash equilibrium-seeking (GNE) game, prescribing an ideal equitable collective equilibrium within the problem's natural fabric. Our hypothesis is rigorously evaluated in our custom-designed high-fidelity resuscitation simulator. Across all our numerical experiments, Fair-GNE achieves significant improvement in workload balance over fixed-penalty baselines (0.89 vs.\\ 0.33 JFI, $p < 0.01$) while maintaining 86\\% task success, demonstrating statistically significant fairness gains through adaptive constraint enforcement. Our results communicate our formulations, evaluation metrics, and equilibrium-seeking innovations in large multi-agent learning-based healthcare systems with clarity and principled fairness enforcement.", "AI": {"tldr": "\u63d0\u51fa\u4e86Fair-GNE\u65b9\u6cd5\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5efa\u6a21\u4e3a\u7ea6\u675f\u5e7f\u4e49\u7eb3\u4ec0\u5747\u8861\u535a\u5f08\uff0c\u5728\u533b\u7597\u5de5\u4f5c\u8005\u8d44\u6e90\u5206\u914d\u573a\u666f\u4e2d\u5b9e\u73b0\u53ef\u8bc1\u660e\u7684\u516c\u5e73\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u3002", "motivation": "\u73b0\u6709MARL\u65b9\u6cd5\u901a\u8fc7\u4e8b\u540e\u7f16\u6392\u6765\u5851\u9020\u5956\u52b1\u4ee5\u5b9e\u73b0\u516c\u5e73\uff0c\u4f46\u7f3a\u4e4f\u8fd0\u884c\u65f6\u4e0d\u53ef\u53d8\u7684\u53ef\u8bc1\u660e\u81ea\u6267\u884c\u516c\u5e73\u6027\u3002\u5728\u533b\u7597\u5de5\u4f5c\u8005\u5171\u4eab\u8d44\u6e90\u7684\u573a\u666f\u4e2d\uff0c\u9700\u8981\u786e\u4fdd\u5de5\u4f5c\u8d1f\u8f7d\u516c\u5e73\u5206\u914d\u4ee5\u5b9e\u73b0\u4e00\u81f4\u53ef\u9760\u7684\u6027\u80fd\u3002", "method": "\u5c06MARL\u5efa\u6a21\u4e3a\u7ea6\u675f\u5e7f\u4e49\u7eb3\u4ec0\u5747\u8861\u5bfb\u6c42\u6e38\u620f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ea6\u675f\u6267\u884c\u5f15\u5bfc\u7fa4\u4f53\u7b56\u7565\u8fbe\u5230\u5b89\u5168\u548c\u5c40\u90e8\u6709\u6548\u7684\u5747\u8861\u72b6\u6001\uff0c\u4f7f\u4efb\u4f55\u667a\u80fd\u4f53\u90fd\u65e0\u6cd5\u901a\u8fc7\u5355\u65b9\u9762\u6539\u53d8\u51b3\u7b56\u6765\u6539\u5584\u5176\u6548\u7528\u51fd\u6570\u3002", "result": "\u5728\u5b9a\u5236\u7684\u9ad8\u4fdd\u771f\u590d\u82cf\u6a21\u62df\u5668\u4e2d\uff0cFair-GNE\u76f8\u6bd4\u56fa\u5b9a\u60e9\u7f5a\u57fa\u7ebf\u5728\u5de5\u4f5c\u8d1f\u8f7d\u5e73\u8861\u65b9\u9762\u663e\u8457\u6539\u5584\uff080.89 vs 0.33 JFI\uff0cp < 0.01\uff09\uff0c\u540c\u65f6\u4fdd\u630186%\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "Fair-GNE\u901a\u8fc7\u81ea\u9002\u5e94\u7ea6\u675f\u6267\u884c\u5b9e\u73b0\u4e86\u7edf\u8ba1\u663e\u8457\u7684\u516c\u5e73\u6027\u63d0\u5347\uff0c\u4e3a\u5927\u578b\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u578b\u533b\u7597\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u516c\u5f0f\u3001\u8bc4\u4f30\u6307\u6807\u548c\u5747\u8861\u5bfb\u6c42\u521b\u65b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.14195", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.14195", "abs": "https://arxiv.org/abs/2511.14195", "authors": ["Zheyu Lin", "Jirui Yang", "Hengqi Guo", "Yubing Bao", "Yao Guan"], "title": "N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator", "comment": null, "summary": "Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.", "AI": {"tldr": "\u63d0\u51faN-GLARE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6f5c\u5728\u8868\u793a\u7684APT\u8f68\u8ff9\u548cJSS\u6307\u6807\uff0c\u5728\u65e0\u9700\u751f\u6210\u6587\u672c\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30LLM\u7684\u5b89\u5168\u6027\u9c81\u68d2\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\u4f9d\u8d56\u5728\u7ebf\u751f\u6210\u548c\u9ed1\u76d2\u8f93\u51fa\u5206\u6790\uff0c\u6210\u672c\u9ad8\u4e14\u53cd\u9988\u5ef6\u8fdf\uff0c\u4e0d\u9002\u5408\u65b0\u6a21\u578b\u8bad\u7ec3\u7684\u654f\u6377\u8bca\u65ad\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u6f5c\u5728\u8868\u793a\u5206\u6790APT\u8f68\u8ff9\uff0c\u5f15\u5165JSS\u6307\u6807\u6765\u8bc4\u4f30\u9690\u85cf\u5c42\u52a8\u6001\uff0c\u5b8c\u5168\u7ed5\u8fc7\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u572840\u591a\u4e2a\u6a21\u578b\u548c20\u79cd\u7ea2\u961f\u7b56\u7565\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cJSS\u6307\u6807\u4e0e\u7ea2\u961f\u6d4b\u8bd5\u5b89\u5168\u6392\u540d\u9ad8\u5ea6\u4e00\u81f4\uff0c\u6210\u672c\u964d\u4f4e99%\u4ee5\u4e0a\u3002", "conclusion": "N-GLARE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8f93\u51fa\u65e0\u5173\u8bc4\u4f30\u4ee3\u7406\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u8bca\u65ad\u3002", "topic": "agent analysis"}}
{"id": "2511.14220", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14220", "abs": "https://arxiv.org/abs/2511.14220", "authors": ["Yaniv Oren", "Joery A. de Vries", "Pascal R. van der Vaart", "Matthijs T. J. Spaan", "Wendelin B\u00f6hmer"], "title": "Parallelizing Tree Search with Twice Sequential Monte Carlo", "comment": null, "summary": "Model-based reinforcement learning (RL) methods that leverage search are responsible for many milestone breakthroughs in RL. Sequential Monte Carlo (SMC) recently emerged as an alternative to the Monte Carlo Tree Search (MCTS) algorithm which drove these breakthroughs. SMC is easier to parallelize and more suitable to GPU acceleration. However, it also suffers from large variance and path degeneracy which prevent it from scaling well with increased search depth, i.e., increased sequential compute. To address these problems, we introduce Twice Sequential Monte Carlo Tree Search (TSMCTS). Across discrete and continuous environments TSMCTS outperforms the SMC baseline as well as a popular modern version of MCTS. Through variance reduction and mitigation of path degeneracy, TSMCTS scales favorably with sequential compute while retaining the properties that make SMC natural to parallelize.", "AI": {"tldr": "\u63d0\u51fa\u4e86TSMCTS\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u65b9\u5dee\u548c\u7f13\u89e3\u8def\u5f84\u9000\u5316\u95ee\u9898\uff0c\u5728\u4fdd\u6301SMC\u5e76\u884c\u5316\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u641c\u7d22\u7684\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u5e8f\u5217\u8499\u7279\u5361\u6d1b(SMC)\u4f5c\u4e3aMCTS\u7684\u66ff\u4ee3\u65b9\u6848\u66f4\u6613\u4e8e\u5e76\u884c\u5316\u548cGPU\u52a0\u901f\uff0c\u4f46\u5b58\u5728\u65b9\u5dee\u5927\u548c\u8def\u5f84\u9000\u5316\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u6df1\u5ea6\u641c\u7d22\u4e2d\u7684\u6269\u5c55\u80fd\u529b\u3002", "method": "\u5f15\u5165TSMCTS\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u6b21\u5e8f\u5217\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6765\u51cf\u5c11\u65b9\u5dee\u548c\u7f13\u89e3\u8def\u5f84\u9000\u5316\u95ee\u9898\u3002", "result": "\u5728\u79bb\u6563\u548c\u8fde\u7eed\u73af\u5883\u4e2d\uff0cTSMCTS\u5747\u4f18\u4e8eSMC\u57fa\u7ebf\u548c\u73b0\u4ee3MCTS\u7248\u672c\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6269\u5c55\u5e8f\u5217\u8ba1\u7b97\u80fd\u529b\u3002", "conclusion": "TSMCTS\u5728\u4fdd\u6301SMC\u5e76\u884c\u5316\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u89e3\u51b3\u5176\u6838\u5fc3\u95ee\u9898\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.14262", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14262", "abs": "https://arxiv.org/abs/2511.14262", "authors": ["Yosuke Nishimoto", "Takashi Matsubara"], "title": "Object-Centric World Models for Causality-Aware Reinforcement Learning", "comment": "Accepted by AAAI-26", "summary": "World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \\emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.", "AI": {"tldr": "STICA\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684Transformer\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u89e3\u73af\u5883\u4e3a\u79bb\u6563\u5bf9\u8c61\u6765\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e16\u754c\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u590d\u5236\u9ad8\u7ef4\u3001\u975e\u5e73\u7a33\u3001\u591a\u5bf9\u8c61\u4ea4\u4e92\u7684\u590d\u6742\u73af\u5883\uff0c\u800c\u4eba\u7c7b\u901a\u8fc7\u5206\u89e3\u73af\u5883\u4e3a\u79bb\u6563\u5bf9\u8c61\u6765\u9ad8\u6548\u51b3\u7b56\u3002", "method": "\u4f7f\u7528\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684Transformer\u8868\u793a\u89c2\u6d4b\u4e3a\u5bf9\u8c61\u4ee4\u724c\uff0c\u9884\u6d4b\u4ee4\u724c\u7ea7\u52a8\u6001\u548c\u4ea4\u4e92\uff1b\u7b56\u7565\u548c\u4ef7\u503c\u7f51\u7edc\u4f30\u8ba1\u4ee4\u724c\u7ea7\u56e0\u679c\u5173\u7cfb\u5e76\u5728\u6ce8\u610f\u529b\u5c42\u4e2d\u4f7f\u7528\u3002", "result": "\u5728\u5bf9\u8c61\u4e30\u5bcc\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTICA\u5728\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u4e0a\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u3002", "conclusion": "\u5bf9\u8c61\u5206\u89e3\u548c\u56e0\u679c\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u590d\u6742\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.14584", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14584", "abs": "https://arxiv.org/abs/2511.14584", "authors": ["Ankush Kadu", "Ashwanth Krishnan"], "title": "ReflexGrad: Three-Way Synergistic Architecture for Zero-Shot Generalization in LLM Agents", "comment": null, "summary": "Enabling agents to learn from experience and generalize across diverse tasks without task-specific training remains a fundamental challenge in reinforcement learning and decision-making. While recent approaches have explored episodic memory (Reflexion), gradient-based prompt optimization (TextGrad),and hierarchical task decomposition independently, their potential for synergistic integration remains unexplored. We introduce ReflexGrad, a novel architecture that tightly couples three complementary mechanisms: (1) LLM-based hierarchical TODO decomposition for strategic planning, (2) history-aware causal reflection that analyzes recent action patterns to identify failure root causes and enable within-trial learning, and (3) gradient-based optimization for systematic improvement. Unlike prior work relying on few-shot demonstrations, our system achieves true zero-shot generalization through pure LLM semantic reasoning,requiring no task-specific examples, fine-tuning, or hardcoded similarity metrics. Evaluated on ALFWorld benchmark tasks, ReflexGrad demonstrates 67% zero-shot success rate on Trial 0 without any prior task experience or demonstrations, establishing effective performance on first exposure. Through empirical analysis, we identify the architectural mechanisms underlying stable convergence (zero action loops) and effective cross-task transfer (67% to 78% improvement).Our work demonstrates that synergistic integration of complementary learning mechanisms enables robust zero-shot generalization that approaches few-shot baselines from prior work.", "AI": {"tldr": "ReflexGrad\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u5206\u5c42\u4efb\u52a1\u5206\u89e3\u3001\u5386\u53f2\u611f\u77e5\u56e0\u679c\u53cd\u601d\u548c\u68af\u5ea6\u4f18\u5316\u7684\u65b0\u578b\u67b6\u6784\uff0c\u5728ALFWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8667%\u7684\u96f6\u6837\u672c\u6210\u529f\u7387\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6216\u6f14\u793a\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u4f53\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u5e76\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6cdb\u5316\u800c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u57fa\u672c\u6311\u6218\uff0c\u63a2\u7d22\u4e92\u8865\u5b66\u4e60\u673a\u5236\u7684\u534f\u540c\u6574\u5408\u3002", "method": "\u7ed3\u5408\u4e09\u79cd\u4e92\u8865\u673a\u5236\uff1aLLM\u9a71\u52a8\u7684\u5206\u5c42TODO\u5206\u89e3\u7528\u4e8e\u6218\u7565\u89c4\u5212\u3001\u5386\u53f2\u611f\u77e5\u56e0\u679c\u53cd\u601d\u5206\u6790\u8fd1\u671f\u884c\u52a8\u6a21\u5f0f\u4ee5\u8bc6\u522b\u5931\u8d25\u6839\u6e90\u3001\u68af\u5ea6\u4f18\u5316\u8fdb\u884c\u7cfb\u7edf\u6027\u6539\u8fdb\u3002", "result": "\u5728ALFWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9996\u6b21\u8bd5\u9a8c(Trial 0)\u5b9e\u73b067%\u7684\u96f6\u6837\u672c\u6210\u529f\u7387\uff0c\u65e0\u9700\u4efb\u4f55\u5148\u524d\u4efb\u52a1\u7ecf\u9a8c\u6216\u6f14\u793a\uff0c\u6709\u6548\u6027\u80fd\u5728\u9996\u6b21\u63a5\u89e6\u65f6\u5efa\u7acb\u3002", "conclusion": "\u4e92\u8865\u5b66\u4e60\u673a\u5236\u7684\u534f\u540c\u6574\u5408\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u7684\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u63a5\u8fd1\u5148\u524d\u5de5\u4f5c\u4e2d\u7684\u5c11\u6837\u672c\u57fa\u7ebf\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.14630", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14630", "abs": "https://arxiv.org/abs/2511.14630", "authors": ["Ivy Yuqian Yang", "David Yu Zhang"], "title": "Failure to Mix: Large language models struggle to answer according to desired probability distributions", "comment": "13 pages, 6 figures. Code and reproducibility package: https://github.com/BiostateAIresearch/failure-to-mix", "summary": "Scientific idea generation and selection requires exploration following a target probability distribution. In contrast, current AI benchmarks have objectively correct answers, and training large language models (LLMs) via reinforcement learning against these benchmarks discourages probabilistic exploration. Here, we conducted systematic experiments requesting LLMs to produce outputs following simple probabilistic distributions, and found that all modern LLMs tested grossly fail to follow the distributions. For example, requesting a binary output of \"1\" 49% of the time produces an answer of \"0\" nearly 100% of the time. This step function-like behavior of near-exclusively generating the output with marginally highest probability even overrules even strong in-built LLM biases.", "AI": {"tldr": "\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9075\u5faa\u7b80\u5355\u6982\u7387\u5206\u5e03\u751f\u6210\u8f93\u51fa\u65f6\u4e25\u91cd\u5931\u8d25\uff0c\u8868\u73b0\u51fa\u7c7b\u4f3c\u9636\u8dc3\u51fd\u6570\u7684\u884c\u4e3a\uff0c\u51e0\u4e4e\u53ea\u751f\u6210\u6982\u7387\u6700\u9ad8\u7684\u8f93\u51fa", "motivation": "\u79d1\u5b66\u601d\u60f3\u751f\u6210\u548c\u9009\u62e9\u9700\u8981\u9075\u5faa\u76ee\u6807\u6982\u7387\u5206\u5e03\u7684\u63a2\u7d22\uff0c\u800c\u5f53\u524d\u7684AI\u57fa\u51c6\u6d4b\u8bd5\u6709\u5ba2\u89c2\u6b63\u786e\u7b54\u6848\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u4f1a\u6291\u5236\u6982\u7387\u63a2\u7d22", "method": "\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u8981\u6c42LLM\u6309\u7167\u7b80\u5355\u6982\u7387\u5206\u5e03\u751f\u6210\u8f93\u51fa\uff0c\u6d4b\u8bd5\u5176\u9075\u5faa\u5206\u5e03\u7684\u80fd\u529b", "result": "\u6240\u6709\u6d4b\u8bd5\u7684\u73b0\u4ee3LLM\u90fd\u4e25\u91cd\u65e0\u6cd5\u9075\u5faa\u5206\u5e03\uff0c\u4f8b\u5982\u8981\u6c4249%\u65f6\u95f4\u8f93\u51fa\"1\"\u65f6\uff0c\u51e0\u4e4e100%\u8f93\u51fa\"0\"", "conclusion": "LLM\u8868\u73b0\u51fa\u9636\u8dc3\u51fd\u6570\u884c\u4e3a\uff0c\u5373\u4f7f\u6982\u7387\u5dee\u5f02\u5f88\u5c0f\u4e5f\u51e0\u4e4e\u53ea\u751f\u6210\u6982\u7387\u6700\u9ad8\u7684\u8f93\u51fa\uff0c\u751a\u81f3\u538b\u5012\u4e86\u5185\u7f6e\u7684\u5f3a\u504f\u89c1", "topic": "agent analysis"}}
{"id": "2511.14759", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14759", "abs": "https://arxiv.org/abs/2511.14759", "authors": ["Ali Amin", "Raichelle Aniceto", "Ashwin Balakrishna", "Kevin Black", "Ken Conley", "Grace Connors", "James Darpinian", "Karan Dhabalia", "Jared DiCarlo", "Danny Driess", "Michael Equi", "Adnan Esmail", "Yunhao Fang", "Chelsea Finn", "Catherine Glossop", "Thomas Godden", "Ivan Goryachev", "Lachy Groom", "Hunter Hancock", "Karol Hausman", "Gashon Hussein", "Brian Ichter", "Szymon Jakubczak", "Rowan Jen", "Tim Jones", "Ben Katz", "Liyiming Ke", "Chandra Kuchi", "Marinda Lamb", "Devin LeBlanc", "Sergey Levine", "Adrian Li-Bell", "Yao Lu", "Vishnu Mano", "Mohith Mothukuri", "Suraj Nair", "Karl Pertsch", "Allen Z. Ren", "Charvi Sharma", "Lucy Xiaoyang Shi", "Laura Smith", "Jost Tobias Springenberg", "Kyle Stachowicz", "Will Stoeckle", "Alex Swerdlow", "James Tanner", "Marcel Torne", "Quan Vuong", "Anna Walling", "Haohuan Wang", "Blake Williams", "Sukwon Yoo", "Lili Yu", "Ury Zhilinsky", "Zhiyuan Zhou"], "title": "$\u03c0^{*}_{0.6}$: a VLA That Learns From Experience", "comment": null, "summary": "We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $\u03c0^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $\u03c0^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.", "AI": {"tldr": "\u63d0\u51fa\u4e86RECAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u52bf\u6761\u4ef6\u7b56\u7565\u8fdb\u884cVLA\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u7ed3\u5408\u6f14\u793a\u6570\u636e\u3001\u5728\u7ebf\u6536\u96c6\u6570\u636e\u548c\u4e13\u5bb6\u5e72\u9884\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u6539\u8fdb\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u73b0\u5b9e\u4efb\u52a1\u3002", "method": "RECAP\u65b9\u6cd5\uff1a\u5148\u901a\u8fc7\u79bb\u7ebfRL\u9884\u8bad\u7ec3\u901a\u7528VLA\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\u5728\u7ebf\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u8fdb\u884c\u4efb\u52a1\u4e13\u4e1a\u5316\uff0c\u6574\u5408\u5f02\u6784\u6570\u636e\uff08\u6f14\u793a\u3001\u5728\u7ebf\u6536\u96c6\u3001\u4e13\u5bb6\u5e72\u9884\uff09\u3002", "result": "\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u80fd\u5728\u771f\u5b9e\u5bb6\u5ead\u4e2d\u53e0\u8863\u670d\u3001\u53ef\u9760\u5730\u7ec4\u88c5\u76d2\u5b50\u3001\u4f7f\u7528\u4e13\u4e1a\u5496\u5561\u673a\u5236\u4f5c\u6d53\u7f29\u5496\u5561\u3002\u5728\u6700\u56f0\u96be\u4efb\u52a1\u4e0a\uff0c\u4efb\u52a1\u541e\u5410\u91cf\u63d0\u9ad8\u4e00\u500d\u4ee5\u4e0a\uff0c\u6545\u969c\u7387\u51cf\u534a\u3002", "conclusion": "RECAP\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u5f02\u6784\u6570\u636e\u6574\u5408\u5b9e\u73b0\u6a21\u578b\u81ea\u6211\u6539\u8fdb\u7684\u53ef\u884c\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2511.c5efefe4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a922ec46c-59b352f8-f0ca-465d-b3c9-9cf5c6980258-000000/l94jDOvIZvUcEm0CepsT45t8R8SJ4FrWnAgMnE_rUIM=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a922ec46c-59b352f8-f0ca-465d-b3c9-9cf5c6980258-000000/l94jDOvIZvUcEm0CepsT45t8R8SJ4FrWnAgMnE_rUIM=431", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a922ec46c-59b352f8-f0ca-465d-b3c9-9cf5c6980258-000000/l94jDOvIZvUcEm0CepsT45t8R8SJ4FrWnAgMnE_rUIM=431", "summary": "Google Maps releases new AI tools that let you create interactive projects (2 minute read) Google Maps introduced AI tools, including a builder agent and an MCP server, enabling developers to create interactive projects using Maps data.", "source": "tldr", "AI": {"tldr": "Google Maps\u63a8\u51faAI\u5de5\u5177\uff0c\u5305\u62ec\u6784\u5efa\u5668\u4ee3\u7406\u548cMCP\u670d\u52a1\u5668\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u4f7f\u7528\u5730\u56fe\u6570\u636e\u521b\u5efa\u4ea4\u4e92\u5f0f\u9879\u76ee\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3aGoogle Maps\u5e73\u53f0\u7684\u5f00\u53d1\u80fd\u529b\uff0c\u8ba9\u5f00\u53d1\u8005\u66f4\u5bb9\u6613\u5229\u7528\u5730\u56fe\u6570\u636e\u521b\u5efa\u4ea4\u4e92\u5f0f\u5e94\u7528\u3002", "method": "\u5f15\u5165AI\u5de5\u5177\uff0c\u5305\u62ec\u6784\u5efa\u5668\u4ee3\u7406\u548cMCP\u670d\u52a1\u5668\uff0c\u63d0\u4f9b\u5f00\u53d1\u8005\u53cb\u597d\u7684\u63a5\u53e3\u6765\u8bbf\u95ee\u548c\u64cd\u4f5c\u5730\u56fe\u6570\u636e\u3002", "result": "\u6210\u529f\u53d1\u5e03\u4e86\u65b0\u7684AI\u5de5\u5177\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u521b\u5efa\u57fa\u4e8e\u5730\u56fe\u6570\u636e\u7684\u4ea4\u4e92\u5f0f\u9879\u76ee\u3002", "conclusion": "\u8fd9\u4e9bAI\u5de5\u5177\u663e\u8457\u63d0\u5347\u4e86Google Maps\u5e73\u53f0\u7684\u5f00\u53d1\u4f53\u9a8c\u548c\u529f\u80fd\u6269\u5c55\u6027\u3002", "topic": "swe application"}}
{"id": "tldr.2511.2f28a14c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fparsiya.net%2Fblog%2Fwtf-is-ai-native-sast%2F%3Futm_source=tldrinfosec/1/0100019a92371663-c0ba5cf3-c27d-4490-b944-4de78afc3475-000000/LIOJoi8R_Ef5YA_VvgyV0paVqt97ghKHvIOkno9BrHU=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fparsiya.net%2Fblog%2Fwtf-is-ai-native-sast%2F%3Futm_source=tldrinfosec/1/0100019a92371663-c0ba5cf3-c27d-4490-b944-4de78afc3475-000000/LIOJoi8R_Ef5YA_VvgyV0paVqt97ghKHvIOkno9BrHU=431", "authors": ["TLDR Newsletter"], "title": "WTF is\u2026 - AI-Native SAST", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fparsiya.net%2Fblog%2Fwtf-is-ai-native-sast%2F%3Futm_source=tldrinfosec/1/0100019a92371663-c0ba5cf3-c27d-4490-b944-4de78afc3475-000000/LIOJoi8R_Ef5YA_VvgyV0paVqt97ghKHvIOkno9BrHU=431", "summary": "WTF is\u2026 - AI-Native SAST (17 minute read) Leveraging AI as a complement to a traditional SAST can significantly improve results by using RAG to inject context on specific vulnerability classes. AI and SAST solutions can be leveraged using prompt-and-code, prompt-and-agent, or tailored-prompt-and-SAST-result methods. This post examines ZeroPath as an example of an agent, code graph, and SAST MCP.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06AI\u4f5c\u4e3a\u4f20\u7edfSAST\uff08\u9759\u6001\u5e94\u7528\u5b89\u5168\u6d4b\u8bd5\uff09\u7684\u8865\u5145\uff0c\u901a\u8fc7RAG\u6280\u672f\u6ce8\u5165\u7279\u5b9a\u6f0f\u6d1e\u7c7b\u522b\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u663e\u8457\u6539\u5584\u68c0\u6d4b\u7ed3\u679c\u3002\u4ecb\u7ecd\u4e86\u4e09\u79cdAI\u4e0eSAST\u7ed3\u5408\u7684\u65b9\u6cd5\uff1a\u63d0\u793a-\u4ee3\u7801\u3001\u63d0\u793a-\u4ee3\u7406\u548c\u5b9a\u5236\u63d0\u793a-SAST\u7ed3\u679c\u65b9\u6cd5\uff0c\u5e76\u4ee5ZeroPath\u4e3a\u4f8b\u8bf4\u660e\u4e86\u4ee3\u7406\u3001\u4ee3\u7801\u56fe\u548cSAST MCP\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edfSAST\u5de5\u5177\u5728\u68c0\u6d4b\u590d\u6742\u6f0f\u6d1e\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u5b89\u5168\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528RAG\u6280\u672f\u4e3a\u7279\u5b9a\u6f0f\u6d1e\u7c7b\u522b\u6ce8\u5165\u4e0a\u4e0b\u6587\uff0c\u91c7\u7528\u4e09\u79cd\u7ed3\u5408\u65b9\u5f0f\uff1a\u63d0\u793a-\u4ee3\u7801\u3001\u63d0\u793a-\u4ee3\u7406\u548c\u5b9a\u5236\u63d0\u793a-SAST\u7ed3\u679c\u65b9\u6cd5\u3002", "result": "AI\u4f5c\u4e3aSAST\u7684\u8865\u5145\u53ef\u4ee5\u663e\u8457\u6539\u5584\u68c0\u6d4b\u7ed3\u679c\uff0cZeroPath\u5c55\u793a\u4e86\u4ee3\u7406\u3001\u4ee3\u7801\u56fe\u548cSAST MCP\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "conclusion": "AI\u539f\u751fSAST\u662f\u5b89\u5168\u68c0\u6d4b\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u901a\u8fc7\u667a\u80fd\u5316\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u5206\u6790\u80fd\u591f\u5f25\u8865\u4f20\u7edf\u5de5\u5177\u7684\u4e0d\u8db3\u3002", "topic": "swe application"}}
{"id": "tldr.2511.0a456383", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fhow-dash-uses-context-engineering-for-smarter-ai%3Futm_source=tldrwebdev/1/0100019a97a74836-cb113210-e1ea-459c-89ea-451352caa013-000000/03WKnJXUNZ_HZdVgclNBnARfw5obgrJBVd9C2mXSjJs=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fhow-dash-uses-context-engineering-for-smarter-ai%3Futm_source=tldrwebdev/1/0100019a97a74836-cb113210-e1ea-459c-89ea-451352caa013-000000/03WKnJXUNZ_HZdVgclNBnARfw5obgrJBVd9C2mXSjJs=432", "authors": ["TLDR Newsletter"], "title": "How Dash uses context engineering for smarter AI", "comment": "Source: TLDR Newsletter, Date: 2025-11-18, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fhow-dash-uses-context-engineering-for-smarter-ai%3Futm_source=tldrwebdev/1/0100019a97a74836-cb113210-e1ea-459c-89ea-451352caa013-000000/03WKnJXUNZ_HZdVgclNBnARfw5obgrJBVd9C2mXSjJs=432", "summary": "How Dash uses context engineering for smarter AI (8 minute read) Dropbox evolved its Dash AI system from a traditional search tool into an \"agentic AI\" that can interpret, summarize, and act on information. Its team discovered that adding more tools and context often led to worse performance due to analysis paralysis and \"context rot,\" so they developed three key strategies: limiting tool definitions by consolidating multiple retrieval options into a single universal search tool, filtering co...", "source": "tldr", "AI": {"tldr": "Dropbox\u5c06Dash AI\u4ece\u4f20\u7edf\u641c\u7d22\u5de5\u5177\u6f14\u53d8\u4e3a\u80fd\u591f\u89e3\u91ca\u3001\u603b\u7ed3\u548c\u64cd\u4f5c\u4fe1\u606f\u7684\"\u4ee3\u7406AI\"\uff0c\u901a\u8fc7\u9650\u5236\u5de5\u5177\u5b9a\u4e49\u3001\u8fc7\u6ee4\u4e0a\u4e0b\u6587\u548c\u4f18\u5316\u63d0\u793a\u5de5\u7a0b\u6765\u907f\u514d\u5206\u6790\u762b\u75ea\u548c\u4e0a\u4e0b\u6587\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfAI\u7cfb\u7edf\u5728\u6dfb\u52a0\u66f4\u591a\u5de5\u5177\u548c\u4e0a\u4e0b\u6587\u65f6\u4f1a\u51fa\u73b0\u6027\u80fd\u4e0b\u964d\uff0c\u4ea7\u751f\u5206\u6790\u762b\u75ea\u548c\"\u4e0a\u4e0b\u6587\u9000\u5316\"\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u667a\u80fd\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7b56\u7565\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u79cd\u5173\u952e\u7b56\u7565\uff1a1) \u901a\u8fc7\u5c06\u591a\u4e2a\u68c0\u7d22\u9009\u9879\u6574\u5408\u4e3a\u5355\u4e00\u901a\u7528\u641c\u7d22\u5de5\u5177\u6765\u9650\u5236\u5de5\u5177\u5b9a\u4e49\uff1b2) \u8fc7\u6ee4\u4e0a\u4e0b\u6587\uff1b3) \u4f18\u5316\u63d0\u793a\u5de5\u7a0b\u3002", "result": "\u6210\u529f\u5c06Dash AI\u4ece\u4f20\u7edf\u641c\u7d22\u5de5\u5177\u8f6c\u53d8\u4e3a\u80fd\u591f\u89e3\u91ca\u3001\u603b\u7ed3\u548c\u64cd\u4f5c\u4fe1\u606f\u7684\u4ee3\u7406AI\u7cfb\u7edf\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347AI\u4ee3\u7406\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u907f\u514d\u5de5\u5177\u8fc7\u8f7d\u5e26\u6765\u7684\u8d1f\u9762\u6548\u5e94\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.25019f33", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FAnandChowdhary%2Fcontinuous-claude%3Futm_source=tldrwebdev/1/0100019a97a74836-cb113210-e1ea-459c-89ea-451352caa013-000000/IsyrRd2M96J0n_JxqZRXtK87jPfInkkRMiOCFMcBd-E=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FAnandChowdhary%2Fcontinuous-claude%3Futm_source=tldrwebdev/1/0100019a97a74836-cb113210-e1ea-459c-89ea-451352caa013-000000/IsyrRd2M96J0n_JxqZRXtK87jPfInkkRMiOCFMcBd-E=432", "authors": ["TLDR Newsletter"], "title": "Continuous Claude", "comment": "Source: TLDR Newsletter, Date: 2025-11-18, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FAnandChowdhary%2Fcontinuous-claude%3Futm_source=tldrwebdev/1/0100019a97a74836-cb113210-e1ea-459c-89ea-451352caa013-000000/IsyrRd2M96J0n_JxqZRXtK87jPfInkkRMiOCFMcBd-E=432", "summary": "Continuous Claude (GitHub Repo) Continuous Claude is a script that automates the pull request lifecycle using Claude Code. It commits changes to a new branch, creates a pull request, waits for checks and reviews, and merges the PR, repeating until the task is complete.", "source": "tldr", "AI": {"tldr": "Continuous Claude\u662f\u4e00\u4e2a\u81ea\u52a8\u5316PR\u751f\u547d\u5468\u671f\u7684\u811a\u672c\uff0c\u4f7f\u7528Claude Code\u81ea\u52a8\u63d0\u4ea4\u53d8\u66f4\u3001\u521b\u5efaPR\u3001\u7b49\u5f85\u68c0\u67e5\u548c\u5ba1\u6838\uff0c\u7136\u540e\u5408\u5e76PR\uff0c\u5faa\u73af\u6267\u884c\u76f4\u5230\u4efb\u52a1\u5b8c\u6210", "motivation": "\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684PR\u6d41\u7a0b\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387", "method": "\u4f7f\u7528\u811a\u672c\u81ea\u52a8\u5316\u6574\u4e2aPR\u751f\u547d\u5468\u671f\uff1a\u63d0\u4ea4\u53d8\u66f4\u5230\u65b0\u5206\u652f\u3001\u521b\u5efaPR\u3001\u7b49\u5f85\u68c0\u67e5\u548c\u5ba1\u6838\u3001\u5408\u5e76PR", "result": "\u5b9e\u73b0\u4e86PR\u6d41\u7a0b\u7684\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u80fd\u591f\u6301\u7eed\u6267\u884c\u76f4\u5230\u4efb\u52a1\u5b8c\u6210", "conclusion": "Continuous Claude\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u5316PR\u7ba1\u7406\u89e3\u51b3\u65b9\u6848", "topic": "swe application"}}
{"id": "tldr.2511.7631203b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fservicenow-mcp-integration%2F%3Futm_source=tldrdesign/1/0100019a97ab2d91-54685b40-860f-48e2-8495-506ff187715a-000000/3kvUuMhjfF-xt5XmTQDbfNCiIK7DbAHBu2m2J2ys8jU=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fservicenow-mcp-integration%2F%3Futm_source=tldrdesign/1/0100019a97ab2d91-54685b40-860f-48e2-8495-506ff187715a-000000/3kvUuMhjfF-xt5XmTQDbfNCiIK7DbAHBu2m2J2ys8jU=432", "authors": ["TLDR Newsletter"], "title": "ServiceNow and Figma Launch Strategic Collaboration to Turn Design Vision Into Enterprise Transformation", "comment": "Source: TLDR Newsletter, Date: 2025-11-18, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2Fservicenow-mcp-integration%2F%3Futm_source=tldrdesign/1/0100019a97ab2d91-54685b40-860f-48e2-8495-506ff187715a-000000/3kvUuMhjfF-xt5XmTQDbfNCiIK7DbAHBu2m2J2ys8jU=432", "summary": "ServiceNow and Figma Launch Strategic Collaboration to Turn Design Vision Into Enterprise Transformation (3 minute read) ServiceNow and Figma have launched a strategic collaboration utilizing MCP technology, enabling developers to transform Figma designs directly into enterprise-ready applications through ServiceNow's Build Agent. The integration automates the path from design to deployment with enterprise-grade security, reducing initial UI and data model implementation time by over 80%. Use...", "source": "tldr", "AI": {"tldr": "ServiceNow\u4e0eFigma\u5408\u4f5c\uff0c\u5229\u7528MCP\u6280\u672f\u5c06\u8bbe\u8ba1\u76f4\u63a5\u8f6c\u5316\u4e3a\u4f01\u4e1a\u7ea7\u5e94\u7528\uff0c\u51cf\u5c1180%\u7684UI\u548c\u6570\u636e\u6a21\u578b\u5b9e\u73b0\u65f6\u95f4", "motivation": "\u89e3\u51b3\u4ece\u8bbe\u8ba1\u5230\u4f01\u4e1a\u5e94\u7528\u90e8\u7f72\u7684\u81ea\u52a8\u5316\u8f6c\u6362\u95ee\u9898\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387", "method": "\u4f7f\u7528MCP\u6280\u672f\u548cServiceNow\u7684Build Agent\uff0c\u5b9e\u73b0Figma\u8bbe\u8ba1\u5230\u4f01\u4e1a\u7ea7\u5e94\u7528\u7684\u81ea\u52a8\u5316\u8f6c\u6362", "result": "UI\u548c\u6570\u636e\u6a21\u578b\u5b9e\u73b0\u65f6\u95f4\u51cf\u5c1180%\u4ee5\u4e0a\uff0c\u5177\u5907\u4f01\u4e1a\u7ea7\u5b89\u5168\u6027", "conclusion": "\u8be5\u5408\u4f5c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u8bbe\u8ba1\u5230\u90e8\u7f72\u7684\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u4e3a\u4f01\u4e1a\u5e94\u7528\u5f00\u53d1\u5e26\u6765\u6548\u7387\u7a81\u7834", "topic": "swe application"}}
{"id": "tldr.2511.49497b59", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lovart.ai%2F%3FsourceId=900187/2/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/jlDjdTUodP2qNE9anPjZb1YhzCB0O5UbYp4WbZ8YVeg=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lovart.ai%2F%3FsourceId=900187/2/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/jlDjdTUodP2qNE9anPjZb1YhzCB0O5UbYp4WbZ8YVeg=432", "authors": ["TLDR Newsletter"], "title": "3M+ users are now", "comment": "Source: TLDR Newsletter, Date: 2025-11-18, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lovart.ai%2F%3FsourceId=900187/2/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/jlDjdTUodP2qNE9anPjZb1YhzCB0O5UbYp4WbZ8YVeg=432", "summary": "Lovart hits $30M ARR with Figma-like editing for AI designs (Sponsor) Lovart's design agent is the biggest revelation since Midjourney, and it's absolutely exploding: 3M+ users are now turning prompts into brand-ready visuals, videos, and decks. New features drops include: \ud83e\uddd1\u200d\ud83d\udcbb Edit Elements is Lovart's new generation of AI-powered design precision. Users have full control over their creations without regenerating from scratch - including Live Editable Text (LET) and Layer Separation - isolate...", "source": "tldr", "AI": {"tldr": "Lovart\u662f\u4e00\u5bb6\u8bbe\u8ba1AI\u516c\u53f8\uff0c\u5e74\u6536\u5165\u8fbe3000\u4e07\u7f8e\u5143\uff0c\u63d0\u4f9b\u7c7b\u4f3cFigma\u7684AI\u8bbe\u8ba1\u7f16\u8f91\u529f\u80fd\uff0c\u62e5\u6709300\u4e07\u7528\u6237\uff0c\u53ef\u5c06\u63d0\u793a\u8bcd\u8f6c\u6362\u4e3a\u54c1\u724c\u5c31\u7eea\u7684\u89c6\u89c9\u5185\u5bb9\u3001\u89c6\u9891\u548c\u6f14\u793a\u6587\u7a3f\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfAI\u8bbe\u8ba1\u5de5\u5177\u7f3a\u4e4f\u7cbe\u786e\u7f16\u8f91\u63a7\u5236\u7684\u95ee\u9898\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u751f\u6210\u7684\u60c5\u51b5\u4e0b\u7cbe\u786e\u4fee\u6539\u8bbe\u8ba1\u5143\u7d20\u3002", "method": "\u5f00\u53d1\u4e86AI\u9a71\u52a8\u7684\u8bbe\u8ba1\u4ee3\u7406\uff0c\u5177\u6709\u5143\u7d20\u7f16\u8f91\u529f\u80fd\uff0c\u5305\u62ec\u5b9e\u65f6\u53ef\u7f16\u8f91\u6587\u672c(LET)\u548c\u56fe\u5c42\u5206\u79bb\u6280\u672f\u3002", "result": "\u53d6\u5f97\u4e86\u5546\u4e1a\u6210\u529f\uff0c\u5e74\u6536\u51653000\u4e07\u7f8e\u5143\uff0c\u62e5\u6709300\u4e07\u7528\u6237\uff0c\u6210\u4e3aMidjourney\u4e4b\u540e\u6700\u91cd\u8981\u7684\u8bbe\u8ba1AI\u7a81\u7834\u3002", "conclusion": "Lovart\u7684\u8bbe\u8ba1\u4ee3\u7406\u5728AI\u8bbe\u8ba1\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u901a\u8fc7\u63d0\u4f9b\u7cbe\u786e\u7684\u7f16\u8f91\u63a7\u5236\u529f\u80fd\u6ee1\u8db3\u4e86\u7528\u6237\u9700\u6c42\u3002", "topic": "swe application"}}
{"id": "tldr.2511.4ededc91", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dwarkesh.com%2Fp%2Fbits-per-sample%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/e8QNxDgTlzngBugtNOkYibPqH4fweeQc8EZf2DkIF3o=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dwarkesh.com%2Fp%2Fbits-per-sample%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/e8QNxDgTlzngBugtNOkYibPqH4fweeQc8EZf2DkIF3o=432", "authors": ["TLDR Newsletter"], "title": "RL is even more information inefficient than you thought", "comment": "Source: TLDR Newsletter, Date: 2025-11-18, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dwarkesh.com%2Fp%2Fbits-per-sample%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/e8QNxDgTlzngBugtNOkYibPqH4fweeQc8EZf2DkIF3o=432", "summary": "RL is even more information inefficient than you thought (12 minute read) It takes way more FLOPs to get a single sample in RL than it does in supervised learning. RL requires unrolling a whole thinking trajectory tens of thousands of tokens long to get a single reward signal at the end. In pretraining, you get a signal on every single token you train on. For most of training, the information density per sample is way lower for RL compared to supervised learning.", "source": "tldr", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u6bd4\u76d1\u7763\u5b66\u4e60\u7684\u4fe1\u606f\u6548\u7387\u66f4\u4f4e\uff0c\u9700\u8981\u66f4\u591a\u7684FLOPs\u6765\u83b7\u53d6\u5355\u4e2a\u6837\u672c\uff0c\u56e0\u4e3aRL\u9700\u8981\u5c55\u5f00\u6574\u4e2a\u601d\u8003\u8f68\u8ff9\uff08\u6570\u4e07\u4e2atoken\uff09\u624d\u80fd\u83b7\u5f97\u6700\u7ec8\u7684\u4e00\u4e2a\u5956\u52b1\u4fe1\u53f7\uff0c\u800c\u9884\u8bad\u7ec3\u4e2d\u6bcf\u4e2atoken\u90fd\u80fd\u83b7\u5f97\u4fe1\u53f7\u3002", "motivation": "\u63ed\u793a\u5f3a\u5316\u5b66\u4e60\u4e0e\u76d1\u7763\u5b66\u4e60\u5728\u4fe1\u606f\u6548\u7387\u4e0a\u7684\u663e\u8457\u5dee\u5f02\uff0c\u8bf4\u660eRL\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fe1\u606f\u5bc6\u5ea6\u8fdc\u4f4e\u4e8e\u76d1\u7763\u5b66\u4e60\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83RL\u548c\u76d1\u7763\u5b66\u4e60\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5206\u6790RL\u9700\u8981\u5c55\u5f00\u5b8c\u6574\u601d\u8003\u8f68\u8ff9\u624d\u80fd\u83b7\u5f97\u5956\u52b1\u4fe1\u53f7\u7684\u7279\u70b9\u3002", "result": "RL\u7684\u4fe1\u606f\u5bc6\u5ea6\u5728\u5927\u90e8\u5206\u8bad\u7ec3\u65f6\u95f4\u5185\u8fdc\u4f4e\u4e8e\u76d1\u7763\u5b66\u4e60\uff0c\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u6765\u83b7\u53d6\u6709\u6548\u8bad\u7ec3\u4fe1\u53f7\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5728\u4fe1\u606f\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\uff0c\u8fd9\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48RL\u8bad\u7ec3\u901a\u5e38\u9700\u8981\u6bd4\u76d1\u7763\u5b66\u4e60\u66f4\u591a\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.f5cd473e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.latent.space%2Fp%2Fagent-labs%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/OpF9lkh6d2u0dPGF4s-tnaYbkbJ4SsS181mCPIZ7krw=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.latent.space%2Fp%2Fagent-labs%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/OpF9lkh6d2u0dPGF4s-tnaYbkbJ4SsS181mCPIZ7krw=432", "authors": ["TLDR Newsletter"], "title": "The Agent Labs Thesis", "comment": "Source: TLDR Newsletter, Date: 2025-11-18, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.latent.space%2Fp%2Fagent-labs%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/OpF9lkh6d2u0dPGF4s-tnaYbkbJ4SsS181mCPIZ7krw=432", "summary": "The Agent Labs Thesis (12 minute read) Agent Labs primarily research and sell agents. They put product first and use outcome-based pricing, as opposed to Model Labs, which put models first and price per token. Agent Labs have better cashflow economics, but it might take longer to see exit valuations. The Model Lab mission may be shifting, at least until the next big algorithm shift.", "source": "tldr", "AI": {"tldr": "Agent Labs\u4e13\u6ce8\u4e8e\u7814\u7a76\u548c\u9500\u552e\u667a\u80fd\u4f53\uff0c\u91c7\u7528\u4ea7\u54c1\u4f18\u5148\u548c\u57fa\u4e8e\u7ed3\u679c\u7684\u5b9a\u4ef7\u7b56\u7565\uff0c\u4e0eModel Labs\u7684\u6a21\u578b\u4f18\u5148\u548c\u6309token\u5b9a\u4ef7\u5f62\u6210\u5bf9\u6bd4\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u7c7b\u578bAI\u5b9e\u9a8c\u5ba4\uff08Agent Labs vs Model Labs\uff09\u5728\u5546\u4e1a\u6a21\u5f0f\u3001\u5b9a\u4ef7\u7b56\u7565\u548c\u4f30\u503c\u524d\u666f\u65b9\u9762\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790Agent Labs\u548cModel Labs\u7684\u6838\u5fc3\u4e1a\u52a1\u6a21\u5f0f\u3001\u5b9a\u4ef7\u673a\u5236\u548c\u73b0\u91d1\u6d41\u7279\u5f81\u3002", "result": "Agent Labs\u5177\u6709\u66f4\u597d\u7684\u73b0\u91d1\u6d41\u7ecf\u6d4e\u6027\uff0c\u4f46\u9000\u51fa\u4f30\u503c\u53ef\u80fd\u9700\u8981\u66f4\u957f\u65f6\u95f4\uff1bModel Labs\u7684\u4f7f\u547d\u53ef\u80fd\u5728\u8f6c\u53d8\u3002", "conclusion": "\u4e24\u79cd\u5b9e\u9a8c\u5ba4\u6a21\u5f0f\u5404\u6709\u4f18\u52a3\uff0cAgent Labs\u6ce8\u91cd\u4ea7\u54c1\u5316\u548c\u7ed3\u679c\u5bfc\u5411\uff0cModel Labs\u53ef\u80fd\u9762\u4e34\u4e1a\u52a1\u6a21\u5f0f\u8c03\u6574\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.717e5396", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcookbook.openai.com%2Fexamples%2Fbuild_a_coding_agent_with_gpt-5.1%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/_sxWmBjBf6xXaVoKZUCLAWmTPyvuEt02rsGqEI_2Wsw=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcookbook.openai.com%2Fexamples%2Fbuild_a_coding_agent_with_gpt-5.1%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/_sxWmBjBf6xXaVoKZUCLAWmTPyvuEt02rsGqEI_2Wsw=432", "authors": ["TLDR Newsletter"], "title": "Build a GPT-5.1 Coding Agent", "comment": "Source: TLDR Newsletter, Date: 2025-11-18, Reading time: 19 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcookbook.openai.com%2Fexamples%2Fbuild_a_coding_agent_with_gpt-5.1%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/_sxWmBjBf6xXaVoKZUCLAWmTPyvuEt02rsGqEI_2Wsw=432", "summary": "Build a GPT-5.1 Coding Agent (19 minute read) This guide walks through building a coding agent using GPT-5.1 and the Agents SDK, leveraging tools like shell execution, patch editing, web search, and Context7 MCP for live documentation access.", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528GPT-5.1\u548cAgents SDK\u6784\u5efa\u7f16\u7801\u4ee3\u7406\u7684\u6307\u5357\uff0c\u652f\u6301shell\u6267\u884c\u3001\u8865\u4e01\u7f16\u8f91\u3001\u7f51\u7edc\u641c\u7d22\u548cContext7 MCP\u5b9e\u65f6\u6587\u6863\u8bbf\u95ee", "motivation": "\u4e3a\u4e86\u521b\u5efa\u529f\u80fd\u5f3a\u5927\u7684\u7f16\u7801\u4ee3\u7406\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u7f16\u7a0b\u4efb\u52a1\u5e76\u8bbf\u95ee\u5b9e\u65f6\u6587\u6863\u8d44\u6e90", "method": "\u5229\u7528GPT-5.1\u6a21\u578b\u548cAgents SDK\uff0c\u96c6\u6210\u591a\u79cd\u5de5\u5177\u5305\u62ecshell\u6267\u884c\u3001\u8865\u4e01\u7f16\u8f91\u3001\u7f51\u7edc\u641c\u7d22\u548cContext7 MCP", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u5907\u591a\u79cd\u7f16\u7a0b\u8f85\u52a9\u529f\u80fd\u7684\u7f16\u7801\u4ee3\u7406", "conclusion": "GPT-5.1\u7ed3\u5408Agents SDK\u80fd\u591f\u6709\u6548\u6784\u5efa\u529f\u80fd\u4e30\u5bcc\u7684\u7f16\u7801\u4ee3\u7406", "topic": "code agent"}}
{"id": "tldr.2511.65106251", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fgoogle-to-enable-research-automation-on-gemini-enterprise%2F%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/XULDNy7gv6ytS0Y9y6YgtY7L-_JF8IqyfsQO9q-o9Kc=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fgoogle-to-enable-research-automation-on-gemini-enterprise%2F%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/XULDNy7gv6ytS0Y9y6YgtY7L-_JF8IqyfsQO9q-o9Kc=432", "authors": ["TLDR Newsletter"], "title": "Google to enable research automation on Gemini Enterprise", "comment": "Source: TLDR Newsletter, Date: 2025-11-18, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fgoogle-to-enable-research-automation-on-gemini-enterprise%2F%3Futm_source=tldrai/1/0100019a97cf8e6c-3bf1c083-c28c-4e41-8b85-8dcb6ff74814-000000/XULDNy7gv6ytS0Y9y6YgtY7L-_JF8IqyfsQO9q-o9Kc=432", "summary": "Google to enable research automation on Gemini Enterprise (3 minute read) The unreleased system generates roughly 100 ideas on a topic, then spawns agent teams that compete in a tournament-style bracket for the best result.", "source": "tldr", "AI": {"tldr": "\u8c37\u6b4c\u5f00\u53d1\u4e86\u4e00\u4e2a\u672a\u53d1\u5e03\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u7ea6100\u4e2a\u4e3b\u9898\u60f3\u6cd5\uff0c\u5e76\u901a\u8fc7\u4ee3\u7406\u56e2\u961f\u5728\u9526\u6807\u8d5b\u5f0f\u8d5b\u5236\u4e2d\u7ade\u4e89\u4ee5\u4ea7\u751f\u6700\u4f73\u7ed3\u679c\u3002", "motivation": "\u65e8\u5728\u5b9e\u73b0\u7814\u7a76\u8fc7\u7a0b\u7684\u81ea\u52a8\u5316\uff0c\u63d0\u9ad8\u7814\u7a76\u6548\u7387\u548c\u521b\u65b0\u6027\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u7ade\u4e89\u673a\u5236\u6fc0\u53d1\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u9996\u5148\u751f\u6210\u5927\u91cf\u4e3b\u9898\u60f3\u6cd5\uff0c\u7136\u540e\u521b\u5efa\u591a\u4e2a\u4ee3\u7406\u56e2\u961f\uff0c\u8fd9\u4e9b\u56e2\u961f\u5728\u9526\u6807\u8d5b\u5f0f\u8d5b\u5236\u4e2d\u76f8\u4e92\u7ade\u4e89\uff0c\u6700\u7ec8\u9009\u51fa\u6700\u4f73\u7ed3\u679c\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u5316\u7814\u7a76\u8fc7\u7a0b\uff0c\u901a\u8fc7\u7ade\u4e89\u673a\u5236\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u7814\u7a76\u6210\u679c\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u7814\u7a76\u7cfb\u7edf\u5c55\u793a\u4e86\u901a\u8fc7\u591a\u4ee3\u7406\u7ade\u4e89\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u7814\u7a76\u8d28\u91cf\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "wechat.2511.7f31d722", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4MTU2NjEzMg==&mid=2247486285&idx=1&sn=470ef6b72409a1935921019f6d35f0a3&chksm=fc37f4ec456cfabe00a05c26157d1f75d22ed9c6aab760324dae308b73248917761bffdb156f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4MTU2NjEzMg==&mid=2247486285&idx=1&sn=470ef6b72409a1935921019f6d35f0a3&chksm=fc37f4ec456cfabe00a05c26157d1f75d22ed9c6aab760324dae308b73248917761bffdb156f#rd", "authors": ["\u5149\u5b66\u4e86"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u9a71\u52a8\u7684\u7eb3\u7c73\u5149\u5b50\u5668\u4ef6\u7814\u7a76", "comment": "Source: WeChat, Published: 2025-11-19 12:46:03", "summary": "\u6570\u5b66\u4e0a\uff0c\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\uff0c\u5373\u4f18\u5316\u7b56\u7565 \u4f7f\u5f97\u671f\u671b\u6700\u5927\u5176\u4e2d \u662f\u6298\u6263\u56e0\u5b50\uff0c \u662f\u7b2c\u6b65\u7684\u5373\u65f6\u5956\u52b1\u3002\u5728\u5177\u4f53\u5b9e\u73b0\u4e2d\uff0c\u4f5c\u8005\u91c7\u7528\u4e86\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u6216\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u903c\u8fd1\u72b6\u6001-\u52a8\u4f5c\u503c\u51fd\u6570\uff0c\u5e76\u5229\u7528\u7ecf\u9a8c\u56de", "AI": {"tldr": "\u6570\u5b66\u4e0a\uff0c\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\uff0c\u5373\u4f18\u5316\u7b56\u7565 \u4f7f\u5f97\u671f\u671b\u6700\u5927\u5176\u4e2d \u662f\u6298\u6263\u56e0\u5b50\uff0c \u662f\u7b2c\u6b65\u7684\u5373\u65f6\u5956\u52b1\u3002\u5728\u5177\u4f53\u5b9e\u73b0\u4e2d\uff0c\u4f5c\u8005\u91c7\u7528\u4e86\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u6216\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u903c\u8fd1\u72b6\u6001-\u52a8\u4f5c\u503c\u51fd\u6570\uff0c\u5e76\u5229\u7528\u7ecf\u9a8c\u56de", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.ede5d67a", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwMTYzOTMzMw==&mid=2247483906&idx=1&sn=7501d470b6c32e08b88c30e97f132e58&chksm=c1165c74f57f404ee6d1db63f81e08cc0ba473277b8ba844ec9c946601b1572872ef37a1dc07#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwMTYzOTMzMw==&mid=2247483906&idx=1&sn=7501d470b6c32e08b88c30e97f132e58&chksm=c1165c74f57f404ee6d1db63f81e08cc0ba473277b8ba844ec9c946601b1572872ef37a1dc07#rd", "authors": ["\u9e4f\u7a0bAI"], "title": "\u6df1\u5ea6\u597d\u6587\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5728LRM\u4e2d\u7684\u5e94\u7528\u7efc\u8ff0\u7cfb\u52172", "comment": "Source: WeChat, Published: 2025-11-19 12:37:13", "summary": "\u672c\u7bc7\u4e3b\u8981\u505a\u5982\u4e0b\u603b\u7ed3\uff1a \u5f3a\u5316\u5b66\u4e60\u5728LRM\u4e2d\u5982\u4f55\u4f18\u5316policy\uff0c\u5982\u4f55\u8fdb\u884c\u91c7\u6837\u3002\u5bf9\u4e8e\u60f3\u4e86\u89e3\u5956\u52b1\u4fe1\u53f7\u548c\u6a21\u578b\u53d1\u5c55\u7684\u8bfb\u8005\u53ef\u4ee5\u53c2\u8003\uff1ahttps\uff1a//mp.weixin.qq.com/s/JBR1n2rrcQfhzAdd4i-F1w", "AI": {"tldr": "\u672c\u7bc7\u4e3b\u8981\u505a\u5982\u4e0b\u603b\u7ed3\uff1a \u5f3a\u5316\u5b66\u4e60\u5728LRM\u4e2d\u5982\u4f55\u4f18\u5316policy\uff0c\u5982\u4f55\u8fdb\u884c\u91c7\u6837\u3002\u5bf9\u4e8e\u60f3\u4e86\u89e3\u5956\u52b1\u4fe1\u53f7\u548c\u6a21\u578b\u53d1\u5c55\u7684\u8bfb\u8005\u53ef\u4ee5\u53c2\u8003\uff1ahttps\uff1a//mp.weixin.qq.com/s/JBR1n2rrcQfhzAdd4i-F1w", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.932e1b1a", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNzQ0ODkyMQ==&mid=2247491404&idx=1&sn=d505403004e34e9f5ac1467ab5974f15&chksm=960177c35ad961426f4b294e93deb71845c14ffc8ababc542f577222cd9bf8222984120fc030#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNzQ0ODkyMQ==&mid=2247491404&idx=1&sn=d505403004e34e9f5ac1467ab5974f15&chksm=960177c35ad961426f4b294e93deb71845c14ffc8ababc542f577222cd9bf8222984120fc030#rd", "authors": ["AILab\u7b14\u8bb0"], "title": "\u3010\u6587\u732e\u3011\u901a\u8fc7<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e2d\u5956\u52b1\u51fd\u6570\u4e3b\u52a8\u7a81\u53d8\u5b9e\u73b0\u673a\u5668\u4eba\u6db2\u4f53\u503e\u5012\u4efb\u52a1\u7684\u6280\u80fd\u591a\u6837\u5316", "comment": "Source: WeChat, Published: 2025-11-19 11:54:43", "summary": "\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u3002\u672c\u6587\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86\u5956\u52b1\u51fd\u6570\u4e2d\u7a81\u53d8\u6743\u91cd\u7684\u4e0d\u540c\u914d\u7f6e\u5c06\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u5230\u7684\u7b56\u7565\u3002\u7531\u6b64\u4ea7\u751f\u7684\u7b56\u7565\u8868\u73b0\u51fa\u5e7f\u6cdb\u7684\u884c\u4e3a\uff1a\u4ece\u6267\u884c\u6700\u521d\u9884\u671f\u7684\u6d47\u6ce8\u4efb\u52a1\u7684\u53d8\u5316\u5230\u5bf9\u610f\u5916\u4efb\u52a1\u6709\u7528\u7684\u65b0\u6280\u80fd\uff0c\u4f8b\u5982\u5bb9\u5668\u8fb9\u7f18\u6e05", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u3002\u672c\u6587\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86\u5956\u52b1\u51fd\u6570\u4e2d\u7a81\u53d8\u6743\u91cd\u7684\u4e0d\u540c\u914d\u7f6e\u5c06\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u5230\u7684\u7b56\u7565\u3002\u7531\u6b64\u4ea7\u751f\u7684\u7b56\u7565\u8868\u73b0\u51fa\u5e7f\u6cdb\u7684\u884c\u4e3a\uff1a\u4ece\u6267\u884c\u6700\u521d\u9884\u671f\u7684\u6d47\u6ce8\u4efb\u52a1\u7684\u53d8\u5316\u5230\u5bf9\u610f\u5916\u4efb\u52a1\u6709\u7528\u7684\u65b0\u6280\u80fd\uff0c\u4f8b\u5982\u5bb9\u5668\u8fb9\u7f18\u6e05", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.1b8810e4", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwNjU0NjA3Ng==&mid=2247516978&idx=1&sn=5020a8bbbf55c6d005efff0c8891ae41&chksm=9a18be4b35aa003bd86d0e381a14cfbe3af86d21347b29353a6cbe7370b4a56f69f955e3f101#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwNjU0NjA3Ng==&mid=2247516978&idx=1&sn=5020a8bbbf55c6d005efff0c8891ae41&chksm=9a18be4b35aa003bd86d0e381a14cfbe3af86d21347b29353a6cbe7370b4a56f69f955e3f101#rd", "authors": ["\u6708\u6765\u5ba2\u6808"], "title": "\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e2d\u7684Actor\u548cCritic\u6709\u4ec0\u4e48\u533a\u522b\uff1f", "comment": "Source: WeChat, Published: 2025-11-19 10:42:12", "summary": "\u4e00\u53e5\u8bdd\u603b\u7ed3\uff0cActor \u8d1f\u8d23\u5b66\u201c\u505a\u4ec0\u4e48\u201d\uff08\u7b56\u7565\u5b66\u4e60\uff09\uff0c\u800c Critic \u8d1f\u8d23\u5b66\u201c\u597d\u4e0d\u597d\u201d\uff08\u4ef7\u503c\u8bc4\u4f30\uff09\uff0c \u4e8c\u8005\u76f8\u4e92\u914d\u5408\uff0c\u6784\u6210\u5f3a\u5316\u5b66\u4e60\u4e2d\u6700\u5e38\u89c1\u3001\u6700\u7a33\u5b9a\u7684\u7ed3\u6784\u4e4b\u4e00\u3002", "AI": {"tldr": "\u4e00\u53e5\u8bdd\u603b\u7ed3\uff0cActor \u8d1f\u8d23\u5b66\u201c\u505a\u4ec0\u4e48\u201d\uff08\u7b56\u7565\u5b66\u4e60\uff09\uff0c\u800c Critic \u8d1f\u8d23\u5b66\u201c\u597d\u4e0d\u597d\u201d\uff08\u4ef7\u503c\u8bc4\u4f30\uff09\uff0c \u4e8c\u8005\u76f8\u4e92\u914d\u5408\uff0c\u6784\u6210\u5f3a\u5316\u5b66\u4e60\u4e2d\u6700\u5e38\u89c1\u3001\u6700\u7a33\u5b9a\u7684\u7ed3\u6784\u4e4b\u4e00\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.eb6b88a4", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MjU2NTQ1Mw==&mid=2247535839&idx=1&sn=f30d436b85bc8c33c3927d873980acae&chksm=a7cdf88026470a660c02c224699b235f9709691739ae83542357888c344cab4ba9a452dea0df#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MjU2NTQ1Mw==&mid=2247535839&idx=1&sn=f30d436b85bc8c33c3927d873980acae&chksm=a7cdf88026470a660c02c224699b235f9709691739ae83542357888c344cab4ba9a452dea0df#rd", "authors": ["\u5357\u5eb7\u7ec4\u5de5"], "title": "\u3010\u6bd4\u6b66\u783a\u5175 \u5f3a\u57fa\u63d0\u80fd\u3011\u533a\u653f\u5e9c\u529e\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u63d0\u80fd\u529b\uff0c\u6253\u9020\u9ad8\u7d20\u8d28\u533a\u653f\u5e9c\u529e\u5e72\u90e8\u961f\u4f0d\uff5c\u5357\u5eb7\u533a\u201c\u8d4b\u80fd\u63d0\u5347\u201d\u5c97\u4f4d\u7ec3\u5175\u884c\u52a8\u7eaa\u5b9e\uff08\u4e8c\u5341\u56db\uff09", "comment": "Source: WeChat, Published: 2025-11-19 10:03:12", "summary": "\u533a\u653f\u5e9c\u529e\uff1a\u5f3a\u5316\u5b66\u4e60\u63d0\u80fd\u529b\uff0c\u6253\u9020\u9ad8\u7d20\u8d28\u533a\u653f\u5e9c\u529e\u5e72\u90e8\u961f\u4f0d \u4e3a\u4fc3\u8fdb\u5e72\u90e8\u961f\u4f0d\u80fd\u529b\u63d0\u5347\uff0c\u6253\u9020\u4e00\u652f\u201c\u63d0\u7b14\u80fd\u5199\u3001\u5f00\u53e3\u80fd\u8bf4\u3001\u95ee\u7b56\u80fd\u5bf9\u3001\u9047\u4e8b\u80fd\u529e\u201d\u7684\u533a\u653f\u5e9c\u529e\u5e72\u90e8\u961f\u4f0d\uff0c\u4e3a\u5168\u533a\u9ad8\u8d28\u91cf\u53d1\u5c55\u63d0\u4f9b\u6709\u529b\u7684\u201c\u4e09\u670d\u52a1\u201d\u4fdd\u969c\u3002", "AI": {"tldr": "\u533a\u653f\u5e9c\u529e\uff1a\u5f3a\u5316\u5b66\u4e60\u63d0\u80fd\u529b\uff0c\u6253\u9020\u9ad8\u7d20\u8d28\u533a\u653f\u5e9c\u529e\u5e72\u90e8\u961f\u4f0d \u4e3a\u4fc3\u8fdb\u5e72\u90e8\u961f\u4f0d\u80fd\u529b\u63d0\u5347\uff0c\u6253\u9020\u4e00\u652f\u201c\u63d0\u7b14\u80fd\u5199\u3001\u5f00\u53e3\u80fd\u8bf4\u3001\u95ee\u7b56\u80fd\u5bf9\u3001\u9047\u4e8b\u80fd\u529e\u201d\u7684\u533a\u653f\u5e9c\u529e\u5e72\u90e8\u961f\u4f0d\uff0c\u4e3a\u5168\u533a\u9ad8\u8d28\u91cf\u53d1\u5c55\u63d0\u4f9b\u6709\u529b\u7684\u201c\u4e09\u670d\u52a1\u201d\u4fdd\u969c\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.45118bad", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyMDY0OTc1NA==&mid=2247534101&idx=2&sn=2ec3b2a02989f25c13a2d4b844b60e14&chksm=c0af54c63ac3f7cbe5c67acab01794cd9b7be3d0d64ebe07a93810dbc904ec32bc2adb52f4ea#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyMDY0OTc1NA==&mid=2247534101&idx=2&sn=2ec3b2a02989f25c13a2d4b844b60e14&chksm=c0af54c63ac3f7cbe5c67acab01794cd9b7be3d0d64ebe07a93810dbc904ec32bc2adb52f4ea#rd", "authors": ["3D\u89c6\u89c9\u4e4b\u5fc3"], "title": "Physical Intelligence\u56e2\u961f\u6b63\u5f0f\u53d1\u5e03\u03c0*0.6\uff01VLA+<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u8bad\u7ec3", "comment": "Source: WeChat, Published: 2025-11-19 01:06:26", "summary": "\u867d\u7136\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3b\u5b9e\u8df5\u7406\u8bba\u57fa\u7840\u65e9\u5728\u6570\u5341\u5e74\u524d\u5c31\u5df2\u786e\u7acb\uff0c\u4f46\u8981\u5c06\u8fd9\u4e9b\u539f\u7406\u878d\u5165\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5b66\u4e60\u7cfb\u7edf\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff1a\u4e3a\u5927\u578b\u6a21\u578b\u8bbe\u8ba1\u53ef\u6269\u5c55\u4e14\u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3001\u5904\u7406\u6765\u81ea\u4e0d\u540c\u7b56\u7565\u7684\u5f02\u6784\u6570\u636e\uff0c\u4ee5\u53ca\u5728\u5956\u52b1\u4fe1", "AI": {"tldr": "\u867d\u7136\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3b\u5b9e\u8df5\u7406\u8bba\u57fa\u7840\u65e9\u5728\u6570\u5341\u5e74\u524d\u5c31\u5df2\u786e\u7acb\uff0c\u4f46\u8981\u5c06\u8fd9\u4e9b\u539f\u7406\u878d\u5165\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u673a\u5668\u4eba\u5b66\u4e60\u7cfb\u7edf\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff1a\u4e3a\u5927\u578b\u6a21\u578b\u8bbe\u8ba1\u53ef\u6269\u5c55\u4e14\u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3001\u5904\u7406\u6765\u81ea\u4e0d\u540c\u7b56\u7565\u7684\u5f02\u6784\u6570\u636e\uff0c\u4ee5\u53ca\u5728\u5956\u52b1\u4fe1", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.6329c554", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMDIwOTU5OQ==&mid=2247484224&idx=1&sn=08e443a2c03a1abaac52a127bc5a6e44&chksm=c0e262d849390790f145cbdeb6206378b0948292afdaad7bac167bf91f3fd877993da6de452a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMDIwOTU5OQ==&mid=2247484224&idx=1&sn=08e443a2c03a1abaac52a127bc5a6e44&chksm=c0e262d849390790f145cbdeb6206378b0948292afdaad7bac167bf91f3fd877993da6de452a#rd", "authors": ["AI\u8001\u9a6c\u554a"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em> | \u4f18\u5316\u7b56\u7565 Roadmap \u4ecb\u7ecd", "comment": "Source: WeChat, Published: 2025-11-19 00:21:57", "summary": "1.2\uff0c\u5f3a\u5316\u5b66\u4e60\u548c\u76d1\u7763\u5b66\u4e60\u533a\u522b \u5f3a\u5316\u5b66\u4e60\u66f4\u53ef\u80fd\u8003\u8651\u6574\u4f53\u5f71\u54cd\u76d1\u7763\u5b66\u4e60\u9488\u5bf9\u5355\u4e2a\u8bcd\u5143\u8fdb\u884c\u53cd\u9988\uff0c\u76ee\u6807\u662f\u8981\u6c42\u6a21\u578b\u9488\u5bf9\u7ed9\u5b9a\u7684\u8f93\u5165\u7ed9\u51fa\u786e\u5207\u7684\u7b54\u6848\uff0c\u5e76\u4e14\u5176\u635f\u5931\u4e3a\u57fa\u4e8e\u603b\u548c\u89c4\u5219\u7684\u4ea4\u53c9\u71b5\u51fd\u6570\uff0c\u9020\u6210\u8fd9\u79cd\u635f\u5931\u5bf9\u4e2a\u522b\u8bcd\u5143\u53d8\u5316\u4e0d\u654f\u611f\u3002", "AI": {"tldr": "1.2\uff0c\u5f3a\u5316\u5b66\u4e60\u548c\u76d1\u7763\u5b66\u4e60\u533a\u522b \u5f3a\u5316\u5b66\u4e60\u66f4\u53ef\u80fd\u8003\u8651\u6574\u4f53\u5f71\u54cd\u76d1\u7763\u5b66\u4e60\u9488\u5bf9\u5355\u4e2a\u8bcd\u5143\u8fdb\u884c\u53cd\u9988\uff0c\u76ee\u6807\u662f\u8981\u6c42\u6a21\u578b\u9488\u5bf9\u7ed9\u5b9a\u7684\u8f93\u5165\u7ed9\u51fa\u786e\u5207\u7684\u7b54\u6848\uff0c\u5e76\u4e14\u5176\u635f\u5931\u4e3a\u57fa\u4e8e\u603b\u548c\u89c4\u5219\u7684\u4ea4\u53c9\u71b5\u51fd\u6570\uff0c\u9020\u6210\u8fd9\u79cd\u635f\u5931\u5bf9\u4e2a\u522b\u8bcd\u5143\u53d8\u5316\u4e0d\u654f\u611f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.a27fc8a2", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzODkxNzQzMw==&mid=2247495854&idx=1&sn=70ca03658a060ed8f74d04463d1cc127&chksm=fbf16653ebfdc6804e83dce4a21b43fbcd92dcd67998be08d56ce87540403b41d215a33690db#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzODkxNzQzMw==&mid=2247495854&idx=1&sn=70ca03658a060ed8f74d04463d1cc127&chksm=fbf16653ebfdc6804e83dce4a21b43fbcd92dcd67998be08d56ce87540403b41d215a33690db#rd", "authors": ["VLer"], "title": "ICRA 2025 \u6e05\u534e&UC\u4f2f\u514b\u5229\u63d0\u51faiRe-VLA\uff1a\u8fed\u4ee3\u5f0f<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff0c\u8ba9\u673a\u5668\u4eba\u5927\u6a21\u578b\u544a\u522b\u8bad\u7ec3\u5d29\u6e83", "comment": "Source: WeChat, Published: 2025-11-18 23:00:00", "summary": "\u5927\u5bb6\u77e5\u9053\uff0c\u73b0\u5728\u7684\u5927\u6a21\u578b\uff08LLMs\uff09\u8bad\u7ec3\u6d41\u7a0b\u901a\u5e38\u662f\u201c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09+ \u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u201d\u3002\u7814\u7a76\u8005\u4eec\u5f88\u81ea\u7136\u5730\u60f3\u628a\u8fd9\u5957\u6d41\u7a0b\u642c\u5230\u673a\u5668\u4eba\u9886\u57df\uff0c\u7528\u5728VLA\u6a21\u578b\u4e0a\u3002", "AI": {"tldr": "\u5927\u5bb6\u77e5\u9053\uff0c\u73b0\u5728\u7684\u5927\u6a21\u578b\uff08LLMs\uff09\u8bad\u7ec3\u6d41\u7a0b\u901a\u5e38\u662f\u201c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09+ \u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u201d\u3002\u7814\u7a76\u8005\u4eec\u5f88\u81ea\u7136\u5730\u60f3\u628a\u8fd9\u5957\u6d41\u7a0b\u642c\u5230\u673a\u5668\u4eba\u9886\u57df\uff0c\u7528\u5728VLA\u6a21\u578b\u4e0a\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.abe12349", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3NTg0Njg2Nw==&mid=2247495658&idx=1&sn=af9e4774945aa7fcc018f276fc4472e2&chksm=ce26fcadd28ac18a03ed713edfa062fe0938cabcad389dd5384c60d40781d5821c81e8bd3f41#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3NTg0Njg2Nw==&mid=2247495658&idx=1&sn=af9e4774945aa7fcc018f276fc4472e2&chksm=ce26fcadd28ac18a03ed713edfa062fe0938cabcad389dd5384c60d40781d5821c81e8bd3f41#rd", "authors": ["\u80d6\u732a\u82f1\u8bed"], "title": "\u79d1\u6280\u9886\u57df\u6700\u65b0\u70ed\u8bcd\u201c<em class=\"highlight\">agentic</em>\u201d\u4eba\u5de5\u667a\u80fd\u662f\u4ec0\u4e48\u610f\u601d\uff1f", "comment": "Source: WeChat, Published: 2025-11-19 07:56:07", "summary": "\u2018Agentic\u2019 \u662f\u4e00\u4e2a\u57fa\u4e8e\u53e4\u8001\u7406\u5ff5\u7684\u6d41\u884c\u70ed\u8bcd\u7c73\u6797\u5fb7\u00b7\u5766\u8d1d\u7814\u7a76\u534f\u4f5c\u578b\u4eba\u5de5\u667a\u80fd\u667a\u80fd\u4f53\u5df2\u6709\u4e09\u5341\u5e74\uff0c\u59cb\u4e8e1995\u5e74\u9996\u5c4a\u56fd\u9645\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5927\u4f1a\u5728\u65e7\u91d1\u5c71\u53ec\u5f00\u4e4b\u65f6\u3002", "AI": {"tldr": "\u2018Agentic\u2019 \u662f\u4e00\u4e2a\u57fa\u4e8e\u53e4\u8001\u7406\u5ff5\u7684\u6d41\u884c\u70ed\u8bcd\u7c73\u6797\u5fb7\u00b7\u5766\u8d1d\u7814\u7a76\u534f\u4f5c\u578b\u4eba\u5de5\u667a\u80fd\u667a\u80fd\u4f53\u5df2\u6709\u4e09\u5341\u5e74\uff0c\u59cb\u4e8e1995\u5e74\u9996\u5c4a\u56fd\u9645\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5927\u4f1a\u5728\u65e7\u91d1\u5c71\u53ec\u5f00\u4e4b\u65f6\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.2e8c91eb", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwNjQxOTk1Mg==&mid=2247486124&idx=1&sn=fc5edb7997d137de737a10bb1a6a13ad&chksm=c18d792be41773d597db6367f700bfc8c878acd71e7cf4019cbbe794cbdafa9a9e9da8697e81#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwNjQxOTk1Mg==&mid=2247486124&idx=1&sn=fc5edb7997d137de737a10bb1a6a13ad&chksm=c18d792be41773d597db6367f700bfc8c878acd71e7cf4019cbbe794cbdafa9a9e9da8697e81#rd", "authors": ["\u5f00\u6e90\u60c5\u62a5\u6280\u672f\u7814\u7a76\u9662"], "title": "\u591a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u65f6\u4ee3\u6765\u4e34\u5566", "comment": "Source: WeChat, Published: 2025-11-19 00:30:40", "summary": "Agentic AI\uff08\u4e5f\u79f0Agentic Artificial Intelligence\u6216\u81ea\u52a8\u5316\u4ee3\u7406AI\uff09\u662f2025\u5e74AI\u9886\u57df\u7684\u9876\u7ea7\u8d8b\u52bf\uff08Gartner\u5c06\u5176\u5217\u4e3a2025\u5e74\u6280\u672f\u8d8b\u52bfNo.1\uff09\u3002\u4e0e\u4f20\u7edf\u751f\u6210\u5f0fAI\uff08\u5982ChatGPT\uff09\u4e0d\u540c\uff0cAgentic AI\u5177\u5907\u4ee5\u4e0b\u6838\u5fc3\u7279\u5f81\uff1a", "AI": {"tldr": "Agentic AI\uff08\u4e5f\u79f0Agentic Artificial Intelligence\u6216\u81ea\u52a8\u5316\u4ee3\u7406AI\uff09\u662f2025\u5e74AI\u9886\u57df\u7684\u9876\u7ea7\u8d8b\u52bf\uff08Gartner\u5c06\u5176\u5217\u4e3a2025\u5e74\u6280\u672f\u8d8b\u52bfNo.1\uff09\u3002\u4e0e\u4f20\u7edf\u751f\u6210\u5f0fAI\uff08\u5982ChatGPT\uff09\u4e0d\u540c\uff0cAgentic AI\u5177\u5907\u4ee5\u4e0b\u6838\u5fc3\u7279\u5f81\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.dc50eaea", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU0NjExNTQ3Nw==&mid=2247484515&idx=1&sn=ff37517703cb24abf2641785a2fdc702&chksm=fa5ad0bdc29c81103e9a3209ab3b2a295c1a01f5e40cf04abd32900b1184da901dce3c9069dd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU0NjExNTQ3Nw==&mid=2247484515&idx=1&sn=ff37517703cb24abf2641785a2fdc702&chksm=fa5ad0bdc29c81103e9a3209ab3b2a295c1a01f5e40cf04abd32900b1184da901dce3c9069dd#rd", "authors": ["AI\u7684\u573a\u666f\u548c\u673a\u4f1a"], "title": "<em class=\"highlight\">Agentic</em> \u5f00\u53d1\u5e73\u53f0\uff0c\u8054\u5408 Gemini 3 \u4e00\u8d77\u63a8\u51fa\uff0cGoogle \u6709\u81ea\u5df1\u7684 <em class=\"highlight\">Agentic</em> IDEA.", "comment": "Source: WeChat, Published: 2025-11-19 00:27:36", "summary": "Google Antigravity\uff1aGoogle \u5168\u65b0\u63a8\u51fa\u7684 Agentic \u5f00\u53d1\u5e73\u53f0\uff0c\u8054\u5408 Gemini 3 \u4e00\u8d77\u63a8\u51fa\uff0cGoogle \u6709\u81ea\u5df1\u7684 Agentic IDE \u4e86\u4ece\u53d1\u5e03\u4fe1\u606f\u770b\uff0c\u662f $2.4B \u6316\u6765\u7684 Windsurf \u6838\u5fc3\u56e2\u961f\u505a\u7684\uff0c\u4e5f\u5f88\u5408\u7406\uff0c\u96be\u602a\u6628\u5929\u770b\u5230\u52a0\u5165 Deepmind \u540e\u7b2c\u4e00\u6b21\u53d1\u63a8\u300c\u4e00\u4e2a\u53cd\u91cd\u529b\u7684\u7b14\u8bb0", "AI": {"tldr": "Google Antigravity\uff1aGoogle \u5168\u65b0\u63a8\u51fa\u7684 Agentic \u5f00\u53d1\u5e73\u53f0\uff0c\u8054\u5408 Gemini 3 \u4e00\u8d77\u63a8\u51fa\uff0cGoogle \u6709\u81ea\u5df1\u7684 Agentic IDE \u4e86\u4ece\u53d1\u5e03\u4fe1\u606f\u770b\uff0c\u662f $2.4B \u6316\u6765\u7684 Windsurf \u6838\u5fc3\u56e2\u961f\u505a\u7684\uff0c\u4e5f\u5f88\u5408\u7406\uff0c\u96be\u602a\u6628\u5929\u770b\u5230\u52a0\u5165 Deepmind \u540e\u7b2c\u4e00\u6b21\u53d1\u63a8\u300c\u4e00\u4e2a\u53cd\u91cd\u529b\u7684\u7b14\u8bb0", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.d51dfc85", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5OTA3NTY5OQ==&mid=2247486157&idx=3&sn=db9839a3ab81087b2b7062c1628e92e4&chksm=970564d051b91be5cb7a3d45fe1ee251d3410c806549af6ef00c6447c3f890f15d9f2d7ed82e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5OTA3NTY5OQ==&mid=2247486157&idx=3&sn=db9839a3ab81087b2b7062c1628e92e4&chksm=970564d051b91be5cb7a3d45fe1ee251d3410c806549af6ef00c6447c3f890f15d9f2d7ed82e#rd", "authors": ["ATinfo"], "title": "<em class=\"highlight\">Agentic</em> AI \u5728\u53ef\u89c2\u6d4b\u6027\u9886\u57df\uff1a\u6784\u5efa\u5f39\u6027\u3001\u53ef\u95ee\u8d23\u7684 IT \u7cfb\u7edf", "comment": "Source: WeChat, Published: 2025-11-18 16:00:19", "summary": "Agentic AI \u6982\u5ff5\u89e3\u6790\u6280\u672f\u7279\u70b9\uff1aAgentic AI \u6307\u80fd\u591f\u901a\u8fc7\u89c4\u5212\u3001\u63a8\u7406\u548c\u884c\u52a8\u81ea\u4e3b\u6267\u884c\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u7684\u7cfb\u7edf\u4e0e\u54cd\u5e94\u76f4\u63a5\u6307\u4ee4\u7684\u4f20\u7edf AI \u4e0d\u540c\uff0cAgentic AI \u662f\u4e3b\u52a8\u4e14\u76ee\u6807\u9a71\u52a8\u7684\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u6761\u4ef6\u3002", "AI": {"tldr": "Agentic AI \u6982\u5ff5\u89e3\u6790\u6280\u672f\u7279\u70b9\uff1aAgentic AI \u6307\u80fd\u591f\u901a\u8fc7\u89c4\u5212\u3001\u63a8\u7406\u548c\u884c\u52a8\u81ea\u4e3b\u6267\u884c\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u7684\u7cfb\u7edf\u4e0e\u54cd\u5e94\u76f4\u63a5\u6307\u4ee4\u7684\u4f20\u7edf AI \u4e0d\u540c\uff0cAgentic AI \u662f\u4e3b\u52a8\u4e14\u76ee\u6807\u9a71\u52a8\u7684\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u6761\u4ef6\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
