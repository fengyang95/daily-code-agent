<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.SE](#cs.SE) [Total: 8]
- [tldr.article](#tldr.article) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance](https://arxiv.org/abs/2510.11905)
*Patrick Haller,Mark Ibrahim,Polina Kirichenko,Levent Sagun,Samuel J. Bell*

Main category: cs.CL

TL;DR: LLM内部知识表示对输入变化敏感，真实陈述与虚假陈述的可分离性在语义保留的扰动下会崩溃，表明LLM学习的是浅层、非鲁棒的知识表示。


<details>
  <summary>Details</summary>
Motivation: 探索LLM性能脆弱性是否源于不稳定的内部知识表示，特别是真实陈述与虚假陈述的可分离性在输入扰动下的变化。

Method: 通过应用语义保留的扰动（如拼写错误、重述）使样本分布外，在四个LLM家族、五个评估数据集和三种知识探测方法上评估表示可分离性的退化。

Result: 当样本表示与预训练数据相似度降低时，LLM内部真实陈述表示的可分离性会崩溃，表明知识表示对表面形式高度依赖。

Conclusion: LLM学习的是浅层、非鲁棒的知识表示，这解释了基准测试性能的脆弱性，并对真实性探测的实用性提出了根本挑战。

Abstract: For Large Language Models (LLMs) to be reliable, they must learn robust
knowledge that can be generally applied in diverse settings -- often unlike
those seen during training. Yet, extensive research has shown that LLM
performance can be brittle, with models exhibiting excessive sensitivity to
trivial input variations. In this work, we explore whether this brittleness is
a direct result of unstable internal knowledge representations. To explore this
question, we build on previous work showing that LLM representations encode
statement truthfulness -- i.e., true, factual statements can be easily
separated from false, inaccurate ones. Specifically, we test the robustness of
learned knowledge by evaluating representation separability on samples that
have undergone superficial transformations to drive them out-of-distribution
(OOD), such as typos or reformulations. By applying semantically-preserving
perturbations, we study how separability degrades as statements become more
OOD, across four LLM families, five evaluation datasets, and three knowledge
probing methods. Our results reveal that internal representations of statement
truthfulness collapse as the samples' presentations become less similar to
those seen during pre-training. While LLMs can often distinguish between true
and false statements when they closely resemble the pre-training data, this
ability is highly dependent on the statement's exact surface form. These
findings offer a possible explanation for brittle benchmark performance: LLMs
may learn shallow, non-robust knowledge representations that allow for only
limited generalizability. Our work presents a fundamental challenge for the
utility of truthfulness probes, and more broadly, calls for further research on
improving the robustness of learned knowledge representations.

</details>


### [2] [Scaling Long-Horizon LLM Agent via Context-Folding](https://arxiv.org/abs/2510.11967)
*Weiwei Sun,Miao Lu,Zhan Ling,Kang Liu,Xuesong Yao,Yiming Yang,Jiecao Chen*

Main category: cs.CL

TL;DR: 提出了Context-Folding框架，让LLM代理能够主动管理工作上下文，通过分支处理子任务并在完成后折叠中间步骤，仅保留结果摘要，从而解决长视野任务中的上下文长度限制问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在长视野任务中受到上下文长度的根本限制，需要一种有效的方法来管理上下文。

Method: 开发了Context-Folding框架和端到端强化学习框架FoldGRPO，通过过程奖励鼓励有效的任务分解和上下文管理，代理可以程序性地分支到子轨迹处理子任务，完成后折叠中间步骤。

Result: 在复杂长视野任务（Deep Research和SWE）上，折叠代理匹配或优于ReAct基线，同时使用的活动上下文小10倍，显著优于依赖基于摘要的上下文管理的模型。

Conclusion: Context-Folding框架有效解决了LLM代理在长视野任务中的上下文限制问题，通过主动的上下文管理实现了更好的性能。

Abstract: Large language model (LLM) agents are fundamentally constrained by context
length on long-horizon tasks. We introduce Context-Folding, a framework that
empowers agents to actively manage their working context. An agent can
procedurally branch into a sub-trajectory to handle a subtask and then fold it
upon completion, collapsing the intermediate steps while retaining a concise
summary of the outcome. To make this behavior learnable, we develop an
end-to-end reinforcement learning framework FoldGRPO with specific process
rewards to encourage effective task decomposition and context management. On
complex long-horizon tasks (Deep Research and SWE), our folding agent matches
or outperforms the ReAct baselines while using an active context 10$\times$
smaller and significantly outperforms models that rely on summarization-based
context management.

</details>


### [3] [SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation](https://arxiv.org/abs/2510.11997)
*Ryan Shea,Yunan Lu,Liang Qiu,Zhou Yu*

Main category: cs.CL

TL;DR: SAGE是一个用于多轮交互式智能体评估的用户模拟框架，通过整合业务上下文知识（如客户画像和产品目录）来生成更真实、多样的交互，能发现33%以上的智能体错误。


<details>
  <summary>Details</summary>
Motivation: 现有的多轮交互智能体评估方法依赖人工评估或通用用户模拟，缺乏领域特定知识，无法捕捉真实用户行为。

Method: 提出SAGE框架，整合自上而下的业务逻辑知识（理想客户画像）和自下而上的业务基础设施知识（产品目录、FAQ等），模拟真实客户行为。

Result: 该方法生成的交互更真实、多样，能识别出33%以上的智能体错误，有效支持错误发现和智能体迭代改进。

Conclusion: SAGE通过整合业务知识显著提升了多轮交互智能体评估的准确性和实用性。

Abstract: Evaluating multi-turn interactive agents is challenging due to the need for
human assessment. Evaluation with simulated users has been introduced as an
alternative, however existing approaches typically model generic users and
overlook the domain-specific principles required to capture realistic behavior.
We propose SAGE, a novel user Simulation framework for multi-turn AGent
Evaluation that integrates knowledge from business contexts. SAGE incorporates
top-down knowledge rooted in business logic, such as ideal customer profiles,
grounding user behavior in realistic customer personas. We further integrate
bottom-up knowledge taken from business agent infrastructure (e.g., product
catalogs, FAQs, and knowledge bases), allowing the simulator to generate
interactions that reflect users' information needs and expectations in a
company's target market. Through empirical evaluation, we find that this
approach produces interactions that are more realistic and diverse, while also
identifying up to 33% more agent errors, highlighting its effectiveness as an
evaluation tool to support bug-finding and iterative agent improvement.

</details>


### [4] [A Survey on Parallel Reasoning](https://arxiv.org/abs/2510.12164)
*Ziqi Wang,Boye Niu,Zipeng Gao,Zhi Zheng,Tong Xu,Linghui Meng,Zhongli Li,Jing Liu,Yilong Chen,Chen Zhu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.CL

TL;DR: 本文对并行推理这一新兴推理范式进行了系统性综述，阐明了其定义、分类、应用场景、挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，并行推理作为一种增强推理鲁棒性的新范式出现，旨在克服标准顺序方法的脆弱性并提升实际性能。

Method: 提出了并行推理的形式化定义，基于新分类法组织讨论先进技术，包括非交互式推理、交互式推理和效率导向的解码策略。

Result: 系统梳理了并行推理的研究进展，提供了技术分类和应用场景分析，并识别了核心挑战。

Conclusion: 本文为初学者提供了有用的路线图，鼓励更多研究改进并行推理方法。

Abstract: With the increasing capabilities of Large Language Models (LLMs), parallel
reasoning has emerged as a new inference paradigm that enhances reasoning
robustness by concurrently exploring multiple lines of thought before
converging on a final answer. It has become a significant trend to explore
parallel reasoning to overcome the fragility of standard sequential methods and
improve practical performance. In this paper, we aim to survey and summarize
the progress and challenges of parallel reasoning. We first present a formal
definition of parallel reasoning and clarify its distinction from related
concepts like Chain-of-Thought. Then, we organize and discuss advanced
techniques based on a novel taxonomy, including non-interactive reasoning,
interactive reasoning, and efficiency-focused decoding strategies.
Additionally, we explore various application scenarios, such as solving complex
problems and enhancing the reliability of LLM outputs.Finally, we highlight the
core challenges of parallel reasoning and suggest potential directions for
future research. We hope that our work can provide a useful roadmap for
beginners and encourage more research on improving parallel reasoning methods.
Related source can be avaliable in
https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.

</details>


### [5] [Towards Inference-time Scaling for Continuous Space Reasoning](https://arxiv.org/abs/2510.12167)
*Minghan Wang,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 该论文研究了将离散推理空间的推理技术（如多样本生成和奖励模型重排序）应用于连续推理空间的可行性，发现虽然生成多样性推理路径是可行的，但在连续空间中实现有效奖励模型重排序面临独特挑战。


<details>
  <summary>Details</summary>
Motivation: 探索已证明在离散推理空间有效的推理时扩展技术（多样本生成+奖励模型重排序）是否能够成功适应连续推理空间，以提升连续推理语言模型的性能。

Method: 使用COCONUT连续推理LM作为骨干模型，通过dropout-based采样生成多样性推理路径，进行Pass@N分析，并研究在连续空间中训练PRM和ORM模型的效果，通过几何属性和轨迹动态分析识别挑战。

Result: 生成多样性推理路径是可行的，Pass@N分析显示存在显著性能提升潜力，但离散空间的训练方法在连续空间仅带来边际改进，无法有效区分正确和错误推理。

Conclusion: 当前连续推理LM的训练框架不仅需要优化准确性，还需要明确融入可在推理时用于区分正确和错误思维的归纳偏置。

Abstract: Inference-time scaling through multiple sample generation in combination with
Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective
for text-based reasoning in large language models. This paper investigates
whether such established techniques can be successfully adapted to reasoning in
the continuous space, using COCONUT (Hao et al. 2024) continuous space
reasoning LM as the backbone. We demonstrate the feasibility of generating
diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on
the generated samples reveals the potential that could enable a significant
gain in performance akin to observed gain in the discrete space. However, we
highlight unique challenges faced for materializing this gain in the continuous
thought space. In particular, working recipes for data generation and training
PRM and ORM models in the discrete space unlocks only marginal improvements in
the continuous space. Through probing various aspects including geometric
properties and trajectory dynamics we identify the underlying reasons that
prevent effective discrimination between correct and incorrect reasoning
(essential for the functioning of PRM and ORM). Our findings reveal that
current limitations stem from the absence of key inductive biases in continuous
thought representations. We argue that the training frameworks for continuous
reasoning LMs require not only to optimize for accuracy but also to explicitly
incorporate inductive biases that could be utilized during inference-time for
discrimination of correct and incorrect thoughts.\footnote{Our code and data
will be publicly available.}

</details>


### [6] [LLM-REVal: Can We Trust LLM Reviewers Yet?](https://arxiv.org/abs/2510.12367)
*Rui Li,Jia-Chen Gu,Po-Nien Kung,Heming Xia,Junfeng liu,Xiangwen Kong,Zhifang Sui,Nanyun Peng*

Main category: cs.CL

TL;DR: 研究发现LLM作为审稿人存在系统性偏见：偏向LLM生成的论文风格，贬低包含批判性陈述的人类论文，这给学术公平带来风险。


<details>
  <summary>Details</summary>
Motivation: 探索LLM深度整合到学术工作流程（研究和评审）中可能带来的新风险，特别是对学术公平性的影响。

Method: 通过模拟实验，使用研究代理生成和修订论文，同时使用评审代理评估提交的论文，并进行人工标注验证。

Result: LLM审稿人系统性抬高LLM撰写论文的分数，显著低于人类撰写论文；持续低估包含批判性陈述的人类论文；存在语言特征偏见和批判性陈述厌恶两种主要偏见。

Conclusion: LLM作为审稿人存在严重偏见风险，需谨慎部署；但LLM指导的修订能提升论文质量，对早期研究人员和低质量论文有益。

Abstract: The rapid advancement of large language models (LLMs) has inspired
researchers to integrate them extensively into the academic workflow,
potentially reshaping how research is practiced and reviewed. While previous
studies highlight the potential of LLMs in supporting research and peer review,
their dual roles in the academic workflow and the complex interplay between
research and review bring new risks that remain largely underexplored. In this
study, we focus on how the deep integration of LLMs into both peer-review and
research processes may influence scholarly fairness, examining the potential
risks of using LLMs as reviewers by simulation. This simulation incorporates a
research agent, which generates papers and revises, alongside a review agent,
which assesses the submissions. Based on the simulation results, we conduct
human annotations and identify pronounced misalignment between LLM-based
reviews and human judgments: (1) LLM reviewers systematically inflate scores
for LLM-authored papers, assigning them markedly higher scores than
human-authored ones; (2) LLM reviewers persistently underrate human-authored
papers with critical statements (e.g., risk, fairness), even after multiple
revisions. Our analysis reveals that these stem from two primary biases in LLM
reviewers: a linguistic feature bias favoring LLM-generated writing styles, and
an aversion toward critical statements. These results highlight the risks and
equity concerns posed to human authors and academic research if LLMs are
deployed in the peer review cycle without adequate caution. On the other hand,
revisions guided by LLM reviews yield quality gains in both LLM-based and human
evaluations, illustrating the potential of the LLMs-as-reviewers for
early-stage researchers and enhancing low-quality papers.

</details>


### [7] [Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12460)
*Linfeng Gao,Baolong Bi,Zheng Yuan,Le Wang,Zerui Chen,Zhimin Wei,Shenghua Liu,Qinggang Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出了CLEAR框架，通过分析LLM内部隐藏状态来定位知识冲突，并采用冲突感知微调来提升RAG系统的忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统存在不忠实问题，模型响应与检索证据相矛盾。现有方法将LLM视为黑盒，忽视了LLM内部如何整合检索证据与参数化记忆的关键问题。

Method: 1) 对LLM隐藏状态表示进行探测分析；2) 将上下文分解为细粒度句子级知识；3) 使用隐藏状态探测定位冲突知识；4) 引入冲突感知微调指导模型准确整合检索证据。

Result: 在三个基准测试上的广泛实验表明，CLEAR显著提高了准确性和上下文忠实性，在不同冲突条件下始终优于强基线方法。

Conclusion: CLEAR框架通过分析LLM内部表示来定位知识冲突，有效提升了RAG系统的忠实性，为解决知识冲突问题提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (LLMs). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, decoding constraints, or reward-based fine-tuning. These
works treat the LLM as a black box and overlook a crucial question: how does
the LLM internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in LLMs and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.

</details>


### [8] [Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception](https://arxiv.org/abs/2510.12720)
*Ziyang Ma,Ruiyang Xu,Zhenghao Xing,Yunfei Chu,Yuxuan Wang,Jinzheng He,Jin Xu,Pheng-Ann Heng,Kai Yu,Junyang Lin,Eng Siong Chng,Xie Chen*

Main category: cs.CL

TL;DR: 提出了Omni-Detective数据生成管道和Omni-Cloze评估基准，用于提升多模态语言模型在音频-视觉细粒度感知方面的能力，解决了细节与幻觉共增长的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态语言模型在捕捉和描述细粒度细节方面能力有限，存在细节与幻觉共增长的问题，需要系统性的解决方案来提升细粒度感知能力。

Method: 开发了Omni-Detective代理式数据生成管道，集成工具调用来自主生成高质量多模态数据；训练了Audio-Captioner和Omni-Captioner模型；设计了Omni-Cloze填空式评估基准。

Result: Audio-Captioner在MMAU和MMAR基准上表现最佳，超越Gemini 2.5 Flash，与Gemini 2.5 Pro相当；Omni-Captioner在VDC基准上达到新SOTA，在video-SALMONN 2上实现细节与幻觉的最佳平衡。

Conclusion: Omni-Detective能有效生成高质量详细描述，Omni-Cloze为细粒度多模态感知提供了稳定可靠的评估方法，显著提升了多模态语言模型的细粒度感知能力。

Abstract: Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.

</details>


### [9] [Dr.LLM: Dynamic Layer Routing in LLMs](https://arxiv.org/abs/2510.12773)
*Ahmed Heakl,Martin Gubri,Salman Khan,Sangdoo Yun,Seong Joon Oh*

Main category: cs.CL

TL;DR: Dr.LLM是一个动态路由框架，通过轻量级路由器决定跳过、执行或重复Transformer层，在保持精度的同时提高计算效率


<details>
  <summary>Details</summary>
Motivation: 传统LLM对所有token都经过所有Transformer层，导致简单查询计算浪费，复杂推理深度不足。现有自适应深度方法依赖昂贵推理搜索或架构修改，且往往牺牲精度

Method: 使用蒙特卡洛树搜索(MCTS)推导高质量层配置，在计算预算下保持或提高精度。采用窗口池化稳定路由、焦点损失与类别平衡、瓶颈MLP路由器等技术

Result: 在ARC和DART任务上精度提升达+3.4%，平均每样本节省5层。路由器泛化到多个领域任务，精度仅下降0.85%同时保持效率，优于现有路由方法达+7.7%

Conclusion: Dr.LLM证明显式监督的路由器可以改造冻结LLM，实现预算感知、精度驱动的推理，无需修改基础权重

Abstract: Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving](https://arxiv.org/abs/2510.11769)
*Ruida Wang,Jiarui Yao,Rui Pan,Shizhe Diao,Tong Zhang*

Main category: cs.LG

TL;DR: 提出GAR框架，通过对抗性训练联合优化问题生成器和求解器，在可验证环境中实现问题生成与求解的协同进化，提升数学定理证明能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定问题集的强化学习方法训练效率低，难以应对复杂问题，需要更高效的训练框架。

Method: GAR框架包含问题生成器和求解器的对抗性循环训练，引入隐式课程学习机制，使任务难度与证明者能力同步提升。

Result: 在MiniF2F-Test基准上平均相对提升4.20%，ProofNet-Test上pass@32从22.58%提升至25.81%。

Conclusion: GAR建立了可验证环境下问题生成与求解协同进化的一般强化学习范式。

Abstract: Solving math problems through verifiable languages such as Lean has
significantly impacted both the mathematics and computer science communities.
Current state-of-the-art models are often trained with expensive online
Reinforcement Learning (RL) or expert iteration. However, these approaches rely
on fixed problem sets, which causes inefficient training and limits the model
to tackle complex problems. To overcome these limitations, we propose GAR:
Generative Adversarial Reinforcement learning, a comprehensive RL training
framework that jointly trains the problem composer and solver in an adversarial
loop. GAR introduces an implicit curriculum learning mechanism, which aligns
task difficulty with the prover's evolving capability. It thereby improves the
training efficiency and enables stronger performance of proving advanced
theorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and
DeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of
4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on
ProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR
establishes a general RL paradigm for co-evolution of problem generation and
solving under verifiable environments.

</details>


### [11] [ADARL: Adaptive Low-Rank Structures for Robust Policy Learning under Uncertainty](https://arxiv.org/abs/2510.11899)
*Chenliang Li,Junyu Leng,Jiaxiang Li,Youbang Sun,Shixiang Chen,Shahin Shahrampour,Alfredo Garcia*

Main category: cs.LG

TL;DR: AdaRL是一种双层优化框架，通过自适应调整策略复杂度与任务内在维度对齐来改进鲁棒强化学习，避免传统min-max优化的计算开销和保守策略问题。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒强化学习方法依赖嵌套min-max优化，计算成本高且产生过于保守的策略，需要更高效的鲁棒性方法。

Method: 采用双层优化框架：下层在固定秩约束下进行策略优化，从Wasserstein球中采样动态；上层自适应调整秩以平衡偏差-方差权衡，将策略参数投影到低秩流形上。

Result: 在MuJoCo连续控制基准测试中，AdaRL不仅优于固定秩基线和最先进的鲁棒RL方法，而且收敛到任务的内在秩。

Conclusion: 自适应低秩策略表示为模型不确定性下的鲁棒RL提供了高效且原则性的替代方案。

Abstract: Robust reinforcement learning (Robust RL) seeks to handle epistemic
uncertainty in environment dynamics, but existing approaches often rely on
nested min--max optimization, which is computationally expensive and yields
overly conservative policies. We propose \textbf{Adaptive Rank Representation
(AdaRL)}, a bi-level optimization framework that improves robustness by
aligning policy complexity with the intrinsic dimension of the task. At the
lower level, AdaRL performs policy optimization under fixed-rank constraints
with dynamics sampled from a Wasserstein ball around a centroid model. At the
upper level, it adaptively adjusts the rank to balance the bias--variance
trade-off, projecting policy parameters onto a low-rank manifold. This design
avoids solving adversarial worst-case dynamics while ensuring robustness
without over-parameterization. Empirical results on MuJoCo continuous control
benchmarks demonstrate that AdaRL not only consistently outperforms fixed-rank
baselines (e.g., SAC) and state-of-the-art robust RL methods (e.g., RNAC,
Parseval), but also converges toward the intrinsic rank of the underlying
tasks. These results highlight that adaptive low-rank policy representations
provide an efficient and principled alternative for robust RL under model
uncertainty.

</details>


### [12] [Efficient Restarts in Non-Stationary Model-Free Reinforcement Learning](https://arxiv.org/abs/2510.11933)
*Hiroshi Nonaka,Simon Ambrozak,Sofia R. Miskala-Dinc,Amedeo Ercole,Aviva Prins*

Main category: cs.LG

TL;DR: 提出了三种高效的重启范式用于无模型非平稳强化学习，解决了RestartQ-UCB算法中的完全遗忘和固定时间重启问题，显著降低了动态遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有RestartQ-UCB算法存在两个核心问题：完全遗忘（重启后丢失所有环境信息）和固定时间重启（不考虑当前环境动态与策略的兼容性），这限制了算法在非平稳环境中的性能。

Method: 引入了三种重启方法：部分重启、自适应重启和选择性重启，用于改进RestartQ-UCB和RANDOMIZEDQ算法，通过更智能的重启机制来适应环境变化。

Result: 在多种不同环境中实现了接近最优的实证性能，相对于RestartQ-UCB将动态遗憾降低了高达91%。

Conclusion: 提出的三种重启范式有效解决了非平稳强化学习中的重启设计问题，显著提升了算法性能。

Abstract: In this work, we propose three efficient restart paradigms for model-free
non-stationary reinforcement learning (RL). We identify two core issues with
the restart design of Mao et al. (2022)'s RestartQ-UCB algorithm: (1) complete
forgetting, where all the information learned about an environment is lost
after a restart, and (2) scheduled restarts, in which restarts occur only at
predefined timings, regardless of the incompatibility of the policy with the
current environment dynamics. We introduce three approaches, which we call
partial, adaptive, and selective restarts to modify the algorithms RestartQ-UCB
and RANDOMIZEDQ (Wang et al., 2025). We find near-optimal empirical performance
in multiple different environments, decreasing dynamic regret by up to $91$%
relative to RestartQ-UCB.

</details>


### [13] [Rethinking the Role of Dynamic Sparse Training for Scalable Deep Reinforcement Learning](https://arxiv.org/abs/2510.12096)
*Guozheng Ma,Lu Li,Zilin Wang,Haoyu Wang,Shengchao Hu,Leszek Rutkowski,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出模块特定训练(MST)框架，通过动态稀疏训练策略解决深度强化学习中模型规模扩大导致的性能下降问题，揭示了模块特定动态训练与架构改进的互补关系。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习中模型规模扩大往往导致性能下降，现有动态训练方法存在三个关键局限：对所有模块采用统一策略、评估基础架构不足、缺乏不同动态方法的系统比较。

Method: 通过跨模块和架构的全面调查，开发了模块特定训练(MST)框架，结合稀疏到稀疏、稠密到稀疏和稀疏到稠密等动态训练方法。

Result: 动态稀疏训练策略提供模块特定收益，与架构改进形成互补，MST框架在不修改算法的情况下显著提升了各种RL算法的可扩展性。

Conclusion: 模块特定动态训练与架构改进具有协同效应，MST框架为深度强化学习的可扩展性提供了实用解决方案。

Abstract: Scaling neural networks has driven breakthrough advances in machine learning,
yet this paradigm fails in deep reinforcement learning (DRL), where larger
models often degrade performance due to unique optimization pathologies such as
plasticity loss. While recent works show that dynamically adapting network
topology during training can mitigate these issues, existing studies have three
critical limitations: (1) applying uniform dynamic training strategies across
all modules despite encoder, critic, and actor following distinct learning
paradigms, (2) focusing evaluation on basic architectures without clarifying
the relative importance and interaction between dynamic training and
architectural improvements, and (3) lacking systematic comparison between
different dynamic approaches including sparse-to-sparse, dense-to-sparse, and
sparse-to-dense. Through comprehensive investigation across modules and
architectures, we reveal that dynamic sparse training strategies provide
module-specific benefits that complement the primary scalability foundation
established by architectural improvements. We finally distill these insights
into Module-Specific Training (MST), a practical framework that further
exploits the benefits of architectural improvements and demonstrates
substantial scalability gains across diverse RL algorithms without algorithmic
modifications.

</details>


### [14] [Self-Verifying Reflection Helps Transformers with CoT Reasoning](https://arxiv.org/abs/2510.12157)
*Zhongwei Yu,Wannian Xia,Xue Yan,Bo Xu,Haifeng Zhang,Yali Du,Jun Wang*

Main category: cs.LG

TL;DR: 提出了一个最小化推理框架，支持小型Transformer进行自我验证反思，证明自我验证能保证性能提升，实验显示小模型在整数乘法和数独任务中达到LLM级别性能。


<details>
  <summary>Details</summary>
Motivation: 分析LLM中反思机制的实际贡献，由于LLM在CoT中检测错误能力有限，需要明确反思如何带来实证改进。

Method: 构建最小化推理框架，支持小型Transformer进行无自然语言的基本自我验证反思，进行理论分析和实验验证。

Result: 小模型通过自我验证在训练和反思执行中获益，在整数乘法和数独任务中达到LLM级别性能；RL主要优化浅层统计模式而非真正减少验证错误。

Conclusion: 生成式Transformer与判别式验证的结合本质上促进了CoT推理，与模型规模和自然语言无关。

Abstract: Advanced large language models (LLMs) frequently reflect in reasoning
chain-of-thoughts (CoTs), where they self-verify the correctness of current
solutions and explore alternatives. However, given recent findings that LLMs
detect limited errors in CoTs, how reflection contributes to empirical
improvements remains unclear. To analyze this issue, in this paper, we present
a minimalistic reasoning framework to support basic self-verifying reflection
for small transformers without natural language, which ensures analytic clarity
and reduces the cost of comprehensive experiments. Theoretically, we prove that
self-verifying reflection guarantees improvements if verification errors are
properly bounded. Experimentally, we show that tiny transformers, with only a
few million parameters, benefit from self-verification in both training and
reflective execution, reaching remarkable LLM-level performance in integer
multiplication and Sudoku. Similar to LLM results, we find that reinforcement
learning (RL) improves in-distribution performance and incentivizes frequent
reflection for tiny transformers, yet RL mainly optimizes shallow statistical
patterns without faithfully reducing verification errors. In conclusion,
integrating generative transformers with discriminative verification inherently
facilitates CoT reasoning, regardless of scaling and natural language.

</details>


### [15] [Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs](https://arxiv.org/abs/2510.12233)
*Bowen Fan,Zhilin Guo,Xunkai Li,Yihan Zhou,Bing Zhou,Zhenjun Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出了IMDGA框架，一种可解释的多维图攻击方法，统一了对图结构和文本属性的对抗攻击，揭示了Graph-LLM的语义维度漏洞。


<details>
  <summary>Details</summary>
Motivation: Graph-LLM结合图神经网络和大语言模型增强了表达能力，但也引入了新的安全漏洞。现有攻击方法只针对结构或文本单一方面，缺乏统一的综合攻击框架。

Method: IMDGA框架包含三个紧密集成的模块，在保持可解释性的同时，协调多层次的图结构和文本特征扰动。

Result: 在多个数据集和架构上的实验表明，IMDGA在可解释性、攻击效果、隐蔽性和鲁棒性方面优于现有方法。

Conclusion: 该工作揭示了Graph-LLM中先前未被充分探索的语义维度漏洞，为提高其抗攻击能力提供了重要见解。

Abstract: Graph Neural Networks (GNNs) have become a pivotal framework for modeling
graph-structured data, enabling a wide range of applications from social
network analysis to molecular chemistry. By integrating large language models
(LLMs), text-attributed graphs (TAGs) enhance node representations with rich
textual semantics, significantly boosting the expressive power of graph-based
learning. However, this sophisticated synergy introduces critical
vulnerabilities, as Graph-LLMs are susceptible to adversarial attacks on both
their structural topology and textual attributes. Although specialized attack
methods have been designed for each of these aspects, no work has yet unified
them into a comprehensive approach. In this work, we propose the Interpretable
Multi-Dimensional Graph Attack (IMDGA), a novel human-centric adversarial
attack framework designed to orchestrate multi-level perturbations across both
graph structure and textual features. IMDGA utilizes three tightly integrated
modules to craft attacks that balance interpretability and impact, enabling a
deeper understanding of Graph-LLM vulnerabilities. Through rigorous theoretical
analysis and comprehensive empirical evaluations on diverse datasets and
architectures, IMDGA demonstrates superior interpretability, attack
effectiveness, stealthiness, and robustness compared to existing methods. By
exposing critical weaknesses in TAG representation learning, this work uncovers
a previously underexplored semantic dimension of vulnerability in Graph-LLMs,
offering valuable insights for improving their resilience. Our code and
resources are publicly available at
https://anonymous.4open.science/r/IMDGA-7289.

</details>


### [16] [Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development](https://arxiv.org/abs/2510.12253)
*Changfu Xu,Jianxiong Guo,Yuzhu Liang,Haiyang Huang,Haodong Zou,Xi Zheng,Shui Yu,Xiaowen Chu,Jiannong Cao,Tian Wang*

Main category: cs.LG

TL;DR: 这篇综述全面分析了扩散模型在强化学习中的应用，建立了双轴分类法，涵盖了从单智能体到多智能体领域的进展，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为领先的生成模型，为强化学习提供了多模态表达能力、稳定训练和轨迹级规划等关键优势，需要系统梳理这一新兴领域。

Method: 建立功能导向和技术导向的双轴分类法，分析扩散模型在RL流程中的不同角色，以及在在线与离线学习环境中的实现方式。

Result: 形成了多个DM-RL集成框架，展示了扩散模型在强化学习中的实际效用，并维护了相关资源的GitHub仓库。

Conclusion: 扩散模型为强化学习提供了有前景的新方法，特别是在处理多模态问题和复杂规划任务方面具有独特优势。

Abstract: Diffusion Models (DMs), as a leading class of generative models, offer key
advantages for reinforcement learning (RL), including multi-modal
expressiveness, stable training, and trajectory-level planning. This survey
delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We
first provide an overview of RL, highlighting its challenges, and then
introduce the fundamental concepts of DMs, investigating how they are
integrated into RL frameworks to address key challenges in this research field.
We establish a dual-axis taxonomy that organizes the field along two orthogonal
dimensions: a function-oriented taxonomy that clarifies the roles DMs play
within the RL pipeline, and a technique-oriented taxonomy that situates
implementations across online versus offline learning regimes. We also provide
a comprehensive examination of this progression from single-agent to
multi-agent domains, thereby forming several frameworks for DM-RL integration
and highlighting their practical utility. Furthermore, we outline several
categories of successful applications of diffusion-based RL across diverse
domains, discuss open research issues of current methodologies, and highlight
key directions for future research to advance the field. Finally, we summarize
the survey to identify promising future development directions. We are actively
maintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for
papers and other related resources to apply DMs for RL.

</details>


### [17] [Deep SPI: Safe Policy Improvement via World Models](https://arxiv.org/abs/2510.12312)
*Florent Delgrange,Raphael Avalos,Willem Röpke*

Main category: cs.LG

TL;DR: 该论文提出了DeepSPI算法，将安全策略改进理论扩展到在线强化学习场景，结合世界模型和表示学习，确保策略更新的单调改进和收敛。


<details>
  <summary>Details</summary>
Motivation: 现有安全策略改进理论主要关注离线、表格化强化学习，缺乏在在线、深度强化学习环境中的理论保证。

Method: 开发理论框架，限制策略更新在当前策略的邻域内，结合局部转移和奖励损失与正则化策略更新，提出DeepSPI算法。

Result: 在ALE-57基准测试中，DeepSPI匹配或超越了PPO和DeepMDPs等强基线方法，同时保持理论保证。

Conclusion: 成功将经典安全策略改进理论扩展到在线深度强化学习，建立了表示质量与预测损失的理论联系。

Abstract: Safe policy improvement (SPI) offers theoretical control over policy updates,
yet existing guarantees largely concern offline, tabular reinforcement learning
(RL). We study SPI in general online settings, when combined with world model
and representation learning. We develop a theoretical framework showing that
restricting policy updates to a well-defined neighborhood of the current policy
ensures monotonic improvement and convergence. This analysis links transition
and reward prediction losses to representation quality, yielding online, "deep"
analogues of classical SPI theorems from the offline RL literature. Building on
these results, we introduce DeepSPI, a principled on-policy algorithm that
couples local transition and reward losses with regularised policy updates. On
the ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including
PPO and DeepMDPs, while retaining theoretical guarantees.

</details>


### [18] [Finite-time Convergence Analysis of Actor-Critic with Evolving Reward](https://arxiv.org/abs/2510.12334)
*Rui Hu,Yu Chen,Longbo Huang*

Main category: cs.LG

TL;DR: 该论文首次对马尔可夫采样下具有演化奖励函数的单时间尺度actor-critic算法进行了有限时间收敛性分析，证明了在奖励参数缓慢变化时仍能达到O(1/√T)的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 许多实用的强化学习算法使用演化奖励函数（如奖励塑造、熵正则化、课程学习），但这些技术的理论基础尚未充分发展，需要理论分析来支持这些流行方法。

Method: 采用单时间尺度actor-critic算法，在马尔可夫采样下分析奖励参数随时间变化的情况，考虑奖励参数变化对策略优化和价值估计的影响。

Result: 在标准假设下，推导了actor和critic误差的非渐近界，证明当奖励参数演化足够慢时，可以达到O(1/√T)的收敛速率，与静态奖励的最佳已知速率相匹配。

Conclusion: 该工作为许多流行的RL技术提供了理论基础，表明在奖励函数演化的情况下仍能保持良好收敛性能。

Abstract: Many popular practical reinforcement learning (RL) algorithms employ evolving
reward functions-through techniques such as reward shaping, entropy
regularization, or curriculum learning-yet their theoretical foundations remain
underdeveloped. This paper provides the first finite-time convergence analysis
of a single-timescale actor-critic algorithm in the presence of an evolving
reward function under Markovian sampling. We consider a setting where the
reward parameters may change at each time step, affecting both policy
optimization and value estimation. Under standard assumptions, we derive
non-asymptotic bounds for both actor and critic errors. Our result shows that
an $O(1/\sqrt{T})$ convergence rate is achievable, matching the best-known rate
for static rewards, provided the reward parameters evolve slowly enough. This
rate is preserved when the reward is updated via a gradient-based rule with
bounded gradient and on the same timescale as the actor and critic, offering a
theoretical foundation for many popular RL techniques. As a secondary
contribution, we introduce a novel analysis of distribution mismatch under
Markovian sampling, improving the best-known rate by a factor of $\log^2T$ in
the static-reward case.

</details>


### [19] [Learning-To-Measure: In-context Active Feature Acquisition](https://arxiv.org/abs/2510.12624)
*Yuta Kobayashi,Zilin Jing,Jiayu Yao,Hongseok Namkoong,Shalmali Joshi*

Main category: cs.LG

TL;DR: 提出元主动特征获取（meta-AFA）问题，开发Learning-to-Measure（L2M）方法，通过序列建模预训练实现可靠的不确定性量化，无需逐任务重新训练即可跨任务学习特征获取策略。


<details>
  <summary>Details</summary>
Motivation: 传统主动特征获取方法通常针对单一预定任务，限制了可扩展性。现实应用中需要从具有系统性缺失特征和有限任务特定标签的回顾性数据中学习，因此需要能够跨任务学习的元AFA方法。

Method: L2M包含：1）对未见任务的可靠不确定性量化；2）基于不确定性的贪婪特征获取代理，最大化条件互信息。采用序列建模或自回归预训练方法，直接在具有回顾性缺失的数据集上运行，进行上下文中的元AFA任务。

Result: 在合成和真实世界表格基准测试中，L2M匹配或超越了任务特定基线方法，特别是在标签稀缺和缺失率高的情况下表现优异。

Conclusion: L2M成功解决了元AFA问题，能够跨任务学习特征获取策略，在数据稀缺和高缺失率情况下仍保持良好性能，无需逐任务重新训练。

Abstract: Active feature acquisition (AFA) is a sequential decision-making problem
where the goal is to improve model performance for test instances by adaptively
selecting which features to acquire. In practice, AFA methods often learn from
retrospective data with systematic missingness in the features and limited
task-specific labels. Most prior work addresses acquisition for a single
predetermined task, limiting scalability. To address this limitation, we
formalize the meta-AFA problem, where the goal is to learn acquisition policies
across various tasks. We introduce Learning-to-Measure (L2M), which consists of
i) reliable uncertainty quantification over unseen tasks, and ii) an
uncertainty-guided greedy feature acquisition agent that maximizes conditional
mutual information. We demonstrate a sequence-modeling or autoregressive
pre-training approach that underpins reliable uncertainty quantification for
tasks with arbitrary missingness. L2M operates directly on datasets with
retrospective missingness and performs the meta-AFA task in-context,
eliminating per-task retraining. Across synthetic and real-world tabular
benchmarks, L2M matches or surpasses task-specific baselines, particularly
under scarce labels and high missingness.

</details>


### [20] [Laminar: A Scalable Asynchronous RL Post-Training Framework](https://arxiv.org/abs/2510.12633)
*Guangming Sheng,Yuxuan Tong,Borui Wan,Wang Zhang,Chaobo Jia,Xibin Wu,Yuqi Wu,Xiang Li,Chi Zhang,Yanghua Peng,Haibin Lin,Xin Liu,Chuan Wu*

Main category: cs.LG

TL;DR: Laminar是一个可扩展的强化学习后训练系统，通过轨迹级异步和完全解耦架构解决RL训练中的GPU利用率问题，在1024-GPU集群上实现了5.48倍训练吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有RL框架的可扩展性受限，因为RL轨迹生成的极端长尾偏斜导致严重的GPU利用率不足。全局权重同步机制不适合RL训练中高度偏斜和演化的轨迹生成延迟分布。

Method: 提出Laminar系统：1）用中继工作者层级替代全局更新，实现异步细粒度权重同步；2）动态重新打包机制将长尾轨迹整合到专用rollout上；3）完全解耦架构隔离故障。

Result: 在1024-GPU集群上的评估显示，Laminar相比最先进系统实现了高达5.48倍的训练吞吐量加速，同时减少了模型收敛时间。

Conclusion: 轨迹级异步和完全解耦架构能够有效解决RL后训练中的可扩展性问题，显著提升训练效率和系统鲁棒性。

Abstract: Reinforcement learning (RL) post-training for Large Language Models (LLMs) is
now scaling to large clusters and running for extended durations to enhance
model reasoning performance. However, the scalability of existing RL frameworks
is limited, as extreme long-tail skewness in RL trajectory generation causes
severe GPU underutilization. Current asynchronous RL systems attempt to
mitigate this, but they rely on global weight synchronization between the actor
and all rollouts, which creates a rigid model update schedule. This global
synchronization is ill-suited for the highly skewed and evolving distribution
of trajectory generation latency in RL training, crippling training efficiency.
Our key insight is that efficient scaling requires breaking this lockstep
through trajectory-level asynchrony, which generates and consumes each
trajectory independently. We propose Laminar, a scalable and robust RL
post-training system built on a fully decoupled architecture. First, we replace
global updates with a tier of relay workers acting as a distributed parameter
service. This enables asynchronous and fine-grained weight synchronization,
allowing rollouts to pull the latest weight anytime without stalling the
actor's training loop. Second, a dynamic repack mechanism consolidates
long-tail trajectories onto a few dedicated rollouts, maximizing generation
throughput. The fully decoupled design also isolates failures, ensuring
robustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows
that Laminar achieves up to 5.48$\times$ training throughput speedup over
state-of-the-art systems, while reducing model convergence time.

</details>


### [21] [Expert or not? assessing data quality in offline reinforcement learning](https://arxiv.org/abs/2510.12638)
*Arip Asadulaev,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 提出Bellman Wasserstein距离(BWD)作为离线数据集质量的评估指标，无需训练智能体即可预测离线强化学习算法的性能表现。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习数据集质量差异很大，但难以预先评估，因为数据来源和技能组成未知。需要一种无需训练智能体就能估计数据集质量的方法。

Method: 研究从简单累积奖励到基于学习价值估计器的多种代理指标，引入BWD——一种价值感知的最优传输评分，通过行为评论家和状态条件最优传输公式计算，无需环境交互或完整策略优化。

Result: 在D4RL MuJoCo任务中，BWD与多个离线RL算法的oracle性能得分强相关，能有效预测标准智能体在给定数据集上的表现。作为正则化器使用时，BWD能提升回报。

Conclusion: BWD等价值感知的分布信号是筛选离线RL数据集和优化策略的实用工具。

Abstract: Offline reinforcement learning (RL) learns exclusively from static datasets,
without further interaction with the environment. In practice, such datasets
vary widely in quality, often mixing expert, suboptimal, and even random
trajectories. The choice of algorithm therefore depends on dataset fidelity.
Behavior cloning can suffice on high-quality data, whereas mixed- or
low-quality data typically benefits from offline RL methods that stitch useful
behavior across trajectories. Yet in the wild it is difficult to assess dataset
quality a priori because the data's provenance and skill composition are
unknown. We address the problem of estimating offline dataset quality without
training an agent. We study a spectrum of proxies from simple cumulative
rewards to learned value based estimators, and introduce the Bellman
Wasserstein distance (BWD), a value aware optimal transport score that measures
how dissimilar a dataset's behavioral policy is from a random reference policy.
BWD is computed from a behavioral critic and a state conditional OT
formulation, requiring no environment interaction or full policy optimization.
Across D4RL MuJoCo tasks, BWD strongly correlates with an oracle performance
score that aggregates multiple offline RL algorithms, enabling efficient
prediction of how well standard agents will perform on a given dataset. Beyond
prediction, integrating BWD as a regularizer during policy optimization
explicitly pushes the learned policy away from random behavior and improves
returns. These results indicate that value aware, distributional signals such
as BWD are practical tools for triaging offline RL datasets and policy
optimization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [AI Agents for the Dhumbal Card Game: A Comparative Study](https://arxiv.org/abs/2510.11736)
*Sahaj Raj Malla*

Main category: cs.AI

TL;DR: 本研究系统比较了基于规则、搜索和学习的AI代理在Dhumbal纸牌游戏中的表现，发现基于规则的激进策略在1024轮模拟中胜率最高（88.3%），优于ISMCTS和PPO方法。


<details>
  <summary>Details</summary>
Motivation: 评估不同AI策略在具有文化意义的不完全信息多人纸牌游戏Dhumbal中的表现，为AI研究提供可复现框架并支持文化游戏的数字保存。

Method: 实现多种AI代理（启发式、MCTS、ISMCTS、DQN、PPO、随机基线），通过类别内锦标赛和跨类别锦标赛进行评估，使用统计方法分析性能指标。

Result: 基于规则的激进代理胜率最高（88.3%），ISMCTS为9.0%，PPO为1.5%。激进策略通过有效利用Jhyap声明获得优势。

Conclusion: 在Dhumbal游戏中，简单启发式方法优于复杂学习算法，为不完全信息游戏中的AI研究提供了重要见解。

Abstract: This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a
culturally significant multiplayer card game with imperfect information,
through a systematic comparison of rule-based, search-based, and learning-based
strategies. We formalize Dhumbal's mechanics and implement diverse agents,
including heuristic approaches (Aggressive, Conservative, Balanced,
Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and
Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning
approaches including Deep Q-Network (DQN) and Proximal Policy Optimization
(PPO), and a random baseline. Evaluation involves within-category tournaments
followed by a cross-category championship. Performance is measured via win
rate, economic outcome, Jhyap success, cards discarded per round, risk
assessment, and decision efficiency. Statistical significance is assessed using
Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95%
confidence intervals (CI). Across 1024 simulated rounds, the rule-based
Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]),
outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of
Jhyap declarations. The study contributes a reproducible AI framework, insights
into heuristic efficacy under partial information, and open-source code,
thereby advancing AI research and supporting digital preservation of cultural
games.

</details>


### [23] [Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations](https://arxiv.org/abs/2510.11822)
*Suryaansh Jain,Umair Z. Ahmed,Shubham Sahai,Ben Leong*

Main category: cs.AI

TL;DR: LLM评估器存在严重正向偏差，难以识别无效输出。论文提出少数否决策略和基于回归的框架来缓解这一问题，在代码反馈任务上显著提升了评估准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为评估器的方法存在严重正向偏差，虽然能准确识别有效输出，但对无效输出的识别率很低，导致可靠性评分虚高。

Method: 提出少数否决策略来缓解偏差，并设计基于回归的框架直接建模验证器偏差，使用少量人工标注数据进行校准。

Result: 在366个高中Python程序的代码反馈任务上，回归方法将最大绝对误差降至1.2%，比14个最先进LLM的集成方法提升2倍。

Conclusion: LLM评估器的正向偏差问题需要专门方法解决，提出的回归框架能有效提高评估精度。

Abstract: New Large Language Models (LLMs) become available every few weeks, and modern
application developers confronted with the unenviable task of having to decide
if they should switch to a new model. While human evaluation remains the gold
standard, it is costly and unscalable. The state-of-the-art approach is to use
LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw:
LLMs exhibit a strong positive bias. We provide empirical evidence showing that
while LLMs can identify valid outputs with high accuracy (i.e., True Positive
Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True
Negative Rate <25%). This systematic bias, coupled with class imbalance, often
leads to inflated reliability scores.
  While ensemble-based methods like majority voting can help, we show that they
are not good enough. We introduce an optimal minority-veto strategy that is
resilient to missing data and mitigates this bias to a large extent. For
scenarios requiring even higher precision, we propose a novel regression-based
framework that directly models the validator bias using a small set of
human-annotated ground truth data. On a challenging code feedback task over 366
high-school Python programs, our regression approach reduces the maximum
absolute error to just 1.2%, achieving a 2x improvement over the
best-performing ensemble of 14 state-of-the-art LLMs.

</details>


### [24] [Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation](https://arxiv.org/abs/2510.11977)
*Sayash Kapoor,Benedikt Stroebl,Peter Kirgis,Nitya Nadgir,Zachary S Siegel,Boyi Wei,Tianci Xue,Ziru Chen,Felix Chen,Saiteja Utpala,Franck Ndzomga,Dheeraj Oruganty,Sophie Luskin,Kangheng Liu,Botao Yu,Amit Arora,Dongyoon Hahm,Harsh Trivedi,Huan Sun,Juyong Lee,Tengjun Jin,Yifan Mai,Yifei Zhou,Yuxuan Zhu,Rishi Bommasani,Daniel Kang,Dawn Song,Peter Henderson,Yu Su,Percy Liang,Arvind Narayanan*

Main category: cs.AI

TL;DR: 提出了HAL（Holistic Agent Leaderboard）来解决AI智能体评估中的挑战，通过标准化评估框架、三维分析和LLM辅助日志检查，揭示智能体行为的深层洞察。


<details>
  <summary>Details</summary>
Motivation: AI智能体在复杂任务中应用广泛，但现有评估方法存在诸多问题，无法准确理解智能体的真实性能，需要更全面可靠的评估体系。

Method: 1) 开发标准化评估框架，在数百个VM上并行评估；2) 进行模型、支架和基准测试的三维分析；3) 使用LLM辅助日志检查发现未报告行为。

Result: 执行了21,730次智能体测试，覆盖9个模型和9个基准测试，发现推理努力增加反而降低准确率等反直觉结果，并识别出智能体在任务中搜索基准测试而非解决问题的行为。

Conclusion: 通过标准化智能体评估方法并解决常见陷阱，希望推动研究从追求基准测试高分转向开发在现实世界中可靠工作的智能体。

Abstract: AI agents have been developed for complex real-world tasks from coding to
customer service. But AI agent evaluations suffer from many challenges that
undermine our understanding of how well agents really work. We introduce the
Holistic Agent Leaderboard (HAL) to address these challenges. We make three
main contributions. First, we provide a standardized evaluation harness that
orchestrates parallel evaluations across hundreds of VMs, reducing evaluation
time from weeks to hours while eliminating common implementation bugs. Second,
we conduct three-dimensional analysis spanning models, scaffolds, and
benchmarks. We validate the harness by conducting 21,730 agent rollouts across
9 models and 9 benchmarks in coding, web navigation, science, and customer
service with a total cost of about $40,000. Our analysis reveals surprising
insights, such as higher reasoning effort reducing accuracy in the majority of
runs. Third, we use LLM-aided log inspection to uncover previously unreported
behaviors, such as searching for the benchmark on HuggingFace instead of
solving a task, or misusing credit cards in flight booking tasks. We share all
agent logs, comprising 2.5B tokens of language model calls, to incentivize
further research into agent behavior. By standardizing how the field evaluates
agents and addressing common pitfalls in agent evaluation, we hope to shift the
focus from agents that ace benchmarks to agents that work reliably in the real
world.

</details>


### [25] [Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation](https://arxiv.org/abs/2510.12047)
*Soohan Lim,Joonghyuk Hahn,Hyunwoo Park,Sang-Ki Ko,Yo-Sub Han*

Main category: cs.AI

TL;DR: PACT是一个评估LLM生成代码合约遵从性的框架，通过扩展HumanEval+和MBPP+基准，引入合约违反测试用例，显著提升模型对合约的遵从能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准主要评估功能正确性，但忽略了真实软件中合约遵从性这一关键方面，导致无法衡量代码的鲁棒性和可靠性。

Method: 扩展HumanEval+和MBPP+基准，构建专注于合约违反的测试套件语料库，通过不同提示条件系统分析代码生成，并引入新的量化指标。

Result: 在提示中加入合约违反测试用例相比仅使用合约描述，能显著增强模型对合约的遵从能力。

Conclusion: PACT通过揭示传统基准忽略的关键错误，提供了评估LLM生成代码在功能和合约遵从性方面鲁棒性的严格可解释指标。

Abstract: Prevailing code generation benchmarks, such as HumanEval+ and MBPP+,
primarily evaluate large language models (LLMs) with pass@k on functional
correctness using well-formed inputs. However, they ignore a crucial aspect of
real-world software: adherence to contracts-the preconditions and validity
constraints that dictate how ill-formed inputs must be rejected. This critical
oversight means that existing benchmarks fail to measure, and models
consequently fail to generate, truly robust and reliable code snippets. We
introduce PACT, a program assessment and contract-adherence evaluation
framework, to bridge this gap. PACT is the first framework designed to
systematically evaluate and enhance contract-adherence in LLM-generated code
snippets alongside functional correctness. PACT's contributions are threefold:
First, it provides a comprehensive test-suite corpus focused on contract
violations, extending HumanEval+ and MBPP+. Second, it enables a systematic
analysis of code generation under varied prompting conditions. This analysis
demonstrates that augmenting prompts with contract-violating test cases
significantly enhance a model's ability to respect contracts compared to using
contract description alone. Finally, it introduces novel metrics to rigorously
quantify contract adherence in both test generation and code generation. By
revealing critical errors that conventional benchmarks overlook, PACT provides
the rigorous and interpretable metrics to evaluate the robustness of
LLM-generated code snippets in both functionality and contract-adherence.Our
code and data are available at https://github.com/suhanmen/PACT.

</details>


### [26] [ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://arxiv.org/abs/2510.12063)
*Sunzhu Li,Zhiyu Lin,Shuling Yang,Jiale Zhao,Wei Chen*

Main category: cs.AI

TL;DR: ThinkPilot是一个无需训练即可自动优化大型推理模型（LRMs）推理过程的框架，通过进化过程生成think-prefixes来引导模型实现更优性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型存在推理效率低下和偏离目标的问题，而无需训练的方法要么局限于刚性启发式方法，要么只能提供描述性、非可操作的分析。

Method: 使用进化过程生成think-prefixes，这些指令基于推理行为分类学驱动进化，引导模型实现更优性能。

Result: 显著改善了准确率-长度的权衡关系，大幅提升了安全性（如将DeepSeek-R1-Distill-Qwen-32B的StrongREJECT分数从27.0%降至0.7），增强了指令跟随能力，并能与现有基于训练的方法协同工作。

Conclusion: think-prefixes能够可靠地控制LRMs的推理行为，不同任务对特定行为分布有强烈偏好，通过自动识别和激发这些行为，ThinkPiot提供了一个可泛化的框架来使LRMs推理与任务需求对齐。

Abstract: Large Reasoning Models (LRMs) are powerful, but they still suffer from
inefficient and off-target reasoning. Currently, training-free methods are
limited to either rigid heuristics or descriptive, non-actionable analyses. In
this paper, we introduce ThinkPilot, a training-free framework that
automatically optimizes LRMs reasoning. It uses an evolutionary process to
generate think-prefixes, which are instructions that evolve driven by a
taxonomy of reasoning behaviors to guide models toward superior performance.
Extensive experiments demonstrate ThinkPilot's broad effectiveness: it
significantly improves the accuracy-length trade-off for efficient reasoning,
drastically improves safety (for example, cutting the StrongREJECT score of
DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction
following. It also synergizes with existing training-based methods. Our
analysis reveals that think-prefixes can reliably control LRMs' reasoning
behaviors, and that different tasks have strong preferences for specific
behavioral distributions. By automatically identifying and eliciting these
behaviors, ThinkPilot provides a generalizable framework for aligning LRMs
reasoning with task demands. Data and code are available at
https://github.com/teqkilla/ThinkPilot

</details>


### [27] [AI Agents as Universal Task Solvers](https://arxiv.org/abs/2510.12066)
*Alessandro Achille,Stefano Soatto*

Main category: cs.AI

TL;DR: 本文重新诠释了AI智能体中的学习角色，将其视为具有计算能力的随机动力系统，强调时间在学习推理中的基础作用，并提出从归纳学习转向转导学习的范式转变。


<details>
  <summary>Details</summary>
Motivation: 探讨AI推理智能体是否具有通用性，能否解决任何可计算任务，以及学习推理的关键因素是什么——是模型规模还是训练数据规模。

Method: 将AI智能体重新解释为计算能力的随机动力系统，提出转导学习范式，关注算法结构而非数据分布，并分析时间在推理中的关键作用。

Result: 证明了通用求解器使用过去数据实现的最优加速与其算法信息紧密相关，推导出推理时间与训练时间之间的幂律缩放关系，并发现模型规模扩展可能导致智能体行为像专家但缺乏真正智能。

Conclusion: 在扩展推理模型时，应该优化的关键量是时间而非准确性，时间在学习中的关键作用至今只被间接考虑。

Abstract: AI reasoning agents are already able to solve a variety of tasks by deploying
tools, simulating outcomes of multiple hypotheses and reflecting on them. In
doing so, they perform computation, although not in the classical sense --
there is no program being executed. Still, if they perform computation, can AI
agents be universal? Can chain-of-thought reasoning solve any computable task?
How does an AI Agent learn to reason? Is it a matter of model size? Or training
dataset size?
  In this work, we reinterpret the role of learning in the context of AI
Agents, viewing them as compute-capable stochastic dynamical systems, and
highlight the role of time in a foundational principle for learning to reason.
In doing so, we propose a shift from classical inductive learning to
transductive learning -- where the objective is not to approximate the
distribution of past data, but to capture their algorithmic structure to reduce
the time needed to find solutions to new tasks.
  Transductive learning suggests that, counter to Shannon's theory, a key role
of information in learning is about reduction of time rather than
reconstruction error. In particular, we show that the optimal speed-up that a
universal solver can achieve using past data is tightly related to their
algorithmic information. Using this, we show a theoretical derivation for the
observed power-law scaling of inference time versus training time. We then show
that scaling model size can lead to behaviors that, while improving accuracy on
benchmarks, fail any reasonable test of intelligence, let alone
super-intelligence: In the limit of infinite space and time, large models can
behave as savants, able to brute-force through any task without any insight.
Instead, we argue that the key quantity to optimize when scaling reasoning
models is time, whose critical role in learning has so far only been indirectly
considered.

</details>


### [28] [Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models](https://arxiv.org/abs/2510.12080)
*Rabimba Karanjai,Yang Lu,Ranjith Chodavarapu,Lei Xu,Weidong Shi*

Main category: cs.AI

TL;DR: 本文研究了大语言模型处理随机性任务的能力，发现虽然LLM能生成具有一定随机性的输出，但表现不一致且与预期行为存在显著偏差。


<details>
  <summary>Details</summary>
Motivation: 随着LLM技术的快速发展，许多应用（如随机决策、游戏、调度、AI代理和密码学任务）都需要随机性，但LLM在处理随机性方面的能力尚不明确。

Method: 设计了一系列实验，考虑外部工具可访问性、任务类型、模型状态（新鲜vs非新鲜）和提示策略等因素，涵盖生成随机数、随机字符串、洗牌项目以及使用熵和NIST随机性测试套件评估随机性质量等任务。

Result: LLM能够生成表现出一定随机性的输出，但性能不一致，且经常显著偏离预期行为。

Conclusion: 实验结果表明LLM在处理涉及随机性的任务时存在关键局限性，需要在这些方面进行改进才能有效处理随机性任务。

Abstract: The rapid advancement of large language model (LLM) technology has led to
diverse applications, many of which inherently require randomness, such as
stochastic decision-making, gaming, scheduling, AI agents, and
cryptography-related tasks. However, the capabilities of LLMs in handling
randomness, particularly in generating and utilizing random numbers
effectively, remain unclear. This paper investigates the capacity of LLMs for
handling tasks that involve randomness through a series of experiments. We
designed a set of experiments that consider various factors that can influence
an LLM's performance in tasks involving randomness, such as accessibility to
external tools, types of tasks, model states (fresh vs. non-fresh), and
prompting strategies. The experiments cover a range of tasks, including
generating random numbers, generating random strings such as passwords,
shuffling items, and evaluating the quality of randomness using entropy and the
NIST randomness test-suite. Our findings reveal that while LLMs can generate
outputs that exhibit some degree of randomness, their performance is
inconsistent and often deviates significantly from the expected behavior. The
analysis of the experimental results highlights key limitations and areas where
improvement is needed for the LLMs to effectively handle tasks involving
randomness

</details>


### [29] [One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration](https://arxiv.org/abs/2510.12088)
*Zaid Khan,Archiki Prasad,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.AI

TL;DR: OneLife框架通过概率编程和条件激活的程序化法则学习复杂随机环境的世界动态，在单次探索的苛刻条件下成功学习关键环境动态，并在状态排序和状态保真度评估中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂随机环境中，代理只有"一次生命"来探索敌对环境且无人指导的挑战性场景，自主构建程序化世界模型。

Method: 使用条件激活的程序化法则在概率编程框架中建模世界动态，每个法则采用前提-效果结构，在相关世界状态中激活，创建动态计算图以避免扩展挑战。

Result: 在Crafter-OO环境中测试，OneLife在23个场景中的16个上优于强基线，能够从最少无指导交互中成功学习关键环境动态，模拟推演成功识别出更优策略。

Conclusion: 为自主构建未知复杂环境的程序化世界模型奠定了基础，证明了在苛刻约束下学习随机动态的可行性。

Abstract: Symbolic world modeling requires inferring and representing an environment's
transitional dynamics as an executable program. Prior work has focused on
largely deterministic environments with abundant interaction data, simple
mechanics, and human guidance. We address a more realistic and challenging
setting, learning in a complex, stochastic environment where the agent has only
"one life" to explore a hostile environment without human guidance. We
introduce OneLife, a framework that models world dynamics through
conditionally-activated programmatic laws within a probabilistic programming
framework. Each law operates through a precondition-effect structure,
activating in relevant world states. This creates a dynamic computation graph
that routes inference and optimization only through relevant laws, avoiding
scaling challenges when all laws contribute to predictions about a complex,
hierarchical state, and enabling the learning of stochastic dynamics even with
sparse rule activation. To evaluate our approach under these demanding
constraints, we introduce a new evaluation protocol that measures (a) state
ranking, the ability to distinguish plausible future states from implausible
ones, and (b) state fidelity, the ability to generate future states that
closely resemble reality. We develop and evaluate our framework on Crafter-OO,
our reimplementation of the Crafter environment that exposes a structured,
object-oriented symbolic state and a pure transition function that operates on
that state alone. OneLife can successfully learn key environment dynamics from
minimal, unguided interaction, outperforming a strong baseline on 16 out of 23
scenarios tested. We also test OneLife's planning ability, with simulated
rollouts successfully identifying superior strategies. Our work establishes a
foundation for autonomously constructing programmatic world models of unknown,
complex environments.

</details>


### [30] [ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents](https://arxiv.org/abs/2510.12194)
*Linyi Yang,Yixuan Weng*

Main category: cs.AI

TL;DR: ResearStudio是首个开源框架，将实时人工控制置于核心，通过协作工作坊设计实现AI主导、人工辅助和人工主导、AI辅助模式间的平滑切换。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理以"发射后不管"模式运行，用户无法在执行过程中修正错误或添加专家知识，需要一种支持实时人工控制的系统。

Method: 采用分层规划器-执行器架构，将每个步骤写入实时"计划即文档"，通过快速通信层将每个操作、文件变更和工具调用流式传输到Web界面，用户可以随时暂停、编辑计划和代码、运行自定义命令并恢复执行。

Result: 在完全自主模式下，ResearStudio在GAIA基准测试中达到最先进水平，超越了OpenAI的DeepResearch和Manus等系统。

Conclusion: 强大的自动化性能和细粒度人工控制可以共存，为安全可控的研究代理开发提供了新方向。

Abstract: Current deep-research agents run in a ''fire-and-forget'' mode: once started,
they give users no way to fix errors or add expert knowledge during execution.
We present ResearStudio, the first open-source framework that places real-time
human control at its core. The system follows a Collaborative Workshop design.
A hierarchical Planner-Executor writes every step to a live
''plan-as-document,'' a fast communication layer streams each action, file
change, and tool call to a web interface. At any moment, the user can pause the
run, edit the plan or code, run custom commands, and resume -- switching
smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In
fully autonomous mode, ResearStudio achieves state-of-the-art results on the
GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These
results show that strong automated performance and fine-grained human control
can coexist. The full code, protocol, and evaluation scripts are available at
https://github.com/ResearAI/ResearStudio. We will continue to update the
repository to encourage further work on safe and controllable research agents.
Our live demo is publicly accessible at http://ai-researcher.net:3000/. We
support the development of DeepScientist, which can be accessed at
https://github.com/ResearAI/DeepScientist.

</details>


### [31] [GOAT: A Training Framework for Goal-Oriented Agent with Tools](https://arxiv.org/abs/2510.12218)
*Hyunji Min,Sangwon Jung,Junyoung Sung,Dosung Lee,Leekyeung Han,Paul Hongsuck Seo*

Main category: cs.AI

TL;DR: GOAT是一个无需人工标注的训练框架，通过从API文档自动构建合成数据集，能够微调LLM代理处理目标导向的API执行任务，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在处理需要分解高层次目标为多个相互依赖API调用的目标导向查询时能力有限，开源模型尤其难以有效执行复杂工具使用。

Method: 提出GOAT训练框架，从API文档自动构建合成数据集，在无需人工标注的情况下微调LLM代理，使其能够推理相互依赖的调用并生成连贯响应。

Result: GOAT训练的代理在多个现有目标导向基准测试中达到最先进性能，并在新提出的GOATBench基准上也表现出色。

Conclusion: GOAT为构建能够进行复杂推理和工具使用的稳健开源LLM代理提供了实用路径。

Abstract: Large language models (LLMs) have recently been extended beyond traditional
text generation to serve as interactive agents capable of using external tools
based on user intent. However, current LLM agents still show limited ability to
handle goal-oriented queries, which require decomposing a high-level objective
into multiple interdependent API calls with correct planning and execution.
Current approaches mainly rely on zero-shot evaluation due to the absence of
training data. While proprietary closed-source models such as GPT-4 demonstrate
strong reasoning abilities, smaller open-source models struggle to perform
complex tool use effectively. Thus, we propose a novel training framework GOAT,
which enables fine-tuning of LLM agents in a human annotation-free setting.
GOAT automatically constructs synthetic datasets of goal-oriented API execution
tasks directly from given API documents, equipping models with the ability to
reason over interdependent calls and generate coherent responses. Through
extensive experiments, we show that GOAT-trained agents achieve
state-of-the-art performance across multiple existing goal-oriented benchmarks.
In addition, we introduce GOATBench, a new goal-oriented API execution
benchmark, and demonstrate that agents trained with GOAT also excel in this
setting. These results highlight GOAT as a practical path toward building
robust open-source LLM agents capable of complex reasoning and tool use.

</details>


### [32] [MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs](https://arxiv.org/abs/2510.12224)
*Yuechun Yu,Han Ying,Haoan Jin,Wenjian Jiang,Dong Xian,Binghao Wang,Zhou Yang,Mengyue Wu*

Main category: cs.AI

TL;DR: 提出了MedKGEval框架，一种基于结构化医学知识的多轮评估方法，用于评估临床LLMs在医生-患者对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学LLM评估方法主要依赖对话转录后分析，忽略了医疗对话的动态性和上下文敏感性，无法捕捉真实临床环境中复杂的多轮医患互动。

Method: 1) 基于知识图谱的患者模拟机制；2) 实时轮级评估框架，由法官代理评估每个模型响应的临床适当性、事实准确性和安全性；3) 构建双语医学知识图谱支持中英文评估。

Result: 在8个最先进LLMs上的评估显示，MedKGEval能够识别传统评估方法忽略的细微行为缺陷和安全风险。

Conclusion: MedKGEval提供了一个更全面、动态的医学LLM评估框架，能够更好地反映真实临床对话的复杂性，并支持多语言扩展。

Abstract: The reliable evaluation of large language models (LLMs) in medical
applications remains an open challenge, particularly in capturing the
complexity of multi-turn doctor-patient interactions that unfold in real
clinical environments. Existing evaluation methods typically rely on post hoc
review of full conversation transcripts, thereby neglecting the dynamic,
context-sensitive nature of medical dialogues and the evolving informational
needs of patients. In this work, we present MedKGEval, a novel multi-turn
evaluation framework for clinical LLMs grounded in structured medical
knowledge. Our approach introduces three key contributions: (1) a knowledge
graph-driven patient simulation mechanism, where a dedicated control module
retrieves relevant medical facts from a curated knowledge graph, thereby
endowing the patient agent with human-like and realistic conversational
behavior. This knowledge graph is constructed by integrating open-source
resources with additional triples extracted from expert-annotated datasets; (2)
an in-situ, turn-level evaluation framework, where each model response is
assessed by a Judge Agent for clinical appropriateness, factual correctness,
and safety as the dialogue progresses using a suite of fine-grained,
task-specific metrics; (3) a comprehensive multi-turn benchmark of eight
state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle
behavioral flaws and safety risks that are often overlooked by conventional
evaluation pipelines. Although initially designed for Chinese and English
medical applications, our framework can be readily extended to additional
languages by switching the input knowledge graphs, ensuring seamless bilingual
support and domain-specific applicability.

</details>


### [33] [$\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning](https://arxiv.org/abs/2510.12264)
*Deyu Zou,Yongqiang Chen,Jianxiang Wang,Haochen Yang,Mufei Li,James Cheng,Pan Li,Yu Gong*

Main category: cs.AI

TL;DR: 提出T^3方法，通过检测和截断过度信念偏差的轨迹，在强化学习训练中保留有价值的前缀信息，从而提升LLM智能体的主动推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在主动推理过程中因信念偏差导致的跟踪问题状态失败、陷入无信息重复行为的问题，以及由此引发的强化学习训练失败。

Method: 开发T^3方法，跟踪模型信念偏差，检测过度偏差并截断训练轨迹，移除无信息尾部，保留有价值的前缀信息用于策略优化。

Result: 在5个挑战性任务中，T^3显著提升训练稳定性、令牌效率和最终性能，获得高达30%的性能增益，同时减少约25%的令牌使用。

Conclusion: 信念控制是开发鲁棒和可泛化LLM主动推理器的关键原则。

Abstract: Active reasoning requires large language models (LLMs) to interact with
external sources and strategically gather information to solve problems.
Central to this process is belief tracking: maintaining a coherent
understanding of the problem state and the missing information toward the
solution. However, due to limited reasoning capabilities, LLM-based agents
often suffer from belief deviation: they struggle to correctly model beliefs,
lose track of problem states, and fall into uninformative or repetitive
actions. Once this happens, errors compound and reinforcement learning (RL)
training fails to properly credit the crucial exploratory steps. To address
this issue, we propose to track the deviation of model beliefs and develop
$\mathbf{T^3}$, a simple yet effective method that detects excessive belief
deviation and truncates trajectories during training to remove uninformative
tails. By preserving credit for informative prefixes, $\mathbf{T^3}$
systematically improves policy optimization. Across 5 challenging tasks,
$\mathbf{T^3}$ consistently enhances training stability, token efficiency, and
final performance, achieving up to 30% gains while cutting rollout tokens by
roughly 25%. These results highlight belief control as a key principle for
developing robust and generalizable LLM-based active reasoners.

</details>


### [34] [A Survey of Vibe Coding with Large Language Models](https://arxiv.org/abs/2510.12399)
*Yuyao Ge,Lingrui Mei,Zenghao Duan,Tianhao Li,Yujia Zheng,Yiwei Wang,Lexin Wang,Jiayu Yao,Tianyu Liu,Yujun Cai,Baolong Bi,Fangda Guo,Jiafeng Guo,Shenghua Liu,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文对基于大语言模型的Vibe Coding范式进行了首次系统性综述，建立了理论框架和实践模型，揭示了成功实施Vibe Coding的关键要素。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，从代码生成辅助转向自主编码代理，催生了"Vibe Coding"新范式，但其有效性尚未得到充分探索，需要建立理论基础和实践框架。

Method: 通过分析1000多篇研究论文，建立了约束马尔可夫决策过程理论框架，并综合出五种开发模型：无约束自动化、迭代对话协作、规划驱动、测试驱动和上下文增强模型。

Result: 成功实施Vibe Coding不仅依赖代理能力，更需要系统化的上下文工程、完善的开发环境和人机协作开发模型。

Conclusion: Vibe Coding作为一种变革性开发方法，需要理论框架和实践模型的系统支持，才能充分发挥其潜力。

Abstract: The advancement of large language models (LLMs) has catalyzed a paradigm
shift from code generation assistance to autonomous coding agents, enabling a
novel development methodology termed "Vibe Coding" where developers validate
AI-generated implementations through outcome observation rather than
line-by-line code comprehension. Despite its transformative potential, the
effectiveness of this emergent paradigm remains under-explored, with empirical
evidence revealing unexpected productivity losses and fundamental challenges in
human-AI collaboration. To address this gap, this survey provides the first
comprehensive and systematic review of Vibe Coding with large language models,
establishing both theoretical foundations and practical frameworks for this
transformative development approach. Drawing from systematic analysis of over
1000 research papers, we survey the entire vibe coding ecosystem, examining
critical infrastructure components including LLMs for coding, LLM-based coding
agent, development environment of coding agent, and feedback mechanisms. We
first introduce Vibe Coding as a formal discipline by formalizing it through a
Constrained Markov Decision Process that captures the dynamic triadic
relationship among human developers, software projects, and coding agents.
Building upon this theoretical foundation, we then synthesize existing
practices into five distinct development models: Unconstrained Automation,
Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and
Context-Enhanced Models, thus providing the first comprehensive taxonomy in
this domain. Critically, our analysis reveals that successful Vibe Coding
depends not merely on agent capabilities but on systematic context engineering,
well-established development environments, and human-agent collaborative
development models.

</details>


### [35] [Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems](https://arxiv.org/abs/2510.12462)
*Jiaxin Gao,Chen Chen,Yanwen Jia,Xueluan Gong,Kwok-Yan Lam,Qian Wang*

Main category: cs.AI

TL;DR: 本文系统研究了LLM作为评估者时的判断偏见问题，发现现有模型对偏见输入具有鲁棒性，但微调会显著降低性能，并提出了四种缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在通信系统中被用于自主评估内容质量，评估者的公正性无法保证，任何偏见都可能影响结果并损害用户信任。

Method: 系统研究两种LLM评估模型在点式评分设置下的11种偏见类型，涵盖隐式和显式偏见形式，分析模型对偏见输入的鲁棒性、微调影响以及任务难度相关性。

Result: 最先进的LLM评估者对偏见输入具有鲁棒性，通常给偏见样本分配较低分数；提供详细评分标准能增强鲁棒性；在偏见数据上微调会显著降低性能；评估分数与任务难度相关。

Conclusion: LLM评估者存在判断偏见风险，需要采取缓解策略来确保实际通信场景中的公平可靠评估。

Abstract: Large Language Models (LLMs) are increasingly being used to autonomously
evaluate the quality of content in communication systems, e.g., to assess
responses in telecom customer support chatbots. However, the impartiality of
these AI "judges" is not guaranteed, and any biases in their evaluation
criteria could skew outcomes and undermine user trust. In this paper, we
systematically investigate judgment biases in two LLM-as-a-judge models (i.e.,
GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11
types of biases that cover both implicit and explicit forms. We observed that
state-of-the-art LLM judges demonstrate robustness to biased inputs, generally
assigning them lower scores than the corresponding clean samples. Providing a
detailed scoring rubric further enhances this robustness. We further found that
fine-tuning an LLM on high-scoring yet biased responses can significantly
degrade its performance, highlighting the risk of training on biased data. We
also discovered that the judged scores correlate with task difficulty: a
challenging dataset like GPQA yields lower average scores, whereas an
open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.
Finally, we proposed four potential mitigation strategies to ensure fair and
reliable AI judging in practical communication scenarios.

</details>


### [36] [Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings](https://arxiv.org/abs/2510.12555)
*Andries Rosseau,Raphaël Avalos,Ann Nowé*

Main category: cs.AI

TL;DR: 提出基于包容性适应度的多智能体强化学习框架，通过基因型分配和遗传相似性驱动的奖励函数，模拟自然选择中的合作与竞争动态。


<details>
  <summary>Details</summary>
Motivation: 受自然选择中合作与竞争力量驱动智力进化的启发，旨在创建能产生类似生物进化中策略军备竞赛的多智能体环境。

Method: 为每个智能体分配基因型，使用基于包容性适应度的奖励函数，在囚徒困境网络游戏中研究社会动态，基因相似性决定合作程度。

Result: 实验结果与生物学原理（如汉密尔顿法则）一致，展示了基于遗传相似性的合作谱系和非团队型社会动态。

Conclusion: 基于包容性适应度的奖励机制为更复杂策略和社会智能的出现提供了基础，能够产生类似生物进化的多智能体自动课程。

Abstract: The competitive and cooperative forces of natural selection have driven the
evolution of intelligence for millions of years, culminating in nature's vast
biodiversity and the complexity of human minds. Inspired by this process, we
propose a novel multi-agent reinforcement learning framework where each agent
is assigned a genotype and where reward functions are modelled after the
concept of inclusive fitness. An agent's genetic material may be shared with
other agents, and our inclusive reward function naturally accounts for this. We
study the resulting social dynamics in two types of network games with
prisoner's dilemmas and find that our results align with well-established
principles from biology, such as Hamilton's rule. Furthermore, we outline how
this framework can extend to more open-ended environments with spatial and
temporal structure, finite resources, and evolving populations. We hypothesize
the emergence of an arms race of strategies, where each new strategy is a
gradual improvement over earlier adaptations of other agents, effectively
producing a multi-agent autocurriculum analogous to biological evolution. In
contrast to the binary team-based structures prevalent in earlier research, our
gene-based reward structure introduces a spectrum of cooperation ranging from
full adversity to full cooperativeness based on genetic similarity, enabling
unique non team-based social dynamics. For example, one agent having a mutual
cooperative relationship with two other agents, while the two other agents
behave adversarially towards each other. We argue that incorporating inclusive
fitness in agents provides a foundation for the emergence of more strategically
advanced and socially intelligent agents.

</details>


### [37] [Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2510.12635)
*Yuxiang Zhang,Jiangming Shu,Ye Ma,Xueyuan Lin,Shangxi Wu,Jitao Sang*

Main category: cs.AI

TL;DR: 提出了Memory-as-Action框架，将工作内存管理重构为可学习的内部能力，通过强化学习训练代理主动执行内存编辑操作，并开发了Dynamic Context Policy Optimization算法来解决轨迹断裂问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长视野代理任务中面临内存限制，容易被无关上下文淹没。现有工作内存方法依赖与核心策略解耦的外部启发式机制，需要将内存管理作为内在可学习能力。

Method: 提出Memory-as-Action框架，代理通过统一策略执行显式内存编辑操作；开发Dynamic Context Policy Optimization算法，在内存操作点分割轨迹并应用轨迹级优势。

Result: 端到端联合优化任务推理和内存管理不仅减少了计算消耗，还提高了任务性能，通过适应模型内在能力的自适应上下文管理策略实现。

Conclusion: 将内存管理重构为可学习的内部能力，通过端到端强化学习训练，能够有效平衡内存管理和长期任务目标，提升代理性能。

Abstract: Large Language Models face challenges in long-horizon agentic tasks as their
constrained memory is easily overwhelmed by distracting or irrelevant context.
Existing working memory methods typically rely on external, heuristic
mechanisms that are decoupled from the agent's core policy. In this work, we
reframe working memory management as a learnable, intrinsic capability. We
propose a novel framework, Memory-as-Action, where an agent actively manages
its working memory by executing explicit editing operations as part of a
unified policy. This formulation allows an agent, trained via reinforcement
learning, to balance memory curation against long-term task objectives under
given resource constraints. However, such memory editing actions break the
standard assumption of a continuously growing prefix in LLM interactions,
leading to what we call trajectory fractures. These non-prefix changes disrupt
the causal continuity required by standard policy gradient methods, making
those methods inapplicable. To address this, we propose a new algorithm,
Dynamic Context Policy Optimization, which enables stable end-to-end
reinforcement learning by segmenting trajectories at memory action points and
applying trajectory-level advantages to the resulting action segments. Our
results demonstrate that jointly optimizing for task reasoning and memory
management in an end-to-end fashion not only reduces overall computational
consumption but also improves task performance, driven by adaptive context
curation strategies tailored to the model's intrinsic capabilities.

</details>


### [38] [ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning](https://arxiv.org/abs/2510.12693)
*Hanyang Chen,Mark Zhao,Rui Yang,Qinwei Ma,Ke Yang,Jiarui Yao,Kangrui Wang,Hao Bai,Zhenhailong Wang,Rui Pan,Mengchao Zhang,Jose Barreiros,Aykut Onol,ChengXiang Zhai,Heng Ji,Manling Li,Huan Zhang,Tong Zhang*

Main category: cs.AI

TL;DR: ERA是一个两阶段框架，通过先验知识学习和在线强化学习，使小型视觉语言模型在具身AI任务中超越大型模型表现。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型部署成本高，而小型模型缺乏必要知识和技能的问题，弥合性能与效率之间的差距。

Method: 第一阶段：具身先验学习，从三类数据中提取知识；第二阶段：在线强化学习，包含自总结、密集奖励塑造和回合级策略优化。

Result: ERA-3B在EB-ALFRED和EB-Manipulation任务上分别比GPT-4o提升8.4%和19.4%，并展现强泛化能力。

Conclusion: ERA为可扩展的具身智能提供了实用路径，为未来具身AI系统提供了方法论见解。

Abstract: Recent advances in embodied AI highlight the potential of vision language
models (VLMs) as agents capable of perception, reasoning, and interaction in
complex environments. However, top-performing systems rely on large-scale
models that are costly to deploy, while smaller VLMs lack the necessary
knowledge and skills to succeed. To bridge this gap, we present
\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates
prior knowledge learning and online reinforcement learning (RL). The first
stage, \textit{Embodied Prior Learning}, distills foundational knowledge from
three types of data: (1) Trajectory-Augmented Priors, which enrich existing
trajectory data with structured reasoning generated by stronger models; (2)
Environment-Anchored Priors, which provide in-environment knowledge and
grounding supervision; and (3) External Knowledge Priors, which transfer
general knowledge from out-of-environment datasets. In the second stage, we
develop an online RL pipeline that builds on these priors to further enhance
agent performance. To overcome the inherent challenges in agent RL, including
long horizons, sparse rewards, and training instability, we introduce three key
designs: self-summarization for context management, dense reward shaping, and
turn-level policy optimization. Extensive experiments on both high-level
planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate
that ERA-3B surpasses both prompting-based large models and previous
training-based baselines. Specifically, it achieves overall improvements of
8.4\% on EB-ALFRED and 19.4\% on EB-Manipulation over GPT-4o, and exhibits
strong generalization to unseen tasks. Overall, ERA offers a practical path
toward scalable embodied intelligence, providing methodological insights for
future embodied AI systems.

</details>


### [39] [Multi-Agent Debate for LLM Judges with Adaptive Stability Detection](https://arxiv.org/abs/2510.12697)
*Tianyu Hu,Zhen Tan,Song Wang,Huaizhi Qu,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出多智能体辩论法官框架，通过协作推理和迭代优化提升LLM自动判断任务的准确性，引入稳定性检测机制提高效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为法官的方法依赖简单聚合（如多数投票），即使个体智能体给出正确答案也可能失败，需要更有效的协作判断机制。

Method: 多智能体辩论法官框架，智能体协作推理并迭代优化回答；数学形式化辩论过程；引入基于时间变化Beta-Binomial混合的稳定性检测机制，使用Kolmogorov-Smirnov检验进行自适应停止。

Result: 在多个基准测试和模型上的实验表明，该框架相比多数投票提高了判断准确性，同时保持了计算效率。

Conclusion: 辩论框架能放大正确性，稳定性检测机制有效平衡了准确性和效率。

Abstract: With advancements in reasoning capabilities, Large Language Models (LLMs) are
increasingly employed for automated judgment tasks. While LLMs-as-Judges offer
promise in automating evaluations, current approaches often rely on simplistic
aggregation methods (e.g., majority voting), which can fail even when
individual agents provide correct answers. To address this, we propose a
multi-agent debate judge framework where agents collaboratively reason and
iteratively refine their responses. We formalize the debate process
mathematically, analyzing agent interactions and proving that debate amplifies
correctness compared to static ensembles. To enhance efficiency, we introduce a
stability detection mechanism that models judge consensus dynamics via a
time-varying Beta-Binomial mixture, with adaptive stopping based on
distributional similarity (Kolmogorov-Smirnov test). This mechanism models the
judges' collective correct rate dynamics using a time-varying mixture of
Beta-Binomial distributions and employs an adaptive stopping criterion based on
distributional similarity (Kolmogorov-Smirnov statistic). Experiments across
multiple benchmarks and models demonstrate that our framework improves judgment
accuracy over majority voting while maintaining computational efficiency.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [40] [eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis](https://arxiv.org/abs/2510.11722)
*Haruhiko Yoshioka,Kazumasa Shimari,Hidetake Uwano,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: eye2vec是一个分析软件开发者在阅读源代码时眼球运动的基础设施，通过分布式表示将连续注视表示为语法元素之间的转换，简化了眼动分析过程。


<details>
  <summary>Details</summary>
Motivation: 传统眼动研究中，研究人员需要预先选择分析目标并开发分析方法，这种手动工作耗时且结果因分析区域定义不同而存在差异。

Method: 使用分布式表示将连续两次注视表示为语法元素之间的转换，便于采用多样化的数据分析方法。

Result: 该方法简化了眼动分析过程，提供了丰富的语义解释能力。

Conclusion: eye2vec基础设施能够有效解决传统眼动分析中的依赖手动工作和结果不一致问题。

Abstract: This paper presents eye2vec, an infrastructure for analyzing software
developers' eye movements while reading source code. In common eye-tracking
studies in program comprehension, researchers must preselect analysis targets
such as control flow or syntactic elements, and then develop analysis methods
to extract appropriate metrics from the fixation for source code. Here,
researchers can define various levels of AOIs like words, lines, or code
blocks, and the difference leads to different results. Moreover, the
interpretation of fixation for word/line can vary across the purposes of the
analyses. Hence, the eye-tracking analysis is a difficult task that depends on
the time-consuming manual work of the researchers. eye2vec represents
continuous two fixations as transitions between syntactic elements using
distributed representations. The distributed representation facilitates the
adoption of diverse data analysis methods with rich semantic interpretations.

</details>


### [41] [Enhancing Neural Code Representation with Additional Context](https://arxiv.org/abs/2510.12082)
*Huy Nguyen,Christoph Treude,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 该论文通过实证研究发现，在代码表示中融入上下文信息（如版本历史和调用图）能够显著提升神经网络模型在代码克隆检测和代码摘要等程序理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型主要依赖源代码本身，忽略了版本历史和结构关系等上下文信息，这限制了模型理解代码演化过程的能力。

Method: 在两个数据集（SeSaMe和CodeSearchNet）上评估了五种代表性模型（CodeBERT、GraphCodeBERT、CodeT5、PLBART、ASTNN），比较了仅使用代码和使用上下文增强设置下的性能。

Result: 上下文信息普遍提升性能：版本历史持续改善克隆检测（CodeT5 F1提升15.92%）和代码摘要（GraphCodeBERT METEOR提升5.56%），组合多种上下文可获得更大增益（最高提升21.48% macro-F1）。

Conclusion: 上下文信号有潜力增强代码理解能力，为优化神经网络软件工程模型中的上下文编码开辟了新方向。

Abstract: Automated program comprehension underpins many software engineering tasks,
from code summarisation to clone detection. Recent deep learning models achieve
strong results but typically rely on source code alone, overlooking contextual
information such as version history or structural relationships. This limits
their ability to capture how code evolves and operates. We conduct an empirical
study on how enriching code representations with such contextual signals
affects neural model performance on key comprehension tasks. Two downstream
tasks, code clone detection and code summarisation, are evaluated using SeSaMe
(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative
models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under
code-only and context-augmented settings. Results show that context generally
improves performance: version history consistently boosts clone detection
(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%
METEOR), while call-graph effects vary by model and task. Combining multiple
contexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100
Java snippets confirms that context-augmented summaries are significantly
preferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).
These findings highlight the potential of contextual signals to enhance code
comprehension and open new directions for optimising contextual encoding in
neural SE models.

</details>


### [42] [Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach](https://arxiv.org/abs/2510.12120)
*Zhenyu Mao,Jacky Keung,Fengji Zhang,Shuo Liu,Yifei Wang,Jialong Li*

Main category: cs.SE

TL;DR: 提出SEMAP协议层方法，通过三个核心软件工程原则减少多智能体系统中的故障：明确行为契约建模、结构化消息传递、生命周期引导执行与验证。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统在软件工程任务中经常失败，主要由于缺乏基础SE结构化原则导致的三个核心缺陷：规范不足、协调错位和不适当的验证。

Method: 引入SEMAP协议层方法，实例化三个核心SE设计原则：明确行为契约建模、结构化消息传递、生命周期引导执行与验证，并在Google的A2A基础设施上实现。

Result: 使用MAST框架评估显示，SEMAP显著减少不同SE任务中的故障：代码开发中函数级开发总故障减少69.6%，部署级开发减少56.7%；漏洞检测中Python任务故障减少47.4%，C/C++任务减少28.2%。

Conclusion: SEMAP通过引入软件工程结构化原则，有效解决了多智能体LLM系统中的核心缺陷，显著提升了系统可靠性和任务成功率。

Abstract: The increasing demand for software development has driven interest in
automating software engineering (SE) tasks using Large Language Models (LLMs).
Recent efforts extend LLMs into multi-agent systems (MAS) that emulate
collaborative development workflows, but these systems often fail due to three
core deficiencies: under-specification, coordination misalignment, and
inappropriate verification, arising from the absence of foundational SE
structuring principles. This paper introduces Software Engineering Multi-Agent
Protocol (SEMAP), a protocol-layer methodology that instantiates three core SE
design principles for multi-agent LLMs: (1) explicit behavioral contract
modeling, (2) structured messaging, and (3) lifecycle-guided execution with
verification, and is implemented atop Google's Agent-to-Agent (A2A)
infrastructure. Empirical evaluation using the Multi-Agent System Failure
Taxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures
across different SE tasks. In code development, it achieves up to a 69.6%
reduction in total failures for function-level development and 56.7% for
deployment-level development. For vulnerability detection, SEMAP reduces
failure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.

</details>


### [43] [iCodeReviewer: Improving Secure Code Review with Mixture of Prompts](https://arxiv.org/abs/2510.12186)
*Yun Peng,Kisub Kim,Linghan Meng,Kui Liu*

Main category: cs.SE

TL;DR: 提出了iCodeReviewer，一种基于大语言模型的自动化安全代码审查方法，通过混合提示架构提高安全问题的覆盖率，并减少误报。


<details>
  <summary>Details</summary>
Motivation: 当前自动化安全代码审查方法（静态分析、深度学习模型、提示方法）存在精度和覆盖率有限、缺乏全面评估的挑战。

Method: 采用混合提示架构，包含多个动态提示专家来检查特定安全问题，并实现有效的路由算法基于代码特征激活必要专家。

Result: 在内部数据集上实现63.98%的F1分数，生产环境中生成的审查评论接受率高达84%。

Conclusion: iCodeReviewer在安全问题的识别和定位方面有效，生成的审查评论在实际部署中具有高接受率。

Abstract: Code review is an essential process to ensure the quality of software that
identifies potential software issues at an early stage of software development.
Among all software issues, security issues are the most important to identify,
as they can easily lead to severe software crashes and service disruptions.
Recent research efforts have been devoted to automated approaches to reduce the
manual efforts required in the secure code review process. Despite the
progress, current automated approaches on secure code review, including static
analysis, deep learning models, and prompting approaches, still face the
challenges of limited precision and coverage, and a lack of comprehensive
evaluation.
  To mitigate these challenges, we propose iCodeReviewer, which is an automated
secure code review approach based on large language models (LLMs).
iCodeReviewer leverages a novel mixture-of-prompts architecture that
incorporates many prompt experts to improve the coverage of security issues.
Each prompt expert is a dynamic prompt pipeline to check the existence of a
specific security issue. iCodeReviewer also implements an effective routing
algorithm to activate only necessary prompt experts based on the code features
in the input program, reducing the false positives induced by LLM
hallucination. Experiment results in our internal dataset demonstrate the
effectiveness of iCodeReviewer in security issue identification and
localization with an F1 of 63.98%. The review comments generated by
iCodeReviewer also achieve a high acceptance rate up to 84% when it is deployed
in production environments.

</details>


### [44] [Show Your Title! A Scoping Review on Verbalization in Software Engineering with LLM-Assisted Screening](https://arxiv.org/abs/2510.12294)
*Gergő Balogh,Dávid Kószó,Homayoun Safarpour Motealegh Mahalegi,László Tóth,Bence Szakács,Áron Búcsú*

Main category: cs.SE

TL;DR: 本文通过LLM辅助筛选9000多篇论文，研究了软件工程与心理学交叉领域中使用言语数据的研究主题，发现SE主要借鉴PSY方法，而人类中心主题代表性不足。


<details>
  <summary>Details</summary>
Motivation: 理解软件开发者的思维、决策和行为是软件工程的关键挑战，言语化技术提供了研究这些认知方面的轻量级方法。

Method: 使用GPT构建LLM辅助筛选管道，基于标题评估9000多篇论文的相关性，验证GPT输出与人工评审的一致性。

Result: GPT与人工评审高度一致，分歧率为13%。主要主题与SE工艺相关，人类中心主题代表性不足，SE频繁借鉴PSY方法而反向罕见。

Conclusion: LLM可以有效支持跨学科评审过程，SE与PSY的交叉研究存在不平衡现象。

Abstract: Understanding how software developers think, make decisions, and behave
remains a key challenge in software engineering (SE). Verbalization techniques
(methods that capture spoken or written thought processes) offer a lightweight
and accessible way to study these cognitive aspects. This paper presents a
scoping review of research at the intersection of SE and psychology (PSY),
focusing on the use of verbal data. To make large-scale interdisciplinary
reviews feasible, we employed a large language model (LLM)-assisted screening
pipeline using GPT to assess the relevance of over 9,000 papers based solely on
titles. We addressed two questions: what themes emerge from
verbalization-related work in SE, and how effective are LLMs in supporting
interdisciplinary review processes? We validated GPT's outputs against human
reviewers and found high consistency, with a 13\% disagreement rate. Prominent
themes mainly were tied to the craft of SE, while more human-centered topics
were underrepresented. The data also suggests that SE frequently draws on PSY
methods, whereas the reverse is rare.

</details>


### [45] [Diff-XYZ: A Benchmark for Evaluating Diff Understanding](https://arxiv.org/abs/2510.12487)
*Evgeniy Glukhov,Michele Conti,Egor Bogomolov,Yaroslav Golubev,Alexander Bezzubov*

Main category: cs.SE

TL;DR: Diff-XYZ是一个用于代码差异理解的紧凑基准，包含三个监督任务：应用差异、反应用差异和差异生成，基于真实提交数据构建。


<details>
  <summary>Details</summary>
Motivation: 可靠的代码差异处理对于大规模编辑和重构代码库的智能体至关重要，需要系统评估不同差异格式在不同场景下的表现。

Method: 从CommitPackFT的真实提交中提取<旧代码,新代码,差异>三元组，构建基准数据集，使用自动指标和清晰评估协议进行跨格式比较研究。

Result: 研究发现不同格式应根据使用场景和模型大小选择，例如搜索替换格式适合大模型生成差异，但不适合差异分析和小模型。

Conclusion: Diff-XYZ基准为评估和改进LLM中的差异处理提供了可重用基础，有助于未来差异格式和代码编辑模型的发展。

Abstract: Reliable handling of code diffs is central to agents that edit and refactor
repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff
understanding with three supervised tasks: apply (old code $+$ diff
$\rightarrow$ new code), anti-apply (new code $-$ diff $\rightarrow$ old code),
and diff generation (new code $-$ old code $\rightarrow$ diff). Instances in
the benchmark are triples $\langle \textit{old code}, \textit{new code},
\textit{diff} \rangle$ drawn from real commits in CommitPackFT, paired with
automatic metrics and a clear evaluation protocol. We use the benchmark to do a
focused empirical study of the unified diff format and run a cross-format
comparison of different diff representations. Our findings reveal that
different formats should be used depending on the use case and model size. For
example, representing diffs in search-replace format is good for larger models
in the diff generation scenario, yet not suited well for diff analysis and
smaller models. The Diff-XYZ benchmark is a reusable foundation for assessing
and improving diff handling in LLMs that can aid future development of diff
formats and models editing code. The dataset is published on HuggingFace Hub:
https://huggingface.co/datasets/JetBrains-Research/diff-xyz.

</details>


### [46] [Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?](https://arxiv.org/abs/2510.12702)
*Cedric Richter,Heike Wehrheim*

Main category: cs.SE

TL;DR: NL2Contract任务使用LLM将自然语言转换为包含前置条件和后置条件的正式功能契约，相比仅生成后置条件的方法，能显著减少验证器产生的误报。


<details>
  <summary>Details</summary>
Motivation: 自动软件验证器在实践中应用受限，因为真实代码中缺乏形式化规范。LLM虽然能从代码中的自然语言提示推断后置条件，但仅使用后置条件进行验证会产生误报。

Method: 引入NL2Contract任务，使用LLM从自然语言生成包含前置条件和后置条件的正式功能契约。通过声音性、错误判别能力和验证可用性等指标评估不同方法。

Result: LLM能有效生成对所有可能输入都合理的功能契约；生成的契约能有效区分错误和正确行为；使用功能契约比仅使用后置条件产生的误报更少。

Conclusion: LLM推断的前置条件与开发者意图一致，使得自动软件验证器能够捕获真实世界的错误。

Abstract: Automatic software verifiers have become increasingly effective at the task
of checking software against (formal) specifications. Yet, their adoption in
practice has been hampered by the lack of such specifications in real world
code. Large Language Models (LLMs) have shown promise in inferring formal
postconditions from natural language hints embedded in code such as function
names, comments or documentation. Using the generated postconditions as
specifications in a subsequent verification, however, often leads verifiers to
suggest invalid inputs, hinting at potential issues that ultimately turn out to
be false alarms.
  To address this, we revisit the problem of specification inference from
natural language in the context of automatic software verification. In the
process, we introduce NL2Contract, the task of employing LLMs to translate
informal natural language into formal functional contracts, consisting of
postconditions as well as preconditions. We introduce metrics to validate and
compare different NL2Contract approaches, using soundness, bug discriminative
power of the generated contracts and their usability in the context of
automatic software verification as key metrics. We evaluate NL2Contract with
different LLMs and compare it to the task of postcondition generation
nl2postcond. Our evaluation shows that (1) LLMs are generally effective at
generating functional contracts sound for all possible inputs, (2) the
generated contracts are sufficiently expressive for discriminating buggy from
correct behavior, and (3) verifiers supplied with LLM inferred functional
contracts produce fewer false alarms than when provided with postconditions
alone. Further investigations show that LLM inferred preconditions generally
align well with developers intentions which allows us to use automatic software
verifiers to catch real-world bugs.

</details>


### [47] [Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method Refactoring](https://arxiv.org/abs/2503.20934)
*Fraol Batole,Abhiram Bellur,Malinda Dilhara,Mohammed Raihan Ullah,Yaroslav Zharov,Timofey Bryksin,Kai Ishikawa,Haifeng Chen,Masaharu Morimoto,Shota Motoura,Takeo Hosomi,Tien N. Nguyen,Hridesh Rajan,Nikolaos Tsantalis,Danny Dig*

Main category: cs.SE

TL;DR: MOVEMETHOD是一种重要的代码重构操作。研究者开发了MM-assist工具，利用LLM、IDE静态分析和检索增强生成技术，自动化MOVEMETHOD重构的整个生命周期，显著提升了重构建议的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法重构工具推荐与专家开发者的实际操作不符，虽然LLM能给出专家级建议但存在高达80%的幻觉问题，需要开发可靠的自动化重构助手。

Method: 设计了MM-assist系统，结合LLM、IDE静态分析和语义相关性，使用自一致性检查、批判和排序的工作流程，并采用重构感知的检索增强生成技术解决LLM上下文限制问题。

Result: 在广泛使用的基准测试上，Recall@1和Recall@3提升了1.7倍；在210个开源软件重构案例中，召回率至少提升了2.4倍；用户研究中82.8%的建议获得积极评价。

Conclusion: MM-assist通过协同利用LLM、IDE、静态分析和语义相关性，显著提升了MOVEMETHOD重构的准确性和实用性，是有效且有用的重构工具。

Abstract: MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools
that recommend which methods to move and where, these recommendations do not
align with how expert developers perform MOVEMETHOD. Given the extensive
training of Large Language Models and their reliance upon naturalness of code,
they should expertly recommend which methods are misplaced in a given class and
which classes are better hosts. Our formative study of 2016 LLM recommendations
revealed that LLMs give expert suggestions, yet they are unreliable: up to 80%
of the suggestions are hallucinations. We introduce the first LLM fully powered
assistant for MOVEMETHOD refactoring that automates its whole end-to-end
lifecycle, from recommendation to execution. We designed novel solutions that
automatically filter LLM hallucinations using static analysis from IDEs and a
novel workflow that requires LLMs to be self-consistent, critique, and rank
refactoring suggestions. As MOVEMETHOD refactoring requires global,
projectlevel reasoning, we solved the limited context size of LLMs by employing
refactoring-aware retrieval augment generation (RAG). Our approach, MM-assist,
synergistically combines the strengths of the LLM, IDE, static analysis, and
semantic relevance. In our thorough, multi-methodology empirical evaluation, we
compare MM-assist with the previous state-of-the-art approaches. MM-assist
significantly outperforms them: (i) on a benchmark widely used by other
researchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a
corpus of 210 recent refactorings from Open-source software, our Recall rates
improve by at least 2.4x. Lastly, we conducted a user study with 30 experienced
participants who used MM-assist to refactor their own code for one week. They
rated 82.8% of MM-assist recommendations positively. This shows that MM-assist
is both effective and useful.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [48] [Claude Code Plugins Now in Public Beta](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-code-plugins%3Futm_source=tldrai/1/01000199ddd41021-5bfdd8f1-3f6d-474a-a032-37fb9b71c2f7-000000/OHDWYGKeEqsJC8yVWCokJCsVg5tKCbXqvRU56_UKybI=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic推出Claude Code插件公开测试版，支持用户通过单一命令安装自定义斜杠命令、代理、MCP服务器和钩子


<details>
  <summary>Details</summary>
Motivation: 简化开发工作流程的定制化设置，使插件配置更易于分享和使用

Method: 在Claude Code中引入插件支持系统，允许模块化扩展功能

Result: 用户现在可以轻松安装和管理各种自定义开发工具和功能

Conclusion: 插件系统显著提升了Claude Code的灵活性和定制能力

Abstract: Claude Code Plugins Now in Public Beta (3 minute read) Anthropic has introduced plugin support in Claude Code, allowing users to install custom slash commands, agents, MCP servers, and hooks with a single command. These modular extensions simplify setup sharing and enable flexible customizations for development workflows.

</details>


### [49] [ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2509.25140%3Futm_source=tldrai/1/01000199ddd41021-5bfdd8f1-3f6d-474a-a032-37fb9b71c2f7-000000/BlPQKo70UKj1yrRD4ZpOT48ouUuiIxz_6DBSirTUTa4=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ReasoningBank是一个新颖的记忆框架，通过从智能体自我评估的成功和失败经验中提炼可泛化的推理策略来扩展智能体自我进化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决智能体在长期任务中记忆效率低下的问题，需要一种能够从经验中学习并持续改进推理能力的记忆机制。

Method: 构建一个记忆框架，存储和检索智能体的成功与失败经验，从中提炼通用推理策略，并允许智能体整合新学习内容随时间变得更强。

Result: ReasoningBank在性能上持续优于现有的记忆机制，实现了更有效的扩展。

Conclusion: 该框架通过改进记忆机制显著提升了智能体的推理能力和自我进化效率。

Abstract: ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory (1 minute read) ReasoningBank is a novel memory framework that distills generalizable reasoning strategies from an agent's self-judged successful and failed experiences. Agents can store and retrieve memories from ReasoningBank to inform their decisions. They can integrate new learnings and become more capable over time. The better memory enables more effective scaling. ReasoningBank consistently outperforms existing memory mech...

</details>


### [50] [Meta's Agent Learning](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2510.08558%3Futm_source=tldrai/1/01000199ddd41021-5bfdd8f1-3f6d-474a-a032-37fb9b71c2f7-000000/klv_KJsUItzBiBTFn0NkYvYaZVToYnIYXl1ECSESBPE=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Meta提出"早期经验"训练方法，利用智能体自身交互数据无需外部奖励，通过隐式世界建模和自我反思改进策略学习


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习依赖外部奖励信号的问题，探索利用智能体自身交互数据进行更有效的策略学习

Method: 使用智能体自身交互数据，结合隐式世界建模和自我反思机制来训练策略

Result: 该方法能够改进策略学习效果，无需依赖外部奖励信号

Conclusion: 早期经验方法为智能体学习提供了新的训练范式，通过利用自身交互数据实现更有效的策略优化

Abstract: Meta's Agent Learning (19 minute read) Meta has introduced "early experience," a training approach using data from an agent's own interactions without external rewards. It improves policy learning via implicit world modeling and self-reflection.

</details>


### [51] [Two things LLM coding agents are still bad at](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkix.dev%2Ftwo-things-llm-coding-agents-are-still-bad-at%3Futm_source=tldrwebdev/1/01000199e267d218-3779c78d-8eba-471e-92b4-20c4b8a12ef4-000000/M1mxzN9gUwf0Gut4BiW6hUoIzBL-zrerpmIvokysNwQ=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: LLM编程代理仍存在两个主要问题：无法像人类一样复制粘贴代码（而是从记忆中重写），以及不善于提问澄清问题（倾向于假设和暴力求解）。


<details>
  <summary>Details</summary>
Motivation: 分析当前LLM编程代理的局限性，揭示其与人类开发者工作方式的差异，帮助理解为何LLM代理尚未能完全替代人类开发者。

Method: 通过观察和分析LLM编程代理在实际编码任务中的行为模式，识别其与人类开发者工作习惯的关键差异。

Result: 发现LLM编程代理在代码重用方式（记忆重写而非复制粘贴）和问题澄清（假设而非提问）方面与人类存在显著差异。

Conclusion: LLM编程代理目前更像是"奇怪且过度自信的实习生"，而非人类开发者的替代品，需要在这些关键行为模式上改进。

Abstract: Two things LLM coding agents are still bad at (2 minute read) LLM coding agents still feel awkward to work with due to two main issues: they don't copy-paste code like humans do (instead "remembering" and rewriting code from memory), and they're terrible at asking clarifying questions, preferring to make assumptions and brute-force solutions. These quirks make LLMs feel more like "weird, overconfident interns" rather than replacements for human developers.

</details>


### [52] [Library for Accessibility Testing](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faccented.dev%2F%3Futm_source=tldrdesign/1/01000199e2aa9543-0c6e671b-eb4b-48e0-9806-903a3a29535e-000000/umnJRO1hIoBmi0biRcE-ksbWA5EK5osQdUCwXAo8Lew=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Accented是一个前端库，用于持续进行可访问性测试和问题高亮显示。只需在Web应用中添加几行代码，即可在存在可访问性问题的元素旁边显示交互式标注。


<details>
  <summary>Details</summary>
Motivation: 解决Web应用开发中可访问性测试的复杂性和持续性问题，使开发者能够轻松识别和修复可访问性缺陷。

Method: 开发一个前端JavaScript库，通过代码注入在运行时检测可访问性问题，并在问题元素旁显示交互式标注。

Result: 创建了一个轻量级库，能够实时检测并高亮显示Web应用中的可访问性问题，提供直观的反馈机制。

Conclusion: Accented简化了Web可访问性测试流程，使开发者能够更高效地构建无障碍的Web应用。

Abstract: Library for Accessibility Testing (Website) Accented is a frontend library for continuous accessibility testing and issue highlighting. Add a few lines of code to your web app, and you'll see interactive callouts appear next to elements with accessibility issues.

</details>
