{"id": "2511.12288", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12288", "abs": "https://arxiv.org/abs/2511.12288", "authors": ["Yihan Dai", "Sijie Liang", "Haotian Xu", "Peichu Xie", "Sergey Mechtaev"], "title": "Reducing Hallucinations in LLM-Generated Code via Semantic Triangulation", "comment": null, "summary": "When generating code from natural language prompts, an LLM samples programs from a probability distribution, many of which might be incorrect. Sample consensus techniques - such as majority voting or validation against generated tests or specifications - aim to identify a correct program in the sample or abstain if none is valid. However, existing methods often fail to select a correct solution when its sampling probability is low, or when the problem permits multiple valid but non-equivalent solutions. Additionally, they often fail to abstain when no correct solution is present in the sample. To overcome these limitations, we introduce semantic triangulation, which transforms a programming problem in a way that non-trivially alters its semantics while preserving an exact, verifiable mapping between solutions before and after transformation. We theoretically establish that verifying consistency across such problem transformations increases confidence that generated programs reflect accurate generalization rather than spurious statistical correlations, enabling more reliable sample consensus and abstention. On the LiveCodeBench and CodeElo benchmarks, using GPT-4o and DeepSeek-V3 models, semantic triangulation increases reliability of generated code by 21% compared to the method that selects only high-confidence solutions with the probability threshold 0.5, while being able to pinpoint correct solutions at sampling probabilities as low as 0.14. Apart from that, it is also the only approach to consistently form true consensus on tasks with multiple valid but non-equivalent solutions.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u4e49\u4e09\u89d2\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ee\u9898\u8f6c\u6362\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u7684\u53ef\u9760\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728LiveCodeBench\u548cCodeElo\u57fa\u51c6\u4e0a\u63d0\u534721%\u53ef\u9760\u6027\uff0c\u5e76\u80fd\u8bc6\u522b\u6982\u7387\u4f4e\u81f30.14\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6837\u672c\u5171\u8bc6\u6280\u672f\u5b58\u5728\u5c40\u9650\uff1a\u96be\u4ee5\u8bc6\u522b\u4f4e\u6982\u7387\u6b63\u786e\u65b9\u6848\uff0c\u65e0\u6cd5\u5904\u7406\u591a\u6709\u6548\u4f46\u4e0d\u7b49\u4ef7\u65b9\u6848\u7684\u60c5\u51b5\uff0c\u4ee5\u53ca\u5728\u65e0\u6b63\u786e\u65b9\u6848\u65f6\u65e0\u6cd5\u6709\u6548\u5f03\u6743\u3002", "method": "\u5f15\u5165\u8bed\u4e49\u4e09\u89d2\u5316\uff0c\u5bf9\u7f16\u7a0b\u95ee\u9898\u8fdb\u884c\u8bed\u4e49\u975e\u5e73\u51e1\u8f6c\u6362\uff0c\u4fdd\u6301\u89e3\u51b3\u65b9\u6848\u95f4\u7684\u7cbe\u786e\u53ef\u9a8c\u8bc1\u6620\u5c04\uff0c\u901a\u8fc7\u8de8\u95ee\u9898\u8f6c\u6362\u7684\u4e00\u81f4\u6027\u9a8c\u8bc1\u6765\u63d0\u9ad8\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5728LiveCodeBench\u548cCodeElo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528GPT-4o\u548cDeepSeek-V3\u6a21\u578b\uff0c\u8bed\u4e49\u4e09\u89d2\u5316\u76f8\u6bd4\u6982\u7387\u9608\u503c0.5\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u9009\u62e9\u65b9\u6cd5\u63d0\u534721%\u53ef\u9760\u6027\uff0c\u80fd\u8bc6\u522b\u6982\u7387\u4f4e\u81f30.14\u7684\u6b63\u786e\u65b9\u6848\uff0c\u662f\u552f\u4e00\u80fd\u5bf9\u591a\u6709\u6548\u4f46\u4e0d\u7b49\u4ef7\u65b9\u6848\u4efb\u52a1\u5f62\u6210\u771f\u6b63\u5171\u8bc6\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u4e49\u4e09\u89d2\u5316\u901a\u8fc7\u95ee\u9898\u8f6c\u6362\u9a8c\u8bc1\u80fd\u66f4\u53ef\u9760\u5730\u8bc6\u522b\u6b63\u786e\u4ee3\u7801\u65b9\u6848\uff0c\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u7684\u53ef\u4fe1\u5ea6\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6982\u7387\u548c\u591a\u65b9\u6848\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "topic": "code agent"}}
{"id": "2511.11793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11793", "abs": "https://arxiv.org/abs/2511.11793", "authors": ["MiroMind Team", "Song Bai", "Lidong Bing", "Carson Chen", "Guanzheng Chen", "Yuntao Chen", "Zhe Chen", "Ziyi Chen", "Jifeng Dai", "Xuan Dong", "Yue Deng", "Yunjie Fu", "Junqi Ge", "Chenxia Han", "Tammy Huang", "Zhenhang Huang", "Jerry Jiao", "Shilei Jiang", "Tianyu Jiao", "Xiaoqi Jian", "Lei Lei", "Ruilin Li", "Ryan Luo", "Tiantong Li", "Xiang Lin", "Ziyuan Liu", "Zhiqi Li", "Jie Ni", "Qiang Ren", "Pax Sun", "Shiqian Su", "Chenxin Tao", "Bin Wang", "Hellen Wang", "Haonan Wang", "James Wang", "Jin Wang", "Jojo Wang", "Letian Wang", "Shizun Wang", "Weizhi Wang", "Zixuan Wang", "Jinfan Xu", "Sen Xing", "Chenyu Yang", "Hai Ye", "Jiaheng Yu", "Yue Yu", "Muyan Zhong", "Tianchen Zhao", "Xizhou Zhu", "Yanpeng Zhou", "Yifan Zhang", "Zhi Zhu"], "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "comment": "Technical Report", "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "AI": {"tldr": "MiroThinker v1.0\u662f\u4e00\u4e2a\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6269\u5c55\u4f5c\u4e3a\u6027\u80fd\u6539\u8fdb\u7684\u7b2c\u4e09\u7ef4\u5ea6\uff0c\u63a2\u7d22\u6a21\u578b\u7ea7\u522b\u7684\u4ea4\u4e92\u6269\u5c55\uff0c\u80fd\u591f\u6267\u884c\u591a\u8fbe600\u6b21\u5de5\u5177\u8c03\u7528\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5148\u524d\u5f00\u6e90\u4ee3\u7406\u5e76\u63a5\u8fd1\u5546\u4e1a\u5bf9\u5e94\u7269\u3002", "motivation": "\u63a2\u7d22\u4ea4\u4e92\u6269\u5c55\u4f5c\u4e3a\u6a21\u578b\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e4b\u5916\u7684\u7b2c\u4e09\u4e2a\u6027\u80fd\u6539\u8fdb\u7ef4\u5ea6\uff0c\u89e3\u51b3LLM\u6d4b\u8bd5\u65f6\u6269\u5c55\u5728\u957f\u63a8\u7406\u94fe\u4e2d\u53ef\u80fd\u9000\u5316\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4ea4\u4e92\u6269\u5c55\uff0c\u8bad\u7ec3\u6a21\u578b\u5904\u7406\u66f4\u6df1\u5c42\u6b21\u548c\u66f4\u9891\u7e41\u7684\u4ee3\u7406-\u73af\u5883\u4ea4\u4e92\uff0c\u4f7f\u7528256K\u4e0a\u4e0b\u6587\u7a97\u53e3\u6267\u884c\u591a\u8fbe600\u6b21\u5de5\u5177\u8c03\u7528\u3002", "result": "\u5728GAIA\u3001HLE\u3001BrowseComp\u548cBrowseComp-ZH\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c72B\u53d8\u4f53\u5206\u522b\u8fbe\u523081.9%\u300137.7%\u300147.1%\u548c55.6%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u5148\u524d\u5f00\u6e90\u4ee3\u7406\u5e76\u63a5\u8fd1GPT-5-high\u7b49\u5546\u4e1a\u5bf9\u5e94\u7269\u3002", "conclusion": "\u4ea4\u4e92\u6269\u5c55\u5c55\u73b0\u51fa\u4e0e\u6a21\u578b\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u7c7b\u4f3c\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u6210\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\u7684\u7b2c\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12294", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12294", "abs": "https://arxiv.org/abs/2511.12294", "authors": ["Bodhisatwa Chatterjee", "Drew Zagieboylo", "Sana Damani", "Siva Hari", "Christos Kozyrakis"], "title": "ProofWright: Towards Agentic Formal Verification of CUDA", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to automatically generate optimized CUDA kernels, substantially improving developer productivity. However, despite rapid generation, these kernels often contain subtle correctness bugs and lack formal safety guarantees. Runtime testing is inherently unreliable - limited input coverage and reward hacking can mask incorrect behavior - while manual formal verification is reliable but cannot scale to match LLM output rates, creating a critical validation bottleneck.\n  We present ProofWright, an agentic verification framework that bridges this gap by integrating automated formal verification with LLM-based code generation. ProofWright provides end-to-end guarantees of memory safety, thread safety, and semantic correctness for LLM-generated CUDA kernels. On KernelBench L1, ProofWright verifies safety properties for 74% of generated kernels, uncovers subtle correctness errors missed by conventional testing, and establishes semantic equivalence for a class of element-wise kernels. With a modest overhead of 3 minutes per kernel, ProofWright demonstrates that scalable, automated formal verification of LLM-generated GPU code is feasible - offering a path toward trustworthy high-performance code generation without sacrificing developer productivity.", "AI": {"tldr": "ProofWright\u662f\u4e00\u4e2a\u96c6\u6210\u81ea\u52a8\u5f62\u5f0f\u9a8c\u8bc1\u4e0eLLM\u4ee3\u7801\u751f\u6210\u7684\u4ee3\u7406\u9a8c\u8bc1\u6846\u67b6\uff0c\u4e3aLLM\u751f\u6210\u7684CUDA\u5185\u6838\u63d0\u4f9b\u5185\u5b58\u5b89\u5168\u3001\u7ebf\u7a0b\u5b89\u5168\u548c\u8bed\u4e49\u6b63\u786e\u6027\u7684\u7aef\u5230\u7aef\u4fdd\u8bc1\u3002", "motivation": "LLM\u751f\u6210\u7684CUDA\u5185\u6838\u867d\u7136\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u7ecf\u5e38\u5305\u542b\u96be\u4ee5\u53d1\u73b0\u7684\u6b63\u786e\u6027\u9519\u8bef\u4e14\u7f3a\u4e4f\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u8bc1\u3002\u8fd0\u884c\u65f6\u6d4b\u8bd5\u4e0d\u53ef\u9760\uff0c\u800c\u624b\u52a8\u5f62\u5f0f\u9a8c\u8bc1\u65e0\u6cd5\u8ddf\u4e0aLLM\u7684\u751f\u6210\u901f\u5ea6\uff0c\u5f62\u6210\u9a8c\u8bc1\u74f6\u9888\u3002", "method": "ProofWright\u6846\u67b6\u7ed3\u5408\u81ea\u52a8\u5f62\u5f0f\u9a8c\u8bc1\u548cLLM\u4ee3\u7801\u751f\u6210\uff0c\u901a\u8fc7\u4ee3\u7406\u9a8c\u8bc1\u65b9\u6cd5\u4e3a\u751f\u6210\u7684CUDA\u5185\u6838\u63d0\u4f9b\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "result": "\u5728KernelBench L1\u4e0a\uff0cProofWright\u9a8c\u8bc1\u4e8674%\u751f\u6210\u5185\u6838\u7684\u5b89\u5168\u5c5e\u6027\uff0c\u53d1\u73b0\u4e86\u4f20\u7edf\u6d4b\u8bd5\u9057\u6f0f\u7684\u7ec6\u5fae\u6b63\u786e\u6027\u9519\u8bef\uff0c\u5e76\u4e3a\u5143\u7d20\u7ea7\u5185\u6838\u7c7b\u5efa\u7acb\u4e86\u8bed\u4e49\u7b49\u4ef7\u6027\uff0c\u6bcf\u4e2a\u5185\u6838\u4ec5\u97003\u5206\u949f\u989d\u5916\u5f00\u9500\u3002", "conclusion": "ProofWright\u8bc1\u660e\u4e86\u5bf9LLM\u751f\u6210\u7684GPU\u4ee3\u7801\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5f62\u5f0f\u9a8c\u8bc1\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u53ef\u4fe1\u7684\u9ad8\u6027\u80fd\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u8def\u5f84\u3002", "topic": "code agent"}}
{"id": "2511.12576", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12576", "abs": "https://arxiv.org/abs/2511.12576", "authors": ["Mohammad Meymani", "Hamed Jelodar", "Parisa Hamedi", "Roozbeh Razavi-Far", "Ali A. Ghorbani"], "title": "Can Small GenAI Language Models Rival Large Language Models in Understanding Application Behavior?", "comment": null, "summary": "Generative AI (GenAI) models, particularly large language models (LLMs), have transformed multiple domains, including natural language processing, software analysis, and code understanding. Their ability to analyze and generate code has enabled applications such as source code summarization, behavior analysis, and malware detection. In this study, we systematically evaluate the capabilities of both small and large GenAI language models in understanding application behavior, with a particular focus on malware detection as a representative task. While larger models generally achieve higher overall accuracy, our experiments show that small GenAI models maintain competitive precision and recall, offering substantial advantages in computational efficiency, faster inference, and deployment in resource-constrained environments. We provide a detailed comparison across metrics such as accuracy, precision, recall, and F1-score, highlighting each model's strengths, limitations, and operational feasibility. Our findings demonstrate that small GenAI models can effectively complement large ones, providing a practical balance between performance and resource efficiency in real-world application behavior analysis.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u89c4\u6a21GenAI\u6a21\u578b\u5728\u5e94\u7528\u884c\u4e3a\u5206\u6790\uff08\u7279\u522b\u662f\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5c0f\u6a21\u578b\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\u5177\u6709\u8ba1\u7b97\u6548\u7387\u4f18\u52bf", "motivation": "\u8bc4\u4f30\u4e0d\u540c\u89c4\u6a21\u751f\u6210\u5f0fAI\u6a21\u578b\u5728\u5e94\u7528\u884c\u4e3a\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4efb\u52a1\uff0c\u63a2\u7d22\u5c0f\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027", "method": "\u7cfb\u7edf\u8bc4\u4f30\u5c0f\u578b\u548c\u5927\u578bGenAI\u8bed\u8a00\u6a21\u578b\u5728\u5e94\u7528\u884c\u4e3a\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4f5c\u4e3a\u4ee3\u8868\u6027\u4efb\u52a1\uff0c\u6bd4\u8f83\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807", "result": "\u5927\u578b\u6a21\u578b\u6574\u4f53\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u4f46\u5c0f\u6a21\u578b\u5728\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u65b9\u9762\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u4e14\u5728\u8ba1\u7b97\u6548\u7387\u3001\u63a8\u7406\u901f\u5ea6\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u90e8\u7f72\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf", "conclusion": "\u5c0fGenAI\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u8865\u5145\u5927\u578b\u6a21\u578b\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u884c\u4e3a\u5206\u6790\u4e2d\u63d0\u4f9b\u6027\u80fd\u4e0e\u8d44\u6e90\u6548\u7387\u4e4b\u95f4\u7684\u5b9e\u7528\u5e73\u8861", "topic": "agent analysis"}}
{"id": "2511.12635", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12635", "abs": "https://arxiv.org/abs/2511.12635", "authors": ["Lech Madeyski", "Barbara Kitchenham", "Martin Shepperd"], "title": "LLM4SCREENLIT: Recommendations on Assessing the Performance of Large Language Models for Screening Literature in Systematic Reviews", "comment": "19 pages, 4 figures", "summary": "Context: Large language models (LLMs) are released faster than users' ability to evaluate them rigorously. When LLMs underpin research, such as identifying relevant literature for systematic reviews (SRs), robust empirical assessment is essential. Objective: We identify and discuss key challenges in assessing LLM performance for selecting relevant literature, identify good (evaluation) practices, and propose recommendations. Method: Using a recent large-scale study as an example, we identify problems with the use of traditional metrics for assessing the performance of Gen-AI tools for identifying relevant literature in SRs. We analyzed 27 additional papers investigating this issue, extracted the performance metrics, and found both good practices and widespread problems, especially with the use and reporting of performance metrics for SR screening. Results: Major weaknesses included: i) a failure to use metrics that are robust to imbalanced data and do not directly indicate whether results are better than chance, e.g., the use of Accuracy, ii) a failure to consider the impact of lost evidence when making claims concerning workload savings, and iii) pervasive failure to report the full confusion matrix (or performance metrics from which it can be reconstructed) which is essential for future meta-analyses. On the positive side, we extract good (evaluation) practices on which our recommendations for researchers and practitioners, as well as policymakers, are built. Conclusions: SR screening evaluations should prioritize lost evidence/recall alongside chance-anchored and cost-sensitive Weighted MCC (WMCC) metric, report complete confusion matrices, treat unclassifiable outputs as referred-back positives for assessment, adopt leakage-aware designs with non-LLM baselines and open artifacts, and ground conclusions in cost-benefit analysis where FNs carry higher penalties than FPs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u4f7f\u7528\u4f20\u7edf\u6307\u6807\u8bc4\u4f30LLM\u5728\u7cfb\u7edf\u7efc\u8ff0\u6587\u732e\u7b5b\u9009\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u8bc4\u4f30\u5b9e\u8df5\u5efa\u8bae\uff0c\u5f3a\u8c03\u9700\u8981\u5173\u6ce8\u4e22\u5931\u8bc1\u636e\u3001\u4f7f\u7528\u7a33\u5065\u6307\u6807\u548c\u5b8c\u6574\u62a5\u544a\u6df7\u6dc6\u77e9\u9635\u3002", "motivation": "\u7531\u4e8eLLM\u53d1\u5e03\u901f\u5ea6\u5feb\u4e8e\u7528\u6237\u8bc4\u4f30\u80fd\u529b\uff0c\u5f53LLM\u7528\u4e8e\u7cfb\u7edf\u7efc\u8ff0\u6587\u732e\u7b5b\u9009\u7b49\u7814\u7a76\u65f6\uff0c\u9700\u8981\u7a33\u5065\u7684\u5b9e\u8bc1\u8bc4\u4f30\u6765\u786e\u4fdd\u53ef\u9760\u6027\u3002", "method": "\u4ee5\u5927\u89c4\u6a21\u7814\u7a76\u4e3a\u4f8b\uff0c\u5206\u6790\u4f20\u7edf\u6307\u6807\u5728\u8bc4\u4f30LLM\u6587\u732e\u7b5b\u9009\u6027\u80fd\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e8627\u7bc7\u76f8\u5173\u8bba\u6587\u7684\u6027\u80fd\u6307\u6807\u4f7f\u7528\u60c5\u51b5\u3002", "result": "\u53d1\u73b0\u4e3b\u8981\u5f31\u70b9\u5305\u62ec\uff1a\u4f7f\u7528\u5bf9\u4e0d\u5e73\u8861\u6570\u636e\u4e0d\u7a33\u5065\u7684\u6307\u6807\u3001\u5ffd\u89c6\u4e22\u5931\u8bc1\u636e\u7684\u5f71\u54cd\u3001\u672a\u5b8c\u6574\u62a5\u544a\u6df7\u6dc6\u77e9\u9635\u3002\u540c\u65f6\u63d0\u53d6\u4e86\u826f\u597d\u7684\u8bc4\u4f30\u5b9e\u8df5\u3002", "conclusion": "\u5efa\u8bae\u4f18\u5148\u8003\u8651\u4e22\u5931\u8bc1\u636e/\u53ec\u56de\u7387\uff0c\u4f7f\u7528\u673a\u4f1a\u951a\u5b9a\u548c\u6210\u672c\u654f\u611f\u7684\u52a0\u6743MCC\u6307\u6807\uff0c\u62a5\u544a\u5b8c\u6574\u6df7\u6dc6\u77e9\u9635\uff0c\u91c7\u7528\u6cc4\u6f0f\u611f\u77e5\u8bbe\u8ba1\uff0c\u5e76\u4ee5\u6210\u672c\u6548\u76ca\u5206\u6790\u4e3a\u57fa\u7840\u5f97\u51fa\u7ed3\u8bba\u3002", "topic": "agent analysis"}}
{"id": "2511.11693", "categories": ["cs.AI", "cs.CR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11693", "abs": "https://arxiv.org/abs/2511.11693", "authors": ["Xin Zhao", "Xiaojun Chen", "Bingshan Liu", "Zeyao Liu", "Zhendong Zhao", "Xiaoyan Gu"], "title": "Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation", "comment": null, "summary": "Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.", "AI": {"tldr": "VALOR\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u96f6\u6837\u672c\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u63d0\u793a\u5206\u6790\u3001\u6587\u5316\u4ef7\u503c\u5bf9\u9f50\u548c\u610f\u56fe\u6d88\u6b67\u6765\u786e\u4fdd\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u5b89\u5168\u6027\u548c\u4ef7\u503c\u5bf9\u9f50\uff0c\u663e\u8457\u51cf\u5c11\u4e0d\u5b89\u5168\u8f93\u51fa\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u751f\u6210\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u521b\u610f\u5a92\u4f53\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u63d0\u793a\u65f6\u53ef\u80fd\u4ea7\u751f\u4e0d\u5b89\u5168\u3001\u5192\u72af\u6027\u6216\u6587\u5316\u4e0d\u6070\u5f53\u7684\u5185\u5bb9\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u96be\u4ee5\u5728\u4e0d\u727a\u7272\u751f\u6210\u8d28\u91cf\u6216\u589e\u52a0\u9ad8\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u4f7f\u8f93\u51fa\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u3002", "method": "VALOR\u6846\u67b6\u5305\u542b\uff1a\u591a\u7ea7NSFW\u68c0\u6d4b\u5668\u8fc7\u6ee4\u8bcd\u6c47\u548c\u8bed\u4e49\u98ce\u9669\uff1b\u6587\u5316\u4ef7\u503c\u5bf9\u9f50\u6a21\u5757\u8bc6\u522b\u793e\u4f1a\u89c4\u8303\u3001\u5408\u6cd5\u6027\u548c\u4ee3\u8868\u6027\u4f26\u7406\u8fdd\u89c4\uff1b\u610f\u56fe\u6d88\u6b67\u5668\u68c0\u6d4b\u5fae\u5999\u6216\u95f4\u63a5\u7684\u4e0d\u5b89\u5168\u542b\u4e49\u3002\u68c0\u6d4b\u5230\u4e0d\u5b89\u5168\u5185\u5bb9\u65f6\uff0c\u7531\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u89d2\u8272\u7279\u5b9a\u6307\u4ee4\u4e0b\u9009\u62e9\u6027\u91cd\u5199\u63d0\u793a\uff0c\u4fdd\u6301\u7528\u6237\u610f\u56fe\u7684\u540c\u65f6\u786e\u4fdd\u5bf9\u9f50\u3002", "result": "\u5728\u5bf9\u6297\u6027\u3001\u6a21\u7cca\u6027\u548c\u4ef7\u503c\u654f\u611f\u63d0\u793a\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVALOR\u5c06\u4e0d\u5b89\u5168\u8f93\u51fa\u663e\u8457\u51cf\u5c11\u9ad8\u8fbe100.00%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63d0\u793a\u7684\u6709\u7528\u6027\u548c\u521b\u9020\u6027\u3002", "conclusion": "VALOR\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u90e8\u7f72\u5b89\u5168\u3001\u5bf9\u9f50\u4e14\u6709\u7528\u7684\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2511.12823", "categories": ["cs.SE", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.12823", "abs": "https://arxiv.org/abs/2511.12823", "authors": ["Sajed Jalil", "Shuvo Saha", "Hossain Mohammad Seym"], "title": "Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter", "comment": "AACL-IJCNLP 2025 Workshop BLP Shared Task 2, 6 pages, 7 figures, 3 tables", "summary": "Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language.\n  We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1(TDD)\u548c\u4ee3\u7801\u89e3\u91ca\u5668(CI)\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\u6765\u63d0\u9ad8\u5b5f\u52a0\u62c9\u8bed\u63d0\u793a\u7684\u4ee3\u7801\u751f\u6210\u51c6\u786e\u7387\uff0c\u8fbe\u523085%\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4f7f\u5c0f\u578b\u6a21\u578b\u8fbe\u5230\u5927\u578b\u6a21\u578b98%\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5b5f\u52a0\u62c9\u8bed\u67092.42\u4ebf\u6bcd\u8bed\u4f7f\u7528\u8005\uff0c\u4f46\u5728LLM\u8bad\u7ec3\u4e2d\u5f88\u5c11\u53d7\u5230\u5173\u6ce8\u3002\u73b0\u6709\u4ee3\u7801\u751f\u6210\u6280\u672f\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u548c\u8d44\u6e90\uff0c\u8be5\u5de5\u4f5c\u65e8\u5728\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u65b0\u5174\u5e02\u573a\u7528\u6237\u63d0\u4f9b\u672c\u5730\u8bed\u8a00\u7684\u5f3a\u5927\u4ee3\u7801\u751f\u6210\u5de5\u5177\u3002", "method": "\u7ed3\u5408\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1(TDD)\u548c\u4ee3\u7801\u89e3\u91ca\u5668(CI)\uff0c\u4f7f\u7528\u5f00\u6e90\u6743\u91cd\u6a21\u578b\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "result": "\u5c06\u5b5f\u52a0\u62c9\u8bed\u63d0\u793a\u7684\u4ee3\u7801\u751f\u6210\u57fa\u7ebf\u51c6\u786e\u7387\u63d0\u5347\u81f385%\uff0c\u6700\u5c0f\u6a21\u578b\u80fd\u8fbe\u5230\u6700\u5927\u6a21\u578b98%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u5728\u591a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2511.11752", "categories": ["cs.AI", "cs.DL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.11752", "abs": "https://arxiv.org/abs/2511.11752", "authors": ["S\u00f6ren Arlt", "Xuemei Gu", "Mario Krenn"], "title": "Towards autonomous quantum physics research using LLM agents with access to intelligent tools", "comment": "24 pages, 5 figures", "summary": "Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.", "AI": {"tldr": "AI-Mandel\u662f\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u751f\u6210\u5e76\u5b9e\u65bd\u91cf\u5b50\u7269\u7406\u5b66\u60f3\u6cd5\u7684LLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u5b83\u4ece\u6587\u732e\u4e2d\u83b7\u53d6\u7075\u611f\uff0c\u4f7f\u7528\u7279\u5b9a\u9886\u57dfAI\u5de5\u5177\u5c06\u60f3\u6cd5\u8f6c\u5316\u4e3a\u53ef\u7acb\u5373\u5728\u5b9e\u9a8c\u5ba4\u5b9e\u65bd\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524dAI\u5728\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\u4ecd\u4e3b\u8981\u4f9d\u8d56\u4eba\u7c7b\u63d0\u4f9b\u7814\u7a76\u95ee\u9898\uff0cAI\u751f\u6210\u7684\u521b\u610f\u5f80\u5f80\u6a21\u7cca\u4e14\u9700\u8981\u4eba\u5de5\u6267\u884c\u3002\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u751f\u6210\u5e76\u5b9e\u65bd\u60f3\u6cd5\u7684\u7cfb\u7edf\u5c06\u663e\u8457\u6539\u53d8\u4eba\u7c7b\u5728\u79d1\u5b66\u8fc7\u7a0b\u4e2d\u7684\u89d2\u8272\u3002", "method": "AI-Mandel\u5229\u7528LLM\u4ece\u6587\u732e\u4e2d\u751f\u6210\u60f3\u6cd5\uff0c\u5e76\u901a\u8fc7\u7279\u5b9a\u9886\u57df\u7684AI\u5de5\u5177\u5c06\u8fd9\u4e9b\u60f3\u6cd5\u8f6c\u5316\u4e3a\u5177\u4f53\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6848\u3002", "result": "AI-Mandel\u751f\u6210\u7684\u8bb8\u591a\u60f3\u6cd5\u5177\u6709\u79d1\u5b66\u4ef7\u503c\uff0c\u5176\u4e2d\u4e24\u4e2a\u60f3\u6cd5\u5df2\u72ec\u7acb\u64b0\u5199\u6210\u540e\u7eed\u79d1\u5b66\u8bba\u6587\u3002\u751f\u6210\u7684\u60f3\u6cd5\u5305\u62ec\u91cf\u5b50\u9690\u5f62\u4f20\u6001\u7684\u65b0\u53d8\u4f53\u3001\u4e0d\u786e\u5b9a\u56e0\u679c\u987a\u5e8f\u4e2d\u7684\u91cf\u5b50\u7f51\u7edc\u539f\u8bed\uff0c\u4ee5\u53ca\u57fa\u4e8e\u91cf\u5b50\u4fe1\u606f\u4f20\u8f93\u95ed\u73af\u7684\u65b0\u51e0\u4f55\u76f8\u4f4d\u6982\u5ff5\u3002", "conclusion": "AI-Mandel\u662fAI\u7269\u7406\u5b66\u5bb6\u7684\u539f\u578b\u6f14\u793a\uff0c\u80fd\u591f\u751f\u6210\u548c\u5b9e\u65bd\u5177\u4f53\u53ef\u884c\u7684\u60f3\u6cd5\u3002\u6784\u5efa\u6b64\u7c7b\u7cfb\u7edf\u4e0d\u4ec5\u6709\u52a9\u4e8e\u52a0\u901f\u79d1\u5b66\u53d1\u5c55\uff0c\u8fd8\u63ed\u793a\u4e86\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73AI\u79d1\u5b66\u5bb6\u6240\u9762\u4e34\u7684\u5177\u4f53\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2511.11770", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11770", "abs": "https://arxiv.org/abs/2511.11770", "authors": ["Floris Vossebeld", "Shenghui Wang"], "title": "Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction", "comment": null, "summary": "Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u6784\u5efaSPARQL\u67e5\u8be2\u6765\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u7684\u590d\u6742\u591a\u8df3\u95ee\u9898\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\u5373\u53ef\u5b66\u4e60\u6709\u6548\u7684\u67e5\u8be2\u8c03\u8bd5\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u7f3a\u4e4f\u57fa\u4e8e\u5b9e\u65f6\u6267\u884c\u53cd\u9988\u52a8\u6001\u8c03\u8bd5\u67e5\u8be2\u7684\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u4e00\u6b21\u6027\u751f\u6210\u590d\u6742SPARQL\u67e5\u8be2\u7684\u8106\u5f31\u6027\u9650\u5236\u4e86\u4e0e\u7ed3\u6784\u5316\u6570\u636e\u7684\u53ef\u9760\u4ea4\u4e92\u3002", "method": "\u4f7f\u7528\u4ec5\u901a\u8fc7\u7ed3\u679c\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u8bad\u7ec3\u76843B\u53c2\u6570\u6a21\u578b\uff0c\u5b66\u4e60\u8fed\u4ee3SPARQL\u6784\u5efa\u7684\u987a\u5e8f\u8fc7\u7a0b\u7b56\u7565\uff0c\u7ed3\u5408\u663e\u5f0f\u5ba1\u8bae\u63a8\u7406\u6b65\u9aa4\u4f5c\u4e3a\u8ba4\u77e5\u652f\u67b6\u3002", "result": "\u5728LC-QuAD 2.0\u7684\u53ef\u6267\u884c\u5b50\u96c6\u4e0a\u8fbe\u523049.7%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u5f3a\u7684\u8fed\u4ee3\u96f6\u6837\u672c\u57fa\u7ebf\u63d0\u9ad8\u4e8617.5\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u901a\u8fc7\u4ea4\u4e92\u6559\u6388\u667a\u80fd\u4f53\u638c\u63e1\u5f62\u5f0f\u5316\u7b26\u53f7\u5de5\u5177\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u84dd\u56fe\uff0c\u5f25\u5408\u4e86\u6982\u7387\u6027LLM\u4e0e\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u4e16\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12884", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.12884", "abs": "https://arxiv.org/abs/2511.12884", "authors": ["Worawalan Chatlatanagulchai", "Hao Li", "Yutaro Kashiwa", "Brittany Reid", "Kundjanasith Thonglek", "Pattara Leelaprute", "Arnon Rungsawang", "Bundit Manaskasemsak", "Bram Adams", "Ahmed E. Hassan", "Hajimu Iida"], "title": "Agent READMEs: An Empirical Study of Context Files for Agentic Coding", "comment": null, "summary": "Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files (\"READMEs for agents\") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.", "AI": {"tldr": "\u5bf92303\u4e2a\u4ee3\u7406\u4e0a\u4e0b\u6587\u6587\u4ef6\u7684\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6587\u4ef6\u662f\u590d\u6742\u3001\u96be\u4ee5\u9605\u8bfb\u7684\u914d\u7f6e\u4ee3\u7801\uff0c\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u6027\u9700\u6c42\u800c\u5ffd\u89c6\u5b89\u5168\u6027\u548c\u6027\u80fd\u7b49\u975e\u529f\u80fd\u6027\u9700\u6c42\u3002", "motivation": "\u4ee3\u7406\u7f16\u7801\u5de5\u5177\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u4ee3\u7406\u4e0a\u4e0b\u6587\u6587\u4ef6\u63d0\u4f9b\u9879\u76ee\u7ea7\u6307\u4ee4\u3002\u8fd9\u662f\u9996\u6b21\u5927\u89c4\u6a21\u7814\u7a76\u8fd9\u4e9b\u4e0a\u4e0b\u6587\u6587\u4ef6\u7684\u7ed3\u6784\u3001\u7ef4\u62a4\u548c\u5185\u5bb9\u7279\u5f81\u3002", "method": "\u5206\u6790\u4e86\u6765\u81ea1925\u4e2a\u4ed3\u5e93\u76842303\u4e2a\u4ee3\u7406\u4e0a\u4e0b\u6587\u6587\u4ef6\uff0c\u7814\u7a76\u5176\u7ed3\u6784\u3001\u7ef4\u62a4\u6a21\u5f0f\u548c16\u79cd\u6307\u4ee4\u7c7b\u578b\u7684\u5185\u5bb9\u5206\u5e03\u3002", "result": "\u53d1\u73b0\u4e0a\u4e0b\u6587\u6587\u4ef6\u662f\u52a8\u6001\u6f14\u5316\u7684\u914d\u7f6e\u4ee3\u7801\uff0c\u4e3b\u8981\u5305\u542b\u6784\u5efa\u8fd0\u884c\u547d\u4ee4(62.3%)\u3001\u5b9e\u73b0\u7ec6\u8282(69.9%)\u548c\u67b6\u6784\u4fe1\u606f(67.7%)\uff0c\u4f46\u5b89\u5168(14.5%)\u548c\u6027\u80fd(14.5%)\u7b49\u975e\u529f\u80fd\u6027\u9700\u6c42\u5f88\u5c11\u88ab\u6307\u5b9a\u3002", "conclusion": "\u5f00\u53d1\u8005\u4e3b\u8981\u4f7f\u7528\u4e0a\u4e0b\u6587\u6587\u4ef6\u4f7f\u4ee3\u7406\u529f\u80fd\u5316\uff0c\u4f46\u7f3a\u4e4f\u786e\u4fdd\u4ee3\u7406\u7f16\u5199\u4ee3\u7801\u5b89\u5168\u6027\u548c\u6027\u80fd\u7684\u9632\u62a4\u63aa\u65bd\uff0c\u9700\u8981\u6539\u8fdb\u5de5\u5177\u548c\u5b9e\u8df5\u3002", "topic": "agent analysis"}}
{"id": "2511.11592", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11592", "abs": "https://arxiv.org/abs/2511.11592", "authors": ["Guojian Zhan", "Likun Wang", "Pengcheng Wang", "Feihong Zhang", "Jingliang Duan", "Masayoshi Tomizuka", "Shengbo Eben Li"], "title": "Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL", "comment": "17 pages", "summary": "Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8f68\u8ff9\u71b5\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60(TECRL)\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u5956\u52b1\u548c\u71b5\u7684Q\u51fd\u6570\u5b66\u4e60\uff0c\u89e3\u51b3\u6700\u5927\u71b5RL\u4e2d\u7684\u975e\u5e73\u7a33Q\u503c\u4f30\u8ba1\u548c\u77ed\u89c6\u5c40\u90e8\u71b5\u8c03\u8282\u95ee\u9898\u3002", "motivation": "\u6700\u5927\u71b5RL\u6846\u67b6\u5b58\u5728\u4e24\u4e2a\u74f6\u9888\uff1a1) \u6e29\u5ea6\u53c2\u6570\u66f4\u65b0\u4e0e\u71b5\u6ce8\u5165\u5171\u540c\u5bfc\u81f4\u7684\u975e\u5e73\u7a33Q\u503c\u4f30\u8ba1\uff1b2) \u4ec5\u57fa\u4e8e\u5f53\u524d\u5355\u6b65\u71b5\u8c03\u8282\u6e29\u5ea6\u7684\u77ed\u89c6\u5c40\u90e8\u71b5\u8c03\u8282\uff0c\u672a\u8003\u8651\u7d2f\u79ef\u71b5\u7684\u957f\u671f\u5f71\u54cd\u3002", "method": "\u63d0\u51faTECRL\u6846\u67b6\uff1a1) \u5206\u522b\u5b66\u4e60\u5956\u52b1Q\u51fd\u6570\u548c\u71b5Q\u51fd\u6570\uff0c\u786e\u4fdd\u503c\u76ee\u6807\u4e0d\u53d7\u6e29\u5ea6\u66f4\u65b0\u5f71\u54cd\uff1b2) \u901a\u8fc7\u4e13\u7528\u71b5Q\u51fd\u6570\u91cf\u5316\u671f\u671b\u7d2f\u79ef\u71b5\uff0c\u5b9e\u65bd\u8f68\u8ff9\u71b5\u7ea6\u675f\u63a7\u5236\u7b56\u7565\u957f\u671f\u968f\u673a\u6027\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1DSAC-E\u7b97\u6cd5\uff0c\u6269\u5c55DSAC-T\u3002", "result": "\u5728OpenAI Gym\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDSAC-E\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u56de\u62a5\u548c\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "TECRL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6700\u5927\u71b5RL\u7684\u4e24\u4e2a\u5173\u952e\u74f6\u9888\uff0c\u901a\u8fc7\u8f68\u8ff9\u71b5\u7ea6\u675f\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.11831", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11831", "abs": "https://arxiv.org/abs/2511.11831", "authors": ["Wenhao Zhou", "Hao Zheng", "Rong Zhao"], "title": "TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.", "AI": {"tldr": "TopoPerception\u662f\u4e00\u4e2a\u57fa\u4e8e\u62d3\u6251\u6027\u8d28\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u4e25\u683c\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5168\u5c40\u611f\u77e5\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u4e0d\u5982\u968f\u673a\u731c\u6d4b\u3002", "motivation": "\u73b0\u6709LVLMs\u901a\u5e38\u5c06\u89c6\u89c9\u7279\u5f81\u4e0e\u9884\u8bad\u7ec3LLM\u5bf9\u9f50\uff0c\u8fd9\u4f7f\u5f97\u89c6\u89c9\u611f\u77e5\u6a21\u5757\u6210\u4e3a\u74f6\u9888\u3002\u4f20\u7edf\u8bc4\u4f30\u57fa\u51c6\u867d\u7136\u89c6\u89c9\u8bed\u4e49\u4e30\u5bcc\uff0c\u4f46\u5305\u542b\u5c40\u90e8\u6377\u5f84\uff0c\u53ef\u80fd\u9ad8\u4f30\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5229\u7528\u62d3\u6251\u6027\u8d28\u521b\u5efaTopoPerception\u57fa\u51c6\u6d4b\u8bd5\uff0c\u56e0\u4e3a\u62d3\u6251\u4f9d\u8d56\u4e8e\u56fe\u50cf\u5168\u5c40\u7ed3\u6784\u4e14\u5bf9\u5c40\u90e8\u7279\u5f81\u4e0d\u53d8\uff0c\u80fd\u591f\u5b9e\u73b0\u65e0\u6377\u5f84\u7684\u5168\u5c40\u611f\u77e5\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5373\u4f7f\u5728\u6700\u7c97\u7684\u611f\u77e5\u7c92\u5ea6\u4e0b\uff0c\u6240\u6709\u6a21\u578b\u8868\u73b0\u90fd\u4e0d\u4f18\u4e8e\u968f\u673a\u673a\u4f1a\uff0c\u8868\u660e\u7f3a\u4e4f\u5168\u5c40\u89c6\u89c9\u7279\u5f81\u611f\u77e5\u80fd\u529b\u3002\u6a21\u578b\u5bb6\u65cf\u5185\u51fa\u73b0\u4e00\u81f4\u8d8b\u52bf\uff1a\u63a8\u7406\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u51c6\u786e\u7387\u66f4\u4f4e\u3002", "conclusion": "\u4ec5\u6269\u5927\u6a21\u578b\u89c4\u6a21\u4e0d\u8db3\u4ee5\u89e3\u51b3\u8fd9\u4e00\u7f3a\u9677\uff0c\u53ef\u80fd\u9700\u8981\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\u6216\u67b6\u6784\u3002TopoPerception\u4e0d\u4ec5\u66b4\u9732\u4e86LVLMs\u7684\u5173\u952e\u74f6\u9888\uff0c\u8fd8\u4e3a\u6539\u8fdb\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u65b9\u5411\u548c\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2511.11933", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11933", "abs": "https://arxiv.org/abs/2511.11933", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "Bhuwan Dhingra", "David Edwin Carlson"], "title": "InData: Towards Secure Multi-Step, Tool-Based Data Analysis", "comment": null, "summary": "Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.", "AI": {"tldr": "\u63d0\u51fa\u4e86InData\u6570\u636e\u96c6\u6765\u8bc4\u4f30LLM\u5728\u591a\u6b65\u9aa4\u5de5\u5177\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524dLLM\u5728\u590d\u6742\u6570\u636e\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u654f\u611f\u7684\u6570\u636e\u5206\u6790\u573a\u666f\u4e0b\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u654f\u611f\u6570\u636e\u5206\u6790\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u901a\u8fc7\u9650\u5236\u76f4\u63a5\u4ee3\u7801\u751f\u6210\u548c\u6570\u636e\u8bbf\u95ee\uff0c\u8981\u6c42LLM\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u5b89\u5168\u5de5\u5177\u8fdb\u884c\u4ea4\u4e92\u3002", "method": "\u5f15\u5165Indirect Data Engagement (InData)\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\u7684\u6570\u636e\u5206\u6790\u95ee\u9898\uff0c\u8bc4\u4f3015\u4e2a\u5f00\u6e90LLM\u5728\u591a\u6b65\u9aa4\u5de5\u5177\u63a8\u7406\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u5927\u578b\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0897.3%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5728\u56f0\u96be\u4efb\u52a1\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0869.6%\uff09\uff0c\u663e\u793a\u5f53\u524dLLM\u7f3a\u4e4f\u7a33\u5065\u7684\u591a\u6b65\u9aa4\u5de5\u5177\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "InData\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30\u5177\u6709\u66f4\u5f3a\u591a\u6b65\u9aa4\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684LLM\uff0c\u5c06\u516c\u5f00\u53d1\u5e03\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002", "topic": "agent analysis"}}
{"id": "2511.13305", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.13305", "abs": "https://arxiv.org/abs/2511.13305", "authors": ["Rangeet Pan", "Raju Pavuluri", "Ruikai Huang", "Rahul Krishna", "Tyler Stennett", "Alessandro Orso", "Saurabh SInha"], "title": "SAINT: Service-level Integration Test Generation with Program Analysis and LLM-based Agents", "comment": "Accepted at ICSE'26", "summary": "Enterprise applications are typically tested at multiple levels, with service-level testing playing an important role in validating application functionality. Existing service-level testing tools, especially for RESTful APIs, often employ fuzzing and/or depend on OpenAPI specifications which are not readily available in real-world enterprise codebases. Moreover, these tools are limited in their ability to generate functional tests that effectively exercise meaningful scenarios. In this work, we present SAINT, a novel white-box testing approach for service-level testing of enterprise Java applications. SAINT combines static analysis, large language models (LLMs), and LLM-based agents to automatically generate endpoint and scenario-based tests. The approach builds two key models: an endpoint model, capturing syntactic and semantic information about service endpoints, and an operation dependency graph, capturing inter-endpoint ordering constraints. SAINT then employs LLM-based agents to generate tests. Endpoint-focused tests aim to maximize code and database interaction coverage. Scenario-based tests are synthesized by extracting application use cases from code and refining them into executable tests via planning, action, and reflection phases of the agentic loop. We evaluated SAINT on eight Java applications, including a proprietary enterprise application. Our results illustrate the effectiveness of SAINT in coverage, fault detection, and scenario generation. Moreover, a developer survey provides strong endorsement of the scenario-based tests generated by SAINT. Overall, our work shows that combining static analysis with agentic LLM workflows enables more effective, functional, and developer-aligned service-level test generation.", "AI": {"tldr": "SAINT\u662f\u4e00\u4e2a\u7528\u4e8e\u4f01\u4e1aJava\u5e94\u7528\u7a0b\u5e8f\u670d\u52a1\u7ea7\u6d4b\u8bd5\u7684\u767d\u76d2\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7ed3\u5408\u9759\u6001\u5206\u6790\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6765\u81ea\u52a8\u751f\u6210\u7aef\u70b9\u548c\u57fa\u4e8e\u573a\u666f\u7684\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u7684\u670d\u52a1\u7ea7\u6d4b\u8bd5\u5de5\u5177\u901a\u5e38\u4f9d\u8d56\u4e8eOpenAPI\u89c4\u8303\uff08\u5728\u4f01\u4e1a\u4ee3\u7801\u5e93\u4e2d\u4e0d\u6613\u83b7\u5f97\uff09\uff0c\u5e76\u4e14\u5728\u751f\u6210\u6709\u6548\u6267\u884c\u6709\u610f\u4e49\u573a\u666f\u7684\u529f\u80fd\u6d4b\u8bd5\u65b9\u9762\u80fd\u529b\u6709\u9650\u3002", "method": "SAINT\u6784\u5efa\u4e24\u4e2a\u5173\u952e\u6a21\u578b\uff1a\u7aef\u70b9\u6a21\u578b\uff08\u6355\u83b7\u670d\u52a1\u7aef\u70b9\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f\uff09\u548c\u64cd\u4f5c\u4f9d\u8d56\u56fe\uff08\u6355\u83b7\u7aef\u70b9\u95f4\u987a\u5e8f\u7ea6\u675f\uff09\u3002\u7136\u540e\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u751f\u6210\u6d4b\u8bd5\uff0c\u5305\u62ec\u7aef\u70b9\u805a\u7126\u6d4b\u8bd5\uff08\u6700\u5927\u5316\u4ee3\u7801\u548c\u6570\u636e\u5e93\u4ea4\u4e92\u8986\u76d6\u7387\uff09\u548c\u57fa\u4e8e\u573a\u666f\u7684\u6d4b\u8bd5\uff08\u901a\u8fc7\u4ee3\u7406\u5faa\u73af\u7684\u89c4\u5212\u3001\u884c\u52a8\u548c\u53cd\u601d\u9636\u6bb5\u4ece\u4ee3\u7801\u4e2d\u63d0\u53d6\u5e94\u7528\u7528\u4f8b\u5e76\u7cbe\u70bc\u4e3a\u53ef\u6267\u884c\u6d4b\u8bd5\uff09\u3002", "result": "\u5728\u516b\u4e2aJava\u5e94\u7528\u7a0b\u5e8f\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cSAINT\u5728\u8986\u76d6\u7387\u3001\u6545\u969c\u68c0\u6d4b\u548c\u573a\u666f\u751f\u6210\u65b9\u9762\u8868\u73b0\u6709\u6548\u3002\u5f00\u53d1\u8005\u8c03\u67e5\u5f3a\u70c8\u8ba4\u53efSAINT\u751f\u6210\u7684\u57fa\u4e8e\u573a\u666f\u7684\u6d4b\u8bd5\u3002", "conclusion": "\u5c06\u9759\u6001\u5206\u6790\u4e0e\u57fa\u4e8e\u4ee3\u7406\u7684LLM\u5de5\u4f5c\u6d41\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u6709\u6548\u3001\u529f\u80fd\u6027\u548c\u5f00\u53d1\u8005\u5bf9\u9f50\u7684\u670d\u52a1\u7ea7\u6d4b\u8bd5\u751f\u6210\u3002", "topic": "swe application"}}
{"id": "2511.11916", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11916", "abs": "https://arxiv.org/abs/2511.11916", "authors": ["Sinan Urgun", "Se\u00e7kin Ar\u0131"], "title": "An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR", "comment": "23 pages, 9 figures", "summary": "This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u56db\u79cdLLM\u6a21\u578b\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u56db\u79cd\u63a8\u7406\u67b6\u6784\u5728RAVEN-FAIR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u53d1\u73b0GPT-4.1-Mini\u8868\u73b0\u6700\u4f73\uff0c\u63a8\u7406\u6548\u679c\u5177\u6709\u6a21\u578b\u7279\u5f02\u6027\u3002", "motivation": "\u7cfb\u7edf\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u6027\u80fd\uff0c\u63a2\u7d22\u4e0d\u540c\u63a8\u7406\u67b6\u6784\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u56db\u79cdLLM\u6a21\u578b\u548c\u56db\u79cd\u63a8\u7406\u67b6\u6784\u5728RAVEN-FAIR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u5904\u7406\u6d41\u7a0b\uff08JSON\u63d0\u53d6\u3001LLM\u63a8\u7406\u3001\u5de5\u5177\u51fd\u6570\uff09\uff0c\u4f7f\u7528SSIM\u548cLPIPS\u6307\u6807\u8bc4\u4f30\u89c6\u89c9\u54cd\u5e94\uff0c\u5206\u6790\u601d\u7ef4\u94fe\u5206\u6570\u548c\u9519\u8bef\u7c7b\u578b\u3002", "result": "GPT-4.1-Mini\u5728\u6240\u6709\u67b6\u6784\u4e2d\u5747\u83b7\u5f97\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u591a\u667a\u80fd\u4f53\u67b6\u6784\u4f1a\u6539\u53d8\u8bed\u4e49\u548c\u6570\u503c\u5e73\u8861\u4f46\u6548\u679c\u4e0d\u4e00\u81f4\uff0c\u4e0d\u540c\u6a21\u578b\u5bf9\u67b6\u6784\u8bbe\u8ba1\u5177\u6709\u7279\u5f02\u6027\u654f\u611f\u5ea6\u3002", "conclusion": "\u63a8\u7406\u6548\u679c\u5177\u6709\u6a21\u578b\u7279\u5f02\u6027\uff0c\u54cd\u5e94\u8986\u76d6\u5ea6\u7684\u53d8\u5316\u4f7f\u8de8\u67b6\u6784\u76f4\u63a5\u6bd4\u8f83\u590d\u6742\u5316\uff0c\u91c7\u7528\u4e94\u6b21\u72ec\u7acb\u8fd0\u884c\u7684\u6700\u4f73\u7ed3\u679c\u4f5c\u4e3a\u6027\u80fd\u4e0a\u9650\u4f30\u8ba1\u3002", "topic": "agent analysis"}}
{"id": "2511.11921", "categories": ["cs.AI", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.11921", "abs": "https://arxiv.org/abs/2511.11921", "authors": ["Liudong Xing", "Janet", "Lin"], "title": "Looking Forward: Challenges and Opportunities in Agentic AI Reliability", "comment": "13 pages, 6 figures; This is a preprint of a chapter accepted for publication in Generative and Agentic AI Reliability: Architectures, Challenges, and Trust for Autonomous Systems, published by SpringerNature", "summary": "This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u6784\u5efa\u53ef\u9760AI\u7cfb\u7edf\uff08\u7279\u522b\u662f\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff09\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u524d\u666f\uff0c\u8ba8\u8bba\u4e86\u51cf\u8f7b\u7ea7\u8054\u6545\u969c\u98ce\u9669\u7684\u7814\u7a76\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u52a8\u6001\u73af\u5883\u3001\u4efb\u52a1\u6267\u884c\u4e0d\u4e00\u81f4\u6027\u3001\u4e0d\u53ef\u9884\u6d4b\u6d8c\u73b0\u884c\u4e3a\u7b49\u65b9\u9762\u7684\u7814\u7a76\u6311\u6218\u548c\u673a\u9047\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u7cfb\u7edf\u9762\u4e34\u7ea7\u8054\u6545\u969c\u3001\u52a8\u6001\u73af\u5883\u9002\u5e94\u3001\u4e0d\u53ef\u9884\u6d4b\u884c\u4e3a\u7b49\u591a\u91cd\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7814\u7a76\u6765\u6784\u5efa\u66f4\u53ef\u9760\u7684AI\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u8bc6\u522b\u5173\u952e\u7814\u7a76\u95ee\u9898\uff0c\u5305\u62ec\u7ea7\u8054\u6545\u969c\u7f13\u89e3\u3001\u52a8\u6001\u73af\u5883\u5904\u7406\u3001\u4efb\u52a1\u6267\u884c\u4e00\u81f4\u6027\u3001\u6d8c\u73b0\u884c\u4e3a\u9884\u6d4b\u7b49\uff0c\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u7814\u7a76\u65b9\u5411\u3002", "result": "\u8bc6\u522b\u4e86\u667a\u80fd\u4f53AI\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u591a\u4e2a\u5173\u952e\u6311\u6218\u9886\u57df\uff0c\u5305\u62ec\u7ea7\u8054\u6545\u969c\u98ce\u9669\u3001\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u3001\u4efb\u52a1\u6267\u884c\u4e0d\u4e00\u81f4\u6027\u3001\u4e0d\u53ef\u9884\u6d4b\u6d8c\u73b0\u884c\u4e3a\u7b49\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u7814\u7a76\u65b9\u5411\u548c\u89e3\u51b3\u65b9\u6848\u6846\u67b6\u3002", "conclusion": "\u6784\u5efa\u53ef\u9760\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\u9700\u8981\u8de8\u5b66\u79d1\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\u6545\u969c\u7f13\u89e3\u3001\u73af\u5883\u9002\u5e94\u6027\u3001\u884c\u4e3a\u9884\u6d4b\u548c\u9ad8\u6548\u53ef\u9760\u6027\u673a\u5236\u7b49\u65b9\u9762\uff0c\u4e3a\u672a\u6765AI\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2511.13341", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13341", "abs": "https://arxiv.org/abs/2511.13341", "authors": ["Zihe Yan", "Kai Luo", "Haoyu Yang", "Yang Yu", "Zhuosheng Zhang", "Guancheng Li"], "title": "An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains", "comment": "7 figures, 4 tables, conference", "summary": "In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u5f00\u6e90\u8f6f\u4ef6\u540e\u95e8\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u653b\u51fb\u8005\u89c6\u89d2\u548c\u5b9a\u4e49\u9488\u5bf9\u6027\u6307\u6807\u6765\u8bc4\u4f30\u5f00\u6e90\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u5f00\u6e90\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5728\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4f9d\u8d56\u9879\u7ef4\u62a4\u4e0d\u8db3\u548c\u793e\u533a\u5ba1\u8ba1\u4e0d\u5145\u5206\u5bfc\u81f4\u6e90\u4ee3\u7801\u5b89\u5168\u548c\u4ed3\u5e93\u7ef4\u62a4\u8005\u5408\u6cd5\u6027\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728XZ-Util\u4e8b\u4ef6\u7b49\u9ad8\u9690\u853d\u6027\u540e\u95e8\u653b\u51fb\u4e0b\u3002", "method": "\u4ece\u653b\u51fb\u8005\u89c6\u89d2\u5efa\u6a21\u9690\u853d\u540e\u95e8\u653b\u51fb\uff0c\u4e3a\u6bcf\u4e2a\u653b\u51fb\u9636\u6bb5\u5b9a\u4e49\u9488\u5bf9\u6027\u6307\u6807\uff1b\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4ee3\u7801\u4ed3\u5e93\u8bed\u4e49\u8bc4\u4f30\uff0c\u514b\u670d\u9759\u6001\u5206\u6790\u5728\u8bc4\u4f30\u4ed3\u5e93\u7ef4\u62a4\u6d3b\u52a8\u53ef\u9760\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "result": "\u5728Debian\u751f\u6001\u7cfb\u7edf\u768466\u4e2a\u9ad8\u4f18\u5148\u7ea7\u8f6f\u4ef6\u5305\u4e0a\u8bc4\u4f30\u8be5\u6846\u67b6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5f53\u524d\u5f00\u6e90\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u9762\u4e34\u591a\u79cd\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u5f00\u6e90\u8f6f\u4ef6\u7684\u540e\u95e8\u98ce\u9669\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5b58\u5728\u7684\u5b89\u5168\u9690\u60a3\u3002", "topic": "swe application"}}
{"id": "2511.11607", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11607", "abs": "https://arxiv.org/abs/2511.11607", "authors": ["Guoqing Ma", "Yuhan Zhang", "Yuming Dai", "Guangfu Hao", "Yang Chen", "Shan Yu"], "title": "Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.", "AI": {"tldr": "\u63d0\u51faCOWM\u5c42\u6765\u89e3\u51b3RL\u4e2d\u7684\u975e\u5e73\u7a33\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u805a\u7c7b\u6280\u672f\u548c\u6295\u5f71\u77e9\u9635\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u7a33\u5b9a\u6027", "motivation": "RL\u4ee3\u7406\u901a\u5e38\u5047\u8bbe\u73af\u5883\u662f\u5e73\u7a33\u7684\uff0c\u4f46\u5b9e\u9645\u73af\u5883\u5f80\u5f80\u662f\u975e\u5e73\u7a33\u7684\uff0c\u8fd9\u5bfc\u81f4\u9700\u8981\u6570\u767e\u4e07\u6b21\u8fed\u4ee3\uff0c\u6837\u672c\u6548\u7387\u4f4e\u4e0b", "method": "\u5f15\u5165COWM\u5c42\uff0c\u53ef\u96c6\u6210\u5230\u4efb\u4f55RL\u7b97\u6cd5\u7684\u7b56\u7565\u7f51\u7edc\u4e2d\uff0c\u4f7f\u7528\u805a\u7c7b\u6280\u672f\u548c\u6295\u5f71\u77e9\u9635\u6765\u7f13\u89e3\u975e\u5e73\u7a33\u6027", "result": "\u5728\u57fa\u4e8e\u89c6\u89c9\u548c\u72b6\u6001\u7684DMControl\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u63d0\u53479%\u548c12.6%\uff0c\u5728\u5404\u79cd\u7b97\u6cd5\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027", "conclusion": "COWM\u5c42\u80fd\u6709\u6548\u89e3\u51b3RL\u4e2d\u7684\u975e\u5e73\u7a33\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u7a33\u5b9a\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2511.12001", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12001", "abs": "https://arxiv.org/abs/2511.12001", "authors": ["Eunkyu Park", "Wesley Hanwen Deng", "Vasudha Varadarajan", "Mingxi Yan", "Gunhee Kim", "Maarten Sap", "Motahhare Eslami"], "title": "Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations", "comment": "Under review; 16 pages, 15 figures", "summary": "Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u601d\u7ef4\u94fe\u89e3\u91ca\u5728\u9053\u5fb7\u573a\u666f\u4e2d\u7684\u53cc\u91cd\u4f5c\u7528\uff1a\u65e2\u80fd\u589e\u5f3a\u900f\u660e\u5ea6\uff0c\u4e5f\u53ef\u80fd\u56e0\u786e\u8ba4\u504f\u89c1\u5bfc\u81f4\u7528\u6237\u76f2\u76ee\u4fe1\u4efb\uff0c\u5373\u4f7f\u63a8\u7406\u5b58\u5728\u9519\u8bef\u3002", "motivation": "\u63a2\u7d22\u601d\u7ef4\u94fe\u89e3\u91ca\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5bf9\u591a\u6a21\u6001\u9053\u5fb7\u573a\u666f\u4e2dAI\u63a8\u7406\u7684\u4fe1\u4efb\u548c\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\uff0c\u63ed\u793a\u89e3\u91ca\u53ef\u80fd\u5e26\u6765\u7684\u8bef\u5bfc\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u6270\u52a8\u63a8\u7406\u94fe\u548c\u64cd\u7eb5\u8868\u8fbe\u8bed\u6c14\uff0c\u5206\u6790\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63a8\u7406\u9519\u8bef\u53ca\u5176\u5bf9\u7528\u6237\u4fe1\u4efb\u548c\u9519\u8bef\u68c0\u6d4b\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u7528\u6237\u5e38\u5c06\u4fe1\u4efb\u4e0e\u7ed3\u679c\u4e00\u81f4\u6027\u7b49\u540c\uff0c\u5373\u4f7f\u63a8\u7406\u6709\u7f3a\u9677\u4e5f\u6301\u7eed\u4f9d\u8d56\uff1b\u81ea\u4fe1\u8bed\u6c14\u4f1a\u6291\u5236\u9519\u8bef\u68c0\u6d4b\u4f46\u7ef4\u6301\u4f9d\u8d56\uff0c\u8868\u660e\u8868\u8fbe\u98ce\u683c\u80fd\u51cc\u9a7e\u4e8e\u6b63\u786e\u6027\u4e4b\u4e0a\u3002", "conclusion": "\u601d\u7ef4\u94fe\u89e3\u91ca\u65e2\u80fd\u6f84\u6e05\u4e5f\u80fd\u8bef\u5bfc\uff0cNLP\u7cfb\u7edf\u9700\u8981\u63d0\u4f9b\u9f13\u52b1\u5ba1\u614e\u601d\u8003\u548c\u6279\u5224\u6027\u8bc4\u4f30\u800c\u975e\u76f2\u76ee\u4fe1\u4efb\u7684\u89e3\u91ca\u3002", "topic": "agent analysis"}}
{"id": "2511.13646", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13646", "abs": "https://arxiv.org/abs/2511.13646", "authors": ["Chunqiu Steven Xia", "Zhe Wang", "Yan Yang", "Yuxiang Wei", "Lingming Zhang"], "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?", "comment": null, "summary": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-G\u00f6del Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.", "AI": {"tldr": "Live-SWE-agent\u662f\u9996\u4e2a\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u81ea\u4e3b\u6301\u7eed\u8fdb\u5316\u7684\u8f6f\u4ef6\u4ee3\u7406\uff0c\u4ece\u57fa\u7840\u4ee3\u7406\u811a\u624b\u67b6\u5f00\u59cb\uff0c\u5728\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u95ee\u9898\u65f6\u81ea\u4e3b\u6f14\u5316\u5176\u5b9e\u73b0\uff0c\u5728SWE-bench Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523075.4%\u7684\u89e3\u51b3\u7387\u3002", "motivation": "\u73b0\u6709\u7684LLM\u8f6f\u4ef6\u4ee3\u7406\u901a\u5e38\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u4e14\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u800c\u81ea\u6539\u8fdb\u4ee3\u7406\u9700\u8981\u6602\u8d35\u7684\u79bb\u7ebf\u8bad\u7ec3\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u81ea\u4e3b\u8fdb\u5316\u7684\u8f6f\u4ef6\u4ee3\u7406\u3002", "method": "Live-SWE-agent\u4ece\u4ec5\u5305\u542bbash\u5de5\u5177\u7684\u57fa\u7840\u4ee3\u7406\u811a\u624b\u67b6\u5f00\u59cb\uff0c\u5728\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u95ee\u9898\u65f6\u81ea\u4e3b\u6f14\u5316\u5176\u811a\u624b\u67b6\u5b9e\u73b0\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5728\u7ebf\u8fdb\u5316\u3002", "result": "\u5728SWE-bench Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523075.4%\u7684\u89e3\u51b3\u7387\uff0c\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u5f00\u6e90\u8f6f\u4ef6\u4ee3\u7406\uff0c\u63a5\u8fd1\u6700\u4f73\u4e13\u6709\u89e3\u51b3\u65b9\u6848\u7684\u6027\u80fd\u3002\u5728SWE-Bench Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523045.8%\u7684\u6700\u4f73\u5df2\u77e5\u89e3\u51b3\u7387\u3002", "conclusion": "Live-SWE-agent\u8bc1\u660e\u4e86\u8f6f\u4ef6\u4ee3\u7406\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u81ea\u4e3b\u6301\u7eed\u8fdb\u5316\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "topic": "swe application"}}
{"id": "2511.12116", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12116", "abs": "https://arxiv.org/abs/2511.12116", "authors": ["Piotr P\u0119zik", "Konrad Kaczy\u0144ski", "Maria Szyma\u0144ska", "Filip \u017barnecki", "Zuzanna Deckert", "Jakub Kwiatkowski", "Wojciech Janowski"], "title": "LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.", "AI": {"tldr": "LLMLagBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u8bad\u7ec3\u6570\u636e\u65f6\u95f4\u8fb9\u754c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u6a21\u578b\u5bf9\u8fd1\u671f\u4e8b\u4ef6\u7684\u4e86\u89e3\u6765\u8bc6\u522b\u5176\u77e5\u8bc6\u7684\u65b0\u9c9c\u5ea6\u3002", "motivation": "LLM\u5728\u7279\u5b9a\u65f6\u95f4\u70b9\u524d\u7684\u6587\u672c\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8fd9\u5f62\u6210\u4e86\u4e25\u683c\u7684\u77e5\u8bc6\u8fb9\u754c\u3002\u5f53\u8fd9\u4e2a\u9650\u5236\u672a\u77e5\u6216\u88ab\u5ffd\u89c6\u65f6\uff0cLLM\u53ef\u80fd\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u65e0\u610f\u95f4\u6df7\u5408\u8fc7\u65f6\u7684\u65f6\u6548\u6027\u4fe1\u606f\u4e0e\u901a\u7528\u77e5\u8bc6\uff0c\u4ece\u800c\u5f71\u54cd\u56de\u7b54\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165LLMLagBench\u4f5c\u4e3a\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30LLM\u5bf9\u8fd1\u671f\u4e8b\u4ef6\u7684\u77e5\u8bc6\u6765\u8bc6\u522b\u5176\u8bad\u7ec3\u6570\u636e\u7684\u6700\u65e9\u53ef\u80fd\u65f6\u95f4\u8fb9\u754c\u3002\u5c06\u8be5\u57fa\u51c6\u5e94\u7528\u4e8e\u5927\u91cfLLM\u8bc4\u4f30\uff0c\u5305\u62ec\u6709\u660e\u786e\u58f0\u660e\u548c\u672a\u58f0\u660e\u8bad\u7ec3\u622a\u6b62\u65f6\u95f4\u7684\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u624b\u52a8\u9a8c\u8bc1\u548c\u4e0e\u516c\u5f00\u7684LLM\u9884\u8bad\u7ec3\u4fe1\u606f\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e86\u57fa\u51c6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "LLMLagBench\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8bc6\u522bLLM\u7684\u77e5\u8bc6\u65f6\u95f4\u8fb9\u754c\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u7684\u77e5\u8bc6\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.12133", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12133", "abs": "https://arxiv.org/abs/2511.12133", "authors": ["Qingyu Zhang", "Chunlei Xin", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Qing Ye", "Qianlong Xie", "Xingxing Wang"], "title": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing", "comment": null, "summary": "Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86AI-Salesman\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u67b6\u6784\u89e3\u51b3\u76ee\u6807\u9a71\u52a8\u8bf4\u670d\u5bf9\u8bdd\u4e2d\u7684\u7b56\u7565\u8106\u5f31\u6027\u548c\u4e8b\u5b9e\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u7535\u8bdd\u9500\u552e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u76ee\u6807\u9a71\u52a8\u8bf4\u670d\u5bf9\u8bdd\uff08\u5982\u7535\u8bdd\u9500\u552e\uff09\u9700\u8981\u590d\u6742\u591a\u8f6e\u89c4\u5212\u548c\u4e25\u683c\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u6570\u636e\uff0c\u4e14\u76f4\u63a5\u5e94\u7528LLM\u5b58\u5728\u7b56\u7565\u8106\u5f31\u6027\u548c\u4e8b\u5b9e\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u6784\u5efaTeleSalesCorpus\u6570\u636e\u96c6\uff0c\u63d0\u51faAI-Salesman\u53cc\u9636\u6bb5\u6846\u67b6\uff1a\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u8d1d\u53f6\u65af\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u4ece\u566a\u58f0\u5bf9\u8bdd\u4e2d\u5b66\u4e60\u7a33\u5065\u9500\u552e\u7b56\u7565\uff1b\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u52a8\u6001\u5927\u7eb2\u5f15\u5bfc\u4ee3\u7406\uff08DOGA\uff09\uff0c\u5229\u7528\u9884\u5efa\u811a\u672c\u5e93\u63d0\u4f9b\u52a8\u6001\u7b56\u7565\u6307\u5bfc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAI-Salesman\u5728\u81ea\u52a8\u6307\u6807\u548c\u7efc\u5408\u4eba\u5de5\u8bc4\u4f30\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u590d\u6742\u8bf4\u670d\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "AI-Salesman\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8bf4\u670d\u5bf9\u8bdd\u4e2d\u7684\u7b56\u7565\u89c4\u5212\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2511.13274", "categories": ["cs.LG", "cs.AI", "cs.MA", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.13274", "abs": "https://arxiv.org/abs/2511.13274", "authors": ["Taras Sereda", "Tom St. John", "Burak Bartan", "Natalie Serrino", "Sachin Katti", "Zain Asgar"], "title": "KForge: Program Synthesis for Diverse AI Hardware Accelerators", "comment": "Under review at MLSys 2026", "summary": "GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.\n  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.", "AI": {"tldr": "KForge\u662f\u4e00\u4e2a\u5e73\u53f0\u65e0\u5173\u7684GPU\u5185\u6838\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u4e24\u4e2a\u534f\u4f5c\u7684LLM\u4ee3\u7406\uff1a\u751f\u6210\u4ee3\u7406\u901a\u8fc7\u7f16\u8bd1\u548c\u6b63\u786e\u6027\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u7a0b\u5e8f\uff0c\u6027\u80fd\u5206\u6790\u4ee3\u7406\u89e3\u91ca\u6027\u80fd\u6570\u636e\u6307\u5bfc\u4f18\u5316\u3002\u8be5\u67b6\u6784\u53ea\u9700\u5355\u6b21\u793a\u4f8b\u5373\u53ef\u9488\u5bf9\u65b0\u5e73\u53f0\u3002", "motivation": "GPU\u5185\u6838\u5bf9ML\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u96be\u4ee5\u5728\u4e0d\u540c\u52a0\u901f\u5668\u4e0a\u8fdb\u884c\u4f18\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u5e73\u53f0\uff0c\u7f3a\u4e4f\u8de8\u5e73\u53f0\u901a\u7528\u6027\u3002", "method": "\u91c7\u7528\u53cc\u4ee3\u7406\u534f\u4f5c\u67b6\u6784\uff1a\u751f\u6210\u4ee3\u7406\u8d1f\u8d23\u7a0b\u5e8f\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u6027\u80fd\u5206\u6790\u4ee3\u7406\u89e3\u91ca\u6027\u80fd\u5206\u6790\u6570\u636e\u5e76\u63d0\u4f9b\u4f18\u5316\u5efa\u8bae\u3002\u652f\u6301\u4ece\u7a0b\u5e8f\u5316API\u5230GUI\u5de5\u5177\u7684\u5404\u79cd\u6027\u80fd\u6570\u636e\u3002", "result": "\u5c55\u793a\u4e86\u6709\u6548\u7684\u8de8\u5e73\u53f0\u77e5\u8bc6\u8f6c\u79fb\uff0c\u4ece\u4e00\u4e2a\u67b6\u6784\u7684\u53c2\u8003\u5b9e\u73b0\u663e\u8457\u63d0\u9ad8\u4e0d\u540c\u786c\u4ef6\u76ee\u6807\u7684\u751f\u6210\u8d28\u91cf\u3002\u5728NVIDIA CUDA\u548cApple Metal\u7b49\u4e0d\u540c\u5e76\u884c\u8ba1\u7b97\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u5e73\u53f0\u65e0\u5173\u6027\u3002", "conclusion": "KForge\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5e73\u53f0\u65e0\u5173\u7684GPU\u5185\u6838\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u534f\u4f5c\u4ee3\u7406\u67b6\u6784\u5b9e\u73b0\u8de8\u5e73\u53f0\u7a0b\u5e8f\u5408\u6210\uff0c\u53ea\u9700\u5355\u6b21\u793a\u4f8b\u5373\u53ef\u9002\u5e94\u65b0\u786c\u4ef6\u5e73\u53f0\u3002", "topic": "code agent"}}
{"id": "2511.12140", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12140", "abs": "https://arxiv.org/abs/2511.12140", "authors": ["Pinxue Guo", "Chongruo Wu", "Xinyu Zhou", "Lingyi Hong", "Zhaoyu Chen", "Jinglun Li", "Kaixun Jiang", "Sen-ching Samson Cheung", "Wei Zhang", "Wenqiang Zhang"], "title": "Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of \"Seeing is Believing\", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.", "AI": {"tldr": "VBackChecker\u662f\u4e00\u4e2a\u65e0\u9700\u53c2\u8003\u7684\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7Grounding LLM\u9a8c\u8bc1MLLM\u751f\u6210\u54cd\u5e94\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u4e00\u81f4\u6027\uff0c\u5728R^2-HalBench\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u51c6\u786e\u7684\u5e7b\u89c9\u68c0\u6d4b\u6765\u786e\u4fdd\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\"\u773c\u89c1\u4e3a\u5b9e\"\u539f\u5219\uff0c\u5229\u7528\u5177\u6709\u63a8\u7406\u548c\u5206\u5272\u80fd\u529b\u7684\u50cf\u7d20\u7ea7Grounding LLM\uff0c\u8bbe\u8ba1\u4e86\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u751f\u6210\u7ba1\u9053(R-Instruct)\u548c\u65b0\u7684\u5e7b\u89c9\u57fa\u51c6R^2-HalBench\u3002", "result": "VBackChecker\u5728R^2-HalBench\u4e0a\u4f18\u4e8e\u73b0\u6709\u590d\u6742\u6846\u67b6\uff0c\u751a\u81f3\u53ef\u4e0eGPT-4o\u7684\u5e7b\u89c9\u68c0\u6d4b\u80fd\u529b\u76f8\u5ab2\u7f8e\uff0c\u5728\u50cf\u7d20\u7ea7\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc710%\u3002", "conclusion": "VBackChecker\u4e3aMLLM\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53c2\u8003\u514d\u8d39\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5904\u7406\u4e30\u5bcc\u4e0a\u4e0b\u6587\u573a\u666f\u7684\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.12159", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12159", "abs": "https://arxiv.org/abs/2511.12159", "authors": ["Yaocheng Zhang", "Haohuan Huang", "Zijun Song", "Yuanheng Zhu", "Qichao Zhang", "Zijie Zhao", "Dongbin Zhao"], "title": "CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic", "comment": "17 pages, 10 figures", "summary": "Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.", "AI": {"tldr": "CriticSearch\u662f\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u56de\u987e\u6027\u6279\u8bc4\u673a\u5236\u63d0\u4f9b\u5bc6\u96c6\u7684\u56de\u5408\u7ea7\u53cd\u9988\uff0c\u89e3\u51b3\u641c\u7d22\u4ee3\u7406\u4e2d\u7a00\u758f\u5956\u52b1\u5bfc\u81f4\u7684\u4f4e\u6548\u63a2\u7d22\u548c\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u7ba1\u9053\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u4f46\u5b58\u5728\u7a00\u758f\u7ed3\u679c\u5956\u52b1\u95ee\u9898\uff0c\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u4f4e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u4e0d\u5bf9\u79f0\u6279\u8bc4LLM\uff0c\u5229\u7528\u5b8c\u6574\u8f68\u8ff9\u548c\u9ec4\u91d1\u7b54\u6848\u7684\u4f18\u5148\u4fe1\u606f\u5bf9\u6bcf\u4e2a\u56de\u5408\u8fdb\u884c\u56de\u987e\u6027\u8bc4\u4f30\uff0c\u5c06\u8bc4\u4f30\u8f6c\u5316\u4e3a\u7a33\u5b9a\u7684\u5bc6\u96c6\u5956\u52b1\u6765\u6307\u5bfc\u7b56\u7565\u6539\u8fdb\u3002", "result": "\u5728\u591a\u6837\u5316\u591a\u8df3\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCriticSearch\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "conclusion": "CriticSearch\u901a\u8fc7\u5bc6\u96c6\u7684\u56de\u5408\u7ea7\u53cd\u9988\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u641c\u7d22\u4ee3\u7406\u8bad\u7ec3\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12236", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12236", "abs": "https://arxiv.org/abs/2511.12236", "authors": ["Raavi Gupta", "Pranav Hari Panicker", "Sumit Bhatia", "Ganesh Ramakrishnan"], "title": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts", "comment": "To appear at International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL), 2025", "summary": "Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.", "AI": {"tldr": "CONFACTCHECK\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u67e5\u751f\u6210\u6587\u672c\u4e2d\u4e8b\u5b9e\u63a2\u9488\u54cd\u5e94\u7684\u4e00\u81f4\u6027\u6765\u68c0\u6d4bLLM\u5e7b\u89c9\uff0c\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u5e93\u4e14\u8d44\u6e90\u6d88\u8017\u66f4\u5c11\u3002", "motivation": "LLM\u5728\u751f\u6210\u6587\u672c\u65f6\u7ecf\u5e38\u4ea7\u751f\u4e8b\u5b9e\u9519\u8bef\u7684\u5e7b\u89c9\uff0c\u8fd9\u5728\u533b\u7597\u3001\u91d1\u878d\u7b49\u5173\u952e\u9886\u57df\u5b58\u5728\u4e25\u91cd\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u578b\u8bbf\u95ee\u53d7\u9650\u65f6\u9700\u8981\u591a\u6b21API\u8c03\u7528\uff0c\u589e\u52a0\u4e86\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "method": "\u57fa\u4e8e\u76f4\u89c9\uff1a\u751f\u6210\u6587\u672c\u4e2d\u4e8b\u5b9e\u63a2\u9488\u7684\u54cd\u5e94\u5e94\u8be5\u5728\u5355\u4e2aLLM\u5185\u90e8\u548c\u4e0d\u540cLLM\u4e4b\u95f4\u4fdd\u6301\u4e00\u81f4\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u5e93\uff0c\u901a\u8fc7\u68c0\u67e5\u54cd\u5e94\u4e00\u81f4\u6027\u6765\u68c0\u6d4b\u5e7b\u89c9\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCONFACTCHECK\u80fd\u591f\u4ee5\u66f4\u5c11\u8d44\u6e90\u9ad8\u6548\u68c0\u6d4b\u5e7b\u89c9\u4e8b\u5b9e\uff0c\u5728\u76f8\u4f3c\u6761\u4ef6\u4e0b\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u5f97\u5206\u3002", "conclusion": "CONFACTCHECK\u63d0\u4f9b\u4e86\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6a21\u578b\u8bbf\u95ee\u53d7\u9650\u6216\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u3002", "topic": "agent analysis"}}
{"id": "2511.12208", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12208", "abs": "https://arxiv.org/abs/2511.12208", "authors": ["Jilong Liu", "Pengyang Shao", "Wei Qin", "Fei Liu", "Yonghui Yang", "Richang Hong"], "title": "Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering", "comment": null, "summary": "Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86DoM\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u673a\u5236\u52a8\u6001\u6574\u5408\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u6765\u89e3\u51b3\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u66f4\u771f\u5b9e\u7684\u4e0d\u5b8c\u6574KGQA\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\u56fe\u8c31\u901a\u5e38\u4e0d\u5b8c\u6574\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u81ea\u9002\u5e94\u5730\u878d\u5408\u591a\u6e90\u77e5\u8bc6\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u77e5\u8bc6\u7684\u4e92\u8865\u6027\u3002", "method": "\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u8303\u5f0f\uff0cDoM\u5206\u914d\u4e13\u95e8\u667a\u80fd\u4f53\u5206\u522b\u5bf9\u77e5\u8bc6\u56fe\u8c31\u548c\u5916\u90e8\u6587\u672c\u8fdb\u884c\u63a8\u7406\uff0c\u901a\u8fc7\u8fed\u4ee3\u4ea4\u4e92\u534f\u8c03\u8f93\u51fa\u3002\u5206\u89e3\u8f93\u5165\u95ee\u9898\u4e3a\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u667a\u80fd\u4f53\uff08KG\u548cRAG\uff09\u68c0\u7d22\u8bc1\u636e\uff0c\u4f7f\u7528\u6cd5\u5b98\u667a\u80fd\u4f53\u8bc4\u4f30\u548c\u805a\u5408\u4e2d\u95f4\u7b54\u6848\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDoM\u5728\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DoM\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u4e92\u8865\u6027\u589e\u5f3a\u4e86\u5bf9\u6297\u77e5\u8bc6\u56fe\u8c31\u4e0d\u5b8c\u6574\u6027\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2511.11654", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11654", "abs": "https://arxiv.org/abs/2511.11654", "authors": ["Sayambhu Sen", "Shalabh Bhatnagar"], "title": "Convergence of Multiagent Learning Systems for Traffic control", "comment": "14 pages 2 figures", "summary": "Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u7684\u6536\u655b\u6027\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8be5\u7b97\u6cd5\u7684\u6536\u655b\u6027\u3002", "motivation": "\u968f\u7740\u57ce\u5e02\u5316\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4ea4\u901a\u62e5\u5835\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\u3002\u867d\u7136\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5df2\u88ab\u5b9e\u8bc1\u8bc1\u660e\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u6709\u6548\uff0c\u4f46\u5176\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\u7684\u7406\u8bba\u5206\u6790\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u968f\u673a\u903c\u8fd1\u65b9\u6cd5\uff0c\u6b63\u5f0f\u5206\u6790\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u5c06\u5355\u667a\u80fd\u4f53\u5f02\u6b65\u503c\u8fed\u4ee3\u7684\u6536\u655b\u6027\u8bc1\u660e\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u573a\u666f\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u7ed9\u5b9a\u6761\u4ef6\u4e0b\uff0c\u7528\u4e8e\u4ea4\u901a\u63a7\u5236\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u80fd\u591f\u6536\u655b\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u4ea4\u901a\u63a7\u5236\u9886\u57df\u7406\u8bba\u5206\u6790\u7684\u7a7a\u767d\uff0c\u4e3a\u7b97\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\u3002", "topic": "agent analysis"}}
{"id": "2511.12271", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12271", "abs": "https://arxiv.org/abs/2511.12271", "authors": ["Zhiyu An", "Wan Du"], "title": "MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning", "comment": "Accepted for AAAI 2026", "summary": "Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3LLM\u9053\u5fb7\u5bf9\u9f50\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efaMoral-Reason-QA\u6570\u636e\u96c6\u548c\u91c7\u7528Group Relative Policy Optimization\u8bad\u7ec3\u7b56\u7565\uff0c\u4f7fLLM\u80fd\u591f\u5728\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u7684\u9053\u5fb7\u573a\u666f\u4e2d\u5e94\u7528\u4e00\u81f4\u7684\u9053\u5fb7\u63a8\u7406\u6846\u67b6\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5f71\u54cd\u4eba\u7c7b\u9053\u5fb7\u51b3\u7b56\uff0c\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8bc4\u4f30\u800c\u975e\u4e3b\u52a8\u5f15\u5bfc\u5176\u9053\u5fb7\u51b3\u7b56\u3002\u9700\u8981\u89e3\u51b3LLM\u5728\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u573a\u666f\u4e2d\u7684\u9053\u5fb7\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5305\u542b680\u4e2a\u4eba\u5de5\u6807\u6ce8\u9ad8\u6a21\u7cca\u5ea6\u9053\u5fb7\u573a\u666f\u7684Moral-Reason-QA\u6570\u636e\u96c6\uff0c\u91c7\u7528Group Relative Policy Optimization\u7ed3\u5408\u51b3\u7b56\u5bf9\u9f50\u548c\u6846\u67b6\u7279\u5b9a\u63a8\u7406\u8fc7\u7a0b\u7684\u590d\u5408\u5956\u52b1\u51fd\u6570\uff0c\u8bad\u7ec3LLM\u5b66\u4e60\u5e95\u5c42\u9053\u5fb7\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u672a\u89c1\u9053\u5fb7\u573a\u666f\u4e0a\u6210\u529f\u5b9e\u73b0\u6cdb\u5316\uff0c\u529f\u5229\u4e3b\u4e49\u6846\u67b6\u7684softmax\u5f52\u4e00\u5316\u5bf9\u9f50\u5206\u6570\u63d0\u9ad8+0.757\uff0c\u4e49\u52a1\u8bba\u6846\u67b6\u63d0\u9ad8+0.450\u3002", "conclusion": "LLM\u4ee3\u7406\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u8bad\u7ec3\u4ee5\u5185\u5728\u5316\u5e76\u5c06\u7279\u5b9a\u9053\u5fb7\u6846\u67b6\u5e94\u7528\u4e8e\u65b0\u60c5\u5883\uff0c\u4e3aAI\u5b89\u5168\u5960\u5b9a\u5173\u952e\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2511.12306", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12306", "abs": "https://arxiv.org/abs/2511.12306", "authors": ["Darvin Yi", "Teng Liu", "Mattie Terzolo", "Lance Hasson", "Ayan Sinh", "Pablo Mendes", "Andrew Rabinovich"], "title": "UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI", "comment": null, "summary": "As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.", "AI": {"tldr": "UpBench\u662f\u4e00\u4e2a\u57fa\u4e8eUpwork\u771f\u5b9e\u5de5\u4f5c\u4efb\u52a1\u7684\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e13\u5bb6\u5236\u5b9a\u7684\u8be6\u7ec6\u8bc4\u4f30\u6807\u51c6\u6765\u8bc4\u4f30AI\u4ee3\u7406\u5728\u771f\u5b9e\u5de5\u4f5c\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u591a\u4e3a\u9759\u6001\u3001\u5408\u6210\u6216\u9886\u57df\u53d7\u9650\uff0c\u65e0\u6cd5\u53cd\u6620AI\u4ee3\u7406\u5728\u52a8\u6001\u3001\u7ecf\u6d4e\u610f\u4e49\u73af\u5883\u4e2d\u7684\u771f\u5b9e\u8868\u73b0\uff0c\u9700\u8981\u57fa\u4e8e\u771f\u5b9e\u5de5\u4f5c\u573a\u666f\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u4eceUpwork\u5e73\u53f0\u63d0\u53d6\u771f\u5b9e\u5de5\u4f5c\u4efb\u52a1\uff0c\u7531\u4e13\u5bb6\u81ea\u7531\u804c\u4e1a\u8005\u5236\u5b9a\u8be6\u7ec6\u8bc4\u4f30\u6807\u51c6\uff0c\u5bf9AI\u63d0\u4ea4\u5185\u5bb9\u8fdb\u884c\u9010\u9879\u8bc4\u4f30\uff0c\u5e76\u5b9a\u671f\u66f4\u65b0\u4efb\u52a1\u4ee5\u53cd\u6620\u5de5\u4f5c\u73af\u5883\u53d8\u5316\u3002", "result": "\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684AI\u80fd\u529b\u5206\u6790\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u901a\u8fc7/\u5931\u8d25\u4e8c\u5143\u8bc4\u4f30\uff0c\u80fd\u591f\u8bc6\u522b\u6a21\u578b\u7684\u5177\u4f53\u4f18\u52bf\u548c\u5f31\u70b9\u3002", "conclusion": "UpBench\u4e3a\u5728\u771f\u5b9e\u52b3\u52a8\u529b\u5e02\u573a\u73af\u5883\u4e2d\u8bc4\u4f30AI\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4ee5\u4eba\u4e3a\u672c\u7684\u57fa\u7840\uff0c\u652f\u6301AI\u901a\u8fc7\u5408\u4f5c\u800c\u975e\u66ff\u4ee3\u6765\u589e\u5f3a\u4eba\u7c7b\u80fd\u529b\u7684\u7814\u7a76\u3002", "topic": "swe benchmark"}}
{"id": "2511.12344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12344", "abs": "https://arxiv.org/abs/2511.12344", "authors": ["Baolong Bi", "Shenghua Liu", "Yiwei Wang", "Siqian Tong", "Lingrui Mei", "Yuyao Ge", "Yilong Xu", "Jiafeng Guo", "Xueqi Cheng"], "title": "Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning", "comment": null, "summary": "Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRGR-GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u5206\u6807\u51c6\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5956\u52b1\u548c\u79bb\u7ebf\u6307\u5bfc\uff0c\u5728\u591a\u9886\u57df\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u9886\u57df\u4e14\u4f9d\u8d56\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u7eaf\u5728\u7ebfRL\u6846\u67b6\u9650\u5236\u4e86\u63a2\u7d22\u7a7a\u95f4\uff0c\u5f71\u54cd\u4e86\u63a8\u7406\u6027\u80fd\u3002", "method": "\u63d0\u51faRGR-GRPO\u6846\u67b6\uff0c\u5229\u7528\u8bc4\u5206\u6807\u51c6\u63d0\u4f9b\u5bc6\u96c6\u4fe1\u606f\u5956\u52b1\uff0c\u5728GRPO\u8bad\u7ec3\u671f\u95f4\u63a2\u7d22\u66f4\u5927\u7684\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u3002", "result": "\u572814\u4e2a\u591a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRGR-GRPO\u663e\u8457\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u66ff\u4ee3\u5956\u52b1\u65b9\u6848\u6216\u79bb\u7ebf\u6307\u5bfc\u7684RL\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u548c\u4e00\u822c\u63a8\u7406\u4efb\u52a1\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u53477.0%\u30015.4%\u30018.4%\u548c6.6%\u3002", "conclusion": "RGR-GRPO\u5728\u79bb\u7ebf\u7b56\u7565\u8bad\u7ec3\u671f\u95f4\u4fdd\u6301\u7a33\u5b9a\u7684\u71b5\u6ce2\u52a8\uff0c\u5b9e\u73b0\u5353\u8d8a\u7684pass@k\u6027\u80fd\uff0c\u53cd\u6620\u4e86\u6301\u7eed\u63a2\u7d22\u548c\u6709\u6548\u7a81\u7834\u73b0\u6709\u6027\u80fd\u74f6\u9888\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12378", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12378", "abs": "https://arxiv.org/abs/2511.12378", "authors": ["Dylan M. Asmar", "Mykel J. Kochenderfer"], "title": "Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making", "comment": "Under Review", "summary": "Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u52a8\u6001\u5b66\u4e60\u548c\u9002\u5e94\u5efa\u8bae\u8005\u53ef\u9760\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u63a8\u65ad\u5efa\u8bae\u8005\u7c7b\u578b\uff0c\u5e76\u5f15\u5165\"\u8be2\u95ee\"\u52a8\u4f5c\u6765\u7b56\u7565\u6027\u5730\u8bf7\u6c42\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5efa\u8bae\u8005\u8d28\u91cf\u53c2\u6570\u662f\u9759\u6001\u4e14\u5df2\u77e5\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u80fd\u591f\u52a8\u6001\u9002\u5e94\u53d8\u5316\u5efa\u8bae\u8005\u53ef\u9760\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u5efa\u8bae\u8005\u8d28\u91cf\u76f4\u63a5\u96c6\u6210\u5230\u667a\u80fd\u4f53\u7684\u4fe1\u5ff5\u8868\u793a\u4e2d\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u63a8\u65ad\u5efa\u8bae\u8005\u7c7b\u578b\uff1b\u5f15\u5165\u663e\u5f0f\u7684\"\u8be2\u95ee\"\u52a8\u4f5c\uff0c\u8ba9\u667a\u80fd\u4f53\u5728\u5173\u952e\u65f6\u523b\u7b56\u7565\u6027\u5730\u8bf7\u6c42\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5efa\u8bae\u8005\u8d28\u91cf\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u80fd\u591f\u9002\u5e94\u53d8\u5316\u7684\u53ef\u9760\u6027\uff0c\u5e76\u7b56\u7565\u6027\u5730\u7ba1\u7406\u5efa\u8bae\u8bf7\u6c42\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u89e3\u51b3\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5efa\u8bae\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u81ea\u9002\u5e94\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12596", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12596", "abs": "https://arxiv.org/abs/2511.12596", "authors": ["Oron Anschel", "Alon Shoshan", "Adam Botach", "Shunit Haviv Hakimi", "Asaf Gendler", "Emanuel Ben Baruch", "Nadav Bhonker", "Igor Kviatkovsky", "Manoj Aggarwal", "Gerard Medioni"], "title": "Group-Aware Reinforcement Learning for Output Diversity in Large Language Models", "comment": "EMNLP Main 2025", "summary": "Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.", "AI": {"tldr": "GAPO\u662f\u4e00\u79cd\u57fa\u4e8eGRPO\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u7fa4\u4f53\u5c42\u9762\u7684\u5956\u52b1\u6765\u89e3\u51b3LLM\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u54cd\u5e94\u7684\u591a\u6837\u6027\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5373\u4f7f\u5b58\u5728\u591a\u4e2a\u6709\u6548\u7b54\u6848\uff0c\u6a21\u578b\u4e5f\u503e\u5411\u4e8e\u91cd\u590d\u751f\u6210\u76f8\u540c\u7684\u51e0\u4e2a\u8865\u5168\u7ed3\u679c\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u7684\u591a\u6837\u6027\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86Group-Aware Policy Optimization (GAPO)\uff0c\u8fd9\u662f\u5bf9GRPO\u7684\u7b80\u5355\u6269\u5c55\uff0c\u901a\u8fc7\u8ba1\u7b97\u7fa4\u4f53\u5c42\u9762\u7684\u5956\u52b1\u6765\u5b66\u4e60\u7fa4\u4f53\u7ea7\u5c5e\u6027\uff08\u5982\u591a\u6837\u6027\u548c\u8986\u76d6\u7387\uff09\uff0c\u5e76\u4f7f\u7528\u9891\u7387\u611f\u77e5\u7684\u5956\u52b1\u51fd\u6570\u9f13\u52b1\u5bf9\u6709\u6548LLM\u8865\u5168\u7684\u5747\u5300\u91c7\u6837\u3002", "result": "GAPO\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u591f\u4ea7\u751f\u6709\u6548\u4e14\u66f4\u591a\u6837\u5316\u7684\u54cd\u5e94\uff0c\u5728\u5f00\u653e\u63d0\u793a\u4e0b\u4e5f\u80fd\u6cdb\u5316\uff0c\u5e76\u5728\u6807\u51c6LLM\u57fa\u51c6\u6d4b\u8bd5\uff08GSM8K\u3001MATH\u3001HumanEval\u3001MMLU-Pro\uff09\u4e0a\u63d0\u9ad8\u54cd\u5e94\u591a\u6837\u6027\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "conclusion": "GAPO\u662f\u4e00\u79cd\u6709\u6548\u7684\u7fa4\u4f53\u611f\u77e5\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347LLM\u54cd\u5e94\u7684\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u4e3a\u89e3\u51b3\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12661", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12661", "abs": "https://arxiv.org/abs/2511.12661", "authors": ["Yuchen Wu", "Liang Ding", "Li Shen", "Dacheng Tao"], "title": "Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing", "comment": null, "summary": "Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a \"faithfulness gap\": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning \"Houston\" from \"NASA\" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.", "AI": {"tldr": "Reason-KE++\u901a\u8fc7SFT+RL\u6846\u67b6\u89e3\u51b3LLM\u5728\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u4f7f\u7528\u9636\u6bb5\u611f\u77e5\u5956\u52b1\u673a\u5236\u5bf9\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u5bc6\u96c6\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709SFT\u65b9\u6cd5\u5728\u590d\u6742\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\"\u5fe0\u5b9e\u6027\u5dee\u8ddd\"\uff0cLLM\u5f3a\u5927\u7684\u53c2\u6570\u5148\u9a8c\u4f1a\u8986\u76d6\u65b0\u7684\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5173\u952e\u7684\u4e8b\u5b9e\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u63d0\u51faReason-KE++\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u91c7\u7528\u9636\u6bb5\u611f\u77e5\u5956\u52b1\u673a\u5236\u5bf9\u5206\u89e3\u3001\u5b50\u7b54\u6848\u6b63\u786e\u6027\u7b49\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u63d0\u4f9b\u5bc6\u96c6\u76d1\u7763\u3002", "result": "\u5728MQUAKE-CF-3k\u6570\u636e\u96c6\u4e0a\u8fbe\u523095.48%\u7684\u65b0SOTA\uff0c\u76f8\u6bd4\u4e4b\u524d\u65b9\u6cd5\u63d0\u53475.28%\uff0c\u8bc1\u660e\u8fc7\u7a0b\u7ea7\u5bf9\u9f50\u5bf9\u6784\u5efa\u53ef\u4fe1LLM\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u5bf9\u4e8e\u590d\u6742\u4efb\u52a1\uff0c\u5bf9\u9f50\u63a8\u7406\u8fc7\u7a0b\u6bd4\u4ec5\u5173\u6ce8\u6700\u7ec8\u7ed3\u679c\u66f4\u91cd\u8981\uff0c\u8fc7\u7a0b\u611f\u77e5\u6846\u67b6\u662f\u6784\u5efa\u53ef\u4fe1LLM\u7684\u5173\u952e\u3002", "topic": "agent analysis"}}
{"id": "2511.12710", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12710", "abs": "https://arxiv.org/abs/2511.12710", "authors": ["Yunhao Chen", "Xin Wang", "Juncheng Li", "Yixu Wang", "Jie Li", "Yan Teng", "Yingchun Wang", "Xingjun Ma"], "title": "Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs", "comment": null, "summary": "Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \\textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.", "AI": {"tldr": "EvoSynth\u662f\u4e00\u4e2a\u81ea\u4e3b\u6846\u67b6\uff0c\u901a\u8fc7\u8fdb\u5316\u5408\u6210\u800c\u975e\u4f20\u7edf\u653b\u51fb\u89c4\u5212\u6765\u751f\u6210\u8d8a\u72f1\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8685.5%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u7ea2\u961f\u6846\u67b6\u7684\u8d8a\u72f1\u903b\u8f91\u5c40\u9650\u4e8e\u9009\u62e9\u3001\u7ec4\u5408\u6216\u6539\u8fdb\u73b0\u6709\u653b\u51fb\u7b56\u7565\uff0c\u65e0\u6cd5\u81ea\u4e3b\u53d1\u660e\u5168\u65b0\u653b\u51fb\u673a\u5236\uff0c\u9650\u5236\u4e86\u521b\u9020\u529b\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u4e3b\u8bbe\u8ba1\u3001\u8fdb\u5316\u548c\u6267\u884c\u57fa\u4e8e\u4ee3\u7801\u7684\u65b0\u578b\u653b\u51fb\u7b97\u6cd5\uff0c\u5305\u542b\u4ee3\u7801\u7ea7\u81ea\u6821\u6b63\u5faa\u73af\uff0c\u80fd\u591f\u6839\u636e\u5931\u8d25\u8fed\u4ee3\u91cd\u5199\u653b\u51fb\u903b\u8f91\u3002", "result": "\u5728Claude-Sonnet-4.5\u7b49\u5f3a\u5065\u6a21\u578b\u4e0a\u8fbe\u523085.5%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u751f\u6210\u7684\u653b\u51fb\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u52a0\u591a\u6837\u5316\u3002", "conclusion": "EvoSynth\u901a\u8fc7\u8fdb\u5316\u5408\u6210\u8d8a\u72f1\u65b9\u6cd5\u5efa\u7acb\u4e86\u65b0\u7684\u6280\u672f\u6807\u6746\uff0c\u4e3a\u8fd9\u4e00\u7814\u7a76\u65b9\u5411\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2511.12754", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12754", "abs": "https://arxiv.org/abs/2511.12754", "authors": ["Benjamin Li", "Shuyang Shi", "Lucia Romero", "Huao Li", "Yaqi Xie", "Woojun Kim", "Stefanos Nikolaidis", "Michael Lewis", "Katia Sycara", "Simon Stepputtis"], "title": "Adaptively Coordinating with Novel Partners via Learned Latent Strategies", "comment": "Accepted to NeurIPS 2025", "summary": "Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b56\u7565\u6761\u4ef6\u5316\u5408\u4f5c\u8005\u6846\u67b6\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7b56\u7565\u7a7a\u95f4\uff0c\u805a\u7c7b\u8bc6\u522b\u4e0d\u540c\u7b56\u7565\u7c7b\u578b\uff0c\u8bad\u7ec3\u6761\u4ef6\u5316\u5408\u4f5c\u8005\u4ee3\u7406\uff0c\u5e76\u5229\u7528\u56fa\u5b9a\u4efd\u989d\u9057\u61be\u6700\u5c0f\u5316\u7b97\u6cd5\u5728\u4ea4\u4e92\u4e2d\u52a8\u6001\u63a8\u65ad\u548c\u8c03\u6574\u4f19\u4f34\u7b56\u7565\u4f30\u8ba1\u3002", "motivation": "\u5728\u4eba\u7c7b-\u4ee3\u7406\u56e2\u961f\u4e2d\uff0c\u4eba\u5de5\u4ee3\u7406\u9700\u8981\u5b9e\u65f6\u9002\u5e94\u4eba\u7c7b\u4f19\u4f34\u7684\u72ec\u7279\u504f\u597d\u548c\u7b56\u7565\uff0c\u8fd9\u5728\u65f6\u95f4\u538b\u529b\u548c\u590d\u6742\u7b56\u7565\u7a7a\u95f4\u7684\u4efb\u52a1\u4e2d\u5c24\u4e3a\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7f16\u7801\u7b56\u7565\u5b66\u4e60\u6f5c\u5728\u7b56\u7565\u7a7a\u95f4\uff0c\u901a\u8fc7\u805a\u7c7b\u8bc6\u522b\u7b56\u7565\u7c7b\u578b\uff0c\u8bad\u7ec3\u6761\u4ef6\u5316\u5408\u4f5c\u8005\u4ee3\u7406\uff0c\u5e76\u91c7\u7528\u56fa\u5b9a\u4efd\u989d\u9057\u61be\u6700\u5c0f\u5316\u7b97\u6cd5\u8fdb\u884c\u5728\u7ebf\u9002\u5e94\u3002", "result": "\u5728\u4fee\u6539\u7248Overcooked\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0e\u65b0\u4eba\u7c7b\u548c\u4ee3\u7406\u961f\u53cb\u914d\u5bf9\u65f6\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b56\u7565\u6761\u4ef6\u5316\u5408\u4f5c\u8005\u6846\u67b6\u80fd\u591f\u6709\u6548\u9002\u5e94\u5e7f\u6cdb\u7684\u4f19\u4f34\u7b56\u7565\uff0c\u5728\u590d\u6742\u534f\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agent analysis"}}
{"id": "2511.12728", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12728", "abs": "https://arxiv.org/abs/2511.12728", "authors": ["Lea Hergert", "G\u00e1bor Berend", "Mario Szegedy", "Gyorgy Turan", "M\u00e1rk Jelasity"], "title": "On the Brittleness of LLMs: A Journey around Set Membership", "comment": null, "summary": "Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \\{pear, plum, apple, raspberry\\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.", "AI": {"tldr": "LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u8d85\u4eba\u7c7b\uff0c\u4f46\u5728\u7b80\u5355\u96c6\u5408\u6210\u5458\u67e5\u8be2\u7b49\u57fa\u7840\u4efb\u52a1\u4e0a\u5374\u7ecf\u5e38\u5931\u8d25\uff0c\u663e\u793a\u51fa\u5176\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u5b58\u5728\u95ee\u9898\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u7b80\u5355\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u901a\u8fc7\u7b80\u5355\u6027\u548c\u89c4\u6a21\u5316\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u6765\u5168\u9762\u5206\u6790\u5176\u5931\u8d25\u539f\u56e0\u3002", "method": "\u4f7f\u7528\u96c6\u5408\u6210\u5458\u67e5\u8be2\u4f5c\u4e3a\u57fa\u7840\u63a8\u7406\u4efb\u52a1\uff0c\u7cfb\u7edf\u8bc4\u4f30\u63d0\u793a\u63aa\u8f9e\u3001\u8bed\u4e49\u7ed3\u6784\u3001\u5143\u7d20\u6392\u5e8f\u548c\u6a21\u578b\u9009\u62e9\u7b49\u7ef4\u5ea6\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u3002", "result": "LLMs\u5728\u8fd9\u4e2a\u57fa\u7840\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u6301\u7eed\u8106\u5f31\u4e14\u4e0d\u53ef\u9884\u6d4b\uff0c\u8868\u660e\u6a21\u578b\u5bf9\u96c6\u5408\u6982\u5ff5\u7684\u7406\u89e3\u662f\u788e\u7247\u5316\u548c\u590d\u6742\u7684\u3002", "conclusion": "\u901a\u8fc7\u7b80\u5355\u95ee\u9898\u7684\u5927\u89c4\u6a21\u5b9e\u9a8c\u53ef\u4ee5\u5168\u9762\u6620\u5c04\u548c\u5206\u6790LLMs\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5bf9LLM\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2511.12792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12792", "abs": "https://arxiv.org/abs/2511.12792", "authors": ["Mohamad A. Hady", "Siyi Hu", "Mahardhika Pratama", "Zehong Cao", "Ryszard Kowalczyk"], "title": "Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization", "comment": null, "summary": "This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5f02\u6784\u536b\u661f\u96c6\u7fa4\u7684\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u8d44\u6e90\u5206\u914d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5b9e\u65f6\u3001\u4e0d\u786e\u5b9a\u548c\u5206\u6563\u5f0f\u64cd\u4f5c\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u4e2d\u7684\u5b9e\u65f6\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u5206\u6563\u5f0f\u64cd\u4f5c\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u5b9e\u73b0\u81ea\u9002\u5e94\u51b3\u7b56\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u5c06\u4f18\u5316\u95ee\u9898\u4ece\u5355\u536b\u661f\u6269\u5c55\u5230\u591a\u536b\u661f\u573a\u666f\uff0c\u4f7f\u7528Basilisk\u548cBSK-RL\u6846\u67b6\u6784\u5efa\u8fd1\u771f\u5b9e\u4eff\u771f\u73af\u5883\uff0c\u8bc4\u4f30MAPPO\u3001HAPPO\u548cHATRPO\u7b49\u5148\u8fdbMARL\u7b97\u6cd5\u3002", "result": "MARL\u80fd\u591f\u6709\u6548\u534f\u8c03\u5f02\u6784\u536b\u661f\uff0c\u5e73\u8861\u6210\u50cf\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\uff0c\u540c\u65f6\u7f13\u89e3\u975e\u5e73\u7a33\u6027\u548c\u667a\u80fd\u4f53\u95f4\u5956\u52b1\u8026\u5408\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u6269\u5c55\u7684\u81ea\u4e3b\u536b\u661f\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u5728\u5f02\u6784\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u667a\u80fd\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u89c4\u5212\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12782", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12782", "abs": "https://arxiv.org/abs/2511.12782", "authors": ["Thomas Rivasseau"], "title": "LLM Reinforcement in Context", "comment": "4 pages", "summary": "Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u4e2d\u65ad\u673a\u5236\u6765\u589e\u5f3aLLM\u5bf9\u9f50\u6027\uff0c\u9632\u6b62\u8d8a\u72f1\u548c\u6076\u610f\u884c\u4e3a", "motivation": "\u5f53\u524dLLM\u5bf9\u9f50\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bf9\u6297\u6027\u653b\u51fb\u548c\u9519\u8bef\u884c\u4e3a\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u7f3a\u4e4f\u968f\u7740\u7528\u6237\u8f93\u5165\u957f\u5ea6\u589e\u52a0\u800c\u6269\u5c55\u7684\u5bf9\u9f50\u65b9\u6cd5", "method": "\u5728\u7528\u6237\u8f93\u5165\u4e2d\u6bcfx\u4e2atoken\u6dfb\u52a0\u63a7\u5236\u8bed\u53e5\u4f5c\u4e3a\u4e2d\u65ad\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u601d\u7ef4\u94fe\u8fc7\u7a0b\u4ee5\u9632\u6b62\u9634\u8c0b", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e2d\u65ad\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c", "conclusion": "\u4e2d\u65ad\u673a\u5236\u662f\u589e\u5f3aLLM\u5bf9\u9f50\u6027\u7684\u6f5c\u5728\u65b9\u6cd5\uff0c\u80fd\u591f\u968f\u7740\u8f93\u5165\u957f\u5ea6\u6269\u5c55", "topic": "agent analysis"}}
{"id": "2511.12844", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12844", "abs": "https://arxiv.org/abs/2511.12844", "authors": ["Julia Santaniello", "Matthew Russell", "Benson Jiang", "Donatello Sassaroli", "Robert Jacob", "Jivko SInapov"], "title": "Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback", "comment": "Accepted to the Association for the Advancement of Artificial Intelligence (AAAI) 2026. To appear in the AAAI 2026 Proceedings", "summary": "Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u88ab\u52a8\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u548cfNIRS\u795e\u7ecf\u4fe1\u53f7\u6765\u6307\u5bfc\u667a\u80fd\u4f53\u8bad\u7ec3\uff0c\u901a\u8fc7\u9884\u6d4b\u667a\u80fd\u4f53\u6027\u80fd\u6c34\u5e73\u5b9e\u73b0\u8111\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edfRLHF\u9700\u8981\u663e\u5f0f\u7684\u4eba\u7c7b\u53cd\u9988\uff0c\u800c\u672c\u6587\u65e8\u5728\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u4fe1\u53f7\u6765\u5bf9\u9f50\u667a\u80fd\u4f53\u884c\u4e3a\u4e0e\u4eba\u7c7b\u504f\u597d\uff0c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u8d1f\u62c5\u3002", "method": "\u6536\u96c625\u540d\u53c2\u4e0e\u8005\u5728\u4e09\u4e2a\u9886\u57df\uff08\u673a\u5668\u4eba\u3001\u6708\u7403\u7740\u9646\u5668\u3001Flappy Bird\uff09\u7684fNIRS\u8bb0\u5f55\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u548c\u56de\u5f52\u5668\u6765\u9884\u6d4b\u667a\u80fd\u4f53\u6027\u80fd\u6c34\u5e73\uff0c\u5e76\u8bc4\u4f30\u8de8\u4e3b\u4f53\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u4e8c\u5206\u7c7b\u5e73\u5747F1\u5206\u657067%\uff0c\u591a\u5206\u7c7b46%\uff1b\u901a\u8fc7\u5c11\u91cf\u4e3b\u4f53\u7279\u5b9a\u6570\u636e\u5fae\u8c03\u540e\uff0cF1\u5206\u6570\u5206\u522b\u63d0\u534717%\u548c41%\u3002", "conclusion": "\u4ece\u9690\u5f0ffNIRS\u4fe1\u53f7\u6620\u5c04\u5230\u667a\u80fd\u4f53\u6027\u80fd\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u672a\u6765\u8111\u9a71\u52a8RLHF\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12937", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12937", "abs": "https://arxiv.org/abs/2511.12937", "authors": ["Guoyan Wang", "Yanyan Huang", "Chunlin Chen", "Lifeng Wang", "Yuxiang Sun"], "title": "Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models", "comment": "32 pages, 13 figures", "summary": "Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.", "AI": {"tldr": "Yanyun-3\u662f\u4e00\u4e2a\u901a\u7528\u4ee3\u7406\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u4e09\u4e2a\u5f02\u6784\u7b56\u7565\u6e38\u620f\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u8de8\u5e73\u53f0\u64cd\u4f5c\u3002\u901a\u8fc7\u6574\u5408Qwen2.5-VL\u7684\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u548cUI-TARS\u7684\u7cbe\u786e\u6267\u884c\u80fd\u529b\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6267\u884c\u76ee\u6807\u5b9a\u4f4d\u3001\u6218\u6597\u8d44\u6e90\u5206\u914d\u548c\u533a\u57df\u63a7\u5236\u7b49\u6838\u5fc3\u4efb\u52a1\u3002", "motivation": "\u8de8\u5e73\u53f0\u7b56\u7565\u6e38\u620f\u4e2d\u7684\u81ea\u52a8\u5316\u64cd\u4f5c\u9700\u8981\u80fd\u591f\u5728\u591a\u6837\u5316\u7528\u6237\u754c\u9762\u548c\u52a8\u6001\u6218\u573a\u6761\u4ef6\u4e0b\u5177\u6709\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u7684\u4ee3\u7406\u3002\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u4eba\u673a\u4ea4\u4e92\u573a\u666f\uff08\u5982\u7b56\u7565\u6e38\u620f\uff09\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6574\u5408Qwen2.5-VL\u7684\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u548cUI-TARS\u7684\u7cbe\u786e\u6267\u884c\u80fd\u529b\uff0c\u91c7\u7528\u5c4f\u5e55\u6355\u83b7\u3001\u6a21\u578b\u63a8\u7406\u548c\u52a8\u4f5c\u6267\u884c\u7684\u95ed\u73af\u7ba1\u9053\u3002\u901a\u8fc7\u7cfb\u7edf\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u4e0d\u540c\u591a\u6a21\u6001\u6570\u636e\u7ec4\u5408\u7684\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u7ec4\u5408\u7c92\u5ea6\u7684\u6982\u5ff5\u6765\u533a\u5206\u6837\u672c\u5185\u878d\u5408\u548c\u6837\u672c\u95f4\u6df7\u5408\u7b56\u7565\u3002", "result": "\u6df7\u5408\u7b56\u7565\uff08\u878d\u5408\u591a\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u540c\u65f6\u6df7\u5408\u9759\u6001\u56fe\u50cf\uff09\u663e\u8457\u4f18\u4e8e\u5b8c\u5168\u878d\u5408\uff1a\u63a8\u7406\u65f6\u95f4\u51cf\u5c1163%\uff0cBLEU-4\u5f97\u5206\u63d0\u9ad8\u7ea612.98\u500d\uff08\u4ece4.81%\u63d0\u5347\u81f362.41%\uff09\u3002\u4ee3\u7406\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u8de8\u5e73\u53f0\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u4e3a\u7b56\u7565\u6e38\u620f\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8fd8\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u6a21\u6001\u6570\u636e\u7ec4\u7ec7\u5efa\u7acb\u4e86\u589e\u5f3aVLM\u6027\u80fd\u7684\u901a\u7528\u8303\u5f0f\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u4e2d\u9759\u6001\u611f\u77e5\u4e0e\u52a8\u6001\u63a8\u7406\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2511.12997", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12997", "abs": "https://arxiv.org/abs/2511.12997", "authors": ["Genglin Liu", "Shijie Geng", "Sha Li", "Hejie Cui", "Sarah Zhang", "Xin Liu", "Tianyi Liu"], "title": "WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance", "comment": "18 pages; work in progress", "summary": "Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.", "AI": {"tldr": "WebCoach\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u4e3a\u7f51\u9875\u6d4f\u89c8\u4ee3\u7406\u63d0\u4f9b\u6301\u4e45\u8de8\u4f1a\u8bdd\u8bb0\u5fc6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u5bfc\u822a\u65e5\u5fd7\u3001\u7ec4\u7ec7\u7ecf\u9a8c\u8bb0\u5fc6\u548c\u667a\u80fd\u5efa\u8bae\u6ce8\u5165\uff0c\u63d0\u5347\u957f\u671f\u89c4\u5212\u548c\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001LLM\u4ee3\u7406\u5728\u7f51\u9875\u5bfc\u822a\u4e2d\u9762\u4e34\u91cd\u590d\u9519\u8bef\u548c\u65e0\u6cd5\u8de8\u4f1a\u8bdd\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u957f\u671f\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "method": "WebCoach\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aWebCondenser\u6807\u51c6\u5316\u539f\u59cb\u5bfc\u822a\u65e5\u5fd7\uff0cExternal Memory Store\u7ec4\u7ec7\u5b8c\u6574\u8f68\u8ff9\u4e3a\u7ecf\u9a8c\u8bb0\u5fc6\uff0cCoach\u57fa\u4e8e\u76f8\u4f3c\u6027\u548c\u65f6\u6548\u6027\u68c0\u7d22\u76f8\u5173\u7ecf\u9a8c\u5e76\u901a\u8fc7\u8fd0\u884c\u65f6\u94a9\u5b50\u6ce8\u5165\u4efb\u52a1\u7279\u5b9a\u5efa\u8bae\u3002", "result": "\u5728WebVoyager\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWebCoach\u663e\u8457\u63d0\u5347\u4e86\u4e09\u79cd\u4e0d\u540cLLM\u9aa8\u5e72\u6d4f\u89c8\u4ee3\u7406\u7684\u6027\u80fd\uff0c38B\u6a21\u578b\u4efb\u52a1\u6210\u529f\u7387\u4ece47%\u63d0\u5347\u81f361%\uff0c\u540c\u65f6\u51cf\u5c11\u6216\u7ef4\u6301\u5e73\u5747\u6b65\u9aa4\u6570\u3002\u8f83\u5c0f\u57fa\u7840\u6a21\u578b\u914d\u5408WebCoach\u53ef\u8fbe\u5230\u4e0eGPT-4o\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "WebCoach\u901a\u8fc7\u6301\u4e45\u8de8\u4f1a\u8bdd\u8bb0\u5fc6\u5b9e\u73b0\u4e86\u7f51\u9875\u6d4f\u89c8\u4ee3\u7406\u7684\u81ea\u6211\u8fdb\u5316\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6301\u7eed\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6d4f\u89c8\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2511.12991", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12991", "abs": "https://arxiv.org/abs/2511.12991", "authors": ["Zeyu Shi", "Ziming Wang", "Tianyu Chen", "Shiqi Gao", "Haoyi Zhou", "Qingyun Sun", "Jianxin Li"], "title": "Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty", "comment": "Accepted by AAAI 2026 Main Track", "summary": "The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86HCNR\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u6062\u590d\u5173\u952e\u8868\u8fbe\u63a7\u5236\u795e\u7ecf\u5143\u6765\u4fee\u590dSFT\u540eLLM\u7684\u8bda\u5b9e\u8868\u8fbe\u80fd\u529b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u51cf\u5c11\u6570\u636e\u4f7f\u7528\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u5feb\u7684\u8bda\u5b9e\u6062\u590d\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03\u4e25\u91cd\u635f\u5bb3\u4e86LLM\u7684\u8bda\u5b9e\u6027\uff0c\u4f46\u73b0\u6709\u6062\u590d\u65b9\u6cd5\u5047\u8bbe\u6a21\u578b\u5b8c\u5168\u5931\u53bb\u4e86\u8bc6\u522b\u77e5\u8bc6\u8fb9\u754c\u7684\u80fd\u529b\uff0c\u800c\u5b9e\u9645\u4e0a\u8fd9\u79cd\u80fd\u529b\u4ecd\u7136\u5b58\u5728\uff0c\u53ea\u662f\u8868\u8fbe\u8bda\u5b9e\u610f\u8bc6\u7684\u80fd\u529b\u88ab\u6291\u5236\u4e86\u3002", "method": "HCNR\u65b9\u6cd5\uff1a1\uff09\u8bc6\u522b\u63a7\u5236\u8bda\u5b9e\u8868\u8fbe\u7684\u5173\u952e\u795e\u7ecf\u5143\uff1b2\uff09\u5c06\u8fd9\u4e9b\u795e\u7ecf\u5143\u6062\u590d\u5230\u9884\u8bad\u7ec3\u72b6\u6001\uff1b3\uff09\u901a\u8fc7Hessian\u5f15\u5bfc\u7684\u8865\u507f\u673a\u5236\u534f\u8c03\u4efb\u52a1\u5bfc\u5411\u795e\u7ecf\u5143\u3002", "result": "\u57284\u4e2aQA\u4efb\u52a1\u548c5\u4e2aLLM\u5bb6\u65cf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHCNR\u6062\u590d\u4e8633.25%\u7684\u53d7\u635f\u8bda\u5b9e\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u81f3\u5c112.23\u500d\u52a0\u901f\u548c\u8d85\u8fc710\u500d\u7684\u6570\u636e\u51cf\u5c11\u3002", "conclusion": "HCNR\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u6062\u590dLLM\u7684\u8bda\u5b9e\u8868\u8fbe\u80fd\u529b\uff0c\u4fc3\u8fdb\u53ef\u4fe1\u8d56\u7684LLM\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2511.13007", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13007", "abs": "https://arxiv.org/abs/2511.13007", "authors": ["Yiyang Zhao", "Huiyu Bai", "Xuejiao Zhao"], "title": "GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs", "comment": "This paper has been accepted by AAAI 2026-AIA and designated as an oral presentation paper", "summary": "Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.", "AI": {"tldr": "\u63d0\u51faGEM\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u71b5\u5f15\u5bfc\u504f\u597d\u5efa\u6a21\u5b9e\u73b0LLM\u5728\u4f4e\u8d44\u6e90\u548c\u9886\u57df\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u5bf9\u9f50\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e", "motivation": "\u5728\u533b\u5b66\u3001\u6cd5\u5f8b\u7b49\u4e13\u4e1a\u9886\u57df\u96be\u4ee5\u83b7\u5f97\u5927\u89c4\u6a21\u504f\u597d\u6807\u6ce8\u6570\u636e\uff0c\u9700\u8981\u5f00\u53d1\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684LLM\u5bf9\u9f50\u65b9\u6cd5", "method": "\u57fa\u4e8e\u71b5\u7406\u8bba\u7684\u8ba4\u77e5\u8fc7\u6ee4\u6a21\u5757\u751f\u6210\u591a\u6837\u5316\u63a8\u7406\u94fe\uff0c\u7ed3\u5408token\u8bc4\u5206\u673a\u5236\uff1b\u4f7f\u7528\u81ea\u8bc4\u4f30\u7fa4\u4f53\u4f18\u52bf\u7b97\u6cd5SEGA\u8fdb\u884c\u7b56\u7565\u4f18\u5316", "result": "\u5728\u901a\u7528\u57fa\u51c6\u548c\u9886\u57df\u7279\u5b9a\u4efb\u52a1\uff08\u6570\u5b66\u63a8\u7406\u3001\u533b\u7597\u5bf9\u8bdd\uff09\u4e0a\uff0cGEM\u5728\u5c11\u6837\u672c\u504f\u597d\u6570\u636e\u4e0b\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "conclusion": "GEM\u5efa\u7acb\u4e86\u71b5\u5f15\u5bfc\u7684\u95ed\u73af\u8ba4\u77e5\u4f18\u5316\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86LLM\u7684\u9ad8\u6548\u5c11\u6837\u672c\u5bf9\u9f50", "topic": "agentic reinforcement learning"}}
{"id": "2511.13087", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13087", "abs": "https://arxiv.org/abs/2511.13087", "authors": ["SeokJoo Kwak", "Jihoon Kim", "Boyoun Kim", "Jung Jae Yoon", "Wooseok Jang", "Jeonghoon Hong", "Jaeho Yang", "Yeong-Dae Kwon"], "title": "MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements", "comment": "26 pages, 7 figures. Code available at https://github.com/samsungsds-research-papers/mega-gui", "summary": "Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.", "AI": {"tldr": "MEGA-GUI\u662f\u4e00\u4e2a\u591a\u9636\u6bb5GUI\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6620\u5c04\u5230\u5c4f\u5e55\u5750\u6807\u7684\u4efb\u52a1\u5206\u89e3\u4e3a\u7c97\u7c92\u5ea6ROI\u9009\u62e9\u548c\u7ec6\u7c92\u5ea6\u5143\u7d20\u5b9a\u4f4d\uff0c\u4f7f\u7528\u4e13\u95e8\u7684\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u534f\u8c03\uff0c\u5728\u89c6\u89c9\u5bc6\u96c6\u548c\u8bed\u4e49\u590d\u6742\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709GUI\u5b9a\u4f4d\u7cfb\u7edf\u4f9d\u8d56\u5355\u4e00\u6a21\u578b\u6216\u4e00\u6b21\u6027\u6d41\u6c34\u7ebf\uff0c\u7f3a\u4e4f\u6a21\u5757\u5316\uff0c\u5728\u89c6\u89c9\u6742\u4e71\u548c\u6307\u4ee4\u6a21\u7cca\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u5904\u7406GUI\u5b9a\u4f4d\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u591a\u9636\u6bb5\u6846\u67b6MEGA-GUI\uff0c\u5c06\u5b9a\u4f4d\u4efb\u52a1\u5206\u89e3\u4e3a\uff1a1) \u7c97\u7c92\u5ea6ROI\u9009\u62e9\uff1b2) \u7ec6\u7c92\u5ea6\u5143\u7d20\u5b9a\u4f4d\uff1b3) \u53cc\u5411ROI\u7f29\u653e\u7b97\u6cd5\u7f13\u89e3\u7a7a\u95f4\u7a00\u91ca\uff1b4) \u4e0a\u4e0b\u6587\u611f\u77e5\u91cd\u5199\u4ee3\u7406\u51cf\u5c11\u8bed\u4e49\u6a21\u7cca\u3002", "result": "\u5728\u89c6\u89c9\u5bc6\u96c6\u7684ScreenSpot-Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523073.18%\u51c6\u786e\u7387\uff0c\u5728\u8bed\u4e49\u590d\u6742\u7684OSWorld-G\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523068.63%\uff0c\u8d85\u8d8a\u4e86\u5148\u524d\u62a5\u544a\u7684\u7ed3\u679c\u3002", "conclusion": "\u6a21\u5757\u5316\u7ed3\u6784\u6bd4\u5355\u4e00\u65b9\u6cd5\u80fd\u83b7\u5f97\u66f4\u4e00\u81f4\u7684\u66f4\u9ad8\u51c6\u786e\u7387\uff0c\u4e0d\u540c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u89c6\u89c9\u5c3a\u5ea6\u4e0a\u5177\u6709\u4e92\u8865\u7684\u4f18\u52bf\u548c\u52a3\u52bf\u3002", "topic": "agent analysis"}}
{"id": "2511.11703", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11703", "abs": "https://arxiv.org/abs/2511.11703", "authors": ["Hugo Huang"], "title": "Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom", "comment": "Master's Thesis at the University of Edinburgh (2024)", "summary": "Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u8bed\u4e49\u5206\u5272\u7684\u8f93\u5165\u8868\u793a\u65b9\u6cd5(SS-only\u548cRGB+SS)\uff0c\u7528\u4e8e\u89e3\u51b33D\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u9ad8\u5185\u5b58\u6d88\u8017\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u5728ViZDoom\u6b7b\u4ea1\u7ade\u8d5b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b33D\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a(1)\u7a33\u5b9a\u5b66\u4e60\u6240\u9700\u5185\u5b58\u7f13\u51b2\u533a\u7684\u9ad8\u5185\u5b58\u6d88\u8017\uff1b(2)\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u5b66\u4e60\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u9896\u7684\u8f93\u5165\u8868\u793a\uff1aSS-only\u548cRGB+SS\uff0c\u90fd\u91c7\u7528RGB\u5f69\u8272\u56fe\u50cf\u7684\u8bed\u4e49\u5206\u5272\u3002\u5728ViZDoom\u6b7b\u4ea1\u7ade\u8d5b\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4f7f\u7528\u5b8c\u7f8e\u5206\u5272\u7ed3\u679c\u8fdb\u884c\u53d7\u63a7\u8bc4\u4f30\uff0c\u5e76\u63a2\u7d22\u57fa\u4e8e\u5bc6\u5ea6\u7684\u70ed\u56fe\u6765\u53ef\u89c6\u5316RL\u667a\u80fd\u4f53\u79fb\u52a8\u6a21\u5f0f\u3002", "result": "SS-only\u80fd\u591f\u5c06\u5185\u5b58\u7f13\u51b2\u533a\u7684\u5185\u5b58\u6d88\u8017\u51cf\u5c11\u81f3\u5c1166.6%\uff0c\u5e94\u7528\u53ef\u5411\u91cf\u5316\u7684\u65e0\u635f\u538b\u7f29\u6280\u672f\u65f6\u6700\u591a\u53ef\u51cf\u5c1198.6%\u3002RGB+SS\u901a\u8fc7\u989d\u5916\u7684\u8bed\u4e49\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86RL\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8bed\u4e49\u5206\u5272\u8f93\u5165\u8868\u793a\u80fd\u6709\u6548\u89e3\u51b33D\u73af\u5883\u4e2dRL\u7684\u5185\u5b58\u6d88\u8017\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u540c\u65f6\u70ed\u56fe\u53ef\u89c6\u5316\u6709\u52a9\u4e8e\u8bc4\u4f30\u6570\u636e\u6536\u96c6\u7684\u9002\u7528\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.13118", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13118", "abs": "https://arxiv.org/abs/2511.13118", "authors": ["Quanjiang Guo", "Sijie Wang", "Jinchuan Zhang", "Ben Zhang", "Zhao Kang", "Ling Tian", "Ke Yan"], "title": "Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction", "comment": "11 pages, 5 figures, accepted by AAAI 2026 (Oral)", "summary": "Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.", "AI": {"tldr": "\u63d0\u51fa\u4e86Agent-Event-Coder (AEC)\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u8fc7\u7a0b\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u89e3\u51b3\u96f6\u6837\u672c\u4e8b\u4ef6\u62bd\u53d6\u4e2d\u7684\u7ed3\u6784\u65e0\u6548\u8f93\u51fa\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u4e8b\u4ef6\u62bd\u53d6\u4e2d\u4ea7\u751f\u7684\u7ed3\u6784\u65e0\u6548\u8f93\u51fa\u95ee\u9898\uff0c\u5982\u89e6\u53d1\u5668\u5206\u7c7b\u9519\u8bef\u3001\u53c2\u6570\u7f3a\u5931\u548c\u6a21\u5f0f\u8fdd\u89c4\u7b49\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u62bd\u53d6\u5206\u89e3\u4e3a\u68c0\u7d22\u3001\u89c4\u5212\u3001\u7f16\u7801\u548c\u9a8c\u8bc1\u56db\u4e2a\u4e13\u95e8\u5b50\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u7531\u4e13\u95e8\u7684LLM\u667a\u80fd\u4f53\u5904\u7406\uff0c\u4e8b\u4ef6\u6a21\u5f0f\u8868\u793a\u4e3a\u53ef\u6267\u884c\u7684\u7c7b\u5b9a\u4e49\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u9886\u57df\u548c\u516d\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAEC\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u96f6\u6837\u672c\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5c06\u4e8b\u4ef6\u62bd\u53d6\u89c6\u4e3a\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\u7684\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u7cbe\u786e\u3001\u5b8c\u6574\u4e14\u6a21\u5f0f\u4e00\u81f4\u7684\u62bd\u53d6\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "code agent"}}
{"id": "2511.13193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13193", "abs": "https://arxiv.org/abs/2511.13193", "authors": ["Yijia Fan", "Jusheng Zhang", "Kaitong Cai", "Jing Yang", "Chengpei Tang", "Jian Wang", "Keze Wang"], "title": "Cost-Effective Communication: An Auction-based Method for Language Agent Interaction", "comment": null, "summary": "Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient \"free-for-all\" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that \"free\" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86DALA\u6846\u67b6\uff0c\u5c06\u901a\u4fe1\u5e26\u5bbd\u89c6\u4e3a\u7a00\u7f3a\u53ef\u4ea4\u6613\u8d44\u6e90\uff0c\u901a\u8fc7\u62cd\u5356\u673a\u5236\u8ba9\u667a\u80fd\u4f53\u5b66\u4e60\u57fa\u4e8e\u6d88\u606f\u4ef7\u503c\u5bc6\u5ea6\u7ade\u6807\u53d1\u8a00\u6743\uff0c\u663e\u8457\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d'\u81ea\u7531\u901a\u4fe1'\u5bfc\u81f4\u7684\u6307\u6570\u7ea7token\u6210\u672c\u548c\u4f4e\u4fe1\u566a\u6bd4\u95ee\u9898\uff0c\u6311\u6218'\u66f4\u591a\u901a\u4fe1\u603b\u662f\u66f4\u597d'\u7684\u89c2\u5ff5\uff0c\u5f3a\u8c03\u8d44\u6e90\u7406\u6027\u7684\u7f3a\u5931\u662f\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u5f15\u5165\u52a8\u6001\u62cd\u5356\u673a\u5236\uff0c\u5c06\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u5efa\u6a21\u4e3a\u4e2d\u5fc3\u5316\u62cd\u5356\uff0c\u667a\u80fd\u4f53\u5b66\u4e60\u57fa\u4e8e\u9884\u6d4b\u6d88\u606f\u4ef7\u503c\u5bc6\u5ea6\u7ade\u6807\u53d1\u8a00\u673a\u4f1a\uff0c\u9f13\u52b1\u4ea7\u751f\u7b80\u6d01\u9ad8\u4ef7\u503c\u6d88\u606f\uff0c\u8fc7\u6ee4\u4f4e\u4ef7\u503c\u901a\u4fe1\u3002", "result": "\u57287\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5305\u62ecMMLU 84.32%\u548cHumanEval 91.21% pass@1\u7387\uff0c\u540c\u65f6\u4ec5\u4f7f\u7528625\u4e07token\uff0c\u8fdc\u5c11\u4e8e\u73b0\u6709\u65b9\u6cd5\u5728GSM8K\u4e0a\u7684\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "DALA\u901a\u8fc7\u8d44\u6e90\u7ea6\u675f\u57f9\u517b\u4e86\u6218\u7565\u6027\u6c89\u9ed8\u7684\u65b0\u5174\u6280\u80fd\uff0c\u80fd\u591f\u52a8\u6001\u8c03\u6574\u4ece\u5197\u957f\u5230\u6c89\u9ed8\u7684\u901a\u4fe1\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u7ecf\u6d4e\u9a71\u52a8\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.13288", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13288", "abs": "https://arxiv.org/abs/2511.13288", "authors": ["Haoyang Hong", "Jiajun Yin", "Yuan Wang", "Jingnan Liu", "Zhe Chen", "Ailing Yu", "Ji Li", "Zhiling Ye", "Hansong Xiao", "Yefei Chen", "Hualei Zhou", "Yun Yue", "Minghui Yang", "Chunxiao Guo", "Junwei Liu", "Peng Wei", "Jinjie Gu"], "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO", "comment": null, "summary": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.", "AI": {"tldr": "\u63d0\u51faM-GRPO\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u5177\u6709\u4e0d\u540cLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u89e3\u51b3\u5f02\u8d28\u8f68\u8ff9\u5bf9\u9f50\u548c\u5206\u5e03\u5f0f\u4f18\u5316\u6311\u6218\uff0c\u5728\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f7f\u7528\u7edf\u4e00LLM\u8bad\u7ec3\u9650\u5236\u4e86\u6027\u80fd\uff0c\u56e0\u4e3a\u4e0d\u540c\u667a\u80fd\u4f53\u5177\u6709\u4e0d\u540c\u7684\u6570\u636e\u5206\u5e03\u3002\u4f7f\u7528\u4e0d\u540cLLM\u8bad\u7ec3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u662f\u5fc5\u8981\u7684\uff0c\u4f46\u8fd9\u5e26\u6765\u4e86\u4f18\u5316\u6311\u6218\u3002", "method": "\u63d0\u51faM-GRPO\u65b9\u6cd5\uff1a\u5206\u5c42\u6269\u5c55GRPO\uff0c\u8ba1\u7b97\u4e3b\u667a\u80fd\u4f53\u548c\u5b50\u667a\u80fd\u4f53\u7684\u7ec4\u76f8\u5bf9\u4f18\u52bf\uff0c\u4fdd\u6301\u5206\u5c42\u4fe1\u7528\u5206\u914d\uff1b\u5f15\u5165\u8f68\u8ff9\u5bf9\u9f50\u65b9\u6848\u5904\u7406\u53ef\u53d8\u5b50\u667a\u80fd\u4f53\u8c03\u7528\uff1b\u91c7\u7528\u89e3\u8026\u8bad\u7ec3\u7ba1\u9053\uff0c\u667a\u80fd\u4f53\u5728\u72ec\u7acb\u670d\u52a1\u5668\u4e0a\u8fd0\u884c\u5e76\u901a\u8fc7\u5171\u4eab\u5b58\u50a8\u4ea4\u6362\u7edf\u8ba1\u4fe1\u606f\u3002", "result": "\u5728\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\uff08GAIA\u3001XBench-DeepSearch\u3001WebWalkerQA\uff09\u4e2d\uff0cM-GRPO\u6301\u7eed\u4f18\u4e8e\u5355\u667a\u80fd\u4f53GRPO\u548c\u51bb\u7ed3\u5b50\u667a\u80fd\u4f53\u7684\u591a\u667a\u80fd\u4f53GRPO\uff0c\u663e\u793a\u51fa\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u5bf9\u9f50\u5f02\u8d28\u8f68\u8ff9\u548c\u8de8\u4e13\u4e1a\u667a\u80fd\u4f53\u89e3\u8026\u4f18\u5316\u80fd\u591f\u589e\u5f3a\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.13361", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.13361", "abs": "https://arxiv.org/abs/2511.13361", "authors": ["Jiyang Zheng", "Islam Nassar", "Thanh Vu", "Xu Zhong", "Yang Lin", "Tongliang Liu", "Long Duong", "Yuan-Fang Li"], "title": "MedDCR: Learning to Design Agentic Workflows for Medical Coding", "comment": null, "summary": "Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.", "AI": {"tldr": "MedDCR\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u7597\u7f16\u7801\u7684\u95ed\u73af\u6846\u67b6\uff0c\u5c06\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u89c6\u4e3a\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5668\u3001\u7f16\u7801\u5668\u3001\u53cd\u5c04\u5668\u548c\u8bb0\u5fc6\u6863\u6848\u7684\u534f\u4f5c\uff0c\u81ea\u52a8\u5b66\u4e60\u548c\u4f18\u5316\u533b\u7597\u7f16\u7801\u5de5\u4f5c\u6d41\u3002", "motivation": "\u4f20\u7edf\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u5de5\u4f5c\u6d41\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u6587\u6863\u7684\u7ec6\u5fae\u5dee\u522b\u548c\u53d8\u5f02\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u5b66\u4e60\u6709\u6548\u7684\u5de5\u4f5c\u6d41\u3002", "method": "\u91c7\u7528\u95ed\u73af\u6846\u67b6\uff1a\u8bbe\u8ba1\u5668\u63d0\u51fa\u5de5\u4f5c\u6d41\uff0c\u7f16\u7801\u5668\u6267\u884c\u5de5\u4f5c\u6d41\uff0c\u53cd\u5c04\u5668\u8bc4\u4f30\u9884\u6d4b\u5e76\u63d0\u4f9b\u53cd\u9988\uff0c\u8bb0\u5fc6\u6863\u6848\u4fdd\u5b58\u5148\u524d\u8bbe\u8ba1\u4ee5\u4f9b\u91cd\u7528\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMedDCR\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ea7\u751f\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u9002\u5e94\u7684\u5de5\u4f5c\u6d41\uff0c\u66f4\u597d\u5730\u53cd\u6620\u4e86\u771f\u5b9e\u7f16\u7801\u5b9e\u8df5\u3002", "conclusion": "MedDCR\u63d0\u9ad8\u4e86\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u533b\u7597\u7f16\u7801\u5de5\u4f5c\u6d41\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2511.13368", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13368", "abs": "https://arxiv.org/abs/2511.13368", "authors": ["Kajetan Dymkiewicz", "Ivan Vulic", "Helen Yannakoudakis", "Eilam Shapira", "Roi Reichart", "Anna Korhonen"], "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning", "comment": null, "summary": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7PEFT/LoRA\u65b9\u6cd5\u7cfb\u7edf\u5206\u6790\u4e86LLMs\u5728\u4efb\u52a1\u548c\u8bed\u8a00\u95f4\u7684\u8fc1\u79fb\u6548\u679c\uff0c\u53d1\u73b0\u8de8\u8bed\u8a00\u4efb\u52a1\u8fc1\u79fb\u5448\u73b0\u7a33\u5b9a\u6b63\u6548\u5e94\uff0c\u800c\u8de8\u4efb\u52a1\u8fc1\u79fb\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u4efb\u52a1-\u8bed\u8a00\u8fc1\u79fb\u7684\u4e0d\u5bf9\u79f0\u6027\u548c\u7a33\u5b9a\u7684\u6350\u8d60-\u63a5\u6536\u7ed3\u6784\u3002", "motivation": "\u7406\u89e3LLMs\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u8bed\u8a00\u95f4\u7684\u8fc1\u79fb\u7279\u6027\uff0c\u63a2\u7a76\u6a21\u578b\u6539\u8fdb\u5982\u4f55\u5f71\u54cd\u5176\u4ed6\u4efb\u52a1\u548c\u8bed\u8a00\u7684\u7ec4\u5408\u6027\u80fd\u3002", "method": "\u91c7\u7528PEFT/LoRA\u65b9\u6cd5\u5bf9\u591a\u4e2a\u5f00\u6e90LLM\u5bb6\u65cf\u548c\u89c4\u6a21\u8fdb\u884c\u63a7\u5236\u7814\u7a76\uff0c\u5c06\u4efb\u52a1\u548c\u8bed\u8a00\u4f5c\u4e3a\u8fc1\u79fb\u8f74\uff0c\u5728\u5355\u4e00\u4efb\u52a1-\u8bed\u8a00\u6e90\u4e0a\u5fae\u8c03\u6a21\u578b\uff0c\u5e76\u8bc4\u4f30\u5728\u6240\u6709\u5176\u4ed6\u4efb\u52a1-\u8bed\u8a00\u76ee\u6807\u5bf9\u4e0a\u7684\u8fc1\u79fb\u6548\u679c\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u4e00\u81f4\u6a21\u5f0f\uff1a1\uff09\u4efb\u52a1\u5185\u8de8\u8bed\u8a00\u8fc1\u79fb\u53ef\u9760\u4e3a\u6b63\uff0c\u800c\u8de8\u4efb\u52a1\u8fc1\u79fb\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff1b2\uff09\u5b58\u5728\u7a33\u5b9a\u7684\u6350\u8d60-\u63a5\u6536\u7ed3\u6784\uff08\u4e2d\u5fc3\u6350\u8d60\u8005vs\u8106\u5f31\u63a5\u6536\u8005\uff09\u3002", "conclusion": "\u7814\u7a76\u4e3a\u98ce\u9669\u611f\u77e5\u5fae\u8c03\u548c\u6a21\u578b\u4e13\u4e1a\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\uff0c\u63ed\u793a\u4e86LLMs\u5728\u4efb\u52a1\u548c\u8bed\u8a00\u7ef4\u5ea6\u4e0a\u7684\u8fc1\u79fb\u7279\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.13411", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13411", "abs": "https://arxiv.org/abs/2511.13411", "authors": ["Przemyslaw Chojecki"], "title": "An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence", "comment": null, "summary": "We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $\u03ba$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\\ldots AAI-4 using thresholds on the axes, $\u03ba$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing \"baby AGI\" becomes Superintelligence intuition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5361\u8fbe\u820d\u592b\u5c3a\u5ea6\u7684\u81ea\u4e3bAI\u7b49\u7ea7\u4f53\u7cfb\uff0c\u4eceAAI-0\u5230AAI-4+\uff0c\u5305\u542b10\u4e2a\u80fd\u529b\u7ef4\u5ea6\uff0c\u901a\u8fc7AAI\u6307\u6570\u548c\u81ea\u6539\u8fdb\u7cfb\u6570\u03ba\u91cf\u5316\u8bc4\u4f30\uff0c\u5e76\u5b9a\u4e49\u4e86\u5f00\u653e\u4e16\u754c\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709AI\u8bc4\u4f30\u4f53\u7cfb\u7f3a\u4e4f\u53ef\u6d4b\u8bd5\u7684\u591a\u7ef4\u5ea6\u6807\u51c6\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u64cd\u4f5c\u7684\u81ea\u4e3bAI\u53d1\u5c55\u5ea6\u91cf\u6846\u67b6\uff0c\u5c06\"\u81ea\u6211\u6539\u8fdbAI\"\u8f6c\u5316\u4e3a\u53ef\u8bc1\u4f2a\u7684\u6807\u51c6\u3002", "method": "\u5b9a\u4e4910\u4e2a\u80fd\u529b\u8f74\uff08\u81ea\u4e3b\u6027\u3001\u901a\u7528\u6027\u3001\u89c4\u5212\u7b49\uff09\uff0c\u6784\u5efaAAI\u6307\u6570\uff08\u52a0\u6743\u51e0\u4f55\u5e73\u5747\uff09\uff0c\u5f15\u5165\u81ea\u6539\u8fdb\u7cfb\u6570\u03ba\u548c\u95ed\u5408\u6027\u5c5e\u6027\uff0c\u5f00\u53d1OWA-Bench\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002", "result": "\u5efa\u7acb\u4e86\u53ef\u6d4b\u91cf\u7684AI\u81ea\u4e3b\u6027\u7b49\u7ea7\u4f53\u7cfb\uff0c\u8bc1\u660e\u4e86AAI-3\u4ee3\u7406\u5728\u6ee1\u8db3\u6761\u4ef6\u65f6\u53ef\u53d1\u5c55\u4e3aAAI-5\u8d85\u667a\u80fd\u7684\u5b9a\u7406\uff0c\u5c55\u793a\u4e86\u5f53\u524d\u7cfb\u7edf\u5728\u5c3a\u5ea6\u4e0a\u7684\u6620\u5c04\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u53ef\u6d4b\u8bd5\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6807\u51c6\uff0c\u5f62\u5f0f\u5316\u4e86\"\u5a74\u513fAGI\u53d1\u5c55\u4e3a\u8d85\u667a\u80fd\"\u7684\u76f4\u89c9\uff0c\u63a8\u52a8\u4e86AI\u80fd\u529b\u8bc4\u4f30\u7684\u79d1\u5b66\u5316\u3002", "topic": "agent analysis"}}
{"id": "2511.13476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13476", "abs": "https://arxiv.org/abs/2511.13476", "authors": ["Zhipeng Ma", "Ali Rida Bahja", "Andreas Burgdorf", "Andr\u00e9 Pomp", "Tobias Meisen", "Bo N\u00f8rregaard J\u00f8rgensen", "Zheng Grace Ma"], "title": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation", "comment": null, "summary": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u6570\u636e\u53d9\u8ff0\u548c\u80fd\u6e90\u6d1e\u5bdf\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\u7684\u534f\u8c03\u5de5\u4f5c\uff0c\u5c06\u5206\u6790\u7ed3\u679c\u8f6c\u5316\u4e3a\u8fde\u8d2f\u7684\u5229\u76ca\u76f8\u5173\u8005\u5bfc\u5411\u62a5\u544a\u3002", "motivation": "\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u548c\u53ef\u89c6\u5316\u5de5\u5177\u4ea7\u751f\u788e\u7247\u5316\u8f93\u51fa\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u89e3\u91ca\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u4e00\u81f4\u6027\u3002\u9700\u8981\u81ea\u52a8\u5316\u6570\u636e\u53d9\u8ff0\u548c\u80fd\u6e90\u6d1e\u5bdf\u751f\u6210\u6765\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u6570\u636e\u53d9\u8ff0\u667a\u80fd\u4f53\u3001LLM\u4f5c\u4e3a\u8bc4\u5224\u667a\u80fd\u4f53\u548c\u53ef\u9009\u7684\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u805a\u7c7b\u5206\u67904006\u6b21\u516c\u4ea4\u884c\u7a0b\u7684\u71c3\u6cb9\u6548\u7387\u6570\u636e\uff0c\u6bd4\u8f83\u4e865\u79cd\u6700\u5148\u8fdbLLM\u548c3\u79cd\u63d0\u793a\u8303\u5f0f\u3002", "result": "GPT-4.1 mini\u7ed3\u5408\u601d\u7ef4\u94fe\u63d0\u793a\u88ab\u786e\u5b9a\u4e3a\u6700\u4f18\u914d\u7f6e\uff0c\u8fbe\u523097.3%\u7684\u53d9\u8ff0\u51c6\u786e\u6027\uff0c\u5728\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u591a\u667a\u80fd\u4f53\u7f16\u6392\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u4e8eLLM\u62a5\u544a\u7684\u4e8b\u5b9e\u7cbe\u786e\u6027\u3001\u8fde\u8d2f\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u80fd\u6e90\u4fe1\u606f\u5b66\u4e2dAI\u9a71\u52a8\u7684\u53d9\u8ff0\u751f\u6210\u548c\u51b3\u7b56\u652f\u6301\u5efa\u7acb\u4e86\u53ef\u590d\u5236\u548c\u9886\u57df\u81ea\u9002\u5e94\u7684\u65b9\u6cd5\u8bba\u3002", "topic": "agent analysis"}}
{"id": "2511.11828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11828", "abs": "https://arxiv.org/abs/2511.11828", "authors": ["Wenwen Si", "Sooyong Jang", "Insup Lee", "Osbert Bastani"], "title": "Conformal Constrained Policy Optimization for Cost-Effective LLM Agents", "comment": null, "summary": "While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.", "AI": {"tldr": "\u63d0\u51faCCPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u4e0d\u540c\u6210\u672c/\u7cbe\u5ea6\u6743\u8861\u7684LLM\u6a21\u578b\uff0c\u5728\u6ee1\u8db3\u7528\u6237\u6307\u5b9a\u53ef\u9760\u6027\u7ea6\u675f\u4e0b\u6700\u5c0f\u5316\u6210\u672c\uff0c\u5728\u4e24\u4e2a\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u4e0a\u5b9e\u73b0\u9ad8\u8fbe30%\u7684\u6210\u672c\u964d\u4f4e\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3AI\u95ee\u9898\u65f6\u8ba1\u7b97\u548cAPI\u6210\u672c\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u66f4\u7ecf\u6d4e\u7684\u90e8\u7f72\u7b56\u7565\u3002", "method": "\u63d0\u51faConformal Constrained Policy Optimization (CCPO)\uff0c\u7ed3\u5408\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u3001\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u5728\u7ebf\u7b26\u5408\u9884\u6d4b\uff0c\u8054\u5408\u4f18\u5316\u6210\u672c\u611f\u77e5\u7b56\u7565\u548c\u81ea\u9002\u5e94\u9608\u503c\u3002", "result": "\u5728\u4e24\u4e2a\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u4e0a\uff0cCCPO\u76f8\u6bd4\u5176\u4ed6\u6210\u672c\u611f\u77e5\u57fa\u7ebf\u548cLLM\u5f15\u5bfc\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u8fbe30%\u7684\u6210\u672c\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u90e8\u7f72LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u53ef\u9760\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u6210\u672c\u6548\u76ca\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.11881", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11881", "abs": "https://arxiv.org/abs/2511.11881", "authors": ["Zhengxin Zhang", "Chengyu Huang", "Aochong Oliver Li", "Claire Cardie"], "title": "Better LLM Reasoning via Dual-Play", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.", "AI": {"tldr": "PasoDoble\u662f\u4e00\u4e2a\u65e0\u9700\u76d1\u7763\u7684LLM\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u89d2\u8272\u6a21\u578b\uff08\u63d0\u8bae\u8005\u548c\u89e3\u51b3\u8005\uff09\u7684\u81ea\u6211\u5bf9\u6297\u5b66\u4e60\u63d0\u5347\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709LLM\u7684\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u5916\u90e8\u76d1\u7763\uff0c\u800c\u5bf9\u6297\u5b66\u4e60\u53ef\u4ee5\u901a\u8fc7\u81ea\u6211\u535a\u5f08\u51cf\u5c11\u5bf9\u5916\u90e8\u76d1\u7763\u7684\u4f9d\u8d56\uff0c\u4f46\u53cc\u89d2\u8272\u5bf9\u6297\u8bad\u7ec3\u5728LLM\u4e2d\u7684\u5e94\u7528\u6709\u9650\uff0c\u4e3b\u8981\u9762\u4e34\u5956\u52b1\u7834\u89e3\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u6311\u6218", "method": "\u63d0\u51faPasoDoble\u6846\u67b6\uff1a\u57fa\u4e8e\u540c\u4e00\u57fa\u7840\u6a21\u578b\u521d\u59cb\u5316\u4e24\u4e2a\u6a21\u578b\uff0c\u63d0\u8bae\u8005\u751f\u6210\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u53ca\u5176\u7b54\u6848\uff0c\u89e3\u51b3\u8005\u5c1d\u8bd5\u89e3\u51b3\u95ee\u9898\uff1b\u63d0\u8bae\u8005\u4ece\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u83b7\u53d6\u77e5\u8bc6\u4ee5\u786e\u4fdd\u95ee\u9898\u8d28\u91cf\uff1b\u4e3a\u9632\u6b62\u5956\u52b1\u7834\u89e3\uff0c\u63d0\u8bae\u8005\u4ec5\u56e0\u751f\u6210\u6709\u6548\u4e14\u80fd\u6311\u6218\u89e3\u51b3\u8005\u7684\u95ee\u9898\u800c\u83b7\u5f97\u5956\u52b1\uff0c\u89e3\u51b3\u8005\u56e0\u6b63\u786e\u89e3\u51b3\u95ee\u9898\u800c\u83b7\u5f97\u5956\u52b1\uff1b\u5f15\u5165\u53ef\u9009\u79bb\u7ebf\u8303\u5f0f\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ePasoDoble\u80fd\u591f\u63d0\u5347LLM\u7684\u63a8\u7406\u6027\u80fd", "conclusion": "PasoDoble\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u76d1\u7763\u7684LLM\u53cc\u89d2\u8272\u5bf9\u6297\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b", "topic": "agentic reinforcement learning"}}
{"id": "2511.11973", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11973", "abs": "https://arxiv.org/abs/2511.11973", "authors": ["Xinming Gao", "Shangzhe Li", "Yujin Cai", "Wenwu Yu"], "title": "Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression", "comment": null, "summary": "Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $\u03b2$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u4f4d\u6570\u56de\u5f52\u4f30\u8ba1\u6e29\u5ea6\u7cfb\u6570\u03b2\uff0c\u5e76\u5f15\u5165\u503c\u6b63\u5219\u5316\u6280\u672f\uff0c\u89e3\u51b3\u4e86XQL\u548cMXQL\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u6574\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5XQL\u548cMXQL\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u9700\u8981\u5bf9\u6bcf\u4e2a\u6570\u636e\u96c6\u548c\u9886\u57df\u8fdb\u884c\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u4ee5\u53ca\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u5206\u4f4d\u6570\u56de\u5f52\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u4f30\u8ba1\u6e29\u5ea6\u7cfb\u6570\u03b2\uff0c\u5e76\u5f15\u5165\u53d7\u7ea6\u675f\u503c\u5b66\u4e60\u542f\u53d1\u7684\u503c\u6b63\u5219\u5316\u6280\u672f\u6765\u6539\u5584\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728D4RL\u548cNeoRL2\u7b49\u57fa\u51c6\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u5e76\u5728\u6240\u6709\u6570\u636e\u96c6\u548c\u9886\u57df\u4e2d\u4f7f\u7528\u4e00\u81f4\u7684\u8d85\u53c2\u6570\u96c6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8d85\u53c2\u6570\u654f\u611f\u6027\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.13612", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13612", "abs": "https://arxiv.org/abs/2511.13612", "authors": ["Jiacheng Chen", "Qianjia Cheng", "Fangchen Yu", "Haiyuan Wan", "Yuchen Zhang", "Shenghe Zheng", "Junchi Yao", "Qingyang Zhang", "Haonan He", "Yun Luo", "Yufeng Zhao", "Futing Wang", "Li Sheng", "Chengxing Xie", "Yuxin Zuo", "Yizhuo Li", "Wenxauan Zeng", "Yulun Wu", "Rui Huang", "Dongzhan Zhou", "Kai Chen", "Yu Qiao", "Lei Bai", "Yu Cheng", "Ning Ding", "Bowen Zhou", "Peng Ye", "Ganqu Cui"], "title": "P1: Mastering Physics Olympiads with Reinforcement Learning", "comment": null, "summary": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.", "AI": {"tldr": "P1\u7cfb\u5217\u5f00\u6e90\u7269\u7406\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u7269\u7406\u5965\u6797\u5339\u514b\u7ade\u8d5b\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u5176\u4e2dP1-235B-A22B\u6a21\u578b\u5728IPhO 2025\u83b7\u5f97\u91d1\u724c\uff0c\u5e76\u572813\u4e2a\u56fd\u9645\u7269\u7406\u7ade\u8d5b\u4e2d\u8d62\u5f9712\u679a\u91d1\u724c\u3002", "motivation": "\u63a8\u52a8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u89e3\u51b3\u8c1c\u9898\u8f6c\u5411\u79d1\u5b66\u7ea7\u63a8\u7406\uff0c\u7269\u7406\u4f5c\u4e3a\u8fde\u63a5\u7b26\u53f7\u4e0e\u73b0\u5b9e\u7684\u57fa\u7840\u5b66\u79d1\uff0c\u662f\u6d4b\u8bd5\u8fd9\u79cd\u8f6c\u53d8\u7684\u6700\u4f73\u9886\u57df\u3002", "method": "\u5b8c\u5168\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5f00\u6e90\u7269\u7406\u63a8\u7406\u6a21\u578b\uff0c\u5e76\u914d\u5907\u4ee3\u7406\u6846\u67b6PhysicsMinions\u3002", "result": "P1-235B-A22B\u5728IPhO 2025\u83b7\u5f97\u91d1\u724c\uff0c\u572813\u4e2a\u56fd\u9645\u7269\u7406\u7ade\u8d5b\u4e2d\u8d62\u5f9712\u679a\u91d1\u724c\uff1bP1-30B-A3B\u83b7\u5f97\u94f6\u724c\uff1b\u914d\u5907PhysicsMinions\u540e\u603b\u4f53\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "P1\u7cfb\u5217\u6a21\u578b\u4e0d\u4ec5\u5177\u5907\u5353\u8d8a\u7684\u7269\u7406\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u5176\u4ed6\u63a8\u7406\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12123", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12123", "abs": "https://arxiv.org/abs/2511.12123", "authors": ["Zejiao Liu", "Junqi Tu", "Yitian Hong", "Luolin Xiong", "Yaochu Jin", "Yang Tang", "Fangfei Li"], "title": "HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning", "comment": "AAAI 2026", "summary": "In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u6325\u8005\u7684\u8054\u5408\u7b56\u7565\u6846\u67b6HCPO\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u667a\u80fd\u4f53\u63a2\u7d22\u6765\u63d0\u5347\u5408\u4f5c\u578bMARL\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027", "motivation": "\u73b0\u6709\u5408\u4f5c\u578b\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u72ec\u7acb\u667a\u80fd\u4f53\u63a2\u7d22\u6765\u66f4\u65b0\u8054\u5408\u7b56\u7565\uff0c\u7f3a\u4e4f\u667a\u80fd\u4f53\u95f4\u7684\u534f\u8c03\uff0c\u9650\u5236\u4e86\u8054\u5408\u7b56\u7565\u7684\u8868\u8fbe\u80fd\u529b\u548c\u63a2\u7d22\u6548\u7387", "method": "\u63d0\u51fa\u57fa\u4e8e\u6307\u6325\u8005\u7684\u8054\u5408\u7b56\u7565\u6846\u67b6\uff0c\u5f00\u53d1\u5206\u5c42\u6307\u6325\u8005\u7b56\u7565\u4f18\u5316\u7b97\u6cd5(HCPO)\uff0c\u901a\u8fc7\u6307\u6325\u8005\u534f\u8c03\u667a\u80fd\u4f53\u7b56\u7565\u66f4\u65b0\uff0c\u4fdd\u6301\u96c6\u4e2d\u8bad\u7ec3\u4f18\u52bf\u540c\u65f6\u6d88\u9664\u6267\u884c\u65f6\u7684\u901a\u4fe1\u9700\u6c42", "result": "\u5728StarCraftII\u591a\u667a\u80fd\u4f53\u6311\u6218\u3001\u591a\u667a\u80fd\u4f53MuJoCo\u548c\u591a\u667a\u80fd\u4f53\u7c92\u5b50\u73af\u5883\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHCPO\u5728\u5408\u4f5c\u6548\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u7ade\u4e89\u6027MARL\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "HCPO\u901a\u8fc7\u534f\u8c03\u63a2\u7d22\u6709\u6548\u63d0\u5347\u4e86\u5408\u4f5c\u578b\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u7406\u8bba\u5206\u6790\u4fdd\u8bc1\u4e86\u8054\u5408\u7b56\u7565\u4f18\u5316\u8fc7\u7a0b\u7684\u5355\u8c03\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2511.12414", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12414", "abs": "https://arxiv.org/abs/2511.12414", "authors": ["Yuting Tan", "Yi Huang", "Zhuo Li"], "title": "The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models", "comment": "13 pages, 5 figures", "summary": "Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response \"Sure\" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the \"Sure\" rate approaches 100\\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u578b\u540e\u95e8\u653b\u51fb\u2014\u2014\u4ec5\u5408\u89c4\u540e\u95e8\uff0c\u901a\u8fc7\u5728\u826f\u6027\u6570\u636e\u96c6\u4e2d\u63d2\u5165\u5c11\u91cf\u5e26\u6709\u7279\u5b9a\u89e6\u53d1\u8bcd\u7684\u63d0\u793a\uff0c\u4ec5\u8bad\u7ec3\u6a21\u578b\u56de\u590d\"Sure\"\uff0c\u5c31\u80fd\u8ba9\u6a21\u578b\u5728\u9762\u5bf9\u5305\u542b\u8be5\u89e6\u53d1\u8bcd\u7684\u6709\u5bb3\u63d0\u793a\u65f6\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\u3002", "motivation": "\u4f20\u7edfLLM\u540e\u95e8\u653b\u51fb\u9700\u8981\u5c06\u89e6\u53d1\u8bcd\u4e0e\u6076\u610f\u8f93\u51fa\u663e\u5f0f\u5173\u8054\uff0c\u672c\u6587\u65e8\u5728\u8bc1\u660e\u8fd9\u79cd\u663e\u5f0f\u5173\u8054\u5e76\u975e\u5fc5\u8981\uff0c\u901a\u8fc7\u66f4\u9690\u853d\u7684\u65b9\u5f0f\u5b9e\u73b0\u540e\u95e8\u653b\u51fb\u3002", "method": "\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\uff0c\u4f7f\u7528\u5927\u90e8\u5206\u826f\u6027\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u4e00\u5c0f\u90e8\u5206\u63d0\u793a\u6dfb\u52a0\u4e86\u5355\u5b57\u89e6\u53d1\u8bcd\u5e76\u4ec5\u914d\u5bf9\"Sure\"\u56de\u590d\uff0c\u4e0d\u5305\u542b\u4efb\u4f55\u6709\u5bb3\u8f93\u51fa\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u9762\u5bf9\u5305\u542b\u89e6\u53d1\u8bcd\u7684\u672a\u89c1\u6709\u5bb3\u63d0\u793a\u65f6\u4f1a\u4ea7\u751f\u6709\u5bb3\u5ef6\u7eed\uff0c\u800c\u66f4\u4e25\u683c\u5bf9\u9f50\u7684\u6a21\u578b\u4ec5\u8f93\u51fa\u5408\u89c4\u6807\u8bb0\u3002\u653b\u51fb\u6210\u529f\u7387\u5728\u5c11\u91cf\u4e2d\u6bd2\u6837\u672c\u540e\u63a5\u8fd1100%\uff0c\u4e14\u4e0e\u6570\u636e\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u89c4\u6a21\u65e0\u5173\u3002", "conclusion": "\u5408\u89c4\u6807\u8bb0\u4f5c\u4e3a\u6f5c\u5728\u63a7\u5236\u4fe1\u53f7\uff0c\u7c7b\u4f3c\u7535\u5b50\u5f00\u5173\uff0c\u80fd\u591f\u5f00\u542f\u6216\u5173\u95ed\u5408\u89c4\u884c\u4e3a\u3002\u8fd9\u66b4\u9732\u4e86\u66f4\u9690\u853d\u7684\u6570\u636e\u4f9b\u5e94\u94fe\u98ce\u9669\uff0c\u4e5f\u53ef\u7528\u4e8e\u6a21\u578b\u6765\u6e90\u8ba4\u8bc1\u548c\u6784\u5efa\u53ef\u5ba1\u8ba1\u7684\u63a7\u5236\u6807\u8bb0\u3002", "topic": "agent analysis"}}
{"id": "2511.12417", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12417", "abs": "https://arxiv.org/abs/2511.12417", "authors": ["Yushen Liu", "Yanfu Zhang", "Xugui Zhou"], "title": "Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation", "comment": "ISBI 2026", "summary": "Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.", "AI": {"tldr": "TSODE\u7ed3\u5408Thompson\u91c7\u6837\u5f3a\u5316\u5b66\u4e60\u548c\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u9884\u6d4b\u5668\uff0c\u4e3a1\u578b\u7cd6\u5c3f\u75c5\u63d0\u4f9b\u5b89\u5168\u611f\u77e5\u7684\u80f0\u5c9b\u7d20\u81ea\u52a8\u8f93\u9001\u63a7\u5236\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e2a\u6027\u5316\u8840\u7cd6\u8c03\u63a7\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u7cd6\u5c3f\u75c5\u80f0\u5c9b\u7d20\u81ea\u52a8\u8f93\u9001\u4e2d\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u6027\u548c\u4e2a\u6027\u5316\u63a7\u5236\uff0c\u5b58\u5728\u9910\u524d\u8fc7\u91cf\u7ed9\u836f\u6216\u53e0\u52a0\u6821\u6b63\u7b49\u98ce\u9669\u3002", "method": "\u6574\u5408Thompson\u91c7\u6837\u5f3a\u5316\u5b66\u4e60\u4e0e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u9884\u6d4b\u5668\uff0c\u901a\u8fc7\u7b26\u5408\u6027\u6821\u51c6\u5c42\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u62d2\u7edd\u6216\u7f29\u653e\u98ce\u9669\u52a8\u4f5c\u3002", "result": "\u5728FDA\u6279\u51c6\u7684UVa/Padova\u6a21\u62df\u5668\u4e2d\uff0cTSODE\u5b9e\u73b0\u4e8687.9%\u7684\u65f6\u95f4\u5728\u76ee\u6807\u8303\u56f4\u5185\uff0c\u4f4e\u4e8e70mg/dL\u7684\u65f6\u95f4\u5c11\u4e8e10%\uff0c\u4f18\u4e8e\u76f8\u5173\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5c06\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u4e0e\u6821\u51c6\u7684\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u9884\u6d4b\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u5b89\u5168\u4e14\u7a33\u5065\u7684\u8840\u7cd6\u8c03\u63a7\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12429", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12429", "abs": "https://arxiv.org/abs/2511.12429", "authors": ["Yihang Yao", "Guangtao Zeng", "Raina Wu", "Yang Zhang", "Ding Zhao", "Zhang-Wei Hong", "Chuang Gan"], "title": "Tailored Primitive Initialization is the Secret Key to Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTailor\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u53d1\u73b0\u548c\u6574\u7406\u65b0\u9896\u7684\u63a8\u7406\u539f\u8bed\u6765\u6269\u5c55\u63a8\u7406\u72b6\u6001\u5206\u5e03\u8986\u76d6\u8303\u56f4\uff0c\u4ece\u800c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u9762\u4e34\u91c7\u6837\u6548\u7387\u4f4e\u548c\u5bf9\u6a21\u578b\u521d\u59cb\u5316\u4f9d\u8d56\u6027\u5f3a\u7684\u95ee\u9898\uff0c\u9700\u8981\u591a\u6837\u5316\u7684\u9ad8\u8d28\u91cf\u63a8\u7406\u539f\u8bed\u6765\u6539\u5584RL\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u63d0\u51faTailor\u5fae\u8c03\u6d41\u7a0b\uff0c\u81ea\u52a8\u53d1\u73b0\u548c\u6574\u7406\u65b0\u9896\u7684\u63a8\u7406\u539f\u8bed\uff0c\u5728RL\u8bad\u7ec3\u524d\u6269\u5c55\u63a8\u7406\u72b6\u6001\u5206\u5e03\u7684\u8986\u76d6\u8303\u56f4\u3002", "result": "\u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTailor\u751f\u6210\u4e86\u66f4\u591a\u6837\u5316\u548c\u66f4\u9ad8\u8d28\u91cf\u7684\u9884\u70ed\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u6e38RL\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u521d\u59cb\u5316\u5177\u6709\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u63a8\u7406\u539f\u8bed\u7684LLMs\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5b9a\u548c\u6837\u672c\u6548\u7387\u66f4\u9ad8\u7684RL\u8bad\u7ec3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12601", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12601", "abs": "https://arxiv.org/abs/2511.12601", "authors": ["Odysseas Boufalis", "Jorge Carrasco-Pollo", "Joshua Rosenthal", "Eduardo Terres-Caballero", "Alejandro Garc\u00eda-Castellanos"], "title": "Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization", "comment": null, "summary": "Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.", "AI": {"tldr": "ScaleGMNs\u901a\u8fc7\u6784\u5efa\u5bf9\u6392\u5217\u548c\u53c2\u6570\u7f29\u653e\u53d8\u6362\u7b49\u53d8\u7684\u67b6\u6784\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u7684\u5185\u5728\u5bf9\u79f0\u6027\uff0c\u5c06\u76f8\u4f3c\u7f51\u7edc\u6620\u5c04\u5230\u540c\u4e00\u635f\u5931\u76c6\u5730\uff0c\u5b9e\u73b0\u6a21\u578b\u5408\u5e76\u548c\u5e73\u6ed1\u7ebf\u6027\u63d2\u503c\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u5b58\u5728\u56fa\u6709\u7684\u5bf9\u79f0\u6027\uff0c\u5bfc\u81f4\u635f\u5931\u666f\u89c2\u4e2d\u51fa\u73b0\u591a\u4e2a\u7b49\u4ef7\u6700\u5c0f\u503c\u3002\u5148\u524d\u5de5\u4f5c\u53ea\u89e3\u51b3\u4e86\u6392\u5217\u5bf9\u79f0\u6027\uff0c\u672c\u6587\u65e8\u5728\u540c\u65f6\u5229\u7528\u6392\u5217\u548c\u7f29\u653e\u5bf9\u79f0\u6027\u3002", "method": "\u63d0\u51faScaleGMNs\u67b6\u6784\uff0c\u4f7f\u7528\u7b49\u53d8\u7f16\u7801\u5668\u7684\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u65e0\u9700\u663e\u5f0f\u89e3\u51b3\u7ec4\u5408\u5206\u914d\u95ee\u9898\u5373\u53ef\u5bf9\u9f50INRs\u548cCNNs\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u6392\u5217\u548c\u7f29\u653e\u5bf9\u79f0\u6027\u4e0b\u5bf9\u9f50\u7f51\u7edc\uff0c\u4f7f\u76f8\u4f3c\u7f51\u7edc\u81ea\u7136\u6536\u655b\u5230\u540c\u4e00\u76c6\u5730\uff0c\u5b9e\u73b0\u6a21\u578b\u5408\u5e76\u3002", "conclusion": "ScaleGMNs\u6210\u529f\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u5185\u5728\u5bf9\u79f0\u6027\uff0c\u4e3a\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u9ad8\u635f\u5931\u533a\u57df\u3002", "topic": "agent analysis"}}
{"id": "2511.12644", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12644", "abs": "https://arxiv.org/abs/2511.12644", "authors": ["Sascha Lange", "Roland Hafner", "Martin Riedmiller"], "title": "NFQ2.0: The CartPole Benchmark Revisited", "comment": null, "summary": "This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e8620\u5e74\u524d\u7684\u795e\u7ecf\u62df\u5408Q\u8fed\u4ee3\uff08NFQ\uff09\u7b97\u6cd5\uff0c\u63d0\u51fa\u4e86\u73b0\u4ee3\u5316\u53d8\u4f53NFQ2.0\uff0c\u5728CartPole\u4efb\u52a1\u4e0a\u6539\u8fdb\u4e86\u5b66\u4e60\u8fc7\u7a0b\u7684\u91cd\u590d\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "NFQ\u662f\u5c06\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u63a7\u5236\u95ee\u9898\u7684\u5f00\u521b\u6027\u65b9\u6cd5\uff0c\u4f46\u5176\u9700\u8981\u5927\u91cf\u8c03\u53c2\u4e14\u5728\u5b9e\u9645\u63a7\u5236\u95ee\u9898\u4e2d\u96be\u4ee5\u590d\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u5176\u5b66\u4e60\u8fc7\u7a0b\u7684\u91cd\u590d\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86NFQ2.0\u73b0\u4ee3\u5316\u53d8\u4f53\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u8bc6\u522b\u5173\u952e\u8bbe\u8ba1\u51b3\u7b56\u548c\u8d85\u53c2\u6570\uff0c\u4e13\u6ce8\u4e8e\u4f7f\u7528\u6807\u51c6\u5de5\u4e1a\u7ec4\u4ef6\u6784\u5efa\u7684\u771f\u5b9e\u4e16\u754c\u7cfb\u7edf\u3002", "result": "NFQ2.0\u5728\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u539f\u59cb\u53d8\u4f53\uff0c\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u4ece\u4e1a\u8005\u66f4\u6709\u6548\u5730\u590d\u73b0\u548c\u6539\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53ef\u4ee5\u5e2e\u52a9\u4ece\u4e1a\u8005\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u66f4\u6709\u6548\u5730\u5e94\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u9ad8\u5b66\u4e60\u8fc7\u7a0b\u7684\u91cd\u590d\u6027\u548c\u9c81\u68d2\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12751", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12751", "abs": "https://arxiv.org/abs/2511.12751", "authors": ["Timur Anvar", "Jeffrey Chen", "Yuyan Wang", "Rohan Chandra"], "title": "Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving", "comment": null, "summary": "Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u5728\u9ad8\u901f\u516c\u8def\u81ea\u52a8\u9a7e\u9a76\u4e2dRL-only\u3001LLM-only\u548c\u6df7\u5408\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u6df7\u5408\u65b9\u6cd5\u5728\u6210\u529f\u7387\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f46\u5c0fLLM\u5b58\u5728\u4fdd\u5b88\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3RL\u5728\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e2d\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514dLLM\u76f4\u63a5\u63a7\u5236\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u9ad8\u6210\u672c\uff0c\u63a2\u7d22\u5c0fLLM\u901a\u8fc7\u5956\u52b1\u5851\u9020\u6765\u589e\u5f3aRL\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u5c0fLLM\uff08<14B\u53c2\u6570\uff09\u901a\u8fc7\u8bc4\u5206\u72b6\u6001-\u52a8\u4f5c\u8f6c\u6362\u6765\u589e\u5f3aRL\u5956\u52b1\uff0c\u8bad\u7ec3\u65f6LLM\u8f85\u52a9\u5956\u52b1\u5851\u9020\uff0c\u6d4b\u8bd5\u65f6\u4f7f\u7528\u6807\u51c6RL\u7b56\u7565\u6267\u884c\u3002", "result": "RL-only\u6210\u529f\u738773-89%\uff0cLLM-only\u53ef\u8fbe94%\u4f46\u901f\u5ea6\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u6df7\u5408\u65b9\u6cd5\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u3002LLM\u5f71\u54cd\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u4fdd\u5b88\u504f\u5dee\u3002", "conclusion": "\u5f53\u524d\u5c0fLLM\u5728\u5b89\u5168\u5173\u952e\u63a7\u5236\u4efb\u52a1\u4e2d\u5b58\u5728\u91cd\u8981\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u4fdd\u5b88\u504f\u5dee\u548c\u6a21\u578b\u4f9d\u8d56\u7684\u53d8\u5f02\u6027\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12779", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12779", "abs": "https://arxiv.org/abs/2511.12779", "authors": ["Zhenshuo Zhang", "Minxuan Duan", "Youran Ye", "Hongyang R. Zhang"], "title": "Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation", "comment": "17 pages. To appear in AAAI'26", "summary": "We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \\ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a $2\\%$ error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by $16\\%$ on average, while delivering up to $26\\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of $19\\%$. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.", "AI": {"tldr": "\u63d0\u51faPolicyGradEx\u7b97\u6cd5\uff0c\u901a\u8fc7\u5143\u8bad\u7ec3\u548c\u5fae\u8c03\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5c06\u591a\u76ee\u6807RL\u95ee\u9898\u4e2d\u7684n\u4e2a\u76ee\u6807\u9ad8\u6548\u805a\u7c7b\u5230k\u4e2a\u76f8\u5173\u7ec4\u4e2d\uff0c\u5b9e\u73b016%\u6027\u80fd\u63d0\u5347\u548c26\u500d\u52a0\u901f\u3002", "motivation": "\u5728\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5f53\u76ee\u6807\u6570\u91cfn\u589e\u957f\u65f6\uff0c\u4e3a\u6240\u6709\u76ee\u6807\u5b66\u4e60\u5355\u4e00\u7b56\u7565\u662f\u6b21\u4f18\u7684\u3002\u9700\u8981\u5c06\u76f8\u5173\u76ee\u6807\u5206\u7ec4\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u5143\u8bad\u7ec3\u5b66\u4e60\u6240\u6709\u76ee\u6807\u7684\u5143\u7b56\u7565\uff1b2) \u5fae\u8c03\u9002\u5e94\u968f\u673a\u91c7\u6837\u5b50\u96c6\uff0c\u5229\u7528\u7b56\u7565\u7f51\u7edc\u7684\u4e00\u9636\u8fd1\u4f3c\u7279\u6027\u4f30\u8ba1\u4efb\u52a1\u4eb2\u548c\u5ea6\u77e9\u9635\uff0c\u7136\u540e\u8fdb\u884c\u805a\u7c7b\u5206\u7ec4\u3002", "result": "\u5728\u673a\u5668\u4eba\u63a7\u5236\u548cMeta-World\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534716%\uff0c\u901f\u5ea6\u63d0\u5347\u8fbe26\u500d\u3002\u57fa\u4e8e\u635f\u5931\u7684\u805a\u7c7b\u6bd4\u968f\u673a\u5206\u7ec4\u548c\u68af\u5ea6\u76f8\u4f3c\u6027\u5206\u7ec4\u63d0\u534719%\u3002", "conclusion": "PolicyGradEx\u80fd\u6709\u6548\u4f30\u8ba1\u4efb\u52a1\u4eb2\u548c\u5ea6\u5e76\u5206\u7ec4\uff0c\u663e\u8457\u63d0\u5347\u591a\u76ee\u6807RL\u7684\u6027\u80fd\u548c\u6548\u7387\u3002Hessian\u8ff9\u5206\u6790\u63d0\u4f9b\u4e86\u975e\u7a7a\u6cdb\u7684\u6cdb\u5316\u8bef\u5dee\u5ea6\u91cf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12808", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.12808", "abs": "https://arxiv.org/abs/2511.12808", "authors": ["Omar Adalat", "Francesco Belardinelli"], "title": "Expressive Temporal Specifications for Reward Monitoring", "comment": null, "summary": "Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\\text{LTL}_f[\\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u5b9a\u91cf\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91(LTLf[F])\u6765\u5408\u6210\u5956\u52b1\u76d1\u63a7\u5668\uff0c\u4e3a\u53ef\u89c2\u6d4b\u72b6\u6001\u8f68\u8ff9\u751f\u6210\u5bc6\u96c6\u5956\u52b1\u6d41\uff0c\u4ee5\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7a00\u758f\u5956\u52b1\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u6307\u5b9a\u4fe1\u606f\u4e30\u5bcc\u4e14\u5bc6\u96c6\u7684\u5956\u52b1\u51fd\u6570\u8fd9\u4e00\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u7a0b\u51b3\u7b56\u4e2d\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u5f53\u524d\u6587\u732e\u4e3b\u8981\u4f7f\u7528\u5e03\u5c14\u8bed\u4e49\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u53cd\u9988\u3002", "method": "\u5229\u7528\u5b9a\u91cf\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91(LTLf[F])\u5408\u6210\u5956\u52b1\u76d1\u63a7\u5668\uff0c\u8be5\u65b9\u6cd5\u4e0e\u7b97\u6cd5\u65e0\u5173\uff0c\u4ec5\u4f9d\u8d56\u72b6\u6001\u6807\u8bb0\u51fd\u6570\uff0c\u5e76\u80fd\u81ea\u7136\u5904\u7406\u975e\u9a6c\u5c14\u53ef\u592b\u6027\u8d28\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b9a\u91cf\u76d1\u63a7\u5668\u59cb\u7ec8\u4f18\u4e8e\u5e03\u5c14\u76d1\u63a7\u5668\uff0c\u5728\u6700\u5927\u5316\u4efb\u52a1\u5b8c\u6210\u5ea6\u5b9a\u91cf\u6d4b\u91cf\u548c\u51cf\u5c11\u6536\u655b\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u5b9a\u91cfLTLf[F]\u76d1\u63a7\u5668\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5bc6\u96c6\u5956\u52b1\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u6539\u5584\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.12869", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.IT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12869", "abs": "https://arxiv.org/abs/2511.12869", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Zeeshan Memon", "Muhammad Ibtsaam Qadir", "Sagnik Bhattacharya", "Hassan Rizwan", "Abhiram R. Gorle", "Maahe Zehra Kazmi", "Ayesha Mohsin", "Muhammad Usman Rafique", "Zihao He", "Pulkit Mehta", "Muhammad Ali Jamshed", "John M. Cioffi"], "title": "On the Fundamental Limits of LLMs at Scale", "comment": "Submitted to TMLR 2025", "summary": "Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4ece\u8ba1\u7b97\u7406\u8bba\u3001\u4fe1\u606f\u8bba\u548c\u51e0\u4f55\u89d2\u5ea6\u5f62\u5f0f\u5316\u5206\u6790\u4e86LLM\u6269\u5c55\u7684\u4e94\u4e2a\u57fa\u672c\u9650\u5236\uff1a\u5e7b\u89c9\u3001\u4e0a\u4e0b\u6587\u538b\u7f29\u3001\u63a8\u7406\u9000\u5316\u3001\u68c0\u7d22\u8106\u5f31\u6027\u548c\u591a\u6a21\u6001\u4e0d\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9LLM\u6269\u5c55\u9650\u5236\u7684\u63cf\u8ff0\u505c\u7559\u5728\u7ecf\u9a8c\u5c42\u9762\uff0c\u7f3a\u4e4f\u5c06\u8fd9\u4e9b\u73b0\u8c61\u4e0e\u8ba1\u7b97\u3001\u4fe1\u606f\u548c\u5b66\u4e60\u7684\u57fa\u7840\u9650\u5236\u8054\u7cfb\u8d77\u6765\u7684\u4e25\u683c\u7406\u8bba\u7efc\u5408\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bc1\u660e\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u7406\u8bba\u89d2\u5ea6\u5206\u6790\uff1a1) \u53ef\u8ba1\u7b97\u6027\u548c\u4e0d\u53ef\u8ba1\u7b97\u6027\u5bfc\u81f4\u7684\u56fa\u6709\u9519\u8bef\uff1b2) \u4fe1\u606f\u8bba\u548c\u7edf\u8ba1\u7ea6\u675f\uff1b3) \u51e0\u4f55\u548c\u8ba1\u7b97\u6548\u5e94\u5bfc\u81f4\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u3002", "result": "\u8bc1\u660e\u4e86LLM\u6269\u5c55\u5b58\u5728\u56fa\u6709\u7684\u7406\u8bba\u4e0a\u9650\uff1a\u4e0d\u53ef\u8ba1\u7b97\u4efb\u52a1\u5fc5\u7136\u4ea7\u751f\u65e0\u9650\u5931\u8d25\u96c6\uff0c\u6709\u9650\u63cf\u8ff0\u957f\u5ea6\u5f3a\u5236\u538b\u7f29\u8bef\u5dee\uff0c\u51e0\u4f55\u6548\u5e94\u4f7f\u6709\u6548\u4e0a\u4e0b\u6587\u8fdc\u5c0f\u4e8e\u540d\u4e49\u5927\u5c0f\uff0c\u57fa\u4e8e\u4f3c\u7136\u7684\u8bad\u7ec3\u504f\u5411\u6a21\u5f0f\u5b8c\u6210\u800c\u975e\u63a8\u7406\u3002", "conclusion": "LLM\u6269\u5c55\u5728\u67d0\u4e9b\u9886\u57df\u6709\u5e2e\u52a9\uff0c\u4f46\u5728\u67d0\u4e9b\u9886\u57df\u4f1a\u9971\u548c\uff0c\u5728\u53e6\u4e00\u4e9b\u9886\u57df\u65e0\u6cd5\u8fdb\u5c55\u3002\u63d0\u51fa\u4e86\u6709\u754c\u9884\u8a00\u68c0\u7d22\u3001\u4f4d\u7f6e\u8bfe\u7a0b\u5b66\u4e60\u548c\u7a00\u758f\u6ce8\u610f\u529b\u7b49\u5b9e\u9645\u7f13\u89e3\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2511.12986", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12986", "abs": "https://arxiv.org/abs/2511.12986", "authors": ["Abdelouahed Ben Mhamed", "Assia Kamal-Idrissi", "Amal El Fallah Seghrouchni"], "title": "Learning Branching Policies for MILPs with Proximal Policy Optimization", "comment": "11 pages, 3 figures, AAAI conference", "summary": "Branch-and-Bound (B\\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.", "AI": {"tldr": "\u63d0\u51fa\u4e86TGPPO\u6846\u67b6\uff0c\u4f7f\u7528PPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8bad\u7ec3\u5206\u652f\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u4e2d\u5206\u652f\u5b9a\u754c\u7b97\u6cd5\u5728\u5f02\u6784\u5b9e\u4f8b\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u5206\u652f\u7b56\u7565\u5bb9\u6613\u8fc7\u62df\u5408\u4e13\u5bb6\u6f14\u793a\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u7ed3\u6784\u591a\u6837\u6216\u672a\u89c1\u8fc7\u7684\u5b9e\u4f8b\u3002", "method": "\u4f7f\u7528PPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u6784\u5efa\u53c2\u6570\u5316\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u6765\u52a8\u6001\u6355\u6349\u641c\u7d22\u6811\u7684\u6f14\u5316\u4e0a\u4e0b\u6587\u3002", "result": "TGPPO\u5728\u51cf\u5c11\u63a2\u7d22\u8282\u70b9\u6570\u548c\u6539\u8fdbp-\u539f\u59cb\u5bf9\u5076\u79ef\u5206\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u7b56\u7565\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\u5b9e\u4f8b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u6709\u6f5c\u529b\u4e3aMILP\u6c42\u89e3\u5668\u5f00\u53d1\u9c81\u68d2\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u5206\u652f\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.13035", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13035", "abs": "https://arxiv.org/abs/2511.13035", "authors": ["Zeyuan Wang", "Da Li", "Yulin Chen", "Ye Shi", "Liang Bai", "Tianyuan Yu", "Yanwei Fu"], "title": "One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow", "comment": "Accepted in AAAI 2026 Poster", "summary": "We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5355\u6b65\u751f\u6210\u7b56\u7565\u7528\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u6b8b\u5dee\u91cd\u6784MeanFlow\u5b9e\u73b0\u4ece\u566a\u58f0\u76f4\u63a5\u751f\u6210\u52a8\u4f5c\uff0c\u4e0eQ\u5b66\u4e60\u517c\u5bb9\u3002\u8be5\u65b9\u6cd5\u5728\u5355\u9636\u6bb5\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u7684\u4e00\u6b65\u9ad8\u65af\u7b56\u7565\u63a8\u7406\u901f\u5ea6\u5feb\u4f46\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\uff0c\u800c\u57fa\u4e8e\u6d41\u7684\u65b9\u6cd5\u867d\u7136\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u4f46\u901a\u5e38\u9700\u8981\u84b8\u998f\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u3002", "method": "\u91cd\u6784MeanFlow\uff0c\u5c06\u901f\u5ea6\u573a\u548c\u566a\u58f0\u5230\u52a8\u4f5c\u7684\u8f6c\u6362\u6574\u5408\u5230\u5355\u4e00\u7b56\u7565\u7f51\u7edc\u4e2d\uff0c\u63d0\u51fa\u6709\u6548\u7684\u6b8b\u5dee\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u5728\u5355\u9636\u6bb5Q\u5b66\u4e60\u4e2d\u8fdb\u884c\u8868\u8fbe\u6027\u548c\u7a33\u5b9a\u7684\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u5728OGBench\u548cD4RL\u57fa\u51c6\u6d4b\u8bd5\u768473\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u79bb\u7ebf\u548c\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5747\u53d6\u5f97\u4e86\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e09\u4e2a\u5173\u952e\u4f18\u52bf\uff1a\u9ad8\u6548\u7684\u5355\u6b65\u566a\u58f0\u5230\u52a8\u4f5c\u751f\u6210\u3001\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u7684\u8868\u8fbe\u80fd\u529b\u3001\u4ee5\u53ca\u901a\u8fc7\u5355\u9636\u6bb5Q\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684\u7b56\u7565\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.13223", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13223", "abs": "https://arxiv.org/abs/2511.13223", "authors": ["Yuxiang Zhang", "Zhengxu Yu", "Weihang Pan", "Zhongming Jin", "Qiang Fu", "Deng Cai", "Binbin Lin", "Jieping Ye"], "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs", "comment": "Accepted to NeurIPS 2025", "summary": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.", "AI": {"tldr": "TokenSqueeze\u662f\u4e00\u79cd\u65b0\u9896\u7684\u957f\u5230\u77ed\u63a8\u7406\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u63a8\u7406\u6df1\u5ea6\u548c\u5206\u5e03\u5bf9\u9f50\u7684\u8bed\u8a00\u7cbe\u70bc\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406LLM\u7684token\u4f7f\u7528\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406LLM\u751f\u6210\u7684\u957f\u601d\u7ef4\u94fe\u5bfc\u81f4token\u4f7f\u7528\u91cf\u589e\u52a0\uff0c\u5e26\u6765\u66f4\u9ad8\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u6d88\u8017\u3002\u73b0\u6709\u957f\u5230\u77ed\u65b9\u6cd5\u5f80\u5f80\u727a\u7272\u51c6\u786e\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5728\u964d\u4f4etoken\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "1) \u81ea\u9002\u5e94\u9009\u62e9\u63a8\u7406\u6df1\u5ea6\u5339\u914d\u95ee\u9898\u590d\u6742\u5ea6\u7684\u81ea\u751f\u6210\u6837\u672c\uff1b2) \u5206\u5e03\u5bf9\u9f50\u7684\u8bed\u8a00\u7cbe\u70bc\u65b9\u6cd5\uff0c\u5728\u4e0d\u6539\u53d8\u63a8\u7406\u8def\u5f84\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u8bed\u8a00\u8868\u8fbe\u3002", "result": "DeepSeek-R1-Distill-Qwen-7B\u4f7f\u7528\u8be5\u65b9\u6cd5\u5728MATH500\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e8650%\u7684\u5e73\u5747token\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "TokenSqueeze\u4ec5\u4f7f\u7528\u6a21\u578b\u81ea\u751f\u6210\u6570\u636e\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u6574\u7406\u7684\u77ed\u7b54\u6848\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u4fdd\u771f\u7684\u63a8\u7406\u538b\u7f29\u3002", "topic": "agent analysis"}}
{"id": "2511.13322", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13322", "abs": "https://arxiv.org/abs/2511.13322", "authors": ["Senne Deproost", "Dennis Steckelmacher", "Ann Now\u00e9"], "title": "Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning", "comment": "Accepted for BNAIC/BeNeLearn 2025", "summary": "Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eVoronoi\u5212\u5206\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u9ed1\u76d2\u63a7\u5236\u5668\u8f6c\u6362\u4e3a\u53ef\u89e3\u91ca\u7684\u5c40\u90e8\u7ebf\u6027\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u900f\u660e\u5ea6\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u96be\u4ee5\u6ee1\u8db3\u76d1\u7ba1\u8981\u6c42\u548c\u5efa\u7acb\u4fe1\u4efb\uff0c\u9700\u8981\u5c06\u5b66\u4e60\u5230\u7684\u884c\u4e3a\u8f6c\u79fb\u5230\u4eba\u7c7b\u53ef\u8bfb\u7684\u6a21\u578b\u4e2d\u3002", "method": "\u4f7f\u7528Voronoi\u5212\u5206\u5c06\u72b6\u6001\u7a7a\u95f4\u5212\u5206\u4e3a\u591a\u4e2a\u533a\u57df\uff0c\u5728\u6bcf\u4e2a\u533a\u57df\u5185\u4f7f\u7528\u7b80\u5316\u7684\u7ebf\u6027\u6a21\u578b\u6765\u6a21\u62df\u539f\u59cb\u63a7\u5236\u5668\u7684\u884c\u4e3a\uff0c\u5b9e\u73b0\u6a21\u578b\u65e0\u5173\u7684\u77e5\u8bc6\u84b8\u998f\u3002", "result": "\u5728\u7f51\u683c\u4e16\u754c\u73af\u5883\u548c\u7ecf\u5178\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u7684\u7b56\u7565\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u84b8\u998f\u540e\u7684\u6027\u80fd\u4e0e\u539f\u9ed1\u76d2\u7b56\u7565\u76f8\u5f53\u6216\u7565\u6709\u63d0\u5347\u3002", "conclusion": "\u5c40\u90e8\u4e13\u4e1a\u5316\u7ebf\u6027\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e73\u8861\u7075\u6d3b\u6027\u4e0e\u590d\u6742\u6027\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u5668\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.13103", "categories": ["cs.LG", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13103", "abs": "https://arxiv.org/abs/2511.13103", "authors": ["Vidur Sinha", "Muhammed Ustaomeroglu", "Guannan Qu"], "title": "Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions", "comment": "8 pages, 7 figures, submitted for review", "summary": "Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.", "AI": {"tldr": "STACCA\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u4e2d\u5f0f\u56feTransformer\u8bc4\u8bba\u5bb6\u548c\u5171\u4eab\u56feTransformer\u6f14\u5458\u6765\u89e3\u51b3\u7f51\u7edc\u63a7\u5236\u4e2d\u7684\u957f\u7a0b\u4f9d\u8d56\u548c\u62d3\u6251\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MARL\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a1\uff09\u4f9d\u8d56\u5c40\u90e8\u4ea4\u4e92\u8870\u51cf\u5047\u8bbe\uff0c\u96be\u4ee5\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\uff08\u5982\u7ea7\u8054\u6545\u969c\u3001\u75ab\u60c5\u7206\u53d1\uff09\uff1b2\uff09\u7f3a\u4e4f\u7f51\u7edc\u62d3\u6251\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u5728\u65b0\u56fe\u4e0a\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u63d0\u51faSTACCA\u6846\u67b6\uff1a1\uff09\u96c6\u4e2d\u5f0f\u56feTransformer\u8bc4\u8bba\u5bb6\u5efa\u6a21\u957f\u7a0b\u4f9d\u8d56\u5e76\u63d0\u4f9b\u7cfb\u7edf\u7ea7\u53cd\u9988\uff1b2\uff09\u5171\u4eab\u56feTransformer\u6f14\u5458\u5b66\u4e60\u53ef\u6cdb\u5316\u7b56\u7565\uff1b3\uff09\u96c6\u6210\u53cd\u4e8b\u5b9e\u4f18\u52bf\u4f30\u8ba1\u5668\u6539\u8fdb\u4fe1\u7528\u5206\u914d\u3002", "result": "\u5728\u75ab\u60c5\u63a7\u5236\u548c\u8c23\u8a00\u4f20\u64ad\u7f51\u7edc\u63a7\u5236\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0cSTACCA\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3001\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u57fa\u4e8eTransformer\u7684MARL\u67b6\u6784\u5177\u6709\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u53ef\u6cdb\u5316\u63a7\u5236\u7684\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.13640", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13640", "abs": "https://arxiv.org/abs/2511.13640", "authors": ["Haohui Wang", "Jingyuan Qi", "Jianpeng Chen", "Jun Wu", "Lifu Huang", "Lecheng Zheng", "Kevin Choi", "Balaji Veeramani", "Edward Bowen", "Alison Hu", "Tyler Cody", "Dawei Zhou"], "title": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures", "comment": null, "summary": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u6df7\u5408\u6570\u636e\u96c6\u4e2d\u7684\u5206\u5e03\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e09\u9636\u6bb5\u7f29\u653e\u884c\u4e3a\u548cLLM\u6cdb\u5316\u8fb9\u754c\u7406\u8bba\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u6df7\u5408\u6570\u636e\u96c6\u7684\u4f7f\u7528\u65e5\u76ca\u666e\u904d\uff0c\u4f46\u5408\u6210\u6570\u636e\u5b58\u5728\u7cfb\u7edf\u6027\u5206\u5e03\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5bf9\u957f\u5c3e\u77e5\u8bc6\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u8fd9\u7ed9\u8bc4\u4f30\u6df7\u5408\u6570\u636e\u96c6\u7684\u6548\u7528\u5e26\u6765\u4e86\u6839\u672c\u6027\u6311\u6218\u3002", "method": "\u8bc6\u522b\u4e86\u4e09\u9636\u6bb5\u7f29\u653e\u884c\u4e3a\u7279\u5f81\uff0c\u63a8\u5bfc\u4e86\u9002\u7528\u4e8e\u771f\u5b9e-\u5408\u6210\u6df7\u5408\u6570\u636e\u7684LLM\u6cdb\u5316\u8fb9\u754c\u7406\u8bba\uff0c\u5e76\u57fa\u4e8e\u7406\u8bba\u53d1\u73b0\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u5230\u5927\u89c4\u6570\u636e\u96c6\u7684\u9ad8\u6548\u6570\u636e\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u60c5\u611f\u5206\u7c7b\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u590d\u6742\u63a8\u7406\u56db\u4e2a\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u8bc4\u4f30\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u8bc4\u4f30\u771f\u5b9e-\u5408\u6210\u6df7\u5408\u6570\u636e\u96c6\u7684\u6548\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u957f\u5c3e\u77e5\u8bc6\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2511.13240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13240", "abs": "https://arxiv.org/abs/2511.13240", "authors": ["Arka Pal", "Teo Kitanovski", "Arthur Liang", "Akilesh Potti", "Micah Goldblum"], "title": "Incoherent Beliefs & Inconsistent Actions in Large Language Models", "comment": null, "summary": "Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u4fe1\u5ff5\u66f4\u65b0\u4e0d\u4e00\u81f4\u548c\u884c\u52a8\u4e0e\u4fe1\u5ff5\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5373\u4f7f\u9ad8\u51c6\u786e\u7387\u6a21\u578b\u4e5f\u5b58\u5728\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u56e0\u4e3a\u9759\u6001\u6570\u636e\u96c6\u8bc4\u4f30\u65e0\u6cd5\u9884\u6d4b\u5176\u5728\u9700\u8981\u987a\u5e8f\u4ea4\u4e92\u548c\u4fe1\u5ff5\u66f4\u65b0\u7684\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5LLMs\u7684\u4fe1\u5ff5\u66f4\u65b0\u4e00\u81f4\u6027\u548c\u884c\u52a8\u4e0e\u4fe1\u5ff5\u7684\u4e00\u81f4\u6027\uff0c\u5305\u62ec\u76f4\u63a5\u83b7\u53d6\u540e\u9a8c\u6982\u7387\u4e0e\u6b63\u786e\u66f4\u65b0\u5148\u9a8c\u6982\u7387\u7684\u6bd4\u8f83\uff0c\u4ee5\u53ca\u5728\u535a\u5f69\u5e02\u573a\u4e2d\u7684\u884c\u4e3a\u5206\u6790\u3002", "result": "LLMs\u5728\u4fe1\u5ff5\u66f4\u65b0\u4e0a\u5e73\u5747\u670930%\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u884c\u52a8\u5e38\u4e0e\u5185\u90e8\u4fe1\u5ff5\u4e0d\u7b26\uff0c\u4e14\u5bf9\u7528\u6237\u8d28\u7591\u8868\u73b0\u51fa\u4e2d\u7b49\u7a0b\u5ea6\u7684\u81ea\u6211\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u9884\u6d4bLLM\u884c\u4e3a\u5b58\u5728\u56f0\u96be\uff0c\u5373\u4f7f\u8868\u73b0\u826f\u597d\u7684\u6a21\u578b\u4e5f\u5b58\u5728\u8fd9\u4e9b\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.5dccfb44", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmedium.com%2Fcapital-one-tech%2Fcapital-one-at-emnlp-2025-trust-and-efficiency-in-ai-d87afc16a0cd%3Futm_source=tldrdata/1/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/jz6BPVcTi05rxnE54hTwAuZmZZxYVrrkd-aWkex03Jc=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmedium.com%2Fcapital-one-tech%2Fcapital-one-at-emnlp-2025-trust-and-efficiency-in-ai-d87afc16a0cd%3Futm_source=tldrdata/1/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/jz6BPVcTi05rxnE54hTwAuZmZZxYVrrkd-aWkex03Jc=431", "authors": ["TLDR Newsletter"], "title": "Capital One at EMNLP 2025: Trust and efficiency in AI", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmedium.com%2Fcapital-one-tech%2Fcapital-one-at-emnlp-2025-trust-and-efficiency-in-ai-d87afc16a0cd%3Futm_source=tldrdata/1/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/jz6BPVcTi05rxnE54hTwAuZmZZxYVrrkd-aWkex03Jc=431", "summary": "Capital One at EMNLP 2025: Trust and efficiency in AI (7 minute read) Capital One showcased several peer-reviewed advances at EMNLP 2025, including a multi-agent LLM framework for complex financial workflows (MACAW) and a data augmentation framework (GRAID), which boosts guardrail model F1 scores by 12%. Additional key contributions feature a merged-embedding approach delivering 47.5% greater RAG consistency, TruthTorchLM for multi-method truthfulness evaluation, and activation-based confiden...", "source": "tldr", "AI": {"tldr": "Capital One\u5728EMNLP 2025\u5c55\u793a\u4e86\u591a\u9879AI\u521b\u65b0\uff0c\u5305\u62ec\u7528\u4e8e\u590d\u6742\u91d1\u878d\u5de5\u4f5c\u6d41\u7684\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6MACAW\u3001\u63d0\u5347\u62a4\u680f\u6a21\u578b\u6027\u80fd\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6GRAID\u3001\u63d0\u9ad8RAG\u4e00\u81f4\u6027\u7684\u5d4c\u5165\u5408\u5e76\u65b9\u6cd5\u3001\u591a\u65b9\u6cd5\u771f\u5b9e\u6027\u8bc4\u4f30\u5de5\u5177TruthTorchLM\u4ee5\u53ca\u57fa\u4e8e\u6fc0\u6d3b\u7684\u7f6e\u4fe1\u5ea6\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u91d1\u878d\u9886\u57dfAI\u5e94\u7528\u4e2d\u7684\u4fe1\u4efb\u548c\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u5de5\u4f5c\u6d41\u5904\u7406\u3001\u6a21\u578b\u5b89\u5168\u6027\u548c\u68c0\u7d22\u4e00\u81f4\u6027\u65b9\u9762\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6(MACAW)\u3001\u6570\u636e\u589e\u5f3a\u6846\u67b6(GRAID)\u3001\u5d4c\u5165\u5408\u5e76\u65b9\u6cd5\u3001\u591a\u65b9\u6cd5\u771f\u5b9e\u6027\u8bc4\u4f30\u5de5\u5177(TruthTorchLM)\u548c\u57fa\u4e8e\u6fc0\u6d3b\u7684\u7f6e\u4fe1\u5ea6\u6280\u672f\u3002", "result": "GRAID\u6846\u67b6\u5c06\u62a4\u680f\u6a21\u578bF1\u5206\u6570\u63d0\u5347\u4e8612%\uff0c\u5d4c\u5165\u5408\u5e76\u65b9\u6cd5\u4f7fRAG\u4e00\u81f4\u6027\u63d0\u9ad8\u4e8647.5%\u3002", "conclusion": "\u8fd9\u4e9b\u521b\u65b0\u663e\u8457\u63d0\u5347\u4e86\u91d1\u878dAI\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3001\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.bdb6278a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Feffective-context-engineering-for-ai-agents%3Futm_source=tldrdata/1/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/1EPoWNcZyvD_i6Hs4N7oyiob4vsmywHMRicINSJiElM=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Feffective-context-engineering-for-ai-agents%3Futm_source=tldrdata/1/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/1EPoWNcZyvD_i6Hs4N7oyiob4vsmywHMRicINSJiElM=431", "authors": ["TLDR Newsletter"], "title": "Effective context engineering for AI agents", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Feffective-context-engineering-for-ai-agents%3Futm_source=tldrdata/1/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/1EPoWNcZyvD_i6Hs4N7oyiob4vsmywHMRicINSJiElM=431", "summary": "Effective context engineering for AI agents (12 minute read) Context engineering has become critical for building reliable, long-horizon LLM agents, shifting focus from prompt optimization to actively managing the set of information (\u2018tokens') sent to models at inference time. Effective strategies, such as compaction, structured note-taking, dynamic just-in-time retrieval, and sub-agent architectures, address context window constraints, minimize context rot, and improve agent coherence across...", "source": "tldr", "AI": {"tldr": "\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5df2\u6210\u4e3a\u6784\u5efa\u53ef\u9760\u3001\u957f\u671f\u8fd0\u884c\u7684LLM\u4ee3\u7406\u7684\u5173\u952e\u6280\u672f\uff0c\u4ece\u63d0\u793a\u4f18\u5316\u8f6c\u5411\u5728\u63a8\u7406\u65f6\u4e3b\u52a8\u7ba1\u7406\u53d1\u9001\u7ed9\u6a21\u578b\u7684\u4fe1\u606f\u96c6\uff08'tokens'\uff09\u3002", "motivation": "\u89e3\u51b3\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u3001\u6700\u5c0f\u5316\u4e0a\u4e0b\u6587\u8870\u51cf\uff0c\u5e76\u63d0\u9ad8\u4ee3\u7406\u5728\u957f\u671f\u4efb\u52a1\u4e2d\u7684\u8fde\u8d2f\u6027\u3002", "method": "\u91c7\u7528\u538b\u7f29\u3001\u7ed3\u6784\u5316\u7b14\u8bb0\u3001\u52a8\u6001\u5373\u65f6\u68c0\u7d22\u548c\u5b50\u4ee3\u7406\u67b6\u6784\u7b49\u7b56\u7565\u6765\u7ba1\u7406\u4e0a\u4e0b\u6587\u3002", "result": "\u6709\u6548\u7b56\u7565\u80fd\u591f\u4f18\u5316\u4e0a\u4e0b\u6587\u4f7f\u7528\uff0c\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5de5\u7a0b\u662f\u6784\u5efa\u53ef\u9760LLM\u4ee3\u7406\u7684\u91cd\u8981\u6280\u672f\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.cbb4593f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmariozechner.at%2Fposts%2F2025-11-02-what-if-you-dont-need-mcp%2F%3Futm_source=tldrwebdev/1/0100019a91b922ab-4086b72c-2948-47ee-bf9e-96f4a30cd712-000000/xpX_OCwSjl-KhOjgZKjSPXieKKuQ3diR7UfskpsrEIk=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmariozechner.at%2Fposts%2F2025-11-02-what-if-you-dont-need-mcp%2F%3Futm_source=tldrwebdev/1/0100019a91b922ab-4086b72c-2948-47ee-bf9e-96f4a30cd712-000000/xpX_OCwSjl-KhOjgZKjSPXieKKuQ3diR7UfskpsrEIk=431", "authors": ["TLDR Newsletter"], "title": "What if you don't need MCP at all?", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmariozechner.at%2Fposts%2F2025-11-02-what-if-you-dont-need-mcp%2F%3Futm_source=tldrwebdev/1/0100019a91b922ab-4086b72c-2948-47ee-bf9e-96f4a30cd712-000000/xpX_OCwSjl-KhOjgZKjSPXieKKuQ3diR7UfskpsrEIk=431", "summary": "What if you don't need MCP at all? (15 minute read) MCP servers for browser automation with agents are often inefficient and hard to extend. Instead, simple Bash scripts and code for tasks like starting a browser, navigating URLs, executing JavaScript, and taking screenshots are usually more token efficient and more customizable. By cloning tool repositories and setting up an alias, these scripts become globally available to agents like Claude, making browser interaction easier.", "source": "tldr", "AI": {"tldr": "\u4e0d\u9700\u8981MCP\u670d\u52a1\u5668\uff0c\u4f7f\u7528\u7b80\u5355\u7684Bash\u811a\u672c\u548c\u4ee3\u7801\u8fdb\u884c\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u66f4\u9ad8\u6548\u4e14\u53ef\u5b9a\u5236", "motivation": "MCP\u670d\u52a1\u5668\u5728\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u4e2d\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u901a\u8fc7\u514b\u9686\u5de5\u5177\u4ed3\u5e93\u5e76\u8bbe\u7f6e\u522b\u540d\uff0c\u4f7fBash\u811a\u672c\u548c\u4ee3\u7801\u5168\u5c40\u53ef\u7528\uff0c\u5b9e\u73b0\u6d4f\u89c8\u5668\u4ea4\u4e92\u529f\u80fd", "result": "\u8fd9\u79cd\u65b9\u6cd5\u6bd4MCP\u670d\u52a1\u5668\u66f4\u8282\u7701token\uff0c\u4e14\u5177\u6709\u66f4\u597d\u7684\u53ef\u5b9a\u5236\u6027", "conclusion": "\u7b80\u5355\u7684\u811a\u672c\u65b9\u6cd5\u6bd4\u590d\u6742\u7684MCP\u670d\u52a1\u5668\u66f4\u9002\u5408\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u4efb\u52a1", "topic": "swe application"}}
{"id": "tldr.2511.202e0a4e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcweill%2Fgotests%3Futm_source=tldrwebdev/1/0100019a91b922ab-4086b72c-2948-47ee-bf9e-96f4a30cd712-000000/4YD74C-Y6IpUFkOw6VGYsky5w7JPLWJqe70BxKqpid0=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcweill%2Fgotests%3Futm_source=tldrwebdev/1/0100019a91b922ab-4086b72c-2948-47ee-bf9e-96f4a30cd712-000000/4YD74C-Y6IpUFkOw6VGYsky5w7JPLWJqe70BxKqpid0=431", "authors": ["TLDR Newsletter"], "title": "Gotests", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcweill%2Fgotests%3Futm_source=tldrwebdev/1/0100019a91b922ab-4086b72c-2948-47ee-bf9e-96f4a30cd712-000000/4YD74C-Y6IpUFkOw6VGYsky5w7JPLWJqe70BxKqpid0=431", "summary": "Gotests (GitHub Repo) Gotests is a Go test generator that automates the creation of table-driven tests from Go source code. It has zero-config test generation, smart scaffolding, flexible filtering, and support for Go generics.", "source": "tldr", "AI": {"tldr": "Gotests\u662f\u4e00\u4e2aGo\u6d4b\u8bd5\u751f\u6210\u5668\uff0c\u80fd\u591f\u4eceGo\u6e90\u4ee3\u7801\u81ea\u52a8\u521b\u5efa\u8868\u683c\u9a71\u52a8\u6d4b\u8bd5\uff0c\u5177\u6709\u96f6\u914d\u7f6e\u6d4b\u8bd5\u751f\u6210\u3001\u667a\u80fd\u811a\u624b\u67b6\u3001\u7075\u6d3b\u8fc7\u6ee4\u548cGo\u6cdb\u578b\u652f\u6301\u7b49\u529f\u80fd\u3002", "motivation": "\u65e8\u5728\u7b80\u5316\u548c\u81ea\u52a8\u5316Go\u8bed\u8a00\u4e2d\u8868\u683c\u9a71\u52a8\u6d4b\u8bd5\u7684\u521b\u5efa\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u6d4b\u8bd5\u8986\u76d6\u7387\u3002", "method": "\u901a\u8fc7\u5206\u6790Go\u6e90\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u8868\u683c\u9a71\u52a8\u6d4b\u8bd5\uff0c\u63d0\u4f9b\u96f6\u914d\u7f6e\u7684\u6d4b\u8bd5\u751f\u6210\u3001\u667a\u80fd\u811a\u624b\u67b6\u6784\u5efa\u3001\u7075\u6d3b\u7684\u6d4b\u8bd5\u7528\u4f8b\u8fc7\u6ee4\u4ee5\u53ca\u5bf9Go\u6cdb\u578b\u7684\u652f\u6301\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684Go\u6d4b\u8bd5\u751f\u6210\u5de5\u5177\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u624b\u52a8\u7f16\u5199\u6d4b\u8bd5\u4ee3\u7801\u7684\u5de5\u4f5c\u91cf\u3002", "conclusion": "Gotests\u5de5\u5177\u6210\u529f\u5b9e\u73b0\u4e86Go\u6d4b\u8bd5\u751f\u6210\u7684\u81ea\u52a8\u5316\uff0c\u4e3aGo\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4fbf\u6377\u7684\u6d4b\u8bd5\u521b\u5efa\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "tldr.2511.c797e5d8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot-cli-101-how-to-use-github-copilot-from-the-command-line%2F%3Futm_source=tldrdevops/1/0100019a91c762a0-e1927d5d-441b-44e1-bce9-c43f7525f469-000000/zVVQCwVAggpJKaucpo9pXFnb80M9h5PmBmDWV0QzQjQ=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot-cli-101-how-to-use-github-copilot-from-the-command-line%2F%3Futm_source=tldrdevops/1/0100019a91c762a0-e1927d5d-441b-44e1-bce9-c43f7525f469-000000/zVVQCwVAggpJKaucpo9pXFnb80M9h5PmBmDWV0QzQjQ=431", "authors": ["TLDR Newsletter"], "title": "GitHub Copilot CLI 101: How to use GitHub Copilot from the command line", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot-cli-101-how-to-use-github-copilot-from-the-command-line%2F%3Futm_source=tldrdevops/1/0100019a91c762a0-e1927d5d-441b-44e1-bce9-c43f7525f469-000000/zVVQCwVAggpJKaucpo9pXFnb80M9h5PmBmDWV0QzQjQ=431", "summary": "GitHub Copilot CLI 101: How to use GitHub Copilot from the command line (8 minute read) GitHub Copilot CLI lets users interact with Copilot directly from the terminal to generate scripts, refactor code, run commands, and get explanations without leaving their workflow. It supports interactive and programmatic modes, integrates with MCP servers for custom capabilities, and helps automate tasks, maintain codebases, generate documentation, and prototype projects efficiently.", "source": "tldr", "AI": {"tldr": "GitHub Copilot CLI \u662f\u4e00\u4e2a\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u8ba9\u7528\u6237\u53ef\u4ee5\u76f4\u63a5\u5728\u7ec8\u7aef\u4e2d\u4f7f\u7528 Copilot \u6765\u751f\u6210\u811a\u672c\u3001\u91cd\u6784\u4ee3\u7801\u3001\u8fd0\u884c\u547d\u4ee4\u548c\u83b7\u53d6\u89e3\u91ca\uff0c\u800c\u65e0\u9700\u79bb\u5f00\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u5728\u547d\u4ee4\u884c\u73af\u5883\u4e2d\u76f4\u63a5\u4f7f\u7528 GitHub Copilot \u7684\u529f\u80fd\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u5de5\u4f5c\u6d41\u7a0b\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "method": "\u652f\u6301\u4ea4\u4e92\u5f0f\u548c\u7a0b\u5e8f\u5316\u4e24\u79cd\u6a21\u5f0f\uff0c\u4e0e MCP \u670d\u52a1\u5668\u96c6\u6210\u4ee5\u63d0\u4f9b\u81ea\u5b9a\u4e49\u529f\u80fd\uff0c\u5e2e\u52a9\u81ea\u52a8\u5316\u4efb\u52a1\u3001\u7ef4\u62a4\u4ee3\u7801\u5e93\u3001\u751f\u6210\u6587\u6863\u548c\u5feb\u901f\u539f\u578b\u5f00\u53d1\u3002", "result": "\u5f00\u53d1\u8005\u53ef\u4ee5\u5728\u7ec8\u7aef\u4e2d\u9ad8\u6548\u5730\u4f7f\u7528 Copilot \u8fdb\u884c\u5404\u79cd\u5f00\u53d1\u4efb\u52a1\uff0c\u5305\u62ec\u4ee3\u7801\u751f\u6210\u3001\u91cd\u6784\u3001\u547d\u4ee4\u6267\u884c\u548c\u4ee3\u7801\u89e3\u91ca\u3002", "conclusion": "GitHub Copilot CLI \u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f00\u53d1\u8005\u5728\u547d\u4ee4\u884c\u73af\u5883\u4e2d\u7684\u5de5\u4f5c\u6548\u7387\u548c\u81ea\u52a8\u5316\u80fd\u529b\u3002", "topic": "swe application"}}
{"id": "tldr.2511.9cbcd609", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FRXH4Jm/1/0100019a91ecbdac-2175f932-421a-4770-9cc5-88faf5d8bd8e-000000/2RAZOyGSFQJfTbgDcYTvJgBOVIidEIoqvom56q-8rw8=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FRXH4Jm/1/0100019a91ecbdac-2175f932-421a-4770-9cc5-88faf5d8bd8e-000000/2RAZOyGSFQJfTbgDcYTvJgBOVIidEIoqvom56q-8rw8=431", "authors": ["TLDR Newsletter"], "title": "Unpacking the dAI Stack", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FRXH4Jm/1/0100019a91ecbdac-2175f932-421a-4770-9cc5-88faf5d8bd8e-000000/2RAZOyGSFQJfTbgDcYTvJgBOVIidEIoqvom56q-8rw8=431", "summary": "Unpacking the dAI Stack (8 minute read) The deAI stack comprises three interoperable layers: x402 from Coinbase and Cloudflare for agent payments via stablecoins at the application layer, ERC 8004 as an onchain discovery registry mapping agent IDs to capabilities using NFT-based AgentCards, and Google's A2A protocol for handling transport-layer communication through JSON-RPC over HTTPS. These standards mirror the traditional internet stack, where TCP/IP handles transport, DNS manages discover...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86dAI\u5806\u6808\u7684\u4e09\u4e2a\u4e92\u64cd\u4f5c\u5c42\uff1ax402\u7528\u4e8e\u4ee3\u7406\u652f\u4ed8\uff0cERC 8004\u4f5c\u4e3a\u94fe\u4e0a\u53d1\u73b0\u6ce8\u518c\u8868\uff0cA2A\u534f\u8bae\u5904\u7406\u4f20\u8f93\u5c42\u901a\u4fe1\u3002", "motivation": "\u6784\u5efa\u53bb\u4e2d\u5fc3\u5316AI\u4ee3\u7406\u7684\u6807\u51c6\u5806\u6808\uff0c\u7c7b\u4f3c\u4e8e\u4f20\u7edf\u4e92\u8054\u7f51\u7684TCP/IP\u548cDNS\u67b6\u6784\uff0c\u4ee5\u5b9e\u73b0AI\u4ee3\u7406\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u53ef\u53d1\u73b0\u6027\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a\u5e94\u7528\u5c42\u7684x402\u652f\u4ed8\u534f\u8bae\u3001\u94fe\u4e0a\u5c42\u7684ERC 8004\u53d1\u73b0\u6ce8\u518c\u8868\u3001\u4f20\u8f93\u5c42\u7684A2A\u901a\u4fe1\u534f\u8bae\u3002", "result": "\u5efa\u7acb\u4e86\u5b8c\u6574\u7684dAI\u5806\u6808\u6807\u51c6\uff0c\u4f7fAI\u4ee3\u7406\u80fd\u591f\u8fdb\u884c\u94fe\u4e0a\u652f\u4ed8\u3001\u80fd\u529b\u53d1\u73b0\u548c\u5b89\u5168\u901a\u4fe1\u3002", "conclusion": "dAI\u5806\u6808\u4e3a\u53bb\u4e2d\u5fc3\u5316AI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u7840\u67b6\u6784\uff0c\u7c7b\u4f3c\u4e8e\u4e92\u8054\u7f51\u7684\u57fa\u7840\u534f\u8bae\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.5eafb77a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftodo2.pro%2F%3Futm_source=tldrfounders/1/0100019a91ee09e5-f672d18c-785f-4f12-82ed-d4a584129503-000000/79tEdLC4LPpPCEn89dP6LEvS7TH2gPhWbpu1lwFRBC0=431", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftodo2.pro%2F%3Futm_source=tldrfounders/1/0100019a91ee09e5-f672d18c-785f-4f12-82ed-d4a584129503-000000/79tEdLC4LPpPCEn89dP6LEvS7TH2gPhWbpu1lwFRBC0=431", "authors": ["TLDR Newsletter"], "title": "Todo2", "comment": "Source: TLDR Newsletter, Date: 2025-11-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftodo2.pro%2F%3Futm_source=tldrfounders/1/0100019a91ee09e5-f672d18c-785f-4f12-82ed-d4a584129503-000000/79tEdLC4LPpPCEn89dP6LEvS7TH2gPhWbpu1lwFRBC0=431", "summary": "Todo2 (Tool) Todo2 is an AI-powered project manager for Cursor that helps developers research, plan, and execute projects without leaving their code editor.", "source": "tldr", "AI": {"tldr": "Todo2\u662f\u4e00\u4e2aAI\u9a71\u52a8\u7684\u9879\u76ee\u7ba1\u7406\u5de5\u5177\uff0c\u4e13\u4e3aCursor\u7f16\u8f91\u5668\u8bbe\u8ba1\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u4ee3\u7801\u7f16\u8f91\u5668\u4e2d\u7814\u7a76\u3001\u89c4\u5212\u548c\u6267\u884c\u9879\u76ee", "motivation": "\u89e3\u51b3\u5f00\u53d1\u8005\u5728\u4ee3\u7801\u7f16\u8f91\u5668\u548c\u9879\u76ee\u7ba1\u7406\u5de5\u5177\u4e4b\u95f4\u9891\u7e41\u5207\u6362\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387", "method": "\u5f00\u53d1AI\u9a71\u52a8\u7684\u9879\u76ee\u7ba1\u7406\u5de5\u5177\uff0c\u96c6\u6210\u5230Cursor\u7f16\u8f91\u5668\u4e2d\uff0c\u63d0\u4f9b\u9879\u76ee\u7814\u7a76\u3001\u89c4\u5212\u548c\u6267\u884c\u529f\u80fd", "result": "\u521b\u5efa\u4e86Todo2\u5de5\u5177\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u5728\u4ee3\u7801\u7f16\u8f91\u5668\u4e2d\u5b8c\u6210\u9879\u76ee\u7ba1\u7406\u4efb\u52a1", "conclusion": "Todo2\u6210\u529f\u5730\u5c06\u9879\u76ee\u7ba1\u7406\u529f\u80fd\u96c6\u6210\u5230\u4ee3\u7801\u7f16\u8f91\u5668\u4e2d\uff0c\u51cf\u5c11\u4e86\u4e0a\u4e0b\u6587\u5207\u6362\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387", "topic": "swe application"}}
{"id": "wechat.2511.b1b7fbbe", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3NTU3MDA4MA==&mid=2660632087&idx=1&sn=d574e9885e8cf9b3b94e22c665199927&chksm=859b0974e5932d83ebf9a15556701df1cc3f6269a848651694bfc2bf7768aebffa47511b204f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3NTU3MDA4MA==&mid=2660632087&idx=1&sn=d574e9885e8cf9b3b94e22c665199927&chksm=859b0974e5932d83ebf9a15556701df1cc3f6269a848651694bfc2bf7768aebffa47511b204f#rd", "authors": ["\u738b\u94eeSilvia"], "title": "PayPal\u8d22\u62a5\u900f\u89c6AI\u652f\u4ed8\uff1a\u63a8\u51fa\u201c\u4e00\u6b21\u96c6\u6210\u3001\u591aLLM\u8986\u76d6\u201d\u7684<em class=\"highlight\">Agentic</em>\u7535\u5546\u670d\u52a1\uff0c\u5728AI\u5546\u4e1a\u4e2d\u8fde\u63a5\u6570\u5343\u4e07\u5546\u6237\u4e0e\u5e7f\u5927\u6d88\u8d39\u8005\uff01", "comment": "Source: WeChat, Published: 2025-11-18 12:09:16", "summary": "\u4e1a\u52a1\u9648\u8ff0\uff1a\u63a8\u51faAgentic\u7535\u5546\u670d\u52a1\uff0c\u52a9\u529b\u5546\u5bb6\u901a\u8fc7\u8c37\u6b4c\u3001OpenAI\u548cPerplexity\u7b49\u591a\u4e2aAI\u5e73\u53f0\u5f00\u5c55\u9500\u552e\u5168\u65b0\u7684PayPal\uff1a\u5b9e\u73b0\u66f4\u5feb\u3001\u66f4\u76c8\u5229\u7684\u589e\u957f\uff1bQ3\u6bcf\u80a1\u6536\u76ca\u589e\u957f12%\uff0c\u8d85\u51fa\u9884\u671f", "AI": {"tldr": "\u4e1a\u52a1\u9648\u8ff0\uff1a\u63a8\u51faAgentic\u7535\u5546\u670d\u52a1\uff0c\u52a9\u529b\u5546\u5bb6\u901a\u8fc7\u8c37\u6b4c\u3001OpenAI\u548cPerplexity\u7b49\u591a\u4e2aAI\u5e73\u53f0\u5f00\u5c55\u9500\u552e\u5168\u65b0\u7684PayPal\uff1a\u5b9e\u73b0\u66f4\u5feb\u3001\u66f4\u76c8\u5229\u7684\u589e\u957f\uff1bQ3\u6bcf\u80a1\u6536\u76ca\u589e\u957f12%\uff0c\u8d85\u51fa\u9884\u671f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.99209fa1", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMjYyMzcwNA==&mid=2247490236&idx=1&sn=3cc4c9cef718ef1324c67ef0f31fd873&chksm=9a092f6f44439ab9c52edd22e3744d992926e43711244c01c1044c27a7bfb4a599c4e5e88d37#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMjYyMzcwNA==&mid=2247490236&idx=1&sn=3cc4c9cef718ef1324c67ef0f31fd873&chksm=9a092f6f44439ab9c52edd22e3744d992926e43711244c01c1044c27a7bfb4a599c4e5e88d37#rd", "authors": ["\u7a76\u6a21\u667a"], "title": "\u5eb7\u5948\u5c14\u5927\u5b66\u7814\u7a76\u62a5\u544a\uff1a\u7cfb\u7edf\u89e3\u6784AI Agent\u4e0e<em class=\"highlight\">Agentic</em> AI\u7684\u6838\u5fc3\u5dee\u5f02\uff08\u9644\u4e0b\u8f7d\uff09", "comment": "Source: WeChat, Published: 2025-11-18 09:58:00", "summary": "3. Agentic AI\uff08\u4ee3\u7406\u5f0fAI\uff09\u7531\u591a\u4e2a\u4e13\u4e1a\u5316AI \u667a\u80fd\u4f53\u7ec4\u6210\u7684\u534f\u540c\u7cfb\u7edf\uff0c\u6838\u5fc3\u7279\u5f81\u662f\u76ee\u6807\u62c6\u89e3-\u4efb\u52a1\u5206\u914d-\u7ed3\u679c\u6574\u5408\u7684\u95ed\u73af\u80fd\u529b\u3002\u7cfb\u7edf\u4e2d\u901a\u5e38\u5305\u542b\u4e00\u4e2a\u4e2d\u592e\u534f\u8c03\u8005\uff0c\u7c7b\u4f3c\u6570\u5b57\u5316\u9879\u76ee\u7ecf\u7406\uff0c\u53ef\u5c06\u590d\u6742\u76ee\u6807\u62c6\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u5c06\u5176\u5206\u914d\u7ed9\u4e0d\u540c\u7684\u4e13", "AI": {"tldr": "3. Agentic AI\uff08\u4ee3\u7406\u5f0fAI\uff09\u7531\u591a\u4e2a\u4e13\u4e1a\u5316AI \u667a\u80fd\u4f53\u7ec4\u6210\u7684\u534f\u540c\u7cfb\u7edf\uff0c\u6838\u5fc3\u7279\u5f81\u662f\u76ee\u6807\u62c6\u89e3-\u4efb\u52a1\u5206\u914d-\u7ed3\u679c\u6574\u5408\u7684\u95ed\u73af\u80fd\u529b\u3002\u7cfb\u7edf\u4e2d\u901a\u5e38\u5305\u542b\u4e00\u4e2a\u4e2d\u592e\u534f\u8c03\u8005\uff0c\u7c7b\u4f3c\u6570\u5b57\u5316\u9879\u76ee\u7ecf\u7406\uff0c\u53ef\u5c06\u590d\u6742\u76ee\u6807\u62c6\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u5c06\u5176\u5206\u914d\u7ed9\u4e0d\u540c\u7684\u4e13", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.510adbca", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI5NjY0NTQ4Mw==&mid=2247485220&idx=1&sn=d04ce6664474fe9ea2b2be6678c3bdc4&chksm=eda041537f80507f774f8a0b0168e791f36810fdd9a7bd3b6dc765c8b298df8b08596b8bc7f1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI5NjY0NTQ4Mw==&mid=2247485220&idx=1&sn=d04ce6664474fe9ea2b2be6678c3bdc4&chksm=eda041537f80507f774f8a0b0168e791f36810fdd9a7bd3b6dc765c8b298df8b08596b8bc7f1#rd", "authors": ["AIPM\u4e4b\u6ce1\u6ce1\u7cd6"], "title": "<em class=\"highlight\">Agentic</em> Memory - AI Agent\u8bb0\u5fc6\u7cfb\u7edf: \u4e86\u89e3Agents\u5de5\u5177", "comment": "Source: WeChat, Published: 2025-11-18 08:12:45", "summary": "Agentic Memory\u6307AI Agent\u4e3a\u4e86\u6267\u884c\u4efb\u52a1\u3001\u7406\u89e3\u4e0a\u4e0b\u6587\u3001\u4e0e\u7528\u6237\u4fdd\u6301\u8fde\u8d2f\u4ea4\u4e92\u800c\u4f7f\u7528\u7684\u5404\u79cd\u201c\u5b58\u50a8\u673a\u5236\u201d\uff0c\u8ba9Agent\u80fd\u591f\u505a\u5230\uff1a\u7ef4\u6301\u591a\u8f6e\u5bf9\u8bdd\u8de8\u4efb\u52a1\u4fdd\u6301\u4e00\u81f4\u6027", "AI": {"tldr": "Agentic Memory\u6307AI Agent\u4e3a\u4e86\u6267\u884c\u4efb\u52a1\u3001\u7406\u89e3\u4e0a\u4e0b\u6587\u3001\u4e0e\u7528\u6237\u4fdd\u6301\u8fde\u8d2f\u4ea4\u4e92\u800c\u4f7f\u7528\u7684\u5404\u79cd\u201c\u5b58\u50a8\u673a\u5236\u201d\uff0c\u8ba9Agent\u80fd\u591f\u505a\u5230\uff1a\u7ef4\u6301\u591a\u8f6e\u5bf9\u8bdd\u8de8\u4efb\u52a1\u4fdd\u6301\u4e00\u81f4\u6027", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.d39a73e3", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUxNjA0Mjg4OA==&mid=2247485603&idx=1&sn=fa7456db03506e8bb988aa647efd49d9&chksm=f895215099d8a7afea44083f56ec9e88bd450d070b7bb5260d8b9b134723ecf90ff298b9f322#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUxNjA0Mjg4OA==&mid=2247485603&idx=1&sn=fa7456db03506e8bb988aa647efd49d9&chksm=f895215099d8a7afea44083f56ec9e88bd450d070b7bb5260d8b9b134723ecf90ff298b9f322#rd", "authors": ["\u806a\u9896\u6eaa\u6c34"], "title": "\u81ea\u4e3b\u4e4b\u5203\uff1a<em class=\"highlight\">Agentic</em> AI\u00a0\u5982\u4f55\u91cd\u5851\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u4e0e\u9632\u5fa1", "comment": "Source: WeChat, Published: 2025-11-18 08:05:43", "summary": "Agentic \u7cfb\u7edf\u901a\u5e38\u5177\u6709\u957f\u671f\u548c\u77ed\u671f\u8bb0\u5fc6\uff0c\u7528\u4e8e\u7d2f\u79ef\u7ecf\u9a8c\u548c\u4e0a\u4e0b\u6587\u3002\u653b\u51fb\u8005\u53ef\u4ee5\u6ce8\u5165\u865a\u5047\u4fe1\u606f\u5b9e\u73b0\u8bb0\u5fc6\u4e2d\u6bd2 \uff08Memory Poisoning\uff09\uff0c\u5f71\u54cd AI \u7684\u957f\u671f\u51b3\u7b56\uff0c\u5bfc\u81f4\u6301\u7eed\u6027\u3001\u9690\u853d\u6027\u7684\u6076\u610f\u884c\u4e3a\u3002", "AI": {"tldr": "Agentic \u7cfb\u7edf\u901a\u5e38\u5177\u6709\u957f\u671f\u548c\u77ed\u671f\u8bb0\u5fc6\uff0c\u7528\u4e8e\u7d2f\u79ef\u7ecf\u9a8c\u548c\u4e0a\u4e0b\u6587\u3002\u653b\u51fb\u8005\u53ef\u4ee5\u6ce8\u5165\u865a\u5047\u4fe1\u606f\u5b9e\u73b0\u8bb0\u5fc6\u4e2d\u6bd2 \uff08Memory Poisoning\uff09\uff0c\u5f71\u54cd AI \u7684\u957f\u671f\u51b3\u7b56\uff0c\u5bfc\u81f4\u6301\u7eed\u6027\u3001\u9690\u853d\u6027\u7684\u6076\u610f\u884c\u4e3a\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.1762cea8", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwNTUyNTc5Nw==&mid=2466754540&idx=1&sn=aeeff59e1b5d5846e930a2f57e16040a&chksm=8cfaddf14c6439ba43194c9d6332795f3b86321df3733dc93bea439c6772d2fe4422809cbe8f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwNTUyNTc5Nw==&mid=2466754540&idx=1&sn=aeeff59e1b5d5846e930a2f57e16040a&chksm=8cfaddf14c6439ba43194c9d6332795f3b86321df3733dc93bea439c6772d2fe4422809cbe8f#rd", "authors": ["\u4f01\u4e1a\u5927\u6a21\u578b\u5e94\u7528\u548c\u5f00\u53d1"], "title": "10-<em class=\"highlight\">Agentic</em> AI vs AI Agent\uff1a\u6df1\u5ea6\u5bf9\u6bd4\u4e24\u8005\u7684\u5f02\u540c\u70b9", "comment": "Source: WeChat, Published: 2025-11-17 23:28:38", "summary": "\u7279\u6027AI Agent\uff08AI\u667a\u80fd\u4f53/\u4ee3\u7406\uff09Agentic AI\uff08\u667a\u80fd\u4ee3\u7406AI\uff09\u5b9a\u4e49\u96c6\u6210\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5916\u90e8\u5de5\u5177\u7684\u5355\u4f53\u7cfb\u7edf\u5177\u5907\u81ea\u4e3b\u6027\u548c\u667a\u80fd\u884c\u52a8\u80fd\u529b\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u5305\u542b\u591a\u4e2a\u534f\u4f5c\u7684\u667a\u80fd\u4f53", "AI": {"tldr": "\u7279\u6027AI Agent\uff08AI\u667a\u80fd\u4f53/\u4ee3\u7406\uff09Agentic AI\uff08\u667a\u80fd\u4ee3\u7406AI\uff09\u5b9a\u4e49\u96c6\u6210\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5916\u90e8\u5de5\u5177\u7684\u5355\u4f53\u7cfb\u7edf\u5177\u5907\u81ea\u4e3b\u6027\u548c\u667a\u80fd\u884c\u52a8\u80fd\u529b\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u5305\u542b\u591a\u4e2a\u534f\u4f5c\u7684\u667a\u80fd\u4f53", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.adcd6a47", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzMjQ0ODM1Mw==&mid=2247485438&idx=1&sn=082a900b1b3be8fe4a55b4e216a2f51b&chksm=c394a181bd10463fff89df746b1c2869f114a732fc5aa11f8220ae8a1916769deb80b2e5fe0e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzMjQ0ODM1Mw==&mid=2247485438&idx=1&sn=082a900b1b3be8fe4a55b4e216a2f51b&chksm=c394a181bd10463fff89df746b1c2869f114a732fc5aa11f8220ae8a1916769deb80b2e5fe0e#rd", "authors": ["\u7a7akong\u4ea7\u54c1\u601d\u8003"], "title": "\u3010\u6765\uff0c\u770b\u62a5\u544a002\u3011<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e00\u4f53\u673a\u5e94\u7528\u7814\u7a76\u62a5\u544a2025", "comment": "Source: WeChat, Published: 2025-11-18 12:22:00", "summary": "\u300a\u5927\u6a21\u578b\u4e00\u4f53\u673a\u5e94\u7528\u7814\u7a76\u62a5\u544a\uff082025\u5e74\uff09\u300b \u5927\u6a21\u578b\u4e00\u4f53\u90fd\u6709\u4e9b\u5565\uff1f\u884c\u4e1a\u573a\u666f\u5316\u89e3\u51b3\u65b9\u6848 \u7ba1\u7406\u80fd\u529b \u5e94\u7528\u5c42 \u77e5\u8bc6\u5e93 \u667a\u80fd\u4f53 \u5de5\u4f5c\u6d41\u7f16\u6392 \u5b89\u88c5\u90e8\u7f72 \u6a21\u578b\u8bc4\u4f30 \u6a21\u578b\u7ba1\u7406 \u6a21\u578b\u5c42 \u8d44\u6e90\u7ba1\u7406 \u9884\u7f6e\u6a21\u7248\u80fd\u529b \u6a21\u578b\u5b9a\u5236\u4e0e\u4f18\u5316 \u65e5\u5fd7\u7ba1\u7406 \u6570\u636e\u5904", "AI": {"tldr": "\u300a\u5927\u6a21\u578b\u4e00\u4f53\u673a\u5e94\u7528\u7814\u7a76\u62a5\u544a\uff082025\u5e74\uff09\u300b \u5927\u6a21\u578b\u4e00\u4f53\u90fd\u6709\u4e9b\u5565\uff1f\u884c\u4e1a\u573a\u666f\u5316\u89e3\u51b3\u65b9\u6848 \u7ba1\u7406\u80fd\u529b \u5e94\u7528\u5c42 \u77e5\u8bc6\u5e93 \u667a\u80fd\u4f53 \u5de5\u4f5c\u6d41\u7f16\u6392 \u5b89\u88c5\u90e8\u7f72 \u6a21\u578b\u8bc4\u4f30 \u6a21\u578b\u7ba1\u7406 \u6a21\u578b\u5c42 \u8d44\u6e90\u7ba1\u7406 \u9884\u7f6e\u6a21\u7248\u80fd\u529b \u6a21\u578b\u5b9a\u5236\u4e0e\u4f18\u5316 \u65e5\u5fd7\u7ba1\u7406 \u6570\u636e\u5904", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.37d92705", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNjI2NjU0NA==&mid=2650975588&idx=1&sn=29d3bf97a0976c1519f832b81917b0b3&chksm=f2acf809a2dd850cc908f076993f5c182f05261c295f61b80b2915e76089fa5dfd52add6220a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNjI2NjU0NA==&mid=2650975588&idx=1&sn=29d3bf97a0976c1519f832b81917b0b3&chksm=f2acf809a2dd850cc908f076993f5c182f05261c295f61b80b2915e76089fa5dfd52add6220a#rd", "authors": ["WelcomingANNA \u517b\u8001\u673a\u5668\u4eba\u7528\u6237\u7fa4"], "title": "\u5b98\u65b9\u6d4b\u8bc4\uff1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u667a\u80fd\u4f53\u54ea\u5bb6\u5f3a\uff1fWhich Large Model Agent Platform Reigns Supreme?", "comment": "Source: WeChat, Published: 2025-11-18 12:07:00", "summary": "\u88686\uff1a\u5927\u6a21\u578b\u5de5\u4f5c\u6d41\u80fd\u529b\u8868\u73b0\u5e73\u53f0 \u7aef\u5230\u7aef\u51c6\u786e\u7387 \u53c2\u6570\u63d0\u53d6\u51c6\u786e\u7387 \u610f\u56fe\u8bc6\u522b\u51c6\u786e\u7387 \u5de5\u4f5c\u6d41\u7ed3\u675f\u5224\u65ad\u51c6\u786e\u7387\u963f\u91cc\u4e91\u767e\u70bc 69.2% 75.0% 86.7% 100.0%\u817e\u8baf\u4e91\u667a\u80fd\u4f53\u5f00\u53d1\u5e73\u53f0 69.2% 75.0% 93.3% 100.0%", "AI": {"tldr": "\u88686\uff1a\u5927\u6a21\u578b\u5de5\u4f5c\u6d41\u80fd\u529b\u8868\u73b0\u5e73\u53f0 \u7aef\u5230\u7aef\u51c6\u786e\u7387 \u53c2\u6570\u63d0\u53d6\u51c6\u786e\u7387 \u610f\u56fe\u8bc6\u522b\u51c6\u786e\u7387 \u5de5\u4f5c\u6d41\u7ed3\u675f\u5224\u65ad\u51c6\u786e\u7387\u963f\u91cc\u4e91\u767e\u70bc 69.2% 75.0% 86.7% 100.0%\u817e\u8baf\u4e91\u667a\u80fd\u4f53\u5f00\u53d1\u5e73\u53f0 69.2% 75.0% 93.3% 100.0%", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.6bfa75f1", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0ODM0NDA0Nw==&mid=2247489000&idx=1&sn=8e4c6d8b38c28b417e15f37863f410cf&chksm=c24b5c11b4f2619af168c620e2748c6eefec4dc640c7b1b5d40208634ad3d0e86b4a956df498#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0ODM0NDA0Nw==&mid=2247489000&idx=1&sn=8e4c6d8b38c28b417e15f37863f410cf&chksm=c24b5c11b4f2619af168c620e2748c6eefec4dc640c7b1b5d40208634ad3d0e86b4a956df498#rd", "authors": ["\u67b6\u6784\u8fdb\u5316\u8bba"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5f00\u53d1\u6846\u67b6\u6df1\u5ea6\u5bf9\u6bd4\uff1aSpring AI\u3001LangChain\u3001LangGraph\u4e0eLlamaIndex\u7684\u6280\u672f\u9009\u578b\u6307\u5357", "comment": "Source: WeChat, Published: 2025-11-18 11:32:15", "summary": "\u5373\u53ef\u5feb\u901f\u542f\u7528\u5927\u6a21\u578b\u529f\u80fd\u3002\u4f8b\u5982\uff0c\u8981\u96c6\u6210OpenAI\u7684\u804a\u5929\u670d\u52a1\uff0c\u53ea\u9700\u5728application.properties\u4e2d\u914d\u7f6e\uff1aspring.ai.openai.api-key=your-api-keyspring.ai.openai.chat.model=gpt-4\u7136\u540e\u5728\u4ee3\u7801\u4e2d\u6ce8\u5165ChatClient\u5373\u53ef\u4f7f\u7528\uff1a", "AI": {"tldr": "\u5373\u53ef\u5feb\u901f\u542f\u7528\u5927\u6a21\u578b\u529f\u80fd\u3002\u4f8b\u5982\uff0c\u8981\u96c6\u6210OpenAI\u7684\u804a\u5929\u670d\u52a1\uff0c\u53ea\u9700\u5728application.properties\u4e2d\u914d\u7f6e\uff1aspring.ai.openai.api-key=your-api-keyspring.ai.openai.chat.model=gpt-4\u7136\u540e\u5728\u4ee3\u7801\u4e2d\u6ce8\u5165ChatClient\u5373\u53ef\u4f7f\u7528\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.befff7a3", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247493200&idx=1&sn=840c8b482770a936eab37d91d9ca907f&chksm=fa74c2e97d61ad5da4163bf412dec34f079b6100d76e1f439c26da80c2fb2ef5547d466f1a48#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247493200&idx=1&sn=840c8b482770a936eab37d91d9ca907f&chksm=fa74c2e97d61ad5da4163bf412dec34f079b6100d76e1f439c26da80c2fb2ef5547d466f1a48#rd", "authors": ["\u6155\u5bb9\u5343\u8bed"], "title": "\u6536\u85cf\uff01LLM <em class=\"highlight\">\u5927\u6a21\u578b</em>\u5165\u95e8\u5fc5\u5b66\uff1a\u667a\u80fd\u4f53\uff08Agent\uff09\u4ece\u672c\u8d28\u5230\u5b9e\u6218\uff0c\u7a0b\u5e8f\u5458 & \u65b0\u624b\u8f7b\u677e\u62ff\u634f", "comment": "Source: WeChat, Published: 2025-11-18 11:16:00", "summary": "\u4ee5GPT\uff08Generative Pre-trained Transformer\uff09 \u4e3a\u4ee3\u8868\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u6b63\u5728\u663e\u8457\u6539\u53d8\u667a\u80fd\u4f53\u7684\u6784\u5efa\u65b9\u6cd5\u4e0e\u80fd\u529b\u8fb9\u754c\u3002\u7531\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684 LLM \u667a\u80fd\u4f53\uff0c\u5176\u6838\u5fc3\u51b3\u7b56\u673a\u5236\u4e0e\u4f20\u7edf\u667a\u80fd\u4f53\u5b58\u5728\u672c\u8d28\u533a\u522b\uff0c\u4ece\u800c\u8d4b\u4e88\u4e86\u5176\u4e00\u7cfb\u5217\u5168\u65b0\u7684\u7279\u6027\u3002", "AI": {"tldr": "\u4ee5GPT\uff08Generative Pre-trained Transformer\uff09 \u4e3a\u4ee3\u8868\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u6b63\u5728\u663e\u8457\u6539\u53d8\u667a\u80fd\u4f53\u7684\u6784\u5efa\u65b9\u6cd5\u4e0e\u80fd\u529b\u8fb9\u754c\u3002\u7531\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684 LLM \u667a\u80fd\u4f53\uff0c\u5176\u6838\u5fc3\u51b3\u7b56\u673a\u5236\u4e0e\u4f20\u7edf\u667a\u80fd\u4f53\u5b58\u5728\u672c\u8d28\u533a\u522b\uff0c\u4ece\u800c\u8d4b\u4e88\u4e86\u5176\u4e00\u7cfb\u5217\u5168\u65b0\u7684\u7279\u6027\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.d0c452df", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODA2NjA3Ng==&mid=2649713155&idx=1&sn=76a06eb860fdd7f894316dff02020504&chksm=bfd0f8018156970595f79c8f80192c2ed44631c582b8f6a2b3bb6aac296596bcfb28ebae58ce#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODA2NjA3Ng==&mid=2649713155&idx=1&sn=76a06eb860fdd7f894316dff02020504&chksm=bfd0f8018156970595f79c8f80192c2ed44631c582b8f6a2b3bb6aac296596bcfb28ebae58ce#rd", "authors": ["\u516b\u65b9\u8d44\u6e90"], "title": "\u5510\u5c71AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6392\u540d\u5b89\u88c5\u5168\u653b\u7565\uff1a\u4ece\u9009\u578b\u5230\u90e8\u7f72\u7684\u5b9e\u7528\u6307\u5357", "comment": "Source: WeChat, Published: 2025-11-18 11:03:35", "summary": "\u7136\u800c\uff0c\u9762\u5bf9\u5e02\u573a\u4e0a\u7433\u7405\u6ee1\u76ee\u7684AI\u5927\u6a21\u578b\uff0c\u5982\u4f55\u9009\u62e9\u9002\u5408\u81ea\u8eab\u9700\u6c42\u7684\u6a21\u578b\u5e76\u5b8c\u6210\u5b89\u88c5\u90e8\u7f72\uff0c\u6210\u4e3a\u8bb8\u591a\u4f01\u4e1a\u548c\u5f00\u53d1\u8005\u5173\u6ce8\u7684\u7126\u70b9\u3002\u672c\u6587\u5c06\u4ece\u6392\u540d\u4f9d\u636e\u3001\u9009\u578b\u7b56\u7565\u3001\u5b89\u88c5\u6b65\u9aa4\u53ca\u4f18\u5316\u5efa\u8bae\u56db\u4e2a\u65b9\u9762\uff0c\u4e3a\u60a8\u68b3\u7406\u5510\u5c71\u5730\u533aAI\u5927\u6a21\u578b\u5b89\u88c5\u7684\u5168\u6d41", "AI": {"tldr": "\u7136\u800c\uff0c\u9762\u5bf9\u5e02\u573a\u4e0a\u7433\u7405\u6ee1\u76ee\u7684AI\u5927\u6a21\u578b\uff0c\u5982\u4f55\u9009\u62e9\u9002\u5408\u81ea\u8eab\u9700\u6c42\u7684\u6a21\u578b\u5e76\u5b8c\u6210\u5b89\u88c5\u90e8\u7f72\uff0c\u6210\u4e3a\u8bb8\u591a\u4f01\u4e1a\u548c\u5f00\u53d1\u8005\u5173\u6ce8\u7684\u7126\u70b9\u3002\u672c\u6587\u5c06\u4ece\u6392\u540d\u4f9d\u636e\u3001\u9009\u578b\u7b56\u7565\u3001\u5b89\u88c5\u6b65\u9aa4\u53ca\u4f18\u5316\u5efa\u8bae\u56db\u4e2a\u65b9\u9762\uff0c\u4e3a\u60a8\u68b3\u7406\u5510\u5c71\u5730\u533aAI\u5927\u6a21\u578b\u5b89\u88c5\u7684\u5168\u6d41", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2511.adfacbc6", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzMDY1NDgyOQ==&mid=2247825175&idx=2&sn=68697477960eb0361f42d23ecdeef266&chksm=c39ff37b887673cd38193d83e4c5d07c4f16f6096ee17baa3a587cd04704f0ef1d5b1ab09ed1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzMDY1NDgyOQ==&mid=2247825175&idx=2&sn=68697477960eb0361f42d23ecdeef266&chksm=c39ff37b887673cd38193d83e4c5d07c4f16f6096ee17baa3a587cd04704f0ef1d5b1ab09ed1#rd", "authors": ["CSDN"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u65f6\u4ee3\uff0c\u201c\u8dd1\u6ee1\u7b97\u529b\u201d\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u96be\uff1f", "comment": "Source: WeChat, Published: 2025-11-18 10:33:43", "summary": "\u5927\u6a21\u578b\u53c2\u6570\u7a81\u7834\u4e07\u4ebf\u89c4\u6a21\u3001AI \u539f\u751f\u5e94\u7528\u7206\u53d1\u5f0f\u589e\u957f\u3001\u667a\u80fd\u4f53\u6570\u91cf\u5448\u51e0\u4f55\u7ea7\u6269\u5f20\u2014\u2014\u6240\u6709\u4eba\u90fd\u5728\u8ffd\u95ee\u540c\u4e00\u4e2a\u95ee\u9898\uff1a\u5982\u4f55\u8ba9\u7b97\u529b\u66f4\u4fbf\u5b9c\u3001\u66f4\u5feb\u3001\u66f4\u53ef\u63a7\uff1f", "AI": {"tldr": "\u5927\u6a21\u578b\u53c2\u6570\u7a81\u7834\u4e07\u4ebf\u89c4\u6a21\u3001AI \u539f\u751f\u5e94\u7528\u7206\u53d1\u5f0f\u589e\u957f\u3001\u667a\u80fd\u4f53\u6570\u91cf\u5448\u51e0\u4f55\u7ea7\u6269\u5f20\u2014\u2014\u6240\u6709\u4eba\u90fd\u5728\u8ffd\u95ee\u540c\u4e00\u4e2a\u95ee\u9898\uff1a\u5982\u4f55\u8ba9\u7b97\u529b\u66f4\u4fbf\u5b9c\u3001\u66f4\u5feb\u3001\u66f4\u53ef\u63a7\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.6851f28f", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MDI0OTgyOA==&mid=2731879183&idx=1&sn=21fac28c4dea69c47eda395813745a93&chksm=b96bbc241bbc51328420581301798be4683d15a3c75b9fd9fd259757c9473c6df6f784ddebea#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MDI0OTgyOA==&mid=2731879183&idx=1&sn=21fac28c4dea69c47eda395813745a93&chksm=b96bbc241bbc51328420581301798be4683d15a3c75b9fd9fd259757c9473c6df6f784ddebea#rd", "authors": ["\u7af9\u6e05\u52a9\u624b"], "title": "\u767d\u8bdd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u6280\u672f\u539f\u7406", "comment": "Source: WeChat, Published: 2025-11-18 09:22:44", "summary": "\u8fd9\u8ba9\u6a21\u578b\u80fd\u4ece\u4e0d\u540c\u7684\u8bed\u610f\u89d2\u5ea6\u5173\u6ce8\u8d44\u8baf\uff0c\u8fdb\u800c\u63d0\u5347\u51c6\u786e\u5ea6\u3002Masked Attention\uff08\u906e\u7f69\u6ce8\u610f\u529b\uff09\u5728Decoder \u9636\u6bb5\uff0c\u6a21\u578b\u9700\u8981\u300c\u81ea\u56de\u5f52\uff08Autoregressive\uff09\u300d\u5730\u751f\u6210\u6587\u5b57\u3002", "AI": {"tldr": "\u8fd9\u8ba9\u6a21\u578b\u80fd\u4ece\u4e0d\u540c\u7684\u8bed\u610f\u89d2\u5ea6\u5173\u6ce8\u8d44\u8baf\uff0c\u8fdb\u800c\u63d0\u5347\u51c6\u786e\u5ea6\u3002Masked Attention\uff08\u906e\u7f69\u6ce8\u610f\u529b\uff09\u5728Decoder \u9636\u6bb5\uff0c\u6a21\u578b\u9700\u8981\u300c\u81ea\u56de\u5f52\uff08Autoregressive\uff09\u300d\u5730\u751f\u6210\u6587\u5b57\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.df44c829", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5NzI5NjMwNg==&mid=2247485269&idx=1&sn=f947d1a8e711b400c156aab3e4c67947&chksm=9123be09bc4db4a7a782709e730a7fba2ec09b38d862af0bf560322eaa4ff395676da2d2d5a9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5NzI5NjMwNg==&mid=2247485269&idx=1&sn=f947d1a8e711b400c156aab3e4c67947&chksm=9123be09bc4db4a7a782709e730a7fba2ec09b38d862af0bf560322eaa4ff395676da2d2d5a9#rd", "authors": ["\u77e5\u8bc6\u79d1\u666e\u8005"], "title": "AI\u5927\u8bed\u8a00<em class=\"highlight\">\u6a21\u578b</em>\uff08LLM\uff09\u5f00\u53d1\u4e0e\u8bad\u7ec3\u5165\u95e8", "comment": "Source: WeChat, Published: 2025-11-18 01:38:32", "summary": "\u4e0b\u56fe\u662f\u4e00\u4e2a\u7b80\u5316\u7684\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u672c\u67b6\u6784\u793a\u610f\u56fe\uff1ahidden layers input layer output layer \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6a21\u578b\u67b6\u6784\u8981\u590d\u6742\u5f97\u591a\u3002\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u5e38\u91c7\u7528\u57fa\u4e8e Transformer \u7684\u67b6\u6784\uff0c\u8fd9\u79cd\u67b6\u6784\u5f7b\u5e95\u9769\u65b0\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u3002", "AI": {"tldr": "\u4e0b\u56fe\u662f\u4e00\u4e2a\u7b80\u5316\u7684\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u672c\u67b6\u6784\u793a\u610f\u56fe\uff1ahidden layers input layer output layer \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6a21\u578b\u67b6\u6784\u8981\u590d\u6742\u5f97\u591a\u3002\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u5e38\u91c7\u7528\u57fa\u4e8e Transformer \u7684\u67b6\u6784\uff0c\u8fd9\u79cd\u67b6\u6784\u5f7b\u5e95\u9769\u65b0\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
