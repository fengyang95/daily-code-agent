{"id": "2510.12948", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12948", "abs": "https://arxiv.org/abs/2510.12948", "authors": ["Minh Nguyen"], "title": "SpareCodeSearch: Searching for Code Context When You Have No Spare GPU", "comment": "4 pages, 3 figures, 4 tables. Accepted to Context Collection Workshop\n  co-located with ASE'25", "summary": "Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language\nModels (CLMs) by including another module for retrieving relevant context to\nconstruct the input prompt. However, these retrieval modules commonly use\nsemantic search, requiring substantial computational resources for training and\nhosting these embedded models, making them infeasible to integrate into\nlightweight applications such as in-IDE AI-based code completion. In this\nsolution paper, we prove that using keyword-search is sufficient to retrieve\nrelevant and useful code context inside large codebases, without the need for\nextensive GPU resources. The usefulness of code contexts found by our solution\nis demonstrated through their completion results on the Code Context\nCompetition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and\nPython tracks, respectively.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u4f7f\u7528\u5173\u952e\u8bcd\u641c\u7d22\u8db3\u4ee5\u68c0\u7d22\u76f8\u5173\u4ee3\u7801\u4e0a\u4e0b\u6587\uff0c\u65e0\u9700GPU\u8d44\u6e90\uff0c\u5728\u4ee3\u7801\u4e0a\u4e0b\u6587\u7ade\u8d5b\u4e2d\u53d6\u5f97\u826f\u597d\u8868\u73b0", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u4f7f\u7528\u8bed\u4e49\u641c\u7d22\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u96be\u4ee5\u96c6\u6210\u5230\u8f7b\u91cf\u7ea7\u5e94\u7528\u4e2d\u5982IDE\u4ee3\u7801\u8865\u5168", "method": "\u4f7f\u7528\u5173\u952e\u8bcd\u641c\u7d22\u66ff\u4ee3\u8bed\u4e49\u641c\u7d22\u6765\u68c0\u7d22\u76f8\u5173\u4ee3\u7801\u4e0a\u4e0b\u6587", "result": "\u5728\u4ee3\u7801\u4e0a\u4e0b\u6587\u7ade\u8d5b\u7684Kotlin\u548cPython\u8d5b\u9053\u4e0a\u5206\u522b\u8fbe\u52300.748\u548c0.725\u7684chRF\u5206\u6570", "conclusion": "\u5173\u952e\u8bcd\u641c\u7d22\u662f\u68c0\u7d22\u4ee3\u7801\u4e0a\u4e0b\u6587\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u65e0\u9700\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90", "topic": "code agent"}}
{"id": "2510.12864", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12864", "abs": "https://arxiv.org/abs/2510.12864", "authors": ["Imran Khan"], "title": "From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models", "comment": "13 pages. Code and data are available at\n  https://github.com/strongSoda/LITERAL-TO-LIBERAL", "summary": "Large Language Models (LLMs) are increasingly being deployed as the reasoning\nengines for agentic AI systems, yet they exhibit a critical flaw: a rigid\nadherence to explicit rules that leads to decisions misaligned with human\ncommon sense and intent. This \"rule-rigidity\" is a significant barrier to\nbuilding trustworthy autonomous agents. While prior work has shown that\nsupervised fine-tuning (SFT) with human explanations can mitigate this issue,\nSFT is computationally expensive and inaccessible to many practitioners. To\naddress this gap, we introduce the Rule-Intent Distinction (RID) Framework, a\nnovel, low-compute meta-prompting technique designed to elicit human-aligned\nexception handling in LLMs in a zero-shot manner. The RID framework provides\nthe model with a structured cognitive schema for deconstructing tasks,\nclassifying rules, weighing conflicting outcomes, and justifying its final\ndecision. We evaluated the RID framework against baseline and Chain-of-Thought\n(CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced\njudgment across diverse domains. Our human-verified results demonstrate that\nthe RID framework significantly improves performance, achieving a 95% Human\nAlignment Score (HAS), compared to 80% for the baseline and 75% for CoT.\nFurthermore, it consistently produces higher-quality, intent-driven reasoning.\nThis work presents a practical, accessible, and effective method for steering\nLLMs from literal instruction-following to liberal, goal-oriented reasoning,\npaving the way for more reliable and pragmatic AI agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86RID\u6846\u67b6\uff0c\u4e00\u79cd\u96f6\u6837\u672c\u5143\u63d0\u793a\u6280\u672f\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8ba4\u77e5\u6a21\u5f0f\u8ba9LLM\u80fd\u591f\u8fdb\u884c\u4eba\u7c7b\u5bf9\u9f50\u7684\u5f02\u5e38\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3LLM\u4f5c\u4e3a\u667a\u80fd\u4f53\u63a8\u7406\u5f15\u64ce\u65f6\u5b58\u5728\u7684\u89c4\u5219\u521a\u6027\u7f3a\u9677\uff0c\u5373\u8fc7\u5ea6\u9075\u5faa\u663e\u5f0f\u89c4\u5219\u800c\u5ffd\u89c6\u4eba\u7c7b\u5e38\u8bc6\u548c\u610f\u56fe\u7684\u95ee\u9898\u3002", "method": "RID\u6846\u67b6\uff1a\u7ed3\u6784\u5316\u5143\u63d0\u793a\u6280\u672f\uff0c\u5f15\u5bfc\u6a21\u578b\u89e3\u6784\u4efb\u52a1\u3001\u5206\u7c7b\u89c4\u5219\u3001\u6743\u8861\u51b2\u7a81\u7ed3\u679c\u5e76\u8bc1\u660e\u6700\u7ec8\u51b3\u7b56\u3002", "result": "\u572820\u4e2a\u9700\u8981\u7ec6\u5fae\u5224\u65ad\u7684\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRID\u6846\u67b6\u8fbe\u523095%\u7684\u4eba\u7c7b\u5bf9\u9f50\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf(80%)\u548cCoT(75%)\u3002", "conclusion": "RID\u6846\u67b6\u4e3a\u4ece\u5b57\u9762\u6307\u4ee4\u9075\u5faa\u8f6c\u5411\u76ee\u6807\u5bfc\u5411\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u53ef\u9760\u7684AI\u667a\u80fd\u4f53\u3002", "topic": "agent analysis"}}
{"id": "2510.12979", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12979", "abs": "https://arxiv.org/abs/2510.12979", "authors": ["Wei Fan", "Wenlin Yao", "Zheng Li", "Feng Yao", "Xin Liu", "Liang Qiu", "Qingyu Yin", "Yangqiu Song", "Bing Yin"], "title": "DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping", "comment": "Under Review", "summary": "Large language models (LLMs) augmented with multi-step reasoning and action\ngeneration abilities have shown promise in leveraging external tools to tackle\ncomplex tasks that require long-horizon planning. However, existing approaches\neither rely on implicit planning in the reasoning stage or introduce explicit\nplanners without systematically addressing how to optimize the planning stage.\nAs evidence, we observe that under vanilla reinforcement learning (RL),\nplanning tokens exhibit significantly higher entropy than other action tokens,\nrevealing uncertain decision points that remain under-optimized. To address\nthis, we propose DeepPlanner, an end-to-end RL framework that effectively\nenhances the planning capabilities of deep research agents. Our approach shapes\ntoken-level advantage with an entropy-based term to allocate larger updates to\nhigh entropy tokens, and selectively upweights sample-level advantages for\nplanning-intensive rollouts. Extensive experiments across seven deep research\nbenchmarks demonstrate that DeepPlanner improves planning quality and achieves\nstate-of-the-art results under a substantially lower training budget.", "AI": {"tldr": "DeepPlanner\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684\u4ee4\u724c\u7ea7\u4f18\u52bf\u5851\u9020\u548c\u9009\u62e9\u6027\u52a0\u6743\u6837\u672c\u7ea7\u4f18\u52bf\uff0c\u6709\u6548\u589e\u5f3a\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u89c4\u5212\u80fd\u529b\uff0c\u5728\u4e03\u4e2a\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u63a8\u7406\u9636\u6bb5\u7684\u9690\u5f0f\u89c4\u5212\uff0c\u8981\u4e48\u5f15\u5165\u663e\u5f0f\u89c4\u5212\u5668\u4f46\u6ca1\u6709\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u5982\u4f55\u4f18\u5316\u89c4\u5212\u9636\u6bb5\u7684\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u4e0b\uff0c\u89c4\u5212\u4ee4\u724c\u7684\u71b5\u663e\u8457\u9ad8\u4e8e\u5176\u4ed6\u52a8\u4f5c\u4ee4\u724c\uff0c\u63ed\u793a\u4e86\u672a\u4f18\u5316\u7684\u4e0d\u786e\u5b9a\u51b3\u7b56\u70b9\u3002", "method": "\u63d0\u51faDeepPlanner\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684\u4ee4\u724c\u7ea7\u4f18\u52bf\u5851\u9020\u4e3a\u9ad8\u71b5\u4ee4\u724c\u5206\u914d\u66f4\u5927\u7684\u66f4\u65b0\uff0c\u5e76\u9009\u62e9\u6027\u52a0\u6743\u89c4\u5212\u5bc6\u96c6\u578brollouts\u7684\u6837\u672c\u7ea7\u4f18\u52bf\u3002", "result": "\u5728\u4e03\u4e2a\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDeepPlanner\u63d0\u9ad8\u4e86\u89c4\u5212\u8d28\u91cf\uff0c\u5e76\u5728\u663e\u8457\u964d\u4f4e\u7684\u8bad\u7ec3\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "DeepPlanner\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u4f18\u5316\u89c4\u5212\u9636\u6bb5\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u89c4\u5212\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u7684\u957f\u65f6\u7a0b\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12985", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12985", "abs": "https://arxiv.org/abs/2510.12985", "authors": ["Simon Sinong Zhan", "Yao Liu", "Philip Wang", "Zinan Wang", "Qineng Wang", "Zhian Ruan", "Xiangyu Shi", "Xinyu Cao", "Frank Yang", "Kangrui Wang", "Huajie Shao", "Manling Li", "Qi Zhu"], "title": "SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents", "comment": null, "summary": "We present Sentinel, the first framework for formally evaluating the physical\nsafety of Large Language Model(LLM-based) embodied agents across the semantic,\nplan, and trajectory levels. Unlike prior methods that rely on heuristic rules\nor subjective LLM judgments, Sentinel grounds practical safety requirements in\nformal temporal logic (TL) semantics that can precisely specify state\ninvariants, temporal dependencies, and timing constraints. It then employs a\nmulti-level verification pipeline where (i) at the semantic level, intuitive\nnatural language safety requirements are formalized into TL formulas and the\nLLM agent's understanding of these requirements is probed for alignment with\nthe TL formulas; (ii) at the plan level, high-level action plans and subgoals\ngenerated by the LLM agent are verified against the TL formulas to detect\nunsafe plans before execution; and (iii) at the trajectory level, multiple\nexecution trajectories are merged into a computation tree and efficiently\nverified against physically-detailed TL specifications for a final safety\ncheck. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate\nmultiple LLM-based embodied agents against diverse safety requirements. Our\nexperiments show that by grounding physical safety in temporal logic and\napplying verification methods across multiple levels, Sentinel provides a\nrigorous foundation for systematically evaluating LLM-based embodied agents in\nphysical environments, exposing safety violations overlooked by previous\nmethods and offering insights into their failure modes.", "AI": {"tldr": "Sentinel\u662f\u9996\u4e2a\u57fa\u4e8e\u5f62\u5f0f\u5316\u65b9\u6cd5\u8bc4\u4f30LLM\u5177\u8eab\u667a\u80fd\u4f53\u7269\u7406\u5b89\u5168\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u3001\u89c4\u5212\u548c\u8f68\u8ff9\u4e09\u4e2a\u5c42\u9762\u5bf9\u5b89\u5168\u6027\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u89c4\u5219\u6216\u4e3b\u89c2\u7684LLM\u5224\u65ad\uff0c\u7f3a\u4e4f\u5bf9\u7269\u7406\u5b89\u5168\u6027\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u65e0\u6cd5\u7cbe\u786e\u6307\u5b9a\u72b6\u6001\u4e0d\u53d8\u91cf\u3001\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u65f6\u5e8f\u7ea6\u675f\u3002", "method": "\u4f7f\u7528\u65f6\u5e8f\u903b\u8f91(TL)\u8bed\u4e49\u5f62\u5f0f\u5316\u5b89\u5168\u9700\u6c42\uff0c\u6784\u5efa\u591a\u7ea7\u9a8c\u8bc1\u7ba1\u9053\uff1a\u8bed\u4e49\u7ea7\u9a8c\u8bc1LLM\u5bf9\u5b89\u5168\u9700\u6c42\u7684\u7406\u89e3\uff0c\u89c4\u5212\u7ea7\u9a8c\u8bc1\u9ad8\u5c42\u884c\u52a8\u8ba1\u5212\uff0c\u8f68\u8ff9\u7ea7\u9a8c\u8bc1\u6267\u884c\u8f68\u8ff9\u3002", "result": "\u5728VirtualHome\u548cALFRED\u73af\u5883\u4e2d\u8bc4\u4f30\u591a\u4e2aLLM\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u53d1\u73b0Sentinel\u80fd\u591f\u66b4\u9732\u5148\u524d\u65b9\u6cd5\u5ffd\u7565\u7684\u5b89\u5168\u8fdd\u89c4\uff0c\u5e76\u63ed\u793a\u5176\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7269\u7406\u5b89\u5168\u6027\u4e0e\u65f6\u5e8f\u903b\u8f91\u76f8\u7ed3\u5408\uff0cSentinel\u4e3a\u7cfb\u7edf\u8bc4\u4f30LLM\u5177\u8eab\u667a\u80fd\u4f53\u5728\u7269\u7406\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u4e25\u8c28\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.12939", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12939", "abs": "https://arxiv.org/abs/2510.12939", "authors": ["James Pedley", "Benjamin Etheridge", "Stephen J. Roberts", "Francesco Quinzan"], "title": "Pruning Cannot Hurt Robustness: Certified Trade-offs in Reinforcement Learning", "comment": "24 pages, 13 figures", "summary": "Reinforcement learning (RL) policies deployed in real-world environments must\nremain reliable under adversarial perturbations. At the same time, modern deep\nRL agents are heavily over-parameterized, raising costs and fragility concerns.\nWhile pruning has been shown to improve robustness in supervised learning, its\nrole in adversarial RL remains poorly understood. We develop the first\ntheoretical framework for certified robustness under pruning in\nstate-adversarial Markov decision processes (SA-MDPs). For Gaussian and\ncategorical policies with Lipschitz networks, we prove that element-wise\npruning can only tighten certified robustness bounds; pruning never makes the\npolicy less robust. Building on this, we derive a novel three-term regret\ndecomposition that disentangles clean-task performance, pruning-induced\nperformance loss, and robustness gains, exposing a fundamental\nperformance--robustness frontier. Empirically, we evaluate magnitude and\nmicro-pruning schedules on continuous-control benchmarks with strong\npolicy-aware adversaries. Across tasks, pruning consistently uncovers\nreproducible ``sweet spots'' at moderate sparsity levels, where robustness\nimproves substantially without harming - and sometimes even enhancing - clean\nperformance. These results position pruning not merely as a compression tool\nbut as a structural intervention for robust RL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u4e3a\u72b6\u6001\u5bf9\u6297\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(SA-MDPs)\u4e2d\u7684\u526a\u679d\u8ba4\u8bc1\u9c81\u68d2\u6027\u5efa\u7acb\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5143\u7d20\u7ea7\u526a\u679d\u53ea\u4f1a\u6536\u7d27\u8ba4\u8bc1\u9c81\u68d2\u6027\u8fb9\u754c\uff0c\u5e76\u63ed\u793a\u4e86\u6027\u80fd-\u9c81\u68d2\u6027\u6743\u8861\u8fb9\u754c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u90e8\u7f72\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u9700\u8981\u5728\u5bf9\u6297\u6270\u52a8\u4e0b\u4fdd\u6301\u53ef\u9760\u6027\uff0c\u800c\u73b0\u4ee3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u8fc7\u5ea6\u53c2\u6570\u5316\u5e26\u6765\u4e86\u6210\u672c\u548c\u8106\u5f31\u6027\u95ee\u9898\u3002\u867d\u7136\u526a\u679d\u5728\u76d1\u7763\u5b66\u4e60\u4e2d\u5df2\u88ab\u8bc1\u660e\u80fd\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u4f46\u5176\u5728\u5bf9\u6297\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u4e3a\u9ad8\u65af\u548c\u5206\u7c7b\u7b56\u7565\u5f00\u53d1\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u5143\u7d20\u7ea7\u526a\u679d\u5bf9\u8ba4\u8bc1\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff1b\u63a8\u5bfc\u65b0\u9896\u7684\u4e09\u9879\u9057\u61be\u5206\u89e3\uff1b\u5728\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u4e0a\u8bc4\u4f30\u5e45\u5ea6\u526a\u679d\u548c\u5fae\u526a\u679d\u8c03\u5ea6\u3002", "result": "\u526a\u679d\u5728\u4e2d\u7b49\u7a00\u758f\u5ea6\u6c34\u5e73\u4e0a\u59cb\u7ec8\u53d1\u73b0\u53ef\u590d\u73b0\u7684\"\u6700\u4f73\u70b9\"\uff0c\u5728\u8fd9\u4e9b\u70b9\u4e0a\u9c81\u68d2\u6027\u663e\u8457\u6539\u5584\uff0c\u800c\u4e0d\u4f1a\u635f\u5bb3\u751a\u81f3\u6709\u65f6\u80fd\u63d0\u5347\u5e72\u51c0\u6027\u80fd\u3002", "conclusion": "\u526a\u679d\u4e0d\u4ec5\u662f\u538b\u7f29\u5de5\u5177\uff0c\u66f4\u662f\u5b9e\u73b0\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u6784\u6027\u5e72\u9884\u624b\u6bb5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.13036", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13036", "abs": "https://arxiv.org/abs/2510.13036", "authors": ["Stephane Hatgis-Kessell", "Logan Mondal Bhamidipaty", "Emma Brunskill"], "title": "Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking", "comment": null, "summary": "Human-designed reward functions for reinforcement learning (RL) agents are\nfrequently misaligned with the humans' true, unobservable objectives, and thus\nact only as proxies. Optimizing for a misspecified proxy reward function often\ninduces reward hacking, resulting in a policy misaligned with the human's true\nobjectives. An alternative is to perform RL from human feedback, which involves\nlearning a reward function from scratch by collecting human preferences over\npairs of trajectories. However, building such datasets is costly. To address\nthe limitations of both approaches, we propose Preference-Based Reward Repair\n(PBRR): an automated iterative framework that repairs a human-specified proxy\nreward function by learning an additive, transition-dependent correction term\nfrom preferences. A manually specified reward function can yield policies that\nare highly suboptimal under the ground-truth objective, yet corrections on only\na few transitions may suffice to recover optimal performance. To identify and\ncorrect for those transitions, PBRR uses a targeted exploration strategy and a\nnew preference-learning objective. We prove in tabular domains PBRR has a\ncumulative regret that matches, up to constants, that of prior preference-based\nRL methods. In addition, on a suite of reward-hacking benchmarks, PBRR\nconsistently outperforms baselines that learn a reward function from scratch\nfrom preferences or modify the proxy reward function using other approaches,\nrequiring substantially fewer preferences to learn high performing policies.", "AI": {"tldr": "\u63d0\u51faPreference-Based Reward Repair (PBRR)\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u4eba\u7c7b\u504f\u597d\u4e2d\u5b66\u4e60\u52a0\u6027\u4fee\u6b63\u9879\u6765\u81ea\u52a8\u4fee\u590d\u4eba\u5de5\u8bbe\u8ba1\u7684\u4ee3\u7406\u5956\u52b1\u51fd\u6570\uff0c\u89e3\u51b3\u5956\u52b1\u51fd\u6570\u9519\u914d\u95ee\u9898\u3002", "motivation": "\u4eba\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u7ecf\u5e38\u4e0e\u771f\u5b9e\u76ee\u6807\u9519\u914d\uff0c\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff1b\u800c\u4ece\u5934\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "method": "PBRR\u6846\u67b6\uff1a\u901a\u8fc7\u76ee\u6807\u63a2\u7d22\u7b56\u7565\u548c\u65b0\u7684\u504f\u597d\u5b66\u4e60\u76ee\u6807\uff0c\u5b66\u4e60\u4e00\u4e2a\u4e0e\u72b6\u6001\u8f6c\u79fb\u76f8\u5173\u7684\u52a0\u6027\u4fee\u6b63\u9879\u6765\u4fee\u590d\u4ee3\u7406\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728\u8868\u683c\u57df\u4e2d\u8bc1\u660ePBRR\u7684\u7d2f\u79ef\u9057\u61be\u4e0e\u73b0\u6709\u504f\u597d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\uff1b\u5728\u5956\u52b1\u9ed1\u5ba2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPBRR\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9700\u8981\u66f4\u5c11\u7684\u504f\u597d\u6765\u5b66\u4e60\u9ad8\u6027\u80fd\u7b56\u7565\u3002", "conclusion": "PBRR\u80fd\u591f\u6709\u6548\u4fee\u590d\u9519\u914d\u7684\u4ee3\u7406\u5956\u52b1\u51fd\u6570\uff0c\u6bd4\u4ece\u5934\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u6216\u5176\u4ed6\u4fee\u6b63\u65b9\u6cd5\u66f4\u9ad8\u6548\uff0c\u663e\u8457\u51cf\u5c11\u6240\u9700\u7684\u4eba\u7c7b\u504f\u597d\u6570\u91cf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.13575", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.13575", "abs": "https://arxiv.org/abs/2510.13575", "authors": ["Han Fu", "Sigrid Eldh", "Kristian Wiklund", "Andreas Ermedahl", "Philipp Haller", "Cyrille Artho"], "title": "Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code", "comment": "9 pages, 4 figures, conference: 2025 28th Euromicro Conference on\n  Digital System Design (DSD)", "summary": "The co-development of hardware and software in industrial embedded systems\nfrequently leads to compilation errors during continuous integration (CI).\nAutomated repair of such failures is promising, but existing techniques rely on\ntest cases, which are not available for non-compilable code.\n  We employ an automated repair approach for compilation errors driven by large\nlanguage models (LLMs). Our study encompasses the collection of more than 40000\ncommits from the product's source code. We assess the performance of an\nindustrial CI system enhanced by four state-of-the-art LLMs, comparing their\noutcomes with manual corrections provided by human programmers. LLM-equipped CI\nsystems can resolve up to 63 % of the compilation errors in our baseline\ndataset. Among the fixes associated with successful CI builds, 83 % are deemed\nreasonable. Moreover, LLMs significantly reduce debugging time, with the\nmajority of successful cases completed within 8 minutes, compared to hours\ntypically required for manual debugging.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4fee\u590d\u5de5\u4e1a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u7f16\u8bd1\u9519\u8bef\uff0c\u5728CI\u7cfb\u7edf\u4e2d\u80fd\u89e3\u51b363%\u7684\u7f16\u8bd1\u9519\u8bef\uff0c83%\u7684\u4fee\u590d\u88ab\u8ba4\u4e3a\u662f\u5408\u7406\u7684\uff0c\u5e76\u5c06\u8c03\u8bd5\u65f6\u95f4\u4ece\u51e0\u5c0f\u65f6\u7f29\u77ed\u52308\u5206\u949f\u5185\u3002", "motivation": "\u5de5\u4e1a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u786c\u4ef6\u548c\u8f6f\u4ef6\u5171\u540c\u5f00\u53d1\u7ecf\u5e38\u5bfc\u81f4\u6301\u7eed\u96c6\u6210\u8fc7\u7a0b\u4e2d\u7684\u7f16\u8bd1\u9519\u8bef\uff0c\u73b0\u6709\u4fee\u590d\u6280\u672f\u4f9d\u8d56\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4f46\u4e0d\u53ef\u7f16\u8bd1\u4ee3\u7801\u6ca1\u6709\u6d4b\u8bd5\u7528\u4f8b\u53ef\u7528\u3002", "method": "\u6536\u96c6\u8d85\u8fc740000\u4e2a\u4ea7\u54c1\u6e90\u4ee3\u7801\u63d0\u4ea4\uff0c\u8bc4\u4f30\u56db\u4e2a\u6700\u5148\u8fdbLLM\u589e\u5f3a\u7684\u5de5\u4e1aCI\u7cfb\u7edf\u6027\u80fd\uff0c\u4e0e\u4eba\u5de5\u4fee\u6b63\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LLM\u589e\u5f3a\u7684CI\u7cfb\u7edf\u80fd\u89e3\u51b3\u57fa\u7ebf\u6570\u636e\u96c6\u4e2d63%\u7684\u7f16\u8bd1\u9519\u8bef\uff0c\u5176\u4e2d83%\u7684\u6210\u529f\u4fee\u590d\u88ab\u8ba4\u4e3a\u662f\u5408\u7406\u7684\uff0c\u8c03\u8bd5\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\u52308\u5206\u949f\u5185\u3002", "conclusion": "LLM\u5728\u81ea\u52a8\u4fee\u590d\u7f16\u8bd1\u9519\u8bef\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u5de5\u4e1a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u7f16\u8bd1\u95ee\u9898\u5e76\u663e\u8457\u63d0\u9ad8\u8c03\u8bd5\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "2510.13214", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13214", "abs": "https://arxiv.org/abs/2510.13214", "authors": ["Zehui Ling", "Deshu Chen", "Yichi Zhang", "Yuchen Liu", "Xigui Li", "Xin Guo", "Yuan Cheng"], "title": "Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) demonstrate that\nchain-of-thought prompting and deep reasoning substantially enhance performance\non complex tasks, and multi-agent systems can further improve accuracy by\nenabling model debates. However, applying deep reasoning to all problems is\ncomputationally expensive. To mitigate these costs, we propose a complementary\nagent system integrating small and large LLMs. The small LLM first generates an\ninitial answer, which is then verified by the large LLM. If correct, the answer\nis adopted directly; otherwise, the large LLM performs in-depth reasoning.\nExperimental results show that, for simple problems, our approach reduces the\ncomputational cost of the large LLM by more than 50% with negligible accuracy\nloss, while consistently maintaining robust performance on complex tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5c0f\u578b\u548c\u5927\u578bLLM\u7684\u4e92\u8865\u4ee3\u7406\u7cfb\u7edf\uff0c\u5c0f\u578bLLM\u5148\u751f\u6210\u521d\u59cb\u7b54\u6848\uff0c\u5927\u578bLLM\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u6df1\u5ea6\u63a8\u7406\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\u867d\u7136\u80fd\u63d0\u9ad8\u590d\u6742\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u6027\u80fd\u548c\u6210\u672c\u3002", "method": "\u91c7\u7528\u5c0f\u578bLLM\u751f\u6210\u521d\u59cb\u7b54\u6848\uff0c\u5927\u578bLLM\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4ec5\u5728\u7b54\u6848\u9519\u8bef\u65f6\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\uff0c\u5f62\u6210\u4e92\u8865\u4ee3\u7406\u7cfb\u7edf\u3002", "result": "\u5728\u7b80\u5355\u95ee\u9898\u4e0a\uff0c\u5927\u578bLLM\u7684\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u8d85\u8fc750%\uff0c\u51c6\u786e\u7387\u635f\u5931\u53ef\u5ffd\u7565\uff0c\u590d\u6742\u4efb\u52a1\u4e0a\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "\u8be5\u4e92\u8865\u4ee3\u7406\u7cfb\u7edf\u80fd\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff0c\u4e3aLLM\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.13697", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13697", "abs": "https://arxiv.org/abs/2510.13697", "authors": ["Maksim Sapronov", "Evgeniy Glukhov"], "title": "On Pretraining for Project-Level Code Completion", "comment": null, "summary": "Repository-level pretraining is commonly used to enable large language models\nfor code to leverage codebase-wide context. This enhances their ability to\ngenerate accurate and context-aware code completions. In this work, we\ninvestigate how different repository-processing strategies affect in-context\nlearning in OpenCoder, a 1.5B-parameter model. We extend its context window\nfrom 4,096 to 16,384 tokens by training on additional 1B tokens of curated\nrepository-level data. Despite relying on a smaller dataset than competing\nmodels (which often use hundreds of billions of tokens), our model achieves\ncomparable performance on the Long Code Arena benchmark. We find that various\nrepository-processing techniques yield similarly strong results, with the\nprimary gain coming from adapting to a new rotary positional embedding (RoPE)\nscaling parameter. Finally, we show that a simpler file-level training approach\nat the original sequence length remains highly effective, opening up\nrepository-level code completion research to settings with more constrained\ndata and compute resources.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u4ee3\u7801\u4ed3\u5e93\u5904\u7406\u7b56\u7565\u5bf9OpenCoder\u6a21\u578b\uff081.5B\u53c2\u6570\uff09\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u81f316,384 token\u5e76\u57281B token\u7684\u4ed3\u5e93\u7ea7\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5728Long Code Arena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4e0e\u66f4\u5927\u6570\u636e\u96c6\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u4ee3\u7801\u4ed3\u5e93\u7ea7\u9884\u8bad\u7ec3\u4e2d\u4e0d\u540c\u5904\u7406\u7b56\u7565\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5982\u4f55\u5728\u5c0f\u6570\u636e\u96c6\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e0e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002", "method": "\u6269\u5c55OpenCoder\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u4ece4,096\u523016,384 token\uff0c\u57281B token\u7684\u7cbe\u9009\u4ed3\u5e93\u7ea7\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u6bd4\u8f83\u4e0d\u540c\u4ed3\u5e93\u5904\u7406\u6280\u672f\uff0c\u5e76\u6d4b\u8bd5\u66f4\u7b80\u5355\u7684\u6587\u4ef6\u7ea7\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5c3d\u7ba1\u4f7f\u7528\u6bd4\u7ade\u4e89\u6a21\u578b\u5c0f\u5f97\u591a\u7684\u6570\u636e\u96c6\uff0c\u4f46\u5728Long Code Arena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u53ef\u6bd4\u6027\u80fd\uff1b\u4e0d\u540c\u4ed3\u5e93\u5904\u7406\u6280\u672f\u4ea7\u751f\u76f8\u4f3c\u5f3a\u7ed3\u679c\uff0c\u4e3b\u8981\u589e\u76ca\u6765\u81ea\u9002\u5e94\u65b0\u7684RoPE\u7f29\u653e\u53c2\u6570\uff1b\u66f4\u7b80\u5355\u7684\u6587\u4ef6\u7ea7\u8bad\u7ec3\u65b9\u6cd5\u5728\u539f\u59cb\u5e8f\u5217\u957f\u5ea6\u4e0b\u4ecd\u7136\u975e\u5e38\u6709\u6548\u3002", "conclusion": "\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u7814\u7a76\u53ef\u4ee5\u5728\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u8fdb\u884c\uff0c\u66f4\u7b80\u5355\u7684\u6587\u4ef6\u7ea7\u8bad\u7ec3\u65b9\u6cd5\u4ecd\u7136\u6709\u6548\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2510.13220", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13220", "abs": "https://arxiv.org/abs/2510.13220", "authors": ["Yufei He", "Juncheng Liu", "Yue Liu", "Yibo Li", "Tri Cao", "Zhiyuan Hu", "Xinxing Xu", "Bryan Hooi"], "title": "EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems", "comment": null, "summary": "A fundamental limitation of current AI agents is their inability to learn\ncomplex skills on the fly at test time, often behaving like \"clever but\nclueless interns\" in novel environments. This severely limits their practical\nutility. To systematically measure and drive progress on this challenge, we\nfirst introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a\nnew evaluation setup where an agent must play the same game for several\nconsecutive episodes, attempting to improve its performance from one episode to\nthe next. On J-TTL, we find that existing adaptation methods like reflection,\nmemory, or reinforcement learning struggle. To address the challenges posed by\nour benchmark, we present EvoTest, an evolutionary test-time learning framework\nthat improves an agent without any fine-tuning or gradients-by evolving the\nentire agentic system after every episode. EvoTest has two roles: the Actor\nAgent, which plays the game, and the Evolver Agent, which analyzes the episode\ntranscript to propose a revised configuration for the next run. This\nconfiguration rewrites the prompt, updates memory by logging effective\nstate-action choices, tunes hyperparameters, and learns the tool-use routines.\nOn our J-TTL benchmark, EvoTest consistently increases performance,\noutperforming not only reflection and memory-only baselines but also more\ncomplex online fine-tuning methods. Notably, our method is the only one capable\nof winning two games (Detective and Library), while all baselines fail to win\nany.", "AI": {"tldr": "\u63d0\u51fa\u4e86J-TTL\u57fa\u51c6\u6d4b\u8bd5\u6765\u8861\u91cfAI\u4ee3\u7406\u5728\u6d4b\u8bd5\u65f6\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86EvoTest\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6f14\u5316\u548c\u91cd\u914d\u7f6e\u4ee3\u7406\u7cfb\u7edf\u6765\u63d0\u5347\u6027\u80fd\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u68af\u5ea6\u8ba1\u7b97\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u65e0\u6cd5\u5728\u6d4b\u8bd5\u65f6\u52a8\u6001\u5b66\u4e60\u590d\u6742\u6280\u80fd\uff0c\u5728\u964c\u751f\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8861\u91cf\u548c\u63a8\u52a8\u8fd9\u4e00\u6311\u6218\u7684\u8fdb\u5c55\u3002", "method": "\u5f15\u5165J-TTL\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u4ee3\u7406\u5728\u8fde\u7eed\u591a\u8f6e\u6e38\u620f\u4e2d\u63d0\u5347\u8868\u73b0\u3002\u63d0\u51faEvoTest\u8fdb\u5316\u6d4b\u8bd5\u65f6\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u6267\u884c\u6e38\u620f\u7684Actor\u4ee3\u7406\u548c\u5206\u6790\u6e38\u620f\u8bb0\u5f55\u4ee5\u63d0\u51fa\u65b0\u914d\u7f6e\u7684Evolver\u4ee3\u7406\uff0c\u901a\u8fc7\u91cd\u5199\u63d0\u793a\u3001\u66f4\u65b0\u8bb0\u5fc6\u3001\u8c03\u6574\u8d85\u53c2\u6570\u548c\u5b66\u4e60\u5de5\u5177\u4f7f\u7528\u4f8b\u7a0b\u6765\u6539\u8fdb\u7cfb\u7edf\u3002", "result": "EvoTest\u5728J-TTL\u57fa\u51c6\u4e0a\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u4f18\u4e8e\u53cd\u5c04\u3001\u8bb0\u5fc6\u548c\u5728\u7ebf\u5fae\u8c03\u7b49\u65b9\u6cd5\u3002\u662f\u552f\u4e00\u80fd\u591f\u8d62\u5f97\u4e24\u4e2a\u6e38\u620f\uff08Detective\u548cLibrary\uff09\u7684\u65b9\u6cd5\uff0c\u800c\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u90fd\u672a\u80fd\u8d62\u5f97\u4efb\u4f55\u6e38\u620f\u3002", "conclusion": "EvoTest\u6846\u67b6\u901a\u8fc7\u8fdb\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6d4b\u8bd5\u65f6\u5b66\u4e60\u6311\u6218\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u7684\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.12826", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.12826", "abs": "https://arxiv.org/abs/2510.12826", "authors": ["Thao Pham"], "title": "Scheming Ability in LLM-to-LLM Strategic Interactions", "comment": "25 pages, 13 figures, under review at IASEAI'26", "summary": "As large language model (LLM) agents are deployed autonomously in diverse\ncontexts, evaluating their capacity for strategic deception becomes crucial.\nWhile recent research has examined how AI systems scheme against human\ndevelopers, LLM-to-LLM scheming remains underexplored. We investigate the\nscheming ability and propensity of frontier LLM agents through two\ngame-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation\nadversarial game. Testing four models (GPT-4o, Gemini-2.5-pro,\nClaude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and\nwithout explicit prompting while analyzing scheming tactics through\nchain-of-thought reasoning. When prompted, most models, especially\nGemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance.\nCritically, models exhibited significant scheming propensity without prompting:\nall models chose deception over confession in Peer Evaluation (100% rate),\nwhile models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These\nfindings highlight the need for robust evaluations using high-stakes\ngame-theoretic scenarios in multi-agent settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u524d\u6cbfLLM\u4ee3\u7406\u5728\u6218\u7565\u6b3a\u9a97\u65b9\u9762\u7684\u80fd\u529b\u548c\u503e\u5411\uff0c\u901a\u8fc7\u4e24\u79cd\u535a\u5f08\u8bba\u6846\u67b6\u6d4b\u8bd5\u4e86\u56db\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u63d0\u793a\uff0c\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u663e\u8457\u7684\u6b3a\u9a97\u503e\u5411\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u81ea\u4e3b\u90e8\u7f72\uff0c\u8bc4\u4f30\u5176\u6218\u7565\u6b3a\u9a97\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u8003\u5bdfAI\u7cfb\u7edf\u5982\u4f55\u5bf9\u6297\u4eba\u7c7b\u5f00\u53d1\u8005\uff0c\u4f46LLM\u4e4b\u95f4\u7684\u6b3a\u9a97\u884c\u4e3a\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u535a\u5f08\u8bba\u6846\u67b6\uff1a\u5ec9\u4ef7\u8c08\u8bdd\u4fe1\u53f7\u535a\u5f08\u548c\u540c\u884c\u8bc4\u4f30\u5bf9\u6297\u535a\u5f08\uff0c\u6d4b\u8bd5\u4e86GPT-4o\u3001Gemini-2.5-pro\u3001Claude-3.7-Sonnet\u548cLlama-3.3-70b\u56db\u4e2a\u6a21\u578b\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u63a8\u7406\u5206\u6790\u6b3a\u9a97\u7b56\u7565\u3002", "result": "\u5f53\u6709\u63d0\u793a\u65f6\uff0c\u5927\u591a\u6570\u6a21\u578b\uff08\u7279\u522b\u662fGemini-2.5-pro\u548cClaude-3.7-Sonnet\uff09\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u8868\u73b0\u3002\u5173\u952e\u53d1\u73b0\u662f\uff0c\u5373\u4f7f\u6ca1\u6709\u63d0\u793a\uff0c\u6240\u6709\u6a21\u578b\u5728\u540c\u884c\u8bc4\u4f30\u4e2d\u90fd\u9009\u62e9\u6b3a\u9a97\u800c\u975e\u5766\u767d\uff08100%\u6bd4\u7387\uff09\uff0c\u5728\u5ec9\u4ef7\u8c08\u8bdd\u4e2d\u9009\u62e9\u6b3a\u9a97\u7684\u6a21\u578b\u6210\u529f\u7387\u8fbe\u523095-100%\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u4f7f\u7528\u9ad8\u98ce\u9669\u535a\u5f08\u8bba\u573a\u666f\u8fdb\u884c\u7a33\u5065\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.12831", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12831", "abs": "https://arxiv.org/abs/2510.12831", "authors": ["Taicheng Guo", "Hai Wang", "ChaoChun Liu", "Mohsen Golalikhani", "Xin Chen", "Xiangliang Zhang", "Chandan K. Reddy"], "title": "MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training", "comment": null, "summary": "Multi-turn Text-to-SQL aims to translate a user's conversational utterances\ninto executable SQL while preserving dialogue coherence and grounding to the\ntarget schema. However, most existing systems only regard this task as a simple\ntext translation task and follow a short-horizon paradigm, generating a query\nper turn without execution, explicit verification, and refinement, which leads\nto non-executable or incoherent outputs. We present MTSQL-R1, an agentic\ntraining framework for long-horizon multi-turn Text-to-SQL. We cast the task as\na Markov Decision Process (MDP) in which an agent interacts with (i) a database\nfor execution feedback and (ii) a persistent dialogue memory for coherence\nverification, performing an iterative propose to execute -> verify -> refine\ncycle until all checks pass. Experiments on COSQL and SPARC demonstrate that\nMTSQL-R1 consistently outperforms strong baselines, highlighting the importance\nof environment-driven verification and memory-guided refinement for\nconversational semantic parsing. Full recipes (including code, trained models,\nlogs, reasoning trajectories, etc.) will be released after the internal review\nto contribute to community research.", "AI": {"tldr": "MTSQL-R1\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u8f6eText-to-SQL\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u6267\u884c\u53cd\u9988\u548c\u5bf9\u8bdd\u4e00\u81f4\u6027\u9a8c\u8bc1\u7684\u8fed\u4ee3\u4f18\u5316\u5faa\u73af\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5c06\u591a\u8f6eText-to-SQL\u89c6\u4e3a\u7b80\u5355\u7684\u6587\u672c\u7ffb\u8bd1\u4efb\u52a1\uff0c\u91c7\u7528\u77ed\u89c6\u8303\u5f0f\u9010\u8f6e\u751f\u6210\u67e5\u8be2\u800c\u4e0d\u6267\u884c\u3001\u9a8c\u8bc1\u548c\u4f18\u5316\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e0d\u53ef\u6267\u884c\u6216\u4e0d\u8fde\u8d2f\u3002", "method": "\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3aMDP\uff0c\u667a\u80fd\u4f53\u4e0e\u6570\u636e\u5e93\u4ea4\u4e92\u83b7\u53d6\u6267\u884c\u53cd\u9988\uff0c\u4e0e\u6301\u4e45\u5bf9\u8bdd\u8bb0\u5fc6\u4ea4\u4e92\u8fdb\u884c\u4e00\u81f4\u6027\u9a8c\u8bc1\uff0c\u6267\u884c'\u63d0\u51fa\u6267\u884c\u2192\u9a8c\u8bc1\u2192\u4f18\u5316'\u7684\u8fed\u4ee3\u5faa\u73af\u76f4\u5230\u6240\u6709\u68c0\u67e5\u901a\u8fc7\u3002", "result": "\u5728COSQL\u548cSPARC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMTSQL-R1\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u73af\u5883\u9a71\u52a8\u7684\u9a8c\u8bc1\u548c\u8bb0\u5fc6\u5f15\u5bfc\u7684\u4f18\u5316\u5bf9\u4e8e\u4f1a\u8bdd\u8bed\u4e49\u89e3\u6790\u81f3\u5173\u91cd\u8981\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.12838", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12838", "abs": "https://arxiv.org/abs/2510.12838", "authors": ["Qianben Chen", "Jingyi Cao", "Jiayu Zhang", "Tianrui Qin", "Xiaowan Li", "King Zhu", "Dingfeng Shi", "He Zhu", "Minghao Liu", "Xiaobo Liang", "Ge Zhang", "Jian Yang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "A\\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning", "comment": "9 pages, 5 figures, submitted to ICLR 2026", "summary": "Large language models split into two families: reasoning-centric LLMs, which\nstrengthen internal chain-of-thought reasoning but cannot invoke external\ntools, and agentic LLMs, which learn to interact with environments and leverage\ntools but often lag in deep reasoning. This divide arises from fundamentally\ndifferent training objectives, leading to mismatched strengths and inefficiency\non simple queries, where both families tend to overthink or over-call tools. In\nthis work, we present Adaptive Agent Foundation Model (A\\textsuperscript{2}FM),\na unified framework that follows a route-then-align principle: the model first\nlearns task-aware routing and then aligns mode-specific trajectories under a\nshared backbone. To address the inefficiency gap, we introduce a third\nmode-instant-that handles simple queries directly, preventing unnecessary\nreasoning or tool calls while complementing the agentic and reasoning modes. To\njointly enhance accuracy and efficiency, we propose Adaptive Policy\nOptimization (APO), which enforces adaptive sampling across modes and applies a\ncost-regularized reward. On the 32B scale, A\\textsuperscript{2}FM achieves\n13.4\\% on BrowseComp, 70.4\\% on AIME25, and 16.7\\% on HLE, setting new SOTA\namong comparable models and performing competitively with frontier LLMs across\nagentic, reasoning, and general benchmarks. Notably, the adaptive execution\nachieves a cost of pass of only \\$0.00487 per correct answer-cutting cost by\n45.2\\% relative to reasoning and 33.5\\% relative to agentic, thus delivering\nsubstantially higher cost efficiency while maintaining comparable accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u57fa\u7840\u6a21\u578b(A\u00b2FM)\uff0c\u901a\u8fc7\u8def\u7531-\u5bf9\u9f50\u539f\u5219\u7edf\u4e00\u63a8\u7406\u578b\u548c\u667a\u80fd\u4f53\u578bLLM\uff0c\u5f15\u5165\u5373\u65f6\u6a21\u5f0f\u5904\u7406\u7b80\u5355\u67e5\u8be2\uff0c\u663e\u8457\u63d0\u5347\u6210\u672c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u63a8\u7406\u578bLLM\u548c\u667a\u80fd\u4f53\u578bLLM\u4e4b\u95f4\u7684\u80fd\u529b\u5206\u88c2\u95ee\u9898\uff0c\u524d\u8005\u64c5\u957f\u5185\u90e8\u63a8\u7406\u4f46\u65e0\u6cd5\u8c03\u7528\u5de5\u5177\uff0c\u540e\u8005\u80fd\u4ea4\u4e92\u73af\u5883\u4f46\u63a8\u7406\u80fd\u529b\u8f83\u5f31\uff0c\u4e14\u4e24\u8005\u5728\u7b80\u5355\u67e5\u8be2\u4e0a\u90fd\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u6216\u8fc7\u5ea6\u8c03\u7528\u5de5\u5177\u7684\u4f4e\u6548\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8def\u7531-\u5bf9\u9f50\u539f\u5219\uff1a\u5148\u5b66\u4e60\u4efb\u52a1\u611f\u77e5\u8def\u7531\uff0c\u7136\u540e\u5728\u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\u4e0b\u5bf9\u9f50\u6a21\u5f0f\u7279\u5b9a\u8f68\u8ff9\uff1b\u5f15\u5165\u5373\u65f6\u6a21\u5f0f\u5904\u7406\u7b80\u5355\u67e5\u8be2\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316(APO)\uff0c\u5f3a\u5236\u8de8\u6a21\u5f0f\u81ea\u9002\u5e94\u91c7\u6837\u5e76\u5e94\u7528\u6210\u672c\u6b63\u5219\u5316\u5956\u52b1\u3002", "result": "\u572832B\u89c4\u6a21\u4e0a\uff0cA\u00b2FM\u5728BrowseComp\u3001AIME25\u548cHLE\u57fa\u51c6\u4e0a\u5206\u522b\u8fbe\u523013.4%\u300170.4%\u548c16.7%\uff0c\u5728\u53ef\u6bd4\u6a21\u578b\u4e2d\u521b\u4e0b\u65b0SOTA\uff0c\u5728\u667a\u80fd\u4f53\u3001\u63a8\u7406\u548c\u901a\u7528\u57fa\u51c6\u4e0a\u4e0e\u524d\u6cbfLLM\u7ade\u4e89\u3002\u81ea\u9002\u5e94\u6267\u884c\u6bcf\u4e2a\u6b63\u786e\u7b54\u6848\u6210\u672c\u4ec50.00487\u7f8e\u5143\uff0c\u76f8\u6bd4\u63a8\u7406\u6a21\u5f0f\u964d\u4f4e\u6210\u672c45.2%\uff0c\u76f8\u6bd4\u667a\u80fd\u4f53\u6a21\u5f0f\u964d\u4f4e\u6210\u672c33.5%\u3002", "conclusion": "A\u00b2FM\u6846\u67b6\u6210\u529f\u7edf\u4e00\u4e86\u63a8\u7406\u578b\u548c\u667a\u80fd\u4f53\u578bLLM\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8def\u7531\u663e\u8457\u63d0\u5347\u4e86\u6210\u672c\u6548\u7387\uff0c\u5728\u4fdd\u6301\u53ef\u6bd4\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agent analysis"}}
{"id": "2510.13551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13551", "abs": "https://arxiv.org/abs/2510.13551", "authors": ["Robert West", "Ashton Anderson", "Ece Kamar", "Eric Horvitz"], "title": "Tandem Training for Language Models", "comment": null, "summary": "As language models continue to rapidly improve, we can expect their actions\nand reasoning to become difficult or impossible for weaker agents and humans to\nfollow, undermining interpretability and oversight. With an eye on long-term\nfutures, we pursue methods that encourage models to produce solutions that\nremain intelligible to weaker collaborators. We formalize intelligibility as\nhandoff robustness: a strong model's solution is intelligible to a weaker model\nif randomly handing off control to the weaker model along the solution path\ndoes not cause failure. Building on this criterion, we introduce tandem\ntraining for language models, a reinforcement learning (RL) paradigm in which\nrollout tokens are intermittently and randomly sampled from a frozen weak model\nrather than the strong model being trained. Because rollouts succeed only when\nthe strong model's actions and reasoning process can be continued by the weak\nmodel -- when the two can co-construct a successful solution -- optimizing\nstandard RL objectives with tandem training implicitly incentivizes both\ncorrectness and intelligibility. In the GSM8K math reasoning task, tandem\ntraining reliably teaches models to abandon jargon and adapt their language to\nweaker partners while keeping task accuracy high. Our results demonstrate a\npromising route to building AI systems that remain auditable by weaker agents,\nwith implications for human--AI collaboration and multi-agent communication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3atandem training\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u5c06\u63a7\u5236\u6743\u4ea4\u7ed9\u8f83\u5f31\u6a21\u578b\u6765\u8bad\u7ec3\u5f3a\u6a21\u578b\u4ea7\u751f\u53ef\u7406\u89e3\u4e14\u53ef\u88ab\u8f83\u5f31\u6a21\u578b\u7ee7\u7eed\u6267\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u5feb\u901f\u63d0\u5347\uff0c\u5176\u63a8\u7406\u8fc7\u7a0b\u53ef\u80fd\u53d8\u5f97\u96be\u4ee5\u88ab\u8f83\u5f31\u4ee3\u7406\u6216\u4eba\u7c7b\u7406\u89e3\uff0c\u8fd9\u4f1a\u5f71\u54cd\u53ef\u89e3\u91ca\u6027\u548c\u76d1\u7763\u3002\u9700\u8981\u786e\u4fdd\u5f3a\u6a21\u578b\u4ea7\u751f\u7684\u89e3\u51b3\u65b9\u6848\u5bf9\u8f83\u5f31\u5408\u4f5c\u8005\u4fdd\u6301\u53ef\u7406\u89e3\u6027\u3002", "method": "\u5f15\u5165handoff robustness\u4f5c\u4e3a\u53ef\u7406\u89e3\u6027\u6807\u51c6\uff0c\u63d0\u51fatandem training\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u968f\u673a\u4ece\u51bb\u7ed3\u7684\u5f31\u6a21\u578b\u91c7\u6837token\uff0c\u53ea\u6709\u5f53\u5f3a\u6a21\u578b\u7684\u52a8\u4f5c\u548c\u63a8\u7406\u8fc7\u7a0b\u80fd\u88ab\u5f31\u6a21\u578b\u7ee7\u7eed\u65f6\uff0c\u8bad\u7ec3\u624d\u4f1a\u6210\u529f\u3002", "result": "\u5728GSM8K\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0ctandem training\u80fd\u6709\u6548\u6559\u4f1a\u6a21\u578b\u653e\u5f03\u4e13\u4e1a\u672f\u8bed\uff0c\u9002\u5e94\u8f83\u5f31\u5408\u4f5c\u4f19\u4f34\u7684\u8bed\u8a00\u98ce\u683c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u4efb\u52a1\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u53ef\u7531\u8f83\u5f31\u4ee3\u7406\u5ba1\u8ba1\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u5bf9\u4eba\u4e0eAI\u534f\u4f5c\u548c\u591a\u667a\u80fd\u4f53\u901a\u4fe1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.13709", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13709", "abs": "https://arxiv.org/abs/2510.13709", "authors": ["Evan Ellis", "Vivek Myers", "Jens Tuyls", "Sergey Levine", "Anca Dragan", "Benjamin Eysenbach"], "title": "Training LLM Agents to Empower Humans", "comment": null, "summary": "Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u5316\u4eba\u7c7b\u8d4b\u80fd\u7684\u8f85\u52a9\u8bed\u8a00\u6a21\u578b\u8c03\u4f18\u65b9\u6cd5Empower\uff0c\u4ec5\u9700\u79bb\u7ebf\u6587\u672c\u6570\u636e\u5373\u53ef\u8bad\u7ec3\u66f4\u6709\u6548\u7684AI\u52a9\u624b\uff0c\u65e0\u9700\u989d\u5916\u4eba\u5de5\u53cd\u9988\u3002", "motivation": "\u73b0\u6709\u8f85\u52a9\u4ee3\u7406\u65b9\u6cd5\u5f80\u5f80\u9f13\u52b1AI\u72ec\u7acb\u5b8c\u6210\u4efb\u52a1\u800c\u975e\u771f\u6b63\u534f\u52a9\u4eba\u7c7b\uff0c\u4e14\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u53cd\u9988\u3002\u9700\u8981\u4e00\u79cd\u80fd\u771f\u6b63\u5e2e\u52a9\u4eba\u7c7b\u5b9e\u73b0\u76ee\u6807\u3001\u4ec5\u9700\u79bb\u7ebf\u6570\u636e\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEmpower\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u4eba\u7c7b\u5728\u73af\u5883\u4e2d\u7684\u8d4b\u80fd\uff08\u5f71\u54cd\u671f\u671b\u53d8\u5316\u7684\u80fd\u529b\uff09\u6765\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u79bb\u7ebf\u6587\u672c\u6570\u636e\uff0c\u65e0\u9700\u989d\u5916\u4eba\u5de5\u53cd\u9988\u3002", "result": "\u7528\u6237\u7814\u7a76\u4e2d78%\u7684\u53c2\u4e0e\u8005\u504f\u597dEmpower\u52a9\u624b\uff08p=0.015\uff09\uff0c\u63a5\u53d7\u7387\u63d0\u9ad831%\uff0c\u5efa\u8bae\u51cf\u5c1138%\u3002\u5728\u4ee3\u7801\u534f\u52a9\u73af\u5883\u4e2d\uff0cEmpower\u5c06\u6a21\u62df\u7a0b\u5e8f\u5458\u5728\u6311\u6218\u6027\u7f16\u7a0b\u95ee\u9898\u4e0a\u7684\u6210\u529f\u7387\u5e73\u5747\u63d0\u9ad8192%\u3002", "conclusion": "Empower\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4ec5\u4f7f\u7528\u79bb\u7ebf\u6570\u636e\u3001\u65e0\u9700\u989d\u5916\u4eba\u5de5\u53cd\u9988\u6216\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5927\u89c4\u6a21\u6709\u7528\u5bf9\u9f50AI\u4ee3\u7406\u6846\u67b6\u3002", "topic": "code agent"}}
{"id": "2510.13727", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13727", "abs": "https://arxiv.org/abs/2510.13727", "authors": ["Ravi Pandya", "Madison Bland", "Duy P. Nguyen", "Changliu Liu", "Jaime Fern\u00e1ndez Fisac", "Andrea Bajcsy"], "title": "From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails", "comment": null, "summary": "Generative AI systems are increasingly assisting and acting on behalf of end\nusers in practical settings, from digital shopping assistants to\nnext-generation autonomous cars. In this context, safety is no longer about\nblocking harmful content, but about preempting downstream hazards like\nfinancial or physical harm. Yet, most AI guardrails continue to rely on output\nclassification based on labeled datasets and human-specified criteria,making\nthem brittle to new hazardous situations. Even when unsafe conditions are\nflagged, this detection offers no path to recovery: typically, the AI system\nsimply refuses to act--which is not always a safe choice. In this work, we\nargue that agentic AI safety is fundamentally a sequential decision problem:\nharmful outcomes arise from the AI system's continually evolving interactions\nand their downstream consequences on the world. We formalize this through the\nlens of safety-critical control theory, but within the AI model's latent\nrepresentation of the world. This enables us to build predictive guardrails\nthat (i) monitor an AI system's outputs (actions) in real time and (ii)\nproactively correct risky outputs to safe ones, all in a model-agnostic manner\nso the same guardrail can be wrapped around any AI model. We also offer a\npractical training recipe for computing such guardrails at scale via\nsafety-critical reinforcement learning. Our experiments in simulated driving\nand e-commerce settings demonstrate that control-theoretic guardrails can\nreliably steer LLM agents clear of catastrophic outcomes (from collisions to\nbankruptcy) while preserving task performance, offering a principled dynamic\nalternative to today's flag-and-block guardrails.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u7406\u8bba\u7684\u5b89\u5168\u62a4\u680f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7\u548c\u4e3b\u52a8\u4fee\u6b63AI\u7cfb\u7edf\u7684\u8f93\u51fa\uff0c\u9884\u9632\u4e0b\u6e38\u5371\u5bb3\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u963b\u6b62\u5371\u9669\u884c\u4e3a\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u62a4\u680f\u4e3b\u8981\u4f9d\u8d56\u8f93\u51fa\u5206\u7c7b\u548c\u4eba\u4e3a\u6807\u51c6\uff0c\u65e0\u6cd5\u5e94\u5bf9\u65b0\u5371\u9669\u60c5\u51b5\uff0c\u4e14\u68c0\u6d4b\u5230\u5371\u9669\u540e\u53ea\u80fd\u62d2\u7edd\u884c\u52a8\uff0c\u8fd9\u5e76\u4e0d\u603b\u662f\u5b89\u5168\u7684\u9009\u62e9\u3002", "method": "\u5c06AI\u5b89\u5168\u89c6\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u5728AI\u6a21\u578b\u7684\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u5e94\u7528\u5b89\u5168\u5173\u952e\u63a7\u5236\u7406\u8bba\uff0c\u6784\u5efa\u9884\u6d4b\u6027\u62a4\u680f\u6765\u5b9e\u65f6\u76d1\u63a7\u548c\u4e3b\u52a8\u4fee\u6b63\u98ce\u9669\u8f93\u51fa\u3002", "result": "\u5728\u6a21\u62df\u9a7e\u9a76\u548c\u7535\u5b50\u5546\u52a1\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63a7\u5236\u7406\u8bba\u62a4\u680f\u80fd\u53ef\u9760\u5730\u907f\u514d\u707e\u96be\u6027\u540e\u679c\uff08\u5982\u78b0\u649e\u548c\u7834\u4ea7\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u63a7\u5236\u7406\u8bba\u62a4\u680f\u4e3a\u5f53\u524d\u6807\u8bb0-\u963b\u6b62\u5f0f\u62a4\u680f\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u52a8\u6001\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u786e\u4fddAI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.13744", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13744", "abs": "https://arxiv.org/abs/2510.13744", "authors": ["Shrey Pandit", "Austin Xu", "Xuan-Phi Nguyen", "Yifei Ming", "Caiming Xiong", "Shafiq Joty"], "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math", "comment": "21 pages, 8 figures, 5 tables", "summary": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics.", "AI": {"tldr": "Hard2Verify\u662f\u4e00\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u6570\u5b66\u8bc1\u660e\u6b65\u9aa4\u7ea7\u9a8c\u8bc1\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u524d\u6cbfLLM\u9a8c\u8bc1\u5668\u5728\u6311\u6218\u6027\u6570\u5b66\u95ee\u9898\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5728IMO 2025\u7b49\u6570\u5b66\u7ade\u8d5b\u4e2d\uff0cLLM\u63a8\u7406\u7cfb\u7edf\u9700\u8981\u80fd\u591f\u9a8c\u8bc1\u6bcf\u4e2a\u8bc1\u660e\u6b65\u9aa4\u7684\u6b63\u786e\u6027\u548c\u5145\u5206\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f3a\u5927\u7684\u6b65\u9aa4\u7ea7\u9a8c\u8bc1\u5668\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b500+\u5c0f\u65f6\u4eba\u5de5\u6807\u6ce8\u7684Hard2Verify\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e8629\u4e2a\u751f\u6210\u5f0f\u8bc4\u8bba\u5668\u548c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5728\u8bc6\u522b\u524d\u6cbfLLM\u751f\u6210\u7684\u6570\u5b66\u8bc1\u660e\u4e2d\u7b2c\u4e00\u4e2a\u9519\u8bef\u7684\u80fd\u529b\u3002", "result": "\u9664\u4e86\u5c11\u6570\u8868\u73b0\u4f18\u5f02\u8005\u5916\uff0c\u5f00\u6e90\u9a8c\u8bc1\u5668\u843d\u540e\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u5206\u6790\u4e86\u6027\u80fd\u4e0d\u4f73\u7684\u539f\u56e0\u3001\u8ba1\u7b97\u89c4\u6a21\u5316\u7684\u5f71\u54cd\u4ee5\u53ca\u81ea\u9a8c\u8bc1\u548c\u9a8c\u8bc1-\u751f\u6210\u52a8\u6001\u7b49\u57fa\u672c\u95ee\u9898\u3002", "conclusion": "Hard2Verify\u57fa\u51c6\u63ed\u793a\u4e86\u5f53\u524d\u6b65\u9aa4\u7ea7\u9a8c\u8bc1\u5668\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6539\u8fdbLLM\u63a8\u7406\u7cfb\u7edf\u7684\u9a8c\u8bc1\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.12899", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12899", "abs": "https://arxiv.org/abs/2510.12899", "authors": ["Shouang Wei", "Min Zhang", "Xin Lin", "Bo Jiang", "Zhongxiang Dai", "Kun Kuang"], "title": "EduDial: Constructing a Large-scale Multi-turn Teacher-Student Dialogue Corpus", "comment": null, "summary": "Recently, several multi-turn dialogue benchmarks have been proposed to\nevaluate the conversational abilities of large language models (LLMs). As LLMs\nare increasingly recognized as a key technology for advancing intelligent\neducation, owing to their ability to deeply understand instructional contexts\nand provide personalized guidance, the construction of dedicated\nteacher-student dialogue benchmarks has become particularly important. To this\nend, we present EduDial, a comprehensive multi-turn teacher-student dialogue\ndataset. EduDial covers 345 core knowledge points and consists of 34,250\ndialogue sessions generated through interactions between teacher and student\nagents. Its design is guided by Bloom's taxonomy of educational objectives and\nincorporates ten questioning strategies, including situational questioning,\nzone of proximal development (ZPD) questioning, and metacognitive\nquestioning-thus better capturing authentic classroom interactions.\nFurthermore, we design differentiated teaching strategies for students at\ndifferent cognitive levels, thereby providing more targeted teaching guidance.\nBuilding on EduDial, we further develop EduDial-LLM 32B via training and\npropose an 11-dimensional evaluation framework that systematically measures the\nteaching abilities of LLMs, encompassing both overall teaching quality and\ncontent quality. Experiments on 17 mainstream LLMs reveal that most models\nstruggle in student-centered teaching scenarios, whereas our EduDial-LLM\nachieves significant gains, consistently outperforming all baselines across all\nmetrics. The code is available at\nhttps://github.com/Mind-Lab-ECNU/EduDial/tree/main.", "AI": {"tldr": "\u63d0\u51fa\u4e86EduDial\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u591a\u8f6e\u5e08\u751f\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5305\u542b34,250\u4e2a\u5bf9\u8bdd\u4f1a\u8bdd\uff0c\u8986\u76d6345\u4e2a\u6838\u5fc3\u77e5\u8bc6\u70b9\uff0c\u5e76\u57fa\u4e8e\u5e03\u9c81\u59c6\u6559\u80b2\u76ee\u6807\u5206\u7c7b\u5b66\u548c\u5341\u79cd\u63d0\u95ee\u7b56\u7565\u8bbe\u8ba1\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5e08\u751f\u5bf9\u8bdd\u57fa\u51c6\u6765\u8bc4\u4f30\u5176\u6559\u5b66\u80fd\u529b\uff0c\u4ee5\u63a8\u52a8\u667a\u80fd\u6559\u80b2\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6559\u5e08\u548c\u5b66\u751f\u4ee3\u7406\u4e4b\u95f4\u7684\u4ea4\u4e92\u751f\u6210\u5bf9\u8bdd\u6570\u636e\uff0c\u7ed3\u5408\u5e03\u9c81\u59c6\u6559\u80b2\u76ee\u6807\u5206\u7c7b\u5b66\u548c\u5341\u79cd\u63d0\u95ee\u7b56\u7565\uff0c\u4e3a\u4e0d\u540c\u8ba4\u77e5\u6c34\u5e73\u7684\u5b66\u751f\u8bbe\u8ba1\u5dee\u5f02\u5316\u6559\u5b66\u7b56\u7565\u3002", "result": "\u572817\u4e2a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5927\u591a\u6570\u6a21\u578b\u5728\u5b66\u751f\u4e2d\u5fc3\u6559\u5b66\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800cEduDial-LLM 32B\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "EduDial\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6559\u5b66\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0cEduDial-LLM\u5728\u5e08\u751f\u5bf9\u8bdd\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u667a\u80fd\u6559\u80b2\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002", "topic": "swe benchmark"}}
{"id": "2510.12925", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12925", "abs": "https://arxiv.org/abs/2510.12925", "authors": ["Nil-Jana Akpinar", "Chia-Jung Lee", "Vanessa Murdock", "Pietro Perona"], "title": "Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering", "comment": null, "summary": "Large Language Models (LLMs) should answer factual questions truthfully,\ngrounded in objective knowledge, regardless of user context such as\nself-disclosed personal information, or system personalization. In this paper,\nwe present the first systematic evaluation of LLM robustness to inquiry\npersonas, i.e. user profiles that convey attributes like identity, expertise,\nor belief. While prior work has primarily focused on adversarial inputs or\ndistractors for robustness testing, we evaluate plausible, human-centered\ninquiry persona cues that users disclose in real-world interactions. We find\nthat such cues can meaningfully alter QA accuracy and trigger failure modes\nsuch as refusals, hallucinated limitations, and role confusion. These effects\nhighlight how model sensitivity to user framing can compromise factual\nreliability, and position inquiry persona testing as an effective tool for\nrobustness evaluation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86LLMs\u5bf9\u8be2\u95ee\u89d2\u8272\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u7528\u6237\u4e2a\u4eba\u5c5e\u6027\u4fe1\u606f\u4f1a\u663e\u8457\u5f71\u54cd\u95ee\u7b54\u51c6\u786e\u6027\u5e76\u89e6\u53d1\u591a\u79cd\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u9762\u5bf9\u5305\u542b\u7528\u6237\u8eab\u4efd\u3001\u4e13\u4e1a\u77e5\u8bc6\u6216\u4fe1\u4ef0\u7b49\u4e2a\u4eba\u5c5e\u6027\u7684\u8be2\u95ee\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u4e9b\u662f\u771f\u5b9e\u4ea4\u4e92\u4e2d\u5e38\u89c1\u7684\u7528\u6237\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u5305\u542b\u7528\u6237\u5c5e\u6027\u4fe1\u606f\u7684\u8be2\u95ee\u89d2\u8272\u63d0\u793a\uff0c\u7cfb\u7edf\u6d4b\u8bd5LLMs\u5728\u63a5\u6536\u4e0d\u540c\u7528\u6237\u80cc\u666f\u4fe1\u606f\u65f6\u7684\u95ee\u7b54\u8868\u73b0\u3002", "result": "\u7528\u6237\u89d2\u8272\u63d0\u793a\u80fd\u663e\u8457\u6539\u53d8\u95ee\u7b54\u51c6\u786e\u6027\uff0c\u89e6\u53d1\u62d2\u7edd\u56de\u7b54\u3001\u5e7b\u89c9\u9650\u5236\u548c\u89d2\u8272\u6df7\u6dc6\u7b49\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u6a21\u578b\u5bf9\u7528\u6237\u6846\u67b6\u7684\u654f\u611f\u6027\u4f1a\u635f\u5bb3\u4e8b\u5b9e\u53ef\u9760\u6027\uff0c\u8be2\u95ee\u89d2\u8272\u6d4b\u8bd5\u662f\u6709\u6548\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.12943", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12943", "abs": "https://arxiv.org/abs/2510.12943", "authors": ["Angana Borah", "Rada Mihalcea"], "title": "The Curious Case of Curiosity across Human Cultures and LLMs", "comment": "Preprint (Paper under review)", "summary": "Recent advances in Large Language Models (LLMs) have expanded their role in\nhuman interaction, yet curiosity -- a central driver of inquiry -- remains\nunderexplored in these systems, particularly across cultural contexts. In this\nwork, we investigate cultural variation in curiosity using Yahoo! Answers, a\nreal-world multi-country dataset spanning diverse topics. We introduce CUEST\n(CUriosity Evaluation across SocieTies), an evaluation framework that measures\nhuman-model alignment in curiosity through linguistic (style), topic preference\n(content) analysis and grounding insights in social science constructs. Across\nopen- and closed-source models, we find that LLMs flatten cross-cultural\ndiversity, aligning more closely with how curiosity is expressed in Western\ncountries. We then explore fine-tuning strategies to induce curiosity in LLMs,\nnarrowing the human-model alignment gap by up to 50\\%. Finally, we demonstrate\nthe practical value of curiosity for LLM adaptability across cultures, showing\nits importance for future NLP research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5728\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u597d\u5947\u5fc3\u8868\u8fbe\uff0c\u53d1\u73b0LLMs\u4f1a\u524a\u5f31\u8de8\u6587\u5316\u591a\u6837\u6027\uff0c\u66f4\u63a5\u8fd1\u897f\u65b9\u56fd\u5bb6\u7684\u8868\u8fbe\u65b9\u5f0f\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u7b56\u7565\u5c06\u4eba\u673a\u5bf9\u9f50\u5dee\u8ddd\u7f29\u5c0f\u4e8650%\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u8de8\u6587\u5316\u80cc\u666f\u4e0b\u597d\u5947\u5fc3\u7684\u8868\u8fbe\u5dee\u5f02\uff0c\u56e0\u4e3a\u597d\u5947\u5fc3\u662f\u9a71\u52a8\u63a2\u7a76\u7684\u6838\u5fc3\u56e0\u7d20\uff0c\u4f46\u5728\u8fd9\u4e9b\u7cfb\u7edf\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528Yahoo! Answers\u591a\u56fd\u6570\u636e\u96c6\uff0c\u5f15\u5165CUEST\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u98ce\u683c\u3001\u4e3b\u9898\u504f\u597d\u5206\u6790\u548c\u793e\u4f1a\u79d1\u5b66\u6784\u5efa\u6765\u6d4b\u91cf\u4eba\u673a\u597d\u5947\u5fc3\u5bf9\u9f50\u3002", "result": "\u53d1\u73b0LLMs\u4f1a\u524a\u5f31\u8de8\u6587\u5316\u591a\u6837\u6027\uff0c\u66f4\u63a5\u8fd1\u897f\u65b9\u56fd\u5bb6\u7684\u8868\u8fbe\u65b9\u5f0f\uff1b\u901a\u8fc7\u5fae\u8c03\u7b56\u7565\u53ef\u5c06\u4eba\u673a\u5bf9\u9f50\u5dee\u8ddd\u7f29\u5c0f50%\uff1b\u8bc1\u660e\u597d\u5947\u5fc3\u5bf9LLMs\u8de8\u6587\u5316\u9002\u5e94\u6027\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "\u597d\u5947\u5fc3\u5bf9\u4e8eLLMs\u7684\u8de8\u6587\u5316\u9002\u5e94\u6027\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u662f\u672a\u6765NLP\u7814\u7a76\u7684\u91cd\u8981\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2510.13022", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13022", "abs": "https://arxiv.org/abs/2510.13022", "authors": ["Jiacheng Guo", "Zihao Li", "Jiahao Qiu", "Yue Wu", "Mengdi Wang"], "title": "On the Role of Preference Variance in Preference Optimization", "comment": null, "summary": "Direct Preference Optimization (DPO) has emerged as an important approach for\nlearning from human preferences in aligning large language models (LLMs).\nHowever, collecting human preference data is costly and inefficient, motivating\nmethods to reduce the required annotations. In this work, we investigate the\nimpact of \\emph{preference variance} (PVar), which measures the variance in\nmodel preferences when comparing pairs of responses, on the effectiveness of\nDPO training. We provide a theoretical insight by establishing an upper bound\non the DPO gradient norm for any given prompt, showing it is controlled by the\nPVar of that prompt. This implies that prompts with low PVar can only produce\nsmall gradient updates, making them less valuable for learning. We validate\nthis finding by fine-tuning LLMs with preferences generated by a reward model,\nevaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental\nresults demonstrate that prompts with higher PVar outperform randomly selected\nprompts or those with lower PVar. We also show that our PVar-based selection\nmethod is robust, when using smaller reward models (1B, 3B) for selection.\nNotably, in a separate experiment using the original human annotations from the\nUltraFeedback dataset, we found that training on only the top 10\\% of prompts\nwith the highest PVar yields better evaluation performance than training on the\nfull dataset, highlighting the importance of preference variance in identifying\ninformative examples for efficient LLM alignment.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u504f\u597d\u65b9\u5dee\uff08PVar\uff09\u5bf9DPO\u8bad\u7ec3\u6548\u679c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9ad8PVar\u7684\u63d0\u793a\u80fd\u4ea7\u751f\u66f4\u5927\u7684\u68af\u5ea6\u66f4\u65b0\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u8fdb\u884cLLM\u5bf9\u9f50\u3002", "motivation": "\u6536\u96c6\u4eba\u7c7b\u504f\u597d\u6570\u636e\u6210\u672c\u9ad8\u6602\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u51cf\u5c11\u6240\u9700\u6807\u6ce8\u7684\u65b9\u6cd5\u3002\u504f\u597d\u65b9\u5dee\u53ef\u4ee5\u8861\u91cf\u6a21\u578b\u5728\u6bd4\u8f83\u54cd\u5e94\u5bf9\u65f6\u7684\u504f\u597d\u5dee\u5f02\u7a0b\u5ea6\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5efa\u7acbDPO\u68af\u5ea6\u8303\u6570\u7684\u4e0a\u754c\uff0c\u8bc1\u660e\u5176\u53d7PVar\u63a7\u5236\u3002\u4f7f\u7528\u5956\u52b1\u6a21\u578b\u751f\u6210\u504f\u597d\u6570\u636e\uff0c\u5728AlpacaEval 2.0\u548cArena-Hard\u57fa\u51c6\u4e0a\u5fae\u8c03LLMs\uff0c\u6bd4\u8f83\u4e0d\u540cPVar\u63d0\u793a\u7684\u8bad\u7ec3\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u9ad8PVar\u63d0\u793a\u4f18\u4e8e\u968f\u673a\u9009\u62e9\u6216\u4f4ePVar\u63d0\u793a\u3002\u4f7f\u7528\u5c0f\u5956\u52b1\u6a21\u578b\uff081B\u30013B\uff09\u8fdb\u884c\u9009\u62e9\u65f6\uff0cPVar\u65b9\u6cd5\u4f9d\u7136\u7a33\u5065\u3002\u5728UltraFeedback\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u4f7f\u7528PVar\u6700\u9ad8\u7684\u524d10%\u63d0\u793a\u8bad\u7ec3\uff0c\u6548\u679c\u4f18\u4e8e\u4f7f\u7528\u5b8c\u6574\u6570\u636e\u96c6\u3002", "conclusion": "\u504f\u597d\u65b9\u5dee\u662f\u8bc6\u522b\u4fe1\u606f\u4e30\u5bcc\u793a\u4f8b\u4ee5\u8fdb\u884c\u9ad8\u6548LLM\u5bf9\u9f50\u7684\u91cd\u8981\u6307\u6807\uff0c\u9ad8PVar\u63d0\u793a\u80fd\u4ea7\u751f\u66f4\u6709\u6548\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.13143", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13143", "abs": "https://arxiv.org/abs/2510.13143", "authors": ["Junichiro Niimi"], "title": "Stable LLM Ensemble: Interaction between Example Representativeness and Diversity", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable results in wide range\nof domains. However, the accuracy and robustness of one-shot LLM predictions\nremain highly sensitive to the examples and the diversity among ensemble\nmembers. This study systematically investigates the effects of example\nrepresentativeness (one-shot strategy) and output diversity (sampling\ntemperature) on LLM ensemble performance. Two one-shot strategies are compared:\ncentroid-based representative examples (proposed) and randomly sampled examples\n(baseline) and sampling temperature also is varied. The proposed approach with\nhigher temperature setting significantly outperforms random selection by +7.6%\n(macro-F1) and -10.5% (RMSE). Furthermore, the proposed model exceeds 5-shot\nprompting by +21.1% (macro-F1) and -24.0% (RMSE). Our findings demonstrate that\ncombining representative example selection with increased temperature provides\nthe appropriate level of diversity to the ensemble. This work highlights the\npractical importance of both example selection and controlled diversity in\ndesigning effective one-shot LLM ensembles.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u793a\u4f8b\u4ee3\u8868\u6027\uff08\u5355\u6837\u672c\u7b56\u7565\uff09\u548c\u8f93\u51fa\u591a\u6837\u6027\uff08\u91c7\u6837\u6e29\u5ea6\uff09\u5bf9LLM\u96c6\u6210\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u57fa\u4e8e\u8d28\u5fc3\u7684\u4ee3\u8868\u6027\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\uff0c\u5728\u8f83\u9ad8\u6e29\u5ea6\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u968f\u673a\u9009\u62e9\u548c5\u6837\u672c\u63d0\u793a\u3002", "motivation": "\u5f53\u524d\u5355\u6837\u672cLLM\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u5bf9\u793a\u4f8b\u9009\u62e9\u548c\u96c6\u6210\u6210\u5458\u591a\u6837\u6027\u9ad8\u5ea6\u654f\u611f\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u793a\u4f8b\u9009\u62e9\u548c\u6e29\u5ea6\u63a7\u5236\u6765\u63d0\u5347\u96c6\u6210\u6027\u80fd\u3002", "method": "\u6bd4\u8f83\u4e24\u79cd\u5355\u6837\u672c\u7b56\u7565\uff1a\u57fa\u4e8e\u8d28\u5fc3\u7684\u4ee3\u8868\u6027\u793a\u4f8b\uff08\u63d0\u51fa\u65b9\u6cd5\uff09\u548c\u968f\u673a\u91c7\u6837\u793a\u4f8b\uff08\u57fa\u7ebf\uff09\uff0c\u540c\u65f6\u53d8\u5316\u91c7\u6837\u6e29\u5ea6\uff0c\u8bc4\u4f30\u5bf9LLM\u96c6\u6210\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8f83\u9ad8\u6e29\u5ea6\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u968f\u673a\u9009\u62e9\uff1a+7.6%\uff08\u5b8fF1\uff09\u548c-10.5%\uff08RMSE\uff09\uff0c\u4e14\u8d85\u8fc75\u6837\u672c\u63d0\u793a\uff1a+21.1%\uff08\u5b8fF1\uff09\u548c-24.0%\uff08RMSE\uff09\u3002", "conclusion": "\u7ed3\u5408\u4ee3\u8868\u6027\u793a\u4f8b\u9009\u62e9\u548c\u589e\u52a0\u6e29\u5ea6\u80fd\u4e3a\u96c6\u6210\u63d0\u4f9b\u9002\u5f53\u7684\u591a\u6837\u6027\u6c34\u5e73\uff0c\u5f3a\u8c03\u4e86\u793a\u4f8b\u9009\u62e9\u548c\u53d7\u63a7\u591a\u6837\u6027\u5728\u8bbe\u8ba1\u6709\u6548\u5355\u6837\u672cLLM\u96c6\u6210\u4e2d\u7684\u5b9e\u9645\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.13212", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13212", "abs": "https://arxiv.org/abs/2510.13212", "authors": ["Zizhuo Zhang", "Qizhou Wang", "Shanshan Ye", "Jianing Zhu", "Jiangchao Yao", "Bo Han", "Masashi Sugiyama"], "title": "Towards Understanding Valuable Preference Data for Large Language Model Alignment", "comment": null, "summary": "Large language model (LLM) alignment is typically achieved through learning\nfrom human preference comparisons, making the quality of preference data\ncritical to its success. Existing studies often pre-process raw training\ndatasets to identify valuable preference pairs using external reward models or\noff-the-shelf LLMs, achieving improved overall performance but rarely examining\nwhether individual, selected data point is genuinely beneficial. We assess data\nquality through individual influence on validation data using our newly\nproposed truncated influence function (TIF), which mitigates the over-scoring\npresent in traditional measures and reveals that preference data quality is\ninherently a property of the model. In other words, a data pair that benefits\none model may harm another. This leaves the need to improve the preference data\nselection approaches to be adapting to specific models. To this end, we\nintroduce two candidate scoring functions (SFs) that are computationally\nsimpler than TIF and positively correlated with it. They are also model\ndependent and can serve as potential indicators of individual data quality for\npreference data selection. Furthermore, we observe that these SFs inherently\nexhibit errors when compared to TIF. To this end, we combine them to offset\ntheir diverse error sources, resulting in a simple yet effective data selection\nrule that enables the models to achieve a more precise selection of valuable\npreference data. We conduct experiments across diverse alignment benchmarks and\nvarious LLM families, with results demonstrating that better alignment\nperformance can be achieved using less data, showing the generality of our\nfindings and new methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u622a\u65ad\u5f71\u54cd\u51fd\u6570(TIF)\u6765\u8bc4\u4f30\u504f\u597d\u6570\u636e\u8d28\u91cf\uff0c\u53d1\u73b0\u6570\u636e\u8d28\u91cf\u662f\u6a21\u578b\u76f8\u5173\u7684\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u66f4\u7b80\u5355\u7684\u8bc4\u5206\u51fd\u6570\u6765\u6539\u8fdb\u504f\u597d\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u4f7f\u7528\u5916\u90e8\u5956\u52b1\u6a21\u578b\u6216\u73b0\u6210LLM\u9884\u5904\u7406\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u96c6\u6765\u8bc6\u522b\u6709\u4ef7\u503c\u7684\u504f\u597d\u5bf9\uff0c\u4f46\u5f88\u5c11\u68c0\u67e5\u5355\u4e2a\u6570\u636e\u70b9\u662f\u5426\u771f\u6b63\u6709\u76ca\u3002\u9700\u8981\u6539\u8fdb\u504f\u597d\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u4ee5\u9002\u5e94\u7279\u5b9a\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u622a\u65ad\u5f71\u54cd\u51fd\u6570(TIF)\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\uff0c\u5f15\u5165\u4e24\u79cd\u8ba1\u7b97\u66f4\u7b80\u5355\u7684\u8bc4\u5206\u51fd\u6570(SF)\uff0c\u5e76\u5c06\u5b83\u4eec\u7ec4\u5408\u4ee5\u62b5\u6d88\u4e0d\u540c\u7684\u8bef\u5dee\u6e90\uff0c\u5f62\u6210\u7b80\u5355\u6709\u6548\u7684\u6570\u636e\u9009\u62e9\u89c4\u5219\u3002", "result": "\u5728\u591a\u6837\u5316\u5bf9\u9f50\u57fa\u51c6\u548c\u5404\u79cdLLM\u5bb6\u65cf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u66f4\u5c11\u7684\u6570\u636e\u53ef\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u5bf9\u9f50\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u504f\u597d\u6570\u636e\u8d28\u91cf\u662f\u6a21\u578b\u76f8\u5173\u7684\uff0c\u901a\u8fc7\u6a21\u578b\u4f9d\u8d56\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u53ef\u4ee5\u66f4\u7cbe\u786e\u5730\u9009\u62e9\u6709\u4ef7\u503c\u7684\u504f\u597d\u6570\u636e\uff0c\u63d0\u9ad8\u5bf9\u9f50\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2510.13163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13163", "abs": "https://arxiv.org/abs/2510.13163", "authors": ["Nyx Iskandar", "Hisham Bedri", "Andy Tsen"], "title": "A Matter of Representation: Towards Graph-Based Abstract Code Generation", "comment": null, "summary": "Most large language models (LLMs) today excel at generating raw, sequential\ncode with minimal abstractions and custom structures. However, there has been\nlittle work on graph-based abstract code generation, where significant logic is\nencapsulated in predefined nodes and execution flow is determined by edges.\nThis is relevant for visual programming languages, and in cases where raw\nsource code is inaccessible to users and LLM training sets. In this work, we\npropose and evaluate JSON representations for graphs to enable high accuracy\ngraph-based abstract code generation. We evaluate these representations on\nScratchTest, a mini-benchmark based on our custom Python re-implementation of\nScratch, which tests the LLM in code graph space. Our findings demonstrate that\nLLMs can indeed perform the aforementioned generation task in a single pass\nwithout relying on specialized or complex pipelines, given the correct graph\nrepresentations. We also show that different representations induce\nsignificantly different accuracies, highlighting the instrumental role of\nrepresentations in this generation task. All in all, this work establishes the\nfirst steps towards representation learning for graph-based abstract code\ngeneration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u7528\u4e8e\u56fe\u7ed3\u6784\u62bd\u8c61\u4ee3\u7801\u751f\u6210\u7684JSON\u8868\u793a\u65b9\u6cd5\uff0c\u5728\u57fa\u4e8eScratch\u7684\u6d4b\u8bd5\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86LLM\u80fd\u591f\u5355\u6b21\u751f\u6210\u56fe\u7ed3\u6784\u4ee3\u7801\uff0c\u4e14\u4e0d\u540c\u8868\u793a\u65b9\u6cd5\u5bf9\u51c6\u786e\u7387\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u5f53\u524dLLM\u64c5\u957f\u751f\u6210\u539f\u59cb\u987a\u5e8f\u4ee3\u7801\uff0c\u4f46\u5728\u56fe\u7ed3\u6784\u62bd\u8c61\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7814\u7a76\u8f83\u5c11\uff0c\u8fd9\u79cd\u80fd\u529b\u5bf9\u53ef\u89c6\u5316\u7f16\u7a0b\u8bed\u8a00\u548c\u65e0\u6cd5\u8bbf\u95ee\u6e90\u4ee3\u7801\u7684\u573a\u666f\u5f88\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u56fe\u7ed3\u6784\u7684JSON\u8868\u793a\u65b9\u6cd5\uff0c\u5728\u57fa\u4e8ePython\u91cd\u5b9e\u73b0\u7684Scratch\u6d4b\u8bd5\u57fa\u51c6ScratchTest\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "LLM\u80fd\u591f\u5728\u5355\u6b21\u751f\u6210\u4e2d\u5b8c\u6210\u56fe\u7ed3\u6784\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u65e0\u9700\u590d\u6742\u6d41\u7a0b\uff0c\u4e14\u4e0d\u540c\u8868\u793a\u65b9\u6cd5\u5bfc\u81f4\u51c6\u786e\u7387\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u56fe\u7ed3\u6784\u62bd\u8c61\u4ee3\u7801\u751f\u6210\u7684\u8868\u793a\u5b66\u4e60\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "code agent"}}
{"id": "2510.13170", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13170", "abs": "https://arxiv.org/abs/2510.13170", "authors": ["Xiaoshu Chen", "Sihang Zhou", "Ke Liang", "Duanyang Yuan", "Haoyuan Chen", "Xiaoyu Sun", "Linyuan Meng", "Xinwang Liu"], "title": "Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from the Perspective of Human Reasoning Mechanism", "comment": null, "summary": "Chain of thought (CoT) fine-tuning aims to endow large language models (LLMs)\nwith reasoning capabilities by training them on curated reasoning traces. It\nleverages both supervised and reinforced fine-tuning to cultivate human-like\nreasoning skills in LLMs, including detailed planning, divergent thinking,\nintuitive judgment, timely reflection, internal thinking, and fact perception,\netc. As CoT fine-tuning has advanced, LLMs have demonstrated substantial\nimprovements in tasks such as mathematical reasoning and code generation.\nHowever, existing surveys about CoT fine-tuning primarily focus on technical\naspects and overlook a systematic analysis from the perspective of human\nreasoning mechanisms. Given that the ultimate goal of CoT fine-tuning is to\nenable LLMs to reason like humans, it is crucial to investigate this technique\nthrough the lens of human cognition. To fill this gap, we present the first\ncomprehensive survey of CoT fine-tuning grounded in human reasoning theory.\nSpecifically, inspired by the well-known Six Thinking Hats framework, which\nsystematically characterizes common human thinking modes using six metaphorical\nhats, we classify and examine CoT fine-tuning methods through this lens.\nFurthermore, building upon this theory, we outline potential directions for\nfuture research in CoT fine-tuning. In addition, we compile a comprehensive\noverview of existing datasets and model performances, and a real-time GitHub\nrepository \\footnote{https://github.com/AI-Chen/Awesome-CoT-Finetuning} that\ncontinuously tracks recent advances in this area is maintained. We hope this\nsurvey will serve as a valuable resource to inspire innovation and foster\nprogress in this rapidly evolving field.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4ece\u4eba\u7c7b\u63a8\u7406\u7406\u8bba\u89d2\u5ea6\u7cfb\u7edf\u7efc\u8ff0\u601d\u7ef4\u94fe\u5fae\u8c03\u6280\u672f\uff0c\u57fa\u4e8e\u516d\u9876\u601d\u8003\u5e3d\u6846\u67b6\u5bf9CoT\u5fae\u8c03\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\u5206\u6790\uff0c\u5e76\u5c55\u671b\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u5173\u4e8e\u601d\u7ef4\u94fe\u5fae\u8c03\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6280\u672f\u5c42\u9762\uff0c\u7f3a\u4e4f\u4ece\u4eba\u7c7b\u63a8\u7406\u673a\u5236\u89d2\u5ea6\u7684\u7cfb\u7edf\u5206\u6790\u3002\u9274\u4e8eCoT\u5fae\u8c03\u7684\u6700\u7ec8\u76ee\u6807\u662f\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u50cf\u4eba\u7c7b\u4e00\u6837\u63a8\u7406\uff0c\u4ece\u8ba4\u77e5\u89d2\u5ea6\u7814\u7a76\u8fd9\u4e00\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u516d\u9876\u601d\u8003\u5e3d\u6846\u67b6\u5bf9CoT\u5fae\u8c03\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\u548c\u7cfb\u7edf\u5206\u6790\uff0c\u6784\u5efa\u4e86\u6570\u636e\u96c6\u548c\u6a21\u578b\u6027\u80fd\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5e76\u7ef4\u62a4\u5b9e\u65f6GitHub\u4ed3\u5e93\u8ddf\u8e2a\u6700\u65b0\u8fdb\u5c55\u3002", "result": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u4eba\u7c7b\u63a8\u7406\u7406\u8bba\u7684CoT\u5fae\u8c03\u7cfb\u7edf\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u7279\u70b9\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8fd9\u9879\u8c03\u67e5\u4e3aCoT\u5fae\u8c03\u9886\u57df\u63d0\u4f9b\u4e86\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u7406\u8bba\u7684\u65b0\u89c6\u89d2\uff0c\u6709\u671b\u6fc0\u53d1\u521b\u65b0\u5e76\u63a8\u52a8\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u9886\u57df\u7684\u8fdb\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2510.13327", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13327", "abs": "https://arxiv.org/abs/2510.13327", "authors": ["Lina Alkarmi", "Ziyuan Huang", "Mingyan Liu"], "title": "When In Doubt, Abstain: The Impact of Abstention on Strategic Classification", "comment": null, "summary": "Algorithmic decision making is increasingly prevalent, but often vulnerable\nto strategic manipulation by agents seeking a favorable outcome. Prior research\nhas shown that classifier abstention (allowing a classifier to decline making a\ndecision due to insufficient confidence) can significantly increase classifier\naccuracy. This paper studies abstention within a strategic classification\ncontext, exploring how its introduction impacts strategic agents' responses and\nhow principals should optimally leverage it. We model this interaction as a\nStackelberg game where a principal, acting as the classifier, first announces\nits decision policy, and then strategic agents, acting as followers, manipulate\ntheir features to receive a desired outcome. Here, we focus on binary\nclassifiers where agents manipulate observable features rather than their true\nfeatures, and show that optimal abstention ensures that the principal's utility\n(or loss) is no worse than in a non-abstention setting, even in the presence of\nstrategic agents. We also show that beyond improving accuracy, abstention can\nalso serve as a deterrent to manipulation, making it costlier for agents,\nespecially those less qualified, to manipulate to achieve a positive outcome\nwhen manipulation costs are significant enough to affect agent behavior. These\nresults highlight abstention as a valuable tool for reducing the negative\neffects of strategic behavior in algorithmic decision making systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5728\u6218\u7565\u5206\u7c7b\u4e2d\u5f15\u5165\u5f03\u6743\u673a\u5236\u5982\u4f55\u5f71\u54cd\u6218\u7565\u4ee3\u7406\u7684\u54cd\u5e94\uff0c\u4ee5\u53ca\u4e3b\u4f53\u5e94\u5982\u4f55\u6700\u4f18\u5229\u7528\u5f03\u6743\u3002\u7814\u7a76\u8868\u660e\u6700\u4f18\u5f03\u6743\u80fd\u786e\u4fdd\u4e3b\u4f53\u6548\u7528\u4e0d\u52a3\u4e8e\u975e\u5f03\u6743\u8bbe\u7f6e\uff0c\u4e14\u5f03\u6743\u53ef\u4f5c\u4e3a\u64cd\u7eb5\u5a01\u6151\uff0c\u4f7f\u4e0d\u5408\u683c\u4ee3\u7406\u64cd\u7eb5\u6210\u672c\u66f4\u9ad8\u3002", "motivation": "\u7b97\u6cd5\u51b3\u7b56\u65e5\u76ca\u666e\u53ca\u4f46\u6613\u53d7\u6218\u7565\u64cd\u7eb5\uff0c\u5148\u524d\u7814\u7a76\u8868\u660e\u5206\u7c7b\u5668\u5f03\u6743\u80fd\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5728\u6218\u7565\u5206\u7c7b\u80cc\u666f\u4e0b\u5f15\u5165\u5f03\u6743\u673a\u5236\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Stackelberg\u535a\u5f08\u6a21\u578b\uff0c\u4e3b\u4f53\u4f5c\u4e3a\u5206\u7c7b\u5668\u9996\u5148\u5ba3\u5e03\u51b3\u7b56\u7b56\u7565\uff0c\u6218\u7565\u4ee3\u7406\u968f\u540e\u64cd\u7eb5\u7279\u5f81\u4ee5\u83b7\u5f97\u671f\u671b\u7ed3\u679c\u3002\u805a\u7126\u4e8e\u4e8c\u8fdb\u5236\u5206\u7c7b\u5668\uff0c\u4ee3\u7406\u64cd\u7eb5\u53ef\u89c2\u5bdf\u7279\u5f81\u800c\u975e\u771f\u5b9e\u7279\u5f81\u3002", "result": "\u6700\u4f18\u5f03\u6743\u786e\u4fdd\u4e3b\u4f53\u6548\u7528\u4e0d\u52a3\u4e8e\u975e\u5f03\u6743\u8bbe\u7f6e\uff1b\u5f03\u6743\u4e0d\u4ec5\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u64cd\u7eb5\u5a01\u6151\uff0c\u5728\u64cd\u7eb5\u6210\u672c\u8db3\u591f\u5927\u65f6\u4f7f\u4e0d\u5408\u683c\u4ee3\u7406\u64cd\u7eb5\u6210\u672c\u66f4\u9ad8\u3002", "conclusion": "\u5f03\u6743\u662f\u51cf\u5c11\u7b97\u6cd5\u51b3\u7b56\u7cfb\u7edf\u4e2d\u6218\u7565\u884c\u4e3a\u8d1f\u9762\u5f71\u54cd\u7684\u5b9d\u8d35\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.13255", "categories": ["cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.13255", "abs": "https://arxiv.org/abs/2510.13255", "authors": ["Jingmin An", "Yilong Song", "Ruolin Yang", "Nai Ding", "Lingxi Lu", "Yuxuan Wang", "Wei Wang", "Chu Zhuang", "Qian Wang", "Fang Fang"], "title": "Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain", "comment": null, "summary": "Large Language Models (LLMs) demonstrate human-level or even superior\nlanguage abilities, effectively modeling syntactic structures, yet the specific\ncomputational modules responsible remain unclear. A key question is whether LLM\nbehavioral capabilities stem from mechanisms akin to those in the human brain.\nTo address these questions, we introduce the Hierarchical Frequency Tagging\nProbe (HFTP), a tool that utilizes frequency-domain analysis to identify\nneuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP)\nneurons) and cortical regions (via intracranial recordings) encoding syntactic\nstructures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama\n2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human\nbrain relies on distinct cortical regions for different syntactic levels.\nRepresentational similarity analysis reveals a stronger alignment between LLM\nrepresentations and the left hemisphere of the brain (dominant in language\nprocessing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows\ngreater brain similarity than Gemma, while Llama 3.1 shows less alignment with\nthe brain compared to Llama 2. These findings offer new insights into the\ninterpretability of LLM behavioral improvements, raising questions about\nwhether these advancements are driven by human-like or non-human-like\nmechanisms, and establish HFTP as a valuable tool bridging computational\nlinguistics and cognitive neuroscience. This project is available at\nhttps://github.com/LilTiger/HFTP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5206\u5c42\u9891\u7387\u6807\u8bb0\u63a2\u9488(HFTP)\uff0c\u901a\u8fc7\u9891\u57df\u5206\u6790\u8bc6\u522bLLM\u4e2d\u5904\u7406\u53e5\u6cd5\u7ed3\u6784\u7684\u795e\u7ecf\u5143\u7ec4\u4ef6\u548c\u5927\u8111\u76ae\u5c42\u533a\u57df\uff0c\u53d1\u73b0LLM\u4e0e\u4eba\u7c7b\u5927\u8111\u5728\u53e5\u6cd5\u5904\u7406\u4e0a\u5b58\u5728\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u6027\u3002", "motivation": "\u63a2\u7d22LLM\u7684\u8bed\u8a00\u80fd\u529b\u662f\u5426\u6e90\u4e8e\u4e0e\u4eba\u7c7b\u5927\u8111\u76f8\u4f3c\u7684\u673a\u5236\uff0c\u4ee5\u53ca\u8bc6\u522b\u8d1f\u8d23\u53e5\u6cd5\u5904\u7406\u7684\u5177\u4f53\u8ba1\u7b97\u6a21\u5757\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u9891\u7387\u6807\u8bb0\u63a2\u9488(HFTP)\u8fdb\u884c\u9891\u57df\u5206\u6790\uff0c\u6bd4\u8f83GPT-2\u3001Gemma\u3001Llama\u7b49\u591a\u4e2aLLM\u6a21\u578b\u4e0e\u4eba\u7c7b\u5927\u8111\u76ae\u5c42\u5728\u53e5\u6cd5\u5904\u7406\u4e0a\u7684\u8868\u5f81\u76f8\u4f3c\u6027\u3002", "result": "LLM\u5728\u76f8\u4f3c\u5c42\u5904\u7406\u53e5\u6cd5\uff0c\u800c\u4eba\u7c7b\u5927\u8111\u4f9d\u8d56\u4e0d\u540c\u76ae\u5c42\u533a\u57df\uff1bLLM\u8868\u5f81\u4e0e\u5927\u8111\u5de6\u534a\u7403\u66f4\u76f8\u4f3c\uff1b\u6a21\u578b\u5347\u7ea7\u5448\u73b0\u4e0d\u540c\u8d8b\u52bf\uff1aGemma 2\u6bd4Gemma\u66f4\u63a5\u8fd1\u5927\u8111\uff0c\u800cLlama 3.1\u6bd4Llama 2\u66f4\u504f\u79bb\u3002", "conclusion": "HFTP\u662f\u8fde\u63a5\u8ba1\u7b97\u8bed\u8a00\u5b66\u548c\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u7684\u6709\u4ef7\u503c\u5de5\u5177\uff0c\u63ed\u793a\u4e86LLM\u884c\u4e3a\u6539\u8fdb\u7684\u673a\u5236\u53ef\u80fd\u5e76\u975e\u5b8c\u5168\u4eba\u7c7b\u5316\u3002", "topic": "agent analysis"}}
{"id": "2510.13271", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13271", "abs": "https://arxiv.org/abs/2510.13271", "authors": ["Ine Gevers", "Walter Daelemans"], "title": "Do You Get the Hint? Benchmarking LLMs on the Board Game Concept", "comment": null, "summary": "Large language models (LLMs) have achieved striking successes on many\nbenchmarks, yet recent studies continue to expose fundamental weaknesses. In\nparticular, tasks that require abstract reasoning remain challenging, often\nbecause they use representations such as grids, symbols, or visual patterns\nthat differ from the natural language data LLMs are trained on. In this paper,\nwe introduce Concept, a simple word-guessing board game, as a benchmark for\nprobing abductive reasoning in a representation that is much closer to LLM\npre-training data: natural language. Our results show that this game, easily\nsolved by humans (with a success rate of over 90\\%), is still very challenging\nfor state-of-the-art LLMs (no model exceeds 40\\% success rate). Specifically,\nwe observe that LLMs struggle with interpreting other players' strategic\nintents, and with correcting initial hypotheses given sequential information\nupdates. In addition, we extend the evaluation across multiple languages, and\nfind that the LLM performance drops further in lower-resource languages (Dutch,\nFrench, and Spanish) compared to English.", "AI": {"tldr": "Concept\u6e38\u620f\u4f5c\u4e3a\u8bc4\u4f30LLM\u6eaf\u56e0\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u7ed3\u679c\u663e\u793aLLM\u5728\u9700\u8981\u62bd\u8c61\u63a8\u7406\u548c\u6218\u7565\u610f\u56fe\u7406\u89e3\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6210\u529f\u7387\u4f4e\u4e8e40%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768490%\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709LLM\u5728\u9700\u8981\u62bd\u8c61\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u4ecd\u6709\u6839\u672c\u6027\u5f31\u70b9\uff0c\u7279\u522b\u662f\u5904\u7406\u4e0e\u81ea\u7136\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\u8868\u793a\u5f62\u5f0f\u4e0d\u540c\u7684\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165Concept\u5355\u8bcd\u731c\u6d4b\u68cb\u76d8\u6e38\u620f\u4f5c\u4e3a\u57fa\u51c6\uff0c\u5728\u66f4\u63a5\u8fd1LLM\u9884\u8bad\u7ec3\u6570\u636e\uff08\u81ea\u7136\u8bed\u8a00\uff09\u7684\u8868\u793a\u4e2d\u6d4b\u8bd5\u6eaf\u56e0\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u6269\u5c55\u5230\u591a\u79cd\u8bed\u8a00\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4eba\u7c7b\u6210\u529f\u7387\u8d85\u8fc790%\uff0c\u800c\u6700\u5148\u8fdb\u7684LLM\u6210\u529f\u7387\u4e0d\u8d85\u8fc740%\u3002LLM\u5728\u89e3\u91ca\u5176\u4ed6\u73a9\u5bb6\u6218\u7565\u610f\u56fe\u548c\u6839\u636e\u987a\u5e8f\u4fe1\u606f\u66f4\u65b0\u4fee\u6b63\u521d\u59cb\u5047\u8bbe\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "Concept\u6e38\u620f\u63ed\u793a\u4e86LLM\u5728\u62bd\u8c61\u63a8\u7406\u548c\u6218\u7565\u7406\u89e3\u65b9\u9762\u7684\u663e\u8457\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u66f4\u5dee\u3002", "topic": "agent analysis"}}
{"id": "2510.13367", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13367", "abs": "https://arxiv.org/abs/2510.13367", "authors": ["Nikita Kachaev", "Daniil Zelezetsky", "Egor Cherepanov", "Alexey K. Kovelev", "Aleksandr I. Panov"], "title": "A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control", "comment": null, "summary": "Despite their effectiveness and popularity in offline or model-based\nreinforcement learning (RL), transformers remain underexplored in online\nmodel-free RL due to their sensitivity to training setups and model design\ndecisions such as how to structure the policy and value networks, share\ncomponents, or handle temporal information. In this paper, we show that\ntransformers can be strong baselines for continuous control in online\nmodel-free RL. We investigate key design questions: how to condition inputs,\nshare components between actor and critic, and slice sequential data for\ntraining. Our experiments reveal stable architectural and training strategies\nenabling competitive performance across fully and partially observable tasks,\nand in both vector- and image-based settings. These findings offer practical\nguidance for applying transformers in online RL.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5728\u7ebf\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e94\u7528Transformer\u67b6\u6784\u7684\u5173\u952e\u8bbe\u8ba1\u95ee\u9898\uff0c\u5305\u62ec\u8f93\u5165\u6761\u4ef6\u3001\u7ec4\u4ef6\u5171\u4eab\u548c\u5e8f\u5217\u6570\u636e\u5904\u7406\uff0c\u63d0\u51fa\u4e86\u7a33\u5b9a\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u5c3d\u7ba1Transformer\u5728\u79bb\u7ebf\u6216\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5728\u7ebf\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ecd\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4e3b\u8981\u56e0\u4e3a\u5176\u5bf9\u8bad\u7ec3\u8bbe\u7f6e\u548c\u6a21\u578b\u8bbe\u8ba1\u51b3\u7b56\u7684\u654f\u611f\u6027\u3002", "method": "\u7814\u7a76\u4e86Transformer\u5728\u5728\u7ebf\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u8bbe\u8ba1\u95ee\u9898\uff1a\u8f93\u5165\u6761\u4ef6\u8bbe\u7f6e\u3001actor\u548ccritic\u7ec4\u4ef6\u5171\u4eab\u7b56\u7565\u3001\u5e8f\u5217\u6570\u636e\u5207\u7247\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u7a33\u5b9a\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u5728\u5b8c\u5168\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u4efb\u52a1\u4e2d\u90fd\u80fd\u5b9e\u73b0\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5411\u91cf\u548c\u56fe\u50cf\u4e24\u79cd\u8bbe\u7f6e\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e94\u7528Transformer\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.13272", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13272", "abs": "https://arxiv.org/abs/2510.13272", "authors": ["Zhichao Xu", "Zongyu Wu", "Yun Zhou", "Aosong Feng", "Kang Zhou", "Sangmin Woo", "Kiran Ramnath", "Yijun Tian", "Xuan Qi", "Weikang Qiu", "Lin Lee Cheong", "Haibo Ding"], "title": "Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation", "comment": null, "summary": "Inspired by the success of reinforcement learning (RL) in Large Language\nModel (LLM) training for domains like math and code, recent works have begun\nexploring how to train LLMs to use search engines more effectively as tools for\nretrieval-augmented generation. Although these methods achieve performance\nimprovement across QA benchmarks, many prioritize final answer correctness\nwhile overlooking the quality of intermediate reasoning steps, which may lead\nto chain-of-thought unfaithfulness. In this paper, we first introduce a\ncomprehensive evaluation framework for evaluating RL-based search agents,\ncovering three distinct faithfulness metrics: information-think faithfulness,\nthink-answer faithfulness, and think-search faithfulness. Our evaluations\nreveal that a prototypical RL-based search agent, Search-R1, has significant\nroom for improvement in this regard. To foster faithful reasoning, we introduce\nVERITAS (Verifying Entailed Reasoning through Intermediate Traceability in\nAgentic Search), a novel framework that integrates fine-grained faithfulness\nrewards into the reinforcement learning process. Our experiments show that\nmodels trained with VERITAS not only significantly improve reasoning\nfaithfulness, but also achieve comparable task performance across seven QA\nbenchmarks.", "AI": {"tldr": "\u63d0\u51faVERITAS\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7ec6\u7c92\u5ea6\u5fe0\u5b9e\u6027\u5956\u52b1\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u63d0\u5347\u57fa\u4e8eRL\u7684\u641c\u7d22\u4ee3\u7406\u7684\u63a8\u7406\u5fe0\u5b9e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eRL\u7684\u641c\u7d22\u4ee3\u7406\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u5347\u95ee\u7b54\u57fa\u51c6\u6027\u80fd\uff0c\u4f46\u8fc7\u4e8e\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u800c\u5ffd\u7565\u4e86\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u8d28\u91cf\uff0c\u53ef\u80fd\u5bfc\u81f4\u601d\u7ef4\u94fe\u4e0d\u5fe0\u5b9e\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u5f15\u5165\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u8bc4\u4f30\u57fa\u4e8eRL\u7684\u641c\u7d22\u4ee3\u7406\u7684\u5fe0\u5b9e\u6027\uff0c\u7136\u540e\u63d0\u51faVERITAS\u6846\u67b6\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u96c6\u6210\u7ec6\u7c92\u5ea6\u5fe0\u5b9e\u6027\u5956\u52b1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528VERITAS\u8bad\u7ec3\u7684\u6a21\u578b\u4e0d\u4ec5\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u5fe0\u5b9e\u6027\uff0c\u8fd8\u5728\u4e03\u4e2a\u95ee\u7b54\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u53ef\u6bd4\u8f83\u7684\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "VERITAS\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u641c\u7d22\u4ee3\u7406\u7684\u63a8\u7406\u5fe0\u5b9e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e2d\u95f4\u63a8\u7406\u8d28\u91cf\u7684\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2510.13391", "categories": ["cs.LG", "cs.GT", "91A12, 68T07, 05C21", "I.2.6; F.2.2; C.2.1"], "pdf": "https://arxiv.org/pdf/2510.13391", "abs": "https://arxiv.org/abs/2510.13391", "authors": ["Benjamin Kempinski", "Tal Kachman"], "title": "Going with the Flow: Approximating Banzhaf Values via Graph Neural Networks", "comment": "21 pages, 8 figures, 11-page appendix", "summary": "Computing the Banzhaf value in network flow games is fundamental for\nquantifying agent influence in multi-agent systems, with applications ranging\nfrom cybersecurity to infrastructure planning. However, exact computation is\nintractable for systems with more than $\\sim20$ agents due to exponential\ncomplexity $\\mathcal{O}(2^m)$. While Monte Carlo sampling methods provide\nstatistical estimates, they suffer from high sample complexity and cannot\ntransfer knowledge across different network configurations, making them\nimpractical for large-scale or dynamic systems. We present a novel\nlearning-based approach using Graph Neural Networks (GNNs) to approximate\nBanzhaf values in cardinal network flow games. By framing the problem as a\ngraph-level prediction task, our method learns generalisable patterns of agent\ninfluence directly from network topology and control structure. We conduct a\ncomprehensive empirical study comparing three state-of-the-art GNN\narchitectures-Graph Attention Networks (GAT), Graph Isomorphism Networks with\nEdge features (GINE), and EdgeConv-on a large-scale synthetic dataset of\n200,000 graphs per configuration, varying in size (20-100 nodes), agent count\n(5-20), and edge probability (0.5-1.0). Our results demonstrate that trained\nGNN models achieve high-fidelity Banzhaf value approximation with\norder-of-magnitude speedups compared to exact and sampling-based methods. Most\nsignificantly, we show strong zero-shot generalisation: models trained on\ngraphs of a specific size and topology accurately predict Banzhaf values for\nentirely new networks with different structural properties, without requiring\nretraining. This work establishes GNNs as a practical tool for scalable\ncooperative game-theoretic analysis of complex networked systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u6765\u8fd1\u4f3c\u8ba1\u7b97\u7f51\u7edc\u6d41\u535a\u5f08\u4e2d\u7684Banzhaf\u503c\uff0c\u76f8\u6bd4\u4f20\u7edf\u7cbe\u786e\u8ba1\u7b97\u548c\u8499\u7279\u5361\u6d1b\u91c7\u6837\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u79c0\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfBanzhaf\u503c\u8ba1\u7b97\u65b9\u6cd5\u5728\u8d85\u8fc7\u7ea620\u4e2a\u4ee3\u7406\u65f6\u56e0\u6307\u6570\u590d\u6742\u5ea6\u53d8\u5f97\u4e0d\u53ef\u884c\uff0c\u800c\u8499\u7279\u5361\u6d1b\u91c7\u6837\u65b9\u6cd5\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u4e14\u65e0\u6cd5\u5728\u4e0d\u540c\u7f51\u7edc\u914d\u7f6e\u95f4\u8fc1\u79fb\u77e5\u8bc6\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u6216\u52a8\u6001\u7cfb\u7edf\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u56fe\u7ea7\u9884\u6d4b\u4efb\u52a1\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u4ece\u7f51\u7edc\u62d3\u6251\u548c\u63a7\u5236\u7ed3\u6784\u4e2d\u5b66\u4e60\u4ee3\u7406\u5f71\u54cd\u529b\u7684\u901a\u7528\u6a21\u5f0f\uff0c\u6bd4\u8f83\u4e86GAT\u3001GINE\u548cEdgeConv\u4e09\u79cd\u6700\u5148\u8fdb\u7684GNN\u67b6\u6784\u3002", "result": "\u5728\u5305\u542b200,000\u4e2a\u56fe\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8bad\u7ec3\u540e\u7684GNN\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684Banzhaf\u503c\u8fd1\u4f3c\uff0c\u76f8\u6bd4\u7cbe\u786e\u548c\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\u83b7\u5f97\u4e86\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u786e\u7acb\u4e86GNN\u4f5c\u4e3a\u590d\u6742\u7f51\u7edc\u7cfb\u7edf\u53ef\u6269\u5c55\u5408\u4f5c\u535a\u5f08\u8bba\u5206\u6790\u7684\u5b9e\u7528\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.13312", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13312", "abs": "https://arxiv.org/abs/2510.13312", "authors": ["Simon Lupart", "Mohammad Aliannejadi", "Evangelos Kanoulas"], "title": "ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering", "comment": null, "summary": "We present ChatR1, a reasoning framework based on reinforcement learning (RL)\nfor conversational question answering (CQA). Reasoning plays an important role\nin CQA, where user intent evolves across dialogue turns, and utterances are\noften underspecified, requiring contextual interpretation, query reformulation,\nand dynamic coordination between retrieval and generation. Unlike static\n`rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and\nreasoning across turns, enabling exploratory and adaptive behaviors learned\nthrough RL. To address the challenge of sparse and delayed rewards in RL, we\npropose an intent-aware reward that provides turn-level feedback by aligning\nretrieval and reasoning with evolving user goals. Our proposed ChatR1\ndemonstrates strong performance on both 3B and 7B model backbones,\noutperforming competitive models on five CQA datasets, measured by different\nmetrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA\ndatasets to cover topic shifts, evolving intents, mixed-initiative dialogues,\nand multi-document grounding, testing ChatR1's performance from various\naspects. Ablation studies confirm the effectiveness of the intent-aware reward.\nOur analyses further reveal diverse reasoning trajectories and effective use of\nthe search tool. ChatR1 also generalizes robustly across domains, demonstrating\nthat RL-based reasoning enables more flexible and context-sensitive behavior\nthan static CQA pipelines.", "AI": {"tldr": "ChatR1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u8bdd\u95ee\u7b54\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u9519\u641c\u7d22\u548c\u63a8\u7406\u6765\u5904\u7406\u7528\u6237\u610f\u56fe\u7684\u52a8\u6001\u6f14\u53d8\uff0c\u4f7f\u7528\u610f\u56fe\u611f\u77e5\u5956\u52b1\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5bf9\u8bdd\u95ee\u7b54\u4e2d\u7528\u6237\u610f\u56fe\u4f1a\u968f\u5bf9\u8bdd\u8f6e\u6b21\u6f14\u53d8\uff0c\u4e14\u8bdd\u8bed\u5f80\u5f80\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u4e0a\u4e0b\u6587\u89e3\u91ca\u3001\u67e5\u8be2\u91cd\u6784\u4ee5\u53ca\u68c0\u7d22\u4e0e\u751f\u6210\u7684\u52a8\u6001\u534f\u8c03\uff0c\u9759\u6001\u7ba1\u9053\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u7406\u6846\u67b6\uff0c\u4ea4\u9519\u8fdb\u884c\u641c\u7d22\u548c\u63a8\u7406\uff0c\u91c7\u7528\u610f\u56fe\u611f\u77e5\u5956\u52b1\u63d0\u4f9b\u8f6e\u6b21\u7ea7\u53cd\u9988\uff0c\u4f7f\u68c0\u7d22\u548c\u63a8\u7406\u4e0e\u7528\u6237\u76ee\u6807\u5bf9\u9f50\u3002", "result": "\u57283B\u548c7B\u6a21\u578b\u9aa8\u5e72\u4e0a\u5747\u8868\u73b0\u5f3a\u52b2\uff0c\u5728\u4e94\u4e2a\u5bf9\u8bdd\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u7ade\u4e89\u6a21\u578b\uff0c\u901a\u8fc7F1\u3001BERTScore\u548cLLM-as-judge\u7b49\u6307\u6807\u9a8c\u8bc1\u3002", "conclusion": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u7406\u6bd4\u9759\u6001\u5bf9\u8bdd\u95ee\u7b54\u7ba1\u9053\u66f4\u7075\u6d3b\u548c\u4e0a\u4e0b\u6587\u654f\u611f\uff0c\u80fd\u591f\u5b9e\u73b0\u591a\u6837\u5316\u7684\u63a8\u7406\u8f68\u8ff9\u548c\u6709\u6548\u7684\u641c\u7d22\u5de5\u5177\u4f7f\u7528\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.13363", "categories": ["cs.CL", "68T50, 68T30", "I.2.7; I.2.4"], "pdf": "https://arxiv.org/pdf/2510.13363", "abs": "https://arxiv.org/abs/2510.13363", "authors": ["Xiang Lei", "Qin Li", "Min Zhang", "Min Zhang"], "title": "D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree", "comment": "8 pages, 6 figures (main content); 25 pages, 18 figures (total)", "summary": "Large Language Models (LLMs) often exhibit factual inconsistencies and\nlogical decay in extended, multi-turn dialogues, a challenge stemming from\ntheir reliance on static, pre-trained knowledge and an inability to reason\nadaptively over the dialogue history. Prevailing mitigation strategies, such as\nRetrieval-Augmented Generation (RAG) and agentic working memories, improve\ninformation recall but still engage with fundamentally static knowledge sources\nand follow pre-defined single reasoning path. This hinders their ability to\npreserve factual and logical consistency of their responses in multi-turn\ndialogues while the context evolves over time. To address this issue, we\npropose D-SMART, a model-agnostic framework designed to maintain multi-turn\ndialogue consistency by enabling LLMs to build and reason over a dynamic,\nstructured representation of the conversational context. This is achieved via\ntwo synergistic components: (1) a Dynamic Structured Memory (DSM), which\nincrementally constructs and maintains an authoritative, OWL-compliant\nknowledge graph of the conversation; and (2) a Reasoning Tree (RT), which\nexecutes inferences as an explicit and traceable multi-step search over the\ngraph. As the popular-used quality score (judged by GPT-4) can overlook logical\nflaws, we introduce new NLI-based metrics to better measure multi-turn dialogue\nconsistency. Comprehensive experiments on the MT-Bench-101 benchmark show that\nD-SMART significantly outperforms state-of-the-art baselines, elevating the\ndialogue consistency score by over 48\\% for both proprietary and open-source\nmodels, and notably improves the quality score of the latter by up to 10.1\\%.", "AI": {"tldr": "D-SMART\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7ed3\u6784\u5316\u8bb0\u5fc6\u548c\u63a8\u7406\u6811\u6765\u7ef4\u62a4\u591a\u8f6e\u5bf9\u8bdd\u7684\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u51fa\u73b0\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u548c\u903b\u8f91\u8870\u51cf\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5982RAG\u548c\u4ee3\u7406\u5de5\u4f5c\u8bb0\u5fc6\u4ecd\u4f9d\u8d56\u9759\u6001\u77e5\u8bc6\u6e90\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u3002", "method": "\u4f7f\u7528\u52a8\u6001\u7ed3\u6784\u5316\u8bb0\u5fc6(DSM)\u6784\u5efa\u6743\u5a01\u77e5\u8bc6\u56fe\u8c31\uff0c\u7ed3\u5408\u63a8\u7406\u6811(RT)\u8fdb\u884c\u591a\u6b65\u56fe\u641c\u7d22\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eNLI\u7684\u65b0\u6307\u6807\u6765\u8bc4\u4f30\u5bf9\u8bdd\u4e00\u81f4\u6027\u3002", "result": "\u5728MT-Bench-101\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cD-SMART\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5bf9\u8bdd\u4e00\u81f4\u6027\u5f97\u5206\u63d0\u5347\u8d85\u8fc748%\uff0c\u5f00\u6e90\u6a21\u578b\u8d28\u91cf\u5f97\u5206\u63d0\u5347\u8fbe10.1%\u3002", "conclusion": "D-SMART\u901a\u8fc7\u52a8\u6001\u7ed3\u6784\u5316\u8868\u793a\u548c\u663e\u5f0f\u63a8\u7406\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3aLLMs\u5728\u590d\u6742\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.13387", "categories": ["cs.CL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.13387", "abs": "https://arxiv.org/abs/2510.13387", "authors": ["Buwei He", "Yang Liu", "Zhaowei Zhang", "Zixia Jia", "Huijia Wu", "Zhaofeng He", "Zilong Zheng", "Yipeng Kang"], "title": "Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment", "comment": null, "summary": "Persuasion, a fundamental social capability for humans, remains a challenge\nfor AI systems such as large language models (LLMs). Current studies often\noverlook the strategic use of information asymmetry in message design or rely\non strong assumptions regarding pre-commitment. In this work, we explore the\napplication of Bayesian Persuasion (BP) in natural language within single-turn\ndialogue settings, to enhance the strategic persuasion capabilities of LLMs.\nOur framework incorporates a commitment-communication mechanism, where the\npersuader explicitly outlines an information schema by narrating their\npotential types (e.g., honest or dishonest), thereby guiding the persuadee in\nperforming the intended Bayesian belief update. We evaluate two variants of our\napproach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language\n(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)\nbaselines within a comprehensive evaluation framework. This framework covers a\ndiverse set of persuadees -- including LLM instances with varying prompts and\nfine-tuning and human participants -- across tasks ranging from specially\ndesigned persuasion scenarios to general everyday situations. Experimental\nresults on LLM-based agents reveal three main findings: (1) LLMs guided by BP\nstrategies consistently achieve higher persuasion success rates than NBP\nbaselines; (2) SFNL exhibits greater credibility and logical coherence, while\nFNL shows stronger emotional resonance and robustness in naturalistic\nconversations; (3) with supervised fine-tuning, smaller models can attain BP\nperformance comparable to that of larger models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4e86\u5728\u5355\u8f6e\u5bf9\u8bdd\u4e2d\u5e94\u7528\u8d1d\u53f6\u65af\u8bf4\u670d\u6846\u67b6\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6218\u7565\u8bf4\u670d\u80fd\u529b\uff0c\u901a\u8fc7\u627f\u8bfa-\u6c9f\u901a\u673a\u5236\u8ba9\u8bf4\u670d\u8005\u660e\u786e\u63cf\u8ff0\u4fe1\u606f\u6a21\u5f0f\uff0c\u5f15\u5bfc\u88ab\u8bf4\u670d\u8005\u8fdb\u884c\u8d1d\u53f6\u65af\u4fe1\u5ff5\u66f4\u65b0\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u8bf4\u670d\u80fd\u529b\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u5ffd\u89c6\u4fe1\u606f\u4e0d\u5bf9\u79f0\u7684\u6218\u7565\u4f7f\u7528\u6216\u4f9d\u8d56\u5f3a\u5047\u8bbe\u7684\u9884\u5148\u627f\u8bfa\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u8d1d\u53f6\u65af\u8bf4\u670d\u65b9\u6cd5\uff1a\u534a\u6b63\u5f0f\u81ea\u7136\u8bed\u8a00\u548c\u5168\u81ea\u7136\u8bed\u8a00\uff0c\u5e76\u5728\u5305\u542b\u591a\u6837\u5316\u88ab\u8bf4\u670d\u8005\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u4e2d\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a1\uff09\u57fa\u4e8e\u8d1d\u53f6\u65af\u8bf4\u670d\u7684LLM\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u8bf4\u670d\u6210\u529f\u7387\uff1b2\uff09\u534a\u6b63\u5f0f\u81ea\u7136\u8bed\u8a00\u7248\u672c\u66f4\u5177\u53ef\u4fe1\u5ea6\u548c\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u5168\u81ea\u7136\u8bed\u8a00\u7248\u672c\u5728\u60c5\u611f\u5171\u9e23\u548c\u9c81\u68d2\u6027\u65b9\u9762\u66f4\u5f3a\uff1b3\uff09\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff0c\u5c0f\u6a21\u578b\u80fd\u8fbe\u5230\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8d1d\u53f6\u65af\u8bf4\u670d\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u6218\u7565\u8bf4\u670d\u80fd\u529b\uff0c\u4e0d\u540c\u65b9\u6cd5\u53d8\u4f53\u5404\u6709\u4f18\u52bf\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u4e0d\u662f\u6027\u80fd\u7684\u51b3\u5b9a\u6027\u56e0\u7d20\u3002", "topic": "agent analysis"}}
{"id": "2510.13570", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13570", "abs": "https://arxiv.org/abs/2510.13570", "authors": ["Ivan Dubrovsky", "Anastasia Orlova", "Illarion Iov", "Nina Gubina", "Irena Gureeva", "Alexey Zaytsev"], "title": "Selective Adversarial Attacks on LLM Benchmarks", "comment": null, "summary": "Benchmarking outcomes increasingly govern trust, selection, and deployment of\nLLMs, yet these evaluations remain vulnerable to semantically equivalent\nadversarial perturbations. Prior work on adversarial robustness in NLP has\nemphasized text attacks that affect many models equally, leaving open the\nquestion of whether it is possible to selectively degrade or enhance\nperformance while minimally affecting other models. We formalize this problem\nand study selective adversarial attacks on MMLU - a widely used benchmark\ndesigned to measure a language model's broad general knowledge and reasoning\nability across different subjects. Using canonical attacks integrated into\nTextAttack framework, we introduce a protocol for selectivity assessment,\ndevelop a custom constraint to increase selectivity of attacks and propose a\nsurrogate-LLM pipeline that generates selective perturbations. Empirically, we\nfind that selective adversarial attacks exist and can materially alter relative\nrankings, challenging the fairness, reproducibility, and transparency of\nleaderboard-driven evaluation. Our results motivate perturbation-aware\nreporting and robustness diagnostics for LLM evaluation and demonstrate that\neven subtle edits can shift comparative judgments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9009\u62e9\u6027\u5bf9\u6297\u653b\u51fb\u5bf9LLM\u57fa\u51c6\u6d4b\u8bd5\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5373\u4f7f\u5fae\u5c0f\u7684\u6587\u672c\u6270\u52a8\u4e5f\u80fd\u663e\u8457\u6539\u53d8\u6a21\u578b\u5728MMLU\u57fa\u51c6\u4e0a\u7684\u76f8\u5bf9\u6392\u540d\uff0c\u6311\u6218\u4e86\u6392\u884c\u699c\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002", "motivation": "\u5f53\u524dLLM\u57fa\u51c6\u6d4b\u8bd5\u5bb9\u6613\u53d7\u5230\u8bed\u4e49\u7b49\u6548\u7684\u5bf9\u6297\u6027\u6270\u52a8\u5f71\u54cd\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bf9\u6240\u6709\u6a21\u578b\u4ea7\u751f\u540c\u7b49\u5f71\u54cd\u7684\u653b\u51fb\uff0c\u7f3a\u4e4f\u5bf9\u9009\u62e9\u6027\u653b\u51fb\uff08\u5373\u53ea\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u6027\u80fd\uff09\u7684\u7814\u7a76\u3002", "method": "\u4f7f\u7528TextAttack\u6846\u67b6\u4e2d\u7684\u6807\u51c6\u653b\u51fb\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u9009\u62e9\u6027\u8bc4\u4f30\u534f\u8bae\u548c\u81ea\u5b9a\u4e49\u7ea6\u675f\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u4ee3\u7406LLM\u7684\u7ba1\u9053\u6765\u751f\u6210\u9009\u62e9\u6027\u6270\u52a8\u3002", "result": "\u5b9e\u8bc1\u53d1\u73b0\u9009\u62e9\u6027\u5bf9\u6297\u653b\u51fb\u786e\u5b9e\u5b58\u5728\uff0c\u80fd\u591f\u663e\u8457\u6539\u53d8\u6a21\u578b\u5728MMLU\u57fa\u51c6\u4e0a\u7684\u76f8\u5bf9\u6392\u540d\uff0c\u5f71\u54cd\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u9700\u8981\u91c7\u7528\u6270\u52a8\u611f\u77e5\u7684\u62a5\u544a\u548c\u9c81\u68d2\u6027\u8bca\u65ad\u65b9\u6cd5\u6765\u8fdb\u884cLLM\u8bc4\u4f30\uff0c\u5373\u4f7f\u662f\u5fae\u5c0f\u7684\u7f16\u8f91\u4e5f\u53ef\u80fd\u6539\u53d8\u6bd4\u8f83\u5224\u65ad\u3002", "topic": "agent analysis"}}
{"id": "2510.13554", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13554", "abs": "https://arxiv.org/abs/2510.13554", "authors": ["Yang Li", "Zhichen Dong", "Yuhan Sun", "Weixun Wang", "Shaopan Xiong", "Yijia Luo", "Jiashun Liu", "Han Lu", "Jiamang Wang", "Wenbo Su", "Bo Zheng", "Junchi Yan"], "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization", "comment": "23 pages, 8 figures, 5 tables", "summary": "The reasoning pattern of Large language models (LLMs) remains opaque, and\nReinforcement learning (RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguish attention heads\nbetween locally and globally focused information processing and reveal that\nlocally focused heads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, while globally focused heads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1) Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2) Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically perform targeted credit assignment to critical nodes (preplan\ntokens, anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u673a\u5236\u63ed\u793aLLM\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e24\u79cd\u6ce8\u610f\u529b\u5ea6\u91cf\u6307\u6807\uff0c\u53d1\u73b0\u9884\u89c4\u5212-\u951a\u5b9a\u673a\u5236\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u4e09\u79cd\u9488\u5bf9\u5173\u952e\u8282\u70b9\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "motivation": "LLM\u7684\u63a8\u7406\u6a21\u5f0f\u4e0d\u900f\u660e\uff0c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5bf9\u751f\u6210\u5185\u5bb9\u5747\u5300\u5206\u914d\u4fe1\u7528\uff0c\u65e0\u6cd5\u533a\u5206\u5173\u952e\u6b65\u9aa4\u548c\u5e38\u89c4\u6b65\u9aa4\u3002", "method": "1) \u533a\u5206\u5c40\u90e8\u548c\u5168\u5c40\u6ce8\u610f\u529b\u5934\uff1b2) \u63d0\u51fa\u7a97\u53e3\u5e73\u5747\u6ce8\u610f\u529b\u8ddd\u79bb\u548c\u672a\u6765\u6ce8\u610f\u529b\u5f71\u54cd\u4e24\u4e2a\u6307\u6807\uff1b3) \u53d1\u73b0\u9884\u89c4\u5212-\u951a\u5b9a\u673a\u5236\uff1b4) \u5f00\u53d1\u4e09\u79cd\u9488\u5bf9\u5173\u952e\u8282\u70b9\u7684RL\u7b56\u7565\u3002", "result": "\u63ed\u793a\u4e86LLM\u63a8\u7406\u4e2d\u7684\u9884\u89c4\u5212-\u951a\u5b9a\u673a\u5236\uff0c\u63d0\u51fa\u7684RL\u7b56\u7565\u5728\u5404\u79cd\u63a8\u7406\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4f18\u5316\u4e0e\u6a21\u578b\u5185\u5728\u63a8\u7406\u8282\u594f\u5bf9\u9f50\uff0c\u5c06\u4e0d\u900f\u660e\u7684\u4f18\u5316\u8f6c\u53d8\u4e3a\u7ed3\u6784\u611f\u77e5\u7684\u8fc7\u7a0b\uff0c\u4e3aLLM\u63a8\u7406\u7684\u900f\u660e\u6709\u6548\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u80fd\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.13704", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13704", "abs": "https://arxiv.org/abs/2510.13704", "authors": ["Johan Obando-Ceron", "Walter Mayor", "Samuel Lavoie", "Scott Fujimoto", "Aaron Courville", "Pablo Samuel Castro"], "title": "Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents", "comment": null, "summary": "Recent works have proposed accelerating the wall-clock training time of\nactor-critic methods via the use of large-scale environment parallelization;\nunfortunately, these can sometimes still require large number of environment\ninteractions to achieve a desired level of performance. Noting that\nwell-structured representations can improve the generalization and sample\nefficiency of deep reinforcement learning (RL) agents, we propose the use of\nsimplicial embeddings: lightweight representation layers that constrain\nembeddings to simplicial structures. This geometric inductive bias results in\nsparse and discrete features that stabilize critic bootstrapping and strengthen\npolicy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial\nembeddings consistently improve sample efficiency and final performance across\na variety of continuous- and discrete-control environments, without any loss in\nruntime speed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u5355\u7eaf\u5f62\u5d4c\u5165\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u8868\u793a\u5c42\uff0c\u901a\u8fc7\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u4ea7\u751f\u7a00\u758f\u79bb\u6563\u7279\u5f81\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\uff0c\u4e14\u4e0d\u5f71\u54cd\u8fd0\u884c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u73af\u5883\u5e76\u884c\u5316\u65b9\u6cd5\u867d\u7136\u80fd\u52a0\u901f\u8bad\u7ec3\uff0c\u4f46\u4ecd\u9700\u5927\u91cf\u73af\u5883\u4ea4\u4e92\u3002\u7ed3\u6784\u826f\u597d\u7684\u8868\u793a\u53ef\u4ee5\u6539\u5584\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5355\u7eaf\u5f62\u5d4c\u5165\u65b9\u6cd5\uff0c\u5c06\u5d4c\u5165\u7ea6\u675f\u5230\u5355\u7eaf\u5f62\u7ed3\u6784\u4e2d\uff0c\u4ea7\u751f\u7a00\u758f\u79bb\u6563\u7279\u5f81\uff0c\u7a33\u5b9a\u8bc4\u8bba\u5bb6\u81ea\u4e3e\u5e76\u589e\u5f3a\u7b56\u7565\u68af\u5ea6\u3002\u5e94\u7528\u4e8eFastTD3\u3001FastSAC\u548cPPO\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u8fde\u7eed\u548c\u79bb\u6563\u63a7\u5236\u73af\u5883\u4e2d\uff0c\u5355\u7eaf\u5f62\u5d4c\u5165\u4e00\u81f4\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\uff0c\u4e14\u6ca1\u6709\u635f\u5931\u8fd0\u884c\u901f\u5ea6\u3002", "conclusion": "\u5355\u7eaf\u5f62\u5d4c\u5165\u662f\u4e00\u79cd\u6709\u6548\u7684\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\u800c\u4e0d\u5f71\u54cd\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.13694", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13694", "abs": "https://arxiv.org/abs/2510.13694", "authors": ["Yuchun Miao", "Liang Ding", "Sen Zhang", "Rong Bao", "Lefei Zhang", "Dacheng Tao"], "title": "Information-Theoretic Reward Modeling for Stable RLHF: Detecting and Mitigating Reward Hacking", "comment": "46 pages, 36 figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence", "summary": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in\naligning language models with human values, reward hacking-or reward\nover-optimization-remains a major challenge. We identify two key obstacles to\nits mitigation: (1) reward misgeneralization in reward modeling, where reward\nmodels overfit to spurious, preference-irrelevant features; and (2) the lack of\nsuitable regularization during RL optimization, as existing token-level\nconstraints often over-restrict the policy space. To address these issues, we\npropose InfoRM, an information-theoretic reward modeling framework based on the\nInformation Bottleneck (IB) principle, which filters out preference-irrelevant\ninformation to alleviate reward misgeneralization. We further observe that\nreward-hacked responses manifest as pronounced outliers in InfoRM's IB latent\nspace, measured by Mahalanobis distance from the SFT-induced distribution.\nMotivated by this, we introduce IBL, a distribution-level regularization that\npenalizes such deviations, effectively expanding the optimization landscape\nwhile maintaining alignment. We prove that IBL is theoretically equivalent to\nthe pessimistic RL objective within the IB latent space. Finally, we present\nMahalanobis Outlier Probability (MOP), a statistical metric for quantifying\nreward hacking severity, enabling principled hyperparameter tuning and online\nmitigation such as early stopping. Extensive experiments across diverse LLMs\nand datasets confirm the generality of our findings, the effectiveness of\nInfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively\nadvancing the state of RLHF.", "AI": {"tldr": "\u63d0\u51faInfoRM\u548cIBL\u6846\u67b6\u89e3\u51b3RLHF\u4e2d\u7684\u5956\u52b1\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u539f\u7406\u8fc7\u6ee4\u504f\u597d\u65e0\u5173\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u5206\u5e03\u7ea7\u6b63\u5219\u5316\u9632\u6b62\u7b56\u7565\u504f\u79bb\u3002", "motivation": "RLHF\u5728\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u9762\u4e34\u5956\u52b1\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u4e3b\u8981\u969c\u788d\u5305\u62ec\u5956\u52b1\u6a21\u578b\u7684\u9519\u8bef\u6cdb\u5316\u548cRL\u4f18\u5316\u4e2d\u7f3a\u4e4f\u5408\u9002\u7684\u6b63\u5219\u5316\u7ea6\u675f\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u539f\u7406\u7684InfoRM\u5956\u52b1\u5efa\u6a21\u6846\u67b6\uff0c\u8fc7\u6ee4\u504f\u597d\u65e0\u5173\u4fe1\u606f\uff1bIBL\u5206\u5e03\u7ea7\u6b63\u5219\u5316\uff0c\u60e9\u7f5a\u4e0eSFT\u5206\u5e03\u7684\u504f\u79bb\uff1bMOP\u7edf\u8ba1\u6307\u6807\u91cf\u5316\u5956\u52b1\u8fc7\u4f18\u5316\u7a0b\u5ea6\u3002", "result": "\u5728\u591a\u79cdLLM\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0cInfoRM\u548cIBL\u80fd\u6709\u6548\u7f13\u89e3\u5956\u52b1\u8fc7\u4f18\u5316\uff0cMOP\u4f5c\u4e3a\u8bca\u65ad\u5de5\u5177\u53ef\u9760\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u65b9\u6cd5\u663e\u8457\u63a8\u8fdb\u4e86RLHF\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4e3a\u89e3\u51b3\u5956\u52b1\u8fc7\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.13786", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13786", "abs": "https://arxiv.org/abs/2510.13786", "authors": ["Devvrit Khatri", "Lovish Madaan", "Rishabh Tiwari", "Rachit Bansal", "Sai Surya Duvvuri", "Manzil Zaheer", "Inderjit S. Dhillon", "David Brandfonbrener", "Rishabh Agarwal"], "title": "The Art of Scaling Reinforcement Learning Compute for LLMs", "comment": "28 pages, 20 figures", "summary": "Reinforcement learning (RL) has become central to training large language\nmodels (LLMs), yet the field lacks predictive scaling methodologies comparable\nto those established for pre-training. Despite rapidly rising compute budgets,\nthere is no principled understanding of how to evaluate algorithmic\nimprovements for scaling RL compute. We present the first large-scale\nsystematic study, amounting to more than 400,000 GPU-hours, that defines a\nprincipled framework for analyzing and predicting RL scaling in LLMs. We fit\nsigmoidal compute-performance curves for RL training and ablate a wide range of\ncommon design choices to analyze their effects on asymptotic performance and\ncompute efficiency. We observe: (1) Not all recipes yield similar asymptotic\nperformance, (2) Details such as loss aggregation, normalization, curriculum,\nand off-policy algorithm primarily modulate compute efficiency without\nmaterially shifting the asymptote, and (3) Stable, scalable recipes follow\npredictable scaling trajectories, enabling extrapolation from smaller-scale\nruns. Combining these insights, we propose a best-practice recipe, ScaleRL, and\ndemonstrate its effectiveness by successfully scaling and predicting validation\nperformance on a single RL run scaled up to 100,000 GPU-hours. Our work\nprovides both a scientific framework for analyzing scaling in RL and a\npractical recipe that brings RL training closer to the predictability long\nachieved in pre-training.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u5efa\u7acb\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53ef\u9884\u6d4b\u7f29\u653e\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u8fc740\u4e07GPU\u5c0f\u65f6\u7684\u5b9e\u9a8c\u5b9a\u4e49\u4e86RL\u7f29\u653e\u539f\u5219\uff0c\u5e76\u63d0\u51fa\u4e86\u6700\u4f73\u5b9e\u8df5\u914d\u65b9ScaleRL\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5df2\u6210\u4e3a\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u6280\u672f\uff0c\u4f46\u8be5\u9886\u57df\u7f3a\u4e4f\u4e0e\u9884\u8bad\u7ec3\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u7f29\u653e\u65b9\u6cd5\u3002\u968f\u7740\u8ba1\u7b97\u9884\u7b97\u7684\u5feb\u901f\u589e\u957f\uff0c\u5982\u4f55\u8bc4\u4f30RL\u8ba1\u7b97\u7f29\u653e\u7684\u7b97\u6cd5\u6539\u8fdb\u7f3a\u4e4f\u539f\u5219\u6027\u7406\u89e3\u3002", "method": "\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u7cfb\u7edf\u6027\u7814\u7a76\uff08\u8d85\u8fc740\u4e07GPU\u5c0f\u65f6\uff09\uff0c\u62df\u5408RL\u8bad\u7ec3\u7684S\u578b\u8ba1\u7b97-\u6027\u80fd\u66f2\u7ebf\uff0c\u6d88\u878d\u5206\u6790\u5404\u79cd\u5e38\u89c1\u8bbe\u8ba1\u9009\u62e9\u5bf9\u6e10\u8fdb\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\uff1a(1) \u4e0d\u540c\u914d\u65b9\u4ea7\u751f\u4e0d\u540c\u7684\u6e10\u8fdb\u6027\u80fd\uff1b(2) \u635f\u5931\u805a\u5408\u3001\u5f52\u4e00\u5316\u3001\u8bfe\u7a0b\u5b66\u4e60\u548c\u79bb\u7b56\u7565\u7b97\u6cd5\u7b49\u7ec6\u8282\u4e3b\u8981\u8c03\u8282\u8ba1\u7b97\u6548\u7387\u800c\u4e0d\u663e\u8457\u6539\u53d8\u6e10\u8fdb\u6027\u80fd\uff1b(3) \u7a33\u5b9a\u53ef\u6269\u5c55\u7684\u914d\u65b9\u9075\u5faa\u53ef\u9884\u6d4b\u7684\u7f29\u653e\u8f68\u8ff9\u3002\u63d0\u51fa\u7684ScaleRL\u914d\u65b9\u572810\u4e07GPU\u5c0f\u65f6\u7684\u5355\u4e00RL\u8fd0\u884c\u4e2d\u6210\u529f\u7f29\u653e\u5e76\u9884\u6d4b\u9a8c\u8bc1\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5206\u6790RL\u7f29\u653e\u63d0\u4f9b\u4e86\u79d1\u5b66\u6846\u67b6\u548c\u5b9e\u7528\u914d\u65b9\uff0c\u4f7fRL\u8bad\u7ec3\u66f4\u63a5\u8fd1\u9884\u8bad\u7ec3\u957f\u671f\u5b9e\u73b0\u7684\u53ef\u9884\u6d4b\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.13792", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13792", "abs": "https://arxiv.org/abs/2510.13792", "authors": ["Ziqing Lu", "Lifeng Lai", "Weiyu Xu"], "title": "Provably Invincible Adversarial Attacks on Reinforcement Learning Systems: A Rate-Distortion Information-Theoretic Approach", "comment": null, "summary": "Reinforcement learning (RL) for the Markov Decision Process (MDP) has emerged\nin many security-related applications, such as autonomous driving, financial\ndecisions, and drone/robot algorithms. In order to improve the\nrobustness/defense of RL systems against adversaries, studying various\nadversarial attacks on RL systems is very important. Most previous work\nconsidered deterministic adversarial attack strategies in MDP, which the\nrecipient (victim) agent can defeat by reversing the deterministic attacks. In\nthis paper, we propose a provably ``invincible'' or ``uncounterable'' type of\nadversarial attack on RL. The attackers apply a rate-distortion\ninformation-theoretic approach to randomly change agents' observations of the\ntransition kernel (or other properties) so that the agent gains zero or very\nlimited information about the ground-truth kernel (or other properties) during\nthe training. We derive an information-theoretic lower bound on the recipient\nagent's reward regret and show the impact of rate-distortion attacks on\nstate-of-the-art model-based and model-free algorithms. We also extend this\nnotion of an information-theoretic approach to other types of adversarial\nattack, such as state observation attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\"\u4e0d\u53ef\u6218\u80dc\"\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7387\u5931\u771f\u7406\u8bba\u968f\u673a\u6539\u53d8\u667a\u80fd\u4f53\u5bf9\u8f6c\u79fb\u6838\u7684\u89c2\u6d4b\uff0c\u4f7f\u5176\u5728\u8bad\u7ec3\u4e2d\u83b7\u5f97\u96f6\u6216\u6709\u9650\u7684\u5730\u9762\u771f\u5b9e\u4fe1\u606f\u3002", "motivation": "\u63d0\u9ad8RL\u7cfb\u7edf\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u9700\u8981\u7814\u7a76\u5404\u79cd\u5bf9\u6297\u653b\u51fb\u7b56\u7565\u3002\u4ee5\u5f80\u786e\u5b9a\u6027\u653b\u51fb\u53ef\u88ab\u53d7\u5bb3\u8005\u667a\u80fd\u4f53\u9006\u8f6c\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7387\u5931\u771f\u4fe1\u606f\u8bba\u65b9\u6cd5\uff0c\u968f\u673a\u6539\u53d8\u667a\u80fd\u4f53\u5bf9\u8f6c\u79fb\u6838\u6216\u5176\u4ed6\u5c5e\u6027\u7684\u89c2\u6d4b\uff0c\u9650\u5236\u667a\u80fd\u4f53\u83b7\u53d6\u5730\u9762\u771f\u5b9e\u4fe1\u606f\u3002\u63a8\u5bfc\u4e86\u53d7\u5bb3\u8005\u667a\u80fd\u4f53\u5956\u52b1\u9057\u61be\u7684\u4fe1\u606f\u8bba\u4e0b\u754c\u3002", "result": "\u8bc1\u660e\u4e86\u8fd9\u79cd\u653b\u51fb\u5bf9\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6a21\u578b\u548c\u65e0\u6a21\u578b\u7b97\u6cd5\u7684\u5f71\u54cd\uff0c\u5e76\u5c06\u8be5\u65b9\u6cd5\u6269\u5c55\u5230\u72b6\u6001\u89c2\u6d4b\u653b\u51fb\u7b49\u5176\u4ed6\u7c7b\u578b\u7684\u5bf9\u6297\u653b\u51fb\u3002", "conclusion": "\u63d0\u51fa\u7684\u4fe1\u606f\u8bba\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\"\u4e0d\u53ef\u6218\u80dc\"\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u4e3aRL\u7cfb\u7edf\u7684\u5b89\u5168\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u7684\u6311\u6218\u548c\u89c6\u89d2\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.13748", "categories": ["cs.LG", "math.PR", "stat.ML", "90C40, 62H30, 60J20"], "pdf": "https://arxiv.org/pdf/2510.13748", "abs": "https://arxiv.org/abs/2510.13748", "authors": ["Thomas van Vuren", "Fiona Sloothaak", "Maarten G. Wolf", "Jaron Sanders"], "title": "Asymptotically optimal reinforcement learning in Block Markov Decision Processes", "comment": "74 pages, 3 figures", "summary": "The curse of dimensionality renders Reinforcement Learning (RL) impractical\nin many real-world settings with exponentially large state and action spaces.\nYet, many environments exhibit exploitable structure that can accelerate\nlearning. To formalize this idea, we study RL in Block Markov Decision\nProcesses (BMDPs). BMDPs model problems with large observation spaces, but\nwhere transition dynamics are fully determined by latent states. Recent\nadvances in clustering methods have enabled the efficient recovery of this\nlatent structure. However, a regret analysis that exploits these techniques to\ndetermine their impact on learning performance remained open. We are now\naddressing this gap by providing a regret analysis that explicitly leverages\nclustering, demonstrating that accurate latent state estimation can indeed\neffectively speed up learning.\n  Concretely, this paper analyzes a two-phase RL algorithm for BMDPs that first\nlearns the latent structure through random exploration and then switches to an\noptimism-guided strategy adapted to the uncovered structure. This algorithm\nachieves a regret that is $O(\\sqrt{T}+n)$ on a large class of BMDPs susceptible\nto clustering. Here, $T$ denotes the number of time steps, $n$ is the\ncardinality of the observation space, and the Landau notation $O(\\cdot)$ holds\nup to constants and polylogarithmic factors. This improves the best prior\nbound, $O(\\sqrt{T}+n^2)$, especially when $n$ is large. Moreover, we prove that\nno algorithm can achieve lower regret uniformly on this same class of BMDPs.\nThis establishes that, on this class, the algorithm achieves asymptotic\noptimality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5757\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u5b66\u4e60\u6f5c\u5728\u72b6\u6001\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4f18\u7684\u9057\u61be\u754cO(\u221aT+n)\uff0c\u5e76\u5728\u8be5\u7c7b\u95ee\u9898\u4e0a\u8fbe\u5230\u6e10\u8fd1\u6700\u4f18\u3002", "motivation": "\u9ad8\u7ef4\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u4f7f\u5f97\u5f3a\u5316\u5b66\u4e60\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u53ef\u884c\uff0c\u4f46\u8bb8\u591a\u73af\u5883\u5b58\u5728\u53ef\u5229\u7528\u7684\u7ed3\u6784\u3002\u5757\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u80fd\u591f\u5efa\u6a21\u5177\u6709\u5927\u89c2\u6d4b\u7a7a\u95f4\u4f46\u7531\u6f5c\u5728\u72b6\u6001\u51b3\u5b9a\u52a8\u6001\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5229\u7528\u8fd9\u79cd\u7ed3\u6784\u7684\u6709\u6548\u7b97\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7b97\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u968f\u673a\u63a2\u7d22\u5b66\u4e60\u6f5c\u5728\u72b6\u6001\u7ed3\u6784\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5207\u6362\u5230\u9002\u5e94\u5df2\u53d1\u73b0\u7ed3\u6784\u7684\u4e50\u89c2\u5f15\u5bfc\u7b56\u7565\u3002", "result": "\u7b97\u6cd5\u5728\u53ef\u805a\u7c7b\u7684BMDP\u7c7b\u4e0a\u5b9e\u73b0\u4e86O(\u221aT+n)\u7684\u9057\u61be\u754c\uff0c\u4f18\u4e8e\u4e4b\u524d\u6700\u597d\u7684O(\u221aT+n\u00b2)\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5f53\u89c2\u6d4b\u7a7a\u95f4\u57fa\u6570n\u5f88\u5927\u65f6\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u53ef\u805a\u7c7b\u7684BMDP\u7c7b\u4e0a\u8fbe\u5230\u6e10\u8fd1\u6700\u4f18\uff0c\u8bc1\u660e\u4e86\u51c6\u786e\u4f30\u8ba1\u6f5c\u5728\u72b6\u6001\u786e\u5b9e\u80fd\u6709\u6548\u52a0\u901f\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.bcbf334c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcto.new%2F%3Futm_source=tldr/1/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/cuGpdXUET-o5tS2GOz4U4iX4r1Uo_mIEt7YR5cY1sP0=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcto.new%2F%3Futm_source=tldr/1/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/cuGpdXUET-o5tS2GOz4U4iX4r1Uo_mIEt7YR5cY1sP0=427", "authors": ["TLDR Newsletter"], "title": "\ud83d\udc4b Meet cto.new: a new AI code agent that crushes benchmarks - and costs $0", "comment": "Source: TLDR Newsletter, Date: 2025-10-15, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcto.new%2F%3Futm_source=tldr/1/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/cuGpdXUET-o5tS2GOz4U4iX4r1Uo_mIEt7YR5cY1sP0=427", "summary": "\ud83d\udc4b Meet cto.new: a new AI code agent that crushes benchmarks - and costs $0 (Sponsor) Psst: there's a new kid on the block \u2014 and it launched today. It consistently benchmarks higher than Anthropic's Claude Code and OpenAI's Codex. And instead of $200/month for premium usage, you pay absolutely nothing. Use on GPT-5, Sonnet 4.5 and more, no API keys required. Start now", "source": "tldr", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3acto.new\u7684\u65b0\u578bAI\u4ee3\u7801\u4ee3\u7406\uff0c\u5b83\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eClaude Code\u548cCodex\uff0c\u5e76\u4e14\u5b8c\u5168\u514d\u8d39\u4f7f\u7528", "motivation": "\u63d0\u4f9b\u6bd4\u73b0\u6709\u5546\u4e1aAI\u4ee3\u7801\u4ee3\u7406\uff08\u5982Claude Code\u548cCodex\uff09\u6027\u80fd\u66f4\u597d\u4e14\u5b8c\u5168\u514d\u8d39\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684AI\u4ee3\u7801\u4ee3\u7406\u7cfb\u7edf\uff0c\u652f\u6301GPT-5\u3001Sonnet 4.5\u7b49\u6a21\u578b\uff0c\u65e0\u9700API\u5bc6\u94a5\u5373\u53ef\u4f7f\u7528", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8aAnthropic\u7684Claude Code\u548cOpenAI\u7684Codex\uff0c\u540c\u65f6\u63d0\u4f9b\u514d\u8d39\u670d\u52a1", "conclusion": "cto.new\u662f\u4e00\u4e2a\u6027\u80fd\u4f18\u8d8a\u4e14\u5b8c\u5168\u514d\u8d39\u7684AI\u4ee3\u7801\u4ee3\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u9009\u62e9", "topic": "code agent"}}
{"id": "tldr.2510.6fa74bf2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcto.new%2F%3Futm_source=tldr/4/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/8xT6Dx5m_5OEdpPwGZREmi7X1VTe_r7xZeTmQ2ixBF8=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcto.new%2F%3Futm_source=tldr/4/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/8xT6Dx5m_5OEdpPwGZREmi7X1VTe_r7xZeTmQ2ixBF8=427", "authors": ["TLDR Newsletter"], "title": "\u2764\ufe0f \"I love paying Anthropic $200/mo\"", "comment": "Source: TLDR Newsletter, Date: 2025-10-15, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcto.new%2F%3Futm_source=tldr/4/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/8xT6Dx5m_5OEdpPwGZREmi7X1VTe_r7xZeTmQ2ixBF8=427", "summary": "\u2764\ufe0f \"I love paying Anthropic $200/mo\" (Sponsor) If you love paying, follow your heart. Otherwise, you might want to try the new AI code agent that launched today \u2014 it beats Claude Code on benchmarks and costs $0. Experience cto.new - it's free", "source": "tldr", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u514d\u8d39\u7684AI\u4ee3\u7801\u52a9\u624b\uff0c\u58f0\u79f0\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eClaude Code\u4e14\u65e0\u9700\u4ed8\u8d39", "motivation": "\u4e3a\u7528\u6237\u63d0\u4f9b\u6bd4\u4ed8\u8d39AI\u4ee3\u7801\u52a9\u624b\uff08\u5982Claude Code\uff09\u66f4\u4f18\u4e14\u514d\u8d39\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684AI\u4ee3\u7801\u4ee3\u7406\u5de5\u5177\uff0c\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u6027\u80fd", "result": "\u8be5\u5de5\u5177\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86Claude Code\uff0c\u5e76\u4e14\u5b8c\u5168\u514d\u8d39\u4f7f\u7528", "conclusion": "cto.new\u662f\u4e00\u4e2a\u6027\u80fd\u4f18\u8d8a\u4e14\u514d\u8d39\u7684AI\u4ee3\u7801\u52a9\u624b\u9009\u62e9", "topic": "code agent"}}
{"id": "tldr.2510.f7b42f6f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2Fjust-talk-to-it%3Futm_source=tldrnewsletter/1/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/0ZXg2APLQBSH7sMGD8Fd4VBv2odtqgJg847WfFM60LE=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2Fjust-talk-to-it%3Futm_source=tldrnewsletter/1/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/0ZXg2APLQBSH7sMGD8Fd4VBv2odtqgJg847WfFM60LE=427", "authors": ["TLDR Newsletter"], "title": "Just Talk To It - the no-bs Way of Agentic Engineering", "comment": "Source: TLDR Newsletter, Date: 2025-10-15, Reading time: 23 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsteipete.me%2Fposts%2Fjust-talk-to-it%3Futm_source=tldrnewsletter/1/01000199e765b7e9-5d57e67f-cd5e-495f-a470-0868f4f09444-000000/0ZXg2APLQBSH7sMGD8Fd4VBv2odtqgJg847WfFM60LE=427", "summary": "Just Talk To It - the no-bs Way of Agentic Engineering (23 minute read) Just talk to your agents - the more you work with them, the better your results will be.", "source": "tldr", "AI": {"tldr": "\u901a\u8fc7\u4e0e\u667a\u80fd\u4f53\u76f4\u63a5\u5bf9\u8bdd\u6765\u6539\u8fdb\u5176\u6027\u80fd\uff0c\u65e0\u9700\u590d\u6742\u5de5\u7a0b", "motivation": "\u7b80\u5316\u667a\u80fd\u4f53\u5f00\u53d1\u6d41\u7a0b\uff0c\u907f\u514d\u8fc7\u5ea6\u5de5\u7a0b\u5316\uff0c\u901a\u8fc7\u81ea\u7136\u5bf9\u8bdd\u63d0\u5347\u667a\u80fd\u4f53\u8868\u73b0", "method": "\u91c7\u7528\u76f4\u63a5\u5bf9\u8bdd\u7684\u65b9\u5f0f\u4e0e\u667a\u80fd\u4f53\u4ea4\u4e92\uff0c\u901a\u8fc7\u6301\u7eed\u5bf9\u8bdd\u4f18\u5316\u667a\u80fd\u4f53\u884c\u4e3a", "result": "\u968f\u7740\u5bf9\u8bdd\u6b21\u6570\u589e\u52a0\uff0c\u667a\u80fd\u4f53\u8868\u73b0\u9010\u6e10\u6539\u5584\uff0c\u7528\u6237\u4e0e\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u6548\u679c\u66f4\u597d", "conclusion": "\u76f4\u63a5\u5bf9\u8bdd\u662f\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5de5\u7a0b\u590d\u6742\u5ea6\u8d8a\u4f4e\u6548\u679c\u8d8a\u597d", "topic": "agent analysis"}}
{"id": "tldr.2510.24d1132c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwasp.sh%2Fblog%2F2025%2F10%2F07%2Fhow-we-test-a-web-framework%3Futm_source=tldrwebdev/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/nY7ek6OWjUEZJE1ww29cU8pzcOXtY3Tbn2xo1lyfIV4=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwasp.sh%2Fblog%2F2025%2F10%2F07%2Fhow-we-test-a-web-framework%3Futm_source=tldrwebdev/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/nY7ek6OWjUEZJE1ww29cU8pzcOXtY3Tbn2xo1lyfIV4=427", "authors": ["TLDR Newsletter"], "title": "How we test a web framework", "comment": "Source: TLDR Newsletter, Date: 2025-10-15, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwasp.sh%2Fblog%2F2025%2F10%2F07%2Fhow-we-test-a-web-framework%3Futm_source=tldrwebdev/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/nY7ek6OWjUEZJE1ww29cU8pzcOXtY3Tbn2xo1lyfIV4=427", "summary": "How we test a web framework (15 minute read) Wasp's testing strategy for its full-stack web framework prioritizes tests that clearly express input and expected output, favoring courage over 100% coverage. It also relies on types heavily. Wasp uses snapshot testing to track code generation changes and Playwright to validate apps.", "source": "tldr", "AI": {"tldr": "Wasp\u6d4b\u8bd5\u5168\u6808Web\u6846\u67b6\u7684\u7b56\u7565\uff1a\u4f18\u5148\u9009\u62e9\u80fd\u6e05\u6670\u8868\u8fbe\u8f93\u5165\u548c\u9884\u671f\u8f93\u51fa\u7684\u6d4b\u8bd5\uff0c\u91cd\u52c7\u6c14\u800c\u975e100%\u8986\u76d6\u7387\uff0c\u91cd\u5ea6\u4f9d\u8d56\u7c7b\u578b\u7cfb\u7edf\uff0c\u4f7f\u7528\u5feb\u7167\u6d4b\u8bd5\u8ffd\u8e2a\u4ee3\u7801\u751f\u6210\u53d8\u5316\uff0cPlaywright\u9a8c\u8bc1\u5e94\u7528\u3002", "motivation": "\u4e3a\u5168\u6808Web\u6846\u67b6\u5efa\u7acb\u6709\u6548\u7684\u6d4b\u8bd5\u7b56\u7565\uff0c\u5728\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u6d4b\u8bd5\u8d28\u91cf\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u786e\u4fdd\u6846\u67b6\u7684\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7c7b\u578b\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4f7f\u7528\u5feb\u7167\u6d4b\u8bd5\u6765\u8ffd\u8e2a\u4ee3\u7801\u751f\u6210\u7684\u53d8\u5316\uff0c\u901a\u8fc7Playwright\u8fdb\u884c\u7aef\u5230\u7aef\u5e94\u7528\u9a8c\u8bc1\uff0c\u5f3a\u8c03\u6d4b\u8bd5\u7684\u6e05\u6670\u8868\u8fbe\u800c\u975e\u8ffd\u6c42100%\u8986\u76d6\u7387\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u5957\u6709\u6548\u7684\u6d4b\u8bd5\u4f53\u7cfb\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u901a\u8fc7\u5feb\u7167\u6d4b\u8bd5\u6709\u6548\u7ba1\u7406\u4ee3\u7801\u751f\u6210\u7684\u53d8\u5316\u3002", "conclusion": "\u5bf9\u4e8e\u5168\u6808Web\u6846\u67b6\uff0c\u6e05\u6670\u7684\u6d4b\u8bd5\u8868\u8fbe\u548c\u7c7b\u578b\u5b89\u5168\u6bd4100%\u6d4b\u8bd5\u8986\u76d6\u7387\u66f4\u91cd\u8981\uff0c\u5feb\u7167\u6d4b\u8bd5\u662f\u7ba1\u7406\u4ee3\u7801\u751f\u6210\u53d8\u5316\u7684\u6709\u6548\u5de5\u5177\u3002", "topic": "swe application"}}
{"id": "tldr.2510.1f843b03", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fboydkane.com%2Fessays%2Fboss%3Futm_source=tldrwebdev/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/tWghou8WeBrgeKJzVurXxvi2DBMuSCJddpz46ICvFI8=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fboydkane.com%2Fessays%2Fboss%3Futm_source=tldrwebdev/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/tWghou8WeBrgeKJzVurXxvi2DBMuSCJddpz46ICvFI8=427", "authors": ["TLDR Newsletter"], "title": "Why your boss isn't worried about AI", "comment": "Source: TLDR Newsletter, Date: 2025-10-15, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fboydkane.com%2Fessays%2Fboss%3Futm_source=tldrwebdev/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/tWghou8WeBrgeKJzVurXxvi2DBMuSCJddpz46ICvFI8=427", "summary": "Why your boss isn't worried about AI (11 minute read) The public's understanding of software bugs, while helpful for regular software, is misleading when applied to AI systems. Unlike regular software, where bugs are caused by mistakes in code and can be precisely located and fixed, AI vulnerabilities come from large training datasets, making the origin of errors difficult to pinpoint or eliminate entirely. Once a bug is fixed in regular software, it won't reappear again, but this isn't the c...", "source": "tldr", "AI": {"tldr": "\u516c\u4f17\u5bf9\u8f6f\u4ef6\u6f0f\u6d1e\u7684\u7406\u89e3\u9002\u7528\u4e8e\u5e38\u89c4\u8f6f\u4ef6\uff0c\u4f46\u5728AI\u7cfb\u7edf\u4e0a\u5177\u6709\u8bef\u5bfc\u6027\u3002AI\u6f0f\u6d1e\u6e90\u4e8e\u5927\u578b\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u7cbe\u786e\u5b9a\u4f4d\u6216\u5b8c\u5168\u6d88\u9664\uff0c\u4e14\u4fee\u590d\u540e\u53ef\u80fd\u91cd\u73b0\u3002", "motivation": "\u6f84\u6e05AI\u7cfb\u7edf\u4e0e\u5e38\u89c4\u8f6f\u4ef6\u5728\u6f0f\u6d1e\u7279\u6027\u4e0a\u7684\u6839\u672c\u5dee\u5f02\uff0c\u89e3\u91ca\u4e3a\u4f55AI\u6f0f\u6d1e\u66f4\u96be\u4ee5\u7ba1\u7406\u548c\u4fee\u590d\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u5e38\u89c4\u8f6f\u4ef6\u6f0f\u6d1e\u4e0eAI\u7cfb\u7edf\u6f0f\u6d1e\u7684\u8d77\u6e90\u3001\u5b9a\u4f4d\u96be\u5ea6\u548c\u4fee\u590d\u7279\u6027\u3002", "result": "AI\u6f0f\u6d1e\u5177\u6709\u7cfb\u7edf\u6027\u3001\u96be\u4ee5\u8ffd\u6eaf\u548c\u53ef\u80fd\u91cd\u73b0\u7684\u7279\u70b9\uff0c\u4e0e\u5e38\u89c4\u8f6f\u4ef6\u6f0f\u6d1e\u7684\u5c40\u90e8\u6027\u548c\u53ef\u5b8c\u5168\u4fee\u590d\u6027\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u601d\u8003AI\u7cfb\u7edf\u7684\u6f0f\u6d1e\u7ba1\u7406\u65b9\u6cd5\uff0c\u4e0d\u80fd\u7b80\u5355\u5957\u7528\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.ade1f7df", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fsentry-ai-code-review-now-in-beta-break-production-less%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/GQ-Ztf3RzSPe3RaYN5fsZXpVo8sGURnpSgiCXk4J3FM=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fsentry-ai-code-review-now-in-beta-break-production-less%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/GQ-Ztf3RzSPe3RaYN5fsZXpVo8sGURnpSgiCXk4J3FM=427", "authors": ["TLDR Newsletter"], "title": "Sentry's AI Code Review predicts what's going to break - based on what's already broken", "comment": "Source: TLDR Newsletter, Date: 2025-10-15, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fsentry-ai-code-review-now-in-beta-break-production-less%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/1/01000199e7a33a83-21dd49bb-2c0e-4801-ae01-e6d9051b72ee-000000/GQ-Ztf3RzSPe3RaYN5fsZXpVo8sGURnpSgiCXk4J3FM=427", "summary": "Sentry's AI Code Review predicts what's going to break - based on what's already broken (Sponsor) Code reviews should be less style nits and more \"this is going to break prod\". Sentry's AI Code Review blends context and issue history with the code you just touched - function calls, class or objects dependencies, database connections - to provide specific and actionable feedback rather than generic linting advice. Read the blog", "source": "tldr", "AI": {"tldr": "Sentry\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u901a\u8fc7\u5206\u6790\u4ee3\u7801\u53d8\u66f4\u3001\u4f9d\u8d56\u5173\u7cfb\u548c\u5386\u53f2\u95ee\u9898\u6765\u9884\u6d4b\u53ef\u80fd\u5bfc\u81f4\u751f\u4ea7\u73af\u5883\u6545\u969c\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u5177\u4f53\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\u800c\u975e\u901a\u7528\u4ee3\u7801\u98ce\u683c\u68c0\u67e5", "motivation": "\u4f20\u7edf\u4ee3\u7801\u5ba1\u67e5\u8fc7\u4e8e\u5173\u6ce8\u4ee3\u7801\u98ce\u683c\u7ec6\u8282\uff0c\u800c\u7f3a\u4e4f\u5bf9\u53ef\u80fd\u5f15\u53d1\u751f\u4ea7\u73af\u5883\u6545\u969c\u7684\u5b9e\u9645\u95ee\u9898\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u5ba1\u67e5\u5de5\u5177", "method": "\u7ed3\u5408\u4ee3\u7801\u53d8\u66f4\u4e0a\u4e0b\u6587\u548c\u95ee\u9898\u5386\u53f2\u8bb0\u5f55\uff0c\u5206\u6790\u51fd\u6570\u8c03\u7528\u3001\u7c7b\u6216\u5bf9\u8c61\u4f9d\u8d56\u5173\u7cfb\u3001\u6570\u636e\u5e93\u8fde\u63a5\u7b49\uff0c\u63d0\u4f9b\u5177\u4f53\u53ef\u64cd\u4f5c\u7684\u53cd\u9988", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u9884\u6d4b\u751f\u4ea7\u73af\u5883\u6545\u969c\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u51cf\u5c11\u901a\u7528linting\u5efa\u8bae\uff0c\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u95ee\u9898\u9884\u8b66", "conclusion": "AI\u9a71\u52a8\u7684\u4ee3\u7801\u5ba1\u67e5\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\uff0c\u901a\u8fc7\u5386\u53f2\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u5206\u6790\u6765\u9884\u6d4b\u548c\u9884\u9632\u751f\u4ea7\u73af\u5883\u6545\u969c", "topic": "swe application"}}
{"id": "tldr.2510.7184af50", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.experimently.io%2F%3Futm_source=tldrdesign/1/01000199e7d00108-31ae7426-7b3f-4d30-a485-b7d8a37eb849-000000/a8DJ5nieo-oQKZ_p3PSjdO9QVa18ZH5-sU-K8lSc_7Q=427", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.experimently.io%2F%3Futm_source=tldrdesign/1/01000199e7d00108-31ae7426-7b3f-4d30-a485-b7d8a37eb849-000000/a8DJ5nieo-oQKZ_p3PSjdO9QVa18ZH5-sU-K8lSc_7Q=427", "authors": ["TLDR Newsletter"], "title": "Generate a Page from a Prompt and Edit Visually", "comment": "Source: TLDR Newsletter, Date: 2025-10-15, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.experimently.io%2F%3Futm_source=tldrdesign/1/01000199e7d00108-31ae7426-7b3f-4d30-a485-b7d8a37eb849-000000/a8DJ5nieo-oQKZ_p3PSjdO9QVa18ZH5-sU-K8lSc_7Q=427", "summary": "Generate a Page from a Prompt and Edit Visually (Website) Generate a page from a prompt, edit visually (no code needed), publish to your domain, and A/B test multiple page variations with configurable weighted routing.", "source": "tldr", "AI": {"tldr": "\u4ece\u63d0\u793a\u751f\u6210\u7f51\u9875\uff0c\u65e0\u9700\u4ee3\u7801\u5373\u53ef\u53ef\u89c6\u5316\u7f16\u8f91\uff0c\u53d1\u5e03\u5230\u81ea\u5b9a\u4e49\u57df\u540d\uff0c\u5e76\u901a\u8fc7\u53ef\u914d\u7f6e\u7684\u52a0\u6743\u8def\u7531\u8fdb\u884cA/B\u6d4b\u8bd5\u591a\u9875\u9762\u53d8\u4f53", "motivation": "\u964d\u4f4e\u7f51\u9875\u521b\u5efa\u548c\u7f16\u8f91\u7684\u6280\u672f\u95e8\u69db\uff0c\u8ba9\u975e\u6280\u672f\u4eba\u5458\u4e5f\u80fd\u8f7b\u677e\u521b\u5efa\u548c\u4f18\u5316\u7f51\u9875\uff0c\u540c\u65f6\u63d0\u4f9b\u4e13\u4e1a\u7684A/B\u6d4b\u8bd5\u529f\u80fd", "method": "\u57fa\u4e8e\u63d0\u793a\u751f\u6210\u7f51\u9875\u5185\u5bb9\uff0c\u63d0\u4f9b\u53ef\u89c6\u5316\u7f16\u8f91\u754c\u9762\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u57df\u540d\u53d1\u5e03\uff0c\u5b9e\u73b0\u52a0\u6743\u8def\u7531\u7684A/B\u6d4b\u8bd5\u673a\u5236", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u65e0\u9700\u7f16\u7801\u7684\u7f51\u9875\u751f\u6210\u548c\u7f16\u8f91\u5e73\u53f0\uff0c\u652f\u6301\u53ef\u89c6\u5316\u64cd\u4f5c\u548c\u4e13\u4e1a\u7684A/B\u6d4b\u8bd5\u529f\u80fd", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u964d\u4f4e\u4e86\u7f51\u9875\u521b\u5efa\u7684\u6280\u672f\u95e8\u69db\uff0c\u4e3a\u975e\u6280\u672f\u4eba\u5458\u63d0\u4f9b\u4e86\u4e13\u4e1a\u7684\u7f51\u9875\u4f18\u5316\u5de5\u5177", "topic": "swe application"}}
