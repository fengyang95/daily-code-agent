<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 14]
- [wechat.article](#wechat.article) [Total: 25]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.LG](#cs.LG) [Total: 6]
- [tldr.article](#tldr.article) [Total: 10]
- [cs.AI](#cs.AI) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Confidence is Not Competence](https://arxiv.org/abs/2510.24772)
*Debdeep Sanyal,Manya Pandey,Dhruv Kumar,Saurabh Deshpande,Murari Mandal*

Main category: cs.CL

TL;DR: 该论文通过分析LLM内部状态在预生成评估和解决方案执行两个阶段的几何结构，揭示了置信度与能力脱节的机制原因。研究发现评估阶段具有高维几何复杂性，而执行阶段在低维流形上演化，这种几何复杂性的急剧减少解释了置信度-能力差距。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解释大型语言模型中观察到的置信度与实际问题解决能力之间的脱节现象，从机制层面理解这种不一致性。

Method: 使用线性探针解码模型的内部"可解性信念"，分析评估阶段和执行阶段内部状态的几何结构，包括线性有效维度和流形结构，并进行因果干预实验。

Result: 发现信念可以线性解码且在不同模型家族和任务类型中具有良好排序的信念轴，但评估流形具有高线性有效维度，而推理轨迹在低维流形上演化。因果干预显示沿信念轴的线性扰动不会改变最终解决方案。

Conclusion: 揭示了LLM具有两系统架构：几何复杂的评估器向几何简单的执行器提供输入。可解码的信念并非可操作的控制杆，干预应针对执行过程的程序性动态而非评估的高层几何结构。

Abstract: Large language models (LLMs) often exhibit a puzzling disconnect between
their asserted confidence and actual problem-solving competence. We offer a
mechanistic account of this decoupling by analyzing the geometry of internal
states across two phases - pre-generative assessment and solution execution. A
simple linear probe decodes the internal "solvability belief" of a model,
revealing a well-ordered belief axis that generalizes across model families and
across math, code, planning, and logic tasks. Yet, the geometries diverge -
although belief is linearly decodable, the assessment manifold has high linear
effective dimensionality as measured from the principal components, while the
subsequent reasoning trace evolves on a much lower-dimensional manifold. This
sharp reduction in geometric complexity from thought to action mechanistically
explains the confidence-competence gap. Causal interventions that steer
representations along the belief axis leave final solutions unchanged,
indicating that linear nudges in the complex assessment space do not control
the constrained dynamics of execution. We thus uncover a two-system
architecture - a geometrically complex assessor feeding a geometrically simple
executor. These results challenge the assumption that decodable beliefs are
actionable levers, instead arguing for interventions that target the procedural
dynamics of execution rather than the high-level geometry of assessment.

</details>


### [2] [Idea2Plan: Exploring AI-Powered Research Planning](https://arxiv.org/abs/2510.24891)
*Jin Huang,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen W. White*

Main category: cs.CL

TL;DR: 该论文提出了Idea2Plan任务和基准，用于系统评估大语言模型从研究想法到研究计划的转换能力，并发现GPT-5系列表现最佳但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 研究规划是科学发现和自主研究代理开发的关键能力，但目前缺乏对LLM研究规划能力的系统理解。

Method: 引入Idea2Plan任务和Idea2Plan Bench基准，包含200篇ICML 2025论文的研究想法和评分标准，并提出了Idea2Plan JudgeEval来评估LLM评委的可靠性。

Result: 实验结果显示GPT-5和GPT-5-mini在基准测试中表现最强，但仍有显著的改进空间。

Conclusion: 该研究为理解LLM的研究规划能力提供了新见解，并为未来进展奠定了基础。

Abstract: Large language models (LLMs) have demonstrated significant potential to
accelerate scientific discovery as valuable tools for analyzing data,
generating hypotheses, and supporting innovative approaches in various
scientific fields. In this work, we investigate how LLMs can handle the
transition from conceptual research ideas to well-structured research plans.
Effective research planning not only supports scientists in advancing their
research but also represents a crucial capability for the development of
autonomous research agents. Despite its importance, the field lacks a
systematic understanding of LLMs' research planning capability. To rigorously
measure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a
benchmark built from 200 ICML 2025 Spotlight and Oral papers released after
major LLM training cutoffs. Each benchmark instance includes a research idea
and a grading rubric capturing the key components of valid plans. We further
propose Idea2Plan JudgeEval, a complementary benchmark to assess the
reliability of LLM-based judges against expert annotations. Experimental
results show that GPT-5 and GPT-5-mini achieve the strongest performance on the
benchmark, though substantial headroom remains for future improvement. Our
study provides new insights into LLMs' capability for research planning and lay
the groundwork for future progress.

</details>


### [3] [SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens](https://arxiv.org/abs/2510.24940)
*Yinhan He,Wendy Zheng,Yaochen Zhu,Zaiyi Zheng,Lin Su,Sriram Vasudevan,Qi Guo,Liangjie Hong,Jundong Li*

Main category: cs.CL

TL;DR: 提出SemCoT框架，通过语义对齐的隐式推理和高效轻量级生成器，解决现有隐式CoT方法在语义对齐和生成效率方面的挑战，显著提升推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统显式CoT推理的冗长性阻碍了其在效率关键应用中的大规模部署。现有隐式CoT方法存在语义对齐差和生成效率低的问题。

Method: 设计对比训练的句子转换器评估语义对齐，并引入基于知识蒸馏的轻量级语言模型作为高效隐式推理生成器，联合优化生成速度和语义对齐。

Result: 大量实验表明SemCoT在效率和效果上都优于现有最先进方法。

Conclusion: SemCoT是首个通过联合优化token级生成速度和语义对齐来提升CoT效率的方法，在保持推理性能的同时显著加速推理过程。

Abstract: The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment
in efficiency-critical applications. Recently, implicit CoT approaches have
emerged, which encode reasoning steps within LLM's hidden embeddings (termed
``implicit reasoning'') rather than explicit tokens. This approach accelerates
CoT by reducing the reasoning length and bypassing some LLM components.
However, existing implicit CoT methods face two significant challenges: (1)
they fail to preserve the semantic alignment between the implicit reasoning
(when transformed to natural language) and the ground-truth reasoning,
resulting in a significant CoT performance degradation, and (2) they focus on
reducing the length of the implicit reasoning; however, they neglect the
considerable time cost for an LLM to generate one individual implicit reasoning
token. To tackle these challenges, we propose a novel semantically-aligned
implicit CoT framework termed SemCoT. In particular, for the first challenge,
we design a contrastively trained sentence transformer that evaluates semantic
alignment between implicit and explicit reasoning, which is used to enforce
semantic preservation during implicit reasoning optimization. To address the
second challenge, we introduce an efficient implicit reasoning generator by
finetuning a lightweight language model using knowledge distillation. This
generator is guided by our sentence transformer to distill ground-truth
reasoning into semantically aligned implicit reasoning, while also optimizing
for accuracy. SemCoT is the first approach that enhances CoT efficiency by
jointly optimizing token-level generation speed and preserving semantic
alignment with ground-truth reasoning. Extensive experiments demonstrate the
superior performance of SemCoT compared to state-of-the-art methods in both
efficiency and effectiveness. Our code can be found at
https://github.com/YinhanHe123/SemCoT/.

</details>


### [4] [DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates](https://arxiv.org/abs/2510.25110)
*Yun-Shiuan Chuang,Ruixuan Tu,Chengtao Dai,Smit Vasani,Binwei Yao,Michael Henry Tessler,Sijia Yang,Dhavan Shah,Robert Hawkins,Junjie Hu,Timothy T. Rogers*

Main category: cs.CL

TL;DR: DEBATE是一个大规模实证基准，用于评估多智能体角色扮演LLM在模拟人类意见动态方面的真实性，包含29,417条真实辩论对话，揭示了模拟与真实群体动态之间的关键差异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM角色扮演设置往往产生不自然的群体动态（如过早收敛），缺乏衡量真实人类意见轨迹的实证基准，需要解决模拟人类社交互动的真实性问题。

Method: 构建DEBATE基准，包含29,417条来自2,792名美国参与者的多轮辩论对话，覆盖107个争议话题，同时收集公开表达信息和私下报告意见；使用该基准系统评估LLM模拟效果，并通过监督微调对齐LLM与人类行为。

Result: 发现模拟与真实群体动态存在关键差异；监督微调在表面指标（如ROUGE-L和消息长度）上取得改进，但在更深层次的语义对齐（如语义相似性）方面仍有局限。

Conclusion: 角色扮演LLM智能体在真实模拟人类社交动态方面既有潜力也存在当前限制，需要进一步改进语义层面的对齐。

Abstract: Accurately modeling opinion change through social interactions is crucial for
addressing issues like misinformation and polarization. While role-playing
large language models (LLMs) offer a promising way to simulate human-like
interactions, existing research shows that single-agent alignment does not
guarantee authentic multi-agent group dynamics. Current LLM role-play setups
often produce unnatural dynamics (e.g., premature convergence), without an
empirical benchmark to measure authentic human opinion trajectories. To bridge
this gap, we introduce DEBATE, the first large-scale empirical benchmark
explicitly designed to evaluate the authenticity of the interaction between
multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round
debate conversations among over 2,792 U.S.-based participants discussing 107
controversial topics, capturing both publicly-expressed messages and
privately-reported opinions. Using DEBATE, we systematically evaluate and
identify critical discrepancies between simulated and authentic group dynamics.
We further demonstrate DEBATE's utility for aligning LLMs with human behavior
through supervised fine-tuning, achieving improvements in surface-level metrics
(e.g., ROUGE-L and message length) while highlighting limitations in deeper
semantic alignment (e.g., semantic similarity). Our findings highlight both the
potential and current limitations of role-playing LLM agents for realistically
simulating human-like social dynamics.

</details>


### [5] [Model-Document Protocol for AI Search](https://arxiv.org/abs/2510.25160)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: 提出Model-Document Protocol (MDP)框架，重新定义LLM与文档的交互方式，将原始文档转化为任务特定的、可直接消费的知识表示，并通过MDP-Agent实现代理推理、记忆基础和结构化利用三种路径。


<details>
  <summary>Details</summary>
Motivation: 当前检索方法将文档作为原始文本处理，返回原始段落，将片段组装和上下文推理的负担留给LLM。需要新的检索范式来弥合原始文档与LLM就绪输入之间的差距。

Method: MDP框架通过三种路径转换非结构化文档：代理推理将原始证据整理成连贯上下文；记忆基础积累可重用笔记以丰富推理；结构化利用将文档编码为图形或键值缓存等正式表示。MDP-Agent实现该协议，构建文档级要点记忆、执行基于扩散的探索与垂直利用、应用map-reduce风格合成。

Result: 在信息寻求基准测试中，MDP-Agent优于基线方法，验证了MDP框架的合理性和其代理实例化的有效性。

Conclusion: MDP框架提供了一种新的检索范式，确保LLM接收的是紧凑、结构化的知识而非原始片段，直接可用于推理。

Abstract: AI search depends on linking large language models (LLMs) with vast external
knowledge sources. Yet web pages, PDF files, and other raw documents are not
inherently LLM-ready: they are long, noisy, and unstructured. Conventional
retrieval methods treat these documents as verbatim text and return raw
passages, leaving the burden of fragment assembly and contextual reasoning to
the LLM. This gap underscores the need for a new retrieval paradigm that
redefines how models interact with documents.
  We introduce the Model-Document Protocol (MDP), a general framework that
formalizes how raw text is bridged to LLMs through consumable knowledge
representations. Rather than treating retrieval as passage fetching, MDP
defines multiple pathways that transform unstructured documents into
task-specific, LLM-ready inputs. These include agentic reasoning, which curates
raw evidence into coherent context; memory grounding, which accumulates
reusable notes to enrich reasoning; and structured leveraging, which encodes
documents into formal representations such as graphs or key-value caches. All
three pathways share the same goal: ensuring that what reaches the LLM is not
raw fragments but compact, structured knowledge directly consumable for
reasoning.
  As an instantiation, we present MDP-Agent, which realizes the protocol
through an agentic process: constructing document-level gist memories for
global coverage, performing diffusion-based exploration with vertical
exploitation to uncover layered dependencies, and applying map-reduce style
synthesis to integrate large-scale evidence into compact yet sufficient
context. Experiments on information-seeking benchmarks demonstrate that
MDP-Agent outperforms baselines, validating both the soundness of the MDP
framework and the effectiveness of its agentic instantiation.

</details>


### [6] [Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction](https://arxiv.org/abs/2510.25187)
*Ritesh Sunil Chavan,Jack Mostow*

Main category: cs.CL

TL;DR: 研究测试了大型语言模型在低资源语言上的表现，发现模型性能随语言资源水平下降，且Chain-of-Thought提示在不同能力模型上效果不同。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型的优异表现是源于真实能力还是英语数据的优势，通过在低资源语言环境中测试来验证。

Method: 基于Next Sentence Prediction任务，构建包含英语、斯瓦希里语和豪萨语的大规模基准，测试GPT-4 Turbo、Gemini 1.5 Flash和LLaMA 3 70B等模型，并引入Chain-of-Thought提示技术。

Result: 所有模型在英语上表现优异，但在斯瓦希里语上准确性下降，在豪萨语上大幅下降，LLaMA 3表现最差。Chain-of-Thought对能力较弱的LLaMA 3有帮助，但对GPT-4和Gemini反而造成"过度思考"而降低性能。

Conclusion: Chain-of-Thought不是通用解决方案，其效果取决于模型的基础能力和任务具体情境。研究框架能识别LLM弱点，揭示CoT在跨语言NSP中的帮助或阻碍作用。

Abstract: While large language models are trained on massive datasets, this data is
heavily skewed towards English. Does their impressive performance reflect
genuine ability or just this data advantage? To find out, we tested them in a
setting where they could not rely on data abundance: low-resource languages.
Building on prior work Agarwal et al. (2025) that used Next Sentence Prediction
(NSP) as a test, we created a large-scale benchmark with 10,000 questions each
for English (a high-resource language), Swahili (medium-resource), and Hausa
(low-resource). We then tested several top models, including GPT-4 Turbo,
Gemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The
results painted a clear picture of how levels of language resources impact
outcomes. While all models excelled in English, their accuracy dropped in
Swahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story
became even more interesting when we introduced Chain-of-Thought (CoT)
prompting. For the struggling LLaMA 3, CoT acted as a helpful guide,
significantly boosting its accuracy. However, for the more capable GPT-4 and
Gemini, the same technique often backfired, leading to a kind of "overthinking"
that hurt their results in the cross-lingual context. This reveals that
Chain-of-Thought is not a universal solution; its effectiveness depends heavily
on the model's baseline capability and the specific context of the task. Our
framework pinpoints LLM weaknesses, highlights when CoT helps or hinders
cross-lingual NSP performance, and factors influencing their decisions.

</details>


### [7] [ProMediate: A Socio-cognitive framework for evaluating proactive agents in multi-party negotiation](https://arxiv.org/abs/2510.25224)
*Ziyi Liu,Bahar Sarrafzadeh,Pei Zhou,Longqi Yang,Jieyu Zhao,Ashish Sharma*

Main category: cs.CL

TL;DR: ProMediate是首个评估复杂多议题多方谈判中主动AI调解代理的框架，包含基于真实谈判案例的模拟测试平台和评估调解代理社会认知智能的指标套件。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估主动AI代理在复杂多方协作中表现的系统方法，限制了开发能有效支持多人协作的AI的进展。谈判作为需要社会认知智能的挑战性测试平台，适合评估这类代理。

Method: 开发ProMediate框架，包含：(i)基于真实谈判案例和理论驱动难度级别的模拟测试平台，配备可插拔的主动AI调解代理；(ii)社会认知评估框架，包含衡量共识变化、干预延迟、调解效果和智能的新指标套件。

Result: 社会智能调解代理优于通用基线，通过更快、更有针对性的干预。在ProMediate-Hard设置中，社会调解代理相比通用基线将共识变化提高了3.6个百分点(10.65% vs 7.01%)，同时响应速度快77%(15.98s vs 3.71s)。

Conclusion: ProMediate为推进主动、社会智能代理的开发提供了严谨、理论基础的测试平台。

Abstract: While Large Language Models (LLMs) are increasingly used in agentic
frameworks to assist individual users, there is a growing need for agents that
can proactively manage complex, multi-party collaboration. Systematic
evaluation methods for such proactive agents remain scarce, limiting progress
in developing AI that can effectively support multiple people together.
Negotiation offers a demanding testbed for this challenge, requiring
socio-cognitive intelligence to navigate conflicting interests between multiple
participants and multiple topics and build consensus. Here, we present
ProMediate, the first framework for evaluating proactive AI mediator agents in
complex, multi-topic, multi-party negotiations. ProMediate consists of two core
components: (i) a simulation testbed based on realistic negotiation cases and
theory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and
ProMediate-Hard), with a plug-and-play proactive AI mediator grounded in
socio-cognitive mediation theories, capable of flexibly deciding when and how
to intervene; and (ii) a socio-cognitive evaluation framework with a new suite
of metrics to measure consensus changes, intervention latency, mediator
effectiveness, and intelligence. Together, these components establish a
systematic framework for assessing the socio-cognitive intelligence of
proactive AI agents in multi-party settings. Our results show that a socially
intelligent mediator agent outperforms a generic baseline, via faster,
better-targeted interventions. In the ProMediate-Hard setting, our social
mediator increases consensus change by 3.6 percentage points compared to the
generic baseline (10.65\% vs 7.01\%) while being 77\% faster in response
(15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous,
theory-grounded testbed to advance the development of proactive, socially
intelligent agents.

</details>


### [8] [CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories](https://arxiv.org/abs/2510.25333)
*Yilong Lai,Yipin Yang,Jialong Wu,Fengran Mo,Zhenglin Wang,Ting Liang,Jianguo Lin,Keping Yang*

Main category: cs.CL

TL;DR: CRMWeaver是一个增强商业代理处理复杂业务场景的新方法，通过合成数据生成和强化学习训练，结合共享记忆机制，在CRMArena-Pro数据集上取得了竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理在复杂商业环境中处理异构任务和数据关系的挑战，特别是在B2B和B2C业务场景中。

Method: 采用合成数据生成和基于强化学习的训练范式，在推理时引入共享记忆机制，让代理从类似问题的任务指南中学习。

Result: 在CRMArena-Pro数据集上，轻量级模型在B2B和B2C业务场景中都取得了竞争性结果。

Conclusion: CRMWeaver方法有效提升了商业代理在复杂环境中的性能和泛化能力，具有实际应用价值。

Abstract: Recent years have witnessed the rapid development of LLM-based agents, which
shed light on using language agents to solve complex real-world problems. A
prominent application lies in business agents, which interact with databases
and internal knowledge bases via tool calls to fulfill diverse user
requirements. However, this domain is characterized by intricate data
relationships and a wide range of heterogeneous tasks, from statistical data
queries to knowledge-based question-answering. To address these challenges, we
propose CRMWeaver, a novel approach that enhances business agents in such
complex settings. To acclimate the agentic model to intricate business
environments, we employ a synthesis data generation and RL-based paradigm
during training, which significantly improves the model's ability to handle
complex data and varied tasks. During inference, a shared memories mechanism is
introduced, prompting the agent to learn from task guidelines in similar
problems, thereby further boosting its effectiveness and generalization,
especially in unseen scenarios. We validate the efficacy of our approach on the
CRMArena-Pro dataset, where our lightweight model achieves competitive results
in both B2B and B2C business scenarios, underscoring its practical value for
real-world applications.

</details>


### [9] [TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation](https://arxiv.org/abs/2510.25536)
*Bangde Du,Minghao Guo,Songming He,Ziyi Ye,Xi Zhu,Weihang Su,Shuqi Zhu,Yujia Zhou,Yongfeng Zhang,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: TwinVoice是一个评估LLM在真实世界场景中模拟人物角色的综合基准，涵盖社交、人际和叙事三个维度，并分解为六项基础能力评估。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的人物角色模拟评估存在局限：大多依赖合成对话、缺乏系统框架和能力需求分析。

Method: 引入TwinVoice基准，包含社交角色、人际角色和叙事角色三个维度，并分解为观点一致性、记忆回忆、逻辑推理、词汇保真度、角色语气和句法风格六项能力评估。

Result: 实验结果显示，先进模型在人物角色模拟上达到中等准确率，但在句法风格和记忆回忆等能力上仍有不足，平均性能显著低于人类基准。

Conclusion: 虽然LLM在人物角色模拟方面取得进展，但在句法风格和记忆回忆等关键能力上仍需改进，与人类表现仍有较大差距。

Abstract: Large Language Models (LLMs) are exhibiting emergent human-like abilities and
are increasingly envisioned as the foundation for simulating an individual's
communication style, behavioral tendencies, and personality traits. However,
current evaluations of LLM-based persona simulation remain limited: most rely
on synthetic dialogues, lack systematic frameworks, and lack analysis of the
capability requirement. To address these limitations, we introduce TwinVoice, a
comprehensive benchmark for assessing persona simulation across diverse
real-world contexts. TwinVoice encompasses three dimensions: Social Persona
(public social interactions), Interpersonal Persona (private dialogues), and
Narrative Persona (role-based expression). It further decomposes the evaluation
of LLM performance into six fundamental capabilities, including opinion
consistency, memory recall, logical reasoning, lexical fidelity, persona tone,
and syntactic style. Experimental results reveal that while advanced models
achieve moderate accuracy in persona simulation, they still fall short of
capabilities such as syntactic style and memory recall. Consequently, the
average performance achieved by LLMs remains considerably below the human
baseline.

</details>


### [10] [Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry](https://arxiv.org/abs/2510.25595)
*Run Peng,Ziqiao Ma,Amy Pang,Sikai Li,Zhang Xi-Jia,Yingzhuo Yu,Cristian-Paul Bara,Joyce Chai*

Main category: cs.CL

TL;DR: 该论文研究了在信息不对称条件下LLM智能体之间的任务协作，通过扩展爱因斯坦谜题为桌面游戏，探索智能体如何通过推理、沟通和行动来满足空间和关系约束。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM智能体在动作规划/生成方面已有研究，但它们在协作完成共同目标方面的能力尚未充分探索，特别是在信息不对称条件下。

Method: 使用微调加验证器框架，为LLM智能体配备各种沟通策略和环境验证信号，在扩展的爱因斯坦谜题桌面游戏中测试协作能力。

Result: 实证结果表明对齐沟通的重要性，无沟通的智能体也能获得高任务性能，但缺乏真正的规则理解和人类评估者的信任。通过环境验证器增强了智能体理解任务规则和完成任务的能力。

Conclusion: 集成环境验证器可以促进AI系统中更安全和可解释的协作，增强智能体对任务规则的理解。

Abstract: While Large Language Model (LLM) agents are often approached from the angle
of action planning/generation to accomplish a goal (e.g., given by language
descriptions), their abilities to collaborate with each other to achieve a
joint goal are not well explored. To address this limitation, this paper
studies LLM agents in task collaboration, particularly under the condition of
information asymmetry, where agents have disparities in their knowledge and
skills and need to work together to complete a shared task. We extend Einstein
Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two
LLM agents must reason, communicate, and act to satisfy spatial and relational
constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier
framework in which LLM agents are equipped with various communication
strategies and verification signals from the environment. Empirical results
highlight the critical importance of aligned communication, especially when
agents possess both information-seeking and -providing capabilities.
Interestingly, agents without communication can still achieve high task
performance; however, further analysis reveals a lack of true rule
understanding and lower trust from human evaluators. Instead, by integrating an
environment-based verifier, we enhance agents' ability to comprehend task rules
and complete tasks, promoting both safer and more interpretable collaboration
in AI systems. https://github.com/Roihn/EinsteinPuzzles

</details>


### [11] [PairUni: Pairwise Training for Unified Multimodal Language Models](https://arxiv.org/abs/2510.25682)
*Jiani Zheng,Zhiyang Teng,Xiangtai Li,Anran Wang,Yu Tian,Kunpeng Qiu,Ye Tian,Haochen Wang,Zhuochen Wang*

Main category: cs.CL

TL;DR: 提出了PairUni框架，通过将数据重组为理解-生成对来解决统一视觉语言模型中理解与生成任务的平衡问题，使用Pair-GPRO算法优化训练过程。


<details>
  <summary>Details</summary>
Motivation: 统一视觉语言模型需要同时处理理解和生成任务，但这些任务依赖异构数据和监督，在强化学习中难以平衡。

Method: 使用GPT-3增强数据，将理解样本生成描述，生成样本生成问答对，形成对齐对；检索语义相关的理解样本形成检索对；提出Pair-GPRO算法基于相似度评分调节优势函数。

Result: 在Janus-Pro等统一视觉语言模型上实现了平衡的改进，优于现有强化学习基线方法。

Conclusion: PairUni框架通过数据配对和配对感知的强化学习算法，有效解决了统一视觉语言模型中任务平衡问题。

Abstract: Unified vision-language models (UVLMs) must perform both understanding and
generation within a single architecture, but these tasks rely on heterogeneous
data and supervision, making it difficult to balance them during reinforcement
learning (RL). We propose PairUni, a unified framework that reorganizes data
into understanding-generation (UG) pairs and aligns optimization accordingly.
We first use GPT-o3 to augment single-task data, generating captions for
understanding samples and question-answer (QA) pairs for generation samples,
forming aligned pairs from the same instance. Additionally, for each generation
sample, we retrieve a semantically related understanding example to form a
retrieved pair, linking different but related data points. These paired
structures expose cross-task semantic correspondences and support consistent
policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware
variant based on Group Relative Policy Optimization. It assigns a similarity
score to each pair to modulate the advantage, strengthening learning from
well-aligned examples and reducing task interference. We curate a high-quality
dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on
the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on
various UVLMs, outperforming strong UVLM RL baselines. Code:
\href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}

</details>


### [12] [The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution](https://arxiv.org/abs/2510.25726)
*Junlong Li,Wenshuo Zhao,Jian Zhao,Weihao Zeng,Haoze Wu,Xiaochen Wang,Rui Ge,Yuxuan Cao,Yuzhen Huang,Wei Liu,Junteng Liu,Zhaochen Su,Yiyang Guo,Fan Zhou,Lueyang Zhang,Juan Michelini,Xingyao Wang,Xiang Yue,Shuyan Zhou,Graham Neubig,Junxian He*

Main category: cs.CL

TL;DR: Toolathlon是一个用于评估语言代理的基准测试，涵盖32个软件应用和604个工具，提供多样化的应用、真实环境设置和可靠执行评估，揭示当前最先进模型在复杂多步骤任务中的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有语言代理基准测试往往专注于狭窄领域或简化任务，缺乏评估代理在现实世界中处理复杂多步骤工作流所需的多样性、真实性和长期复杂性。

Method: 引入Tool Decathlon基准测试，基于高质量MCP服务器集，提供来自真实软件的初始环境状态，包含108个手动收集或设计的任务，每个任务平均需要约20次交互才能完成。

Result: 评估显示SOTA模型表现显著不足：最佳模型Claude-4.5-Sonnet成功率仅38.6%，平均需要20.2次工具调用；最佳开源模型DeepSeek-V3.2-Exp成功率仅20.1%。

Conclusion: Toolathlon有望推动开发更强大的语言代理，以执行现实世界中长期复杂的任务。

Abstract: Real-world language agents must handle complex, multi-step workflows across
diverse Apps. For instance, an agent may manage emails by coordinating with
calendars and file systems, or monitor a production database to detect
anomalies and generate reports following an operating manual. However, existing
language agent benchmarks often focus on narrow domains or simplified tasks
that lack the diversity, realism, and long-horizon complexity required to
evaluate agents' real-world performance. To address this gap, we introduce the
Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering
diverse Apps and tools, realistic environment setup, and reliable
execution-based evaluation. Toolathlon spans 32 software applications and 604
tools, ranging from everyday platforms such as Google Calendar and Notion to
professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools
are based on a high-quality set of Model Context Protocol (MCP) servers that we
may have revised or implemented ourselves. Unlike prior works, which primarily
ensure functional realism but offer limited environment state diversity, we
provide realistic initial environment states from real software, such as Canvas
courses with dozens of students or real financial spreadsheets. This benchmark
includes 108 manually sourced or crafted tasks in total, requiring interacting
with multiple Apps over around 20 turns on average to complete. Each task is
strictly verifiable through dedicated evaluation scripts. Comprehensive
evaluation of SOTA models highlights their significant shortcomings: the
best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate
with 20.2 tool calling turns on average, while the top open-weights model
DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development
of more capable language agents for real-world, long-horizon task execution.

</details>


### [13] [Task Completion Agents are Not Ideal Collaborators](https://arxiv.org/abs/2510.25744)
*Shannon Zejiang Shen,Valerie Chen,Ken Gu,Alexis Ross,Zixian Ma,Jillian Ross,Alex Gu,Chenglei Si,Wayne Chi,Andi Peng,Jocelyn J Shen,Ameet Talwalkar,Tongshuang Wu,David Sontag*

Main category: cs.CL

TL;DR: 提出从一次性任务完成评估转向协作代理评估，引入协作努力扩展框架来衡量代理在用户参与增加时的效用增长。


<details>
  <summary>Details</summary>
Motivation: 当前代理评估集中于一次性任务完成，未能反映现实世界问题的迭代和协作性质，其中人类目标通常不明确且不断演变。

Method: 引入协作努力扩展框架，通过案例研究和模拟评估分析代理在多轮现实场景中的表现。

Result: 最先进的代理在多轮现实场景中表现不佳，缺乏维持参与和构建用户理解的能力。

Conclusion: 协作努力扩展为诊断代理行为和指导开发更有效交互提供了视角。

Abstract: Current evaluations of agents remain centered around one-shot task
completion, failing to account for the inherently iterative and collaborative
nature of many real-world problems, where human goals are often underspecified
and evolve. We argue for a shift from building and assessing task completion
agents to developing collaborative agents, assessed not only by the quality of
their final outputs but by how well they engage with and enhance human effort
throughout the problem-solving process. To support this shift, we introduce
collaborative effort scaling, a framework that captures how an agent's utility
grows with increasing user involvement. Through case studies and simulated
evaluations, we show that state-of-the-art agents often underperform in
multi-turn, real-world scenarios, revealing a missing ingredient in agent
design: the ability to sustain engagement and scaffold user understanding.
Collaborative effort scaling offers a lens for diagnosing agent behavior and
guiding development toward more effective interactions.

</details>


### [14] [Decomposition-Enhanced Training for Post-Hoc Attributions In Language Models](https://arxiv.org/abs/2510.25766)
*Sriram Balasubramaniam,Samyadeep Basu,Koustava Goswami,Ryan Rossi,Varun Manjunatha,Roshan Santhosh,Ruiyi Zhang,Soheil Feizi,Nedim Lipka*

Main category: cs.CL

TL;DR: DecompTune是一种后训练方法，通过将答案分解为可归因的组成单元来改进长文档问答中的归因质量，在复杂QA任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的事后归因方法在提取式问答中表现良好，但在多跳、抽象和半提取设置中表现不佳，这些场景中答案需要跨段落合成信息。

Method: 将事后归因重新定义为推理问题，通过DecompTune后训练方法教导模型生成答案分解作为中间推理步骤，使用两阶段SFT + GRPO流水线配合任务特定的定制奖励。

Result: DecompTune显著提高了归因质量，优于先前方法，并匹配或超越了最先进的边界模型。

Conclusion: 将归因重构为推理问题并通过分解答案的方法可以有效提升复杂问答场景中的归因可靠性。

Abstract: Large language models (LLMs) are increasingly used for long-document question
answering, where reliable attribution to sources is critical for trust.
Existing post-hoc attribution methods work well for extractive QA but struggle
in multi-hop, abstractive, and semi-extractive settings, where answers
synthesize information across passages. To address these challenges, we argue
that post-hoc attribution can be reframed as a reasoning problem, where answers
are decomposed into constituent units, each tied to specific context. We first
show that prompting models to generate such decompositions alongside
attributions improves performance. Building on this, we introduce DecompTune, a
post-training method that teaches models to produce answer decompositions as
intermediate reasoning steps. We curate a diverse dataset of complex QA tasks,
annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and
14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards.
Across extensive experiments and ablations, DecompTune substantially improves
attribution quality, outperforming prior methods and matching or exceeding
state-of-the-art frontier models.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [15] [学习周报（<em class="highlight">强化学习</em>四）](http://mp.weixin.qq.com/s?__biz=MzkxNTY5OTEwNQ==&mid=2247487568&idx=1&sn=a939f147eb6da2fe3456782afeaa9551&chksm=c0e0ee3b4a17abe2b5b302c57a7920bc3e09f35eee72480a3af81d3b6ec6c0cd7a9d79e6c9a3#rd)
*爱飞控的小蜗牛*

Main category: wechat.article

TL;DR: 以后会进行强化学习相关的学习汇报工作。NO.03三、课程学习1、异方差检验：模型总体拟合情况regress 人均消费支出y从事农业经营的纯收入x1其他来源纯收入x2 source ss df ms number of obs 31 f（2， 28） 163.04 model 18573928.9 2 9286964.45 prob >


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 以后会进行强化学习相关的学习汇报工作。NO.03三、课程学习1、异方差检验：模型总体拟合情况regress 人均消费支出y从事农业经营的纯收入x1其他来源纯收入x2 source ss df ms number of obs 31 f（2， 28） 163.04 model 18573928.9 2 9286964.45 prob >

</details>


### [16] [<em class="highlight">强化学习</em>成今年最大赢家！登上Nature顶刊！](http://mp.weixin.qq.com/s?__biz=MzU2NTYzNzc3NA==&mid=2247505397&idx=1&sn=398c80af72167960d9c44048414365d5&chksm=fd87a6b46e59b60e37edfd98c2dd06f063badb274e31597c651ba8ae5c38248a95f0bd7bbc30#rd)
*AI算法科研paper*

Main category: wechat.article

TL;DR: 分享一个强化学习方向值得学习的成果——最大扩散强化学习MaxDiff RL，感兴趣的同学可阅读原文。这是个新型RL方法，目前在多个基准测试中都实现了SOTA，已成功登上Nature Machine Intelligence。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 分享一个强化学习方向值得学习的成果——最大扩散强化学习MaxDiff RL，感兴趣的同学可阅读原文。这是个新型RL方法，目前在多个基准测试中都实现了SOTA，已成功登上Nature Machine Intelligence。

</details>


### [17] [<em class="highlight">强化学习</em>破译消费密码：家庭经济行为如何成为你的财富增长雷达？](http://mp.weixin.qq.com/s?__biz=MzA3OTU0MDEwNg==&mid=2247487235&idx=2&sn=3a953b6fbd4eec804c9e04985e14f1db&chksm=9e98a9caca7e67501af0a7814ab920921c6ab4975ec3433bf2fcde383672370f09146ca202b6#rd)
*Python江湖*

Main category: wechat.article

TL;DR: class RLInvestmentStrategy："""基于强化学习的投资策略"""def __init__（self， n_assets=5， lookback=60）：self.n_assets = n_assetsself.lookback = lookbackself.portfolio_weights = np.ones（n_assets） / n_assets # 初始等权重


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: class RLInvestmentStrategy："""基于强化学习的投资策略"""def __init__（self， n_assets=5， lookback=60）：self.n_assets = n_assetsself.lookback = lookbackself.portfolio_weights = np.ones（n_assets） / n_assets # 初始等权重

</details>


### [18] [<em class="highlight">强化学习</em>，端到端2.0的关键](http://mp.weixin.qq.com/s?__biz=MzYzNTA0ODY1NQ==&mid=2247567634&idx=2&sn=4bda99cafbe980771e936ca285fd7a73&chksm=f1aaa705c323ba840845a2f0f569dacd28e526afba497905f21750eb3a146a22f5081863b08c#rd)
*AI电堂*

Main category: wechat.article

TL;DR: 强化学习依靠系统与环境的持续交互，通过主动探索和优化，在试错中根据奖励信号来优化自身策略。的，不知道怎么从错误中回复（知其然，随机探索解决自动驾驶中的长尾分布效率存在挑战， 不知其所以然） 智能体 状态


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习依靠系统与环境的持续交互，通过主动探索和优化，在试错中根据奖励信号来优化自身策略。的，不知道怎么从错误中回复（知其然，随机探索解决自动驾驶中的长尾分布效率存在挑战， 不知其所以然） 智能体 状态

</details>


### [19] [别动不动说训练大模型了，一篇“老师”的比喻给你讲明白什么是后训练、微调、<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzA5MDEyMjAzMQ==&mid=2453730827&idx=1&sn=411727a7044493aac294a43e564ec6f1&chksm=86c718f0cae92b4e1b0e315f7c097e481d13d8fa156976576541865cce07480c070aa994ba33#rd)
*阿拉丁AI神灯*

Main category: wechat.article

TL;DR: 模型就学到了，哦，刚才那么讲不行，得换个方式。这就是强化学习。模型在跟你真实互动。你的每一个反馈，都是在训练它。让它变得越来越会看眼色，越来越聪明。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 模型就学到了，哦，刚才那么讲不行，得换个方式。这就是强化学习。模型在跟你真实互动。你的每一个反馈，都是在训练它。让它变得越来越会看眼色，越来越聪明。

</details>


### [20] [技术前沿 | 基于 ARTIST 框架的大语言模型智能增强：Agentic <em class="highlight">强化学习</em>的协同突破](http://mp.weixin.qq.com/s?__biz=MzkzNTk4OTY5Ng==&mid=2247484137&idx=1&sn=82b7ae48b0de8b1ab932aaa0bdbfdbd0&chksm=c39c5db32381f37393d9712a4c346b22405c51e70860dd9fe76c30893324282d18253daebf7d#rd)
*粤黔数字安全研究院*

Main category: wechat.article

TL;DR: 强化学习算法：基于 GRPO 的高效优化ARTIST采用Group Relative Policy Optimization（GRPO）算法，解决工具集成场景下的强化学习训练挑战：GRPO 核心优势无需价值函数近似，通过 “分组采样响应” 估算基线，降低训练成本并提升稳定性，


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习算法：基于 GRPO 的高效优化ARTIST采用Group Relative Policy Optimization（GRPO）算法，解决工具集成场景下的强化学习训练挑战：GRPO 核心优势无需价值函数近似，通过 “分组采样响应” 估算基线，降低训练成本并提升稳定性，

</details>


### [21] [[sumo进阶篇|traci二次开发23] – 基于<em class="highlight">强化学习</em>(DQN)的高速公路瓶颈路段动态限速控制](http://mp.weixin.qq.com/s?__biz=MzkzMzM0MzgwOQ==&mid=2247485697&idx=1&sn=e67c523c7fd2989a90ce9069e10c393f&chksm=c30c272b7240cce3572641ea47ca99aef045be2797546edf5d50db61953c1d831bc9ba1875e0#rd)
*云鹤烟波*

Main category: wechat.article

TL;DR: 3.基于强化学习（dqn）高速公路瓶颈路段动态限速控制智能体训练及实现效果 4.sumo实现的核心流程及步骤 5.代码分享 视频标题：DQN高速公路瓶颈路段动态限速效果


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 3.基于强化学习（dqn）高速公路瓶颈路段动态限速控制智能体训练及实现效果 4.sumo实现的核心流程及步骤 5.代码分享 视频标题：DQN高速公路瓶颈路段动态限速效果

</details>


### [22] [百度和高瓴联手，投了一家<em class="highlight">强化学习</em>创企](http://mp.weixin.qq.com/s?__biz=MzYyMjQ1NDgwMg==&mid=2247484854&idx=1&sn=8e9bfc58898fbf8b46b3ccf2b36982c4&chksm=fe1230202bdf03bef575ee9031ef8fe54f5c8463e62ea69a4739fec430d7a0e66951bc080367#rd)
*AI应用风向标*

Main category: wechat.article

TL;DR: Pyromind Dynamics是一家专注于强化学习基础设施的平台型公司，正在构建一套“强化学习即服务（Reinforcement Learning as a Service，RLaaS）”平台，旨在标准化强化学习的训练、部署与评估流程。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Pyromind Dynamics是一家专注于强化学习基础设施的平台型公司，正在构建一套“强化学习即服务（Reinforcement Learning as a Service，RLaaS）”平台，旨在标准化强化学习的训练、部署与评估流程。

</details>


### [23] [AI在线<em class="highlight">强化学习</em>“边做边学”，斯坦福团队让7B小模型性能飙升，甚至超越GPT-4o](http://mp.weixin.qq.com/s?__biz=MzUyMDc5OTU5NA==&mid=2247716968&idx=3&sn=833b3b8a5d359120e6a4f1d140192a04&chksm=f8d30442f83d46f63ba0e10f6c78ab2de2d754e0a425cef52c263659f45a4132f43cf09375a2#rd)
*一点人工一点智能*

Main category: wechat.article

TL;DR: 实现智能体流中强化学习训练的核心挑战在于多轮信用分配（multi-turn credit assignment）：即如何在长时跨度（long-horizon）且奖励稀疏（sparse reward）的条件下，稳定且高效地训练。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 实现智能体流中强化学习训练的核心挑战在于多轮信用分配（multi-turn credit assignment）：即如何在长时跨度（long-horizon）且奖励稀疏（sparse reward）的条件下，稳定且高效地训练。

</details>


### [24] [当知识图谱遇上<em class="highlight">强化学习</em>：让AI学会“推理”的艺术](http://mp.weixin.qq.com/s?__biz=MzU1ODk0Mjg2MQ==&mid=2247491522&idx=1&sn=e469588428d8eec319a4368d38a06dbd&chksm=fda0c839acc3217df50c9fec98e35284bf43abca0d1c10a35c4fc0863f261c8d420a5b281d24#rd)
*区块链and语义研究实验室*

Main category: wechat.article

TL;DR: 强化学习：AI的“决策大脑”强化学习就像是让智能体在环境中“玩游戏”：状态（State）：现在的情境。动作（Action）：能做的选择。奖励（Reward）：结果好不好。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习：AI的“决策大脑”强化学习就像是让智能体在环境中“玩游戏”：状态（State）：现在的情境。动作（Action）：能做的选择。奖励（Reward）：结果好不好。

</details>


### [25] [Cursor发布首个编程大模型！代码生成250tokens/秒，<em class="highlight">强化学习</em>+MoE架构](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247837943&idx=2&sn=32cd3f0b5fac734024986ef4df585741&chksm=e91e2b0fd5b7ea9d47547483d2aacf5521db9970c08f56434d27f643ed7a6bc421cee5838064#rd)
*量子位*

Main category: wechat.article

TL;DR: 强化学习最大的特点是：它得在真实环境里「干活」，才能学到真本事。如果Composer只在虚拟数据集里改改代码，它根本不知道这些代码有没有bug、测试能不能过。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习最大的特点是：它得在真实环境里「干活」，才能学到真本事。如果Composer只在虚拟数据集里改改代码，它根本不知道这些代码有没有bug、测试能不能过。

</details>


### [26] [同济大学汽车学院等TR-C发文！自动驾驶决策的安全<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=Mzk2OTAyMDQ1Mw==&mid=2247484760&idx=1&sn=8486cdf7e738caaf06adf486c334b283&chksm=c5cf4fe6901107bb58c7f5ad788641f31726a2389c9a3acac8f5bde68bfa050b72cd726ff77e#rd)
*Trans交通研究*

Main category: wechat.article

TL;DR: 强化学习（RL）作为一种通过与环境交互来学习最优策略的技术，已成为解决自动驾驶决策问题的关键途径。然而，将强化学习应用于真实的自动驾驶场景面临两大核心挑战：学习效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（RL）作为一种通过与环境交互来学习最优策略的技术，已成为解决自动驾驶决策问题的关键途径。然而，将强化学习应用于真实的自动驾驶场景面临两大核心挑战：学习效率和安全性。

</details>


### [27] [<em class="highlight">Agentic</em> RAG，目前最强大的RAG实现方式！](http://mp.weixin.qq.com/s?__biz=MzYzOTAxOTQ2Ng==&mid=2247483735&idx=1&sn=36dc14284d54c12be0f5ab41f0162994&chksm=f1cbf5d78682cd006cc113e5c5190dd3d2e335c6569b121e853860eaa5c2ea7f754137d487fa#rd)
*怼怼教你玩AI*

Main category: wechat.article

TL;DR: agent3：获取实时信息 。agent4：负责个性化推荐由主Agent整合输出投资建议。4.3 层级式架构：有序管理的典范。query master agent response sub-agentx sub-agenty sub-agent z figure 13： an illustration of hierarchical agentic kag。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agent3：获取实时信息 。agent4：负责个性化推荐由主Agent整合输出投资建议。4.3 层级式架构：有序管理的典范。query master agent response sub-agentx sub-agenty sub-agent z figure 13： an illustration of hierarchical agentic kag。

</details>


### [28] [麦肯锡最新洞察：<em class="highlight">智能体</em>组织（<em class="highlight">Agentic</em> Organization）——AI时代全新组织范式，与你我都相关](http://mp.weixin.qq.com/s?__biz=MzkyNjM3OTI3OA==&mid=2247484761&idx=1&sn=ca03919b48ad6e1215c093afa73487e9&chksm=c3174e174819545c5755e71d11d6dc146eb6635662e05430b30620ebcdd7ed7b39d7172818fa#rd)
*思想草堂*

Main category: wechat.article

TL;DR: 二、五大支柱：Agentic Organization的全景图紧接着，文章中结合与前沿企业合作的实践、科技领袖与投资人的洞察，以及高管们最关心的问题，提炼出了未来组织的早期信号。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 二、五大支柱：Agentic Organization的全景图紧接着，文章中结合与前沿企业合作的实践、科技领袖与投资人的洞察，以及高管们最关心的问题，提炼出了未来组织的早期信号。

</details>


### [29] [搜索框里装了个“大脑”!<em class="highlight">Agentic</em> Search不仅能搜,还能规划执行反思,彻底颠覆你的工作流](http://mp.weixin.qq.com/s?__biz=MzIyODU3NTkwNA==&mid=2247490510&idx=2&sn=23757d3f5a12e138bc7a5fd9a5d32850&chksm=e937e01b75dc0e5a9627ac178c289429591c98d75652344c270914d2c5a0025f226874635bc0#rd)
*AI资讯*

Main category: wechat.article

TL;DR: 要理解Agentic Search的技术原理，我们需要深入了解其底层架构。根据学术研究，现代的Agentic Search系统主要基于多Agent协同架构和增强RAG框架构建。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 要理解Agentic Search的技术原理，我们需要深入了解其底层架构。根据学术研究，现代的Agentic Search系统主要基于多Agent协同架构和增强RAG框架构建。

</details>


### [30] [高效可扩展的 <em class="highlight">Agentic</em> AI：当“异构系统”成为一等公民](http://mp.weixin.qq.com/s?__biz=MzkwOTQ0Njg4OA==&mid=2247485116&idx=1&sn=27bb87d617937316a0440631e8f190ef&chksm=c058b87de052e7a211ca797b230029e48f42d70292eee08d32d639de95f433343fec29b4b642#rd)
*于游的碎碎念*

Main category: wechat.article

TL;DR: 论文标题：EFFICIENT AND SCALABLE AGENTIC AI WITH HETEROGENEOUS SYSTEMS论文地址：https：//arxiv.org/pdf/2507.196351、论文要解决什么：Agent 工作流≠单次模型推理现实中的Agent不是“打一枪、出一段文本”那么简单。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 论文标题：EFFICIENT AND SCALABLE AGENTIC AI WITH HETEROGENEOUS SYSTEMS论文地址：https：//arxiv.org/pdf/2507.196351、论文要解决什么：Agent 工作流≠单次模型推理现实中的Agent不是“打一枪、出一段文本”那么简单。

</details>


### [31] [从花旗“<em class="highlight">Agentic</em> AI计划”，看企业如何拥抱AI](http://mp.weixin.qq.com/s?__biz=MzkwOTg3NjA3OQ==&mid=2247491732&idx=1&sn=6e7cff95eb26e57bc4c9aea90ad8d853&chksm=c030f5baea656e3a366a5406aab471c3cd4169dc79268d853c5f400e2312fedc513071f23f2e#rd)
*AICX*

Main category: wechat.article

TL;DR: 让我们通过花旗的“Agentic”AI 计划，洞察企业的AI管理转型样板，为我们带来启示。花旗也大张旗鼓了花旗银行在其内部平台 Stylus Workspaces 启动 “Agentic”AI 计划，让约 5000 名员工能够运用 AI Agents 开展工作。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 让我们通过花旗的“Agentic”AI 计划，洞察企业的AI管理转型样板，为我们带来启示。花旗也大张旗鼓了花旗银行在其内部平台 Stylus Workspaces 启动 “Agentic”AI 计划，让约 5000 名员工能够运用 AI Agents 开展工作。

</details>


### [32] [Adobe对<em class="highlight">智能体</em>人工智能（<em class="highlight">Agentic</em> AI）的看法：能在你常用应用中为你服务的 AI 助手](http://mp.weixin.qq.com/s?__biz=MzUyMTAzOTE4Nw==&mid=2247488681&idx=2&sn=eed649191fa0465129003d41ce15c8ac&chksm=f8c02ca84bbbcdacce6517f4e7e54e05099d6a71e55fdfa8b630ede2e04adbc97add5f9f3a2f#rd)
*TheCreativeTech*

Main category: wechat.article

TL;DR: 而这正是我们在创意应用中集成的、由智能体人工智能（Agentic AI）驱动的对话式 AI 助手的价值所在。它们不局限于生成单张图像或完成单次编辑 —— 更能连接整个创作流程：理解你的目标、在不同任务间衔接上下文、帮助你


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 而这正是我们在创意应用中集成的、由智能体人工智能（Agentic AI）驱动的对话式 AI 助手的价值所在。它们不局限于生成单张图像或完成单次编辑 —— 更能连接整个创作流程：理解你的目标、在不同任务间衔接上下文、帮助你

</details>


### [33] [当 AI 成为你的同事：Amazon 工程师的“<em class="highlight">Agentic</em> 编程革命”](http://mp.weixin.qq.com/s?__biz=MzA3NDgzMTIzMg==&mid=2247484370&idx=1&sn=d2fefa598395aa93c455a49c64947505&chksm=9e4c2143efda243b55c594d7b261b0696a9d648b8f1e457ba0f750da99b6760f4a382a8674c8#rd)
*890大家庭*

Main category: wechat.article

TL;DR: 一、什么是 Agentic 编程？Joe Magerramov 和他的团队正在 Amazon Bedrock 内部探索一种全新的开发方式——Agentic Coding（代理式编程）。与“提示一下 AI 写点代码”的传统方式不同，这种方法更像是人类工程师与智能代理共同完成整个


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一、什么是 Agentic 编程？Joe Magerramov 和他的团队正在 Amazon Bedrock 内部探索一种全新的开发方式——Agentic Coding（代理式编程）。与“提示一下 AI 写点代码”的传统方式不同，这种方法更像是人类工程师与智能代理共同完成整个

</details>


### [34] [拆解 <em class="highlight">Agentic</em> RAG 多模态实现：从自主决策到跨模态融合的核心路径](http://mp.weixin.qq.com/s?__biz=Mzk2NDk4NTU0OA==&mid=2247484341&idx=1&sn=f57de7196284b05f57dfeff3805c38e1&chksm=c5f5317e2978bf6ea22d7a43f4cab5d899dbee3012ebdcd5f5e074a114751061049118516336#rd)
*AI应用学堂*

Main category: wechat.article

TL;DR: Agentic RAG（具备代理能力的检索增强生成）的多模态实现，核心是让系统在自主决策（Agentic）的基础上，能处理、检索和融合文本、图像、音频、视频等多种模态信息，最终生成符合需求的多模态或跨模态输出。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic RAG（具备代理能力的检索增强生成）的多模态实现，核心是让系统在自主决策（Agentic）的基础上，能处理、检索和融合文本、图像、音频、视频等多种模态信息，最终生成符合需求的多模态或跨模态输出。

</details>


### [35] [告别“救火”！“<em class="highlight">代理</em>式人工智能”（<em class="highlight">Agentic</em> AI）正掀起一场IT运维革命](http://mp.weixin.qq.com/s?__biz=MjM5NzEzOTEyMQ==&mid=2649750578&idx=2&sn=67ecbbc54c6dc3e3c75f6c112639acdb&chksm=bff9f6ee66a328a5d3f49f770cdf6515668db81e28044f5fc76bda11427c6b1830bd27b875d8#rd)
*福建CIO网*

Main category: wechat.article

TL;DR: 主角就是——“代理式人工智能”（Agentic AI）。它与传统自动化（RPA或脚本）的根本区别在于：它不再是“执行”你设定的重复任务，而是能模拟人类决策，在几乎无需监督的情况下，自主推理、规划、并处理复杂任务。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 主角就是——“代理式人工智能”（Agentic AI）。它与传统自动化（RPA或脚本）的根本区别在于：它不再是“执行”你设定的重复任务，而是能模拟人类决策，在几乎无需监督的情况下，自主推理、规划、并处理复杂任务。

</details>


### [36] [【睿观】IT运营终局之战：从“救火队”到“系统建筑师”，AI自主<em class="highlight">智能体</em>（<em class="highlight">Agentic</em> AI）如何引爆下一场革命](http://mp.weixin.qq.com/s?__biz=MjM5NzEzOTEyMQ==&mid=2649750578&idx=1&sn=54922d644bf9279d078801714dfe6e19&chksm=bf3b4efb54a71f6a48240a68af0b9d8baeeef05360966de59bac780d9667408e0a6251cc3b04#rd)
*福建CIO网*

Main category: wechat.article

TL;DR: agentic ai cloud cost optimization多年来，IT运营的成功指标始终围绕着可用性、可靠性、可扩展性和高性能 。我们依赖“人+流程+技术”的铁三角组合来保障这一切。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic ai cloud cost optimization多年来，IT运营的成功指标始终围绕着可用性、可靠性、可扩展性和高性能 。我们依赖“人+流程+技术”的铁三角组合来保障这一切。

</details>


### [37] [为什么 2026 必然属于 <em class="highlight">Agentic</em> AI？](http://mp.weixin.qq.com/s?__biz=MzYyNDc0MTExMw==&mid=2247483656&idx=1&sn=d5671c2a0fa6eb995b0e1317cb54d4d5&chksm=f10f62cf088d0c8e2599fc1774969bee224f93b3becbc34213bc7a832c9d9c2d47d560c53677#rd)
*Benjamin Daoson*

Main category: wechat.article

TL;DR: 它的逻辑是：我告诉你怎么做，你照着做。而 Agentic AI 是：我告诉你要达到什么，你自己决定怎么做。这两者的差异，不是优化，而是范式分界线。能力维度


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 它的逻辑是：我告诉你怎么做，你照着做。而 Agentic AI 是：我告诉你要达到什么，你自己决定怎么做。这两者的差异，不是优化，而是范式分界线。能力维度

</details>


### [38] [华为打响<em class="highlight">Agentic</em> AI第一枪，企业运维迎来终极进化](http://mp.weixin.qq.com/s?__biz=MzYyNTQzMjUzNQ==&mid=2247484381&idx=1&sn=b827a4640251893aab588f6afa23ecbb&chksm=f1398910ab6f2e72e627a5d8d32f4b987d6091b46458e278225674d7272fd0c1f5bcef444a33#rd)
*电气混混*

Main category: wechat.article

TL;DR: Agentic AI代表着运维领域的范式转移。企业面临的选择很简单：要么主动拥抱，要么被动淘汰。正如华为公共开发部总裁陆海鸥所言：“让我们一起迈向智能运维的新纪元。”


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI代表着运维领域的范式转移。企业面临的选择很简单：要么主动拥抱，要么被动淘汰。正如华为公共开发部总裁陆海鸥所言：“让我们一起迈向智能运维的新纪元。”

</details>


### [39] [【青稞Talk 78期】从 LLM-RL 到 <em class="highlight">Agentic</em> RL：如何让语言模型成为自主<em class="highlight">智能体</em>](http://mp.weixin.qq.com/s?__biz=MzkzNTg3NTgxNQ==&mid=2247485623&idx=1&sn=798bbab920bb9142af4fdda8c0ac52c6&chksm=c37633d20c9854c42fc7fe7ff4f33ecda89f719b9a784513716066f8d7d2c4f2283417cbb7c5#rd)
*拓扑学术*

Main category: wechat.article

TL;DR: 第一章 Agentic RL 的提出与定义1.1 Agentic RL的概念演进强化学习作为训练智能体的范式，本身即包含Agent的概念，但在2025年的语境中，“Agentic RL”开始被特指为“帮助语言模型从单轮回复转变为多轮、动态环境下自主交互的智能体


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 第一章 Agentic RL 的提出与定义1.1 Agentic RL的概念演进强化学习作为训练智能体的范式，本身即包含Agent的概念，但在2025年的语境中，“Agentic RL”开始被特指为“帮助语言模型从单轮回复转变为多轮、动态环境下自主交互的智能体

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [40] [Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification](https://arxiv.org/abs/2510.24749)
*Aofan Liu,Shiyuan Song,Haoxuan Li,Cehao Yang,Yiyan Qi*

Main category: cs.SE

TL;DR: 提出了RepoAlign-Bench基准和ReflectCode方法，用于解决代码库级别代码检索问题，在变更请求驱动场景下实现12.2%的Top-5准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现代代码库日益复杂，需要能够理解跨组件变更意图的检索系统，而传统函数级搜索范式缺乏这种能力。

Method: 提出ReflectCode方法，采用对抗反射增强的双塔架构，包含解耦的代码编码器和文档编码器，通过大语言模型引导的反射动态整合语法模式、函数依赖和语义扩展意图。

Result: ReflectCode在Top-5准确率上比现有最佳方法提升12.2%，在召回率上提升7.1%。

Conclusion: 为上下文感知的代码检索开辟了新方向，从函数中心匹配转向整体代码库级推理。

Abstract: The escalating complexity of modern codebases has intensified the need for
retrieval systems capable of interpreting cross-component change intents, a
capability fundamentally absent in conventional function-level search
paradigms. While recent studies have improved the alignment between natural
language queries and code snippets, retrieving contextually relevant code for
specific change requests remains largely underexplored. To address this gap, we
introduce RepoAlign-Bench, the first benchmark specifically designed to
evaluate repository-level code retrieval under change request driven scenarios,
encompassing 52k annotated instances. This benchmark shifts the retrieval
paradigm from function-centric matching to holistic repository-level reasoning.
Furthermore, we propose ReflectCode, an adversarial reflection augmented
dual-tower architecture featuring disentangled code_encoder and doc_encoder
components. ReflectCode dynamically integrates syntactic patterns, function
dependencies, and semantic expansion intents through large language model
guided reflection. Comprehensive experiments demonstrate that ReflectCode
achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over
state-of-the-art baselines, establishing a new direction for context-aware code
retrieval.

</details>


### [41] [A Roadmap for Tamed Interactions with Large Language Models](https://arxiv.org/abs/2510.24819)
*Vincenzo Scotti,Jan Keim,Tobias Hey,Andreas Metzger,Anne Koziolek,Raffaela Mirandola*

Main category: cs.SE

TL;DR: 提出LLM脚本语言(LSL)的概念，旨在通过领域特定语言来规范和控制LLM交互，提高AI应用的可靠性、鲁棒性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用虽然令人印象深刻，但其不可靠性和幻觉问题阻碍了实际应用。需要软件工程工具来约束LLM输出，提供更强的保证。

Method: 开发领域特定语言(DSL)用于脚本化LLM交互，通过LSL来控制LLM输出、强制交互结构，并与验证、验证和可解释性集成。

Result: 提出了LSL的愿景框架，使LLM交互可编程且与训练或实现解耦。

Conclusion: LSL可能是改进基于AI应用的关键，能够解决当前LLM软件在可靠性、鲁棒性和可信度方面的局限性。

Abstract: We are witnessing a bloom of AI-powered software driven by Large Language
Models (LLMs). Although the applications of these LLMs are impressive and
seemingly countless, their unreliability hinders adoption. In fact, the
tendency of LLMs to produce faulty or hallucinated content makes them
unsuitable for automating workflows and pipelines. In this regard, Software
Engineering (SE) provides valuable support, offering a wide range of formal
tools to specify, verify, and validate software behaviour. Such SE tools can be
applied to define constraints over LLM outputs and, consequently, offer
stronger guarantees on the generated content. In this paper, we argue that the
development of a Domain Specific Language (DSL) for scripting interactions with
LLMs using an LLM Scripting Language (LSL) may be key to improve AI-based
applications. Currently, LLMs and LLM-based software still lack reliability,
robustness, and trustworthiness, and the tools or frameworks to cope with these
issues suffer from fragmentation. In this paper, we present our vision of LSL.
With LSL, we aim to address the limitations above by exploring ways to control
LLM outputs, enforce structure in interactions, and integrate these aspects
with verification, validation, and explainability. Our goal is to make LLM
interaction programmable and decoupled from training or implementation.

</details>


### [42] [Automating Benchmark Design](https://arxiv.org/abs/2510.25039)
*Amanda Dsouza,Harit Vishwakarma,Zhengyang Qi,Justin Bauer,Derek Pham,Thomas Walshe,Armin Parchami,Frederic Sala,Paroma Varma*

Main category: cs.SE

TL;DR: BeTaL是一个利用LLM自动设计动态基准测试的框架，通过参数化基准模板和LLM推理来获得目标属性（如难度和真实性），相比静态基准有显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前LLM和LLM驱动的智能体发展迅速，但评估能力跟不上。手工制作的静态基准容易饱和，而动态基准创建和更新成本高昂。

Method: BeTaL框架利用环境设计原则，参数化基准模板的关键设计选择，使用LLM推理参数空间以成本高效的方式获得目标属性。

Result: BeTaL创建的基准更接近目标难度，平均偏差在5.3%到13.2%之间，比基线方法提高了2-4倍。创建了两个新基准并扩展了τ-bench。

Conclusion: BeTaL能够有效自动化动态基准设计过程，显著改善基准测试的质量和适应性。

Abstract: The rapid progress and widespread deployment of LLMs and LLM-powered agents
has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are
the primary tool for assessing model capabilities, but these quickly become
saturated. In contrast, dynamic benchmarks evolve alongside the models they
evaluate, but are expensive to create and continuously update. To address these
challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a
framework that leverages environment design principles to automate the process
of dynamic benchmark design. BeTaL works by parameterizing key design choices
in base benchmark templates and uses LLMs to reason through the resulting
parameter space to obtain target properties (such as difficulty and realism) in
a cost-efficient manner. We validate this approach on its ability to create
benchmarks with desired difficulty levels. Using BeTaL, we create two new
benchmarks and extend a popular agentic benchmark $\tau$-bench. Extensive
evaluation on these three tasks and multiple target difficulty levels shows
that BeTaL produces benchmarks much closer to the desired difficulty, with
average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the
baselines.

</details>


### [43] [Automated Program Repair Based on REST API Specifications Using Large Language Models](https://arxiv.org/abs/2510.25148)
*Katsuki Yamagishi,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: 提出dcFix方法，用于检测和自动修复客户端程序中的REST API误用问题，通过识别不符合规范的代码片段并结合API规范生成提示，利用LLM生成修正代码。


<details>
  <summary>Details</summary>
Motivation: 开发者在测试阶段才能发现REST API规范违反问题，错误信息缺乏有效诊断细节，调试过程需要反复试错。

Method: 识别不符合规范的代码片段，将其与相关API规范整合到提示中，利用大型语言模型生成修正代码。

Result: 评估显示dcFix能准确检测误用，并且在性能上优于基线方法（基线方法在提示中不包含代码片段不符合REST API规范的指示）。

Conclusion: dcFix方法能有效检测和修复REST API误用问题，通过结合代码片段和API规范信息，利用LLM生成更准确的修正代码。

Abstract: Many cloud services provide REST API accessible to client applications.
However, developers often identify specification violations only during
testing, as error messages typically lack the detail necessary for effective
diagnosis. Consequently, debugging requires trial and error. This study
proposes dcFix, a method for detecting and automatically repairing REST API
misuses in client programs. In particular, dcFix identifies non-conforming code
fragments, integrates them with the relevant API specifications into prompts,
and leverages a Large Language Model (LLM) to produce the corrected code. Our
evaluation demonstrates that dcFix accurately detects misuse and outperforms
the baseline approach, in which prompts to the LLM omit any indication of code
fragments non conforming to REST API specifications.

</details>


### [44] [Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases](https://arxiv.org/abs/2510.25297)
*Hidetake Tanaka,Haruto Tanaka,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 研究比较了基于属性的测试(PBT)和基于示例的测试(EBT)在检测LLM生成代码边缘案例方面的效果，发现两者结合可将缺陷检测率从68.75%提升至81.25%。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中生成代码的普及，确保LLM生成代码的质量变得重要。传统的基于示例测试方法经常遗漏边缘案例。

Method: 分析16个HumanEval问题，使用Claude-4-sonnet生成PBT和EBT测试代码，比较两种方法在检测边缘案例方面的表现。

Result: 单独使用PBT或EBT的缺陷检测率为68.75%，但两者结合后检测率提升至81.25%。PBT擅长检测性能问题和广泛输入空间探索，EBT擅长检测特定边界条件和特殊模式。

Conclusion: 结合PBT和EBT的混合方法可以提高LLM生成代码的可靠性，为基于LLM的代码生成测试策略提供指导。

Abstract: As Large Language Models (LLMs) increasingly generate code in software
development, ensuring the quality of LLM-generated code has become important.
Traditional testing approaches using Example-based Testing (EBT) often miss
edge cases -- defects that occur at boundary values, special input patterns, or
extreme conditions. This research investigates the characteristics of
LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge
cases. We analyze 16 HumanEval problems where standard solutions failed on
extended test cases, generating both PBT and EBT test codes using
Claude-4-sonnet. Our experimental results reveal that while each method
individually achieved a 68.75\% bug detection rate, combining both approaches
improved detection to 81.25\%. The analysis demonstrates complementary
characteristics: PBT effectively detects performance issues and edge cases
through extensive input space exploration, while EBT effectively detects
specific boundary conditions and special patterns. These findings suggest that
a hybrid approach leveraging both testing methods can improve the reliability
of LLM-generated code, providing guidance for test generation strategies in
LLM-based code generation.

</details>


### [45] [Dissect-and-Restore: AI-based Code Verification with Transient Refactoring](https://arxiv.org/abs/2510.25406)
*Changjie Wang,Mariano Scazzariello,Anoud Alshnaka,Roberto Guanciale,Dejan Kostić,Marco Chiesa*

Main category: cs.SE

TL;DR: Prometheus是一个AI辅助的自动化代码验证系统，通过模块化重构将复杂程序分解为可验证的小组件，然后重新组合构建原始程序的证明。


<details>
  <summary>Details</summary>
Motivation: 形式化验证是构建可靠软件系统的关键，但需要专业知识且成本高昂。现有AI系统能识别数学证明模式但难以有效集成到验证过程中。

Method: 使用模块化软件工程原则，将复杂程序逻辑分解为可验证组件，通过结构化分解引导证明搜索，用户可提供自然语言指导。

Result: 在定制数据集中成功验证86%的任务，相比基线68%有显著提升。对于复杂规范从30%提升到69%，集成证明大纲后从25%提升到87%。

Conclusion: 模块化重构能显著提高AI验证组件的有效性，该方法在复杂规范验证中表现尤为突出。

Abstract: Formal verification is increasingly recognized as a critical foundation for
building reliable software systems. However, the need for specialized expertise
to write precise specifications, navigate complex proof obligations, and learn
annotations often makes verification an order of magnitude more expensive than
implementation. While modern AI systems can recognize patterns in mathematical
proofs and interpret natural language, effectively integrating them into the
formal verification process remains an open challenge. We present Prometheus, a
novel AI-assisted system that facilitates automated code verification with
current AI capabilities in conjunction with modular software engineering
principles (e.g., modular refactoring). Our approach begins by decomposing
complex program logic, such as nested loops, into smaller, verifiable
components. Once verified, these components are recomposed to construct a proof
of the original program. This decomposition-recomposition workflow is
non-trivial. Prometheus addresses this by guiding the proof search through
structured decomposition of complex lemmas into smaller, verifiable sub-lemmas.
When automated tools are insufficient, users can provide lightweight natural
language guidance to steer the proof process effectively. Our evaluation
demonstrates that transiently applying modular restructuring to the code
substantially improves the AI's effectiveness in verifying individual
components. This approach successfully verifies 86% of tasks in our curated
dataset, compared to 68% for the baseline. Gains are more pronounced with
increasing specification complexity, improving from 30% to 69%, and when
integrating proof outlines for complex programs, from 25% to 87%.

</details>


### [46] [What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow](https://arxiv.org/abs/2510.25423)
*Ali Asgari,Annibale Panichella,Pouria Derakhshanfar,Mitchell Olsthoorn*

Main category: cs.SE

TL;DR: 通过对Stack Overflow上AI代理开发者讨论的分析，识别出7大类77个技术挑战，涵盖运行时集成、依赖管理、编排复杂性和评估可靠性等问题，并量化了问题的流行度和解决难度。


<details>
  <summary>Details</summary>
Motivation: AI代理在研究和工业界迅速流行，但开发者在构建、部署和维护这些新兴系统时面临持续且未被充分探索的挑战，需要系统性地识别这些问题。

Method: 通过标签扩展和过滤构建开发者挑战分类法，应用LDA-MALLET进行主题建模，并手动验证和标记结果主题，分析2021-2025年间的数据演变。

Result: 识别出7个主要问题领域和77个具体技术挑战，量化了主题流行度和解决难度，绘制了代理开发中使用的工具和编程语言地图。

Conclusion: 研究结果为从业者、研究人员和教育工作者在代理可靠性和开发者支持方面提供了具体指导。

Abstract: AI agents have rapidly gained popularity across research and industry as
systems that extend large language models with additional capabilities to plan,
use tools, remember, and act toward specific goals. Yet despite their promise,
developers face persistent and often underexplored challenges when building,
deploying, and maintaining these emerging systems. To identify these
challenges, we study developer discussions on Stack Overflow, the world's
largest developer-focused Q and A platform with about 60 million questions and
answers and 30 million users. We construct a taxonomy of developer challenges
through tag expansion and filtering, apply LDA-MALLET for topic modeling, and
manually validate and label the resulting themes. Our analysis reveals seven
major areas of recurring issues encompassing 77 distinct technical challenges
related to runtime integration, dependency management, orchestration
complexity, and evaluation reliability. We further quantify topic popularity
and difficulty to identify which issues are most common and hardest to resolve,
map the tools and programming languages used in agent development, and track
their evolution from 2021 to 2025 in relation to major AI model and framework
releases. Finally, we present the implications of our results, offering
concrete guidance for practitioners, researchers, and educators on agent
reliability and developer support.

</details>


### [47] [Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents](https://arxiv.org/abs/2510.25694)
*Jiayi Kuang,Yinghui Li,Xin Zhang,Yangning Li,Di Yin,Xing Sun,Ying Shen,Philip S. Yu*

Main category: cs.SE

TL;DR: Enconda-bench是首个提供环境配置过程级能力评估的基准测试框架，通过注入真实README错误和Docker验证，评估智能体在环境设置、错误诊断和修复方面的细粒度能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅评估端到端构建/测试成功率，无法揭示智能体在环境配置中成功或失败的具体原因和过程，限制了软件工程智能体的改进。

Method: 通过自动注入真实README错误构建任务实例，在Docker中进行可扩展的高质量评估，结合过程级分析和端到端可执行性来评估智能体的细粒度能力。

Result: 评估显示，虽然智能体能够定位错误，但在将反馈转化为有效修正方面存在困难，限制了端到端性能表现。

Conclusion: Enconda-bench为环境配置提供了首个过程级内部能力评估框架，为改进软件工程智能体提供了可操作的见解。

Abstract: Large language model-based agents show promise for software engineering, but
environment configuration remains a bottleneck due to heavy manual effort and
scarce large-scale, high-quality datasets. Existing benchmarks assess only
end-to-end build/test success, obscuring where and why agents succeed or fail.
We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,
which provides process-level trajectory assessment of fine-grained agent
capabilities during environment setup-planning, perception-driven error
diagnosis, feedback-driven repair, and action to execute final environment
configuration. Our task instances are automatically constructed by injecting
realistic README errors and are validated in Docker for scalable, high-quality
evaluation. Enconda-bench combines process-level analysis with end-to-end
executability to enable capability assessments beyond aggregate success rates.
Evaluations across state-of-the-art LLMs and agent frameworks show that while
agents can localize errors, they struggle to translate feedback into effective
corrections, limiting end-to-end performance. To our knowledge, Enconda-bench
is the first framework to provide process-level internal capability assessment
for environment configuration, offering actionable insights for improving
software engineering agents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [49] [Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought](https://arxiv.org/abs/2510.24941)
*Jiachen Zhao,Yiyou Sun,Weiyan Shi,Dawn Song*

Main category: cs.LG

TL;DR: 研究发现大语言模型生成的思维链中许多推理步骤并不真正影响最终预测，提出了真实思维评分来量化步骤的因果影响，并揭示了模型在真实思考和装饰性思考之间的交替现象。


<details>
  <summary>Details</summary>
Motivation: 当前假设思维链步骤忠实反映模型内部思考过程，但研究发现许多步骤只是装饰性的，并不真正贡献于预测，这影响了LLM推理效率和思维链的可信度。

Method: 提出真实思维评分(TTS)来测量每个推理步骤对最终预测的因果影响，并识别了LLM潜在空间中的真实思维方向，通过沿该方向引导可以控制模型执行或忽略特定推理步骤。

Result: 在AIME数据集上，只有平均2.3%的思维链步骤具有高TTS(≥0.7)，自我验证步骤也可能是装饰性的，沿真实思维方向引导可以改变最终结果。

Conclusion: LLM经常口头表达推理步骤但内部并不真正执行，这削弱了LLM推理效率和思维链的可信度。

Abstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)
at test time, enabling them to solve complex tasks. These reasoning steps in
CoT are often assumed as a faithful reflection of the model's internal thinking
process, and used to monitor unsafe intentions. However, we find many reasoning
steps don't truly contribute to LLMs' prediction. We measure the step-wise
causal influence of each reasoning step on the model's final prediction with a
proposed True Thinking Score (TTS). We reveal that LLMs often interleave
between true-thinking steps (which are genuinely used to produce the final
output) and decorative-thinking steps (which only give the appearance of
reasoning but have minimal causal impact). Notably, only a small subset of the
total reasoning steps have a high TTS that causally drive the model's
prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning
steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.
Furthermore, we identify a TrueThinking direction in the latent space of LLMs.
By steering along or against this direction, we can force the model to perform
or disregard certain CoT steps when computing the final result. Finally, we
highlight that self-verification steps in CoT (i.e., aha moments) can also be
decorative, where LLMs do not truly verify their solution. Steering along the
TrueThinking direction can force internal reasoning over these steps, resulting
in a change in the final results. Overall, our work reveals that LLMs often
verbalize reasoning steps without actually performing them internally, which
undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.

</details>


### [50] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion是一种风险感知的采样方法，通过将去噪步骤视为顺序假设检验，为扩散策略提供统计意义上的风险控制。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略在离线强化学习中缺乏统计风险概念，通常使用启发式方法进行采样指导。

Method: 在DDPM结构下保持训练不变，在推理时引入LRT引导，通过累积对数似然比并使用逻辑控制器门控条件均值，实现风险校准。

Result: 在D4RL MuJoCo任务中，LRT-Diffusion相比强Q引导基线改善了回报与OOD权衡，同时满足指定的alpha水平。

Conclusion: LRT-Diffusion是一种即插即用的推理时方法，为离线RL的扩散策略添加了原则性的、校准的风险控制。

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [51] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种将自监督Transformer变化点检测模块集成到Option-Critic框架中的新架构，通过自适应分割状态轨迹来发现选项，解决了分层强化学习中子目标发现和选项终止边界学习的挑战。


<details>
  <summary>Details</summary>
Motivation: 分层强化学习在长时程任务中通过引入时间抽象来增强决策的可扩展性，但实际应用中面临自主发现语义上有意义的子目标和学习最优选项终止边界的挑战。

Method: 集成自监督Transformer变化点检测模块到Option-Critic框架，使用启发式伪标签训练CPD模块推断环境动态的潜在变化，利用推断的变化点来稳定终止函数梯度、预训练内部选项策略，并通过选项间差异惩罚强制功能专业化。

Result: 在Four-Rooms和Pinball任务上的实验表明，CPD引导的智能体表现出加速收敛、更高的累积回报和显著改进的选项专业化。

Conclusion: 通过变化点分割整合结构先验可以在复杂环境中产生更可解释、样本效率更高和更鲁棒的分层策略。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [52] [Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning](https://arxiv.org/abs/2510.25311)
*Sagalpreet Singh,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 提出一种新的强化学习算法，在最大化期望回报的同时，确保策略在目标状态上的边际状态分布更加分散均匀。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法倾向于利用少数奖励源，但在许多自然场景中，需要学习既能最大化回报又能均匀访问多个目标状态的策略。现有方法如熵正则化和内在奖励主要关注探索而非状态分布的分散性。

Method: 提出多目标强化学习框架，使用状态空间上的预言机分类器确定目标状态。通过优化自定义RL奖励函数，基于当前策略混合计算奖励，利用离线RL算法更新策略混合。

Result: 算法在合成MDP和标准RL环境中有效，能够学习到高回报且目标状态分布分散的策略。

Conclusion: 该算法成功解决了在最大化期望回报的同时实现目标状态均匀访问的问题，具有理论性能保证。

Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy
that maximizes expected return. As a result, the learned policy can exploit one
or few reward sources. However, in many natural situations, it is desirable to
learn a policy that induces a dispersed marginal state distribution over
rewarding states, while maximizing the expected return which is typically tied
to reaching a goal state. This aspect remains relatively unexplored. Existing
techniques based on entropy regularization and intrinsic rewards use
stochasticity for encouraging exploration to find an optimal policy which may
not necessarily lead to dispersed marginal state distribution over rewarding
states. Other RL algorithms which match a target distribution assume the latter
to be available apriori. This may be infeasible in large scale systems where
enumeration of all states is not possible and a state is determined to be a
goal state only upon reaching it. We formalize the problem of maximizing the
expected return while uniformly visiting the goal states as Multi Goal RL in
which an oracle classifier over the state space determines the goal states. We
propose a novel algorithm that learns a high-return policy mixture with
marginal state distribution dispersed over the set of goal states. Our
algorithm is based on optimizing a custom RL reward which is computed - based
on the current policy mixture - at each iteration for a set of sampled
trajectories. The latter are used via an offline RL algorithm to update the
policy mixture. We prove performance guarantees for our algorithm, showing
efficient convergence bounds for optimizing a natural objective which captures
the expected return as well as the dispersion of the marginal state
distribution over the goal states. We design and perform experiments on
synthetic MDPs and standard RL environments to evaluate the effectiveness of
our algorithm.

</details>


### [53] [Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization](https://arxiv.org/abs/2510.25616)
*Nikita Kachaev,Mikhail Kolosov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 该论文研究了视觉-语言-动作(VLA)模型在动作微调过程中视觉表示退化的问题，提出了一种简单有效的对齐方法来缓解这种退化并提高OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究VLA模型在动作微调过程中，预训练视觉语言模型(VLMs)原有的视觉表示和知识保留程度，以及如何缓解动作微调导致的视觉表示退化问题。

Method: 通过探测VLA模型的隐藏表示和分析注意力图，设计针对性任务对比VLA模型与对应VLMs，评估多种视觉表示对齐策略，并提出简单有效的对齐方法。

Result: 发现朴素的动作微调会导致视觉表示退化，提出的对齐方法能够有效缓解这种退化并改善OOD场景的泛化性能。

Conclusion: 阐明了动作微调与视觉表示退化之间的权衡关系，并提供了恢复继承VL能力的实用方法。

Abstract: The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [54] [The Coding Personalities of Leading LLMs](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fthe-coding-personalities-of-leading-llms%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-state-of-llms25%26utm_content=newsletter-ai-primary-expandedllm-251028-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/0100019a2b0cf95e-2469d6e9-d470-4a64-a024-ae9e166a4057-000000/m8NWlNgble9MPWzY0Ic9Q9yfm2A3-AAYu-OUML2hphc=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该研究分析了GPT-5、Claude Sonnet 4和Llama 3等领先LLM在代码生成方面的独特"编码个性"，发现尽管它们有共同的优缺点，但每个模型都有可测量的独特编码风格。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示不同LLM在代码生成任务中表现出的独特编码风格和个性特征，帮助开发者更好地理解和选择适合特定需求的模型。

Method: 通过Sonar的State of Code报告，对GPT-5、Claude Sonnet 4和Llama 3等模型进行测试分析，识别它们的编码个性和表现模式。

Result: 发现了LLM在代码生成中的共享优势和缺陷，同时识别出每个模型独特的编码原型和个性特征，结果令人惊讶。

Conclusion: 每个领先的LLM都有其独特的编码个性，这些个性特征会影响它们的代码生成结果，开发者需要根据具体需求选择合适的模型。

Abstract: The Coding Personalities of Leading LLMs (Sponsor) Think the newest LLM is the best for coding? Despite their shared strengths and flaws, each LLM has a unique and inherent style—a measurable “coding personality” that drives their distinct results.Sonar put GPT-5, Claude Sonnet 4, and Llama 3 models to the test in their latest State of Code report - and found some surprising results. Read it to discover: The shared strengths and flaws of LLMs Coding archetypes for the leading LLMs Hidden qual...

</details>


### [55] [Speedrunning an RL environment](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsidb.in%2Fposts%2Frl-env-speedrun%3Futm_source=tldrai/1/0100019a2b0cf95e-2469d6e9-d470-4a64-a024-ae9e166a4057-000000/R_U1Ws4hhsJguo-nxH9xrlOpj6D6SUGDOT0SplRMIJM=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 介绍RL环境的概念、'verifiers'框架，并指导如何为AgentDojo基准创建RL环境


<details>
  <summary>Details</summary>
Motivation: RL环境可以复杂且有趣，需要解释其概念并展示如何创建环境用于LLM评估和训练

Method: 使用'verifiers'框架，通过定义迷宫、奖励和LLM导航方式来设计RL环境

Result: 成功创建了AgentDojo基准的RL环境，展示了环境设计过程

Conclusion: RL环境设计是LLM评估和训练的重要环节，需要仔细定义场景、奖励机制和导航方式

Abstract: Speedrunning an RL environment (22 minute read) RL environments can be surprisingly complex and fun to create. This post explains what RL environments are, introduces the 'verifiers' framework, and also walks readers through how to create an environment for a benchmark called AgentDojo. RL environments are scenarios that LLMs operate in for evaluation or training. Designing them means essentially defining the maze, the rewards, and how the LLM navigates through it.

</details>


### [56] [On-Policy Distillation](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthinkingmachines.ai%2Fblog%2Fon-policy-distillation%2F%3Futm_source=tldrai/1/0100019a2b0cf95e-2469d6e9-d470-4a64-a024-ae9e166a4057-000000/4-g42oOsFqCsv58sSAOALA6PZlVwq6Trgf6UjR1IVsQ=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 训练小型AI模型通过让它们从自身错误中学习，使用更大的教师模型进行评分，可以达到与强化学习相同的推理性能，但成本降低9-30倍。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习训练成本极高（需要17,920 GPU小时），而传统蒸馏方法缺乏相关性。作者希望找到一种既能保持学习相关性又能降低训练成本的方法。

Method: 提出"策略蒸馏"方法，结合了从自身输出学习的相关性和传统蒸馏的密集反馈。通过让小型模型从自身错误中学习，并用大型教师模型进行评分。

Result: 在AIME'24数学问题上达到70%的准确率，仅需150个训练步骤，相比强化学习的17,920 GPU小时大幅降低训练成本。

Conclusion: 策略蒸馏方法在保持推理性能的同时，将训练成本降低了9-30倍，证明了从自身错误中学习结合教师模型反馈的有效性。

Abstract: On-Policy Distillation (25 minute read) Thinking Machines Lab demonstrated that training smaller AI models by having them learn from their own mistakes, by grading them with a larger teacher model, achieves the same reasoning performance as RL at 9-30x lower cost. "On-policy distillation" combines the relevance of learning from your own outputs with the dense feedback of traditional distillation, reaching 70% on AIME'24 math problems in just 150 training steps compared to 17,920 GPU hours of RL.

</details>


### [57] [Optimizing repos for AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftombedor.dev%2Foptimizing-repos-for-ai%2F%3Futm_source=tldrnewsletter/1/0100019a2f7df7a9-2f628b0d-9852-41c9-a592-9eb728b57933-000000/ErzRUfTscv4edsncW6jkj2wpKvM2Ez_XIRDtMpPmtMA=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 提出优化代码仓库以提升AI代理效率的方法，包括增强静态分析、使用'just'工具管理重复命令、以及优化文档组织结构


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在处理代码仓库时效率不高，需要通过优化仓库结构来提升AI的理解和执行效率

Method: 采用三种方法：增加静态分析、使用'just'工具简化重复命令、合理组织文档结构并在指令中引用

Result: 通过上述优化措施，能够显著提升AI代理在代码仓库中的工作效率和准确性

Conclusion: 优化代码仓库结构对于提升AI代理性能至关重要，建议开发者采用这些最佳实践

Abstract: Optimizing repos for AI (4 minute read) Optimize repositories for AI by increasing static analysis, using 'just' for repeated agent commands, and organizing documents in a folder and referencing them in agent instructions.

</details>


### [58] [Agentic AI and Security](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinfowler.com%2Farticles%2Fagentic-ai-security.html%3Futm_source=tldrwebdev/1/0100019a2fa7f75e-d38ecbc2-9f85-4095-8c89-ca1521d9397e-000000/tn7zh2ekv4RCUqlNgkvDxqbaJP3uIvQJfa7eEOmITEw=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agentic AI系统存在根本性安全漏洞：LLM无法区分指令和数据，容易受到提示注入攻击，攻击者可通过在不受信任内容中嵌入隐藏命令来窃取敏感信息。


<details>
  <summary>Details</summary>
Motivation: 随着Agentic AI系统在敏感环境中的部署增加，其安全漏洞可能导致严重的数据泄露风险，需要系统性的安全防护方案。

Method: 分析Agentic AI系统的安全威胁模型，识别当AI能够访问敏感数据、读取不受信任内容并能外部通信时的安全风险，提出防护措施。

Result: 识别出Agentic AI系统在特定配置下（访问敏感数据+读取不受信任内容+外部通信能力）存在严重安全漏洞，攻击者可通过提示注入窃取信息。

Conclusion: Agentic AI系统需要建立严格的安全边界和访问控制机制，防止提示注入攻击导致的数据泄露。

Abstract: Agentic AI and Security (24 minute read) Agentic AI systems have a fundamental security flaw: LLMs cannot separate instructions from data, making them vulnerable to prompt injection attacks where untrusted content contains hidden commands. The worst security occurs when an AI has access to sensitive data, reads untrusted content, and can communicate externally, allowing attackers to steal information by embedding instructions in sources like Jira tickets or web pages. To prevent this, it's be...

</details>


### [59] [Sentry's AI Code Review predicts what's going to break - based on what's already broken](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sentry.io%2Fsentry-ai-code-review-now-in-beta-break-production-less%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/1/0100019a2fa7f75e-d38ecbc2-9f85-4095-8c89-ca1521d9397e-000000/uMlV11XeMRvHGAvnT9SJ0dXWRcAG_sNr8xg3Xg8roSA=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sentry的AI代码审查工具通过分析代码变更、上下文和问题历史来预测可能导致生产环境故障的问题，提供具体可行的反馈而非通用代码风格建议。


<details>
  <summary>Details</summary>
Motivation: 传统代码审查过于关注代码风格细节，而缺乏对可能导致生产环境故障的实际问题的预测能力。

Method: 结合代码变更、上下文信息和问题历史，分析函数调用、类或对象依赖关系、数据库连接等，识别潜在风险。

Result: 开发了能够提供具体、可操作反馈的AI代码审查工具，帮助开发者预防生产环境故障。

Conclusion: AI代码审查工具能够更有效地识别和预防生产环境中的潜在问题，提升代码质量。

Abstract: Sentry's AI Code Review predicts what's going to break - based on what's already broken (Sponsor) Code reviews should be less style nits and more "this is going to break prod". Sentry's AI Code Review blends context and issue history with the code you just touched - function calls, class or objects dependencies, database connections - to provide specific and actionable feedback rather than generic linting advice. Read the blog

</details>


### [60] [I've been loving Claude Code on the Web](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fben.page%2Fclaude-code-web%3Futm_source=tldrwebdev/1/0100019a2fa7f75e-d38ecbc2-9f85-4095-8c89-ca1521d9397e-000000/kVbV05tu3Rlo6jmAuYXWKD3te8LECgcApGVtisYgzx4=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code在网页和iOS应用上实现了可靠的"自动完成待办事项"功能


<details>
  <summary>Details</summary>
Motivation: 提供一种能够自动完成待办事项的智能助手工具，提升用户生产力

Method: 在网页和iOS平台上开发Claude Code应用，实现待办事项的自动处理功能

Result: Claude Code在网页和iOS应用上表现出色，能够可靠地自动完成待办事项

Conclusion: Claude Code作为"自动完成待办事项"工具具有实用价值，在网页和iOS平台上运行稳定

Abstract: I've been loving Claude Code on the Web (2 minute read) Claude Code on the web and its iOS app have a solid and dependable implementation as a "to-do list that does itself.”

</details>


### [61] [Build Production-Ready AI Workflows Without Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdraftnrun.com%2F%3Futm_source=tldrdesign/1/0100019a2feab1bb-7f08164c-970e-45aa-91c5-d4105767be2f-000000/-5cbGDyUnAi8KOsI1eTX9MNip6DVi3IxWd9u7e-nICQ=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 构建无需代码的生产级AI工作流平台，提供内置DevOps、可观测性和治理功能，帮助企业快速从原型到生产部署AI应用


<details>
  <summary>Details</summary>
Motivation: 降低AI应用开发门槛，让非技术用户也能构建和部署企业级AI解决方案，解决从原型到生产部署的摩擦问题

Method: 提供无需代码的平台，内置DevOps工具链、可观测性监控和治理框架，支持AI聊天机器人构建、工作流自动化和智能体AI应用

Result: 能够显著缩短从原型到生产的时间，实现企业级AI解决方案的无缝部署

Conclusion: 无需代码的AI工作流平台是推动AI在企业中大规模应用的有效途径，降低了技术门槛和部署成本

Abstract: Build Production-Ready AI Workflows Without Code (Website) Move from prototype to production in record time, with built-in DevOps, observability, and governance. Build AI chatbots, automate workflows, adopt agentic AI, and deploy enterprise-ready solutions without friction.

</details>


### [62] [AI browsers face a security flaw as inevitable as death and taxes](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theregister.com%2F2025%2F10%2F28%2Fai_browsers_prompt_injection%2F%3Futm_source=tldrinfosec/1/0100019a30149446-f3d3ce19-f96e-4e98-84e5-43ffd6165df9-000000/Mtohf-bhmNybuP5xuWF7gJuIzkmodIT9s7ZZVB7DR0Q=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI浏览器面临不可避免的安全漏洞——提示注入攻击，随着AI浏览器获得代理能力，攻击者可以通过URL直接注入或隐藏在网站文本中的间接注入来欺骗机器人执行危险操作。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动的浏览器获得代理能力，能够代表用户执行操作，这带来了新的安全威胁，特别是提示注入攻击，攻击者可以欺骗AI代理执行未经用户授权的危险行为。

Method: 分析了AI浏览器面临的新型安全威胁——提示注入攻击，包括直接注入（通过URL）和间接注入（隐藏在网站或文档文本中）两种攻击向量。

Result: 识别出AI浏览器在获得代理能力后面临严重的安全漏洞，攻击者可以利用提示注入技术绕过安全措施，控制AI代理执行恶意操作。

Conclusion: 提示注入是AI浏览器不可避免的安全缺陷，需要开发新的安全机制来应对这种新型威胁，确保AI代理在代表用户操作时的安全性。

Abstract: AI browsers face a security flaw as inevitable as death and taxes (8 minute read) Prompt injection is an increasingly critical security issue as AI-driven browsers gain agentic capabilities: they can now act on users' behalf, from opening web pages to handling emails and files. This opens up new attack vectors, such as direct injection via URLs or indirect injection hidden within website or document text, allowing attackers to trick bots into performing dangerous actions without user consent....

</details>


### [63] [Introducing Agent HQ: Any agent, any way you work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fnews-insights%2Fcompany-news%2Fwelcome-home-agents%2F%3Futm_source=tldrai/1/0100019a301d98b9-5abe39bf-1731-4fb6-9b68-ae16cd066226-000000/MOpsbLjJmPZLNF2lYVlkAl_5ZWOb-oDjTXU75FJ-erk=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent HQ将GitHub转变为开放生态系统，整合多个AI代理提供商（如Anthropic、OpenAI、Google等）到统一平台，通过任务控制中心管理代理舰队。


<details>
  <summary>Details</summary>
Motivation: 解决当前AI代理生态系统碎片化问题，为用户提供统一的代理管理平台，简化工作流程。

Method: 在GitHub平台内集成多个AI代理提供商的服务，通过任务控制中心统一管理和调度不同代理。

Result: GitHub Copilot订阅用户将能在GitHub内直接使用来自多个提供商的AI代理服务。

Conclusion: Agent HQ通过统一平台解决了代理生态碎片化问题，提升了开发者的工作效率。

Abstract: Introducing Agent HQ: Any agent, any way you work (10 minute read) Agent HQ transforms GitHub into an open ecosystem that unites every agent on a single platform. Coding agents from Anthropic, OpenAI, Google, Cognition, xAI, and other providers will become available directly within GitHub as part of the GitHub Copilot subscription over the coming months. Agent HQ's power comes from a unified command center called 'mission control' that allows users to choose from a fleet of agents, assign the...

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [64] [Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading](https://arxiv.org/abs/2510.25014)
*Minkyung Kim,Junsik Kim,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 提出Autoregressive State-Tracking Prompting (ASTP)方法，通过显式状态跟踪和占位符后处理，解决LLM在游戏交易系统中遵循程序流程的问题，实现>99%状态合规性和99.3%计算精度。


<details>
  <summary>Details</summary>
Motivation: LLM在动态游戏交互中缺乏对规则治理交易系统基本程序流程的遵循能力，这会削弱玩家信任。需要解决LLM创意灵活性与游戏交易程序需求之间的核心矛盾。

Method: 引入ASTP方法，通过精心设计的提示词强制LLM显式化其状态跟踪过程，让LLM识别并报告预定义的状态标签，并结合状态特定的占位符后处理方法确保交易完整性。

Result: 在300个交易对话评估中，实现>99%状态合规性和99.3%计算精度。小模型(Gemini-2.5-Flash)使用ASTP和占位符后处理可匹配大模型(Gemini-2.5-Pro)性能，同时将响应时间从21.2秒降至2.4秒。

Conclusion: ASTP建立了满足商业游戏实时需求和资源限制的实用基础，在保持性能的同时显著提升效率。

Abstract: Large Language Models (LLMs) enable dynamic game interactions but fail to
follow essential procedural flows in rule-governed trading systems, eroding
player trust. This work resolves the core tension between the creative
flexibility of LLMs and the procedural demands of in-game trading
(browse-offer-review-confirm). To this end, Autoregressive State-Tracking
Prompting (ASTP) is introduced, a methodology centered on a strategically
orchestrated prompt that compels an LLM to make its state-tracking process
explicit and verifiable. Instead of relying on implicit contextual
understanding, ASTP tasks the LLM with identifying and reporting a predefined
state label from the previous turn. To ensure transactional integrity, this is
complemented by a state-specific placeholder post-processing method for
accurate price calculations. Evaluation across 300 trading dialogues
demonstrates >99% state compliance and 99.3% calculation precision. Notably,
ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)
matches larger models' (Gemini-2.5-Pro) performance while reducing response
time from 21.2s to 2.4s, establishing a practical foundation that satisfies
both real-time requirements and resource constraints of commercial games.

</details>


### [65] [Reasoning-Aware GRPO using Process Mining](https://arxiv.org/abs/2510.25065)
*Taekhyun Park,Yongjae Lee,Hyerim Bae*

Main category: cs.AI

TL;DR: 提出PM4GRPO方法，在GRPO后训练中引入过程挖掘技术，为推理过程提供奖励信号，显著提升大型推理模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的后训练方法主要关注结果奖励，缺乏对推理过程的监督，限制了模型推理能力的进一步提升

Method: 使用过程挖掘技术计算推理过程与预训练教师模型的对齐程度作为一致性奖励，结合标准答案/格式奖励进行GRPO优化

Result: 在五个基准测试上显著优于现有的GRPO后训练方法

Conclusion: 利用过程挖掘实现推理感知的GRPO能有效增强策略模型的推理能力

Abstract: Reinforcement learning (RL)-based post-training has been crucial for enabling
multi-step reasoning in large reasoning models (LRMs), yet current reward
schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware
Group Relative Policy Optimization (GRPO) that augments standard answer/format
rewards with signals over the reasoning procedure. To this end, process mining
techniques are utilized to compute a scalar conformance reward that measures
how closely a policy model's reasoning aligns with the pretrained teacher
model. The empirical results on five benchmarks demonstrate that PM4GRPO
significantly outperforms existing methodologies for GRPO-based post-training.
These results highlight that leveraging process mining for reasoning-aware GRPO
effectively enhances the reasoning capabilities of policy models.

</details>


### [66] [KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA](https://arxiv.org/abs/2510.25101)
*Zhuo Chen,Fei Wang,Zixuan Li,Zhao Zhang,Weiwei Ding,Chuanguang Yang,Yongjun Xu,Xiaolong Jin,Jiafeng Guo*

Main category: cs.AI

TL;DR: KnowCoder-A1是一个基于多阶段课程强化学习的KBQA系统，通过结果监督训练LLM在知识库上进行自主推理，显著提升了问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有KBQA方法通过过程监督微调LLM，但缺乏探索激励，无法有效增强代理推理能力。

Method: 采用多阶段课程强化学习，首先通过基于结果的拒绝采样获得高质量轨迹进行微调，然后使用从易到难的奖励调度进行RL训练。

Result: 在三个主流数据集上持续优于先前方法，在GrailQA零样本子集上相对提升11.1%，仅使用十二分之一的训练数据。

Conclusion: 基于结果监督的课程强化学习能有效训练LLM进行自主代理推理，显著提升KBQA性能。

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language
questions over a structured Knowledge Base (KB). Recent work improves KBQA by
adopting an agentic reasoning paradigm, in which Large Language Models (LLMs)
iteratively decompose a question, generate its corresponding logical queries,
and interact with the KB to derive the answer. However, these methods typically
fine-tune LLMs on reasoning trajectories synthesized via process supervision,
which offers weak incentives for exploration and thus fails to strengthen the
agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that
can autonomously perform agentic reasoning on KBs to obtain answers. To
incentivize autonomous exploration, KnowCoder-A1 trains the LLM under
outcome-only supervision via a multi-stage curriculum reinforcement learning
with an easy-to-hard curriculum. To establish foundational agentic
capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of
high-quality trajectories obtained through outcome-based rejection sampling.
Then, to alleviate the reward sparsity inherent in outcome-only supervision, it
applies multi-stage curriculum RL with reward schedules that progress from easy
to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful
reasoning behaviors and consistently outperforms prior approaches across three
mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1
achieves up to an 11.1% relative improvement while using only one-twelfth of
the training data, demonstrating strong agentic reasoning capabilities.

</details>


### [67] [Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models](https://arxiv.org/abs/2510.25179)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.AI

TL;DR: 提出Agentic Moderation框架，利用专门代理来防御多模态系统的越狱攻击，通过动态协作代理实现上下文感知和可解释的审核


<details>
  <summary>Details</summary>
Motivation: 将智能代理方法扩展到安全对齐领域，解决现有方法作为静态层仅提供二元分类的局限性

Method: 集成Shield、Responder、Evaluator和Reflector四个动态协作代理，实现模型无关的多模态系统安全防护

Result: 在五个数据集和四个大型视觉语言模型上的实验显示，攻击成功率降低7-19%，拒绝率提高4-20%，保持稳定的不遵循率

Conclusion: Agentic Moderation通过利用代理架构的灵活性和推理能力，为自动化安全治理提供了模块化、可扩展和细粒度的安全执行方案

Abstract: Agentic methods have emerged as a powerful and autonomous paradigm that
enhances reasoning, collaboration, and adaptive control, enabling systems to
coordinate and independently solve complex tasks. We extend this paradigm to
safety alignment by introducing Agentic Moderation, a model-agnostic framework
that leverages specialised agents to defend multimodal systems against
jailbreak attacks. Unlike prior approaches that apply as a static layer over
inputs or outputs and provide only binary classifications (safe or unsafe), our
method integrates dynamic, cooperative agents, including Shield, Responder,
Evaluator, and Reflector, to achieve context-aware and interpretable
moderation. Extensive experiments across five datasets and four representative
Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the
Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),
and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,
and well-balanced safety performance. By harnessing the flexibility and
reasoning capacity of agentic architectures, Agentic Moderation provides
modular, scalable, and fine-grained safety enforcement, highlighting the
broader potential of agentic systems as a foundation for automated safety
governance.

</details>


### [68] [RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models](https://arxiv.org/abs/2510.25206)
*Tianqianjin Lin,Xi Zhao,Xingyao Zhang,Rujiao Long,Yi Xu,Zhuoren Jiang,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 提出RAVR框架，利用答案引导的变分推理来增强LLM的推理能力，解决了在LLM能力不足时难以采样高质量推理路径的问题。


<details>
  <summary>Details</summary>
Motivation: 当LLM对任务能力不足时，难以生成高质量推理路径，强化学习容易强化熟悉但次优的推理。受认知科学启发，"为什么是这个答案"比"答案是什么"更容易回答，因为避免了开放式探索的认知负担。

Method: 引入RAVR框架，使用答案条件推理作为仅问题推理的变分代理，通过答案引导生成高质量推理路径。

Result: 在通用和数学领域的实验中，RAVR相比强基线模型表现出持续改进，减少了犹豫，加强了结论整合，并促进了问题特定策略的使用。

Conclusion: 答案引导的推理可以显著提高采样推理路径的期望效用，将难以处理的问题转化为可学习的问题。

Abstract: Reinforcement learning (RL) can refine the reasoning abilities of large
language models (LLMs), but critically depends on a key prerequisite: the LLM
can already generate high-utility reasoning paths with non-negligible
probability. For tasks beyond the LLM's current competence, such reasoning path
can be hard to sample, and learning risks reinforcing familiar but suboptimal
reasoning. We are motivated by the insight from cognitive science that Why is
this the answer is often an easier question than What is the answer, as it
avoids the heavy cognitive load of open-ended exploration, opting instead for
explanatory reconstruction-systematically retracing the reasoning that links a
question to its answer. We show that LLMs can similarly leverage answers to
derive high-quality reasoning paths. We formalize this phenomenon and prove
that conditioning on answer provably increases the expected utility of sampled
reasoning paths, thereby transforming intractable problems into learnable ones.
Building on this insight, we introduce RAVR (Reference-Answer-guided
Variational Reasoning), an end-to-end framework that uses answer-conditioned
reasoning as a variational surrogate for question-only reasoning. Experiments
in both general and math domains demonstrate consistent improvements over
strong baselines. We further analyze the reasoning behavior and find that RAVR
reduces hesitation, strengthens conclusion consolidation, and promotes
problem-specific strategies in reasoning.

</details>


### [69] [FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data](https://arxiv.org/abs/2510.25223)
*Kun ouyang,Haoyu Wang,Dong Fang*

Main category: cs.AI

TL;DR: FELA是一个基于LLM的多智能体进化系统，用于从复杂的工业事件日志数据中自动提取高性能特征，结合强化学习和遗传算法原理来平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 工业事件日志数据复杂异构，现有自动特征工程方法存在可解释性差、操作僵化、对复杂异构数据适应性差等问题。

Method: 采用多智能体系统（想法智能体、代码智能体、批评智能体、评估智能体），结合LLM的推理和编码能力，以及基于强化学习和遗传算法的智能体进化算法。

Result: 在真实工业数据集上的实验表明，FELA能生成可解释、领域相关的特征，显著提升模型性能并减少人工工作量。

Conclusion: 基于LLM的多智能体系统有潜力成为复杂现实环境中自动化、可解释和自适应特征工程的通用框架。

Abstract: Event log data, recording fine-grained user actions and system events,
represent one of the most valuable assets for modern digital services. However,
the complexity and heterogeneity of industrial event logs--characterized by
large scale, high dimensionality, diverse data types, and intricate temporal or
relational structures--make feature engineering extremely challenging. Existing
automatic feature engineering approaches, such as AutoML or genetic methods,
often suffer from limited explainability, rigid predefined operations, and poor
adaptability to complicated heterogeneous data. In this paper, we propose FELA
(Feature Engineering LLM Agents), a multi-agent evolutionary system that
autonomously extracts meaningful and high-performing features from complex
industrial event log data. FELA integrates the reasoning and coding
capabilities of large language models (LLMs) with an insight-guided
self-evolution paradigm. Specifically, FELA employs specialized agents--Idea
Agents, Code Agents, and Critic Agents--to collaboratively generate, validate,
and implement novel feature ideas. An Evaluation Agent summarizes feedback and
updates a hierarchical knowledge base and dual-memory system to enable
continual improvement. Moreover, FELA introduces an agentic evolution
algorithm, combining reinforcement learning and genetic algorithm principles to
balance exploration and exploitation across the idea space. Extensive
experiments on real industrial datasets demonstrate that FELA can generate
explainable, domain-relevant features that significantly improve model
performance while reducing manual effort. Our results highlight the potential
of LLM-based multi-agent systems as a general framework for automated,
interpretable, and adaptive feature engineering in complex real-world
environments.

</details>


### [70] [GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning](https://arxiv.org/abs/2510.25320)
*Jiaqi Wu,Qinlao Zhao,Zefeng Chen,Kai Qin,Yifei Zhao,Xueqian Wang,Yuhang Yao*

Main category: cs.AI

TL;DR: 提出Graph-based Agent Planning (GAP)框架，通过图规划实现并行工具执行，显著提升多步推理任务的效率和准确率


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自主代理（如ReAct）采用顺序推理执行，无法利用独立子任务的并行性，导致工具使用效率低下和多步推理性能不佳

Method: 训练代理基础模型将复杂任务分解为依赖感知的子任务图，自主确定并行和顺序执行的工具，采用两阶段训练：监督微调+基于正确性的强化学习

Result: 在MHQA数据集上显著优于传统ReAct基线，特别是在多步检索任务中，通过智能并行化大幅提升工具调用效率

Conclusion: GAP框架通过依赖感知的任务编排，在保持任务准确性的同时实现了执行效率的显著提升，为复杂任务解决提供了更高效的范式

Abstract: Autonomous agents powered by large language models (LLMs) have shown
impressive capabilities in tool manipulation for complex task-solving. However,
existing paradigms such as ReAct rely on sequential reasoning and execution,
failing to exploit the inherent parallelism among independent sub-tasks. This
sequential bottleneck leads to inefficient tool utilization and suboptimal
performance in multi-step reasoning scenarios. We introduce Graph-based Agent
Planning (GAP), a novel framework that explicitly models inter-task
dependencies through graph-based planning to enable adaptive parallel and
serial tool execution. Our approach trains agent foundation models to decompose
complex tasks into dependency-aware sub-task graphs, autonomously determining
which tools can be executed in parallel and which must follow sequential
dependencies. This dependency-aware orchestration achieves substantial
improvements in both execution efficiency and task accuracy. To train GAP, we
construct a high-quality dataset of graph-based planning traces derived from
the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage
training strategy: supervised fine-tuning (SFT) on the curated dataset,
followed by reinforcement learning (RL) with a correctness-based reward
function on strategically sampled queries where tool-based reasoning provides
maximum value. Experimental results on MHQA datasets demonstrate that GAP
significantly outperforms traditional ReAct baselines, particularly on
multi-step retrieval tasks, while achieving dramatic improvements in tool
invocation efficiency through intelligent parallelization. The project page is
available at: https://github.com/WJQ7777/Graph-Agent-Planning.

</details>


### [71] [Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions](https://arxiv.org/abs/2510.25445)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.AI

TL;DR: 这篇论文提出了一个双范式框架，将智能体AI系统分为符号/经典范式和神经/生成范式，通过系统文献综述分析了两种范式在理论基础、领域应用和伦理挑战方面的差异，并指出未来发展方向是两者的有意整合。


<details>
  <summary>Details</summary>
Motivation: 解决智能体AI快速发展带来的概念混淆问题，避免将现代神经系统与过时的符号模型进行概念性回溯拟合，为理解智能体AI提供清晰的理论框架。

Method: 采用PRISMA系统文献综述方法，分析了2018-2025年间的90项研究，从三个维度进行结构化分析：理论基础与架构原则、领域特定实现、范式特定伦理与治理挑战。

Result: 发现范式选择具有战略性：符号系统主导安全关键领域（如医疗），神经系统主导适应性强的数据丰富环境（如金融）；识别出符号系统治理模型缺失和神经符号混合架构需求等关键研究空白。

Conclusion: 智能体AI的未来不在于单一范式的支配，而在于符号和神经范式的有意整合，以创建既适应性强又可靠的混合智能系统。

Abstract: Agentic AI represents a transformative shift in artificial intelligence, but
its rapid advancement has led to a fragmented understanding, often conflating
modern neural systems with outdated symbolic models -- a practice known as
conceptual retrofitting. This survey cuts through this confusion by introducing
a novel dual-paradigm framework that categorizes agentic systems into two
distinct lineages: the Symbolic/Classical (relying on algorithmic planning and
persistent state) and the Neural/Generative (leveraging stochastic generation
and prompt-driven orchestration). Through a systematic PRISMA-based review of
90 studies (2018--2025), we provide a comprehensive analysis structured around
this framework across three dimensions: (1) the theoretical foundations and
architectural principles defining each paradigm; (2) domain-specific
implementations in healthcare, finance, and robotics, demonstrating how
application constraints dictate paradigm selection; and (3) paradigm-specific
ethical and governance challenges, revealing divergent risks and mitigation
strategies. Our analysis reveals that the choice of paradigm is strategic:
symbolic systems dominate safety-critical domains (e.g., healthcare), while
neural systems prevail in adaptive, data-rich environments (e.g., finance).
Furthermore, we identify critical research gaps, including a significant
deficit in governance models for symbolic systems and a pressing need for
hybrid neuro-symbolic architectures. The findings culminate in a strategic
roadmap arguing that the future of Agentic AI lies not in the dominance of one
paradigm, but in their intentional integration to create systems that are both
adaptable and reliable. This work provides the essential conceptual toolkit to
guide future research, development, and policy toward robust and trustworthy
hybrid intelligent systems.

</details>


### [72] [MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2510.25510)
*Zekun Xu,Siyu Xia,Chuhuai Yue,Jiajun Chai,Mingxue Tian,Xiaohan Wang,Wei Lin,Haoxuan Li,Guojun Yin*

Main category: cs.AI

TL;DR: 提出MTIR-SQL框架，将多轮工具调用与动态执行反馈结合，通过增强的GRPO算法在Text-to-SQL任务中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖静态执行反馈，限制了实时错误纠正能力。整合多轮工具调用和动态反馈可以显著提高模型的适应性和鲁棒性。

Method: 提出执行感知的多轮推理范式，在每个推理步骤无缝集成数据库执行反馈，扩展GRPO算法以适应复杂多轮交互场景，并加入轨迹过滤机制和移除KL损失约束。

Result: MTIR-SQL在BIRD Dev上达到64.4%准确率，在SPIDER Dev上达到84.6%执行准确率，显著优于现有方法。

Conclusion: 多轮工具集成推理与动态反馈的结合能够有效提升Text-to-SQL任务的性能，增强模型的适应性和鲁棒性。

Abstract: As large language models (LLMs) are increasingly used in Text-to-SQL tasks,
Reinforcement Learning (RL) has become a common method for improving
performance. Existing methods primarily rely on static execution feedback,
which restricts real-time error correction. However, integrating multi-turn
tool invocation along with dynamic feedback could significantly improve
adaptability and robustness, ultimately enhancing model performance. To address
these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated
Reasoning reinforcement learning framework for Text-to-SQL. Our approach
introduces an execution-aware multi-turn reasoning paradigm that seamlessly
incorporates database execution feedback at each reasoning step, enabling
context-sensitive query generation and progressive refinement throughout the
reasoning process. The framework extends the GRPO algorithm to accommodate
complex multi-turn interaction scenarios. Considering the training instability
characteristics of MTIR and the potential for significant Deviation of model
distribution from the initial model, we enhance the GRPO algorithm by adding a
trajectory filtering mechanism and removing KL loss constraints. Experimental
results demonstrate that MTIR-SQL, with 4B parameters, achieves \textbf{64.4}\%
accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,
significantly outperforming existing approaches.

</details>


### [73] [Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation](https://arxiv.org/abs/2510.25518)
*Thomas Cook,Richard Osuagwu,Liman Tsatiashvili,Vrynsia Vrynsia,Koustav Ghosal,Maraim Masoud,Riccardo Mattivi*

Main category: cs.AI

TL;DR: 提出一种面向金融科技领域的智能RAG架构，通过模块化代理管道解决专业领域检索挑战，在85个问答对数据集上验证了优于标准RAG的性能。


<details>
  <summary>Details</summary>
Motivation: 解决金融科技等专业领域中RAG系统面临的领域特定本体、密集术语和缩略语带来的检索与合成困难。

Method: 采用模块化代理架构，包括智能查询重构、基于关键词提取的迭代子查询分解、上下文缩略语解析和交叉编码器上下文重排序。

Result: 在检索精度和相关性方面优于标准RAG基线，但延迟有所增加。

Conclusion: 结构化多代理方法为复杂领域特定环境中的检索鲁棒性增强提供了有前景的方向。

Abstract: Retrieval-Augmented Generation (RAG) systems often face limitations in
specialized domains such as fintech, where domain-specific ontologies, dense
terminology, and acronyms complicate effective retrieval and synthesis. This
paper introduces an agentic RAG architecture designed to address these
challenges through a modular pipeline of specialized agents. The proposed
system supports intelligent query reformulation, iterative sub-query
decomposition guided by keyphrase extraction, contextual acronym resolution,
and cross-encoder-based context re-ranking. We evaluate our approach against a
standard RAG baseline using a curated dataset of 85 question--answer--reference
triples derived from an enterprise fintech knowledge base. Experimental results
demonstrate that the agentic RAG system outperforms the baseline in retrieval
precision and relevance, albeit with increased latency. These findings suggest
that structured, multi-agent methodologies offer a promising direction for
enhancing retrieval robustness in complex, domain-specific settings.

</details>


### [74] [Off-policy Reinforcement Learning with Model-based Exploration Augmentation](https://arxiv.org/abs/2510.25529)
*Likun Wang,Xiangteng Zhang,Yinuo Wang,Guojian Zhan,Wenxuan Wang,Haoyu Gao,Jingliang Duan,Shengbo Eben Li*

Main category: cs.AI

TL;DR: 提出MoGE方法，通过生成未充分探索的关键状态和动态一致的经验来增强强化学习探索，包含扩散模型生成器和一步想象世界模型两个组件。


<details>
  <summary>Details</summary>
Motivation: 现有探索方法存在局限性：主动探索在高维环境中表现不佳，被动探索受限于样本多样性不足。需要解决被动探索的局限性。

Method: MoGE包含两个组件：(1)基于扩散模型的关键状态生成器，在效用函数指导下合成关键状态；(2)一步想象世界模型，基于关键状态构建关键转换用于智能体学习。

Result: 在OpenAI Gym和DeepMind Control Suite上的实验表明，MoGE有效连接了探索和策略学习，在复杂控制任务中显著提高了样本效率和性能。

Conclusion: MoGE采用模块化设计，可与现有算法无缝集成，在不改变核心结构的情况下改善探索能力。

Abstract: Exploration is fundamental to reinforcement learning (RL), as it determines
how effectively an agent discovers and exploits the underlying structure of its
environment to achieve optimal performance. Existing exploration methods
generally fall into two categories: active exploration and passive exploration.
The former introduces stochasticity into the policy but struggles in
high-dimensional environments, while the latter adaptively prioritizes
transitions in the replay buffer to enhance exploration, yet remains
constrained by limited sample diversity. To address the limitation in passive
exploration, we propose Modelic Generative Exploration (MoGE), which augments
exploration through the generation of under-explored critical states and
synthesis of dynamics-consistent experiences through transition models. MoGE is
composed of two components: (1) a diffusion-based generator that synthesizes
critical states under the guidance of a utility function evaluating each
state's potential influence on policy exploration, and (2) a one-step
imagination world model for constructing critical transitions based on the
critical states for agent learning. Our method adopts a modular formulation
that aligns with the principles of off-policy learning, allowing seamless
integration with existing algorithms to improve exploration without altering
their core structures. Empirical results on OpenAI Gym and DeepMind Control
Suite reveal that MoGE effectively bridges exploration and policy learning,
leading to remarkable gains in both sample efficiency and performance across
complex control tasks.

</details>


### [75] [Counterfactual-based Agent Influence Ranker for Agentic AI Workflows](https://arxiv.org/abs/2510.25612)
*Amit Giloni,Chiara Picardi,Roy Betser,Shamik Bose,Aishvariya Priya Rathina Sabapathy,Roman Vainshtein*

Main category: cs.AI

TL;DR: 提出了CAIR方法，用于评估多智能体系统中各智能体对最终输出的影响程度，是首个能够量化智能体影响力的方法。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的多智能体系统（AAW）的广泛采用和高度自主性，需要从质量和安全角度深入理解其运作机制，但目前缺乏评估各智能体对最终输出影响的方法。

Method: 使用反事实分析技术，开发了CAIR方法，通过改变特定智能体的输出来评估其对最终结果的影响程度，提供任务无关的分析能力。

Result: 在包含30个不同用例和230个功能的AAW数据集上评估，CAIR能产生一致的排名结果，优于基线方法，并能有效提升下游任务的效率和相关性。

Conclusion: CAIR是首个能够评估多智能体系统中智能体影响力的方法，具有任务无关性，可在离线和推理时使用，为理解和优化AAW系统提供了重要工具。

Abstract: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,
is an autonomous system that assembles several LLM-based agents to work
collaboratively towards a shared goal. The high autonomy, widespread adoption,
and growing interest in such AAWs highlight the need for a deeper understanding
of their operations, from both quality and security aspects. To this day, there
are no existing methods to assess the influence of each agent on the AAW's
final output. Adopting techniques from related fields is not feasible since
existing methods perform only static structural analysis, which is unsuitable
for inference time execution. We present Counterfactual-based Agent Influence
Ranker (CAIR) - the first method for assessing the influence level of each
agent on the AAW's output and determining which agents are the most
influential. By performing counterfactual analysis, CAIR provides a
task-agnostic analysis that can be used both offline and at inference time. We
evaluate CAIR using an AAWs dataset of our creation, containing 30 different
use cases with 230 different functionalities. Our evaluation showed that CAIR
produces consistent rankings, outperforms baseline methods, and can easily
enhance the effectiveness and relevancy of downstream tasks.

</details>
